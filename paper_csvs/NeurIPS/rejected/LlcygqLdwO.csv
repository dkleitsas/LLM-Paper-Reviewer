Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016260162601626016,"Convolutional Neural Networks (CNNs) have seen significant performance im-
1"
ABSTRACT,0.0032520325203252032,"provements in recent years. However, due to their size and complexity, their
2"
ABSTRACT,0.004878048780487805,"decision-making process remains a black-box, leading to opacity and trust issues.
3"
ABSTRACT,0.0065040650406504065,"State-of-the-art saliency methods can generate local explanations that highlight the
4"
ABSTRACT,0.008130081300813009,"area in the input image where a class is identified but do not explain how different
5"
ABSTRACT,0.00975609756097561,"features contribute to the prediction. On the other hand, concept-based methods,
6"
ABSTRACT,0.011382113821138212,"such as TCAV (Testing with Concept Activation Vectors), provide global explain-
7"
ABSTRACT,0.013008130081300813,"ability, but cannot compute the attribution of a concept in a specific prediction nor
8"
ABSTRACT,0.014634146341463415,"show the locations where the network detects these concepts. This paper introduces
9"
ABSTRACT,0.016260162601626018,"a novel explainability framework, Visual-TCAV, which aims to bridge the gap
10"
ABSTRACT,0.01788617886178862,"between these methods. Visual-TCAV uses Concept Activation Vectors (CAVs) to
11"
ABSTRACT,0.01951219512195122,"generate saliency maps that show where concepts are recognized by the network.
12"
ABSTRACT,0.02113821138211382,"Moreover, it can estimate the attribution of these concepts to the output of any
13"
ABSTRACT,0.022764227642276424,"class using a generalization of Integrated Gradients. Visual-TCAV can provide
14"
ABSTRACT,0.024390243902439025,"both local and global explanations for any CNN-based image classification model
15"
ABSTRACT,0.026016260162601626,"without requiring any modifications. This framework is evaluated on widely used
16"
ABSTRACT,0.027642276422764227,"CNNs and its validity is further confirmed through experiments where a ground
17"
ABSTRACT,0.02926829268292683,"truth for explanations is known.
18"
INTRODUCTION,0.030894308943089432,"1
Introduction
19"
INTRODUCTION,0.032520325203252036,"Recent advancements in Deep Neural Networks (DNNs) have revolutionized the field of Artificial
20"
INTRODUCTION,0.03414634146341464,"Intelligence, and Convolutional Neural Networks (CNNs) have emerged as the state-of-the-art for
21"
INTRODUCTION,0.03577235772357724,"image classification due to their ability to learn complex patterns and features within images. However,
22"
INTRODUCTION,0.03739837398373984,"as the performance of these models has grown significantly over recent years, their complexity has
23"
INTRODUCTION,0.03902439024390244,"also increased. Consequently, it became a challenge to understand how these models produce their
24"
INTRODUCTION,0.04065040650406504,"classifications. This led to the widespread use of the term black-box to describe these models, as only
25"
INTRODUCTION,0.04227642276422764,"their inputs and outputs are known, while their internal mechanisms remain too complex for humans
26"
INTRODUCTION,0.04390243902439024,"to comprehend. The black-box problem results in a lack of transparency [29], which can undermine
27"
INTRODUCTION,0.04552845528455285,"trust in AI-based systems [12]. Indeed, blindly trusting AI poses serious ethical dilemmas, especially
28"
INTRODUCTION,0.04715447154471545,"in critical fields such as healthcare or autonomous driving in which image classification systems are
29"
INTRODUCTION,0.04878048780487805,"becoming increasingly employed [28, 3]. Additionally, debugging black-box models and identifying
30"
INTRODUCTION,0.05040650406504065,"biases becomes difficult without comprehending the process they use to make predictions. To this
31"
INTRODUCTION,0.05203252032520325,"end, the field of Explainable Artificial Intelligence (XAI) has made significant progress in developing
32"
INTRODUCTION,0.05365853658536585,"techniques for producing explanations of AI decisions. However, comprehending the specific features
33"
INTRODUCTION,0.055284552845528454,"or patterns that networks identify in an image and their precise impact on the prediction remains a
34"
INTRODUCTION,0.056910569105691054,"challenge. State-of-the-art approaches for local explainability (i.e., for individual predictions) use
35"
INTRODUCTION,0.05853658536585366,"saliency maps to locate where a class is identified in an input image, but they can’t explain which
36"
INTRODUCTION,0.06016260162601626,"features led the model to its prediction. For instance, when analyzing an image of a golf ball, these
37"
INTRODUCTION,0.061788617886178863,"saliency methods cannot determine whether the golf ball was recognized by the spherical shape, the
38"
INTRODUCTION,0.06341463414634146,"dimples, or some other feature. Striving to cover this need, Kim et al. [11] introduced TCAV (Testing
39"
INTRODUCTION,0.06504065040650407,"with Concept Activation Vectors), a concept-based method that can discern whether a user-defined
40"
INTRODUCTION,0.06666666666666667,"concept (e.g., dimples, spherical) correlates positively with the output of a selected class. However,
41"
INTRODUCTION,0.06829268292682927,"TCAV is designed exclusively for global explainability (i.e., for explaining the general behavior of a
42"
INTRODUCTION,0.06991869918699187,"model) and therefore cannot measure the influence of a concept in a specific prediction or show the
43"
INTRODUCTION,0.07154471544715447,"locations within the input images where the networks recognize these concepts.
44"
INTRODUCTION,0.07317073170731707,"In this article, we introduce a novel explainability framework, Visual-TCAV, which integrates the core
45"
INTRODUCTION,0.07479674796747968,"principles of both saliency methods and concept-based approaches while aiming to overcome their
46"
INTRODUCTION,0.07642276422764227,"respective limitations. Visual-TCAV can be applied to any layer of a CNN model whose output is a
47"
INTRODUCTION,0.07804878048780488,"set of feature maps. Its main contributions are: (a) it provides visual explanations that show where
48"
INTRODUCTION,0.07967479674796749,"the network identifies user-defined concepts; (b) it can estimate the importance of these concepts to
49"
INTRODUCTION,0.08130081300813008,"the output of a selected class; (c) it can be used for both local and global explainability.
50"
RELATED WORKS,0.08292682926829269,"2
Related Works
51"
RELATED WORKS,0.08455284552845528,"In recent years, there has been a significant increase in the body of work exploring the explainability
52"
RELATED WORKS,0.08617886178861789,"of black-box models. For CNN-based image classification, state-of-the-art methods primarily focus
53"
RELATED WORKS,0.08780487804878048,"on providing explanations via saliency maps. These heatmaps highlight the most important regions
54"
RELATED WORKS,0.08943089430894309,"of the input image and therefore can be used to gain insights into how a model makes its decisions.
55"
RELATED WORKS,0.0910569105691057,"One approach for generating such visualizations involves studying the input-output relationship
56"
RELATED WORKS,0.09268292682926829,"of the model by creating a set of perturbed versions of the input and analyzing how the output
57"
RELATED WORKS,0.0943089430894309,"changes with each perturbation. Notable contributions to this approach include Local Interpretable
58"
RELATED WORKS,0.0959349593495935,"Model-Agnostic Explanations (LIME) [17], which uses random perturbations, and SHapley Additive
59"
RELATED WORKS,0.0975609756097561,"exPlanations (SHAP) [14], which estimates the importance of each pixel using Shapley values. A
60"
RELATED WORKS,0.0991869918699187,"different approach that instead tries to access the internal workings of the model was originally
61"
RELATED WORKS,0.1008130081300813,"proposed by Simonyan et al. [22] and consists of generating saliency maps based on the gradients
62"
RELATED WORKS,0.1024390243902439,"of the model output w.r.t. the input images. This idea led many researchers [24, 23] to investigate
63"
RELATED WORKS,0.1040650406504065,"how to exploit gradients to produce more accurate saliency maps. Selvaraju et al. [20] proposed a
64"
RELATED WORKS,0.10569105691056911,"method named Gradient-weighted Class Activation Mapping (Grad-CAM) that extracts the gradients
65"
RELATED WORKS,0.1073170731707317,"of the logits (i.e., raw pre-softmax predictions) w.r.t. the feature maps. It then uses a Global Average
66"
RELATED WORKS,0.10894308943089431,"Pooling (GAP) operation to transform these gradients into class-specific weights for each feature
67"
RELATED WORKS,0.11056910569105691,"map and performs a weighted sum of these feature maps to produce a class localization map, a
68"
RELATED WORKS,0.11219512195121951,"saliency map that highlights where a class is identified. Grad-CAM has gained considerable attention
69"
RELATED WORKS,0.11382113821138211,"and is extensively used for explaining convolutional networks. However, Sundararajan et al. [25]
70"
RELATED WORKS,0.11544715447154472,"demonstrated that gradients can saturate, leading to an inaccurate assessment of feature importance.
71"
RELATED WORKS,0.11707317073170732,"To address this issue, they introduced Integrated Gradients (IG), a method that calculates feature
72"
RELATED WORKS,0.11869918699186992,"attribution by integrating the gradients along a path from a baseline (e.g., a black image) to the
73"
RELATED WORKS,0.12032520325203253,"actual input image. Notable contributions of IG and its variants [10, 16, 30] include the ability to
74"
RELATED WORKS,0.12195121951219512,"provide fine-grained saliency maps (i.e., each pixel has its attribution) and adherence to the axiom of
75"
RELATED WORKS,0.12357723577235773,"completeness (i.e., the sum of the attributions of all pixels equals the logit value).
76"
RELATED WORKS,0.12520325203252033,"While saliency methods are effective and intuitive, they might not always provide a complete picture
77"
RELATED WORKS,0.12682926829268293,"of why a model made a certain decision. This is because these methods perform class localization,
78"
RELATED WORKS,0.12845528455284552,"but cannot explain which features led the model to recognize the highlighted class. Furthermore,
79"
RELATED WORKS,0.13008130081300814,"these techniques rely on per-pixel importance which can’t be generalized across multiple instances, as
80"
RELATED WORKS,0.13170731707317074,"the position of these pixels is only meaningful for a specific input image. Consequently, they can only
81"
RELATED WORKS,0.13333333333333333,"explain one image at a time, preventing them from providing global explanations. To overcome these
82"
RELATED WORKS,0.13495934959349593,"limitations, Kim et al. [11] proposed Testing with Concept Activation Vectors (TCAV), a method that
83"
RELATED WORKS,0.13658536585365855,"investigates the correlations between user-defined concepts and the network’s predictions using a set
84"
RELATED WORKS,0.13821138211382114,"of example images representing a concept. For instance, images of stripes can be used to determine
85"
RELATED WORKS,0.13983739837398373,"whether the network is sensitive to the “striped” concept for predicting the “zebra” class. This is
86"
RELATED WORKS,0.14146341463414633,"accomplished by calculating a Concept Activation Vector (CAV), which is a vector orthogonal to
87"
RELATED WORKS,0.14308943089430895,"the decision boundary of a linear classifier, typically Support Vector Machines (SVMs), trained to
88"
RELATED WORKS,0.14471544715447154,"differentiate between the feature maps of concept examples and random images. From this, a TCAV
89"
RELATED WORKS,0.14634146341463414,"score for any concept and model’s layer can be computed using the signs of the dot products between
90"
RELATED WORKS,0.14796747967479676,"the CAV and the gradients of the loss w.r.t. the feature maps produced by images of a selected class.
91"
RELATED WORKS,0.14959349593495935,"TCAV is effective in detecting specific biases in neural networks (e.g., ethnicity-related) and can be
92"
RELATED WORKS,0.15121951219512195,"Figure 1: The Visual-TCAV process for generating local explanations. A Pooled-CAV is computed
using the feature maps of user-defined concept examples and random images. A concept map is
then produced through a weighted sum of the Pooled-CAV and the image’s feature maps. Finally,
a concept attribution is obtained by extracting the IG attributions of the neurons that the concept
activates using the Pooled-CAV and the concept map, which is used as a spatial mask."
RELATED WORKS,0.15284552845528454,"considered complementary to saliency methods. Indeed, while saliency methods apply exclusively
93"
RELATED WORKS,0.15447154471544716,"to individual predictions, TCAV can only provide global explanations. However, TCAV does not
94"
RELATED WORKS,0.15609756097560976,"provide any information about the locations where concepts are identified within the input images.
95"
RELATED WORKS,0.15772357723577235,"This makes it challenging to assess whether a high score can truly be attributed to the intended
96"
RELATED WORKS,0.15934959349593497,"concept and not to a related one. Moreover, TCAV computes the network’s sensitivity to a concept,
97"
RELATED WORKS,0.16097560975609757,"but not the magnitude of its importance in the prediction as the score only depends on the signs of the
98"
RELATED WORKS,0.16260162601626016,"directional derivatives. For instance, “white” and “dimples” concepts might have identical TCAV
99"
RELATED WORKS,0.16422764227642275,"scores for the “golf ball” class, even if one contributes substantially more to the prediction.
100"
RELATED WORKS,0.16585365853658537,"TCAV has received attention within the XAI community, leading to various extensions [5, 8] and
101"
RELATED WORKS,0.16747967479674797,"applications [13, 2]. While our study focuses on user-defined concepts, unsupervised approaches
102"
RELATED WORKS,0.16910569105691056,"have also been proposed. Ghorbani et al. [7] introduced Automatic Concept Extraction (ACE), a
103"
RELATED WORKS,0.17073170731707318,"method that automatically extracts concepts from images for applying TCAV. This is accomplished
104"
RELATED WORKS,0.17235772357723578,"by segmenting input images and subsequently clustering their activations. Building upon ACE, Zhang
105"
RELATED WORKS,0.17398373983739837,"et al. [31] proposed Invertible Concept-based Explanations (ICE). This extension uses non-negative
106"
RELATED WORKS,0.17560975609756097,"CAVs derived from non-negative matrix factorization and can also be used to explain locally by
107"
RELATED WORKS,0.1772357723577236,"associating extracted concepts with a relevant area in the input image. Later, Bianchi et al. [1]
108"
RELATED WORKS,0.17886178861788618,"proposed an unsupervised method for visualizing the entire feature extraction process of CNNs. They
109"
RELATED WORKS,0.18048780487804877,"perform layer-wise clustering of similar feature maps to extract a set of concepts for each layer to
110"
RELATED WORKS,0.1821138211382114,"which they assign a descriptive label through crowdsourcing. This approach provides local and global
111"
RELATED WORKS,0.183739837398374,"explanations, but the reliance on crowdsourcing can pose a practical challenge. Furthermore, these
112"
RELATED WORKS,0.18536585365853658,"unsupervised approaches may provide opaque explanations. This is because, when the extracted
113"
RELATED WORKS,0.18699186991869918,"image regions contain overlapping concepts (e.g., dimples, spherical, and white in a golf ball), it
114"
RELATED WORKS,0.1886178861788618,"remains unclear which concepts the network has learned to recognize or considers more important.
115"
VISUAL-TCAV,0.1902439024390244,"3
Visual-TCAV
116"
VISUAL-TCAV,0.191869918699187,"This section presents the methodology of our framework, Visual-TCAV, which is designed to explain
117"
VISUAL-TCAV,0.19349593495934958,"the outputs of image classification CNNs using user-defined concepts. Local explanations can be
118"
VISUAL-TCAV,0.1951219512195122,"generated considering any layer and consist of two key components. The first is the Concept Map, a
119"
VISUAL-TCAV,0.1967479674796748,"saliency map that serves as a visual representation of the areas where the network has recognized
120"
VISUAL-TCAV,0.1983739837398374,"the selected concept in the input image. The second is the Concept Attribution, a numerical value
121"
VISUAL-TCAV,0.2,"that estimates the importance of the concept for the output of a selected class. Figure 1 illustrates the
122"
VISUAL-TCAV,0.2016260162601626,"pipeline for generating a local explanation. For global explanations, the process is replicated across
123"
VISUAL-TCAV,0.2032520325203252,"multiple input images. The concept attributions for each image are then averaged to quantify how the
124"
VISUAL-TCAV,0.2048780487804878,"concept influences the network’s decisions across a wide range of inputs.
125"
VISUAL-TCAV,0.20650406504065041,"(a) Hands
(b) Dimples
(c) Arches
(d) Sky
(e) Car
(f) Chequered"
VISUAL-TCAV,0.208130081300813,Figure 2: Examples of class-independent concept maps for various input images and concepts.
CAV GENERATION AND SPATIAL POOLING,0.2097560975609756,"3.1
CAV Generation and Spatial Pooling
126"
CAV GENERATION AND SPATIAL POOLING,0.21138211382113822,"Similarly to the TCAV framework, the initial step of our method consists of computing a Concept
127"
CAV GENERATION AND SPATIAL POOLING,0.21300813008130082,"Activation Vector (CAV) from a set of example images representing a user-defined concept, and a set
128"
CAV GENERATION AND SPATIAL POOLING,0.2146341463414634,"of negative examples (e.g., random images). Specifically, we use the Difference of Means method,
129"
CAV GENERATION AND SPATIAL POOLING,0.216260162601626,"proposed by Martin and Weller [15], to compute the CAV. They demonstrated that this approach
130"
CAV GENERATION AND SPATIAL POOLING,0.21788617886178863,"produces CAVs that are more resilient to perturbation and consistent than logistic classifiers or SVMs.
131"
CAV GENERATION AND SPATIAL POOLING,0.21951219512195122,"As the name suggests, this method uses the arithmetic mean to determine the centroids of both the
132"
CAV GENERATION AND SPATIAL POOLING,0.22113821138211381,"concept’s activations and the activations of random images. Subsequently, it directly computes the
133"
CAV GENERATION AND SPATIAL POOLING,0.22276422764227644,"CAV as the difference between these centroids.
134"
CAV GENERATION AND SPATIAL POOLING,0.22439024390243903,"Since we are interested in identifying which feature maps are activated by the concept, irrespective of
135"
CAV GENERATION AND SPATIAL POOLING,0.22601626016260162,"its location within the example images, we apply a Global Average Pooling (GAP) operation on the
136"
CAV GENERATION AND SPATIAL POOLING,0.22764227642276422,"obtained CAV. The result is a vector of scalar values whose length is equal to the number of feature
137"
CAV GENERATION AND SPATIAL POOLING,0.22926829268292684,"maps of the layer under consideration. Each vector element is associated with a feature map, and its
138"
CAV GENERATION AND SPATIAL POOLING,0.23089430894308943,"raw value approximates the degree of correlation between that feature map and the concept. Moving
139"
CAV GENERATION AND SPATIAL POOLING,0.23252032520325203,"forward, we will refer to this vector as the Pooled-CAV.
140"
CONCEPT MAP,0.23414634146341465,"3.2
Concept Map
141"
CONCEPT MAP,0.23577235772357724,"From the Pooled-CAV, we can construct a concept map that locates a concept (c) within any input
142"
CONCEPT MAP,0.23739837398373984,"image to be explained. This is achieved by performing a weighted sum of the feature maps (fmapsk)
143"
CONCEPT MAP,0.23902439024390243,"of the input image, with the weights being the Pooled-CAV values (pc
k). Equation (1) shows how
144"
CONCEPT MAP,0.24065040650406505,"to compute a raw concept map (M c
raw). We also apply a ReLU function after the weighted sum
145"
CONCEPT MAP,0.24227642276422764,"because we are only interested in the image regions that positively correlate with the concept. The
146"
CONCEPT MAP,0.24390243902439024,"computation is similar to Grad-CAM’s equation, with the difference that we use the elements of the
147"
CONCEPT MAP,0.24552845528455283,"Pooled-CAV as weights instead of the global-average-pooled gradients.
148"
CONCEPT MAP,0.24715447154471545,"M c,raw = ReLU
X"
CONCEPT MAP,0.24878048780487805,"k
pc
k · fmapsk

(1)"
CONCEPT MAP,0.25040650406504067,"We refer to this concept map as raw due to the absence of a scale factor (i.e., a maximum value) that
149"
CONCEPT MAP,0.25203252032520324,"would allow us to compare the degree of activation of the concept map across different concepts,
150"
CONCEPT MAP,0.25365853658536586,"input images, and model layers. To this end, we derive a concept map’s scale factor from the
151"
CONCEPT MAP,0.2552845528455285,"example images the user provided, which represent an ideal concept. Formally, we use Equation (2)
152"
CONCEPT MAP,0.25691056910569104,"to calculate the scale factor (sc) as the maximum value of a hypothetical concept map, computed
153"
CONCEPT MAP,0.25853658536585367,"using the centroid (Cc), derived from the mean of the feature maps of the example images for a
154"
CONCEPT MAP,0.2601626016260163,"concept (c). Subsequently, we normalize the raw concept map by dividing it by the scale factor (sc)
155"
CONCEPT MAP,0.26178861788617885,"and limiting the values to a unitary maximum, as shown in Equation (3). An epsilon (ε) is added to
156"
CONCEPT MAP,0.2634146341463415,"the denominator to prevent division by zero.
157"
CONCEPT MAP,0.26504065040650404,"sc = max

ReLU
X"
CONCEPT MAP,0.26666666666666666,"k
pc
k · Cc
k

(2)
M c
ij = min

1,
M c,raw
ij
sc + ε"
CONCEPT MAP,0.2682926829268293,"
∀i, j
(3)
158"
CONCEPT MAP,0.26991869918699185,"By overlaying the normalized concept map (M c) on the input image, we can generate a class-
159"
CONCEPT MAP,0.27154471544715447,"independent visualization (examples are shown in Figure 2) that highlights the region of the image
160"
CONCEPT MAP,0.2731707317073171,"where the network recognized the concept. This allows us to know, for any input image, the
161"
CONCEPT MAP,0.27479674796747966,"concept’s location and its degree of activation w.r.t an ideal concept defined by the user. Additionally,
162"
CONCEPT MAP,0.2764227642276423,"the concept map can provide a direct validation for the learned CAV, without requiring activation
163"
CONCEPT MAP,0.2780487804878049,"maximization techniques or sorting images based on their similarity to the CAV.
164"
CONCEPT ATTRIBUTION,0.27967479674796747,"3.3
Concept Attribution
165"
CONCEPT ATTRIBUTION,0.2813008130081301,"Once we acquire a set of concepts, we can gain insights into the network’s decision-making process
166"
CONCEPT ATTRIBUTION,0.28292682926829266,"by measuring the attribution of these user-defined concepts towards the raw predictions, also known
167"
CONCEPT ATTRIBUTION,0.2845528455284553,"as the logits. For instance, if the “church” class is predicted with a certain logit, we aim to quantify
168"
CONCEPT ATTRIBUTION,0.2861788617886179,"how much of this value is attributable to the “pews” concept, the “fresco” concept, and so on. More
169"
CONCEPT ATTRIBUTION,0.28780487804878047,"specifically, given an input image and a layer, we compute the attributions of the activations (i.e.,
170"
CONCEPT ATTRIBUTION,0.2894308943089431,"the values of the feature maps) to the logit of a specific target class. Subsequently, we utilize the
171"
CONCEPT ATTRIBUTION,0.2910569105691057,"Pooled-CAV to approximate which activations are attributable to a certain concept, and then we
172"
CONCEPT ATTRIBUTION,0.2926829268292683,"extract and sum these attributions. The attributions of a layer’s activations can be computed through a
173"
CONCEPT ATTRIBUTION,0.2943089430894309,"generalized variant of the IG approach which computes the integrated gradients of a target class’s
174"
CONCEPT ATTRIBUTION,0.2959349593495935,"logit w.r.t. the feature maps, instead of the input image. Specifically, we calculate the gradients along
175"
CONCEPT ATTRIBUTION,0.2975609756097561,"a straight-line path from zero-filled matrices to the actual feature maps and then approximate the
176"
CONCEPT ATTRIBUTION,0.2991869918699187,"integral using the Riemann trapezoidal rule. In our experiments, we consistently used 300 steps,
177"
CONCEPT ATTRIBUTION,0.3008130081300813,"which are sufficient to approximate the integral within a 5% error margin, as shown by Sundararajan
178"
CONCEPT ATTRIBUTION,0.3024390243902439,"et al. [25]. We then calculate the raw attributions by multiplying the integrated gradients with the
179"
CONCEPT ATTRIBUTION,0.3040650406504065,"feature maps, as shown in Figure 1. Since IG respects the completeness axiom regardless of which
180"
CONCEPT ATTRIBUTION,0.3056910569105691,"layer is considered as input, the attributions add up to the logit value of the target class, within the
181"
CONCEPT ATTRIBUTION,0.3073170731707317,"approximation error. A ReLU is then applied to extract positive attributions. These attributions
182"
CONCEPT ATTRIBUTION,0.3089430894308943,"are on the same scale as the raw logits, which can make their interpretation difficult. To obtain a
183"
CONCEPT ATTRIBUTION,0.3105691056910569,"comprehensible unitary scale, we normalize the attributions so that their sum equals a normalized
184"
CONCEPT ATTRIBUTION,0.3121951219512195,"logit, not the raw one. These normalized logits are obtained by applying a ReLU, followed by
185"
CONCEPT ATTRIBUTION,0.31382113821138213,"[0,1] rescaling to retain their relative ratios.
186"
CONCEPT ATTRIBUTION,0.3154471544715447,"To estimate the attribution of a concept (c), we can utilize the Pooled-CAV to perform a weighted sum
187"
CONCEPT ATTRIBUTION,0.3170731707317073,"of the normalized attributions (At,norm). Before this summation, we apply a ReLU and [0,1] rescaling
188"
CONCEPT ATTRIBUTION,0.31869918699186994,"to the Pooled-CAV (pc) so that we extract gradually less attribution for feature maps that are less
189"
CONCEPT ATTRIBUTION,0.3203252032520325,"correlated with the concept. The rationale behind using the ReLU is to discard the attribution of
190"
CONCEPT ATTRIBUTION,0.32195121951219513,"feature maps that show a negative correlation with the concept. In other words, if a certain feature
191"
CONCEPT ATTRIBUTION,0.3235772357723577,"map is activated by other non-correlated features, we discard its attribution. Finally, as shown in
192"
CONCEPT ATTRIBUTION,0.3252032520325203,"Equation (4), we obtain the ConceptAttribution for a concept (c) and a target class (t) by summing
193"
CONCEPT ATTRIBUTION,0.32682926829268294,"all values of an element-wise multiplication of the weighted attributions and the concept map (M c),
194"
CONCEPT ATTRIBUTION,0.3284552845528455,"which is used as a spatial mask. This enables us to discard the attributions of activations related to the
195"
CONCEPT ATTRIBUTION,0.3300813008130081,"regions within the input image where the concept is not present or was not recognized.
196"
CONCEPT ATTRIBUTION,0.33170731707317075,"ConceptAttributionc,t =
X"
CONCEPT ATTRIBUTION,0.3333333333333333,"i,j
M c
ij ·
X"
CONCEPT ATTRIBUTION,0.33495934959349594,"k
ReLU(pc,norm
k
) · At,norm
k
"
CONCEPT ATTRIBUTION,0.33658536585365856,"ij
(4)"
CONCEPT ATTRIBUTION,0.3382113821138211,"The concept attribution is a per-concept metric of importance, meaning that two concepts can have
197"
CONCEPT ATTRIBUTION,0.33983739837398375,"significantly different attributions even if they are recognized in the same location of the input
198"
CONCEPT ATTRIBUTION,0.34146341463414637,"image, resulting in similar concept maps. For instance, considering the “zebra” class, the attribution
199"
CONCEPT ATTRIBUTION,0.34308943089430893,"of the “striped” concept could be significantly different from the attribution of the “fur” concept.
200"
CONCEPT ATTRIBUTION,0.34471544715447155,"This distinction is achieved by focusing not on per-pixel attributions but on the attributions of the
201"
CONCEPT ATTRIBUTION,0.3463414634146341,"activations produced by the neurons responsible for recognizing these two concepts. Moreover, since
202"
CONCEPT ATTRIBUTION,0.34796747967479674,"the attribution of a concept is independent of its location, we can average it across multiple input
203"
CONCEPT ATTRIBUTION,0.34959349593495936,"images to provide a quantitative measure of the overall importance of that concept for that particular
204"
CONCEPT ATTRIBUTION,0.35121951219512193,"class, thus providing a global explanation. For instance, we can calculate a global attribution of the
205"
CONCEPT ATTRIBUTION,0.35284552845528455,"“striped” concept for the “zebra” target class by averaging the attribution of “striped” across a large
206"
CONCEPT ATTRIBUTION,0.3544715447154472,"number (e.g., 200) of images containing zebras.
207"
EXPERIMENTS AND RESULTS,0.35609756097560974,"4
Experiments and Results
208"
EXPERIMENTS AND RESULTS,0.35772357723577236,"In this section, we present the results of applying Visual-TCAV to the following convolutional
209"
EXPERIMENTS AND RESULTS,0.359349593495935,"networks pre-trained on the ImageNet [6] dataset: GoogLeNet [26], InceptionV3 [27], VGG16 [21],
210"
EXPERIMENTS AND RESULTS,0.36097560975609755,"and ResNet50V2 [9]. Examples of “striped”, “zigzagged”, “waffled”, and “chequered” concepts are
211"
EXPERIMENTS AND RESULTS,0.36260162601626017,"hands in GoogLeNet
striped in ResNet50V2
waffled in InceptionV3
pews in VGG16"
EXPERIMENTS AND RESULTS,0.3642276422764228,"Figure 3: Examples of layer-wise local explanations for various concepts and networks. We compute
the attribution of each concept for the top three predicted classes and the last seven layers."
EXPERIMENTS AND RESULTS,0.36585365853658536,"sourced from the Describable Textures Dataset (DTD) [4], while “pews” and “fresco” are generated
212"
EXPERIMENTS AND RESULTS,0.367479674796748,"through Stable Diffusion v1.5 [18] (more on this in Appendix E). Other concepts are obtained from
213"
EXPERIMENTS AND RESULTS,0.36910569105691055,"popular image search engines. Similarly to TCAV, we use a minimum of 30 example images per
214"
EXPERIMENTS AND RESULTS,0.37073170731707317,"concept and 500 random images as negative examples, as suggested by Martin and Weller [15].
215"
EXPERIMENTS AND RESULTS,0.3723577235772358,"Our experiments are conducted on an Intel i7 13700k with an Nvidia RTX 4060Ti 16GB, and 32 GB
216"
EXPERIMENTS AND RESULTS,0.37398373983739835,"of DDR5 RAM. The software runs on TensorFlow 2.15.1, CUDA 12.2, and Python 3.11.5. Local
217"
EXPERIMENTS AND RESULTS,0.375609756097561,"explanations, with 300 steps and seven layers, take less than a minute, while global explanations with
218"
EXPERIMENTS AND RESULTS,0.3772357723577236,"200 class images, 300 steps, and seven layers, can take anywhere from 5 to 20 minutes, depending on
219"
EXPERIMENTS AND RESULTS,0.37886178861788616,"the model. For global explanations, the computation time remains nearly constant regardless of the
220"
EXPERIMENTS AND RESULTS,0.3804878048780488,"number of concepts processed simultaneously. The official implementation is available in our GitHub
221"
EXPERIMENTS AND RESULTS,0.3821138211382114,"repository: removed for anonymity, see supplemental material .zip file.
222"
LOCAL EXPLANATIONS,0.383739837398374,"4.1
Local Explanations
223"
LOCAL EXPLANATIONS,0.3853658536585366,"In Figure 3, we provide local explanations for various concepts. While concept maps are class-
224"
LOCAL EXPLANATIONS,0.38699186991869916,"independent, the attribution of each concept depends on the class considered. We examine the top
225"
LOCAL EXPLANATIONS,0.3886178861788618,"three predicted classes in our examples and apply Visual-TCAV to a subset of the CNNs’ layers. On
226"
LOCAL EXPLANATIONS,0.3902439024390244,"one hand, we can observe a substantial increase in attributions in deeper layers, reaching a peak in
227"
LOCAL EXPLANATIONS,0.39186991869918697,"the final layer, which holds the most information about the importance of each concept for a specific
228"
LOCAL EXPLANATIONS,0.3934959349593496,"class, given its proximity to the output. On the other hand, the most accurate concept maps are
229"
LOCAL EXPLANATIONS,0.3951219512195122,"typically found in slightly earlier layers due to their neurons having smaller receptive fields.
230"
LOCAL EXPLANATIONS,0.3967479674796748,"Figure 4: Results of global explanations for a variety of concepts, classes, and networks. Each bar
chart reports the attributions of three concepts for a given class, throughout the last seven layers of
each network. The attributions of each concept are computed across 200 images of the selected class.
Although the theoretical limit of concept attributions is 1.0, the scale in our charts only extends to 0.6.
This is based on our empirical observations, which rarely identified concepts with a global attribution
exceeding this value."
LOCAL EXPLANATIONS,0.3983739837398374,"Furthermore, these layer-wise explanations enable us to identify when specific concepts are recognized
231"
LOCAL EXPLANATIONS,0.4,"within the network. For instance, the “waffled” concept does not significantly activate the initial
232"
LOCAL EXPLANATIONS,0.4016260162601626,"layers of InceptionV3, but it is recognized by deeper layers with a considerable attribution in the final
233"
LOCAL EXPLANATIONS,0.4032520325203252,"one. We also observe that the “hands” concept is detected mainly by earlier layers and contributes
234"
LOCAL EXPLANATIONS,0.40487804878048783,"only marginally to the score of the top classes for the analyzed image. This observation aligns
235"
LOCAL EXPLANATIONS,0.4065040650406504,"with the common intuition that “hands” are not class-discriminative in this particular case for the
236"
LOCAL EXPLANATIONS,0.408130081300813,"classes “beer glass”, “cocktail shaker”, and “espresso”. In contrast, the “striped” and “pews” concepts
237"
LOCAL EXPLANATIONS,0.4097560975609756,"significantly activate the final layer and substantially contribute to the predictions, although with
238"
LOCAL EXPLANATIONS,0.4113821138211382,"different magnitudes of importance. In the case of the “zebra” image, for instance, the network’s
239"
LOCAL EXPLANATIONS,0.41300813008130083,"decision is largely influenced by the “striped” concept, which accounts for more than half the logit
240"
LOCAL EXPLANATIONS,0.4146341463414634,"value of the “zebra” class. This concept also has a notable impact on the “prairie chicken” class and a
241"
LOCAL EXPLANATIONS,0.416260162601626,"marginal one on the “gondola” class, probably since gondoliers usually wear striped t-shirts. More
242"
LOCAL EXPLANATIONS,0.41788617886178864,"examples of local explanations can be found in Appendix C.
243"
GLOBAL EXPLANATIONS,0.4195121951219512,"4.2
Global Explanations
244"
GLOBAL EXPLANATIONS,0.4211382113821138,"The concept attribution is a per-concept metric of importance, hence we can derive global explanations
245"
GLOBAL EXPLANATIONS,0.42276422764227645,"by aggregating this attribution across a wide range of input images of a selected class. In our
246"
GLOBAL EXPLANATIONS,0.424390243902439,"experiments, we utilize 200 images per class for each global explanation. For concepts that are
247"
GLOBAL EXPLANATIONS,0.42601626016260163,"inherently part of the class (e.g., “striped” for “zebra” or “dimples” for “golf ball”), we can directly
248"
GLOBAL EXPLANATIONS,0.4276422764227642,"use any image representing that class. On the other hand, for concepts that appear sporadically, we
249"
GLOBAL EXPLANATIONS,0.4292682926829268,"only use images where the concept is present. For instance, we only use images of church interiors
250"
GLOBAL EXPLANATIONS,0.43089430894308944,"for “pews” and “fresco” concepts, and images of church exteriors for the “steeple” concept. This
251"
GLOBAL EXPLANATIONS,0.432520325203252,"ensures that the explanations are independent of the frequency of the concept’s appearance in the
252"
GLOBAL EXPLANATIONS,0.43414634146341463,"class images.
253"
GLOBAL EXPLANATIONS,0.43577235772357725,"The results are shown in Figure 4. The attributions match our intuitive expectations, considering, for
254"
GLOBAL EXPLANATIONS,0.4373983739837398,"instance, the importance of the “striped” concept for “zebra” or “spotted” for “dalmatian”. Moreover,
255"
GLOBAL EXPLANATIONS,0.43902439024390244,"the final layer typically provides the highest attribution, which is expected for class discriminative
256"
GLOBAL EXPLANATIONS,0.44065040650406506,"concepts. However, there are instances, such as “chequered” and “newspaper” for “crossword puzzle”,
257"
GLOBAL EXPLANATIONS,0.44227642276422763,"where concepts recognized in the earlier layers have a greater impact on the network’s prediction. We
258"
GLOBAL EXPLANATIONS,0.44390243902439025,"observe a more gradual increase in attribution in VGG16 and GoogleNet, compared to InceptionV3
259"
GLOBAL EXPLANATIONS,0.44552845528455287,"and ResNet50V2. This could be attributed to the depth of the latter networks, which means they
260"
GLOBAL EXPLANATIONS,0.44715447154471544,"perform more convolution operations that could potentially lead to a more complex feature extraction
261"
GLOBAL EXPLANATIONS,0.44878048780487806,"between the analyzed layers. More examples of global explanations are provided in Appendix D.
262"
GLOBAL EXPLANATIONS,0.4504065040650406,"Train Set
Test Set"
GLOBAL EXPLANATIONS,0.45203252032520325,(a) Models results
GLOBAL EXPLANATIONS,0.45365853658536587,"No Tags
100% Tags"
GLOBAL EXPLANATIONS,0.45528455284552843,(b) Visual-TCAV for entities
GLOBAL EXPLANATIONS,0.45691056910569106,"No Tags
100% Tags"
GLOBAL EXPLANATIONS,0.4585365853658537,(c) Visual-TCAV for tags
GLOBAL EXPLANATIONS,0.46016260162601624,"Figure 5: The results of the validation experiment. The upper section of the figure shows the test
results and the concept attributions for both entities and tags across all models. The lower section
provides examples of tagged images and concept maps for the no tags model and 100% tags model."
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.46178861788617886,"4.3
Validation Experiment with Ground Truth
263"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4634146341463415,"We conduct a validation experiment to evaluate the effectiveness of Visual-TCAV. In this experiment,
264"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.46504065040650405,"we train convolutional networks in a controlled setting, where ground truth is known, and assess
265"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4666666666666667,"whether the Visual-TCAV attributions match this ground truth. For this purpose, we create a dataset
266"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4682926829268293,"of three classes – cucumber, taxi, and zebra – which are the same classes used in the TCAV paper.
267"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.46991869918699186,"We then create multiple versions of this dataset by altering a percentage of the images with a tag,
268"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4715447154471545,"represented by a letter enclosed in a randomly sized square and added in a random location of the
269"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.47317073170731705,"image (examples are shown in Figure 5a). Specifically, zebra images are tagged with a “Z” in a
270"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.47479674796747967,"purple square, taxi images with a “T” in a magenta square, and cucumber images with a “C” in a
271"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4764227642276423,"cyan square. From these tagged images, we create five datasets: one of images without tags, and four
272"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.47804878048780486,"others with 25%, 50%, 75%, and 100% of tagged images, respectively. Each dataset is then used
273"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4796747967479675,"to train a different model, each including six convolutional layers and a GAP layer. Depending on
274"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4813008130081301,"the dataset used for training, each model may learn to recognize either the entities (i.e., cucumbers,
275"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.48292682926829267,"taxis, and zebras), the tags, or both and will decide which ones to give more importance. To obtain
276"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4845528455284553,"an approximated ground truth assessing which concept – entity or tag – is more important, we ask
277"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4861788617886179,"the models to classify a set of 200 incorrectly tagged test images per class. In this test set, taxis are
278"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4878048780487805,"tagged with the “Z”, cucumbers are tagged with the “T” and zebras are tagged with the “C”. If the
279"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4894308943089431,"network correctly classifies most of the images, it indicates that the entity is more important than the
280"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.49105691056910566,"tag, and thus, its attribution should be higher. On the other hand, if the performance deteriorates on
281"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4926829268292683,"these wrongly tagged images, it indicates that the tag is more important than the entity, and thus its
282"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4943089430894309,"attribution should be higher. We obtain the CAVs for entities using images of each class as concept
283"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4959349593495935,"examples and random images as negative examples. For tags, we use random images containing that
284"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4975609756097561,"tag as concept examples and images of cucumbers, taxis, and zebras containing the other two tags as
285"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.4991869918699187,"negative examples. We use the same incorrectly tagged test set to compute the concept attributions
286"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5008130081300813,"for both entities and tags across the last convolutional layer of all models.
287"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5024390243902439,"The results are shown in Figure 5. As expected, an increase in the percentage of tagged images
288"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5040650406504065,"correlates with a decrease in accuracy. In particular, for the “cucumber” class the accuracy declines
289"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5056910569105691,"much faster compared to other classes, with the majority of the images being incorrectly classified
290"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5073170731707317,"as taxis. This suggests that even the models trained on a small fraction of tagged images tend to
291"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5089430894308943,"overfit on the “T” tag. The concept attributions for both the “cucumber” entity and the “T” tag
292"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.510569105691057,"closely mirror this ground truth. The “zebra” entity and the “C” tag are also consistent with the
293"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5121951219512195,"ground truth: the attributions for “zebra” show a positive correlation with accuracy, whereas the
294"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5138211382113821,"attributions for the “C” tag demonstrate a clear inverse correlation. Notably, the networks did not
295"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5154471544715448,"pay much attention to the “Z” tag, focusing instead on the absence of the other two tags to classify
296"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5170731707317073,"zebras. Indeed, the model trained with 100% of images tagged classifies any image without a “C”
297"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5186991869918699,"or a “T” tag as “zebra”, regardless of whether the “Z” tag is present or not. This is confirmed by
298"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5203252032520326,"Figure 6: TCAV scores for tags and entities across each validation model. Results marked with an
asterisk (“*”) have been excluded due to statistical insignificance (p-value > 0.05)."
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5219512195121951,"our method, which assigns an attribution of nearly zero to both the “Z” tag and the “taxi” entity for
299"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5235772357723577,"the aforementioned model. We tested other saliency methods, such as Grad-CAM and IG, to further
300"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5252032520325203,"validate these findings. These methods do not highlight the “Z” tag either, but rather the entire image,
301"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.526829268292683,"in search of the “zebra” class (see Appendix B). For models trained with less than 100% of tags, the
302"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5284552845528455,"accuracy for “taxi” remains high, implying that these models are indeed capable of recognizing the
303"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5300813008130081,"“taxi” entity. The concept attribution for the “taxi” entity aligns with this observation. In Figures
304"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5317073170731708,"5b and 5c, we provide examples of concept maps for the model trained without tags and the model
305"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5333333333333333,"trained with 100% of tagged images. The former recognizes the entities but not the tags, while the
306"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5349593495934959,"latter struggles to recognize the entities but effectively identifies the “T” and “C” tags.
307"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5365853658536586,"Comparison with the TCAV Score. The primary difference between our concept attribution and the
308"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5382113821138211,"TCAV score is that the former considers not only the direction of gradients but also their magnitude.
309"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5398373983739837,"This allows us to measure the concept’s impact on the predictions, beyond just the network’s sensitivity
310"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5414634146341464,"to it. To demonstrate this, we compute the TCAV scores for tags and entities across each validation
311"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5430894308943089,"model (see Figure 6). On one hand, TCAV scores match the ground truth in showing that the network
312"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5447154471544715,"trained without tags exhibits high sensitivity to the entities and no sensitivity to the tags. Furthermore,
313"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5463414634146342,"TCAV aligns with the concept attribution in showing that the 100% tags model is sensitive to the
314"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5479674796747968,"“T” and “C” tags but not to the “Z”. On the other hand, TCAV struggles to capture the variations
315"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5495934959349593,"in the concept’s importance defined by ground truth. In fact, all models except the 100% tags show
316"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.551219512195122,"very similar TCAV scores for the entity concepts, even though their importance varies significantly
317"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5528455284552846,"across these models. This is attributable to most of the networks being sensitive to the entities.
318"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5544715447154471,"Indeed, on images without tags, the models’ accuracies are 96.5%, 96.2%, 96.2%, 95.2%, and 36.2%
319"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5560975609756098,"respectively. Similarly, the “C” tag has almost the same TCAV score for the models trained with 25%,
320"
VALIDATION EXPERIMENT WITH GROUND TRUTH,0.5577235772357724,"75%, and 100% tags, which is inconsistent with the decline in accuracy for the “C” tagged zebras.
321"
CONCLUSION,0.5593495934959349,"5
Conclusion
322"
CONCLUSION,0.5609756097560976,"In this article, we introduced a novel method, Visual-TCAV, to explain the outputs of image classi-
323"
CONCLUSION,0.5626016260162602,"fication models. This framework is capable of providing both local and global explanations based
324"
CONCLUSION,0.5642276422764227,"on high-level concepts, by estimating their attribution to the network’s predictions. Additionally,
325"
CONCLUSION,0.5658536585365853,"Visual-TCAV generates saliency maps to show where concepts are identified by the network, thereby
326"
CONCLUSION,0.567479674796748,"assuring the user that the attributions correspond to the intended concepts. The effectiveness of this
327"
CONCLUSION,0.5691056910569106,"method was demonstrated across a range of widely used CNNs and through a validation experiment,
328"
CONCLUSION,0.5707317073170731,"where Visual-TCAV successfully identified the most important concept in each examined model.
329"
CONCLUSION,0.5723577235772358,"Limitations and Future Work. Visual-TCAV provides a novel approach for concept-based explain-
330"
CONCLUSION,0.5739837398373984,"ability, but it has some limitations. Our current implementation only considers positive attributions
331"
CONCLUSION,0.5756097560975609,"for classes with positive logit values. However, since a concept may negatively impact the output,
332"
CONCLUSION,0.5772357723577236,"in future implementations we aim to include negative values, which would improve explanations
333"
CONCLUSION,0.5788617886178862,"and also extend the applicability of Visual-TCAV beyond classification tasks. Another limitation
334"
CONCLUSION,0.5804878048780487,"arises from the accumulation of noise along the IG linear path, which may sometimes result in
335"
CONCLUSION,0.5821138211382114,"slightly underestimated attributions. Future studies could investigate how to mitigate this using
336"
CONCLUSION,0.583739837398374,"alternative IG variants to compute the attributions of feature maps. Additionally, future research
337"
CONCLUSION,0.5853658536585366,"could explore generative approaches such as DreamBooth [19] to generate a large number of concept
338"
CONCLUSION,0.5869918699186992,"images starting from a small set of examples, leading to more robust CAVs and reducing workload for
339"
CONCLUSION,0.5886178861788618,"analysts. Finally, future works could study interconnections between concepts to determine how the
340"
CONCLUSION,0.5902439024390244,"activation of a concept might influence not only the output but also the activation of other concepts.
341"
REFERENCES,0.591869918699187,"References
342"
REFERENCES,0.5934959349593496,"[1] Matteo Bianchi, Antonio De Santis, Andrea Tocchetti, and Marco Brambilla. Interpretable
343"
REFERENCES,0.5951219512195122,"network visualizations: A human-in-the-loop approach for post-hoc explainability of cnn-based
344"
REFERENCES,0.5967479674796748,"image classification, 2024. URL https://doi.org/10.48550/arXiv.2405.03301.
345"
REFERENCES,0.5983739837398374,"[2] Carrie J. Cai, Jonas Jongejan, and Jess Holbrook. The effects of example-based explana-
346"
REFERENCES,0.6,"tions in a machine learning interface. In Proceedings of the 24th International Conference
347"
REFERENCES,0.6016260162601627,"on Intelligent User Interfaces, page 258–26. Association for Computing Machinery, 2019.
348"
REFERENCES,0.6032520325203252,"ISBN 9781450362726. doi: 10.1145/3301275.3302289. URL https://doi.org/10.1145/
349"
REFERENCES,0.6048780487804878,"3301275.3302289.
350"
REFERENCES,0.6065040650406504,"[3] Lei Cai, Jingyang Gao, and Di Zhao. A review of the application of deep learning in medical
351"
REFERENCES,0.608130081300813,"image classification and segmentation. Annals of Translational Medicine, 8(11), 2020. ISSN
352"
REFERENCES,0.6097560975609756,"2305-5847. URL https://atm.amegroups.com/article/view/36944.
353"
REFERENCES,0.6113821138211382,"[4] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild.
354"
REFERENCES,0.6130081300813008,"In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.
355"
REFERENCES,0.6146341463414634,"[5] Jonathan Crabbé and Mihaela van der Schaar.
Concept activation regions:
A gen-
356"
REFERENCES,0.616260162601626,"eralized framework for concept-based explanations.
In Sanmi Koyejo, S. Mohamed,
357"
REFERENCES,0.6178861788617886,"A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural In-
358"
REFERENCES,0.6195121951219512,"formation Processing Systems 35:
Annual Conference on Neural Information Process-
359"
REFERENCES,0.6211382113821138,"ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - Decem-
360"
REFERENCES,0.6227642276422765,"ber 9, 2022, 2022.
URL http://papers.nips.cc/paper_files/paper/2022/hash/
361"
REFERENCES,0.624390243902439,"11a7f429d75f9f8c6e9c630aeb6524b5-Abstract-Conference.html.
362"
REFERENCES,0.6260162601626016,"[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
363"
REFERENCES,0.6276422764227643,"scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
364"
REFERENCES,0.6292682926829268,"Recognition, pages 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.
365"
REFERENCES,0.6308943089430894,"[7] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-
366"
REFERENCES,0.6325203252032521,"based explanations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
367"
REFERENCES,0.6341463414634146,"R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
368"
REFERENCES,0.6357723577235772,"Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/
369"
REFERENCES,0.6373983739837399,"2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf.
370"
REFERENCES,0.6390243902439025,"[8] Mara Graziani, Vincent Andrearczyk, and Henning Müller. Regression concept vectors for
371"
REFERENCES,0.640650406504065,"bidirectional explanations in histopathology. In Danail Stoyanov, Zeike Taylor, Seyed Mostafa
372"
REFERENCES,0.6422764227642277,"Kia, Ipek Oguz, Mauricio Reyes, Anne Martel, Lena Maier-Hein, Andre F. Marquand, Edouard
373"
REFERENCES,0.6439024390243903,"Duchesnay, Tommy Löfstedt, Bennett Landman, M. Jorge Cardoso, Carlos A. Silva, Sergio
374"
REFERENCES,0.6455284552845528,"Pereira, and Raphael Meier, editors, Understanding and Interpreting Machine Learning in
375"
REFERENCES,0.6471544715447154,"Medical Image Computing Applications, pages 124–132, Cham, 2018. Springer International
376"
REFERENCES,0.6487804878048781,"Publishing. ISBN 978-3-030-02628-8.
377"
REFERENCES,0.6504065040650406,"[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
378"
REFERENCES,0.6520325203252032,"networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer
379"
REFERENCES,0.6536585365853659,"Vision – ECCV 2016, pages 630–645, Cham, 2016. Springer International Publishing. ISBN
380"
REFERENCES,0.6552845528455284,"978-3-319-46493-0.
381"
REFERENCES,0.656910569105691,"[10] Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin, Michael Terry, and
382"
REFERENCES,0.6585365853658537,"Tolga Bolukbasi. Guided integrated gradients: an adaptive path method for removing noise.
383"
REFERENCES,0.6601626016260163,"In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
384"
REFERENCES,0.6617886178861788,"5048–5056, 2021. doi: 10.1109/CVPR46437.2021.00501.
385"
REFERENCES,0.6634146341463415,"[11] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas,
386"
REFERENCES,0.6650406504065041,"and Rory sayres. Interpretability beyond feature attribution: Quantitative testing with concept
387"
REFERENCES,0.6666666666666666,"activation vectors (TCAV). In Jennifer Dy and Andreas Krause, editors, Proceedings of the
388"
REFERENCES,0.6682926829268293,"35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
389"
REFERENCES,0.6699186991869919,"Learning Research, pages 2668–2677. PMLR, 10–15 Jul 2018. URL https://proceedings.
390"
REFERENCES,0.6715447154471544,"mlr.press/v80/kim18d.html.
391"
REFERENCES,0.6731707317073171,"[12] Zachary Lipton. The mythos of model interpretability. Communications of the ACM, 61, 10
392"
REFERENCES,0.6747967479674797,"2016. doi: 10.1145/3233231.
393"
REFERENCES,0.6764227642276422,"[13] Adriano Lucieri, Muhammad Naseer Bajwa, Stephan Braun, Muhammad Imran Malik, Andreas
394"
REFERENCES,0.6780487804878049,"Dengel, and Sheraz Ahmed. On interpretability of deep learning based skin lesion classifiers
395"
REFERENCES,0.6796747967479675,"using concept activation vectors. pages 1–10, 07 2020. doi: 10.1109/IJCNN48605.2020.
396"
REFERENCES,0.6813008130081301,"9206946.
397"
REFERENCES,0.6829268292682927,"[14] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In
398"
REFERENCES,0.6845528455284553,"I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
399"
REFERENCES,0.6861788617886179,"editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,
400"
REFERENCES,0.6878048780487804,"Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
401"
REFERENCES,0.6894308943089431,"8a20a8621978632d76c43dfd28b67767-Paper.pdf.
402"
REFERENCES,0.6910569105691057,"[15] Tyler Martin and Adrian Weller. Interpretable Machine Learning. M.Phil. diss., Dept. of
403"
REFERENCES,0.6926829268292682,"Engineering, University of Cambridge, August 2019. URL https://www.mlmi.eng.cam.ac.
404"
REFERENCES,0.6943089430894309,"uk/files/tam_final_reduced.pdf.
405"
REFERENCES,0.6959349593495935,"[16] Deng Pan, Xin Li, and Dongxiao Zhu. Explaining deep neural network models with adversarial
406"
REFERENCES,0.697560975609756,"gradient integration. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International
407"
REFERENCES,0.6991869918699187,"Joint Conference on Artificial Intelligence, IJCAI-21, pages 2876–2883. International Joint
408"
REFERENCES,0.7008130081300813,"Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/396. URL
409"
REFERENCES,0.7024390243902439,"https://doi.org/10.24963/ijcai.2021/396. Main Track.
410"
REFERENCES,0.7040650406504065,"[17] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “why should i trust you?”: Explaining
411"
REFERENCES,0.7056910569105691,"the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International
412"
REFERENCES,0.7073170731707317,"Conference on Knowledge Discovery and Data Mining, KDD ’16. ACM, August 2016. doi:
413"
REFERENCES,0.7089430894308943,"10.1145/2939672.2939778. URL http://dx.doi.org/10.1145/2939672.2939778.
414"
REFERENCES,0.7105691056910569,"[18] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis
415"
REFERENCES,0.7121951219512195,"with latent diffusion models. In 2022 IEEE/CVF Conference on Computer Vision and Pattern
416"
REFERENCES,0.7138211382113822,"Recognition (CVPR), pages 10674–10685, Los Alamitos, CA, USA, jun 2022. IEEE Computer
417"
REFERENCES,0.7154471544715447,"Society. doi: 10.1109/CVPR52688.2022.01042. URL https://doi.ieeecomputersociety.
418"
REFERENCES,0.7170731707317073,"org/10.1109/CVPR52688.2022.01042.
419"
REFERENCES,0.71869918699187,"[19] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning
420"
REFERENCES,0.7203252032520325,"text-to-image diffusion models for subject-driven generation. In 2023 IEEE/CVF Conference
421"
REFERENCES,0.7219512195121951,"on Computer Vision and Pattern Recognition (CVPR), pages 22500–22510, Los Alamitos,
422"
REFERENCES,0.7235772357723578,"CA, USA, jun 2023. IEEE Computer Society. doi: 10.1109/CVPR52729.2023.02155. URL
423"
REFERENCES,0.7252032520325203,"https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.02155.
424"
REFERENCES,0.7268292682926829,"[20] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi
425"
REFERENCES,0.7284552845528456,"Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based
426"
REFERENCES,0.7300813008130081,"localization. In 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, October
427"
REFERENCES,0.7317073170731707,"2017. doi: 10.1109/iccv.2017.74. URL http://dx.doi.org/10.1109/ICCV.2017.74.
428"
REFERENCES,0.7333333333333333,"[21] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
429"
REFERENCES,0.734959349593496,"image recognition. In International Conference on Learning Representations, 2015.
430"
REFERENCES,0.7365853658536585,"[22] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
431"
REFERENCES,0.7382113821138211,"Visualising image classification models and saliency maps. In Workshop at International
432"
REFERENCES,0.7398373983739838,"Conference on Learning Representations, 2014.
433"
REFERENCES,0.7414634146341463,"[23] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Viégas, and Martin Wattenberg.
434"
REFERENCES,0.7430894308943089,"Smoothgrad: removing noise by adding noise. CoRR, abs/1706.03825, 2017. URL http:
435"
REFERENCES,0.7447154471544716,"//arxiv.org/abs/1706.03825.
436"
REFERENCES,0.7463414634146341,"[24] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving
437"
REFERENCES,0.7479674796747967,"for simplicity: The all convolutional net. CoRR, abs/1412.6806, 2014.
438"
REFERENCES,0.7495934959349594,"[25] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
439"
REFERENCES,0.751219512195122,"Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,
440"
REFERENCES,0.7528455284552845,"page 3319–3328. JMLR.org, 2017.
441"
REFERENCES,0.7544715447154472,"[26] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
442"
REFERENCES,0.7560975609756098,"Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
443"
REFERENCES,0.7577235772357723,"In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–9,
444"
REFERENCES,0.759349593495935,"2015. doi: 10.1109/CVPR.2015.7298594.
445"
REFERENCES,0.7609756097560976,"[27] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-
446"
REFERENCES,0.7626016260162601,"thinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer
447"
REFERENCES,0.7642276422764228,"Vision and Pattern Recognition (CVPR), pages 2818–2826, 2016. doi: 10.1109/CVPR.2016.308.
448"
REFERENCES,0.7658536585365854,"[28] Tolga Turay and Tanya Vladimirova. Toward performing image classification and object
449"
REFERENCES,0.767479674796748,"detection with convolutional neural networks in autonomous driving systems: A survey. IEEE
450"
REFERENCES,0.7691056910569106,"Access, 10:14076–14119, 2022. doi: 10.1109/ACCESS.2022.3147495.
451"
REFERENCES,0.7707317073170732,"[29] Warren von Eschenbach. Transparency and the black box problem: Why we do not trust ai.
452"
REFERENCES,0.7723577235772358,"Philosophy & Technology, 34, 12 2021. doi: 10.1007/s13347-021-00477-0.
453"
REFERENCES,0.7739837398373983,"[30] Chase Walker, Sumit Jha, Kenny Chen, and Rickard Ewetz. Integrated decision gradients:
454"
REFERENCES,0.775609756097561,"Compute your attributions where the model makes its decision. Proceedings of the AAAI
455"
REFERENCES,0.7772357723577236,"Conference on Artificial Intelligence, 38:5289–5297, 03 2024. doi: 10.1609/aaai.v38i6.28336.
456"
REFERENCES,0.7788617886178861,"[31] Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A. Ehinger, and Benjamin I. P. Rubinstein.
457"
REFERENCES,0.7804878048780488,"Invertible concept-based explanations for cnn models with non-negative concept activation
458"
REFERENCES,0.7821138211382114,"vectors. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):11682–11690,
459"
REFERENCES,0.7837398373983739,"May 2021. doi: 10.1609/aaai.v35i13.17389. URL https://ojs.aaai.org/index.php/
460"
REFERENCES,0.7853658536585366,"AAAI/article/view/17389.
461"
REFERENCES,0.7869918699186992,"A
Appendix Overview
462"
REFERENCES,0.7886178861788617,"In the appendix, we provide:
463"
REFERENCES,0.7902439024390244,"B. Saliency methods for 100% tags model
464"
REFERENCES,0.791869918699187,"C. Additional results of Local Explanations
465"
REFERENCES,0.7934959349593496,"D. Additional results of Global Explanations
466"
REFERENCES,0.7951219512195122,"E. Example images for generated concepts
467"
REFERENCES,0.7967479674796748,"B
Saliency methods for 100% tags model
468"
REFERENCES,0.7983739837398374,"We provide the results obtained by applying IG and Grad-CAM to the 100% tags model (see Figure 7).
469"
REFERENCES,0.8,"These methods align with Visual-TCAV in showing that this model does not pay attention to the “Z”,
470"
REFERENCES,0.8016260162601626,"but rather to the absence of the “T” and the “C” for predicting the “zebra” class.
471"
REFERENCES,0.8032520325203252,"(a) Integrated Gradients
(b) Grad-CAM"
REFERENCES,0.8048780487804879,"Figure 7: Integrated Gradients and Grad-CAM for the model with 100% tags, searching respectively
for the classes “zebra”, “taxi”, and “cucumber”. Both methods highlight the “T” for class “taxi” and
the “C” for class “cucumber”, but fail to recognize the “Z” for class “zebra”."
REFERENCES,0.8065040650406504,"C
Additional results of Local Explanations
472"
REFERENCES,0.808130081300813,"Continuing from the results presented in Section 4.1, we further provide additional local explanations
473"
REFERENCES,0.8097560975609757,"for more input images and concepts in Figure 8.
474"
REFERENCES,0.8113821138211382,"glass in GoogLeNet
steeple in ResNet50V2
spotted in ResNet50V2
honeycombed in IncV3
car in InceptionV3
fur in VGG16
sky in VGG16"
REFERENCES,0.8130081300813008,Figure 8: More examples of layer-wise local explanations for various concepts and networks.
REFERENCES,0.8146341463414634,"D
Additional results of Global Explanations
475"
REFERENCES,0.816260162601626,"Building upon the results outlined in Section 4.2, we provide additional global explanations for
476"
REFERENCES,0.8178861788617886,"various classes and concepts in Figure 9.
477"
REFERENCES,0.8195121951219512,"Figure 9: More examples of global explanations for various classes, concepts, and networks."
REFERENCES,0.8211382113821138,"E
Example images for generated concepts
478"
REFERENCES,0.8227642276422764,"Some of the concepts used in the paper were automatically generated using Stable Diffusion v1.5 [18]
479"
REFERENCES,0.824390243902439,"with default parameters. In particular, we generated the following concepts: “pews”, “fresco”,
480"
REFERENCES,0.8260162601626017,"“arches”, “sky”, “pipes”, and “brass”. We used just the concept name as a prompt and generated 200
481"
REFERENCES,0.8276422764227642,"images per concept. A subsequent manual revision was still necessary to eliminate errors and strange
482"
REFERENCES,0.8292682926829268,"artifacts. In Figure 10, we provide three example images for each generated concept.
483"
REFERENCES,0.8308943089430895,"(a) Fresco
(b) Arches"
REFERENCES,0.832520325203252,"(c) Pews
(d) Sky"
REFERENCES,0.8341463414634146,"(e) Brass
(f) Pipes"
REFERENCES,0.8357723577235773,Figure 10: We provide three example images for each concept generated with Stable Diffusion v1.5.
REFERENCES,0.8373983739837398,"NeurIPS Paper Checklist
484"
CLAIMS,0.8390243902439024,"1. Claims
485"
CLAIMS,0.8406504065040651,"Question: Do the main claims made in the abstract and introduction accurately reflect the
486"
CLAIMS,0.8422764227642277,"paper’s contributions and scope?
487"
CLAIMS,0.8439024390243902,"Answer: [Yes]
488"
CLAIMS,0.8455284552845529,"Justification: We claim that our method can provide visual explanations through saliency
489"
CLAIMS,0.8471544715447155,"maps based on user-defined concepts, estimate the attributions of these concepts for a
490"
CLAIMS,0.848780487804878,"selected class, and provide both local and global explanations. These claims are all validated
491"
CLAIMS,0.8504065040650407,"through the experimental results performed in the paper.
492"
LIMITATIONS,0.8520325203252033,"2. Limitations
493"
LIMITATIONS,0.8536585365853658,"Question: Does the paper discuss the limitations of the work performed by the authors?
494"
LIMITATIONS,0.8552845528455284,"Answer: [Yes]
495"
LIMITATIONS,0.8569105691056911,"Justification: Our work has some limitations, we discuss them in Section 5.1.
496"
THEORY ASSUMPTIONS AND PROOFS,0.8585365853658536,"3. Theory Assumptions and Proofs
497"
THEORY ASSUMPTIONS AND PROOFS,0.8601626016260162,"Question: For each theoretical result, does the paper provide the full set of assumptions and
498"
THEORY ASSUMPTIONS AND PROOFS,0.8617886178861789,"a complete (and correct) proof?
499"
THEORY ASSUMPTIONS AND PROOFS,0.8634146341463415,"Answer: [NA]
500"
THEORY ASSUMPTIONS AND PROOFS,0.865040650406504,"Justification: The paper does not include any new proof or theorem.
501"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8666666666666667,"4. Experimental Result Reproducibility
502"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8682926829268293,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
503"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8699186991869918,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
504"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8715447154471545,"of the paper (regardless of whether the code and data are provided or not)?
505"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8731707317073171,"Answer: [Yes]
506"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8747967479674796,"Justification: Every experimental result presented in the paper is fully reproducible using
507"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8764227642276423,"the provided code and data.
508"
OPEN ACCESS TO DATA AND CODE,0.8780487804878049,"5. Open access to data and code
509"
OPEN ACCESS TO DATA AND CODE,0.8796747967479674,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
510"
OPEN ACCESS TO DATA AND CODE,0.8813008130081301,"tions to faithfully reproduce the main experimental results, as described in supplemental
511"
OPEN ACCESS TO DATA AND CODE,0.8829268292682927,"material?
512"
OPEN ACCESS TO DATA AND CODE,0.8845528455284553,"Answer: [Yes]
513"
OPEN ACCESS TO DATA AND CODE,0.8861788617886179,"Justification: We provide the code, data, and instructions needed to reproduce every ex-
514"
OPEN ACCESS TO DATA AND CODE,0.8878048780487805,"periment both to reviewers and to the public through a GitHub repository (in case of
515"
OPEN ACCESS TO DATA AND CODE,0.8894308943089431,"publication).
516"
OPEN ACCESS TO DATA AND CODE,0.8910569105691057,"6. Experimental Setting/Details
517"
OPEN ACCESS TO DATA AND CODE,0.8926829268292683,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
518"
OPEN ACCESS TO DATA AND CODE,0.8943089430894309,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
519"
OPEN ACCESS TO DATA AND CODE,0.8959349593495934,"results?
520"
OPEN ACCESS TO DATA AND CODE,0.8975609756097561,"Answer: [Yes]
521"
OPEN ACCESS TO DATA AND CODE,0.8991869918699187,"Justification: The paper describes in detail all the necessary steps to reproduce and un-
522"
OPEN ACCESS TO DATA AND CODE,0.9008130081300812,"derstand the experiments. Furthermore, the code used is also available as supplementary
523"
OPEN ACCESS TO DATA AND CODE,0.9024390243902439,"material.
524"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9040650406504065,"7. Experiment Statistical Significance
525"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9056910569105691,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
526"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9073170731707317,"information about the statistical significance of the experiments?
527"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9089430894308943,"Answer: [Yes]
528"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9105691056910569,"Justification: In our bar plots we always report 2-sigma error bars.
529"
EXPERIMENTS COMPUTE RESOURCES,0.9121951219512195,"8. Experiments Compute Resources
530"
EXPERIMENTS COMPUTE RESOURCES,0.9138211382113821,"Question: For each experiment, does the paper provide sufficient information on the com-
531"
EXPERIMENTS COMPUTE RESOURCES,0.9154471544715447,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
532"
EXPERIMENTS COMPUTE RESOURCES,0.9170731707317074,"the experiments?
533"
EXPERIMENTS COMPUTE RESOURCES,0.9186991869918699,"Answer: [Yes]
534"
EXPERIMENTS COMPUTE RESOURCES,0.9203252032520325,"Justification: We describe in detail the characteristics of the machine used to run all the
535"
EXPERIMENTS COMPUTE RESOURCES,0.9219512195121952,"experiments and the execution time.
536"
CODE OF ETHICS,0.9235772357723577,"9. Code Of Ethics
537"
CODE OF ETHICS,0.9252032520325203,"Question: Does the research conducted in the paper conform, in every respect, with the
538"
CODE OF ETHICS,0.926829268292683,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
539"
CODE OF ETHICS,0.9284552845528455,"Answer: [Yes]
540"
CODE OF ETHICS,0.9300813008130081,"Justification: We have reviewed the NeurIPS Code of Ethics and our research conforms with
541"
CODE OF ETHICS,0.9317073170731708,"it.
542"
BROADER IMPACTS,0.9333333333333333,"10. Broader Impacts
543"
BROADER IMPACTS,0.9349593495934959,"Question: Does the paper discuss both potential positive societal impacts and negative
544"
BROADER IMPACTS,0.9365853658536586,"societal impacts of the work performed?
545"
BROADER IMPACTS,0.9382113821138212,"Answer: [Yes]
546"
BROADER IMPACTS,0.9398373983739837,"Justification: In the introduction, we briefly discuss the problem of transparency in AI
547"
BROADER IMPACTS,0.9414634146341463,"systems, particularly as Convolutional Neural Networks are being widely utilized in critical
548"
BROADER IMPACTS,0.943089430894309,"sectors such as healthcare and autonomous driving. Our work can have a positive societal
549"
BROADER IMPACTS,0.9447154471544715,"impact by facilitating a trustworthy adoption of these systems. We are not aware of any
550"
BROADER IMPACTS,0.9463414634146341,"negative impact our work could have.
551"
SAFEGUARDS,0.9479674796747968,"11. Safeguards
552"
SAFEGUARDS,0.9495934959349593,"Question: Does the paper describe safeguards that have been put in place for responsible
553"
SAFEGUARDS,0.9512195121951219,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
554"
SAFEGUARDS,0.9528455284552846,"image generators, or scraped datasets)?
555"
SAFEGUARDS,0.9544715447154472,"Answer: [NA]
556"
SAFEGUARDS,0.9560975609756097,"Justification: This paper does not release any data or models that pose such risks.
557"
LICENSES FOR EXISTING ASSETS,0.9577235772357724,"12. Licenses for existing assets
558"
LICENSES FOR EXISTING ASSETS,0.959349593495935,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
559"
LICENSES FOR EXISTING ASSETS,0.9609756097560975,"the paper, properly credited and are the license and terms of use explicitly mentioned and
560"
LICENSES FOR EXISTING ASSETS,0.9626016260162602,"properly respected?
561"
LICENSES FOR EXISTING ASSETS,0.9642276422764228,"Answer: [Yes]
562"
LICENSES FOR EXISTING ASSETS,0.9658536585365853,"Justification: All models and datasets used for the experiments are properly cited in the
563"
LICENSES FOR EXISTING ASSETS,0.967479674796748,"paper.
564"
NEW ASSETS,0.9691056910569106,"13. New Assets
565"
NEW ASSETS,0.9707317073170731,"Question: Are new assets introduced in the paper well documented and is the documentation
566"
NEW ASSETS,0.9723577235772358,"provided alongside the assets?
567"
NEW ASSETS,0.9739837398373984,"Answer: [NA]
568"
NEW ASSETS,0.975609756097561,"Justification: The paper does not introduce new assets.
569"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772357723577236,"14. Crowdsourcing and Research with Human Subjects
570"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9788617886178862,"Question: For crowdsourcing experiments and research with human subjects, does the paper
571"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804878048780488,"include the full text of instructions given to participants and screenshots, if applicable, as
572"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821138211382113,"well as details about compensation (if any)?
573"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983739837398374,"Answer: [NA]
574"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853658536585366,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
575"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869918699186991,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
576"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886178861788618,"Subjects
577"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902439024390244,"Question: Does the paper describe potential risks incurred by study participants, whether
578"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991869918699187,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
579"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934959349593496,"approvals (or an equivalent approval/review based on the requirements of your country or
580"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951219512195122,"institution) were obtained?
581"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967479674796748,"Answer: [NA]
582"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983739837398374,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
583"
