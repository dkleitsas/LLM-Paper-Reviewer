Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016666666666666668,"The implicit bias induced by the training of neural networks has become a topic of
rigorous study. In the limit of gradient ﬂow and gradient descent with appropriate
step size, it has been shown that when one trains a deep linear network with logis-
tic or exponential loss on linearly separable data, the weights converge to rank-1
matrices. In this paper, we extend this theoretical result to the last few linear layers
of the much wider class of nonlinear ReLU-activated feedforward networks con-
taining fully-connected layers and skip connections. Similar to the linear case, the
proof relies on speciﬁc local training invariances, sometimes referred to as align-
ment, which we show to hold for submatrices where neurons are stably-activated
in all training examples, and it reﬂects empirical results in the literature. We also
show this is not true in general for the full matrix of ReLU fully-connected lay-
ers. Our proof relies on a speciﬁc decomposition of the network into a multilinear
function and another ReLU network whose weights are constant under a certain
parameter directional convergence."
INTRODUCTION,0.0033333333333333335,"1
INTRODUCTION"
INTRODUCTION,0.005,"Recently, great progress has been made in understanding the trajectory of gradient ﬂow (GF) (Ji &
Telgarsky, 2019; 2020; Lyu & Li, 2020), gradient descent (GD) (Ji & Telgarsky, 2019; Arora et al.,
2018) and stochastic gradient descent (SGD) (Neyshabur et al., 2015; 2017) in the training of neural
networks. While good theory has been developed for deep linear networks (Zhou & Liang, 2018;
Arora et al., 2018), practical architectures such as ReLU fully-connected networks or ResNets are
highly non-linear. This causes the underlying optimization problem (usually empirical risk min-
imization) to be highly non-smooth (e.g. for ReLUs) and non-convex, necessitating special tools
such as the Clarke subdifferential (Clarke, 1983)."
INTRODUCTION,0.006666666666666667,"One of the many exciting results of this body of literature is that gradient-based algorithms exhibit
some form of implicit regularization: the optimization algorithm prefers some stationary points to
others. In particular, a wide range of implicit biases has been shown in practice (Huh et al., 2021)
and proven for deep linear networks (Arora et al., 2018; Ji & Telgarsky, 2019), convolutional neural
networks (Gunasekar et al., 2018) and homogeneous networks (Ji & Telgarsky, 2020). One well
known result for linearly separable data is that in various practical settings, linear networks converge
to the solution of the hard SVM problem, i.e., a max-margin classiﬁer (Ji & Telgarsky, 2019), while
a relaxed version is true for CNNs (Gunasekar et al., 2018). This holds even when the margin does
not explicitly appear in the optimization objective - hence the name implicit regularization."
INTRODUCTION,0.008333333333333333,"Another strong form of implicit regularization relates to the structure of weight matrices of fully
connected networks. In particular, Ji & Telgarsky (2019) prove that for deep linear networks for
binary classiﬁcation, weight matrices tend to rank-1 matrices in Frobenius norm as a result of GF/GD
training, and that adjacent layers’ singular vectors align. The max-margin phenomenon follows as
a result. In practice, Huh et al. (2021) empirically document the low rank bias across different
non-linear architectures. However, their results include ReLU fully-connected networks, CNNs and
ResNet, which are not all covered by the existing theory. Beyond linear fully connected networks,
Du et al. (2018) show vertex-wise invariances for fully-connected ReLU networks and invariances
in Frobenius norm differences between layers for CNNs. Yet, despite the evidence in (Huh et al.,
2021), it has been an open theoretical question how more detailed structural relations between layers,"
INTRODUCTION,0.01,Published as a conference paper at ICLR 2022
INTRODUCTION,0.011666666666666667,"which, e.g., imply the low-rank result, generalize to other, structured or local nonlinear and possibly
non-homogeneous architectures, and how to even characterize these."
INTRODUCTION,0.013333333333333334,"Hence, in this work, we take steps to addressing a wider set of architectures and invariances. First,
we show a class of vertex and edge-wise quantities that remain invariant during gradient ﬂow train-
ing. Applying these invariances to architectures containing fully connected, convolutional and resid-
ual blocks, arranged appropriately, we obtain invariances of the singular values in adjacent (weight)
matrices or submatrices. Second, we argue that a matrix-wise invariance is not always true for gen-
eral ReLU fully-connected layers. Third, we obtain low-rank results for arbitrary non-homogeneous
networks whose last few layers contain linear fully-connected and linear ResNet blocks. To the
best of our knowledge, this is the ﬁrst time a low-rank phenomenon is proven rigorously for these
architectures."
INTRODUCTION,0.015,"Our theoretical results offer explanations for empirical observations on more general architectures,
and apply to the experiments in (Huh et al., 2021) for ResNet and CNNs. They also include the
squared loss used there, in addition to the exponential or logistic loss used in most theoretical
low-rank results. Moreover, our Theorem 2 gives an explanation for the “reduced alignment” phe-
nomenon observed by Ji & Telgarsky (2019), where in experiments on AlexNet over CIFAR-10
the ratio ∥W∥2/∥W∥F converges to a value strictly less than 1 for some fully-connected layer W
towards the end of the network."
INTRODUCTION,0.016666666666666666,"One challenge in the analysis is the non-smoothness of the networks and ensuring an “operational”
chain rule. To cope with this setting, we use a speciﬁc decomposition of an arbitrary ReLU archi-
tecture into a multilinear function and a ReLU network with +1/-1 weights. This reduction holds in
the stable sign regime, a certain convergence setting of the parameters. This regime is different from
stable activations, and is implied, e.g., by directional convergence of the parameters to a vector with
non-zero entries (Lemma 1). This construction may be of independent interest."
INTRODUCTION,0.018333333333333333,"In short, we make the following contributions to analyzing implicit biases of general architectures:"
INTRODUCTION,0.02,"• We show vertex and edge-wise weight invariances during training with gradient ﬂow. Via these
invariances, we prove that for architectures containing fully-connected layers, convolutional layers
and residual blocks, when appropriately organized into matrices, adjacent matrices or submatrices
of neurons with stable activation pattern have bounded singular value (Theorem 1)."
INTRODUCTION,0.021666666666666667,"• In the stable sign regime, we show a low-rank bias for arbitrary nonhomogeneous feedforward
networks whose last few layers are a composition of linear fully-connected and linear ResNet
variant blocks (Theorem 2). In particular, if the Frobenius norms of these layers diverge, then the
ratio between their operator norm and Frobenius norm is bounded non-trivially by an expression
fully speciﬁed by the architecture. To the best of our knowledge, this is the ﬁrst time this type of
bias is shown for nonlinear, nonhomogeneous networks."
INTRODUCTION,0.023333333333333334,"• We prove our results via a decomposition that reduces arbitrarily structured feedforward networks
with positively-homogeneous activation (e.g., ReLU) to a multilinear structure (Lemma 1)."
RELATED WORKS,0.025,"1.1
RELATED WORKS"
RELATED WORKS,0.02666666666666667,"Decomposition of fully-connected neural networks into a multilinear part and a 0-1 part has been
used by Choromanska et al. (2015); Kawaguchi (2016), but their formulation does not apply to
trajectory studies. In Section 3, we give a detailed comparison between their approach and ours.
Our decomposition makes use of the construction of a tree network that Khim & Loh (2019) use
to analyze generalization of fully-connected networks. We describe their approach in Section 2
and show how we extend their construction to arbitrary feedforward networks. This construction
makes up the ﬁrst part of the proof of our decomposition lemma. The paper also makes use of path
enumeration of neural nets, which overlaps with the path-norm literature of Neyshabur et al. (2015;
2017). The distinction is that we are studying classical gradient ﬂow, as opposed to SGD (Neyshabur
et al., 2015) or its variants (Neyshabur et al., 2017)."
RELATED WORKS,0.028333333333333332,"For linear networks, low-rank bias is proven for separable data and exponential-tailed loss in Ji &
Telgarsky (2019). Du et al. (2018) give certain vertex-wise invariances for fully-connected ReLU
networks and Frobenius norm difference invariances for CNNs. Compared to their results, ours are
slightly stronger since we prove that our invariances hold for almost every time t on the gradient"
RELATED WORKS,0.03,Published as a conference paper at ICLR 2022
RELATED WORKS,0.03166666666666667,"ﬂow trajectory, thus allowing for the use of a Fundamental Theorem of Calculus and downstream
analysis. Moreover, the set of invariances we show is strictly larger. Radhakrishnan et al. (2020)
show negative results when generalizing the low-rank bias from (Ji & Telgarsky, 2019) to vector-
valued neural networks. In our work, we only consider scalar-valued neural networks performing
binary classiﬁcation. For linearly inseparable but rank-1 or whitened data, a recent line of work
from Ergen & Pilanci (2021) gives explicit close form optimal solution, which is both low rank and
aligned, for the regularized objective. This was done for both the linear and ReLU neural networks.
In our work, we focus on the properties of the network along the gradient ﬂow trajectory."
PRELIMINARIES AND NOTATION,0.03333333333333333,"2
PRELIMINARIES AND NOTATION"
PRELIMINARIES AND NOTATION,0.035,"For an integer k ∈N, we write the set [k] := {1, 2, . . . , k}. For vectors, we extend the sign function
sgn : R →{−1, 1} coordinate-wise as sgn : (xi)i∈[d] 7→(sgn(xi))i∈[d]. For some (usually the
canonical) basis (ei)i∈[n] in some vector space Rn, for all x ∈Rn we use the notation [x]i = ⟨x, ei⟩
to denote the i-th coordinate of x."
PRELIMINARIES AND NOTATION,0.03666666666666667,"Clarke subdifferential, deﬁnability and nonsmooth analysis. The analysis of non-smooth func-
tions is central to our results. For a locally Lipschitz function f : D →R with open domain D,
there exists a set Dc ⊆D of full Lebesgue measure on which the derivative ∇f exists everywhere
by Rademacher’s theorem. As a result, calculus can usually be done over the Clarke subdifferential
∂f(x) := CONV

limi→∞∇f(xi) | xi ∈Dc, xi →x
	
where CONV denotes the convex hull."
PRELIMINARIES AND NOTATION,0.03833333333333333,"The Clarke subdifferential generalizes both the smooth derivative when f ∈C1 (continuously dif-
ferentiable) and the convex subdifferential when f is convex. However, it only admits a chain rule
with an inclusion and not equality, which is though necessary for backpropagation in deep learn-
ing. We do not delve in too much depth into Clarke subdifferentials in this paper, but use it when
we extend previous results that also use this framework. We refer to e.g. (Davis et al., 2020; Ji &
Telgarsky, 2020; Bolte & Pauwels, 2020) for more details."
PRELIMINARIES AND NOTATION,0.04,"Neural networks. Consider a neural network ν : Rd →R. The computation graph of ν is a
weighted directed graph G = (V, E, w) with weight function w : E →R. For each neuron v ∈V ,
let INv := {u ∈V : uv ∈E} and OUTv := {w ∈V : vw ∈E} be the input and output neurons of
v. Let {i1, i2, . . . , id} =: I ⊂V and O := {o} ⊂V be the set of input and output neurons deﬁned
as IN(i) = ∅= OUT(o), ∀i ∈I. Each neuron v ∈V \I is equipped with a positively 1-homogeneous
activation function σv (such as the ReLU x 7→max(x, 0), leaky ReLU x 7→max(x, αx) for some
small positive α , or the linear activation)."
PRELIMINARIES AND NOTATION,0.041666666666666664,"To avoid unnecessary brackets, we write we := w(e) for some e ∈E. We will also write w ∈RE,
where E is the set of learnable weights, as the vector of learnable parameters. Let P be a path in G,
i.e., a set of edges in E that forms a path. We write v ∈P for some v ∈V if there exists u ∈V
such that uv ∈P or vu ∈P. Let ρ be the number of distinct paths from any i ∈I to o ∈O. Let
P :=

p1, . . . , pρ
	
be the enumeration of these paths. For a path p ∈P and an input x to the neural
network, denote by xp the coordinate of x used in p."
PRELIMINARIES AND NOTATION,0.043333333333333335,"Given a binary classiﬁcation dataset

(xi, yi)"
PRELIMINARIES AND NOTATION,0.045,"i∈[n] with xi ∈Rd, ∥xi∥≤1 and yi ∈{−1, 1}, we"
PRELIMINARIES AND NOTATION,0.04666666666666667,minimize the empirical risk R(w) = 1
PRELIMINARIES AND NOTATION,0.04833333333333333,"n
Pn
i=1 ℓ(yiν(xi)) = 1"
PRELIMINARIES AND NOTATION,0.05,"n
Pn
i=1 ℓ(ν(yixi)) with loss ℓ: R →
R, using gradient ﬂow dw(t)"
PRELIMINARIES AND NOTATION,0.051666666666666666,"dt
∈−∂R(w(t))."
PRELIMINARIES AND NOTATION,0.05333333333333334,"As we detail the architectures used in this paper, we recall that the activation of each neuron is still
positively-homogeneous. The networks considered here are assumed to be bias-free."
PRELIMINARIES AND NOTATION,0.055,"Deﬁnition 1 (Feedforward networks). A neural net ν with graph G is a feedforward network if G
is a directed acyclic graph (DAG)."
PRELIMINARIES AND NOTATION,0.056666666666666664,"Deﬁnition 2 (Fully-connected networks). A feedforward network ν with graph G is a fully-
connected network if there exists a partition of V into V = (I ≡V1) ⊔V2 ⊔. . . ⊔(VL+1 ≡O) such
that for all u, v ∈V, uv ∈E iff there exists i ∈[L] such that u ∈Vi and v ∈Vi+1."
PRELIMINARIES AND NOTATION,0.058333333333333334,"Deﬁnition 3 (Tree networks). A feedforward network ν with graph G is a tree network if the under-
lying undirected graph G is a tree (undirected acyclic graph)."
PRELIMINARIES AND NOTATION,0.06,Published as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATION,0.06166666666666667,"Examples of feedforwards networks include ResNet (He et al., 2016), DenseNet (Huang et al.,
2017), CNNs (Fukushima, 1980; LeCun et al., 2015) and other fully-connected ReLU architectures."
PRELIMINARIES AND NOTATION,0.06333333333333334,"For a fully-connected network ν with layer partition V =: V1 ⊔. . . ⊔VL+1 where L is the number
of (hidden) layers, let ni := |Vi| be the number of neurons in the i-th layer and enumerate Vi =
{vi,j}j∈[ni]. Weights in this architecture can be organized into matrices W [1], W [2], . . . , W [L] where
Rni+1×ni ∋W [i] = ((wvi,jvi+1,k))j∈[ni],k∈[ni+1], for all i ∈[L]."
PRELIMINARIES AND NOTATION,0.065,"Tree
networks.
Most
practical
architectures
are
not
tree
networks,
but
trees
have
been used to prove generalization bounds for adversarial risk.
In particular, for fully-
connected
neural
networks
f
whose
activations
are
monotonically
increasing
and
1-
Lipschitz, Khim & Loh (2019) deﬁne the tree transform as the tree network Tf(x; w)
=
PnL
pL=1 W [L]
1,pLσ

. . . Pn2
p2=1 W [2]
p3,p2σ

wp2..pL + Pn1
p1=1 W [1]
p2,p1xp1

for
vectors
w
with
QL
j=2 nj entries, indexed by an L-tuple (p2, . . . , pL). We extend this idea in the next section."
PRELIMINARIES AND NOTATION,0.06666666666666667,"3
STRUCTURAL LEMMA: DECOMPOSITION OF DEEP NETWORKS"
PRELIMINARIES AND NOTATION,0.06833333333333333,"We begin with a decomposition of a neural network into a multilinear and a non-weighted nonlinear
part, which will greatly facilitate the chain rule that we need to apply in the analysis. Before stating
the decomposition, we need the following deﬁnition of a path enumeration function, which computes
the product of all weights and inputs on each path of a neural network.
Deﬁnition 4 (Path enumeration function). Let ν be a feedforward neural network with graph G
and paths P =

p1, . . . , pρ
	
. The path enumeration function h is deﬁned for this network as"
PRELIMINARIES AND NOTATION,0.07,"h : (x1, x2, . . . , xd) 7→

xp
Q"
PRELIMINARIES AND NOTATION,0.07166666666666667,"e∈p we
"
PRELIMINARIES AND NOTATION,0.07333333333333333,p∈P where xp := xk such that ik ∈p.
PRELIMINARIES AND NOTATION,0.075,"We ﬁrst state the main result of this section, proven in Appendix B.
Lemma 1 (Decomposition). Let ν : Rd →R be a feedforward network with computation graph G,
and ρ the number of distinct maximal paths in G. Then there exists a tree network µ : Rρ →R such
that ν = µ ◦h where h : Rd →Rρ is the path enumeration function of G. Furthermore, all weights
in µ are either −1 or +1 and fully determined by the signs of the weights in ν."
PRELIMINARIES AND NOTATION,0.07666666666666666,"Path activation of ReLU networks in the literature. The viewpoint that for every feedforward
network ν there exists a tree network µ such that ν = µ ◦h is not new and our emphasis here is on
the fact that the description of µ : Rρ →R is fully determined by the signs of the weights. Indeed,
in analyses of the loss landscape (Choromanska et al., 2015; Kawaguchi, 2016), ReLU networks are
described as a sum over paths:"
PRELIMINARIES AND NOTATION,0.07833333333333334,"ν(x; w) =
X"
PRELIMINARIES AND NOTATION,0.08,"p∈P
Zp(x; w)
Y"
PRELIMINARIES AND NOTATION,0.08166666666666667,"e∈p
we =

(Zp(x; w))p∈P, h(x; w)"
PRELIMINARIES AND NOTATION,0.08333333333333333,"Rρ ,
(1)"
PRELIMINARIES AND NOTATION,0.085,"where Zp(x; w) = 1 iff all ReLUs on path p are active (have nonnegative preactivation) and 0
otherwise. One can then take µ as a tree network with no hidden layer, ρ input neurons all connected
to a single output neuron. However, this formulation complicates analyses of gradient trajectories,
because of the explicit dependence of Zp on numerical values of w. In our lemma, µ is a tree network
whose description depends only on the signs of the weights. If the weight signs (not necessarily the
ReLU activation pattern!) are constant, µ is ﬁxed, allowing for a chain rule to differentiate through
it. That weight signs are constant is realistic, in the sense that it is implied by directional parameter
convergence (Section 4.1). To see this, compare the partial derivative with respect to some we (when
it exists) between the two approaches, in the limit where weight signs are constant:"
PRELIMINARIES AND NOTATION,0.08666666666666667,"(using Lemma 1)
∂ν/∂we =
X"
PRELIMINARIES AND NOTATION,0.08833333333333333,"p∈P|e∈p
[∇wµ(x; w)]p · xp
Y"
PRELIMINARIES AND NOTATION,0.09,"f∈p,f̸=e
wf,
(2)"
PRELIMINARIES AND NOTATION,0.09166666666666666,"(using Zp in Eqn. 1)
∂ν/∂we =
X"
PRELIMINARIES AND NOTATION,0.09333333333333334,p∈P|e∈p
PRELIMINARIES AND NOTATION,0.095," 
we∂Zp(x; w)/∂we + Zp(x; w)

Y"
PRELIMINARIES AND NOTATION,0.09666666666666666,"f∈p,f̸=e
wf.
(3)"
PRELIMINARIES AND NOTATION,0.09833333333333333,"In particular, the dependence of Equation 2 on we is extremely simple. The utility of this fact will
be made precise in the next section when we study invariances."
PRELIMINARIES AND NOTATION,0.1,Published as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATION,0.10166666666666667,"Proof sketch. The proof contains two main steps. First, we “unroll” the feedforward network into
a tree network that computes the same function by adding extra vertices, edges and enable weight
sharing. This step is part of the tree transform in Khim & Loh (2019) if the neural network is a
fully-connected network; we generalize it to work with arbitrary feedforward networks. Second,
we “pull back” the weights towards the input nodes using positive homogeneity of the activations:
a · σ(x) = sgn(a) · σ(x|a|). This operation is ﬁrst done on vertices closest to the output vertex
(in number of edges on the unique path between any two vertices in a tree) and continues until all
vertices have been processed. Finally, all the residual signs can be subsumed into µ by subdividing
edges incident to input neurons. We give a quick illustration of the two steps described above for a
fully-connected ReLU-activated network with 1 hidden layer in Appendix A."
PRELIMINARIES AND NOTATION,0.10333333333333333,"4
MAIN THEOREM: TRAINING INVARIANCES"
PRELIMINARIES AND NOTATION,0.105,"In this section, we put the previous decomposition lemma to use in proving an implicit regularization
property of gradient ﬂow when training deep neural networks."
PRELIMINARIES AND NOTATION,0.10666666666666667,"4.1
STABLE SIGN REGIME: A CONSEQUENCE OF DIRECTIONAL CONVERGENCE"
PRELIMINARIES AND NOTATION,0.10833333333333334,"Recall the gradient ﬂow curve {w(t)}t∈[0,∞) deﬁned by the differential inclusion
dw(t)"
PRELIMINARIES AND NOTATION,0.11,"dt
∈
−∂R(w(t)). We ﬁrst state the main assumption in this section.
Assumption 1 (Stable sign regime). For some t0 < tN ∈[0, ∞], we assume that for all t ∈
[t0, tN), sgn(w(t)) = sgn(w(t0)). If this holds, we say that gradient ﬂow is in a stable sign regime.
Without loss of generality, when using this assumption, we identify t0 with 0 and write ”for some
t ≥0” to mean ”for some t ∈[t0, tN)”."
PRELIMINARIES AND NOTATION,0.11166666666666666,"In fact, the following assumption - the existence and ﬁniteness part of which has been proven in (Ji
& Telgarsky, 2020) for homogeneous networks, is sufﬁcient.
Assumption 2 (Directional convergence to non-vanishing limit in each entry). We assume that
w(t)
∥w(t)∥2
t→∞
−−−→w exists, is ﬁnite in each entry and furthermore, for all e ∈E, we ̸= 0."
PRELIMINARIES AND NOTATION,0.11333333333333333,"Motivation and justiﬁcation.
It is straightforward to see that Assumption 1 follows from As-
sumption 2 but we provide a proof in the Appendix (Claim 1). Directional convergence was proven
by Ji & Telgarsky (2020) for the exponential/logistic loss and the class of homogeneous networks,
under additional mild assumptions. This fact justiﬁes the ﬁrst part of Assumption 2 for these archi-
tectures. The second part of Assumption 2 is pathological for our case, in the sense that directional
convergence alone does not imply stable signs (for example, a weight that converges to 0 can change
sign an inﬁnite number of times)."
PRELIMINARIES AND NOTATION,0.115,"Pointwise convergence is too strong in general.
Note also that assuming pointwise convergence
of the weights (i.e. limt→∞w(t) exists and is ﬁnite) is a much stronger statement, which is not true
for the case of exponential/logistic loss and homogeneous networks (since ∥w(t)∥2 diverges, see for
example Lyu & Li (2020), Ji & Telgarsky (2020), Ji & Telgarsky (2019)). Even when pointwise
convergence holds, it would immediately reduce statements on asymptotic properties of gradient
ﬂow on ReLU activated architectures to that of linearly activated architectures. One may want
to assume that gradient ﬂow starts in the ﬁnal afﬁne piece prior to its pointwise convergence and
thus activation patterns are ﬁxed throughout training and the behavior is (multi)linear. In contrast,
directional convergence of the weights does not imply such a reduction from the ReLU-activation
to the linear case. Similarly, with stable signs, the parts of the input where the network is linear are
not convex, as opposed to the linearized case (Hanin & Rolnick, 2019) (see also Claim 2)."
PRELIMINARIES AND NOTATION,0.11666666666666667,"Stable sign implication.
The motivation for Assumption 1 is that weights in the tree network µ
in Lemma 1 are fully determined by the signs of the weights in the original feedforward network ν.
Thus, under Assumption 1, one can completely ﬁx the weights of µ - it has no learnable parameters.
Since we have the decomposition ν = µ ◦h where h is the path enumeration function, dynamics of
µ are fully determined by dynamics of h in the stable sign regime. To complete the picture, observe
that h is highly multilinear in structure: the degree of a particular edge weight we in each entry of h
is at most 1 by deﬁnition of a path; and if ν is fully-connected, then h is a Rn1×n2×...×nL tensor."
PRELIMINARIES AND NOTATION,0.11833333333333333,Published as a conference paper at ICLR 2022
TRAINING INVARIANCES,0.12,"4.2
TRAINING INVARIANCES"
TRAINING INVARIANCES,0.12166666666666667,"First, we state an assumption on the loss function that holds for most losses used in practice, such as
the logistic, exponential or squared loss.
Assumption 3 (Differentiable loss). The loss function ℓ: R →R is differentiable everywhere.
Lemma 2 (Vertex-wise invariance). Under Assumptions 1, and 3, for all v ∈V \{I ∪O} such that
all edges incident to v have learnable weights, for a.e. time t ≥0,
X"
TRAINING INVARIANCES,0.12333333333333334,"u∈INv
w2
uv(t) −
X"
TRAINING INVARIANCES,0.125,"b∈OUTv
w2
vb(t) =
X"
TRAINING INVARIANCES,0.12666666666666668,"u∈INv
w2
uv(0) −
X"
TRAINING INVARIANCES,0.12833333333333333,"b∈OUTv
w2
vb(0).
(4)"
TRAINING INVARIANCES,0.13,"If we also have INu = INv = IN and OUTu = OUTv = OUT and u and v have the same activation
pattern (preactivation has the same sign) for each training example and for a.e time t ≥0, then for
a.e. time t ≥0,
X"
TRAINING INVARIANCES,0.13166666666666665,"a∈IN
wau(t)wav(t) −
X"
TRAINING INVARIANCES,0.13333333333333333,"b∈OUT
wub(t)wvb(t) =
X"
TRAINING INVARIANCES,0.135,"a∈IN
wau(0)wav(0) −
X"
TRAINING INVARIANCES,0.13666666666666666,"b∈OUT
wub(0)wvb(0).
(5)"
TRAINING INVARIANCES,0.13833333333333334,"Comparison to Du et al. (2018)
A closely related form of Equation 4 in Lemma 2 has appeared
in Du et al. (2018) (Theorem 2.1) for fully-connected ReLU/leaky-ReLU networks. In particular,
the authors showed that the difference between incoming and outgoing weights does not change. In-
voking the Fundamental Theorem of Calculus (FTC) over this statement will return ours. However,
their proof may not hold on a nonnegligible set of time t due to the use of the operational chain rule
that holds only for almost all we. Thus the FTC can fail. The stronger form we showed here is useful
in proving downstream algorithmic consequences, such as the low rank phenomenon. Furthermore,
our result also holds for arbitrary feedforward architectures and not just the fully-connected case."
TRAINING INVARIANCES,0.14,"Before we put Lemma 2 to use, we list deﬁnitions of some ResNet variants.
Deﬁnition 5. Denote ResNetIdentity, ResNetDiagonal and ResNetFree to be the version of ResNet
described in He et al. (2016) where the residual block is deﬁned respectively as"
TRAINING INVARIANCES,0.14166666666666666,"1. r(x; U, Y ) = σ(Uσ(Y x) + Ix) where x ∈Ra, Y, U ⊤∈Rb×a, and I is the identity,"
TRAINING INVARIANCES,0.14333333333333334,"2. r(x; U, Y, D) = σ(Uσ(Y x) + Dx) where x ∈Ra, Y, U ⊤∈Rb×a, and D is diagonal,"
TRAINING INVARIANCES,0.145,"3. r(x; U, Y, Z) = σ(Uσ(Y x) + Zx) where x ∈Ra, Y ∈Rb×a, U ∈Rc×b and Z ∈Rc×a."
TRAINING INVARIANCES,0.14666666666666667,"ResNetIdentity is the most common version of ResNet in practice. ResNetIdentity is a special
case of ResNetDiagonal, which is a special case of ResNetFree. Yet, theorems for ResNetFree do
not generalize trivially to the remaining variants, due to the restriction of Lemma 2 and Lemma 3 to
vertices adjacent to all learnable weights and layers containing all learnable weights. For readability,
we introduce the following notation:
Deﬁnition 6 (Submatrices of active neurons). Fix some time t, let W ∈Ra×b be a weight matrix
from some set of a neurons to another set of b neurons. Let Iactive ⊆[b] be the set of b neurons that
are active (linear or ReLU with nonnegative preactivation). We write [W ⊤W]active ∈R|Iactive|×|Iactive|
for the submatrix of W ⊤W with rows and columns from Iactive. Similarly, if W ′ ∈b × c is another
weight matrix from the same set of b neurons to another set of c neurons then [W ′W ′⊤]active is
deﬁned as the submatrix with rows and columns from Iactive."
TRAINING INVARIANCES,0.14833333333333334,"When applying Lemma 2 to speciﬁc architectures, we obtain the following:
Theorem 1 (Matrix-wise invariances). Recall that a convolutional layer with number of input ker-
nels a, kernel size b and number of output kernels c and is a tensor in Ra×b×c. Under Assumptions
1 and 3, we have the following matrix-wise invariance for a.e. time t ≥0: d
dt"
TRAINING INVARIANCES,0.15,"h
W2(t)⊤W2(t)
i"
TRAINING INVARIANCES,0.15166666666666667,"active −
h
W1(t)W1(t)⊤i"
TRAINING INVARIANCES,0.15333333333333332,active
TRAINING INVARIANCES,0.155,"
= 0, for:
(6)"
TRAINING INVARIANCES,0.15666666666666668,"1. (Fully-connected layers) W1 ∈Rb×a and W2 ∈Rc×b consecutive fully-connected layers,"
TRAINING INVARIANCES,0.15833333333333333,"2. (Convolutional layers) W1 is convolutional, viewed as a ﬂattening to a matrix Rc×(a×b), and
W2 adjacent convolutional, viewed as a ﬂattening to a matrix R(d×e)×c,"
TRAINING INVARIANCES,0.16,Published as a conference paper at ICLR 2022
TRAINING INVARIANCES,0.16166666666666665,"3. (Within residual block of ResNet) W1 = Y and W2 = U where r(x; U, Y, Z) is a residual block
of ResNetIdentity, ResNetDiagonal or ResNetFree,"
TRAINING INVARIANCES,0.16333333333333333,"4. (Between residual blocks of ResNet) W1 =
U1
Z1

, W2 =

Y2
Z2"
TRAINING INVARIANCES,0.165,"
where r(x; Uj, Yj, Zj),"
TRAINING INVARIANCES,0.16666666666666666,"j ∈{1, 2} are consecutive ResNetFree blocks,"
TRAINING INVARIANCES,0.16833333333333333,"5. (Convolutional-fully-connected layers) W1 convolutional, viewed as a ﬂattening to a matrix
Rc×(a×b) and W2 adjacent fully-connected layer, viewed as an rearrangement to Rd×c,"
TRAINING INVARIANCES,0.17,"6. (Convolutional-ResNetFree block) W1 convolutional, viewed as a ﬂattening to a matrix
Rc×(a×b) and W2 is a rearrangement of
U
Z
into an element of Rd×c, where r(x; U, Y, Z)
is an adjacent ResNetFree block,"
TRAINING INVARIANCES,0.17166666666666666,"7. (ResNetFree block-fully-connected layers and vice versa) W1 =
U
Z
∈Rb×a and W2 ∈"
TRAINING INVARIANCES,0.17333333333333334,"Rc×b adjacent fully-connected or W1 ∈Rb×a fully-connected and W2 =

Y
Z"
TRAINING INVARIANCES,0.175,"
∈Rc×b adjacent"
TRAINING INVARIANCES,0.17666666666666667,"ResNet block where r(x; U, Y, Z) is the ResNetFree block."
TRAINING INVARIANCES,0.17833333333333334,"We emphasize that the above theorem only makes local requirements on the neural network, to
have local parts that are either fully-connected, convolutional or a residual block. The only global
architecture requirement is feedforward-ness. The ﬁrst point of Theorem 1 admits an extremely
simple proof for the linear fully-connected network case in Arora et al. (2018) (Theorem 1)."
TRAINING INVARIANCES,0.18,"Signiﬁcance of Theorem 1.
If we have a set of neurons that is active throughout training (which
is vacuously true for linear layers), we can invoke an FTC and get W2(t)⊤W2(t)−W1(t)W1(t)⊤=
W2(0)⊤W2(0)−W1(0)W1(0)⊤for the submatrix restricted to these neurons. Assume for simplicity
that the right hand side is 0, then the singular values of W1 and W2 are the same for each of the
cases listed in Theorem 1. If we can form a chain of matrices whose singular values are the same
by iteratively invoking Theorem 1, then all matrices considered have the same singular values as the
ﬁnal fully-connected layer that connects to the output. Recall that our networks are scalar-valued, so
the ﬁnal layer is a row vector, which is rank 1 and thus all layers considered in the chain have rank
1, which is useful in the next section."
TRAINING INVARIANCES,0.18166666666666667,"Proof sketch of Theorem 1.
Given Lemma 2, we demonstrate the proof for the ﬁrst point. The
remaining points admit the exact same proof technique but on different matrices, which require some
bookkeeping. Let W1 ∈Rb×a and W2 ∈Rc×b be two consecutive fully-connected layers for some
a, b, c ∈N number of vertices in these layers. Applying Equation 4 of Lemma 2 to each of the
b shared neurons between these two layers, one obtains the diagonal entries of Equation 6 of the
Theorem. Now, apply Equation 5 to each pair among the b shared neurons between these two layers
to get the off-diagonal entries of Equation 6."
TRAINING INVARIANCES,0.18333333333333332,"Next, we deﬁne layers for architectures where weights are not necessarily organized into matrices,
e.g., ResNet or DenseNet.
Deﬁnition 7 (Layer). Let F ⊂E be such that 1) for all e ̸= f ∈F, there is no path that contains
both e and f; and 2) the graph (V, E\F, w) is disconnected. Then F is called a layer of G."
TRAINING INVARIANCES,0.185,"For this deﬁnition, we have the following invariance:
Lemma 3 (Edge-wise invariance). Under Assumptions 1 and 3, for all layers F and F ′ that contain
all learnable weights, it holds that for a.e. time t ≥0,
X"
TRAINING INVARIANCES,0.18666666666666668,"e∈F
w2
e(t) −
X"
TRAINING INVARIANCES,0.18833333333333332,"f∈F ′
w2
f(t) =
X"
TRAINING INVARIANCES,0.19,"e∈F
w2
e(0) −
X"
TRAINING INVARIANCES,0.19166666666666668,"f∈F ′
w2
f(0).
(7)"
TRAINING INVARIANCES,0.19333333333333333,"Signiﬁcance of Lemma 3.
A ﬂattening of a convolutional parameter tensor and a stacking of
matrices in a ResNetDiagonal and ResNetFree block forms a layer. This lemma implies that the
squared Frobenius norm of these matrices in the same network differs by a value that is ﬁxed at
initialization. The lemma also gives a direct implicit regularization for networks with biases, by
treating neurons with bias as having an extra in-edge whose weight is the bias, from an extra in-
vertex which is an input vertex with ﬁxed input value 1."
TRAINING INVARIANCES,0.195,Published as a conference paper at ICLR 2022
TRAINING INVARIANCES,0.19666666666666666,"4.3
PROOF SKETCH OF LEMMA 2 AND LEMMA 3"
TRAINING INVARIANCES,0.19833333333333333,"The proofs of Lemma 2, Lemma 3 and Theorem 1 share the technique of double counting paths,
which we explain next. For simplicity, we assume here that we are working with a network that is
differentiable everywhere in some domain that we are considering – we give a full general proof in
the Appendix. The main proof idea was used in (Arora et al., 2018) and involves simply writing
down the partial derivative of the risk. We have, for some particular weight we, e ∈E, via the
smooth chain rule ∂R(w)"
TRAINING INVARIANCES,0.2,"∂we
= 1 n n
X"
TRAINING INVARIANCES,0.20166666666666666,"i=1
l′(yiν(xi; w)) · yi · ∂ν(w)"
TRAINING INVARIANCES,0.20333333333333334,"∂we
(8) = 1 n n
X"
TRAINING INVARIANCES,0.205,"i=1
l′(yiν(xi; w)) · yi ·
X"
TRAINING INVARIANCES,0.20666666666666667,"p∈P,p∋e"
TRAINING INVARIANCES,0.20833333333333334,"
µ′(xi; w)
"
TRAINING INVARIANCES,0.21,"p · (xi)p
Y"
TRAINING INVARIANCES,0.21166666666666667,"f∈p,f̸=e
wf,
(9)"
TRAINING INVARIANCES,0.21333333333333335,"where in the second line, we invoke the decomposition Lemma 1 and emphasize that µ has no
learnable parameters in the stable sign regime. Now multiply we to the above expression to get"
TRAINING INVARIANCES,0.215,"we
∂R(w)"
TRAINING INVARIANCES,0.21666666666666667,"∂we
=
X"
TRAINING INVARIANCES,0.21833333333333332,"p∈P,p∋e"
N,0.22,"1
n n
X"
N,0.22166666666666668,"i=1
Ai,p(w),
(10)"
N,0.22333333333333333,"where Ai,p(w) = l′(yiν(xi; w)) · yi ·

µ′(xi; w)
"
N,0.225,"p · (xi)p
Q
f∈p |wf|. Notice that Ai,p(w) does not
depend explicitly on the edge e, with respect to which we are differentiating (only through w). Thus,
we sum over in-edges and out-edges of a particular v satisfying the assumption of Lemma 2 to get
X"
N,0.22666666666666666,"u∈INv
wuv
∂R(w)"
N,0.22833333333333333,"∂wuv
=
X"
N,0.23,"p∈P,p∋v"
N,0.23166666666666666,"1
n n
X"
N,0.23333333333333334,"i=1
Ai,p(w) =
X"
N,0.235,"b∈OUTv
wvb
∂R(w)"
N,0.23666666666666666,"∂wvb
.
(11)"
N,0.23833333333333334,"Note that the only difference between Equations 10 and 11 is the set of paths that we are sum-
ming over, and we double count this set of paths. We use the deﬁnition of gradient ﬂow to obtain
∂R(w)/∂we = dwe(t)/dt and integrate with respect to time using a FTC to get the ﬁrst part of
Lemma 2. More work is needed to get the second part of Lemma 2, which is detailed in Appendix
C. Finally, to get Lemma 3, we double count the set of all paths P."
NONINVARIANCE OF GENERAL RELU LAYERS,0.24,"4.4
NONINVARIANCE OF GENERAL RELU LAYERS"
NONINVARIANCE OF GENERAL RELU LAYERS,0.24166666666666667,"The restriction of Theorem 1 to submatrices of active neurons may appear limiting, but does not
extend to the general case. With the same technique as above, we can write down the gradient for
the Gram matrix W ⊤
1 W1 for ReLU layers and show that it is not equal to its counterpart W2W ⊤
2 ,
thus giving a negative result:
Lemma 4 (Noninvariance in ReLU layers). Even under Assumptions 3 and 1, for a.e. time t ≥0,
d
dt"
NONINVARIANCE OF GENERAL RELU LAYERS,0.24333333333333335,"
W2(t)⊤W2(t) −W1(t)W1(t)⊤
̸= 0,
(12)"
NONINVARIANCE OF GENERAL RELU LAYERS,0.245,"for the different pairs of W1, W2 detailed in Theorem 1."
NONINVARIANCE OF GENERAL RELU LAYERS,0.24666666666666667,"Despite the negative result, the closed form of the gradient for the Gram matrices can be shown to
be low rank with another subgradient model. Details may be found in Appendix C."
NONINVARIANCE OF GENERAL RELU LAYERS,0.24833333333333332,"5
CONSEQUENCES: LOW RANK PHENOMENON FOR NONLINEAR
NONHOMOGENEOUS DEEP FEEDFORWARD NET"
NONINVARIANCE OF GENERAL RELU LAYERS,0.25,"We apply the results from previous parts to prove a low-rank bias result for a large class of feedfor-
ward networks. To the best of our knowledge, this is the ﬁrst time such a result is shown for this
class of deep networks, although the linear fully-connected network analogue has been known for
some time. In light of Theorem 1, we deﬁne a matrix representation of a layer:
Deﬁnition 8 (Matrix representation of a layer). The matrix representation for a ResNetFree block
r(x; U, Y, Z) is
U
Z
; for a T a,b,c ∈Ra×b×c convolutional tensor it is the ﬂattening to an
element of Ra×(b×c); and for a fully-connected layer it is the weight matrix itself."
NONINVARIANCE OF GENERAL RELU LAYERS,0.25166666666666665,Published as a conference paper at ICLR 2022
NONINVARIANCE OF GENERAL RELU LAYERS,0.25333333333333335,"Theorem 2 (Reduced alignment for non-homogeneous networks). Under Assumptions 1 and 3, let ν
consist of an arbitrary feedforward neural network η, followed by K ≥0 linear convolutional layers
(T ak,bk,ck)k∈[K], followed by M ≥0 layers that are either linear ResNetFree blocks or linear fully-
connected layers; and ﬁnally ending with a linear fully-connected layer Fin. For j ∈[K + M],
denote by W [j] the matrix representation of the j-th layer after η, Nr(j) the number of ResNetFree
blocks between j and Fin exclusively and Vc(j) := max dim W [M+1] · Q"
NONINVARIANCE OF GENERAL RELU LAYERS,0.255,"j<k≤M min(ak, bk) if
j ≤M and 1 otherwise. Then there exists a constant D ≥0 ﬁxed at initialization such that for a.e.
time t > 0,
1
8Nr(j)Vc(j)∥W [j](t)∥2
F −∥W [j](t)∥2
2 ≤D, ∀j ∈[K + M].
(13)"
NONINVARIANCE OF GENERAL RELU LAYERS,0.25666666666666665,"Furthermore, assume that ∥W [j]∥F →∞for some j ∈[K + M], then we have, as t →∞:"
NONINVARIANCE OF GENERAL RELU LAYERS,0.25833333333333336,"1/ min

rank(W [k]), 8Nr(j)Vc(j)

≤∥W [k](t)∥2
2/∥W [k](t)∥2
F ≤1,
(14)"
NONINVARIANCE OF GENERAL RELU LAYERS,0.26,"In particular, for the last few fully-connected layers j with Nr(j) = 0 and Vc(j) = 1, we have:

W [j](t)
∥W [j](t)∥F
−uj(t)v⊤
j (t) F"
NONINVARIANCE OF GENERAL RELU LAYERS,0.26166666666666666,"t→∞
−−−→0, ∀k ∈[M],
|

vj+1, uj

|
t→∞
−−−→1,
(15)"
NONINVARIANCE OF GENERAL RELU LAYERS,0.2633333333333333,"where uj and vj are the left and right principal singular vectors of W [j].
Corollary 1. For fully-connected networks with ReLU activations where the last K layers are lin-
ear layers, trained with linearly separable data under logistic loss ℓ, under the assumptions that
R(w(0)) < ℓ(0) and the limiting direction of weight vector (which exists (Ji & Telgarsky, 2020))
has no 0 entries, Equation 15 holds for the last K layers."
NONINVARIANCE OF GENERAL RELU LAYERS,0.265,"Signiﬁcance
Equation 13 and its limiting counterpart Equation 14 quantify a low-rank phe-
nomenon by providing a lower bound on the ratio of the largest squared singular value (the operator
norm) and the sum of all squared singular values (the Frobenius norm). This lower bound depends
on the number (not dimensions) of ResNetFree layers (Nr) and certain dimensions of convolutional
layers (Vc). When the dimensions of ResNetFree layers are large, max dim W [M+1] is small and the
number of input channels of convolutional layers are large, this lower bound is strictly better than the
trivial lower bound of 1/rank(W [k]). This is a quantiﬁcation of the reduced alignment observed in
(Ji & Telgarsky, 2019). In particular, for the last few fully connected layers (Equation 15, Corollary
1), the lower bound matches the upper bound of 1 in the limit of Frobenius norm tending to inﬁnity
and the limiting weight matrices have rank 1 and adjacent layers align."
CONCLUDING REMARKS AND FUTURE DIRECTIONS,0.26666666666666666,"6
CONCLUDING REMARKS AND FUTURE DIRECTIONS"
CONCLUDING REMARKS AND FUTURE DIRECTIONS,0.2683333333333333,"In this paper, we extend the proof of the low rank phenomenon, which has been widely observed in
practice, beyond the linear network case. In particular, we address a variety of nonlinear architectural
structures, homogeneous and non-homogeneous, which in this context have not been addressed
theoretically before. To this end, we decomposed a feedforward ReLU/linear activated network into
a composition of a multilinear function with a tree network. If the weights converge in direction to a
vector with non-zero entries, the tree net is eventually ﬁxed, allowing for chain rules to differentiate
through. This leads to various matrix-wise invariances between fully-connected, convolution layers
and ResNet blocks, enabling us to control the singular values of consecutive layers. In the end, we
obtain a low-rank theorem for said local architectures."
CONCLUDING REMARKS AND FUTURE DIRECTIONS,0.27,"Proving convergence to the stable sign regime for a wider set of architectures will strengthen The-
orem 2.
Another direction is to connect our low-rank bias results to the max-margin implicit
regularization literature, which has been shown for linear networks and, more recently, certain 2-
homogeneous architectures (Ji & Telgarsky, 2020)."
CONCLUDING REMARKS AND FUTURE DIRECTIONS,0.27166666666666667,ACKNOWLEDGMENTS
CONCLUDING REMARKS AND FUTURE DIRECTIONS,0.2733333333333333,"This work was partially funded by NSF CAREER award 1553284 and NSF award 2134108. The
authors thank the anonymous reviewers for their insightful feedback. We would also like to thank
Matus Telgarsky for fruitful discussions on their related papers and on the Clarke subdifferential,
and Kaifeng Lyu for pointing out an error in an earlier version of this paper."
CONCLUDING REMARKS AND FUTURE DIRECTIONS,0.275,Published as a conference paper at ICLR 2022
REFERENCES,0.27666666666666667,REFERENCES
REFERENCES,0.2783333333333333,"Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Jennifer G. Dy and Andreas Krause (eds.), ICML,
volume 80 of Proceedings of Machine Learning Research, pp. 244–253. PMLR, 2018. URL
http://dblp.uni-trier.de/db/conf/icml/icml2018.html#AroraCH18."
REFERENCES,0.28,"J´erˆome Bolte and Edouard Pauwels. A mathematical model for automatic differentiation in ma-
chine learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 10809–10819. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf."
REFERENCES,0.2816666666666667,"Anna Choromanska, MIkael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
Loss Surfaces of Multilayer Networks. In Guy Lebanon and S. V. N. Vishwanathan (eds.), Pro-
ceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics,
volume 38 of Proceedings of Machine Learning Research, pp. 192–204, San Diego, Califor-
nia, USA, 09–12 May 2015. PMLR. URL https://proceedings.mlr.press/v38/
choromanska15.html."
REFERENCES,0.2833333333333333,"F.H. Clarke. Optimization and Nonsmooth Analysis. Wiley New York, 1983."
REFERENCES,0.285,"Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D. Lee. Stochastic subgradient
method converges on tame functions. Foundations of Computational Mathematics, 20(1):119–
154, 2020. doi: 10.1007/s10208-018-09409-5. URL https://doi.org/10.1007/
s10208-018-09409-5."
REFERENCES,0.2866666666666667,"Simon S. Du, Wei Hu, and Jason D. Lee. Algorithmic regularization in learning deep homoge-
neous models: Layers are automatically balanced, 2018."
REFERENCES,0.28833333333333333,"Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of
Machine Learning Research, pp. 3004–3014. PMLR, 2021. URL http://proceedings.
mlr.press/v139/ergen21b.html."
REFERENCES,0.29,"Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193–202, 1980.
doi: 10.1007/BF00344251. URL https://doi.org/10.1007/BF00344251."
REFERENCES,0.2916666666666667,"Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient de-
scent on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf."
REFERENCES,0.29333333333333333,"Boris Hanin and David Rolnick.
Deep relu networks have surprisingly few activation pat-
terns. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf."
REFERENCES,0.295,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90."
REFERENCES,0.2966666666666667,"Christopher Heil. Absolute continuity and the fundamental theorem of calculus, 2019. URL
https://doi.org/10.1007/978-3-030-26903-6_6."
REFERENCES,0.29833333333333334,"Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, 2017."
REFERENCES,0.3,Published as a conference paper at ICLR 2022
REFERENCES,0.3016666666666667,"Minyoung Huh, Hossein Mobahi, Richard Zhang, Pulkit Agrawal, and Phillip Isola. The low-rank
simplicity bias in deep networks. arXiv, 2021."
REFERENCES,0.30333333333333334,"Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In In-
ternational Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=HJflg30qKX."
REFERENCES,0.305,"Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neu-
ral Information Processing Systems, volume 33, pp. 17176–17186. Curran Associates,
Inc., 2020.
URL https://proceedings.neurips.cc/paper/2020/file/
c76e4b2fa54f8506719a5c0dc14c2eb9-Paper.pdf."
REFERENCES,0.30666666666666664,"Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/
paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf."
REFERENCES,0.30833333333333335,"Justin Khim and Po-Ling Loh. Adversarial risk bounds via function transformation, 2019."
REFERENCES,0.31,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,
2015. doi: 10.1038/nature14539. URL https://doi.org/10.1038/nature14539."
REFERENCES,0.31166666666666665,"Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural net-
works.
In International Conference on Learning Representations, 2020.
URL https:
//openreview.net/forum?id=SJeLIgBKPS."
REFERENCES,0.31333333333333335,"Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized op-
timization in deep neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-
ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
eaa32c96f620053cf442ad32258076b9-Paper.pdf."
REFERENCES,0.315,"Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of
optimization and implicit regularization in deep learning, 2017."
REFERENCES,0.31666666666666665,"Adityanarayanan Radhakrishnan, Eshaan Nichani, Daniel Bernstein, and Caroline Uhler. On align-
ment in deep linear neural networks, 2020."
REFERENCES,0.31833333333333336,"Miaoyan Wang, Khanh Dao Duc, Jonathan Fischer, and Yun S. Song. Operator norm inequalities
between tensor unfoldings on the partition lattice, May 2017. ISSN 0024-3795. URL http:
//dx.doi.org/10.1016/j.laa.2017.01.017."
REFERENCES,0.32,"Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and
landscape properties. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=SysEexbRb."
REFERENCES,0.32166666666666666,"A
ILLUSTRATION OF LEMMA 1"
REFERENCES,0.3233333333333333,"We give a quick illustration of the two steps described in Section 3 for a fully-connected ReLU-
activated network with 1 hidden layer. Figure 1 describes the unrolling of the neural network (Figure
1a) into a tree network (Figure 1b). Figure 2a describes the weight pull-back in the hidden layer and
Figure 2b describes the weight pull-back in the input layer. It is clear that the inputs of the tree net
in Figure 2b are coordinates of the path enumeration function h(x; w) in this example. Furthermore,
weights in the tree net depend entirely on the signs of the original weights. The rest of the proof
argues this intuition for general feed-forward neural nets. As a remark, in general, ρ is a very large
number - exponential in the number of layers for a fully-connected net with ﬁxed width."
REFERENCES,0.325,"Published as a conference paper at ICLR 2022 o v1
v2 i1
i2 x1 x2"
REFERENCES,0.32666666666666666,"w[1]
11"
REFERENCES,0.3283333333333333,"w[1]
12"
REFERENCES,0.33,"w[1]
21"
REFERENCES,0.33166666666666667,"w[1]
22"
REFERENCES,0.3333333333333333,"w[2]
1"
REFERENCES,0.335,"w[2]
2 (a) o v1
v2"
REFERENCES,0.33666666666666667,"i11
i12
i21
i22 x1 x2 x1 x2"
REFERENCES,0.3383333333333333,"w[1]
11"
REFERENCES,0.34,"w[1]
12"
REFERENCES,0.3416666666666667,"w[1]
21"
REFERENCES,0.3433333333333333,"w[1]
22"
REFERENCES,0.345,"w[2]
1"
REFERENCES,0.3466666666666667,"w[2]
2 (b)"
REFERENCES,0.34833333333333333,"Figure 1: Transformation of a feedforward network into a tree net. All nodes apart from the input
nodes use ReLU activation. The two neural nets drawn here compute the same function. This idea
has been used in Khim & Loh (2019) to prove generalization bounds for adversarial risk.
. o v1
v2"
REFERENCES,0.35,"i11
i12
i21
i22 x1 x2 x1 x2"
REFERENCES,0.3516666666666667,"w[1]
11|w[2]
1 |"
REFERENCES,0.35333333333333333,"w[1]
12|w[2]
2 |"
REFERENCES,0.355,"w[1]
21|w[2]
1 |"
REFERENCES,0.3566666666666667,"w[1]
22|w[2]
2 |"
REFERENCES,0.35833333333333334,"sgn(w[2]
1 )"
REFERENCES,0.36,"sgn(w[2]
2 ) (a) o v1
v2"
REFERENCES,0.3616666666666667,"i11
i12
i21
i22"
REFERENCES,0.36333333333333334,"x1|w[1]
11w[2]
1 |"
REFERENCES,0.365,"x2|w[1]
21w[2]
1 |"
REFERENCES,0.36666666666666664,"x1|w[1]
12w[2]
2 |"
REFERENCES,0.36833333333333335,"x2|w[1]
22w[2]
2 |"
REFERENCES,0.37,"sgn(w[1]
11)"
REFERENCES,0.37166666666666665,"sgn(w[1]
12)"
REFERENCES,0.37333333333333335,"sgn(w[1]
21)"
REFERENCES,0.375,"sgn(w[1]
22)"
REFERENCES,0.37666666666666665,"sgn(w[2]
1 )"
REFERENCES,0.37833333333333335,"sgn(w[2]
2 ) (b)"
REFERENCES,0.38,"Figure 2: Pulling back of weights in a tree net. All nodes apart from the input nodes use the
ReLU activation. The original net was drawn in Figure 1a and 1b. This is possible due to the
positive-homogeneity of the activation function. From Figure 2b, one can recover the ﬁnal tree net
in Theorem 1 with weights from {−1, +1} and input from the path enumeration function h of the
neural net by subdividing the edge incident to the input neurons and assign weights corresponding
to sgn(h(x))."
REFERENCES,0.38166666666666665,"B
PROOF OF LEMMA 1"
REFERENCES,0.38333333333333336,"We ﬁrst prove an absolute-valued version of Lemma 1 and show that the extension to Lemma 1 is
straightforward. In other words, we ﬁrst prove"
REFERENCES,0.385,"Lemma 5 (Absolute-valued decomposition). For an arbitrary feed-forward neural network ν :
Rd →R, there exists a tree network µ′ : Rρ →R such that ν = µ′ ◦h′ where h′ is the"
REFERENCES,0.38666666666666666,Published as a conference paper at ICLR 2022
REFERENCES,0.3883333333333333,"absolute-valued path enumeration function deﬁned as h′(x) =

xp
Q"
REFERENCES,0.39,"e∈p |we|
"
REFERENCES,0.39166666666666666,"p∈P. Furthermore,"
REFERENCES,0.3933333333333333,"the weights of µ′ is in {−1, 1} and only depends on the sign of the weights of the original network
ν."
REFERENCES,0.395,"We ﬁrst give some extra notation for this section. In general, we deﬁne:"
REFERENCES,0.39666666666666667,"• ℘(S) to be the ﬁnite power set of some ﬁnite set S.
• |p| ∈N≥0 to be the cardinality of a set p. When p is a path then it is viewed as the number
of edges."
REFERENCES,0.3983333333333333,"To make it clear which graph we are referring to, deﬁne for an arbitrary feedforward neural network
ν : Rd →R with computation graph G = (G[V ], G[E], G[w]):"
REFERENCES,0.4,"• IG = {iG
1 , . . . , iG
d } to be the set of input node of G and OG = {oG} to be the set of the
single output node of G.
• PG to be the enumeration of all paths from any input node in IG to the output node oG.
• HG to be the enumeration of all paths from any node v ∈G[V ] to the output node oG.
Note that if G has more than 2 vertices then HG ⊃PG.
• Each v ∈G[V ] to be equipped with a ﬁx activation σv that is positively-1-homogeneous.
To be precise, G is enforced to be a DAG, is connected and is simple (no self-loop, at most
1 edge between any pair of vertices). Each node v of G[V ] is equipped with: an activation
function G[σ](v) that is positively-1-homogeneous; a pre-activation function deﬁned as:"
REFERENCES,0.40166666666666667,(G[PRE](v))(x) =
REFERENCES,0.4033333333333333,"(
xj
if v ≡iG
j for some j ∈[d]
P"
REFERENCES,0.405,"u∈INv POSTv(x)wuv
otherwise;
(16)"
REFERENCES,0.4066666666666667,and a post-activation function deﬁned as
REFERENCES,0.4083333333333333,"(G[POST](v))(x) = σv((G[PRE](v))(x))
(17)"
REFERENCES,0.41,"Note that POSToG = ν.
• vG
p , eG
p to be the vertex and edge, respectively, furthest from oG on some path p of G."
REFERENCES,0.4116666666666667,"• xG
p to be the unique xj such that iG
j ∈p."
REFERENCES,0.41333333333333333,"Deﬁnition 9 (Hasse diagram of inclusion). Let S be a ﬁnite set and a set S ⊂℘(S) of elements
in S. A Hasse diagram of S is a directed unweighted graph HASSE(S) = (S, E) where for any
p, q ∈S, pq ∈E iff p ⊆q and |q| −|p| = 1.
Deﬁnition 10 (Unrolling of feedforward neural networks). The unrolled tree neural network τG of
G is a tree neural network with computation graph TG where"
REFERENCES,0.415,"• Unweighted graph (TG[V ], TG[E]) = HASSE(HG). In particular TG[V ] = HG and we
identify vertices of TG with paths in G."
REFERENCES,0.4166666666666667,• Weight function TG[w] : TG[E] ∋pq 7→G[w](ep).
REFERENCES,0.41833333333333333,"• Activation TG[σ](v) := G[σ](vp).
Lemma 6 (Unrolled network computes the same function). Fix an arbitrary feedforward neural
network ν : Rd →R with computation graph G. Let τG be the unrolled tree network of G with
computation graph TG. Then ν = τG."
REFERENCES,0.42,"Proof. We proceed with induction on αG := maxp∈P |p| the longest path between any input node
and the output node of G. In the base case, set αG = 0. Then VG = {oG} is a singleton and the
neural network ν computes the activation of the input and return it. HG = VT = {p0} then is
a singleton containing just the trivial path that has just the output vertex of G and no edges. The
activation function attached to p0 in T, by construction, is the activation of oG. Thus, τ also simply
returns the activation of the input."
REFERENCES,0.4216666666666667,"Assume that τG = ν for any ν with graph G such that αG ≤t −1 for some t ≥1; and for the τ
constructed as described. We will show that the proposition is also true when ν has graph G with
αG = t. Fix such a ν and G that αG = t. We prove this induction step by:"
REFERENCES,0.42333333333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.425,"1. First constructing G′ from G such that ν′ = ν where ν′ is the neural network computed by
G′."
REFERENCES,0.4266666666666667,"2. Then showing that TG = G′ by constructing an isomorphism π : G′[V ] →TG[V ] that
preserves E, w and σ."
REFERENCES,0.42833333333333334,"The construction of G′
An illustration of the following steps can be found in Figure 3. Recall
that INoG = {v1, . . . , vm} is the set of in-vertices of oG."
REFERENCES,0.43,"1. Create m distinct, identical copies of G: G1, . . . , Gm."
REFERENCES,0.43166666666666664,"2. For each j ∈[m], remove from Gj all vertices u (and their adjacent edges) such that there
are no directed path from u to vj."
REFERENCES,0.43333333333333335,"3. We now note that αGj ≤t −1 (to be argued) and invoke inductive hypothesis over Gj
to get an unrolled tree network τj with graph Tj such that τj = νj where νj is the neural
network computed by Gj."
REFERENCES,0.435,"4. Finally, construct G′ by creating a new output vertex oG′ and connect it to the output
vertices oTj for all j ∈[m]. As a sanity check, since each Tj is a tree network, so is G′.
More precisely,"
REFERENCES,0.43666666666666665,"(a) G′[V ] = {oG′} ∪Sm
j=1 Tj[V ];"
REFERENCES,0.43833333333333335,"(b) G′[E] = {oGjoG′ | j ∈[m]} ∪Sm
j=1 Tj[E]"
REFERENCES,0.44,"(c) G′[w](e) = G[w](vjoG) if e ≡oGjoG′ for some j ∈[m] and Tj[w](e), where
e ∈Tj[E], otherwise.
(d) G′[σ](v) = G[σ](oG) if v ≡oG and Tj[σ](v), where v ∈Tj[V ], otherwise. o v1
v2 i1
i2 x1 x2"
REFERENCES,0.44166666666666665,"w[1]
11"
REFERENCES,0.44333333333333336,"w[1]
12"
REFERENCES,0.445,"w[1]
21"
REFERENCES,0.44666666666666666,"w[1]
22"
REFERENCES,0.4483333333333333,"w[2]
1"
REFERENCES,0.45,"w[2]
2 (a) G v1 i1
i2 x1 x2"
REFERENCES,0.45166666666666666,"w[1]
11"
REFERENCES,0.4533333333333333,"w[1]
21"
REFERENCES,0.455,"(b) G1 ≡TG1 v2 i1
i2 x1 x2"
REFERENCES,0.45666666666666667,"w[1]
12"
REFERENCES,0.4583333333333333,"w[1]
22"
REFERENCES,0.46,(c) G2 = TG2
REFERENCES,0.46166666666666667,"Figure 3: Construction of G′. G1 and G2 are the modiﬁed copies of G in step 2. In step 3, the
transformation TGi happens to coincide with Gi for i = 1, 2 in this case. G′ is created in step 4 by
adding an extra vertex oG′ and connecting it to v1 and v2 with the appropriate weights and activation
and can be seen in Figure 1b.
."
REFERENCES,0.4633333333333333,"G′ is well-deﬁned
We verify each steps in the above construction:"
REFERENCES,0.465,1. This step is well-deﬁned.
REFERENCES,0.4666666666666667,"2. For any Gj, as long as αG ≥2 (by deﬁnition), we always remove oG from each Gj in each
step; since vj is an in-vertex of oG and G is a DAG. Otherwise, this step is well-deﬁned."
REFERENCES,0.4683333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.47,"3. Fix a j ∈[m]. By construction (and since we always remove oG from Gj in the previous
step), OGj = {vj}. If there is a path p∗with length at least t in Gj, then since Gj[E] ⊆
G[E] and oG ∈G[V ] \ Gj[V ], the path p∗∪{oG} created by appending oG to p∗is a
valid path with length t + 1. This violates the assumption that αG = t and we conclude,
by contradiction, that αGj ≤t −1. This justiﬁes the invocation of inductive hypothesis for
νj to get a tree neural net τj."
REFERENCES,0.4716666666666667,4. The ﬁnal step is well-deﬁned.
REFERENCES,0.47333333333333333,"ν′ computes the same function as ν
Recall that ν′ is the neural network with graph G′. We have
for any input x ∈Rd"
REFERENCES,0.475,"ν′(x) = (G′[POST](oG′))(x)
(18)"
REFERENCES,0.4766666666666667,"= G′[σ](oG′)  
m
X"
REFERENCES,0.47833333333333333,"j=1
G′[w](vjoG′) · (G′[POST](vj))(x) "
REFERENCES,0.48,"
(19)"
REFERENCES,0.4816666666666667,"= G[σ](oG)  
m
X"
REFERENCES,0.48333333333333334,"j=1
G[w](vjoG) · (Tj[POST](vj))(x) "
REFERENCES,0.485,"
(20)"
REFERENCES,0.4866666666666667,"= G[σ](oG)  
m
X"
REFERENCES,0.48833333333333334,"j=1
G[w](vjoG) · (G[POST](vj))(x) "
REFERENCES,0.49,"= ν(x)
(21)"
REFERENCES,0.49166666666666664,"where we invoked the inductive hypothesis in the last line to get
(Tj[POST](vj))(x) = τj(x) = νj(x) = (G[POST](vj))(x),
(22)
and the rest are deﬁnitions."
REFERENCES,0.49333333333333335,"G′ is isomorphic to TG
Although this should be straightforward from the construction, we give a
formal proof. Consider the isomorphism π : G′[V ] →TG[V ] given as"
REFERENCES,0.495,π(v) =
REFERENCES,0.49666666666666665,"(
oTG = {oG} ∈HG
if v ≡oG′
p ∪{oG}
if Tj[V ] ∋v ≡p ∈HGj for some j ∈[m],
(23)"
REFERENCES,0.49833333333333335,"where paths are viewed as set of vertices. The second case is well-deﬁned since p ∪{oG} ∈HG for
any p ∈HGj for any j ∈[m] by construction of Gj."
REFERENCES,0.5,"We can now verify that π is an isomorphism between the two structures. Fix pq ∈G′[E]. Consider
two separate cases: pq ∈Tj[E] for some j and pq ̸∈Tj[E] for any j. In the ﬁrst case, by deﬁnition
of Tj as a Hasse diagram, p = {vp} ∪q are paths in HGj. Thus, π(p)π(q) = (p ∪{oG})(q ∪
{oG}) satisfying π(p) = {vp} ∪π(q). Thus, by deﬁnition of Hasse diagram, π(p)π(q) ∈TG[E].
Furthermore, G′[w](pq) = G[w](ep) = TG[w](π(p)π(q)) In the second case, we have pq = vjoG
for some j ∈[m]. Thus, π(p)π(q) = ({vj, oG})({oG}) ∈TG[E] also by deﬁnition of Hasse
diagram. At the same time, G′[w](pq) = G[w](vjoG) = TG[w](π(p)π(q)) by deﬁnition."
REFERENCES,0.5016666666666667,"Fix v ∈G′[V ]. If v ≡oG′ then G′[σ](v) = G[σ](oG). At the same time, TG[σ](π(v)) =
G[σ](v{oG} = G[σ](oG) = G′[σ](v). If v ̸≡oG′ then there is a j ∈[m] and a p ∈HGj such
that v = p ∈Tj[V ]. Then by deﬁnition,
G′[σ](v) = Tj[σ](v) = G[σ](vp) = G[σ](vp ∪{oG′}) = TG[σ](π(v)).
(24)"
REFERENCES,0.5033333333333333,"This completes the inductive proof that shows τG = ν′ = ν for G with αG = t. By mathematical
induction, the claim holds for all neural network ν."
REFERENCES,0.505,"Lemma 7 (Pull-back of numerical weights). Fix an arbitrary feedforward neural network ν : Rd →
R with computation graph G. Let v ∈G[V ] \ (IG ∪OG) be an inner vertex of G with k in-edges
e1, . . . , ek ∈G[E] and a single out-edge f ∈G[E]. Then the network ν′ with computation graph
G′ = (G[V ], G[E], G′[w]) deﬁned as"
REFERENCES,0.5066666666666667,"G′[w] : e 7→ 

 
"
REFERENCES,0.5083333333333333,"G[w](e)|G[w](f)|
if e ≡ej for some j ∈[k]
sgn(G[w](f))
if e ≡f
G[w](e)
otherwise.
(25)"
REFERENCES,0.51,Published as a conference paper at ICLR 2022
REFERENCES,0.5116666666666667,"computes the same function as ν. In other words, we can pull the numerical values of G[w](f)
through v, into its in-edges; leaving behind only its sign."
REFERENCES,0.5133333333333333,"When ﬁxing input x to G, one can extend this operation to ij ∈IG for j ∈[d]. By setting G′[w](f) =
sgn(G[w](f)) and update xj to xj|G[w](f)|."
REFERENCES,0.515,"Proof. Let the single out-vertex of v be b. If sufﬁces to show that G[PRE](b) = G′[PRE](b). Fix
an input x to ν. Let the k in-edges of v be a1, . . . , ak. Since we only change edges incident to v,
G[POST]aj = G′[POST]aj for all j ∈[k]. We have:"
REFERENCES,0.5166666666666667,"G′[PRE](b) = sgn(G[w](vb)) · (G[σ](v))(G′[PRE](v))
(26)"
REFERENCES,0.5183333333333333,"= sgn(G[w](vb)) · (G[σ](v))  
k
X"
REFERENCES,0.52,"j=1
|G[w](vb)|G[w](ajv) · G[POST](aj) "
REFERENCES,0.5216666666666666,"
(27)"
REFERENCES,0.5233333333333333,"= sgn(G[w](vb))|G[w](vb)| · (G[σ](v))  
k
X"
REFERENCES,0.525,"j=1
G[w](ajv) · G[POST](aj) "
REFERENCES,0.5266666666666666,"
(28)"
REFERENCES,0.5283333333333333,"= G[w](vb) · (G[σ](v))  
k
X"
REFERENCES,0.53,"j=1
G[w](ajv) · G[POST](aj) "
REFERENCES,0.5316666666666666,"= G[PRE](b),
(29)"
REFERENCES,0.5333333333333333,where equation 28 is due to positive homogeneity of σv and the rest are just deﬁnitions.
REFERENCES,0.535,We can now give the proof of Lemma 5.
REFERENCES,0.5366666666666666,"Proof of Lemma 5. Given an arbitrary feedforward neural network ν with computation graph
G
=
(G[V ], G[E], G[w]), we use Lemma 6 and get the unrolled tree network TG
=
(TG[V ], TG[E], TG[w]) such that ν = τG."
REFERENCES,0.5383333333333333,"Let π = π1, π2, . . . , πξ be an ordering of TG[V ] \ OTG (so ξ = |H| −1) by a breadth ﬁrst search
on TG starting from oTG. In other words, if dtopo(u, oTG) > dtopo(v, oTG) then u appears after v in
π, where dtopo(a, b) is the number of edges on the unique path from a to b for some a, b ∈TG[V ].
Iteratively apply Lemma 7 to vertex π1, . . . , πξ in TG while maintaining the same function. After
ξ such applications, we arrive at a network µ′
G with graph MG such that µ′
G = τG = ν. Recall
that the pull-back operation of Lemma 7 only changes the tree weights. Furthermore, the π ordering
is chosen so that subsequent weight pull-back does not affect edges closer to oTG. Therefore, at
iteration j,"
REFERENCES,0.54,"1. MG[w](πkq) = sgn(G[w](eπk)) for all k ≤j, for some q ∈MG[V ] such that (πk)q ∈
MG[E]."
REFERENCES,0.5416666666666666,2. if πj ̸∈IMG then MG[w](r(πj)) = G[w](er) Q
REFERENCES,0.5433333333333333,"f∈πj |G[w](f)|, for some r ∈MG[V ]
such that r(πj) ∈MG[E]; otherwise, πj is an input vertex corresponding to input xπj,
then xπj is modiﬁed to xπj
Q"
REFERENCES,0.545,"f∈πj |G[w](f)| = h′
G(xπj) where h′
G is the absolute-valued
path enumeration function."
REFERENCES,0.5466666666666666,This completes the proof.
REFERENCES,0.5483333333333333,Now we present the extension to Lemma 1:
REFERENCES,0.55,"Proof of Lemma 1. Invoke Lemma 5 to get a tree network µ′ such that ν = µ′ ◦h′. Then one
can subdivide each input edges (edges that are incident to some input neuron ij) into two edges
connected by a neuron with linear activation. One of the resulting egde takes the weight of the old
input edge; and the other is used to remove the absolute value in the deﬁnition of the (basic) path
enumeration function."
REFERENCES,0.5516666666666666,"More formally, for all p ∈P, recall that ip is a particular input neuron of µ′ in the decomposition
lemma (Lemma 1). Since µ′ is a tree neural network, we there exists a distinct node up in µ′ that is"
REFERENCES,0.5533333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.555,"adjacent to ip. Remove the edge ipup, add a neuron u′
p, connect ipu′
p and u′
pup, where the weight"
REFERENCES,0.5566666666666666,"of the former neuron is set to sgn
Q"
REFERENCES,0.5583333333333333,"e∈p we

and the latter to w[µ′](ipup). It is straightforward to
see that with µ constructed from above, ν = µ ◦h where h is the path enumeration function."
REFERENCES,0.56,"With slight modiﬁcations to the proof technique, one can show all the results for Theorem 1, The-
orem 2 and Corollary 1 to the same matrix representation as presented in the paper but with the
absolute signs around them."
REFERENCES,0.5616666666666666,"C
PROOF OF TRAINING INVARIANCES"
REFERENCES,0.5633333333333334,"Claim 1 (Directional convergence to non-vanishing point implies stable sign). Assumption 2 implies
Assumption 1."
REFERENCES,0.565,"Proof. Let v be the direction that
w(t)
∥w(t)∥2 converges to. Let O be the orthant that v lies in. Since v
does not have a 0 entry, the ball B with radius mini |vi|/2 and center v is a subset of the interior of
O. Since
w(t)
∥w(t)∥2 converges to v, there exists a time T such that for all s > T,
w(s)
∥w(s)∥2 ∈B. Thus,
eventually, w(s) ∈B where its signs stay constant."
REFERENCES,0.5666666666666667,"Claim 2 (Directional convergence does not imply stable activation). There exists a function w : R →
Rd such that w(t) converges in direction to some vector v but for all u ∈w(R), w−1(u) has inﬁnitely
many elements. This means that for some ReLU network empirical risk function R(w) whose set
of nondifferentiable points D has nonempty intersection with w(R), the trajectory

w(t)"
REFERENCES,0.5683333333333334,"t≥0 can
cross a boundary from one activation pattern to another an inﬁnite number of times."
REFERENCES,0.57,Proof. Fix a vector v ∈Rd. Consider the function t 7→v|t sin(t)|.
REFERENCES,0.5716666666666667,"Lemma 8 (Clarke partial subderivatives of inputs with the same activation pattern is the same).
Assume stable sign regime (Assumption 1). Let p1, p2 ∈P be paths of the same length L. Let
p1 = {v1, . . . , vL} ∈V L and p2 = {u1, . . . , uL} ∈V L. Assume that for each i ∈[L], we have vi
and ui having the same activation pattern for each input training example, where the activation of a
neuron is 0 if it is ReLU activated and has negative preactivation; and is 1 if it is linearly activated
or has nonnegative preactivation. Then ∂p1µ(h) = ∂p2µ(h) where ∂is the Clarke subdifferential."
REFERENCES,0.5733333333333334,"Proof. In this proof, we use the absolute-valued version of the decomposition lemma. Fix a training
example j and some weight w0 and let the output of the path enumeration function be h := (hp =
h(xj; w0))p∈P. Denote X ⊆R2 the input space of all possible pairs of values of (p1, p2) such that
Assumption 1 and the extra assumption that both paths have the same activation pattern on each
neuron hold. Let m : R2 →R be the same function as the tree network µ but with all but the two
inputs at p1, p2 frozen. We will show that m is symmetric in its input. Once this is establish, it is
trivial to invoke the deﬁnition of the Clarke subdifferential to obtain the conclusion of the lemma."
REFERENCES,0.575,"Let (a, b) ∈X ⊆R2. We thus show that if (b, a) ∈X then m(a, b) = m(b, a). Recall that µ is itself
a ReLU-activated (in places where the corresponding original neurons are ReLU-activated) neural
network. The fact that µ has a tree architecture means that for each input node, there is a unique
path going to the output node. Thus, the set of paths from some input node in µ to its output node
can be identiﬁed with the set of inputs itself: P. Now let us considered the product of weights on
some arbitrary path p. It is not hard to see that for each such path, the product is just Pp = Q"
REFERENCES,0.5766666666666667,"e∈p we
since the input to p is |P| and going along the path collects all signs of we for all e ∈p."
REFERENCES,0.5783333333333334,"We now invoke the 0-1 form of ReLU neural network to get m(a, b)
=
µ(h)
=
P"
REFERENCES,0.58,"p∈P Zp(a, b)Pp(a, b) where Zp is 1 iff all neurons on path p is active in the µ network (recall that
we identify paths in µ to input nodes). Consider that what changes between m(a, b) and m(b, a):
since µ is a tree network, exchanging two inputs can only have effect on the activation pattern of
neurons along the two paths from these inputs. However, we restricted X to be the space where
activation pattern of each neuron in the two paths is identical to one another. Since both (a, b) and
(b, a) is in X, swapping one for another does not affect the activation pattern of each neuron in the
two paths! These neurons activation pattern is then identical to those in network µ by construction"
REFERENCES,0.5816666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.5833333333333334,"and hence Zp(a, b) = Zp(b, a) for all p ∈P since activation pattern of each node of the µ network
stays the same. Thus we conclude that m(a, b) = m(b, a). This completes the proof."
REFERENCES,0.585,"Proof of Lemma 2. This proof uses the absolute-value-free version of the decomposition lemma.
This is just to declutter notations, as the same conclusion can also be reach using the other version,
with some keeping track of weight signs. Recall that our real weight vector w(t) is an arc Ji &
Telgarsky (2020) which, by deﬁnition, is absolutely continuous. It then follows from real analysis
that its component we(t) is also absolutely continuous for any e ∈E. For absolutely continuous
functions we(t) for some e ∈E, invoke the Fundamental Theorem of Calculus (FTC) (Chapter 6,
(Heil, 2019)), to get:
X"
REFERENCES,0.5866666666666667,"u∈INv
w2
uv(t) −
X"
REFERENCES,0.5883333333333334,"u∈INv
w2
uv(0) = 2
X u∈INv Z"
REFERENCES,0.59,"[0,t]
wuv(s)dwuv"
REFERENCES,0.5916666666666667,"dt (s)ds
(30)"
REFERENCES,0.5933333333333334,We now proceed to compute dw
REFERENCES,0.595,"dt (s). Since w(t) is absolutely continuous and we are taking the
integral via FTC, we only need to compute dw"
REFERENCES,0.5966666666666667,"dt (s) for a.e. time s. By chain rule (see for example,
the ﬁrst line of the proof of Lemma C.8 in Lyu & Li (2020)), there exists functions (gj)n
j=1 such that
gj ∈∂νxj(w) ⊆R|E| for all j ∈[n] where νxj(w) = ν(xj; w), and for a.e. time s ≥0, dw"
REFERENCES,0.5983333333333334,"dt (s) = 1 n n
X"
REFERENCES,0.6,"j=1
l′(yjν(xj; w(s))) · yj · gj
(31)"
REFERENCES,0.6016666666666667,"Fix j ∈[n], by the inclusion chain rule, since νxj = µ ◦hxj, with h and µ also locally Lipschitz,
we have by Theorem I.1 of Lyu & Li (2020),"
REFERENCES,0.6033333333333334,"∂νxj(w) = ∂(µ ◦hxj)(w) ⊆CONV 
  X"
REFERENCES,0.605,"p∈ρ
[α]pβj,p | α ∈∂µ(h(w)), βj,p ∈∂[hxj]p(w) 
"
REFERENCES,0.6066666666666667,".
(32)"
REFERENCES,0.6083333333333333,"Thus there exists (γa)A
a=1 ≥0, PA
a=1 γa = 1 and (αa ∈∂µ(h(w)), βa
j,p ∈∂[hxj]p(w))A
a=1 such
that: gj = A
X"
REFERENCES,0.61,"a=1
γa
X"
REFERENCES,0.6116666666666667,"p∈ρ
[αa]pβa
j,p."
REFERENCES,0.6133333333333333,"Here we use Assumption 1 to deduce that eventually, all weights are non-zero in gradient ﬂow
trajectory to compute:"
REFERENCES,0.615,∂[hxj]p(w) =
REFERENCES,0.6166666666666667,"(
d[hxj]p(w) d(w) ) = 

 
 "
REFERENCES,0.6183333333333333,"1e∈p · (xj)p
Y"
REFERENCES,0.62,"f∈p,f̸=e
wf   e∈E 

 

."
REFERENCES,0.6216666666666667,"Plug this back into gj to get: gj =  
A
X"
REFERENCES,0.6233333333333333,"a=1
γa
X"
REFERENCES,0.625,"p∈ρ|e∈p
[α]a
p · (xj)p
Y"
REFERENCES,0.6266666666666667,"f∈p,f̸=e
wf   e∈E ."
REFERENCES,0.6283333333333333,Plug this back into dw
REFERENCES,0.63,"dt (s) and to get, coordinate-wise, for a.e. s ≥0, dwe"
REFERENCES,0.6316666666666667,"dt (s) = 1 n n
X"
REFERENCES,0.6333333333333333,"j=1
l′(yiν(xi; w(s))) · yi · A
X"
REFERENCES,0.635,"a=1
γa
X"
REFERENCES,0.6366666666666667,"p∈ρ|e∈p
[αa]p · (xj)p
Y"
REFERENCES,0.6383333333333333,"f∈p,f̸=e
wf(s).
(33)"
REFERENCES,0.64,Multiply both sides with we gives:
REFERENCES,0.6416666666666667,we(s)dwe
REFERENCES,0.6433333333333333,"dt (s) =
X"
REFERENCES,0.645,p∈P|e∈p
N,0.6466666666666666,"1
n n
X"
N,0.6483333333333333,"j=1
dj,p(w(s)),
(34)"
N,0.65,Published as a conference paper at ICLR 2022 where
N,0.6516666666666666,"dj,p(w) = ℓ′(yiν(xi; w)) · yi · A
X"
N,0.6533333333333333,"a=1
γa[α]a
p · (xj)p ·
Y"
N,0.655,"f∈p
|wf(s)|.
(35)"
N,0.6566666666666666,"Note that d does not depend on the speciﬁc edge e used in Equation 33 and also that the term given
by β does not depend on a and we can simply write αp for PA
a=1 γa[α]a
p."
N,0.6583333333333333,Plugging back into the FTC to get: X
N,0.66,"u∈INv
w2
uv(t) −
X"
N,0.6616666666666666,"u∈INv
w2
uv(0) = 2
X u∈INv Z [0,t] X"
N,0.6633333333333333,p∈P|uv∈p
N,0.665,"1
n n
X"
N,0.6666666666666666,"j=1
dj,p(w(s))ds
(36) = 2
Z [0,t] X"
N,0.6683333333333333,p∈P|v∈p
N,0.67,"1
n n
X"
N,0.6716666666666666,"j=1
dj,p(w(s))ds.
(37)"
N,0.6733333333333333,"Finally, by an identical argument but applied to the set of edges vb for some b ∈OUTv, we have: X"
N,0.675,"b∈OUTv
w2
vb(t) −
X"
N,0.6766666666666666,"b∈OUTv
w2
vb(0) = 2
Z [0,t] X"
N,0.6783333333333333,p∈P|v∈p
N,0.68,"1
n n
X"
N,0.6816666666666666,"j=1
dj,p(w(s))ds
(38) =
X"
N,0.6833333333333333,"u∈INv
w2
uv(t) −
X"
N,0.685,"u∈INv
w2
uv(0),
(39)"
N,0.6866666666666666,which completes the proof of the ﬁrst part of the lemma.
N,0.6883333333333334,"For the second part, recall that we have 2 vertices u, v such that INv = INu = IN and OUTv =
OUTu = OUT with stable activation pattern for each training example. To make it more readable, we
drop the explicit dependence on t in our notation and introduce some new ones: for some a ∈IN, let
PI→a be the set of all paths from some input node in I to node a and for some b ∈OUT, let Pb→o
be the set of all paths from b to the output node o. Then one can decompose the sum as"
N,0.69,"d
dtwau = n
X"
N,0.6916666666666667,"j=1
−ℓ′(yν(xj; w)) · y ·
X"
N,0.6933333333333334,p1∈PI→a X b∈OUT X
N,0.695,"p2∈Pb→o
(xj)p1 · wub ·
Y"
N,0.6966666666666667,"f∈p1∪p2
wf · αp1∪{u}∪p2, (40)"
N,0.6983333333333334,where αp is the partial Clarke subdifferential at input p.
N,0.7,Recall that the end goal is to derive
N,0.7016666666666667,"d
dtwauwav = wau
d
dtwav + wav
d
dtwau.
(41)"
N,0.7033333333333334,"Using equation equation 40, the second term on the right hand side becomes"
N,0.705,"wav
d
dtwau = n
X"
N,0.7066666666666667,"j=1
−ℓ′(yν(xj; w)) · y ·
X"
N,0.7083333333333334,p1∈PI→a X b∈OUT X
N,0.71,"p2∈Pb→o
(42) "
N,0.7116666666666667,"(xj)p1 · wav ·  
Y"
N,0.7133333333333334,"f∈p1∪p2
wf "
N,0.715,· wub 
N,0.7166666666666667,"· αp1∪{u}∪p2(xj; w)
(43)"
N,0.7183333333333334,Published as a conference paper at ICLR 2022
N,0.72,"where the product wav ·
Q"
N,0.7216666666666667,"f∈p1∪p2 ·wf

wub is a jagged path. Then one continues with the deriva-
tion to get d
dt  X"
N,0.7233333333333334,"a∈IN
wauwav  =
X a∈IN"
N,0.725,"d
dt(wauwav)
(44) = n
X"
N,0.7266666666666667,"j=1
−ℓ′(yν(xj; w)) · y ·
X a∈IN X b∈OUT X"
N,0.7283333333333334,p1∈PI→a X
N,0.73,"p2∈Pb→o
(45) "
N,0.7316666666666667,"(xj)p1 ·
Y"
N,0.7333333333333333,"f∈p1∪p2
wf "
N,0.735,"

wav · wub · αp1∪{u}∪p2(xj; w) + wau · wvb · αp1∪{v}∪p2(xj; w)
 (46)"
N,0.7366666666666667,"On the other hand d
dt  X"
N,0.7383333333333333,"b∈OUT
wubwvb  =
X b∈OUT"
N,0.74,"d
dt(wubwvb)
(47) = n
X"
N,0.7416666666666667,"j=1
−ℓ′(yν(xj; w)) · y ·
X a∈IN X b∈OUT X"
N,0.7433333333333333,p1∈PI→a X
N,0.745,"p2∈Pb→o
(48) "
N,0.7466666666666667,"(xj)p1 ·
Y"
N,0.7483333333333333,"f∈p1∪p2
wf "
N,0.75,"

wav · wub · αp1∪{v}∪p2(xj, w) + wau · wvb · αp1∪{u}∪p2(xj; w)

. (49)"
N,0.7516666666666667,"This is where the more restrictive assumption that for each training example, u and v have the
same activation pattern as each other (but the same activation pattern between u and v of, say
x1, may differs from that on, say x2). Under this assumption, we can invoke Lemma 8 to get
αp1∪{u}∪p2(xj; w) = αp1∪{v}∪p2(xj; w) as a set. This identiﬁes 46 with 49 and gives us: d
dt  X"
N,0.7533333333333333,"a∈IN
wau(t)wav(t) −
X"
N,0.755,"b∈OUT
wub(t)wvb(t) "
N,0.7566666666666667,"= 0.
(50)"
N,0.7583333333333333,"This holds for any time t where u and v has the same activation pattern. If further, they have the
same activation pattern throughout the training phase being considered, then one can invoke FTC to
get the second conclusion of Lemma 2. Note, however, that we will be using this differential version
in the proof of Theorem 1."
N,0.76,"Proof of Lemma 3. The proof is identical to that of Lemma 2, with the only difference being the set
A that we double count. Here, set A to be P. Then by the deﬁnition of layer (see Deﬁnition 7), one
can double count P by counting paths that goes through any element of a particular layer. The proof
completes by considering (using the same notation as the proof of Lemma 2) for a.e. time s ≥0, X"
N,0.7616666666666667,"e∈F
we(s)dwe"
N,0.7633333333333333,"dt (s) =
X p∈P"
N,0.765,"1
n n
X"
N,0.7666666666666667,"j=1
dj,p(w(s)) · (xj)p ·
Y"
N,0.7683333333333333,"f∈p
wf(s),
(51)"
N,0.77,for any layer F.
N,0.7716666666666666,"Before continuing with the proof of Theorem 1, we state a classiﬁcation of neurons in a convolutional
layer. Recall that all convolutional layers in this paper is linear. Due to massive weight sharing within
a convolutional layer, there are a lot more neurons and edges than the number of free parameters in
this layer. Here, we formalize some concepts:"
N,0.7733333333333333,"Published as a conference paper at ICLR 2022 a
i u v b
o"
N,0.775,"Figure 4: Jagged path. Here the straight arrow denote a single edge in the graph while the snaked
arrow denote a path with possibly more than one edge. u and v are nodes in the statement of the
second part of Lemma 2, a ∈IN = INv = INu, b ∈OUT = OUTv = OUTu."
N,0.7766666666666666,"Deﬁnition 11 (Convolutional tensors). Free parameters in a (bias-free) convolutional layer can be
organized into a tensor T ∈Ra×b×c where a is the number of input channels, b is the size of each
2D ﬁlter and c is the number of output channels."
N,0.7783333333333333,"For example, in the ﬁrst convolutional layer of AlexNet, the input is an image with 3 image channels
(red, blue and green), so a = 3 in this layer; ﬁlters are of size 11 × 11 so b = 121 is the size of
each 2D ﬁlter; and ﬁnally, there are 96 output channels, so c = 96. Note that changing the stride
and padding does not affect the number of free parameters, but does change the number of neurons
in the computation graph of this layer. Thus, we need the following deﬁnition:
Deﬁnition 12 (Convolutional neuron organization). Fix a convolutional layer with free parame-
ters T ∈Rc×b×a. Then neurons in this layer can be organized into a two dimensional array
{vj,k}j∈[l],k∈[c] for some l ∈N such that"
N,0.78,"1. For all j, j′ ∈[l] and for all k ∈[c], vj,k and vj′,k share all its input weights. More
formally, there exists a bijection φ : INvj,k →INvj′,k such that:"
N,0.7816666666666666,"wuvj,k ≡wφ(u)vj′,k,
(52)"
N,0.7833333333333333,for all u ∈INvjk .
N,0.785,"2. For all j ∈[l] and for all k, k′ ∈[c], vj,k and vj,k′ has the same in-vertices and out-vertices.
In other words,"
N,0.7866666666666666,"INvjk = INvj,k′ =: INvj and OUTvjk = OUTvj,k′ =: OUTvj.
(53)"
N,0.7883333333333333,"For example, in the ﬁrst convolutional of AlexNet in R96×121×3, the input image dimension is
224 × 224 × 3 pixels and the stride of the 11 × 11 × 3 ﬁlters in the layer is 4, with no padding."
N,0.79,"Thus a single 2D ﬁlter traverse the image

224−(11−4)"
N,0.7916666666666666,"4
2
=: l times, each corresponds to a different
neuron in the layer. This process is then repeated c times for each output channel, for a total of c × l
neurons in the convolutional layer.
Lemma 9 (Extension of Lemma 2 to weight sharing). Let V := {vj,k}j∈[l],k∈{1,2} be a convolu-
tional neuron organization. Recall that by deﬁnition 12, for all j ∈[l], INvj,1 = INvj,2 =: INj and
OUTvj,1 = OUTvj,2 =: OUTj. We have for a.e. time t ≥0,
X"
N,0.7933333333333333,"(w,w′)∈wIN(V)
|w(t)w′(t)| −|w(0)w′(0)| =
X"
N,0.795,"(z,z′)∈wOUT(V)
|z(t)z′(t)| −|z(0)z′(0)|,
(54) where"
N,0.7966666666666666,"wIN(V) := {(w1, w2) | ∀j ∈[l], ∃aj ∈INj, wajvj,1 ≡w1 and wajvj,2 ≡w2},"
N,0.7983333333333333,"and similarly,"
N,0.8,"wOUT(V) := {(w1, w2) | ∀j ∈[l], ∃bj ∈OUTj, wvj,1bj ≡w1 and wvj,2bj ≡w2}."
N,0.8016666666666666,Published as a conference paper at ICLR 2022
N,0.8033333333333333,"Before we start the proof, some remarks are in order. Let T1 ∈R2×b×a be a convolutional layer and
let V = {vj,k}j∈[l],k∈[2] be its convolutional neuron organization. Then"
N,0.805,"wIN(V) = {([T1]1,j2,j1, [T1]2,j2,j1) | j1 ∈[a], j2 ∈[b]}.
(55)"
N,0.8066666666666666,"If furthermore T2 ∈Re×d×2 is a convolutional layer immediately after T1, then"
N,0.8083333333333333,"wOUT(V) = {([T2]k1,k2,1, [T2]k1,k2,2) | k1 ∈[e], k2 ∈[d]}.
(56)"
N,0.81,"If instead of T2, the subsequent layer to T1 is a fully-connected layer W ∈Rd×(l×2) (recall that
there are 2l neurons in T1 layer), then"
N,0.8116666666666666,"wOUT(V) = {(Wl1,l2×1, [T2]l1,l2×2) | l1 ∈[d], l2 ∈[l]}.
(57)"
N,0.8133333333333334,"Proof of Lemma 9. As before, the proof is identical to that of Lemma 2 with the exception being the
set of paths that we are double counting over. For all j ∈[l], let"
N,0.815,"Aj := {p1 ∪p2 | p1 is a path from I to vj,1 , p2 is a path from vj,2 to o},
(58)"
N,0.8166666666666667,"and
A′
j := {p1 ∪p2 | p1 is a path from I to vj,2 , p2 is a path from vj,1 to o}.
(59)"
N,0.8183333333333334,Let A := S
N,0.82,"j∈[l] Aj ∪A′
j. Then by an identical argument as that of Lemma 2, we can show that X"
N,0.8216666666666667,"(w,w′)∈wIN(V)
w(t)w′(t) −w(0)w′(0) =
Z [0,t] X p∈A"
N,0.8233333333333334,"1
n n
X"
N,0.825,"j=1
dj,p(w(s))ds
(60) =
X"
N,0.8266666666666667,"(z,z′)∈wOUT(V)
z(t)z′(t) −z(0)z′(0).
(61)"
N,0.8283333333333334,"Here the notation di,j is well-deﬁned since we do not have to specify a path for the subgradient
αp. This is because we are working with linear convolutional layers and thus all subgradients are
gradients and is evaluated to 1."
N,0.83,"Proof of Theorem 1. All points in this theorem admit the same proof technique: First, form the
matrices as instructed. Let X = {v1, v2, . . . , vm} ⊆V be the active neurons shared between the
two layers. Check the conditions of the second part of Lemma 2 and invoke the lemma for each pair
u, v ∈X. We now apply this to each points:"
N,0.8316666666666667,"1. Let W1 ∈Rb×a be a fully-connected layer from neurons in V1 to neurons in V2 and W2 ∈
Rc×b be a fully-connected layer from V2 to V3. Then we have INu = V and OUTu = W
for all u ∈U. Furthermore, all weights around any u are learnable for all u in U. Invoke
the second part of Lemma 2 to get the conclusion."
N,0.8333333333333334,"2. let T1 ∈Rc×b×a and T2 ∈Re×d×c be the convolutional tensors with convolutional neuron
organization of T1 (Deﬁnition 12) being {vj,k}j∈[l1],k∈[c]. Form the matrix representation
W1 ∈Rc×(ab) and W2 ∈R(de)×c as per the theorem statement. By Deﬁnition 12, for
k, k′ ∈[c], for all j ∈[l], INvj,k = INvj,k′ and OUTvj,k = OUTvj,k′. Invoke Lemma 9 to get
the conclusion."
N,0.835,"3. Let r(x; U, Y, Z) be a residual block of either ResNetIdentity, ResNetDiagonal or ResNet-
Free. In all variants, skip connection affects neither the edges in U ∈Rb×a and Y ∈Rc×b.
Let Y be fully-connected from neurons V1 to neurons V2 and U be fully-connected from
neurons V2 to neurons V3. Then for each u ∈V2, INu = V1 and OUTu = V3. Furthermore,
all weights around any vertices in V2 are learnable. Invoke the second part of Lemma 2 to
get the conclusion."
N,0.8366666666666667,Published as a conference paper at ICLR 2022
N,0.8383333333333334,"4. Let ri(x; Ui, Yi, Zi), i ∈{1, 2} be consecutive ResNetFree block. Let Y1 be fully con-
nected from neurons V1 to neurons V2, U1 be fully-connected from V2 to neurons V3, Y2 be
fully-connected from neurons V3 to V4 and U2 be fully-connected from V4 to neurons V5."
N,0.84,"Then
U1
Z1

is fully-connected from V1 ∪V2 to V3 and

Y2
Z2"
N,0.8416666666666667,"
is fully-connected from"
N,0.8433333333333334,V3 to V4 ∪V5. Invoke the ﬁrst point to get the conclusion.
N,0.845,"5. Let the convolutional tensor be T ∈Rc×b×a with convolutional neuron organization
{vj,k}j∈[l],k∈[c] for some l ∈N. Let the adjacent fully-connected layer be W ∈Rd×(l×c).
Form the matrix representation W1 ∈Rc×ab and W2 ∈Rdl×c as per the theorem state-
ment. Then we have for any k, k′ ∈[c] and for all j ∈[l], INvj,k = INvj,k′ =: INj and"
N,0.8466666666666667,"OUTvj,k = OUTvj,k′ =: OUTj. Invoke Lemma 9 to get the conclusion."
N,0.8483333333333334,"6. Let r(x; U, Y, Z) be a residual block of either ResNetIdentity, ResNetDiagonal or ResNet-
Free. Let Y be fully-connected from neurons V1 to neurons V2, U be fully-connected from
neurons V2 to neurons V3. Thus, Z is fully-connected from V1 to V3. Then W2 =
U
Z"
N,0.85,is fully connected from V1 to V2 ∪V3.We invoke the ﬁfth point to get the conclusion.
N,0.8516666666666667,"7. ﬁrst consider the case where the ResNetFree block is followed by the fully-connected layer.
Let r(x; U, Y, Z) be the ﬁrst ResNetFree block with input neurons where Y fully-connects
neurons V1 to V2 and U fully connects V2 to V3. Then we have
U
Z
fully-connects
V1∪V2 to V3. If the subsequent layer is a fully-connected layer then invoke the ﬁrst point to
get the conclusion; otherwise if the subsequent layer is a ResNetFree block r(x; U ′, Y ′, Z′)"
N,0.8533333333333334,"with Y ′ fully-connects V3 to V4 and U ′ fully-connects V4 to V5. Then

Y
Z"
N,0.855,"
fully-connects"
N,0.8566666666666667,V3 to V4 ∪V5 and we one again invoke the ﬁrst point to get the conclusion.
N,0.8583333333333333,"Proof of Lemma 4. This is the continuation of the proof of Lemma 2. To obtain noninvariance, one
only needs to show that when u is active and v inactive, the expression in 46 and 49 are not equal in
general. For the sake of notation, we pick the case where the preactivation of u is strictly positive,
that of v is strictly negative, and further assume that the whole network is differentiable at the current
weights w for all training examples."
N,0.86,"In this case, it is not hard to see that the Clarke subdifferential ∂wµ(h) is a singleton and contains
the gradient of the µ network. Furthermore, for any path p = (v1, . . . , vL), the partial derivative ∂µ"
N,0.8616666666666667,"∂p
is 1 if all neurons on p are active and 0 otherwise. Thus, we have d
dt  X"
N,0.8633333333333333,"a∈IN
wauwav "
N,0.865,"
(62) = n
X"
N,0.8666666666666667,"j=1
−ℓ′(yν(xj; w)) · y ·
X a∈IN X b∈OUT X"
N,0.8683333333333333,active p1∈PI→a X
N,0.87,active p2∈Pb→o 
N,0.8716666666666667,"(xj)p1 ·
Y"
N,0.8733333333333333,"f∈p1∪p2
wf "
N,0.875,wav · wub· (63)
N,0.8766666666666667,"We can actually factorize this even further by noticing that the term wav does not depend on b and
wub does not depend on a. Rearranging the sum and factorizes give: d
dt  X"
N,0.8783333333333333,"a∈IN
wauwav "
N,0.88,"
(64) = n
X"
N,0.8816666666666667,"j=1
−ℓ′(yν(xj; w)) · y ·  
X"
N,0.8833333333333333,"active p1∈PI→v
(xj)p1 ·
Y"
N,0.885,"f∈p1
wf    
X"
N,0.8866666666666667,active p2∈Pu→o Y
N,0.8883333333333333,"f∈p2
wf "
N,0.89,".
(65)"
N,0.8916666666666667,Published as a conference paper at ICLR 2022
N,0.8933333333333333,"On the other hand d
dt  X"
N,0.895,"b∈OUT
wubwvb "
N,0.8966666666666666,"
(66) = n
X"
N,0.8983333333333333,"j=1
−ℓ′(yν(xj; w)) · y ·  
X"
N,0.9,"active p1∈PI→u
(xj)p1 ·
Y"
N,0.9016666666666666,"f∈p1
wf    
X"
N,0.9033333333333333,active p2∈Pv→o Y
N,0.905,"f∈p2
wf "
N,0.9066666666666666,".
(67)"
N,0.9083333333333333,"Take, for example, an asymmetric case where the in-edges of v has much larger weights than that of
u while out-edges of v has much smaller weights than that of u, then 65 is much larger than 67 and
therefore the two expressions are not equal in the general case. A symmetric initialization scheme
that prevents the above asymmetry may prevent this from happens, but this requires additional as-
sumptions and is opened to future work."
N,0.91,"Remark 1. Using the automatic differentiation framework while setting the gradient of ReLU to be
1 if the preactivation is nonnegative while 0 otherwise, the same derivation of 65 can be achieved.
Interestingly, if one only has a single example, then the ﬁnal expression of 65 implies that the matrix
d
dt
 
W ⊤
k Wk

has rank at most 2, where Wk is a weight matrix in, for example, ReLU fully-connected
neural network. Controlling the eigenvalues under low-rank updates may allow us to bound singular
values of the full weight matrices Wk. The resulting bound would not be uniform over time, but
improves with training and is thus a different kind of low rank result. This, however, is outside of the
scope of this paper."
N,0.9116666666666666,"D
PROOF OF THEOREM 2"
N,0.9133333333333333,First we state a helper lemma
N,0.915,"Lemma 10 (Largest singular value of different ﬂattening of the same tensor is close). Let T ∈Ra,b,c
be an order 3 tensor (say a convolutional weight tensor). Let T1, T2 be the standard ﬂattening of
this tensor into an element in Rc×(a×b) and R(b×c)×a respectively. Then,"
N,0.9166666666666666,"1
min(a, b)∥T1∥2
2 ≤∥T2∥2
2.
(68)"
N,0.9183333333333333,"Proof. Invoke Theorem 4.8 of Wang et al. (2017) and using the same notation in the same paper, we
have for π1 = {{c}, {a, b}} and π2 = {{b, c}, {a}},"
N,0.92,"dimT (π1, π2)"
N,0.9216666666666666,"dim(T)
∥T1∥2
2 ≤∥T2∥2
2.
(69)"
N,0.9233333333333333,"All that is left is to compute the left hand side in term of a, b, c. By deﬁnition, dim(T) = abc and"
N,0.925,"dimT (π1, π2) = dimT ({{c}, {a, b}}, {{b, c}, {a}})
(70)"
N,0.9266666666666666,"=
h
max
 
DT ({c}, {b, c}), DT ({c}, {a}
i
(71)"
N,0.9283333333333333,"·
h
max
 
DT ({a, b}, {b, c}), DT ({a, b}, {a})
i
(72)"
N,0.93,"=

max(c, 0)

·

max(a, b)

= c max(a, b).
(73)"
N,0.9316666666666666,Plug this in equation 69 to get the ﬁnal result.
N,0.9333333333333333,"Lemma 11 (Shufﬂing layer in linear ResNetFree block preserves largest singular value up to multi-
ple by 8). Recall that for a ResNetFree block r(U, Y, Z) with Y ∈Rb×a, U ∈Rc×b and Z ∈Rc×a,"
N,0.935,"there are two possible rearrangement of the weights A =
U
Z
and B =

Y
Z"
N,0.9366666666666666,"
. We have"
N,0.9383333333333334,"∥B∥2
2 ≥1"
N,0.94,"8∥A∥2
2 −D′ where D′ ≥0 is ﬁxed at inialization."
N,0.9416666666666667,Published as a conference paper at ICLR 2022
N,0.9433333333333334,"Proof. Recall that by point three of Theorem 1, we have matrix invariance"
N,0.945,U ⊤(t)U(t) −Y (t)Y ⊤(t) = U ⊤(0)U(0) −Y (0)Y ⊤(0).
N,0.9466666666666667,"Note that we can obtain this form since we are only considering linear ResNetFree blocks, so all
neurons are active at all time for all training examples. Thus, we can invoke a FTC to get this form
from the differential from in theorem 1."
N,0.9483333333333334,"By line B.2 in Ji & Telgarsky (2019),"
N,0.95,"∥Y ∥2
2 ≥∥U∥2
2 −D,
(74)"
N,0.9516666666666667,"where D = ∥U ⊤(0)U(0) −Y (0)Y ⊤(0)∥2
2 is ﬁxed at initialization."
N,0.9533333333333334,"For positive semideﬁnite matrix X, denote λ(X) to be the function that returns the maximum eigen-
value of X. We have"
N,0.955,"∥B∥2
2 = (λ(B⊤B))2
(75)"
N,0.9566666666666667,"=

λ

Y ⊤Y + Z⊤Z
2
(76) ≥1 4"
N,0.9583333333333334,"
λ

Y ⊤Y

+ λ

Z⊤Z
2
(77) ≥1 4"
N,0.96,"
λ

Y ⊤Y
2
+ 1 4"
N,0.9616666666666667,"
λ

Z⊤Z
2
(78) ≥1 4"
N,0.9633333333333334,"
λ

UU ⊤2
+ 1 4"
N,0.965,"
λ

ZZ⊤2
−D"
N,0.9666666666666667,"4
(79) ≥1 8"
N,0.9683333333333334,"
λ

UU ⊤
+ λ

ZZ⊤2
−D"
N,0.97,"4
(80) ≥1 8"
N,0.9716666666666667,"
λ

UU ⊤+ ZZ⊤2
−D"
N,0.9733333333333334,"4
(81) = 1 8"
N,0.975,"
λ

AA⊤2
−D 4 = 1"
N,0.9766666666666667,"8∥A∥2
2 −D"
N,0.9783333333333334,"2 ,
(82)"
N,0.98,"where 77 is by an application of Weyl’s inequality for Hermitian matrices which states that λ(C +
D) ≥λ(C) + t where t is is the smallest eigenvalue of D, which is nonnegative since matrices here
are all positive semideﬁnite; 79 is a consequence of 74 and 81 is the application of the inequality
λ(C + D) ≤λ(C) + λ(D) which is another of Weyl’s inequality for Hermitian matrices."
N,0.9816666666666667,"Proof of Theorem 2. Fix j ∈[K + M], we ﬁrst invoke Lemma 3 to bound the Frobenius norm of
each of the ﬁnal K + M + 1 layers (counting the last layer Fin) via the last layer. Let Wj be the
matrix representation of the j-th layer among the last M + 1 layer as described in Theorem 1. Note
that even if a layer has more than one matrix representation in Theorem 1, their Frobenius norm is
still the same because different representation merely re-organize the weights. Thus, we can pick
an arbitrary representation in this step. However, the same is not true for the operator norm and we
have to be a lot more careful in the next step. For each j ∈[K + M], we have"
N,0.9833333333333333,"∥Wj(t)∥2
F −∥Fin(t)∥2
F = D0,
(83)"
N,0.985,"where D0 = ∥Wj(0)∥2
F −∥Fin(0)∥2
F ﬁxed at initialization."
N,0.9866666666666667,"Now, we bound the difference between the operator norm of Wj(t) and Fin(t) by a telescoping
argument. By Lemma 11, switching from one matrix representation to the other for ResNet incurs
at most a multiplicative factor of 8 and an additive factor cost that depends only on the initialization.
In each adjacent layer, the maximum number of switch between representation is one (so that it ﬁts
the form prescribed in Theorem 1). By matrix invariance between adjacent layers of the K + M
pairs of adjacent layers,
∥Wl(t)∥2
2 ≥C∥Wl+1(t)∥2
2 −Dl,
(84)"
N,0.9883333333333333,Published as a conference paper at ICLR 2022
N,0.99,"for C = 1/8 if the k + 1 layer is a ResNetFree block (Lemma 11), C = 1/ min(ak+1, bk+1) if the
k + 1 layer is a convolutional layer, C = max dim W [M+1] if k = M (Lemma 10) and C = 1
otherwise; for Dl = ∥W ⊤
l+1(0)Wl+1(0) −Wl(0)W ⊤
l (0)∥2
2."
N,0.9916666666666667,"Telescope the sum and subtract from 83 to get the statement of the theorem. When Frobenius norm
diverges, divide both sides by the Frobenius norm to get the ratio statement. Note that the bound
∥W∥2
F /rank(W) ≤∥W∥2
2 is trivial since the largest singular value squared is at least the average
singular value squared."
N,0.9933333333333333,"When the lower bound of 14 is 1, it matches the upper bound and thus the largest singular value
dominates all other singular values. Alignment follows from the proof of Lemma 2.6 second point
in Ji & Telgarsky (2019)."
N,0.995,"E
PROOF OF COROLLARY 1"
N,0.9966666666666667,"Proof. Let L be the number of layers in the network. Under the conditions stated in Corollary 1,
Lyu & Li (2020) and Ji & Telgarsky (2020) showed that ∥w(t)∥2 diverges. Invoke Lemma 3 for all
k ∈[L −1] and sum up the results, we have L∥WL(t)∥2
F = D′′ + PL
j=1 ∥Wj(t)∥2
F ∥Wk∥2
F =
D′′ + ∥w(t)∥2
2 where D′′ is constant in t. Thus ∥Wj(t)∥2
F diverges for all j ∈[L]. Since the sum of
all but the largest singular value is bounded, but the sum of all singular values diverge, we conclude
that the largest singular value eventually dominates the remaining singular values. Together with
convergence in direction for these architecture Ji & Telgarsky (2020), we have each matrix converges
to its rank-1 approximation."
N,0.9983333333333333,"That all the feedforward neural networks with ReLU/leaky ReLU/linear activations can be deﬁnable
in the same o-minimal structure that contains the exponential function follows from the work of Ji
& Telgarsky (2020) and that deﬁnability is closed under function composition."
