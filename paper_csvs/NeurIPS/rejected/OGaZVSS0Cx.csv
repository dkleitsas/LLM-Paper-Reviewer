Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0030211480362537764,"We present the first mini-batch kernel k-means algorithm. Our algorithm achieves
an order of magnitude improvement in running time compared to the full batch
algorithm, with only a minor negative effect on the quality of the solution. Specifi-
cally, a single iteration of our algorithm requires only O(n(k + b)) time, compared
to O(n2) for the full batch kernel k-means, where n is the size of the dataset and b
is the batch size.
We provide a theoretical analysis for our algorithm with an early stopping condition
and show that if the batch is of size Ω((γ/ϵ)2 log(nγ/ϵ)), the algorithm must
terminate within O(γ2/ϵ) iterations with high probability, where γ is the bound on
the norm of points in the dataset in feature space, and ϵ is a threshold parameter for
termination. Our results hold for any reasonable initialization of centers. When the
algorithm is initialized with the k-means++ initialization scheme, it achieves an
approximation ratio of O(log k).
Many popular kernels are normalized (e.g., Gaussian, Laplacian), which implies
γ = 1. For these kernels, taking ϵ to be a constant and b = Θ(log n), our algorithm
terminates within O(1) iterations where each iteration takes time O(n(log n+k))."
INTRODUCTION,0.006042296072507553,"1
Introduction"
INTRODUCTION,0.00906344410876133,"Mini-batch methods are among the most successful tools for handling huge datasets for machine
learning. Notable examples include Stochastic Gradient Descent (SGD) and mini-batch k-means
[27]. Mini-batch k-means [27] is one of the most popular clustering algorithms used in practice [21]."
INTRODUCTION,0.012084592145015106,"While k-means is widely used due to it’s simplicity and fast running time, it requires the data to be
linearly separable to achieve meaningful clustering. Unfortunately, many real-world datasets do not
have this property. One way to overcome this problem is to project the data into a high, even infinite,
dimensional space (where it is hopefully linearly separable) and run k-means on the projected data
using the “kernel-trick”."
INTRODUCTION,0.015105740181268883,"Kernel k-means achieves significantly better clustering compared to k-means in practice. However,
its running time is considerably slower. Surprisingly, prior to our work there was no attempt to speed
up kernel k-means using a mini-batch approach."
INTRODUCTION,0.01812688821752266,"Problem statement
We are given an input (dataset), X = {xi}n
i=1, of size n and a parameter k
representing the number of clusters. A kernel for X is a function K : X × X →R that can be
realized by inner products. That is, there exists a Hilbert space H and a map ϕ : X →H such that
∀x, y ∈X, ⟨ϕ(x), ϕ(y)⟩= K(x, y). We call H the feature space and ϕ the feature map."
INTRODUCTION,0.021148036253776436,"In kernel k-means the input is a dataset X and a kernel function K as above. Our goal is to find a set
C of k centers (elements in H) such that the following goal function is minimized:"
N,0.02416918429003021,"1
n X"
N,0.027190332326283987,"x∈X
min
c∈C ∥c −ϕ(x)∥2."
N,0.030211480362537766,"Equivalently we may ask for a partition of X into k parts, keeping C implicit.1"
N,0.03323262839879154,"Lloyd’s algorithm
The most popular algorithm for (non kernel) k-means is Lloyd’s algorithm,
often referred to as the k-means algorithm [17]. It works by randomly initializing a set of k centers
and performing the following two steps: (1) Assign every point in X to the center closest to it. (2)
Update every center to be the mean of the points assigned to it. The algorithm terminates when no
point is reassigned to a new center. This algorithm is extremely fast in practice but has a worst-case
exponential running time [3, 30]."
N,0.03625377643504532,"Mini-batch k-means
To update the centers, Lloyd’s algorithm must go over the entire input at
every iteration. This can be computationally expensive when the input data is extremely large. To
tackle this, the mini-batch k-means method was introduced by [27]. It is similar to Lloyd’s algorithm
except that steps (1) and (2) are performed on a batch of b elements sampled uniformly at random
with repetitions, and in step (2) the centers are updated slightly differently. Specifically, every center
is updated to be the weighted average of its current value and the mean of the points (in the batch)
assigned to it. The parameter by which we weigh these values is called the learning rate, and its
value differs between centers and iterations. The larger the learning rate, the more a center will drift
towards the new batch cluster mean."
N,0.03927492447129909,"Lloyd’s algorithm in feature space
Implementing Lloyd’s algorithm in feature space is challenging
as we cannot explicitly keep the set of centers C. Luckily, we can use the kernel function together
with the fact that centers are always set to be the mean of cluster points to compute the distance from
any point x ∈X in feature space to any center c =
1
|A|
P"
N,0.04229607250755287,y∈A ϕ(y) as follows:
N,0.045317220543806644,"∥ϕ(x) −c∥2 = ⟨ϕ(x) −c, ϕ(x) −c⟩= ⟨ϕ(x), ϕ(x)⟩−2⟨ϕ(x), c⟩+ ⟨c, c⟩"
N,0.04833836858006042,"= ⟨ϕ(x), ϕ(x)⟩−2⟨ϕ(x), 1 |A| X"
N,0.0513595166163142,"y∈A
ϕ(y)⟩+ ⟨1 |A| X"
N,0.054380664652567974,"y∈A
ϕ(y), 1 |A| X"
N,0.05740181268882175,"y∈A
ϕ(y)⟩,"
N,0.06042296072507553,"where A can be any subset of the input X. While the above can be computed using only kernel
evaluations, it makes the update step significantly more costly than standard k-means. Specifically,
the complexity of the above may be quadratic in n [9]."
N,0.0634441087613293,"Mini-batch kernel k-means
Applying the mini-batch approach for kernel k-means is even more
difficult because the assumption that cluster centers are always the mean of some subset of X in
feature space no longer holds."
N,0.06646525679758308,"In Section 4 we present our algorithm and derive a recursive expression that allows us to compute the
distances of all points to current cluster centers (in feature space). Our algorithm implements this by
updating a data structure that maintains the inner products between the data and centers in feature
space. This means the running time of each iteration of our algorithm is only O(n(b + k)) compared
to O(n2) for the full-batch algorithm."
N,0.06948640483383686,"In Section 5 we go on to provide theoretical guarantees for our algorithm. This is somewhat tricky for
mini-batch algorithms due to their stochastic nature, as they may not even converge to a local-minima.
To overcome this hurdle, we take the approach of [26] and answer the question: how long does it take
mini-batch kernel k-means to terminate with an early stopping condition. Specifically, we terminate
the algorithm when the improvement on the batch drops below some user provided parameter, ϵ.
Early stopping conditions are very common in practice (e.g., sklearn[21])."
N,0.07250755287009064,"Applying the k-means++ initialization scheme for our initial centers implies we achieve the same ap-
proximation ratio, O(log k) in expectation, as the full-batch algorithm. The approximation guarantee
of k-means++ is guaranteed already in the initialization phase (Theorem 3.1 in [4]), and the execution
of Lloyd’s algorithm following initialization can only improve the solution. We show that w.h.p2 the
global goal function is decreasing throughout our execution which implies that the approximation
guarantee remains the same."
N,0.0755287009063444,"1A common variant of the above is when every x ∈X is assigned a weight wx ∈R+ and we aim to minimize
P"
N,0.07854984894259819,"x∈X wx · minc∈C ∥c −ϕ(x)∥2. Everything that follows, including our results, can be easily generalized to
the weighted case. We present the unweighted case to improve readability.
2This is usually taken to be 1 −1/np for some constant p ≥1. For our case, it holds that p = 1, however,
this can be amplified arbitrarily by increasing the batch size by a multiplicative constant factor."
N,0.08157099697885196,"While our general approach is similar to [26], we must deal with the fact that H may have an infinite
dimension. The guarantees of [26] depend on the dimension of the space in which k-means is
executed, which is unacceptable in our case. We overcome this by parameterizing our results by a new
parameter γ = maxx∈X ∥ϕ(x)∥. We note that for normalized kernels, such as the popular Gaussian
and Laplacian kernels, it holds that γ = 1. We show that if the batch size is Ω((γ/ϵ)2 log(nγ/ϵ))
then w.h.p. our algorithm terminates in O(γ2/ϵ) iterations. Our theoretical results are summarised in
Theorem 1 (where Algorithm 1 is presented in Section 4)."
N,0.08459214501510574,Theorem 1. The following holds for Algorithm 1:
N,0.08761329305135952,"1. Each iteration takes O(n(b + k)) time,"
N,0.09063444108761329,"2. If b = Ω((γ/ϵ)2 log(nγ/ϵ)) then it terminates in O(γ2/ϵ) iterations with high probability,"
N,0.09365558912386707,3. When initialized with k-means++ it achieve a O(log k) approximation ratio in expectation.
N,0.09667673716012085,"Our result improves upon [26] significantly when a normalized kernel is used since Theorem 1 doesn’t
depend on the input dimension. Our algorithm copes better with non linearly separable data and
requires a smaller batch size ( eO(1/ϵ2) vs eO((d/ϵ)2)))3 for normalized kernels. This is particularly
apparent with high dimensional datasets such as MNIST[16] where the dimension squared is already
nearly ten times the number of datapoints."
N,0.09969788519637462,"The learning rate we use, suggested in [26], differs from the standard learning rate of sklearn in that
it does not go to 0 over time. Unfortunately, this new learning rate is non-standard and [26] did not
present experiments comparing their learning rate to that of sklearn."
N,0.1027190332326284,"In Section 6 we evaluate our results experimentally both with the learning rate of [26] and that of
sklearn. We also fill the experimental gap left in [26] by evaluating (non-kernel) mini-batch k-means
with their new learning rate compared to that of sklearn. To allow a fair empirical comparison, we
run each algorithm for a fixed number of iterations without stopping conditions. Our results are as
follows:"
N,0.10574018126888217,"• Mini-batch kernel k-means is significantly faster than full-batch kernel k-means, while achieving
solutions of similar quality, which are superior to the non-kernel version."
N,0.10876132930513595,"• The learning rate of [26] results in solutions with better quality both for mini-batch kernel k-means
and mini-batch k-means."
RELATED WORK,0.11178247734138973,"2
Related work"
RELATED WORK,0.1148036253776435,"Until recently, mini-batch k-means was only considered with a learning rate going to 0 over time.
This was true both in theory [29, 27] and practice [21]. Recently, [26] proposed a new learning which
does not go to 0 over time, and showed that if the batch is of size ˜Ω((d/ϵ)2), mini-batch k-means
must terminate within O(d/ϵ) iterations with high probability, where d is the dimension of the input,
and ϵ is a threshold parameter for termination."
RELATED WORK,0.11782477341389729,"A popular approach to deal with the slow running time of kernel k-means is constructing a coreset of
the data. A coreset for kernel k-means is a weighted subset of X with the guarantee that the solution
quality on the coreset is close to that on the entire dataset up to a (1 + ϵ) multiplicative factor. There
has been a long line of work on coresets for k-means an kernel k-means [25, 10, 5], and the current
state-of-the-art for kernel k-means is due to [13]. They present a coreset algorithm with a nearly
linear (in n and k) construction time which outputs a coreset of size poly(kϵ−1)."
RELATED WORK,0.12084592145015106,"In [7] the authors only compute the kernel matrix for uniformly sampled set of m points from
X. Then they optimize a variant of kernel k-means where the centers are constrained to be linear
combinations of the sampled points. The authors do no provide worst case guarantees for the running
time or approximation of their algorithm."
RELATED WORK,0.12386706948640483,"Another approach to speed up kernel k-means is by computing an approximation for the kernel
matrix. This can be done by computing a low dimensional approximation for ϕ (without computing
ϕ explicitly)[23, 8, 6], or by computing a low rank approximation for the kernel matrix [19, 31]."
RELATED WORK,0.1268882175226586,"3Where eO hides factors that are logarithmic in d, n, 1/ϵ."
RELATED WORK,0.1299093655589124,"Kernel sparsification techniques construct sparse approximations of the full kernel matrix in sub-
quadratic time. For smooth kernel functions such as the polynomial kernel, [22] presents an algorithm
for constructing a (1 + ϵ)-spectral sparsifier for the full kernel matrix with a nearly linear number of
non-zero entries in nearly linear time. For the gaussian kernel, [18] show how to construct a weaker,
cluster preserving sparsifier using a nearly linear number of kernel density estimation querries."
RELATED WORK,0.13293051359516617,"We note that our results are complementary to coresets, dimensionality reduction, and kernel sparsifi-
cation, in the sense that we can compose our method with these techniques."
PRELIMINARIES,0.13595166163141995,"3
Preliminaries"
PRELIMINARIES,0.13897280966767372,"Throughout this paper we work with ordered tuples rather than sets, denoted as Y = (yi)i∈[ℓ], where
[ℓ] = {1, . . . , ℓ}. To reference the i-th element we either write yi or Y [i]. It will be useful to use set
notations for tuples such as x ∈Y ⇐⇒∃i ∈[ℓ], x = yi and Y ⊆Z ⇐⇒∀i ∈[ℓ], yi ∈Z. When
summing we often write P
x∈Y g(x) which is equivalent to Pℓ
i=1 g(Y [i])."
PRELIMINARIES,0.1419939577039275,"We borrow the following notation from [14] and generalize it to Hilbert spaces. For every x, y ∈H
let ∆(x, y) = ∥x −y∥2. We slightly abuse notation and and also write ∆(x, y) = ∥ϕ(x) −ϕ(y)∥2
when x, y ∈X and ∆(x, y) = ∥ϕ(x) −y∥2 when x ∈X, y ∈H (similarly when x ∈H, y ∈X).
For every finite tuple S ⊆X and a vector x ∈H let ∆(S, x) = P"
PRELIMINARIES,0.14501510574018128,"y∈S ∆(y, x). Let us denote
γ = maxx∈X ∥ϕ(x)∥. Let us define for any finite tuple S ⊆X the center of mass of the tuple as
cm(S) =
1
|S|
P"
PRELIMINARIES,0.14803625377643503,x∈S ϕ(x).
PRELIMINARIES,0.1510574018126888,We now state the kernel k-means problem using the above notation.
PRELIMINARIES,0.1540785498489426,"Kernel k-means
We are given an input X = (xi)n
i=1 and a parameter k. Our goal is to (im-
plicitly) find a tuple C ⊆H of k centers such that the following goal function is minimized:
1
n
P"
PRELIMINARIES,0.15709969788519637,"x∈X minC∈C ∆(x, C)."
PRELIMINARIES,0.16012084592145015,"Let us define for every x ∈X the function fx : Hk →R where fx(C) = minC∈C ∆(x, C). We can
treat Hk as the set of k-tuples of vectors in H. We also define the following function for every tuple
A = (ai)ℓ
i=1 ⊆X: fA(C) = 1"
PRELIMINARIES,0.16314199395770393,"ℓ
Pℓ
i=1 fai(C). Note that fX is our original goal function."
PRELIMINARIES,0.1661631419939577,We extensive use of the notion of convex combination:
PRELIMINARIES,0.1691842900302115,Definition 2. We say that y ∈H is a convex combination of X if y = P
PRELIMINARIES,0.17220543806646527,"x∈X pxϕ(x), such that
∀x ∈X, px ≥0 and P"
PRELIMINARIES,0.17522658610271905,x∈X px = 1.
OUR ALGORITHM,0.1782477341389728,"4
Our Algorithm"
OUR ALGORITHM,0.18126888217522658,"We present our pseudo-code as Algorithm 1. It requires an initial set of cluster centers such that every
center is a convex combination of X. This guarantees that all subsequent centers are also a convex
combination of X. Note that if we initialize the centers using the kernel version of k-means++, this
is indeed the case."
OUR ALGORITHM,0.18429003021148035,"Algorithm 1 proceeds by repeatedly sampling a batch of size b (the batch size is a parameter). For
the i-th batch the algorithm (implicitly) updates the centers using the learning rate αi
j for center
j. Note that the learning rate may take on different values for different centers, and may change
between iterations. Finally, the algorithm terminates when the progress on the batch is below ϵ, a
user provided parameter. While our termination guarantees (Section 5) require a specific learning
rate, it does not affect the running time of a single iteration, and we leave it as a parameter for now."
OUR ALGORITHM,0.18731117824773413,"Recursive distance update rule
While for (non kernel) k-means the center updates and assignment
of points to clusters is straightforward, this is tricky for kernel k-means and even harder for mini-batch
kernel k-means. Specifically, how do we overcome the challenge that we do not maintain the centers
explicitly?"
OUR ALGORITHM,0.1903323262839879,"To assign points to centers in the (i + 1)-th iteration, it is sufficient to know ∥ϕ(x) −Cj
i+1∥2 for every
j. If we can keep track of this quantity through the execution of the algorithm, we are done."
OUR ALGORITHM,0.1933534743202417,Algorithm 1: Mini-batch kernel k-means with early stopping
OUR ALGORITHM,0.19637462235649547,1 Input:
OUR ALGORITHM,0.19939577039274925,"• Dataset X = (xi)n
i=1, batch size b, early stopping parameter ϵ"
OUR ALGORITHM,0.20241691842900303,"• Initial centers (Cj
1)k
j=1 where Cj
1 is a convex combination of X for all j ∈[k]"
OUR ALGORITHM,0.2054380664652568,2 for i = 1 to ∞do
OUR ALGORITHM,0.2084592145015106,"3
Sample b elements, Bi = (y1, . . . , yb), uniformly at random from X (with repetitions)"
OUR ALGORITHM,0.21148036253776434,"4
for j = 1 to k do"
BJ,0.21450151057401812,"5
Bj
i =

x ∈Bi | arg minℓ∈[k] ∆(x, Cℓ
i ) = j"
BJ,0.2175226586102719,"6
αj
i is the learning rate for the j-th cluster for iteration i"
CJ,0.22054380664652568,"7
Cj
i+1 = (1 −αj
i)Cj
i + αj
icm(Bj
i )"
CJ,0.22356495468277945,"8
if fBi(Ci+1) −fBi(Ci) < ϵ then Return Ci+1"
CJ,0.22658610271903323,Let us derive a recursive expression for the distances
CJ,0.229607250755287,"∥ϕ(x) −Cj
i+1∥2 = ⟨ϕ(x), ϕ(x)⟩−2⟨ϕ(x), Cj
i+1⟩+ ⟨Cj
i+1, Cj
i+1⟩."
CJ,0.2326283987915408,"We first expand ⟨ϕ(x), Cj
i+1⟩,"
CJ,0.23564954682779457,"⟨ϕ(x), Cj
i+1⟩= ⟨ϕ(x), (1 −αj
i)Cj
i + αj
icm(Bj
i )⟩= (1 −αj
i)⟨ϕ(x), Cj
i ⟩+ αj
i⟨ϕ(x), cm(Bj
i )⟩."
CJ,0.23867069486404835,"Then we expand ⟨Cj
i+1, Cj
i+1⟩,"
CJ,0.24169184290030213,"⟨Cj
i+1, Cj
i+1⟩= ⟨(1 −αj
i)Cj
i + αj
icm(Bj
i ), (1 −αj
i)Cj
i + αj
icm(Bj
i )⟩"
CJ,0.24471299093655588,"= (1 −αj
i)2⟨Cj
i , Cj
i ⟩+ 2αj
i(1 −αj
i)⟨Cj
i , cm(Bj
i )⟩+ (αj
i)2⟨cm(Bj
i ), cm(Bj
i )⟩."
CJ,0.24773413897280966,"Assuming that ⟨Cj
i , Cj
i ⟩and ⟨ϕ(x), Cj
i ⟩are known for all j ∈[k] and for all x ∈X, we can compute
⟨Cj
i+1, Cj
i+1⟩and ⟨ϕ(x), Cj
i+1⟩for all j ∈[k] and x ∈X, which implies we can compute the distances
from any point in the batch to all centers."
CJ,0.25075528700906347,"We now bound the running time of a single iteration of the outer loop in Algorithm 1. Let us denote
bj
i =
Bj
i
 and recall that cm(Bj
i ) =
1
bj
i
P"
CJ,0.2537764350453172,"y∈Bj
i ϕ(y). Therefore, computing ⟨ϕ(x), cm(Bj
i )⟩="
"BJ
I
P",0.256797583081571,"1
bj
i
P"
"BJ
I
P",0.2598187311178248,"y∈Bj
i ⟨ϕ(x), ϕ(y)⟩requires O(bj
i) time. Similarly, computing ⟨cm(Bj
i ), cm(Bj
i )⟩requires"
"BJ
I
P",0.2628398791540785,"O((bj
i)2) time. Let us now bound the time it requires to compute ⟨ϕ(x), Cj
i+1⟩and ⟨Cj
i+1, Cj
i+1⟩."
"BJ
I
P",0.26586102719033233,"Assuming we know ⟨ϕ(x), Cj
i ⟩and ⟨Cj
i , Cj
i ⟩, updating ⟨ϕ(x), Cj
i+1⟩for all x ∈X, j ∈[k] requires
O(n(b + k)) time. Specifically, the ⟨ϕ(x), Cj
i ⟩term is already known from the previous iteration and
we need to compute αj
i⟨ϕ(x), cm(Bj
i )⟩for every x ∈X, j ∈[k] which requires n P"
"BJ
I
P",0.2688821752265861,"j∈[k] bj
i = nb"
"BJ
I
P",0.2719033232628399,"time. Finally, updating ⟨ϕ(x), Cj
i+1⟩for all x ∈X, j ∈[k] requires O(nk) time."
"BJ
I
P",0.27492447129909364,"Updating ⟨Cj
i+1, Cj
i+1⟩requires O(b2 + kb) time. Specifically, ⟨Cj
i , Cj
i ⟩is known from the previous
iteration and computing ⟨cm(Bj
i ), cm(Bj
i )⟩for all j ∈[k] requires O(P"
"BJ
I
P",0.27794561933534745,"j∈[k](bj
i)2) = O(b2) time."
"BJ
I
P",0.2809667673716012,"Computing ⟨Cj
i , cm(Bj
i )⟩for all j ∈[k] requires time O(b) using ⟨ϕ(x), Cj
i ⟩from the previous
iteration. Therefore, the total running time of the update step (assigning points to new centers) is
O(n(b + k)). To perform the update at the (i + 1)-th step we only need ⟨ϕ(x), Cj
i ⟩, ⟨Cj
i , Cj
i ⟩, which
results in a space complexity of O(nk). This completes the first claim of Theorem 1."
TERMINATION GUARANTEE,0.283987915407855,"5
Termination guarantee"
TERMINATION GUARANTEE,0.28700906344410876,"Section preliminaries
We introduce the following definitions and lemmas to aid our proof of the
second claim of Theorem 1.
Lemma 3. For every y which is a convex combination of X it holds that ∥y∥≤γ."
TERMINATION GUARANTEE,0.29003021148036257,Proof. The proof follows by a simple application of the triangle inequality:
TERMINATION GUARANTEE,0.2930513595166163,"∥y∥= ∥
X"
TERMINATION GUARANTEE,0.29607250755287007,"x∈X
pxϕ(x)∥≤
X"
TERMINATION GUARANTEE,0.2990936555891239,"x∈X
∥pxϕ(x)∥=
X"
TERMINATION GUARANTEE,0.3021148036253776,"x∈X
px∥ϕ(x)∥≤
X"
TERMINATION GUARANTEE,0.30513595166163143,"x∈X
pxγ = γ."
TERMINATION GUARANTEE,0.3081570996978852,"Lemma 4. For any tuple of k centers C ⊂Hd which are a convex combination of points in X, it
holds that ∀A ⊆X, fA(C) ≤4γ2."
TERMINATION GUARANTEE,0.311178247734139,"Proof. It is sufficient to upper bound fx. Combining that fact that every C ∈C is a convex
combination of X with the triangle inequality, we have that"
TERMINATION GUARANTEE,0.31419939577039274,"∀x ∈X, fx(C) ≤max
C∈C ∆(x, C) = ∆(x,
X"
TERMINATION GUARANTEE,0.31722054380664655,"y∈X
pyϕ(y))"
TERMINATION GUARANTEE,0.3202416918429003,"= ∥ϕ(x) −
X"
TERMINATION GUARANTEE,0.32326283987915405,"y∈X
pyϕ(y)∥2 ≤(∥ϕ(x)∥+ ∥
X"
TERMINATION GUARANTEE,0.32628398791540786,"y∈X
pyϕ(y)∥)2 ≤4γ2."
TERMINATION GUARANTEE,0.3293051359516616,"We state the following simplified version of an Azuma bound for Hilbert space valued martingales
from [20], followed by a standard Hoeffding bound.
Theorem 5 ([20]). Let H be a Hilbert space and let Y0, ..., Ym be a H-valued martingale, such that"
TERMINATION GUARANTEE,0.3323262839879154,"∀1 ≤i ≤m, ∥Yi −Yi−1∥≤ai. It holds that Pr[∥Ym −Y0∥≥δ] ≤e
Θ
 
δ2
Pm
i=1 a2
i
"
TERMINATION GUARANTEE,0.33534743202416917,".
Theorem 6 ([12]). Let Y1, ..., Ym be independent random variables such that ∀1 ≤i ≤m, E[Yi] = µ
and Yi ∈[amin, amax]. Then Pr"
M,0.338368580060423,"1
m m
X"
M,0.3413897280966767,"i=1
Yk −µ ≥δ !"
M,0.34441087613293053,≤2e−2mδ2/(amax−amin)2.
M,0.3474320241691843,"The following lemma provide concentration guarantees when sampling a batch.
Lemma 7. Let B be a tuple of b elements chosen uniformly at random from X with repetitions.
For any fixed tuple of k centers, C ⊆H which are a convex combination of X, it holds that:
Pr[|fB(C) −fX(C)| ≥δ] ≤2e−bδ2/2γ2."
M,0.3504531722054381,"Proof. Let us write B = (y1, . . . , yb), where yi is a random element selected uniformly at random
from X with repetitions. For every such yi define the random variable Zi = fyi(C). These new
random variables are IID for any fixed C. It also holds that ∀i ∈[b], E[Zi] = 1 n
P"
M,0.35347432024169184,"x∈X fx(C) =
fX(C) and that fB(C) = 1 b
P"
M,0.3564954682779456,x∈B fx(C) = 1
M,0.3595166163141994,"b
Pb
i=1 Zi."
M,0.36253776435045315,"Applying the Hoeffding bound (Theorem 6) with parameters m = b, µ = fX(C), amax−amin ≤4γ2"
M,0.36555891238670696,(due to Lemma 4) we get that: Pr[|fB(C) −fX(C)| ≥δ] ≤2e−bδ2/2γ2.
M,0.3685800604229607,"For any tuple S ⊆X and some tuple of cluster centers C = (Cℓ)ℓ∈[k] ⊂H, C implies a partition
(Sℓ)ℓ∈[k] of the points in S. Specifically, every Sℓcontains the points in S closest to Cℓ(in H) and
every point in S belongs to a single Cℓ(ties are broken arbitrarily). We state the following useful
observation:
Observation 8. Fix some A ⊆X. Let C be a tuple of k centers, S = (Sℓ)ℓ∈[k] be the partition of"
M,0.3716012084592145,"A induced by C and S = (S
ℓ)ℓ∈[k] be any other partition of A. It holds that Pk
j=1 ∆(Sj, Cj) ≤
Pk
j=1 ∆(S
j, Cj)."
M,0.37462235649546827,"Recall that Cj
i is the j-th center in the beginning of the i-th iteration of Algorithm 1 and (Bℓ
i )ℓ∈[k] is
the partition of Bi induced by Ci. Let (Xℓ
i )ℓ∈[k] be the partition of X induced by Ci."
M,0.3776435045317221,"We now have the tools to analyze Algorithm 1 with the learning rate of [26]. Specifically, we assume"
M,0.3806646525679758,"that the algorithm executes for at least t iterations, the learning rate is αj
i =
q"
M,0.38368580060422963,"bj
i/b, where bj
i =
Bj
i
,"
M,0.3867069486404834,"and the batch size is b = Ω((γ/ϵ)2 log(nt)). We show that the algorithm must terminate within
t = O(γ/ϵ) steps w.h.p. Plugging t back into b, we get that a batch size of b = Ω((γ/ϵ)2 log(nγ/ϵ))
is sufficient."
M,0.38972809667673713,"Proof outline
We note that when sampling a batch it holds w.h.p that fBi(Ci) is close to fXi(Ci)
(Lemma 7). This is due to the fact that Bi is sampled after Ci is fixed. If we could show that fBi(Ci+1)
is close fXi(Ci+1) then combined with the fact that we make progress of at least ϵ on the batch we
can conclude that we make progress of at least some constant fraction of ϵ on the entire dataset."
M,0.39274924471299094,"Unfortunately, as Ci+1 depends on Bi, getting the above guarantee is tricky. To overcome this issue
we define the auxiliary value C
j
i+1 = (1 −αj
i)Cj
i + αj
icm(Xj
i ). This is the j-th center at step i + 1
if we were to use the entire dataset for the update, rather than just a batch. Note that this is only used
in the analysis and not in the algorithm. Note that Ci+1 only depends on Ci and X and is independent
of Bi (i.e., we can fix its value before sampling Bi). As Ci+1 does not depend on Bi we use Ci+1
instead of Ci+1 in the above analysis outline. We show that for our choice of learning rate it holds that
Ci+1, Ci+1 are sufficiently close, which implies that fX(Ci+1), fX(Ci+1) and fBi(Ci+1), fBi(Ci+1)
are also sufficiently close. That is, Ci+1 acts as a proxy for Ci+1. Combining everything together we
get our desired result."
M,0.3957703927492447,"We start with the following useful observation, which will allow us to use Lemma 3 to bound the
norm of the centers by γ throughout the execution of the algorithm."
M,0.3987915407854985,"Observation 9. If ∀j ∈[k], Cj
1 is a convex combination of X then ∀i > 1, j ∈[k], Cj
i , C
j
i are also a
convex combinations of X."
M,0.40181268882175225,"Let us state the following useful lemma from [14]. Although their proof is for Euclidean spaces, it
goes through for Hilbert spaces. We provide the proof in the appendix for completeness.
Lemma 10 ([14]). For any set S ⊆X and any C ∈H it holds that ∆(S, C) = ∆(S, cm(S)) +
|S| ∆(C, cm(S))."
M,0.40483383685800606,"We use the above to prove the following useful lemma.
Lemma 11. For any S ⊆X and C, C′ ∈H which are convex combinations of X, it holds that:
|∆(S, C′) −∆(S, C)| ≤2γ |S| ∥C −C′∥."
M,0.4078549848942598,"Proof. Using Lemma 10 we get that ∆(S, C) = ∆(S, cm(S)) + |S| ∆(cm(S), C) and that
∆(S, C′) = ∆(S, cm(S)) + |S| ∆(cm(S), C′).
Thus, it holds that |∆(S, C′) −∆(S, C)| =
|S| · |∆(cm(S), C′) −∆(cm(S), C)|. Let us write"
M,0.4108761329305136,"|∆(cm(S), C′) −∆(cm(S), C)|"
M,0.41389728096676737,"= |⟨cm(S) −C′, cm(S) −C′⟩−⟨cm(S) −C, cm(S) −C⟩|"
M,0.4169184290030212,"= |−2⟨cm(S), C′⟩+ ⟨C′, C′⟩+ 2⟨cm(S), C⟩−⟨C, C⟩|"
M,0.4199395770392749,"= |2⟨cm(S), C −C′⟩+ ⟨C′ −C, C′ + C⟩|"
M,0.4229607250755287,"= |⟨C −C′, 2cm(S) −(C′ + C)⟩|"
M,0.4259818731117825,≤∥C −C′∥∥2cm(S) −(C′ + C)∥≤4γ∥C −C′∥.
M,0.42900302114803623,"Where in the last transition we used the Cauchy-Schwartz inequality, the triangle inequality, and the
fact that C, C′, cm(S) are convex combinations of X and therefore their norm is bounded by γ."
M,0.43202416918429004,"Now we show that due to our choice of learning rate, Cj
i+1 and C
j
i+1 are sufficiently close."
M,0.4350453172205438,"Lemma 12. It holds w.h.p that ∀i ∈[t], j ∈[k], ∥Cj
i+1 −C
j
i+1∥≤
ϵ
20γ ."
M,0.4380664652567976,"Proof. Note that Cj
i+1 −C
j
i+1 = αj
i(cm(Bj
i ) −cm(Xj
i )). Let us fix some iteration i and center
j. To simplify notation, let us denote: X′ = Xj
i , B′ = Bj
i , b′ = bj
i, α′ = αj
i. Although b′ is a
random variable, in what follows we treat it as a fixed value (essentially conditioning on its value).
As what follows holds for all values of b′ it also holds without conditioning due to the law of total
probabilities."
M,0.44108761329305135,"For the rest of the proof, we assume b′ > 0 (if b′ = 0 the claim holds trivially). Let us denote by
{Yℓ}b′"
M,0.44410876132930516,"ℓ=1 the sampled points in B′. Note that a randomly sampled element from X is in B′ if and
only if it is in X′. As batch elements are sampled uniformly at random with repetitions from X,"
M,0.4471299093655589,"conditioning on the fact that an element is in B′ means that it is distributed uniformly over X′. Note
that ∀ℓ, E[ϕ(Yℓ)] =
1
|X′|
P"
M,0.4501510574018127,x∈X′ ϕ(x) = cm(X′) and E[cm(B′)] = 1
M,0.45317220543806647,"b′
Pb′"
M,0.4561933534743202,ℓ=1 E[ϕ(Yℓ)] = cm(X′).
M,0.459214501510574,"Let us define the following martingale: Zr = Pr
ℓ=1(ϕ(Yℓ) −E[ϕ(Yℓ)]). Note that Z0 = 0, and
when r > 0, Zr = Pr
ℓ=1 ϕ(Yℓ) −r · cm(X′). It is easy to see that this is a martingale:"
M,0.4622356495468278,"E[Zr | Zr−1] = E[ r
X"
M,0.4652567975830816,"ℓ=1
ϕ(Yℓ)−r ·cm(X′) | Zr−1] = Zr−1 +E[ϕ(Yr)−cm(X′) | Zr−1] = Zr−1."
M,0.46827794561933533,"Let us now bound the differences
∥Zr −Zr−1∥= ∥ϕ(Yr) −cm(X′)∥≤∥ϕ(Yr)∥+ ∥cm(X′)∥≤2γ."
M,0.47129909365558914,"Now we may use Azuma’s inequality: Pr[∥Zb′ −Z0∥≥δ] ≤eΘ(
δ2"
M,0.4743202416918429,"γ2b′ ). Let us now divide both
sides of the inequality by b′ and set δ =
b′ϵ
20γα′ . We get"
M,0.4773413897280967,"Pr[∥cm(B′) −cm(X′)∥≥
ϵ
20γα′ ] = Pr[∥1 b′ b′
X"
M,0.48036253776435045,"ℓ=1
ϕ(Yℓ) −cm(X′)∥≥
ϵ
20γα′ ] ≤e
Θ(
b′ϵ2"
M,0.48338368580060426,(γα′)2 ).
M,0.486404833836858,"Using the fact that α′ =
p"
M,0.48942598187311176,"b′/b together with the fact that b = Ω((γ/ϵ)2 log(nt)) (for an appropriate
constant) we get that the above is O(1/ntk). Finally, taking a union bound over all t iterations and
all k centers per iteration completes the proof."
M,0.49244712990936557,"We can now bound the goal function when cluster centers are close using Lemma 12.
Lemma 13. Fix some A ⊆X. It holds w.h.p that ∀i ∈[t],
fA(Ci+1) −fA(Ci+1)
 ≤ϵ/5."
M,0.4954682779456193,"Proof. Let S = (Sℓ)ℓ∈[k], S = (S
ℓ)ℓ∈[k] be the partitions induced by Ci+1, Ci+1 on A. Let us
expand the expression"
M,0.4984894259818731,"fA(Ci+1) −fA(Ci+1) = 1 |A| k
X"
M,0.5015105740181269,"j=1
∆(S
j, C
j
i+1) −∆(Sj, Cj
i+1)"
M,0.5045317220543807,"≤
1
|A| k
X"
M,0.5075528700906344,"j=1
∆(Sj, C
j
i+1) −∆(Sj, Cj
i+1)"
M,0.5105740181268882,"≤
1
|A| k
X"
M,0.513595166163142,"j=1
4γ
Sj ∥C
j
i+1 −Cj
i+1∥≤
1
|A| k
X j=1"
M,0.5166163141993958,Sj ϵ/5 = ϵ/5.
M,0.5196374622356495,"Where the first inequality is due to Observation 8, the second is due Lemma 11 and finally we use
Lemma 12 together with the fact that Pk
j=1
Sj = |A|. Using the same argument we also get that
fA(Ci+1) −fA(Ci+1) ≤ϵ/5, which completes the proof."
M,0.5226586102719033,"Let us state the following useful lemma.
Lemma 14. It holds w.h.p that for every i ∈[t],"
M,0.525679758308157,"fX(Ci+1) −fX(Ci+1) ≥−ϵ/5,
(1)"
M,0.5287009063444109,"fBi(Ci+1) −fBi(Ci+1) ≥−ϵ/5,
(2)
fX(Ci) −fBi(Ci) ≥−ϵ/5,
(3)"
M,0.5317220543806647,"fBi(Ci+1) −fX(Ci+1) ≥−ϵ/5.
(4)"
M,0.5347432024169184,"Proof. The first two inequalities follow from Lemma 13. The last two are due to Lemma 7 by setting
δ = ϵ/5, B = Bi:"
M,0.5377643504531722,"Pr[|fBi(C) −fX(C)| ≥δ] ≤2e−bδ2/2γ2 = e−Θ(bϵ2/γ2) = e−Ω(log(nt)) = O(1/nt).
Where the last inequality is due to the fact that b = Ω((γ/ϵ)2 log(nt)) (for an appropriate constant).
The above holds for either C = Ci or C = Ci+1. Taking a union bound over all t iterations we get the
desired result."
M,0.540785498489426,"Putting everything together
We wish to lower bound fX(Ci) −fX(Ci+1). We write the following,
where the ± notation means we add and subtract a term:"
M,0.5438066465256798,"fX(Ci) −fX(Ci+1) = fX(Ci) ± fBi(Ci) −fX(Ci+1)
≥fBi(Ci) −fX(Ci+1) −ϵ/5 = fBi(Ci) ± fBi(Ci+1) −fX(Ci+1) −ϵ/5
≥fBi(Ci+1) −fX(Ci+1) + 4ϵ/5"
M,0.5468277945619335,= fBi(Ci+1) ± fBi(Ci+1) ± fX(Ci+1) −fX(Ci+1) + 4ϵ/5 ≥ϵ/5.
M,0.5498489425981873,"Where the first inequality is due to inequality (3) in Lemma 14 (fX(Ci) −fBi(Ci) ≥−ϵ/5), the
second is due to the stopping condition of the algorithm (fBi(Ci) −fBi(Ci+1) > ϵ), and the last is
due to the remaining inequalities in Lemma 14. The above holds w.h.p over all of the iterations of
the algorithms. We conclude that when b = Ω((γ/ϵ)2 log(γn/ϵ)), w.h.p. the algorithm terminates
within t = O(γ2/ϵ) iterations. This complete the proof of the second claim of Theorem 1."
EXPERIMENTS,0.552870090634441,"6
Experiments"
EXPERIMENTS,0.5558912386706949,We evaluate our mini-batch algorithm on the following datasets:
EXPERIMENTS,0.5589123867069486,"• MNIST: The MNIST dataset [16] has 70,000 grayscale images of handwritten digits (0 to 9), each
image being 28x28 pixels. When flattened, this gives 784 features.
• PenDigits: The PenDigits dataset [1] has 10992 instances, each represented by an 16-dimensional
vector derived from 2D pen movements. The dataset has 10 labelled clusters, one for each digit.
• Letters: The Letters dataset [28] has 20,000 instances of letters from ’A’ to ’Z’, each represented
by 16 features. The dataset has 26 labelled clusters, one for each letter.
• HAR: The HAR dataset [2] has 10,299 instances collected from smartphone sensors, capturing
human activities like walking, sitting, and standing. Each instance is described by 561 features.
The dataset has 6 labelled clusters, corresponding to different types of physical activities."
EXPERIMENTS,0.5619335347432024,"We compare the following algorithms: (full batch) kernel k-means, mini-batch kernel k-means
with the learning rate of [26], mini-batch kernel k-means with the learning rate of sklearn, mini-
batch (non-kernel) k-means with the learning rate of [26], mini-batch (non-kernel) k-means with
the learning rate of sklearn. We evaluate our results with batch sizes: 1024, 256, 64, and 16. We
execute every algorithm for 200 iterations. For the kernel variants we apply the Gaussian kernel:
K(x, y) = e−∥x−y∥2/κ, where the κ parameter is set using the heuristic of [31] followed by some
manual tuning (exact values appear in the supplementary materials). We repeat every experiment
10 times and present the average Adjusted Rand Index (ARI) [11, 24] and Normalized Mutual
Information (NMI) [15] scores for every dataset. All experiments were conducted on a MacBook Pro
equipped with an M2 Max chip and 96 GB of RAM. We present partial results in Figure 1 and the
full results in the appendix. Error bars in the plot measure the standard deviation."
EXPERIMENTS,0.5649546827794562,"Figure 1: Our results for a batch size of size 1024. We use the β prefix to denote the algorithm uses
the learning rate of [26]."
REFERENCES,0.56797583081571,References
REFERENCES,0.5709969788519638,"[1] E. Alpaydin and Fevzi. Alimoglu. Pen-Based Recognition of Handwritten Digits. UCI Machine Learning
Repository, 1998. DOI: https://doi.org/10.24432/C5MG6K. License: CC BY 4.0 DEED, available at
https://creativecommons.org/licenses/by/4.0/."
REFERENCES,0.5740181268882175,"[2] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz, et al. A public domain
dataset for human activity recognition using smartphones. In Esann, volume 3, page 3, 2013. License: CC
BY-NC-SA 4.0 DEED, available at https://creativecommons.org/licenses/by-nc-sa/4.0/."
REFERENCES,0.5770392749244713,"[3] David Arthur and Sergei Vassilvitskii. How slow is the k-means method? In SCG, pages 144–153. ACM,
2006."
REFERENCES,0.5800604229607251,"[4] David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In SODA, pages
1027–1035. SIAM, 2007."
REFERENCES,0.5830815709969789,"[5] Artem Barger and Dan Feldman. Deterministic coresets for k-means of big sparse data. Algorithms,
13(4):92, 2020."
REFERENCES,0.5861027190332326,"[6] Di Chen and Jeff M Phillips. Relative error embeddings of the gaussian kernel distance. In International
Conference on Algorithmic Learning Theory, pages 560–576. PMLR, 2017."
REFERENCES,0.5891238670694864,"[7] Radha Chitta, Rong Jin, Timothy C. Havens, and Anil K. Jain. Approximate kernel k-means: solution to
large scale kernel clustering. In KDD, pages 895–903. ACM, 2011."
REFERENCES,0.5921450151057401,"[8] Radha Chitta, Rong Jin, and Anil K Jain. Efficient kernel clustering using random fourier features. In 2012
IEEE 12th International Conference on Data Mining, pages 161–170. IEEE, 2012."
REFERENCES,0.595166163141994,"[9] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. Kernel k-means: spectral clustering and normalized
cuts. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ’04, page 551–556, New York, NY, USA, 2004. Association for Computing Machinery."
REFERENCES,0.5981873111782477,"[10] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant-size
coresets for k-means, pca, and projective clustering. SIAM Journal on Computing, 49(3):601–657, 2020."
REFERENCES,0.6012084592145015,"[11] Alexander J Gates and Yong-Yeol Ahn. The impact of random models on clustering similarity. Journal of
Machine Learning Research, 18(87):1–28, 2017."
REFERENCES,0.6042296072507553,"[12] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American
Statistical Association, 58(301):13–30, 1963."
REFERENCES,0.6072507552870091,"[13] Shaofeng H.-C. Jiang, Robert Krauthgamer, Jianing Lou, and Yubo Zhang. Coresets for kernel clustering.
CoRR, abs/2110.02898, 2021."
REFERENCES,0.6102719033232629,"[14] Tapas Kanungo, David M. Mount, Nathan S. Netanyahu, Christine D. Piatko, Ruth Silverman, and Angela Y.
Wu. A local search approximation algorithm for k-means clustering. Comput. Geom., 28(2-3):89–112,
2004."
REFERENCES,0.6132930513595166,"[15] Andrea Lancichinetti, Santo Fortunato, and János Kertész. Detecting the overlapping and hierarchical
community structure in complex networks. New journal of physics, 11(3):033015, 2009."
REFERENCES,0.6163141993957704,"[16] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. License:
CC0 1.0 DEED CC0 1.0 Universal, available at https://creativecommons.org/publicdomain/
zero/1.0/."
REFERENCES,0.6193353474320241,"[17] Stuart P. Lloyd. Least squares quantization in PCM. IEEE Trans. Inf. Theory, 28(2):129–136, 1982."
REFERENCES,0.622356495468278,"[18] Peter Macgregor and He Sun. Fast approximation of similarity graphs with kernel density estimation.
Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.6253776435045317,"[19] Cameron Musco and Christopher Musco. Recursive sampling for the nystrom method. Advances in neural
information processing systems, 30, 2017."
REFERENCES,0.6283987915407855,"[20] Assaf Naor. On the banach-space-valued azuma inequality and small-set isoperimetry of alon–roichman
graphs. Combinatorics, Probability and Computing, 21(4):623–634, 2012."
REFERENCES,0.6314199395770392,"[21] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011."
REFERENCES,0.6344410876132931,"[22] Kent Quanrud. Spectral Sparsification of Metrics and Kernels, pages 1445–1464."
REFERENCES,0.6374622356495468,"[23] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in neural
information processing systems, 20, 2007."
REFERENCES,0.6404833836858006,"[24] William M Rand. Objective criteria for the evaluation of clustering methods. Journal of the American
Statistical association, 66(336):846–850, 1971."
REFERENCES,0.6435045317220544,"[25] Melanie Schmidt. Coresets and streaming algorithms for the k-means problem and related clustering
objectives. 2014."
REFERENCES,0.6465256797583081,"[26] Gregory Schwartzman. Mini-batch k-means terminates within O(d/ϵ) iterations. In ICLR, 2023."
REFERENCES,0.649546827794562,"[27] D. Sculley. Web-scale k-means clustering. In WWW, pages 1177–1178. ACM, 2010."
REFERENCES,0.6525679758308157,"[28] David
Slate.
Letter
Recognition.
UCI
Machine
Learning
Repository,
1991.
DOI:
https://doi.org/10.24432/C5ZP40. License: CC BY 4.0 DEED, available at https://creativecommons.
org/licenses/by/4.0/."
REFERENCES,0.6555891238670695,"[29] Cheng Tang and Claire Monteleoni. Convergence rate of stochastic k-means. In AISTATS, volume 54 of
Proceedings of Machine Learning Research, pages 1495–1503. PMLR, 2017."
REFERENCES,0.6586102719033232,"[30] Andrea Vattani. k-means requires exponentially many iterations even in the plane. Discret. Comput. Geom.,
45(4):596–616, 2011."
REFERENCES,0.6616314199395771,"[31] Shusen Wang, Alex Gittens, and Michael W Mahoney. Scalable kernel k-means clustering with nystrom
approximation: Relative-error bounds. Journal of Machine Learning Research, 20(12):1–49, 2019."
REFERENCES,0.6646525679758308,"A
Omitted proofs"
REFERENCES,0.6676737160120846,Proof of Lemma 10
REFERENCES,0.6706948640483383,Proof.
REFERENCES,0.6737160120845922,"∆(S, C) =
X"
REFERENCES,0.676737160120846,"x∈S
∆(x, C) =
X"
REFERENCES,0.6797583081570997,"x∈S
⟨x −C, x −C⟩ =
X"
REFERENCES,0.6827794561933535,"x∈S
⟨(x −cm(S)) + (cm(S) −C), (x −cm(S)) + (cm(S) −C)⟩ =
X"
REFERENCES,0.6858006042296072,"x∈S
∆(x, cm(S)) + ∆(C, cm(S)) + 2⟨x −cm(S), cm(S) −C⟩"
REFERENCES,0.6888217522658611,"= ∆(S, cm(S)) + |S| ∆(C, cm(S)) +
X"
REFERENCES,0.6918429003021148,"x∈S
2⟨x −cm(S), cm(S) −C⟩"
REFERENCES,0.6948640483383686,"= ∆(S, cm(S)) + |S| ∆(C, cm(S)),"
REFERENCES,0.6978851963746223,"where the last step is due to the fact that
X"
REFERENCES,0.7009063444108762,"x∈S
⟨x −cm(S), cm(S) −C⟩= ⟨
X"
REFERENCES,0.7039274924471299,"x∈S
x −|S| cm(S), cm(S) −C⟩ = ⟨
X"
REFERENCES,0.7069486404833837,"x∈S
x −|S| |S| X"
REFERENCES,0.7099697885196374,"x∈S
x, cm(S) −C⟩= 0."
REFERENCES,0.7129909365558912,"B
Full experimental results"
REFERENCES,0.716012084592145,"Figure 2: Our results for all batch sizes. We use the β prefix to denote the algorithm uses the learning
rate of [26]."
REFERENCES,0.7190332326283988,NeurIPS Paper Checklist
CLAIMS,0.7220543806646526,1. Claims
CLAIMS,0.7250755287009063,"Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope?"
CLAIMS,0.7280966767371602,Answer: [Yes]
CLAIMS,0.7311178247734139,"Justification: We prove our theoretical claims are true and show that the performance of our algorithm
is competitive using experiments."
CLAIMS,0.7341389728096677,Guidelines:
CLAIMS,0.7371601208459214,"• The answer NA means that the abstract and introduction do not include the claims made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the contributions
made in the paper and important assumptions and limitations. A No or NA answer to this question
will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how much the results
can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper."
LIMITATIONS,0.7401812688821753,2. Limitations
LIMITATIONS,0.743202416918429,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.7462235649546828,Answer: [Yes]
LIMITATIONS,0.7492447129909365,"Justification: From our experimental results, each iteration of our mini-batch kernel k-means algorithm
is still slower than that of mini-batch kmeans. This difference in speed may be alleviated by lazily
enumerating the kernel matrix for very large datasets."
LIMITATIONS,0.7522658610271903,Guidelines:
LIMITATIONS,0.7552870090634441,"• The answer NA means that the paper has no limitation while the answer No means that the paper
has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
asymptotic approximations only holding locally). The authors should reflect on how these assump-
tions might be violated in practice and what the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on
a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions,
which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or images
are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed
captions for online lectures because it fails to handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms and how they
scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to address problems
of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t
acknowledged in the paper. The authors should use their best judgment and recognize that individual
actions in favor of transparency play an important role in developing norms that preserve the integrity
of the community. Reviewers will be specifically instructed to not penalize honesty concerning
limitations."
THEORY ASSUMPTIONS AND PROOFS,0.7583081570996979,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.7613293051359517,"Question: For each theoretical result, does the paper provide the full set of assumptions and a complete
(and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.7643504531722054,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.7673716012084593,Justification: Proofs are complete and correct to the best of our knowledge.
THEORY ASSUMPTIONS AND PROOFS,0.770392749244713,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.7734138972809668,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if they appear in
the supplemental material, the authors are encouraged to provide a short proof sketch to provide
intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented by formal
proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7764350453172205,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7794561933534743,"Question: Does the paper fully disclose all the information needed to reproduce the main experimental
results of the paper to the extent that it affects the main claims and/or conclusions of the paper
(regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7824773413897281,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7854984894259819,"Justification: Our main claims are theoretical; however, we provide all the information needed to
replicate the experimental results."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7885196374622356,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7915407854984894,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived well by the
reviewers: Making the paper reproducible is important, regardless of whether the code and data are
provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or verifiable."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7945619335347432,"• Depending on the contribution, reproducibility can be accomplished in various ways. For example,
if the contribution is a novel architecture, describing the architecture fully might suffice, or if the
contribution is a specific model and empirical evaluation, it may be necessary to either make it
possible for others to replicate the model with the same dataset, or provide access to the model. In
general. releasing code and data is often one good way to accomplish this, but reproducibility can
also be provided via detailed instructions for how to replicate the results, access to a hosted model
(e.g., in the case of a large language model), releasing of a model checkpoint, or other means that
are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submissions to
provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce
that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe the architec-
ture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should either be a
way to access this model for reproducing the results or a way to reproduce the model (e.g., with
an open-source dataset or instructions for how to construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of closed-
source models, it may be that access to the model is limited in some way (e.g., to registered
users), but it should be possible for other researchers to have some path to reproducing or
verifying the results."
OPEN ACCESS TO DATA AND CODE,0.797583081570997,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8006042296072508,"Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?"
OPEN ACCESS TO DATA AND CODE,0.8036253776435045,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8066465256797583,"Justification: We provide code, data, and instructions on how to reproduce our experimental results in
the supplemental material."
OPEN ACCESS TO DATA AND CODE,0.8096676737160121,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8126888217522659,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/
guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be possible, so
“No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is
central to the contribution (e.g., for a new open-source benchmark).
• The instructions should contain the exact command and environment needed to run to reproduce
the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/
guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how to access the
raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
• Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8157099697885196,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8187311178247734,"Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?"
OPEN ACCESS TO DATA AND CODE,0.8217522658610272,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.824773413897281,Justification: Full experimental details can be found within the code.
OPEN ACCESS TO DATA AND CODE,0.8277945619335347,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8308157099697885,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8338368580060423,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8368580060422961,"Question: Does the paper report error bars suitably and correctly defined or other appropriate informa-
tion about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8398791540785498,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8429003021148036,Justification: We explain how we report error bars using sample standard deviation.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8459214501510574,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8489425981873112,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals,
or statistical significance tests, at least for the experiments that support the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for exam-
ple, train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
• The method for calculating the error bars should be explained (closed form formula, call to a library
function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a
2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not
verified.
• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric
error bars that would yield results that are out of range (e.g. negative error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.851963746223565,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8549848942598187,"Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.8580060422960725,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.8610271903323263,"Justification: We give the runtime of each experiment and the type of machine they were performed
on."
EXPERIMENTS COMPUTE RESOURCES,0.8640483383685801,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.8670694864048338,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual experimental
runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute than the exper-
iments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the
paper)."
CODE OF ETHICS,0.8700906344410876,9. Code Of Ethics
CODE OF ETHICS,0.8731117824773413,"Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code
of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.8761329305135952,Answer: [Yes]
CODE OF ETHICS,0.879154078549849,"Justification: We have read the code of ethics and believe that our research conforms to all the
requirements."
CODE OF ETHICS,0.8821752265861027,Guidelines:
CODE OF ETHICS,0.8851963746223565,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to
laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.8882175226586103,10. Broader Impacts
BROADER IMPACTS,0.8912386706948641,"Question: Does the paper discuss both potential positive societal impacts and negative societal impacts
of the work performed?"
BROADER IMPACTS,0.8942598187311178,Answer: [NA]
BROADER IMPACTS,0.8972809667673716,"Justification: There are many potential societal consequences of our work, none which we feel must
be specifically highlighted here. We believe the broader impact to be similar to that of mini-batch
kmeans++."
BROADER IMPACTS,0.9003021148036254,Guidelines:
BROADER IMPACTS,0.9033232628398792,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal impact or why
the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses (e.g., dis-
information, generating fake profiles, surveillance), fairness considerations (e.g., deployment of
technologies that could make decisions that unfairly impact specific groups), privacy considerations,
and security considerations.
• The conference expects that many papers will be foundational research and not tied to particular
applications, let alone deployments. However, if there is a direct path to any negative applications,
the authors should point it out. For example, it is legitimate to point out that an improvement in the
quality of generative models could be used to generate deepfakes for disinformation. On the other
hand, it is not needed to point out that a generic algorithm for optimizing neural networks could
enable people to train models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is being used as
intended and functioning correctly, harms that could arise when the technology is being used as
intended but gives incorrect results, and harms following from (intentional or unintentional) misuse
of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation strategies
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring
misuse, mechanisms to monitor how a system learns from feedback over time, improving the
efficiency and accessibility of ML)."
SAFEGUARDS,0.9063444108761329,11. Safeguards
SAFEGUARDS,0.9093655589123867,"Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or
scraped datasets)?"
SAFEGUARDS,0.9123867069486404,Answer: [NA]
SAFEGUARDS,0.9154078549848943,Justification: The paper poses no such risks.
SAFEGUARDS,0.918429003021148,Guidelines:
SAFEGUARDS,0.9214501510574018,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere to
usage guidelines or restrictions to access the model or implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort."
LICENSES FOR EXISTING ASSETS,0.9244712990936556,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9274924471299094,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
properly credited and are the license and terms of use explicitly mentioned and properly respected?"
LICENSES FOR EXISTING ASSETS,0.9305135951661632,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9335347432024169,Justification: All datasets are properly credited and licenses and terms are mentioned and respected.
LICENSES FOR EXISTING ASSETS,0.9365558912386707,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9395770392749244,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of service of that
source should be provided.
• If assets are released, the license, copyright information, and terms of use in the package should be
provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some
datasets. Their licensing guide can help determine the license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9425981873111783,"• If this information is not available online, the authors are encouraged to reach out to the asset’s
creators."
NEW ASSETS,0.945619335347432,13. New Assets
NEW ASSETS,0.9486404833836858,"Question: Are new assets introduced in the paper well documented and is the documentation provided
alongside the assets?"
NEW ASSETS,0.9516616314199395,Answer: [Yes]
NEW ASSETS,0.9546827794561934,"Justification: We provide documentation along with our code, which can automatically fetch and
preprocess the data used in our experiments."
NEW ASSETS,0.9577039274924471,Guidelines:
NEW ASSETS,0.9607250755287009,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their submissions
via structured templates. This includes details about training, license, limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either create an
anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9637462235649547,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9667673716012085,"Question: For crowdsourcing experiments and research with human subjects, does the paper include
the full text of instructions given to participants and screenshots, if applicable, as well as details about
compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9697885196374623,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972809667673716,Justification: We did not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758308157099698,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9788519637462235,"• The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
• Including this information in the supplemental material is fine, but if the main contribution of the
paper involves human subjects, then as much detail as possible should be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9818731117824774,15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848942598187311,"Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an
equivalent approval/review based on the requirements of your country or institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879154078549849,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9909365558912386,Justification: We did not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9939577039274925,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969788519637462,"• The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent) may be
required for any human subjects research. If you obtained IRB approval, you should clearly state
this in the paper.
• We recognize that the procedures for this may vary significantly between institutions and locations,
and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if applicable),
such as the institution conducting the review."
