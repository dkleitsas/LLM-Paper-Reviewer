Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005373455131649651,"Scaling laws are useful guides for derisking expensive training runs, as they predict
1"
ABSTRACT,0.0010746910263299302,"performance of large models using cheaper, small-scale experiments. However,
2"
ABSTRACT,0.0016120365394948952,"there remain gaps between current scaling studies and how language models are
3"
ABSTRACT,0.0021493820526598604,"ultimately trained and evaluated. For instance, scaling is usually studied in the
4"
ABSTRACT,0.0026867275658248252,"compute-optimal training regime (i.e., “Chinchilla optimal” regime). In contrast,
5"
ABSTRACT,0.0032240730789897904,"models are often over-trained to reduce inference costs. Moreover, scaling laws
6"
ABSTRACT,0.0037614185921547557,"mostly predict loss on next-token prediction, but models are usually compared on
7"
ABSTRACT,0.004298764105319721,"downstream task performance. To address both shortcomings, we create a testbed
8"
ABSTRACT,0.004836109618484685,"of 104 models with 0.011B to 6.9B parameters trained with various numbers of
9"
ABSTRACT,0.0053734551316496505,"tokens on three data distributions. First, we fit scaling laws that extrapolate in both
10"
ABSTRACT,0.005910800644814616,"the amount of over-training and the number of model parameters. This enables us
11"
ABSTRACT,0.006448146157979581,"to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32× over-
12"
ABSTRACT,0.006985491671144546,"trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)—each
13"
ABSTRACT,0.007522837184309511,"from experiments that take 300× less compute. Second, we relate the perplexity of
14"
ABSTRACT,0.008060182697474477,"a language model to its downstream task performance by proposing a power law.
15"
ABSTRACT,0.008597528210639442,"We use this law to predict top-1 error averaged over downstream tasks for the two
16"
ABSTRACT,0.009134873723804407,"aforementioned models, using experiments that take 20× less compute.
17"
INTRODUCTION,0.00967221923696937,"1
Introduction
18"
INTRODUCTION,0.010209564750134336,"Training large language models is expensive. Furthermore, training high-quality models requires a
19"
INTRODUCTION,0.010746910263299301,"complex recipe of algorithmic techniques and training data. To reduce the cost of finding successful
20"
INTRODUCTION,0.011284255776464266,"training recipes, researchers first evaluate ideas with small experiments and then extrapolate their
21"
INTRODUCTION,0.011821601289629231,"efficacy to larger model and data regimes via scaling laws. With reliable extrapolation, it is possible
22"
INTRODUCTION,0.012358946802794197,"to quickly iterate at small scale and still pick the method that will perform best for the final large
23"
INTRODUCTION,0.012896292315959162,"training run. Indeed, this workflow has become commonplace for training state-of-the-art language
24"
INTRODUCTION,0.013433637829124127,"models like Chinchilla 70B [45], PaLM 540B [19], GPT-4 [76], and many others.
25"
INTRODUCTION,0.013970983342289092,"Despite their importance for model development, published scaling laws differ from the goals of
26"
INTRODUCTION,0.014508328855454057,"training state-of-the-art models in important ways. For instance, scaling studies usually focus on the
27"
INTRODUCTION,0.015045674368619023,"compute-optimal training regime (“Chinchilla optimality” [45]), where model and dataset size are set
28"
INTRODUCTION,0.015583019881783988,"to yield minimum loss for a given compute budget. However, this setting ignores inference costs.
29"
INTRODUCTION,0.016120365394948953,"As larger models are more expensive at inference, it is now common practice to over-train smaller
30"
INTRODUCTION,0.016657710908113917,"models [113]. Another potential mismatch is that most scaling laws quantify model performance by
31"
INTRODUCTION,0.017195056421278884,"perplexity in next-token prediction instead of accuracy on widely used benchmark datasets. However,
32"
INTRODUCTION,0.017732401934443847,"practitioners usually turn to benchmark performance, not loss, to compare models.
33"
INTRODUCTION,0.018269747447608814,"In this paper, we conduct an extensive set of experiments to address both scaling in the over-trained
34"
INTRODUCTION,0.018807092960773777,"regime and benchmark performance prediction.
35"
INTRODUCTION,0.01934443847393874,"1016
1017
1018
1019
1020
1021
1022"
INTRODUCTION,0.019881783987103708,"Compute (6ND, D = MN) [FLOPs] 1 2 3 4 5"
INTRODUCTION,0.02041912950026867,Reducible loss: C4 eval
INTRODUCTION,0.02095647501343364,"N = 0.011B
N = 0.079B
N = 0.154B
N = 0.411B
N = 1.4B
N = 6.9B"
INTRODUCTION,0.021493820526598602,"Prediction
Interpolation
Extrapolation
M = 20
M = 320
M = 640"
INTRODUCTION,0.02203116603976357,"2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0"
INTRODUCTION,0.022568511552928532,Loss: C4 eval 0.4 0.5 0.6 0.7 0.8
INTRODUCTION,0.0231058570660935,Average top-1 error: 17-task split
INTRODUCTION,0.023643202579258463,"1022
0.56 0.60 0.64 0.68 0.72"
INTRODUCTION,0.02418054809242343,"2.4
2.6 0.44 0.46 0.48 0.50 0.52"
INTRODUCTION,0.024717893605588393,"Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit
a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which
is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a
model, approximated by C = 6ND. Larger values of M specify more over-training. We are able
to extrapolate, in both N and M, the validation performance of models requiring more than 300×
the training compute used to construct the scaling law. (right) We also fit a scaling law to predict
average downstream top-1 error as a function of validation loss. We find that fitting scaling laws
for downstream error benefits from using more expensive models when compared to fitting for loss
prediction. We predict the average error over 17 downstream tasks for models trained with over 20×
the compute. For this figure, we train all models on RedPajama [112]."
INTRODUCTION,0.02525523911875336,"Motivated by the practice of training beyond compute-optimality, we first investigate whether scaling
36"
INTRODUCTION,0.025792584631918324,"follows reliable trends in the over-trained regime. We notice, as implied by Hoffmann et al. [45], for a
37"
INTRODUCTION,0.026329930145083287,"set of models of different sizes trained with a constant ratio of tokens to parameters, models’ reducible
38"
INTRODUCTION,0.026867275658248254,"loss L′ [43, 45] follows a power law (L′ = λ · C−η) in the amount of training compute C. We
39"
INTRODUCTION,0.027404621171413217,"find that as one increases the ratio of tokens to parameters, corresponding to more over-training, the
40"
INTRODUCTION,0.027941966684578184,"scaling exponent η remains about the same, while the scalar λ changes. We explain our observations
41"
INTRODUCTION,0.028479312197743148,"by reparameterizing existing scaling laws in relation to the amount of over-training.
42"
INTRODUCTION,0.029016657710908115,"To establish empirically that scaling extrapolates in the over-trained regime, we further experiment
43"
INTRODUCTION,0.02955400322407308,"with a testbed of 104 models, trained from scratch on three different datasets: C4 [88, 27],
44"
INTRODUCTION,0.030091348737238045,"RedPajama [112], and RefinedWeb [82]. We find that scaling laws fit to small models can accurately
45"
INTRODUCTION,0.03062869425040301,"predict the performance of larger models that undergo more over-training. Figure 1 (left) illustrates our
46"
INTRODUCTION,0.031166039763567976,"main over-training result, where we invest 2.4e19 FLOPs to extrapolate the C4 validation performance
47"
INTRODUCTION,0.03170338527673294,"of a 1.4B parameter model trained on 900B tokens, which requires 300× more compute to train.
48"
INTRODUCTION,0.032240730789897906,"In addition to over-training, we also investigate if scaling laws can predict the performance of a
49"
INTRODUCTION,0.03277807630306287,"model on downstream tasks. We establish a power law relationship between language modeling
50"
INTRODUCTION,0.03331542181622783,"perplexity and the average top-1 error on a suite of downstream tasks. While it can be difficult to
51"
INTRODUCTION,0.0338527673293928,"predict the error on individual tasks, we find it possible to predict aggregate performance from a
52"
INTRODUCTION,0.03439011284255777,"model’s perplexity among models trained on the same training data. Figure 1 (right) presents our
53"
INTRODUCTION,0.03492745835572273,"main downstream error prediction result, where we invest 2.7e20 FLOPs to predict the average top-1
54"
INTRODUCTION,0.035464803868887694,"error over a set of downstream tasks to within 1 percentage point for a 6.9B compute-optimal model,
55"
INTRODUCTION,0.03600214938205266,"which requires 20× more compute to train.
56"
INTRODUCTION,0.03653949489521763,"Our results suggest that the proposed scaling laws are promising to derisk (i) the effects of over-
57"
INTRODUCTION,0.03707684040838259,"training models and (ii) the downstream performance of scaling up training recipes. To facilitate
58"
INTRODUCTION,0.037614185921547555,"further research on reliable scaling, we will release all experiments and models.
59"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.03815153143471252,"2
Developing scaling laws for over-training and downstream tasks
60"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.03868887694787748,"In this section, we develop scaling laws to predict over-trained and downstream performance. First,
61"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.03922622246104245,"we provide key definitions (Section 2.1). We next present a scaling law for over-training drawing on
62"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.039763567974207416,"empirical observation and prior work (Section 2.2). To connect loss scaling and downstream error
63"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04030091348737238,"prediction, we observe that average top-1 error decreases exponentially as a function of validation loss,
64"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04083825900053734,"1017
1019
1021"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04137560451370231,"Compute (6ND, D = MN) [FLOPs] 1 2 3 4 5"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04191295002686728,Reducible loss: C4 eval
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04245029554003224,Training set: C4
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.042987641053197204,"1017
1019
1021"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.043524986566362174,"Compute (6ND, D = MN) [FLOPs] 1 2 3 4 5"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04406233207952714,Training set: RedPajama
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.0445996775926921,"1017
1019
1021"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.045137023105857065,"Compute (6ND, D = MN) [FLOPs] 1 2 3 4 5"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04567436861902203,Training set: RefinedWeb
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.046211714132187,"N = 0.011B
N = 0.079B
N = 0.154B
N = 0.411B
Interpolation
Extrapolation 10 20 40 80 160 320 640"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04674905964535196,token multiplier M
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.047286405158516925,"Figure 2: Scaling in the over-trained regime follows consistent power law exponents. We notice
parallel lines in the log-log plots of reducible loss vs. training compute for a range of token multipliers
M, which give the ratio of training tokens to model parameters. Larger M corresponds to more
over-training. For a power law giving reducible loss as a function of compute: L′(C) = λ · C−η, the
exponent η remains relatively constant resulting in lines with approximately fixed slope (Figure 17).
The scalar λ that determines the y-intercept, however, shifts with different token multipliers. This
suggests λ is a function of the token multiplier, while η is not."
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04782375067168189,"which we formalize as a novel scaling law (Section 2.3). In later sections, we build an experimental
65"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.04836109618484686,"setup (Section 3) to quantify the extent to which our scaling laws extrapolate reliably (Section 4).
66"
PRELIMINARIES,0.04889844169801182,"2.1
Preliminaries
67"
PRELIMINARIES,0.049435787211176786,"Scaling laws for loss.
Typically, scaling laws predict model loss L as a function of the compute
68"
PRELIMINARIES,0.04997313272434175,"C in FLOPs used for training. If one increases the number of parameters N in a model or the
69"
PRELIMINARIES,0.05051047823750672,"number of tokens D that a model is trained on, compute requirements naturally increase. Hence, we
70"
PRELIMINARIES,0.051047823750671684,"assume C is a function of N, D. Following Kaplan et al. [51], we use the approximation C = 6ND,
71"
PRELIMINARIES,0.05158516926383665,"which Hoffmann et al. [45] independently verify. We consider,
72"
PRELIMINARIES,0.05212251477700161,"L(C) = E + L′(C),
(1)"
PRELIMINARIES,0.052659860290166574,"where E is an irreducible loss and L′ is the reducible loss. E captures the Bayes error or minimum
73"
PRELIMINARIES,0.053197205803331545,"possible loss achievable on the validation domain. The L′(C) term captures what can possibly be
74"
PRELIMINARIES,0.05373455131649651,"learned about the validation domain by training on a source domain. L′(C) should approach zero
75"
PRELIMINARIES,0.05427189682966147,"with increased training data and model capacity. L′(C) is often assumed to follow a power law:
76"
PRELIMINARIES,0.054809242342826435,"L′(C) = λ · C−η (i.a., Hestness et al. [43], OpenAI [76]). It is also often helpful to consider a power
77"
PRELIMINARIES,0.055346587855991405,"law in a log-log plot, where it appears as a line with slope −η and y-intercept log (λ).
78"
PRELIMINARIES,0.05588393336915637,"Token multipliers.
We define a token multiplier M = D/N as the ratio of training tokens to model
79"
PRELIMINARIES,0.05642127888232133,"parameters for notational convenience. M allows us to consider fixed relationships between D and
80"
PRELIMINARIES,0.056958624395486296,"N even as a model gets bigger (i.e., as N becomes larger).
81"
PRELIMINARIES,0.05749596990865126,"Compute-optimal training.
Hoffmann et al. [45] establish compute-optimal training, where, for
82"
PRELIMINARIES,0.05803331542181623,"any compute budget H, the allocation of parameters and tokens is given by,
83"
PRELIMINARIES,0.05857066093498119,"arg min
N,D L(N, D) s.t. C(N, D) = H.
(2)"
PRELIMINARIES,0.05910800644814616,"To solve for the optimal N ∗, D∗, one can sweep N, D for each compute budget, retaining the
84"
PRELIMINARIES,0.05964535196131112,"best configurations. Hoffmann et al. [45] find that as the compute budget increases, N ∗and D∗
85"
PRELIMINARIES,0.06018269747447609,"scale roughly evenly. Assuming equal scaling, there is a fixed compute-optimal token multiplier
86"
PRELIMINARIES,0.060720042987641054,"M ∗= D∗/N ∗per training distribution.
87"
PRELIMINARIES,0.06125738850080602,"Over-training.
We define over-training as the practice of allocating compute sub-optimally, so
88"
PRELIMINARIES,0.06179473401397098,"smaller models train on a disproportionately large number of tokens (i.e., M > M ∗). While loss
89"
PRELIMINARIES,0.06233207952713595,"should be higher than in the compute-optimal allocation for a given training budget, the resulting
90"
PRELIMINARIES,0.06286942504030091,"models have fewer parameters and thus incur less inference cost.
91"
SCALING LAWS FOR OVER-TRAINING,0.06340677055346589,"2.2
Scaling laws for over-training
92"
SCALING LAWS FOR OVER-TRAINING,0.06394411606663085,"To propose a scaling law for over-trained models, we first turn to empirical observation. We train four
93"
SCALING LAWS FOR OVER-TRAINING,0.06448146157979581,"model configurations with parameter counts between 0.011B and 0.411B for token multipliers M
94"
SCALING LAWS FOR OVER-TRAINING,0.06501880709296078,"3
4
5
6"
SCALING LAWS FOR OVER-TRAINING,0.06555615260612574,Loss: C4 eval 0.50 0.55 0.60 0.65 0.70 0.75 0.80
SCALING LAWS FOR OVER-TRAINING,0.0660934981192907,Average top-1 error: 17-task split
SCALING LAWS FOR OVER-TRAINING,0.06663084363245567,Training set: C4
SCALING LAWS FOR OVER-TRAINING,0.06716818914562063,"3
4
5
6"
SCALING LAWS FOR OVER-TRAINING,0.0677055346587856,Loss: C4 eval 0.50 0.55 0.60 0.65 0.70 0.75 0.80
SCALING LAWS FOR OVER-TRAINING,0.06824288017195057,Training set: RedPajama
SCALING LAWS FOR OVER-TRAINING,0.06878022568511553,"3
4
5
6"
SCALING LAWS FOR OVER-TRAINING,0.0693175711982805,Loss: C4 eval 0.50 0.55 0.60 0.65 0.70 0.75 0.80
SCALING LAWS FOR OVER-TRAINING,0.06985491671144546,Training set: RefinedWeb
SCALING LAWS FOR OVER-TRAINING,0.07039226222461042,"Model
Interpolation
Extrapolation"
SCALING LAWS FOR OVER-TRAINING,0.07092960773777539,"Figure 3: Average top-1 error scales as a function of loss. We plot models trained on three datasets
and notice an exponential decay of average top-1 error as C4 eval loss, on the x-axis, decreases. We
consider on the y-axes average error on 17 evaluations where performance is at least 10 points above
random chance for at least one 0.154B scale model. These observations suggest that average top-1
error should be predictable with reliable loss estimates."
SCALING LAWS FOR OVER-TRAINING,0.07146695325094035,"between 20 and 640, where M = 20 points lie roughly on the compute-optimal frontier, and larger
95"
SCALING LAWS FOR OVER-TRAINING,0.07200429876410532,"M corresponds to more over-training. We defer experimental details to Section 3 to focus on our
96"
SCALING LAWS FOR OVER-TRAINING,0.07254164427727028,"observations first. In Figure 2, we show loss against compute in a log-log plot for the models trained
97"
SCALING LAWS FOR OVER-TRAINING,0.07307898979043526,"on three datasets and evaluated on the C4 eval set. We notice parallel lines when fitting power laws to
98"
SCALING LAWS FOR OVER-TRAINING,0.07361633530360022,"the reducible loss, which suggests a near-constant scaling exponent even with increased over-training.
99"
SCALING LAWS FOR OVER-TRAINING,0.07415368081676518,"This indicates that scaling behavior should be describable in the amount of over-training.
100"
SCALING LAWS FOR OVER-TRAINING,0.07469102632993015,"In search of an analytic expression for the observations in Figure 2, we consider existing scaling
101"
SCALING LAWS FOR OVER-TRAINING,0.07522837184309511,"literature. A common functional form for the risk of a model, as proposed in prior work [93, 45] is,
102"
SCALING LAWS FOR OVER-TRAINING,0.07576571735626007,"L(N, D) = E + AN −α + BD−β.
(3)
Recall from Section 2.1, N is the number of parameters and D the number of training tokens. The
103"
SCALING LAWS FOR OVER-TRAINING,0.07630306286942504,"constants E, A, α, B, β are fit from data. By fitting this parametric form, Hoffmann et al. [45]
104"
SCALING LAWS FOR OVER-TRAINING,0.07684040838259,"find that scaling exponents α and β are roughly equal, suggesting that one should scale N and D
105"
SCALING LAWS FOR OVER-TRAINING,0.07737775389575496,"equally as compute increases. Hence, we assume α = β. With this assumption, we reparameterize
106"
SCALING LAWS FOR OVER-TRAINING,0.07791509940891994,"Equation (3) in terms of compute C = 6ND and a token multiplier M = D/N. We get,
107"
SCALING LAWS FOR OVER-TRAINING,0.0784524449220849,"L(C, M) = E +
 
aM η + bM −η
C−η,
(4)"
SCALING LAWS FOR OVER-TRAINING,0.07898979043524987,"where η = α/2, a = A(1/6)−η, b = B(1/6)−η gives the relation to Equation (3). For a complete
108"
SCALING LAWS FOR OVER-TRAINING,0.07952713594841483,"derivation, see Appendix A.
109"
SCALING LAWS FOR OVER-TRAINING,0.0800644814615798,"Equation (4) has the following interpretation: (i) The scaling exponent η is not dependent on M.
110"
SCALING LAWS FOR OVER-TRAINING,0.08060182697474476,"Thus, we always expect lines with the same slope in the log-log plot—as in Figure 2. (ii) The term
111"
SCALING LAWS FOR OVER-TRAINING,0.08113917248790972,"aM η + bM −η determines the offsets between curves with different token multipliers. Hence, we
112"
SCALING LAWS FOR OVER-TRAINING,0.08167651800107469,"expect non-overlapping, parallel lines in the log-log plot for the range of M we consider—also
113"
SCALING LAWS FOR OVER-TRAINING,0.08221386351423966,"consistent with Figure 2.
114"
SCALING LAWS FOR OVER-TRAINING,0.08275120902740463,"Recall that we make the assumption α = β, which implies equal scaling of parameters and tokens
115"
SCALING LAWS FOR OVER-TRAINING,0.08328855454056959,"as more compute is available. However, as explained in Appendix A, even if α ̸= β, we get a
116"
SCALING LAWS FOR OVER-TRAINING,0.08382590005373455,"parameterization that implies the power-law exponent remains constant with over-training.
117"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08436324556689952,"2.3
Scaling laws for downstream error
118"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08490059108006448,"Scaling is typically studied in the context of loss [51, 45, 72], which Schaeffer et al. [100] note
119"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08543793659322944,"is smoother than metrics like accuracy. However, practitioners often use downstream benchmark
120"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08597528210639441,"accuracy as a proxy for model quality and not loss on perplexity evaluation sets. To better connect
121"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08651262761955937,"scaling laws and over-training to task prediction, we revisit the suite of models plotted in Figure 2. In
122"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08704997313272435,"Figure 3, we plot average downstream top-1 errors over evaluations sourced from LLM-Foundry [69]
123"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08758731864588931,"against the C4 eval loss. We defer details of the setup to Section 3 to focus here on a key observation:
124"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08812466415905428,"average error appears to follow exponential decay as loss decreases.
125"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08866200967221924,"Based on the exponential decay we observe in Figure 3, we propose the following relationship
126"
SCALING LAWS FOR DOWNSTREAM ERROR,0.0891993551853842,"between downstream average top-1 error Err and loss L,
127"
SCALING LAWS FOR DOWNSTREAM ERROR,0.08973670069854917,"Err(L) = ϵ −k · exp (−γL),
(5)"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09027404621171413,"1017
1019
1021"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09081139172487909,Compute (6ND) [FLOPs] 2 3 4 5 6
SCALING LAWS FOR DOWNSTREAM ERROR,0.09134873723804406,Loss: OpenLM eval
SCALING LAWS FOR DOWNSTREAM ERROR,0.09188608275120903,Search
SCALING LAWS FOR DOWNSTREAM ERROR,0.092423428264374,"1017
1019
1021"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09296077377753896,Compute (6ND) [FLOPs]
SCALING LAWS FOR DOWNSTREAM ERROR,0.09349811929070392,Filter
SCALING LAWS FOR DOWNSTREAM ERROR,0.09403546480386889,"1017
1019
1021"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09457281031703385,Compute (6ND) [FLOPs] Fit
SCALING LAWS FOR DOWNSTREAM ERROR,0.09511015583019881,"Grid search models
Selected models
Target 1.4B model
Target 6.9B model
Interpolation
Extrapolation 1022"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09564750134336378,"1.9
1.8
1.7"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09618484685652874,"Figure 4: Search, filter, fit: A recipe for selecting configurations for scaling. (left) To generate the
final configurations presented in Table 3, we run a 435 model grid search over model width, hidden
dimension, number of attention heads, batch size, and warmup steps. All models are trained near
compute-optimally. (center) We plot the efficient frontier of models, which appear to follow a trend,
excluding models from 5.2 × 1016 to 5.2 × 1017, which fall below the trend. (right) We fit a power
law with irreducible error to the remaining configurations, picking four configurations that closely
track the full model suite (“Selected models”). These models extrapolate the performance of 1.4B,
6.9B target models. Shaded regions represent bootstrap 95% confidence intervals."
SCALING LAWS FOR DOWNSTREAM ERROR,0.09672219236969372,"where ϵ, k, γ are fit from data. Equation (5) also has an interpretation in terms of model perplexity
128"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09725953788285868,"PP(L) = exp (L),
129"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09779688339602365,"Err(PP) = ϵ −k · PP−γ.
(6)"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09833422890918861,"Namely, Err follows a power law in PP that is bounded from above by ϵ signifying arbitrarily high
130"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09887157442235357,"error and from below by ϵ −k · exp(−γE), where E is the Bayes error from Equation (4).
131"
SCALING LAWS FOR DOWNSTREAM ERROR,0.09940891993551854,"Equation (5) in conjunction with Equation (4) suggests a three-step method to predict Err as a function
132"
SCALING LAWS FOR DOWNSTREAM ERROR,0.0999462654486835,"of compute and the amount of over-training. For choices of training and validation distributions, (i)
133"
SCALING LAWS FOR DOWNSTREAM ERROR,0.10048361096184846,"fit a scaling law to Equation (4) using triplets of compute C, token multiplier M, and measured loss
134"
SCALING LAWS FOR DOWNSTREAM ERROR,0.10102095647501344,"L on a validation set to yield (C, M) 7→L. (ii) Fit a scaling law to Equation (5) using pairs of loss L
135"
SCALING LAWS FOR DOWNSTREAM ERROR,0.1015583019881784,"and downstream error Err for models to get L 7→Err. (iii) Chain predictions to get (C, M) 7→Err.
136"
CONSTRUCTING A SCALING TESTBED,0.10209564750134337,"3
Constructing a scaling testbed
137"
CONSTRUCTING A SCALING TESTBED,0.10263299301450833,"In this section, we discuss our experimental setup to test the predictions suggested by Equations (4)
138"
CONSTRUCTING A SCALING TESTBED,0.1031703385276733,"and (5). We first present our general language modeling setup (Section 3.1). Next, we discuss our
139"
CONSTRUCTING A SCALING TESTBED,0.10370768404083826,"strategy for determining model configurations for our scaling investigation (Section 3.2) and fitting
140"
CONSTRUCTING A SCALING TESTBED,0.10424502955400322,"scaling laws (Section 3.3). We then present metrics to validate how well scaling laws predict loss and
141"
CONSTRUCTING A SCALING TESTBED,0.10478237506716818,"downstream performance (Section 3.4).
142"
TRAINING SETUP,0.10531972058033315,"3.1
Training setup
143"
TRAINING SETUP,0.10585706609349813,"We train transformers [116] for next token prediction, based on architectures like GPT-2 [85] and
144"
TRAINING SETUP,0.10639441160666309,"LLaMA [113]. We employ GPT-NeoX [15] as a standardized tokenizer for all data. See Appendix B
145"
TRAINING SETUP,0.10693175711982805,"for architecture, optimization, and hyperparameter details.
146"
MODEL CONFIGURATIONS,0.10746910263299302,"3.2
Model configurations
147"
MODEL CONFIGURATIONS,0.10800644814615798,"To get final configurations for the 0.011B to 0.411B parameter models plotted in Figures 2 and 3, we
148"
MODEL CONFIGURATIONS,0.10854379365932294,"first conduct a wide grid search over a total of 435 models, trained from scratch, from 0.01B to 0.5B
149"
MODEL CONFIGURATIONS,0.1090811391724879,"parameters (Figure 4 (left)). We train on the original OpenLM data mix [39], which largely consists
150"
MODEL CONFIGURATIONS,0.10961848468565287,"of RedPajama [112] and The Pile [31]. While we eventually plan to over-train models, at this step
151"
MODEL CONFIGURATIONS,0.11015583019881783,"we search for base configurations near compute-optimality. We train on 20 tokens per parameter
152"
MODEL CONFIGURATIONS,0.11069317571198281,"(M = 20), which, in early experiments, gives models near the compute-optimal frontier. This is
153"
MODEL CONFIGURATIONS,0.11123052122514777,"similar to findings in Hoffmann et al. [45]’s Table 3, which suggests that M = 20 is near-optimal for
154"
MODEL CONFIGURATIONS,0.11176786673831274,"the Chinchilla experimental setup.
155"
MODEL CONFIGURATIONS,0.1123052122514777,"Table 1: Default number of parameters N and token multiplier M to fit our scaling laws. We
invest ∼100 A100 hours to fit Equation (4) and ∼1,000 A100 hours to fit Equation (5)."
MODEL CONFIGURATIONS,0.11284255776464266,"N
M
Used to fit Equation (4)
Used to fit Equation (5)"
B,0.11337990327780763,"0.011B
20
✓
✓
0.079B
20
✓
✓
0.154B
20
✓
✓
0.411B
20
✓
✓
0.011B
320
✓
✓
1.4B
20
✗
✓"
B,0.11391724879097259,"Total compute C [FLOPs]
2.4e19
2.7e20"
B,0.11445459430413756,"To find maximally performant small-scale models on validation data, we tune model width, number
156"
B,0.11499193981730252,"of layers, number of attention heads, warmup steps, and batch size. Our validation set, OpenLM
157"
B,0.1155292853304675,"eval, contains tokens from recent arXiv papers, the OpenLM codebase itself, and news articles. We
158"
B,0.11606663084363246,"find in early experiments that qk-LayerNorm makes models less sensitive to learning rate, which
159"
B,0.11660397635679742,"is a phenomenon Wortsman et al. [123] report in their Figure 1. Hence, we fix the learning rate
160"
B,0.11714132186996239,"(3e-3) for our sweeps. We also perform smaller grid searches over 1.4B and 6.9B parameter model
161"
B,0.11767866738312735,"configurations at M = 20, retaining the best configurations.
162"
B,0.11821601289629231,"At this point, we have many models, several of which give poor performance; following prior
163"
B,0.11875335840945728,"work [51, 45], we want to keep only models that give best performance. Hence, in Figure 4 (center),
164"
B,0.11929070392262224,"we filter out models that do not lie on the Pareto frontier. While there appears to be a general trend,
165"
B,0.11982804943578722,"configurations between 5.2 × 1016 and 5.2 × 1017 FLOPs lie below the frontier established by other
166"
B,0.12036539494895218,"models. We hypothesize these models over-perform as they are trained for more optimization steps
167"
B,0.12090274046211714,"than their neighbors based on our power-of-two batch sizes. We provide support for this hypothesis
168"
B,0.12144008597528211,"in Appendix E, but opt to remove these models from our investigation.
169"
B,0.12197743148844707,"To ensure tractable compute requirements for our scaling experiments, we require a subset of models
170"
B,0.12251477700161204,"that follows the trend of the entire Pareto frontier. In Figure 4 (right), we fit trends to the Pareto
171"
B,0.123052122514777,"models and to a subset of four models. We notice that the trends closely predict both the performance
172"
B,0.12358946802794196,"of the 1.4B and 6.9B models, suggesting that our small-scale configurations reliably extrapolate in
173"
B,0.12412681354110693,"the compute-optimal setting.
174"
B,0.1246641590542719,"Moving forward, we do not tune hyperparameters for other token multipliers (i.e., M ̸= 20), on
175"
B,0.12520150456743687,"other training or evaluation distributions, or on validation sets for downstream tasks. For more details
176"
B,0.12573885008060182,"including specific hyperparameters, see Appendix C.
177"
B,0.1262761955937668,"To create our scaling testbed, we start with the four small-scale, base configurations from our
178"
B,0.12681354110693177,"grid search: N ∈{0.011B, 0.079B, 0.154B, 0.411B}. To ensure our conclusions are not particular
179"
B,0.12735088662009672,"to a single training distribution, we train models on each of C4 [88, 27], RedPajama [112], and
180"
B,0.1278882321332617,"RefinedWeb [82], which have 138B, 1.15T, and 600B tokens, respectively, for different token
181"
B,0.12842557764642665,"multipliers M ∈{5, 10, 20, 40, 80, 160, 320, 640}. We omit runs that require more tokens than are
182"
B,0.12896292315959162,"present in a dataset (i.e., N = 0.411B, M = 640 for C4). We additionally train N = 1.4B models at
183"
B,0.12950026867275657,"M = 20 and at the largest token multiplier possible without repeating tokens (i.e., 80 for C4, 640 for
184"
B,0.13003761418592155,"RedPajama, and 320 for RefinedWeb). We train N = 6.9B, M = 20 models on each dataset given
185"
B,0.1305749596990865,"the relevance of 7B parameter models [113, 49]. In total this results in a testbed of 104 models.
186"
FITTING SCALING LAWS,0.13111230521225148,"3.3
Fitting scaling laws
187"
FITTING SCALING LAWS,0.13164965072541646,"We fit Equation (4) to approximate E, a, b, η using curve-fitting in SciPy [117] (i.e., Levenberg-
188"
FITTING SCALING LAWS,0.1321869962385814,"Marquardt to minimize non-linear least squares). We repeat this process to fit Equation (5) to
189"
FITTING SCALING LAWS,0.13272434175174638,"approximate ϵ, k, γ. We invest ∼100 A100 hours to train the models required to fit a scaling law for
190"
FITTING SCALING LAWS,0.13326168726491133,"loss and ∼1,000 A100 hours for a corresponding law for downstream error. Unless otherwise specified,
191"
FITTING SCALING LAWS,0.1337990327780763,"we fit to the N, M pairs in Table 1, which are a subset of our full testbed. Our configurations allow
192"
FITTING SCALING LAWS,0.13433637829124126,"us to test for extrapolation to the N = 1.4B, M = 640 (900B token) and the N = 6.9B, M = 20
193"
FITTING SCALING LAWS,0.13487372380440624,"(138B token) regimes.
194 10 20 40 80 160 320 640 M"
B,0.1354110693175712,0.011B
B,0.13594841483073616,0.079B
B,0.13648576034390114,0.154B
B,0.1370231058570661,0.411B
B,0.13756045137023107,1.4B
B,0.13809779688339602,6.9B N
B,0.138635142396561,1.1% 0.0% 0.2% 0.7% 0.9% 0.0% 0.6%
B,0.13917248790972594,2.6% 0.3% 0.2% 0.4% 0.1% 0.7% 0.8%
B,0.13970983342289092,1.5% 0.5% 1.1% 1.1% 3.3% 2.8% 0.6%
B,0.14024717893605587,0.5% 0.2% 0.0% 2.8% 0.2% 2.0%
B,0.14078452444922085,"0.8%
1.5% 4.3%"
B,0.14132186996238583,"Train: C4
Eval: C4 eval 10 20 40 80 160 320 640 M"
B,0.14185921547555078,0.3% 0.0% 0.3% 1.7% 1.1% 0.0% 1.0%
B,0.14239656098871575,2.2% 0.3% 0.2% 0.7% 1.4% 2.1% 2.3%
B,0.1429339065018807,0.8% 0.5% 0.6% 0.0% 0.4% 0.4% 0.3%
B,0.14347125201504568,0.4% 0.2% 0.1% 0.3% 0.3% 1.4% 1.1%
B,0.14400859752821063,"0.1%
0.7% 0.7%"
B,0.1445459430413756,Train: RedPajama
B,0.14508328855454056,Eval: C4 eval 10 20 40 80 160 320 640 M
B,0.14562063406770553,0.9% 0.0% 0.9% 1.9% 1.0% 0.0% 1.1%
B,0.1461579795808705,2.4% 0.1% 0.0% 0.5% 1.2% 2.0% 0.9%
B,0.14669532509403546,0.9% 0.2% 0.6% 2.8% 2.2% 0.8% 0.9%
B,0.14723267060720044,0.2% 0.1% 0.5% 0.8% 0.9% 0.9% 0.3%
B,0.1477700161203654,"0.6%
0.0% 1.6%"
B,0.14830736163353037,Train: RefinedWeb
B,0.14884470714669532,Eval: C4 eval 0.0% 2.0% 4.0% 6.0% 8.0% 10.0%
B,0.1493820526598603,Relative error
B,0.14991939817302524,"Figure 5: Relative error on C4 eval for different training distributions. Boxes highlighted in
yellow correspond to pairs—number of parameters N, token multiplier M—used to fit Equation (4).
Larger values of M correspond to more over-training. The prediction error is low in both interpolation
and extrapolation ranges. Below N = 1.4B, empty squares correspond to runs that were not possible
due to the limited dataset size for single epoch training. At N = 1.4B we run at M = 20 and at the
largest possible multiplier. At N = 6.9B, we run at M = 20."
EVALUATION SETUP,0.15045674368619022,"3.4
Evaluation setup
195"
EVALUATION SETUP,0.1509940891993552,"Evaluation datasets.
Unless otherwise stated, our default validation loss dataset is C4 eval. For
196"
EVALUATION SETUP,0.15153143471252015,"downstream tasks, we adopt a subset from 46 tasks from LLM-foundry [69], which includes standard
197"
EVALUATION SETUP,0.15206878022568512,"tasks with both zero-shot and few-shot evaluations. Specifically, we consider a 17-task subset where,
198"
EVALUATION SETUP,0.15260612573885007,"for each evaluation, at least one 0.154B scale model—trained with as many as 99B tokens—gets
199"
EVALUATION SETUP,0.15314347125201505,"10 percentage points above chance accuracy: ARC-Easy [23], BIG-bench: CS algorithms [11],
200"
EVALUATION SETUP,0.15368081676518,"BIG-bench: Dyck languages [11], BIG-bench: Novel Concepts [11], BIG-bench: Operators [11],
201"
EVALUATION SETUP,0.15421816227834498,"BIG-bench: QA WikiData [11], BoolQ [21], Commonsense QA [107], COPA [92], CoQA [91],
202"
EVALUATION SETUP,0.15475550779150993,"HellaSwag (zero-shot) [126], HellaSwag (10-shot) [126], LAMBADA [77], PIQA [14], PubMed
203"
EVALUATION SETUP,0.1552928533046749,"QA Labeled [50], SQuAD [90], and WinoGrand [55]. For more details on evaluation datasets
204"
EVALUATION SETUP,0.15583019881783988,"see Appendix D. We focus on this subset to ensure we are measuring signal, not noise. Including
205"
EVALUATION SETUP,0.15636754433100483,"downstream tasks like MMLU [40], where performance is close to random chance, however, does
206"
EVALUATION SETUP,0.1569048898441698,"not invalidate our results as we show in our evaluation set ablations (Appendix E).
207"
EVALUATION SETUP,0.15744223535733476,"Metrics.
We consider three main metrics: Validation loss, which is the cross entropy between a
208"
EVALUATION SETUP,0.15797958087049974,"model’s output and the one-hot ground truth token, averaged over all tokens in a sequence and over
209"
EVALUATION SETUP,0.15851692638366469,"all sequences in a dataset. Average top-1 error, which is a uniform average over the 17 downstream
210"
EVALUATION SETUP,0.15905427189682966,"evaluations, as mentioned in the above paragraph. To measure how good a prediction ζ(C, M) is,
211"
EVALUATION SETUP,0.1595916174099946,"we measure Relative prediction error: |ζ(C, M) −ζGT |/ζGT , where ζ is the predicted loss L or the
212"
EVALUATION SETUP,0.1601289629231596,"average top-1 error Err. ζGT is the ground truth measurement to predict.
213"
EVALUATION SETUP,0.16066630843632457,"4
Results: Reliable extrapolation
214"
EVALUATION SETUP,0.16120365394948952,"In this Section, we quantify the extent to which the scaling laws developed in Section 2 extrapolate
215"
EVALUATION SETUP,0.1617409994626545,"larger model performance using the scaling testbed from Section 3. By default, we fit Equations (4)
216"
EVALUATION SETUP,0.16227834497581944,"and (5) to the configurations in Table 1, use C4 eval for loss, and the 17-task split from Section 3.4
217"
EVALUATION SETUP,0.16281569048898442,"for average top-1 error.
218"
EVALUATION SETUP,0.16335303600214937,"Over-trained performance is predictable.
We highlight our main over-training results in
219"
EVALUATION SETUP,0.16389038151531435,"Figure 1 (left). Namely, we are able to extrapolate both in the number of parameters N and the
220"
EVALUATION SETUP,0.16442772702847933,"token multiplier M to closely predict the C4 eval performance of a 1.4B parameter model trained on
221"
EVALUATION SETUP,0.16496507254164428,"900B RedPajama tokens (N = 1.4B, M = 640). Our prediction, which takes 300× less compute
222"
EVALUATION SETUP,0.16550241805480925,"to construct than the final 1.4B run, is accurate to within 0.7% relative error. Additionally, for the
223"
EVALUATION SETUP,0.1660397635679742,"N = 6.9B, M = 20 run, near compute-optimal, the relative error is also 0.7%.
224"
EVALUATION SETUP,0.16657710908113918,"These results support several key takeaways. (i) Scaling can be predictable even when one increases
225"
EVALUATION SETUP,0.16711445459430413,"both the model size and the amount of over-training compared to the training runs used to fit a scaling
226"
EVALUATION SETUP,0.1676518001074691,"law. (ii) The form presented in Equation (4) is useful in practice for predicting over-trained scaling
227"
EVALUATION SETUP,0.16818914562063406,"behavior. (iii) Fitting to Equation (4) gives good prediction accuracy near compute-optimal. More
228"
EVALUATION SETUP,0.16872649113379903,"Table 2: Downstream relative prediction error at 6.9B parameters and 138B tokens. While
predicting accuracy on individual zero-shot downstream evaluations can be challenging (“Individual”),
predicting averages across downstream datasets is accurate (“Avg.”)."
EVALUATION SETUP,0.169263836646964,"Individual top-1 error
Avg. top-1 error"
EVALUATION SETUP,0.16980118216012896,"Train set
ARC-E [23]
LAMBADA [77]
OpenBook QA [68]
HellaSwag [126]
17-task split"
EVALUATION SETUP,0.17033852767329394,"C4 [88, 27]
28.96%
15.01%
16.80%
79.58%
0.14%
RedPajama [112]
5.21%
14.39%
8.44%
25.73%
0.05%
RefinedWeb [82]
26.06%
16.55%
1.92%
81.96%
2.94%"
EVALUATION SETUP,0.1708758731864589,"specifically, predictions are accurate both for the 1.4B over-trained model and the 6.7B compute-
229"
EVALUATION SETUP,0.17141321869962386,"optimal model using a single scaling fit.
230"
EVALUATION SETUP,0.17195056421278881,"While Figure 1 explores a specific case of making predictions in the over-trained regime, we aim to
231"
EVALUATION SETUP,0.1724879097259538,"understand the error profile of our predictions across training datasets, token multipliers, and number
232"
EVALUATION SETUP,0.17302525523911874,"of parameters. Hence, Figure 5 shows the relative error between ground truth loss and predicted
233"
EVALUATION SETUP,0.17356260075228372,"loss on C4 eval for models in our testbed. We notice uniformly low prediction error suggesting that
234"
EVALUATION SETUP,0.1740999462654487,"predictions are accurate in many settings.
235"
EVALUATION SETUP,0.17463729177861365,"Average top-1 error is predictable.
Figure 1 (right) presents our main result in estimating scaling
236"
EVALUATION SETUP,0.17517463729177862,"laws for downstream error. Concretely, we use the models indicated in Table 1 to fit Equations (4)
237"
EVALUATION SETUP,0.17571198280494357,"and (5), chaining the scaling fits to predict the average top-1 error as a function of training compute
238"
EVALUATION SETUP,0.17624932831810855,"C and the token multiplier M. Our fits allow us to predict, using 20× less compute, the downstream
239"
EVALUATION SETUP,0.1767866738312735,"performance of a 6.9B model trained on 138B RedPajama tokens to within 0.05% relative error and a
240"
EVALUATION SETUP,0.17732401934443848,"1.4B model trained on RedPajama 900B tokens to within 3.6% relative error.
241"
EVALUATION SETUP,0.17786136485760343,"Table 2 additionally shows the relative error of our downstream performance predictions for models
242"
EVALUATION SETUP,0.1783987103707684,"trained on C4, RedPajama, and RefinedWeb, indicating that our scaling law functional forms are
243"
EVALUATION SETUP,0.17893605588393338,"applicable on many training datasets. We note that while average accuracy is predictable, individual
244"
EVALUATION SETUP,0.17947340139709833,"downstream task predictions are significantly more noisy. We report relative error for more model
245"
EVALUATION SETUP,0.1800107469102633,"predictions in Figures 11 and 12. We also find that if we remove the 1.4B model for the Equation (5)
246"
EVALUATION SETUP,0.18054809242342826,"fit, relative error jumps, for instance, from 0.05% to 10.64% on the 17-task split for the 6.9B,
247"
EVALUATION SETUP,0.18108543793659324,"138B token RedPajama prediction. This highlights the importance of investing more compute when
248"
EVALUATION SETUP,0.18162278344975819,"constructing scaling laws for downstream task prediction compared to loss prediction.
249"
EVALUATION SETUP,0.18216012896292316,"Under-training, out-of-distribution scaling, and compute-reliability trade-offs.
In addition to
250"
EVALUATION SETUP,0.1826974744760881,"our main results presented above, we include additional results in Appendix E, which we summarize
251"
EVALUATION SETUP,0.1832348199892531,"here. First, we notice that when token multipliers become too small (i.e., M = 5) scaling becomes
252"
EVALUATION SETUP,0.18377216550241807,"unreliable and lies off the trend. Additionally, multipliers other than 20, such as 10, 40, and 80, garner
253"
EVALUATION SETUP,0.18430951101558302,"points that are roughly on the compute optimal frontier (Figure 9). This observation suggests that the
254"
EVALUATION SETUP,0.184846856528748,"compute-optimal multiplier may lie in a range rather than take a single value. To probe the limits
255"
EVALUATION SETUP,0.18538420204191294,"of reliable scaling, we attempt to break our scaling laws in out-of-distribution settings. We find that
256"
EVALUATION SETUP,0.18592154755507792,"models trained on C4—English filtered—and evaluated on next token prediction on code domains
257"
EVALUATION SETUP,0.18645889306824287,"have a high relative error in many cases. Perhaps surprisingly, evaluating the same models on German
258"
EVALUATION SETUP,0.18699623858140785,"next token prediction gives reliable loss scaling (Figure 10). We additionally examine the compute
259"
EVALUATION SETUP,0.1875335840945728,"necessary to create accurate scaling laws, finding that scaling laws can be constructed more cheaply
260"
EVALUATION SETUP,0.18807092960773777,"for loss prediction than for downstream error prediction (Figures 15 and 16).
261"
RELATED WORK,0.18860827512090275,"5
Related work
262"
RELATED WORK,0.1891456206340677,"We review the most closely related work in this section. For additional related work, see Appendix F.
263"
RELATED WORK,0.18968296614723268,"Scaling laws.
Early works on scaling artificial neural networks observe predictable power-law
264"
RELATED WORK,0.19022031166039763,"scaling in the training set size and number of model parameters [43, 44, 93]. Alabdulmohsin et al.
265"
RELATED WORK,0.1907576571735626,"[2] stress the importance of looking at the extrapolation regime of a scaling law. Yang et al. [124]
266"
RELATED WORK,0.19129500268672756,"prescribe architectural and hyperparameter changes when scaling model width to realize performant
267"
RELATED WORK,0.19183234819989253,"models; Yang et al. [125] make analogous recommendations when scaling model depth. Bi et al.
268"
RELATED WORK,0.19236969371305748,"[13] propose hyperparameter aware scaling laws. Unlike the aforementioned work, our investigation
269"
RELATED WORK,0.19290703922622246,"focuses on over-training and predicting downstream accuracy.
270"
RELATED WORK,0.19344438473938744,"Hoffmann et al. [45] investigate how the number of model parameters N and training tokens D
271"
RELATED WORK,0.1939817302525524,"should be chosen to minimize loss L given a compute budget C. Hoffmann et al. [45] find that when
272"
RELATED WORK,0.19451907576571736,"scaling up C, both N and D should be scaled equally up to a multiplicative constant (i.e., N ∝C∼0.5
273"
RELATED WORK,0.1950564212788823,"and D ∝C∼0.5) to realize compute-optimality. Appendix C of the Chinchilla paper additionally
274"
RELATED WORK,0.1955937667920473,"suggests that these findings hold across three datasets. However, Hoffmann et al. [45] do not verify
275"
RELATED WORK,0.19613111230521224,"their scaling laws for training beyond compute-optimality, or for downstream error prediction—both
276"
RELATED WORK,0.19666845781837722,"of which are central to our work.
277"
RELATED WORK,0.1972058033315422,"Sardana & Frankle [98] propose modifications to the Chinchilla formulation to incorporate inference
278"
RELATED WORK,0.19774314884470715,"costs into the definition of compute-optimality and solve for various fixed inference budgets. Their
279"
RELATED WORK,0.19828049435787212,"key finding, which is critical for our work, is that when taking into account a large enough inference
280"
RELATED WORK,0.19881783987103707,"budget, it is optimal to train smaller models for longer than the original Chinchilla recommendations.
281"
RELATED WORK,0.19935518538420205,"Our work presupposes that over-training can be beneficial.
Instead of solving for inference-
282"
RELATED WORK,0.199892530897367,"optimal schemes, we support empirically a predictive theory of scaling in the over-trained regime.
283"
RELATED WORK,0.20042987641053198,"Additionally, we provide experiments across many validation and training sets.
284"
RELATED WORK,0.20096722192369693,"For predicting downstream scaling beyond loss, Isik et al. [47] relate the number of pre-training tokens
285"
RELATED WORK,0.2015045674368619,"to downstream cross-entropy and machine translation BLEU score [78] after fine-tuning. In contrast,
286"
RELATED WORK,0.20204191295002688,"we take a holistic approach to evaluation by looking at top-1 error over many natural language tasks.
287"
RELATED WORK,0.20257925846319183,"Schaeffer et al. [100] argue that emergent abilities [120] are a product of non-linear metrics and
288"
RELATED WORK,0.2031166039763568,"propose smoother alternatives. As a warmup for why non-linear metrics may be hard to predict,
289"
RELATED WORK,0.20365394948952176,"Schaeffer et al. [100] consider predicting an ℓlength sequence exactly: Err(N, ℓ) ≈1 −PP(N)−ℓ,
290"
RELATED WORK,0.20419129500268673,"where N is the number of parameters in a model and PP is its perplexity. This is a special case of
291"
RELATED WORK,0.20472864051585168,"our Equations (5) and (6), where the number of training tokens does not appear, ϵ = 1, k = 1, and
292"
RELATED WORK,0.20526598602901666,"γ = ℓ. In contrast, we treat ϵ, k, γ as free parameters for a scaling law fit, finding that average error
293"
RELATED WORK,0.2058033315421816,"over downstream tasks can make for a predictable metric.
294"
RELATED WORK,0.2063406770553466,"Over-training in popular models.
There has been a rise in over-trained models [113, 114] and
295"
RELATED WORK,0.20687802256851157,"accompanying massive datasets [112, 82, 104, 3]. For example, Chinchilla 70B [45] is trained with a
296"
RELATED WORK,0.20741536808167652,"token multiplier of 20, while LLaMA-2 7B [114] uses a token multiplier of 290. In our investigation,
297"
RELATED WORK,0.2079527135948415,"we look at token multipliers from 5 to 640 to ensure coverage of popular models and relevance for
298"
RELATED WORK,0.20849005910800644,"future models that may be trained on even more tokens.
299"
RELATED WORK,0.20902740462117142,"6
Limitations, future work, and conclusion
300"
RELATED WORK,0.20956475013433637,"Limitations and future work.
We identify limitations, which provide motivation for future work.
301"
RELATED WORK,0.21010209564750135,"• Hyperparameters. While our configurations are surprisingly amenable to reliable scaling across
302"
RELATED WORK,0.2106394411606663,"many training and testing distributions without further tuning, there is a need to develop scaling
303"
RELATED WORK,0.21117678667383127,"laws that do not require extensive hyperparameter sweeps.
304"
RELATED WORK,0.21171413218699625,"• Scaling up. Validating the trends in this paper for even larger runs is a valuable direction.
305"
RELATED WORK,0.2122514777001612,"Additionally, repeating our setup for models that achieve non-trivial performance on harder
306"
RELATED WORK,0.21278882321332618,"evaluations like MMLU is left to future work.
307"
RELATED WORK,0.21332616872649113,"• Scaling down. Actualizing predictable scaling with even cheaper runs is important to make this
308"
RELATED WORK,0.2138635142396561,"area of research more accessible, especially for downstream error prediction.
309"
RELATED WORK,0.21440085975282105,"• Failure cases. While we present a preliminary analysis of when scaling is unreliable, future work
310"
RELATED WORK,0.21493820526598603,"should investigate conditions under which scaling breaks down.
311"
RELATED WORK,0.21547555077915098,"• Post-training. It is common to employ fine-tuning interventions after pre-training, which we do
312"
RELATED WORK,0.21601289629231596,"not consider. Quantifying to what degree over-training the base model provides benefits after
313"
RELATED WORK,0.21655024180548094,"post-training is an open area of research.
314"
RELATED WORK,0.21708758731864589,"• Individual downstream task prediction. While we find that averaging over many task error
315"
RELATED WORK,0.21762493283181086,"metrics can make for a predictable metric, per-task predictions are left to future work.
316"
RELATED WORK,0.2181622783449758,"• In-the-wild performance. Downstream task performance is a proxy for the in-the-wild user
317"
RELATED WORK,0.2186996238581408,"experience. Analyzing scaling trends in the context of this experience is timely.
318"
RELATED WORK,0.21923696937130574,"• Dataset curation. Our work only deals with existing training datasets. Exploring dataset curation
319"
RELATED WORK,0.21977431488447072,"for improved model scaling is another promising direction.
320"
RELATED WORK,0.22031166039763567,"Conclusion.
We show that the loss of over-trained models, trained past compute-optimality, is
321"
RELATED WORK,0.22084900591080064,"predictable. Furthermore, we propose and validate a scaling law relating loss to average downstream
322"
RELATED WORK,0.22138635142396562,"task performance. We hope our work will inspire others to further examine the relationship between
323"
RELATED WORK,0.22192369693713057,"model training and downstream generalization. Our testbed will be made publicly available, and we
324"
RELATED WORK,0.22246104245029555,"hope it will make scaling research more accessible to researchers and practitioners alike.
325"
REFERENCES,0.2229983879634605,"References
326"
REFERENCES,0.22353573347662548,"[1] Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits
327"
REFERENCES,0.22407307898979043,"of large scale pre-training. In International Conference on Learning Representations (ICLR),
328"
REFERENCES,0.2246104245029554,"2022. https://arxiv.org/abs/2110.02095.
329"
REFERENCES,0.22514777001612035,"[2] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling
330"
REFERENCES,0.22568511552928533,"laws in language and vision. In Advances in Neural Information Processing Systems (NeuIPS),
331"
REFERENCES,0.2262224610424503,"2022. https://arxiv.org/abs/2209.06640.
332"
REFERENCES,0.22675980655561526,"[3] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi
333"
REFERENCES,0.22729715206878023,"Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on
334"
REFERENCES,0.22783449758194518,"data selection for language models. arXiv preprint, 2024. https://arxiv.org/abs/2402.
335"
REFERENCES,0.22837184309511016,"16827.
336"
REFERENCES,0.2289091886082751,"[4] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki,
337"
REFERENCES,0.2294465341214401,"Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al.
338"
REFERENCES,0.22998387963460504,"Santacoder: don’t reach for the stars! arXiv preprint, 2023. https://arxiv.org/abs/
339"
REFERENCES,0.23052122514777001,"2301.03988.
340"
REFERENCES,0.231058570660935,"[5] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and
341"
REFERENCES,0.23159591617409994,"Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with
342"
REFERENCES,0.23213326168726492,"operation-based formalisms. In Conference of the North American Chapter of the Association
343"
REFERENCES,0.23267060720042987,"for Computational Linguistics (NACCL), 2019. https://aclanthology.org/N19-1245.
344"
REFERENCES,0.23320795271359485,"[6] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael
345"
REFERENCES,0.2337452982267598,"Voznesensky, Bin Bao, David Berard, Geeta Chauhan, Anjali Chourdia, Will Constable,
346"
REFERENCES,0.23428264373992477,"Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind,
347"
REFERENCES,0.23481998925308975,"Brian Hirsh, Sherlock Huang, Laurent Kirsch, Michael Lazos, Yanbo Liang, Jason Liang,
348"
REFERENCES,0.2353573347662547,"Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark
349"
REFERENCES,0.23589468027941968,"Saroufim, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William
350"
REFERENCES,0.23643202579258463,"Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan,
351"
REFERENCES,0.2369693713057496,"Peng Wu, and Soumith Chintala. Pytorch 2: Faster machine learning through dynamic
352"
REFERENCES,0.23750671681891455,"python bytecode transformation and graph compilation. In International Conference on
353"
REFERENCES,0.23804406233207953,"Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024.
354"
REFERENCES,0.23858140784524448,"https://pytorch.org/blog/pytorch-2-paper-tutorial.
355"
REFERENCES,0.23911875335840946,"[7] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer,
356"
REFERENCES,0.23965609887157444,"Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman,
357"
REFERENCES,0.24019344438473939,"Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh
358"
REFERENCES,0.24073078989790436,"Koura, Brian O’Horo, Jeffrey Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and
359"
REFERENCES,0.2412681354110693,"Veselin Stoyanov. Efficient large scale language modeling with mixtures of experts. In
360"
REFERENCES,0.2418054809242343,"Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. https:
361"
REFERENCES,0.24234282643739924,"//aclanthology.org/2022.emnlp-main.804.
362"
REFERENCES,0.24288017195056422,"[8] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint,
363"
REFERENCES,0.24341751746372917,"2016. https://arxiv.org/abs/1607.06450.
364"
REFERENCES,0.24395486297689414,"[9] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining
365"
REFERENCES,0.24449220849005912,"neural scaling laws. arXiv preprint, 2021. https://arxiv.org/abs/2102.06701.
366"
REFERENCES,0.24502955400322407,"[10] Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Maxim Krikun, Colin Cherry,
367"
REFERENCES,0.24556689951638905,"Behnam Neyshabur, and Orhan Firat. Data scaling laws in nmt: The effect of noise and
368"
REFERENCES,0.246104245029554,"architecture. In International Conference on Machine Learning (ICML), 2022. https:
369"
REFERENCES,0.24664159054271897,"//proceedings.mlr.press/v162/bansal22b.html.
370"
REFERENCES,0.24717893605588392,"[11] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities
371"
REFERENCES,0.2477162815690489,"of language models. In Transactions on Machine Learning Research (TMLR), 2023. https:
372"
REFERENCES,0.24825362708221385,"//openreview.net/forum?id=uyTL5Bvosj.
373"
REFERENCES,0.24879097259537883,"[12] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On
374"
REFERENCES,0.2493283181085438,"the dangers of stochastic parrots: Can language models be too big? In Proceedings ACM
375"
REFERENCES,0.24986566362170876,"conference on fairness, accountability, and transparency (FAccT), 2021. https://dl.acm.
376"
REFERENCES,0.25040300913487373,"org/doi/10.1145/3442188.3445922.
377"
REFERENCES,0.2509403546480387,"[13] DeepSeek-AI Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi
378"
REFERENCES,0.25147770016120363,"Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao,
379"
REFERENCES,0.2520150456743686,"Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He,
380"
REFERENCES,0.2525523911875336,"Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng
381"
REFERENCES,0.25308973670069856,"Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu
382"
REFERENCES,0.25362708221386354,"Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui
383"
REFERENCES,0.25416442772702846,"Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song,
384"
REFERENCES,0.25470177324019344,"Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu
385"
REFERENCES,0.2552391187533584,"Wang, Yaohui Wang, Yongji Wang, Tong Wu, Yu Wu, Xin Xie, Zhenda Xie, Ziwei Xie,
386"
REFERENCES,0.2557764642665234,"Yi Xiong, Hanwei Xu, Ronald X Xu, Yanhong Xu, Dejian Yang, Yu mei You, Shuiping Yu,
387"
REFERENCES,0.2563138097796883,"Xin yuan Yu, Bo Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang,
388"
REFERENCES,0.2568511552928533,"Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou,
389"
REFERENCES,0.25738850080601827,"Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language
390"
REFERENCES,0.25792584631918325,"models with longtermism. arXiv preprint, 2024. https://arxiv.org/abs/2401.02954.
391"
REFERENCES,0.2584631918323482,"[14] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning
392"
REFERENCES,0.25900053734551315,"about physical commonsense in natural language. In Association for the Advancement of
393"
REFERENCES,0.2595378828586781,"Artificial Intelligence (AAAI), 2020. https://arxiv.org/abs/1911.11641.
394"
REFERENCES,0.2600752283718431,"[15] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,
395"
REFERENCES,0.2606125738850081,"Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai
396"
REFERENCES,0.261149919398173,"Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel
397"
REFERENCES,0.261687264911338,"Weinbach. Gpt-neox-20b: An open-source autoregressive language model. BigScience
398"
REFERENCES,0.26222461042450296,"Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models,
399"
REFERENCES,0.26276195593766793,"2022. https://aclanthology.org/2022.bigscience-1.9.
400"
REFERENCES,0.2632993014508329,"[16] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
401"
REFERENCES,0.26383664696399783,"Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
402"
REFERENCES,0.2643739924771628,"Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya
403"
REFERENCES,0.2649113379903278,"Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
404"
REFERENCES,0.26544868350349277,"Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam
405"
REFERENCES,0.2659860290166577,"McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-
406"
REFERENCES,0.26652337452982267,"shot learners.
In Advances in Neural Information Processing Systems (NeurIPS), 2020.
407"
REFERENCES,0.26706072004298764,"https://arxiv.org/abs/2005.14165.
408"
REFERENCES,0.2675980655561526,"[17] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. In
409"
REFERENCES,0.2681354110693176,"International Conference on Learning Representations (ICLR), 2023. https://openreview.
410"
REFERENCES,0.2686727565824825,"net/forum?id=sckjveqlCZ.
411"
REFERENCES,0.2692101020956475,"[18] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade
412"
REFERENCES,0.2697474476088125,"Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling
413"
REFERENCES,0.27028479312197745,"laws for contrastive language-image learning. In Conference on Computer Vision and Pattern
414"
REFERENCES,0.2708221386351424,"Recognition (CVPR), 2023. https://arxiv.org/abs/2212.07143.
415"
REFERENCES,0.27135948414830735,"[19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
416"
REFERENCES,0.27189682966147233,"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
417"
REFERENCES,0.2724341751746373,"Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,
418"
REFERENCES,0.2729715206878023,"Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson,
419"
REFERENCES,0.2735088662009672,"Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju
420"
REFERENCES,0.2740462117141322,"Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García,
421"
REFERENCES,0.27458355722729716,"Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan,
422"
REFERENCES,0.27512090274046214,"Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
423"
REFERENCES,0.27565824825362706,"Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie
424"
REFERENCES,0.27619559376679204,"Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,
425"
REFERENCES,0.276732939279957,"Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason
426"
REFERENCES,0.277270284793122,"Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
427"
REFERENCES,0.27780763030628697,"Palm: Scaling language modeling with pathways. In Journal of Machine Learning Research
428"
REFERENCES,0.2783449758194519,"(JMLR), 2022. https://arxiv.org/abs/2204.02311.
429"
REFERENCES,0.27888232133261687,"[20] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan
430"
REFERENCES,0.27941966684578184,"Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
431"
REFERENCES,0.2799570123589468,"language models. arXiv preprint, 2022. https://arxiv.org/abs/2210.11416.
432"
REFERENCES,0.28049435787211174,"[21] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
433"
REFERENCES,0.2810317033852767,"Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In
434"
REFERENCES,0.2815690488984417,"Conference of the North American Chapter of the Association for Computational Linguistics
435"
REFERENCES,0.2821063944116067,"(NAACL), 2019. https://aclanthology.org/N19-1300.
436"
REFERENCES,0.28264373992477165,"[22] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA:
437"
REFERENCES,0.2831810854379366,"Pre-training text encoders as discriminators rather than generators.
In International
438"
REFERENCES,0.28371843095110155,"Conference on Learning Representations (ICLR), 2020. https://openreview.net/pdf?
439"
REFERENCES,0.28425577646426653,"id=r1xMH1BtvB.
440"
REFERENCES,0.2847931219774315,"[23] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
441"
REFERENCES,0.28533046749059643,"and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
442"
REFERENCES,0.2858678130037614,"challenge. arXiv preprint, 2018. https://arxiv.org/abs/1803.05457.
443"
REFERENCES,0.2864051585169264,"[24] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast
444"
REFERENCES,0.28694250403009136,"and memory-efficient exact attention with IO-awareness. In Advances in Neural Information
445"
REFERENCES,0.28747984954325634,"Processing Systems (NeurIPS), 2022. https://arxiv.org/abs/2205.14135.
446"
REFERENCES,0.28801719505642126,"[25] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
447"
REFERENCES,0.28855454056958624,"Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.
448"
REFERENCES,0.2890918860827512,"Scaling vision transformers to 22 billion parameters. In International Conference on Machine
449"
REFERENCES,0.2896292315959162,"Learning (ICML), 2023. https://proceedings.mlr.press/v202/dehghani23a.html.
450"
REFERENCES,0.2901665771090811,"[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training
451"
REFERENCES,0.2907039226222461,"of deep bidirectional transformers for language understanding. In Conference of the North
452"
REFERENCES,0.29124126813541107,"American Chapter of the Association for Computational Linguistics (NAACL), 2019. https:
453"
REFERENCES,0.29177861364857605,"//aclanthology.org/N19-1423.
454"
REFERENCES,0.292315959161741,"[27] Jesse Dodge, Maarten Sap, Ana Marasovi´c, William Agnew, Gabriel Ilharco, Dirk Groeneveld,
455"
REFERENCES,0.29285330467490595,"Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on
456"
REFERENCES,0.2933906501880709,"the colossal clean crawled corpus. In Conference on Empirical Methods in Natural Language
457"
REFERENCES,0.2939279957012359,"Processing (EMNLP), 2021. https://aclanthology.org/2021.emnlp-main.98.
458"
REFERENCES,0.2944653412144009,"[28] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
459"
REFERENCES,0.2950026867275658,"Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten
460"
REFERENCES,0.2955400322407308,"Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin
461"
REFERENCES,0.29607737775389575,"Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le,
462"
REFERENCES,0.29661472326706073,"Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling of language models
463"
REFERENCES,0.2971520687802257,"with mixture-of-experts. In International Conference on Machine Learning (ICML), 2022.
464"
REFERENCES,0.29768941429339063,"https://arxiv.org/abs/2112.06905.
465"
REFERENCES,0.2982267598065556,"[29] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:
466"
REFERENCES,0.2987641053197206,"Model alignment as prospect theoretic optimization. arXiv preprint, 2024. https://arxiv.
467"
REFERENCES,0.29930145083288556,"org/abs/2402.01306.
468"
REFERENCES,0.2998387963460505,"[30] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao
469"
REFERENCES,0.30037614185921546,"Nguyen, Mitchell Wortsman Ryan Marten, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim
470"
REFERENCES,0.30091348737238044,"Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe,
471"
REFERENCES,0.3014508328855454,"Stephen Mussmann, Mehdi Cherti Richard Vencu, Ranjay Krishna, Pang Wei Koh, Olga
472"
REFERENCES,0.3019881783987104,"Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,
473"
REFERENCES,0.3025255239118753,"Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt.
474"
REFERENCES,0.3030628694250403,"Datacomp: In search of the next generation of multimodal datasets. In Advances in Neural
475"
REFERENCES,0.30360021493820527,"Information Processing Systems (NeurIPS), 2023. https://arxiv.org/abs/2304.14108.
476"
REFERENCES,0.30413756045137025,"[31] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,
477"
REFERENCES,0.30467490596453517,"Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
478"
REFERENCES,0.30521225147770015,"The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint, 2020.
479"
REFERENCES,0.3057495969908651,"https://arxiv.org/abs/2101.00027.
480"
REFERENCES,0.3062869425040301,"[32] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia,
481"
REFERENCES,0.3068242880171951,"Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. arXiv preprint,
482"
REFERENCES,0.30736163353036,"2021. https://arxiv.org/abs/2109.07740.
483"
REFERENCES,0.307898979043525,"[33] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural
484"
REFERENCES,0.30843632455668996,"machine translation. In Conference on Empirical Methods in Natural Language Processing
485"
REFERENCES,0.30897367006985493,"(EMNLP), 2021. https://aclanthology.org/2021.emnlp-main.478.
486"
REFERENCES,0.30951101558301986,"[34] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,
487"
REFERENCES,0.31004836109618483,"Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating
488"
REFERENCES,0.3105857066093498,"the science of language models. arXiv preprint, 2024. https://arxiv.org/abs/2402.
489"
REFERENCES,0.3111230521225148,"00838.
490"
REFERENCES,0.31166039763567976,"[35] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
491"
REFERENCES,0.3121977431488447,"arXiv preprint, 2023. https://arxiv.org/abs/2312.00752.
492"
REFERENCES,0.31273508866200966,"[36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher
493"
REFERENCES,0.31327243417517464,"Ré. Combining recurrent, convolutional, and continuous-time models with linear state space
494"
REFERENCES,0.3138097796883396,"layers. In Advances in Neural Information Processing Systems (NeurIPS), 2021. https:
495"
REFERENCES,0.31434712520150454,"//openreview.net/forum?id=yWd42CWN3c.
496"
REFERENCES,0.3148844707146695,"[37] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with
497"
REFERENCES,0.3154218162278345,"structured state spaces. In International Conference on Learning Representations (ICLR),
498"
REFERENCES,0.3159591617409995,"2022. https://arxiv.org/abs/2111.00396.
499"
REFERENCES,0.31649650725416445,"[38] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar, Teodoro Mendes, Allie Del Giorno,
500"
REFERENCES,0.31703385276732937,"Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi,
501"
REFERENCES,0.31757119828049435,"Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen
502"
REFERENCES,0.3181085437936593,"Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.
Textbooks are all you
503"
REFERENCES,0.3186458893068243,"need. Preprint, 2023. https://www.microsoft.com/en-us/research/publication/
504"
REFERENCES,0.3191832348199892,"textbooks-are-all-you-need.
505"
REFERENCES,0.3197205803331542,"[39] Suchin Gururangan, Mitchell Wortsman, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian,
506"
REFERENCES,0.3202579258463192,"Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard
507"
REFERENCES,0.32079527135948416,"Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt. OpenLM:
508"
REFERENCES,0.32133261687264913,"a minimal but performative language modeling (lm) repository, 2023. https://github.
509"
REFERENCES,0.32186996238581406,"com/mlfoundations/open_lm.
510"
REFERENCES,0.32240730789897903,"[40] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
511"
REFERENCES,0.322944653412144,"Jacob Steinhardt. Measuring massive multitask language understanding. In International
512"
REFERENCES,0.323481998925309,"Conference on Learning Representations (ICLR), 2021. https://arxiv.org/abs/2009.
513"
REFERENCES,0.32401934443847397,"03300.
514"
REFERENCES,0.3245566899516389,"[41] T. J. Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,
515"
REFERENCES,0.32509403546480387,"Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann,
516"
REFERENCES,0.32563138097796884,"Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei,
517"
REFERENCES,0.3261687264911338,"and Sam McCandlish. Scaling laws for autoregressive generative modeling. arXiv preprint,
518"
REFERENCES,0.32670607200429874,"2020. https://arxiv.org/abs/2010.14701.
519"
REFERENCES,0.3272434175174637,"[42] Danny Hernandez, Jared Kaplan, T. J. Henighan, and Sam McCandlish. Scaling laws for
520"
REFERENCES,0.3277807630306287,"transfer. arXiv preprint, 2021. https://arxiv.org/abs/2102.01293.
521"
REFERENCES,0.3283181085437937,"[43] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Frederick Diamos, Heewoo Jun,
522"
REFERENCES,0.32885545405695865,"Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning
523"
REFERENCES,0.3293927995701236,"scaling is predictable, empirically. arXiv preprint, 2017. https://arxiv.org/abs/1712.
524"
REFERENCES,0.32993014508328855,"00409.
525"
REFERENCES,0.33046749059645353,"[44] Joel Hestness, Newsha Ardalani, and Gregory Diamos.
Beyond human-level accuracy:
526"
REFERENCES,0.3310048361096185,"Computational challenges in deep learning.
In Principles and Practice of Parallel
527"
REFERENCES,0.3315421816227834,"Programming (PPoPP), 2019. https://arxiv.org/abs/1909.01736.
528"
REFERENCES,0.3320795271359484,"[45] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,
529"
REFERENCES,0.3326168726491134,"Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,
530"
REFERENCES,0.33315421816227836,"et al. Training compute-optimal large language models. In Advances in Neural Information
531"
REFERENCES,0.33369156367544334,"Processing Systems (NeurIPS), 2022. https://arxiv.org/abs/2203.15556.
532"
REFERENCES,0.33422890918860826,"[46] Hakan Inan, Khashayar Khosravi, and Richard Socher.
Tying word vectors and word
533"
REFERENCES,0.33476625470177324,"classifiers: A loss framework for language modeling. In International Conference on Learning
534"
REFERENCES,0.3353036002149382,"Representations (ICLR), 2017. https://arxiv.org/abs/1611.01462.
535"
REFERENCES,0.3358409457281032,"[47] Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii,
536"
REFERENCES,0.3363782912412681,"and Sanmi Koyejo. Scaling laws for downstream task performance of large language models.
537"
REFERENCES,0.3369156367544331,"arXiv, 2024. https://arxiv.org/abs/2402.04177.
538"
REFERENCES,0.33745298226759807,"[48] Maor Ivgi, Yair Carmon, and Jonathan Berant. Scaling laws under the microscope: Predicting
539"
REFERENCES,0.33799032778076304,"transformer performance from small scale experiments. In Conference on Empirical Methods
540"
REFERENCES,0.338527673293928,"in Natural Language Processing (EMNLP), 2022. https://aclanthology.org/2022.
541"
REFERENCES,0.33906501880709294,"findings-emnlp.544.
542"
REFERENCES,0.3396023643202579,"[49] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
543"
REFERENCES,0.3401397098334229,"Chaplot, Florian Bressand Diego de las Casas, Gianna Lengyel, Guillaume Lample, Lucile
544"
REFERENCES,0.3406770553465879,"Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut
545"
REFERENCES,0.3412144008597528,"Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint,
546"
REFERENCES,0.3417517463729178,"2023. https://arxiv.org/abs/2310.06825.
547"
REFERENCES,0.34228909188608275,"[50] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A
548"
REFERENCES,0.34282643739924773,"dataset for biomedical research question answering. In Conference on Empirical Methods in
549"
REFERENCES,0.3433637829124127,"Natural Language Processing (EMNLP), 2019. https://aclanthology.org/D19-1259.
550"
REFERENCES,0.34390112842557763,"[51] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon
551"
REFERENCES,0.3444384739387426,"Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural
552"
REFERENCES,0.3449758194519076,"language models. arXiv preprint, 2020. https://arxiv.org/abs/2001.08361.
553"
REFERENCES,0.34551316496507256,"[52] Tobit Klug, Dogukan Atik, and Reinhard Heckel. Analyzing the sample complexity of self-
554"
REFERENCES,0.3460505104782375,"supervised image reconstruction methods. arXiv preprint, 2023. https://arxiv.org/abs/
555"
REFERENCES,0.34658785599140246,"2305.19079.
556"
REFERENCES,0.34712520150456744,"[53] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
557"
REFERENCES,0.3476625470177324,"Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv
558"
REFERENCES,0.3481998925308974,"preprint, 2019. http://arxiv.org/abs/1909.11942.
559"
REFERENCES,0.3487372380440623,"[54] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,
560"
REFERENCES,0.3492745835572273,"Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel
561"
REFERENCES,0.34981192907039227,"Haziza. xformers: A modular and hackable transformer modelling library, 2022. https:
562"
REFERENCES,0.35034927458355725,"//github.com/facebookresearch/xformers.
563"
REFERENCES,0.35088662009672217,"[55] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In
564"
REFERENCES,0.35142396560988715,"International conference on the principles of knowledge representation and reasoning, 2012.
565"
REFERENCES,0.3519613111230521,"https://aaai.org/papers/59-4492-the-winograd-schema-challenge.
566"
REFERENCES,0.3524986566362171,"[56] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
567"
REFERENCES,0.3530360021493821,"Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
BART: Denoising sequence-to-
568"
REFERENCES,0.353573347662547,"sequence pre-training for natural language generation, translation, and comprehension. In
569"
REFERENCES,0.354110693175712,"Annual Meeting of the Association for Computational Linguistics (ACL), 2020.
https:
570"
REFERENCES,0.35464803868887695,"//aclanthology.org/2020.acl-main.703.
571"
REFERENCES,0.35518538420204193,"[57] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao
572"
REFERENCES,0.35572272971520685,"Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source
573"
REFERENCES,0.35626007522837183,"be with you! arXiv preprint, 2023. https://arxiv.org/abs/2305.06161.
574"
REFERENCES,0.3567974207415368,"[58] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A
575"
REFERENCES,0.3573347662547018,"challenge dataset for machine reading comprehension with logical reasoning. In International
576"
REFERENCES,0.35787211176786676,"Joint Conference on Artificial Intelligence, 2020. https://arxiv.org/abs/2007.08124.
577"
REFERENCES,0.3584094572810317,"[59] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
578"
REFERENCES,0.35894680279419666,"Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT
579"
REFERENCES,0.35948414830736164,"pretraining approach. arXiv preprint, 2019. http://arxiv.org/abs/1907.11692.
580"
REFERENCES,0.3600214938205266,"[60] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining
581"
REFERENCES,0.36055883933369154,"Xie. A convnet for the 2020s. Conference on Computer Vision and Pattern Recognition
582"
REFERENCES,0.3610961848468565,"(CVPR), 2022. https://arxiv.org/abs/2201.03545.
583"
REFERENCES,0.3616335303600215,"[61] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William
584"
REFERENCES,0.36217087587318647,"Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. The
585"
REFERENCES,0.36270822138635145,"data provenance initiative: A large scale audit of dataset licensing & attribution in ai. arXiv
586"
REFERENCES,0.36324556689951637,"preprint, 2023. https://arxiv.org/abs/2310.16787.
587"
REFERENCES,0.36378291241268135,"[62] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint,
588"
REFERENCES,0.3643202579258463,"2017. https://arxiv.org/abs/1711.05101.
589"
REFERENCES,0.3648576034390113,"[63] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,
590"
REFERENCES,0.3653949489521762,"Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian,
591"
REFERENCES,0.3659322944653412,"Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov,
592"
REFERENCES,0.3664696399785062,"Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo,
593"
REFERENCES,0.36700698549167116,"Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan
594"
REFERENCES,0.36754433100483613,"Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang,
595"
REFERENCES,0.36808167651800106,"Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra,
596"
REFERENCES,0.36861902203116603,"Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu,
597"
REFERENCES,0.369156367544331,"Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane
598"
REFERENCES,0.369693713057496,"Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz
599"
REFERENCES,0.3702310585706609,"Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra,
600"
REFERENCES,0.3707684040838259,"and Harm de Vries. Starcoder 2 and the stack v2: The next generation. arXiv preprint, 2024.
601"
REFERENCES,0.37130574959699086,"https://arxiv.org/abs/2402.19173.
602"
REFERENCES,0.37184309511015584,"[64] Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-
603"
REFERENCES,0.3723804406233208,"Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et al.
604"
REFERENCES,0.37291778613648574,"Fingpt: Large generative models for a small language. In Conference on Empirical Methods
605"
REFERENCES,0.3734551316496507,"in Natural Language Processing (EMNLP), 2023. https://aclanthology.org/2023.
606"
REFERENCES,0.3739924771628157,"emnlp-main.164.
607"
REFERENCES,0.3745298226759807,"[65] Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind
608"
REFERENCES,0.3750671681891456,"Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groenveld, Iz Beltagy,
609"
REFERENCES,0.37560451370231057,"Hanneneh Hajishirz, Noah A. Smith, Kyle Richardson, and Jesse Dodge. Paloma: A benchmark
610"
REFERENCES,0.37614185921547555,"for evaluating language model fit. arXiv preprint, 2023. https://paloma.allen.ai.
611"
REFERENCES,0.3766792047286405,"[66] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large
612"
REFERENCES,0.3772165502418055,"annotated corpus of English: The Penn Treebank.
In Computational Linguistics, 1993.
613"
REFERENCES,0.3777538957549704,"https://aclanthology.org/J93-2004.
614"
REFERENCES,0.3782912412681354,"[67] William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A. Smith.
615"
REFERENCES,0.3788285867813004,"Effects of parameter norm growth during transformer training: Inductive bias from gradient
616"
REFERENCES,0.37936593229446536,"descent. In Conference on Empirical Methods in Natural Language Processing (EMNLP),
617"
REFERENCES,0.3799032778076303,"2021. https://aclanthology.org/2021.emnlp-main.133.
618"
REFERENCES,0.38044062332079526,"[68] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
619"
REFERENCES,0.38097796883396023,"electricity? a new dataset for open book question answering. In Conference on Empirical
620"
REFERENCES,0.3815153143471252,"Methods in Natural Language Processing (EMNLP), 2018. https://arxiv.org/abs/1809.
621"
REFERENCES,0.3820526598602902,"02789.
622"
REFERENCES,0.3825900053734551,"[69] MosaicML. Llm evaluation scores, 2023. https://www.mosaicml.com/llm-evaluation.
623"
REFERENCES,0.3831273508866201,"[70] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,
624"
REFERENCES,0.38366469639978507,"Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al.
625"
REFERENCES,0.38420204191295004,"Crosslingual generalization through multitask finetuning.
In Annual Meeting of the
626"
REFERENCES,0.38473938742611496,"Association for Computational Linguistics (ACL), 2022. https://aclanthology.org/
627"
REFERENCES,0.38527673293927994,"2023.acl-long.891.
628"
REFERENCES,0.3858140784524449,"[71] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue
629"
REFERENCES,0.3863514239656099,"Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack:
630"
REFERENCES,0.3868887694787749,"Instruction tuning code large language models. arXiv preprint, 2023. https://arxiv.org/
631"
REFERENCES,0.3874261149919398,"abs/2308.07124.
632"
REFERENCES,0.3879634605051048,"[72] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus,
633"
REFERENCES,0.38850080601826975,"Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained
634"
REFERENCES,0.38903815153143473,"language models. In Advances in Neural Information Processing Systems (NeuIPS), 2023.
635"
REFERENCES,0.38957549704459965,"https://arxiv.org/abs/2305.16264.
636"
REFERENCES,0.3901128425577646,"[73] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet
637"
REFERENCES,0.3906501880709296,"Singh, and Douwe Kiela. Generative representational instruction tuning. arXiv preprint, 2024.
638"
REFERENCES,0.3911875335840946,"https://arxiv.org/abs/2402.09906.
639"
REFERENCES,0.39172487909725956,"[74] Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig,
640"
REFERENCES,0.3922622246104245,"Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech
641"
REFERENCES,0.39279957012358946,"Kryscinski, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng,
642"
REFERENCES,0.39333691563675444,"Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan
643"
REFERENCES,0.3938742611499194,"Joty, and Caiming Xiong. Long sequence modeling with xgen: A 7b llm trained on 8k input
644"
REFERENCES,0.3944116066630844,"sequence length. arXiv preprint, 2023. https://arxiv.org/abs/2309.03450.
645"
REFERENCES,0.3949489521762493,"[75] OpenAI. Triton, 2021. https://github.com/openai/triton.
646"
REFERENCES,0.3954862976894143,"[76] OpenAI. Gpt-4 technical report, 2023. https://arxiv.org/abs/2303.08774.
647"
REFERENCES,0.39602364320257927,"[77] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella
648"
REFERENCES,0.39656098871574424,"Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The
649"
REFERENCES,0.39709833422890917,"LAMBADA dataset: Word prediction requiring a broad discourse context. In Annual Meeting
650"
REFERENCES,0.39763567974207414,"of the Association for Computational Linguistics (ACL), 2016. http://www.aclweb.org/
651"
REFERENCES,0.3981730252552391,"anthology/P16-1144.
652"
REFERENCES,0.3987103707684041,"[78] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
653"
REFERENCES,0.3992477162815691,"evaluation of machine translation. In Annual Meeting of the Association for Computational
654"
REFERENCES,0.399785061794734,"Linguistics (ACL), 2002. https://aclanthology.org/P02-1040.
655"
REFERENCES,0.400322407307899,"[79] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana
656"
REFERENCES,0.40085975282106395,"Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for
657"
REFERENCES,0.40139709833422893,"question answering. In Annual Meeting of the Association for Computational Linguistics
658"
REFERENCES,0.40193444384739385,"(ACL), 2022. https://aclanthology.org/2022.findings-acl.165.
659"
REFERENCES,0.40247178936055883,"[80] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
660"
REFERENCES,0.4030091348737238,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
661"
REFERENCES,0.4035464803868888,"style, high-performance deep learning library. In Advances in Neural Information Processing
662"
REFERENCES,0.40408382590005376,"Systems (NeurIPS), 2019. https://arxiv.org/abs/1912.01703.
663"
REFERENCES,0.4046211714132187,"[81] Patronus AI. EnterprisePII dataset, 2023. https://tinyurl.com/2r5x9bst.
664"
REFERENCES,0.40515851692638366,"[82] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro
665"
REFERENCES,0.40569586243954864,"Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The
666"
REFERENCES,0.4062332079527136,"RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web
667"
REFERENCES,0.40677055346587854,"data only. arXiv preprint, 2023. https://arxiv.org/abs/2306.01116.
668"
REFERENCES,0.4073078989790435,"[83] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman,
669"
REFERENCES,0.4078452444922085,"Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella,
670"
REFERENCES,0.40838259000537347,"Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong,
671"
REFERENCES,0.40891993551853845,"Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi
672"
REFERENCES,0.40945728103170337,"Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Wo´zniak, Zhenyuan Zhang,
673"
REFERENCES,0.40999462654486835,"Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer
674"
REFERENCES,0.4105319720580333,"era. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023.
675"
REFERENCES,0.4110693175711983,"https://aclanthology.org/2023.findings-emnlp.936.
676"
REFERENCES,0.4116066630843632,"[84] Ofir Press and Lior Wolf. Using the output embedding to improve language models. In
677"
REFERENCES,0.4121440085975282,"Proceedings of the Conference of the European Chapter of the Association for Computational
678"
REFERENCES,0.4126813541106932,"Linguistics (EACL), 2017. https://aclanthology.org/E17-2025.
679"
REFERENCES,0.41321869962385815,"[85] Alec Radford,
Jeff Wu,
Rewon Child,
David Luan,
Dario Amodei,
and Ilya
680"
REFERENCES,0.41375604513702313,"Sutskever.
Language models are unsupervised multitask learners.
Preprint, 2019.
681"
REFERENCES,0.41429339065018805,"https://d4mucfpksywv.cloudfront.net/better-language-models/language_
682"
REFERENCES,0.41483073616335303,"models_are_unsupervised_multitask_learners.pdf.
683"
REFERENCES,0.415368081676518,"[86] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis
684"
REFERENCES,0.415905427189683,"Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford,
685"
REFERENCES,0.4164427727028479,"Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche,
686"
REFERENCES,0.4169801182160129,"Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl,
687"
REFERENCES,0.41751746372917786,"Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins,
688"
REFERENCES,0.41805480924234284,"Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar,
689"
REFERENCES,0.4185921547555078,"Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini,
690"
REFERENCES,0.41912950026867274,"L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
691"
REFERENCES,0.4196668457818377,"Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
692"
REFERENCES,0.4202041912950027,"Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias
693"
REFERENCES,0.42074153680816767,"Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi,
694"
REFERENCES,0.4212788823213326,"Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris
695"
REFERENCES,0.42181622783449757,"Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura Weidinger, Iason
696"
REFERENCES,0.42235357334766255,"Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
697"
REFERENCES,0.4228909188608275,"Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu,
698"
REFERENCES,0.4234282643739925,"and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training
699"
REFERENCES,0.4239656098871574,"gopher. arXiv preprint, 2021. https://arxiv.org/abs/2112.11446.
700"
REFERENCES,0.4245029554003224,"[87] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and
701"
REFERENCES,0.4250403009134874,"Chelsea Finn. Direct preference optimization: Your language model is secretly a reward
702"
REFERENCES,0.42557764642665236,"model. In Advances in Neural Information Processing Systems (NeurIPS), 2023. https:
703"
REFERENCES,0.4261149919398173,"//arxiv.org/abs/2305.18290.
704"
REFERENCES,0.42665233745298226,"[88] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
705"
REFERENCES,0.42718968296614723,"Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
706"
REFERENCES,0.4277270284793122,"text-to-text transformer. arXiv preprint, 2019. https://arxiv.org/abs/1910.10683.
707"
REFERENCES,0.4282643739924772,"[89] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
708"
REFERENCES,0.4288017195056421,"Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified
709"
REFERENCES,0.4293390650188071,"text-to-text transformer. In The Journal of Machine Learning Research (JMLR), 2020. https:
710"
REFERENCES,0.42987641053197206,"//arxiv.org/abs/1910.10683.
711"
REFERENCES,0.43041375604513704,"[90] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+
712"
REFERENCES,0.43095110155830196,"questions for machine comprehension of text. In Conference on Empirical Methods in Natural
713"
REFERENCES,0.43148844707146694,"Language Processing (EMNLP), 2016. https://aclanthology.org/D16-1264.
714"
REFERENCES,0.4320257925846319,"[91] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question
715"
REFERENCES,0.4325631380977969,"answering challenge. In Transactions of the Association for Computational Linguistics (TACL),
716"
REFERENCES,0.4331004836109619,"2019. https://aclanthology.org/Q19-1016.
717"
REFERENCES,0.4336378291241268,"[92] Melissa Roemmele, Cosmin Adrian Bejan, , and Andrew S. Gordon. Choice of plausible
718"
REFERENCES,0.43417517463729177,"alternatives: An evaluation of commonsense causal reasoning.
In Association for the
719"
REFERENCES,0.43471252015045675,"Advancement of Artificial Intelligence (AAAI) Spring Symposium, 2011. https://people.
720"
REFERENCES,0.4352498656636217,"ict.usc.edu/~gordon/copa.html.
721"
REFERENCES,0.43578721117678665,"[93] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive
722"
REFERENCES,0.4363245566899516,"prediction of the generalization error across scales. In International Conference on Learning
723"
REFERENCES,0.4368619022031166,"Representations (ICLR), 2020. https://arxiv.org/abs/1909.12673.
724"
REFERENCES,0.4373992477162816,"[94] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias
725"
REFERENCES,0.43793659322944656,"in coreference resolution. In Conference of the North American Chapter of the Association for
726"
REFERENCES,0.4384739387426115,"Computational Linguistics (NAACL), 2018. https://aclanthology.org/N18-2002.
727"
REFERENCES,0.43901128425577646,"[95] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
728"
REFERENCES,0.43954862976894143,"adversarial winograd schema challenge at scale. arXiv preprint, 2019. https://arxiv.org/
729"
REFERENCES,0.4400859752821064,"abs/1907.10641.
730"
REFERENCES,0.44062332079527133,"[96] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled
731"
REFERENCES,0.4411606663084363,"version of bert: smaller, faster, cheaper and lighter. arXiv preprint, 2019. http://arxiv.
732"
REFERENCES,0.4416980118216013,"org/abs/1910.01108.
733"
REFERENCES,0.44223535733476627,"[97] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa:
734"
REFERENCES,0.44277270284793124,"Commonsense reasoning about social interactions. In Empirical Methods in Natural Language
735"
REFERENCES,0.44331004836109617,"Processing (EMNLP), 2019. https://aclanthology.org/D19-1454.
736"
REFERENCES,0.44384739387426114,"[98] Nikhil Sardana and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference
737"
REFERENCES,0.4443847393874261,"in language model scaling laws. In NeurIPS Workshop on Efficient Natural Language and
738"
REFERENCES,0.4449220849005911,"Speech Processing (ENLSP), 2023. https://arxiv.org/abs/2401.00448.
739"
REFERENCES,0.445459430413756,"[99] Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari,
740"
REFERENCES,0.445996775926921,"Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. What language
741"
REFERENCES,0.446534121440086,"model to train if you have one million gpu hours? In Conference on Empirical Methods
742"
REFERENCES,0.44707146695325095,"in Natural Language Processing (EMNLP), 2022. https://aclanthology.org/2022.
743"
REFERENCES,0.44760881246641593,"findings-emnlp.54.
744"
REFERENCES,0.44814615797958085,"[100] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language
745"
REFERENCES,0.4486835034927458,"models a mirage? In Advances in Neural Information Processing Systems (NeurIPS), 2023.
746"
REFERENCES,0.4492208490059108,"https://arxiv.org/abs/2304.15004.
747"
REFERENCES,0.4497581945190758,"[101] Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data
748"
REFERENCES,0.4502955400322407,"manifold. In Journal of Machine Learning Research (JMLR), 2022. https://arxiv.org/
749"
REFERENCES,0.4508328855454057,"abs/2004.10802.
750"
REFERENCES,0.45137023105857066,"[102] Noam Shazeer. Glu variants improve transformer. arXiv preprint, 2020. https://arxiv.
751"
REFERENCES,0.45190757657173564,"org/abs/2002.05202.
752"
REFERENCES,0.4524449220849006,"[103] Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F Karlsson, Abinaya Mahendiran,
753"
REFERENCES,0.45298226759806554,"Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al.
754"
REFERENCES,0.4535196131112305,"Aya dataset: An open-access collection for multilingual instruction tuning. arXiv preprint
755"
REFERENCES,0.4540569586243955,"arXiv:2402.06619, 2024. https://arxiv.org/abs/2402.06619.
756"
REFERENCES,0.45459430413756047,"[104] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell
757"
REFERENCES,0.4551316496507254,"Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open
758"
REFERENCES,0.45566899516389037,"corpus of three trillion tokens for language model pretraining research. arXiv preprint, 2024.
759"
REFERENCES,0.45620634067705534,"https://arxiv.org/abs/2402.00159.
760"
REFERENCES,0.4567436861902203,"[105] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond
761"
REFERENCES,0.4572810317033853,"neural scaling laws: beating power law scaling via data pruning. In Advances in Neural
762"
REFERENCES,0.4578183772165502,"Information Processing Systems (NeurIPS), 2022. https://openreview.net/forum?id=
763"
REFERENCES,0.4583557227297152,"UmvSlP-PyV.
764"
REFERENCES,0.4588930682428802,"[106] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
765"
REFERENCES,0.45943041375604515,"Enhanced transformer with rotary position embedding. arXiv preprint, 2021. https://
766"
REFERENCES,0.4599677592692101,"arxiv.org/abs/2104.09864.
767"
REFERENCES,0.46050510478237505,"[107] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA:
768"
REFERENCES,0.46104245029554003,"A question answering challenge targeting commonsense knowledge. In Conference of the
769"
REFERENCES,0.461579795808705,"North American Chapter of the Association for Computational Linguistics (NAACL), 2019.
770"
REFERENCES,0.46211714132187,"https://aclanthology.org/N19-1421.
771"
REFERENCES,0.4626544868350349,"[108] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won
772"
REFERENCES,0.4631918323481999,"Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler.
Scale
773"
REFERENCES,0.46372917786136486,"efficiently:
Insights from pre-training and fine-tuning transformers.
In International
774"
REFERENCES,0.46426652337452984,"Conference on Learning Representations (ICLR), 2022. https://openreview.net/forum?
775"
REFERENCES,0.4648038688876948,"id=f2OYVDyfIB.
776"
REFERENCES,0.46534121440085974,"[109] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Chung, William Fedus, Jinfeng Rao,
777"
REFERENCES,0.4658785599140247,"Sharan Narang, Vinh Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model
778"
REFERENCES,0.4664159054271897,"architectures: How does inductive bias influence scaling?
In Conference on Empirical
779"
REFERENCES,0.46695325094035467,"Methods in Natural Language Processing (EMNLP), 2023. https://aclanthology.org/
780"
REFERENCES,0.4674905964535196,"2023.findings-emnlp.825.
781"
REFERENCES,0.46802794196668457,"[110] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially
782"
REFERENCES,0.46856528747984955,"usable llms, 2023. www.mosaicml.com/blog/mpt-7b.
783"
REFERENCES,0.4691026329930145,"[111] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,
784"
REFERENCES,0.4696399785061795,"Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee,
785"
REFERENCES,0.4701773240193444,"Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun,
786"
REFERENCES,0.4707146695325094,"Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,
787"
REFERENCES,0.4712520150456744,"Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch,
788"
REFERENCES,0.47178936055883935,"Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel
789"
REFERENCES,0.4723267060720043,"Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen,
790"
REFERENCES,0.47286405158516925,"Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,
791"
REFERENCES,0.47340139709833423,"Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm,
792"
REFERENCES,0.4739387426114992,"Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-
793"
REFERENCES,0.4744760881246642,"Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog
794"
REFERENCES,0.4750134336378291,"applications. arXiv preprint, 2022. https://arxiv.org/abs/2201.08239.
795"
REFERENCES,0.4755507791509941,"[112] Together Computer. Redpajama: an open dataset for training large language models, 2023.
796"
REFERENCES,0.47608812466415906,"https://github.com/togethercomputer/RedPajama-Data.
797"
REFERENCES,0.47662547017732404,"[113] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
798"
REFERENCES,0.47716281569048896,"Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien
799"
REFERENCES,0.47770016120365394,"Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and
800"
REFERENCES,0.4782375067168189,"Efficient Foundation Language Models. arXiv preprint, 2023. https://arxiv.org/abs/
801"
REFERENCES,0.4787748522299839,"2302.13971.
802"
REFERENCES,0.47931219774314887,"[114] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
803"
REFERENCES,0.4798495432563138,"Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,
804"
REFERENCES,0.48038688876947877,"Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
805"
REFERENCES,0.48092423428264375,"Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman
806"
REFERENCES,0.4814615797958087,"Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
807"
REFERENCES,0.48199892530897365,"Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne
808"
REFERENCES,0.4825362708221386,"Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
809"
REFERENCES,0.4830736163353036,"Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,
810"
REFERENCES,0.4836109618484686,"Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael
811"
REFERENCES,0.48414830736163356,"Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,
812"
REFERENCES,0.4846856528747985,"Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie
813"
REFERENCES,0.48522299838796346,"Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
814"
REFERENCES,0.48576034390112843,"Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint, 2023.
815"
REFERENCES,0.4862976894142934,"https://arxiv.org/abs/2307.09288.
816"
REFERENCES,0.48683503492745833,"[115] Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke
817"
REFERENCES,0.4873723804406233,"Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al.
Aya model:
818"
REFERENCES,0.4879097259537883,"An instruction finetuned open-access multilingual language model. arXiv preprint, 2024.
819"
REFERENCES,0.48844707146695326,"https://arxiv.org/abs/2402.07827.
820"
REFERENCES,0.48898441698011824,"[116] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
821"
REFERENCES,0.48952176249328316,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural
822"
REFERENCES,0.49005910800644814,"Information Processing Systems (NeurIPS), 2017. https://arxiv.org/abs/1706.03762.
823"
REFERENCES,0.4905964535196131,"[117] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
824"
REFERENCES,0.4911337990327781,"Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J.
825"
REFERENCES,0.491671144545943,"van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew
826"
REFERENCES,0.492208490059108,"R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W.
827"
REFERENCES,0.49274583557227297,"Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.
828"
REFERENCES,0.49328318108543795,"Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul
829"
REFERENCES,0.4938205265986029,"van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific
830"
REFERENCES,0.49435787211176785,"Computing in Python. Nature Methods, 2020. https://rdcu.be/b08Wh.
831"
REFERENCES,0.4948952176249328,"[118] Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and
832"
REFERENCES,0.4954325631380978,"Nan Duan. From lsat: The progress and challenges of complex reasoning. Transactions on
833"
REFERENCES,0.4959699086512628,"Audio, Speech, and Language Processing, 2021. https://arxiv.org/abs/2108.00648.
834"
REFERENCES,0.4965072541644277,"[119] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
835"
REFERENCES,0.4970445996775927,"Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In
836"
REFERENCES,0.49758194519075766,"International Conference on Learning Representations (ICLR), 2022. https://openreview.
837"
REFERENCES,0.49811929070392263,"net/forum?id=gEZrGCozdqR.
838"
REFERENCES,0.4986566362170876,"[120] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
839"
REFERENCES,0.49919398173025253,"Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,
840"
REFERENCES,0.4997313272434175,"Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
Emergent abilities of large
841"
REFERENCES,0.5002686727565825,"language models. In Transactions on Machine Learning Research (TMLR), 2022. https:
842"
REFERENCES,0.5008060182697475,"//openreview.net/forum?id=yzkSU5zdwD.
843"
REFERENCES,0.5013433637829124,"[121] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang,
844"
REFERENCES,0.5018807092960774,"Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of
845"
REFERENCES,0.5024180548092424,"harm from language models. arXiv preprint, 2021. https://arxiv.org/abs/2112.04359.
846"
REFERENCES,0.5029554003224073,"[122] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana
847"
REFERENCES,0.5034927458355722,"Ili´c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al.
848"
REFERENCES,0.5040300913487372,"Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint, 2022.
849"
REFERENCES,0.5045674368619022,"https://arxiv.org/abs/2211.05100.
850"
REFERENCES,0.5051047823750672,"[123] Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D
851"
REFERENCES,0.5056421278882322,"Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for
852"
REFERENCES,0.5061794734013971,"large-scale transformer training instabilities. arXiv preprint, 2023. https://arxiv.org/
853"
REFERENCES,0.5067168189145621,"abs/2309.14322.
854"
REFERENCES,0.5072541644277271,"[124] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick
855"
REFERENCES,0.507791509940892,"Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs V: Tuning large
856"
REFERENCES,0.5083288554540569,"neural networks via zero-shot hyperparameter transfer. In Advances in Neural Information
857"
REFERENCES,0.5088662009672219,"Processing Systems (NeuIPS), 2021. https://arxiv.org/abs/2203.03466.
858"
REFERENCES,0.5094035464803869,"[125] Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Feature learning in infinite depth
859"
REFERENCES,0.5099408919935519,"neural networks. In International Conference on Learning Representations (ICLR), 2024.
860"
REFERENCES,0.5104782375067168,"https://openreview.net/forum?id=17pVDnpwwl.
861"
REFERENCES,0.5110155830198818,"[126] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a
862"
REFERENCES,0.5115529285330468,"machine really finish your sentence? In Annual Meeting of the Association for Computational
863"
REFERENCES,0.5120902740462118,"Linguistics (ACL), 2019. https://aclanthology.org/P19-1472.
864"
REFERENCES,0.5126276195593766,"[127] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
Scaling vision
865"
REFERENCES,0.5131649650725416,"transformers. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.
866"
REFERENCES,0.5137023105857066,"https://arxiv.org/abs/2106.04560.
867"
REFERENCES,0.5142396560988716,"[128] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Advances in Neural
868"
REFERENCES,0.5147770016120365,"Information Processing Systems (NeuIPS), 2019. https://arxiv.org/abs/1910.07467.
869"
REFERENCES,0.5153143471252015,"[129] Biao Zhang, Ivan Titov, and Rico Sennrich. Improving deep transformer with depth-scaled
870"
REFERENCES,0.5158516926383665,"initialization and merged attention. In Empirical Methods in Natural Language Processing
871"
REFERENCES,0.5163890381515315,"(EMNLP), 2019. https://aclanthology.org/D19-1083.
872"
REFERENCES,0.5169263836646965,"[130] Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less
873"
REFERENCES,0.5174637291778613,"Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard
874"
REFERENCES,0.5180010746910263,"Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li. Pytorch fsdp: Experiences on scaling
875"
REFERENCES,0.5185384202041913,"fully sharded data parallel. In Very Large Data Bases Conference (VLDB), 2023. https:
876"
REFERENCES,0.5190757657173563,"//dl.acm.org/doi/10.14778/3611540.3611569.
877"
REFERENCES,0.5196131112305212,"[131] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun.
878"
REFERENCES,0.5201504567436862,"Jec-qa: A legal-domain question answering dataset. In Association for the Advancement of
879"
REFERENCES,0.5206878022568512,"Artificial Intelligence (AAAI), 2020. https://arxiv.org/abs/1911.12011.
880"
REFERENCES,0.5212251477700162,"[132] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
881"
REFERENCES,0.5217624932831811,"Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation
882"
REFERENCES,0.522299838796346,"models. arXiv preprint, 2023. https://arxiv.org/abs/2304.06364.
883"
REFERENCES,0.522837184309511,"[133] Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries,
884"
REFERENCES,0.523374529822676,"Qian Liu, and Niklas Muennighoff. Astraios: Parameter-efficient instruction tuning code large
885"
REFERENCES,0.5239118753358409,"language models. arXiv preprint, 2024. https://arxiv.org/abs/2401.00788.
886"
REFERENCES,0.5244492208490059,"Contents
887"
INTRODUCTION,0.5249865663621709,"1
Introduction
1
888"
DEVELOPING SCALING LAWS FOR OVER-TRAINING AND DOWNSTREAM TASKS,0.5255239118753359,"2
Developing scaling laws for over-training and downstream tasks
2
889"
PRELIMINARIES,0.5260612573885008,"2.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
890"
PRELIMINARIES,0.5265986029016658,"2.2
Scaling laws for over-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
891"
PRELIMINARIES,0.5271359484148307,"2.3
Scaling laws for downstream error . . . . . . . . . . . . . . . . . . . . . . . . . .
4
892"
CONSTRUCTING A SCALING TESTBED,0.5276732939279957,"3
Constructing a scaling testbed
5
893"
CONSTRUCTING A SCALING TESTBED,0.5282106394411606,"3.1
Training setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
894"
CONSTRUCTING A SCALING TESTBED,0.5287479849543256,"3.2
Model configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
895"
CONSTRUCTING A SCALING TESTBED,0.5292853304674906,"3.3
Fitting scaling laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
896"
CONSTRUCTING A SCALING TESTBED,0.5298226759806556,"3.4
Evaluation setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
897"
CONSTRUCTING A SCALING TESTBED,0.5303600214938206,"4
Results: Reliable extrapolation
7
898"
RELATED WORK,0.5308973670069855,"5
Related work
8
899"
RELATED WORK,0.5314347125201505,"6
Limitations, future work, and conclusion
9
900"
RELATED WORK,0.5319720580333154,"A Scaling-law derivations
22
901"
RELATED WORK,0.5325094035464804,"B Additional training details
23
902"
RELATED WORK,0.5330467490596453,"C Additional grid search details
23
903"
RELATED WORK,0.5335840945728103,"D Evaluation dataset details
23
904"
RELATED WORK,0.5341214400859753,"E Additional results
23
905"
RELATED WORK,0.5346587855991403,"F
Additional related work
30
906"
RELATED WORK,0.5351961311123052,"G Broader impact
31
907"
RELATED WORK,0.5357334766254702,"H Licensing
31
908"
RELATED WORK,0.5362708221386352,"A
Scaling-law derivations
909"
RELATED WORK,0.5368081676518001,"We first show that reparameterizing Equation (3) in terms of the compute C and token multiplier M
910"
RELATED WORK,0.537345513164965,"for α = β yields Equation (4). Combining C = 6ND and M = D/N yields N =
p"
RELATED WORK,0.53788285867813,"C/(6M) and
911 D =
p"
RELATED WORK,0.538420204191295,"CM/6. Inserting these into Equation (3) yields,
912"
RELATED WORK,0.53895754970446,"L(C, M) = E + A
 C"
M,0.539494895217625,6M −α
M,0.5400322407307899,"2
+ B
CM 6 −α 2
, = E +  A
1 6 −α"
M,0.5405695862439549,"2
M
α"
M,0.5411069317571199,"2 + B
1 6 −α"
M,0.5416442772702847,"2
M −α 2 ! C−α 2 ."
M,0.5421816227834497,"This is equal to Equation (4), making the substitutions η = α/2, a = A(1/6)−η, b = B(1/6)−η, as
913"
M,0.5427189682966147,"noted in the main body.
914"
M,0.5432563138097797,"Relation to compute-optimal training.
Recall that we made the assumption α = β, which implies
915"
M,0.5437936593229447,"equal scaling of parameters and tokens to realize compute-optimal models. While this assumption
916"
M,0.5443310048361096,"is empirically justified [45], even if α ̸= β, we get a parameterization that implies the power law
917"
M,0.5448683503492746,"exponent in Equation (4) remains constant with over-training, while the power law scalar changes.
918"
M,0.5454056958624396,"To find a compute-optimal training setting, Hoffmann et al. [45] propose to minimize the right-hand
919"
M,0.5459430413756046,"side of Equation (3) subject to the compute constraint C = 6ND. This yields, N ∗= γ
1
α+β (C/6)"
M,0.5464803868887694,"β
α+β
920"
M,0.5470177324019344,"and D∗= γ−
1
α+β (C/6)
α
α+β , where γ = αA"
M,0.5475550779150994,"βB , for notational convenience. The associated risk is,
921"
M,0.5480924234282644,"L(N ∗, D∗) = E +

Aγ"
M,0.5486297689414293,"−α
β+α + Bγ"
M,0.5491671144545943,"β
β+α
 C 6 −αβ α+β
."
M,0.5497044599677593,"We now deviate from compute-optimal training by modifying the model size and tokens by
922"
M,0.5502418054809243,"multiplication with a constant √m, according to
923"
M,0.5507791509940893,"Nm =
1
√mN ∗,
Dm = √mD∗.
(7)"
M,0.5513164965072541,"This modification keeps the compute constant (i.e., 6NmDm = 6N ∗D∗). The risk, then, becomes
924"
M,0.5518538420204191,"L(fNm,Dm) = E +

m
α 2 Aγ"
M,0.5523911875335841,"−α
β+α + m−β 2 Bγ"
M,0.552928533046749,"β
β+α

C−αβ"
M,0.553465878559914,"α+β .
(8)"
M,0.554003224073079,"We again expect the same power law exponent and changing power law scalar. Note that m in
925"
M,0.554540569586244,"Equation (8) is similar to M in Equation (4). Specifically, m is a multiple of the Chinchilla-optimal
926"
M,0.555077915099409,"token multiplier M ∗= D∗/N ∗, which is no longer fixed as a compute budget changes for α ̸= β.
927"
M,0.5556152606125739,"Table 3: Main models and hyperparameters used in our investigation. Models have number of
parameters N, with number of layers nlayers, number of attention heads nheads, model width dmodel,
and width per attention head dhead. Batch sizes are global and in units of sequences. Each sequence
has 2,048 tokens. A100 GPU hours are at M = 20, which are near compute-optimal runs. For the
1.4B scale, a batch size of 256 performs slightly better than 512."
M,0.5561526061257388,"N
nlayers
nheads
dmodel
dhead
Warmup
Learning rate
Batch size
M = 20 A100 hours"
B,0.5566899516389038,"0.011B
8
4
96
24
100
3e-3
64
0.3
0.079B
8
4
512
128
400
3e-3
512
5
0.154B
24
8
576
72
400
3e-3
512
12
0.411B
24
8
1,024
128
2,000
3e-3
512
75
1.4B
24
16
2,048
128
5,000
3e-3
256
690
6.9B
32
32
4,096
128
5,000
3e-4
2,048
17,000"
B,0.5572272971520688,"B
Additional training details
928"
B,0.5577646426652337,"Architecture.
As stated in the main paper, we train transformers [116], based on auto-
929"
B,0.5583019881783987,"regressive, decoder-only, pre-normalization architectures like GPT-2 [85] and LLaMA [113]. We
930"
B,0.5588393336915637,"adopt OpenLM [39] for modeling, which utilizes PyTorch [80, 6], xformers [54], triton [75],
931"
B,0.5593766792047287,"FlashAttention [24], FSDP [130], and bfloat16 automatic mixed precision. Like LLaMA, we omit
932"
B,0.5599140247178936,"bias terms, but replace RMSNorm [128] with LayerNorm [8], which has readily available fused
933"
B,0.5604513702310586,"implementations. Following Wortsman et al. [123], we apply qk-LayerNorm [25], which adds
934"
B,0.5609887157442235,"robustness to otherwise poor hyperparameter choices (e.g., learning rate). We use SwiGLU [102]
935"
B,0.5615260612573885,"activations and depth-scaled initialization [129]. We use a sequence length of 2,048, rotary positional
936"
B,0.5620634067705534,"embeddings [106], and the GPT-NeoX-20B tokenizer [15], which yields a vocabulary size of 50k.
937"
B,0.5626007522837184,"We do not use weight tying [84, 46]. We sample without replacement during training and employ
938"
B,0.5631380977968834,"sequence packing without attention masking. We separate documents in our training corpora with
939"
B,0.5636754433100484,"end-of-text tokens.
940"
B,0.5642127888232134,"Objectives and optimization.
We train with a standard causal language modeling objective (i.e.,
941"
B,0.5647501343363783,"next token prediction) with an additive z-loss [19] (coefficient 1e-4), which mitigates output logit
942"
B,0.5652874798495433,"norm growth [67] instabilities. We use the AdamW optimizer [62] (PyTorch defaults except beta2 =
943"
B,0.5658248253627082,"0.95), with independent weight decay [123] (coefficient 1e-4). For the learning rate schedule, we use
944"
B,0.5663621708758731,"linear warmup and cosine decay. We cool down to a low learning rate (3e-5).
945"
B,0.5668995163890381,"C
Additional grid search details
946"
B,0.5674368619022031,"Final model configurations.
We present our final hyperparameters in Table 3.
947"
B,0.5679742074153681,"Grid search configuration selection.
Recall in Section 3.3, we run a grid search over many
948"
B,0.5685115529285331,"configurations. We present the architectures we sweep over in Table 4.
949"
B,0.569048898441698,"D
Evaluation dataset details
950"
B,0.569586243954863,"All 46 downstream evaluations are based on MosaicML’s LLM-foundry evaluation suite [69]. We
951"
B,0.570123589468028,"specifically consider the datasets given in Table 5.
Recall that we use a subset of 17 of these
952"
B,0.5706609349811929,"evaluations that give signal (are above random chance) for the compute range we consider. See
953"
B,0.5711982804943578,"Appendix E, where we ablate over the 17 subset design choice by including more and less evaluations.
954"
B,0.5717356260075228,"E
Additional results
955"
B,0.5722729715206878,"Scaling law fits.
We present specific coefficients for our fits in Table 6.
956"
B,0.5728103170338528,"Small-scale experiments can predict model rank order.
We expect to be able to rank hypothetical
957"
B,0.5733476625470177,"models based on their predicted performance, which is useful when deciding what large-scale runs
958"
B,0.5738850080601827,"1016
1017
1018
1019"
B,0.5744223535733477,Compute (6ND) [FLOPs] 3 4 5
B,0.5749596990865127,Loss: OpenLM eval
B,0.5754970445996775,"1016
1017
1018
1019"
B,0.5760343901128425,Compute (6ND) [FLOPs] 3 4
GRID SEARCH MODELS,0.5765717356260075,"5
Grid search models 1000 2000 3000 4000 5000 6000 7000"
GRID SEARCH MODELS,0.5771090811391725,Number of optimization steps
GRID SEARCH MODELS,0.5776464266523375,"Figure 6: Understanding over-performing models in our grid search. (left) Models trained with
5.2 × 1016 to 5.2 × 1017 FLOPs over-perform relative to their neighbors. In looking at the number
of optimization steps, we notice that the over-performing models experience more optimization steps
than their x-axis neighbors. We hypothesize that the number of optimization steps is important,
especially for smaller models, when trying to find models that lie along a trend. (right) A view of the
same phenomenon, specifically on the efficient frontier."
GRID SEARCH MODELS,0.5781837721655024,"to train. To verify, we rank 9 testbed models with N ≥1.4B by ground-truth top-1 error and by
959"
GRID SEARCH MODELS,0.5787211176786674,"estimated top-1 error. We find high rank correlation of 0.88 for the 17-task split.
960"
GRID SEARCH MODELS,0.5792584631918324,"Over-performing grid search models experience more optimization steps.
As mentioned in
961"
GRID SEARCH MODELS,0.5797958087049974,"Section 3.3 and Figure 4, we notice that models between 0.011B to 0.079B (i.e., 5.2 × 1016 to
962"
GRID SEARCH MODELS,0.5803331542181622,"5.2 × 1017 FLOPs trained near compute-optimal) over-perform compared to the trend established by
963"
GRID SEARCH MODELS,0.5808704997313272,"other models in our initial grid searches. This results in a bump in the scaling plot. While we choose
964"
GRID SEARCH MODELS,0.5814078452444922,"to exclude this range of models for our scaling study, we additionally investigate this phenomenon.
965"
GRID SEARCH MODELS,0.5819451907576572,"In Figure 6 we color grid search configurations by the number of optimization steps (i.e., number
966"
GRID SEARCH MODELS,0.5824825362708221,"of tokens seen divided by batch size divided by sequence length). We notice that models in the
967"
GRID SEARCH MODELS,0.5830198817839871,"aforementioned range experience more optimization steps than their x-axis neighbors. For context,
968"
GRID SEARCH MODELS,0.5835572272971521,"Figure 1 (left) in Kaplan et al. [51] also shows a bump; however, there the performance is worse than
969"
GRID SEARCH MODELS,0.5840945728103171,"the general trend instead of better as in our work. We leave understanding more fully the interactions
970"
GRID SEARCH MODELS,0.584631918323482,"between hyperparameters, scaling, and performance to future work.
971"
GRID SEARCH MODELS,0.5851692638366469,"Scaling is largely predictable in-distribution (ID).
Prior work focuses on understanding scaling
972"
GRID SEARCH MODELS,0.5857066093498119,"using ID loss, often using training loss directly [51, 45]. Hence, we also consider Paloma [65] loss
973"
GRID SEARCH MODELS,0.5862439548629769,"evaluation sets, which are designed to probe performance in specific domains. We use Paloma’s
974"
GRID SEARCH MODELS,0.5867813003761418,"C4 [88, 27], RedPajama [112], and Falcon-RefinedWeb [82] splits to probe for ID loss. As seen
975"
GRID SEARCH MODELS,0.5873186458893068,"in Figure 7, relative error is mostly low. Relative error is largest for the N = 1.4B, M = 640
976"
GRID SEARCH MODELS,0.5878559914024718,"RedPajama run at 15.4%. Examining this case specifically, we find that the model performs better
977"
GRID SEARCH MODELS,0.5883933369156368,"than the scaling law prediction. We hypothesize that as a model sees more tokens there is an increased
978"
GRID SEARCH MODELS,0.5889306824288018,"likelihood of near-duplicate sequences ID, resulting in performance that is better than predicted.
979"
GRID SEARCH MODELS,0.5894680279419667,"Relative error is stable across many choices of downstream evaluation suites.
To understand
980"
GRID SEARCH MODELS,0.5900053734551316,"how sensitive our investigation is to our choices of downstream evaluation sets, we consider several
981"
GRID SEARCH MODELS,0.5905427189682966,"other options as seen in Figure 8. We find that our prediction errors are fairly (i) low and (ii) consistent
982"
GRID SEARCH MODELS,0.5910800644814616,"for many choices of downstream evaluation sets including the whole suite of 46 evaluations.
983"
GRID SEARCH MODELS,0.5916174099946265,"Scaling can break down when under-training.
We find that when a token multiple is too small
984"
GRID SEARCH MODELS,0.5921547555077915,"(i.e., under-training regime), scaling appears unreliable. In Figure 9 we see for M = 5 the scaling
985"
GRID SEARCH MODELS,0.5926921010209565,"trend is different. We hypothesize that tuning hyperparameters (e.g., warmup, batch size) directly for
986"
GRID SEARCH MODELS,0.5932294465341215,"smaller multipliers may help mitigate the breakdown in predictability.
987 10 20 40 80 160 320 640 M"
B,0.5937667920472864,0.011B
B,0.5943041375604514,0.079B
B,0.5948414830736163,0.154B
B,0.5953788285867813,0.411B
B,0.5959161740999462,1.4B
B,0.5964535196131112,6.9B N
B,0.5969908651262762,1.1% 0.0% 0.1% 0.7% 0.9% 0.0% 0.6%
B,0.5975282106394412,2.6% 0.3% 0.2% 0.8% 0.2% 0.3% 1.4%
B,0.5980655561526061,2.0% 0.5% 1.3% 1.0% 3.9% 3.5% 1.3%
B,0.5986029016657711,0.2% 0.2% 0.2% 3.5% 0.5% 2.7%
B,0.5991402471789361,"0.4%
2.3% 3.6%"
B,0.599677592692101,"Train: C4
Eval: C4 (Paloma split) 10 20 40 80 160 320 640 M"
B,0.600214938205266,4.6% 0.0% 0.3% 2.8% 1.5% 0.0% 0.8%
B,0.6007522837184309,0.5% 0.0% 1.1% 2.2% 3.0% 3.5% 3.3%
B,0.6012896292315959,0.2% 0.0% 0.5% 1.3% 1.4% 1.0% 0.5%
B,0.6018269747447609,0.1% 0.0% 0.2% 0.3% 0.7%10.0%10.3%
B,0.6023643202579259,"3.0%
15.4% 10.3%"
B,0.6029016657710908,"Train: RedPajama
Eval: RedPajama (Paloma split) 10 20 40 80 160 320 640 M"
B,0.6034390112842558,1.1% 0.0% 0.9% 1.6% 0.8% 0.0% 1.1%
B,0.6039763567974208,2.3% 0.1% 0.4% 1.0% 2.0% 3.2% 2.3%
B,0.6045137023105857,1.1% 0.2% 0.1% 1.5% 0.8% 2.0% 2.4%
B,0.6050510478237506,0.7% 0.1% 1.6% 2.3% 2.0% 2.3% 4.3%
B,0.6055883933369156,"1.4%
6.0% 5.6%"
B,0.6061257388500806,"Train: RefinedWeb
Eval: RefinedWeb (Paloma split) 0.0% 2.0% 4.0% 6.0% 8.0% 10.0%"
B,0.6066630843632456,Relative error
B,0.6072004298764105,"Figure 7: In-distribution (ID) settings. Boxes highlighted in yellow correspond to data points used
to fit Equation (4). Relative error is generally low across interpolation and extrapolation regimes.
Relative error is largest for the RedPajama N = 1.4B, M = 640 prediction at 15.4%. In this case,
we find that our scaling law predicts the model should perform worse than it does in practice."
B,0.6077377753895755,"0
10
20
30
40
50
Inclusion threshold t (i.e., include evals where any model gets"
B,0.6082751209027405,"t percentage points above random chance at 0.154B scales) 10
2 10
1"
B,0.6088124664159055,Relative prediction error
B,0.6093498119290703,"0
10
20
30
40
Number of excluded datasets (out of 46-total) 10
2 10
1"
B,0.6098871574422353,"C4
RedPajama
RefinedWeb"
B,0.6104245029554003,"Figure 8: Downstream evaluation set ablation for 6.9B parameter, 138B token runs. Recall that
we consider a 17 task evaluation suite created by including only test sets where any 0.154B model we
trained (for any token multiplier and training dataset) gets t = 10 percentage points above random
chance. We evaluate over this subset to make sure we are measuring signal not noise. Here, we wish
to understand how sensitive the relative prediction error is to our choice of t. (left) We see that relative
prediction error is fairly low before a threshold of t = 35 (less than 10% relative error). When too
many tasks are excluded (i.e., t ≥40) relative error spikes. Averaging over all 46 datasets (t = −5 as
some evals are worse than random chance) also makes for a predictable metric (less than 3% relative
error). (right) A parallel view, showing how many tasks are removed as t increases. 40 out of the 46
tasks can be removed and relative error is still fairly stable."
B,0.6109618484685653,"1016
1018
1020"
B,0.6114991939817302,"Compute (6ND, D = MN) [FLOPs] 2 3 4 5 6"
B,0.6120365394948952,Reducible loss: C4 eval
B,0.6125738850080602,Training set: C4
B,0.6131112305212252,"1016
1018
1020"
B,0.6136485760343902,"Compute (6ND, D = MN) [FLOPs] 1 2 3 4 5"
B,0.614185921547555,"6
Training set: RedPajama"
B,0.61472326706072,"1016
1018
1020"
B,0.615260612573885,"Compute (6ND, D = MN) [FLOPs] 1 2 3 4 5"
B,0.61579795808705,Training set: RefinedWeb
B,0.6163353036002149,"N = 0.011B
N = 0.079B
N = 0.154B
N = 0.411B
Interpolation
Extrapolation 5 10 20 40 80"
B,0.6168726491133799,token multiplier M
B,0.6174099946265449,"Figure 9: Scaling with small token multipliers. For smaller multipliers (e.g., M = 5 in cyan),
scaling does not follow the same trend as that of larger multipliers. Additionally, many token
multipliers (e.g., M ∈{10, 20, 40, 80}) garner points close to the compute-optimal frontier. 10 20 40 80 160 320 640 M"
B,0.6179473401397099,0.011B
B,0.6184846856528748,0.079B
B,0.6190220311660397,0.154B
B,0.6195593766792047,0.411B
B,0.6200967221923697,1.4B
B,0.6206340677055346,6.9B N
B,0.6211714132186996,15.2%0.0% 8.4% 2.9% 0.2% 0.0% 0.2%
B,0.6217087587318646,2.1% 0.3% 1.7% 0.2% 1.8% 3.0% 9.7%
B,0.6222461042450296,3.3% 0.6% 0.7% 1.6% 6.4% 4.3% 2.8%
B,0.6227834497581946,0.5% 0.3% 0.4% 9.5% 4.1%22.4%
B,0.6233207952713595,"2.5%
4.3% 3.3%"
B,0.6238581407845244,"Train: C4
Eval: 100 programming languages"
B,0.6243954862976894,(Paloma split) 10 20 40 80 160 320 640 M
B,0.6249328318108544,5.7% 0.0% 3.6% 1.1% 1.4% 0.0% 3.5%
B,0.6254701773240193,0.0% 0.4% 1.6% 2.1% 3.0% 5.1%24.3%
B,0.6260075228371843,0.9% 0.7% 0.7% 1.6% 1.3% 4.7% 2.0%
B,0.6265448683503493,0.7% 0.3% 0.1% 0.5% 0.7% 3.9%
B,0.6270822138635143,"9.9%
10.0% 9.8%"
B,0.6276195593766792,"Train: C4
Eval: Penn Tree Bank"
B,0.6281569048898442,(Paloma split) 10 20 40 80 160 320 640 M
B,0.6286942504030091,4.1% 0.0% 2.1% 0.6% 1.5% 0.0% 1.2%
B,0.6292315959161741,1.5% 0.1% 0.5% 0.2% 0.2% 1.1% 1.7%
B,0.629768941429339,2.3% 0.1% 0.6% 0.0% 2.2% 2.2% 0.7%
B,0.630306286942504,0.5% 0.0% 0.0% 2.8% 0.2% 3.1%
B,0.630843632455669,"0.9%
7.6% 3.4%"
B,0.631380977968834,"Train: C4
Eval: C4 German eval 0.0% 2.0% 4.0% 6.0% 8.0% 10.0%"
B,0.631918323481999,Relative error
B,0.6324556689951639,"Figure 10: Out-of-distribution (OOD) settings. Boxes highlighted in yellow correspond to data
points used to fit Equation (4). Recall that the C4 training set is English-filtered. Relative error can
spike, suggesting unreliable scaling, for (left) programming languages and (center) Penn Tree Bank,
which contains many frequently occurring, uncommon substrings. However, scaling is relatively
reliable when evaluating on (right) German. These results motivate future studies of OOD conditions
that affect scaling in the over-trained regime. 10 20 40 80 160 320 640 M"
B,0.6329930145083289,0.011B
B,0.6335303600214938,0.079B
B,0.6340677055346587,0.154B
B,0.6346050510478237,0.411B
B,0.6351423965609887,1.4B
B,0.6356797420741537,6.9B N
B,0.6362170875873187,0.3% 0.2% 0.6% 0.9% 0.2% 0.3% 1.3%
B,0.6367544331004836,1.2% 0.4% 1.0% 0.1% 0.3% 0.3% 0.0%
B,0.6372917786136486,0.2% 0.7% 0.5% 1.2% 1.7% 1.0% 0.4%
B,0.6378291241268136,0.6% 0.2% 0.0% 1.6% 0.0% 0.4%
B,0.6383664696399785,"0.3%
3.1% 0.1%"
B,0.6389038151531434,"Train: C4,
Downstream: 46-task split 10 20 40 80 160 320 640 M"
B,0.6394411606663084,0.1% 0.4% 1.1% 1.3% 0.2% 0.6% 0.4%
B,0.6399785061794734,0.1% 0.0% 0.3% 0.8% 1.0% 0.6% 1.3%
B,0.6405158516926384,0.2% 0.2% 0.0% 0.6% 0.9% 0.9% 0.3%
B,0.6410531972058033,1.3% 0.8% 1.3% 1.5% 1.0% 1.0% 1.0%
B,0.6415905427189683,"0.3%
3.4% 2.1%"
B,0.6421278882321333,"Train: RedPajama,
Downstream: 46-task split 10 20 40 80 160 320 640 M"
B,0.6426652337452983,1.2% 0.1% 0.1% 0.7% 0.3% 0.8% 0.6%
B,0.6432025792584632,0.5% 1.4% 0.4% 0.9% 0.8% 1.0% 1.2%
B,0.6437399247716281,0.8% 0.1% 0.6% 0.3% 1.1% 0.7% 0.9%
B,0.6442772702847931,0.5% 1.1% 1.1% 1.7% 1.6% 0.6% 0.9%
B,0.6448146157979581,"0.3%
4.3% 2.8%"
B,0.645351961311123,"Train: RefinedWeb,
Downstream: 46-task split 0% 2% 4% 6% 8% 10%"
B,0.645889306824288,Relative error
B,0.646426652337453,"Figure 11: Relative error on average top-1 predictions (46 task split). Boxes highlighted in yellow
correspond to data points used to fit Equation (5). Using our fits, we accurately predict downstream
average top-1 error across interpolation and extrapolation regimes. This result supports that (i)
chaining a scaling law and our proposed exponential decay function is a valid procedure and (ii)
average top-1 error can be highly predictable."
B,0.646963997850618,"Scaling can be unpredictable out-of-distribution (OOD).
Our main result shows reliable C4 eval
988"
B,0.647501343363783,"loss predictions with models trained on RedPajama, which is an OOD evaluation setting. However,
989"
B,0.6480386888769479,"both C4 and RedPajama both contain tokens sourced from CommonCrawl.
990"
B,0.6485760343901128,"To further probe OOD performance, we measure the relative error of scaling laws fit to models trained
991"
B,0.6491133799032778,"on C4 and evaluated on Paloma’s 100 programming languages [65], Paloma’s Penn Tree Bank (PTB)
992"
B,0.6496507254164428,"split [66], and a German version of C4 [27]. Recall that the C4 training set we use has been filtered
993"
B,0.6501880709296077,"for English text. Hence we expect (i) the proportion of code is minimal, (ii) the “<unk>” substrings in
994"
B,0.6507254164427727,"PTB raw text do not appear frequently, and (iii) German is not prevalent. We notice that extrapolation
995"
B,0.6512627619559377,"relative error tends to be high for large M, N on programming languages and PTB (Figure 10 (left,
996"
B,0.6518001074691027,"center)). In contrast, for German C4, relative error is still low across the extrapolation range, with a
997"
B,0.6523374529822676,"maximum relative error of 7.6% at the N =1.4B, M = 80 scale (Figure 10 (right)). We hypothesize
998"
B,0.6528747984954326,"that further modifications to scaling laws are necessary to predict when scaling should be reliable as a
999"
B,0.6534121440085975,"function of the training and evaluation distributions.
1000"
B,0.6539494895217625,"Small-scale experiments can predict average downstream top-1 error.
To verify that chaining
1001"
B,0.6544868350349274,"Equations (4) and (5) is effective in practice, we collect C4 eval loss and downstream error pairs for
1002"
B,0.6550241805480924,"the configurations in Table 1. In Figure 11, we look at relative error for our scaling predictions in the
1003"
B,0.6555615260612574,"context of Average top-1 error over 46 evals and in Figure 12 over the high-signal 17 eval subset. We
1004"
B,0.6560988715744224,"again notice reliable scaling in interpolation and extrapolation regimes, suggesting the validity of our
1005"
B,0.6566362170875873,"procedure to predict downstream average top-1 error.
1006 10 20 40 80 160 320 640 M"
B,0.6571735626007523,0.011B
B,0.6577109081139173,0.079B
B,0.6582482536270822,0.154B
B,0.6587855991402471,0.411B
B,0.6593229446534121,1.4B
B,0.6598602901665771,6.9B N
B,0.6603976356797421,1.2% 0.7% 1.2% 2.3% 1.1% 1.1% 3.1%
B,0.6609349811929071,3.9% 1.2% 2.5% 1.0% 0.5% 0.6% 0.4%
B,0.661472326706072,1.3% 2.9% 1.1% 2.2% 3.9% 1.4% 0.2%
B,0.662009672219237,2.3% 1.3% 1.7% 2.8% 2.2% 1.8%
B,0.662547017732402,"0.7%
9.6% 0.1%"
B,0.6630843632455669,"Train: C4,
Downstream: 17-task split 10 20 40 80 160 320 640 M"
B,0.6636217087587318,0.6% 0.4% 2.1% 2.5% 0.6% 0.8% 0.8%
B,0.6641590542718968,0.8% 0.7% 0.2% 1.3% 0.8% 0.5% 1.3%
B,0.6646963997850618,0.9% 1.7% 0.2% 0.1% 0.2% 0.7% 0.0%
B,0.6652337452982268,2.6% 2.0% 1.0% 2.6% 1.9% 3.4% 3.4%
B,0.6657710908113917,"0.7%
3.6% 0.0%"
B,0.6663084363245567,"Train: RedPajama,
Downstream: 17-task split 10 20 40 80 160 320 640 M"
B,0.6668457818377217,2.5% 0.4% 0.7% 1.5% 1.3% 1.7% 0.9%
B,0.6673831273508867,1.0% 2.7% 0.4% 0.4% 0.9% 0.3% 2.3%
B,0.6679204728640515,1.5% 0.1% 0.0% 1.3% 2.1% 0.6% 0.6%
B,0.6684578183772165,0.8% 2.2% 2.0% 3.6% 3.4% 1.2% 1.6%
B,0.6689951638903815,"0.4%
5.6% 2.9%"
B,0.6695325094035465,"Train: RefinedWeb,
Downstream: 17-task split 0% 2% 4% 6% 8% 10%"
B,0.6700698549167114,Relative error
B,0.6706072004298764,"Figure 12: Relative error on average top-1 predictions (17 task split). Boxes highlighted in yellow
correspond to data points used to fit Equation (5). Using our fits, we accurately predict downstream
average top-1 error across interpolation and extrapolation regimes. This result supports that (i)
chaining a scaling law and our proposed exponential decay function is a valid procedure and (ii)
average top-1 error can be highly predictable."
B,0.6711445459430414,"2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0"
B,0.6716818914562064,Loss: C4 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80
B,0.6722192369693714,Average top-1 error: 17-task split
B,0.6727565824825362,"2
3
4
5
6
7
8"
B,0.6732939279957012,Loss: RedPajama 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85
B,0.6738312735088662,"3
4
5
6"
B,0.6743686190220312,Loss: RefinedWeb 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85
B,0.6749059645351961,"C4
RedPajama
RefinedWeb
Interpolation
Extrapolation"
B,0.6754433100483611,"2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0"
B,0.6759806555615261,Loss: C4 0.66 0.68 0.70 0.72 0.74 0.76
B,0.6765180010746911,Average top-1 error: 46-task split
B,0.677055346587856,"2
3
4
5
6
7
8"
B,0.6775926921010209,Loss: RedPajama 0.66 0.68 0.70 0.72 0.74 0.76
B,0.6781300376141859,"3
4
5
6"
B,0.6786673831273509,Loss: RefinedWeb 0.66 0.68 0.70 0.72 0.74 0.76
B,0.6792047286405158,"C4
RedPajama
RefinedWeb
Interpolation
Extrapolation"
B,0.6797420741536808,"Figure 13: Correlation between average top-1 error and evaluation loss. We observe that
regardless of evaluation loss distribution (x-axis), models tend to follow Equation (5). This suggests
that there can be several reasonable choices for the validation loss distribution. Additionally, ID
models trained on C4 and evaluated on a C4 validation set, perform best in terms of loss, but these
gains don’t necessarily translate to lower error downstream (e.g., (left column)). This suggests the
need to fit Equation (5) per dataset and also suggests comparing models trained on different data
distributions with a single loss evaluation can be misleading."
B,0.6802794196668458,"Loss evaluation ablations for downstream trends.
Figure 13 presents the correlation between
1007"
B,0.6808167651800108,"downstream error and loss evaluated on different validation sets (C4, RedPajama, and RefinedWeb).
1008"
B,0.6813541106931758,"Regardless of the validation set (x-axis), models follow the exponential decay relationship given
1009"
B,0.6818914562063407,"in Equation (5), suggesting the choice of validation loss is not critical for the appearance of this
1010"
B,0.6824288017195056,"phenomenon.
1011"
B,0.6829661472326706,"Investing more compute in a scaling law makes it more predictive.
Thus far we have looked
1012"
B,0.6835034927458356,"at standard configurations from Table 1 to construct our scaling laws, mainly to demonstrate
1013"
B,0.6840408382590005,"extrapolation to larger N, M. However, for practitioners, the main constraint is often training
1014"
B,0.6845781837721655,"1018
1019
1020
1021"
B,0.6851155292853305,"Compute [FLOPs] used for the scaling fit 10
4 10
3 10
2 10
1 100"
B,0.6856528747984955,Relative error: C4 eval
B,0.6861902203116604,"5
10
15
20
25
30
Number of samples used for the scaling fit"
B,0.6867275658248254,"Trend
Individual estimates
Default setting from Table 2"
B,0.6872649113379903,"Figure 14: Trade-offs between scaling law for loss fitting considerations and reliability.
Each red circle represents a scaling law fit to Equation (4) with as many as 29 models trained
on RedPajama. Specifically, a grid formed by N ∈{0.011B, 0.079B, 0.154B, 0.411B}, M ∈
{5, 10, 20, 40, 80, 160, 320} gives 28 models and a N = 1.4B, M = 20 run gives the last model. We
sort models by training FLOPs in increasing order and sample models uniformly from index windows
[1, 2, ..., n] for n ∈[5, 6, .., 29] to fit Equation (4). The blue star represents the default configuration
presented in Table 1. The prediction target is a N = 1.4B, M = 640 (D = 900B) model. As the
amount of compute (left) and the number of points (right) used to fit the scaling law increases, relative
error trends downwards. Our default configuration keeps compute and number of points low, while
still providing low prediction error compared to the trend."
B,0.6878022568511553,"1018
1019
1020
1021"
B,0.6883396023643202,"Compute [FLOPs] used for the scaling fit 10
4 10
3 10
2 10
1 100"
B,0.6888769478774852,Relative error: C4 eval
B,0.6894142933906502,"1018
1019
1020
1021"
B,0.6899516389038152,Compute [FLOPs] used for the scaling fit
B,0.6904889844169801,Relative error: 17-task split
B,0.6910263299301451,"Trend
Default setting from Table 2
Individual estimates"
B,0.6915636754433101,"Figure 15: Compute vs. relative error for the 1.4B, 900B token RedPajama run. (left) The
compute necessary to accurately predict loss is less than that needed to accurately predict (right)
average downstream error. This claim is supported by the fact that the slope of the trend for loss is
steeper than for top-1 error. These findings corroborate Figure 16."
B,0.692101020956475,"compute. Hence, we wish to understand the trade-offs between the amount of compute invested
1015"
B,0.6926383664696399,"in creating a scaling law and the relative error of the resulting law in the over-trained regime. In
1016"
B,0.6931757119828049,"Figure 14 (left), we see that as one increases the amount of compute, it is possible to get better fits
1017"
B,0.6937130574959699,"with lower relative error. In Figure 14 (right), we see a similar trend as one increases the number of
1018"
B,0.6942504030091349,"data points used to fit a scaling law. Blue stars indicate the configurations from Table 1, which provide
1019"
B,0.6947877485222999,"accurate predictions relative to the general trends—hinting at their usefulness for our investigation.
1020"
B,0.6953250940354648,"In Figures 15 and 16 we repeat the compute analysis comparing trade-offs for loss prediction and
1021"
B,0.6958624395486298,"error prediction for our RedPajama 1.4B parameter, 900B token and 6.9B parameter, 138B token
1022"
B,0.6963997850617948,"runs respectively. We find that less compute is generally necessary to construct a loss scaling law that
1023"
B,0.6969371305749597,"achieves the same relative error as that of an error prediction scaling law.
1024"
B,0.6974744760881246,"1018
1019
1020
1021"
B,0.6980118216012896,"Compute [FLOPs] used for the scaling fit 10
4 10
3 10
2 10
1 100"
B,0.6985491671144546,Relative error: C4 eval
B,0.6990865126276196,"1018
1019
1020
1021"
B,0.6996238581407845,Compute [FLOPs] used for the scaling fit
B,0.7001612036539495,Relative error: 17-task split
B,0.7006985491671145,"Trend
Default setting from Table 2
Individual estimates"
B,0.7012358946802795,"Figure 16: Compute vs. relative error for the 6.9B, 138B token RedPajama run. (left) The
compute necessary to accurately predict loss is less than that needed to accurately predict (right)
average downstream error. This claim is supported by the fact that the slope of the trend for loss is
steeper than for top-1 error. These findings corroborate Figure 15."
B,0.7017732401934443,"20
40
80
160
320
640
Token multiplier M 0.11 0.12 0.13 0.14 0.15"
B,0.7023105857066093,Scaling exponent
B,0.7028479312197743,"C4
RedPajama
RefinedWeb
Trend"
B,0.7033852767329393,"Figure 17: Scaling exponent vs. token multiplier. In Figure 2, we notice roughly parallel lines
(i.e., roughly constant scaling exponent η) in the log-log plot of loss vs. compute, even as the token
multiplier M changes. Here we plot η vs. M directly, where the shaded region gives a 95% bootstrap
confidence interval for the trend. This view supports that η is relatively constant."
B,0.7039226222461042,"On compute-optimal token multipliers.
We consider 20 tokens per parameter as close to compute-
1025"
B,0.7044599677592692,"optimal for our experiments. Here we investigate, using different approaches, what the compute-
1026"
B,0.7049973132724342,"optimal token multipliers are for each dataset—assuming one should scale number of parameter and
1027"
B,0.7055346587855992,"training tokens equally as Hoffmann et al. [45] suggest.
1028"
B,0.7060720042987642,"Turning to Figure 9, we notice that there are many multipliers, between 10 and 80 that yield models
1029"
B,0.706609349811929,"close to the frontier. Hence, empirically, it appears choices within this range should be suitable for
1030"
B,0.707146695325094,"the optimal token multiplier.
1031"
B,0.707684040838259,"We can also compute an optimal token multiplier using the coefficients in Table 6. Based on Hoffmann
1032"
B,0.708221386351424,"et al. [45]’s Equation (4) and the assumption that α = β, we write,
1033"
B,0.7087587318645889,"N ∗(C) = G
C 6  1"
B,0.7092960773777539,"2
, D∗(C) = G−1
C 6  1"
B,0.7098334228909189,"2
, G =
a b  1"
B,0.7103707684040839,"4η .
(9)"
B,0.7109081139172488,"To compute M ∗= D∗/N ∗, we then have,
1034"
B,0.7114454594304137,"M ∗=
 b a  1"
B,0.7119828049435787,"2η
.
(10)"
B,0.7125201504567437,"Using the values from Table 6 and plugging into Equation (10), we find M ∗
C4 = 2.87, M ∗
RedPajama =
1035"
B,0.7130574959699086,"4.30, M ∗
RefinedWeb = 3.79, where the subscript gives the dataset name. These values conflict with the
1036"
B,0.7135948414830736,"observation in Figure 9, which suggests M = 5 is already too small to give points on the Pareto
1037"
B,0.7141321869962386,"frontier. We hypothesize this mismatch arises because we fit our scaling laws using models with
1038"
B,0.7146695325094036,"M ≥20.
1039"
B,0.7152068780225685,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7157442235357335,Loss: C4 eval 0.72 0.74 0.76 0.78 0.80 0.82 0.84
B,0.7162815690488984,Top-1 error: AGIEval LSAT AR
B,0.7168189145620634,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7173562600752283,Loss: C4 eval 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78
B,0.7178936055883933,Top-1 error: AGIEval LSAT LR
B,0.7184309511015583,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7189682966147233,Loss: C4 eval 0.72 0.74 0.76 0.78
B,0.7195056421278883,Top-1 error: AGIEval LSAT RC
B,0.7200429876410532,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7205803331542182,Loss: C4 eval 0.70 0.72 0.74 0.76 0.78 0.80
B,0.7211176786673831,Top-1 error: AGIEval SAT English
B,0.721655024180548,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.722192369693713,Loss: C4 eval 0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800
B,0.722729715206878,Top-1 error: ARC-Challenge
B,0.723267060720043,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.723804406233208,Loss: C4 eval 0.3 0.4 0.5 0.6 0.7
B,0.7243417517463729,Top-1 error: ARC-Easy
B,0.7248790972595379,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7254164427727029,Loss: C4 eval 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60
B,0.7259537882858678,Top-1 error: BBQ
B,0.7264911337990327,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7270284793121977,Loss: C4 eval 0.675 0.700 0.725 0.750 0.775 0.800 0.825
B,0.7275658248253627,Top-1 error: BIG-bench: Conceptual combinations
B,0.7281031703385277,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7286405158516926,Loss: C4 eval 0.96 0.97 0.98 0.99 1.00
B,0.7291778613648576,Top-1 error: BIG-bench: Conlang translation
B,0.7297152068780226,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7302525523911876,Loss: C4 eval 0.6 0.7 0.8 0.9 1.0
B,0.7307898979043524,Top-1 error: BIG-bench: CS algorithms
B,0.7313272434175174,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7318645889306824,Loss: C4 eval 0.70 0.75 0.80 0.85 0.90 0.95 1.00
B,0.7324019344438474,Top-1 error: BIG-bench: Dyck languages
B,0.7329392799570124,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7334766254701773,Loss: C4 eval 0.73 0.74 0.75 0.76 0.77
B,0.7340139709833423,Top-1 error: BIG-bench: Elementary math QA
B,0.7345513164965073,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7350886620096723,Loss: C4 eval 0.740 0.745 0.750 0.755 0.760
B,0.7356260075228371,Top-1 error: BIG-bench: Language identification
B,0.7361633530360021,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7367006985491671,Loss: C4 eval 0.72 0.73 0.74 0.75 0.76 0.77
B,0.7372380440623321,Top-1 error: BIG-bench: Logical deduction
B,0.737775389575497,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.738312735088662,Loss: C4 eval 0.44 0.46 0.48 0.50 0.52 0.54 0.56
B,0.738850080601827,Top-1 error: BIG-bench: Misconceptions
B,0.739387426114992,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.739924771628157,Loss: C4 eval 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85
B,0.7404621171413218,Top-1 error: BIG-bench: Novel Concepts
B,0.7409994626544868,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7415368081676518,Loss: C4 eval 0.80 0.85 0.90 0.95 1.00
B,0.7420741536808168,Top-1 error: BIG-bench: Operators
B,0.7426114991939817,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7431488447071467,Loss: C4 eval 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
B,0.7436861902203117,Top-1 error: BIG-bench: QA WikiData
B,0.7442235357334767,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7447608812466416,Loss: C4 eval 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00
B,0.7452982267598065,Top-1 error: BIG-bench: Repeat copy logic
B,0.7458355722729715,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7463729177861365,Loss: C4 eval 0.425 0.450 0.475 0.500 0.525 0.550 0.575 0.600
B,0.7469102632993014,Top-1 error: BIG-bench: Strange stories
B,0.7474476088124664,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7479849543256314,Loss: C4 eval 0.44 0.46 0.48 0.50 0.52 0.54
B,0.7485222998387964,Top-1 error: BIG-bench: Strategy QA
B,0.7490596453519613,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7495969908651263,Loss: C4 eval 0.68 0.70 0.72 0.74 0.76 0.78
B,0.7501343363782912,Top-1 error: BIG-bench: Understanding fables
B,0.7506716818914562,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7512090274046211,Loss: C4 eval 0.35 0.40 0.45 0.50 0.55 0.60
B,0.7517463729177861,Top-1 error: BoolQ
B,0.7522837184309511,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7528210639441161,Loss: C4 eval 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82
B,0.753358409457281,Top-1 error: Commonsense QA
B,0.753895754970446,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.754433100483611,Loss: C4 eval 0.2 0.3 0.4 0.5 0.6
B,0.7549704459967759,Top-1 error: COPA
B,0.7555077915099409,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7560451370231058,Loss: C4 eval 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
B,0.7565824825362708,Top-1 error: CoQA
B,0.7571198280494358,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7576571735626008,Loss: C4 eval 0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58
B,0.7581945190757657,Top-1 error: Enterprise PII classification
B,0.7587318645889307,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7592692101020957,Loss: C4 eval 0.3 0.4 0.5 0.6 0.7
B,0.7598065556152606,Top-1 error: HellaSwag (10-shot)
B,0.7603439011284255,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7608812466415905,Loss: C4 eval 0.3 0.4 0.5 0.6 0.7
B,0.7614185921547555,Top-1 error: HellaSwag (zero-shot)
B,0.7619559376679205,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7624932831810854,Loss: C4 eval 0.6 0.7 0.8 0.9 1.0
B,0.7630306286942504,Top-1 error: Jeopardy
B,0.7635679742074154,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7641053197205804,Loss: C4 eval 0.4 0.5 0.6 0.7 0.8 0.9 1.0
B,0.7646426652337452,Top-1 error: LAMBADA
B,0.7651800107469102,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7657173562600752,Loss: C4 eval 0.70 0.72 0.74 0.76 0.78 0.80
B,0.7662547017732402,Top-1 error: LogiQA
B,0.7667920472864052,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7673293927995701,Loss: C4 eval 0.740 0.745 0.750 0.755 0.760 0.765
B,0.7678667383127351,Top-1 error: MathQA
B,0.7684040838259001,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7689414293390651,Loss: C4 eval 0.730 0.735 0.740 0.745 0.750 0.755 0.760 0.765 0.770
B,0.7694787748522299,Top-1 error: MMLU (5-shot)
B,0.7700161203653949,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7705534658785599,Loss: C4 eval 0.73 0.74 0.75 0.76 0.77
B,0.7710908113917249,Top-1 error: MMLU (zero-shot)
B,0.7716281569048898,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7721655024180548,Loss: C4 eval 0.600 0.625 0.650 0.675 0.700 0.725 0.750 0.775
B,0.7727028479312198,Top-1 error: OpenBook QA
B,0.7732401934443848,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7737775389575497,Loss: C4 eval 0.25 0.30 0.35 0.40 0.45 0.50
B,0.7743148844707146,Top-1 error: PIQA
B,0.7748522299838796,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7753895754970446,Loss: C4 eval 0.5 0.6 0.7 0.8 0.9 1.0
B,0.7759269210102095,Top-1 error: PubMed QA Labeled
B,0.7764642665233745,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7770016120365395,Loss: C4 eval 0.975 0.980 0.985 0.990 0.995 1.000
B,0.7775389575497045,Top-1 error: Simple Arithmetic: NoSpaces
B,0.7780763030628695,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7786136485760344,Loss: C4 eval 0.96 0.97 0.98 0.99 1.00
B,0.7791509940891993,Top-1 error: Simple Arithmetic: WithSpaces
B,0.7796883396023643,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7802256851155293,Loss: C4 eval 0.48 0.49 0.50 0.51 0.52
B,0.7807630306286942,Top-1 error: SIQA
B,0.7813003761418592,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7818377216550242,Loss: C4 eval 0.5 0.6 0.7 0.8 0.9 1.0
B,0.7823750671681892,Top-1 error: SQuAD
B,0.7829124126813541,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7834497581945191,Loss: C4 eval 0.45 0.50 0.55 0.60
B,0.783987103707684,Top-1 error: WinoGender MC: Female
B,0.784524449220849,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7850617947340139,Loss: C4 eval 0.35 0.40 0.45 0.50 0.55 0.60
B,0.7855991402471789,Top-1 error: WinoGender MC: Male
B,0.7861364857603439,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7866738312735089,Loss: C4 eval 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
B,0.7872111767866738,Top-1 error: WinoGrand
B,0.7877485222998388,"2.5
3.0
3.5
4.0
4.5
5.0
5.5"
B,0.7882858678130038,Loss: C4 eval 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52
B,0.7888232133261688,Top-1 error: WinoGrande
B,0.7893605588393336,"C4
RedPajama
RefinedWeb
Random chance"
B,0.7898979043524986,"Figure 18: Downstream top-1 error vs. C4 eval loss for each of the 46 downstream evals. Here
we plot models from our testbed for each scatter plot. We see that some individual evaluations, like
ARC-Easy, follow exponential decay. Others, like BIG-bench: CS algorithms, show step function
behavior. Still others, like MathQA, hover around random chance."
B,0.7904352498656636,"F
Additional related work
1040"
B,0.7909725953788286,"Language modeling.
Language models can be grouped into encoder-only [26, 53, 59, 96, 22],
1041"
B,0.7915099408919936,"encoder-decoder [56, 89], and decoder-only architectures [85, 113, 114, 110, 49, 38, 74, 7, 111,
1042"
B,0.7920472864051585,"28, 64, 99, 122, 4, 57, 63, 34]. Most current implementations are based on the transformer [116].
1043"
B,0.7925846319183235,"However, there has been a recent resurgence in scaling language models based on non-transformer
1044"
B,0.7931219774314885,"architectures [83, 36, 37, 35]. Further, there has been substantial work on adapting pre-trained
1045"
B,0.7936593229446535,"language models to better follow instructions [119, 20, 70, 61, 71, 133, 87, 29, 115, 103, 73].
1046"
B,0.7941966684578183,"However, following prior work [45, 72] and given their overall prevalence, we limit ourselves to
1047"
B,0.7947340139709833,"GPT-style, decoder-only transformers that have solely been pre-trained.
1048"
B,0.7952713594841483,"Scaling laws.
Kaplan et al. [51] investigate scaling trends in GPT language models. Bahri et al.
1049"
B,0.7958087049973133,"[9] investigate different scaling regimes theoretically, and Sharma & Kaplan [101] relate scaling
1050"
B,0.7963460505104782,"coefficients to data manifold dimensions. Tay et al. [108, 109] elucidate the connection between
1051"
B,0.7968833960236432,"model architecture and scaling trends, while Hernandez et al. [42], Tay et al. [108] develop scaling
1052"
B,0.7974207415368082,"laws for transfer learning. Ivgi et al. [48] also consider transfer learning scaling laws and highlight
1053"
B,0.7979580870499732,"the importance of hyperparameter selection in the low-compute regime. Ghorbani et al. [32], Gordon
1054"
B,0.7984954325631382,"et al. [33], Bansal et al. [10] develop scaling laws for neural machine translation. Caballero et al. [17]
1055"
B,0.799032778076303,"propose a scaling law functional form, which they demonstrate is predictive in several domains.
1056"
B,0.799570123589468,"Scaling beyond language modeling.
There is a large body of work on scaling neural networks
1057"
B,0.800107469102633,"beyond language modeling, for example in computer vision [60, 127, 105, 1, 2], multimodal
1058"
B,0.800644814615798,"learning [41, 18, 30], and image reconstruction [52].
1059"
B,0.8011821601289629,"Over-training in existing models.
To contextualize the extent to which we over-train, we provide
1060"
B,0.8017195056421279,"token multipliers for popular models in Table 8.
1061"
B,0.8022568511552929,"G
Broader impact
1062"
B,0.8027941966684579,"Language models have known risks in terms harmful language, toxicity, and human automation—to
1063"
B,0.8033315421816228,"name a few [121, 12]. We will include the following for our public release “WARNING: These are
1064"
B,0.8038688876947877,"base models and not aligned with post-training. They are provided as is and intended as research
1065"
B,0.8044062332079527,"artifacts only.” However, even as research artifacts, we recognize that models can still be misused
1066"
B,0.8049435787211177,"by malicious actors or can be harmful to benevolent actors. When deciding to release our models
1067"
B,0.8054809242342826,"and experiments, we considered (i) the benefit to the scientific community and (ii) the benchmark
1068"
B,0.8060182697474476,"performance relative to other models that have already been released. For (i) we feel that our testbed
1069"
B,0.8065556152606126,"is of use to others in the community who want to do scaling research, but do not necessarily have the
1070"
B,0.8070929607737776,"means to train these model artifacts themselves. Hence, we predict (and hope) releasing all models
1071"
B,0.8076303062869425,"and experiments will be helpful to others wanting to participate in scaling research. For (ii), we note
1072"
B,0.8081676518001075,"that there are publicly available models [113, 114, 49], which outperform models from our testbed
1073"
B,0.8087049973132724,"and that are more likely to be widely adopted. Finally, we recognize that advancing scaling science
1074"
B,0.8092423428264374,"also has potential for harm. Specifically, while we are concerned with loss and downstream task
1075"
B,0.8097796883396023,"performance for popular evaluation settings, it is possible that nefarious actors may use scaling laws
1076"
B,0.8103170338527673,"to help design more harmful models.
1077"
B,0.8108543793659323,"H
Licensing
1078"
B,0.8113917248790973,"In terms of licensing, we will release our code, models, and experiments under an MIT licence, which
1079"
B,0.8119290703922623,"is also attached to our supplementary submission.
1080"
B,0.8124664159054272,"Table 4: Topologies for our grid searches. We consider 130 architectures for our grid search. After
sweeping over batch size and warmup, we get a total of 435 configurations."
B,0.8130037614185922,"nlayers
nheads
dmodel
Number of
parameters [B]"
B,0.8135411069317571,"4
4
96
0.010
4
12
96
0.010
12
12
96
0.011
12
4
96
0.011
8
4
96
0.011
16
4
96
0.011
16
12
96
0.011
8
12
96
0.011
24
4
96
0.012
24
12
96
0.012
4
4
192
0.021
4
8
192
0.021
4
12
192
0.021
8
8
192
0.023
8
4
192
0.023
8
12
192
0.023
12
4
192
0.025
12
8
192
0.025
12
12
192
0.025
16
4
192
0.026
16
8
192
0.026
16
12
192
0.026
24
8
192
0.030
24
4
192
0.030
24
12
192
0.030
4
12
288
0.033
4
4
288
0.033
8
12
288
0.037
8
4
288
0.037
4
4
320
0.038
4
8
320
0.038
12
12
288
0.041
12
4
288
0.041
8
8
320
0.043
8
4
320
0.043
16
4
288
0.045
16
12
288
0.045
12
4
320
0.049
12
8
320
0.049
24
4
288
0.053
24
12
288
0.053
16
8
320
0.055
16
4
320
0.055
4
12
488
0.062
4
4
512
0.065
4
16
512
0.065
4
8
512
0.065
24
8
320
0.066
24
4
320
0.066
4
4
576
0.074
4
8
576
0.074
4
12
576
0.074
8
12
488
0.075
8
4
512
0.079
8
8
512
0.079
8
16
512
0.079
4
4
640
0.085
4
16
640
0.085
4
8
640
0.085
12
12
488
0.087
8
4
576
0.090
8
12
576
0.090
8
8
576
0.090
12
16
512
0.093
12
8
512
0.093"
B,0.814078452444922,"nlayers
nheads
dmodel
Number of
parameters [B]"
B,0.814615797958087,"12
4
512
0.093
16
12
488
0.100
8
16
640
0.105
8
4
640
0.105
8
8
640
0.105
12
8
576
0.106
16
16
512
0.106
4
4
768
0.106
12
12
576
0.106
16
8
512
0.106
4
8
768
0.106
12
4
576
0.106
4
16
768
0.106
16
4
512
0.106
4
12
768
0.106
16
12
576
0.122
16
4
576
0.122
16
8
576
0.122
12
4
640
0.126
24
12
488
0.126
12
16
640
0.126
12
8
640
0.126
24
8
512
0.133
24
4
512
0.133
24
16
512
0.133
8
8
768
0.134
8
16
768
0.134
8
4
768
0.134
8
12
768
0.134
16
16
640
0.146
16
8
640
0.146
16
4
640
0.146
24
8
576
0.154
24
4
576
0.154
24
12
576
0.154
4
8
1024
0.155
4
16
1024
0.155
4
4
1024
0.155
12
8
768
0.162
12
4
768
0.162
12
12
768
0.162
12
16
768
0.162
24
16
640
0.186
24
8
640
0.186
24
4
640
0.186
16
16
768
0.191
16
4
768
0.191
16
8
768
0.191
16
12
768
0.191
8
8
1024
0.206
8
4
1024
0.206
8
16
1024
0.206
24
8
768
0.247
24
12
768
0.247
24
4
768
0.247
24
16
768
0.247
12
8
1024
0.257
12
4
1024
0.257
12
16
1024
0.257
16
8
1024
0.309
16
4
1024
0.309
16
16
1024
0.309
24
16
1024
0.412
24
8
1024
0.412
24
4
1024
0.412"
B,0.815153143471252,"Table 5: 46 downstream tasks. All downstream tasks considered in this work, evaluated via LLM-
foundry [69]. For more information on each dataset and specifics about the LLM-foundry category
and evaluation type, please see: https://www.mosaicml.com/llm-evaluation."
B,0.815690488984417,"Downstream task
LLM-foundry category
Evaluation type
Shots
Samples
Baseline"
B,0.816227834497582,"AGIEval LSAT AR [132, 131, 118]
symbolic problem solving
multiple choice
3
230
0.25
AGIEval LSAT LR [132, 131, 118]
reading comprehension
multiple choice
3
510
0.25
AGIEval LSAT RC [132, 131, 118]
reading comprehension
multiple choice
3
268
0.25
AGIEval SAT English [132]
reading comprehension
multiple choice
3
206
0.25
ARC-Challenge [23]
world knowledge
multiple choice
10
2376
0.25
ARC-Easy [23]
world knowledge
multiple choice
10
2376
0.25
BBQ [79]
safety
multiple choice
3
58492
0.50
BIG-bench: CS algorithms [11]
symbolic problem solving
language modeling
10
1320
0.00
BIG-bench: Conceptual combinations [11]
language understanding
multiple choice
10
103
0.25
BIG-bench: Conlang translation [11]
language understanding
language modeling
0
164
0.00
BIG-bench: Dyck languages [11]
symbolic problem solving
language modeling
10
1000
0.00
BIG-bench: Elementary math QA [11]
symbolic problem solving
multiple choice
10
38160
0.25
BIG-bench: Language identification [11]
language understanding
multiple choice
10
10000
0.25
BIG-bench: Logical deduction [11]
symbolic problem solving
multiple choice
10
1500
0.25
BIG-bench: Misconceptions [11]
world knowledge
multiple choice
10
219
0.50
BIG-bench: Novel Concepts [11]
commonsense reasoning
multiple choice
10
32
0.25
BIG-bench: Operators [11]
symbolic problem solving
language modeling
10
210
0.00
BIG-bench: QA WikiData [11]
world knowledge
language modeling
10
20321
0.00
BIG-bench: Repeat copy logic [11]
symbolic problem solving
language modeling
10
32
0.00
BIG-bench: Strange stories [11]
commonsense reasoning
multiple choice
10
174
0.50
BIG-bench: Strategy QA [11]
commonsense reasoning
multiple choice
10
2289
0.50
BIG-bench: Understanding fables [11]
reading comprehension
multiple choice
10
189
0.25
BoolQ [21]
reading comprehension
multiple choice
10
3270
0.50
COPA [92]
commonsense reasoning
multiple choice
0
100
0.50
CoQA [91]
reading comprehension
language modeling
0
7983
0.00
Commonsense QA [107]
commonsense reasoning
multiple choice
10
1221
0.25
Enterprise PII classification [81]
safety
multiple choice
10
3395
0.50
HellaSwag (10-shot) [126]
language understanding
multiple choice
10
10042
0.25
HellaSwag (zero-shot) [126]
language understanding
multiple choice
0
10042
0.25
Jeopardy [69]
world knowledge
language modeling
10
2117
0.00
LAMBADA [77]
language understanding
language modeling
0
5153
0.00
LogiQA [58]
symbolic problem solving
multiple choice
10
651
0.25
MMLU (5-shot) [40]
world knowledge
multiple choice
5
14042
0.25
MMLU (zero-shot) [40]
world knowledge
multiple choice
0
14042
0.25
MathQA [5]
symbolic problem solving
multiple choice
10
2983
0.25
OpenBook QA [68]
commonsense reasoning
multiple choice
0
500
0.25
PIQA [14]
commonsense reasoning
multiple choice
10
1838
0.50
PubMed QA Labeled [50]
reading comprehension
language modeling
10
1000
0.00
SIQA [97]
commonsense reasoning
multiple choice
10
1954
0.50
SQuAD [90]
reading comprehension
language modeling
10
10570
0.00
Simple Arithmetic: NoSpaces [69]
symbolic problem solving
language modeling
10
1000
0.00
Simple Arithmetic: WithSpaces [69]
symbolic problem solving
language modeling
10
1000
0.00
WinoGender MC: Female [94]
safety
multiple choice
10
60
0.50
WinoGender MC: Male [94]
safety
multiple choice
10
60
0.50
WinoGrande [95]
language understanding
schema
0
1267
0.50
WinoGrand [55]
language understanding
schema
0
273
0.50"
B,0.8167651800107469,"Table 6: Scaling law fit parameters. Here we present our scaling coefficients fit to Equations (4)
and (5) using configurations from Table 1."
B,0.8173025255239119,"Training dataset
Fit for Equation (4): L(C, M) =
Fit for Equation (5): Err(L) =
E + (a · M η + b · M −η)Cη
ϵ −k · exp (−γL)"
B,0.8178398710370769,"C4 [88, 27]
1.51 +
 
114 · M 0.242 + 190 · M −0.242
C−0.242
0.850 −2.08 · exp (−0.756 · L)
RedPajama [112]
1.84 +
 
166 · M 0.272 + 367 · M −0.272
C−0.272
0.857 −2.21 · exp (−0.715 · L)
RefinedWeb [82]
1.73 +
 
125 · M 0.254 + 246 · M −0.254
C−0.254
0.865 −2.21 · exp (−0.707 · L)"
B,0.8183772165502418,"Table 7: Downstream relative prediction error at 6.9B, 138B tokens, with and without the 1.4B
data point. Recall in Table 1, we introduce a N = 1.4B, M = 20 run to get better downstream error
predictions. Here we compare, prediction errors with and without this model for fitting the scaling
law. Note that without the model (i.e., rows with “w/o 1.4B”) average top-1 predictions, over the 17
tasks. are less accurate."
B,0.8189145620634067,"Scaling law fit
Train set
ARC-E
LAMBADA
OpenBook QA
HellaSwag
17 eval
[23]
[77]
[68]
[126]"
B,0.8194519075765717,"Table 1
C4 [88, 27]
28.96%
15.01%
16.80%
79.58%
0.14%
Table 1 w/o 1.4B
C4 [88, 27]
0.92%
2.04%
96.16%
61.79%
0.42%"
B,0.8199892530897367,"Table 1
RedPajama [112]
5.21%
14.39%
8.44%
25.73%
0.05%
Table 1 w/o 1.4B
RedPajama [112]
8.13%
11.07%
7.56%
30.98%
10.64%"
B,0.8205265986029017,"Table 1
RefinedWeb [82]
26.06%
16.55%
1.92%
81.96%
2.94%
Table 1 w/o 1.4B
RefinedWeb [82]
15.39%
6.26%
6.79%
6.52%
15.79%"
B,0.8210639441160666,"Table 8: Token multipliers of existing models. In our work, we run experiments with token
multipliers between 5 and 640 for {GPT-2 [85], LLaMA [113]}-style decoder-only architectures."
B,0.8216012896292316,"Model family
Parameters N
Training tokens D
Token multiplier M"
B,0.8221386351423966,"T5 [89]
11B
34B
3.1
GPT-3 [16]
175B
300B
1.7
Gopher [86]
280B
300B
1.1
Chinchilla [45]
70B
1.4T
20.0
LLaMA [113]
7B
1T
140.0
LLaMA [113]
70B
1.4T
20.0
LLaMA-2 [114]
7B
2T
290.0
LLaMA-2 [114]
70B
2T
30.0
XGen [74]
7B
1.5T
210.0
MPT [110]
7B
1T
140.0"
B,0.8226759806555616,"NeurIPS Paper Checklist
1081"
CLAIMS,0.8232133261687264,"1. Claims
1082"
CLAIMS,0.8237506716818914,"Question: Do the main claims made in the abstract and introduction accurately reflect the
1083"
CLAIMS,0.8242880171950564,"paper’s contributions and scope?
1084"
CLAIMS,0.8248253627082214,"Answer: [Yes]
1085"
CLAIMS,0.8253627082213864,"Justification: The experiment section justify the claims made in the abstract and introduction,
1086"
CLAIMS,0.8259000537345513,"namely that the developed scaling laws for over-training and downstream task prediction are
1087"
CLAIMS,0.8264373992477163,"predictive in practice for larger scale runs.
1088"
CLAIMS,0.8269747447608813,"Guidelines:
1089"
CLAIMS,0.8275120902740463,"• The answer NA means that the abstract and introduction do not include the claims
1090"
CLAIMS,0.8280494357872111,"made in the paper.
1091"
CLAIMS,0.8285867813003761,"• The abstract and/or introduction should clearly state the claims made, including the
1092"
CLAIMS,0.8291241268135411,"contributions made in the paper and important assumptions and limitations. A No or
1093"
CLAIMS,0.8296614723267061,"NA answer to this question will not be perceived well by the reviewers.
1094"
CLAIMS,0.830198817839871,"• The claims made should match theoretical and experimental results, and reflect how
1095"
CLAIMS,0.830736163353036,"much the results can be expected to generalize to other settings.
1096"
CLAIMS,0.831273508866201,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
1097"
CLAIMS,0.831810854379366,"are not attained by the paper.
1098"
LIMITATIONS,0.832348199892531,"2. Limitations
1099"
LIMITATIONS,0.8328855454056958,"Question: Does the paper discuss the limitations of the work performed by the authors?
1100"
LIMITATIONS,0.8334228909188608,"Answer: [Yes]
1101"
LIMITATIONS,0.8339602364320258,"Justification: The final section discusses limitations, which provide motivation for future
1102"
LIMITATIONS,0.8344975819451907,"work.
1103"
LIMITATIONS,0.8350349274583557,"Guidelines:
1104"
LIMITATIONS,0.8355722729715207,"• The answer NA means that the paper has no limitation while the answer No means that
1105"
LIMITATIONS,0.8361096184846857,"the paper has limitations, but those are not discussed in the paper.
1106"
LIMITATIONS,0.8366469639978507,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
1107"
LIMITATIONS,0.8371843095110156,"• The paper should point out any strong assumptions and how robust the results are to
1108"
LIMITATIONS,0.8377216550241805,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
1109"
LIMITATIONS,0.8382590005373455,"model well-specification, asymptotic approximations only holding locally). The authors
1110"
LIMITATIONS,0.8387963460505105,"should reflect on how these assumptions might be violated in practice and what the
1111"
LIMITATIONS,0.8393336915636754,"implications would be.
1112"
LIMITATIONS,0.8398710370768404,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
1113"
LIMITATIONS,0.8404083825900054,"only tested on a few datasets or with a few runs. In general, empirical results often
1114"
LIMITATIONS,0.8409457281031704,"depend on implicit assumptions, which should be articulated.
1115"
LIMITATIONS,0.8414830736163353,"• The authors should reflect on the factors that influence the performance of the approach.
1116"
LIMITATIONS,0.8420204191295003,"For example, a facial recognition algorithm may perform poorly when image resolution
1117"
LIMITATIONS,0.8425577646426652,"is low or images are taken in low lighting. Or a speech-to-text system might not be
1118"
LIMITATIONS,0.8430951101558302,"used reliably to provide closed captions for online lectures because it fails to handle
1119"
LIMITATIONS,0.8436324556689951,"technical jargon.
1120"
LIMITATIONS,0.8441698011821601,"• The authors should discuss the computational efficiency of the proposed algorithms
1121"
LIMITATIONS,0.8447071466953251,"and how they scale with dataset size.
1122"
LIMITATIONS,0.8452444922084901,"• If applicable, the authors should discuss possible limitations of their approach to
1123"
LIMITATIONS,0.845781837721655,"address problems of privacy and fairness.
1124"
LIMITATIONS,0.84631918323482,"• While the authors might fear that complete honesty about limitations might be used
1125"
LIMITATIONS,0.846856528747985,"by reviewers as grounds for rejection, a worse outcome might be that reviewers
1126"
LIMITATIONS,0.8473938742611499,"discover limitations that aren’t acknowledged in the paper. The authors should use
1127"
LIMITATIONS,0.8479312197743148,"their best judgment and recognize that individual actions in favor of transparency play
1128"
LIMITATIONS,0.8484685652874798,"an important role in developing norms that preserve the integrity of the community.
1129"
LIMITATIONS,0.8490059108006448,"Reviewers will be specifically instructed to not penalize honesty concerning limitations.
1130"
THEORY ASSUMPTIONS AND PROOFS,0.8495432563138098,"3. Theory Assumptions and Proofs
1131"
THEORY ASSUMPTIONS AND PROOFS,0.8500806018269748,"Question: For each theoretical result, does the paper provide the full set of assumptions and
1132"
THEORY ASSUMPTIONS AND PROOFS,0.8506179473401397,"a complete (and correct) proof?
1133"
THEORY ASSUMPTIONS AND PROOFS,0.8511552928533047,"Answer: [Yes]
1134"
THEORY ASSUMPTIONS AND PROOFS,0.8516926383664697,"Justification: All assumptions are clearly stated and full proofs/derivations are provided in
1135"
THEORY ASSUMPTIONS AND PROOFS,0.8522299838796346,"the Appendix.
1136"
THEORY ASSUMPTIONS AND PROOFS,0.8527673293927995,"Guidelines:
1137"
THEORY ASSUMPTIONS AND PROOFS,0.8533046749059645,"• The answer NA means that the paper does not include theoretical results.
1138"
THEORY ASSUMPTIONS AND PROOFS,0.8538420204191295,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
1139"
THEORY ASSUMPTIONS AND PROOFS,0.8543793659322945,"referenced.
1140"
THEORY ASSUMPTIONS AND PROOFS,0.8549167114454594,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
1141"
THEORY ASSUMPTIONS AND PROOFS,0.8554540569586244,"• The proofs can either appear in the main paper or the supplemental material, but if
1142"
THEORY ASSUMPTIONS AND PROOFS,0.8559914024717894,"they appear in the supplemental material, the authors are encouraged to provide a short
1143"
THEORY ASSUMPTIONS AND PROOFS,0.8565287479849544,"proof sketch to provide intuition.
1144"
THEORY ASSUMPTIONS AND PROOFS,0.8570660934981192,"• Inversely, any informal proof provided in the core of the paper should be complemented
1145"
THEORY ASSUMPTIONS AND PROOFS,0.8576034390112842,"by formal proofs provided in appendix or supplemental material.
1146"
THEORY ASSUMPTIONS AND PROOFS,0.8581407845244492,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
1147"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8586781300376142,"4. Experimental Result Reproducibility
1148"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8592154755507792,"Question: Does the paper fully disclose all the information needed to reproduce the
1149"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8597528210639441,"main experimental results of the paper to the extent that it affects the main claims and/or
1150"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8602901665771091,"conclusions of the paper (regardless of whether the code and data are provided or not)?
1151"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8608275120902741,"Answer: [Yes]
1152"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8613648576034391,"Justification: We point to all public datasets and open source training infrastructure. We
1153"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8619022031166039,"additionally specify all hyperparameters used for training.
1154"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8624395486297689,"Guidelines:
1155"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8629768941429339,"• The answer NA means that the paper does not include experiments.
1156"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8635142396560989,"• If the paper includes experiments, a No answer to this question will not be perceived
1157"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8640515851692638,"well by the reviewers: Making the paper reproducible is important, regardless of
1158"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8645889306824288,"whether the code and data are provided or not.
1159"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8651262761955938,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
1160"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8656636217087588,"to make their results reproducible or verifiable.
1161"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8662009672219237,"• Depending on the contribution, reproducibility can be accomplished in various ways.
1162"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8667383127350886,"For example, if the contribution is a novel architecture, describing the architecture fully
1163"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8672756582482536,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
1164"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8678130037614186,"be necessary to either make it possible for others to replicate the model with the same
1165"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8683503492745835,"dataset, or provide access to the model. In general. releasing code and data is often
1166"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8688876947877485,"one good way to accomplish this, but reproducibility can also be provided via detailed
1167"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8694250403009135,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
1168"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8699623858140785,"of a large language model), releasing of a model checkpoint, or other means that are
1169"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8704997313272435,"appropriate to the research performed.
1170"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8710370768404084,"• While NeurIPS does not require releasing code, the conference does require all
1171"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8715744223535733,"submissions to provide some reasonable avenue for reproducibility, which may depend
1172"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8721117678667383,"on the nature of the contribution. For example
1173"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8726491133799033,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
1174"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8731864588930682,"to reproduce that algorithm.
1175"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8737238044062332,"(b) If the contribution is primarily a new model architecture, the paper should describe
1176"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8742611499193982,"the architecture clearly and fully.
1177"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8747984954325632,"(c) If the contribution is a new model (e.g., a large language model), then there should
1178"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8753358409457281,"either be a way to access this model for reproducing the results or a way to reproduce
1179"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8758731864588931,"the model (e.g., with an open-source dataset or instructions for how to construct
1180"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.876410531972058,"the dataset).
1181"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.876947877485223,"(d) We recognize that reproducibility may be tricky in some cases, in which case
1182"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8774852229983879,"authors are welcome to describe the particular way they provide for reproducibility.
1183"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8780225685115529,"In the case of closed-source models, it may be that access to the model is limited in
1184"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8785599140247179,"some way (e.g., to registered users), but it should be possible for other researchers
1185"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8790972595378829,"to have some path to reproducing or verifying the results.
1186"
OPEN ACCESS TO DATA AND CODE,0.8796346050510478,"5. Open access to data and code
1187"
OPEN ACCESS TO DATA AND CODE,0.8801719505642128,"Question: Does the paper provide open access to the data and code, with sufficient
1188"
OPEN ACCESS TO DATA AND CODE,0.8807092960773778,"instructions to faithfully reproduce the main experimental results, as described in
1189"
OPEN ACCESS TO DATA AND CODE,0.8812466415905427,"supplemental material?
1190"
OPEN ACCESS TO DATA AND CODE,0.8817839871037076,"Answer: [Yes]
1191"
OPEN ACCESS TO DATA AND CODE,0.8823213326168726,"Justification: We include code and data needed to reproduce all figures in the paper. Our
1192"
OPEN ACCESS TO DATA AND CODE,0.8828586781300376,"datasets are sourced from HuggingFace and our training code utilizes OpenLM, which is
1193"
OPEN ACCESS TO DATA AND CODE,0.8833960236432026,"open-source.
1194"
OPEN ACCESS TO DATA AND CODE,0.8839333691563676,"Guidelines:
1195"
OPEN ACCESS TO DATA AND CODE,0.8844707146695325,"• The answer NA means that paper does not include experiments requiring code.
1196"
OPEN ACCESS TO DATA AND CODE,0.8850080601826975,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
1197"
OPEN ACCESS TO DATA AND CODE,0.8855454056958625,"public/guides/CodeSubmissionPolicy) for more details.
1198"
OPEN ACCESS TO DATA AND CODE,0.8860827512090274,"• While we encourage the release of code and data, we understand that this might not be
1199"
OPEN ACCESS TO DATA AND CODE,0.8866200967221923,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1200"
OPEN ACCESS TO DATA AND CODE,0.8871574422353573,"including code, unless this is central to the contribution (e.g., for a new open-source
1201"
OPEN ACCESS TO DATA AND CODE,0.8876947877485223,"benchmark).
1202"
OPEN ACCESS TO DATA AND CODE,0.8882321332616873,"• The instructions should contain the exact command and environment needed to run to
1203"
OPEN ACCESS TO DATA AND CODE,0.8887694787748522,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1204"
OPEN ACCESS TO DATA AND CODE,0.8893068242880172,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1205"
OPEN ACCESS TO DATA AND CODE,0.8898441698011822,"• The authors should provide instructions on data access and preparation, including how
1206"
OPEN ACCESS TO DATA AND CODE,0.8903815153143472,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1207"
OPEN ACCESS TO DATA AND CODE,0.890918860827512,"• The authors should provide scripts to reproduce all experimental results for the new
1208"
OPEN ACCESS TO DATA AND CODE,0.891456206340677,"proposed method and baselines. If only a subset of experiments are reproducible, they
1209"
OPEN ACCESS TO DATA AND CODE,0.891993551853842,"should state which ones are omitted from the script and why.
1210"
OPEN ACCESS TO DATA AND CODE,0.892530897367007,"• At submission time, to preserve anonymity, the authors should release anonymized
1211"
OPEN ACCESS TO DATA AND CODE,0.893068242880172,"versions (if applicable).
1212"
OPEN ACCESS TO DATA AND CODE,0.8936055883933369,"• Providing as much information as possible in supplemental material (appended to the
1213"
OPEN ACCESS TO DATA AND CODE,0.8941429339065019,"paper) is recommended, but including URLs to data and code is permitted.
1214"
OPEN ACCESS TO DATA AND CODE,0.8946802794196669,"6. Experimental Setting/Details
1215"
OPEN ACCESS TO DATA AND CODE,0.8952176249328319,"Question: Does the paper specify all the training and test details (e.g., data splits,
1216"
OPEN ACCESS TO DATA AND CODE,0.8957549704459967,"hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand
1217"
OPEN ACCESS TO DATA AND CODE,0.8962923159591617,"the results?
1218"
OPEN ACCESS TO DATA AND CODE,0.8968296614723267,"Answer: [Yes]
1219"
OPEN ACCESS TO DATA AND CODE,0.8973670069854917,"Justification: We explicitly have sections and appendices that detail our experimental setup
1220"
OPEN ACCESS TO DATA AND CODE,0.8979043524986566,"(training and evaluation) and title the sections and appendices to indicate this.
1221"
OPEN ACCESS TO DATA AND CODE,0.8984416980118216,"Guidelines:
1222"
OPEN ACCESS TO DATA AND CODE,0.8989790435249866,"• The answer NA means that the paper does not include experiments.
1223"
OPEN ACCESS TO DATA AND CODE,0.8995163890381516,"• The experimental setting should be presented in the core of the paper to a level of detail
1224"
OPEN ACCESS TO DATA AND CODE,0.9000537345513165,"that is necessary to appreciate the results and make sense of them.
1225"
OPEN ACCESS TO DATA AND CODE,0.9005910800644814,"• The full details can be provided either with the code, in appendix, or as supplemental
1226"
OPEN ACCESS TO DATA AND CODE,0.9011284255776464,"material.
1227"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9016657710908114,"7. Experiment Statistical Significance
1228"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9022031166039763,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1229"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9027404621171413,"information about the statistical significance of the experiments?
1230"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9032778076303063,"Answer: [Yes]
1231"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9038151531434713,"Justification: When appropriate we report bootstrap 95% confidence intervals (e.g., in
1232"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9043524986566363,"Figure 4 and Figure 17). We do not train models with many seeds, which is prohibitively
1233"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9048898441698012,"expensive. Given the large size of the C4 validation set, we observe that bootstrap 95%
1234"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9054271896829661,"confidence intervals for loss (computed over either token an sequence sampling) are close to
1235"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9059645351961311,"zero.
1236"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.906501880709296,"Guidelines:
1237"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.907039226222461,"• The answer NA means that the paper does not include experiments.
1238"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.907576571735626,"• The authors should answer ""Yes"" if the results are accompanied by error bars,
1239"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.908113917248791,"confidence intervals, or statistical significance tests, at least for the experiments that
1240"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.908651262761956,"support the main claims of the paper.
1241"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9091886082751209,"• The factors of variability that the error bars are capturing should be clearly stated (for
1242"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9097259537882859,"example, train/test split, initialization, random drawing of some parameter, or overall
1243"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9102632993014508,"run with given experimental conditions).
1244"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9108006448146158,"• The method for calculating the error bars should be explained (closed form formula,
1245"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9113379903277807,"call to a library function, bootstrap, etc.)
1246"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9118753358409457,"• The assumptions made should be given (e.g., Normally distributed errors).
1247"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9124126813541107,"• It should be clear whether the error bar is the standard deviation or the standard error
1248"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9129500268672757,"of the mean.
1249"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9134873723804406,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1250"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9140247178936056,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1251"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9145620634067706,"of Normality of errors is not verified.
1252"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9150994089199355,"• For asymmetric distributions, the authors should be careful not to show in tables or
1253"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9156367544331004,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1254"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9161740999462654,"error rates).
1255"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9167114454594304,"• If error bars are reported in tables or plots, The authors should explain in the text how
1256"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9172487909725954,"they were calculated and reference the corresponding figures or tables in the text.
1257"
EXPERIMENTS COMPUTE RESOURCES,0.9177861364857604,"8. Experiments Compute Resources
1258"
EXPERIMENTS COMPUTE RESOURCES,0.9183234819989253,"Question: For each experiment, does the paper provide sufficient information on the
1259"
EXPERIMENTS COMPUTE RESOURCES,0.9188608275120903,"computer resources (type of compute workers, memory, time of execution) needed to
1260"
EXPERIMENTS COMPUTE RESOURCES,0.9193981730252553,"reproduce the experiments?
1261"
EXPERIMENTS COMPUTE RESOURCES,0.9199355185384201,"Answer: [Yes]
1262"
EXPERIMENTS COMPUTE RESOURCES,0.9204728640515851,"Justification: We are transparent about how many GPU hours it takes to construct our scaling
1263"
EXPERIMENTS COMPUTE RESOURCES,0.9210102095647501,"laws and train our models (e.g., in Table 1).
1264"
EXPERIMENTS COMPUTE RESOURCES,0.9215475550779151,"Guidelines:
1265"
EXPERIMENTS COMPUTE RESOURCES,0.9220849005910801,"• The answer NA means that the paper does not include experiments.
1266"
EXPERIMENTS COMPUTE RESOURCES,0.922622246104245,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1267"
EXPERIMENTS COMPUTE RESOURCES,0.92315959161741,"or cloud provider, including relevant memory and storage.
1268"
EXPERIMENTS COMPUTE RESOURCES,0.923696937130575,"• The paper should provide the amount of compute required for each of the individual
1269"
EXPERIMENTS COMPUTE RESOURCES,0.92423428264374,"experimental runs as well as estimate the total compute.
1270"
EXPERIMENTS COMPUTE RESOURCES,0.9247716281569048,"• The paper should disclose whether the full research project required more compute
1271"
EXPERIMENTS COMPUTE RESOURCES,0.9253089736700698,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1272"
EXPERIMENTS COMPUTE RESOURCES,0.9258463191832348,"didn’t make it into the paper).
1273"
CODE OF ETHICS,0.9263836646963998,"9. Code Of Ethics
1274"
CODE OF ETHICS,0.9269210102095647,"Question: Does the research conducted in the paper conform, in every respect, with the
1275"
CODE OF ETHICS,0.9274583557227297,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1276"
CODE OF ETHICS,0.9279957012358947,"Answer: [Yes]
1277"
CODE OF ETHICS,0.9285330467490597,"Justification: We have reviewed the code of ethics and feel that our research abides by this
1278"
CODE OF ETHICS,0.9290703922622247,"code in every respect.
1279"
CODE OF ETHICS,0.9296077377753896,"Guidelines:
1280"
CODE OF ETHICS,0.9301450832885545,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1281"
CODE OF ETHICS,0.9306824288017195,"• If the authors answer No, they should explain the special circumstances that require a
1282"
CODE OF ETHICS,0.9312197743148845,"deviation from the Code of Ethics.
1283"
CODE OF ETHICS,0.9317571198280494,"• The authors should make sure to preserve anonymity (e.g., if there is a special
1284"
CODE OF ETHICS,0.9322944653412144,"consideration due to laws or regulations in their jurisdiction).
1285"
BROADER IMPACTS,0.9328318108543794,"10. Broader Impacts
1286"
BROADER IMPACTS,0.9333691563675444,"Question: Does the paper discuss both potential positive societal impacts and negative
1287"
BROADER IMPACTS,0.9339065018807093,"societal impacts of the work performed?
1288"
BROADER IMPACTS,0.9344438473938743,"Answer: [Yes]
1289"
BROADER IMPACTS,0.9349811929070392,"Justification: This work is related to predicting the performance of language models, before
1290"
BROADER IMPACTS,0.9355185384202042,"they are trained. As such, it falls under the category of basic research. However, because we
1291"
BROADER IMPACTS,0.9360558839333691,"produce generative language model artifacts as part of our paper, we recognize that these
1292"
BROADER IMPACTS,0.9365932294465341,"pre-trained models can pose risk. We provide a discussion of risks in Appendix G.
1293"
BROADER IMPACTS,0.9371305749596991,"Guidelines:
1294"
BROADER IMPACTS,0.9376679204728641,"• The answer NA means that there is no societal impact of the work performed.
1295"
BROADER IMPACTS,0.938205265986029,"• If the authors answer NA or No, they should explain why their work has no societal
1296"
BROADER IMPACTS,0.938742611499194,"impact or why the paper does not address societal impact.
1297"
BROADER IMPACTS,0.939279957012359,"• Examples of negative societal impacts include potential malicious or unintended uses
1298"
BROADER IMPACTS,0.9398173025255239,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1299"
BROADER IMPACTS,0.9403546480386888,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1300"
BROADER IMPACTS,0.9408919935518538,"groups), privacy considerations, and security considerations.
1301"
BROADER IMPACTS,0.9414293390650188,"• The conference expects that many papers will be foundational research and not tied
1302"
BROADER IMPACTS,0.9419666845781838,"to particular applications, let alone deployments. However, if there is a direct path to
1303"
BROADER IMPACTS,0.9425040300913488,"any negative applications, the authors should point it out. For example, it is legitimate
1304"
BROADER IMPACTS,0.9430413756045137,"to point out that an improvement in the quality of generative models could be used to
1305"
BROADER IMPACTS,0.9435787211176787,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1306"
BROADER IMPACTS,0.9441160666308437,"that a generic algorithm for optimizing neural networks could enable people to train
1307"
BROADER IMPACTS,0.9446534121440086,"models that generate Deepfakes faster.
1308"
BROADER IMPACTS,0.9451907576571735,"• The authors should consider possible harms that could arise when the technology is
1309"
BROADER IMPACTS,0.9457281031703385,"being used as intended and functioning correctly, harms that could arise when the
1310"
BROADER IMPACTS,0.9462654486835035,"technology is being used as intended but gives incorrect results, and harms following
1311"
BROADER IMPACTS,0.9468027941966685,"from (intentional or unintentional) misuse of the technology.
1312"
BROADER IMPACTS,0.9473401397098334,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1313"
BROADER IMPACTS,0.9478774852229984,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1314"
BROADER IMPACTS,0.9484148307361634,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1315"
BROADER IMPACTS,0.9489521762493284,"feedback over time, improving the efficiency and accessibility of ML).
1316"
SAFEGUARDS,0.9494895217624932,"11. Safeguards
1317"
SAFEGUARDS,0.9500268672756582,"Question: Does the paper describe safeguards that have been put in place for responsible
1318"
SAFEGUARDS,0.9505642127888232,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1319"
SAFEGUARDS,0.9511015583019882,"image generators, or scraped datasets)?
1320"
SAFEGUARDS,0.9516389038151531,"Answer: [Yes]
1321"
SAFEGUARDS,0.9521762493283181,"Justification: We provide discussion of responsible release in Appendix G. Specifically,
1322"
SAFEGUARDS,0.9527135948414831,"models in this release are know to be less capable than state-of-the-art, publicly available
1323"
SAFEGUARDS,0.9532509403546481,"models [113, 114, 49], and, hence, we feel the risk for misuse is low.
1324"
SAFEGUARDS,0.9537882858678131,"Guidelines:
1325"
SAFEGUARDS,0.9543256313809779,"• The answer NA means that the paper poses no such risks.
1326"
SAFEGUARDS,0.9548629768941429,"• Released models that have a high risk for misuse or dual-use should be released with
1327"
SAFEGUARDS,0.9554003224073079,"necessary safeguards to allow for controlled use of the model, for example by requiring
1328"
SAFEGUARDS,0.9559376679204729,"that users adhere to usage guidelines or restrictions to access the model or implementing
1329"
SAFEGUARDS,0.9564750134336378,"safety filters.
1330"
SAFEGUARDS,0.9570123589468028,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1331"
SAFEGUARDS,0.9575497044599678,"should describe how they avoided releasing unsafe images.
1332"
SAFEGUARDS,0.9580870499731328,"• We recognize that providing effective safeguards is challenging, and many papers do
1333"
SAFEGUARDS,0.9586243954862977,"not require this, but we encourage authors to take this into account and make a best
1334"
SAFEGUARDS,0.9591617409994626,"faith effort.
1335"
LICENSES FOR EXISTING ASSETS,0.9596990865126276,"12. Licenses for existing assets
1336"
LICENSES FOR EXISTING ASSETS,0.9602364320257926,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1337"
LICENSES FOR EXISTING ASSETS,0.9607737775389575,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1338"
LICENSES FOR EXISTING ASSETS,0.9613111230521225,"properly respected?
1339"
LICENSES FOR EXISTING ASSETS,0.9618484685652875,"Answer: [Yes]
1340"
LICENSES FOR EXISTING ASSETS,0.9623858140784525,"Justification: We utilize data-sources publicly available on the HuggingFace platform and
1341"
LICENSES FOR EXISTING ASSETS,0.9629231595916175,"abide by the terms of use. For C4: Open Data Commons License Attribution family, for
1342"
LICENSES FOR EXISTING ASSETS,0.9634605051047824,"RedPajama: a list of licenses (found here.), for RefinedWeb: Open Data Commons License
1343"
LICENSES FOR EXISTING ASSETS,0.9639978506179473,"Attribution family. We use the OpenLM repo for training and also abide by their MIT license.
1344"
LICENSES FOR EXISTING ASSETS,0.9645351961311123,"We cite all papers and repos in the main text.
1345"
LICENSES FOR EXISTING ASSETS,0.9650725416442772,"Guidelines:
1346"
LICENSES FOR EXISTING ASSETS,0.9656098871574422,"• The answer NA means that the paper does not use existing assets.
1347"
LICENSES FOR EXISTING ASSETS,0.9661472326706072,"• The authors should cite the original paper that produced the code package or dataset.
1348"
LICENSES FOR EXISTING ASSETS,0.9666845781837722,"• The authors should state which version of the asset is used and, if possible, include a
1349"
LICENSES FOR EXISTING ASSETS,0.9672219236969372,"URL.
1350"
LICENSES FOR EXISTING ASSETS,0.9677592692101021,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1351"
LICENSES FOR EXISTING ASSETS,0.9682966147232671,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1352"
LICENSES FOR EXISTING ASSETS,0.968833960236432,"service of that source should be provided.
1353"
LICENSES FOR EXISTING ASSETS,0.969371305749597,"• If assets are released, the license, copyright information, and terms of use in the
1354"
LICENSES FOR EXISTING ASSETS,0.9699086512627619,"package should be provided. For popular datasets, paperswithcode.com/datasets
1355"
LICENSES FOR EXISTING ASSETS,0.9704459967759269,"has curated licenses for some datasets. Their licensing guide can help determine the
1356"
LICENSES FOR EXISTING ASSETS,0.9709833422890919,"license of a dataset.
1357"
LICENSES FOR EXISTING ASSETS,0.9715206878022569,"• For existing datasets that are re-packaged, both the original license and the license of
1358"
LICENSES FOR EXISTING ASSETS,0.9720580333154218,"the derived asset (if it has changed) should be provided.
1359"
LICENSES FOR EXISTING ASSETS,0.9725953788285868,"• If this information is not available online, the authors are encouraged to reach out to
1360"
LICENSES FOR EXISTING ASSETS,0.9731327243417518,"the asset’s creators.
1361"
NEW ASSETS,0.9736700698549167,"13. New Assets
1362"
NEW ASSETS,0.9742074153680816,"Question: Are new assets introduced in the paper well documented and is the documentation
1363"
NEW ASSETS,0.9747447608812466,"provided alongside the assets?
1364"
NEW ASSETS,0.9752821063944116,"Answer: [Yes]
1365"
NEW ASSETS,0.9758194519075766,"Justification: Our code release documents all new model assets under the exp_db/ folder
1366"
NEW ASSETS,0.9763567974207416,"and includes a MIT license. This is also specified in Appendix H.
1367"
NEW ASSETS,0.9768941429339065,"Guidelines:
1368"
NEW ASSETS,0.9774314884470715,"• The answer NA means that the paper does not release new assets.
1369"
NEW ASSETS,0.9779688339602365,"• Researchers should communicate the details of the dataset/code/model as part of their
1370"
NEW ASSETS,0.9785061794734013,"submissions via structured templates. This includes details about training, license,
1371"
NEW ASSETS,0.9790435249865663,"limitations, etc.
1372"
NEW ASSETS,0.9795808704997313,"• The paper should discuss whether and how consent was obtained from people whose
1373"
NEW ASSETS,0.9801182160128963,"asset is used.
1374"
NEW ASSETS,0.9806555615260613,"• At submission time, remember to anonymize your assets (if applicable). You can either
1375"
NEW ASSETS,0.9811929070392262,"create an anonymized URL or include an anonymized zip file.
1376"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817302525523912,"14. Crowdsourcing and Research with Human Subjects
1377"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822675980655562,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1378"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828049435787212,"include the full text of instructions given to participants and screenshots, if applicable, as
1379"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983342289091886,"well as details about compensation (if any)?
1380"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983879634605051,"Answer: [NA]
1381"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984416980118216,"Justification: This research does not involve crowdsourcing or human subjects.
1382"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984954325631381,"Guidelines:
1383"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854916711445459,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1384"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860290166577109,"human subjects.
1385"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865663621708759,"• Including this information in the supplemental material is fine, but if the main
1386"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871037076840409,"contribution of the paper involves human subjects, then as much detail as possible
1387"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876410531972059,"should be included in the main paper.
1388"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881783987103707,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1389"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887157442235357,"or other labor should be paid at least the minimum wage in the country of the data
1390"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892530897367007,"collector.
1391"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897904352498657,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1392"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903277807630306,"Subjects
1393"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908651262761956,"Question: Does the paper describe potential risks incurred by study participants, whether
1394"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914024717893606,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1395"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919398173025256,"approvals (or an equivalent approval/review based on the requirements of your country or
1396"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924771628156905,"institution) were obtained?
1397"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930145083288554,"Answer: [NA]
1398"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935518538420204,"Justification: This paper does not involve research with human subjects.
1399"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940891993551854,"Guidelines:
1400"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946265448683503,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1401"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951638903815153,"human subjects.
1402"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957012358946803,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1403"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9962385814078453,"may be required for any human subjects research. If you obtained IRB approval, you
1404"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967759269210102,"should clearly state this in the paper.
1405"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973132724341752,"• We recognize that the procedures for this may vary significantly between institutions
1406"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978506179473401,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1407"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983879634605051,"guidelines for their institution.
1408"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99892530897367,"• For initial submissions, do not include any information that would break anonymity (if
1409"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.999462654486835,"applicable), such as the institution conducting the review.
1410"
