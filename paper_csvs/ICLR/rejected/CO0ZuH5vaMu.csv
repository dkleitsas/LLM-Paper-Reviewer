Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017421602787456446,"Translating source code from one programming language to another is a critical,
time-consuming task in modernizing legacy applications and codebases. Recent
work in this space has drawn inspiration from the software naturalness hypothesis
by applying natural language processing techniques towards automating the code
translation task. However, due to the paucity of parallel data in this domain, super-
vised techniques have only been applied to a limited set of popular programming
languages. To bypass this limitation, unsupervised neural machine translation
techniques have been proposed to learn code translation using only monolingual
corpora. In this work, we propose to use document similarity methods to create
noisy parallel datasets of code, thus enabling supervised techniques to be applied
for automated code translation without having to rely on the availability or expen-
sive curation of parallel code datasets. We explore the noise tolerance of models
trained on such automatically-created datasets and show that these models per-
form comparably to models trained on ground truth for reasonable levels of noise.
Finally, we exhibit the practical utility of the proposed method by creating parallel
datasets for languages beyond the ones explored in prior work, thus expanding the
set of programming languages for automated code translation."
INTRODUCTION,0.003484320557491289,"1
INTRODUCTION"
INTRODUCTION,0.005226480836236934,"As the pace of software development increases and the famous adage “software is eating the
world” (Andreessen, 2011) is borne out, there is a corresponding increase in the amount of source
code and number of software artefacts in active use for which support has lapsed. At the same time,
the number of software professionals and programmers who can support and understand such code
is unable to keep pace with the rate at which it is produced. This problem, while important when
it comes to relatively modern programming languages (such as Java and Python), becomes even
more pressing when it come to legacy languages (like COBOL) that mission-critical applications
and systems are written in (Charette, 2020). In recent years, there have been multiple instances of
organizations struggling to maintain their legacy systems and making considerable investments to
upgrade them. In 2021 the Commonwealth Bank of Australia upgraded its core banking platform
originally written in COBOL: this ultimately took 5 years and more than 1 Billion AUD to com-
plete (Irrera, 2017). During the COVID-19 pandemic, software systems implemented in COBOL
slowed down the release of US unemployment stimulus checks (Kelly, 2020), leaving governments
scrambling to ﬁnd COBOL experts who were already hard to come by. A recent study by the United
States Government Accountability Ofﬁce (Walsh, 2021) has identiﬁed 65 critical federal legacy sys-
tems in need of urgent modernization. Some of these systems are over 50 years old, and cost millions
of dollars annually to operate and maintain."
INTRODUCTION,0.006968641114982578,"Parallel to these developments are recent efforts at the intersection of software engineering, machine
learning (ML), and natural language processing (NLP), which have posited the naturalness hypoth-
esis of software (Hindle et al., 2016). The hypothesis states that “...Software is a form of human
communication; software corpora have similar statistical properties to natural language corpora;
and these properties can be exploited to build better software engineering tools” (Allamanis et al.,
2018). This hypothesis has been used to extend breakthroughs and advances from various NLP
sub-ﬁelds to software engineering tasks such as code translation. Prior works in the code trans-
lation domain have proposed the application of statistical, supervised, and unsupervised machine
translation techniques to learn code translation models to varying degrees of success."
INTRODUCTION,0.008710801393728223,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.010452961672473868,"A key limitation of a majority of the proposed code translation approaches, however, is the lack of
availability of parallel data for training. Unlike natural language, where a piece of text is verbatim
translated in multiple languages – legal documents, parliamentary proceedings in multilingual soci-
eties – code is rarely implemented as is in multiple languages; thus making it hard to create parallel
datasets. A few limited datasets – such as Java ↔C# (Nguyen et al., 2013) and AVATAR for Java
↔Python (Ahmad et al., 2021b) – are currently available. However, these are extremely limited
in the number of programming language they cover, and manually curating a dataset for a speciﬁc
use-case is impractical. To bypass this limitation, unsupervised techniques have been applied to the
code translation task. Unsupervised techniques come with their own limitations however; and often,
supervised techniques can outperform them when the source and target corpora are from different
domains, the source and target languages use different scripts, and on low-resource language pairs,
among other concerns (Kim et al., 2020; Marchisio et al., 2020)."
INTRODUCTION,0.012195121951219513,"It is for this reason that in this work, we focus on one of the main blockers impeding the application
of supervised techniques to code translation: the availability of parallel corpora and datasets. Specif-
ically, we propose to utilize document similarity methods to create parallel source code datasets that
are noisy by design. In this work, we empirically demonstrate the effectiveness of document sim-
ilarity methods in creating such parallel datasets with high levels of accuracy. Given that datasets
created in this manner are bound to be noisy, we study the performance characteristics of models
for code translation that have been trained on data with varying degrees of noise; and show that
these models have considerable resistance to noise and perform well even with moderate amounts of
noise. Finally, we demonstrate the practical utility of the proposed approach by training models to
translate between 10 pairs of languages – a majority of which have not been looked at in prior work."
RELATED WORK,0.013937282229965157,"2
RELATED WORK"
RELATED WORK,0.0156794425087108,"Code translation datasets:
Typical methods for creating parallel datasets for code translation
have either relied on the availability of open-sourced projects with implementations in multiple lan-
guages, or on the existence of transpilers. The earliest widely-used large-scale dataset for code
translation was for Java ↔C# (Nguyen et al., 2013) translation, created by indexing open-sourced
projects implemented in both languages. Aggarwal et al. (2015) used the Python 2to3 1 transpiler
to create a dataset; while Chen et al. (2018) used CoffeeScript’s compiler (which compiles down to
JavaScript) to create a parallel dataset. More recently, Ahmad et al. (2021b) released AVATAR – a
parallel corpus of Java to Python manually curated through submissions on competitive program-
ming websites. Publicly available datasets for code translation are however extremely limited, and
manually curating these datasets for a speciﬁc use-case is expensive and often impractical."
RELATED WORK,0.017421602787456445,"Source-to-Source translation:
The earliest code translation models were rule-based systems, op-
erating on handcrafted rules. These systems require a lot of effort to build, are not easily extend-
able to other languages, and are also outperformed by neural techniques. Some of these systems
are: Java2CSharp 2, Java2Python 3, SmallTalk to C (Yasumatsu & Doi, 1995), Cobol to
Java (Mossienko, 2003), and Tangible Software Solutions 4 (VB.NET, C#, Java, C++, and Python).
Moving away from rule-based systems, Nguyen et al. (2013), Karaivanov et al. (2014), and Nguyen
et al. (2014) applied different versions of Phrase-Based Statistical Machine Translation to translate
between Java and C#. Chen et al. (2018) proposed a tree-to-tree neural network to translate the
parsed tree of the source code into the target code parse tree. The aforementioned supervised tech-
niques have all been benchmarked on the Java ↔C# dataset, and are limited by the availability of
parallel datasets. To bypass this limitation, Roziere et al. (2020) used unsupervised neural machine
translation techniques to translate between languages using only monolingual corpora, and showed
impressive results for translation between Java, C++, and Python. While Roziere et al. (2020) trained
the model speciﬁcally for code translation, large language models – such as GPT-2 (Radford et al.,
2019), GPT-3 (Brown et al., 2020), and Codex (Chen et al., 2021) – have also been shown to have
some competence in generating code (Hendrycks et al., 2021)."
RELATED WORK,0.01916376306620209,"Parallel corpus mining:
Prior work in natural language research has looked at various ways of
creating parallel corpora from a non-parallel corpus. Munteanu & Marcu (2005) train a maximum"
RELATED WORK,0.020905923344947737,"1https://docs.python.org/3/library/2to3.html
2https://sourceforge.net/projects/j2cstranslator/
3https://github.com/natural/java2python
4https://www.tangiblesoftwaresolutions.com/"
RELATED WORK,0.02264808362369338,Under review as a conference paper at ICLR 2022
RELATED WORK,0.024390243902439025,"entropy classiﬁer to identify if two given sentences are translations of each other. They extract
parallel data from large-scale Chinese, Arabic, and English non-parallel newspaper corpora, and
show improvement in model performance when trained with a combination of a small parallel corpus
and the extracted dataset. Uszkoreit et al. (2010) describe a system that uses n-gram features to mine
parallel documents from a billion-scale corpus. Smith et al. (2010) focus on aligning Wikipedia
documents by creating features suitable for such documents. Artetxe & Schwenk (2019) utilize
speciﬁc scoring functions based on multilingual sentence embeddings to create parallel corpora,
and Hangya & Fraser (2019) rely on continuous parallel segments rather than word similarities to
ﬁnd parallel sentences. Ban´on et al. (2020) released the largest publicly available parallel corpora
of sentences (223 million parallel sentences) by aligning sentences from data crawled over the web.
There is a substantial precedence of parallel corpus mining in the natural language domain; however,
such studies in the code translation domain are non-existent."
RELATED WORK,0.02613240418118467,"Machine Translation using noisy data:
Prior studies have aimed to study the impact of noise
on the performance of machine translation systems. Formiga & Fonollosa (2012) study the impact
of misspelled words on the performance of Statistical Machine Translation and suggest strategies
to deal with them, while Goutte et al. (2012) study the impact of sentence alignment errors on the
performance of SMT. Further, Khayrallah & Koehn (2018) deﬁne 5 categories of artiﬁcial noise in
Neural Machine Translation, and study the impact each of these types has on performance. We moti-
vate our work from these prior efforts in order to study the impact that noise has on the performance
of code translation models."
PROPOSED METHOD,0.027874564459930314,"3
PROPOSED METHOD"
PROPOSED METHOD,0.029616724738675958,"In this work, we propose to utilize document similarity methods to create noisy parallel datasets for
code translation. We refer to the datasets created in this manner as “noisy” because unlike manually
curated datasets, there is no guarantee of a parallel implementation of the speciﬁc source ﬁle/code
being available in the corpus: this may result in near-similar code samples being paired as ground
truth examples instead. Algorithm 1 presents the proposed approach as pseudocode."
PROPOSED METHOD,0.0313588850174216,Algorithm 1 Creating parallel code corpus
PROPOSED METHOD,0.033101045296167246,"1: CreateParallelCorpora(D, D
′, M, δ)
2: initialize P = {}
3: for i = 1 to |D| do
4:
Dsim = GetSimilarDocuments(di, D
′, M)
5:
for (di, d
′
j) in Dsim do"
PROPOSED METHOD,0.03484320557491289,"6:
if (·, d
′
j) /∈P and M(di, d
′
j) ≤δ then"
PROPOSED METHOD,0.036585365853658534,"7:
P = P ∪(di, d
′
j)
8:
break
9: Dres = sort((d1, d2) ∈P, key = M(d1, d2))
10: Return: Dres"
PROPOSED METHOD,0.03832752613240418,"The algorithm expects two non-parallel sets
of documents D = {d1, · · · , dn} and D
′ =
{d
′
1, · · · , d
′
m} as input. Within the context
of our work, the documents in these two sets
represent code samples from two distinct pro-
gramming languages. Along with the docu-
ments, the algorithm also expects a similarity
measure M(d, d
′) as input, to compare two
given documents for similarity. A lower score
from the similarity measure indicates higher
similarity between documents.
Finally, the
algorithm expects a similarity threshold δ to
help keep only sufﬁciently similar documents
in the resulting parallel corpus. Thereafter,
the algorithm follows a simple procedure of
iterating over all documents; ﬁnding the most similar documents in the target set; and adding the
newly found similar document pairs to the result only if the target document has not been paired be-
fore, and if the similarity is below the threshold value. Once all the documents are iterated upon, the
algorithm produces a list of unique pairs of code segments (documents) ordered by their similarity,
ready to be used for downstream tasks."
EXPERIMENTAL SETUP,0.04006968641114982,"4
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.041811846689895474,"To better understand the effectiveness and practical utility of the method proposed in Section 3, we
devise 3 research questions and design experiments to empirically answer them (see Section 5). In
this section, we brieﬂy summarize the different document similarity methods, datasets, pre-trained
models, and evaluation metrics we use in our experiments."
EXPERIMENTAL SETUP,0.04355400696864112,Under review as a conference paper at ICLR 2022
DOCUMENT SIMILARITY METHODS,0.04529616724738676,"4.1
DOCUMENT SIMILARITY METHODS"
DOCUMENT SIMILARITY METHODS,0.047038327526132406,"TF-IDF:
TF-IDF (Salton & Buckley, 1988) computes the product of the term frequency (TF)
(fraction of times a term appears in a document) with the inverse document frequency (IDF) (log-
arithm of the inverse fraction of documents a particular token occurs in). The cosine similarity
between the document vectors thus created computes the similarity of documents."
DOCUMENT SIMILARITY METHODS,0.04878048780487805,"Okapi-BM25:
The Okapi-BM25 model (Robertson et al., 1995) uses the following scoring func-
tion (Equation 1) to score the importance of a word w in a document D. Here, IDF(w) represents
the inverse document frequency of the word w, TF(w, D) represents the term frequency of the
word w in the document D, |D| and Davg are the lengths of the current document and the average
document lengths respectively, and k1 and b are free parameters of the model."
DOCUMENT SIMILARITY METHODS,0.050522648083623695,"BM25(w, D) = IDF(w) ×
TF(w, D)(k1 + 1)"
DOCUMENT SIMILARITY METHODS,0.05226480836236934,"TF(w, D) + k1(1 −b + b |D|"
DOCUMENT SIMILARITY METHODS,0.05400696864111498,"Davg )
(1)"
DOCUMENT SIMILARITY METHODS,0.05574912891986063,"Latent Dirichlet Allocation (LDA):
LDA (Blei et al., 2003) is a hierarchical generative Bayesian
model that models each document as a ﬁnite mixture over an underlying set of topics. The cosine
similarity of the topic distribution of two documents computes their similarity."
DOCUMENT SIMILARITY METHODS,0.05749128919860627,"Latent Semantic Indexing (LSI):
LSI (Deerwester et al., 1990) computes the Singular Value
Decomposition of the Bag of Words representation of documents. The cosine similarity of the
decomposed vectors of documents computes their similarity."
DOCUMENT SIMILARITY METHODS,0.059233449477351915,"Word Movers Distance (WMD):
WMD (Kusner et al., 2015) models the document distance
problem as a variant of the Earth Movers Distance (Monge, 1781; Rubner et al., 1998) and solves
the optimization problem deﬁned in Equation 2."
DOCUMENT SIMILARITY METHODS,0.06097560975609756,"min
T ≥0 n
X"
DOCUMENT SIMILARITY METHODS,0.0627177700348432,"i,j=1
Tijc(i, j)"
DOCUMENT SIMILARITY METHODS,0.06445993031358885,"subject to: n
X"
DOCUMENT SIMILARITY METHODS,0.06620209059233449,"j=1
Tij = di
∀i ∈{1, · · · , n} n
X"
DOCUMENT SIMILARITY METHODS,0.06794425087108014,"i=1
Tij = dj
∀j ∈{1, · · · , n} (2)"
DOCUMENT SIMILARITY METHODS,0.06968641114982578,"Here, T ∈Rn×n is a ﬂow matrix where Tij ≥0 and denotes how much of word i in document d
travels to word j in document d
′. c(i, j) = ∥xi −xj∥2 is the cost associated with travelling from
one word to another, d and d
′ are the nBOW representations of the documents, and X ∈Rd×n is the
word embedding matrix where xi ∈Rd represents the d-dimensional embedding of the ith word."
DATASETS,0.07142857142857142,"4.2
DATASETS"
DATASETS,0.07317073170731707,"For the experiments whose results are detailed in Section 5, we utilize the following datasets. We
provide representative code samples and statistics from the datasets in Appendix A."
DATASETS,0.07491289198606271,"Java ↔C#:
The Java ↔C# dataset is one of the earliest large-scale datasets introduced for the
code translation task (Nguyen et al., 2013; Zhong et al., 2010). It is created by indexing several
open-source projects which have both Java and C# implementations, and pairing methods with the
same ﬁle name and method name. The earlier version of this data was created by indexing the db4o
and Lucene projects. More recently however, Chen et al. (2018) indexed 6 open-sourced projects
to create the dataset. We use the version provided by Chen et al. (2018) in our work."
DATASETS,0.07665505226480836,"Java ↔Python, Java ↔C++, and Python ↔C++:
Roziere et al. (2020) extracted parallel func-
tions in C++, Python, and Java from the online competitive programming platform GeeksForGeeks5,"
DATASETS,0.078397212543554,5https://practice.geeksforgeeks.org/
DATASETS,0.08013937282229965,Under review as a conference paper at ICLR 2022
DATASETS,0.08188153310104529,"and used these code samples as validation and test sets. We, however, concatenate the two datasets
and use the uniﬁed dataset for our experiments. The code samples in this dataset are function-scope
code samples that solve an algorithmic problem."
DATASETS,0.08362369337979095,"CodeNet:
Project CodeNet (Puri et al., 2021) is a recently released large-scale AI for Code dataset,
created by indexing two online competitive programming websites. The dataset is organized into
about 4000 different problem sets, and contains a little under 14 million total solutions in 55 pro-
gramming languages. Besides providing the code samples, CodeNet also provides input-output pairs
to evaluate solutions to the problem sets."
MODELS,0.08536585365853659,"4.3
MODELS"
MODELS,0.08710801393728224,"CodeBERT:
CodeBERT (Feng et al., 2020) is a Transformer (Vaswani et al., 2017) based model,
pre-trained on a unimodal data of function-level code samples, and a bimodal data of code and
the associated documentation in natural language. The pre-training data contains code samples in
Go, Java, JavaScript, PHP, Python, and Ruby, and is trained using the Masked Language Modeling
(Devlin et al., 2019) (MLM) and the Replaced Token Detection (Clark et al., 2019) objectives."
MODELS,0.08885017421602788,"GraphCodeBERT:
GraphCodeBERT (Guo et al., 2020) is a Transformer based model for code
that also considers the inherent structure in code by integrating the data ﬂow in the pre-training
stage. The model is trained on the CodeSearchNet dataset (Husain et al., 2019) using the MLM,
Edge Prediction, and Node Alignment objectives."
MODELS,0.09059233449477352,"PLBART:
PLBART (Ahmad et al., 2021a) is a BART (Lewis et al., 2020) based model pre-trained
on over 700 million Java, Python, and natural language documents collected from open-sourced code
on Github and posts on StackOverﬂow. The model is pre-trained via denoising autoencoding, where
the model learns to reconstruct input corrupted by a noise function. The authors use three noising
strategies: token masking, token deletion, and token inﬁlling to create the corrupted inputs."
EVALUATION METRICS,0.09233449477351917,"4.4
EVALUATION METRICS"
EVALUATION METRICS,0.09407665505226481,"BLEU score:
BLEU score (Papineni et al., 2002) is a common automatic evaluation metric for
machine-generated text, and exhibits a high correlation with human judgment of quality. BLEU
score is computed as the overlapping fraction of n-grams between the machine-generated text and
the reference text. The metric has however been shown to not be a reliable measure for source code
(Ren et al., 2020; Allamanis et al., 2018; Austin et al., 2021)."
EVALUATION METRICS,0.09581881533101046,"CodeBLEU score:
Ren et al. (2020) propose the CodeBLEU score to leverage the tree structure
and semantic information in code. It is computed as a combination of the standard BLEU score,
weighted n-gram match, syntactic abstract syntax tree match, and the semantic data ﬂow match."
EVALUATION METRICS,0.0975609756097561,"Exact Match (EM):
EM (Nguyen et al., 2013) evaluates if the generated code matches exactly to
the reference code."
EVALUATION METRICS,0.09930313588850175,"Computational Accuracy @ k (CA@k):
Recent work in code synthesis has adopted the CA@k
metric (Austin et al., 2021; Roziere et al., 2020) to evaluate code generation models. To compute
CA@k, k samples are generated from the model, and the problem is considered solved if any of the
generated k samples pass the unit tests associated with the problem."
RESEARCH QUESTIONS AND RESULTS,0.10104529616724739,"5
RESEARCH QUESTIONS AND RESULTS"
RESEARCH QUESTIONS AND RESULTS,0.10278745644599303,"To validate the central hypothesis of this paper – using document similarity methods to create
datasets for supervised training of code translation models – we deﬁne and seek answers to the
following research questions (RQ):"
RESEARCH QUESTIONS AND RESULTS,0.10452961672473868,"RQ1:
How accurate are document similarity methods in creating parallel datasets for code?"
RESEARCH QUESTIONS AND RESULTS,0.10627177700348432,"RQ2:
Given the created dataset will be noisy, what is the effect of varying degrees of noise on
code translation models?"
RESEARCH QUESTIONS AND RESULTS,0.10801393728222997,"RQ3:
Can the proposed method be used in practice to train models for programming languages
not explored in prior work?"
RESEARCH QUESTIONS AND RESULTS,0.10975609756097561,Under review as a conference paper at ICLR 2022
RESEARCH QUESTIONS AND RESULTS,0.11149825783972125,"5.1
RQ1: EFFICACY OF DOCUMENT SIMILARITY METHODS"
RESEARCH QUESTIONS AND RESULTS,0.1132404181184669,"We start our analysis by examining how effective document similarity methods are in creating code
translation datasets. For this experiment, we utilize 4 datasets with known ground-truth mapping
between pairs of programming languages – Java ↔C#, Java ↔Python, Java ↔C++, and Python
↔C++. For each of these datasets, we create a parallel dataset using 5 different document similarity
methods, and compute the match accuracy as the number of correctly matched code samples."
RESEARCH QUESTIONS AND RESULTS,0.11498257839721254,"We summarize the results for this experiment in Table 1, and observe that similarity methods that
operate in a latent space (such as LDA and LSI) perform much worse than methods that operate in
the original space (such as TF-IDF, Okapi-BM25, and WMD). We posit that because code is written
in a more formal language than natural language, and each data sample in the datasets used in this
experiment implements an independent unique function, there is likely no underlying topic or latent
semantic associations that can be captured by LSI and LDA. Therefore these methods perform worse
than methods that directly utilize the tokens in the original space."
RESEARCH QUESTIONS AND RESULTS,0.11672473867595819,"Table 1: Match accuracy of parallel datasets created using various document similarity methods.
Match accuracy computes True if the matched code sample is the same as the code sample in the
ground truth dataset."
RESEARCH QUESTIONS AND RESULTS,0.11846689895470383,"Java ↔C#
Java ↔Python
Java ↔C++
Python ↔C++
N (→)
10,300
1,418
1,418
1,418
LDA
47.21%
21.44%
35.83%
16.64%
LSI
57.21%
66.08%
87.66%
78.84%
TF-IDF
87.36%
86.10%
94.08%
89.35%
Okapi-BM25
87.86%
89.91%
95.77%
89.99%
WMD
89.53%
91.04%
95.06%
94.08%"
RESEARCH QUESTIONS AND RESULTS,0.12020905923344948,"We note that the datasets used for the experiments in Table 1 are not true representatives of code
we expect to ﬁnd while using this method in practice. This is due to the fact that the 4 datasets
used contain code samples for which a true parallel implementation in the other language exists.
When trying to create a parallel dataset from code collected in the wild, we cannot be sure of the
availability of a true parallel implementation, which might affect the performance of the proposed
method. Thus, to account for this phenomenon, we conduct a similar experiment with the CodeNet
dataset. Since code samples in the CodeNet dataset are not parallel implementations, this gives us
a better idea of the effectiveness of document similarity methods in creating parallel datasets from
code in the wild. We select 6 languages and randomly sub-sample 50 problems from the dataset. For
each of the code sample in each of the 50 sampled problem sets, we create a parallel dataset using 3
different document similarity methods. Since we do not have a ground-truth parallel implementation
available for the CodeNet dataset, we cannot compute the match accuracy like we did in the previous
experiment. We therefore compute the pseudo-match accuracy instead. The pseudo-match accuracy
computes True if the matched code sample is a solution of the same problem set, and False otherwise."
RESEARCH QUESTIONS AND RESULTS,0.12195121951219512,"We report the pseudo-match accuracy results for this experiment in Table 2. We note that while the
TF-IDF and Okapi-BM25 methods performed well in the previous experiment, their performance
varies greatly for this experiment. Datasets created using TF-IDF and Okapi-BM25 methods are
matched to the correct problem set with as little as 30% accuracy and as high as 70% accuracy in
some cases. Datasets created using the WMD method however achieve a high match accuracy for
both experiments (Table 1 and Table 2)."
RESEARCH QUESTIONS AND RESULTS,0.12369337979094076,"From the experiments designed to answer RQ1, we conclude that document similarity methods are
capable of creating parallel datasets of code with a signiﬁcantly high degree of match accuracy.
Speciﬁcally, the WMD metric seems to be quite adept at delineating datasets for code translation."
RESEARCH QUESTIONS AND RESULTS,0.1254355400696864,"5.2
RQ2: NOISE TOLERANCE OF MODELS TRAINED ON CODE"
RESEARCH QUESTIONS AND RESULTS,0.12717770034843207,"In the previous section, we showed that document similarity methods – and speciﬁcally the Word
Movers Distance (WMD) – are quite adept at creating parallel datasets for code. However, datasets
created in this manner contain errors; therefore in this section, we seek to understand the effect that
varying the degree of such noise has on the performance of code translation models."
RESEARCH QUESTIONS AND RESULTS,0.1289198606271777,Under review as a conference paper at ICLR 2022
RESEARCH QUESTIONS AND RESULTS,0.13066202090592335,"Table 2: Pseudo-match accuracy of datasets created by different document similarity methods on 50
subsampled problems from CodeNet. Pseudo-match accuracy computes True if the matched code
sample is from the same problem set, and False otherwise."
RESEARCH QUESTIONS AND RESULTS,0.13240418118466898,"Source language: Go
Java
JavaScript
PHP
Python
Ruby
TF-IDF
29.56%
41.91%
30.70%
40.35%
27.07%
BM25
50.93%
49.07%
50.52%
36.93%
35.37%
WMD
71.47%
55.70%
51.35%
60.89%
45.12%"
RESEARCH QUESTIONS AND RESULTS,0.13414634146341464,"Source language: Java
Go
JavaScript
PHP
Python
Ruby
TF-IDF
65.05%
53.40%
29.41%
31.32%
23.47%
BM25
63.94%
73.46%
34.11%
51.63%
36.09%
WMD
79.30%
73.93%
57.51%
66.49%
56.06%"
RESEARCH QUESTIONS AND RESULTS,0.13588850174216027,"Source language: JavaScript
Go
Java
PHP
Python
Ruby
TF-IDF
50.93%
46.64%
44.89%
41.18%
55.34%
BM25
51.74%
58.12%
56.26%
59.74%
61.60%
WMD
66.70%
74.71%
67.52%
70.88%
73.55%"
RESEARCH QUESTIONS AND RESULTS,0.13763066202090593,"Source language: PHP
Go
Java
JavaScript
Python
Ruby
TF-IDF
64.31%
27.49%
45.66%
26.04%
37.62%
BM25
62.54%
50.48%
48.71%
27.97%
55.63%
WMD
71.38%
61.25%
73.79%
85.05%
84.08%"
RESEARCH QUESTIONS AND RESULTS,0.13937282229965156,"Source language: Python
Go
Java
JavaScript
PHP
Ruby
TF-IDF
67.19%
52.79%
66.67%
69.54%
72.46%
BM25
73.52%
57.53%
74.13%
73.33%
80.28%
WMD
82.08%
73.75%
77.39%
79.59%
87.31%"
RESEARCH QUESTIONS AND RESULTS,0.14111498257839722,"Source language: Ruby
Go
Java
JavaScript
PHP
Python
TF-IDF
61.94%
51.53%
69.85%
61.42%
67.51%
BM25
67.33%
58.66%
74.37%
62.22%
82.45%
WMD
68.80%
54.35%
78.59%
77.05%
90.39%"
RESEARCH QUESTIONS AND RESULTS,0.14285714285714285,"We use the CodeBERT and the GraphCodeBERT pre-trained models for this experiments, and ﬁne-
tune these models on different pairings of the Java ↔C# dataset (Nguyen et al., 2013) created using
the different document similarity methods. We compare the performance of models trained on these
paired datasets with models trained on the ground-truth dataset, and a random baseline with random
pairings of code samples from the two programming languages. Following Ahmad et al. (2021a),
we compute the BLEU score, CodeBLEU score, and the Exact Match score."
RESEARCH QUESTIONS AND RESULTS,0.1445993031358885,"The results for this experiment are summarized in Table 3. We additionally refer the reader to Table 1
to see the corresponding match accuracy of the different document similarity methods. Interestingly,
we ﬁnd that models trained on noisy code datasets have a certain degree of resistance to noise; and
while the performance drops with increasing levels of noise, the degradation is not sudden. Even
with high levels of noise, the models perform considerably well. With a high-performing method
such as the Word Movers Distance (WMD) – with about 90% match accuracy – the degradation
in performance is roughly 1 percentage point across the three measures and for both directions of
translation. For methods with a higher level of noise – such as LDA with 47.21% match accuracy –
while the performance goes down signiﬁcantly, it is still signiﬁcantly higher than the performance of
the random baseline. We posit that although noisy datasets create pairs of code with incorrect parallel
implementations, much of the semantics is still retained by the dataset due to the formal nature of
programming languages. For example, even if code samples are incorrectly paired, the syntax for
function and variable deﬁnition, code blocks, and indentation stays the same and is preserved. This
allows the model to learn the translation task to a certain degree."
RESEARCH QUESTIONS AND RESULTS,0.14634146341463414,"Table 3: Model performance on the Java ↔C# dataset matched using various document similarity
methods. Each method introduces a different amount of noise in the resulting dataset (see Table 1)
thereby affecting the performance."
RESEARCH QUESTIONS AND RESULTS,0.1480836236933798,"Java −→C#
C# −→Java
BLEU
CodeBLEU
EM
BLEU
CodeBLEU
EM"
RESEARCH QUESTIONS AND RESULTS,0.14982578397212543,CodeBERT
RESEARCH QUESTIONS AND RESULTS,0.15156794425087108,"Random baseline
12.2
31.71
0.0%
4.4
16.56
0.0%
LDA
57.55
65.97
33.2%
43.75
55.39
23.7%
LSI
71.8
77.64
47.9%
52.44
66.26
30.9%
Okapi-BM25
79.39
83.54
59.5%
73.83
80.64
57.6%
TF-IDF
78.78
82.70
58.1%
72.52
77.34
57.1%
WMD
79.59
83.85
58.2%
75.16
80.93
59.2%
Ground-truth
80.83
84.86
60.6%
75.84
81.64
59.9%"
RESEARCH QUESTIONS AND RESULTS,0.15331010452961671,GraphCodeBERT
RESEARCH QUESTIONS AND RESULTS,0.15505226480836237,"Random baseline
6.88
20.09
0.0%
2.94
14.31
0.0%
LDA
60.34
67.91
37.3%
47.14
58.79
26.3%
LSI
73.74
79.30
49.1%
52.86
66.49
30.5%
TF-IDF
79.14
83.04
57.8%
73.14
77.55
57.9%
Okapi-BM25
79.65
83.42
59.4%
74.24
80.56
58.0%
WMD
79.47
83.72
59.0 %
75.63
81.06
60.2 %
Ground-truth
80.89
85.05
61.1 %
76.76
82.03
62.3 %"
RESEARCH QUESTIONS AND RESULTS,0.156794425087108,"While the preceding experiment allowed us to understand the performance of models trained on
noisy datasets created using different document similarity methods, we wish to understand the per-"
RESEARCH QUESTIONS AND RESULTS,0.15853658536585366,Under review as a conference paper at ICLR 2022
RESEARCH QUESTIONS AND RESULTS,0.1602787456445993,"formance characteristics of models trained with varying levels of noise on a more granular level.
Thus we create datasets for translation between Java and C# by artiﬁcially injecting noise of varying
levels. For explanation, in a dataset with x% noise level, we randomly misalign x% of code sam-
ples in that dataset, while keeping the remaining code samples correctly paired. We then train the
GraphCodeBERT model on these datasets, and compute the BLEU score, CodeBLEU score, and
the Exact Match (EM) score. Figure 1 shows the performance curve with levels of noise varying
from 0% (ground-truth dataset) to 100% (complete random pairings). We observe that the degra-
dation in the performance is gradual for initial levels of noise – compared to performance at 0%
noise, the performance at about 30% noise goes down slowly by about 20 percentage points across
all measures and both ways of translation. Post this 30% noise level, we see a sharper degradation
in performance; and post 70% noise, we observe that the performance is just slightly better than the
performance at 100% noise."
RESEARCH QUESTIONS AND RESULTS,0.16202090592334495,"0
20
40
60
80
100
Noise (%) 0 20 40 60 80"
RESEARCH QUESTIONS AND RESULTS,0.16376306620209058,Evaluation metric
RESEARCH QUESTIONS AND RESULTS,0.16550522648083624,"CodeBLEU
BLEU
Exact Match"
RESEARCH QUESTIONS AND RESULTS,0.1672473867595819,"0
20
40
60
80
100
Noise (%) 0 20 40 60 80"
RESEARCH QUESTIONS AND RESULTS,0.16898954703832753,Evaluation metric
RESEARCH QUESTIONS AND RESULTS,0.17073170731707318,"CodeBLEU
BLEU
Exact Match"
RESEARCH QUESTIONS AND RESULTS,0.17247386759581881,"Figure 1: Noise performance curve for GraphCodeBERT model ﬁne-tuned for Java →C# translation
(left) and C# →Java translation (right). While the performance decreases with increasing levels of
noise, the degradation is gradual for initial levels of noise."
RESEARCH QUESTIONS AND RESULTS,0.17421602787456447,"Through the experiments in Table 3 and in Figure 1, we get a better idea of the performance charac-
teristics of models for code under varying levels of noise. We conclude that the performance of these
models is not severely affected with moderate amounts of noise; therefore when creating datasets
from code in the wild where we expect a certain amount of noise, we can expect the models to
perform reasonably well."
RESEARCH QUESTIONS AND RESULTS,0.1759581881533101,"5.3
RQ3: TRANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES"
RESEARCH QUESTIONS AND RESULTS,0.17770034843205576,"In Section 5.1, we concluded that document similarity methods are adept at creating parallel datasets
for code with acceptable levels of noise; and in Section 5.2, we concluded that models trained on
noisy datasets of code perform reasonably well under moderate levels of noise. In this section, we
take advantage of these two ﬁndings and demonstrate the practical utility of the proposed method by
creating noisy code translation datasets for languages not explored previously in the literature; and
by training models for translating between these languages."
RESEARCH QUESTIONS AND RESULTS,0.1794425087108014,"For this experiment, we utilize the CodeNet dataset by creating noisy parallel datasets between the
following 10 languages – C, C#, C++, Go, Java, JavaScript, PHP, Python, Ruby, and Scala. We
choose these languages in order of their frequency in the CodeNet dataset, thereby maximizing the
number of data samples we can potentially create. We also sub-sample about 2500 problem sets from
the original 4000 problem sets from the CodeNet dataset for computational reasons. Thereafter, we
match the solutions in one programming language to another for each of the 2500 sub-sampled
problem sets using the WMD metric. Since the PLBART model can only translate sequences with a
maximum of 512 tokens, we only match code samples with less than 512 tokens. We additionally use
a similarity threshold of 3.0 and ﬁlter samples accordingly. In Appendix B.3, we provide statistics
of the ﬁnal dataset along with some representative code samples and their corresponding similarity
scores. To create the test set for the language pairs, we sub-sample 100 problems that are not seen in
the training and the validation set, and randomly sample 5 different implementations in the source
language from each of the 100 problem sets for a ﬁnal test set size of 500 code samples."
RESEARCH QUESTIONS AND RESULTS,0.18118466898954705,Under review as a conference paper at ICLR 2022
RESEARCH QUESTIONS AND RESULTS,0.18292682926829268,"C
C#
C++
Go
Java
PHP
Python Ruby"
RESEARCH QUESTIONS AND RESULTS,0.18466898954703834,Target language C C# C++ Go Java
RESEARCH QUESTIONS AND RESULTS,0.18641114982578397,JavaScript PHP
RESEARCH QUESTIONS AND RESULTS,0.18815331010452963,Python Ruby Scala
RESEARCH QUESTIONS AND RESULTS,0.18989547038327526,Source language
RESEARCH QUESTIONS AND RESULTS,0.1916376306620209,"14.53
44.41
8.10
29.83
17.57
31.13
19.80"
RESEARCH QUESTIONS AND RESULTS,0.19337979094076654,"21.74
26.79
32.55
22.47
22.34
24.79"
RESEARCH QUESTIONS AND RESULTS,0.1951219512195122,"22.66
9.72
5.96
18.49
9.29
26.33
13.47"
RESEARCH QUESTIONS AND RESULTS,0.19686411149825783,"20.07
16.61
30.38
37.73
17.83
22.28
22.43"
RESEARCH QUESTIONS AND RESULTS,0.1986062717770035,"33.84
19.37
41.06
20.97
18.35
24.87
30.49"
RESEARCH QUESTIONS AND RESULTS,0.20034843205574912,"25.59
19.06
28.35
14.53
26.50
16.75
13.92
24.91"
RESEARCH QUESTIONS AND RESULTS,0.20209059233449478,"25.60
23.28
31.53
17.46
28.33
30.56
32.99"
RESEARCH QUESTIONS AND RESULTS,0.2038327526132404,"20.88
10.46
30.65
14.31
16.69
9.64
30.47"
RESEARCH QUESTIONS AND RESULTS,0.20557491289198607,"11.74
11.84
24.23
15.53
16.24
28.43"
RESEARCH QUESTIONS AND RESULTS,0.2073170731707317,"19.21
13.47
29.35
17.83
21.34
20.44
27.61
28.66 8 16 24 32 40"
RESEARCH QUESTIONS AND RESULTS,0.20905923344947736,"Figure 2: CA@5 for PLBART model ﬁne-tuned on
code translation dataset created from the CodeNet
dataset using the WMD metric."
RESEARCH QUESTIONS AND RESULTS,0.21080139372822299,"C
C++
Java
PHP
Python
Ruby"
RESEARCH QUESTIONS AND RESULTS,0.21254355400696864,Target language C C# C++ Go Java
RESEARCH QUESTIONS AND RESULTS,0.21428571428571427,JavaScript PHP
RESEARCH QUESTIONS AND RESULTS,0.21602787456445993,Python Ruby Scala
RESEARCH QUESTIONS AND RESULTS,0.21777003484320556,Source language
RESEARCH QUESTIONS AND RESULTS,0.21951219512195122,"33.05
4.75
2.71
1.52
3.25"
RESEARCH QUESTIONS AND RESULTS,0.22125435540069685,"21.74
17.31
2.52
5.64
2.37
2.05"
RESEARCH QUESTIONS AND RESULTS,0.2229965156794425,"5.19
2.05
2.41
2.92
-0.52"
RESEARCH QUESTIONS AND RESULTS,0.22473867595818817,"20.07
13.49
8.29
17.66
-2.21
1.71"
RESEARCH QUESTIONS AND RESULTS,0.2264808362369338,"5.27
24.71
0.50
1.84
3.91"
RESEARCH QUESTIONS AND RESULTS,0.22822299651567945,"9.60
19.86
5.17
0.00
-7.98
-2.03"
RESEARCH QUESTIONS AND RESULTS,0.22996515679442509,"-2.06
4.18
1.37
-0.86
6.91"
RESEARCH QUESTIONS AND RESULTS,0.23170731707317074,"4.38
28.60
0.50
-1.70
7.94"
RESEARCH QUESTIONS AND RESULTS,0.23344947735191637,"-1.76
23.25
2.59
0.48
3.83"
RESEARCH QUESTIONS AND RESULTS,0.23519163763066203,"-1.52
8.85
0.00
-1.37
3.38
0.60 0 8 16 24 32"
RESEARCH QUESTIONS AND RESULTS,0.23693379790940766,"Figure 3: Difference between the CA@5 of
PLBART trained using data created using the
proposed method and the CA@5 of PLBART
trained using randomly matched dataset."
RESEARCH QUESTIONS AND RESULTS,0.23867595818815332,"We ﬁne-tune the PLBART model on the matched training data for each language pair, and evaluate
the computational accuracy @ 5 on the test set. We compare the performance of this ﬁne-tuned
model against a model ﬁne-tuned on a dataset created by randomly matching solutions from each
problem set rather than using the WMD metric. To compare these two models fairly, we keep the
dataset sizes, problem sets, and all the hyperparameters constant across the two training procedures.
The CA@5 results for the model trained on the WMD-matched dataset are shown in Figure 2; while
Figure 3 shows the difference in the performance of the model ﬁne-tuned using the WMD-matched
data and the performance of the model ﬁne-tuned using the randomly matched data. We present
some of the model generated code samples in Appendix B.4."
RESEARCH QUESTIONS AND RESULTS,0.24041811846689895,"Overall, we observe that models trained using the WMD-matched datasets achieve noteworthy per-
formance across language pairs. More importantly, when compared with models trained on ran-
domly paired data, we see substantial improvements for a majority of the language pairs. While the
common language pairs, such as C →C++, Python →C++, Ruby →C++ see the biggest improve-
ments, more obscure language pairs such as PHP →C++, PHP →Ruby, JavaScript →C++, and
Python →Ruby also demonstrate substantial improvements over their random counterparts. This
leads us to the conclusion that the proposed method is a viable way of creating high-quality datasets
for code translation, thereby alleviating the paucity of training data in the domain."
DISCUSSION & CONCLUSION,0.2421602787456446,"6
DISCUSSION & CONCLUSION"
DISCUSSION & CONCLUSION,0.24390243902439024,"Modernizing legacy applications into a new programming language is a process that requires a lot of
time, intellect, and monetary investment. Automatic code translation techniques have the potential
to speed up this process, and to reduce the human effort required by either working in tandem with
humans or automatically translating legacy code to a modern language of choice. While multiple
techniques have been proposed to improve the quality of code translation, their practical utility is
hampered due to the limited availability of parallel data required to train these models between
languages of choice. In this work, we proposed a simple technique to utilize document similarity
methods to create noisy datasets for code translation; and demonstrated that models for code have a
certain amount of tolerance for noise and perform well even under signiﬁcant amounts of noise. We
speciﬁcally demonstrated the effectiveness of the Word Movers Distance (WMD) metric in creating
parallel datasets between numerous language pairs that have not been explored in prior literature;
and showed signiﬁcantly improved model performances as compared to models trained on randomly
matched datasets. Future work will explore better metrics in terms of both match accuracy and
computational efﬁciency, thereby further reducing the noise in the dataset; and incorporating the
similarity score in the model to weight samples according to their computed similarity."
DISCUSSION & CONCLUSION,0.2456445993031359,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.24738675958188153,"7
ETHICS STATEMENT"
ETHICS STATEMENT,0.24912891986062718,"One of the major ethical points to consider when dealing with the automatic creation and translation
of source code centers around the effects on humans: both humans who create and maintain code
for a living; and humans that are affected by the decisions and outcomes produced by the execution
of such code. For the former concern, our work merely seeks to align pre-existing bits of open-
sourced code so that downstream data-hungry techniques may have more reasonable approximations
of correct and on-purpose code to learn from. Our work does not replace jobs that humans are
trained to do and more adept at; and indeed defers to and takes inspiration from prior studies (Weisz
et al., 2021) that show that human generators of code are very likely to engage in partnerships
with automatically learned models to produce or maintain code better. For the latter concern, we
acknowledge that it is possible to use the output of artefacts from our work in downstream systems
that can produce automatic code with little to no oversight. Similar to work on examining the
effects of large language models for human natural languages (Bender et al., 2021), much attention
is needed where it comes to automatic code generation and translation techniques and models. We
look forward to studying some of these issues in partnership with colleagues in the future."
REPRODUCIBILITY STATEMENT,0.2508710801393728,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.25261324041811845,"To allow for the reproducibility of experiments conducted in this work, we provide the source code
of the experiments in the supplementary material attached with the submission. The supplementary
material is grouped by the three research questions we deﬁne in our work. Each set of ﬁles within a
folder corresponding to a research question contains the source code to the respective experiments.
Another critical aspect of our work is the creation of parallel code translation datasets across many
languages from the CodeNet dataset. Along with the source code, we also provide the train, valida-
tion, and test data sets for a small subset of language pairs in the attached supplementary material.
We provide a small subset due to the limitations on the amount of data that can be provided as
supplementary material. However, we plan to release the complete noisy dataset we created for our
experiments with the ﬁnal version of the paper."
REFERENCES,0.25435540069686413,REFERENCES
REFERENCES,0.25609756097560976,"Karan Aggarwal, Mohammad Salameh, and Abram Hindle. Using machine translation for convert-
ing python 2 to python 3 code. Technical report, PeerJ PrePrints, 2015."
REFERENCES,0.2578397212543554,"Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
2655–2668, 2021a."
REFERENCES,0.259581881533101,"Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. Avatar:
A parallel corpus for java-python program translation. arXiv preprint arXiv:2108.11590, 2021b."
REFERENCES,0.2613240418118467,"Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine
learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37, 2018."
REFERENCES,0.26306620209059234,"Marc Andreessen. Why software is eating the world. Wall Street Journal, 20(2011):C2, 2011."
REFERENCES,0.26480836236933797,"Mikel Artetxe and Holger Schwenk. Margin-based parallel corpus mining with multilingual sen-
tence embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computa-
tional Linguistics, pp. 3197–3203, 2019."
REFERENCES,0.2665505226480836,"Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021."
REFERENCES,0.2682926829268293,"Marta Ban´on, Pinzhen Chen, Barry Haddow, Kenneth Heaﬁeld, Hieu Hoang, Miquel Espla-Gomis,
Mikel L Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, et al. Paracrawl: Web-scale
acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 4555–4567, 2020."
REFERENCES,0.2700348432055749,Under review as a conference paper at ICLR 2022
REFERENCES,0.27177700348432055,"Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Transparency, pp. 610–623, 2021."
REFERENCES,0.2735191637630662,"David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of
machine Learning research, 3:993–1022, 2003."
REFERENCES,0.27526132404181186,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.2770034843205575,"Robert N Charette. No one notices the creaky software systems that run the world—until they fail.
IEEE Spectrum, 57(9):24–30, 2020."
REFERENCES,0.2787456445993031,"Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Ed-
wards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021."
REFERENCES,0.2804878048780488,"Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation.
Advances in Neural Information Processing Systems, 31, 2018."
REFERENCES,0.28222996515679444,"Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training
text encoders as discriminators rather than generators. In International Conference on Learning
Representations, 2019."
REFERENCES,0.28397212543554007,"Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman.
Indexing by latent semantic analysis. Journal of the American society for information science, 41
(6):391–407, 1990."
REFERENCES,0.2857142857142857,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019."
REFERENCES,0.2874564459930314,"Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural
languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: Findings, pp. 1536–1547, 2020."
REFERENCES,0.289198606271777,"Lluis Formiga and Jos´e AR Fonollosa. Dealing with input noise in statistical machine translation.
In Proceedings of COLING 2012: Posters, pp. 319–328, 2012."
REFERENCES,0.29094076655052264,"Cyril Goutte, Marine Carpuat, and George Foster. The impact of sentence alignment errors on
phrase-based machine translation performance. In Proceedings of the 10th Conference of the
Association for Machine Translation in the Americas: Research Papers, San Diego, California,
USA, October 28-November 1 2012. Association for Machine Translation in the Americas. URL
https://aclanthology.org/2012.amta-papers.7."
REFERENCES,0.2926829268292683,"Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with
data ﬂow. In International Conference on Learning Representations, 2020."
REFERENCES,0.29442508710801396,"Viktor Hangya and Alexander Fraser. Unsupervised parallel sentence extraction with parallel seg-
ment detection helps machine translation. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pp. 1224–1234, 2019."
REFERENCES,0.2961672473867596,"Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin
Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence
with apps. arXiv preprint arXiv:2105.09938, 2021."
REFERENCES,0.2979094076655052,"Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. On the natural-
ness of software. Communications of the ACM, 59(5):122–131, 2016."
REFERENCES,0.29965156794425085,"Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
Codesearchnet challenge:
Evaluating the state of semantic code search.
arXiv preprint
arXiv:1909.09436, 2019."
REFERENCES,0.30139372822299654,Under review as a conference paper at ICLR 2022
REFERENCES,0.30313588850174217,"Anna Irrera.
Banks scramble to ﬁx old systems as it ’cowboys’ ride into sunset, Apr
2017.
URL
https://www.reuters.com/article/us-usa-banks-cobol/
banks-scramble-to-fix-old-systems-as-it-cowboys-ride-into-sunset-idUSKBN17C0D8."
REFERENCES,0.3048780487804878,"Svetoslav Karaivanov, Veselin Raychev, and Martin Vechev. Phrase-based statistical translation of
programming languages. In Proceedings of the 2014 ACM International Symposium on New
Ideas, New Paradigms, and Reﬂections on Programming & Software, pp. 173–184, 2014."
REFERENCES,0.30662020905923343,"Makena Kelly.
Unemployment checks are being held up by a coding language almost no-
body knows, Apr 2020. URL https://www.theverge.com/2020/4/14/21219561/
coronavirus-pandemic-unemployment-systems-cobol-legacy-software-infrastructure."
REFERENCES,0.3083623693379791,"Huda Khayrallah and Philipp Koehn. On the impact of various types of noise on neural machine
translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,
pp. 74–83, 2018."
REFERENCES,0.31010452961672474,"Yunsu Kim, Miguel Grac¸a, and Hermann Ney. When and why is unsupervised neural machine
translation useless? In Proceedings of the 22nd Annual Conference of the European Association
for Machine Translation, pp. 35–44, 2020."
REFERENCES,0.3118466898954704,"Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document
distances. In International conference on machine learning, pp. 957–966. PMLR, 2015."
REFERENCES,0.313588850174216,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
Bart: Denoising sequence-to-sequence pre-
training for natural language generation, translation, and comprehension. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020."
REFERENCES,0.3153310104529617,"Kelly Marchisio, Kevin Duh, and Philipp Koehn. When does unsupervised machine translation
work? In Proceedings of the Fifth Conference on Machine Translation, pp. 571–583, 2020."
REFERENCES,0.3170731707317073,"Gaspard Monge. M´emoire sur la th´eorie des d´eblais et des remblais. Histoire de l’Acad´emie Royale
des Sciences de Paris, 1781."
REFERENCES,0.31881533101045295,"Maxim Mossienko. Automated cobol to java recycling. In Seventh European Conference onSoftware
Maintenance and Reengineering, 2003. Proceedings., pp. 40–50. IEEE, 2003."
REFERENCES,0.3205574912891986,"Dragos Stefan Munteanu and Daniel Marcu. Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Linguistics, 31(4):477–504, 2005."
REFERENCES,0.32229965156794427,"Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. Lexical statistical machine translation
for language migration. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software
Engineering, pp. 651–654, 2013."
REFERENCES,0.3240418118466899,"Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. Migrating code with statistical ma-
chine translation. In Companion Proceedings of the 36th International Conference on Software
Engineering, pp. 544–547, 2014."
REFERENCES,0.32578397212543553,"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics, pp. 311–318, 2002."
REFERENCES,0.32752613240418116,"Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov,
Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, et al. Project codenet: A large-scale ai
for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655, 2021."
REFERENCES,0.32926829268292684,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.3310104529616725,"Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou,
Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis.
arXiv preprint arXiv:2009.10297, 2020."
REFERENCES,0.3327526132404181,"Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,
et al. Okapi at trec-3. Nist Special Publication Sp, 109:109, 1995."
REFERENCES,0.3344947735191638,Under review as a conference paper at ICLR 2022
REFERENCES,0.3362369337979094,"Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised
translation of programming languages. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
20601–20611. Curran Associates, Inc., 2020.
URL https://proceedings.neurips.
cc/paper/2020/file/ed23fbf18c2cd35f8c7f8de44f85c08d-Paper.pdf."
REFERENCES,0.33797909407665505,"Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas.
A metric for distributions with applica-
tions to image databases.
In Sixth International Conference on Computer Vision (IEEE Cat.
No. 98CH36271), pp. 59–66. IEEE, 1998."
REFERENCES,0.3397212543554007,"Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval.
Information processing & management, 24(5):513–523, 1988."
REFERENCES,0.34146341463414637,"Jason Smith, Chris Quirk, and Kristina Toutanova. Extracting parallel sentences from comparable
corpora using document level alignment. In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, pp.
403–411, 2010."
REFERENCES,0.343205574912892,"Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. Large scale parallel document mining
for machine translation. In Proceedings of the 23rd International Conference on Computational
Linguistics (Coling 2010), pp. 1101–1109, 2010."
REFERENCES,0.34494773519163763,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.34668989547038326,"Kevin Walsh. Agencies need to develop and implement modernization plans for critical legacy
systems. 2021. URL https://www.gao.gov/assets/gao-21-524t.pdf."
REFERENCES,0.34843205574912894,"Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Mar-
tinez, Mayank Agarwal, and Kartik Talamadupula. Perfection not required? human-ai partner-
ships in code translation. In 26th International Conference on Intelligent User Interfaces, pp.
402–412, 2021."
REFERENCES,0.3501742160278746,"Kazuki Yasumatsu and Norihisa Doi. Spice: a system for translating smalltalk programs into a c
environment. IEEE Transactions on Software Engineering, 21(11):902–912, 1995."
REFERENCES,0.3519163763066202,"Hao Zhong, Suresh Thummalapenta, Tao Xie, Lu Zhang, and Qing Wang. Mining api mapping for
language migration. In Proceedings of the 32nd ACM/IEEE International Conference on Software
Engineering-Volume 1, pp. 195–204, 2010."
REFERENCES,0.35365853658536583,Under review as a conference paper at ICLR 2022
REFERENCES,0.3554006968641115,"A
REPRESENTATIVE CODE SAMPLES FROM UTILIZED DATASETS"
REFERENCES,0.35714285714285715,"In this section, we provide representative code samples from the datasets we use in this work. List-
ings 1 and 2 are data samples from the Java ↔C# dataset. Listings 3 and 4 are data samples from
the Java ↔Python dataset. Listings 5 and 6 are data samples from the Java ↔C++ dataset. Listings
7 and 8 are data samples from the C++ ↔Python dataset. Finally, listings 9, 10, 11, 12, 13, 14, 15,
16 provide code samples from one particular problem set from the CodeNet dataset."
REFERENCES,0.3588850174216028,Listing 1: Java ↔C#: Java code sample
REFERENCES,0.3606271777003484,"public static Cell getCell(Row row, int columnIndex)
{"
REFERENCES,0.3623693379790941,"Cell cell = row.getCell(columnIndex);
if (cell == null)
{"
REFERENCES,0.3641114982578397,"cell = row.createCell(columnIndex);
}
return cell;
}"
REFERENCES,0.36585365853658536,Listing 2: Java ↔C#: C# code sample
REFERENCES,0.367595818815331,"public static ICell GetCell(IRow row, int column)
{"
REFERENCES,0.3693379790940767,"ICell cell = row.GetCell(column);
if (cell == null)
{"
REFERENCES,0.3710801393728223,"cell = row.CreateCell(column);
}
return cell;
}"
REFERENCES,0.37282229965156793,Listing 3: Java ↔Python: Java code sample
REFERENCES,0.37456445993031356,"static int binaryToDecimal ( int n )
{"
REFERENCES,0.37630662020905925,"int num = n ;
int dec_value = 0 ;
int base = 1 ;
int temp = num ;
while ( temp > 0 ) {"
REFERENCES,0.3780487804878049,"int last_digit = temp % 10 ;
temp = temp / 10 ;
dec_value += last_digit * base ;
base = base * 2 ;
}
return dec_value ;
}"
REFERENCES,0.3797909407665505,Listing 4: Java ↔Python: Python code sample
REFERENCES,0.38153310104529614,def binaryToDecimal ( n ) :
REFERENCES,0.3832752613240418,"num = n ;
dec_value = 0 ;
base = 1 ;
temp = num ;
while ( temp ) :"
REFERENCES,0.38501742160278746,"last_digit = temp % 10 ;
temp = int ( temp / 10 ) ;
dec_value += last_digit * base ;
base = base * 2 ;
return dec_value ;"
REFERENCES,0.3867595818815331,Listing 5: Java ↔C++: Java code sample
REFERENCES,0.3885017421602787,static int findS ( int s ) {
REFERENCES,0.3902439024390244,"int sum = 0 ;
for ( int n = 1 ;
sum < s ;
n ++ ) {"
REFERENCES,0.39198606271777003,"sum += n * n ;
if ( sum == s ) return n ;
}
return - 1 ;
}"
REFERENCES,0.39372822299651566,Listing 6: Java ↔C++: C++ code sample
REFERENCES,0.39547038327526135,int findS ( int s ) {
REFERENCES,0.397212543554007,"int sum = 0;
for ( int n = 1;
sum < s;
n ++ ) {"
REFERENCES,0.3989547038327526,"sum += n * n;
if ( sum == s ) return n;
}
return - 1;
}"
REFERENCES,0.40069686411149824,Listing 7: C++ ↔Python: C++ code sample
REFERENCES,0.4024390243902439,"void printDistinct ( int arr [ ], int n ) {"
REFERENCES,0.40418118466898956,"sort ( arr, arr + n );
for ( int i = 0;
i < n;
i ++ ) {"
REFERENCES,0.4059233449477352,while ( i < n - 1 && arr [ i ] == arr [ i +
REFERENCES,0.4076655052264808,",→1 ] ) i ++;
cout << arr [ i ] << "" "";
}
}"
REFERENCES,0.4094076655052265,Listing 8: C++ ↔Python: Python code sample
REFERENCES,0.41114982578397213,"def printDistinct ( arr , n ) :"
REFERENCES,0.41289198606271776,"arr.sort ( ) ;
for i in range ( n ) :"
REFERENCES,0.4146341463414634,if ( i < n - 1 and arr [ i ] == arr [ i + 1
REFERENCES,0.4163763066202091,",→] ) :
while ( i < n - 1 and ( arr [ i ] == arr"
REFERENCES,0.4181184668989547,",→
[ i + 1 ] ) ) :
i += 1 ;
else :"
REFERENCES,0.41986062717770034,"print ( arr [ i ] , end = "" "" ) ;"
REFERENCES,0.42160278745644597,Listing 9: CodeNet: C code sample
REFERENCES,0.42334494773519166,"#include<stdio.h>
int main(){"
REFERENCES,0.4250871080139373,"int i,j;
for(i=1;i<10;i++){"
REFERENCES,0.4268292682926829,for(j=1;j<10;j++){
REFERENCES,0.42857142857142855,"printf(""%dx%d=%d\n"",i,j,i*j);
}
}
return 0;
}"
REFERENCES,0.43031358885017423,Listing 10: CodeNet: C# code sample
REFERENCES,0.43205574912891986,"using System;
class test
{"
REFERENCES,0.4337979094076655,"static void Main()
{for (int i = 1; i < 10; i++) for (int j = 1; j"
REFERENCES,0.4355400696864111,",→< 10; j++) Console.WriteLine(i+""x""+j+""=""
,→+i*j);}
}"
REFERENCES,0.4372822299651568,Under review as a conference paper at ICLR 2022
REFERENCES,0.43902439024390244,Listing 11: CodeNet: Go code sample
REFERENCES,0.44076655052264807,package main
REFERENCES,0.4425087108013937,"import ""fmt"""
REFERENCES,0.4442508710801394,func main() {
REFERENCES,0.445993031358885,for i := 1; i < 10; i++ {
REFERENCES,0.44773519163763065,for j := 1; j < 10; j++ {
REFERENCES,0.44947735191637633,"fmt.Printf(""%dx%d=%d\n"", i,"
REFERENCES,0.45121951219512196,",→j, i*j)
}
}
}"
REFERENCES,0.4529616724738676,Listing 12: CodeNet: Java code sample
REFERENCES,0.4547038327526132,"public class Main
{"
REFERENCES,0.4564459930313589,"public static void main(String[] args)
{"
REFERENCES,0.45818815331010454,for(int a=1;a<=9;a++){
REFERENCES,0.45993031358885017,for(int b=1;b<=9;b++){
REFERENCES,0.4616724738675958,"System.out.println(a+""x""+b+""=""+a*b);
}
}
}
}"
REFERENCES,0.4634146341463415,Listing 13: CodeNet: JavaScript code sample
REFERENCES,0.4651567944250871,for (var i=1; i<10; i++) {
REFERENCES,0.46689895470383275,for (var j=1; j<10; j++) {
REFERENCES,0.4686411149825784,"console.log(i + ’x’ + j + ’=’ + i*j)
}
}"
REFERENCES,0.47038327526132406,Listing 14: CodeNet: PHP code sample
REFERENCES,0.4721254355400697,"<?php
for($i=1;$i<10;$i++){
for($j=1;$j<10;$j++){
echo $i.""x"".$j.""="".$i*$j.""\n"";
}
}"
REFERENCES,0.4738675958188153,Listing 15: CodeNet: Ruby code sample
REFERENCES,0.47560975609756095,# Your code here!
REFERENCES,0.47735191637630664,9.times{|i|
REFERENCES,0.47909407665505227,"i=i+1
9.times{|j|
j=j+1"
REFERENCES,0.4808362369337979,"puts i.to_s+’x’+j.to_s+’=’+(i*j).to_s
}}"
REFERENCES,0.48257839721254353,Listing 16: CodeNet: Scala code sample
REFERENCES,0.4843205574912892,object Main{
REFERENCES,0.48606271777003485,def main(args: Array[String]){
REFERENCES,0.4878048780487805,for(i <- 1 to 9){
REFERENCES,0.4895470383275261,for(j <- 1 to 9){
REFERENCES,0.4912891986062718,"println(i + ""x"" + j + ""="" + i*j)
}
}
}
}"
REFERENCES,0.4930313588850174,"B
PARALLEL DATA CREATED FROM CODENET"
REFERENCES,0.49477351916376305,"In this section we look at the various properties of the parallel dataset created from the CodeNet
dataset. In section B.1, we present most similar data samples identiﬁed by the WMD metric across
various language pairs. In section B.2, we present the histograms of similarity scores in the ﬁnal
dataset. Finally, in section B.3, we present the number of data samples in the ﬁnal dataset created
from the CodeNet dataset."
REFERENCES,0.4965156794425087,"B.1
IDENTIFIED MOST SIMILAR DATA SAMPLES ACROSS LANGUAGE PAIRS"
REFERENCES,0.49825783972125437,"In table 4, we provide the most-similar code samples identiﬁed by the WMD metric across various
language pairs along with the computed similarity between the two samples."
REFERENCES,0.5,Under review as a conference paper at ICLR 2022
REFERENCES,0.5017421602787456,"Table 4: Most similar code samples across various language pairs as
identiﬁed by the WMD metric"
REFERENCES,0.5034843205574913,C →Python dataset: Similarity: 0.75
REFERENCES,0.5052264808362369,"#include <stdio.h>
#include <math.h>"
REFERENCES,0.5069686411149826,"int main()
{"
REFERENCES,0.5087108013937283,"int n;
int i;
double x1, x2, x3, y1, y2, y3, px, py, r;"
REFERENCES,0.5104529616724739,"scanf(""%d"", &n);"
REFERENCES,0.5121951219512195,for(i = 0; i < n; i++){
REFERENCES,0.5139372822299652,"scanf(""%lf %lf %lf %lf %lf %lf"", &x1, &y1, &x2"
REFERENCES,0.5156794425087108,",→, &y2, &x3, &y3);"
REFERENCES,0.5174216027874564,px = ((y2 - y3)*(x1*x1 + y1*y1) + (y3 - y1)*(
REFERENCES,0.519163763066202,",→x2*x2 + y2*y2) + (y1 - y2)*(x3*x3 +
,→y3*y3))/(2*(x1*(y2 - y3) + x2*(y3 -
,→y1) + x3*(y1 - y2)));
py = ((x2 - x3)*(x1*x1 + y1*y1) + (x3 - x1)*("
REFERENCES,0.5209059233449478,",→x2*x2 + y2*y2) + (x1 - x2)*(x3*x3 +
,→y3*y3))/(2*(y1*(x2 - x3) + y2*(x3 -
,→x1) + y3*(x1 - x2)));"
REFERENCES,0.5226480836236934,"r = sqrt(pow((x1 - px), 2) + pow((y1 - py), 2) ,→);"
REFERENCES,0.524390243902439,"printf(""%.3f %.3f %.3f\n"", px, py, r);
}"
REFERENCES,0.5261324041811847,"return 0;
}"
REFERENCES,0.5278745644599303,import math
REFERENCES,0.5296167247386759,n = int(raw_input())
REFERENCES,0.5313588850174216,for i in range(n):
REFERENCES,0.5331010452961672,"x1, y1, x2, y2, x3, y3 = map(float, raw_input()."
REFERENCES,0.5348432055749129,",→split())"
REFERENCES,0.5365853658536586,px = ((y2 - y3)*(x1*x1 + y1*y1) + (y3 - y1)*(x2*
REFERENCES,0.5383275261324042,",→x2 + y2*y2) + (y1 - y2)*(x3*x3 + y3*y3)
,→)/(2*(x1*(y2 - y3) + x2*(y3 - y1) + x3
,→*(y1 - y2)))
py = ((x2 - x3)*(x1*x1 + y1*y1) + (x3 - x1)*(x2*"
REFERENCES,0.5400696864111498,",→x2 + y2*y2) + (x1 - x2)*(x3*x3 + y3*y3)
,→)/(2*(y1*(x2 - x3) + y2*(x3 - x1) + y3
,→*(x1 - x2)))"
REFERENCES,0.5418118466898955,"r = math.sqrt(pow((x1 - px), 2) + pow((y1 - py),"
REFERENCES,0.5435540069686411,",→
2))"
REFERENCES,0.5452961672473867,"print ""%.3f %.3f %.3f"" % (px, py, r)"
REFERENCES,0.5470383275261324,C# →Java dataset: Similarity: 0.62
REFERENCES,0.5487804878048781,using System;
REFERENCES,0.5505226480836237,"class Program
{"
REFERENCES,0.5522648083623694,"static void Main(string[] args)
{"
REFERENCES,0.554006968641115,"for (int i = 0; i < 1000; i++)
{"
REFERENCES,0.5557491289198606,"Console.WriteLine(""Hello World"");
}
}
}"
REFERENCES,0.5574912891986062,"class Main
{"
REFERENCES,0.5592334494773519,"public static void main(String[] args)
{"
REFERENCES,0.5609756097560976,"for (int i = 0; i < 1000; i++)
{"
REFERENCES,0.5627177700348432,"System.out.println(""Hello World"");
}
}
}"
REFERENCES,0.5644599303135889,Scala →Ruby dataset: Similarity: 0.91
REFERENCES,0.5662020905923345,object Main extends App {
REFERENCES,0.5679442508710801,"val a = Array(1, 1, 1, 2, 1, 2, 1, 5, 2, 2, 1,"
REFERENCES,0.5696864111498258,",→
5, 1, 2, 1, 14, 1, 5, 1, 5, 2, 2, 1,
,→
15, 2, 2, 5, 4, 1, 4, 1, 51)
val k = scala.io.StdIn.readInt - 1
println(a(k))
}"
REFERENCES,0.5714285714285714,"k = gets.to_i
a = ""1, 1, 1, 2, 1, 2, 1, 5, 2, 2, 1, 5, 1, 2, 1,"
REFERENCES,0.573170731707317,",→14, 1, 5, 1, 5, 2, 2, 1, 15, 2, 2, 5, 4,
,→1, 4, 1, 51"".split("","").map(&:to_i)
p a[k-1]"
REFERENCES,0.5749128919860628,PHP →Python dataset: Similarity: 1.34
REFERENCES,0.5766550522648084,Under review as a conference paper at ICLR 2022 <?php
REFERENCES,0.578397212543554,while ($line = trim(fgets(STDIN))) {
REFERENCES,0.5801393728222997,"$num = explode("" "",$line);
if ($num[0] == 0 && $num[1] == 0) {"
REFERENCES,0.5818815331010453,"exit();
}
sort($num);
$line = implode("" "",$num);
echo $line . PHP_EOL;
} ?>"
REFERENCES,0.5836236933797909,while True:
REFERENCES,0.5853658536585366,num = [int(x) for x in input().rstrip().split
REFERENCES,0.5871080139372822,",→()]
if num[0] ==0 and num[1] == 0:"
REFERENCES,0.5888501742160279,"break
num.sort()
ans = "" "".join(str(x) for x in num)
print(ans)"
REFERENCES,0.5905923344947736,# <?php
REFERENCES,0.5923344947735192,"# while ($line = trim(fgets(STDIN))) {
#
$num = explode("" "",$line);
#
if ($num[0] == 0 && $num[1] == 0) {
#
exit();
#
}
#
sort($num);
#
$line = implode("" "",$num);
#
echo $line . PHP_EOL;
# } # ?>"
REFERENCES,0.5940766550522648,"B.2
SIMILARITY HISTOGRAMS IN THE CREATED DATASET"
REFERENCES,0.5958188153310104,"We show the histograms of the similarity scores in the datasets matched through the WMD metric
on the CodeNet dataset in ﬁgure 4."
REFERENCES,0.5975609756097561,"B.3
DATA STATISTICS"
REFERENCES,0.5993031358885017,"In table 5, we provide the number of data samples in the ﬁnal dataset created from the CodeNet
dataset."
REFERENCES,0.6010452961672473,"Table 5: Number of data samples for each code translation dataset. Each row represents the source
language, while each column represents the target language. Due to the skewed number of submis-
sions in different languages in the CodeNet dataset, we see the same in the created parallel dataset."
REFERENCES,0.6027874564459931,"C
C#
C++
Go
Java
JS
PHP
Python
Ruby
Scala"
REFERENCES,0.6045296167247387,"C
×
2,295
15,534
709
8,900
1,354
837
10,499
5,564
1,247"
REFERENCES,0.6062717770034843,"C#
2,504
×
3,145
595
2,629
846
635
2,639
2,007
821"
REFERENCES,0.60801393728223,"C++
30,977
3,388
×
1,358
17,809
1,793
1,550
44,707
14,375
2,183"
REFERENCES,0.6097560975609756,"Go
1,732
974
2,496
×
2,077
642
669
1,755
1,315
729"
REFERENCES,0.6114982578397212,"Java
15,909
5,016
23,821
2,858
×
2,262
2,202
12,846
7,746
2,034"
REFERENCES,0.6132404181184669,"JS
2,876
1,907
2,954
998
2,653
×
1,238
2,893
2,307
1,349"
REFERENCES,0.6149825783972126,"PHP
2,802
1,741
2,898
1,180
2,454
1,389
×
2,996
2,310
1,133"
REFERENCES,0.6167247386759582,"Python
27,947
7,778
76,825
7,235
29,007
6,666
7,641
×
48,301
7,126"
REFERENCES,0.6184668989547039,"Ruby
28,423
12,595
48,985
8,838
23,515
8,322
9,252
62,298
×
6,457"
REFERENCES,0.6202090592334495,"Scala
5,506
4,720
6,660
3,343
5,953
3,536
3,249
6,937
5,586
×"
REFERENCES,0.6219512195121951,Under review as a conference paper at ICLR 2022 0 10 20 30 40 50 60
REFERENCES,0.6236933797909407,"C# 
 C dataset 0 10 20 30 40 50 60 70 80"
REFERENCES,0.6254355400696864,"C# 
 C++ dataset 0 2 4 6 8 10 12 14 16"
REFERENCES,0.627177700348432,"C# 
 Go dataset 0 10 20 30 40 50 60 70 80"
REFERENCES,0.6289198606271778,"C# 
 Java dataset 0 5 10 15 20 25"
REFERENCES,0.6306620209059234,"C# 
 JavaScript dataset 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
REFERENCES,0.632404181184669,"C# 
 PHP dataset 0 10 20 30 40 50 60 70"
REFERENCES,0.6341463414634146,"C# 
 Python dataset 0 10 20 30 40 50 60"
REFERENCES,0.6358885017421603,"C# 
 Ruby dataset 0 5 10 15 20 25 30"
REFERENCES,0.6376306620209059,"C# 
 Scala dataset 0 100 200 300 400 500 600 700 800"
REFERENCES,0.6393728222996515,"C++ 
 C dataset 0 20 40 60 80"
REFERENCES,0.6411149825783972,"C++ 
 C# dataset 0 10 20 30 40"
REFERENCES,0.6428571428571429,"C++ 
 Go dataset 0 100 200 300 400 500"
REFERENCES,0.6445993031358885,"C++ 
 Java dataset 0 10 20 30 40 50 60"
REFERENCES,0.6463414634146342,"C++ 
 JavaScript dataset 0 10 20 30 40 50"
REFERENCES,0.6480836236933798,"C++ 
 PHP dataset 0 200 400 600 800 1000 1200 1400"
REFERENCES,0.6498257839721254,"C++ 
 Python dataset 0 100 200 300 400 500"
REFERENCES,0.6515679442508711,"C++ 
 Ruby dataset 0 10 20 30 40 50 60 70"
REFERENCES,0.6533101045296167,"C++ 
 Scala dataset 0 10 20 30 40 50"
REFERENCES,0.6550522648083623,"C 
 C# dataset 0 100 200 300 400"
REFERENCES,0.6567944250871081,"C 
 C++ dataset 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
REFERENCES,0.6585365853658537,"C 
 Go dataset 0 50 100 150 200"
REFERENCES,0.6602787456445993,"C 
 Java dataset 0 10 20 30 40"
REFERENCES,0.662020905923345,"C 
 JavaScript dataset 0 5 10 15 20 25"
"C 
 PHP DATASET",0.6637630662020906,"30
C 
 PHP dataset 0 50 100 150 200 250 300"
"C 
 PYTHON DATASET",0.6655052264808362,"350
C 
 Python dataset 0 20 40 60 80 100 120 140 160"
"C 
 PYTHON DATASET",0.6672473867595818,"C 
 Ruby dataset 0 5 10 15 20 25 30 35"
"C 
 PYTHON DATASET",0.6689895470383276,"C 
 Scala dataset 0 10 20 30 40 50"
"C 
 PYTHON DATASET",0.6707317073170732,"Go 
 C dataset 0 5 10 15 20 25 30"
"C 
 PYTHON DATASET",0.6724738675958188,"Go 
 C# dataset 0 10 20 30 40 50 60 70 80"
"C 
 PYTHON DATASET",0.6742160278745645,"Go 
 C++ dataset 0 10 20 30 40 50 60 70 80"
"C 
 PYTHON DATASET",0.6759581881533101,"Go 
 Java dataset 0 5 10 15 20"
"GO 
 JAVASCRIPT DATASET",0.6777003484320557,"25
Go 
 JavaScript dataset 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
"GO 
 JAVASCRIPT DATASET",0.6794425087108014,"Go 
 PHP dataset 0 10 20 30 40 50 60"
"GO 
 JAVASCRIPT DATASET",0.681184668989547,"Go 
 Python dataset 0 5 10 15 20 25 30 35"
"GO 
 JAVASCRIPT DATASET",0.6829268292682927,"Go 
 Ruby dataset 0 5 10 15 20"
"GO 
 JAVASCRIPT DATASET",0.6846689895470384,"Go 
 Scala dataset 0 100 200 300 400"
"GO 
 JAVASCRIPT DATASET",0.686411149825784,"Java 
 C dataset 0 20 40 60 80 100 120 140"
JAVA,0.6881533101045296,"160
Java 
 C# dataset 0 100 200 300 400 500 600"
JAVA,0.6898954703832753,"700
Java 
 C++ dataset 0 20 40 60 80"
"JAVA 
 GO DATASET",0.6916376306620209,"100
Java 
 Go dataset 0 10 20 30 40 50 60 70"
"JAVA 
 GO DATASET",0.6933797909407665,"Java 
 JavaScript dataset 0 10 20 30 40 50 60 70 80"
"JAVA 
 GO DATASET",0.6951219512195121,"Java 
 PHP dataset 0 100 200 300 400 500"
"JAVA 
 GO DATASET",0.6968641114982579,"Java 
 Python dataset 0 50 100 150 200"
"JAVA 
 RUBY DATASET",0.6986062717770035,"250
Java 
 Ruby dataset 0 10 20 30 40 50"
"JAVA 
 RUBY DATASET",0.7003484320557491,"Java 
 Scala dataset 0 20 40 60 80"
"JAVA 
 RUBY DATASET",0.7020905923344948,"JavaScript 
 C dataset 0 10 20 30 40 50 60"
"JAVA 
 RUBY DATASET",0.7038327526132404,"JavaScript 
 C# dataset 0 20 40 60 80 100"
"JAVA 
 RUBY DATASET",0.705574912891986,"JavaScript 
 C++ dataset 0 5 10 15 20 25 30"
"JAVA 
 RUBY DATASET",0.7073170731707317,"JavaScript 
 Go dataset 0 10 20 30 40 50 60 70 80"
"JAVA 
 RUBY DATASET",0.7090592334494773,"JavaScript 
 Java dataset 0 10 20 30 40"
"JAVA 
 RUBY DATASET",0.710801393728223,"JavaScript 
 PHP dataset 0 20 40 60 80"
"JAVA 
 RUBY DATASET",0.7125435540069687,"JavaScript 
 Python dataset 0 10 20 30 40 50 60 70 80"
"JAVA 
 RUBY DATASET",0.7142857142857143,"JavaScript 
 Ruby dataset 0 10 20 30 40"
"JAVA 
 RUBY DATASET",0.7160278745644599,"JavaScript 
 Scala dataset 0 20 40 60 80"
"JAVA 
 RUBY DATASET",0.7177700348432056,"PHP 
 C dataset 0 10 20 30 40 50"
"JAVA 
 RUBY DATASET",0.7195121951219512,"PHP 
 C# dataset 0 20 40 60 80 100"
"JAVA 
 RUBY DATASET",0.7212543554006968,"PHP 
 C++ dataset 0 5 10 15 20 25 30 35"
"JAVA 
 RUBY DATASET",0.7229965156794426,"PHP 
 Go dataset 0 10 20 30 40 50 60 70 80"
"JAVA 
 RUBY DATASET",0.7247386759581882,"PHP 
 Java dataset 0 10 20 30 40"
"JAVA 
 RUBY DATASET",0.7264808362369338,"PHP 
 JavaScript dataset 0 20 40 60 80 100"
"JAVA 
 RUBY DATASET",0.7282229965156795,"PHP 
 Python dataset 0 20 40 60 80 100 120"
"JAVA 
 RUBY DATASET",0.7299651567944251,"PHP 
 Ruby dataset 0 5 10 15 20 25 30 35 40"
"JAVA 
 RUBY DATASET",0.7317073170731707,"PHP 
 Scala dataset 0 200 400 600 800"
"JAVA 
 RUBY DATASET",0.7334494773519163,"Python 
 C dataset 0 25 50 75 100 125 150 175 200"
"JAVA 
 RUBY DATASET",0.735191637630662,"Python 
 C# dataset 0 500 1000 1500 2000 2500 3000"
"JAVA 
 RUBY DATASET",0.7369337979094077,"Python 
 C++ dataset 0 25 50 75 100 125 150 175 200"
"JAVA 
 RUBY DATASET",0.7386759581881533,"Python 
 Go dataset 0 200 400 600 800 1000"
"PYTHON 
 JAVA DATASET",0.740418118466899,"1200
Python 
 Java dataset 0 50 100 150 200 250"
"PYTHON 
 JAVA DATASET",0.7421602787456446,"Python 
 JavaScript dataset 0 50 100 150 200 250 300 350"
"PYTHON 
 JAVA DATASET",0.7439024390243902,"Python 
 PHP dataset 0 200 400 600 800 1000 1200 1400 1600"
"PYTHON 
 JAVA DATASET",0.7456445993031359,"Python 
 Ruby dataset 0 50 100 150 200 250"
"PYTHON 
 JAVA DATASET",0.7473867595818815,"Python 
 Scala dataset 0 200 400 600 800 1000 1200"
"PYTHON 
 JAVA DATASET",0.7491289198606271,"Ruby 
 C dataset 0 50 100 150 200 250 300 350 400"
"PYTHON 
 JAVA DATASET",0.7508710801393729,"Ruby 
 C# dataset 0 250 500 750 1000 1250 1500 1750"
RUBY,0.7526132404181185,"2000
Ruby 
 C++ dataset 0 50 100 150 200 250"
RUBY,0.7543554006968641,"Ruby 
 Go dataset 0 100 200 300 400 500 600 700 800"
RUBY,0.7560975609756098,"Ruby 
 Java dataset 0 50 100 150 200 250"
"RUBY 
 JAVASCRIPT DATASET",0.7578397212543554,"300 Ruby 
 JavaScript dataset 0 100 200 300 400"
"RUBY 
 JAVASCRIPT DATASET",0.759581881533101,"Ruby 
 PHP dataset 0 500 1000 1500 2000"
"RUBY 
 JAVASCRIPT DATASET",0.7613240418118467,"Ruby 
 Python dataset 0 50 100 150 200"
"RUBY 
 JAVASCRIPT DATASET",0.7630662020905923,"Ruby 
 Scala dataset"
"RUBY 
 JAVASCRIPT DATASET",0.764808362369338,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 25 50 75 100 125 150 175"
"RUBY 
 JAVASCRIPT DATASET",0.7665505226480837,"Scala 
 C dataset"
"RUBY 
 JAVASCRIPT DATASET",0.7682926829268293,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 25 50 75 100 125 150 175"
"RUBY 
 JAVASCRIPT DATASET",0.7700348432055749,"Scala 
 C# dataset"
"RUBY 
 JAVASCRIPT DATASET",0.7717770034843205,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 50 100 150 200"
"RUBY 
 JAVASCRIPT DATASET",0.7735191637630662,"Scala 
 C++ dataset"
"RUBY 
 JAVASCRIPT DATASET",0.7752613240418118,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 20 40 60 80 100 120"
"RUBY 
 JAVASCRIPT DATASET",0.7770034843205574,"Scala 
 Go dataset"
"RUBY 
 JAVASCRIPT DATASET",0.7787456445993032,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 25 50 75 100 125 150 175"
"RUBY 
 JAVASCRIPT DATASET",0.7804878048780488,"Scala 
 Java dataset"
"RUBY 
 JAVASCRIPT DATASET",0.7822299651567944,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 20 40 60 80 100 120 140"
"RUBY 
 JAVASCRIPT DATASET",0.7839721254355401,"Scala 
 JavaScript dataset"
"RUBY 
 JAVASCRIPT DATASET",0.7857142857142857,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 20 40 60 80 100 120 140"
"SCALA 
 PHP DATASET",0.7874564459930313,"160
Scala 
 PHP dataset"
"SCALA 
 PHP DATASET",0.789198606271777,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 50 100 150 200 250"
"SCALA 
 PYTHON DATASET",0.7909407665505227,"300
Scala 
 Python dataset"
"SCALA 
 PYTHON DATASET",0.7926829268292683,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0 25 50 75 100 125 150 175 200"
"SCALA 
 PYTHON DATASET",0.794425087108014,"Scala 
 Ruby dataset"
"SCALA 
 PYTHON DATASET",0.7961672473867596,Figure 4: Histogram of similarity scores for various language pairs matched by WMD
"SCALA 
 PYTHON DATASET",0.7979094076655052,"B.4
CODE GENERATED FROM FINE-TUNED PLBART MODEL"
"SCALA 
 PYTHON DATASET",0.7996515679442509,"In table 6, we show examples of code generated from a PLBART model ﬁne-tuned on a dataset
created using the WMD metric. We show examples from C →Python, C# →Java, C++ →Ruby,
Java →Ruby, JavaScript →C, PHP →Python, Python →C, Ruby →C, and Scala →Python
language pairs. For a given input source code, we show an example of the correctly generated and
incorrectly generated code."
"SCALA 
 PYTHON DATASET",0.8013937282229965,Under review as a conference paper at ICLR 2022
"SCALA 
 PYTHON DATASET",0.8031358885017421,"Table 6: Examples of correct and incorrect translations given the source
code to a PLBART model ﬁne-tuned on a dataset matched using the
WMD metric"
"SCALA 
 PYTHON DATASET",0.8048780487804879,"C →Python translation
Source C code
Correct Python translation
Incorrect Python translation"
"SCALA 
 PYTHON DATASET",0.8066202090592335,#include <stdio.h>
"SCALA 
 PYTHON DATASET",0.8083623693379791,"int main(void) {
long n;
scanf(""%ld"", &n);
long a[n];
for (long i = 0; i < n; i++) {
scanf(""%ld"", &a[i]);
}
for (long i = 0; i < n; i++) {
if (a[i]%2 != 0) {
printf(""first\n"");
return 0;
}
}
printf(""second\n"");
return 0;
}"
"SCALA 
 PYTHON DATASET",0.8101045296167247,"n = int(input())
a = list(map(int, input().split()"
"SCALA 
 PYTHON DATASET",0.8118466898954704,",→))
for i in range(n):"
"SCALA 
 PYTHON DATASET",0.813588850174216,if a[i]%2 != 0:
"SCALA 
 PYTHON DATASET",0.8153310104529616,"print(""first"")
exit()
print(""second"")"
"SCALA 
 PYTHON DATASET",0.8170731707317073,"n = int(input())
a = list(map(int, input().split()"
"SCALA 
 PYTHON DATASET",0.818815331010453,",→))
for i in range(n):"
"SCALA 
 PYTHON DATASET",0.8205574912891986,if a[i]%2 == 0:
"SCALA 
 PYTHON DATASET",0.8222996515679443,"print(""first"")
exit()
print(""second"")"
"SCALA 
 PYTHON DATASET",0.8240418118466899,"C# →Java translation
Source C# code
Correct Java translation
Incorrect Java translation"
"SCALA 
 PYTHON DATASET",0.8257839721254355,using System;
"SCALA 
 PYTHON DATASET",0.8275261324041812,"namespace Tenka1A
{
class Program
{
static void Main(string[] args)
{
string[] input = Console."
"SCALA 
 PYTHON DATASET",0.8292682926829268,",→ReadLine().Split(’ ’);
int A = int.Parse(input[0]);
int B = int.Parse(input[1]);
int C = int.Parse(input[2]);
bool flg = false;
if (A > B)
{
if (C > B&&A>C)
{
flg = true;
}
}else if (B>A) {
if (B>C&&C>A)
{
flg=true;
}
}
if (flg)
{
Console.WriteLine(""Yes"");
}
else {
Console.WriteLine(""No"");
}
}
}
}"
"SCALA 
 PYTHON DATASET",0.8310104529616724,"import java.util.Scanner;
public class Main {
public static void main(String[]"
"SCALA 
 PYTHON DATASET",0.8327526132404182,",→args) {
Scanner sc = new Scanner(System."
"SCALA 
 PYTHON DATASET",0.8344947735191638,",→in);
int A = sc.nextInt();
int B = sc.nextInt();
int C = sc.nextInt();
boolean flg = false;
if (A > B) {
if (C > B && A>C) {
flg = true;
}
} else if (A < B) {
if (B>C && C>A) {
flg = true;
}
}
if (flg) {
System.out.println(""Yes"");
} else {
System.out.println(""No"");
}
}
}"
"SCALA 
 PYTHON DATASET",0.8362369337979094,"import java.util.Scanner;
public class Main {
public static void main(String[]"
"SCALA 
 PYTHON DATASET",0.837979094076655,",→args) {
Scanner sc = new Scanner(System."
"SCALA 
 PYTHON DATASET",0.8397212543554007,",→in);
int A = sc.nextInt();
int B = sc.nextInt();
int C = sc.nextInt();
boolean flg = false;
if (A > B) {
if (C > B && A>C) {
flg = true;
}
} else if (A < B) {
if (B>C && A>C) {
flg = true;
}
}
if (flg) {
System.out.println(""Yes"");
} else {
System.out.println(""No"");
}
sc.close();
}
}"
"SCALA 
 PYTHON DATASET",0.8414634146341463,"C++ →Ruby translation
Source C++ code
Correct Ruby translation
Incorrect Ruby translation"
"SCALA 
 PYTHON DATASET",0.8432055749128919,Under review as a conference paper at ICLR 2022
"SCALA 
 PYTHON DATASET",0.8449477351916377,#include <cstdio>
"SCALA 
 PYTHON DATASET",0.8466898954703833,"#include <iostream>
#include <algorithm>
#include <cstring>
#include <cmath>
using namespace std;
typedef long long LL;
int n,k,ans = 0,flg = 0;
int a[500005];
int mx[500005] = {0};
int main(){
ios::sync_with_stdio(false);
cin >> n;
memset(mx,-0x3f,sizeof(mx));
mx[0] = 0;
for(int i = 1;i <= n;i ++) cin"
"SCALA 
 PYTHON DATASET",0.8484320557491289,",→>> a[i];
for(int i = 1;i <= n;i ++){
if(a[i] == 1) flg = 1;
mx[a[i]] = max(mx[a[i]],mx[a[i]"
"SCALA 
 PYTHON DATASET",0.8501742160278746,",→- 1] + 1);
ans = max(ans,mx[a[i]]);
}
if(!flg) cout << -1 << endl;
else cout << n - ans << endl;
return 0;
}"
"SCALA 
 PYTHON DATASET",0.8519163763066202,"N = gets.to_i
as = gets.split.map(&:to_i)
mx = Array.new(N + 1, -Float::"
"SCALA 
 PYTHON DATASET",0.8536585365853658,",→INFINITY)
mx[0] = 0
as.each do |a|
if a == 1
mx[a] = 1
elsif mx[a] < mx[a - 1]
mx[a] = mx[a - 1] + 1
end
end
puts N - mx.max"
"SCALA 
 PYTHON DATASET",0.8554006968641115,"N = gets.to_i
as = gets.split.map(&:to_i)
mx = Array.new(N + 1, -Float::"
"SCALA 
 PYTHON DATASET",0.8571428571428571,",→INFINITY)
mx[0] = 0
as.each do |a|
if a == 1
mx[a] = 1
elsif mx[a] < mx[a - 1]
mx[a] = mx[a - 1] + 1
end
end
ans = N - (mx.max + 1)
puts ans"
"SCALA 
 PYTHON DATASET",0.8588850174216028,"Java →Ruby translation
Source Java code
Correct Ruby translation
Incorrect Ruby translation"
"SCALA 
 PYTHON DATASET",0.8606271777003485,import java.util.HashMap;
"SCALA 
 PYTHON DATASET",0.8623693379790941,"import java.util.Map;
import java.util.Scanner;
public class Main {
public static void main(String[]"
"SCALA 
 PYTHON DATASET",0.8641114982578397,",→
args) throws Exception
,→
{
// File file = new File(""test."
"SCALA 
 PYTHON DATASET",0.8658536585365854,",→txt"");
// Scanner sc = new Scanner(file"
"SCALA 
 PYTHON DATASET",0.867595818815331,",→);
Scanner sc = new Scanner(System."
"SCALA 
 PYTHON DATASET",0.8693379790940766,",→in);
int n = sc.nextInt();
Map<Integer, Boolean> map = new"
"SCALA 
 PYTHON DATASET",0.8710801393728222,",→HashMap<>();
for(int i = 0; i < n; i++){
int a = sc.nextInt();
if(null != map.get(a)){
System.out.println(""NO"");
sc.close();
return;
}else{
map.put(a, true);;
}
}
sc.close();
System.out.println(""YES"");
}
}"
"SCALA 
 PYTHON DATASET",0.872822299651568,"n = gets.to_i
a = gets.split.map(&:to_i)
h = Hash.new(0)
a.each do |i|
h[i] += 1
end
puts h.values.all? { |v| v == 1 }"
"SCALA 
 PYTHON DATASET",0.8745644599303136,",→
? ""YES"" : ""NO"""
"SCALA 
 PYTHON DATASET",0.8763066202090593,"n = gets.to_i
a = gets.split.map(&:to_i)
h = Hash.new(0)
a.each do |i|
h[i] += 1
end
puts h.values.all? { |v| v == 0 }"
"SCALA 
 PYTHON DATASET",0.8780487804878049,",→
? ""YES"" : ""NO"""
"SCALA 
 PYTHON DATASET",0.8797909407665505,"Javascript →C translation
Source JavaScript code
Correct C translation
Incorrect C translation"
"SCALA 
 PYTHON DATASET",0.8815331010452961,Under review as a conference paper at ICLR 2022
"SCALA 
 PYTHON DATASET",0.8832752613240418,’use strict’
"SCALA 
 PYTHON DATASET",0.8850174216027874,"const main = input => {
const args = input.split(’\n’)."
"SCALA 
 PYTHON DATASET",0.8867595818815331,",→map(arg => arg.split(’
,→’))
const A = parseInt(args[0][0],"
"SCALA 
 PYTHON DATASET",0.8885017421602788,",→10)
const B = parseInt(args[0][1],"
"SCALA 
 PYTHON DATASET",0.8902439024390244,",→10)
if (A % 3 === 0 || B % 3 === 0"
"SCALA 
 PYTHON DATASET",0.89198606271777,",→|| (A + B) % 3 === 0)
,→console.log(’Possible’)
else console.log(’Impossible’)
}
main(require(’fs’).readFileSync"
"SCALA 
 PYTHON DATASET",0.8937282229965157,",→(’/dev/stdin’, ’utf8’))"
"SCALA 
 PYTHON DATASET",0.8954703832752613,"#include <stdio.h>
int main(void)
{
int a, b;
scanf(""%d %d"", &a, &b);
if (a % 3 == 0 || b % 3 == 0 || ("
"SCALA 
 PYTHON DATASET",0.8972125435540069,",→a + b) % 3 == 0) printf(
,→""Possible\n"");
else printf(""Impossible\n"");
return 0;
}"
"SCALA 
 PYTHON DATASET",0.8989547038327527,"#include <stdio.h>
int main(void)
{
int a, b;
scanf(""%d %d"", &a, &b);
if (a % 3 == 0 || b % 3 == 0)
printf(""Possible\n"");
else
printf(""Impossible\n"");
return 0;
}"
"SCALA 
 PYTHON DATASET",0.9006968641114983,"PHP →Python translation
Source PHP code
Correct Python translation
Incorrect Python translation <?php"
"SCALA 
 PYTHON DATASET",0.9024390243902439,"fscanf(STDIN,""%s"",$s);$flg ="
"SCALA 
 PYTHON DATASET",0.9041811846689896,",→false;
if(strpos($s, ""A"") === false){
}else{
if(strpos($s,""B"") === false){
}else{
$flg = true;
}
}"
"SCALA 
 PYTHON DATASET",0.9059233449477352,"if($flg){
echo ""Yes"";
}else{
echo ""No"";
}"
"SCALA 
 PYTHON DATASET",0.9076655052264808,"s = input()
flg = False
if ""A"" in s or ""B"" in s:"
"SCALA 
 PYTHON DATASET",0.9094076655052264,"flg = True
else:"
"SCALA 
 PYTHON DATASET",0.9111498257839721,"if ""A"" in s:"
"SCALA 
 PYTHON DATASET",0.9128919860627178,"flg = True
print(""Yes"" if flg else ""No"")"
"SCALA 
 PYTHON DATASET",0.9146341463414634,"s = input()
a = 0
b = 0
for i in range(len(s)):"
"SCALA 
 PYTHON DATASET",0.9163763066202091,"if ""A"" in s[i]:"
"SCALA 
 PYTHON DATASET",0.9181184668989547,"if ""B"" in s[i]:"
"SCALA 
 PYTHON DATASET",0.9198606271777003,"a += 1
else:"
"SCALA 
 PYTHON DATASET",0.921602787456446,"b += 1
if a == b:"
"SCALA 
 PYTHON DATASET",0.9233449477351916,"print(""Yes"")
else:"
"SCALA 
 PYTHON DATASET",0.9250871080139372,"print(""No"")"
"SCALA 
 PYTHON DATASET",0.926829268292683,"Python →C translation
Source Python code
Correct C translation
Incorrect C translation"
"SCALA 
 PYTHON DATASET",0.9285714285714286,import sys
"SCALA 
 PYTHON DATASET",0.9303135888501742,"N = int(input())
a = list(map(int, input().split"
"SCALA 
 PYTHON DATASET",0.9320557491289199,",→()))
ans = 0
count = 1
if 1 not in a :"
"SCALA 
 PYTHON DATASET",0.9337979094076655,"print(-1)
sys.exit()
for i in range(N) :"
"SCALA 
 PYTHON DATASET",0.9355400696864111,if a[i] == count :
"SCALA 
 PYTHON DATASET",0.9372822299651568,"count += 1
else :"
"SCALA 
 PYTHON DATASET",0.9390243902439024,"ans += 1
print(ans)"
"SCALA 
 PYTHON DATASET",0.9407665505226481,"#include <stdio.h>
int main()
{
int n;
scanf(""%d"", &n);
int i;
int a[100005];
for (i = 0; i < n; i++)
scanf(""%d"", &a[i]);
int ans = 0;
int count = 1;
for (i = 0; i < n; i++)
{
if (a[i] == count)
count++;
else
ans++;
}
printf(""%d\n"", ans);
return 0;
}"
"SCALA 
 PYTHON DATASET",0.9425087108013938,"#include <stdio.h>
int main(void){
int N;
scanf(""%d"", &N);
int a[N];
for (int i = 0; i < N; i++) {
scanf(""%d"", &a[i]);
}
int ans = 0;
int count = 1;
for (int i = 0; i < N; i++) {
if (a[i] == count) {
ans++;
count++;
}
}
printf(""%d\n"", ans);
return 0;
}"
"SCALA 
 PYTHON DATASET",0.9442508710801394,"Ruby →C translation
Source Ruby code
Correct C translation
Incorrect C translation"
"SCALA 
 PYTHON DATASET",0.945993031358885,Under review as a conference paper at ICLR 2022
"SCALA 
 PYTHON DATASET",0.9477351916376306,"A,B = gets.split.map(&:to_i)"
"SCALA 
 PYTHON DATASET",0.9494773519163763,"S = gets.chomp
if /ˆ\d{#{A}}-\d{#{B}}/.match(S)
puts ’Yes’
else
puts ’No’
end"
"SCALA 
 PYTHON DATASET",0.9512195121951219,"#include<stdio.h>
int main(void)
{
int A,B;
scanf(""%d %d"",&A,&B);
char S[100];
scanf(""%s"",S);
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A"
"SCALA 
 PYTHON DATASET",0.9529616724738676,",→’){
printf(""No"");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A"
"SCALA 
 PYTHON DATASET",0.9547038327526133,",→’){
printf(""No"");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A"
"SCALA 
 PYTHON DATASET",0.9564459930313589,",→’){
printf(""No"");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A"
"SCALA 
 PYTHON DATASET",0.9581881533101045,",→’){
printf(""No"");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A"
"SCALA 
 PYTHON DATASET",0.9599303135888502,",→’){
printf(""No"");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A"
"SCALA 
 PYTHON DATASET",0.9616724738675958,",→’){
printf(""No"");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A"
"SCALA 
 PYTHON DATASET",0.9634146341463414,",→’){
printf(""No"");
return 0;
}
if(S[0]==’A’&&S[1]==’B’&&S[2]==’B"
"SCALA 
 PYTHON DATASET",0.9651567944250871,",→’){
printf(""No"");
return 0;
}
printf(""Yes"");
return 0;
}"
"SCALA 
 PYTHON DATASET",0.9668989547038328,"#include<stdio.h>
int main(void)
{
int A,B;
scanf(""%d %d"",&A,&B);
char S[100];
scanf(""%s"",S);
if(S[0]==’A’&&S[1]==’B’&&S[2]==’A"
"SCALA 
 PYTHON DATASET",0.9686411149825784,",→’&&S[3]==’B’&&S[4]==’A’
,→&&S[5]==’B’&&S[6]==’A’){
printf(""Yes\n"");
}
else{
printf(""No\n"");
}
return 0;"
"SCALA 
 PYTHON DATASET",0.9703832752613241,"Scala →Python translation
Source Scala code
Correct Python translation
Incorrect Python translation"
"SCALA 
 PYTHON DATASET",0.9721254355400697,import scala.io.Source
"SCALA 
 PYTHON DATASET",0.9738675958188153,"object Main extends App {
val lines: Iterator[String] ="
"SCALA 
 PYTHON DATASET",0.975609756097561,",→Source.stdin.getLines()
val line = lines.next.split("" "")"
"SCALA 
 PYTHON DATASET",0.9773519163763066,",→.map(_.toInt).take(2)
val a = line.head
val b = line(1)
println((if (b >= a) a else a -"
"SCALA 
 PYTHON DATASET",0.9790940766550522,",→1).toString)
}"
"SCALA 
 PYTHON DATASET",0.980836236933798,"import sys
lines = sys.stdin.readlines()
for line in lines:"
"SCALA 
 PYTHON DATASET",0.9825783972125436,"a, b = map(int, line.split()"
"SCALA 
 PYTHON DATASET",0.9843205574912892,",→)
if b >= a:"
"SCALA 
 PYTHON DATASET",0.9860627177700348,"print(a)
else:"
"SCALA 
 PYTHON DATASET",0.9878048780487805,print(b - 1)
"SCALA 
 PYTHON DATASET",0.9895470383275261,"import sys
import os
f = lambda:list(map(int,input()."
"SCALA 
 PYTHON DATASET",0.9912891986062717,",→split()))
if ’local’ in os.environ :"
"SCALA 
 PYTHON DATASET",0.9930313588850174,sys.stdin = open(’./input.
"SCALA 
 PYTHON DATASET",0.9947735191637631,",→txt’, ’r’)
def solve():"
"SCALA 
 PYTHON DATASET",0.9965156794425087,"a = f()[0]
b = f()[0]
print(a if b >= a else a -"
"SCALA 
 PYTHON DATASET",0.9982578397212544,",→1)
solve()"
