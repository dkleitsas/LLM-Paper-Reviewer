Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001949317738791423,"The application of Convolutional Neural Networks (CNNs) to process point cloud
1"
ABSTRACT,0.003898635477582846,"data as geometric representations of real objects has gained considerable attention.
2"
ABSTRACT,0.005847953216374269,"However, point clouds are less structured than images, which makes it difficult to
3"
ABSTRACT,0.007797270955165692,"directly transfer important CNN operations (initially developed for use on images)
4"
ABSTRACT,0.009746588693957114,"to point clouds. For instance, the order of a set of points does not contain semantic
5"
ABSTRACT,0.011695906432748537,"information. Therefore, ideally, all operations must be invariant to the point order.
6"
ABSTRACT,0.01364522417153996,"Inspired by CNN-related operations applied to images, we transfer the concept of
7"
ABSTRACT,0.015594541910331383,"strided and transposed convolutions to point cloud CNNs, enabling deterministic
8"
ABSTRACT,0.017543859649122806,"network modules to operate directly on points. To this end, we propose a novel
9"
ABSTRACT,0.01949317738791423,"strided convolutional layer with an auxiliary loss, which, as we prove theoretically,
10"
ABSTRACT,0.021442495126705652,"enforces a uniform distribution of the selected points within the lower feature
11"
ABSTRACT,0.023391812865497075,"hierarchy. This loss ensures a learnable and deterministic selection, unlike the
12"
ABSTRACT,0.025341130604288498,"iterative Farthest Point Sampling (FPS), which is commonly used in point cloud
13"
ABSTRACT,0.02729044834307992,"CNNs. The high flexibility of the proposed operations is evaluated by deploying
14"
ABSTRACT,0.029239766081871343,"them in exemplary network architectures and comparing their performances with
15"
ABSTRACT,0.031189083820662766,"those of similar (already existing) structures. Notably, we develop a light-weight
16"
ABSTRACT,0.03313840155945419,"autoencoder architecture based on our proposed operators, which shows the best
17"
ABSTRACT,0.03508771929824561,"generalization performance.
18"
INTRODUCTION,0.037037037037037035,"1
Introduction
19"
INTRODUCTION,0.03898635477582846,"The processing of point clouds is crucial for numerous modern applications. For example, in
20"
INTRODUCTION,0.04093567251461988,"autonomous driving, LiDAR sensors enable vehicles to create a 3D scan of their surroundings and
21"
INTRODUCTION,0.042884990253411304,"operate based on the obtained information. While there are several sophisticated CNN architectures
22"
INTRODUCTION,0.04483430799220273,"for image data, key network modules like convolutions with varying stride and max-pooling cannot
23"
INTRODUCTION,0.04678362573099415,"be directly applied to point clouds. This is because point clouds are sets of points, which are located
24"
INTRODUCTION,0.04873294346978557,"arbitrarily in the three-dimensional Euclidean space, and are not structured like pixels in images.
25"
INTRODUCTION,0.050682261208576995,"Therefore, no grid structure defines how to concatenate the feature vectors of individual points to
26"
INTRODUCTION,0.05263157894736842,"a tensor that captures all nodes of the point cloud. Consequently, operations on the tensor must be
27"
INTRODUCTION,0.05458089668615984,"permutation invariant to ensure consistent computation and resemble those of image processing.
28"
INTRODUCTION,0.056530214424951264,"While there are several concepts of convolutions with a stride of one that have been applied to point
29"
INTRODUCTION,0.05847953216374269,"clouds [21, 24, 20, 3], there is no approach that transfers permutation invariant convolutions with a
30"
INTRODUCTION,0.06042884990253411,"higher stride directly to point clouds without an additional auxiliary grid. Yet for image processing,
31"
INTRODUCTION,0.06237816764132553,"feature hierarchies enforced with strided convolutions are a crucial concept in many well-known deep
32"
INTRODUCTION,0.06432748538011696,"learning architectures, e.g., Autoencoders [10], u-net [19], and YOLO [18, 16, 17].
33"
INTRODUCTION,0.06627680311890838,"Hence, this work draws motivation from the assumption that transferring deterministic strided
34"
INTRODUCTION,0.0682261208576998,"and transposed convolutions to point clouds offers great potential. It’s main contributions can be
35"
INTRODUCTION,0.07017543859649122,"summarized as follows:
36"
INTRODUCTION,0.07212475633528265,"• We provide a proxy for permutation invariant convolutions with a step size larger than one
37"
INTRODUCTION,0.07407407407407407,"for point clouds based on an auxiliary loss, which, as we will prove, ensures selection
38"
INTRODUCTION,0.07602339181286549,"diversity. Complementary, we provide a proxy for transposed convolutions that does not
39"
INTRODUCTION,0.07797270955165692,"require knowledge of the points. Since both approaches operate directly on the points, they
40"
INTRODUCTION,0.07992202729044834,"are closely related to the operation of their original counterparts from image-based CNNs.
41"
INTRODUCTION,0.08187134502923976,"• Using these building blocks, we construct an autoencoder1 that not only outperforms the
42"
INTRODUCTION,0.08382066276803118,"current state-of-the-art for reconstructing the complete point cloud but also generalizes
43"
INTRODUCTION,0.08576998050682261,"better than existing approaches.
44"
INTRODUCTION,0.08771929824561403,"• We show that a properly configured version of our model can learn meaningful high-level
45"
INTRODUCTION,0.08966861598440545,"features despite being light-weight. Moreover, we demonstrate that our selection strategy can
46"
INTRODUCTION,0.09161793372319688,"be integrated into existing architectures and replace FPS without a great loss in performance.
47"
INTRODUCTION,0.0935672514619883,"The remainder of this paper is organized as follows: The relevant work related to CNNs on point
48"
INTRODUCTION,0.09551656920077972,"clouds is outlined in Section 2. In Section 3, we introduce our proposed approaches that essentially
49"
INTRODUCTION,0.09746588693957114,"transfer strided and transposed convolutions from the image domain, placing a particular emphasis on
50"
INTRODUCTION,0.09941520467836257,"the auxiliary selection loss. Potential applications for the network modules as well as the associated
51"
INTRODUCTION,0.10136452241715399,"experimental and ablation studies are presented in Section 4. At last, Section 5 concludes this work.
52"
RELATED WORK ON POINT CLOUD CNNS,0.10331384015594541,"2
Related Work on Point Cloud CNNs
53"
RELATED WORK ON POINT CLOUD CNNS,0.10526315789473684,"One of the first techniques that enabled the use of CNNs and convolution operations for point clouds
54"
RELATED WORK ON POINT CLOUD CNNS,0.10721247563352826,"was to voxelize them. However, these operations cannot be applied directly to point clouds without
55"
RELATED WORK ON POINT CLOUD CNNS,0.10916179337231968,"the supporting grid structure. Moreover, voxelization scales cubically with voxel resolution, so these
56"
RELATED WORK ON POINT CLOUD CNNS,0.1111111111111111,"approaches represent a tradeoff between computational cost and accuracy.
57"
RELATED WORK ON POINT CLOUD CNNS,0.11306042884990253,"PointNet, introduced by Qi et al. (2017), was the pioneering neural network architecture for applying
58"
RELATED WORK ON POINT CLOUD CNNS,0.11500974658869395,"deep learning directly to point clouds. The concept of the network is to first process each point
59"
RELATED WORK ON POINT CLOUD CNNS,0.11695906432748537,"individually and finally apply global max-pooling to enable the processing of the feature vector with a
60"
RELATED WORK ON POINT CLOUD CNNS,0.1189083820662768,"fully connected neural network. The main disadvantage of PointNet is that it cannot directly incorpo-
61"
RELATED WORK ON POINT CLOUD CNNS,0.12085769980506822,"rate local features of neighboring points into the convolution. The enhanced version PointNet++ [15],
62"
RELATED WORK ON POINT CLOUD CNNS,0.12280701754385964,"was the first network to introduce feature hierarchies while working directly with points. PointNet++
63"
RELATED WORK ON POINT CLOUD CNNS,0.12475633528265107,"uses iterative farthest point sampling (FPS) [5] to define regions processed by lower-level PointNets,
64"
RELATED WORK ON POINT CLOUD CNNS,0.1267056530214425,"with higher-level features captured in the points sampled by FPS.
65"
RELATED WORK ON POINT CLOUD CNNS,0.1286549707602339,"The Dynamic Graph CNN (DGCNN) proposed by Wang et al. (2019) introduces two novel ideas: first,
66"
RELATED WORK ON POINT CLOUD CNNS,0.13060428849902533,"EdgeConv, a new type of convolution that operates on the k-nearest-neighbor-graph (k-NN-graph) of
67"
RELATED WORK ON POINT CLOUD CNNS,0.13255360623781676,"the point cloud, and second, a dynamic update of the k-NN-graph giving the network its name. The
68"
RELATED WORK ON POINT CLOUD CNNS,0.13450292397660818,"former is the first convolution operation on points which is conceptually transferred from the image
69"
RELATED WORK ON POINT CLOUD CNNS,0.1364522417153996,"domain. It is based on the observation that in images, the convolutional kernel operates on the eight
70"
RELATED WORK ON POINT CLOUD CNNS,0.13840155945419103,"nearest neighbor pixels (also known as Moore neighborhood) of a pixel. A concept for convolutions
71"
RELATED WORK ON POINT CLOUD CNNS,0.14035087719298245,"with a stride greater than one, however, is missing.
72"
RELATED WORK ON POINT CLOUD CNNS,0.14230019493177387,"Other approaches to convolutions on point clouds include KPConv [20] and PAConv [24]. Like
73"
RELATED WORK ON POINT CLOUD CNNS,0.1442495126705653,"DGCNN, PAConv processes the k-NN-relationships. However, instead of directly learning weights,
74"
RELATED WORK ON POINT CLOUD CNNS,0.14619883040935672,"an assembled weight matrix is predicted. This matrix is used to compute features of neighbor relation-
75"
RELATED WORK ON POINT CLOUD CNNS,0.14814814814814814,"ships. PAConv can therefore be integrated into existing architectures as a novel and versatile concept.
76"
RELATED WORK ON POINT CLOUD CNNS,0.15009746588693956,"KPConv, contrarily, operates with a spatial kernel instead of relying on the k-NN-neighborhood.
77"
RELATED WORK ON POINT CLOUD CNNS,0.15204678362573099,"Notably, KPConv proposes an analogy to convolutions with a stride greater than one, which is based
78"
RELATED WORK ON POINT CLOUD CNNS,0.1539961013645224,"on a grid subsampling strategy with a cell size depending on the radius of the kernels. The points for
79"
RELATED WORK ON POINT CLOUD CNNS,0.15594541910331383,"different hierarchies are determined by the barycenters of the original points in each cell. Hence, the
80"
RELATED WORK ON POINT CLOUD CNNS,0.15789473684210525,"stride approach does not operate directly on the points. Since all information must be incapsulated in
81"
RELATED WORK ON POINT CLOUD CNNS,0.15984405458089668,"a code word, the upsampling approach from KPConv, passing the information gathered in barycenters
82"
RELATED WORK ON POINT CLOUD CNNS,0.1617933723196881,"to the before aggregated points, is not suitable to construct a decoder for this task.
83"
RELATED WORK ON POINT CLOUD CNNS,0.16374269005847952,"Generally, the approaches for point cloud autoencoders are based on the idea that point clouds
84"
RELATED WORK ON POINT CLOUD CNNS,0.16569200779727095,"describe the surfaces of objects. The approach is to fit a 2D grid by trying to stretch, squeeze and
85"
RELATED WORK ON POINT CLOUD CNNS,0.16764132553606237,"fold it onto the 3D surface. The corresponding origami instructions are saved in the code word of the
86"
RELATED WORK ON POINT CLOUD CNNS,0.1695906432748538,"autoencoder. The first network to propose this idea was FoldingNet [26]. However, this technique
87"
RELATED WORK ON POINT CLOUD CNNS,0.17153996101364521,1The corresponding code can be found in the supplementary material.
RELATED WORK ON POINT CLOUD CNNS,0.17348927875243664,"struggles when objects possess holes, or multiple objects are present, as the network then has to
88"
RELATED WORK ON POINT CLOUD CNNS,0.17543859649122806,"stretch the 2D grid across empty space. To solve this issue the most recent approach to reconstructing
89"
RELATED WORK ON POINT CLOUD CNNS,0.17738791423001948,"a complete point cloud, TearingNet [13], additionally learns how to cut and tear the 2D grid into
90"
RELATED WORK ON POINT CLOUD CNNS,0.1793372319688109,"the desired shapes. This is done very successfully as it is the state-of-the-art for autoencoding point
91"
RELATED WORK ON POINT CLOUD CNNS,0.18128654970760233,"clouds with multiple objects. Recently, the utilization of transformer architectures has extended to the
92"
RELATED WORK ON POINT CLOUD CNNS,0.18323586744639375,"point cloud domain as well [27, 6, 28]. In particular Point-M2AE [28] leverages FPS-based feature
93"
RELATED WORK ON POINT CLOUD CNNS,0.18518518518518517,"hierarchies and is pre-trained with the self-supervised task of masked autoencoding. Contrarily to
94"
RELATED WORK ON POINT CLOUD CNNS,0.1871345029239766,"complete reconstruction, only parts of the point clouds are covered and reconstructed. They are never
95"
RELATED WORK ON POINT CLOUD CNNS,0.18908382066276802,"seen by the entire model, necessitating the meaningful reconstruction of previously unseen points.
96"
RELATED WORK ON POINT CLOUD CNNS,0.19103313840155944,"Consequently, skip connections between encoder and decoder are permissible as the objective is to
97"
RELATED WORK ON POINT CLOUD CNNS,0.19298245614035087,"uncover points that have not been exposed through those skip connections.
98"
RELATED WORK ON POINT CLOUD CNNS,0.1949317738791423,"Instead of using stride to reduce the number of processed points as done by image CNNs, other
99"
RELATED WORK ON POINT CLOUD CNNS,0.1968810916179337,"learnable subset approaches have been proposed for point clouds, aiming to improve over FPS with a
100"
RELATED WORK ON POINT CLOUD CNNS,0.19883040935672514,"network-decided and permutation independent selection of points. While according to its definition
101"
RELATED WORK ON POINT CLOUD CNNS,0.20077972709551656,"FPS is permuation invariant, in most applications a greedy or iterative version of FPS is considered to
102"
RELATED WORK ON POINT CLOUD CNNS,0.20272904483430798,"decrease computational complexity. The greedy sampling technique is not permutation invariant as
103"
RELATED WORK ON POINT CLOUD CNNS,0.2046783625730994,"the subsampled output depends on a random starting point and can, thus, result in inconsistent outputs.
104"
RELATED WORK ON POINT CLOUD CNNS,0.20662768031189083,"Gumbel Subset Sampling (GSS) proposed in Yang et al. [25] employs a Gumbel-Softmax to enable a
105"
RELATED WORK ON POINT CLOUD CNNS,0.20857699805068225,"soft learnable selection of points during training and performs a reparameterization during inference.
106"
RELATED WORK ON POINT CLOUD CNNS,0.21052631578947367,"This leads to a Gumbel-Max which then selects specific points in the point cloud. Nevertheless, GSS
107"
RELATED WORK ON POINT CLOUD CNNS,0.2124756335282651,"does not ensure diversely selected points and the final network setup requires a combination of FPS
108"
RELATED WORK ON POINT CLOUD CNNS,0.21442495126705652,"and GSS in order to improve the performance. Critical point layers proposed by Nezhadarya et al.
109"
RELATED WORK ON POINT CLOUD CNNS,0.21637426900584794,"[12] select points based on the number of highest feature activations per point. This operation is
110"
RELATED WORK ON POINT CLOUD CNNS,0.21832358674463936,"permutation invariant. However, an equal distribution of the points is not enforced and it cannot be
111"
RELATED WORK ON POINT CLOUD CNNS,0.2202729044834308,"ensured that the network identifies critical points at the beginning of the training. Finally, Lin et al.
112"
RELATED WORK ON POINT CLOUD CNNS,0.2222222222222222,"[11] present different sampling strategies that are tailored in advance to specific tasks and can be
113"
RELATED WORK ON POINT CLOUD CNNS,0.22417153996101363,"learned by the network. These strategies are designed to enable the network to perform well on their
114"
RELATED WORK ON POINT CLOUD CNNS,0.22612085769980506,"respective tasks, but lack generality.
115"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.22807017543859648,"3
Developing Strided and Transposed Convolutions for Point Clouds
116"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2300194931773879,"As outlined above, there exists a research gap regarding the important concept of convolutions with a
117"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.23196881091617932,"stride greater than one operating directly on the points. Currently, iterative FPS is used for this purpose.
118"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.23391812865497075,"However, it is not permutation invariant, and hence results in inconsistent outputs. Our approach
119"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.23586744639376217,"to convolutions with a stride greater than one operates on the k-NN-graph and samples points by
120"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2378167641325536,"itself for higher feature hierarchies. This is done with an auxiliary loss which enforces a uniform
121"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.23976608187134502,"distribution of the selected points. Further, we propose a counteracting transposed convolution. The
122"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.24171539961013644,"individual network components are described in the following.
123"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.24366471734892786,"Strided Convolutions
Generally, in the convolution operation on images, the learnable kernel
124"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.24561403508771928,"weights are multiplied by the pixels that match the weights in their position relative to a central pixel.
125"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2475633528265107,"This central pixel has neighboring central pixels that the kernel is applied to as well. The distance
126"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.24951267056530213,"between those positions is determined by the stride as its value corresponds to the step size between
127"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.25146198830409355,"two kernel positions. For point clouds, the convolutional layers with a stride of one proposed in this
128"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.253411306042885,"paper, operate similarly to those from DGCNN with the nearest neighbors being determined based
129"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2553606237816764,"on the dynamic graph. Slightly deviating from DGCNN, the feature vectors from the current node
130"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2573099415204678,"to its k-nearest-neighbors are gathered behind the feature vector of the current node, and a (1 × 1)
131"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.25925925925925924,"convolutional kernel is applied to this tensor. Thus, all neighboring points contribute to the feature
132"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.26120857699805067,"result and not just those with the highest activation.
133"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2631578947368421,"Unlike basic convolution, convolutional layers with a stride greater than one decrease the size of the
134"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2651072124756335,"input, which requires the network to compress the information. While basic convolutions can be
135"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.26705653021442494,"used on the k-NN-graph, this approach does not work for strides greater than one because there is no
136"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.26900584795321636,"inherent method to select the central points. In the image domain, a typical convolution that reduces
137"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2709551656920078,"the size of a feature map has a stride of two and a convolutional kernel of size (3 × 3). To mimic this
138"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2729044834307992,"convolution in the case of point clouds, the kernel should only be applied on ⌊n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.27485380116959063,"4 ⌋nodes. To ensure
139"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.27680311890838205,"a similar spacing as between the central pixels of images, the points should not overlap each other
140"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2787524366471735,"within the 4-nearest-neighbor neighborhood. However, splitting the point cloud into such subgroups
141"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2807017543859649,"may not always be possible (e.g., selecting nine points of a uniform 6 × 6 point grid) and may not
142"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2826510721247563,"always yield the same result (consider points with equal distances to its 2-nearest neighbors placed
143"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.28460038986354774,"on a circle). The main idea to overcome these obstacles and realize a stride greater than one for point
144"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.28654970760233917,"clouds, nevertheless, is to let the network decide which nodes to process further in the successive
145"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2884990253411306,"layers and simultaneously enforce a diverse selection of points. Letting the network decide which
146"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.290448343079922,"nodes to process further is implemented with an attention map, i.e., a vector with one importance
147"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.29239766081871343,"value per node, predicted by the network. This resembles a score function s(pi) computing a score
148"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.29434697855750486,"for each node pi of the point cloud P. From this attention map, the ⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2962962962962963,"fd ⌉highest values and the
149"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.2982456140350877,"corresponding node indices are selected. Here, fd is the factor by which the set of points should be
150"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3001949317738791,"decreased. This means that the value of fd corresponding to the typical strided convolution is 4. The
151"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.30214424951267055,"nodes that have not been selected in the multi-dimensional feature map are then dropped.
152"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.30409356725146197,"Auxiliary Selection Loss
If the network decides itself which nodes to keep and which not, it may
153"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3060428849902534,"happen that only nodes in an arbitrary fraction of the point cloud are selected at the beginning of
154"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3079922027290448,"the training. Then, a diverse selection of points cannot be simultaneously enforced. This does not
155"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.30994152046783624,"correspond to the idea of strided convolutions for images, and the network cannot process the full
156"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.31189083820662766,"information. Additionally, the selection may be unstable and unpredictable for the network, causing
157"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3138401559454191,"problems in the learning process. Thus, the network needs to learn which nodes are neighbors in the
158"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3157894736842105,"feature map and preferably select non-neighboring nodes to attain evenly distributed points. To guide
159"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.31773879142300193,"the prediction of the attention map referred to above, in this direction, an auxiliary loss capturing the
160"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.31968810916179335,"diversity of the selected nodes is computed. In order to do so, for every node, the attention values of
161"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3216374269005848,"its (fd −1)-nearest neighbors are gathered behind the attention value of the corresponding node in
162"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3235867446393762,"the attention map, resulting in a matrix M. Ideally, from a selection diversity perspective, the two
163"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3255360623781676,"following conditions are met: the first entry in each row of the selected points equals one2, and the
164"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.32748538011695905,"other values of this row equal zero. Further, in the rows of the non-selected points, the first entry
165"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.32943469785575047,"should be zero and one of the other entries one. Table 1 shows an example for an ideal M. If both
166"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3313840155945419,"conditions are met, it is ensured that 1) there are no neighboring selected points (all row entries
167"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3333333333333333,"are zero except for the first one), and 2) every non-selected point has a selected neighbor (one of
168"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.33528265107212474,"the entries, except for the first one, is not zero). Then, the selected points are evenly spread in the
169"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.33723196881091616,"k-NN-graph over which the strided convolution will slide. To measure to what extent the desired
170"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3391812865497076,"properties are met by the network, a loss per row and column is computed. The values in every row
171"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.341130604288499,"are summed, and as it should yield one for every point in the ideal setting, the deviation is measured
172"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.34307992202729043,"in terms of the squared error. The column-wise loss is computed only for the selected points, in which
173"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.34502923976608185,case the sum of the first column entries should yield ⌈n
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3469785575048733,"fd ⌉, and the sum of the remaining columns
174"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3489278752436647,"should yield zero. The total loss is the sum of all parts multiplied by
1
fd as a weighting factor in the
175"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3508771929824561,"case of different
1
fd throughout the network. An illustration of the whole selection operation can
176"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.35282651072124754,"be seen in Figure 3. Assuming that mi,j is the entry in the ith row and the jth column of M, the
177"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.35477582846003897,"mathematical equation for the auxiliary loss LS is
178"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3567251461988304,"LS = 1 fd
·   ""⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3586744639376218,"fd ⌉
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.36062378167641324,"i=1
mi,1"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.36257309941520466,"
−
 n fd #2 + fd
X j=2 ⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3645224171539961,"fd ⌉
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3664717348927875,"i=1
mi,j 2
+ n
X i=1"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3684210526315789,""" fd
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.37037037037037035,"j=1
mi,j 
−1 #2"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3723196881091618,".
(1)"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3742690058479532,"Theorem 3.1. In the case of the global optimum with LS = 0 and under the requirement that
179"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3762183235867446,"s(pi) ≥0, ∀pi ∈P, there cannot be two neighboring selected points.
180"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.37816764132553604,Proof. It is sufficient to show that the matrix until row ⌈n
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.38011695906432746,"fd ⌉will cause LS
>
0 as
181"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3820662768031189,"Pn
i=⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3840155945419103,"fd ⌉+1((Pfd
j=1 mi,j) −1)2 ≥0. To this end, we show that in the simplest setting LS > 0 if
182"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.38596491228070173,"two neighboring points are selected. We proceed to show that if a matrix already caused LS > 0
183"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.38791423001949316,"independent of the particular deviation, neither an addition of a row nor a column can lead to LS = 0
184"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3898635477582846,"which completes the induction. For the global optimum of LS = 0 all row sums ri = Pfd
j=1 mi,j
185"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.391812865497076,"must equal the optimal value r∗
i = 1, ∀i and all column sums cj = P⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3937621832358674,"fd ⌉
i=1 mi,j must equal the
186"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.39571150097465885,"optimal value c∗
1 = ⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.39766081871345027,"fd ⌉and c∗
j = 0, ∀j ∈{2, . . . , fd}. In the simplest case of fd = 2 and two
187"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.3996101364522417,"selected points p1 and p2 (i.e., n ∈{3, 4}) with attention values s(p1) = a1 > s(p2) = a2 if p1 is a
188"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4015594541910331,"2Note that the value corresponding to a node being selected can be any value > 0 if the network does not
need too large weights to attain it. Here, 1 is chosen as an analogy to true and false."
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.40350877192982454,"Table 1: Illustration of the ideal setting for diversely selected points if fd = 3. p1 and p2 are the
selected points (blue) and receive an importance value of 1 (first value column s(pi) of the table
depicting M). Their fd-nearest-neighbor neighborhood is represented by the orange lines. All other
points receive an importance value of 0 but either p1 or p2 is their first nearest neighbor (NN1). P1 P6 P2
P5 P4 P3"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.40545808966861596,Node s(pi) NN1 NN2
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4074074074074074,"p1
1
0
0
p2
1
0
0
p3
0
1
0
p4
0
1
0
p5
0
1
0
p6
0
1
0"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4093567251461988,"nearest neighbor of p2 to attain the global optimum for LS it is required that c1 = a1 + a2
!= c∗
1 = 2
189"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.41130604288499023,"and r2 = a1 + a2
!= r∗
2 = 1 yielding a contradiction. Thus, either dc,1 = c1 −c∗
1 ̸= 0 or
190"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.41325536062378165,"dr,2 = r2 −r∗
2 ̸= 0 or both. In the general case ∃j : dc,j ̸= 0 ∨∃i : dr,i ̸= 0 and thus
191 d = ⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4152046783625731,"fd ⌉
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4171539961013645,"i=1
|dr,i| + fd
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4191033138401559,"j=1
|dc,j| > 0.
(2)"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.42105263157894735,"If adding a new column by increasing fd to fd + 1 could cause d = 0 its attention values must be
192"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.42300194931773877,"mi,fd+1 = −dr,i, ∀i and ∃i : dr,i ̸= 0 ∧∄j : dc,j ̸= 0 needs to be true. Thus, the loss requires
193"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4249512670565302,"c∗
fd+1 = 0
!="
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4269005847953216,"⌈n/fd⌉
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.42884990253411304,"i=1
−dr,i.
(3)"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.43079922027290446,"Per requirement there can only be mi,j ≥0 and therefore dr,i ≤0, ∀i which yields a sum greater
194"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4327485380116959,"than zero and thus a contradiction to 3. If adding a new row by increasing the number of selected
195"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4346978557504873,"points could cause d = 0, its attention values must be m⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.43664717348927873,"fd ⌉+1,1 = −dc,1 + 1 and m⌈n"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.43859649122807015,"fd ⌉+1,j =
196"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4405458089668616,"−dc,j, ∀j ∈{2, . . . , fd} since c∗
1 increases by 1 if a new row is added. Further, analogously to above
197"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.442495126705653,"∃j : dc,j ̸= 0 ∧∄i : dr,i ̸= 0 must be true. Hence, for this case g = 0 requires
198"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4444444444444444,"r∗
⌈n/fd⌉+1 = 1
!= (−dc,1 + 1) + fd
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.44639376218323584,"j=2
−dc,j = fd
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.44834307992202727,"j=1
−dc,j + 1
⇒
0
!= fd
X"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4502923976608187,"j=1
−dc,j.
(4)"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4522417153996101,"The argument now is the same as it was for adding a new column. Thus, adding rows and columns
199"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.45419103313840153,"cannot change d to be equal to zero which is the requirement for the global optimum and therefore,
200"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.45614035087719296,"two selected points neighboring each other yields LS ̸= 0.
201"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4580896686159844,"While in theory, the proof requires that s(pi) ≥0, ∀pi ∈P, our experiments have shown that
202"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4600389863547758,"in practice, it is sufficient to employ a LeakyReLU instead of a ReLU activation function on the
203"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4619883040935672,"predictions. Advantageously, the complete selection operation causes the network to learn an order
204"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.46393762183235865,"of nodes from the point cloud represented by the attention vector. This order will be independent
205"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.46588693957115007,"of the order of nodes in the input tensor as all operations performed are permutation invariant. The
206"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4678362573099415,"independence property of point clouds enables the construction of autoencoders for point clouds that
207"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4697855750487329,"can learn a common representation for it, disregarding the input permutation of points. Further, a
208"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.47173489278752434,"reduced point subset allows the network to connect nodes previously far apart from one another.
209"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.47368421052631576,"Transposed Convolutions
Upsampling of feature maps, in the case of CNNs for images, often
210"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4756335282651072,"happens via transposed convolutions. Transposed convolutions are also referred to as convolutions
211"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.4775828460038986,"with fractional strides. This step size, smaller than one, is obtained by adding spacing between the
212"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.47953216374269003,"entries of the feature map. Thus, a kernel processes neighboring relations in a less dense but rather
213"
DEVELOPING STRIDED AND TRANSPOSED CONVOLUTIONS FOR POINT CLOUDS,0.48148148148148145,more spacious manner. If a stride of 1
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.4834307992202729,"2 is applied the distance between positions in the input is doubled
214"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.4853801169590643,"and the void positions are filled with zero entries. On this new feature map, the kernel is applied
215"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.4873294346978557,"max
gather"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.48927875243664715,select
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.49122807017543857,"backpropagate
auxiliary loss"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.49317738791423,output
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.4951267056530214,"(a)
(b)"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.49707602339181284,"Figure 1: (a) Illustration of the proposed convolution with a stride greater than one on point clouds.
The input tensor (omitting the batch dimension) consists of the points in orange (two channels for
two-dimensional points) and the gathered k-nearest-neighbor points attached to them in blue (with
k = 1 for visibility reasons). Potentially, multiple layers of 1 × 1 kernel convolutions (grey box over
the input tensor) operate on this tensor and from the resulting importance vector, the ⌈n"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.49902534113060426,"fd ⌉values with
the highest score are selected (curly braces). The input feature map is reduced to the corresponding
nodes (bottom right) and the auxiliary loss is computed based on the importance vector (top right).
(b) Illustration of the upsampling operation on point clouds. The exemplary input feature map (left)
contains 3 points (shown by different colors). Every k-nearest-neighbor relationship is processed
individually (middle) leading to an increase of the point dimension."
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5009746588693957,"with a step size of one. Thus, in most cases, only two previous positions are covered by a kernel.
216"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5029239766081871,"Compliant with the desired reproduction of convolution operations for point clouds during the novel
217"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5048732943469786,"upsampling operation, the neighboring relationships are considered individually. In this operation, a
218"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.50682261208577,"kernel creates a feature of a new point by processing the current point and one of its nearest neighbors
219"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5087719298245614,"without knowledge of the point positions which should be sampled. This way, learned information
220"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5107212475633528,"about the surrounding of a current point captured in its feature vector can be translated into new
221"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5126705653021443,"points. This operation can be seen in Figure 3. Specifically, every point pi is repeated fu times and
222"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5146198830409356,"stacked with the vectors pointing from pi to its fu −1 nearest neighbors and the null vector. This
223"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5165692007797271,"yields n ∗fu different point vector pairs which are processed by a normal convolution operation
224"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.5185185185185185,"producing n ∗fu points in a new feature space. This way, the convolutions with a stride greater than
225"
IS APPLIED THE DISTANCE BETWEEN POSITIONS IN THE INPUT IS DOUBLED,0.52046783625731,"one can be undone and higher-level features can be translated into lower-level ones.
226"
EXPERIMENTS,0.5224171539961013,"4
Experiments
227"
EXPERIMENTS,0.5243664717348928,"Autoencoders make use of down- and upsampling operations, and hence, are suitable to exemplarily
228"
EXPERIMENTS,0.5263157894736842,"apply both presented proxies for the point cloud domain. The encoder of the network presented in
229"
EXPERIMENTS,0.5282651072124757,"this work is analogous to the well-known ResNet structure [9], with the additional auxiliary loss
230"
EXPERIMENTS,0.530214424951267,"LS from Equation 1 enabling strided convolutions. The decoder structure also employs the ResNet
231"
EXPERIMENTS,0.5321637426900585,"blocks and implements the concept for transposed convolutions. For a more detailed description of
232"
EXPERIMENTS,0.5341130604288499,"the architecture specifics see the appendix. To test the performance of our proposed selection strategy
233"
EXPERIMENTS,0.5360623781676414,"in other architectures, we replace the FPS module in Point-M2AE with our FPS alternative. However,
234"
EXPERIMENTS,0.5380116959064327,"incorporating our selection into the Point-M2AE Transformer is not straightforward since their
235"
EXPERIMENTS,0.5399610136452242,"masking strategy depends on the selection module in a way that the neighborhood embeddings of the
236"
EXPERIMENTS,0.5419103313840156,"unmasked points do not interfere with those of the masked points. Consequently, the selection must
237"
EXPERIMENTS,0.543859649122807,"be completed before the network calculations are carried out. Our proposed selection is intentionally
238"
EXPERIMENTS,0.5458089668615984,"based on the previous layer output. This way the selection module can leverage the knowledge of what
239"
EXPERIMENTS,0.5477582846003899,"points cause high activations in previous layers. Incorporating this selection after the computation
240"
EXPERIMENTS,0.5497076023391813,"of the different hierarchical layers, however, enables the network to select points at the boundary
241"
EXPERIMENTS,0.5516569200779727,"between masked and unmasked tokens in the transformer and causes the training to not converge.
242"
EXPERIMENTS,0.5536062378167641,"Therefore, we build two versions: Point-M2AE-c with a selection module only processing the spatial
243"
EXPERIMENTS,0.5555555555555556,"information of the points and Point-M2AE-e for which the first selection module is based on the
244"
EXPERIMENTS,0.557504873294347,"neighborhood embeddings. These embeddings are independent of the masking and the following
245"
EXPERIMENTS,0.5594541910331384,"selections are again based on the points themselves.
246"
EXPERIMENTS,0.5614035087719298,"N/A
24.58
28.19
28.30"
EXPERIMENTS,0.5633528265107213,"4.72
 6.54
 8.92
11.58"
EXPERIMENTS,0.5653021442495126,"16.22
18.97
21.35
N/A"
EXPERIMENTS,0.5672514619883041,"N/A
24.98
28.54
28.89"
EXPERIMENTS,0.5692007797270955,"4.45
 6.21
 8.45
11.01"
EXPERIMENTS,0.571150097465887,"16.39
19.19
21.72
N/A"
EXPERIMENTS,0.5730994152046783,"N/A
 6.16
 7.29
 9.19"
EXPERIMENTS,0.5750487329434698,"6.21
 7.10
 9.08
N/A"
EXPERIMENTS,0.5769980506822612,"4.21
 5.53
 7.49
 8.25"
EXPERIMENTS,0.5789473684210527,"FoldingNet
TearingNet
Ours"
EXPERIMENTS,0.580896686159844,"KIMO3
KIMO4
KIMO5
KIMO6
KIMO3
KIMO4
KIMO5
KIMO6
KIMO3
KIMO4
KIMO5
KIMO6"
EXPERIMENTS,0.5828460038986355,KIMO (+1)
EXPERIMENTS,0.5847953216374269,KIMO ( 0)
EXPERIMENTS,0.5867446393762183,KIMO (-1)
EXPERIMENTS,0.5886939571150097,Train Data
EXPERIMENTS,0.5906432748538012,Test Data
EXPERIMENTS,0.5925925925925926,"Figure 2: The performance values of the different models given in terms of the extended chamfer
distance (Equation 5), multiplied by factor 100 for better readability. Each of the models (represented
by three separate blocks) was trained on four training data sets (columns per block) and tested with
those validations sets deviating in their grid size by ±1 (rows). The best performing not cross tested
model is marked in bold."
POINT CLOUD RECONSTRUCTION,0.594541910331384,"4.1
Point Cloud Reconstruction
247"
POINT CLOUD RECONSTRUCTION,0.5964912280701754,"In the first experiment, we investigate on the reconstruction ability of the proposed autoencoder
248"
POINT CLOUD RECONSTRUCTION,0.5984405458089669,"and train it as well as Folding- and TearingNet3 on the KIMO3-6 data sets proposed by [13],
249"
POINT CLOUD RECONSTRUCTION,0.6003898635477583,"synthesized out of the KITTI 3D object data set [7] by cutting out traffic participants, and designed
250"
POINT CLOUD RECONSTRUCTION,0.6023391812865497,"with challenging multiple-object scenes in mind. In our experiments, each data set contains 50,000
251"
POINT CLOUD RECONSTRUCTION,0.6042884990253411,"training instances and 10,000 test instances, deviating from the original construction description of
252"
POINT CLOUD RECONSTRUCTION,0.6062378167641326,"the data sets to make the trained models more comparable. Therefore, Tearing- and FoldingNet are
253"
POINT CLOUD RECONSTRUCTION,0.6081871345029239,"trained with their default parameters. Our own networks are trained with a learning rate of 4 · 10−3
254"
POINT CLOUD RECONSTRUCTION,0.6101364522417154,"which is exponentially decreasing and halved every 80 epochs. The k is set to 10 and the fd and fu
255"
POINT CLOUD RECONSTRUCTION,0.6120857699805068,"values are set to 6 for every layer, yielding a code word that consists of 57 points described each by
256"
POINT CLOUD RECONSTRUCTION,0.6140350877192983,"9 code word channels (cc). The performance of all models is evaluated with the extended chamfer
257"
POINT CLOUD RECONSTRUCTION,0.6159844054580896,"distance [26, 13] between the reconstructed and the original point cloud. This metric between two
258"
POINT CLOUD RECONSTRUCTION,0.6179337231968811,"point sets S1 and S2 is defined as
259"
POINT CLOUD RECONSTRUCTION,0.6198830409356725,"d(S1, S2) = max"
POINT CLOUD RECONSTRUCTION,0.621832358674464,"(
1
|S1| X"
POINT CLOUD RECONSTRUCTION,0.6237816764132553,"x1∈S1
min
x2∈S2∥x1 −x2∥2,
1
|S2| X"
POINT CLOUD RECONSTRUCTION,0.6257309941520468,"x2∈S2
min
x1∈S1∥x2 −x1∥2
) .
(5)"
POINT CLOUD RECONSTRUCTION,0.6276803118908382,"The chamfer distance is the loss function of the Folding- and TearingNet. For our network, we found
260"
POINT CLOUD RECONSTRUCTION,0.6296296296296297,"a slightly improved performance when using squared distances between points in the loss function.
261"
POINT CLOUD RECONSTRUCTION,0.631578947368421,"We analyze the reconstruction capabilities of the different networks (a) for the same type of data set
262"
POINT CLOUD RECONSTRUCTION,0.6335282651072125,"that was used for training, and (b) across corresponding neighboring data sets (see Figure 2). To
263"
POINT CLOUD RECONSTRUCTION,0.6354775828460039,"ensure better comparability of the results for the cross-data set studies, we scaled all point clouds to
264"
POINT CLOUD RECONSTRUCTION,0.6374269005847953,"the size of the point clouds that were used for training the respective model. The results can be seen
265"
POINT CLOUD RECONSTRUCTION,0.6393762183235867,"in Figure 2. Our approach outperforms the state-of-the-art models, when the test data corresponds to
266"
POINT CLOUD RECONSTRUCTION,0.6413255360623782,"the training data. The most significant performance gain is achieved for the KIMO6 data set with
267"
POINT CLOUD RECONSTRUCTION,0.6432748538011696,"many small objects on average. Notably, our proposed approach is very robust, as evidenced by the
268"
POINT CLOUD RECONSTRUCTION,0.645224171539961,"competitive performance on the neighboring data sets. This means that it can handle other data sets
269"
POINT CLOUD RECONSTRUCTION,0.6471734892787524,"much better (i.e., it generalizes more) than its two main competitors. However, in our approach, the
270"
POINT CLOUD RECONSTRUCTION,0.6491228070175439,"typically superior model is the one trained on the corresponding training data. There is an anomaly
271"
POINT CLOUD RECONSTRUCTION,0.6510721247563352,"with KIMO5, as the models trained on KIMO4 outperform it. In contrast, all other models trained on
272"
POINT CLOUD RECONSTRUCTION,0.6530214424951267,"non-corresponding training data achieved slightly inferior, yet still comparable, results.
273"
CLASSIFICATION,0.6549707602339181,"4.2
Classification
274"
CLASSIFICATION,0.6569200779727096,"To assess the capability of our autoencoder to effectively represent 3D data in the generated code
275"
CLASSIFICATION,0.6588693957115009,"words, following the methodology employed in prior studies (see Table 2 on the left), we conduct a
276"
CLASSIFICATION,0.6608187134502924,"two-step evaluation. First, we train the autoencoder on the ShapeNet [4] data set, which encompasses
277"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6627680311890838,"3Note that for the task of compressing the complete information of the scene and reconstructing the original
point cloud, we are not directly comparable with existing transformers like Point-M2AE. This is because they
are trained using a masking strategy to learn as much global information as possible and do not compress
information as networks trained to fully reconstruct a point cloud do. Both approaches are beneficial in different
use cases."
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6647173489278753,"Table 2: The achieved accuracy of an SVM trained on the representation obtained by different
self-supervised learning models for the ModelNet40 data set."
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6666666666666666,"Model
Acc. (%)"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6686159844054581,"T-L Network [8]
65.4
3D-GAN [22]
83.3
Latent-GAN [1]
84.5
FoldingNet [26]
88.4
DGCNN + CrossPoint [2]
91.2
Transformer + OcCo [27]
89.6
Point-BERT [27]
87.4
Point-M2AE [28]
92.9"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6705653021442495,"Model
Subsampling
fd
Acc. (%)"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.672514619883041,"Config-1
ours
(6, 6)
37.1
Config-2
ours
(14, 8)
67.0
Config-3
ours
(12, 12)
81.8"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6744639376218323,"Point-M2AE-fps
fps
(8, 4, 2)
91.6
Point-M2AE-c
ours
(4, 2, 4)
90.3
Point-M2AE-e-1
ours + emb
(4, 2, 4)
91.2
Point-M2AE-e-2
ours + emb
(8, 4, 2)
90.7"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6764132553606238,"55 distinct 3D object categories and over 50,000 3D shapes. Then, we store the code words of the
278"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6783625730994152,"previously unseen 3D objects in the ModelNet40 [23] data set and aggregate the information by
279"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6803118908382066,"taking the sum of the mean and maximum for every code word. Utilizing those aggregated code
280"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.682261208576998,"words, we train a linear Support Vector Machine (SVM) as our classifier. We conduct experiments
281"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6842105263157895,"involving different variations of our architecture, as well as variants of Point-M2AE, by replacing
282"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6861598440545809,"FPS with our own selection approach. The results of these experiments are presented in Table 2.
283"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6881091617933723,"We test our network in three different configurations to vary the amount of features and points in
284"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6900584795321637,"the code word. The first one, ""Config-1"", is equivalent to the network configuration used during
285"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6920077972709552,"point cloud reconstruction. Despite outperforming its competitors during the reconstruction task,
286"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6939571150097466,"the SVM struggles to effectively distinguish the code word representations. We hypothesize that
287"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.695906432748538,"this can be partially attributed to the limited nature of our network’s code word, which comprises 57
288"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6978557504873294,"points with 9 cc each. By contrast, the competing model with the highest accuracy ""Point-M2AE""
289"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.6998050682261209,"has 64 points with 348 cc. If we alter the composition of our model’s code word to 19 points and
290"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7017543859649122,"27 cc (""Config-2"") we are able to increase the SVM accuracy by almost 50% without changing the
291"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7037037037037037,"total information of the code word (57 × 9 = 19 × 27). If we allow more information in the code
292"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7056530214424951,"word with ""Config-3"" (15 points and 137 cc) we are able to achieve a competitive accuracy while
293"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7076023391812866,"still maintaining significantly less information in the code word (15 × 137 < 64 × 348). In addition
294"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7095516569200779,"to the aforementioned comparisons, it is crucial to consider the parameter count of the models
295"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7115009746588694,"being compared. For instance, Point-M2AE utilizes approximately 15.3 × 106 parameters, while
296"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7134502923976608,"FoldingNet employs around 2×106 parameters. In contrast, our network operates with a substantially
297"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7153996101364523,"lower parameter count of approximately 3.4 × 105. This disparity in parameter count highlights an
298"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7173489278752436,"important aspect to consider when evaluating the efficiency and computational requirements of the
299"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7192982456140351,"different models under investigation.
300"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7212475633528265,"As expected, Point-M2AE-c with a selection only based on the spatial performance of the points
301"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.723196881091618,"performs worse than Point-M2AE-e-1, which, in turn, however, cannot achieve the same performance
302"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7251461988304093,"as the original Point-M2AE with the equivalent fd configurations. Nevertheless, the new selection
303"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7270955165692008,"strategy proves to be more robust when a large fd is employed. It can be seen that the decrease in
304"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7290448343079922,"performance is not as significant for Point-M2AE-e-2 compared to Point-M2AE-e-1 as for Point-
305"
NOTE THAT FOR THE TASK OF COMPRESSING THE COMPLETE INFORMATION OF THE SCENE AND RECONSTRUCTING THE ORIGINAL,0.7309941520467836,"M2AE-fps compared to Point-M2AE.
306"
ABLATION,0.732943469785575,"4.3
Ablation
307"
ABLATION,0.7348927875243665,"In the ablation study, we investigate the influence of the choice for the number of nearest neighbors
308"
ABLATION,0.7368421052631579,"per point processed within one convolution and the choice of the fd and fu values. The reconstruction
309"
ABLATION,0.7387914230019493,"quality does not differ significantly for all tested parameters and stabilizes by a chamfer distance
310"
ABLATION,0.7407407407407407,"of approximately 0.018, suggesting that fd and fu are not the bottleneck for the compression of
311"
ABLATION,0.7426900584795322,"information – encouraging a lighter parameter choice leading to computationally less expensive
312"
ABLATION,0.7446393762183235,"models. Further, we implement FPS in our proposed architecture instead of a network guided
313"
ABLATION,0.746588693957115,"sampling and find that we can achieve the same reconstruction performance as FPS in our architecture
314"
ABLATION,0.7485380116959064,"while producing a permutation invariant code word. The detailed results can be found in the appendix.
315"
ABLATION,0.7504873294346979,"To get a better understanding of the model’s learning process, the distribution of the selected points
316"
ABLATION,0.7524366471734892,"can be analyzed, e.g., using the point sampling depicted in Figure 3. The blue points are selected in
317"
ABLATION,0.7543859649122807,"the first level and the red points in both the first and the second level. If the points are not distributed
318"
ABLATION,0.7563352826510721,"Epoch 1
Epoch 50
Epoch 300"
ABLATION,0.7582846003898636,"Figure 3: The model’s point selection (top row and bottom left), and model predictions (bottom
middle and bottom right). Columns correspond to the different epochs (1, 50, and 300) and depict
different aspects of the learning procedure."
ABLATION,0.7602339181286549,"well the network will (during the upsampling process) tend to clump up points in the dense areas and
319"
ABLATION,0.7621832358674464,"produce sparse regions for areas that have only a few points selected. This process is controlled by
320"
ABLATION,0.7641325536062378,"the auxiliary loss LS, and its influence can be observed in the left column of Figure 3. The model
321"
ABLATION,0.7660818713450293,"demonstrates inadequate point distribution during the start of the first epoch (top left), whereas,
322"
ABLATION,0.7680311890838206,"by the end of the first epoch (bottom left), it has learned to distribute the points. However, simply
323"
ABLATION,0.7699805068226121,"distributing the points evenly will result in fuzzy edges and poor capturing of surfaces, as can be seen
324"
ABLATION,0.7719298245614035,"in the predictions in the middle column of Figure 3, which displays the original object (top middle)
325"
ABLATION,0.7738791423001949,"and its prediction (bottom middle), respectively, after roughly 50 epochs of training. This prediction
326"
ABLATION,0.7758284600389863,"shows that the model has learned to locate the object and pinpoint the center of mass, but has not
327"
ABLATION,0.7777777777777778,"captured any exact surfaces yet. Also, there are some points that are distributed completely outside
328"
ABLATION,0.7797270955165692,"the desired shape. This happens because the model has problems bounding the figure during the
329"
ABLATION,0.7816764132553606,"upsampling process. Thus, it is also important for the network to specifically select points that are at
330"
ABLATION,0.783625730994152,"the corner or edges of an object. In the top right of Figure 3, it can be seen that, specifically, the red
331"
ABLATION,0.7855750487329435,"points are very frequently distributed at the contour in a model that has been trained for roughly 300
332"
ABLATION,0.7875243664717348,"epochs. On the bottom right, the corresponding prediction is depicted. The quality of the prediction
333"
ABLATION,0.7894736842105263,"can be visually assessed, as the points form even surfaces with only minimal deviations. Furthermore,
334"
ABLATION,0.7914230019493177,"there are no outliers that scatter far away from their intended position.
335"
CONCLUSION,0.7933723196881092,"5
Conclusion
336"
CONCLUSION,0.7953216374269005,"In conclusion, we introduce a novel network-based point selection strategy that guarantees the diver-
337"
CONCLUSION,0.797270955165692,"sity of selected points equivalent to FPS, while possessing the advantages of permutation invariance
338"
CONCLUSION,0.7992202729044834,"and learnability. Employing the proposed strategy in a simple yet effective autoencoder we show its
339"
CONCLUSION,0.8011695906432749,"superiority compared to previous state-of-the-art approaches on the task of fully reconstructing a
340"
CONCLUSION,0.8031189083820662,"point cloud with multiple objects. Interestingly, both established methods had difficulties processing
341"
CONCLUSION,0.8050682261208577,"data types on which they were not trained, while our proposed model generalizes much better and
342"
CONCLUSION,0.8070175438596491,"suffers only minor performance losses. Even though the model is considerably smaller than recent
343"
CONCLUSION,0.8089668615984406,"unsupervised learning models it is able to represent a given 3D shape well. Further, the proposed
344"
CONCLUSION,0.8109161793372319,"FPS alternative proves to be integrateable into other existing architectures and is more helpful for
345"
CONCLUSION,0.8128654970760234,"the model if based on hierarchical features learned by the model rather than the spatial locations
346"
CONCLUSION,0.8148148148148148,"of the points. For future work, we plan to integrate our procedure into a transformer architecture
347"
CONCLUSION,0.8167641325536062,"trained with a masking strategy not dependent on the selected points and further investigate on more
348"
CONCLUSION,0.8187134502923976,"complex architectures. Moreover, synergies between our selection strategy focusing on diversity, and
349"
CONCLUSION,0.8206627680311891,"strategies targeting other point subsets, e.g., ones with little noise, are promising to investigate with a
350"
CONCLUSION,0.8226120857699805,"combined auxiliary loss function.
351"
REFERENCES,0.8245614035087719,"References
352"
REFERENCES,0.8265107212475633,"[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas. Learning representations and genera-
353"
REFERENCES,0.8284600389863548,"tive models for 3d point clouds. In International conference on machine learning, pages 40–49.
354"
REFERENCES,0.8304093567251462,"PMLR, 2018.
355"
REFERENCES,0.8323586744639376,"[2] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna, and R. Rodrigo.
356"
REFERENCES,0.834307992202729,"Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding.
357"
REFERENCES,0.8362573099415205,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
358"
REFERENCES,0.8382066276803118,"(CVPR), pages 9902–9912, June 2022.
359"
REFERENCES,0.8401559454191033,"[3] A. Boulch. Generalizing Discrete Convolutions for Unstructured Point Clouds. In S. Biasotti,
360"
REFERENCES,0.8421052631578947,"G. Lavoué, and R. Veltkamp, editors, Eurographics Workshop on 3D Object Retrieval. The
361"
REFERENCES,0.8440545808966862,"Eurographics Association, 2019. ISBN 978-3-03868-077-2. doi: 10.2312/3dor.20191064.
362"
REFERENCES,0.8460038986354775,"[4] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva,
363"
REFERENCES,0.847953216374269,"S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich 3D Model Repository.
364"
REFERENCES,0.8499025341130604,"Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University —
365"
REFERENCES,0.8518518518518519,"Toyota Technological Institute at Chicago, 2015.
366"
REFERENCES,0.8538011695906432,"[5] Y. Eldar, M. Lindenbaum, M. Porat, and Y. Zeevi. The farthest point strategy for progressive
367"
REFERENCES,0.8557504873294347,"image sampling. IEEE Transactions on Image Processing, 6(9):1305–1315, 1997. doi: 10.
368"
REFERENCES,0.8576998050682261,"1109/83.623193.
369"
REFERENCES,0.8596491228070176,"[6] N. Engel, V. Belagiannis, and K. Dietmayer. Point transformer. IEEE Access, 9:134826–134840,
370"
REFERENCES,0.8615984405458089,"2021. doi: 10.1109/ACCESS.2021.3116304.
371"
REFERENCES,0.8635477582846004,"[7] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision
372"
REFERENCES,0.8654970760233918,"benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
373"
REFERENCES,0.8674463937621832,"[8] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable and generative
374"
REFERENCES,0.8693957115009746,"vector representation for objects. In Computer Vision–ECCV 2016: 14th European Conference,
375"
REFERENCES,0.8713450292397661,"Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pages 484–499.
376"
REFERENCES,0.8732943469785575,"Springer, 2016.
377"
REFERENCES,0.8752436647173489,"[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016
378"
REFERENCES,0.8771929824561403,"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
379"
REFERENCES,0.8791423001949318,"doi: 10.1109/CVPR.2016.90.
380"
REFERENCES,0.8810916179337231,"[10] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
381"
REFERENCES,0.8830409356725146,"science, 313(5786):504–507, 2006.
382"
REFERENCES,0.884990253411306,"[11] Y. Lin, L. Chen, H. Huang, C. Ma, X. Han, and S. Cui. Task-aware sampling layer for point-wise
383"
REFERENCES,0.8869395711500975,"analysis. IEEE Transactions on Visualization and Computer Graphics, pages 1–1, 2022. doi:
384"
REFERENCES,0.8888888888888888,"10.1109/TVCG.2022.3171794.
385"
REFERENCES,0.8908382066276803,"[12] E. Nezhadarya, E. Taghavi, R. Razani, B. Liu, and J. Luo. Adaptive hierarchical down-sampling
386"
REFERENCES,0.8927875243664717,"for point cloud classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
387"
REFERENCES,0.8947368421052632,"Recognition (CVPR), pages 12953–12961, 2020. doi: 10.1109/CVPR42600.2020.01297.
388"
REFERENCES,0.8966861598440545,"[13] J. Pang, D. Li, and D. Tian. Tearingnet: Point cloud autoencoder to learn topology-friendly
389"
REFERENCES,0.898635477582846,"representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
390"
REFERENCES,0.9005847953216374,"Recognition, pages 7453–7462, 2021.
391"
REFERENCES,0.9025341130604289,"[14] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d
392"
REFERENCES,0.9044834307992202,"classification and segmentation. In Proceedings of the IEEE conference on computer vision and
393"
REFERENCES,0.9064327485380117,"pattern recognition, pages 652–660, 2017.
394"
REFERENCES,0.9083820662768031,"[15] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on
395"
REFERENCES,0.9103313840155945,"point sets in a metric space. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
396"
REFERENCES,0.9122807017543859,"S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Sys-
397"
REFERENCES,0.9142300194931774,"tems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
398"
REFERENCES,0.9161793372319688,"paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf.
399"
REFERENCES,0.9181286549707602,"[16] J. Redmon and A. Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE
400"
REFERENCES,0.9200779727095516,"Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
401"
REFERENCES,0.9220272904483431,"[17] J. Redmon and A. Farhadi. Yolov3: An incremental improvement. CoRR, abs/1804.02767,
402"
REFERENCES,0.9239766081871345,"2018. URL http://arxiv.org/abs/1804.02767.
403"
REFERENCES,0.9259259259259259,"[18] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time
404"
REFERENCES,0.9278752436647173,"object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern
405"
REFERENCES,0.9298245614035088,"Recognition (CVPR), June 2016.
406"
REFERENCES,0.9317738791423001,"[19] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image
407"
REFERENCES,0.9337231968810916,"segmentation. In International Conference on Medical image computing and computer-assisted
408"
REFERENCES,0.935672514619883,"intervention, pages 234–241. Springer, 2015.
409"
REFERENCES,0.9376218323586745,"[20] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. Guibas. Kpconv:
410"
REFERENCES,0.9395711500974658,"Flexible and deformable convolution for point clouds.
In 2019 IEEE/CVF International
411"
REFERENCES,0.9415204678362573,"Conference on Computer Vision (ICCV), pages 6410–6419, 2019. doi: 10.1109/ICCV.2019.
412"
REFERENCES,0.9434697855750487,"00651.
413"
REFERENCES,0.9454191033138402,"[21] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon. Dynamic graph
414"
REFERENCES,0.9473684210526315,"cnn for learning on point clouds. ACM Trans. Graph., 38(5), oct 2019. ISSN 0730-0301. doi:
415"
REFERENCES,0.949317738791423,"10.1145/3326362. URL https://doi.org/10.1145/3326362.
416"
REFERENCES,0.9512670565302144,"[22] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learning a probabilistic latent
417"
REFERENCES,0.9532163742690059,"space of object shapes via 3d generative-adversarial modeling. Advances in neural information
418"
REFERENCES,0.9551656920077972,"processing systems, 29, 2016.
419"
REFERENCES,0.9571150097465887,"[23] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep
420"
REFERENCES,0.9590643274853801,"representation for volumetric shapes. In 2015 IEEE Conference on Computer Vision and Pattern
421"
REFERENCES,0.9610136452241715,"Recognition (CVPR), pages 1912–1920, 2015. doi: 10.1109/CVPR.2015.7298801.
422"
REFERENCES,0.9629629629629629,"[24] M. Xu, R. Ding, H. Zhao, and X. Qi. Paconv: Position adaptive convolution with dynamic
423"
REFERENCES,0.9649122807017544,"kernel assembling on point clouds. In 2021 IEEE/CVF Conference on Computer Vision and
424"
REFERENCES,0.9668615984405458,"Pattern Recognition (CVPR), pages 3172–3181, 2021. doi: 10.1109/CVPR46437.2021.00319.
425"
REFERENCES,0.9688109161793372,"[25] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian. Modeling point clouds with
426"
REFERENCES,0.9707602339181286,"self-attention and gumbel subset sampling. In 2019 IEEE/CVF Conference on Computer Vision
427"
REFERENCES,0.9727095516569201,"and Pattern Recognition (CVPR), pages 3318–3327, 2019. doi: 10.1109/CVPR.2019.00344.
428"
REFERENCES,0.9746588693957114,"[26] Y. Yang, C. Feng, Y. Shen, and D. Tian. Foldingnet: Point cloud auto-encoder via deep grid
429"
REFERENCES,0.9766081871345029,"deformation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
430"
REFERENCES,0.9785575048732943,"pages 206–215, 2018.
431"
REFERENCES,0.9805068226120858,"[27] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu. Point-bert: Pre-training 3d point cloud
432"
REFERENCES,0.9824561403508771,"transformers with masked point modeling. In 2022 IEEE/CVF Conference on Computer Vision
433"
REFERENCES,0.9844054580896686,"and Pattern Recognition (CVPR), pages 19291–19300, 2022. doi: 10.1109/CVPR52688.2022.
434"
REFERENCES,0.98635477582846,"01871.
435"
REFERENCES,0.9883040935672515,"[28] R. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, and H. Li. Point-m2ae:
436"
REFERENCES,0.9902534113060428,"Multi-scale masked autoencoders for hierarchical point cloud pre-training. In S. Koyejo,
437"
REFERENCES,0.9922027290448343,"S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neu-
438"
REFERENCES,0.9941520467836257,"ral Information Processing Systems, volume 35, pages 27061–27074. Curran Associates,
439"
REFERENCES,0.9961013645224172,"Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
440"
REFERENCES,0.9980506822612085,"ad1d7a4df30a9c0c46b387815a774a84-Paper-Conference.pdf.
441"
