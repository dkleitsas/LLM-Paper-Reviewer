Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017857142857142857,"Reversible architectures have been shown to be capable of performing on par with
1"
ABSTRACT,0.0035714285714285713,"their non-reversible architectures, being applied in deep learning for memory sav-
2"
ABSTRACT,0.005357142857142857,"ings and generative modeling. In this work, we show how reversible architectures
3"
ABSTRACT,0.007142857142857143,"can solve challenges in parallelizing deep model training. We introduce PETRA,
4"
ABSTRACT,0.008928571428571428,"a novel alternative to backpropagation for parallelizing gradient computations.
5"
ABSTRACT,0.010714285714285714,"PETRA facilitates effective model parallelism by enabling stages (i.e., a set of
6"
ABSTRACT,0.0125,"layers) to compute independently on different devices, while only needing to com-
7"
ABSTRACT,0.014285714285714285,"municate activations and gradients between each other. By decoupling the forward
8"
ABSTRACT,0.01607142857142857,"and backward passes and keeping a single updated version of the parameters, the
9"
ABSTRACT,0.017857142857142856,"need for weight stashing is also removed. We develop a custom autograd-like
10"
ABSTRACT,0.019642857142857142,"training framework for PETRA, and we demonstrate its effectiveness on CIFAR-
11"
ABSTRACT,0.02142857142857143,"10, ImageNet32, and ImageNet, achieving competitive accuracies comparable to
12"
ABSTRACT,0.023214285714285715,"backpropagation using ResNet-18, ResNet-34, and ResNet-50 models.
13"
INTRODUCTION,0.025,"1
Introduction
14"
INTRODUCTION,0.026785714285714284,"First-order methods using stochastic gradients computed via backpropagation on mini-batches are the
15"
INTRODUCTION,0.02857142857142857,"de-facto standard for computing parameter updates in Deep Neural Networks [25]. As datasets and
16"
INTRODUCTION,0.030357142857142857,"models continue to grow [1] there is an urgent need for memory-efficient and scalable parallelization
17"
INTRODUCTION,0.03214285714285714,"of deep learning training across multiple workers. Data parallelism via mini-batches [25] has been
18"
INTRODUCTION,0.033928571428571426,"widely adopted in deep learning frameworks [26]. This approach computes gradients across model
19"
INTRODUCTION,0.03571428571428571,"replicas distributed among workers, yet it requires frequent synchronization to aggregate gradients,
20"
INTRODUCTION,0.0375,"leading to high communication costs, as well as substantial memory redundancy. Furthermore, with
21"
INTRODUCTION,0.039285714285714285,"the increasing size and scale of models exceeding that of the growth of on-device memory, the
22"
INTRODUCTION,0.04107142857142857,"forward and backward passes now often exceed a single device’s memory capacity [35]. To further
23"
INTRODUCTION,0.04285714285714286,"address these issues, methods have attempted to mitigate this memory overhead and to parallelize
24"
INTRODUCTION,0.044642857142857144,"the sequential backpropagation steps themselves across devices, while computing exact gradients.
25"
INTRODUCTION,0.04642857142857143,"Techniques like optimizer sharding [34], tensor parallelism [36], activation checkpointing [6], or
26"
INTRODUCTION,0.048214285714285716,"pipelining [15], have been deployed individually or combined, leading for instance to the development
27"
INTRODUCTION,0.05,"of 3D parallelism [37], a popular methodology which improves the efficiency of the backpropagation
28"
INTRODUCTION,0.05178571428571429,"implementation. On the other hand, the fundamental inefficiency underlying the parallelization of
29"
INTRODUCTION,0.05357142857142857,"backpropagation has not been addressed by these methods.
30"
INTRODUCTION,0.055357142857142855,"However, the use of exact gradient restricts algorithmic choices and parallel implementations, as
31"
INTRODUCTION,0.05714285714285714,"highlighted by [20]. For instance, backpropagation is backward locked: the inputs of each layer
32"
INTRODUCTION,0.05892857142857143,"must be propagated through the network and preserved until an error signal is retropropagated to the
33"
INTRODUCTION,0.060714285714285714,"layer of origin. This requirement enforces a synchronous dependency among subsequent layers and
34"
INTRODUCTION,0.0625,"requires them to systematically store intermediary activations, potentially impeding overall resource
35"
INTRODUCTION,0.06428571428571428,"efficiency as workers must wait for each other to continue their computations and release memory
36"
INTRODUCTION,0.06607142857142857,"used for activations. To unlock the potential of backpropagation, inexact backpropagation procedures
37"
INTRODUCTION,0.06785714285714285,a) Backpropagation
INTRODUCTION,0.06964285714285715,b) PETRA Time
INTRODUCTION,0.07142857142857142,Stage 2
INTRODUCTION,0.07321428571428572,Stage 1
INTRODUCTION,0.075,Stage 0
INTRODUCTION,0.07678571428571429,Stage 2
INTRODUCTION,0.07857142857142857,Stage 1
INTRODUCTION,0.08035714285714286,Stage 0
INTRODUCTION,0.08214285714285714,"Propagation
of one batch Layer Loss"
INTRODUCTION,0.08392857142857142,"Figure 1: Comparison of PETRA with standard backpropagation. This approach splits the
stages of a model and decouples their forward and backward passes, resulting in a sixfold increase in
parallelization speed in this example."
INTRODUCTION,0.08571428571428572,"have been proposed. These procedures are generally conceptualized within the context of model
38"
INTRODUCTION,0.0875,"parallelism, where a neural network is split into stages that can process their activations in parallel,
39"
INTRODUCTION,0.08928571428571429,"potentially on multiple devices. For example, some methods use outdated parameters or activations,
40"
INTRODUCTION,0.09107142857142857,"such as double-buffered pipelining [14] or delayed gradient approaches [44]. However, these methods
41"
INTRODUCTION,0.09285714285714286,"introduce significant memory overhead due to the use of ad hoc buffers for activations, parameters,
42"
INTRODUCTION,0.09464285714285714,"or both. Following an opposite direction, local learning methods [33, 4], which estimate inexact
43"
INTRODUCTION,0.09642857142857143,"gradients via a local auxiliary neural network, pave the way to parallel gradient computations but
44"
INTRODUCTION,0.09821428571428571,"often lead to unrecoverable performance drops [11]. This underscores the need for a robust alternative
45"
INTRODUCTION,0.1,"to backpropagation, with limited memory overhead.
46"
INTRODUCTION,0.10178571428571428,"In this work, we introduce PETRA (Parallel End-to-End Training with Reversible Architectures),
47"
INTRODUCTION,0.10357142857142858,"a novel method designed to parallelize gradient computations within reversible architectures with
48"
INTRODUCTION,0.10535714285714286,"minimal computational overhead. Reversible architectures are an ideal candidate for this task,
49"
INTRODUCTION,0.10714285714285714,"as they can significantly reduce memory overhead during standard backpropagation with limited
50"
INTRODUCTION,0.10892857142857143,"communication costs. Furthermore, reversibility is a minor requirement, as many studies have
51"
INTRODUCTION,0.11071428571428571,"demonstrated that standard architectures can be adapted into reversible ones without any performance
52"
INTRODUCTION,0.1125,"drops [12, 19, 29, 22]. By allowing parameters to evolve in parallel and by computing an approximate
53"
INTRODUCTION,0.11428571428571428,"inversion during backward, we propose an effective alternative to backpropagation which allows
54"
INTRODUCTION,0.11607142857142858,"high model parallelism with a constant communication overhead and no additional buffers. In fact,
55"
INTRODUCTION,0.11785714285714285,"for a constant increase in communication overhead, PETRA achieves a linear speedup compared to
56"
INTRODUCTION,0.11964285714285715,"standard backpropagation with respect to the number J of stages the network is split into. We illustrate
57"
INTRODUCTION,0.12142857142857143,"our approach in Fig. 1, by contrasting the evolution of PETRA with a standard backpropagation pass.
58"
INTRODUCTION,0.12321428571428572,"Contributions.
Our contributions are as follows: (1) We introduce PETRA, a streamlined approach
59"
INTRODUCTION,0.125,"for parallelizing the training of reversible architectures. This method leverages a delayed, approximate
60"
INTRODUCTION,0.12678571428571428,"inversion of activations during the backward pass, allowing for enhanced computational efficiency. (2)
61"
INTRODUCTION,0.12857142857142856,"Our technique significantly reduces memory overhead by minimizing the necessity to store extensive
62"
INTRODUCTION,0.13035714285714287,"computational graphs. (3) It enables the parallelization of forward and backward pass computations
63"
INTRODUCTION,0.13214285714285715,"across multiple devices, effectively distributing the workload and reducing training time. (4) We
64"
INTRODUCTION,0.13392857142857142,"validate the efficacy of PETRA through rigorous testing on benchmark datasets such as CIFAR-10,
65"
INTRODUCTION,0.1357142857142857,"ImageNet-32, and ImageNet, where it demonstrates robust performance with minimal impact on
66"
INTRODUCTION,0.1375,"accuracy. (5) Additionally, we provide a flexible reimplementation of the autograd system in PyTorch,
67"
INTRODUCTION,0.1392857142857143,"specifically tailored for our experimental setup, which we make available to the research community.
68"
RELATED WORK,0.14107142857142857,"2
Related work
69"
RELATED WORK,0.14285714285714285,"Reversible architectures.
Reversible DNNs are composed of layers that are invertible, meaning
70"
RELATED WORK,0.14464285714285716,"that the input of a layer can be computed from its output. This approach allows to avoid the need to
71"
RELATED WORK,0.14642857142857144,"store intermediary activations during the forward pass by reconstructing them progressively during
72"
RELATED WORK,0.14821428571428572,"the backward pass [12], at the cost of an extra computation per layer. Invertible networks further
73"
RELATED WORK,0.15,"improve this method by removing dimensionality reduction steps such as downsamplings, making
74"
RELATED WORK,0.15178571428571427,"the networks fully invertible [18]. Reversibility is not restricted to a type of architecture or tasks
75"
RELATED WORK,0.15357142857142858,"and has been extensively used for generative models [9], for ResNets [12], and Transformers [29].
76"
RELATED WORK,0.15535714285714286,"However, as far as we know, reversible architectures have never been used to enhance parallelization
77"
RELATED WORK,0.15714285714285714,"capabilities.
78"
RELATED WORK,0.15892857142857142,"Alternatives to backpropagation.
Multiple alternatives to backpropagation have been proposed
79"
RELATED WORK,0.16071428571428573,"previously to improve over its computational efficiency. For instance, DNI [20] is the first to mention
80"
RELATED WORK,0.1625,"the backpropagation inefficiency and its inherent synchronization locks. However, they address
81"
RELATED WORK,0.16428571428571428,"those locks with a method non-competitive with simple baselines. Local (or greedy) learning [33, 3]
82"
RELATED WORK,0.16607142857142856,"propose to use layerwise losses to decouple the training of layers, allowing them to train in parallel
83"
RELATED WORK,0.16785714285714284,"[5]. Local learning in videos [28] notably uses the similarity between successive temporal features
84"
RELATED WORK,0.16964285714285715,"to remove buffer memory. However, the difference in training dynamics between local training and
85"
RELATED WORK,0.17142857142857143,"backpropagation still limits such approaches [11, 38].
86"
RELATED WORK,0.1732142857142857,"Pipeline parallelism.
Pipelining encompasses a range of model parallel techniques that divide the
87"
RELATED WORK,0.175,"components of a network into stages that compute in parallel, while avoiding idle workers. Initially
88"
RELATED WORK,0.1767857142857143,"popularized by [15], a batch of data is divided into micro-batches that are processed independently at
89"
RELATED WORK,0.17857142857142858,"each stage. Although more efficient pipelining schedules have been proposed [10], notably to mitigate
90"
RELATED WORK,0.18035714285714285,"the peak memory overhead, keeping an exact batch gradient computation requires leaving a bubble of
91"
RELATED WORK,0.18214285714285713,"idle workers. By alternating one forward and one backward pass for each worker, PipeDream [31]
92"
RELATED WORK,0.18392857142857144,"can allow to get rid of idleness bubbles, but at the expense of introducing staleness in the gradients
93"
RELATED WORK,0.18571428571428572,"used. [32] mitigates this staleness to only one optimization step by accumulating gradients, thus also
94"
RELATED WORK,0.1875,"reducing the parameter memory overhead to only two versions of the parameters. Nevertheless, these
95"
RELATED WORK,0.18928571428571428,"approaches still suffer from a quadratic activation memory overhead with regard to the number of
96"
RELATED WORK,0.19107142857142856,"stages, as micro-batch activations pile up in buffers, especially for early layers. Some implementations
97"
RELATED WORK,0.19285714285714287,"propose to limit this overhead by combining activation checkpointing [6] with pipelining [21, 27],
98"
RELATED WORK,0.19464285714285715,"although the memory overhead still scales with the number of stages.
99"
RELATED WORK,0.19642857142857142,"Delayed gradient.
By allowing stale gradients in the update process, these previous methods
100"
RELATED WORK,0.1982142857142857,"provide the context for our approach. Delayed gradient optimization methods are model parallel
101"
RELATED WORK,0.2,"techniques that aim to decouple and process layers in parallel during backpropagation. In these
102"
RELATED WORK,0.2017857142857143,"approaches, delays occur stage-wise: the backward pass may be computed with outdated parameters
103"
RELATED WORK,0.20357142857142857,"or activations compared to the forward pass. For instance, [16] proposes a feature replay approach,
104"
RELATED WORK,0.20535714285714285,"where a forward pass first stores intermediary activations, which are then ""replayed"" to compute the
105"
RELATED WORK,0.20714285714285716,"backward pass in parallel. This method still requires heavy synchronization between layers, yielding
106"
RELATED WORK,0.20892857142857144,"a lock on computations. In [42] and [43], stale gradients are computed from older parameter versions
107"
RELATED WORK,0.21071428571428572,"differing from the parameters used during the update. This staleness can be mitigated: [43] ’shrinks’
108"
RELATED WORK,0.2125,"the gradient by the delay value, but more advanced techniques also exist [41, 23]. Still, these methods
109"
RELATED WORK,0.21428571428571427,"are limited like previous pipelining methods by their memory overhead as the computational graph
110"
RELATED WORK,0.21607142857142858,"is fully stored. A first step to reduce this, as proposed in Diversely Stale Parameters (DSP) [40],
111"
RELATED WORK,0.21785714285714286,"PipeMare [41] and [23], is to keep a single set of parameters and approximate the gradients computed
112"
RELATED WORK,0.21964285714285714,"during the backward pass with the updated parameters, which differ from the ones used in the forward
113"
RELATED WORK,0.22142857142857142,"pass. This requires, like in activation checkpointing, an additional reconstruction of the computational
114"
RELATED WORK,0.22321428571428573,"graph. Furthermore, the quadratic activation memory overhead still limits the scalability of these
115"
RELATED WORK,0.225,"methods for a large number of stages.
116"
METHOD,0.22678571428571428,"3
Method
117"
STANDARD BACKPROPAGATION,0.22857142857142856,"3.1
Standard backpropagation
118"
STANDARD BACKPROPAGATION,0.23035714285714284,"We consider a DNN composed of J stages (e.g., a layer or a set of layers). An input x0 is propagated
119"
STANDARD BACKPROPAGATION,0.23214285714285715,"through the network, recursively defined by
120"
STANDARD BACKPROPAGATION,0.23392857142857143,"xj ≜Fj(xj−1, θj) ,
(1)"
STANDARD BACKPROPAGATION,0.2357142857142857,"where Fj is the j-th stage parameterized by θj. The backpropagation algorithm is the ubiquitous
121"
STANDARD BACKPROPAGATION,0.2375,"algorithm to compute parameter gradients. First, an input is propagated through the network with
122"
STANDARD BACKPROPAGATION,0.2392857142857143,"a forward pass, while storing its intermediate activations. A scalar loss L is then deduced from the
123"
STANDARD BACKPROPAGATION,0.24107142857142858,"corresponding output xJ. Parameter gradients are then computed during the backward pass by taking
124 + x1"
STANDARD BACKPROPAGATION,0.24285714285714285,"j−1
x2 j−1 x1 j
x2 j"
STANDARD BACKPROPAGATION,0.24464285714285713,"(b) RevNet Forward
(c) RevNet Reverse ˜Fj"
STANDARD BACKPROPAGATION,0.24642857142857144,ThsxEMa9gRZY2gLtsRertFIPNrlABxRe+kxlRqCxEbRrHc2ceM/K9sLjax9CK7wHn0BnoBjH6TtDeENOTQwkqWfP8+MZ768Ety6JPkdVZWnz1fW9+IN1+8fLW1vfP6xOraMOwzLbQ5zcGi4Ar7juBp5VBkLnAQT790r4PztFYrtV3N6twKGseMkZuCANfo78j09pM9reTbrJPOhTSBewe/z+z6+b82/vdFOtJ8VmtUSlWMCrD1Lk8oNPRjHmcAmzmqLFbApjPEsoAKJdujn8zb0Q1AKWmoTjnJ0rv5f4UFaO5P5Hg0gwU32aGHb1FDaXu1ye1ceDT1XVe1QsYfuZS2o07TdmBbcIHNiFgCY4WFAyiZgLngS5y1fUuQXMz8GLUZc2iy4GcoaV/iTOEF01KCKnxWGZ03PmtnKzvNUt7+nmamyLshbhRy2aOA4up489fQon+930oHvwLdj9mTzEOnlL3pGPJCWH5Jh8JT3SJ4xMySW5ItfRbfQvusUjvRAt6Qpehs3AP9JrlC</latexit>xj−1
STANDARD BACKPROPAGATION,0.24821428571428572,ThsxEPamBdLlN+2xF6sUiQOKdjnQHhG9cEylBpDYKJr1ziYG/6xsLxBZ+wxcy4v0BfoAPfdB6Kmq6k04NGUkS98M945srwa1Lkp9R58XLldW17qt4fWNza3un9/rM6towHDItLnIwaLgCoeO4EXlUGQucDz/PpTWz+/QWO5Vl/crMKRhIniJWfgAjW8G/urZryzm/STedDnIH0Cu8fvH79v1n/NRj3osOs0KyWqBwTYO1lmlRu5ME4zgQ2cVZbrIBdwQvA1Qg0Y78fNuG7gWmoKU24SlH5+y/Cg/S2pnMD2gAEtz0gBa2bQ3SNrXL4135ceS5qmqHi2ml7WgTtP2Xlpwg8yJWQDADA8LUjYFA8wFV+KsnVuC5GLmJ6jNhEOTBTeDpK3EmcJbpqUEVfisMjpvfNbuUFg/aJbu9PM2N10mZS3Cj1o0cRxcTv/39Dk4O+ynR/2jz8HuE7KILnlL3pF9kpIP5JickgEZEkY4uSdfyUP0I3qMfkd/Fq2d6EnzhixFZ+0vHU+5zw=</latexit>xj
STANDARD BACKPROPAGATION,0.25,(a) ResNet x1
STANDARD BACKPROPAGATION,0.2517857142857143,"j−1
x2 j−1 x1 j
x2 j ˜Fj"
STANDARD BACKPROPAGATION,0.25357142857142856,yGysoUhZk1M2CsESwYTlIDCDoEap2V89Y+NOy3cDI6hOwTRacgoNwAo7BDrEh7hkWTCjJ0qtX9cpVL68Ety5J7qPO3Lv3H+YXFuOPnz5/+bq0vHJodW0Y9pkW2hznYFwhX3HncDjyiDIXOBRfr7b1o8u0Fiu1YEbVziQMFS85AxcoPZ/nS2tJd1kEvQtSF/A2vZidXNye/XcO1uONrJCs1qickyAtadpUrmB+M4E9jEW2xAnYOQzwNUIFEO/CTRv6IzAFLbUJTzk6YV8rPEhrxzJfpwFIcKN1Wti2NUjb1M6Od+XWwHNV1Q4Vm04va0Gdpu2tOAGmRPjAIAZHhakbAQGmAuOxFk7twTJxdgPUZshyYLTgZJW4kzhZdMSwmq8FldN74rN2hsL7XzNzpJ21uNEvKWoQftWjiOLic/u/pW3C40U03u5v7we4dMo0F8o18Jz9JSn6TbJHeqRPGEFyTf6Qv9Fd9BA9Rk/T1k70olklM9GZ+wfTF7ck</latexit>−
STANDARD BACKPROPAGATION,0.25535714285714284,"PxU2zq+bIwuFM="">ACVXicZVDNbtQwEHZCaUsKtIUjF4uCxKFaJT2UHisqVRwXiW0rNdFq4kx2Tf0TbKdoZeU5ei0vwQvwEIgHgRsSTrYHlo5k6Ztv5hvPfGUjuHVp+jOKH6w9XN/YfJRsPX7ydHtn9mZ1a1hOGFaHNRgkXBFU4cdwIvGoMgS4Hn5dVJXz+/RmO5Vh/dosFCwkzxmjNwgSpyCW7OQPjTbvpurOXjtIh6H2Q3YG941e/vn2/3vo9nu5GB3mlWStROSbA2sbVzhwTjOBHZJ3lpsgF3BDC8DVCDRFn7YuqOvA1PRWpvwlKMD+6/Cg7R2Ict9GkC/5z6tbN8apH1qV8e7+qjwXDWtQ8W0+tWUKdpfzetuEHmxCIAYIaHBSmbgwHmgjtJ3s+tQXKx8DPUZsahy4OrQdJXklzhF6alBFX5vDG67PxgXWX9uFu50w9tbr5KylaEH7XokiS4nP3v6X1wdjDKDkeH4Ld78gyNskL8pK8IRl5S47JezImE8LIZ3JDbsnX6Ef0J16L15etcXSneU5WIt7+C9YRuyU=</latexit>Fj"
STANDARD BACKPROPAGATION,0.2571428571428571,"Figure 2: Differences between the residual block of a ResNet and its reversible counterpart. (a)
Forward of a residual block. (b) Forward and (c) Reverse forward of a reversible residual block. For
reversible blocks, similarly to [12], the input xj is doubled in size and split equally into {x1
j, x2
j}
along the channel dimension. The function Fj includes a skip-connection while ˜Fj does not."
STANDARD BACKPROPAGATION,0.25892857142857145,"advantage of the chain rule: starting from the last stage with δJ = ∇xJL, the gradients with regard
125"
STANDARD BACKPROPAGATION,0.26071428571428573,"to the activations are given by
126"
STANDARD BACKPROPAGATION,0.2625,"δj ≜∇xj−1L = ∂xFj(xj−1, θj)Tδj+1 ,
(2)"
STANDARD BACKPROPAGATION,0.2642857142857143,"and the gradients with regard to the parameters are defined as
127"
STANDARD BACKPROPAGATION,0.26607142857142857,"∆j ≜∇θjL = ∂θFj(xj−1, θj)Tδj+1 .
(3)"
STANDARD BACKPROPAGATION,0.26785714285714285,"Note that these computations follow a synchronous and sequential order. The parameters θj can then
128"
STANDARD BACKPROPAGATION,0.26964285714285713,"be updated given their gradient estimate ∆j, using any optimizer.
129"
REVERSIBLE ARCHITECTURES,0.2714285714285714,"3.2
Reversible architectures
130"
REVERSIBLE ARCHITECTURES,0.2732142857142857,"We focus on the reversible neural networks presented in [12], although our method is not dependent
131"
REVERSIBLE ARCHITECTURES,0.275,"on this architecture. In practice, only a few stages which do not preserve feature dimensionality
132"
REVERSIBLE ARCHITECTURES,0.2767857142857143,"are not reversible and correspond to the downsampling blocks in the ResNet. Fig. 2 highlights
133"
REVERSIBLE ARCHITECTURES,0.2785714285714286,"how reversible residual blocks Fj differ from their standard counterpart. The input is split into two
134"
REVERSIBLE ARCHITECTURES,0.28035714285714286,"equal-size inputs, along the channel dimension, that are propagated forward according to Fig. 2b
135"
REVERSIBLE ARCHITECTURES,0.28214285714285714,"using an ad-hoc operator ˜Fj. It can be reconstructed by reverse propagating the output according to
136"
REVERSIBLE ARCHITECTURES,0.2839285714285714,"Fig. 2c, by subtracting the output of ˜Fj rather than adding it like in the previous forward.
137"
REVERSIBLE ARCHITECTURES,0.2857142857142857,"Reversible stages.
In order to compute the exact gradients during the backpropagation phase, each
138"
REVERSIBLE ARCHITECTURES,0.2875,"reversible stage needs to retrieve its output from the stage above. We note F −1
j
the reverse stage
139"
REVERSIBLE ARCHITECTURES,0.2892857142857143,"function, which reconstructs the input from the output. We recursively apply the reconstruction to the
140"
REVERSIBLE ARCHITECTURES,0.2910714285714286,"final activation xJ, such that
141"
REVERSIBLE ARCHITECTURES,0.29285714285714287,"
xj−1
δj"
REVERSIBLE ARCHITECTURES,0.29464285714285715,"
=

F −1
j
(xj, θj)
∂xFj(F −1
j
(xj, θj), θj)Tδj+1"
REVERSIBLE ARCHITECTURES,0.29642857142857143,"
.
(4)"
REVERSIBLE ARCHITECTURES,0.2982142857142857,"Note that reconstructing the input in our procedure is computationally equivalent to recomputing the
142"
REVERSIBLE ARCHITECTURES,0.3,"activations in activation checkpointing, meaning it is equivalent to a single forward pass. Thus, this
143"
REVERSIBLE ARCHITECTURES,0.30178571428571427,"augmented backward procedure is equivalent to one regular forward call and backward call. However,
144"
REVERSIBLE ARCHITECTURES,0.30357142857142855,"one should observe that since the input xj−1 must be sent to the reversible stages, this doubles the
145"
REVERSIBLE ARCHITECTURES,0.3053571428571429,"cost of backward communications.
146"
REVERSIBLE ARCHITECTURES,0.30714285714285716,"Non-reversible stages.
In practice, a reversible architecture includes layers that reduce dimension-
147"
REVERSIBLE ARCHITECTURES,0.30892857142857144,"ality for computational efficiency, which thus correspond to non-invertible functions. For those very
148"
REVERSIBLE ARCHITECTURES,0.3107142857142857,"few stages, we employ a buffer mechanism to store activations and, like activation checkpointing,
149"
REVERSIBLE ARCHITECTURES,0.3125,"we recompute the computational graph with a forward pass during the backward pass. Note that
150"
REVERSIBLE ARCHITECTURES,0.3142857142857143,"this would not be the case for invertible (i.e., bijective) architectures [18], which use an invertible
151"
REVERSIBLE ARCHITECTURES,0.31607142857142856,"downsampling.
152"
REVERSIBLE ARCHITECTURES,0.31785714285714284,"Table 1: Comparisons with other methods in an ideal setting for one stage. We compare several
methods to compute a gradient estimate in a model parallel setting. Here, J is the total number of
stages while j is the stage index. For the sake of simplicity, we assume that a backward pass requires
approximately 2 times more FLOPs than a forward pass. Full Graph indicates that it is required to
store the full computational graph of a local forward pass. With a limited increase in communication
volume and FLOPs, PETRA requires the least storage of all methods while being linearly faster than
backpropagation. We assume that the forward and backward passes can be executed in parallel for
PETRA or delayed gradients, making the backward pass responsible for most of the computation
time in parallelizable approaches."
REVERSIBLE ARCHITECTURES,0.3196428571428571,"Storage
Comm.
FLOPs
Mean time
Methods
Activations
Params.
Volume
per batch"
REVERSIBLE ARCHITECTURES,0.32142857142857145,"Backpropagation
Full Graph (FG)
1
1
3J
3J"
REVERSIBLE ARCHITECTURES,0.32321428571428573,"Reversible backprop. [12]
0
1
4
4J
4J"
REVERSIBLE ARCHITECTURES,0.325,"Delayed gradients [42]
2(J −j)× FG
2(J−j)"
REVERSIBLE ARCHITECTURES,0.3267857142857143,"k
1
3J
2
+ Checkpointing [40]
2(J −j)
1
1
4J
3"
REVERSIBLE ARCHITECTURES,0.32857142857142857,"PETRA (ours)
0
1
4
4J
3"
REVERSIBLE ARCHITECTURES,0.33035714285714285,"3.3
A parallelizable approach: PETRA
153"
REVERSIBLE ARCHITECTURES,0.33214285714285713,"As with any model parallel training technique, PETRA requires to partition the network architecture
154"
REVERSIBLE ARCHITECTURES,0.3339285714285714,"into stages Fj that are distributed across distinct devices. Each device j needs only to communicate
155"
REVERSIBLE ARCHITECTURES,0.3357142857142857,"with its neighboring devices j −1 and j + 1. The pseudo-code in Alg. 1 details the operations
156"
REVERSIBLE ARCHITECTURES,0.3375,"performed by each device, and the whole algorithm execution can be summarized as follows. The first
157"
REVERSIBLE ARCHITECTURES,0.3392857142857143,"device sequentially accesses mini-batches, initiating the data propagation process. When receiving its
158"
REVERSIBLE ARCHITECTURES,0.3410714285714286,"input xt
j−1 from the previous stage, each stage processes it in forward mode and passes it to the next
159"
REVERSIBLE ARCHITECTURES,0.34285714285714286,"stage, until the final stage is reached. The final stage evaluates the loss and computes the gradients
160"
REVERSIBLE ARCHITECTURES,0.34464285714285714,"with regard to its input and parameters, thus initiating the backward process, which is performed in
161"
REVERSIBLE ARCHITECTURES,0.3464285714285714,"parallel of the forward process. In it, each stage processes the input and its associated gradient from
162"
REVERSIBLE ARCHITECTURES,0.3482142857142857,"the next stage. This means first reconstructing the computational graph, either while reconstructing
163"
REVERSIBLE ARCHITECTURES,0.35,"the input ˜xt
j−1 for reversible stages or with a forward pass as in activation checkpointing otherwise.
164"
REVERSIBLE ARCHITECTURES,0.3517857142857143,"Then, the parameter gradient approximation ∆t+1
j
and the input gradient are computed before passing
165"
REVERSIBLE ARCHITECTURES,0.3535714285714286,"the latter to the previous stage. For intermediary reversible stages, this translates into the following
166"
REVERSIBLE ARCHITECTURES,0.35535714285714287,"equations, where t corresponds to the current time step of the training,
167"
REVERSIBLE ARCHITECTURES,0.35714285714285715,"







"
REVERSIBLE ARCHITECTURES,0.35892857142857143,"






"
REVERSIBLE ARCHITECTURES,0.3607142857142857,"xt+1
j
= Fj(xt
j−1, θt
j)"
REVERSIBLE ARCHITECTURES,0.3625,"˜xt+1
j−1 = F −1
j
(˜xt
j, θt
j)"
REVERSIBLE ARCHITECTURES,0.36428571428571427,"δt+1
j
= ∂xFj(˜xt+1
j−1, θt
j)Tδt
j+1
∆t+1
j
= ∂θFj(˜xt+1
j−1, θt
j)Tδt
j+1
θt+1
j
= Optimizert
j(θt
j, ∆t+1
j
) . (5)"
REVERSIBLE ARCHITECTURES,0.36607142857142855,"Note that this complete set of equations effectively decouples communications, computations, and
168"
REVERSIBLE ARCHITECTURES,0.3678571428571429,"parameter updates between independent devices. Indeed, reversible stages are able to operate without
169"
REVERSIBLE ARCHITECTURES,0.36964285714285716,"maintaining any state between the forward and corresponding backward phase by simply avoiding
170"
REVERSIBLE ARCHITECTURES,0.37142857142857144,"weight stashing, similarly to [40], and by reversing the output into the input during the backward
171"
REVERSIBLE ARCHITECTURES,0.3732142857142857,"phase, removing the need for an input buffer. As parameters are updated between the forward and
172"
REVERSIBLE ARCHITECTURES,0.375,"backward phases, the reversible stage produces an approximate input reconstruction, thus evaluating
173"
REVERSIBLE ARCHITECTURES,0.3767857142857143,"gradients with an approximate set of inputs and parameters during the backward phase. We illustrate
174"
REVERSIBLE ARCHITECTURES,0.37857142857142856,"in Fig. 3 the mechanism of PETRA compared to standard delayed gradient approaches that rely on
175"
REVERSIBLE ARCHITECTURES,0.38035714285714284,"additional buffers [44, 42].
176"
REVERSIBLE ARCHITECTURES,0.3821428571428571,"Complexity analysis.
We now discuss the benefits of our method, which are summarized in Tab. 1.
177"
REVERSIBLE ARCHITECTURES,0.38392857142857145,"In this discussion, we assume a homogeneous setting in which almost identical stages are distributed
178"
REVERSIBLE ARCHITECTURES,0.38571428571428573,"across J devices uniformly. First, we consider the backpropagation setting, assuming a model
179 Loss"
REVERSIBLE ARCHITECTURES,0.3875,"Buffer
size 2"
REVERSIBLE ARCHITECTURES,0.3892857142857143,Stage 0
REVERSIBLE ARCHITECTURES,0.39107142857142857,Stage 1
REVERSIBLE ARCHITECTURES,0.39285714285714285,Stage 2
REVERSIBLE ARCHITECTURES,0.39464285714285713,"Buffer
size 4"
REVERSIBLE ARCHITECTURES,0.3964285714285714,"Buffer
size 6 Loss"
REVERSIBLE ARCHITECTURES,0.3982142857142857,"Forward connection
(activations)"
REVERSIBLE ARCHITECTURES,0.4,"Backward connection
(reconstructed activations)"
REVERSIBLE ARCHITECTURES,0.4017857142857143,"Backward connection
(gradients)"
REVERSIBLE ARCHITECTURES,0.4035714285714286,Gradient computation
REVERSIBLE ARCHITECTURES,0.40535714285714286,a) Delayed
REVERSIBLE ARCHITECTURES,0.40714285714285714,Gradients
REVERSIBLE ARCHITECTURES,0.4089285714285714,b) PETRA
REVERSIBLE ARCHITECTURES,0.4107142857142857,"Figure 3: Comparison of our PETRA method to a standard Delayed Gradient method [42]. By
avoiding weight stashing and reversing the output into the input during the backward phase, we are
able to fully decouple the forward and backward phases in all reversible stages, with no memory
overhead, compared to standard delayed gradient approaches."
REVERSIBLE ARCHITECTURES,0.4125,"parallelism strategy: a standard backpropagation pass requires storing locally both the parameters and
180"
REVERSIBLE ARCHITECTURES,0.4142857142857143,"the computational graph and due to the update lock of backpropagation [20], requires synchronization
181"
REVERSIBLE ARCHITECTURES,0.4160714285714286,"between subsequent layers which impede the speed of computations. Standard Delayed Gradients
182"
REVERSIBLE ARCHITECTURES,0.41785714285714287,"strategies as implemented in [44, 42] allow to unlock this barrier, but they require buffers for storing
183"
REVERSIBLE ARCHITECTURES,0.41964285714285715,"both the computational graph and parameters which can become impractical when using large models.
184"
REVERSIBLE ARCHITECTURES,0.42142857142857143,"In [40], an activation checkpointing strategy removes the need for storing parameters, yet it requires
185"
REVERSIBLE ARCHITECTURES,0.4232142857142857,"a small computational overhead of 33% (assuming a backward pass is approximatively two times
186"
REVERSIBLE ARCHITECTURES,0.425,"slower than a forward pass, see Fig. 6 of [17] and [30]). To avoid storing activations, we rely on
187"
REVERSIBLE ARCHITECTURES,0.42678571428571427,"reversible architectures [12] which increases the amount of forward communications by a factor of 2
188"
REVERSIBLE ARCHITECTURES,0.42857142857142855,"and backward communication by a factor of 4 – activations sizes double and one has to pass both
189"
REVERSIBLE ARCHITECTURES,0.4303571428571429,"activations and gradients at the same time during backward. None of the aforementioned methods
190"
REVERSIBLE ARCHITECTURES,0.43214285714285716,"scale with the depth J: PETRA combines all the advantages of the previous methods, allowing an
191"
REVERSIBLE ARCHITECTURES,0.43392857142857144,"efficient parallelization, while leading to a limited overhead in computations and communications.
192"
NUMERICAL EXPERIMENTS,0.4357142857142857,"4
Numerical experiments
193"
CLASSIFICATION ACCURACY,0.4375,"4.1
Classification accuracy
194"
CLASSIFICATION ACCURACY,0.4392857142857143,"We now describe our experimental setup on CIFAR-10 [24], ImageNet-32 [7], and ImageNet [8].
195"
CLASSIFICATION ACCURACY,0.44107142857142856,"Experimental setup.
All our experiments use a standard SGD optimizer with a Nesterov momen-
196"
CLASSIFICATION ACCURACY,0.44285714285714284,"tum factor of 0.9. We train all models for 300 epochs on CIFAR-10 and 90 epochs on ImageNet32
197"
CLASSIFICATION ACCURACY,0.4446428571428571,"and ImageNet. We apply standard data augmentation, including horizontal flip, random cropping,
198"
CLASSIFICATION ACCURACY,0.44642857142857145,"and standard normalization but we do not follow the more involved training settings of [39], which
199"
CLASSIFICATION ACCURACY,0.44821428571428573,"potentially leads to higher accuracy. We perform a warm-up of 5 epochs where the learning rate
200"
CLASSIFICATION ACCURACY,0.45,"linearly increases from 0 to 0.1, following [13]. Then, the learning rate is decayed by a factor of 0.1
201"
CLASSIFICATION ACCURACY,0.4517857142857143,"at epochs 30, 60, and 80 for ImageNet32 and ImageNet – it is decayed at epochs 150 and 225 for
202"
CLASSIFICATION ACCURACY,0.45357142857142857,"CIFAR-10. We use a weight decay of 5e-4 for CIFAR-10 and 1e-4 for ImageNet32 and ImageNet. As
203"
CLASSIFICATION ACCURACY,0.45535714285714285,"suggested in [13], we do not apply weight decay on the batch norm learnable parameters and biases
204"
CLASSIFICATION ACCURACY,0.45714285714285713,"of affine and convolutional layers. For our standard backpropagation experiments, we follow the
205"
CLASSIFICATION ACCURACY,0.4589285714285714,"standard practice and use a batch size of 128 on ImageNet32 and CIFAR-10, and 256 on ImageNet32.
206"
CLASSIFICATION ACCURACY,0.4607142857142857,"However, we made a few adaptations to train our models with PETRA. As suggested by [42, 43], we
207"
CLASSIFICATION ACCURACY,0.4625,"employ an accumulation factor k and a batch size of 64, which allows to reduce the effective staleness
208"
CLASSIFICATION ACCURACY,0.4642857142857143,"during training: in this case, k batches of data must be successively processed before updating the
209"
CLASSIFICATION ACCURACY,0.4660714285714286,"parameters of a stage (see Alg. 1). Such gradient accumulation however also increases the effective
210"
CLASSIFICATION ACCURACY,0.46785714285714286,"batch size, and we apply the training recipe used in [13] to adjust the learning rate; note that we use
211"
CLASSIFICATION ACCURACY,0.46964285714285714,"the average of the accumulated gradients instead of the sum. The base learning rate is thus given by
212"
CLASSIFICATION ACCURACY,0.4714285714285714,the formula lr = 0.1 64k
CLASSIFICATION ACCURACY,0.4732142857142857,"256, with k the accumulation factor.
213"
CLASSIFICATION ACCURACY,0.475,"Algorithm 1 Worker perspective for training in parallel with PETRA, on a stage j, assuming
initialized parameters θj and time step t, as well as an accumulation factor k > 1."
CLASSIFICATION ACCURACY,0.4767857142857143,"1: In parallel on the j-th stage, 1 ≤j < J, perform:
2:
Forward Communications and Computations:
3:
If j = 1 then
4:
x0 ←Readdataset
5:
Else
6:
xj−1 ←Wait and Receive from j−1
7:
If stage j is not reversible :
8:
Bufferj ←xj
9:
xj ←Fj(xj−1, θj)
10:
Send to j+1(xj)
11:
Backward Communications and Computations:
12:
(˜xj, δj+1) ←Wait and Receive from j+1
13:
If stage j is reversible:
14:
˜xj−1 ←F −1
j
(˜xj, θj) and keep computational graph in memory
15:
Else :
16:
˜xj−1 ←Bufferj
17:
xj ←Fj(˜xj−1, θj) to recompute the computational graph
18:
δj ←∂xFj(˜xj−1, θj)T δj+1
19:
∆j ←∆j + 1"
CLASSIFICATION ACCURACY,0.4785714285714286,"k∂θFj(˜xj−1, θj)T δj+1
20:
If t mod k = 0 then:
21:
Update parameters θj with ∆j
22:
∆j ←0
23:
t ←t + 1
24:
Send to j−1(xj, δj)
25:
26: In parallel on the final stage J, perform:
27:
xJ−1 ←Wait and Receive from J−1
28:
L ←FJ(xJ−1, θJ)
29:
δJ ←∇xJL
30:
∆J ←∆J + 1"
CLASSIFICATION ACCURACY,0.48035714285714287,"k∇θJL
31:
If t mod k = 0 then:
32:
Update parameters θJ with ∆J
33:
∆J ←0
34:
t ←t + 1
35:
Send to J−1(xJ−1, δJ)"
CLASSIFICATION ACCURACY,0.48214285714285715,"Model adaptations.
For designing our RevNet architectures, we adopt a methodology similar
214"
CLASSIFICATION ACCURACY,0.48392857142857143,"to [12]: the number of channels in each stage is multiplied by 2 to account for the second data
215"
CLASSIFICATION ACCURACY,0.4857142857142857,"stream according to Fig. 2. However, as the stage function ˜Fj operates only on one of the two
216"
CLASSIFICATION ACCURACY,0.4875,"streams, the number of parameters stays almost the same between a residual block and its revertible
217"
CLASSIFICATION ACCURACY,0.48928571428571427,"counterpart. Consequently, the DNNs are split to preserve each residual block, resulting in 10 stages
218"
CLASSIFICATION ACCURACY,0.49107142857142855,"for RevNet18, and 18 stages for RevNet34 and RevNet50; thus varying the level of staleness between
219"
CLASSIFICATION ACCURACY,0.4928571428571429,"configurations. On CIFAR-10, the input layer uses 3x3 convolutions instead of 7x7 convolutions and
220"
CLASSIFICATION ACCURACY,0.49464285714285716,"does not perform max-pooling. The running statistics of batch normalization layers are updated when
221"
CLASSIFICATION ACCURACY,0.49642857142857144,"recomputing the activations during the backward pass and are then used during model evaluation –
222"
CLASSIFICATION ACCURACY,0.4982142857142857,"the running statistics are not updated during the forward pass.
223"
CLASSIFICATION ACCURACY,0.5,"Performance comparison.
Tab. 2 reports our numerical accuracy on several vision datasets,
224"
CLASSIFICATION ACCURACY,0.5017857142857143,"comparing a backpropagation performance from an official PyTorch implementation of ResNets
225"
CLASSIFICATION ACCURACY,0.5035714285714286,"(the numbers can be found as v1 of https://pytorch.org/hub/pytorch_vision_resnet/),
226"
CLASSIFICATION ACCURACY,0.5053571428571428,"for our own implementation of ResNets and RevNets in our custom computational framework, and
227"
CLASSIFICATION ACCURACY,0.5071428571428571,"our proposed method, PETRA. For PETRA, we report the best classification accuracy after the last
228"
CLASSIFICATION ACCURACY,0.5089285714285714,"learning rate drop, using the best value (picked on the training set) of accumulation steps within
229"
CLASSIFICATION ACCURACY,0.5107142857142857,"{1, 2, 4, 8, 16, 32}. Our CIFAR-10 accuracies are averaged over 3 runs, with a variance smaller than
230"
CLASSIFICATION ACCURACY,0.5125,"Table 2: Classification accuracies using our PETRA method with RevNets, compared to standard
backpropagation on ResNets and RevNets on CIFAR-10, ImageNet32, and ImageNet. Our method
delivers competitive results with backpropagation, even on ImageNet."
CLASSIFICATION ACCURACY,0.5142857142857142,"Method
Model
Param. count
CIFAR-10
ImNet32
ImNet"
CLASSIFICATION ACCURACY,0.5160714285714286,"Backprop
ResNet18 (PyTorch)
11.7M
-
-
69.8
Backprop
ResNet18 (Ours)
11.7M
95.0
54.0
70.8
Backprop
RevNet18 (Ours)
12.2M
94.9
54.6
70.8
PETRA
RevNet18 (Ours)
12.2M
94.9
54.6
71.0"
CLASSIFICATION ACCURACY,0.5178571428571429,"Backprop
ResNet34 (PyTorch)
21.8M
-
-
73.3
Backprop
ResNet34 (Ours)
21.8M
95.5
56.5
74.0
Backprop
RevNet34 (Ours)
22.3M
95.3
56.4
73.2
PETRA
RevNet34 (Ours)
22.3M
94.8
56.1
73.5"
CLASSIFICATION ACCURACY,0.5196428571428572,"Backprop
ResNet50 (PyTorch)
25.6M
-
-
76.1
Backprop
ResNet50 (Ours)
25.6M
94.8
58.8
75.6
Backprop
RevNet50 (Ours)
30.4M
95.2
59.7
75.4
PETRA
RevNet50 (Ours)
30.4M
94.5
59.6
74.8"
CLASSIFICATION ACCURACY,0.5214285714285715,"0.1. We observe that while our reversible models have about the same parameter count, they all
231"
CLASSIFICATION ACCURACY,0.5232142857142857,"perform in the same range of accuracy as their non-reversible counterparts. Only the RevNet-50
232"
CLASSIFICATION ACCURACY,0.525,"leads to a small drop in accuracy on ImageNet of about 0.6%: using different downsampling layers
233"
CLASSIFICATION ACCURACY,0.5267857142857143,"removes this gap at the expense of a substantial increase in the parameter count (30.4M to 50M).
234"
CLASSIFICATION ACCURACY,0.5285714285714286,"However, we decided not to include this result for the sake of comparison with respect to the original
235"
CLASSIFICATION ACCURACY,0.5303571428571429,"ResNets.
236"
CLASSIFICATION ACCURACY,0.5321428571428571,"Impact of the accumulation k.
We test the impact of the accumulation on a RevNet-18 trained
237"
CLASSIFICATION ACCURACY,0.5339285714285714,"via PETRA for various values of accumulations with k spanning {1, 2, 4, 8, 16, 32} on the ImageNet
238"
CLASSIFICATION ACCURACY,0.5357142857142857,"dataset. Fig. 4 indicates that our method can benefit from large accumulation factors, with the
239"
CLASSIFICATION ACCURACY,0.5375,"well-known trade-off of large batches mentioned in [13]. Increasing the accumulation factor reduces
240"
CLASSIFICATION ACCURACY,0.5392857142857143,"the effective staleness during training, and closes the performance gap with standard backpropagation
241"
CLASSIFICATION ACCURACY,0.5410714285714285,"with perfect matching for k = 32. This confirms that this large-batch training recipe derived for
242"
CLASSIFICATION ACCURACY,0.5428571428571428,"synchronous data parallelism is also particularly suited for our model parallel approach.
243"
TECHNICAL DETAILS,0.5446428571428571,"4.2
Technical details
244"
TECHNICAL DETAILS,0.5464285714285714,"A note on the implementation.
We shortly describe our implementation details. We base our
245"
TECHNICAL DETAILS,0.5482142857142858,"method on PyTorch [2], although we require significant modifications to the Autograd framework
246"
TECHNICAL DETAILS,0.55,"in order to manage delayed first-order quantities consistently with PETRA. We rely heavily on
247"
TECHNICAL DETAILS,0.5517857142857143,"the Vector Jacobian Product of PyTorch to compute gradients during the backward pass of each
248"
TECHNICAL DETAILS,0.5535714285714286,"stage, but other backends could be used. The backward pass for reversible stages only necessitates a
249"
TECHNICAL DETAILS,0.5553571428571429,"reconstruction step and a backward step – a naive implementation would use a reconstruction step,
250"
TECHNICAL DETAILS,0.5571428571428572,"followed by a forward and a backward step. This is because we only need the output gradient as well
251"
TECHNICAL DETAILS,0.5589285714285714,"as the computational graph of ˜Fj to compute the input and parameter gradients at line 12 and 13 of
252"
TECHNICAL DETAILS,0.5607142857142857,"Alg. 1, which can be obtained during the input reconstruction phase. For non-reversible stages, we
253"
TECHNICAL DETAILS,0.5625,"reconstruct the computational graph with a forward pass on the input retrieved from the buffer during
254"
TECHNICAL DETAILS,0.5642857142857143,"the backward pass. Our models can run on a single A100, 80GB.
255"
TECHNICAL DETAILS,0.5660714285714286,"Memory benefits and training time.
To better understand the advantage of our method compared
256"
TECHNICAL DETAILS,0.5678571428571428,"to other delayed gradient approaches [14, 40, 23], we emphasize the practical memory savings
257"
TECHNICAL DETAILS,0.5696428571428571,"associated with different methods in Tab. 3. We estimate the memory needed in gigabytes, as the
258"
TECHNICAL DETAILS,0.5714285714285714,"sum of the model size, the input buffer size, and the parameter buffer size, while excluding the input
259"
TECHNICAL DETAILS,0.5732142857142857,"buffer size of the first stage, which corresponds to retrievable dataset inputs. We do not include the
260"
TECHNICAL DETAILS,0.575,"effect of gradient accumulation since it depends on the value of k and only affects the length of
261"
TECHNICAL DETAILS,0.5767857142857142,"the parameter buffer, which is small in our case, i.e., we use k = 1. Note that the batch size also
262"
TECHNICAL DETAILS,0.5785714285714286,"affects the memory savings, and we set it to 64 for consistency with Tab. 2. Storing both inputs
263"
TECHNICAL DETAILS,0.5803571428571429,"and parameters into a buffer corresponds to the PipeDream approach [14]. Only storing inputs into
264"
TECHNICAL DETAILS,0.5821428571428572,"0
5
10
15
20
25
30
Accumulation steps 60.0 62.5 65.0 67.5 70.0 72.5 75.0 77.5 80.0"
TECHNICAL DETAILS,0.5839285714285715,Validation accuracy
TECHNICAL DETAILS,0.5857142857142857,"Backprop
PETRA"
TECHNICAL DETAILS,0.5875,"Figure 4: Validation accuracy of PETRA and backpropagation for a various number of accu-
mulation steps, for a RevNet18 trained on ImageNet with k ∈{1, 2, 4, 8, 16, 32}. The validation
accuracies are averaged over the last 10 epochs. As the number of accumulation steps increases, the
effective staleness in PETRA decreases, closing the gap with standard backpropagation."
TECHNICAL DETAILS,0.5892857142857143,"Table 3: Memory savings for RevNet50 on ImageNet with our method for different configura-
tions. We indicate the use of memory buffers for inputs or parameters. The savings are computed
with respect to the first configuration, where inputs and buffers are stored. Our method achieves
54.3% memory reduction over the base configuration of Delayed Gradients."
TECHNICAL DETAILS,0.5910714285714286,"Buffer
Memory (GB)
Saving (%)
Input
Params.
√
√
44.5
0.0
√
×
43.6
2.0
×
√
21.2
52.3
×
×
20.3
54.3"
TECHNICAL DETAILS,0.5928571428571429,"buffers would correspond to the approach in [40, 23]. The third and fourth lines are only applicable
265"
TECHNICAL DETAILS,0.5946428571428571,"to reversible architectures as they do not store the input into buffers. As can be seen, the input buffer
266"
TECHNICAL DETAILS,0.5964285714285714,"has the biggest impact on the total memory needed, being responsible for 52.3% of the memory
267"
TECHNICAL DETAILS,0.5982142857142857,"footprint. Dropping the parameter buffer in PETRA pushes the memory savings further to 54.3% for
268"
TECHNICAL DETAILS,0.6,"a RevNet50 on ImageNet. Note that non-reversible stages account for the majority of total memory
269"
TECHNICAL DETAILS,0.6017857142857143,"use, meaning that savings would be much higher for fully invertible architectures.
270"
CONCLUSION,0.6035714285714285,"5
Conclusion
271"
CONCLUSION,0.6053571428571428,"In this work, we introduce PETRA, a novel model parallel training technique for reversible ar-
272"
CONCLUSION,0.6071428571428571,"chitectures which is a novel promising alternative to backpropagation. It achieves a significant
273"
CONCLUSION,0.6089285714285714,"parallelization with a limited overhead compared to standard backpropagation or other competitive
274"
CONCLUSION,0.6107142857142858,"alternatives to end-to-end training, like delayed gradients approaches. Our method has the potential
275"
CONCLUSION,0.6125,"to achieve linear speedup compared to standard backpropagation and allows reversible layers to
276"
CONCLUSION,0.6142857142857143,"operate without any parameter or activation buffers, effectively decoupling the forward and backward
277"
CONCLUSION,0.6160714285714286,"phases. Despite using an approximate delayed gradient estimate, our method delivers competitive
278"
CONCLUSION,0.6178571428571429,"performances compared to standard backpropagation on standard computer vision datasets.
279"
CONCLUSION,0.6196428571428572,"In future work, we aim to implement and optimize PETRA for Large Language Models (LLMs),
280"
CONCLUSION,0.6214285714285714,"with a first baseline being Reformers [22], invertible transformers that have been shown to scale. This
281"
CONCLUSION,0.6232142857142857,"will validate PETRA’s effectiveness and robustness, solidifying its potential as a cutting-edge training
282"
CONCLUSION,0.625,"technique.
283"
REFERENCES,0.6267857142857143,"References
284"
REFERENCES,0.6285714285714286,"[1] I. M. Alabdulmohsin, B. Neyshabur, and X. Zhai. Revisiting neural scaling laws in language
285"
REFERENCES,0.6303571428571428,"and vision. Advances in Neural Information Processing Systems, 35:22300–22312, 2022.
286"
REFERENCES,0.6321428571428571,"[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard,
287"
REFERENCES,0.6339285714285714,"E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison,
288"
REFERENCES,0.6357142857142857,"W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos,
289"
REFERENCES,0.6375,"M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. K. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso,
290"
REFERENCES,0.6392857142857142,"M. Saroufim, M. Y. Siraichi, H. Suk, S. Zhang, M. Suo, P. Tillet, X. Zhao, E. Wang, K. Zhou,
291"
REFERENCES,0.6410714285714286,"R. Zou, X. Wang, A. Mathews, W. Wen, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster
292"
REFERENCES,0.6428571428571429,"machine learning through dynamic python bytecode transformation and graph compilation. In
293"
REFERENCES,0.6446428571428572,"Proceedings of the 29th ACM International Conference on Architectural Support for Program-
294"
REFERENCES,0.6464285714285715,"ming Languages and Operating Systems, Volume 2, ASPLOS ’24, page 929–947, New York,
295"
REFERENCES,0.6482142857142857,"NY, USA, 2024. Association for Computing Machinery.
296"
REFERENCES,0.65,"[3] E. Belilovsky, M. Eickenberg, and E. Oyallon. Greedy layerwise learning can scale to imagenet.
297"
REFERENCES,0.6517857142857143,"In International conference on machine learning, pages 583–593. PMLR, 2019.
298"
REFERENCES,0.6535714285714286,"[4] E. Belilovsky, M. Eickenberg, and E. Oyallon. Decoupled greedy learning of cnns. In Interna-
299"
REFERENCES,0.6553571428571429,"tional Conference on Machine Learning, pages 736–745. PMLR, 2020.
300"
REFERENCES,0.6571428571428571,"[5] E. Belilovsky, L. Leconte, L. Caccia, M. Eickenberg, and E. Oyallon. Decoupled greedy
301"
REFERENCES,0.6589285714285714,"learning of cnns for synchronous and asynchronous distributed learning.
arXiv preprint
302"
REFERENCES,0.6607142857142857,"arXiv:2106.06401, 2021.
303"
REFERENCES,0.6625,"[6] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost,
304"
REFERENCES,0.6642857142857143,"2016.
305"
REFERENCES,0.6660714285714285,"[7] P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an alternative
306"
REFERENCES,0.6678571428571428,"to the cifar datasets, 2017.
307"
REFERENCES,0.6696428571428571,"[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
308"
REFERENCES,0.6714285714285714,"image database. In 2009 IEEE conference on computer vision and pattern recognition, pages
309"
REFERENCES,0.6732142857142858,"248–255. Ieee, 2009.
310"
REFERENCES,0.675,"[9] L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation.
311"
REFERENCES,0.6767857142857143,"arXiv preprint arXiv:1410.8516, 2014.
312"
REFERENCES,0.6785714285714286,"[10] S. Fan, Y. Rong, C. Meng, Z. Cao, S. Wang, Z. Zheng, C. Wu, G. Long, J. Yang, L. Xia, et al.
313"
REFERENCES,0.6803571428571429,"Dapple: A pipelined data parallel approach for training large models. In Proceedings of the
314"
REFERENCES,0.6821428571428572,"26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages
315"
REFERENCES,0.6839285714285714,"431–445, 2021.
316"
REFERENCES,0.6857142857142857,"[11] L. Fournier, S. Rivaud, E. Belilovsky, M. Eickenberg, and E. Oyallon. Can forward gradient
317"
REFERENCES,0.6875,"match backpropagation? In Fortieth International Conference on Machine Learning, 2023.
318"
REFERENCES,0.6892857142857143,"[12] A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse. The reversible residual network: Back-
319"
REFERENCES,0.6910714285714286,"propagation without storing activations. Advances in neural information processing systems, 30,
320"
REFERENCES,0.6928571428571428,"2017.
321"
REFERENCES,0.6946428571428571,"[13] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia,
322"
REFERENCES,0.6964285714285714,"and K. He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint
323"
REFERENCES,0.6982142857142857,"arXiv:1706.02677, 2017.
324"
REFERENCES,0.7,"[14] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, and P. Gibbons.
325"
REFERENCES,0.7017857142857142,"Pipedream: Fast and efficient pipeline parallel dnn training. arXiv preprint arXiv:1806.03377,
326"
REFERENCES,0.7035714285714286,"2018.
327"
REFERENCES,0.7053571428571429,"[15] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu,
328"
REFERENCES,0.7071428571428572,"et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in
329"
REFERENCES,0.7089285714285715,"neural information processing systems, 32, 2019.
330"
REFERENCES,0.7107142857142857,"[16] Z. Huo, B. Gu, and H. Huang. Training neural networks using features replay. Advances in
331"
REFERENCES,0.7125,"Neural Information Processing Systems, 31, 2018.
332"
REFERENCES,0.7142857142857143,"[17] Z. Huo, B. Gu, H. Huang, et al. Decoupled parallel backpropagation with convergence guarantee.
333"
REFERENCES,0.7160714285714286,"In International Conference on Machine Learning, pages 2098–2106. PMLR, 2018.
334"
REFERENCES,0.7178571428571429,"[18] J.-H. Jacobsen, A. Smeulders, and E. Oyallon. i-revnet: Deep invertible networks. arXiv
335"
REFERENCES,0.7196428571428571,"preprint arXiv:1802.07088, 2018.
336"
REFERENCES,0.7214285714285714,"[19] J.-H. Jacobsen, A. W. M. Smeulders, and E. Oyallon. i-revnet: Deep invertible networks. ArXiv,
337"
REFERENCES,0.7232142857142857,"abs/1802.07088, 2018.
338"
REFERENCES,0.725,"[20] M. Jaderberg, W. M. Czarnecki, S. Osindero, O. Vinyals, A. Graves, D. Silver, and
339"
REFERENCES,0.7267857142857143,"K. Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In International
340"
REFERENCES,0.7285714285714285,"conference on machine learning, pages 1627–1635. PMLR, 2017.
341"
REFERENCES,0.7303571428571428,"[21] C. Kim, H. Lee, M. Jeong, W. Baek, B. Yoon, I. Kim, S. Lim, and S. Kim. torchgpipe: On-the-fly
342"
REFERENCES,0.7321428571428571,"pipeline parallelism for training giant models, 2020.
343"
REFERENCES,0.7339285714285714,"[22] N. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efficient transformer. arXiv preprint
344"
REFERENCES,0.7357142857142858,"arXiv:2001.04451, 2020.
345"
REFERENCES,0.7375,"[23] A. Kosson, V. Chiley, A. Venigalla, J. Hestness, and U. Koster. Pipelined backpropagation at
346"
REFERENCES,0.7392857142857143,"scale: training large models without batches. Proceedings of Machine Learning and Systems,
347"
REFERENCES,0.7410714285714286,"3:479–501, 2021.
348"
REFERENCES,0.7428571428571429,"[24] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
349"
REFERENCES,0.7446428571428572,"[25] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature, 521(7553):436–444, 2015.
350"
REFERENCES,0.7464285714285714,"[26] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke, J. Smith, B. Vaughan,
351"
REFERENCES,0.7482142857142857,"P. Damania, et al. Pytorch distributed: Experiences on accelerating data parallel training. arXiv
352"
REFERENCES,0.75,"preprint arXiv:2006.15704, 2020.
353"
REFERENCES,0.7517857142857143,"[27] Y. Liu, S. Li, J. Fang, Y. Shao, B. Yao, and Y. You. Colossal-auto: Unified automation of
354"
REFERENCES,0.7535714285714286,"parallelization and activation checkpoint for large-scale models, 2023.
355"
REFERENCES,0.7553571428571428,"[28] M. Malinowski, D. Vytiniotis, G. Swirszcz, V. Patraucean, and J. Carreira. Gradient forward-
356"
REFERENCES,0.7571428571428571,"propagation for large-scale temporal video modelling. In Proceedings of the IEEE/CVF Confer-
357"
REFERENCES,0.7589285714285714,"ence on Computer Vision and Pattern Recognition, pages 9249–9259, 2021.
358"
REFERENCES,0.7607142857142857,"[29] K. Mangalam, H. Fan, Y. Li, C.-Y. Wu, B. Xiong, C. Feichtenhofer, and J. Malik. Reversible
359"
REFERENCES,0.7625,"vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and
360"
REFERENCES,0.7642857142857142,"Pattern Recognition, pages 10830–10840, 2022.
361"
REFERENCES,0.7660714285714286,"[30] E. Mizutani and S. Dreyfus. On complexity analysis of supervised mlp-learning for algorithmic
362"
REFERENCES,0.7678571428571429,"comparisons. In IJCNN’01. International Joint Conference on Neural Networks. Proceedings
363"
REFERENCES,0.7696428571428572,"(Cat. No.01CH37222), volume 1, pages 347–352 vol.1, 2001.
364"
REFERENCES,0.7714285714285715,"[31] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B.
365"
REFERENCES,0.7732142857142857,"Gibbons, and M. Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In
366"
REFERENCES,0.775,"Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1–15, 2019.
367"
REFERENCES,0.7767857142857143,"[32] D. Narayanan, A. Phanishayee, K. Shi, X. Chen, and M. Zaharia. Memory-efficient pipeline-
368"
REFERENCES,0.7785714285714286,"parallel dnn training. In International Conference on Machine Learning, pages 7937–7947.
369"
REFERENCES,0.7803571428571429,"PMLR, 2021.
370"
REFERENCES,0.7821428571428571,"[33] A. Nøkland and L. H. Eidnes. Training neural networks with local error signals. In International
371"
REFERENCES,0.7839285714285714,"conference on machine learning, pages 4839–4850. PMLR, 2019.
372"
REFERENCES,0.7857142857142857,"[34] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training
373"
REFERENCES,0.7875,"trillion parameter models, 2020.
374"
REFERENCES,0.7892857142857143,"[35] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He.
375"
REFERENCES,0.7910714285714285,"{Zero-offload}: Democratizing {billion-scale} model training. In 2021 USENIX Annual
376"
REFERENCES,0.7928571428571428,"Technical Conference (USENIX ATC 21), pages 551–564, 2021.
377"
REFERENCES,0.7946428571428571,"[36] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm:
378"
REFERENCES,0.7964285714285714,"Training multi-billion parameter language models using model parallelism. arXiv preprint
379"
REFERENCES,0.7982142857142858,"arXiv:1909.08053, 2019.
380"
REFERENCES,0.8,"[37] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,
381"
REFERENCES,0.8017857142857143,"G. Zerveas, V. Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg
382"
REFERENCES,0.8035714285714286,"530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.
383"
REFERENCES,0.8053571428571429,"[38] Y. Wang, Z. Ni, S. Song, L. Yang, and G. Huang. Revisiting locally supervised learning: an
384"
REFERENCES,0.8071428571428572,"alternative to end-to-end training. arXiv preprint arXiv:2101.10832, 2021.
385"
REFERENCES,0.8089285714285714,"[39] R. Wightman, H. Touvron, and H. Jégou. Resnet strikes back: An improved training procedure
386"
REFERENCES,0.8107142857142857,"in timm, 2021.
387"
REFERENCES,0.8125,"[40] A. Xu, Z. Huo, and H. Huang. On the acceleration of deep learning model parallelism with
388"
REFERENCES,0.8142857142857143,"staleness. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
389"
REFERENCES,0.8160714285714286,"pages 2085–2094, 2019.
390"
REFERENCES,0.8178571428571428,"[41] B. Yang, J. Zhang, J. Li, C. Ré, C. Aberger, and C. De Sa. Pipemare: Asynchronous pipeline
391"
REFERENCES,0.8196428571428571,"parallel dnn training. Proceedings of Machine Learning and Systems, 3:269–296, 2021.
392"
REFERENCES,0.8214285714285714,"[42] H. Zhuang, Z. Lin, and K.-A. Toh. Accumulated decoupled learning: Mitigating gradient
393"
REFERENCES,0.8232142857142857,"staleness in inter-layer model parallelization. arXiv preprint arXiv:2012.03747, 2020.
394"
REFERENCES,0.825,"[43] H. Zhuang, Y. Wang, Q. Liu, and Z. Lin. Fully decoupled neural network learning using delayed
395"
REFERENCES,0.8267857142857142,"gradients. IEEE transactions on neural networks and learning systems, 33(10):6013–6020,
396"
REFERENCES,0.8285714285714286,"2021.
397"
REFERENCES,0.8303571428571429,"[44] H. Zhuang, Z. Weng, F. Luo, T. Kar-Ann, H. Li, and Z. Lin. Accumulated decoupled learn-
398"
REFERENCES,0.8321428571428572,"ing with gradient staleness mitigation for convolutional neural networks. In International
399"
REFERENCES,0.8339285714285715,"Conference on Machine Learning, pages 12935–12944. PMLR, 2021.
400"
REFERENCES,0.8357142857142857,"NeurIPS Paper Checklist
401"
CLAIMS,0.8375,"1. Claims
402"
CLAIMS,0.8392857142857143,"Question: Do the main claims made in the abstract and introduction accurately reflect the
403"
CLAIMS,0.8410714285714286,"paper’s contributions and scope?
404"
CLAIMS,0.8428571428571429,"Answer: [Yes]
405"
CLAIMS,0.8446428571428571,"Justification: Our novel method is presented in Sec. 3 as described. We provide numerical
406"
CLAIMS,0.8464285714285714,"accuracy and ablations in Sec. 4.
407"
LIMITATIONS,0.8482142857142857,"2. Limitations
408"
LIMITATIONS,0.85,"Question: Does the paper discuss the limitations of the work performed by the authors?
409"
LIMITATIONS,0.8517857142857143,"Answer: [Yes]
410"
LIMITATIONS,0.8535714285714285,"Justification: We discuss it along with our experimental results in Sec. 4.2.
411"
THEORY ASSUMPTIONS AND PROOFS,0.8553571428571428,"3. Theory Assumptions and Proofs
412"
THEORY ASSUMPTIONS AND PROOFS,0.8571428571428571,"Question: For each theoretical result, does the paper provide the full set of assumptions and
413"
THEORY ASSUMPTIONS AND PROOFS,0.8589285714285714,"a complete (and correct) proof?
414"
THEORY ASSUMPTIONS AND PROOFS,0.8607142857142858,"Answer: [NA]
415"
THEORY ASSUMPTIONS AND PROOFS,0.8625,"Justification: We do not provide theoretical results in this paper.
416"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8642857142857143,"4. Experimental Result Reproducibility
417"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8660714285714286,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
418"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8678571428571429,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
419"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8696428571428572,"of the paper (regardless of whether the code and data are provided or not)?
420"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8714285714285714,"Answer: [Yes]
421"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8732142857142857,"Justification: The experimental results are detailed in the article, and can be reproduced with
422"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.875,"the provided code.
423"
OPEN ACCESS TO DATA AND CODE,0.8767857142857143,"5. Open access to data and code
424"
OPEN ACCESS TO DATA AND CODE,0.8785714285714286,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
425"
OPEN ACCESS TO DATA AND CODE,0.8803571428571428,"tions to faithfully reproduce the main experimental results, as described in supplemental
426"
OPEN ACCESS TO DATA AND CODE,0.8821428571428571,"material?
427"
OPEN ACCESS TO DATA AND CODE,0.8839285714285714,"Answer: [Yes]
428"
OPEN ACCESS TO DATA AND CODE,0.8857142857142857,"Justification: The data is openly accessible and properly referenced in the text. The code is
429"
OPEN ACCESS TO DATA AND CODE,0.8875,"included in the supplementary material.
430"
OPEN ACCESS TO DATA AND CODE,0.8892857142857142,"6. Experimental Setting/Details
431"
OPEN ACCESS TO DATA AND CODE,0.8910714285714286,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
432"
OPEN ACCESS TO DATA AND CODE,0.8928571428571429,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
433"
OPEN ACCESS TO DATA AND CODE,0.8946428571428572,"results?
434"
OPEN ACCESS TO DATA AND CODE,0.8964285714285715,"Answer: [Yes]
435"
OPEN ACCESS TO DATA AND CODE,0.8982142857142857,"Justification: The hyperparameters are detailed in the article and the code.
436"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9,"7. Experiment Statistical Significance
437"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9017857142857143,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
438"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9035714285714286,"information about the statistical significance of the experiments?
439"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9053571428571429,"Answer: [Yes]
440"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9071428571428571,"Justification: We report accuracy averaged over 3 runs on CIFAR10 which have low variance,
441"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9089285714285714,"and we follow to the standard practice on ImageNet and ImageNet32.
442"
EXPERIMENTS COMPUTE RESOURCES,0.9107142857142857,"8. Experiments Compute Resources
443"
EXPERIMENTS COMPUTE RESOURCES,0.9125,"Question: For each experiment, does the paper provide sufficient information on the com-
444"
EXPERIMENTS COMPUTE RESOURCES,0.9142857142857143,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
445"
EXPERIMENTS COMPUTE RESOURCES,0.9160714285714285,"the experiments?
446"
EXPERIMENTS COMPUTE RESOURCES,0.9178571428571428,"Answer: [Yes]
447"
EXPERIMENTS COMPUTE RESOURCES,0.9196428571428571,"Justification: Details about resources and executions are presented Sec 4.2.
448"
CODE OF ETHICS,0.9214285714285714,"9. Code Of Ethics
449"
CODE OF ETHICS,0.9232142857142858,"Question: Does the research conducted in the paper conform, in every respect, with the
450"
CODE OF ETHICS,0.925,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
451"
CODE OF ETHICS,0.9267857142857143,"Answer: [Yes]
452"
CODE OF ETHICS,0.9285714285714286,"Justification: Our research conforms to the Code of Ethics.
453"
BROADER IMPACTS,0.9303571428571429,"10. Broader Impacts
454"
BROADER IMPACTS,0.9321428571428572,"Question: Does the paper discuss both potential positive societal impacts and negative
455"
BROADER IMPACTS,0.9339285714285714,"societal impacts of the work performed?
456"
BROADER IMPACTS,0.9357142857142857,"Answer: [NA]
457"
BROADER IMPACTS,0.9375,"Justification: Our article is concerned with allowing general deep learning optimization, and
458"
BROADER IMPACTS,0.9392857142857143,"does not have particular societal impacts.
459"
SAFEGUARDS,0.9410714285714286,"11. Safeguards
460"
SAFEGUARDS,0.9428571428571428,"Question: Does the paper describe safeguards that have been put in place for responsible
461"
SAFEGUARDS,0.9446428571428571,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
462"
SAFEGUARDS,0.9464285714285714,"image generators, or scraped datasets)?
463"
SAFEGUARDS,0.9482142857142857,"Answer: [NA]
464"
SAFEGUARDS,0.95,"Justification: This paper does not present such risks.
465"
LICENSES FOR EXISTING ASSETS,0.9517857142857142,"12. Licenses for existing assets
466"
LICENSES FOR EXISTING ASSETS,0.9535714285714286,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
467"
LICENSES FOR EXISTING ASSETS,0.9553571428571429,"the paper, properly credited and are the license and terms of use explicitly mentioned and
468"
LICENSES FOR EXISTING ASSETS,0.9571428571428572,"properly respected?
469"
LICENSES FOR EXISTING ASSETS,0.9589285714285715,"Answer: [Yes]
470"
LICENSES FOR EXISTING ASSETS,0.9607142857142857,"Justification: The datasets used are cited, and we credit the author of the code used for our
471"
LICENSES FOR EXISTING ASSETS,0.9625,"implementation.
472"
NEW ASSETS,0.9642857142857143,"13. New Assets
473"
NEW ASSETS,0.9660714285714286,"Question: Are new assets introduced in the paper well documented and is the documentation
474"
NEW ASSETS,0.9678571428571429,"provided alongside the assets?
475"
NEW ASSETS,0.9696428571428571,"Answer: [Yes]
476"
NEW ASSETS,0.9714285714285714,"Justification: The code will be provided with a README file describing how to run
477"
NEW ASSETS,0.9732142857142857,"experiments.
478"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975,"14. Crowdsourcing and Research with Human Subjects
479"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767857142857143,"Question: For crowdsourcing experiments and research with human subjects, does the paper
480"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785714285714285,"include the full text of instructions given to participants and screenshots, if applicable, as
481"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9803571428571428,"well as details about compensation (if any)?
482"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821428571428571,"Answer: [NA]
483"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9839285714285714,"Justification: No crowdsourcing or human subjects.
484"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857142857142858,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
485"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875,"Subjects
486"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892857142857143,"Question: Does the paper describe potential risks incurred by study participants, whether
487"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910714285714286,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
488"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928571428571429,"approvals (or an equivalent approval/review based on the requirements of your country or
489"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946428571428572,"institution) were obtained?
490"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964285714285714,"Answer: [NA]
491"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982142857142857,"Justification: No potential risks or study participants.
492"
