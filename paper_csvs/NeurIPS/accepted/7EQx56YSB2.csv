Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003236245954692557,"The increasing size of neural networks has led to a growing demand for methods of
efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm was introduced
that uses orthogonal matrices for adapting the weights of a pretrained model. In
this paper, we introduce a new class of structured matrices, which unifies and
generalizes structured classes from previous works. We examine properties of
this class and build a structured orthogonal parametrization upon it. We then use
this parametrization to modify the orthogonal fine-tuning framework, improving
parameter and computational efficiency. We empirically validate our method
on different domains, including adapting of text-to-image diffusion models and
downstream task fine-tuning in language modeling. Additionally, we adapt our
construction for orthogonal convolutions and conduct experiments with 1-Lipschitz
neural networks."
INTRODUCTION,0.006472491909385114,"1
Introduction"
INTRODUCTION,0.009708737864077669,"Orthogonal transforms have proven useful in different deep learning tasks. For example, they were
shown to stabilize CNNs [Li et al., 2019, Singla and Feizi, 2021] or used in RNNs to combat the
problem of exploding/vanishing gradients [Arjovsky et al., 2016]. Recent works OFT (Orthogonal
Fine-Tuning) and BOFT (Butterfly Orthogonal Fine-Tuning) [Qiu et al., 2023, Liu et al., 2024b] use
learnable orthogonal matrices for parameter-efficient fine-tuning of neural networks, which prevents
training instabilities and overfitting that alternative methods like LoRA [Hu et al., 2022] suffer from."
INTRODUCTION,0.012944983818770227,"Nevertheless, parametrization of orthogonal matrices is a challenging task, and the existing methods
typically lack in either computational efficiency or expressiveness. Classical methods like Cayley
parametrization and matrix exponential map cannot operate under low parameter budget, while
Givens rotations and Householder reflections requires computing products of several matrices, which
makes their use less efficient in deep learning tasks. Alternative approach in OFT method uses block-
diagonal matrix structure in an attempt to be more computationally efficient and use less trainable
parameters. Unfortunately, this simple structure can be too restrictive. Thus, arises the problem of
constructing dense orthogonal matrix while still being parameter-efficient. While attempting to tackle
this task, BOFT method uses a variation of butterfly matrices, parametrizing orthogonal matrices as a
product of several matrices with different sparsity patterns, enforcing orthogonality on each of them.
This parametrization is able to construct dense matrices while still being parameter-efficient. However
it requires to compute a product of multiple matrices (typically up to 6) which can be computationally
expensive. In this paper, we aim to overcome these issues and build dense orthogonal matrices in a
more efficient way."
INTRODUCTION,0.016181229773462782,"We present a novel structured matrix class parametrized by an alternating product of block-diagonal
matrices and several permutations. Multiplying by these matrices can be seen as a consecutive
application of independent linear transforms within certain small groups and then shuffling the
elements between them, hence the name Group-and-Shuffle matrices (or GS-matrices for short). This
class generalizes Monarch matrices [Dao et al., 2022] and with the right permutation choices, is
able to form dense orthogonal matrices more effectively compared to approach proposed in BOFT,
decreasing number of matrices in the product as well as the number of trainable parameters. We
build efficient structured orthogonal parametrization with this class and use it to construct a new
parameter-efficient fine-tuning method named GSOFT."
INTRODUCTION,0.019417475728155338,Our contributions:
INTRODUCTION,0.022653721682847898,"• We introduce a new class of structured matrices, called GS, that is more effective at forming
dense matrices than block butterfly matrices from the BOFT method.
• Using GS-matrices, we propose an efficient structured orthogonal parametrization, provide
theoretical insights and study its performance in the orthogonal fine-tuning framework.
• We adapt our ideas for convolutional architectures, providing a framework to compress and
speed-up orthogonal convolution layers."
ORTHOGONAL FINE-TUNING,0.025889967637540454,"2
Orthogonal Fine-tuning"
ORTHOGONAL FINE-TUNING,0.02912621359223301,"Orthogonal Fine-tuning method (OFT) introduced in [Qiu et al., 2023] is a Parameter-Efficient
Fine-Tuning (PEFT) method which fine-tunes pre-trained weight matrices through a learnable
orthogonal block-diagonal matrix. Some of the properties that make orthogonal transforms desirable
are preservation of pair-wise angles of neurons, spectral properties and hyperspherical energy. More
precisely, OFT optimizes an orthogonal matrix Q ∈Rd×d for a pre-trained frozen weight matrix
W 0 ∈Rd×n and modifies the multiplication y = (W 0)⊤x to y = (QW 0)⊤x. Note that the identity
matrix I is orthogonal, which makes it a natural initialization for Q. OFT uses block-diagonal
structure for Q, parameterizing it as
Q = diag(Q1, Q2, . . . , Qr),
where Qi ∈Rb×b are small orthogonal matrices and br = d. Orthogonality is enforced by Cayley
parametrization, i.e.
Qi = (I + Ki)(I −Ki)−1,
where Ki are skew-symmetric: Ki = −K⊤
i . This ensures orthogonality of Qi and, hence, of Q."
ORTHOGONAL FINE-TUNING,0.032362459546925564,"Nevertheless, block-diagonal matrices can be too restrictive, as they divide neurons into r independent
groups based on their indices. This motivates the construction of dense parameter-efficient orthogonal
matrices. To address this problem, the Orthogonal Butterfly method (BOFT) was introduced [Liu
et al., 2024b]. BOFT uses block-butterfly structure to construct Q. Essentially, Q is parameterized as
a product of m orthogonal sparse matrices:
Q = BmBm−1 . . . B1.
Each matrix Bi is a block-diagonal matrix up to a permutation of rows and columns, consisting
of r block matrices of sizes b × b. Similarly to OFT, the orthogonality is enforced by the Cayley
parametrization applied to each block. However, BOFT method has some areas for improvement as
well. To construct a dense matrix, BOFT requires at least
m = 1 + ⌈log2(r)⌉
matrices. For example, the authors of BOFT use m = 5 or 6 matrices in the BOFT method for
fine-tuning of Stable Diffusion [Rombach et al., 2022]. Large amount of stacked matrices leads to
significant time and memory overhead during training. There is also a room for improvement in terms
of parameter-efficiency. To overcome these issues, we introduce a new class of structured matrices
that we denote GS (group-and-shuffle) that generalizes Monarch matrices [Dao et al., 2022, Fu et al.,
2024] and show how to use this class to construct parameter-efficient orthogonal parametrization.
Similarly to BOFT, our approach uses block-diagonal matrices and permutations, but requires only
m = 1 + ⌈logb(r)⌉
matrices of the same size to construct a dense matrix. See details in Section 5.2. The reduced
requirements on m allow us to use m = 2 in experiments to maximize computational efficiency,
while still maintaining accurate results."
GS-MATRICES,0.03559870550161812,"3
GS-matrices"
GS-MATRICES,0.038834951456310676,Our motivation within this work is to utilize orthogonal matrices of the form:
GS-MATRICES,0.042071197411003236,"A = PL(LPR)PR
(1)"
GS-MATRICES,0.045307443365695796,"where matrices L and R are block-diagonal matrices with r blocks of sizes b × b and PL, P, PR
are certain permutation matrices, e.g. PL = P ⊤, PR = I in the orthogonal fine-tuning setting and
PR = P, PL = I for convolutional architectures. Note that although the case PL = P ⊤, PR = I
resembles Monarch matrices [Dao et al., 2022], they are unable to form such a structure, e.g., with
equal-sized blocks in L and R. The issue is that the Monarch class has a constraint that interconnects
the number of blocks in matrix L and the number of blocks in matrix R (see Appendix C for details).
Moreover, Monarch matrices have not been considered with orthogonality constraints."
GS-MATRICES,0.04854368932038835,"To build matrices of the form (1), we first introduce a general class of GS-matrices and study its
properties. We then discuss orthogonal matrices from this class in Section 4."
DEFINITION OF GS-MATRICES,0.05177993527508091,"3.1
Definition of GS-matrices"
DEFINITION OF GS-MATRICES,0.05501618122977346,"Definition 3.1. An m × n matrix A is in GS(PL, P, PR) class with kL, kR blocks and block sizes
b1
L × b2
L, b1
R × b2
R if
A = PL(LPR)PR,"
DEFINITION OF GS-MATRICES,0.05825242718446602,"where L = diag(L1, L2, . . . , LkL), Li ∈Rb1
L×b2
L, R = diag(R1, R2, . . . , RkR), Ri ∈Rb1
R×b2
R,
PL, P, PR are permutation matrices and b2
L · kL = b1
R · kR = s, b1
L · kL = m, b2
R · kR = n. L
P
R"
DEFINITION OF GS-MATRICES,0.061488673139158574,"x
Rx
P(Rx)
L(PRx)"
DEFINITION OF GS-MATRICES,0.06472491909385113,"Figure 1:
GS(I, P, I) matrices
with b1
L = b2
L = 3, b1
R = b2
R = 2,
kL = 2, kR = 3. Edges between
nodes denote nonzero weights."
DEFINITION OF GS-MATRICES,0.06796116504854369,"In practice, we fix PL, P, PR depending on the application
and only make matrices L, R subject for change. GS-matrices
are hardware-efficient, as they are parametrized by two simple
types of operations that can implemented efficiently: multipli-
cations by block-diagonal matrices and permutations."
DEFINITION OF GS-MATRICES,0.07119741100323625,"Let us also illustrate a forward pass Ax ≡LPRx for a matrix
A ∈GS(I, P, I) as a building block for the more general class
with two additional permutations. The first operation y = Rx
consists of several fully-connected layers, applied individually
to subgroups of x, see Figure 1. The next multiplication LPy
ensures that these groups interact with each other. Indeed,
the permutation matrix P shuffles the entries of y into new
subgroups. These subgroups are then again processed by a
number of fully-connected layers using L. This motivates the
naming for our class of matrices: Group-and-Shuffle or GS for
short."
DEFINITION OF GS-MATRICES,0.0744336569579288,"Another useful insight on these matrices is that the class
GS(I, P, I) consists of block matrices with low-rank blocks.
The permutation matrix P is responsible for the formation of
these blocks and defines their ranks (note that rank may vary
from block to block). The result below formally describes our findings and is key to the projection
operation that we describe afterwards."
DEFINITION OF GS-MATRICES,0.07766990291262135,"Proposition 1. Let A be a matrix from GS(I, P, I) with a permutation matrix P defined by the
function σ : {0, . . . , n −1} →{0, . . . , n −1}. Let {v⊤
i } – be the rows of the blocks R1, . . . , RkR,
{ui} – the columns of the blocks L1, . . . , LkL in the consecutive order. Then the matrix A can be
written as a block matrix with kL × kR blocks using the following formula for each block Ak1,k2:"
DEFINITION OF GS-MATRICES,0.08090614886731391,"Ak1,k2 =
X ⌊σ(i)"
DEFINITION OF GS-MATRICES,0.08414239482200647,"kL ⌋=k1
⌊
i
kR ⌋=k2"
DEFINITION OF GS-MATRICES,0.08737864077669903,"uσ(i)v⊤
i ."
DEFINITION OF GS-MATRICES,0.09061488673139159,Note that we use zero-indexing for this proposition for simplicity of formulas.
DEFINITION OF GS-MATRICES,0.09385113268608414,". . . + u2v⊺
4 = u2 v⊺
4"
DEFINITION OF GS-MATRICES,0.0970873786407767,"Figure 2: Illustration of Proposition 1 that provides block low-rank interpretation of GS(I, P, I)
matrices. The matrix R contains 2 blocks and matrix L contains 4 blocks."
DEFINITION OF GS-MATRICES,0.10032362459546926,"Let us illustrate this proposition in Figure 2. We consider GS(I, P, I) with kL = 4 and kR = 2
blocks in L and R and with the block sizes 3 × 3 and 6 × 6 respectively. Let us consider the leading
block A00 of the size 3 × 6. According to Proposition 1, A00 = u0v⊤
2 + u2v⊤
4 . Indeed, let us take a
closer look, e.g., at the term u2v⊤
4 . In the permutation matrix P, we have a nonzero element in the
position (2, 4) as i = 4 and σ(4) = 2. Therefore, we select the third column u2 in L1 and the fifth
row v⊤
4 in R1. This leads to adding a rank-one term u2v⊤
4 to A00 as we see in the formula above."
DEFINITION OF GS-MATRICES,0.10355987055016182,"Another direct corollary from Proposition 1 is a projection operation π: Rm×n →GS(PL, P, PR)
that satisfies:
π(A) ∈
arg min
B∈GS(PL,P ,PR)
∥A −B∥F ,"
DEFINITION OF GS-MATRICES,0.10679611650485436,"where ∥· ∥F is the Frobenius norm.
Thanks to the block low-rank representation of matri-
ces from GS(PL, P, PR), the projection π is simply constructed using SVD truncations of the
blocks (P ⊤
L AP ⊤
R )k1,k2 and is summarized in Algorithm 1."
DEFINITION OF GS-MATRICES,0.11003236245954692,"Algorithm 1 Projection π(·) of A onto GS(PL, P, PR)"
DEFINITION OF GS-MATRICES,0.11326860841423948,"Input: A, PL, P, PR
Return: L, R
for k1 = 1 . . . kL do"
DEFINITION OF GS-MATRICES,0.11650485436893204,for k2 = 1 . . . kR do
DEFINITION OF GS-MATRICES,0.11974110032362459,"Compute SVD of (P T
L AP T
R )k1,k2 = UΣV ⊤;
Set r = rk1,k2 – rank of block determined by P;
Take Ur = U[: r, :], Σr = Σ[: r, : r], Vr = V [: r, :];
Pack columns of UrΣ1/2
r
into Lk1 and rows of Σ1/2
r
Vr into Rk2 according to P;
end for
end for"
DEFINITION OF GS-MATRICES,0.12297734627831715,"4
Orthogonal GS(PL, P, PR) matrices"
DEFINITION OF GS-MATRICES,0.1262135922330097,"In this section, we study the orthogonality constraint for the GS(PL, P, PR) to obtain structured
orthogonal representation. This is one of the main contributions of our paper and we utilize this class
in all the numerical experiments. Since we are interested only in square orthogonal matrices, we
additionally assume that m = n and b1
L = b2
L = bL; b1
R = b2
R = bR. Similarly to parametrizations
in OFT and BOFT, a natural way to enforce orthogonality of GS(PL, P, PR)-matrices is to enforce
orthogonality of each block of L and R. This indeed leads an orthogonal matrix since permutation
matrices are also orthogonal as well as a product of orthogonal matrices. However, it is not immedi-
ately obvious that there exist no orthogonal matrices from GS(PL, P, PR) that cannot be represented
this way. Surprisingly, we find that such a way to enforce orthogonality is indeed sufficient for
covering of all orthogonal matrices from GS(PL, P, PR).
Theorem 1. Let A be any orthogonal matrix from GS(PL, P, PR). Then, A admits PL(LPR)PR
representation with the matrices L, R consisting of orthogonal blocks."
DEFINITION OF GS-MATRICES,0.12944983818770225,"Proof. Matrices PL, PR are orthogonal as they are permutation matrices. It means that it is sufficient
to prove theorem in the case when A is from GS(I, P, I), which means that we can use low block-"
DEFINITION OF GS-MATRICES,0.13268608414239483,"rank structure interpretation from Proposition 1. Consider a skeleton decomposition of the blocks
Aij = UijV ⊤
ij , Uij ∈RbL×rij, Vij ∈RbR×rij such that U ⊤
ij Uij = Irij (this can be ensured, e.g.,
using the QR decomposition). Then A =  
"
DEFINITION OF GS-MATRICES,0.13592233009708737,"U1,1V ⊤
1,1
. . .
U1,kLV ⊤
1,kL
...
...
...
UkL,1V ⊤
kL,1
. . .
UkL,kRV ⊤
kL,kR  
."
DEFINITION OF GS-MATRICES,0.13915857605177995,"Take the j-th block-column of A. Since A is an orthogonal matrix, we get:"
DEFINITION OF GS-MATRICES,0.1423948220064725," V1,jU ⊤
1,j
. . .
VkL,jU ⊤
kL,j
  
"
DEFINITION OF GS-MATRICES,0.14563106796116504,"U1,jV ⊤
1,j
...
UkL,jV ⊤
kL,j "
DEFINITION OF GS-MATRICES,0.1488673139158576,"
= IbR"
DEFINITION OF GS-MATRICES,0.15210355987055016,"Multiplying matrices in the l.h.s. we get V1,jU ⊤
1,jU1,jV ⊤
1,j+· · ·+VkL,jU ⊤
kL,jUkL,jV ⊤
kL,j = IbR. Since
U ⊤
ij Uij = Irij we conclude V1,jV ⊤
1,j + · · · + VkL,jV ⊤
kL,j = IbR. This implies that (V1,j
. . .
VkL,j)
is an orthogonal matrix. Note that if we now parameterize A = LPR with the matrices Vij packed
into R and Uij packed into L, then (V1,j
. . .
VkL,j) is exactly the j-th block matrix in R up to
permutation of rows. Therefore, every block in R is an orthogonal matrix. Since we now proved that
V ⊤
ij Vij = I, we can use same the derivation for the rows of A and conclude that blocks of L are also
orthogonal."
DEFINITION OF GS-MATRICES,0.1553398058252427,"5
GS(Pm+1, . . . , P1) matrices"
DEFINITION OF GS-MATRICES,0.15857605177993528,"In this section we describe an the extension of GS-matrices that uses more than two block-diagonal
matrices and show that with the right permutations choices GS-matrices are more effective than block
butterfly matrices in forming dense matrices. Here by dense matrices we imply matrices that do not
contain zero entries at all."
DEFINITION OF GS-MATRICES,0.16181229773462782,"Definition 5.1. A is said to be in GS(Pm+1, . . . , P1) if"
DEFINITION OF GS-MATRICES,0.1650485436893204,A = Pm+1
Y,0.16828478964401294,"1
Y"
Y,0.1715210355987055,"i=m
(BiPi),"
Y,0.17475728155339806,"where each matrix Bi is a block-diagonal matrix with ki blocks of size b1
i × b2
i , matrices Pi are
permutation matrices and b1
i · ki = b2
i+1 · ki+1."
Y,0.1779935275080906,"Remark 1. Similarly to the case m = 2 described in Section 3, we may use orthogonal blocks in Bi,
i = 1, . . . , m + 1 to obtain orthogonal matrices. However, it is not clear if an analog to Theorem 1
is correct in this case as well."
Y,0.18122977346278318,"Remark 2. For each of the classes of Block Butterfly matrices [Chen et al., 2022], Monarch
matrices [Dao et al., 2022] and order-p Monarch matrices [Fu et al., 2024], there exist permutation
matrices Pm+1, . . . , P1 such that GS(Pm+1, . . . P1) coincides with a respective class. Indeed,
Monarch matrices have the form of alternating block-diagonal matrices and permutations with some
specific size constraints and sparse matrices in the product of Block Butterfly matrices can be easily
transformed to block-diagonal matrices with permutations of rows and columns."
CHOOSING PERMUTATION MATRICES,0.18446601941747573,"5.1
Choosing permutation matrices"
CHOOSING PERMUTATION MATRICES,0.18770226537216828,"We suggest using the following matrices with k = ki for Pi. Note that this is efficient for forming
dense matrices as follows from the proof of Theorem 2. This is by contrast to the permutations used
in [Fu et al., 2024] that are restricted to particular matrix sizes."
CHOOSING PERMUTATION MATRICES,0.19093851132686085,"Definition 5.2 ([Dao et al., 2022]). Let P(k,n) be a permutation matrix given by permutation σ on
{0, 1, . . . , n −1}:"
CHOOSING PERMUTATION MATRICES,0.1941747572815534,σ(i) = (i mod k) · n
CHOOSING PERMUTATION MATRICES,0.19741100323624594,"k +
 i k 
."
CHOOSING PERMUTATION MATRICES,0.20064724919093851,Applying this permutation to a vector can be viewed as reshaping an input of size n into an k × n
CHOOSING PERMUTATION MATRICES,0.20388349514563106,"k
matrix in a row-major order, transposing it, and then vectorizing the result back into a vector (again
in row-major column). We provide several examples of such permutations in Figure 3."
CHOOSING PERMUTATION MATRICES,0.20711974110032363,"P(3,12)
P(4,12)
P(6,12)
P(2,12)"
CHOOSING PERMUTATION MATRICES,0.21035598705501618,"Figure 3: Illustraion of P(k,12) permutations for k ∈{3, 4, 6, 2}."
COMPARISON TO BLOCK BUTTERFLY MATRICES AND BOFT,0.21359223300970873,"5.2
Comparison to block butterfly matrices and BOFT"
COMPARISON TO BLOCK BUTTERFLY MATRICES AND BOFT,0.2168284789644013,"Block Butterfly matrices were introduced in [Chen et al., 2022] and are used to construct orthogonal
matrices in the BOFT method. Block Butterfly matrix class is a special case of higher-order GS-
matrices with ki = r and b1
i = b2
i = b = 2s and certain permutation choices. However, we
argue that the choice of these permutations are sub-optimal for construction of dense matrix and
using permutations from Definition 5.2 is more effective. When using block-diagonal matrices with
r blocks, block butterfly matrices need 1 + ⌈log2(r)⌉matrices to construct a dense matrix. For
GS-matrices we have the following result."
COMPARISON TO BLOCK BUTTERFLY MATRICES AND BOFT,0.22006472491909385,"Theorem 2. Let ki = r, b1
i = b2
i = b. Then using m = 1 + ⌈logb(r)⌉is sufficient for the class
GS(PL, P(r,br), . . . , P(r,br), PR) to form a dense matrix for any PL, PR. Moreover, the choice of
P2 = · · · = Pm = P(r,br) is optimal in the sense that all matrices from GS(Pm+1, . . . , P1) contain
zero blocks for any integer m < 1 + ⌈logb(r)⌉and any permutations P1, . . . , Pm+1."
COMPARISON TO BLOCK BUTTERFLY MATRICES AND BOFT,0.22330097087378642,Proof. See Appendix D.
COMPARISON TO BLOCK BUTTERFLY MATRICES AND BOFT,0.22653721682847897,"For example, let us consider a case of constructing a dense orthogonal matrix of the size 1024 × 1024.
Suppose also that we use block matrices with block size 32. Constructing a dense matrix with Block
Butterfly matrices requires 1 + log2(32) = 6 butterfly matrices, which leads to 6 × 323 parameters in
the representation. GS(PL, P, PR) matrices with P = P(32,1024) only need two matrices to construct
a dense matrix yielding 2 × 323 parameters. The GS(PL, P, PR) parametrization is also naturally
more computationally efficient as fewer number of multiplications is both faster and requires less
cached memory for activations."
APPLICATIONS,0.2297734627831715,"6
Applications"
APPLICATIONS,0.23300970873786409,"6.1
Orthogonal fine-tuning with GS(PL, P, PR) (GSOFT)"
APPLICATIONS,0.23624595469255663,"We utilize the pipeline of OFT and BOFT methods with the exception of parametrizing Q with
orthogonal permuted GS(PL, P, PR) matrices. In particular, for parametrization of Q ∈Rd×d,
we utilize the GS(P ⊤, P, I) class, i.e. Q = P ⊤LPR, where L = diag(L1, . . . Lr), Li ∈Rb×b,
R = diag(R1, . . . , Rr), Ri ∈Rb×b. For consistency, we use the same notation for the number of
blocks and block sizes as in BOFT and OFT methods. We use P(r,br) as a permutation matrix P. To
enforce orthogonality, we parameterize each block in matrices L, R with the Cayley parametrization.
We initialize Q as an identity matrix by initializing each block to be an identity matrix. Additional"
APPLICATIONS,0.23948220064724918,"techniques like magnitude scaling and multiplicative dropout that are used in OFT and BOFT can
be utilized the same way in our method, though we only use scaling in our experiments. Note
that likewise in OFT, BOFT weights of the matrix Q can be merged with the pretrained weight W
producing no inference overhead."
APPLICATIONS,0.24271844660194175,"6.2
Two-sided orthogonal fine-tuning (Double GSOFT)"
APPLICATIONS,0.2459546925566343,"Consider SVD decomposition of a matrix W 0 = UΣV ⊤. Applying orthogonal fine-tuning, we get
W ′ = (QU)ΣV ⊤, which is an SVD decomposition for the adapted weight W ′. This shows that we
can only change left singular vectors U with the standard orthogonal fine-tuning paradigm. At the
same time, the LoRA method modifies both matrices U and V . Moreover, recent papers [Meng et al.,
2024, Li et al., 2023] show that initializing matrices A, B with singular vectors can additionally boost
performance of LoRA. This motivates an extension of orthogonal fine-tuning method, that can adapt
both matrices U and V . We introduce a simple approach that multiplies pre-trained weight matrices
from both sides, rather than one. This method modifies forward pass from z = (W 0)⊤x to"
APPLICATIONS,0.24919093851132687,z = (QUW 0QV )⊤x
APPLICATIONS,0.2524271844660194,"Where QU and QV are parametrized as orthogonal GS-matrices. In cases where BOFT utilizes 5-6
matrices, we can leverage the fact that our method uses only 2 and adapt both sides while still using
less matrices and trainable parameters than BOFT."
GS ORTHOGONAL CONVOLUTIONS,0.255663430420712,"6.3
GS Orthogonal Convolutions"
GS ORTHOGONAL CONVOLUTIONS,0.2588996763754045,"Recall, that due to linearity of a multichannel convolution operation, we can express the convolution
of tensor X ∈Rcin×h×w with a kernel L ∈Rcout×cin×k×k L ⋆X in terms of matrix multiplication
[Singla and Feizi, 2021]:"
GS ORTHOGONAL CONVOLUTIONS,0.2621359223300971,"Y = L ⋆X
⇔
vec(Y ) =  "
GS ORTHOGONAL CONVOLUTIONS,0.26537216828478966,"L0,0
. . .
L0,cin−1
...
...
...
Lcout−1,0
. . .
Lcout−1,cin−1 "
GS ORTHOGONAL CONVOLUTIONS,0.2686084142394822,"vec(X),
(2)"
GS ORTHOGONAL CONVOLUTIONS,0.27184466019417475,"where Li,j is doubly Toeplitz matrix, corresponding to convolution between i-th and j-th channels
and vec(X) is a vectorization of tensor into a vector in a row-major order. Thus, the convolution
is essentially a block matrix, where each block represents a standard convolution operation. Using
this block interpretation (2), we may apply the concept of GS matrices to convolutional layers as
well. Considering each convolution between channels as an element of our block matrix, we can
set some of these blocks to zero, obtaining some additional structure. Thus, we can construct block
matrix which has block-diagonal structure, corresponding to grouped convolution (further, in all
equations we will denote it as GrConv). Then, defining ChShuffle as a permutation of channels,
like in [Zhang et al., 2018], we obtain structure, which is similar to GSOFT, defined in Section 6:"
GS ORTHOGONAL CONVOLUTIONS,0.2750809061488673,"Y = GrConv2(ChShuffle2(GrConv1(ChShuffle1(X)))).
(3)"
GS ORTHOGONAL CONVOLUTIONS,0.2783171521035599,"The proposed GS convolutional layer shuffles information between each pair of input channels and
requires less parameters and FLOPs during computations. In this example we can also choose
permutations of channels and change kernel size.
This convolutional layer can be treated as
GS(Pm+1, . . . , P1) matrix in vectorized view, that is why choosing permutations between con-
volutional layers is also very important for information transition properties. In Appendix F we
explain the choice of ChShuffle operation."
GS ORTHOGONAL CONVOLUTIONS,0.2815533980582524,"We can use the proposed layer to construct orthogonal convolutions (transformations with an or-
thogonal Jacobian matrix) similarly to skew orthogonal convolution (SOC) architecture, that uses
Taylor expansion of a matrix exponential. One major downside of methods such as SOC and BCOP
[Li et al., 2019] is that they require more time than basic convolution operation. For instance, in
the SOC method, one layer requires multiple applications of convolution (6 convolutions per layer).
In our framework, we propose a parametrization of a convolutional layer, in which imposing an
orthogonality to convolutions has fewer number of FLOPs and parameters thanks to the usage of
grouped convolutions."
GS ORTHOGONAL CONVOLUTIONS,0.284789644012945,"Let us discuss in more details how SOC works and the way we modify it. In SOC, a convolutional
filter is parametrized in the following way:"
GS ORTHOGONAL CONVOLUTIONS,0.28802588996763756,"L = M −ConvTranspose(M),"
GS ORTHOGONAL CONVOLUTIONS,0.2912621359223301,where M ∈Rcin×cout×r×s is an arbitrary kernel and the ConvTranspose is the following operation:
GS ORTHOGONAL CONVOLUTIONS,0.29449838187702265,"ConvTranspose(M)i,j,k,l = Mj,i,r−k−1,s−l−1"
GS ORTHOGONAL CONVOLUTIONS,0.2977346278317152,"This parametrization of filter L makes the matrix from Equation 2 skew-symmetric. As matrix
exponential of skew-symmetric matrix is an orthogonal matrix, in SOC the authors define convolution
exponential operation, which is equivalent to matrix exponential in matrix-vector notation:
Definition 6.1. [Singla and Feizi, 2021] Let X ∈Rc×h×w be an input tensor and L ∈Rc×c×k×k
be a convolution kernel. Then, define convolution exponential L ⋆e X as follows:"
GS ORTHOGONAL CONVOLUTIONS,0.30097087378640774,L ⋆e X = X + L ⋆X
GS ORTHOGONAL CONVOLUTIONS,0.3042071197411003,"1!
+ L ⋆2 X"
GS ORTHOGONAL CONVOLUTIONS,0.3074433656957929,"2!
+ . . ."
GS ORTHOGONAL CONVOLUTIONS,0.3106796116504854,where L ⋆i X is a convolution with kernel L applied i times consequently.
GS ORTHOGONAL CONVOLUTIONS,0.313915857605178,"As mentioned above, with proper initialization we get a convolutional layer with orthogonal Jacobian
matrix. Using the parametrization of convolution layer from the Equation 3 and substituting there
two grouped convolution exponentials (e.g. in our parametrization we have the same convolution
exponential, but we have grouped convolution instead of basic one) with the parameterized kernel:"
GS ORTHOGONAL CONVOLUTIONS,0.31715210355987056,Y = GrExpConv2(ChShuffle2(GrExpConv1(ChShuffle1(X))))
GS ORTHOGONAL CONVOLUTIONS,0.32038834951456313,"In our experiments we tried different layer architectures and we found that making kernel size of the
second convolution equal to 1 speeds up our convolutional layer, maintaining quality metrics. Thus,
if convolutional layer consists of two grouped convolutional exponentials, the second convolutional
exponential has kernel_size = 1 × 1"
EXPERIMENTS,0.32362459546925565,"7
Experiments"
EXPERIMENTS,0.3268608414239482,"All the experiments below were conducted on NVIDIA V100-SXM2-32Gb GPU. We ran all the
experiments within ∼2000 GPU hours."
EXPERIMENTS,0.3300970873786408,Source code is available at: https://github.com/Skonor/group_and_shuffle
NATURAL LANGUAGE UNDERSTANDING,0.3333333333333333,"7.1
Natural language understanding"
NATURAL LANGUAGE UNDERSTANDING,0.3365695792880259,"We report result on the GLUE [Wang et al., 2018] benchmark with RoBERTa-base [Liu et al., 2019]
model. Benchmark includes several classification tasks that evaluate general language understanding.
We follow training settings of [Liu et al., 2024b, Zhang et al., 2023]. We apply adapters for all linear
layers and only tune learning rate for all methods. Table 1 reports best results on the evaluation set
from the whole training. LoRA, OFT and BOFT are implemented with PEFT library [Mangrulkar
et al., 2022]. GSOFT method outperforms OFT, BOFT and also have a slight edge over LoRA.
Note that even though skew-symmetric K theoretically matrix only requires approximately half the
parameters of a full matrix, in practice it is parametrized as K = A−AT for the ease of computations.
However, after fine-tuning, one can only save upper-triangular part of K. Doing this, orthogonal
fine-tuning methods become approximately 2 times more efficient in terms of memory savings."
SUBJECT-DRIVEN GENERATION,0.33980582524271846,"7.2
Subject-driven generation"
SUBJECT-DRIVEN GENERATION,0.343042071197411,"Subject-driven generation [Ruiz et al., 2023, Gal et al., 2022] is an important and challenging task in
the field of generative modelling. Given several photos of a particular concept, we want to introduce
it to the diffusion model so that we can generate this particular object in different scenes described by
textual prompts. The main way to do this is to fine-tune the model. However, the large number of
fine-tuning parameters together with the lack of training images make the model prone to overfitting,
i.e. the model reconstructs the concept almost perfectly, but starts to ignore the textual prompt
during generation. To solve this problem and stabilize the fine-tuning process, different lightweight
parameterizations [Qiu et al., 2023, Liu et al., 2024b, Hu et al., 2022, Tewel et al., 2023, Han et al.,"
SUBJECT-DRIVEN GENERATION,0.34627831715210355,"Table 1: Results on GLUE benchmark with RoBERTa-base model. We report Pearson correlation for
STS-B, Matthew’s correlation for CoLA and accuracy for other tasks. # Params denotes number of
trainable parameters"
SUBJECT-DRIVEN GENERATION,0.34951456310679613,"Method
# Params
MNLI
SST-2
CoLA
QQP
QNLI
RTE
MRPC
STS-B
ALL
FT
125M
87.62
94.38
61.97
91.5
93.06
80.14
88.97
90.91
86.07
LoRAr=8
1.33M
87.82
95.07
64.02
90.97
92.81
81.95
88.73
90.84
86.53
OFTb=16
1.41M
87.21
95.07
64.37
90.6
92.48
79.78
89.95
90.71
86.27
BOFTm=2
b=8
1.42M
87.14
94.38
62.57
90.48
92.39
80.14
88.97
90.67
85.84
GSOFTb=8
1.42M
87.16
95.06
65.3
90.46
92.46
81.95
90.2
90.76
86.67"
SUBJECT-DRIVEN GENERATION,0.35275080906148865,"Table 2: Results on subject-driven generation. # Params denotes the number of training parameters in
each parametrization. Training time is computed for 3000 iterations on a single GPU V100 in hours."
SUBJECT-DRIVEN GENERATION,0.3559870550161812,"Model
Full
LoRA
BOFT
GSOFT (Ours)
Double GSOFT (Ours)"
SUBJECT-DRIVEN GENERATION,0.3592233009708738,"rank
r, m
r
r"
SUBJECT-DRIVEN GENERATION,0.36245954692556637,"4
32
128
32, 4
32, 6
16, 5
32
16
8
64
32
16"
SUBJECT-DRIVEN GENERATION,0.3656957928802589,"# Params
99.9M
0.8M
6.6M
26.6M
13.6M
20.4M
33.8M
6.8M
13.6M
27.1M
6.5M
13.0M
25.9M
Training time
1.3
1.3
1.3
1.3
2.0
2.2
2.3
1.5
1.6
1.8
1.7
2.0
1.8
CLIP-I ↑
0.805
0.805
0.819
0.813
0.803
0.796
0.789
0.805
0.803
0.783
0.815
0.802
0.783
CLIP-T ↑
0.212
0.246
0.236
0.223
0.244
0.234
0.223
0.256
0.245
0.227
0.256
0.242
0.225"
SUBJECT-DRIVEN GENERATION,0.36893203883495146,"2023] and regularization techniques [Ruiz et al., 2023, Kumari et al., 2023] are widely used in
this task. Therefore, we chose this setting to evaluate the effectiveness of the proposed orthogonal
parameterization compared to other approaches."
SUBJECT-DRIVEN GENERATION,0.37216828478964403,"We use StableDiffusion [Rombach et al., 2022] and the Dreambooth [Ruiz et al., 2023] dataset for all
our experiments. The following parameterizations were considered as baselines in this task: full (q, k,
v and out.0 layers in all cross- and self- attentions of the UNet are trained), LoRA [Hu et al., 2022]
and BOFT [Liu et al., 2024b] applied to the same layer. We use our GSOFT parameterization and a
two-sided orthogonal GSOFT (Double GSOFT) applied to the same layers as baselines. For a more
comprehensive comparison, we consider different hyperparameters for the models, adjusting the total
number of optimized parameters. More training and evaluation details can be found in Appendix E."
SUBJECT-DRIVEN GENERATION,0.37540453074433655,"CLIP image similarity, CLIP text similarity and visual comparison for this task are presented in
Table 2 and Figure 4. As the results show, GSOFT and DoubleGSOFT are less prone to overfitting
compared to the baselines. They show better alignment with text prompts while maintaining a
high level of concept fidelity. Furthermore, both methods with optimal hyperparameters are more
efficient than BOFT and comparable to LoRA and full parameterization in terms of training time.
See Appendix E for more visual and quantitative comparison."
GS ORTHOGONAL CONVOLUTIONS,0.3786407766990291,"7.3
GS Orthogonal Convolutions"
GS ORTHOGONAL CONVOLUTIONS,0.3818770226537217,"Following [Singla and Feizi, 2021], we train LipConvnet-n on CIFAR-100 dataset. LipConvnet-n
is 1-Lipschitz neural network, i.e. neural network with Lipschitz constant equal to 1, his property
provides certified adversarial robustness. LipConvnet uses orthogonal convolutions and gradient
preserving activations in order to maintain 1-Lipschitz property."
GS ORTHOGONAL CONVOLUTIONS,0.3851132686084142,"LipConvnet-n architecture consists of 5 equal blocks, each having n"
GS ORTHOGONAL CONVOLUTIONS,0.3883495145631068,"5 skew orthogonal convolutions,
where the last convolution at each level downsamples image size. We replace the skew orthogonal
convolution layer with the structured version using GSorthogonal convolutions and test it in the
setting of [Singla and Feizi, 2021], using the same hyperparameters (learning rate, batch size and
scheduler stable during testing). In layers where we have two GrExpConv, the second convolution
has kernel size equal to 1."
GS ORTHOGONAL CONVOLUTIONS,0.39158576051779936,"We also use a modified activation function (MaxMinPermuted instead of MaxMin), which uses differ-
ent pairing of channels. This makes activations aligned with the ChShuffle operation and grouped
convolutions. The choice of permutation for ChShuffle also slightly differs from permutations
defind in Definition 5.2 because of the interplay between activations and convolutional layers. We
provide definitions and intuition regarding activations and permutations for ChShuffle in Appenix F."
GS ORTHOGONAL CONVOLUTIONS,0.3948220064724919,"""a V* in a purple wizzard outfit"""
GS ORTHOGONAL CONVOLUTIONS,0.39805825242718446,"""a V* on a cobblestone street"""
GS ORTHOGONAL CONVOLUTIONS,0.40129449838187703,"LoRA
BOFT"
GS ORTHOGONAL CONVOLUTIONS,0.4045307443365696,"""a purple V*"""
GS ORTHOGONAL CONVOLUTIONS,0.4077669902912621,"GSOFT 
Concept
Double GSOFT"
GS ORTHOGONAL CONVOLUTIONS,0.4110032362459547,Figure 4: Subject-driven generation visual results on 3000 training iterations.
GS ORTHOGONAL CONVOLUTIONS,0.41423948220064727,"Table 3: Results of training LipConvnet-15 architecture on CIFAR-100. (a, b) in “Groups” column
denotes number of groups in two grouped exponential convolutions (with kernel sizes 3 and 1). (a, −)
corresponds to only one GS orthogonal convolutional layer. Before each grouped layer with k groups
use a ChShuffle operator."
GS ORTHOGONAL CONVOLUTIONS,0.4174757281553398,"Conv. Layer
# Params
Groups
Speedup
Activation
Accuracy
Robust Accuracy
SOC
24.1M
-
1
MaxMin
43.15%
29.18%
GS-SOC
6.81M
(4, -)
1.64
MaxMinPermuted
43.48%
29.26%
GS-SOC
8.91M
(4, 1)
1.21
MaxMinPermuted
43.42%
29.56%
GS-SOC
7.86M
(4, 2)
1.22
MaxMinPermuted
42.86%
28.98%
GS-SOC
7.3M
(4, 4)
1.23
MaxMinPermuted
42.75%
28.7%"
CONCLUDING REMARKS,0.42071197411003236,"8
Concluding remarks"
CONCLUDING REMARKS,0.42394822006472493,"In this paper, we introduce a new class of structured matrices, called GS-matrices, build a structured
orthogonal parametrization with them and use them in several domains within deep learning applica-
tions. However, we hope that our orthogonal parametrization can be adapted to different settings in
future (including tasks outside of deep learning), as it makes orthogonal parametrizations less of a
computational burden. GS-matrices without orthogonality constraints is another promising direction."
LIMITATIONS,0.42718446601941745,"9
Limitations"
LIMITATIONS,0.43042071197411,"Although our method for orthogonal fine-tuning is faster than BOFT, it is still slower than LoRA
during training. Additionally, since our parametrization provides a trade-off between expressivity
and parameter-efficiency, it might be unable to represent some particular orthogonal matrices, which
might be required in other settings apart from parameter-efficient fine-tuning."
ACKNOWLEDGMENTS,0.4336569579288026,"10
Acknowledgments"
ACKNOWLEDGMENTS,0.4368932038834951,"The article was prepared within the framework of the HSE University Basic Research Program.
This research was supported in part through computational resources of HPC facilities at HSE
University [Kostenetskiy et al., 2021]."
REFERENCES,0.4401294498381877,References
REFERENCES,0.44336569579288027,"Cem Anil, James Lucas, and Roger Grosse. Sorting out Lipschitz function approximation. In
Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages
291–301. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/anil19a.
html."
REFERENCES,0.44660194174757284,"Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International conference on machine learning, pages 1120–1128. PMLR, 2016."
REFERENCES,0.44983818770226536,"Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re.
Pixelated butterfly: Simple and efficient sparse training for neural network models. In International
Conference on Learning Representations (ICLR), 2022."
REFERENCES,0.45307443365695793,"Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander
Liu, Aniruddh Rao, Atri Rudra, and Christopher Re. Monarch: Expressive structured matrices
for efficient and accurate training. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference
on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 4690–
4721. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/dao22a.html."
REFERENCES,0.4563106796116505,"Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning
of quantized llms. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.459546925566343,"Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi
Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint
arXiv:2212.10650, 2022."
REFERENCES,0.4627831715210356,"Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin
Spector, Michael Poli, Atri Rudra, and Christopher Ré. Monarch mixer: A simple sub-quadratic
gemm-based architecture. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.46601941747572817,"Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual
inversion. arXiv preprint arXiv:2208.01618, 2022."
REFERENCES,0.4692556634304207,"Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff:
Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 7323–7334, 2023."
REFERENCES,0.47249190938511326,"Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
cayley transform. In International Conference on Machine Learning, pages 1969–1978. PMLR,
2018."
REFERENCES,0.47572815533980584,"Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for
nlp. In International conference on machine learning, pages 2790–2799. PMLR, 2019."
REFERENCES,0.47896440129449835,"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International
Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=
nZeVKeeFYf9."
REFERENCES,0.48220064724919093,"Stephanie Hyland and Gunnar Rätsch. Learning unitary operators with help from u (n). In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 31, 2017."
REFERENCES,0.4854368932038835,"Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank
hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035,
2021."
REFERENCES,0.4886731391585761,"PS Kostenetskiy, RA Chulkevich, and VI Kozyrev. HPC resources of the higher school of economics.
In Journal of Physics: Conference Series, volume 1740, page 012050, 2021."
REFERENCES,0.4919093851132686,"Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept
customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 1931–1941, 2023."
REFERENCES,0.49514563106796117,"Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, and Victor S. Lempitsky.
Speeding-up convolutional neural networks using fine-tuned cp-decomposition. In Yoshua Bengio
and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6553."
REFERENCES,0.49838187702265374,"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021."
REFERENCES,0.5016181229773463,"Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B Grosse, and Jörn-Henrik Jacobsen.
Preventing gradient attenuation in lipschitz constrained convolutional networks. Advances in
neural information processing systems, 32, 2019."
REFERENCES,0.5048543689320388,"Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.
In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online,
August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353.
URL https://aclanthology.org/2021.acl-long.353."
REFERENCES,0.5080906148867314,"Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo
Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models, 2023."
REFERENCES,0.511326860841424,"Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-
Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint
arXiv:2402.09353, 2024a."
REFERENCES,0.5145631067961165,"Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen
Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard
Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The Twelfth
International Conference on Learning Representations, 2024b. URL https://openreview.
net/forum?id=7NzgkEdGyr."
REFERENCES,0.517799352750809,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.5210355987055016,"Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin
Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/
huggingface/peft, 2022."
REFERENCES,0.5242718446601942,"Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors
adaptation of large language models, 2024."
REFERENCES,0.5275080906148867,"Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural
networks. Advances in neural information processing systems, 28, 2015."
REFERENCES,0.5307443365695793,"Bernd Prach, Fabio Brau, Giorgio Buttazzo, and Christoph H Lampert. 1-lipschitz layers compared:
Memory speed and certifiable robustness.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 24574–24583, 2024."
REFERENCES,0.5339805825242718,"Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian
Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning.
In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=K30wTdIIYc."
REFERENCES,0.5372168284789643,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine
learning, pages 8821–8831. Pmlr, 2021."
REFERENCES,0.540453074433657,"Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022."
REFERENCES,0.5436893203883495,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition, pages 10684–10695, 2022."
REFERENCES,0.5469255663430421,"Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
22500–22510, 2023."
REFERENCES,0.5501618122977346,"Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic
text-to-image diffusion models with deep language understanding. Advances in neural information
processing systems, 35:36479–36494, 2022."
REFERENCES,0.5533980582524272,"Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. In Marina Meila and Tong Zhang,
editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pages 9756–9766. PMLR, 18–24 Jul 2021. URL
https://proceedings.mlr.press/v139/singla21a.html."
REFERENCES,0.5566343042071198,"Sahil Singla, Surbhi Singla, and Soheil Feizi. Improved deterministic l2 robustness on cifar-10 and
cifar-100. arXiv preprint arXiv:2108.04062, 2021."
REFERENCES,0.5598705501618123,"Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-
image personalization. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1–11, 2023."
REFERENCES,0.5631067961165048,"Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. In International Conference on Machine Learning,
pages 3570–3578. PMLR, 2017."
REFERENCES,0.5663430420711975,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461, 2018."
REFERENCES,0.56957928802589,"Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding
visual concepts into textual embeddings for customized text-to-image generation. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 15943–15953, 2023."
REFERENCES,0.5728155339805825,"Xiaojun Xu, Linyi Li, and Bo Li. Lot: Layer-wise orthogonal training on improving l2 certified
robustness. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
Advances in Neural Information Processing Systems, volume 35, pages 18904–18915. Curran As-
sociates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/
file/77d52754ff6b2de5a5d96ee921b6b3cd-Paper-Conference.pdf."
REFERENCES,0.5760517799352751,"Yifan Yang, Jiajun Zhou, Ngai Wong, and Zheng Zhang. Loretta: Low-rank economic tensor-train
adaptation for ultra-low-parameter fine-tuning of large language models, 2024."
REFERENCES,0.5792880258899676,"Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo
Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International
Conference on Learning Representations, 2023."
REFERENCES,0.5825242718446602,"Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient
convolutional neural network for mobile devices. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 6848–6856, 2018."
REFERENCES,0.5857605177993528,"Yufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. Enhancing detail preservation for customized
text-to-image generation: A regularization-free approach. arXiv preprint arXiv:2305.13579, 2023."
REFERENCES,0.5889967637540453,"A
Related work"
REFERENCES,0.5922330097087378,"Parameter-Efficient Fine-Tuning (PEFT) With the growth of model sizes, end-to-end training
became unavailable for those who want to adapt powerful architectures for specific tasks, as even
full fine-tuning became too expensive. This problem sparked research in the direction of parameter-
efficient fine-tuning methods, including methods that focus on prompt tuning [Lester et al., 2021,
Li and Liang, 2021] and adapter tuning (e.g. [Houlsby et al., 2019, Karimi Mahabadi et al., 2021]),
which include LoRA [Hu et al., 2022] and its variations [Meng et al., 2024, Zhang et al., 2023, Liu
et al., 2024a, Dettmers et al., 2024, Li et al., 2023], that inject learnable low-rank matrices as an
additive injection to the weights of pretrained models. OFT [Qiu et al., 2023], BOFT [Liu et al.,
2024b] and our method use similar approach to LoRA, but learn multiplicative injection rather than
an additive one."
REFERENCES,0.5954692556634305,"Structured sparsity Structured sparsity is an approach that replaces dense weight layers with
different structured ones, such as matrix factorizations or tensor decompositions in order to compress
or speed-up models [Dao et al., 2022, Chen et al., 2022, Novikov et al., 2015, Lebedev et al., 2015].
Some of these techniques were also adapted to PEFT methods in works like [Karimi Mahabadi
et al., 2021, Edalati et al., 2022, Yang et al., 2024] or BOFT [Liu et al., 2024b] method, that utilizes
a variation of butterfly matrices as a parametrization for parameter-efficient orthogonal matrices,
imposing orthogonality to each butterfly factor. See details in Section 2. Monarch matrices [Dao
et al., 2022, Fu et al., 2024] are most relevant to our work as our proposed matrix class is their
generalization that utilizes similar structure."
REFERENCES,0.598705501618123,"Subject-driven generation The emergence of large text-to-image models [Ramesh et al., 2022, 2021,
Saharia et al., 2022, Rombach et al., 2022] has propelled the advancement of personalized generation
techniques in the research field. Customizing a text-to-image model to generate specific concepts
based on multiple input images presents a key challenge. Various methods [Ruiz et al., 2023, Gal et al.,
2022, Kumari et al., 2023, Han et al., 2023, Qiu et al., 2023, Zhou et al., 2023, Wei et al., 2023, Tewel
et al., 2023] have been proposed to address this challenge, requiring either extensive fine-tuning
of the model as a whole [Ruiz et al., 2023] or specific parts [Kumari et al., 2023] to accurately
reconstruct concept-related training images. While this facilitates precise learning of the input
concept, it also raises concerns regarding overfitting, potentially limiting the model’s flexibility in
generating diverse outputs in response to different textual prompts. Efforts to mitigate overfitting and
reduce computational burden have led to the development of lightweight parameterization techniques
[Qiu et al., 2023, Liu et al., 2024b, Hu et al., 2022, Tewel et al., 2023, Han et al., 2023] such as
those proposed among others. These methods aim to preserve editing capabilities while sacrificing
some degree of concept fidelity. The primary objective is to identify parameterization strategies that
enable high-quality concept learning without compromising the model’s ability to edit and generate
variations of the concept. Our investigation indicates that the orthogonal parameterization approach
we propose represents a significant step towards achieving this goal."
REFERENCES,0.6019417475728155,"Orthogonal transforms In papers [Vorontsov et al., 2017, Hyland and Rätsch, 2017, Helfrich
et al., 2018] authors work in the setting of dense matrices and try to parameterize essentially the
whole manifold of orthogonal matrices. For n × n matrices it requires computing inverses or
exponential maps of n × n matrices at every optimization step. This takes O(n3) time, which can be
computationally challenging for larger architectures. Moreover, such a parametrization utilizes O(n2)
trainable parameters, which makes it inapplicable for PEFT (see the OFT paper [Qiu et al., 2023]
for more details). Our proposed method is different in a sense that it provides a trade-off between
expressivity (describing only a subset of the orthogonal manifold) and efficiency."
REFERENCES,0.6051779935275081,"In [Li et al., 2019, Singla et al., 2021] authors discuss main issues of bounding of Lipschitz constant
of neural networks and provide Gradient-Norm-Preserving (GNP) architecture in order to avoid van-
ishing of gradients while bounding Lipschitz constant. The authors propose a specific convolutional
layer (Block Convolutional Orthogonal Parametrization) which Jacobian is orthogonal, also providing
orthogonal activations with Lipschitz constant equal to 1. These constraints guarantee that the norm
of the gradient will not change through backward pass. In other works [Singla and Feizi, 2021,
Singla et al., 2021] authors provide a modification of the orthogonal convolutions (Skew Orthogonal
Convolution) in terms of hardware-efficiency. Authors provide neural network architecture where
each layer is 1-Lipschitz and make a comparison between these two convolutional layers. The work
[Li et al., 2019] was outperformed by the SOC method that we utilize (there is a comparison in
the SOC method [Singla and Feizi, 2021]). In [Xu et al., 2022], the authors utilize periodicity of"
REFERENCES,0.6084142394822006,"convolution and their padding is not equivalent to a widely-used zero-padded convolution. Survey
[Prach et al., 2024] suggests that [Xu et al., 2022] and other 1-Lipschitz architectures yield worse
robustness metrics than SOC, which we use as a baseline in our paper."
REFERENCES,0.6116504854368932,"B
Proof of Prop. 1"
REFERENCES,0.6148867313915858,"Proof. Let R′ = PR. R′ can be viewed as a block matrix with kL × kR blocks of sizes bL
2 × bR
2 . L
can be viewed as a block matrix with kL × kL blocks from which only diagonal are non-zero. The A
can be written in the following form:
 
"
REFERENCES,0.6181229773462783,"A0,0
. . .
A0,kR−1
...
...
...
AkL−1,0
. . .
AkL−1,kR−1  
=  
"
REFERENCES,0.6213592233009708,"L0
. . .
0
...
...
...
0
. . .
LkL−1  
  
"
REFERENCES,0.6245954692556634,"R′
0,0
. . .
R′
0,kR−1
...
...
...
R′
kL−1,0
. . .
R′
kL−1,kR−1  
."
REFERENCES,0.627831715210356,"Using block matrix product formulas, we get:"
REFERENCES,0.6310679611650486,"Ak1,k2 = Lk1R′
k1,k2."
REFERENCES,0.6343042071197411,"We can now rewrite Lk1R′
k1,k2 product in terms of their columns and rows:"
REFERENCES,0.6375404530744336,"Lk1R′
k1,k2 =
 l1 . . . lb2
L

·  
"
REFERENCES,0.6407766990291263,"r⊤
1...
r⊤
b2
L "
REFERENCES,0.6440129449838188,"
=
X"
REFERENCES,0.6472491909385113,"t
ltr⊤
t .
(4)"
REFERENCES,0.6504854368932039,Columns of Lk1 are just vectors uj such that ⌊j
REFERENCES,0.6537216828478964,"kL ⌋= k1. Let us examine the rows of R′
k1,k2. Since
R′ is a matrix formed by permuting the rows of block-diagonal matrix R, R′
k1,k2 can only contain
rows that were in the Rk2 before permutation. Formally, this means that R′
k1,k2 can only contain
vector-rows vT
i such that ⌊i"
REFERENCES,0.656957928802589,"kR ⌋= k2. Additionally, rows after permutation should get into the"
REFERENCES,0.6601941747572816,k1-block row. That implies ⌊σ(i)
REFERENCES,0.6634304207119741,"kL ⌋= k1. Other rows of R′
k1,k2 are zero-rows. Notice that in (4)
non-zero rows r⊤
t represented by v⊤
i will match exactly with columns uσ(i) that represent lt. Keeping
only non-zero terms in P"
REFERENCES,0.6666666666666666,"t ltr⊤
t gets us to the desired conclusion."
REFERENCES,0.6699029126213593,"C
Comparison of Monarch matrices and GS-matrices"
REFERENCES,0.6731391585760518,"GS(PL, P, Pr) class is inspired by Monarch matrices [Dao et al., 2022] and their primary goal is
to introduce additional flexibility in the block structure of matrices L and R. Generalized Monarch
matrices are parameterized as P1LP2R, where L and R are block-diagonal matrices and P1 and
P2 are certain permutations defined in Definition 5.2. This resembles GS(P1, P2, I) matrix class,
however in comparison monarch matrices have additional hard constraints on relation between kL
and kR. Being more precise, Monarch matrices are a special case of GS(P1, P2, I) matrices with
additional constraints kL = b1
R, kR = b2
L. Such constraints lead to several theoretical and practical
limitations of Monarch matrices. From theoretical point of view, Monarch matrices can only describe
permuted block matrices with blocks of ranks 1. In contrast GS-matrices with can describe matrices
with different rank structure of blocks (including structures where rank of each block is equal to
arbitrary r). From practical point of view, due to this constraint Monarch matrices are often unable
to form a desirable block structure of matrices L and R. For demonstration of this phenomena,
consider a case of square matrices with square blocks – the structure needed in Orthogonal fine-tuning
paradigm. Formally, we have b1
L = b2
L = bL, b1
R = bR
2 = bR, m = n. Additional Monarch constraint
would mean that bR = kL; bL = kR. This in turn means that kL · kR = n. As we can see, it makes
impossible to stack two matrices with small number of blocks (say, 4) or large number of blocks,
which is required in situations with low parameter budget. In contrast, GS parametrization allows for
both of these structures, which we use in our experiments."
REFERENCES,0.6763754045307443,"Note, that the work [Fu et al., 2024] provides a slightly different definition for monarch matrices,
introducing order-p Monarch matrices. These matrices are also a special case of GS class, however
they are very restrictive as they can only parametrize matrices with both sides equal to ap for some
integers a, p."
B,0.6796116504854369,"1
b
b2
bk . . . . . . . . . . . . ... ... ..."
B,0.6828478964401294,"...
... ... ... ... ... ... ... ... ... ... ..."
B,0.686084142394822,"Figure 5: Demonstration of information transition through a block structure. Each node is connected
to exactly b consecutive nodes from the next level."
B,0.6893203883495146,"D
Proof of Theorem 2"
B,0.6925566343042071,"We use information transition framework from [Liu et al., 2024b], representing product of m sparse
d × d matrices as an transmitting information in a grid with d × (m + 1) nodes. Edges between nodes
j and i represent that element i, j in sparse matrix is non-zero. Element i, j from final matrix can
only be non-zero if there exists a path from the j-th node from the right column to the i-th node in
the left column (see Figure 5)."
B,0.6957928802588996,"Proof. Consider an information transmission graph for the matrix BiP(r,br). In this graph, the first
node connects with b first edges, the second node connects with the edges from b + 1 to 2b and so
on. Now consider a graph for the product of m such matrices. As shown in Figure 5, now each
node from the first level has paths to bk unique nodes from the kth-th level. It means that using
m = ⌈logb(d)⌉= ⌈logb(br)⌉= 1 + ⌈logb(r)⌉matrices is sufficient to reach all nodes and therefore
form a dense matrix. Note that the number of paths for each node is always equal to bm regardless of
permutation choice. This observation shows that it is impossible to reach d unique elements on the
final level with m < 1 + ⌈logb(r)⌉."
B,0.6990291262135923,"E
Subject-driven generation"
B,0.7022653721682848,"Training details All the models are trained using Adam optimizer with batch size = 4, learning rate
= 0.00002, betas = (0.9, 0.999) and weight decay = 0.01. The Stable Diffusion-2-base model is used
for all experiments."
B,0.7055016181229773,"Evaluation details We use the DreamBooth dataset for evaluation. The dataset contains 25 different
contextual prompts for 30 various objects including pets, toys and furnishings. For each concept
we generate 10 images per contextual prompt and 30 images per base prompt “a photo of an S*”,
resulting in 780 unique concept-prompt pairs and a total of 8400 images for fair evaluation."
B,0.7087378640776699,"To measure concept fidelity, we use the average pairwise cosine similarity (IS) between CLIP ViTB/32
embeddings of real and generated images as in [Gal et al., 2022]. This means that the image similarity
is calculated using only the base prompt, i.e. “a photo of an S*”. Higher values of this metric usually
indicate better subject fidelity, while keeping this evaluation scene-independent. To evaluate the
correspondence between generated images and contextual prompts (TS), the average cosine similarity"
B,0.7119741100323624,"between CLIP ViTB/32 embeddings of the prompt and generated images [Ruiz et al., 2023, Gal et al.,
2022]."
B,0.7152103559870551,"Overfitting discussion Typically, the more parameters are trained, the more easily the model overfits.
This results in higher image similarity and lower text similarity. In addition, if a model with a large
number of training parameters is trained for a long time, the image similarity can often start to
decrease as the model starts to collapse and artifacts start to appear. This sometimes leads to models
with more trainable parameters having a worse score in both image and text similarity than the models
with fewer ones."
B,0.7184466019417476,"Models with fewer trainable parameters overfit less, but require longer training to capture the concept
carefully, and usually have an upper limit on the maximum image similarity: at some point the
increase in image similarity becomes small, while text similarity starts to decrease dramatically.
Therefore, the very common result of a usual fine-tuning is either an overfitting with poor context
preservation or an undertraining with poor concept fidelity. The orthogonal fine-tuning shows a
different behavior. The model with less trainable parameters can be trained longer (GSOFT, OFT,
BOFT) and capture the concept more carefully without artifacts. At the same time, it overfits less and
shows higher text similarity."
B,0.7216828478964401,"Additional results In Figures 6 we show a graphical representation of the metrics for 1000 and 3000
iterations. Examples of generation for different methods are presented in Figure 7, 8."
B,0.7249190938511327,"F
GS Orthogonal Convolution"
B,0.7281553398058253,"In this section, we provide some details and insights about the choice of the ChShuffle permutation
and the activation function."
B,0.7313915857605178,"In experiments, we apply the ChShuffle operation right before grouped convolutional layers. Stack-
ing several layers of that form resembles higher-order GS-matrices, which motivates the usage of
permutations from Definition 5.2 for optimal information transition (see Appendix D). However, in
the LipConvnet architecture, the activation function can also shuffle information between channels.
Thus, this additional shuffling of information can negatively affect our information transition proper-
ties. In the original SOC paper [Singla and Feizi, 2021], the authors use MaxMin activation, firstly
proposed in [Anil et al., 2019].
Definition F.1. [Singla and Feizi, 2021] Given a feature tensor X ∈R2m×n×n, the MaxMin(X)
activation of a tensor X is defined as follows:"
B,0.7346278317152104,"A = X:m,:,:, B = Xm:,:,:,
MaxMin(X):m,:,: = max(A, B),
MaxMin(X)m:,:,: = min(A, B)."
B,0.7378640776699029,"This activation shuffles information between different groups in convolution which harms performance
of our experiments, as permutations that we use in ChShuffle become sub-optimal in terms of
information transmission. Thus, we introduce a modification of MaxMin activation, that splits
channels into pairs in a different way. Rather than constructing pairs from different halves of input"
B,0.7411003236245954,"0.22
0.24
0.26
TS 0.79 0.80 0.81 0.82"
B,0.7443365695792881,BaseIS
B,0.7475728155339806,num steps = 1000
B,0.7508090614886731,"0.22
0.24
0.26
TS"
B,0.7540453074433657,num steps = 3000
B,0.7572815533980582,"LoRA rank=4
LoRA rank=32
LoRA rank=128
GSOFT r=8
GSOFT r=16
GSOFT r=32
Double GSOFT r=16
Double GSOFT r=32
Double GSOFT r=64
BOFT r=16, m=5
BOFT r=32, m=6
BOFT r=32, m=4
Full"
B,0.7605177993527508,Figure 6: Image and text similarity visualisation for different methods on subject-driven generation.
B,0.7637540453074434,Figure 7: Subject-driven generation visual results on 3000 training iterations.
B,0.7669902912621359,"tensor, we use neighboring channels for forming of pairs (first channel pairs with second, third
with fourth and so on). With this modification information does not transfer between groups during
activations, which enables more optimal information transmission in-between layers with ChShuffle
operator. In further experiments we denote this activation function as MaxMinPermuted and define it
below:"
B,0.7702265372168284,Definition F.2. Given a feature map X ∈R2m×n×n. MaxMinPermuted(X) is defined as follows:
B,0.7734627831715211,"A = X::2,:,:, B = X1::2,:,:,"
B,0.7766990291262136,"MaxMinPermuted(X)::2,:,: = max(A, B),"
B,0.7799352750809061,"MaxMinPermuted(X)1::2,:,: = min(A, B)"
B,0.7831715210355987,"However, we also empirically find that it is crucial for the channels that interact within activations
functions to also interact during convolutions. This means that they should always stay in the same
group. This motivates us to use a slightly different permutation for the ChShuffle operation, which
permutes channels in pairs. We use the following permutation"
B,0.7864077669902912,"σ(i)paired
(k,n)
=
 i 2"
B,0.7896440129449838,"
mod k

· n"
B,0.7928802588996764,"k + 2 ·
 i"
K,0.7961165048543689,2k
K,0.7993527508090615,"
+ (i mod 2)"
K,0.8025889967637541,Figure 8: Subject-driven generation visual results on 1000 training iterations.
K,0.8058252427184466,"This permutation can be seen as an adaptation of P(k,n) that operates on pairs of channels instead
of single channels. This permutation is also optimal in terms of information transition. We call this
permutation “paired”. Using this paired permutation as a ChShuffle with our modified activation
saves connection between pairs while also transmitting information in the most efficient way. We
provide the results of comparison of approaches with activations and permutations in Table 4."
K,0.8090614886731392,"Table 4: Comparison of activations on LipConvnet-15 architecture and CIFAR-100. (a, b) in “Groups”
column denotes that we have two grouped exponential convolutions (the first one with kernel_size =
3, the second with kernel_size = 1). If b is not mentioned, we have only one GS orthogonal
convolutional layer."
K,0.8122977346278317,"Conv. Layer
# Params
Groups
Speedup
Activation
Permutation
Accuracy
Robust Accuracy
SOC
24.1M
-
1
MaxMin
-
43.15%
29.18%
GS-SOC
6.81M
(4, -)
1.64
MaxMinPermuted
paired
43.48%
29.26%
GS-SOC
6.81M
(4, -)
1.64
MaxMinPermuted
not paired
40.46%
26.18%
GS-SOC
6.81M
(4, -)
1.64
MaxMin
paired
37.99%
24.19%
GS-SOC
6.81M
(4, -)
1.64
MaxMin
not paired
39.72%
25.96%
GS-SOC
8.91M
(4, 1)
1.21
MaxMinPermuted
paired
43.42%
29.56%
GS-SOC
8.91M
(4, 1)
1.21
MaxMinPermuted
not paired
40.15%
26.4%
GS-SOC
8.91M
(4, 1)
1.21
MaxMin
paired
40.3%
26.74%
GS-SOC
8.91M
(4, 1)
1.21
MaxMin
not paired
41.7%
27.66%
GS-SOC
7.86M
(4, 2)
1.22
MaxMinPermuted
paired
42.86%
28.98%
GS-SOC
7.86M
(4, 2)
1.22
MaxMinPermuted
not paired
41.13%
27.53%
GS-SOC
7.86M
(4, 2)
1.22
MaxMin
paired
41.55%
27.45%
GS-SOC
7.86M
(4, 2)
1.22
MaxMin
not paired
41.25%
27.29%
GS-SOC
7.3M
(4, 4)
1.23
MaxMinPermuted
paired
42.75%
28.7%
GS-SOC
7.3M
(4, 4)
1.23
MaxMinPermuted
not paired
38.93%
25.59%
GS-SOC
7.3M
(4, 4)
1.23
MaxMin
paired
40.34%
27.06%
GS-SOC
7.3M
(4, 4)
1.23
MaxMin
not paired
41.57%
27.48%"
K,0.8155339805825242,"It can be seen that using “paired” permutation used with MinMaxPermuted activation significantly
improves quality metrics."
K,0.8187702265372169,NeurIPS Paper Checklist
CLAIMS,0.8220064724919094,1. Claims
CLAIMS,0.8252427184466019,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Yes, all the assumptions claimed in abstract and introduction were discussed
in our paper.
Guidelines:"
CLAIMS,0.8284789644012945,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8317152103559871,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: All the limitations of the work are discussed in Section 9.
Guidelines:"
CLAIMS,0.8349514563106796,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8381877022653722,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.8414239482200647,"Justification: Yes, we prove all theorems which were introduced in our paper or we have all
the necessary references to theory we used.
Guidelines:"
CLAIMS,0.8446601941747572,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.8478964401294499,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Yes, we provide all the necessary information to reproduce the results.
Guidelines:"
CLAIMS,0.8511326860841424,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
CLAIMS,0.8543689320388349,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
CLAIMS,0.8576051779935275,"Answer: [Yes]
Justification: Yes, we provide a link to github repo in Section 7.
Guidelines:"
CLAIMS,0.86084142394822,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
CLAIMS,0.8640776699029126,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Yes, in Section 7 we provide all the necessary details.
Guidelines:"
CLAIMS,0.8673139158576052,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
CLAIMS,0.8705501618122977,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: We conducted the experiments within limited computational resources and
demanding settings. Nevertheless, we plan to add error bars to some of the experiments on
convolution NNs.
Guidelines:"
CLAIMS,0.8737864077669902,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors)."
CLAIMS,0.8770226537216829,"• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8802588996763754,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.883495145631068,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.8867313915857605,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.889967637540453,"Justification: Yes, we wrote the information about the resources used for experiments in
Section 7."
EXPERIMENTS COMPUTE RESOURCES,0.8932038834951457,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.8964401294498382,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.8996763754045307,9. Code Of Ethics
CODE OF ETHICS,0.9029126213592233,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9061488673139159,Answer: [Yes]
CODE OF ETHICS,0.9093851132686084,"Justification: Yes, our research conforms with the NeurIPS Code of Ethics."
CODE OF ETHICS,0.912621359223301,Guidelines:
CODE OF ETHICS,0.9158576051779935,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.919093851132686,10. Broader Impacts
BROADER IMPACTS,0.9223300970873787,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9255663430420712,Answer: [NA]
BROADER IMPACTS,0.9288025889967637,Justification: There is no societal impact of the work performed.
BROADER IMPACTS,0.9320388349514563,Guidelines:
BROADER IMPACTS,0.9352750809061489,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.9385113268608414,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.941747572815534,11. Safeguards
SAFEGUARDS,0.9449838187702265,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.948220064724919,Answer: [NA]
SAFEGUARDS,0.9514563106796117,Justification: Our paper poses no such risks.
SAFEGUARDS,0.9546925566343042,Guidelines:
SAFEGUARDS,0.9579288025889967,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9611650485436893,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9644012944983819,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9676375404530745,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.970873786407767,"Justification: Yes, we cite all papers appeared in the text."
LICENSES FOR EXISTING ASSETS,0.9741100323624595,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9773462783171522,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9805825242718447,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9838187702265372,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We don’t release any new assets in our paper.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9870550161812298,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9902912621359223,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: In our paper there is no crowdsourcing experiments.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9935275080906149,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9967637540453075,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
