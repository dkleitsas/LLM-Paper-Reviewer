Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009523809523809524,"We propose “collision cross-entropy” as a robust alternative to Shannon’s cross-
1"
ABSTRACT,0.0019047619047619048,"entropy (CE) loss when class labels are represented by soft categorical distributions
2"
ABSTRACT,0.002857142857142857,"y. In general, soft labels can naturally represent ambiguous targets in classification.
3"
ABSTRACT,0.0038095238095238095,"They are particularly relevant for self-labeled clustering methods, where latent
4"
ABSTRACT,0.004761904761904762,"pseudo-labels y are jointly estimated with the model parameters and uncertainty is
5"
ABSTRACT,0.005714285714285714,"prevalent. In case of soft labels y, Shannon’s CE teaches the model predictions σ
6"
ABSTRACT,0.006666666666666667,"to reproduce the uncertainty in each training example, which inhibits the model’s
7"
ABSTRACT,0.007619047619047619,"ability to learn and generalize from these examples. As an alternative loss, we
8"
ABSTRACT,0.008571428571428572,"propose the negative log of “collision probability” that maximizes the chance of
9"
ABSTRACT,0.009523809523809525,"equality between two random variables, predicted class and unknown true class,
10"
ABSTRACT,0.010476190476190476,"whose distributions are σ and y. We show that it has the properties of a generalized
11"
ABSTRACT,0.011428571428571429,"CE. The proposed collision CE agrees with Shannon’s CE for one-hot labels y, but
12"
ABSTRACT,0.012380952380952381,"the training from soft labels differs. For example, unlike Shannon’s CE, data points
13"
ABSTRACT,0.013333333333333334,"where y is a uniform distribution have zero contribution to the training. Collision
14"
ABSTRACT,0.014285714285714285,"CE significantly improves classification supervised by soft uncertain targets. Unlike
15"
ABSTRACT,0.015238095238095238,"Shannon’s, collision CE is symmetric for y and σ, which is particularly relevant
16"
ABSTRACT,0.01619047619047619,"when both distributions are estimated in the context of self-labeled clustering.
17"
ABSTRACT,0.017142857142857144,"Focusing on discriminative deep clustering where self-labeling and entropy-based
18"
ABSTRACT,0.018095238095238095,"losses are dominant, we show that the use of collision CE improves the state-of-
19"
ABSTRACT,0.01904761904761905,"the-art. We also derive an efficient EM algorithm that significantly speeds up the
20"
ABSTRACT,0.02,"pseudo-label estimation with collision CE.
21"
INTRODUCTION AND MOTIVATION,0.02095238095238095,"1
Introduction and Motivation
22"
INTRODUCTION AND MOTIVATION,0.021904761904761906,"Shannon’s cross-entropy H(y, σ) is the most common loss for training network predictions σ from
23"
INTRODUCTION AND MOTIVATION,0.022857142857142857,"ground truth labels y in the context of classification, semantic segmentation, etc. However, this
24"
INTRODUCTION AND MOTIVATION,0.023809523809523808,"loss may not be ideal for applications where the targets y are soft distributions representing various
25"
INTRODUCTION AND MOTIVATION,0.024761904761904763,"forms of uncertainty. For example, this paper is focused on self-labeled classification [17, 1, 15, 16]
26"
INTRODUCTION AND MOTIVATION,0.025714285714285714,"where the ground truth is not available and the network training is done jointly with estimating
27"
INTRODUCTION AND MOTIVATION,0.02666666666666667,"latent pseudo-labels y. In this case soft y can represent the distribution of label uncertainty. Similar
28"
INTRODUCTION AND MOTIVATION,0.02761904761904762,"uncertainty of class labels is also natural for supervised problems where the ground truth has errors
29"
INTRODUCTION AND MOTIVATION,0.02857142857142857,"[26, 41]. In any cases of label uncertainty, if soft distribution y is used as a target in H(y, σ), the
30"
INTRODUCTION AND MOTIVATION,0.029523809523809525,"network is trained to reproduce the uncertainty, see the dashed curves in Fig.1.
31"
INTRODUCTION AND MOTIVATION,0.030476190476190476,"Our work is inspired by generalized entropy measures [33, 18].
Besides mathematical gen-
32"
INTRODUCTION AND MOTIVATION,0.03142857142857143,"erality, the need for such measures “stems from practical aspects when modelling real world
33"
INTRODUCTION AND MOTIVATION,0.03238095238095238,"phenomena though entropy optimization algorithms” [30]. Similarly to Lp norms, parametric
34"
INTRODUCTION AND MOTIVATION,0.03333333333333333,"families of generalized entropy measures offer a wide spectrum of options.
The Shannon’s
35"
INTRODUCTION AND MOTIVATION,0.03428571428571429,"entropy is just one of them.
Other measures could be more “natual” for any given problem.
36"
INTRODUCTION AND MOTIVATION,0.035238095238095235,"Figure 1: Collision cross-entropy H2(y, σ) in (9) for
fixed soft labels y (red, green, and blue). Assuming binary
classification, all possible predictions σ = (x, 1 −x) ∈
∆2 are represented by points x ∈[0, 1] on the horizontal
axis. For comparison, thin dashed curves show Shannon’s
cross-entropy H(y, σ) in (8). Note that H converges
to infinity at both endpoints of the interval. In contrast,
H2 is bounded for any non-hot y. Such boundedness
suggests robustness to target errors represented by soft
labels y. Also, collision cross-entropy H2 gradually turns
off the training (sets zero-gradients) as soft labels become
highly uncertain (solid blue). In contrast, H(y, σ) trains
the network to copy this uncertainty, e.g. observe the
optimum σ for all dashed curves."
INTRODUCTION AND MOTIVATION,0.03619047619047619,"A simple experiment in Figure 2 shows that
37"
INTRODUCTION AND MOTIVATION,0.037142857142857144,"Shannon’s cross-entropy produces deficient so-
38"
INTRODUCTION AND MOTIVATION,0.0380952380952381,"lutions for soft labels y compared to the pro-
39"
INTRODUCTION AND MOTIVATION,0.039047619047619046,"posed collision cross-entropy. The limitation
40"
INTRODUCTION AND MOTIVATION,0.04,"of the standard cross-entropy is that it encour-
41"
INTRODUCTION AND MOTIVATION,0.040952380952380955,"ages the distributions σ and y to be equal, see
42"
INTRODUCTION AND MOTIVATION,0.0419047619047619,"the dashed curves in Fig.1. For example, the
43"
INTRODUCTION AND MOTIVATION,0.04285714285714286,"model predictions σ are trained to copy the un-
44"
INTRODUCTION AND MOTIVATION,0.04380952380952381,"certainty of the label distribution y, even when
45"
INTRODUCTION AND MOTIVATION,0.04476190476190476,"y is an uninformative uniform distribution. In
46"
INTRODUCTION AND MOTIVATION,0.045714285714285714,"contrast, our collision cross-entropy (the solid
47"
INTRODUCTION AND MOTIVATION,0.04666666666666667,"curves) gradually weakens the training as y
48"
INTRODUCTION AND MOTIVATION,0.047619047619047616,"gets less certain. This numerical property of
49"
INTRODUCTION AND MOTIVATION,0.04857142857142857,"our cross-entropy follows from its definition
50"
INTRODUCTION AND MOTIVATION,0.049523809523809526,"(9) - it maximizes the probability of “colli-
51"
INTRODUCTION AND MOTIVATION,0.05047619047619047,"sion”, which is an event when two random
52"
INTRODUCTION AND MOTIVATION,0.05142857142857143,"variables sampled from the distributions σ and
53"
INTRODUCTION AND MOTIVATION,0.05238095238095238,"y are equal. This means that the predicted class
54"
INTRODUCTION AND MOTIVATION,0.05333333333333334,"value is equal to the latent label. This is signif-
55"
INTRODUCTION AND MOTIVATION,0.054285714285714284,"icantly different from the σ = y encouraged
56"
INTRODUCTION AND MOTIVATION,0.05523809523809524,"by the Shannon’s cross-entropy. For example,
57"
INTRODUCTION AND MOTIVATION,0.05619047619047619,"if y is uniform then it does not matter what the
58"
INTRODUCTION AND MOTIVATION,0.05714285714285714,"model predicts as the probability of collision
59"
INTRODUCTION AND MOTIVATION,0.058095238095238096,"1
K would not change.
60"
INTRODUCTION AND MOTIVATION,0.05904761904761905,"Organization of the paper: After the summary of our contributions below, Section 2 reviews the
61"
INTRODUCTION AND MOTIVATION,0.06,"relevant background on self-labeling models/losses and generalized information measures for entropy,
62"
INTRODUCTION AND MOTIVATION,0.06095238095238095,"divergence, and cross-entropy. Then, Section 3 introduces our collision cross entropy measure,
63"
INTRODUCTION AND MOTIVATION,0.06190476190476191,"discusses its properties, related formulations of Rényi cross-entropy, and relation to noisy labels in
64"
INTRODUCTION AND MOTIVATION,0.06285714285714286,"fully-supervised settings. Section 4 formulates our self-labeling loss by replacing the Shannon’s cross
65"
INTRODUCTION AND MOTIVATION,0.06380952380952382,"entropy term in a representative state-of-the-art formulation using soft pseudo-labels [16] with our
66"
INTRODUCTION AND MOTIVATION,0.06476190476190476,"collision-cross-entropy. The obtained loss function is convex w.r.t. pseudo-labels y, which makes
67"
INTRODUCTION AND MOTIVATION,0.06571428571428571,"estimation of y amenable to generic projected gradient descent. However, Section 4 derives a much
68"
INTRODUCTION AND MOTIVATION,0.06666666666666667,"faster EM algorithm for estimating y. As common for self-labeling, optimization of the total loss
69"
INTRODUCTION AND MOTIVATION,0.06761904761904762,"w.r.t. network parameters is done via backpropagation. Section 5 presents our experiments, followed
70"
INTRODUCTION AND MOTIVATION,0.06857142857142857,"by conclusions.
71"
INTRODUCTION AND MOTIVATION,0.06952380952380953,"Summary of Contributions: We propose the collision cross-entropy as an alternative to the standard
72"
INTRODUCTION AND MOTIVATION,0.07047619047619047,"Shannon’s cross-entropy mainly in the context of self-labeled classification with soft pseudo-labels.
73"
INTRODUCTION AND MOTIVATION,0.07142857142857142,"The main practical advantage is its robustness to uncertainty in the labels, which could also be
74"
INTRODUCTION AND MOTIVATION,0.07238095238095238,"useful in other applications. The definition of our cross-entropy has an intuitive probabilistic
75"
INTRODUCTION AND MOTIVATION,0.07333333333333333,"interpretation that agrees with the numerical and empirical properties. Unlike the Shannon’s cross-
76"
INTRODUCTION AND MOTIVATION,0.07428571428571429,"entropy, our formulation is symmetric w.r.t. predictions σ and pseudo-labels y. This is a conceptual
77"
INTRODUCTION AND MOTIVATION,0.07523809523809524,"advantage since both σ and y are estimated/optimized distributions. Our cross-entropy allows efficient
78"
INTRODUCTION AND MOTIVATION,0.0761904761904762,"optimization of pseudo-labels by a proposed EM algorithm, that significantly accelerates a generic
79"
INTRODUCTION AND MOTIVATION,0.07714285714285714,"projected gradient descent. Our experiments show consistent improvement over multiple examples of
80"
INTRODUCTION AND MOTIVATION,0.07809523809523809,"unsupervised and semi-supervised clustering, and several standard network architectures.
81"
BACKGROUND REVIEW,0.07904761904761905,"2
Background Review
82"
BACKGROUND REVIEW,0.08,"We study a new generalized cross-entropy measure in the context of deep clustering. The models are
83"
BACKGROUND REVIEW,0.08095238095238096,"trained on unlabeled data, but applications with partially labeled data are also relevant. Self-labeled
84"
BACKGROUND REVIEW,0.08190476190476191,"deep clustering is a popular area of research [5, 31]. More recently, the-state-of-the-art is achieved by
85"
BACKGROUND REVIEW,0.08285714285714285,"discriminative clustering methods based on maximizing the mutual information between the input and
86"
BACKGROUND REVIEW,0.0838095238095238,"the output of the deep model [3]. There is a large group of relevant methods [22, 10, 15, 17, 1, 16]
87"
BACKGROUND REVIEW,0.08476190476190476,"and we review the most important loss functions, all of which use standard information-theoretic
88"
BACKGROUND REVIEW,0.08571428571428572,"measures such as Shannon’s entropy. In the second part of this section, we overview the necessary
89"
BACKGROUND REVIEW,0.08666666666666667,"mathematical background on the generalized entropy measures, which are central to our work.
90"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.08761904761904762,"2.1
Information-based Self-labeled Clustering
91"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.08857142857142856,"The work of Bridle, Heading, and MacKay from 1991 [3] formulated mutual information (MI) loss for
92"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.08952380952380952,"unsupervised discriminative training of neural networks using probability-type outputs, e.g. softmax
93"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09047619047619047,"σ : RK →∆K mapping K logits lk ∈R to a point in the probability simplex ∆K. Such output
94"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09142857142857143,"σ = (σ1, . . . , σK) is often interpreted as a posterior over K classes, where σk =
exp lk
P"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09238095238095238,"i exp li is a scalar
95"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09333333333333334,"prediction for each class k.
96"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09428571428571429,"The unsupervised loss proposed in [3] trains the model predictions to keep as much information about
97"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09523809523809523,"the input as possible. They derived an estimate of MI as the difference between the average entropy
98"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09619047619047619,"of the output and the entropy of the average output
99"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09714285714285714,"\
la
bel {e q:
m
i} L _ {mi}\;\;:=\;\;- MI(c,X)\;\;\;\;\approx \;\;\;\;\overline {H(\sigma )} \;-\; H(\overline {\sigma }) 
(1)"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.0980952380952381,"where c is a random variable representing class prediction, X represents the input, and the av-
100"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.09904761904761905,"eraging is done over all input samples {Xi}M
i=1, i.e. over M training examples. The derivation
101"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1,"in [3] assumes that softmax represents the distribution Pr(c|X). However, since softmax is not
102"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10095238095238095,"a true posterior, the right hand side in (1) can be seen only as an MI loss. In any case, (1)
103"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1019047619047619,"has a clear discriminative interpretation that stands on its own: H(σ) encourages “fair” predic-
104"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10285714285714286,"tions with a balanced support of all categories across the whole training data set, while H(σ)
105"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10380952380952381,"encourages confident or “decisive” prediction at each data point implying that decision bound-
106"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10476190476190476,"aries are away from the training examples [11]. Generally, we call clustering losses for soft-
107"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10571428571428572,"max models “information-based” if they use measures from the information theory, e.g. entropy.
108"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10666666666666667,"0.0
0.2
0.4
0.6
0.8
corruption level 25% 35% 45% 55% 65% 75% 85% 95%"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10761904761904761,accuracy
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10857142857142857,"shannon CE: H(y,
)"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.10952380952380952,"shannon CE: H(y,
)"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11047619047619048,"collision CE: H2(y,
)"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11142857142857143,"Figure 2: Robustness to label uncertainty: collision cross-
entropy (9) vs Shannon’s cross-entropy (8). The test uses
ResNet-18 architecture on fully-supervised Natural Scene
dataset [27] where we corrupted some labels. The hor-
izontal axis shows the percentage η of training images
where the correct ground truth labels were replaced by a
random label. Both losses trained the model using soft
target distributions ˆy = η∗u+(1−η)∗y representing the
mixture of one-hot distribution y for the observed corrupt
label and the uniform distribution u, as recommended in
[26]. The vertical axis shows the test accuracy. Training
with the collision cross-entropy is robust to much higher
levels of label uncertainty. As discussed in the last part of
Sec.3, in the context of classification supervised by hard
noisy labels, collision CE with soft labels can be related
to the forward correction methods [28]. 109"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11238095238095239,"Discriminative clustering loss (1) can be ap-
110"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11333333333333333,"plied to deep or shallow models. For clarity,
111"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11428571428571428,"this paper distinguishes parameters w of the
112"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11523809523809524,"representation layers of the network comput-
113"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11619047619047619,"ing features fw(X) ∈RN for any input X
114"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11714285714285715,"and the linear classifier parameters v of the
115"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1180952380952381,"output layer computing K-logit vector v⊤f
116"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.11904761904761904,"for any feature f ∈RN. The overall network
117"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12,"model is defined as
118"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12095238095238095,"\label {eq:postmodel_deep} \sigma (\wc ^\top f_\wf (X)). 
(2)"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1219047619047619,"A special “shallow” case in (2) is a basic linear
119"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12285714285714286,"discriminator
120"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12380952380952381,"\label {eq:postmodel_shallow} \sigma (\wc ^\top X) 
(3)"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12476190476190477,"directly operating on low-level input features
121"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12571428571428572,"f = X. Optimization of the loss (1) for the
122"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12666666666666668,"shallow model (3) is done only over linear clas-
123"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12761904761904763,"sifier parameters v, but the deeper network
124"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.12857142857142856,"model (2) is optimized over all network pa-
125"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1295238095238095,"rameters [v, w]. Typically, this is done via
126"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.13047619047619047,"gradient descent or backpropagation [35, 3].
127"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.13142857142857142,"Optimization of MI losses (1) during network
128"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.13238095238095238,"training is mostly done with standard gradi-
129"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.13333333333333333,"ent descent or backpropagation [3, 22, 15].
130"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.13428571428571429,"However, due to the entropy term represent-
131"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.13523809523809524,"ing the decisiveness, such loss functions are
132"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1361904761904762,"non-convex and present challenges to the gradient descent. This motivates alternative formulations
133"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.13714285714285715,"and optimization approaches. For example, it is common to incorporate into the loss auxiliary
134"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1380952380952381,"variables y representing pseudo-labels for unlabeled data points X and to estimate them jointly
135"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.13904761904761906,"with optimization of the network parameters [10, 1, 16]. Typically, such self-labeling approaches
136"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.14,"to unsupervised network training iterate optimization of the loss over pseudo-labels and network
137"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.14095238095238094,"parameters, similarly to the Lloyd’s algorithm for K-means [2]. While the network parameters are
138"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1419047619047619,"still optimized via gradient descent, the pseudo-labels can be optimized via more powerful algorithms.
139"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.14285714285714285,"For example, self-labeling in [1] uses the following constrained optimization problem with discrete
140"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1438095238095238,"pseudo-labels y
  \"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.14476190476190476,"lab
e
l {e q:
vida
l d i}
 L_
{ce
} \ ;\;=\;\; \overline {H(y,\sigma )} \;\;\;\;\;\;\;\; s.t.\;\;\; y\in \Delta ^K_{0,1} \;\;\;and\;\;\; \Bar {y}=u
(4)"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1457142857142857,"where ∆K
0,1 are one-hot distributions, i.e. corners of the probability simplex ∆K. Training the
142"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.14666666666666667,"network predictions σ is driven by the standard cross entropy loss H(y, σ), which is convex assuming
143"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.14761904761904762,"fixed (pseudo) labels y. With respect to variables y, the cross entropy is linear. Without the balancing
144"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.14857142857142858,"constraint ¯y = u, the optimal y corresponds to the hard arg max(σ). However, the balancing
145"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.14952380952380953,"constraint converts this into an integer programming problem that can be solved approximately via
146"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.15047619047619049,"optimal transport [9]. The cross-entropy in (4) encourages the predictions σ to approximate one-hot
147"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.15142857142857144,"pseudo-labels y, which implies the decisiveness.
148"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1523809523809524,"Self-labeling methods for unsupervised clustering can also use soft pseudo-labels y ∈∆K as target
149"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.15333333333333332,"distributions in cross-entropy H(y, σ). In general, soft targets y are common in H(y, σ), e.g. in the
150"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.15428571428571428,"context of noisy labels [41, 38]. Softened targets y can also assist network calibration [12, 26] and
151"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.15523809523809523,"improve generalization by reducing over-confidence [29]. In the context of unsupervised clustering,
152"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.15619047619047619,"cross-entropy H(y, σ) with soft pseudo-labels y approximates the decisiveness since it encourages
153"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.15714285714285714,"σ ≈y implying H(y, σ) ≈H(y) ≈H(σ) where the latter is the first term in (1). Instead of the
154"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1580952380952381,"hard constraint ¯y = u used in (4), the soft fairness constraint can be represented by KL divergence
155"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.15904761904761905,"KL(¯y ∥u), as in [10, 16]. In particular, [16] formulates the following self-labeled clustering loss
  \"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.16,"label 
{
eq:i sm
a il} L _ {ce+kl} \;\;= &\;\;\;\;\;\;\overline {H(y,\sigma )} \;\;\;\;\;+ \;\; KL(\Bar {y}\,\|\,u)
(5)"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.16095238095238096,"encouraging decisiveness and fairness as discussed. Similarly to (4), the network parameters in
157"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1619047619047619,"loss (5) are trained by the standard cross-entropy term, but optimization over relaxed pseudo-labels
158"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.16285714285714287,"y ∈∆K is relatively easy due to convexity. While there is no closed-form solution, the authors offer
159"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.16380952380952382,"an efficient approximate solver for y. Iterating steps that estimate pseudo-labels y and optimize the
160"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.16476190476190475,"model parameters resembles the Lloyd’s algorithm for K-means. The results in [16] also establish a
161"
INFORMATION-BASED SELF-LABELED CLUSTERING,0.1657142857142857,"formal relation between the loss (5) and the K-means objective.
162"
GENERALIZED ENTROPY MEASURES,0.16666666666666666,"2.2
Generalized Entropy Measures
163"
GENERALIZED ENTROPY MEASURES,0.1676190476190476,"Below, we review relevant generalized formulations of the information-theoretic concepts: entropy,
divergence, and cross-entropy. Rényi [33] introduced the entropy of order α > 0 for any probability
distribution p"
GENERALIZED ENTROPY MEASURES,0.16857142857142857,"Hα(p) :=
1
1 −α ln
X"
GENERALIZED ENTROPY MEASURES,0.16952380952380952,"k
pα
k
(α ̸= 1)"
GENERALIZED ENTROPY MEASURES,0.17047619047619048,"derived as the most general measure of uncertainty in p satisfying four intuitively evident postulates.
The entropy measures the average information and the order parameter α relates to the power of the
corresponding mean statistic [44]. The general formula above includes the Shannon’s entropy"
GENERALIZED ENTROPY MEASURES,0.17142857142857143,"H(p) = −
X"
GENERALIZED ENTROPY MEASURES,0.17238095238095238,"k
pk ln pk"
GENERALIZED ENTROPY MEASURES,0.17333333333333334,"as a special case when α →1. The quadratic or second-order Rényi entropy
164"
GENERALIZED ENTROPY MEASURES,0.1742857142857143,"\la be l  {
e"
GENERALIZED ENTROPY MEASURES,0.17523809523809525,"q
:c
ollision_entropy} H_2 (p) \;:=\; -\ln \sum _k p_k^2 
(6)"
GENERALIZED ENTROPY MEASURES,0.1761904761904762,"is also known as a collision entropy since it is a negative log-likelihood of a “collision” or “rolling
165"
GENERALIZED ENTROPY MEASURES,0.17714285714285713,"double” when two i.i.d. samples from distribution p have equal values.
166"
GENERALIZED ENTROPY MEASURES,0.17809523809523808,"Basic characterization postulates in [33] also lead to the general Rényi formulation of the divergence,
also known as the relative entropy, of order α > 0"
GENERALIZED ENTROPY MEASURES,0.17904761904761904,"Dα(p | q) :=
1
α −1 ln
X"
GENERALIZED ENTROPY MEASURES,0.18,"k
pα
k q1−α
k
(α ̸= 1)"
GENERALIZED ENTROPY MEASURES,0.18095238095238095,"defined for any pair of distributions p and q. This reduces to the standard KL divergence when α →1
167 168"
GENERALIZED ENTROPY MEASURES,0.1819047619047619,"\l ab e
l"
GENERALIZED ENTROPY MEASURES,0.18285714285714286,{e q: KL
GENERALIZED ENTROPY MEASURES,0.1838095238095238,"} D(p,q)=\sum _k p_k\ln \frac {p_k}{q_k} (7)"
GENERALIZED ENTROPY MEASURES,0.18476190476190477,and to the Bhattacharyya distance for α = 1
GENERALIZED ENTROPY MEASURES,0.18571428571428572,"2.
169"
GENERALIZED ENTROPY MEASURES,0.18666666666666668,"Optimization of entropy and divergence [24] is fundamental to many machine learning problems
170"
GENERALIZED ENTROPY MEASURES,0.18761904761904763,"[37, 20, 19, 30], including pattern classification and cluster analysis [36]. However, the entropy-
171"
GENERALIZED ENTROPY MEASURES,0.18857142857142858,"related terminology is often mixed-up. For example, when discussing the cross-entropy minimization
172"
GENERALIZED ENTROPY MEASURES,0.1895238095238095,"principle (MinxEnt), many of the references cited earlier in this paragraph define cross-entropy using
173"
GENERALIZED ENTROPY MEASURES,0.19047619047619047,"the expression for KL-divergence (7). Nowadays, it is standard to define the Shannon’s cross-entropy
174"
GENERALIZED ENTROPY MEASURES,0.19142857142857142,"as
175"
GENERALIZED ENTROPY MEASURES,0.19238095238095237,\l ab e l
GENERALIZED ENTROPY MEASURES,0.19333333333333333,"{
eq :S CE} H(p,q)\;=\;-\sum _k p_k\ln q_k. 
(8)"
GENERALIZED ENTROPY MEASURES,0.19428571428571428,"One simple explanation for the confusion is that KL-divergence D(p, q) and cross-entropy H(p, q)
176"
GENERALIZED ENTROPY MEASURES,0.19523809523809524,"as functions of q only differ by a constant if p is a fixed known target, which is often the case.
177"
COLLISION CROSS-ENTROPY,0.1961904761904762,"3
Collision Cross-Entropy
178"
COLLISION CROSS-ENTROPY,0.19714285714285715,"Minimizing divergence enforces proximity between two distributions, which may work as a loss
for training model predictions σ with labels y, for example, if y are ground truth one-hot labels.
However, if y are pseudo-labels that are estimated jointly with σ, proximity between y and σ is not a
good criterion for the loss. For example, highly uncertain model predictions σ in combination with
uniformly distributed pseudo-labels y correspond to the optimal zero divergence, but this is not a very
useful result for self-labeling. Instead, all existing self-labeling losses for deep clustering minimize
Shannon’s cross-entropy (8) that reduces the divergence and uncertainty at the same time"
COLLISION CROSS-ENTROPY,0.1980952380952381,"H(y, σ) ≡D(y, σ) + H(y)."
COLLISION CROSS-ENTROPY,0.19904761904761906,"The entropy term corresponds to the “decisiveness” constraint in unsupervised discriminative clus-
179"
COLLISION CROSS-ENTROPY,0.2,"tering [3, 17, 1, 15, 16]. In general, it is recommended as a regularizer for unsupervised and
180"
COLLISION CROSS-ENTROPY,0.20095238095238097,"semi-supervised network training [11] to encourage decision boundaries away from the data points
181"
COLLISION CROSS-ENTROPY,0.2019047619047619,"implicitly increasing the decision margins.
182"
COLLISION CROSS-ENTROPY,0.20285714285714285,"We propose a new form of cross-entropy
183"
COLLISION CROSS-ENTROPY,0.2038095238095238,"\la be l { eq
:"
COLLISION CROSS-ENTROPY,0.20476190476190476,"c
ol lision_CE} H_2 (p,q)\;:=\;-\ln \sum _k p_k\, q_k 
(9)"
COLLISION CROSS-ENTROPY,0.2057142857142857,"that we call collision cross-entropy since it extends the collision entropy in (6). Indeed, (9) is the
184"
COLLISION CROSS-ENTROPY,0.20666666666666667,"negative log-probability of an event that two random variables with (different) distributions p and q
185"
COLLISION CROSS-ENTROPY,0.20761904761904762,"are equal. When training softmax σ with pseudo-label distribution y, the collision event is the exact
186"
COLLISION CROSS-ENTROPY,0.20857142857142857,"equality of the predicted class and the pseudo-label, where these are interpreted as specific outcomes
187"
COLLISION CROSS-ENTROPY,0.20952380952380953,"for random variables with distributions σ and y. Note that the collision event, i.e. the equality of
188"
COLLISION CROSS-ENTROPY,0.21047619047619048,"two random variables, has very little to do with the equality of distributions σ = y. The collision
189"
COLLISION CROSS-ENTROPY,0.21142857142857144,"may happen when σ ̸= y, as long as σ · y > 0. Vice versa, this event is not guaranteed even when
190"
COLLISION CROSS-ENTROPY,0.2123809523809524,"σ = y. It will happen almost surely only if the two distributions are the same one-hot. However, if
191"
COLLISION CROSS-ENTROPY,0.21333333333333335,"the distributions are both uniform, the collision probability is only 1/K.
192"
COLLISION CROSS-ENTROPY,0.21428571428571427,"As easy to check, the collision cross-entropy (9) can be equivalently represented as"
COLLISION CROSS-ENTROPY,0.21523809523809523,"H2(p, q) ≡−ln cos(p, q) + H2(p) + H2(q) 2"
COLLISION CROSS-ENTROPY,0.21619047619047618,"where cos(p, q) is the cosine of the angle between p and q as vectors in RK and H2 is the collision
193"
COLLISION CROSS-ENTROPY,0.21714285714285714,"entropy (6). The first term corresponds to a “distance” between the two distributions: it is non-
194"
COLLISION CROSS-ENTROPY,0.2180952380952381,"negative, equals 0 iff p = q, and −ln cos(·) is a convex function of an angle, which can be interpreted
195"
COLLISION CROSS-ENTROPY,0.21904761904761905,"as a spherical metric. Thus, analogously to the Shannon’s cross-entropy, H2 is the sum of divergence
196"
COLLISION CROSS-ENTROPY,0.22,"and entropy.
197"
COLLISION CROSS-ENTROPY,0.22095238095238096,"The formula (9) can be found as a definition of quadratic Rényi cross-entropy [30, 32, 46]. However,
198"
COLLISION CROSS-ENTROPY,0.2219047619047619,"we could not identify information-theoretic axioms characterizing a generalized cross-entropy. Rényi
199"
COLLISION CROSS-ENTROPY,0.22285714285714286,"himself did not discuss the concept of cross-entropy in his seminal work [33]. Also, two different
200"
COLLISION CROSS-ENTROPY,0.22380952380952382,"formulations of “natural” and “shifted” Rényi cross-entropy of arbitrary order could be found in
201"
COLLISION CROSS-ENTROPY,0.22476190476190477,"[44, 42]. In particular, the shifted version of order 2 agrees with our formulation of collision cross-
202"
COLLISION CROSS-ENTROPY,0.2257142857142857,"entropy (9). However, lack of postulates or characterization for the cross-entropy, and the existence of
203"
COLLISION CROSS-ENTROPY,0.22666666666666666,"multiple non-equivalent formulations did not give us the confidence to use the name Rényi. Instead,
204"
COLLISION CROSS-ENTROPY,0.2276190476190476,"we use “collision” due to its clear intuitive interpretation of the loss (9). But, the term “cross-entropy”
205"
COLLISION CROSS-ENTROPY,0.22857142857142856,"is used only informally.
206"
COLLISION CROSS-ENTROPY,0.22952380952380952,"The numerical and empirical properties of the collision cross-entropy (9) are sufficiently different
207"
COLLISION CROSS-ENTROPY,0.23047619047619047,"from the Shannons cross-entropy (8). Figure 1 illustrates H2(y, σ) as a function of σ for different
208"
COLLISION CROSS-ENTROPY,0.23142857142857143,"label distributions y. For confident y it behaves the same way as the standard cross entropy H(y, σ),
209"
COLLISION CROSS-ENTROPY,0.23238095238095238,"but softer low-confident labels y naturally have little influence on the training. In contrast, the
210"
COLLISION CROSS-ENTROPY,0.23333333333333334,"standard cross entropy encourages prediction σ to be the exact copy of uncertainty in distribution
211"
COLLISION CROSS-ENTROPY,0.2342857142857143,"y. Self-labeling methods based on H(y, σ) often “prune out” uncertain pseudo-labels [4]. Collision
212"
COLLISION CROSS-ENTROPY,0.23523809523809525,"cross entropy H2(y, σ) makes such heuristics redundant. We also demonstrate the “robustness to
213"
COLLISION CROSS-ENTROPY,0.2361904761904762,"label uncertainty” on an example where the ground truth labels are corrupted by noise, see Fig.2.
214"
COLLISION CROSS-ENTROPY,0.23714285714285716,"This artificial fully-supervised test is used only to compare the robustness of (9) and (8) in complete
215"
COLLISION CROSS-ENTROPY,0.23809523809523808,"isolation from other terms in the self-labeled clustering losses, which are the focus of this work.
216"
COLLISION CROSS-ENTROPY,0.23904761904761904,"Due to the symmetry of the arguments in (9), such robustness of H2(y, σ) also works the other way
217"
COLLISION CROSS-ENTROPY,0.24,"around. Indeed, self-labeling losses are often used for both training σ and estimating y: the loss is
218"
COLLISION CROSS-ENTROPY,0.24095238095238095,"iteratively optimized over predictions σ (i.e. model parameters responsible for it) and over pseudo-
219"
COLLISION CROSS-ENTROPY,0.2419047619047619,"label distribution y. Thus, it helps if y also demonstrates “robustness to prediction uncertainty”.
220"
COLLISION CROSS-ENTROPY,0.24285714285714285,"Soft labels vs noisy labels: Our collision CE for soft labels, represented by distributions y, can
be related to loss functions used for supervised classification with noisy labels [40, 28, 38], which
assume some observed hard target labels l that may not be true due to corruption or “noise”. Instead
of our probability of collision"
COLLISION CROSS-ENTROPY,0.2438095238095238,"Pr(C = T) =
X"
COLLISION CROSS-ENTROPY,0.24476190476190476,"k
Pr(C = k, T = k) =
X"
COLLISION CROSS-ENTROPY,0.24571428571428572,"k
σkyk ≡y⊤σ"
COLLISION CROSS-ENTROPY,0.24666666666666667,"between the predicted class C and unknown true class T, whose distributions are prediction σ and
221"
COLLISION CROSS-ENTROPY,0.24761904761904763,"soft target y, they maximize the probability that a random variable L representing a corrupted target
222"
COLLISION CROSS-ENTROPY,0.24857142857142858,"equals the observed value l
223"
COLLISION CROSS-ENTROPY,0.24952380952380954,"Pr(L = l) =
X"
COLLISION CROSS-ENTROPY,0.25047619047619046,"k
Pr(L = l|T = k) Pr(T = k) ≈
X"
COLLISION CROSS-ENTROPY,0.25142857142857145,"k
Pr(L = l|T = k) σk ≡Ql σ"
COLLISION CROSS-ENTROPY,0.2523809523809524,"where the approximation uses the model predictions σk instead of true class probabilities Pr(T = k),
224"
COLLISION CROSS-ENTROPY,0.25333333333333335,"which is a significant assumption. Vector Ql is the l-th row of the transition matrix Q, such that
225"
COLLISION CROSS-ENTROPY,0.2542857142857143,"Qlk = Pr(L = l|T = k), that has to be obtained in addition to hard noisy labels l.
226"
COLLISION CROSS-ENTROPY,0.25523809523809526,"Our approach maximizing the collision probability based on soft labels y is a generalization of the
227"
COLLISION CROSS-ENTROPY,0.2561904761904762,"methods for hard noisy labels. Their transitional matrix Q can be interpreted as an operator for
228"
COLLISION CROSS-ENTROPY,0.2571428571428571,"converting any hard label l into a soft label y = Q⊤1l = Ql. Then, the two methods are numerically
229"
COLLISION CROSS-ENTROPY,0.2580952380952381,"equivalent, though our statistical motivation is significantly different. Moreover, our approach is more
230"
COLLISION CROSS-ENTROPY,0.259047619047619,"general since it applies to a wider set of problems where the class target T can be directly specified
231"
COLLISION CROSS-ENTROPY,0.26,"by a distribution, a soft label y, representing the target uncertainty. For example, in fully supervised
232"
COLLISION CROSS-ENTROPY,0.26095238095238094,"classification or segmentation the human annotator can directly indicate uncertainty (odds) for classes
233"
COLLISION CROSS-ENTROPY,0.2619047619047619,"present in the image or at a specific pixel. In fact, class ambiguity is common in many data sets,
234"
COLLISION CROSS-ENTROPY,0.26285714285714284,"though for efficiency, the annotators are typically forced to provide one hard label. Moreover, in the
235"
COLLISION CROSS-ENTROPY,0.2638095238095238,"context of self-supervised clustering, it is natural to estimate pseudo-labels as soft distributions y.
236"
COLLISION CROSS-ENTROPY,0.26476190476190475,"Such methods directly benefit from our collision CE, as this paper shows.
237"
OUR SELF-LABELING LOSS AND EM,0.26571428571428574,"4
Our Self-labeling Loss and EM
238"
OUR SELF-LABELING LOSS AND EM,0.26666666666666666,"Based on prior work (5), we replace the standard cross-entropy with our collision cross-entropy to
239"
OUR SELF-LABELING LOSS AND EM,0.26761904761904765,"formulate our self-labeling loss as follows:
240"
OUR SELF-LABELING LOSS AND EM,0.26857142857142857,"\l
ab
el {e q:  o ur CCE} L_{CCE}\;\;:=\;\;\overline {H_2(y,\sigma )}+\lambda \,KL(\bar {y}\|u) 
(10)"
OUR SELF-LABELING LOSS AND EM,0.2695238095238095,"To optimize such loss, we iterate between two alternating steps for σ and y. For σ, we use the standard
241"
OUR SELF-LABELING LOSS AND EM,0.2704761904761905,"stochastic gradient descent algorithms[34]. For y, we use the projected gradient descent (PGD) [7].
242"
OUR SELF-LABELING LOSS AND EM,0.2714285714285714,"However, the speed of PGD is slow as shown in Table 1 especially when there are more classes. This
243"
OUR SELF-LABELING LOSS AND EM,0.2723809523809524,"motivates us to find more efficient algorithms for optimizing y. To derive such an algorithm, we made
244"
OUR SELF-LABELING LOSS AND EM,0.2733333333333333,"a minor change to (10) by switching the order of variables in the divergence term:
245"
OUR SELF-LABELING LOSS AND EM,0.2742857142857143,"\la
be
l {eq : o u r CCE+} L_{CCE+}\;\;:=\;\;\overline {H_2(y,\sigma )}+\lambda \,KL(u\|\bar {y}) 
(11)"
OUR SELF-LABELING LOSS AND EM,0.2752380952380952,"Such change allows us to use the Jensen’s inequality on the divergence term to derive an efficient EM
246"
OUR SELF-LABELING LOSS AND EM,0.2761904761904762,"algorithm while the quality of the self-labeled classification results is almost the same as shown in
247"
OUR SELF-LABELING LOSS AND EM,0.27714285714285714,"the Appendix D.
248"
OUR SELF-LABELING LOSS AND EM,0.2780952380952381,"EM algorithm for optimizing y
We derive the EM algorithm introducing latent variables, K
249"
OUR SELF-LABELING LOSS AND EM,0.27904761904761904,"distributions Sk ∈∆M representing normalized support for each cluster over M data points. We
250"
OUR SELF-LABELING LOSS AND EM,0.28,"refer to each vector Sk as a normalized cluster k. Note the difference with distributions represented
251"
OUR SELF-LABELING LOSS AND EM,0.28095238095238095,"by pseudo-labels y ∈∆K showing support for each class at a given data point. Since we explicitly
252"
OUR SELF-LABELING LOSS AND EM,0.2819047619047619,"use individual data points below, we will start to carefully index them by i ∈{1, . . . , M}. Thus, we
253"
OUR SELF-LABELING LOSS AND EM,0.28285714285714286,"will use yi ∈∆K and σi ∈∆K. Individual components of distribution Sk ∈∆M corresponding to
254"
OUR SELF-LABELING LOSS AND EM,0.2838095238095238,"data point i will be denoted by scalar Sk
i .
255"
OUR SELF-LABELING LOSS AND EM,0.28476190476190477,"First, we expand (11) introducing the latent variables Sk ∈∆M
  \"
OUR SELF-LABELING LOSS AND EM,0.2857142857142857,"label
 {
eq:EM  i n i tial } \
hspa"
OUR SELF-LABELING LOSS AND EM,0.2866666666666667,"c
e {-3 ex } L
_"
OUR SELF-LABELING LOSS AND EM,0.2876190476190476,"{
CC E+
}"
OUR SELF-LABELING LOSS AND EM,0.2885714285714286,"\
;\
;
&\
e
qc
 \ ;
\;\ov er l i
n e "
OUR SELF-LABELING LOSS AND EM,0.2895238095238095,"{
H_2(
y ,\
si
g
ma
 )}+\lambda \,H(u,\bar {y})\\\small &=\;\;\overline {H_2(y,\sigma )}-\lambda \,\sum _{k}u^k\ln {\sum _{i}S_i^k\frac {y_i^k}{S_i^k M}} \leq \;\;\overline {H_2(y,\sigma )}-\lambda \,\sum _{k}\sum _{i}u^k S_i^k \ln {\frac {y_i^k}{S_i^k M}} \label {eq:EM derivation}
(13)"
OUR SELF-LABELING LOSS AND EM,0.2904761904761905,"Due to the convexity of negative log, we apply the Jensen’s inequality to derive an upper bound, i.e.
257"
OUR SELF-LABELING LOSS AND EM,0.2914285714285714,"(13), to LCCE+. Such bound becomes tight when:
258"
OUR SELF-LABELING LOSS AND EM,0.2923809523809524,"\la b
el
 {
eq
:
E"
OUR SELF-LABELING LOSS AND EM,0.29333333333333333,"st
ep} \textbf {E step}:\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;S_i^k=\frac {y_i^k}{\sum _{j}y_j^k}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (14)"
OUR SELF-LABELING LOSS AND EM,0.29428571428571426,"Next,
we
derive
the
M
step.
Introducing
the
hidden
variable
S
breaks
the
259"
OUR SELF-LABELING LOSS AND EM,0.29523809523809524,"fairness
term
into
the
sum
of
independent
terms
for
pseudo-labels
yi
∈
∆K
260"
OUR SELF-LABELING LOSS AND EM,0.29619047619047617,"at
each
data
point
i.
The
solution
for
S
does
not
change
(E
step).
Lets
261"
OUR SELF-LABELING LOSS AND EM,0.29714285714285715,"running time in sec.
number of iterations
running time in sec.
per iteration
(to convergence)
(to convergence)"
OUR SELF-LABELING LOSS AND EM,0.2980952380952381,"K
2
20
200
2
20
200
2
20
200"
OUR SELF-LABELING LOSS AND EM,0.29904761904761906,"PGD (η1)
7.8e−4
2.9e−3
6.7e−2
326
742
540
0.25
2.20
36.25
PGD (η2)
9.3e−4
3.3e−3
6.8e−2
101
468
344
0.09
1.55
23.35
PGD (η3)
9.9e−4
3.2e−3
7.0e−2
24
202
180
0.02
0.65
12.60"
OUR SELF-LABELING LOSS AND EM,0.3,"our EM
1.8e−3
1.6e−3
5.1e−3
25
53
71
0.04
0.09
0.36"
OUR SELF-LABELING LOSS AND EM,0.30095238095238097,"Table 1: Comparison of our EM algorithm to Projected
Gradient Descent (PGD). η is the step size. For K = 2,
η1 ∼η3 are 1, 10 and 20 respectively. For K = 20 and
K = 200, η1 ∼η3 are 0.1, 1 and 5 respectively. Higher
step size leads to divergence of PGD."
OUR SELF-LABELING LOSS AND EM,0.3019047619047619,"focus on the loss with respect to y. The col-
262"
OUR SELF-LABELING LOSS AND EM,0.3028571428571429,"lision cross-entropy (CCE) also breaks into
263"
OUR SELF-LABELING LOSS AND EM,0.3038095238095238,"the sum of independent parts for each yi. For
264"
OUR SELF-LABELING LOSS AND EM,0.3047619047619048,"simplicity, we will drop all indices i in vari-
265"
OUR SELF-LABELING LOSS AND EM,0.3057142857142857,"ables yk
i , Sk
i , σk
i . Then, the combination of
266"
OUR SELF-LABELING LOSS AND EM,0.30666666666666664,"CCE loss with the corresponding part of the
267"
OUR SELF-LABELING LOSS AND EM,0.3076190476190476,"fairness constraint can be written for each
268"
OUR SELF-LABELING LOSS AND EM,0.30857142857142855,"y = {yk} ∈∆K as
269 \
l"
OUR SELF-LABELING LOSS AND EM,0.30952380952380953,"a
bel 
{ e
q"
OUR SELF-LABELING LOSS AND EM,0.31047619047619046,":
RCE- EM -loss1} -\ln \sum _k \sigma _k y_ k \;\;-\;\;\lambda \sum _k u_k S_k \ln y_k. 
(15)"
OUR SELF-LABELING LOSS AND EM,0.31142857142857144,"First, observe that this loss must achieve its global optimum in the interior of the simplex if Sk > 0
270"
OUR SELF-LABELING LOSS AND EM,0.31238095238095237,"and uk > 0 for all k. Indeed, the second term enforces the “log-barier” at the boundary of the
271"
OUR SELF-LABELING LOSS AND EM,0.31333333333333335,"simplex. Thus, we do not need to worry about KKT conditions in this case. Note that Sk might be
272"
OUR SELF-LABELING LOSS AND EM,0.3142857142857143,"zero, in which case we need to consider the full KKT conditions. However, the Property 1 that will
273"
OUR SELF-LABELING LOSS AND EM,0.31523809523809526,"be mentioned later eliminates such concern if we use positive initialization. For completeness, we
274"
OUR SELF-LABELING LOSS AND EM,0.3161904761904762,"also give the detailed derivation for such case and it can be found in the Appendix B.
275"
OUR SELF-LABELING LOSS AND EM,0.3171428571428571,"Adding the Lagrange multiplier γ for the simplex constraint, we get an unconstrained loss −ln
X"
OUR SELF-LABELING LOSS AND EM,0.3180952380952381,"k
σkyk
−λ
X"
OUR SELF-LABELING LOSS AND EM,0.319047619047619,"k
ukSk ln yk
+ γ X"
OUR SELF-LABELING LOSS AND EM,0.32,"k
yk −1 !"
OUR SELF-LABELING LOSS AND EM,0.32095238095238093,"that must have a stationary point inside the simplex. The following theorem indicates the way to
276"
OUR SELF-LABELING LOSS AND EM,0.3219047619047619,"solve the problem above. All the missing proofs can be found in Appendix A.
277"
OUR SELF-LABELING LOSS AND EM,0.32285714285714284,Theorem 1. [M-step solution]: The sum P
OUR SELF-LABELING LOSS AND EM,0.3238095238095238,"k yk as in (16) is positive, continuous, convex, and
278"
OUR SELF-LABELING LOSS AND EM,0.32476190476190475,"monotonically decreasing function of x on the specified interval. Moreover, there exists a unique
279"
OUR SELF-LABELING LOSS AND EM,0.32571428571428573,"solution {yk} ∈∆k and x such that
280 "
OUR SELF-LABELING LOSS AND EM,0.32666666666666666,"\l
a
b"
OUR SELF-LABELING LOSS AND EM,0.32761904761904764,"e
l {eq
:RCE-M- st"
OUR SELF-LABELING LOSS AND EM,0.32857142857142857,"e
p
2
}\f
o o
t
note
size \ r esiz
ebox {.9\hsize }{!}{$ \sum _k y_k \;\;\equiv \;\;\sum _k \frac {\lambda u_k S_k}{\lambda u^\top S + 1 - \frac {\sigma _k}{x}} \;\; = \;\; 1 \;\;\;\text {and}\;\;\;\; x\in \left (\frac {\sigma _{max}}{1+\lambda u^\top S} \, ,\,\sigma _{max}\right ]$} (16)"
OUR SELF-LABELING LOSS AND EM,0.3295238095238095,"The monotonicity and convexity of P
k yk with respect to x suggest that the problem (16) formulated
281"
OUR SELF-LABELING LOSS AND EM,0.3304761904761905,"in Theorem 1 allows efficient algorithms for finding the corresponding unique solution. For example,
282"
OUR SELF-LABELING LOSS AND EM,0.3314285714285714,"one can use the iterative Newton’s updates to search for x in the specified interval. The following
283"
OUR SELF-LABELING LOSS AND EM,0.3323809523809524,"Lemma gives us a proper starting point
284"
OUR SELF-LABELING LOSS AND EM,0.3333333333333333,"Lemma 1. Assuming ukSk is positive for each k, then the reachable left end point in Theorem 1 can
be written as
l := max
k
σk
1 + λu⊤S −λukSk
."
OUR SELF-LABELING LOSS AND EM,0.3342857142857143,"for Newton’s method. The algorithm for M-step solution is summarized in Algorithm 1 in Appendix
285"
OUR SELF-LABELING LOSS AND EM,0.3352380952380952,"C. Note that we present the algorithm for only one data point, and we can easily and efficiently scale
286"
OUR SELF-LABELING LOSS AND EM,0.3361904761904762,"up for more data in a batch by using the Numba compiler. In the following, we give the property
287"
OUR SELF-LABELING LOSS AND EM,0.33714285714285713,"about the positivity of the solution. This property implies that if our EM algorithm has only (strictly)
288"
OUR SELF-LABELING LOSS AND EM,0.3380952380952381,"positive variables Sk or yk at initialization, these variables will remain positive during all iterations.
289"
OUR SELF-LABELING LOSS AND EM,0.33904761904761904,"Property 1. For any category k such that uk > 0, the set of strictly positive variables yk or Sk can
290"
OUR SELF-LABELING LOSS AND EM,0.34,"only grow during iterations of our EM algorithm for the loss (15) based on the collision cross-entropy.
291"
OUR SELF-LABELING LOSS AND EM,0.34095238095238095,"Note that Property 1 does not rule out the possibility that yk may become arbitrarily close to zero
292"
OUR SELF-LABELING LOSS AND EM,0.3419047619047619,"during EM iterations. Empirically, we did not observe any numerical issues. The complete algorithm
293"
OUR SELF-LABELING LOSS AND EM,0.34285714285714286,"is given in Appendix C. Inspired by [39, 15], we also update our y in each batch. Intuitively, updating
294"
OUR SELF-LABELING LOSS AND EM,0.3438095238095238,"y on the fly can prevent the network from being easily trapped in some local minima created by the
295"
OUR SELF-LABELING LOSS AND EM,0.34476190476190477,"incorrect pseudo-labels.
296"
EXPERIMENTS,0.3457142857142857,"5
Experiments
297"
EXPERIMENTS,0.3466666666666667,"We apply our new loss to self-labeled classification problems in both shallow and deep settings, as
298"
EXPERIMENTS,0.3476190476190476,"well as semi-supervised modes. All the results are reproduced using either public codes or our own
299"
EXPERIMENTS,0.3485714285714286,"implementation under the same experimental settings for fair comparison. Our approach consistently
300"
EXPERIMENTS,0.3495238095238095,"achieves either the best or highly competitive results across all the datasets and is therefore more
301"
EXPERIMENTS,0.3504761904761905,"robust. All the missing details in the experiments can be found in Appendix E.
302"
EXPERIMENTS,0.3514285714285714,"Dataset
We use four standard datasets: MNIST [25], CIFAR10/100 [43] and STL10 [8]. The
303"
EXPERIMENTS,0.3523809523809524,"training and test data are the same unless otherwise specified.
304"
EXPERIMENTS,0.35333333333333333,"Evaluation
As for the evaluation of self-labeled classification, we set the number of clusters to
305"
EXPERIMENTS,0.35428571428571426,"the number of ground-truth categories. To calculate the accuracy, we use the standard Hungarian
306"
EXPERIMENTS,0.35523809523809524,"algorithm [23] to find the best one-to-one mapping between clusters and labels. We don’t need this
307"
EXPERIMENTS,0.35619047619047617,"matching step if we use other metrics, i.e. NMI, ARI.
308"
CLUSTERING WITH FIXED FEATURES,0.35714285714285715,"5.1
Clustering with Fixed Features
309"
CLUSTERING WITH FIXED FEATURES,0.3580952380952381,"STL10
CIFAR10
CIFAR100-20
MNIST"
CLUSTERING WITH FIXED FEATURES,0.35904761904761906,"Kmeans
85.20%(5.9)
67.78%(4.6)
42.99%(1.3)
47.62%(2.1)
MIGD [22]
89.56%(6.4)
72.32%(5.8)
43.59%(1.1)
52.92%(3.0)
SeLa [1]
90.33%(4.8)
63.31%(3.7)
40.74%(1.1)
52.38%(5.2)
MIADM [16]
88.64%(7.1)
60.57%(3.3)
41.2%(1.4)
50.61%(1.3)"
CLUSTERING WITH FIXED FEATURES,0.36,"Our
92.33%(6.4)
73.51%(6.3)
43.72%(1.1)
58.4%(3.2)"
CLUSTERING WITH FIXED FEATURES,0.36095238095238097,"Table 2: Comparison of different methods on clustering
with fixed features extracted from Resnet-50. The num-
bers are the average accuracy and the standard deviation
over trials. We use the 20 coarse categories for CIFAR100
similarly to others."
CLUSTERING WITH FIXED FEATURES,0.3619047619047619,"In this section, we test our loss as a proper clus-
310"
CLUSTERING WITH FIXED FEATURES,0.3628571428571429,"tering loss and compare it to the widely used
311"
CLUSTERING WITH FIXED FEATURES,0.3638095238095238,"Kmeans (generative) and other closely related
312"
CLUSTERING WITH FIXED FEATURES,0.3647619047619048,"losses (entropy-based and discriminative). We
313"
CLUSTERING WITH FIXED FEATURES,0.3657142857142857,"use the pretrained (ImageNet) Resnet-50 [14]
314"
CLUSTERING WITH FIXED FEATURES,0.36666666666666664,"to extract the features. For Kmeans, the model
315"
CLUSTERING WITH FIXED FEATURES,0.3676190476190476,"is parameterized by K cluster centers. Com-
316"
CLUSTERING WITH FIXED FEATURES,0.36857142857142855,"parably, we use a one-layer linear classifier
317"
CLUSTERING WITH FIXED FEATURES,0.36952380952380953,"followed by softmax for all other losses includ-
318"
CLUSTERING WITH FIXED FEATURES,0.37047619047619046,"ing ours. Kmeans results were obtained using
319"
CLUSTERING WITH FIXED FEATURES,0.37142857142857144,"scikit-learn package in Python. To optimize
320"
CLUSTERING WITH FIXED FEATURES,0.37238095238095237,"the model parameters for other losses, we use
321"
CLUSTERING WITH FIXED FEATURES,0.37333333333333335,"stochastic gradient descent. Here we report the average accuracy and standard deviation over 6
322"
CLUSTERING WITH FIXED FEATURES,0.3742857142857143,"randomly initialized trials in Table 2.
323"
DEEP CLUSTERING,0.37523809523809526,"5.2
Deep Clustering
324"
DEEP CLUSTERING,0.3761904761904762,"STL10
CIFAR10
CIFAR100-20
MNIST"
DEEP CLUSTERING,0.37714285714285717,"IMSAT [15]
25.28%(0.5)
21.4%(0.5)
14.39%(0.7)
92.90%(6.3)
IIC [17]
24.12%(1.7)
21.3%(1.4)
12.58%(0.6)
82.51%(2.3)
SeLa [1]
23.99%(0.9)
24.16%(1.5)
15.34%(0.3)
52.86%(1.9)
MIADM [16]
23.37%(0.9)
23.26%(0.6)
14.02%(0.5)
78.88%(3.3)"
DEEP CLUSTERING,0.3780952380952381,"Our
25.98%(1.1)
24.26%(0.8)
15.14%(0.5)
95.11%(4.3)
Table 3:
Quantitative comparison of discriminative
clustering-based classification methods with simultaneous
feature training from the scratch. The network architecture
is VGG-4. We reuse the code published by [17, 1, 15] and
use our improved implementation of [16] (also for other
tables)."
DEEP CLUSTERING,0.379047619047619,"In this section, we train a deep network to
325"
DEEP CLUSTERING,0.38,"jointly learn the features and cluster the data.
326"
DEEP CLUSTERING,0.38095238095238093,"We test our method on both a small architec-
327"
DEEP CLUSTERING,0.3819047619047619,"ture (VGG4) and a large one (ResNet-18). The
328"
DEEP CLUSTERING,0.38285714285714284,"only extra standard technique we add here is
329"
DEEP CLUSTERING,0.3838095238095238,"self-augmentation following [15, 1, 6].
330"
DEEP CLUSTERING,0.38476190476190475,"To train the VGG4, we use random initial-
331"
DEEP CLUSTERING,0.38571428571428573,"ization for network parameters.
From Ta-
332"
DEEP CLUSTERING,0.38666666666666666,"ble 3, it can be seen that our approach con-
333"
DEEP CLUSTERING,0.38761904761904764,"sistently achieves the most competitive re-
334"
DEEP CLUSTERING,0.38857142857142857,"sults in terms of accuracy (ACC). Most of the methods we compared in our work (including
335"
DEEP CLUSTERING,0.38952380952380955,"our method) are general concepts applicable to single-stage end-to-end training.
To be fair,
336"
DEEP CLUSTERING,0.3904761904761905,"we tested all of them on the same simple architecture.
But, these general methods can be
337"
DEEP CLUSTERING,0.3914285714285714,"easily integrated into other more complex systems with larger architecture such as ResNet-18.
338"
DEEP CLUSTERING,0.3923809523809524,"CIFAR10
CIFAR100-20
STL10"
DEEP CLUSTERING,0.3933333333333333,"ACC
NMI
ARI
ACC
NMI
ARI
ACC
NMI
ARI"
DEEP CLUSTERING,0.3942857142857143,"SCAN [45]
81.8%
(0.3)
71.2%
(0.4)
66.5%
(0.4)
42.2%
(3.0)
44.1%
(1.0)
26.7%
(1.3)
75.5%
(2.0)
65.4%
(1.2)
59.0%
(1.6)
IMSAT [15]
77.64%
(1.3)
71.05%
(0.4)
64.85%
(0.3)
43.68%
(0.4)
42.92%
(0.2)
26.47%
(0.1)
70.23%
(2.0)
62.22%
(1.2)
53.54%
(1.1)
MIADM [16]
74.76%
(0.3)"
DEEP CLUSTERING,0.3952380952380952,"69.17%
(0.2)"
DEEP CLUSTERING,0.3961904761904762,"62.51%
(0.2)"
DEEP CLUSTERING,0.39714285714285713,"43.47%
(0.5)"
DEEP CLUSTERING,0.3980952380952381,"42.85%
(0.4)"
DEEP CLUSTERING,0.39904761904761904,"27.78%
(0.4)"
DEEP CLUSTERING,0.4,"67.84%
(0.2)"
DEEP CLUSTERING,0.40095238095238095,"60.33%
(0.5)"
DEEP CLUSTERING,0.40190476190476193,"51.67%
(0.6)"
DEEP CLUSTERING,0.40285714285714286,"Our
83.27%
(0.2)"
DEEP CLUSTERING,0.4038095238095238,"71.95%
(0.2)"
DEEP CLUSTERING,0.40476190476190477,"68.15%
(0.1)"
DEEP CLUSTERING,0.4057142857142857,"47.01%
(0.2)"
DEEP CLUSTERING,0.4066666666666667,"43.28%
(0.1)"
DEEP CLUSTERING,0.4076190476190476,"29.11%
(0.1)"
DEEP CLUSTERING,0.4085714285714286,"78.12%
(0.1)"
DEEP CLUSTERING,0.4095238095238095,"68.11%
(0.3)"
DEEP CLUSTERING,0.4104761904761905,"62.34%
(0.3)"
DEEP CLUSTERING,0.4114285714285714,"Table 4: Quantitative comparison using network ResNet-
18. The most related work MIADM (5) is also highlighted
in all tables."
DEEP CLUSTERING,0.4123809523809524,"In Table 4, we show the results using the
339"
DEEP CLUSTERING,0.41333333333333333,"pretext-trained network from SCAN [45] as
340"
DEEP CLUSTERING,0.4142857142857143,"initialization for our clustering loss as well as
341"
DEEP CLUSTERING,0.41523809523809524,"IMSAT and MIADM. We use only the cluster-
342"
DEEP CLUSTERING,0.41619047619047617,"ing loss together with the self-augmentation
343"
DEEP CLUSTERING,0.41714285714285715,"(one augmentation per image). As shown in
344"
DEEP CLUSTERING,0.4180952380952381,"the table below, our method reaches a higher
345"
DEEP CLUSTERING,0.41904761904761906,"number with more robustness almost for every
346"
DEEP CLUSTERING,0.42,"metric on all datasets compared to the SOTA
347"
DEEP CLUSTERING,0.42095238095238097,"method SCAN. More importantly, we consis-
348"
DEEP CLUSTERING,0.4219047619047619,"tently improve over the most related method, MIADM, by a large margin, which clearly demonstrates
349"
DEEP CLUSTERING,0.4228571428571429,"the effectiveness of our proposed loss together with the optimization algorithm.
350"
SEMI-SUPERVISED CLASSIFICATION,0.4238095238095238,"5.3
Semi-supervised Classification
351"
SEMI-SUPERVISED CLASSIFICATION,0.4247619047619048,"Although our paper is focused on self-labeled classification, we find it also interesting and natural to
352"
SEMI-SUPERVISED CLASSIFICATION,0.4257142857142857,"test our loss under semi-supervised settings where partial data is provided with ground-truth labels.
353"
SEMI-SUPERVISED CLASSIFICATION,0.4266666666666667,"We use the standard cross-entropy loss for labeled data and directly add it to the self-labeled loss to
354"
SEMI-SUPERVISED CLASSIFICATION,0.4276190476190476,"train the network initialized by the pretext-trained network following [45].
355"
CONCLUSION,0.42857142857142855,"6
Conclusion
356"
CONCLUSION,0.42952380952380953,"0.1
0.05
0.01"
CONCLUSION,0.43047619047619046,"STL10
CIFAR10
STL10
CIFAR10
STL10
CIFAR10"
CONCLUSION,0.43142857142857144,"Only seeds
78.4%
81.2%
74.1%
76.8%
68.8%
71.8%
+ IMSAT [15]
88.1%
91.5%
81.1%
85.2%
74.1%
80.2%
+ IIC [17]
85.2%
90.3%
78.2%
84.8%
72.5%
80.5%
+ SeLa [1]
86.2%
88.6%
79.5%
82.7%
69.9%
79.1%
+ MIADM [16]
84.9%
86.1%
77.9%
80.1%
69.6%
77.5%"
CONCLUSION,0.43238095238095237,"+ Our
88.9%
92.3%
82.9%
86.2%
75.7%
82.4%"
CONCLUSION,0.43333333333333335,"Table 5: Quantitative results for semi-supervised classi-
fication on STL10 and CIFAR10 using ResNet18. The
numbers 0.1, 0.05 and 0.01 correspond to different ratio
of labels used for supervision. “Only seeds” means we
only use standard cross-entropy loss on seeds for training."
CONCLUSION,0.4342857142857143,"We propose a new collision cross-entropy loss.
357"
CONCLUSION,0.43523809523809526,"Such loss is naturally interpreted as measur-
358"
CONCLUSION,0.4361904761904762,"ing the probability of the equality between two
359"
CONCLUSION,0.43714285714285717,"random variables represented by the two distri-
360"
CONCLUSION,0.4380952380952381,"butions σ and y, which perfectly fits the goal of
361"
CONCLUSION,0.439047619047619,"self-labeled classification. It is symmetric w.r.t.
362"
CONCLUSION,0.44,"the two distributions instead of treating one
363"
CONCLUSION,0.44095238095238093,"as the target, like the standard cross-entropy.
364"
CONCLUSION,0.4419047619047619,"While the latter makes the network copy the uncertainty in estimated pseudo-labels, our cross-entropy
365"
CONCLUSION,0.44285714285714284,"naturally weakens the training on data points where pseudo labels are more uncertain. This makes
366"
CONCLUSION,0.4438095238095238,"our cross-entropy robust to labeling errors. In fact, the robustness works both for prediction and for
367"
CONCLUSION,0.44476190476190475,"pseudo-labels due to the symmetry. We also developed an efficient EM algorithm for optimizing the
368"
CONCLUSION,0.44571428571428573,"pseudo-labels. Such EM algorithm takes much less time compared to the standard projected gradient
369"
CONCLUSION,0.44666666666666666,"descent. Experimental results show that our method consistently produces top or near-top results on
370"
CONCLUSION,0.44761904761904764,"all tested clustering and semi-supervised benchmarks.
371"
REFERENCES,0.44857142857142857,"References
372"
REFERENCES,0.44952380952380955,"[1] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous
373"
REFERENCES,0.4504761904761905,"clustering and representation learning. In International Conference on Learning Representations,
374"
REFERENCES,0.4514285714285714,"2020.
375"
REFERENCES,0.4523809523809524,"[2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
376"
REFERENCES,0.4533333333333333,"[3] John S. Bridle, Anthony J. R. Heading, and David J. C. MacKay. Unsupervised classifiers,
377"
REFERENCES,0.4542857142857143,"mutual information and ’phantom targets’. In NIPS, pages 1096–1101, 1991.
378"
REFERENCES,0.4552380952380952,"[4] Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep
379"
REFERENCES,0.4561904761904762,"adaptive image clustering. In International Conference on Computer Vision (ICCV), pages
380"
REFERENCES,0.45714285714285713,"5879–5887, 2017.
381"
REFERENCES,0.4580952380952381,"[5] Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep
382"
REFERENCES,0.45904761904761904,"adaptive image clustering. In Proceedings of the IEEE international conference on computer
383"
REFERENCES,0.46,"vision, pages 5879–5887, 2017.
384"
REFERENCES,0.46095238095238095,"[6] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings
385"
REFERENCES,0.46190476190476193,"of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750–15758,
386"
REFERENCES,0.46285714285714286,"2021.
387"
REFERENCES,0.4638095238095238,"[7] Yunmei Chen and Xiaojing Ye. Projection onto a simplex, 2011.
388"
REFERENCES,0.46476190476190476,"[8] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuper-
389"
REFERENCES,0.4657142857142857,"vised feature learning. In Proceedings of the fourteenth international conference on artificial
390"
REFERENCES,0.4666666666666667,"intelligence and statistics, pages 215–223. JMLR Workshop and Conference Proceedings, 2011.
391"
REFERENCES,0.4676190476190476,"[9] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in
392"
REFERENCES,0.4685714285714286,"neural information processing systems, 26, 2013.
393"
REFERENCES,0.4695238095238095,"[10] Kamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng, Weidong Cai, and Heng Huang.
394"
REFERENCES,0.4704761904761905,"Deep clustering via joint convolutional autoencoder embedding and relative entropy minimiza-
395"
REFERENCES,0.4714285714285714,"tion. In Proceedings of the IEEE international conference on computer vision, pages 5736–5745,
396"
REFERENCES,0.4723809523809524,"2017.
397"
REFERENCES,0.47333333333333333,"[11] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization.
398"
REFERENCES,0.4742857142857143,"Advances in neural information processing systems, 17, 2004.
399"
REFERENCES,0.47523809523809524,"[12] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
400"
REFERENCES,0.47619047619047616,"networks. In International conference on machine learning, pages 1321–1330. PMLR, 2017.
401"
REFERENCES,0.47714285714285715,"[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
402"
REFERENCES,0.4780952380952381,"Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE
403"
REFERENCES,0.47904761904761906,"international conference on computer vision, pages 1026–1034, 2015.
404"
REFERENCES,0.48,"[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
405"
REFERENCES,0.48095238095238096,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
406"
REFERENCES,0.4819047619047619,"pages 770–778, 2016.
407"
REFERENCES,0.4828571428571429,"[15] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning
408"
REFERENCES,0.4838095238095238,"discrete representations via information maximizing self-augmented training. In International
409"
REFERENCES,0.4847619047619048,"conference on machine learning, pages 1558–1567. PMLR, 2017.
410"
REFERENCES,0.4857142857142857,"[16] Mohammed Jabi, Marco Pedersoli, Amar Mitiche, and Ismail Ben Ayed. Deep clustering: On
411"
REFERENCES,0.4866666666666667,"the link between discriminative models and k-means. IEEE Transactions on Pattern Analysis
412"
REFERENCES,0.4876190476190476,"and Machine Intelligence, 43(6):1887–1896, 2021.
413"
REFERENCES,0.48857142857142855,"[17] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsuper-
414"
REFERENCES,0.4895238095238095,"vised image classification and segmentation. In Proceedings of the IEEE/CVF International
415"
REFERENCES,0.49047619047619045,"Conference on Computer Vision, pages 9865–9874, 2019.
416"
REFERENCES,0.49142857142857144,"[18] Jagat N. Kapur. Measures of Information and Their Applications. John Wiley and Sons, 1994.
417"
REFERENCES,0.49238095238095236,"[19] Jagat N. Kapur and Hiremagalur K. Kesavan. Entropy Optimization Principles and Applications.
418"
REFERENCES,0.49333333333333335,"Springer, 1992.
419"
REFERENCES,0.4942857142857143,"[20] Hiremagalur K. Kesavan and Jagat N. Kapur. Maximum Entropy and Minimum Cross-Entropy
420"
REFERENCES,0.49523809523809526,"Principles: Need for a Broader Perspective, pages 419–432. Springer, 1990.
421"
REFERENCES,0.4961904761904762,"[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR
422"
REFERENCES,0.49714285714285716,"(Poster), 2015.
423"
REFERENCES,0.4980952380952381,"[22] Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized
424"
REFERENCES,0.4990476190476191,"information maximization. Advances in neural information processing systems, 23, 2010.
425"
REFERENCES,0.5,"[23] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics
426"
REFERENCES,0.5009523809523809,"quarterly, 2(1-2):83–97, 1955.
427"
REFERENCES,0.5019047619047619,"[24] Solomon Kullback. Information Theory and Statistics. Wiley, New York, 1959.
428"
REFERENCES,0.5028571428571429,"[25] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
429"
REFERENCES,0.5038095238095238,"recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
430"
REFERENCES,0.5047619047619047,"[26] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?
431"
REFERENCES,0.5057142857142857,"Advances in neural information processing systems, 32, 2019.
432"
REFERENCES,0.5066666666666667,"[27] NSD.
Natural Scenes Dataset [NSD].
https://www.kaggle.com/datasets/
433"
REFERENCES,0.5076190476190476,"nitishabharathi/scene-classification, 2020.
434"
REFERENCES,0.5085714285714286,"[28] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu.
435"
REFERENCES,0.5095238095238095,"Making deep neural networks robust to label noise: A loss correction approach. In Proceedings
436"
REFERENCES,0.5104761904761905,"of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 1944–1952,
437"
REFERENCES,0.5114285714285715,"2017.
438"
REFERENCES,0.5123809523809524,"[29] Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regular-
439"
REFERENCES,0.5133333333333333,"izing neural networks by penalizing confident output distributions. 2017.
440"
REFERENCES,0.5142857142857142,"[30] Jose C. Principe, Dongxin Xu, and John W. Fisher III. Information-theoretic learning. Advances
441"
REFERENCES,0.5152380952380953,"in unsupervised adaptive filtering, 2000.
442"
REFERENCES,0.5161904761904762,"[31] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
443"
REFERENCES,0.5171428571428571,"deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
444"
REFERENCES,0.518095238095238,"[32] Sudhir Rao, Allan de Medeiros Martins, and José C. Príncipe. Mean shift: An information
445"
REFERENCES,0.5190476190476191,"theoretic perspective. Pattern Recognition Letters, 30:222–230, 2009.
446"
REFERENCES,0.52,"[33] Alfréd Rényi. On measures of entropy and information. Fourth Berkeley Symp. Math. Stat.
447"
REFERENCES,0.520952380952381,"Probab., 1:547–561, 1961.
448"
REFERENCES,0.5219047619047619,"[34] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
449"
REFERENCES,0.5228571428571429,"arXiv:1609.04747, 2016.
450"
REFERENCES,0.5238095238095238,"[35] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
451"
REFERENCES,0.5247619047619048,"back-propagating errors. Nature, 323(6088):533–536, 1986.
452"
REFERENCES,0.5257142857142857,"[36] John E. Shore and Robert M. Gray. Minimum cross-entropy pattern classification and cluster
453"
REFERENCES,0.5266666666666666,"analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 11–17, 1982.
454"
REFERENCES,0.5276190476190477,"[37] John E. Shore and Rodney W. Johnson. Axiomatic derivation of the principle of maximum
455"
REFERENCES,0.5285714285714286,"entropy and the principle of minimum cross-entropy. IEEE Transactions on Information Theory,
456"
REFERENCES,0.5295238095238095,"26(1):547–561, 1980.
457"
REFERENCES,0.5304761904761904,"[38] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from
458"
REFERENCES,0.5314285714285715,"noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and
459"
REFERENCES,0.5323809523809524,"Learning Systems, 2022.
460"
REFERENCES,0.5333333333333333,"[39] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical genera-
461"
REFERENCES,0.5342857142857143,"tive adversarial networks. In International Conference on Learning Representations, 2015.
462"
REFERENCES,0.5352380952380953,"[40] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training
463"
REFERENCES,0.5361904761904762,"convolutional networks with noisy labels. ICLR workshop, 2015.
464"
REFERENCES,0.5371428571428571,"[41] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization
465"
REFERENCES,0.5380952380952381,"framework for learning with noisy labels. In Proceedings of the IEEE conference on computer
466"
REFERENCES,0.539047619047619,"vision and pattern recognition, pages 5552–5560, 2018.
467"
REFERENCES,0.54,"[42] Ferenc C. Thierrin, Fady Alajaji, and Tamás Linder. Rényi cross-entropy measures for common
468"
REFERENCES,0.540952380952381,"distributions and processes with memory. Entropy, 24(10), 2022.
469"
REFERENCES,0.5419047619047619,"[43] Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data
470"
REFERENCES,0.5428571428571428,"set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and
471"
REFERENCES,0.5438095238095239,"machine intelligence, 30(11):1958–1970, 2008.
472"
REFERENCES,0.5447619047619048,"[44] Francisco J. Valverde-Albacete and Carmen Peláez-Moreno. The case for shifting the Rényi
473"
REFERENCES,0.5457142857142857,"entropy. Entropy, 21(1), 2019.
474"
REFERENCES,0.5466666666666666,"[45] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc
475"
REFERENCES,0.5476190476190477,"Van Gool. Scan: Learning to classify images without labels. In Computer Vision–ECCV 2020:
476"
REFERENCES,0.5485714285714286,"16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X, pages
477"
REFERENCES,0.5495238095238095,"268–285. Springer, 2020.
478"
REFERENCES,0.5504761904761905,"[46] Xiao-Tong Yuan and Bao-Gang Hu. Robust feature extraction via information theoretic learning.
479"
REFERENCES,0.5514285714285714,"In International Conference on Machine Learning, (ICML), page 1193–1200, 2009.
480"
REFERENCES,0.5523809523809524,"A
Missing proofs
481"
REFERENCES,0.5533333333333333,Theorem 2. [M-step solution]: The sum P
REFERENCES,0.5542857142857143,"k yk as in (17) is positive, continuous, convex, and
482"
REFERENCES,0.5552380952380952,"monotonically decreasing function of x on the specified interval. Moreover, there exists a unique
483"
REFERENCES,0.5561904761904762,"solution {yk} ∈∆k and x such that
484 "
REFERENCES,0.5571428571428572,"\l
a
b"
REFERENCES,0.5580952380952381,"e
l {eq
:RCE-M- st"
REFERENCES,0.559047619047619,"e
p
3
}\f
o o
t
note
size \ r esiz
ebox {.9\hsize }{!}{$ \sum _k y_k \;\;\equiv \;\;\sum _k \frac {\lambda u_k S_k}{\lambda u^\top S + 1 - \frac {\sigma _k}{x}} \;\; = \;\; 1 \;\;\;\text {and}\;\;\;\; x\in \left (\frac {\sigma _{max}}{1+\lambda u^\top S} \, ,\,\sigma _{max}\right ]$} (17)"
REFERENCES,0.56,"Proof. All yk in (17) are positive, continuous, convex, and monotonically decreasing functions of x
485"
REFERENCES,0.560952380952381,"on the specified interval. Thus, P yk behaves similarly. Assuming that max is the index of prediction
486"
REFERENCES,0.5619047619047619,"σmax, we have ymax →+∞when approaching the interval’s left endpoint x →
σmax
1+λu⊤S . Thus,
487
P yk > 1 for smaller values of x. At the right endpoint x = σmax we have yk ≤λukSk"
REFERENCES,0.5628571428571428,"λu⊤S for all k
488"
REFERENCES,0.5638095238095238,"implying P yk ≤1. Monotonicity and continuity of P yk w.r.t. x imply the theorem.
489"
REFERENCES,0.5647619047619048,"Lemma 2. Assuming ukSk is positive for each k, then the reachable left end point in Theorem 1 can
be written as
l := max
k
σk
1 + λu⊤S −λukSk
."
REFERENCES,0.5657142857142857,"Proof. Firstly, we prove that l is (strictly) inside the interior of the interval in Theorem 1. For the left
490"
REFERENCES,0.5666666666666667,"end point, we have
 \n"
REFERENCES,0.5676190476190476,"o nu mbe
r
 l
& : =\ma x  _k{\"
REFERENCES,0.5685714285714286,"f
rac 
{ \ sigm a  _k}{1+\l"
REFERENCES,0.5695238095238095,"a
mbda
 u ^\to
p S - \l am bda u_k S_k}} \\\nonumber &\geq \;\;\;\;\;\frac {\sigma _{max}}{1+\lambda u^\top S-\lambda u_{max} S_{max}} \\\nonumber &> \;\;\;\;\;\frac {\sigma _{max}}{1+\lambda u^\top S} && \text {$u_{max}S_{max}$ is positive}"
REFERENCES,0.5704761904761905,"For the right end point, we have
 \n"
REFERENCES,0.5714285714285714,"o nu mbe
r
 l
& : =\ma x  _k{\
f
rac
 
{\
s i gma _ k}{1+ \ l"
REFERENCES,0.5723809523809524,"a
mbda u^\top S - \lambda u_k S_k}} \\\nonumber &< \;\;\;\;\;\max _k{\sigma _k} && \text {$1+\lambda u^\top S-\lambda u_k S_k>1$}\\\nonumber &=\;\;\;\;\;\sigma _{max}"
REFERENCES,0.5733333333333334,"Therefore, l is a reachable point. Moreover, any
σmax
1+λu⊤S < x < l will still induce positive yk for any
k and we will also use this to prove that x should not be smaller than l. Let"
REFERENCES,0.5742857142857143,"c := arg max
k"
REFERENCES,0.5752380952380952,"σk
1 + λu⊤S −λukSk"
REFERENCES,0.5761904761904761,"then we can substitute l into the x of yc. It can be easily verified that yc = 1 at such l. Since yc is
493"
REFERENCES,0.5771428571428572,"monotonically decreasing in terms of x, any x smaller than l will cause yc to be greater than 1. At
494"
REFERENCES,0.5780952380952381,"the same time, other yk is still positive as mentioned just above, so the P"
REFERENCES,0.579047619047619,"k yk will be greater than 1.
495"
REFERENCES,0.58,"Thus, l is a reachable left end point.
496"
REFERENCES,0.580952380952381,"Property 2. For any category k such that uk > 0, the set of strictly positive variables yk or Sk can
497"
REFERENCES,0.5819047619047619,"only grow during iterations of our EM algorithm for the loss (d) based on the collision cross-entropy.
498"
REFERENCES,0.5828571428571429,"Proof. As obvious from the E-step (14), it is sufficient to prove this for variables yk. If yk = 0, then
499"
REFERENCES,0.5838095238095238,"the E-step (14) gives Sk = 0. According to the M-step for the case of collision cross-entropy, variable
500"
REFERENCES,0.5847619047619048,"yk may become (strictly) positive at the next iteration if σk = σmax. Once yk becomes positive, the
501"
REFERENCES,0.5857142857142857,"following E-step (14) produces Sk > 0. Then, the fairness term effectively enforces the log-barrier
502"
REFERENCES,0.5866666666666667,"from the corresponding simplex boundary making M-step solution yk = 0 prohibitively expensive.
503"
REFERENCES,0.5876190476190476,"Thus, yk will remain strictly positive at all later iterations.
504"
REFERENCES,0.5885714285714285,"B
Complete Solutions for M step
505 \
l"
REFERENCES,0.5895238095238096,"a
bel 
{ e
q"
REFERENCES,0.5904761904761905,":
RCE- EM -loss}\tag {d} -\ln \sum _k \sigma _k y_ k \;\;-\;\;\lambda \sum _k u_k S_k \ln y_k. 
(d)"
REFERENCES,0.5914285714285714,"The main case when ukSk > 0 for all k is presented in the main paper. Here we derive the case when
there exist some k such that ukSk = 0. Assume a non-empty subset of categories/classes"
REFERENCES,0.5923809523809523,"Ko := {k | ukSk = 0}
̸=
∅"
REFERENCES,0.5933333333333334,"and its non-empty complement
¯Ko := {k | ukSk > 0}
̸=
∅."
REFERENCES,0.5942857142857143,"In this case the second term (fairness) in our loss (d) does not depend on variables yk for k ∈Ko.
Also, note that the first term ( collision cross-entropy) in (d) depends on these variables only via their
linear combination P"
REFERENCES,0.5952380952380952,"k∈Ko σkyk. It is easy to see that for any given confidences yk for k ∈¯Ko it is
optimal to put all the remaining confidence 1 −P"
REFERENCES,0.5961904761904762,"k∈¯
Ko yk into one class c ∈Ko corresponding to
the larges prediction among the classes in Ko
c := arg max
k∈Ko
σk"
REFERENCES,0.5971428571428572,"so that
yc = 1 −
X"
REFERENCES,0.5980952380952381,"k∈¯
Ko
yk
and
yk = 0, ∀k ∈Ko \ c."
REFERENCES,0.599047619047619,"Then, our loss function (d) can be written as
506 \
l"
REFERENCES,0.6,"ab e
l {eq:
RCE-
E M
-"
REFERENCES,0.6009523809523809,"lo s
s-
case 2} \tag {e} -\ln \sum _{k\in \bar {K}_o\cup \{c\}} \sigma _k y_ k \;\;-\;\; \lambda \sum _{k\in \bar {K}_o} u_k S_k \ln y_k 
(e)"
REFERENCES,0.6019047619047619,"that gives the Lagrangian function incorporating the probability simplex constraint
507 \ c"
REFERENCES,0.6028571428571429,"en t
ering \non
u m b"
REFERENCES,0.6038095238095238,"er  
\f ootn ot es
i z
e"
REFERENCES,0.6047619047619047,"\r e
sizebo x { \
h
size }{!}{$-\ln \sum _{k\in \bar {K}_o\cup \{c\}} \sigma _k y_ k \;\;-\;\; \lambda \sum _{k\in \bar {K}_o} u_k S_k \ln y_k\;\;+\;\; \gamma \left (\sum _{k\in \bar {K}_o\cup \{c\}} y_k -1\right ).$}"
REFERENCES,0.6057142857142858,"The stationary point for this Lagrangian function should satisfy equations
508 \"
REFERENCES,0.6066666666666667,cen t ering
REFERENCES,0.6076190476190476,"\n o n
u
mb
er  \fo
otn
o
te
siz e  
\
resizebox {\hsize }{!}{$ -\frac {\sigma _k}{\sigma ^\top y} - \lambda u_k S_k \frac {1}{y_k} + \gamma \;\;=\;\;0 ,\;\;\;\forall k\in \bar {K}_o\;\;\;\;\;\;\;\;\;\;\text {and}\;\;\;\;\;\;\;\;\; -\frac {\sigma _c}{\sigma ^\top y} + \gamma \;\;=\;\;0$}"
REFERENCES,0.6085714285714285,"which could be easily written as a linear system w.r.t variables yk for k ∈¯Ko ∪{c}.
509"
REFERENCES,0.6095238095238096,"We derive a closed-form solution for the stationary point as follows. Substituting γ from the right
510"
REFERENCES,0.6104761904761905,"equation into the left equation, we get
511 \ la"
REFERENCES,0.6114285714285714,"bel
 {
e
q:yk-c
as e -B} \tag {f} \frac {\sigma _c - \sigma _k}{\sigma ^\top y} \,y_k \;\;=\;\; \lambda u_k S_k ,\;\;\;\;\;\;\;\forall k\in \bar {K}_o\;. 
(f)"
REFERENCES,0.6123809523809524,"Summing over k ∈¯Ko we further obtain
512"
REFERENCES,0.6133333333333333,"\centeri
ng  
\n onum
ber
 \foo
t
notesi"
REFERENCES,0.6142857142857143,"ze 
\
resizebox {\hsize }{!}{$ \frac { \sigma _c (1- y_c) - \sum _{k\in \bar {K}_o}\sigma _k y_k}{\sigma ^\top y}\;=\; \lambda u^\top S \;\;\;\;\;\;\;\;\;\;\Rightarrow \;\;\;\;\;\;\;\;\;\;\; \frac { \sigma _c - \sigma ^\top y}{\sigma ^\top y}\;\;=\;\; \lambda u^\top S $}"
REFERENCES,0.6152380952380953,giving a closed-form solution for σ⊤y
REFERENCES,0.6161904761904762,"σ⊤y
=
σc
1 + λu⊤S ."
REFERENCES,0.6171428571428571,Substituting this back into (f) we get closed-form solutions for yk
REFERENCES,0.6180952380952381,"yk
=
λukSk
(1 + λu⊤S)(1 −σk"
REFERENCES,0.6190476190476191,"σc ) ,
∀k ∈¯Ko ."
REFERENCES,0.62,"Note that positivity and boundedness of yk requires σc > σk for all k ∈¯Ko. In particular, this means
σc = σmax, but it also requires that all σk for k ∈¯Ko are strictly smaller than σmax. We can also
write the corresponding closed-form solution for yc"
REFERENCES,0.6209523809523809,"yc
=
1 −
X"
REFERENCES,0.621904761904762,"k∈¯
Ko
yk
=
1 −
σc
1 + λu⊤S X"
REFERENCES,0.6228571428571429,"k∈¯
Ko"
REFERENCES,0.6238095238095238,"λukSk
σc −σk
."
REFERENCES,0.6247619047619047,"Note that this solution should be positive yc > 0 as well.
513"
REFERENCES,0.6257142857142857,"In case any of the mentioned constraints (σc > σk, ∀k ∈¯Ko and yc > 0) is not satisfied, the
514"
REFERENCES,0.6266666666666667,"complimentary slackness (KKT) can be used to formally prove that the optimal solution is yc = 0.
515"
REFERENCES,0.6276190476190476,"That is, yk = 0 for all k ∈Ko. This reduces the optimization problem to the earlier case focusing
516"
REFERENCES,0.6285714285714286,"on resolving yk for k ∈¯Ko. This case is guaranteed to find a unique solution in the interior of the
517"
REFERENCES,0.6295238095238095,"simplex ∆¯
Ko. Indeed, since inequality ukSk > 0 holds for all k ∈¯Ko, the strong fairness enforces a
518"
REFERENCES,0.6304761904761905,"log-barrier for all the boundaries of this simplex.
519"
REFERENCES,0.6314285714285715,"C
Optimization algorithms
520"
REFERENCES,0.6323809523809524,"Algorithm 1: Newton’s method for M-step
Input
: {σk}, {Sk}, λ, ϵ
Output : {yk}
Initialize x ←maxk
σk
1+λu⊤S−λukSk
calculate f(x) ←P"
REFERENCES,0.6333333333333333,"k
λukSk
λu⊤S+1−
σk x −1"
REFERENCES,0.6342857142857142,while f(x) ≥ϵ do
REFERENCES,0.6352380952380953,calculate f ′(x) ←P
REFERENCES,0.6361904761904762,"k
−λukSkσk
(λu⊤Sx+x−σk)2
x ←x - f(x)"
REFERENCES,0.6371428571428571,"f ′(x)
calculate f(x) ←P"
REFERENCES,0.638095238095238,"k
λukSk
λu⊤S+1−
σk x −1"
REFERENCES,0.6390476190476191,"end
yk ←
λukSk
λu⊤S+1−
σk x"
REFERENCES,0.64,"Algorithm 2: Optimization for (11)
Input
:network parameters and dataset
Output :network parameters
for each epoch do"
REFERENCES,0.6409523809523809,for each iteration do
REFERENCES,0.6419047619047619,"Initialize y by the network output at current stage as a warm start;
while not convergent do"
REFERENCES,0.6428571428571429,"E step: Sk
i =
yk
i
P"
REFERENCES,0.6438095238095238,"j yk
j ;"
REFERENCES,0.6447619047619048,"M step: find yk
i using Newton’s method;
end
Update network using loss H2(y, σ) via stochastic gradient descent
end
end"
REFERENCES,0.6457142857142857,"D
Self-supervision Loss Comparison
521"
REFERENCES,0.6466666666666666,"\l
ab
el {e q:  o ur CCE2} \tag {a} L_{CCE}\;\;:=\;\;\overline {H_2(y,\sigma )}+\lambda \,KL(\bar {y}\|u) 
(a)"
REFERENCES,0.6476190476190476,"\la
be
l {eq : o u r CCE+2} \tag {b} L_{CCE+}\;\;:=\;\;\overline {H_2(y,\sigma )}+\lambda \,KL(u\|\bar {y}) 
(b)"
REFERENCES,0.6485714285714286,"STL10
CIFAR10
CIFAR100-20
MNIST"
REFERENCES,0.6495238095238095,"(a)
92.32%(6.3)
73.51%(6.4)
43.73%(1.1)
58.4%(3.2)
(b)
92.33%(6.4)
73.51%(6.3)
43.72%(1.1)
58.4%(3.2)"
REFERENCES,0.6504761904761904,Table 6: Using fixed features extracted from Resnet-50.
REFERENCES,0.6514285714285715,"STL10
CIFAR10
CIFAR100-20
MNIST"
REFERENCES,0.6523809523809524,"(a)
25.98%(1.0)
24.26%(0.8)
15.13%(0.6)
95.10%(4.2)
(b)
25.98%(1.1)
24.26%(0.8)
15.14%(0.5)
95.11%(4.3)"
REFERENCES,0.6533333333333333,Table 7: With simultaneous feature training from the scratch. The network architecture is VGG-4.
REFERENCES,0.6542857142857142,"E
Experiments
522"
REFERENCES,0.6552380952380953,"E.1
Network Architecture
523"
REFERENCES,0.6561904761904762,"The network structure of VGG4 is adapted from [17]. We used standard ResNet-18 from the PyTorch
524"
REFERENCES,0.6571428571428571,"library as the backbone architecture for Figure 2. As for the ResNet-18 used for Table 4, we used the
525"
REFERENCES,0.6580952380952381,"code from this repository 1.
526"
REFERENCES,0.659047619047619,"Grey(28x28x1)
RGB(32x32x3)
RGB(96x96x3)"
REFERENCES,0.66,"1xConv(5x5,s=1,p=2)@64
1xConv(5x5,s=1,p=2)@32
1xConv(5x5,s=2,p=2)@128
1xMaxPool(2x2,s=2)
1xMaxPool(2x2,s=2)
1xMaxPool(2x2,s=2)
1xConv(5x5,s=1,p=2)@128
1xConv(5x5,s=1,p=2)@64
1xConv(5x5,s=2,p=2)@256
1xMaxPool(2x2,s=2)
1xMaxPool(2x2,s=2)
1xMaxPool(2x2,s=2)
1xConv(5x5,s=1,p=2)@256
1xConv(5x5,s=1,p=2)@128
1xConv(5x5,s=2,p=2)@512
1xMaxPool(2x2,s=2)
1xMaxPool(2x2,s=2)
1xMaxPool(2x2,s=2)
1xConv(5x5,s=1,p=2)@512
1xConv(5x5,s=1,p=2)@256
1xConv(5x5,s=2,p=2)@1024
1xLinear(512x3x3,K)
1xLinear(256x4x4,K)
1xLinear(1024x1x1,K)
Table 8: Network architecture summary. s: stride; p: padding; K: number of clusters. The first
column is used on MNIST [25]; the second one is used on CIFAR10/100 [43]; the third one is used on
STL10 [8]. Batch normalization is also applied after each Conv layer. ReLu is adopted for non-linear
activation function."
REFERENCES,0.660952380952381,"E.2
Experimental Settings
527"
REFERENCES,0.6619047619047619,"Here we present the missing details of experimental settings for Table 2 - 5. As for Table 2, the
528"
REFERENCES,0.6628571428571428,"weight of the linear classifier is initialized by using Kaiming initialization [13] and the bias is all set
529"
REFERENCES,0.6638095238095238,"to zero at the beginning. We use the l2-norm weight decay and set the coefficient of this term to 0.001,
530"
REFERENCES,0.6647619047619048,"0.02, 0.009, and 0.02 for MNIST, CIFAR10, CIFAR100 and STL10 respectively. The optimizer is
531"
REFERENCES,0.6657142857142857,"stochastic gradient descent with a learning rate set to 0.1. The batch size is set to 250. The number of
532"
REFERENCES,0.6666666666666666,"epochs is 10. We set λ in our loss to 100 and separately tuned the hyperparameters for other methods.
533"
REFERENCES,0.6676190476190477,"For Table 3, we use Adam [21] with learning rate 1e−4 for optimizing the network parameters. We
534"
REFERENCES,0.6685714285714286,"set batch size to 250 for CIFAR10, CIFAR100 and MNIST and we use 160 for STL10. We report the
535"
REFERENCES,0.6695238095238095,"mean accuracy and Std from 6 runs with random initializations. We use 50 epochs for each run and
536"
REFERENCES,0.6704761904761904,"all methods reach convergence within 50 epochs. The weight decay coefficient is set to 0.01.
537"
REFERENCES,0.6714285714285714,"As for the training of ResNet-18 in Table 4, we still use the Adam optimizer, and the learning rate is
538"
REFERENCES,0.6723809523809524,"set to 5e−2 for the linear classifier and 1e−5 for the backbone. The weight decay coefficient is set to
539"
REFERENCES,0.6733333333333333,"1e−4. The batch size is 200 and the number of total epochs is 50. The λ is still set to 100. We only
540"
REFERENCES,0.6742857142857143,"use one augmentation per image, and the coefficient for the augmentation term is set to 0.5, 0.2, and
541"
REFERENCES,0.6752380952380952,"0.4 respectively for STL10, CIFAR10, and CIFAR100 (20).
542"
REFERENCES,0.6761904761904762,"As for the semi-supervised settings, we made two changes compared to the above. First, we added
543"
REFERENCES,0.6771428571428572,"the cross-entropy loss on the labeled images and set the weight to 2, and separately tuned the
544"
REFERENCES,0.6780952380952381,"hyperparameters for other methods. Second, the pseudo-labels on the labeled images are constrained
545"
REFERENCES,0.679047619047619,"to be the ground truth during the optimization.
546"
REFERENCES,0.68,1https://github.com/wvangansbeke/Unsupervised-Classification
REFERENCES,0.680952380952381,"NeurIPS Paper Checklist
547"
REFERENCES,0.6819047619047619,"The checklist is designed to encourage best practices for responsible machine learning research,
548"
REFERENCES,0.6828571428571428,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
549"
REFERENCES,0.6838095238095238,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
550"
REFERENCES,0.6847619047619048,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
551"
REFERENCES,0.6857142857142857,"towards the page limit.
552"
REFERENCES,0.6866666666666666,"Please read the checklist guidelines carefully for information on how to answer these questions. For
553"
REFERENCES,0.6876190476190476,"each question in the checklist:
554"
REFERENCES,0.6885714285714286,"• You should answer [Yes] , [No] , or [NA] .
555"
REFERENCES,0.6895238095238095,"• [NA] means either that the question is Not Applicable for that particular paper or the
556"
REFERENCES,0.6904761904761905,"relevant information is Not Available.
557"
REFERENCES,0.6914285714285714,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
558"
REFERENCES,0.6923809523809524,"The checklist answers are an integral part of your paper submission. They are visible to the
559"
REFERENCES,0.6933333333333334,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
560"
REFERENCES,0.6942857142857143,"(after eventual revisions) with the final version of your paper, and its final version will be published
561"
REFERENCES,0.6952380952380952,"with the paper.
562"
REFERENCES,0.6961904761904761,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
563"
REFERENCES,0.6971428571428572,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
564"
REFERENCES,0.6980952380952381,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
565"
REFERENCES,0.699047619047619,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
566"
REFERENCES,0.7,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
567"
REFERENCES,0.700952380952381,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
568"
REFERENCES,0.7019047619047619,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
569"
REFERENCES,0.7028571428571428,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
570"
REFERENCES,0.7038095238095238,"please point to the section(s) where related material for the question can be found.
571"
REFERENCES,0.7047619047619048,"IMPORTANT, please:
572"
REFERENCES,0.7057142857142857,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
573"
REFERENCES,0.7066666666666667,"• Keep the checklist subsection headings, questions/answers and guidelines below.
574"
REFERENCES,0.7076190476190476,"• Do not modify the questions and only use the provided macros for your answers.
575"
CLAIMS,0.7085714285714285,"1. Claims
576"
CLAIMS,0.7095238095238096,"Question: Do the main claims made in the abstract and introduction accurately reflect the
577"
CLAIMS,0.7104761904761905,"paper’s contributions and scope?
578"
CLAIMS,0.7114285714285714,"Answer: [Yes]
579"
CLAIMS,0.7123809523809523,"Justification: This can be justified from reading the paper.
580"
CLAIMS,0.7133333333333334,"Guidelines:
581"
CLAIMS,0.7142857142857143,"• The answer NA means that the abstract and introduction do not include the claims
582"
CLAIMS,0.7152380952380952,"made in the paper.
583"
CLAIMS,0.7161904761904762,"• The abstract and/or introduction should clearly state the claims made, including the
584"
CLAIMS,0.7171428571428572,"contributions made in the paper and important assumptions and limitations. A No or
585"
CLAIMS,0.7180952380952381,"NA answer to this question will not be perceived well by the reviewers.
586"
CLAIMS,0.719047619047619,"• The claims made should match theoretical and experimental results, and reflect how
587"
CLAIMS,0.72,"much the results can be expected to generalize to other settings.
588"
CLAIMS,0.7209523809523809,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
589"
CLAIMS,0.7219047619047619,"are not attained by the paper.
590"
LIMITATIONS,0.7228571428571429,"2. Limitations
591"
LIMITATIONS,0.7238095238095238,"Question: Does the paper discuss the limitations of the work performed by the authors?
592"
LIMITATIONS,0.7247619047619047,"Answer: [NA]
593"
LIMITATIONS,0.7257142857142858,"Justification:
594"
LIMITATIONS,0.7266666666666667,"Guidelines:
595"
LIMITATIONS,0.7276190476190476,"• The answer NA means that the paper has no limitation while the answer No means that
596"
LIMITATIONS,0.7285714285714285,"the paper has limitations, but those are not discussed in the paper.
597"
LIMITATIONS,0.7295238095238096,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
598"
LIMITATIONS,0.7304761904761905,"• The paper should point out any strong assumptions and how robust the results are to
599"
LIMITATIONS,0.7314285714285714,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
600"
LIMITATIONS,0.7323809523809524,"model well-specification, asymptotic approximations only holding locally). The authors
601"
LIMITATIONS,0.7333333333333333,"should reflect on how these assumptions might be violated in practice and what the
602"
LIMITATIONS,0.7342857142857143,"implications would be.
603"
LIMITATIONS,0.7352380952380952,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
604"
LIMITATIONS,0.7361904761904762,"only tested on a few datasets or with a few runs. In general, empirical results often
605"
LIMITATIONS,0.7371428571428571,"depend on implicit assumptions, which should be articulated.
606"
LIMITATIONS,0.7380952380952381,"• The authors should reflect on the factors that influence the performance of the approach.
607"
LIMITATIONS,0.7390476190476191,"For example, a facial recognition algorithm may perform poorly when image resolution
608"
LIMITATIONS,0.74,"is low or images are taken in low lighting. Or a speech-to-text system might not be
609"
LIMITATIONS,0.7409523809523809,"used reliably to provide closed captions for online lectures because it fails to handle
610"
LIMITATIONS,0.741904761904762,"technical jargon.
611"
LIMITATIONS,0.7428571428571429,"• The authors should discuss the computational efficiency of the proposed algorithms
612"
LIMITATIONS,0.7438095238095238,"and how they scale with dataset size.
613"
LIMITATIONS,0.7447619047619047,"• If applicable, the authors should discuss possible limitations of their approach to
614"
LIMITATIONS,0.7457142857142857,"address problems of privacy and fairness.
615"
LIMITATIONS,0.7466666666666667,"• While the authors might fear that complete honesty about limitations might be used by
616"
LIMITATIONS,0.7476190476190476,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
617"
LIMITATIONS,0.7485714285714286,"limitations that aren’t acknowledged in the paper. The authors should use their best
618"
LIMITATIONS,0.7495238095238095,"judgment and recognize that individual actions in favor of transparency play an impor-
619"
LIMITATIONS,0.7504761904761905,"tant role in developing norms that preserve the integrity of the community. Reviewers
620"
LIMITATIONS,0.7514285714285714,"will be specifically instructed to not penalize honesty concerning limitations.
621"
THEORY ASSUMPTIONS AND PROOFS,0.7523809523809524,"3. Theory Assumptions and Proofs
622"
THEORY ASSUMPTIONS AND PROOFS,0.7533333333333333,"Question: For each theoretical result, does the paper provide the full set of assumptions and
623"
THEORY ASSUMPTIONS AND PROOFS,0.7542857142857143,"a complete (and correct) proof?
624"
THEORY ASSUMPTIONS AND PROOFS,0.7552380952380953,"Answer: [Yes]
625"
THEORY ASSUMPTIONS AND PROOFS,0.7561904761904762,"Justification: Assumptions are clearly stated and missing proofs can be found in the ap-
626"
THEORY ASSUMPTIONS AND PROOFS,0.7571428571428571,"pendix.
627"
THEORY ASSUMPTIONS AND PROOFS,0.758095238095238,"Guidelines:
628"
THEORY ASSUMPTIONS AND PROOFS,0.7590476190476191,"• The answer NA means that the paper does not include theoretical results.
629"
THEORY ASSUMPTIONS AND PROOFS,0.76,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
630"
THEORY ASSUMPTIONS AND PROOFS,0.7609523809523809,"referenced.
631"
THEORY ASSUMPTIONS AND PROOFS,0.7619047619047619,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
632"
THEORY ASSUMPTIONS AND PROOFS,0.7628571428571429,"• The proofs can either appear in the main paper or the supplemental material, but if
633"
THEORY ASSUMPTIONS AND PROOFS,0.7638095238095238,"they appear in the supplemental material, the authors are encouraged to provide a short
634"
THEORY ASSUMPTIONS AND PROOFS,0.7647619047619048,"proof sketch to provide intuition.
635"
THEORY ASSUMPTIONS AND PROOFS,0.7657142857142857,"• Inversely, any informal proof provided in the core of the paper should be complemented
636"
THEORY ASSUMPTIONS AND PROOFS,0.7666666666666667,"by formal proofs provided in appendix or supplemental material.
637"
THEORY ASSUMPTIONS AND PROOFS,0.7676190476190476,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7685714285714286,"4. Experimental Result Reproducibility
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7695238095238095,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7704761904761904,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714285714285715,"of the paper (regardless of whether the code and data are provided or not)?
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723809523809524,"Answer: [Yes]
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733333333333333,"Justification: All the details can be found in the appendix.
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7742857142857142,"Guidelines:
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7752380952380953,"• The answer NA means that the paper does not include experiments.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7761904761904762,"• If the paper includes experiments, a No answer to this question will not be perceived
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7771428571428571,"well by the reviewers: Making the paper reproducible is important, regardless of
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7780952380952381,"whether the code and data are provided or not.
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7790476190476191,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.78,"to make their results reproducible or verifiable.
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.780952380952381,"• Depending on the contribution, reproducibility can be accomplished in various ways.
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7819047619047619,"For example, if the contribution is a novel architecture, describing the architecture fully
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7828571428571428,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7838095238095238,"be necessary to either make it possible for others to replicate the model with the same
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7847619047619048,"dataset, or provide access to the model. In general. releasing code and data is often
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7857142857142857,"one good way to accomplish this, but reproducibility can also be provided via detailed
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7866666666666666,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7876190476190477,"of a large language model), releasing of a model checkpoint, or other means that are
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7885714285714286,"appropriate to the research performed.
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7895238095238095,"• While NeurIPS does not require releasing code, the conference does require all submis-
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7904761904761904,"sions to provide some reasonable avenue for reproducibility, which may depend on the
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7914285714285715,"nature of the contribution. For example
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7923809523809524,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7933333333333333,"to reproduce that algorithm.
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7942857142857143,"(b) If the contribution is primarily a new model architecture, the paper should describe
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7952380952380952,"the architecture clearly and fully.
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7961904761904762,"(c) If the contribution is a new model (e.g., a large language model), then there should
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7971428571428572,"either be a way to access this model for reproducing the results or a way to reproduce
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7980952380952381,"the model (e.g., with an open-source dataset or instructions for how to construct
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.799047619047619,"the dataset).
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8,"(d) We recognize that reproducibility may be tricky in some cases, in which case
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.800952380952381,"authors are welcome to describe the particular way they provide for reproducibility.
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8019047619047619,"In the case of closed-source models, it may be that access to the model is limited in
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8028571428571428,"some way (e.g., to registered users), but it should be possible for other researchers
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8038095238095239,"to have some path to reproducing or verifying the results.
676"
OPEN ACCESS TO DATA AND CODE,0.8047619047619048,"5. Open access to data and code
677"
OPEN ACCESS TO DATA AND CODE,0.8057142857142857,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
678"
OPEN ACCESS TO DATA AND CODE,0.8066666666666666,"tions to faithfully reproduce the main experimental results, as described in supplemental
679"
OPEN ACCESS TO DATA AND CODE,0.8076190476190476,"material?
680"
OPEN ACCESS TO DATA AND CODE,0.8085714285714286,"Answer: [No]
681"
OPEN ACCESS TO DATA AND CODE,0.8095238095238095,"Justification: Code is released upon acceptance.
682"
OPEN ACCESS TO DATA AND CODE,0.8104761904761905,"Guidelines:
683"
OPEN ACCESS TO DATA AND CODE,0.8114285714285714,"• The answer NA means that paper does not include experiments requiring code.
684"
OPEN ACCESS TO DATA AND CODE,0.8123809523809524,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
685"
OPEN ACCESS TO DATA AND CODE,0.8133333333333334,"public/guides/CodeSubmissionPolicy) for more details.
686"
OPEN ACCESS TO DATA AND CODE,0.8142857142857143,"• While we encourage the release of code and data, we understand that this might not be
687"
OPEN ACCESS TO DATA AND CODE,0.8152380952380952,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
688"
OPEN ACCESS TO DATA AND CODE,0.8161904761904762,"including code, unless this is central to the contribution (e.g., for a new open-source
689"
OPEN ACCESS TO DATA AND CODE,0.8171428571428572,"benchmark).
690"
OPEN ACCESS TO DATA AND CODE,0.8180952380952381,"• The instructions should contain the exact command and environment needed to run to
691"
OPEN ACCESS TO DATA AND CODE,0.819047619047619,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
692"
OPEN ACCESS TO DATA AND CODE,0.82,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
693"
OPEN ACCESS TO DATA AND CODE,0.820952380952381,"• The authors should provide instructions on data access and preparation, including how
694"
OPEN ACCESS TO DATA AND CODE,0.8219047619047619,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
695"
OPEN ACCESS TO DATA AND CODE,0.8228571428571428,"• The authors should provide scripts to reproduce all experimental results for the new
696"
OPEN ACCESS TO DATA AND CODE,0.8238095238095238,"proposed method and baselines. If only a subset of experiments are reproducible, they
697"
OPEN ACCESS TO DATA AND CODE,0.8247619047619048,"should state which ones are omitted from the script and why.
698"
OPEN ACCESS TO DATA AND CODE,0.8257142857142857,"• At submission time, to preserve anonymity, the authors should release anonymized
699"
OPEN ACCESS TO DATA AND CODE,0.8266666666666667,"versions (if applicable).
700"
OPEN ACCESS TO DATA AND CODE,0.8276190476190476,"• Providing as much information as possible in supplemental material (appended to the
701"
OPEN ACCESS TO DATA AND CODE,0.8285714285714286,"paper) is recommended, but including URLs to data and code is permitted.
702"
OPEN ACCESS TO DATA AND CODE,0.8295238095238096,"6. Experimental Setting/Details
703"
OPEN ACCESS TO DATA AND CODE,0.8304761904761905,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
704"
OPEN ACCESS TO DATA AND CODE,0.8314285714285714,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
705"
OPEN ACCESS TO DATA AND CODE,0.8323809523809523,"results?
706"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"Answer: [Yes]
707"
OPEN ACCESS TO DATA AND CODE,0.8342857142857143,"Justification: See appendix.
708"
OPEN ACCESS TO DATA AND CODE,0.8352380952380952,"Guidelines:
709"
OPEN ACCESS TO DATA AND CODE,0.8361904761904762,"• The answer NA means that the paper does not include experiments.
710"
OPEN ACCESS TO DATA AND CODE,0.8371428571428572,"• The experimental setting should be presented in the core of the paper to a level of detail
711"
OPEN ACCESS TO DATA AND CODE,0.8380952380952381,"that is necessary to appreciate the results and make sense of them.
712"
OPEN ACCESS TO DATA AND CODE,0.839047619047619,"• The full details can be provided either with the code, in appendix, or as supplemental
713"
OPEN ACCESS TO DATA AND CODE,0.84,"material.
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.840952380952381,"7. Experiment Statistical Significance
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8419047619047619,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8428571428571429,"information about the statistical significance of the experiments?
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8438095238095238,"Answer: [Yes]
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8447619047619047,"Justification: Mean and standard deviation are provided for most of the experiments.
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8457142857142858,"Guidelines:
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8466666666666667,"• The answer NA means that the paper does not include experiments.
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8476190476190476,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8485714285714285,"dence intervals, or statistical significance tests, at least for the experiments that support
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8495238095238096,"the main claims of the paper.
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8504761904761905,"• The factors of variability that the error bars are capturing should be clearly stated (for
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8514285714285714,"example, train/test split, initialization, random drawing of some parameter, or overall
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8523809523809524,"run with given experimental conditions).
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8533333333333334,"• The method for calculating the error bars should be explained (closed form formula,
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8542857142857143,"call to a library function, bootstrap, etc.)
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8552380952380952,"• The assumptions made should be given (e.g., Normally distributed errors).
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8561904761904762,"• It should be clear whether the error bar is the standard deviation or the standard error
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"of the mean.
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8580952380952381,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8590476190476191,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.86,"of Normality of errors is not verified.
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8609523809523809,"• For asymmetric distributions, the authors should be careful not to show in tables or
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.861904761904762,"figures symmetric error bars that would yield results that are out of range (e.g. negative
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8628571428571429,"error rates).
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8638095238095238,"• If error bars are reported in tables or plots, The authors should explain in the text how
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8647619047619047,"they were calculated and reference the corresponding figures or tables in the text.
740"
EXPERIMENTS COMPUTE RESOURCES,0.8657142857142858,"8. Experiments Compute Resources
741"
EXPERIMENTS COMPUTE RESOURCES,0.8666666666666667,"Question: For each experiment, does the paper provide sufficient information on the com-
742"
EXPERIMENTS COMPUTE RESOURCES,0.8676190476190476,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
743"
EXPERIMENTS COMPUTE RESOURCES,0.8685714285714285,"the experiments?
744"
EXPERIMENTS COMPUTE RESOURCES,0.8695238095238095,"Answer: [No]
745"
EXPERIMENTS COMPUTE RESOURCES,0.8704761904761905,"Justification: The datasets are not large. We used single P100 GPU card.
746"
EXPERIMENTS COMPUTE RESOURCES,0.8714285714285714,"Guidelines:
747"
EXPERIMENTS COMPUTE RESOURCES,0.8723809523809524,"• The answer NA means that the paper does not include experiments.
748"
EXPERIMENTS COMPUTE RESOURCES,0.8733333333333333,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
749"
EXPERIMENTS COMPUTE RESOURCES,0.8742857142857143,"or cloud provider, including relevant memory and storage.
750"
EXPERIMENTS COMPUTE RESOURCES,0.8752380952380953,"• The paper should provide the amount of compute required for each of the individual
751"
EXPERIMENTS COMPUTE RESOURCES,0.8761904761904762,"experimental runs as well as estimate the total compute.
752"
EXPERIMENTS COMPUTE RESOURCES,0.8771428571428571,"• The paper should disclose whether the full research project required more compute
753"
EXPERIMENTS COMPUTE RESOURCES,0.878095238095238,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
754"
EXPERIMENTS COMPUTE RESOURCES,0.8790476190476191,"didn’t make it into the paper).
755"
CODE OF ETHICS,0.88,"9. Code Of Ethics
756"
CODE OF ETHICS,0.8809523809523809,"Question: Does the research conducted in the paper conform, in every respect, with the
757"
CODE OF ETHICS,0.8819047619047619,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
758"
CODE OF ETHICS,0.8828571428571429,"Answer: [Yes]
759"
CODE OF ETHICS,0.8838095238095238,"Justification:
760"
CODE OF ETHICS,0.8847619047619047,"Guidelines:
761"
CODE OF ETHICS,0.8857142857142857,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
762"
CODE OF ETHICS,0.8866666666666667,"• If the authors answer No, they should explain the special circumstances that require a
763"
CODE OF ETHICS,0.8876190476190476,"deviation from the Code of Ethics.
764"
CODE OF ETHICS,0.8885714285714286,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
765"
CODE OF ETHICS,0.8895238095238095,"eration due to laws or regulations in their jurisdiction).
766"
BROADER IMPACTS,0.8904761904761904,"10. Broader Impacts
767"
BROADER IMPACTS,0.8914285714285715,"Question: Does the paper discuss both potential positive societal impacts and negative
768"
BROADER IMPACTS,0.8923809523809524,"societal impacts of the work performed?
769"
BROADER IMPACTS,0.8933333333333333,"Answer: [NA]
770"
BROADER IMPACTS,0.8942857142857142,"Justification:
771"
BROADER IMPACTS,0.8952380952380953,"Guidelines:
772"
BROADER IMPACTS,0.8961904761904762,"• The answer NA means that there is no societal impact of the work performed.
773"
BROADER IMPACTS,0.8971428571428571,"• If the authors answer NA or No, they should explain why their work has no societal
774"
BROADER IMPACTS,0.8980952380952381,"impact or why the paper does not address societal impact.
775"
BROADER IMPACTS,0.8990476190476191,"• Examples of negative societal impacts include potential malicious or unintended uses
776"
BROADER IMPACTS,0.9,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
777"
BROADER IMPACTS,0.900952380952381,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
778"
BROADER IMPACTS,0.9019047619047619,"groups), privacy considerations, and security considerations.
779"
BROADER IMPACTS,0.9028571428571428,"• The conference expects that many papers will be foundational research and not tied
780"
BROADER IMPACTS,0.9038095238095238,"to particular applications, let alone deployments. However, if there is a direct path to
781"
BROADER IMPACTS,0.9047619047619048,"any negative applications, the authors should point it out. For example, it is legitimate
782"
BROADER IMPACTS,0.9057142857142857,"to point out that an improvement in the quality of generative models could be used to
783"
BROADER IMPACTS,0.9066666666666666,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
784"
BROADER IMPACTS,0.9076190476190477,"that a generic algorithm for optimizing neural networks could enable people to train
785"
BROADER IMPACTS,0.9085714285714286,"models that generate Deepfakes faster.
786"
BROADER IMPACTS,0.9095238095238095,"• The authors should consider possible harms that could arise when the technology is
787"
BROADER IMPACTS,0.9104761904761904,"being used as intended and functioning correctly, harms that could arise when the
788"
BROADER IMPACTS,0.9114285714285715,"technology is being used as intended but gives incorrect results, and harms following
789"
BROADER IMPACTS,0.9123809523809524,"from (intentional or unintentional) misuse of the technology.
790"
BROADER IMPACTS,0.9133333333333333,"• If there are negative societal impacts, the authors could also discuss possible mitigation
791"
BROADER IMPACTS,0.9142857142857143,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
792"
BROADER IMPACTS,0.9152380952380952,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
793"
BROADER IMPACTS,0.9161904761904762,"feedback over time, improving the efficiency and accessibility of ML).
794"
SAFEGUARDS,0.9171428571428571,"11. Safeguards
795"
SAFEGUARDS,0.9180952380952381,"Question: Does the paper describe safeguards that have been put in place for responsible
796"
SAFEGUARDS,0.919047619047619,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
797"
SAFEGUARDS,0.92,"image generators, or scraped datasets)?
798"
SAFEGUARDS,0.920952380952381,"Answer: [NA]
799"
SAFEGUARDS,0.9219047619047619,"Justification:
800"
SAFEGUARDS,0.9228571428571428,"Guidelines:
801"
SAFEGUARDS,0.9238095238095239,"• The answer NA means that the paper poses no such risks.
802"
SAFEGUARDS,0.9247619047619048,"• Released models that have a high risk for misuse or dual-use should be released with
803"
SAFEGUARDS,0.9257142857142857,"necessary safeguards to allow for controlled use of the model, for example by requiring
804"
SAFEGUARDS,0.9266666666666666,"that users adhere to usage guidelines or restrictions to access the model or implementing
805"
SAFEGUARDS,0.9276190476190476,"safety filters.
806"
SAFEGUARDS,0.9285714285714286,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
807"
SAFEGUARDS,0.9295238095238095,"should describe how they avoided releasing unsafe images.
808"
SAFEGUARDS,0.9304761904761905,"• We recognize that providing effective safeguards is challenging, and many papers do
809"
SAFEGUARDS,0.9314285714285714,"not require this, but we encourage authors to take this into account and make a best
810"
SAFEGUARDS,0.9323809523809524,"faith effort.
811"
LICENSES FOR EXISTING ASSETS,0.9333333333333333,"12. Licenses for existing assets
812"
LICENSES FOR EXISTING ASSETS,0.9342857142857143,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
813"
LICENSES FOR EXISTING ASSETS,0.9352380952380952,"the paper, properly credited and are the license and terms of use explicitly mentioned and
814"
LICENSES FOR EXISTING ASSETS,0.9361904761904762,"properly respected?
815"
LICENSES FOR EXISTING ASSETS,0.9371428571428572,"Answer: [Yes]
816"
LICENSES FOR EXISTING ASSETS,0.9380952380952381,"Justification: We cite them and put the links as well.
817"
LICENSES FOR EXISTING ASSETS,0.939047619047619,"Guidelines:
818"
LICENSES FOR EXISTING ASSETS,0.94,"• The answer NA means that the paper does not use existing assets.
819"
LICENSES FOR EXISTING ASSETS,0.940952380952381,"• The authors should cite the original paper that produced the code package or dataset.
820"
LICENSES FOR EXISTING ASSETS,0.9419047619047619,"• The authors should state which version of the asset is used and, if possible, include a
821"
LICENSES FOR EXISTING ASSETS,0.9428571428571428,"URL.
822"
LICENSES FOR EXISTING ASSETS,0.9438095238095238,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
823"
LICENSES FOR EXISTING ASSETS,0.9447619047619048,"• For scraped data from a particular source (e.g., website), the copyright and terms of
824"
LICENSES FOR EXISTING ASSETS,0.9457142857142857,"service of that source should be provided.
825"
LICENSES FOR EXISTING ASSETS,0.9466666666666667,"• If assets are released, the license, copyright information, and terms of use in the package
826"
LICENSES FOR EXISTING ASSETS,0.9476190476190476,"should be provided. For popular datasets, paperswithcode.com/datasets has
827"
LICENSES FOR EXISTING ASSETS,0.9485714285714286,"curated licenses for some datasets. Their licensing guide can help determine the license
828"
LICENSES FOR EXISTING ASSETS,0.9495238095238095,"of a dataset.
829"
LICENSES FOR EXISTING ASSETS,0.9504761904761905,"• For existing datasets that are re-packaged, both the original license and the license of
830"
LICENSES FOR EXISTING ASSETS,0.9514285714285714,"the derived asset (if it has changed) should be provided.
831"
LICENSES FOR EXISTING ASSETS,0.9523809523809523,"• If this information is not available online, the authors are encouraged to reach out to
832"
LICENSES FOR EXISTING ASSETS,0.9533333333333334,"the asset’s creators.
833"
NEW ASSETS,0.9542857142857143,"13. New Assets
834"
NEW ASSETS,0.9552380952380952,"Question: Are new assets introduced in the paper well documented and is the documentation
835"
NEW ASSETS,0.9561904761904761,"provided alongside the assets?
836"
NEW ASSETS,0.9571428571428572,"Answer: [NA]
837"
NEW ASSETS,0.9580952380952381,"Justification:
838"
NEW ASSETS,0.959047619047619,"Guidelines:
839"
NEW ASSETS,0.96,"• The answer NA means that the paper does not release new assets.
840"
NEW ASSETS,0.960952380952381,"• Researchers should communicate the details of the dataset/code/model as part of their
841"
NEW ASSETS,0.9619047619047619,"submissions via structured templates. This includes details about training, license,
842"
NEW ASSETS,0.9628571428571429,"limitations, etc.
843"
NEW ASSETS,0.9638095238095238,"• The paper should discuss whether and how consent was obtained from people whose
844"
NEW ASSETS,0.9647619047619047,"asset is used.
845"
NEW ASSETS,0.9657142857142857,"• At submission time, remember to anonymize your assets (if applicable). You can either
846"
NEW ASSETS,0.9666666666666667,"create an anonymized URL or include an anonymized zip file.
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9676190476190476,"14. Crowdsourcing and Research with Human Subjects
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685714285714285,"Question: For crowdsourcing experiments and research with human subjects, does the paper
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695238095238096,"include the full text of instructions given to participants and screenshots, if applicable, as
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704761904761905,"well as details about compensation (if any)?
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714285714285714,"Answer: [NA]
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9723809523809523,"Justification:
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9733333333333334,"Guidelines:
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742857142857143,"• The answer NA means that the paper does not involve crowdsourcing nor research with
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752380952380952,"human subjects.
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761904761904762,"• Including this information in the supplemental material is fine, but if the main contribu-
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771428571428571,"tion of the paper involves human subjects, then as much detail as possible should be
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780952380952381,"included in the main paper.
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979047619047619,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98,"or other labor should be paid at least the minimum wage in the country of the data
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809523809523809,"collector.
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981904761904762,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828571428571429,"Subjects
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838095238095238,"Question: Does the paper describe potential risks incurred by study participants, whether
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847619047619047,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857142857142858,"approvals (or an equivalent approval/review based on the requirements of your country or
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866666666666667,"institution) were obtained?
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876190476190476,"Answer: [NA]
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885714285714285,"Justification:
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895238095238095,"Guidelines:
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904761904761905,"• The answer NA means that the paper does not involve crowdsourcing nor research with
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914285714285714,"human subjects.
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923809523809524,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933333333333333,"may be required for any human subjects research. If you obtained IRB approval, you
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942857142857143,"should clearly state this in the paper.
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952380952380953,"• We recognize that the procedures for this may vary significantly between institutions
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961904761904762,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9971428571428571,"guidelines for their institution.
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980952380952381,"• For initial submissions, do not include any information that would break anonymity (if
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990476190476191,"applicable), such as the institution conducting the review.
881"
