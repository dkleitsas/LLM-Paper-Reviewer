Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017331022530329288,"This paper introduces a new method for minimizing matrix-smooth non-convex ob-
1"
ABSTRACT,0.0034662045060658577,"jectives through the use of novel Compressed Gradient Descent (CGD) algorithms
2"
ABSTRACT,0.005199306759098787,"enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically
3"
ABSTRACT,0.006932409012131715,"analyzed ﬁrst in the single-node and subsequently in the distributed settings. Our
4"
ABSTRACT,0.008665511265164644,"theoretical results reveal that the matrix stepsize in CGD can capture the objec-
5"
ABSTRACT,0.010398613518197574,"tive’s structure and lead to faster convergence compared to a scalar stepsize. As
6"
ABSTRACT,0.012131715771230503,"a byproduct of our general results, we emphasize the importance of selecting the
7"
ABSTRACT,0.01386481802426343,"compression mechanism and the matrix stepsize in a layer-wise manner, taking
8"
ABSTRACT,0.01559792027729636,"advantage of model structure. Moreover, we provide theoretical guarantees for
9"
ABSTRACT,0.01733102253032929,"free compression, by designing speciﬁc layer-wise compressors for the non-convex
10"
ABSTRACT,0.019064124783362217,"matrix smooth objectives. Our ﬁndings are supported with empirical evidence.
11"
INTRODUCTION,0.02079722703639515,"1
Introduction
12"
INTRODUCTION,0.022530329289428077,"The minimization of smooth and non-convex functions is a fundamental problem in various domains
13"
INTRODUCTION,0.024263431542461005,"of applied mathematics. Most machine learning algorithms rely on solving optimization problems for
14"
INTRODUCTION,0.025996533795493933,"training and inference, often with structural constraints or non-convex objectives to accurately capture
15"
INTRODUCTION,0.02772963604852686,"the learning and prediction problems in high-dimensional or non-linear spaces. However, non-convex
16"
INTRODUCTION,0.029462738301559793,"problems are typically NP-hard to solve, leading to the popular approach of relaxing them to convex
17"
INTRODUCTION,0.03119584055459272,"problems and using traditional methods. Direct approaches to non-convex optimization have shown
18"
INTRODUCTION,0.03292894280762565,"success but their convergence and properties are not well understood, making them challenging for
19"
INTRODUCTION,0.03466204506065858,"large scale optimization. While its convex alternative has been extensively studied and is generally an
20"
INTRODUCTION,0.036395147313691506,"easier problem, the non-convex setting is of greater practical interest often being the computational
21"
INTRODUCTION,0.038128249566724434,"bottleneck in many applications.
22"
INTRODUCTION,0.03986135181975736,"In this paper, we consider the general minimization problem:
23"
INTRODUCTION,0.0415944540727903,"min
x∈Rd f(x),
(1)"
INTRODUCTION,0.043327556325823226,"where f : Rd →R is a differentiable function. In order for this problem to have a ﬁnite solution we
24"
INTRODUCTION,0.045060658578856154,"will assume throughout the paper that f is bounded from below.
25"
INTRODUCTION,0.04679376083188908,"Assumption 1. There exists f inf ∈R such that f(x) ≥f inf for all x ∈Rd.
26"
INTRODUCTION,0.04852686308492201,"The stochastic gradient descent (SGD) algorithm [MB11, B+15, GLQ+19] is one of the most
27"
INTRODUCTION,0.05025996533795494,"common algorithms to solve this problem. In its most general form, it can be written as
28"
INTRODUCTION,0.05199306759098787,"xk+1 = xk −γg(xk),
(2)"
INTRODUCTION,0.053726169844020795,"where g(xk) is a stochastic estimator of ∇f(xk) and γ > 0 is a positive scalar stepsize. A particular
29"
INTRODUCTION,0.05545927209705372,"case of interest is the compressed gradient descent (CGD) algorithm [KFJ18], where the estimator g
30"
INTRODUCTION,0.05719237435008666,"is taken as a compressed alternative of the initial gradient:
31"
INTRODUCTION,0.058925476603119586,"g(xk) = C(∇f(xk)),
(3)
and the compressor C is chosen to be a ""sparser"" estimator that aims to reduce the communication
32"
INTRODUCTION,0.060658578856152515,"overhead in distributed or federated settings. This is crucial, as highlighted in the seminal paper
33"
INTRODUCTION,0.06239168110918544,"by [KMY+16], which showed that the bottleneck of distributed optimization algorithms is the
34"
INTRODUCTION,0.06412478336221837,"communication complexity. In order to deal with the limited resources of current devices, there are
35"
INTRODUCTION,0.0658578856152513,"various compression objectives that are practical to achieve. These include also compressing the
36"
INTRODUCTION,0.06759098786828423,"model broadcasted from server to clients for local training, and reducing the computational burden
37"
INTRODUCTION,0.06932409012131716,"of local training. These objectives are mostly complementary, but compressing gradients has the
38"
INTRODUCTION,0.07105719237435008,"potential for the greatest practical impact due to slower upload speeds of client connections and the
39"
INTRODUCTION,0.07279029462738301,"beneﬁts of averaging [KMA+21]. In this paper we will focus on this latter problem.
40"
INTRODUCTION,0.07452339688041594,"An important subclass of compressors are the sketches. Sketches are linear operators deﬁned on
41"
INTRODUCTION,0.07625649913344887,"Rd, i.e., C(y) = Sy for every y ∈Rd, where S is a random matrix. A standard example of such
42"
INTRODUCTION,0.0779896013864818,"a compressor is the Rand-k compressor, which randomly chooses k entries of its argument and
43"
INTRODUCTION,0.07972270363951472,"scales them with a scalar multiplier to make the estimator unbiased. Instead of communicating all d
44"
INTRODUCTION,0.08145580589254767,"coordinates of the gradient, one communicates only a subset of size k, thus reducing the number of
45"
INTRODUCTION,0.0831889081455806,"communicated bits by a factor of d/k. Formally, Rand-k is deﬁned as follows: S = Pk
j=1
d
ke⊤
ije⊤
ij,
46"
INTRODUCTION,0.08492201039861352,"where ij are the selected coordinates of the input vector. We refer the reader to [SSR22] for an
47"
INTRODUCTION,0.08665511265164645,"overview on compressions.
48"
INTRODUCTION,0.08838821490467938,"Besides the assumption that function f is bounded from below, we also assume that it is L matrix
49"
INTRODUCTION,0.09012131715771231,"smooth, as we are trying to take advantage of the entire information contained in the smoothness
50"
INTRODUCTION,0.09185441941074524,"matrix L and the stepsize matrix D.
51"
INTRODUCTION,0.09358752166377816,"Assumption 2 (Matrix smoothness). There exists L ∈Sd
+ such that
52"
INTRODUCTION,0.09532062391681109,"f(x) ≤f(y) + ⟨∇f(y), x −y⟩+ 1"
INTRODUCTION,0.09705372616984402,"2 ⟨L(x −y), x −y⟩
(4)"
INTRODUCTION,0.09878682842287695,"holds for all x, y ∈Rd.
53"
INTRODUCTION,0.10051993067590988,"The assumption of matrix smoothness, which is a generalization of scalar smoothness, has been
54"
INTRODUCTION,0.1022530329289428,"shown to be a more powerful tool for improving supervised model training. In [SHR21], the authors
55"
INTRODUCTION,0.10398613518197573,"proposed using smoothness matrices and suggested a novel communication sparsiﬁcation strategy to
56"
INTRODUCTION,0.10571923743500866,"reduce communication complexity in distributed optimization. The technique was adapted to three
57"
INTRODUCTION,0.10745233968804159,"distributed optimization algorithms in the convex setting, resulting in signiﬁcant communication
58"
INTRODUCTION,0.10918544194107452,"complexity savings and consistently outperforming the baselines. The results of this study demonstrate
59"
INTRODUCTION,0.11091854419410745,"the efﬁcacy of the matrix smoothness assumption in improving distributed optimization algorithms.
60"
INTRODUCTION,0.11265164644714037,"The case of block-diagonal smoothness matrices is particularly relevant in various applications, such
61"
INTRODUCTION,0.11438474870017332,"as neural networks (NN). In this setting, each block corresponds to a layer of the network, and we
62"
INTRODUCTION,0.11611785095320624,"characterize the smoothness with respect to nodes in the i-th layer by a corresponding matrix Li.
63"
INTRODUCTION,0.11785095320623917,"Unlike in the scalar setting, we favor the similarity of certain entries of the argument over the others.
64"
INTRODUCTION,0.1195840554592721,"This is because the information carried by the layers becomes more complex, while the nodes in the
65"
INTRODUCTION,0.12131715771230503,"same layers are similar. This phenomenon has been observed visually in various studies, such as
66"
INTRODUCTION,0.12305025996533796,"those by [YCN+15] and [ZCAW17].
67"
INTRODUCTION,0.12478336221837089,"Another motivation for using a layer-dependent stepsize has its roots in physics. In nature, the
68"
INTRODUCTION,0.1265164644714038,"propagation speed of light in media of different densities varies due to frequency variations. Similarly,
69"
INTRODUCTION,0.12824956672443674,"different layers in neural networks carry different information, metric systems, and scaling. Thus, the
70"
INTRODUCTION,0.12998266897746968,"stepsizes need to be picked accordingly to achieve optimal convergence.
71"
INTRODUCTION,0.1317157712305026,"We study two matrix stepsized CGD-type algorithms and analyze their convergence properties for
72"
INTRODUCTION,0.13344887348353554,"non-convex matrix-smooth functions. As mentioned earlier, we put special emphasis on the block-
73"
INTRODUCTION,0.13518197573656845,"diagonal case. We design our sketches and stepsizes in a way that leverages this structure, and we
74"
INTRODUCTION,0.1369150779896014,"show that in certain cases, we can achieve compression without losing in the overall communication
75"
INTRODUCTION,0.1386481802426343,"complexity.
76"
RELATED WORK,0.14038128249566725,"1.1
Related work
77"
RELATED WORK,0.14211438474870017,"Many successful convex optimization techniques have been adapted for use in the non-convex
78"
RELATED WORK,0.1438474870017331,"setting. Here is a non-exhaustive list: adaptivity [DOG+19, ZKV+20], variance reduction [JRSPS16,
79"
RELATED WORK,0.14558058925476602,"LBZR21], and acceleration [GNDG19]. A paper of particular importance for our work is that of
80"
RELATED WORK,0.14731369150779897,"[KR20], which proposes a uniﬁed scheme for analyzing stochastic gradient descent in the non-convex
81"
RELATED WORK,0.14904679376083188,"regime. A comprehensive overview of non-convex optimization can be found in [JK+17, DDG+22].
82"
RELATED WORK,0.15077989601386482,"A classical example of a matrix stepsized method is Newton’s method. This method has been popular
83"
RELATED WORK,0.15251299826689774,"in the optimization community for a long time [GT74, Mie80, Yam87]. However, computing the
84"
RELATED WORK,0.15424610051993068,"stepsize as the inverse Hessian of the current iteration results in signiﬁcant computational complexity.
85"
RELATED WORK,0.1559792027729636,"Instead, quasi-Newton methods use an easily computable estimator to replace the inverse of the
86"
RELATED WORK,0.15771230502599654,"Hessian [Bro65, DM77, ABK07, ABSM14]. An example is the Newton-Star algorithm [IQR21],
87"
RELATED WORK,0.15944540727902945,"which we discuss in Section 2.
88"
RELATED WORK,0.1611785095320624,"[GR15] analyzed sketched gradient descent by making the compressors unbiased with a sketch-and-
89"
RELATED WORK,0.16291161178509533,"project trick. They provided an analysis of the resulting algorithm for the linear feasibility problem.
90"
RELATED WORK,0.16464471403812825,"Later, [HMR18] proposed a variance-reduced version of this method.
91"
RELATED WORK,0.1663778162911612,"Leveraging the layer-wise structure of neural networks has been widely studied for optimizing the
92"
RELATED WORK,0.1681109185441941,"training loss function. For example, [ZTJY19] propose SGD with different scalar stepsizes for each
93"
RELATED WORK,0.16984402079722705,"layer, [YHL+17, GCH+19] propose layer-wise normalization for Stochastic Normalized Gradient
94"
RELATED WORK,0.17157712305025996,"Descent, and [DBA+20, WSR22] propose layer-wise compression in the distributed setting.
95"
RELATED WORK,0.1733102253032929,"DCGD, proposed by [KFJ18], has since been improved in various ways, such as in [HHH+19,
96"
RELATED WORK,0.17504332755632582,"LKQR20]. There is also a large body of literature on other federated learning algorithms with
97"
RELATED WORK,0.17677642980935876,"unbiased compressors [AGL+17, MGTR19, GBLR21, MMSR22, MSR22, HKM+23].
98"
CONTRIBUTIONS,0.17850953206239167,"1.2
Contributions
99"
CONTRIBUTIONS,0.18024263431542462,"Our paper contributes in the following ways:
100"
CONTRIBUTIONS,0.18197573656845753,"• We propose two novel matrix stepsize sketch CGD algorithms in Section 2, which, to the
101"
CONTRIBUTIONS,0.18370883882149047,"best of our knowledge, are the ﬁrst attempts to analyze a ﬁxed matrix stepsize for non-
102"
CONTRIBUTIONS,0.1854419410745234,"convex optimization. We present a uniﬁed theorem in Section 3 that guarantees stationarity
103"
CONTRIBUTIONS,0.18717504332755633,"for minimizing matrix-smooth non-convex functions. The results shows that taking our
104"
CONTRIBUTIONS,0.18890814558058924,"algorithms improve on their scalar alterantives. The complexities are summarized in Table 1
105"
CONTRIBUTIONS,0.19064124783362218,"for some particular cases.
106"
CONTRIBUTIONS,0.1923743500866551,"• We design our algorithms’ sketches and stepsize to take advantage of the layer-wise structure
107"
CONTRIBUTIONS,0.19410745233968804,"of neural networks, assuming that the smoothness matrix is block-diagonal. In Section 4,
108"
CONTRIBUTIONS,0.19584055459272098,"we prove that our algorithms achieve better convergence than classical methods.
109"
CONTRIBUTIONS,0.1975736568457539,"• Assuming the that the server-to-client communication is less expensive [KMY+16,
110"
CONTRIBUTIONS,0.19930675909878684,"KMA+21], we propose distributed versions of our algorithms in Section 5, following
111"
CONTRIBUTIONS,0.20103986135181975,"the standard FL scheme, and prove weighted stationarity guarantees. Our theorem recovers
112"
CONTRIBUTIONS,0.2027729636048527,"the result for DCGD in the scalar case and improves it in general.
113"
CONTRIBUTIONS,0.2045060658578856,"• We validate our theoretical results with experiments. The plots and framework are provided
114"
CONTRIBUTIONS,0.20623916811091855,"in the Appendix.
115"
PRELIMINARIES,0.20797227036395147,"1.3
Preliminaries
116"
PRELIMINARIES,0.2097053726169844,"The usual Euclidean norm on Rd is deﬁned as ∥·∥. We use bold capital letters to denote matrices.
117"
PRELIMINARIES,0.21143847487001732,"By Id we denote the d × d identity matrix, and by Od we denote the d × d zero matrix. Let Sd
++
118"
PRELIMINARIES,0.21317157712305027,"(resp. Sd
+) be the set of d × d symmetric positive deﬁnite (resp. semi-deﬁnite) matrices. Given
119"
PRELIMINARIES,0.21490467937608318,"Q ∈Sd
++ and x ∈Rd, we write ∥x∥Q :=
p"
PRELIMINARIES,0.21663778162911612,"⟨Qx, x⟩, where ⟨·, ·⟩is the standard Euclidean inner
120"
PRELIMINARIES,0.21837088388214904,"product on Rd. For a matrix A ∈Sd
++, we deﬁne by λmax(A) (resp. λmin(A)) the largest (resp.
121"
PRELIMINARIES,0.22010398613518198,"smallest) eigenvalue of the matrix A. Let Ai ∈Rdi×di and d = d1 + . . . + dℓ. Then the matrix
122"
PRELIMINARIES,0.2218370883882149,"A = Diag(A1, . . . , Aℓ) is deﬁned as a block diagonal d × d matrix where the i-th block is equal to
123"
PRELIMINARIES,0.22357019064124783,"Ai. We will use diag(A) ∈Rd×d to denote the diagonal of any matrix A ∈Rd×d. Given a function
124"
PRELIMINARIES,0.22530329289428075,"f : Rd →R, its gradient and its Hessian at point x ∈Rd are respectively denoted as ∇f(x) and
125"
PRELIMINARIES,0.2270363951473137,"∇2f(x).
126"
THE ALGORITHMS,0.22876949740034663,"2
The algorithms
127"
THE ALGORITHMS,0.23050259965337955,"Below we deﬁne our two main algorithms:
128"
THE ALGORITHMS,0.2322357019064125,"xk+1 = xk −DSk∇f(xk),
(det-CGD1)"
THE ALGORITHMS,0.2339688041594454,"and
129"
THE ALGORITHMS,0.23570190641247835,"xk+1 = xk −T kD∇f(xk).
(det-CGD2)"
THE ALGORITHMS,0.23743500866551126,"Here, D ∈Sd
++ is the ﬁxed stepsize matrix. The sequences of random matrices Sk and T k satisfy
130"
THE ALGORITHMS,0.2391681109185442,"the next assumption.
131"
THE ALGORITHMS,0.24090121317157712,"Assumption 3. We will assume that the random sketches that appear in our algorithms are i.i.d.,
132"
THE ALGORITHMS,0.24263431542461006,"unbiased, symmetric and positive semi-deﬁnite for each algorithm. That is
133"
THE ALGORITHMS,0.24436741767764297,"Sk, T k ∈Sd
+,
Sk iid
∼S
and
T k iid
∼T"
THE ALGORITHMS,0.24610051993067592,"E

Sk
= E

T k
= Id,
for every
k ∈N."
THE ALGORITHMS,0.24783362218370883,"A simple instance of det-CGD1 and det-CGD2 is the vanilla GD. Indeed, if Sk = T k = Id and
134"
THE ALGORITHMS,0.24956672443674177,"D = γId, then xk+1 = xk −γ∇f(xk). In general, one may view these algorithms as Newton-type
135"
THE ALGORITHMS,0.2512998266897747,"methods. In particular, our setting includes the Newton Star (NS) algorithm by [IQR21]:
136"
THE ALGORITHMS,0.2530329289428076,"xk+1 = xk −
 
∇2f(xinf)
−1 ∇f(xk).
(NS)"
THE ALGORITHMS,0.25476603119584057,"The authors prove that in the convex case it converges to the unique solution xinf locally quadratically,
137"
THE ALGORITHMS,0.2564991334488735,"provided certain assumptions are met. However, it is not a practical method as it requires knowledge
138"
THE ALGORITHMS,0.2582322357019064,"of the Hessian at the optimal point. This method, nevertheless, hints that constant matrix stepsize can
139"
THE ALGORITHMS,0.25996533795493937,"yield fast convergence guarantees. Our results allow us to choose the D depending on the smoothness
140"
THE ALGORITHMS,0.2616984402079723,"matrix L. The latter can be seen as a uniform upper bound on the Hessian.
141"
THE ALGORITHMS,0.2634315424610052,"The difference between det-CGD1 and det-CGD2 is the update rule. In particular, the order of the
142"
THE ALGORITHMS,0.2651646447140381,"sketch and the stepsize is interchanged. When the sketch S and the stepsize D are commutative w.r.t.
143"
THE ALGORITHMS,0.2668977469670711,"matrix product, the algorithms become equivalent. In general, a simple calculation shows that if we
144"
THE ALGORITHMS,0.268630849220104,"take
145"
THE ALGORITHMS,0.2703639514731369,"T k = DSkD−1,
(5)"
THE ALGORITHMS,0.2720970537261698,"then det-CGD1 and det-CGD2 are the same. Deﬁning T k according to (5), we recover the unbiased-
146"
THE ALGORITHMS,0.2738301559792028,"ness condition:
147"
THE ALGORITHMS,0.2755632582322357,"E

T k
= DE

Sk
D−1 = Id.
(6)"
THE ALGORITHMS,0.2772963604852686,"However, in general DE

Sk
D−1 is not necessarily symmetric, which contradicts to Assumption 3.
148"
THE ALGORITHMS,0.27902946273830154,"Thus, det-CGD1 and det-CGD2 are not equivalent for our purposes.
149"
MAIN RESULTS,0.2807625649913345,"3
Main results
150"
MAIN RESULTS,0.2824956672443674,"Before we state the main result, we present a stepsize condition for det-CGD1 and det-CGD2,
151"
MAIN RESULTS,0.28422876949740034,"respectively:
152"
MAIN RESULTS,0.28596187175043325,"E

SkDLDSk
⪯D,
(7)"
MAIN RESULTS,0.2876949740034662,"and
153"
MAIN RESULTS,0.28942807625649913,"E

DT kLT kD

⪯D.
(8)"
MAIN RESULTS,0.29116117850953205,"In the case of vanilla GD (7) and (8) become γ < L−1, which is the standard condition for conver-
154"
MAIN RESULTS,0.292894280762565,"gence.
155"
MAIN RESULTS,0.29462738301559793,"Below is the main convergence theorem for both algorithms in the single-node regime.
156"
MAIN RESULTS,0.29636048526863085,"Theorem 1. Suppose that Assumptions 1-3 are satisﬁed. Then, for each k ≥0
157"
K,0.29809358752166376,"1
K K−1
X"
K,0.29982668977469673,"k=0
E
h∇f(xk)
2 D"
K,0.30155979202772965,"i
≤2(f(x0) −f inf)"
K,0.30329289428076256,"K
,
(9)"
K,0.3050259965337955,"if one of the below conditions is true:
158"
K,0.30675909878682844,"i) The vectors xk are the iterates of det-CGD1 and D satisﬁes (7);
159"
K,0.30849220103986136,"ii) The vectors xk are the iterates of det-CGD2 and D satisﬁes (8).
160"
K,0.31022530329289427,"It is important to note that Theorem 1 yields the same convergence rate for any D ∈Sd
++, despite
161"
K,0.3119584055459272,"the fact that the matrix norms on the left-hand side cannot be compared for different weight matrices.
162"
K,0.31369150779896016,"To ensure comparability of the right-hand side of (9), it is necessary to normalize the weight matrix
163"
K,0.31542461005199307,"D that is used to measure the gradient norm. We propose using determinant normalization, which
164"
K,0.317157712305026,"involves dividing both sides of (9) by det(D)1/d, yielding the following:
165"
K,0.3188908145580589,"1
K K−1
X"
K,0.32062391681109187,"k=0
E
∇f(xk)
2"
K,0.3223570190641248,"D
det(D)1/d"
K,0.3240901213171577,"
≤2(f(x0) −f inf)"
K,0.32582322357019067,"det(D)1/dK .
(10)"
K,0.3275563258232236,"This normalization is meaningful because adjusting the weight matrix to
D
det(D)1/d allows its determi-
166"
K,0.3292894280762565,"nant to be 1, making the norm on the left-hand side comparable to the standard Euclidean norm. It is
167"
K,0.3310225303292894,"important to note that the volume of the normalized ellipsoid

x ∈Rd : ∥x∥2
D/det(D)1/d ≤1
	
does
168"
K,0.3327556325823224,"not depend on the choice of D ∈Sd
++. Therefore, the results of (9) are comparable across different
169"
K,0.3344887348353553,"D in the sense that the right-hand side of (9) measures the volume of the ellipsoid containing the
170"
K,0.3362218370883882,"gradient.
171"
OPTIMAL MATRIX STEPSIZE,0.3379549393414211,"3.1
Optimal matrix stepsize
172"
OPTIMAL MATRIX STEPSIZE,0.3396880415944541,"In this section, we describe how to choose the optimal stepsize that minimizes the iteration complexity.
173"
OPTIMAL MATRIX STEPSIZE,0.341421143847487,"The problem is easier for det-CGD2. We notice that (8) can be explicitly solved. Speciﬁcally, it is
174"
OPTIMAL MATRIX STEPSIZE,0.3431542461005199,"equivalent to
175"
OPTIMAL MATRIX STEPSIZE,0.34488734835355284,"D ⪯
 
E

T kLT k−1 .
(11)"
OPTIMAL MATRIX STEPSIZE,0.3466204506065858,"We want to emphasize that the RHS matrix is invertible despite the sketches not being so. Indeed.
176"
OPTIMAL MATRIX STEPSIZE,0.3483535528596187,"The map h : T →T LT is convex on Sd
+. Therefore, Jensen’s inequality implies
177"
OPTIMAL MATRIX STEPSIZE,0.35008665511265163,"E

T kLT k
⪰E

T k
LE

T k
= L ≻Od."
OPTIMAL MATRIX STEPSIZE,0.35181975736568455,"This explicit condition on D can assist in determining the optimal stepsize. Since both D and
178"
OPTIMAL MATRIX STEPSIZE,0.3535528596187175,"(T kLT k)−1 are positive deﬁnite, then the right-hand side of (10) is minimized exactly when
179"
OPTIMAL MATRIX STEPSIZE,0.35528596187175043,"D = (T kLT k)−1.
(12)"
OPTIMAL MATRIX STEPSIZE,0.35701906412478335,"The situation is different for det-CGD1. According to (10), the optimal D is deﬁned as the solution
180"
OPTIMAL MATRIX STEPSIZE,0.3587521663778163,"of the following constrained optimization problem:
181"
OPTIMAL MATRIX STEPSIZE,0.36048526863084923,"minimize
log det(D−1)"
OPTIMAL MATRIX STEPSIZE,0.36221837088388215,"subject to
E

SkDLDSk
⪯D
(13)"
OPTIMAL MATRIX STEPSIZE,0.36395147313691506,"D ∈Sd
++. 182"
OPTIMAL MATRIX STEPSIZE,0.36568457538994803,"Proposition 1. The optimization problem (13) with respect to stepsize matrix D ∈Sd
++, is a convex
183"
OPTIMAL MATRIX STEPSIZE,0.36741767764298094,"optimization problem with convex constraint.
184"
OPTIMAL MATRIX STEPSIZE,0.36915077989601386,"The proof of this proposition can be found in the Appendix. It is based on the reformulation of the
185"
OPTIMAL MATRIX STEPSIZE,0.3708838821490468,"constraint to its equivalent quadratic form inequality. Using the trace trick, we can prove that for
186"
OPTIMAL MATRIX STEPSIZE,0.37261698440207974,"every vector chosen in the quadratic form, it is convex. Since the intersection of convex sets is convex,
187"
OPTIMAL MATRIX STEPSIZE,0.37435008665511266,"we conclude the proof.
188"
OPTIMAL MATRIX STEPSIZE,0.37608318890814557,"One could consider using the CVXPY [DB16] package to solve (13), provided that it is ﬁrst transformed
189"
OPTIMAL MATRIX STEPSIZE,0.3778162911611785,"into a Disciplined Convex Programming (DCP) form [GBY06]. Nevertheless, (7) is not recognized
190"
OPTIMAL MATRIX STEPSIZE,0.37954939341421146,"as a DCP constraint in the general case. To make CVXPY applicable, additional steps tailored to the
191"
OPTIMAL MATRIX STEPSIZE,0.38128249566724437,"problem at hand must be taken.
192"
OPTIMAL MATRIX STEPSIZE,0.3830155979202773,"Table 1: Summary of communication complexities of det-CGD1 and det-CGD2 with different
sketches and stepsize matrices. The Di here for det-CGD1 is Wi with the optimal scaling determined
using Theorem 2, for det-CGD2 it is the optimal stepsize matrix deﬁned in (11). The constant
2(f(x0) −f inf)/ε2 is hidden, ℓis the number of layers, ki is the mini-batch size for the i-th layer if
we use the rand-k sketch. The notation ˜Li,k is deﬁned as d−k"
OPTIMAL MATRIX STEPSIZE,0.3847487001733102,d−1 diag(Li) + k−1
OPTIMAL MATRIX STEPSIZE,0.38648180242634317,d−1Li.
OPTIMAL MATRIX STEPSIZE,0.3882149046793761,"No.
The
method"
OPTIMAL MATRIX STEPSIZE,0.389948006932409,"
Sk
i , Di

l ≥1, di, ki, Pℓ
i=1 ki = k, layer structure
l = 1, ki = k, general structure"
OPTIMAL MATRIX STEPSIZE,0.39168110918544197,"1.
det-CGD1"
OPTIMAL MATRIX STEPSIZE,0.3934142114384749,"
Id, γL−1
i"
OPTIMAL MATRIX STEPSIZE,0.3951473136915078,"
d · det(L)1/d
d · det(L)1/d"
OPTIMAL MATRIX STEPSIZE,0.3968804159445407,"2.
det-CGD1"
OPTIMAL MATRIX STEPSIZE,0.3986135181975737,"
Id, γ diag−1(Li)

d · det
 
diag(L)
1/d
d · det
 
diag(L)
1/d"
OPTIMAL MATRIX STEPSIZE,0.4003466204506066,"3.
det-CGD1"
OPTIMAL MATRIX STEPSIZE,0.4020797227036395,"
Id, γIdi"
OPTIMAL MATRIX STEPSIZE,0.4038128249566724,"
d ·
Ql
i=1 λdi
max(Li)
1/d
d · λmax(L)"
OPTIMAL MATRIX STEPSIZE,0.4055459272097054,"4.
det-CGD1"
OPTIMAL MATRIX STEPSIZE,0.4072790294627383,"
rand-1, γIdi"
OPTIMAL MATRIX STEPSIZE,0.4090121317157712,"
ℓ·
Ql
i=1 ddi
i
 
maxj(Li)jj
di
1/d
d · maxj(Ljj)"
OPTIMAL MATRIX STEPSIZE,0.41074523396880414,"5.
det-CGD1"
OPTIMAL MATRIX STEPSIZE,0.4124783362218371,"
rand-1, γL−1
i 
ℓ· "
OPTIMAL MATRIX STEPSIZE,0.41421143847487,"


"
OPTIMAL MATRIX STEPSIZE,0.41594454072790293,"Ql
i=1 ddi
i
λdi
max "
OPTIMAL MATRIX STEPSIZE,0.41767764298093585,"L
1
2
i
diag(L−1
i
)L
1
2
i !"
OPTIMAL MATRIX STEPSIZE,0.4194107452339688,"Ql
i=1 det(L−1
i
) "
OPTIMAL MATRIX STEPSIZE,0.42114384748700173,"


"
OPTIMAL MATRIX STEPSIZE,0.42287694974003465,"1/d
dλmax "
OPTIMAL MATRIX STEPSIZE,0.4246100519930676,"L
1
2 diag

L−1
L
1
2 !"
OPTIMAL MATRIX STEPSIZE,0.42634315424610053,"det

L−1
1/d"
OPTIMAL MATRIX STEPSIZE,0.42807625649913345,"6.
det-CGD1"
OPTIMAL MATRIX STEPSIZE,0.42980935875216636,"
rand-1, γL−1/2
i 
ℓ·"
OPTIMAL MATRIX STEPSIZE,0.43154246100519933,"Ql
i=1 ddi
i
λdi
max(L1/2
i
)
Ql
i=1 det(L−1/2
i
)"
OPTIMAL MATRIX STEPSIZE,0.43327556325823224,"!1/d
d · λ1/2
max(L) det(L)1/(2d)"
OPTIMAL MATRIX STEPSIZE,0.43500866551126516,"7.
det-CGD1"
OPTIMAL MATRIX STEPSIZE,0.43674176776429807,"
rand-1, γ diag−1(Li)

ℓ·"
OPTIMAL MATRIX STEPSIZE,0.43847487001733104,"Ql
i=1 ddi
i
Qd
j=1(L−1
jj )"
OPTIMAL MATRIX STEPSIZE,0.44020797227036396,"!1/d
d · det
 
diag(L)
1/d"
OPTIMAL MATRIX STEPSIZE,0.44194107452339687,"8.
det-CGD1"
OPTIMAL MATRIX STEPSIZE,0.4436741767764298,"
rand-ki, γ diag−1(Li)

k ·
Ql
i=1
 di ki"
OPTIMAL MATRIX STEPSIZE,0.44540727902946275,"di det
 
diag(L)
1/d
d · det
 
diag(L)
1/d"
OPTIMAL MATRIX STEPSIZE,0.44714038128249567,"9.
det-CGD2"
OPTIMAL MATRIX STEPSIZE,0.4488734835355286,"
Id, L−1
i"
OPTIMAL MATRIX STEPSIZE,0.4506065857885615,"
d · det(L)1/d
d · det(L)1/d"
OPTIMAL MATRIX STEPSIZE,0.45233968804159447,"10.
det-CGD2"
OPTIMAL MATRIX STEPSIZE,0.4540727902946274,"
rand-1, diag−1(Li) di"
OPTIMAL MATRIX STEPSIZE,0.4558058925476603,"
ℓ·
Ql
i=1 ddi
i"
OPTIMAL MATRIX STEPSIZE,0.45753899480069327,"1/d det(diag L)1/d
d · det(diag(L))1/d"
OPTIMAL MATRIX STEPSIZE,0.4592720970537262,"11.
det-CGD2"
OPTIMAL MATRIX STEPSIZE,0.4610051993067591,"
rand-k, ki"
OPTIMAL MATRIX STEPSIZE,0.462738301559792,"di
˜
L−1
i,ki 
k ·"
OPTIMAL MATRIX STEPSIZE,0.464471403812825,"Ql
i=1
 di ki  di"
OPTIMAL MATRIX STEPSIZE,0.4662045060658579,"d
! Ql
i=1 det( ˜
Li,ki )
1/d
d · det( ˜
L1,k)"
OPTIMAL MATRIX STEPSIZE,0.4679376083188908,"12.
det-CGD2"
OPTIMAL MATRIX STEPSIZE,0.4696707105719237,"
Bern-qi, qiL−1
i"
OPTIMAL MATRIX STEPSIZE,0.4714038128249567,"
Pl
i=1 qidi

· Ql
i=1

1
qi  di"
OPTIMAL MATRIX STEPSIZE,0.4731369150779896,"d det(L)1/d
d · det(L)1/d"
GD,0.4748700173310225,"13.
GD

Id, λ−1
max(L)Id

N/A
d · λmax(L)"
LEVERAGING THE LAYER-WISE STRUCTURE,0.47660311958405543,"4
Leveraging the layer-wise structure
193"
LEVERAGING THE LAYER-WISE STRUCTURE,0.4783362218370884,"In this section we focus on the block-diagonal case of L for both det-CGD1 and det-CGD2. In
194"
LEVERAGING THE LAYER-WISE STRUCTURE,0.4800693240901213,"particular, we propose hyper-parameters of det-CGD1 designed speciﬁcally for training NNs. Let
195"
LEVERAGING THE LAYER-WISE STRUCTURE,0.48180242634315423,"us assume that L = Diag(L1, . . . , Lℓ), where Li ∈Sdi
++. This setting is a generalization of the
196"
LEVERAGING THE LAYER-WISE STRUCTURE,0.48353552859618715,"classical smoothness condition, as in the latter case Li = LIdi for all i = 1, . . . , ℓ. Respectively,
197"
LEVERAGING THE LAYER-WISE STRUCTURE,0.4852686308492201,"we choose both the sketches and the stepsize to be block diagonal: D = Diag(D1, . . . , Dℓ) and
198"
LEVERAGING THE LAYER-WISE STRUCTURE,0.48700173310225303,"Sk = Diag(Sk
1, . . . , Sk
ℓ), where Di, Sk
i ∈Sdi
++.
199"
LEVERAGING THE LAYER-WISE STRUCTURE,0.48873483535528595,"Let us notice that the left hand side of the inequality constraint in (13) has quadratic dependence on
200"
LEVERAGING THE LAYER-WISE STRUCTURE,0.4904679376083189,"D, while the right hand side is linear. Thus, for every matrix W ∈Sd
++, there exists γ > 0 such that
201"
LEVERAGING THE LAYER-WISE STRUCTURE,0.49220103986135183,"γ2λmax
 
E

SkW LW Sk
≤γλmin(W ).
Therefore, for γW we deduce
202"
LEVERAGING THE LAYER-WISE STRUCTURE,0.49393414211438474,"E

Sk(γW )L(γW )Sk
⪯γ2λmax
 
E

SkW LW Sk
Id ⪯γλmin(W )Id ⪯γW .
(14)
The following theorem is based on this simple fact applied to the corresponding blocks of the matrices
203"
LEVERAGING THE LAYER-WISE STRUCTURE,0.49566724436741766,"D, L, Sk for det-CGD1.
204"
LEVERAGING THE LAYER-WISE STRUCTURE,0.49740034662045063,"Theorem 2. Let f : Rd →R satisfy Assumptions 1 and 2, with L admitting the layer-separable struc-
205"
LEVERAGING THE LAYER-WISE STRUCTURE,0.49913344887348354,"ture L = Diag(L1, . . . , Lℓ), where L1, . . . , Lℓ∈Sdi
++. Choose random matrices Sk
1, . . . , Sk
ℓ∈Sd
+
206"
LEVERAGING THE LAYER-WISE STRUCTURE,0.5008665511265165,"to satisfy Assumption 3 for all i ∈[ℓ], and let Sk := Diag(Sk
1, . . . , Sk
ℓ). Furthermore, choose
207"
LEVERAGING THE LAYER-WISE STRUCTURE,0.5025996533795494,"matrices W1, . . . , Wℓ∈Sd
++ and scalars γ1, . . . , γℓ> 0 such that
208"
LEVERAGING THE LAYER-WISE STRUCTURE,0.5043327556325823,"γi ≤λ−1
max

E
h
W −1/2
i
Sk
i WiLiWiSk
i W −1/2
i
i
∀i ∈[ℓ].
(15)"
LEVERAGING THE LAYER-WISE STRUCTURE,0.5060658578856152,"Letting W := Diag(W1, . . . , Wℓ), Γ := Diag(γ1Id1, . . . , γℓIdℓ) and D := ΓW , we get
209"
K,0.5077989601386482,"1
K K−1
X"
K,0.5095320623916811,"k=0
E
∇f(xk)
2"
K,0.511265164644714,"ΓW
det(ΓW )1/d"
K,0.512998266897747,"
≤2(f(x0) −f inf)"
K,0.5147313691507799,"det (ΓW )1/d K
.
(16)"
K,0.5164644714038128,"In particular, if the scalars {γi} are chosen to be equal to their maximum allowed values from (15),
210"
K,0.5181975736568457,"then the convergence factor of (16) is equal to
211"
K,0.5199306759098787,"det (ΓW )−1 d = "" ℓ
Y"
K,0.5216637781629117,"i=1
λdi
max

E
h
W
−1"
"I
SK
I WILIWISK
I W",0.5233968804159446,"2
i
Sk
i WiLiWiSk
i W
−1"
I,0.5251299826689775,"2
i
i# 1"
I,0.5268630849220104,"d
det(W −1)
1
d ."
I,0.5285961871750433,"Table 1 contains the (expected) communication complexities of det-CGD1, det-CGD2 and GD for
212"
I,0.5303292894280762,"several choices of W , D and Sk. Here are a few comments about the table. We deduce that taking a
213"
I,0.5320623916811091,"matrix stepsize without compression (row 1) we improve GD (row 13). A careful analysis reveals
214"
I,0.5337954939341422,"that the result in row 5 is always worse than row 7 in terms of both communication and iteration
215"
I,0.5355285961871751,"complexity. However, the results in row 6 and row 7 are not comparable in general, meaning that
216"
I,0.537261698440208,"neither of them is universally better. More discussion on this table can be found in the Appendix.
217"
I,0.5389948006932409,"Compression for free.
Now, let us focus on row 12, which corresponds to a sampling scheme
218"
I,0.5407279029462738,"where the i-th layer is independently selected with probability qi. Mathematically, it goes as follows:
219"
I,0.5424610051993067,"T k
i = ηi"
I,0.5441941074523396,"qi
Idi,
where
ηi ∼Bernoulli(qi).
(17)"
I,0.5459272097053726,"Jensen’s inequality implies that
220 l
X"
I,0.5476603119584056,"i=1
qidi ! · lY i=1  1 qi  di"
I,0.5493934142114385,"d
≥d.
(18)"
I,0.5511265164644714,"The equality is attained when qi = q for all i ∈[ℓ]. The expected bits transferred per iteration of
221"
I,0.5528596187175043,"this algorithm is then equal to kexp = qd and the communication complexity equals d det(L)1/d.
222"
I,0.5545927209705372,"Comparing with the results for det-CGD2 with rand-kexp on row 11 and using the fact that det(L) ≤
223"
I,0.5563258232235702,"det (diag(L)), we deduce that the Bernoulli scheme is better than the uniform sampling scheme.
224"
I,0.5580589254766031,"Notice also, the communication complexity matches the one for the uncompressed det-CGD2
225"
I,0.5597920277296361,"displayed on row 9. This, in particular means that using the Bern-q sketches we can compress the
226"
I,0.561525129982669,"gradients for free. The latter means that we reduce the number of bits broadcasted at each iteration
227"
I,0.5632582322357019,"without losing in the total communication complexity. In particular, when all the layers have the same
228"
I,0.5649913344887348,"width di, the number of broadcasted bits for each iteration is reduced by a factor of q.
229"
DISTRIBUTED SETTING,0.5667244367417678,"5
Distributed setting
230"
DISTRIBUTED SETTING,0.5684575389948007,"In this section we describe the distributed versions of our algorithms and present convergence
231"
DISTRIBUTED SETTING,0.5701906412478336,"guarantees for them. Let us consider an objective function that is sum decomposable:
232"
DISTRIBUTED SETTING,0.5719237435008665,"f(x) := 1 n n
X"
DISTRIBUTED SETTING,0.5736568457538995,"i=1
fi(x),"
DISTRIBUTED SETTING,0.5753899480069324,"where each fi : Rd →R is a differentiable function. We assume that f satisﬁes Assumption 1 and
233"
DISTRIBUTED SETTING,0.5771230502599654,"the component functions satisfy the below condition.
234"
DISTRIBUTED SETTING,0.5788561525129983,"Assumption 4. Each component function fi is Li-smooth and is bounded from below: fi(x) ≥f inf
i
235"
DISTRIBUTED SETTING,0.5805892547660312,"for all x ∈Rd.
236"
DISTRIBUTED SETTING,0.5823223570190641,"This assumption also implies that f is of matrix smoothness with ¯L ∈Sd
++, where ¯L = 1"
DISTRIBUTED SETTING,0.584055459272097,"n
Pn
i=1 Li.
237"
DISTRIBUTED SETTING,0.58578856152513,"Following the standard FL framework [KMY+16, MMR+17, KFJ18], we assume that the i-th
238"
DISTRIBUTED SETTING,0.587521663778163,"component function fi is stored on the i-th client. At each iteration, the clients in parallel compute
239"
DISTRIBUTED SETTING,0.5892547660311959,"and compress the local gradient ∇fi and communicate it to the central server. The server, then
240"
DISTRIBUTED SETTING,0.5909878682842288,"aggregates the compressed gradients, computes the next iterate, and in parallel broadcasts it to the
241"
DISTRIBUTED SETTING,0.5927209705372617,"clients. See the algorithms below for the pseudo-codes.
242"
DISTRIBUTED SETTING,0.5944540727902946,Algorithm 1 Distributed det-CGD1
DISTRIBUTED SETTING,0.5961871750433275,"1: Input: Starting point x0, stepsize matrix D,
number of iterations K
2: for k = 0, 1, 2, . . . , K −1 do
3:
The devices in parallel:"
DISTRIBUTED SETTING,0.5979202772963604,"4:
sample Sk
i ∼S;
5:
compute Sk
i ∇fi(xk);
6:
broadcast Sk
i ∇fi(xk).
7:
The server:
8:
combines gk = D"
DISTRIBUTED SETTING,0.5996533795493935,"n
Pn
i Sk
i ∇fi(xk);
9:
computes xk+1 = xk −gk;
10:
broadcasts xk+1.
11: end for
12: Return: xK"
DISTRIBUTED SETTING,0.6013864818024264,Algorithm 2 Distributed det-CGD2
DISTRIBUTED SETTING,0.6031195840554593,"1: Input: Starting point x0, stepsize matrix D,
number of iterations K
2: for k = 0, 1, 2, . . . , K −1 do
3:
The devices in parallel:"
DISTRIBUTED SETTING,0.6048526863084922,"4:
sample T k
i ∼T ;
5:
compute T k
i D∇fi(xk);
6:
broadcast T k
i D∇fi(xk).
7:
The server:
8:
combines gk = 1"
DISTRIBUTED SETTING,0.6065857885615251,"n
Pn
i T k
i D∇fi(xk);
9:
computes xk+1 = xk −gk;
10:
broadcasts xk+1.
11: end for
12: Return: xK 243"
DISTRIBUTED SETTING,0.608318890814558,"Theorem 3. Let fi : Rd →R satisfy Assumption 4 and let f satisfy Assumption 1 and Assumption 2
244"
DISTRIBUTED SETTING,0.610051993067591,"with smoothness matrix L. If the stepsize satisﬁes
245"
DISTRIBUTED SETTING,0.6117850953206239,"DLD ⪯D,
(19)"
DISTRIBUTED SETTING,0.6135181975736569,"then the following convergence bound is true for the iteration of Algorithm 1:
246"
DISTRIBUTED SETTING,0.6152512998266898,"min
0≤k≤K−1 E
∇f(xk)
2"
DISTRIBUTED SETTING,0.6169844020797227,"D
det(D)1/d"
DISTRIBUTED SETTING,0.6187175043327556,"
≤2(1 + λD"
DISTRIBUTED SETTING,0.6204506065857885,"n )K  
f(x0) −f inf"
DISTRIBUTED SETTING,0.6221837088388215,"det(D)1/d K
+
2λD∆inf"
DISTRIBUTED SETTING,0.6239168110918544,"det(D)1/d n,
(20)"
DISTRIBUTED SETTING,0.6256499133448874,where ∆inf := f inf −1
DISTRIBUTED SETTING,0.6273830155979203,"n
Pn
i=1 f inf
i
and
247"
DISTRIBUTED SETTING,0.6291161178509532,"λD := max
i"
DISTRIBUTED SETTING,0.6308492201039861,"n
λmax

E
h
L"
DISTRIBUTED SETTING,0.6325823223570191,"1
2
i
 
Sk
i −Id

DLD
 
Sk
i −Id

L"
DISTRIBUTED SETTING,0.634315424610052,"1
2
i
io
."
DISTRIBUTED SETTING,0.6360485268630849,"The same result is true for Algorithm 2 with a different constant λD. The proof of Theorem 3 and its
248"
DISTRIBUTED SETTING,0.6377816291161178,"analogue for Algorithm 2 are presented in the Appendix. The analysis is largely inspired by [KR20,
249"
DISTRIBUTED SETTING,0.6395147313691508,"Theorem 1]. Now, let us examine the right-hand side of (20). We start by observing that the ﬁrst term
250"
DISTRIBUTED SETTING,0.6412478336221837,"has exponential dependence in K. However, the term inside the brackets, 1 + λD/n, depends on the
251"
DISTRIBUTED SETTING,0.6429809358752167,"stepsize D. Furthermore, it has a second-order dependence on D, implying that λαD = α2λD, as
252"
DISTRIBUTED SETTING,0.6447140381282496,"opposed to det(αD)1/d, which is linear in α. Therefore, we can choose a small enough coefﬁcient α
253"
DISTRIBUTED SETTING,0.6464471403812825,"to ensure that λD is of order n/K. This means that for a ﬁxed number of iterations K, we choose the
254"
DISTRIBUTED SETTING,0.6481802426343154,"matrix stepsize to be ""small enough"" to guarantee that the denominator of the ﬁrst term is bounded.
255"
DISTRIBUTED SETTING,0.6499133448873483,"The following corollary summarizes these arguments, and its proof can be found in the Appendix.
256"
DISTRIBUTED SETTING,0.6516464471403813,"Corollary 1. We reach an error level of ε2 in (20) if the following conditions are satisﬁed:
257"
DISTRIBUTED SETTING,0.6533795493934142,"DLD ⪯D,
λD ≤min
 n"
DISTRIBUTED SETTING,0.6551126516464472,"K , nε2"
DISTRIBUTED SETTING,0.6568457538994801,"4∆inf det(D)1/d

,
K ≥12(f(x0) −f inf)"
DISTRIBUTED SETTING,0.658578856152513,"det(D)1/d ε2
.
(21)"
DISTRIBUTED SETTING,0.6603119584055459,"Proposition 2 in the Appendix proves that these conditions with respect to D are convex. In order to
258"
DISTRIBUTED SETTING,0.6620450606585788,"minimize the iteration complexity for getting ε2 error, one needs to solve the following optimization
259"
DISTRIBUTED SETTING,0.6637781629116117,"problem
260"
DISTRIBUTED SETTING,0.6655112651646448,"minimize
log det(D−1)
subject to
D
satisﬁes
(21)."
DISTRIBUTED SETTING,0.6672443674176777,"Choosing the optimal stepsize for Algorithm 1 is analogous to solving (13). One can formulate the
261"
DISTRIBUTED SETTING,0.6689774696707106,"distributed counterpart of Theorem 2 and attempt to solve it for different sketches. Furthermore,
262"
DISTRIBUTED SETTING,0.6707105719237435,"this leads to a convex matrix minimization problem involving D. We provide a formal proof of this
263"
DISTRIBUTED SETTING,0.6724436741767764,"property in the Appendix. Similar to the single-node case, computational methods can be employed
264"
DISTRIBUTED SETTING,0.6741767764298093,"using the CVXPY package. However, some additional effort is required to transform (21) into the
265"
DISTRIBUTED SETTING,0.6759098786828422,"disciplined convex programming (DCP) format.
266"
DISTRIBUTED SETTING,0.6776429809358753,"The second term in (20) corresponds to the convergence neighborhood of the algorithm. It does
267"
DISTRIBUTED SETTING,0.6793760831889082,"not depend on the number of iteration, thus it remains unchanged, after we choose the stepsize.
268"
DISTRIBUTED SETTING,0.6811091854419411,"0
2000
4000
6000
8000
10000
Iterations 10−3 10−2 10−1 GK,D"
DISTRIBUTED SETTING,0.682842287694974,"a1a, rand-1 sketch, λ = 0.1, n = 100"
DISTRIBUTED SETTING,0.6845753899480069,"Standard DCGD
DCGD-mat
D det-CGD1 diagonal D
D det-CGD2 diagonal D"
DISTRIBUTED SETTING,0.6863084922010398,"0
2000
4000
6000
8000
10000
Iterations 10−4 10−3 10−2 GK,D"
DISTRIBUTED SETTING,0.6880415944540728,"phishing, rand-1 sketch, λ = 0.1, n = 500"
DISTRIBUTED SETTING,0.6897746967071057,"Standard DCGD
DCGD-mat
D det-CGD1 diagonal D
D det-CGD2 diagonal D"
DISTRIBUTED SETTING,0.6915077989601387,"0
2000
4000
6000
8000
10000
Iterations 10−4 10−3 10−2 10−1 GK,D"
DISTRIBUTED SETTING,0.6932409012131716,"a8a, rand-1 sketch, λ = 0.1, n = 500"
DISTRIBUTED SETTING,0.6949740034662045,"Standard DCGD
DCGD-mat
D det-CGD1 diagonal D
D det-CGD2 diagonal D"
DISTRIBUTED SETTING,0.6967071057192374,"Figure 1: Comparison of standard DCGD, DCGD with matrix smoothness, D-det-CGD1 and D-
det-CGD2 with optimal diagonal stepsizes under rand-1 sketch. The stepsize for standard DCGD
is determined using [KR20, Proposition 4], the stepsize for DCGD with matrix smoothness along
with D1, D2 is determined using Corollary 1, the error level is set to be ε2 = 0.0001. Here
GK,D := 1"
DISTRIBUTED SETTING,0.6984402079722704,"K
  PK−1
k=0
∇f(xk)
2
D/det(D)1/d

."
DISTRIBUTED SETTING,0.7001733102253033,"Nevertheless, it depends on the number of clients n. In general, the term ∆inf/n can be unbounded,
269"
DISTRIBUTED SETTING,0.7019064124783362,"when n →+∞. However, per Corollary 1, we require λD to be upper-bounded by n/K. Thus,
270"
DISTRIBUTED SETTING,0.7036395147313691,"the neighborhood term will indeed converge to zero when K →+∞, if we choose the stepsize
271"
DISTRIBUTED SETTING,0.7053726169844021,"accordingly.
272"
DISTRIBUTED SETTING,0.707105719237435,"We compare our results with the existing results for DCGD. In particular we use the technique
273"
DISTRIBUTED SETTING,0.708838821490468,"from [KR20] for the scalar smooth DCGD with scalar stepsizes. This means that the parameters of
274"
DISTRIBUTED SETTING,0.7105719237435009,"algorithms are Li = LiId, L = LId, D = γId, ω = λmax

E
h 
Sk
i
⊤Sk
i
i
−1. One may check
275"
DISTRIBUTED SETTING,0.7123050259965338,"that (21) reduces to
276"
DISTRIBUTED SETTING,0.7140381282495667,"γ ≤min
 1"
DISTRIBUTED SETTING,0.7157712305025996,"L,
r
n
KLmaxLω ,
nε2"
DISTRIBUTED SETTING,0.7175043327556326,4∆infLmaxLω
DISTRIBUTED SETTING,0.7192374350086655,"
and
Kγ ≥12(f(x0) −f inf)"
DISTRIBUTED SETTING,0.7209705372616985,"ε2
(22)"
DISTRIBUTED SETTING,0.7227036395147314,"As expected, this coincides with the results from [KR20, Corollary 1]. See the Appendix for the
277"
DISTRIBUTED SETTING,0.7244367417677643,"details on the analysis of [KR20]. Finally, we back up our theoretical ﬁndings with experiments.
278"
DISTRIBUTED SETTING,0.7261698440207972,"See Figure 1 for a simple experiment conﬁrming that Algorithms 1 and 2 have better iteration and
279"
DISTRIBUTED SETTING,0.7279029462738301,"communication complexity compared to scalar stepsized DCGD. For more details on the experiments
280"
DISTRIBUTED SETTING,0.729636048526863,"we refer the reader to the corresponding section in the Appendix.
281"
CONCLUSION,0.7313691507798961,"6
Conclusion
282"
LIMITATIONS,0.733102253032929,"6.1
Limitations
283"
LIMITATIONS,0.7348353552859619,"It is worth noting that every point in Rd can be enclosed within some volume 1 ellipsoid. To see
284"
LIMITATIONS,0.7365684575389948,"this, let 0 ̸= v ∈Rd and deﬁne Q :=
α
∥v∥2 vv⊤+ β Pd
i=1 viv⊤
i , where v1 =
v
∥v∥, v2, . . . , vd form an
285"
LIMITATIONS,0.7383015597920277,"orthonormal basis. The eigenvalues of Q are β (with multiplicity d −1) and α (with multiplicity 1),
286"
LIMITATIONS,0.7400346620450606,"so we have det(Q) = βd−1α ≤1. Furthermore, we have ∥v∥2
Q = v⊤Qv = α ∥v∥2. By choosing
287"
LIMITATIONS,0.7417677642980935,"α =
1
∥v∥2 and β = ∥v∥2/(d−1), we can obtain det(Q) = 1 while ∥v∥2
Q ≤1. Therefore, having the
288"
LIMITATIONS,0.7435008665511266,"average D-norm of the gradient bounded by a small number does not guarantee that the average
289"
LIMITATIONS,0.7452339688041595,"Euclidean norm is small. This implies that the theory does not guarantee stationarity in the Euclidean
290"
LIMITATIONS,0.7469670710571924,"sense.
291"
FUTURE WORK,0.7487001733102253,"6.2
Future work
292"
FUTURE WORK,0.7504332755632582,"Matrix stepsize gradient methods are still not well studied and require further analysis. Although
293"
FUTURE WORK,0.7521663778162911,"many important algorithms have been proposed using scalar stepsizes and are known to have good
294"
FUTURE WORK,0.7538994800693241,"performance, their matrix analogs have yet to be thoroughly examined. The distributed algorithms
295"
FUTURE WORK,0.755632582322357,"proposed in Section 5 follow the structure of DCGD by [KFJ18]. However, other federated learning
296"
FUTURE WORK,0.75736568457539,"mechanisms such as MARINA, which has variance reduction [GBLR21], or EF21 by [RSF21], which
297"
FUTURE WORK,0.7590987868284229,"has powerful practical performance, should also be explored.
298"
REFERENCES,0.7608318890814558,"References
299"
REFERENCES,0.7625649913344887,"[ABK07] Mehiddin Al-Baali and H Khalfan. An overview of some practical quasi-newton
300"
REFERENCES,0.7642980935875217,"methods for unconstrained optimization. Sultan Qaboos University Journal for Science
301"
REFERENCES,0.7660311958405546,"[SQUJS], 12(2):199–209, 2007.
302"
REFERENCES,0.7677642980935875,"[ABSM14] Mehiddin Al-Baali, Emilio Spedicato, and Francesca Maggioni. Broyden’s quasi-
303"
REFERENCES,0.7694974003466204,"Newton methods for a nonlinear system of equations and unconstrained optimization: a
304"
REFERENCES,0.7712305025996534,"review and open problems. Optimization Methods and Software, 29(5):937–954, 2014.
305"
REFERENCES,0.7729636048526863,"[AGL+17] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
306"
REFERENCES,0.7746967071057193,"Communication-efﬁcient SGD via gradient quantization and encoding. Advances in
307"
REFERENCES,0.7764298093587522,"neural information processing systems, 30, 2017.
308"
REFERENCES,0.7781629116117851,"[B+15] Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations
309"
REFERENCES,0.779896013864818,"and Trends® in Machine Learning, 8(3-4):231–357, 2015.
310"
REFERENCES,0.7816291161178509,"[Bro65] Charles G Broyden. A class of methods for solving nonlinear simultaneous equations.
311"
REFERENCES,0.7833622183708839,"Mathematics of computation, 19(92):577–593, 1965.
312"
REFERENCES,0.7850953206239168,"[CL11] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines.
313"
REFERENCES,0.7868284228769498,"ACM transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.
314"
REFERENCES,0.7885615251299827,"[DB16] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language
315"
REFERENCES,0.7902946273830156,"for convex optimization. The Journal of Machine Learning Research, 17(1):2909–2913,
316"
REFERENCES,0.7920277296360485,"2016.
317"
REFERENCES,0.7937608318890814,"[DBA+20] Aritra Dutta, El Houcine Bergou, Ahmed M Abdelmoniem, Chen-Yu Ho, Atal Narayan
318"
REFERENCES,0.7954939341421143,"Sahu, Marco Canini, and Panos Kalnis. On the discrepancy between the theoretical anal-
319"
REFERENCES,0.7972270363951474,"ysis and practical implementations of compressed communication for distributed deep
320"
REFERENCES,0.7989601386481803,"learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34,
321"
REFERENCES,0.8006932409012132,"pages 3817–3824, 2020.
322"
REFERENCES,0.8024263431542461,"[DDG+22] Marina Danilova, Pavel Dvurechensky, Alexander Gasnikov, Eduard Gorbunov, Sergey
323"
REFERENCES,0.804159445407279,"Guminov, Dmitry Kamzolov, and Innokentiy Shibaev. Recent theoretical advances in
324"
REFERENCES,0.8058925476603119,"non-convex optimization. In High-Dimensional Optimization and Probability: With a
325"
REFERENCES,0.8076256499133448,"View Towards Data Science, pages 79–163. Springer, 2022.
326"
REFERENCES,0.8093587521663779,"[DM77] John E Dennis, Jr and Jorge J Moré. Quasi-Newton methods, motivation and theory.
327"
REFERENCES,0.8110918544194108,"SIAM review, 19(1):46–89, 1977.
328"
REFERENCES,0.8128249566724437,"[DOG+19] Darina Dvinskikh, Aleksandr Ogaltsov, Alexander Gasnikov, Pavel Dvurechensky,
329"
REFERENCES,0.8145580589254766,"Alexander Tyurin, and Vladimir Spokoiny. Adaptive gradient descent for convex and
330"
REFERENCES,0.8162911611785095,"non-convex stochastic optimization. arXiv preprint arXiv:1911.08380, 2019.
331"
REFERENCES,0.8180242634315424,"[GBLR21] Eduard Gorbunov, Konstantin P Burlachenko, Zhize Li, and Peter Richtárik. MARINA:
332"
REFERENCES,0.8197573656845754,"Faster non-convex distributed learning with compression. In International Conference
333"
REFERENCES,0.8214904679376083,"on Machine Learning, pages 3788–3798. PMLR, 2021.
334"
REFERENCES,0.8232235701906413,"[GBY06] Michael Grant, Stephen Boyd, and Yinyu Ye. Disciplined convex programming. Global
335"
REFERENCES,0.8249566724436742,"optimization: From theory to implementation, pages 155–210, 2006.
336"
REFERENCES,0.8266897746967071,"[GCH+19] Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly
337"
REFERENCES,0.82842287694974,"Lavrukhin, Ryan Leary, Jason Li, Huyen Nguyen, Yang Zhang, and Jonathan M Cohen.
338"
REFERENCES,0.830155979202773,"Stochastic gradient methods with layer-wise adaptive moments for training of deep
339"
REFERENCES,0.8318890814558059,"networks. arXiv preprint arXiv:1905.11286, 2019.
340"
REFERENCES,0.8336221837088388,"[GLQ+19] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin,
341"
REFERENCES,0.8353552859618717,"and Peter Richtárik. SGD: General analysis and improved rates. In International
342"
REFERENCES,0.8370883882149047,"Conference on Machine Learning, pages 5200–5209. PMLR, 2019.
343"
REFERENCES,0.8388214904679376,"[GNDG19] SV Guminov, Yu E Nesterov, PE Dvurechensky, and AV Gasnikov.
Accelerated
344"
REFERENCES,0.8405545927209706,"primal-dual gradient descent with linesearch for convex, nonconvex, and nonsmooth
345"
REFERENCES,0.8422876949740035,"optimization problems. In Doklady Mathematics, volume 99, pages 125–128. Springer,
346"
REFERENCES,0.8440207972270364,"2019.
347"
REFERENCES,0.8457538994800693,"[GR15] Robert M Gower and Peter Richtárik. Randomized iterative methods for linear systems.
348"
REFERENCES,0.8474870017331022,"SIAM Journal on Matrix Analysis and Applications, 36(4):1660–1690, 2015.
349"
REFERENCES,0.8492201039861352,"[GT74] William B Gragg and Richard A Tapia.
Optimal error bounds for the Newton–
350"
REFERENCES,0.8509532062391681,"Kantorovich theorem. SIAM Journal on Numerical Analysis, 11(1):10–13, 1974.
351"
REFERENCES,0.8526863084922011,"[HHH+19] Samuel Horváth, Chen-Yu Ho, Ludovit Horvath, Atal Narayan Sahu, Marco Canini,
352"
REFERENCES,0.854419410745234,"and Peter Richtárik.
Natural compression for distributed deep learning.
CoRR,
353"
REFERENCES,0.8561525129982669,"abs/1905.10988, 2019.
354"
REFERENCES,0.8578856152512998,"[HKM+23] Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Peter Richtárik, and Sebas-
355"
REFERENCES,0.8596187175043327,"tian Stich. Stochastic distributed learning with gradient quantization and double-variance
356"
REFERENCES,0.8613518197573656,"reduction. Optimization Methods and Software, 38(1):91–106, 2023.
357"
REFERENCES,0.8630849220103987,"[HMR18] Filip Hanzely, Konstantin Mishchenko, and Peter Richtárik. SEGA: Variance reduction
358"
REFERENCES,0.8648180242634316,"via gradient sketching. Advances in Neural Information Processing Systems, 31, 2018.
359"
REFERENCES,0.8665511265164645,"[IQR21] Rustem Islamov, Xun Qian, and Peter Richtárik. Distributed second order methods
360"
REFERENCES,0.8682842287694974,"with fast rates and compressed communication. In International conference on machine
361"
REFERENCES,0.8700173310225303,"learning, pages 4617–4628. PMLR, 2021.
362"
REFERENCES,0.8717504332755632,"[JK+17] Prateek Jain, Purushottam Kar, et al. Non-convex optimization for machine learning.
363"
REFERENCES,0.8734835355285961,"Foundations and Trends® in Machine Learning, 10(3-4):142–363, 2017.
364"
REFERENCES,0.8752166377816292,"[JRSPS16] Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal
365"
REFERENCES,0.8769497400346621,"stochastic methods for nonsmooth nonconvex ﬁnite-sum optimization. Advances in
366"
REFERENCES,0.878682842287695,"neural information processing systems, 29, 2016.
367"
REFERENCES,0.8804159445407279,"[KFJ18] Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning
368"
REFERENCES,0.8821490467937608,"with compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
369"
REFERENCES,0.8838821490467937,"[KMA+21] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis,
370"
REFERENCES,0.8856152512998267,"Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel
371"
REFERENCES,0.8873483535528596,"Cummings, et al. Advances and open problems in federated learning. Foundations and
372"
REFERENCES,0.8890814558058926,"Trends® in Machine Learning, 14(1–2):1–210, 2021.
373"
REFERENCES,0.8908145580589255,"[KMY+16] Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha
374"
REFERENCES,0.8925476603119584,"Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
375"
REFERENCES,0.8942807625649913,"efﬁciency. arXiv preprint arXiv:1610.05492, 2016.
376"
REFERENCES,0.8960138648180243,"[KR20] Ahmed Khaled and Peter Richtárik. Better theory for SGD in the nonconvex world.
377"
REFERENCES,0.8977469670710572,"arXiv preprint arXiv:2002.03329, 2020.
378"
REFERENCES,0.8994800693240901,"[LBZR21] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtárik. PAGE: A simple and
379"
REFERENCES,0.901213171577123,"optimal probabilistic gradient estimator for nonconvex optimization. In International
380"
REFERENCES,0.902946273830156,"conference on machine learning, pages 6286–6295. PMLR, 2021.
381"
REFERENCES,0.9046793760831889,"[LKQR20] Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtárik. Acceleration for com-
382"
REFERENCES,0.9064124783362218,"pressed gradient descent in distributed and federated optimization. arXiv preprint
383"
REFERENCES,0.9081455805892548,"arXiv:2002.11364, 2020.
384"
REFERENCES,0.9098786828422877,"[MB11] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation
385"
REFERENCES,0.9116117850953206,"algorithms for machine learning. Advances in neural information processing systems,
386"
REFERENCES,0.9133448873483535,"24, 2011.
387"
REFERENCES,0.9150779896013865,"[MGTR19] Konstantin Mishchenko, Eduard Gorbunov, Martin Takáˇc, and Peter Richtárik.
388"
REFERENCES,0.9168110918544194,"Distributed learning with compressed gradient differences.
arXiv preprint
389"
REFERENCES,0.9185441941074524,"arXiv:1901.09269, 2019.
390"
REFERENCES,0.9202772963604853,"[Mie80] George J Miel. Majorizing sequences and error bounds for iterative methods. Mathe-
391"
REFERENCES,0.9220103986135182,"matics of Computation, 34(149):185–202, 1980.
392"
REFERENCES,0.9237435008665511,"[MMR+17] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
393"
REFERENCES,0.925476603119584,"y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In
394"
REFERENCES,0.9272097053726169,"Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics
395"
REFERENCES,0.92894280762565,"(AISTATS), 2017.
396"
REFERENCES,0.9306759098786829,"[MMSR22] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik.
397"
REFERENCES,0.9324090121317158,"ProxSkip: Yes! Local gradient steps provably lead to communication acceleration!
398"
REFERENCES,0.9341421143847487,"Finally! In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang
399"
REFERENCES,0.9358752166377816,"Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on
400"
REFERENCES,0.9376083188908145,"Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages
401"
REFERENCES,0.9393414211438474,"15750–15769. PMLR, 17–23 Jul 2022.
402"
REFERENCES,0.9410745233968805,"[MSR22] Artavazd Maranjyan, Mher Safaryan, and Peter Richtárik. GradSkip: Communication-
403"
REFERENCES,0.9428076256499134,"Accelerated Local Gradient Methods with Better Computational Complexity. arXiv
404"
REFERENCES,0.9445407279029463,"preprint arXiv:2210.16402, 2022.
405"
REFERENCES,0.9462738301559792,"[RSF21] Peter Richtárik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically
406"
REFERENCES,0.9480069324090121,"better, and practically faster error feedback. Advances in Neural Information Processing
407"
REFERENCES,0.949740034662045,"Systems, 34:4384–4396, 2021.
408"
REFERENCES,0.951473136915078,"[SHR21] Mher Safaryan, Filip Hanzely, and Peter Richtárik. Smoothness matrices beat smooth-
409"
REFERENCES,0.9532062391681109,"ness constants: Better communication compression techniques for distributed optimiza-
410"
REFERENCES,0.9549393414211439,"tion. Advances in Neural Information Processing Systems, 34:25688–25702, 2021.
411"
REFERENCES,0.9566724436741768,"[SSR22] Mher Safaryan, Egor Shulgin, and Peter Richtárik. Uncertainty principle for communi-
412"
REFERENCES,0.9584055459272097,"cation compression in distributed and federated learning and the search for an optimal
413"
REFERENCES,0.9601386481802426,"compressor. Information and Inference: A Journal of the IMA, 11(2):557–580, 2022.
414"
REFERENCES,0.9618717504332756,"[Sti19] Sebastian U Stich. Uniﬁed optimal analysis of the (stochastic) gradient method. arXiv
415"
REFERENCES,0.9636048526863085,"preprint arXiv:1907.04232, 2019.
416"
REFERENCES,0.9653379549393414,"[WSR22] Bokun Wang, Mher Safaryan, and Peter Richtárik. Theoretically better and numeri-
417"
REFERENCES,0.9670710571923743,"cally faster distributed optimization with smoothness-aware quantization techniques.
418"
REFERENCES,0.9688041594454073,"Advances in Neural Information Processing Systems, 35:9841–9852, 2022.
419"
REFERENCES,0.9705372616984402,"[Yam87] Tetsuro Yamamoto. A convergence theorem for newton-like methods in banach spaces.
420"
REFERENCES,0.9722703639514731,"Numerische Mathematik, 51:545–557, 1987.
421"
REFERENCES,0.9740034662045061,"[YCN+15] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Under-
422"
REFERENCES,0.975736568457539,"standing neural networks through deep visualization. arXiv preprint arXiv:1506.06579,
423"
REFERENCES,0.9774696707105719,"2015.
424"
REFERENCES,0.9792027729636048,"[YHL+17] Adams Wei Yu, Lei Huang, Qihang Lin, Ruslan Salakhutdinov, and Jaime Carbonell.
425"
REFERENCES,0.9809358752166378,"Block-normalized gradient method: An empirical study for training deep neural network.
426"
REFERENCES,0.9826689774696707,"arXiv preprint arXiv:1707.04822, 2017.
427"
REFERENCES,0.9844020797227037,"[ZCAW17] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neu-
428"
REFERENCES,0.9861351819757366,"ral network decisions: Prediction difference analysis. arXiv preprint arXiv:1702.04595,
429"
REFERENCES,0.9878682842287695,"2017.
430"
REFERENCES,0.9896013864818024,"[ZKV+20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
431"
REFERENCES,0.9913344887348353,"Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention
432"
REFERENCES,0.9930675909878682,"models? Advances in Neural Information Processing Systems, 33:15383–15393, 2020.
433"
REFERENCES,0.9948006932409013,"[ZTJY19] Qinghe Zheng, Xinyu Tian, Nan Jiang, and Mingqiang Yang. Layer-wise learning based
434"
REFERENCES,0.9965337954939342,"stochastic gradient descent method for the optimization of deep convolutional neural
435"
REFERENCES,0.9982668977469671,"network. Journal of Intelligent & Fuzzy Systems, 37(4):5641–5654, 2019.
436"
