Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004016064257028112,"We propose INFERNO, a method to infer object-centric representations of visual
scenes without relying on annotations. Our method learns to decompose a scene
into multiple objects, with each object having a structured representation that dis-
entangles its shape, appearance and 3D pose. To impose this structure we rely on
recent advances in neural 3D rendering. Each object representation deﬁnes a lo-
calized neural radiance ﬁeld that is used to generate 2D views of the scene through
a differentiable rendering process. Our model is subsequently trained by minimiz-
ing a reconstruction loss between inputs and corresponding rendered scenes. We
empirically show that INFERNO discovers objects in a scene without supervision.
We also validate the interpretability of the learned representations by manipulat-
ing inferred scenes and showing the corresponding effect in the rendered output.
Finally, we demonstrate the usefulness of our 3D object representations in a visual
reasoning task using the CATER dataset."
INTRODUCTION,0.008032128514056224,"1
INTRODUCTION"
INTRODUCTION,0.012048192771084338,"Inferring objects and their 3D geometry in a scene is a fundamental ability of biological visual
systems (Kahneman et al., 1992; Roelfsema et al., 1998; Spelke et al., 1993). Replicating this ability
in machine is a promising step towards visual reasoning valuable to several applications involving
object manipulation, navigation or forecasting."
INTRODUCTION,0.01606425702811245,"Recent works (Jiang et al., 2019; Locatello et al., 2020; Burgess et al., 2019) have shown that neural
networks can learn object-centric representations from low-level perceptual features. They learn to
recognize the objects in a visual scene from a singe image without relying on supervision. However,
most of those approaches only consider the 2D structure of images and ignore the underlying 3D
geometry of the visual scenes. On the other hand, Neural Radiance Fields (NeRFs) (Mildenhall
et al., 2020) have demonstrated that differentiable renderers can be combined with gradient-based
optimization to learn high-ﬁdelity 3D scene reconstructions. NeRFs have been subsequently used
to learn 3D-aware generative models, including compositional scene models (Niemeyer & Geiger,
2021)."
INTRODUCTION,0.020080321285140562,"In this work, we leverage these recent advances in object-centric representation learning (Locatello
et al., 2020) and 3D modelling through implicit functions (Mildenhall et al., 2020; Niemeyer &
Geiger, 2021) and propose INFERNO, a model which infers a structured representation of objects
and their 3D poses from a single image. Each object is represented by latent variables characterizing
its shape and appearance, together with an explicit representation of their 3D poses (translation, scale
and rotation). The object representations are then decoded using implicit functions that are localized
in the scene according to the objects poses and combined together to generate a 2D output view. Our
model does not need supervision and instead is ﬁtted through minimizing a reconstruction loss, akin
to an auto-encoder."
INTRODUCTION,0.024096385542168676,"Disentangling the object appearance and pose in a scene representation allows for the model to
manipulate a visual scene. In particular, we demonstrate that INFERNO learns interpretable object
poses, which we can modify and render to alter the pose of an object in a scene. We also validate
that our approach learns meaningful representations for object discovery and visual reasoning. More
speciﬁcally, we show that our approach obtains competitive performance on the CLEVR6 object
discovery benchmark (Johnson et al., 2017; Greff et al., 2019) as well as for the snitch localization
visual reasoning task of CATER (Girdhar & Ramanan, 2019)."
INTRODUCTION,0.028112449799196786,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0321285140562249,In summary our contributions are the following:
INTRODUCTION,0.03614457831325301,"• We propose a model able to infer and render 3D scene representations composed of multiple
objects, each of them modeled by an implicit function and explicitly localized in the scene."
INTRODUCTION,0.040160642570281124,"• We show that the representations learned by the model are interpretable and amenable to
manipulations."
INTRODUCTION,0.04417670682730924,"• We demonstrate that the inferred representations are useful for downstream tasks by show-
ing competitive performance in object discovery and reasoning tasks."
METHOD,0.04819277108433735,"2
METHOD Shape"
METHOD,0.05220883534136546,Appearance Pose Shape
METHOD,0.05622489959839357,Appearance Pose Shape
METHOD,0.060240963855421686,Appearance Pose Shape
METHOD,0.0642570281124498,Appearance
METHOD,0.06827309236947791,Camera
METHOD,0.07228915662650602,"INFERENCE
RENDERING"
METHOD,0.07630522088353414,Input View
METHOD,0.08032128514056225,"Slots
Scene 
Representation"
METHOD,0.08433734939759036,Canonical
METHOD,0.08835341365461848,Objects
METHOD,0.09236947791164658,Transformed
METHOD,0.0963855421686747,Objects
METHOD,0.10040160642570281,Background
METHOD,0.10441767068273092,Low Resolution
METHOD,0.10843373493975904,"Rendering
Reconstruction"
METHOD,0.11244979919678715,"1
2
3
4
5
6"
METHOD,0.11646586345381527,"Figure 1: Model Overview: We propose INFERNO, a model that infers and renders object-centric
3D scene representations. 1 Our model ﬁrst decomposes an input observation into multiple object
slots. 2 For each slot we infer a structured 3D representation. 3 The shape and appearance determine
canonical objects rendered through NeRFs. 4 Objects are transformed and located in the overall
scene according to their pose. 5 We combine objects and background and render a low-resolution
scene given a camera location. 6 The input is reconstructed by upscaling the low resolution scene."
METHOD,0.12048192771084337,"We propose INFERNO (Infer NeRF Objects). The goal of our method is to infer object-centric 3D
scene representations from single 2D views. Given an image x ∈RH×W ×3, we learn an inference
function fθ that maps images to scene representations s = fθ(x) = (o1, o2, ..., oK, obg, c). Scenes
are composed of K objects oi, a background object obg and a camera location c."
METHOD,0.12449799196787148,"Each object is composed of three tensors oi = (oshape
i
, oapp
i
, opose
i
). The object shape oshape
i
∈
RDshape and object appearance oapp ∈RDshape are tensors that respectively describe the 3D shape
occupancy and color of an object with an implicit function. The object location opose
i
∈R4×4 is an
afﬁne matrix that describes the object 3D pose (i.e. scale, translation and rotation) in the scene."
METHOD,0.1285140562248996,"The background object obg = (bgshape, bgapp) only models 3D shape and color, and its location is
ﬁxed, encompassing the back-of-scene cube. We also deﬁne a camera matrix c ∈R3×4, that deﬁnes
the location of the scene camera and determines a 2D projection of the 3D scene."
METHOD,0.13253012048192772,"To optimize our inference function we formulate an optimization problem in which we minimize
a reconstruction loss over a dataset, similar to an auto-encoder. We deﬁne a rendering function gγ
that takes as input a scene representation and generates a 2D view of that scene ˆx = gγ(fθ(x)). We"
METHOD,0.13654618473895583,Under review as a conference paper at ICLR 2022
METHOD,0.14056224899598393,"assume a isotropic Gaussian likelihood model with unit covariance and optimize the probability of
the data under our model, which is equivalent to minimizing the mean squared error of inputs and
reconstructions:"
METHOD,0.14457831325301204,"γ∗, θ∗= arg min
γ,θ
p(X|γ, θ) = arg min
γ,θ"
N,0.14859437751004015,"1
N X"
N,0.15261044176706828,"i≤N
(xi −gγ(fθ(xi)))2
(1)"
N,0.1566265060240964,"In the following sections we describe in more detail our inference mechanism, our rendering pipeline
and their implementation."
RENDERING PIPELINE,0.1606425702811245,"2.1
RENDERING PIPELINE"
RENDERING PIPELINE,0.1646586345381526,"We represent 3D objects as Neural Radiance Fields (NeRFs) (Mildenhall et al., 2020) with a similar
setup as that of GIRAFFE (Niemeyer & Geiger, 2021). A NeRF is a function gτ that deﬁnes a 3D
shape implicitly. It takes as input a 3D location l = (x, y, z) and a 2D viewing direction d = (ψ, φ)
and outputs an occupancy value σ and a color value a = (r, g, b). NeRFs are usually implemented
using fully connected neural networks. Additionally, the inputs are usually embedded into a higher-
dimensional space using positional encodings γ that embed locations and viewing directions into
higher dimensional spaces RPl and RPd, respectively."
RENDERING PIPELINE,0.1686746987951807,"gτ : RPl × RPd →R+ × R3
;
(γ(l), γ(d)) →(σ, a)
(2)"
RENDERING PIPELINE,0.17269076305220885,"To represent multiple 3D shapes with the same NeRF function, we can augment it with latent vari-
ables that determine which shape is being modeled (Schwarz et al., 2020). NeRFs are usually
augmented with two random variables: one random variable µ ∈RDshape deﬁnes the shape of the
entity being modeled, while υ ∈RDapp models its appearance. In practice, this specialization is
enforced by making the occupancy output a function of only the shape latent, while the color output
is conditioned on the appearance latent."
RENDERING PIPELINE,0.17670682730923695,"g′
τ : RPl × RPd × RDshape × RDapp →R+ × R3
;
(γ(l), γ(d), µ, υ) →(σ, a)
(3)"
RENDERING PIPELINE,0.18072289156626506,"In INFERNO, we share a single parametrization of a NeRF function across all objects. Each ob-
ject speciﬁc shape and appearance is deﬁned by the shape and appearance latent variables, which
correspond to the object attributes oshape, oapp. The background is deﬁned as another NeRF with
separate parameters. The background NeRF also has shape and appearance latent variables to model
different backgrounds."
RENDERING PIPELINE,0.18473895582329317,"The pose of an object oi in the scene is determined by the afﬁne transformation matrix opose
i
. We
denote the coordinate system of the NeRF function of an object as the object space, and the coordi-
nate system of the scene (and the background NeRF) as scene space. Given an object pose, we can
convert points from the scene space to the object space by applying the opose transformation matrix
on those points, and we can transform points from object space to scene space by computing the
inverse of the object pose matrix."
RENDERING PIPELINE,0.18875502008032127,"To render a scene, we cast rays from each pixel in the 2D plane deﬁned by a given camera to the
3D scene. We evaluate NeRFs at different points along a given ray, and integrate their occupancy
and color outputs to determine pixel values. Rays might traverse multiple object NeRFs in addition
to the background NeRF. To determine the occupancy and color of points described by multiple
NeRFs, we ﬁrst query each NeRF at those particular points. To query the object NeRFs, we ﬁrst
need to transform the points from scene space to the particular object space. Then, we compose the
results of each NeRF with a pooling function C, which in our case is a weighted average:"
RENDERING PIPELINE,0.1927710843373494,"C(l, d) = (σ = N
X"
RENDERING PIPELINE,0.19678714859437751,"i=1
σi, 1 σ N
X"
RENDERING PIPELINE,0.20080321285140562,"i=1
σiai)
(4)"
RENDERING PIPELINE,0.20481927710843373,"Since rendering with NeRFs as originally proposed is computationally expensive, we render output
views at a ﬁxed low resolution. Low resolution scenes are then upscaled to the desired output"
RENDERING PIPELINE,0.20883534136546184,Under review as a conference paper at ICLR 2022
RENDERING PIPELINE,0.21285140562248997,"resolution using a convolutional neural network, keeping the entire rendering pipeline differentiable.
Additionally, low resolution scenes are rendered with additional channels, allowing for more detailed
upscalings beyond those possible when just rendering low resolution RGB outputs. For more details
about NeRF rendering and compositional NeRF models refer to (Mildenhall et al., 2020; Niemeyer
& Geiger, 2021)."
INFERENCE,0.21686746987951808,"2.2
INFERENCE"
INFERENCE,0.22088353413654618,"Given the rendering pipeline, the goal of our method is to infer representations that reconstruct a
given scene. Our inference mechanism computes image features through a neural network encoder
and then extracts K object and a background slots. These slots are then mapped to our structured
scene representation through learned neural networks."
INFERENCE,0.2248995983935743,"To extract image features for each object and background slots, we use Slot Attention (Locatello
et al., 2020). Slot Attention is a mechanism that maps a set of K entities, called slots, to image
features without annotations. It extracts image features I ∈RH×W ×D from a given input using
a resolution-preserving convolutional encoder. These features are then attended to by a set of K
randomly sampled slots πj ∼N(µ, σ) of dimension D, where µ and σ are learnable parameters. We
denote by π the matrix concatenating all the sampled slots. Slots attend spatial chunks of the input
features I through soft-attention u = TQT , where T = k(I) and Q = q(π) are the embeddings of
the inputs and slots respectively. The attention weights are normalized through a softmax operating
on the slots axis, which makes slots compete among themselves and discourages multiple slots
from attending the same input region w = softmax(u). The weighted average of I according
to the attention weights is then computed and fed to a GRU network, to update each slot value:
πj = GRU(wj ∗I, πj) ∀j ∈1, K. Multiple rounds of soft attention are performed to iteratively
reﬁne the slots. For more details about Slot-Attention, please refer to (Locatello et al., 2020)."
INFERENCE,0.2289156626506024,"Object slots are unstructured tensors that result from aggregating image features. We map these
slots to our structured scene representation through small fully-connected networks that operate on
individual objects. More concretely, we map object slots to their 3D pose in the scene through a
2-layer MLP. To infer the object shape and appearance tensors we also use 2-layer MLPs, but we
make them conditional on the predicted object pose through conditional normalization (Dumoulin
et al., 2018)."
RELATED WORK,0.23293172690763053,"3
RELATED WORK"
RELATED WORK,0.23694779116465864,"3.1
3D SHAPE REPRESENTATIONS"
RELATED WORK,0.24096385542168675,"There are different ways to represent 3D geometry such as voxels or meshes (Rematas & Ferrari,
2020; Gkioxari et al., 2019). For example, the GAN models of Nguyen-Phuoc et al. (2019; 2020)
successfully use voxel-based representations to render images. Voxel-based methods have trouble
scaling up to high resolutions as the size of a voxel representation scales cubically with the resolu-
tion."
RELATED WORK,0.24497991967871485,"Recently, the use of functions that implicitly model 3D volumes has gained popularity (Park et al.,
2019; Mescheder et al., 2019; Sitzmann et al., 2019; 2020b;a; Kosiorek et al., 2021; Pumarola et al.,
2021; Yu et al., 2021a). Implicit representations have better scaling properties, as usually the output
resolution does not directly affect the dimensionality of the learned function. NeRFs (Mildenhall
et al., 2020) generate scenes by learning a function that outputs the occupancy and color of points in
a scene when viewed from a particular direction. By casting rays through a plane and aggregating
the output values NeRFs can generate 2D views of an implicitly modeled 3D scenes. NeRFs have
obtained superior reconstructions compared to other implict methods, and our model uses NeRFs to
represent multiple objects and the background of a scene."
RELATED WORK,0.24899598393574296,"Most methods using NeRFs represent scenes monolithically as a single entity. GIRAFFE (Niemeyer
& Geiger, 2021) is a GAN-based method that represents multiple objects in a scene using NeRFs
as part of their generator. Their factored representations are amenable to object manipulations. Our
model uses a rendering pipeline inspired by GIRAFFE. However, we focus on recovering scene
representations from existing images, while GIRAFFE does not have an inference mechanism. Ob-
jSURF (Stelzner et al., 2021) and UORF (Yu et al., 2021b) infer scene representations composed"
RELATED WORK,0.25301204819277107,Under review as a conference paper at ICLR 2022
RELATED WORK,0.2570281124497992,"of multiple objects, each object represented with a differently instantiated NeRF. Different from our
work, they focus on novel view generation. These methods do not explicitly infer the pose of the
different objects in the scene. Both methods use ground-truth camera locations and multiple scene
views during training, and ObjSURF additionally requires depth annotations."
OBJECT-CENTRIC SCENE MODELS,0.26104417670682734,"3.2
OBJECT-CENTRIC SCENE MODELS"
OBJECT-CENTRIC SCENE MODELS,0.26506024096385544,"Segmenting objects in a scene is a landmark computer vision task with an extensive literature. Re-
cently, there is a line of work on object-centric generative models of scenes (Kosiorek et al., 2021;
Burgess et al., 2019; Locatello et al., 2020; Lin et al., 2020; Greff et al., 2019). These models learn
to generate scenes as a composition of multiple objects and a background. When equipped with an
inference mechanism, these models learn to segment objects in a scene without annotations, driven
by their compositional generative process. MONet (Burgess et al., 2019) implements a multi-object
VAE that segments object sequentially by infering latents corresponding over parts of the scene not
yet attended to iteratively. IODINE (Greff et al., 2019) uses a similar multi-object VAE and per-
forms multiple rounds of inference to settle on a scene decomposition. Slot-Attention (Locatello
et al., 2020) maps a set of entities, called slots, to image features through multiple rounds of soft
attention. The slots compete among themselves to attend to features, making each slot attend to a
region of the input image. When driven with alpha-compositing decoder, slot attention learns to
segment objects in a scene. Our model uses a variant of Slot Attention to decide on which part of a
2D view should each object in our scene attend to, but we infer 3D-aware representations for each
object. Object-centric scene models have also been implemented as world models, with the goal of
simulating dynamics (Lin et al., 2020). By decomposing the scene into objects, these methods can
simulate dynamics at an object level, which are usually simpler and shared among objects. Con-
trary to most previous approaches which segment 2D shapes, our method infers object-centric scene
representations in 3D space."
EXPERIMENTS,0.26907630522088355,"4
EXPERIMENTS"
EXPERIMENTS,0.27309236947791166,"In this section we showcase the capabilities of INFERNO with three main experiments. First, we
demonstrate the interpretability of the scene representations through manipulating scenes and ver-
ifying the corresponding effects in the rendered outputs. Then we show that it learns to identify
and segment the objects in a scene without supervision. Finally, we highlight the usefulness of such
representations for downstream tasks by applying our model on the CATER snitch localization task."
TRAINING SETUP,0.27710843373493976,"4.1
TRAINING SETUP"
TRAINING SETUP,0.28112449799196787,"We use the same training setup for all experiments unless otherwise mentioned. We train our mod-
els for 400K iterations using the Adam optimizer Kingma & Ba (2014) with a learning rate of
1 × 10−4. We use a batch size of 128 and we use up to 16 nVidia V100 GPUs. We use learning
rate warmup (Goyal et al., 2017), which was found to be helpful to avoid optimization issues with
Slot Attention. We use a weight decay rate λ = 1 × 10−6. We use three iterations of slot attention
during training and evaluation. We set up the number of objects in a scene as the maximum possible
number of objects in a dataset, i.e. ﬁve objects for CLEVR2345, 6 for CLEVR6 and 10 for CATER.
Please refer to Appendix B for more details on the experimental setting."
SCENE INFERENCE,0.285140562248996,"4.2
SCENE INFERENCE"
SCENE INFERENCE,0.2891566265060241,"In this section we demonstrate the properties of our scene representation. Our model infers 3D
object-centric scene representations from single 2D views. These representations disentangle the
3D appearance and pose of objects, which allows for semantic manipulations of the scene not pos-
sible otherwise. These manipulations can be validated by rendering the modiﬁed scene represen-
tations. Additionally, we verify that decomposing the scene into multiple objects leads to better
reconstructions."
SCENE INFERENCE,0.2931726907630522,"First, we verify the quality of INFERNO’s generations by comparing them two baselines: i) a
version of our model that does not consider multiple objects, and ii) the GAN method of GI-
RAFFE (Niemeyer & Geiger, 2021). We perform this comparison on the CLEVR-2345 dataset"
SCENE INFERENCE,0.2971887550200803,Under review as a conference paper at ICLR 2022
SCENE INFERENCE,0.30120481927710846,"Input
Recon.
Addition
Removal
Scale
Forward
Right"
SCENE INFERENCE,0.30522088353413657,"Figure 2: Manipulations on CLEVR: we show some examples of the manipulations we perform
to CLEVR2345 images, including object removal and addition, changing the scale of an object, and
object translation. Our model can perform these transformations because it disentangles object pose
and appearance."
SCENE INFERENCE,0.3092369477911647,"Table 1: Reconstruction error on CLEVR2345 We consider an autoencoder baseline that uses
a single NeRF object covering the whole scene (NeRF-AE), and compare it to our model on the
test set of CLEVR-2345. NeRF-AE struggles to reconstruct multiple objects accurately. In con-
trast, INFERNO obtains much more precise reconstructions under all metrics and allows for object
identity/pose manipulations."
SCENE INFERENCE,0.3132530120481928,"Model
MSE (↓)
PSNR (↑)
SSIM (↑)
LPIPS (↓)"
SCENE INFERENCE,0.3172690763052209,"NeRF-AE
5.14 × 10−4
44.89
59.24 %
168.9 × 10−3"
SCENE INFERENCE,0.321285140562249,"Ours
1.22 × 10−4
52.07
72.4 %
18.93 × 10−3"
SCENE INFERENCE,0.3253012048192771,"introduced by GIRAFFE, which contains CLEVR images with 2 to 5 objects. For reconstruction,
we compare models using reconstruction metrics including mean-squared error, PSNR and SSIM.
We rely on the population metric Frechet Inception Distance (FID) to evaluate the generation quality."
SCENE INFERENCE,0.3293172690763052,"In INFERNO, we generate novel scenes by inferring representations for ground-truth images and
then manipulating them. To compare to GIRAFFE, we manipulate scenes by adding additional
objects and swapping object shapes and appearances across scenes. Manipulations are described
in more detail in the Appendix. We highlight that GIRAFFE is an unconditional model, while
INFERNO generates novel scenes conditioned on existing ones."
SCENE INFERENCE,0.3333333333333333,"We also investigate different interpretable manipulations of scene representations and visualize the
effects in the corresponding output renderings. We also conduct this experiment on the CLEVR-
2345 dataset. We validate that our model is able to render out-of-distribution scenes not correspond-
ing to training examples, such as scenes having 1 or 6 objects, and verify that the pose manipulations
have semantically coherent effects."
SCENE INFERENCE,0.3373493975903614,Under review as a conference paper at ICLR 2022
SCENE INFERENCE,0.3413654618473896,"Table 2: FID on CLEVR2345 We consider our model as a NeRF scene generator and compare
it to the state-of-the-art. When reconstructing ground-truth images, our model obtains better FID
than GIRAFFE. We then consider object manipulations to generate novel scenes from existing ones.
While adding new objects to a scene slightly increases our FID score, when exchanging object
identities across scenes we set a new state-of-the-art for image generation on CLEVR2345."
SCENE INFERENCE,0.3453815261044177,"Model
FID (↓)"
SCENE INFERENCE,0.3493975903614458,"GIRAFFE
37.7
Ours - Reconstruction
23.5
Ours - Add Object
27.2
Ours - Swap Object
23.7"
SCENE INFERENCE,0.3534136546184739,"Table 1 shows the reconstruction metrics obtained by our model and baseline on the CLEVR2345
dataset. Note that GIRAFFE is a GAN-method that does not have an inference mechanism, and
therefore it cannot reconstruct scenes. We observe that the NeRF-AE baseline obtains higher re-
construction error than our regular model, as our object-centric method can make better use of its
capacity. In Table 2 we compare INFERNO with GIRAFFE using the FID metric. Our model re-
constructions have better FID than the generations of the GIRAFFE. Additionally, our model can
perform inference and manipulations on existing scenes. We use that capability to generate novel
scenes by manipulating existing ones. Our model is able to generate novel scenes with additional
objects or with altered object shapes and appearances, with better FID than GIRAFFE."
SCENE INFERENCE,0.357429718875502,"In Figure 2 we show some examples of the scene manipulations possible with our model. Given
a scene representation, we can remove or add objects, rearrange object poses, translate the objects
to new locations or change the object scales. While some of these manipulations can be performed
with regular object-centric models, modiﬁcations to the scale and location of the objects are hard to
implement without explicitly modeling 3D object pose."
OBJECT DISCOVERY,0.3614457831325301,"4.3
OBJECT DISCOVERY"
OBJECT DISCOVERY,0.3654618473895582,"Table 3: Object Discovery on CLEVR6 INFERNO, despite inferring more complex 3D object seg-
mentations without annotations, is competitive with the current state-of-the-art 2D object discovery
methods on CLEVR6."
OBJECT DISCOVERY,0.36947791164658633,"Model
ARI % (↑)"
OBJECT DISCOVERY,0.37349397590361444,"Slot-Attention
98.8 ± 0.3
IODINE
98.8 ± 0.0
MONet
96.2 ± 0.6
Slot MLP
60.4 ± 6.6"
OBJECT DISCOVERY,0.37751004016064255,"Ours
96.7 ± 0.2"
OBJECT DISCOVERY,0.3815261044176707,"Unsupervised object discovery consists in learning to segment the objects in a scene without using
annotations. We test our model on CLEVR6 benchmark, a variant of the CLEVR dataset with
scenes of up to 6 objects and annotated with 2D object masks. We choose this dataset to compare
to previous work in unsupervised object discovery. Note that this setup evaluates 2D segmentation
masks, although our model naturally provides 3D segmentations. We evaluate the quality of the
segmentations using the Adjusted Random Index (ARI) metric (Rand, 1971), which is a measure
of clustering similarity. In line with previous work, we compute only the foreground ARI, which
does not take into account the background segmentation mask. In particular, we consider object as a
different clusters and compare the cluster assignment of each foreground pixel in the original image
to its prediction. To determine which pixels correspond to each object in our model we make use of
the input segmentation masks predicted by our inference mechanism."
OBJECT DISCOVERY,0.3855421686746988,"Table 3 reports the ARI metric of our model and different baselines in this task. We observe that IN-
FERNO is competitive with state-of-the-art methods, surpassing MONet and having slightly lower"
OBJECT DISCOVERY,0.3895582329317269,Under review as a conference paper at ICLR 2022
OBJECT DISCOVERY,0.39357429718875503,"Input
Background
Object 1
Object 2
Object 3
Object 4
Object 5
Object 6"
OBJECT DISCOVERY,0.39759036144578314,"Figure 3: Object Discovery on CLEVR6: INFERNO identiﬁes the different objects in a scene
without supervision. For each input image, we show which regions of the input are attended by each
object as well as the background. We include an example of a failed segmentation in the last row,
where one object slot (4) is trying to represent multiple objects at the same time."
OBJECT DISCOVERY,0.40160642570281124,"ARI than Slot-Attention and IODINE. However, our model learns to segment objects in 3D, while
the other baselines extract 2D segmentation masks. Figure 3 shows some examples of the discovered
object masks. We can see that the model learns to segment different objects and properly discards
object slots when the number of objects in the scene is lower than needed. We also include an ex-
ample that our model fails to segment properly - multiple objects are segmented by the same object
slot, and a single object is represented in multiple parts by different slots."
SNITCH LOCALIZATION,0.40562248995983935,"4.4
SNITCH LOCALIZATION"
SNITCH LOCALIZATION,0.40963855421686746,"The goal of this experiment is to show that the representation learned by our model is useful for
the snitch localization task from the CATER dataset (Girdhar & Ramanan, 2019). We focus on the
CATER task involving videos with a static camera. The objective is to predict the ﬁnal position of
an object (the snitch) in a video. The scenes show multiple objects that move over time, one being
the snitch object. The snitch can be occluded and moved around simultaneously by other objects,
requiring object tracking and reasoning about dynamics to solve the task."
SNITCH LOCALIZATION,0.41365461847389556,"We follow the experimental setup of Ding et al. (2020). First, we train INFERNO to reconstruct
images from the CATER dataset. Once INFERNO is trained, we discard the rendering pipeline of
our model, and instead feed the scene representations to a 12-layer transformer model to predict the
ﬁnal snitch position. Each object in our representation is given as an input element to the transformer.
We add a learned positional encoding to the object representations based on their frame index.
Objects in the same frame have the same positional encoding. The transformer last output is given
to a MLP head that predicts the logits for the 36 possible output positions."
SNITCH LOCALIZATION,0.41767068273092367,"We minimize the sum of the cross-entropy and a L1 loss between the predicted and the true snitch
ﬁnal position. Following (Ding et al., 2020), we also consider the use of auxiliary SSL loss after
pretraining. The SSL loss randomly masks one object per-frame and tries to predict its representation
at the corresponding output. The model minimizes the L2 distance between the predicted object"
SNITCH LOCALIZATION,0.42168674698795183,Under review as a conference paper at ICLR 2022
SNITCH LOCALIZATION,0.42570281124497994,"Table 4: Snitch Localization on CATER. We report Top-1 and Top-5 accuracies for the snitch
localization task. Our model outperforms the R3D LSTM and R3D NL LSTM models that learn
unstructured representation. It indicates that the structured representation learned by INFERNO is
useful for this task. INFERNO pretraining is also critical, showing that the pretraining and not the
encoder architecture is a key component. Overall, INFERNO achieves performances close to the
state-of-art approaches."
SNITCH LOCALIZATION,0.42971887550200805,"Model
Top-1
Top-5"
SNITCH LOCALIZATION,0.43373493975903615,"R3D LSTM
60.2
81.8
R3D + NL LSTM
46.2
69.9
OPNet (extra anns.)
74.8
-
Hopper
73.2
93.8
Slot-Attention
59.1
88.0
Aloe (w/out SSL loss)
60.1
-
Aloe
74.0
94.0"
SNITCH LOCALIZATION,0.43775100401606426,"Ours (w/out pretraining)
2.91
12.9
Ours (w/out SSL loss)
69.17
87.68
Ours
71.7
88.9"
SNITCH LOCALIZATION,0.44176706827309237,"representation and the observed one. The SSL loss is only backpropagated through the transformer
and not the inference network."
SNITCH LOCALIZATION,0.4457831325301205,"During training, we randomly samples 40 frames from a video and predict the snitch localization
from this one crop. At test time, we randomly sample 10 temporal crops of 40 frames each and
average the prediction over the 10 crops as our ﬁnal prediction. Refer to Appendix B for more
details about the experiment setup."
SNITCH LOCALIZATION,0.4497991967871486,"Table 4 reports CATER Top-1 and Top-5 accuracies for different methods. We ﬁrst compare our
model that is pretrained to reconstruct image on CATER with a randomly initialized encoder. Using
a randomly initialized encoder does not perform well. It indicates that INFERNO pretraining, and
not the encoder architecture, is key to learn useful representation for the task. We also observe that
the additional SSL loss provides regularization and slightly improves the performance of our model."
SNITCH LOCALIZATION,0.4538152610441767,"We next compare our approach with Aloe (Ding et al., 2020), an object-centric baseline, R3D and
R3D NL, 3D convolutional models proposed by (Girdhar & Ramanan, 2019) and architectures us-
ing strong inductive biases toward object tracking such as OPNet (Shamsian et al., 2020) or Hop-
per (Zhou et al., 2021). Our model outperforms the R3D and R3D NL models that rely on unstruc-
tured representation. This result suggests that the object-centric representation learned by INFERNO
is useful for the visual reasoning task. Our model also outperforms Aloe, an object-centric method
using 2D object representation, when both methods do not use additional SSL loss. However Aloe
beneﬁts more from the use of an additional SSL loss. Overall, INFERNO achieves performances
close to the state-of-art approaches."
SNITCH LOCALIZATION,0.4578313253012048,"We ﬁnally evaluate the performance of a slot-attention baseline (Locatello et al., 2020) in Table 4.
The slot-attention baseline ﬁrst pretrains a slot-attention encoder, with a similar architecture than
our model, by reconstructing CATER frames using a mask decoder. It then ﬁne-tunes the encoder
using the same procedure than our model to solve the snitch localization task. We observe that our
model signiﬁcantly outperforms the slot-attention model which focus on the 2D geometry of the
scene. This result supports the advantage of 3D-aware representation for solving the CATER task."
CONCLUSIONS,0.46184738955823296,"5
CONCLUSIONS"
CONCLUSIONS,0.46586345381526106,"We propose INFERNO, a model for inferring object-centric 3D scene representations. Our model
is able to discover objects in a scene without annotations, and the inferred scene representations are
interpretable and amenable to manipulations. Further, the scene representation is useful for visual
reasoning downstream tasks such as the snitch localization task in CATER."
CONCLUSIONS,0.46987951807228917,Under review as a conference paper at ICLR 2022
REFERENCES,0.4738955823293173,REFERENCES
REFERENCES,0.4779116465863454,"Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt
Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and represen-
tation. arXiv preprint arXiv:1901.11390, 2019."
REFERENCES,0.4819277108433735,"David Ding, Felix Hill, Adam Santoro, and Matt Botvinick.
Object-based attention for spatio-
temporal reasoning: Outperforming neuro-symbolic models with ﬂexible distributed architec-
tures. arXiv preprint arXiv:2012.08508, 2020."
REFERENCES,0.4859437751004016,"Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron Courville,
and Yoshua Bengio. Feature-wise transformations. Distill, 3(7):e11, 2018."
REFERENCES,0.4899598393574297,"Rohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and tem-
poral reasoning. arXiv preprint arXiv:1910.04744, 2019."
REFERENCES,0.4939759036144578,"Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 9785–9795, 2019."
REFERENCES,0.4979919678714859,"Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.5020080321285141,"Klaus Greff, Rapha¨el Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel
Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation
learning with iterative variational inference. In International Conference on Machine Learning,
pp. 2424–2433. PMLR, 2019."
REFERENCES,0.5060240963855421,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, 30, 2017."
REFERENCES,0.5100401606425703,"Jindong Jiang, Sepehr Janghorbani, Gerard De Melo, and Sungjin Ahn. Scalor: Generative world
models with scalable object representations. arXiv preprint arXiv:1910.02384, 2019."
REFERENCES,0.5140562248995983,"Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 2901–2910, 2017."
REFERENCES,0.5180722891566265,"Daniel Kahneman, Anne Treisman, and Brian J Gibbs. The reviewing of object ﬁles: Object-speciﬁc
integration of information. Cognitive psychology, 24(2):175–219, 1992."
REFERENCES,0.5220883534136547,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.5261044176706827,"Adam R Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Soˇna Mokr´a,
and Danilo J Rezende. Nerf-vae: A geometry aware 3d scene generative model. arXiv preprint
arXiv:2104.00587, 2021."
REFERENCES,0.5301204819277109,"Zhixuan Lin, Yi-Fu Wu, Skand Peri, Bofeng Fu, Jindong Jiang, and Sungjin Ahn. Improving gen-
erative imagination in object-centric world models.
In International Conference on Machine
Learning, pp. 6140–6149. PMLR, 2020."
REFERENCES,0.5341365461847389,"Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,
Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-
tion. arXiv preprint arXiv:2006.15055, 2020."
REFERENCES,0.5381526104417671,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Oc-
cupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 4460–4470, 2019."
REFERENCES,0.5421686746987951,"Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In European
conference on computer vision, pp. 405–421. Springer, 2020."
REFERENCES,0.5461847389558233,Under review as a conference paper at ICLR 2022
REFERENCES,0.5502008032128514,"Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang.
Holo-
gan: Unsupervised learning of 3d representations from natural images. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 7588–7597, 2019."
REFERENCES,0.5542168674698795,"Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, and Niloy Mitra. Block-
gan: Learning 3d object-aware scene representations from unlabelled images. arXiv preprint
arXiv:2002.08988, 2020."
REFERENCES,0.5582329317269076,"Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional genera-
tive neural feature ﬁelds. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 11453–11464, 2021."
REFERENCES,0.5622489959839357,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
Deepsdf: Learning continuous signed distance functions for shape representation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 165–174,
2019."
REFERENCES,0.5662650602409639,"Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural
radiance ﬁelds for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10318–10327, 2021."
REFERENCES,0.570281124497992,"William M Rand. Objective criteria for the evaluation of clustering methods. Journal of the Ameri-
can Statistical association, 66(336):846–850, 1971."
REFERENCES,0.5742971887550201,"Konstantinos Rematas and Vittorio Ferrari. Neural voxel renderer: Learning an accurate and con-
trollable rendering tool. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 5417–5427, 2020."
REFERENCES,0.5783132530120482,"Pieter R Roelfsema, Victor AF Lamme, and Henk Spekreijse. Object-based attention in the primary
visual cortex of the macaque monkey. Nature, 395(6700):376–381, 1998."
REFERENCES,0.5823293172690763,"Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance ﬁelds
for 3d-aware image synthesis. arXiv preprint arXiv:2007.02442, 2020."
REFERENCES,0.5863453815261044,"Aviv Shamsian, Ofri Kleinfeld, Amir Globerson, and Gal Chechik. Learning object permanence
from video. In European Conference on Computer Vision, pp. 35–50. Springer, 2020."
REFERENCES,0.5903614457831325,"Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wetzstein. Scene representation networks: Con-
tinuous 3d-structure-aware neural scene representations. arXiv preprint arXiv:1906.01618, 2019."
REFERENCES,0.5943775100401606,"Vincent Sitzmann, Eric R Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. Metasdf:
Meta-learning signed distance functions. arXiv preprint arXiv:2006.09662, 2020a."
REFERENCES,0.5983935742971888,"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33, 2020b."
REFERENCES,0.6024096385542169,"Elizabeth S Spelke, Karen Breinlinger, Kristen Jacobson, and Ann Phillips. Gestalt relations and
object perception: A developmental study. Perception, 22(12):1483–1501, 1993."
REFERENCES,0.606425702811245,"Karl Stelzner, Kristian Kersting, and Adam R Kosiorek. Decomposing 3d scenes into objects via
unsupervised volume segmentation. arXiv preprint arXiv:2104.01148, 2021."
REFERENCES,0.6104417670682731,"Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:
from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–
612, 2004."
REFERENCES,0.6144578313253012,"Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance ﬁelds from
one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 4578–4587, 2021a."
REFERENCES,0.6184738955823293,"Hong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsupervised discovery of object radiance ﬁelds.
arXiv preprint arXiv:2107.07905, 2021b."
REFERENCES,0.6224899598393574,Under review as a conference paper at ICLR 2022
REFERENCES,0.6265060240963856,"Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 586–595, 2018."
REFERENCES,0.6305220883534136,"Honglu Zhou, Asim Kadav, Farley Lai, Alexandru Niculescu-Mizil, Martin Renqiang Min, Mub-
basir Kapadia, and Hans Peter Graf. Hopper: Multi-hop transformer for spatiotemporal reasoning.
arXiv preprint arXiv:2103.10574, 2021."
REFERENCES,0.6345381526104418,"A
ADDITIONAL MODEL DETAILS"
REFERENCES,0.6385542168674698,"Our model is composed of ﬁve main modules: encoder, slot attention, slot to object mapping, NeRF
decoder, neural network upscaler. In this section we provide additional details about each of these
modules as well as describe their architecture."
REFERENCES,0.642570281124498,Table 5: Encoder Neural Network
REFERENCES,0.6465863453815262,"Layer Type
Size
Normalization
Activation
Other details"
REFERENCES,0.6506024096385542,"Conv 5 × 5
64
-
ReLU
Stride 1 Pad. 1
Conv 5 × 5
64
-
ReLU
Stride 1 Pad. 1
Conv 5 × 5
64
-
ReLU
Stride 1 Pad. 1
Conv 5 × 5
64
-
ReLU
Stride 1 Pad. 1"
REFERENCES,0.6546184738955824,"Encoder
The goal of the encoder is to extract image features. We use an encoder with no down-
sampling, as it is typically used with Slot Attention. Details about the encoder architecture are
described in Table 5."
REFERENCES,0.6586345381526104,Table 6: Slot Attention Neural Network
REFERENCES,0.6626506024096386,"Name
Size
Description"
REFERENCES,0.6666666666666666,"Positional emb.
64
Additive embedding, same size as CNN input features
Flatten
-
Flattens the spatial dimensions of CNN features
QKV MLP
128
Linear layers that map slots and input features to the same dimension
LayerNorm
128
Normalizes the slots/inputs
MLP + GRU
128
The output of soft-attention goes through a linear layer + GRU"
REFERENCES,0.6706827309236948,"Slot Attention
We employ Slot Attention to map image features to object slots. Slots are sampled
randomly from a Gaussian distribution with learned parameters. We use different distributions for
the background slot and the object slots. During training we employ three iterations of slot attention
to reﬁne the image features to slot assignments."
REFERENCES,0.6746987951807228,Table 7: Slot to Object MLP details
REFERENCES,0.678714859437751,"MLP Name
Size
Act and Norm.
Description"
REFERENCES,0.6827309236947792,"Obj Pose
7
ReLU, LayerNorm
Slot to translation, scale and rotation
Obj Shape/App.
128
ReLU, CondLayerNorm
Slot to shape and appearance
BG Shape/App.
128
ReLU, LayerNorm
Background slot to its shape/app, ﬁxed pose."
REFERENCES,0.6867469879518072,"Slot to Object Net
The background and object slots are mapped to scene parameters using a
series of MLP. For each object slot, we ﬁrst map the object to its pose parameters. We use a 2-layer
MLP with LayerNorm to map a slot to its pose. The size of the hidden dimension is the same than"
REFERENCES,0.6907630522088354,Under review as a conference paper at ICLR 2022
REFERENCES,0.6947791164658634,"the output dimension size. We parametrize object pose as a 7-dimensional tensor. We use three
dimensions for the object location along each axis, three dimensions for the scale of the object and
a single dimension to express a rotation of the object along the X axis. These parameters are then
mapped to their corresponding 4 × 4 afﬁne transformation matrix for each object. Note that in
practice we are not modeling rotations in the experimental section."
REFERENCES,0.6987951807228916,"Once we have inferred the object poses, we infer object shapes and appearances. These are inferred
individually for each object using a common 2-layer MLP. The MLPs are conditional on the object
pose using Conditional Layer Normalization, that makes the learned parameters of LayerNorm be
a function of a condition. Speciﬁcally, we map the 7-dimensional pose tensor to LayerNorm pa-
rameters with a single linear layer with no activation or normalization. Shapes and appearances are
deﬁned by the output of the MLPs, which produce two 128-dimensional tensors."
REFERENCES,0.7028112449799196,"For the background object we only infer shape and appearance, and deﬁne its pose to be that of the
scene cube. The shape and appearance of the background are inferred through another 2-layer MLP
with ReLU and LayerNorm."
REFERENCES,0.7068273092369478,"Note that our scene representation also admits a camera pose. In our experiments we ﬁx the camera
location to look at the scene from a standard location (centered and 33 degrees above the Z plane).
Other concurrent approaches (Yu et al., 2021b; Stelzner et al., 2021) use ground-truth camera loca-
tions to generate novel views, while we focus on recovering scene representations without the use
of ground-truth annotations."
REFERENCES,0.7108433734939759,Table 8: Details about the NeRF MLPs used
REFERENCES,0.714859437751004,"MLP Name
Layers
Size
Description"
REFERENCES,0.7188755020080321,"Obj MLP
8
64
ReLU activation, no norm. Skip connection with layer 4.
BG MLP
4
16
ReLU activation , no norm."
REFERENCES,0.7228915662650602,"NeRF MLPs
With the object shape and appearance tensors we can render them following
GRAF (Schwarz et al., 2020). We use one NeRF for the objects and one NeRF MLP for the back-
ground. The details about each NeRF architecture can be found in Table 8. Note that, to render ob-
jects according to their pose, we query their NeRF MLP in a canonical object space by transforming
input coordinates in scene space to object space using the object pose. To reduce the computational
complexity of rendering with many NeRFs, we render scenes at a ﬁxed resolution of 16x16. Instead
of rendering RGB pixels, we render feature images with 128 channels. The output of the rendering
is then upscaled and mapped to RGB views with a neural network upscaler."
REFERENCES,0.7269076305220884,Table 9: Neural Upscaler architecture
REFERENCES,0.7309236947791165,"Layer
Size
Activation
Normalization
Other"
REFERENCES,0.7349397590361446,"Conv 3 × 3
64
ReLU
Instance
Stride=1 Pad=1
Upsample
-
-
-
Nearest Neighbors
Conv 3 × 3
64
ReLU
Instance
Stride=1 Pad=1
Upsample
-
-
-
Nearest Neighbors
(Only 128px) Conv 3 × 3
64
ReLU
Instance
Stride=1 Pad=1
(Only 128px) Upsample
-
-
-
Nearest Neighbors
Conv 3 × 3
3
-
-
Stride=1 Pad=1"
REFERENCES,0.7389558232931727,"Neural Upscaler
The neural upscaler takes the low resolution output of the NeRF MLPs and
upscales it to the full output resolution. Additionally, it maps the rendered image to RGB space.
This module is implemented using a convolutional neural network. We always render the NeRF
output at 16px. Consequently, we add additional layers to the neural upscaler depending on the
desired output resolution. For most experiments we use a resolution of 64px, while for the Scene
Inference experiments on CLEVR2345 we use a resolution of 128px. The architecture of the neural
upscaler can be found in Table 9."
REFERENCES,0.7429718875502008,Under review as a conference paper at ICLR 2022
REFERENCES,0.7469879518072289,"B
EXPERIMENT DETAILS"
REFERENCES,0.751004016064257,"B.1
SCENE MANIPULATION"
REFERENCES,0.7550200803212851,"Dataset
For generating manipulated scenes we consider the CLEVR2345 dataset (Niemeyer &
Geiger, 2021). Images in the CLEVR2345 dataset contain from 2 to 5 objects. We use the original
train and test splits. Images are resized to 128x128 pixels and RGB values are normalized in the [0,
1] range."
REFERENCES,0.7590361445783133,"Training
We use a batch size of 128 and train our model for 400k iterations. We use Adam with a
learning rate of 1 × 10−4 and weight decay 1 × 10−6. We use 5 objects and rely on the model to not
use additional slots if the scene shows less than 5 objects. We use the neural upscaler with additional
layers to upscale to 128px. Additionally, for this experiment we use an additional LPIPS loss. We
use the LPIPS metric computed by an AlexNet network, and we add this loss to our regular MSE
loss. We weight the LPIPS loss by a factor of 100, so that it has a comparable order of magnitude to
the MSE loss."
REFERENCES,0.7630522088353414,"Manipulations
Manipulations are done as follows:"
REFERENCES,0.7670682730923695,• Substraction: We randomly delete up to two of the object slots.
REFERENCES,0.7710843373493976,• Addition: We randomly add an object slot from another scene.
REFERENCES,0.7751004016064257,• Scale: We reduce the scale (in all XYZ axis) of one of the objects in the substraction scene.
REFERENCES,0.7791164658634538,"• Forward: We manipulate the pose vector of one object in the substraction scene and move
it forward on the Z axis."
REFERENCES,0.7831325301204819,"• Right: We manipulate the pose vector of one object in the substraction scene and move it
forward on the X axis."
REFERENCES,0.7871485943775101,"Additionally, we consider the Swap transformation for Table 1. This transformations modiﬁes a
scene by replacing the object shape and appearance vectors with those of an object from another
scene."
REFERENCES,0.7911646586345381,"Metrics
To compare reconstructions we use Mean-Squared Error, Peak Signal-to-Noise Ratio
(PSNR), Structural Similarity (SSIM) and the LPIPS metrics."
REFERENCES,0.7951807228915663,MSE measure the average squared difference between pixel values.
REFERENCES,0.7991967871485943,"MSE(x, x′) = 1 N X"
REFERENCES,0.8032128514056225,"N
(x −x′)2
(5)"
REFERENCES,0.8072289156626506,PSNR is a metric commonly used in signal processing.
REFERENCES,0.8112449799196787,"PSNR(x, x′) = −10 log10(MSE(x, x′))
(6)"
REFERENCES,0.8152610441767069,"SSIM (Wang et al., 2004) provides scores more aligned with human perception, specially under the
presence of image noise. Scores are computed convolutionally by applying a kernel over images,
which are then contrasted."
REFERENCES,0.8192771084337349,"LPIPS (Zhang et al., 2018) computes differences in neural network activations for two images. It is
a perceptual metric that has been shown to have higher correlation to human perception than other
metrics not based on neural networks."
REFERENCES,0.8232931726907631,"To compare populations of generated images we use the Frechet Inception Distance (Heusel et al.,
2017). The Frechet Inception Distance embeds images into a neural network space and then ﬁts a
Gaussian distribution to the generated and ground-truth activation statistics. The score is obtained
by then computing the Frechet distance between the two. Note that other metrics such as Inception
Score are not applicable for the CLEVR2345 since there are no well-deﬁned classes."
REFERENCES,0.8273092369477911,Under review as a conference paper at ICLR 2022
REFERENCES,0.8313253012048193,"B.2
OBJECT DISCOVERY"
REFERENCES,0.8353413654618473,"Dataset
For object discovery we consider the CLEVR6 dataset. We use the original CLEVR6
dataset and extract the images from TFRecord ﬁles available at this URL. We use the original train-
ing/test split, using the ﬁrst 70% images for training and the remaining ones for test. We take a
crop between pixels [29, 221] and [64, 256], for the height and width respectively, and then resize
the crop to 64px. We normalize the value of the images between [0, 1]. To generate the CLEVR6
dataset, we keep only those images that have at maximum 6 objects according to the annotation ﬁles."
REFERENCES,0.8393574297188755,"Training
We use a batch size of 128 and train our model for 400k iterations. We use Adam with
a learning rate of 1 × 10−4 and weight decay 1 × 10−6. We use 6 objects and rely on the model to
not use additional slots when needed."
REFERENCES,0.8433734939759037,"Metrics
We follow previous work (Greff et al., 2019) and use the Adjusted Rand Index
(ARI) (Rand, 1971) to evaluate cluster assignments in object discovery. ARI scores range from 0
(random assigment) to 1 (perfect match). As in previous works, we do not consider a segmentation
mask for the background."
REFERENCES,0.8473895582329317,"B.3
VISUAL REASONING ON CATER"
REFERENCES,0.8514056224899599,"For the visual reasoning task, we consider the CATER dataset and uses 5K videos that do not have
camera motion. All the videos are resized to a 64x64 resolution."
REFERENCES,0.8554216867469879,"B.3.1
PRETRAINING"
REFERENCES,0.8594377510040161,"We ﬁrst pretrain our INFERNO to reconstruct individual frames from the CATER dataset. We
bootstrap a model trained on CLEVR6 for object discovery for 400k iterations and train it on the
CATER dataset for an additional 100k iterations to speed-up training, as the iteration time of a
model with 10 objects is larger. We use a batch size of 128 and we use Adam with a learning rate
of 1 × 10−4 and weight decay 1 × 10−6. When training on CLEVR6 we use 6 object slots, while
when training on CATER we use 10 object slots."
REFERENCES,0.8634538152610441,"B.3.2
FINETUNING"
REFERENCES,0.8674698795180723,"After pretraining, we ﬁnetune the INFERNO encoder to the supervised task of snitch localization.
We discard the rendering pipeline of our model, and instead feed the inferred object slots represen-
tations to a transformer that aims at predicting the ﬁnal position of the snitch."
REFERENCES,0.8714859437751004,"To predict the snitch, we consider a 12 layers transformer with the hidden dimension of 128 which
takes the slot representation as input. The transformer treats each object as input element. A learned
positional embedding is added each slots representation based on their frames index, i.e. the position
of the objects is the same within a frame. The ﬁnal output to the transformer is given to a 1 layer
MLP head with an hidden dimension of 128. It outputs 36 logits that correspond to possible snitch
location. We minimize the sum of the cross-entropy between the predicted position and the true
target and a the l1 loss between the prediction and target."
REFERENCES,0.8755020080321285,"We optionally use a SSL loss similar to Ding et al. (2020). The SSL loss randomly masks one object
per-frame and tries to predict its representation at the corresponding output. The model minimizes
the L2 distance between the predicted object representation and the observed one. The SSL loss
is only backpropagated through the transformer and not the encoder. We weight the SSL loss by a
factor of 1.0e −3."
REFERENCES,0.8795180722891566,"We use an Adam optimizer to minimize the loss. The initial learning rate is set to the 1.0e −4 and
gradually decreased to 1.0e −6 using a cosine learning rate decay. Similarly, we use a initial weight
decay of 1.0e −5 that we increases to 1.0e −3 using a cosine schedule. Our model is ﬁnetuned for
500 epochs. We don’t make use of learning rate decay."
REFERENCES,0.8835341365461847,"During training, we randomly samples 40 frames from a video and predict the snitch localization
from this one crop. At test time, we randomly sample 10 temporal crop of 40 frames each and
average the prediction over the 10 crops as our ﬁnal prediction."
REFERENCES,0.8875502008032129,Under review as a conference paper at ICLR 2022
REFERENCES,0.891566265060241,"C
ADDITIONAL VISUALIZATIONS"
REFERENCES,0.8955823293172691,"Input
Recon.
Input
Recon.
Input
Recon."
REFERENCES,0.8995983935742972,Figure 4: Additional reconstructions on CLEVR2345.
REFERENCES,0.9036144578313253,"We have included additional manipulations of one scene in GIF format in the supplementary mate-
rial. We show: i) each object rendered individually, ii) object identity swaps with other scenes, iii)
object translations along one axis, iv) translations in diagonal (two axis), and v) objects moving in a
circle."
REFERENCES,0.9076305220883534,Under review as a conference paper at ICLR 2022
REFERENCES,0.9116465863453815,"Input
Add
Remove
Input
Add
Remove"
REFERENCES,0.9156626506024096,"Figure 5: Additional object additions and removals on CLEVR2345. For each scene, we show
images with one randomly added object, and with 1-3 random objects removed. Some of these
images show out-of-distribution samples with a number of objects not seen during training (2-5
objects)."
REFERENCES,0.9196787148594378,Under review as a conference paper at ICLR 2022
REFERENCES,0.9236947791164659,"D
NOVEL VIEW SYNTHESIS"
REFERENCES,0.927710843373494,"In this section we synthesize novel views of a scene by modifying the camera pose. For each
example we show the input image, our reconstruction, then two images as a result of moving the
camera ±15◦in the azimuth axis, and two images as a result of zooming in the scene. While our
model is trained without ground-truth camera poses and with single views of scenes, it is able to
generalize to small camera pose modiﬁcations and render novel views of a scene"
REFERENCES,0.9317269076305221,"Input
Recon.
Rot. +15 deg. Rot. -15 deg.
Zoom x1.5
Zoom x2"
REFERENCES,0.9357429718875502,"Figure 6: Novel view synthesis on CLEVR2345. For each scene, we move the camera ±15◦
on the azimuth axis. Additionally, we zoom in the scene twice. While our model is trained with
a ﬁxed default camera pose and single scene views, it is able to generalize to small camera pose
modiﬁcations and render novel views of a scene."
REFERENCES,0.9397590361445783,Under review as a conference paper at ICLR 2022
REFERENCES,0.9437751004016064,"E
NERF OUTPUT"
REFERENCES,0.9477911646586346,"In this section we show the raw outputs of the NeRF function. These outputs show scene views
rendered at low resolution, which are then upscaled with a neural network. While these generations
have reduced details due to their resolution, they clearly show the different objects and their location
in the scene. NeRFs are rendered at low resolution to ease the computational costs, as the time and
memory requirements of rendering with a NeRF are linearly correlated with the number of casted
rays/pixels in the output image."
REFERENCES,0.9518072289156626,"Input
Recon.
NeRF out
Input
Recon.
NeRF out"
REFERENCES,0.9558232931726908,"Figure 7: NeRF outputs on CLEVR2345. For each input scene we show the reconstructed image
as well as each the low resolution output of the NeRF function. This output is then upscaled with a
neural network to obtain the reconstructed scene. While NeRF output lack full detail, they correctly
depict each individual object and their position in the scene."
REFERENCES,0.9598393574297188,Under review as a conference paper at ICLR 2022
REFERENCES,0.963855421686747,"F
SHAPE - APPEARANCE DISENTANGLEMENT"
REFERENCES,0.9678714859437751,"In this section we show examples of the disentanglement of shape and appearance of objects. To
demonstrate this property of our representations, we ﬁrst reconstruct a given scene, and we then
randomly change the appearance vector of one of the objects in the scene by the appearance vectors
of other objects in the same scene. While the shading and color of the object changes, the overall
shape remains the same."
REFERENCES,0.9718875502008032,"Input
Recon.
App. 1
App. 2"
REFERENCES,0.9759036144578314,"Figure 8: Shape and appearance disentanglement on CLEVR2345. For each input scene we alter
the appearance vector of an object in the scene. First, we show the reconstructed scene. Then, for
one of the objects in the scene, we replace its appearance vector by that of another object in the
scene, while keeping its shape vector ﬁxed. We perform this operation two times. We observe that,
while the shading and color of the object changes as we change the appearance vector, its shape
remains constant."
REFERENCES,0.9799196787148594,Under review as a conference paper at ICLR 2022
REFERENCES,0.9839357429718876,"G
OCCUPANCY MAPS"
REFERENCES,0.9879518072289156,"In this section we show the occupancy maps for each rendered object. The occupancy maps are
obtained by integrating rays going from the image plane to the scene according to the NeRF density,
but without taking into account the RGB output. We observe that each object is rendering a part of
the scene corresponding to a single object instance. NeRF outputs are rendered at low resolution and
then upscaled to full resolution with a neural network. To perform unsupervised object discovery
we use the outputs of slot attention, since they do not require any upsampling and thus can provide
more accurate segmentations."
REFERENCES,0.9919678714859438,"Input
Recon.
Obj. 1
Obj. 2
Obj. 3
Obj. 4
Obj. 5"
REFERENCES,0.9959839357429718,"Figure 9: Object occupancy maps on CLEVR2345. For each input scene we show the recon-
structed image as well as each individual object occupancy map. Occupancy maps are obtained by
integrating the density outputs of the NeRF function along rays going from the image plane to the
scene. We observe that each object is rendering a part of the scene corresponding to a single object
instance. Outputs of the NeRF function have low resolution and are upscaled by a neural network.
The last row shows an example of a bad grouping of two objects in the scene."
