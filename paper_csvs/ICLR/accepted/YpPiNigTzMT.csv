Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013679890560875513,"Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large
datasets for training data-hungry models. These approaches synthesize multiple noisy
but cheaply-acquired estimates of labels into a set of high-quality pseudolabels for down-
stream training. However, the synthesis technique is speciÔ¨Åc to a particular kind of label,
such as binary labels or sequences, and each new label type requires manually designing
a new synthesis algorithm. Instead, we propose a universal technique that enables weak
supervision over any label type while still offering desirable properties, including practical
Ô¨Çexibility, computational efÔ¨Åciency, and theoretical guarantees. We apply this technique to
important problems previously not tackled by WS frameworks including learning to rank,
regression, and learning in hyperbolic space. Theoretically, our synthesis approach pro-
duces a consistent estimators for learning some challenging but important generalizations
of the exponential family model. Experimentally, we validate our framework and show
improvement over baselines in diverse settings including real-world learning-to-rank and
regression problems along with learning on hyperbolic manifolds."
INTRODUCTION,0.0027359781121751026,"1
INTRODUCTION"
INTRODUCTION,0.004103967168262654,"Weak supervision (WS) frameworks help overcome the labeling bottleneck: the challenge of building a large
dataset for use in training data-hungry deep models. WS approaches replace hand-labeling with synthesizing
multiple noisy but cheap sources, called labeling functions, applied to unlabeled data. As these sources may
vary in quality and be dependent, a crucial step is to model their accuracies and correlations. Informed by
this model, high-quality pseudolabels are produced and used to train a downstream model. This simple yet
Ô¨Çexible approach is highly successful in research and industry settings (Bach et al., 2019; R¬¥e et al., 2020) ."
INTRODUCTION,0.005471956224350205,"WS frameworks offer three advantages: they are (i) Ô¨Çexible and subsume many existing ways to integrate
side information, (ii) computationally efÔ¨Åcient, and (iii) they offer theoretical guarantees, including estimator
consistency. Unfortunately, these beneÔ¨Åts come at the cost of being particular to very speciÔ¨Åc problem
settings: categorical, usually binary, labels. Extensions, e.g., to time-series data (Safranchik et al., 2020),
or to segmentation masks (Hooper et al., 2021), require a new model for source synthesis, a new algorithm,
and more. We seek to side-step this expensive one-at-a time process via a universal approach, enabling WS
to work in any problem setting while still providing the three advantageous properties above."
INTRODUCTION,0.006839945280437756,"The main technical challenge for universal WS is the diversity of label settings: classiÔ¨Åcation, structured
prediction, regression, rankings, and more. Each of these settings seemingly demands a different approach
for learning the source synthesis model, which we refer to as the label model. For example, Ratner et al.
(2019) assumed that the distribution of the sources and latent true label is an Ising model and relied on a
property of such distributions: that the inverse covariance matrix is graph-structured. It is not clear how to
lift such a property to spaces of permutations or to Riemannian manifolds."
INTRODUCTION,0.008207934336525308,Published as a conference paper at ICLR 2022
INTRODUCTION,0.009575923392612859,"1
2
3
ùìß= ‚Ñùùíå; ùì®= ùë∫3"
INTRODUCTION,0.01094391244870041,"Rankings
(Board Games Rankings)"
INTRODUCTION,0.012311901504787962,"Regression
(Movie Ratings)"
INTRODUCTION,0.013679890560875513,"Classification
(Image Classification)
Geodesic Regression"
INTRODUCTION,0.015047879616963064,"ùìß= ‚Ñùùíå; ùì®‚àà{ùêÇ, ùêÉ, ùêì}"
INTRODUCTION,0.016415868673050615,Universal Weak Supervision Framework
INTRODUCTION,0.017783857729138167,Universal Weak Supervision
INTRODUCTION,0.019151846785225718,Framework
INTRODUCTION,0.02051983584131327,"ùõå1
ùõåm
‚Ä¶"
INTRODUCTION,0.02188782489740082,"‡∑°ùíÄ
‡∑°ùíÄ
‡∑°ùíÄ
‡∑°ùíÄ
‡∑°ùíÄ"
INTRODUCTION,0.023255813953488372,"ùõå1
ùõåm
‚Ä¶
ùõå1
ùõåm
‚Ä¶
ùõå1
ùõåm
‚Ä¶
ùõå1
ùõåm
‚Ä¶"
INTRODUCTION,0.024623803009575923,"‡∑ùùíöùíä‚ààùë∫3
‡∑ùùíöùíä‚àà‚Ñù
‡∑ùùíöùíä‚àà{ùêÇ, ùêÉ, ùêì}
‡∑ùùíöùíä‚ààùë¥
‡∑ùùíöùíä‚ààùì®"
INTRODUCTION,0.025991792065663474,"ùõåùíä: ùìß‚Ü¶ùë∫3
ùõåùíä: ùìß‚Ü¶‚Ñù"
INTRODUCTION,0.027359781121751026,ùìß= ‚Ñùùíå; ùì®= ‚Ñù ùìß= ‚Ñùùíå ùì®= ùë¥
INTRODUCTION,0.028727770177838577,"ùõåùíä: ùìß‚Ü¶{ùêÇ, ùêÉ, ùêì} ùìß, ùì®"
INTRODUCTION,0.030095759233926128,"ùõåùíä: ùìß‚Ü¶‚Ñù
ùõåùíä: ùìß‚Ü¶ùì® ùëå"
INTRODUCTION,0.03146374829001368,Label Model ùõå1 ùõåùüê ùõå3 ùõå4
INTRODUCTION,0.03283173734610123,"Learning To Rank Model 
Regression Model
Classification Model"
INTRODUCTION,0.03419972640218878,"(ùëø,
)
(ùëø, ùüó. ùüñ)
(ùëø, ùë™)
(ùëø,
)
Geodesic Regression"
INTRODUCTION,0.03556771545827633,"Model
End Model"
INTRODUCTION,0.036935704514363885,"(ùëø, ‡∑°ùíÄ)"
INTRODUCTION,0.038303693570451436,Label Embeddings
INTRODUCTION,0.03967168262653899,"ùíà: ùì®‚Ü¶‚àíùüè, +ùüèùêù"
INTRODUCTION,0.04103967168262654,ùíà: ùì®‚Ü¶‚ÑùùíÖ or
INTRODUCTION,0.04240766073871409,"Parameter Estimation
Estimate parameters using"
INTRODUCTION,0.04377564979480164,triplet method
INTRODUCTION,0.04514363885088919,"Universal Weak Supervision Framework
Other Task"
INTRODUCTION,0.046511627906976744,"‡∑†ùêÑ[dg(Œªa, Œªb)]"
INTRODUCTION,0.047879616963064295,"‡∑†ùêÑ[dg(Œªa, y)] ‡∑†Œ∏ùíÇ"
INTRODUCTION,0.049247606019151846,Label Inference ‡∑°ùíÄ
INTRODUCTION,0.0506155950752394,Figure 1: Applications enabled by our approach (left) and weak supervision pipeline (right).
INTRODUCTION,0.05198358413132695,"We propose a general recipe to handle any type of label. The data generation process for the weak sources
is modeled with an exponential family distribution that can represent a label from any metric space (Y, dY).
We embed labels from Y into two tractable choices of space: the Boolean hypercube {¬±1}d and Euclidean
space Rd. The label model (used for source synthesis) is learned with an efÔ¨Åcient method-of-moments
approach in the embedding space. It only requires solving a number of scalar linear or quadratic equations.
Better yet, for certain cases, we show this estimator is consistent via Ô¨Ånite sample bounds."
INTRODUCTION,0.0533515731874145,"Experimentally, we demonstrate our approach on Ô¨Åve choices of problems never before tackled in WS:"
INTRODUCTION,0.05471956224350205,"‚Ä¢ Learning rankings: on two real-world rankings tasks, our approach with as few as Ô¨Åve sources performs
better than supervised learning with a smaller number of true labels. In contrast, an adaptation of the
Snorkel (Ratner et al., 2018) framework cannot reach this performance with as many as 18 sources.
‚Ä¢ Regression: on two real-world regression datasets, when using 6 or more labeling function, the perfor-
mance of our approach is comparable to fully-supervised models.
‚Ä¢ Learning in hyperbolic spaces: on a geodesic regression task in hyperbolic space, we consistently out-
perform fully-supervised learning, even when using only 3 labeling functions (LFs).
‚Ä¢ Estimation in generic metric spaces: in a synthetic setting of metric spaces induced by random graphs,
we demonstrate that our method handles LF heterogeneity better than the majority vote baseline.
‚Ä¢ Learning parse trees: in semantic dependency parsing, we outperform strong baseline models."
INTRODUCTION,0.0560875512995896,"In each experiment, we conÔ¨Årm the following desirable behaviors of weak supervision: (i) more high-quality
and independent sources yield better pseudolabels, (ii) more pseudolabels can yield better performance
compared to training on fewer clean labels, (iii) when source accuracy varies, our approach outperforms
generalizations of majority vote."
PROBLEM FORMULATION AND LABEL MODEL,0.057455540355677154,"2
PROBLEM FORMULATION AND LABEL MODEL"
PROBLEM FORMULATION AND LABEL MODEL,0.058823529411764705,"We give background on weak supervision frameworks, provide the problem formulation, describe our choice
of universal label model, special cases, the embedding technique we use to reduce our problem to two easily-
handled cases, and discuss end model training."
PROBLEM FORMULATION AND LABEL MODEL,0.060191518467852256,"Background: WS frameworks are a principled way to integrate weak or noisy information to produce label
estimates. These weak sources include small pieces of code expressing heuristic principles, crowdworkers,
lookups in external knowledge bases, pretrained models, and many more (Karger et al., 2011; Mintz et al.,
2009; Gupta & Manning, 2014; Dehghani et al., 2017; Ratner et al., 2018). Given an unlabeled dataset, users
construct a set of labeling functions (LFs) based on weak sources and apply them to the data. The estimates
produced by each LF are synthesized to produce pseudolabels that can be used to train a downstream model."
PROBLEM FORMULATION AND LABEL MODEL,0.06155950752393981,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION AND LABEL MODEL,0.06292749658002736,"Problem Formulation: Let x1, x2, . . . , xn be a dataset of unlabeled datapoints from X. Associated with
these are labels from an arbitrary metric space Y with metric dY 1. In conventional supervised learning,
we would have pairs (x1, y1), . . . , (xn, yn); however, in WS, we do not have access to the labels. Instead,
we have estimates of y from m labeling functions (LFs). Each such LF s : X ‚ÜíY produces an estimate
of the true label y from a datapoint x. We write Œªa(i) ‚ààY for the output of the ath labeling function
sa applied to the ith sample. Our goal is to obtain an estimate of the true label yi using the LF outputs
Œª1(i), . . . , Œªm(i). This estimate ÀÜy, is used to train a downstream model. To produce it, we learn the label
model P(Œª1, Œª2, . . . , Œªm|y). The main challenge is that we never observe samples of y; it is latent."
PROBLEM FORMULATION AND LABEL MODEL,0.06429548563611491,We can summarize the weak supervision procedure in two steps:
PROBLEM FORMULATION AND LABEL MODEL,0.06566347469220246,"‚Ä¢ Learning the label model: use the samples Œªa(i) to learn the label model P(Œª1, Œª2, . . . , Œªm|y),
‚Ä¢ Perform inference: compute ÀÜyi, or P(yi|Œª1(i), . . . , Œªm(i)), or a related quantity."
PROBLEM FORMULATION AND LABEL MODEL,0.06703146374829001,"Modeling the sources Previous approaches, e.g., Ratner et al. (2019), select a particular parametric choice
to model p(Œª1, . . . , Œªm|y) that balances two goals: (i) model richness that captures differing LF accuracies
and correlations, and (ii) properties that permit efÔ¨Åcient learning. Our setting demands greater generality.
However, we still wish to exploit the properties of exponential family models. The natural choice is"
PROBLEM FORMULATION AND LABEL MODEL,0.06839945280437756,"p(Œª1, . . . , Œªm|y) = 1"
PROBLEM FORMULATION AND LABEL MODEL,0.06976744186046512,"Z exp
  m
X"
PROBLEM FORMULATION AND LABEL MODEL,0.07113543091655267,"a=1
‚àíŒ∏adY(Œªa, y)"
PROBLEM FORMULATION AND LABEL MODEL,0.07250341997264022,"|
{z
}
Accuracy Potentials +
X"
PROBLEM FORMULATION AND LABEL MODEL,0.07387140902872777,"(a,b)‚ààE
‚àíŒ∏a,bdY(Œªa, Œªb)"
PROBLEM FORMULATION AND LABEL MODEL,0.07523939808481532,"|
{z
}
Correlation Potentials"
PROBLEM FORMULATION AND LABEL MODEL,0.07660738714090287,"
.
(1)"
PROBLEM FORMULATION AND LABEL MODEL,0.07797537619699042,"Here, the set E is a set of correlations corresponding to the graphical representation of the model (Figure 1,
right). Observe how source quality is modeled in (1). If the value of Œ∏a is very large, any disagreement
between the estimate Œªa and y is penalized through the distance dY(Œªa, y) and so has low probability. If Œ∏a
is very small, such disagreements will be common; the source is inaccurate."
PROBLEM FORMULATION AND LABEL MODEL,0.07934336525307797,"We also consider a more general version of (1). We replace ‚àíŒ∏adY(Œªa, y) with a per-source distance dŒ∏a.
For example, for Y = {¬±1}d, dŒ∏a(Œªa, y) = ‚àíŒ∏T
a |Œªa ‚àíy|, with Œ∏a ‚ààRd, generalizes the Hamming distance.
Similarly, for Y = Rd, we can generalize ‚àíŒ∏a‚à•Œªa ‚àíy‚à•2 with dŒ∏a(Œªa, y) = ‚àí(Œªa ‚àíy)T Œ∏a(Œªa ‚àíy), with
Œ∏a ‚ààRd√ód p.d., so that LF errors are not necessarily isotropic. In the Appendix B, we detail variations and
special cases of such models along with relationships to existing weak supervision work. Below, we give a
selection of examples, noting that the last three cannot be tackled with existing methods."
PROBLEM FORMULATION AND LABEL MODEL,0.08071135430916553,"‚Ä¢ Binary classiÔ¨Åcation: Y = {¬±1}, dY is the Hamming distance: this yields a shifted Ising model for
standard binary classiÔ¨Åcation, as in Fu et al. (2020).
‚Ä¢ Sequence learning: Y = {¬±1}d, dY is the Hamming distance: this yields an Ising model for sequences,
as in Sala et al. (2019) and Safranchik et al. (2020).
‚Ä¢ Ranking: Y = SœÅ, the permutation group on {1, . . . , œÅ}, dY is the Kendall tau distance. This is a
heterogenous Mallows model, where rankings are produced from varying-quality sources. If m = 1, we
obtain a variant of the conventional Mallows model (Mallows, 1957).
‚Ä¢ Regression: Y = R, dY is the squared ‚Ñì2 distance: it produces sources in Rd with Gaussian errors.
‚Ä¢ Learning on Riemannian manifolds: Y = M, a Riemannian manifold (e.g., hyperbolic space), dY is the
Riemannian distance dM induced by the space‚Äôs Riemannian metric."
PROBLEM FORMULATION AND LABEL MODEL,0.08207934336525308,"Majority Vote (MV) and its Relatives A simplifying approach often used as a baseline in weak supervision
is the majority vote. Assume that the sources are conditionally independent (i.e. E is empty in (1)) and all
accuracies are identical. In this case, there is no need to learn the model (1); instead, the most ‚Äúpopular‚Äù"
PROBLEM FORMULATION AND LABEL MODEL,0.08344733242134063,"1We slightly abuse notation by allowing dY to be dc, where d is some base metric and c is an exponent. This permits
us to use, for example, the squared Euclidean distance‚Äînot itself a metric‚Äîwithout repeatedly writing the exponent c."
PROBLEM FORMULATION AND LABEL MODEL,0.08481532147742818,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION AND LABEL MODEL,0.08618331053351573,"Problem
Set
Distance
MV Equivalent
Im(g)
End Model
Binary ClassiÔ¨Åcation
{¬±1}
‚Ñì1
Majority Vote
{¬±1}
Binary ClassiÔ¨Åer
Ranking
SœÅ
Kendall tau
Kemeny Rule
{¬±1}(œÅ
2)
Learning to Rank
Regression
R
squared-‚Ñì2
Arithmetic Mean
R
Linear Regression
Riemannian Manifold
M
Riemannian
Fr¬¥echet Mean
Rd
Geodesic Regression
Dependency Parsing
T
‚Ñì2
Fr¬¥echet Mean
Rd√ód
Parsing Model"
PROBLEM FORMULATION AND LABEL MODEL,0.08755129958960328,"Table 1: A variety of problems enabled by universal WS, with speciÔ¨Åcations for sets, distances, and models."
PROBLEM FORMULATION AND LABEL MODEL,0.08891928864569083,"label is used. For binary labels, this is the majority label. In the universal setting, a natural generalization is"
PROBLEM FORMULATION AND LABEL MODEL,0.09028727770177838,"ÀÜyMV = arg min
z‚ààY"
M,0.09165526675786594,"1
m Xm"
M,0.09302325581395349,"a=1 dY(Œªa, z).
(2)"
M,0.09439124487004104,"Special cases of (2) have their own name; for SœÅ, it is the Kemeny rule (Kemeny, 1959). For dM, the squared
Riemannian manifold distance, ÀÜyMV is called the Fr¬¥echet or Karcher mean."
M,0.09575923392612859,"Majority vote, however, is insufÔ¨Åcient in cases where there is variation in the source qualities. We must
learn the label model. However, generically learning (1) is an ambitious goal. Even cases that specialize
(1) in multiple ways have only recently been fully solved, e.g., the permutation case for identical Œ∏a‚Äôs was
fully characterized by Mukherjee (2016). To overcome the challenge of generality, we opt for an embedding
approach that reduces our problem to two tractable cases."
M,0.09712722298221614,"Universality via Embeddings To deal with the very high level of generality, we reduce the problem to
just two metric spaces: the boolean hypercube {‚àí1, +1}d and Euclidean space Rd. To do so, let g :
Y ‚Üí{¬±1}d (or g : Y ‚ÜíRd) be an injective embedding function. The advantage of this approach is
that if g is isometric‚Äîdistance preserving‚Äîthen probabilities are preserved under g. This is because the
sufÔ¨Åcient statistics are preserved: for example, for g : Y ‚ÜíRd, ‚àíŒ∏adY(Œªa, y) = ‚àíŒ∏ad(g(Œªa), g(y)) =
‚àíŒ∏a‚à•g(Œªa) ‚àíg(y)‚à•c , so that if g is a bijection, we obtain a multivariate normal for c = 2. If g is not
isometric, there is a rich literature on low-distortion embeddings, with Bourgain‚Äôs theorem as a cornerstone
result (Bourgain, 1985). This can be used to bound the error in recovering the parameters for any label type."
M,0.09849521203830369,"End Model Once we have produced pseudolabels‚Äîeither by applying generalized majority vote or by
learning the label model and performing inference‚Äîwe can use the labels to train an end model. Table 2
summarizes examples of problem types, explaining the underlying label set, the metric, the generalization
of majority vote, the embedding space Im(g), and an example of an end model."
UNIVERSAL LABEL MODEL LEARNING,0.09986320109439124,"3
UNIVERSAL LABEL MODEL LEARNING"
UNIVERSAL LABEL MODEL LEARNING,0.1012311901504788,"Now that we have a speciÔ¨Åcation of the label model distribution (1) (or its generalized form with per-source
distances), we must learn the distribution from the observed LFs Œª1, . . . , Œªm. Afterwards, we can perform
inference to compute ÀÜy or p(y|Œª1, . . . , Œªm) or a related quantity, and use these to train a downstream model.
A simpliÔ¨Åed model with an intuitive explanation for the isotropic Gaussian case is given in Appendix D."
UNIVERSAL LABEL MODEL LEARNING,0.10259917920656635,"Learning the Universal Label Model Our general approach is described in Algorithm 1; its steps can be
seen in the pipeline of Figure 1. It involves Ô¨Årst computing an embedding g(Œªa) into {¬±1}d or Rd; we
use multidimensional scaling into Rd as our standard. Next, we learn the per-source mean parameters,
then Ô¨Ånally compute the canonical parameters Œ∏a. The mean parameters are E[dG(g(Œªa)g(y))]; which
reduce to moments like E[g(Œªa)ig(y)i]. Here dG is some distance function associated with the embedding
space. To estimate the mean parameters without observing y (and thus not knowing g(y)), we exploit
observable quantities and conditional independence. As long as we can Ô¨Ånd, for each LF, two others that are
mutually conditionally independent, we can produce a simple non-linear system over the three sources (in
each component of the moment). Solving this system recovers the mean parameters up to sign. We recover
these as long as the LFs are better than random on average (see Ratner et al. (2019))."
UNIVERSAL LABEL MODEL LEARNING,0.1039671682626539,Published as a conference paper at ICLR 2022
UNIVERSAL LABEL MODEL LEARNING,0.10533515731874145,Algorithm 1: Universal Label Model Learning
UNIVERSAL LABEL MODEL LEARNING,0.106703146374829,"Input: Output of labeling functions Œªa(i), correlation set E, prior p for Y , optionally embedding function g.
Embedding: If g is not given, use Multidimensional Scaling (MDS) to obtain embeddings g(Œªa) ‚ààRd ‚àÄa
for a ‚àà{1, 2, . . . , m} do"
UNIVERSAL LABEL MODEL LEARNING,0.10807113543091655,"For b : (a, b) ‚ààE
Estimate Correlations:‚àÄi, j, ÀÜE

g(Œªa)ig(Œªb)j

= 1"
UNIVERSAL LABEL MODEL LEARNING,0.1094391244870041,"n
Pn
t=1 g(Œªa(t))ig(Œªb(t))j
Estimate Accuracy: Pick b, c : (a, b) Ã∏‚ààE, (a, c) Ã∏‚ààE, (b, c) Ã∏‚ààE.
if Im(g) = {¬±1}d"
UNIVERSAL LABEL MODEL LEARNING,0.11080711354309165,"‚àÄi, Estimate ÀÜOa,b = ÀÜP(g(Œªa)i = 1, g(Œªb)i = 1), ÀÜOa,c, ÀÜOb,c, Estimate ‚Ñìa = ÀÜP(g(Œªa)i = 1), ‚Ñìb, ‚Ñìc"
UNIVERSAL LABEL MODEL LEARNING,0.1121751025991792,"Accuracies ‚ÜêQUADRATICTRIPLETS(Oa,b, Oa,c, Ob,c, ‚Ñìa, ‚Ñìb, ‚Ñìc, p, i)
else ‚àÄi, Estimate ÀÜea,b := ÀÜE

g(Œªa)ig(Œªb)i

= 1"
UNIVERSAL LABEL MODEL LEARNING,0.11354309165526676,"n
Pn
t=1 g(Œªa(t))ig(Œªb(t))i, ÀÜea,c, ÀÜeb,c
Accuracies ‚ÜêCONTINUOUSTRIPLETS(ÀÜea,b, ÀÜea,c, ÀÜeb,c, p, i)
Recover accuracy signs (Fu et al., 2020)
end for
return ÀÜŒ∏a, ÀÜŒ∏a,b by running the backward mapping on accuracies and correlations"
UNIVERSAL LABEL MODEL LEARNING,0.11491108071135431,"The systems formed by the conditional independence relations differ based on whether g maps to the Boolean
hypercube {¬±1}d or Euclidean space Rd. In the latter case we obtain a system that has a simple closed
form solution, detailed in Algorithm 2. In the discrete case (Algorithm 4, Appendix E), we need to use
the quadratic formula to solve the system. We require an estimate of the prior p on the label y; there are
techniques do so (Anandkumar et al., 2014; Ratner et al., 2019); we tacitly assume we have access to it.
The Ô¨Ånal step uses the backward mapping from mean to canonical parameters (Wainwright & Jordan, 2008).
This approach is general, but it is easy in special cases: for Gaussians the canonical Œ∏ parameters are the
inverse of the mean parameters."
UNIVERSAL LABEL MODEL LEARNING,0.11627906976744186,"Performing Inference: Maximum Likelihood Estimator Having estimated the label model canonical
parameters ÀÜŒ∏a and ÀÜŒ∏a,b for all the sources, we use the maximum-likelihood estimator"
UNIVERSAL LABEL MODEL LEARNING,0.11764705882352941,"ÀÜyi = arg min
z‚ààY"
M,0.11901504787961696,"1
m Xm"
M,0.12038303693570451,"a=1 dÀÜŒ∏a(Œªa(i), z)
(3)"
M,0.12175102599179206,"Compare this approach to majority vote (2), observing that MV can produce arbitrarily bad outcomes. For
example, suppose that we have m LFs, one of which has a very high accuracy (e.g., large Œ∏1) and the others
very low accuracy (e.g., Œ∏2, Œ∏3, . . . , Œ∏m = 0). Then, Œª1 is nearly always correct, while the other LFs are
nearly random. However, (2) weights them all equally, which will wash out the sole source of signal Œª1. On
the other hand, (3) resolves the issue of equal weights on bad LFs by directly downweighting them."
M,0.12311901504787962,"SimpliÔ¨Åcations While the above models can handle very general scenarios, special cases are dramati-
cally simpler.
In particular, in the case of isotropic Gaussian errors, where dŒ∏a(Œªa, y) = ‚àíŒ∏a‚à•Œªa ‚àí
y‚à•2, there is no need to perform an embedding, since we can directly rely on empirical averages like
1
n
Pn
i=1 dŒ∏a(Œªa(i), Œªb(i)). The continuous triplet step simpliÔ¨Åes to directly estimating the covariance entries
ÀÜŒ∏‚àí1
a ; the backward map is simply inverting this. More details can be found in Appendix D."
M,0.12448700410396717,"4
THEORETICAL ANALYSIS: ESTIMATION ERROR & CONSISTENCY"
M,0.12585499316005472,"We show that, under certain conditions, Algorithm 1 produces consistent estimators of the mean parameters.
We provide convergence rates for the estimators. As corollaries, we apply these to the settings of rankings
and regression, where isometric embeddings are available. Finally, we give a bound on the inconsistency
due to embedding distortion in the non-isometric case."
M,0.12722298221614228,"Boolean Hypercube Case We introduce the following Ô¨Ånite-sample estimation error bound. It demonstrates
consistency for mean parameter estimation for a triplet of LFs when using Algorithm 1. To keep our pre-
sentation simple, we assume: (i) the class balance P(Y = y) are known, (ii) there are two possible values"
M,0.12859097127222982,Published as a conference paper at ICLR 2022
M,0.12995896032831739,"of Y , y1 and y2, (iii) we can correctly recover signs (see Ratner et al. (2019)), (iv) we can Ô¨Ånd at least three
conditionally independent labeling functions that can form a triplet, (v) the embeddings are isometric.
Theorem 4.1. For any Œ¥ > 0, for some y1 and y2 with known class probabilities p = P(Y = y1), the
quadratic triplet method recovers Œ±i = P(g(Œªa)i = 1|Y = y), Œ≤i = P(g(Œªb)i = 1|Y = y), Œ≥i =
P(g(Œªc)i = 1|Y = y) up to error O(( ln(2d2/Œ¥)"
N,0.13132694938440492,"2n
)1/4) with probability at least 1 ‚àíŒ¥ for any conditionally
independent label functions Œªa, Œªb, Œªc and for all i ‚àà[d]."
N,0.1326949384404925,Algorithm 2: CONTINUOUSTRIPLETS
N,0.13406292749658003,"Input: Estimates ÀÜea,b, ÀÜea,c, ÀÜeb,c, prior p, index i
Obtain variance Eg(Y )2
i from prior p
ÀÜE[g(Œªa)g(y)] ‚Üê
p"
N,0.1354309165526676,"|ÀÜea,b| ¬∑ |ÀÜea,c| ¬∑ E [Y 2] / |ÀÜeb,c|
ÀÜE[g(Œªb)g(y)] ‚Üê
p"
N,0.13679890560875513,"|ÀÜea,b| ¬∑ |ÀÜeb,c| ¬∑ E [Y 2] / |ÀÜea,c|
ÀÜE[g(Œªc)g(y)] ‚Üê
p"
N,0.1381668946648427,"|ÀÜea,c| ¬∑ |ÀÜeb,c| ¬∑ E [Y 2] / |ÀÜea,b|
return Accuracies
ÀÜE[g(Œªa)g(y)], ÀÜE[g(Œªb)g(y)], ÀÜE[g(Œªc)g(y)]"
N,0.13953488372093023,"We
state
our
result
via
terms
like
Œ±i
=
P(g(Œªa)i|Y
= y); these can be used to obtain
ÀÜE [g(Œªa)i|y] and so recover ÀÜE [dY(Œªa, y)]."
N,0.1409028727770178,"To showcase the power of this result, we apply it
to rankings from the symmetric group SœÅ equipped
with the Kendall tau distance dœÑ (Kendall, 1938).
This estimation problem is more challenging than
learning the conventional Mallows model (Mal-
lows, 1957)‚Äîand the standard Kemeny rule (Ke-
meny, 1959) used for rank aggregation will fail if
applied to it. Our result yields consistent estimation
when coupled with the aggregation step (3)."
N,0.14227086183310533,"We use the isometric embedding g into {¬±1}(
œÅ
2): For œÄ ‚ààSœÅ, each entry in g(œÄ) corresponds to a pair (i, j)
with i < j, and this entry is 1 if in œÄ we have i < j and ‚àí1 otherwise. We can show for œÄ, Œ≥ ‚ààSœÅ that
P"
N,0.1436388508891929,"i=1 g(œÄ)ig(Œ≥)i =
 œÅ
2

‚àí2dœÑ(œÄ, Œ≥), and so recover ÀÜE [g(Œªa)ig(y)i], and thus ÀÜE [dœÑ(Œªa, y)]. Then,
Corollary 4.1.1. For any Œ¥ > 0, U > 0, a prior over y1 and y2 with known class probability p, and
using Algorithm 1 and Algorithm 4, for any conditionally independent triplet Œªa, Œªb, Œªc, with parameters
U > Œ∏a, Œ∏b, Œ∏c > 4 ln(2), we can recover Œ∏a, Œ∏b, Œ∏c up to error O(g‚àí1
2 (Œ∏ + (log(2œÅ2)/(2Œ¥n))1/4) ‚àíŒ∏) with
probability at least 1 ‚àíŒ¥, where g2(U) = (‚àíœÅe‚àíU)/((1 ‚àíe‚àíU)2) + PœÅ
j=1(j2e‚àíUj)/((1 ‚àíe‚àíUj)2)."
N,0.14500683994528044,"Note that the error goes to zero as O(n‚àí1/4). This is an outcome of using the quadratic system, which
is needed for generality. In the easier cases, where the quadratic approach is not needed (including the
regression case), the rate is O(n‚àí1/2). Next, note that the scaling in terms of the embedding dimension d is
O((log(d))1/4)‚Äîthis is efÔ¨Åcient. The g2 function captures the cost of the backward mapping."
N,0.146374829001368,"Euclidean Space Rd Case The following is an estimation error bound for the continuous triplets method in
regression, where we use the squared Euclidean distance. The result is for d = 1 but can be easily extended."
N,0.14774281805745554,"Theorem 4.2. Let ÀÜE [g(Œªa)g(y)] be an estimate of the accuracies E [g(Œªa)g(y)] using n samples, where all
LFs are conditionally independent given Y . If the signs of a are recoverable, then with high probability"
N,0.1491108071135431,"E[||ÀÜE [g(Œªa)g(y)] ‚àíE [g(Œªa)g(y)] ||2] = O

a‚àí10
|min| + a‚àí6
|min|
 p"
N,0.15047879616963064,"max(e5max, e6max)/n

."
N,0.1518467852257182,"Here, a|min| = mini |E

g(Œªi)g(y)

| and emax = maxj,k ej,k."
N,0.15321477428180574,The error tends to zero with rate O(n‚àí1/2) as expected.
N,0.1545827633378933,"Distortion & Inconsistency Bound Next, we show how to control the inconsistency in parameter estima-
tion as a function of the distortion. We write Œ∏ to be the vector of the canonical parameters, Œ∏‚Ä≤ for their
distorted counterparts (obtained with a consistent estimator on the embedded distances), and ¬µ, ¬µ‚Ä≤ be the
corresponding mean parameters. Let 1 ‚àíŒµ ‚â§dg(g(y), g(y‚Ä≤))/dY(y, y‚Ä≤) ‚â§1 for all y, y‚Ä≤ ‚ààY √ó Y. Then,
for a constant emin (the value of which we characterize in the Appendix)."
N,0.15595075239398085,Published as a conference paper at ICLR 2022
N,0.1573187414500684,Theorem 4.3. The inconsistency in estimating Œ∏ is bounded as ‚à•Œ∏ ‚àíŒ∏‚Ä≤‚à•‚â§Œµ‚à•¬µ‚à•/emin.
EXPERIMENTS,0.15868673050615595,"5
EXPERIMENTS"
EXPERIMENTS,0.1600547195622435,"We evaluated our universal approach with four sample applications, all new to WS: learning to rank, regres-
sion, learning in hyperbolic space, and estimation in generic metric spaces given by random graphs."
EXPERIMENTS,0.16142270861833105,"Our hypothesis is that the universal approach is capable of learning a label model and producing high-quality
pseudolabels with the following properties:"
EXPERIMENTS,0.16279069767441862,"‚Ä¢ The quality of the pseudolabels is a function of the number and quality of the available sources, with more
high-quality, independent sources yielding greater pseudolabel quality,
‚Ä¢ Despite pseudolabels being noisy, an end model trained on more pseudolabels can perform as well or
better than a fully supervised model trained on fewer true labels,
‚Ä¢ The label model and inference procedure (3) improves on the majority vote equivalent‚Äîbut only when
LFs are of varying quality; for LFs of similar quality, the two will have similar performance."
EXPERIMENTS,0.16415868673050615,"Additionally, we expect to improve on naive applications of existing approaches that do not take structure
into account (such as using Snorkel (Ratner et al., 2018) by mapping permutations to integer classes).
Application I: Rankings
We applied our approach to obtain pseudorankings to train a downstream rank-
ing model. We hypothesize that given enough signal, the produced pseudorankings can train a higher-quality
model than using a smaller proportion of true labels. We expect our method produces better performance
than the Snorkel baseline where permutations are converted into multi-class classiÔ¨Åcation. We also anticipate
that our inference procedure (3) improves on the MV baseline (2) when LFs have differing accuracies."
EXPERIMENTS,0.16552667578659372,"Approach, Datasets, Labeling Functions, and End Model We used the isotropic simpliÔ¨Åcation of Algo-
rithm 1. For inference, we applied (3). We compared against baseline fully-supervised models with only a
proportion of the true labels (e.g., 20%, 50%, ...), the Snorkel framework (Ratner et al., 2018) converting
rankings into classes, and majority vote (2). We used real-world datasets compatible with multiple label
types, including a movies dataset and the BoardGameGeek dataset (2017) (BGG), along with synthetic data.
For our movies dataset, we combined IMDb, TMDb, Rotten Tomatoes, and MovieLens movie review data
to obtain features and weak labels. In Movies dataset, rankings were generated by picking d = 5 Ô¨Ålm items
and producing a ranking based on their tMDb average rating. In BGG, we used the available rankings."
EXPERIMENTS,0.16689466484268126,"We created both real and simulated LFs. For simulated LFs, we sampled 1/3 of LFs from less noisy Mallows
model, 2/3 of LFs from very noisy Mallows model. Details are in the Appendix H. For real LFs, we built
labeling functions using external KBs as WS sources based on alternative movie ratings along with popular-
ity, revenue, and vote count-based LFs. For the end model, we used PTRanking (Yu, 2020), with ListMLE
(Xia et al., 2008) loss. We report the Kendall tau distance (dœÑ)."
EXPERIMENTS,0.16826265389876882,"Results Figure 2 reports end model performance for the two datasets with varying numbers of simulated LFs.
We observe that (i) as few as 12 LFs are sufÔ¨Åcient to improve on a fully-supervised model trained on less data
(as much as 20% of the dataset) and that (ii) as more LFs are added, and signal improves, performance also
improves‚Äîas expected. Crucially, the Snorkel baseline, where rankings are mapped into classes, cannot
perform as well as the universal approach; it is not meant to be effective general label settings. Table 2
shows the results when using real LFs, some good, some bad, constructed from alternative ratings and simple
heuristics. Alternative ratings are quite accurate: MV and our method perform similarly. However, when
poorer-quality LFs are added, MV rule tends to degrade more than our proposed model, as we anticipated.
Application II: Regression
We used universal WS in the regression setting. We expect that with more
LFs, we can obtain increasingly high-quality pseudolabels, eventually matching fully-supervised baselines."
EXPERIMENTS,0.16963064295485636,"Approach, Datasets, Labeling Functions, End Model, Results We used Algorithm 1, which uses the
continuous triplets approach 2. For inference, we used the Gaussians simpliÔ¨Åcation. As before, we compared"
EXPERIMENTS,0.17099863201094392,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.17236662106703146,"3
6
9
12
15
18
Num LFs 0.25 0.30 0.35"
EXPERIMENTS,0.17373461012311903,"d (y, y)"
EXPERIMENTS,0.17510259917920656,(a) Movies dataset
EXPERIMENTS,0.17647058823529413,"3
6
9
12
15
18
Num LFs 0.20 0.25 0.30 0.35 0.40"
EXPERIMENTS,0.17783857729138167,"d (y, y)"
EXPERIMENTS,0.17920656634746923,"WS (Snorkel)
WS (Majority Vote)
WS (Ours)
Fully supervised (20%)
Fully supervised (50%)
Fully supervised (100%)"
EXPERIMENTS,0.18057455540355677,(b) BGG dataset
EXPERIMENTS,0.18194254445964433,"Figure 2:
End model performance with ranking LFs
(Left: Movies, Right: BGG). Training a model on pseu-
dolabels is compared to fully-supervised baselines on vary-
ing proportions of the dataset along with the Snorkel base-
line. Metric is the Kendall tau distance; lower is better."
EXPERIMENTS,0.18331053351573187,"103
105"
EXPERIMENTS,0.18467852257181944,"Samples 10
3 10
2"
EXPERIMENTS,0.18604651162790697,"Mu param error, L2"
EXPERIMENTS,0.18741450068399454,estimation error
EXPERIMENTS,0.18878248974008208,"103
105"
EXPERIMENTS,0.19015047879616964,"Samples 10
3 10
2"
EXPERIMENTS,0.19151846785225718,"Tot, param error, L2"
EXPERIMENTS,0.19288645690834474,estimation error
EXPERIMENTS,0.19425444596443228,"103
105"
EXPERIMENTS,0.19562243502051985,"Samples 10
1 100"
EXPERIMENTS,0.19699042407660738,"Y value error, MSE"
EXPERIMENTS,0.19835841313269495,Label estimation error
EXPERIMENTS,0.1997264021887825,"Baseline
Our Approach"
EXPERIMENTS,0.20109439124487005,(a) Parameter Estimation Error
EXPERIMENTS,0.2024623803009576,"Figure 3:
Regression: parameter estimation error
(left two plots) and label estimation error comparing
to majority vote baseline (rightmost) with increasing
number of samples."
EXPERIMENTS,0.20383036935704515,"Setting
dœÑ
MSE
Setting
dœÑ
MSE"
EXPERIMENTS,0.2051983584131327,"Fully supervised (10%)
0.2731
0.3357
WS (One LF, Rotten tomatoes)
0.2495
0.4272
Fully supervised (25%)
0.2465
0.2705
WS (One LF, IMDb score)
0.2289
0.2990
Fully supervised (50%)
0.2313
0.2399
WS (One LF, MovieLens score)
0.2358
0.2690
Fully supervised (100%)
0.2282
0.2106
WS (3 LFs, MV (2))
0.2273
0.2754
WS (3 scores + 3 bad LFs, MV (2))
0.2504
-
WS (3 LFs, Ours)
0.2274
0.2451
WS (3 good + 3 bad LFs, Ours)
0.2437
-"
EXPERIMENTS,0.20656634746922026,"Table 2: End model performance with real-world rankings and regression LFs on movies. WS (3 scores, ¬∑) shows the
result of our algorithm combining 3 LFs. In ranking, high-quality LFs perform well (and better than fewer clean labels),
but mixing in lower-quality LFs hurts majority vote (2) more than our proposed approach. In regression, our method
yields performance similar to fully-supervised with 50% data, while outperforming MV."
EXPERIMENTS,0.2079343365253078,"against baseline fully-supervised models with a fraction of the true labels (e.g., 20%, 50%, ...) and MV (2).
We used the Movies rating datasets with the label being the average rating of TMDb review scores across
users and the BGG dataset with the label being the average rating of board games. We split data into 75%
for training set, and 25% for the test set. For real-world LFs, we used other movie ratings in the movies
dataset. Details are in the Appendix H for our synthetic LF generation procedure.For the end model, we
used gradient boosting (Friedman, 2001). The performance metric is MSE."
EXPERIMENTS,0.20930232558139536,"Figure 9 (Appendix) shows the end model performance with WS compared to fully supervised on the movie
reviews and board game reviews datasets. We also show parameter estimation error in Figure 3. As expected,
our parameter estimation error goes down in the amount of available data. Similarly, our label estimator is
consistent, while majority vote is not. Table 2 also reports the result of using real LFs for movies. Here, MV
shows even worse performance than the best individual LF - Movie Lens Score. On the other side, our label
model lower MSE than the best individual LF, giving similar performance with fully supervised learning
with 50% training data.
Application III: Geodesic Regression in Hyperbolic Space
Next, we evaluate our approach on the prob-
lem of geodesic regression on a Riemannian manifold M, speciÔ¨Åcally, a hyperbolic space with curvature
K = ‚àí50. The goal of this task is analogous to Euclidean linear regression, except the dependent variable
lies on a geodesic in hyperbolic space. Further background is in the Appendix H."
EXPERIMENTS,0.2106703146374829,"Approach, Datasets, LFs, Results We generate y‚àó
i by taking points along a geodesic (a generalization of
a line) parametrized by a tangent vector Œ≤ starting at p, further affecting them by noise. The objective of
the end-model is to recover parameters p and Œ≤, which is done using Riemannian gradient descent (RGD)
to minimize the least-squares objective: ÀÜp, ÀÜŒ≤ = arg minq,Œ±
Pn
i=1 d(expq(xiŒ±), yi)2 where d(¬∑, ¬∑) is the
hyperbolic distance. To generate each LF Œªj, we use noisier estimates of y‚àó
i , where the distribution is
N(0, œÉ2
j ) and œÉ2
j ‚àºU[1.5+(15zj),4.5+(15zj)], zj ‚àºBernoulli(0.5) to simulate heterogeneity across LFs;"
EXPERIMENTS,0.21203830369357046,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.213406292749658,"10
20
Num LFs 0.99 1.00 Loss"
EXPERIMENTS,0.21477428180574556,"Fully supervised (80%)
Fully supervised (90%)
Fully supervised (100%)
WS (Ours)
WS (Majority Vote)"
EXPERIMENTS,0.2161422708618331,"Figure 4: Comparison between our approach, (2),
and fully-supervised in geodesic regression. Metric
is least-squares objective; lower is better."
EXPERIMENTS,0.21751025991792067,"0
2
4
Theta average 0.5 1.0"
EXPERIMENTS,0.2188782489740082,Accuracy
EXPERIMENTS,0.22024623803009577,"Majority Vote
Label Model"
EXPERIMENTS,0.2216142270861833,(a) Heterogeneous LFs
EXPERIMENTS,0.22298221614227087,"0
2
4
Theta average 0.5 1.0"
EXPERIMENTS,0.2243502051983584,Accuracy
EXPERIMENTS,0.22571819425444598,"Majority Vote
Label Model"
EXPERIMENTS,0.2270861833105335,"(b) Homogenous LFs
Figure 5: Comparison between our label model and ma-
jority voting in generic metric space. Metric is accuracy;
higher is better."
EXPERIMENTS,0.22845417236662108,"the noise vectors are parallel transported to the appropriate space. For label model learning, we used the
isotropic Gaussian simpliÔ¨Åcation. Finally, inference used RGD to compute (3). We include MV (2) as a
baseline. We compare fully supervised end models each with n ‚àà{80, 90, 100} labeled examples to weak
supervision using only weak labels. In Figure 4 we see that the Fr¬¥echet mean baseline and our approach both
outperform fully supervised geodesic regression despite the total lack of labeled examples. Intuitively, this
is because with multiple noisier labels, we can produce a better pseudolabel than a single (but less noisy)
label. As expected, our approach yields consistently lower test loss than the Fr¬¥echet mean baseline."
EXPERIMENTS,0.22982216142270862,"Dataset/UAS
ÀÜY
Œª1
Œª2
Œª3"
EXPERIMENTS,0.23119015047879618,"cs pdt-ud
0.873
0.861
0.758
0.842
en ewt-ud
0.795
0.792
0.733
0.792
en lines-ud
0.850
0.833
0.847
0.825
en partut-ud
0.866
0.869
0.866
0.817
Table 3: UAS scores for semantic dependency pars-
ing. ÀÜY is synthesized from off-the-shelf parsing LFs."
EXPERIMENTS,0.23255813953488372,"Application IV: Generic Metric Spaces
We also
evaluated our approach on a structureless problem‚Äî
a generic metric space generated by selecting random
graphs G with a Ô¨Åxed number of nodes and edges. The
metric is the shortest-hop distance between a pair of
nodes. We sampled nodes uniformly and obtain LF val-
ues with (1). Despite the lack of structure, we still an-
ticipate that our approach will succeed in recovering the
latent nodes y when LFs have sufÔ¨Åciently high quality.
We expect that the LM improves on MV (2) when LFs
have heterogeneous quality, while the two will have similar performance on homogeneous quality LFs."
EXPERIMENTS,0.23392612859097128,"Approach, Datasets, LFs, Results We generated random graphs and computed the distance matrix yield-
ing the metric. We used Algorithm (1) with isotropic Gaussian embedding and continuous triplets. For
label model inference, we used (3). Figure 5 shows results on our generic metric space experiment. As ex-
pected, when LFs have a heterogeneous quality, LM yield better accuracy than MV. However, when labeling
functions are of similar quality, LM and MV give similar accuracies.
Application V: Semantic Dependency Parsing
We ran our technique on semantic dependency parsing
tasks, using datasets in English and Czech from the Universal Dependencies collection (Nivre et al., 2020).
The LFs are off-the-shelf parsers from Stanza (Qi et al., 2020) trained on different datasets in the same
language. We model a space of trees with a metric given by the ‚Ñì2 norm on the difference between the
adjacency matrices. We measure the quality of the synthesized tree ÀÜY with the unlabeled attachment scores
(UAS). Our results are shown in Table 3. As expected, when the parsers are of different quality, we can
obtain a better result."
CONCLUSION,0.23529411764705882,"6
CONCLUSION"
CONCLUSION,0.23666210670314639,"Weak supervision approaches allow users to overcome the manual labeling bottleneck for dataset construc-
tion. While successful, such methods do not apply to each potential label type. We proposed an approach to
universally apply WS, demonstrating it for three applications new to WS: rankings, regression, and learning
in hyperbolic space. We hope our proposed technique encourages applying WS to many more applications."
CONCLUSION,0.23803009575923392,Published as a conference paper at ICLR 2022
CONCLUSION,0.2393980848153215,ACKNOWLEDGMENTS
CONCLUSION,0.24076607387140903,"We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision)
and the Wisconsin Alumni Research Foundation (WARF)."
REFERENCES,0.2421340629274966,REFERENCES
REFERENCES,0.24350205198358413,Imdb movie dataset. https://www.imdb.com/interfaces/.
REFERENCES,0.2448700410396717,MSLR-WEB10K. https://www.microsoft.com/en-us/research/project/mslr/.
REFERENCES,0.24623803009575923,Tmdb 5k movie dataset version 2. https://www.kaggle.com/tmdb/tmdb-movie-metadata.
REFERENCES,0.2476060191518468,"BoardGameGeek
Reviews
Version
2.
https://www.kaggle.com/jvanelteren/
boardgamegeek-reviews, 2017."
REFERENCES,0.24897400820793433,"Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-
positions for learning latent variable models. Journal of Machine Learning Research, 15:2773‚Äì2832,
2014."
REFERENCES,0.2503419972640219,"Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen,
Alex Ratner, Braden Hancock, Houman Alborzi, et al. Snorkel drybell: A case study in deploying weak
supervision at industrial scale. In Proceedings of the 2019 International Conference on Management of
Data, pp. 362‚Äì375, 2019."
REFERENCES,0.25170998632010944,"Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proc. of the
eleventh annual conference on Computational learning theory, pp. 92‚Äì100. ACM, 1998."
REFERENCES,0.253077975376197,"J. Bourgain. On lipschitz embedding of Ô¨Ånite metric spaces in hilbert space. Israel Journal of Mathematics,
52(1-2):46‚Äì52, 1985."
REFERENCES,0.25444596443228457,"R¬¥obert Busa-Fekete, Dimitris Fotakis, Bal¬¥azs Sz¬®or¬¥enyi, and Manolis Zampetakis. Optimal learning of mal-
lows block model. In Alina Beygelzimer and Daniel Hsu (eds.), Conference on Learning Theory, COLT
2019, 2019."
REFERENCES,0.2558139534883721,"Ioannis Caragiannis, Ariel D. Procaccia, and Nisarg Shah. When do noisy votes reveal the truth?
ACM
Trans. Econ. Comput., 4(3), March 2016. ISSN 2167-8375. doi: 10.1145/2892565. URL https:
//doi.org/10.1145/2892565."
REFERENCES,0.25718194254445964,"Arun Tejasvi Chaganty and Percy Liang. Estimating latent-variable graphical models using moments and
likelihoods. In International Conference on Machine Learning, pp. 1872‚Äì1880, 2014."
REFERENCES,0.2585499316005472,"Mayee F. Chen, Benjamin Cohen-Wang, Stephen Mussmann, Frederic Sala, and Christopher R¬¥e. Comparing
the value of labeled and unlabeled data in method-of-moments latent variable estimation. In International
Conference on ArtiÔ¨Åcial Intelligence and Statistics (AISTATS), 2021."
REFERENCES,0.25991792065663477,"Flavio Chierichetti, Anirban Dasgupta, Ravi Kumar, and Silvio Lattanzi. On reconstructing a hidden permu-
tation. In 17th Int‚Äôl Workshop on Approximation Algorithms for Combinatorial Optimization Problems
(APPROX‚Äô14), 2014."
REFERENCES,0.2612859097127223,"Andrew Davenport and Jayant Kalagnanam.
A computational study of the kemeny rule for preference
aggregation. In Proceedings of the 19th national conference on ArtiÔ¨Åcal intelligence (AAAI 04), 2004."
REFERENCES,0.26265389876880985,"Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using
the em algorithm. Applied statistics, pp. 20‚Äì28, 1979."
REFERENCES,0.2640218878248974,Published as a conference paper at ICLR 2022
REFERENCES,0.265389876880985,"Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. Neural ranking
models with weak supervision. In Proceedings of the 40th International ACM SIGIR Conferenceon Re-
search and Development in Information Retrieval, 2017."
REFERENCES,0.2667578659370725,"M.A. Fligner and J.S. Verducci.
Distance based ranking models.
J.R. Statist. Soc. B, 48(3), 1986.
URL https://www-jstor-org.ezproxy.library.wisc.edu/stable/2345433?seq=
1#metadata_info_tab_contents."
REFERENCES,0.26812585499316005,"Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp.
1189‚Äì1232, 2001."
REFERENCES,0.2694938440492476,"Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher R¬¥e.
Fast and three-rious: Speeding up weak supervision with triplet methods. In Proceedings of the 37th
International Conference on Machine Learning (ICML 2020), 2020."
REFERENCES,0.2708618331053352,"Sonal Gupta and Christopher D Manning. Improved pattern learning for bootstrapped entity extraction. In
Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pp. 98‚Äì108,
2014."
REFERENCES,0.2722298221614227,"Sarah Hooper, Michael Wornow, Ying Hang Seah, Peter Kellman, Hui Xue, Frederic Sala, Curtis Langlotz,
and Christopher R¬¥e. Cut out the annotator, keep the cutout: better segmentation with weak supervision.
In Proceedings of the International Conference on Learning Representations (ICLR 2021), 2021."
REFERENCES,0.27359781121751026,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning, pp. 448‚Äì456. PMLR, 2015."
REFERENCES,0.2749658002735978,"Manas Joglekar, Hector Garcia-Molina, and Aditya Parameswaran. Evaluating the crowd with conÔ¨Ådence.
In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 686‚Äì694, 2013."
REFERENCES,0.2763337893296854,"Manas Joglekar, Hector Garcia-Molina, and Aditya Parameswaran.
Comprehensive and reliable crowd
assessment algorithms. In Data Engineering (ICDE), 2015 IEEE 31st International Conference on, pp.
195‚Äì206. IEEE, 2015."
REFERENCES,0.2777017783857729,"David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. In
Advances in neural information processing systems, pp. 1953‚Äì1961, 2011."
REFERENCES,0.27906976744186046,"J. Kemeny. Mathematics without numbers. Daedalus, 88(4):577‚Äì591, 1959."
REFERENCES,0.280437756497948,"Maurice Kendall. A new measure of rank correlation. Biometrika, pp. 81‚Äì89, 1938."
REFERENCES,0.2818057455540356,"Claire Kenyon-Mathieu and Warren Schudy. How to rank with few errors. In Proceedings of the thirty-ninth
annual ACM symposium on Theory of computing (STOC ‚Äô07), 2007."
REFERENCES,0.28317373461012313,"C. L. Mallows. Non-Null Ranking Models. I. Biometrika, 44, 1957. doi: 10.1093/biomet/44.1-2.114. URL
https://doi.org/10.1093/biomet/44.1-2.114."
REFERENCES,0.28454172366621067,"J. Marden. Analyzing and modeling rank data. Chapman and Hall/CRC, 2014."
REFERENCES,0.2859097127222982,"Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pp.
1003‚Äì1011. Association for Computational Linguistics, 2009."
REFERENCES,0.2872777017783858,"Sumit Mukherjee. Estimation in exponential families on permutations. The Annals of Statistics, 44(2):
853‚Äì875, 2016."
REFERENCES,0.28864569083447333,Published as a conference paper at ICLR 2022
REFERENCES,0.29001367989056087,"Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan HajiÀác, Christopher D. Manning, Sampo
Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal Dependencies v2: An ever-
growing multilingual treebank collection. In Proceedings of the 12th Language Resources and Evalu-
ation Conference, pp. 4034‚Äì4043, Marseille, France, May 2020. URL https://aclanthology.
org/2020.lrec-1.497."
REFERENCES,0.2913816689466484,"R. L. Plackett. The analysis of permutations. J. Applied Statistics, 24:193‚Äì202, 1975."
REFERENCES,0.292749658002736,"Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. Stanza: A Python natural
language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics: System Demonstrations, 2020. URL https://nlp.
stanford.edu/pubs/qi2020stanza.pdf."
REFERENCES,0.29411764705882354,"Aditi Raghunathan, Roy Frostig, John Duchi, and Percy Liang. Estimation from indirect supervision with
linear moments. In International conference on machine learning, pp. 2568‚Äì2577, 2016."
REFERENCES,0.2954856361149111,"A. J. Ratner, Christopher M. De Sa, Sen Wu, Daniel Selsam, and C. R¬¥e. Data programming: Creating large
training sets, quickly. In Proceedings of the 29th Conference on Neural Information Processing Systems
(NIPS 2016), Barcelona, Spain, 2016."
REFERENCES,0.2968536251709986,"A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. R¬¥e. Training complex models with
multi-task weak supervision. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, Honolulu,
Hawaii, 2019."
REFERENCES,0.2982216142270862,"Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R¬¥e. Snorkel:
Rapid training data creation with weak supervision. In Proceedings of the 44th International Conference
on Very Large Data Bases (VLDB), Rio de Janeiro, Brazil, 2018."
REFERENCES,0.29958960328317374,"Christopher R¬¥e, Feng Niu, Pallavi Gudipati, and Charles Srisuwananukorn. Overton: A data system for
monitoring and improving machine-learned products. In Proceedings of the 10th Annual Conference on
Innovative Data Systems Research, 2020."
REFERENCES,0.3009575923392613,"Esteban Safranchik, Shiying Luo, and Stephen Bach. Weakly supervised sequence tagging from noisy rules.
In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI), pp. 5570‚Äì5578, Apr. 2020."
REFERENCES,0.3023255813953488,"Frederic Sala, Paroma Varma, Jason Fries, Daniel Y. Fu, Shiori Sagawa, Saelig Khattar, Ashwini Ramamoor-
thy, Ke Xiao, Kayvon Fatahalian, James Priest, and Christopher R¬¥e. Multi-resolution weak supervision
for sequential data. In Advances in Neural Information Processing Systems 32, pp. 192‚Äì203, 2019."
REFERENCES,0.3036935704514364,Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. 2014.
REFERENCES,0.30506155950752395,"P. Varma, F. Sala, A. He, A. J. Ratner, and C. R¬¥e. Learning dependency structures for weak supervision
models. In Proceedings of the 36th International Conference on Machine Learning (ICML 2019), 2019."
REFERENCES,0.3064295485636115,"Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational infer-
ence. Foundations and Trends¬Æ in Machine Learning, 1(1-2):1‚Äì305, 2008."
REFERENCES,0.307797537619699,"Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank:
theory and algorithm. In Proceedings of the 25th international conference on Machine learning, pp.
1192‚Äì1199, 2008."
REFERENCES,0.3091655266757866,"Hai-Tao Yu.
Pt-ranking:
A benchmarking platform for neural learning-to-rank.
arXiv preprint
arXiv:2008.13368, 2020."
REFERENCES,0.31053351573187415,Published as a conference paper at ICLR 2022
REFERENCES,0.3119015047879617,"The appendix contains additional details, proofs, and experimental results. The glossary contains a conve-
nient reminder of our terminology (Appendix A). We provide a detailed related work section, explaining
the context for our work (Appendix B). Next, we give the statement of the quadratic triplets algorithm (Ap-
pendix E). Afterwards, we give additional theoretical details: more details on the backward mapping, along
with an extended discussion on the rankings problem. We continue with proofs of Theorem 4.1.1 and Theo-
rem 4.2 (Appendix F). Finally, we give more details on the experimental setup (Appendix H) and additional
experimental results including partial rankings (Appendix I, J)."
REFERENCES,0.31326949384404923,"A
GLOSSARY"
REFERENCES,0.3146374829001368,The glossary is given in Table 4 below.
REFERENCES,0.31600547195622436,"Symbol
DeÔ¨Ånition"
REFERENCES,0.3173734610123119,"X
feature space
Y
label metric space
dY
label metric
x1, x2, . . . , xn
unlabeled datapoints from X
y1, y2, . . . , yn
latent (unobserved) labels from Y
s1, s2, . . . , sm
labeling functions / sources
Œª1, Œª2, . . . , Œªm
output of labeling functions
n
number of data points
m
number of LFs
Œªa(i)
output of ath labeling function applied to ith sample xi
Œ∏a, Œ∏a,b
canonical parameters in model (1)
E[da(Œªa, y)], E[da(Œªa, Œªb)]
mean parameters in (1)
g
injective mapping g : Y ‚ÜíRd or {¬±1}d
œÅ
number of items in ranking setting
ea,b
E[g(Œªa)ig(Œªb)i]
Oa,b
P(g(Œªa)i = 1, g(Œªb)i = 1)
la
P(g(Œªa)i = 1)
SœÅ
symmetric group on {1, ..., œÅ}
œÄ
permutation ‚ààSœÅ
dœÑ(¬∑, ¬∑)
Kendall tau distance on permutations (Kendall, 1938)"
REFERENCES,0.31874145006839943,Table 4: Glossary of variables and symbols used in this paper.
REFERENCES,0.320109439124487,"B
RELATED WORK"
REFERENCES,0.32147742818057456,"Weak Supervision
Existing weak supervision frameworks, starting with Ratner et al. (2016), select a
particular model for the joint distribution among the sources and the latent true label, and then use the
properties of the distribution to select an algorithm to learn the parameters. In Ratner et al. (2016), a factor
model is used and a Gibbs-sampling based optimizer is the algorithm of choice. In Ratner et al. (2018), the
model is a discrete Markov random Ô¨Åeld (MRF), and in particular, an Ising model. The algorithm used to
learn the parameters solves a linear system obtained from a component of the inverse covariance matrix. In
Sala et al. (2019) and Fu et al. (2020), the requirement to use the inverse covariance matrix is removed, and
a set of systems among as few as three sources are used instead. These systems have closed-form solutions.
All of these models could be represented within our framework. The quadratic triplets idea is described, but"
REFERENCES,0.3228454172366621,Published as a conference paper at ICLR 2022
REFERENCES,0.32421340629274964,"not analyzed, in the appendix of Fu et al. (2020), applied to just the particular case of Ising models with
singleton potentials. Our work uses these ideas and their extensions to the general setting, and provides
theoretical analyses for the two sample applications we are interested in."
REFERENCES,0.32558139534883723,"All of these papers refer to weak supervision frameworks; all of them permit the use of various types of
information as labeling functions. For example, labeling functions might include crowdworkers Karger
et al. (2011), distant supervision Mintz et al. (2009), co-training Blum & Mitchell (1998), and many others.
Note that works like Dehghani et al. (2017) provide one type of weak supervision signal for rankings and
can be used as a high-quality labeling function in our framework for the rankings case. That is, our work
can integrate such sources, but does not directly compete with them."
REFERENCES,0.32694938440492477,"The idea of learning notions like source accuracy and correlations, despite the presence of the latent label, is
central to weak supervision. It has been shown up in other problems as well, such as crowdsourcing Dawid
& Skene (1979); Karger et al. (2011) or topic modeling Anandkumar et al. (2014). Early approaches use ex-
pectation maximization (EM), but in the last decade, a number of exciting approaches have been introduced
based on the method of moments. These include the tensor power method approach of Anandkumar et al.
(2014) and its follow-on work, the explicitly crowdsourcing setting of Joglekar et al. (2013; 2015), and the
graphical model learning procedures of Chaganty & Liang (2014); Raghunathan et al. (2016). Our approach
can be thought of as an extension of some of these approaches to the more general setting we consider."
REFERENCES,0.3283173734610123,"Comparison With Existing Label Models
There are several existing label models; these closely resemble
(1) under particular specializations. For example, one of the models used for binary labels in Ratner et al.
(2019); Varma et al. (2019) is the Ising model for Œª1, . . . , y ‚àà{¬±1}"
REFERENCES,0.32968536251709984,"p(Œª1, . . . , Œªm, y) = 1"
REFERENCES,0.33105335157318744,"Z exp
  m
X"
REFERENCES,0.332421340629275,"a=1
Œ∏aŒªay +
X"
REFERENCES,0.3337893296853625,"(a,b)‚ààE
Œ∏a,bŒªaŒªb + Œ∏Y y

.
(4)"
REFERENCES,0.33515731874145005,"A difference is that this is a joint model; it assumes y is part of the model and adds a singleton potential prior
term to it. Note that this model promotes agreement Œªay rather than penalizes disagreement ‚àíŒªadY(Œªa, y),
but this is nearly equivalent."
REFERENCES,0.33652530779753764,"Rankings and Permutations
One of our chief applications is to rankings. There are several classes of
distributions over permutations, including the Mallows model (Mallows, 1957) and the Plackett-Luce model
(Plackett, 1975). We are particularly concerned with the Mallows model in this work, as it can be extended
naturally to include labeling functions and the latent true label. Other generalizations include the gener-
alized Mallows model (Marden, 2014) and the Mallows block model (Busa-Fekete et al., 2019). These
generalizations, however, do not cover the heterogenous accuracy setting we are interested in."
REFERENCES,0.3378932968536252,"A common goal is to learn the parameters of the model (the single parameter Œ∏ in the conventional Mallows
model) and then to learn the central permutation that the samples are drawn from. A number of works in this
direction include Caragiannis et al. (2016); Mukherjee (2016); Busa-Fekete et al. (2019). Our work extends
results of this Ô¨Çavor to the generalization on the Mallows case that we consider. In order to learn the center
permutation, estimators like the Kemeny rule (the procedure we generalize in this work) are used. Studies
of the Kemeny rule include Kenyon-Mathieu & Schudy (2007); Caragiannis et al. (2016)."
REFERENCES,0.3392612859097127,"C
EXTENSION TO MIXED LABEL TYPES"
REFERENCES,0.34062927496580025,"In the body of the paper, we discussed models for tasks where only one label type is considered. As a simple
extension, we can operate with multiple label types. For example, this might include a classiÔ¨Åcation task
where weak label sources give their output as class labels and also might provide conÔ¨Ådence values; that is, a
pair of label types that include a discrete value and a continuous value. The main idea is to construct a Ô¨Ånite"
REFERENCES,0.34199726402188785,Published as a conference paper at ICLR 2022
REFERENCES,0.3433652530779754,"product space with individual label types. Suppose there are k possible label types, i.e. Y1, Y2, ¬∑ ¬∑ ¬∑ , Yk. We
construct the label space by the Cartesian product"
REFERENCES,0.3447332421340629,Y = Y1 √ó Y2 √ó ¬∑ ¬∑ ¬∑ √ó Yk.
REFERENCES,0.34610123119015046,All that is left is to deÔ¨Åne the distance:
REFERENCES,0.34746922024623805,"d2
Y(y1, y2) = k
X"
REFERENCES,0.3488372093023256,"i=1
d2
Yi(proji(y1), proji(y2)),"
REFERENCES,0.35020519835841313,"where proji is projection onto the i-th factor space. Then, using this combination, we extend the exponential
family model (1), yielding"
REFERENCES,0.35157318741450067,"p(Œª1, . . . , Œªm|y) = 1"
REFERENCES,0.35294117647058826,"Z exp
  m
X a=1 k
X"
REFERENCES,0.3543091655266758,"i=1
‚àíŒ∏(i)
a dYi(proji(Œªa), proji(y)) +
X"
REFERENCES,0.35567715458276333,"(a,b)‚ààE k
X"
REFERENCES,0.35704514363885087,"i=1
‚àíŒ∏(i)
a,bdYi(proji(Œªa), proji(Œªb))

."
REFERENCES,0.35841313269493846,"We can learn the parameters Œ∏(i)
a , Œ∏(i)
a,b by Algorithm 1 using the same approach. Similarly, the inference
procedure (3) can be extended to"
REFERENCES,0.359781121751026,"ÀÜyj = arg min
z‚ààY m
X a=1 k
X"
REFERENCES,0.36114911080711354,"i=1
dÀÜŒ∏(i)
a (proji(Œªa(j)), proji(z))."
REFERENCES,0.3625170998632011,"There are two additional aspects worth mentioning. First, the user may wish to consider the scale of distances
in each label space, since the scale of one of the factor spaces‚Äô distance might be dominant. To weight each
label space properly, we can normalize each label space‚Äôs distance. Second, we may wish to consider the
abstention symbol as an additional element in each space. This permits LFs to output one or another type of
label without necessarily emitting a full vector. This can also be handled; the underlying algorithm is simply
modiÔ¨Åed to permit abstains as in Fu et al. (2020)."
REFERENCES,0.36388508891928867,"D
UNIVERSAL LABEL MODEL FOR ISOTROPIC GAUSSIAN EMBEDDING"
REFERENCES,0.3652530779753762,"We illustrate the isotropic Gaussian version of the label model. While simple, it captures all of the challenges
involved in label model learning, and it performs well in practice. The steps are shown in Algorithm 3."
REFERENCES,0.36662106703146374,"Why does Algorithm 3 obtain the estimates of Œ∏ without observing the true label y? To see this, Ô¨Årst, note
that post-embedding, the model we are working with is given by"
REFERENCES,0.3679890560875513,"p(Œª1, . . . , Œªm|y) = 1"
REFERENCES,0.3693570451436389,"Z exp
  m
X"
REFERENCES,0.3707250341997264,"a=1
‚àíŒ∏a‚à•g(Œªa) ‚àíg(y)‚à•2 +
X"
REFERENCES,0.37209302325581395,"(a,b)‚ààE
‚àíŒ∏a,b‚à•g(Œªa) ‚àíg(Œªb)‚à•2
."
REFERENCES,0.3734610123119015,"If the embedding is a bijection, the resulting model is indeed is a multivariate Gaussian. Note that the term
‚Äúisotropic‚Äù here refers to the fact that for d > 1, the covariance term for the random vector Œªa is a multiple
of I."
REFERENCES,0.3748290013679891,"Now, observe that if (a, b) Ã∏‚ààE, we have that E

‚à•Œªa ‚àíŒªb‚à•2
= E

‚à•(Œªa ‚àíy) ‚àí(Œªb ‚àíy)‚à•2
=
E

‚à•Œªa ‚àíy‚à•2
+E

‚à•Œªb ‚àíy‚à•2
. Note that we can estimate the left-hand side E

‚à•Œªa ‚àíŒªb‚à•2
from samples,"
REFERENCES,0.3761969904240766,Published as a conference paper at ICLR 2022
REFERENCES,0.37756497948016415,Algorithm 3: Isotropic Gaussian Label Model Learning
REFERENCES,0.3789329685362517,"1: Input: Output of labeling functions Œªa(i), correlation set E
2: for a ‚àà{1, 2, . . . , m} do
3:
for b ‚àà{1, 2, . . . , m} \ a do
4:
Estimate Correlations:‚àÄi, j, ÀÜE

d(Œªa, Œªb)

‚Üê1"
REFERENCES,0.3803009575923393,"n
Pn
t=1 d(Œªa(i), Œªb(i))
5:
end for
6:
Estimate Accuracy: Pick b, c : (a, b) Ã∏‚ààE, (a, c) Ã∏‚ààE, (b, c) Ã∏‚ààE
ÀÜE [d(Œªa, y)] ‚Üê1/2(E

d(Œªa, Œªb)

+ E [d(Œªa, Œªc)] ‚àíE

d(Œªb, Œªc)

)"
REFERENCES,0.3816689466484268,"7:
Form estimated covariance matrix ÀÜŒ£ from accuracies and correlations; Compute ÀÜŒ∏ ‚ÜêÀÜŒ£‚àí1"
REFERENCES,0.38303693570451436,"8: end for
9: return ÀÜŒ∏a, ÀÜŒ∏a,b"
REFERENCES,0.3844049247606019,"while the right-hand side contains two of our accuracy terms. We can then form two more equations of this
type (involving the pairs a, c and b, c) and solve the resulting linear system. Concretely, we have that"
REFERENCES,0.3857729138166895,"E

‚à•g(Œªa) ‚àíg(Œªb)‚à•2
= E

‚à•g(Œªa) ‚àíy‚à•2
+ E

‚à•g(Œªb) ‚àíy‚à•2"
REFERENCES,0.387140902872777,"E

‚à•g(Œªa) ‚àíg(Œªc)‚à•2
= E

‚à•g(Œªa) ‚àíy‚à•2
+ E

‚à•g(Œªc) ‚àíy‚à•2"
REFERENCES,0.38850889192886456,"E

‚à•g(Œªb) ‚àíg(Œªc)‚à•2
= E

‚à•g(Œªb) ‚àíy‚à•2
+ E

‚à•g(Œªc) ‚àíy‚à•2
."
REFERENCES,0.3898768809849521,"To obtain the estimate of E

‚à•g(Œªa) ‚àíy‚à•2
, we add the Ô¨Årst two equations, subtract the third, and divide
by two.This produces the accuracy estimation step in Algorithm 3. To obtain the estimate of the canonical
parameters Œ∏, it is sufÔ¨Åcient to invert our estimate of the covariance matrix. In practice, note that we need not
actually compute an embedding; we can directly work with the original metric space distances by implicitly
assuming that there exists an isometric embedding."
REFERENCES,0.3912448700410397,"E
ADDITIONAL ALGORITHMIC DETAILS"
REFERENCES,0.39261285909712723,"Quadratic Triplets
We give the full statement of Algorithm 4, which is the accuracy estimation algorithms
for the case Im(g) = {¬±1}d."
REFERENCES,0.39398084815321477,"Inference simpliÔ¨Åcation in R
For the simpliÔ¨Åcation of inference in the isotropic Gaussian embedding, we
do not even need to recover the canonical parameters; it is enough to use the mean parameters estimated
in the label model learning step. For example, if d = 1, we can write these accuracy mean parameters as
ÀÜE

g(Œªi)g(y)

and put them into a vector ÀÜE [Œ£]Œõy. The correlations can be placed into the sample covariance
matrix ÀÜŒ£g(Œª1),...,g((Œªm). Then,"
REFERENCES,0.3953488372093023,"ÀÜy(i) := E

y|Œª1(i), . . . Œªm(i)

= ÀÜŒ£T
Œõy ÀÜŒ£‚àí1
Œõ [Œª1(i), . . . , Œªm(i)].
(5)"
REFERENCES,0.3967168262653899,This is simply the mean of a conditional (scalar) Gaussian.
REFERENCES,0.39808481532147744,"F
ADDITIONAL THEORY DETAILS"
REFERENCES,0.399452804377565,"We discuss further details for several theoretical notions. The Ô¨Årst provides more details on how to obtain the
canonical accuracy parameters Œ∏a from the mean parameters E [dœÑ(Œªa, y)] in the rankings case. The second
involves a discussion of learning to rank problems, with additional details on the weighted estimator (3)."
REFERENCES,0.4008207934336525,Published as a conference paper at ICLR 2022
REFERENCES,0.4021887824897401,Algorithm 4: QUADRATICTRIPLETS
REFERENCES,0.40355677154582764,"Input: Estimates Oa,b, Oa,c, Ob,c, ‚Ñìa, ‚Ñìb, ‚Ñìc, prior p, index i
for y in Y do"
REFERENCES,0.4049247606019152,"Obtain probability p‚Ä≤ = P(Y = y) from prior p
Set Œ≤ ‚Üê(Oa,b(1 ‚àíp‚Ä≤) + (‚Ñìa ‚àípz)‚Ñìb)/(p‚Ä≤z ‚àíp‚Ä≤‚Ñìa)
Set Œ≥ ‚Üê(Oa,c(1 ‚àíp‚Ä≤) + (‚Ñìa ‚àípz)‚Ñìc)/(p‚Ä≤z ‚àíp‚Ä≤‚Ñìa)
Solve quadratic (pŒ≤Œ≥ + ‚Ñìb‚Ñìc ‚àípŒ≤‚Ñìc ‚àípŒ≥‚Ñìb ‚àíOa,b(1 ‚àíp))(p‚Ä≤Œ± ‚àíp‚Ä≤‚Ñìa)2 = 0 in z"
REFERENCES,0.4062927496580027,"ÀÜP(g(Œªa)i|Y = y) ‚Üêz
ÀÜP(g(Œªb)i|Y = y) ‚Üê(Oa,b(1 ‚àíp‚Ä≤) + (‚Ñìa ‚àíp ÀÜP(g(Œªa)i|Y = y))‚Ñìb)/(p‚Ä≤ ÀÜP(g(Œªa)i|Y = y) ‚àíp‚Ä≤‚Ñìa)
ÀÜP(g(Œªc)i|Y = y) ‚Üê(Oa,c(1 ‚àíp‚Ä≤) + (‚Ñìa ‚àíp ÀÜP(g(Œªa)i|Y = y))‚Ñìc)/(p‚Ä≤ ÀÜP(g(Œªa)i|Y = y) ‚àíp‚Ä≤‚Ñìa)
end for
return Accuracies ÀÜP(g(Œªa)i|Y = y), ÀÜP(g(Œªb)i|Y = y), ÀÜP(g(Œªc)i|Y = y)"
REFERENCES,0.4076607387140903,"More on the backward mapping for rankings
We note, Ô¨Årst, that the backward mapping is quite simple
for the Gaussian cases in Rd: it just involves inverting the mean parameters. As an intuitive example,
note that the canonical parameter in the interior of the multivariate Gaussian density is Œ£‚àí1, the inverse
covariance matrix. The more challenging aspect is to deal with the discrete setting, and, in particular, its
application to rankings. We do so below."
REFERENCES,0.40902872777017785,"We describe how we can recover the canonical accuracy parameter Œ∏a for a label function Œªa given accuracy
estimates P(g(Œªa)i = 1|Y = y) for all y ‚ààY. By equation (1), the marginal distribution of Œªa is speciÔ¨Åed
by"
REFERENCES,0.4103967168262654,p(Œªa) = 1
REFERENCES,0.4117647058823529,"Z exp(‚àíg(Œªa)T Œ∏ag(y)).
(6)"
REFERENCES,0.4131326949384405,"Since this is an exponential model, it follows from Fligner & Verducci (1986) that"
REFERENCES,0.41450068399452805,E[D] = d log(M(t)) dt
REFERENCES,0.4158686730506156,"t=‚àíŒ∏a
,
(7)"
REFERENCES,0.4172366621067031,where D = P
REFERENCES,0.4186046511627907,"i 1{g(Œªa)i Ã∏= g(y)i} and M(t) is the moment generating function of D under (5). E[D] can
be easily estimated from the accuracy parameters obtained from the triplet algorithm, and the inverse of (6)
can then be solved for. For instance, in the rankings case, it can be shown (Fligner & Verducci, 1986) that"
REFERENCES,0.41997264021887826,M(‚àíŒ∏) = 1
REFERENCES,0.4213406292749658,œÅ!Z(Œ∏).
REFERENCES,0.42270861833105333,"Additionally, we have that the partition function satisÔ¨Åes (Chierichetti et al., 2014)"
REFERENCES,0.4240766073871409,"Z(Œ∏) =
Y j‚â§k"
REFERENCES,0.42544459644322846,1 ‚àíeŒ∏j
REFERENCES,0.426812585499316,1 ‚àíeŒ∏ .
REFERENCES,0.42818057455540354,It follows that
REFERENCES,0.42954856361149113,M(t) = 1 k! Y j‚â§k
REFERENCES,0.43091655266757867,1 ‚àíeŒ∏j
REFERENCES,0.4322845417236662,"1 ‚àíeŒ∏ .
(8)"
REFERENCES,0.43365253077975374,"Using this, we can then solve for (6) numerically."
REFERENCES,0.43502051983584133,"Rank aggregation and the weighted Kemeny estimator
Next, we provide some additional details on the
learning to rank problem. The model (1) without correlations can be written"
REFERENCES,0.4363885088919289,"p(Œª1, . . . , Œªm|y) = 1 Z exp X"
REFERENCES,0.4377564979480164,"a
‚àíŒ∏adœÑ(Œªa, y) ! .
(9)"
REFERENCES,0.43912448700410395,Published as a conference paper at ICLR 2022
REFERENCES,0.44049247606019154,"Thus, if we only have one labeling function, we obtain the Mallows model (Mallows, 1957) for permutations,
whose standard form is p(Œª1|y) = 1/Z exp(‚àíŒ∏1dœÑ(Œª1, y)). Permutation learning problems often use the
Mallows model and its relatives. The permutation y (called the center) is Ô¨Åxed and n samples of Œª1 are
drawn from the model. The goal is to (1) estimate the parameter Œ∏1 and (2) estimate the center y. This neatly
mirrors our setting, where (1) is label model learning and (2) is the inference procedure."
REFERENCES,0.4418604651162791,"However, our problem is harder in two ways. While we do have n samples from each marginal model with
parameter Œ∏i, these are for different centers y1, . . . , yn‚Äîso we cannot aggregate them or directly estimate
Œ∏i. That is, for LF a, we get to observe Œªa(1), Œªa(2), . . . , Œªa(n). However, these all come from different
conditional models P(¬∑|yi), with a different yi for each draw. In the uninteresting case where the yi‚Äôs are
identical, we obtain the standard setting. On the other hand, we do get to observe m views (from the LFs)
of the same permutation yi. But, unlike in standard rank aggregation, these do not come from the same
model‚Äîthe Œ∏a accuracy parameters differ in (9). However, if Œ∏a is identical (same accuracy) for all LFs, we
get back to the standard case. Thus we can recover the standard permutation learning problem in the special
cases of identical labels or identical accuracies‚Äîbut such assumptions are unlikely to hold in practice."
REFERENCES,0.4432284541723666,"Note that our inference procedure (3) is the weighted version of the standard Kemeny procedure used for
rank aggregation. Observe that this is the maximum likelihood estimator on the model (1), specialized
to permutations. This is nearly immediate: maximizing the linear combination (where the parameters are
weights) produces the smallest negative term on the inside of the exponent above."
REFERENCES,0.44459644322845415,"Next, we note that it is possible to show that the sample complexity of learning the permutation y using this
estimator is still Œò(log(m/Œµ)) by using the pairwise majority result in Caragiannis et al. (2016)."
REFERENCES,0.44596443228454175,"Finally, a brief comment on computational complexity: it is known that Ô¨Ånding the minimizer of the Kemeny
rule (or our weighted variant) is NP-hard (Davenport & Kalagnanam, 2004). However, there are PTAS
available for it (Kenyon-Mathieu & Schudy, 2007). In practice, our permutations are likely to be reasonably
short (for example, in our experiments, we typically use length 5) so that we can directly perform the
minimization. In cases with longer permutations, we can rely on the PTAS."
REFERENCES,0.4473324213406293,"G
PROOF OF THEOREMS"
REFERENCES,0.4487004103967168,"Next we give the proofs of the proposition and our two theorems, restated for convenience."
REFERENCES,0.45006839945280436,"G.1
DISTORTION BOUND"
REFERENCES,0.45143638850889195,"Our result that captures the impact of distortion on parameter error is
Theorem 4.3. The inconsistency in estimating Œ∏ is bounded as ‚à•Œ∏ ‚àíŒ∏‚Ä≤‚à•‚â§Œµ‚à•¬µ‚à•/emin."
REFERENCES,0.4528043775649795,"Before we begin the proof, we restate, in greater detail, some of our notation. We write pŒ∏;dY for the true
model"
REFERENCES,0.454172366621067,"pŒ∏;dY(Œª1, . . . , Œªm|y) = 1"
REFERENCES,0.45554035567715456,"Z exp
  m
X"
REFERENCES,0.45690834473324216,"a=1
‚àíŒ∏adY(Œªa, y) +
X"
REFERENCES,0.4582763337893297,"(a,b)‚ààE
‚àíŒ∏a,bdY(Œªa, Œªb)
"
REFERENCES,0.45964432284541723,and pŒ∏‚Ä≤;dg for models that rely on distances in the embedding space
REFERENCES,0.46101231190150477,"pŒ∏;dg(Œª1, . . . , Œªm|y) = 1"
REFERENCES,0.46238030095759236,"Z exp
  m
X"
REFERENCES,0.4637482900136799,"a=1
‚àíŒ∏‚Ä≤
adg(g(Œªa), g(y)) +
X"
REFERENCES,0.46511627906976744,"(a,b)‚ààE
‚àíŒ∏‚Ä≤
a,bdg(g(Œªa), g(Œªb))

."
REFERENCES,0.466484268125855,"We also write Œ∏ to be the vector of the canonical parameters Œ∏a and Œ∏a,b in pŒ∏;dY and Œ∏‚Ä≤ to be its counter-
part for pŒ∏‚Ä≤;dg. Let ¬µ be the vector of mean parameters whose terms are E[dY(Œªa, y)] and E[dY(Œªa, Œªb)]."
REFERENCES,0.46785225718194257,Published as a conference paper at ICLR 2022
REFERENCES,0.4692202462380301,"Similarly, we write ¬µ‚Ä≤ for the version given by pŒ∏‚Ä≤;dg. Let Œò be a subspace so that Œ∏, Œ∏‚Ä≤ ‚ààŒò. Since our
exponential family is minimal, we have that the log partition function A(ÀúŒ∏) has the property that ‚àá2A(ÀúŒ∏) is
the covariance matrix and is positive deÔ¨Ånite. Suppose that the smallest eigenvalue of ‚àá2A(ÀúŒ∏) for ÀúŒ∏ ‚ààŒò is
emin."
REFERENCES,0.47058823529411764,"For our embedding, suppose we Ô¨Årst normalize (ie, divide by a constant) our embedding function so that"
REFERENCES,0.4719562243502052,"dg(g(y), g(y‚Ä≤))"
REFERENCES,0.47332421340629277,"dY(y, y‚Ä≤)
‚â§1"
REFERENCES,0.4746922024623803,"for all y, y‚Ä≤ ‚ààY √ó Y. In other words, our embedding function g never expands the distances in the original
metric space, but, of course, it may compress them. The distortion measures how bad the compression can
be, Let Œµ be the smallest value so that"
REFERENCES,0.47606019151846785,"1 ‚àíŒµ ‚â§dg(g(y), g(y‚Ä≤))"
REFERENCES,0.4774281805745554,"dY(y, y‚Ä≤)
‚â§1"
REFERENCES,0.478796169630643,"for all y, y‚Ä≤ ‚ààY √ó Y. If g is isometric, then we obtain Œµ = 0. On the other hand, as Œµ approaches 1, the
amount of compression can be arbitrarily bad."
REFERENCES,0.4801641586867305,Proof. We use Lemma 8 from Fu et al. (2020). It states that
REFERENCES,0.48153214774281805,"‚à•Œ∏ ‚àíŒ∏‚Ä≤‚à•‚â§
1
emin
‚à•¬µ ‚àí¬µ‚Ä≤‚à•"
REFERENCES,0.4829001367989056,"‚â§
Œµ
emin
‚à•¬µ‚à•."
REFERENCES,0.4842681258549932,"To see the latter, we note that ‚à•¬µ ‚àí¬µ‚Ä≤‚à•‚â§‚à•¬µ(1 ‚àí¬µ‚Ä≤"
REFERENCES,0.4856361149110807,"¬µ )‚à•‚â§‚à•¬µ‚à•(1 ‚àí(1 ‚àíŒµ)) = ‚à•¬µ‚à•Œµ, where ¬µ‚Ä≤"
REFERENCES,0.48700410396716826,"¬µ is the
element-wise ratios, and where we applied our distortion bound."
REFERENCES,0.4883720930232558,"G.2
BINARY HYPERCUBE CASE"
REFERENCES,0.4897400820793434,"Theorem 4.1. For any Œ¥ > 0, for some y1 and y2 with known class probabilities p = P(Y = y1), the
quadratic triplet method recovers Œ±i = P(g(Œªa)i = 1|Y = y), Œ≤i = P(g(Œªb)i = 1|Y = y), Œ≥i =
P(g(Œªc)i = 1|Y = y) up to error O(( ln(2d2/Œ¥)"
N,0.4911080711354309,"2n
)1/4) with probability at least 1 ‚àíŒ¥ for any conditionally
independent label functions Œªa, Œªb, Œªc and for all i ‚àà[d]."
N,0.49247606019151846,Proof. DeÔ¨Åne
N,0.493844049247606,"¬µa
i =

P(g(Œªa)i = 1|Y = y)
P(g(Œªa)i = 1|Y Ã∏= y)
P(g(Œªa)i = ‚àí1|Y = y)
P(g(Œªa)i = ‚àí1|Y Ã∏= y)"
N,0.4952120383036936,"
,
P =

P(Y = y)
0
0
P(Y Ã∏= y) 
, and"
N,0.49658002735978113,"Oab
i
=

P(g(Œªa)i = 1, g(Œªb)i = 1)
P(g(Œªa)i = 1, g(Œªb)i = ‚àí1)
P(g(Œªa)i = ‚àí1, g(Œªb)i = 1)
P(g(Œªa)i = ‚àí1, g(Œªb)i = ‚àí1) 
."
N,0.49794801641586867,"By conditional independence, we have that"
N,0.4993160054719562,"¬µa
i P(¬µb
i)T = Oab
i .
(10)"
N,0.5006839945280438,Note that we can express
N,0.5020519835841313,P(g(Œªa)i = 1|Y Ã∏= y) = P(g(Œªa)i = 1)
N,0.5034199726402189,"P(Y Ã∏= y)
‚àíP(g(Œªa)i = 1|Y = y)P(Y = y)"
N,0.5047879616963065,"P(Y Ã∏= y)
."
N,0.506155950752394,Published as a conference paper at ICLR 2022
N,0.5075239398084815,"We can therefore rewrite the top row of ¬µa
i as [Œ±i, qa
i ‚àírŒ±i] where qa
i = P (g(Œªa)i=1)"
N,0.5088919288645691,"P (Y Ã∏=y)
and r = P (Y =y)"
N,0.5102599179206566,"P (Y Ã∏=y). After
estimating the entries of Oi‚Äôs and qi‚Äôs, we consider the top-left entry of (10) for every pair of a, b and c, to
get the following system of equations"
N,0.5116279069767442,"tŒ±iŒ≤i + ÀÜqa
i ÀÜqb
i ‚àíÀÜqa
i rŒ≤i ‚àíÀÜqb
i rŒ±i =
ÀÜOab
i
1 ‚àíp,"
N,0.5129958960328317,"tŒ±iŒ≥i + ÀÜqa
i ÀÜqc
i ‚àíÀÜqa
i rŒ≥i ‚àíÀÜqc
i rŒ±i =
ÀÜOac
i
1 ‚àíp,"
N,0.5143638850889193,"tŒ≤iŒ≥i + ÀÜqb
i ÀÜqc
i ‚àíÀÜqb
i rŒ≥i ‚àíÀÜqc
i rŒ≤i =
ÀÜObc
i
1 ‚àíp,"
N,0.5157318741450069,"where Œ≤i and Œ≥i are the top-left entries of ¬µb
i and ¬µc
i, respectively, p = P(Y = y), and t =
p
(1‚àíp)2 ."
N,0.5170998632010944,"For ease of notation, we write ÀÜqa
i as ÀÜqa and so on. Rearranging the Ô¨Årst and third equations gives us expres-
sions for Œ± and Œ≥ in terms of Œ≤. Œ± ="
N,0.518467852257182,"ÀÜ
Oab
1‚àíp + ÀÜqarŒ≤ ‚àíÀÜqaÀÜqb"
N,0.5198358413132695,"tŒ≤ ‚àíÀÜqbr
, Œ≥ ="
N,0.521203830369357,"ÀÜ
Obc
1‚àíp + ÀÜqcrŒ≤ ‚àíÀÜqbÀÜqc"
N,0.5225718194254446,"tŒ≤ ‚àíÀÜqbr
."
N,0.5239398084815321,Substituting these expressions into the second equation of the system gives
N,0.5253077975376197,"t
ÀÜqarŒ≤ +
ÀÜ
Oab
1‚àíp ‚àíÀÜqaÀÜqb
tŒ≤ ‚àíÀÜqbr
¬∑
ÀÜqcrŒ≤ +
ÀÜ
Obc
1‚àíp ‚àíÀÜqbÀÜqc
tŒ≤ ‚àíÀÜqbr
+ ÀÜqaÀÜqc ‚àíÀÜqar
ÀÜqcrŒ≤ +
ÀÜ
Obc
1‚àíp ‚àíÀÜqbÀÜqc
tŒ≤ ‚àíÀÜqbr
‚àíÀÜqcr
ÀÜqarŒ≤ +
ÀÜ
Oab
1‚àíp ‚àíÀÜqaÀÜqb
tŒ≤ ‚àíÀÜqbr
=
ÀÜOac
1 ‚àíp."
N,0.5266757865937073,We can multiply the equation by (tŒ≤ ‚àíÀÜqbr)2 to get
N,0.5280437756497948,"t(ÀÜqarŒ≤ +
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb) ¬∑ (ÀÜqcrŒ≤ +
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc) + ÀÜqaÀÜqc((tŒ≤ ‚àíÀÜqbr)2)"
N,0.5294117647058824,"‚àíÀÜqar(ÀÜqcrŒ≤ +
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc) ¬∑ (tŒ≤ ‚àíÀÜqbr) ‚àíÀÜqcr(ÀÜqarŒ≤ +
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb) ¬∑ (tŒ≤ ‚àíÀÜqbr)"
N,0.53077975376197,"=
ÀÜOac
1 ‚àíp ¬∑ (tŒ≤ ‚àíÀÜqbr)2"
N,0.5321477428180574,"‚áít

ÀÜqaÀÜqcr2Œ≤2 + (
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)ÀÜqcrŒ≤ + (
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)ÀÜqarŒ≤ + (
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)(
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)
"
N,0.533515731874145,+ÀÜqaÀÜqc
N,0.5348837209302325,"
t2Œ≤2 ‚àí2ÀÜqbrtŒ≤ + ÀÜq2
br2
"
N,0.5362517099863201,"‚àíÀÜqar

ÀÜqcrtŒ≤2 + (
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)tŒ≤ ‚àíÀÜqbÀÜqcr2Œ≤ ‚àíÀÜqbr(
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)
"
N,0.5376196990424077,"‚àíÀÜqcr

ÀÜqartŒ≤2 + (
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)tŒ≤ ‚àíÀÜqaÀÜqbr2Œ≤ ‚àíÀÜqbr(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)
"
N,0.5389876880984952,"=
ÀÜOac
1 ‚àíp ¬∑

t2Œ≤2 ‚àí2ÀÜqbrtŒ≤ + ÀÜq2
br2
"
N,0.5403556771545828,Published as a conference paper at ICLR 2022
N,0.5417236662106704,"‚áíŒ≤2

ÀÜqaÀÜqct2 ‚àíÀÜqaÀÜqcr2e ‚àí
ÀÜOac
1 ‚àíp ¬∑ t2
"
N,0.5430916552667578,"+Œ≤

t

(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)ÀÜqcr + (
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)ÀÜqar

‚àí2ÀÜqaÀÜqcÀÜqbrt"
N,0.5444596443228454,"‚àíÀÜqar

(
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)t ‚àíÀÜqbÀÜqcr2

‚àíÀÜqcr

(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)t ‚àíÀÜqaÀÜqbr2

+
ÀÜOac
1 ‚àíp ¬∑

2ÀÜqbrt
"
N,0.5458276333789329,"+

t(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)(
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc) + ÀÜqaÀÜqcÀÜq2
br2"
N,0.5471956224350205,"+ÀÜqaÀÜqbr2(
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc) + ÀÜqbÀÜqcr2(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb) ‚àí
ÀÜOac
1 ‚àíp ¬∑ ÀÜq2
bt2

= 0."
N,0.5485636114911081,"The only sources of error are from estimating c‚Äôs and O‚Äôs. Let Œµc‚Ä≤ denote the error for the c‚Äôs. Let ŒµO denote
the error from O‚Äôs. Applying the quadratic formula we obtain Œ≤ = (‚àíb‚Ä≤ ¬±
p"
N,0.5499316005471956,"(b‚Ä≤)2 ‚àí4a‚Ä≤c‚Ä≤)/(2a‚Ä≤) where the
coefÔ¨Åcients are"
N,0.5512995896032832,"a‚Ä≤ = ÀÜqaÀÜqct2 ‚àíÀÜqaÀÜqcr2t ‚àí
ÀÜOac
1 ‚àíp ¬∑ t2"
N,0.5526675786593708,"b‚Ä≤ = t

(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)ÀÜqcr + (
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)ÀÜqar

‚àí2ÀÜqaÀÜqcÀÜqbrt
"
N,0.5540355677154583,"‚àíÀÜqar

(
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)t ‚àíÀÜqbÀÜqcr2

‚àíÀÜqcr

(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)t ‚àíÀÜqaÀÜqbr2
"
N,0.5554035567715458,"+
ÀÜOac
1 ‚àíp ¬∑

2ÀÜqbrt
"
N,0.5567715458276333,"c‚Ä≤ = t(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb)(
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc) + ÀÜqaÀÜqcÀÜq2
br2 + ÀÜqaÀÜqbr2(
ÀÜObc
1 ‚àíp ‚àíÀÜqbÀÜqc)"
N,0.5581395348837209,"+ÀÜqbÀÜqcr2(
ÀÜOab
1 ‚àíp ‚àíÀÜqaÀÜqb) ‚àí
ÀÜOac
1 ‚àíp ¬∑ ÀÜq2
br2."
N,0.5595075239398085,"Let Œµa‚Ä≤, Œµb‚Ä≤, Œµc‚Ä≤ denote the error for each coefÔ¨Åcient. That is, Œµa‚Ä≤ = |(a‚Ä≤)‚àó‚àía‚Ä≤|, where (a‚Ä≤)‚àóis the population-
level coefÔ¨Åcient, and similarly for b‚Ä≤, c‚Ä≤. Let Œµc and ŒµO indicate the estimation error for the c and O terms.
Then"
N,0.560875512995896,"Œµa‚Ä≤ = O

t2pŒµc +
t2"
N,0.5622435020519836,"1 ‚àípŒµO 
,"
N,0.5636114911080712,"Œµb‚Ä≤ = O

rt
1 ‚àíp (Œµc + ŒµO)

,"
N,0.5649794801641587,"Œµc‚Ä≤ = O

t
(1 ‚àíp)2 +
r2"
N,0.5663474692202463,"1 ‚àíp + r2

Œµc +

t2"
N,0.5677154582763337,"(1 ‚àíp)2 +
r2 1 ‚àíp 
ŒµO 
."
N,0.5690834473324213,"Because d and e are functions of p, if we ignore the dependence on p, we get that"
N,0.5704514363885089,Œµa‚Ä≤ = Œµb‚Ä≤ = Œµc‚Ä≤ = O(Œµc + ŒµO).
N,0.5718194254445964,"Furthermore,
Œµ(b‚Ä≤)2 = O(Œµb), Œµa‚Ä≤c‚Ä≤ = O(Œµa + Œµc)."
N,0.573187414500684,Published as a conference paper at ICLR 2022
N,0.5745554035567716,"It follows that
ŒµŒ≤ = O(‚àöŒµc‚Ä≤ + ŒµO)."
N,0.5759233926128591,"Next, note that O and c are both the averages of indicator variables, where the ci‚Äôs involve P(g(Œªa)i = 1 and
the Oab
i ‚Äôs upper-left corners compute P(g(Œªa)i = 1, g(Œªb)i = 1). Thu we can apply Hoeffding‚Äôs inequality
and combine this with the union bound to bound the above terms. We have with probability at least 1 ‚àíŒ¥ 2,"
N,0.5772913816689467,"Œµci ‚â§
q"
N,0.5786593707250342,log(2d/Œ¥)
N,0.5800273597811217,"2n
and similarly with probability at least 1 ‚àíŒ¥"
N,0.5813953488372093,"2, ŒµOi ‚â§
q"
N,0.5827633378932968,log(2d/Œ¥)
N,0.5841313269493844,"2n
for all i ‚àà[d]. It follows"
N,0.585499316005472,"that with probability at least 1 ‚àíŒ¥, ŒµŒ± = ŒµŒ≤ = ŒµŒ≥ = O(( log(2d/Œ¥)"
N,0.5868673050615595,"2n
)1/4)."
N,0.5882352941176471,"We can now prove the main theorem in the rankings case.
Corollary 4.1.1. For any Œ¥ > 0, U > 0, a prior over y1 and y2 with known class probability p, and
using Algorithm 1 and Algorithm 4, for any conditionally independent triplet Œªa, Œªb, Œªc, with parameters
U > Œ∏a, Œ∏b, Œ∏c > 4 ln(2), we can recover Œ∏a, Œ∏b, Œ∏c up to error O(g‚àí1
2 (Œ∏ + (log(2œÅ2)/(2Œ¥n))1/4) ‚àíŒ∏) with
probability at least 1 ‚àíŒ¥, where g2(U) = (‚àíœÅe‚àíU)/((1 ‚àíe‚àíU)2) + PœÅ
j=1(j2e‚àíUj)/((1 ‚àíe‚àíUj)2)."
N,0.5896032831737346,"Proof. Consider a pair of items (a, b). Without loss of generality, suppose a ‚â∫y1 b and a ‚âªy2 b. DeÔ¨Åne
Œ±a,b = P(a ‚â∫Œªa b|y1), Œ±‚Ä≤
a,b = P(a ‚âªŒªa b|y2) then our estimate for P(Œªa
(a,b) ‚àºY(a,b)) where Œªa
(a,b) ‚àº
Y(a,b) denotes the event that label function i ranks (a, b) correctly would be"
N,0.5909712722298222,"ÀÜP(Œªa
(a,b) ‚àºY(a,b)) = pÀÜŒ±a,b + (1 ‚àíp)(1 ‚àíÀÜŒ±‚Ä≤a,b)"
N,0.5923392612859097,"which has error O(œµŒ±). Then, note that E[d(Œªa, Y )] = P"
N,0.5937072503419972,"a,b P(Œªa
(a,b) ‚àºY(a,b)). Therefore, we can compute"
N,0.5950752393980848,"the estimate ÀÜE[d(Œªa, Y )] = P"
N,0.5964432284541724,"a,b ÀÜE[Œªi ‚àºY ]a,b which has error O(
 œÅ
2

( ln(6)/Œ¥"
N,0.5978112175102599,"2n
)1/4)."
N,0.5991792065663475,Recall from (7) we have that
N,0.600547195622435,EŒ∏[D] = d[log(M(t))] dt
N,0.6019151846785226,"t=‚àíŒ∏
,"
N,0.6032831737346102,"where M(t) is the moment generating function, and recall from (8) that"
N,0.6046511627906976,M(t) = 1 k! Y j‚â§k
N,0.6060191518467852,1 ‚àíeŒ∏j
N,0.6073871409028728,1 ‚àíeŒ∏ .
N,0.6087551299589603,It follows that
N,0.6101231190150479,"EŒ∏[D] =
ke‚àíŒ∏"
N,0.6114911080711354,"1 ‚àíe‚àíŒ∏ ‚àí
X j‚â§k je‚àíŒ∏j"
N,0.612859097127223,"1 ‚àíe‚àíŒ∏j
(11) ‚áíd"
N,0.6142270861833106,"dŒ∏EŒ∏[D] =
‚àíke‚àíŒ∏"
N,0.615595075239398,"(1 ‚àíe‚àíŒ∏)2 +
X j‚â§k"
N,0.6169630642954856,j2e‚àíŒ∏j
N,0.6183310533515732,(1 ‚àíe‚àíŒ∏j)2 .
N,0.6196990424076607,"Let gk(Œ∏) =
‚àíke‚àíŒ∏"
N,0.6210670314637483,(1‚àíe‚àíŒ∏)2 + P
N,0.6224350205198358,"j‚â§k
j2e‚àíŒ∏j"
N,0.6238030095759234,"(1‚àíe‚àíŒ∏j)2 . By the lemma below, gk is non positive and increasing in Œ∏ for
Œ∏ > 0. This means we can numerically compute the inverse function of (7), with the stated error."
N,0.625170998632011,"Lemma G.1. gk is non-positive and increasing in Œ∏ for Œ∏ > 4 ln(2). Additionally, gk is decreasing in k."
N,0.6265389876880985,"Proof. We Ô¨Årst show non-positivity. For this, it is sufÔ¨Åcient to show"
N,0.627906976744186,j2e‚àíŒ∏j
N,0.6292749658002736,"(1 ‚àíe‚àíŒ∏j)2 ‚â§
e‚àíŒ∏"
N,0.6306429548563611,"(1 ‚àíe‚àíŒ∏)2 .
(12)"
N,0.6320109439124487,Published as a conference paper at ICLR 2022
N,0.6333789329685362,holds for all 1 ‚â§j and Œ∏ > 0. This clearly holds for j = 1. Rearranging gives
N,0.6347469220246238,"j2e‚àíŒ∏(j‚àí1) ‚â§
1 ‚àíe‚àíŒ∏j"
N,0.6361149110807114,"1 ‚àíe‚àíŒ∏ 2
."
N,0.6374829001367989,"The right-hand term is greater than or equal to 1, so it sufÔ¨Åces to choose Œ∏ such that"
N,0.6388508891928865,j2e‚àíŒ∏(j‚àí1) ‚â§1
N,0.640218878248974,‚áíŒ∏ ‚â•ln(j2)
N,0.6415868673050615,j ‚àí1 .
N,0.6429548563611491,"It can be shown that the right-hand term decreases with j for j > 1, so it sufÔ¨Åces to take j = 2, which
implies Œ∏ ‚â•2 ln(2). By choice of Œ∏ this is clearly true."
N,0.6443228454172366,"To show that gk is increasing in Œ∏, we consider"
N,0.6456908344733242,"d
dŒ∏gk(Œ∏) = ke‚àíŒ∏(1 ‚àíe‚àí2Œ∏)"
N,0.6470588235294118,"(1 ‚àíe‚àíŒ∏)4
+
X j‚â§k"
N,0.6484268125854993,j3e‚àíŒ∏j(e‚àí2Œ∏j ‚àí1)
N,0.6497948016415869,(1 ‚àíe‚àíŒ∏j)4
N,0.6511627906976745,"Similarly, it sufÔ¨Åces to show
j3e‚àíŒ∏j(1 ‚àíe‚àí2Œ∏j)"
N,0.652530779753762,"(1 ‚àíe‚àíŒ∏j)4
‚â§e‚àíŒ∏(1 ‚àíe‚àí2Œ∏)"
N,0.6538987688098495,"(1 ‚àíe‚àíŒ∏)4
."
N,0.655266757865937,"Rearranging, we have"
N,0.6566347469220246,j3e‚àíŒ∏(j‚àí1) ‚â§1 ‚àíe‚àí2Œ∏
N,0.6580027359781122,1 ‚àíe‚àí2Œ∏j
N,0.6593707250341997,1 ‚àíe‚àíŒ∏j
N,0.6607387140902873,"1 ‚àíe‚àíŒ∏ 4
."
N,0.6621067031463749,"Note that
1 ‚àíe‚àí2Œ∏"
N,0.6634746922024624,1 ‚àíe‚àí2Œ∏j
N,0.66484268125855,1 ‚àíe‚àíŒ∏j
N,0.6662106703146374,1 ‚àíe‚àíŒ∏
N,0.667578659370725,"4
‚â•1 ‚àíe‚àíŒ∏j"
N,0.6689466484268126,1 ‚àíe‚àí2Œ∏j .
N,0.6703146374829001,The term on the right is greater than 1
N,0.6716826265389877,"2 for Œ∏ > 0 and j ‚â•1. Therefore, it sufÔ¨Åces to choose Œ∏ such that"
N,0.6730506155950753,j3e‚àíŒ∏(j‚àí1) ‚â§1 2
N,0.6744186046511628,‚áíŒ∏ ‚â•ln(2j3)
N,0.6757865937072504,j ‚àí1 .
N,0.6771545827633378,"Once again, the term on the right is decreasing in j, so we can take j = 2, giving Œ∏ ‚â•4 ln(2), which is
satisÔ¨Åed by our choice of Œ∏."
N,0.6785225718194254,"Finally, the fact that gk is decreasing in k follows from (12)."
N,0.679890560875513,"G.3
EUCLIDEAN EMBEDDING CASE"
N,0.6812585499316005,"Theorem 4.2. Let ÀÜE [g(Œªa)g(y)] be an estimate of the accuracies E [g(Œªa)g(y)] using n samples, where all
LFs are conditionally independent given Y . If the signs of a are recoverable, then with high probability"
N,0.6826265389876881,"E[||ÀÜE [g(Œªa)g(y)] ‚àíE [g(Œªa)g(y)] ||2] = O

a‚àí10
|min| + a‚àí6
|min|
 p"
N,0.6839945280437757,"max(e5max, e6max)/n

."
N,0.6853625170998632,"Here, a|min| = mini |E

g(Œªi)g(y)

| and emax = maxj,k ej,k."
N,0.6867305061559508,Published as a conference paper at ICLR 2022
N,0.6880984952120383,"Proof. For three conditionally independent label functions Œªa, Œªb, Œªc, our estimate of Eg(Œªa)g(y)] is"
N,0.6894664842681258,"|ÀÜE[g(Œªa)g(y)]| =
|ÀÜeab||ÀÜea,c|E[Y 2]"
N,0.6908344733242134,"|ÀÜeb,c|  1 2
."
N,0.6922024623803009,"Furthermore, if we deÔ¨Åne xa,b = |ÀÜea,b|"
N,0.6935704514363885,"|ea,b|, we can write the ratio of elements of ÀÜa to a as"
N,0.6949384404924761,ka = |ÀÜE[g(Œªa)g(y)]|
N,0.6963064295485636,"|E[g(Œªa)g(y)]| =
|ÀÜeij|"
N,0.6976744186046512,"|eij| ¬∑ |ÀÜea,c|"
N,0.6990424076607387,"|ea,c| ¬∑ |eb,c|"
N,0.7004103967168263,"|ÀÜeb,c|  1"
N,0.7017783857729138,"2
=
xa,bxa,c xb,c  1 2
."
N,0.7031463748290013,(and the other deÔ¨Ånitions are symmetric for kb and kb). Now note that because we assume that signs are
N,0.7045143638850889,"completely recoverable, |ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]| =
|ÀÜE[g(Œªa)g(y)]| ‚àí|E[g(Œªa)g(y)]|
."
N,0.7058823529411765,Next note that
N,0.707250341997264,|ÀÜE[g(Œªa)g(y)]2 ‚àíE[g(Œªa)g(y)]2| = |ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]||ÀÜE[g(Œªa)g(y)] + E[g(Œªa)g(y)]|.
N,0.7086183310533516,"By the reverse triangle inequality,"
N,0.7099863201094391,(|ÀÜE[g(Œªa)g(y)]| ‚àí|E[g(Œªa)g(y)]|)2 =||ÀÜE[g(Œªa)g(y)]| ‚àí|E[g(Œªa)g(y)]||2
N,0.7113543091655267,‚â§|ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]|2 =
N,0.7127222982216143,|ÀÜE[g(Œªa)g(y)]2 ‚àíE[g(Œªa)g(y)]2|
N,0.7140902872777017,|ÀÜE[g(Œªa)g(y)] + E[g(Œªa)g(y)]| !2 ‚â§1
N,0.7154582763337893,"c2 |ÀÜE[g(Œªa)g(y)]2 ‚àíE[g(Œªa)g(y)]2|2,"
N,0.7168262653898769,"where we deÔ¨Åne c as mina |ÀÜE[g(Œªa)g(y)]+E[g(Œªa)g(y)]|. (With high probability c = ÀÜa|min||+|a|min|, but
there is a chance that ÀÜE[g(Œªa)g(y)] and E[g(Œªa)g(y)] have opposite signs.) For ease of notation, suppose
we examine a particular (a, b, c) = (1, 2, 3). Then,"
N,0.7181942544459644,(|E[g(Œª1)g(y)]| ‚àí|ÀÜE[g(Œª1)g(y)]|)2 ‚â§1
N,0.719562243502052,c2 |ÀÜE[g(Œª1)g(y)]2 ‚àíE[g(Œª1)g(y)]|2 = 1 c2
N,0.7209302325581395,|ÀÜe12||ÀÜe13|
N,0.7222982216142271,"|ÀÜe23|
‚àí|e12||e13| |e23|  2 = 1 c2"
N,0.7236662106703147,|ÀÜe12||ÀÜe13|
N,0.7250341997264022,"|ÀÜe23|
‚àí|ÀÜe12||ÀÜe13|"
N,0.7264021887824897,"|e23|
+ |ÀÜe12||ÀÜe13|"
N,0.7277701778385773,"|e23|
‚àí|ÀÜe12||e13|"
N,0.7291381668946648,"|e23|
+ |ÀÜe12||e13|"
N,0.7305061559507524,"|e23|
‚àí|e12||e13| |e23|  2 ‚â§1 c2"
N,0.7318741450068399, ÀÜe12ÀÜe13
N,0.7332421340629275,ÀÜe23e23
N,0.7346101231190151,"||ÀÜe23| ‚àí|e23|| +
 ÀÜe12 e23"
N,0.7359781121751026,"||ÀÜe13| ‚àí|e13|| +
e13 e23"
N,0.7373461012311902,"||ÀÜe12| ‚àí|e12||
2 ‚â§1 c2"
N,0.7387140902872777, ÀÜe12ÀÜe13
N,0.7400820793433652,ÀÜe23e23
N,0.7414500683994528,"|ÀÜe23 ‚àíe23| +
 ÀÜe12 e23"
N,0.7428180574555403,"|ÀÜe13 ‚àíe13| +
e13 e23"
N,0.7441860465116279,"|ÀÜe12 ‚àíe12|
2
."
N,0.7455540355677155,"With high probability, all elements of ÀÜe and e must be less than emax = maxj,k ej,k. We further know that
elements of |e| are at least a2
|min|/E[Y 2]. Now suppose (with high probability) that elements of |ÀÜe| are at"
N,0.746922024623803,Published as a conference paper at ICLR 2022
N,0.7482900136798906,"least ÀÜa2
|min| > 0, and deÔ¨Åne ‚ñ≥a,b = ÀÜea,b ‚àíea,b. Then,"
N,0.7496580027359782,(|E[g(Œª1)g(y)]2| ‚àí|E[g(Œª1)g(y)]|)2
N,0.7510259917920656,"‚â§max(emax, e2
max)
c2"
N,0.7523939808481532,"1
a2
|min|ÀÜa2
|min|E[Y 2]2 |‚ñ≥23| +
1
a2
|min|E[Y 2]|‚ñ≥13| +
1
a2
|min|E[Y 2]|‚ñ≥12| !2"
N,0.7537619699042407,"‚â§max(emax, e2
max)
c2
(‚ñ≥2
23 + ‚ñ≥2
13 + ‚ñ≥2
12)"
N,0.7551299589603283,"1
a4
|min|ÀÜa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 ! ."
N,0.7564979480164159,The original expression is now
N,0.7578659370725034,|ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]| ‚â§
N,0.759233926128591,"max(emax, e2
max)
c2"
N,0.7606019151846786,"1
a4
|min|ÀÜa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 !"
N,0.761969904240766,"(‚ñ≥2
a,b + ‚ñ≥2
a,c + ‚ñ≥2
b,c) ! 1 2
."
N,0.7633378932968536,"To bound the maximum absolute value between elements of ÀÜe and e, note that
 
2
 
‚ñ≥2
ij + ‚ñ≥2
a,c + ‚ñ≥2
b,c
 1"
N,0.7647058823529411,"2 ‚â§||ÀÜea,b,c ‚àíea,b,c||F ,"
N,0.7660738714090287,"where ea,b,c is a 3 √ó 3 matrix with ei,j in the (i, j)-th position. Moreover, it is a fact that ||ÀÜea,b,c‚àíea,b,c||F ‚â§
‚àör||ÀÜea,b,c‚àíea,b,c||2 ‚â§
‚àö"
N,0.7674418604651163,"3||ÀÜea,b,c‚àíea,b,c||2 where r is the rank of ÀÜea,b,c‚àíea,b,c. Putting everything together,"
N,0.7688098495212038,|ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]| ‚â§
N,0.7701778385772914,"max(emax, e2
max)
c2"
N,0.771545827633379,"1
a4
|min|ÀÜa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 ! ¬∑ 1"
N,0.7729138166894665,"2||ÀÜea,b,c ‚àíea,b,c||2
F ! 1 2 ‚â§"
N,0.774281805745554,"max(emax, e2
max)
c2"
N,0.7756497948016415,"1
a4
|min|ÀÜa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 ! ¬∑ 3"
N,0.7770177838577291,"2||ÀÜea,b,c ‚àíea,b,c||2
2 ! 1 2
."
N,0.7783857729138167,"Lastly, to compute E[||ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]||2], we use Jensen‚Äôs inequality (concave version, due
to the square root) and linearity of expectation:"
N,0.7797537619699042,E[|ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]|] ‚â§
N,0.7811217510259918,"max(emax, e2
max)
c2"
N,0.7824897400820794,"1
a4
|min|ÀÜa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 ! ¬∑ 3"
N,0.7838577291381669,"2E[||ÀÜea,b,c ‚àíea,b,c||2
2] ! 1 2
."
N,0.7852257181942545,"We use the covariance matrix inequality as described in Tropp (2014), which states that"
N,0.786593707250342,"P(||ÀÜe ‚àíe||2 ‚â•Œ≥) ‚â§max

2e3‚àínŒ≥"
N,0.7879616963064295,"œÉ2C , 2e3‚àínŒ≥2"
N,0.7893296853625171,"œÉ4C2

,"
N,0.7906976744186046,where œÉ = maxa eaa and C > 0 is a universal constant.
N,0.7920656634746922,"To get the probability distribution over ||ÀÜea,b,c ‚àíea,b,c||2
2, we just note that P(||ÀÜea,b,c ‚àíea,b,c||2 ‚â•Œ≥) =
P(||ÀÜe ‚àíe||2
2 ‚â•Œ≥2) to get"
N,0.7934336525307798,"P(||ÀÜea,b,c ‚àíea,b,c||2
2 ‚â•Œ≥) ‚â§2e3 max

e‚àín‚àöŒ≥"
N,0.7948016415868673,"œÉ2C , e‚àí
nŒ≥
œÉ4C2

."
N,0.7961696306429549,Published as a conference paper at ICLR 2022
N,0.7975376196990424,From this we can integrate to get
N,0.79890560875513,"E[||ÀÜea,b,c ‚àíea,b,c||2
2] =
Z ‚àû"
N,0.8002735978112175,"0
P(||ÀÜea,b,c ‚àíea,b,c||2
2 ‚â•Œ≥)dŒ≥ ‚â§2e3 max
œÉ4C2"
N,0.801641586867305,"n
, 2œÉ4C2 n2"
N,0.8030095759233926,"
= O(œÉ4 n )."
N,0.8043775649794802,"Substituting this back in, we get"
N,0.8057455540355677,E[|ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]|] ‚â§
N,0.8071135430916553,"max(emax, e2
max)
(ÀÜa|min| + a|min|)2"
N,0.8084815321477428,"1
a4
|min|ÀÜa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 !"
N,0.8098495212038304,"¬∑ O
œÉ4 n ! 1 2 ‚â§"
N,0.811217510259918,"max(emax, e2
max)
(ÀÜa|min| + a|min|)2 ¬∑"
N,0.8125854993160054,"1
a4
|min|ÀÜa4
|min|E[Y 2]4 +
2
a4
|min|E[Y 2]2 !"
N,0.813953488372093,"¬∑ O
œÉ4 n ! 1 2 ‚â§"
N,0.8153214774281806,"max(emax, e2
max)
(ÀÜa|min| + a|min|)2 min(E[Y 2]4, E[Y 2]2) ¬∑"
N,0.8166894664842681,"1
a4
|min|ÀÜa4
|min|
+
2
a4
|min| !"
N,0.8180574555403557,"¬∑ O
œÉ4 n ! 1 2"
N,0.8194254445964432,"with high probability. Finally, we clean up the a|min| and ÀÜa|min| terms. The terms involving a and ÀÜa can be
rewritten as"
N,0.8207934336525308,"1 + 2ÀÜa4
|min|
a6
|min|ÀÜa4
|min| + 2a5
|min|ÀÜa5
|min| + a4
|min|ÀÜa6
|min|
."
N,0.8221614227086184,"We suppose that
ÀÜa|min|
a|min| ‚àà[1 ‚àíœµ, 1 + œµ] for some œµ > 0 with high probability. Then, this becomes less than"
N,0.8235294117647058,"1 + 2ÀÜa4
|min|
(1 ‚àíœµ)4a10
|min| + 2(1 ‚àíœµ)5a10
|min| + (1 ‚àíœµ)6a10
|min|
‚â§
1 + 2ÀÜa4
|min|
4(1 ‚àíœµ)6a10
|min| = O"
N,0.8248974008207934,"1
a10
|min|
+
1
a6
|min| ! ."
N,0.826265389876881,"Therefore, with high probability, the sampling error for the accuracy is bounded by"
N,0.8276333789329685,E[||ÀÜE[g(Œªa)g(y)] ‚àíE[g(Œªa)g(y)]||2] = O 
N,0.8290013679890561,"œÉ2
 
1
a10
|min|
+
1
a6
|min| ! r"
N,0.8303693570451436,"max(emax, e2max) n ! = O"
N,0.8317373461012312,"1
a10
|min|
+
1
a6
|min| ! r"
N,0.8331053351573188,"max(e5max, e6max) n ! ."
N,0.8344733242134063,"Note that if all label functions are conditionally independent, we only need to know the sign
of one accuracy to recover the rest.
For example,
if we know if E[g(Œª1)g(y)] is positive
or negative, we can use E[g(Œª1)g(y)]E[g(Œª2)g(y)]
=
e1,2E[Y 2]2, E[g(Œª1)g(y)]E[g(Œª3)g(y)]
=
e1,3E[Y 2]2, . . . , E[g(Œª1)g(y)]E[g(Œªm)g(y)] = e1,mE[Y 2]2 to recover all other signs."
N,0.8358413132694938,Published as a conference paper at ICLR 2022
N,0.8372093023255814,"H
EXTENDED EXPERIMENTAL DETAILS"
N,0.8385772913816689,"In this section, we provide some additional experimental results and details. All experiments were conducted
on a machine with Intel Broadwell 2.7GHz CPU and NVIDIA GK210 GPU. Each experiment takes from 30
minutes up to a few hours depending on the experiment conditions."
N,0.8399452804377565,"Hyperbolic Space and Geodesic Regression
The following is basic background useful for understanding
our hyperbolic space models. Hyperbolic space is a Riemannian manifold. Unlike Euclidean space, it does
not have a vector space structure. Therefore it is not possible to directly add points. However, geometric
notions like length, distance, and angles do exist; these are obtained through the Riemannian metric for the
space. Points p in Smooth manifolds M are equipped with a tangent space denoted TpM. The elements
(tangent vectors v ‚ààTpM) in this space are used for linear approximations of a function at a point."
N,0.841313269493844,"The shortest paths in a Riemannian manifold are called geodesics. Each tangent vector x ‚ààTpM is equiv-
alent to a particular geodesic that takes p to a point q. SpeciÔ¨Åcally, the exponential map is the operation
exp : TpM ‚ÜíM that takes p to the point q = expp(v) that the tangent vector v points to. In addition, the
length ‚à•v‚à•of the tangent vector is also the distance d(p, q) on the manifold. This operation can be reversed
with the log map, which, given q ‚ààM, provides the tangent vector v = logp(q)."
N,0.8426812585499316,"The geodesic regression model is the following. Set some intercept p ‚ààM. Scalars xi ‚ààR are selected
according to some distribution. Without noise, the output points are selected along a geodesic through p
parametrized by Œ≤: yi = expp(xi √ó Œ≤), where Œ≤ is the weight vector, a tangent vector in Tp(M) that is not
known. This is a noiseless model; the more interesting problem allows for noise, generalizing the typical
situation in linear regression. This noise is added using the following approach. For notational convenience,
we write expp(v) as exp(p, v). Then, the noisy y are yi = exp(exp(p, xi √óŒ≤), Œµi), where Œµi ‚ààTexp(p,xi)M.
This notion generalizes adding zero-mean Gaussian noise to the y‚Äôs in conventional linear regression."
N,0.8440492476060192,"The equivalent of least squares estimation is then given by ÀÜp, ÀÜŒ≤ = arg minq,Œ±
Pn
i=1 d(exp(q, Œ±xi), yi)2."
N,0.8454172366621067,"Semantic Dependency Parsing
We used datasets on Czech and English taken from the Universal De-
pendencies Nivre et al. (2020) repository. The labeling functions were Stanza Qi et al. (2020) pre-trained
semantic dependency parsing models. These pre-trained models were trained over other datasets from the
same language. For the Czech experiments, these were the models cs.cltt, cs.fictree, cs.pdt,
while for English, they were taken from en.gum, en.lines, en.partut, en.ewt. The metric is
the standard unlabeled attachment score (UAS) metric used for unlabeled dependency parsing. Finally, we
used access to a subset of labels to compute expected distances, as in the label model variant in Chen et al.
(2021)."
N,0.8467852257181943,"Movies dataset processing
In order to have access to ample features for learning to rank and regression
tasks, we combined IMDb (imd) and TMDb(tmd) datasets based on movie id. In the IMDb dataset, we
mainly used movie metadata, which has information chieÔ¨Çy about the indirect index of the popularity (e.g.
the number of director facebook likes) and movie (e.g. genres). The TMDb dataset gives information
mainly about the movie, such as runtime and production country. For ranking and regression, we chose
vote average column from TMDb dataset as a target feature. In rankings, we converted the vote average
column into rankings so the movie with the highest review has a top ranking. We excluded the movie scores
in IMDb dataset from input features since it is directly related to TMDb vote average. But we later included
IMDb, Rotten Tomatoes, MovieLens ratings as weak labels in Table 2."
N,0.8481532147742818,"After merging the two datasets, we performed feature engineering as follows."
N,0.8495212038303693,"1. One-hot encoding: color, content rating, original language, genres, status"
N,0.8508891928864569,Published as a conference paper at ICLR 2022
N,0.8522571819425444,"2. Top (frequency) 0.5% one-hot encoding: plot keywords, keywords, production companies, actors"
N,0.853625170998632,"3. Count: production countries, spoken languages"
N,0.8549931600547196,"4. Merge (add) actor 1 facebook likes, actor 2 facebook likes, actor 3 facebook likes into ac-
tor facebook likes"
N,0.8563611491108071,5. Make a binary feature regarding whether the movie‚Äôs homepage is missing or not.
N,0.8577291381668947,"6. Transform date into one-hot features such as month, the day of week."
N,0.8590971272229823,"By adding the features above, we were able to enhance performance signiÔ¨Åcantly in the fully-supervised
regression task used as a baseline."
N,0.8604651162790697,"In the ranking experiments, we randomly sampled 5000 sets of movies as the training set, and 1000 sets of
movies as the test set. The number of items in an item set (ranking set) was 5. Note that the movies in the
training set and test set are strictly separated, i.e. if a movie appears in the training set, it is not included in
the test set."
N,0.8618331053351573,"Model and hyperparameters
In the ranking setup, we used 4-layer MLP with ReLU activations. Each
hidden layer had 30 units and batch normalization(Ioffe & Szegedy, 2015) was applied for all hidden layers.
We used the SGD optimizer with ListMLE loss (Xia et al., 2008); the learning rate was 0.01."
N,0.8632010943912448,"In the regression experiments, we used gradient boosting regression implemented in sklearn with
n estimators=250. Other than n estimators, we used the default hyperparameters in sklearn‚Äôs implemen-
tation."
N,0.8645690834473324,"BoardGameGeek data processing
In the BoardGameGeek dataset (boa, 2017), we used metadata of
board games only. Since this dataset showed enough model performance without additional feature engi-
neering, we used the existing features: yearpublished, minplayers, maxplayers, playingtime, minplaytime,
maxplaytime, minage, owned, trading, wanting, wishing, numcomments, numweights, averageweight. The
target variable was average ratings (average) for regression, and board game ranking (Board Game Rank).
Note that ‚ÄòBoard Game Rank‚Äò cannot be directly calculated from average ratings. BoardGameGeek has its
own internal formula to determine the ranking of board games. In the ranking setup, we randomly sampled
5000 board game sets as the training set, and 1000 board game sets as the test set."
N,0.86593707250342,"Simulated LFs generation in ranking and regression
In rankings, we sampled from Œªa(i)
‚àº
1
Z e‚àíŒ≤adœÑ (œÄ,Yi) with Œ≤a.
For more realistic heterogeneous LFs, 1/3 (good) LFs Œ≤a are sampled from
Uniform(0.2, 1), and 2/3 (bad) LFs‚Äô Œ≤a are sampled from Uniform(0.001, 0.01). Note that higher value
of Œ≤a yields less noisy weak labels. In regression, we used the conditional distribution Œõ|Y to generate
samples of Œõ. SpeciÔ¨Åcally, where Œõ|Y ‚àºN(¬Ø¬µ, ¬ØŒ£), where ¬Ø¬µ = Œ£ŒõY Œ£‚àí1
Y y and ¬ØŒ£ = Œ£Œõ ‚àíŒ£ŒõY Œ£‚àí1
Y Œ£Y Œõ,
from the assumption (Œõ, Y ) ‚àºN(0, Œ£)."
N,0.8673050615595075,"I
ADDITIONAL SYNTHETIC EXPERIMENTS"
N,0.8686730506155951,"We present several additional synthetic experiments, including results for partial ranking labeling functions
and for parameter recovery in regression settings."
N,0.8700410396716827,"I.1
RANKING"
N,0.8714090287277702,"To check whether our algorithm works well under different conditions, we performed additional experiments
with varied parameters. In addition, we performed a partial ranking experiment."
N,0.8727770177838577,Published as a conference paper at ICLR 2022
N,0.8741450068399452,"2
4
6
8
10 12 14 16 18"
N,0.8755129958960328,Num LFs (m) 0.0 0.5 1.0 1.5 2.0
N,0.8768809849521204,Mean KT Distance 1e‚àí2
N,0.8782489740082079,"Unweighted
Optimal Weights
Estimated Weights"
N,0.8796169630642955,"(a) d = 10, n = 250"
N,0.8809849521203831,"2
4
6
8
10 12 14 16 18"
N,0.8823529411764706,Num LFs (m) 0.0 0.2 0.4 0.6 0.8 1.0 1.2
N,0.8837209302325582,Mean KT Distance 1e‚àí2
N,0.8850889192886456,"Unweighted
Optimal Weights
Estimated Weights"
N,0.8864569083447332,"(b) d = 20, n = 250"
N,0.8878248974008208,"Figure 6: Inference via weighted and standard Kemeny rule over full rankings (top) with permutations of
size d = 10, 20. Error metric is Kendall tau distance (lower is better). Proposed weighted Kemeny rule is
nearly optimal on full rankings."
N,0.8891928864569083,"Ranking synthetic data generation
First, n true rankings Y are sampled uniformly at random. In the full
ranking setup, each LF œÄi is sampled from the Mallows Œªa(i) ‚àº1"
N,0.8905608755129959,"Z e‚àíŒ≤adœÑ (œÄ,Yi) with parameters Œ≤a, Yi and
in the partial ranking setup it is sampled from a selective Mallows with parameters Œ≤a, Yi, Si where each
Si ‚äÜ[d] is chosen randomly while ensuring that each x ‚àà[d] appears in at least a p fraction of these subsets.
Higher p corresponds to dense partial rankings and smaller p leads to sparse partial rankings. We generate
18 LFs with 10 of then having Œ≤a ‚àºUniform(0.1, 0.2) and rest have Œ≤a ‚àºUniform(2, 5). This was done
in order to model LFs of different quality. These LFs are then randomly shufÔ¨Çed so that the order in which
they are added is not a factor. For the partial rankings setup we use the same process to get Œ≤a and randomly
generate Si according to the sparsity parameter p. For a set of LFs parameters we run the experiments for 5
random trials and record the mean and standard deviation."
N,0.8919288645690835,"Full Ranking Experiments
Figure 6 shows synthetic data results without an end model, i.e., just using the
inference procedure as an estimate of the label. We report the ‚ÄòUnweighted‚Äô Kemeny baseline that ignores
differing accuracies. ‚ÄòEstimated Weights‚Äô uses our approach, while ‚ÄòOptimal Weights‚Äô is based on an oracle
with access to the true Œ∏a parameters. As expected, synthesis with estimated parameters improves on the
standard Kemeny baseline. The improvement for the full rankings case (top) is higher for fewer LFs; this is
intuitive, as adding more LFs globally improves estimation even when accuracies differ."
N,0.893296853625171,"Partial Ranking Experiments
In the synthetic data partial ranking setup, we vary the value of p (ob-
servation probability) from 0.9 (dense) to 0.2 (sparse) and apply our extension of the inference method to
partial rankings. Figure 7 shows the results obtained. Our observations in terms of unweighted vs weighted
aggregation remain consistent here with the full rankings setup. This suggests that the universal approach
can provide the same type of gains in the partial rankings."
N,0.8946648426812586,"I.2
REGRESSION"
N,0.896032831737346,"Similarly, we performed a synthetic experiment to show how our algorithm performs in parameter recovery."
N,0.8974008207934336,"Regression synthetic data generation
The data generation model is linear Y = Œ≤T X, where our data is
given by (X, Y ) with X ‚ààRq and Y ‚ààR. We generate n such samples. Note that we did not add a noise
variable Œµ ‚àºN(0, œÉ2) here since we do not directly interact with Y ; the noise exists instead in the labeling
functions (i.e., the weak labels)."
N,0.8987688098495212,Published as a conference paper at ICLR 2022 3 6 9 12 15 18
N,0.9001367989056087,Num LFs (m) 0.01 0.02 0.03 0.04 0.05
N,0.9015047879616963,Mean KT Distance
N,0.9028727770177839,"Unweighted
Optimal Weights
Estimated Weights"
N,0.9042407660738714,"(a) d = 10, n = 250, p = 0.9 3 6 9 12 15 18"
N,0.905608755129959,Num LFs (m) 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16
N,0.9069767441860465,Mean KT Distance
N,0.908344733242134,"Unweighted
Optimal Weights
Estimated Weights"
N,0.9097127222982216,"(b) d = 10, n = 250, p = 0.7 3 6 9 12 15 18"
N,0.9110807113543091,Num LFs (m) 0.05 0.10 0.15 0.20 0.25 0.30
N,0.9124487004103967,Mean KT Distance
N,0.9138166894664843,"Unweighted
Optimal Weights
Estimated Weights"
N,0.9151846785225718,"(c) d = 10, n = 250, p = 0.5 3 6 9 12 15 18"
N,0.9165526675786594,Num LFs (m) 0.200 0.225 0.250 0.275 0.300 0.325 0.350
N,0.9179206566347469,Mean KT Distance
N,0.9192886456908345,"Unweighted
Optimal Weights
Estimated Weights"
N,0.920656634746922,"(d) d = 10, n = 250, p = 0.2"
N,0.9220246238030095,"Figure 7: Inference via weighted and standard majority vote over partial rankings with permutations of size
d = 10. Error metric is Kendall tau distance (lower is better). Proposed inference rule is nearly optimal on
full rankings."
N,0.9233926128590971,"Parameter estimation in synthetic regression experiments
Figure 8 reports results on synthetic data
capturing label model estimation error for the accuracy and correlation parameters (¬µ, œÉ) and for directly
estimating the label Y . As expected, estimation improves as the number of samples increases. The top-right
plot is particularly intuitive: here, our improved inference procedure vastly improves over naive averaging
as it accesses sufÔ¨Åciently many samples to estimate the label model itself. On the bottom, we observe, as
expected, that label estimation signiÔ¨Åcantly improves with access to more labels."
N,0.9247606019151847,"J
ADDITIONAL REAL LF EXPERIMENTS AND RESULTS"
N,0.9261285909712722,We present a few more experiments with different types of labeling functions.
N,0.9274965800273598,"J.1
BOARD GAME GEEK DATASET"
N,0.9288645690834473,"In the board games dataset, we built labeling functions using simple programs expressing heuristics. For
regression, we picked several continuous features and scaled them to a range and removed outliers. Similarly,
for rankings, we picked what we expected to be meaningful features and produced rankings based on them.
The selected features were [‚Äôowned‚Äô, ‚Äôtrading‚Äô, ‚Äôwanting‚Äô, ‚Äôwishing‚Äô, ‚Äônumcomments‚Äô]."
N,0.9302325581395349,Published as a conference paper at ICLR 2022
N,0.9316005471956225,"103
105"
N,0.93296853625171,"Samples 10
3 10
2"
N,0.9343365253077975,"Mu param error, L2"
N,0.9357045143638851,estimation error
N,0.9370725034199726,"103
105"
N,0.9384404924760602,"Samples 10
3 10
2"
N,0.9398084815321477,"Tot, param error, L2"
N,0.9411764705882353,estimation error
N,0.9425444596443229,"103
105"
N,0.9439124487004104,"Samples 10
1 100"
N,0.945280437756498,"Y value error, MSE"
N,0.9466484268125855,Label estimation error
N,0.948016415868673,"Baseline
Our Approach"
N,0.9493844049247606,"(a) Number of samples 23
25"
N,0.9507523939808481,Num LFs (m)
N,0.9521203830369357,"2 √ó 10
3"
N,0.9534883720930233,"3 √ó 10
3"
N,0.9548563611491108,"4 √ó 10
3"
N,0.9562243502051984,"6 √ó 10
3"
N,0.957592339261286,"Mu param error, L2"
N,0.9589603283173734,"estimation errors 23
25"
N,0.960328317373461,"Num LFs (m) 10
3"
N,0.9616963064295485,"Tot, param error, L2"
N,0.9630642954856361,"estimation error 23
25"
N,0.9644322845417237,"Num LFs (m) 10
2 10
1"
N,0.9658002735978112,"Y value error, MSE"
N,0.9671682626538988,Label estimation error
N,0.9685362517099864,"Baseline
Our Approach"
N,0.9699042407660738,(b) Number of LFs
N,0.9712722298221614,"Figure 8: Parameter and label estimation with varying the number of samples (top) and the number of
labeling functions (bottom)"
N,0.9726402188782489,"# of training examples
Kendall Tau distance (mean ¬± std)"
N,0.9740082079343365,"Fully supervised (5%)
250
0.1921 ¬± 0.0094
Fully supervised (10%)
500
0.1829 ¬± 0.0068
WS (Heuristics)
5000
0.1915 ¬± 0.0011"
N,0.9753761969904241,Table 5: End model performance with true ranking LFs in BGG dataset.
N,0.9767441860465116,"In this case, we observed that despite not having access to any labels, we can produce performance similar to
fully-supervised training on 5-10% of the true labels. We expect that further LF development will produce
even better performance."
N,0.9781121751025992,"J.2
MSLR-WEB10K"
N,0.9794801641586868,"To further illustrate the strength of our approach we ran an experiment using unsupervised learning methods
in information retrieval (such as BM25) as weak supervision sources. The task is information retrieval, the
dataset is MSLR-WEB10Kmsl, and the model and training details are identical to the other experiments.
We used several labeling functions including BM25 and relied on our framework to integrate these. The
labeling functions were written over BM25 and features such as covered query term number, covered query"
N,0.9808481532147743,Published as a conference paper at ICLR 2022
N,0.9822161422708618,"# of training examples
MSE (mean ¬± std)"
N,0.9835841313269493,"Fully supervised (1%)
144
0.4605 ¬± 0.0438
Fully supervised with ‚Äúbad‚Äù subset (10%)
1442
0.8043 ¬± 0.0013
Fully supervised with ‚Äúbad‚Äù subset (25%)
3605
0.4628 ¬± 0.0006
WS (Heuristics)
14422
0.8824 ¬± 0.0005"
N,0.9849521203830369,"Table 6:
End model performance with true regression LFs in BGG dataset. The training data was picked
based on the residuals in linear regression (resulting in a ‚Äúbad‚Äù subset scenario for a challenging dataset).
We obtain comparable performance."
N,0.9863201094391245,"5
10
15
Num LFs 0.225 0.250 0.275 0.300 MSE"
N,0.987688098495212,(a) Movies dataset
N,0.9890560875512996,"5
10
15
Num LFs 0.300 0.325 0.350 0.375 MSE"
N,0.9904240766073872,"WS (Majority Vote)
WS (Ours)
Fully supervised (25%)
Fully supervised (50%)
Fully supervised (100%)"
N,0.9917920656634747,"(b) BoardGameGeek dataset
Figure 9: End model performance with regression LFs (Left: Movies dataset, Right: BGG Dataset). Results
from training a model on pseudolabels are compared to fully-supervised baselines on varying proportions of
the dataset. Baseline is the averaging of weak labels. Metric is (MSE); lower is better."
N,0.9931600547195623,"term ratio, boolean model, vector space model, LMIR.ABS, LMIR.DIR, LMIR.JM, and query-url click
count."
N,0.9945280437756497,"As expected, the synthesis of multiple sources produced better performance than BM25 Dehghani et al.
(2017) alone. Despite not using any labels, we outperform training on 10% of the data with true labels. This
suggests that our framework for integrating multiple sources is a better choice than either hand-labeling or
using a single source of weak supervision to provide weak labels. Below, for Kendall tau, lower is better."
N,0.9958960328317373,"Kendall tau distance
NDCG@1"
N,0.9972640218878249,"Fully supervised (10%)
0.4003 ¬± 0.0151
0.7000 ¬± 0.0200
Fully supervised (25%)
0.3736 ¬± 0.0090
0.7288 ¬± 0.0077
WS (Dehghani et al. (2017))
0.4001 ¬± 0.0063
0.7288 ¬± 0.0077
WS (Ours)
0.3929 ¬± 0.0052
0.7402 ¬± 0.0119"
N,0.9986320109439124,"Table 7: End model performance with true ranking LFs in MSLR-WEB10K dataset. Since the dataset has a
lot of tie scores and the number of items is not uniform across examples, we sampled the examples with Ô¨Åve
unique scores (0, 1, 2, 3, 4). Also, in each example, items are randomly chosen so that each score occurs
only once in each item set."
