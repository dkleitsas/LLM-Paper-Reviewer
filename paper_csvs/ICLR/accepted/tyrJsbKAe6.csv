Section,Section Appearance Order,Paragraph
INTRODUCTION,0.0,"1
INTRODUCTION
Ofﬂine Reinforcement Learning (RL) is one of the important areas of RL where the learner is pre-
sented with a static dataset consisting of transition-related information (state, action, reward, and
next state) collected by some behavior policy, and needs to learn purely from the ofﬂine data without
any future online interaction with the environment. Ofﬂine RL is used in a number of applications
where online random experimentation is costly or dangerous such as health care (Kosorok & Laber,
2019), digital marketing (Chen et al., 2019) and robotics (Levine et al., 2020)."
INTRODUCTION,0.0010384215991692627,"The performance guarantees of ofﬂine RL often rely on two quantities: the coverage of the ofﬂine
data and the property of the function approximation used in the algorithms. For instance, for the
classic Fitted-Q-iteration (FQI) algorithm (Ernst et al., 2005; Munos & Szepesv´ari, 2008), it requires
(a) full coverage in the ofﬂine data, i.e., max(s,a) dπ(s, a)/ρ(s, a) < ∞for any stochastic policies
π including history-dependent non-Markovian policies, where dπ(s, a) is a state-action occupancy
distribution of a policy π and ρ(s, a) is an ofﬂine distribution, (b) realizability in a Q function
class, i.e., the optimal Q function belongs to the function class, and (c) Bellman completeness, i.e.,
applying the Bellman operator on any function in the function class results in a new function that
also belongs to the function class (see the ﬁrst row in Table 1). Among these three assumptions,
the full coverage and the Bellman completeness are particularly strong. The full coverage means
that the behavior policy needs to be exploratory enough, although ﬁguring out an exploratory policy
itself is an extremely hard problem for large-scale MDPs. The Bellman completeness assumption
does not have a monotonic property, i.e., even starting with a function class that originally permits
Bellman completeness, slightly increasing the capacity of the function class could result in a new
class that does not have Bellman completeness anymore. Thus, we aim to relax the assumptions on
the ofﬂine data and the function class. Particularly, we are interested in the following question:"
INTRODUCTION,0.0020768431983385254,"Given a realizable function class and an ofﬂine distribution that only provides
partial coverage, can we learn a policy that is able to compete with any policy that
is covered by the ofﬂine distribution?"
INTRODUCTION,0.003115264797507788,"We study this question from a model-based learning perspective and provide an afﬁrmative answer
to the question. More speciﬁcally, different from FQI, we start with a realizable model class, i.e.,
the ground truth transition falls into the model class. We further abandon the strong full coverage
assumption, and instead, assume partial coverage which means the ofﬂine data distribution only
covers a state-action distribution of some high-quality comparator policy π∗(π∗is not necessarily"
INTRODUCTION,0.004153686396677051,Published as a conference paper at ICLR 2022
INTRODUCTION,0.005192107995846314,"Methods
Type
Coverage
Additional Structures
FQI (Munos & Szepesv´ari, 2008)
F
Full: maxs,a
dπ(s,a)"
INTRODUCTION,0.006230529595015576,"ρ(s,a) < ∞, ∀π
Bellman complete"
INTRODUCTION,0.007268951194184839,"Minimax Way (Uehara et al., 2020)
F
Full: maxs,a
dπ(s,a)"
INTRODUCTION,0.008307372793354102,"ρ(s,a) < ∞, ∀π
Realizability in density ratio
Duan et al. (2020)
F
Full: Es,a∼ρφ(s, a)φ(s, a)⊤is PSD
Linear Bellman complete
Xie & Jiang (2020)
F
Full: maxs,a,s′ P ∗(s′|s,a)"
INTRODUCTION,0.009345794392523364,"ρ(s′)
< ∞
None"
INTRODUCTION,0.010384215991692628,"Liu et al. (2020)
F
Partial† : maxs,a
dπ∗(s,a)"
INTRODUCTION,0.01142263759086189,"ρ(s,a)
< ∞
Bellman / Policy class complete"
INTRODUCTION,0.012461059190031152,"Rashidinejad et al. (2021)
F
Partial: maxs,a
dπ∗(s,a)"
INTRODUCTION,0.013499480789200415,"ρ(s,a)
< ∞
Tabular MDP"
INTRODUCTION,0.014537902388369679,"Jin et al. (2020b); Zhang et al. (2021b)
F
Partial††: maxx
x⊤Es,a∼dπ∗φ(s,a)(φ(s,a))⊤x"
INTRODUCTION,0.01557632398753894,"x⊤Es,a∼ρφ(s,a)(φ(s,a))⊤x
< ∞
Linear MDP (Jin et al., 2020a)"
INTRODUCTION,0.016614745586708203,"Xie et al. (2021)
F
Partial: maxf
∥f−T f∥2
dπ∗
∥f−T f∥2µ
< ∞
Bellman complete"
INTRODUCTION,0.017653167185877467,"Zanette et al. (2021)
F
Partial : maxx
x⊤Es,a∼dπ∗φ(s,a)(φ(s,a))⊤x"
INTRODUCTION,0.018691588785046728,"x⊤Es,a∼ρφ(s,a)(φ(s,a))⊤x
< ∞
Linear Bellman complete"
INTRODUCTION,0.01973001038421599,"Batch (Ross & Bagnell, 2012)
B
Full: maxs,a
dπ(s,a)"
INTRODUCTION,0.020768431983385256,"ρ(s,a) < ∞, ∀π
None"
INTRODUCTION,0.021806853582554516,"Milo (Chang et al., 2021)
B
Partial: maxx
x⊤Es,a∼dπ∗φ(s,a)(φ(s,a))⊤x"
INTRODUCTION,0.02284527518172378,"x⊤Es,a∼ρφ(s,a)(φ(s,a))⊤x
< ∞
KNR / GP"
INTRODUCTION,0.023883696780893044,"Partial†††: maxs,a
dπ∗(s,a)"
INTRODUCTION,0.024922118380062305,"ρ(s,a)
< ∞
None"
INTRODUCTION,0.02596053997923157,"Partial: maxx
x⊤Es,a∼dπ∗φ(s,a)(φ(s,a))⊤x"
INTRODUCTION,0.02699896157840083,"x⊤Es,a∼ρφ(s,a)(φ(s,a))⊤x
< ∞
Linear MDP /KNR / GP
CPPO (Ours)
B"
INTRODUCTION,0.028037383177570093,"Partial: maxx
x⊤Es,a∼dπ∗φ∗(s,a)(φ∗(s,a))⊤x"
INTRODUCTION,0.029075804776739357,"x⊤Es,a∼ρφ∗(s,a)(φ∗(s,a))⊤x
< ∞
Low-rank MDP (unknown φ∗)"
INTRODUCTION,0.030114226375908618,"Table 1: Comparison among existing works regarding their type, coverage, and additional structural
assumptions on the function class or MDPs. Type F means model-free and type B means model-
based. Partial coverage means 2that the ofﬂine distribution ρ covers a state-action distribution of
a comparator policy π∗. † means it assumes an accurate density estimator for ρ(s, a). †† means
although the analysis in Jin et al. (2020a) is done under the full coverage for linear MDPs, based on
the argument (Zhang et al., 2021b), we can show the algorithm has the PAC guarantee under partial
coverage in terms of the relative condition number for linear MDPs. † † † means that we can reﬁne
it to a more adaptive quantity using the model class (i.e., Deﬁnition 1). All the methods in the table
require realizability in the function class."
INTRODUCTION,0.03115264797507788,"the optimal policy, and π∗could be non-Markovian), i.e., maxs,a dπ∗(s, a)/ρ(s, a) < ∞, We design
an algorithm — Constrained Pessimistic Policy Optimization (CPPO), which can learn a policy that
is as good as any comparator policy π∗that is covered by the ofﬂine data. The fact that CPPO can
learn to compete against history-dependent policies is meaningful in ofﬂine RL when the ofﬂine
data does not cover the optimal policy."
INTRODUCTION,0.032191069574247146,"While one could assume density ratio based concentrability coefﬁcient (maxs,a dπ∗(s, a)/ρ(s, a))
to be under control for small size MDPs, in large-scale MDPs (e.g. continuous state space), the den-
sity ratio could quickly become an extremely large quantity which makes the performance guarantee
vacuous. When applying CPPO to MDPs with additional structural assumptions, we can seamlessly
reﬁne the density ratio based concentrability coefﬁcient to more natural and tighter quantities. No-
tably, we consider the ofﬂine representation learning setting where the underlying MDPs permit a
low-rank structure (unlikely linear MDPs (Jin et al., 2020a; Yang & Wang, 2020), we do not assume
the ground truth state-action feature representation φ⋆is known, and instead we need to learn φ⋆)
and we show that we can reﬁne the density ratio to a relative condition number that is deﬁned us-
ing the unknown true state-action feature representation φ⋆. Intuitively this means that as long as
there exists a high-quality comparator policy that only visits the subspace (deﬁned using the true
representation φ) that is covered by the ofﬂine data, CPPO can compete against such a policy, even
without knowing the true φ⋆. Such bounded relative condition number assumption is much weaker
than the bounded density ratio assumption.3 While the concept of relative condition number was
originally introduced in the online RL setting (e.g., Agarwal et al. (2020c;a) with a known linear
feature φ), and later was introduced in ofﬂine RL (Zhang et al. (2021b); Chang et al. (2021)), these
prior works all rely on the fact that the feature representation φ is known to the learner a priori
(see Table 1 for the comparison). Another interesting example is factored MDPs (Kearns & Koller,
1999) where we show CPPO reﬁnes the density ratios to be density ratio associated with individual
factors, which leverages the factored structure and is provably tighter. We also give examples on
linear MDPs (Yang & Wang, 2020), kernelized nonlinear regulator (KNRs) (Kakade et al., 2020),
where we again show that CPPO enjoys problem speciﬁc quantities for measuring the coverage."
INTRODUCTION,0.033229491173416406,"Our contributions.
Our contributions are two folds, which we summarize below:"
INTRODUCTION,0.03426791277258567,"3Strictly speaking, in Jin et al. (2020b); Rashidinejad et al. (2021), a comparator policy is restricted to the
optimal policy. In Chang et al. (2021); Zanette et al. (2021); Xie et al. (2021) and CPPO, a comparator policy
can be any policy."
INTRODUCTION,0.035306334371754934,Published as a conference paper at ICLR 2022
INTRODUCTION,0.036344755970924195,"1. We show that in the model-based setting, realizability and partial coverage is enough to
learn a high-quality comparator policy (Theorem 1). Notably, (1) this result holds for any
MDPs with realizable model classes, (2) we can compete against even history-dependent
policies. This is in sharp contrast to the state-of-art provable model-free ofﬂine RL results:
see Table 1 on page 2 for detailed comparisons to prior works."
INTRODUCTION,0.037383177570093455,"2. Under additional structural assumptions (e.g., KNRs, linear MDPs (Yang & Wang, 2020),
linear mixture MDPs (Ayoub et al., 2020), low-rank MDPs, factored MDPs), we show that
we can seamlessly reﬁne the density ratio based concentrability coefﬁcients to problem spe-
ciﬁc quantities. This ﬂexibility to adapt to problem speciﬁc coverage measuring quantities
is in sharp contrast to other model-free ofﬂine RL algorithms such as minimax based ap-
proaches (Uehara et al., 2020) which, to the best of our knowledge, cannot leverage MDP’s
structures (e.g., linear MDPs) to reﬁne its density ratio based concentrability coefﬁcients."
INTRODUCTION,0.03842159916926272,"While we focus on the model-based setting and have demonstrated advantages of our approach over
model-free ones (i.e., no more Bellman completeness assumption on function classes, being able to
compete against a larger pool of policies, and the ability to seamlessly adapt to problem dependent
structures), it is worth noting that realizability in the model-based setting is usually considered
stronger than the one in the model-free setting. On the empirical side, model-based ofﬂine RL
algorithms are the state-of-art (e.g., Yu et al. (2020); Kidambi et al. (2020); Matsushima et al. (2020);
Cang et al. (2021); Chang et al. (2021)). Our theoretical results provide a sharp contrast between
model-based and model-free approaches in ofﬂine RL."
RELATED WORK,0.03946002076843198,"2
RELATED WORK"
RELATED WORK,0.040498442367601244,"We discuss two families of related works of ofﬂine RL. In Appendix C, we discuss related works
about representation learning in RL."
RELATED WORK,0.04153686396677051,"Insufﬁcient coverage of the dataset due to the lack of online exploration is known as the main
challenge in ofﬂine RL (Wang et al., 2020). To deal with this problem, a number of methods have
been recently proposed from both model-free (Wu et al., 2019; Touati et al., 2020; Kumar et al.,
2020; Liu et al., 2020; Rezaeifar et al., 2021; Fujimoto et al., 2019; Fakoor et al., 2021; Ghasemipour
et al., 2021; Buckman et al., 2020) and model-based perspectives (Yu et al., 2020; Kidambi et al.,
2020; Matsushima et al., 2020; Yin et al., 2021). More or less, their methods rely on the idea
of pessimism and its variants in the sense that the learned policy can avoid uncertain regions not
covered by ofﬂine data. As a theoretical side, Munos & Szepesv´ari (2008); Duan et al. (2020;
2021); Fan et al. (2020) proved FQI has a PAC (probably approximately correct) guarantee under
realizability, the global coverage, and Bellman completeness conditions. Other ofﬂine model-free
RL methods such as minimax ofﬂine RL methods also require realizability and the global coverage
(Chen & Jiang, 2019; Antos et al., 2008; Uehara et al., 2021; Duan et al., 2021; Zhang et al., 2020;
Nachum et al., 2019). Recently, by leveraging the aforementioned the pessimism idea, Jin et al.
(2020a); Rajaraman et al. (2020) showed that pessimistic FQI can be applied to partial coverage
setting for linear and tabular MDPs. Comparing to their works, our analysis focuses on model-
based approaches with general function approximation. The ofﬂine model-based method is known
to have a PAC guarantee under the realizability and the global coverage (Ross & Bagnell, 2012;
Chen & Jiang, 2019). As the most closely related work, Chang et al. (2021) proved a model-based
method with an additional penalty term can weaken the assumption from the global coverage to the
partial coverage for structured MDPs such as KNRs and Gaussian Processes models (Deisenroth &
Rasmussen, 2011). In this work, we consider arbitrary MDPs with a realizable model class and aim
for PAC bounds under a partial coverage condition."
PRELIMINARIES,0.04257528556593977,"3
PRELIMINARIES"
PRELIMINARIES,0.04361370716510903,"We consider a Markov Decision process (MDP) M = {S, A, P, γ, r, d0} where P : S ×A →∆(S)
is the transition, r : S × A →[0, 1] is the reward function, γ ∈[0, 1) is the discount factor, and
d0 ∈∆(S) is the initial state distribution. A policy π maps from state (or history) to distribution
over actions. Given a policy π and a transition distribution P, V π
P denotes the expected cumulative
reward of π under P, d0 and r. Similarly, Qπ
P : S × A →R, Aπ
P : S × A →R are a Q-
function and advantage-function under P and π. Given a transition P, we denote π(P) as the
optimal policy associated with model P under reward r. We also denote dπ
P ⊂∆(S × A) as the
average state-action distribution of π under the transition model P, i.e, dπ
P = (1 −γ) P∞
t=0 γtdπ
P,t,"
PRELIMINARIES,0.0446521287642783,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.04569055036344756,"where dπ
P,t ∈∆(S × A) is the distribution of (st, at) under π and P at a time-step t. We denote the
true transition distribution as P ⋆, which we do not know in advance. For simplicity, we suppose r
is known. The extension to the unknown reward is straightforward."
PRELIMINARIES,0.04672897196261682,"In the ofﬂine RL setting, we have an ofﬂine distribution ρ ∈∆(S × A), and an ofﬂine dataset
D = {s(i), a(i), r(i), s′(i)}n
i=1 which is sampled in the following way: s, a ∼ρ, r = r(s, a), s′ ∼
P ⋆(·|s, a). We hope to obtain π(P ⋆) = arg maxπ V π
P ⋆from this ofﬂine dataset without any further
interaction with the environment. We often denote ED[f(s, a, s′)] = 1/n P"
PRELIMINARIES,0.04776739356178609,"(s,a,s′)∈D f(s, a, s′).
Our goal is to construct an ofﬂine RL algorithm Alg, which maps from D to π so that the subopti-
mality gap V π∗
P ⋆−V Alg(D)
P ⋆
for any comparator policy π∗∈Π is minimized, where Π in this work
can be an unrestricted policy class (e.g., including non-Markovian policies). Hereafter, c, c1, c2, · · ·
are always universal constants."
PRELIMINARIES,0.04880581516095535,"Partial coverage.
Throughout this work, we do not assume ρ has global coverage. The global cov-
erage in this work means that the density ratio based concentrability coefﬁcient dπ
P ⋆(s, a)/ρ(s, a)
is upper-bounded by some constant C ∈R+ for all polices π ∈Π , or the feature covariance matrix
corresponding to the ofﬂine distribution Es,a∼ρφ(s, a)φ(s, a)⊤(φ ∈S × A →R is a feature
representation) is full rank and has a non-zero minimum eigenvalue, which are commonly used
assumptions in ofﬂine RL (Munos, 2005; Antos et al., 2008; Chen & Jiang, 2019; Duan et al., 2020).
Under the full coverage, they show the output policy can compete with the globally optimal policy
π(P ⋆). However, this assumption may not be true in practice as computing an exploratory policy
itself is a challenging task for large-scale RL problems. Instead, we are interested in the partial
coverage setting such as dπ∗
P ⋆(s, a)/ρ(s, a) ≤C, which means the state-action occupancy measure
under some comparator policy π∗is covered by the ofﬂine dataset. We want to design an algorithm
that can compete against any policy π∗that is covered by the ofﬂine data. This assumption is much
weaker than the global coverage."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.04984423676012461,"4
PESSIMISTIC MODEL-BASED OFFLINE RL"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.05088265835929388,"We ﬁrst introduce a general model-based algorithm that has a PAC guarantee of the suboptimality
gap under partial coverage deﬁned with a newly introduced concentrability coefﬁcient. The algo-
rithm takes a realizable model class as input and outputs a policy that is as good as any comparator
policy that is covered by the ofﬂine data in the sense of the bounded concentrability coefﬁcient."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.05192107995846314,"Our algorithm, Constrained Pessimistic Policy Optimization (CPPO) (Algorithm 1), takes a realiz-
able hypothesis class M (with P ⋆∈M) consisting of |M| candidate models as input, computes
the maximum likelihood estimator (MLE) bPMLE using the given ofﬂine data D = {s, a, s′}. It then
forms a min-max objective subject to a constraint. The min-max objective introduces pessimism via
searching for the least favorable model P (in terms of its policy’s value V π
P ) that is feasible with re-
spect to the constraint. We can also express the constrained optimization procedure using a version
space MD and a policy optimization procedure deﬁned below:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.0529595015576324,"max
π∈Π
min
P ∈MD V π
P , where MD =
n
P | P ∈M, ED
h
TV( bPMLE(·|s, a), P(·|s, a))2i
≤ξ
o
,
(1)"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.05399792315680166,"where TV(P1, P2) is a total variation (TV) distance between two distributions P1 and P2. The
version space MD contains models that are not far away from bPMLE in terms of the average TV
distance under D. The version space is constructed such that with high probability P ⋆∈MD."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.055036344755970926,"Below we state the algorithm’s performance guarantee. Assuming for now that P ⋆∈MD holds
with high probability, then, ˆV π := minP ∈MD V π
P is a pessimistic policy evaluation estimator, which
satisﬁes ˆV π ≤V π
P ⋆for all π ∈Π. Using the idea of pessimism, we have the following observation:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.056074766355140186,"V π∗
P ⋆−V ˆπ
P ⋆= V π∗
P ⋆−ˆV π∗+ ˆV π∗−V ˆπ
P ⋆≤V π∗
P ⋆−ˆV π∗+ ˆV ˆπ −V ˆπ
P ⋆≤V π∗
P ⋆−ˆV π∗,"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.05711318795430945,"where the ﬁrst inequality uses ˆπ = arg maxπ∈Π ˆV π and the second inequality uses ˆV π ≤V π
P ⋆for
all π ∈Π. Thus, the ﬁnal error only incurs the policy evaluation error for the comparator policy π∗,
which leads to the error only depending on the concentrability coefﬁcient for the comparator policy."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.058151609553478714,We deﬁne the following new concentrability coefﬁcient that uses the model class M :
PESSIMISTIC MODEL-BASED OFFLINE RL,0.059190031152647975,Published as a conference paper at ICLR 2022
PESSIMISTIC MODEL-BASED OFFLINE RL,0.060228452751817235,Algorithm 1 Constrained Pessimistic Policy Optimization (CPPO)
PESSIMISTIC MODEL-BASED OFFLINE RL,0.0612668743509865,"1: Require: Models M, dataset D, parameter ξ, policy class Π (note Π could be unrestricted)
2: Obtain the estimator ˆPMLE by MLE: bPMLE = arg maxP ∈M ED[ln P(s′ | s, a)].
3: Constrained policy optimization:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06230529595015576,"ˆπ = arg maxπ∈Π minP ∈M V π
P , s.t., ED
h
TV( bPMLE(·|s, a), P(·|s, a))2i
≤ξ."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06334371754932502,4: Return ˆπ
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06438213914849429,"Deﬁnition 1 (Model-based Concentrability Coefﬁcient). For a comparator policy π∗, we deﬁne the
concentrability coefﬁcient C†
π∗as follows:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06542056074766354,"C†
π∗= supP ′∈M"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06645898234683281,"E(s,a)∼dπ∗
P ⋆[TV(P ′(·|s,a),P ⋆(·|s,a))2]"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06749740394600208,"E(s,a)∼ρ[TV(P ′(·|s,a),P ⋆(·|s,a))2] ."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06853582554517133,"The following theorem shows CPPO learns a policy that competes against π∗when C†
π∗< ∞."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.0695742471443406,"Theorem 1 (PAC Bound for CPPO with general function class). Assume P ⋆∈M. We set ξ =
c1
ln(c2|M|/δ)"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07061266874350987,"n
. Then, with probability 1 −δ, for any comparator policy π∗∈Π (Π can be the
unrestricted policy class containing non-Markovian policies),"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07165109034267912,"V π∗
P ⋆−V ˆπ
P ⋆≤c3(1 −γ)−2
q"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07268951194184839,"C†
π∗ln(c2|M|/δ) n
."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07372793354101766,"To the best of our knowledge, this is the ﬁrst algorithm that achieves a PAC guarantee for any
MDPs under the partial coverage assumption C†
π∗< ∞with only a realizable hypothesis class.
We emphasize that the inequality in the above uniformly holds for all policies with probability
1 −δ including history-dependent non-Markovian policies (see Remark 2). Note that the ability
to compete against non-Markovian policies in ofﬂine RL is meaningful when the ofﬂine data does
not cover the optimal policy π⋆(i.e., there could be a high-quality history-dependent policy that
is covered by the ofﬂine data against which we want to compete). In model-free approaches, this
type of result generally cannot be obtained. Indeed, the model-free approach from Xie et al. (2021)
requires Π to be a restricted Markovian policy class, since their bound contains poly(ln(|Π|))
dependence. For the detailed discussion, refer to Remark 1."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07476635514018691,"The quantity C†
π∗adaptively captures the discrepancy between the ofﬂine data and the state-action
occupancy measure under a comparator policy π∗depending on the model class M. For example,
C†
π∗can be reduced to a relative condition number in KNRs. Besides, it is always upper bounded by
the density ratio based concentrability coefﬁcient:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07580477673935618,"Cπ∗,∞:= sup(s,a)
dπ∗
P ⋆(s,a)"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07684319833852545,"ρ(s,a) ."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.0778816199376947,"One extreme case is that functions in M are all the same, which implies C†
π∗= 1 regardless."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07892004153686397,"Theorem 1 consider the case where the hypothesis class M is ﬁnite. When the hypothesis class is
inﬁnite, we can still obtain the PAC guarantee by utilizing the generalized result in Section A for any
realizable model class with valid statistical complexity (e.g., localized Rademacher complexity)."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07995846313603323,"Prior works that achieve PAC guarantees with only realizable model classes rely on much stronger
global coverage supπ Cπ,∞< ∞(Chen & Jiang, 2019). Even when the comparator policy is
the optimal policy π(P ⋆), the partial coverage condition Cπ(P ⋆),∞< ∞is weaker.
Existing
pessimistic model-based algorithms and their theoretical results (Chang et al., 2021) often assume
that a point-wise model uncertainty measure is given as a by-product of model ﬁtting, which limits
the applicability to special linear models such as KNRs/GPs. CPPO can work for any MDPs with
the realizable function class having a valid statistical complexity such that the MLE properly works."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08099688473520249,"Remark 1 (Comparison to the model-free approach from Xie et al. (2021); Zanette et al. (2021)
). Xie et al. (2021) study the model-free setting where the function class Q models Q functions
assumed to be Bellman complete for any Markovian policy in Π. While directly comparing model-
based approaches to model-free approaches is hard as they use different inductive biases in function
classes, we can leverage the approach from Chen & Jiang (2019, Corollary 6) to convert a model"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08203530633437175,Published as a conference paper at ICLR 2022
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08307372793354102,"class M to a pair of Q and Π class. Speciﬁcally, we can convert a model class M to a pair of Q
class and Π class such that Q will be realizable and also Bellman complete with respect to all π ∈Π.
After such conversion from the model-based setting to the model-free setting, running the algorithm
from Xie et al. (2021) using Q and Π achieves V π∗
P ⋆−V ˆπ
P ⋆=
p"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08411214953271028,"C⋄ln(|M||Π|/n), ∀π∗∈Π, where
C⋄is some concentrability coefﬁcient. For the detailed derivation, we refer readers to Appendix
D. Since the suboptimality gap from such conversion incurs log |Π|, a policy class Π cannot be too
large. Especially, unlike our results, it cannot take the unrestricted policy class as Π. This restriction
cannot be ﬁxed even if we use natural policy gradient (NPG) algorithms unless models have special
structures (Xie et al., 2021; Zanette et al., 2021). The details are given in Section D."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08515057113187954,"Thus, our theorem indicates two advantages of model-based approaches: (1) realizability in function
class is enough to ensure a PAC guarantee under a partial coverage condition, (2) it can compete
against a larger pool of candidate policies including history-dependent non-Markovian policies,
which is a meaningful property when the ofﬂine data does not cover the globally optimal policy.
Next, we demonstrate another key advantage of our approach which is its ﬂexibility to be seam-
lessly applied to MDPs with special structures."
EXAMPLES WITH REFINED CONCENTRABILITY COEFFICIENTS,0.08618899273104881,"5
EXAMPLES WITH REFINED CONCENTRABILITY COEFFICIENTS"
EXAMPLES WITH REFINED CONCENTRABILITY COEFFICIENTS,0.08722741433021806,"In the previous section, our results apply to any MDP as long as its true transition belongs to a
function class M. In this section, we consider several concrete MDPs with additional structural
conditions. We show that by leveraging the additional structural conditions, we can reﬁne the model-
based concentrability coefﬁcient to more natural quantities. The examples that we discuss here are:
(1) linear mixture MDPs which generalize linear MDPs from Yang & Wang (2020) and tabular
MDPs, (2) KNRs which generalize LQRs, (3) low-rank MDPs, and (4) factored MDPs."
EXAMPLES WITH REFINED CONCENTRABILITY COEFFICIENTS,0.08826583592938733,"Before proceeding, we clarify CPPO cannot capture linear MDPs in Jin et al. (2020a) that is differ-
ent from the one (Yang & Wang, 2020) we use, and linear Bellman-complete MDPs (Duan et al.,
2020) without any modiﬁcation since MLE-based model learning is no longer applicable to them.
However, other objective functions for learning models could be applied to these models (e.g., see
the nonparametric model-based learning approach from Lykouris et al. (2021); Neu & Pike-Burke
(2020) in the online setting), which we leave it as a future work."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.0893042575285566,"5.1
TABULAR MDPS AND LINEAR MIXTURE MDPS
Tabular MDPs
Tabular MDPs are MDPs where the state and action spaces are ﬁnite. Although the
corresponding hypothesis class for tabular MDPs is inﬁnite, we can still run MLE, that is, estimating
P ⋆by the empirical distribution. Then, Algorithm 1 has the following guarantee."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09034267912772585,"Corollary 1 (PAC bound for tabular MDP). We set ξ = c1
|S|2|A| ln(n|S|A|c2/δ)"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09138110072689512,"n
. Then with proba-
bility 1 −δ, for all π∗∈Π,"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09241952232606439,"V π∗
P ⋆−V ˆπ
P ⋆≤c3(1 −γ)−2
q"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09345794392523364,"Cπ∗,∞|S|2|A| ln(n|S||A|c4/δ) n 
."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09449636552440291,"Here, for tabular MDPs with M = {P : P(·|s, a) ∈∆(S), ∀s, a}, the model-based concentrability
coefﬁcient in Deﬁnition 1 is equal to the density ratio based concentrability coefﬁcient Cπ∗,∞which
is the right quantity for small-size tabular MDPs."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09553478712357218,"Linear mixture MDPs
We deﬁne linear mixture MDPs (Ayoub et al., 2020; Modi et al., 2020).
Deﬁnition 2 (Linear mixture MDPs). Given a feature vector ψ : (S, A, S) →Rd, a linear mixture
MDP is an MDP where the ground truth transition is P ⋆(s′|s, a) := θ⋆⊤ψ(s, a, s′), θ⋆∈Rd."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09657320872274143,"By setting, ψ(s, a, s′) = µ(s′) N φ(s, a) (⊗denotes the Kronecker product), linear mixture MDPs
include the following linear MDPs (Yang & Wang, 2020):"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.0976116303219107,"Deﬁnition 3 (Linear MDPs). Linear MDP has P ⋆(s′|s, a) := Pd1
i=1
Pd2
j=1 M ⋆
ijµi(s′)φj(s, a) with
µ : S →Rd1 and φ : S × A →Rd2 are known features, and M ⋆⊂Rd1×d2."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09865005192107996,We use CPPO to learn on linear mixture MDPs. The corresponding M is
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09968847352024922,"MMix =

θ⊤ψ(s, a, s′) | θ ∈Θ ⊂Rd,
R
θ⊤ψ(s, a, s′)d(s′) = 1
∀(s, a)
	
."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10072689511941849,Published as a conference paper at ICLR 2022
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10176531671858775,"Given a function V
: S →R, deﬁne the state-action feature indexed by V as ψV (s, a) :=
R
ψ(s, a, s′)V (s′)d(s′), we have the following PAC guarantee."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.102803738317757,"Corollary 2 (PAC bound for linear mixture MDPs). Suppose infs,a,s′ P ⋆(s′ | s, a) ≥c3 > 0,
Θ = {θ : ∥θ∥2 ≤R}, ∥ψV (s, a)∥2 ≤1, ∀V ∈S →[0, 1] and P ⋆∈MMix. We set ξ =
c1d ln2(c2nR/δ)/n. Then, with probability 1 −δ, for any π∗in Π (again Π can be the unrestricted
policy class), CPPO outputs a policy ˆπ such that:"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10384215991692627,"V π∗
P ⋆−V ˆπ
P ⋆≤c4(1 −γ)−2 s"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10488058151609553,"min(dC†
π∗, d2 ¯Cπ∗,mix)ln2(c5nR/δ)"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.1059190031152648,"n
,
(2)"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10695742471443406,"where the concentrability coefﬁcient ¯Cπ∗,mix is deﬁned as:"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10799584631360332,"¯Cπ∗,mix :=
sup
P ∈ZP ⋆
sup
x∈Rd "
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10903426791277258,"
x⊤Σπ∗,ψV π∗
P x"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11007268951194185,"x⊤Σρ,ψV π∗
P x  "
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.1111111111111111,"with the localized class ZP ⋆:= {P : E(s,a)∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] ≤ξ}, Σρ,ψV π∗
P
="
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11214953271028037,"E(s,a)∼ρ[ψV π∗
P (s, a)ψV π∗
P (s, a)⊤], and Σπ∗,ψV π∗
P
= Es,a∼dπ∗
P ⋆[ψV π∗
P (s, a)ψV π∗
P (s, a)⊤]."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11318795430944964,"When specializing to linear MDPs, the above bound still holds with ¯Cπ∗,mix being replaced by the
relative condition number ¯Cπ∗:"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.1142263759086189,"¯Cπ∗:= sup
x∈Rd
xT Σπ∗x
x⊤Σρ, x, where Σρ = E(s,a)∼ρ[φ(s, a)φ(s, a)⊤], Σπ∗= E(s,a)∼dπ∗
P ⋆[φ(s, a)φ(s, a)⊤]."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11526479750778816,"This is the ﬁrst PAC-guarantee result in the ofﬂine setting under partial coverage ¯Cπ∗,mix < ∞for
linear mixture MDPs. ¯Cπ∗,mix is a newly-introduced concentrability coefﬁcient for linear mixture
MDPs. This coefﬁcient is measured on the integrated feature vectors φV (s, a) for V : S →[0, 1].
Note the class of V is localized, i.e., we consider state-value functions V π∗
P (s) for all P centered
around P ⋆under data distribution ρ (i.e., P ∈ZP ⋆). Such localization property ensures that
¯Cπ∗,mix ≤C†
π⋆(see Lemma 10 in Section F)."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11630321910695743,"Note that these relative condition number based quantiﬁers are always tighter than the density ratio
based concentrability coefﬁcients (i.e., max{ ¯Cπ∗, ¯Cπ∗,mix} ≤Cπ∗,∞). For the special case where
φ(s, a) is a one-hot encoding vector, then they are reduced to the density ratio based concentrability
coefﬁcient. In a non-tabular setting, even if when the density ratio is inﬁnite, the relative condition
number can be still ﬁnite. Intuitively, the bounded relative condition number implies that the ofﬂine
data covers the subspace that the comparator policy π∗visits."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11734164070612668,"We remark P ⋆(s′ | s, a) ≥c3 > 0 in Corollary 2 is a technical condition that allows us to calculate
the entropy integral of the hypothesis class easily. It can be potentially discarded by a more careful
argument following (van de Geer, 2000, Chapter 7). The norm assumption ∥ψV (s, a)∥2 < 1 is
commonly assumed in the online setting (Zhou et al., 2021)."
KERNELIZED NOLINEAR REGULATORS,0.11838006230529595,"5.2
KERNELIZED NOLINEAR REGULATORS"
KERNELIZED NOLINEAR REGULATORS,0.11941848390446522,"We consider the example of KNRs in this section. A kernelized Nonlinear Regulator (KNR) (Kakade
et al., 2020) is a model where the ground truth transition P ⋆(s′|s, a) is deﬁned as s′ = W ⋆φ(s, a)+ϵ,
ϵ ∼N(0, ζ2I), with φ : S × A →Rd being a possibly nonlinear feature mapping. We denote
the corresponding model on W by P(W). We can apply Algorithm 1 and obtain its guarantee.
Especially, since TV(P(W)(· | s, a), P(W ⋆)(· | s, a))2 = Θ(∥(W −W ⋆)φ(s, a)∥2
2) (Devroye
et al., 2018), C†
π∗is upper-bounded by the relative condition number ¯Cπ∗."
KERNELIZED NOLINEAR REGULATORS,0.12045690550363447,"Then, we can also recover the result of Chang et al. (2021) which proposes a reward penalty-based
pessimistic ofﬂine RL algorithm. The detail is given in Section B. In summary, we can show"
KERNELIZED NOLINEAR REGULATORS,0.12149532710280374,"V π∗
P ⋆−V ˆπ
P ⋆≤c1(1 −γ)−2 min(d1/2, ¯R)
√¯R
q"
KERNELIZED NOLINEAR REGULATORS,0.122533748701973,"dS ¯
Cπ∗ln(1+n) n
, ."
KERNELIZED NOLINEAR REGULATORS,0.12357217030114226,where ¯R := rank[Σρ]{rank[Σρ] + ln(c2/δ)} and dS is the dimension of the state.
KERNELIZED NOLINEAR REGULATORS,0.12461059190031153,Published as a conference paper at ICLR 2022
KERNELIZED NOLINEAR REGULATORS,0.1256490134994808,"This implies CPPO can learn a policy that can compete against π∗with partial coverage ¯Cπ∗< ∞.
Note that the condition ¯Cπ∗< ∞does not require Σρ to be full-rank. Also the bound uses rank[Σρ]
instead of d, which means that our bound is distribution dependent and is still valid even when
d = ∞as long as the ofﬂine data only concentrate on a low-dimensional subspace."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.12668743509865005,"5.3
LOW-RANK MDPS WITH REPRESENTATION LEARNING
We consider the representation learning in ofﬂine RL. Following FLAMBE (Agarwal et al., 2020b),
we study low-rank MDPs but in the ofﬂine setting. Note that low-rank MDPs here are a more
generalized model of the aforementioned linear MDPs (Yang & Wang, 2020) since the true feature
representation φ⋆in a low-rank MDP is unknown.
Deﬁnition 4 (Low rank MDPs). The ground-truth model P ⋆admits a low rank decomposition with
a dimension d if there exists two embedding functions µ∗: S →Rd, φ∗: S × A →Rd s.t.
P ⋆(s′ | s, a) = µ∗(s′)⊤φ∗(s, a). Neither µ∗nor φ∗is known to the learner."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.1277258566978193,"One interesting special case of a low-rank MDP is the following latent variable model (see Agarwal
et al. (2020b) for more details).
Deﬁnition 5 (Latent variable models). There exists a latent space Z along with functions µ∗: Z →
∆(S) and φ∗: S × A →∆(Z) s.t. P ⋆(· | s, a) = P"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.12876427829698858,"z∈Z µ∗(· | z)φ∗(z | s, a)."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.12980269989615784,"To tackle representation learning under partial coverage on low-rank MDPs, we setup function
classes as follows: given two function classes Ψ ⊂S →Rd, Φ ⊂S × A →Rd (both are re-
alizable in the sense that µ∗∈Ψ and φ∗∈Φ), we consider a hypothesis class {µ(s′)⊤φ(s, a); µ ∈
Ψ, φ ∈Φ}. Then, CPPO (Algorithm 1) and Theorem 1 still work under this setting. Note that this
function class setup is exactly the same as the one from FLAMBE."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.1308411214953271,"Here we show that by leveraging the low-rankness, we can reﬁne the concentrability coefﬁcient to
a relative condition number deﬁned by the unknown true representation φ∗. We emphasize that this
does not depend on the other features. Particularly, given a comparator policy π∗, we deﬁne ¯Cπ∗,φ⋆:"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13187954309449637,"¯Cπ∗,φ⋆= sup
x∈Rd
x⊤Σπ∗x"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13291796469366562,"x⊤Σρx ,
Σπ∗:= Es,a∼dπ∗
P ⋆φ∗(s, a)φ∗(s, a)⊤,
Σρ := Es,a∼ρφ∗(s, a)φ∗(s, a)⊤."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13395638629283488,"We can show CPPO learns a policy that can compete against π∗as long as ¯Cπ∗,φ⋆< ∞."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13499480789200416,"Theorem 2 (PAC bound for low-rank MDP). We set ξ
=
c1
ln(|Φ||Ψ|c2/δ)"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.1360332294911734,"n
.
Suppose (a):
∥φ(s, a)∥2 ≤1, ∀(s, a) ∈S ×A, ∀φ ∈Φ,
R
µ(s′)⊤φ(s, a)d(s′) = 1 and
R
∥µ(s)∥2ds ≤
√"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13707165109034267,"d, ∀µ ∈
Ψ, φ ∈Φ, (b) ρ(s, a) = dπb
P ⋆(s, a), (c) P ⋆(s′|s, a) = µ∗(s′)⊤φ∗(s, a) for some µ∗∈Ψ, φ∗∈Φ.
With probability at least 1 −δ, for all π∗∈Π (again Π can be an unrestricted policy class), CPPO
(Algorithm 1) ﬁnds ˆπ such that:"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13811007268951195,"V π∗
P ⋆−V ˆπ
P ⋆≤c3
q"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.1391484942886812,"¯Cπ∗,φ⋆ωπ∗rank(Σρ) ln(|Ψ||Φ|c4/δ)"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.14018691588785046,"(1−γ)4n
, ωπ∗=

max(s,a)
π∗(a|s)
πb(a|s)

(3)"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.14122533748701974,"To the best of our knowledge, this is the ﬁrst established PAC result under the partial coverage
condition ¯Cπ∗,φ⋆< ∞, ωπ∗< ∞for low-rank MDPs in the ofﬂine setting. We also emphasize
that our bound in Theorem 2 is distribution dependent, i.e., it depends on rank(Σρ) rather than
the exact rank d. Note that rank(Σρ) ≤d, and rank(Σρ) could be much smaller than d when the
ofﬂine distribution only concentrates on a low-dimensional subspace (deﬁned using φ∗). Note that
the assumption that ωπ∗< ∞does not imply the state-action density ratio Cπ∗,∞is small. Indeed,
ωπ∗< ∞is much weaker than Cπ∗,∞< ∞."
FACTORED MDPS,0.142263759086189,"5.4
FACTORED MDPS
The last example we include is the factored MDP (Kearns & Koller, 1999) deﬁned as follows:
Deﬁnition 6 (Factored MDPs). Let d ∈N+ and O being a small ﬁnite set. The state space S = Od,
and for each state s, we denote s[i] ∈O as the i-th variable of the state s. For each i ∈[1, · · · , d],
the parents of i, pai ⊂[1, · · · , d], is the subset of state variables that directly inﬂuences i, i.e., the
transition is deﬁned as follows:"
FACTORED MDPS,0.14330218068535824,"∀s, a, s′ : P ⋆(s′|s, a) = Qd
i=1 P ⋆
i (s′[i]|s[pai], a)."
FACTORED MDPS,0.14434060228452752,"We will denote Si = O|pai|, and given s ∈S, we will have s[pai] ∈Si"
FACTORED MDPS,0.14537902388369678,Published as a conference paper at ICLR 2022
FACTORED MDPS,0.14641744548286603,"Due to the factorization, the transition operator P ⋆can be described with L := Pd
i=1 |A||O|1+|pai|"
FACTORED MDPS,0.1474558670820353,"many parameters. In contrast, the non-factored transition will need O(|O|d) parameters. When
|pai| ≪d ∀i, it is expected that we can learn this model with lower sample complexity by leveraging
the factorization which has been demonstrated in the online setting (Kearns & Koller, 1999). We
remark a factored MDP is an example where model-based approaches are necessary as neither the
optimal policy nor the Q functions are factored (Koller & Parr, 2000)."
FACTORED MDPS,0.14849428868120457,"We will slightly modify Algorithm 1 to take the factorization into consideration. First, we perform
MLE for model learning: each factor P ⋆
i is independently learned via MLE:"
FACTORED MDPS,0.14953271028037382,"∀i ∈[d], bPMLE,i = arg maxP ED[ln P(s′[i]|s[pai], a)],
bP = Q"
FACTORED MDPS,0.1505711318795431,"i bPMLE,i.
Next, the constrained policy optimization procedure is deﬁned as"
FACTORED MDPS,0.15160955347871236,"ˆπ = arg max
π
min
P :=Q"
FACTORED MDPS,0.1526479750778816,"i Pi V π
P , s.t., ED[TV(Pi(· | s, a), bPMLE,i(· | s, a))2] ≤ξi (∀i ∈[1, · · · , d])."
FACTORED MDPS,0.1536863966770509,"Note that in the above objective, there is no restriction on the policy, i.e., the arg max operator
searches over all possible policies including non-Markovian ones."
FACTORED MDPS,0.15472481827622014,"To analyze the performance of the above modiﬁed CPPO, we introduce a specialized con-
centration coefﬁcient for factored MDPs that utilizes the factored structure.
We focus on
density ratio based concentrability coefﬁcients since in a factored MDP with the function class
M := {P = Q
i Pi : Pi ∈Si × A →∆(O)}, the concentrability coefﬁcient associated with
M in Deﬁnition 1 will be reduced to the density ratio. For any π∗, we deﬁne the concentrability
coefﬁcients for the factored MDP as follows:"
FACTORED MDPS,0.1557632398753894,"¨Cπ∗,∞:=
max
j∈[1,··· ,d]
max
sj∈Sj,a∈A
dπ∗
P ⋆(sj, a)
ρ(sj, a) ,"
FACTORED MDPS,0.15680166147455868,"where for sj ∈Sj, we denote ν(sj, a) := P"
FACTORED MDPS,0.15784008307372793,"s∈S:s[paj]=sj ν(s, a) for any distribution ν ∈∆(S×A)."
FACTORED MDPS,0.1588785046728972,"Comparing to Cπ∗,∞deﬁned on the original state space S, here ¨Cπ∗,∞is deﬁned over each state
space Sj associated with each factor j. Note that when |paj| = Θ(1), |Sj| is exponentially smaller
than |S|. One can verify that ¨Cπ∗,∞≤Cπ∗,∞(see Appendix E.7), where Cπ∗,∞ignores the
factored structure and treat S as a whole single space. This formally demonstrates the beneﬁt of the
factored structure in terms of the coverage condition in ofﬂine RL."
FACTORED MDPS,0.15991692627206647,"With the new deﬁnition of the concentrability coefﬁcients, now we are ready to state the PAC bound
of CPPO for factored MDPs. Recall L := Pd
i=1 Li, Li = |A||O|1+|pai|."
FACTORED MDPS,0.16095534787123572,"Theorem 3 (PAC bound for factored MDP). We set ξi = c1
Li ln(Lic2d/δ)"
FACTORED MDPS,0.16199376947040497,"n
. Then with probability
1 −δ, CPPO ﬁnds a policy ˆπ such that for all comparator policy π∗∈Π (Π can be unrestricted),"
FACTORED MDPS,0.16303219106957426,"V π∗
P ⋆−V ˆπ
P ⋆≤c3(1 −γ)−2
q"
FACTORED MDPS,0.1640706126687435,"d ¨
Cπ∗,∞L·ln(nLc4d/δ) n
."
FACTORED MDPS,0.16510903426791276,"Note that our sub-optimality gap scales polynomially with respect to L, i.e., the complexity of the
factored MDP, rather than |S| which can be Ω(exp(d))."
CONCLUSION,0.16614745586708204,"6
CONCLUSION"
CONCLUSION,0.1671858774662513,"We study model-based ofﬂine RL with function approximation under partial coverage. We show that
for the model-based setting, realizability in function class and partial coverage together are enough
to learn a policy that is comparable to any policies (including history-dependent policies) covered by
the ofﬂine distribution. Our result demonstrates a sharp contrast to model-free ofﬂine RL approaches
which often require additional structural conditions in the function class (e.g., Bellman completion)
and have restrictions on the pool of candidate policies that they can compete against."
CONCLUSION,0.16822429906542055,"Some readers might wonder whether CPPO is computationally efﬁcient. The minimax optimiza-
tion problem arg maxπ∈Π minP ∈M V π
P ﬁts into a framework of planning on robust MDPs (Nilim
& El Ghaoui, 2005; Iyengar, 2005). By introducing a robust Bellman equation, they proposed value
iteration and policy iteration algorithms, and showed that algorithms are practically tractable in the
tabular setting. In the non-tabular setting, Lim & Autef (2019); Tamar et al. (2014) propose the ex-
tension using function approximation. Thus, we can apply their methods to approximately solve the
minimax optimization problem in a model-free fashion. We leave the formal theoretical justiﬁcation
when using these approximation planning algorithms as an important direction for future work."
CONCLUSION,0.16926272066458983,Published as a conference paper at ICLR 2022
CONCLUSION,0.1703011422637591,ACKNOWLEDGEMENT
CONCLUSION,0.17133956386292834,"The authors would like to thank Nan Jiang, Tengyang Xie for valuable feedback."
CONCLUSION,0.17237798546209762,Masatoshi Ueharra is partially supported by Masason foundation.
REFERENCES,0.17341640706126688,REFERENCES
REFERENCES,0.17445482866043613,"Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed ex-
ploration for provable policy gradient learning. In Advances in Neural Information Processing
Systems, volume 33, pp. 13399–13412, 2020a."
REFERENCES,0.1754932502596054,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complex-
ity and representation learning of low rank mdps. In Advances in Neural Information Processing
Systems, volume 33, pp. 20095–20107, 2020b."
REFERENCES,0.17653167185877466,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approxima-
tion with policy gradient methods in markov decision processes. In Proceedings of Thirty Third
Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp.
64–66, 2020c."
REFERENCES,0.17757009345794392,"Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning,
71:89–129, 2008."
REFERENCES,0.1786085150571132,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463–474. PMLR, 2020."
REFERENCES,0.17964693665628245,"Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in ﬁxed-
dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020."
REFERENCES,0.1806853582554517,"Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors and
dynamics models: Improving performance and domain transfer in ofﬂine rl.
arXiv preprint
arXiv:2106.09119, 2021."
REFERENCES,0.181723779854621,"Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
covariate shift in imitation learning via ofﬂine data without great coverage. 2021."
REFERENCES,0.18276220145379024,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 1042–
1051, 2019."
REFERENCES,0.1838006230529595,"Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed Chi.
Top-k
off-policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM
International Conference on web search and data mining, WSDM ’19, pp. 456–464, 2019."
REFERENCES,0.18483904465212878,"Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efﬁcient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465–472. Citeseer, 2011."
REFERENCES,0.18587746625129803,"Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-
dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018."
REFERENCES,0.18691588785046728,"Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear func-
tion approximation. In Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pp. 2701–2709, 2020."
REFERENCES,0.18795430944963656,"Yaqi Duan, Chi Jin, and Zhiyuan Li. Risk bounds and rademacher complexity in batch reinforcement
learning. arXiv preprint arXiv:2103.13883, 2021."
REFERENCES,0.18899273104880582,"Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503–556, 2005."
REFERENCES,0.19003115264797507,Published as a conference paper at ICLR 2022
REFERENCES,0.19106957424714435,"Rasool Fakoor, Jonas Mueller, Pratik Chaudhari, and Alexander J Smola. Continuous doubly con-
strained batch reinforcement learning. arXiv preprint arXiv:2102.09225, 2021."
REFERENCES,0.1921079958463136,"Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume
120 of Proceedings of Machine Learning Research, pp. 486–489, 2020."
REFERENCES,0.19314641744548286,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019."
REFERENCES,0.19418483904465214,"Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective ofﬂine and online rl. In International Conference
on Machine Learning, pp. 3682–3691. PMLR, 2021."
REFERENCES,0.1952232606438214,"Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesv´ari, and Mengdi Wang. Sparse feature selec-
tion makes batch reinforcement learning more sample efﬁcient. In International Conference on
Machine Learning, pp. 4063–4073. PMLR, 2021."
REFERENCES,0.19626168224299065,"Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257–280, 2005."
REFERENCES,0.19730010384215993,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement learn-
ing with linear function approximation. In Proceedings of Thirty Third Conference on Learning
Theory, volume 125 of Proceedings of Machine Learning Research, pp. 2137–2143, 2020a."
REFERENCES,0.19833852544132918,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efﬁcient for ofﬂine rl? arXiv
preprint arXiv:2012.15085, 2020b."
REFERENCES,0.19937694704049844,"Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Infor-
mation theoretic regret bounds for online nonlinear control. In Advances in Neural Information
Processing Systems, volume 33, pp. 15312–15325, 2020."
REFERENCES,0.20041536863966772,"Michael Kearns and Daphne Koller. Efﬁcient reinforcement learning in factored mdps. In IJCAI,
volume 16, pp. 740–747, 1999."
REFERENCES,0.20145379023883697,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofﬂine reinforcement learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21810–21823. Curran Associates, Inc., 2020."
REFERENCES,0.20249221183800623,"Daphne Koller and Ronald Parr. Policy iteration for factored mdps. In Proceedings of the Sixteenth
conference on Uncertainty in artiﬁcial intelligence, pp. 326–334, 2000."
REFERENCES,0.2035306334371755,"Michael R. Kosorok and Eric B. Laber. Precision medicine. 6:263–286, 2019."
REFERENCES,0.20456905503634476,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.205607476635514,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.2066458982346833,"Shiau Hong Lim and Arnaud Autef. Kernel-based reinforcement learning in robust markov decision
processes. In International Conference on Machine Learning, pp. 3973–3981. PMLR, 2019."
REFERENCES,0.20768431983385255,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy
reinforcement learning without great exploration. In Advances in Neural Information Processing
Systems, volume 33, pp. 1264–1274, 2020."
REFERENCES,0.2087227414330218,"Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration
in episodic reinforcement learning. In Conference on Learning Theory, pp. 3242–3245. PMLR,
2021."
REFERENCES,0.20976116303219106,"Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oﬁr Nachum, and Shixiang Gu. Deployment-
efﬁcient reinforcement learning via model-based ofﬂine optimization. ICLR, 2020."
REFERENCES,0.21079958463136034,Published as a conference paper at ICLR 2022
REFERENCES,0.2118380062305296,"Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 2010–2020. PMLR, 2020."
REFERENCES,0.21287642782969884,"Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps. arXiv preprint arXiv: 2102.07035,
2021."
REFERENCES,0.21391484942886813,"R´emi Munos. Error bounds for approximate value iteration. In Proceedings of the National Confer-
ence on Artiﬁcial Intelligence, volume 20, pp. 1006. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2005."
REFERENCES,0.21495327102803738,"R´emi Munos and Csaba Szepesv´ari. Finite-time bounds for ﬁtted value iteration. Journal of Machine
Learning Research, 9(May):815–857, 2008."
REFERENCES,0.21599169262720663,"Oﬁr Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019."
REFERENCES,0.21703011422637591,"Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning.
arXiv preprint arXiv:2007.01891, 2020."
REFERENCES,0.21806853582554517,"Chengzhuo Ni, Anru Zhang, Yaqi Duan, and Mengdi Wang. Learning good state and action repre-
sentations via tensor decomposition. arXiv preprint arXiv:2105.01136, 2021."
REFERENCES,0.21910695742471442,"Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780–798, 2005."
REFERENCES,0.2201453790238837,"Matteo Papini, Andrea Tirinzoni, Marcello Restelli, Alessandro Lazaric, and Matteo Pirotta. Lever-
aging good representations in linear contextual bandits. arXiv preprint arXiv:2104.03781, 2021."
REFERENCES,0.22118380062305296,"Nived Rajaraman, Lin F Yang, Jiantao Jiao, and Kannan Ramachandran. Toward the fundamental
limits of imitation learning. arXiv preprint arXiv:2009.05990, 2020."
REFERENCES,0.2222222222222222,"Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofﬂine rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021."
REFERENCES,0.2232606438213915,"Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, L´eonard Hussenot, Olivier Bachem, Olivier
Pietquin, and Matthieu Geist. Ofﬂine reinforcement learning as anti-exploration. arXiv preprint
arXiv:2106.06431, 2021."
REFERENCES,0.22429906542056074,"St´ephane Ross and J Andrew Bagnell. Agnostic system identiﬁcation for model-based reinforcement
learning. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, pp. 1905–1912, 2012."
REFERENCES,0.22533748701973,"Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of
Proceedings of Machine Learning Research, pp. 2898–2933, 2019."
REFERENCES,0.22637590861889928,"Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation. In
International conference on machine learning, pp. 181–189. PMLR, 2014."
REFERENCES,0.22741433021806853,"Ahmed Touati, Amy Zhang, Joelle Pineau, and Pascal Vincent. Stable policy optimization via off-
policy divergence regularization. arXiv preprint arXiv:2003.04108, 2020."
REFERENCES,0.2284527518172378,"Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-
policy evaluation. In Proceedings of the 37th International Conference on Machine Learning, pp.
9659–9668, 2020."
REFERENCES,0.22949117341640707,"Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie.
Finite sample analysis of minimax ofﬂine reinforcement learning: Completeness, fast rates and
ﬁrst-order efﬁciency. arXiv preprint arXiv:2102.02981, 2021."
REFERENCES,0.23052959501557632,Published as a conference paper at ICLR 2022
REFERENCES,0.23156801661474558,"S van de Geer. Empirical Processes in M-Estimation. Cambridge Series in Statistical and Proba-
bilistic Mathematics. Cambridge University Press, 2000."
REFERENCES,0.23260643821391486,"Martin J Wainwright.
High-Dimensional Statistics : A Non-Asymptotic Viewpoint.
Cambridge
University Press, New York, 2019."
REFERENCES,0.2336448598130841,"Ruosong Wang, Dean P. Foster, and Sham M. Kakade. What are the statistical limits of ofﬂine rl
with linear function approximation?. arXiv preprint arXiv:2010.11895, 2020."
REFERENCES,0.23468328141225336,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.23572170301142265,"Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. arXiv
preprint arXiv:2008.04990, 2020."
REFERENCES,0.2367601246105919,"Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for ofﬂine reinforcement learning. arXiv preprint arXiv:2106.06926, 2021."
REFERENCES,0.23779854620976115,"Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In Proceedings of the 37th International Conference on Machine Learning, pp.
10746–10756, 2020."
REFERENCES,0.23883696780893043,"Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal ofﬂine reinforcement learning via double
variance reduction. arXiv preprint arXiv:2102.01748, 2021."
REFERENCES,0.2398753894080997,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based ofﬂine policy optimization. In Advances in Neural
Information Processing Systems, volume 33, pp. 14129–14142, 2020."
REFERENCES,0.24091381100726894,"Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable beneﬁts of actor-critic meth-
ods for ofﬂine reinforcement learning. arXiv preprint arXiv:2108.08812, 2021."
REFERENCES,0.24195223260643822,"Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized ofﬂine estimation of
stationary values. In International Conference on Learning Representations, 2020."
REFERENCES,0.24299065420560748,"Weitong Zhang, Jiafan He, Dongruo Zhou, Amy Zhang, and Quanquan Gu.
Provably efﬁcient
representation learning in low-rank markov decision processes. arXiv preprint arXiv:2106.11935,
2021a."
REFERENCES,0.24402907580477673,"Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust ofﬂine reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021b."
REFERENCES,0.245067497403946,"Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learn-
ing for linear mixture markov decision processes. In Conference on Learning Theory, pp. 4532–
4576. PMLR, 2021."
REFERENCES,0.24610591900311526,Published as a conference paper at ICLR 2022
REFERENCES,0.24714434060228452,"A
GENERALIZATION OF THEOREM 1"
REFERENCES,0.2481827622014538,"We present the generalized version of Theorem 1 when the hypothesis class is inﬁnite. We deﬁne
the modiﬁed function class of M: H = (r"
REFERENCES,0.24922118380062305,P + P ∗
REFERENCES,0.2502596053997923,"2
| P ∈M ) ."
REFERENCES,0.2512980269989616,"Given a function class F, let N[](δ, F, d) be the bracketing number of F w.r.t the metric d(a, b)
given by"
REFERENCES,0.2523364485981308,"d(a, b) = E(s,a)∼ρ"
REFERENCES,0.2533748701973001,"Z
(a(s′ | s, a) −b(s′ | s, a))2d(s′)
1/2
."
REFERENCES,0.2544132917964694,"Then, the entropy integral of F is given by"
REFERENCES,0.2554517133956386,"JB(δ, F, d) = max Z δ"
REFERENCES,0.2564901349948079,"δ2/2
(log N[](u, F, d))1/2du, δ ! .
(4)"
REFERENCES,0.25752855659397716,We also deﬁne the localized class of H:
REFERENCES,0.2585669781931464,"H(δ) = {h ∈H : E(s,a)∼ρ[h2(P(· | s, a)∥P ⋆(· | s, a))] ≤δ2},"
REFERENCES,0.25960539979231567,"where h(P(· | s, a)∥P ⋆(· | s, a)) denotes Hellinger distance deﬁned by

0.5
Z
{
p"
REFERENCES,0.26064382139148495,"P(s′ | s, a) −
p"
REFERENCES,0.2616822429906542,"P ⋆(s′ | s, a)}2d(s′)
1/2
."
REFERENCES,0.26272066458982346,"Based on Theorem 7.4 (van de Geer, 2000), the MLE has the following guarantee."
REFERENCES,0.26375908618899274,"Theorem 4 (MLE guarantee with general function approximation). We take a function G(ϵ) :
[0, 1] →R s.t. G(ϵ) ≥JB[ϵ, H(ϵ), d] and G(ϵ)/ϵ2 is a non-increasing function w.r.t ϵ. Then,
letting ξn be a solution to √nϵ2 ≥cG(ϵ) w.r.t ϵ. With probability 1 −δ, we have"
REFERENCES,0.26479750778816197,"E(s,a)∼ρ[∥ˆPMLE(· | s, a) −P(· | s, a)∥2
1] ≤c1
n
ξn +
p"
REFERENCES,0.26583592938733125,"log(c2/δ)/n
o2
."
REFERENCES,0.26687435098650053,"We remark that the original guarantee in (van de Geer, 2000) is given for the estimation of uncondi-
tional distributions. The adaption to the conditional case is straightforward. For more details, refer
to Section A.1. Besides, when we assume the convexity of the function class, the entropy integral
with bracketing (4) can be replaced with the entropy integral with covering number (Wainwright,
2019, Chapter 14)."
REFERENCES,0.26791277258566976,"By using the above notation and the MLE guarantee, we can generalize Theorem 1."
REFERENCES,0.26895119418483904,"Theorem 5 (Finite sample error bound of CPPO with an inﬁnite hypothesis class). Assume P ⋆∈
M. Let f(P)(s, a) = TV(P(· | s, a), P ⋆(· | s, a))2. Deﬁne"
REFERENCES,0.2699896157840083,"M1 =

P : E(s,a)∼ρ [f(P)(s, a)] ≤c

ξ2
n + ln(c/δ) n 
,"
REFERENCES,0.27102803738317754,"M2 =

P : E(s,a)∼D[f(P)(s, a)] ≤c

G(M1) + ξ2
n + ln(c/δ) n 
,"
REFERENCES,0.2720664589823468,"G(M1) = E[ sup
P ∈M1
|(ED −Eρ)[f(P)]|], G(M2) = E[ sup
P ∈MD
|(ED −Eρ)[f(P)]|]."
REFERENCES,0.2731048805815161,"Here, in G(M1) and G(M2), the expectation is taken over the data. We set ξ = cG(M1) + cξ2
n +"
REFERENCES,0.27414330218068533,"c

ln(c/δ)"
REFERENCES,0.2751817237798546,"n

. Then, for all π∗∈Π, we have"
REFERENCES,0.2762201453790239,"V π∗
P ⋆−V ˆπ
P ⋆≤(1 −γ)−2c1
q C†
π∗ r"
REFERENCES,0.2772585669781931,"G(M2) + G(M1) + ξ2n + ln(c/δ) n
."
REFERENCES,0.2782969885773624,Published as a conference paper at ICLR 2022
REFERENCES,0.2793354101765317,"This theorem shows once we can calculate GM1, GM2 and ξn, we can obtain the tight rate. Im-
portantly, GM1 and GM2 are upper-bounded by the localized versions of Rademacher complexities
based on symmetrization argument. Hence, their rates are faster than the ones of the nonlocalized
versions."
REFERENCES,0.2803738317757009,"For example, when |M| is ﬁnite, we ﬁrst have ξn =
p"
REFERENCES,0.2814122533748702,"ln(|M|c/δ)/n. Then, from Bernstein’s
inequality (Wainwright, 2019, Exercise 2.8) and union bound, G(M1) is upper-bounded by"
REFERENCES,0.2824506749740395,"G(M1) ≲
ξn
|{z}
Variance term
×
p"
REFERENCES,0.2834890965732087,"ln(|M|c/δ)/n
|
{z
}
Union bound"
REFERENCES,0.284527518172378,"≲ξ2
n."
REFERENCES,0.28556593977154726,"Similarly, from empirical Bernstein’s inequality,"
REFERENCES,0.2866043613707165,"G(M2) ≲
ξn
|{z}
Variance term
×
p"
REFERENCES,0.28764278296988577,"ln(|M|c/δ)/n
|
{z
}
Union bound"
REFERENCES,0.28868120456905505,"≲ξ2
n."
REFERENCES,0.2897196261682243,"Then, we can obtain the result of Theorem 1:"
REFERENCES,0.29075804776739356,"V π∗
P ⋆−V ˆπ
P ⋆≤(1 −γ)−2c1
q C†
π∗ r"
REFERENCES,0.29179646936656284,ln(|M|c/δ)
REFERENCES,0.29283489096573206,"n
,
∀π∗∈Π."
REFERENCES,0.29387331256490135,"We stress if we use Hoeffeding’s inequality above, we would immediately get the slower rate
O(n−1/4). To calculate G(M1) and G(M2) in a tight manner, we need to leverage the knowl-
edge that the variance of each element in M1 and M2 is controlled from the restriction P ∈M1 or
P ∈M2."
REFERENCES,0.2949117341640706,"A.1
RATE OF CONVERGENCE OF MAXIMUM LIKELIHOOD ESTIMATION WITH INFINITE
HYPOTHESIS CLASS"
REFERENCES,0.29595015576323985,"We aim for obtaining a PAC guarantee of MLE following (van de Geer, 2000). We explain how we
should modify the proof of van de Geer (2000) for unconditional density estimation to conditional
density estimation. For simplicity, we assume P ⋆> 0."
REFERENCES,0.29698857736240913,We ﬁrst introduce the notation:
REFERENCES,0.2980269989615784,"¯P = (P + P ⋆)/2, gP = 0.5 log
¯P
P ⋆, ( ¯
M)1/2 =
np"
REFERENCES,0.29906542056074764,"¯P | P ∈M
o
."
REFERENCES,0.3001038421599169,Recall
REFERENCES,0.3011422637590862,"h2(P1(· | s, a), P2(· | s, a)) =

0.5
Z
P 1/2
1
(s′ | s, a) −P 1/2(s′ | s, a)d(s′)
0.5
."
REFERENCES,0.30218068535825543,"Here, from Lemma 4.2 (van de Geer, 2000), the following holds:
Lemma 1 (Some property of Hellinger distance)."
REFERENCES,0.3032191069574247,"E(s,a)∼ρ[h2( ¯P1(· | s, a), ¯P2(· | s, a))] ≤0.5E(s,a)∼ρ[h2(P1(· | s, a), P2(· | s, a))],"
REFERENCES,0.304257528556594,"E(s,a)∼ρ[h2(P(· | s, a), P ⋆(· | s, a))] ≤E(s,a)∼ρ[16h2( ¯P(· | s, a), P ⋆(· | s, a))]."
REFERENCES,0.3052959501557632,"We also recall Hellinger distance is stronger than TV distance:
Lemma 2 (Relation of Hellinger distance and TV distance )."
REFERENCES,0.3063343717549325,"TV(P1(· | s, a), P2(· | s, a)) ≤
√"
REFERENCES,0.3073727933541018,"2h(P1(· | s, a), P2(· | s, a))."
REFERENCES,0.308411214953271,"The following lemma is useful to connect the log-loss and the Hellinger distance.
Lemma 3 (Basic Inequality for MLE)."
REFERENCES,0.3094496365524403,"E(s,a)∼ρ[h2( ˆPMLE(· | s, a), P ⋆(· | s, a))] ≤(ED −E(s,a)∼ρ,s′∼P ⋆(·|s,a))[gP (s, a, s′)]."
REFERENCES,0.31048805815160957,"This is proved by Lemma 4.1 (van de Geer, 2000). To simplify the notation, we deﬁne"
REFERENCES,0.3115264797507788,"H2(P1, P2) = E(s,a)∼ρ[h2(P1(· | s, a), P2(· | s, a))]."
REFERENCES,0.3125649013499481,Published as a conference paper at ICLR 2022
REFERENCES,0.31360332294911736,"Here, our goal is showing with probability 1 −δ,"
REFERENCES,0.3146417445482866,"H2( ˆPMLE, P ⋆) ≤{ξn +
p"
REFERENCES,0.31568016614745587,log(c2/δ)/n}2.
REFERENCES,0.31671858774662515,"This is proved by showing for x ≥ξn,"
REFERENCES,0.3177570093457944,"P(H2( ˆPMLE, P ⋆) ≥x2) ≤c exp(−nx2/c2)."
REFERENCES,0.31879543094496365,"This corresponds to the statement in Theorem 7.4 (van de Geer, 2000). To prove the above, we ﬁrst
use"
REFERENCES,0.31983385254413293,"P(H2( ˆPMLE, P ⋆) ≥x2) ≤P(16H2( ¯ˆPMLE, P ⋆) ≥x2),"
REFERENCES,0.32087227414330216,"from Lemma 1. Then, from Lemma 3, this is upper-bounded by"
REFERENCES,0.32191069574247144,"P(
sup
P ∈M,H2( ¯
P ,P ⋆)≥x2/16
νn(gP ) −√nH2(P, P ⋆) ≥0)
(5)"
REFERENCES,0.3229491173416407,"where νn = √n(ED −E(s,a)∼ρ,s′∼P ⋆(·|s,a)). To prove the term 5 is less than c exp(−nδ2/c2),
we use Theorem 5.11 (van de Geer, 2000), that is, some uniform inequality based on entropy with
bracketing. The rest of the proof is the same as Theorem 7.4 (van de Geer, 2000). In summary, the
only difference is we use the distance H(a, b) tailored to the conditional density estimation instead
of unconditional density estimation."
REFERENCES,0.32398753894080995,"B
MORE DETAILS FOR KNRS"
REFERENCES,0.32502596053997923,"We explain the algorithm and present the PAC guaranteed for KNRs. Here, we denote the dimension
of S by dS."
REFERENCES,0.3260643821391485,"We tailor Algorithm 1 to KNRs as follows to obtain a tighter guarantee. First, MLE procedure is
replaced with ˆWMLE by regularized MLE:"
REFERENCES,0.32710280373831774,"ˆWMLE = arg min
W ∈RdS ×d ED[∥Wφ(s, a) −s′∥2
2] + λ∥W∥2
F ,"
REFERENCES,0.328141225337487,"where ∥· ∥F is a Frobenius norm. Then, the ﬁnal policy optimization procedure is"
REFERENCES,0.3291796469366563,"ˆπ = arg maxπ∈Π minW ∈WD V π
P (W ), s.t., WD = {W ∈RdS×d : ∥( ˆWMLE −W)(Σn)1/2∥2 ≤ξ}"
REFERENCES,0.3302180685358255,"where Σn = Pn
i=1 φ(si, ai)φ⊤(si, ai). We state the theoretical guarantee for KNRs below."
REFERENCES,0.3312564901349948,"Corollary 3 (PAC bound for KNRs). Assume ∥φ(s, a)∥2 ≤1, ∀(s, a) ∈S × A. We set ξ =
q"
REFERENCES,0.3322949117341641,"2λ∥W ⋆∥2
2 + 8ζ2  
dS ln(5) + ln(1/δ) + ¯In

,
¯In = ln (det(Σn)/ det(λI)) ."
REFERENCES,0.3333333333333333,"Suppose the KNR model is well-speciﬁed. By letting ∥W ⋆∥2
2 = O(1), ζ2 = O(1), λ = O(1), with
probability 1 −δ, for all π∗, we have"
REFERENCES,0.3343717549325026,"V π∗
P ⋆−V ˆπ
P ⋆≤c1H2 min(d1/2, ¯R)
p ¯R r"
REFERENCES,0.3354101765316719,"dS ¯Cπ∗,P ⋆ln(1 + n)"
REFERENCES,0.3364485981308411,"n
,
where ¯R := rank[Σρ]{rank[Σρ] + ln(c2/δ)}."
REFERENCES,0.3374870197300104,The proof is deferred to Section E.5.
REFERENCES,0.33852544132917967,"C
MORE RELATED WORKS"
REFERENCES,0.3395638629283489,We discuss literature related to representation learning in RL.
REFERENCES,0.3406022845275182,"Representation learning for low-rank MDPs (ground truth feature representation is unknown) in
online learning is studied from a model-based perspective (Agarwal et al., 2020b) and model-free
perspective (Modi et al., 2021). In the online setting, Zhang et al. (2021a); Papini et al. (2021)
also study representation learning under different model assumptions. Comparing with these works,
since our setting is ofﬂine, the algorithm and analysis are totally different."
REFERENCES,0.34164070612668745,Published as a conference paper at ICLR 2022
REFERENCES,0.3426791277258567,"In the ofﬂine setting, Ni et al. (2021) study dimensionality reduction in a given kernel space, and Hao
et al. (2021) study feature selection in sparse linear MDPs. Their focus is different as they do not
study PAC guarantees under partial coverage. Ni et al. (2021) assumes the transition operator can be
properly embedded into predeﬁned Reproducing Kernel Hilbert Spaces and learns low-dimensional
state-action representations via kernelized embedding and low-rank tensor decomposition. However,
they did not study the errors for policy optimization after using these learned features. Regarding
ofﬂine distribution coverage, Ni et al. (2021) assumes that the feature covariance matrix (feature
associated with the pre-deﬁned kernel) of the ofﬂine distribution is full rank. Hao et al. (2021) studies
an OPE problem on sparse linear Bellman complete MDPs in the ofﬂine learning setting where
they assume all covariance matrices (covariance matrices that correspond to all possible subsets of
features) under the ofﬂine distribution are full rank as well. We study policy optimization in low-
rank MDPs (with unknown feature representation), and we do not assume full coverage, i.e., we do
not assume the feature covariance matrix is full rank, and indeed our result is distribution-dependent
since it scales with respect to the rank of the covariance matrix that is deﬁned using the ground truth
feature representation."
REFERENCES,0.34371754932502596,"D
COMPARISON TO XIE ET AL. (2021)"
REFERENCES,0.34475597092419524,"We compare a result in (Xie et al., 2021) to our result in detail. Let F be a function class for Q-
functions. Here, we consider a more general version of their algorithm by replacing the original
E(f, π; D) in their algorithm with"
REFERENCES,0.34579439252336447,"E(f, π; D) := L(f, f; π, D) −min
g∈G L(g, f; π, D)."
REFERENCES,0.34683281412253375,"In their original algorithm, they set G = F. Here, we consider the version such that a discriminator
class G can be different from F."
REFERENCES,0.34787123572170303,"They show the PAC result under partial coverage as follows. Here, T π
P ⋆is a Bellman operator under
π and P ⋆:"
REFERENCES,0.34890965732087226,"T π
P ⋆: {S × A →R} ∋f 7→r(s, a) + EP ∗(s′|s,a)[f(s′, π)] ∈{S × A →R}."
REFERENCES,0.34994807892004154,"Theorem 6 (Extension of Result in (Xie et al., 2021) ). Suppose realizaibility Qπ
P ⋆∈F, ∀π ∈Π
and closeness maxf∈F ming∈G Es,a∼ρ[(g −T π
P ⋆f)2(s, a)] = 0, ∀π ∈Π. Then, with 1 −δ, for any
π∗∈Π, we have"
REFERENCES,0.3509865005192108,"V π∗
P ⋆−V ˆπ
P ⋆= O(
p"
REFERENCES,0.35202492211838005,"C⋄ln(|Π||F||G|/δ)/n),
C⋄= sup
f∈F"
REFERENCES,0.3530633437175493,"E(s,a)∼dπ∗
P ⋆[(f −T f)2(s, a)]"
REFERENCES,0.3541017653167186,"E(s,a)∼ρ[(f −T f)2(s, a)] ."
REFERENCES,0.35514018691588783,"By combining this result with the conversion from model-free results to model-based results in
(Chen & Jiang, 2019, Corollary 6), we can obtain the following result under partial coverage."
REFERENCES,0.3561786085150571,"Theorem 7 (PAC guarantee from the direct application of (Xie et al., 2021) to mode-based RL ).
Assume P ⋆∈M. Then, there exists an algorithm s.t. with 1 −δ, for any policy π⋆∈Π,"
REFERENCES,0.3572170301142264,"V π∗
P ⋆−V ˆπ
P ⋆= O(
p"
REFERENCES,0.3582554517133956,C⋄ln(|Π||M|/δ)/n).
REFERENCES,0.3592938733125649,"As we mentioned, this is worse than our result since it includes |Π|. Besides, the algorithm can
only compete against policies restricted in Π, while our algorithm works for the unrestricted policy
class Π which could even include history dependent policies. For completeness, we give the proof
as follows."
REFERENCES,0.3603322949117342,"We remark that their results (Theorem 4.1) with NPG that can possibly compete with any stochastic
policies, are not applicable here. This is because they need an assumption that the comparator policy
π∗needs to satisfy Qπ∗
P ⋆∈F and maxf∈F ming∈G Es,a∼ρ[(g −T π∗
P ⋆f)2(s, a)] = 0, which does not
hold for the corresponding Q-function class F after the conversion. As a notable exception, when
the model is a linear Bellman-complete MDP (Zanette et al., 2021), any stochastic policies satisfy
the Bellman completeness for the linear Q-function class; then, their algorithms can learn policies
that can compete with any stochastic policies satisfying partial coverage."
REFERENCES,0.3613707165109034,Published as a conference paper at ICLR 2022
REFERENCES,0.3624091381100727,"Proof of Theorem 7. Given a model class M, consider the following reduction. We deﬁne a Q-
function class:
F = {qπ
P | π ∈Π, P ∈M}.
Then, we deﬁne a discriminator class G:"
REFERENCES,0.363447559709242,"G = {T π′
P ′ qπ
P | π ∈Π, π′ ∈Π, P ∈M, P ′ ∈M}."
REFERENCES,0.3644859813084112,"The above satisﬁes the realizability Qπ
P ⋆∈F, ∀π ∈Π and the closedness T π
P ⋆F ⊂G, ∀π ∈Π.
Thus, the assumptions in Theorem 6 are satisﬁed. Then, we have"
REFERENCES,0.3655244029075805,"V π∗
P ⋆−V ˆπ
P ⋆= O(
p"
REFERENCES,0.36656282450674976,C⋄ln(|Π||F||G|/δ)/n)
REFERENCES,0.367601246105919,"= O(
p"
REFERENCES,0.36863966770508827,"C⋄ln(|Π||M|/δ)/n),"
REFERENCES,0.36967808930425755,noting |F| = |Π||M| and |G| = |Π|2|M|2.
REFERENCES,0.3707165109034268,"E
MISSING PROOFS"
REFERENCES,0.37175493250259606,"Below we use c, c1, c2, · · · to denote universal constants. For a d-dimensional vector a and a matrix
A ∈Rd×d, we denote ∥a∥2
A = a⊤Aa. Here, a ≲B means a ≤cB for some universal constant. c"
REFERENCES,0.37279335410176534,"E.1
PROOFS FOR GENERAL FUNCTION APPROXIMATION"
REFERENCES,0.37383177570093457,"Proof of Theorem 1. From Lemma 6, the MLE guarantee gives us the following generalization
bound: with probability 1 −δ,"
REFERENCES,0.37487019730010385,"Es,a∼ρ[TV( bPMLE(· | s, a), P ⋆(· | s, a))2] ≲ln(|M|/δ)"
REFERENCES,0.37590861889927313,"n
.
(6)"
REFERENCES,0.37694704049844235,"Letting
A(P) := |Es,a∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] −ED[TV(P(· | s, a), P ⋆(· | s, a))2]|.
with probability 1 −δ, from union bound and Bernstein’s inequality, we also have"
REFERENCES,0.37798546209761164,A(P) ≤ r
REFERENCES,0.3790238836967809,"c1var(s,a)∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] ln(|M|/δ)"
REFERENCES,0.38006230529595014,"n
+ c2 ln(|M|/δ)"
REFERENCES,0.3811007268951194,"n
, ∀P ∈M."
REFERENCES,0.3821391484942887,"(7)
Hereafter, we condition on the above two events. Recall that we construct the version space using
D and bPMLE as follows:"
REFERENCES,0.38317757009345793,"MD :=
n
P ∈M : ED[TV(P(· | s, a), ˆPMLE(· | s, a))2] ≤ξ
o
."
REFERENCES,0.3842159916926272,"First Step: Show P ⋆∈MD in high-probability.
We set ξ = c ln(|M|/δ)"
REFERENCES,0.3852544132917965,"n
. Conditioning on the
above two events equations (6) and (7), we prove P ⋆∈MD. This is proved by"
REFERENCES,0.3862928348909657,"ED[TV( bPMLE(· | s, a), P ⋆(· | s, a))2]"
REFERENCES,0.387331256490135,"= ED[TV( bPMLE(· | s, a), P ⋆(· | s, a))2] −E(s,a)∼ρ[TV( bPMLE(· | s, a), P ⋆(· | s, a))2]"
REFERENCES,0.3883696780893043,"+ E(s,a)∼ρ[TV( bPMLE(· | s, a), P ⋆(· | s, a))2]"
REFERENCES,0.3894080996884735,"= ED[TV( bPMLE(· | s, a), P ⋆(· | s, a))2] −E(s,a)∼ρ[TV( bPMLE(· | s, a), P ⋆(· | s, a))2] + c1 ln(|M|/δ) n ≲ s"
REFERENCES,0.3904465212876428,"var(s,a)∼ρ[TV( bPMLE(· | s, a), P ⋆(· | s, a))2] ln(|M|/δ)"
REFERENCES,0.39148494288681207,"n
+ ln(|M|/δ)"
REFERENCES,0.3925233644859813,"n
(From (7)) ≲ s"
REFERENCES,0.3935617860851506,"E(s,a)∼ρ[TV( bPMLE(· | s, a), P ⋆(· | s, a))2] ln(|M|/δ)"
REFERENCES,0.39460020768431986,"n
+ ln(|M|/δ)"
REFERENCES,0.3956386292834891,"n
(TV( bPMLE(· | s, a), P ⋆(· | s, a))2 ≤4) ≲1"
REFERENCES,0.39667705088265837,"n ln(|M|/δ).
(Plug in MLE guarantee)"
REFERENCES,0.39771547248182765,Published as a conference paper at ICLR 2022
REFERENCES,0.3987538940809969,"Second Step: Show Es,a∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] ≤cξ,
∀P ∈MD in high probabil-
ity.
We show for any P ∈MD, the distance between P ⋆is sufﬁciently controlled in terms of TV
distance. More concretely (conditioning on the above two events (6) and (7) ), we show"
REFERENCES,0.39979231568016615,"Es,a∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] ≲ξ,
∀P ∈MD."
REFERENCES,0.40083073727933544,"In order to observe this, for any P ∈MD, we have"
REFERENCES,0.40186915887850466,"ED[TV(P(· | s, a), P ⋆(· | s, a))2]"
REFERENCES,0.40290758047767394,"≤2ED[TV( bPMLE(· | s, a), P(· | s, a))2] + 2ED[TV( bPMLE(· | s, a), P ⋆(· | s, a))2] ≤4ξ
(From (a + b)2 ≤2a2 + 2b2.)"
REFERENCES,0.4039460020768432,"Thus, we have:"
REFERENCES,0.40498442367601245,"Es,a∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2]"
REFERENCES,0.40602284527518173,"= Es,a∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] −ED[TV(P(· | s, a), P ⋆(· | s, a))2] + ED[TV(P(· | s, a), P ⋆(· | s, a))2]
≤A(P) + cξ.
(8)"
REFERENCES,0.407061266874351,"Here, from (7), we have"
REFERENCES,0.40809968847352024,A(P) ≤ r
REFERENCES,0.4091381100726895,"c1var(s,a)∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2]] ln(|M|/δ)"
REFERENCES,0.4101765316718588,"n
+ c2 ln(|M|/δ)"
REFERENCES,0.411214953271028,"n
, ∀P ∈MD."
REFERENCES,0.4122533748701973,"Then, for any P ∈MD, we have"
REFERENCES,0.4132917964693666,A(P) ≤ r
REFERENCES,0.4143302180685358,"c1E(s,a)∼ρ[TV(P(· | s, a), P ⋆(· | s, a))4] ln(|M|/δ)"
REFERENCES,0.4153686396677051,"n
+ c2 ln(|M|/δ) n ≤ r"
REFERENCES,0.4164070612668744,"4c1E(s,a)∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] ln(|M|/δ)"
REFERENCES,0.4174454828660436,"n
+ c2 ln(|M|/δ)"
REFERENCES,0.4184839044652129,"n
([TV(P(· | s, a), P ⋆(· | s, a))2] ≤4.) ≤ r"
REFERENCES,0.4195223260643821,4c1(A(P) + cξ) ln(|M|/δ)
REFERENCES,0.4205607476635514,"n
+ c2 ln(|M|/δ) n
."
REFERENCES,0.4215991692627207,"From (a + b)2 ≤2a2 + 2b2,"
REFERENCES,0.4226375908618899,A2(P) ≲ r
REFERENCES,0.4236760124610592,c(A(P) + ξ) ln(|M|/δ)
REFERENCES,0.42471443406022846,"n
+ c ln(|M|/δ) n !2"
REFERENCES,0.4257528556593977,≲(A(P) + ξ) ln(|M|/δ)
REFERENCES,0.42679127725856697,"n
+
c ln(|M|/δ) n 2"
REFERENCES,0.42782969885773625,≲(A(P) + ξ) ln(|M|/δ)
REFERENCES,0.4288681204569055,"n
(ξ includes ln(|M|/δ))"
REFERENCES,0.42990654205607476,"≲(A(P) + 1/n ln(|M|/δ)) ln(|M|/δ) n
."
REFERENCES,0.43094496365524404,"Then, we have"
REFERENCES,0.43198338525441327,"A2(P) −B1A(P) −B2 ≤0,
B1 = c ln(|M|/δ)/n,
B2 = c(1/n)2 ln(|M|/δ)2."
REFERENCES,0.43302180685358255,This concludes
REFERENCES,0.43406022845275183,"0 ≤A(P) ≤B1 +
p"
REFERENCES,0.43509865005192105,"B2
1 + 4B2
2
≤c(B1 +
p"
REFERENCES,0.43613707165109034,"B2) ≤cln(|M|/δ) n
≲ξ."
REFERENCES,0.4371754932502596,"Thus, by using the above A(P) ≲ξ(P ∈MD) and (8), with probability 1 −δ, we have:"
REFERENCES,0.43821391484942884,"Es,a∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] ≤A(P) + cξ ≲ξ,
P ∈MD."
REFERENCES,0.4392523364485981,"Third Step: Calculate the ﬁnal error bound taking the distribution shift into account
For
any P ∈MD, we prove"
REFERENCES,0.4402907580477674,"V π∗
P ⋆−V π∗
P
≤(1 −γ)−2c
q C†
π∗ r"
REFERENCES,0.44132917964693663,ln(|M|/δ)
REFERENCES,0.4423676012461059,"n
.
(9)"
REFERENCES,0.4434060228452752,Published as a conference paper at ICLR 2022
REFERENCES,0.4444444444444444,"For any P ∈MD, this is proved as follows:"
REFERENCES,0.4454828660436137,"V π∗
P ⋆−V π∗
P
≤(1 −γ)−2E(s,a)∼dπ∗
P ⋆[TV(P(· | s, a), P ⋆(· | s, a))]"
REFERENCES,0.446521287642783,"(Simulation lemma, Lemma 5)"
REFERENCES,0.4475597092419522,≤(1 −γ)−2q
REFERENCES,0.4485981308411215,"E(s,a)∼dπ∗
P ⋆[TV(P(· | s, a), P ⋆(· | s, a))2]"
REFERENCES,0.44963655244029077,"≤(1 −γ)−2
q"
REFERENCES,0.45067497403946,"C†
π∗E(s,a)∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2]"
REFERENCES,0.4517133956386293,"≤c(1 −γ)−2
q C†
π∗ r"
REFERENCES,0.45275181723779856,ln(|M|/δ)
REFERENCES,0.4537902388369678,"n
.
(Based on the consequence of the second step)"
REFERENCES,0.45482866043613707,"Combining all things together, with probability 1 −δ, for any π∗∈Π, we have"
REFERENCES,0.45586708203530635,"V π∗
P ⋆−V ˆπ
P ⋆≤V π∗
P ⋆−min
P ∈MD V π∗
P
+ min
P ∈MD V π∗
P
−V ˆπ
P ⋆"
REFERENCES,0.4569055036344756,"≤V π∗
P ⋆−min
P ∈MD V π∗
P
+ min
P ∈MD V ˆπ
P −V ˆπ
P ⋆
(deﬁnition of ˆπ)"
REFERENCES,0.45794392523364486,"≤V π∗
P ⋆−min
P ∈MD V π∗
P
(Fist step, P ⋆∈MD)"
REFERENCES,0.45898234683281414,"≲(1 −γ)−2c1
q C†
π∗ r"
REFERENCES,0.46002076843198336,ln(|M|c2/δ)
REFERENCES,0.46105919003115264,"n
.
(From (9))"
REFERENCES,0.4620976116303219,"Remark 2 (To compete with all history-dependent polices). Consider the case where Π is all
Markovian polices. We want to show we can compete with all history-dependent non-Markovian
polices: ¯Π = ( ∞
Y"
REFERENCES,0.46313603322949115,"i=1
πi | πi ∈"
REFERENCES,0.46417445482866043,""" i−1
Y"
REFERENCES,0.4652128764278297,"k=1
S × A ! →∆(A) #) ."
REFERENCES,0.46625129802699894,"We take an element π∗from ¯Π. Then, V π∗
P ⋆and dπ∗
P ⋆are still well-deﬁned. Then, every step in the
proof still holds. The only step we need to check carefully is this line:"
REFERENCES,0.4672897196261682,"V π∗
P ⋆−V ˆπ
P ⋆≤V π∗
P ⋆−min
P ∈MD V π∗
P
+ min
P ∈MD V π∗
P
−V ˆπ
P ⋆"
REFERENCES,0.4683281412253375,"≤V π∗
P ⋆−min
P ∈MD V π∗
P
+ min
P ∈MD V ˆπ
P −V ˆπ
P ⋆."
REFERENCES,0.46936656282450673,"This is proved by maxπ∈¯Π V π
P = maxπ∈Π V π
P for any P."
REFERENCES,0.470404984423676,"E.2
PROOFS FOR GENERAL FUNCTION APPROXIMATION WITH INFINITE HYPOTHESIS CLASS"
REFERENCES,0.4714434060228453,"Proof of Theorem 5. From Theorem 4, the MLE guarantee gives us the following generalization
bound: with probability 1 −δ,"
REFERENCES,0.4724818276220145,"E(s,a)∼ρ[TV( bPMLE(· | s, a), P ⋆(· | s, a))2] ≲

ξ2
n + ln(c/δ) n"
REFERENCES,0.4735202492211838,"
.
(10)"
REFERENCES,0.4745586708203531,We deﬁne M1 = (
REFERENCES,0.4755970924195223,"P ∈M : E(s,a)∼ρ

TV(P(· | s, a), P ⋆(· | s, a))2
≤c "
REFERENCES,0.4766355140186916,"ξ2
n + r"
REFERENCES,0.47767393561786087,ln(c/δ) n !) .
REFERENCES,0.4787123572170301,"for some large c. From a functional Bernstein’s inequality (Lemma 12), by deﬁning"
REFERENCES,0.4797507788161994,"f(P)(s, a) = TV(P(· | s, a), P ⋆(· | s, a))2, G(M1) = E[ sup
P ∈M1
|(ED −Eρ)[f(P)]|]."
REFERENCES,0.48078920041536866,Published as a conference paper at ICLR 2022
REFERENCES,0.4818276220145379,"with probability 1 −δ, we have"
REFERENCES,0.48286604361370716,"sup
P ∈M1
|(ED −Eρ)[f(P)]| ≲G(M1) + {G(M1) + sup
P ∈M1
Eρ[f(P)2]}1/2
r"
REFERENCES,0.48390446521287644,log(c1/δ)
REFERENCES,0.48494288681204567,"n
+ log(c1/δ) n"
REFERENCES,0.48598130841121495,"≲G(M1) + {G(M1) + sup
P ∈M1
Eρ[f(P)]}1/2
r"
REFERENCES,0.48701973001038423,log(c1/δ)
REFERENCES,0.48805815160955346,"n
+ log(c1/δ) n"
REFERENCES,0.48909657320872274,"≲G(M1) + ξ2
n +
ln(c/δ) n"
REFERENCES,0.490134994807892,"
.
(11)"
REFERENCES,0.49117341640706125,"Similarly, by deﬁning"
REFERENCES,0.49221183800623053,"M2 =

P : E(s,a)∼D[TV(P(· | s, a), P ⋆(· | s, a))2] ≤cG(M1) + cξ2
n + c
ln(c/δ) n 
,"
REFERENCES,0.4932502596053998,"G(M2) = E[ sup
P ∈MD
|(ED −Eρ)[f(P)]|],"
REFERENCES,0.49428868120456904,"Zn = sup
P ∈M2
E(s,a)∼ρ[f(P)(s, a)]."
REFERENCES,0.4953271028037383,"from a functional Bernstein’s inequality, with probability 1 −δ, we have"
REFERENCES,0.4963655244029076,"sup
P ∈M2
|(ED −Eρ)[f(P)]| ≲G(M2) + {G(M2) + Zn}1/2
r"
REFERENCES,0.4974039460020768,log(c1/δ)
REFERENCES,0.4984423676012461,"n
+ log(c1/δ) n"
REFERENCES,0.4994807892004154,≲G(M2) + r
REFERENCES,0.5005192107995846,"Zn
ln(c/δ)"
REFERENCES,0.5015576323987538,"n
+ ln(c/δ)"
REFERENCES,0.5025960539979232,"n
(12)"
REFERENCES,0.5036344755970924,"Hereafter, we condition on the above three events:(10), (11) and (12)."
REFERENCES,0.5046728971962616,"First step: Show P ∗∈MD in high probability.
From (10), we have"
REFERENCES,0.505711318795431,"E(s,a)∼ρ[f( ˆPMLE)(s, a)] ≤ξ2
n + ln(c/δ) n
."
REFERENCES,0.5067497403946002,"Thus, P ⋆∈M1. Then, from (11) and (10),"
REFERENCES,0.5077881619937694,"E(s,a)∼D[f( ˆPMLE)(s, a)]"
REFERENCES,0.5088265835929388,"= |E(s,a)∼D[f( ˆPMLE)(s, a)] −E(s,a)∼ρ[f( ˆPMLE)(s, a)]| + E(s,a)∼ρ[f( ˆPMLE)(s, a)]"
REFERENCES,0.509865005192108,"≲G(M1) + ξ2
n + ln(c/δ) n
."
REFERENCES,0.5109034267912772,"Thus, P ⋆∈MD recalling the deﬁnition of MD (we set ξ = G(M1) + ξ2
n + ln(c/δ) n
)."
REFERENCES,0.5119418483904465,"Second step: Show the upper bound of E(s,a)∼ρ[f(P)(s, a)] for any P ∈MD.
For any
P ∈MD, as the proof of Theorem 1, we can prove P ∈M2. Thus,"
REFERENCES,0.5129802699896158,"sup
P ∈MD
E(s,a)∼ρ[f(P)(s, a)] ≤Zn."
REFERENCES,0.514018691588785,"Thus, we will hereafter analyze Zn. Here from (12), for any P ∈M2, we have"
REFERENCES,0.5150571131879543,"E(s,a)∼ρ[f(P)(s, a)] ≤sup
P ∈M2"
REFERENCES,0.5160955347871236," 
|(E(s,a)∼D −E(s,a)∼ρ)[f(P)(s, a)]|

+ sup
P ∈M2
E(s,a)∼D[f(P)(s, a)]"
REFERENCES,0.5171339563862928,≲G(M2) + r
REFERENCES,0.5181723779854621,"Zn
ln(c/δ)"
REFERENCES,0.5192107995846313,"n
+ G(M1) + ξ2
n + ln(c/δ) n
."
REFERENCES,0.5202492211838006,"Hence,"
REFERENCES,0.5212876427829699,Zn ≤G(M2) + r
REFERENCES,0.5223260643821391,"Zn
ln(c/δ)"
REFERENCES,0.5233644859813084,"n
+ G(M1) + ξ2
n + ln(c/δ) n
."
REFERENCES,0.5244029075804777,This shows
REFERENCES,0.5254413291796469,"Zn ≤G(M2) + G(M1) + ξ2
n + ln(c/δ) n
."
REFERENCES,0.5264797507788161,Published as a conference paper at ICLR 2022
REFERENCES,0.5275181723779855,"Third step: Calculate the ﬁnal error bound.
Following the proof of Theorem 1, we can prove"
REFERENCES,0.5285565939771547,"V π∗
P ⋆−V ˆπ
P ⋆≤(1 −γ)−2c1
q C†
π∗ r"
REFERENCES,0.5295950155763239,"G(M2) + G(M1) + ξ2n + ln(c/δ) n
."
REFERENCES,0.5306334371754933,"E.3
PROOFS FOR TABULAR MDPS"
REFERENCES,0.5316718587746625,Proof of Corollary 1. We prove in a similar way as Theorem 1.
REFERENCES,0.5327102803738317,"First step
We set ξ = c |S|2|A| ln(n|S|A|c2/δ)}"
REFERENCES,0.5337487019730011,"n
. Then, from Lemma 7, with probability 1 −δ, we
can show P ⋆∈MD since"
REFERENCES,0.5347871235721703,"Es,a∼D
h
TV( bPMLE(· | s, a), P ⋆(· | s, a))2i
≤ξ."
REFERENCES,0.5358255451713395,"Hereafter, we condition on the above event."
REFERENCES,0.5368639667705088,"Second step.
Following the second step in the proof of Theorem 1 based on (8), for any P ∈MD,
we have"
REFERENCES,0.5379023883696781,"Es,a∼ρ

TV(P(· | s, a), P ⋆(· | s, a))2
≤cξ + A(P)
(13) where"
REFERENCES,0.5389408099688473,"A(P) := |Es,a∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] −ED[TV(P(· | s, a), P ⋆(· | s, a))2]|."
REFERENCES,0.5399792315680166,"Our goal here is showing with probability 1 −δ,"
REFERENCES,0.5410176531671859,"A(P) ≲ξ, ∀P ∈MD.
(14)"
REFERENCES,0.5420560747663551,"To prove (14), consider an ϵ-net {P1(s, a), · · · , PK(s, a)} covering a simplex in terms of ∥·∥1 4 for
each ﬁxed pair (s, a) ∈S × A. We take ϵ = 1/n. Since the covering number K is upper-bounded
by (c/ϵ)|S| (Wainwright, 2019, Lemma 5.7), we can obtain ¯
M = {P1, · · · , PK|S|×|A|} s.t. for any
possible P ⊂S × A →∆(S), there exists Pi s.t."
REFERENCES,0.5430944963655244,"TV(Pi(· | s, a), P(· | s, a)) ≤ϵ, ∀(s, a)."
REFERENCES,0.5441329179646937,"This implies for any P ⊂S × A →∆(S), there exists Pi(· | s, a) s.t. ∀(s, a),"
REFERENCES,0.5451713395638629,"|TV(P(· | s, a), P ⋆(· | s, a))2 −TV(Pi(· | s, a), P ⋆(· | s, a))2|"
REFERENCES,0.5462097611630322,"≤4|TV(P(· | s, a), P ⋆(· | s, a)) −TV(Pi(· | s, a), P ⋆(· | s, a))|
(a2 −b2 = (a −b)(a + b))
≤4TV(P· | s, a), Pi(· | s, a))
(|∥a∥−∥b∥| ≤∥a −b∥)
≤4ϵ.
(15)"
REFERENCES,0.5472481827622014,We often use this property (15) hereafter.
REFERENCES,0.5482866043613707,"Next, we deﬁne M′ ⊂¯
M so that it covers MD. Concretely, we deﬁne M′:"
REFERENCES,0.54932502596054,"M′ = {P ∈¯
M : ∃P ′′ ∈MD, TV(P(· | s, a), P ′′(· | s, a)) ≤ϵ
∀(s, a)}.
(16)"
REFERENCES,0.5503634475597092,"The construction is illustrated in Figure 1. Here, from the deﬁnition, for any P ∈MD , we can also
ﬁnd P ′ ∈M′ s.t.
TV(P(· | s, a), P ′(· | s, a)) ≤ϵ, ∀(s, a).
This is because from the deﬁnition of ¯
M, we can always ﬁnd P ∈¯
M satisfying the above. Such P
belongs to M′ from the deﬁnition of M′. We use this fact later."
REFERENCES,0.5514018691588785,"Then, from (16) and recalling (13), we have"
REFERENCES,0.5524402907580478,"Es,a∼ρ

TV(P(· | s, a), P ⋆(· | s, a))2
≲ξ + A(P),
∀P ∈M′.
(17)"
REFERENCES,0.553478712357217,"4In the tabular setting, since the state space is countable, it is equivalent to L1 distance."
REFERENCES,0.5545171339563862,Published as a conference paper at ICLR 2022
REFERENCES,0.5555555555555556,"Figure 1: MD is colored in gray. M′ corresponds to the set of black dots. Orange dots correspond
to ¯
M, which do not belong to M′."
REFERENCES,0.5565939771547248,because
REFERENCES,0.557632398753894,"Es,a∼ρ

TV(P(· | s, a), P ⋆(· | s, a))2"
REFERENCES,0.5586708203530634,"≤Es,a∼ρ

TV(P(· | s, a), P ′′(· | s, a))2
+ Es,a∼ρ

TV(P ′′(· | s, a), P ⋆(· | s, a))2"
REFERENCES,0.5597092419522326,"≤Es,a∼ρ

TV(P ′′(· | s, a), P ⋆(· | s, a))2
+ ϵ2
(Take some P ′′ ∈MD noting (16))"
REFERENCES,0.5607476635514018,"≤cξ + A(P).
(From (13))"
REFERENCES,0.5617860851505712,"Then, with probability 1 −δ, from Bernstein’s inequality, we have"
REFERENCES,0.5628245067497404,A(P) ≤ r
REFERENCES,0.5638629283489096,"cvar[TV(P(· | s, a), P ⋆(· | s, a))2] ln(K|S|×|A|/δ)"
REFERENCES,0.564901349948079,"n
+ c ln(K|S|×|A|/δ)"
REFERENCES,0.5659397715472482,"n
, ∀P ∈M."
REFERENCES,0.5669781931464174,"Hereafter, we condition on the above event. Based on (17), we can state"
REFERENCES,0.5680166147455867,"var[TV(P(· | s, a), P ⋆(· | s, a))2] ≲E[TV(P(· | s, a), P ⋆(· | s, a))2] ≲ξ + A(P),
∀P ∈M′,"
REFERENCES,0.569055036344756,"with probability 1 −δ. Following the argument of Theorem 1, for P ∈M′, we have"
REFERENCES,0.5700934579439252,"A2(P) −A(P)B1 −B2 ≤0,
B1 = ln(K|S|×|A|/δ)"
REFERENCES,0.5711318795430945,"n
,
B2 = ξ ln(K|S|×|A|/δ)"
REFERENCES,0.5721703011422637,"n
+
ln(K|S|×|A|/δ) n 2
."
REFERENCES,0.573208722741433,"Then, with probability 1 −δ, we have"
REFERENCES,0.5742471443406023,"A(P) ≤ln(K|S|×|A|/δ) n
+ r"
REFERENCES,0.5752855659397715,ln(K|S|×|A|/δ)
REFERENCES,0.5763239875389408,"n
ξ1/2 ≲ξ,
∀P ∈M′.
(18)"
REFERENCES,0.5773624091381101,"This shows for any P ∈MD, we have"
REFERENCES,0.5784008307372793,"|{ED −E(s,a)∼ρ}[TV(P(· | s, a), P ⋆(· | s, a))2]|"
REFERENCES,0.5794392523364486,"≤|{ED −E(s,a)∼ρ}[TV(P ′(· | s, a), P(· | s, a))2 + TV(P ′(· | s, a), P ⋆(· | s, a))2]|
(We take P ′ ∈M′ such that (16))"
REFERENCES,0.5804776739356179,"≤|{ED −E(s,a)∼ρ}[TV(P ′(· | s, a), P ⋆(· | s, a))2] + 8ϵ
(From the deﬁnition of M′)"
REFERENCES,0.5815160955347871,"≲ξ.
(From (18) and P ′ ∈M′)"
REFERENCES,0.5825545171339563,"Thus, (14) is proved."
REFERENCES,0.5835929387331257,"Third step.
We follow the third step of Theorem 1:"
REFERENCES,0.5846313603322949,"V π∗
P ⋆−V ˆπ
P ⋆≲(1 −γ)−2
q"
REFERENCES,0.5856697819314641,"C†
π∗ξ."
REFERENCES,0.5867082035306335,Published as a conference paper at ICLR 2022
REFERENCES,0.5877466251298027,"E.4
PROOFS FOR LINEAR MIXTURE MDPS"
REFERENCES,0.5887850467289719,"Proof of Corollary 2. Here, letting P(θ) = θ⊤ψ(s, a, s′), recall"
REFERENCES,0.5898234683281413,"MMix =

P(θ) | θ ∈Θ ⊂Rd,
Z
θ⊤ψ(s, a, s′)d(s′) = 1
∀(s, a)

, H = (r"
REFERENCES,0.5908618899273105,P + P ⋆
REFERENCES,0.5919003115264797,"2
| P ∈MMix ) ."
REFERENCES,0.592938733125649,"Upper-bounding E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2].
By invoking Theorem 4, we ﬁrst show"
REFERENCES,0.5939771547248183,"E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2] ≤c{(d/n) ln2(nR) + ln(c/δ)/n}."
REFERENCES,0.5950155763239875,"To do that, we calculate the entropy integral with bracketing. First, we have
N[](ϵ, H, d) ≤N[](ϵ, MMix, d′).
(19)
where"
REFERENCES,0.5960539979231568,"d′(a, b) = E(s,a)∼ρ"
REFERENCES,0.5970924195223261,"Z
(a(s, a, s′) −b(s, a, s′))2d(s′)
1/2
,
(20)"
REFERENCES,0.5981308411214953,"d(a, b) = E(s,a)∼ρ"
REFERENCES,0.5991692627206646,"Z
(
p"
REFERENCES,0.6002076843198338,"a(s, a, s′) −
p"
REFERENCES,0.6012461059190031,"b(s, a, s′))2d(s′)
1/2
.
(21)"
REFERENCES,0.6022845275181724,"Here, we use two observations. The ﬁrst observation is d2
 r"
REFERENCES,0.6033229491173416,"P(θ′) + P ⋆ 2
, r"
REFERENCES,0.6043613707165109,P(θ′′) + P ⋆ 2 !
REFERENCES,0.6053997923156802,"≤c1d′2(P(θ′), P(θ′′))"
REFERENCES,0.6064382139148494,"due to the mean-value theorem√a −
√"
REFERENCES,0.6074766355140186,"b ≤max(1/√a, 1/
√"
REFERENCES,0.608515057113188,"b)(a −b)
and assumption P ⋆(s′ | s, a) ≥c0 > 0. The second observation is when we have P ′ < g < P ′′, we
also have
p"
REFERENCES,0.6095534787123572,"(P ′ + P ⋆)/2 <
p"
REFERENCES,0.6105919003115264,"(g + P ⋆)/2 <
p"
REFERENCES,0.6116303219106958,"(P ′′ + P ⋆)/2. Then, (19) is concluded."
REFERENCES,0.612668743509865,"Next, by letting θ(1), · · · , θ(K) be an ϵ-cover of the d-dimensional ball with a radius R, i.e, Bd(R),
we have the brackets {[P(θ(i)) −ϵ, P(θ(i)) + ϵ]}K
i=1 which cover Mmix. This is because for any
P(θ) ∈Mmix, we can take θ(i) s.t. ∥θ −θ(i)∥2 ≤ϵ, then,"
REFERENCES,0.6137071651090342,"P(θ(i)) −ϵ < P(θ) < P(θ(i)) + ϵ,
∀(s, a, s′)
noting"
REFERENCES,0.6147455867082036,"|P(θ)(s, a, s′) −P(θ(i))(s, a, s′)| ≤∥θ −θ(i)∥2 ≤ϵ,
∀(s, a, s′)
(22)
The last equality is from Lemma 10."
REFERENCES,0.6157840083073728,"The brackets above are size of ϵ. Therefore, we have
N[](ϵ, Mmix, ∥· ∥2) ≤N(ϵ, Bd(cR), ∥· ∥2),"
REFERENCES,0.616822429906542,"where N(ϵ, Bd(cR), ∥· ∥2) is a covering number of Bd(cR) w.r.t ∥· ∥2. This is upper-bounded by
(cR/ϵ)d (Wainwright, 2019, Lemma 5.7). Thus, we can calculate the upper bound of the entropy
integral JB(δ, Mmix, ∥· ∥2):
Z δ"
REFERENCES,0.6178608515057114,"0
d1/2 ln1/2(cR/u)du ≤
Z δ"
REFERENCES,0.6188992731048806,"0
d1/2 ln(1/u)du + δd1/2 ln(c1R)"
REFERENCES,0.6199376947040498,= cd1/2(δ + δ ln(1/δ)) + δd1/2 ln(cR)
REFERENCES,0.6209761163032191,≤cd1/2δ ln(cR/δ).
REFERENCES,0.6220145379023884,"By taking G(x) = d1/2x ln(cR/x) in Theorem 4, δn = (d/n)1/2 ln(nR) satisﬁes the critical in-
equality
√nδ2
n ≥d1/2δn ln(cR/δn).
Finally, with probability 1 −δ"
REFERENCES,0.6230529595015576,"E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2] ≤ξ′,
ξ′ := {(d/n) ln2(nR) + ln(c/δ)/n}.
(23)
Hereafter, we condition on this event."
REFERENCES,0.6240913811007269,Published as a conference paper at ICLR 2022
REFERENCES,0.6251298026998962,"Upper bounding ED[TV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2].
We take an ϵ-cover of the ball
Bd(R) in terms of ∥·∥2, i.e., ¯
M = {θ(1), · · · , θ(K)}, where K = (cR/ϵ)d. We take ϵ = 1/n. Then,
for any θ ∈Bd(R), there exists θ(i) s.t. ∀(s, a),"
REFERENCES,0.6261682242990654,"|TV(P(θ)(· | s, a), P(ˆθMLE)(· | s, a))2 −TV(P(θ(i))(· | s, a), P(ˆθMLE)(· | s, a))2|"
REFERENCES,0.6272066458982347,"≤4|TV(P(θ)(· | s, a), P(ˆθMLE)(· | s, a)) −TV(P(θ(i))(· | s, a), P(ˆθMLE)(· | s, a))|
(a2 −b2 = (a −b)(a + b))"
REFERENCES,0.6282450674974039,"≤4TV(P(θ)(· | s, a), P(θ(i))(· | s, a))
(|∥a∥−∥b∥| ≤∥a −b∥)"
REFERENCES,0.6292834890965732,"≤4∥θ −θ(i)∥2
(From Lemma 10.)
≤4ϵ.
(24)"
REFERENCES,0.6303219106957425,"Hereafter, we condition on the event:"
REFERENCES,0.6313603322949117,"|(ED −E(s,a)∼ρ)[TV(P(θ)(· | s, a), P(θ⋆)(· | s, a))2]|
(25) ≲ r"
REFERENCES,0.632398753894081,"var(s,a)∼ρ[TV(P(θ)(· | s, a), P(θ⋆)(· | s, a))2] ln(K/δ)"
REFERENCES,0.6334371754932503,"n
+ ln(K/δ)"
REFERENCES,0.6344755970924195,"n
,
∀θ ∈¯
M."
REFERENCES,0.6355140186915887,This event holds with probability 1 −δ from Bernstein’s inequality.
REFERENCES,0.6365524402907581,"Here, note for ˆθMLE, we have θ(i) s.t. ∥ˆθMLE −θ(i)∥2 ≤ϵ. Then, following the ﬁrst step of
Theorem 1,"
REFERENCES,0.6375908618899273,"E(s,a)∼DTV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2"
REFERENCES,0.6386292834890965,"≲E(s,a)∼DTV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2 + ϵ
(From (24))"
REFERENCES,0.6396677050882659,"≲(ED −E(s,a)∼ρ)TV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2 + ϵ + E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2 ≲ s"
REFERENCES,0.6407061266874351,"var(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2] ln(K/δ)"
REFERENCES,0.6417445482866043,"n
+ ln(K/δ) n"
REFERENCES,0.6427829698857737,"+ ϵ + E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2
(From (25)) ≲ s"
REFERENCES,0.6438213914849429,"E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2] ln(K/δ)"
REFERENCES,0.6448598130841121,"n
+ ln(K/δ) n"
REFERENCES,0.6458982346832814,"+ ϵ + E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2].
(TV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2 ≤4)"
REFERENCES,0.6469366562824507,"Then, we have"
REFERENCES,0.6479750778816199,"ED[TV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2] ≲ s"
REFERENCES,0.6490134994807892,"{E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2] + ϵ} ln(K/δ)"
REFERENCES,0.6500519210799585,"n
+ ln(K/δ) n"
REFERENCES,0.6510903426791277,"+ ϵ + E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2 ≲ r"
REFERENCES,0.652128764278297,{ξ′ + ϵ} ln(K/δ)
REFERENCES,0.6531671858774662,"n
+ ln(K/δ)"
REFERENCES,0.6542056074766355,"n
+ ϵ + ξ′.
(From (23))"
REFERENCES,0.6552440290758048,"In the end, by taking ϵ = 1/n, we have with probability 1 −δ,"
REFERENCES,0.656282450674974,"EDTV(P(θ⋆)(· | s, a), P(ˆθMLE)(· | s, a))2 ≤ξ,
ξ = c{(d/n) ln2(nR) + ln(c/δ)/n}."
REFERENCES,0.6573208722741433,"This also implies with probability 1 −δ, P ⋆∈MD."
REFERENCES,0.6583592938733126,"Show E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2 ≲ξ, ∀P(θ) ∈MD.
We show for any P ∈MD, the distance between P ⋆is controlled in terms of TV distance. Our
goal is showing"
REFERENCES,0.6593977154724818,"E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2 ≲ξ,
∀P(θ) ∈MD."
REFERENCES,0.660436137071651,Published as a conference paper at ICLR 2022
REFERENCES,0.6614745586708204,"First, following the second step of Theorem 1 based on equation 8, we have
E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2 ≤A(θ) + cξ,
∀P(θ) ∈MD
(26)
where
A(θ) := |(ED −E(s,a)∼ρ)TV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2|."
REFERENCES,0.6625129802699896,"From now on, we again consider an ϵ-cover of the ball Bd(R) in terms of ∥· ∥2, i.e.,
¯
M =
{θ(1), · · · , θ(K)}, where K = (c1R/ϵ)d (ϵ = 1/n). This also covers the space MD. We take
M′ = {θ(i1), θ(i2) · · · , } ⊂M which covers MD, that is,"
REFERENCES,0.6635514018691588,"M′ =

θ ∈¯
M | ∃θ′s.t.P(θ′) ∈MD, ∥θ −θ′∥2 ≤ϵ
	
."
REFERENCES,0.6645898234683282,"Recall Figure 1, which illustrates this deﬁnition. Here, for any θ s.t. ∀P(θ) ∈MD, we can take
θ′ ∈M′ s.t. ∥θ −θ′∥2 ≤1/n. This is because we can take θ ∈¯
M s.t. ∥θ −θ′∥2 ≤ϵ noting ¯
M is
an ϵ-net, but such θ belongs to M′ from the deﬁnition of M′."
REFERENCES,0.6656282450674974,"Then, we have
E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2 ≤A(θ) + cξ,
∀θ ∈M′.
(27)"
REFERENCES,0.6666666666666666,"This is because for any θ(i) ∈M′, we can take P(θ) ∈MD such that"
REFERENCES,0.667705088265836,"E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2"
REFERENCES,0.6687435098650052,"≤E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ(i))(· | s, a))2 −TV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2]"
REFERENCES,0.6697819314641744,"+ E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2]"
REFERENCES,0.6708203530633438,"≤4ϵ + E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2]
(∥θ −θ(i)∥2 ≤ϵ and from (24))"
REFERENCES,0.671858774662513,"≲A(θ) + ξ.
( From (26))"
REFERENCES,0.6728971962616822,"Here, from (25), we have"
REFERENCES,0.6739356178608515,A(θ) ≤ r
REFERENCES,0.6749740394600208,"cvar(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2] ln(K/δ)"
REFERENCES,0.67601246105919,"n
+ c ln(K/δ)"
REFERENCES,0.6770508826583593,"n
,
∀θ ∈M′"
REFERENCES,0.6780893042575286,"Based on the construction of M′ and (27), we have
var(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2] ≲A(θ) + ξ,
∀θ ∈M′."
REFERENCES,0.6791277258566978,"Then, following the second step of Theorem 1, A(θ) satisﬁes"
REFERENCES,0.6801661474558671,"A2(θ) −A(θ)B1 −B2 ≤0,
B1 = ln(K/δ)"
REFERENCES,0.6812045690550363,"n
, B2 = ξ ln(K/δ)"
REFERENCES,0.6822429906542056,"n
+
ln(K/δ) n 2
."
REFERENCES,0.6832814122533749,"Then, we have"
REFERENCES,0.6843198338525441,A(θ) ≤ln(K/δ)
REFERENCES,0.6853582554517134,"n
+ ξ1/2
r"
REFERENCES,0.6863966770508827,ln(K/δ)
REFERENCES,0.6874350986500519,"n
≲ξ,
∀θ ∈M′.
(28)"
REFERENCES,0.6884735202492211,"We combine all steps. Recall for any ∀P(θ) ∈MD, we can take θ′ ∈M′ s.t. ∥θ −θ′∥2 ≤1/n.
Then, for any P(θ) ∈MD, we have"
REFERENCES,0.6895119418483905,"A(θ) = |(ED −E(s,a)∼ρ)TV(P(θ)(· | s, a), P(θ⋆)(· | s, a))2"
REFERENCES,0.6905503634475597,"≤|(ED −E(s,a)∼ρ)[TV(P(θ)(· | s, a), P(θ⋆)(· | s, a))2 −TV(P(θ′)(· | s, a), P(θ⋆)(· | s, a))2]"
REFERENCES,0.6915887850467289,"+ (ED −E(s,a)∼ρ)[TV(P(θ′)(· | s, a), P(θ⋆)(· | s, a))2]"
REFERENCES,0.6926272066458983,"≤8ϵ + |(ED −E(s,a)∼ρ)[TV(P(θ′)(· | s, a), P(θ⋆)(· | s, a))2]
(From equation 24)"
REFERENCES,0.6936656282450675,"≲ξ.
(From equation 28 and θ′ ∈M′)
Then, we have with probability 1 −δ,
A(θ) ≲ξ,
∀P(θ) ∈MD.
(29)
Finally, for any P(θ) ∈MD, with probability 1 −δ, we have"
REFERENCES,0.6947040498442367,"E(s,a)∼ρ[TV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2] ≤A(θ) + cξ
(From equation 26)"
REFERENCES,0.6957424714434061,"≲ξ.
(From equation 29)"
REFERENCES,0.6967808930425753,Published as a conference paper at ICLR 2022
REFERENCES,0.6978193146417445,"Distribution shift part
Here, for P ∈MD we prove"
REFERENCES,0.6988577362409139,"V π∗
P ⋆−V π∗
P
≲(1 −γ)−2p"
REFERENCES,0.6998961578400831,"dCπ∗,mixξ,
(30)"
REFERENCES,0.7009345794392523,"V π∗
P ⋆−V π∗
P
≲(1 −γ)−2
q"
REFERENCES,0.7019730010384216,"C†
π∗ξ.
(31)"
REFERENCES,0.7030114226375909,"Following the third step of the proof of Theorem 5, this immediately concludes the bound"
REFERENCES,0.7040498442367601,"V π∗
P ⋆−V ˆπ
P ⋆≲(1 −γ)−2p"
REFERENCES,0.7050882658359294,"dCπ∗,mixξ,"
REFERENCES,0.7061266874350987,"V π∗
P ⋆−V ˆπ
P ⋆≲(1 −γ)−2
q"
REFERENCES,0.7071651090342679,"C†
π∗ξ."
REFERENCES,0.7082035306334372,"Since (31) is obvious from simulation lemma, we only prove (30). To prove (30), we take a distri-
bution P(θ) ∈MD. First, recall for P(θ) ∈MD, we have"
REFERENCES,0.7092419522326064,"E(s,a)∼ρTV(P(θ⋆)(· | s, a), P(θ)(· | s, a))2 ≲ξ."
REFERENCES,0.7102803738317757,"From the third statement of Lemma 10, for any V : S →[0, 1], we have"
REFERENCES,0.711318795430945,"E(s,a)∼ρ[|(θ −θ∗)⊤ψV (s, a)|2] ≲ξ. Thus,"
REFERENCES,0.7123572170301142,"∀V : S →[0, 1],
(θ −θ∗)⊤Σρ,V (θ −θ∗) ≲ξ,
Σρ,V = E(s,a)∼ρ[ψV (s, a)ψ⊤
V (s, a)]."
REFERENCES,0.7133956386292835,"Here, we have"
REFERENCES,0.7144340602284528,"V π∗
P ⋆−V π∗
P
≤(1 −γ)−1
E(s,a)∼dπ∗
P ⋆"
REFERENCES,0.715472481827622,"Z
{P(s′ | s, a) −P ⋆(s′ | s, a)}V π∗
P (s′)d(s′)
"
REFERENCES,0.7165109034267912,"(Simulation lemma, Lemma 5)"
REFERENCES,0.7175493250259606,"≤(1 −γ)−1 E(s,a)∼dπ∗
P ⋆"
REFERENCES,0.7185877466251298,"h
(θ −θ∗)ψV π∗
P (s, a)
i"
REFERENCES,0.719626168224299,"(Recall ψV =
R
ψ(s, a, s′)V π∗
P (s′)d(s′))"
REFERENCES,0.7206645898234684,"≤(1 −γ)−1 ∥θ −θ∗∥λI+Σρ,V π∗
P
|
{z
}
(a)"
REFERENCES,0.7217030114226376,"E(s,a)∼dπ∗
P ⋆"
REFERENCES,0.7227414330218068,"
∥ψV π∗
P (s, a)∥(Σρ,V π∗
P
+λI)−1
"
REFERENCES,0.7237798546209762,"|
{z
}
(b) ."
REFERENCES,0.7248182762201454,(CS inequality)
REFERENCES,0.7258566978193146,"The ﬁrst term (a) is upper-bounded by
p"
REFERENCES,0.726895119418484,"{(1 −γ)−2ξ + λR2} noting 0 ≤V π∗
P
≤(1 −γ)−1. The
term (b) is upper-bounded by"
REFERENCES,0.7279335410176532,"E(s,a)∼dπ∗
P ⋆"
REFERENCES,0.7289719626168224,"
∥ψV π∗
P (s, a)∥(Σρ,V π∗
P
+λI)−1

≤E(s,a)∼dπ∗
P ⋆"
REFERENCES,0.7300103842159917,"
∥ψV π∗
P (s, a)∥2
(Σρ,V π∗
P
+λI)−1
1/2"
REFERENCES,0.731048805815161,"(Jensen’s inequality) =
q"
REFERENCES,0.7320872274143302,"Tr(Σdπ∗
P ⋆,V π∗
P (λI + Σρ,V π∗
P )−1) ≤
q"
REFERENCES,0.7331256490134995,"Cπ∗,mix Tr(Σρ,V π∗
P (λI + Σρ,V π∗
P )−1)"
REFERENCES,0.7341640706126688,"(From Lemma 11) ≤
q"
REFERENCES,0.735202492211838,"Cπ∗,mixrank(Σρ,V π∗
P ) ≤
p"
REFERENCES,0.7362409138110073,"Cπ∗,mixd."
REFERENCES,0.7372793354101765,"By taking λ s.t. λR2 ≲(1 −γ)−2ξ, (30) is proved."
REFERENCES,0.7383177570093458,"For linear MDPs, from the fourth statement of Lemma 10, Cπ∗,mix ≤¯Cπ∗. Then, the statement is
concluded."
REFERENCES,0.7393561786085151,"E.5
PROOFS FOR KNRS"
REFERENCES,0.7403946002076843,"Proof of Corollary 3.
We prove in a similar way as Theorem 1."
REFERENCES,0.7414330218068536,Published as a conference paper at ICLR 2022
REFERENCES,0.7424714434060229,"First Step
Recall ξ =
q"
REFERENCES,0.7435098650051921,"2λ∥W ⋆∥2
2 + 8ζ2  
dS ln(5) + ln(1/δ) + ¯In

,
¯In = ln (det(Σn)/ det(λI)) ."
REFERENCES,0.7445482866043613,"Thus, from Lemma 8, with probability 1 −δ, we can show W ∗∈WD since


c
WMLE −W ⋆
(Σn)1/2
2 ≤ξ."
REFERENCES,0.7455867082035307,"Hereafter, we condition on this event."
REFERENCES,0.7466251298026999,"Second step
For any W ∈WD, with probability 1 −δ, we have
(W −W ⋆) (Σn)1/2
2 ≤


W −c
W

(Σn)1/2
2 +


W ∗−c
W

(Σn)1/2
2 ≤ξ."
REFERENCES,0.7476635514018691,"Third step
Note P ⋆= P(W ∗). Then,"
REFERENCES,0.7487019730010385,"V π∗
P ⋆−V ˆπ
P ⋆≤V π∗
P ⋆−min
W ∈WD V π∗
P (W ) + min
W ∈WD V π∗
P (W ) −V ˆπ
P ⋆"
REFERENCES,0.7497403946002077,"≤V π∗
P ⋆−min
W ∈WD V π∗
P (W ) + min
W ∈WD V ˆπ
P (W ) −V ˆπ
P ⋆
(deﬁnition of ˆπ)"
REFERENCES,0.7507788161993769,"≤V π∗
P ⋆−min
W ∈WD V π∗
P (W ).
(Fist step, W ∗∈WD)"
REFERENCES,0.7518172377985463,"Then, by setting W ′ = arg minW ∈MD V π∗
P (W ), we have"
REFERENCES,0.7528556593977155,"V π∗
P ⋆−V ˆπ
P ⋆≤(1 −γ)−2E(s,a)∼dπ∗
P ⋆[∥P ′(s, a) −P ⋆(s, a)∥TV]"
REFERENCES,0.7538940809968847,≤(1 −γ)−2
REFERENCES,0.754932502596054,"ζ
E(s,a)∼dπ∗
P ⋆[∥(W ′ −W ⋆)φ(s, a)∥2]
(Lemma 9)"
REFERENCES,0.7559709241952233,≤(1 −γ)−2
REFERENCES,0.7570093457943925,"ζ
E(s,a)∼dπ∗
P ⋆"
REFERENCES,0.7580477673935618,"h(W ′ −W ⋆)(Σn)1/2
2 ∥φ(s, a)∥Σ−1
n"
REFERENCES,0.7590861889927311,"i
(CS inequality)"
REFERENCES,0.7601246105919003,≤(1 −γ)−2
REFERENCES,0.7611630321910696,"ζ
ξE(s,a)∼dπ∗
P ⋆[∥φ(s, a)∥Σ−1
n ]
(Second step)"
REFERENCES,0.7622014537902388,"From Chang et al. (2021, Theorem 20), with probability 1 −δ, we have"
REFERENCES,0.7632398753894081,"ξ ≤c1
q"
REFERENCES,0.7642782969885774,"∥W ∗∥2 + dS min(rank(Σρ){rank(Σρ) + ln(c2/δ)}, d) ln(1 + n)."
REFERENCES,0.7653167185877466,"In addition, from Chang et al. (2021, Theorem 21), with probability 1 −δ, we also have"
REFERENCES,0.7663551401869159,"E(s,a)∼dπ∗
P ⋆[∥φ(s, a)∥Σ−1
n ] ≤c1"
REFERENCES,0.7673935617860852,"r ¯Cπ∗rank[Σρ]{rank[Σρ] + ln(c2/δ)} n
."
REFERENCES,0.7684319833852544,"Finally, by combining all things, we have"
REFERENCES,0.7694704049844237,"V π∗
P ⋆−V ˆπ
P ⋆≤c1(1 −γ)−2 min(d1/2, ¯R)
p ¯R r"
REFERENCES,0.770508826583593,dS ¯Cπ∗ln(1 + n)
REFERENCES,0.7715472481827622,"n
, ¯R = rank[Σρ]{rank[Σρ] + ln(c2/δ)}."
REFERENCES,0.7725856697819314,"E.6
PROOFS FOR LOW-RANK MDPS"
REFERENCES,0.7736240913811008,"Proof of Theorem 2. Until the second step, we can perform the same analysis as Theorem 1. More
concretely, with probability 1 −δ, we have P ⋆∈MD and"
REFERENCES,0.77466251298027,"Es,a∼ρ[TV(P(· | s, a), P ⋆(· | s, a))2] ≤ξ,
∀P ∈MD, ξ := cln(|M|/δ)"
REFERENCES,0.7757009345794392,"n
.
(32)"
REFERENCES,0.7767393561786086,"Hereafter, we condition on the above event."
REFERENCES,0.7777777777777778,Published as a conference paper at ICLR 2022
REFERENCES,0.778816199376947,"Letting f(s, a) = TV(P(· | s, a), P ⋆(· | s, a)), we use Lemma 4. Then,"
REFERENCES,0.7798546209761164,"E(s,a)∼dπ
P ⋆[f(s, a)] ≤E(s,a)∼dπ
P ⋆[∥φ⋆(s, a)∥Σ−1
ρ,φ⋆]
q"
REFERENCES,0.7808930425752856,"nγωπEρ[f 2(s, a)] + 4γ2λd +
q"
REFERENCES,0.7819314641744548,"(1 −γ)ωπEρ[f 2(s, a)]"
REFERENCES,0.7829698857736241,"where Σ = nEρ[φ⋆φ⋆⊤] + λI. We consider how to bound E(s,a)∼dπ
P ⋆[∥φ⋆(s, a)∥Σ−1
ρ,φ⋆]. This is
upper-bounded by"
REFERENCES,0.7840083073727934,"E(s,a)∼dπ
P ⋆[∥φ⋆(s, a)∥Σ−1
ρ,φ⋆] ≤
q"
REFERENCES,0.7850467289719626,"tr(E(s,a)∼dπ
P ⋆[φ⋆φ⋆⊤]Σ−1
ρ,φ⋆) ≤
q"
REFERENCES,0.7860851505711319,"¯Cπ∗,φ⋆tr(E(s,a)∼ρ[φ⋆φ⋆⊤]Σ−1
ρ,φ⋆)
(From Lemma 11) ≤
q"
REFERENCES,0.7871235721703012,"¯Cπ∗,φ⋆rank(Σρ)/n."
REFERENCES,0.7881619937694704,"Here, in the last line, by letting the SVD of Σρ = Eρ[φφ⊤] be U ˜ΣρU ⊤where ˜Σρ is a d×d diagonal
matrix and U is a d × d orthogonal matrix , we use"
REFERENCES,0.7892004153686397,"tr

ΣρΣ−1
ρ,φ⋆

= tr(U ˜ΣρU ⊤{nU ˜ΣρU ⊤+ λI}−1) = tr(˜ΣρU ⊤{nU ˜ΣρU ⊤+ λI}−1U)"
REFERENCES,0.7902388369678089,= tr(˜ΣρU ⊤{U{n˜Σρ + λI}U ⊤}−1U)
REFERENCES,0.7912772585669782,= tr(˜ΣρU ⊤U{n˜Σρ + λI}−1U ⊤U)
REFERENCES,0.7923156801661475,= tr(˜Σρ{n˜Σρ + λI})−1 ≤rank(Σρ)/n.
REFERENCES,0.7933541017653167,"Hence, when P ∈MD, by setting λ s.t. λd ≲nωπξ, we have"
REFERENCES,0.794392523364486,"E(s,a)∼dπ
P ⋆[f(s, a)] ≤ r"
REFERENCES,0.7954309449636553,"γ ¯Cπ∗,φ⋆rank(Σρ)ωπ ln(|M|/δ) n
+ r"
REFERENCES,0.7964693665628245,"(1 −γ)ωπ ln(|M|/δ) n
."
REFERENCES,0.7975077881619937,We use (32) here.
REFERENCES,0.7985462097611631,"Finally,"
REFERENCES,0.7995846313603323,"V π∗
P ⋆−V ˆπ
P ⋆"
REFERENCES,0.8006230529595015,"≤V π∗
P ⋆−min
P ∈MD V π∗
P
(Recall the proof of the third step in the proof of Theorem 1)"
REFERENCES,0.8016614745586709,"≤(1 −γ)−2Es,a∼dπ⋆
P ⋆TV(P ′(s, a), P ⋆(· | s, a))
(P ′ = arg minP ∈MD V π∗
P ) ≲ s"
REFERENCES,0.8026998961578401,"¯Cπ∗,φ⋆rank(Σρ)ωπ∗ln(|M|/δ)"
REFERENCES,0.8037383177570093,"n(1 −γ)4
."
REFERENCES,0.8047767393561787,"The following inequality is an important lemma to connect E(s,a)∼dπ
P ⋆{f(s, a)} with an elliptical
potential E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆."
REFERENCES,0.8058151609553479,"Lemma 4 (One-step back inequality). Take any f ⊂S × A →R s.t. ∥f∥∞≤B and 0 < λ ∈R.
Letting ω = maxs,a(π(a | s)/πb(a | s)), for any policy π, we have"
REFERENCES,0.8068535825545171,"|E(s,a)∼dπ
P ⋆{f(s, a)} | ≤E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
q
nωπγE(s,a)∼ρ [f 2(s, a)]
	
+ γ2λdB2 +
q"
REFERENCES,0.8078920041536864,"(1 −γ)ωπE(s,a)∼ρ [f 2(s, a)]."
REFERENCES,0.8089304257528557,"where Σ = nE(s,a)∼ρ[φ⋆(s, a)φ⋆⊤(s, a)] + λI."
REFERENCES,0.8099688473520249,"Proof of Lemma 4. First, we have an equality:"
REFERENCES,0.8110072689511942,"E(s,a)∼dπ
P ⋆{f(s, a)} = γE(˜s,˜a)∼dπ
P ⋆,s∼P ⋆(˜s,˜a) {f(s, a)} + (1 −γ)Es∼d0,a∼π(s0) {f(s, a)} .
(33)"
REFERENCES,0.8120456905503635,Published as a conference paper at ICLR 2022
REFERENCES,0.8130841121495327,The second term in (33) is upper-bounded by
REFERENCES,0.814122533748702,"Es∼d0,a∼π(s0) {f(s, a)} ≤Es∼d0,a∼π(s0)

f 2(s, a)
	
}1/2 =
q"
REFERENCES,0.8151609553478713,"ωπE(s,a)∼ρ [f 2(s, a)] /(1 −γ)."
REFERENCES,0.8161993769470405,"Next we consider the ﬁrst term in (33). By CS inequality, we have"
REFERENCES,0.8172377985462098,"E(˜s,˜a)∼dπ
P ⋆,s∼P ⋆(˜s,˜a) {f(s, a)}
 =
E(˜s,˜a)∼dπ
P ⋆φ⋆(˜s, ˜a)⊤
Z
ˆµ(s)π(a | s)f(s, a)d(s, a)"
REFERENCES,0.818276220145379,"≤E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆∥
Z
ˆµ(s)π(a | s)f(s, a)d(s, a)∥Σρ,φ⋆. Then,"
REFERENCES,0.8193146417445483,"∥
Z
ˆµ(s)π(a | s)f(s, a)d(s, a)∥2
Σρ,φ⋆"
REFERENCES,0.8203530633437176,"≤
Z
ˆµ(s)π(a | s)f(s, a)d(s, a)
⊤n
nE(s,a)∼ρ[φ⋆φ⋆⊤] + λI
o Z
ˆµ(s)π(a | s)f(s, a)d(s, a)
"
REFERENCES,0.8213914849428868,"≤n

E(˜s,˜a)∼ρ"
REFERENCES,0.822429906542056,"Z
ˆµ(s)⊤φ⋆(˜s, ˜a)π(a | s)f(s, a)d(s, a)
2
+ B2λd"
REFERENCES,0.8234683281412254,"(Use the assumption ∥f(s, a)∥∞≤B and ∥
R
ˆµ(s)d(s)∥2 ≤
√ d)"
REFERENCES,0.8245067497403946,"= n

E(˜s,˜a)∼ρ,s∼P ⋆(˜s,˜a),a∼π(s) [f(s, a)]
	2 + B2λd"
REFERENCES,0.8255451713395638,"≤n

E(˜s,˜a)∼ρ,s∼P ⋆(˜s,˜a),a∼π(s)

f 2(s, a)
	
+ B2λd.
(Jensen)"
REFERENCES,0.8265835929387332,"Finally, the the ﬁrst term in (33) is upper-bounded by"
REFERENCES,0.8276220145379024,"n

E(˜s,˜a)∼ρ,s∼P ⋆(˜s,˜a),a∼π(s)

f 2(s, a)
	
+ λdB2"
REFERENCES,0.8286604361370716,"≤nωπ

E(˜s,˜a)∼ρ,s∼P ⋆(˜s,˜a),a∼πb(s)

f 2(s, a)
	
+ λdB2
(Importance sampling) ≤nωπ  1"
REFERENCES,0.829698857736241,"γ E(s,a)∼ρ

f 2(s, a)

+ λdB2.
(Deﬁnition of ρ)"
REFERENCES,0.8307372793354102,The ﬁnal statement is immediately concluded.
REFERENCES,0.8317757009345794,"E.7
PROOFS FOR FACTORED MDPS"
REFERENCES,0.8328141225337488,"Proof of Theorem 3. We denote the constrained set as MD: MD = ( P =
Y"
REFERENCES,0.833852544132918,"i
Pi | ED
h
TV( bPMLE,i(· | s[pai], a), Pi(· | s[pai], a))2i
≤ξi, ∀i ∈[1, · · · , d] ) ."
REFERENCES,0.8348909657320872,"Following the ﬁrst step in the proof of Corollary 1, with probability 1 −δ, the product Q"
REFERENCES,0.8359293873312564,"i P ⋆
i is in
MD, i.e.,"
REFERENCES,0.8369678089304258,"Es,a∼D
h
TV( bPMLE,i(· | s[pai], a), P ⋆
i (· | s[pai], a))2i
≤ξi, ∀i ∈[1, · · · , d],
ξi =
q"
REFERENCES,0.838006230529595,"Li log(Lid/δ) n
."
REFERENCES,0.8390446521287642,"Note d comes from the union bound. Besides, following the second step in the proof of Corollary 1,
for any P ∈MD, with probability 1 −δ,"
REFERENCES,0.8400830737279336,"Es,a∼ρ
h
TV ( bPi(· | s[pai], a), P ⋆
i (· | s[pai], a))2i
≤ξi, ∀i ∈[1, · · · , d]."
REFERENCES,0.8411214953271028,Published as a conference paper at ICLR 2022
REFERENCES,0.842159916926272,"After conditioning on the above two events, then, for any P ∈MD and π⋆∈Π, we have"
REFERENCES,0.8431983385254413,"V π∗
P ⋆−V π∗
P
≤(1 −γ)−2E(s,a)∼dπ∗
P ⋆[TV(P(· | s, a), P ⋆(· | s, a))]"
REFERENCES,0.8442367601246106,"(Simulation lemma, Lemma 5)"
REFERENCES,0.8452751817237798,"≤(1 −γ)−2E(s,a)∼dπ∗
P ⋆[
X"
REFERENCES,0.8463136033229491,"i
TV(Pi(· | s[pai], a), P ⋆
i (· | s[pai], a))]"
REFERENCES,0.8473520249221184,≤(1 −γ)−2 X i
REFERENCES,0.8483904465212876,"v
u
u
tE(s,a)∼ρ"
REFERENCES,0.8494288681204569,"""dπ∗
P ⋆(s[pai], a)
ρ(s[pai], a) 2#"
REFERENCES,0.8504672897196262,"E(s,a)∼ρ[TV(Pi(· | s[pai], a), P ⋆
i (· | s[pai], a))2]"
REFERENCES,0.8515057113187954,(CS inequality)
REFERENCES,0.8525441329179647,≤(1 −γ)−2 X i q
REFERENCES,0.8535825545171339,"¨Cπ∗,∞E(s,a)∼ρ[TV(Pi(· | s, a), P ⋆
i (· | s, a))2] ≤(1 −γ)−2 X i q"
REFERENCES,0.8546209761163032,"¨Cπ∗,∞ξi"
REFERENCES,0.8556593977154725,"≤(1 −γ)−2
s"
REFERENCES,0.8566978193146417,"d ¨Cπ∗,∞
X"
REFERENCES,0.857736240913811,"i
ξi
(CS inequality)"
REFERENCES,0.8587746625129803,"≤c(1 −γ)−2
r"
REFERENCES,0.8598130841121495,"d ¨Cπ∗,∞
L ln(Lnd/δ) n
."
REFERENCES,0.8608515057113187,"Here, recall"
REFERENCES,0.8618899273104881,"¨Cπ∗,∞=
max
i∈[1,··· ,d] E(s,a)∼ρ"
REFERENCES,0.8629283489096573,"""dπ∗
P ⋆(s[pai], a)
ρ(s[pai], a) 2# ."
REFERENCES,0.8639667705088265,"Following the third step in the proof of Corollary 1, the statement is concluded."
REFERENCES,0.8650051921079959,"Next, we show that ¨Cπ∗,∞≤Cπ∗,P ⋆= maxs,a
dπ∗
P ⋆(s,a)"
REFERENCES,0.8660436137071651,"ρ(s,a) ."
REFERENCES,0.8670820353063343,"Proposition 1 (Comparison of L∞-density-ratio based concentrabiliity coefﬁcient between factored
MDPs and non-factored MDPs ). For any π∗, we have:
¨Cπ∗,∞≤Cπ∗,∞.
(34)"
REFERENCES,0.8681204569055037,"Proof. From now on, for any i ∈[1, · · · , d], by deﬁning S′
i s.t. S = Si × S′
i, we prove"
REFERENCES,0.8691588785046729,"max
si∈Si,a∈A
dπ∗
P ⋆(si, a)
ρ(si, a)
≤
max
s∈Si,s′
i∈Si,a∈A
dπ∗
P ⋆(si, s′
i, a)
ρ(si, s′
i, a)
= Cπ∗,∞."
REFERENCES,0.8701973001038421,"Then, (34) is easily proved."
REFERENCES,0.8712357217030114,"First, for any si ∈Si, a ∈A, we have"
REFERENCES,0.8722741433021807,"max
s′
i"
REFERENCES,0.8733125649013499,"dπ∗
P ⋆(si, s′
i, a)
ρ(si, s′
i, a)
= max
s′
i"
REFERENCES,0.8743509865005192,"dπ∗
P ⋆(si, a)dπ∗
P ⋆(s′
i | si, a)
ρ(si, a)ρ(s′
i | si, a)
= dπ∗
P ⋆(si, a)
ρ(si, a)
max
s′
i"
REFERENCES,0.8753894080996885,"dπ∗
P ⋆(s′
i | si, a)
ρ(s′
i | si, a)
≥dπ∗
P ⋆(si, a)
ρ(si, a) ."
REFERENCES,0.8764278296988577,"(35)
Here, we use"
REFERENCES,0.877466251298027,"1 ≤max
s′
i"
REFERENCES,0.8785046728971962,"dπ∗
P ⋆(s′
i | si, a)
ρ(s′
i | si, a) ,"
REFERENCES,0.8795430944963655,"which is proved by the contradiction argument, that is, if 1 > maxs′
i
dπ∗
P ⋆(s′
i|si,a)
ρ(s′
i|si,a) , both ρ and dπ∗
P ⋆
cannot be probability mass functions since we would get 1 =
X"
REFERENCES,0.8805815160955348,"s′
i
dπ∗
P ⋆(s′
i | si, a) ≤max
s′
i"
REFERENCES,0.881619937694704,"dπ∗
P ⋆(s′
i | si, a)
ρ(s′
i | si, a)  X"
REFERENCES,0.8826583592938733,"s′
i
ρ(s′
i | si, a) <
X"
REFERENCES,0.8836967808930426,"s′
i
ρ(s′
i | si, a)."
REFERENCES,0.8847352024922118,"Then, by taking the maximum over si ∈Si, a ∈A for both sides on (35), we have"
REFERENCES,0.885773624091381,"max
si,a
dπ∗
P ⋆(si, a)
ρ(si, a)
≤max
si,s′
i,a
dπ∗
P ⋆(si, s′
i, a)
ρ(s1, s′
i, a) ."
REFERENCES,0.8868120456905504,Published as a conference paper at ICLR 2022
REFERENCES,0.8878504672897196,"F
AUXILIARY LEMMAS"
REFERENCES,0.8888888888888888,"Lemma 5 (Simulation Lemma). Consider any two transitions P and bP, and any policy π : S →
∆(A). We have:"
REFERENCES,0.8899273104880582,"|V π
P −V π
b
P | ≤|(1 −γ)−1Es,a∼dπ
P [Es′∼P (s,a)[V π
b
P (s′)] −Es′∼P (s,a)[V π
b
P (s′)]]|"
REFERENCES,0.8909657320872274,"≤(1 −γ)−2Es,a∼dπ
P
h
TV(P(·|s, a), bP(·|s, a))
i
."
REFERENCES,0.8920041536863966,"Proof. Such simulation lemma is standard in model-based RL literature and the derivation can be
found, for instance, in the proof of Lemma 10 from Sun et al. (2019)."
REFERENCES,0.893042575285566,"Lemma 6 (MLE guarantee). Given a set of models M = {P : S × A →∆(S)} with P ⋆∈M,
and a dataset D = {si, ai, s′
i}n
i=1 with si, ai ∼ρ, and s′
i ∼P ⋆(si, ai), let bPMLE be"
REFERENCES,0.8940809968847352,"bPMLE = arg min
P ∈M n
X"
REFERENCES,0.8951194184839044,"i=1
−ln P(s′
i|si, ai)."
REFERENCES,0.8961578400830738,"With probability at least 1 −δ, we have:"
REFERENCES,0.897196261682243,"Es,a∼ρTV( bPMLE(·|s, a), P ⋆(·|s, a))2 ≲ln(|M|/δ) n
."
REFERENCES,0.8982346832814122,"Proof. Refer to (Agarwal et al., 2020b, Section E)"
REFERENCES,0.8992731048805815,Lemma 7 (MLE guarantee for tabular models).
REFERENCES,0.9003115264797508,"ED
h
TV(P(·|s, a), bPMLE(·|s, a))2i
≤|S|A|{|S| ln 2 + ln(2|S||A|/δ)}"
N,0.90134994807892,"2n
."
N,0.9023883696780893,"Proof. From Chang et al. (2021, Lemma 12) , with probability 1 −δ,"
N,0.9034267912772586,"TV(P(·|s, a), bPMLE(·|s, a))2 ≤|S| ln 2 + ln(2|S||A|/δ)"
N,0.9044652128764278,"2N(s, a)
∀(s, a) ∈S × A,"
N,0.9055036344755971,"where N(s, a) is the number of visiting times for (s, a). Then,"
N,0.9065420560747663,"ED
h
TV(P(·|s, a), bPMLE(·|s, a))2i
≤ED"
N,0.9075804776739356,|S| ln 2 + ln(2|S||A|/δ)
N,0.9086188992731049,"2N(s, a)  ≤
X (s,a)"
N,0.9096573208722741,|S| ln 2 + ln(2|S||A|/δ)
N,0.9106957424714434,2n 
N,0.9117341640706127,= |S|A|{|S| ln 2 + ln(2|S||A|/δ)}
N,0.9127725856697819,"2n
."
N,0.9138110072689511,Lemma 8 (MLE guarantee for KNRs).
N,0.9148494288681205,"
c
WMLE −W ⋆
(Σn)1/2
2 ≤βn."
N,0.9158878504672897,"Proof. The proof directly follows the conﬁdence ball construction and proof from (Kakade et al.,
2020)."
N,0.9169262720664589,"Lemma 9 (ℓ1 Distance between two Gaussians). Consider two Gaussian distributions P1 :=
N(µ1, ζ2I) and P2 := N(µ2, ζ2I). We have:"
N,0.9179646936656283,"TV(P1, P2) ≤1"
N,0.9190031152647975,ζ ∥µ1 −µ2∥2 .
N,0.9200415368639667,Published as a conference paper at ICLR 2022
N,0.9210799584631361,"Proof. This lemma is proved by Pinsker’s inequality and the closed-form of the KL divergence
between P1 and P2. Refer to (Kakade et al., 2020)."
N,0.9221183800623053,"Lemma 10 (Property of linear mixture MDPs). Let P(θ) = θ⊤ψ(s, a, s′). Suppose P(θ) ∈S ×
A →∆(S). For any function V ∈S →[0, 1], letting ψV (s, a) =
R
ψ(s, a, s′)V (s′)d(s′), we
suppose ∥ψV (s, a)∥2 ≤1. The following theorems hold:"
N,0.9231568016614745,"1. For any (s, a, s′), we have |P(θ)(s, a, s′) −P(θ′)(s, a, s′)| ≤∥θ −θ′∥2."
N,0.9241952232606438,"2. For any (s, a), we have TV(P(θ)(s, a, ·), P(θ′)(s, a, ·)) ≤∥θ −θ′∥2. Besides, for any
V : S →[0, 1], we have"
N,0.9252336448598131,"|(θ −θ′)ψV (s, a)| ≤TV(P(θ)(s, a, ·), P(θ′)(s, a, ·)). 3."
N,0.9262720664589823,"C†
π⋆,P ⋆= sup
x"
N,0.9273104880581516,"x⊤E(s,a)∼dπ∗
P ⋆[ψV(s,a,x)(s, a)ψ⊤
V(s,a,x)(s, a)]x"
N,0.9283489096573209,"x⊤E(s,a)∼ρ[ψV(s,a,x)(s, a)ψ⊤
V(s,a,x)(s, a)]x ,"
N,0.9293873312564901,"V(s,a,x) = arg max
V :S→[0,1]"
N,0.9304257528556594,"x⊤
Z
φ(s, a, s′)V (s′)d(s′)
 ."
N,0.9314641744548287,"4. In linear MDPs (i.e., ψ(s, a, s′) = φ(s, a) ⊗µ(s′)), we have"
N,0.9325025960539979,"sup
V ∈{S→[0,1]}
sup
x"
N,0.9335410176531672,"x⊤E(s,a)∼dπ∗
P ⋆[ψV (s, a)ψ⊤
V (s, a)]x"
N,0.9345794392523364,"x⊤E(s,a)∼ρ[ψV (s, a)ψ⊤
V (s, a)]x
= sup
x"
N,0.9356178608515057,"x⊤Edπ∗
P ⋆[φ(s, a)φ(s, a)⊤]x"
N,0.936656282450675,"x⊤Eρ[φ(s, a)φ(s, a)⊤]x ."
N,0.9376947040498442,Proof. We prove the ﬁrst statement. This is proved by
N,0.9387331256490135,"|P(θ) −P(θ′)| = |(θ −θ′)ψ(s, a, s′)| ≤∥θ −θ′∥2∥ψ(s, a, s′)∥2 ≤∥θ −θ′∥2,"
N,0.9397715472481828,"Here, we use ∥ψ(s, a, s′)∥2 ≤1 which is proved by the assumption by setting V (s) = I(s′ = s)
for any s′."
N,0.940809968847352,"Next, we prove the second statement. For ﬁxed θ ∈Rd and (s, a) ∈S × A, we have"
N,0.9418483904465212,"TV(P(θ)(s, a, ·), P(θ⋆)(s, a, ·)) =
sup
V :S→[0,1]
|
Z
(θ −θ⋆)⊤ψ(s, a, s′)V (s′)d(s′)|"
N,0.9428868120456906,"=
sup
V :S→[0,1]
|(θ −θ⋆)⊤
Z
ψ(s, a, s′)V (s′)d(s′)|"
N,0.9439252336448598,"= |(θ −θ⋆)⊤
Z
ψ(s, a, s′)V(s,a,θ)(s′)d(s′)|"
N,0.944963655244029,"= |(θ −θ⋆)⊤ψV(s,a,θ)(s, a)|."
N,0.9460020768431984,"In the third line, we deﬁne V (s, a, θ) = arg maxV :S→[0,1] |(θ −θ⋆)⊤R
ψ(s, a, s′)V (s′)d(s′)|."
N,0.9470404984423676,"Then, from CS inequality,"
N,0.9480789200415368,"TV(P(θ)(s, a, ·), P(θ⋆)(s, a, ·)) ≤∥(θ −θ⋆∥2∥ψV(s,a,θ)(s, a)|∥2 ≤∥θ −θ⋆∥2."
N,0.9491173416407062,"We use the assumption ∥ψV(s,a,θ)(s, a)∥2 ≤1. This concludes the second statement. Besides, for
any V : S →[0, 1], we have"
N,0.9501557632398754,"|(θ −θ′)ψV (s, a)| ≤|(θ −θ⋆)⊤ψV(s,a,θ)(s, a)|"
N,0.9511941848390446,"≤TV(P(θ)(s, a, ·), P(θ′)(s, a, ·))."
N,0.952232606438214,The third statement is immediately concluded by
N,0.9532710280373832,"E(s,a)∼dπ∗
P ⋆[TV(P(θ)(s, a, ·), P(θ⋆)(s, a, ·))2]"
N,0.9543094496365524,"E(s,a)∼ρ[TV(P(θ)(s, a, ·), P(θ⋆)(s, a, ·))2]
=
E(s,a)∼dπ∗
P ⋆[|(θ −θ⋆)⊤ψV(s,a,θ)(s, a)|2]"
N,0.9553478712357217,"E(s,a)∼ρ[|(θ −θ⋆)⊤ψV(s,a,θ)(s, a)|2] .
(36)"
N,0.956386292834891,Published as a conference paper at ICLR 2022
N,0.9574247144340602,"Finally, we prove the fourth statement. Suppose ψ(s, a, s′) = φ(s, a) ⊗µ(s′) (⊗denotes kronerker
product). Then, φV (s, a, s′) = φ(s, a) ⊗
R
µ(s′)V (s′)d(s′). Then, by deﬁning a vector µ(V ) =
R
µ(s′)V (s′)d(s′), we immediately have"
N,0.9584631360332295,"x⊤E(s,a)∼dπ∗
P ⋆[ψV (s, a)ψ⊤
V (s, a)]x"
N,0.9595015576323987,"x⊤E(s,a)∼ρ[ψV (s, a)ψ⊤
V (s, a)]x
= sup
x"
N,0.960539979231568,"x⊤E(s,a)∼dπ∗
P ⋆[(φ(s, a) ⊗µ(V ))(φ(s, a) ⊗µ(V ))⊤]x"
N,0.9615784008307373,"x⊤E(s,a)∼ρ[(φ(s, a) ⊗µ(V ))(φ(s, a) ⊗µ(V ))⊤]x . (37)"
N,0.9626168224299065,"Here, we have"
N,0.9636552440290758,"Eρ[(φ(s, a) ⊗µ(V ))(φ(s, a) ⊗µ(V ))⊤] = Eρ[(φ(s, a) ⊗µ(V ))(φ(s, a)⊤⊗µ(V )⊤)]"
N,0.9646936656282451,"= Eρ[(φ(s, a)φ(s, a)⊤)] ⊗(µ(V )µ(V )⊤)."
N,0.9657320872274143,We notice
N,0.9667705088265836,"{Eρ[(φ(s, a)φ(s, a)⊤)] ⊗(µ(V )µ(V )⊤)}1/2 = Eρ[φ(s, a)φ(s, a)⊤]1/2 ⊗(µ(V )µ(V )⊤)1/2."
N,0.9678089304257529,"This is because the square root of a matrix is unique and we have (A1/2 ⊗B1/2)(A1/2 ⊗B1/2) =
AB for symmetric matrices A and B. Then, by denoting Fρ = Eρ[φ(s, a)φ(s, a)⊤], Fdπ
P ⋆=
Edπ
P ⋆[φ(s, a)φ(s, a)⊤] and denoting the pseudo inverse of F as F +, we can see (37) is equal to"
N,0.9688473520249221,"{F 1/2
ρ
⊗(µ(V )µ(V )⊤)1/2}+{Fdπ
P ⋆⊗(µ(V )µ(V )⊤)}{F 1/2
ρ
⊗(µ(V )µ(V )⊤)1/2}+"
N,0.9698857736240913,"= {F −1/2
ρ
⊗(µ(V )µ(V )⊤)−1/2}{Fdπ
P ⋆⊗(µ(V )µ(V )⊤)}{F −1/2
ρ
⊗(µ(V )µ(V )⊤)−1/2}"
N,0.9709241952232607,"= {F −1/2
ρ
Fdπ
P ⋆F −1/2
ρ
} ⊗{(µ(V )µ(V )⊤)−1/2(µ(V )µ(V )⊤)(µ(V )µ(V )⊤)−1/2}"
N,0.9719626168224299,"= {F −1/2
ρ
Fdπ
P ⋆F −1/2
ρ
} ⊗Ik (k = rank(µ(V )µ(V )⊤))."
N,0.9730010384215991,"Here, Ik is a diagonal matrix s.t. k ∈N+ values in the diagonal entries are 1 and the rest of
values are 0. Then, the maximum singular value of {F −1/2
ρ
Fdπ
P ⋆F −1/2
ρ
} ⊗Ik is equal to the one of"
N,0.9740394600207685,"{F −1/2
ρ
Fdπ
P ⋆F −1/2
ρ
}. This is equal to sup
x"
N,0.9750778816199377,"x⊤Fdπ
P ⋆x x⊤Fρx"
N,0.9761163032191069,"Hence, the fourth statement is concluded."
N,0.9771547248182763,"Lemma 11 (Distribution shift lemma). Suppose A1, A2, A3 are semipositive deﬁnite matrices:"
N,0.9781931464174455,"Tr(A1A2) ≤σmax(A−1/2
3
A1A−1/2
3
) Tr(A3A2). Note"
N,0.9792315680166147,"σmax(A−1/2
3
A1A−1/2
3
) = sup
x∈Rd
x⊤A1x
x⊤A3x."
N,0.980269989615784,Proof.
N,0.9813084112149533,"Tr(A1A2) = Tr(A1/2
1
A2A1/2
1
) = Tr(A1/2
1
A−1/2
3
A1/2
3
A2A1/2
3
A−1/2
3
A1/2
1
)"
N,0.9823468328141225,"= Tr(A−1/2
3
A1A−1/2
3
A1/2
3
A2A1/2
3
)."
N,0.9833852544132918,"In addition, for any semipositive deﬁnite matrices A, B we have"
N,0.9844236760124611,"Tr(AB) = Tr(UΛU ⊤B) = Tr(ΛU ⊤BU) ≤σmax(Λ) Tr(U ⊤BU) = σmax(A) Tr(B),"
N,0.9854620976116303,where UΛU ⊤is the SVD decomoposition of A. This concludes that
N,0.9865005192107996,"Tr(A1A2) ≤σmax(A−1/2
3
A1A−1/2
3
) Tr(A3A2)."
N,0.9875389408099688,Published as a conference paper at ICLR 2022
N,0.9885773624091381,"The following lemma is useful to obtain the generalized result of Theorem 1. The proof is given in
(Wainwright, 2019, Theorem 3.27). We ﬁrst deﬁne"
N,0.9896157840083074,"Z = sup
f∈F
|{ED −Eρ}[f]"
N,0.9906542056074766,"Σ2 = sup
f∈F
ED[{f(s, a) −Eρ[f(s, a)]}2], σ2 = sup
f∈F
var[f(s, a)]."
N,0.9916926272066459,"Lemma 12 (Functional Bernstein’s inequality: Talagrand concentration inequality for empirical
process). Suppose ∥f∥∞≤B. With probability 1 −δ,"
N,0.9927310488058152,"|Z −E[Z]| ≤Σ2
r"
N,0.9937694704049844,log(c/δ)
N,0.9948078920041536,"n
+ B log(c/δ) n
."
N,0.995846313603323,"As an immediate corollary,"
N,0.9968847352024922,|Z −E[Z]| ≤{σ2 + BE[Z]} r
N,0.9979231568016614,log(c/δ)
N,0.9989615784008308,"n
+ B log(c/δ) n
."
