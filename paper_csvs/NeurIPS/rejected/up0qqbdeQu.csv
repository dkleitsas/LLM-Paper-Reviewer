Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001177856301531213,"The power of large vision-language models (VLMs) has been demonstrated for
1"
ABSTRACT,0.002355712603062426,"diverse vision tasks including multi-label recognition with training-free approach or
2"
ABSTRACT,0.0035335689045936395,"prompt tuning by measuring the cosine similarity between the text features related
3"
ABSTRACT,0.004711425206124852,"to class names and the visual features of images. Prior works usually formed the
4"
ABSTRACT,0.005889281507656066,"class-related text features by averaging simple hand-crafted text prompts with class
5"
ABSTRACT,0.007067137809187279,"names (e.g., “a photo of {class name}”). However, they may not fully exploit the
6"
ABSTRACT,0.008244994110718492,"capability of VLMs considering how humans form the concepts on words using rich
7"
ABSTRACT,0.009422850412249705,"contexts with the patterns of co-occurrence with other words. Inspired by that, we
8"
ABSTRACT,0.01060070671378092,"propose class concept representation for zero-shot multi-label recognition to better
9"
ABSTRACT,0.011778563015312132,"exploit rich contexts in the massive descriptions on images (e.g., captions from MS-
10"
ABSTRACT,0.012956419316843345,"COCO) using large VLMs. Then, for better aligning visual features of VLMs to our
11"
ABSTRACT,0.014134275618374558,"class concept representation, we propose context-guided visual representation that
12"
ABSTRACT,0.015312131919905771,"is in the same linear space as class concept representation. Experimental results
13"
ABSTRACT,0.016489988221436984,"on diverse benchmarks show that our proposed methods substantially improved
14"
ABSTRACT,0.0176678445229682,"the performance of zero-shot methods like Zero-Shot CLIP and yielded better
15"
ABSTRACT,0.01884570082449941,"performance than zero-shot prompt tunings that require additional training like
16"
ABSTRACT,0.020023557126030624,"TaI-DPT. In addition, our proposed methods can synergetically work with existing
17"
ABSTRACT,0.02120141342756184,"prompt tuning methods, consistently improving the performance of DualCoOp and
18"
ABSTRACT,0.02237926972909305,"TaI-DPT in a training-free manner with negligible increase in inference time.
19"
INTRODUCTION,0.023557126030624265,"1
Introduction
20"
INTRODUCTION,0.024734982332155476,"The goal of multi-label image recognition is to assign all semantic labels (or class names) within an
21"
INTRODUCTION,0.02591283863368669,"image [10, 44, 48, 11, 27, 33, 31]. Differing from single-label recognition, multi-label recognition
22"
INTRODUCTION,0.027090694935217905,"addresses a broader range of practical applications such as image retrieval [36, 39], recommendation
23"
INTRODUCTION,0.028268551236749116,"systems [52, 8], medical diagnosis recognition [43] and retail checkout recognition [17, 45]. However,
24"
INTRODUCTION,0.02944640753828033,"one of the challenges in multi-label recognition is the difficulty of collecting full label annotations,
25"
INTRODUCTION,0.030624263839811542,"which is laborious and prone to missing. To alleviate it, recent works have investigated training with
26"
INTRODUCTION,0.03180212014134275,"incomplete labels such as partial labels [37, 6, 31, 15, 9] or a single positive label [13, 46].
27"
INTRODUCTION,0.03297997644287397,"Recent advances of large vision-language models (VLMs) [32, 2, 22, 25, 47, 49] has demon-
28"
INTRODUCTION,0.03415783274440518,"strated their strong transferability on various downstream tasks with great performance. Contrastive
29"
INTRODUCTION,0.0353356890459364,"Language-Image Pretraining (CLIP) achieved impressive performance in zero-shot classification by
30"
INTRODUCTION,0.03651354534746761,"measuring the cosine similarity between images and class-related hand-crafted text prompts [32].
31"
INTRODUCTION,0.03769140164899882,"Fine-tuning VLMs for adapting desired downstream datasets [32] can further improve performance
32"
INTRODUCTION,0.038869257950530034,"for targeted tasks, but tuning millions of parameters is usually undesirable due to computation burden
33"
INTRODUCTION,0.04004711425206125,"and possible forgetting. Prompt tuning has been investigated as an efficient and low-cost training
34"
INTRODUCTION,0.04122497055359246,"paradigm [54, 53], learning only a few context tokens of VLMs for a given task. In multi-label
35"
INTRODUCTION,0.04240282685512368,"recognition, prompt tuning with CLIP has been investigated for distinguishing multiple objects in an
36"
INTRODUCTION,0.043580683156654886,"Figure 1: Illustration of our methods applied to zero-shot CLIP (ZSCLIP) [32]. (a→b) Class concept
is formed from the text descriptions that contain rich contextual information with relevant class names
and other related words, yielding substantially improved performance without aligning with visual
features yet. (b→c) Context-guided visual feature is transformed from visual feature so that it is in
the same linear space as class concept representation, yielding significantly improved performance."
INTRODUCTION,0.0447585394581861,"image [37, 18, 41], mitigating the difficulty of acquiring annotated samples. However, prompt tuning
37"
INTRODUCTION,0.045936395759717315,"inherently requires labeled data with additional training and may be susceptible to overfitting for
38"
INTRODUCTION,0.04711425206124853,"context tokens, hindering generalization. The capability of VLMs for label-free and/or training-free
39"
INTRODUCTION,0.048292108362779744,"classification has been exploited using prompt engineering [32, 34, 50, 4]. However, prompt ensem-
40"
INTRODUCTION,0.04946996466431095,"bles by averaging text features from simple hand-crafted prompts (e.g.,“a sketch of {class name}”)
41"
INTRODUCTION,0.050647820965842166,"yielded marginal improvements and struggled with multi-label recognition. Thus, the approach of
42"
INTRODUCTION,0.05182567726737338,"prior works on zero-shot or prompt-tuning based multi-label recognition using class names to obtain
43"
INTRODUCTION,0.053003533568904596,"class-related text features from VLMs may not use the full capacity of VLMs properly.
44"
INTRODUCTION,0.05418138987043581,"Humans form concepts on words from past experience, especially using their patterns of co-occurrence
45"
INTRODUCTION,0.05535924617196702,"with other words [5, 29, 20]. Inspired by this perspective in cognitive neuroscience, we propose a
46"
INTRODUCTION,0.05653710247349823,"novel approach of exploiting VLMs for multi-label recognition by replacing single class name-related
47"
INTRODUCTION,0.05771495877502945,"hand-crafted prompts with our proposed class concept representation using text descriptions such
48"
INTRODUCTION,0.05889281507656066,"as “A person holding a large pair of scissors,” capturing rich contextual information with target
49"
INTRODUCTION,0.06007067137809187,"class names (e.g., person) as well as related words (e.g., holding, scissors). Our class concept will
50"
INTRODUCTION,0.061248527679623084,"be constructed from rich contextual descriptions on classes that may contain diverse and realistic
51"
INTRODUCTION,0.0624263839811543,"patterns of co-occurrence with target class name and other related class names. Then, this novel text
52"
INTRODUCTION,0.0636042402826855,"features with class concept representation requires aligned visual features with them for multi-label
53"
INTRODUCTION,0.06478209658421673,"recognition to properly match them with our class concepts. Thus, we propose context-guided visual
54"
INTRODUCTION,0.06595995288574794,"features to bring VLM’s visual features to the same representation domain as our class concept
55"
INTRODUCTION,0.06713780918727916,"representation by using our sequential attention. See Fig. 1 for the differences of performing multi-
56"
INTRODUCTION,0.06831566548881036,"label recognition using (a) prior zero-shot approach (ZS-CLIP), (b) our proposed class concepts from
57"
INTRODUCTION,0.06949352179034157,"text descriptions and (c) our proposed context-guided visual features on the same space as the class
58"
INTRODUCTION,0.0706713780918728,"concepts. We demonstrated that our proposed methods achieved improved performance on multiple
59"
INTRODUCTION,0.071849234393404,"benchmark datasets without additional training (tuning), without additional labels (text-image pairs)
60"
INTRODUCTION,0.07302709069493522,"and with negligible increase in inference time. Here is the summary of the contributions:
61"
INTRODUCTION,0.07420494699646643,"• Proposing a novel class concept representation for training-free multi-label recognition tasks
62"
INTRODUCTION,0.07538280329799764,"using VLMs from massive text descriptions inspired by how human forms concept on words.
63"
INTRODUCTION,0.07656065959952886,"• Proposing a context-guided visual feature, transformed onto the same text feature space as
64"
INTRODUCTION,0.07773851590106007,"class concepts using sequential attention for better aligning multi-modal features.
65"
INTRODUCTION,0.07891637220259129,"• Demonstrating that our methods synergetically improve the performance of ZSCLIP and
66"
INTRODUCTION,0.0800942285041225,"other state-of-the-art prompt tuning methods with a negligible increase in inference time.
67"
RELATED WORKS,0.0812720848056537,"2
Related Works
68"
RELATED WORKS,0.08244994110718493,"Multi-label image recognition with CLIP. Multi-Label Recognition (MLR) aims to identify all
69"
RELATED WORKS,0.08362779740871613,"semantic labels within an image. However, it is difficult to collect the annotation of multi-label images
70"
RELATED WORKS,0.08480565371024736,"which involve complex scenes and diverse objects. Recently, prompt tuning with the pre-trained vision-
71"
RELATED WORKS,0.08598351001177856,"language model CLIP has been developed to address the high labeling costs of multi-label images in
72"
RELATED WORKS,0.08716136631330977,"incomplete label setting. Among them, DualCoOp [37] proposed a novel prompt tuning approach
73"
RELATED WORKS,0.08833922261484099,"that trains positive and negative learnable contexts with class names in the partially labeled setting.
74"
RELATED WORKS,0.0895170789163722,"For mitigating data-limited or label-limited issues, TaI-DPT [18] proposed effective dual-grained
75"
RELATED WORKS,0.09069493521790342,"prompt tuning method using easily accessible text descriptions. It is worth noting that TaI-DPT
76"
RELATED WORKS,0.09187279151943463,"used the same text descriptions as ours not for performing training-free multi-label recognition
77"
RELATED WORKS,0.09305064782096584,"itself, but for label-free prompt tuning by replacing the image features with the contextual text
78"
RELATED WORKS,0.09422850412249706,"features (text as image) under the conventional framework of multi-label recognition with class
79"
RELATED WORKS,0.09540636042402827,"name. SCPNet [14] is designed to leverage the structured semantic prior from CLIP to complement
80"
RELATED WORKS,0.09658421672555949,"deficiency of label supervision for MLR with incomplete labels. CDUL [1] proposed unsupervised
81"
RELATED WORKS,0.0977620730270907,"multi-label recognition through pseudo-labeling using CLIP, alleviating the annotation burden. Even
82"
RELATED WORKS,0.0989399293286219,"though recent works has demonstrated outstanding performance of multi-label recognition task, they
83"
RELATED WORKS,0.10011778563015312,"still require tuning costs or labeled dataset to adapt pre-trained CLIP to various downstream tasks. In
84"
RELATED WORKS,0.10129564193168433,"this work, our method enables training-free and label-free adaptation of CLIP into downstream tasks,
85"
RELATED WORKS,0.10247349823321555,"utilizing the text descriptions.
86"
RELATED WORKS,0.10365135453474676,"Training-free enhancement with CLIP. For single-label recognition, recent works has developed
87"
RELATED WORKS,0.10482921083627797,"the training-free enhancement of CLIP. ZPE [4] weighted-averaged many prompts by automatically
88"
RELATED WORKS,0.10600706713780919,"scoring the importance of each prompt in zero-shot manner for improving prompt ensemble technique.
89"
RELATED WORKS,0.1071849234393404,"CALIP [19] designed a simple parameter-free attention module for zero-shot enhancement over CLIP
90"
RELATED WORKS,0.10836277974087162,"without any tuning of model parameter. With few-shot samples, Tip-Adapter [51] proposed training-
91"
RELATED WORKS,0.10954063604240283,"free approach for fast adaptation to target task, obtaining the weights of adapter using few-shot
92"
RELATED WORKS,0.11071849234393404,"samples during inference. Since these methods were originally developed for single-label recognition,
93"
RELATED WORKS,0.11189634864546526,"it is difficult to be directly applied to multi-label recognition. In multi-label recognition, our method
94"
RELATED WORKS,0.11307420494699646,"enables training-free enhancement and demonstrated its effectiveness on the benchmark dataset.
95"
METHOD,0.11425206124852769,"3
Method
96"
METHOD,0.1154299175500589,"First of all, we propose class concept representation as a training-free approach for multi-label
97"
METHOD,0.1166077738515901,"recognition instead of class name by exploiting pre-trained VLM and rich contextual text descriptions.
98"
METHOD,0.11778563015312132,"Secondly, we also propose context-guided visual feature that can enhance the alignment of the
99"
METHOD,0.11896348645465253,"visual feature of VLM with our novel class concept. Our proposed methods are label-free as well as
100"
METHOD,0.12014134275618374,"training-free so that they can be applicable synergetically for most existing VLM-based multi-label
101"
METHOD,0.12131919905771496,"recognition methods. The overall pipeline of our method is illustrated in Figure 2.
102"
CLASS CONCEPT REPRESENTATION,0.12249705535924617,"3.1
Class Concept Representation
103"
CLASS CONCEPT REPRESENTATION,0.12367491166077739,"Humans form concepts on words from past experience, often using their patterns of co-occurrence
104"
CLASS CONCEPT REPRESENTATION,0.1248527679623086,"with other words [5, 29, 20]. For example, the word “apple” does not exist alone, but often comes
105"
CLASS CONCEPT REPRESENTATION,0.12603062426383982,"with the verb “eat” or the noun “basket.” However, it may not well associate with other words such
106"
CLASS CONCEPT REPRESENTATION,0.127208480565371,"as “fly” or “space.” Fortunately, we can easily obtain rich contextual text descriptions from various
107"
CLASS CONCEPT REPRESENTATION,0.12838633686690223,"public sources, including captions from benchmark datasets [26, 23, 24, 30], web crawling and large
108"
CLASS CONCEPT REPRESENTATION,0.12956419316843346,"language models [38, 7, 40, 28]. These text descriptions do not only contain class names, but also
109"
CLASS CONCEPT REPRESENTATION,0.13074204946996468,"include other words like class-related verbs and nouns in real-world contexts.
110"
CLASS CONCEPT REPRESENTATION,0.13191990577149587,"Assume that rich contextual text descriptions were gathered from the public sources that include one
111"
CLASS CONCEPT REPRESENTATION,0.1330977620730271,"or multiple class names. We denote the set of text descriptions as Zall = {z1, z2, ..., zM} where zi
112"
CLASS CONCEPT REPRESENTATION,0.13427561837455831,"refers to an individual text description. M denotes the total number of text descriptions across all
113"
CLASS CONCEPT REPRESENTATION,0.1354534746760895,"classes. Note that M can be dynamically changed at inference since our proposed method does not
114"
CLASS CONCEPT REPRESENTATION,0.13663133097762073,"require additional training, thus can be seen as test-time adaptation. Assuming that the target task
115"
CLASS CONCEPT REPRESENTATION,0.13780918727915195,"uses the class names of person, scissors, clock, building and cake, the examples of the contextual text
116"
CLASS CONCEPT REPRESENTATION,0.13898704358068315,"descriptions from Zall are as follows:
117"
CLASS CONCEPT REPRESENTATION,0.14016489988221437,"“A person holding a large pair of scissors.”
118"
CLASS CONCEPT REPRESENTATION,0.1413427561837456,"“A clock mounted on top of a building in the city.”
119"
CLASS CONCEPT REPRESENTATION,0.1425206124852768,"“Half of a white cake with coconuts on top.”
120"
CLASS CONCEPT REPRESENTATION,0.143698468786808,"TaI-DPT [18] used these descriptions with rich contextual information as a surrogate for images
121"
CLASS CONCEPT REPRESENTATION,0.14487632508833923,"to propose a label-free prompt tuning. In this work, we propose to use these descriptions to form
122"
CLASS CONCEPT REPRESENTATION,0.14605418138987045,"concepts on class names to compare with images, so that ways of using them are completely different.
123"
CLASS CONCEPT REPRESENTATION,0.14723203769140164,"Figure 2: (a) Overall pipeline of our method. 1) Class concept representation: VLM’s text features
from the rich contextual descriptions associated with each class name are used to construct the class
concept. 2) Context-guided visual features: VLM’s visual features are sequentially transformed onto
the class concept representation space using (b) sequential attention mechanism."
CLASS CONCEPT REPRESENTATION,0.14840989399293286,"We define the class concept as a vector in the space constructed by the text descriptions as fol-
124"
CLASS CONCEPT REPRESENTATION,0.14958775029446408,"lows. Firstly, the linear space Z can be constructed by spanning the VLM’s text features from
125"
CLASS CONCEPT REPRESENTATION,0.15076560659599528,"all text descriptions zi in Zall using the VLM’s text encoder Etxt(zi) ∈R1×D, leading to
126"
CLASS CONCEPT REPRESENTATION,0.1519434628975265,"Z = span{Etxt(z1), Etxt(z2), . . . , Etxt(zM)}. Secondly, we propose the class concept for a target
127"
CLASS CONCEPT REPRESENTATION,0.15312131919905772,"class name c as a vector tconcept
c
in the space Z by defining it as follows:
128"
CLASS CONCEPT REPRESENTATION,0.15429917550058891,"tconcept
c
= M
X"
CLASS CONCEPT REPRESENTATION,0.15547703180212014,"i=1
wc,i1c(zi)Etxt(zi) ∈R1×D
(1)"
CLASS CONCEPT REPRESENTATION,0.15665488810365136,"where 1c(zi) an indicator function such that 1c(zi) = 1 if the text description zi contains the class
129"
CLASS CONCEPT REPRESENTATION,0.15783274440518258,"name c and 1c(zi) = 0 otherwise. The weight wc,i is assigned to the text feature of each text
130"
CLASS CONCEPT REPRESENTATION,0.15901060070671377,"description within a class c and it is assumed to be normalized within the class. In this work, we set
131"
CLASS CONCEPT REPRESENTATION,0.160188457008245,"wc,i = 1/ PM
j 1c(zj) for ∀i, thus will be the same for all i for each class, which was guided by the
132"
CLASS CONCEPT REPRESENTATION,0.16136631330977622,"prior work on prompt ensembling [4], demonstrated that the prompt ensembling with equal weights
133"
CLASS CONCEPT REPRESENTATION,0.1625441696113074,"achieved significant performance gains that were comparable to weighted ensembling for single-label
134"
CLASS CONCEPT REPRESENTATION,0.16372202591283863,"recognition. Each class concept can be stored individually or together as a matrix.
135"
CLASS CONCEPT REPRESENTATION,0.16489988221436985,"Our class concept representation thus consists of various text features including diverse contextual
136"
CLASS CONCEPT REPRESENTATION,0.16607773851590105,"information related to the target class name. For instance, the descriptions for the class name “dog”
137"
CLASS CONCEPT REPRESENTATION,0.16725559481743227,"should contain the target class name as the following examples of the text descriptions:
138"
CLASS CONCEPT REPRESENTATION,0.1684334511189635,"“A dog greets a sheep that is in a sheep pen.”
139"
CLASS CONCEPT REPRESENTATION,0.1696113074204947,"“A woman walks her dog on a city sidewalk.”
140"
CLASS CONCEPT REPRESENTATION,0.1707891637220259,"“A dog with goggles is in a motorcycle side car.”
141"
CLASS CONCEPT REPRESENTATION,0.17196702002355713,"Note that the descriptions include the target class name (bold) as well as other related words in class-
142"
CLASS CONCEPT REPRESENTATION,0.17314487632508835,"related contexts (underline) as intended. We expect that our novel class concepts will be beneficial for
143"
CLASS CONCEPT REPRESENTATION,0.17432273262661954,"multi-label recognition due to other nouns (other class names) as well as other verbs to better explain
144"
CLASS CONCEPT REPRESENTATION,0.17550058892815076,"the context where the target class name is used. In this work, we obtain the texts from two sources to
145"
CLASS CONCEPT REPRESENTATION,0.17667844522968199,"collect the sufficient contextual text descriptions. The first source is the MS-COCO dataset [26] that is
146"
CLASS CONCEPT REPRESENTATION,0.17785630153121318,"publicly available and the second source is large language model(i.e., GPT-3.5[28]) that can generate
147"
CLASS CONCEPT REPRESENTATION,0.1790341578327444,"the several sentences quickly if the set of class names related to the target task were provided.
148"
CONTEXT-GUIDED VISUAL FEATURE,0.18021201413427562,"3.2
Context-Guided Visual Feature
149"
CONTEXT-GUIDED VISUAL FEATURE,0.18138987043580684,"Our novel class concept representation forms new vectors for diverse class names in the linear space
150"
CONTEXT-GUIDED VISUAL FEATURE,0.18256772673733804,"Z instead of the embedding space of the VLM where the text and image encoders were relatively
151"
CONTEXT-GUIDED VISUAL FEATURE,0.18374558303886926,"well-aligned. Thus, it is expected that the class concept representation and the VLM’s visual feature
152"
CONTEXT-GUIDED VISUAL FEATURE,0.18492343934040048,"Figure 3: Softmax values can be used to weigh the relevance with the given image. However, (a) naive
attention mechanisms yielded almost equal softmax values, thus may include texts with low relevance.
The proposed sequential attention method focuses on a subset of texts most relevant to the test image,
thus can transforms visual features to context-guided visual features for multi-label recognition by
assigning very high softmax value to the relevant text at index 0 while very low softmax value to the
irrelevant text at index 5000."
CONTEXT-GUIDED VISUAL FEATURE,0.18610129564193167,"may not be aligned well. Here, we propose context-guided visual feature by transforming the visual
153"
CONTEXT-GUIDED VISUAL FEATURE,0.1872791519434629,"features of the VLM onto the same space as the class concept representation Z by using our sequential
154"
CONTEXT-GUIDED VISUAL FEATURE,0.18845700824499412,"attention with the text descriptions Zall that were used for class concept construction.
155"
CONTEXT-GUIDED VISUAL FEATURE,0.1896348645465253,"For the target image q and the VLM’s visual encoder Eimg(q), the L2-normalized global visual feature
156"
CONTEXT-GUIDED VISUAL FEATURE,0.19081272084805653,"f is obtained by using Eimg(q) ∈R1×D and the flatten local visual feature F ∈RHW ×D is also
157"
CONTEXT-GUIDED VISUAL FEATURE,0.19199057714958775,"constructed by using Eimg(Pi,j(q)) where Pi,j(·) is an extractor of the (i, j)th patch of the input image.
158"
CONTEXT-GUIDED VISUAL FEATURE,0.19316843345111898,"Then, we aim to transform both the global visual feature vector f and the local visual feature matrix F
159"
CONTEXT-GUIDED VISUAL FEATURE,0.19434628975265017,"onto the same linear space Z as our class concept representation. One easy way is to “project” these
160"
CONTEXT-GUIDED VISUAL FEATURE,0.1955241460541814,"visual features f and F onto the space Z by computing the cosine similarity between visual features
161"
CONTEXT-GUIDED VISUAL FEATURE,0.1967020023557126,"(f and the column vectors of F) and all the text features ti = Etxt(zi) ∈R1×D, i = 1, . . . , M
162"
CONTEXT-GUIDED VISUAL FEATURE,0.1978798586572438,"that constructed Z. Unfortunately, when the softmax function is applied to the cosine similarity
163"
CONTEXT-GUIDED VISUAL FEATURE,0.19905771495877503,"values, they tend to become similar, thus weigh both relevant and irrelevant texts almost equally
164"
CONTEXT-GUIDED VISUAL FEATURE,0.20023557126030625,"as illustrated in Figure 3 (a). To address this challenge, we propose sequential attention, applying
165"
CONTEXT-GUIDED VISUAL FEATURE,0.20141342756183744,"the softmax function to part of the cosine similarity values by dividing them into G groups. For the
166"
CONTEXT-GUIDED VISUAL FEATURE,0.20259128386336867,"text feature matrix T = [t1 t2 · · · tM] ∈RM×D, let us determine Mi for i = 1, . . . , G such that
167"
CONTEXT-GUIDED VISUAL FEATURE,0.2037691401648999,"M = ΠG
i=1Mi and reshape the text feature matrix to be T ∈RM1×···×MG×D. Then, propose to
168"
CONTEXT-GUIDED VISUAL FEATURE,0.2049469964664311,"sequentially apply the following attention process for G iterations for estimating both global and
169"
CONTEXT-GUIDED VISUAL FEATURE,0.2061248527679623,"local context-guided visual features v(k) and V (k), respectively, at the kth iteration:
170"
CONTEXT-GUIDED VISUAL FEATURE,0.20730270906949352,v(k) =
CONTEXT-GUIDED VISUAL FEATURE,0.20848056537102475,"(
T
if k = 0,"
CONTEXT-GUIDED VISUAL FEATURE,0.20965842167255594,"Softmaxdimk

f(v(k−1))t αf"
CONTEXT-GUIDED VISUAL FEATURE,0.21083627797408716,"
v(k−1)
if k > 0,
(2) 171"
CONTEXT-GUIDED VISUAL FEATURE,0.21201413427561838,V (k) =
CONTEXT-GUIDED VISUAL FEATURE,0.21319199057714958,"(
T
if k = 0,"
CONTEXT-GUIDED VISUAL FEATURE,0.2143698468786808,Softmaxdimk( F (V (k−1))t
CONTEXT-GUIDED VISUAL FEATURE,0.21554770318021202,"αF
)V (k−1)
if k > 0,
(3)"
CONTEXT-GUIDED VISUAL FEATURE,0.21672555948174324,"where αf and αF denote the modulation parameters, SoftmaxMk refers to the softmax operation
172"
CONTEXT-GUIDED VISUAL FEATURE,0.21790341578327443,"applied along the dimension corresponding to Mk. In this work, we utilize v(3) and V(3) to compute
173"
CONTEXT-GUIDED VISUAL FEATURE,0.21908127208480566,"classification score. The sequential attention process is illustrated in Figure 2 (b). Figure 3 further
174"
CONTEXT-GUIDED VISUAL FEATURE,0.22025912838633688,"demonstrates that our sequential attention is particularly effective in handling massive text descriptions.
175"
CONTEXT-GUIDED VISUAL FEATURE,0.22143698468786807,"Without sequential attention, weighted averaging essentially becomes equal averaging.
176"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.2226148409893993,"3.3
Multi-Label Recognition with Class Concepts
177"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.22379269729093051,"Architecture of model. Two encoders of CLIP are denoted as Eimg and Etxt for the visual encoder
178"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.2249705535924617,"and text encoder, respectively. Following TaI-DPT [18], we adopt the structure of double-grained
179"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.22614840989399293,"prompts (DPT), which has been shown effective for enhancing zero-shot multi-label recognition
180"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.22732626619552415,"performance. To obtain visual representations at both coarse-grained and fine-grained levels, we
181"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.22850412249705537,"extract the local visual feature map F = Eimg(x) ∈RHW ×D is extracted before attention pooling
182"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.22968197879858657,"layer, where H and W are spatial dimension of visual feature. After attention pooling layer, we
183"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.2308598351001178,"obtain the global visual feature f ∈R1×D. Similarly, text features t = Etxt(z) ∈R1×Dare obtained
184"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.232037691401649,"by projecting the End-of-Sentence (EOS) token of the text prompt. Thus, we leverage both global
185"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.2332155477031802,"and local visual features for multi-label recognition.
186"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.23439340400471143,"Inference. Through our sequential attention, we obtain the context-guided visual features v(G) and
187"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.23557126030624265,"V (G) at both global and local levels, respectively. The similarity score Sglo and Sloc are calculated
188"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.23674911660777384,"between the transformed context-guided visual features v(G), V (G) and the class concepts tconcept
c
189"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.23792697290930506,"using the cosine similarity Ψ(·,·) as follows:
190"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.23910482921083628,"Stot
c
= Sglo
c
+ Sloc
c
= Ψ(v(G), tconcept
c
) + PHW
j=1 Softmax(sloc
c,j) · sloc
c,j
(4)"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.24028268551236748,"where Stot
c
is the classification score for the class c and sloc
c,j = Ψ([V (G)]j, tconcept
c
) for the class c.
191"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.2414605418138987,"For obtaining Sloc
c , we employ the spatial aggregation over HW [37].
192"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.24263839811542992,"Finally, we combined ZSCLIP[32] and other prompt tuning methods with our training-free approach
193"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.24381625441696114,"through simple logit ensemble. In our experiments, we demonstrate the effectiveness of integrating of
194"
MULTI-LABEL RECOGNITION WITH CLASS CONCEPTS,0.24499411071849234,"our method with existing methods, thereby boosting the performance of multi-label recognition.
195"
EXPERIMENTS,0.24617196702002356,"4
Experiments
196"
IMPLEMENTATION DETAILS,0.24734982332155478,"4.1
Implementation Details
197"
IMPLEMENTATION DETAILS,0.24852767962308597,"Architecture. We empoly CLIP ResNet-50 in the Table. 2 and Table. 3 and ResNet-101 in other
198"
IMPLEMENTATION DETAILS,0.2497055359246172,"experiments as the visual encoders and the CLIP transformer as the text encoder for ZSCLIP[32],
199"
IMPLEMENTATION DETAILS,0.2508833922261484,"TaI-DPT [18], DualCoOP [37] and our method in the paper. In addition, ZSCLIP[32], TaI-DPT [18]
200"
IMPLEMENTATION DETAILS,0.25206124852767964,"and our method are based on the double-grained prompt [18] for both global and local features1.
201"
IMPLEMENTATION DETAILS,0.25323910482921086,"Datasets. For evaluation, we performed multi-label recognition experiments on 3 benchmark datasets.
202"
IMPLEMENTATION DETAILS,0.254416961130742,"MS-COCO [26] consists of 80 classes with 82,081 images for training and 40,504 images for test.
203"
IMPLEMENTATION DETAILS,0.25559481743227325,"VOC2007[16] consists of 20 object classes with 5,011 image for training and 4,952 images for test.
204"
IMPLEMENTATION DETAILS,0.25677267373380447,"NUS-WIDE[12] consists of 81 concepts with 161,789 image for training and 107,859 image for
205"
IMPLEMENTATION DETAILS,0.2579505300353357,"test. For MS-COCO [26] and VOC2007 [26], text description source is from MS-COCO [26]. For
206"
IMPLEMENTATION DETAILS,0.2591283863368669,"NUS-WIDE[12], we gathered the text descriptions from GPT-3.5. Note that there is example of text
207"
IMPLEMENTATION DETAILS,0.26030624263839813,"template for extracting sentence from GPT-3.5 in supplementary.
208"
IMPLEMENTATION DETAILS,0.26148409893992935,"Inference Details. In the paper, we set the total number of text descriptions, denoted as M, for
209"
IMPLEMENTATION DETAILS,0.2626619552414605,"the MSCOCO[26], VOC2007[16], and NUS-WIDE[12] at 40,000, 64,000, and 57,600, respectively.
210"
IMPLEMENTATION DETAILS,0.26383981154299174,"Note that we prepared the text embeddings of every text descriptions from CLIP text encoder in
211"
IMPLEMENTATION DETAILS,0.26501766784452296,"advance. We set values of modulation parameter α via validation.
212"
EVALUATION ON LIMITED DATA SETTING,0.2661955241460542,"4.2
Evaluation on Limited Data Setting
213"
EVALUATION ON LIMITED DATA SETTING,0.2673733804475854,"To evaluate our method, we conducted the experiments in limited data scenarios, including zero-shot
214"
EVALUATION ON LIMITED DATA SETTING,0.26855123674911663,"and few-shot settings for data-limited cases and partially labeled setting for label-limited cases. Note
215"
EVALUATION ON LIMITED DATA SETTING,0.2697290930506478,"that only our method provides training-free enhancement of CLIP without tuining cost for multi-label
216"
EVALUATION ON LIMITED DATA SETTING,0.270906949352179,"recognition. Therefore, our method can be easily combined with existing methods to improve their
217"
EVALUATION ON LIMITED DATA SETTING,0.27208480565371024,"performance.
218"
EVALUATION ON LIMITED DATA SETTING,0.27326266195524146,"Evaluation on Zero-Shot Setting. We performed comparison studies for different zero-shot and fully
219"
EVALUATION ON LIMITED DATA SETTING,0.2744405182567727,"supervised methods in multi-label image recognition. To evaluate the effectiveness of our method
220"
EVALUATION ON LIMITED DATA SETTING,0.2756183745583039,"which, we combined our method with existing zero-shot methods, ZSCLIP[32] and TaI-DPT [18],
221"
EVALUATION ON LIMITED DATA SETTING,0.2767962308598351,"for zero-shot setting, as shown in Table 1. Additionally, we utilized the fully supervised method,
222"
EVALUATION ON LIMITED DATA SETTING,0.2779740871613663,"DualCoOp[37] with our method, for zero-shot learning setting (ZSL) as presented in Table 2.
223"
EVALUATION ON LIMITED DATA SETTING,0.2791519434628975,"Table 1 summarizes the results of the zero-shot experiment on benchmark datasets. In MS-COCO [26]
224"
EVALUATION ON LIMITED DATA SETTING,0.28032979976442873,"and VOC2007 [16], TaI-DPT [18] and our method utilized the public language data from MS-
225"
EVALUATION ON LIMITED DATA SETTING,0.28150765606595995,"COCO [26]. By applying our method to ZSCLIP[32] and TaI-DPT [18] during inference, we yield
226"
EVALUATION ON LIMITED DATA SETTING,0.2826855123674912,"performance improvements without tuning costs. Especially, the performance of ZSCLIP[32] with
227"
EVALUATION ON LIMITED DATA SETTING,0.2838633686690224,1https://github.com/guozix/TaI-DPT
EVALUATION ON LIMITED DATA SETTING,0.2850412249705536,"Table 1: Multi-label recognition with zero-shot methods on MS-COCO [26], VOC2007 [16] and
NUS-WIDE [12]. Without training, our method significantly enhances the performance of existing
zero-shot methods. The evaluation is based on mAP."
EVALUATION ON LIMITED DATA SETTING,0.2862190812720848,"Training-free
Methods
MS-COCO[26]
VOC2007[16]
NUS-WIDE[12]
✓
ZSCLIP[32]
57.4
82.8
37.3
✓
+Ours
70.0 (+12.6)
89.2 (+6.4)
46.6 (+9.3)
✗
TaI-DPT[18]
68.0
88.9
46.5
✓
+Ours
70.9 (+2.9)
90.1 (+1.2)
49.1 (+2.6)"
EVALUATION ON LIMITED DATA SETTING,0.287396937573616,"Table 2: Multi-label recognition with 17 unseen classes on MS-COCO [26]. In zero-shot learning
(ZSL , recognizing only unseen classes) and generalized ZSL (GZSL, recognizing both seen and
unseen classes), our method effectively supplements the complementary information of unseen classes
to the supervised DualCoOp[37] on 48 seen classes. The evaluation is based on mAP."
EVALUATION ON LIMITED DATA SETTING,0.28857479387514723,"Methods
ResNet-50
ResNet-101
ZSL
GZSL
ZSL
GZSL
DualCoOp[37]
78.2
70.2
82.9
74.9
+Ours
82.9 (+4.7)
73.2 (+3.0)
87.6 (+4.7)
78.0 (+3.1)"
EVALUATION ON LIMITED DATA SETTING,0.28975265017667845,"our method is notably enhanced, achieving better and comparable performance to TaI-DPT [18],
228"
EVALUATION ON LIMITED DATA SETTING,0.29093050647820967,"which requires mild tuning. In NUSWIDE [12], we incorporate contextual text descriptions from
229"
EVALUATION ON LIMITED DATA SETTING,0.2921083627797409,"a large language model (GPT-3.5) to validate the potential of utilizing generated texts instead of
230"
EVALUATION ON LIMITED DATA SETTING,0.29328621908127206,"well-curated caption data. With provided class name of NUSWIDE [12], we readily gathered the
231"
EVALUATION ON LIMITED DATA SETTING,0.2944640753828033,"massive set of text descriptions within a short amount of time. TaI-DPT [18] is trained with the
232"
EVALUATION ON LIMITED DATA SETTING,0.2956419316843345,"public caption data from OpenImages[23]. Our method exceeds the performance of ZSCLIP[32] and
233"
EVALUATION ON LIMITED DATA SETTING,0.2968197879858657,"TaI-DPT [18] by a large margin, with improvements of 9.3 mAP and 2.6 mAP, respectively.
234"
EVALUATION ON LIMITED DATA SETTING,0.29799764428739695,"Table 2 shows the results of the zero-shot learning setting for unseen classes. In MS-COCO [26],
235"
EVALUATION ON LIMITED DATA SETTING,0.29917550058892817,"we follow the DualCoOp[37] and split the dataset into 48 seen classes and 17 unseen classes.
236"
EVALUATION ON LIMITED DATA SETTING,0.3003533568904594,"The evaluation is conducted in both zero-shot setting (ZSL, recognizing only unseen classes) and
237"
EVALUATION ON LIMITED DATA SETTING,0.30153121319199055,"generalized zero-shot setting (GZSL, recognizing both seen and unseen classes). Based on prompt
238"
EVALUATION ON LIMITED DATA SETTING,0.3027090694935218,"tuning, DualCoOp[37] trains learnable context tokens on 48 seen classes and achieves the state-of-the-
239"
EVALUATION ON LIMITED DATA SETTING,0.303886925795053,"art performance on both ZSL and GZSL. Our method was originally designed to handle novel classes
240"
EVALUATION ON LIMITED DATA SETTING,0.3050647820965842,"(unseen classes) by leveraging text descriptions. As a result, our method significantly improved
241"
EVALUATION ON LIMITED DATA SETTING,0.30624263839811544,"the ZSL and GZSL performance of the supervised DualCoOp[37] by providing complementary
242"
EVALUATION ON LIMITED DATA SETTING,0.30742049469964666,"information. Table 1 and Table 2 demonstrate the effectiveness of our method performing training-
243"
EVALUATION ON LIMITED DATA SETTING,0.30859835100117783,"free enhancement of CLIP with only text descriptions that are easily obtained.
244"
EVALUATION ON LIMITED DATA SETTING,0.30977620730270905,"Evaluation on Few-Shot Setting. We performed comparison study with few-shot methods in multi-
245"
EVALUATION ON LIMITED DATA SETTING,0.31095406360424027,"label recognition. In TaI-DPT [18], they have investigate to confirm the effectiveness of their zero-shot
246"
EVALUATION ON LIMITED DATA SETTING,0.3121319199057715,"method. Here, we further validate our method, which is zero-shot test-time task adaption without
247"
EVALUATION ON LIMITED DATA SETTING,0.3133097762073027,"tuning costs.
248"
EVALUATION ON LIMITED DATA SETTING,0.31448763250883394,"Table 3 summarizes the results of the few-shot methods on MS-COCO dataset [26], especially using
249"
EVALUATION ON LIMITED DATA SETTING,0.31566548881036516,"1 and 5 shot samples for all classes. While existing few-shot methods [3, 35, 54, 51] demonstrated
250"
EVALUATION ON LIMITED DATA SETTING,0.3168433451118963,"the performance enhancements with an increase of labeled samples, TaI-DPT [18] and our method
251"
EVALUATION ON LIMITED DATA SETTING,0.31802120141342755,"are performed within the zero-shot setting. By applying our method with existing zero-shot methods
252"
EVALUATION ON LIMITED DATA SETTING,0.31919905771495877,"(ZSCLIP[32] and TaI-DPT [18]), we consistently enhance performance, as already demonstrated in a
253"
EVALUATION ON LIMITED DATA SETTING,0.32037691401649,"zero-shot setting. In the absence of labeled samples and tuning, we achieved comparable performance
254"
EVALUATION ON LIMITED DATA SETTING,0.3215547703180212,"with ML-FSL[35] and better results than other few-shot methods utilizing 5-shot samples.
255"
EVALUATION ON LIMITED DATA SETTING,0.32273262661955243,"Evaluation on Partially Labeled Setting. Due to high costs of annotation in multi-label image
256"
EVALUATION ON LIMITED DATA SETTING,0.32391048292108365,"recognition, training with partially labeled samples [37, 21, 31, 6] has been studied. Following
257"
EVALUATION ON LIMITED DATA SETTING,0.3250883392226148,"DualCoOp [37], we performed the evaluation of partially labeled setting. As shown in Table 4,
258"
EVALUATION ON LIMITED DATA SETTING,0.32626619552414604,"our method supplements the decreased performance of DualCoOp [37] caused by partially labeled
259"
EVALUATION ON LIMITED DATA SETTING,0.32744405182567726,"samples by providing complementary information during inference. Through zero-shot test time task
260"
EVALUATION ON LIMITED DATA SETTING,0.3286219081272085,"adaptation without tuning costs, we consistently enhance the the performance of DualCoOp [37] on
261"
EVALUATION ON LIMITED DATA SETTING,0.3297997644287397,"Table 3: Comparison with few-shot methods on MS-COCO [26]. The evaluation is based on mAP
with 16 novel classes. For each shot, we highlighted the best performance in bold."
EVALUATION ON LIMITED DATA SETTING,0.3309776207302709,"Training-free
Methods
0-shot
1-shot
5-shot"
EVALUATION ON LIMITED DATA SETTING,0.3321554770318021,"✗
LaSO[3]
-
45.3
58.1
✗
ML-FSL[35]
-
54.4
63.6
✗
CoOp[54]
-
46.9
55.6
✓
Tip-Adapter[51]
-
53.8
59.7"
EVALUATION ON LIMITED DATA SETTING,0.3333333333333333,"✓
ZSCLIP[32]
49.7
-
-
✓
+Ours
58.5 (+8.8)
-
-
✗
TaI-DPT[18]
59.2
-
-
✓
+Ours
61.4 (+2.2)
-
-"
EVALUATION ON LIMITED DATA SETTING,0.33451118963486454,"Table 4: Performance of multi-label recognition based on the partially labeled dataset [26, 16,
12]. Without training and labeled samples, our method consistently enhanced the performance of
supervised DualCoOp [37] over all partial label ratio. DualCoOp [37] is reproduced and the evaluation
is based on mAP."
EVALUATION ON LIMITED DATA SETTING,0.33568904593639576,"Datasets
Method
Partial label"
EVALUATION ON LIMITED DATA SETTING,0.336866902237927,"10%
20%
30%
40%
50%
60%
70%
80%
90%
Avg."
EVALUATION ON LIMITED DATA SETTING,0.3380447585394582,"MS-COCO
SARB[31]
71.2
75.0
77.1
78.3
79.6
79.6
80.5
80.5
80.5
77.9
DualCoOp[37]
80.8
82.2
82.8
83.0
83.5
83.8
83.9
84.1
84.2
82.7
DualCoOp[37]+Ours
81.5
82.8
83.3
83.5
84.0
84.2
84.4
84.5
84.6
83.6"
EVALUATION ON LIMITED DATA SETTING,0.3392226148409894,"VOC2007
SARB[31]
83.5
88.6
90.7
91.4
91.9
92.2
92.6
92.8
92.9
90.7
DualCoOp[37]
91.6
93.3
93.7
94.3
94.5
94.7
94.8
94.9
94.8
94.0
DualCoOp[37]+Ours
92.5
93.9
94.3
94.7
94.9
95.0
95.1
95.2
95.1
94.5"
EVALUATION ON LIMITED DATA SETTING,0.3404004711425206,"NUS-WIDE
DualCoOp[37]
54.0
56.1
56.9
57.4
57.9
57.8
58.0
58.4
58.8
57.3
DualCoOp[37]+Ours
55.0
56.9
57.7
58.2
58.6
58.6
58.8
59.2
59.5
58.1"
EVALUATION ON LIMITED DATA SETTING,0.3415783274440518,"all benchmark dataset. Furthermore, we achieved the performance of DualCoOp [37] trained with
262"
EVALUATION ON LIMITED DATA SETTING,0.34275618374558303,"90% labels by applying our method with DualCoOp trained with 60% labels from MS-COCO [26],
263"
EVALUATION ON LIMITED DATA SETTING,0.34393404004711425,"50% labels from VOC2007 [16], and 70% labels from NUSWIDE [12].
264"
ABLATION STUDY AND ANALYSIS,0.3451118963486455,"4.3
Ablation Study and Analysis
265"
EFFECTIVENESS OF OUR METHOD,0.3462897526501767,"4.3.1
Effectiveness of our method
266"
EFFECTIVENESS OF OUR METHOD,0.3474676089517079,"To verify the effectiveness of components of our method, we conducted an ablation study for analyzing
267"
EFFECTIVENESS OF OUR METHOD,0.3486454652532391,"our method. As shown in Table 5, we first proposed a novel class concept representation with text
268"
EFFECTIVENESS OF OUR METHOD,0.3498233215547703,"descriptions by class to ZSCLIP[32]. Since the text descriptions contain the semantic meaning among
269"
EFFECTIVENESS OF OUR METHOD,0.3510011778563015,"multiple class names and contextual information for multi-label recognition, the alignment between
270"
EFFECTIVENESS OF OUR METHOD,0.35217903415783275,"visual features of test image and text features are improved compared to the hand-crafted prompts as
271"
EFFECTIVENESS OF OUR METHOD,0.35335689045936397,"shown in the Fig.1. Thus, the performance is increased by 4.1 mAP and 1.1 mAP on MS-COCO [26]
272"
EFFECTIVENESS OF OUR METHOD,0.3545347467608952,"and VOC2007 [16], respectively. Then, we performed the context-guided visual feature using a large
273"
EFFECTIVENESS OF OUR METHOD,0.35571260306242636,"set of text descriptions, Zall. Transforming the visual features into same text feature space as our class
274"
EFFECTIVENESS OF OUR METHOD,0.3568904593639576,"concept representation is essential to minimize the gap between visual feature from task-agnostic
275"
EFFECTIVENESS OF OUR METHOD,0.3580683156654888,"visual encoder and text features for each class. Constructing context-guided visual feature, our method
276"
EFFECTIVENESS OF OUR METHOD,0.35924617196702,"yield remarkable performance gain by 8.5 mAP and 5.3 mAP on MS-COCO [26] and VOC2007 [16],
277"
EFFECTIVENESS OF OUR METHOD,0.36042402826855124,"respectively. Thus, we effectively designed our method that improves the alignment between visual
278"
EFFECTIVENESS OF OUR METHOD,0.36160188457008247,"and text features.
279"
THE NUMBER OF TEXT DESCRIPTIONS,0.3627797408716137,"4.3.2
The Number of Text Descriptions
280"
THE NUMBER OF TEXT DESCRIPTIONS,0.36395759717314485,"We investigate the effect of the number of text descriptions for our method. As shown in Table 6,
281"
THE NUMBER OF TEXT DESCRIPTIONS,0.3651354534746761,"we evaluated performance by increasing the number of randomly selected text descriptions from 1K
282"
THE NUMBER OF TEXT DESCRIPTIONS,0.3663133097762073,"to 32K texts. With only 1K text descriptions, our method enhances performance by approximately
283"
THE NUMBER OF TEXT DESCRIPTIONS,0.3674911660777385,"Table 5: Effectiveness of our method on MS-COCO [26] and VOC2007 [16]. Each component of
our method consistently improves performance, with significant enhancements achieved particularly
in context-guided visual feature through narrowing the gap between visual and text features. The
evaluation is based on mAP."
THE NUMBER OF TEXT DESCRIPTIONS,0.36866902237926974,"Method
MS-COCO [26]
VOC2007 [16]
Baseline (ZSCLIP[32])
57.4
82.8
+Class concept representation
61.5(+4.1)
83.9(+1.1)
+Context-guided visual feature
70.0(+8.5)
89.2(+5.3)"
THE NUMBER OF TEXT DESCRIPTIONS,0.36984687868080096,"Table 6: Ablation studies in terms of the number of the text descriptions. As increasing the number of
texts, we measured the performance of ZSCLIP[32] with our method in mAP on MS-COCO [26] and
VOC2007 [16]. Note that ZSCLIP[32] achieves 57.4 mAP and 82.8 mAP for MS-COCO [26] and
VOC2007 [16], respectively."
THE NUMBER OF TEXT DESCRIPTIONS,0.3710247349823322,"Dataset
Number of text descriptions
1K
2K
4K
8K
16K
32K"
THE NUMBER OF TEXT DESCRIPTIONS,0.37220259128386335,"MS-COCO [26]
65.8
68.4
68.5
69.1
69.6
69.9
VOC2007 [16]
88.1
88.5
88.8
88.9
89.0
89.1"
THE NUMBER OF TEXT DESCRIPTIONS,0.37338044758539457,"8 mAP on MS-COCO [26] and 5 mAP on VOC2007 [16], respectively. As the number of text
284"
THE NUMBER OF TEXT DESCRIPTIONS,0.3745583038869258,"descriptions ranges from 1K to 32K, the text embeddings of Zall can cover the wider range of test
285"
THE NUMBER OF TEXT DESCRIPTIONS,0.375736160188457,"dataset, resulting in increased performance gains. For adapting to novel classes during inference, our
286"
THE NUMBER OF TEXT DESCRIPTIONS,0.37691401648998824,"method not only achieves a significant performance improvement with only 1K texts but also further
287"
THE NUMBER OF TEXT DESCRIPTIONS,0.37809187279151946,"enhances performance as the quantity of texts increases.
288"
ANALYSIS OF INFERENCE TIME,0.3792697290930506,"4.3.3
Analysis of Inference Time
289"
ANALYSIS OF INFERENCE TIME,0.38044758539458184,"We analyzed the inference time of our method depending on the number of text descriptions. When
290"
ANALYSIS OF INFERENCE TIME,0.38162544169611307,"extracting text embeddings from the text descriptions in advance, we measure the inference time
291"
ANALYSIS OF INFERENCE TIME,0.3828032979976443,"as the number of text descriptions increases. ZSCLIP[32], as the baseline model, processes each
292"
ANALYSIS OF INFERENCE TIME,0.3839811542991755,"sample for classification in 7.2ms. When the number of texts increases from 1K to 32K, integrating
293"
ANALYSIS OF INFERENCE TIME,0.38515901060070673,"ZSCLIP[32] with our method only increases the inference time by 0.4-0.5ms, with tests conducted on
294"
ANALYSIS OF INFERENCE TIME,0.38633686690223795,"the RTX3090. In addition, Our method (6.8GB) requires slightly more memory than ZSCLIP (6.5GB)
295"
ANALYSIS OF INFERENCE TIME,0.3875147232037691,"on VOC2007 [16]. Therefore, our method presents a simple and efficient approach for training-free
296"
ANALYSIS OF INFERENCE TIME,0.38869257950530034,"enhancement approach at inference.
297"
CONCLUSION,0.38987043580683156,"5
Conclusion
298"
CONCLUSION,0.3910482921083628,"In this paper, we propose a novel class concept representation from massive text descriptions for
299"
CONCLUSION,0.392226148409894,"training-free multi-label recognition tasks. Inspired by how humans form concepts based on words,
300"
CONCLUSION,0.3934040047114252,"as studied in cognitive neuroscience, we replace single class name prompts with the class concept
301"
CONCLUSION,0.3945818610129564,"representation that capture various patterns of co-occurrence with other words. To further enhance
302"
CONCLUSION,0.3957597173144876,"alignment between multi-modal features of VLMs, we propose a context-guided visual representation
303"
CONCLUSION,0.39693757361601884,"that is transformed onto the same linear space as the class concept representation. Remarkably,
304"
CONCLUSION,0.39811542991755006,"our proposed method outperforms zero-shot prompt tuning methods such as TaI-DPT and achieves
305"
CONCLUSION,0.3992932862190813,"significant enhancements over ZSCLIP and other state-of-the-art prompt tuning methods without
306"
CONCLUSION,0.4004711425206125,"requiring parameter tuning or labeled samples, and with minimal inference time overhead.
307"
CONCLUSION,0.4016489988221437,"Limitations. While our method achieved impressive results with training-free enhancement of CLIP,
308"
CONCLUSION,0.4028268551236749,"it exhibits limitations. First, a significant performance gap exists compared to prompt tuning methods
309"
CONCLUSION,0.4040047114252061,"with full samples, like DualCoOp [37]. Second, the computational memory demands of our method
310"
CONCLUSION,0.40518256772673733,"grow at a faster rate than ZSCLIP[32] as the batch size increases.
311"
REFERENCES,0.40636042402826855,"References
312"
REFERENCES,0.4075382803297998,"[1] Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng Wang, and Song Wang. Cdul: Clip-driven
313"
REFERENCES,0.408716136631331,"unsupervised learning for multi-label image classification. In ICCV, 2023.
314"
REFERENCES,0.4098939929328622,"[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
315"
REFERENCES,0.4110718492343934,"Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for
316"
REFERENCES,0.4122497055359246,"few-shot learning. NeurIPS, 2022.
317"
REFERENCES,0.4134275618374558,"[3] Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok, Sivan Harary, Rogerio Feris, Raja Giryes, and
318"
REFERENCES,0.41460541813898705,"Alex M Bronstein. Laso: Label-set operations networks for multi-label few-shot learning. In CVPR, 2019.
319"
REFERENCES,0.41578327444051827,"[4] James Urquhart Allingham, Jie Ren, Michael W Dusenberry, Xiuye Gu, Yin Cui, Dustin Tran, Jeremiah Zhe
320"
REFERENCES,0.4169611307420495,"Liu, and Balaji Lakshminarayanan. A simple zero-shot prompt weighting technique to improve prompt
321"
REFERENCES,0.41813898704358066,"ensembling in text-image models. In ICML, 2023.
322"
REFERENCES,0.4193168433451119,"[5] Lawrence W Barsalou. Perceptual symbol systems. Behavioral and brain sciences, 22(4):577–660, 1999.
323"
REFERENCES,0.4204946996466431,"[6] Emanuel Ben-Baruch, Tal Ridnik, Itamar Friedman, Avi Ben-Cohen, Nadav Zamir, Asaf Noy, and Lihi
324"
REFERENCES,0.4216725559481743,"Zelnik-Manor. Multi-label classification with partial annotations using class-aware selective loss. In CVPR,
325"
REFERENCES,0.42285041224970554,"2022.
326"
REFERENCES,0.42402826855123676,"[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
327"
REFERENCES,0.425206124852768,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
328"
REFERENCES,0.42638398115429915,"In NeurIPS, 2020.
329"
REFERENCES,0.4275618374558304,"[8] Dolly Carrillo, Vivian F López, and María N Moreno. Multi-label classification for recommender systems.
330"
REFERENCES,0.4287396937573616,"In Trends in Practical Applications of Agents and Multiagent Systems: 11th International Conference on
331"
REFERENCES,0.4299175500588928,"Practical Applications of Agents and Multi-Agent Systems, pages 181–188. Springer, 2013.
332"
REFERENCES,0.43109540636042404,"[9] Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, and Liang Lin. Structured semantic transfer for multi-label
333"
REFERENCES,0.43227326266195526,"recognition with partial labels. In AAAI, 2022.
334"
REFERENCES,0.4334511189634865,"[10] Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and Liang Lin. Learning semantic-specific graph
335"
REFERENCES,0.43462897526501765,"representation for multi-label image recognition. In CVPR, pages 522–531, 2019.
336"
REFERENCES,0.43580683156654887,"[11] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo. Multi-label image recognition with graph
337"
REFERENCES,0.4369846878680801,"convolutional networks. In CVPR, 2019.
338"
REFERENCES,0.4381625441696113,"[12] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. Nus-wide: a
339"
REFERENCES,0.43934040047114253,"real-world web image database from national university of singapore. In CIVR, pages 1–9, 2009.
340"
REFERENCES,0.44051825677267376,"[13] Elijah Cole, Oisin Mac Aodha, Titouan Lorieul, Pietro Perona, Dan Morris, and Nebojsa Jojic. Multi-label
341"
REFERENCES,0.4416961130742049,"learning from single positive labels. In CVPR, 2021.
342"
REFERENCES,0.44287396937573614,"[14] Zixuan Ding, Ao Wang, Hui Chen, Qiang Zhang, Pengzhang Liu, Yongjun Bao, Weipeng Yan, and Jungong
343"
REFERENCES,0.44405182567726736,"Han. Exploring structured semantic prior for multi label recognition with incomplete labels. In CVPR,
344"
REFERENCES,0.4452296819787986,"2023.
345"
REFERENCES,0.4464075382803298,"[15] Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning a deep convnet for multi-label classification
346"
REFERENCES,0.44758539458186103,"with partial labels. In CVPR, 2019.
347"
REFERENCES,0.44876325088339225,"[16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The
348"
REFERENCES,0.4499411071849234,"pascal visual object classes (voc) challenge. IJCV, 2010.
349"
REFERENCES,0.45111896348645464,"[17] Marian George and Christian Floerkemeier. Recognizing products: A per-exemplar multi-label image
350"
REFERENCES,0.45229681978798586,"classification approach. In ECCV, 2014.
351"
REFERENCES,0.4534746760895171,"[18] Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, and Wangmeng Zuo. Texts as images in
352"
REFERENCES,0.4546525323910483,"prompt tuning for multi-label image recognition. In CVPR, 2023.
353"
REFERENCES,0.4558303886925795,"[19] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and Bin Cui. Calip:
354"
REFERENCES,0.45700824499411075,"Zero-shot enhancement of clip with parameter-free attention. In AAAI, 2023.
355"
REFERENCES,0.4581861012956419,"[20] Paul Hoffman, James L. McClelland, and Matthew A. Lambon Ralph. Concepts, control, and context: A
356"
REFERENCES,0.45936395759717313,"connectionist account of normal and disordered semantic cognition. Psychological Review, 125(3):293–328,
357"
REFERENCES,0.46054181389870436,"Apr.
358"
REFERENCES,0.4617196702002356,"[21] Ping Hu, Ximeng Sun, Stan Sclaroff, and Kate Saenko. Dualcoop++: Fast and effective adaptation to
359"
REFERENCES,0.4628975265017668,"multi-label recognition with limited annotations. arXiv preprint arXiv:2308.01890, 2023.
360"
REFERENCES,0.464075382803298,"[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,
361"
REFERENCES,0.4652532391048292,"Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text
362"
REFERENCES,0.4664310954063604,"supervision. In ICML. PMLR, 2021.
363"
REFERENCES,0.46760895170789163,"[23] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan
364"
REFERENCES,0.46878680800942285,"Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: A public dataset for large-scale
365"
REFERENCES,0.46996466431095407,"multi-label and multi-class image classification. Dataset available from https://github. com/openimages,
366"
REFERENCES,0.4711425206124853,"2017.
367"
REFERENCES,0.4723203769140165,"[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
368"
REFERENCES,0.4734982332155477,"Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
369"
REFERENCES,0.4746760895170789,"using crowdsourced dense image annotations. IJCV, 2017.
370"
REFERENCES,0.4758539458186101,"[25] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie
371"
REFERENCES,0.47703180212014135,"Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.
372"
REFERENCES,0.47820965842167257,"arXiv preprint arXiv:2110.05208, 2021.
373"
REFERENCES,0.4793875147232038,"[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
374"
REFERENCES,0.48056537102473496,"and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.
375"
REFERENCES,0.4817432273262662,"[27] Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang, and Chunhong Pan. Multi-label image
376"
REFERENCES,0.4829210836277974,"classification via knowledge distillation from weakly-supervised detection. In ACMMM, 2018.
377"
REFERENCES,0.4840989399293286,"[28] OpenAI. Gpt-4 technical report, 2023.
378"
REFERENCES,0.48527679623085984,"[29] Karalyn Patterson, Peter J Nestor, and Timothy T Rogers. Where do you know what you know? the
379"
REFERENCES,0.48645465253239106,"representation of semantic knowledge in the human brain. Nature reviews neuroscience, 8(12):976–987,
380"
REFERENCES,0.4876325088339223,"2007.
381"
REFERENCES,0.48881036513545345,"[30] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana
382"
REFERENCES,0.48998822143698467,"Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence
383"
REFERENCES,0.4911660777385159,"models. In ICCV, 2015.
384"
REFERENCES,0.4923439340400471,"[31] Tao Pu, Tianshui Chen, Hefeng Wu, and Liang Lin. Semantic-aware representation blending for multi-label
385"
REFERENCES,0.49352179034157834,"image recognition with partial labels. In AAAI, 2022.
386"
REFERENCES,0.49469964664310956,"[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
387"
REFERENCES,0.4958775029446408,"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
388"
REFERENCES,0.49705535924617195,"natural language supervision. In ICML, 2021.
389"
REFERENCES,0.49823321554770317,"[33] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris. Deep imbalanced attribute classification using
390"
REFERENCES,0.4994110718492344,"visual attention aggregation. In ECCV, 2018.
391"
REFERENCES,0.5005889281507656,"[34] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei
392"
REFERENCES,0.5017667844522968,"Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural
393"
REFERENCES,0.502944640753828,"Information Processing Systems, 35:14274–14289, 2022.
394"
REFERENCES,0.5041224970553593,"[35] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. Meta-learning for multi-label few-shot classifica-
395"
REFERENCES,0.5053003533568905,"tion. In WACV, 2022.
396"
REFERENCES,0.5064782096584217,"[36] Josef Sivic and Andrew Zisserman. Video google: Efficient visual search of videos. Toward category-level
397"
REFERENCES,0.5076560659599529,"object recognition, pages 127–144, 2006.
398"
REFERENCES,0.508833922261484,"[37] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation to multi-label recognition with limited
399"
REFERENCES,0.5100117785630153,"annotations. NeurIPS, 2022.
400"
REFERENCES,0.5111896348645465,"[38] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
401"
REFERENCES,0.5123674911660777,"and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for
402"
REFERENCES,0.5135453474676089,"Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html, 2023.
403"
REFERENCES,0.5147232037691402,"[39] Ivona Tautkute, Tomasz Trzci´nski, Aleksander P Skorupa, Łukasz Brocki, and Krzysztof Marasek. Deep-
404"
REFERENCES,0.5159010600706714,"style: Multimodal search engine for fashion and interior design. IEEE Access, 2019.
405"
REFERENCES,0.5170789163722026,"[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
406"
REFERENCES,0.5182567726737338,"Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
407"
REFERENCES,0.519434628975265,"fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
408"
REFERENCES,0.5206124852767963,"[41] Ao Wang, Hui Chen, Zijia Lin, Zixuan Ding, Pengzhang Liu, Yongjun Bao, Weipeng Yan, and Guiguang
409"
REFERENCES,0.5217903415783275,"Ding. Hierarchical prompt learning using clip for multi-label classification with single positive labels. In
410"
REFERENCES,0.5229681978798587,"Proceedings of the 31st ACM International Conference on Multimedia, pages 5594–5604, 2023.
411"
REFERENCES,0.5241460541813898,"[42] Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, and Rui Li. Feature alignment and uniformity for
412"
REFERENCES,0.525323910482921,"test time adaptation. In CVPR, 2023.
413"
REFERENCES,0.5265017667844523,"[43] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M Summers. Tienet: Text-image embedding
414"
REFERENCES,0.5276796230859835,"network for common thorax disease classification and reporting in chest x-rays. In CVPR, 2018.
415"
REFERENCES,0.5288574793875147,"[44] Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou, Jinwen Ma, and Shilei Wen. Multi-label
416"
REFERENCES,0.5300353356890459,"classification with label graph superimposing. In AAAI, 2020.
417"
REFERENCES,0.5312131919905771,"[45] Xiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, and Lingqiao Liu. Rpc: A large-scale retail product
418"
REFERENCES,0.5323910482921084,"checkout dataset. arXiv preprint arXiv:1901.07249, 2019.
419"
REFERENCES,0.5335689045936396,"[46] Ning Xu, Congyu Qiao, Jiaqi Lv, Xin Geng, and Min-Ling Zhang. One positive label is sufficient:
420"
REFERENCES,0.5347467608951708,"Single-positive multi-label learning with label enhancement. NeurIPS, 2022.
421"
REFERENCES,0.535924617196702,"[47] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,
422"
REFERENCES,0.5371024734982333,"Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint
423"
REFERENCES,0.5382803297997645,"arXiv:2111.07783, 2021.
424"
REFERENCES,0.5394581861012956,"[48] Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa, Bartlomiej Twardowski, and Joost van de Weijer.
425"
REFERENCES,0.5406360424028268,"Orderless recurrent models for multi-label classification. In CVPR, 2020.
426"
REFERENCES,0.541813898704358,"[49] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong
427"
REFERENCES,0.5429917550058893,"Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv
428"
REFERENCES,0.5441696113074205,"preprint arXiv:2111.11432, 2021.
429"
REFERENCES,0.5453474676089517,"[50] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and
430"
REFERENCES,0.5465253239104829,"Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF
431"
REFERENCES,0.5477031802120141,"Conference on Computer Vision and Pattern Recognition, pages 18123–18133, 2022.
432"
REFERENCES,0.5488810365135454,"[51] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hong-
433"
REFERENCES,0.5500588928150766,"sheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint
434"
REFERENCES,0.5512367491166078,"arXiv:2111.03930, 2021.
435"
REFERENCES,0.552414605418139,"[52] Yong Zheng, Bamshad Mobasher, and Robin Burke. Context recommendation using multi-label classi-
436"
REFERENCES,0.5535924617196702,"fication. In IEEE/WIC/ACM International Joint Conferences on WI and IAT, volume 2, pages 288–295,
437"
REFERENCES,0.5547703180212014,"2014.
438"
REFERENCES,0.5559481743227326,"[53] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for
439"
REFERENCES,0.5571260306242638,"vision-language models. In CVPR, 2022.
440"
REFERENCES,0.558303886925795,"[54] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language
441"
REFERENCES,0.5594817432273262,"models. IJCV, 2022.
442"
REFERENCES,0.5606595995288575,"A
Generation of Text Descriptions using LLMs
443"
REFERENCES,0.5618374558303887,"Our proposed method leverages the text descriptions for enhancing the alignment between the
444"
REFERENCES,0.5630153121319199,"visual and text features. In practice, gathering the proper text descriptions is an essential process for
445"
REFERENCES,0.5641931684334511,"replacing the hand-crafted prompts. As mentioned in the main paper, the text descriptions can be
446"
REFERENCES,0.5653710247349824,"readily gathered from benchmark dataset, web crawling, or large language models. Recent advances
447"
REFERENCES,0.5665488810365136,"in large language models (LLMs) enable to rapidly generate text descriptions that are similar to
448"
REFERENCES,0.5677267373380448,"image captions in MS-COCO [26]. Therefore, we utilized the generated text descriptions from large
449"
REFERENCES,0.568904593639576,"language model. With provided class name of NUSWIDE [12], Fig. 6 illustrates the example of input
450"
REFERENCES,0.5700824499411072,"prompt template and corresponding generated text descriptions using GPT3.5. We carefully designed
451"
REFERENCES,0.5712603062426383,"the instruction of input prompt including main description, constraints, examples of bad and good
452"
REFERENCES,0.5724381625441696,"cases, class names of target task and output format.
453"
REFERENCES,0.5736160188457008,"B
Implementing Other Zero-Shot Training-Free Method
454"
REFERENCES,0.574793875147232,"In single-label recognition, CALIP [19] proposed zero-shot alignment enhancement of CLIP for adapt-
455"
REFERENCES,0.5759717314487632,"ing target task without few-shot samples or additional training. The parameter-free attention module of
456"
REFERENCES,0.5771495877502945,"cross-modal interaction effectively enhances the alignment of visual and text features. CALIP utilized
457"
REFERENCES,0.5783274440518257,"the visual feature F =Encv(xk)∈RHW ×D via reshaping and the text feature T =Enct(P h)∈RC×D
458"
REFERENCES,0.5795053003533569,"Table 7: Ablation study of hyperparameter searching on validation set. We varied the modulation
parameters αf,t and αF,t and searched the proper values for context-guided visual feature."
REFERENCES,0.5806831566548881,"1/αf,t,1/αF,t
MS-COCO
VOC2007
NUS-WIDE"
REFERENCES,0.5818610129564193,"100, 50
69.45
87.62
47.32
80, 40
69.51
87.94
48.31
60, 30
69.25
88.06
49.05
40, 20
67.47
87.37
49.82
20, 10
64.13
85.04
47.33"
REFERENCES,0.5830388692579506,"where P h is a hand-crafted description and C denotes the number of classes. The parameter-free
459"
REFERENCES,0.5842167255594818,"attention module is formulated as follows:
460"
REFERENCES,0.585394581861013,"F a
=
Softmax(A/αt)T,
(5)"
REFERENCES,0.5865724381625441,"T a
=
Softmax(AT /αv)F
(6)"
REFERENCES,0.5877502944640753,"where the attention matrix is A=FT T ∈RHW ×C, αt and αv are the modulation parameters of textual
461"
REFERENCES,0.5889281507656066,"and visual features, respectively, and T a and F a are bidirectionally updated textual and visual features.
462"
REFERENCES,0.5901060070671378,"After pooling the updated visual feature F a
v ∈R1×D and the global visual feature Fv∈R1×D, the
463"
REFERENCES,0.591283863368669,"classification logit S is obtained as below:
464"
REFERENCES,0.5924617196702002,"S = β1 · FvT T + β2 · FvT aT + β3 · F a
v T T ,
(7)"
REFERENCES,0.5936395759717314,"where β1, β2, β3 are the weights for the three logits.
465"
REFERENCES,0.5948174322732627,"CALIP [19] tuned the hyperparameters β2, β3 for each dataset while fixed β1 to be 1 for simplicity.
466"
REFERENCES,0.5959952885747939,"As shown in Fig. 4, we have explored the value of β2, β3 for multi-label recognition setting on
467"
REFERENCES,0.5971731448763251,"MS-COCO [26] and have observed that the parameter-free attention module consistently decreases
468"
REFERENCES,0.5983510011778563,"the mAP performance since multi-label recognition covers the identification of multiple objects
469"
REFERENCES,0.5995288574793876,"within an image, involving complex scene and diverse objects.
470"
REFERENCES,0.6007067137809188,"C
Exploring Modulation Parameters
471"
REFERENCES,0.6018845700824499,"For hyperparameter searching, following existing methods for classification tasks, such as zero-
472"
REFERENCES,0.6030624263839811,"shot [18, 19], training-free [51, 19], and test-time adaptation [42], we explore the modulation pa-
473"
REFERENCES,0.6042402826855123,"rameters αt by conducting ablation studies on validation set. For simplicity, we set the value of
474"
REFERENCES,0.6054181389870436,"αf,t to be half of αF,t. As shown in Table 7, the value of (1/αf,t, 1/αF,t) is suitable in the range
475"
REFERENCES,0.6065959952885748,"of (40∼80,20∼40). In the experiments of main paper, we set the (1/αf,t, 1/αF,t) as (80,40) for
476"
REFERENCES,0.607773851590106,"MS-COCO [26], (60,30) for VOC2007 [16] and (40,20) for NUSWIDE [12].
477"
REFERENCES,0.6089517078916372,"D
Examples of Local Alignment Enhancement
478"
REFERENCES,0.6101295641931684,"In Fig. 5, we visualized the examples of local alignment enhancement by applying our method.
479"
REFERENCES,0.6113074204946997,"Enhancing local alignment is important to recognize multiple objects in a test image [37]. Our
480"
REFERENCES,0.6124852767962309,"proposed method enhances the local alignment between the visual features of test image and the
481"
REFERENCES,0.6136631330977621,"text features of each class name, thereby suppressing the false-positive prediction. Therefore, Fig. 5
482"
REFERENCES,0.6148409893992933,"demonstrates the effectiveness of our method.
483"
REFERENCES,0.6160188457008245,"E
Positive and Negative Societal Impacts
484"
REFERENCES,0.6171967020023557,"As a positive societal impact, our method can allow people with limited computing resources to
485"
REFERENCES,0.6183745583038869,"achieve better performance in multi-label classification using existing vision-language models. This
486"
REFERENCES,0.6195524146054181,"is because it does not require extensive training or labeled data. However, as a negative societal
487"
REFERENCES,0.6207302709069493,"impact, the failure of classification could produce the negative side effects. For example, in security
488"
REFERENCES,0.6219081272084805,"applications, incorrect classification of objects could lead to false alarms or missed detections,
489"
REFERENCES,0.6230859835100118,"potentially compromising safety and security.
490"
REFERENCES,0.624263839811543,"Figure 4: Results of hyperparameter searching of CALIP [19] on MS-COCO [26] on β2 and β3.
Applying the parametric-free attention module of CALIP consistently decreases performance as
compared to the zero-shot CLIP (ZSCLIP) [32]."
REFERENCES,0.6254416961130742,"Figure 5: Additional examples of local alignment enhancement via our method. We visualized the test
image in the left column and its corresponding spatial similarity map of each class name in the right
column. The yellow and red boxes refer to the bounding boxes for different labels in a multi-label
setting. By applying our method, the local alignment is enhanced across multiple objects in a test
image, thereby suppressing false-positive predictions."
REFERENCES,0.6266195524146054,"Figure 6: Example of text description generation using GPT3.5 for contextual text descriptions of
NUSWIDE [12]. We carefully designed the input prompt to ensure that the generated sentences
include the class name of the target task. The elements considered in designing the input prompt
include the main description, constraints, examples, class names, and the desired output format."
REFERENCES,0.6277974087161367,"NeurIPS Paper Checklist
491"
CLAIMS,0.6289752650176679,"1. Claims
492"
CLAIMS,0.6301531213191991,"Question: Do the main claims made in the abstract and introduction accurately reflect the
493"
CLAIMS,0.6313309776207303,"paper’s contributions and scope?
494"
CLAIMS,0.6325088339222615,"Answer: [Yes]
495"
CLAIMS,0.6336866902237926,"Justification: We clearly state our contributions in both the abstract and introduction. Espe-
496"
CLAIMS,0.6348645465253239,"cially, we summarize our contributions in the last part of the introduction.
497"
CLAIMS,0.6360424028268551,"Guidelines:
498"
CLAIMS,0.6372202591283863,"• The answer NA means that the abstract and introduction do not include the claims
499"
CLAIMS,0.6383981154299175,"made in the paper.
500"
CLAIMS,0.6395759717314488,"• The abstract and/or introduction should clearly state the claims made, including the
501"
CLAIMS,0.64075382803298,"contributions made in the paper and important assumptions and limitations. A No or
502"
CLAIMS,0.6419316843345112,"NA answer to this question will not be perceived well by the reviewers.
503"
CLAIMS,0.6431095406360424,"• The claims made should match theoretical and experimental results, and reflect how
504"
CLAIMS,0.6442873969375736,"much the results can be expected to generalize to other settings.
505"
CLAIMS,0.6454652532391049,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
506"
CLAIMS,0.6466431095406361,"are not attained by the paper.
507"
LIMITATIONS,0.6478209658421673,"2. Limitations
508"
LIMITATIONS,0.6489988221436984,"Question: Does the paper discuss the limitations of the work performed by the authors?
509"
LIMITATIONS,0.6501766784452296,"Answer: [Yes]
510"
LIMITATIONS,0.6513545347467609,"Justification: We discuss the limitations of our method in conclusion.
511"
LIMITATIONS,0.6525323910482921,"Guidelines:
512"
LIMITATIONS,0.6537102473498233,"• The answer NA means that the paper has no limitation while the answer No means that
513"
LIMITATIONS,0.6548881036513545,"the paper has limitations, but those are not discussed in the paper.
514"
LIMITATIONS,0.6560659599528857,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
515"
LIMITATIONS,0.657243816254417,"• The paper should point out any strong assumptions and how robust the results are to
516"
LIMITATIONS,0.6584216725559482,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
517"
LIMITATIONS,0.6595995288574794,"model well-specification, asymptotic approximations only holding locally). The authors
518"
LIMITATIONS,0.6607773851590106,"should reflect on how these assumptions might be violated in practice and what the
519"
LIMITATIONS,0.6619552414605419,"implications would be.
520"
LIMITATIONS,0.6631330977620731,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
521"
LIMITATIONS,0.6643109540636042,"only tested on a few datasets or with a few runs. In general, empirical results often
522"
LIMITATIONS,0.6654888103651354,"depend on implicit assumptions, which should be articulated.
523"
LIMITATIONS,0.6666666666666666,"• The authors should reflect on the factors that influence the performance of the approach.
524"
LIMITATIONS,0.6678445229681979,"For example, a facial recognition algorithm may perform poorly when image resolution
525"
LIMITATIONS,0.6690223792697291,"is low or images are taken in low lighting. Or a speech-to-text system might not be
526"
LIMITATIONS,0.6702002355712603,"used reliably to provide closed captions for online lectures because it fails to handle
527"
LIMITATIONS,0.6713780918727915,"technical jargon.
528"
LIMITATIONS,0.6725559481743227,"• The authors should discuss the computational efficiency of the proposed algorithms
529"
LIMITATIONS,0.673733804475854,"and how they scale with dataset size.
530"
LIMITATIONS,0.6749116607773852,"• If applicable, the authors should discuss possible limitations of their approach to
531"
LIMITATIONS,0.6760895170789164,"address problems of privacy and fairness.
532"
LIMITATIONS,0.6772673733804476,"• While the authors might fear that complete honesty about limitations might be used by
533"
LIMITATIONS,0.6784452296819788,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
534"
LIMITATIONS,0.6796230859835101,"limitations that aren’t acknowledged in the paper. The authors should use their best
535"
LIMITATIONS,0.6808009422850412,"judgment and recognize that individual actions in favor of transparency play an impor-
536"
LIMITATIONS,0.6819787985865724,"tant role in developing norms that preserve the integrity of the community. Reviewers
537"
LIMITATIONS,0.6831566548881036,"will be specifically instructed to not penalize honesty concerning limitations.
538"
THEORY ASSUMPTIONS AND PROOFS,0.6843345111896348,"3. Theory Assumptions and Proofs
539"
THEORY ASSUMPTIONS AND PROOFS,0.6855123674911661,"Question: For each theoretical result, does the paper provide the full set of assumptions and
540"
THEORY ASSUMPTIONS AND PROOFS,0.6866902237926973,"a complete (and correct) proof?
541"
THEORY ASSUMPTIONS AND PROOFS,0.6878680800942285,"Answer: [NA]
542"
THEORY ASSUMPTIONS AND PROOFS,0.6890459363957597,"Justification: Our paper does not include theoretical results, assumptions and proof.
543"
THEORY ASSUMPTIONS AND PROOFS,0.690223792697291,"Guidelines:
544"
THEORY ASSUMPTIONS AND PROOFS,0.6914016489988222,"• The answer NA means that the paper does not include theoretical results.
545"
THEORY ASSUMPTIONS AND PROOFS,0.6925795053003534,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
546"
THEORY ASSUMPTIONS AND PROOFS,0.6937573616018846,"referenced.
547"
THEORY ASSUMPTIONS AND PROOFS,0.6949352179034158,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
548"
THEORY ASSUMPTIONS AND PROOFS,0.696113074204947,"• The proofs can either appear in the main paper or the supplemental material, but if
549"
THEORY ASSUMPTIONS AND PROOFS,0.6972909305064782,"they appear in the supplemental material, the authors are encouraged to provide a short
550"
THEORY ASSUMPTIONS AND PROOFS,0.6984687868080094,"proof sketch to provide intuition.
551"
THEORY ASSUMPTIONS AND PROOFS,0.6996466431095406,"• Inversely, any informal proof provided in the core of the paper should be complemented
552"
THEORY ASSUMPTIONS AND PROOFS,0.7008244994110718,"by formal proofs provided in appendix or supplemental material.
553"
THEORY ASSUMPTIONS AND PROOFS,0.702002355712603,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7031802120141343,"4. Experimental Result Reproducibility
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7043580683156655,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7055359246171967,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7067137809187279,"of the paper (regardless of whether the code and data are provided or not)?
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7078916372202592,"Answer: [Yes]
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7090694935217904,"Justification: We provide the details of the used model, hyperparameters, source of datasets
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7102473498233216,"and proposed algorithm for reproducing main experimental results.
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7114252061248527,"Guidelines:
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7126030624263839,"• The answer NA means that the paper does not include experiments.
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7137809187279152,"• If the paper includes experiments, a No answer to this question will not be perceived
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7149587750294464,"well by the reviewers: Making the paper reproducible is important, regardless of
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7161366313309776,"whether the code and data are provided or not.
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7173144876325088,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.71849234393404,"to make their results reproducible or verifiable.
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7196702002355713,"• Depending on the contribution, reproducibility can be accomplished in various ways.
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7208480565371025,"For example, if the contribution is a novel architecture, describing the architecture fully
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7220259128386337,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7232037691401649,"be necessary to either make it possible for others to replicate the model with the same
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7243816254416962,"dataset, or provide access to the model. In general. releasing code and data is often
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7255594817432274,"one good way to accomplish this, but reproducibility can also be provided via detailed
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7267373380447585,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7279151943462897,"of a large language model), releasing of a model checkpoint, or other means that are
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7290930506478209,"appropriate to the research performed.
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7302709069493521,"• While NeurIPS does not require releasing code, the conference does require all submis-
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7314487632508834,"sions to provide some reasonable avenue for reproducibility, which may depend on the
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7326266195524146,"nature of the contribution. For example
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7338044758539458,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.734982332155477,"to reproduce that algorithm.
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7361601884570083,"(b) If the contribution is primarily a new model architecture, the paper should describe
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7373380447585395,"the architecture clearly and fully.
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7385159010600707,"(c) If the contribution is a new model (e.g., a large language model), then there should
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7396937573616019,"either be a way to access this model for reproducing the results or a way to reproduce
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7408716136631331,"the model (e.g., with an open-source dataset or instructions for how to construct
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7420494699646644,"the dataset).
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7432273262661955,"(d) We recognize that reproducibility may be tricky in some cases, in which case
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7444051825677267,"authors are welcome to describe the particular way they provide for reproducibility.
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7455830388692579,"In the case of closed-source models, it may be that access to the model is limited in
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7467608951707891,"some way (e.g., to registered users), but it should be possible for other researchers
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7479387514723204,"to have some path to reproducing or verifying the results.
593"
OPEN ACCESS TO DATA AND CODE,0.7491166077738516,"5. Open access to data and code
594"
OPEN ACCESS TO DATA AND CODE,0.7502944640753828,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
595"
OPEN ACCESS TO DATA AND CODE,0.751472320376914,"tions to faithfully reproduce the main experimental results, as described in supplemental
596"
OPEN ACCESS TO DATA AND CODE,0.7526501766784452,"material?
597"
OPEN ACCESS TO DATA AND CODE,0.7538280329799765,"Answer: [Yes]
598"
OPEN ACCESS TO DATA AND CODE,0.7550058892815077,"Justification: We provide open access to the code for our proposed method. In our experi-
599"
OPEN ACCESS TO DATA AND CODE,0.7561837455830389,"ments, we utilize a publicly accessible benchmark dataset described in Section ??.
600"
OPEN ACCESS TO DATA AND CODE,0.7573616018845701,"Guidelines:
601"
OPEN ACCESS TO DATA AND CODE,0.7585394581861012,"• The answer NA means that paper does not include experiments requiring code.
602"
OPEN ACCESS TO DATA AND CODE,0.7597173144876325,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
603"
OPEN ACCESS TO DATA AND CODE,0.7608951707891637,"public/guides/CodeSubmissionPolicy) for more details.
604"
OPEN ACCESS TO DATA AND CODE,0.7620730270906949,"• While we encourage the release of code and data, we understand that this might not be
605"
OPEN ACCESS TO DATA AND CODE,0.7632508833922261,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
606"
OPEN ACCESS TO DATA AND CODE,0.7644287396937574,"including code, unless this is central to the contribution (e.g., for a new open-source
607"
OPEN ACCESS TO DATA AND CODE,0.7656065959952886,"benchmark).
608"
OPEN ACCESS TO DATA AND CODE,0.7667844522968198,"• The instructions should contain the exact command and environment needed to run to
609"
OPEN ACCESS TO DATA AND CODE,0.767962308598351,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
610"
OPEN ACCESS TO DATA AND CODE,0.7691401648998822,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
611"
OPEN ACCESS TO DATA AND CODE,0.7703180212014135,"• The authors should provide instructions on data access and preparation, including how
612"
OPEN ACCESS TO DATA AND CODE,0.7714958775029447,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
613"
OPEN ACCESS TO DATA AND CODE,0.7726737338044759,"• The authors should provide scripts to reproduce all experimental results for the new
614"
OPEN ACCESS TO DATA AND CODE,0.773851590106007,"proposed method and baselines. If only a subset of experiments are reproducible, they
615"
OPEN ACCESS TO DATA AND CODE,0.7750294464075382,"should state which ones are omitted from the script and why.
616"
OPEN ACCESS TO DATA AND CODE,0.7762073027090695,"• At submission time, to preserve anonymity, the authors should release anonymized
617"
OPEN ACCESS TO DATA AND CODE,0.7773851590106007,"versions (if applicable).
618"
OPEN ACCESS TO DATA AND CODE,0.7785630153121319,"• Providing as much information as possible in supplemental material (appended to the
619"
OPEN ACCESS TO DATA AND CODE,0.7797408716136631,"paper) is recommended, but including URLs to data and code is permitted.
620"
OPEN ACCESS TO DATA AND CODE,0.7809187279151943,"6. Experimental Setting/Details
621"
OPEN ACCESS TO DATA AND CODE,0.7820965842167256,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
622"
OPEN ACCESS TO DATA AND CODE,0.7832744405182568,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
623"
OPEN ACCESS TO DATA AND CODE,0.784452296819788,"results?
624"
OPEN ACCESS TO DATA AND CODE,0.7856301531213192,"Answer: [Yes]
625"
OPEN ACCESS TO DATA AND CODE,0.7868080094228505,"Justification: We provide the details of the experimental setting, including hyperparameters,
626"
OPEN ACCESS TO DATA AND CODE,0.7879858657243817,"in Sections 4.1 and C. The generation details of the texts used in our method are also
627"
OPEN ACCESS TO DATA AND CODE,0.7891637220259128,"provided in Section A.
628"
OPEN ACCESS TO DATA AND CODE,0.790341578327444,"Guidelines:
629"
OPEN ACCESS TO DATA AND CODE,0.7915194346289752,"• The answer NA means that the paper does not include experiments.
630"
OPEN ACCESS TO DATA AND CODE,0.7926972909305064,"• The experimental setting should be presented in the core of the paper to a level of detail
631"
OPEN ACCESS TO DATA AND CODE,0.7938751472320377,"that is necessary to appreciate the results and make sense of them.
632"
OPEN ACCESS TO DATA AND CODE,0.7950530035335689,"• The full details can be provided either with the code, in appendix, or as supplemental
633"
OPEN ACCESS TO DATA AND CODE,0.7962308598351001,"material.
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7974087161366313,"7. Experiment Statistical Significance
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7985865724381626,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7997644287396938,"information about the statistical significance of the experiments?
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.800942285041225,"Answer: [Yes]
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8021201413427562,"Justification: We do not report the statistical significance of the experimental results, as our
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8032979976442874,"method does not rely on statistical variables for inference.
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8044758539458187,"Guidelines:
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8056537102473498,"• The answer NA means that the paper does not include experiments.
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.806831566548881,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8080094228504122,"dence intervals, or statistical significance tests, at least for the experiments that support
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8091872791519434,"the main claims of the paper.
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8103651354534747,"• The factors of variability that the error bars are capturing should be clearly stated (for
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8115429917550059,"example, train/test split, initialization, random drawing of some parameter, or overall
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8127208480565371,"run with given experimental conditions).
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8138987043580683,"• The method for calculating the error bars should be explained (closed form formula,
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8150765606595995,"call to a library function, bootstrap, etc.)
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8162544169611308,"• The assumptions made should be given (e.g., Normally distributed errors).
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.817432273262662,"• It should be clear whether the error bar is the standard deviation or the standard error
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8186101295641932,"of the mean.
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8197879858657244,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8209658421672555,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8221436984687868,"of Normality of errors is not verified.
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.823321554770318,"• For asymmetric distributions, the authors should be careful not to show in tables or
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8244994110718492,"figures symmetric error bars that would yield results that are out of range (e.g. negative
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8256772673733804,"error rates).
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8268551236749117,"• If error bars are reported in tables or plots, The authors should explain in the text how
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8280329799764429,"they were calculated and reference the corresponding figures or tables in the text.
661"
EXPERIMENTS COMPUTE RESOURCES,0.8292108362779741,"8. Experiments Compute Resources
662"
EXPERIMENTS COMPUTE RESOURCES,0.8303886925795053,"Question: For each experiment, does the paper provide sufficient information on the computer
663"
EXPERIMENTS COMPUTE RESOURCES,0.8315665488810365,"resources (type of compute workers, memory, time of execution) needed to reproduce the
664"
EXPERIMENTS COMPUTE RESOURCES,0.8327444051825678,"experiments?
665"
EXPERIMENTS COMPUTE RESOURCES,0.833922261484099,"Answer: [Yes]
666"
EXPERIMENTS COMPUTE RESOURCES,0.8351001177856302,"Justification: We provide the information of types of compute worker (GPU model), memory
667"
EXPERIMENTS COMPUTE RESOURCES,0.8362779740871613,"usage and inference time in Section. 4.3.3.
668"
EXPERIMENTS COMPUTE RESOURCES,0.8374558303886925,"Guidelines:
669"
EXPERIMENTS COMPUTE RESOURCES,0.8386336866902238,"• The answer NA means that the paper does not include experiments.
670"
EXPERIMENTS COMPUTE RESOURCES,0.839811542991755,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
671"
EXPERIMENTS COMPUTE RESOURCES,0.8409893992932862,"or cloud provider, including relevant memory and storage.
672"
EXPERIMENTS COMPUTE RESOURCES,0.8421672555948174,"• The paper should provide the amount of compute required for each of the individual
673"
EXPERIMENTS COMPUTE RESOURCES,0.8433451118963486,"experimental runs as well as estimate the total compute.
674"
EXPERIMENTS COMPUTE RESOURCES,0.8445229681978799,"• The paper should disclose whether the full research project required more compute
675"
EXPERIMENTS COMPUTE RESOURCES,0.8457008244994111,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
676"
EXPERIMENTS COMPUTE RESOURCES,0.8468786808009423,"didn’t make it into the paper).
677"
CODE OF ETHICS,0.8480565371024735,"9. Code Of Ethics
678"
CODE OF ETHICS,0.8492343934040048,"Question: Does the research conducted in the paper conform, in every respect, with the
679"
CODE OF ETHICS,0.850412249705536,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
680"
CODE OF ETHICS,0.8515901060070671,"Answer: [Yes]
681"
CODE OF ETHICS,0.8527679623085983,"Justification: We abide by the NeurIPS Code of Ethics.
682"
CODE OF ETHICS,0.8539458186101295,"Guidelines:
683"
CODE OF ETHICS,0.8551236749116607,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
684"
CODE OF ETHICS,0.856301531213192,"• If the authors answer No, they should explain the special circumstances that require a
685"
CODE OF ETHICS,0.8574793875147232,"deviation from the Code of Ethics.
686"
CODE OF ETHICS,0.8586572438162544,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
687"
CODE OF ETHICS,0.8598351001177856,"eration due to laws or regulations in their jurisdiction).
688"
BROADER IMPACTS,0.8610129564193169,"10. Broader Impacts
689"
BROADER IMPACTS,0.8621908127208481,"Question: Does the paper discuss both potential positive societal impacts and negative
690"
BROADER IMPACTS,0.8633686690223793,"societal impacts of the work performed?
691"
BROADER IMPACTS,0.8645465253239105,"Answer: [Yes]
692"
BROADER IMPACTS,0.8657243816254417,"Justification: We discuss the positive and negative societal impacts of our paper in the
693"
BROADER IMPACTS,0.866902237926973,"supplementary material.
694"
BROADER IMPACTS,0.8680800942285041,"Guidelines:
695"
BROADER IMPACTS,0.8692579505300353,"• The answer NA means that there is no societal impact of the work performed.
696"
BROADER IMPACTS,0.8704358068315665,"• If the authors answer NA or No, they should explain why their work has no societal
697"
BROADER IMPACTS,0.8716136631330977,"impact or why the paper does not address societal impact.
698"
BROADER IMPACTS,0.872791519434629,"• Examples of negative societal impacts include potential malicious or unintended uses
699"
BROADER IMPACTS,0.8739693757361602,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
700"
BROADER IMPACTS,0.8751472320376914,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
701"
BROADER IMPACTS,0.8763250883392226,"groups), privacy considerations, and security considerations.
702"
BROADER IMPACTS,0.8775029446407538,"• The conference expects that many papers will be foundational research and not tied
703"
BROADER IMPACTS,0.8786808009422851,"to particular applications, let alone deployments. However, if there is a direct path to
704"
BROADER IMPACTS,0.8798586572438163,"any negative applications, the authors should point it out. For example, it is legitimate
705"
BROADER IMPACTS,0.8810365135453475,"to point out that an improvement in the quality of generative models could be used to
706"
BROADER IMPACTS,0.8822143698468787,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
707"
BROADER IMPACTS,0.8833922261484098,"that a generic algorithm for optimizing neural networks could enable people to train
708"
BROADER IMPACTS,0.8845700824499411,"models that generate Deepfakes faster.
709"
BROADER IMPACTS,0.8857479387514723,"• The authors should consider possible harms that could arise when the technology is
710"
BROADER IMPACTS,0.8869257950530035,"being used as intended and functioning correctly, harms that could arise when the
711"
BROADER IMPACTS,0.8881036513545347,"technology is being used as intended but gives incorrect results, and harms following
712"
BROADER IMPACTS,0.889281507656066,"from (intentional or unintentional) misuse of the technology.
713"
BROADER IMPACTS,0.8904593639575972,"• If there are negative societal impacts, the authors could also discuss possible mitigation
714"
BROADER IMPACTS,0.8916372202591284,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
715"
BROADER IMPACTS,0.8928150765606596,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
716"
BROADER IMPACTS,0.8939929328621908,"feedback over time, improving the efficiency and accessibility of ML).
717"
SAFEGUARDS,0.8951707891637221,"11. Safeguards
718"
SAFEGUARDS,0.8963486454652533,"Question: Does the paper describe safeguards that have been put in place for responsible
719"
SAFEGUARDS,0.8975265017667845,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
720"
SAFEGUARDS,0.8987043580683156,"image generators, or scraped datasets)?
721"
SAFEGUARDS,0.8998822143698468,"Answer: [NA]
722"
SAFEGUARDS,0.901060070671378,"Justification: Our paper does not pose a high risk for misuse in terms of model and dataset.
723"
SAFEGUARDS,0.9022379269729093,"Guidelines:
724"
SAFEGUARDS,0.9034157832744405,"• The answer NA means that the paper poses no such risks.
725"
SAFEGUARDS,0.9045936395759717,"• Released models that have a high risk for misuse or dual-use should be released with
726"
SAFEGUARDS,0.9057714958775029,"necessary safeguards to allow for controlled use of the model, for example by requiring
727"
SAFEGUARDS,0.9069493521790342,"that users adhere to usage guidelines or restrictions to access the model or implementing
728"
SAFEGUARDS,0.9081272084805654,"safety filters.
729"
SAFEGUARDS,0.9093050647820966,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
730"
SAFEGUARDS,0.9104829210836278,"should describe how they avoided releasing unsafe images.
731"
SAFEGUARDS,0.911660777385159,"• We recognize that providing effective safeguards is challenging, and many papers do
732"
SAFEGUARDS,0.9128386336866903,"not require this, but we encourage authors to take this into account and make a best
733"
SAFEGUARDS,0.9140164899882215,"faith effort.
734"
LICENSES FOR EXISTING ASSETS,0.9151943462897526,"12. Licenses for existing assets
735"
LICENSES FOR EXISTING ASSETS,0.9163722025912838,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
736"
LICENSES FOR EXISTING ASSETS,0.917550058892815,"the paper, properly credited and are the license and terms of use explicitly mentioned and
737"
LICENSES FOR EXISTING ASSETS,0.9187279151943463,"properly respected?
738"
LICENSES FOR EXISTING ASSETS,0.9199057714958775,"Answer: [Yes]
739"
LICENSES FOR EXISTING ASSETS,0.9210836277974087,"Justification: We cite the papers that provide datasets, code and models in the Section. 4.
740"
LICENSES FOR EXISTING ASSETS,0.9222614840989399,"Guidelines:
741"
LICENSES FOR EXISTING ASSETS,0.9234393404004712,"• The answer NA means that the paper does not use existing assets.
742"
LICENSES FOR EXISTING ASSETS,0.9246171967020024,"• The authors should cite the original paper that produced the code package or dataset.
743"
LICENSES FOR EXISTING ASSETS,0.9257950530035336,"• The authors should state which version of the asset is used and, if possible, include a
744"
LICENSES FOR EXISTING ASSETS,0.9269729093050648,"URL.
745"
LICENSES FOR EXISTING ASSETS,0.928150765606596,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
746"
LICENSES FOR EXISTING ASSETS,0.9293286219081273,"• For scraped data from a particular source (e.g., website), the copyright and terms of
747"
LICENSES FOR EXISTING ASSETS,0.9305064782096584,"service of that source should be provided.
748"
LICENSES FOR EXISTING ASSETS,0.9316843345111896,"• If assets are released, the license, copyright information, and terms of use in the
749"
LICENSES FOR EXISTING ASSETS,0.9328621908127208,"package should be provided. For popular datasets, paperswithcode.com/datasets
750"
LICENSES FOR EXISTING ASSETS,0.934040047114252,"has curated licenses for some datasets. Their licensing guide can help determine the
751"
LICENSES FOR EXISTING ASSETS,0.9352179034157833,"license of a dataset.
752"
LICENSES FOR EXISTING ASSETS,0.9363957597173145,"• For existing datasets that are re-packaged, both the original license and the license of
753"
LICENSES FOR EXISTING ASSETS,0.9375736160188457,"the derived asset (if it has changed) should be provided.
754"
LICENSES FOR EXISTING ASSETS,0.9387514723203769,"• If this information is not available online, the authors are encouraged to reach out to
755"
LICENSES FOR EXISTING ASSETS,0.9399293286219081,"the asset’s creators.
756"
NEW ASSETS,0.9411071849234394,"13. New Assets
757"
NEW ASSETS,0.9422850412249706,"Question: Are new assets introduced in the paper well documented and is the documentation
758"
NEW ASSETS,0.9434628975265018,"provided alongside the assets?
759"
NEW ASSETS,0.944640753828033,"Answer: [Yes]
760"
NEW ASSETS,0.9458186101295641,"Justification: We provide the detail documentation for the code in our submission.
761"
NEW ASSETS,0.9469964664310954,"Guidelines:
762"
NEW ASSETS,0.9481743227326266,"• The answer NA means that the paper does not release new assets.
763"
NEW ASSETS,0.9493521790341578,"• Researchers should communicate the details of the dataset/code/model as part of their
764"
NEW ASSETS,0.950530035335689,"submissions via structured templates. This includes details about training, license,
765"
NEW ASSETS,0.9517078916372202,"limitations, etc.
766"
NEW ASSETS,0.9528857479387515,"• The paper should discuss whether and how consent was obtained from people whose
767"
NEW ASSETS,0.9540636042402827,"asset is used.
768"
NEW ASSETS,0.9552414605418139,"• At submission time, remember to anonymize your assets (if applicable). You can either
769"
NEW ASSETS,0.9564193168433451,"create an anonymized URL or include an anonymized zip file.
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9575971731448764,"14. Crowdsourcing and Research with Human Subjects
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9587750294464076,"Question: For crowdsourcing experiments and research with human subjects, does the paper
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9599528857479388,"include the full text of instructions given to participants and screenshots, if applicable, as
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9611307420494699,"well as details about compensation (if any)?
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9623085983510011,"Answer: [NA]
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9634864546525324,"Justification: Our paper does not involve neither crowdsourcing nor research with human
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9646643109540636,"subjects.
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9658421672555948,"Guidelines:
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967020023557126,"• The answer NA means that the paper does not involve crowdsourcing nor research with
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681978798586572,"human subjects.
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693757361601885,"• Including this information in the supplemental material is fine, but if the main contribu-
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9705535924617197,"tion of the paper involves human subjects, then as much detail as possible should be
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717314487632509,"included in the main paper.
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9729093050647821,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9740871613663133,"or other labor should be paid at least the minimum wage in the country of the data
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752650176678446,"collector.
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764428739693758,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776207302709069,"Subjects
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787985865724381,"Question: Does the paper describe potential risks incurred by study participants, whether
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799764428739693,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811542991755006,"approvals (or an equivalent approval/review based on the requirements of your country or
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823321554770318,"institution) were obtained?
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983510011778563,"Answer: [NA]
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846878680800942,"Justification: Our paper does not involve neither crowdsourcing nor research with human
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858657243816255,"subjects.
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870435806831567,"Guidelines:
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882214369846879,"• The answer NA means that the paper does not involve crowdsourcing nor research with
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893992932862191,"human subjects.
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905771495877503,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917550058892816,"may be required for any human subjects research. If you obtained IRB approval, you
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929328621908127,"should clearly state this in the paper.
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941107184923439,"• We recognize that the procedures for this may vary significantly between institutions
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952885747938751,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964664310954063,"guidelines for their institution.
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976442873969376,"• For initial submissions, do not include any information that would break anonymity (if
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988221436984688,"applicable), such as the institution conducting the review.
806"
