Section,Section Appearance Order,Paragraph
SEOUL NATIONAL UNIVERSITY,0.0,"1Seoul National University
2Yonsei University
3Korea Institute for Advanced Study"
ABSTRACT,0.005050505050505051,Abstract
ABSTRACT,0.010101010101010102,"Despite the success of diffusion models (DMs), we still lack a thorough understand-
ing of their latent space. To understand the latent space xt ∈X, we analyze them
from a geometrical perspective. Our approach involves deriving the local latent
basis within X by leveraging the pullback metric associated with their encoding
feature maps. Remarkably, our discovered local latent basis enables image editing
capabilities by moving xt, the latent space of DMs, along the basis vector at specific
timesteps. We further analyze how the geometric structure of DMs evolves over
diffusion timesteps and differs across different text conditions. This confirms the
known phenomenon of coarse-to-fine generation, as well as reveals novel insights
such as the discrepancy between xt across timesteps, the effect of dataset complex-
ity, and the time-varying influence of text prompts. To the best of our knowledge,
this paper is the first to present image editing through x-space traversal, editing only
once at specific timestep t without any additional training, and providing thorough
analyses of the latent structure of DMs. The code to reproduce our experiments can
be found at https://github.com/enkeejunior1/Diffusion-Pullback."
INTRODUCTION,0.015151515151515152,"1
Introduction"
INTRODUCTION,0.020202020202020204,"The diffusion models (DMs) are powerful generative models that have demonstrated impressive
performance [22, 51, 52, 17, 37]. DMs have shown remarkable applications, including text-to-image
synthesis [45, 46, 5, 36], inverse problems [14, 31], and image editing [21, 54, 39, 35]."
INTRODUCTION,0.025252525252525252,"Despite their achievements, the research community lacks a comprehensive understanding of the
latent space of DMs and its influence on the generated results. So far, the completely diffused images
are considered as latent variables but it does not have useful properties for controlling the results. For
example, traversing along a direction from a latent produces weird changes in the results. Fortunately,
Kwon et al. [26] consider the intermediate feature space of the diffusion kernel, referred to as H,
as a semantic latent space and show its usefulness on controlling generated images. In the similar
sense, some works investigate the feature maps of the self-attention or cross-attention operations
for controlling the results [21, 54, 39], improving sample quality [8], or downstream tasks such as
semantic segmentation [32, 55]."
INTRODUCTION,0.030303030303030304,"Still, the structure of the space Xt where latent variables {xt} live remains unexplored despite its
crucial role in understanding DMs. It is especially challenging because 1) the model is trained
to estimate the forward noise which does not depend on the input, as opposed to other typical
supervisions such as classification or similarity, and 2) there are lots of latent variables over multiple
recursive timesteps. In this paper, we aim to analyze X in conjunction with its corresponding"
INTRODUCTION,0.03535353535353535,"∗Equal Contribution
† Corresponding authors"
INTRODUCTION,0.04040404040404041,Latent space
INTRODUCTION,0.045454545454545456,structure
INTRODUCTION,0.050505050505050504,Feature space
INTRODUCTION,0.05555555555555555,structure
INTRODUCTION,0.06060606060606061,"Figure 1: Conceptual illustration of local geometric structure. (a) The local basis {v1, v2, · · · } of
the local latent subspace Txt within the latent space X is interlinked with the local basis {u1, u2, · · · }
of the local tangent space Tht in the feature space H. (b) The derivation of these local bases is
facilitated through the singular value decomposition (SVD) of the Jacobian, which emanates from the
U-Net responsible for encoding the feature map f, linking X and H."
INTRODUCTION,0.06565656565656566,"representation H, by incorporating a local geometry to X using the concept of a pullback metric in
Riemannian geometry."
INTRODUCTION,0.0707070707070707,"First, we discover the local latent basis for X and the corresponding local tangent basis for H. The
local basis is obtained by performing singular value decomposition (SVD) of the Jacobian of the
mapping from X to H. To validate the discovered local latent basis, we demonstrate that walking
along the basis can edit real images in a semantically meaningful way. Furthermore, we can use
the discovered local latent basis vector to edit other samples by using parallel transport, when they
exhibit comparable local geometric structures. Note that existing editing methods manipulate the
self-attention map or cross-attention map over multiple timesteps [21, 54, 39]. On the other hand, we
manipulate only xt once at a specific timestep."
INTRODUCTION,0.07575757575757576,"Second, we investigate how the latent structures differ across different timesteps and samples as
follows. The frequency domain of the local latent basis shifts from low-frequency to high-frequency
along the generative process. We explicitly confirm it using power spectral density analysis. The
difference between local tangent spaces of different samples becomes larger along the generative
process. The local tangent spaces at various diffusion timesteps are similar to each other if the model
is trained on aligned datasets such as CelebA-HQ or Flowers. However, this homogeneity does not
occur on complex datasets such as ImageNet."
INTRODUCTION,0.08080808080808081,"Finally, we examine how the prompts affect the latent structure of text-to-image DMs as follows.
Similar prompts yield similar latent structures. Specifically, we find a positive correlation between
the similarity of prompts and the similarity of local tangent spaces. The influence of text on the local
tangent space becomes weaker along the generative process."
INTRODUCTION,0.08585858585858586,"Our work examines the geometry of X and H using Riemannian geometry. We discover the latent
structure of X and how it evolves during the generative process and is influenced by prompts. This
geometric exploration deepens our understanding of DMs."
RELATED WORKS,0.09090909090909091,"2
Related works"
RELATED WORKS,0.09595959595959595,"Diffusion Models.
Recent advances in DMs make great progress in the field of image synthesis
and show state-of-the-art performance [50, 22, 51]. An important subject in the diffusion model is
the introduction of gradient guidance, including classifier-free guidance, to control the generative
process [17, 47, 4, 30, 36, 46]. The work by Song et al. [52] has facilitated the unification of DMs
with score-based models using SDEs, enhancing our understanding of DMs as a reverse diffusion
process. However, the latent space is still largely unexplored, and our understanding is limited."
RELATED WORKS,0.10101010101010101,"The study of latent space in GANs.
The study of latent spaces has gained significant attention in
recent years. In the field of Generative Adversarial Networks (GANs), researchers have proposed
various methods to manipulate the latent space to achieve the desired effect in the generated images"
RELATED WORKS,0.10606060606060606,"[44, 41, 1, 20, 49, 59, 38]. More recently, several studies [60, 10] have examined the geometrical
properties of latent space in GANs and utilized these findings for image manipulations. These studies
bring the advantage of better understanding the characteristics of the latent space and facilitating the
analysis and utilization of GANs. In contrast, the latent space of DMs remains poorly understood,
making it difficult to fully utilize their capabilities."
RELATED WORKS,0.1111111111111111,"Image manipulation in DMs.
Early works include Choi et al. [11] and Meng et al. [33] have
attempted to manipulate the resulting images of DMs by replacing latent variables, allowing the
generation of desired random images. However, due to the lack of semantics in the latent variables of
DMs, current approaches have critical problems with semantic image editing. Alternative approaches
have explored the potential of using the feature space within the U-Net for semantic image manipula-
tion. For example, Kwon et al. [26] have shown that the bottleneck of the U-Net, H, can be used as a
semantic latent space. Specifically, they used CLIP [43] to identify directions within H that facilitate
genuine image editing. Baranchuk et al. [6] and Tumanyan et al. [54] use the feature map of the
U-Net for semantic segmentation and maintaining the structure of generated images. Unlike previous
works, our editing method finds the editing direction without supervision, and directly traverses the
latent variable along the latent basis."
RELATED WORKS,0.11616161616161616,"Riemannain Geometry.
Some studies have applied Riemannian geometry to analyze the latent
spaces of deep generative models, such as Variational Autoencoders (VAEs) and GANs [2, 48, 9,
3, 27, 28, 57]. Shao et al. [48] proposed a pullback metric on the latent space from image space
Euclidean metric to analyze the latent space’s geometry. This method has been widely used in VAEs
and GANs because it only requires a differentiable map from latent space to image space. However,
no studies have investigated the geometry of latent space of DMs utilizing the pullback metric."
DISCOVERING THE LATENT BASIS OF DMS,0.12121212121212122,"3
Discovering the latent basis of DMs"
DISCOVERING THE LATENT BASIS OF DMS,0.12626262626262627,"In this section, we explain how to extract a latent structure of X using differential geometry. First, we
introduce a key concept in our method: the pullback metric. Next, by adopting the local Euclidean
metric of H and utilizing the pullback metric, we discover the local latent basis of the X. Moreover,
although the direction we found is local, we show how it can be applied to other samples via parallel
transport. Finally, we introduce x-space guidance for editing data in the X to enhance the quality of
image generation."
PULLBACK METRIC,0.13131313131313133,"3.1
Pullback metric"
PULLBACK METRIC,0.13636363636363635,"We consider a curved manifold, X, where our latent variables xt exist. The differential geometry
represents X through patches of tangent spaces, Tx, which are vector spaces defined at each point
x. Then, all the geometrical properties of X can be obtained from the inner product of ||dx||2 =
⟨dx, dx⟩x in Tx. However, we do not have any knowledge of ⟨dx, dx⟩x. It is definitely not a Euclidean
metric. Furthermore, samples of xt at intermediate timesteps of DMs include inevitable noise, which
prevents finding semantic directions in Tx."
PULLBACK METRIC,0.1414141414141414,"Fortunately, Kwon et al. [26] observed that H, defined by the bottleneck layer of the U-Net, exhibits
local linear structure. This allows us to adopt the Euclidean metric on H. In differential geometry,
when a metric is not available on a space, pullback metric is used. If a smooth map exists between the
original metric-unavailable domain and a metric-available codomain, the pullback metric is used to
measure the distances in the domain space. Our idea is to use the pullback Euclidean metric on H to
define the distances between the samples in X."
PULLBACK METRIC,0.14646464646464646,"DMs are trained to infer the noise ϵt from a latent variable xt at each diffusion timestep t. Each xt
has a different internal representation ht, the bottleneck representation of the U-Net, at different
t’s. The differentiable map between X and H is denoted as f : X →H. Hereafter, we refer to xt
as x for brevity unless it causes confusion. It is important to note that our method can be applied
at any timestep in the denoising process. We consider a linear map, Tx →Th, between the domain
and codomain tangent spaces. This linear map can be described by the Jacobian Jx = ∇xh which
determines how a vector v ∈Tx is mapped into a vector u ∈Th by u = Jxv."
PULLBACK METRIC,0.15151515151515152,"Using the local linearity of H, we assume the metric, ||dh||2 = ⟨dh, dh⟩h = dh
Tdh as a usual dot
product defined in the Euclidean space. To assign a geometric structure to X, we use the pullback"
PULLBACK METRIC,0.15656565656565657,"w/o prompt
w/ prompt"
PULLBACK METRIC,0.16161616161616163,"Bear
Ostrich
Lion
Tiger"
PULLBACK METRIC,0.16666666666666666,Input image ①DDIM
PULLBACK METRIC,0.1717171717171717,inversion
PULLBACK METRIC,0.17676767676767677,"②DDIM
generation"
PULLBACK METRIC,0.18181818181818182,"⑤DDIM
generation"
PULLBACK METRIC,0.18686868686868688,: Probability flow ODE trajectories
PULLBACK METRIC,0.1919191919191919,"④x-space guidance
③Identify the local basis vector (𝐯)"
PULLBACK METRIC,0.19696969696969696,"(a) Method overview
(b) Editing results"
PULLBACK METRIC,0.20202020202020202,"𝐱𝑇
𝐱𝑡
෤𝐱𝑡 𝐱𝑡 ෤𝐱𝑡 𝐱0"
PULLBACK METRIC,0.20707070707070707,"𝑝(𝐱0)
𝑝(𝐱𝑇)"
PULLBACK METRIC,0.21212121212121213,"Figure 2: Image editing with the discovered latent basis. (a) Schematic depiction of our image
editing procedure. ①An input image is subjected to DDIM inversion, resulting in an initial noisy
sample xT . ②The sample xT is progressively denoised until reaching the point t through DDIM
generation. ③Subsequently, the local latent basis {v1, · · · , vn} is identified by using the pullback
metric. ④This enables the manipulation of the sample xt along one of the basis vectors using
x-space guidance. ⑤The DDIM generation concludes with the progression from the modified latent
variable ˜xt. (b) Examples of edited images using a selected basis vector. The latent basis vector could
be conditioned on prompts and it facilitates text-aligned manipulations."
PULLBACK METRIC,0.21717171717171718,"metric of the corresponding H. In other words, the norm of v ∈Tx is measured by the norm of
corresponding codomain tangent vector:"
PULLBACK METRIC,0.2222222222222222,"||v||2
pb ≜⟨u, u⟩h = v
TJx"
PULLBACK METRIC,0.22727272727272727,"TJxv.
(1)"
FINDING LOCAL LATENT BASIS,0.23232323232323232,"3.2
Finding local latent basis"
FINDING LOCAL LATENT BASIS,0.23737373737373738,"Using the pullback metric, we define the local latent vector v ∈Tx that shows a large variability
in Th. We find a unit vector v1 that maximizes ||v||2
pb. By maximizing ||v||2
pb while remaining
orthogonal to v1, one can obtain the second unit vector v2. This process can be repeated to have
n latent directions of {v1, v2, · · · , vn} in Tx. In practice, vi corresponds to the i-th right singular
vector from the singular value decomposition (SVD) of Jx = UΛV
T, i.e., Jxvi = Λiui. Since the
Jacobian of too many parameters is not tractable, we use a power method [18, 34, 19] to approximate
the SVD of Jx (See Appendix D for the time complexity and Appendix F for the detailed algorithm)."
FINDING LOCAL LATENT BASIS,0.24242424242424243,"Henceforth, we refer to Tx as a local latent subspace, and {v1, v2, · · · , vn} as the corresponding
local latent basis."
FINDING LOCAL LATENT BASIS,0.2474747474747475,"Tx ≜span{v1, v2 · · · , vn}, where vi is i-th right singular vector of Jx.
(2)"
FINDING LOCAL LATENT BASIS,0.25252525252525254,"Using the linear transformation between Tx and Th via the Jacobian Jx, one can also obtain corre-
sponding directions in Th. In practice, ui corresponds to the i-th left singular vector from the SVD of
Jx. After selecting the top n (e.g., n = 50) directions of large eigenvalues, we can approximate any
vector in Th with a finite basis, {u1, u2, · · · , un}. When we refer to a local tangent space henceforth,
it means the n-dimensional low-rank approximation of the original tangent space."
FINDING LOCAL LATENT BASIS,0.25757575757575757,"Th ≜span{u1, u2 · · · , un}, where ui is the i-th left singular vector of Jx.
(3)"
FINDING LOCAL LATENT BASIS,0.26262626262626265,"The collection of local latent basis vectors, {v1, v2, · · · , vn}, obtained through our proposed method,
can be interpreted as a signal that the model is highly response to for a given x. On the other hand, the
basis of the local tangent space, denoted as {u1, u2 · · · , un}, can be viewed as the corresponding
representation associated with the signal."
FINDING LOCAL LATENT BASIS,0.2676767676767677,"In Stable Diffusion, the prompt also influences the Jacobian, which means that the local basis also
depends on it. We can utilize any prompt to obtain a local latent basis, and different prompts create
distinct geometrical structures. For the sake of brevity, we will omit the word local unless it leads to
confusion."
GENERATING EDITED IMAGES WITH X-SPACE GUIDANCE,0.2727272727272727,"3.3
Generating edited images with x-space guidance"
GENERATING EDITED IMAGES WITH X-SPACE GUIDANCE,0.2777777777777778,"A naïve approach for manipulating a latent variable x using a latent vector v is through simple
addition, specifically x + γv. However, using the naïve approach sometime leads to noisy image"
GENERATING EDITED IMAGES WITH X-SPACE GUIDANCE,0.2828282828282828,"generation. To address this issue, instead of directly using the basis for manipulation, we use a basis
vector that has passed through the decoder once for manipulation. The x-space guidance is defined
as follows"
GENERATING EDITED IMAGES WITH X-SPACE GUIDANCE,0.2878787878787879,"˜xXG = x + γ[ϵθ(x + v) −ϵθ(x)]
(4)"
GENERATING EDITED IMAGES WITH X-SPACE GUIDANCE,0.29292929292929293,"where γ is a hyper-parameter controlling the strength of editing and ϵθ is a diffusion model. Equation 4
is inspired by classifier-free guidance, but the key difference is that it is directly applied in the latent
space X. Our x-space guidance provides qualitatively similar results to direct addition, while it shows
better fidelity. (See Appendix C for ablation study.)"
THE OVERALL PROCESS OF IMAGE EDITING,0.29797979797979796,"3.4
The overall process of image editing"
THE OVERALL PROCESS OF IMAGE EDITING,0.30303030303030304,"In this section, we summarize the entire editing process with five steps: 1) The input image is inverted
into initial noise xT using DDIM inversion. 2) xT is gradually denoised until t through DDIM
generation. 3) Identify local latent basis {v1, · · · , vn} using the pullback metric at t. 4) Manipulate
xt along the one of basis vectors using the x-space guidance. 5) The DDIM generation is then
completed with the modified latent variable ˜xt. Figure 2 illustrates the entire editing process."
THE OVERALL PROCESS OF IMAGE EDITING,0.30808080808080807,"In the context of a text-to-image model, such as Stable Diffusion, it becomes possible to include
textual conditions while deriving local basis vectors. Although we do not use any text guidance
during DDIM inversion and generation, a local basis with text conditions enables semantic editing
that matches the given text. Comprehensive experiments can be found in Section 4.1."
THE OVERALL PROCESS OF IMAGE EDITING,0.31313131313131315,"It is noteworthy that our approach needs no extra training and simplifies image editing by only
adjusting the latent variable within a single timestep."
EDITING VARIOUS SAMPLES WITH PARALLEL TRANSPORT,0.3181818181818182,"3.5
Editing various samples with parallel transport"
EDITING VARIOUS SAMPLES WITH PARALLEL TRANSPORT,0.32323232323232326,"Let us consider a scenario where our aim is to edit ten images, changing straight hair to curly hair. Due
to the nature of the unsupervised image editing method, it is becomes imperative to manually check
the semantic relevance of the latent basis vector in the edited results. Thus, to edit every samples, we
have to manually find a straight-to-curly basis vector for individual samples."
EDITING VARIOUS SAMPLES WITH PARALLEL TRANSPORT,0.3282828282828283,"One way to alleviate this tedious task is to apply the curly hair vector obtained from one image to
other images. However, the basis vector v ∈Tx obtained at x cannot be used for the other sample x′
because v /∈Tx′. Thus, in order to apply the direction we obtained to another sample, it is necessary
to relocate the extracted direction to a new tangent space."
EDITING VARIOUS SAMPLES WITH PARALLEL TRANSPORT,0.3333333333333333,"To achieve this, we use parallel transport that moves vi onto the new tangent space Tx′. In the realm
of differential geometry, parallel transport is a technique to relocate a tangent vector to different
position with minimal direction change, while keeping the vector tangent on the manifold [48]. It is
notable that in curved space, parallel transport can significantly modify the original vector. Therefore,
it is beneficial to apply the parallel transport in H, where relatively flatter than X."
EDITING VARIOUS SAMPLES WITH PARALLEL TRANSPORT,0.3383838383838384,"The process of moving a tangent vector v ∈Tx to v′ ∈Tx′ through parallel transport in H is
summarized as follows. First, we convert the latent direction vi ∈Tx to the corresponding direction
of ui ∈Th. Second, we apply the parallel transport ui ∈Th to u′i ∈Th′, where h′ = f(x′). In the
general case, parallel transport involves iterative projection and normalization on the tangent space
along the path connecting two points [48]. However, in our case, we assume that H has Euclidean
geometry. Therefore, we move u directly onto Th′ through projection, without the need for an iterative
process. Finally, transform u′
i into v′
i ∈X. Using this parallel transport of vi →v′
i via H, we can
apply the local latent basis obtained from x to edit or modify the input x′."
FINDINGS AND RESULTS,0.3434343434343434,"4
Findings and results"
FINDINGS AND RESULTS,0.3484848484848485,"In this section, we analyze the geometric structure of DMs with our method. § 4.1 demonstrates that
the latent basis found by our method can be used for image editing. In § 4.2, we investigate how the
geometric structure of DMs evolves as the generative process progresses. Lastly, in § 4.3, we examine
how the geometric properties of the text-condition model change with a given text."
FINDINGS AND RESULTS,0.35353535353535354,"The implementation details of our work are provided on Appendix B. The source code for our
experiments can be found at https://github.com/enkeejunior1/Diffusion-Pullback."
FINDINGS AND RESULTS,0.35858585858585856,"Real image
(Old)
Real image
(Woman)
Real image
(Red nose)"
FINDINGS AND RESULTS,0.36363636363636365,"Real image
(Sunflower)
Real image
(Yellow)
Real image
(Pinky)"
FINDINGS AND RESULTS,0.3686868686868687,"Real image
(Bullmastiff)
Real image
(Open mouth)
Real image
(Wink)"
FINDINGS AND RESULTS,0.37373737373737376,"Real image
(Two-story)
Real image
(Long roof)
Real image
(White paint)"
FINDINGS AND RESULTS,0.3787878787878788,"Real image
(Woman)
Real image
(Overweighted)"
FINDINGS AND RESULTS,0.3838383838383838,"FFHQ
Flowers
AFHQ
LSUN-Church
Stable Diffusion"
FINDINGS AND RESULTS,0.3888888888888889,"Figure 3: Examples of image edition using the latent basis. The attributes are manually interpreted
since the editing directions are not intentionally supervised. For Stable Diffusion, we used an empty
string as a prompt. Each column represents edits made at different diffusion timesteps (0.6T, 0.8T,
and T for the unconditional diffusion model; 0.6T and 0.7T for Stable Diffusion)."
FINDINGS AND RESULTS,0.3939393939393939,Real image
FINDINGS AND RESULTS,0.398989898989899,"w/ “Lion”
w/o prompt"
FINDINGS AND RESULTS,0.40404040404040403,"Figure 4: Examples of image edition using top-3 latent basis vectors. Each column is edited
using a different latent vector {v1, v2, v3}. Each group of columns represents edits made at different
diffusion timesteps (0.6T and 0.7T). Notably, when given the “Lion"" prompt, it is evident that all the
top latent basis vectors align with the direction of the prompt."
IMAGE EDITING WITH THE LATENT BASIS,0.4090909090909091,"4.1
Image editing with the latent basis"
IMAGE EDITING WITH THE LATENT BASIS,0.41414141414141414,"In this subsection, we demonstrate the capability of our discovered latent basis for image editing.
To extract the latent variables from real images for editing purposes, we use DDIM inversion. In
experiments with Stable Diffusion (SD), we do not use guidance, i.e., unconditional sampling, for
both DDIM inversion and DDIM sampling processes. This ensures that our editing results solely
depend on the latent variable, not on other factors such as prompt conditions."
IMAGE EDITING WITH THE LATENT BASIS,0.41919191919191917,"Real image
“Tiger”
“Bear”"
IMAGE EDITING WITH THE LATENT BASIS,0.42424242424242425,“Running dog”
IMAGE EDITING WITH THE LATENT BASIS,0.4292929292929293,“Sitting dog”
IMAGE EDITING WITH THE LATENT BASIS,0.43434343434343436,“Lion”
IMAGE EDITING WITH THE LATENT BASIS,0.4393939393939394,“Running cat”
IMAGE EDITING WITH THE LATENT BASIS,0.4444444444444444,“Standing cat”
IMAGE EDITING WITH THE LATENT BASIS,0.4494949494949495,"Figure 5: Examples of image edition using latent basis vectors discovered with various prompts.
Each column is edited using the latent basis vector obtained from a different text prompt. Importantly,
our method employs each prompt only once to derive the local latent basis."
IMAGE EDITING WITH THE LATENT BASIS,0.45454545454545453,"Figures 2 and 3 illustrate the example results edited by the latent basis found by our method. The
latent basis clearly contains semantics such as age, gender, species, structure, and texture. Note that
editing at timestep T yields coarse changes such as age and species. On the other hand, editing at
timestep 0.6T leads to fine changes, such as nose color and facial expression."
IMAGE EDITING WITH THE LATENT BASIS,0.4595959595959596,"Figure 4 demonstrates the example results edited by the various latent basis vectors. Interestingly,
using the text “lion” as a condition, the entire latent basis captures lion-related attributes. Furthermore,
Figure 5 shows that the latent basis aligns with the text not only in terms of object types but also
in relation to pose or action. For a qualitative comparison with other state-of-the-art image editing
methods, refer to Appendix D. For more examples of editing results, refer to Appendix G."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.46464646464646464,"4.2
Evolution of latent structures during generative processes"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.4696969696969697,"Figure 6: Power Spectral Density (PSD)
of latent basis. The PSD at t = T (pur-
ple) exhibits a greater proportion of low-
frequency signals, while the PSD at smaller
t (beige) reveals a larger proportion of high-
frequency signals. The latent vectors vi are
min-max normalized for visual clarity."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.47474747474747475,"In this subsection, we demonstrate how the latent struc-
ture evolves during the generative process and identify
three trends. 1) The frequency domain of the latent ba-
sis changes from low to high frequency. It agrees with
the previous observation that DMs generate samples in
coarse-to-fine manner. 2) The difference between the
tangent spaces of different samples increases over the
generative process. It implies finding generally appli-
cable editing direction in latent space becomes harder
in later timesteps. 3) The differences of tangent spaces
between timesteps depend on the complexity of the
dataset."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.4797979797979798,"Latent bases gradually evolve from low- to high-
frequency structures.
Figure 6 is the power spectral
density (PSD) of the discovered latent basis over var-
ious timesteps. The early timesteps contain a larger
portion of low frequency than the later timesteps and
the later timesteps contain a larger portion of high frequency."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.48484848484848486,"This suggests that the model focuses on low-frequency signals at the beginning of the generative
process and then shifts its focus to high-frequency signals over time. This result strengthens the
common understanding about the coarse-to-fine behavior of DMs over the generative process [12, 15]."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.4898989898989899,"The discrepancy of tangent spaces from different samples increases along the generative process.
To investigate the geometry of the tangent basis, we employ a metric on the Grassmannian manifold."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.494949494949495,"The Grassmannian manifold is a manifold where each point is a vector space, and the metric defined
above represents the distortion across various vector spaces. We use geodesic metric [10, 56] to define
the discrepancy between two subspaces {T (1), T (2)}:"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5,"Dgeo(T (1), T (2)) =
sX"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5050505050505051,"k
θ2
k,
(5)"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.51010101010101,"Figure 7: Geodesic distance across tan-
gent space of different samples at various
diffusion timesteps. Each point represents
the average geodesic distance between pairs
of 15 samples. It is notable that the similar-
ity of tangent spaces among different sam-
ples diminishes as the generative process
progresses."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5151515151515151,where θk denotes the k-th principle angle between T (1)
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5202020202020202,"and T (2). Intuitively, the concept of geodesic metric can
be understood as an angle between two vector spaces.
Here, the comparison between two different spaces was
conducted for {Th1, Th2}. Unlike the X, the H assumes
a Euclidean space which makes the computation of
geodesic metric that requires an inner product between
tangent spaces easier. The relationship between tangent
space and latent subspace is covered in more detail in
Appendix E."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5252525252525253,"Figure 7 demonstrates that the tangent spaces of the
different samples are the most similar at t = T and
diverge as timestep becomes zero."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5303030303030303,"Moreover, the similarity across tangent spaces allows
us to effectively transfer the latent basis from one sam-
ple to another through parallel transport as shown in
Figure 8. In T, where the tangent spaces are homo-
geneous, we consistently obtain semantically aligned
editing results. On the other hand, parallel transport at
t = 0.6T does not lead to satisfactory editing because
the tangent spaces are hardly homogeneous. Thus, we
should examine the similarity of local subspaces to
ensure consistent editing across samples."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5353535353535354,"DMs trained on simpler datasets exhibit more consistent tangent spaces over time.
In Figure 9
(a), we provide a distance matrix of the tangent spaces across different timesteps, measured by the
geodesic metric. We observe that the tangent spaces are more similar to each other when a model
is trained on CelebA-HQ, compared to ImageNet. To verify this trend, we measure the geodesic
distances between tangent spaces of different timesteps and plot the average distances of the same"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5404040404040404,"Real image
(Man)
Real image
(Woman)
Real image
(Red lip) P.T P.T"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5454545454545454,"Figure 8: Examples of image edition using parallel transport. The first row demonstrates the
results of editing with their respective latent vectors, while the subsequent rows exhibit the results
of editing through the parallel transport (P.T) of the latent vectors used in the first row. The latent
vector performs effectively when t = T (left and middle), but comparatively less satisfactorily for
0.6T (right)."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5505050505050505,"(b)
(a)"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5555555555555556,CelebA-HQ Max 2/3 1/3 0
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5606060606060606,ImageNet
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5656565656565656,"𝑇
0.1𝑇
𝑇
0.1𝑇 0.1𝑇 𝑇"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5707070707070707,"0
0.25𝑇
0.5𝑇
0.75𝑇"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5757575757575758,"ImageNet
LSUN-horse
LSUN-cat
LSUN-bedroom
LSUN-church
FFHQ*
Flowers*
CelebA-HQ |Δ𝑇| 0 2 4 6 8 10"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5808080808080808,"Figure 9: Simpler datasets lead to more similar tangent spaces across diffusion timesteps. (a)
Distance matrix visualization of tangent space measured by geodesic metric across various timesteps.
(b) Average geodesic distance based on timestep differences, indicating that the complexity of the
dataset correlates with greater distances between tangent spaces."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5858585858585859,"0
0.2
0.4
0.6
0.8"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5909090909090909,"Theoretical Upper Bound
Random Basis"
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.5959595959595959,"Figure 10: Similar prompts create similar tangent spaces, and the impact of the prompt
decreases as the generative process progresses. (a) The horizontal axis represents the CLIP
similarity between two different prompts, while the vertical axis represents the geodesic distance
in the tangent space from each prompt. Different colors represent various diffusion timesteps. A
negative relationship is observed between prompt similarity and tangent space distance. (b) The
R2 score of the linear regression between clip similarity and geodesic distance of tangent spaces
decreases throughout the generative process. (c) Each point represents the distance between tangent
spaces created from different prompts. Until around t = 0.7T, the distance between tangent spaces is
very large, but it gradually decreases thereafter. This indicates that the influence of the prompt on the
tangent space diminishes."
EVOLUTION OF LATENT STRUCTURES DURING GENERATIVE PROCESSES,0.601010101010101,"difference in timestep in Figure 9 (b). As expected, we find that DMs trained on datasets, that are
generally considered simpler, have similar local tangent spaces over time."
EFFECT OF CONDITIONING PROMPTS ON THE LATENT STRUCTURE,0.6060606060606061,"4.3
Effect of conditioning prompts on the latent structure"
EFFECT OF CONDITIONING PROMPTS ON THE LATENT STRUCTURE,0.6111111111111112,"In this subsection, we aim to investigate how prompts influence the generative process from a
geometrical perspective. We randomly sampled 50 captions from the MS-COCO dataset [29] and
used them as text conditions."
EFFECT OF CONDITIONING PROMPTS ON THE LATENT STRUCTURE,0.6161616161616161,"Similar text conditions induce similar tangent spaces.
In Figure 10 (a), we observe a negative
correlation between the CLIP similarity of texts and the distance between tangent spaces. In other
words, when provided with similar texts, the tangent spaces are more similar. The linear relationship
between the text and the discrepancy of the tangent spaces is particularly strong in the early phase of
the generative process as shown by R2 score in Figure 10 (b)."
EFFECT OF CONDITIONING PROMPTS ON THE LATENT STRUCTURE,0.6212121212121212,"The generative process depends less on text conditions in later timesteps.
Figure 10 (c) illustrates
the distances between local tangent spaces for given different prompts with respect to the timesteps."
EFFECT OF CONDITIONING PROMPTS ON THE LATENT STRUCTURE,0.6262626262626263,"Real image
(Beard)
Real image"
EFFECT OF CONDITIONING PROMPTS ON THE LATENT STRUCTURE,0.6313131313131313,"(a) Entanglement of semantic directions
(b) Abrupt changes P.T"
EFFECT OF CONDITIONING PROMPTS ON THE LATENT STRUCTURE,0.6363636363636364,"Figure 11: Limitations. (a) Entanglement between attributes due to dataset biases (b) Abrupt changes
in Stable Diffusion."
EFFECT OF CONDITIONING PROMPTS ON THE LATENT STRUCTURE,0.6414141414141414,"Notably, as the diffusion timestep approaches values below 0.7T, the distances between the local
tangent spaces start to decrease. It implies that the variation due to walking along the local tangent
basis depends less on the text conditions, i.e., the text less influences the generative process, in later
timesteps. It is a possible reason why the correlation between the similarity of prompts and the
similarity of tangent spaces reduces over timesteps."
DISCUSSION,0.6464646464646465,"5
Discussion"
DISCUSSION,0.6515151515151515,"In this section, we provide additional intuitions and implications. It is interesting that our latent basis
usually conveys disentangled attributes even though we do not adopt attribute annotation to enforce
disentanglement. We suppose that decomposing the Jacobian of the encoder in the U-Nets naturally
yields disentanglement to some extent. However, it does not guarantee the perfect disentanglement
and some directions are entangled. For example, the editing for beard converts a female subject to a
male as shown in Figure 11 (a). This kind of entanglement often occurs in other editing methods due
to the dataset bias: female faces seldom have beard."
DISCUSSION,0.6565656565656566,"While our method has shown effectiveness in Stable Diffusion, more research is needed to fully
validate its potential. We have observed that some of the discovered latent vector occasionally leads
to abrupt changes during the editing process in Stable Diffusion, as depicted in Figure 11 (b). This
observation highlights the complex geometry of X in achieving seamless editing. Exploring this topic
in future research is an interesting area to delve into."
DISCUSSION,0.6616161616161617,"Our approach is broadly applicable when the feature space in the DM adheres to a Euclidean metric,
as demonstrated by H. This characteristic has been observed in the context of U-Net within Kwon
et al. [26]. It would be intriguing to investigate if other architectural designs, especially those similar
to transformer structures as introduced in [42, 53], also exhibit a Euclidean metric."
DISCUSSION,0.6666666666666666,"Despite these limitations, our method provides a significant advance in the field of image editing for
DMs, and provides a deep understanding of DM through several experiments."
CONCLUSION,0.6717171717171717,"6
Conclusion"
CONCLUSION,0.6767676767676768,"We have analyzed the latent space of DMs from a geometrical perspective. We used the pullback metric
to identify the latent and tangent bases in X and H. The latent basis found by the pullback metric
allows editing images by traversal along the basis. We have observed properties of the bases in two
aspects. First, we discovered that 1) the latent bases evolve from low- to high-frequency components;
2) the discrepancy of tangent spaces from different samples increases along the generative process;
and 3) DMs trained on simpler datasets exhibit more consistent tangent spaces over timesteps. Second,
we investigated how the latent structure changes based on the text conditions in Stable Diffusion,
and discovered that similar prompts make tangent space analogous but its effect becomes weaker
over timesteps. We believe that a better understanding of the geometry of DMs will open up new
possibilities for adopting DMs in useful applications."
ACKNOWLEDGEMENT,0.6818181818181818,"7
Acknowledgement"
ACKNOWLEDGEMENT,0.6868686868686869,"This work was supported in part by the Creative-Pioneering Researchers Program through
Seoul National University, the National Research Foundation of Korea (NRF) grant (Grant No.
2022R1A2C1006871) (J. J.), KIAS Individual Grant [AP087501] via the Center for AI and Natural
Sciences at Korea Institute for Advanced Study, and the National Research Foundation of Korea
(NRF) grant (RS-2023-00223062)."
REFERENCES,0.6919191919191919,References
REFERENCES,0.696969696969697,"[1] Rameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka. Styleflow: Attribute-conditioned
exploration of stylegan-generated images using conditional continuous normalizing flows. ACM
Transactions on Graphics (ToG), 40(3):1–21, 2021."
REFERENCES,0.702020202020202,"[2] Georgios Arvanitidis, Lars Kai Hansen, and Søren Hauberg. Latent space oddity: on the
curvature of deep generative models. arXiv preprint arXiv:1710.11379, 2017."
REFERENCES,0.7070707070707071,"[3] Georgios Arvanitidis, Søren Hauberg, and Bernhard Schölkopf. Geometrically enriched latent
spaces. arXiv preprint arXiv:2008.00565, 2020."
REFERENCES,0.7121212121212122,"[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of
natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 18208–18218, 2022."
REFERENCES,0.7171717171717171,"[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika
Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models
with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022."
REFERENCES,0.7222222222222222,"[6] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko.
Label-efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126,
2021."
REFERENCES,0.7272727272727273,"[7] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow
image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 18392–18402, 2023."
REFERENCES,0.7323232323232324,"[8] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or.
Attend-and-
excite: Attention-based semantic guidance for text-to-image diffusion models. arXiv preprint
arXiv:2301.13826, 2023."
REFERENCES,0.7373737373737373,"[9] Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, and Patrick Smagt.
Metrics for deep generative models. In International Conference on Artificial Intelligence and
Statistics, pages 1540–1550. PMLR, 2018."
REFERENCES,0.7424242424242424,"[10] Jaewoong Choi, Junho Lee, Changyeon Yoon, Jung Ho Park, Geonho Hwang, and Myungjoo
Kang. Do not escape from the manifold: Discovering the local coordinates on the latent space
of gans. arXiv preprint arXiv:2106.06959, 2021."
REFERENCES,0.7474747474747475,"[11] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon.
Ilvr: Conditioning method for denoising diffusion probabilistic models.
arXiv preprint
arXiv:2108.02938, 2021."
REFERENCES,0.7525252525252525,"[12] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh
Yoon. Perception prioritized training of diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 11472–11481, 2022."
REFERENCES,0.7575757575757576,"[13] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image
synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 8188–8197, 2020."
REFERENCES,0.7626262626262627,"[14] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models
for inverse problems using manifold constraints. arXiv preprint arXiv:2206.00941, 2022."
REFERENCES,0.7676767676767676,"[15] Giannis Daras and Alexandros G Dimakis. Multiresolution textual inversion. arXiv preprint
arXiv:2211.17115, 2022."
REFERENCES,0.7727272727272727,"[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009."
REFERENCES,0.7777777777777778,"[17] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.
Advances in Neural Information Processing Systems, 34:8780–8794, 2021."
REFERENCES,0.7828282828282829,"[18] Gene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013."
REFERENCES,0.7878787878787878,"[19] René Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, and Tomer Michaeli. Discover-
ing interpretable directions in the semantic latent space of diffusion models. arXiv preprint
arXiv:2303.11073, 2023."
REFERENCES,0.7929292929292929,"[20] Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering
interpretable gan controls. Advances in Neural Information Processing Systems, 33:9841–9850,
2020."
REFERENCES,0.797979797979798,"[21] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.
Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,
2022."
REFERENCES,0.803030303030303,"[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
in Neural Information Processing Systems, 33:6840–6851, 2020."
REFERENCES,0.8080808080808081,"[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. In International Conference on Learning Representations,
2018."
REFERENCES,0.8131313131313131,"[24] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 4401–4410, 2019."
REFERENCES,0.8181818181818182,"[25] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022."
REFERENCES,0.8232323232323232,"[26] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic
latent space. arXiv preprint arXiv:2210.10960, 2022."
REFERENCES,0.8282828282828283,"[27] Yonghyeon Lee and Frank C Park. On explicit curvature regularization in deep generative
models. 2023."
REFERENCES,0.8333333333333334,"[28] Yonghyeon Lee, Seungyeon Kim, Jinwon Choi, and Frank Park. A statistical manifold frame-
work for point cloud data. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,
Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Ma-
chine Learning, volume 162 of Proceedings of Machine Learning Research, pages 12378–12402.
PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lee22d.html."
REFERENCES,0.8383838383838383,"[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13, pages 740–755. Springer, 2014."
REFERENCES,0.8434343434343434,"[30] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu,
Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis
with semantic diffusion guidance. arXiv preprint arXiv:2112.05744, 2021."
REFERENCES,0.8484848484848485,"[31] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc
Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11461–11471,
2022."
REFERENCES,0.8535353535353535,"[32] Chaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Jinxiang Liu, Yu Wang, Ya Zhang, and Yanfeng
Wang. Diffusionseg: Adapting diffusion towards unsupervised object discovery. arXiv preprint
arXiv:2303.09813, 2023."
REFERENCES,0.8585858585858586,"[33] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.
Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073, 2021."
REFERENCES,0.8636363636363636,"[34] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018."
REFERENCES,0.8686868686868687,"[35] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion
for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022."
REFERENCES,0.8737373737373737,"[36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing
with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021."
REFERENCES,0.8787878787878788,"[37] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic
models. In International Conference on Machine Learning, pages 8162–8171. PMLR, 2021."
REFERENCES,0.8838383838383839,"[38] Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian
Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold.
In ACM SIGGRAPH 2023 Conference Proceedings, pages 1–11, 2023."
REFERENCES,0.8888888888888888,"[39] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu.
Zero-shot image-to-image translation. arXiv preprint arXiv:2302.03027, 2023."
REFERENCES,0.8939393939393939,"[40] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017."
REFERENCES,0.898989898989899,"[41] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip:
Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 2085–2094, 2021."
REFERENCES,0.9040404040404041,"[42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023."
REFERENCES,0.9090909090909091,"[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning,
pages 8748–8763. PMLR, 2021."
REFERENCES,0.9141414141414141,"[44] Aditya Ramesh, Youngduck Choi, and Yann LeCun. A spectral regularizer for unsupervised
disentanglement. arXiv preprint arXiv:1812.01161, 2018."
REFERENCES,0.9191919191919192,"[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022."
REFERENCES,0.9242424242424242,"[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022."
REFERENCES,0.9292929292929293,"[47] Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, and Cristian Canton. Generating
high fidelity data from low-density regions using diffusion models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11492–11501,
2022."
REFERENCES,0.9343434343434344,"[48] Hang Shao, Abhishek Kumar, and P Thomas Fletcher. The riemannian geometry of deep
generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 315–323, 2018."
REFERENCES,0.9393939393939394,"[49] Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
1532–1540, 2021."
REFERENCES,0.9444444444444444,"[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In International Conference on Machine
Learning, pages 2256–2265. PMLR, 2015."
REFERENCES,0.9494949494949495,"[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020."
REFERENCES,0.9545454545454546,"[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
preprint arXiv:2011.13456, 2020."
REFERENCES,0.9595959595959596,"[53] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano.
Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022."
REFERENCES,0.9646464646464646,"[54] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features
for text-driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022."
REFERENCES,0.9696969696969697,"[55] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello.
Open-vocabulary panoptic segmentation with text-to-image diffusion models. arXiv preprint
arXiv:2303.04803, 2023."
REFERENCES,0.9747474747474747,"[56] Ke Ye and Lek-Heng Lim. Schubert varieties and distances between subspaces of different
dimensions. SIAM Journal on Matrix Analysis and Applications, 37(3):1176–1197, 2016."
REFERENCES,0.9797979797979798,"[57] LEE Yonghyeon, Sangwoong Yoon, MinJun Son, and Frank C Park. Regularized autoencoders
for isometric representation learning. In International Conference on Learning Representations,
2021."
REFERENCES,0.9848484848484849,"[58] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015."
REFERENCES,0.98989898989899,"[59] O˘guz Kaan Yüksel, Enis Simsar, Ezgi Gülperi Er, and Pinar Yanardag. Latentclr: A contrastive
learning approach for unsupervised discovery of interpretable directions. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 14263–14272, 2021."
REFERENCES,0.9949494949494949,"[60] Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zheng-Jun Zha, Jingren Zhou, and Qifeng
Chen. Low-rank subspaces in gans. Advances in Neural Information Processing Systems, 34:
16648–16658, 2021."
