Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003937007874015748,"Graph Neural Networks (GNNs) have received extensive affirmation for their
promising performance in graph learning problems. Despite their various neural
architectures, most are intrinsically graph filters that provide theoretical foundations
for model explanations. In particular, low-pass filters show superiority in label
prediction in many benchmarks. However, recent empirical research suggests
that models with only low-pass filters do not always perform well. Although
increasing attempts to understand graph filters, it is unclear how a particular
graph affects the performance of different filters. In this paper, we carry out
a comprehensive theoretical analysis of the synergy of graph structure and node
features on graph filters’ behaviors in node classification, relying on the introduction
of interaction probability and frequency distribution. We show that the homophily
degree of graphs significantly affects the prediction error of graph filters. Our
theory provides a guideline for graph filters design in a data-driven manner. Since
it is hard for a single graph filter to live up to this, we propose a general strategy
for exploring a data-specified filter bank. Experimental results show that our
model achieves consistent and significant performance improvements across all
benchmarks. Furthermore, we empirically validate our theoretical analysis and
explain the behavior of baselines and our model."
INTRODUCTION,0.007874015748031496,"1
INTRODUCTION"
INTRODUCTION,0.011811023622047244,"Graph Neural Networks (GNNs) have continuously attracted interest as their promising performance
in various graph learning problems. It is known that most of GNNs are intrinsically graph filters
(Kipf & Welling, 2017; Defferrard et al., 2016; Ortega et al., 2018; Nt & Maehara, 2019). With the
theoretical foundation of filters, there is an increasing attempt at model explanation, e.g. explaining
the behavior of various GNNs in node classification. Nt & Maehara (2019) investigated the superiority
of low-pass filters backed up with theoretical arguments while recent research (Balcilar et al., 2020;
Chang et al., 2020; Bo et al., 2021) empirically revealed the weakness of GNNs with only low-pass
filters in certain datasets. These contradictory views on low-pass filters pose a significant problem:
Why does a filter work on one dataset but not on another? More precisely, for a given filter, what
kinds of structure and features are useful for prediction? This makes it clear to us that in order to solve
this problem, it is necessary to take into account graph information, including the graph structure,
features, and labels."
INTRODUCTION,0.015748031496062992,"Existing theoretical research is mostly restricted to the investigation of filters themselves such as
exploring their expressive power (Oono & Suzuki, 2020; Balcilar et al., 2020), without considering
their inconsistency of performance on different graphs. It is clear that structural and feature informa-
tion lead to the possible inconsistency. However, there has been little explicit analysis of how graph
information influences the performance of graph filters. For instance, GNNs have formulated a variety
of graph filters in a heuristic manner under a suppressed homophily assumption, i.e., nodes with
similar attributes/labels tend to have connections. There remains a paucity of quantitative description
of homophily until Pei et al. (2020) designed a rough index to measure it."
INTRODUCTION,0.01968503937007874,"In this paper, we establish a comprehensive theoretical analysis of the effect of structure and feature
information on node label prediction to fill the gap and provide deep insights into the explanation
of graph filters. We first establish a systematic investigation on graphs with an indicator in terms of
homophily - the interaction probability and a distributional representation of input information - the"
INTRODUCTION,0.023622047244094488,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.027559055118110236,"frequency distribution. The interaction probability derived from random walk theory relates node
labels with its local topology and quantifies the degree of clustering of nodes in the same/different
class. We argue that interaction probability reflects the difficulty in identifying one class from others.
In terms of feature information, we draw on spectral analysis representing features as frequency
distributions. Furthermore, we consider the moment of frequency and build an explicit relation with
graph structure. Interestingly, we find that the moment of label frequency (noting that a one-hot
label vector can be regarded as a special node feature) is determined by interaction probability. The
aforementioned preparations underpin our deep understanding of graph filters."
INTRODUCTION,0.031496062992125984,"We validate the prediction error of a graph filter under two settings: a. fixed graph structure,
unravel the influence of input (original or transformed node features); b. given input, show how
structure matters, and provide analysis utilizing frequency distribution and interaction probability.
The main conclusions are: 1. given structure, the frequency response of an ideal graph filter should
be consistent with the main frequency band of label frequency, that is, a matched frequency response
is the premise of success; 2. given input, a graph filter essentially tunes the weight of edges - failing
to make a homophily degree large enough may cause an unsatisfactory prediction accuracy. These
interpretations of graph filters imply a data-driven filter design principle. In addition, we apply these
theoretical results to three types of filters - low-pass, high-pass, and band-pass filters with specified
form. It shows that a single graph filter is hard to comply with the principle of ideal filters, especially
when the homophily degree and label frequency distribution of different classes are very different. For
example, when frequency distributions of labels are far from each other, it is hard to find a single filter
whose frequency response can cover all the main frequency bands well. In this paper, we leverage a
combination of band-pass graph filters to overcome this problem and develop a simple yet effective
framework to show how to learn multiple filters depending on datasets. We empirically validate our
theoretical analysis and investigate structure and feature information of benchmarks. We verify our
model on a variety of datasets and explain the behavior of baselines and our model. Experimental
results show that our model achieves a consistent and significant performance improvement across all
benchmarks."
INTRODUCTION,0.03543307086614173,"Our main contributions are: 1.We develop a theoretical analysis of graph information based on the
introduction of interaction probability and frequency distribution; 2.We provide a deep understanding
of the performance of graph filters illustrating how graph structure and input information matter;
3.We indicate the weakness of GNNs with a single graph filter and propose a general framework to
learn a data-specified filter bank which contributes to significant improvement."
RELATED WORK,0.03937007874015748,"2
RELATED WORK"
RELATED WORK,0.04330708661417323,"In this paper, we focus on the analysis of graph filters in the context of graph neural networks.
Since Bruna et al. (2014) defined spectral graph filters and extended convolutional operations
to graphs, various spectral graph neural networks have been developed. For example, ChebNet
(Defferrard et al., 2016) defines the Chebyshev polynomial filter which can be exactly localized in
the k-hop neighborhood. Kipf & Welling (2017) simplified the Chebyshev filters using a first-order
approximation and derived the well-known graph convolutional networks (GCNs). Bianchi et al.
(2021) proposed the rational auto-regressive moving average graph filters (ARMA) which are more
powerful in modeling the localization and provide more flexible graph frequency response, however
more computationally expensive and also more unstable. Very recently, Min et al. (2020) augmented
conventional GCNs with geometric scattering transforms which enabled band-pass filtering of graph
signals and alleviated the oversmoothing issue. In addition, most graph neural networks originally
defined in the spatial domain are also found essentially connected to the spectral filtering (Balcilar
et al., 2020). By bridging the gap between spatial and spectral graph neural networks, Balcilar et al.
(2020) further investigated the expressiveness of all graph neural networks from their spectral analysis.
However, their analysis is limited to the spectrum coverage of a graph filter itself and lacks deeper
insights into the graph-dependent performance of these filters."
RELATED WORK,0.047244094488188976,"Another related topic is the measurement of graph homophily. Beyond the interaction probability that
we define in this paper, there are some other heuristic metrics for homophily. Pei et al. (2020) defined
a node homophily index to characterize their datasets and help explain their experimental results
for Geom_GCN: β =
1
#nodes
P
v
#neighbors of v that have the same label as v"
RELATED WORK,0.051181102362204724,"#neighbors of v
. Zhu et al. (2020) defined edge
homophily ratio instead and identified a set of key designs that can boost learning from the graph"
RELATED WORK,0.05511811023622047,Under review as a conference paper at ICLR 2022
RELATED WORK,0.05905511811023622,structure in heterophily: h = #edges whose end nodes have same labels
RELATED WORK,0.06299212598425197,"#edge
. This edge homophily definition is
sensitive to the number of classes and size of each class, and Lim et al. (2021) made a modification to
alleviate this problem. Our work differentiates from these works in that we not only use our definition
to characterize the graph but also directly relate it to the performance of graph filters (or GNNs)."
THEORETICAL ANALYSIS OF GRAPH INFORMATION,0.06692913385826772,"3
THEORETICAL ANALYSIS OF GRAPH INFORMATION"
NOTATION,0.07086614173228346,"3.1
NOTATION"
NOTATION,0.07480314960629922,"Let Gn
= (Vn, En) be an undirected graph with additional self-connection, where Vn
=
{v0, . . . , vn−1} is the set of nodes and En ⊂Vn × Vn is the set of edges. Let A ∈Rn×n be
the adjacency matrix and L = D −A be the Laplacian matrix, where D is a diagonal degree
matrix with Dii = P"
NOTATION,0.07874015748031496,j Aij. we denote ˜A = D−1
NOTATION,0.08267716535433071,2 AD−1
NOTATION,0.08661417322834646,"2 , then ˜L = D−1"
NOTATION,0.09055118110236221,2 LD−1
NOTATION,0.09448818897637795,"2 = I −˜A is the
symmetric normalized Laplacian. Let (λi, ui) be a pair of eigenvalue and unit eigenvector of ˜L,
where 0 = λ0 ≤· · · ≤λn−1 ≤2."
PROBLEM SETTING,0.0984251968503937,"3.2
PROBLEM SETTING"
PROBLEM SETTING,0.10236220472440945,"In this paper, we are mainly interested in node classification problems on undirected graphs. Given
Gn = (Vn, En), we consider T = {0, . . . , K −1} as the set of all node labels. For ∀k ∈T , we denote
Ck as the set of nodes with label k and R ∈RK×K as a size matrix which is a diagonal matrix with
Rk = |Ck|. Considering single-label problems in which classes are mutually exclusive, we use one-
hot encoding to indicate the class label and introduce a label matrix Y ∈Rn×K = (y0, . . . , yK−1)
to represent the labels of Vn, where yk is the indicator vector of Ck. Obviously, R = Y ⊤Y ,
Y ⊤1 = diag(R) and Y 1 = 1. A signal x on Gn can be arranged the signal values in a vector form
x = (x0, . . . , xn−1)⊤. Particularly, labels {yk|k ∈T } are also graph signals."
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.1062992125984252,"3.3
A STRUCTURE INDICATOR - INTERACTION PROBABILITY"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.11023622047244094,"Homophily of graphs is an implicit assumption widely leveraged in graph learning methods including
GNNs. It is considered an indisputable common property of most graphs, despite its descriptive and
unquantifiable definition, which introduces a variety of uncertainties. In this section, starting with the
random walk, we introduce interaction probability to overcome this challenge."
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.1141732283464567,"For a random walk on Gn, we denote P = D−1A as its transition matrix which is also a row Markov
matrix. From the random walk theory, P k is the k-step transition matrix, and P k
ij is the probability
that a random walker starting from node vi arrives at vj after k steps. For a node v and a class Cl,
we denote πk
i (Cl) as the probability that a random walker starting from vi stays in Cl at the k-th
step. It is trivial that πk
i (Cl) = P"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.11811023622047244,"j∈Cl P k
ij with P"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.1220472440944882,"l∈T πk
i (Cl) = 1. πk
i (Cl) demonstrates the relative
preference/closeness of node vi for Cl with k-scale. To meet the homophily assumption, for vi in Cl,
πk
i (Cl) is expected to gap away from others. Since πk
i (Cl) −P
m̸=l πk
i (Cm) = 2πk
i (Cl) −1, πk
i (Cl)
can be regarded as a measure of the k-scale homophily degree of node vi. Particularly, for ∀k ∈N
and vi ∈Cl, πk
i (Cl) = 1 means that Cl is a community and will never communicate with other classes.
However, this case is rare in real graphs. Below, we investigate the homophily of a class and propose
a method to measure the communication strength between two classes.
Definition 3.1 (k-step interaction probability). For l, m ∈T , we define Πk as the k-step interaction
probability matrix formulated as follows:"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.12598425196850394,"Πk
lm = 1 Rl X"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.12992125984251968,"vi∈Cl
πk
i (Cm) = 1 Rl X"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.13385826771653545,"vi∈Cl,vj∈Cm
P k
ij = y⊤
l P kym"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.1377952755905512,"y⊤
l yl
(1)"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.14173228346456693,"Πk = (Y ⊤Y )−1Y ⊤P kY = R−1Y ⊤P kY.
(2)"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.14566929133858267,"Πk
lm is the probability that a random walker from Cl arrives at Cm after k steps.
Remark 1. Obviously, Πk1 = 1.
Πk
lm is the mean proportion of Cm in the k-hop neigh-
bors of nodes from Cl.
Noting that rank(Y ) = K, when K ̸= n, Y R−1Y ⊤̸= I, thus
(R−1Y ⊤PY )k ̸= R−1Y ⊤P kY , i.e. (Π)m ̸= Πm. More generally, for an arbitrary polyno-
mial function g, R−1Y ⊤g(P)Y is likely not equal to g(R−1Y ⊤P kY ). In the rest of paper, we"
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.14960629921259844,Under review as a conference paper at ICLR 2022
A STRUCTURE INDICATOR - INTERACTION PROBABILITY,0.15354330708661418,"write ˜g(Π) = R−1Y ⊤g(P)Y and g(Π) = g(R−1Y ⊤P kY ). For instance, if g(·) = (·)m, then
˜g(Π) = Πm and g(Π) = (Π)m. Also, we denote Πk
ll, the self-interaction probability, as πk
l for short."
-STEP INTERACTION PROBABILITY INTUITIVELY REFLECTS THE DEGREE OF CLUSTERING OF TWO CLASSES AND PK,0.15748031496062992,"1-step interaction probability intuitively reflects the degree of clustering of two classes and Pk
i=1 Πi
measures the strength of interaction between classes in the scale of k steps. Since P is not symmetric,
Πk
lm ̸= Πk
ml. To facilitate analysis, here we propose a symmetric variant of interaction probability to
identify the interactions between two classes. We denote this symmetric k-step interaction probability
matrix as ˜
Πk, by replacing P with ˜A = D−1"
-STEP INTERACTION PROBABILITY INTUITIVELY REFLECTS THE DEGREE OF CLUSTERING OF TWO CLASSES AND PK,0.16141732283464566,2 AD−1
-STEP INTERACTION PROBABILITY INTUITIVELY REFLECTS THE DEGREE OF CLUSTERING OF TWO CLASSES AND PK,0.16535433070866143,"2 , we obtain ˜Πk = R−1"
-STEP INTERACTION PROBABILITY INTUITIVELY REFLECTS THE DEGREE OF CLUSTERING OF TWO CLASSES AND PK,0.16929133858267717,2 Y ⊤˜AkY R−1
-STEP INTERACTION PROBABILITY INTUITIVELY REFLECTS THE DEGREE OF CLUSTERING OF TWO CLASSES AND PK,0.1732283464566929,"2 . Below,
we investigate the important properties of Πk and ˜Πk.
Proposition 3.1. For l, m ∈T and an arbitrary polynomial function g(·), we have:
a. RlΠk
lm + RmΠk
ml ≥2√RlRm ˜Πk
lm, where Rl is the l-th diagonal element of R;
b. (˜g2(˜Π))ll ≥(˜g(˜Π)ll)2, where ˜gk(˜Π) = R−1"
-STEP INTERACTION PROBABILITY INTUITIVELY REFLECTS THE DEGREE OF CLUSTERING OF TWO CLASSES AND PK,0.17716535433070865,2 Y ⊤gk( ˜A)Y R−1 2 .
-STEP INTERACTION PROBABILITY INTUITIVELY REFLECTS THE DEGREE OF CLUSTERING OF TWO CLASSES AND PK,0.18110236220472442,"The proof can be found in Appendix B. Since ˜Πk1 ̸= 1, that is, the measure is no longer a probability
measure. However, according to Prop.3.1.a ( let m = l ), ˜πk
l is the lower bound of πk
l , and ˜πk
l = πk
l
when Gn is a regular graph. In the rest of theoretical analysis, we use ˜πk
l to measure the degree of
Cl’s clustering. Let g(·) = (·)k, from Prop.3.1.b, we have ˜π2k
l
≥(˜πk
l )2. In Section.4.2, we leverage
this inequality to derive a lower bound of our prediction error and further illustrate how structure
influences the performance of a given filter."
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.18503937007874016,"3.4
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION"
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.1889763779527559,"Following the graph signal processing (GSP) concepts, λ0, . . . , λn−1 are graph frequencies and
u0 . . . , un−1 are the corresponding frequency components which are invariant of graph filters.
Through Fourier transform, we obtain {αi = ⟨ui, x⟩|i = 0, . . . , n −1} the spectral representation of
a graph signal x, called graph signal spectrum. Moreover, a graph signal can be represented as a linear
combination of frequency components, i.e., x = P αiui. For a label vector yl which is also a graph
signal, we denote {γ0, . . . , γn−1} as its spectrum. There is an intuitive assumption: information
of label vectors is all we need for classification - we will validate this assumption in Section 4.1.
Under this context, γ2
i / P
i γ2
i reflects how much the frequency component uk contributes to the
distinctiveness of Cl, without considering the positivity and negativity of effects. Interestingly, we
find that the normalized signal spectrum is a histogram/discrete distribution defined below.
Definition 3.2 (Frequency distribution). We define f, the frequency of signal x, as a random variable
taking values in the set of graph frequencies with probability Pr(f = λk) = α2
k
 P
i α2
i . The
probability describes the frequency distribution of signal x."
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.19291338582677164,"With this definition, we derive distributional representations of signals from their spectral representa-
tions/spectra. One can evaluate the signal effect by comparing frequency distributions of signals and
label vectors under a specified distribution metric, such as Wasserstein distance. Below, we consider
the moment of frequency distribution to show how graph structure influences signal frequency."
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.1968503937007874,"Proposition 3.2. For G = {V, E}, let f be the frequency of signal x, then E[f n] = x⊤(I−˜
A)nx
x⊤x
."
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.20078740157480315,"The proof of this proposition can be found in Appendix B. With the definition of interaction probability,
we further represent the moment of the label vector’s frequency."
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.2047244094488189,"Corollary 3.3. For label frequency fl of yl, we have E[f n
l ] =
 
˜g(I −˜Π)
"
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.20866141732283464,ll with g = (·)n.
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.2125984251968504,Recall that ˜g(I −˜Π) = R−1
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.21653543307086615,2 Y ⊤(I −˜A)nY R−1
A FEATURE INDICATOR - FREQUENCY DISTRIBUTION,0.2204724409448819,"2 , we have E[fl] = 1 −˜πl, E[f 2
l ] = 1 −2˜πl + ˜π2
l
and the variance of fl: Var(fl) = ˜π2
l −(˜πl)2. It can be seen that both the mean and variance of
label frequency are close to 0 when ˜πl approaches 1, which reflects a high homophily degree (as
˜πl ≤πl ≤1). In Section 4.1, we conduct a more detailed analysis of feature information of spectral
space with frequency distribution."
ANALYSIS OF GRAPH FILTERS,0.22440944881889763,"4
ANALYSIS OF GRAPH FILTERS"
ANALYSIS OF GRAPH FILTERS,0.2283464566929134,"A graph filter is defined as a function g with applied Laplacian matrix or adjacency matrix. Denote
R[ ˜A] as a polynomial ring in ˜A over R, here we are mainly interested in g ∈R[ ˜A]. In this section, we"
ANALYSIS OF GRAPH FILTERS,0.23228346456692914,Under review as a conference paper at ICLR 2022
ANALYSIS OF GRAPH FILTERS,0.23622047244094488,"provide a deep understanding of the performance of graph filters concerning label prediction based
on the above theoretical analysis of graph information. In general, there are two major concerns:
with fixed graph structure, how does the input impact the performance of a given filter? and with
fixed input, how does graph structure impact the performance of a given filter?. In this section, we
provide the theoretical analysis of these two questions in Sections 4.1 and 4.2, respectively."
ANALYSIS OF GRAPH FILTERS,0.24015748031496062,"The general formulation of the l + 1-th layer of spectral GNNs is X(l+1) = σ(g( ˜A)X(l)W (l+1)),
here σ is an activation function, X(l) is the output of the l-th layer, X(0) is a feature matrix and
W (l+1) is a learnable transformation matrix. We call X(l)W (l+1) the input of g( ˜A) in l + 1-th
layer and denote X as the input of g( ˜A) in the last layer. In the following sections, we discuss
the prediction error of spectral GNNs with a given graph filter without activation function before
prediction. That is, in the last layer with X as input, g( ˜A)X is directly used for prediction.
Definition 4.1 (Prediction error). Let X ∈Rn×K be the input of graph filter g( ˜A), Y ∈Rn×K is
the label matrix, the prediction error is formulated by:"
ANALYSIS OF GRAPH FILTERS,0.2440944881889764,"Er(g, X) =∥g( ˜A)X −Y ∥2
F = tr(X⊤g2( ˜A)X) −2tr(X⊤g( ˜A)Y )+ ∥Y ∥2
F
(3)"
ANALYSIS OF GRAPH FILTERS,0.24803149606299213,"Remark 2. For a label vector yl, we denote Er(g, xl) =∥g( ˜A)xl −yl ∥2
F as the error of g( ˜A)
predicting class l. Obviously, Er(g, X) = P"
ANALYSIS OF GRAPH FILTERS,0.25196850393700787,"l∈T Er(g, xl), where xl is the l-th column of X."
ANALYSIS OF GRAPH FILTERS,0.2559055118110236,"In particular, we will apply our conclusion to specified filters and make concrete analysis.
Definition 4.2. With ϵ ∈[0, ϵ0] and ϵ′ ∈[−1, 1], ϵ0 is a small constant, we define low-pass filters
gl(ϵ)( ˜A), high-pass filters gh(ϵ)( ˜A) and band-pass filters gb(ϵ′)( ˜A) as:"
ANALYSIS OF GRAPH FILTERS,0.25984251968503935,"gl(ϵ)( ˜A) = ϵI + ˜A,
gh(ϵ)( ˜A) = ϵI −˜A,
gb(ϵ′)( ˜A) = I −(1 + |ϵ′|)−2(ϵ′I −˜A)2."
ANALYSIS OF GRAPH FILTERS,0.2637795275590551,"For λ, an eigenvalue of ˜L, we have gl(ϵ)(λ) ∈[ϵ −1, 1 + ϵ], gh(ϵ)(λ) ∈[ϵ −1, 1 + ϵ] and gb(ϵ′)(λ) ∈
[0, 1] since λ ∈[0, 2]. Particularly, gl(0) is the GCN filter."
HOW INPUT MATTERS,0.2677165354330709,"4.1
HOW INPUT MATTERS"
HOW INPUT MATTERS,0.27165354330708663,"Denote ˜X = U ⊤X = (˜x0, . . . , ˜xK−1) and ˜Y = U ⊤Y = (˜y0, . . . , ˜yK−1), where U is a matrix
with unit eigenvectors of ˜L (recall that eigenvectors of ˜A are consistent with that of ˜L), revisiting
Er(g, xl) and Er(g, yl) in spectral domain, we have:"
HOW INPUT MATTERS,0.2755905511811024,"Er(g, xl) = ∥g(I −Λ)˜xl −˜yl ∥2
F =
X"
HOW INPUT MATTERS,0.2795275590551181,"i
(g(1 −λi)αi −γi)2
(4)"
HOW INPUT MATTERS,0.28346456692913385,"Er(g, yl) =
X"
HOW INPUT MATTERS,0.2874015748031496,"i
γ2(1 −g(1 −λi))2 = Rl
X"
HOW INPUT MATTERS,0.29133858267716534,"i
pi(1 −g(1 −λi))2 = RlE[1 −g(1 −fl)]2
(5)"
HOW INPUT MATTERS,0.2952755905511811,"where Λ is the eigenvalue matrix of ˜L, αi and γi are the spectra of xl and yl respectively, pi = Pr(fl =
λi), fl is the frequency of yl. For better comparison, we normalize the input xl: ∥xl ∥2
F =∥yl ∥2
F ,
i.e., P α2
i = P γ2
i . g is re-scaled function with g([0, 2]) concentrating in [−1, 1]."
HOW INPUT MATTERS,0.2992125984251969,"How input information matter? With normalized feature and graph filters, it indicates that the
performance of graph filters greatly depends on label spectra. Particularly, when the frequency
response of a graph filter does not fit the label frequency, it might be inferior to all-pass filters,
such as MLP. On the other hand, it poses a principle of filter design: make feature response of
filters be consistent with the main frequency band of label frequency as much as possible. In terms
of input information, it determines the performance of a filter - if the frequency distribution of
input vector is far from that of label vector, even an ideal filter would fail. This observation is
identical to our assumption in Section 3.4 - information of label vector is all we need and the distance
between frequency distribution of input and label vectors reflects its usefulness. Therefore, Er(g, yl)
is the lower bound of Er(g, xl) when g( ˜A) are given. While an input vector may be useful for
distinguishing one class, it may be helpless for another. In most GNNs, they tune the frequency
distribution of features with a learnable linear transformation to generate a more informative input."
HOW INPUT MATTERS,0.3031496062992126,"Here, we discuss the Er(g, yl) of three types of filters:
Er(gl(ϵ), yl)/Rl = Var(fl −ϵ) + E[fl −ϵ]2 = Var(fl) + (E[fl] −ϵ)2
(6)"
HOW INPUT MATTERS,0.30708661417322836,"Er(gh(ϵ), yl)/Rl = Var(2 −fl −ϵ) + E[2 −fl −ϵ]2 = Var(fl) + (E[fl] + ϵ −2)2
(7)"
HOW INPUT MATTERS,0.3110236220472441,Under review as a conference paper at ICLR 2022
HOW INPUT MATTERS,0.31496062992125984,"Er(gb(ϵ′), yl)/Rl ≈(E[fl] + ϵ′ −1)4 + 6Var(fl)(E[fl] + ϵ′ −1)2 + 8(1 −ϵ′)Var(fl)E[fl]"
HOW INPUT MATTERS,0.3188976377952756,"(1 + |ϵ′|)4
.
(8)"
HOW INPUT MATTERS,0.3228346456692913,"where we use Var(f 2
l ) ≈4E[fl]2Var(fl) derived from the delta method."
HOW INPUT MATTERS,0.32677165354330706,"Discussion. An interesting observation is that for a class with high dispersive spectrum, efforts of any
single filters are to no avail. From Corollary 3.3, we know that E[fl] = 1−˜πl and Var(fl) = ˜π2
l −(˜πl)2.
It demonstrates that higher homophily means lower E[fl], lower Var(fl), and also lower prediction
error for low-pass filters. On the other hand, we indicate that, in most cases, band-pass filters are
more powerful than low-pass filters, let alone high-pass filters. However, the prediction capacity of a
signal filter is very limited when the means of spectra vary widely."
HOW STRUCTURE MATTERS,0.33070866141732286,"4.2
HOW STRUCTURE MATTERS"
HOW STRUCTURE MATTERS,0.3346456692913386,"Above, we catch a glimpse of spectral explanation of the behavior of graph filters. Below, we expand
more understanding of graph filters. Assume that with learnable transformation, GNNs enable to
generate an informative input. Here we discuss the prediction error of different graph filters under the
optimal input Y . We revisit Er(g, yl) using symmetric interaction matrix and propose a lower bound
er(g, yl) leveraging Proposition 3.1:"
HOW STRUCTURE MATTERS,0.33858267716535434,"Er(g, yl) = y⊤
l (I −g( ˜A))2yl = Rl(I −2˜g(˜Π) + ˜g2(˜Π))ll ≥er(g, yl) = Rl(I −˜g(˜Π)ll)2. (9)"
HOW STRUCTURE MATTERS,0.3425196850393701,"How structural information matters? We indicate that, in the spatial point of view, graph filters
can be interpreted as weight-tuning mechanisms on edges. The lower bound clearly demonstrates
that a graph filter would have unsatisfactory prediction accuracy if it fails to make the homophily
degree of the tuned graph large enough (g[˜Π]ll are far from 1)."
HOW STRUCTURE MATTERS,0.3464566929133858,"Applying the prediction error lower bound to aforementioned specified filters, we have:"
HOW STRUCTURE MATTERS,0.35039370078740156,"er(gl(ϵ), yl) = (1 −˜πl −ϵ)2Rl;
er(gh(ϵ), yl) = (1 + ˜πl −ϵ)2Rl
(10)"
HOW STRUCTURE MATTERS,0.3543307086614173,"er(gb(ϵ′), yl) = (1 + |ϵ′|)−4Rl(ϵ′2 −2ϵ′˜πl + ˜π2
l )2 ≥(1 + |ϵ′|)−4Rl(ϵ′ −˜πl)4.
(11)"
HOW STRUCTURE MATTERS,0.35826771653543305,"Discussion. These error bounds indicate that: 1. a low-pass filter would fail on classes with low
homophily degree - in turn, it confirms that the importance of homophily assumption for low-pass
filters like GCN - it is identical with our spectral point of view; 2. high-pass filters have poor
performances particularly on the high homophily graphs; 3. for a graph whose classes have consistent
homophily degree (their self-interaction probabilities concentrate around a constant ¯ϵ), gb(¯ϵ) would
work better than others. However, it is predictable that any single filters would fail on graphs with
diverse self-interaction probabilities."
MODEL AND EMPIRICAL STUDY,0.36220472440944884,"5
MODEL AND EMPIRICAL STUDY"
MODEL AND EMPIRICAL STUDY,0.3661417322834646,"Our theoretical analysis of graph information demonstrates that: 1. when node classes have inconsis-
tent homophily degree or their label frequency distribution are far from each other, a single graph filter
is prone to fail; 2. in most cases, band-pass filters would perform better than low-pass and high-pass
filters; 3. a feature may contribute to the classification of one class but hinder the discrimination of
another. Inspired by these, we propose a disentangled multi band-pass filter framework (DEMUF)
which can be applied to any type of graphs no matter what kinds of graph information they have.
The key point of our model is to learn multi band-pass filters which are used to capture different
disentangled feature information respectively."
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.3700787401574803,"5.1
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.37401574803149606,"Our framework includes feature disentanglement and frequency filtering. As we have emphasized
the limitations of single filters, it is natural to leverage multi graph filters. Theoretically, piling up
sufficient numbers of graph filters to capture all the frequency components can improve prediction
performance. However, it is very expensive. To avoid this problem, we consider feature disentan-
glement - essentially, it is to disentangle frequency distributions of features into different families.
Features in the same family are expected to have similar spectral properties, that is, they have similar
frequency distributions or have overlap on their main frequency bands. Then for each family, we apply"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.3779527559055118,Under review as a conference paper at ICLR 2022 𝐻 𝑀𝐿𝑃 𝑋 𝐹ଵ
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.38188976377952755,"𝜔ଵ𝐻ଵ
𝜔ே𝐻ே
…"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.3858267716535433,"𝐹ଶ
𝐹ேିଵ
𝐹ே"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.38976377952755903,"𝐻ଵ
𝐻ଶ
𝐻ேିଵ
𝐻ே
… 𝐷𝐸"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.3937007874015748,"𝜔ଵ𝐻ଵ
𝜔ே𝐻ே … 𝑋 𝐻ଶ
𝐻ଵ 𝐹ଵ …"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.39763779527559057,𝐷𝐸ଷ|𝐹ଷ
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.4015748031496063,𝐷𝐸ଶ|𝐹ଶ 𝑀𝐿𝑃 𝐻 𝐷𝐸ଵ = 𝑋௜
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.40551181102362205,𝑔௕ሺఢ೔ሻ 𝐹௜ =
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.4094488188976378,𝐷𝐸௜|𝐹௜ 𝐹௜ 𝐷𝐸௜ 𝐷𝐸௜ = Φ௜𝑋𝑖
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.41338582677165353,"Plain‐DEMUF
Tree‐DEMUF
DISENTANGLE BLOCK
& FILTER BLOCK"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.41732283464566927,"Figure 1: Illustration of Plain-DEMUF and Tree-DEMUF. There are two main model blocks of
DEMUF frameworks: disentangle block and filter block. In Plain-DEMUF, all filter blocks run in
parallel as their disentangled input are generated through a single disentangle block at the same time.
Differently, each Tree-DEMUF layer contains two branches - one is early stopped while the other
will be disentangled into two branches of the next layer after going through a filter."
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.421259842519685,"a band-pass graph filter to capture their main frequency components. We propose two frameworks
with different structures of filters: Plain-DEMUF and Tree-DEMUF (depicted in Fig. 1)."
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.4251968503937008,The DISENTANGLE block and FILTER block are formulated as follows:
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.42913385826771655,"Xk = DISENTANGLE(X, Φk) = Φk(X), Hk = FILTER

Xk, ϵk, hk

= (gb(ϵk))hkXk.
(12)"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.4330708661417323,"In our implementation, we provide two samples of DISENTANGLE functions Φk: one is linear
transformations, the other is GUMBEL_SOFTMAX (Jang et al., 2017) used to generate learnable masks
for feature selection. In terms of the FILTER block, we use the band-pass filter defined in Definition
4.2, i.e., gb(ϵ) = I −(1 + |ϵ|)−2( ˜A −ϵI)2 as the identical filter form. Here, ϵ is the parameter of
filter constrained in [−1, 1] noting that 1 −ϵ is the center of frequency response gb(ϵ). In each FILTER
block, h is the number of layers. The framework of Plain-DEMUF with N graph filters is:"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.43700787401574803,"H = MLP

CONCAT
n
FILTER

DISENTANGLE

X, Φk

, ϵk, hk

, ωk
k = 1, . . . , N
o
."
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.4409448818897638,"Based on this, we implement a simple model called P-DEMUF. Precisely, we leverage a GUM-
BEL_SOFTMAX to generate N learnable masks {M1, . . . , MN} for feature sampling at once followed
by different MLP. That is, Φk(X) = MLPk(X ⊙Mk)."
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.4448818897637795,"Similarly, we develop a model, T-DEMUF, under the framework of Tree-DEMUF formulated by:"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.44881889763779526,"H1, X1 = FILTER
n
DISENTANGLE

X, Φ1

, ϵ, h

,

DISENTANGLE

X, Ψ1

, ϵ1, h1
o"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.452755905511811,"Hk+1, Xk+1 =
n
DISENTANGLE

Xk, Φk

, FILTER

DISENTANGLE

Xk, Ψk

, ϵk, hk
o"
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.4566929133858268,"H = MLP

CONCAT
n
ωkHk, k = 1, . . . , N
o
."
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.46062992125984253,"In each T-DEMUF layer, we use GUMBEL_SOFTMAX with different parameters to generate two
masks Mk and M ′
k and Φk(Xk) = Xk ⊙Mk and Ψk(Xk) = Xk ⊙M ′
k. In each layer, we stop
further disentangling of the branch of Hk by utilizing an additional constraint L(Xk−1, Hk) =∥
Xk−1 ⊙M ′
k −Hk ∥2
2. Noting that Hk = (gb(ϵk))hkXk−1 ⊙M ′
k, this constraint is to make the main
frequency bands of Hk be consistent with frequency response of (gb(ϵk))hk."
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.4645669291338583,"Model discussion. Compared with filter-bank learning methods which directly apply an array of
filters to features, our models use subsets of features. It can greatly reduce the amount of computation
and parameters and help learning filters more efficiently and effectively. In addition, T-DEMUF uses
an additional constraint to guide the filter learning process while P-DEMUF is a combination of multi
graph neural networks which would not interfere with each other. Therefore, P-DEMUF is likely to
obtain similar filters and require more filters to improve performance than T-DEMUF. The model
visualization results in Fig. 2 validate this statement."
ARCHITECTURE OF TWO FRAMEWORKS OF DEMUF,0.468503937007874,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.47244094488188976,"5.2
EXPERIMENTS"
EXPERIMENTS,0.4763779527559055,"To validate DEMUF, we compare the performances of P-DEMUF and T-DEMUF with that of spectral
GNNs, spatial GNNs and MLP on extensive datasets."
EXPERIMENT SETTINGS,0.48031496062992124,"5.2.1
EXPERIMENT SETTINGS"
EXPERIMENT SETTINGS,0.484251968503937,"Datasets. We use four types of real datasets - Citation network, WebKB, Actor co-occurrence network
and Wikipedia network, to validate our proposed models. Cora and Citeseer (Sen et al., 2008) are
widely used citation benchmarks which represent paper as nodes and citation between two papers
as edges. Cornell, Texas, and Wisconsin (Pei et al., 2020) are three subgraphs of WebKB which is
a webpage network with web pages as nodes and hyperlinks between them as edges. Chameleon
and Squirrel (Rozemberczki et al., 2021) are two Wikipedia networks with web pages as nodes
and links between pages as edges. The nodes originally have five classes while Bo et al. (2021)
proposed a new classification criteria which divides nodes into three main categories. In this paper,
the relabeled networks are called Chameleon2 and Squirrel2. Actor (Tang et al., 2009) is a subgraph
of the fillm-director-actor-writer network whose nodes only represent actors and edges represent their
collaborations. For all data, we use 60% nodes for training, 20% for validation and 20% for testing.
To intuitively show the homophily degree of a dataset, we calculate the mean of self-interaction
probability (diagonal of interaction probability matrix) and show it in Table 1. This metric is similar
to the node homophily in (Pei et al., 2020). More statistics of datasets can be found in Appendix A."
EXPERIMENT SETTINGS,0.4881889763779528,"Baselines. We compare our models with four spectral GNNs: GCN (Kipf & Welling, 2017),
ChebNet (Defferrard et al., 2016), GIN (Xu et al., 2019) (despite a spatial GNN, we can easily get its
spectral form), ARMA (Bianchi et al., 2021). We list their spectral filter forms in Appendix A. In
short, GCN is a well-known low-pass filter. The filter shape of GIN depends on its parameter ϵ. In
this paper, we fix ϵ = 0.3 and thus it is also a low-pass filter. ChebNet and ARMA are high-order
polynomial filters. In addition, we also add three spatial GNNs (whose spectral forms are hardly
analyzed): GAT (Veliˇckovi´c et al., 2018), FAGCN (Bo et al., 2021), Geom_GCN (Pei et al., 2020).
Both GAT and FAGCN utilize attention mechanism and FAGCN takes high frequency information
into account. Geom_GCN is a novel aggregation method based on the geometry of graph (it is related
because it was also empirically studied on graphs with different levels of homophily (Pei et al., 2020)).
Finally, we also compare with MLP, a baseline without using any graph information."
EXPERIMENT SETTINGS,0.4921259842519685,"Experimental Setup. For all experiments, we report the mean prediction accuracy on the testing
data for 10 runs. We search learning rate, hidden unit, weight decay and dropout for all models in
the same search space. Finally, we choose learning rate of 0.01, dropout rate of 0.5, and hidden unit
of 32 over all datasets. The number of filters are searched between 2 to 10, and the final setting is:
for T-DEMUF, we use 4 filters with 7 layers for Citation networks, 2 filters with 15 layers for all
WebKB and Wikipedia networks, 5 filters with 1 layer for Actor. The numbers of MLP layers are 2,
2, 3 and 4, respectively. P-DEMUF uses: 3 filters with 8 layers for Citation networks; 5 filters for
Cornell, 4 filters for Wisconsin and 3 filters for Texas - all of them are 1 layer; 7 filters with 9 layers
for WebKB; 5 filters with 2 layers for Actor. P-DEMUF applies 2-layer MLP to all benchmarks. In
addition, as the setting of benchmarks are the same as that in Geom_GCN, we refer to the results
reported in Pei et al. (2020)."
RESULT AND ANALYSIS,0.49606299212598426,"5.3
RESULT AND ANALYSIS"
RESULT AND ANALYSIS,0.5,"The experimental results are summarized in Table 1. Our models consistently outperform baselines
over most benchmarks with significant improvement. On Cora and Citeseer, the datasets with a high
level of homophily, our models are only comparable to GCN and other baselines. However, on all
other datasets with a lower level of homophily, our models both obtained great performance gain."
RESULT AND ANALYSIS,0.5039370078740157,"To understand the impact of graph homophily on different types of graph filters, let us analyze the
performance of all spectral GNNs. On high-homophily datasets, all GNNs perform similarly and
the accuracy is much higher than MLP. That means the graph structure information is extremely
useful in this case. However, on low-homophily datasets, many of them are even worse than MLP.
GCN and GIN, the two low-pass filter based models, perform worst. The two GNNs with high-order
graph filters, ChebNet and ARMA, are clearly superior to other models due to their higher spectrum
coverage. However, they cannot beat our models with specially designed multiple filters. The"
RESULT AND ANALYSIS,0.5078740157480315,Under review as a conference paper at ICLR 2022
RESULT AND ANALYSIS,0.5118110236220472,Table 1: Node classification accuracy. The first row is the mean of self-interaction probability.
RESULT AND ANALYSIS,0.515748031496063,"Cora
Cite.
Cornell Texas Wisc. Cham.
Squi.
Cham.2 Squi.2 Actor
#mean-πi
0.861 0.809
0.436
0.356
0.413
0.338
0.290
0.516
0.425
0.393"
RESULT AND ANALYSIS,0.5196850393700787,Spectral
RESULT AND ANALYSIS,0.5236220472440944,"GCN
88.5
76.2
54.05
57.84
51.37
41.23
27.95
66.93
57.12
28.05
Cheb.
88.21 76.26
80.00
78.38
78.43
51.71
36.52
75.44
66.11
35.76
GIN
87.06
74.1
55.68
52.97
49.02
36.58
23.73
44.74
52.22
26.30
ARMA
87.56 74.86
71.35
75.68
75.29
52.54
36.56
76.14
66.78
35.27"
RESULT AND ANALYSIS,0.5275590551181102,Spatial
RESULT AND ANALYSIS,0.531496062992126,"GAT
88.32 76.85
55.14
61.08
54.51
47.46
32.66
70.92
61.42
29.32
FAGCN
89.19 77.15
73.51
65.41
76.86
49.82
33.68
74.47
65.86
34.61
Geom_GCN 85.27
77.9
60.81
67.57
64.12
60.90
38.14
73.20
63.30
31.63
MLP
75.33
71.4
80.00
80.00
84.31
49.56
34.89
77.28
63.19
36.38 Ours"
RESULT AND ANALYSIS,0.5354330708661418,"T-DEMUF
86.72 74.57
86.15
87.83
85.31
69.52
56.47
81.89
70.66
37.53
P-DEMUF
87.85 75.69
86.49
89.73
89.68
69.46
56.45
79.17
68.87
37.68
↓1.34 ↓2.21
↑6.49
↑9.73 ↑4.37
↑8.62
↑18.33
↑4.61
↑3.88
↑1.30"
RESULT AND ANALYSIS,0.5393700787401575,"0
1
2
3
4
5
6 0 1 2 3 4 5 6"
RESULT AND ANALYSIS,0.5433070866141733,Interaction Probability Label 0.80 0.83 0.94 0.88 0.89 0.84 0.84 Cora
RESULT AND ANALYSIS,0.547244094488189,"0.00
0.50
1.00
1.50 0 1 2 3 4 5 6"
RESULT AND ANALYSIS,0.5511811023622047,Frequency Distribution Label
RESULT AND ANALYSIS,0.5551181102362205,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.5590551181102362,P-DEMUF g( )
RESULT AND ANALYSIS,0.562992125984252,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.5669291338582677,T-DEMUF g( )
RESULT AND ANALYSIS,0.5708661417322834,"0
1
2
3
4 0 1 2 3 4 0.40 0.56 0.40 0.32 0.39"
RESULT AND ANALYSIS,0.5748031496062992,Wisconsin
RESULT AND ANALYSIS,0.5787401574803149,"0.00
0.50
1.00
1.50 0 1 2 3 4"
RESULT AND ANALYSIS,0.5826771653543307,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.5866141732283464,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.5905511811023622,"0
1
2
3
4 0 1 2 3 4 0.23 0.24 0.27 0.31 0.39"
RESULT AND ANALYSIS,0.594488188976378,Squirrel
RESULT AND ANALYSIS,0.5984251968503937,"0.00
0.50
1.00
1.50 0 1 2 3 4"
RESULT AND ANALYSIS,0.6023622047244095,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.6062992125984252,"0
1
2
0.0 0.5 1.0 0
1
2 0 1 2 0.21 0.38 0.68"
RESULT AND ANALYSIS,0.610236220472441,Squirrel2
RESULT AND ANALYSIS,0.6141732283464567,"0.00
0.50
1.00
1.50 0 1 2"
RESULT AND ANALYSIS,0.6181102362204725,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.6220472440944882,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.6259842519685039,"0
1
2
3
4 0 1 2 3 4 0.32 0.36 0.39 0.46 0.43 Actor 0.0 0.2 0.4 0.6 0.8 1.0"
RESULT AND ANALYSIS,0.6299212598425197,"0.00
0.50
1.00
1.50 0 1 2 3 4 0.00 0.02 0.04 0.06 0.08 0.10"
RESULT AND ANALYSIS,0.6338582677165354,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.6377952755905512,"0
1
2
0.0 0.5 1.0"
RESULT AND ANALYSIS,0.6417322834645669,"Figure 2: Visualizations of interaction probability matrix and frequency distribution of five datasets
here and filters gh
b(1−ϵ) learnt by P-DEMUF and T-DEMUF, where the thickness of curve reflects its
weight ω. It shows that frequency responses of our filters are consistent with labels frequency."
RESULT AND ANALYSIS,0.6456692913385826,"reason might be that the high complexity of their filters makes it more difficult to learn one optimal
single filter. Finally, our model T-DEMUF yields over 18% higher accuracy than the best baselines
(Geom_GCN) on Squirrel; and P-DEMUF yields almost 10% higher accuracy than MLP on Texas"
RESULT AND ANALYSIS,0.6496062992125984,"In addition, we select some typical datasets and show the frequency distribution on these graphs in
Fig. 2. We can obviously see that on Cora the spectrum is focused on low frequency components.
This can explain why the low-pass filter based models can also perform well on it. On other datasets,
the frequency distribution is more diverse, so the low-pass filters can not match with the important
frequency components anymore. In contrast, both of our models, T-DEMUF and P-DEMUF, learn
graph filters corresponding well to those components (as shown in the last two rows of Fig. 2). T-
DEMUF uses fewer number of (more dispersed) filters but achieves comparable or better performance."
CONCLUSION,0.6535433070866141,"6
CONCLUSION"
CONCLUSION,0.65748031496063,"In this paper, we propose a theoretical analysis of graph information with the introduction of
interaction probability and frequency distribution. We develop a deep understanding of how different
structures and input influence the performance of graph filters. We also design a simple framework to
learn a filter bank. Empirical results on extensive datasets validate the power of our model."
CONCLUSION,0.6614173228346457,Under review as a conference paper at ICLR 2022
REFERENCES,0.6653543307086615,REFERENCES
REFERENCES,0.6692913385826772,"Muhammet Balcilar, Guillaume Renton, Pierre Héroux, Benoit Gaüzère, Sébastien Adam, and Paul
Honeine. Analyzing the expressive power of graph neural networks in a spectral perspective. In
International Conference on Learning Representations, 2020."
REFERENCES,0.6732283464566929,"Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. Graph neural networks
with convolutional arma filters. IEEE Transactions on Pattern Analysis and Machine Intelligence,
2021."
REFERENCES,0.6771653543307087,"Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph
convolutional networks. In Thirty-Fifth AAAI Conference on Artificial Intelligence, pp. 3950–3957,
2021."
REFERENCES,0.6811023622047244,"Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and deep locally
connected networks on graphs. In International Conference on Learning Representations, 2014."
REFERENCES,0.6850393700787402,"Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Somayeh Sojoudi, Junzhou Huang, and
Wenwu Zhu. Spectral graph attention network. arXiv preprint arXiv:2003.07450, 2020."
REFERENCES,0.6889763779527559,"Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank
graph neural network. In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.
net/forum?id=n6jl7fLxrP."
REFERENCES,0.6929133858267716,"Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems,
pp. 3844–3852, 2016."
REFERENCES,0.6968503937007874,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
International Conference on Learning Representations, 2017."
REFERENCES,0.7007874015748031,"Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations, 2017."
REFERENCES,0.7047244094488189,"Derek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. New benchmarks for learning on non-
homophilous graphs. arXiv preprint arXiv:2104.01404, 2021."
REFERENCES,0.7086614173228346,"Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in graph
convolutional networks. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.7125984251968503,"Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
arXiv preprint arXiv:1905.09550, 2019."
REFERENCES,0.7165354330708661,"Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations, 2020."
REFERENCES,0.7204724409448819,"Antonio Ortega, Pascal Frossard, Jelena Kovacevic, José M. F. Moura, and Pierre Vandergheynst.
Graph signal processing: Overview, challenges, and applications. Proc. IEEE, 106(5):808–828,
2018. doi: 10.1109/JPROC.2018.2820126. URL https://doi.org/10.1109/JPROC.
2018.2820126."
REFERENCES,0.7244094488188977,"Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In International Conference on Learning Representations, 2020."
REFERENCES,0.7283464566929134,"Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
of Complex Networks, 9(2), 2021."
REFERENCES,0.7322834645669292,"Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI Magazine, 29(3):93–93, 2008."
REFERENCES,0.7362204724409449,"Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks. In
Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 807–816, 2009."
REFERENCES,0.7401574803149606,Under review as a conference paper at ICLR 2022
REFERENCES,0.7440944881889764,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.7480314960629921,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019."
REFERENCES,0.7519685039370079,"Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond
homophily in graph neural networks: Current limitations and effective designs. In Advances in
Neural Information Processing Systems, 2020."
REFERENCES,0.7559055118110236,"A
BENCHMARKS AND MODEL DISCUSSION"
REFERENCES,0.7598425196850394,"A.1
STATISTICS INFORMATION OF BENCHMARKS."
REFERENCES,0.7637795275590551,We provide statistics information of our benchmarks in Table. A.1.
REFERENCES,0.7677165354330708,"Table 2: Datasets statistics.
Dataset
Cora
Cite.
Cornell
Texas
Wisc.
Cham.
Squi.
Cham.2
Squi.2
Actor
# Nodes
2708
3327
183
183
251
2277
5201
2277
5201
7600
# Edges
5429
4732
295
309
499
36101
217073
36101
217073
33544
# Features
1433
3703
1703
1703
1703
2325
2089
2325
2089
931
# Classes
7
6
5
5
5
5
5
3
3
5"
REFERENCES,0.7716535433070866,"A.2
SPECTRAL FILTERS."
REFERENCES,0.7755905511811023,"In our paper, we use four spectral GNN as baselines whose spectral filters are listed as Table.A.2 and
define a band-pass filter gb(ϵ) as a quadratic function with respect to the adjacency matrix."
REFERENCES,0.7795275590551181,Table 3: Spectral filters.
REFERENCES,0.7834645669291339,"Model
Filter
GCN
˜A
GIN
A + (I + ϵ)I"
REFERENCES,0.7874015748031497,"ChebNet
C(s) = 2C(2)C(s−1) −C(s−2);
C(2) = 2L/λmax −I; C1 = I
ARMA
(I + PK
k=1 qkLk)−1(PK−1
k=0 pkLk)
Ours
I −(1 + |ϵ|)−2( ˜A −ϵI)2"
REFERENCES,0.7913385826771654,"Since gb(ϵ)( ˜A) = I −(1+|ϵ′|)−2(ϵI −˜A)2 = (1+|ϵ|)−2((1+|ϵ|−ϵ)I +A)((1+|ϵ|+ϵ)I −A), it is
exactly an overlap between a low-pass filter (1+|ϵ|−ϵ)I+A) and a high-pass filter ((1+|ϵ|+ϵ)I−A).
That is why gb(ϵ) is a band-pass filter."
REFERENCES,0.7952755905511811,"A.3
MODEL DISCUSSION."
REFERENCES,0.7992125984251969,"A.3.1
MOTIVATION OF DISENTANGLEMENT."
REFERENCES,0.8031496062992126,"Overlap if not disentangled. Without disentanglement, it is highly possible that the learned
filters have large overlaps if we do not induce any constraints on them. In our algorithm, we
aim to train filters to capture the main frequency information of their input and assign different
weights to that captured information depending on how much they contribute to label prediction.
Therefore, it is natural to assume that if the input of filters is different, filters are less likely to overlap.
Disentanglement can make the “input” different and more adaptable to each filter."
REFERENCES,0.8070866141732284,Under review as a conference paper at ICLR 2022
REFERENCES,0.8110236220472441,"Disentanglement reduces the model complexity. With disentanglement, we divide node features
into several subsets by learnable masking or map them into several low-dimensional spaces through
linear transformations which lowers the dimension of corresponding features for each filter and
meanwhile makes the input feature fits better to each filter."
REFERENCES,0.8149606299212598,"A.3.2
MOTIVATION OF T-DEMUF."
REFERENCES,0.8188976377952756,"As we clarified in Section 5.1, our implementation of disentanglement is not random masking but
learnable masking leveraging GUMBEL-SOFTMAX. These learnable maskings disentangle node
features into several subsets of features. With a constraint L(Xk−1, Hk) =∥Xk−1 ⊙M ′
k −Hk ∥2
2,
for each subset of features we train a band-pass filter to through their main frequency. As shown
in Figure.2, this constraint guides the process of filter learning which can help reduce the overlap
of filters’ frequency responses such that can reduce the number of graph filters. At the same time,
to minimize the supervised loss, maskings are trained to disentangle features whose frequency
distribution are similar to those of labels and assign a higher weight; those useless captured feature
information will be assigned a lower weight. Weights are also learnable."
REFERENCES,0.8228346456692913,"A.3.3
HOW CAN OUR FILTER BANK SELECTION BE DATA-DRIVEN."
REFERENCES,0.8267716535433071,"In our algorithm, although the form of our band-pass filter gb(ϵ) is predefined, its parameters
including ϵ and weight ω are learned from specified graphs. Moreover, the parameters of our feature
disentanglement blocks (the linear transformations and learnable masking) are also learned from data
which will affect the learning of the filter bank. Therefore, our filter bank selection is data-driven."
REFERENCES,0.8307086614173228,"A.4
MORE EXPERIMENTAL RESULTS."
REFERENCES,0.8346456692913385,"A.4.1
ADDITIONAL BASELINE - GPRGNN."
REFERENCES,0.8385826771653543,"Here we compare our models with a related baseline GPRGNN(Chien et al., 2021). It is worth noting
that the splitting in our paper is different from that in GPRGNN. In GPRGNN, its training set consists
of the same number of nodes from each class while we just randomly choose our training data. We
find that WebKB datasets are sensitive to the way of splitting due to their uneven distribution of
labels ( the numbers of classes are: Cornell and Texas: 33/1/18/101/30, Wisconsin: 10/70/118/32/21).
Although GPRGNN’s splitting is likely better for model training, our model still outperforms it in the
Wikipedia datasets, i.e., Chameleon and Squirrel. In GPRGNN’s setting, the performances of MLP
on WebKB are comparable to GPRGNNs’ while in our setting, our proposed models’ performances
are much better than MLPs’. We also test T-DEMUF on Actor, Cornell, and Texas following the
splitting of GPRGNN. As shown in Table.A.4.1, in most of the benchmarks, our model performs
better than GPRGNN."
REFERENCES,0.84251968503937,Table 4: Node classification accuracy of P-DEMUF and GPRGNN.
REFERENCES,0.8464566929133859,"Different Splitting
GPRGNN Splitting
Dataset
Cham.
Squi.
Cornell
texas
actor
GPRGNN
69.52
49.93
91.36
92.92
39.30
T-DEMUF
67.48
56.47
91.26
92.79
41.11
↑2.04
↑6.54
↓0.1
↓0.13
↑1.81"
REFERENCES,0.8503937007874016,"A.4.2
ABLATION STUDY."
REFERENCES,0.8543307086614174,"To show the advantage of using disentanglement, we provide an ablation study on five benchmarks.
Here, we propose two ablation models based on P-DEMUF. Recall that the disentanglement block
of P-DEMUF consists of learnable masking and linear transformations, we design our ablation
models by taking off the component of masking and linear transformation. Also, for fair and intuitive
comparison, we simply fix the number of filters as 2. The results shown as Table.A.4.2 validate
that if we take off the disentanglement blocks of P-DEMUF, the results become worse in most of
benchmarks."
REFERENCES,0.8582677165354331,Under review as a conference paper at ICLR 2022
REFERENCES,0.8622047244094488,"Table 5: Node classification accuracy of P-DEMUF and its ablation models. We fix the number of
band-pass filters as 2."
REFERENCES,0.8661417322834646,"Dataset
Cornell
Texas
Cham.
Squi.
Actor
P-DEMUF
81.08
85.14
68.46
55.45
36.95
P-DEMUF (w/o masking )
80.00
84.86
67.20
53.54
37.20
P-DEMUF (w/o masking & linear)
77.30
86.49
62.5
44.19
36.71"
REFERENCES,0.8700787401574803,"B
PROOF OF PROPOSITION"
REFERENCES,0.8740157480314961,"Here, we provide the proof of Proposition 3.2."
REFERENCES,0.8779527559055118,"Proof. Since x =
n−1
P"
REFERENCES,0.8818897637795275,"i=0
αiui, ui is the i-th unit eigenvector of ˜L and λn = u⊤
i ˜Lnui then we have"
REFERENCES,0.8858267716535433,"E[f n] = n−1
X"
REFERENCES,0.889763779527559,"i=0
P(f = λi)λn
i =
P(αiui)⊤˜Ln(αiui)
P α2
i
= x⊤˜Lnx"
REFERENCES,0.8937007874015748,"x⊤x
= x⊤(I −˜A)nx"
REFERENCES,0.8976377952755905,"x⊤x
.
(13)"
REFERENCES,0.9015748031496063,Below is the proof of Proposition 3.1.
REFERENCES,0.905511811023622,Proof. For P = D−1A and ˜A = D−1
REFERENCES,0.9094488188976378,2 AD−1
REFERENCES,0.9133858267716536,"2 , and Π, ˜Π defined by Definition 3.1, the inequality
can be represented as (RΠk + (Πk)⊤R)lm ≥2(R
1
2 ˜ΠkR
1
2 )lm which is equivalent to prove y⊤
m(P k +
(P k)⊤)yl ≥2y⊤
m ˜Akyl. Noting that, (P k)⊤= DP kD−1 and ˜Ak = D
1
2 P kD−1"
REFERENCES,0.9173228346456693,"2 , with B ="
REFERENCES,0.9212598425196851,"P k + (P k)⊤, we have Bij = P k
ij + di"
REFERENCES,0.9251968503937008,"dj P k
ij ≥2
q"
REFERENCES,0.9291338582677166,"di
dj P k
ij = 2 ˜Ak
ij. Therefore, y⊤
m(B −2 ˜Ak)yl ≥0."
REFERENCES,0.9330708661417323,"Let m = l, then we get πk
l ≥˜πk
l ."
REFERENCES,0.937007874015748,"To prove proposition b, we utilize lemma B.1. Since g( ˜A) is symmetric, then we have"
REFERENCES,0.9409448818897638,(g2[˜Π])ll = y⊤(g( ˜A))2y
REFERENCES,0.9448818897637795,"y⊤y
≥
y⊤g( ˜A)y y⊤y"
REFERENCES,0.9488188976377953,"2
= (g[˜Π]ll)2."
REFERENCES,0.952755905511811,"Lemma B.1. Let B ∈Rn×n is a symmetric matrix, ∀ij, y ∈Rn, we have y⊤B2y"
REFERENCES,0.9566929133858267,"y⊤y
≥
y⊤By y⊤y 2
."
REFERENCES,0.9606299212598425,"Proof. Since B is symmetric, then we have B = UΛU ⊤, here U is matrix of unit eigenvectors"
REFERENCES,0.9645669291338582,"of B. From the proof of Proposition 3.2, we obtain that y⊤B2y"
REFERENCES,0.968503937007874,"y⊤y
=
P(αiλi)2"
REFERENCES,0.9724409448818898,"P α2
i
and

y⊤By"
REFERENCES,0.9763779527559056,"y⊤y
2
="
REFERENCES,0.9803149606299213,"(P α2
i λi)2"
REFERENCES,0.984251968503937,"(P α2
i )2 . From Hölder’s inequality, we have (P(αiλi)2)(P α2
i ) ≥(P α2
i λi)2. Therefore, we"
REFERENCES,0.9881889763779528,"have
P(αiλi)2"
REFERENCES,0.9921259842519685,"P α2
i
≥(P α2
i λi)2"
REFERENCES,0.9960629921259843,"(P α2
i )2 ."
