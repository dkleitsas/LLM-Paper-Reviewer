Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010256410256410256,"Recently, generative graph models have shown promising results in learning graph
1"
ABSTRACT,0.0020512820512820513,"representations through self-supervised methods. However, most existing gener-
2"
ABSTRACT,0.003076923076923077,"ative graph representation learning (GRL) approaches rely on random masking
3"
ABSTRACT,0.0041025641025641026,"across the entire graph, which overlooks the entanglement of learned representa-
4"
ABSTRACT,0.005128205128205128,"tions. This oversight results in non-robustness and a lack of explainability. Fur-
5"
ABSTRACT,0.006153846153846154,"thermore, disentangling the learned representations remains a significant challenge
6"
ABSTRACT,0.0071794871794871795,"and has not been sufficiently explored in GRL research. Based on these insights,
7"
ABSTRACT,0.008205128205128205,"this paper introduces DiGGR (Disentangled Generative Graph Representation
8"
ABSTRACT,0.009230769230769232,"Learning), a self-supervised learning framework. DiGGR aims to learn latent
9"
ABSTRACT,0.010256410256410256,"disentangled factors and utilizes them to guide graph mask modeling, thereby
10"
ABSTRACT,0.011282051282051283,"enhancing the disentanglement of learned representations and enabling end-to-end
11"
ABSTRACT,0.012307692307692308,"joint learning. Extensive experiments on 11 public datasets for two different graph
12"
ABSTRACT,0.013333333333333334,"learning tasks demonstrate that DiGGR consistently outperforms many previous
13"
ABSTRACT,0.014358974358974359,"self-supervised methods, verifying the effectiveness of the proposed approach.
14"
INTRODUCTION,0.015384615384615385,"1
Introduction
15"
INTRODUCTION,0.01641025641025641,"Self-supervised learning (SSL) has received much attention due to its appealing capacity for learning
16"
INTRODUCTION,0.017435897435897435,"data representation without label supervision. While contrastive SSL approaches are becoming
17"
INTRODUCTION,0.018461538461538463,"increasingly utilized on images [Chen et al., 2020] and graphs [You et al., 2020], generative SSL has
18"
INTRODUCTION,0.019487179487179488,"been gaining significance, driven by groundbreaking practices such as BERT for language [Devlin
19"
INTRODUCTION,0.020512820512820513,"et al., 2018], BEiT [Bao et al., 2021], and MAE [He et al., 2022a] for images. Along this line, there is
20"
INTRODUCTION,0.021538461538461538,"a growing interest in constructing generative SSL models for other modalities, such as graph masked
21"
INTRODUCTION,0.022564102564102566,"autoencoders (GMAE). Generally, the fundamental concept of GMAE [Tan et al., 2022] is to utilize
22"
INTRODUCTION,0.02358974358974359,"an autoencoder architecture to reconstruct input node features, structures, or both, which are randomly
23"
INTRODUCTION,0.024615384615384615,"masked before the encoding step. Recently, various well-designed GMAEs have emerged , achieving
24"
INTRODUCTION,0.02564102564102564,"remarkable results in both node classification and graph classification [Hou et al., 2022, Tu et al.,
25"
INTRODUCTION,0.02666666666666667,"2023, Tian et al., 2023].
26"
INTRODUCTION,0.027692307692307693,"Despite their significant achievements, most GMAE approaches typically treat the entire graph as
27"
INTRODUCTION,0.028717948717948718,"holistic, ignoring the graph’s latent structure. As a result, the representation learned for a node tends
28"
INTRODUCTION,0.029743589743589743,"to encapsulate the node’s neighborhood as a perceptual whole, disregarding the nuanced distinctions
29"
INTRODUCTION,0.03076923076923077,"between different parts of the neighborhood [Ma et al., 2019, Li et al., 2021, Mo et al., 2023].
30"
INTRODUCTION,0.031794871794871796,"For example, in a social network G, individual n is a member of both a mathematics group and
31"
INTRODUCTION,0.03282051282051282,"several sports interest groups. Due to the diversity of these different communities, she may exhibit
32"
INTRODUCTION,0.033846153846153845,"different characteristics when interacting with members from various communities. Specifically,
33"
INTRODUCTION,0.03487179487179487,"the information about the mathematics group may be related to her professional research, while the
34"
INTRODUCTION,0.035897435897435895,"information about sports clubs may be associated with her hobbies. However, the existing approach
35"
INTRODUCTION,0.036923076923076927,"overlooks the heterogeneous factors of node n, failing to identify and disentangle these pieces of
36"
INTRODUCTION,0.03794871794871795,Factors
INTRODUCTION,0.038974358974358976,Factors
INTRODUCTION,0.04,Factors
INTRODUCTION,0.041025641025641026,Factors
INTRODUCTION,0.04205128205128205,Factors
INTRODUCTION,0.043076923076923075,Factors
INTRODUCTION,0.0441025641025641,Factors
INTRODUCTION,0.04512820512820513,Factors
INTRODUCTION,0.046153846153846156,Factors
INTRODUCTION,0.04717948717948718,Factors
INTRODUCTION,0.048205128205128206,Factors
INTRODUCTION,0.04923076923076923,Factors
INTRODUCTION,0.050256410256410255,Factors
INTRODUCTION,0.05128205128205128,Factors
INTRODUCTION,0.052307692307692305,"(a) Applying previous methods to GMAE 
(b) Latent factor learning"
INTRODUCTION,0.05333333333333334,Factor Analysis
INTRODUCTION,0.05435897435897436,Probability Bar
INTRODUCTION,0.055384615384615386,Maximum Probability
INTRODUCTION,0.05641025641025641,Different node types
INTRODUCTION,0.057435897435897436,"Figure 1: The number of latent factors is set to 4. In Fig. 1(a), the probabilities of nodes belonging
to different latent groups are similar, resulting in nodes of the same type being incorrectly assigned to
different factors. In contrast, Fig. 1(b) shows that the probabilities of node-factor affiliation are more
discriminative, correctly categorizing nodes of the same type into the same latent group."
INTRODUCTION,0.05846153846153846,"information effectively [Hou et al., 2022]. Consequently, the learned features may be easily influenced
37"
INTRODUCTION,0.059487179487179485,"by irrelevant factors, resulting in poor robustness and difficulty in interpretation.
38"
INTRODUCTION,0.06051282051282051,"To alleviate the challenge described above, there is an increasing interest in disentangled graph
39"
INTRODUCTION,0.06153846153846154,"representation learning [Bengio et al., 2013, Li et al., 2021, Ma et al., 2019, Mo et al., 2023, Xiao
40"
INTRODUCTION,0.06256410256410257,"et al., 2022], which aims at acquiring representations that can disentangle the underlying explanatory
41"
INTRODUCTION,0.06358974358974359,"factors of variation in the graph. Specifically, many of these methods rely on a latent factor detection
42"
INTRODUCTION,0.06461538461538462,"module, which learns the latent factors of each node by comparing node representations with various
43"
INTRODUCTION,0.06564102564102564,"latent factor prototypes. By leveraging these acquired latent factors, these models adeptly capture
44"
INTRODUCTION,0.06666666666666667,"factor-wise graph representations, effectively encapsulating the latent structure of the graph. Despite
45"
INTRODUCTION,0.06769230769230769,"significant progress, few studies have endeavored to adapt these methods to to generative graph
46"
INTRODUCTION,0.06871794871794872,"representation learning methods, such as GMAE. This primary challenge arises from the difficulty of
47"
INTRODUCTION,0.06974358974358974,"achieving convergence in the latent factor detection module under the generative training target, thus
48"
INTRODUCTION,0.07076923076923076,"presenting obstacles in practical implementation. As shown in Fig.1(a), directly applying the previous
49"
INTRODUCTION,0.07179487179487179,"factor learning method to GMAE would make the factor learning module difficult to converge,
50"
INTRODUCTION,0.07282051282051281,"resulting in undistinguished probabilities and misallocation of similar nodes to different latent factor
51"
INTRODUCTION,0.07384615384615385,"groups.
52"
INTRODUCTION,0.07487179487179488,"To address these challenges, we introduce Disentangled Generative Graph Representation Learning
53"
INTRODUCTION,0.0758974358974359,"(DiGGR), a self-supervised graph generation representation learning framework. Generally speaking,
54"
INTRODUCTION,0.07692307692307693,"DiGGR learns how to generate graph structures from latent disentangle factors z and leverages this to
55"
INTRODUCTION,0.07794871794871795,"guide graph mask reconstruction, while enabling end-to-end joint learning. Specifically, i) To capture
56"
INTRODUCTION,0.07897435897435898,"the heterogeneous factors in the nodes, we introduce the latent factor learning module. This module
57"
INTRODUCTION,0.08,"models how edges and nodes are generated from latent factors, allowing graphs to be factorized into
58"
INTRODUCTION,0.08102564102564103,"multiple disentangled subgraphs. ii) To learn a deeper disentangled graph representation, we design a
59"
INTRODUCTION,0.08205128205128205,"factor-wise self-supervised graph representation learning framework. For each subgraph, we employ
60"
INTRODUCTION,0.08307692307692308,"a distinct masking strategy to learn an improved factor-specific graph representation. Evaluation
61"
INTRODUCTION,0.0841025641025641,"shows that the proposed framework can achieve significant performance enhancement on various
62"
INTRODUCTION,0.08512820512820513,"node and graph classification benchmarks.
63"
INTRODUCTION,0.08615384615384615,"The main contributions of this paper can be summarized as follows:
64"
INTRODUCTION,0.08717948717948718,"• We utilized the latent disentangled factor to guide mask modeling. A probabilistic graph
65"
INTRODUCTION,0.0882051282051282,"generation model is employed to identify the latent factors within a graph, and it can be
66"
INTRODUCTION,0.08923076923076922,"jointly trained with GMAE through variational inference.
67"
INTRODUCTION,0.09025641025641026,"• Introducing DiGGR (Disentangled Generative Graph Representation Learning) to further
68"
INTRODUCTION,0.09128205128205129,"capture the disentangled information in the latent factors, enhancing the disentanglement of
69"
INTRODUCTION,0.09230769230769231,"the learned node representations.
70"
INTRODUCTION,0.09333333333333334,"• Empirical results show that the proposed DiGGR outperforms many previous self-supervised
71"
INTRODUCTION,0.09435897435897436,"methods in various node- and graph-level classification tasks.
72"
RELATED WORKS,0.09538461538461539,"2
Related works
73"
RELATED WORKS,0.09641025641025641,"Graph Self-Supervised Learning:
Graph SSL has achieved remarkable success in addressing label
74"
RELATED WORKS,0.09743589743589744,"scarcity in real-world network data, mainly consisting of contrastive and generative methods. Con-
75"
RELATED WORKS,0.09846153846153846,"trastive methods, includes feature-oriented approaches[Hu et al., 2019, Zhu et al., 2020, Veliˇckovi´c
76"
RELATED WORKS,0.09948717948717949,"et al., 2018], proximity-oriented techniques [Hassani and Khasahmadi, 2020, You et al., 2020], and
77"
RELATED WORKS,0.10051282051282051,"graph-sampling-based methods [Qiu et al., 2020]. A common limitation across these approaches
78"
RELATED WORKS,0.10153846153846154,"is their heavy reliance on the design of pretext tasks and augmentation techniques. Compared to
79"
RELATED WORKS,0.10256410256410256,"contrastive methods, generative methods are generally simpler to implement. Recently, to tackle the
80"
RELATED WORKS,0.10358974358974359,"challenge of overemphasizing neighborhood information at the expense of structural information
81"
RELATED WORKS,0.10461538461538461,"[Hassani and Khasahmadi, 2020, Veliˇckovi´c et al., 2018], the Graph Masked Autoencoder (GMAE)
82"
RELATED WORKS,0.10564102564102563,"has been proposed. It applies a masking strategy to graph structure [Li et al., 2023a], node attributes
83"
RELATED WORKS,0.10666666666666667,"[Hou et al., 2022], or both [Tian et al., 2023] for representation learning. Unlike most GMAEs, which
84"
RELATED WORKS,0.1076923076923077,"employ random mask strategies, this paper builds disentangled mask strategies.
85"
RELATED WORKS,0.10871794871794872,"Disentangled Graph Learning:
Disentangled representation learning aims to discover and isolate
86"
RELATED WORKS,0.10974358974358975,"the fundamental explanatory factors inherent in the data [Bengio et al., 2013]. Existing efforts in
87"
RELATED WORKS,0.11076923076923077,"disentangled representation learning have primarily focused on computer vision [Higgins et al.,
88"
RELATED WORKS,0.1117948717948718,"2017, Jiang et al., 2020]. Recently, there has been a surge of interest in applying these techniques
89"
RELATED WORKS,0.11282051282051282,"to graph-structured data [Li et al., 2021, Ma et al., 2019, Mercatali et al., 2022, Mo et al., 2023].
90"
RELATED WORKS,0.11384615384615385,"For example, DisenGCN [Ma et al., 2019] utilizes an attention-based methodology to discriminate
91"
RELATED WORKS,0.11487179487179487,"between distinct latent factors, enhancing the representation of each node to more accurately reflect
92"
RELATED WORKS,0.1158974358974359,"its features across multiple dimensions. DGCL [Li et al., 2021] suggests learning disentangled
93"
RELATED WORKS,0.11692307692307692,"graph-level representations through self-supervision, ensuring that the factorized representations
94"
RELATED WORKS,0.11794871794871795,"independently capture expressive information from various latent factors. Despite the excellent results
95"
RELATED WORKS,0.11897435897435897,"achieved by the aforementioned methods on various tasks, these methods are difficult to converge
96"
RELATED WORKS,0.12,"in generative graph SSL, as we demonstrated in the experiment of Table.3. Therefore, this paper
97"
RELATED WORKS,0.12102564102564102,"proposes a disentangled-guided framework for generative graph representation learning, capable of
98"
RELATED WORKS,0.12205128205128205,"learning disentangled representations in an end-to-end self-supervised manner.
99"
PROPOSED METHOD,0.12307692307692308,"3
Proposed Method
100"
PROPOSED METHOD,0.12410256410256411,"In this section, we propose DiGGR (Disentangled Generative Graph Representation Learning) for
101"
PROPOSED METHOD,0.12512820512820513,"self-supervised graph representation learning with mask modeling. The framework was depicted
102"
PROPOSED METHOD,0.12615384615384614,"in Figure 2, comprises three primary components: Latent Factor Learning (Section 3.2), Graph
103"
PROPOSED METHOD,0.12717948717948718,"Factorization (Section 3.2) and Disentangled Graph Masked autoencder (Section 3.3). Before
104"
PROPOSED METHOD,0.1282051282051282,"elaborating on them, we first show some notations.
105"
PRELIMINARIES,0.12923076923076923,"3.1
Preliminaries
106"
PRELIMINARIES,0.13025641025641024,"A graph G can be represented as a multi-tuple G = {V, A, X} with N nodes and M edges, where
107"
PRELIMINARIES,0.13128205128205128,"|V | = N is the node set, |A| = M is the edge set, and X ∈RN×L is the feature matrix for N
108"
PRELIMINARIES,0.13230769230769232,"nodes with L dimensional feature vector. The topology structure of graph G can be found in its
109"
PRELIMINARIES,0.13333333333333333,"adjacency matrix A ∈RN×N. z ∈RN×K is the latent disentangled factor matrix, and K is the
110"
PRELIMINARIES,0.13435897435897437,"predefined factor number. Since we aim to obtain the z to guide the mask modeling, we first utilize a
111"
PRELIMINARIES,0.13538461538461538,"probabilistic graph generation model to factorize the graph before employing the mask mechanism.
112"
PRELIMINARIES,0.13641025641025642,"Given the graph G, it is factorized into {G1, G2, ..., GK}, and each factor-specific graph Gk consists
113"
PRELIMINARIES,0.13743589743589743,"of its factor-specific edges A(k), node set V (k) and node feature matrix X(k). Other notations will be
114"
PRELIMINARIES,0.13846153846153847,"elucidated as they are employed.
115"
LATENT FACTOR LEARNING,0.13948717948717948,"3.2
Latent Factor Learning
116"
LATENT FACTOR LEARNING,0.14051282051282052,"In this subsection, we describe the latent factor learning method. In this phase, our objective is to
117"
LATENT FACTOR LEARNING,0.14153846153846153,"derive factor-specific node sets {V (1), V (2), ..., V (K)} and adjacency matrices {A(1), A(2), ..., A(K)},
118"
LATENT FACTOR LEARNING,0.14256410256410257,"serving as basic unit of the graph to guide the subsequent masking. The specific approach involves
119"
LATENT FACTOR LEARNING,0.14358974358974358,"modeling the distribution of nodes and edges, utilizing the generative process developed in EPM
120"
LATENT FACTOR LEARNING,0.14461538461538462,"[Zhou, 2015]. The generative process of EPM under the Bernoulli-Poisson link [Zhou, 2015] can be
121"
LATENT FACTOR LEARNING,0.14564102564102563,"described as:
122"
LATENT FACTOR LEARNING,0.14666666666666667,"Muv ∼Poisson(
XK"
LATENT FACTOR LEARNING,0.1476923076923077,"k=1 γkzukzvk), zuk ∼Gamma (α, β) , u, v ∈[1, N]
(1)"
LATENT FACTOR LEARNING,0.14871794871794872,"where K is the predefined number of latent factors, and u and v are the indexes of the nodes. Here,
123"
LATENT FACTOR LEARNING,0.14974358974358976,"Muv is the latent count variable between node u and v; γk is a positive factor activation level indicator,
124"
LATENT FACTOR LEARNING,0.15076923076923077,"which measures the node interaction frequency via factor k; zuk is a positive latent variable for node
125"
LATENT FACTOR LEARNING,0.1517948717948718,Graph Factorization via Mask Mask Mask
LATENT FACTOR LEARNING,0.15282051282051282,Feature Reconstruction
LATENT FACTOR LEARNING,0.15384615384615385,Random Mask phase 𝑳𝒐𝒔𝒔
LATENT FACTOR LEARNING,0.15487179487179487,"GNN
FeedBack 𝒌 𝝀"
LATENT FACTOR LEARNING,0.1558974358974359,Input Graph 𝑮
LATENT FACTOR LEARNING,0.15692307692307692,Masked Node
LATENT FACTOR LEARNING,0.15794871794871795,Hidden
LATENT FACTOR LEARNING,0.15897435897435896,"Latent Factor Learning
Reconstruction-based
Representation Learning GAE GAE GAE"
LATENT FACTOR LEARNING,0.16,"GAE
Disentangled Graph Mask"
LATENT FACTOR LEARNING,0.16102564102564101,"Figure 2: The overview of proposed DiGGR’s computation graph. The input data successively passes
three modules described in Sections 3.2 and 3.3: Latent Factor Learning, Graph Factorization, and
Disentangled Graph Mask Autoencoder. Graph information will be first processed through Latent
Factor Learning and Graph Factorization, the former processed the input graph to get the latent factor
z; the latter performs graph factorization via z, such that in each factorized subgraph, nodes exchange
more information with intensively interacted neighbors. Hence, during the disentangled graph
masking phase, we will individually mask each factorized subgraph to enhance the disentanglement
of the obtained node representations."
LATENT FACTOR LEARNING,0.16205128205128205,"u, which measures how strongly node u is affiliated with factor k. The prior distribution of latent
126"
LATENT FACTOR LEARNING,0.16307692307692306,"factor variable zuk is set to Gamma distribution, where α and β are normally set to 1. Therefore, the
127"
LATENT FACTOR LEARNING,0.1641025641025641,"intuitive explanation for this generative process is that, with zuk and zvk measuring how strongly
128"
LATENT FACTOR LEARNING,0.16512820512820514,"node u and v are affiliated with the k-th factor, respectively, the product γkzukzvk measures how
129"
LATENT FACTOR LEARNING,0.16615384615384615,"strongly nodes u and v are connected due to their affiliations with the k-th factor.
130"
LATENT FACTOR LEARNING,0.1671794871794872,"Node Factorization:
Equation 1 can be further augmented as follows:
131"
LATENT FACTOR LEARNING,0.1682051282051282,"Muv =
XK"
LATENT FACTOR LEARNING,0.16923076923076924,"k Mukv, Mukv ∼Poisson (γkzukzvk)
(2)"
LATENT FACTOR LEARNING,0.17025641025641025,"where Mukv represents how often nodes u and v interact due to their affiliations with the k-th factor.
132"
LATENT FACTOR LEARNING,0.1712820512820513,"To represent how often node u is affiliated with the k-th factor, we further introduce the latent count
133"
LATENT FACTOR LEARNING,0.1723076923076923,Muk· = P
LATENT FACTOR LEARNING,0.17333333333333334,"v̸=u Mukv. Then, we can soft assign node u to multiple factors in {k : Muk·} ≥1, or
134"
LATENT FACTOR LEARNING,0.17435897435897435,"hard assign node u to a single factor using arg max
k
(Muk·). However, our experiments show that
135"
LATENT FACTOR LEARNING,0.1753846153846154,"soft assignment method results in significant overlap among node sets from different factor group,
136"
LATENT FACTOR LEARNING,0.1764102564102564,"diminishing the distinctiveness. Note that previous study addressed a similar issue by selecting the
137"
LATENT FACTOR LEARNING,0.17743589743589744,"top-k most attended regions [Kakogeorgiou et al., 2022]. Thus, we choose the hard assign strategy to
138"
LATENT FACTOR LEARNING,0.17846153846153845,"factorize the graph node set V graph into factor-specific node sets {V (1), V (2), · · · , V (K)}.
139"
LATENT FACTOR LEARNING,0.1794871794871795,"Edge Factorization:
To create factor-specific edges A(k) for a factor-specific node set V (k), a
140"
LATENT FACTOR LEARNING,0.18051282051282053,"straightforward method involves removing all external nodes connected to other factor groups. This
141"
LATENT FACTOR LEARNING,0.18153846153846154,"can be defined as:
142"
LATENT FACTOR LEARNING,0.18256410256410258,"A(k)
uv ="
LATENT FACTOR LEARNING,0.1835897435897436,"(
Auv, ∀u, v ∈V (k); u, v ∈[1, N] ;"
LATENT FACTOR LEARNING,0.18461538461538463,"0,
∃u, v /∈V (k); u, v ∈[1, N] .
(3)"
LATENT FACTOR LEARNING,0.18564102564102564,"Besides, the global graph edge A can also be factorized into positive-weighted edges [He et al.,
143"
LATENT FACTOR LEARNING,0.18666666666666668,"2022b] for each latent factor as:
144"
LATENT FACTOR LEARNING,0.18769230769230769,"A(k)
uv = Auv ·
exp (γkzukzvk)
P"
LATENT FACTOR LEARNING,0.18871794871794872,"k′ exp (γk′zuk′zvk′); k ∈[1, K] , u, v ∈[1, N] .
(4)"
LATENT FACTOR LEARNING,0.18974358974358974,"Applying Equation 4 to all pairs of nodes yields weighted adjacency matrices {A(k)}k
k=1, with A(k)
145"
LATENT FACTOR LEARNING,0.19076923076923077,"corresponding to latent factor zk. Note that A(k) has the same dimension as A and Equation 4
146"
LATENT FACTOR LEARNING,0.19179487179487179,"presents a trainable weight for each edge, which can be jointly optimized through network training,
147"
LATENT FACTOR LEARNING,0.19282051282051282,"showcasing an advantage over Equation 3 in this aspect. Therefore, we apply Equation 4 for edge
148"
LATENT FACTOR LEARNING,0.19384615384615383,"factorization.
149"
LATENT FACTOR LEARNING,0.19487179487179487,"Variational Inference:
The latent factor variable z determines the quality of node and edge factor-
150"
LATENT FACTOR LEARNING,0.19589743589743588,"ization, so we need to approximate its posterior distribution. Denoting zu = (zu1, ..., zuK), zu ∈RK
+ ,
151"
LATENT FACTOR LEARNING,0.19692307692307692,"which measures how strongly node u is affiliated with all the K latent factors, we adopt a Weibull
152"
LATENT FACTOR LEARNING,0.19794871794871796,"variational graph encoder [Zhang et al., 2018, He et al., 2022b]:
153"
LATENT FACTOR LEARNING,0.19897435897435897,"q(zu | A, X) = Weibull(ku, λu), (ku, λu) = GNNEPM(A, X),
u ∈[1, N]
(5)"
LATENT FACTOR LEARNING,0.2,"where GNNEPM(·) stands for graph neural networks, and we select a two-layer Graph Convolution
154"
LATENT FACTOR LEARNING,0.20102564102564102,"Networks (i.e., GCN [Kipf and Welling, 2016a]) for our models; ku, λu ∈RK
+ are the shape and
155"
LATENT FACTOR LEARNING,0.20205128205128206,"scale parameters of the variational Weibull distribution, respectively. The latent variable zu can be
156"
LATENT FACTOR LEARNING,0.20307692307692307,"conveniently reparameterized as:
157"
LATENT FACTOR LEARNING,0.2041025641025641,"zu = λu(−ln(1 −ε))1/ku, ε ∼Uniform(0, 1).
(6)"
LATENT FACTOR LEARNING,0.20512820512820512,"The optimization objective of latent factor learning phase can be achieved by maximizing the evidence
158"
LATENT FACTOR LEARNING,0.20615384615384616,"lower bound (ELBO) of the log marginal likelihood of edge log p(A), which can be computed as:
159"
LATENT FACTOR LEARNING,0.20717948717948717,"Lz = Eq(Z | A,X) [ln p (A | Z)] − N
X"
LATENT FACTOR LEARNING,0.2082051282051282,"u=1
Eq(zu | A,X)"
LATENT FACTOR LEARNING,0.20923076923076922,"
ln q(zu | A, X) p(zu) 
(7)"
LATENT FACTOR LEARNING,0.21025641025641026,"where the first term is the expected log-likelihood or reconstruction error of edge, and the second
160"
LATENT FACTOR LEARNING,0.21128205128205127,"term is the Kullback–Leibler (KL) divergence that constrains q(zu) to be close to its prior p(zu). The
161"
LATENT FACTOR LEARNING,0.2123076923076923,"analytical expression for the KL divergence and the straightforward reparameterization of the Weibull
162"
LATENT FACTOR LEARNING,0.21333333333333335,"distribution simplify the gradient estimation of the ELBO concerning the decoder parameters and
163"
LATENT FACTOR LEARNING,0.21435897435897436,"other parameters in the inference network.
164"
DISENTANGLED GRPAH MASKED AUTOENCODER,0.2153846153846154,"3.3
Disentangled Grpah Masked Autoencoder
165"
DISENTANGLED GRPAH MASKED AUTOENCODER,0.2164102564102564,"With the latent factor learning phase discussed in 3.2, the graph can be factorized into a series of
166"
DISENTANGLED GRPAH MASKED AUTOENCODER,0.21743589743589745,"factor-specific subgraphs {G1, G2, ..., GK} via the latent factor z. To incorporate the disentangled
167"
DISENTANGLED GRPAH MASKED AUTOENCODER,0.21846153846153846,"information encapsulated in z into the graph masked autoencoder, we proposed Disentangled Graph
168"
DISENTANGLED GRPAH MASKED AUTOENCODER,0.2194871794871795,"Masked Autoencoder in this section. Specifically, this section will first introduce the latent factor-wise
169"
DISENTANGLED GRPAH MASKED AUTOENCODER,0.2205128205128205,"GMAE and the graph-level GMAE.
170"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.22153846153846155,"3.3.1
Latent Factor-wise Grpah Masked Autoencoder
171"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.22256410256410256,"To capture disentangled patterns within the latent factor z , for each latent subgraph
172"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2235897435897436,"Gk = (V (k), A(k), X(k)), the latent factor-wise GMAE can be described as:
173"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2246153846153846,"H(k)
d
= GNNenc(A(k), ¯X(k)), ˜Xd = GNNdec(A, Hd).
(8)"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.22564102564102564,"where ¯X(k) is the masked node feature matrix for the k-th latent factor, and ˜Xd denotes the recon-
174"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.22666666666666666,"structed node features. GNNenc(.) and GNNdec(.) are the graph encoder and decoder, respectively;
175"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2276923076923077,"H(k)
d
∈RN×D are factor-wise hidden representations, and Hd = H(1)
d
⊕H(2)
d
· · · ⊕H(K)
d
. After the
176"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2287179487179487,"concatenation operation ⊕in feature dimension, the multi factor-wise hidden representation becomes
177"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.22974358974358974,"Hd ∈RN×(K·D), which is used as the input of GNNdec(.).
178"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.23076923076923078,"Regarding the mask opeartion, we uniformly random sample a subset of nodes ¯V (k) ∈V (k) and
179"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2317948717948718,"mask each of their features with a mask token, such as a learnable vector X[M] ∈Rd. Thus, the node
180"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.23282051282051283,"feature in the masked feature matrix can be defined as:
181"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.23384615384615384,"¯X(k)
i
="
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.23487179487179488,"(
X[M]; vi ∈¯V (k) ;"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2358974358974359,"Xi
; vi /∈¯V (k).
(9)"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.23692307692307693,"The objective of latent factor-wise GMAE is to reconstruct the masked features of nodes in ¯V (k)
182"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.23794871794871794,"given the partially observed node signals ¯X(k) and the input adjacency matrix A(k). Another crucial
183"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.23897435897435898,"component of the GMAE is the feature reconstruction criterion, often used in language as cross-
184"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24,"entropy error [Devlin et al., 2018] and in the image as mean square error [He et al., 2022a]. However,
185"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24102564102564103,"texts and images typically involve tokenized input features, whereas graph autoencoders (GAE) do
186"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24205128205128204,"not have a universal tokenizer. We adopt the scored cosine error of GraphMAE [Hou et al., 2022] as
187"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24307692307692308,"the loss function. Generally, given the original feature X(k) and reconstructed node feature ˜X(k), the
188"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2441025641025641,"defined SCE is:
189"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24512820512820513,"LD =
1
| ¯V | X i∈¯V "
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24615384615384617,"1 −
XT
i ˜Xd
i
∥Xi∥· ∥˜Xd
i ∥ !γ"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24717948717948718,", γ ≥1
(10)"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24820512820512822,"where ¯V = ¯V (1) ∪¯V (2)... ∪¯V (K) and Equation 10 are averaged over all masked nodes.The scaling
190"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.24923076923076923,"factor γ is a hyper-parameter adjustable over different datasets. This scaling technique could also
191"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.25025641025641027,"be viewed as adaptive sample reweighting, and the weight of each sample is adjusted with the
192"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2512820512820513,"reconstruction error. This error is also famous in the field of supervised object detection as the focal
193"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2523076923076923,"loss [Lin et al., 2017].
194"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.25333333333333335,"Graph-level Graph Mask Autoencoder:
For the node classification task, we have integrated
195"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.25435897435897437,"graph-level GMAE into DiGGR. We provide a detailed experimental analysis and explanation for
196"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2553846153846154,"this difference in Appendix A.1.2. The graph-level masked graph autoencoder is designed with the
197"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2564102564102564,"aim of further capturing the global patterns, which can be designed as:
198"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.25743589743589745,"Hg = GNNenc(A, ¯X),
˜Xg = GNNdec(A, Hg).
(11)"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.25846153846153846,"¯X is the masked node feature matrix, whose mask can be generated by uniformly random sampling
199"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2594871794871795,"a subset of nodes ˜V ∈V , or obtained by concatenating the masks of all factor-specific groups
200"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2605128205128205,"˜V = ¯V (1) ∪¯V (2)... ∪¯V (K). The global hidden representation encoded by GNNenc(.) is Hg, which is
201"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.26153846153846155,"then passed to the decoder. Similar to Equation 10, we can define the graph-level reconstruct loss as:
202"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.26256410256410256,"LG =
1
| ˜V | X"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.2635897435897436,"i∈˜V
(1 −
XT
i ˜Xg
i
∥Xi∥· ∥˜Xg
i ∥
)γ, γ ≥1.
(12)"
LATENT FACTOR-WISE GRPAH MASKED AUTOENCODER,0.26461538461538464,"which is averaged over all masked nodes.
203"
JOINT TRAINING AND INFERENCE,0.26564102564102565,"3.4
Joint Training and Inference
204"
JOINT TRAINING AND INFERENCE,0.26666666666666666,"Benefiting from the effective variational inference method, the proposed latent factor learning and
205"
JOINT TRAINING AND INFERENCE,0.2676923076923077,"dsientangled graph masked autoencoder can be jointly trained in one framework. We combine the
206"
JOINT TRAINING AND INFERENCE,0.26871794871794874,"aforementioned losses with three mixing coefficient λd, λg and λz during training, and the loss for
207"
JOINT TRAINING AND INFERENCE,0.26974358974358975,"joint training can be written as
208"
JOINT TRAINING AND INFERENCE,0.27076923076923076,"L = λd · LD + λg · LG + λz · Lz.
(13)"
JOINT TRAINING AND INFERENCE,0.2717948717948718,"Since Weibull distributions have easy reparameterization functions, these parameters can be jointly
209"
JOINT TRAINING AND INFERENCE,0.27282051282051284,"trained by stochastic gradient descent with low-variance gradient estimation. We summarize the
210"
JOINT TRAINING AND INFERENCE,0.27384615384615385,"training algorithm at Algorithm 1 in Appendix A.4. For downstream applications, the encoder is
211"
JOINT TRAINING AND INFERENCE,0.27487179487179486,"applied to the input graph without any masking in the inference stage. The generated factor-wise
212"
JOINT TRAINING AND INFERENCE,0.27589743589743587,"node embeddings Hd and graph-level embeddings Hg can either be concatenated in the feature
213"
JOINT TRAINING AND INFERENCE,0.27692307692307694,"dimensions or used separately. The resulting final representation H can be employed for various
214"
JOINT TRAINING AND INFERENCE,0.27794871794871795,"graph learning tasks, such as node classification and graph classification. For graph-level tasks, we
215"
JOINT TRAINING AND INFERENCE,0.27897435897435896,"use a non-parameterized graph pooling (readout) function, e.g., MaxPooling and MeanPooling to
216"
JOINT TRAINING AND INFERENCE,0.28,"obtain the graph-level representation.
217"
JOINT TRAINING AND INFERENCE,0.28102564102564104,"Time and space complexity: Let’s recall that in our context, N, M, and K represent the number of
218"
JOINT TRAINING AND INFERENCE,0.28205128205128205,"nodes, edges, and latent factors in the graph, respectively. The feature dimension is denoted by F,
219"
JOINT TRAINING AND INFERENCE,0.28307692307692306,"while L1, L2, L3, and L4 represent the number of layers in the latent factor learning encoder, the
220"
JOINT TRAINING AND INFERENCE,0.2841025641025641,"latent factor-wise GMAE’s encoder, the graph-level GMAE’s encoder, and the decoder respectively.
221"
JOINT TRAINING AND INFERENCE,0.28512820512820514,"In DiGGR, we constrain the hidden dimension size in latent factor-wise GMAE’s encoder to be
222"
JOINT TRAINING AND INFERENCE,0.28615384615384615,"1/K of the typical baseline dimensions. Consequently, the time complexity for training DiGGR can
223"
JOINT TRAINING AND INFERENCE,0.28717948717948716,"be expressed as O((L1 + L2 + L3)MF + (L1 + L2/K + L3)NF 2 + N 2F + L4NF 2), and the
224"
JOINT TRAINING AND INFERENCE,0.2882051282051282,"space complexity is O((L1 + L2 + L3 + L4)NF + KM + (L1 + L2/K + L3 + L4)F 2), with
225"
JOINT TRAINING AND INFERENCE,0.28923076923076924,"O((L1 +L2/K +L3 +L4)F 2) attributed to model parameters. We utilize the Bayesian factor model
226"
JOINT TRAINING AND INFERENCE,0.29025641025641025,"in our approach to reconstruct edges. Its time complexity aligns with that of variational inference
227"
JOINT TRAINING AND INFERENCE,0.29128205128205126,"in SeeGera Li et al. [2023b], predominantly at O(N 2F); Therefore, the complexity of DiGGR is
228"
JOINT TRAINING AND INFERENCE,0.2923076923076923,"comparable to previous works.
229"
JOINT TRAINING AND INFERENCE,0.29333333333333333,"Table 1: Experiment results for node classification. Micro-F1 score is reported for PPI, and accuracy
for other datasets. The best unsupervised method scores in each dataset are highlighted in bold."
JOINT TRAINING AND INFERENCE,0.29435897435897435,"Methods
Cora
Citeseer
Pubmed
PPI"
JOINT TRAINING AND INFERENCE,0.2953846153846154,"GCN [Kipf and Welling, 2016a]
81.50
70.30
79.00
75.70 ± 0.10
GAT [Velickovic et al., 2017]
83.00 ± 0.70
72.50 ± 0.70
79.00 ± 0.30
97.30 ± 0.20
DisenGCN[Ma et al., 2019]
83.7
73.4
80.5
-
VEPM[He et al., 2022b]
84.3 ± 0.1
72.5 ± 0.1
82.4 ± 0.2
-"
JOINT TRAINING AND INFERENCE,0.2964102564102564,"MVGRL [Hassani and Khasahmadi, 2020]
83.50 ± 0.40
73.30 ± 0.50
80.10 ± 0.70
-
InfoGCL [Xu et al., 2021]
83.50 ± 0.30
73.50 ± 0.40
79.10 ± 0.20
-
DGI [Veliˇckovi´c et al., 2018]
82.30 ± 0.60
71.80 ± 0.70
76.80 ± 0.60
63.80 ± 0.20
GRACE [Zhu et al., 2020]
81.90 ± 0.40
71.20 ± 0.50
80.60 ± 0.40
69.71 ± 0.17
BGRL [Thakoor et al., 2021]
82.70 ± 0.60
71.10 ± 0.80
79.60 ± 0.50
73.63 ± 0.16
CCA-SSG [Zhang et al., 2021]
84.20 ± 0.40
73.10 ± 0.30
81.00 ± 0.40
73.34 ± 0.17"
JOINT TRAINING AND INFERENCE,0.29743589743589743,"GAE [Kipf and Welling, 2016b]
71.50 ± 0.40
65.80 ± 0.40
72.10 ± 0.50
-
VGAE [Kipf and Welling, 2016b]
76.30 ± 0.20
66.80 ± 0.20
75.80 ± 0.40
-
Bandana [Zhao et al., 2024]
84.62 ± 0.37
73.60 ± 0.16
83.53 ± 0.51
-
GiGaMAE[Shi et al., 2023]
84.72 ± 0.47
72.31 ± 0.50
-
-
SEEGERA [Shi et al., 2023]
84.30 ± 0.40
73.00 ± 0.80
80.40 ± 0.40
-
GraphMAE [Hou et al., 2022]
84.20 ± 0.40
73.40 ± 0.40
81.10 ± 0.40
74.50 ± 0.29
GraphMAE2[Hou et al., 2023]
84.50 ± 0.60
73.40 ± 0.30
81.40 ± 0.50
-"
JOINT TRAINING AND INFERENCE,0.29846153846153844,"DiGGR
84.96 ± 0.32
73.98 ± 0.27
81.30 ± 0.26
78.30 ± 0.71"
EXPERIMENTS,0.2994871794871795,"4
Experiments
230"
EXPERIMENTS,0.3005128205128205,"We compare the proposed self-supervised framework DiGGR against related baselines on two funda-
231"
EXPERIMENTS,0.30153846153846153,"mental tasks: unsupervised representation learning on node classification and graph classification.
232"
EXPERIMENTS,0.30256410256410254,"We evaluate DiGGR on 11 benchmarks. For node classification, we use 3 citation networks (Cora,
233"
EXPERIMENTS,0.3035897435897436,"Citeseer, Pubmed [Yang et al., 2016]), and protein-protein interaction networks (PPI) [Hamilton et al.,
234"
EXPERIMENTS,0.3046153846153846,"2017]. For graph classification, we use 3 bioinformatics datasets (MUTAG, NCI1, PROTEINS) and 4
235"
EXPERIMENTS,0.30564102564102563,"social network datasets (IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and COLLAB). The
236"
EXPERIMENTS,0.30666666666666664,"specific information of the dataset and the hyperparameters used by the network are listed in the
237"
EXPERIMENTS,0.3076923076923077,"Appendix A.2 in table 5 and 6. We also provide the detailed experiment setup in Appendix A.2 for
238"
EXPERIMENTS,0.3087179487179487,"node classification (4.1) and graph classification (4.2)
239"
NODE CLASSIFICATION,0.30974358974358973,"4.1
Node Classification
240"
NODE CLASSIFICATION,0.31076923076923074,"The baseline models for node classification can be divided into three categories: i) supervised methods,
241"
NODE CLASSIFICATION,0.3117948717948718,"including GCN [Kipf and Welling, 2016a] , DisenGCN[Ma et al., 2019], VEPM[He et al., 2022b]
242"
NODE CLASSIFICATION,0.3128205128205128,"and GAT [Velickovic et al., 2017]; ii) contrastive learning methods, including MVGRL [Hassani and
243"
NODE CLASSIFICATION,0.31384615384615383,"Khasahmadi, 2020], InfoGCL [Xu et al., 2021], DGI [Veliˇckovi´c et al., 2018], GRACE [Zhu et al.,
244"
NODE CLASSIFICATION,0.3148717948717949,"2020], BGRL [Thakoor et al., 2021] and CCA-SSG [Zhang et al., 2021]; iii) generative learning
245"
NODE CLASSIFICATION,0.3158974358974359,"methods, including GraphMAE [Hou et al., 2022], GraphMAE2[Hou et al., 2023], Bandana[Zhao
246"
NODE CLASSIFICATION,0.3169230769230769,"et al., 2024], GiGaMAE[Shi et al., 2023], SeeGera[Li et al., 2023b], GAE and VGAE [Kipf and
247"
NODE CLASSIFICATION,0.31794871794871793,"Welling, 2016b]. The node classification results were listed in Table 1. DiGGR demonstrates
248"
NODE CLASSIFICATION,0.318974358974359,"competitive results on the provided dataset, achieving results comparable to those of supervised
249"
NODE CLASSIFICATION,0.32,"methods.
250"
GRAPH CLASSIFICATION,0.321025641025641,"4.2
Graph Classification
251"
GRAPH CLASSIFICATION,0.32205128205128203,"Baseline Models
We categorized the baseline models into four groups: i) supervised methods,
252"
GRAPH CLASSIFICATION,0.3230769230769231,"including GIN [Xu et al., 2018], DiffPool[Ying et al., 2018] and VEPM[He et al., 2022b]; ii) classical
253"
GRAPH CLASSIFICATION,0.3241025641025641,"graph kernel methods: Weisfeiler-Lehman sub-tree kernel (WL) [Shervashidze et al., 2011] and
254"
GRAPH CLASSIFICATION,0.3251282051282051,"deep graph kernel (DGK) [Yanardag and Vishwanathan, 2015]; iii) contrastive learning methods,
255"
GRAPH CLASSIFICATION,0.3261538461538461,"including GCC [Qiu et al., 2020], graph2vec [Narayanan et al., 2017], Infograph [Sun et al., 2019],
256"
GRAPH CLASSIFICATION,0.3271794871794872,"GraphCL [You et al., 2020], JOAO [You et al., 2021], MVGRL [Hassani and Khasahmadi, 2020],
257"
GRAPH CLASSIFICATION,0.3282051282051282,"and InfoGCL [Xu et al., 2021]; 4) generative learning methods, including graph2vec [Narayanan
258"
GRAPH CLASSIFICATION,0.3292307692307692,"et al., 2017], sub2vec [Adhikari et al., 2018], node2vec [Grover and Leskovec, 2016], GraphMAE
259"
GRAPH CLASSIFICATION,0.3302564102564103,"[Hou et al., 2022], GraphMAE2[Hou et al., 2023], GAE and VGAE [Kipf and Welling, 2016b]. Per
260"
GRAPH CLASSIFICATION,0.3312820512820513,"graph classification research tradition, we report results from previous papers if available.
261"
GRAPH CLASSIFICATION,0.3323076923076923,"Table 2: Experiment results in unsupervised representation learning for graph classification. We report
accuracy (%) for all datasets. The optimal outcomes for methods, excluding supervised approaches
(GIN and DiffPool), on each dataset are emphasized in bold."
GRAPH CLASSIFICATION,0.3333333333333333,"Methods
IMDB-B
IMDB-M
MUTAG
NCI1
REDDIT-B
PROTEINS
COLLAB"
GRAPH CLASSIFICATION,0.3343589743589744,"GIN
75.1± 5.1
52.3 ± 2.8
89.4 ± 5.6
82.7 ± 1.7
92.4 ± 2.5
76.2 ± 2.8
80.2 ± 1.9
DiffPool
72.6 ± 3.9
-
85.0 ± 10.3
-
92.1 ± 2.6
75.1 ± 3.5
78.9 ± 2.3
VEPM
76.7 ± 3.1
54.1 ± 2.1
93.6 ± 3.4
83.9 ± 1.8
90.5 ± 1.8
80.5 ± 2.8
-"
GRAPH CLASSIFICATION,0.3353846153846154,"WL
72.30 ± 3.44
46.95 ± 0.46
80.72 ± 3.00
80.31 ± 0.46
68.82 ± 0.41
72.92 ± 0.56
-
DGK
66.96 ± 0.56
44.55 ± 0.52
87.44 ± 2.72
80.31 ± 0.46
78.04 ± 0.39
73.30 ± 0.82
73.09 ± 0.25"
GRAPH CLASSIFICATION,0.3364102564102564,"Infograph
73.03 ± 0.87
49.69 ± 0.53
89.01 ± 1.13
76.20 ± 1.06
82.50 ± 1.42
74.44 ± 0.31
70.65 ± 1.13
GraphCL
71.14 ± 0.44
48.58 ± 0.67
86.80 ± 1.34
77.87 ± 0.41
89.53 ± 0.84
74.39 ± 0.45
71.36 ± 1.15
JOAO
70.21 ± 3.08
49.20 ± 0.77
87.35 ± 1.02
78.07 ± 0.47
85.29 ± 1.35
74.55 ± 0.41
69.50 ± 0.36
GCC
72.0
49.4
-
-
89.9
-
78.9
MVGRL
74.20 ± 0.70
51.20 ± 0.50
89.70 ± 1.10
-
84.50 ± 0.60
-
-
InfoGCL
75.10 ± 0.90
51.40 ± 0.80
91.20 ± 1.30
80.20 ± 0.60
-
-
80.00 ± 1.30"
GRAPH CLASSIFICATION,0.3374358974358974,"graph2vec
71.10 ± 0.54
50.44 ± 0.87
83.15 ± 9.25
73.22 ± 1.81
75.78 ± 1.03
73.30 ± 2.05
-
sub2vec
55.3 ± 1.5
36.7 ± 0.8
61.1 ± 15.8
52.8 ± 1.5
71.5 ± 0.4
53.0 ± 5.6
-
node2vec
-
-
72.6 ± 10.2
54.9 ± 1.6
-
57.5 ± 3.6
-
GAE
52.1 ± 0.2
-
84.0 ± 0.6
73.3 ± 0.6
74.8± 0.2
74.1 ± 0.5
-
VGAE
52.1 ± 0.2
-
84.4 ± 0.6
73.7 ± 0.3
74.8 ± 0.2
74.8 ± 0.2
-
GraphMAE
75.52 ± 0.66
51.63 ± 0.52
88.19 ± 1.26
80.40 ± 0.30
88.01± 0.19
75.30 ± 0.39
80.32 ± 0.46
GraphMAE2 73.88 ± 0.53
51.80 ± 0.60
86.63 ± 1.33
78.56 ± 0.26
76.84 ± 0.21
74.86 ± 0.34
77.59 ± 0.22"
GRAPH CLASSIFICATION,0.3384615384615385,"DiGGR
77.68 ± 0.48
54.77 ± 2.63
88.72 ± 1.03
81.23 ± 0.40
88.19 ± 0.28
77.40 ± 0.05
83.76 ± 3.70"
GRAPH CLASSIFICATION,0.3394871794871795,"Performance Comparison
The graph classification results are presented in Table 2. In general, we
262"
GRAPH CLASSIFICATION,0.3405128205128205,"find that DiGGR gained the best performance among other baselines on five out of seven datasets,
263"
GRAPH CLASSIFICATION,0.3415384615384615,"while achieving competitive results on the other two datasets. The performance of DiGGR is
264"
GRAPH CLASSIFICATION,0.3425641025641026,"comparable to that of supervised learning methods. For instance, the accuracy on IMDB-B and
265"
GRAPH CLASSIFICATION,0.3435897435897436,"IMDB-M surpasses that of GIN and DiffPool. Moreover, within the reported datasets, our method
266"
GRAPH CLASSIFICATION,0.3446153846153846,"demonstrates improved performance compared to random mask methods like GraphMAE, particularly
267"
GRAPH CLASSIFICATION,0.34564102564102567,"on the IMDB-M, COLLAB, and PROTEINS datasets. This underscores the effectiveness of the
268"
GRAPH CLASSIFICATION,0.3466666666666667,"proposed method.
269"
EXPLORATORY STUDIES,0.3476923076923077,"4.3
Exploratory Studies
270"
EXPLORATORY STUDIES,0.3487179487179487,"Visualizing latent representations
To examine the influence of the learned latent factor on classifi-
271"
EXPLORATORY STUDIES,0.34974358974358977,"cation results, we visualized the latent disentangled factor z, which reflects the node-factor affiliation,
272"
EXPLORATORY STUDIES,0.3507692307692308,"and the hidden representation H used for classification. MUTAG is selected as the representative
273"
EXPLORATORY STUDIES,0.3517948717948718,"for classification benchmarks. We encodes the representations into 2-D space via t-SNE [Van der
274"
EXPLORATORY STUDIES,0.3528205128205128,"Maaten and Hinton, 2008]. The result is shown in Figure 3(a), where each node is colored according
275"
EXPLORATORY STUDIES,0.35384615384615387,"to its node labels. The clusters in Figure 3(a) still exhibit differentiation in the absence of label
276"
EXPLORATORY STUDIES,0.3548717948717949,"supervision, suggesting that z obtained through unsupervised learning can enhance node information
277"
EXPLORATORY STUDIES,0.3558974358974359,"and offer a guidance for the mask modeling. We then visualize the hidden representation used for
278"
EXPLORATORY STUDIES,0.3569230769230769,"classification tasks, and color each node according to the latent factor to which it belongs. The results
279"
EXPLORATORY STUDIES,0.35794871794871796,"are depicted in Figure 3(b), showcasing separability among different color clusters. This illustrates
280"
EXPLORATORY STUDIES,0.358974358974359,"the model’s ability to extract information from the latent factor, thereby enhancing the quality of the
281"
EXPLORATORY STUDIES,0.36,"learned representations.
282"
EXPLORATORY STUDIES,0.36102564102564105,"Task-relevant factors
To assess the statistical correlation between the learned latent factor and the
283"
EXPLORATORY STUDIES,0.36205128205128206,"task, we follow the approach in [He et al., 2022b] and compute the Normalized Mutual Information
284"
EXPLORATORY STUDIES,0.3630769230769231,"(NMI) between the nodes in the factor label and the actual node labels. NMI is a metric that ranges
285"
EXPLORATORY STUDIES,0.3641025641025641,"from 0 to 1, where higher values signify more robust statistical dependencies between two random
286"
EXPLORATORY STUDIES,0.36512820512820515,"variables. In the experiment, we utilized the MUTAG dataset, comprising 7 distinct node types,
287"
EXPLORATORY STUDIES,0.36615384615384616,"and the NMI value we obtained was 0.5458. These results highlight that the latent factors obtained
288"
EXPLORATORY STUDIES,0.3671794871794872,"through self-supervised training are meaningful for the task, enhancing the correlation between the
289"
EXPLORATORY STUDIES,0.3682051282051282,"inferred latent factors and the task.
290"
EXPLORATORY STUDIES,0.36923076923076925,"Disentangled representations
To assess DiGGR’s capability to disentangle the learned represen-
291"
EXPLORATORY STUDIES,0.37025641025641026,"tation for downstream task, we provide a qualitative evaluation by plotting the correlation of the
292"
EXPLORATORY STUDIES,0.3712820512820513,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8"
EXPLORATORY STUDIES,0.3723076923076923,"1.0
node class  0
node class  1
node class  2"
EXPLORATORY STUDIES,0.37333333333333335,"(a) z, node label"
EXPLORATORY STUDIES,0.37435897435897436,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8"
EXPLORATORY STUDIES,0.37538461538461537,"1.0
node class  0
node class  1
node class  2
node class  3
node class  4
node class  5
node class  6"
EXPLORATORY STUDIES,0.3764102564102564,"(b) H, factor label"
EXPLORATORY STUDIES,0.37743589743589745,"Figure 3: T-SNE visualization of MUTAG
dataset, where z is the latent factor, H is the
learned node representation used for down-
stream tasks."
EXPLORATORY STUDIES,0.37846153846153846,"(a) GraphMAE
(b) DiGGR"
EXPLORATORY STUDIES,0.37948717948717947,"Figure 4: representation correlation matrix on
Cora with number of factors K = 4. 4(a)
depicts the representation of entanglement,
while 4(b) illustrates disentanglement."
EXPLORATORY STUDIES,0.38051282051282054,"Table 3: The NMI between the latent factors extracted by DiGGR and Non-probabilistic factor
learning method across various datasets, and its performance improvement compared to GraphMAE,
are examined. A lower NMI indicates a more pronounced disentanglement between factor-specific
graphs, resulting in a greater performance enhancement."
EXPLORATORY STUDIES,0.38153846153846155,"Dataset
RDT-B
MUTAG
NCI-1
IMDB-B
PROTEINS
COLLAB
IMDB-M"
EXPLORATORY STUDIES,0.38256410256410256,"DiGGR
NMI
0.95
0.90
0.89
0.82
0.76
0.35
0.24
ACC Gain
+ 0.18%
+ 0.53%
+ 0.83%
+ 2.16%
+ 2.1%
+ 3.44%
+ 3.14%"
EXPLORATORY STUDIES,0.38358974358974357,"Non-probabilistic
Factor Learning"
EXPLORATORY STUDIES,0.38461538461538464,"NMI
1.00
1.00
0.80
1.00
0.60
1.00
0.94
ACC Gain
-2.23%
-2.02%
-0.45%
-0.80%
-2.15%
-3.00%
-0.11%"
EXPLORATORY STUDIES,0.38564102564102565,"node representation in Figure 4. The figure shows the absolute values of the correlation between the
293"
EXPLORATORY STUDIES,0.38666666666666666,"elements of 512-dimensional graph representation and representation obtained from GraphMAE and
294"
EXPLORATORY STUDIES,0.38769230769230767,"DiGGR, respectively. From the results, we can see that the representation produced by GraphMAE
295"
EXPLORATORY STUDIES,0.38871794871794874,"exhibits entanglement, whereas DiGGR’s representation displays a overall block-level pattern,
296"
EXPLORATORY STUDIES,0.38974358974358975,"indicating that DiGGR can capture mutually exclusive information in the graph and disentangle the
297"
EXPLORATORY STUDIES,0.39076923076923076,"hidden representation to some extent. Results for more datasets can be found in Appendix A.3.
298"
EXPLORATORY STUDIES,0.39179487179487177,"299
Why DiGGR works better: To validate that disentangled learning can indeed enhance the quality of
300"
EXPLORATORY STUDIES,0.39282051282051283,"the representations learned by GMAE, we further conduct quantitative experiments. The Normalized
301"
EXPLORATORY STUDIES,0.39384615384615385,"Mutual Information (NMI) is used to quantify the disentangling degree of different datasets. Generally,
302"
EXPLORATORY STUDIES,0.39487179487179486,"the NMI represents the similarity of node sets between different factor-specific graphs, and the lower
303"
EXPLORATORY STUDIES,0.3958974358974359,"NMI suggests a better-disentangled degree with lower similarity among factor-specific graphs. The
304"
EXPLORATORY STUDIES,0.39692307692307693,"NMI between latent factors and the corresponding performance gain (compared to GraphMAE)
305"
EXPLORATORY STUDIES,0.39794871794871794,"are shown in the Table.3. As the results show, DiGGR’s performance improvement has a positive
306"
EXPLORATORY STUDIES,0.39897435897435896,"correlation with disentangled degree, where the better the disentangled degree, the more significant
307"
EXPLORATORY STUDIES,0.4,"the performance improvement. For methods relying on Non-probabilistic Factor Learning, the NMI
308"
EXPLORATORY STUDIES,0.40102564102564103,"tends to approach 1. This is attributed to the challenges faced by the factor learning module in
309"
EXPLORATORY STUDIES,0.40205128205128204,"converging, thereby hindering the learning of distinct latent factors. The presence of confused latent
310"
EXPLORATORY STUDIES,0.40307692307692305,"factors offers misleading guidance for representation learning, consequently leading to decreased
311"
EXPLORATORY STUDIES,0.4041025641025641,"performance.
312"
CONCLUSIONS,0.40512820512820513,"5
Conclusions
313"
CONCLUSIONS,0.40615384615384614,"In this paper, we propose DiGGR (Disentangled Generative Graph Representation Learning), de-
314"
CONCLUSIONS,0.40717948717948715,"signed to achieve disentangled representations in graph masked autoencoders by leveraging latent
315"
CONCLUSIONS,0.4082051282051282,"disentangled factors. In particular, we achieve this by two steps: 1) We utilize a probabilistic graph
316"
CONCLUSIONS,0.40923076923076923,"generation model to factorize the graph via the learned disentangled latent factor; 2) We develop a
317"
CONCLUSIONS,0.41025641025641024,"Disentangled Graph Masked Autoencoder framework, with the aim of integrating the disentangled in-
318"
CONCLUSIONS,0.4112820512820513,"formation into the representation learning of Graph Masked Autoencoders. Experiments demonstrate
319"
CONCLUSIONS,0.4123076923076923,"that our model can acquire disentangled representations, and achieve favorable results on downstream
320"
CONCLUSIONS,0.41333333333333333,"tasks.
321"
REFERENCES,0.41435897435897434,"References
322"
REFERENCES,0.4153846153846154,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
323"
REFERENCES,0.4164102564102564,"contrastive learning of visual representations. In International conference on machine learning,
324"
REFERENCES,0.41743589743589743,"pages 1597–1607. PMLR, 2020.
325"
REFERENCES,0.41846153846153844,"Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph
326"
REFERENCES,0.4194871794871795,"contrastive learning with augmentations. Advances in neural information processing systems, 33:
327"
REFERENCES,0.4205128205128205,"5812–5823, 2020.
328"
REFERENCES,0.42153846153846153,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
329"
REFERENCES,0.42256410256410254,"bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
330"
REFERENCES,0.4235897435897436,"Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.
331"
REFERENCES,0.4246153846153846,"arXiv preprint arXiv:2106.08254, 2021.
332"
REFERENCES,0.4256410256410256,"Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
333"
REFERENCES,0.4266666666666667,"autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer
334"
REFERENCES,0.4276923076923077,"vision and pattern recognition, pages 16000–16009, 2022a.
335"
REFERENCES,0.4287179487179487,"Qiaoyu Tan, Ninghao Liu, Xiao Huang, Rui Chen, Soo-Hyun Choi, and Xia Hu. Mgae: Masked
336"
REFERENCES,0.4297435897435897,"autoencoders for self-supervised learning on graphs. arXiv preprint arXiv:2201.02534, 2022.
337"
REFERENCES,0.4307692307692308,"Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang.
338"
REFERENCES,0.4317948717948718,"GraphMAE: Self-supervised masked graph autoencoders.
In Proceedings of the 28th ACM
339"
REFERENCES,0.4328205128205128,"SIGKDD Conference on Knowledge Discovery and Data Mining, pages 594–604, 2022.
340"
REFERENCES,0.4338461538461538,"Wenxuan Tu, Qing Liao, Sihang Zhou, Xin Peng, Chuan Ma, Zhe Liu, Xinwang Liu, and Zhiping
341"
REFERENCES,0.4348717948717949,"Cai. Rare: Robust masked graph autoencoder. arXiv preprint arXiv:2304.01507, 2023.
342"
REFERENCES,0.4358974358974359,"Yijun Tian, Kaiwen Dong, Chunhui Zhang, Chuxu Zhang, and Nitesh V Chawla. Heterogeneous
343"
REFERENCES,0.4369230769230769,"graph masked autoencoders. In Proceedings of the AAAI Conference on Artificial Intelligence,
344"
REFERENCES,0.4379487179487179,"volume 37, pages 9997–10005, 2023.
345"
REFERENCES,0.438974358974359,"Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional
346"
REFERENCES,0.44,"networks. In International conference on machine learning, pages 4212–4221. PMLR, 2019.
347"
REFERENCES,0.441025641025641,"Haoyang Li, Xin Wang, Ziwei Zhang, Zehuan Yuan, Hang Li, and Wenwu Zhu. Disentangled
348"
REFERENCES,0.442051282051282,"contrastive learning on graphs. Advances in Neural Information Processing Systems, 34:21872–
349"
REFERENCES,0.4430769230769231,"21884, 2021.
350"
REFERENCES,0.4441025641025641,"Yujie Mo, Yajie Lei, Jialie Shen, Xiaoshuang Shi, Heng Tao Shen, and Xiaofeng Zhu. Disentangled
351"
REFERENCES,0.4451282051282051,"multiplex graph representation learning. In International Conference on Machine Learning, pages
352"
REFERENCES,0.4461538461538462,"24983–25005. PMLR, 2023.
353"
REFERENCES,0.4471794871794872,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
354"
REFERENCES,0.4482051282051282,"perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,
355"
REFERENCES,0.4492307692307692,"2013.
356"
REFERENCES,0.4502564102564103,"Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang. Decoupled self-
357"
REFERENCES,0.4512820512820513,"supervised learning for graphs. Advances in Neural Information Processing Systems, 35:620–634,
358"
REFERENCES,0.4523076923076923,"2022.
359"
REFERENCES,0.4533333333333333,"Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
360"
REFERENCES,0.4543589743589744,"Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.
361"
REFERENCES,0.4553846153846154,"Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive
362"
REFERENCES,0.4564102564102564,"representation learning. arXiv preprint arXiv:2006.04131, 2020.
363"
REFERENCES,0.4574358974358974,"Petar Veliˇckovi´c, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon
364"
REFERENCES,0.4584615384615385,"Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.
365"
REFERENCES,0.4594871794871795,"Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on
366"
REFERENCES,0.4605128205128205,"graphs. In International conference on machine learning, pages 4116–4126. PMLR, 2020.
367"
REFERENCES,0.46153846153846156,"Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang,
368"
REFERENCES,0.4625641025641026,"and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings
369"
REFERENCES,0.4635897435897436,"of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages
370"
REFERENCES,0.4646153846153846,"1150–1160, 2020.
371"
REFERENCES,0.46564102564102566,"Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin
372"
REFERENCES,0.4666666666666667,"Zheng, and Weiqiang Wang. What’s behind the mask: Understanding masked graph modeling
373"
REFERENCES,0.4676923076923077,"for graph autoencoders. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
374"
REFERENCES,0.4687179487179487,"Discovery and Data Mining, pages 1268–1279, 2023a.
375"
REFERENCES,0.46974358974358976,"Irina Higgins, Loic Matthey, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick,
376"
REFERENCES,0.4707692307692308,"Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
377"
REFERENCES,0.4717948717948718,"constrained variational framework. ICLR (Poster), 3, 2017.
378"
REFERENCES,0.4728205128205128,"Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng, and Shuicheng Yan. Psgan: Pose
379"
REFERENCES,0.47384615384615386,"and expression robust spatial-aware gan for customizable makeup transfer. In Proceedings of the
380"
REFERENCES,0.4748717948717949,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5194–5202, 2020.
381"
REFERENCES,0.4758974358974359,"Giangiacomo Mercatali, André Freitas, and Vikas Garg. Symmetry-induced disentanglement on
382"
REFERENCES,0.47692307692307695,"graphs. Advances in neural information processing systems, 35:31497–31511, 2022.
383"
REFERENCES,0.47794871794871796,"Mingyuan Zhou. Infinite edge partition models for overlapping community detection and link
384"
REFERENCES,0.47897435897435897,"prediction. In Artificial intelligence and statistics, pages 1135–1143. PMLR, 2015.
385"
REFERENCES,0.48,"Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos
386"
REFERENCES,0.48102564102564105,"Karantzalos, and Nikos Komodakis. What to hide from your students: Attention-guided masked
387"
REFERENCES,0.48205128205128206,"image modeling. In European Conference on Computer Vision, pages 300–318. Springer, 2022.
388"
REFERENCES,0.48307692307692307,"Yilin He, Chaojie Wang, Hao Zhang, Bo Chen, and Mingyuan Zhou. A variational edge partition
389"
REFERENCES,0.4841025641025641,"model for supervised graph representation learning. Advances in Neural Information Processing
390"
REFERENCES,0.48512820512820515,"Systems, 35:12339–12351, 2022b.
391"
REFERENCES,0.48615384615384616,"Hao Zhang, Bo Chen, Dandan Guo, and Mingyuan Zhou. Whai: Weibull hybrid autoencoding
392"
REFERENCES,0.48717948717948717,"inference for deep topic modeling. arXiv preprint arXiv:1803.01328, 2018.
393"
REFERENCES,0.4882051282051282,"Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
394"
REFERENCES,0.48923076923076925,"arXiv preprint arXiv:1609.02907, 2016a.
395"
REFERENCES,0.49025641025641026,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense
396"
REFERENCES,0.49128205128205127,"object detection. In Proceedings of the IEEE international conference on computer vision, pages
397"
REFERENCES,0.49230769230769234,"2980–2988, 2017.
398"
REFERENCES,0.49333333333333335,"Xiang Li, Tiandi Ye, Caihua Shan, Dongsheng Li, and Ming Gao. Seegera: Self-supervised semi-
399"
REFERENCES,0.49435897435897436,"implicit graph variational auto-encoders with masking. In Proceedings of the ACM web conference
400"
REFERENCES,0.49538461538461537,"2023, pages 143–153, 2023b.
401"
REFERENCES,0.49641025641025643,"Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with
402"
REFERENCES,0.49743589743589745,"graph embeddings. In International conference on machine learning, pages 40–48. PMLR, 2016.
403"
REFERENCES,0.49846153846153846,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
404"
REFERENCES,0.49948717948717947,"Advances in neural information processing systems, 30, 2017.
405"
REFERENCES,0.5005128205128205,"Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio,
406"
REFERENCES,0.5015384615384615,"et al. Graph attention networks. stat, 1050(20):10–48550, 2017.
407"
REFERENCES,0.5025641025641026,"Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information-
408"
REFERENCES,0.5035897435897436,"aware graph contrastive learning.
Advances in Neural Information Processing Systems, 34:
409"
REFERENCES,0.5046153846153846,"30414–30425, 2021.
410"
REFERENCES,0.5056410256410256,"Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer, Remi
411"
REFERENCES,0.5066666666666667,"Munos, Petar Veliˇckovi´c, and Michal Valko. Large-scale representation learning on graphs via
412"
REFERENCES,0.5076923076923077,"bootstrapping. arXiv preprint arXiv:2102.06514, 2021.
413"
REFERENCES,0.5087179487179487,"Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation
414"
REFERENCES,0.5097435897435898,"analysis to self-supervised graph neural networks. Advances in Neural Information Processing
415"
REFERENCES,0.5107692307692308,"Systems, 34:76–89, 2021.
416"
REFERENCES,0.5117948717948718,"Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, and Jie Tang.
417"
REFERENCES,0.5128205128205128,"Graphmae2: A decoding-enhanced masked self-supervised graph learner. In Proceedings of the
418"
REFERENCES,0.5138461538461538,"ACM Web Conference 2023, pages 737–746, 2023.
419"
REFERENCES,0.5148717948717949,"Ziwen Zhao, Yuhua Li, Yixiong Zou, Jiliang Tang, and Ruixuan Li. Masked graph autoencoder with
420"
REFERENCES,0.5158974358974359,"non-discrete bandwidths. arXiv preprint arXiv:2402.03814, 2024.
421"
REFERENCES,0.5169230769230769,"Yucheng Shi, Yushun Dong, Qiaoyu Tan, Jundong Li, and Ninghao Liu. Gigamae: Generalizable
422"
REFERENCES,0.517948717948718,"graph masked autoencoder via collaborative latent space reconstruction. In Proceedings of the 32nd
423"
REFERENCES,0.518974358974359,"ACM International Conference on Information and Knowledge Management, pages 2259–2269,
424"
REFERENCES,0.52,"2023.
425"
REFERENCES,0.521025641025641,"Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308,
426"
REFERENCES,0.522051282051282,"2016b.
427"
REFERENCES,0.5230769230769231,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
428"
REFERENCES,0.5241025641025641,"networks? arXiv preprint arXiv:1810.00826, 2018.
429"
REFERENCES,0.5251282051282051,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hier-
430"
REFERENCES,0.5261538461538462,"archical graph representation learning with differentiable pooling. Advances in neural information
431"
REFERENCES,0.5271794871794871,"processing systems, 31, 2018.
432"
REFERENCES,0.5282051282051282,"Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
433"
REFERENCES,0.5292307692307693,"Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.
434"
REFERENCES,0.5302564102564102,"Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
435"
REFERENCES,0.5312820512820513,"SIGKDD international conference on knowledge discovery and data mining, pages 1365–1374,
436"
REFERENCES,0.5323076923076923,"2015.
437"
REFERENCES,0.5333333333333333,"Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu,
438"
REFERENCES,0.5343589743589744,"and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint
439"
REFERENCES,0.5353846153846153,"arXiv:1707.05005, 2017.
440"
REFERENCES,0.5364102564102564,"Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-
441"
REFERENCES,0.5374358974358975,"supervised graph-level representation learning via mutual information maximization. arXiv preprint
442"
REFERENCES,0.5384615384615384,"arXiv:1908.01000, 2019.
443"
REFERENCES,0.5394871794871795,"Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated.
444"
REFERENCES,0.5405128205128205,"In International Conference on Machine Learning, pages 12121–12132. PMLR, 2021.
445"
REFERENCES,0.5415384615384615,"Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan, and B Aditya Prakash. Sub2vec: Feature
446"
REFERENCES,0.5425641025641026,"learning for subgraphs. In Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia
447"
REFERENCES,0.5435897435897435,"Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part II 22,
448"
REFERENCES,0.5446153846153846,"pages 170–182. Springer, 2018.
449"
REFERENCES,0.5456410256410257,"Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
450"
REFERENCES,0.5466666666666666,"of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
451"
REFERENCES,0.5476923076923077,"pages 855–864, 2016.
452"
REFERENCES,0.5487179487179488,"Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
453"
REFERENCES,0.5497435897435897,"learning research, 9(11), 2008.
454"
REFERENCES,0.5507692307692308,"Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM
455"
REFERENCES,0.5517948717948717,"transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.
456"
REFERENCES,0.5528205128205128,"Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using pagerank vectors.
457"
REFERENCES,0.5538461538461539,"In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pages
458"
REFERENCES,0.5548717948717948,"475–486. IEEE, 2006.
459"
REFERENCES,0.5558974358974359,"A
Appendix / supplemental material
460"
REFERENCES,0.556923076923077,"Optionally include supplemental material (complete proofs, additional experiments and plots) in
461"
REFERENCES,0.5579487179487179,"appendix. All such materials SHOULD be included in the main submission.
462"
REFERENCES,0.558974358974359,"A.1
Ablation Study
463"
REFERENCES,0.56,"A.1.1
Number of factors
464"
REFERENCES,0.561025641025641,"One of the crucial hyperparameters in DiGGR is the number of latent factors, denoted as K. When
465"
REFERENCES,0.5620512820512821,"K = 1 DiGGR degenerates into ordinary GMAE, only performing random masking over the entire
466"
REFERENCES,0.563076923076923,"input graph on the nodes. The influence of tuning K is illustrated in Figure 5. Given the relatively
467"
REFERENCES,0.5641025641025641,"small size of the graphs in the dataset, the number of meaningful latent disentangled factor z is
468"
REFERENCES,0.5651282051282052,"not expected to be very large. The optimal number of z that maximizes performance tends to be
469"
REFERENCES,0.5661538461538461,"concentrated in the range of 2-4.
470"
REFERENCES,0.5671794871794872,"1
2
4
8
16
Factor Number K 82.5 83.5 84.5 85.5 86.5"
REFERENCES,0.5682051282051283,Accuracy (%)
REFERENCES,0.5692307692307692,(a) Cora
REFERENCES,0.5702564102564103,"1
2
4
8
16
Factor Number K 86 87 88 89 90"
REFERENCES,0.5712820512820512,Accuracy (%)
REFERENCES,0.5723076923076923,(b) MUTAG
REFERENCES,0.5733333333333334,"1
2
4
8
16
Factor Number K 72.5 73.0 73.5 74.0 74.5"
REFERENCES,0.5743589743589743,Accuracy (%)
REFERENCES,0.5753846153846154,(c) Citeseer
REFERENCES,0.5764102564102564,"1
2
4
8
16
Factor Number K 72 74 76 78 80"
REFERENCES,0.5774358974358974,Accuracy (%)
REFERENCES,0.5784615384615385,(d) IMDB-B
REFERENCES,0.5794871794871795,"Figure 5: Performance of the task under different choices of latent factor number K, where the
horizontal axis represents the change in K and the vertical axis is accuracy."
REFERENCES,0.5805128205128205,"A.1.2
Representation for downstream tasks
471"
REFERENCES,0.5815384615384616,"We investigate the impact of various combinations of representation levels on downstream tasks. As
472"
REFERENCES,0.5825641025641025,"illustrated in Table 4, for the node classification task, both Hd and Hg are required, i.e., concatenating
473"
REFERENCES,0.5835897435897436,"them in feature dimension, whereas for the graph classification task, Hd alone is sufficient. This
474"
REFERENCES,0.5846153846153846,"difference may be due to the former not utilizing pooling operations, while the latter does. Specifically,
475"
REFERENCES,0.5856410256410256,"the graph pooling operation aggregates information from all nodes, providing a comprehensive
476"
REFERENCES,0.5866666666666667,"view of the entire graph structure. Thus, in node classification, where the node representation has
477"
REFERENCES,0.5876923076923077,"not undergone pooling, a graph-level representation (Hg) is more critical. In contrast, in graph
478"
REFERENCES,0.5887179487179487,"classification, the node representation undergoes pooling, making disentangled information Hd more
479"
REFERENCES,0.5897435897435898,effective.
REFERENCES,0.5907692307692308,"Table 4: The average accuracy of datasets is calculated through 5 random initialization tests when
using different representations."
REFERENCES,0.5917948717948718,"Hd
Hg
Cora
IMDB-MULTI
Citeseer
PROTEINS"
REFERENCES,0.5928205128205128,"✓
61.10 ± 1.83
54.77 ± 2.63
71.82 ± 0.98
77.76 ± 2.46
✓
84.22 ± 0.38
51.62 ± 0.61
73.41 ± 0.43
75.52 ± 0.49
✓
✓
84.96 ± 0.32
53.69 ± 2.06
73.98 ± 0.27
77.61 ± 0.97 480"
REFERENCES,0.5938461538461538,"A.2
Implementation Details
481"
REFERENCES,0.5948717948717949,"Environment
All experiments are conducted on Linux servers equipped with an 12th Gen Intel(R)
482"
REFERENCES,0.5958974358974359,"Core(TM) i7-12700, 256GB RAM and a NVIDIA 3090 GPU. Models of node and graph classification
483"
REFERENCES,0.5969230769230769,"are implemented in PyTorch version 1.12.1, scikit-learn version 1.0.2 and Python 3.7.
484"
REFERENCES,0.597948717948718,"Experiment Setup for Node Classification
The node classification task involves predicting the
485"
REFERENCES,0.598974358974359,"unknown node labels in networks. Cora, Citeseer, and Pubmed are employed for transductive learning,
486"
REFERENCES,0.6,"whereas PPI follows the inductive setup outlined in GraphSage [Hamilton et al., 2017]. For evaluation,
487"
REFERENCES,0.601025641025641,Table 5: Statistics for node classification datasets.
REFERENCES,0.602051282051282,"Dataset
Cora
Citeseer
Pubmed
PPI"
REFERENCES,0.6030769230769231,Statistics
REFERENCES,0.6041025641025641,"# node
2708
3327
19717
56944
# feature
1433
3703
500
50
# edges
5429
4732
44338
818736
# classes
7(s)
6(s)
3(s)
121(m)"
REFERENCES,0.6051282051282051,Hyper-parameters
REFERENCES,0.6061538461538462,"Mask Rate
0.5
0.5
0.75
0.5
Hidden Size
512
512
1024
1024
Max Epoch
1750
200
1000
1000
λd; λg; λz
1; 1; 1
1; 1; 2
1; 1; 1
1; 1; 1
Learning Rate
0.001
0.0005
0.001
0.0001
Factor_Num
4
4
2
2"
REFERENCES,0.6071794871794872,"we use the concatenated representations of Hd and Hg in the feature dimension for the downstream
488"
REFERENCES,0.6082051282051282,"task. We then train a linear classifier, report the mean accuracy on the test nodes through 5 random
489"
REFERENCES,0.6092307692307692,"initializations. The graph encoder GNNenc(.) and decoder GNNdec(.) are both specified as standard
490"
REFERENCES,0.6102564102564103,"GAT [Velickovic et al., 2017].We train the model using Adam Optimizer with β1 = 0.9, β2 = 0.999,
491"
REFERENCES,0.6112820512820513,"ϵ = 1 × 108, and we use the cosine learning rate decay without warmup. We follow the public data
492"
REFERENCES,0.6123076923076923,"splits of Cora, Citeseer, and PubMed.
493"
REFERENCES,0.6133333333333333,"Experiment Setup for Graph Classification
The graph classification experiment was conducted on
494"
REFERENCES,0.6143589743589744,"7 benchmarks, in which node labels are used as input features in MUTAG, PROTEINS and NCI1, and
495"
REFERENCES,0.6153846153846154,"node degrees are used in IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, and COLLAB. The
496"
REFERENCES,0.6164102564102564,"backbone of encoder and decoder is GIN [Xu et al., 2018], which is commonly used in previous graph
497"
REFERENCES,0.6174358974358974,"classification works. The evaluation protocol primarily follows GraphMAE [Hou et al., 2022]. Notice
498"
REFERENCES,0.6184615384615385,"that we only utilize the factor-wise latent representation Hd for the downstream task. Subsequently,
499"
REFERENCES,0.6194871794871795,"we feed it into a downstream LIBSVM [Chang and Lin, 2011] classifier to predict the label and
500"
REFERENCES,0.6205128205128205,"report the mean 10-fold cross-validation accuracy with standard deviation after 5 runs. We set the
501"
REFERENCES,0.6215384615384615,"initial learning rate to 0.0005 with cosine learning rate decay for most cases. For the evaluation, the
502"
REFERENCES,0.6225641025641026,"parameter C of SVM is searched in the sets {103, ..., 10}.
503"
REFERENCES,0.6235897435897436,"Data Preparation
The node features for the citation networks (Cora, Citeseer, Pubmed) are bag-of-
504"
REFERENCES,0.6246153846153846,"words document representations. For the protein-protein interaction networks (PPI), the features of
505"
REFERENCES,0.6256410256410256,"each node are composed of positional gene sets, motif gene sets and immunological signatures (50 in
506"
REFERENCES,0.6266666666666667,"total). For graph classification, the MUTAG, PROTEINS, and NCI1 datasets utilize node labels as
507"
REFERENCES,0.6276923076923077,"node features, represented in the form of one-hot encoding. For IMDB-B, IMDB-M, REDDIT-B, and
508"
REFERENCES,0.6287179487179487,"COLLAB, which lack node features, we utilize the node degree and convert it into a one-hot encoding
509"
REFERENCES,0.6297435897435898,"as a substitute feature. The maximum node degree is set to 400. Nodes with degrees surpassing 400
510"
REFERENCES,0.6307692307692307,"are uniformly treated as having a degree of 400, following the methodology of GraphMAE[Hou et al.,
511"
REFERENCES,0.6317948717948718,"2022]. Table 5 and Table 6 show the specific statistics of used datasets.
512"
REFERENCES,0.6328205128205128,"Details for Visualization
MUTAG is selected as the representative benchmark for visualization
513"
REFERENCES,0.6338461538461538,"in 4.3. The MUTAG dataset comprises 3,371 nodes with seven node types. The distribution is
514"
REFERENCES,0.6348717948717949,"highly skewed, as 3,333 nodes belong to three types, while the remaining four types collectively
515"
REFERENCES,0.6358974358974359,"represent less than 1.2% of the nodes. For clarity in legend display, we have visualized only the nodes
516"
REFERENCES,0.6369230769230769,"belonging to the first three types.
517"
REFERENCES,0.637948717948718,"A.3
Disentangled Representations Visualization
518"
REFERENCES,0.638974358974359,"We chose PROTEINS and IMDB-MULTI as representatives of the graph classification dataset, and
519"
REFERENCES,0.64,"followed the same methodology as in Section 4.3 to visualize their representation correlation matrices
520"
REFERENCES,0.6410256410256411,"on GraphMAE, and community representation correlation matrices on DiGGR, respectively. The
521"
REFERENCES,0.642051282051282,"feature dimensions of PROTEINS and IMDB-MULTI are both 512 dimensions, and the number of
522"
REFERENCES,0.6430769230769231,"communities is set to 4.
523"
REFERENCES,0.6441025641025641,Table 6: Statistics for graph classification datasets.
REFERENCES,0.6451282051282051,"Dataset
IMDB-B
IMDB-M
PROTEINS
COLLAB
MUTAG
REDDIT-B
NCI1"
REFERENCES,0.6461538461538462,Statistics
REFERENCES,0.6471794871794871,"Avg. # node
19.8
13.0
39.1
74.5
17.9
429.7
29.8
# features
136
89
3
401
7
401
37
# graphs
1000
1500
1113
5000
188
2000
4110
# classes
2
3
2
3
2
2
2"
REFERENCES,0.6482051282051282,"Hyper-
parameters"
REFERENCES,0.6492307692307693,"Mask Rate
0.5
0.5
0.5
0.75
0.75
0.75
0.25
Hidden Size
512
512
512
256
32
512
1024
Max Epoch
300
200
50
20
20
200
200
Learning Rate
0.0001
0.001
0.0005
0.001
0.001
0.0005
0.0005
λd; λg; λz
1; 1; 1
1; 1; 1
1; 1; 1
1; 1; 1
1; 1; 1
1; 1; 1
1; 0.5; 1
Batch_Size
32
32
32
32
32
16
32
Pooling_Type
mean
mean
max
max
sum
max
max
Factor_Num
2
4
4
4
2
2
4"
REFERENCES,0.6502564102564102,"The result is presented in Figure 6. We can see from the results that the graph representations of
524"
REFERENCES,0.6512820512820513,"GraphMAE are entangled. In contrast, the correlation pattern exhibited by DiGGR reveals four
525"
REFERENCES,0.6523076923076923,"distinct diagonal blocks. This suggests that DiGGR is proficient at capturing mutually exclusive
526"
REFERENCES,0.6533333333333333,"information within the latent factor, resulting in disentangled representations.
527"
REFERENCES,0.6543589743589744,"(a) PROTEINS, GraphMAE
(b) PROTEINS, DiGGR"
REFERENCES,0.6553846153846153,"(c) IMDB-MULTI, GraphMAE
(d) IMDB-MULTI, DiGGR"
REFERENCES,0.6564102564102564,"Figure 6: The absolute correlation between the representations learned by GraphMAE and DiGGR is
measured on the PROTEINS and IMDB-MULTI datasets when K = 4."
REFERENCES,0.6574358974358975,"A.4
Training Algorithm
528"
REFERENCES,0.6584615384615384,Algorithm 1 The Overall Training Algorithm of DiGGR
REFERENCES,0.6594871794871795,"1: Input: Graph G = {V, A, X}; latent factor number K."
REFERENCES,0.6605128205128206,"2: Parameters: Θ in the inference network of Latent Factor Learning phase, Ωin the encoding
network of DiGGR, Ψ in the decoding network of DiGGR."
REFERENCES,0.6615384615384615,"3: Initialize Θ, Ω, and Ψ;"
REFERENCES,0.6625641025641026,"4: for iter = 1,2, · ·· do"
REFERENCES,0.6635897435897435,"5:
Infer the variational posterior of zu based on Eq. 5;"
REFERENCES,0.6646153846153846,"6:
Sample latent factors zu from the variational posterior according to Eq. 6;"
REFERENCES,0.6656410256410257,"7:
Factorize the graph G into K factor-wise groups {G(k)}K
k=1 by node and edge factorization
methods;"
REFERENCES,0.6666666666666666,"8:
Encoding {G(k)}K
k=1 via latent factor-wise Graph Masked Autoencoder according to Eq. 8;"
REFERENCES,0.6676923076923077,"9:
Encoding G via graph-level graph masked autoencoder according to Eq. 11;"
REFERENCES,0.6687179487179488,"10:
Calculate ∇Θ,Ω,ΨL(Θ, Ω, Ψ; G) according to Eq. 13, and update parameters Θ, Ω, and Ψ
jointly."
REFERENCES,0.6697435897435897,11: end for=0
REFERENCES,0.6707692307692308,"A.5
Broader Impacts
529"
REFERENCES,0.6717948717948717,"This paper presents work whose goal is to advance the field of Machine Learning. There are many
530"
REFERENCES,0.6728205128205128,"potential societal consequences of our work, none which we feel must be specifically highlighted
531"
REFERENCES,0.6738461538461539,"here.
532"
REFERENCES,0.6748717948717948,"A.6
Limitations
533"
REFERENCES,0.6758974358974359,"Despite the promising experimental justifications, our work might potentially suffer from limitation:
534"
REFERENCES,0.676923076923077,"Although the complexity of the model is discussed in Section 3.4, and it is comparable to previously
535"
REFERENCES,0.6779487179487179,"published work, extending DiGGR to extremely large graph datasets remains challenging at this stage
536"
REFERENCES,0.678974358974359,"due to the incorporation of an additional probabilistic model into the generative graph framework.
537"
REFERENCES,0.68,"One potential solution to this problem could be utilizing PPR-Nibble [Andersen et al., 2006] for
538"
REFERENCES,0.681025641025641,"efficient implementation, a method that has proven effective in some graph generative models [Hou
539"
REFERENCES,0.6820512820512821,"et al., 2023]. This approach will be pursued in our future work.
540"
REFERENCES,0.683076923076923,"NeurIPS Paper Checklist
541"
CLAIMS,0.6841025641025641,"1. Claims
542"
CLAIMS,0.6851282051282052,"Question: Do the main claims made in the abstract and introduction accurately reflect the
543"
CLAIMS,0.6861538461538461,"paper’s contributions and scope?
544"
CLAIMS,0.6871794871794872,"Answer: [Yes]
545"
CLAIMS,0.6882051282051282,"Justification: We listed our main contribution in the last paragraph of Section 1
546"
CLAIMS,0.6892307692307692,"Guidelines:
547"
CLAIMS,0.6902564102564103,"• The answer NA means that the abstract and introduction do not include the claims
548"
CLAIMS,0.6912820512820513,"made in the paper.
549"
CLAIMS,0.6923076923076923,"• The abstract and/or introduction should clearly state the claims made, including the
550"
CLAIMS,0.6933333333333334,"contributions made in the paper and important assumptions and limitations. A No or
551"
CLAIMS,0.6943589743589743,"NA answer to this question will not be perceived well by the reviewers.
552"
CLAIMS,0.6953846153846154,"• The claims made should match theoretical and experimental results, and reflect how
553"
CLAIMS,0.6964102564102564,"much the results can be expected to generalize to other settings.
554"
CLAIMS,0.6974358974358974,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
555"
CLAIMS,0.6984615384615385,"are not attained by the paper.
556"
LIMITATIONS,0.6994871794871795,"2. Limitations
557"
LIMITATIONS,0.7005128205128205,"Question: Does the paper discuss the limitations of the work performed by the authors?
558"
LIMITATIONS,0.7015384615384616,"Answer: [Yes]
559"
LIMITATIONS,0.7025641025641025,"Justification: We’ve discuss about the limitations in Appendix A.6
560"
LIMITATIONS,0.7035897435897436,"Guidelines:
561"
LIMITATIONS,0.7046153846153846,"• The answer NA means that the paper has no limitation while the answer No means that
562"
LIMITATIONS,0.7056410256410256,"the paper has limitations, but those are not discussed in the paper.
563"
LIMITATIONS,0.7066666666666667,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
564"
LIMITATIONS,0.7076923076923077,"• The paper should point out any strong assumptions and how robust the results are to
565"
LIMITATIONS,0.7087179487179487,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
566"
LIMITATIONS,0.7097435897435898,"model well-specification, asymptotic approximations only holding locally). The authors
567"
LIMITATIONS,0.7107692307692308,"should reflect on how these assumptions might be violated in practice and what the
568"
LIMITATIONS,0.7117948717948718,"implications would be.
569"
LIMITATIONS,0.7128205128205128,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
570"
LIMITATIONS,0.7138461538461538,"only tested on a few datasets or with a few runs. In general, empirical results often
571"
LIMITATIONS,0.7148717948717949,"depend on implicit assumptions, which should be articulated.
572"
LIMITATIONS,0.7158974358974359,"• The authors should reflect on the factors that influence the performance of the approach.
573"
LIMITATIONS,0.7169230769230769,"For example, a facial recognition algorithm may perform poorly when image resolution
574"
LIMITATIONS,0.717948717948718,"is low or images are taken in low lighting. Or a speech-to-text system might not be
575"
LIMITATIONS,0.718974358974359,"used reliably to provide closed captions for online lectures because it fails to handle
576"
LIMITATIONS,0.72,"technical jargon.
577"
LIMITATIONS,0.721025641025641,"• The authors should discuss the computational efficiency of the proposed algorithms
578"
LIMITATIONS,0.7220512820512821,"and how they scale with dataset size.
579"
LIMITATIONS,0.7230769230769231,"• If applicable, the authors should discuss possible limitations of their approach to
580"
LIMITATIONS,0.7241025641025641,"address problems of privacy and fairness.
581"
LIMITATIONS,0.7251282051282051,"• While the authors might fear that complete honesty about limitations might be used by
582"
LIMITATIONS,0.7261538461538461,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
583"
LIMITATIONS,0.7271794871794872,"limitations that aren’t acknowledged in the paper. The authors should use their best
584"
LIMITATIONS,0.7282051282051282,"judgment and recognize that individual actions in favor of transparency play an impor-
585"
LIMITATIONS,0.7292307692307692,"tant role in developing norms that preserve the integrity of the community. Reviewers
586"
LIMITATIONS,0.7302564102564103,"will be specifically instructed to not penalize honesty concerning limitations.
587"
THEORY ASSUMPTIONS AND PROOFS,0.7312820512820513,"3. Theory Assumptions and Proofs
588"
THEORY ASSUMPTIONS AND PROOFS,0.7323076923076923,"Question: For each theoretical result, does the paper provide the full set of assumptions and
589"
THEORY ASSUMPTIONS AND PROOFS,0.7333333333333333,"a complete (and correct) proof?
590"
THEORY ASSUMPTIONS AND PROOFS,0.7343589743589743,"Answer: [NA]
591"
THEORY ASSUMPTIONS AND PROOFS,0.7353846153846154,"Justification: This paper is not focus on theoretical explanations and assumptions
592"
THEORY ASSUMPTIONS AND PROOFS,0.7364102564102564,"Guidelines:
593"
THEORY ASSUMPTIONS AND PROOFS,0.7374358974358974,"• The answer NA means that the paper does not include theoretical results.
594"
THEORY ASSUMPTIONS AND PROOFS,0.7384615384615385,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
595"
THEORY ASSUMPTIONS AND PROOFS,0.7394871794871795,"referenced.
596"
THEORY ASSUMPTIONS AND PROOFS,0.7405128205128205,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
597"
THEORY ASSUMPTIONS AND PROOFS,0.7415384615384616,"• The proofs can either appear in the main paper or the supplemental material, but if
598"
THEORY ASSUMPTIONS AND PROOFS,0.7425641025641025,"they appear in the supplemental material, the authors are encouraged to provide a short
599"
THEORY ASSUMPTIONS AND PROOFS,0.7435897435897436,"proof sketch to provide intuition.
600"
THEORY ASSUMPTIONS AND PROOFS,0.7446153846153846,"• Inversely, any informal proof provided in the core of the paper should be complemented
601"
THEORY ASSUMPTIONS AND PROOFS,0.7456410256410256,"by formal proofs provided in appendix or supplemental material.
602"
THEORY ASSUMPTIONS AND PROOFS,0.7466666666666667,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7476923076923077,"4. Experimental Result Reproducibility
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7487179487179487,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497435897435898,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7507692307692307,"of the paper (regardless of whether the code and data are provided or not)?
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7517948717948718,"Answer: [Yes]
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7528205128205128,"Justification: We have listed the specific settings of the experiment in the appendix A.2 of
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7538461538461538,"the paper, including the datasets used and the hyperparameter settings of the model. We
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7548717948717949,"have also uploaded the code in the supplementary material.
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7558974358974359,"Guidelines:
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7569230769230769,"• The answer NA means that the paper does not include experiments.
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.757948717948718,"• If the paper includes experiments, a No answer to this question will not be perceived
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7589743589743589,"well by the reviewers: Making the paper reproducible is important, regardless of
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76,"whether the code and data are provided or not.
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7610256410256411,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762051282051282,"to make their results reproducible or verifiable.
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7630769230769231,"• Depending on the contribution, reproducibility can be accomplished in various ways.
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.764102564102564,"For example, if the contribution is a novel architecture, describing the architecture fully
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7651282051282051,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7661538461538462,"be necessary to either make it possible for others to replicate the model with the same
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7671794871794871,"dataset, or provide access to the model. In general. releasing code and data is often
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7682051282051282,"one good way to accomplish this, but reproducibility can also be provided via detailed
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7692307692307693,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7702564102564102,"of a large language model), releasing of a model checkpoint, or other means that are
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7712820512820513,"appropriate to the research performed.
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723076923076924,"• While NeurIPS does not require releasing code, the conference does require all submis-
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733333333333333,"sions to provide some reasonable avenue for reproducibility, which may depend on the
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7743589743589744,"nature of the contribution. For example
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7753846153846153,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7764102564102564,"to reproduce that algorithm.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7774358974358975,"(b) If the contribution is primarily a new model architecture, the paper should describe
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7784615384615384,"the architecture clearly and fully.
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7794871794871795,"(c) If the contribution is a new model (e.g., a large language model), then there should
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7805128205128206,"either be a way to access this model for reproducing the results or a way to reproduce
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7815384615384615,"the model (e.g., with an open-source dataset or instructions for how to construct
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7825641025641026,"the dataset).
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7835897435897435,"(d) We recognize that reproducibility may be tricky in some cases, in which case
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7846153846153846,"authors are welcome to describe the particular way they provide for reproducibility.
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7856410256410257,"In the case of closed-source models, it may be that access to the model is limited in
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7866666666666666,"some way (e.g., to registered users), but it should be possible for other researchers
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7876923076923077,"to have some path to reproducing or verifying the results.
643"
OPEN ACCESS TO DATA AND CODE,0.7887179487179488,"5. Open access to data and code
644"
OPEN ACCESS TO DATA AND CODE,0.7897435897435897,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
645"
OPEN ACCESS TO DATA AND CODE,0.7907692307692308,"tions to faithfully reproduce the main experimental results, as described in supplemental
646"
OPEN ACCESS TO DATA AND CODE,0.7917948717948718,"material?
647"
OPEN ACCESS TO DATA AND CODE,0.7928205128205128,"Answer: [Yes]
648"
OPEN ACCESS TO DATA AND CODE,0.7938461538461539,"Justification: We have uploaded the code in the supplementary material.
649"
OPEN ACCESS TO DATA AND CODE,0.7948717948717948,"Guidelines:
650"
OPEN ACCESS TO DATA AND CODE,0.7958974358974359,"• The answer NA means that paper does not include experiments requiring code.
651"
OPEN ACCESS TO DATA AND CODE,0.796923076923077,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
652"
OPEN ACCESS TO DATA AND CODE,0.7979487179487179,"public/guides/CodeSubmissionPolicy) for more details.
653"
OPEN ACCESS TO DATA AND CODE,0.798974358974359,"• While we encourage the release of code and data, we understand that this might not be
654"
OPEN ACCESS TO DATA AND CODE,0.8,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
655"
OPEN ACCESS TO DATA AND CODE,0.801025641025641,"including code, unless this is central to the contribution (e.g., for a new open-source
656"
OPEN ACCESS TO DATA AND CODE,0.8020512820512821,"benchmark).
657"
OPEN ACCESS TO DATA AND CODE,0.803076923076923,"• The instructions should contain the exact command and environment needed to run to
658"
OPEN ACCESS TO DATA AND CODE,0.8041025641025641,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
659"
OPEN ACCESS TO DATA AND CODE,0.8051282051282052,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
660"
OPEN ACCESS TO DATA AND CODE,0.8061538461538461,"• The authors should provide instructions on data access and preparation, including how
661"
OPEN ACCESS TO DATA AND CODE,0.8071794871794872,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
662"
OPEN ACCESS TO DATA AND CODE,0.8082051282051282,"• The authors should provide scripts to reproduce all experimental results for the new
663"
OPEN ACCESS TO DATA AND CODE,0.8092307692307692,"proposed method and baselines. If only a subset of experiments are reproducible, they
664"
OPEN ACCESS TO DATA AND CODE,0.8102564102564103,"should state which ones are omitted from the script and why.
665"
OPEN ACCESS TO DATA AND CODE,0.8112820512820513,"• At submission time, to preserve anonymity, the authors should release anonymized
666"
OPEN ACCESS TO DATA AND CODE,0.8123076923076923,"versions (if applicable).
667"
OPEN ACCESS TO DATA AND CODE,0.8133333333333334,"• Providing as much information as possible in supplemental material (appended to the
668"
OPEN ACCESS TO DATA AND CODE,0.8143589743589743,"paper) is recommended, but including URLs to data and code is permitted.
669"
OPEN ACCESS TO DATA AND CODE,0.8153846153846154,"6. Experimental Setting/Details
670"
OPEN ACCESS TO DATA AND CODE,0.8164102564102564,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
671"
OPEN ACCESS TO DATA AND CODE,0.8174358974358974,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
672"
OPEN ACCESS TO DATA AND CODE,0.8184615384615385,"results?
673"
OPEN ACCESS TO DATA AND CODE,0.8194871794871795,"Answer: [Yes]
674"
OPEN ACCESS TO DATA AND CODE,0.8205128205128205,"Justification: We have listed the specific exprimental setup in Appendix A.2
675"
OPEN ACCESS TO DATA AND CODE,0.8215384615384616,"Guidelines:
676"
OPEN ACCESS TO DATA AND CODE,0.8225641025641026,"• The answer NA means that the paper does not include experiments.
677"
OPEN ACCESS TO DATA AND CODE,0.8235897435897436,"• The experimental setting should be presented in the core of the paper to a level of detail
678"
OPEN ACCESS TO DATA AND CODE,0.8246153846153846,"that is necessary to appreciate the results and make sense of them.
679"
OPEN ACCESS TO DATA AND CODE,0.8256410256410256,"• The full details can be provided either with the code, in appendix, or as supplemental
680"
OPEN ACCESS TO DATA AND CODE,0.8266666666666667,"material.
681"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8276923076923077,"7. Experiment Statistical Significance
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8287179487179487,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
683"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8297435897435897,"information about the statistical significance of the experiments?
684"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8307692307692308,"Answer: [Yes]
685"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8317948717948718,"Justification: In the main experiment 4, we ran 5 different random seeds for each dataset
686"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8328205128205128,"and reported the average results and variances in the table6 and 5.
687"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8338461538461538,"Guidelines:
688"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8348717948717949,"• The answer NA means that the paper does not include experiments.
689"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8358974358974359,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
690"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8369230769230769,"dence intervals, or statistical significance tests, at least for the experiments that support
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.837948717948718,"the main claims of the paper.
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.838974358974359,"• The factors of variability that the error bars are capturing should be clearly stated (for
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.84,"example, train/test split, initialization, random drawing of some parameter, or overall
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.841025641025641,"run with given experimental conditions).
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8420512820512821,"• The method for calculating the error bars should be explained (closed form formula,
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8430769230769231,"call to a library function, bootstrap, etc.)
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8441025641025641,"• The assumptions made should be given (e.g., Normally distributed errors).
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8451282051282051,"• It should be clear whether the error bar is the standard deviation or the standard error
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8461538461538461,"of the mean.
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8471794871794872,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8482051282051282,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8492307692307692,"of Normality of errors is not verified.
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8502564102564103,"• For asymmetric distributions, the authors should be careful not to show in tables or
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8512820512820513,"figures symmetric error bars that would yield results that are out of range (e.g. negative
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8523076923076923,"error rates).
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8533333333333334,"• If error bars are reported in tables or plots, The authors should explain in the text how
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8543589743589743,"they were calculated and reference the corresponding figures or tables in the text.
708"
EXPERIMENTS COMPUTE RESOURCES,0.8553846153846154,"8. Experiments Compute Resources
709"
EXPERIMENTS COMPUTE RESOURCES,0.8564102564102564,"Question: For each experiment, does the paper provide sufficient information on the com-
710"
EXPERIMENTS COMPUTE RESOURCES,0.8574358974358974,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
711"
EXPERIMENTS COMPUTE RESOURCES,0.8584615384615385,"the experiments?
712"
EXPERIMENTS COMPUTE RESOURCES,0.8594871794871795,"Answer: [Yes]
713"
EXPERIMENTS COMPUTE RESOURCES,0.8605128205128205,"Justification: We listed the experiment environment in AppendixA.2
714"
EXPERIMENTS COMPUTE RESOURCES,0.8615384615384616,"Guidelines:
715"
EXPERIMENTS COMPUTE RESOURCES,0.8625641025641025,"• The answer NA means that the paper does not include experiments.
716"
EXPERIMENTS COMPUTE RESOURCES,0.8635897435897436,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
717"
EXPERIMENTS COMPUTE RESOURCES,0.8646153846153846,"or cloud provider, including relevant memory and storage.
718"
EXPERIMENTS COMPUTE RESOURCES,0.8656410256410256,"• The paper should provide the amount of compute required for each of the individual
719"
EXPERIMENTS COMPUTE RESOURCES,0.8666666666666667,"experimental runs as well as estimate the total compute.
720"
EXPERIMENTS COMPUTE RESOURCES,0.8676923076923077,"• The paper should disclose whether the full research project required more compute
721"
EXPERIMENTS COMPUTE RESOURCES,0.8687179487179487,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
722"
EXPERIMENTS COMPUTE RESOURCES,0.8697435897435898,"didn’t make it into the paper).
723"
CODE OF ETHICS,0.8707692307692307,"9. Code Of Ethics
724"
CODE OF ETHICS,0.8717948717948718,"Question: Does the research conducted in the paper conform, in every respect, with the
725"
CODE OF ETHICS,0.8728205128205129,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
726"
CODE OF ETHICS,0.8738461538461538,"Answer: [Yes]
727"
CODE OF ETHICS,0.8748717948717949,"Justification: We conform with the NeurIPS Code of Ethics in every respect for this paper.
728"
CODE OF ETHICS,0.8758974358974358,"Guidelines:
729"
CODE OF ETHICS,0.8769230769230769,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
730"
CODE OF ETHICS,0.877948717948718,"• If the authors answer No, they should explain the special circumstances that require a
731"
CODE OF ETHICS,0.8789743589743589,"deviation from the Code of Ethics.
732"
CODE OF ETHICS,0.88,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
733"
CODE OF ETHICS,0.8810256410256411,"eration due to laws or regulations in their jurisdiction).
734"
BROADER IMPACTS,0.882051282051282,"10. Broader Impacts
735"
BROADER IMPACTS,0.8830769230769231,"Question: Does the paper discuss both potential positive societal impacts and negative
736"
BROADER IMPACTS,0.884102564102564,"societal impacts of the work performed?
737"
BROADER IMPACTS,0.8851282051282051,"Answer: [Yes]
738"
BROADER IMPACTS,0.8861538461538462,"Justification: We have discussed in appendix A.5
739"
BROADER IMPACTS,0.8871794871794871,"Guidelines:
740"
BROADER IMPACTS,0.8882051282051282,"• The answer NA means that there is no societal impact of the work performed.
741"
BROADER IMPACTS,0.8892307692307693,"• If the authors answer NA or No, they should explain why their work has no societal
742"
BROADER IMPACTS,0.8902564102564102,"impact or why the paper does not address societal impact.
743"
BROADER IMPACTS,0.8912820512820513,"• Examples of negative societal impacts include potential malicious or unintended uses
744"
BROADER IMPACTS,0.8923076923076924,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
745"
BROADER IMPACTS,0.8933333333333333,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
746"
BROADER IMPACTS,0.8943589743589744,"groups), privacy considerations, and security considerations.
747"
BROADER IMPACTS,0.8953846153846153,"• The conference expects that many papers will be foundational research and not tied
748"
BROADER IMPACTS,0.8964102564102564,"to particular applications, let alone deployments. However, if there is a direct path to
749"
BROADER IMPACTS,0.8974358974358975,"any negative applications, the authors should point it out. For example, it is legitimate
750"
BROADER IMPACTS,0.8984615384615384,"to point out that an improvement in the quality of generative models could be used to
751"
BROADER IMPACTS,0.8994871794871795,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
752"
BROADER IMPACTS,0.9005128205128206,"that a generic algorithm for optimizing neural networks could enable people to train
753"
BROADER IMPACTS,0.9015384615384615,"models that generate Deepfakes faster.
754"
BROADER IMPACTS,0.9025641025641026,"• The authors should consider possible harms that could arise when the technology is
755"
BROADER IMPACTS,0.9035897435897436,"being used as intended and functioning correctly, harms that could arise when the
756"
BROADER IMPACTS,0.9046153846153846,"technology is being used as intended but gives incorrect results, and harms following
757"
BROADER IMPACTS,0.9056410256410257,"from (intentional or unintentional) misuse of the technology.
758"
BROADER IMPACTS,0.9066666666666666,"• If there are negative societal impacts, the authors could also discuss possible mitigation
759"
BROADER IMPACTS,0.9076923076923077,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
760"
BROADER IMPACTS,0.9087179487179488,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
761"
BROADER IMPACTS,0.9097435897435897,"feedback over time, improving the efficiency and accessibility of ML).
762"
SAFEGUARDS,0.9107692307692308,"11. Safeguards
763"
SAFEGUARDS,0.9117948717948718,"Question: Does the paper describe safeguards that have been put in place for responsible
764"
SAFEGUARDS,0.9128205128205128,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
765"
SAFEGUARDS,0.9138461538461539,"image generators, or scraped datasets)?
766"
SAFEGUARDS,0.9148717948717948,"Answer: [NA]
767"
SAFEGUARDS,0.9158974358974359,"Justification: this paper poses no such risks.
768"
SAFEGUARDS,0.916923076923077,"Guidelines:
769"
SAFEGUARDS,0.9179487179487179,"• The answer NA means that the paper poses no such risks.
770"
SAFEGUARDS,0.918974358974359,"• Released models that have a high risk for misuse or dual-use should be released with
771"
SAFEGUARDS,0.92,"necessary safeguards to allow for controlled use of the model, for example by requiring
772"
SAFEGUARDS,0.921025641025641,"that users adhere to usage guidelines or restrictions to access the model or implementing
773"
SAFEGUARDS,0.9220512820512821,"safety filters.
774"
SAFEGUARDS,0.9230769230769231,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
775"
SAFEGUARDS,0.9241025641025641,"should describe how they avoided releasing unsafe images.
776"
SAFEGUARDS,0.9251282051282051,"• We recognize that providing effective safeguards is challenging, and many papers do
777"
SAFEGUARDS,0.9261538461538461,"not require this, but we encourage authors to take this into account and make a best
778"
SAFEGUARDS,0.9271794871794872,"faith effort.
779"
LICENSES FOR EXISTING ASSETS,0.9282051282051282,"12. Licenses for existing assets
780"
LICENSES FOR EXISTING ASSETS,0.9292307692307692,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
781"
LICENSES FOR EXISTING ASSETS,0.9302564102564103,"the paper, properly credited and are the license and terms of use explicitly mentioned and
782"
LICENSES FOR EXISTING ASSETS,0.9312820512820513,"properly respected?
783"
LICENSES FOR EXISTING ASSETS,0.9323076923076923,"Answer: [Yes]
784"
LICENSES FOR EXISTING ASSETS,0.9333333333333333,"Justification: All these are properly credited.
785"
LICENSES FOR EXISTING ASSETS,0.9343589743589743,"Guidelines:
786"
LICENSES FOR EXISTING ASSETS,0.9353846153846154,"• The answer NA means that the paper does not use existing assets.
787"
LICENSES FOR EXISTING ASSETS,0.9364102564102564,"• The authors should cite the original paper that produced the code package or dataset.
788"
LICENSES FOR EXISTING ASSETS,0.9374358974358974,"• The authors should state which version of the asset is used and, if possible, include a
789"
LICENSES FOR EXISTING ASSETS,0.9384615384615385,"URL.
790"
LICENSES FOR EXISTING ASSETS,0.9394871794871795,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
791"
LICENSES FOR EXISTING ASSETS,0.9405128205128205,"• For scraped data from a particular source (e.g., website), the copyright and terms of
792"
LICENSES FOR EXISTING ASSETS,0.9415384615384615,"service of that source should be provided.
793"
LICENSES FOR EXISTING ASSETS,0.9425641025641026,"• If assets are released, the license, copyright information, and terms of use in the
794"
LICENSES FOR EXISTING ASSETS,0.9435897435897436,"package should be provided. For popular datasets, paperswithcode.com/datasets
795"
LICENSES FOR EXISTING ASSETS,0.9446153846153846,"has curated licenses for some datasets. Their licensing guide can help determine the
796"
LICENSES FOR EXISTING ASSETS,0.9456410256410256,"license of a dataset.
797"
LICENSES FOR EXISTING ASSETS,0.9466666666666667,"• For existing datasets that are re-packaged, both the original license and the license of
798"
LICENSES FOR EXISTING ASSETS,0.9476923076923077,"the derived asset (if it has changed) should be provided.
799"
LICENSES FOR EXISTING ASSETS,0.9487179487179487,"• If this information is not available online, the authors are encouraged to reach out to
800"
LICENSES FOR EXISTING ASSETS,0.9497435897435897,"the asset’s creators.
801"
NEW ASSETS,0.9507692307692308,"13. New Assets
802"
NEW ASSETS,0.9517948717948718,"Question: Are new assets introduced in the paper well documented and is the documentation
803"
NEW ASSETS,0.9528205128205128,"provided alongside the assets?
804"
NEW ASSETS,0.9538461538461539,"Answer: [NA]
805"
NEW ASSETS,0.9548717948717949,"Justification: This paper does not release new assets.
806"
NEW ASSETS,0.9558974358974359,"Guidelines:
807"
NEW ASSETS,0.9569230769230769,"• The answer NA means that the paper does not release new assets.
808"
NEW ASSETS,0.9579487179487179,"• Researchers should communicate the details of the dataset/code/model as part of their
809"
NEW ASSETS,0.958974358974359,"submissions via structured templates. This includes details about training, license,
810"
NEW ASSETS,0.96,"limitations, etc.
811"
NEW ASSETS,0.961025641025641,"• The paper should discuss whether and how consent was obtained from people whose
812"
NEW ASSETS,0.9620512820512821,"asset is used.
813"
NEW ASSETS,0.963076923076923,"• At submission time, remember to anonymize your assets (if applicable). You can either
814"
NEW ASSETS,0.9641025641025641,"create an anonymized URL or include an anonymized zip file.
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9651282051282051,"14. Crowdsourcing and Research with Human Subjects
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9661538461538461,"Question: For crowdsourcing experiments and research with human subjects, does the paper
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9671794871794872,"include the full text of instructions given to participants and screenshots, if applicable, as
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9682051282051282,"well as details about compensation (if any)?
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9692307692307692,"Answer: [NA]
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702564102564103,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712820512820513,"Guidelines:
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9723076923076923,"• The answer NA means that the paper does not involve crowdsourcing nor research with
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9733333333333334,"human subjects.
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9743589743589743,"• Including this information in the supplemental material is fine, but if the main contribu-
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9753846153846154,"tion of the paper involves human subjects, then as much detail as possible should be
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764102564102564,"included in the main paper.
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774358974358974,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784615384615385,"or other labor should be paid at least the minimum wage in the country of the data
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794871794871794,"collector.
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805128205128205,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815384615384616,"Subjects
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825641025641025,"Question: Does the paper describe potential risks incurred by study participants, whether
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835897435897436,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846153846153847,"approvals (or an equivalent approval/review based on the requirements of your country or
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856410256410256,"institution) were obtained?
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866666666666667,"Answer: [NA]
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876923076923076,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887179487179487,"Guidelines:
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897435897435898,"• The answer NA means that the paper does not involve crowdsourcing nor research with
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907692307692307,"human subjects.
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917948717948718,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928205128205129,"may be required for any human subjects research. If you obtained IRB approval, you
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938461538461538,"should clearly state this in the paper.
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948717948717949,"• We recognize that the procedures for this may vary significantly between institutions
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958974358974358,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969230769230769,"guidelines for their institution.
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997948717948718,"• For initial submissions, do not include any information that would break anonymity (if
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989743589743589,"applicable), such as the institution conducting the review.
849"
