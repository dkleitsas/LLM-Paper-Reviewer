Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017574692442882249,"We propose a novel approach to the problem of mutual information (MI) estimation
via introducing a family of estimators based on normalizing flows. The estimator
maps original data to the target distribution, for which MI is easier to estimate. We
additionally explore the target distributions with known closed-form expressions
for MI. Theoretical guarantees are provided to demonstrate that our approach yields
MI estimates for the original data. Experiments with high-dimensional data are
conducted to highlight the practical advantages of the proposed method."
ABSTRACT,0.0035149384885764497,"(ξ,η)∼qξ,η"
ABSTRACT,0.005272407732864675,"I(ξ;η)
(easy to estimate) ξ∼qξ η∼qη fX"
ABSTRACT,0.007029876977152899,(diffeomorphism) fY
ABSTRACT,0.008787346221441126,(diffeomorphism)
ABSTRACT,0.01054481546572935,"I(X;Y )
(unknown)"
ABSTRACT,0.012302284710017574,(a) MIENF (general base distribution).
ABSTRACT,0.014059753954305799,"(ξ,η)∼N(0,Σ)"
ABSTRACT,0.015817223198594025,"I(ξ;η)
(tractable)"
ABSTRACT,0.01757469244288225,"ξ∼N(0,Idξ)"
ABSTRACT,0.019332161687170474,"η∼N(0,Idη) fX"
ABSTRACT,0.0210896309314587,(diffeomorphism) fY
ABSTRACT,0.022847100175746926,(diffeomorphism)
ABSTRACT,0.02460456942003515,"I(X;Y )
(unknown)"
ABSTRACT,0.026362038664323375,(b) N-MIENF (Gaussian base distribution).
ABSTRACT,0.028119507908611598,"Figure 1: We propose transforming a pair of random vectors (RVs) via a Cartesian product of learnable
diffeomorphisms to facilitate mutual information (MI) estimation. Ideally, we achieve tractable MI in
the latent space. As diffeomorphisms preserve information, MI between latent representations equals
MI between the original RVs."
INTRODUCTION,0.029876977152899824,"1
Introduction"
INTRODUCTION,0.03163444639718805,"Information-theoretic analysis of deep neural networks (DNN) has attracted recent interest due to
intriguing fundamental results and new hypotheses. Applying information theory to DNNs may
provide novel tools for explainable AI via estimation of information flows [1–5], as well as new
ways to encourage models to extract and generalize information [1, 6–8]. Useful applications of
information theory to the classical problem of independence testing are also worth noting [9–11]."
INTRODUCTION,0.033391915641476276,"∗Skolkovo Institute of Science and Technology
†Moscow Institute of Physics and Technology
‡Sirius University of Science and Technology"
INTRODUCTION,0.0351493848857645,"Most of the information theory applications to the field of machine learning are based on the two
central information-theoretic quantities: differential entropy and mutual information (MI). The latter
quantity is widely used as an invariant measure of the non-linear dependence between random
variables, while differential entropy is usually viewed as a measure of randomness. However, as it
has been shown in the previous works [12, 13], MI and differential entropy are extremely hard to
estimate in the case of high-dimensional data. It is argued that such estimation is also challenging
for long-tailed distributions and large values of MI [14]. These problems considerably limit the
applications of information theory to real-scale machine learning problems. However, recent advances
in the neural estimation methods show that complex parametric estimators achieve relative practical
success in the cases where classical MI estimation techniques fail [7, 15–20]."
INTRODUCTION,0.03690685413005272,"This paper addresses the mentioned problem of the mutual information estimation in high dimensions
via using normalizing flows [21–24]. Some recent works also utilize generative models to estimate
MI. According to a general generative approach described in [16], generative models can be used
to reconstruct probability density functions (PDFs) of marginal and joint distributions to estimate
differential entropy and MI via a Monte Carlo (MC) integration. However, as it is mentioned in the
original work, when flow-based generative models are used, the resulting estimates are poor even
when the data is of simple structure. This approach is further investigated in [25]. The estimator
proposed in [20] uses score-based diffusion models to estimate the differential entropy and MI
without an explicit reconstruction of the PDFs. Increased accuracy of this estimator comes at a cost of
training score networks and using them to compute an MC estimate of Kullback–Leibler divergence
(KLD). Finally, in [11] normalizing flows are used to transform marginal distributions into Gaussian
distributions, after which a zero-correlation criterion is employed to test the zero-MI hypothesis. The
same idea is later used in [25] (see DINE-Gaussian) to acquire an MI estimate, but no corresponding
error bounds are possible to derive, as knowing marginal distributions only is insufficient to calculate
the MI (see Remark 4.5), which makes this estimator substantially flawed. We also note the work,
where normalizing flows are combined with a k-NN entropy estimator [18]."
INTRODUCTION,0.03866432337434095,"In contrast, our method allows for simplified (cheap and low-variance MC integration is required)
or even direct (i.e., no MC integration, nearest neighbors search or other similar data manipulations
are required) and the accurate MI estimation with asymptotic and non-asymptotic error bounds. Our
contributions in this work are the following:"
WE PROPOSE A MI-PRESERVING TECHNIQUE TO SIMPLIFY THE JOINT DISTRIBUTION OF TWO RANDOM,0.040421792618629174,"1. We propose a MI-preserving technique to simplify the joint distribution of two random
vectors (RVs) via a Cartesian product of trainable normalizing flows in order to facilitate the
MI estimation. Non-asymptotic error bounds are provided, with the gap approaching zero
under certain commonly satisfied assumptions, showing that our estimator is consistent."
WE SUGGEST RESTRICTING THE PROPOSED MI ESTIMATOR TO ALLOW FOR A DIRECT MI CALCULATION VIA A,0.0421792618629174,"2. We suggest restricting the proposed MI estimator to allow for a direct MI calculation via a
simple closed-form formula. We further refine our approach to require only O(d) additional
learnable parameters to estimate the MI (here d denotes the dimension of the data). We
provide additional theoretical and statistical guarantees for our restricted estimator: variance
and non-asymptotic error bounds are derived."
WE VALIDATE AND EVALUATE OUR METHOD VIA EXPERIMENTS WITH HIGH-DIMENSIONAL SYNTHETIC DATA,0.043936731107205626,"3. We validate and evaluate our method via experiments with high-dimensional synthetic data
with known ground truth MI. We show that the proposed MI estimator performs well in
comparison to the ground truth and some other advanced estimators during the tests with
high-dimensional compressible and incompressible data of various complexity."
WE VALIDATE AND EVALUATE OUR METHOD VIA EXPERIMENTS WITH HIGH-DIMENSIONAL SYNTHETIC DATA,0.04569420035149385,"This article is organized as follows. In Section 2, the necessary background is provided and the
key concepts of information theory are introduced. Section 3 describes the general method and
corresponding theoretical results. In Section 4 we restrict our method to allow for accurate MI
estimation via a closed-form formula. In Section 5, a series of experiments is performed to evaluate
the proposed method and compare it to several other key MI estimators. Finally, the results are
discussed in Section 6."
WE VALIDATE AND EVALUATE OUR METHOD VIA EXPERIMENTS WITH HIGH-DIMENSIONAL SYNTHETIC DATA,0.04745166959578207,"We provide all the proofs in Appendix A, additional details on the benchmarks we use in Ap-
pendix B, overfitting analysis in Appendix C, supplementary results regarding the information-based
disentanglement of real data in Appendix D, and technical details in Appendix E."
PRELIMINARIES,0.0492091388400703,"2
Preliminaries"
PRELIMINARIES,0.050966608084358524,"Consider random vectors, denoted as X : Ω→Rn and Y : Ω→Rm, where Ωrepresents the sample
space. Let us assume that these random vectors are absolutely continuous, having probability density
functions (PDF) denoted as p(x), p(y), and p(x, y), respectively, where the latter refers to the joint
PDF. The differential entropy of X is defined as follows:"
PRELIMINARIES,0.05272407732864675,"h(X) = −E log p(x) = −
Z"
PRELIMINARIES,0.054481546572934976,supp X
PRELIMINARIES,0.056239015817223195,"p(x) log p(x) dx,"
PRELIMINARIES,0.05799648506151142,"where supp X ⊆Rn represents the support of X, and log(·) denotes the natural logarithm. Similarly,
we define the joint differential entropy as h(X, Y ) = −E log p(x, y) and conditional differential
entropy as h(X | Y ) = −E log p (X|Y ) = −EY
 
EX|Y =y log p(X | Y = y)

. Finally, the mutual
information (MI) is given by I(X; Y ) = h(X) −h(X | Y ), and the following equivalences hold"
PRELIMINARIES,0.05975395430579965,"I(X; Y ) = h(X) −h(X | Y ) = h(Y ) −h(Y | X),
(1)"
PRELIMINARIES,0.061511423550087874,"I(X; Y ) = h(X) + h(Y ) −h(X, Y ),
(2)
I(X; Y ) = DKL (pX,Y ∥pX ⊗pY )
(3)
Mutual information can also be defined as an expectation of the pointwise mutual information:"
PRELIMINARIES,0.0632688927943761,"PMIX,Y (x, y) = log
p(x | y) p(x)"
PRELIMINARIES,0.06502636203866433,"
,
I(X; Y ) = E PMIX,Y (X, Y )
(4)"
PRELIMINARIES,0.06678383128295255,"The above definitions can be generalized via Radon-Nikodym derivatives and induced densities in
case of distributions supports being manifolds, see [26]."
PRELIMINARIES,0.06854130052724078,"The differential entropy estimation is a separate classical statistical problem. Recent works have
proposed several novel ways to acquire the estimate in the high-dimensional case [12, 18, 20, 27–30].
Due to Equation (2), mutual information can be found by estimating entropy values separately. In
contrast, this paper suggests an approach that estimates MI values directly."
PRELIMINARIES,0.070298769771529,"We also have to mention the well-known fundamental property of MI, which is invariance under
smooth injective mappings. The following theorem appears in literature in slightly different forms [14,
19, 31–33]; we utilize the one, which is the most convenient to use with normalizing flows."
PRELIMINARIES,0.07205623901581722,"Theorem 2.1. Let ξ : Ω→Rn′ be an absolutely continuous random vector, and let g: Rn′ →Rn
be an injective piecewise-smooth mapping with Jacobian J, satisfying n ≥n′ and det
 
JT J

̸= 0
almost everywhere. Let PDFs pξ and pξ|η exist. Then"
PRELIMINARIES,0.07381370826010544,"PMIξ,η(ξ, η)
a.s.
= PMIg(ξ),η(g(ξ), η),
I(ξ; η) = I (g(ξ); η)
(5)"
PRELIMINARIES,0.07557117750439367,"In our work, we heavily rely on the normalizing flows [23, 24] – trainable smooth bijective mappings
with tractable Jacobian. However, to understand our results, it is sufficient to know that flow models
(a) satisfy the conditions on g in Theorem 2.1 by definition, (b) can model any absolutely continuous
Borel probability measure (universality property) and (c) are trained via a likelihood maximization,
which is equivalent to a Kullback-Leibler divergence minimization. For more details, we refer the
reader to a more complete and rigorous overview of normalizing flows provided in [34]."
GENERAL METHOD,0.0773286467486819,"3
General method"
GENERAL METHOD,0.07908611599297012,"Our task is to estimate I(X; Y ), where X, Y are random vectors. Here we focus on the absolutely
continuous (X, Y ), as it is the most common case in practice. Note that Theorem 2.1 allows us to
train normalizing flows fX, fY , apply them to X, Y and consider estimating MI between the latent
representations, as I(fX(X); fY (Y )) = I(X; Y )."
GENERAL METHOD,0.08084358523725835,"The key idea of our method is to train fX and fY in such a way that I(fX(X); fY (Y )) is easy to
estimate. For example, one can hope to acquire tractable pointwise mutual information (PMI), which
can be then averaged via MC integration [32]. Unfortunately, the PMI invariance (Theorem 2.1)
restricts the possible distributions of (fX(X), fY (Y )) to an unknown family, making the exact MI
recovery via such technique unfeasible."
GENERAL METHOD,0.08260105448154657,"However, one can always approximate the PDF in latent space via a (preferably, simple) model
q ∈Q with tractable PMI, and train q, fX and fY to minimize the discrepancy between the real
and the proposed PMI. The complexity of q serves as a tradeoff: by selecting a poor Q, one might
experience a considerable bias of the estimate; on the other hand, choosing Q to be a universal PDF
approximation family, one acquires a consistent, but computationally expensive MI estimate. Flows
fX, fY are used to tighten the approximation bound. We formalize this intuition in the following
theorems:
Theorem 3.1. Let (ξ, η) be absolutely continuous with PDF pξ,η. Let qξ,η be a PDF defined on the
same space as pξ,η. Let pξ, pη, qξ and qη be the corresponding marginal PDFs. Then"
GENERAL METHOD,0.0843585237258348,"I(ξ; η) = EPξ,η log
 qξ,η(ξ, η)"
GENERAL METHOD,0.08611599297012303,qξ(ξ)qη(η) 
GENERAL METHOD,0.08787346221441125,"|
{z
}
Iq(ξ;η)"
GENERAL METHOD,0.08963093145869948,"+ DKL (pξ,η ∥qξ,η) −DKL (pξ ⊗pη ∥qξ ⊗qη)
(6)"
GENERAL METHOD,0.0913884007029877,"Corollary 3.2. Under the assumptions of Theorem 3.1, |I(ξ; η) −Iq(ξ; η)| ≤DKL (pξ,η ∥qξ,η)."
GENERAL METHOD,0.09314586994727592,This allows us to define the following MI estimate:
GENERAL METHOD,0.09490333919156414,"ˆIMIENF({(xk, yk)}N
k=1) ≜ˆIˆq( ˆfX(X); ˆfY (Y )) = 1 N N
X"
GENERAL METHOD,0.09666080843585237,"k=1
log"
GENERAL METHOD,0.0984182776801406,"""
ˆqξ,η( ˆfX(xk), ˆfY (yk))"
GENERAL METHOD,0.10017574692442882,"ˆqξ( ˆfX(xk))ˆqη( ˆfY (yk)) # ,
(7)"
GENERAL METHOD,0.10193321616871705,"where {(xk, yk)}N
k=1 is a sampling from (X, Y ), and ˆq, ˆfX and ˆfY are selected according to the
maximum likelihood. The latter makes ˆIMIENF a consistent estimator:"
GENERAL METHOD,0.10369068541300527,"Corollary 3.3 (ˆIMIENF is consistent). Let X, Y , ˆf −1
X and ˆf −1
Y
satisfy the conditions of Theorem 2.1.
Let {(xk, yk)}N
k=1 be an i.i.d. sampling from (X, Y ). Let Q be a family of universal PDF approxi-
mators for a class of densities containing PX,Y ◦(f −1
X × f −1
Y ) (pushforward probability measure in
the latent space), meaning the convergence in probability of a maximum-likelihood estimate from Q
to the ground-truth distribution if N increases. Let ˆqN ∈Q be a maximum-likelihood estimate of
PX,Y ◦(f −1
X × f −1
Y ) from the samples {(fX(xk), fY (yk))}N
k=1. Let IˆqN (fX(X); fY (Y )) exist for
every N. Then
ˆIMIENF({(xk, yk)}N
k=1)
P
−→
N→∞I(X; Y )"
GENERAL METHOD,0.1054481546572935,"Note that maximum-likelihood training of fX, fY also minimizes DKL
 
PX,Y ◦(f −1
X × f −1
Y ) ∥ˆqξ,η

,
which allows for surprisingly simple q ∈Q to be used, as we show in the subsequent sections."
GENERAL METHOD,0.10720562390158173,"The described approach is as general as possible. We use it as a starting point for a development of a
more elegant, cheap and practically sound MI estimator. We also do not incorporate conditions, under
which the universality property of Q holds, as they depend on the choice of Q; if one is interested in
using normalizing flows as Q, we refer to Section 3.4.3 in [34] or to [25] for more details."
USING GAUSSIAN BASE DISTRIBUTION,0.10896309314586995,"4
Using Gaussian base distribution"
USING GAUSSIAN BASE DISTRIBUTION,0.11072056239015818,"Note that the general approach requires finding the maximum-likelihood estimate ˆq and using it to
perform an MC integration to acquire ˆIˆq(fX(X); fY (Y ))."
USING GAUSSIAN BASE DISTRIBUTION,0.11247803163444639,"In this section, we drop these requirements by restricting our estimator via choosing Q to be a family
of multivariate Gaussian PDFs. This allows us (a) to directly estimate the MI via a closed-form
expression, (b) to employ a closed-form expression for optimal ˆq, (c) to leverage the maximum
entropy principle for Gaussian distributions, thus acquiring better non-asymptotic bounds, and (d) to
analyze the variance of the proposed estimate.
Theorem 4.1 (Theorem 8.6.5 in [35]). Let Z be a d-dimensional absolutely continuous random
vector with probability density function pZ, mean m and covariance matrix Σ. Then"
USING GAUSSIAN BASE DISTRIBUTION,0.11423550087873462,"h(Z) = h (N(m, Σ))−DKL (pZ ∥N(m, Σ)) = d"
USING GAUSSIAN BASE DISTRIBUTION,0.11599297012302284,2 log(2πe)+ 1
USING GAUSSIAN BASE DISTRIBUTION,0.11775043936731107,"2 log det Σ−DKL (pZ ∥N(m, Σ))"
USING GAUSSIAN BASE DISTRIBUTION,0.1195079086115993,Remark 4.2. Note that h(Z) may not be equal to h(Z′) −DKL (pZ ∥pZ′) for arbitrary Z′.
USING GAUSSIAN BASE DISTRIBUTION,0.12126537785588752,"Corollary 4.3. Let (ξ, η) be an absolutely continuous pair of random vectors with joint and marginal
probability density functions pξ,η, pξ and pη correspondingly, and mean and covariance matrix being"
USING GAUSSIAN BASE DISTRIBUTION,0.12302284710017575,"m =

mξ
mη"
USING GAUSSIAN BASE DISTRIBUTION,0.12478031634446397,"
,
Σ =

Σξ,ξ
Σξ,η
Ση,ξ
Ση,η  Then"
USING GAUSSIAN BASE DISTRIBUTION,0.1265377855887522,I(ξ; η) = 1
USING GAUSSIAN BASE DISTRIBUTION,0.1282952548330404,"2 [log det Σξ,ξ + log det Ση,η −log det Σ] +"
USING GAUSSIAN BASE DISTRIBUTION,0.13005272407732865,"+ DKL (pξ,η ∥N(m, Σ)) −DKL (pξ ⊗pη ∥N(m, diag(Σξ,ξ, Ση,η)) ,"
USING GAUSSIAN BASE DISTRIBUTION,0.13181019332161686,which implies the following in the case of marginally Gaussian ξ and η:
USING GAUSSIAN BASE DISTRIBUTION,0.1335676625659051,I(ξ; η) ≥1
USING GAUSSIAN BASE DISTRIBUTION,0.13532513181019332,"2 [log det Σξ,ξ + log det Ση,η −log det Σ] ,
(8)"
USING GAUSSIAN BASE DISTRIBUTION,0.13708260105448156,"with the equality holding if and only if (ξ, η) are jointly Gaussian."
USING GAUSSIAN BASE DISTRIBUTION,0.13884007029876977,"Corollary 4.4. Under the assumptions of Corollary 4.3,
I(ξ; η) −1"
USING GAUSSIAN BASE DISTRIBUTION,0.140597539543058,"2 [log det Σξ,ξ + log det Ση,η −log det Σ]
 ≤DKL (pξ,η ∥N(m, Σ)) ."
USING GAUSSIAN BASE DISTRIBUTION,0.14235500878734622,"Remark 4.5. The upper bound from Corollary 4.4 is tight, consider ξ ∼N(0, 1), η = (2B −1) · ξ,
where B ∼Bernoulli(1/2) and is independent of ξ."
USING GAUSSIAN BASE DISTRIBUTION,0.14411247803163443,"From now on we denote fX(X) and fY (Y ) as ξ and η correspondingly. Note that, in contrast
to Theorem 3.1 and Corollary 3.2, Iq(ξ; η) is replaced by a closed-form expression, which is not
possible to achieve in general. The provided closed-form expression allows for calculating MI for
jointly Gaussian (ξ, η), and serves as a lower bound on MI in the general case of ξ and η being only
marginally Gaussian."
GENERAL BINORMALIZATION APPROACH,0.14586994727592267,"4.1
General binormalization approach"
GENERAL BINORMALIZATION APPROACH,0.14762741652021089,"In order to minimize DKL (pξ,η ∥N(m, Σ)), we train fX × fY as a single normalizing flow. Instead
of maximizing the log-likelihood using two separate and fixed base (latent) distributions, we maximize
the log-likelihood of the joint sampling {(xk, yk)}N
k=1 using the whole set of Gaussian distributions
as possible base distributions."
GENERAL BINORMALIZATION APPROACH,0.14938488576449913,"Definition 4.6.
We denote a set of d-dimensional Gaussian distributions as Sd
N
≜

N(m, Σ) | m ∈Rd, Σ ∈Rd×d	
.4"
GENERAL BINORMALIZATION APPROACH,0.15114235500878734,"Definition 4.7. The log-likelihood of a sampling {zk}N
k=1 with respect to a set of absolutely continu-
ous probability distributions S is defined as follows:"
GENERAL BINORMALIZATION APPROACH,0.15289982425307558,"LS({zk}) ≜sup
µ∈S
Lµ({zk}) = sup
µ∈S N
X"
GENERAL BINORMALIZATION APPROACH,0.1546572934973638,"k=1
log
dµ dz"
GENERAL BINORMALIZATION APPROACH,0.15641476274165203,"
(zk)
"
GENERAL BINORMALIZATION APPROACH,0.15817223198594024,"Let us define f ≜fX × fY (Cartesian product of flows) and S ◦f = {µ ◦f | µ ∈S} (set of
pushforward measures). In our case, LSN ◦f ({(xk, yk)}) can be expressed in a closed-form via
the change of variables formula (identically to a classical normalizing flows setup) and maximum-
likelihood estimates for m and Σ.
Statement 4.8."
GENERAL BINORMALIZATION APPROACH,0.15992970123022848,"LSN ◦(fX×fY ) ({(xk, yk)}) = log
det ∂f(x, y)"
GENERAL BINORMALIZATION APPROACH,0.1616871704745167,"∂(x, y)"
GENERAL BINORMALIZATION APPROACH,0.1634446397188049,"+ LN( ˆm,ˆΣ)({f(xk, yk)}), where"
GENERAL BINORMALIZATION APPROACH,0.16520210896309315,"log
det ∂f(x, y)"
GENERAL BINORMALIZATION APPROACH,0.16695957820738136,"∂(x, y)"
GENERAL BINORMALIZATION APPROACH,0.1687170474516696,"= log
det ∂fX(x) ∂x"
GENERAL BINORMALIZATION APPROACH,0.1704745166959578,"+ log
det ∂fY (y) ∂y ,"
GENERAL BINORMALIZATION APPROACH,0.17223198594024605,"ˆm = 1 N N
X"
GENERAL BINORMALIZATION APPROACH,0.17398945518453426,"k=1
f(xk, yk),
ˆΣ = 1 N N
X"
GENERAL BINORMALIZATION APPROACH,0.1757469244288225,"k=1
(f(xk, yk) −ˆm)(f(xk, yk) −ˆm)T"
GENERAL BINORMALIZATION APPROACH,0.17750439367311072,4We omit d whenever it can be deduced from the context.
GENERAL BINORMALIZATION APPROACH,0.17926186291739896,"Maximization of LSN ◦(fX×fY ) ({(xk, yk)}) with respect to parameters of fX and fY minimizes
DKL (pξ,η ∥N(m, Σ)) [34], making it possible to apply Theorem 2.1 and Corollary 4.3 to acquire an
MI estimate with corresponding non-asymptotic error bounds from Corollary 4.4:"
GENERAL BINORMALIZATION APPROACH,0.18101933216168717,"ˆIN-MIENF({(xk, yk)}N
k=1) ≜1 2"
GENERAL BINORMALIZATION APPROACH,0.1827768014059754,"h
log det ˆΣξ,ξ + log det ˆΣη,η −log det ˆΣ
i
(9)"
GENERAL BINORMALIZATION APPROACH,0.18453427065026362,"Note that if only marginal Gaussianization is achieved, Equation (9) serves as a lower bound estimate.
As ˆIN-MIENF involves maximum-likelihood estimates of covariance matrices, existing results can be
employed to acquire the asymptotic variance:"
GENERAL BINORMALIZATION APPROACH,0.18629173989455183,"Lemma 4.9 (Lemma 2 in [25]). Let fX, fY be fixed. Let (ξ, η) have finite covariance matrix. Then,
the asymptotic variance of ˆIN-MIENF is O(d2/N), with d being the dimensionality. If (ξ, η) is also
Gaussian, the asymptotic variance is further improved to O(d/N)."
REFINED APPROACH,0.18804920913884007,"4.2
Refined approach"
REFINED APPROACH,0.18980667838312829,"Although the proposed general method is compelling, as it requires only the slightest modifications
to the conventional normalizing flow setup to make the application of the closed-form expressions for
MI possible, we have to mention several drawbacks."
REFINED APPROACH,0.19156414762741653,"Firstly, the log-likelihood maximum is ambiguous, as LSN is invariant under invertible affine
mappings, which makes the proposed log-likelihood maximization an ill-posed problem:
Remark 4.10. Let A ∈Rd×d be a non-singular matrix, b ∈R, {zk}N
k=1 ⊆Rd. Then"
REFINED APPROACH,0.19332161687170474,LSN ◦(Az+b)({zk}) = LSN ({zk})
REFINED APPROACH,0.19507908611599298,"Secondly, this method requires a regular (ideally, after every gradient descent step) updating of
ˆm and ˆΣ for the whole dataset, which is expensive. In practice, these estimates can be replaced
with batchwise maximum likelihood estimates, which are used to update ˆm and ˆΣ via exponential
moving average (EMA). This approach, however, requires tuning EMA multiplication coefficient in
accordance with the learning rate to make the training fast yet stable. We also note that ˆm and ˆΣ can
be made learnable via the gradient ascent, but the benefits of the closed-form expressions for LSN in
Statement 4.8 are thus lost."
REFINED APPROACH,0.1968365553602812,"Finally, each loss function evaluation requires inversion of ˆΣ, and each MI estimation requires
evaluation of det ˆΣ and determinants of two diagonal blocks of ˆΣ. This might be resource-consuming
in high-dimensional cases, as matrices may not be sparse. Numerical instabilities might also occur if
ˆΣ happens to be ill-conditioned (might happen in the case of data lying on a manifold or due to high
MI)."
REFINED APPROACH,0.19859402460456943,"That is why we propose an elegant and simple way to eliminate all the mentioned problems by further
narrowing down SN to a subclass of Gaussian distributions with simple and bounded covariance
matrices and fixed means. This approach is somewhat reminiscent of the non-linear canonical
correlation analysis [36, 37]."
REFINED APPROACH,0.20035149384885764,Definition 4.11.
REFINED APPROACH,0.20210896309314588,"Sdξ,dη
tridiag-N ≜

N(0, Σ) | Σξ,ξ = Idξ, Ση,η = Idη, Σξ,η(≡ΣT
η,ξ) = diag({ρj})dξ×dη, ρj ∈[0; 1)"
REFINED APPROACH,0.2038664323374341,"This approach solves all the aforementioned problems without any loss in generality, as it is shown
by the following results:"
REFINED APPROACH,0.2056239015817223,"Corollary 4.12. If (ξ, η) ∼µ ∈Stridiag-N , then"
REFINED APPROACH,0.20738137082601055,I(ξ; η) = −1 2 X
REFINED APPROACH,0.20913884007029876,"j
log(1 −ρ2
j)
(10)"
REFINED APPROACH,0.210896309314587,"Statement 4.13 (Canonical correlation analysis). Let (ξ, η) ∼N(m, Σ), where Σ is non-singular.
There exist invertible affine mappings φξ, φη such that (φξ(ξ), φη(η)) ∼µ ∈Stridiag-N . Due to
Theorem 2.1, the following also holds: I(ξ; η) = I(φξ(ξ); φη(η))."
REFINED APPROACH,0.2126537785588752,"Statement 4.14. Let (ξ, η) ∼N(0, Σ) ∈Stridiag-N , {zk}N
k=1 ⊆Rdξ+dη. Then"
REFINED APPROACH,0.21441124780316345,"LN(0,Σ)({zk}) = I(ξ; η) + LN(0,I)({Σ−1/2zk}),"
REFINED APPROACH,0.21616871704745166,"where (implying ρj = 0 for j > min{dξ, dη})"
REFINED APPROACH,0.2179261862917399,Σ−1/2 =  
REFINED APPROACH,0.21968365553602812,"αj + βj
αj −βj
...
...
αj −βj
αj + βj"
REFINED APPROACH,0.22144112478031636,"|
{z
}
dξ ..."
REFINED APPROACH,0.22319859402460457,"|
{z
}
dη ...  "
REFINED APPROACH,0.22495606326889278,"αj =
1
2p1 + ρj"
REFINED APPROACH,0.22671353251318102,"βj =
1
2p1 −ρj"
REFINED APPROACH,0.22847100175746923,and I(ξ; η) is calculated via (10).
REFINED APPROACH,0.23022847100175747,"Maximization of LStridiag-N ◦(fX×fY ) ({(xk, yk)}) with respect to the parameters of fX and fY yields
the following MI estimate:"
REFINED APPROACH,0.23198594024604569,"ˆItridiag-N-MIENF({(xk, yk)}N
k=1) ≜−1 2 X"
REFINED APPROACH,0.23374340949033393,"j
log(1 −ˆρ2
j),
(11)"
REFINED APPROACH,0.23550087873462214,where ˆρj are the maximum-likelihood estimates of ρj.
TRACTABLE ERROR BOUNDS,0.23725834797891038,"4.3
Tractable error bounds"
TRACTABLE ERROR BOUNDS,0.2390158172231986,"Note that Corollary 3.2 and Corollary 4.4 provide us with non-asymptotic, but untractable bounds.
These bounds can be estimated via various KL divergence estimators [7, 20, 38–40]. However, this
requires training an additional neural network, which is computationally expensive."
TRACTABLE ERROR BOUNDS,0.24077328646748683,"Conveniently, as the proposed method involves maximization of the likelihood, a cheap and tractable
lower bound on the KL divergence can be obtained via an entropy-cross-entropy decomposition:"
TRACTABLE ERROR BOUNDS,0.24253075571177504,DKL (p ∥q) = EZ∼p log p(Z)
TRACTABLE ERROR BOUNDS,0.24428822495606328,"q(Z) = −EZ∼p log q(Z) −h(Z) ≥−EZ∼p log q(Z) −h(N(m, Σ))"
TRACTABLE ERROR BOUNDS,0.2460456942003515,"(12)
Note that E log q(Z) in Equation (12) is estimated by the log-likelihood of the samples in the latent
space (which is inevitably evaluated each training step), and h(Z) can be upper bounded by the
entropy of a Gaussian distribution (see Theorem 4.1). Unfortunately, as Theorem 4.1 has already
been employed to derive Corollary 4.3, the proposed bound is trivial (equaling to zero) in our
Gaussian-based setup. However, this idea might still be useful in the general case."
IMPLEMENTATION DETAILS,0.2478031634446397,"4.4
Implementation details"
IMPLEMENTATION DETAILS,0.24956063268892795,"In this section, we would like to emphasize the computational simplicity of the proposed amendments
to the conventional normalizing flow setup."
IMPLEMENTATION DETAILS,0.2513181019332162,"Firstly, Statement 4.14 allows for a cheap log-likelihood computation and sample generation, as Σ,
Σ−1/2 and Σ−1 are easily calculated from the {ρj} and are sparse (tridiagonal, block-diagonalizable
with 2 × 2 blocks). Secondly, the method requires only d′ = min{dξ, dη} additional parameters:
the estimates for {ρj}. As ρj ∈[0; 1), an appropriate parametrization should be chosen to allow for
stable learning of {ˆρj} via the gradient ascend. We propose using the logarithm of cross-component
MI5: wj ≜log I(ξj; ηj) = log

−1"
IMPLEMENTATION DETAILS,0.2530755711775044,"2 log(1 −ρ2
j)

. In this parametrization wj ∈R and"
IMPLEMENTATION DETAILS,0.2548330404217926,"ˆItridiag-N-MIENF({(xk, yk)}N
k=1) =
X"
IMPLEMENTATION DETAILS,0.2565905096660808,"j
e ˆ
wj,
ρj =
p"
IMPLEMENTATION DETAILS,0.2583479789103691,"1 −exp(−2 · ewj) ∈(0; 1)
(13)"
IMPLEMENTATION DETAILS,0.2601054481546573,"Although ρj = 0 is not achievable in the proposed parametrization, it can be made arbitrarily close to
0 with wj →−∞."
IMPLEMENTATION DETAILS,0.2618629173989455,"5One can also consider the softplus parametrization, which allows to avoid the double exponentiation in (13).
We, however, did not encounter any issues with the plain logarithm parametrization."
EXTENSION TO NON-GAUSSIAN BASE DISTRIBUTIONS AND NON-BIJECTIVE FLOWS,0.26362038664323373,"4.5
Extension to non-Gaussian base distributions and non-bijective flows"
EXTENSION TO NON-GAUSSIAN BASE DISTRIBUTIONS AND NON-BIJECTIVE FLOWS,0.26537785588752194,"The proposed method can be easily generalized to account for any base distribution with closed-form
expression for MI, or even a combination of such distributions. For example, a smoothed uniform
distribution can be considered, with the learnable parameter being the smoothing constant ε, see Ap-
pendix B.2, Equation (14). However, due to Remark 4.2, neither Corollary 3.2, nor Corollary 4.4 can
be used to bound the estimation error in this case."
EXTENSION TO NON-GAUSSIAN BASE DISTRIBUTIONS AND NON-BIJECTIVE FLOWS,0.2671353251318102,"Also note that, as Theorem 2.1 does not require g to be bijective, our method is naturally extended
to injective normalizing flows [41, 42]. Moreover, according to [19], such approach may actually
facilitate the estimation of MI."
EXPERIMENTS,0.2688927943760984,"5
Experiments"
EXPERIMENTS,0.27065026362038663,"To evaluate our estimator, we utilize synthetic datasets with known mutual information. In [14] and
[19], extensive frameworks for evaluation of MI estimators have been proposed. In our work, we
borrow complex high-dimensional tests from [19] and all non-Gaussian base distributions with known
MI from [14] (see Appendix B for more details). The formal description of the dataset generation
and estimator evaluation is provided in Algorithm 1. Essentially similar setups are widely used to test
the MI estimators [7, 13, 19, 20, 31, 43]."
EXPERIMENTS,0.27240773286467485,Algorithm 1 MI estimator evaluation
EXPERIMENTS,0.2741652021089631,"1: Generate two datasets of samples from random vectors ξ and η with known ground truth mutual
information (see Corollary 4.3, Corollary 4.12 and Appendix B for examples): {(ak, bk)}N
k=1.
2: Choose functions gξ and gη satisfying conditions of Theorem 2.1, so I(ξ; η) = I
 
gξ(ξ); gη(η)

.
3: Estimate I
 
gξ(ξ); gη(η)

via {(gξ(ak), gη(bk))}N
k=1; compare the result with the ground truth."
EXPERIMENTS,0.2759226713532513,"For the first set of experiments, we map a low-dimensional correlated Gaussian distribution to a
distribution of high-dimensional images of geometric shapes (see Figure 2). We compare our method
with the Mutual Information Neural Estimator (MINE) [7], Nguyen-Wainwright-Jordan (NWJ) [7, 39]
and Nishiyama’s [40] estimators, nearest neighbor Kraskov-Stoegbauer-Grassberger [31] and 5-
nearest neighbors weighted Kozachenko-Leonenko (WKL) estimator [27, 44]; the latter is fed with
the data compressed via a convolutional autoencoder (CNN AE) in accordance to the pipeline
from [19]."
EXPERIMENTS,0.27768014059753954,"(a) 2D Gaussians
(b) Rectangles"
EXPERIMENTS,0.27943760984182775,"Figure 2: Examples of synthetic images used in the tests. Note that images are high-dimensional, but
admit latent structure, which is similar to real datasets."
EXPERIMENTS,0.281195079086116,"For the second set of experiments, incompressible, high-dimensional non-Gaussian-based distribu-
tions are considered. These experiments are conducted to evaluate the robustness of our estimator to
the distributions, which can not be precisely Gaussianized via a Cartesian product of two flows. In
this section, we compare our method to the ground truth value only. We also provide a comparison
with MINE and DINE-Gaussian [25] in Appendix B. For a more elaborate benchmarking of other
estimators on these distributions, we refer the reader to [14]."
EXPERIMENTS,0.28295254833040423,"For the tests with synthetic images, we use GLOW [45] normalizing flow architecture with
log2(image size) splits, 2 blocks between splits and 16 hidden channels in each block, appended with
a learnable orthogonal linear layer and a small 4-layer Real NVP flow [46]. For the other tests, we
use 6-layer Real NVP architecture. For further details (including the architecture of MINE critic
network and CNN autoencoder), we refer the reader to Appendix E."
EXPERIMENTS,0.28471001757469244,"The results of the experiments performed with the high-dimensional synthetic images and non-
Gaussian-based distribution are provided in Figure 3 and Figure 4 correspondingly. 0 2 4 6 8 10"
EXPERIMENTS,0.28646748681898065,"0
2
4
6
8
10"
EXPERIMENTS,0.28822495606326887,"ground truth
AE+WKL 5-NN
KSG
MINE
NWJ
Nishiyama
MIENF (ours)"
EXPERIMENTS,0.28998242530755713,(a) 16 × 16 images (Gaussians) 0 2 4 6 8 10
EXPERIMENTS,0.29173989455184535,"0
2
4
6
8
10"
EXPERIMENTS,0.29349736379613356,"ground truth
AE+WKL 5-NN
KSG
MINE
NWJ
Nishiyama
MIENF (ours)"
EXPERIMENTS,0.29525483304042177,(b) 32 × 32 images (Gaussians) 0 2 4 6 8 10
EXPERIMENTS,0.29701230228471004,"0
2
4
6
8
10"
EXPERIMENTS,0.29876977152899825,"ground truth
AE+WKL 5-NN
KSG
MINE
NWJ
Nishiyama
MIENF (ours)"
EXPERIMENTS,0.30052724077328646,(c) 16 × 16 images (rectangles) 0 2 4 6 8 10
EXPERIMENTS,0.3022847100175747,"0
2
4
6
8
10"
EXPERIMENTS,0.30404217926186294,"ground truth
AE+WKL 5-NN"
EXPERIMENTS,0.30579964850615116,"KSG
MINE"
EXPERIMENTS,0.30755711775043937,"NWJ
Nishiyama
MIENF (ours)"
EXPERIMENTS,0.3093145869947276,(d) 32 × 32 images (rectangles)
EXPERIMENTS,0.3110720562390158,"Figure 3: Comparison of the selected estimators. Along x axes is I(X; Y ), along y axes is ˆI(X; Y ).
We plot 99.9% asymptotic CIs acquired either from the MC integration standard deviation (WKL,
KSG) or from the epochwise averaging (other methods, 200 last epochs). 10 · 103 samples were used. 7 7.5 8 8.5 9 9.5 10 10.5"
EXPERIMENTS,0.31282952548330406,"2
4
8
16
32"
EXPERIMENTS,0.3145869947275923,"ˆI(X, Y )"
EXPERIMENTS,0.3163444639718805,dimensionality
EXPERIMENTS,0.3181019332161687,"Ground truth
Uniform
Smoothed uniform
Student (dof = 4)
arcsinh(Student) (dof = 1)
arcsinh(Student) (dof = 2)"
EXPERIMENTS,0.31985940246045697,(a) N-MIENF 7 7.5 8 8.5 9 9.5 10 10.5
EXPERIMENTS,0.3216168717047452,"2
4
8
16
32"
EXPERIMENTS,0.3233743409490334,"ˆI(X, Y )"
EXPERIMENTS,0.3251318101933216,dimensionality
EXPERIMENTS,0.3268892794376098,"Ground truth
Uniform
Smoothed uniform
Student (dof = 4)
arcsinh(Student) (dof = 1)
arcsinh(Student) (dof = 2)"
EXPERIMENTS,0.3286467486818981,(b) tridiag-N-MIENF
EXPERIMENTS,0.3304042179261863,"Figure 4: Tests with incompressible multidimensional data. “Uniform” denotes the uniformly
distributed samples acquired from the correlated Gaussians via the Gaussian CDF. “Smoothed
uniform” and “Student” denote the non-Gaussian-based distributions described in Appendix B.
“arcsinh(Student)” denotes the arcsinh function applied to the “Student” example (this is done to
avoid numerical instabilities in the case of long-tailed distributions). We run each test 5 times
and plot 99.9% asymptotic Gaussian CIs. 10 · 103 samples were used. Note that N-MIENF and
tridiag-N-MIENF yield almost the same results with similar bias."
EXPERIMENTS,0.3321616871704745,"We attribute the good performance of AE+WKL to the fact that the proposed synthetic datasets are
easily and almost losslessly compressed via a CNN AE. We run additional experiments with much
simpler, but incompressible data to show that the estimation error of WKL rapidly increases with
the dimensionality. The results are provided in Table 1. In contrast, our method yields reasonable
estimates in the same or similar cases presented in Figure 4."
EXPERIMENTS,0.3339191564147627,"Table 1: Evaluation of 5-NN weighted Kozachenko-Leonenko estimator on multidimensional uni-
formly distributed data. For each dimension dX = dY , 11 estimates of MI are acquired with the
ground truth ranging from 0 to 10 with a fixed step. The RMSE of the estimated MI relative to the
ground-truth MI is calculated for each set of estimates."
EXPERIMENTS,0.335676625659051,"dX,Y
2
4
8
16
32
64"
EXPERIMENTS,0.3374340949033392,"RMSE
2.2
1.0
127.9
227.5
522.4
336.2"
EXPERIMENTS,0.3391915641476274,"Overall, the proposed estimator performs well during all the experiments, including the incompressible
high-dimensional data, large MI values and non-Gaussian-based tests. In Appendix D, we also apply
our method to acquire disentengled representations of real data. Additionally, we give a brief
commentary on the sample complexity of the proposed method and other NN-based estimators
in Appendix C."
DISCUSSION,0.3409490333919156,"6
Discussion"
DISCUSSION,0.3427065026362039,"Information-theoretic analysis of deep neural networks is a novel and developing approach. As it
relies on a well-established theory of information, it potentially can provide fundamental, robust and
intuitive results [47, 48]. Currently, this analysis is complicated due to main information-theoretic
qualities — differential entropy and mutual information — being hard to measure in the case of
high-dimensional data."
DISCUSSION,0.3444639718804921,"We have shown that it is possible to modify the conventional normalizing flow setup to harness all the
benefits of simple and robust closed-form expressions for mutual information. Non-asymptotic error
bounds for both variants of our method are derived, asymptotic variance and consistency analysis
is carried out. We provide useful theoretical and practical insights on using the proposed method
effectively. We have demonstrated the effectiveness of our estimator in various settings, including
compressible and incompressible high-dimensional data, high values of mutual information and the
data not acquired from the Gaussian distribution via invertible mappings."
DISCUSSION,0.3462214411247803,"Finally, it is worth noting that despite normalizing flows and Gaussian base distributions being used
throughout our work, the proposed method can be extended to any type of base distribution with
closed-form expression for mutual information and to any injective generative model. For example, a
subclass of diffusion models can be considered [49, 50]. Injective normalizing flows [41, 42] are also
compatible with the proposed pipeline. Gaussian mixtures can also be used as base distributions due
to a relatively cheap MI calculation and the universality property [32]."
DISCUSSION,0.34797891036906853,"Limitations
The main limitation of the general method is the ambiguity of Q (the family of PDF
estimators used to estimate MI in the latent space), which can be either rich (yielding a consistent,
but possibly expensive estimator), or poor (leading to the inconsistency of the estimate). However,
in [32] it is argued that mixture models can achieve rather good tradeoff between the quality and the
cost of a PMI approximation."
DISCUSSION,0.34973637961335674,"The major limitation of N-MIENF is that its consistency is proven only for a certain class of
distributions: the probability distribution should be equivalent to a Gaussian via a Cartesian product
of diffeomorphisms. However, mathematical simplicity, rigorous bounds, low variance and relative
practical success of the estimator suggest that the proposed method achieves a decent tradeoff."
REFERENCES,0.351493848857645,References
REFERENCES,0.3532513181019332,"[1] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle.
2015 IEEE Information Theory Workshop (ITW), pages 1–5, 2015."
REFERENCES,0.35500878734622143,"[2] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability
of learning algorithms. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Sys-
tems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf."
REFERENCES,0.35676625659050965,"[3] Ziv Goldfeld, Ewout van den Berg, Kristjan H. Greenewald, Igor V. Melnyk, Nam H. Nguyen,
Brian Kingsbury, and Yury Polyanskiy. Estimating information flow in deep neural networks.
In ICML, 2019."
REFERENCES,0.3585237258347979,"[4] Thomas Steinke and Lydia Zakynthinou. Reasoning About Generalization via Conditional
Mutual Information. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty
Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning
Research, pages 3437–3452. PMLR, 09–12 Jul 2020. URL https://proceedings.mlr.
press/v125/steinke20a.html."
REFERENCES,0.3602811950790861,"[5] Rana Ali Amjad, Kairen Liu, and Bernhard C. Geiger. Understanding neural networks and
individual neuron importance via information-ordered cumulative ablation. IEEE Transactions
on Neural Networks and Learning Systems, 33(12):7842–7852, 2022. doi: 10.1109/TNNLS.
2021.3088685."
REFERENCES,0.36203866432337434,"[6] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.
Infogan: Interpretable representation learning by information maximizing generative adversarial
nets. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain,
pages 2172–2180, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html."
REFERENCES,0.36379613356766255,"[7] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio,
Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy
and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pages 531–540. PMLR,
07 2018. URL https://proceedings.mlr.press/v80/belghazi18a.html."
REFERENCES,0.3655536028119508,"[8] Lynton Ardizzone, Radek Mackowiak, Carsten Rother, and Ullrich Köthe. Training nor-
malizing flows with the information bottleneck for competitive generative classification. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in
Neural Information Processing Systems, volume 33, pages 7828–7840. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
593906af0d138e69f49d251d3e7cbed0-Paper.pdf."
REFERENCES,0.36731107205623903,"[9] Thomas Berrett and Richard Samworth. Nonparametric independence testing via mutual
information. Biometrika, 106, 11 2017. doi: 10.1093/biomet/asz024."
REFERENCES,0.36906854130052724,"[10] Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros G Dimakis,
and Sanjay Shakkottai.
Model-powered conditional independence test.
In I. Guyon,
U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-
itors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,
Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
02f039058bd48307e6f653a2005c9dd2-Paper.pdf."
REFERENCES,0.37082601054481545,"[11] Bao Duong and Thin Nguyen. Normalizing flows for conditional independence testing. Knowl-
edge and Information Systems, 66, 08 2023. doi: 10.1007/s10115-023-01964-w."
REFERENCES,0.37258347978910367,"[12] Z. Goldfeld, K. Greenewald, J. Niles-Weed, and Y. Polyanskiy. Convergence of smoothed
empirical measures with applications to entropy estimation. IEEE Transactions on Information
Theory, 66(7):4368–4391, 2020. doi: 10.1109/TIT.2020.2975480."
REFERENCES,0.37434094903339193,"[13] David McAllester and Karl Stratos. Formal limitations on the measurement of mutual in-
formation.
In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty"
REFERENCES,0.37609841827768015,"Third International Conference on Artificial Intelligence and Statistics, volume 108 of Pro-
ceedings of Machine Learning Research, pages 875–884. PMLR, 08 2020. URL https:
//proceedings.mlr.press/v108/mcallester20a.html.
[14] Paweł Czy˙z, Frederic Grabowski, Julia E Vogt, Niko Beerenwinkel, and Alexander Marx. Be-
yond normal: On the evaluation of mutual information estimators. In Thirty-seventh Conference
on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?
id=25vRtG56YH.
[15] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
predictive coding, 2019.
[16] Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual infor-
mation estimators. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=B1x62TNtDS.
[17] Benjamin Rhodes, Kai Xu, and Michael U. Gutmann. Telescoping density-ratio estimation.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in
Neural Information Processing Systems, volume 33, pages 4905–4916. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
33d3b157ddc0896addfb22fa2a519097-Paper.pdf.
[18] Ziqiao Ao and Jinglai Li. Entropy estimation via normalizing flow. Proceedings of the AAAI
Conference on Artificial Intelligence, 36(9):9990–9998, Jun. 2022. doi: 10.1609/aaai.v36i9.
21237. URL https://ojs.aaai.org/index.php/AAAI/article/view/21237.
[19] Ivan Butakov, Alexander Tolmachev, Sofia Malanchuk, Anna Neopryatnaya, Alexey Frolov, and
Kirill Andreev. Information bottleneck analysis of deep neural networks via lossy compression.
In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=huGECz8dPp.
[20] Giulio Franzese, Mustapha BOUNOUA, and Pietro Michiardi. MINDE: Mutual information
neural diffusion estimation. In The Twelfth International Conference on Learning Representa-
tions, 2024. URL https://openreview.net/forum?id=0kWd8SJq8d.
[21] Esteban G. Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-
likelihood. Communications in Mathematical Sciences, 8(1):217 – 233, 2010.
[22] E. G. Tabak and Cristina V. Turner. A family of nonparametric density estimation algorithms.
Communications on Pure and Applied Mathematics, 66(2):145–164, 2013. doi: https://doi.org/
10.1002/cpa.21423. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.
21423.
[23] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components
estimation. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track
Proceedings, 2015. URL http://arxiv.org/abs/1410.8516.
[24] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference
on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop
and Conference Proceedings, pages 1530–1538. JMLR.org, 2015. URL http://proceedings.
mlr.press/v37/rezende15.html.
[25] Bao Duong and Thin Nguyen. Diffeomorphic information neural estimation. Proceedings of
the AAAI Conference on Artificial Intelligence, 37(6):7468–7475, Jun. 2023. doi: 10.1609/aaai.
v37i6.25908. URL https://ojs.aaai.org/index.php/AAAI/article/view/25908.
[26] M. Spivak. Calculus On Manifolds: A Modern Approach To Classical Theorems Of Advanced
Calculus. Avalon Publishing, 1965. ISBN 9780805390216.
[27] Thomas B. Berrett, Richard J. Samworth, and Ming Yuan. Efficient multivariate entropy
estimation via k-nearest neighbour distances. Ann. Statist., 47(1):288–318, 02 2019. doi:
10.1214/18-AOS1688. URL https://doi.org/10.1214/18-AOS1688.
[28] I. D. Butakov, S. V. Malanchuk, A. M. Neopryatnaya, A. D. Tolmachev, K. V. Andreev, S. A.
Kruglik, E. A. Marshakov, and A. A. Frolov. High-dimensional dataset entropy estimation via
lossy compression. Journal of Communications Technology and Electronics, 66(6):764–768,
7 2021. ISSN 1555-6557. doi: 10.1134/S1064226921060061. URL https://doi.org/10.
1134/S1064226921060061."
REFERENCES,0.37785588752196836,"[29] Kristjan H. Greenewald, Brian Kingsbury, and Yuancheng Yu. High-dimensional smoothed
entropy estimation via dimensionality reduction. In IEEE International Symposium on Informa-
tion Theory, ISIT 2023, Taipei, Taiwan, June 25-30, 2023, pages 2613–2618. IEEE, 2023. doi:
10.1109/ISIT54713.2023.10206641. URL https://doi.org/10.1109/ISIT54713.2023.
10206641.
[30] Linara Adilova, Bernhard C. Geiger, and Asja Fischer. Information plane analysis for dropout
neural networks, 2023.
[31] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information.
Phys. Rev. E, 69:066138, Jun 2004. doi: 10.1103/PhysRevE.69.066138. URL https://link.
aps.org/doi/10.1103/PhysRevE.69.066138.
[32] Paweł Czy˙z, Frederic Grabowski, Julia E. Vogt, Niko Beerenwinkel, and Alexander Marx.
The mixtures and the neural critics: On the pointwise mutual information profiles of fine
distributions, 2023.
[33] Y. Polyanskiy and Y. Wu. Information Theory: From Coding to Learning. Cambridge Uni-
versity Press, 2024. ISBN 9781108832908. URL https://books.google.ru/books?id=
CySo0AEACAAJ.
[34] I. Kobyzev, S. D. Prince, and M. A. Brubaker. Normalizing flows: An introduction and review
of current methods. IEEE Transactions on Pattern Analysis & Machine Intelligence, 43(11):
3964–3979, nov 2021. ISSN 1939-3539. doi: 10.1109/TPAMI.2020.2992934.
[35] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in
Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006.
[36] H. O. Lancaster. The structure of bivariate distributions. The Annals of Mathematical Statistics,
29(3):719–736, 1958. ISSN 00034851. URL http://www.jstor.org/stable/2237259.
[37] E. J. Hannan.
The general theory of canonical correlation and its relation to functional
analysis.
Journal of the Australian Mathematical Society, 2(2):229–242, 1961.
doi:
10.1017/S1446788700026707.
[38] M. D. Donsker and S. R.S. Varadhan. Asymptotic evaluation of certain markov process
expectations for large time. iv. Communications on Pure and Applied Mathematics, 36(2):
183–212, March 1983. ISSN 0010-3640. doi: 10.1002/cpa.3160360204.
[39] XuanLong Nguyen, Martin J Wainwright, and Michael Jordan. Estimating divergence func-
tionals and the likelihood ratio by penalized convex risk minimization. In J. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, vol-
ume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper_
files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf.
[40] Tomohiro Nishiyama. A new lower bound for kullback-leibler divergence based on hammersley-
chapman-robbins bound, 2019.
[41] Yunfei Teng and Anna Choromanska. Invertible autoencoder for domain adaptation. Com-
putation, 7(2), 2019. ISSN 2079-3197. doi: 10.3390/computation7020020. URL https:
//www.mdpi.com/2079-3197/7/2/20.
[42] Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density
estimation. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,
Advances in Neural Information Processing Systems, volume 33, pages 442–453. Curran
Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/
2020/file/051928341be67dcba03f0e04104d9047-Paper.pdf.
[43] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On vari-
ational bounds of mutual information. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pages 5171–5180. PMLR, 06 2019. URL
https://proceedings.mlr.press/v97/poole19a.html.
[44] L. F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector.
Problems Inform. Transmission, 23:95–101, 1987.
[45] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates,"
REFERENCES,0.37961335676625657,"Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/
d139db6a236200b21cc7f752979132d0-Paper.pdf.
[46] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In
International Conference on Learning Representations, 2017. URL https://openreview.
net/forum?id=HkpbnH9lx.
[47] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via
information, 2017.
[48] Haiyun He, Christina Yu, and Ziv Goldfeld. Information-theoretic generalization bounds
for deep neural networks. In NeurIPS 2023 workshop: Information-Theoretic Principles in
Cognitive Systems, 2023. URL https://openreview.net/forum?id=udEjq72DFO.
[49] Qinsheng Zhang and Yongxin Chen.
Diffusion normalizing flow.
In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in
Neural Information Processing Systems, volume 34, pages 16280–16291. Curran Associates,
Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
876f1f9954de0aa402d91bb988d12cd4-Paper.pdf.
[50] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with
diffusion models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,
Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/
forum?id=2LdBqxc1Yv.
[51] T. Tony Cai, Tengyuan Liang, and Harrison H. Zhou. Law of log determinant of sample
covariance matrix and optimal estimation of differential entropy for high-dimensional gaussian
distributions. Journal of Multivariate Analysis, 137:161–172, 2015. ISSN 0047-259X. doi: https:
//doi.org/10.1016/j.jmva.2015.02.003. URL https://www.sciencedirect.com/science/
article/pii/S0047259X1500038X.
[52] R. Arellano-Valle, Javier Contreras-Reyes, and Marc Genton. Shannon entropy and mutual
information for multivariate skew-elliptical distributions. Scandinavian Journal of Statistics,
40:42–62, 03 2013. doi: 10.1111/j.1467-9469.2011.
[53] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs
[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
[54] Vincent Stimper, David Liu, Andrew Campbell, Vincent Berenz, Lukas Ryll, Bernhard
Schölkopf, and José Miguel Hernández-Lobato. normflows: A pytorch package for normalizing
flows. Journal of Open Source Software, 8(86):5361, 2023. doi: 10.21105/joss.05361. URL
https://doi.org/10.21105/joss.05361.
[55] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."
REFERENCES,0.38137082601054484,"A
Complete proofs"
REFERENCES,0.38312829525483305,"Theorem 2.1. Let ξ : Ω→Rn′ be an absolutely continuous random vector, and let g: Rn′ →Rn
be an injective piecewise-smooth mapping with Jacobian J, satisfying n ≥n′ and det
 
JT J

̸= 0
almost everywhere. Let PDFs pξ and pξ|η exist. Then"
REFERENCES,0.38488576449912126,"PMIξ,η(ξ, η)
a.s.
= PMIg(ξ),η(g(ξ), η),
I(ξ; η) = I (g(ξ); η)
(5)"
REFERENCES,0.3866432337434095,"Proof of Theorem 2.1. For any function g, let us denote
p"
REFERENCES,0.3884007029876977,"det (JT (x)J(x)) (area transformation
coefficient) by α(x) where it exists."
REFERENCES,0.39015817223198596,"Foremost, let us note that in both cases, pξ(x | η) and pg(ξ)(x′ | η) = pξ(x | η)/α(x) exist.
Hereinafter, we integrate over supp ξ∩{x | α(x) ̸= 0} instead of supp ξ; as α ̸= 0 almost everywhere
by the assumption, the values of the integrals are not altered."
REFERENCES,0.39191564147627417,"According to the definition of the differential entropy,"
REFERENCES,0.3936731107205624,"h(g(ξ)) = −
Z pξ(x)"
REFERENCES,0.3954305799648506,"α(x) log
pξ(x) α(x)"
REFERENCES,0.39718804920913886,"
α(x) dx ="
REFERENCES,0.3989455184534271,"= −
Z
pξ(x) log (pξ(x)) dx +
Z
pξ(x) log (α(x)) dx ="
REFERENCES,0.4007029876977153,= h(ξ) + E log α(ξ).
REFERENCES,0.4024604569420035,h(g(ξ) | η) = Eη
REFERENCES,0.40421792618629176,"
−
Z pξ(x | η)"
REFERENCES,0.40597539543058,"α(x)
log
pξ(x | η) α(x)"
REFERENCES,0.4077328646748682,"
α(x) dx

= = Eη"
REFERENCES,0.4094903339191564,"
−
Z
pξ(x | η) log (pξ(x | η)) dx +
Z
pξ(x | η) log (α(x)) dx

="
REFERENCES,0.4112478031634446,= h(ξ | η) + E log α(ξ)
REFERENCES,0.4130052724077329,"Finally, by the MI definition,"
REFERENCES,0.4147627416520211,I(g(ξ); η) = h(g(ξ)) −h(g(ξ) | η) = h(ξ) −h(ξ | η) = I(ξ; η).
REFERENCES,0.4165202108963093,Dropping the expectations/integrals in the equations above yields the proof of the PMI invariance.
REFERENCES,0.4182776801405975,"Theorem 3.1. Let (ξ, η) be absolutely continuous with PDF pξ,η. Let qξ,η be a PDF defined on the
same space as pξ,η. Let pξ, pη, qξ and qη be the corresponding marginal PDFs. Then"
REFERENCES,0.4200351493848858,"I(ξ; η) = EPξ,η log
 qξ,η(ξ, η)"
REFERENCES,0.421792618629174,qξ(ξ)qη(η) 
REFERENCES,0.4235500878734622,"|
{z
}
Iq(ξ;η)"
REFERENCES,0.4253075571177504,"+ DKL (pξ,η ∥qξ,η) −DKL (pξ ⊗pη ∥qξ ⊗qη)
(6)"
REFERENCES,0.4270650263620387,"Proof of Theorem 3.1. In the following text, all the expectations are in terms of Pξ,η."
REFERENCES,0.4288224956063269,"I(ξ; η) = E log
 pξ,η(ξ, η)"
REFERENCES,0.4305799648506151,pξ(ξ)pη(η)
REFERENCES,0.43233743409490333,"
= E log
 qξ,η(ξ, η)"
REFERENCES,0.43409490333919154,"qξ(ξ)qη(η) · pξ,η(ξ, η)"
REFERENCES,0.4358523725834798,"qξ,η(ξ, η) · qξ(ξ)qη(η)"
REFERENCES,0.437609841827768,"pξ(ξ)pη(η) 
="
REFERENCES,0.43936731107205623,"= Iq(ξ; η) + E log
pξ,η(ξ, η)"
REFERENCES,0.44112478031634444,"qξ,η(ξ, η)"
REFERENCES,0.4428822495606327,"
+ E log
qξ(ξ) pξ(ξ)"
REFERENCES,0.4446397188049209,"
+ E log
qη(η) pη(η) 
="
REFERENCES,0.44639718804920914,"= Iq(ξ; η) + DKL (pξ,η ∥qξ,η) −DKL (pξ ⊗pη ∥qξ ⊗qη)"
REFERENCES,0.44815465729349735,"Corollary 3.2. Under the assumptions of Theorem 3.1, |I(ξ; η) −Iq(ξ; η)| ≤DKL (pξ,η ∥qξ,η)."
REFERENCES,0.44991212653778556,"Proof of Corollary 3.2. As DKL (pξ ⊗pη ∥qξ ⊗qη) ≥0,"
REFERENCES,0.45166959578207383,"I(ξ; η) ≤Iq(ξ, η) + DKL (pξ,η ∥qξ,η)"
REFERENCES,0.45342706502636204,"As DKL (pξ,η ∥qξ,η) ≥DKL (pξ ∥qξ) and DKL (pξ,η ∥qξ,η) ≥DKL (pη ∥qη) (monotonicity prop-
erty, see Theorem 2.16 in [33]),"
REFERENCES,0.45518453427065025,"I(ξ; η) ≥Iq(ξ; η) + DKL (pξ,η ∥qξ,η) −2 · DKL (pξ,η ∥qξ,η) = Iq(ξ; η) −DKL (pξ,η ∥qξ,η)"
REFERENCES,0.45694200351493847,"Corollary 3.3 (ˆIMIENF is consistent). Let X, Y , ˆf −1
X and ˆf −1
Y
satisfy the conditions of Theorem 2.1.
Let {(xk, yk)}N
k=1 be an i.i.d. sampling from (X, Y ). Let Q be a family of universal PDF approxi-
mators for a class of densities containing PX,Y ◦(f −1
X × f −1
Y ) (pushforward probability measure in
the latent space), meaning the convergence in probability of a maximum-likelihood estimate from Q
to the ground-truth distribution if N increases. Let ˆqN ∈Q be a maximum-likelihood estimate of
PX,Y ◦(f −1
X × f −1
Y ) from the samples {(fX(xk), fY (yk))}N
k=1. Let IˆqN (fX(X); fY (Y )) exist for
every N. Then
ˆIMIENF({(xk, yk)}N
k=1)
P
−→
N→∞I(X; Y )"
REFERENCES,0.45869947275922673,"Proof of Corollary 3.3. Following the assumptions on ˆqN, DKL
 
PX,Y ◦(f −1
X × f −1
Y ) ∥(ˆqN)ξ,η

P
−→
N→∞
0 (universality property).
Due to Corollary 3.2, this ensures IˆqN (fX(X); fY (Y ))
P
−→
N→∞
I(fX(X); fY (Y ))
=
I(X; Y ) (the latter equality is due to Theorem 2.1).
Finally,
ˆIMIENF(X; Y )
P
−→
N→∞IˆqN (fX(X); fY (Y )) as an MC estimate."
REFERENCES,0.46045694200351495,"Theorem 4.1 (Theorem 8.6.5 in [35]). Let Z be a d-dimensional absolutely continuous random
vector with probability density function pZ, mean m and covariance matrix Σ. Then"
REFERENCES,0.46221441124780316,"h(Z) = h (N(m, Σ))−DKL (pZ ∥N(m, Σ)) = d"
REFERENCES,0.46397188049209137,2 log(2πe)+ 1
REFERENCES,0.46572934973637964,"2 log det Σ−DKL (pZ ∥N(m, Σ))"
REFERENCES,0.46748681898066785,"Proof of Theorem 4.1. As h(Z −m) = h(Z), let us consider a centered random vector Z. We denote
probability density function of N(0, Σ) as ϕΣ."
REFERENCES,0.46924428822495606,"DKL (pZ ∥N(0, Σ)) =
Z"
REFERENCES,0.4710017574692443,Rd pZ(z) log pZ(z)
REFERENCES,0.4727592267135325,"ϕΣ(z) dz = −h(Z) −
Z"
REFERENCES,0.47451669595782076,Rd pZ(z) log ϕΣ(z) dz
REFERENCES,0.47627416520210897,"Note that
Z"
REFERENCES,0.4780316344463972,Rd pZ(z) log ϕΣ(z) dz = const+1
REFERENCES,0.4797891036906854,2 EZ zT Σ−1z = const+1
REFERENCES,0.48154657293497366,"2 EN(0,Σ) zT Σ−1z =
Z"
REFERENCES,0.4833040421792619,"Rd ϕΣ(z) log ϕΣ(z) dz,"
REFERENCES,0.4850615114235501,from which we arrive at the final result:
REFERENCES,0.4868189806678383,"DKL (pZ ∥N(0, Σ)) = −h(Z) + h(N(0, Σ))"
REFERENCES,0.48857644991212656,"Corollary 4.3. Let (ξ, η) be an absolutely continuous pair of random vectors with joint and marginal
probability density functions pξ,η, pξ and pη correspondingly, and mean and covariance matrix being"
REFERENCES,0.4903339191564148,"m =

mξ
mη"
REFERENCES,0.492091388400703,"
,
Σ =

Σξ,ξ
Σξ,η
Ση,ξ
Ση,η  Then"
REFERENCES,0.4938488576449912,I(ξ; η) = 1
REFERENCES,0.4956063268892794,"2 [log det Σξ,ξ + log det Ση,η −log det Σ] +"
REFERENCES,0.4973637961335677,"+ DKL (pξ,η ∥N(m, Σ)) −DKL (pξ ⊗pη ∥N(m, diag(Σξ,ξ, Ση,η)) ,"
REFERENCES,0.4991212653778559,which implies the following in the case of marginally Gaussian ξ and η:
REFERENCES,0.5008787346221442,I(ξ; η) ≥1
REFERENCES,0.5026362038664324,"2 [log det Σξ,ξ + log det Ση,η −log det Σ] ,
(8)"
REFERENCES,0.5043936731107206,"with the equality holding if and only if (ξ, η) are jointly Gaussian."
REFERENCES,0.5061511423550088,"Proof of Corollary 4.3. By applying Theorem 4.1 to Equation (2), we derive the following expres-
sion:"
REFERENCES,0.507908611599297,I(ξ; η) = 1
REFERENCES,0.5096660808435852,"2 [log det Σξ,ξ + log det Ση,η −log det Σ] +"
REFERENCES,0.5114235500878734,"+ DKL (pξ,η ∥N(m, Σ)) −DKL (pξ ∥N(mξ, Σξ,ξ)) −DKL (pη ∥N(mη, Ση,η))"
REFERENCES,0.5131810193321616,Note that
REFERENCES,0.5149384885764499,"DKL (pξ ∥N(mξ, Σξ,ξ)) + DKL (pη ∥N(mη, Ση,η)) = DKL (pξ ⊗pη ∥N(m, diag(Σξ,ξ, Ση,η)) ,"
REFERENCES,0.5166959578207382,which results in
REFERENCES,0.5184534270650264,I(ξ; η) = 1
REFERENCES,0.5202108963093146,"2 [log det Σξ,ξ + log det Ση,η −log det Σ] +"
REFERENCES,0.5219683655536028,"+ DKL (pξ,η ∥N(m, Σ)) −DKL (pξ ⊗pη ∥N(m, diag(Σξ,ξ, Ση,η))"
REFERENCES,0.523725834797891,"Corollary 4.4. Under the assumptions of Corollary 4.3,
I(ξ; η) −1"
REFERENCES,0.5254833040421792,"2 [log det Σξ,ξ + log det Ση,η −log det Σ]
 ≤DKL (pξ,η ∥N(m, Σ)) ."
REFERENCES,0.5272407732864675,Proof of Corollary 4.4. The same as for Corollary 3.2
REFERENCES,0.5289982425307557,Statement 4.8.
REFERENCES,0.5307557117750439,"LSN ◦(fX×fY ) ({(xk, yk)}) = log
det ∂f(x, y)"
REFERENCES,0.5325131810193322,"∂(x, y)"
REFERENCES,0.5342706502636204,"+ LN( ˆm,ˆΣ)({f(xk, yk)}), where"
REFERENCES,0.5360281195079086,"log
det ∂f(x, y)"
REFERENCES,0.5377855887521968,"∂(x, y)"
REFERENCES,0.539543057996485,"= log
det ∂fX(x) ∂x"
REFERENCES,0.5413005272407733,"+ log
det ∂fY (y) ∂y ,"
REFERENCES,0.5430579964850615,"ˆm = 1 N N
X"
REFERENCES,0.5448154657293497,"k=1
f(xk, yk),
ˆΣ = 1 N N
X"
REFERENCES,0.546572934973638,"k=1
(f(xk, yk) −ˆm)(f(xk, yk) −ˆm)T"
REFERENCES,0.5483304042179262,"Proof of Statement 4.8. Due to the change of variables formula,"
REFERENCES,0.5500878734622144,"LSN ◦(fX×fY ) ({(xk, yk)}) = log
det ∂f(x, y)"
REFERENCES,0.5518453427065027,"∂(x, y)"
REFERENCES,0.5536028119507909,"+ LN ({f(xk, yk)})"
REFERENCES,0.5553602811950791,"As f = fX × fY , the Jacobian matrix ∂f(x,y)"
REFERENCES,0.5571177504393673,"∂(x,y) is block-diagonal, so"
REFERENCES,0.5588752196836555,"log
det ∂f(x, y)"
REFERENCES,0.5606326889279437,"∂(x, y)"
REFERENCES,0.562390158172232,"= log
det ∂fX(x) ∂x"
REFERENCES,0.5641476274165202,"+ log
det ∂fY (y) ∂y "
REFERENCES,0.5659050966608085,"Finally, we use the maximum-likelihood estimates for m and Σ to drop the supremum in
LN ({f(xk, yk)}):"
REFERENCES,0.5676625659050967,"ˆm = 1 N N
X"
REFERENCES,0.5694200351493849,"k=1
f(xk, yk),
ˆΣ = 1 N N
X"
REFERENCES,0.5711775043936731,"k=1
(f(xk, yk) −ˆm)(f(xk, yk) −ˆm)T
=⇒"
REFERENCES,0.5729349736379613,"=⇒
LN ({f(xk, yk)}) = LN( ˆm,ˆΣ)({f(xk, yk)})"
REFERENCES,0.5746924428822495,"Lemma 4.9 (Lemma 2 in [25]). Let fX, fY be fixed. Let (ξ, η) have finite covariance matrix. Then,
the asymptotic variance of ˆIN-MIENF is O(d2/N), with d being the dimensionality. If (ξ, η) is also
Gaussian, the asymptotic variance is further improved to O(d/N)."
REFERENCES,0.5764499121265377,"Proof of Lemma 4.9. The variance of ˆIN-MIENF is upper bounded by the sum of the variances of the
log-det terms. If (ξ, η) is Gaussian, the log-det terms are asymptotically normal with the asymptotic
variance being 2d/N (Corollary 1 in [51]). If (ξ, η) is not Gaussian, the central limit theorem can be
applied to each element of the covariance matrix, which in combination with the delta method yields
the final result."
REFERENCES,0.5782073813708261,"Remark 4.10. Let A ∈Rd×d be a non-singular matrix, b ∈R, {zk}N
k=1 ⊆Rd. Then"
REFERENCES,0.5799648506151143,LSN ◦(Az+b)({zk}) = LSN ({zk})
REFERENCES,0.5817223198594025,Proof of Remark 4.10.
REFERENCES,0.5834797891036907,"LSN ◦(Az+b)({zk}) = log | det A|+LSN ({Azk+b}) = log | det A|+LN(A ˆm+b,AˆΣAT )({Azk+b}) ="
REFERENCES,0.5852372583479789,"= log | det A| + log | det A−1| + LN( ˆm,ˆΣ)({zk}) = LSN ({zk})"
REFERENCES,0.5869947275922671,"Corollary 4.12. If (ξ, η) ∼µ ∈Stridiag-N , then"
REFERENCES,0.5887521968365553,I(ξ; η) = −1 2 X
REFERENCES,0.5905096660808435,"j
log(1 −ρ2
j)
(10)"
REFERENCES,0.5922671353251318,"Proof of Corollary 4.12. Under the proposed assumptions, log det Σξ,ξ = log det Ση,η = 0, so
I(ξ; η) = −1"
REFERENCES,0.5940246045694201,2 log det Σ. The matrix Σ is block-diagonalizable via the following permutation:
REFERENCES,0.5957820738137083,"(ξ1, . . . , ξdξ, η1, . . . , ηdη) 7→(ξ1, η1, ξ2, η2, . . .),"
REFERENCES,0.5975395430579965,with the blocks being
REFERENCES,0.5992970123022847,"Σξj,ηj =

1
ρj
ρj
1 "
REFERENCES,0.6010544815465729,"The determinant of block-diagonal matrix is a product of the block determinants, so I(ξ; η) =
−1"
P,0.6028119507908611,"2
P
j log(1 −ρ2
j)."
P,0.6045694200351494,"Statement 4.13 (Canonical correlation analysis). Let (ξ, η) ∼N(m, Σ), where Σ is non-singular.
There exist invertible affine mappings φξ, φη such that (φξ(ξ), φη(η)) ∼µ ∈Stridiag-N . Due to
Theorem 2.1, the following also holds: I(ξ; η) = I(φξ(ξ); φη(η))."
P,0.6063268892794376,"Proof of Statement 4.13. Let us consider centered ξ and η, as shifting is an invertible affine mapping.
Note that Σξ,ξ and Ση,η are positive definite and symmetric, so the following symmetric matrix square
roots are defined: A = Σ−1/2
ξ,ξ
, B = Σ−1/2
η,η . By applying these invertible linear transformations to ξ
and η we get"
P,0.6080843585237259,"cov(Aξ, Bη) ="
P,0.6098418277680141,"""
Σ−1/2
ξ,ξ
Σξ,ξ(Σ−1/2
ξ,ξ
)T
Σ−1/2
ξ,ξ
Σξ,η(Σ−1/2
η,η )T"
P,0.6115992970123023,"Σ−1/2
η,η Ση,ξ(Σ−1/2
ξ,ξ
)T
Σ−1/2
η,η Ση,η(Σ−1/2
η,η )T #"
P,0.6133567662565905,"=
 I
C
CT
I 
,"
P,0.6151142355008787,"where C = Σ−1/2
ξ,ξ
Σξ,η(Σ−1/2
η,η )T ."
P,0.616871704745167,"Then, the singular value decomposition is performed: C = URV T , where U and V are orthogonal,
R = diag({ρj}). Finally, we apply U T to Aξ and V T to Bη:"
P,0.6186291739894552,"cov(U T Aξ, V T Bη) =

U T U
U T CV
(U T CV )T
V T V"
P,0.6203866432337434,"
=
 I
R
RT
I 
,"
P,0.6221441124780316,Note that U T A and V T B are invertible.
P,0.6239015817223199,"Statement 4.14. Let (ξ, η) ∼N(0, Σ) ∈Stridiag-N , {zk}N
k=1 ⊆Rdξ+dη. Then"
P,0.6256590509666081,"LN(0,Σ)({zk}) = I(ξ; η) + LN(0,I)({Σ−1/2zk}),"
P,0.6274165202108963,"where (implying ρj = 0 for j > min{dξ, dη})"
P,0.6291739894551845,Σ−1/2 =  
P,0.6309314586994728,"αj + βj
αj −βj
...
...
αj −βj
αj + βj"
P,0.632688927943761,"|
{z
}
dξ ..."
P,0.6344463971880492,"|
{z
}
dη ...  "
P,0.6362038664323374,"αj =
1
2p1 + ρj"
P,0.6379613356766256,"βj =
1
2p1 −ρj"
P,0.6397188049209139,and I(ξ; η) is calculated via (10).
P,0.6414762741652021,"Proof of Statement 4.14. Note that Σ is positive definite and symmetric, so the following symmetric
matrix square root is defined: Σ−1/2. This matrix is a normalization matrix: cov(Σ−1/2(ξ, η)) =
Σ−1/2 Σ (Σ−1/2)T = I."
P,0.6432337434094904,"According to the change of variable formula,"
P,0.6449912126537786,"LN(0,Σ)({zk}) = log det Σ−1/2 + LN(0,I)({Σ−1/2zk})"
P,0.6467486818980668,"As Σξ,ξ = Idξ and Ση,η = Idη, from the equation (8) we derive"
P,0.648506151142355,I(ξ; η) = −1
P,0.6502636203866432,2 log det Σ = log det Σ−1/2
P,0.6520210896309314,"Finally, it is sufficient to validate the proposed closed-form expression for Σ1/2 in the case of 2 × 2
matrices, as Σ is block-diagonalizable (with 2 × 2 blocks) via the following permutation:"
P,0.6537785588752196,"M : (ξ1, . . . , ξdξ, η1, . . . , ηdη) 7→(ξ1, η1, ξ2, η2, . . .),"
P,0.655536028119508,"Note that

α + β
α −β
α −β
α + β"
P,0.6572934973637962,"2
= 2 ·

α2 + β2
α2 −β2"
P,0.6590509666080844,"α2 −β2
α2 + β2"
P,0.6608084358523726,"
=
1
1 −ρ2"
P,0.6625659050966608,"
1
−ρ
−ρ
1 "
P,0.664323374340949,"1
1 −ρ2"
P,0.6660808435852372,"
1
−ρ
−ρ
1"
P,0.6678383128295254,"
·

1
ρ
ρ
1"
P,0.6695957820738138,"
=

1
0
0
1 "
P,0.671353251318102,"B
Non-Gaussian-based tests"
P,0.6731107205623902,"As our estimator is based on Gaussianization, it seems natural that we observe good performance
in the experiments with synthetic data acquired from the correlated Gaussian vectors via invertible
transformations. Possible bias towards such data can not be discriminated via the independency and
self-consistency tests, and hard to discriminate via the data-processing test proposed in [14, 20] for
the following reasons:"
P,0.6748681898066784,"1. Independency test requires ˆI(X; Y ) ≈0 for independent X and Y . In such case, as
cov(fX(X), fY (Y )) = 0 for any measurable fX and fY , ˆIMIENF(X; Y ) ≈0 in any mean-
ingful scenario (no overfitting, no ill-posed transformations), regardless of the marginal
distributions of X and Y .
2. Self-consistency test requires ˆI(X; Y ) ≈ˆI(g(X); Y ) for X, Y and g satisfying Theorem 2.1.
In our setup, this test only measures the ability of normalizing flows to invert g, and provides
no information about the quality of ˆI(X; Y ) and ˆI(g(X); Y ).
Moreover, as we leverage Algorithm 1 with the Gaussian base distribution for the dataset
generation, we somewhat test our estimator for the self-consistency."
P,0.6766256590509666,"3. Data-processing test leverages the data processing inequality [35] via requiring ˆI(X; Y ) ≥
ˆI(g(X); Y ) for any X, Y and measurable g. Theoretically, this test may highlight the bias
of our estimator towards binormalizable data. However, this requires constructing X, Y and
g, so X and Y are not binormalizable, g(X) and Y are and ˆI(X; Y ) < ˆI(g(X); Y ), which
seems challenging to achieve."
P,0.6783831282952548,"That is why we use two additional, non-Gaussian-based families of distributions with known closed-
form expressions for MI and easy sampling procedures: multivariate Student distribution [52] and
smoothed uniform distribution [14]."
P,0.680140597539543,"In the following subsections, we provide additional information about the distributions, closed-form
expressions for MI and sampling procedures."
P,0.6818980667838312,"B.1
Multivariate Student distribution"
P,0.6836555360281195,"Consider (n+m)-dimensional ( ˜X; ˜Y ) ∼N(0, Σ), where Σ is selected to achieve I( ˜X; ˜Y ) = κ > 0.
Firstly, a correction term is calculated in accordance to the following formula:"
P,0.6854130052724078,"c(k, n, m) = f(k) + f(k + n + m) −f(k + n) −f(k + m),
f(x) = log Γ
x 2 
−x"
P,0.687170474516696,"2 ψ
x 2 
,"
P,0.6889279437609842,"where k is the number of degrees of freedom, ψ is the digamma function. Secondly, X = ˜X/
p"
P,0.6906854130052724,"k/U
and Y = ˜Y /
p"
P,0.6924428822495606,"k/U are defined, where U ∼χ2
k. The resulting vectors are distributed according
to the multivariate Student distribution with k degrees of freedom. According to [52], I(X; Y ) =
κ + c(k, n, m). During the generation, κ is set to I(X; Y ) −c(k, n, m) to achieve the desired value
of I(X; Y )."
P,0.6942003514938488,"Note that I(X; Y ) ̸= 0 even in the case of independent ˜X and ˜Y , as some information between X
and Y is shared via the magnitude."
P,0.6959578207381371,"B.2
Smoothed uniform distribution"
P,0.6977152899824253,"Lemma B.1. Consider independent X ∼U[0; 1], Z ∼U[−ε; ε] and Y = X + Z. Then"
P,0.6994727592267135,I(X; Y ) =
P,0.7012302284710018,"(
ε −log(2ε),
ε < 1/2"
P,0.70298769771529,"(4ε)−1,
ε ≥1/2
(14)"
P,0.7047451669595782,Proof. Probability density function of Y (two cases):
P,0.7065026362038664,"(ε < 1/2) :
pY (y) = (pX ∗pZ)(y) ="
P,0.7082601054481547,"


 

"
P,0.7100175746924429,"0,
y < −ε ∨y ≥1 + ε
y+ε"
P,0.7117750439367311,"2ε ,
−ε ≤y < ε
1,
ε ≤y < 1 −ε
1+ε−y"
P,0.7135325131810193,"2ε
,
1 −ε ≤y < 1 + ε"
P,0.7152899824253075,"(ε ≥1/2) :
pY (y) = (pX ∗pZ)(y) ="
P,0.7170474516695958,"


 

"
P,0.718804920913884,"0,
y < −ε ∨y ≥1 + ε
y+ε"
P,0.7205623901581723,"2ε ,
−ε ≤y < 1 −ε
1
2ε,
1 −ε ≤y < ε
1+ε−y"
P,0.7223198594024605,"2ε
,
ε ≤y < 1 + ε
Differential entropy of a uniformly distributed random variable:"
P,0.7240773286467487,h(U[a; b]) = log(b −a)
P,0.7258347978910369,Conditional differential entropy of Y with respect to X:
P,0.7275922671353251,h(Y | X) = Ex∼X h(Y | X = x) = Ex∼X h(Z + x | X = x)
P,0.7293497363796133,"As X and Z are independent,"
P,0.7311072056239016,Ex∼X h(Z + x | X = x) = Ex∼X h(Z + x) =
Z,0.7328646748681898,"1
Z"
Z,0.7346221441124781,"0
log(2ε) dx = log(2ε)
(15)"
Z,0.7363796133567663,Differential entropy of Y :
Z,0.7381370826010545,"h(Y ) = − ∞
Z"
Z,0.7398945518453427,"−∞
pY (y) dy =
ε,
ε < 1/2
(4ε)−1 + log(2ε),
ε ≥1/2
(16)"
Z,0.7416520210896309,The final result is acquired via substituting (15) and (16) into (1).
Z,0.7434094903339191,Equation (14) can be inverted:
Z,0.7451669595782073,"ε =
(4 · I(X; Y ))−1,
I(X; Y ) < 1/2
−W

−1"
Z,0.7469244288224957,"2 exp(−I(X; Y ))

,
I(X; Y ) ≥1/2 ,
(17)"
Z,0.7486818980667839,where W is the product logarithm function.
Z,0.7504393673110721,"B.3
Additional experiments"
Z,0.7521968365553603,"Recall that in Section 5, we do not evaluate other estimators on the tests discussed in this part of the
Appendix. To address this, we provide additional results for MINE and DINE-Gaussian in Figure 5.
We chose MINE as it is the best performing critic-based method judging by the results from Figure 3,
and is widely considered as a decent modern baseline. We chose DINE-Gaussian as (a) this method
also employs normalizing flows, and (b) we were not able to acquire reliable estimates via this method
during the tests presented in Figure 3. The results are presented in Figure 5. 0 2 4 6 8 10 12 14"
Z,0.7539543057996485,"2
4
8
16
32"
Z,0.7557117750439367,"ˆI(X, Y )"
Z,0.7574692442882249,dimensionality
Z,0.7592267135325131,Ground truth
Z,0.7609841827768014,"Uniform
Smoothed uniform"
Z,0.7627416520210897,"Student (dof = 4)
arcsinh(Student) (dof = 1)
arcsinh(Student) (dof = 2)"
Z,0.7644991212653779,(a) DINE-Gaussian 0 2 4 6 8 10 12 14
Z,0.7662565905096661,"2
4
8
16
32"
Z,0.7680140597539543,"ˆI(X, Y )"
Z,0.7697715289982425,dimensionality
Z,0.7715289982425307,Ground truth
Z,0.773286467486819,"Uniform
Smoothed uniform"
Z,0.7750439367311072,"Student (dof = 4)
arcsinh(Student) (dof = 1)
arcsinh(Student) (dof = 2)"
Z,0.7768014059753954,(b) MINE 0 2 4 6 8 10
Z,0.7785588752196837,"2
4
8
16
32"
Z,0.7803163444639719,"ˆI(X, Y )"
Z,0.7820738137082601,dimensionality
Z,0.7838312829525483,Ground truth
Z,0.7855887521968365,"Uniform
Smoothed uniform"
Z,0.7873462214411248,"Student (dof = 4)
arcsinh(Student) (dof = 1)
arcsinh(Student) (dof = 2)"
Z,0.789103690685413,(c) N-MIENF 0 2 4 6 8 10
Z,0.7908611599297012,"2
4
8
16
32"
Z,0.7926186291739895,"ˆI(X, Y )"
Z,0.7943760984182777,dimensionality
Z,0.7961335676625659,Ground truth
Z,0.7978910369068541,"Uniform
Smoothed uniform"
Z,0.7996485061511424,"Student (dof = 4)
arcsinh(Student) (dof = 1)
arcsinh(Student) (dof = 2)"
Z,0.8014059753954306,(d) tridiag-N-MIENF
Z,0.8031634446397188,"Figure 5: Additional tests with incompressible multidimensional data from Figure 4. 10 · 103 samples
were used. We conduct only one experiment per point for DINE-Gaussian and MINE, acquiring CIs
from epoch-wise averaging instead. Note that DINE-Gaussian failed to estimate MI for in the case of
high-dimensional smoothed uniform distribution due to numerical instabilities. We also provide the
plots for N-MIENF and tridiag-N-MIENF to facilitate the comparison."
Z,0.804920913884007,"C
Overfitting and sample complexity"
Z,0.8066783831282952,"Due to the curse of dimensionality being a universal issue for MI-related tasks [13], our method, as
any other NN-based estimator, is prone to overfitting in the case of small sample size. We illustrate
this by performing an MI estimation on one of the benchmarks from Section 5, with the sampling
being of normal and tiny size. We also conduct similar experiments with MINE. The resulting
probability density and pointwise mutual information functions are presented in Figures 6 and 7."
Z,0.8084358523725835,"(a) I(X; Y ) = 1.0,
ˆI(X; Y ) ≈0.99, 2048 samples."
Z,0.8101933216168717,"(b) I(X; Y ) = 0.0,
ˆI(X; Y ) ≈0.0, 2048 samples."
Z,0.81195079086116,"(c) I(X; Y ) = 0.0,
ˆI(X; Y ) ≈1.84, 5 samples."
Z,0.8137082601054482,"Figure 6: Point-wise mutual information plots for MINE. Correlated uniform distribution is used,
with varying ground truth MI and sampling size. Note that in the case of an insufficient sampling
size, MINE “memorizes” the data points and “hallucinates” the relation between X and Y , which
severely increases the value of the MI estimate."
Z,0.8154657293497364,"(a) I(X; Y ) = 1.0,
ˆI(X; Y ) ≈0.99, 2048 samples."
Z,0.8172231985940246,"(b) I(X; Y ) = 0.0,
ˆI(X; Y ) ≈0.0, 2048 samples."
Z,0.8189806678383128,"(c) I(X; Y ) = 0.0,
ˆI(X; Y ) ≈0.92, 5 samples."
Z,0.820738137082601,"Figure 7: Probability density function plots for tridiag-N-MIENF. Correlated uniform distribution
is used, with varying ground truth MI and sampling size. Note that in the case of an insufficient
sampling size, MIENF “memorizes” the data points and “hallucinates” the relation between X and
Y , which severely increases the value of the MI estimate."
Z,0.8224956063268892,"D
Information-theoretic disentanglement"
Z,0.8242530755711776,"To explore additional applications of our method, we consider the task of representation disentangle-
ment, i.e., the process of separating data into independent variables with distinct semantic meaning.
For this example, we use the MNIST dataset of handwritten digits [53]."
Z,0.8260105448154658,"Let X be a random image of a handwritten digit. Consider a Markov kernel X →(X′, X′′)
corresponding to a pair of random augmentations applied to X (we use random translation, rotation,
zoom, and shear from torchvision.transforms). Now consider the task of estimating I(X′; X′′)
(MI between the two augmented versions of the same image). Note that tridiag-N-MIENF estimates
the MI and performs a nonlinear canonical correlation analysis simultaneously (because of the
tridiagonal covariance matrix in the latent space). Moreover, {ρj} (from Definition 4.11) represent"
Z,0.827768014059754,"the dependence between the nonlinear components. Higher values of ρj (and, as a consequence, of
the per-component MI) are expected to correspond to the nonlinear components, which are invariant
to the selected augmentations (e.g., width/height ratio of a digit, thickness of strokes, etc.). We also
expect small values of ρj to represent the components, which parametrize the augmentations used in
our setup (e.g, translation, zoom, etc.)."
Z,0.8295254833040422,"To perform the experiment, we train tridiag-N-MIENF on samples from (X′, X′′). We then randomly
select several images from X, acquire their latent representations, apply a small perturbation along
the axes corresponding to the highest and the lowest values of (one axis at a time), and perform an
inverse transformation to visualize the result. We observe the expected behavior. The results are
provided in Figure 8. We use a convolutional autoencoder beforehand to reduce the dimensionality
(the size of the bottleneck is 64) and speed up the experiment."
Z,0.8312829525483304,"E
Technical details"
Z,0.8330404217926186,"In this section, we describe the technical details of our experimental setup: architecture of the neural
networks, hyperparameters, etc."
Z,0.8347978910369068,"For the tests described in Section 5, we use architectures listed in Table 2. For the flow models,
we use the normflows package [54]. The autoencoders are trained via Adam [55] optimizer on
5 · 103 images with a batch size 5 · 103, a learning rate 10−3 and MAE loss for 2 · 103 epochs. The
MINE/NWJ/Nishiyama critic network is trained via the Adam optimizer on 5 · 103 pairs of images
with a batch size 512, a learning rate 10−3 for 5 · 103 epochs. The GLOW normalizing flow is trained
via the Adam optimizer on 10 · 103 images with a batch size 1024, a learning rate decaying from
5 · 10−4 to 1 · 10−5 for 2 · 103 epochs. Nvidia Titan RTX was used to train the models. In any setup,
each experiment took no longer than one hour to be completed. In the following repositories, we
provide PyTorch implementations of the NN-based estimators we used: https://github.com/
VanessB/pytorch-kld and https://github.com/VanessB/pytorch-mienf."
Z,0.836555360281195,Table 2: The NN architectures used to conduct the tests in Section 5.
Z,0.8383128295254832,"NN
Architecture"
Z,0.8400702987697716,"AEs,
16 × 16 (32 × 32)
images"
Z,0.8418277680140598,"×1:
Conv2d(1, 4, ks=3), BatchNorm2d, LeakyReLU(0.2), MaxPool2d(2)
×1:
Conv2d(4, 8, ks=3), BatchNorm2d, LeakyReLU(0.2), MaxPool2d(2)
×2(3):
Conv2d(8, 8, ks=3), BatchNorm2d, LeakyReLU(0.2), MaxPool2d(2)
×1:
Dense(8, dim), Tanh, Dense(dim, 8), LeakyReLU(0.2)
×2(3):
Upsample(2), Conv2d(8, 8, ks=3), BatchNorm2d, LeakyReLU(0.2)
×1:
Upsample(2), Conv2d(8, 4, ks=3), BatchNorm2d, LeakyReLU(0.2)
×1:
Conv2d(4, 1, ks=3), BatchNorm2d, LeakyReLU(0.2)"
Z,0.843585237258348,"MINE, critic NN,
16 × 16 (32 × 32)
images"
Z,0.8453427065026362,"×1:
[Conv2d(1, 16, ks=3), MaxPool2d(2), LeakyReLU(0.01)]×2 in parallel"
Z,0.8471001757469244,"×1(2):
[Conv2d(16, 16, ks=3), MaxPool2d(2), LeakyReLU(0.01)]×2 in parallel"
Z,0.8488576449912126,"×1:
Dense(256, 128), LeakyReLU(0.01)
×1:
Dense(128, 128), LeakyReLU(0.01)
×1:
Dense(128, 1)"
Z,0.8506151142355008,"GLOW,
16 × 16 (32 × 32)
images"
Z,0.8523725834797891,"×1:
4 (5) splits, 2 GLOW blocks between splits,
16 hidden channels in each block, leaky constant = 0.01
×1:
Orthogonal linear layer
×4:
RealNVP(AffineCouplingBlock(MLP(d/2, 32, d)), Permute-swap)"
Z,0.8541300527240774,"RealNVP,
d-dimensional data
×6:
RealNVP(AffineCouplingBlock(MLP(d/2, 64, d)), Permute-swap)"
Z,0.8558875219683656,"Here we do not explicitly define gξ and gη used in the tests with synthetic data, as these functions
smoothly map low-dimensional vectors to high-dimensional images and, thus, are very complex. A
Python implementation of the functions in question is available in the supplementary code repository,
see https://github.com/VanessB/mutinfo."
Z,0.8576449912126538,(a) Nonlinear component corresponding to the stroke thickness. MI ≈0.96.
Z,0.859402460456942,"(b) Nonlinear component corresponding to the width of a digit. MI ≈0.75.
..."
Z,0.8611599297012302,(c) Nonlinear component corresponding to zoom transformation. MI ≈0.002.
Z,0.8629173989455184,(d) Nonlinear component corresponding to vertical translation. MI < 0.001.
Z,0.8646748681898067,"Figure 8: Results of an information-based nonlinear canonical correlation analysis performed on
the MNIST handwritten digits dataset. The task of MI estimation between augmented (trans-
lated/rotated/...) versions of pictures is considered. Our method (the tridiagonal version) allows
for simultaneous MI estimation and nonlinear independent components learning. We illustrate the
semantics of the learned nonlinear components via small perturbations along the corresponding
directions in the latent space. The center of each row contains an original, unperturbed picture;
pictures to the left and to the right are the results of the perturbations. We also provide the values
of per-component MI. Components with high MI represent the features, which are invariant to the
selected augmentations."
Z,0.8664323374340949,NeurIPS Paper Checklist
CLAIMS,0.8681898066783831,1. Claims
CLAIMS,0.8699472759226714,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8717047451669596,Answer: [Yes]
CLAIMS,0.8734622144112478,"Justification: in our work, we dedicate Sections 3 and 4 to a proper introduction and
theoretical justification of the proposed approach. Section 5 contains the experimental
results showing a decent performance of our method. All the results match the claims from
the abstract and introduction."
CLAIMS,0.875219683655536,Guidelines:
CLAIMS,0.8769771528998243,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8787346221441125,2. Limitations
LIMITATIONS,0.8804920913884007,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8822495606326889,Answer: [Yes]
LIMITATIONS,0.8840070298769771,Justification: we have a dedicated paragraph in Section 6.
LIMITATIONS,0.8857644991212654,Guidelines:
LIMITATIONS,0.8875219683655536,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8892794376098418,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8910369068541301,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: we provide all the proofs in Appendix A.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8927943760984183,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.8945518453427065,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: we describe our experimental setup in Section 5, with corresponding technical
details being provided in Appendix E.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8963093145869947,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8980667838312829,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8998242530755711,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9015817223198594,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9033391915641477,"Justification:
the source code is available at https://github.com/VanessB/
pytorch-mienf."
OPEN ACCESS TO DATA AND CODE,0.9050966608084359,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9068541300527241,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9086115992970123,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9103690685413005,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9121265377855887,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9138840070298769,"Justification: we describe our experimental setup in Section 5, with corresponding technical
details being provided in Appendix E."
OPEN ACCESS TO DATA AND CODE,0.9156414762741653,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9173989455184535,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9191564147627417,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9209138840070299,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9226713532513181,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9244288224956063,Justification: results presented in Section 5 include confidence intervals.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9261862917398945,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9279437609841827,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.929701230228471,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9314586994727593,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: we provide all the information in Appendix E
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9332161687170475,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9349736379613357,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: the research conducted in the paper conform with the NeurIPS Code of Ethics
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9367311072056239,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9384885764499121,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: there is no societal impact of the work performed.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9402460456942003,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9420035149384886,"• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9437609841827768,11. Safeguards
SAFEGUARDS,0.945518453427065,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9472759226713533,Answer: [NA]
SAFEGUARDS,0.9490333919156415,Justification: the paper poses no such risks.
SAFEGUARDS,0.9507908611599297,Guidelines:
SAFEGUARDS,0.9525483304042179,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9543057996485061,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9560632688927944,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9578207381370826,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9595782073813708,"Justification: we cite the authors of all the benchmark datasets used in our work in Section 5.
We cite the authors of the normflows package in Appendix E."
LICENSES FOR EXISTING ASSETS,0.961335676625659,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9630931458699473,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.9648506151142355,"• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9666080843585237,13. New Assets
NEW ASSETS,0.968365553602812,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9701230228471002,Answer: [Yes]
NEW ASSETS,0.9718804920913884,Justification: the source code is documented via in-code commentary.
NEW ASSETS,0.9736379613356766,Guidelines:
NEW ASSETS,0.9753954305799648,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771528998242531,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9789103690685413,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806678383128296,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824253075571178,Justification: the paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984182776801406,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859402460456942,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876977152899824,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894551845342706,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912126537785588,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929701230228472,Justification: the paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947275922671354,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964850615114236,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982425307557118,"• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
