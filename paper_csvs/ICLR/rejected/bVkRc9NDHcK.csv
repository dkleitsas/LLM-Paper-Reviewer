Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0072992700729927005,"Steganography is the task of hiding and recovering secret data inside a non-secret
container data while making imperceptible changes to the container. When using
steganography to hide audio inside an image, current approaches neither allow
the encoding of a signal with variable length nor allow making a trade-off be-
tween secret data reconstruction quality and imperceptibility in the changes made
to the container image. To address this problem, we propose VLVQ (Variable
Length Variable Quality Audio Steganography), a deep learning based stegano-
graphic framework capable of hiding variable-length audio inside an image by
training the network to iteratively encode and decode the audio data from the con-
tainer image. Complementary to the standard reconstruction loss, we propose an
optional conditional loss term that allows the users to make quality trade-offs be-
tween audio and image reconstruction on inference time, without needing to train
a separate model for each trade-off setups. Our experiments on ImageNet and
AudioSet demonstrate VLVQ’s ability to retain reasonable image quality (28.99
psnr) and audio reconstruction quality (23.79 snrseg) while encoding 19 sec-
onds of audio. We also show VLVQ’s capability to generalize to signals longer
than what is seen during training."
INTRODUCTION,0.014598540145985401,"1
INTRODUCTION"
INTRODUCTION,0.021897810218978103,"Natural images are redundant (Ruderman, 1994). They contain predictable structures, repeating tex-
tures, and non-uniform color distribution that contribute to their low overall entropy. Meanwhile,
the human visual system is insensitive to minor changes to its visual ﬁeld. These features make
steganography, the science of hiding a secret data within a non-secret container data, possible. Sev-
eral researches have hidden various data inside images such as binary messages (Zhu et al., 2018),"
INTRODUCTION,0.029197080291970802,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0364963503649635,"separate images (Baluja, 2017; Tamimi et al., 2013; Wu et al., 2018; Zhang et al., 2019a;b; Duan
et al., 2019), and audio signals (Kaul & Bajaj, 2013; Santosa & Bao, 2005; Huu et al., 2019). Here,
we focus on the task of hiding audio inside images."
INTRODUCTION,0.043795620437956206,"Many algorithms have successfully hidden audio data inside images, especially with the recent ad-
vances on deep learning based steganography. However, the use of deep learning restricts these
algorithms to only receiving input dimensions that are allowed by the design of the network ar-
chitecture, which usually have ﬁxed dimensions. This condition makes these methods difﬁcult to
operate in the real world, where the signals may easily vary in length. Naively applying these tra-
ditional models iteratively on different chunks of the audio fails to work, as the input distribution
starts to diverge from that of the training set."
INTRODUCTION,0.051094890510948905,"Furthermore, as the bottleneck of most steganographic systems comes from the container data’s
capacity, hiding a varying length signal inside a ﬁxed-sized container requires the capability to
trade-off audio quality and image ﬁdelity. One approach to do so is by changing the coefﬁcients of
the different loss terms that each control the reconstruction quality of the container and the secret
data. However, this requires the training of several systems with different coefﬁcients, which is
not only time and memory consuming, but also fails to cover the full continuous range of possible
coefﬁcient combinations."
INTRODUCTION,0.058394160583941604,"We propose a new steganographic framework VLVQ (Variable Length Variable Quality Audio
Steganography) to encode variable length audio signals in an image and make audio quality and
image ﬁdelity trade-off with a single model on inference time. Figure 1 demonstrates an example of
VLVQ encoding 19 seconds of audio inside an 224x224 image then decoding it back."
INTRODUCTION,0.06569343065693431,"Given the initial container image, VLVQ iteratively embeds an audio chunk inside the image until
all of the audio has been encoded. To recover the audio back from this ﬁnal container image, VLVQ
isolates the latest audio chunk embedded inside the container, and update the container image to
feed itself back into the decoder recursively. This is repeated until all audio is recovered. An
important design choice is to use the trade-off coefﬁcient that linearly weighs the image and audio
reconstruction quality as an input to the network. By randomly sampling this coefﬁcient at each
training step and feeding it as input to the model, a single model can learn a distribution of solutions."
INTRODUCTION,0.072992700729927,"Section 2 reviews previous works related to VLVQ. Section 3 describes the problem setup and
VLVQ’s methodology in depth. Then, Section 4 evaluates our approach on ImageNet (Russakovsky
et al., 2015) and AudioSet (Gemmeke et al., 2017), demonstrating reasonable reconstruction qual-
ity even on signals longer than what is seen during training. Finally, Section 5 discusses various
observations and possible directions for future work."
RELATED WORKS,0.08029197080291971,"2
RELATED WORKS"
RELATED WORKS,0.08759124087591241,"Image Steganography. Steganography is the task of encoding a secret data inside a non-secret con-
tainer data without making noticeable changes to the container. Image steganography describes a
scenario that uses an image as the container that carries the secret data. The limitations of the human
visual system in noticing slight changes in brightness or color among many pixels make these image
steganography methods effective. Traditional image steganography methods exploit the image spa-
tial domain by modifying the least signiﬁcant bits (LSB) of the image, or the frequency domain with
Discrete Fourier Transform (DFT), Discrete Cosine Transform (DCT), and Discrete Wavelet Trans-
form (DWT). With the help of recent advancements in learning-based algorithms, steganographic
systems have seen tremendous advancements. Many of the learning-based steganographic systems
employs deep neural networks that are trained end-to-end with reconstruction losses that minimize
the change in the container while maximizing the similarity between the recovered secret data and
the ground truth. Baluja (2017) uses convolutional architecture to hide images inside images, while
Zhu et al. (2018) adopts differentiable augmentations to hide and recover binary data inside images
with robustness to image corruptions. Different approaches make improvements in architectural de-
signs (Duan et al., 2019; Cui et al., 2020; Wu et al., 2018) while some adopt Generative Adversarial
Networks (GANs) (Goodfellow et al., 2014) to make the changes further imperceptible."
RELATED WORKS,0.0948905109489051,"Hiding Audio in Image. Hiding audio signal inside images is a speciﬁc instance of image steganog-
raphy. Similar to image steganography in general, methods such as modifying the least signiﬁcant
bits (LSB) (Kaul & Bajaj, 2013) as well as wavelet transformations (Kaul & Bajaj, 2013; Santosa &"
RELATED WORKS,0.10218978102189781,Under review as a conference paper at ICLR 2022
RELATED WORKS,0.10948905109489052,"Figure 2: Difference between the original image and the ﬁnal container image containing 19 seconds
of audio, inferenced using the same model with different conditions. The additional artifact encoding
for the STFT features (top part of the difference) becomes more apparent with larger values of γ,
a coefﬁcient used to trade-off between audio quality and image ﬁdelity. (Left: γ = 0.1. Center
γ = 1.0. Right: γ = 10.0)"
RELATED WORKS,0.11678832116788321,"Bao, 2005) have been applied to the task. Deep learning based approaches such as Huu et al. (2019)
encodes STFT features of audio into the container image with a convolutional architecture. Ye et al.
(2019) incorporates GANs. Still, these methods are limited to ﬁxed dimensional inputs, prohibiting
its applications in the real world with audio signals that vary in length."
RELATED WORKS,0.12408759124087591,"Loss-Conditional Training. Generally, deep learning handles multi-objective training by linearly
combining the different sub-objective terms with their respective coefﬁcients. If the coefﬁcient val-
ues are closely tied to the model’s use cases, one must train multiple models with different coefﬁcient
combinations to cover all such cases, which is computationally expensive and impractical. Loss-
conditional training resolves this by conditioning the model on the randomly sampled coefﬁcients
during training. Babaeizadeh & Ghiasi (2018) ﬁrst adopted loss-conditional training in style transfer
to make the style and content coefﬁcients in style transfer adjustable at inference time. Dosovitskiy
& Djolonga (2019) then proposed a general framework for loss-conditional training, demonstrat-
ing its applications in autoencoder, image compression, and style transfer. Lee et al. (2020) adopts
loss-conditional training in creating a dynamic residual network with adaptable sparsity. Since the
importance of audio data reconstruction quality varies among the different use cases of steganogra-
phy (e.g., in speaker identiﬁcation, high audio quality may not be necessary as long as the voice is
identiﬁable), we adopt an optional loss-conditional training process to allow the trade-off between
these two objectives at inference time."
METHOD,0.13138686131386862,"3
METHOD"
METHOD,0.1386861313868613,"In this section, we describe the VLVQ (Variable Length and Variable Quality Audio Steganography)
framework in detail. We ﬁrst describe the problem setting, then elaborate on the speciﬁc methods.
Please refer to Algorithm 1 for an overview of the methods."
PROBLEM SETUP,0.145985401459854,"3.1
PROBLEM SETUP"
PROBLEM SETUP,0.15328467153284672,"In steganography, a system H takes a container data c with a secret data s to produce a modiﬁed
container ˆc = H(c, s) that encodes s while remaining visually similar to c. Then, a system F
takes ˆc to recover the secret data s ∼ˆs = F(ˆc). In Audio-in-Image Steganography, c is an image,
and s is an audio of variable length. Since the container’s capacity is the limiting factor in what is
capable of being hidden, we adopt γ to represent the trade-off parameter between the reconstruction
quality of the container and the secret data. Here, a larger value of γ increases the quality of the
audio reconstruction at the sacriﬁce of the container image quality, while a smaller value makes the
change on the container image less noticeable at the sacriﬁce of the audio reconstruction quality."
PROBLEM SETUP,0.16058394160583941,"The audio data goes through STFT (Short-Time Fourier Transform) during the preprocessing step
and converts into the frequency domain. This makes s, ˆs ∈RN×T ×2, with N as the number of
frequencies, T as the varying length of the audio, and 2 as the real and complex channel. c and ˆc is"
PROBLEM SETUP,0.1678832116788321,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.17518248175182483,"Algorithm 1: VLVQ Framework conditioned on γ
Result: Fully trained H and F
Algorithm parameters: Container image c, Secret audio s, γ range γmin, γmax, total iteration
Ttotal, audio chunk number range Cmin, Cmax, frequency number and image resolution h, w;
Initialize H, F with random weights;
repeat Ttotal times"
PROBLEM SETUP,0.18248175182481752,"Sample random integer k in range [Cmin, Cmax];
Let T = w × k, c1 = c;
Sample γ from log(γ) ∼U(log(γmin), log(γmax));
Sample s ∈Rh×T ×2 and c ∈Rh×w×3 from the dataset;
Crop s into si ∈Rh×w×2, i = 1 . . . k;
for i = 1 to k do"
PROBLEM SETUP,0.1897810218978102,"ci+1 = H(ci, si, γ);
end
ˆck+1 = ck+1;
for i = k + 1 to 2 do"
PROBLEM SETUP,0.19708029197080293,"ˆci−1, ˆsi−1 = F(ˆci, γ);
end
Limg = 1"
PROBLEM SETUP,0.20437956204379562,"k
Pk+1
i=2 ||ci −c1||2;
Ldec = 1"
PROBLEM SETUP,0.2116788321167883,"k
Pk
i=1 ||ˆci −ˆci+1||2;
Lstft = 1"
PROBLEM SETUP,0.21897810218978103,"k
Pk
i=1 ||si −ˆsi||2;
Ltotal = Limg + Ldec + γLstft;
Backpropagate and optimize H and F with respect to Ltotal;
end"
PROBLEM SETUP,0.22627737226277372,"an RGB image with c, ˆc ∈Rh×w×3. We set N = h = w for simplicity and only use h and w to
denote spatial dimensions, thus s, ˆs ∈Rh×T ×2."
RECURSIVE PATCH INFERENCE,0.23357664233576642,"3.2
RECURSIVE PATCH INFERENCE"
RECURSIVE PATCH INFERENCE,0.24087591240875914,"Algorithm. T is a value that changes across different inputs. However, having T = h = w would
keep the spatial resolution of both s and c identical and constant, which is an ideal condition for
inference on a deep neural network. We sample s to have T as a multiple of w during training, so
we can crop s into k = T"
RECURSIVE PATCH INFERENCE,0.24817518248175183,"w chunks, with each chunk si ∈Rh×w×2, i = 1 . . . k. To guarantee this
during inference, we pad s to s ∈Rh×w⌈T"
RECURSIVE PATCH INFERENCE,0.25547445255474455,w ⌉×2 and crop s into k = ⌈T
RECURSIVE PATCH INFERENCE,0.26277372262773724,w⌉chunks.
RECURSIVE PATCH INFERENCE,0.27007299270072993,"Encoding Process. Let ci denote the container image with i −1 audio chunks encoded inside.
c1 = c, and ci+1 = H(ci, si), which denotes that the current container and a chunk of the secret
data is fed into H to update the container, and it is recursively fed back into H with the next chunk
of the secret data until i = k + 1. Finally, ˆck+1 = ck+1, which denotes the ﬁnal container that
conceals all the audio chunks."
RECURSIVE PATCH INFERENCE,0.2773722627737226,"Decoding Process. Then, ˆci−1, ˆsi−1 = F(ˆci), which recursively feeds the container image to F to
decode each audio chunks, while updating the container image to recursively feed itself back into
F. With this process, we obtain ˆs1 . . . ˆsk that corresponds to s1 . . . sk. Figure 3 visualizes both the
encoding and the decoding process."
OPTIMIZATION OBJECTIVE,0.2846715328467153,"3.3
OPTIMIZATION OBJECTIVE"
OPTIMIZATION OBJECTIVE,0.291970802919708,"We optimize both H and F with respect to Ltotal, an objective function that linearly weighs three
sub-objective functions."
OPTIMIZATION OBJECTIVE,0.29927007299270075,"Audio Container Distance. To ensure that the decoded audio is similar to the ground truth audio
at the same chunk index, we minimize the average of their mean-squared distances Lstft. Also, to
minimize the change H makes on the container image, we minimize Limg which is the average of"
OPTIMIZATION OBJECTIVE,0.30656934306569344,Under review as a conference paper at ICLR 2022
OPTIMIZATION OBJECTIVE,0.31386861313868614,"Figure 3: The overall process of the VLVQ framework. Iterative encoding on H minimizes the
distance between original container image and the modiﬁed container image (Limg). Iterative de-
coding on F minimally updates the container image (Ldec), while making the reconstructed audio
data to be as close to the ground truth (Lstft)."
OPTIMIZATION OBJECTIVE,0.32116788321167883,the mean-squared distances between c1 and ci for i = 2 . . . k + 1.
OPTIMIZATION OBJECTIVE,0.3284671532846715,"Lstft = 1 k k
X"
OPTIMIZATION OBJECTIVE,0.3357664233576642,"i=1
||si −ˆsi||2
Limg = 1 k k+1
X"
OPTIMIZATION OBJECTIVE,0.34306569343065696,"i=2
||ci −c1||2
(1)"
OPTIMIZATION OBJECTIVE,0.35036496350364965,"Decoding Consistency. F continuously updates the container image from the previous iteration. To
prevent this process from destroying the information about the remaining audio chunks, it is essential
to prevent F from making large modiﬁcations to the container. We achieve this by minimizing the
mean-squared distance between the container images of two consecutive decoding iterations, Ldec."
OPTIMIZATION OBJECTIVE,0.35766423357664234,"Ldec = 1 k k
X"
OPTIMIZATION OBJECTIVE,0.36496350364963503,"i=1
||ˆci −ˆci+1||2
(2)"
OPTIMIZATION OBJECTIVE,0.3722627737226277,"Full Objective. The ﬁnal loss term Ltotal is a linear combination of the above three loss terms,
and the hyperparameter γ determines the trade-off between audio and image reconstruction quality.
Figure 2 demonstrates the effects of this parameter by visualizing the changes of the residuals. We
simply keep the coefﬁcient of Ldec as 1.0, as it didn’t have a signiﬁcant effect on the outcome."
OPTIMIZATION OBJECTIVE,0.3795620437956204,"Ltotal = Limg + Ldec + γLstft
(3)"
TRADE-OFF COEFFICIENT CONDITIONING,0.38686131386861317,"3.4
TRADE-OFF COEFFICIENT CONDITIONING"
TRADE-OFF COEFFICIENT CONDITIONING,0.39416058394160586,"Modifying γ in equation 3 gives control over the trade-off between audio reconstruction quality and
container image quality. However, to allow such trade-offs at inference time with varying γ terms,
one must train several VLVQ frameworks, which would be computationally expensive. To address
this, the VLVQ framework has the option to adopt a strategy of directly conditioning the models
with γ, such that H(ci, si) and F(ˆci) may become H(ci, si, γ) and F(ˆci, γ)."
TRADE-OFF COEFFICIENT CONDITIONING,0.40145985401459855,"γ Conditioning. Both H and F are composed of several convolutional blocks, with each block
following the order of convolution (Conv), normalization (Norm), and activation function (Act).
Given the input and output of a block X, Y ∈RH×W ×C, with γ ∈R1, γ > 0."
TRADE-OFF COEFFICIENT CONDITIONING,0.40875912408759124,"Y = Act(Norm(Conv(X)))
(4)"
TRADE-OFF COEFFICIENT CONDITIONING,0.41605839416058393,Under review as a conference paper at ICLR 2022
TRADE-OFF COEFFICIENT CONDITIONING,0.4233576642335766,"To condition γ, we adopt the FiLM conditioning layer of Perez et al. (2018), which does a lin-
ear transformation on the convolutional feature with scale and shift parameters generated from the
conditional variable."
TRADE-OFF COEFFICIENT CONDITIONING,0.4306569343065693,"Figure 4: Conditioning module of the VLVQ framework. The architecture is conditioned on γ via
the FiLM layer (Perez et al., 2018), and the loss term is conditioned on γ via linear combination."
TRADE-OFF COEFFICIENT CONDITIONING,0.43795620437956206,"α = f(γ)
β = h(γ)
H = Norm(Conv(X))
Y = Act(αH + β)
(5)"
TRADE-OFF COEFFICIENT CONDITIONING,0.44525547445255476,"Here, α ∈RC, β ∈RC, H, Y ∈RH×W ×C. f and h are two-layer fully-connected networks with
ReLU activation. Figure 4 illustrates this conditioning module."
TRADE-OFF COEFFICIENT CONDITIONING,0.45255474452554745,"Sampling Distribution of Gamma Since γ is no longer a hyperparameter, we sample γ from a
ﬁxed distribution each training iteration. We apply a commonly used hyperparameter distribution of
log-uniform distribution (Bergstra & Bengio, 2012) and ﬁnd γmin = 0.01 and γmax = 100.0 to be
a reasonable range."
TRADE-OFF COEFFICIENT CONDITIONING,0.45985401459854014,"log(γ) ∼U(log(γmin), log(γmax))
(6)"
EXPERIMENTS,0.46715328467153283,"4
EXPERIMENTS"
EXPERIMENTS,0.4744525547445255,"Section 4.1 describe the implementation details of the VLVQ framework, and Section 4.2 describes
the datasets and metrics used in our experiments. Section 4.3 demonstrates the effects of modifying
the γ parameter to both conditional and non-conditional versions of the framework, and Section 4.4
tests VLVQ’s capability to extrapolate and encode variable length signals."
IMPLEMENTATION DETAILS,0.48175182481751827,"4.1
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.48905109489051096,"Architecture. We adopt a U-Net (Ronneberger et al., 2015) - like architecture for both H and F.
When conditioning the network on γ, we insert a FiLM layer (Perez et al., 2018) in all convolutional
and transposed convolutional blocks. H receives a 5-channel input by concatenating ci and si, and
F produces a 5-channel output which is split in the channel dimension into ˆci and ˆsi. Please refer
to the supplementary materials for more details."
IMPLEMENTATION DETAILS,0.49635036496350365,"Hyperparameters. The network is trained end-to-end via Adam (Kingma & Ba, 2014) with the
initial learning rate of 0.001, batch size of 4, and betas of 0.9 and 0.999 for two epochs. The
learning rate decays by a factor of 10 halfway through the training. The framework is implemented
on the PyTorch (Paszke et al., 2019) framework, and all models are trained on a single Nvidia Tesla
V100 GPU."
IMPLEMENTATION DETAILS,0.5036496350364964,Under review as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.5109489051094891,"1
2
3
4
5
6
7
total number of images encoded 32 34 36 38 40 psnr"
IMPLEMENTATION DETAILS,0.5182481751824818,"psnr
snrseg 20 22 24 26 28"
IMPLEMENTATION DETAILS,0.5255474452554745,snrseg
IMPLEMENTATION DETAILS,0.5328467153284672,"(a) Evaluating a non-condition model trained with
γ = 1.0."
IMPLEMENTATION DETAILS,0.5401459854014599,"1
2
3
4
5
6
7
total number of images encoded 24 26 28 30 32 34 psnr"
IMPLEMENTATION DETAILS,0.5474452554744526,"psnr
snrseg 18 20 22 24 26"
IMPLEMENTATION DETAILS,0.5547445255474452,snrseg
IMPLEMENTATION DETAILS,0.5620437956204379,"(b) Evaluating a condition model trained with γ
from log(γ) ∼U(log(0.01), log(100.0)) and
conditioned on γ = 1.0 during inference."
IMPLEMENTATION DETAILS,0.5693430656934306,"Figure 5: Evaluation of encoding variable number of chunks inside the VLVQ framework. Lighter
color indicates the encoding of later audio chunks. Despite being trained on maximum three chunks,
the model can be reasonably extrapolated to encode longer sequences."
EXPERIMENT SETUP,0.5766423357664233,"4.2
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.583941605839416,"Dataset. For the images, we use the ILSVRC 2012 subset of the ImageNet (Russakovsky et al.,
2015) dataset, containing 1.28 million natural images for training and 50k for validation. Images
are preprocessed with a resize and a center crop of 224x224. For the audio, we use the AudioSet
(Gemmeke et al., 2017) dataset, containing 1.7M audio clips that are each 10 seconds long. We
convert all the samples into the frequency domain image with a height of 224 via STFT. After
concatenating all the samples, one to three 224x224 random crops are taken from them every training
iteration, with each crop representing an audio chunk that accounts for approximately 6 seconds of
audio."
EXPERIMENT SETUP,0.5912408759124088,"Evaluation metrics. For the container image quality, we measure PSNR (Peak Signal-to-Noise
Ratio) between the original container image and the current container image (with the audio data
embedded inside). For the audio reconstruction, we measure SegSNR (Segmental Signal-to-Noise
Ratio) between the original audio and the reconstructed audio."
EFFECTS OF MODIFYING GAMMA,0.5985401459854015,"4.3
EFFECTS OF MODIFYING GAMMA"
EFFECTS OF MODIFYING GAMMA,0.6058394160583942,"10−2
10−1
100
101
102
gamma 15 20 25 30 35 40 psnr 21 22 23 24 25 26 27"
EFFECTS OF MODIFYING GAMMA,0.6131386861313869,snrseg
EFFECTS OF MODIFYING GAMMA,0.6204379562043796,image (psnr)
EFFECTS OF MODIFYING GAMMA,0.6277372262773723,audio (snrseg)
EFFECTS OF MODIFYING GAMMA,0.635036496350365,"Figure 6: Conditional model (curve).
Non-
conditional models (point)."
EFFECTS OF MODIFYING GAMMA,0.6423357664233577,"Verifying Trade-offs. Increasing the γ parameter
on VLVQ should increase the audio quality at the
sacriﬁce of the container image quality and vice
versa. Such trend is shown for both conditional and
non-conditional models in Figure 6, demonstrating
that modiﬁcations in γ yield meaningful trade-offs
between different objectives. An ideal conditional
VLVQ framework should have near-identical re-
sults compared with the non-conditional VLVQ
framework individually trained on their respective
γ parameters. However, a slight degradation is ex-
pected as the consequence of having a difﬁcult op-
timization space that covers many values of γ, forc-
ing the model to learn a distribution of solutions.
Again, such trend is observed between the condi-
tional and non-conditional model."
EFFECTS OF MODIFYING GAMMA,0.6496350364963503,Under review as a conference paper at ICLR 2022
EFFECTS OF MODIFYING GAMMA,0.656934306569343,"Method
γ
psnr for different number of audio chunks
1
2
3
4
5
6
7
non-cond
10.0
35.47
31.77
29.67
28.26
27.02
26.22
25.49
1.0
40.42
36.67
34.85
33.47
32.30
31.46
30.63
0.1
44.08
40.38
38.03
36.32
35.07
34.02
33.07
cond
10.0
31.40
27.85
25.75
24.37
23.04
22.64
21.78
1.0
34.75
31.03
28.99
27.40
26.26
25.56
24.46
0.1
36.06
32.13
30.29
28.48
26.99
26.29
25.02"
EFFECTS OF MODIFYING GAMMA,0.6642335766423357,"Method
γ
snrseg for different number of audio chunks
1
2
3
4
5
6
7
non-cond
10.0
30.70
28.32
27.02
26.32
25.44
24.62
24.50
1.0
27.85
25.92
25.42
24.18
23.26
22.76
22.07
0.1
25.59
24.43
24.21
22.92
22.09
22.12
21.73
cond
10.0
25.11
27.73
25.09
24.67
23.49
23.50
22.94
1.0
24.79
25.00
23.79
23.87
22.93
21.84
21.16
0.1
25.12
22.16
22.82
20.65
19.84
19.63
18.49"
EFFECTS OF MODIFYING GAMMA,0.6715328467153284,"Table 1: Effects of encoding different audio lengths (1 to 7 chunks) inside the VLVQ frameworks.
The snrseg values are averaged across the whole audio reconstruction. The effects of the trade-off
becomes apparent as γ decreases, with the increase in image quality and decrease in audio quality."
NUMBER OF ENCODED AUDIO CHUNKS,0.6788321167883211,"4.4
NUMBER OF ENCODED AUDIO CHUNKS"
NUMBER OF ENCODED AUDIO CHUNKS,0.6861313868613139,"We test whether the VLVQ framework stands to the claim of being capable of encoding audio sig-
nals with varying length. To evaluate this, we test beyond the expected number of chunks during
training (Cmin = 1, Cmax = 3) by evaluating up to 7 audio chunks. For both non-conditional
and conditional framework, Figure 5a, 5b and Table 1 each shows that while both image and audio
reconstruction quality drops with longer encoded sequences, the drop continues to maintain a grad-
ual decline from 1 to 7 instead of a rapid drop at 4. This trend demonstrates VLVQ’s reasonable
extrapolation capability."
DISCUSSION,0.6934306569343066,"5
DISCUSSION"
DISCUSSION,0.7007299270072993,"Defenses Against Steganalysis. While we make no claims regarding VLVQ’s defensibility against
steganalysis, future work may evaluate and equip VLVQ with defenses (Lyu & Farid, 2002; Fridrich,
2004; Fridrich & Kodovsky, 2012; Kodovsky et al., 2011; Qian et al., 2015) to prevent algorithmic
detections."
DISCUSSION,0.708029197080292,"Failure Modes. When dealing with multiple audio chunks, F sometimes fails to separate the sig-
nals between different chunks, resulting in the leakage of audio chunks into different timesteps. This
usually happens when different audio chunks greatly vary in volume, as the louder chunks dominate
the signals in the container image. Future work may employ an adversarial framework by intro-
ducing an adversarial framework to discourage F from producing signals that can be classiﬁed as
originating from a different chunk."
DISCUSSION,0.7153284671532847,"Conditioning Methods While we used FiLM on all convolutional blocks due to its simple architec-
ture choice, one may search for a better method of conditioning γ, such as searching for the optimal
layers to place the conditioning module."
DISCUSSION,0.7226277372262774,"Limitations. Although a simple U-Net (Ronneberger et al., 2015) was sufﬁcient to yield satisfac-
tory results, there are clear limitations to this architecture. For example, a convolutional layer’s
inductive bias forces the network to rely mostly on local information. Figure 2 demonstrates that
most residuals are visible at the top of the image, where the most prominent STFT features lie. To
exploit the full capacity of the container image, architectures with minimal inductive biases such as
transformers (Vaswani et al., 2017) may be employed."
DISCUSSION,0.7299270072992701,Under review as a conference paper at ICLR 2022
CONCLUSION,0.7372262773722628,"6
CONCLUSION"
CONCLUSION,0.7445255474452555,"In this work we present VLVQ, a steganographic framework that encodes variable length audio data
into images with varying quality trade-offs. Compared to other frameworks of this kind, VLVQ
enables a greater degree of freedom in terms of secret audio length and infernce-time quality trade-
offs. We achieve this through recursive inference of multiple audio chunks and conditioning the
model with the trade-off parameter via FiLM layers. Experiments on ImageNet and Audioset verify
these claims and demonstrate extrapolation capabilities beyond audio length seen during training.
We hope these efforts make audio-in-image steganography feasible in diverse real-world scenarios."
CONCLUSION,0.7518248175182481,"A
ETHICS STATEMENT"
CONCLUSION,0.7591240875912408,"With growing concerns over various societal issues regarding individual privacy, tools such as
steganography can empower individuals to have greater control over their information. For exam-
ple, activists in nations where encryption is criminalized may apply steganography in a non-secret
medium to hide their messages. In this aspect, our work helps widen the available pool of stegano-
graphic medium an individual can hide their information on. But this raises concern to a potential
risk based on the malicious uses of steganography, such as its applications as digital ﬁngerprints in
intelligence services. Hence, researches on steganography and its applications must be developed
with these positive and negative effects in consideration."
REFERENCES,0.7664233576642335,REFERENCES
REFERENCES,0.7737226277372263,"Mohammad Babaeizadeh and Golnaz Ghiasi. Adjustable real-time style transfer. arXiv preprint
arXiv:1811.08560, 2018."
REFERENCES,0.781021897810219,"Shumeet Baluja. Hiding images in plain sight: Deep steganography. In Proceedings of the 31st
International Conference on Neural Information Processing Systems, pp. 2066–2076, 2017."
REFERENCES,0.7883211678832117,"James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
machine learning research, 13(2), 2012."
REFERENCES,0.7956204379562044,"Wenxue Cui, Shaohui Liu, Feng Jiang, Yongliang Liu, and Debin Zhao. Multi-stage residual hiding
for image-into-audio steganography. In ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 2832–2836. IEEE, 2020."
REFERENCES,0.8029197080291971,"Alexey Dosovitskiy and Josip Djolonga. You only train once: Loss-conditional training of deep
networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.8102189781021898,"Xintao Duan, Kai Jia, Baoxia Li, Daidou Guo, En Zhang, and Chuan Qin.
Reversible image
steganography scheme based on a u-net structure. IEEE Access, 7:9314–9323, 2019."
REFERENCES,0.8175182481751825,"Jessica Fridrich. Feature-based steganalysis for jpeg images and its implications for future design of
steganographic schemes. In International Workshop on Information Hiding, pp. 67–81. Springer,
2004."
REFERENCES,0.8248175182481752,"Jessica Fridrich and Jan Kodovsky. Rich models for steganalysis of digital images. IEEE Transac-
tions on Information Forensics and Security, 7(3):868–882, 2012."
REFERENCES,0.8321167883211679,"Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing
Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for
audio events. In Proc. IEEE ICASSP 2017, New Orleans, LA, 2017."
REFERENCES,0.8394160583941606,"Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014."
REFERENCES,0.8467153284671532,"Quang Pham Huu, Thoi Hoang Dinh, Ngoc N Tran, Toan Pham Van, and Thanh Ta Minh. Deep
neural networks based invisible steganography for audio-into-image algorithm. In 2019 IEEE 8th
Global Conference on Consumer Electronics (GCCE), pp. 423–427. IEEE, 2019."
REFERENCES,0.8540145985401459,Under review as a conference paper at ICLR 2022
REFERENCES,0.8613138686131386,"Nitin Kaul and Nikesh Bajaj. Audio in image steganography based on wavelet transform. Interna-
tional Journal of Computer Applications, 79(3), 2013."
REFERENCES,0.8686131386861314,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.8759124087591241,"Jan Kodovsky, Jessica Fridrich, and Vojtˇech Holub. Ensemble classiﬁers for steganalysis of digital
media. IEEE Transactions on Information Forensics and Security, 7(2):432–444, 2011."
REFERENCES,0.8832116788321168,"Sangho Lee, Simyung Chang, and Nojun Kwak.
Urnet: User-resizable residual networks with
conditional gating module. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 34, pp. 4569–4576, 2020."
REFERENCES,0.8905109489051095,"Siwei Lyu and Hany Farid. Detecting hidden messages using higher-order statistics and support
vector machines. In International Workshop on information hiding, pp. 340–354. Springer, 2002."
REFERENCES,0.8978102189781022,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026–8037, 2019."
REFERENCES,0.9051094890510949,"Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual
reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 32, 2018."
REFERENCES,0.9124087591240876,"Yinlong Qian, Jing Dong, Wei Wang, and Tieniu Tan. Deep learning for steganalysis via convolu-
tional neural networks. In Media Watermarking, Security, and Forensics 2015, volume 9409, pp.
94090J. International Society for Optics and Photonics, 2015."
REFERENCES,0.9197080291970803,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.927007299270073,"Daniel L Ruderman. The statistics of natural images. Network: computation in neural systems, 5
(4):517–548, 1994."
REFERENCES,0.9343065693430657,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y."
REFERENCES,0.9416058394160584,"Rully Adrian Santosa and Paul Bao. Audio-to-image wavelet transform based audio steganography.
In 47th International Symposium ELMAR, 2005., pp. 209–212. IEEE, 2005."
REFERENCES,0.948905109489051,"Abdelfatah A Tamimi, Ayman M Abdalla, and Omaima Al-Allaf. Hiding an image inside another
image using variable-rate steganography. International Journal of Advanced Computer Science
and Applications (IJACSA), 4(10), 2013."
REFERENCES,0.9562043795620438,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.9635036496350365,"Pin Wu, Yang Yang, and Xiaoqiang Li. Image-into-image steganography using deep convolutional
network. In Paciﬁc Rim Conference on Multimedia, pp. 792–802. Springer, 2018."
REFERENCES,0.9708029197080292,"Dengpan Ye, Shunzhi Jiang, and Jiaqin Huang. Heard more than heard: An audio steganography
method based on gan. arXiv preprint arXiv:1907.04986, 2019."
REFERENCES,0.9781021897810219,"Kevin Alex Zhang, Alfredo Cuesta-Infante, Lei Xu, and Kalyan Veeramachaneni. Steganogan: High
capacity image steganography with gans. arXiv preprint arXiv:1901.03892, 2019a."
REFERENCES,0.9854014598540146,"Ru Zhang, Shiqi Dong, and Jianyi Liu. Invisible steganography via generative adversarial networks.
Multimedia tools and applications, 78(7):8559–8575, 2019b."
REFERENCES,0.9927007299270073,"Jiren Zhu, Russell Kaplan, Justin Johnson, and Li Fei-Fei. Hidden: Hiding data with deep networks.
In Proceedings of the European conference on computer vision (ECCV), pp. 657–672, 2018."
