Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010570824524312897,"Neural data compression based on nonlinear transform coding has made great
progress over the last few years, mainly due to improvements in prior models,
quantization methods and nonlinear transforms.
A general trend in many re-
cent works pushing the limit of rate-distortion performance is to use ever more
expensive prior models that can lead to prohibitively slow decoding. Instead,
we focus on more expressive transforms that result in a better rate-distortion-
computation trade-off. Speciﬁcally, we show that nonlinear transforms built on
Swin-transformers can achieve better compression efﬁciency than transforms built
on convolutional neural networks (ConvNets), while requiring fewer parameters
and shorter decoding time. Paired with a compute-efﬁcient Channel-wise Auto-
Regressive Model prior, our SwinT-ChARM model outperforms VTM-12.1 by
3.68% in BD-rate on Kodak with comparable decoding speed. In P-frame video
compression setting, we are able to outperform the popular ConvNet-based scale-
space-ﬂow model by 12.35% in BD-rate on UVG. We provide model scaling stud-
ies to verify the computational efﬁciency of the proposed solutions and conduct
several analyses to reveal the source of coding gain of transformers over Conv-
Nets, including better spatial decorrelation, ﬂexible effective receptive ﬁeld, and
more localized response of latent pixels during progressive decoding."
INTRODUCTION,0.0021141649048625794,"1
INTRODUCTION"
INTRODUCTION,0.003171247357293869,"Transform coding (Goyal, 2001) is the dominant paradigm for compression of multi-media signals,
and serves as the technical foundation for many successful coding standards such as JPEG, AAC,
and HEVC/VVC. Codecs based on transform coding divide the task of lossy compression into three
modularized components: transform, quantization, and entropy coding. All three components can
be enhanced by deep neural networks: autoencoder networks are adopted as ﬂexible nonlinear trans-
forms, deep generative models are used as powerful learnable entropy models, and various differen-
tiable quantization schemes are proposed to aid end-to-end training. Thanks to these advancements,
we have seen rapid progress in the domain of image and video compression. Particularly, the hyper-
prior line of work (Ball´e et al., 2018; Minnen et al., 2018; Lee et al., 2019; Agustsson et al., 2020;
Minnen & Singh, 2020) has led to steady progress of neural compression performance over the past
two years, reaching or even surpassing state-of-the-art traditional codecs. For example, in image
compression, BPG444 was surpassed by a neural codec in 2018 (Minnen et al., 2018), and (Cheng
et al., 2020; Xie et al., 2021; Ma et al., 2021; Guo et al., 2021; Wu et al., 2020) have claimed on-par
or better performance than VTM (a test model of the state-of-the-art non-learned VVC standard)."
INTRODUCTION,0.004228329809725159,"One general trend in the advancement of neural image compression schemes is to develop ever
more expressive yet expensive prior models based on spatial context. However, the rate-distortion
improvement from context based prior modeling often comes with a hefty price tag1 in terms of
decoding complexity. Noteably, all existing works that claimed on-par or better performance than
VTM (Cheng et al., 2020; Xie et al., 2021; Ma et al., 2021; Guo et al., 2021; Wu et al., 2020) rely
on slow and expensive spatial context based prior models."
INTRODUCTION,0.005285412262156448,"∗Equal contribution.
†Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.
1In the extreme case when a latent-pixel-level spatial autoregressive prior is used, decoding of a single
512x768 image requires no less than 1536 interleaved executions of prior model inference and entropy decoding
(assuming the latent is downsampled by a factor of 16x16)."
INTRODUCTION,0.006342494714587738,Published as a conference paper at ICLR 2022
INTRODUCTION,0.007399577167019027,"The development of nonlinear transforms, on the other hand, are largely overlooked. This leads us
to the following questions: can we achieve the same performance as that of expensive prior models
by designing a more expressive transform together with simple prior models? And if so, how much
more complexity in the transform is required? 100 150 200 250 300"
INTRODUCTION,0.008456659619450317,Decoding time (ms) 5 0 5 10 15 20 25
INTRODUCTION,0.009513742071881607,BD-rate over VTM-12.1 (%) on Kodak
INTRODUCTION,0.010570824524312896,SwinT-ChARM
INTRODUCTION,0.011627906976744186,Conv-ChARM
INTRODUCTION,0.012684989429175475,VTM-12.1
INTRODUCTION,0.013742071881606765,"SwinT-Hyperprior
(different sizes)
Conv-Hyperprior
(different sizes) 103 104"
INTRODUCTION,0.014799154334038054,Lee et al. 2019
INTRODUCTION,0.015856236786469344,Guo et al. 2021
INTRODUCTION,0.016913319238900635,Cheng et al. 2020
INTRODUCTION,0.017970401691331923,Wu et al. 2020
INTRODUCTION,0.019027484143763214,"Figure 1: BD-rate (smaller is better) vs decod-
ing time.
Our Swin transformer based image
compression models land in a favorable region
of rate-distortion-computation trade-off that has
never been achieved before. See Section 4.2 for
more results and evaluation setup."
INTRODUCTION,0.0200845665961945,"Interestingly, we show that by leveraging and
adapting the recent development of vision
transformers, not only can we build neural
codecs with simple prior models that can out-
perform ones built on expensive spatial auto-
regressive priors, but do so with smaller trans-
form complexity compared to its convolutional
counterparts, attaining a strictly better rate-
distortion-complexity trade-off. As can be seen
in Figure 1, our proposed neural image codec
SwinT-ChARM can outperform VTM-12.1 at
comparable decoding time, which, to the best
of our knowledge, is a ﬁrst in the neural com-
pression literature."
INTRODUCTION,0.021141649048625793,"As main contributions, we 1) extend Swin-
Transformer (Liu et al., 2021) to a decoder set-
ting and build Swin-transformer based neural
image codecs that attain better rate-distortion
performance with lower complexity compared
with existing solutions, 2) verify its effective-
ness in video compression by enhancing scale-
space-ﬂow, a popular neural P-frame codec,
and 3) conduct extensive analysis and ablation study to explore differences between convolution
and transformers, and investigate potential source of coding gain."
BACKGROUND & RELATED WORK,0.022198731501057084,"2
BACKGROUND & RELATED WORK"
BACKGROUND & RELATED WORK,0.023255813953488372,"Conv-Hyperprior
The seminal hyperprior architecture (Ball´e et al., 2018; Minnen et al., 2018) is
a two-level hierarchical variational autoencoder, consisting of a pair of encoder/decoder ga, gs, and a
pair of hyper-encoder/hyper-decoder ha, hs. Given an input image x, a pair of latent y = ga(x) and
hyper-latent z = ha(y) is computed. The quantized hyper-latent ˆz = Q(z) is modeled and entropy-
coded with a learned factorized prior. The latent y is modeled with a factorized Gaussian distribution
p(y|ˆz) = N(µ, diag(σ)) whose parameter is given by the hyper-decoder (µ, σ) = hs(ˆz). The
quantized version of the latent ˆy = Q(y−µ)+µ is then entropy coded and passed through decoder
gs to derive reconstructed image ˆx = gs(ˆy). The tranforms ga, gs, ha, hs are all parameterized as
ConvNets (for details, see Appendix A.1)."
BACKGROUND & RELATED WORK,0.024312896405919663,"Conv-ChARM
(Minnen & Singh, 2020) extends the baseline hyperprior architecture with a
channel-wise auto-regressive model (ChARM)2, in which latent y is split along channel dimension
into S groups (denoted as y1, . . . , yS), and the Gaussian prior p(ys|ˆz, ˆy<s) is made autoregressive
across groups where the mean/scale of ys depends on quantized latent in the previous groups ˆy<s.
In practice, S = 10 provides a good balance of performance and complexity and is adopted here."
BACKGROUND & RELATED WORK,0.02536997885835095,"Spatial AR models
Most of recent performance advancements of neural image compression is
driven by the use of spatial auto-regressive/context models. Variants include causal global predic-
tion (Guo et al., 2021), 3D context (Ma et al., 2021), block-level context (Wu et al., 2020), nonlocal
context (Li et al., 2020; Qian et al., 2021). One common issue with these designs is that decoding
cannot be parallelized along spatial dimensions, leading to impractical3decoding latency, especially
for large resolution images."
BACKGROUND & RELATED WORK,0.026427061310782242,"2For details refer to Figure 11 and 12 in the Appendix.
3It is reported in (Wu et al., 2020)(Table I) that decoding time of spatial autoregressive models on a 512x768
image range from 2.6s to more than half a minute, depending on the speciﬁc designs. Also see Figure 1."
BACKGROUND & RELATED WORK,0.02748414376321353,Published as a conference paper at ICLR 2022
BACKGROUND & RELATED WORK,0.02854122621564482,"ConvNet-based transforms
While the design space of prior models is extensively explored, non-
linear transforms, as an important component, have received less attention. A standard convolution
encoder-decoder with GDN (Ball´e et al., 2016; 2017) as activation is widely adopted in the literature.
Later works introduce new transform designs, such as residual blocks with smaller kernels (Cheng
et al., 2020), nonlocal (sigmoid gating) layers (Zhou et al., 2019; Chen et al., 2021), invertible neural
networks (Xie et al., 2021), and PReLU as an efﬁcient replacement of GDN (Egilmez et al., 2021)."
BACKGROUND & RELATED WORK,0.02959830866807611,"Vision transformers
Although many transform networks are proposed, they are still mainly based
on ConvNets. Recently transformers (Vaswani et al., 2017) have been introduced to the vision do-
main and have shown performance competitive with ConvNets in many tasks, e.g. object detec-
tion (Carion et al., 2020), classiﬁcation (Dosovitskiy et al., 2021), image enhancement (Chen et al.,
2020), and semantic segmentation (Zheng et al., 2021). Inspired by their success, in this work we
explore how vision transformers work as nonlinear transforms for image and video compression."
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.0306553911205074,"3
SWIN-TRANSFORMER BASED TRANSFORM CODING"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.03171247357293869,"Among the large number of vision transformer variants, we choose Swin Transformer (Liu et al.,
2021) (hereafter referred to as SwinT) to build the nonlinear transforms, mainly because of 1) its lin-
ear complexity w.r.t. input resolution due to local window attention, and 2) its ﬂexibility in handling
varying input resolutions at test time, enabled by relative position bias and hierarchical architecture."
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.03276955602536998,Input Image
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.03382663847780127,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.03488372093023256,Patch Merge
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.035940803382663845,Patch Merge
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.03699788583509514,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.03805496828752643,Patch Merge
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.039112050739957716,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.040169133192389,Patch Merge
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.0412262156448203,"Swin Transformer
Block Q"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.042283298097251586,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.04334038054968287,Patch Merge
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.04439746300211417,Patch Merge
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.045454545454545456,"Swin Transformer
Block Q"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.046511627906976744,Reconstruction
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.04756871035940803,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.048625792811839326,Patch Split
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.049682875264270614,Patch Split
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.0507399577167019,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.05179704016913319,Patch Split
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.052854122621564484,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.05391120507399577,Patch Split
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.05496828752642706,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.056025369978858354,Patch Split
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.05708245243128964,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.05813953488372093,Patch Split
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.05919661733615222,"Swin Transformer
Block"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.06025369978858351,"Channel-wise
AutoRegressive
Model AE AD"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.0613107822410148,"Bits
Bits"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.06236786469344609,"Factorized
 Model AE AD _ +"
SWIN-TRANSFORMER BASED TRANSFORM CODING,0.06342494714587738,"Figure 2: Network architecture of our proposed SwinT-ChARM model4. In the SwinT-Hyperprior
model, the ChARM component is removed and instead µ and σ are directly output by the hyper-
decoder hs (For details see Figure 13 in Appendix A.1)."
SWINT ENCODER AND DECODER,0.06448202959830866,"3.1
SWINT ENCODER AND DECODER"
SWINT ENCODER AND DECODER,0.06553911205073996,"The original SwinT is proposed as a vision backbone, i.e. an encoder transform with downsampling.
As shown in Figure 2, the SwinT encoder ga contains SwinT blocks interleaved with Patch Merge
blocks. The Patch Merge block contains Space-to-Depth (for downsampling), LayerNorm, and
Linear layers sequentially. SwinT block performs local self-attention within each non-overlapping
window of the feature maps and preserves feature size. Consecutive SwinT blocks at the same
feature size shift the window partitioning with respect to the previous block to promote information
propagation across nearby windows in the previous block."
SWINT ENCODER AND DECODER,0.06659619450317125,"We adopt SwinT encoder as the encoder transform ga in our model, and extend it to SwinT decoder
gs by reversing the order of blocks in ga, and replacing the Patch Merge block with a Patch Split
block, which contains Linear, LayerNorm, Depth-to-Space (for upsampling) layers in sequence. The
architectures for hyper transforms ha, hs are similar to ga, gs with different conﬁgurations."
SWINT ENCODER AND DECODER,0.06765327695560254,"4The ChARM architecture (Minnen & Singh, 2020) is detailed in Figure 12 of Appendix A.1."
SWINT ENCODER AND DECODER,0.06871035940803383,Published as a conference paper at ICLR 2022
SWINT ENCODER AND DECODER,0.06976744186046512,"With these four SwinT transforms, we propose two image compression models, SwinT-Hyperprior
and SwinT-ChARM, whose prior and hyper prior models are respectively the same as in Conv-
Hyperprior and Conv-ChARM introduced in Section 2. The full model architectures are shown in
Figure 2 and Figure 13."
EXTENSION TO P-FRAME COMPRESSION,0.0708245243128964,"3.2
EXTENSION TO P-FRAME COMPRESSION"
EXTENSION TO P-FRAME COMPRESSION,0.07188160676532769,"To investigate the effectiveness of SwinT transforms for video compression, we study one popular
P-frame compression model called Scale-Space Flow (SSF) (Agustsson et al., 2020). There are three
instances of Conv-Hyperprior in SSF, which are respectively for compressing I-frames, scale-space
ﬂow and residual. We propose a SwinT variant, referred to as SwinT-SSF, which is obtained by
replacing Conv transforms ga, gs in the ﬂow codec and residual codec of SSF with SwinT tranforms.
To stabilize training of ﬂow codec in SwinT-SSF, we need to remove all LayerNorm layers and
reduce the window size (e.g. from 8 to 4). The baseline SSF model will be referred as Conv-SSF.
Even though we build our solution on top of SSF, we believe this general extension can be applied
to other ConvNet-based video compression models (Rippel et al., 2021; Hu et al., 2021) as well."
EXPERIMENTS AND ANALYSIS,0.07293868921775898,"4
EXPERIMENTS AND ANALYSIS"
EXPERIMENT SETUP,0.07399577167019028,"4.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.07505285412262157,"Training
All image compression models are trained on the CLIC2020 training set.
Conv-
Hyperprior and SwinT-Hyperprior are trained with 2M batches. Conv-ChARM and SwinT-ChARM
are trained with 3.5M and 3.1M steps. Each batch contains 8 random 256 × 256 crops from training
images. Training loss L = D + βR is a weighted combination of distortion D and bitrate R, with
β being the Lagrangian multiplier steering rate-distortion trade-off. Distortion D is MSE in RGB
color space. To cover a wide range of rate and distortion, for each solution, we train 5 models with
β ∈{0.003, 0.001, 0.0003, 0.0001, 0.00003}. The detailed training schedule is in Appendix B.1."
EXPERIMENT SETUP,0.07610993657505286,"For P-frame compression models, we follow the training setup of SSF. Both Conv-SSF and SwinT-
SSF are trained on Vimeo-90k Dataset (Xue et al., 2019) for 1M steps with learning rate 10−4, batch
size of 8, crop size of 256×256, followed by 50K steps of training with learning rate 10−5 and crop
size 384 × 256. The models are trained with 8 β values 2γ × 10−4 : γ ∈{0, 1, ..., 7}. We adopt one
critical trick to stablize the training from (Jaegle et al., 2021; Meister et al., 2018), i.e. to forward
each video sequence twice during one optimization step (mini-batch), once in the original frame
order, once in the reversed frame order. Finally we add ﬂow loss5 only between 0 and 200K steps,
which we found not critical for stable training but improves the RD."
EXPERIMENT SETUP,0.07716701902748414,"Evaluation
We evaluate image compression models on 4 datasets:
Kodak (Kodak, 1999),
CLIC2021 testset (CLIC, 2021), Tecnick testset (Asuni & Giachetti, 2014), and JPEG-AI test-
set (JPEG-AI, 2020). We use BPG and VTM-12.1 to code the images in YUV444 mode, and then
calculate PSNR in RGB. For a fair comparison all images are cropped to multiples of 256 to avoid
padding for neural codecs."
EXPERIMENT SETUP,0.07822410147991543,"We evaluated P-frame models on UVG (Mercat et al., 2020) 6 and MCL-JCV (Wang et al., 2016),
and compare them with the test model implementation of HEVC, referred to as HEVC (HM), and
open source library implementation of HEVC, refered to as HEVC (x265). To align conﬁguration,
all video codecs are evaluated in low-delay-P model with a ﬁxed GOP size of 12."
EXPERIMENT SETUP,0.07928118393234672,"Besides rate-distortion curves, we also evaluate different models using BD-rate (Tan et al., 2016),
which represents the average bitrate savings for the same reconstruction quality. For image codecs,
BD-rate is computed for each image and then averaged across all images; for video codecs, BD-rate
is computed for each video and then averaged across all videos. More details on testset preprocess-
ing, and traditional codecs conﬁgurations can be found in Appendix B.2."
EXPERIMENT SETUP,0.080338266384778,"5We did not observe RD improvement when applying ﬂow loss to Conv-SSF training.
6We use the original 7 UVG sequences that are commonly used in other works (Agustsson et al., 2020)."
EXPERIMENT SETUP,0.08139534883720931,Published as a conference paper at ICLR 2022
EXPERIMENT SETUP,0.0824524312896406,"0.5
1.0
1.5
2.0
2.5
Rate (bits per pixel) 32 34 36 38 40 42 44"
EXPERIMENT SETUP,0.08350951374207188,PSNR (RGB) Kodak
EXPERIMENT SETUP,0.08456659619450317,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM
Conv-Hyperprior
VTM-12.1
BPG444"
EXPERIMENT SETUP,0.08562367864693446,(a) Rate distortion comparison on Kodak.
EXPERIMENT SETUP,0.08668076109936575,"32
34
36
38
40
42
44
PSNR (RGB) 30 25 20 15 10 5 0 5"
EXPERIMENT SETUP,0.08773784355179703,Rate saving (%) over VTM-12.1
EXPERIMENT SETUP,0.08879492600422834,(larger is better) Kodak
EXPERIMENT SETUP,0.08985200845665962,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM"
EXPERIMENT SETUP,0.09090909090909091,"Conv-Hyperprior
VTM-12.1
BPG444"
EXPERIMENT SETUP,0.0919661733615222,(b) Rate saving over VTM-12.1 (larger is better)
EXPERIMENT SETUP,0.09302325581395349,"Figure 3: Comparison of compression efﬁciency on Kodak8. Note that the encoding time of VTM-
12.1 is much longer than all neural codecs, as shown in Table 4 and Table 7 in the Appendix."
RESULTS,0.09408033826638477,"4.2
RESULTS"
RESULTS,0.09513742071881606,"RD and BD-rate for image codecs
The RD curves for all compared image codecs evaluated on
Kodak are shown in Figure 3a, and the relative rate reduction of each codec compared to VTM-12.1
at a range of PSNR levels is shown in Figure 3b 7."
RESULTS,0.09619450317124736,"As can be seen from Figure 3, SwinT transform consistently outperforms its convolutional counter-
part; the RD-performance of SwinT-Hyperprior is on-par with Conv-ChARM, despite the simpler
prior; SwinT-ChARM outperforms VTM-12.1 across a wide PSNR range. In the Appendix (Fig-
ure 28 and Figure 30), we further incorporate the results from existing literature known to us for
a complete comparison. Particularly, our Conv-Hyperprior is much better than the results reported
in (Minnen et al., 2018) (no context), and Conv-ChARM is on par with (Minnen & Singh, 2020)."
RESULTS,0.09725158562367865,"Image Codec
Kodak
CLIC2021
Tecnick
JPEG-AI"
RESULTS,0.09830866807610994,"BPG444
20.87%
28.45%
27.74%
27.14%"
RESULTS,0.09936575052854123,"Conv-Hyperprior
11.65%
12.23%
12.49%
20.98%
Conv-ChARM
3.44%
4.14%
3.50%
9.59%
SwinT-Hyperprior
1.69%
0.83%
-0.15%
6.86%
SwinT-ChARM
-3.68%
-5.46%
-7.10%
0.69%"
RESULTS,0.10042283298097252,Table 1: BD-rate of image codecs relative to VTM-12.1 (smaller is better).
RESULTS,0.1014799154334038,"In Table 1, we summarize the BD-rate of image codecs across all four dataset with VTM-12.1 as
anchor. On average SwinT-ChARM is able to achieve 3.8% rate reduction compared to VTM-
12.1. The relative gain from Conv-Hyperprior to SwinT-Hyperprior is on-average 12% and that
from Conv-ChARM to SwinT-ChARM is on-average 9%. Further gain over VTM-12.1 can be
obtained by test-time latent optimization (Campos et al., 2019) or full model instance adaptation (van
Rozendaal et al., 2021), which are out of the scope of this work."
RESULTS,0.10253699788583509,"7The relative rate-saving curves in Figure 3b is generated by ﬁrst interpolating the discrete RD points (av-
eraged across the testset) with a cubic spline, and then compare bitrate of different models at ﬁxed PSNR.
8RD plot for the other three datasets can be found in Appendix (Figure 14, Figure 15, and Figure 16)"
RESULTS,0.10359408033826638,Published as a conference paper at ICLR 2022
RESULTS,0.10465116279069768,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
Rate (bits per pixel) 32 33 34 35 36 37 38 39 40 41"
RESULTS,0.10570824524312897,PSNR (RGB) UVG
RESULTS,0.10676532769556026,"SwinT-SSF
Conv-SSF (reproduced)
Conv-SSF (Agustsson et al.)
HEVC (x265)
AVC (x264)
HEVC (HM)"
RESULTS,0.10782241014799154,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
Rate (bits per pixel) 32 34 36 38 40"
RESULTS,0.10887949260042283,PSNR (RGB)
RESULTS,0.10993657505285412,MCL-JCV
RESULTS,0.1109936575052854,"SwinT-SSF
Conv-SSF (reproduced)
Conv-SSF (Agustsson et al.)
HEVC (x265)
AVC (x264)
HEVC (HM)"
RESULTS,0.11205073995771671,Figure 4: PSNR vs bitrate curves of P-frame codecs on UVG and MCL-JCV datasets.
RESULTS,0.113107822410148,"Table 2: BD-rate of video codecs, with Conv-
SSF (reproduced) as anchor."
RESULTS,0.11416490486257928,"Video Codec
UVG
MCL-JCV"
RESULTS,0.11522198731501057,"HEVC (x265)
25.97%
25.83%
HEVC (HM)
-15.80%
-24.96%"
RESULTS,0.11627906976744186,"SwinT-SSF
-12.35%
-10.03%
SwinT-SSF-Res
-5.08%
-4.16%"
RESULTS,0.11733615221987315,"RD and BD-rate for video codecs
For P-frame
compression, we evaluated SwinT-SSF on UVG
and MCL-JCV, with RD comparison shown in Fig-
ure 4. Again, SwinT transform leads to consis-
tently better RD. Table 2 summarizes BD-rate with
our reproduced Conv-SSF model as anchor. We
can see that SwinT-SSF achieves an average of
11% rate saving over Conv-SSF. Additionally, we
show that if SwinT transform is only applied to
residual-autoencoder (labeled as SwinT-SSF-Res),
it can only get about 4.6% gain, which indicates
that both ﬂow and residual compression beneﬁt from SwinT as encoder and decoder transforms.
Note that SwinT-SSF still lags behind HM, suggesting lots of room for improvement in neural video
compression. For per-video breakdown of BD-rate, see Figure 18 and Figure 17 in the Appendix."
RESULTS,0.11839323467230443,"Table 3: Decoding complexity. All models are trained with β = 0.001, evaluated on 768 × 512
images (average 0.7 bpp). Decoding time is broken down into inference time of hyper-decoder hs
and decoder gs, entropy decoding time of hyper-code ˆz and code ˆy (including inference time of the
prior model if it is ChARM)."
RESULTS,0.11945031712473574,"Codec
Time (ms)
GMACs
Peak
Model
ˆz
hs
ˆy
gs
total
memory
params"
RESULTS,0.12050739957716702,"Conv-Hyperprior
5.5
4.0
38.2
168.9
219.3
350
0.50GB
21.4M
Conv-ChARM
5.3
4.1
82.9
168.5
264.0
362
0.53GB
29.3M
SwinT-Hyperprior
6.0
4.8
38.1
59.6
114.0
99
1.44GB
24.7M
SwinT-ChARM
5.9
4.8
90.7
60.1
167.3
111
1.47GB
32.6M"
RESULTS,0.12156448202959831,"Decoding complexity
We evaluate the decoding complexity of 4 image codecs on 100 images of
size 768 × 512 and show the metrics in Table 3, including decoding time, GMACs and GPU peak
memory during decoding and total model parameters. The models run with PyTorch 1.9.0 on a
workstation with one RTX 2080 Ti GPU. From the table, the inference time of SwinT decoder is
less than that of Conv decoder. The entropy decoding time of ChARM prior is about twice than the
factorized prior. The total decoding time of SwinT-based models is less than Conv-based models.
In ablation study A5, we show a smaller SwinT-Hyperprior with 20.6M parameters has almost the
same RD as the SwinT-Hyperprior proﬁled here. For details on encoding complexity, proﬁling setup,
scaling to image resolution, please refer to Table 4 and Section D.3 in the Appendix."
RESULTS,0.1226215644820296,Published as a conference paper at ICLR 2022
RESULTS,0.12367864693446089,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
MMACs per pixel (for decoding) 0 5 10 15 20 25"
RESULTS,0.12473572938689217,BD-rate over VTM-12.1 (%) on Kodak
M,0.12579281183932348,9.1M small
M,0.12684989429175475,24.7M
M,0.12790697674418605,medium
M,0.12896405919661733,41.8M large
M,0.13002114164904863,8.1M small
M,0.13107822410147993,21.4M
M,0.1321353065539112,medium
M,0.1331923890063425,41.0M large
M,0.13424947145877378,SwinT-Hyperprior
M,0.13530655391120508,Conv-Hyperprior
M,0.13636363636363635,Figure 5: Model size scaling.
M,0.13742071881606766,"Scaling behavior
To see how the BD-rate
varies with model size,
we scale SwinT-
Hyperprior and Conv-Hyperprior to be twice or
half of the size of the base models (i.e. medium
size)9. The result is shown in Figure 5. For
both types of models, as we reduce the base
model size, there is a sharp drop in perfor-
mance, while doubling model size only leads to
marginal gain. Noticeably, SwinT-Hyperprior-
small is on-par with Conv-Hyperprior-medium
even with half of the parameters, and SwinT
transforms in general incur fewer MACs per pa-
rameter."
M,0.13847780126849896,"In Figure 1, we further consolidate the decoding
latency and scaling behavior study into a sin-
gle plot and show that SwinT-ChARM runs at
comparable speed as VTM-12.1 while achiev-
ing better performance,10 as opposed to state-of-the-art neural codecs with spatial autoregressive
prior that decodes orders of magnitude slower."
ANALYSIS,0.13953488372093023,"4.3
ANALYSIS"
ANALYSIS,0.14059196617336153,"Latent correlation
One of the motivating principles of transform coding is that simple coding
can be made more effective in the transform domain than in the original signal space (Goyal, 2001;
Ball´e et al., 2021). A desirable transform would decorrelate the source signal so that simple scalar
quantization and factorized entropy model can be applied without constraining coding performance.
In most mature neural compression solutions, uniform scalar quantization is adopted together with a
learned factorized or conditionally factorized Gaussian prior distribution. It is critical, then, to effec-
tively factorize and Gaussianize the source distribution so that coding overhead can be minimized."
ANALYSIS,0.1416490486257928,"-2
-1
0
1
2"
ANALYSIS,0.1427061310782241,"Conv-Hyperprior(Beta=0.001)
Kodak: PSNR=33.86dB, BPP=0.52 -2 -1 0 1 2"
ANALYSIS,0.14376321353065538,0.0108
ANALYSIS,0.14482029598308668,0.0151
ANALYSIS,0.14587737843551796,0.0307
ANALYSIS,0.14693446088794926,0.0129
ANALYSIS,0.14799154334038056,0.0076
ANALYSIS,0.14904862579281183,0.0126
ANALYSIS,0.15010570824524314,0.0210
ANALYSIS,0.1511627906976744,0.0589
ANALYSIS,0.1522198731501057,0.0186
ANALYSIS,0.15327695560253699,0.0118
ANALYSIS,0.1543340380549683,0.0253
ANALYSIS,0.1553911205073996,0.0514 1.0
ANALYSIS,0.15644820295983086,0.0519
ANALYSIS,0.15750528541226216,0.0255
ANALYSIS,0.15856236786469344,0.0115
ANALYSIS,0.15961945031712474,0.0184
ANALYSIS,0.160676532769556,0.0586
ANALYSIS,0.16173361522198731,0.0208
ANALYSIS,0.16279069767441862,0.0124
ANALYSIS,0.1638477801268499,0.0073
ANALYSIS,0.1649048625792812,0.0125
ANALYSIS,0.16596194503171247,0.0305
ANALYSIS,0.16701902748414377,0.0148
ANALYSIS,0.16807610993657504,0.0106
ANALYSIS,0.16913319238900634,"-2
-1
0
1
2"
ANALYSIS,0.17019027484143764,"SwinT-Hyperprior(Beta=0.001)
Kodak: PSNR=34.49dB, BPP=0.52 -2 -1 0 1 2"
ANALYSIS,0.17124735729386892,0.0040
ANALYSIS,0.17230443974630022,0.0052
ANALYSIS,0.1733615221987315,0.0132
ANALYSIS,0.1744186046511628,0.0033
ANALYSIS,0.17547568710359407,0.0027
ANALYSIS,0.17653276955602537,0.0036
ANALYSIS,0.17758985200845667,0.0047
ANALYSIS,0.17864693446088795,0.0171
ANALYSIS,0.17970401691331925,0.0047
ANALYSIS,0.18076109936575052,0.0032
ANALYSIS,0.18181818181818182,0.0100
ANALYSIS,0.1828752642706131,0.0140 1.0
ANALYSIS,0.1839323467230444,0.0141
ANALYSIS,0.1849894291754757,0.0096
ANALYSIS,0.18604651162790697,0.0036
ANALYSIS,0.18710359408033828,0.0047
ANALYSIS,0.18816067653276955,0.0170
ANALYSIS,0.18921775898520085,0.0047
ANALYSIS,0.19027484143763213,0.0031
ANALYSIS,0.19133192389006343,0.0030
ANALYSIS,0.19238900634249473,0.0033
ANALYSIS,0.193446088794926,0.0133
ANALYSIS,0.1945031712473573,0.0050
ANALYSIS,0.19556025369978858,0.0041 0.00 0.01 0.02 0.03 0.04 0.05 0.06
ANALYSIS,0.19661733615221988,"Figure 6: Spatial correlation11of (y −µ)/σ with models trained at β = 0.001. SwinT-Hyperprior (right)
achieves uniformly smaller correlation than Conv-Hyperprior (left)."
ANALYSIS,0.19767441860465115,"Speciﬁcally, in hyperprior based models (Ball´e et al., 2018), ¯y ≜(y −µ)/σ is modeled as a stan-
dard spherical normal vector. The effectiveness of the analysis transform ga can then be evaluated
by measuring how much correlation there is among different elements in ¯y. We are particularly in-
terested in measuring the correlation between nearby spatial positions, which are heavily correlated
in the source domain for natural images. In Figure 6, we visualize the normalized spatial correlation
of ¯y averaged over all latent channels, and compare Conv-Hyperprior with SwinT-Hyperprior at
β = 0.001. It can be observed that while both lead to small cross-correlations, Swin-Transformer
does a much better job with uniformly smaller correlation values, and the observation is consistent"
ANALYSIS,0.19873150105708245,"9detailed model conﬁgurations are provided in Appendix A.3.
10Note that it is difﬁcult to fairly compare the decoding time of VTM and neural codecs since they run on
different hardware. For more discussion please refer to Appendix D.3."
ANALYSIS,0.19978858350951373,"11The value with index (i, j) corresponds to the normalized cross-correlation of latents at spatial location
(w, h) and (w + i, h + j), averaged across all latent elements of all images on Kodak."
ANALYSIS,0.20084566596194503,Published as a conference paper at ICLR 2022
ANALYSIS,0.20190274841437633,"with other β values, which are provided in Figure 20 in the Appendix. This suggests that trans-
former based transforms incur less redundancy across different spatial latent locations compared
with convolutional ones, leading to an overall better rate-distortion trade-off. The larger spatial
correlation (and thus redundancy) in Conv-Hyperprior also explains why a compute-heavy spatial
auto-regressive model is often needed to improve RD with convolutional based transforms (Minnen
et al., 2018; Lee et al., 2019; Ma et al., 2021; Guo et al., 2021; Wu et al., 2020). Figure 6 also reveals
that most of the correlation of a latent comes from the four elements surrounding it. This suggests
that a checkerboard-based conditional prior model (He et al., 2021) may yield further coding gain."
ANALYSIS,0.2029598308668076,(a) Conv-Hyperprior
ANALYSIS,0.2040169133192389,"20
0
20 30 20 10 0 10 20"
ANALYSIS,0.20507399577167018,"30
0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007"
ANALYSIS,0.20613107822410148,"0.008
(b) Conv-ChARM"
ANALYSIS,0.20718816067653276,"20
0
20 30 20 10 0 10 20"
ANALYSIS,0.20824524312896406,"30
0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008"
ANALYSIS,0.20930232558139536,(c) Conv-SSF-Res
ANALYSIS,0.21035940803382663,"20
0
20 30 20 10 0 10 20"
ANALYSIS,0.21141649048625794,"30
0.0 0.2 0.4 0.6 0.8"
ANALYSIS,0.2124735729386892,(d) Conv-SSF-Flow
ANALYSIS,0.2135306553911205,"100
50
0
50
100 100 50 0 50"
ANALYSIS,0.21458773784355178,"100
0.000 0.001 0.002 0.003 0.004 0.005"
ANALYSIS,0.2156448202959831,(e) SwinT-Hyperprior
ANALYSIS,0.2167019027484144,"20
0
20 30 20 10 0 10 20"
ANALYSIS,0.21775898520084566,"30
0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007"
ANALYSIS,0.21881606765327696,"0.008
(f) SwinT-ChARM"
ANALYSIS,0.21987315010570824,"20
0
20 30 20 10 0 10 20"
ANALYSIS,0.22093023255813954,"30
0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008"
ANALYSIS,0.2219873150105708,(g) SwinT-SSF-Res
ANALYSIS,0.22304439746300211,"20
0
20 30 20 10 0 10 20"
ANALYSIS,0.22410147991543342,"30
0.0 0.2 0.4 0.6 0.8"
ANALYSIS,0.2251585623678647,(h) SwinT-SSF-Flow
ANALYSIS,0.226215644820296,"100
50
0
50
100 100 50 0 50"
ANALYSIS,0.22727272727272727,"100
0.000 0.001 0.002 0.003 0.004 0.005"
ANALYSIS,0.22832980972515857,"Figure 7: Comparison of effective receptive ﬁeld (ERF) of the encoders ga, which is visualized as absolution
gradients of the center pixel in the latent (i.e. dy/dx) with respect to the input image. The plot shows the close
up of the gradient maps averaged over all channels in each input of test images/videos. Check Figure 21 for the
ERF of the composed encoding transform ha ◦ga."
ANALYSIS,0.22938689217758984,"Effective receptive ﬁeld
Intra prediction in HEVC or AV1 only rely on left and top boarders
of the current coding block (Sullivan et al., 2012; Chen et al., 2018), except for intra block copy
for screen content (Xu et al., 2016). We would like to see how large the effective receptive ﬁeld
(ERF) (Luo et al., 2017) of SwinT encoders compared to Conv encoders. The theoretical receptive
ﬁeld of the encoders (ga, ha ◦ga) in SwinT-based codecs is much larger than that of Conv-based
codecs. However comparing Figure 7a with 7e and Figure 7b with 7f, the ERF of SwinT encoders
after training is even smaller than Conv encoders. When we examine the ERF of the released Swin
transformers for classiﬁcation, detection and segmentation tasks, they are all spanning the whole
input image. This contrast suggests that (natural) image compression with rate-distortion objective
is a local task, even with transformer-based nonlinear transforms. We further look into P-frame
compression models, particularly the ERF of two types of transforms in ﬂow codec and residual
codec, as shown in Figure 7d & 7h, and Figure 7c & 7g. Clearly for ﬂow codec, SwinT transform has
much larger ERF than the convolution counterpart. For residual codec, the ERF of SwinT transforms
is similar to image (I-frame) compression case. This shows of ﬂexibility of SwinT encoders to attend
to longer or shorter range depending on the tasks. To get a better picture of the behavior of attention
layers in SwinT transforms, we also show the attention distance in each layer in Figure 22."
ANALYSIS,0.23044397463002114,"Progressive decoding
The ERF in the previous section shows the behavior of the encoder trans-
forms, here we further investigate the decoder transforms through the lens of progressive decod-
ing (Rippel et al., 2014; Minnen & Singh, 2020; Lu et al., 2021). Initialized with the prior mean,
the input to the decoder is progressively updated with the dequantized latent ˆy in terms of coding
units, leading to gradually improved reconstruction quality. For the latent with shape (C, H, W),
we consider three types of coding units, i.e. per channel (1, H, W), per pixel (C, 1, 1), per element
(1, 1, 1). The coding units are ordered by the sum of prior std of all elements within each unit. The
RD curves of progressive decoding for SwinT-Hyperprior and Conv-Hyperprior are shown in Fig-
ure 8a, which closely follow each other when ordered by channel or element, but signiﬁcantly apart
when ordered by pixel (spatial dim). Particularly, we show an extreme case when the half pixels in
the latent (masked by checkerboard pattern) are updated with dequantized values, corresponding to
the two scatter points in Figure 8a. One visual example (CLIC2021 test) is shown in Figure 8b under"
ANALYSIS,0.23150105708245244,Published as a conference paper at ICLR 2022
ANALYSIS,0.23255813953488372,"this setup, where we can clearly see SwinT decoder achieves better reconstruction quality than the
Conv decoder, mainly in terms of more localized response to a single latent pixel. This is potentially
useful for region-of-interest decompression. More visual examples are shown in Figure 26."
ANALYSIS,0.23361522198731502,"0.25
0.50
0.75
1.00
1.25
1.50
Rate (bits per pixel) 22.5 25.0 27.5 30.0 32.5 35.0 37.5 40.0"
ANALYSIS,0.2346723044397463,PSNR (RGB)
ANALYSIS,0.2357293868921776,"SwinT, channel
Conv, channel
SwinT, pixel
Conv, pixel
SwinT, element
Conv, element
SwinT-checkerboard
Conv-checkerboard"
ANALYSIS,0.23678646934460887,"(a) Progressive decoding.
(b) Recon with checkerboard masked latent."
ANALYSIS,0.23784355179704017,"Figure 8: (Left) Progressive decoding according to the order of the sum of prior std of all elements within
each latent coding unit, which can be one latent channel, pixel or element. SwinT-Hyperprior archives about
2dB better reconstruction at the same bitrate than Conv-Hyperprior for per-pixel progressive decoding. (Right)
The reconstructions with SwinT-Hyperprior (top) and Conv-Hyperprior (bottom) for the latent masked with
checkerboard pattern corresponding to the two plus markers on the left subﬁgure. The SwinT decoder has more
localized reconstruction to a single latent pixel.
4.4
ABLATION STUDY"
ANALYSIS,0.23890063424947147,"32
34
36
38
40
PSNR (RGB) 2.5 0.0 2.5 5.0 7.5 10.0 12.5 15.0"
ANALYSIS,0.23995771670190275,Rate saving (%) over Conv-Hyperprior
ANALYSIS,0.24101479915433405,(larger is better)
ANALYSIS,0.24207188160676532,"SwinT-Hyperprior
[A1] No relative positional bias
[A2] No window shift
[A3] Deeper ConvNet
[A4] Replace Attention with DS-Conv
[A5] SwinT-depths=[2,2,5,1]
Conv-Hyperprior"
ANALYSIS,0.24312896405919662,Figure 9: Ablation study
ANALYSIS,0.2441860465116279,"Relative position bias
There are two sources
of positional information in SwinT transforms,
namely the Space-to-Depth modules and the
additive relative position bias (RPB). Even
when the RPB is removed, SwinT-Hyperprior
still outperforms Conv-Hyperprior across all bi-
trates, which indicates image compression may
not require accurate relative position."
ANALYSIS,0.2452431289640592,"Shifted window
The original motivation of
shifted window design is to promotes the
inter-layer feature propagation across non-
overlapping windows.
Image compression
performance drops slightly when there is no
shifted window at all. This further suggests im-
age compression requires local information."
ANALYSIS,0.2463002114164905,"The details of ablations A3-A5 in Figure 9 can
be found in Section F of the appendix."
CONCLUSION,0.24735729386892177,"5
CONCLUSION"
CONCLUSION,0.24841437632135308,"In this work we propose Swin transformer based transforms for image and video compression. In the
image compression setting, SwinT transform consistently outperforms its convolutional counterpart.
Particularly, the proposed SwinT-ChARM model outperforms VTM-12.1 at comparable decoding
speed, which, to the best of our knowledge, is the ﬁrst in learning-based methods. We also show the
effectiveness of SwinT transforms when extended to the P-frame compression setting. Compared
with convolution transforms, SwinT transforms can spatially decorrelate the latent better, have more
ﬂexible receptive ﬁeld to adapt to tasks that requires either short-range (image) and long-range
(motion) information, and better progressive decoding of latent pixels. While pushing the neural
image compression to a new level in terms of rate-distortion-computation trade-off, we believe it is
only the starting point for developing more efﬁcient transformer-based image and video codecs."
CONCLUSION,0.24947145877378435,Published as a conference paper at ICLR 2022
CONCLUSION,0.25052854122621565,ACKNOWLEDGMENTS
CONCLUSION,0.25158562367864695,"We would like to thank Amir Said for developing entropy coding and great advice on data compres-
sion in general. We would also appreciate the helpful discussions from Reza Pourreza and Hoang
Le, and draft reviews from Auke Wiggers and Johann Brehmer."
CONCLUSION,0.2526427061310782,Published as a conference paper at ICLR 2022
REFERENCES,0.2536997885835095,REFERENCES
REFERENCES,0.2547568710359408,"Eirikur Agustsson, David Minnen, Nick Johnston, Johannes Balle, Sung Jin Hwang, and George
Toderici. Scale-space ﬂow for end-to-end optimized video compression. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8503–8512, 2020."
REFERENCES,0.2558139534883721,"Nicola Asuni and Andrea Giachetti. Testimages: a large-scale archive for testing visual devices and
basic image processing algorithms. In STAG, pp. 63–70, 2014."
REFERENCES,0.2568710359408034,"Johannes Ball´e, Valero Laparra, and Eero P. Simoncelli. Density modeling of images using a gener-
alized normalization transformation. In Yoshua Bengio and Yann LeCun (eds.), 4th International
Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.06281."
REFERENCES,0.25792811839323465,"Johannes Ball´e, Valero Laparra, and Eero P. Simoncelli. End-to-end optimized image compres-
sion. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
URL https:
//openreview.net/forum?id=rJxdQ3jeg."
REFERENCES,0.25898520084566595,"Johannes Ball´e, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational Im-
age Compression with a Scale Hyperprior. In 6th International Conference on Learning Represen-
tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceed-
ings. OpenReview.net, 2018. URL https://openreview.net/forum?id=rkcQFMZRb."
REFERENCES,0.26004228329809725,"Johannes Ball´e, Philip A. Chou, David Minnen, Saurabh Singh, Nick Johnston, Eirikur Agusts-
son, Sung Jin Hwang, and George Toderici.
Nonlinear transform coding.
IEEE J. Sel. Top.
Signal Process., 15(2):339–353, 2021.
doi: 10.1109/JSTSP.2020.3034501.
URL https:
//doi.org/10.1109/JSTSP.2020.3034501."
REFERENCES,0.26109936575052856,"Joaquim Campos, Simon Meierhans, Abdelaziz Djelouah, and Christopher Schroers.
Con-
tent Adaptive Optimization for Neural Image Compression.
In IEEE Conference on
Computer Vision and Pattern Recognition Workshops,
CVPR Workshops 2019,
Long
Beach, CA, USA, June 16-20, 2019, pp.
0. Computer Vision Foundation / IEEE, 2019.
URL
http://openaccess.thecvf.com/content_CVPRW_2019/html/CLIC_
2019/Campos_Content_Adaptive_Optimization_for_Neural_Image_
Compression_CVPRW_2019_paper.html."
REFERENCES,0.26215644820295986,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 - 16th Eu-
ropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346 of Lec-
ture Notes in Computer Science, pp. 213–229. Springer, 2020. doi: 10.1007/978-3-030-58452-8\"
REFERENCES,0.2632135306553911,13. URL https://doi.org/10.1007/978-3-030-58452-8_13.
REFERENCES,0.2642706131078224,"Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,
Chunjing Xu, Chao Xu, and Wen Gao.
Pre-trained image processing transformer.
CoRR,
abs/2012.00364, 2020. URL https://arxiv.org/abs/2012.00364."
REFERENCES,0.2653276955602537,"Tong Chen, Haojie Liu, Zhan Ma, Qiu Shen, Xun Cao, and Yao Wang. End-to-end learnt image
compression via non-local attention optimization and improved context modeling. IEEE Trans.
Image Process., 30:3179–3191, 2021. doi: 10.1109/TIP.2021.3058615. URL https://doi.
org/10.1109/TIP.2021.3058615."
REFERENCES,0.266384778012685,"Yue Chen, Debargha Murherjee, Jingning Han, Adrian Grange, Yaowu Xu, Zoe Liu, Sarah Parker,
Cheng Chen, Hui Su, Urvang Joshi, Ching-Han Chiang, Yunqing Wang, Paul Wilkins, Jim
Bankoski, Luc Trudeau, Nathan Egge, Jean-Marc Valin, Thomas Davies, Steinar Midtskogen,
Andrey Norkin, and Peter de Rivaz. An overview of core coding tools in the av1 video codec. In
2018 Picture Coding Symposium (PCS), pp. 41–45, 2018. doi: 10.1109/PCS.2018.8456249."
REFERENCES,0.26744186046511625,"Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. Learned image compression with
discretized gaussian mixture likelihoods and attention modules. In 2020 IEEE/CVF Conference
on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020,
pp. 7936–7945. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00796. URL https://doi.
org/10.1109/CVPR42600.2020.00796."
REFERENCES,0.26849894291754756,Published as a conference paper at ICLR 2022
REFERENCES,0.26955602536997886,"CLIC. Clic2021 challenge on learned image compression, 2021. URL http://compression.
cc/tasks/."
REFERENCES,0.27061310782241016,"Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: pure
attention loses rank doubly exponentially with depth. In Marina Meila and Tong Zhang (eds.),
Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 2793–2803.
PMLR, 2021. URL http://proceedings.mlr.press/v139/dong21a.html."
REFERENCES,0.27167019027484146,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?
id=YicbFdNTTy."
REFERENCES,0.2727272727272727,"Hilmi E. Egilmez, Ankitesh K. Singh, Muhammed Z. Coban, Marta Karczewicz, Yinhao Zhu, Yang
Yang, Amir Said, and Taco S. Cohen. Transform network architectures for deep learning based
end-to-end image/video coding in subsampled color spaces. CoRR, abs/2103.01760, 2021. URL
https://arxiv.org/abs/2103.01760."
REFERENCES,0.273784355179704,"Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand
Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, and Herv´e J´egou. Xcit:
Cross-covariance image transformers. CoRR, abs/2106.09681, 2021. URL https://arxiv.
org/abs/2106.09681."
REFERENCES,0.2748414376321353,"V.K. Goyal. Theoretical foundations of transform coding. IEEE Signal Processing Magazine, 18
(5):9–21, 2001. doi: 10.1109/79.952802."
REFERENCES,0.2758985200845666,"Zongyu Guo, Zhizheng Zhang, Runsen Feng, and Zhibo Chen. Causal contextual prediction for
learned image compression. IEEE Transactions on Circuits and Systems for Video Technology,
pp. 1–1, 2021. doi: 10.1109/TCSVT.2021.3089491."
REFERENCES,0.2769556025369979,"Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong Wang. Demysti-
fying local vision transformer: Sparse connectivity, weight sharing, and dynamic weight. CoRR,
abs/2106.04263, 2021. URL https://arxiv.org/abs/2106.04263."
REFERENCES,0.27801268498942916,"Dailan He, Yaoyan Zheng, Baocheng Sun, Yan Wang, and Hongwei Qin. Checkerboard context
model for efﬁcient learned image compression. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 14771–14780, 2021."
REFERENCES,0.27906976744186046,"Zhihao Hu, Guo Lu, and Dong Xu. FVC: A New Framework towards Deep Video Compression in
Feature Space. arXiv preprint arXiv:2105.09600, 2021."
REFERENCES,0.28012684989429176,"Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David
Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. H´enaff,
Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Jo˜ao Carreira.
Perceiver IO:
A General Architecture for Structured Inputs & Outputs. CoRR, abs/2107.14795, 2021. URL
https://arxiv.org/abs/2107.14795."
REFERENCES,0.28118393234672306,"JPEG-AI. JPEG-AI Test Images. https://jpegai.github.io/test_images/, 2020."
REFERENCES,0.2822410147991543,"Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoen-
coders and nonlinear ica: A unifying framework. In International Conference on Artiﬁcial Intel-
ligence and Statistics, pp. 2207–2217. PMLR, 2020."
REFERENCES,0.2832980972515856,"Kodak. Kodak Test Images. http://r0k.us/graphics/kodak/, 1999."
REFERENCES,0.2843551797040169,"Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In International Conference on Machine Learning, pp. 3519–
3529. PMLR, 2019."
REFERENCES,0.2854122621564482,Published as a conference paper at ICLR 2022
REFERENCES,0.2864693446088795,"Jooyoung Lee, Seunghyun Cho, and Seung-Kwon Beack. Context-adaptive entropy model for end-
to-end optimized image compression. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=HyxKIiAqYQ."
REFERENCES,0.28752642706131076,"Mu Li, Kai Zhang, Wangmeng Zuo, Radu Timofte, and David Zhang. Learning context-based non-
local entropy modeling for image compression. CoRR, abs/2005.04661, 2020. URL https:
//arxiv.org/abs/2005.04661."
REFERENCES,0.28858350951374206,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo.
Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.
CoRR,
abs/2103.14030, 2021. URL https://arxiv.org/abs/2103.14030."
REFERENCES,0.28964059196617337,"Yadong Lu, Yinhao Zhu, Yang Yang, Amir Said, and Taco S Cohen. Progressive neural image
compression with nested quantization and latent ordering. In 2021 IEEE International Conference
on Image Processing (ICIP), pp. 539–543, 2021. doi: 10.1109/ICIP42928.2021.9506026."
REFERENCES,0.29069767441860467,"Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard S. Zemel. Understanding the Effective Receptive
Field in Deep Convolutional Neural Networks.
CoRR, abs/1701.04128, 2017.
URL http:
//arxiv.org/abs/1701.04128."
REFERENCES,0.2917547568710359,"Changyue Ma, Zhao Wang, Ru-Ling Liao, and Yan Ye. A cross channel context model for latents in
deep image compression. CoRR, abs/2103.02884, 2021. URL https://arxiv.org/abs/
2103.02884."
REFERENCES,0.2928118393234672,"Simon Meister, Junhwa Hur, and Stefan Roth. Unﬂow: Unsupervised learning of optical ﬂow with
a bidirectional census loss. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings
of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative
Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018,
pp. 7251–7259. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/
AAAI/AAAI18/paper/view/16502."
REFERENCES,0.2938689217758985,"Alexandre Mercat, Marko Viitanen, and Jarno Vanne. Uvg dataset: 50/120fps 4k sequences for
video codec analysis and development. In Proceedings of the 11th ACM Multimedia Systems
Conference, MMSys ’20, pp. 297–302, New York, NY, USA, 2020. Association for Computing
Machinery. ISBN 9781450368452. doi: 10.1145/3339825.3394937. URL https://doi.
org/10.1145/3339825.3394937."
REFERENCES,0.2949260042283298,"David Minnen and Saurabh Singh.
Channel-Wise Autoregressive Entropy Models for Learned
Image Compression. In IEEE International Conference on Image Processing, ICIP 2020, Abu
Dhabi, United Arab Emirates, October 25-28, 2020, pp. 3339–3343. IEEE, 2020.
doi: 10.
1109/ICIP40778.2020.9190935. URL https://doi.org/10.1109/ICIP40778.2020.
9190935."
REFERENCES,0.2959830866807611,"David Minnen, Johannes Ball´e, and George Toderici.
Joint autoregressive and hierarchi-
cal priors for learned image compression.
In Samy Bengio, Hanna M. Wallach, Hugo
Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances
in Neural Information Processing Systems 31:
Annual Conference on Neural Informa-
tion Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada,
pp. 10794–10803, 2018.
URL https://proceedings.neurips.cc/paper/2018/
hash/53edebc543333dfbf7c5933af792c9c4-Abstract.html."
REFERENCES,0.29704016913319237,"Yichen Qian, Zhiyu Tan, Xiuyu Sun, Ming Lin, Dongyang Li, Zhenhong Sun, Hao Li, and
Rong Jin. Learning accurate entropy model with global reference for image compression. In
9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021. OpenReview.net, 2021.
URL https://openreview.net/forum?id=
cTbIjyrUVwJ."
REFERENCES,0.29809725158562367,"Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the ex-
pressive power of deep neural networks. In international conference on machine learning, pp.
2847–2854. PMLR, 2017."
REFERENCES,0.29915433403805497,Published as a conference paper at ICLR 2022
REFERENCES,0.30021141649048627,"Oren Rippel, Michael A. Gelbart, and Ryan P. Adams.
Learning ordered representations with
nested dropout. In Proceedings of the 31th International Conference on Machine Learning, ICML
2014, Beijing, China, 21-26 June 2014, volume 32 of JMLR Workshop and Conference Proceed-
ings, pp. 1746–1754. JMLR.org, 2014. URL http://proceedings.mlr.press/v32/
rippel14.html."
REFERENCES,0.3012684989429176,"Oren Rippel, Alexander G. Anderson, Kedar Tatwawadi, Sanjay Nair, Craig Lytle, and Lubomir D.
Bourdev. ELF-VC: efﬁcient learned ﬂexible-rate video coding. CoRR, abs/2104.14335, 2021.
URL https://arxiv.org/abs/2104.14335."
REFERENCES,0.3023255813953488,"Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high
efﬁciency video coding (hevc) standard. IEEE Transactions on circuits and systems for video
technology, 22(12):1649–1668, 2012."
REFERENCES,0.3033826638477801,"Thiow Keng Tan, Rajitha Weerakkody, Marta Mrak, Naeem Ramzan, Vittorio Baroncini, Jens-
Rainer Ohm, and Gary J. Sullivan. Video quality evaluation methodology and veriﬁcation testing
of hevc compression performance. IEEE Transactions on Circuits and Systems for Video Tech-
nology, 26(1):76–90, 2016. doi: 10.1109/TCSVT.2015.2477916."
REFERENCES,0.3044397463002114,"Ties van Rozendaal, Iris A. M. Huijben, and Taco Cohen. Overﬁtting for Fun and Proﬁt: Instance-
Adaptive Data Compression.
In 9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
URL https:
//openreview.net/forum?id=oFp8Mx_V5FL."
REFERENCES,0.3054968287526427,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html."
REFERENCES,0.30655391120507397,"Haiqiang Wang, Weihao Gan, Sudeng Hu, Joe Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang,
Ioannis Katsavounidis, Anne Aaron, and C-C Jay Kuo. MCL-JCV: a JND-based H. 264/AVC
video quality assessment dataset. In 2016 IEEE International Conference on Image Processing
(ICIP), pp. 1509–1513. IEEE, 2016."
REFERENCES,0.30761099365750527,"Yaojun Wu, Xin Li, Zhizheng Zhang, Xin Jin, and Zhibo Chen. Learned block-based hybrid image
compression. arXiv preprint arXiv:2012.09550, 2020."
REFERENCES,0.3086680761099366,"Yueqi Xie, Ka Leong Cheng, and Qifeng Chen. Enhanced invertible encoding for learned image
compression. August 2021."
REFERENCES,0.3097251585623679,"Xiaozhong Xu, Shan Liu, Tzu-Der Chuang, Yu-Wen Huang, Shaw-Min Lei, Krishnakanth Rapaka,
Chao Pang, Vadim Seregin, Ye-Kui Wang, and Marta Karczewicz.
Intra block copy in hevc
screen content coding extensions. IEEE Journal on Emerging and Selected Topics in Circuits and
Systems, 6(4):409–419, 2016. doi: 10.1109/JETCAS.2016.2597645."
REFERENCES,0.3107822410147992,"Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video Enhancement
with Task-Oriented Flow. International Journal of Computer Vision (IJCV), 127(8):1106–1125,
2019."
REFERENCES,0.3118393234672304,"Sixiao Zheng,
Jiachen Lu,
Hengshuang Zhao,
Xiatian Zhu,
Zekun Luo,
Yabiao Wang,
Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang.
Rethink-
ing semantic segmentation from a sequence-to-sequence perspective with transformers.
In IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2021,
vir-
tual,
June 19-25,
2021,
pp. 6881–6890. Computer Vision Foundation / IEEE, 2021.
URL
https://openaccess.thecvf.com/content/CVPR2021/html/Zheng_
Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_
Perspective_With_Transformers_CVPR_2021_paper.html."
REFERENCES,0.3128964059196617,"Lei Zhou, Zhenhong Sun, Xiangji Wu, and Junmin Wu. End-to-end optimized image compression
with attention mechanism. In CVPR workshops, pp. 0, 2019."
REFERENCES,0.313953488372093,Published as a conference paper at ICLR 2022
REFERENCES,0.3150105708245243,Appendix
REFERENCES,0.31606765327695563,Table of Contents
REFERENCES,0.3171247357293869,"A Models
15
A.1
Convolution baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
A.2
Swin-Transformer based compression models . . . . . . . . . . . . . . . . . . .
15
A.3
Model conﬁgurations for model size scaling study
. . . . . . . . . . . . . . . .
16"
REFERENCES,0.3181818181818182,"B
Training and Evaluation
17
B.1
Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
B.2
Traditional codec evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19"
REFERENCES,0.3192389006342495,"C BD rate computation
19
C.1
BD rate for image codec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
C.2
BD rate for video codec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.3202959830866808,"D More Results
21
D.1
Image compression
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
D.2
Video Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
D.3
Coding complexity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
REFERENCES,0.321353065539112,"E
Analysis
24
E.1
Spatial correlation of latent
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
E.2
Effective Receptive Field
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
E.3
Rate distribution across latent channels . . . . . . . . . . . . . . . . . . . . . .
25
E.4
Centered kernel alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
E.5
Progressive decoding
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25"
REFERENCES,0.3224101479915433,"F
More ablation studies
26"
REFERENCES,0.32346723044397463,"A
MODELS"
REFERENCES,0.32452431289640593,"A.1
CONVOLUTION BASELINES"
REFERENCES,0.32558139534883723,"Conv-Hyperprior and Conv-ChARM
The architecture of Conv-Hyperprior and Conv-
ChARM are shown in Figure 10 and Figure 11.
For both architecture, our base model
(i.e.
medium size) has the following hyperparameters:
(C1, C2, C3, C4, C5, C6, C7)
=
(320, 320, 320, 320, 192, 192, 192)."
REFERENCES,0.3266384778012685,"A.2
SWIN-TRANSFORMER BASED COMPRESSION MODELS"
REFERENCES,0.3276955602536998,"SwinT-Hyperprior, SwinT-ChARM
For both SwinT-Hyperprior and SwinT-ChARM, we use the
same conﬁgurations: (wg, wh) = (8, 4), (C1, C2, C3, C4, C5, C6) = (128, 192, 256, 320, 192, 192),
(d1, d2, d3, d4, d5, d6) = (2, 2, 6, 2, 5, 1) where C, d, and w are deﬁned in Figure 13 and Figure 2.
The head dim is 32 for all attention layers in SwinT-based models."
REFERENCES,0.3287526427061311,"SwinT-SSF
For SwinT transforms used in SSF variant, the ﬁrst Patch Merge block is with down-
sample rate of 4 and two other Patch Merge blocks with downsampling rate of 2. Thus the down-
sampling rate for the encoder is still 16, the same as the image compression models. There are only
3 transformer stages with depths 2, 4, 2. The embedding dim is 96. The number of latent and hyper
latent channels are all 192. The window size is 4 for ﬂow codec and 8 for residual codec."
REFERENCES,0.3298097251585624,Published as a conference paper at ICLR 2022
REFERENCES,0.3308668076109937,Input Image
REFERENCES,0.33192389006342493,"Conv 5x5
Stride=2 GDN"
REFERENCES,0.33298097251585623,"Conv 5x5
Stride=2"
REFERENCES,0.33403805496828753,"Conv 5x5
Stride=2"
REFERENCES,0.33509513742071884,"Conv 5x5
Stride=2 Q"
REFERENCES,0.3361522198731501,"Conv 5x5
Stride=2"
REFERENCES,0.3372093023255814,"Conv 5x5
Stride=2 Q"
REFERENCES,0.3382663847780127,Reconstruction
REFERENCES,0.339323467230444,"ConvT 5x5
Stride=2"
REFERENCES,0.3403805496828753,"ConvT 5x5
Stride=2"
REFERENCES,0.34143763213530653,"ConvT 5x5
Stride=2"
REFERENCES,0.34249471458773784,"ConvT 5x5
Stride=2"
REFERENCES,0.34355179704016914,"ConvT 5x5
Stride=2"
REFERENCES,0.34460887949260044,"ConvT 5x5
Stride=2 AE AD"
REFERENCES,0.3456659619450317,"Bits
Bits"
REFERENCES,0.346723044397463,"Factorized
 Model AE AD _ +"
REFERENCES,0.3477801268498943,Conv 3x3 stride 1 ReLU
REFERENCES,0.3488372093023256,Conv 3x3 stride1 GDN GDN IGDN IGDN IGDN ReLU ReLU ReLU
REFERENCES,0.3498942917547569,Figure 10: Conv-Hyperprior
REFERENCES,0.35095137420718814,Input Image
REFERENCES,0.35200845665961944,"Conv 5x5
Stride=2 GDN"
REFERENCES,0.35306553911205074,"Conv 5x5
Stride=2"
REFERENCES,0.35412262156448204,"Conv 5x5
Stride=2"
REFERENCES,0.35517970401691334,"Conv 5x5
Stride=2 Q"
REFERENCES,0.3562367864693446,"Conv 5x5
Stride=2"
REFERENCES,0.3572938689217759,"Conv 5x5
Stride=2 Q"
REFERENCES,0.3583509513742072,Reconstruction
REFERENCES,0.3594080338266385,"ConvT 5x5
Stride=2"
REFERENCES,0.36046511627906974,"ConvT 5x5
Stride=2"
REFERENCES,0.36152219873150104,"ConvT 5x5
Stride=2"
REFERENCES,0.36257928118393234,"ConvT 5x5
Stride=2"
REFERENCES,0.36363636363636365,"ConvT 5x5
Stride=2"
REFERENCES,0.36469344608879495,"ConvT 5x5
Stride=2"
REFERENCES,0.3657505285412262,"Channel-wise
AutoRegressive 
Model (ChARM) AE AD"
REFERENCES,0.3668076109936575,"Bits
Bits"
REFERENCES,0.3678646934460888,"Factorized
 Model AE AD _ +"
REFERENCES,0.3689217758985201,Conv 3x3 stride 1 ReLU
REFERENCES,0.3699788583509514,Conv 3x3 stride1 GDN GDN IGDN IGDN IGDN ReLU ReLU ReLU
REFERENCES,0.37103594080338265,Figure 11: Conv-ChARM
REFERENCES,0.37209302325581395,"SwinT-SSF-Res
This is a variant where only residual autoencoder uses SwinT transforms. Same
architecture as the residual autoencoder in SwinT-SSF."
REFERENCES,0.37315010570824525,"A.3
MODEL CONFIGURATIONS FOR MODEL SIZE SCALING STUDY"
REFERENCES,0.37420718816067655,"A.3.1
SWINT-HYPERPRIOR"
REFERENCES,0.3752642706131078,"Set of model hyperparameters that are common to all experiments: (d1, d2, d3, d4, d5, d6) =
(2, 2, 6, 2, 5, 1) (wg, wh) = (8, 4)"
REFERENCES,0.3763213530655391,"SwinT-Hyperprior (small)
(C1, C2, C3, C4, C5, C6) = (96, 128, 160, 192, 96, 128)"
REFERENCES,0.3773784355179704,"SwinT-Hyperprior (medium)
(C1, C2, C3, C4, C5, C6) = (128, 192, 256, 320, 192, 192)"
REFERENCES,0.3784355179704017,"SwinT-Hyperprior (large)
(C1, C2, C3, C4, C5, C6) = (160, 256, 352, 448, 192, 256)"
REFERENCES,0.379492600422833,"A.3.2
CONV-HYPERPRIOR"
REFERENCES,0.38054968287526425,"Conv-Hyperprior (small)
(C1, C2, C3, C4, C5, C6, C7) = (192, 192, 192, 192, 128, 128, 128)"
REFERENCES,0.38160676532769555,"Conv-Hyperprior (medium)
(C1, C2, C3, C4, C5, C6, C7) = (320, 320, 320, 320, 192, 192, 192)"
REFERENCES,0.38266384778012685,Published as a conference paper at ICLR 2022
REFERENCES,0.38372093023255816,Channel Split
REFERENCES,0.38477801268498946,Conv 3x3 stride1 ReLU ReLU
REFERENCES,0.3858350951374207,Conv 3x3 stride1
REFERENCES,0.386892177589852,Conv 3x3 stride1
REFERENCES,0.3879492600422833,Conv 3x3 stride1 ReLU ReLU
REFERENCES,0.3890063424947146,Conv 3x3 stride1
REFERENCES,0.39006342494714585,Conv 3x3 stride1
REFERENCES,0.39112050739957716,ChARM-Block
REFERENCES,0.39217758985200846,"Channel-wise 
AutoRegressive 
Model 
(ChARM)"
REFERENCES,0.39323467230443976,"ChARM-
Block"
REFERENCES,0.39429175475687106,"Channel 
Concat"
REFERENCES,0.3953488372093023,"ChARM-
Block"
REFERENCES,0.3964059196617336,"Channel 
Concat"
REFERENCES,0.3974630021141649,"ChARM-
Block"
REFERENCES,0.3985200845665962,"Channel 
Concat"
REFERENCES,0.39957716701902746,"ChARM-
Block"
REFERENCES,0.40063424947145876,"Channel 
Concat"
REFERENCES,0.40169133192389006,"...
..."
REFERENCES,0.40274841437632136,"...
..."
REFERENCES,0.40380549682875266,"Figure 12: ChARM architecture. Since yi can only be decoded after µi and σi is obtained, the S
ChARM-blocks are executed sequentially. We use S = 10 in all our experiments, which is consistent
with (Minnen & Singh, 2020)."
REFERENCES,0.4048625792811839,Input Image
REFERENCES,0.4059196617336152,"Swin Transformer
Block"
REFERENCES,0.4069767441860465,Patch Merge
REFERENCES,0.4080338266384778,Patch Merge
REFERENCES,0.4090909090909091,"Swin Transformer
Block"
REFERENCES,0.41014799154334036,Patch Merge
REFERENCES,0.41120507399577166,"Swin Transformer
Block"
REFERENCES,0.41226215644820297,Patch Merge
REFERENCES,0.41331923890063427,"Swin Transformer
Block Q"
REFERENCES,0.4143763213530655,"Swin Transformer
Block"
REFERENCES,0.4154334038054968,Patch Merge
REFERENCES,0.4164904862579281,Patch Merge
REFERENCES,0.4175475687103594,"Swin Transformer
Block Q"
REFERENCES,0.4186046511627907,Reconstruction
REFERENCES,0.41966173361522197,"Swin Transformer
Block"
REFERENCES,0.42071881606765327,Patch Split
REFERENCES,0.42177589852008457,Patch Split
REFERENCES,0.42283298097251587,"Swin Transformer
Block"
REFERENCES,0.42389006342494717,Patch Split
REFERENCES,0.4249471458773784,"Swin Transformer
Block"
REFERENCES,0.4260042283298097,Patch Split
REFERENCES,0.427061310782241,"Swin Transformer
Block"
REFERENCES,0.4281183932346723,Patch Split
REFERENCES,0.42917547568710357,"Swin Transformer
Block"
REFERENCES,0.43023255813953487,Patch Split
REFERENCES,0.4312896405919662,"Swin Transformer
Block AE AD"
REFERENCES,0.4323467230443975,"Bits
Bits"
REFERENCES,0.4334038054968288,"Factorized
 Model AE AD _ +"
REFERENCES,0.43446088794926,Figure 13: SwinT-Hyperprior
REFERENCES,0.4355179704016913,"Conv-Hyperprior (large)
(C1, C2, C3, C4, C5, C6, C7) = (448, 448, 448, 448, 256, 256, 256)"
REFERENCES,0.4365750528541226,"B
TRAINING AND EVALUATION"
REFERENCES,0.4376321353065539,"B.1
TRAINING"
REFERENCES,0.43868921775898523,"All image compression models are trained on CLIC2020 training set, which contains both profes-
sional and mobile training sets, in total 1,633 high resolution natural images. Conv-Hyperprior and
SwinT-Hyperprior are trained with 2M batches. Each batch contains 8 patches of size 256 × 256
randomly cropped from the training images. Learning rate starts at 10−4 and is reduced to 10−5 at
1.8M step."
REFERENCES,0.4397463002114165,"For Conv-ChARM, we ﬁrst train a model Conv-ChARM at β = 0.0001 from scratch for 2M steps,
with it as the starting point, we continue to train other beta values Conv-ChARM-β, β ∈B for 1.5M
steps. For SwinT-ChARM-β, we load the transform weights from the checkpoint at 2M step of the
pretrained SwinT-Hyperprior-β, then ﬁnetune the transforms together with the random initialized
ChARM prior for 1.1M steps. Learning rate starts at 10−4 and is reduced to 10−5 for the last 100K
steps."
REFERENCES,0.4408033826638478,Published as a conference paper at ICLR 2022
REFERENCES,0.4418604651162791,"Training loss L = D + βR is a weighted combination of distortion D and bitrate R, with β being
the Lagrangian multiplier steering rate-distortion trade-off. Distortion D is MSE in RGB color
space. To cover a wide range of rate and distortion, for each solution, we train 5 models with
β ∈B = {0.003, 0.001, 0.0003, 0.0001, 0.00003}."
REFERENCES,0.4429175475687104,"Usually we need to train longer for the model with larger bitrates (i.e. smaller β) to converge.
Particularly for the results presented in this paper, We train SwinT-Hyperprior-0.00003 for 2.5M
steps instead of 2M steps for the other 4 lower bitrates."
REFERENCES,0.4439746300211416,"For P-frame compression models, we follow the training setup of SSF. Both Conv-SSF and SwinT-
SSF are trained on Vimeo-90k Dataset (Xue et al., 2019) for 1M steps with learning rate 10−4,
batch size of 8, crop size of 256 × 256, followed by 50K steps of training with learning rate 10−5
and crop size12 384×256. The models are trained with 8 β values 2γ ×10−4 : γ ∈{0, 1, ..., 7}. We
adopt one critical trick to stablize the training from (Jaegle et al., 2021; Meister et al., 2018), i.e. to
forward each video sequence twice during one optimization step (mini-batch), once in the original
frame order, once in the reversed frame order. When this trick is used, we set the batch size to be 4
instead of 8. Finally we add ﬂow loss only between 0 and 200K steps, which we found not critical
for stable training but helps improve the RD."
REFERENCES,0.4450317124735729,"For all model training, Adam optimizer is used without weighted decay. Training for 2M steps takes
about 10 days and 14 days respectively for Conv-Hyperprior and SwinT-Hyperprior on a single
Nvidia V100 GPU. Total training time is about 7.5 days on a single Nvidia V100 GPU."
REFERENCES,0.44608879492600423,"For all models, we use mixed quantization during training (Minnen & Singh, 2020), i.e. adding
uniform noise to the continuous latent before passing to the prior model, subtracting prior mean
from the continuous latent followed by rounding before passing to the decoder transform."
REFERENCES,0.44714587737843553,"12We did not use the crop size 384 × 384 during the second stage as in the original paper because the
resolution of Vimeo dataset is 448 × 256. We found in our case increasing crop size from 256 × 256 to
384 × 256 in the second stage does not improve RD."
REFERENCES,0.44820295983086683,Published as a conference paper at ICLR 2022
REFERENCES,0.4492600422832981,"B.2
TRADITIONAL CODEC EVALUATION"
REFERENCES,0.4503171247357294,"In this section, we provide evaluation script used to generate results for traditional codecs."
REFERENCES,0.4513742071881607,"B.2.1
IMAGE CODECS"
REFERENCES,0.452431289640592,"VTM-12.1:
VTM-12.1 software is built from https://vcgit.hhi.fraunhofer.de/
jvet/VVCSoftware_VTM/-/tags/VTM-12.1 and we use the script from Compres-
sAI (https://github.com/InterDigitalInc/CompressAI/tree/efc69ea24) for
dataset evaluation. Speciﬁcally, the following command is issued to gather VTM-12.1 image com-
pression evaluation results:"
REFERENCES,0.45348837209302323,"python -m compressai.utils.bench vtm [path to image folder]
-c [path to VVCSoftware_VTM folder]/cfg/encoder_intra_vtm.cfg
-b [path to VVCSoftware_VTM folder]/bin
-q 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40"
REFERENCES,0.45454545454545453,"BPG:
BPG software is obtained from https://bellard.org/bpg/ and the following com-
mands are used for encoding and decoding."
REFERENCES,0.45560253699788583,"bpgenc -e x265 -q [0 to 51] -f 444
-o [encoded heic file] [original png file]
bpgdec -o [decoded png file] [encoded heic file]"
REFERENCES,0.45665961945031713,"B.2.2
VIDEO CODECS"
REFERENCES,0.45771670190274844,HEVC (x265)
REFERENCES,0.4587737843551797,"ffmpeg -y -pix_fmt yuv420p -s [resolution] -r [frame-rate]
-i [input yuv420 raw video] -c:v libx265 -preset medium
-crf [9, 12, 15, 18, 21, 24, 27, 30] -tune zerolatency
-x265-params ""keyint=12:min-keyint=12:verbose=1""
[output mkv file path]"
REFERENCES,0.459830866807611,AVC (x264)
REFERENCES,0.4608879492600423,"ffmpeg -y -pix_fmt yuv420p -s [resolution] -r [frame-rate]
-i [input yuv420 raw video] -c:v libx264 -preset medium
-crf [9, 12, 15, 18, 21, 24, 27, 30] -tune zerolatency
-x264-params ""keyint=12:min-keyint=12:verbose=1""
[output mkv file path]"
REFERENCES,0.4619450317124736,HEVC (HM)
REFERENCES,0.4630021141649049,"[Path to HM folder]/bin/TAppEncoderStatic
-c [Path to HM folder]/cfg/encoder_lowdelay_P_main.cfg
-i [input yuv raw video] --InputBitDepth=8 -wdt [width]
-hgt [height] -fr [frame-rate] -f [number of frames]
-o [output yuv video] -b [encoded bitstream bin file]
-ip 12
-q [12, 17, 22, 27, 32, 37, 42]"
REFERENCES,0.46405919661733613,"C
BD RATE COMPUTATION"
REFERENCES,0.46511627906976744,"def Bjontegaard_Delta_Rate(
# rate and psnr in ascending order
rate_ref, psnr_ref, # reference
rate_new, psnr_new, # new result
):
min_psnr = max(psnr_ref[0], psnr_new[0], 30)"
REFERENCES,0.46617336152219874,Published as a conference paper at ICLR 2022
REFERENCES,0.46723044397463004,"max_psnr = min(psnr_ref[-1], psnr_new[-1], 44)"
REFERENCES,0.4682875264270613,"log_rate_ref = log(rate_ref)
log_rate_new = log(rate_new)"
REFERENCES,0.4693446088794926,"spline_ref = scipy.interpolate.CubicSpline(
psnr_ref, log_rate_ref, bc_type=’not-a-knot’,
extrapolate=True,
)
spline_new = scipy.interpolate.CubicSpline(
psnr_new, log_rate_new, bc_type=’not-a-knot’,
extrapolate=True,
)"
REFERENCES,0.4704016913319239,"delta_log_rate = (
spline_new.integrate(min_psnr, max_psnr) -
spline_ref.integrate(min_psnr, max_psnr)
)"
REFERENCES,0.4714587737843552,delta_rate = exp(delta_log_rate / (max_psnr - min_psnr))
REFERENCES,0.4725158562367865,return 100 * (delta_rate - 1)
REFERENCES,0.47357293868921774,"C.1
BD RATE FOR IMAGE CODEC"
REFERENCES,0.47463002114164904,"# Evaluate BD-rate on an image dataset
bd_rates = list()
for image in image_dataset:
# evaluate rate and psnr on reference and new codec
# for this image with different qualities
rate_ref, psnr_ref = ReferenceCodec(image, qp=[...])
rate_new, psnr_new = NewImageCodec(image, beta=[...])
bd_rates.append(
Bjontegaard_Delta_Rate(
rate_ref, psnr_ref,
rate_new, psnr_new,
)
)
# BD is computed per image and then averaged
bd_rate = bd_rates.mean()"
REFERENCES,0.47568710359408034,"C.2
BD RATE FOR VIDEO CODEC"
REFERENCES,0.47674418604651164,"# Evaluate BD-rate on a video dataset
bd_rates = list()
for video in video_dataset:
# evaluate rate and psnr on reference and new codec
# for this video with different qualities
rate_ref, psnr_ref = ReferenceCodec(video, qp=[...])
rate_new, psnr_new = NewVideoCodec(video, beta=[...])
bd_rates.append(
Bjontegaard_Delta_Rate(
rate_ref, psnr_ref,
rate_new, psnr_new,
)
)
# BD is computed per video and then averaged
bd_rate = bd_rates.mean()"
REFERENCES,0.47780126849894294,Published as a conference paper at ICLR 2022
REFERENCES,0.4788583509513742,"D
MORE RESULTS"
REFERENCES,0.4799154334038055,"D.1
IMAGE COMPRESSION"
REFERENCES,0.4809725158562368,"Additional rate-distortion results on CLIC2021, Tecnick, and JPEG-AI are provided in Figure 14,
Figure 15, and Figure 16."
REFERENCES,0.4820295983086681,"For a complete comparison of results from existing literatures, we provide a summary RD plot of all
neural image codec solutions known to us in Figure 28 on Kodak. In Figure 29 and Figure 30, we
plot the percentage rate saving with BPG444 and VTM-12.1 as reference, respectively."
REFERENCES,0.48308668076109934,"0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Rate (bits per pixel) 34 36 38 40 42 44 46"
REFERENCES,0.48414376321353064,PSNR (RGB)
REFERENCES,0.48520084566596194,CLIC Test 2021
REFERENCES,0.48625792811839325,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM
Conv-Hyperprior
VTM-12.1
BPG444"
REFERENCES,0.48731501057082455,(a) Rate distortion comparison.
REFERENCES,0.4883720930232558,"34
36
38
40
42
44
PSNR (RGB) 30 20 10 0 10"
REFERENCES,0.4894291754756871,Rate saving (%) over VTM-12.1
REFERENCES,0.4904862579281184,(larger is better)
REFERENCES,0.4915433403805497,CLIC Test 2021
REFERENCES,0.492600422832981,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM"
REFERENCES,0.49365750528541225,"Conv-Hyperprior
VTM-12.1
BPG444"
REFERENCES,0.49471458773784355,(b) BD-rate reduction over VTM-12.1.
REFERENCES,0.49577167019027485,Figure 14: Comparison of compression efﬁciency on CLIC Test 2021.
REFERENCES,0.49682875264270615,"0.5
1.0
1.5
2.0
2.5
Rate (bits per pixel) 32 34 36 38 40 42 44"
REFERENCES,0.4978858350951374,PSNR (RGB)
REFERENCES,0.4989429175475687,Tecnick Test
REFERENCES,0.5,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM
Conv-Hyperprior
VTM-12.1
BPG444"
REFERENCES,0.5010570824524313,(a) Rate distortion comparison.
REFERENCES,0.5021141649048626,"34
36
38
40
42
PSNR (RGB) 30 20 10 0 10"
REFERENCES,0.5031712473572939,Rate saving (%) over VTM-12.1
REFERENCES,0.5042283298097252,(larger is better)
REFERENCES,0.5052854122621564,Tecnick Test
REFERENCES,0.5063424947145877,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM"
REFERENCES,0.507399577167019,"Conv-Hyperprior
VTM-12.1
BPG444"
REFERENCES,0.5084566596194503,(b) BD-rate reduction over VTM-12.1.
REFERENCES,0.5095137420718816,Figure 15: Comparison of compression efﬁciency on Tecnick Test.
REFERENCES,0.5105708245243129,Published as a conference paper at ICLR 2022
REFERENCES,0.5116279069767442,"0.5
1.0
1.5
2.0
2.5
Rate (bits per pixel) 30 32 34 36 38 40 42"
REFERENCES,0.5126849894291755,PSNR (RGB)
REFERENCES,0.5137420718816068,JPEG AI Test
REFERENCES,0.514799154334038,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM
Conv-Hyperprior
VTM-12.1
BPG444"
REFERENCES,0.5158562367864693,(a) Rate distortion comparison.
REFERENCES,0.5169133192389006,"32
34
36
38
40
42
PSNR (RGB) 30 20 10 0"
REFERENCES,0.5179704016913319,Rate saving (%) over VTM-12.1
REFERENCES,0.5190274841437632,(larger is better)
REFERENCES,0.5200845665961945,JPEG AI Test
REFERENCES,0.5211416490486258,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM"
REFERENCES,0.5221987315010571,"Conv-Hyperprior
VTM-12.1
BPG444"
REFERENCES,0.5232558139534884,(b) BD-rate reduction over VTM-12.1.
REFERENCES,0.5243128964059197,Figure 16: Comparison of compression efﬁciency on JPEG AI Test.
REFERENCES,0.5253699788583509,"D.2
VIDEO COMPRESSION"
REFERENCES,0.5264270613107822,"In Figure 17 and Figure 18, we provide performance comparison of Conv-SSF, SwinT-SSF, HEVC
(x265), and AVC (x264) with per-video breakdown."
REFERENCES,0.5274841437632135,"29
28
16
21
08
01
17
07
30
14
26
02
23
05
06
11
22
27
03
12
19
09
04
18
13
15
24
10
25
20
0 25 50 75 100 125"
REFERENCES,0.5285412262156448,File size ratio (%)
REFERENCES,0.5295983086680761,MCL-JCV - File Size Relative to HEVC (x264)
REFERENCES,0.5306553911205074,"HEVC (x265)
Conv-SSF (reproduced)
SwinT-SSF"
REFERENCES,0.5317124735729387,"Figure 17: Rate savings for each video in the MCL-JCV dataset. Values represent the ﬁle size
relative to H.264 as estimated by BD rate. [18, 20, 24, 25] are animated sequences."
REFERENCES,0.53276955602537,Beauty
REFERENCES,0.5338266384778013,ShakeNDry
REFERENCES,0.5348837209302325,Bosphorus
REFERENCES,0.5359408033826638,HoneyBee
REFERENCES,0.5369978858350951,YachtRide
REFERENCES,0.5380549682875264,Jockey
REFERENCES,0.5391120507399577,ReadySetGo 0 20 40 60 80
REFERENCES,0.540169133192389,File size ratio (%)
REFERENCES,0.5412262156448203,UVG - File Size Relative to HEVC (x264)
REFERENCES,0.5422832980972516,"HEVC (x265)
Conv-SSF (reproduced)
SwinT-SSF"
REFERENCES,0.5433403805496829,"Figure 18: Rate savings for each video in the UVG dataset. Values represent the ﬁle size relative to
H.264 as estimated by BD rate."
REFERENCES,0.5443974630021141,"D.3
CODING COMPLEXITY"
REFERENCES,0.5454545454545454,"We evaluate the decoding complexity of all neural image codecs in terms of the time for network
inference and entropy coding, peak GPU memory, model size, etc. We select 100 high resolution
images from CLIC testset and center crop them to three resolutions (768 × 512, 1280 × 768, 1792 ×"
REFERENCES,0.5465116279069767,Published as a conference paper at ICLR 2022
REFERENCES,0.547568710359408,"1024) to see how those metrics scale with image size. Batch size is one for all model inference.
We run the experiment on a local workstation with one RTX 2080 Ti GPU, with PyTorch 1.9.0 and
Cuda toolkit 11.1. For bit-exact entropy coding, we need to use deterministic13 convolution. The
neural networks run on the single GPU and entropy coding runs on CPU with 8 threads. We follow
the standard protocols to measure the inference time and peak memory of neural nets, such as GPU
warm up and synchronization. File open/close is excluded from coding time measurement. MACs,
GPU peak memory, and model parameter count are proﬁled using get model profile function
of deepspeed proﬁler14."
REFERENCES,0.5486257928118393,"We show more details on the coding complexity of neural image codecs in Figure 19, particularly
the linear scaling to image resolution of both SwinT-based and Conv-based models. The break-down
of encoding complexity is shown in Table 4."
REFERENCES,0.5496828752642706,"0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
Mega pixels 0 100 200 300 400 500 600"
REFERENCES,0.5507399577167019,Transform inference time (ms)
REFERENCES,0.5517970401691332,"SwinT encoder
SwinT decoder
Conv encoder
Conv decoder"
REFERENCES,0.5528541226215645,"0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
Mega pixels 50 100 150 200 250 300 350 400 450"
REFERENCES,0.5539112050739958,Encoding time (ms)
REFERENCES,0.554968287526427,"SwinT-Hyperprior
Conv-Hyperprior
SwinT-ChARM
Conv-ChARM"
REFERENCES,0.5560253699788583,"0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
Mega pixels 100 200 300 400 500 600 700 800"
REFERENCES,0.5570824524312896,Decoding time (ms)
REFERENCES,0.5581395348837209,"SwinT-Hyperprior
Conv-Hyperprior
SwinT-ChARM
Conv-ChARM"
REFERENCES,0.5591966173361522,"0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
Mega pixels 1 2 3 4 5 6"
REFERENCES,0.5602536997885835,Decoding peak memory (GB)
REFERENCES,0.5613107822410148,"SwinT-Hyperprior
Conv-Hyperprior
SwinT-ChARM
Conv-ChARM"
REFERENCES,0.5623678646934461,"Figure 19: Detailed proﬁling on a single RTX 2080 Ti with deterministic conv. Note that while
Conv encoder and Conv decoder are of symmetric architectures, Conv decoder takes more time than
Conv encoder mainly because Transposed Conv layers in the decoder take much longer than Conv
layers in the encoder."
REFERENCES,0.5634249471458774,"For completeness, we also report the proﬁling for CPU coding time in Table 5 and Table 6. The
evaluation setup is the same as the GPU proﬁling case, except models are run on the CPU instead
(same host machine with Intel(R) Xeon(R) W-2123 CPU @ 3.60GHz)."
REFERENCES,0.5644820295983086,"Table 7 reports encoding and decoding time of VTM-12.1 under different quantization parameters
(QPs), evaluated on an Intel Core i9-9940 CPU @ 3.30GHz, averaged over 24 Kodak images. As
can be seen from the table, decoding time of VTM-12.1 is a function of reconstruction quality, where
longer decoding time is observed for higher quality reconstruction. In Figure 1, the reported VTM-
12.1 decoding speed corresponds to a QP value of 28, where the bpp value is similar to that obtained
by models trained with β = 0.001. It is worth pointing out that VTM-12.1 encoding process is much
slower, ranging anywhere from 1 to 5 minutes per image, whereas neural codec runs much faster."
REFERENCES,0.5655391120507399,"13https://pytorch.org/docs/1.9.0/generated/torch.use_deterministic_
algorithms.html?highlight=deterministic#torch.use_deterministic_
algorithms
14https://www.deepspeed.ai/tutorials/flops-profiler/
#usage-outside-the-deepspeed-runtime"
REFERENCES,0.5665961945031712,Published as a conference paper at ICLR 2022
REFERENCES,0.5676532769556025,"Table 4: GPU encoding complexity. All models are trained with β = 0.001, evaluated on the
resolution 768 × 512 (the average bitrate is around 0.7 bpp). For the encoding time, we show the
network inference time for the encoder ga, the hyper encoder ha, the hyper decoder hs, the entropy
encoding time for the hyper latent ˆz and the latent ˆy (including network inference time for the prior
model if it is ChARM). GMACs and GPU peak memory during encoding and the parameter count
of the entire model are also listed."
REFERENCES,0.5687103594080338,"Codec
Time (ms)
GMACs
Memory
Params
ga
ha
ˆz
hs
ˆy
total
(GB)
(M)"
REFERENCES,0.5697674418604651,"Conv-Hyperprior
22.7
2.3
2.8
4.0
29.8
62.0
102
0.62
21.4
SwinT-Hyperprior
57.5
1.2
2.4
4.8
34.3
100.5
100
1.47
24.7
Conv-ChARM
22.9
2.4
2.6
4.1
63.4
95.7
114
0.66
29.3
SwinT-ChARM
57.8
1.3
2.4
4.8
68.7
135.3
112
1.51
32.6"
REFERENCES,0.5708245243128964,"Table 5: CPU decoding complexity. All models are trained with β = 0.001, evaluated on 768 × 512
images (average 0.7 bpp). Decoding time is broken down into inference time of hyper-decoder hs
and decoder gs, entropy decoding time of hyper-code ˆz and code ˆy (including inference time of the
prior model if it is ChARM)."
REFERENCES,0.5718816067653277,"Codec
CPU decoding Time (ms)
GMACs
ˆz
hs
ˆy
gs
total"
REFERENCES,0.572938689217759,"Conv-Hyperprior
5.1
20.9
30.5
1636.2
1703.3
350
SwinT-Hyperprior
5.7
18.3
30.3
1912.8
1979.5
99
Conv-ChARM
4.7
20.9
154.6
1634.3
1825.6
362
SwinT-ChARM
5.8
18.2
141.1
1877.3
2054.7
111"
REFERENCES,0.5739957716701902,"Table 6: CPU encoding complexity. All models are trained with β = 0.001, evaluated on the
resolution 768 × 512 (the average bitrate is around 0.7 bpp). For the encoding time, we show the
network inference time for the encoder ga, the hyper encoder ha, the hyper decoder hs, the entropy
encoding time for the hyper latent ˆz and the latent ˆy (including network inference time for the prior
model if it is ChARM)."
REFERENCES,0.5750528541226215,"Codec
CPU encoding Time (ms)
GMACs
ga
ha
ˆz
hs
ˆy
total"
REFERENCES,0.5761099365750528,"Conv-Hyperprior
896.0
8.9
2.7
20.9
31.1
960.9
102
SwinT-Hyperprior
1781.6
14.5
2.6
18.3
29.7
1847.9
100
Conv-ChARM
895.7
8.9
2.5
20.9
146.3
1075.5
114
SwinT-ChARM
1697.1
14.5
2.6
18.2
133.5
1867.1
112"
REFERENCES,0.5771670190274841,Table 7: Decoding and encoding time of VTM-12.1 averaged over 24 Kodak images.
REFERENCES,0.5782241014799154,"QP
bits per pixel
PSNR (dB)
Decoding time(s)
Encoding time (s)"
REFERENCES,0.5792811839323467,"16
2.5441
44.09
0.284
300.47
28
0.7844
36.76
0.249
118.85
40
0.1557
29.51
0.149
71.10"
REFERENCES,0.580338266384778,"E
ANALYSIS"
REFERENCES,0.5813953488372093,"E.1
SPATIAL CORRELATION OF LATENT"
REFERENCES,0.5824524312896406,"We visualize the spatial correlation map for Conv-Hyperprior and SwinT-Hyperprior at different β
in Figure 20."
REFERENCES,0.5835095137420718,Published as a conference paper at ICLR 2022
REFERENCES,0.5845665961945031,"E.2
EFFECTIVE RECEPTIVE FIELD"
REFERENCES,0.5856236786469344,"See Figure 21 for the effective receptive ﬁeld for the composed encoding tranforms ha ◦ga and
Figure 22 for the mean attention distance visualization of each head within each transformer layer."
REFERENCES,0.5866807610993657,"E.3
RATE DISTRIBUTION ACROSS LATENT CHANNELS"
REFERENCES,0.587737843551797,"It is generally believed that ConvNets learn to extract various features and store them in each chan-
nel of the activations. Here we look into the features in the latent channels which are to be quantized
and entropy coded to bitstreams. Particularly we order the total bitrate of each channel averaged
over Kodak dataset (24 768 × 256 images). The result is shown in Figure 23. We ﬁnd an interesting
phenomenon across models under different bitrates: there is a cutoff point of the bitrate-vs-channel
curve where the bitrate suddenly drops to zero, which manifest the rate constraint in the loss func-
tion. As expected, the cutoff index decreases for the model trained for smaller bitrate (larger β)."
REFERENCES,0.5887949260042283,"E.4
CENTERED KERNEL ALIGNMENT"
REFERENCES,0.5898520084566596,"To investigate the difference or similarity between latent features of Conv-based and SwinT-based
models, we resort to a commonly used tool in representation learning called centered kernel align-
ment (CKA) (Kornblith et al., 2019). We evaluate CKA between each of the Conv latent channel and
SwinT latent channel (both models are trained under the same β) over the 24 Kodak images. There
are 320 channels for both Conv latent and SwinT latent, resulting a 320 × 320 CKA matrix. The
result is shown in Figure 24. The latent channel is ordered by the averaged bitrate of each channel
over Kodak images (same as in Section E.3). The CKA matrix has clear block structure, where high
similarity region corresponds to the latent channels before the bitrate cutoff in the rate distribution
curve (Figure 23)."
REFERENCES,0.5909090909090909,"Identiﬁcation of SwinT and Conv latent channels with CKA
Within the block of high similarity
(from the CKA matrix), we identify the ‘less’ similar SwinT latent channels with lowest CKA values
between this SwinT channel and all other Conv latent channels. For each of the identiﬁed SwinT
channel, we ﬁnd the Conv latent channel with the largest CKA value between the two. This way,
we are able to identify latent channels of two different models with high similarity. We show the
identiﬁed top 8 channels in Figure 25. The channels are indeed highly similar, up to a sign ﬂip,
even through the two model architectures are quite different. This empirical result is relevant to the
literature on the identiﬁability of generative models (Khemakhem et al., 2020)."
REFERENCES,0.5919661733615222,"E.5
PROGRESSIVE DECODING"
REFERENCES,0.5930232558139535,"More visual examples of reconstructions of checkerboard (spatially) masked latent are provided in
Figure 26."
REFERENCES,0.5940803382663847,"Channel-wise progressive decoding
Here we visualize the behavior of channel-wise progressive
decoding of Conv and SwinT models. For both models, we order the latent channels with a heuristic
importance metric: the reduction of distortion over the increase of rate if one channel is included for
decoding. We start with the order of bitrate per channel, we pass the leading channels of bitrate order
(zero out all rest channels) to the decoder to obtain reconstruction and calculate distortion. We plot
the top 8 channels following this importance order. For each channel, we show 6 maps from top to
bottom: latent values, mean prediction from the hyper decoder, standard deviation prediction from
the hyper decoder, the bitmap, the reconstruction with only current channel, the reconstruction with
up to current channel (all leading channels). The result is shown in Figure 27. For Conv models,
usually the top 3 important channels are responsible for lightness, and two color components (blue-
yellow, red-green), similar to the LAB colorspace. The rest of the latent channels are responsible for
adding details like texture and edges. For the Swint models, at low bitrate, there is one signiﬁcantly
different channel (the ﬁrst column in Figure 27b), which is in a coarse scale (with smooth blocks)
and responsible for the reconstruction of a nearly constant image with value close to 120 (the mean
value of natural image dataset). This latent channel costs extremely small bitrate but reaches PSNR
of 13dB. We tried remove this ﬁrst channel, the progressive reconstruction with the rest leading 7
channels only leads to PSNR around 16dB, instead of 26dB shown in the ﬁgure."
REFERENCES,0.595137420718816,Published as a conference paper at ICLR 2022
REFERENCES,0.5961945031712473,"F
MORE ABLATION STUDIES"
REFERENCES,0.5972515856236786,"Local self-attention
To see if local self-attention is the most important component in transform-
ers, we replace it by depthwise separable convolution block15 (Han et al., 2021; El-Nouby et al.,
2021), which performs similar spatial feature aggregation as self-attention, while keeping all other
components the same as in the SwinT. We found this change only leads to minor degradation in RD.
This suggests other components in transformers such as MLPs and skip connections may also play
a big role, other than just self-attention, for the leading performance in our work and many other
tasks (Dong et al., 2021)."
REFERENCES,0.5983086680761099,"Small depths
Upon investigating the mean attention distance as shown in Figure 22, we ﬁnd the
last block in each of the last two encoder stages has about half of its attention heads degenerate to
attending to ﬁxed nearly pixels. This suggests redundant transformer blocks at that stage, so we
remove those two blocks, i.e. from depths [2, 2, 6, 2] to [2, 2, 5, 1]. The resulting SwinT-Hyperprior
has even less parameters (20.6M) than Conv-Hyperprior (21.4M) while with almost no RD loss
compared to the larger model. We expect more hyperparameter search will identify models with
better RD-complexity trade-off than we currently show in this work."
REFERENCES,0.5993657505285412,"Deeper Conv encoder
Deeper models are usually more expressive (Raghu et al., 2017) and the
state-of-the-art Conv-based compression models typically use much deeper layers than the encoder
in the original Hyperprior model (Ball´e et al., 2018). As a sanity check on whether deeper con-
volutional transforms can outperform SwinT-based encoder transforms with 12 blocks, we take an
existing design (Chen et al., 2021) with residual blocks and attention (sigmoid gating) layers, which
has over 50 conv layers in either encoder or decoder, and more parameters than conv baseline. It
indeed improves the RD in lower bitrate, but still worse than SwinT-Hyperprior, and gets much
worse in higher bitrate. This is probably the reason that compression models based on this type of
transforms did not report results at higher bitrates."
REFERENCES,0.6004228329809725,15Conv1×1-LayerNorm-ReLU-DSConv3×3-LayerNorm-ReLU-Conv1×1-LayerNorm-ReLU
REFERENCES,0.6014799154334038,Published as a conference paper at ICLR 2022
REFERENCES,0.6025369978858351,"-2
-1
0
1
2"
REFERENCES,0.6035940803382663,"Conv-Hyperprior(Beta=0.0001)
Kodak: PSNR=40.80dB, BPP=1.61 -2 -1 0 1 2"
REFERENCES,0.6046511627906976,0.0033
REFERENCES,0.6057082452431289,0.0033
REFERENCES,0.6067653276955602,0.0078
REFERENCES,0.6078224101479915,0.0031
REFERENCES,0.6088794926004228,0.0023
REFERENCES,0.6099365750528541,0.0040
REFERENCES,0.6109936575052854,0.0025
REFERENCES,0.6120507399577167,0.0096
REFERENCES,0.6131078224101479,0.0029
REFERENCES,0.6141649048625792,0.0027
REFERENCES,0.6152219873150105,0.0071
REFERENCES,0.6162790697674418,0.0080 1.0
REFERENCES,0.6173361522198731,0.0080
REFERENCES,0.6183932346723044,0.0070
REFERENCES,0.6194503171247357,0.0027
REFERENCES,0.620507399577167,0.0028
REFERENCES,0.6215644820295984,0.0096
REFERENCES,0.6226215644820295,0.0026
REFERENCES,0.6236786469344608,0.0039
REFERENCES,0.6247357293868921,0.0026
REFERENCES,0.6257928118393234,0.0029
REFERENCES,0.6268498942917548,0.0078
REFERENCES,0.627906976744186,0.0034
REFERENCES,0.6289640591966174,0.0032
REFERENCES,0.6300211416490487,"-2
-1
0
1
2"
REFERENCES,0.63107822410148,"SwinT-Hyperprior(Beta=0.0001)
Kodak: PSNR=40.85dB, BPP=1.56 -2 -1 0 1 2"
REFERENCES,0.6321353065539113,0.0017
REFERENCES,0.6331923890063424,0.0019
REFERENCES,0.6342494714587738,0.0058
REFERENCES,0.635306553911205,0.0019
REFERENCES,0.6363636363636364,0.0007
REFERENCES,0.6374207188160677,0.0018
REFERENCES,0.638477801268499,0.0003
REFERENCES,0.6395348837209303,0.0090
REFERENCES,0.6405919661733616,0.0015
REFERENCES,0.6416490486257929,0.0006
REFERENCES,0.642706131078224,0.0044
REFERENCES,0.6437632135306554,0.0072 1.0
REFERENCES,0.6448202959830867,0.0074
REFERENCES,0.645877378435518,0.0041
REFERENCES,0.6469344608879493,0.0009
REFERENCES,0.6479915433403806,0.0015
REFERENCES,0.6490486257928119,0.0090
REFERENCES,0.6501057082452432,0.0006
REFERENCES,0.6511627906976745,0.0014
REFERENCES,0.6522198731501057,0.0010
REFERENCES,0.653276955602537,0.0016
REFERENCES,0.6543340380549683,0.0058
REFERENCES,0.6553911205073996,0.0021
REFERENCES,0.6564482029598309,0.0017 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040
REFERENCES,0.6575052854122622,"-2
-1
0
1
2"
REFERENCES,0.6585623678646935,"Conv-Hyperprior(Beta=0.0003)
Kodak: PSNR=37.62dB, BPP=1.00 -2 -1 0 1 2"
REFERENCES,0.6596194503171248,0.0051
REFERENCES,0.6606765327695561,0.0069
REFERENCES,0.6617336152219874,0.0143
REFERENCES,0.6627906976744186,0.0062
REFERENCES,0.6638477801268499,0.0036
REFERENCES,0.6649048625792812,0.0071
REFERENCES,0.6659619450317125,0.0083
REFERENCES,0.6670190274841438,0.0235
REFERENCES,0.6680761099365751,0.0087
REFERENCES,0.6691331923890064,0.0064
REFERENCES,0.6701902748414377,0.0131
REFERENCES,0.671247357293869,0.0211 1.0
REFERENCES,0.6723044397463002,0.0212
REFERENCES,0.6733615221987315,0.0133
REFERENCES,0.6744186046511628,0.0061
REFERENCES,0.6754756871035941,0.0084
REFERENCES,0.6765327695560254,0.0234
REFERENCES,0.6775898520084567,0.0085
REFERENCES,0.678646934460888,0.0072
REFERENCES,0.6797040169133193,0.0036
REFERENCES,0.6807610993657506,0.0059
REFERENCES,0.6818181818181818,0.0141
REFERENCES,0.6828752642706131,0.0069
REFERENCES,0.6839323467230444,0.0052
REFERENCES,0.6849894291754757,"-2
-1
0
1
2"
REFERENCES,0.686046511627907,"SwinT-Hyperprior(Beta=0.0003)
Kodak: PSNR=37.85dB, BPP=0.97 -2 -1 0 1 2"
REFERENCES,0.6871035940803383,0.0026
REFERENCES,0.6881606765327696,0.0027
REFERENCES,0.6892177589852009,0.0068
REFERENCES,0.6902748414376322,0.0022
REFERENCES,0.6913319238900634,0.0012
REFERENCES,0.6923890063424947,0.0022
REFERENCES,0.693446088794926,0.0010
REFERENCES,0.6945031712473573,0.0117
REFERENCES,0.6955602536997886,0.0021
REFERENCES,0.6966173361522199,0.0011
REFERENCES,0.6976744186046512,0.0050
REFERENCES,0.6987315010570825,0.0090 1.0
REFERENCES,0.6997885835095138,0.0091
REFERENCES,0.7008456659619451,0.0048
REFERENCES,0.7019027484143763,0.0016
REFERENCES,0.7029598308668076,0.0021
REFERENCES,0.7040169133192389,0.0116
REFERENCES,0.7050739957716702,0.0012
REFERENCES,0.7061310782241015,0.0019
REFERENCES,0.7071881606765328,0.0016
REFERENCES,0.7082452431289641,0.0018
REFERENCES,0.7093023255813954,0.0066
REFERENCES,0.7103594080338267,0.0029
REFERENCES,0.7114164904862579,0.0025 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040
REFERENCES,0.7124735729386892,"-2
-1
0
1
2"
REFERENCES,0.7135306553911205,"Conv-Hyperprior(Beta=0.001)
Kodak: PSNR=33.86dB, BPP=0.52 -2 -1 0 1 2"
REFERENCES,0.7145877378435518,0.0108
REFERENCES,0.7156448202959831,0.0151
REFERENCES,0.7167019027484144,0.0307
REFERENCES,0.7177589852008457,0.0129
REFERENCES,0.718816067653277,0.0076
REFERENCES,0.7198731501057083,0.0126
REFERENCES,0.7209302325581395,0.0210
REFERENCES,0.7219873150105708,0.0589
REFERENCES,0.7230443974630021,0.0186
REFERENCES,0.7241014799154334,0.0118
REFERENCES,0.7251585623678647,0.0253
REFERENCES,0.726215644820296,0.0514 1.0
REFERENCES,0.7272727272727273,0.0519
REFERENCES,0.7283298097251586,0.0255
REFERENCES,0.7293868921775899,0.0115
REFERENCES,0.7304439746300211,0.0184
REFERENCES,0.7315010570824524,0.0586
REFERENCES,0.7325581395348837,0.0208
REFERENCES,0.733615221987315,0.0124
REFERENCES,0.7346723044397463,0.0073
REFERENCES,0.7357293868921776,0.0125
REFERENCES,0.7367864693446089,0.0305
REFERENCES,0.7378435517970402,0.0148
REFERENCES,0.7389006342494715,0.0106
REFERENCES,0.7399577167019028,"-2
-1
0
1
2"
REFERENCES,0.741014799154334,"SwinT-Hyperprior(Beta=0.001)
Kodak: PSNR=34.49dB, BPP=0.52 -2 -1 0 1 2"
REFERENCES,0.7420718816067653,0.0040
REFERENCES,0.7431289640591966,0.0052
REFERENCES,0.7441860465116279,0.0132
REFERENCES,0.7452431289640592,0.0033
REFERENCES,0.7463002114164905,0.0027
REFERENCES,0.7473572938689218,0.0036
REFERENCES,0.7484143763213531,0.0047
REFERENCES,0.7494714587737844,0.0171
REFERENCES,0.7505285412262156,0.0047
REFERENCES,0.7515856236786469,0.0032
REFERENCES,0.7526427061310782,0.0100
REFERENCES,0.7536997885835095,0.0140 1.0
REFERENCES,0.7547568710359408,0.0141
REFERENCES,0.7558139534883721,0.0096
REFERENCES,0.7568710359408034,0.0036
REFERENCES,0.7579281183932347,0.0047
REFERENCES,0.758985200845666,0.0170
REFERENCES,0.7600422832980972,0.0047
REFERENCES,0.7610993657505285,0.0031
REFERENCES,0.7621564482029598,0.0030
REFERENCES,0.7632135306553911,0.0033
REFERENCES,0.7642706131078224,0.0133
REFERENCES,0.7653276955602537,0.0050
REFERENCES,0.766384778012685,0.0041 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040
REFERENCES,0.7674418604651163,"-2
-1
0
1
2"
REFERENCES,0.7684989429175476,"Conv-Hyperprior(Beta=0.003)
Kodak: PSNR=31.09dB, BPP=0.28 -2 -1 0 1 2"
REFERENCES,0.7695560253699789,0.0161
REFERENCES,0.7706131078224101,0.0205
REFERENCES,0.7716701902748414,0.0459
REFERENCES,0.7727272727272727,0.0196
REFERENCES,0.773784355179704,0.0150
REFERENCES,0.7748414376321353,0.0186
REFERENCES,0.7758985200845666,0.0240
REFERENCES,0.7769556025369979,0.0824
REFERENCES,0.7780126849894292,0.0239
REFERENCES,0.7790697674418605,0.0188
REFERENCES,0.7801268498942917,0.0388
REFERENCES,0.781183932346723,0.0696 1.0
REFERENCES,0.7822410147991543,0.0704
REFERENCES,0.7832980972515856,0.0393
REFERENCES,0.7843551797040169,0.0181
REFERENCES,0.7854122621564482,0.0235
REFERENCES,0.7864693446088795,0.0819
REFERENCES,0.7875264270613108,0.0241
REFERENCES,0.7885835095137421,0.0187
REFERENCES,0.7896405919661733,0.0150
REFERENCES,0.7906976744186046,0.0195
REFERENCES,0.7917547568710359,0.0455
REFERENCES,0.7928118393234672,0.0205
REFERENCES,0.7938689217758985,0.0163
REFERENCES,0.7949260042283298,"-2
-1
0
1
2"
REFERENCES,0.7959830866807611,"SwinT-Hyperprior(Beta=0.003)
Kodak: PSNR=31.66dB, BPP=0.29 -2 -1 0 1 2"
REFERENCES,0.7970401691331924,0.0062
REFERENCES,0.7980972515856237,0.0076
REFERENCES,0.7991543340380549,0.0183
REFERENCES,0.8002114164904862,0.0071
REFERENCES,0.8012684989429175,0.0048
REFERENCES,0.8023255813953488,0.0062
REFERENCES,0.8033826638477801,0.0084
REFERENCES,0.8044397463002114,0.0272
REFERENCES,0.8054968287526427,0.0083
REFERENCES,0.806553911205074,0.0064
REFERENCES,0.8076109936575053,0.0146
REFERENCES,0.8086680761099366,0.0206 1.0
REFERENCES,0.8097251585623678,0.0206
REFERENCES,0.8107822410147991,0.0136
REFERENCES,0.8118393234672304,0.0070
REFERENCES,0.8128964059196617,0.0082
REFERENCES,0.813953488372093,0.0269
REFERENCES,0.8150105708245243,0.0083
REFERENCES,0.8160676532769556,0.0053
REFERENCES,0.8171247357293869,0.0053
REFERENCES,0.8181818181818182,0.0070
REFERENCES,0.8192389006342494,0.0181
REFERENCES,0.8202959830866807,0.0074
REFERENCES,0.821353065539112,0.0058 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040
REFERENCES,0.8224101479915433,"Figure 20: Spatial correlation of (y −µ(ˆz))/σ(ˆz), averaged across all latent elements of all images
on Kodak. The value with index (i, j) corresponds to the normalized cross-correlation of latents at
spatial location (w, h) and (w + i, h + j). Left column corresponds to Conv-Hyperprior and right
columns corresponds to SwinT-Hyperprior. Each row shows a pair of models trained at the same β,
where the β values from top to bottom are 0.0001, 0.0003, 0.001, 0.003. A consistent observation
across all these models is that SwinT-Hyperprior achieves uniformly smaller correlation than its
convolutional counterpart. As β gets smaller (rate becomes lower), the correlation in both models
increase, with SwinT-Hyperprior increasing much slower than Conv-Hyperprior."
REFERENCES,0.8234672304439746,Published as a conference paper at ICLR 2022
REFERENCES,0.8245243128964059,(a) Conv-Hyperprior
REFERENCES,0.8255813953488372,"100
0
100 100 50 0 50 100"
REFERENCES,0.8266384778012685,0.0000
REFERENCES,0.8276955602536998,0.0001
REFERENCES,0.828752642706131,0.0002
REFERENCES,0.8298097251585623,0.0003
REFERENCES,0.8308668076109936,0.0004
REFERENCES,0.8319238900634249,0.0005
REFERENCES,0.8329809725158562,0.0006
REFERENCES,0.8340380549682875,0.0007
REFERENCES,0.8350951374207188,0.0008
REFERENCES,0.8361522198731501,(b) Conv-ChARM
REFERENCES,0.8372093023255814,"100
0
100 100 50 0 50 100"
REFERENCES,0.8382663847780126,0.0000
REFERENCES,0.8393234672304439,0.0001
REFERENCES,0.8403805496828752,0.0002
REFERENCES,0.8414376321353065,0.0003
REFERENCES,0.8424947145877378,0.0004
REFERENCES,0.8435517970401691,0.0005
REFERENCES,0.8446088794926004,0.0006
REFERENCES,0.8456659619450317,0.0007
REFERENCES,0.846723044397463,0.0008
REFERENCES,0.8477801268498943,(c) Conv-SSF-Res
REFERENCES,0.8488372093023255,"100
0
100 100 50 0 50 100 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8498942917547568,"0.30
(d) Conv-SSF-Flow"
REFERENCES,0.8509513742071881,"100
0
100 100 50 0 50 100 0.000 0.002 0.004 0.006 0.008 0.010"
REFERENCES,0.8520084566596194,(e) SwinT-Hyperprior
REFERENCES,0.8530655391120507,"100
0
100 100 50 0 50 100"
REFERENCES,0.854122621564482,0.0000
REFERENCES,0.8551797040169133,0.0001
REFERENCES,0.8562367864693446,0.0002
REFERENCES,0.857293868921776,0.0003
REFERENCES,0.8583509513742071,0.0004
REFERENCES,0.8594080338266384,0.0005
REFERENCES,0.8604651162790697,0.0006
REFERENCES,0.861522198731501,0.0007
REFERENCES,0.8625792811839323,0.0008
REFERENCES,0.8636363636363636,(f) SwinT-ChARM
REFERENCES,0.864693446088795,"100
0
100 100 50 0 50 100"
REFERENCES,0.8657505285412262,0.0000
REFERENCES,0.8668076109936576,0.0001
REFERENCES,0.8678646934460887,0.0002
REFERENCES,0.86892177589852,0.0003
REFERENCES,0.8699788583509513,0.0004
REFERENCES,0.8710359408033826,0.0005
REFERENCES,0.872093023255814,0.0006
REFERENCES,0.8731501057082452,0.0007
REFERENCES,0.8742071881606766,0.0008
REFERENCES,0.8752642706131079,(g) SwinT-SSF-Res
REFERENCES,0.8763213530655392,"100
0
100 100 50 0 50 100 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8773784355179705,"0.30
(h) SwinT-SSF-Flow"
REFERENCES,0.8784355179704016,"100
0
100 100 50 0 50 100 0.000 0.002 0.004 0.006 0.008 0.010"
REFERENCES,0.879492600422833,"Figure 21: Comparison of effective receptive ﬁeld (ERF) of the composed encoders ha ◦ga. The
ERF is visualized as the absolution gradients of the center pixel in the hyper latent (i.e. dz/dx) with
respect to the input images/videos, speciﬁcally 24 Kodak images cropped to 512 × 512 for image
codecs (the left four models, all with β = 0.003), and 24 randomly selected batches from UVG
video dataset cropped to 768×768 for P-frame codecs (the right four models, all with β = 0.0008)."
REFERENCES,0.8805496828752643,"0
20
40
60
80
Sorted attention head 1 2 3 4 5 6"
REFERENCES,0.8816067653276956,Mean attention distance
REFERENCES,0.8826638477801269,"(a) SwinT-Hyperprior encoder, β = 0.0001"
REFERENCES,0.8837209302325582,"0
20
40
60
80
Sorted attention head 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5"
REFERENCES,0.8847780126849895,Mean attention distance
REFERENCES,0.8858350951374208,"(b) SwinT-Hyperprior decoder, β = 0.0001"
REFERENCES,0.8868921775898521,"0
20
40
60
80
Sorted attention head 1 2 3 4 5 6"
REFERENCES,0.8879492600422833,Mean attention distance
REFERENCES,0.8890063424947146,"(c) SwinT-Hyperprior encoder, β = 0.001"
REFERENCES,0.8900634249471459,"0
20
40
60
80
Sorted attention head 1 2 3 4 5"
REFERENCES,0.8911205073995772,Mean attention distance
REFERENCES,0.8921775898520085,"(d) SwinT-Hyperprior decoder, β = 0.001"
REFERENCES,0.8932346723044398,"Figure 22: Mean attention distance of SwinT-Hyperprior models evaluated on Kodak. It is calculated
as the average relative distance between each query and key weighted by the attention weight for
each query, in each head of each layer. Each vertical color bar in the ﬁgure shows the mean and
1-std for the mean attention distance of one head, dashed black bar separates heads from different
transformer stages (feature resolutions). The order of input to output is from left to right within each
ﬁgure. Lower bitrate models have smaller attention distance."
REFERENCES,0.8942917547568711,Published as a conference paper at ICLR 2022
REFERENCES,0.8953488372093024,"0
50
100
150
200
250
300
Channel index (sorted by BPP) 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007"
REFERENCES,0.8964059196617337,Mean BPP per channel over Kodak
REFERENCES,0.8974630021141649,"Conv-Hyperprior
SwinT-Hyperprior"
REFERENCES,0.8985200845665962,(a) Hyperprior (rate in linear scale)
REFERENCES,0.8995771670190275,"0
50
100
150
200
250
300
Channel index (sorted by BPP) 10
8 10
7 10
6 10
5 10
4 10
3 10
2"
REFERENCES,0.9006342494714588,Mean BPP per channel over Kodak
REFERENCES,0.9016913319238901,"Conv-Hyperprior
SwinT-Hyperprior"
REFERENCES,0.9027484143763214,(b) Hyperprior (rate in log scale)
REFERENCES,0.9038054968287527,"0
50
100
150
200
250
300
Channel index (sorted by BPP) 0.000 0.001 0.002 0.003 0.004 0.005 0.006"
REFERENCES,0.904862579281184,Mean BPP per channel over Kodak
REFERENCES,0.9059196617336153,"Conv-ChARM
SwinT-ChARM"
REFERENCES,0.9069767441860465,(c) ChARM prior (rate in linear scale)
REFERENCES,0.9080338266384778,"0
50
100
150
200
250
300
Channel index (sorted by BPP) 10
8 10
7 10
6 10
5 10
4 10
3 10
2"
REFERENCES,0.9090909090909091,Mean BPP per channel over Kodak
REFERENCES,0.9101479915433404,"Conv-ChARM
SwinT-ChARM"
REFERENCES,0.9112050739957717,(d) ChARM prior (rate in log scale)
REFERENCES,0.912262156448203,"Figure 23: Rate distribution of latent channels. Five lines with the same colormap from bright to
dark corresponds to the model trained under ﬁve β values from small to large (i.e. overall bitrate
from large to small). The cutoff index decreases as the total rate goes down."
REFERENCES,0.9133192389006343,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9143763213530656,SwinT latent channel index (sorted by mean BPP) 0.2 0.4 0.6 0.8
REFERENCES,0.9154334038054969,(a) Hyperprior 0.003
REFERENCES,0.9164904862579282,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9175475687103594,SwinT latent channel index (sorted by mean BPP) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9186046511627907,(b) Hyperprior 0.001
REFERENCES,0.919661733615222,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9207188160676533,SwinT latent channel index (sorted by mean BPP) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9217758985200846,(c) Hyperprior 3e-4
REFERENCES,0.9228329809725159,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9238900634249472,SwinT latent channel index (sorted by mean BPP) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9249471458773785,(d) Hyperprior 1e-4
REFERENCES,0.9260042283298098,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.927061310782241,SwinT latent channel index (sorted by mean BPP) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9281183932346723,(e) Hyperprior 3e-5
REFERENCES,0.9291754756871036,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9302325581395349,SwinT latent channel index (sorted by mean BPP) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9312896405919662,(f) ChARM 0.003
REFERENCES,0.9323467230443975,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9334038054968288,SwinT latent channel index (sorted by mean BPP) 0.2 0.4 0.6 0.8
REFERENCES,0.9344608879492601,(g) ChARM 0.001
REFERENCES,0.9355179704016914,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9365750528541226,SwinT latent channel index (sorted by mean BPP) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9376321353065539,(h) ChAMR 3e-4
REFERENCES,0.9386892177589852,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9397463002114165,SwinT latent channel index (sorted by mean BPP) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9408033826638478,(i) ChARM 1e-4
REFERENCES,0.9418604651162791,"0
50
100
150
200
250
300
Conv latent channel index (sorted by mean BPP) 0 50 100 150 200 250 300"
REFERENCES,0.9429175475687104,SwinT latent channel index (sorted by mean BPP) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9439746300211417,(j) ChARM 3e-5
REFERENCES,0.945031712473573,"Figure 24: Centered Kernel Alignment between 320 SwinT latent channels and 320 Conv latent
channels. Blocks with high similarity correspond to latent channels before the cutoff in Figure 23."
REFERENCES,0.9460887949260042,Published as a conference paper at ICLR 2022 23.0 22.5 22.0 21.5 21.0 20.5 5.0 2.5 0.0 2.5 5.0 7.5 20 15 10 5 0 5 10 20 10 0 10 20 30 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2 1 0 1 2 3 40 30 20 10 0 10 20 7.5 5.0 2.5 0.0 2.5 5.0 7.5 5 0 5 10 15 20 25 40 30 20 10 0 10 20 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 2 1 0 1 2 2 1 0 1 2
REFERENCES,0.9471458773784355,"(a) Hyperprior, β = 0.003 75 50 25 0 25 50 75 40 20 0 20 40 60 60 40 20 0 20 40 60 100 80 60 40 20 0 20 60 40 20 0 20 40 60 40 20 0 20 40 60 1 0 1 2 3 4 5 5 0 5 10 15 20 25 30 250 200 150 100 50 0 50 200 100 0 100 75 50 25 0 25 50 75 100 250 200 150 100 50 0 50 200 100 0 100 250 200 150 100 50 0 50 200 100 0 100 250 200 150 100 50 0 50"
REFERENCES,0.9482029598308668,(b) Hyperprior β = 0.00003 20 15 10 5 0 5 10 5.0 2.5 0.0 2.5 5.0 7.5 21.4 21.2 21.0 20.8 20.6 20.4 20 10 0 10 20 30 1.0 0.5 0.0 0.5 1 0 1 2 3 4 0.5 0.0 0.5 1.0 1.5 2.0 2 1 0 1 2 3 5 0 5 10 15 4 2 0 2 4 20 15 10 5 0 5 10 15 20 10 0 10 20 30 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1 0 1 2 3
REFERENCES,0.9492600422832981,"(c) ChARM, β = 0.003 100 75 50 25 0 25 50 150 100 50 0 50 150 100 50 0 50 100 150 30 20 10 0 10 20 30 150 125 100 75 50 25 0 60 40 20 0 3 2 1 0 1 2 3 1 0 1 2 3 40 20 0 20 40 60 80 200 150 100 50 0 50 200 100 0 100 200 40 20 0 20 40 60 80 10 5 0 5 10 4 2 0 2 8 6 4 2 0 2 4 30 20 10 0 10 20 30"
REFERENCES,0.9503171247357294,(d) ChARM β = 0.00003
REFERENCES,0.9513742071881607,"Figure 25: Identiﬁcation of latent features from SwinT and Conv models with CKA (on 23rd image
in Kodak). For each subﬁgure, top row shows 8 channels of SwinT latents with the smallest mean
CKA values between this channel and all Conv latent channels (before the rate cutoff in Figure 23).
The corresponding image on the bottom shows the Conv latent channel with the largest CKA with
the top SwinT channel. The identiﬁed channels are highly similar, up to sign ﬂip."
REFERENCES,0.952431289640592,Published as a conference paper at ICLR 2022
REFERENCES,0.9534883720930233,"(a)
himanshu-singh-gurjar-iSi02D Qx w-
unsplash"
REFERENCES,0.9545454545454546,(b) matthaeus-hew8vAvvvz4-unsplash
REFERENCES,0.9556025369978859,"Figure 26:
More examples of reconstructions with SwinT-Hyperprior (top row) and Conv-
Hyperprior (bottom row) for the latent masked with checkerboard pattern where half of the latent
dequantized values are replaced with the corresponding prior mean before passing to the decoder
gs. The example image shown in Figure 8b is andy-kelly-0E vhMVqL9g-unsplash from CLIC2021
test, same as the two images shown here."
REFERENCES,0.9566596194503171,Published as a conference paper at ICLR 2022 code
REFERENCES,0.9577167019027484,"c57
c122
c164
c255
c47
c186
c265
c240"
REFERENCES,0.9587737843551797,"mean
scale"
REFERENCES,0.959830866807611,bpp=0.0123
REFERENCES,0.9608879492600423,code bitmap
REFERENCES,0.9619450317124736,"bpp=0.0073
bpp=0.0063
bpp=0.0073
bpp=0.0059
bpp=0.0013
bpp=0.0009
bpp=0.0015"
REFERENCES,0.9630021141649049,"psnr=16.92, bpp=0.0123"
REFERENCES,0.9640591966173362,recon w/ code[i]
REFERENCES,0.9651162790697675,"psnr=20.93, bpp=0.0196
psnr=24.31, bpp=0.0259
psnr=25.27, bpp=0.0332
psnr=26.06, bpp=0.0391
psnr=26.23, bpp=0.0403
psnr=26.33, bpp=0.0412
psnr=26.51, bpp=0.0427"
REFERENCES,0.9661733615221987,bpp=0.0014
REFERENCES,0.96723044397463,recon w/ code[:i+1]
REFERENCES,0.9682875264270613,"bpp=0.0012
bpp=0.0011
bpp=0.0010
bpp=0.0007
bpp=0.0007
bpp=0.0006
bpp=0.0006 40 30 20 10 0 10 20 0 10 20 5 0 5 10 15 10 5 0 5 10 15 10 5 0 5 10 6 4 2 0 2 4 6 3 2 1 0 1 2 5 0 5 10 15 30 20 10 0 10 20 10 5 0 5 10 15 20 5 0 5 6 4 2 0 2 4 6 5.0 2.5 0.0 2.5 5.0 7.5 0.03 0.02 0.01 0.00 0.01 0.02 0.03 0.02 0.01 0.00 0.01 0.01 0.00 0.01 0.02 2 4 6 8 10 12 1 2 3 4 5 6 0.5 1.0 1.5 2.0 2 4 6 2 4 6 8 10 1 2 3 4 0.5 1.0 1.5 2.0 1 2 3 4 5 2 4 6 8 2 4 6 8 2 4 6 8 10 2 4 6 8 2 4 6 8 1 2 3 1 2 3 4 5 6 2 4 6 50 100 150 200 250 0 50 100 150 200 80 100 120 140 160 50 75 100 125 150 175 50 75 100 125 150 175 80 100 120 140 90 100 110 120 130 50 100 150 200 50 100 150 200 250 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250"
REFERENCES,0.9693446088794926,"(a) Conv-Hyperprior, β = 0.003 code"
REFERENCES,0.9704016913319239,"c22
c83
c85
c154
c137
c261
c77
c170"
REFERENCES,0.9714587737843552,"mean
scale"
REFERENCES,0.9725158562367865,bpp=0.0001
REFERENCES,0.9735729386892178,code bitmap
REFERENCES,0.9746300211416491,"bpp=0.0095
bpp=0.0076
bpp=0.0049
bpp=0.0064
bpp=0.0055
bpp=0.0022
bpp=0.0007"
REFERENCES,0.9756871035940803,"psnr=13.00, bpp=0.0001"
REFERENCES,0.9767441860465116,recon w/ code[i]
REFERENCES,0.9778012684989429,"psnr=17.38, bpp=0.0097
psnr=20.89, bpp=0.0173
psnr=23.84, bpp=0.0222
psnr=25.00, bpp=0.0286
psnr=26.03, bpp=0.0341
psnr=26.24, bpp=0.0363
psnr=26.30, bpp=0.0371"
REFERENCES,0.9788583509513742,bpp=0.0013
REFERENCES,0.9799154334038055,recon w/ code[:i+1]
REFERENCES,0.9809725158562368,"bpp=0.0013
bpp=0.0012
bpp=0.0010
bpp=0.0009
bpp=0.0008
bpp=0.0007
bpp=0.0007 23 22 21 20 10 0 10 20 30 20 15 10 5 0 5 10 5.0 2.5 0.0 2.5 5.0 7.5 20 10 0 10 20 10 0 10 2 0 2 4 2 1 0 1 2 3 23.0 22.5 22.0 21.5 21.0 20.5 20 10 0 10 20 30 20 15 10 5 0 5 10 5.0 2.5 0.0 2.5 5.0 7.5 10 5 0 5 10 10 5 0 5 10 15 0.0 0.2 0.4 0.6 0.8 0.05 0.00 0.05 0.10 0.2 0.4 0.6 0.8 1.0 1 2 3 4 0.5 1.0 1.5 2.0 2.5 3.0 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 4 6 2 4 6 8 0.5 1.0 1.5 2.0 2.5 0.25 0.50 0.75 1.00 1.25 1.50 1 2 3 2 4 6 8 10 12 2.5 5.0 7.5 10.0 12.5 15.0 2 4 6 8 2 4 6 2 4 6 8 2 4 6 8 2 4 6 8 100 110 120 130 140 50 100 150 200 250 0 50 100 150 200 250 50 100 150 200 50 100 150 200 250 50 100 150 200 250 50 100 150 200 50 75 100 125 150 175 200 100 110 120 130 140 50 100 150 200 250 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250"
REFERENCES,0.9820295983086681,(b) SwinT-Hyperprior β = 0.003
REFERENCES,0.9830866807610994,"Figure 27: Channel-wise progressive decoding. The 3 leading latent channels following an impor-
tance order are responsible for grayscale, and two color components of the reconstruction (second
to the last row). For SwinT models, we ﬁnd a coarse-scale latent channel (the left most channel)
which costs negligible bits but is important for reconstruction (without it, the 7 other leading chan-
nels can only decode to a reconstruction with PSNR around 16dB). For each subﬁgure, from top
to bottom: latent, prior mean, prior std, bitmap, reconstruction with current channel, reconstruction
with leading channels (up to current channel)."
REFERENCES,0.9841437632135307,Published as a conference paper at ICLR 2022
REFERENCES,0.985200845665962,"0.5
1.0
1.5
2.0
2.5
Rate (bits per pixel) 32 34 36 38 40 42 44"
REFERENCES,0.9862579281183932,PSNR (RGB) Kodak
REFERENCES,0.9873150105708245,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM
Conv-Hyperprior
VTM-12.1
BPG444
Cheng et al. 2020
Xie et al. 2021
Ma et al. 2021
Guo et al. 2021
Balle et al. 2018
Minnen et al. 2020
Lee et al. 2019
Wu et al. 2020
Minnen et al. 2018
(no context)
Minnen et al. 2018
(with context)"
REFERENCES,0.9883720930232558,"Figure 28: Rate-distortion performance on Kodak, comparing with existing works (Cheng et al.,
2020; Xie et al., 2021; Ma et al., 2021; Guo et al., 2021; Ball´e et al., 2018; Minnen & Singh, 2020;
Lee et al., 2019; Wu et al., 2020; Minnen et al., 2018). Dashed line-style indicates the use of spatial
autoregressive model (block-wise or pixel-wise) as prior model."
REFERENCES,0.9894291754756871,Published as a conference paper at ICLR 2022
REFERENCES,0.9904862579281184,"30
32
34
36
38
40
42
44
PSNR (RGB) 10 0 10 20 30"
REFERENCES,0.9915433403805497,Rate saving (%) over BPG444 (larger is better) Kodak
REFERENCES,0.992600422832981,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM
Conv-Hyperprior
VTM-12.1
BPG444
Cheng et al. 2020
Xie et al. 2021
Ma et al. 2021
Guo et al. 2021
Balle et al. 2018
Minnen et al. 2020
Lee et al. 2019
Wu et al. 2020
Minnen et al. 2018
(no context)
Minnen et al. 2018
(with context)"
REFERENCES,0.9936575052854123,"Figure 29: Percentage of rate-saving over BPG444 evaluated on Kodak (extended version of Fig-
ure 3b), comparing with existing works (Cheng et al., 2020; Xie et al., 2021; Ma et al., 2021; Guo
et al., 2021; Ball´e et al., 2018; Minnen & Singh, 2020; Lee et al., 2019; Wu et al., 2020; Minnen
et al., 2018). Dashed line-style indicates the use of spatial autoregressive model (block-wise or
pixel-wise) as prior model."
REFERENCES,0.9947145877378436,Published as a conference paper at ICLR 2022
REFERENCES,0.9957716701902748,"30
32
34
36
38
40
42
44
PSNR (RGB) 50 40 30 20 10 0 10"
REFERENCES,0.9968287526427061,Rate saving (%) over VTM-12.1 (larger is better) Kodak
REFERENCES,0.9978858350951374,"SwinT-ChARM
SwinT-Hyperprior
Conv-ChARM
Conv-Hyperprior
VTM-12.1
BPG444
Cheng et al. 2020
Xie et al. 2021
Ma et al. 2021
Guo et al. 2021
Balle et al. 2018
Minnen et al. 2020
Lee et al. 2019
Wu et al. 2020
Minnen et al. 2018
(no context)
Minnen et al. 2018
(with context)"
REFERENCES,0.9989429175475687,"Figure 30: Percentage of rate-saving over VTM-12.1 evaluated on Kodak (extended version of Fig-
ure 3b), comparing with existing works (Cheng et al., 2020; Xie et al., 2021; Ma et al., 2021; Guo
et al., 2021; Ball´e et al., 2018; Minnen & Singh, 2020; Lee et al., 2019; Wu et al., 2020; Minnen
et al., 2018). Dashed line-style indicates the use of spatial autoregressive model (block-wise or
pixel-wise) as prior model."
