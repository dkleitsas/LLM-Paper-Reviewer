Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010869565217391304,"Predictive modeling often faces challenges due to limited data availability and
1"
ABSTRACT,0.002173913043478261,"quality, especially in domains where collected features are weakly correlated
2"
ABSTRACT,0.003260869565217391,"with outcomes and where additional data collection is constrained by ethical
3"
ABSTRACT,0.004347826086956522,"or practical difficulties. Traditional machine learning (ML) models struggle to
4"
ABSTRACT,0.005434782608695652,"incorporate unobserved yet critical factors. We propose a framework that leverages
5"
ABSTRACT,0.006521739130434782,"large language models (LLMs) to augment observed features with latent features,
6"
ABSTRACT,0.007608695652173913,"enhancing the predictive power of ML models in downstream tasks. Our novel
7"
ABSTRACT,0.008695652173913044,"approach transforms the latent feature mining task to a text-to-text propositional
8"
ABSTRACT,0.009782608695652175,"reasoning task. We validate our framework with a case study in the criminal justice
9"
ABSTRACT,0.010869565217391304,"system, a domain characterized by limited and ethically challenging data collection.
10"
ABSTRACT,0.011956521739130435,"Our results show that inferred latent features align well with ground truth labels and
11"
ABSTRACT,0.013043478260869565,"significantly enhance the downstream classifier. Our framework is generalizable
12"
ABSTRACT,0.014130434782608696,"across various domains with minimal domain-specific customization, ensuring easy
13"
ABSTRACT,0.015217391304347827,"transfer to other areas facing similar challenges in data availability.
14"
INTRODUCTION,0.016304347826086956,"1
Introduction
15"
INTRODUCTION,0.017391304347826087,"In numerous application domains, predicting individual outcomes and optimizing resource planning
16"
INTRODUCTION,0.01847826086956522,"are critical but often limited by gaps in data availability and quality. Despite the popular belief that
17"
INTRODUCTION,0.01956521739130435,"we operate in a “large data regime,” many decisions, especially those impacting human lives, have to
18"
INTRODUCTION,0.020652173913043477,"be made based on small amounts of data with limited features, such as in criminal justice, healthcare,
19"
INTRODUCTION,0.021739130434782608,"and social services (Lu et al., 2021; Yuan et al., 2023). This poses both technical limitations and
20"
INTRODUCTION,0.02282608695652174,"ethical concerns. Traditional ML models, while powerful, are limited by the availability of collected
21"
INTRODUCTION,0.02391304347826087,"(observed) data features. This limitation is especially prominent when it comes to incorporating
22"
INTRODUCTION,0.025,"unstructured data or inferring nuanced relationships between observed features and the outcomes. In
23"
INTRODUCTION,0.02608695652173913,"this paper, we explore how domain-informed language models can help identify latent (unobserved)
24"
INTRODUCTION,0.02717391304347826,"features and improve prediction accuracy for downstream tasks.
25"
INTRODUCTION,0.02826086956521739,"We illustrate our motivation with an example from the criminal justice setting. Predicting an
26"
INTRODUCTION,0.029347826086956522,"individual’s in-program revocation probability (chance of committing a new crime during probation)
27"
INTRODUCTION,0.030434782608695653,"is critical for determining their eligibility for incarceration-diversion programs and for planning
28"
INTRODUCTION,0.03152173913043478,"resources like staffing ratios (Rotter and Barber-Rioja, 2015; Li et al., 2024). Typically, the data
29"
INTRODUCTION,0.03260869565217391,"collected includes only a limited set of features, e.g., basic demographic and criminal history
30"
INTRODUCTION,0.03369565217391304,"information. Crucial factors such as socio-economic status, community support availability, or
31"
INTRODUCTION,0.034782608695652174,"psychological profiles, which significantly influence outcomes, are often missing from these datasets.
32"
INTRODUCTION,0.035869565217391305,"Collecting such sensitive information can be invasive and raises ethical concerns. Additionally, the
33"
INTRODUCTION,0.03695652173913044,"process of gathering these data can be logistically challenging and resource-intensive. Human case
34"
INTRODUCTION,0.03804347826086957,"managers in these settings often have the advantage of drawing on their professional experience
35"
INTRODUCTION,0.0391304347826087,"and human intuition to infer these critical but unrecorded details from observed data. In contrast,
36"
INTRODUCTION,0.04021739130434782,"traditional ML models cannot reason beyond the explicit data provided, leading to predictions based
37"
INTRODUCTION,0.041304347826086954,"on incomplete information. This limitation not only undermines the accuracy of the models but also
38"
INTRODUCTION,0.042391304347826085,"poses concerns regarding the fairness of decisions derived from such data. Moreover, ML models are
39"
INTRODUCTION,0.043478260869565216,"not designed to handle unstructured data like case notes, which may contain contextual insights to
40"
INTRODUCTION,0.04456521739130435,"improve prediction accuracy.
41"
INTRODUCTION,0.04565217391304348,"Recent advancements in large language models (LLMs) offer a promising avenue to bridge these data
42"
INTRODUCTION,0.04673913043478261,"gaps (Brown et al., 2020; Ouyang et al., 2022; Achiam et al., 2023). LLMs are capable of processing
43"
INTRODUCTION,0.04782608695652174,"and generating information in a way that mimics human reasoning, allowing for the inference of latent
44"
INTRODUCTION,0.04891304347826087,"features that are not directly observable but are critical for accurate predictions and decision-making.
45"
INTRODUCTION,0.05,"They can also analyze both structured and unstructured data to offer a holistic view of the underlying
46"
INTRODUCTION,0.051086956521739134,"factors influencing individual outcomes.
47"
INTRODUCTION,0.05217391304347826,"Our proposed framework leverages LLMs to augment observed features collected in given datasets
48"
INTRODUCTION,0.05326086956521739,"with latent features, enhancing the predictive power of ML models for downstream tasks such as
49"
INTRODUCTION,0.05434782608695652,"classifications. Unlike conventional data augmentation approaches to increase the sample size, we
50"
INTRODUCTION,0.05543478260869565,"train LLMs to infer underlying socio-economic conditions, treatment needs, and other critical but
51"
INTRODUCTION,0.05652173913043478,"often unrecorded characteristics from collected features. This augments the feature space X to
52"
INTRODUCTION,0.057608695652173914,"improve predictions. Additionally, our framework enables generating more complete and realistic
53"
INTRODUCTION,0.058695652173913045,"synthetic data points via learned correlations between observed and unobserved features for simulation
54"
INTRODUCTION,0.059782608695652176,"and counterfactual policy analysis. We summarize our main contributions as follows.
55"
WE INTRODUCE A NOVEL APPROACH TO FORMULATE LATENT FEATURE MINING AS TEXT-TO-TEXT PROPOSITIONAL,0.06086956521739131,"1. We introduce a novel approach to formulate latent feature mining as text-to-text propositional
56"
WE INTRODUCE A NOVEL APPROACH TO FORMULATE LATENT FEATURE MINING AS TEXT-TO-TEXT PROPOSITIONAL,0.06195652173913044,"logical reasoning. This approach effectively infers latent features from observed features, offering
57"
WE INTRODUCE A NOVEL APPROACH TO FORMULATE LATENT FEATURE MINING AS TEXT-TO-TEXT PROPOSITIONAL,0.06304347826086956,"significantly improved accuracy and interpretability compared to alternative approaches.
58"
WE INTRODUCE A NOVEL APPROACH TO FORMULATE LATENT FEATURE MINING AS TEXT-TO-TEXT PROPOSITIONAL,0.0641304347826087,"2. We develop a four-step framework to implement our approach, which is generalizable with
59"
WE INTRODUCE A NOVEL APPROACH TO FORMULATE LATENT FEATURE MINING AS TEXT-TO-TEXT PROPOSITIONAL,0.06521739130434782,"minimal domain-specific customization and has remarkably low human-annotated training data
60"
WE INTRODUCE A NOVEL APPROACH TO FORMULATE LATENT FEATURE MINING AS TEXT-TO-TEXT PROPOSITIONAL,0.06630434782608696,"requirements. This framework expands data utility by enhancing downstream predictions without
61"
WE INTRODUCE A NOVEL APPROACH TO FORMULATE LATENT FEATURE MINING AS TEXT-TO-TEXT PROPOSITIONAL,0.06739130434782609,"additional invasive or forbidden data collection.
62"
WE EMPIRICALLY VALIDATE OUR FRAMEWORK IN THE CRIMINAL JUSTICE SETTING TO ADDRESS WEAK OBSERVED,0.06847826086956521,"3. We empirically validate our framework in the criminal justice setting to address weak observed
63"
WE EMPIRICALLY VALIDATE OUR FRAMEWORK IN THE CRIMINAL JUSTICE SETTING TO ADDRESS WEAK OBSERVED,0.06956521739130435,"features and unbalanced datasets. Designed as a plug-and-play solution, we demonstrate our
64"
WE EMPIRICALLY VALIDATE OUR FRAMEWORK IN THE CRIMINAL JUSTICE SETTING TO ADDRESS WEAK OBSERVED,0.07065217391304347,"framework’s adaptability through two different prediction tasks, making it valuable for various
65"
WE EMPIRICALLY VALIDATE OUR FRAMEWORK IN THE CRIMINAL JUSTICE SETTING TO ADDRESS WEAK OBSERVED,0.07173913043478261,"applications with similar challenges.
66"
BACKGROUND AND RELATED WORKS,0.07282608695652174,"2
Background and Related Works
67"
BACKGROUND AND RELATED WORKS,0.07391304347826087,"Data Augmentation and Latent Feature Extraction. Data augmentation is a technique commonly
68"
BACKGROUND AND RELATED WORKS,0.075,"used in AI (Van Dyk and Meng, 2001). Generative models, such as Generative Adversarial Networks
69"
BACKGROUND AND RELATED WORKS,0.07608695652173914,"(GANs) and Variational Autoencoders (VAEs), learn data patterns and generate synthetic data to
70"
BACKGROUND AND RELATED WORKS,0.07717391304347826,"augment training sample size (Goodfellow et al., 2014; Kingma and Welling, 2013). Unlike these
71"
BACKGROUND AND RELATED WORKS,0.0782608695652174,"approaches, our framework leverages LLMs to augment the features of different individuals. Trained
72"
BACKGROUND AND RELATED WORKS,0.07934782608695652,"on crowd-sourced data rich in human behavior and societal context, LLMs have the potential to
73"
BACKGROUND AND RELATED WORKS,0.08043478260869565,"enhance feature spaces for social computing and operations improvement.
74"
BACKGROUND AND RELATED WORKS,0.08152173913043478,"Latent features are hidden characteristics in a dataset that are not directly observed but can be
75"
BACKGROUND AND RELATED WORKS,0.08260869565217391,"inferred from available data. Incorporating meaningful latent features can enhance the performance
76"
BACKGROUND AND RELATED WORKS,0.08369565217391305,"of downstream applications (Zhai and Peng, 2016; Jiang et al., 2023). Two common approaches to
77"
BACKGROUND AND RELATED WORKS,0.08478260869565217,"infer latent features are human annotation and machine learning models. Human annotation, while
78"
BACKGROUND AND RELATED WORKS,0.08586956521739131,"reliable, is often expensive and time-consuming. It requires significant effort and resources, making
79"
BACKGROUND AND RELATED WORKS,0.08695652173913043,"it impractical for large-scale tasks. Machine learning methods like Expectation-Maximization (EM)
80"
BACKGROUND AND RELATED WORKS,0.08804347826086957,"and VAEs offer alternative techniques to infer latent features from observed data. EM algorithms
81"
BACKGROUND AND RELATED WORKS,0.0891304347826087,"estimate latent variable assignments and update model parameters to maximize data likelihood, but
82"
BACKGROUND AND RELATED WORKS,0.09021739130434783,"their results can be hard to interpret and require strong parametric assumptions. Similarly, VAEs use
83"
BACKGROUND AND RELATED WORKS,0.09130434782608696,"probabilistic approaches to describe data distribution with latent variables, but the learned mappings
84"
BACKGROUND AND RELATED WORKS,0.09239130434782608,"can also be difficult to interpret.
85"
BACKGROUND AND RELATED WORKS,0.09347826086956522,"Synthetic Data for Training. Fine-tuning is a promising approach for LLMs to reduce hallucinations
86"
BACKGROUND AND RELATED WORKS,0.09456521739130434,"and align outputs with real-world data and human preferences (Tonmoy et al., 2024; Qiao et al.,
87"
BACKGROUND AND RELATED WORKS,0.09565217391304348,"2022; Hu et al., 2021). Synthetic data has proven to be an effective, low-cost alternative to real data
88"
BACKGROUND AND RELATED WORKS,0.0967391304347826,"to improve the LLMs’ reasoning performance across various domains (Liu et al., 2024). Studies
89"
BACKGROUND AND RELATED WORKS,0.09782608695652174,"by (Zelikman et al., 2022), (Wang et al., 2022) demonstrate that synthetic data improves model
90"
BACKGROUND AND RELATED WORKS,0.09891304347826087,"generalization and robustness. Our approach also uses synthetic data to augment training during
91"
BACKGROUND AND RELATED WORKS,0.1,"fine-tuning. Unlike existing work that directly mimics observed features, we are one of the first
92"
BACKGROUND AND RELATED WORKS,0.10108695652173913,"to formulate the generation of synthetic latent features as a reasoning task. Our approach employs
93"
BACKGROUND AND RELATED WORKS,0.10217391304347827,"few-shot prompting to create synthetic data that infers these latent features, followed by fine-tuning
94"
BACKGROUND AND RELATED WORKS,0.10326086956521739,"to enhance model accuracy and reduce hallucinations. This technique falls under the self-instruction
95"
BACKGROUND AND RELATED WORKS,0.10434782608695652,"paradigm, where models iteratively learn from augmented data.
96"
BACKGROUND AND RELATED WORKS,0.10543478260869565,"Note that we distinguish between augmenting the feature space and augmenting training data. Our
97"
BACKGROUND AND RELATED WORKS,0.10652173913043478,"primary goal is to augment the feature space by inferring and adding latent features to the observed
98"
BACKGROUND AND RELATED WORKS,0.10760869565217392,"data to improve downstream predictions. As part of the steps in our framework to achieve this
99"
BACKGROUND AND RELATED WORKS,0.10869565217391304,"goal, we augment training data for LLM fine-tuning with synthetic samples to improve the model’s
100"
BACKGROUND AND RELATED WORKS,0.10978260869565218,"reasoning capabilities.
101"
BACKGROUND AND RELATED WORKS,0.1108695652173913,"Incarceration-diversion Programs and Data Description. This work conducts case studies on
102"
BACKGROUND AND RELATED WORKS,0.11195652173913044,"incarceration-diversion programs, which aim to support individuals who have committed minor
103"
BACKGROUND AND RELATED WORKS,0.11304347826086956,"offenses by providing community-based services to improve societal reintegration and reduce re-
104"
BACKGROUND AND RELATED WORKS,0.11413043478260869,"cidivism. Eligible individuals were diverted from traditional incarceration to such programs after
105"
BACKGROUND AND RELATED WORKS,0.11521739130434783,"risk assessment and screening. Case managers determined specific program requirements, such as
106"
BACKGROUND AND RELATED WORKS,0.11630434782608695,"substance use treatment and cognitive-behavioral therapy. There are four types of program outcomes:
107"
BACKGROUND AND RELATED WORKS,0.11739130434782609,"Completed (successfully completed the program), Revoked (committed new crimes while in the
108"
BACKGROUND AND RELATED WORKS,0.11847826086956521,"program), Not Completed (unable to finish for various reasons), and Other (unrecorded reasons).
109"
BACKGROUND AND RELATED WORKS,0.11956521739130435,"We obtained de-identified data from our community partner for a state-wide incarceration-diversion
110"
BACKGROUND AND RELATED WORKS,0.12065217391304348,"program in Illinois. The consolidated dataset includes records of adult participants admitted to the
111"
BACKGROUND AND RELATED WORKS,0.12173913043478261,"program. The collected data features include timestamps such as the arrival and termination dates to
112"
BACKGROUND AND RELATED WORKS,0.12282608695652174,"the program, program outcomes, and individual features such as the race, gender, education, county,
113"
BACKGROUND AND RELATED WORKS,0.12391304347826088,"marriage status, housing, risk assessment scores, prior crime history, and sources of referral (e.g.,
114"
BACKGROUND AND RELATED WORKS,0.125,"from probation officer or from the court). See Appendix F for summary statistics.
115"
THE PROBLEM SETTING,0.12608695652173912,"3
The Problem Setting
116"
THE PROBLEM SETTING,0.12717391304347825,"In this section we formally describe our problem setting that leverages latent features to enhance
117"
THE PROBLEM SETTING,0.1282608695652174,"downstream tasks. The downstream task we focus on is a multi-class classification problem, but the
118"
THE PROBLEM SETTING,0.12934782608695652,"framework can easily extend to other downstream prediction tasks such as regression problems.
119"
THE PROBLEM SETTING,0.13043478260869565,"In a standard multi-class classification problem setting, suppose we have a dataset D
=
120"
THE PROBLEM SETTING,0.13152173913043477,"(x1, y1), (x2, y2), . . . , (xn, yn), where xi is a d-dimensional vector representing the input features
121"
THE PROBLEM SETTING,0.13260869565217392,"X ∈X and yi ∈Y = {1, 2, . . . , C} denotes the corresponding class label Y for individual
122"
THE PROBLEM SETTING,0.13369565217391305,"i = 1, . . . , n. The goal is to learn a classifier f : X →Y that accurately predicts the class labels.
123"
THE PROBLEM SETTING,0.13478260869565217,"Consider the following scenarios in which f struggles to capture the relationship between X and Y :
124"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.1358695652173913,"1. The size of the training dataset is small relative to the complexity of the classification task or the
125"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.13695652173913042,"dimensionality of the feature space;
126"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.13804347826086957,"2. When the input features X are weakly correlated with class labels Y , the input features may not
127"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.1391304347826087,"provide discriminating information to accurately predict the corresponding class labels.
128"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.14021739130434782,"To address these challenges, we could use additional informative features to enhance the classifier’s
129"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.14130434782608695,"ability to capture the relationship between X and Y . Latent features can serve such a purpose.
130"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.1423913043478261,"Definition of Latent Features.
Latent features, denoted as Z, represent underlying attributes that are
not directly observed within the dataset but are correlated with both the
observed features X and the class labels Y . We use a function g with
Z = g(X) to denote the correlations between the latent features and the
observed features X. As shown in figure 3, latent features Z are correlated
with X and Y . One can learn the latent features from the original features
X and augment the features f(X, Z) to learn the classifier Y ."
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.14347826086956522,"In typical ML settings, latent features primarily reduce the dimensionality of the feature space.
131"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.14456521739130435,"Beyond this, latent features can capture discriminative information not explicitly present in the original
132"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.14565217391304347,"features. Our approach focuses on this latter benefit, extracting informative latent representations
133"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.14673913043478262,"to help classifiers better differentiate between classes. Essentially, Z acts as ensemble features
134"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.14782608695652175,"derived from the original features X, capturing complex patterns that individual features might miss,
135"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.14891304347826087,"especially when X is weakly correlated with the outcome Y .
136"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.15,"While this approach seems beneficial intuitively, it is important to note that adding more features is
137"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.15108695652173912,"not always helpful if the extracted features are not meaningful and introduce noise. In the following
138"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.15217391304347827,"lemma, we show in a simple logistic regression setting that while adding features can reduce in-sample
139"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.1532608695652174,"loss, it does not always reduce out-of-sample loss if the added features are not informative. We use
140"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.15434782608695652,"the log-loss (the cross-entropy loss) of the logistics regression for binary outcome Y ∈{0, 1}. We
141"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.15543478260869564,"denote the optimal coefficients that minimize the in-sample log-loss function as β∗for the original
142"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.1565217391304348,"features and ˜β∗for the augmented features.
143"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.15760869565217392,"Lemma 1. The in-sample log-loss always follows Lin( ˜D, ˜β∗) ≤Lin(D, β∗). When the added
144"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.15869565217391304,"features are non-informative, there exist instances such that the out-of-sample log-loss Lout( ˜D, ˜β∗) >
145"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.15978260869565217,"Lout(D, β∗).
146"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.1608695652173913,"The results in the lemma can be generalized to multi-class labels. Since augmenting the feature space
147"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.16195652173913044,"is not necessarily beneficial unless the added features are meaningful, a major part of our case study
148"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.16304347826086957,"is to empirically test whether the extracted features from our framework indeed improve downstream
149"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.1641304347826087,"prediction. If the added features significantly enhance downstream prediction accuracy, this provides
150"
THE SIZE OF THE TRAINING DATASET IS SMALL RELATIVE TO THE COMPLEXITY OF THE CLASSIFICATION TASK OR THE,0.16521739130434782,"strong evidence that the inferred latent features are meaningful.
151"
LATENT FEATURE MINING WITH LLMS,0.16630434782608697,"4
Latent Feature Mining with LLMs
152"
LATENT FEATURE MINING WITH LLMS,0.1673913043478261,"To overcome the limitations of existing approaches, we propose a new approach to efficiently and
153"
LATENT FEATURE MINING WITH LLMS,0.16847826086956522,"accurately extract latent features and augment observed features to enhance the prediction accuracy.
154"
LATENT FEATURE MINING WITH LLMS,0.16956521739130434,"At a high level, our approach transform the latent feature mining as a text-to-text propositional
155"
LATENT FEATURE MINING WITH LLMS,0.17065217391304346,"reasoning task, i.e., infer the relationship Z = g(X) through logical reasoning with natural language.
156"
LATENT FEATURE MINING WITH LLMS,0.17173913043478262,"Following the framework established in previous work (Zhang et al., 2022), we denote the predicates
157"
LATENT FEATURE MINING WITH LLMS,0.17282608695652174,"related to the observed features as P1, P2, . . . , Pm. Consider a propositional theory S that contains
158"
LATENT FEATURE MINING WITH LLMS,0.17391304347826086,"rules that connect P’s to the latent feature Z. We say Z can be deduced from S if the logic implication
159"
LATENT FEATURE MINING WITH LLMS,0.175,"(P1 ∧P2 ∧. . . ∧Pm) →Z is covered in S. For potentially complicated logical connections between
160"
LATENT FEATURE MINING WITH LLMS,0.17608695652173914,"P’s and Z, we also introduce intermediate predicates O’s and formulate a logical chain (a sequence
161"
LATENT FEATURE MINING WITH LLMS,0.17717391304347826,"of logical implications) that connects X to the latent features Z as follows:
162"
LATENT FEATURE MINING WITH LLMS,0.1782608695652174,"X →(P1 ∧P2 ∧. . . ∧Pm) →(O1 ∧O2 ∧. . . ∧Oℓ) →Z.
(1)"
LATENT FEATURE MINING WITH LLMS,0.1793478260869565,"Our approach formulates this logical chain as a multi-stage Chain of Thoughts (CoT) prompt template,
163"
LATENT FEATURE MINING WITH LLMS,0.18043478260869567,"and then guide LLMs to infer Z from X using the prompt template. Specifically, we first extract
164"
LATENT FEATURE MINING WITH LLMS,0.1815217391304348,"predicates P’s from X. Then we infer intermediate predicates with a rule (P1 ∧P2 ∧. . .∧Pm) →Ol
165"
LATENT FEATURE MINING WITH LLMS,0.1826086956521739,"for l = 1, . . . , ℓ−1, and forward the intermediate predicates into the next stage to infer Ol+1. Finally,
166"
LATENT FEATURE MINING WITH LLMS,0.18369565217391304,"we infer latent features with (O1 ∧O2 ∧. . .∧Oℓ) →Z. With the formulated multi-stage CoT prompt
167"
LATENT FEATURE MINING WITH LLMS,0.18478260869565216,"template, we generate synthetic data to fine-tune LLMs to enhance the logical reasoning ability of
168"
LATENT FEATURE MINING WITH LLMS,0.1858695652173913,"LLMs in self-instruct fashion (Wang et al., 2022), and ensure that the generate text is aligned with
169"
LATENT FEATURE MINING WITH LLMS,0.18695652173913044,"each step of our desired “chain of reasoning” format.
170"
LATENT FEATURE MINING WITH LLMS,0.18804347826086956,"Age:    28
Race:  White"
LATENT FEATURE MINING WITH LLMS,0.1891304347826087,Gender:   Male
LATENT FEATURE MINING WITH LLMS,0.19021739130434784,Employment:  Part Time
LATENT FEATURE MINING WITH LLMS,0.19130434782608696,Education:  Less than 10th grade
LATENT FEATURE MINING WITH LLMS,0.1923913043478261,Admitting offense:  Property Offense
LATENT FEATURE MINING WITH LLMS,0.1934782608695652,Primary Drug:  Marijuana
LATENT FEATURE MINING WITH LLMS,0.19456521739130433,Housing Level:  Rent
LATENT FEATURE MINING WITH LLMS,0.1956521739130435,Living Area:  High Crime area
LATENT FEATURE MINING WITH LLMS,0.1967391304347826,Assessed Risk Level:  High Risk
LATENT FEATURE MINING WITH LLMS,0.19782608695652174,Socioeconomic Status: Low
LATENT FEATURE MINING WITH LLMS,0.19891304347826086,Challenges:
LATENT FEATURE MINING WITH LLMS,0.2,Program Requirements:
SUBSTANCE ABUSE ISSUE,0.20108695652173914,2. Substance Abuse Issue
FINANCIAL DIFFICULTY,0.20217391304347826,1. Financial Difficulty
FINANCIAL DIFFICULTY,0.20326086956521738,"Social Relationship: 
1. Education
Lack of long-term support
2. Substance Abuse Treatment"
FINANCIAL DIFFICULTY,0.20434782608695654,Marriage:   Single
COMMUNITY SERVICE,0.20543478260869566,3. Community Service
COMMUNITY SERVICE,0.20652173913043478,Figure 1: Example of latent feature mining through chain of reasoning
COMMUNITY SERVICE,0.2076086956521739,"We use a hypothetical example from our case study setting to illustrate the formulation of the logic
171"
COMMUNITY SERVICE,0.20869565217391303,"chain. The blue (leftmost) box in Figure 1 shows the observed feature X for one individual. Examples
172"
COMMUNITY SERVICE,0.20978260869565218,"for the predicates P’s formulated from X could be:
173"
COMMUNITY SERVICE,0.2108695652173913,"P1 :“the client has part-time job”, P2 : "" the client hasn’t complete high school"",
174"
COMMUNITY SERVICE,0.21195652173913043,"P3 :“the client is single”, P4 : ""the client has drug issue"", P5 :"" the client lives in
175"
COMMUNITY SERVICE,0.21304347826086956,"high crime area"", P6 : "" the client is assessed with high risk"" ...
176"
COMMUNITY SERVICE,0.2141304347826087,"To infer the latent feature Z – in this example, the required programs to attend during probation – we
177"
COMMUNITY SERVICE,0.21521739130434783,"go through a multi-stage reasoning to infer the intermediate predicates O’s; see the white (middle)
178"
COMMUNITY SERVICE,0.21630434782608696,"boxes in Figure 1. One example logic that connects P’s to O’s could be:
179"
COMMUNITY SERVICE,0.21739130434782608,"P1 = ""The client has unstable employment""
180"
COMMUNITY SERVICE,0.2184782608695652,"P2 = ""The highest education level of client is less than 10th grade""
181"
COMMUNITY SERVICE,0.21956521739130436,"O1 = ""The client has low socioeconomic status""
182"
COMMUNITY SERVICE,0.22065217391304348,"If (P1 ∧P2 →O1) ∈S, then O1 is True.
183"
COMMUNITY SERVICE,0.2217391304347826,"Finally, with P’s and O’s, we can connect X with Z though the logic chains. One example of the
184"
COMMUNITY SERVICE,0.22282608695652173,"logical chain is as follows:
185"
COMMUNITY SERVICE,0.22391304347826088,"“The client is grappling with unstable employment and a relatively low educational
186"
COMMUNITY SERVICE,0.225,"level, factors that likely contribute to a low socioeconomic status. Additionally,
187"
COMMUNITY SERVICE,0.22608695652173913,"being single, struggling with drug issues, and residing in a high-crime area further
188"
COMMUNITY SERVICE,0.22717391304347825,"exacerbate the lack of positive social support. Given these circumstances, education
189"
COMMUNITY SERVICE,0.22826086956521738,"could serve as a valuable intervention. Community service can be particularly
190"
COMMUNITY SERVICE,0.22934782608695653,"beneficial for someone who is single and may lack a broad support network.
191"
COMMUNITY SERVICE,0.23043478260869565,"Substance abuse treatment is crucial for individuals from lower socioeconomic
192"
COMMUNITY SERVICE,0.23152173913043478,"backgrounds to aid in recovery from substance abuse. Hence we can choose
193"
COMMUNITY SERVICE,0.2326086956521739,"education, substance abuse treatment, community service for this client.”
194"
COMMUNITY SERVICE,0.23369565217391305,"Here, “unstable employment and a relatively low educational level” and “being single, struggling
195"
COMMUNITY SERVICE,0.23478260869565218,"with drug issues, and residing in a high-crime area” are P’s extracted from the features X, while
196"
COMMUNITY SERVICE,0.2358695652173913,"“a low socioeconomic status” and “lack of positive social support” are O’s. Finally, the rationales
197"
COMMUNITY SERVICE,0.23695652173913043,"“education could serve as a valuable intervention . . . recovery from substance abuse. Hence we
198"
COMMUNITY SERVICE,0.23804347826086958,"can choose education, substance abuse treatment, community service for this client connect the
199"
COMMUNITY SERVICE,0.2391304347826087,"intermediate predicates to the latent variables Z (program requirements) we want to infer, i.e.,
200"
COMMUNITY SERVICE,0.24021739130434783,"Z1=‘education’, Z2=‘substance abuse treatment’, Z3=‘community service’.
201"
COMMUNITY SERVICE,0.24130434782608695,Observed Features
COMMUNITY SERVICE,0.24239130434782608,Latent Features
COMMUNITY SERVICE,0.24347826086956523,Step 1: Formulate Correlation
COMMUNITY SERVICE,0.24456521739130435,rationales
COMMUNITY SERVICE,0.24565217391304348,correlated
COMMUNITY SERVICE,0.2467391304347826,Step 2: Augment Synthetic Data
COMMUNITY SERVICE,0.24782608695652175,generate
COMMUNITY SERVICE,0.24891304347826088,Self-Instructed
COMMUNITY SERVICE,0.25,CoT Data Human
COMMUNITY SERVICE,0.2510869565217391,Step 3. Finetune LLMs LLMs
COMMUNITY SERVICE,0.25217391304347825,verify
COMMUNITY SERVICE,0.2532608695652174,"Infer Latent Feature
from Observed Feature
Step 4:"
COMMUNITY SERVICE,0.2543478260869565,Figure 2: Overview of latent feature inference framework.
COMMUNITY SERVICE,0.2554347826086957,"Figure 2 illustrates the full process of of our proposed framework with four steps.
202"
COMMUNITY SERVICE,0.2565217391304348,"(1) Formulate baseline rationales: The first step is to formulate baseline rationales, whic serve as
203"
COMMUNITY SERVICE,0.2576086956521739,"guidelines for LLMs to infer latent features from observed ones. This involves two sub-steps:
204"
COMMUNITY SERVICE,0.25869565217391305,"– The first sub-step is to develop some baseline rationales, i.e., identify observed features potentially
205"
COMMUNITY SERVICE,0.2597826086956522,"correlated with latent features and formulate their relationships – the logic chain that connects X to Z.
206"
COMMUNITY SERVICE,0.2608695652173913,"Sources to help formulate these baseline rationales include established correlations (e.g., risk score
207"
COMMUNITY SERVICE,0.2619565217391304,"formulas), human input, and external information like socio-economic status in the neighborhood.
208"
COMMUNITY SERVICE,0.26304347826086955,"– In the second sub-step, we craft prompts with interactive alignment. This is a critical component
209"
COMMUNITY SERVICE,0.26413043478260867,"to establish correct reasoning steps for prompts used in Step 2 to generate synthetic rationales. We
210"
COMMUNITY SERVICE,0.26521739130434785,"involve human who are experienced in the domain to provide a prompt template for LLMs to generate
211"
COMMUNITY SERVICE,0.266304347826087,"rationales aligned with the baseline rationales, then test the prompt template on a few examples
212"
COMMUNITY SERVICE,0.2673913043478261,"using zero-shot. If the LLM fails to certain example, we provide the ground truth back to the LLM,
213"
COMMUNITY SERVICE,0.2684782608695652,"allowing it to revise the prompt template (Miao et al., 2023). This process iteratively refines the
214"
COMMUNITY SERVICE,0.26956521739130435,"template until LLMs consistently generate the desired output for all selected examples.
215"
COMMUNITY SERVICE,0.27065217391304347,"(2) Enlarge data with synthetic rationales for fine-tuning: We generate synthetic training data in
216"
COMMUNITY SERVICE,0.2717391304347826,"self-instruct fashion (Wang et al., 2022). With a handful of examples of the baseline rationales as a
217"
COMMUNITY SERVICE,0.2728260869565217,"reference, we then guide the LLMs via in-context learning to generate similar rationales to enlarge
218"
COMMUNITY SERVICE,0.27391304347826084,"the training data samples. To ensure the quality and diversity of the generated dataset, we introduce
219"
COMMUNITY SERVICE,0.275,"human-in-the-loop interventions to filter out low-quality or invalid data based on heuristics. We
220"
COMMUNITY SERVICE,0.27608695652173915,"also leverage automatic evaluation metrics for quality control, e.g., removing data that lack essential
221"
COMMUNITY SERVICE,0.27717391304347827,"keywords.
222"
COMMUNITY SERVICE,0.2782608695652174,"(3) Fine-tuning LLMs: To enhance the reasoning capabilities of the LLMs and better align their
223"
COMMUNITY SERVICE,0.2793478260869565,"outputs in specific domains, we employ a fine-tuning process which utilizes the processed dataset
224"
COMMUNITY SERVICE,0.28043478260869564,"from the previous step (Qiao et al., 2022). Fine-tuning not only boosts the accuracy and reliability of
225"
COMMUNITY SERVICE,0.28152173913043477,"the LLMs, but also significantly improves their ability to reason with complex inputs, and reducing
226"
COMMUNITY SERVICE,0.2826086956521739,"hallucination (Tonmoy et al., 2024).
227"
COMMUNITY SERVICE,0.28369565217391307,"(4) Latent feature inference: The fine-tuned model is able to mirror the nuanced decision-making
228"
COMMUNITY SERVICE,0.2847826086956522,"process of human experts. We use the fine-tuned model to identify latent features and feed them into
229"
COMMUNITY SERVICE,0.2858695652173913,"downstream prediction tasks.
230"
COMMUNITY SERVICE,0.28695652173913044,"Regarding the generalizability of our framework, Steps 2-4 rely primarily on the mechanics of
231"
COMMUNITY SERVICE,0.28804347826086957,"LLMs, which naturally have a high degree of adaptability across different domains. Step 1, which
232"
COMMUNITY SERVICE,0.2891304347826087,"involves the identification and formulation of baseline domain-specific rationales, requires more
233"
COMMUNITY SERVICE,0.2902173913043478,"expert knowledge. To assist with Step 1, our interactive-alignment strategy can help craft effective
234"
COMMUNITY SERVICE,0.29130434782608694,"prompts by allowing iterative refinement based on feedback, reducing the burden on domain experts.
235"
EXPERIMENTS SETUP,0.29239130434782606,"5
Experiments Setup
236"
EXPERIMENTS SETUP,0.29347826086956524,"In this section, we demonstrate the efficacy of our proposed framework on a unique dataset from
237"
EXPERIMENTS SETUP,0.29456521739130437,"a state-wide incarceration diversion program as described in Section 2. We design two sets of
238"
EXPERIMENTS SETUP,0.2956521739130435,"experiments to empirically investigate: (1) Can our approach accurately imitate the human thinking
239"
EXPERIMENTS SETUP,0.2967391304347826,"process to infer latent features? (2) Is our approach more effective than alternative techniques to infer
240"
EXPERIMENTS SETUP,0.29782608695652174,"latent features? (3) Does our approach enhance the performance of downstream prediction tasks?
241"
EXPERIMENTS SETUP,0.29891304347826086,"In the first experiment, we treat the risk level of individuals as a latent feature, despite it being collected
242"
EXPERIMENTS SETUP,0.3,"in the dataset. This experiment examines whether the latent features ˆZ inferred by LLMs match well
243"
EXPERIMENTS SETUP,0.3010869565217391,"with the actual features Z. In the second experiment, we assume that the program requirements are
244"
EXPERIMENTS SETUP,0.30217391304347824,"latent features, which lack ground truth labels for most individuals (only a few dozen individuals
245"
EXPERIMENTS SETUP,0.3032608695652174,"have the program requirements recorded in the data). We first have LLMs deduce these requirements,
246"
EXPERIMENTS SETUP,0.30434782608695654,"then add them to the downstream prediction task of program outcomes Y ∼f(X, ˆZ) and evaluate
247"
EXPERIMENTS SETUP,0.30543478260869567,"whether the prediction accuracy is improved, i.e., the inferred features are indeed beneficial and not
248"
EXPERIMENTS SETUP,0.3065217391304348,"detrimental (recall the results in Lemma 1).
249"
RISK LEVEL PREDICTION,0.3076086956521739,"5.1
Risk Level Prediction
250"
RISK LEVEL PREDICTION,0.30869565217391304,"Task Description. In this task we treat an observed feature—Risk Level—as the latent feature to
251"
RISK LEVEL PREDICTION,0.30978260869565216,"infer. The task is a multi-classification problem to learn Z ∼g(X) among four labels for the latent
252"
RISK LEVEL PREDICTION,0.3108695652173913,"variable Z ∈{moderate, high, very_high} based on each client’s profile X.
253"
RISK LEVEL PREDICTION,0.3119565217391304,"Implementation Details. We implement our proposed framework as follows. All prompt templates
254"
RISK LEVEL PREDICTION,0.3130434782608696,"are attached to Appendix section C.
255"
RISK LEVEL PREDICTION,0.3141304347826087,"- Step 0. Profile writing: In this pre-processing step, we translate structured profile data X into
256"
RISK LEVEL PREDICTION,0.31521739130434784,"text that can be better handled by LLMs, i.e., formulating predicates P’s from the features X. To
257"
RISK LEVEL PREDICTION,0.31630434782608696,"enrich the profile with important in formations that could potential benefits the following steps,
258"
RISK LEVEL PREDICTION,0.3173913043478261,"we formulate the intermediate predicates O’s, where we prompt LLMs to extract and summarize
259"
RISK LEVEL PREDICTION,0.3184782608695652,"underlying information such as background, socio-economic status, and challenges in two or three
260"
RISK LEVEL PREDICTION,0.31956521739130433,"sentences. We then merge these sentences into the client’s profile. We use zero-shot prompting with
261"
RISK LEVEL PREDICTION,0.32065217391304346,"GPT-4 for this step.
262"
RISK LEVEL PREDICTION,0.3217391304347826,"- Step 1. Formulating rationales: Using human input, established risk score calculations (Corrections),
263"
RISK LEVEL PREDICTION,0.32282608695652176,"and the code book with risk calculation details provided by our community partner, we summarize a
264"
RISK LEVEL PREDICTION,0.3239130434782609,"general rule for inferring risk levels from the predicates, i.e., establishing the logic chains from P’s
265"
RISK LEVEL PREDICTION,0.325,"and O’s to Z. We then sample 40 client features from the dataset and manually formulate 40 baseline
266"
RISK LEVEL PREDICTION,0.32608695652173914,"rationales that logically connect features to corresponding risk levels and that are aligned with the
267"
RISK LEVEL PREDICTION,0.32717391304347826,"high-level general rule. To avoid the primacy effect of LLMs, we rate risk scores from 0 to 10 to add
268"
RISK LEVEL PREDICTION,0.3282608695652174,"variability in the labels, categorized as follows: 0-4 (moderate risk), 4-7.5 (high risk), and 7.5-10
269"
RISK LEVEL PREDICTION,0.3293478260869565,"(very high risk).
270"
RISK LEVEL PREDICTION,0.33043478260869563,"- Step 2. Enlarge fine-tuning data: With the 40 baseline rationales, we generate additional synthetic
271"
RISK LEVEL PREDICTION,0.33152173913043476,"rationales. We sample client features and corresponding ground truth risk scores from the dataset,
272"
RISK LEVEL PREDICTION,0.33260869565217394,"using one of the 40 rationales as an example, to prompt LLMs to produce similar narratives with CoT
273"
RISK LEVEL PREDICTION,0.33369565217391306,"prompts. In total we got 3000 rationales for the training data.
274"
RISK LEVEL PREDICTION,0.3347826086956522,"- Step 3. Fine-tune LLMs: Our framework is designed to be plug-and-play, allowing the synthetic
275"
RISK LEVEL PREDICTION,0.3358695652173913,"data generated in the previous step to be used across different language models. We fine-tune two
276"
RISK LEVEL PREDICTION,0.33695652173913043,"pre-trained language models for cross-validation purposes: GPT-3.5 and Llama2-13b(OpenAI, 2021).
277"
RISK LEVEL PREDICTION,0.33804347826086956,"We use OpenAI API to fine-tune GPT-3.5-turbo-0125 (Touvron et al., 2023; OpenAI). We fine-tune
278"
RISK LEVEL PREDICTION,0.3391304347826087,"Llama2-13b-chat using LoRA (Hu et al., 2021).
279"
RISK LEVEL PREDICTION,0.3402173913043478,"- Step 4. Inference with LLMs: We prompt fine-tuned LLMs to infer risk level ˆZi from features Xi
280"
RISK LEVEL PREDICTION,0.34130434782608693,"for each client i in the test data and evaluate the out-of-sample accuracy by comparing the inferred
281"
RISK LEVEL PREDICTION,0.3423913043478261,"latent variable (risk level) ˆZi with the ground truth label Zi.
282"
RISK LEVEL PREDICTION,0.34347826086956523,"Evaluation. We choose ML classifiers (e.g., Neural Networks or Gradient Boosting Trees) as the
283"
RISK LEVEL PREDICTION,0.34456521739130436,"baseline to infer ˆZi from features Xi. We compare the prediction performance of ˆZi inferred from
284"
RISK LEVEL PREDICTION,0.3456521739130435,"our approach with that from ML models using out-of-sample accuracy and F1 score. Additionally,
285"
RISK LEVEL PREDICTION,0.3467391304347826,"we evaluate the quality of generated text with an automatic evaluation metric. In the pre-processing
286"
RISK LEVEL PREDICTION,0.34782608695652173,"step, we assess the keyword coverage rate in the generated profile assuming each feature value is
287"
RISK LEVEL PREDICTION,0.34891304347826085,"a keyword. For synthetic rationales, we use YAKE, a pretrained keyword extractor (Campos et al.,
288"
RISK LEVEL PREDICTION,0.35,"2020), to identify keywords. We then evaluate the keyword coverage rate with a rule-based detector
289"
RISK LEVEL PREDICTION,0.35108695652173916,"to determine how many logical information points are covered.
290"
OUTCOME PREDICTION,0.3521739130434783,"5.2
Outcome Prediction
291"
OUTCOME PREDICTION,0.3532608695652174,"Task Description.
In this task, we treat the program requirements (e.g., substance treatment,
292"
OUTCOME PREDICTION,0.35434782608695653,"counseling) for each client as the latent features Z and use them to augment the original feature X for
293"
OUTCOME PREDICTION,0.35543478260869565,"outcome prediction, which is a multi-classification problem to learn Y ∼f(X, Z) among four labels
294"
OUTCOME PREDICTION,0.3565217391304348,"for the outcome Y ∈{Completed, Revoked, NotCompleted, Other}. The raw dataset does not
295"
OUTCOME PREDICTION,0.3576086956521739,"record the program requirements except for a very few clients; thus, the latent feature Z in this task
296"
OUTCOME PREDICTION,0.358695652173913,"is truly unobservable (in contrast to the one used in the first task). Available program requirement
297"
OUTCOME PREDICTION,0.35978260869565215,"options for this task are attached to the appendix section D.
298"
OUTCOME PREDICTION,0.36086956521739133,"Implementation Details. Steps 0 and 2-4 remain almost the same as in the risk-level prediction
299"
OUTCOME PREDICTION,0.36195652173913045,"task. Step 1 requires a slight adjustment (as discussed in Section 4, this step is the main part in
300"
OUTCOME PREDICTION,0.3630434782608696,"our framework that requires customization). Here, we formulate 40 baseline rationales in step 1 to
301"
OUTCOME PREDICTION,0.3641304347826087,"deduce clients’ program requirements from their features. We leverage multi-stage prompting strategy
302"
OUTCOME PREDICTION,0.3652173913043478,"(Qiao et al., 2022) to break down the task into three sub-tasks: (1) identify the main challenges
303"
OUTCOME PREDICTION,0.36630434782608695,"from the client’s profile, (2) rank these challenges by priority, (3) match the challenges with suitable
304"
OUTCOME PREDICTION,0.3673913043478261,"requirements. Particularly, the third task is our main goal, with the first two serving as steps to
305"
OUTCOME PREDICTION,0.3684782608695652,"streamline the process and simplified the task.
306"
OUTCOME PREDICTION,0.3695652173913043,"Evaluation. We train an ML classifier to predict outcomes with and without the inferred latent
307"
OUTCOME PREDICTION,0.3706521739130435,"features, i.e., ˆYi ∼f(Xi, ˆZi) versus ˆYi ∼f(Xi). We evaluate the out-of-sample accuracy by
308"
OUTCOME PREDICTION,0.3717391304347826,"comparing the predicted outcome ˆYi with the true label Yi in the test data. This comparison allows us
309"
OUTCOME PREDICTION,0.37282608695652175,"to assess whether incorporating the latent features enhances the classifier’s performance.
310"
RESULTS,0.3739130434782609,"6
Results
311"
RESULTS,0.375,"In this section, we demonstrate experiments results for two case studies we designed and additional
312"
RESULTS,0.3760869565217391,"results for sensitivity analyses.
313"
RISK LEVEL PREDICTION RESULTS,0.37717391304347825,"6.1
Risk Level Prediction Results
314"
RISK LEVEL PREDICTION RESULTS,0.3782608695652174,"As mentioned in Section 5.1, we infer risk level on the client’s profile. We compare our approach’s
315"
RISK LEVEL PREDICTION RESULTS,0.3793478260869565,"performance to baseline ML model’s performance using the accuracy score and F1 score. Before
316"
RISK LEVEL PREDICTION RESULTS,0.3804347826086957,"showing this performance comparison, we first show results on the generated text quality.
317"
RISK LEVEL PREDICTION RESULTS,0.3815217391304348,"Generated Text Quality. For profile writing in Step 0, we treat each individual feature in Xi as a
318"
RISK LEVEL PREDICTION RESULTS,0.3826086956521739,"keyword to cover, and measure the keyword coverage rate. The generated profiles demonstrated an
319"
RISK LEVEL PREDICTION RESULTS,0.38369565217391305,"average keyword coverage rate of 98%, indicating that they effectively capture the most important
320"
RISK LEVEL PREDICTION RESULTS,0.3847826086956522,"information from the original data. For the generated synthetic rationales in Step 2, we treat terms
321"
RISK LEVEL PREDICTION RESULTS,0.3858695652173913,"such as age, gender, employment, and education as critical keywords and assess their coverage rate.
322"
RISK LEVEL PREDICTION RESULTS,0.3869565217391304,"The fine-tuned GPT-3.5 and Llama2-13b-chat both achieved a keyword coverage rate of 100%. This
323"
RISK LEVEL PREDICTION RESULTS,0.38804347826086955,"indicates that the generated content adheres strictly to the guidelines established in the training data,
324"
RISK LEVEL PREDICTION RESULTS,0.38913043478260867,"ensuring that all necessary information is accurately represented.
325"
RISK LEVEL PREDICTION RESULTS,0.39021739130434785,"Latent Variable Inference Performance.
As shown in Figure 3(a), our approach achieves the
326"
RISK LEVEL PREDICTION RESULTS,0.391304347826087,"highest overall accuracy. In particular, the fine-tuned GPT-3.5 achieves an accuracy that is 20%
327"
RISK LEVEL PREDICTION RESULTS,0.3923913043478261,"higher than other baseline ML approaches. The reason that ML models struggle to predict well
328"
RISK LEVEL PREDICTION RESULTS,0.3934782608695652,"is due to the fact that there is no strong correlation between the observed features and the targets
329"
RISK LEVEL PREDICTION RESULTS,0.39456521739130435,"(risk level); see the correlation plot in Appendix F. In contrast, our approach demonstrates superior
330"
RISK LEVEL PREDICTION RESULTS,0.39565217391304347,"performance, since it more effectively handles datasets with subtle or non-obvious relationships
331"
RISK LEVEL PREDICTION RESULTS,0.3967391304347826,"between the observed and target variables. This result shows that our approach is able to make
332"
RISK LEVEL PREDICTION RESULTS,0.3978260869565217,"accurate inference of latent features and outperforms traditional ML approaches.
333"
RISK LEVEL PREDICTION RESULTS,0.39891304347826084,(a) Model accuracy
RISK LEVEL PREDICTION RESULTS,0.4,"Category
LR
MLP
RF
GBT
LLaMA2
GPT3.5"
RISK LEVEL PREDICTION RESULTS,0.40108695652173915,"Moderate
0.51
0.54
0.44
0.46
0.57
0.69"
RISK LEVEL PREDICTION RESULTS,0.40217391304347827,"High
0.65
0.55
0.69
0.66
0.70
0.81"
RISK LEVEL PREDICTION RESULTS,0.4032608695652174,"Very High
0.20
0.11
0.18
0.18
0.38
0.81"
RISK LEVEL PREDICTION RESULTS,0.4043478260869565,(b) F1 scores
RISK LEVEL PREDICTION RESULTS,0.40543478260869564,"Figure 3: Risk level prediction results: (a) Model accuracy; (b) F1 scores per-category. LR - logistic
regression; MLP - Neural Networks; RF- random forest; GBT - Gradient Boosting Trees."
RISK LEVEL PREDICTION RESULTS,0.40652173913043477,"Table 3(b) details the prediction performance by class, showing F1 scores for each class using ML
334"
RISK LEVEL PREDICTION RESULTS,0.4076086956521739,"models and our approach. Notably, all ML models struggle with the ‘Very High Risk’ category
335"
RISK LEVEL PREDICTION RESULTS,0.40869565217391307,"– this category is often misclassified as ‘High Risk’ due to similar feature distributions of these
336"
RISK LEVEL PREDICTION RESULTS,0.4097826086956522,"two categories and unbalanced data (only 371 training points for ‘Very High Risk’). In contrast,
337"
RISK LEVEL PREDICTION RESULTS,0.4108695652173913,"our approach significantly improves the prediction performance for this category, highlighting its
338"
RISK LEVEL PREDICTION RESULTS,0.41195652173913044,"effectiveness for unbalanced datasets. This improvement is likely because our LLM-based approach
339"
RISK LEVEL PREDICTION RESULTS,0.41304347826086957,"has intermediate steps (profile writing to obtain the socio-economic status and other contextual factors
340"
RISK LEVEL PREDICTION RESULTS,0.4141304347826087,"in step 0 and connecting these factors with the latent variables in step 1), which help capturing the
341"
RISK LEVEL PREDICTION RESULTS,0.4152173913043478,"subtle distinctions between ‘High Risk’ and ‘Very High Risk’ that are not explicitly recorded.
342"
OUTCOME PREDICTION RESULTS,0.41630434782608694,"6.2
Outcome Prediction Results
343"
OUTCOME PREDICTION RESULTS,0.41739130434782606,"As mentioned in Section 5.2, we infer program requirements as additional latent features and use
344"
OUTCOME PREDICTION RESULTS,0.41847826086956524,"them for the downstream outcome prediction task. We compare the performance of the downstream
345"
OUTCOME PREDICTION RESULTS,0.41956521739130437,"classifiers that trained with and without the latent features. Note that in the first task (risk-level
346"
OUTCOME PREDICTION RESULTS,0.4206521739130435,"inferrence), GPT3.5 demonstrated better performance than llama2-13b. Thus, we focused on fine-
347"
OUTCOME PREDICTION RESULTS,0.4217391304347826,"tuning GPT-3.5 when using our approach for this task.
348"
OUTCOME PREDICTION RESULTS,0.42282608695652174,"As illustrated in Table 4(a), incorporating latent features significantly improves the performance
349"
OUTCOME PREDICTION RESULTS,0.42391304347826086,"of the downstream classifiers. Specifically, the addition of latent features increases the ROC AUC
350"
OUTCOME PREDICTION RESULTS,0.425,"score of Logistic Regression from 0.70 to 0.89 and from 0.84 to 0.92 for the Gradient Boosting Tree.
351"
OUTCOME PREDICTION RESULTS,0.4260869565217391,"Furthermore, the feature importance in Figure 4(b) shows that the inferred features – ‘requirement_1’,
352"
OUTCOME PREDICTION RESULTS,0.42717391304347824,"‘requirement_2’, and ‘requirement_3’ – are among the top-ranked features. This implies the significant
353"
OUTCOME PREDICTION RESULTS,0.4282608695652174,"relevance of these features on the downstream classification task. Hence, we can conclude that our
354"
OUTCOME PREDICTION RESULTS,0.42934782608695654,"approach has the capability of enhancing the downstream classifier’s accuracy with inferred
355"
OUTCOME PREDICTION RESULTS,0.43043478260869567,"latent features.
356"
OUTCOME PREDICTION RESULTS,0.4315217391304348,"without latent feature
LR
MLP
GBT"
OUTCOME PREDICTION RESULTS,0.4326086956521739,"ROC AUC Score
0.70
0.81
0.84"
OUTCOME PREDICTION RESULTS,0.43369565217391304,"F1 Score
0.69
0.70
0.71"
OUTCOME PREDICTION RESULTS,0.43478260869565216,"with latent feature
LR
MLP
GBT"
OUTCOME PREDICTION RESULTS,0.4358695652173913,"ROC AUC Score
0.89
0.88
0.92"
OUTCOME PREDICTION RESULTS,0.4369565217391304,"F1 Score
0.75
0.73
0.77"
OUTCOME PREDICTION RESULTS,0.4380434782608696,"(a) Model Performance
(b) Feature Importance Plot
Figure 4: Outcome prediction results: (a) Model performance with/without the inferred latent
features (program requirements); (b) feature importance plot. LR - logistic regression; MLP - Neural
Networks; GBT - Gradient Boosting Trees."
SENSITIVITY ANALYSIS,0.4391304347826087,"6.3
Sensitivity Analysis
357"
SENSITIVITY ANALYSIS,0.44021739130434784,"In our sensitivity analysis, we further investigate the following three questions: (1) How sensitive is
358"
SENSITIVITY ANALYSIS,0.44130434782608696,"our approach to the quality of human guidelines? (2) How important is fine-tuning in our framework?
359"
SENSITIVITY ANALYSIS,0.4423913043478261,"For the first question, perhaps not surprisingly, our approach is sensitive to human guidelines,
360"
SENSITIVITY ANALYSIS,0.4434782608695652,"specifically the baseline rationales and prompt templates formulated in Step 1. We have conducted
361"
SENSITIVITY ANALYSIS,0.44456521739130433,"an ablation study to determine the optimal level of details required in the prompts. As shown in
362"
SENSITIVITY ANALYSIS,0.44565217391304346,"Figure 9 in Appendix D, the best performance was achieved with the most reasoning steps and a
363"
SENSITIVITY ANALYSIS,0.4467391304347826,"sentence length of two per step. In other words, increasing the number of reasoning steps allows
364"
SENSITIVITY ANALYSIS,0.44782608695652176,"us to decompose the task into simpler components and enhances the performance of LLMs. More
365"
SENSITIVITY ANALYSIS,0.4489130434782609,"importantly, while human guidelines are important, the interactive self-revise alignment strategy
366"
SENSITIVITY ANALYSIS,0.45,"can significantly help during the sub-step of Step 1 (prompt crafting). By providing ground truth and
367"
SENSITIVITY ANALYSIS,0.45108695652173914,"encouraging self-reflection, GPT-4 can revise the prompt template to include crucial details, ensuring
368"
SENSITIVITY ANALYSIS,0.45217391304347826,"a more accurate evaluation.
369"
SENSITIVITY ANALYSIS,0.4532608695652174,"The answer to the second question is that fine-tuning is necessary. We have conducted another
370"
SENSITIVITY ANALYSIS,0.4543478260869565,"ablation study, where we repeated the risk-level prediction task with zero-shot, one-shot, and three-
371"
SENSITIVITY ANALYSIS,0.45543478260869563,"shot prompting to compare with our fine-tuned model. In zero-shot, we provided only the task
372"
SENSITIVITY ANALYSIS,0.45652173913043476,"description. In one-shot and three-shot, we included randomly selected human-verified examples.
373"
SENSITIVITY ANALYSIS,0.45760869565217394,"Accuracy rankings from lowest to highest were: three-shot (40%), zero-shot (55%), one-shot (60%),
374"
SENSITIVITY ANALYSIS,0.45869565217391306,"and the fine-tuned model (75%); see Table 9 in Appendix D. The three-shot’s poor performance
375"
SENSITIVITY ANALYSIS,0.4597826086956522,"may be due to information loss from long inputs. Zero-shot responses are highly variable and not
376"
SENSITIVITY ANALYSIS,0.4608695652173913,"well-suited for downstream tasks. Although one-shot showed improvement, the fine-tuned model
377"
SENSITIVITY ANALYSIS,0.46195652173913043,"significantly outperformed all others.
378"
DISCUSSION,0.46304347826086956,"7
Discussion
379"
DISCUSSION,0.4641304347826087,"This study presents a framework that leverages the capabilities of LLMs to enhance the prediction
380"
DISCUSSION,0.4652173913043478,"accuracy in downstream tasks without necessitating invasive data collection methods. Our approach
381"
DISCUSSION,0.46630434782608693,"reduces the need for collecting extensive personal data, thus mitigating privacy concerns. This aligns
382"
DISCUSSION,0.4673913043478261,"with ethical data usage standards, especially in sensitive domains. Note that we do not explicitly
383"
DISCUSSION,0.46847826086956523,"address bias in the data or LLM reasoning processes in this paper. We excluded the ‘race’ feature in
384"
DISCUSSION,0.46956521739130436,"our case study and found alignment in risk level distributions across genders, implying no additional
385"
DISCUSSION,0.4706521739130435,"bias introduced by our approach. However, existing biases in LLMs could be perpetuated if not
386"
DISCUSSION,0.4717391304347826,"monitored and adjusted. Addressing these biases is beyond this paper’s scope and is left for future
387"
DISCUSSION,0.47282608695652173,"research as a critical area.
388"
DISCUSSION,0.47391304347826085,"This framework has vast potential applications, particularly in areas with limited data and ethical
389"
DISCUSSION,0.475,"constraints. For example, in healthcare, our framework can help predict readmission or post-discharge
390"
DISCUSSION,0.47608695652173916,"mortality by inferring unrecorded social determinants of health. For low-volume niche product rec-
391"
DISCUSSION,0.4771739130434783,"ommendations, our framework can synthesize customer preference data to enhance recommendation
392"
DISCUSSION,0.4782608695652174,"systems without extensive user tracking.
393"
REFERENCES,0.47934782608695653,"References
394"
REFERENCES,0.48043478260869565,"Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
395"
REFERENCES,0.4815217391304348,"Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
396"
REFERENCES,0.4826086956521739,"arXiv preprint arXiv:2303.08774, 2023.
397"
REFERENCES,0.483695652173913,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
398"
REFERENCES,0.48478260869565215,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
399"
REFERENCES,0.48586956521739133,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
400"
REFERENCES,0.48695652173913045,"Ricardo Campos, Vítor Mangaravite, Arian Pasquali, Alípio Jorge, Célia Nunes, and Adam Jatowt.
401"
REFERENCES,0.4880434782608696,"Yake! keyword extraction from single documents using multiple local features. Information
402"
REFERENCES,0.4891304347826087,"Sciences, 509:257–289, 2020.
403"
REFERENCES,0.4902173913043478,"South
Dakota
Department
Of
Corrections.
Lsi-r
assessment
and
case
planning.
404"
REFERENCES,0.49130434782608695,"https://doc.sd.gov/documents/about/policies/LSI-R%20Assessment%20and%
405"
REFERENCES,0.4923913043478261,"20Case%20Planning.pdf. [Accessed 19-05-2024].
406"
REFERENCES,0.4934782608695652,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
407"
REFERENCES,0.4945652173913043,"Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
408"
REFERENCES,0.4956521739130435,"processing systems, 27, 2014.
409"
REFERENCES,0.4967391304347826,"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
410"
REFERENCES,0.49782608695652175,"and Weizhu Chen.
Lora: Low-rank adaptation of large language models.
arXiv preprint
411"
REFERENCES,0.4989130434782609,"arXiv:2106.09685, 2021.
412"
REFERENCES,0.5,"Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng,
413"
REFERENCES,0.5010869565217392,"and Trishul Chilimbi. Understanding and constructing latent modality structures in multi-modal
414"
REFERENCES,0.5021739130434782,"representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
415"
REFERENCES,0.5032608695652174,"Pattern Recognition, pages 7661–7671, 2023.
416"
REFERENCES,0.5043478260869565,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
417"
REFERENCES,0.5054347826086957,"arXiv:1312.6114, 2013.
418"
REFERENCES,0.5065217391304347,"Bingxuan Li, Antonio Castellanos, Pengyi Shi, and Amy Ward. Combining machine learning and
419"
REFERENCES,0.5076086956521739,"queueing theory for data-driven incarceration-diversion program management. In Proceedings of
420"
REFERENCES,0.508695652173913,"the Thirty-Sixth Annual Conference on Innovative Applications of Artificial Intelligence. AAAI,
421"
REFERENCES,0.5097826086956522,"2024.
422"
REFERENCES,0.5108695652173914,"Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi
423"
REFERENCES,0.5119565217391304,"Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for
424"
REFERENCES,0.5130434782608696,"language models. arXiv preprint arXiv:2404.07503, 2024.
425"
REFERENCES,0.5141304347826087,"Qiuhao Lu, Dejing Dou, and Thien Huu Nguyen. Textual data augmentation for patient outcomes
426"
REFERENCES,0.5152173913043478,"prediction. In 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),
427"
REFERENCES,0.5163043478260869,"pages 2817–2821, 2021. doi: 10.1109/BIBM52615.2021.9669861.
428"
REFERENCES,0.5173913043478261,"Ning Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own
429"
REFERENCES,0.5184782608695652,"step-by-step reasoning. arXiv preprint arXiv:2308.00436, 2023.
430"
REFERENCES,0.5195652173913043,"OpenAI.
Fine-tuning.
https://platform.openai.com/docs/guides/fine-tuning.
Ac-
431"
REFERENCES,0.5206521739130435,"cessed: 2024-05-22.
432"
REFERENCES,0.5217391304347826,"OpenAI. Gpt-3.5. https://platform.openai.com/docs/models/gpt-3.5, 2021. Accessed:
433"
REFERENCES,0.5228260869565218,"2024-05-22.
434"
REFERENCES,0.5239130434782608,"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
435"
REFERENCES,0.525,"Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
436"
REFERENCES,0.5260869565217391,"instructions with human feedback. Advances in neural information processing systems, 35:27730–
437"
REFERENCES,0.5271739130434783,"27744, 2022.
438"
REFERENCES,0.5282608695652173,"Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei
439"
REFERENCES,0.5293478260869565,"Huang, and Huajun Chen. Reasoning with language model prompting: A survey. arXiv preprint
440"
REFERENCES,0.5304347826086957,"arXiv:2212.09597, 2022.
441"
REFERENCES,0.5315217391304348,"Merrill Rotter and Virginia Barber-Rioja. Diversion programs and alternatives to incarceration.
442"
REFERENCES,0.532608695652174,"Oxford University Press, May 2015. doi: 10.1093/med/9780199360574.003.0021. URL http:
443"
REFERENCES,0.533695652173913,"//dx.doi.org/10.1093/med/9780199360574.003.0021.
444"
REFERENCES,0.5347826086956522,"SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das.
445"
REFERENCES,0.5358695652173913,"A comprehensive survey of hallucination mitigation techniques in large language models. arXiv
446"
REFERENCES,0.5369565217391304,"preprint arXiv:2401.01313, 2024.
447"
REFERENCES,0.5380434782608695,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
448"
REFERENCES,0.5391304347826087,"Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
449"
REFERENCES,0.5402173913043479,"and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
450"
REFERENCES,0.5413043478260869,"David A Van Dyk and Xiao-Li Meng. The art of data augmentation. Journal of Computational and
451"
REFERENCES,0.5423913043478261,"Graphical Statistics, 10(1):1–50, 2001.
452"
REFERENCES,0.5434782608695652,"Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
453"
REFERENCES,0.5445652173913044,"Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions.
454"
REFERENCES,0.5456521739130434,"arXiv preprint arXiv:2212.10560, 2022.
455"
REFERENCES,0.5467391304347826,"Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Llm for patient-trial matching: Privacy-
456"
REFERENCES,0.5478260869565217,"aware data augmentation towards better performance and generalizability. In American Medical
457"
REFERENCES,0.5489130434782609,"Informatics Association (AMIA) Annual Symposium, 2023.
458"
REFERENCES,0.55,"Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with
459"
REFERENCES,0.5510869565217391,"reasoning. Advances in Neural Information Processing Systems, 35:15476–15488, 2022.
460"
REFERENCES,0.5521739130434783,"Chenyu Zhai and Jing Peng. Mining latent features from reviews and ratings for item recommendation.
461"
REFERENCES,0.5532608695652174,"In 2016 International Conference on Computational Science and Computational Intelligence
462"
REFERENCES,0.5543478260869565,"(CSCI), pages 1119–1125, 2016. doi: 10.1109/CSCI.2016.0213.
463"
REFERENCES,0.5554347826086956,"Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the
464"
REFERENCES,0.5565217391304348,"paradox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022.
465"
REFERENCES,0.5576086956521739,"Appendix
466"
REFERENCES,0.558695652173913,"A
Proof of Lemma 1
467"
REFERENCES,0.5597826086956522,"We use the log-loss, defined as
468"
REFERENCES,0.5608695652173913,"L(D, β) = −1 n n
X"
REFERENCES,0.5619565217391305,"i=1
[yi log(pi) + (1 −yi) log(1 −pi)]
(2)"
REFERENCES,0.5630434782608695,"for given data D = {(xi, yi)}n
i=1 and pi = 1/
 
1 + e−(β0+β1xi)
. When using the augmented feature
469"
REFERENCES,0.5641304347826087,"˜xi = (xi, zi), we denote the data as ˜D = {
 
(xi, zi), yi

}n
i=1.
470"
REFERENCES,0.5652173913043478,"For the first part of the lemma, we note that the in-sample log-loss for the original features follows
471"
REFERENCES,0.566304347826087,"Lin(D, β) = −1 n n
X"
REFERENCES,0.5673913043478261,"i=1
[yi log(pi) + (1 −yi) log(1 −pi)] ,
(3)"
REFERENCES,0.5684782608695652,"and the in-sample log-loss for the augmented features follows
472"
REFERENCES,0.5695652173913044,"Lin( ˜D, β) = −1 n n
X"
REFERENCES,0.5706521739130435,"i=1
[yi log(˜pi) + (1 −yi) log(1 −˜pi)] ,
(4)"
REFERENCES,0.5717391304347826,"where pi = 1/
 
1 + e−(β0+β1xi)
and ˜pi = 1/
 
1 + e−(β0+β1xi+β2zi)
.
473"
REFERENCES,0.5728260869565217,"We denote the optimal coefficients that minimize the log-loss in (3) as β∗= (β∗
0, β∗
1), and the
474"
REFERENCES,0.5739130434782609,"coefficients that minimize the log-loss in (4) as ˜β∗= (˜β∗
0, ˜β∗
1, ˜β∗
2). Note that ˇβ = (β∗
0, β∗
1, 0) is a
475"
REFERENCES,0.575,"feasible solution for the log-loss in (4). Therefore, using the optimization property, we have
476"
REFERENCES,0.5760869565217391,"Lin( ˜D, ˜β∗) ≤Lin( ˜D, ˇβ) = Lin(D, β∗),"
REFERENCES,0.5771739130434783,"which completes the first part of the lemma.
477"
REFERENCES,0.5782608695652174,"For the second part of the lemma, we first assume that for the given data D, Lin( ˜D, ˜β∗) =
478"
REFERENCES,0.5793478260869566,"Lin(D, β∗) −ϵ/n where ϵ ≥0 from the first part of the lemma. We now construct an instance
479"
REFERENCES,0.5804347826086956,"with an out-of-sample dataset D′ that contains n + 1 samples, where D′ consists of (i) the n data
480"
REFERENCES,0.5815217391304348,"points that exactly match with D (or ˜D) for the first n samples, and (ii) one additional sample
481"
REFERENCES,0.5826086956521739,"(xi+1, yi+1) (or ((xi+1, zi+1), yi+1) when using the augmented features). Without loss of generality,
482"
REFERENCES,0.5836956521739131,"assume that yi+1 = 1. Then we have
483"
REFERENCES,0.5847826086956521,"Lout(D′, β∗) =
1
n + 1
 
nLin(D, β∗) −log(pi+1)
"
REFERENCES,0.5858695652173913,"and
484"
REFERENCES,0.5869565217391305,"Lout( ˜D′, ˜β∗) =
1
n + 1"
REFERENCES,0.5880434782608696,"
nLin( ˜D, ˜β∗) −log(˜pi+1)

."
REFERENCES,0.5891304347826087,"When the added features Z’s are non-informative, we consider the scenarios that they are noise and
485"
REFERENCES,0.5902173913043478,"the additional term ˜β∗
2Z also contributes noise to the predictions. In other words, the coefficients ˜β∗
486"
REFERENCES,0.591304347826087,"do not generalize well to the test data. Therefore, there exists an instance where the realization of Z,
487"
REFERENCES,0.592391304347826,"zi+1 deviates from the predicted probability significantly, such that
488"
REFERENCES,0.5934782608695652,˜pi+1 < pi+1/ exp(ϵ) ≤pi+1.
REFERENCES,0.5945652173913043,"Note that this instance exists since the noise terms do not correspond to any actual pattern in the test
489"
REFERENCES,0.5956521739130435,"data, causing incorrect predictions, and in our construction, a smaller predicted probability would be
490"
REFERENCES,0.5967391304347827,"less accurate as the label yi+1 = 1. Therefore,
491"
REFERENCES,0.5978260869565217,"−log(˜pi+1) > −log(pi+1) + ϵ,"
REFERENCES,0.5989130434782609,"and
492"
REFERENCES,0.6,"Lout( ˜D′, ˜β∗)
=
1
n + 1
 
nLin(D, β∗) −ϵ −log(˜pi+1)
"
REFERENCES,0.6010869565217392,">
1
n + 1
 
nLin(D, β∗) −log(pi+1)

= Lout(D′, β∗)."
REFERENCES,0.6021739130434782,"B
Compute Resources
493"
REFERENCES,0.6032608695652174,"For all experiments, we split data into training and testing dataset with ratio of 8:2.
494"
REFERENCES,0.6043478260869565,"For experiment 1 (risk level prediction), we finetune LLaMA2-13b-chat on 2 X NVIDIA RTX A6000
495"
REFERENCES,0.6054347826086957,"for 4 hours with LoRA. And we finetuned three times for different subtasks. We use OpenAI offical
496"
REFERENCES,0.6065217391304348,"API to finetune GPT3.5 model, which requires no GPUs. Each finetune job takes about 2 hours. We
497"
REFERENCES,0.6076086956521739,"repeat 3 times for different sub tasks. Additionally, we also run Machine Learning baseline model on
498"
REFERENCES,0.6086956521739131,"CPU (Intel i7). We run grid search for each classifier.
499"
REFERENCES,0.6097826086956522,"For experiment 2 (outcome prediction), we use OpenAI offical API to finetune GPT3.5 model, which
500"
REFERENCES,0.6108695652173913,"requires no GPUs. Each finetune job takes about 2 hours. We repeat 6 times for different sub
501"
REFERENCES,0.6119565217391304,"tasks.Additionally, we also run Machine Learning baseline model on CPU (Intel i7). We run grid
502"
REFERENCES,0.6130434782608696,"search for each classifier.
503"
REFERENCES,0.6141304347826086,"All other experiments (e.g. sensitive experiment) are conducted on ChatGPT, which requires no GPU.
504"
REFERENCES,0.6152173913043478,"C
Prompt template
505"
REFERENCES,0.616304347826087,"Task: Write a paragraph to profile the client, please include following:"
REFERENCES,0.6173913043478261,"1. Write sentences to cover all basic information provided.
2. Provide information about the area of this client live in, as much more details as you can. 
3. Infer social economic status of this client
4. Infer the challenges that this client might facing."
REFERENCES,0.6184782608695653,Here are the basic information of the client: <features>.
REFERENCES,0.6195652173913043,Here is the reference of living area context: <additional info>
REFERENCES,0.6206521739130435,Figure 5: Profile writing prompt
REFERENCES,0.6217391304347826,"Here is the profile of a client: <profile>
Given the client’s information, please infer a risk score out of 10."
REFERENCES,0.6228260869565218,"Given client’s information to infer risk score out of 10, we know that:
1. Employment (If client has unstable employment status, increase the score by 1. 
Adjust score if needed):  ___
2. Financial Status (If client has financial difficulty, increase the risk score by 1. 
If client relies on social economic assistance, further increase the risk score by 1. 
Adjust score if needed.): ___
3. Education (Increase the risk score by 1 if the highest grade of school completed is 
less than grade 12. Further increase the risk score by 1 if the highest grade completed 
is less than grade 10): ___
4. Family and Marital (Increase score if client is dissatisfied with his/her current 
marital relationships situation. Increase risk score if the client is a social isolate. 
Adjust score if needed.):  ___
5. Drug (Increase risk score by 1 if the client has ever had a drug problem. If the 
drug problem is related with Heroin, further increase the risk score by 1. Adjust score 
if needed.):  ___
6. Living Area (Increase risk score by 1 if the client lives in a high crime 
neighborhood): ___
7. Age (Increase risk score by 0.3 if the client is under the age of sixteen): 
8. Gender (Increase risk score by 0.3 if the client is male): 
Conclusion: ___"
REFERENCES,0.6239130434782608,Figure 6: Risk Level Prediction: Prompt template and response CoT template
REFERENCES,0.625,"Here is the profile of a client: <profile>
Analyze the provided profile of the client to infer the main challenges he faces."
REFERENCES,0.6260869565217392,"Given the identified challenges for the client, infer the priority of each 
challenge in terms of immediate action and long-term impact on his reintegration 
into society. Please response in the ranking order. Here are the challenges: Here 
are the challenges <challenges>:"
REFERENCES,0.6271739130434782,"Here is the available list of programs <program list>: 
Given the profile and challenges of the client, select the top 3 program 
requirements that would be most beneficial for the client.
Here is the profile of client: <profile + top 3 ordered challenges>"
REFERENCES,0.6282608695652174,Figure 7: Requirement selection: Multi-stage Prompt template
REFERENCES,0.6293478260869565,"To select the top 3 programs that would be most beneficial for the client, let’s analyze each 
available options:
1. Thinking for a Change (It aims to transform criminogenic thinking patterns with designed 
cognitive-behavioral curriculum. Recommend for clients assessed at relatively high risk 
level): __
2. Employment (It aims to help client develop employability. Recommend this for clients with 
unstable employment status): __
3. Education (It aims to engage clients in educational programs. Recommend clients without a 
high school diploma or GED):__
4. Positive Peer Mentoring (It offers positive role models and fosters a supportive network, 
which can deter criminal associations. Recommend this for clients residing in high-crime 
areas):__
5. Community Service (It aids in building a sense of responsibility and community connection. 
Recommend for clients with property offense or drug-related offenses):__
6. Mental Health Treatment (It addresses underlying mental health issues that may contribute 
to criminal behavior. Recommend for clients with a history of substance abuse or unstable 
living conditions):__
7. Anger Management (It focuses on teaching effective emotion and reaction management 
techniques. Recommend for clients who exhibit aggressive behaviors or have property-related 
offenses):__
8. Substance Abuse Treatment (It aims to help clients overcome substance dependencies. 
Recommend for clients with histories of drug-related offenses or primary drug use):__
9. Domestic Violence Counseling (It aims to address and modify violent behavior patterns. 
Recommend for clients involved in violent incidents):__
10. Sex Offender Counseling (It focuses on behavior modification and preventing recidivism. 
Recommend for clients with sex-related offenses):__
Conclusion: ___"
REFERENCES,0.6304347826086957,Figure 8: Requirement selection: Response CoT template
REFERENCES,0.6315217391304347,"D
Ablation Study Results
506"
REFERENCES,0.6326086956521739,"Setting
Accuracy"
REFERENCES,0.633695652173913,"Zero-shot
55%"
REFERENCES,0.6347826086956522,"One-shot
60%"
REFERENCES,0.6358695652173914,"Three-shot
40%"
REFERENCES,0.6369565217391304,"Fine-tune
75%"
REFERENCES,0.6380434782608696,"(a) Risk level prediction
results across different
setting"
REFERENCES,0.6391304347826087,(b) Risk level prediction results across different strategy
REFERENCES,0.6402173913043478,"Figure 9: Ablation study results: (a) Experiments on risk level prediction task using GPT4 with
different prompting setting. (b) Experiments using GPT4 with different prompting setting different
prompting strategies."
REFERENCES,0.6413043478260869,"E
Program Requirements
507"
REFERENCES,0.6423913043478261,"Requirement Name
Description
Thinking for a Change
Aimed at transforming criminogenic thinking patterns using a cognitive-
behavioral curriculum, recommended for clients at a high risk level.
Employment
Helps develop employability, recommended for clients with unstable
employment status.
Education
Engages clients in educational programs, recommended for those with-
out a high school diploma or GED.
Positive Peer Mentoring
Provides positive role models and a supportive network, recommended
for clients in high-crime areas.
Community Service
Builds a sense of responsibility and community connection, recom-
mended for clients with property or drug-related offenses.
Mental Health Treatment
Addresses underlying mental health issues, recommended for clients
with a history of substance abuse or unstable living conditions.
Anger Management
Teaches emotion and reaction management techniques, recommended
for clients who exhibit aggressive behaviors or have property-related
offenses.
Substance Abuse Treatment
Helps overcome substance dependencies, recommended for clients
with drug-related offenses or primary drug use.
Domestic Violence Counsel-
ing"
REFERENCES,0.6434782608695652,"Addresses and modifies violent behavior patterns, recommended for
clients involved in violent incidents.
Sex Offender Counseling
Focuses on behavior modification and preventing recidivism, recom-
mended for clients with sex-related offenses."
REFERENCES,0.6445652173913043,Table 1: Available Programs
REFERENCES,0.6456521739130435,"F
Data Description
508"
REFERENCES,0.6467391304347826,Table 2: Categorical Covariates Summary Statistics (N/A or Other Categories are Omitted).
REFERENCES,0.6478260869565218,"Variable
Categories
County"
REFERENCES,0.6489130434782608,"DuPage
Cook
Will
Peoria"
REFERENCES,0.65,"Risk
Highest
24.3
32.0
2.3
1.0
High
60.7
26.2
35.1
24.7
Medium
11.0
15.6
42.1
47.0
AdOffense
Drugs
43.0
67.8
31.7
37.0
Property
31.1
17.6
52.5
46.3
DUI
11.1
2.3
3.8
1.0
OffenseClass
Class 4
42.5
–
11.5
20.6
Class 3
13.5
–
5.7
5.7
Class 2
16.0
–
5.7
5.1
Pdrug
Heroin
27.0
43.6
32.3
9.5
THC
18.6
18.5
17.5
21.6
Coc.Crack
7.8
10.9
21.0
11.6
ReferralReason
Tech Violation
31.2
0.0
12.8
0.0
3/4 Felon
20.5
70.5
59.2
80.0
1/2 Felon
9.8
16.5
23.7
14.7
WhoReferred
Prob Officer
64.7
97.3
1.8
0.0
Judge
32.0
1.3
0.7
91.3
Pub. Defender
0.6
0.0
75.3
2.8
Gender
Female
25.2
21.3
21.7
19.8
Male
74.8
77.5
78.2
80.0
EmplymntS
Full Time
49.7
85.7
38.2
6.7
None
32.3
4.8
59.2
92.0
Part Time
18.0
9.4
2.7
1.3
MaritalS
Single
86.4
85.6
15.0
22.9
Married
5.9
7.1
1.8
5.7
Divorced
4.7
2.3
0.2
1.8
EducationS
HighSchool
40.3
37.2
34.3
13.6
No HighSchool
32.6
52.4
10.8
12.3
Some College
19.4
3.5
11.8
4.4
or Graduated
HousingS
Friend or
62.3
27.9
6.2
17.7
Family
Own/Rent
29.0
15.5
2.7
11.1
No Home
5.9
23.9
16.5
70.2
Reported
MedicaidS
Yes
23.8
48.4
8.3
3.3
UniqueAgents
4
11.6
2.2
8.6
–
3
27.9
31.9
22.3
2.3
2
60.6
65.9
69.1
97.7
FinalProgPhase
Level 3/4
11.1
15.7
32.3
0.3
Level 1/2
56.5
14.4
22.7
3.1
Level 0
2.9
35.5
7.0
27.0
RewardedBehv
Yes
4.0
29.1
2.5
1.5
Sanctions
Yes
91.8
99.3
89.8
41.1"
REFERENCES,0.6510869565217391,Figure 10: Correlation Matrix of features
REFERENCES,0.6521739130434783,"NeurIPS Paper Checklist
509"
CLAIMS,0.6532608695652173,"1. Claims
510"
CLAIMS,0.6543478260869565,"Question: Do the main claims made in the abstract and introduction accurately reflect the
511"
CLAIMS,0.6554347826086957,"paper’s contributions and scope?
512"
CLAIMS,0.6565217391304348,"Answer: [Yes]
513"
CLAIMS,0.657608695652174,"Justification: Our abstract and introduction accurately reflect the paper’s contribution and
514"
CLAIMS,0.658695652173913,"scope.
515"
CLAIMS,0.6597826086956522,"Guidelines:
516"
CLAIMS,0.6608695652173913,"• The answer NA means that the abstract and introduction do not include the claims
517"
CLAIMS,0.6619565217391304,"made in the paper.
518"
CLAIMS,0.6630434782608695,"• The abstract and/or introduction should clearly state the claims made, including the
519"
CLAIMS,0.6641304347826087,"contributions made in the paper and important assumptions and limitations. A No or
520"
CLAIMS,0.6652173913043479,"NA answer to this question will not be perceived well by the reviewers.
521"
CLAIMS,0.6663043478260869,"• The claims made should match theoretical and experimental results, and reflect how
522"
CLAIMS,0.6673913043478261,"much the results can be expected to generalize to other settings.
523"
CLAIMS,0.6684782608695652,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
524"
CLAIMS,0.6695652173913044,"are not attained by the paper.
525"
LIMITATIONS,0.6706521739130434,"2. Limitations
526"
LIMITATIONS,0.6717391304347826,"Question: Does the paper discuss the limitations of the work performed by the authors?
527"
LIMITATIONS,0.6728260869565217,"Answer:[Yes]
528"
LIMITATIONS,0.6739130434782609,"Justification: Yes. We discuss the limitations in section 7.
529"
LIMITATIONS,0.675,"Guidelines:
530"
LIMITATIONS,0.6760869565217391,"• The answer NA means that the paper has no limitation while the answer No means that
531"
LIMITATIONS,0.6771739130434783,"the paper has limitations, but those are not discussed in the paper.
532"
LIMITATIONS,0.6782608695652174,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
533"
LIMITATIONS,0.6793478260869565,"• The paper should point out any strong assumptions and how robust the results are to
534"
LIMITATIONS,0.6804347826086956,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
535"
LIMITATIONS,0.6815217391304348,"model well-specification, asymptotic approximations only holding locally). The authors
536"
LIMITATIONS,0.6826086956521739,"should reflect on how these assumptions might be violated in practice and what the
537"
LIMITATIONS,0.683695652173913,"implications would be.
538"
LIMITATIONS,0.6847826086956522,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
539"
LIMITATIONS,0.6858695652173913,"only tested on a few datasets or with a few runs. In general, empirical results often
540"
LIMITATIONS,0.6869565217391305,"depend on implicit assumptions, which should be articulated.
541"
LIMITATIONS,0.6880434782608695,"• The authors should reflect on the factors that influence the performance of the approach.
542"
LIMITATIONS,0.6891304347826087,"For example, a facial recognition algorithm may perform poorly when image resolution
543"
LIMITATIONS,0.6902173913043478,"is low or images are taken in low lighting. Or a speech-to-text system might not be
544"
LIMITATIONS,0.691304347826087,"used reliably to provide closed captions for online lectures because it fails to handle
545"
LIMITATIONS,0.6923913043478261,"technical jargon.
546"
LIMITATIONS,0.6934782608695652,"• The authors should discuss the computational efficiency of the proposed algorithms
547"
LIMITATIONS,0.6945652173913044,"and how they scale with dataset size.
548"
LIMITATIONS,0.6956521739130435,"• If applicable, the authors should discuss possible limitations of their approach to
549"
LIMITATIONS,0.6967391304347826,"address problems of privacy and fairness.
550"
LIMITATIONS,0.6978260869565217,"• While the authors might fear that complete honesty about limitations might be used by
551"
LIMITATIONS,0.6989130434782609,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
552"
LIMITATIONS,0.7,"limitations that aren’t acknowledged in the paper. The authors should use their best
553"
LIMITATIONS,0.7010869565217391,"judgment and recognize that individual actions in favor of transparency play an impor-
554"
LIMITATIONS,0.7021739130434783,"tant role in developing norms that preserve the integrity of the community. Reviewers
555"
LIMITATIONS,0.7032608695652174,"will be specifically instructed to not penalize honesty concerning limitations.
556"
THEORY ASSUMPTIONS AND PROOFS,0.7043478260869566,"3. Theory Assumptions and Proofs
557"
THEORY ASSUMPTIONS AND PROOFS,0.7054347826086956,"Question: For each theoretical result, does the paper provide the full set of assumptions and
558"
THEORY ASSUMPTIONS AND PROOFS,0.7065217391304348,"a complete (and correct) proof?
559"
THEORY ASSUMPTIONS AND PROOFS,0.7076086956521739,"Answer: [Yes]
560"
THEORY ASSUMPTIONS AND PROOFS,0.7086956521739131,"Justification: We have theoretical result in section 3, and we have more detailed proof in
561"
THEORY ASSUMPTIONS AND PROOFS,0.7097826086956521,"Appendix section A.
562"
THEORY ASSUMPTIONS AND PROOFS,0.7108695652173913,"Guidelines:
563"
THEORY ASSUMPTIONS AND PROOFS,0.7119565217391305,"• The answer NA means that the paper does not include theoretical results.
564"
THEORY ASSUMPTIONS AND PROOFS,0.7130434782608696,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
565"
THEORY ASSUMPTIONS AND PROOFS,0.7141304347826087,"referenced.
566"
THEORY ASSUMPTIONS AND PROOFS,0.7152173913043478,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
567"
THEORY ASSUMPTIONS AND PROOFS,0.716304347826087,"• The proofs can either appear in the main paper or the supplemental material, but if
568"
THEORY ASSUMPTIONS AND PROOFS,0.717391304347826,"they appear in the supplemental material, the authors are encouraged to provide a short
569"
THEORY ASSUMPTIONS AND PROOFS,0.7184782608695652,"proof sketch to provide intuition.
570"
THEORY ASSUMPTIONS AND PROOFS,0.7195652173913043,"• Inversely, any informal proof provided in the core of the paper should be complemented
571"
THEORY ASSUMPTIONS AND PROOFS,0.7206521739130435,"by formal proofs provided in appendix or supplemental material.
572"
THEORY ASSUMPTIONS AND PROOFS,0.7217391304347827,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7228260869565217,"4. Experimental Result Reproducibility
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7239130434782609,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.725,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7260869565217392,"of the paper (regardless of whether the code and data are provided or not)?
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7271739130434782,"Answer: [Yes]
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7282608695652174,"Justification: We provide all information needed to reproduce the main experimental results
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7293478260869565,"in section 5. We have provided all implementation detail for reproduction.
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7304347826086957,"Guidelines:
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7315217391304348,"• The answer NA means that the paper does not include experiments.
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7326086956521739,"• If the paper includes experiments, a No answer to this question will not be perceived
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7336956521739131,"well by the reviewers: Making the paper reproducible is important, regardless of
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7347826086956522,"whether the code and data are provided or not.
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7358695652173913,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7369565217391304,"to make their results reproducible or verifiable.
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7380434782608696,"• Depending on the contribution, reproducibility can be accomplished in various ways.
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7391304347826086,"For example, if the contribution is a novel architecture, describing the architecture fully
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7402173913043478,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.741304347826087,"be necessary to either make it possible for others to replicate the model with the same
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7423913043478261,"dataset, or provide access to the model. In general. releasing code and data is often
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7434782608695653,"one good way to accomplish this, but reproducibility can also be provided via detailed
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7445652173913043,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7456521739130435,"of a large language model), releasing of a model checkpoint, or other means that are
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7467391304347826,"appropriate to the research performed.
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7478260869565218,"• While NeurIPS does not require releasing code, the conference does require all submis-
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7489130434782608,"sions to provide some reasonable avenue for reproducibility, which may depend on the
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"nature of the contribution. For example
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7510869565217392,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7521739130434782,"to reproduce that algorithm.
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7532608695652174,"(b) If the contribution is primarily a new model architecture, the paper should describe
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7543478260869565,"the architecture clearly and fully.
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7554347826086957,"(c) If the contribution is a new model (e.g., a large language model), then there should
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7565217391304347,"either be a way to access this model for reproducing the results or a way to reproduce
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7576086956521739,"the model (e.g., with an open-source dataset or instructions for how to construct
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.758695652173913,"the dataset).
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597826086956522,"(d) We recognize that reproducibility may be tricky in some cases, in which case
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7608695652173914,"authors are welcome to describe the particular way they provide for reproducibility.
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7619565217391304,"In the case of closed-source models, it may be that access to the model is limited in
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7630434782608696,"some way (e.g., to registered users), but it should be possible for other researchers
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7641304347826087,"to have some path to reproducing or verifying the results.
612"
OPEN ACCESS TO DATA AND CODE,0.7652173913043478,"5. Open access to data and code
613"
OPEN ACCESS TO DATA AND CODE,0.7663043478260869,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
614"
OPEN ACCESS TO DATA AND CODE,0.7673913043478261,"tions to faithfully reproduce the main experimental results, as described in supplemental
615"
OPEN ACCESS TO DATA AND CODE,0.7684782608695652,"material?
616"
OPEN ACCESS TO DATA AND CODE,0.7695652173913043,"Answer: [No]
617"
OPEN ACCESS TO DATA AND CODE,0.7706521739130435,"Justification: Access to the data and code is restricted under the terms of the non-disclosure
618"
OPEN ACCESS TO DATA AND CODE,0.7717391304347826,"agreement signed with our data-providing partner. The code includes sensitive details perti-
619"
OPEN ACCESS TO DATA AND CODE,0.7728260869565218,"nent to the data, such as specific information embedded within the prompts. Consequently,
620"
OPEN ACCESS TO DATA AND CODE,0.7739130434782608,"we are unable to share the code at this time.
621"
OPEN ACCESS TO DATA AND CODE,0.775,"Guidelines:
622"
OPEN ACCESS TO DATA AND CODE,0.7760869565217391,"• The answer NA means that paper does not include experiments requiring code.
623"
OPEN ACCESS TO DATA AND CODE,0.7771739130434783,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
624"
OPEN ACCESS TO DATA AND CODE,0.7782608695652173,"public/guides/CodeSubmissionPolicy) for more details.
625"
OPEN ACCESS TO DATA AND CODE,0.7793478260869565,"• While we encourage the release of code and data, we understand that this might not be
626"
OPEN ACCESS TO DATA AND CODE,0.7804347826086957,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
627"
OPEN ACCESS TO DATA AND CODE,0.7815217391304348,"including code, unless this is central to the contribution (e.g., for a new open-source
628"
OPEN ACCESS TO DATA AND CODE,0.782608695652174,"benchmark).
629"
OPEN ACCESS TO DATA AND CODE,0.783695652173913,"• The instructions should contain the exact command and environment needed to run to
630"
OPEN ACCESS TO DATA AND CODE,0.7847826086956522,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
631"
OPEN ACCESS TO DATA AND CODE,0.7858695652173913,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
632"
OPEN ACCESS TO DATA AND CODE,0.7869565217391304,"• The authors should provide instructions on data access and preparation, including how
633"
OPEN ACCESS TO DATA AND CODE,0.7880434782608695,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
634"
OPEN ACCESS TO DATA AND CODE,0.7891304347826087,"• The authors should provide scripts to reproduce all experimental results for the new
635"
OPEN ACCESS TO DATA AND CODE,0.7902173913043479,"proposed method and baselines. If only a subset of experiments are reproducible, they
636"
OPEN ACCESS TO DATA AND CODE,0.7913043478260869,"should state which ones are omitted from the script and why.
637"
OPEN ACCESS TO DATA AND CODE,0.7923913043478261,"• At submission time, to preserve anonymity, the authors should release anonymized
638"
OPEN ACCESS TO DATA AND CODE,0.7934782608695652,"versions (if applicable).
639"
OPEN ACCESS TO DATA AND CODE,0.7945652173913044,"• Providing as much information as possible in supplemental material (appended to the
640"
OPEN ACCESS TO DATA AND CODE,0.7956521739130434,"paper) is recommended, but including URLs to data and code is permitted.
641"
OPEN ACCESS TO DATA AND CODE,0.7967391304347826,"6. Experimental Setting/Details
642"
OPEN ACCESS TO DATA AND CODE,0.7978260869565217,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
643"
OPEN ACCESS TO DATA AND CODE,0.7989130434782609,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
644"
OPEN ACCESS TO DATA AND CODE,0.8,"results?
645"
OPEN ACCESS TO DATA AND CODE,0.8010869565217391,"Answer: [Yes]
646"
OPEN ACCESS TO DATA AND CODE,0.8021739130434783,"Justification: We clarify all experiment setting in section 5. We also provide more training
647"
OPEN ACCESS TO DATA AND CODE,0.8032608695652174,"details on Appendix section C.
648"
OPEN ACCESS TO DATA AND CODE,0.8043478260869565,"Guidelines:
649"
OPEN ACCESS TO DATA AND CODE,0.8054347826086956,"• The answer NA means that the paper does not include experiments.
650"
OPEN ACCESS TO DATA AND CODE,0.8065217391304348,"• The experimental setting should be presented in the core of the paper to a level of detail
651"
OPEN ACCESS TO DATA AND CODE,0.8076086956521739,"that is necessary to appreciate the results and make sense of them.
652"
OPEN ACCESS TO DATA AND CODE,0.808695652173913,"• The full details can be provided either with the code, in appendix, or as supplemental
653"
OPEN ACCESS TO DATA AND CODE,0.8097826086956522,"material.
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8108695652173913,"7. Experiment Statistical Significance
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8119565217391305,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8130434782608695,"information about the statistical significance of the experiments?
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8141304347826087,"Answer: [Yes]
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8152173913043478,"Justification: We change the random seed during the train/test splitting, and repeat the
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.816304347826087,"experiment 5 times with different seed. The standard deviation of results are within 0.02.
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8173913043478261,"Guidelines:
661"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8184782608695652,"• The answer NA means that the paper does not include experiments.
662"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8195652173913044,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
663"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8206521739130435,"dence intervals, or statistical significance tests, at least for the experiments that support
664"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8217391304347826,"the main claims of the paper.
665"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8228260869565217,"• The factors of variability that the error bars are capturing should be clearly stated (for
666"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8239130434782609,"example, train/test split, initialization, random drawing of some parameter, or overall
667"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.825,"run with given experimental conditions).
668"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8260869565217391,"• The method for calculating the error bars should be explained (closed form formula,
669"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8271739130434783,"call to a library function, bootstrap, etc.)
670"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8282608695652174,"• The assumptions made should be given (e.g., Normally distributed errors).
671"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8293478260869566,"• It should be clear whether the error bar is the standard deviation or the standard error
672"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8304347826086956,"of the mean.
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8315217391304348,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326086956521739,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
675"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336956521739131,"of Normality of errors is not verified.
676"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8347826086956521,"• For asymmetric distributions, the authors should be careful not to show in tables or
677"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8358695652173913,"figures symmetric error bars that would yield results that are out of range (e.g. negative
678"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8369565217391305,"error rates).
679"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8380434782608696,"• If error bars are reported in tables or plots, The authors should explain in the text how
680"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8391304347826087,"they were calculated and reference the corresponding figures or tables in the text.
681"
EXPERIMENTS COMPUTE RESOURCES,0.8402173913043478,"8. Experiments Compute Resources
682"
EXPERIMENTS COMPUTE RESOURCES,0.841304347826087,"Question: For each experiment, does the paper provide sufficient information on the com-
683"
EXPERIMENTS COMPUTE RESOURCES,0.842391304347826,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
684"
EXPERIMENTS COMPUTE RESOURCES,0.8434782608695652,"the experiments?
685"
EXPERIMENTS COMPUTE RESOURCES,0.8445652173913043,"Answer: [Yes]
686"
EXPERIMENTS COMPUTE RESOURCES,0.8456521739130435,"Justification: We have a brief introduction of experiments compute resources in section 5.
687"
EXPERIMENTS COMPUTE RESOURCES,0.8467391304347827,"We have more detailed information in Appendix section C.
688"
EXPERIMENTS COMPUTE RESOURCES,0.8478260869565217,"Guidelines:
689"
EXPERIMENTS COMPUTE RESOURCES,0.8489130434782609,"• The answer NA means that the paper does not include experiments.
690"
EXPERIMENTS COMPUTE RESOURCES,0.85,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
691"
EXPERIMENTS COMPUTE RESOURCES,0.8510869565217392,"or cloud provider, including relevant memory and storage.
692"
EXPERIMENTS COMPUTE RESOURCES,0.8521739130434782,"• The paper should provide the amount of compute required for each of the individual
693"
EXPERIMENTS COMPUTE RESOURCES,0.8532608695652174,"experimental runs as well as estimate the total compute.
694"
EXPERIMENTS COMPUTE RESOURCES,0.8543478260869565,"• The paper should disclose whether the full research project required more compute
695"
EXPERIMENTS COMPUTE RESOURCES,0.8554347826086957,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
696"
EXPERIMENTS COMPUTE RESOURCES,0.8565217391304348,"didn’t make it into the paper).
697"
CODE OF ETHICS,0.8576086956521739,"9. Code Of Ethics
698"
CODE OF ETHICS,0.8586956521739131,"Question: Does the research conducted in the paper conform, in every respect, with the
699"
CODE OF ETHICS,0.8597826086956522,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
700"
CODE OF ETHICS,0.8608695652173913,"Answer: [Yes]
701"
CODE OF ETHICS,0.8619565217391304,"Justification: We reviewed the NeurIPS Code of Ethics. The research conducted in the paper
702"
CODE OF ETHICS,0.8630434782608696,"conform, in every respect, with the NeurIPS Code of Ethics.
703"
CODE OF ETHICS,0.8641304347826086,"Guidelines:
704"
CODE OF ETHICS,0.8652173913043478,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
705"
CODE OF ETHICS,0.866304347826087,"• If the authors answer No, they should explain the special circumstances that require a
706"
CODE OF ETHICS,0.8673913043478261,"deviation from the Code of Ethics.
707"
CODE OF ETHICS,0.8684782608695653,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
708"
CODE OF ETHICS,0.8695652173913043,"eration due to laws or regulations in their jurisdiction).
709"
BROADER IMPACTS,0.8706521739130435,"10. Broader Impacts
710"
BROADER IMPACTS,0.8717391304347826,"Question: Does the paper discuss both potential positive societal impacts and negative
711"
BROADER IMPACTS,0.8728260869565218,"societal impacts of the work performed?
712"
BROADER IMPACTS,0.8739130434782608,"Answer: [Yes]
713"
BROADER IMPACTS,0.875,"Justification: We discuss the potential societal impacts in the section 1 Introduction, and
714"
BROADER IMPACTS,0.8760869565217392,"section7 Discussion.
715"
BROADER IMPACTS,0.8771739130434782,"Guidelines:
716"
BROADER IMPACTS,0.8782608695652174,"• The answer NA means that there is no societal impact of the work performed.
717"
BROADER IMPACTS,0.8793478260869565,"• If the authors answer NA or No, they should explain why their work has no societal
718"
BROADER IMPACTS,0.8804347826086957,"impact or why the paper does not address societal impact.
719"
BROADER IMPACTS,0.8815217391304347,"• Examples of negative societal impacts include potential malicious or unintended uses
720"
BROADER IMPACTS,0.8826086956521739,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
721"
BROADER IMPACTS,0.883695652173913,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
722"
BROADER IMPACTS,0.8847826086956522,"groups), privacy considerations, and security considerations.
723"
BROADER IMPACTS,0.8858695652173914,"• The conference expects that many papers will be foundational research and not tied
724"
BROADER IMPACTS,0.8869565217391304,"to particular applications, let alone deployments. However, if there is a direct path to
725"
BROADER IMPACTS,0.8880434782608696,"any negative applications, the authors should point it out. For example, it is legitimate
726"
BROADER IMPACTS,0.8891304347826087,"to point out that an improvement in the quality of generative models could be used to
727"
BROADER IMPACTS,0.8902173913043478,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
728"
BROADER IMPACTS,0.8913043478260869,"that a generic algorithm for optimizing neural networks could enable people to train
729"
BROADER IMPACTS,0.8923913043478261,"models that generate Deepfakes faster.
730"
BROADER IMPACTS,0.8934782608695652,"• The authors should consider possible harms that could arise when the technology is
731"
BROADER IMPACTS,0.8945652173913043,"being used as intended and functioning correctly, harms that could arise when the
732"
BROADER IMPACTS,0.8956521739130435,"technology is being used as intended but gives incorrect results, and harms following
733"
BROADER IMPACTS,0.8967391304347826,"from (intentional or unintentional) misuse of the technology.
734"
BROADER IMPACTS,0.8978260869565218,"• If there are negative societal impacts, the authors could also discuss possible mitigation
735"
BROADER IMPACTS,0.8989130434782608,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
736"
BROADER IMPACTS,0.9,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
737"
BROADER IMPACTS,0.9010869565217391,"feedback over time, improving the efficiency and accessibility of ML).
738"
SAFEGUARDS,0.9021739130434783,"11. Safeguards
739"
SAFEGUARDS,0.9032608695652173,"Question: Does the paper describe safeguards that have been put in place for responsible
740"
SAFEGUARDS,0.9043478260869565,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
741"
SAFEGUARDS,0.9054347826086957,"image generators, or scraped datasets)?
742"
SAFEGUARDS,0.9065217391304348,"Answer: [Yes]
743"
SAFEGUARDS,0.907608695652174,"Justification: We promote human-in-the loop verification and emphasized on domain exper-
744"
SAFEGUARDS,0.908695652173913,"tise. Moreover,we are not gonna make the dataset public - the framework is genralizable but
745"
SAFEGUARDS,0.9097826086956522,"we caution users to be aware of bias and use human-in the loop verification.
746"
SAFEGUARDS,0.9108695652173913,"Guidelines:
747"
SAFEGUARDS,0.9119565217391304,"• The answer NA means that the paper poses no such risks.
748"
SAFEGUARDS,0.9130434782608695,"• Released models that have a high risk for misuse or dual-use should be released with
749"
SAFEGUARDS,0.9141304347826087,"necessary safeguards to allow for controlled use of the model, for example by requiring
750"
SAFEGUARDS,0.9152173913043479,"that users adhere to usage guidelines or restrictions to access the model or implementing
751"
SAFEGUARDS,0.9163043478260869,"safety filters.
752"
SAFEGUARDS,0.9173913043478261,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
753"
SAFEGUARDS,0.9184782608695652,"should describe how they avoided releasing unsafe images.
754"
SAFEGUARDS,0.9195652173913044,"• We recognize that providing effective safeguards is challenging, and many papers do
755"
SAFEGUARDS,0.9206521739130434,"not require this, but we encourage authors to take this into account and make a best
756"
SAFEGUARDS,0.9217391304347826,"faith effort.
757"
LICENSES FOR EXISTING ASSETS,0.9228260869565217,"12. Licenses for existing assets
758"
LICENSES FOR EXISTING ASSETS,0.9239130434782609,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
759"
LICENSES FOR EXISTING ASSETS,0.925,"the paper, properly credited and are the license and terms of use explicitly mentioned and
760"
LICENSES FOR EXISTING ASSETS,0.9260869565217391,"properly respected?
761"
LICENSES FOR EXISTING ASSETS,0.9271739130434783,"Answer: [NA]
762"
LICENSES FOR EXISTING ASSETS,0.9282608695652174,"Justification: Our paper does not use existing assets.
763"
LICENSES FOR EXISTING ASSETS,0.9293478260869565,"Guidelines:
764"
LICENSES FOR EXISTING ASSETS,0.9304347826086956,"• The answer NA means that the paper does not use existing assets.
765"
LICENSES FOR EXISTING ASSETS,0.9315217391304348,"• The authors should cite the original paper that produced the code package or dataset.
766"
LICENSES FOR EXISTING ASSETS,0.9326086956521739,"• The authors should state which version of the asset is used and, if possible, include a
767"
LICENSES FOR EXISTING ASSETS,0.933695652173913,"URL.
768"
LICENSES FOR EXISTING ASSETS,0.9347826086956522,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
769"
LICENSES FOR EXISTING ASSETS,0.9358695652173913,"• For scraped data from a particular source (e.g., website), the copyright and terms of
770"
LICENSES FOR EXISTING ASSETS,0.9369565217391305,"service of that source should be provided.
771"
LICENSES FOR EXISTING ASSETS,0.9380434782608695,"• If assets are released, the license, copyright information, and terms of use in the
772"
LICENSES FOR EXISTING ASSETS,0.9391304347826087,"package should be provided. For popular datasets, paperswithcode.com/datasets
773"
LICENSES FOR EXISTING ASSETS,0.9402173913043478,"has curated licenses for some datasets. Their licensing guide can help determine the
774"
LICENSES FOR EXISTING ASSETS,0.941304347826087,"license of a dataset.
775"
LICENSES FOR EXISTING ASSETS,0.9423913043478261,"• For existing datasets that are re-packaged, both the original license and the license of
776"
LICENSES FOR EXISTING ASSETS,0.9434782608695652,"the derived asset (if it has changed) should be provided.
777"
LICENSES FOR EXISTING ASSETS,0.9445652173913044,"• If this information is not available online, the authors are encouraged to reach out to
778"
LICENSES FOR EXISTING ASSETS,0.9456521739130435,"the asset’s creators.
779"
NEW ASSETS,0.9467391304347826,"13. New Assets
780"
NEW ASSETS,0.9478260869565217,"Question: Are new assets introduced in the paper well documented and is the documentation
781"
NEW ASSETS,0.9489130434782609,"provided alongside the assets?
782"
NEW ASSETS,0.95,"Answer: [NA]
783"
NEW ASSETS,0.9510869565217391,"Justification: Our paper does not release new assets.
784"
NEW ASSETS,0.9521739130434783,"Guidelines:
785"
NEW ASSETS,0.9532608695652174,"• The answer NA means that the paper does not release new assets.
786"
NEW ASSETS,0.9543478260869566,"• Researchers should communicate the details of the dataset/code/model as part of their
787"
NEW ASSETS,0.9554347826086956,"submissions via structured templates. This includes details about training, license,
788"
NEW ASSETS,0.9565217391304348,"limitations, etc.
789"
NEW ASSETS,0.9576086956521739,"• The paper should discuss whether and how consent was obtained from people whose
790"
NEW ASSETS,0.9586956521739131,"asset is used.
791"
NEW ASSETS,0.9597826086956521,"• At submission time, remember to anonymize your assets (if applicable). You can either
792"
NEW ASSETS,0.9608695652173913,"create an anonymized URL or include an anonymized zip file.
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9619565217391305,"14. Crowdsourcing and Research with Human Subjects
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9630434782608696,"Question: For crowdsourcing experiments and research with human subjects, does the paper
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9641304347826087,"include the full text of instructions given to participants and screenshots, if applicable, as
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9652173913043478,"well as details about compensation (if any)?
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.966304347826087,"Answer: [NA]
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967391304347826,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684782608695652,"Guidelines:
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695652173913043,"• The answer NA means that the paper does not involve crowdsourcing nor research with
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9706521739130435,"human subjects.
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717391304347827,"• Including this information in the supplemental material is fine, but if the main contribu-
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728260869565217,"tion of the paper involves human subjects, then as much detail as possible should be
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739130434782609,"included in the main paper.
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760869565217392,"or other labor should be paid at least the minimum wage in the country of the data
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771739130434782,"collector.
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9782608695652174,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793478260869565,"Subjects
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804347826086957,"Question: Does the paper describe potential risks incurred by study participants, whether
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815217391304348,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826086956521739,"approvals (or an equivalent approval/review based on the requirements of your country or
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836956521739131,"institution) were obtained?
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847826086956522,"Answer: [Yes]
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858695652173913,"Justification: We received IRB approval from University of Chicago. The study title is
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869565217391304,"Data-driven Evaluation of Alternative Sentencing Allocation.
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880434782608696,"Guidelines:
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891304347826086,"• The answer NA means that the paper does not involve crowdsourcing nor research with
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902173913043478,"human subjects.
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991304347826087,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923913043478261,"may be required for any human subjects research. If you obtained IRB approval, you
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934782608695653,"should clearly state this in the paper.
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945652173913043,"• We recognize that the procedures for this may vary significantly between institutions
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956521739130435,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967391304347826,"guidelines for their institution.
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978260869565218,"• For initial submissions, do not include any information that would break anonymity (if
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989130434782608,"applicable), such as the institution conducting the review.
828"
