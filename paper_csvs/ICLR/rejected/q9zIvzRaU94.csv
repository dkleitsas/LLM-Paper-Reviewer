Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005780346820809248,"Causal discovery, i.e., inferring underlying cause-effect relationships from obser-
vations of a scene or system, is an inherent mechanism in human cognition, but
has been shown to be highly challenging to automate. The majority of approaches
in the literature aiming for this task consider constrained scenarios with fully ob-
served variables or data from stationary time-series.
In this work we aim for causal discovery in a more general class of scenarios,
scenes with non-stationary behavior over time. For our purposes we here regard
a scene as a composition objects interacting with each other over time. Non-
stationarity is modeled as stationarity conditioned on an underlying variable, a
state, which can be of varying dimension, more or less hidden given observations
of the scene, and also depend more or less directly on these observations.
We propose a probabilistic deep learning approach called State-Dependent Causal
Inference (SDCI) for causal discovery in such conditionally stationary time-series
data. Results in two different synthetic scenarios show that this method is able to
recover the underlying causal dependencies with high accuracy even in cases with
hidden states."
INTRODUCTION,0.011560693641618497,"1
INTRODUCTION"
INTRODUCTION,0.017341040462427744,"The ability of deep learning approaches to discover and reason about causal relationships in data has
become a prominent direction of work over the recent years (Yi et al., 2020; Girdhar & Ramanan,
2020; Sauer & Geiger, 2021). Despite the recent success of deep learning methods in related tasks
such as classiﬁcation, localization, and segmentation, causal discovery and reasoning, an inherent
mechanism in human cognition (Spelke & Kinzler, 2007) allowing reasoning about counterfactuals
and understanding the reasons of events, still poses an considerable challenge."
INTRODUCTION,0.023121387283236993,"Causal discovery involves uncovering the underlying logic, temporal and causal structure of the
observed processes in the data. Current approaches (see Section 2) commonly address quite con-
strained scenarios with a stationary behavior over time. In the present paper, we extend the current
work by addressing scenarios with conditional stationarity, where the dynamics of the observed
system changes with the value of underlying variables. This is the case in almost all real-world
scenarios, e.g. with people who behave differently and take different decisions depending on un-
derlying factors such as mood, previous experience, and the actions of other agents. We propose
a method (see Section 3) for causal discovery from time-series observations of systems where the
underlying causal graph changes depending on a state variable."
INTRODUCTION,0.028901734104046242,"The causal discovery task from such conditionally stationary time-series poses different challenges
depending on the observability of the underlying state variable. Four scenario classes can be seen:"
INTRODUCTION,0.03468208092485549,"1. The ﬁrst class concerns a simpliﬁed version of the problem, where the state variable is
observed and not dependent on other observed time-series data."
INTRODUCTION,0.04046242774566474,"2. In the second class of scenarios, the state is not directly observed, but directly dependent
on and continuously inferable from an observed variable. A real-life example is a trafﬁc
scenario where taxis (visually distinguishable by the sign on their roof) follow slightly
different rules than normal cars, i.e. are allowed to drive in bus lanes."
INTRODUCTION,0.046242774566473986,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05202312138728324,"3. A more challenging scenario class is when the state depends on earlier events, and thus is
not continuously observable. A real-life example is a chain of events in a football game,
where the action of one player is triggered by an earlier action by another player."
INTRODUCTION,0.057803468208092484,"4. Finally, a large share of scenarios in the real world are governed by underlying state vari-
ables that are not fully inferable from the observations from the scenario over time. In
such scenarios, the state is an unknown confounder to the observed time-series, and causal
discovery from such scenarios is inherently ill-deﬁned."
INTRODUCTION,0.06358381502890173,"We evaluate the method (see Section 4) in two different synthetic scenarios, where we vary the
complexity of system dynamics and observability of the underlying state variable covering the ﬁrst
three scenario classes above. Finally we conclude and discuss directions for future work (see Section
5)."
RELATED WORK,0.06936416184971098,"2
RELATED WORK"
RELATED WORK,0.07514450867052024,"Causal discovery approaches aim to identify causal relationships over a set of variables from obser-
vational data. These methods can basically be classiﬁed into three different types (Glymour et al.,
2019): 1) Constraint-based; 2) Score-based; 3) Functional causal model based methods."
RELATED WORK,0.08092485549132948,"Constraint-based methods rely on conditional independence tests to recover the underlying DAG
structure of the data, such as the PC algorithm (Spirtes et al., 2000), which assumes faithfulness and
causal Markov condition and considers i.i.d. sampling and no latent confounders. There exists a
great variety of variations of PC. One of them is the Fast Causal Inference (FCI) (Spirtes, 2001),
which is able to cope with the unknown confounders and selection bias; furthermore, it can be
adapted for time-series data, such as such as tsFCI (Entner & Hoyer, 2010)."
RELATED WORK,0.08670520231213873,"Score-based methods deﬁne score functions of causal graph structures and then optimize score
functions by performing a search to identify the underlying causal structure, such as the Greedy
Equivalence Search (GES) (Chickering, 2002). Notice that searching in the graph space poses a
combinatorial optimization problem. Recent approaches try to avoid this by reformulating it as a
continuous optimization problem which introduces a score function h for measuring the acyclicity
of the graph (Zheng et al., 2018). Regarding time-series data, these methods are reformulated as
learning dynamic Bayesian Networks (DBNs) from data (Murphy et al., 2002). Among these algo-
rithms we recently ﬁnd DYNOTEARS (Pamﬁl et al., 2020), which aims to simultaneously estimate
instantaneous and time-lagged relationships between variables in a time-series."
RELATED WORK,0.09248554913294797,"Functional causal model-based methods represent the effect as a function of its direct causes and
their independent immeasurable noise (Glymour et al., 2019). For non-temporal data, there are
linear non-Gaussian acyclic models (Shimizu et al., 2006), additive noise models (Peters et al.,
2014), post-nonlinear models (Zhang & Hyv¨arinen, 2009), etc. For temporal data, these approaches
ﬁt a dynamics model, which is often regularized in terms of its sparsity, and its form is analyzed
to identify the underlying causal connections in the data. Granger causal analysis falls into this
category, since we ﬁrst model the dynamics and some analysis is performed to extract the latent
causal structure (Granger, 1969)."
RELATED WORK,0.09826589595375723,"Causal discovery is, in general, a challenging task and its study arises a great amount of practical
issues. The problem is ill-posed when considering linearity and Gaussian disturbances, since it can
be proved that the underlying causal model is not identiﬁable, while under proper assumptions, such
as non-Gaussianity, it becomes identiﬁable (Shimizu et al., 2006). When considering non-linear
transformations, the symmetry between observed variables disappears, allowing the identiﬁcation
of causal relations in the context of Gaussian disturbances (Hoyer et al., 2008). Other practical
issues consist on the existence of latent confounders (Ranganath & Perotte, 2018), the presence of
measurement error (Zhang et al., 2017) or considering observations with missing data (Tu et al.,
2019). In order to avoid these common problems, simpliﬁcations of the problem need to be applied.
In fact, the assumptions we make in this work are: (i) all the instances belonging to the causal graph
are observed, (ii) we have no missing data selection bias, and (iii) no latent confounders exist."
RELATED WORK,0.10404624277456648,"The work most related to ours are the approaches by (L¨owe et al., 2020; Li et al., 2020); we extend
these by allowing the causal model of the underlying process to vary depending on a state variable.
Our method can in the future be applied to a wider class of non-stationary visual scenarios where the"
RELATED WORK,0.10982658959537572,Under review as a conference paper at ICLR 2022
RELATED WORK,0.11560693641618497,"interacting objects are only partially and noisily observed as semantically segmented visual regions,
or by tracked image keypoints (L¨owe et al., 2020). This would allow addressing challenging tasks
such as scene understanding, counterfactual reasoning, etc. The recent work by (Sauer & Geiger,
2021) also uses the concept of causality for a similar task, generation of counterfactual images."
STATE-DEPENDENT CAUSAL INFERENCE,0.12138728323699421,"3
STATE-DEPENDENT CAUSAL INFERENCE"
STATE-DEPENDENT CAUSAL INFERENCE,0.12716763005780346,"In this section, we introduce our formulation to extract causal graphs from time-series data where
their dynamics are altered by means of a categorical variable, referred to as their state. We refer to
our method as State-Dependent Causal Inference (SDCI)."
PROBLEM FORMULATION,0.1329479768786127,"3.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.13872832369942195,"The input consists of a set of N time-series which not only obey some dynamics that might change
over time but also undergo different states along the sequence. These states are responsible for the
changes in the dynamics of the sample. We observe the sequence for a total of T frames and we
denote the sample x as"
PROBLEM FORMULATION,0.14450867052023122,"x = {{x1
i }N
i=1, ..., {xT
i }N
i=1},
xt
i = {pt
i, st
i},
(1)"
PROBLEM FORMULATION,0.15028901734104047,"where {s1:T
i
}N
i=1 represents the hidden states and {p1:T
i
}N
i=1 are the observed quantities of interest.
For simplicity, we drop the subscript when referring to all the elements in a single time-step (e.g. xt,
pt, st, etc). In a causal graph, the observed quantities are represented by the vertices and the edge of
the causal graph represents the interaction type between vertices. We denote the amount of possible
interaction types by nϵ."
PROBLEM FORMULATION,0.15606936416184972,"Assumptions.
In this work, we assume that the data generation process obeys a structural causal
model (SCM) (Pearl, 2009), G1:T , and that the model satisﬁes the ﬁrst-order Markov property.
Moreover, according to the deﬁnitions of causality (Granger, 1969), we assume that edges of a
causal graph cannot go back in time. The ﬁrst assumption follows related works concerning samples
where the generative process also follows an SCM (L¨owe et al., 2020; Li et al., 2020). Although we
assume the ﬁrst-order Markov property, one can extend to the more general p −th order Markov
property in a more complex scenario."
PROBLEM FORMULATION,0.16184971098265896,"State-dependent causal inference.
Based on the assumptions we mainly focus on the non-
stationary causal graph, which means that we can ﬁnd different edge-types at different times. As for
an edge between two vertices, the edge-type interaction between two vertices changes according to
the state of the variable which is the source of the interaction. The main focus of our method consists
on extracting a causal summary graph G (also denoted as such by Li et al. (2020) and L¨owe et al.
(2020)). Previous approaches aiming for this task assume stationary time-series data and therefore,
this causal summary graph is constant. Nonetheless, since we condition the stationarity of the sam-
ples on the states, our causal summary graph is expressed by means of this categorical variable. In
other words, our method will extract K summary graphs, one per state considered. The edge-type
interaction can be then queried at each time-step t as follows:"
PROBLEM FORMULATION,0.1676300578034682,"zt
ij = Gij(st
i)
(2)"
PROBLEM FORMULATION,0.17341040462427745,"where zt
ij ∈{0, ..., nϵ −1} denotes the edge-type interaction from i to j at time-step t. Figure 1
illustrates this task. In general, this causal summary graph is speciﬁc to the input sample and hidden
from the model. Therefore, not only we require to design a parametrizable function to infer the
latent causal structure, but also to evaluate how this inference ﬁts to the actual dynamics observed
in the input sequence."
PROBLEM FORMULATION,0.1791907514450867,Let us denote the ﬁrst step of extracting the latent causal structure
PROBLEM FORMULATION,0.18497109826589594,"G(s) = fφ(p1:T , s1:T )
(3)"
PROBLEM FORMULATION,0.1907514450867052,"where fφ denotes a parameterizable function that receives all the observed sequence as input. The
next step is to ﬁt this extracted latent sturcture into our assumed ﬁrst-order Markov dynamics."
PROBLEM FORMULATION,0.19653179190751446,"(˜pt+1,˜st+1) = fψ(xt, G(st))
(4)"
PROBLEM FORMULATION,0.2023121387283237,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.20809248554913296,"Figure 1: SDCI aims to extract a causal summary graph that describes the edge-type interaction for
every pair of edges conditioned on the state of the source variable with respect to the interaction."
PROBLEM FORMULATION,0.2138728323699422,"where the parameterizable function fψ represents a one-step computation of the dynamics starting
from the observed values at time-step t. In this expression, we have deﬁned fψ to predict the value
of the states at the next time-step as well. However, in our experiments we will also consider settings
where the states are observed at all times and modelling the dynamics is only performed with respect
to the quantity p1:T . To allow for this setting, one only needs to exclude the state variable from
supervision. We provide more details in the next section."
PROBLEM FORMULATION,0.21965317919075145,"Objective.
Both the causal inference and dynamics modelling modules can be optimized by min-
imizing some objective deﬁned for the parameterizable functions fφ and fψ."
PROBLEM FORMULATION,0.2254335260115607,"min
φ,ψ"
PROBLEM FORMULATION,0.23121387283236994,"T −1
X"
PROBLEM FORMULATION,0.23699421965317918,"t=1
L(xt+1, fψ(xt, G(st))) + R(G(·)),
G(st) = fφ(x)
(5)"
PROBLEM FORMULATION,0.24277456647398843,"where R(·) is a regularizer on the extracted graph structure, which can be applied to enforce a
preferred interaction type."
IMPLEMENTATION,0.24855491329479767,"3.2
IMPLEMENTATION"
IMPLEMENTATION,0.2543352601156069,"Since the interactions that are considered in the scene are complex, we introduce uncertainty in
our approach. Therefore, we are interested in computing the probability distribution over the edge
type given two instances and the state of the source instance, p(zij|si, pi, pj). This implementation
gathers inspiration from related works concerning this type of tasks (Li et al., 2020; L¨owe et al.,
2020; Kipf et al., 2018)."
IMPLEMENTATION,0.26011560693641617,"The structure of the implementation is very similar to a VAE (Kingma & Welling, 2014). In this case,
the latent space is represented by the set of edges zij which are discrete variables. The latent edges
are conditioned to the state of the variable which is the source of the interaction, si. We allow for a
total of K states. Let zij be an edge from instance i to instance j. This value represents the type of
causal interaction that there is between i and j and will depend on the state of i. zt
ij ∈{0, ..., nϵ−1},
where nϵ is the amount of inter-object interactions considered in our setting. zt
ij = 0 means that
component i is not inﬂuenced by component j at time t. Any other value models a different type of
interaction. Since we condition the edge type on the state variable, our objective is more close to the
one deﬁned in CVAE (Sohn et al., 2015)."
IMPLEMENTATION,0.2658959537572254,"L = Eqφ(z|s,x)

log pψ(x|z)

+ KL
 
qφ(z|s, x)||p(z|s)

(6)"
IMPLEMENTATION,0.27167630057803466,"where p(z|s) is a prior deﬁned over the edge types conditioned on each state, which acts as a regu-
larizer over the inferred edge-type distribution. We use z to denote all the edges represented in the
latent space. In our settings, we set this prior to enforce a uniform distribution no matter what the
state is. We might ﬁnd applications where this prior could be leveraged to encourage sparsity in the
extracted graph structure."
IMPLEMENTATION,0.2774566473988439,"Encoder.
Following related approaches (L¨owe et al., 2020), we use graph neural networks as the
encoder. The model receives all the information from the sample available x1:T . We concatenate
p1:T with a one-hot representation of the state variable s1:T . We aim to extract an embedding that
represents the causal interaction conditioned on the state for every pair of elements present in the
sample:
φij = fφ(x1:T )ij
(7)"
IMPLEMENTATION,0.2832369942196532,Under review as a conference paper at ICLR 2022
IMPLEMENTATION,0.28901734104046245,"where φij ∈Rnϵ×K denotes the embedding for every pair i −→j. The approximate posterior
distribution conditioned on the state variable qφ(zij|k, x) is calculated as follows:"
IMPLEMENTATION,0.2947976878612717,"qφ(zij|k, x) = Θ(φijk/τ)
(8)"
IMPLEMENTATION,0.30057803468208094,"where k is the state to which the edge-type distribution of i −→j is conditioned and Θ denotes a soft-
max activation with temperature τ. We use φijk ∈Rnϵ to the note the embedding for i −→j at state
k. Since this formulation yields a discrete latent space, we relax the resulting categorical distribution
using the Gumble-softmax (Maddison et al., 2017) technique to enable back-propagation."
IMPLEMENTATION,0.3063583815028902,"Once we have inferred the approximate posterior distribution over the edge-types, we can sample
the interactions at every time-step t conditioning on st:"
IMPLEMENTATION,0.31213872832369943,"zt
ij ∼qφ(zij|st
i, x)
(9)"
IMPLEMENTATION,0.3179190751445087,"Notice that this requires sampling at each time-step t, which might pose great computational ex-
penses for large sequences. In practice, we sample right after determining the distribution of the
edge-types. Then, the interactions are queried for each i −→j depending on st
i:"
IMPLEMENTATION,0.3236994219653179,"wijk ∼qφ(zij|k, x),
zt
ij = wijk′,
k′ = st
i
(10)"
IMPLEMENTATION,0.32947976878612717,"where we have stored the sampled values using wijk ∈Rnϵ, which denotes the edge-type using a
one-hot representation. In our experiments, we refer to SDCI-Static or SDCI-Temporal for imple-
mentations based on MLPs or CNNs respectively. For more details, see the supplementary material."
IMPLEMENTATION,0.3352601156069364,"Decoder.
Let us describe the decoder pψ(x|z), which models the dynamics of the generative pro-
cess. Consider the dynamics modelling for element j. At each time t, we ﬁrst query the edge-types
depending on the states of the source variable i from the sampled quantities wijk (see equation 10)."
IMPLEMENTATION,0.34104046242774566,"zt
ij = wijk,
k = st
i
(11)"
IMPLEMENTATION,0.3468208092485549,The information along the predicted edge-type interactions is then retrieved as follows
IMPLEMENTATION,0.35260115606936415,"gt
ij =
X"
IMPLEMENTATION,0.3583815028901734,"e>0
1(zt
ij=e)fe(xt
i, xt
j)
(12)"
IMPLEMENTATION,0.36416184971098264,"{fe}nϵ−1
e=1
is a family of parametrizable functions, one deﬁned for each edge type excluding the
no-edge interaction."
IMPLEMENTATION,0.3699421965317919,"The inter-object interactions are ﬁnally integrated to model the dynamics of each variable. The
updates of the variable pt+1
j
˜pt+1
j
= pt
j + fp
 X"
IMPLEMENTATION,0.37572254335260113,"i̸=j
gt
ij, xt
j

(13)"
IMPLEMENTATION,0.3815028901734104,"are predicted by means of fp, where it provides information about the update between the actual and
the next time-step. The supervision of the decoder considering p1:T uses the negative log-likelihood
of the following probability distribution"
IMPLEMENTATION,0.3872832369942196,"pψ(pt+1
j
|xt, zt) = N(˜pt+1
j
, σ2I)
(14)"
IMPLEMENTATION,0.3930635838150289,where σ is the variance of the resulting Gaussian distribution.
IMPLEMENTATION,0.3988439306358382,"We have previously introduced the idea of considering the state variable s1:T as being independent
from the dynamics of the sample, i.e. p1:T . If this happens, our method will have information about
the states of the samples at all times and consequently, no supervision upon s1:T will be required.
However, we might ﬁnd practical to consider the setting where the state is in fact dependent on p1:T ,
and therefore our method will require to model its behavior and perform supervision. In this case,
we present a straightforward approach to compute the estimation of the next state:"
IMPLEMENTATION,0.4046242774566474,"˜st+1
j
= fs
 X"
IMPLEMENTATION,0.41040462427745666,"i̸=j
gt
ij, xt
i

(15)"
IMPLEMENTATION,0.4161849710982659,and the corresponding categorical distribution can be computed as follows
IMPLEMENTATION,0.42196531791907516,"pψ(st+1
j
|xt, zt) = Θ(˜st+1
j
)
(16)"
IMPLEMENTATION,0.4277456647398844,where Θ is a softmax activation.
IMPLEMENTATION,0.43352601156069365,Under review as a conference paper at ICLR 2022
IMPLEMENTATION,0.4393063583815029,"In summary, the probability distribution of the decoder can be expressed as follows:"
IMPLEMENTATION,0.44508670520231214,pψ(x|z) =
IMPLEMENTATION,0.4508670520231214,"T −1
Y"
IMPLEMENTATION,0.45664739884393063,"t=1
pψ(xt+1|xt, zt),
pψ(xt+1|xt, zt) = pψ(pt+1|xt, zt)pψ(st+1|xt, zt)
(17)"
IMPLEMENTATION,0.4624277456647399,"Notice that with this formulation, p and s can be expressed as separate factors. We ﬁnd this result
useful, since in practice we calculate the log-likelihood of both probability distributions separately
and balance the resulting loss term"
IMPLEMENTATION,0.4682080924855491,"L = Eqφ(z|s,x)

log pψ(p|z)

+ λEqφ(z|s,x)

log pψ(s|z)

+ KL
 
qφ(z|s, x)||p(z|s)

(18)"
IMPLEMENTATION,0.47398843930635837,where we use λ to balance the contribution of both terms to the optimization process.
IMPLEMENTATION,0.4797687861271676,"Hidden state regime.
When considering the state as a variable hidden from the input data, we can
still sample from the estimated posterior distribution as follows:"
IMPLEMENTATION,0.48554913294797686,"zt
ij ∼ K
X"
IMPLEMENTATION,0.4913294797687861,"k=1
qφ(zij|k, x)p(k|x),
st
i ∼p(k|x)
(19)"
IMPLEMENTATION,0.49710982658959535,"where p(k|x) is the probability distribution which models the states at each time-step. In this setting,
we modify Eq. 16 and condition solely on the object dynamic variables."
IMPLEMENTATION,0.5028901734104047,"pψ(st
i|xt) = Θ(˜st
i/γ),
˜st
i = fs′(xt
i)
(20)"
IMPLEMENTATION,0.5086705202312138,"γ < 1 is a temperature factor which increases the conﬁdence of the model predictions. Since we
should be sampling from Eq. 19 and we consider p(k|x) = pψ(k|xt), in practice we have"
IMPLEMENTATION,0.5144508670520231,"zt
ij = K
X"
IMPLEMENTATION,0.5202312138728323,"k=1
wijk · pψ(k|xt)
(21)"
EXPERIMENTS,0.5260115606936416,"4
EXPERIMENTS"
EXPERIMENTS,0.5317919075144508,"We now present the experiments that have been performed to evaluate the method and compare it
to the baseline method, amortized causal discovery (ACD) (L¨owe et al., 2020). All models have
been implemented using Pytorch (Paszke et al., 2019) and all training and test processes have been
carried out on NVIDIA RTX 2080Ti GPUs."
EXPERIMENTS ON LINEAR DATA,0.5375722543352601,"4.1
EXPERIMENTS ON LINEAR DATA"
EXPERIMENTS ON LINEAR DATA,0.5433526011560693,"We start with a simple scenario with linear message passing operations between a number of differ-
ent time-series, where each time-step t in series i is a one-dimensional continuous variable pt
i ∈R."
EXPERIMENTS ON LINEAR DATA,0.5491329479768786,"The variables are connected by edges of nϵ different types. Each edge-type represents a different
causal effect that one variable can perform to another. This is denoted by {βk}nϵ−1
k=1 . β0 represents
no connection, i.e., no causal interaction between a pair of variables. At each time-step t, each
element in the sample performs the following operation to update pi:"
EXPERIMENTS ON LINEAR DATA,0.5549132947976878,"pt+1
i
= αpt
i + N
X"
EXPERIMENTS ON LINEAR DATA,0.5606936416184971,"i̸=j
βkpt
j
(22)"
EXPERIMENTS ON LINEAR DATA,0.5664739884393064,"where α ∈R controls the self-connection, and βk ∈R represents the edge-type connection between
j and i, of type k."
EXPERIMENTS ON LINEAR DATA,0.5722543352601156,"We carry out experiments in this scenario to compare the results obtained by the two proposed
architectures and the baseline method ACD L¨owe et al. (2020). We are interested in observing
whether our formulation is capable of recovering the underlying parameters of the linear message
passing mechanism and identify the causal interactions in each sample in different types of scenarios
(see Section 1). In this case, we experiment with observed states that are independent from the
observations, and hidden states (ﬁrst and third scenario classes). Unless noted otherwise, we set
K = 2 states."
EXPERIMENTS ON LINEAR DATA,0.5780346820809249,Under review as a conference paper at ICLR 2022
EXPERIMENTS ON LINEAR DATA,0.5838150289017341,"Table 1: Edge-type accuracy (in %), reconstruction MSE, and distance to world parameters using
linear data for 3 variables, 2 states, and 2 edge-types."
EXPERIMENTS ON LINEAR DATA,0.5895953757225434,"METHOD
EDGE ACCURACY
RECONST. MSE
DIST. TO WORLD
TRAIN
TEST
TRAIN
TEST
ACD (L¨owe et al., 2020) - FIXED DECODER
66.35 ± 0.12
66.02 ± 0.29
0.49 ± 9.01 · 10−3
0.49 ± 1.89 · 10−2
-
ACD (L¨owe et al., 2020)
66.90 ± 0.13
66.44 ± 0.29
0.47 ± 8.60 · 10−3
0.47 ± 1.98 · 10−2
5.62 · 10−3"
EXPERIMENTS ON LINEAR DATA,0.5953757225433526,"SDCI - STATIC - FIXED DEC.
90.69 ± 0.10
90.43 ± 0.23
2.23 · 10−2 ± 1.31 · 10−3
2.64 · 10−2 ± 4.55 · 10−3
-
SDCI - STATIC
93.84 ± 0.09
93.84 ± 0.19
1.34 · 10−2 ± 1.13 · 10−3
1.57 · 10−2 ± 4.03 · 10−3
6.60 · 10−6"
EXPERIMENTS ON LINEAR DATA,0.6011560693641619,"SDCI - TEMPORAL - FIXED DEC.
82.97 ± 0.13
82.79 ± 0.28
7.03 · 10−2 ± 1.62 · 10−3
7.43 · 10−2 ± 4.79 · 10−3
-
SDCI - TEMPORAL
49.92 ± 0.13
49.97 ± 0.28
0.86 ± 1.61 · 10−2
0.84 ± 3.29 · 10−2
2.18 · 10−2"
DATA GENERATION,0.6069364161849711,"4.1.1
DATA GENERATION"
DATA GENERATION,0.6127167630057804,"The procedure for generating this dataset is as follows. First, we set the edge-type interactions.
In our experiments we set α = 1 and since we experiment with 2 edge-types, we set β1 = 0.05.
To generate each sample, we need to sample the initial values of the continuous variable for each
element p0
i and the underlying causal structure dependent on the state, G(s). At each time-step, it
sufﬁces to query the edge-type k for each pair of variables and apply the corresponding causal effect
βk following Equation 22. The edge-type is k = G(st
j)ji, where G(s)ji denotes the causal effect
from j to i, which has been deﬁned at the beginning of the sequence. For all our experiments with
this dataset, we simulate N = 3 variables for T = 40 time-steps. We decide to keep N and the
interaction values (βk) low because the generated samples are unstable, which imply that the data is
not restricted within a certain range and could be problematic for higher values of N and T."
DATA GENERATION,0.6184971098265896,"When considering hidden states, the state is updated as follows: st
i = 1(pt
i<0)."
TRAINING SPECIFICATIONS,0.6242774566473989,"4.1.2
TRAINING SPECIFICATIONS"
TRAINING SPECIFICATIONS,0.630057803468208,"All the models participating in the experiments of this section have been trained following the same
training scheme, including ACD (L¨owe et al., 2020)."
TRAINING SPECIFICATIONS,0.6358381502890174,"Customized decoder.
One of our objectives is to recover the underlying world parameters βk.
Thus, we implement a decoder which imitates the message passing operation presented in Equation
22, which allows us to initialize the decoder using the underlying world parameters and analyse the
performance of the encoder as a separate entity from the whole model."
TRAINING SPECIFICATIONS,0.6416184971098265,"Model parameters.
Following Kipf et al. (2018), the models have been trained using ADAM
optimizer (Kingma & Ba, 2015). The learning rate of the encoder is 5 · 10−4, the learning rate of
the decoder is 1 · 10−3, and both are decayed by a factor of 0.5 every 200 epochs. We train for 1000
epochs using a batch size of 128. We use teacher forcing every 10 time-steps during training. This
implies that the decoder receives the ground-truth as input every 10 time-steps, otherwise it uses its
previous output. The temperature τ is set to 0.5 and the variance of the Gaussian distribution of the
decoder for p is σ2 = 5 · 10−5. When considering the setting where we make the state dependent
on the dynamics of the objects, we set λ = 103. For hidden states, we set the temperature γ = 0.1."
RESULTS,0.6473988439306358,"4.1.3
RESULTS"
RESULTS,0.653179190751445,"We ﬁrstly consider two edge-types and only one state to search for suitable learning rates for the
encoder and the decoder. The edge-type accuracy of our method considering SDCI-Static for this
setting is 94.88% on training data and 94.87% on test data. Since SDCI-Static is equivalent to ACD
(L¨owe et al., 2020) in this setting, we use this value as a reference for the following experiments."
RESULTS,0.6589595375722543,"We now proceed to comparing the performance between the two architecture choices SCDI-Static
and SCDI-Temporal with the baseline ACD, and evaluate the effect of explicitly modeling the under-
lying state. Furthermore, we compare the two proposed methods SCDI-Static and SCDI-Temporal
with a a variant where the decoder is ﬁxed and uses the ground-truth parameters, βk. We will in the
following compare all these models considering two edge-types."
RESULTS,0.6647398843930635,"Table 1 shows the edge-type identiﬁcation accuracy, the distance to the world parameters, and the
reconstruction MSE for three variables, two states, and two edge-types (no-edge and β1). As we
can see, our method SDCI-Static successfully performs the task of identifying the state-dependent
causal interactions. Furthermore, it recovers the underlying parameters of the generative process"
RESULTS,0.6705202312138728,Under review as a conference paper at ICLR 2022
RESULTS,0.6763005780346821,"with great accuracy. On the other hand, our SDCI-Temporal is not able to identify properly the
causal interactions nor recover the underlying parameters with enough accuracy. However, when
ﬁxing the decoder using the true underlying parameters, it achieves decent performance. Regarding
our predecessor (ACD), we observe that its performance is considerably lower than our proposed ar-
chitectures. One should note that this formulation can still recover the underlying world parameters,
but less accurately compared to our formulation with SDCI-Static."
RESULTS,0.6820809248554913,"We also experimented with SDCI-Static when considering the state variable hidden from the input
data. The results show an edge-type identiﬁcation accuracy of 92.25% and 91.96% in training and
test data respectively. The distance to the world parameters is 1.06·10−4. To assess the effectiveness
of the model to identify different behaviors (states) when considering this setting, we can inspect
the estimation of the latent state probability distribution. The state decoder accuracy of the latent
state function is 99.10% and 98.97% in training and test data respectively. We can observe that our
formulation allows considering hidden state variables. SDCI-Static is able to successfully identify
the causal interactions between the elements and successfully decomposes the different behaviors
into the actual underlying states of the generative process."
EXPERIMENTS ON SPRING DATA,0.6878612716763006,"4.2
EXPERIMENTS ON SPRING DATA"
EXPERIMENTS ON SPRING DATA,0.6936416184971098,"In the second experiment setting, we evaluate our methods on data similar to that used in recent
work (Kipf et al., 2018; L¨owe et al., 2020), consisting of particles (or small balls) connected by
springs with directed impact - meaning that e.g. particle i could affect particle j with a force through
a connecting spring, but leaving particle i unaffected by this spring force."
EXPERIMENTS ON SPRING DATA,0.6994219653179191,"The following experiments focus solely on evaluating the performance of our SDCI-Static and com-
paring it with its predecessor (ACD) under the three ﬁrst scenario classes introduced in the Introduc-
tion. The data generation process follows the description of Kipf et al. (2018). The only difference
is the addition of the state variable to the generative process, which affects the edge-type at each
time-step as in the previous case. For more details, see the supplementary material. As in the ﬁrst
experimental setting, the experiments regarding this data always consider 2 edge-type connections
(presence/absence of directed spring) and a sequence length of T = 80."
TRAINING SPECIFICATIONS,0.7052023121387283,"4.2.1
TRAINING SPECIFICATIONS"
TRAINING SPECIFICATIONS,0.7109826589595376,"We use the same training scheme for all the models present in the experiments of this section. In
this case, the conﬁguration is identical to the one used by Kipf et al. (2018). Thus, the experiments
have been trained using ADAM optimizer (Kingma & Ba, 2015). The learning rate of both the
encoder and decoder is 5 · 10−4 and decayed by a factor of 0.5 every 200 epochs. We train for 500
epochs using a batch size of 128. We also use teacher forcing every 10 time-steps during training.
The temperature τ is set to 0.5 and the variance of the Gaussian distribution of the decoder for p is
σ2 = 5 · 10−5. When considering the setting where we make the state dependent on the dynamics
of the objects, we set λ = 103. For hidden states, we set γ = 0.05."
RESULTS,0.7167630057803468,"4.2.2
RESULTS"
RESULTS,0.7225433526011561,"Let us consider the ﬁrst scenario class, where the state is known and independent from the obser-
vations. The state transitions incrementally into the next one every 10 time-steps, and alternative
settings with up to 8 states are explored. Table 2 shows the corresponding results, where for each
for state (from 1 to 8) a new dataset is generated and used to train the SDCI-Static method. We
observe that with one state, the original results reported by L¨owe et al. (2020) are obtained since this
case corresponds to stationary time-series. As the number of states increase, the accuracy in edge-
type identiﬁcation decreases and the reconstruction MSE becomes larger. However, our SDCI-Static
is able to maintain promising results achieving 74.87% accuracy in edge-type identiﬁcation when
having as much as 8 states."
RESULTS,0.7283236994219653,"We now proceed to the scenario class number 3, where the state of a particle transitions when it
collides with the wall of the box where it is contained. For simplicity, we only consider two states
that transition alternatively on wall collision. Our proposed method SDCI-Static achieves an edge-
type identiﬁcation accuracy of 85.13% on training data and 79.21% on test data. The reconstrucion
MSE is 1.32 · 10−4 and 1.38 · 10−3 on training and test data respectively. Since now our decoder"
RESULTS,0.7341040462427746,Under review as a conference paper at ICLR 2022
RESULTS,0.7398843930635838,"Table 2: Edge-type accuracy (in %) and reconst. MSE using spring data with different states for
SDCI-Static."
RESULTS,0.7456647398843931,"NUM. STATES
EDGE ACCURACY
RECONST. MSE
TRAIN
TEST
TRAIN
TEST
1
99.70 ± 0.02
99.67 ± 0.13
7.88 · 10−5 ± 5.38 · 10−6
7.88 · 10−5 ± 4.64 · 10−4"
RESULTS,0.7514450867052023,"2
98.80 ± 0.02
97.11 ± 0.08
8.00 · 10−4 ± 1.53 · 10−5
4.02 · 10−2 ± 1.96 · 10−4"
RESULTS,0.7572254335260116,"3
98.00 ± 0.03
95.79 ± 0.09
2.50 · 10−3 ± 3.26 · 10−5
2.33 · 10−2 ± 1.64 · 10−4"
RESULTS,0.7630057803468208,"5
89.88 ± 0.04
80.34 ± 0.10
5.36 · 10−3 ± 2.98 · 10−5
6.57 · 10−2 ± 3.23 · 10−4"
RESULTS,0.7687861271676301,"8
79.37 ± 0.03
74.87 ± 0.08
9.59 · 10−3 ± 3.50 · 10−5
3.02 · 10−2 ± 1.63 · 10−4"
RESULTS,0.7745664739884393,"requires modelling the conditions for state transition, we can evaluate its performance in identifying
state changes. The state accuracy of the decoder is 99.73% on training data and 98.53% on test data,
which shows that our method is able to detect the effect that the event of a particle colliding with
the wall will have on the whole sample. If we consider ACD, it achieves an edge-type accuracy
of 69.87% on training data and 68.61% on test data. The reconstruction MSE is 4.45 · 10−4 and
1.46 · 10−3 on training and test data respectively, and the state accuracy of the decoder is 99.13%
on training data and 98.21% on test data. As we can see, the limitation of ACD of considering only
stationary time-series data limits the performance of the causal graph inference task. However, the
results for the reconstruction MSE are comparable to the ones obtained with our formulation (SDCI-
Static), which indicates that the method still makes decent predictions although it fails in identifying
the edge-type interactions."
RESULTS,0.7803468208092486,"Finally we address a scenario of class 2 as listed in the Introduction. In this scenario, the underlying
state of a particle changes depending its location in the box; thus, the state is not directly observable
but indeed directly dependent on an observable variable, the position of the particle. Our proposed
method SDCI-Static shows an edge-type identiﬁcation accuracy of 85.35% and 80.82% on training
and test data respectively. The reconstruction MSE is 1.44 · 10−4 and 1.19 · 10−3 on training and
test data respectively. Notice that in this case, we also ﬁnd accuracy levels that are similar to the
second and third layout. If we perform inspection on the estimation of the latent state variable
distribution, we observe a state decoder accuracy of 98.96% in training data and 99.41% in test data.
As we can see, our formulation allows to learn the dynamics of the state without any supervision.
Regarding ACD, it achieves an edge-type accuracy of 71.06% in training data and 69.60% in test
data. The reconstruction MSE is 4.11·10−4 and 1.31·10−3 in training and test data respectively. As
before, ACD is restricted since it considers a stationary generative process. However, the behavior of
the sequences in this data regime is non-stationary as the state variable is hidden. Our SDCI-Static
successfully decomposes the non-stationary dynamics into the true underlying conditional stationary
ones and is able to identify the causal links between particles successfully enough."
CONCLUSIONS,0.7861271676300579,"5
CONCLUSIONS"
CONCLUSIONS,0.791907514450867,"In this work we propose a method, SDCI, for performing causal discovery in scenes with multiple
objects interacting over time, and where the dynamics depend on the value of underlying states.
SDCI allows recovering the underlying causal structure of non-stationary time-series data by condi-
tioning its stationarity on categorical state variables. Furthermore, we provide a deep probabilistic
implementation of this method and perform an empirical study on two synthetic scenarios. Our
results show the effectiveness of our formulation in modelling conditional time-series data, in com-
parison to the state-of-the-art approaches that consider stationary time-series data."
FUTURE WORK,0.7976878612716763,"5.1
FUTURE WORK"
FUTURE WORK,0.8034682080924855,"This work can be regarded as a preliminary but necessary contribution towards causal discovery in
video. In order to approach this goal, we propose to add a visual front-end as a pre-processing step of
the SDGI module. This front-end would operate on the video and output probabilistic segmentation
of video into object hypotheses, pose extraction of humans in the scene, detection and recognition
of known object classes, etc."
FUTURE WORK,0.8092485549132948,"This automatical extraction of causal abstractions of natural scenes would enable taking steps to-
wards challenging tasks such as high-level scene understanding or counterfactual reasoning."
FUTURE WORK,0.815028901734104,Under review as a conference paper at ICLR 2022
REFERENCES,0.8208092485549133,REFERENCES
REFERENCES,0.8265895953757225,"David Maxwell Chickering. Optimal structure identiﬁcation with greedy search. Journal of machine
learning research, 3(Nov):507–554, 2002."
REFERENCES,0.8323699421965318,"Doris Entner and Patrik O Hoyer. On causal discovery from time series data using fci. Probabilistic
graphical models, pp. 121–128, 2010."
REFERENCES,0.838150289017341,"Rohit Girdhar and Deva Ramanan. CATER: A diagnostic dataset for Compositional Actions and
TEmporal Reasoning. In ICLR, 2020."
REFERENCES,0.8439306358381503,"Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graph-
ical models. Frontiers in genetics, 10:524, 2019."
REFERENCES,0.8497109826589595,"Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica: journal of the Econometric Society, pp. 424–438, 1969."
REFERENCES,0.8554913294797688,"Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Sch¨olkopf. Nonlinear
causal discovery with additive noise models. Advances in neural information processing systems,
21:689–696, 2008."
REFERENCES,0.861271676300578,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
REFERENCES,0.8670520231213873,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014."
REFERENCES,0.8728323699421965,"Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. In International Conference on Machine Learning, pp. 2688–
2697. PMLR, 2018."
REFERENCES,0.8786127167630058,"Yunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, and Animesh Garg. Causal discov-
ery in physical systems from videos. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9180–9192.
Curran Associates, Inc., 2020."
REFERENCES,0.884393063583815,"Sindy L¨owe, David Madras, Richard S. Zemel, and Max Welling. Amortized causal discovery:
Learning to infer causal graphs from time-series data. ArXiv, abs/2006.10833, 2020."
REFERENCES,0.8901734104046243,"Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relax-
ation of discrete random variables. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017."
REFERENCES,0.8959537572254336,"Kevin P Murphy et al. Dynamic bayesian networks. Probabilistic Graphical Models, M. Jordan, 7:
431, 2002."
REFERENCES,0.9017341040462428,"Roxana Pamﬁl, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Geor-
gatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series data.
In International Conference on Artiﬁcial Intelligence and Statistics, pp. 1595–1605. PMLR, 2020."
REFERENCES,0.9075144508670521,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024–8035.
Curran Associates, Inc., 2019."
REFERENCES,0.9132947976878613,"Judea Pearl. Causality. Cambridge university press, 2009."
REFERENCES,0.9190751445086706,"Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Sch¨olkopf. Causal discovery with
continuous additive noise models. Journal of Machine Learning Research, 15(58):2009–2053,
2014."
REFERENCES,0.9248554913294798,Under review as a conference paper at ICLR 2022
REFERENCES,0.930635838150289,"Rajesh Ranganath and Adler Perotte. Multiple causal inference with latent confounding. arXiv
preprint arXiv:1805.08273, 2018."
REFERENCES,0.9364161849710982,"Axel Sauer and Andreas Geiger. Counterfactual generative networks. In International Conference
on Learning Representations, 2021."
REFERENCES,0.9421965317919075,"Shohei Shimizu, Patrik O Hoyer, Aapo Hyv¨arinen, Antti Kerminen, and Michael Jordan. A linear
non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10),
2006."
REFERENCES,0.9479768786127167,"Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using
deep conditional generative models. Advances in neural information processing systems, 28:
3483–3491, 2015."
REFERENCES,0.953757225433526,"Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):89–96,
2007."
REFERENCES,0.9595375722543352,"Peter Spirtes. An anytime algorithm for causal inference. In AISTATS, 2001."
REFERENCES,0.9653179190751445,"Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction,
and search. MIT press, 2000."
REFERENCES,0.9710982658959537,"Ruibo Tu, Cheng Zhang, Paul Ackermann, Karthika Mohan, Hedvig Kjellstr¨om, and Kun Zhang.
Causal discovery in the presence of missing data.
In The 22nd International Conference on
Artiﬁcial Intelligence and Statistics, pp. 1762–1770. PMLR, 2019."
REFERENCES,0.976878612716763,"Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B.
Tenenbaum. CLEVRER: collision events for video representation and reasoning. In ICLR, 2020."
REFERENCES,0.9826589595375722,"Kun Zhang and Aapo Hyv¨arinen. On the identiﬁability of the post-nonlinear causal model. In
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI ’09,
pp. 647–655, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958."
REFERENCES,0.9884393063583815,"Kun Zhang, Mingming Gong, Joseph Ramsey, Kayhan Batmanghelich, Peter Spirtes, and Clark
Glymour. Causal discovery in the presence of measurement error: Identiﬁability conditions. In
UAI 2017 Workshop on Causality: Learning, Inference, and Decision-Making, 2017."
REFERENCES,0.9942196531791907,"Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Con-
tinuous Optimization for Structure Learning. In Advances in Neural Information Processing Sys-
tems, 2018."
