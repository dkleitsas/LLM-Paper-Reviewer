Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0037593984962406013,"We introduce a self-supervised vision representation model BEIT, which stands
for Bidirectional Encoder representation from Image Transformers. Following
BERT (Devlin et al., 2019) developed in the natural language processing area, we
propose a masked image modeling task to pretrain vision Transformers. SpeciÔ¨Å-
cally, each image has two views in our pre-training, i.e., image patches (such as
16 √ó 16 pixels), and visual tokens (i.e., discrete tokens). We Ô¨Årst ‚Äútokenize‚Äù the
original image into visual tokens. Then we randomly mask some image patches
and fed them into the backbone Transformer. The pre-training objective is to
recover the original visual tokens based on the corrupted image patches. After
pre-training BEIT, we directly Ô¨Åne-tune the model parameters on downstream
tasks by appending task layers upon the pretrained encoder. Experimental results
on image classiÔ¨Åcation and semantic segmentation show that our model achieves
competitive results with previous pre-training methods."
INTRODUCTION,0.007518796992481203,"1
INTRODUCTION"
INTRODUCTION,0.011278195488721804,"Transformer (Vaswani et al., 2017) has achieved promising performance in computer vision (Dosovit-
skiy et al., 2020; Touvron et al., 2020). However, empirical studies show that vision Transformers
require more training data than convolutional neural networks. In order to solve the data-hungry
issue (Liu et al., 2021a), self-supervised pre-training is a promising solution to leverage large-scale im-
age data. Several strands of methods have been explored for vision Transformers, such as contrastive
learning (Chen et al., 2021; Xie et al., 2021), and self-distillation (Caron et al., 2021)."
INTRODUCTION,0.015037593984962405,"Concurrently, BERT (Devlin et al., 2019) has achieved great success in natural language processing.
Its masked language modeling task Ô¨Årst randomly masks some proportion of tokens within a text,
and then recovers the masked tokens based on the Transformer encoding results of the corrupted text.
Motivated by BERT, we turn to the denoising auto-encoding idea to pretrain vision Transformers,
which has not been well studied by the vision community. It is challenging to directly apply BERT-
style pre-training for image data. First of all, there is no pre-exist vocabulary for vision Transformer‚Äôs
input unit, i.e., image patches. So we cannot simply employ a softmax classiÔ¨Åer to predict over all
possible candidates for masked patches. In contrast, the language vocabulary, such as words and
BPE (Sennrich et al., 2016), is well-deÔ¨Åned and eases auto-encoding prediction. A straightforward
alternative is regarding the task as a regression problem, which predicts the raw pixels of masked
patches. However, such pixel-level recovery task tends to waste modeling capability on pre-training
short-range dependencies and high-frequency details (Ramesh et al., 2021). Our goal is to overcome
the above issues for pre-training of vision Transformers."
INTRODUCTION,0.018796992481203006,"In this work, we introduce a self-supervised vision representation model BEIT, which stands for
Bidirectional Encoder representation from Image Transformers. Inspired by BERT, we propose a
pre-training task, namely, masked image modeling (MIM). As shown in Figure 1, MIM uses two
views for each images, i.e., image patches, and visual tokens. We split the image into a grid of patches
that are the input representation of backbone Transformer. Moreover, we ‚Äútokenize‚Äù the image to
discrete visual tokens, which is obtained by the latent codes of discrete VAE (Ramesh et al., 2021)."
INTRODUCTION,0.022556390977443608,‚àóContribution during internship at Microsoft.
INTRODUCTION,0.02631578947368421,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03007518796992481,123 234 456 567
INTRODUCTION,0.03383458646616541,987 876 765 543
INTRODUCTION,0.03759398496240601,112 223 334 445
INTRODUCTION,0.041353383458646614,211 322 433 544
INTRODUCTION,0.045112781954887216,"+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+"
INTRODUCTION,0.04887218045112782,"BEIT Encoder
Blockwise"
INTRODUCTION,0.05263157894736842,Masking
INTRODUCTION,0.05639097744360902,"1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
0"
INTRODUCTION,0.06015037593984962,Flatten
INTRODUCTION,0.06390977443609022,"Tokenizer
Decoder"
INTRODUCTION,0.06766917293233082,"Position 
Embedding"
INTRODUCTION,0.07142857142857142,"Patch 
Embedding"
INTRODUCTION,0.07518796992481203,Original Image
INTRODUCTION,0.07894736842105263,"Image 
Patches"
INTRODUCTION,0.08270676691729323,"Visual Tokens ùê°2 L
ùê°3 L
ùê°6 L
ùê°7 L
ùê°14 L"
INTRODUCTION,0.08646616541353383,Masked Image Modeling Head
INTRODUCTION,0.09022556390977443,Reconstructed
INTRODUCTION,0.09398496240601503,"Image
Unused During"
INTRODUCTION,0.09774436090225563,Pre-Training
INTRODUCTION,0.10150375939849623,"234 456
876 765
322"
INTRODUCTION,0.10526315789473684,"[S]
[M]
[M]
[M]
[M]
[M]"
INTRODUCTION,0.10902255639097744,"Figure 1: Overview of BEIT pre-training. Before pre-training, we learn an ‚Äúimage tokenizer‚Äù via
autoencoding-style reconstruction, where an image is tokenized into discrete visual tokens according
to the learned vocabulary. During pre-training, each image has two views, i.e., image patches, and
visual tokens. We randomly mask some proportion of image patches (gray patches in the Ô¨Ågure) and
replace them with a special mask embedding [M]. Then the patches are fed to a backbone vision
Transformer. The pre-training task aims at predicting the visual tokens of the original image based
on the encoding vectors of the corrupted image."
INTRODUCTION,0.11278195488721804,"During pre-training, we randomly mask some proportion of image patches, and feed the corrupted
input to Transformer. The model learns to recover the visual tokens of the original image, instead of
the raw pixels of masked patches."
INTRODUCTION,0.11654135338345864,"We perform self-supervised learning and then Ô¨Åne-tune the pretrained BEIT on two downstream
tasks, i.e., image classiÔ¨Åcation, and semantic segmentation. Experimental results indicate that BEIT
outperforms both from-scratch training and previous strong self-supervised models. Moreover, BEIT
is complementary to supervised pre-training. Performance of BEIT can be further improved by
intermediate Ô¨Åne-tuning with ImageNet labels. Ablation studies show that our proposed techniques
are critical to the effectiveness of BERT-style pre-training for image data. Apart from performance,
the improvements of convergence speed and stability of Ô¨Åne-tuning reduce training costs on end tasks.
In addition, we demonstrate that self-supervised BEIT can learn reasonable semantic regions via
pre-training, unleashing the rich supervision signals contained in images."
INTRODUCTION,0.12030075187969924,Our contributions are summarized as follows:
INTRODUCTION,0.12406015037593984,"‚Ä¢ We propose a masked image modeling task to pretrain vision Transformers in a self-supervised
manner. We also provide a theoretical explanation from the perspective of variational autoencoder."
INTRODUCTION,0.12781954887218044,"‚Ä¢ We pretrain BEIT and conduct extensive Ô¨Åne-tuning experiments on downstream tasks, such as
image classiÔ¨Åcation, and semantic segmentation."
INTRODUCTION,0.13157894736842105,"‚Ä¢ We present that the self-attention mechanism of self-supervised BEIT learns to distinguish
semantic regions and object boundaries, although without using any human annotation."
METHODS,0.13533834586466165,"2
METHODS"
METHODS,0.13909774436090225,"Given an input image x, BEIT encodes it to contextualized vector representations. As shown
in Figure 1, BEIT is pretrained by the masked image modeling (MIM) task in a self-supervised
learning manner. MIM aims at recovering the masked image patches based on encoding vectors. For"
METHODS,0.14285714285714285,Published as a conference paper at ICLR 2022
METHODS,0.14661654135338345,"downstream tasks (such as image classiÔ¨Åcation, and semantic segmentation), we append task layers
upon pretrained BEIT and Ô¨Åne-tune the parameters on the speciÔ¨Åc datasets."
IMAGE REPRESENTATIONS,0.15037593984962405,"2.1
IMAGE REPRESENTATIONS"
IMAGE REPRESENTATIONS,0.15413533834586465,"The images have two views of representations in our method, namely, image patch, and visual tokens.
The two types serve as input and output representations during pre-training, respectively."
IMAGE PATCH,0.15789473684210525,"2.1.1
IMAGE PATCH"
IMAGE PATCH,0.16165413533834586,"The 2D image is split into a sequence of patches (Dosovitskiy et al., 2020), so that a standard
Transformer can directly accept image data. Formally, we reshape the image x ‚ààRH√óW √óC into
N = HW/P 2 patches xp ‚ààRN√ó(P 2C), where C is the number of channels, (H, W) is the input
image resolution, and (P, P) is the resolution of each patch. The image patches {xp
i }N
i=1 are Ô¨Çattened
into vectors and are linearly projected, which is similar to word embeddings in BERT (Devlin et al.,
2019). Image patches preserve raw pixels and are used as input features in BEIT."
IMAGE PATCH,0.16541353383458646,"In our experiments, we split each 224 √ó 224 image into a 14 √ó 14 grid of image patches, where each
patch is 16 √ó 16."
VISUAL TOKEN,0.16917293233082706,"2.1.2
VISUAL TOKEN"
VISUAL TOKEN,0.17293233082706766,"Similar to natural language, we represent the image as a sequence of discrete tokens obtained by an
‚Äúimage tokenizer‚Äù, instead of raw pixels. SpeciÔ¨Åcally, we tokenize the image x ‚ààRH√óW √óC into
z = [z1, . . . , zN] ‚ààVh√ów, where the vocabulary V = {1, . . . , |V|} contains discrete token indices."
VISUAL TOKEN,0.17669172932330826,"Following (Ramesh et al., 2021), we use the image tokenizer learned by discrete variational autoen-
coder (dVAE). There are two modules during visual token learning, namely, tokenizer and decoder.
The tokenizer qœÜ(z|x) maps image pixels x into discrete tokens z according to a visual codebook
(i.e., vocabulary). The decoder pœà(x|z) learns to reconstruct the input image x based on the visual
tokens z. The reconstruction objective can be written as Ez‚àºqœÜ(z|x)[log pœà(x|z)]. Because the latent
visual tokens are discrete, the model training is non-differentiable. Gumbel-softmax relaxation (Jang
et al., 2017; Maddison et al., 2017) is employed to train the model parameters. Moreover, a uniform
prior is put on qœÜ during dVAE training. Refer to (Ramesh et al., 2021) for more training details of
the image tokenizer."
VISUAL TOKEN,0.18045112781954886,"We tokenize each image to a 14 √ó 14 grid of visual tokens. Notice the number of visual tokens and
the number of image patches for one image are the same. The vocabulary size is set to |V| = 8192.
In our work, we directly use the publicly available1 image tokenizer described in (Ramesh et al.,
2021). We also compare it with a re-implemented tokenizer in Appendix C."
VISUAL TOKEN,0.18421052631578946,"2.2
BACKBONE NETWORK: IMAGE TRANSFORMER"
VISUAL TOKEN,0.18796992481203006,"Following ViT (Dosovitskiy et al., 2020), we use the standard Transformer (Vaswani et al., 2017) as
the backbone network. So the results can be directly compared with previous work in terms of the
network architecture."
VISUAL TOKEN,0.19172932330827067,"The input of Transformer is a sequence of image patches {xp
i }N
i=1. The patches are then linearly
projected to obtain patch embeddings Exp
i , where E ‚ààR(P 2C)√óD. Moreover, we prepend a
special token [S] to the input sequence. We also add standard learnable 1D position embeddings
Epos ‚ààRN√óD to patch embeddings. The input vectors H0 = [e[S], Exp
i , . . . , Exp
N] + Epos is fed
into Transformer. The encoder contains L layers of Transformer blocks Hl = Transformer(Hl‚àí1),
where l = 1, . . . , L. The output vectors of the last layer HL = [hL
[S], hL
1 , . . . , hL
N] are used as the
encoded representations for the image patches, where hL
i is the vector of the i-th image patch."
VISUAL TOKEN,0.19548872180451127,1https://github.com/openai/DALL-E
VISUAL TOKEN,0.19924812030075187,Published as a conference paper at ICLR 2022
VISUAL TOKEN,0.20300751879699247,"2.3
PRE-TRAINING BEIT: MASKED IMAGE MODELING"
VISUAL TOKEN,0.20676691729323307,"We propose a masked image modeling (MIM) task. We randomly mask some percentage of image
patches, and then predict the visual tokens that are corresponding to the masked patches."
VISUAL TOKEN,0.21052631578947367,"Figure 1 shows the overview of our method. As presented in Section 2.1, given an input image
x, we split it into N image patches ({xp
i }N
i=1), and tokenize it to N visual tokens ({zi}N
i=1). We
randomly mask approximately 40% image patches, where the masked positions are denoted as
M ‚àà{1, . . . , N}0.4N. Next we replace the masked patches with a learnable embedding e[M] ‚ààRD.
The corrupted image patches xM = {xp
i : i /‚ààM}N
i=1
S{e[M] : i ‚ààM}N
i=1 are then fed into the
L-layer Transformer as described in Section 2.2. The Ô¨Ånal hidden vectors {hL
i }N
i=1 are regarded as
encoded representations of the input patches. For each masked position {hL
i : i ‚ààM}N
i=1, we use a
softmax classiÔ¨Åer to predict the corresponding visual tokens pMIM(z‚Ä≤|xM) = softmaxz‚Ä≤(WchL
i +bc),
where xM is the corrupted image, Wc ‚ààR|V|√óD, and bc ‚ààR|V|. The pre-training objective is to
maximize the log-likelihood of the correct visual tokens zi given the corrupted image: max
X"
VISUAL TOKEN,0.21428571428571427,"x‚ààD
EM "" X"
VISUAL TOKEN,0.21804511278195488,"i‚ààM
log pMIM(zi|xM) # (1)"
VISUAL TOKEN,0.22180451127819548,"where D is the training corpus, M represents randomly masked positions, and xM is the corrupted
image that is masked according to M."
VISUAL TOKEN,0.22556390977443608,Algorithm 1 Blockwise Masking
VISUAL TOKEN,0.22932330827067668,"Input: N(= h √ó w) image patches
Output: Masked positions M
M ‚Üê{}
repeat"
VISUAL TOKEN,0.23308270676691728,"s ‚ÜêRand(16, 0.4N ‚àí|M|)
‚ñ∑Block size
r ‚ÜêRand(0.3,
1
0.3)
‚ñ∑Aspect ratio of block
a ‚Üê‚àös ¬∑ r; b ‚Üê
p"
VISUAL TOKEN,0.23684210526315788,"s/r
t ‚ÜêRand(0, h ‚àía) ; l ‚ÜêRand(0, w ‚àíb)
M ‚ÜêM S{(i, j) : i ‚àà[t, t + a), j ‚àà[l, l + b)}
until |M| > 0.4N
‚ñ∑Masking ratio is 40%
return M"
VISUAL TOKEN,0.24060150375939848,"Rather than randomly choosing patches
for the masked positions M, we employ
blockwise masking in our work. As sum-
marized in Algorithm 1, a block of image
patches is masked each time. For each
block, we set the minimum number of
patches to 16. Then we randomly choose
an aspect ratio for the masking block. We
repeat the above two steps until obtaining
enough masked patches, i.e., 0.4N, where
N is the total number of image patches,
and 0.4 is masking ratio."
VISUAL TOKEN,0.24436090225563908,"The MIM task is greatly inspired by masked language modeling (Devlin et al., 2019), which is one of
the most successful pre-training objective in natural language processing. Moreover, blockwise (or
n-gram) masking is also widely applied in BERT-like models (Joshi et al., 2020; Bao et al., 2020;
Raffel et al., 2020). However, directly using pixel-level auto-encoding (i.e., recovering the pixels
of masked patches) for vision pre-training pushes the model to focus on short-range dependencies
and high-frequency details (Ramesh et al., 2021). BEIT overcomes the above issue by predicting
discrete visual tokens, which summarizes the details to high-level abstractions. Ablation studies in
Section 3.3 show that our proposed method signiÔ¨Åcantly outperforms pixel-level auto-encoding."
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.24812030075187969,"2.4
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2518796992481203,"The BEIT pre-training can be viewed as variational autoencoder (Kingma & Welling, 2014) training.
Let x denote the original image, Àúx the masked image, and z the visual tokens. Considering the
evidence lower bound (ELBO) of the log-likelihood p(x|Àúx), i.e., recovering the original image from
its corrupted version:
X"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2556390977443609,"(xi,Àúxi)‚ààD
log p(xi|Àúxi) ‚â•
X"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2593984962406015,"(xi,Àúxi)‚ààD"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2631578947368421," 
Ezi‚àºqœÜ(z|xi)[log pœà(xi|zi)]
|
{z
}
Visual Token Reconstruction"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2669172932330827,"‚àíDKL[qœÜ(z|xi), pŒ∏(z|Àúxi)]

(2)"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2706766917293233,"where (1) qœÜ(z|x) denotes the image tokenizer that obtains visual tokens; (2) pœà(x|z) decodes the
original image given input visual tokens; (3) pŒ∏(z|Àúx) recovers the visual tokens based on the masked
image, which is our MIM pre-training task."
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2744360902255639,"We learn the model following a two-stage procedure similar to (van den Oord et al., 2017; Razavi
et al., 2019).
In the Ô¨Årst stage, we obtain the image tokenizer as a discrete variational au-
toencoder (Ramesh et al., 2021). SpeciÔ¨Åcally, the Ô¨Årst stage minimizes the reconstruction loss"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2781954887218045,Published as a conference paper at ICLR 2022
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2819548872180451,"‚àíEzi‚àºqœÜ(z|xi)[log pœà(xi|zi)] with an uniform prior as described in Equation (2). In the second stage,
we learn the prior pŒ∏ while keeping qœÜ and pœà Ô¨Åxed. We simplify qœÜ(z|xi) to a one-point distribution
with the most likely visual tokens ÀÜzi = arg maxz qœÜ(z|xi). Then Equation (2) can be rewritten as:
X"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2857142857142857,"(xi,Àúxi)‚ààD"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2894736842105263," 
Ezi‚àºqœÜ(z|xi)[log pœà(xi|zi)]
|
{z
}
Stage 1: Visual Token Reconstruction"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.2932330827067669,"+
log pŒ∏(ÀÜzi|Àúxi)
|
{z
}
Stage 2: Masked Image Modeling 
(3)"
FROM THE PERSPECTIVE OF VARIATIONAL AUTOENCODER,0.29699248120300753,where the second term is our BEIT pre-training objective.
PRE-TRAINING SETUP,0.3007518796992481,"2.5
PRE-TRAINING SETUP"
PRE-TRAINING SETUP,0.30451127819548873,"The network architecture of BEIT follows that of ViT-Base (Dosovitskiy et al., 2020) for a fair
comparison. We use a 12-layer Transformer with 768 hidden size, and 12 attention heads. The
intermediate size of feed-forward networks is 3072. We employ the default 16 √ó 16 input patch size.
We directly borrow the image tokenizer trained by Ramesh et al. (2021). The vocabulary size of
visual tokens is 8192."
PRE-TRAINING SETUP,0.3082706766917293,"We pretrain BEIT on the training set of ImageNet-1K (Russakovsky et al., 2015), which contains
about 1.2M images. Our augmentation policy includes random resized cropping, horizontal Ô¨Çipping,
color jittering (Wu et al., 2018). Notice that we do not use the labels for self-supervised learning. We
use the 224 √ó 224 resolution in our experiments. So the input is split to 14 √ó 14 image patches, and
the same amount of visual tokens. We randomly mask at most 75 patches (i.e., roughly 40% of total
image patches)."
PRE-TRAINING SETUP,0.31203007518796994,"The pre-training runs for about 500k steps (i.e., 800 epochs) with 2k batch size. Adam (Loshchilov &
Hutter, 2019) with Œ≤1 = 0.9, Œ≤2 = 0.999 is employed for optimization. The learning rate is set to
1.5e-3, with a warmup of 10 epochs, and cosine learning rate decay. The weight decay is 0.05. We
employ stochastic depth (Huang et al., 2016) with a 0.1 rate, and disable dropout. The 500k training
steps take about Ô¨Åve days using 16 Nvidia Telsa V100 32GB GPU cards."
PRE-TRAINING SETUP,0.3157894736842105,"We Ô¨Ånd that proper initialization is important to stabilize Transformer, especially for large-scale pre-
training. We Ô¨Årst randomly initialize all the parameters within a small range, such as [‚àí0.02, 0.02].
Then, for the l-th Transformer layer, we rescale the output matrices (i.e., the last linear projection
within each sub-layer) of the self-attention module and the feed-forward network by
1
‚àö 2l."
FINE-TUNING BEIT ON DOWNSTREAM VISION TASKS,0.31954887218045114,"2.6
FINE-TUNING BEIT ON DOWNSTREAM VISION TASKS"
FINE-TUNING BEIT ON DOWNSTREAM VISION TASKS,0.3233082706766917,"After pre-training BEIT, we append a task layer upon the Transformer, and Ô¨Åne-tune the parameters
on downstream tasks, like BERT. We take image classiÔ¨Åcation and semantic segmentation as examples
in our work. It is straightforward to leverage the pre-training-then-Ô¨Åne-tuning paradigm on other
vision tasks with BEIT."
FINE-TUNING BEIT ON DOWNSTREAM VISION TASKS,0.32706766917293234,"Image classiÔ¨Åcation.
For image classiÔ¨Åcation tasks, we directly employ a simple linear clas-
siÔ¨Åer as the task layer.
SpeciÔ¨Åcally, we use average pooling to aggregate the representa-
tions, and feed the global to a softmax classiÔ¨Åer.
The category probabilities are computed
as softmax(avg({hL
i }N
i=1Wc)), where hL
i is the Ô¨Ånal encoding vector of the i-th image patch,
Wc ‚ààRD√óC is a parameter matrix, and C is the number of labels. We maximize the likelihood of
labeled data by updating the parameters of BEIT and the softmax classiÔ¨Åer."
FINE-TUNING BEIT ON DOWNSTREAM VISION TASKS,0.3308270676691729,"Semantic segmentation.
For semantic segmentation, we follow the task layer used in SETR-
PUP (Zheng et al., 2020). To be speciÔ¨Åc, we use pretrained BEIT as a backbone encoder, and
incorporate several deconvolution layers as decoder to produce segmentation. The model is also
end-to-end Ô¨Åne-tuned similar to image classiÔ¨Åcation."
FINE-TUNING BEIT ON DOWNSTREAM VISION TASKS,0.33458646616541354,"Intermediate Ô¨Åne-tuning.
After self-supervised pre-training, we can further train BEIT on a data-
rich intermediate dataset (i.e., ImageNet-1K in our work), and then Ô¨Ånetune the model on the target
downstream tasks. Such intermediate Ô¨Åne-tuning is the common practice of BERT Ô¨Åne-tuning in
NLP (Pruksachatkun et al., 2020). We directly follow the method for BEIT."
FINE-TUNING BEIT ON DOWNSTREAM VISION TASKS,0.3383458646616541,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.34210526315789475,"3
EXPERIMENTS"
EXPERIMENTS,0.3458646616541353,"We conduct full Ô¨Åne-tuning experiments on image classiÔ¨Åcation and semantic segmentation. Moreover,
we present various ablation studies for pre-training and analyze the representations learned by BEIT.
We also report linear probes on ImageNet in Appendix D."
IMAGE CLASSIFICATION,0.34962406015037595,"3.1
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.3533834586466165,"The image classiÔ¨Åcation task classiÔ¨Åes input images to various categories. We evaluate BEIT on
the ILSVRC-2012 ImageNet dataset (Russakovsky et al., 2015) with 1k classes and 1.3M images.
We directly follow the most of hyperparameters of DeiT (Touvron et al., 2020) in our Ô¨Åne-tuning
experiments for a fair comparison. We reduce Ô¨Åne-tuning epochs compared with training from scratch,
as BEIT has been pre-trained. Accordingly, we use a larger learning rate with layer-wise decay. The
detailed hyperparameters are summarized in Appendix H."
IMAGE CLASSIFICATION,0.35714285714285715,"Table 1 reports top-1 accuracy on image classiÔ¨Åcation. We compare BEIT with vision Transformers
trained by random initialization, supervised pre-training, and previous self-supervised learning
methods. All the compared models are base-size, except iGPT has 1.36B parameters. Pre-training is
conducted on ImageNet for the comparison purpose, except ViT-JFT300M is pretrained on Google‚Äôs
in-house 300M images."
IMAGE CLASSIFICATION,0.3609022556390977,"Compared with the models trained by random initialization, we Ô¨Ånd that pre-trained BEIT signiÔ¨Å-
cantly improves performance on both datasets. BEIT improves the performance on ImageNet, which
shows the effectiveness under the rich-resource setting."
IMAGE CLASSIFICATION,0.36466165413533835,"Moreover, we compare BEIT with previous state-of-the-art self-supervised methods for Transformer,
such as DINO (Caron et al., 2021), and MoCo v3 (Chen et al., 2021). Our proposed method
outperforms previous models on ImageNet Ô¨Åne-tuning. Among them, iGPT-1.36B (Chen et al.,
2020a) uses much more parameters (i.e., 1.36B vs 86M), and ViT-JFT300M (Dosovitskiy et al., 2020)
is pretrained on larger corpus (i.e., 300M vs 1.3M), while others pretrain ViT-Base on ImageNet-1K.
iGPT-1.36B and ViT-JFT300M are the most comparable methods, which also follows auto-encoding
pre-training for vision Transformer. SpeciÔ¨Åcally, iGPT uses clustered image tokens as both input and
output for image GPT or image BERT. In contrast, we use image patches as input to preserve raw
pixels, and employ discrete visual tokens as a prediction bottleneck. ViT-JFT300 predicts the mean,
3-bit color of each masked patch, rather than visual tokens learned by discrete VAE. We also pretrain
the self-supervised tasks of BEIT and DINO in a multi-task learning manner, which is presented in
Appendix E."
IMAGE CLASSIFICATION,0.3684210526315789,"In addition, we evaluate our proposed method with intermediate Ô¨Åne-tuning. In other words, we Ô¨Årst
pretrain BEIT in a self-supervised manner, and then Ô¨Åne-tune the pretrained model on ImageNet with
labeled data. The results show that BEIT is complementary to supervised pre-training, achieving
additional gain after intermediate Ô¨Åne-tuning on ImageNet."
IMAGE CLASSIFICATION,0.37218045112781956,"Fine-tuning to 384 √ó 384 resolution.
After Ô¨Åne-tuning with resolution 224 √ó 224, we additionally
Ô¨Åne-tune the model on 384√ó384 images by 10 more epochs. We follow the standard higher-resolution
setting of DeiT (Touvron et al., 2020), except using fewer epochs. Notice that we keep patch size
the same for both 224 √ó 224 and 384 √ó 384 images. So the input sequence length of Transformers
becomes longer for higher resolutions. Table 1 shows that higher resolution improves the BEIT
results by 1+ points on ImageNet. More importantly, BEIT384 pretrained on ImageNet-1K even
outperforms supervised pre-training ViT384 that uses ImageNet-22K, when they use the same input
resolution."
IMAGE CLASSIFICATION,0.37593984962406013,"Scaling up to larger size.
We further scale up BEIT to the large size (same as ViT-L). As shown in
Table 1, ViT384-L is worse than ViT384 on ImageNet, when training from scratch. The results veriÔ¨Åes
the data-hungry issue of vision Transformers. Supervised pre-training on ImageNet-22K partially
relieves the issue, where ViT384-L Ô¨Ånally outperforms ViT384 by 1.2. In comparison, BEIT-L is
better than BEIT by 2.0, and BEIT384-L outperforms BEIT384 by 1.7. In other words, the beneÔ¨Åts
of scaling up BEIT from base to large are greater than supervised pre-training with ImageNet-22K.
More importantly, comparing between BEIT384 with ViT384 that conducts supervised pre-training
on ImageNet-22K, the improvements of BEIT become greater along with scaling the size from base"
IMAGE CLASSIFICATION,0.37969924812030076,Published as a conference paper at ICLR 2022
IMAGE CLASSIFICATION,0.38345864661654133,"Models
Model Size
Resolution
ImageNet"
IMAGE CLASSIFICATION,0.38721804511278196,"Training from scratch (i.e., random initialization)
ViT384-B (Dosovitskiy et al., 2020)
86M
3842
77.9
ViT384-L (Dosovitskiy et al., 2020)
307M
3842
76.5
DeiT-B (Touvron et al., 2020)
86M
2242
81.8
DeiT384-B (Touvron et al., 2020)
86M
3842
83.1"
IMAGE CLASSIFICATION,0.39097744360902253,"Supervised Pre-Training on ImageNet-22K (using labeled data)
ViT384-B (Dosovitskiy et al., 2020)
86M
3842
84.0
ViT384-L (Dosovitskiy et al., 2020)
307M
3842
85.2"
IMAGE CLASSIFICATION,0.39473684210526316,"Self-Supervised Pre-Training on ImageNet-1K (without labeled data)
iGPT-1.36B‚Ä† (Chen et al., 2020a)
1.36B
2242
66.5
ViT384-B-JFT300M‚Ä° (Dosovitskiy et al., 2020)
86M
3842
79.9
MoCo v3-B (Chen et al., 2021)
86M
2242
83.2
MoCo v3-L (Chen et al., 2021)
307M
2242
84.1
DINO-B (Caron et al., 2021)
86M
2242
82.8
BEIT-B (ours)
86M
2242
83.2
BEIT384-B (ours)
86M
3842
84.6
BEIT-L (ours)
307M
2242
85.2
BEIT384-L (ours)
307M
3842
86.3"
IMAGE CLASSIFICATION,0.39849624060150374,"Table 1: Top-1 accuracy on ImageNet-1K. We evaluate base- (‚Äú-B‚Äù) and large-size (‚Äú-L‚Äù) models at
resolutions 224 √ó 224 and 384 √ó 384. ‚Ä†: iGPT-1.36B contains 1.36 billion parameters, while others
are base-size models. ‚Ä°: ViT384-B-JFT300M is pretrained with the ‚Äúmasked patch prediction‚Äù task
on Google‚Äôs in-house 300M images, while others use ImageNet."
IMAGE CLASSIFICATION,0.40225563909774437,"50
100
150
200
250
300
Epochs 60 65 70 75 80"
IMAGE CLASSIFICATION,0.40601503759398494,Top-1 Acc.
IMAGE CLASSIFICATION,0.40977443609022557,"DeiT (Training from scratch)
BEiT (Fine-tuning)"
IMAGE CLASSIFICATION,0.41353383458646614,"Table 2: Convergence curves of training
DeiT from scratch and Ô¨Åne-tuning BEIT on
ImageNet-1K."
IMAGE CLASSIFICATION,0.41729323308270677,"Models
ADE20K"
IMAGE CLASSIFICATION,0.42105263157894735,"Supervised Pre-Training on ImageNet
45.3"
IMAGE CLASSIFICATION,0.424812030075188,"DINO (Caron et al., 2021)
44.1
BEIT (ours)
45.6
BEIT + Intermediate Fine-Tuning (ours)
47.7"
IMAGE CLASSIFICATION,0.42857142857142855,"Table 3:
Results of semantic segmentation on
ADE20K. We use SETR-PUP (Zheng et al., 2020)
as the task layer and report results of single-scale
inference."
IMAGE CLASSIFICATION,0.4323308270676692,"(i.e., 0.6) to large (i.e., 1.1). The results suggest that BEIT tends to help more for extremely larger
models (such as 1B, or 10B), especially when labeled data are insufÔ¨Åcient2 to conduct supervised
pre-training3 for such large models."
IMAGE CLASSIFICATION,0.43609022556390975,"Convergence curves.
Figure 2 compares the convergence curves of the training-from-scratch and
pre-training-then-Ô¨Åne-tuning paradigms. We Ô¨Ånd that Ô¨Åne-tuning BEIT not only achieves better
performance, but also converging much faster than training DeiT from scratch. Moreover, Ô¨Åne-tuning
BEIT can reach reasonable numbers within very few epochs."
IMAGE CLASSIFICATION,0.4398496240601504,"2Zhai et al. (2021) report that supervised pre-training of a 1.8B-size vision Transformer requires billions of
labeled images.
3Appendix B shows that BEIT Ô¨Åne-tuned on ImageNet-22K (14M) can match the performance of supervised
pre-training on Google‚Äôs in-house JFT-3B (Zhai et al., 2021), while using 214x less labels. We also demonstrate
that large-size BEIT Ô¨Åne-tuned on 70M labeled images can achieve 89.5% top-1 accuracy on ImageNet and
58.4% mIoU on ADE20K, creating new state-of-the-art results for large-size vision Transformers."
IMAGE CLASSIFICATION,0.44360902255639095,Published as a conference paper at ICLR 2022
IMAGE CLASSIFICATION,0.4473684210526316,"Models
ImageNet
ADE20K"
IMAGE CLASSIFICATION,0.45112781954887216,"BEIT (300 Epochs)
82.86
44.65"
IMAGE CLASSIFICATION,0.4548872180451128,"‚àíBlockwise masking
82.77
42.93
‚àíVisual tokens (i.e., recover masked pixels)
81.04
41.38
‚àíVisual tokens ‚àíBlockwise masking
80.50
37.09
+ Recover 100% visual tokens
82.59
40.93
‚àíMasking + Recover 100% visual tokens
81.67
36.73"
IMAGE CLASSIFICATION,0.45864661654135336,"Pretrain longer (800 epochs)
83.19
45.58"
IMAGE CLASSIFICATION,0.462406015037594,Table 4: Ablation studies for BEIT pre-training on image classiÔ¨Åcation and semantic segmentation.
SEMANTIC SEGMENTATION,0.46616541353383456,"3.2
SEMANTIC SEGMENTATION"
SEMANTIC SEGMENTATION,0.4699248120300752,"Semantic segmentation aims to predict a corresponding class for each pixel of the input image.
We evaluate BEIT on the ADE20K benchmark (Zhou et al., 2019) with 25K images and 150
semantic categories. We report the metric of mean Intersection of Union (mIoU) averaged over
all semantic categories. As presented in Section 2.6, we directly follow the task layer and the
most of hyperparameters described in SETR-PUP (Zheng et al., 2020). On ADE20K, we use
Adam (Loshchilov & Hutter, 2019) as the optimizer. The learning rate is set to 1e-3 with layer-wise
decay similar to image classiÔ¨Åcation. We conduct Ô¨Åne-tuning for 160K steps. The batch size is 16.
The detailed hyperparameters are described in Appendix I."
SEMANTIC SEGMENTATION,0.47368421052631576,"As shown in Table 3, we compare BEIT with supervised pre-training that relies on labeled data
of ImageNet. We Ô¨Ånd that our proposed method achieves better performance than supervised pre-
training, although BEIT does not require manual annotations for pre-training. Moreover, we employ
intermediate Ô¨Åne-tuning for BEIT on ImageNet, i.e., we Ô¨Årst Ô¨Åne-tune pretrained BEIT on ImageNet,
and then Ô¨Åne-tune the model on ADE20K. The results indicate that intermediate Ô¨Åne-tuning further
improves BEIT on semantic segmentation."
ABLATION STUDIES,0.4774436090225564,"3.3
ABLATION STUDIES"
ABLATION STUDIES,0.48120300751879697,"We conduct ablation studies to analyze the contributions of each component in BEIT. The models
are evaluated on image classiÔ¨Åcation (i.e., ImageNet) and semantic segmentation (i.e., ADE20K). We
set the default pre-training steps to 300 epochs for the ablation studies, which is 37.5% of the total
steps used in the previous experiments."
ABLATION STUDIES,0.4849624060150376,"Table 4 reports the results of various model variants. First, we ablate blockwise masking by randomly
sample masked positions. We Ô¨Ånd that blockwise masking is beneÔ¨Åcial on both tasks, especially on
semantic segmentation. Second, we ablate the usage of visual tokens by predicting the raw pixels of
masked patches, i.e., the pre-training task becomes a pixel regression problem to recover masked
patches. Our proposed masked image modeling task signiÔ¨Åcantly outperforms naive pixel-level
auto-encoding. Compared with the results in Table 1, the ablation result is worse than training vision
Transformer from scratch on two tasks. The results indicate that the prediction of visual tokens is the
key ingredient of BEIT. Third, we ablate the usage of visual tokens and blockwise masking together.
We Ô¨Ånd that blockwise masking is even more helpful for pixel-level auto-encoding, which relieves
the suffering of short-distance dependency. Forth, recovering all the visual tokens harms performance
on downstream tasks. Fifth, we compare BEIT with different training steps. Pre-training the model
longer can further improve performance on downstream tasks."
ANALYSIS OF SELF-ATTENTION MAP,0.48872180451127817,"3.4
ANALYSIS OF SELF-ATTENTION MAP"
ANALYSIS OF SELF-ATTENTION MAP,0.4924812030075188,"We show that the self-attention mechanism in BEIT can separate objects, even though our pre-training
does not rely on any manual annotation at all. Similar properties are also observed by Caron et al.
(2021). The probing images are taken from the MS COCO (Lin et al., 2014) corpus to avoid appearing
in the pre-training data."
ANALYSIS OF SELF-ATTENTION MAP,0.49624060150375937,Published as a conference paper at ICLR 2022
ANALYSIS OF SELF-ATTENTION MAP,0.5,"Figure 2: Self-attention map for different reference points. The self-attention mechanism in BEIT is
able to separate objects, although self-supervised pre-training does not use manual annotations."
ANALYSIS OF SELF-ATTENTION MAP,0.5037593984962406,"As shown in Figure 2, we plot the self-attention map for different reference points within an image.
The visualizations are produced by attention scores computed via query-key product in the last layer.
For each reference point, we use the corresponding patch as query, and show which patch it attends
to. After pre-training, BEIT learns to distinguish semantic regions using self-attention heads, without
any task-speciÔ¨Åc supervision. The property partially indicates the reason why BEIT is able to help
downstream tasks. Such knowledge acquired by BEIT potentially improves the generalization ability
of Ô¨Åne-tuned models, especially on small-scale datasets."
RELATED WORK,0.5075187969924813,"4
RELATED WORK"
RELATED WORK,0.5112781954887218,"Self-supervised visual representation learning.
Various methods have been introduced over the
years to pretrain vision models in a self-supervised manner. Pioneering works design clever pretext
tasks, such as predicting the patch orderings (Noroozi & Favaro, 2016), colorization (Zhang et al.,
2016), and predicting rotation angles (Komodakis & Gidaris, 2018). In addition, Trinh et al. (2019)
propose to mask some patches within an image, and classify whether the masked patches are real
or fake for each masked position. The method is similar to the masked version of Jigsaw pre-
training (Noroozi & Favaro, 2016). The recent strand of research follows contrastive paradigm (Wu
et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Bachman et al., 2019; He et al., 2020; Chen et al.,
2020b;c). The models typically regard various data augmentations as different views of an image, and
then make the representations of positive pairs similar while pushing negative pairs away. In order to
obtain enough informative negative samples in contrastive learning, the methods usually rely on large
memory banks (Wu et al., 2018; He et al., 2020) or large batch size (Chen et al., 2020b). BYOL (Grill
et al., 2020) and SimSiam (Chen & He, 2020) further eliminate the requirement of negative samples,
using various techniques to avoid representation collapse. Another strand of methods use clustering to
organize image examples (Caron et al., 2018; Asano et al., 2020; Caron et al., 2020; Li et al., 2021)."
RELATED WORK,0.5150375939849624,"Self-supervised vision Transformers.
Pre-training vision Transformers has received signiÔ¨Åcant
attention recently due to the data-hungry issue. iGPT (Chen et al., 2020a) Ô¨Årst creates a 9-bit color
palette by k-means clustering RGB pixels, and then uses the clustered tokens to represent images.
Next iGPT uses the tasks of BERT and GPT to pretrain Transformers. In comparison, our proposed
method uses image patches as input without losing pixel-level information. Moreover, our visual
tokens are obtained by discrete VAE instead of clustering. ViT (Dosovitskiy et al., 2020) conducts a
preliminary exploration with the masked patch prediction task, which predicts the 3-bit mean color of
the masked patches. Dosovitskiy et al. (2020) also report that pixel-level auto-encoding performs"
RELATED WORK,0.518796992481203,Published as a conference paper at ICLR 2022
RELATED WORK,0.5225563909774437,"worse, although it is the most straightforward translation of BERT from NLP to CV. Rather than
using heuristically designed pre-training tasks, our proposed model leverages visual tokens learned by
discrete VAE, which not only achieves better performance but also is better theoretically motivated.
Apart from masked auto-encoding, other mainstream research works use contrastive learning (Chen
et al., 2021; Xie et al., 2021), and self-distillation (Caron et al., 2021). In comparison, BEIT can
achieve several times of improvement in terms of pre-training throughput (Appendix E), and memory
consumption. The advantages make BEIT appealing to scale up vision Transformers."
CONCLUSION,0.5263157894736842,"5
CONCLUSION"
CONCLUSION,0.5300751879699248,"We introduce a self-supervised pre-training framework for vision Transformers, achieving strong
Ô¨Åne-tuning results on downstream tasks, such as image classiÔ¨Åcation, and semantic segmentation.
We show that the proposed method is critical to make BERT-like pre-training (i.e., auto-encoding
with masked input) work well for image Transformers. We also present the intriguing property
of automatically acquired knowledge about semantic regions, without using any human-annotated
data. In the future, we would like to scale up BEIT pre-training in terms of data size and model
size. Moreover, we will conduct multimodal pre-training in a more uniÔ¨Åed way, using the similar
objectives and the shared architecture for texts and images."
REFERENCES,0.5338345864661654,REFERENCES
REFERENCES,0.5375939849624061,"Yuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering
and representation learning. In International Conference on Learning Representations (ICLR),
2020."
REFERENCES,0.5413533834586466,"Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximiz-
ing mutual information across views. In Advances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.5451127819548872,"Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao,
Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-masked language models
for uniÔ¨Åed language model pre-training. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, volume 119 of Proceedings of Machine Learning Research,
pp. 642‚Äì652. PMLR, 2020. URL http://proceedings.mlr.press/v119/bao20a.
html."
REFERENCES,0.5488721804511278,"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsuper-
vised learning of visual features. In Proceedings of the European Conference on Computer Vision
(ECCV), pp. 132‚Äì149, 2018."
REFERENCES,0.5526315789473685,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural
Information Processing Systems, volume 33, pp. 9912‚Äì9924. Curran Associates, Inc., 2020."
REFERENCES,0.556390977443609,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.5601503759398496,"Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
Generative pretraining from pixels. In Hal Daum√© III and Aarti Singh (eds.), Proceedings of the
37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 1691‚Äì1703. PMLR, 13‚Äì18 Jul 2020a. URL http://proceedings.
mlr.press/v119/chen20s.html."
REFERENCES,0.5639097744360902,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. preprint arXiv:2002.05709, 2020b."
REFERENCES,0.5676691729323309,"Xinlei Chen and Kaiming He.
Exploring simple siamese representation learning.
preprint
arXiv:2011.10566, 2020."
REFERENCES,0.5714285714285714,Published as a conference paper at ICLR 2022
REFERENCES,0.575187969924812,"Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. preprint arXiv:2003.04297, 2020c."
REFERENCES,0.5789473684210527,"Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. ArXiv, abs/2104.02057, 2021."
REFERENCES,0.5827067669172933,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 4171‚Äì4186. Association for Computational Linguistics, 2019."
REFERENCES,0.5864661654135338,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929,
2020."
REFERENCES,0.5902255639097744,"Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, Bilal Piot, Koray Kavukcuoglu, R√©mi Munos, and Michal Valko. Bootstrap your own latent:
A new approach to self-supervised learning. In NeurIPS, 2020."
REFERENCES,0.5939849624060151,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020."
REFERENCES,0.5977443609022557,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=Bklr3j0cKX."
REFERENCES,0.6015037593984962,"Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with
stochastic depth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer
Vision ‚Äì ECCV 2016, pp. 646‚Äì661, Cham, 2016. Springer International Publishing. ISBN 978-3-
319-46493-0."
REFERENCES,0.6052631578947368,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.
net/forum?id=rkE3y85ee."
REFERENCES,0.6090225563909775,"Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Span-
BERT: Improving pre-training by representing and predicting spans. Transactions of the As-
sociation for Computational Linguistics, 8:64‚Äì77, 2020. doi: 10.1162/tacl_a_00300. URL
https://www.aclweb.org/anthology/2020.tacl-1.5."
REFERENCES,0.6127819548872181,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, 2014."
REFERENCES,0.6165413533834586,"Nikos Komodakis and Spyros Gidaris. Unsupervised representation learning by predicting image
rotations. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.6203007518796992,"A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master‚Äôs thesis,
Department of Computer Science, University of Toronto, 2009."
REFERENCES,0.6240601503759399,"Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive learning of unsu-
pervised representations. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=KmykpuSrjcq."
REFERENCES,0.6278195488721805,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll√°r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740‚Äì755. Springer, 2014."
REFERENCES,0.631578947368421,"Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, and Marco De Nadai. EfÔ¨Åcient training
of visual transformers with small datasets. In Thirty-Fifth Conference on Neural Information
Processing Systems, 2021a. URL https://openreview.net/forum?id=SCN8UaetXx."
REFERENCES,0.6353383458646616,Published as a conference paper at ICLR 2022
REFERENCES,0.6390977443609023,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021b."
REFERENCES,0.6428571428571429,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
Bkg6RiCqY7."
REFERENCES,0.6466165413533834,"Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous Re-
laxation of Discrete Random Variables. In International Conference on Learning Representations,
2017."
REFERENCES,0.650375939849624,"Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European conference on computer vision, pp. 69‚Äì84. Springer, 2016."
REFERENCES,0.6541353383458647,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. preprint arXiv:1807.03748, 2018."
REFERENCES,0.6578947368421053,"Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe
Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. Intermediate-task transfer learning
with pretrained language models: When and why does it work?
In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics. Association for Computational
Linguistics, July 2020."
REFERENCES,0.6616541353383458,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-
text transformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67, 2020. URL http://jmlr.org/
papers/v21/20-074.html."
REFERENCES,0.6654135338345865,"A. Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021."
REFERENCES,0.6691729323308271,"Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-Ô¨Ådelity images with
VQ-VAE-2. In Advances in Neural Information Processing Systems, volume 32. Curran Associates,
Inc., 2019."
REFERENCES,0.6729323308270677,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet
large scale visual recognition challenge. IJCV, 2015."
REFERENCES,0.6766917293233082,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1715‚Äì1725, Berlin, Germany, August 2016. Association
for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://www.aclweb.
org/anthology/P16-1162."
REFERENCES,0.6804511278195489,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herv√© J√©gou. Training data-efÔ¨Åcient image transformers & distillation through attention. preprint
arXiv:2012.12877, 2020."
REFERENCES,0.6842105263157895,"Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv√© J√©gou. Going
deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021."
REFERENCES,0.6879699248120301,"Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. SelÔ¨Åe: Self-supervised pretraining for image
embedding. arXiv preprint arXiv:1906.02940, 2019."
REFERENCES,0.6917293233082706,"Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, NIPS‚Äô17, pp. 6309‚Äì6318, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN
9781510860964."
REFERENCES,0.6954887218045113,Published as a conference paper at ICLR 2022
REFERENCES,0.6992481203007519,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998‚Äì6008, 2017."
REFERENCES,0.7030075187969925,"Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via
non-parametric instance discrimination. In CVPR, 2018."
REFERENCES,0.706766917293233,"Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. UniÔ¨Åed perceptual parsing for
scene understanding. In ECCV, 2018."
REFERENCES,0.7105263157894737,"Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-supervised
learning with swin transformers. arXiv preprint arXiv:2105.04553, 2021."
REFERENCES,0.7142857142857143,"Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.
arXiv preprint arXiv:2106.04560, 2021."
REFERENCES,0.7180451127819549,"Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016."
REFERENCES,0.7218045112781954,"Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu,
Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentation
from a sequence-to-sequence perspective with transformers. CoRR, abs/2012.15840, 2020. URL
https://arxiv.org/abs/2012.15840."
REFERENCES,0.7255639097744361,"Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
Semantic understanding of scenes through the ADE20K dataset. Int. J. Comput. Vis., 127(3):
302‚Äì321, 2019. doi: 10.1007/s11263-018-1140-0. URL https://doi.org/10.1007/
s11263-018-1140-0."
REFERENCES,0.7293233082706767,Published as a conference paper at ICLR 2022
REFERENCES,0.7330827067669173,"A
ARCHITECTURE VARIANTS OF VISION TRANSFORMER"
REFERENCES,0.7368421052631579,"We use the standard vision Transformer (ViT; Dosovitskiy et al. 2020) in the experiments for fair
comparisons. In addition, we Ô¨Ånd that LayerScale (Touvron et al., 2021) and relative position
bias (Bao et al., 2020; Raffel et al., 2020) improve ViTs on downstream tasks. We employ the same
setting as in Section 3.3 for ablation studies, which pretrains base-size models for 300 epochs on
ImageNet-1K."
REFERENCES,0.7406015037593985,"As shown in Table 5, both LayerScale and relative position bias improve performance on ImageNet
classiÔ¨Åcation and ADE20K semantic segmentation. We denote the improved architecture as BEIT+
and use it for the experiments in Appendix B. We empirically notice that vanilla Transformer is the
most stable when scaling up the model to billions of parameters, so we do not use LayerScale for
extra-large models."
REFERENCES,0.7443609022556391,"Architecture
ImageNet
ADE20K"
REFERENCES,0.7481203007518797,"ViT (used in this paper)
82.86
44.86
ViT+LayerScale
83.00
45.43
ViT+LayerScale+Relative Position Bias
83.22
45.70"
REFERENCES,0.7518796992481203,"Table 5: Ablation studies of architecture variants on image classiÔ¨Åcation and semantic segmentation.
For ADE20K, we use UperNet (Xiao et al., 2018) as the task layer, and report mIoU scores of
single-scale inference."
REFERENCES,0.7556390977443609,"B
COMPARISON WITH LARGE-SCALE SUPERVISED PRE-TRAINING"
REFERENCES,0.7593984962406015,"We compare with state-of-the-art supervised pre-training at scale. In addition to using ImageNet-1K
for fair comparisons with previous work, we pretrain BEIT on ImageNet-22K to boost performance.
We employ the architecture improvements (i.e., LayerScale, and relative position bias) as described
in Appendix A, which is denoted as BEIT+ in Table 6 and Table 7. We follow the same pre-training
setup as in Section 2.5, except we pretrain 150 epochs on ImageNet-22K. After self-supervised
pre-training, we conduct intermediate Ô¨Åne-tuning on ImageNet-22K for 90 epochs. Moreover, we use
an in-house dataset that has about 70M labeled images as a drop-in replacement of ImageNet-22K."
REFERENCES,0.7631578947368421,"Models
Model
Size
Labeled
Data Size
ImageNet
3842
5122"
REFERENCES,0.7669172932330827,"Supervised Pre-Training on ImageNet-22K (using labeled data)
ViT-B (Dosovitskiy et al., 2020)
86M
14M
84.0
-
ViT-L (Dosovitskiy et al., 2020)
307M
14M
85.2
85.30
ViT-H (Dosovitskiy et al., 2020)
632M
14M
85.1
-"
REFERENCES,0.7706766917293233,"Supervised Pre-Training on Google JFT-300M (using labeled data)
ViT-B (Dosovitskiy et al., 2020)
86M
300M
84.2
-
ViT-L (Dosovitskiy et al., 2020)
307M
300M
87.1
87.76
ViT-H (Dosovitskiy et al., 2020)
632M
300M
88.0
88.55"
REFERENCES,0.7744360902255639,"Supervised Pre-Training on Google JFT-3B (using labeled data)
ViT-B (Zhai et al., 2021)
86M
3000M
86.6
-
ViT-L (Zhai et al., 2021)
307M
3000M
88.5
-"
REFERENCES,0.7781954887218046,"Self-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-22K
BEIT-B+ (ours)
86M
14M
86.8
-
BEIT-L+ (ours)
307M
14M
88.4
88.6"
REFERENCES,0.7819548872180451,"Self-Supervised Pre-Training, and Intermediate Fine-Tuning on In-House-70M
BEIT-L+ (ours)
307M
70M
89.3
89.5"
REFERENCES,0.7857142857142857,"Table 6: Top-1 accuracy on ImageNet-1K Ô¨Åne-tuning. We evaluate models at resolutions 3842 and
5122."
REFERENCES,0.7894736842105263,Published as a conference paper at ICLR 2022
REFERENCES,0.793233082706767,"Table 6 compares BEIT with previous state-of-the-art supervised pre-training (Dosovitskiy et al.,
2020; Zhai et al., 2021) on ImageNet Ô¨Åne-tuning. Rather than heavily relying on extremely large-size
labeled data (such as Google‚Äôs in-house JFT-300M and JFT-3B), we demonstrate that BEIT pre-
training can catch up with only ImageNet-22k (14M). SpeciÔ¨Åcally, BEIT-L Ô¨Åne-tuned on ImageNet-
22K achieves comparable performance with ViT-L trained on Google JFT-3B. Moreover, BEIT-L
obtains 89.5% top-1 accuracy on ImageNet after intermediate Ô¨Åne-tuning on an in-house 70M dataset.
The results indicate that BEIT pre-training greatly reduces the required labeling efforts and advances
the new state of the art for large-size vision Transformers."
REFERENCES,0.7969924812030075,"As shown in Table 7, we report the Ô¨Åne-tuning results on the ADE20K semantic segmentation
benchmark. Following Swin (Liu et al., 2021b), we use the same task layer (i.e., UperNet; Xiao et al.
2018) and evaluate the models at the resolution 640√ó640. The BEIT-L model obtains state-of-the-art
performance on ADE20K."
REFERENCES,0.8007518796992481,"Models
mIoU (%)
Multi-Scale mIoU (%)"
REFERENCES,0.8045112781954887,"Supervised Pre-Training on ImageNet-22K (using labeled data)
Swin-B (Liu et al., 2021b)
50.0
51.7
Swin-L (Liu et al., 2021b)
52.1
53.5"
REFERENCES,0.8082706766917294,"Self-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-22K
BEIT-B+ (ours)
53.6
54.2
BEIT-L+ (ours)
56.7
57.0"
REFERENCES,0.8120300751879699,"Self-Supervised Pre-Training, and Intermediate Fine-Tuning on In-House-70M
BEIT-L+ (ours)
57.9
58.4"
REFERENCES,0.8157894736842105,"Table 7: Performance comparison on the ADE20K semantic segmentation. We follow Swin-L (Liu
et al., 2021b) to use UperNet (Xiao et al., 2018) as the task layer and evaluate at resolution 640 √ó 640."
REFERENCES,0.8195488721804511,"C
ABLATION STUDIES OF IMAGE TOKENIZER"
REFERENCES,0.8233082706766918,"For comparison, we re-train the image tokenizer on ImageNet-1K. The reimplementation is based on
https://github.com/lucidrains/DALLE-pytorch. We use the same codebook size
8K as in DALL-E (Ramesh et al., 2021). Then we plug the tokenizer into our pre-training process.
We follow the same experimental setup of ablation studies as in Section 3.3. Table 8 shows that
our reimplemented tokenizer obtains comparable reconstruction loss and ImageNet Ô¨Åne-tuning
performance compared with the off-the-shelf DALL-E tokenizer."
REFERENCES,0.8270676691729323,"Image Tokenizer
Reconstruction Error
ImageNet"
REFERENCES,0.8308270676691729,"DALL-E Tokenizer (Ramesh et al., 2021)
0.0856
82.86
Our reimplementation
0.0880
82.70"
REFERENCES,0.8345864661654135,"Table 8: Top-1 accuracy on ImageNet-1K using different image tokenizers during pre-training. For
image reconstruction, we report mean absolute error of normalized RGB values. The reimplemented
image tokenizer is trained on ImageNet-1K without labels."
REFERENCES,0.8383458646616542,"D
LINEAR PROBES ON IMAGENET"
REFERENCES,0.8421052631578947,"We evaluate linear probes on ImageNet for various pretrained vision Transformers. We compare
BEIT with two main strands of work, namely discriminative and generative self-supervised learning.
The Ô¨Årst one applies discriminative learning for pre-training, such as contrastive learning (Chen et al.,
2021), and self distillation (Caron et al., 2021). The above methods typically learn to aggregate the
image-level features into a global vector, which is relatively suitable for linear probing. In contrast,
the second strand of methods, such as iGPT (Chen et al., 2020a) and ours, usually do not pretrain
such global feature aggregation, which tends to make linear probes difÔ¨Åcult."
REFERENCES,0.8458646616541353,Published as a conference paper at ICLR 2022
REFERENCES,0.849624060150376,"Following iGPT (Chen et al., 2020a), we use average pooling to aggregate the hidden states of each
image patches, and add the probing layer at the middle layer of Transformer instead of always at the
Ô¨Ånal layer. Similarly, we Ô¨Ånd that the best layer lies in 9-th layer for BEIT-B, and 14-th layer for
BEIT-L. To be speciÔ¨Åc, we use AdamW (Loshchilov & Hutter, 2019) to update the linear probe layer
for 50 epochs. The learning rate is 4e-3 with cosine decay. The batch size is 1024. The weight decay
is set to 1e-4. We follow data augmentation used in DINO (Caron et al., 2021), which uses random
resize crops and horizontal Ô¨Çips augmentation during training and evaluates on central crops."
REFERENCES,0.8533834586466166,"Models
Model Size
Accuracy"
REFERENCES,0.8571428571428571,"Discriminative self-supervised learning
DINO-B (Caron et al., 2021)
86M
78.2
MoCo v3-B (Chen et al., 2021)
86M
76.7
MoCo v3-L (Chen et al., 2021)
307M
77.6"
REFERENCES,0.8609022556390977,"Generative self-supervised learning
iGPT-L (Chen et al., 2020a)
1362M
65.2
iGPT-XL (Chen et al., 2020a)
6801M
68.7
iGPT-XL (Chen et al., 2020a)
6801M
72.0‚àó
BEIT-B (ours)
86M
56.7
BEIT-L (ours)
307M
73.5"
REFERENCES,0.8646616541353384,"Table 9: Linear probing accuracy on ImageNet. ‚Äú‚àó‚Äù denotes that iGPT-XL uses concatenation of Ô¨Åve
layers for linear probing, while others use the features of single layer."
REFERENCES,0.868421052631579,"As shown in Table 9, we evaluate linear probes on ImageNet-1K for self-supervised learning. Overall,
discriminative methods perform better than generative pre-training on linear probing. Linear probes
keep the Transformer parameters Ô¨Åxed and only update the linear layer. So the pre-training of global
aggregation of image-level features is beneÔ¨Åcial to linear probing in DINO and MoCo v3, although
full Ô¨Åne-tuning eliminates the gap. Moreover, the results indicate that increasing the model size from
base (86M) to large (304M) signiÔ¨Åcantly improves accuracy for our proposed method. In contrast,
the gap between base- and large-size MoCo v3 is smaller. We also Ô¨Ånd that BEIT outperforms iGPT
by a large margin even using much fewer parameters."
REFERENCES,0.8721804511278195,"E
MULTI-TASK PRE-TRAINING WITH DINO"
REFERENCES,0.8759398496240601,"We train the pre-training tasks of BEIT and DINO (Caron et al., 2021) together in a multi-task
manner. As shown in Table 10, augmenting masked image modeling with DINO improves semantic
segmentation on ADE20K, and obtains comparable results on ImageNet classiÔ¨Åcation. Moreover,
BEIT is more efÔ¨Åcient in terms of pre-training speed, as DINO has two copies of Transformer
parameters for self-distillation and multi-crop augmentation (Caron et al., 2020). For the throughput
comparisons between BEIT and BEIT+DINO, we set batch size to the same. Because BEIT is also
more memory-efÔ¨Åcient, we can use larger batch size to fully utilize GPU cards, which obtains greater
speedup in practice than the reported numbers."
REFERENCES,0.8796992481203008,"Models
ImageNet
ADE20K
Pre-Training Throughput"
REFERENCES,0.8834586466165414,"DINO (400 Epochs)
82.8
44.08
-
BEIT (300 Epochs)
82.9
44.65
4.2x
BEIT + DINO (300 Epochs)
82.9
46.85
1.0x"
REFERENCES,0.8872180451127819,"Table 10: We train the pre-training tasks of BEIT and DINO (Caron et al., 2021) in the way of
multi-task learning. We report the performance by Ô¨Åne-tuning on ImageNet-1K image classiÔ¨Åcation
and ADE20K semantic segmentation. For ADE20K, we use SETR-PUP (Zheng et al., 2020) as the
task layer and report the mIoU score of single-scale inference. The pre-training throughput measures
the speed, where larger numbers indicate faster pre-training."
REFERENCES,0.8909774436090225,Published as a conference paper at ICLR 2022
REFERENCES,0.8947368421052632,"F
IMAGE CLASSIFICATION ON CIFAR-100"
REFERENCES,0.8984962406015038,"In addition to ImageNet classiÔ¨Åcation, we conduct Ô¨Åne-tuning experiments on the CIFAR-
100 (Krizhevsky & Hinton, 2009) benchmark with 100 classes and 60k images. The experimental
setup is the same as in Section 3.1."
REFERENCES,0.9022556390977443,"Table 11 reports the top-1 accuracy on CIFAR-100. Notably, on the smaller CIFAR-100 dataset,
ViT trained from scratch only reaches 48.5% accuracy (Chen et al., 2021). In comparison, BEIT
achieves 90.1% with the help of pre-training. The results indicate that BEIT can greatly reduce
the requirement of annotation efforts. BEIT also outperforms MoCo v3. Moreover, intermediate
Ô¨Åne-tuning on ImageNet-1K further improves the results on CIFAR-100."
REFERENCES,0.9060150375939849,"Models
CIFAR-100"
REFERENCES,0.9097744360902256,"Training from scratch (i.e., random initialization)
ViT384 (Dosovitskiy et al., 2020)
48.5*"
REFERENCES,0.9135338345864662,"Supervised Pre-Training on ImageNet-1K (using labeled data)
ViT384 (Dosovitskiy et al., 2020)
87.1
DeiT (Touvron et al., 2020)
90.8"
REFERENCES,0.9172932330827067,"Self-Supervised Pre-Training on ImageNet-1K (without labeled data)
DINO (Caron et al., 2021)
91.7
MoCo v3 (Chen et al., 2021)
87.1
BEIT (ours)
90.1"
REFERENCES,0.9210526315789473,"Self-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-1K
BEIT (ours)
91.8"
REFERENCES,0.924812030075188,"Table 11: Top-1 accuracy of image classiÔ¨Åcation on CIFAR-100. The models are at resolution
224 √ó 224, except ViT384 uses 384 √ó 384. The results, unless otherwise indicated, are all obtained
by base-size models. *: result is taken from (Chen et al., 2021)."
REFERENCES,0.9285714285714286,"G
HYPERPARAMETERS FOR PRE-TRAINING"
REFERENCES,0.9323308270676691,"Hyperparameters
Base Size
Large Size"
REFERENCES,0.9360902255639098,"Layers
12
24
Hidden size
768
1024
FFN inner hidden size
3072
4096
Attention heads
12
16
Attention head size
64
Patch size
16 √ó 16"
REFERENCES,0.9398496240601504,"Training epochs
800
Batch size
2048
Adam œµ
1e-8
Adam Œ≤
(0.9, 0.999)
Peak learning rate
1.5e-3
Minimal learning rate
1e-5
Learning rate schedule
Cosine
Warmup epochs
10"
REFERENCES,0.943609022556391,"Gradient clipping
3.0
1.0
Dropout

Stoch. depth
0.1
Weight decay
0.05"
REFERENCES,0.9473684210526315,"Data Augment
RandomResizeAndCrop
Input resolution
224 √ó 224
Color jitter
0.4"
REFERENCES,0.9511278195488722,Table 12: Hyperparameters for pre-training BEIT on ImageNet-1K.
REFERENCES,0.9548872180451128,Published as a conference paper at ICLR 2022
REFERENCES,0.9586466165413534,"H
HYPERPARAMETERS FOR IMAGE CLASSIFICATION FINE-TUNING"
REFERENCES,0.9624060150375939,"Hyperparameters
CIFAR-100
ImageNet-1K
Base Size
Base Size
Large Size
Peak learning rate
{2e-3, 3e-3, 4e-3, 5e-3}
Fine-tuning epochs
150
100
50
Batch size
512
1024
1024
Warmup epochs
20
20
5
Layer-wise learning rate decay
0.65
0.65
0.75
Adam œµ
1e-8
Adam Œ≤
(0.9, 0.999)
Minimal learning rate
1e-6
Learning rate schedule
Cosine"
REFERENCES,0.9661654135338346,"Repeated Aug



Weight decay
0.3
0.05
0.05
Label smoothing Œµ
0.1
Stoch. depth
0.1
Dropout

Gradient clipping
"
REFERENCES,0.9699248120300752,"Erasing prob.

0.25
0.25
Input resolution
224 √ó 224
Rand Augment
9/0.5
Mixup prob.
0.8
Cutmix prob.
1.0"
REFERENCES,0.9736842105263158,Table 13: Hyperparameters for Ô¨Åne-tuning BEIT on ImageNet-1K and CIFAR-100.
REFERENCES,0.9774436090225563,"I
HYPERPARAMETERS FOR ADE20K SEMANTIC SEGMENTATION
FINE-TUNING"
REFERENCES,0.981203007518797,"Hyperparameters
Base Size"
REFERENCES,0.9849624060150376,"Peak learning rate
1e-3
Fine-tuning steps
160K
Batch size
16
Adam œµ
1e-8
Adam Œ≤
(0.9, 0.999)
Layer-wise learning rate decay
0.65
Minimal learning rate
0
Learning rate schedule
Linear
Warmup steps
1500"
REFERENCES,0.9887218045112782,"Dropout

Stoch. depth
0.1
Weight decay
0.05"
REFERENCES,0.9924812030075187,"Input resolution
512 √ó 512
Position embedding interpolate
bilinear"
REFERENCES,0.9962406015037594,Table 14: Hyperparameters for Ô¨Åne-tuning BEIT on ADE20K.
