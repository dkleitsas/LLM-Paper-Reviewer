Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035714285714285713,"Graph Neural Networks (GNNs) have emerged as powerful tools to encode graph
structured data. Due to their broad applications, there is an increasing need to
develop tools to explain how GNNs make decisions given graph structured data.
Existing learning-based GNN explanation approaches are task-speciﬁc in training
and hence suffer from crucial drawbacks. Speciﬁcally, they are incapable of pro-
ducing explanations for a multitask prediction model with a single explainer. They
are also unable to provide explanations in cases where the GNN is trained in a
self-supervised manner, and the resulting representations are used in future down-
stream tasks. To address these limitations, we propose a Task-Agnostic GNN
Explainer (TAGE) trained under self-supervision with no knowledge of down-
stream tasks. TAGE enables the explanation of GNN embedding models with-
out downstream tasks and allows efﬁcient explanation of multitask models. Our
extensive experiments show that TAGE can signiﬁcantly speed up the explanation
efﬁciency by using the same model to explain predictions for multiple downstream
tasks while achieving explanation quality as good as or even better than current
state-of-the-art GNN explanation approaches."
INTRODUCTION,0.007142857142857143,"1
INTRODUCTION"
INTRODUCTION,0.010714285714285714,"Graph neural networks (GNNs) (Kipf & Welling, 2017; Veliˇckovi´c et al., 2018; Xu et al., 2019) have
achieved remarkable success in learning from real-world graph-structured data due to their unique
ability to capture both feature-wise and topological information. Extending their success, GNNs
are widely applied in various research ﬁelds and industrial applications including quantum chem-
istry (Gilmer et al., 2017), drug discovery (Wu et al., 2018; Wang et al., 2020), social networks (Fan
et al., 2019), and recommender systems (Ying et al., 2018). While multiple approaches have been
proposed and studied to improve GNN performance, GNN explainability is an emerging area and
has a smaller body of research behind it. Recently, explainability has gained more attention due to
an increasing desire for GNNs with more security, fairness, and reliability. Being able to provide a
good explanation to a GNN prediction increases model reliability and reduces the risk of incorrect
predictions, which is crucial in ﬁelds such as molecular biology, chemistry, fraud detection, etc."
INTRODUCTION,0.014285714285714285,"Existing methods adapting the explanation methods for convolutional neural networks (CNNs) or
speciﬁcally designed for GNNs have shown promising explanations on multiple types of graph data.
A recent survey (Yuan et al., 2020) categorizes existing explanation methods into gradient-based,
perturbation, decomposition, and surrogate methods. In particular, perturbation methods involve
learning or optimization (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Lin et al., 2021) and,
while bearing higher computational costs, generally achieve state-of-the-art performance in terms
of explanation quality. These methods train post-hoc explanation models on top of the prediction
model to be explained. Earlier approaches like GNNExplainer (Ying et al., 2019) require training or
optimizing an individual explainer for each data instance, i.e., a graph or a node to be explained. In
contrast, PGExplainer (Luo et al., 2020) performs inductive learning, i.e., it only requires a one-time
training, and the explainer can be generalized to explain all data instances without individual op-
timization. Compared to other optimization-based explanation methods, PGExplainer signiﬁcantly
improves the efﬁciency in terms of time cost without performance loss by learning."
INTRODUCTION,0.017857142857142856,"However, even state-of-the-art explanation methods like PGExplainer are still task-speciﬁc at train-
ing and hence suffer from two crucial drawbacks. First, current methods are inefﬁcient in explaining
multitask prediction for graph-structured data. For example, one may need to predict multiple chem-
ical properties in drug discovery for a molecular graph. In particular, ToxCast from MoleculeNet has"
INTRODUCTION,0.02142857142857143,Under review as a conference paper at ICLR 2022 GNN
INTRODUCTION,0.025,Embedding Model MLP 1
INTRODUCTION,0.02857142857142857,"Task 1 
prediction
Node/graph"
INTRODUCTION,0.03214285714285714,embedding
INTRODUCTION,0.03571428571428571,Condition vector
INTRODUCTION,0.039285714285714285,"(dimension 
importance)"
INTRODUCTION,0.04285714285714286,Downstream
INTRODUCTION,0.04642857142857143,"Explainer
Embedding"
INTRODUCTION,0.05,Explainer
INTRODUCTION,0.05357142857142857,Important subgraph
INTRODUCTION,0.05714285714285714,"Trained independently from downstream tasks
Can explain multiple downstream tasks"
INTRODUCTION,0.060714285714285714,"Downstream model
Multitask model"
INTRODUCTION,0.06428571428571428,Task-speciﬁc
INTRODUCTION,0.06785714285714285,explainer 1
INTRODUCTION,0.07142857142857142,GNN+MLP
INTRODUCTION,0.075,Predictions
INTRODUCTION,0.07857142857142857,Task-speciﬁc
INTRODUCTION,0.08214285714285714,explainer 2
INTRODUCTION,0.08571428571428572,Task-speciﬁc
INTRODUCTION,0.08928571428571429,"explainer 3
…. …."
INTRODUCTION,0.09285714285714286,"End-to-end task-speciﬁc GNN explainers
Two-stage task-agnostic GNN explanations"
INTRODUCTION,0.09642857142857143,"Need train diﬀerent explainers to explain a multitask 
prediction model. Unable to train without downstream."
INTRODUCTION,0.1,"Figure 1: A comparison between typical end-to-end task-speciﬁc GNN explainers and the proposed
task-agnostic explanation pipeline. To explain a multitask model, typical explanation pipelines need
to optimize multiple explainers, whereas the two-stage explanation pipeline only learns one embed-
ding explainer that can cooperate with multiple lightweight downstream explainers."
INTRODUCTION,0.10357142857142858,"167 prediction tasks. In these cases, it is common to apply a single GNN model with multiple output
dimensions to make predictions for all tasks. However, one is unable to employ a single explainer to
explain the above model, since current explainers are trained speciﬁcally to explain one prediction
task. As a result, in the case of ToxCast, one must train 167 explainers to explain the GNN model.
Second, in industry settings, it is common to train GNN models in a two-stage fashion due to scal-
ing, latency, and label sparsity issues. The ﬁrst stage trains a GNN-based embedding model with
a massive amount of unlabeled data in an unsupervised manner to learn embeddings for nodes or
graphs. The second stage trains lightweight models such as multilayer perceptrons (MLPs) using the
frozen embeddings as input to predict the downstream tasks. In the ﬁrst stage, the downstream tasks
are usually unknown or undeﬁned, and existing task-speciﬁc explainers cannot be applied. Also,
there can be tens to hundreds of downstream tasks trained on these GNN embeddings, and training
a separate explainer for each task is undesirable and downright impossible."
INTRODUCTION,0.10714285714285714,"To address the above limitations, we present a new task-agnostic explanation pipeline, shown in
Figure 1, where we decompose a prediction model into a GNN embedding model and a downstream
model, designing separate explainers for each component. We design the downstream explainers to
cooperate with the embedding explainer. The embedding explainer is trained using a self-supervised
training framework, which we dub Task-Agnostic GNN Explainer (TAGE), with no knowledge of
downstream tasks, models, or labels. In contrast to existing explainers, the learning objective for
TAGE is computed at the graph or node embeddings without involving task-related predictions. In
addition to eliminating the need for downstream tasks in TAGE, we argue that the self-supervision
performed on the embeddings can bring additional performance boost in terms of the explanation
quality compared to existing task-speciﬁc baselines such as GNNExplainer and PGExplainer."
INTRODUCTION,0.11071428571428571,"We summarize our contributions as follows: 1) We introduce the task-agnostic explanation problem
and propose a two-stage explanation pipeline involving an embedding explainer and a downstream
explainer. This enables the explanation of multiple downstream tasks with a single embedding ex-
plainer. 2) We propose a self-supervised training framework TAGE, which is based on conditioned
contrastive learning to train the embedding explainer. TAGE requires no knowledge of downstream
tasks. 3) We perform experiments on real-world datasets and observe that TAGE outperforms ex-
isting learning-based explanation baselines in terms of explanation quality, universal explanation
ability, and the time required for training and inference."
TASK-AGNOSTIC EXPLANATIONS,0.11428571428571428,"2
TASK-AGNOSTIC EXPLANATIONS"
NOTATIONS AND LEARNING-BASED GNN EXPLANATION,0.11785714285714285,"2.1
NOTATIONS AND LEARNING-BASED GNN EXPLANATION"
NOTATIONS AND LEARNING-BASED GNN EXPLANATION,0.12142857142857143,"Our study considers the attributed graph G with node set V and edge set E. We formulate the
attributed graph as a tuple of matrices (A, X), where A ∈{0, 1}|V |×|V | denotes the adjacency
matrix and X ∈R|V |×df denotes the feature matrix with feature dimension of df. We assume
that the prediction model F that is to be explained operates on graph-structured data through two
components: a GNN-based embedding model and lighter downstream models. Denoting the input
space by G, a node-level embedding model En : G →R|V |×d takes a graph as input and computes"
NOTATIONS AND LEARNING-BASED GNN EXPLANATION,0.125,Under review as a conference paper at ICLR 2022
NOTATIONS AND LEARNING-BASED GNN EXPLANATION,0.12857142857142856,"Table 1: Comparisons on properties of common GNN explainers. Inductivity and task-agnosticism
are inapplicable for gradient/rule-based methods as they do not require learning. In the last column,
we show the number of required explainers for a dataset with N samples and M tasks."
NOTATIONS AND LEARNING-BASED GNN EXPLANATION,0.13214285714285715,"Learning
Inductive
Task-agnostic
# explainers required
Gradient- & Rule-based
No
-
-
1
GNNExplainer (Ying et al., 2019)
Yes
No
No
M ∗N
SubgraphX (Yuan et al., 2021)
Yes
No
No
M ∗N
PGExplainer (Luo et al., 2020)
Yes
Yes
No
M
Task-agnostic explainers
Yes
Yes
Yes
1"
NOTATIONS AND LEARNING-BASED GNN EXPLANATION,0.1357142857142857,"embeddings of dimension d for all nodes in the graph, whereas a graph-level embedding model
Eg : G →R1×d computes an embedding for the input graph. Subsequently, the downstream model
D : Rd →R computes predictions for the downstream task based on the embeddings."
NOTATIONS AND LEARNING-BASED GNN EXPLANATION,0.1392857142857143,"Typical GNN explainers consider a task-speciﬁc GNN-based model as a complete unit, i.e., F :=
D ◦E. Given a graph G and the GNN-based model F to be explained , our goal is to identify
the subgraph Gsub that contributes the most to the ﬁnal prediction made by F. In other words,
we claim that a given prediction is made because F captures crucial information provided by some
subgraph Gsub. The learning-based (or optimization-based) GNN explanation employs a parametric
explainer Tθ associated with the GNN model F to compute the subgraph Gsub of the given graph
data. Concretely, the explainer Tθ computes the importance score for each node or edge, denoted as
wi or wij, or masks for node attributes denoted as m. It then selects the subgraph Gsub induced by
important nodes and edges, i.e., whose scores exceed a threshold t, and by masking the unimportant
attributes. In our study, we follow Luo et al. (2020), focusing on the importance of edges to provide
explanations to GNNs. Formally, we have Gsub := (V, Esub) = Tθ(G), where Esub = {(vi, vj) :
(vi, vj) ∈E, wij ≥t}."
TASK-AGNOSTIC EXPLANATIONS,0.14285714285714285,"2.2
TASK-AGNOSTIC EXPLANATIONS"
TASK-AGNOSTIC EXPLANATIONS,0.14642857142857144,"As introduced in Section 1, all existing learning-based or optimization-based explanation approaches
are task-speciﬁc and hence suffer from infeasiblity or inefﬁciency in many real-application scenar-
ios. In particular, they are of limited use when downstream tasks are unknown or undeﬁned and fail
to employ a single explainer to explain a multitask prediction model."
TASK-AGNOSTIC EXPLANATIONS,0.15,"To enable the explanation of GNNs in two-stage training and multitask scenarios, we introduce a new
explanation paradigm called the task-agnostic explanation. The task-agnostic explanation considers
a whole prediction model as an embedding model followed by any number of downstream models. It
focuses on explaining the embedding model regardless of the number or the existence of downstream
models. In particular, the task-agnostic explanation trains only one explainer T (tag)
θ
to explain the
embedding model E, which should satisfy the following features. First, given an input graph G, the
explainer T (tag)
θ
should be able to provide different explanations according to speciﬁc downstream
tasks being studied. Table 1 compares the properties of common GNN explanation methods and the
desired task-agnostic explainers in multitask scenarios. Second, the explainer T (tag)
θ
can be trained
when only the embedding model is available, e.g., at the ﬁrst stage of a two-stage training paradigm,
regardless of the presence of downstream tasks. When downstream tasks and models are unknown,
T (tag)
θ
can still identify which components of the input graph are important for certain embedding
dimensions of interest."
THE TAGE FRAMEWORK,0.15357142857142858,"3
THE TAGE FRAMEWORK"
THE TAGE FRAMEWORK,0.15714285714285714,"Our explanation framework TAGE follows the typical scheme of GNN explanation introduced in
the previous section. It provides explanations by identifying important edges in a given graph and
removing the edges that lead to signiﬁcant changes in the ﬁnal prediction. Speciﬁcally, the goal of
the TAGE is to predict the importance score for each edge in a given graph. Different from existing
methods, the proposed TAGE breaks down typical end-to-end GNN explainers into two components.
We now provide general descriptions and detailed formulations to the proposed framework."
THE TAGE FRAMEWORK,0.16071428571428573,Under review as a conference paper at ICLR 2022
TASK-AGNOSTIC EXPLANATION PIPELINE,0.16428571428571428,"3.1
TASK-AGNOSTIC EXPLANATION PIPELINE"
TASK-AGNOSTIC EXPLANATION PIPELINE,0.16785714285714284,"Following the principle of the desired task-agnostic explanations, we introduce the task-agnostic
explanation pipeline, where a typical explanation procedure is performed in two steps. In particular,
we decompose the typical end-to-end learning-based GNN explainer into two parts: the embedding
explainer TE and the downstream explainer Tdown, corresponding to the two components in the
two-stage training and prediction procedure. We compare the typical explanation pipeline and the
two-stage explanation pipeline in Figure 1. The embedding explainer and downstream explainers
can be trained or constructed independently from each other. In addition, the embedding explainer
can cooperate with any downstream explainers to perform end-to-end explanations on input graphs."
TASK-AGNOSTIC EXPLANATION PIPELINE,0.17142857142857143,"The downstream explainer aims to explain task-speciﬁc downstream models. As downstream mod-
els are usually lightweight MLPs, we simply adopt gradient-based explainers for downstream ex-
plainers without training. The downstream explainer takes a downstream model and the graph or
node embedding vector as inputs and computes the importance score of each dimension on the em-
bedding vector. The importance scores then serve as a condition vector input to the embedding
explainer. Given the condition vector, the embedding explainer explains the GNN-based embed-
ding model by identifying an important subgraph from the input graph data. In other words, given
different condition vectors associated with different downstream tasks or models, the embedding
explainer can provide corresponding explanations for the same embedding model. Formally, we
denote the downstream explainer for models from D by Tdown : D × Rd →Rd, which maps in-
put models and embeddings into importance scores m for all embedding dimensions. We denote
the embedding explainer associated with the embedding model E by TE : Rd × G →G, which
maps a given graph into a subgraph of higher importance, conditioned on the embedding dimension
importance m ∈Rd."
TASK-AGNOSTIC EXPLANATION PIPELINE,0.175,"The training procedures of the embedding explainer are independent of downstream tasks or down-
stream explainers. In particular, the downstream explainer is obtained from the downstream model
only, and the training of the embedding explainer only requires the embedding model and the in-
put graphs. As downstream models are usually constructed as stacked fully connected (FC) layers
and the explanation of FC layers has been well studied, our study mainly focuses on the non-trivial
training procedure and design of the embedding explainer."
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.17857142857142858,"3.2
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.18214285714285713,"A straightforward idea of explaining an embedding model with no knowledge of downstream tasks
is to employ existing explainers and perform explanation on the pretext task, such as graph recon-
struction (Kipf & Welling, 2016) or context prediction (Hu et al., 2020), used during the pre-training
of GNNs. However, such explanations cannot generalize to future downstream tasks as there are lim-
ited dependencies between the pretext task and downstream tasks. Therefore, training an embedding
explainer without downstream models or labels is challenging, and it is desirable to develop a gen-
eralizable training approach for the embedding explainer. To this end, we propose a self-supervised
learning framework for the embedding explainer."
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.18571428571428572,"The learning objective of the proposed framework seeks to maximize the mutual information (MI)
between two embeddings with certain dimensions masked, i.e., one of the given graphs and one of
the corresponding subgraph of high importance induced by the explainer. We introduce a masking
vector p ∈Rd to indicate speciﬁc dimensions of embeddings on which to maximize MI. During
explanation, we obtain the masking vector from the importance vector computed by any downstream
explainer Tdown. As no downstream importance vector is available at training, we sample the mask-
ing vector p from a multivariate Laplace distribution due to the sparse gradient assumption, i.e., only
a few dimensions are of high importance. Formally, the MI-based learning objective is"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.18928571428571428,"max
θ
Ep[MI(p ⊗E(G), p ⊗E(Tθ(p, G)))],
(1)"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.19285714285714287,"where MI(·, ·) computes the mutual information between two random vectors, p denotes the random
masking vector sampled from a certain distribution, Tθ(p, G) computes the subgraph of high impor-
tance, and ⊗denotes the element-wise multiplication, which applies masking to the embeddings
E(·). Figure 2 outlines the training framework and objective. Intuitively, given an input graph and
the desired embedding dimensions to be explained, the explainer Tθ predicts the subgraph whose"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.19642857142857142,Under review as a conference paper at ICLR 2022
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2,Linear Projection x
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.20357142857142857,"Node Embeddings
Concatenation MLP"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.20714285714285716,"Edge 
importance"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.21071428571428572,Graph-level Task
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.21428571428571427,"Node-level Task x
MLP"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.21785714285714286,Embedding for the target node zi zj
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.22142857142857142,[zi; zj; ztarget]
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.225,"fn(p)
p wij zi zj p"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.22857142857142856,"wij
[zi; zj] fg(p) GNN GNN x x"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.23214285714285715,Masked subgraph embedding
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2357142857142857,Randomly generated
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2392857142857143,condition vector
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.24285714285714285,"Embedding 
Explainer T✓"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.24642857142857144,"Mutual Information Maximization
x
Element-wise Multiplication"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.25,Training Framework G
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.25357142857142856,"Gs = E(T✓(p, G))"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2571428571428571,Select Important Edges
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.26071428571428573,Embedding Model E p
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2642857142857143,p ⌦E(G)
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.26785714285714285,"p ⌦E(T✓(p, G))"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2714285714285714,Masked embedding
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.275,"Figure 2: Overviews of the self-supervised training framework for the embedding model (right) and
the architecture of the parametric explainers (left). During training, we generate random condition
vectors p as an input to the embedding explainer and mask the embeddings. The learning objective
seeks to maximize the mutual information between two embeddings on certain dimensions."
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2785714285714286,"embedding shares the maximum mutual information with the original embedding on the desired
dimensions."
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.28214285714285714,"Practically, the mutual information is intractable and is hence hard to directly compute. A common
approach to achieve efﬁcient computation and optimization is to adopt the upper bound estimations
of mutual information, namely, the Jenson-Shannon Estimator (JSE) (Nowozin et al., 2016) and
the InfoNCE (Gutmann & Hyv¨arinen, 2010). These upper bound estimations are also referred to as
contrastive loss and are widely applied in self-supervised representation learning (Hjelm et al., 2019;
Sun et al., 2019; Veliˇckovi´c et al., 2019) for both images and graphs. Adopting these estimators, the
objectives are efﬁciently computed as"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2857142857142857,"min
θ
1
N N
X"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.2892857142857143,"i=1
log

σ
 
(p ⊗zi)T (p ⊗zi,θ)

+
1
N 2 −N X"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.29285714285714287,"i̸=j
log

1 −σ
 
(p ⊗zi)T (p ⊗zj,θ)

, (2)"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.29642857142857143,"min
θ
−1 N N
X i=1 """
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.3,"log
exp{(p ⊗zi)T (p ⊗zi,θ)}
P"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.30357142857142855,"j̸=i exp{(p ⊗zi)T (p ⊗zj,θ)} # ,
(3)"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.30714285714285716,"for JSE and InfoNCE, respectively, where N denotes the number of samples in a mini-batch, σ
denotes Sigmoid function, zi and zi,θ are embeddings of the original graph Gi and its subgraph
Tθ(Gi), or target nodes of the two graphs. Our objective involves condition vectors as masks on
the embeddings, which differs from typical contrastive loss used in self-supervised representation
learning. We hence call the proposed objective the conditioned contrastive loss."
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.3107142857142857,"To restrict the size of subgraphs given by the explainer, we additionally add a size regularization
term R, computed as the averaged importance score, to the above objectives. In the case where edge
importance scores wij ∈[0, 1] are computed, the regularization term is computed as"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.3142857142857143,"R(G) =
X"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.31785714285714284,"(vi,vj)∈E
λs|wij| −λe [wij log wij −(1 −wij) log(1 −wij)] ,
(4)"
TRAINING EMBEDDING EXPLAINER UNDER SELF-SUPERVISION,0.32142857142857145,"where λs and λe are hyper-parameters controlling the size and the entropy of edge importance
scores, respectively."
EXPLAINER ARCHITECTURES,0.325,"3.3
EXPLAINER ARCHITECTURES"
EXPLAINER ARCHITECTURES,0.32857142857142857,"Embedding explainers. Inspired by explainer architectures used by PGExplainer, we adopt the
multilayer perceptron (MLP) to predict the importance score wij for each edge (ui, uj) ∈E, on
top of learned embeddings zi and zj of the two nodes connected by the edge. Edges with scores
higher than a threshold are considered as important edges that remain in the selected subgraph. In
order for the embedding explainer to cooperate with different downstream explainers and provide
diverse explanations for different tasks, it additionally requires a condition vector as input indicating"
EXPLAINER ARCHITECTURES,0.33214285714285713,Under review as a conference paper at ICLR 2022
EXPLAINER ARCHITECTURES,0.3357142857142857,"the speciﬁc downstream task to be explained. We handle the condition vector in a similar manner to
Conditional GAN (Mirza & Osindero, 2014). Formally, the graph-level embedding explainer takes
the embeddings, zi and zj, and the condition vector p as inputs and computes the importance score
by
wij = MLPg
 
[zi; zj] ⊗σ(fg(p))

,
(5)"
EXPLAINER ARCHITECTURES,0.3392857142857143,"where [·; ·] denotes the concatenation along the feature dimension, ⊗denotes the element-wise mul-
tiplication, σ denotes the activation function, and fg : Rd →R2d is a linear projection. The
node-level embedding explainer takes an additional node embedding as its input, as the explainers
are expected to predict different scores for the same edge when explaining different target nodes.
The formulation of computing the importance score is as follows,"
EXPLAINER ARCHITECTURES,0.34285714285714286,"wij = MLPn
 
[zi; zj; ztarget] ⊗σ(fn(p))

,
(6)"
EXPLAINER ARCHITECTURES,0.3464285714285714,"where fg : Rd →R3d is a linear projection, and ztarget denotes the embedding of the target node
whose prediction is to be explained."
EXPLAINER ARCHITECTURES,0.35,"Downstream explainers. We adopt the gradient-based explainer to explain the downstream models.
Formally, given an input embedding z and its prediction probabilities D(z) ∈[0, 1]C among all C
classes, we compute the gradient of the maximal probability w.r.t. the input embedding:"
EXPLAINER ARCHITECTURES,0.3535714285714286,g = ∂maxc≤C D(z)[c]
EXPLAINER ARCHITECTURES,0.35714285714285715,"∂z
∈R1×d,"
EXPLAINER ARCHITECTURES,0.3607142857142857,"where D(z)[c] denotes the probability for class c. To convert the gradient into the condition vector,
we further perform normalization and only take positive values reﬂecting only positive inﬂuence to
the predicted class probability, i.e., p = ReLU(norm(gT ))."
EXPERIMENTAL STUDIES,0.36428571428571427,"4
EXPERIMENTAL STUDIES"
EXPERIMENTAL STUDIES,0.3678571428571429,"We conduct two groups of quantitative studies evaluating the explanation quality and the universal
explanation ability, i.e., training a single explainer to explain all downstream tasks, of TAGE. We
then compare the efﬁciency of multiple learning-based GNN explainers in terms of training and
explanation time cost. We further provide visualizations to demonstrate the explanation quality as
well as the ability to explain GNN models without downstream tasks."
DATASETS,0.37142857142857144,"4.1
DATASETS"
DATASETS,0.375,"To demonstrate the effectiveness of the proposed TAGE on both node-level and graph-level tasks,
we evaluate TAGE on three groups of real-world datasets that contain potentially multiple tasks. The
datasets are described as follows and their statistics are summarized in Appendix A."
DATASETS,0.37857142857142856,"MoleculeNet. The MoleculeNet (Wu et al., 2018) library provides a collection of molecular graph
datasets for the prediction of different molecule properties. In a molecular graph, each atom in
the molecule is considered as a node, and each bond is considered as an edge. The prediction of
molecule properties is a graph-level task. We include three graph classiﬁcation tasks from Molecu-
leNet to evaluate the explanation of graph-level tasks: HIV, SIDER, and BACE."
DATASETS,0.3821428571428571,"Protein-Protein Interaction. The Protein-Protein Interaction (PPI) (Zitnik & Leskovec, 2017)
dataset documents the physical interactions between proteins in 24 different human tissues. In PPI
graphs, each protein is considered as a node with its motif and immunological features, and there
is an edge between two proteins if they interact with each other. Each node in the graphs has 121
binary labels associated with different protein functions. As different protein functions are not ex-
clusive to each other, the prediction of each protein function is considered an individual task. We
utilize the ﬁrst ﬁve out of 121 tasks to evaluate the explanation of node-level tasks."
DATASETS,0.38571428571428573,"E-commerce Product Network. The E-commerce Product Network (EPN)1 is constructed with
subsampled, anonymized logs from an e-commerce store, where entities including buyers, products,
merchants, and reviews are considered as nodes, and interactions between entities are considered as
edges. We subsample the data for the sake of experimental evaluations and the dataset characteristics"
PROPRIETARY DATASET,0.3892857142857143,1Proprietary dataset
PROPRIETARY DATASET,0.39285714285714285,Under review as a conference paper at ICLR 2022
PROPRIETARY DATASET,0.3964285714285714,"Figure 3: Quantitative performance comparisons with baseline methods on six tasks from Molecu-
leNet (top row) and PPI (bottom row). The curves are obtained by varying the threshold for selecting
important edges."
PROPRIETARY DATASET,0.4,"do not mirror actual production trafﬁc. We study the explanation of the classiﬁcation of fraudulent
entities (nodes), where the prediction for different types of entities are considered as individual tasks.
We evaluate our framework speciﬁcally on classiﬁcations of the buyer, merchant, and review nodes."
EXPERIMENT SETTINGS AND EVALUATION METRICS,0.4035714285714286,"4.2
EXPERIMENT SETTINGS AND EVALUATION METRICS"
EXPERIMENT SETTINGS AND EVALUATION METRICS,0.40714285714285714,"For each real-world dataset, we evaluate explainers on multiple downstream tasks that share a single
embedding model. For consistency with industrial use cases, we perform the two-stage training
paradigm to obtain GNN models to be explained. In particular, we ﬁrst use unlabeled graphs to train
the GNN-based embedding model in an unsupervised fashion. We then freeze the embedding model
and use the learned embeddings to train individual downstream models structured as 2-layer MLPs.
Speciﬁcally, for graph-level classiﬁcation tasks in MoleculeNet, we employ the GNN pretraining
strategy context prediction (Hu et al., 2020) to train a 5-layer GIN (Xu et al., 2019) as the embedding
model on ZINC-2M (Sterling & Irwin, 2015) containing 2 million unlabeled molecules. For the
node-level classiﬁcation on PPI, we employ the self-supervised training method GRACE (Zhu et al.,
2020) to train a 2-layer GCN (Kipf & Welling, 2017) on all 21 graphs from PPI without using labels.
For the larger-scale node-level classiﬁcation on EPN, we use graph autoencoder (GAE) (Kipf &
Welling, 2016) to train the embedding model on sampled subgraphs of EPN. More implementation
details are provided in Appendix B."
EXPERIMENT SETTINGS AND EVALUATION METRICS,0.4107142857142857,"As the involved real-world datasets do not have ground truth for explanations, we follow previous
studies (Pope et al., 2019; Yuan et al., 2020; 2021) to adopt a ﬁdelity score and a sparsity score to
quantitatively evaluate the explanations. Intuitively, the ﬁdelity score measures the level of change in
the probability of the predicted class when removing important nodes or edges, whereas the sparsity
score measures the relative amount of important nodes or nodes associated with important edges. A
formulation of the scores are provided in Appendix B. Note that compared to explanation evaluation
with ground truths, the ﬁdelity score is considered more faithful to the model, especially when the
model makes incorrect predictions, in which case the explanation ground truths become inconsistent
with evidence to making the wrong predictions. In practice, one needs to trade off between the
ﬁdelity score and the sparsity score by selecting the proper threshold for the importance."
QUANTITATIVE STUDIES,0.4142857142857143,"4.3
QUANTITATIVE STUDIES"
QUANTITATIVE STUDIES,0.41785714285714287,"We conduct two groups of quantitatively experimental comparisons. We ﬁrst demonstrate the ex-
planation quality of individual tasks in terms of the ﬁdelity score and the sparsity score. We do this
by comparing TAGE with multiple baseline methods including non-learning-based methods Grad-
CAM (Pope et al., 2019) and DeepLIFT (Shrikumar et al., 2017), as well as learning-based methods
GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020). We do not include other
optimization or search-based methods such as Monte-Carlo tree search (Jin et al., 2020) due to the"
QUANTITATIVE STUDIES,0.42142857142857143,Under review as a conference paper at ICLR 2022
QUANTITATIVE STUDIES,0.425,"Table 2: Fidelity scores with controlled sparsity on the node-level classiﬁcation dataset PPI. Each
column corresponds to an explainer model trained on (or without) a speciﬁc downstream task. Un-
derlines highlight the best explanation quality in terms of ﬁdelity, on the same level of sparsity."
QUANTITATIVE STUDIES,0.42857142857142855,"PGExplainer (trained on)
TAGE
Eval on
Task 0
Task 1
Task 2
Task 3
Task 4
w/o downstream
Task 0
0.184 ±0.3443
-0.005 ±0.268
0.033 ±0.335
0.034 ±0.310
0.018 ±0.194
0.271 ±0.385
Task 1
0.046 ±0.447
0.197 ±0.380
0.043 ±0.314
0.008 ±0.297
0.021 ±0.183
0.300 ±0.415
Task 2
0.028 ±0.434
0.001 ±0.283
0.345 ±0.458
0.024 ±0.320
0.097 ±0.320
0.499 ±0.480
Task 3
0.075 ±0.364
-0.015 ±0.219
0.036 ±0.317
0.262 ±0.418
0.040 ±0.221
0.289 ±0.427
Task 4
0.035 ±0.413
-0.021 ±0.238
0.223 ±0.438
0.075 ±0.374
0.242 ±0.373
0.330 ±0.442"
QUANTITATIVE STUDIES,0.43214285714285716,"signiﬁcant time cost on real-world datasets. Note that to show the effectiveness of universal expla-
nations over different downstream tasks, we only train one embedding explainer for all tasks in a
dataset, on top of which a gradient-based downstream explainer is applied to explain multiple down-
stream tasks. In contrast, for existing learning-based methods, we need to train multiple explainers
to explain downstream tasks individually. For all methods, we vary the threshold for selecting im-
portant nodes or edges and compare how ﬁdelity scores change over sparsity scores on each task and
dataset. The results are shown in Figure 4. In particular, TAGE outperforms other learning-based
explainers on BACE, SIDER, and PPI (tasks 0 and 1). For HIV and PPI (task 2), TAGE is more
effective at higher sparsity levels, i.e., when fewer nodes are considered important and masked."
QUANTITATIVE STUDIES,0.4357142857142857,"To justify the necessity of task-agnostic explanation and demonstrate the universal explanation abil-
ity of TAGE, we include PGExplainer as our baseline and compare the explanation quality when
adopting a single explainer to explain multiple downstream tasks. For PGExplainer, we train mul-
tiple explainers on different downstream tasks and evaluate each explainer on different downstream
tasks. For TAGE, we train one explainer without downstream tasks and evaluate it on different down-
stream tasks. Results shown in Table 2 (PPI) and Appendix D (MoleculeNet and EPN) indicate that
task-speciﬁc explainers fail to generalize to different downstream tasks and hence are unable to pro-
vide universal explanations. On the other hand, the task-agnostic explainer, although trained without
downstream tasks, can provide explanations with even higher quality for any downstream tasks."
QUANTITATIVE STUDIES,0.4392857142857143,"GNNExplainer and PGExplainer should generally outperform task-agnostic explainers, as they are
speciﬁc to data examples or tasks. This should especially be true when TAGE and PGExplainer have
the same level of parameters. However, we surprisingly ﬁnd that TAGE outperforms the learning-
based baselines. One possible reason can be the non-injective characteristic of the downstream
MLPs on top of GNNs, where different embeddings can produce similar downstream prediction
results. Due to this characteristic, the learning objective of TAGE computed between embeddings
brings stronger supervision than the objective computed between ﬁnal predictions, as the latter ob-
jective does not guarantee consistency between embeddings or between input graphs and subgraphs."
MULTITASK EXPLANATION EFFICIENCY,0.44285714285714284,"4.4
MULTITASK EXPLANATION EFFICIENCY"
MULTITASK EXPLANATION EFFICIENCY,0.44642857142857145,"A major advantage of the task-agnostic explanation is that it removes the need for training individual
explainers, which consumes the majority of the total time cost to explain a model on a dataset. We
hence evaluate the efﬁciency of TAGE in terms of time cost for explanation and compare it to the
two learning-based explainer baselines. We record the time cost for the training and inference of
different explanation methods on the same dataset and device, shown in Table 3. All results are
obtained from running the explanation on the PPI dataset with 121 node classiﬁcation tasks with a
single Nvidia Tesla V100 GPU. Although the inference time cost of TAGE is slightly higher than
that of PGExplainer, the results show TAGE costs signiﬁcantly less time than GNNExplainer and
PGExplainer, especially in the multitask cases (T > 1). TAGE allows the explanation of many
downstream tasks within a reasonable time duration."
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.45,"4.5
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.45357142857142857,"We visualize the explanations of the three learning-based explanation methods on the BACE task,
which aims to predict the inhibitor effect of molecules to human β-secretase 1 (BACE-1) (Wu et al.,
2018). Additional visualizations on HIV and SIDER are also provided in Appendix E. The visualiza-
tion results are shown in Figure 4. Each molecule visualization shows the top 10% important edges"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.45714285714285713,Under review as a conference paper at ICLR 2022
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.4607142857142857,"Table 3: Comparison of computational time cost among three learning-based GNN explainers on
the PPI dataset. The left two columns record time cost breakdown for T downstream tasks. The
fourth column estimates the total time cost for explaining all 121 tasks of PPI. The last row shows
the speedup times compared to GNNExplainer and PGExplainer, respectively."
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.4642857142857143,"Time cost
Training (s)
Inference (s)
Total time (T=1) (s)
Est. total for 121 tasks
GNNExplainer
20040.1*T
–
20040.1
28 d
PGExplainer
7117.0*T
427.2*T
7604.2
10.7 d
TAGE
1405.3
582.7*T
1988.0
0.83 d
Speedup
14.3*T × / 5.1*T ×
– / 0.73×
10.1× / 3.8×
33.7× / 12.9× F N N N H2N N O O
F F F F F N
H O N NH O O OH H2N+ O N
N+ H2
OH NH
O O O O"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.46785714285714286,-0.4210
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.4714285714285714,-0.0145
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.475,0.4659
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.4785714285714286,"PGExplainer
GNNExplainer F F N
H O N NH O O OH H2N+"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.48214285714285715,0.2883
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.4857142857142857,0.9964
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.48928571428571427,"0.5447 O N
N+ H2
OH NH
O O O O F N N N H2N N O O
F F F TAGE F F N
H O N NH O O OH H2N+"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.4928571428571429,0.7011
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.49642857142857144,0.5022
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5,"0.6183 O N
N+ H2
OH NH
O O O O F N N N H2N N O O
F F F"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5035714285714286,"TAGE-Single Dimension F F N
H O N NH O O OH H2N+ O N
N+ H2
OH NH
O O O O F N N N H2N N O O
F F F"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5071428571428571,(0.0193)
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5107142857142857,(0.0494)
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5142857142857142,(0.6287)
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5178571428571429,"TAGE-Single Dimension F F N
H O N NH O O OH H2N+ O N
N+ H2
OH NH
O O O O F N N N H2N N O O
F F F"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5214285714285715,(0.8153)
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.525,(0.6183)
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5285714285714286,(0.3705)
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5321428571428571,"(With downstream task)
(No downstream task)"
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5357142857142857,"Figure 4: Visualizations on explanations to the GNN model for the BACE task. Top 10% impor-
tant edges are highlighted with red shadow. The numbers below molecules are ﬁdelity scores when
masking-out the top 10% important edges. Right two columns are explanations to two certain em-
bedding dimensions without downstream tasks. Fidelity scores in the right two columns explaining
two embedding dimensions are still computed for the BACE task but are just for reference."
VISUALIZATIONS AND EXPLANATION TO EMBEDDING DIMENSIONS,0.5392857142857143,"(bonds) predicted by an explainer with red shadow, together with the ﬁdelity score on the molecule.
The left three columns are explanation results with the BACE downstream task. The right columns
are explanations by TAGE to two speciﬁc graph embedding dimensions, without downstream mod-
els. Embedding dimensions with greater values among all are selected in the visualizations. To
obtain explanations to certain embedding dimensions, we input the one-hot vectors to the embed-
ding explainer as condition vectors. The visualization results indicate that while baseline methods
select scattered edges as important, TAGE tends to select edges that form a connected substructure,
which is more reasonable when explaining molecule property predictions where a certain functional
group is important for the property. In addition, the right three columns indicate that dimensions in
the embedding correspond to different substructures and TAGE is able to provide explanations to the
dimensions without downstream tasks."
CONCLUSIONS,0.5428571428571428,"5
CONCLUSIONS"
CONCLUSIONS,0.5464285714285714,"Existing task-speciﬁc learning-based explainers become inapplicable under real scenarios when
downstream tasks or models are unavailable and suffer from inefﬁciency when explaining real-world
graph datasets with multiple downstream tasks. We introduced TAGE, including the task-agnostic
GNN explanation pipeline and the self-supervised training framework to train the embedding ex-
plainer without knowing downstream tasks or models. Our experiments demonstrate that the TAGE
generally achieves higher explanation quality in terms of ﬁdelity and sparsity with the signiﬁcantly
reduced explanation time cost. We discuss potential limitations and their solutions in Appendix G."
CONCLUSIONS,0.55,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5535714285714286,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.5571428571428572,"The experiment setting and evaluation protocol are described in Section 4. In particular, we de-
scribe how the models to be explained are trained. To further ensure reproducibility, we provide
implementation details including explainer conﬁgurations, training settings, and how evaluation are
performed in Appendix B. The detailed computation of the sparsity and ﬁdelity scores are provided
in Appendix C. The code to fully reproduce the results will be released upon acceptance."
REFERENCES,0.5607142857142857,REFERENCES
REFERENCES,0.5642857142857143,"Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pp. 417–426, 2019."
REFERENCES,0.5678571428571428,"Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019."
REFERENCES,0.5714285714285714,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International conference on machine learning, pp.
1263–1272. PMLR, 2017."
REFERENCES,0.575,"Michael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference
on Artiﬁcial Intelligence and Statistics, pp. 297–304, 2010."
REFERENCES,0.5785714285714286,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In International Conference on Learning Representations, 2019."
REFERENCES,0.5821428571428572,"Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure
Leskovec. Strategies for pre-training graph neural networks. In International Conference on
Learning Representations, 2020."
REFERENCES,0.5857142857142857,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using
interpretable substructures. In International Conference on Machine Learning, pp. 4849–4859.
PMLR, 2020."
REFERENCES,0.5892857142857143,"Thomas N Kipf and Max Welling.
Variational graph auto-encoders.
arXiv preprint
arXiv:1611.07308, 2016."
REFERENCES,0.5928571428571429,"Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. In International Conference on Learning Representations, 2017."
REFERENCES,0.5964285714285714,"Wanyu Lin, Hao Lan, and Baochun Li. Generative Causal Explanations for Graph Neural Networks.
In International Conference on Machine Learning, 2021."
REFERENCES,0.6,"Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu,
Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and
Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. Journal of
Machine Learning Research, 22(240):1–9, 2021. URL http://jmlr.org/papers/v22/
21-0343.html."
REFERENCES,0.6035714285714285,"Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang
Zhang. Parameterized explainer for graph neural network. In Advances in neural information
processing systems, 2020."
REFERENCES,0.6071428571428571,"Mehdi Mirza and Simon Osindero.
Conditional generative adversarial nets.
arXiv preprint
arXiv:1411.1784, 2014."
REFERENCES,0.6107142857142858,"Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271–279, 2016."
REFERENCES,0.6142857142857143,Under review as a conference paper at ICLR 2022
REFERENCES,0.6178571428571429,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32, pp. 8024–8035. 2019."
REFERENCES,0.6214285714285714,"Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Ex-
plainability methods for graph convolutional neural networks. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 10772–10781, 2019."
REFERENCES,0.625,"Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In International Conference on Machine Learning, pp. 3145–
3153, 2017."
REFERENCES,0.6285714285714286,"Teague Sterling and John J Irwin. Zinc 15–ligand discovery for everyone. Journal of Chemical
Information and Modeling, 55(11):2324–2337, 2015."
REFERENCES,0.6321428571428571,"Fan-Yun Sun, Jordan Hoffman, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-
supervised graph-level representation learning via mutual information maximization. In Interna-
tional Conference on Learning Representations, 2019."
REFERENCES,0.6357142857142857,"Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and Devon
Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2019."
REFERENCES,0.6392857142857142,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.6428571428571429,"Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi,
Zhuoning Yuan, Tianbao Yang, and Shuiwang Ji. Advanced graph and sequence neural networks
for molecular property prediction and drug discovery. arXiv preprint arXiv:2012.01981, 2020."
REFERENCES,0.6464285714285715,"Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S
Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learn-
ing. Chemical Science, 9(2):513–530, 2018."
REFERENCES,0.65,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural
networks? In International Conference on Learning Representations, 2019."
REFERENCES,0.6535714285714286,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974–
983, 2018."
REFERENCES,0.6571428571428571,"Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks. In Advances in neural information processing
systems, pp. 9244–9255, 2019."
REFERENCES,0.6607142857142857,"Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A
taxonomic survey. arXiv preprint arXiv:2012.15445, 2020."
REFERENCES,0.6642857142857143,"Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural
networks via subgraph explorations. arXiv preprint arXiv:2102.05152, 2021."
REFERENCES,0.6678571428571428,"Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive
representation learning.
In ICML Workshop on Graph Representation Learning and Beyond,
2020."
REFERENCES,0.6714285714285714,"Marinka Zitnik and Jure Leskovec.
Predicting multicellular function through multi-layer tissue
networks. Bioinformatics, 33(14):i190–i198, 2017."
REFERENCES,0.675,Under review as a conference paper at ICLR 2022
REFERENCES,0.6785714285714286,"A
DATASET STATISTICS"
REFERENCES,0.6821428571428572,The statistics of datasets used for evaluating TAGE are shown in Table 4.
REFERENCES,0.6857142857142857,"Table 4: Statistics of multitask datasets used for explanation quality evaluation. The column “Total”
under MoleculeNet indicates total number of commonly studied tasks from MoleculeNet."
REFERENCES,0.6892857142857143,"MoleculeNet
PPI
EPN
HIV
BBBP
BACE
Sider
Total
# of Graphs
41127
2039
1513
1427
–
24
1
Avg. # of Nodes
25.53
24.05
34.12
33.64
–
56,944
5.86 mn.
Avg. # of Edges
27.48
25.94
36.89
35.36
–
818,716
63.07 mn.
# of Tasks
1
1
1
27
227
121
3"
REFERENCES,0.6928571428571428,"B
IMPLEMENTATION DETAILS"
REFERENCES,0.6964285714285714,"Structure of explainer. Our implementation is based on Pytorch (Paszke et al., 2019), Pytorch Ge-
ometric (Fey & Lenssen, 2019), and Dive-into-graphs (Liu et al., 2021). We implement the explainer
with a linear projection fp that maps the condition vector p to the same dimension as concatenated
embeddings, and a 2-layer MLP with ReLU activation that maps concatenated embeddings with
mask to the important score."
REFERENCES,0.7,"Implementation of training objectives. We adopt the Jason-Shannon Estimator as the lower bound
for mutual information maximization for the two public datasets. For graph-level tasks, given a mini-
batch of N samples, we consider the embeddings of a graph G and its subgraph Gs as a positive pair
(with N positive pairs in total), and the embeddings of a graph Gi and the subgraph Gj,s of another
sample as a negative pair (with N 2 −N pairs in total). For node-level tasks, we still randomly
sample N nodes from the entire graph at each iteration and compute the contrastive losses on original
embeddings of the N nodes, and embedding of the N nodes when the important subgraph is selected,
respectively for each node. Similar to graph-level tasks, we consider embeddings of the same node
(in original graph or in subgraph) as a positive pair, and a embeddings of node i in full graph and
node j in selected subgraph as a negative pair (N 2 −N in total)."
REFERENCES,0.7035714285714286,"Training conﬁgurations. We set the hyperparameters in the size regularization term to λs = 0.05
and λe = 0.002, respectively. For graph-level explanation on MoleculeNet, we train the embed-
ding explainer on ZINC-2M with learning rate 1e −4 and mini-batch size 256 for 1 epoch. The
random condition vectors are generated from laplace distribution Laplace(0, 0.2). For node-level
explanation on PPI, we train the embedding explainer on PPI without labels with learning rate 5e−6
and mini-batch size 4 for 1 epoch. The random condition vectors are generated from laplace dis-
tribution Laplace(0, 0.1). For the EPN dataset, we train the embedding explainer with InfoNCE
loss, learning rate 1e −4 and mini-batch size 16. The random condition vectors are generated from
laplace distribution Laplace(0, 0.25). The hyperparameters in the size regularization term are set to
λs = 0.5 and λe = 0 for the stable training with InfoNCE."
REFERENCES,0.7071428571428572,"Evaluation. In molecule and protein property prediction, we are usually interested in the positive
samples, i.e., the existence of what substructure lead to a certain property. For learning-based base-
line methods, we ﬁnd it common that only one class of the two have good explanation, and the class
with higher explanation quality is not necessarily the positive class. For example, PGExplainer has
a near-to-zero ﬁdelity score for the positive class of SIDER. We hence compare only the higher
ﬁdelity score among two classes for all explanation methods and datasets."
REFERENCES,0.7107142857142857,Under review as a conference paper at ICLR 2022
REFERENCES,0.7142857142857143,"C
FIDELITY AND SPARSITY"
REFERENCES,0.7178571428571429,"Given a set of graphs {Gi} and node masks m predicted by the explainer, the ﬁdelity score and the
sparsity score are computed as follows."
REFERENCES,0.7214285714285714,Fidelity prob = 1 N XN
REFERENCES,0.725,"i=1

f(Gi)ci −f(G1−mi
i
)ci

,
(7)"
REFERENCES,0.7285714285714285,Sparsity = 1 N XN
REFERENCES,0.7321428571428571,"i=1 |mi|/|Vi|,
(8)"
REFERENCES,0.7357142857142858,"where N denotes the number of graphs or nodes to be explained, f denotes the GNN model as-
sociated with a speciﬁc downstream task, ci denotes the class of interest, which can be either the
labeled class or the original predicted class, Gi and G1−mi
i
denote the original graph and graph with
important nodes removed, respectively. Explanations with both scores higher are better."
REFERENCES,0.7392857142857143,"D
ADDITIONAL RESULTS FOR UNIVERSAL EXPLANATION ABILITY"
REFERENCES,0.7428571428571429,"Universal explanation performance on MoleculeNet and EPN. Evaluation results for the uni-
versal explanation ability on the MoleculeNet and EPN datasets are shown in Table 5 and Table 6,
respectively."
REFERENCES,0.7464285714285714,"Table 5: Fidelity scores with controlled sparsity on graph-level molecule property prediction tasks.
Each column corresponds to an explainer model trained on (or without) a speciﬁc downstream task.
Underlines highlight the best explanation quality in terms of ﬁdelity, on the same level of sparsity."
REFERENCES,0.75,"PGExplainer (trained on)
TAGE
Eval on
BACE
HIV
BBBP
SIDER
w/o downstream
BACE
0.252 ±0.340
0.007 ±0.251
0.026 ±0.022
-0.151 ±0.330
0.378 ±0.293
HIV
-0.001 ±0.197
0.473 ±0.404
0.013 ±0.029
-0.060 ±0.356
0.595 ±0.321
BBBP
0.001 ±0.237
-0.056 ±0.226
0.182 ±0.169
-0.252 ±0.440
0.193 ±0.161
SIDER
0.012 ±0.219
-0.009 ±0.212
0.003 ±0.029
0.444 ±0.391
0.521 ±0.278"
REFERENCES,0.7535714285714286,"Table 6: Fidelity scores with controlled sparsity on the E-commerce product dataset. Each column
corresponds to one explainer model trained on different tasks or without downstream task. Under-
lines highlight the best explanation quality in terms of ﬁdelity, on the same level of sparsity."
REFERENCES,0.7571428571428571,"PGExplainer (trained on)
TAGE
Eval on
Buyers
Sellers
Reviews
w/o downstream
Buyers
0.2009 ±0.2233
0.1731 ±0.3774
0.1740 ±0.4463
0.2713 ±0.1834
Sellers
0.5465 ±0.4773
0.3246 ±0.4026
0.1128 ±0.3019
0.6515 ±0.3426
Reviews
0.4178 ±0.3683
0.1258 ±0.3492
0.2310 ±0.4178
0.5692 ±0.4214"
REFERENCES,0.7607142857142857,"Comparison of explanation performance when trained on different datasets. Speciﬁcally for
the MoleculeNet dataset, as there is a larger unlabeled dataset, ZINC, available for the ﬁrst stage
training of encoder, the training of our exlainer is also performed on the ZINC dataset. For a more
strict comparison with the baseline explainer who is trained on individual MoleculeNet dataset, we
additional evaluate the explanation quality when the same individual MoleculeNet dataset is used
to train TAGE. The results are shown in Table 7. When trained on the same datasets individually,
TAGE still performs better than the baseline explainer in terms of ﬁdelity scores. In the individual
dataset case, we need to train different explainers, similarly to the training of PGExplainer, as the
datasets for the four tasks are different."
REFERENCES,0.7642857142857142,"Evaluation on the synthetic dataset BA-Shapes. On the BACE dataset and task, we additionally
compare TAGE with another recent SOTA learning-based method GEM (Lin et al., 2021) whose
explainer is trained based on the Granger causality in Table 8. Note that GEM is not originally
proposed under our setting. It assumes that there is a ﬁxed number of important nodes when per-
forming explanation and hence the ﬁnal explanation is a boolean selection of nodes. We adapt GEM
to compute ﬁdelity scores under different sparsity scores by varying the threshold when generating
explanation groundtruth with Granger causality."
REFERENCES,0.7678571428571429,Under review as a conference paper at ICLR 2022
REFERENCES,0.7714285714285715,"Table 7: An ablation on training TAGE on different datasets (ZINC v.s. individual MoleculeNet
datasets)."
REFERENCES,0.775,"Method
BACE
HIV
BBBP
SIDER
PGExplainer
0.252 ±0.340
0.473 ±0.404
0.182 ±0.169
0.444 ±0.391
TAGE (individual)
0.402 ±0.281
0.541 ±0.330
0.202 ±0.157
0.516 ±0.292
TAGE (ZINC)
0.378 ±0.293
0.595 ±0.321
0.193 ±0.161
0.521 ±0.278"
REFERENCES,0.7785714285714286,"Table 8: A comparison between TAGE, GEM, and PGExplainer on BACE in terms of ﬁdelity scores
when ﬁxing the sparsity scores. For GEM, we vary the threshold when generating explanation
groundtruth with Granger causality to obtain explanations with different sparsity scores."
REFERENCES,0.7821428571428571,"Sparsity
0.90
0.85
0.80
0.75
TAGE
0.3349
0.4992
0.5383
0.5309
GEM (Lin et al., 2021)
0.2829
0.3607
0.4260
0.4035
PGExplainer
0.2521
0.3207
0.4605
0.5161"
REFERENCES,0.7857142857142857,"E
VISUALIZATIONS ON HIV AND SIDER"
REFERENCES,0.7892857142857143,"Visualizations on HIV and SIDER are shown in Figure 5 and Figure 6, respectively. 0"
REFERENCES,0.7928571428571428,-0.0497
REFERENCES,0.7964285714285714,0.7741
REFERENCES,0.8,"PGExplainer
GNNExplainer"
REFERENCES,0.8035714285714286,-4.276e-17
REFERENCES,0.8071428571428572,0.0027
REFERENCES,0.8107142857142857,0.9810 TAGE
REFERENCES,0.8142857142857143,0.5763
REFERENCES,0.8178571428571428,0.9820
REFERENCES,0.8214285714285714,0.2617
REFERENCES,0.825,TAGE-Single Dimension
REFERENCES,0.8285714285714286,(0.0263) (0)
REFERENCES,0.8321428571428572,(0.4154)
REFERENCES,0.8357142857142857,TAGE-Single Dimension
REFERENCES,0.8392857142857143,(0.9894)
REFERENCES,0.8428571428571429,(0.4357)
REFERENCES,0.8464285714285714,(0.2890)
REFERENCES,0.85,"(With downstream task)
(No downstream task)"
REFERENCES,0.8535714285714285,"0.2156
0.4294"
REFERENCES,0.8571428571428571,"0.6444
(0.0140)
(3.099e-6) O OH OH O OH OH O
HO HO HO OH Cl Cl OH
Cl Cl
Cl O"
REFERENCES,0.8607142857142858,"O
N
Fe3- Cl
N O HO N+
O HO N+ O OH OH O OH OH O OH OH O OH OH O OH OH O
HO HO HO OH Cl Cl OH
Cl Cl
Cl O"
REFERENCES,0.8642857142857143,"O
N
Fe3- Cl
N O HO N+
O HO N+ O OH OH O
HO HO HO
O OH OH O
HO HO HO
O OH OH O
HO HO HO OH Cl Cl OH
Cl Cl
Cl OH Cl Cl OH
Cl Cl
Cl OH Cl Cl OH
Cl Cl
Cl O"
REFERENCES,0.8678571428571429,"O
N
Fe3- Cl
N O HO N+
O HO N+ O"
REFERENCES,0.8714285714285714,"O
N
Fe3- Cl
N O HO N+
O HO N+ O"
REFERENCES,0.875,"O
N
Fe3- Cl
N O HO N+
O HO N+"
REFERENCES,0.8785714285714286,"Figure 5: Visualizations on explanations to the GNN model for the HIV task. Top 10% impor-
tant edges are highlighted with red shadow. The numbers below molecules are ﬁdelity scores when
masking-out the top 10% important edges. Right two columns are explanations to two certain em-
bedding dimensions without downstream tasks."
REFERENCES,0.8821428571428571,Under review as a conference paper at ICLR 2022
REFERENCES,0.8857142857142857,0.3105
REFERENCES,0.8892857142857142,-0.6427
REFERENCES,0.8928571428571429,-0.0387
REFERENCES,0.8964285714285715,"PGExplainer
GNNExplainer"
REFERENCES,0.9,-0.0528
REFERENCES,0.9035714285714286,0.1565
REFERENCES,0.9071428571428571,0.0045 TAGE
REFERENCES,0.9107142857142857,0.4669
REFERENCES,0.9142857142857143,0.6403
REFERENCES,0.9178571428571428,0.3383
REFERENCES,0.9214285714285714,TAGE-Single Dimension
REFERENCES,0.925,(0.0942)
REFERENCES,0.9285714285714286,(0.6403)
REFERENCES,0.9321428571428572,(0.2652)
REFERENCES,0.9357142857142857,TAGE-Single Dimension
REFERENCES,0.9392857142857143,(0.0099)
REFERENCES,0.9428571428571428,(0.1826)
REFERENCES,0.9464285714285714,(0.0875)
REFERENCES,0.95,"(With downstream task)
(No downstream task) N S Cl O Cl NH O N N N O NH2 OH HO N
C O
O OH"
REFERENCES,0.9535714285714286,"-0.2515
-0.1046"
REFERENCES,0.9571428571428572,"0.8071
(0.3551)
(0.0286) N S Cl O Cl NH O N N N O NH2 OH HO N
C O
O OH N S Cl N S Cl N S Cl O Cl NH O N N N O NH2 OH HO N
C O
O OH O Cl NH O Cl NH O N N N O NH2 OH HO O N N N O NH2 OH HO N
C O
O OH N
C O
O OH"
REFERENCES,0.9607142857142857,"Figure 6: Visualizations on explanations to the GNN model for the SIDER task. Top 10% im-
portant edges are highlighted with red shadow. The numbers below molecules are ﬁdelity scores
when masking-out the top 10% important edges. Right two columns are explanations to two certain
embedding dimensions without downstream tasks."
REFERENCES,0.9642857142857143,"F
EXPERIMENTAL STUDIES ON THE SYNTHETIC DATASETS BA-SHAPES"
REFERENCES,0.9678571428571429,"We perform an additional evaluation on the BA-Shapes synthetic datasets used in GNNEx-
plainer Ying et al. (2019) and provided by Pytorch-Geometric (Fey & Lenssen, 2019). The syn-
thetic dataset is less complicated compared to the real-world datasets. We train a 3-layer GCN for
the node-classiﬁcation with a training accuracy of 0.95. The AUC score (for importance edges) of
TAGE is 0.999 compared to 0.963 and 0.925 of PGExplainer and GNNExplainer, respectively. Note
that the baseline scores are from the PGExplainer paper and some re-implementations23 of PGEx-
plainer can also achieve an AUC score of 0.999. Our purpose to show our score on BA-Shapes
is to demonstrate that TAGE is on par with its baselines even when considering the typical single-
task setting. Figure 7 visualizes 20 examples of explanations. TAGE is able to provide accurate
explanations for all the 20 examples."
REFERENCES,0.9714285714285714,"G
DISCUSSION OF LIMITATIONS AND POTENTIAL SOLUTIONS"
REFERENCES,0.975,"Inductive learning of explanations. Our study focus on the setting of inductive learning of the
explanation, i.e., to train the explainer on a given dataset and perform inference on new coming
data. There are many work conducted under the inductive setting, such as PGExplainer. All methods
under this setting may have a potential limitation that the explainer may suffer from some dataset
bias when training data and the data to be explained are inconsistent. This is an interesting problem
that requires further investigation. However, we believe that this is a separate problem and applies"
REFERENCES,0.9785714285714285,"2https://github.com/LarsHoldijk/RE-ParameterizedExplainerForGraphNeuralNetworks
3https://openreview.net/forum?id=tt04glo-VrT"
REFERENCES,0.9821428571428571,Under review as a conference paper at ICLR 2022
REFERENCES,0.9857142857142858,Figure 7: Visualizations on explanations to the synthetic dataset BA-Shapes.
REFERENCES,0.9892857142857143,Under review as a conference paper at ICLR 2022
REFERENCES,0.9928571428571429,"to all inductive learning methods. In addition, the size of graph could be inconsistent for training
and inference. To tackle this issue, we obtain the substructure by selecting top k percentage of edges
according to their important scores."
REFERENCES,0.9964285714285714,"Black-box explanations. Similarly to our baseline method PGExplainer, our explainer relies on
node embeddings as inputs to the explainer. In particular, the node embeddings serve as represen-
tations to allow explainers identify each node. It is required by any (inductively) learning based
explanations to tell neural network-based explainers which edge they are looking at. A limitation of
the inductive methods is that when the node embeddings may become unavailable when explaining
a black-box model. The study of explaining black-box models (where only output is available) is a
different direction of study in scenarios like attacking. Many current SOTA explanation approaches,
such as Grad-Cam, GNN-LRP, and PGExplainer, fail under the black-box setting. However, if one
would like to adapt our approach to the black-box setting, it is still feasible by adopting a surro-
gate model for the black-box model and perform explanation on the surrogate model. In addition,
as mentioned above, the node embeddings are mainly used to identify which node the explainer
is looking at, we does not necessarily require the original embedding. When node embedding are
unavailable, we can still use any representation of nodes as long as it can identify the node based on
its feature and topology."
