Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0038314176245210726,"Knowledge distillation aims to obtain a small and effective deep model (student)
by learning the output from a larger model (teacher). Previous studies found a se-
vere degradation problem, that student performance would degrade unexpectedly
when distilled from oversized teachers. It is well known that larger models tend to
have sharper outputs (Guo et al., 2017). Based on this observation, we found that
the sharpness gap between the teacher and student output may cause this degra-
dation problem. To solve this problem, we ﬁrst propose a metric to quantify the
sharpness of the model output. Based on the second-order Taylor expansion of this
metric, we propose Adaptive Temperature Knowledge Distillation (ATKD), which
automatically changes the temperature of the teacher and the student, to reduce the
sharpness gap Gs gap. We conducted extensive experiments on CIFAR100 and
ImageNet and achieved signiﬁcant improvements. Speciﬁcally, ATKD trained the
best ResNet18 model on ImageNet as we knew (73.0% accuracy)."
INTRODUCTION,0.007662835249042145,"1
INTRODUCTION"
INTRODUCTION,0.011494252873563218,"Deep neural networks have achieved remarkable success in most of computer vision tasks (He et al.,
2016; Deng et al., 2009), while the increasing network capacity in the current state-of-the-art models
also results in severe computational burdens and high inference time (Tian et al., 2020; Yim et al.,
2017). One direction to reduce the model size is knowledge distillation (KD), which trains small
efﬁcient models (student) by learning from the output of large models (teacher) (Tang et al., 2020).
In 2015, Hinton proposed to soften the model output by raising the temperature of the last softmax
layer(Hinton et al., 2015). Since then, this temperature-based kowledge distillation has drawn the
main stream attentions and achieved many successes (Zagoruyko & Komodakis, 2017; Kim et al.,
2020; Furlanello et al., 2018; Fu et al., 2020)."
INTRODUCTION,0.01532567049808429,"However, recent researches found that knowledge distillation suffers from a mysterious performance
degradation problem(Cho & Hariharan, 2019; Mirzadeh et al., 2019). Speciﬁcally, since the idea of
KD is transferring the knowledge of teacher into student, a nature assumption is larger teachers
would train better students. But recent researches refuted this assumption and found that the student
often performs worse with oversized teachers(Cho & Hariharan, 2019)."
INTRODUCTION,0.019157088122605363,"This paper studies this degradation problem by considering the sharpness (or softness) of the model
output. It is well known that networks with more parameters tend to produce sharper output (Guo
et al., 2017; Lee et al., 2018), which would create the gap of sharpness between teacher and student,
i.e. the teacher is usually much sharper than the student. It should be noted that temperature technol-
ogy in vanilla knowledge distillation does not reduce this gap. This is because in vanilla knowledge
distillation, the teacher and student are softened with the same temperature, which maintains the
large gap of sharpness. When the teacher grows larger, this gap will further increases, making it
more difﬁcult for the student to learn from the teacher."
INTRODUCTION,0.022988505747126436,"To solve this problem, let us consider a modiﬁed softer teacher model with the same accuracy. In
this way, the sharpness gap between the student and the teacher models would be narrowed, making
learning more accessible. One way to build this softer teacher is to use higher temperatures for the
teacher model, but it would require labor-intensive searches on the validation data set."
INTRODUCTION,0.02681992337164751,"With this insight, we propose to adaptively change the temperatures of the teacher and the student
by their sharpness. Our contribution is three-fold:"
INTRODUCTION,0.03065134099616858,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.034482758620689655,"1. We propose a metric to quantify the sharpness of models, allowing us to precisely control
the sharpness of model output. Formally, we use the realsoftmax function (Nielsen & Sun,
2016) on the logits vector as the sharpness metric, which approximates a smoother version
of the max function. If we denote student logits as z, the teacher logits as v, teacher
temperature and student temperature as τ T and τ S respectively (τ T is equal to τ S in the
vanilla knowledge distillation), the sharpness metric and the gap of sharpness are deﬁned
as follows:"
INTRODUCTION,0.038314176245210725,"Ssharpness = log
X"
INTRODUCTION,0.0421455938697318,"j
ezj/τ"
INTRODUCTION,0.04597701149425287,"Gs gap = log
X"
INTRODUCTION,0.04980842911877394,"j
evj/τT −log
X"
INTRODUCTION,0.05363984674329502,"j
ezj/τS
(1)"
INTRODUCTION,0.05747126436781609,"2. We propose Adaptive Temperature Knowledge Distillation (ATKD), which automatically
changes the temperatures of the teacher and the student, to reduce the sharpness gap Gs gap.
ATKD relieves the burden of searching for the appropriate temperature on the validation
set. Speciﬁcally, by Taylor second-order expansion, our method can be implemented easily
by normalizing the logits with the standard deviation.
3. Our Ssharpness metric can be used to explain why two existing methods work. Speciﬁcally,
the Early Stopped method and the Teacher Assistant method both reduce the sharpness gap
between the teacher and the student."
INTRODUCTION,0.06130268199233716,"We present comprehensive experiments on CIFAR-100 Krizhevsky et al. (2009) and ImageNet
datasets (Deng et al., 2009) to evaluate our method. The experiment results show that: 1) The
proposed ATKD method tremendously mitigates the performance degradation problem and trains
the best ResNet18 model on ImageNet as we know (73.01% accuracy). 2) The proposed ATKD
method is easy to optimize, exhibits much lower training/test loss with oversized teachers than KD."
METHODOLOGY,0.06513409961685823,"2
METHODOLOGY"
BACKGROUND,0.06896551724137931,"2.1
BACKGROUND"
BACKGROUND,0.07279693486590039,"Vanillay Knowledge Distillation During training, we minimize the negative log likelihood of the
ground truth class to update model parameters. After the model is properly trained, the probability
of the ground truth would be close to 1, while the rest wrong predictions are near zeros."
BACKGROUND,0.07662835249042145,"In 2015, Hinton noticed that these small wrong probabilities are useful to unveil “dark knowledge”
(Hinton et al., 2015). Take a picture of “cat” for example, the model are more likely to output higher
probability for class “dog” than class “airplane”. These wrong probabilities imply the relationship
between the two classes and unveil how a model tends to generalize. This observation inspired to
use the output of large models as soft targets to train efﬁcient small models."
BACKGROUND,0.08045977011494253,"However, modern deep networks tend to produce peaky probabilities (Guo et al., 2017; Lee et al.,
2018), that the numbers of those wrong classes (near zero values) would be negligible compared to
the ground truth (near one). Thus Hinton proposed to raise the temperature of the last softmax to
soften the output probabilities, which can be used as soft targets to train small networks."
BACKGROUND,0.0842911877394636,"We denote logits of the teacher as v, and logits of the student as z, temperature as τ, i and j denote
the ith and jth value of logits (i.e. the ith and jth category of K classes). the loss of knowledge
distillation is as follows:"
BACKGROUND,0.08812260536398467,"LKD = −
X"
BACKGROUND,0.09195402298850575,"i
pT
i log pS
i"
BACKGROUND,0.09578544061302682,"pS
i =
ezi/τ
P"
BACKGROUND,0.09961685823754789,"j ezj/τ , pT
i =
evi/τ
P"
BACKGROUND,0.10344827586206896,j evj/τ (2)
BACKGROUND,0.10727969348659004,"The ﬁnal loss for the student is then the weighted sum of the cross entropy loss Lcls and the knowl-
edge distillation loss LKD:"
BACKGROUND,0.1111111111111111,"L = λLKD + (1 −λ)Lcls
(3)"
BACKGROUND,0.11494252873563218,Under review as a conference paper at ICLR 2022
BACKGROUND,0.11877394636015326,Table 1: The Performance Degradation Problem.
BACKGROUND,0.12260536398467432,"Teacher
ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
BACKGROUND,0.12643678160919541,"Teacher Acc
69.57
70.9
71.9
72.8
73.8
Student Acc
67.4
68.2
68
67.5
67.1
KD loss
1.1
1.7
2.1
2.5
3.3"
BACKGROUND,0.13026819923371646,"The popular choice of the temperature τ is in {3, 4, 5} and the weight λ = 0.9 (Hinton et al., 2015;
Cho & Hariharan, 2019; Tian et al., 2020)."
BACKGROUND,0.13409961685823754,"Performance Degradation Problem While knowledge distillation achieved success in many ﬁelds,
a mysterious performance degradation problem was spotted in 2019. Since the idea of knowledge
distillation is transferring teacher knowledge into students, one natural hypothesis is that a larger
and more accurate teacher would capture more knowledge and thus train better students. Unfortu-
nately, previous studies invalidate this hypothesis by showing that the student performance would
degenerate unexpectedly with larger teachers."
BACKGROUND,0.13793103448275862,"Table 1 shows our experiment result. The student is ResNet14. With larger teachers, the student ac-
curacy degrades, and the KD loss increase. Cho & Hariharan (2019) hypothesizes that the mismatch
of capacity causes this problem. We will discuss this problem in the rest of the section."
ADAPTIVE TEMPERATURE KNOWLEDGE DISTILLATION,0.1417624521072797,"2.2
ADAPTIVE TEMPERATURE KNOWLEDGE DISTILLATION"
ADAPTIVE TEMPERATURE KNOWLEDGE DISTILLATION,0.14559386973180077,"We investigated the degradation problem and proposed to use adaptive temperatures during training
to mitigate this problem:"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.14942528735632185,"1. We found that the mismatch sharpness between the teacher and the student may cause the
degradation problem."
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.1532567049808429,2. We propose a metric to quantify the sharpness of a model.
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.15708812260536398,3. We propose the adaptive temperature knowledge distillation (ATKD) methods.
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.16091954022988506,"The sharpness of neural networks Two main theories explain why knowledge distillation is ef-
fective. The ﬁrst theory originated from Hinton’s ﬁrst paper, which argued that teachers are more
accurate in capturing category similarities. This information of category similarity helps students to
generalize better on unseen data. On the other hand, some contend that the soft output of the teacher
prevents the student network from overconﬁdence (ie. label smoothing technique)."
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.16475095785440613,"However, when the teacher gets larger, these two theories will contradict each other. This is because
that larger models are usually more accurate and peaky at the same time. On the one hand, a more
accurate teacher is beneﬁcial to the student model. On the other hand, a sharper teacher tends to
make the student overconﬁdent. Therefore, the larger teacher capacity is a double-edged sword
for distillation. This analysis shows that the sharpness of teachers should be investigated carefully,
while previous studies have overlooked this critical attribute of teachers."
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.1685823754789272,"To this end, we propose to use realsoftmax (Nielsen & Sun, 2016) to quantify the sharpness of the
model output, which is deﬁned as the logarithm of the sum of the exponentials of the logits:"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.1724137931034483,"Ssharpness = log
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.17624521072796934,"j
ezj
(4)"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.18007662835249041,"Such metric has at least two advantages, therefore reasonably measure the sharpness of the model
output: 1) This metric is a smooth approximation to the maximum function maxj(z) 2) It is also
differentiable."
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.1839080459770115,"Meanwhile, we quantify the sharpness gap between the teacher and the student as the difference of
the sharpness metric:"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.18773946360153257,"Gs gap = log
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.19157088122605365,"j
evj −log
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.19540229885057472,"j
ezj
(5)"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.19923371647509577,Under review as a conference paper at ICLR 2022
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.20306513409961685,Table 2: The Ssharpness of different models with temperature set to one.
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.20689655172413793,"Network
ResNet14
ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.210727969348659,"Ssharpness
12.20
13.11
13.84
14.45
15.37
16.13"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.21455938697318008,"If we consider the temperatures, the sharpness gap is:"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.21839080459770116,"Gs gap = log
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2222222222222222,"j
evj/τ −log
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2260536398467433,"j
ezj/τ
(6)"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.22988505747126436,"Table 2 shows sharpness metric value for models of different capacity. It shows that Ssharpness
increases with network capacity, and there is a sharpness gap between networks of different sizes."
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.23371647509578544,"Adaptive Temperature One idea is using higher temperatures for the teacher to reduce the sharp-
ness gap between teachers and students. However, searching on the validation data set requires a
lot of computational costs. Instead, we propose adaptive temperature knowledge distillation, which
automatically tunes the temperature according to the Ssharpness."
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.23754789272030652,By Taylor second expansion:
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2413793103448276,"Gs gap = log
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.24521072796934865,"j
evj/τ −log
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.24904214559386972,"j
ezj/τ"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.25287356321839083,"≈log(K +
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2567049808429119,"j
vj/τ + 1 2 X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.26053639846743293,"j
v2
j /τ 2)"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.26436781609195403,"−log(K +
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2681992337164751,"j
zj/τ + 1 2 X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2720306513409962,"j
z2
j /τ 2) (7)"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.27586206896551724,"We follow the assumption from Hinton (Hinton et al., 2015), that the logits have been zero-meaned
separately for each training example so that P
j zj = P
j zj = 0. We found from experiments that
the sum of logits is indeed very small numbers close to zero. The experiment results and further
theoretical analysis are provided in the appendix."
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2796934865900383,Given the above assumption P
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2835249042145594,"j zj = P
j zj = 0, We can get:"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.28735632183908044,Gs gap = log(K + 1 2 X
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.29118773946360155,"j
(vj/τ)2) −log(K + 1 2 X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2950191570881226,"j
(zj/τ)2)"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.2988505747126437,"= log(1 +
1
2τ 2K X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.30268199233716475,"j
v2
j −log(1 +
1
2τ 2K X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3065134099616858,"j
z2
j ))
(8) The 1 K
P"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3103448275862069,"j v2
j in the equation is the variance of logits, which can be represented by the square of the
standard deviation std2:"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.31417624521072796,Gs gap = log(1 + 1
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.31800766283524906,2 ∗(stdT /τ)2) −log(1 + 1
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3218390804597701,"2 ∗(stdS/τ)2)
(9)"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.32567049808429116,"This result shows the relationship between the temperature and sharpness gap is pretty straightfor-
ward, that the standard deviation controls the sharpness gap between the teacher and the student.
Therefore, we set the temperature of the teacher and the student as the standard deviation of the
logits vector respectively. These adaptive temperatures would automatically change during training,
reducing the sharpness gap between the teacher and the student. It is worth noting that there is no
need to search for the proper temperature on the validation set like the vanilla KD."
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.32950191570881227,Under review as a conference paper at ICLR 2022
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3333333333333333,"Formally, if we denote standard deviation function as Std, we propose Adaptive Temperature
Knowledge Distillation as follows:
τ T
i = Std(vi), τ S
i = Std(zi)"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3371647509578544,"LAT KD = −
X"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.34099616858237547,"i
pT
i log pS
i"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3448275862068966,"pS
i =
ezi/τ S
i
P"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3486590038314176,"j ezj/τ S
i , pT
i =
evi/τ T
i
P"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3524904214559387,"j evj/τ T
i"
WE FOUND THAT THE MISMATCH SHARPNESS BETWEEN THE TEACHER AND THE STUDENT MAY CAUSE THE,0.3563218390804598,L = λLAT KD + (1 −λ)Lcls (10)
SHARPNESS GAP ANALYSIS,0.36015325670498083,"2.3
SHARPNESS GAP ANALYSIS"
SHARPNESS GAP ANALYSIS,0.36398467432950193,"We investigated the relationship between the sharpness gap and the temperature, and found that
the Gs gap of ATKD decrease with
1
τ 3 , while the vanilla KD decrease with
1
τ 2 . Therefore, If the
temperature of KD and adaptive temperatures of ATKD are in the same temperature range, the
sharpness gap of ATKD Gs gap would be smaller than KD in a considerable margin."
SHARPNESS GAP ANALYSIS,0.367816091954023,"If we use K to denote K classes, the sharpness gap would decrease with
1
τ 2 under vanilla KD:"
SHARPNESS GAP ANALYSIS,0.3716475095785441,"Gs gap = log
X"
SHARPNESS GAP ANALYSIS,0.37547892720306514,"j
evj/τ −log
X"
SHARPNESS GAP ANALYSIS,0.3793103448275862,"j
ezj/τ"
SHARPNESS GAP ANALYSIS,0.3831417624521073,= log(1 + O( 1
SHARPNESS GAP ANALYSIS,0.38697318007662834,τ 2 )) −log(1 + O( 1
SHARPNESS GAP ANALYSIS,0.39080459770114945,τ 2 )) + logK −logK ≈O( 1 τ 2 ) (11)
SHARPNESS GAP ANALYSIS,0.3946360153256705,"This shows that when the temperature is very high and the linear function (1+x) can approximate ex,
Gs gap would be reduced. However, with this linear approximation, the KD loss would degenerate
into MSE loss, as Hinton points out (Hinton et al., 2015):"
SHARPNESS GAP ANALYSIS,0.39846743295019155,"∂L
∂zi
= 1 T"
SHARPNESS GAP ANALYSIS,0.40229885057471265,"ezi/τ
P"
SHARPNESS GAP ANALYSIS,0.4061302681992337,"j ezj/τ −
evi/τ
P"
SHARPNESS GAP ANALYSIS,0.4099616858237548,j evj/τ ! (12) ≈1 T
SHARPNESS GAP ANALYSIS,0.41379310344827586,zi/τ + 1
SHARPNESS GAP ANALYSIS,0.41762452107279696,"K
−vi/τ + 1 K"
SHARPNESS GAP ANALYSIS,0.421455938697318,"
(13)"
SHARPNESS GAP ANALYSIS,0.42528735632183906,"≈
1
Kτ 2 (zi −vi)
(14)"
SHARPNESS GAP ANALYSIS,0.42911877394636017,"However, high temperatures could be harmful to student, because it would encourage the student to
learn more from those negative logits of the teacher (Hinton et al., 2015), and as Hinton pointed out,
these negative values of logits could be very noisy."
SHARPNESS GAP ANALYSIS,0.4329501915708812,"On the other hand, the sharpness gap of ATKD would decrease faster with
1
τ 3 , and enable the
ATKD to achieve much lower sharpness gap than KD. Because we can use second-order Taylor
approximation (1+x+ 1"
SHARPNESS GAP ANALYSIS,0.4367816091954023,"2x2), which is a better approximation than linear function (1+x) in a wide
range."
SHARPNESS GAP ANALYSIS,0.44061302681992337,"Gs gap = log
X"
SHARPNESS GAP ANALYSIS,0.4444444444444444,"j
evj/τ T −log
X"
SHARPNESS GAP ANALYSIS,0.4482758620689655,"j
ezj/τ S"
SHARPNESS GAP ANALYSIS,0.4521072796934866,= log(1 + 1
SHARPNESS GAP ANALYSIS,0.4559386973180077,2 ∗(stdS/τ S)2 + O(( 1
SHARPNESS GAP ANALYSIS,0.45977011494252873,τ S )3))
SHARPNESS GAP ANALYSIS,0.46360153256704983,−log(1 + 1
SHARPNESS GAP ANALYSIS,0.4674329501915709,2 ∗(stdT /τ T )2 + O(( 1
SHARPNESS GAP ANALYSIS,0.47126436781609193,τ T )3))
SHARPNESS GAP ANALYSIS,0.47509578544061304,≈O(( 1
SHARPNESS GAP ANALYSIS,0.4789272030651341,τ T )3) −O(( 1
SHARPNESS GAP ANALYSIS,0.4827586206896552,τ S )3) (15)
EXPERIMENTS,0.48659003831417624,"3
EXPERIMENTS"
EXPERIMENTS,0.4904214559386973,"In this section, we will show comprehensive experiment results to validate the effectiveness of ATKD
from several perspectives. Speciﬁcally, we ﬁrst conducts experiments on two popular CV datasets"
EXPERIMENTS,0.4942528735632184,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.49808429118773945,Table 3: CIFAR-100 experiments.
EXPERIMENTS,0.5019157088122606,"Teacher
Student
WRN-40-2
WRN-16-2
WRN-40-2
WRN-40-1
ResNet56
ResNet20
ResNet110
ResNet20
ResNet110
ResNet32
ResNet32*4
ResNet8*4
VGG13
VGG8"
EXPERIMENTS,0.5057471264367817,"Teacher
75.61
75.61
72.34
74.31
74.31
79.42
74.64
Student
73.26
71.98
69.06
69.06
71.14
72.50
70.36
KD
74.92
73.54
70.66
70.67
73.08
73.33
72.98
FitNet
73.58
72.24
69.21
68.99
71.06
73.50
71.02
AT
74.08
72.77
70.55
70.22
72.31
73.44
71.43
SP
73.83
72.43
69.67
70.04
72.69
72.94
72.68
CC
73.56
72.21
69.63
69.48
71.48
72.97
70.71
VID
74.11
73.30
70.38
70.16
72.61
73.09
71.23
RKD
73.35
72.22
69.61
69.25
71.82
71.90
71.48
PKT
74.54
73.45
70.34
70.25
72.61
73.64
72.88
AB
72.50
72.38
69.47
69.53
70.98
73.17
70.94
FT
73.25
71.59
69.84
70.22
72.37
72.86
70.58
FSP
72.91
-
69.95
70.11
71.89
72.62
70.23
NST
73.68
72.24
69.60
69.53
71.96
73.30
71.53
CRD
75.48
74.14
71.16
71.46
73.48
75.51
73.94
ATKD
75.75
75.06
72.08
72.12
74.09
76.40
74.17"
EXPERIMENTS,0.5095785440613027,Table 4: ImageNet experiments with Top1 accuracy.
EXPERIMENTS,0.5134099616858238,"CE
KD
ES
SP
CC
CRD
AT
ATKD"
EXPERIMENTS,0.5172413793103449,"69.8
69.20
71.40
70.62
69.96
71.38
70.70
72.80"
EXPERIMENTS,0.5210727969348659,"to demonstrate the performance of ATKD. Then we focused on evaluating whether it could alleviate
the performance degradation problem."
EXPERIMENTS,0.524904214559387,"Dataset 1) CIFAR-100 (Krizhevsky et al., 2009) is a relatively small data set and is widely used for
testing various of deep learning methods. CIFAR-100 contains 50,000 images in the training set and
10,000 images in the dev set, divided into 100 ﬁne-grained categories. 2) ImageNet (Deng et al.,
2009) is a much larger one than CIFAR-100. ImageNet contains 1.2M images for training and 50K
for validation, that distributes in 1000 classes."
EXPERIMENTS,0.5287356321839081,"CIFAR Experimental settings We run a total of 240 epochs for all methods. The learning rate is
initialized as 0.05, then decay by 0.1 every 30 epochs after 150 epochs. Temperature is 4 for vanilla
KD, and the weight of ATKD or KD and cross-entropy is 0.9 and 0.1 for all the settings."
EXPERIMENTS,0.5325670498084292,"ImageNet Experimental settings Here we use ResNet18 as student for all of methods. Training
settings like learning rate or training epochs are the same with Heo et al. (2019) for ImageNet. The
teacher network is well-trained previously and ﬁxed during training."
EXPERIMENTS,0.5363984674329502,"3.1
CIFAR-100 AND IMAGENET"
EXPERIMENTS,0.5402298850574713,"Baselines. We selected many SOTA KD methods to evaluate the performances of ATKD. Knowl-
edge deﬁned from intermediate layers: FitNet (Romero et al., 2015), AT (Zagoruyko & Komodakis,
2017), SP (Tung & Mori, 2019), PKT (Passalis & Tefas, 2018), FT (Kim et al., 2020), FSP (Yim
et al., 2017) . 1) Knowledge deﬁned via mutual information: CC (Peng et al., 2019), VID (Ahn et al.,
2019), CRD (Tian et al., 2020). 2) Structured Knowledge: RKD (Park et al., 2019). 3) Knowledge
from logits: KD (Hinton et al., 2015), NST (Huang & Wang, 2017), ES (Cho & Hariharan, 2019),
TA (Mirzadeh et al., 2019)"
EXPERIMENTS,0.5440613026819924,"Results in CIFAR-100. Table 3 shows that ATKD always has an outstanding improvement com-
pared with all other methods. In some situations (e.g. those where teacher/student is WRN-40-
2/WRN-40-1 or ResNet110/ResNet32), the performances of ATKD are even very close to those of
teacher."
EXPERIMENTS,0.5478927203065134,"Results in ImageNet. All experiments used ResNet34 as the teacher and ResNet18 as the student.
Table 4 (the Top1 accuracy) shows that ATKD exceeds all of the previous SOTA by a large mar-
gin. Before ATKD, the improvement of this task is limited. Fig. 1 shows the training process of"
EXPERIMENTS,0.5517241379310345,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.5555555555555556,"Table 5: Performance Degradation Problem on CIFAR-100. Student is ResNet14. ATKD archives
lower training loss and higher accuracy. The Gs gap between the distilled student and teacher is also
reduced signiﬁcantly. Temperature is set to 4 in vanilla KD."
EXPERIMENTS,0.5593869731800766,"ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
EXPERIMENTS,0.5632183908045977,"Training loss
Vanilla KD
1.1
1.7
2.1
2.5
3.3
ATKD
0.9
1.2
1.3
1.4
1.6"
EXPERIMENTS,0.5670498084291188,"Test acc
Vanilla KD
67.4
68.2
68
67.5
67.1
ATKD
68.2
68.7
68.9
68.8
69.2"
EXPERIMENTS,0.5708812260536399,"Gs gap
Vanilla KD
0.09
0.16
0.21
0.30
0.38
ATKD
0.05
0.11
0.14
0.20
0.25"
EXPERIMENTS,0.5747126436781609,Table 6: Performance Degradation Problem on ImageNet.
EXPERIMENTS,0.578544061302682,"Teacher
Method
Accuracy
Teacher
Method
Accuracy"
EXPERIMENTS,0.5823754789272031,"ResNet34
KD
69.43
ResNet101
-
-
ES
70.98
-
-
ATKD
72.80
ATKD
72.85"
EXPERIMENTS,0.5862068965517241,"ResNet50
KD
69.05
ResNet152
-
-
TA
70.65
TA
70.59
ES
70.95
ES
70.74
ATKD
73.01
ATKD
72.70"
EXPERIMENTS,0.5900383141762452,"vanilla KD and ATKD. It is worth noting that ATKD provides comparable performance to KD’s
ﬁnal performance after ﬁrst 30th epoch training."
PERFORMANCE DEGRADATION EXPERIMENTS,0.5938697318007663,"3.2
PERFORMANCE DEGRADATION EXPERIMENTS"
PERFORMANCE DEGRADATION EXPERIMENTS,0.5977011494252874,"CFIFAR-100 On CIFAR-100 task, We trained the ResNet14 with multiple teachers on the CIFAR-
100 dataset, and the result is shown in Table 5. Experimental details follows the above settings.
Under ATKD, student performance continues to increase as the teacher gets bigger. The training
loss of ATKD is also much lower than the vanilla KD. In addition, the Gs gap (computed with
temperatures) metric shows that ATKD signiﬁcantly reduces the sharpness gap."
PERFORMANCE DEGRADATION EXPERIMENTS,0.6015325670498084,"In the analysis of section 2.3, we concluded that larger temperatures would have smaller sharpness
gap. Therefore the comparison between KD and ATKD need to take temperature values into account.
The temperature of KD is 4, while most of the mean adaptive temperatures used by ATKD are
between (3, 4). We provide more details in the appendix."
PERFORMANCE DEGRADATION EXPERIMENTS,0.6053639846743295,"ImageNet Table 6 shows the degradation problem in ImageNet with ResNet18 as the student, and ’-’
denote that this speciﬁc experiment was not conducted in the cited paper. We compared ATKD with
two previous methods that aim to alleviate the degradation problem, Early Stop (Cho & Hariharan,
2019) (ES) and Teacher Assistant (Mirzadeh et al., 2019) (TA). Both of these two methods explicitly
regularizing the teacher capacity: 1) TA proposed to distill the large teacher to an intermediate
teacher and then distill to the student, so that each knowledge distillation step has a better match
between student and teacher capacity. 2) ES methods use the early stopped teacher, the teacher
capacity would be regularized by fewer training steps."
PERFORMANCE DEGRADATION EXPERIMENTS,0.6091954022988506,"We can see that ATKD exceeds Early Stop and TA methods with a large margin in all teacher
settings. For example, when distilled by ResNet50 and ResNet152, the performance exceeds other
methods by 2%. We obtained 73.01% accuracy, which is the best ResNet18 results as we know."
PERFORMANCE DEGRADATION EXPERIMENTS,0.6130268199233716,"An interesting ﬁnding of the early stopped teacher method is that these regularized teachers have
smaller Ssharpness, which can reduce the sharpness gap between the teacher and the student. The
results is in Table 7. Similarly, the Teacher Assistant method also reduces the sharpness gap by
using medium-sized teachers. This ﬁnding indicates that Ssharpness metric could provide a universal
framework to mitigate the degradation problem."
PERFORMANCE DEGRADATION EXPERIMENTS,0.6168582375478927,Under review as a conference paper at ICLR 2022
PERFORMANCE DEGRADATION EXPERIMENTS,0.6206896551724138,"Table 7: The Ssharpness of Early Stopped models, temperature set to 1."
PERFORMANCE DEGRADATION EXPERIMENTS,0.6245210727969349,"Network
ResNet14
ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
PERFORMANCE DEGRADATION EXPERIMENTS,0.6283524904214559,"Ssharpness (240 epochs)
12.20
13.11
13.84
14.45
15.37
16.13
Ssharpness (60 epochs)
-
13.03
13.68
14.29
14.77
15.69"
PERFORMANCE DEGRADATION EXPERIMENTS,0.632183908045977,"1. CIFAR-100 Training Loss
2. ImageNet Test Accuracy"
PERFORMANCE DEGRADATION EXPERIMENTS,0.6360153256704981,"Figure 1: 1. The training/test loss on CIFAR-100. With the teacher size grows, the loss of ATKD
increases slower than KD, which shows that ATKD alleviates the Degradation Performance Problem.
2. The training process of KD and ATKD on ImageNet. ATKD achieves comparable accuracy with
KD at the 30th epoch."
RELATED WORK,0.6398467432950191,"4
RELATED WORK"
RELATED WORK,0.6436781609195402,"Knowledge Distillation Buciluˇa et al. (2006) ﬁrst proposed compressing a trained cumbersome
model into a smaller model by matching the logits between them. Then Hinton et al. (2015) ad-
vanced this idea and formed a more widely used framework known as knowledge distillation (KD).
Knowledge distillation tries to minimize the KL divergence between the soft output probabilities
generated by the logits through softmax. Xu et al. (2020) proposed to normalize the feature, the
penultimate layer of the network, to perform distillation. This method is similar to our methods
except that we perform on the logits layer. Knowledge distillation is also a kind of soft label training
method. Similar to label smoothing, previous studies have found that knowledge distillation helps
to regularize the training of network."
RELATED WORK,0.6475095785440613,"Performance Degradation Problem: Although distillation has shown a great potential in many
tasks, a mysterious problem is found that larger teachers often harm the distillation performance,
despite its more powerful ability (Cho & Hariharan, 2019; Mirzadeh et al., 2019). This problem is
particularly severe on ImageNet, resulting in poor performance for KD. It was widely accepted that
the mismatch of capacity caused this problem. Previous research proposed to regularize the teacher
capacity to alleviate this problem heuristically. Cho & Hariharan (2019) proposed to early stop the
training of the teacher. Moreover, Mirzadeh et al. (2019) proposed to use a medium-size teacher
assistant (TA) to perform a sort of sequence distillation. This TA ﬁrst learns from the teacher, then
the student can learn from this TA. However, the accuracy of the early stopped teacher or TA is also
harmed, which is lower than the original teacher."
CONCLUSION,0.6513409961685823,"5
CONCLUSION"
CONCLUSION,0.6551724137931034,"The vanilla knowledge distillation overlooks the sharpness gap between the teacher and the student,
which may cause the performance degradation problem. In this paper, we propose to use a metric
to measure the sharpness of neural networks, allowing us to control the sharpness of models by
using adaptive temperatures. Recently some papers have focused on combining label smoothing
and knowledge distillation to train neural networks (Shen et al., 2021; M¨uller et al., 2019). An
appealing point of view is that ATKD is inherently consistent with these methods. ATKD smooths
teachers by adapting temperatures, while these methods let students learn from teachers trained
by label smoothing. The deeper relationship between knowledge distillation and label smoothing
remains to be explored."
CONCLUSION,0.6590038314176245,Under review as a conference paper at ICLR 2022
REFERENCES,0.6628352490421456,REFERENCES
REFERENCES,0.6666666666666666,"Sungsoo Ahn, Shell Xu Hu, Andreas C. Damianou, Neil D. Lawrence, and Zhenwen Dai. Variational
information distillation for knowledge transfer. 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 9155–9163, 2019."
REFERENCES,0.6704980842911877,"Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceed-
ings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD ’06, pp. 535–541, New York, NY, USA, 2006."
REFERENCES,0.6743295019157088,"Jang Hyun Cho and Bharath Hariharan. On the efﬁcacy of knowledge distillation. 2019 IEEE/CVF
International Conference on Computer Vision (ICCV), pp. 4793–4801, 2019."
REFERENCES,0.6781609195402298,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale
hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255, 2009."
REFERENCES,0.6819923371647509,"Jie Fu, Xue Geng, Zhijian Duan, Bohan Zhuang, Xingdi Yuan, Adam Trischler, Jie Lin, Chris
Pal, and Hao Dong. Role-wise data augmentation for knowledge distillation. arXiv preprint
arXiv:2004.08861, 2020."
REFERENCES,0.685823754789272,"Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural networks, 2018."
REFERENCES,0.6896551724137931,"Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit number
recognition from street view imagery using deep convolutional neural networks. arXiv preprint
arXiv:1312.6082, 2013."
REFERENCES,0.6934865900383141,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks, 2017."
REFERENCES,0.6973180076628352,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.7011494252873564,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–
778, 2016."
REFERENCES,0.7049808429118773,"Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A
comprehensive overhaul of feature distillation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 1921–1930, 2019."
REFERENCES,0.7088122605363985,"Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
ArXiv, abs/1503.02531, 2015."
REFERENCES,0.7126436781609196,"Zehao Huang and Naiyan Wang.
Like what you like: Knowledge distill via neuron selectivity
transfer. ArXiv, abs/1707.01219, 2017."
REFERENCES,0.7164750957854407,"Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network: Network compres-
sion via factor transfer, 2020."
REFERENCES,0.7203065134099617,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.7241379310344828,"Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training conﬁdence-calibrated classiﬁers
for detecting out-of-distribution samples, 2018."
REFERENCES,0.7279693486590039,"Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan
Ghasemzadeh. Improved knowledge distillation via teacher assistant, 2019."
REFERENCES,0.7318007662835249,"Rafael M¨uller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help?
arXiv
preprint arXiv:1906.02629, 2019."
REFERENCES,0.735632183908046,Under review as a conference paper at ICLR 2022
REFERENCES,0.7394636015325671,"Frank Nielsen and Ke Sun. Guaranteed bounds on the kullback-leibler divergence of univariate
mixtures using piecewise log-sum-exp inequalities. arXiv preprint arXiv:1606.05850, 2016."
REFERENCES,0.7432950191570882,"Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3962–3971,
2019."
REFERENCES,0.7471264367816092,"Nikolaos Passalis and Anastasios Tefas. Learning deep representations with probabilistic knowledge
transfer. In ECCV, 2018."
REFERENCES,0.7509578544061303,"Baoyun Peng, Xiao Jin, Jiaheng Liu, Shunfeng Zhou, Yichao Wu, Yu Liu, Dong sheng Li, and
Zhaoning Zhang. Correlation congruence for knowledge distillation. 2019 IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), pp. 5006–5015, 2019."
REFERENCES,0.7547892720306514,"Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548,
2017."
REFERENCES,0.7586206896551724,"Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. CoRR, abs/1412.6550, 2015."
REFERENCES,0.7624521072796935,"Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang-Ting Cheng, and Marios Savvides.
Is label smoothing truly incompatible with knowledge distillation: An empirical study. arXiv
preprint arXiv:2104.00676, 2021."
REFERENCES,0.7662835249042146,"Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H. Chi, and Sagar Jain. Un-
derstanding and improving knowledge distillation, 2020."
REFERENCES,0.7701149425287356,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. ArXiv,
abs/1910.10699, 2020."
REFERENCES,0.7739463601532567,"Frederick Tung and Greg Mori.
Similarity-preserving knowledge distillation.
2019 IEEE/CVF
International Conference on Computer Vision (ICCV), pp. 1365–1374, 2019."
REFERENCES,0.7777777777777778,"Kunran Xu, Lai Rui, Yishi Li, and Lin Gu. Feature normalized knowledge distillation for image clas-
siﬁcation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August
23–28, 2020, Proceedings, Part XXV 16, pp. 664–680. Springer, 2020."
REFERENCES,0.7816091954022989,"Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast
optimization, network minimization and transfer learning. 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 7130–7138, 2017."
REFERENCES,0.7854406130268199,"Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor-
mance of convolutional neural networks via attention transfer. ArXiv, abs/1612.03928, 2017."
REFERENCES,0.789272030651341,"A
APPENDIX"
REFERENCES,0.7931034482758621,"A.1
EXPERIMENTS OF DIFFERENT FIXED TEMPERATURES"
REFERENCES,0.7969348659003831,"We conducted experiments of different ﬁxed temperatures for vanilla KD (Table 8), which shows no
observable performance improvement. We distilled ResNet14 by ResNet56 on CIFAR-100 dataset,
and the rest details followed the setting in section 3."
REFERENCES,0.8007662835249042,"A.2
EXPERIMENTS VARIES WITH WIDTH"
REFERENCES,0.8045977011494253,"We add degradation problem experiments where teacher models vary with width here (Table 9). We
use Wide ResNet to train on the CIFAR-100 dataset, the rest experiment details follow the setting in
section 3."
REFERENCES,0.8084291187739464,Under review as a conference paper at ICLR 2022
REFERENCES,0.8122605363984674,Table 8: Experiments of different ﬁxed temperatures
REFERENCES,0.8160919540229885,"τ T
4
4
4
4
4
ATKD
τ S
3.5
3.7
4
4.3
4.5
ATKD"
REFERENCES,0.8199233716475096,"Accuracy
68.27
68.11
68.06
68.22
68.30
69.0
Ss gap
0.23
0.25
0.30
0.35
0.37
0.19"
REFERENCES,0.8237547892720306,"τ T
3.5
3.7
4
4.3
4.5
5
τ S
4
4
4
4
4
4"
REFERENCES,0.8275862068965517,"Accuracy
67.79
68.01
68.06
68.10
68.07
68.01
Ss gap
0.40
0.33
0.30
0.27
0.24
0.21"
REFERENCES,0.8314176245210728,Table 9: Experiments varies with width
REFERENCES,0.8352490421455939,"WRN-16-2
WRN-16-3
WRN-16-4
WRN-16-5
WRN-16-6"
REFERENCES,0.8390804597701149,"Test acc
Vanilla KD
68.22
67.88
68.27
67.80
67.2
ATKD
69.39
69.33
69.39
69.40
69.21"
REFERENCES,0.842911877394636,"Gs gap
Vanilla KD
0.37
0.49
0.59
0.71
0.83
ATKD
0.10
0.22
0.34
0.41
0.55"
REFERENCES,0.8467432950191571,"A.3
ENTROPY SCORE OF MODELS"
REFERENCES,0.8505747126436781,"Entropy was used as conﬁdence score at Pereyra et al. (2017), that is related to the sharpness of
model output. We measured the entropy values of different models (Table 10). Experimental details
follow the setting in section 3."
REFERENCES,0.8544061302681992,Table 10: Logits Entropy
REFERENCES,0.8582375478927203,"ResNet14
ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
REFERENCES,0.8620689655172413,"0.97
0.74
0.45
0.35
0.22
0.09"
REFERENCES,0.8659003831417624,"A.4
SHARPNESS GAP DURING TRAINING"
REFERENCES,0.8697318007662835,"Figure 2 shows the sharpness gap changes during the training. ResNet20 and WRN-16-1 are used
as students, and ResNet56 and WRN-16-3 are used as teachers respectively."
REFERENCES,0.8735632183908046,"A.5
EXPERIMENT ON SVHN"
REFERENCES,0.8773946360153256,"We add another dataset SVHN here (Goodfellow et al., 2013). Results show in table 11. SVHN is
similar to MNIST (e.g., the images are of small cropped digits), but comes from real house numbers
in Google Street View images. We use ResNet14 as the student. The rest experiment settings use the
setting of CIFAR-100 in section 3. Further exploration on parameter tuning might get better results."
REFERENCES,0.8812260536398467,"A.6
THE SUM OF LOGITS"
REFERENCES,0.8850574712643678,"In the paper of Hinton et al. (2015), they presume that the P
j zj equals to zero during training
without further analysis (In page 3, Eq. 3). Here, we analyze this assumption theoretically and
experimentally."
REFERENCES,0.8888888888888888,It is well known that the gradient to logits zj is:
REFERENCES,0.89272030651341,"∂L
∂zj
= pj −qj
(16)"
REFERENCES,0.896551724137931,Under review as a conference paper at ICLR 2022
REFERENCES,0.9003831417624522,"1) ResNet ACC
2) ResNet Gs gap
3) WRN ACC
4) WRN Gs gap"
REFERENCES,0.9042145593869731,"Figure 2: 1) The ResNet20 accuracy plot during training 2) The ResNet20 sharpness gap plot during
training. 3) The WRN-16-1 accuracy plot during training 4) The WRN-16-1 sharpness gap plot
during training"
REFERENCES,0.9080459770114943,Table 11: Experiments on SVHN
REFERENCES,0.9118773946360154,"ResNet20
ResNet32
ResNet44
ResNet56
Teacher acc
96.40
96.68
96.73
96.89"
REFERENCES,0.9157088122605364,"Test acc
Vanilla KD
96.57
96.54
96.61
96.59
ATKD
96.70
96.83
96.73
96.77"
REFERENCES,0.9195402298850575,"Gs gap
Vanilla KD
0.05
0.05
0.07
0.07
ATKD
0.03
0.04
0.04
0.04"
REFERENCES,0.9233716475095786,"Where pj is the jth class probability of the student and the qj is the probability of the teacher. We
can get the gradient to P zj by adding these gradients: X j"
REFERENCES,0.9272030651340997,"∂L
∂zj
=
X"
REFERENCES,0.9310344827586207,"j
(pj −qj) = 1 −1 = 0
(17)"
REFERENCES,0.9348659003831418,"Therefore, the gradient to the sum of logits is zero."
REFERENCES,0.9386973180076629,"Besides, we know that the logits is the product of the weight W and the feature vector f in the
penultimate layer:"
REFERENCES,0.9425287356321839,"z = Wf =
X"
REFERENCES,0.946360153256705,"i
Wi ∗fi
(18)"
REFERENCES,0.9501915708812261,"Where Wi is the ith column vector of W, considering that the initial weights of W are sampled
from the zero-meaned normal distribution in the popular initialization settings (He et al., 2015), we
assume that the initialized P
j Wij (the sum of column vector) equals to zero. Therefore, the P zj
should be zero all the time given the above assumptions."
REFERENCES,0.9540229885057471,"We conducted experiments to verify this assumption. We train these models on the CIFAR-100
dataset. Table 12 shows that all these models logits are close to zero."
REFERENCES,0.9578544061302682,Table 12: The value of P zj
REFERENCES,0.9616858237547893,"ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
REFERENCES,0.9655172413793104,"Logits Sum
-5e-5
-4.7e-5
-5.7e-5
-7.9e-4
-6.1e-5"
REFERENCES,0.9693486590038314,"WRN-16-1
WRN-16-2
WRN-16-3
WRN-16-4
VGG13"
REFERENCES,0.9731800766283525,"Logits Sum
-4.8e-5
-6.1e-6
-5.5e-5
-1.2e-5
-3.1e-5"
REFERENCES,0.9770114942528736,Table 13: Mean Value of Adaptive Temperatures
REFERENCES,0.9808429118773946,"ResNet14
ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
REFERENCES,0.9846743295019157,"3.34
3.43
3.49
3.58
3.71
3.87"
REFERENCES,0.9885057471264368,Under review as a conference paper at ICLR 2022
REFERENCES,0.9923371647509579,"A.7
THE ADAPTIVE TEMPERATURES"
REFERENCES,0.9961685823754789,We provide the mean value of the temperatures in table 13.
