Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006134969325153374,"Recently, multi-agent reinforcement learning (MARL) adopts the centralized train-
ing with decentralized execution (CTDE) framework that trains agents using the
data from all agents at a centralized server while each agent takes an action from
its observation. In the real world, however, the training data from some agents
can be unavailable at the centralized server due to practical reasons including
communication failures and security attacks (e.g., data modiﬁcation), which can
slow down training and harm performance. Therefore, we consider the missing
training data problem in MARL, and then propose the imputation assisted multi-
agent reinforcement learning (IA-MARL). IA-MARL consists of two steps: 1) the
imputation of missing training data, which uses generative adversarial imputation
networks (GAIN), and 2) the mask-based update of the networks, which trains each
agent using the training data of corresponding agent, not missed over consecutive
times. In the experimental results, we explore the effects of the data missing proba-
bility, the number of agents, and the number of pre-training episodes for GAIN on
the performance of IA-MARL. We show IA-MARL outperforms a decentralized
approach and even can achieve the performance of MARL without missing training
data when sufﬁcient imputation accuracy is supported. Our ablation study also
shows that both the mask-based update and the imputation accuracy play important
roles in achieving the high performance in IA-MARL."
INTRODUCTION,0.012269938650306749,"1
INTRODUCTION"
INTRODUCTION,0.018404907975460124,"Reinforcement learning (RL) solves many challenging problems including the game playing (Mnih
et al., 2015) and the robot control (Levine et al., 2016), which focus on the single-agent RL envi-
ronment, modeled as the Markov decision process (Sutton and Barto, 2011). However, there exist
many real-world problems that involve interaction among multiple agents such as multi-robot control
(Hüttenrauch et al., 2019) and multiplayer games (Silver et al., 2017; Bard et al., 2020). Hence, the
multi-agent reinforcement learning (MARL) that operates in multi-agent domain has been introduced
and now becomes one of the most active and challenging RL research areas."
INTRODUCTION,0.024539877300613498,"In MARL, the decentralized approach has been used to train each agent based on its trajectory (Tan,
1993). However, it often shows unstable and low performance due to non-stationary environment
and partially observable information (Tan, 1993; Foerster et al., 2017) that inherits from the decen-
tralization. Speciﬁcally, as the agents evolve their policies independently, the environment becomes
non-stationary, which unstabilizes training at each agent. In addition, the agent may not observe
the information of other agents, which causes low performance in the cooperative or competitive
environment (Lowe et al., 2017)."
INTRODUCTION,0.03067484662576687,"Recently, the centralized training with decentralized execution (CTDE) framework has been intro-
duced for MARL (Oliehoek et al., 2008; Foerster et al., 2018). This can alleviate the non-stationary
environment and the partially observable information problems (Lowe et al., 2017) and encourages
coordination among agents (Foerster et al., 2018). In the execution of CTDE, each agent takes an
action based on its observation, while the training of the agents is performed at a centralized server
after collecting observations, actions, and rewards of all agents. In existing works, those data from all
agents are assumed to be available at the centralized server, which may not be always true in reality."
INTRODUCTION,0.03680981595092025,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04294478527607362,"The data from distributed agents can be unavailable due to practical reasons including the communi-
cation failure, hardware limit, and security attacks (Lakshminarayan et al., 1999; Twala, 2009). For
instance, in wireless sensor applications of MARL such as vehicle tracking (Liang et al., 2020) and
environmental monitoring (Li et al., 2020), sensors (i.e., agents) transmit their sensed information to
a receiver (i.e., centralized server) for training. The training data, transmitted from sensors, can be
missed when the communication is unstable. In addition, even when the training data successfully
arrives at the centralized server, certain data can be removed from the training dataset due to security
attacks such as false data injection (Yan et al., 2016) and unauthorized data modiﬁcation (Ferretti
et al., 2014). As one can readily imagine, this missing training data can cause a serious problem in
MARL as the training cannot be performed."
INTRODUCTION,0.049079754601226995,"One possible solution on this missing data problem is to use only the training data that contains data
from all agents without missing. However, in this case, the number of training data can dramatically
decrease as the number of agents increases or the data missing happens more often, which delays
the training. Another solution can be the use of imputation for replacing the missing training data
in the MARL. However, the training data of this case can be different from the original data, which
potentially degrades the performance. Therefore, as discussed above, the missing data problem should
be carefully considered for bringing the MARL to the next level for wider range of applications.
Despite of it, to the best of our knowledge, the missing data problem in MARL has not been taken
into account in existing works."
INTRODUCTION,0.05521472392638037,"In this paper, we propose an imputation assisted multi-agent reinforcement learning (IA-MARL)
with considering the missing training data problem, where the training data of each agent, consists
of observation, action, and reward, can be randomly missed with certain probability. The proposed
IA-MARL consists of two steps: 1) the imputation of missing training data and 2) the mask-based
update of the networks. Speciﬁcally, for the imputation of the missing training data, we use a
generative adversarial imputation network (GAIN) to impute the data from all agents, and we form
the data for the training of agents. We then perform the mask-based update, which trains the value
function and the policy of each agent by selectively using the training data of the corresponding
agent, not missed over the consecutive times. In the experimental results, we show the IA-MARL
outperforms a decentralized approach and also can achieve the performance of MARL with all
training data without missing. We then also show the performance of IA-MARL for different missing
probabilities, the number of agents, and the number of pre-training episodes for GAIN. From the
ablation study, we also verify the importance of the mask-based update as well as the imputation
accuracy in multi-agent environments when the training data can be missed."
RELATED WORK,0.06134969325153374,"2
RELATED WORK"
RELATED WORK,0.06748466257668712,"Independent Q-learning has been proposed as a decentralized approach to train each agent using its
own data independently (Tan, 1993). The independent Q-learning is used for the tabular environment
(Littman, 1994), and deep learning-based approaches are presented in Tampuu et al. (2017); Gupta
et al. (2017). Independent Q-learning, however, suffers from the non-stationary environment and
partially observable information problems."
RELATED WORK,0.0736196319018405,"CTDE is one of the solutions for those problems. In CTDE, the data from all agents are used for the
centralized training, while the execution at each agent only requires its observation (Oliehoek et al.,
2008). For instance, the centralized server trains the value function of each agent using observations,
actions, and rewards of all agents, while each agent takes an action based on its observation (Lowe
et al., 2017) ."
RELATED WORK,0.07975460122699386,"Recently, MARL algorithms that adopt CTDE framework have been presented. For instance, Lowe
et al. (2017) proposes the multi-agent deep deterministic policy gradient (MADDPG) that extends
deep deterministic policy gradient (DDPG) for the continuous control of multiple agents. For the
credit assignment problem, the counterfactual baseline (Foerster et al., 2018) and the value function
factorization that determines the contribution of each agent are used (Sunehag et al., 2018; Rashid
et al., 2018; Son et al., 2019). To improve the performance of MARL, the soft value function and
the multi-head attention are used in Iqbal and Sha (2019), and the communication between agents
that provides additional information to each agent is introduced in Foerster et al. (2016); Mordatch
and Abbeel (2018) while the limited communication channel between agents during the execution"
RELATED WORK,0.08588957055214724,Under review as a conference paper at ICLR 2022
RELATED WORK,0.09202453987730061,"is considered in Kim et al. (2019) to address the real world communication channel constraints in
MARL. However, none of the prior work considers the missing training data problem."
RELATED WORK,0.09815950920245399,"Imputation is the research area that replaces missing data, which has been used for many applications
including medical data and image concealment (Rubin, 2004). For the imputation, some techniques
such as multivariate imputation by chained equations (MICE) (Buuren and Groothuis-Oudshoorn,
2010), matrix completion (Mazumder et al., 2010), and MissForest (Stekhoven and Bühlmann, 2012)
are proposed. However, when the imputation is used for the data that have large data space (e.g.,
the data obtained from agents in CTDE), the techniques with insufﬁcient expressive power might
result in low performance. For this case, some imputation techniques that have more expressiveness
by adopting the deep neural networks can be more suitable such as the multiple imputation using
denoising autoencoders (MIDA) (Gondara and Wang, 2018), the bidirectional recurrent imputation
for time series (BRITS) (Cao et al., 2018), and the generative adversarial imputation network (GAIN)
(Yoon et al., 2018)."
BACKGROUND,0.10429447852760736,"3
BACKGROUND"
BACKGROUND,0.11042944785276074,"We consider a decentralized partially observable Markov decision process, deﬁned by a tuple
(S, A, P, r, Ω, O, γ, n), where S, A, and Ωare set of states, actions, and observations, respectively. r
and γ are the reward and the discount factor, respectively, and n is the number of agents. We use
s ∈S, a ∈A, and o ∈Ωfor a state, an action, and an observation, respectively. We use subscript i
for the corresponding agent and t for the time, e.g., oi,t is the observation of agent i at time t. We use
bold symbols to denote observations, actions, and rewards of all agents, e.g., at = (a1,t, · · · , an,t).
Here, P(st+1|st, at) and O(ot|st) are the transition probability and the conditional observation
probability, respectively."
DDPG AND MADDPG,0.1165644171779141,"3.1
DDPG AND MADDPG"
DDPG AND MADDPG,0.12269938650306748,"The objective of the agent in the environment is to maximize the cumulative reward Rt =
PT
t′=t γt′−trt′. For this, we use the actor-critic method. The expected cumulative reward for
given action and state is Q(st, at) = E[Rt|s = st, a = at], which is called as an action-value
function or value function. Using the Bellman equation, the value function can be rewritten as
Q(st, at) = Ert,st+1,at+1[rt + γQ(st+1, at+1)]. When the parameter θ is used for the value function
approximation, the value function Q can be learned by minimizing the loss L(θ), given as"
DDPG AND MADDPG,0.12883435582822086,"L(θ) = E

(Qθ(st, at|θ) −y)2
, y = rt + γQθ(st+1, at+1).
(1)"
DDPG AND MADDPG,0.13496932515337423,"DDPG is the widely-used choices for the policy update. The policy parameterized by φ takes state s
as an input and outputs deterministic action a = µφ(s) in DDPG, where the gradient of φ is given as
∇φJ(φ) = E[∇φµφ(at|st)∇atQθ(st, at)|at=µφ(st)].
(2)"
DDPG AND MADDPG,0.1411042944785276,"MADDPG is an algorithm that uses DDPG in the CTDE framework (Lowe et al., 2017). The value
function in MADDPG takes observations and actions of all agents as an input. Meanwhile, the policy
of each agent takes its observation as an input since the agent can access to its observation only. In
CTDE, the loss for the value function and the gradient for the policy are given as
L(θi) = E[(Qθi(ot, at) −y)2], y = ri,t + γQθi(ot+1, at+1),
(3)
∇φiJ(φi) = E[∇φiµφi(ai,t|oi,t)∇ai,tQθi(ot, at)|ai,t=µφi(oi,t)].
(4)"
DDPG AND MADDPG,0.147239263803681,"Here, each agent has a different value function and a policy, parameterized respectively as θi and φi."
IMPUTATION,0.15337423312883436,"3.2
IMPUTATION"
IMPUTATION,0.15950920245398773,"In statistics, imputation is used to replace missing data with substituted values. In order to denote
the missingness of the data, the mask M that has the same dimension as the data X is used: the
k-th component of the mask mk = 0, when the k-th data xk is missed, and mk = 1, when xk is
not missed. We denote the obtained data as M ⊙X, where ⊙is elementwise multiplication. In the
imputation, the completed data ˆX is given as
ˆX = M ⊙X + (1 −M) ⊙¯X,
(5)"
IMPUTATION,0.1656441717791411,Under review as a conference paper at ICLR 2022
IMPUTATION,0.17177914110429449,"where ¯X is the imputed data using the imputation algorithm including GAIN. For the loss of the
neural network that imputes data, the mean squared error is used, given as"
IMPUTATION,0.17791411042944785,"LM( ¯X, X) =
X"
IMPUTATION,0.18404907975460122,"k
mk(¯xk −xk)2.
(6)"
IMPUTATION,0.1901840490797546,"Here, ¯xk is k-th component of the imputed data. Note the loss can be estimated when mk = 1 since
xk can be used only when it is not missed."
METHODS,0.19631901840490798,"4
METHODS"
METHODS,0.20245398773006135,"In this section, after introducing the missing training data problem in MARL and describing the
imputation method, we propose IA-MARL."
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.2085889570552147,"4.1
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD"
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.2147239263803681,"We consider the MARL environment where the training data of each agent is randomly missed,
called the missing training data problem. Speciﬁcally, the training data from agent i at time t,
τi,t = (oi,t, ai,t, ri,t), is missed with the probability pmi,t = P(mi,t = 0) < 1, 1 as well. where
mi,t ∈{0, 1} denotes whether the data of agent i at time t is missed (mi,t = 0) or not (mi,t = 1)."
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.22085889570552147,"In the presence of missing data, to obtain completed data of all agents at time t for training, we ﬁrst
set the data for the imputation and the mask, respectively, as"
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.22699386503067484,"Xt = (τt−1, τt, τt+1), Mt = (mt−1, mt, mt+1),
(7)"
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.2331288343558282,"where τt = (τ1,t, · · · , τn,t) and mt = (m1,tJ1,|τ1,t|, · · · , mn,tJ1,|τn,t|). Here, | · | is a cardinality of
set · and J1,|τ| is an all-ones vector with length |τ|. Note that for the accurate imputation of τi,t, the
temporal data correlation as well as the data correlation across different agents should be used.2"
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.2392638036809816,"For the imputation, we use GAIN (Yoon et al., 2018) as it has sufﬁcient expressiveness to impute
multi-agent data, does not require original data for the training, and also has state-of-art performance.
The generator G in GAIN takes the obtained data, the mask matrix, and the random matrix as inputs
and outputs the imputed data. The imputed data and the completed data can be presented, respectively,
as
¯Xt = G(Mt ⊙Xt, Mt, (1 −Mt) ⊙Zt),
(8)
ˆXt = Mt ⊙Xt + (1 −Mt) ⊙¯Xt,
(9)"
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.24539877300613497,Algorithm 1 GAIN Training and Imputation
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.25153374233128833,Require: Dataset DGAIN contains Xt and Mt
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.25766871165644173,"1: Generate random matrix Zt and hint matrix Ht
2: Generate completed data ˆXt using equation 9
3: Discriminate completed data using equation 10
4: Estimate LD using equation 11 and update dis-
criminator
5: Generate random matrix Zt and hint matrix Ht
6: Generate completed data ˆXt using equation 9
7: Discriminate completed data using equation 10
8: Estimate LG using equation 12 and update genera-
tor
9: Return ˆXt"
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.26380368098159507,"where Zt is a random matrix that has the
same dimension as Xt.
In equation 9,
the completed data can be represented as
ˆXt = (ˆτt−1, ˆτt, ˆτt+1). The discriminator
D in GAIN takes ˆXt and the hint matrix
Ht as inputs and outputs the probability of
being masked for the completed data as"
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.26993865030674846,"ˆ
Mt = D( ˆXt, Ht),
(10)
Ht = Bt ⊙Mt + 0.5(1 −Bt)."
MISSING TRAINING DATA PROBLEM AND IMPUTATION METHOD,0.27607361963190186,"In equation 10, Bt = (b1,t−1, · · · , bn,t+1)
is a random sequence of 0 and 1, where
ph = P(bi,t = 1) is the hint probability."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.2822085889570552,"1We do not consider the case of missing all training data of all agents and time since the imputation cannot
proceed. We also note that the proposed IA-MARL can solve the problems of missing a certain part of τi,t (e.g.,
missing oi,t or ai,t)
2One may use Xt = (τi,t−t′, · · · , τi,t+t′), t′ > 1. As t′ increases, Xt contains more information which
may increase the imputation accuracy. At the same time, as t′ increases, the input and output dimensions of
GAIN increase which may decrease the imputation accuracy. Due to the ambiguous effect of t′ on the imputation
accuracy, we left it as a design choice and we use t′ = 1."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.2883435582822086,Under review as a conference paper at ICLR 2022
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.294478527607362,"The role of the hint matrix is to make the discriminator focus on the component with bi,t = 0.
The objective of the discriminator D is to maximize the probability of estimating the mask matrix
correctly, while the objective of the generator G is to minimize the both accuracy of the discriminator
and the loss of the imputation. Therefore, the losses of D and G are, respectively, given by"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3006134969325153,"LD = E(Xt,Mt)∈DGAIN 
−
X"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3067484662576687,"i,t:bi,t=0
(mi,t log( ˆmi,t) + (1 −mi,t) log(1 −ˆmi,t))

,
(11)"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3128834355828221,"LG = E(Xt,Mt)∈DGAIN 
−
X"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.31901840490797545,"i,t:bi,t=0
(1 −mi,t) log( ˆmi,t) + αGLM( ¯Xt, Xt)

,
(12)"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.32515337423312884,"where DGAIN is the dataset that contains obtained data and its mask matrix, and ˆmi,t ∈[0, 1] is
the probability of being masked, i.e., the i-th component of ˆ
Mt in equation 10. In equation 12,
LM( ¯Xt, Xt) is the difference between the imputed data and the obtained data as in equation 6. The
parameter αG controls the importance of the imputation loss, which is a hyperparameter. The training
and the imputation of GAIN are given in Algorithm 1."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3312883435582822,"4.2
IA-MARL: IMPUTATION ASSISTED MULTI-AGENT REINFORCEMENT LEARNING"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3374233128834356,"Figure 1: An example of the mask-based update
in IA-MARL when n = 2."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.34355828220858897,"In this subsection, we propose IA-MARL, which
uses the completed data obtained from GAIN and
the mask-based update that stabilizes the training in
MARL for the missing training data problem. For
IA-MARL, we use MADDPG, but other actor-critic
RL methods including policy gradient are also ap-
plicable for IA-MARL. Note that with some modiﬁ-
cation on the value function update, IA-MARL can
also be used for the value-based MARL algorithms
including VDN and QMIX."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3496932515337423,"In IA-MARL, the value function and the policy of
agent i can be updated using the following loss and
gradient."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3558282208588957,"L(θi) = Eˆτt∼ˆ
Di

(Qθi(ˆot, ˆat) −y)2
,"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3619631901840491,"y = ˆri,t + γQθ′
i(ˆot+1, ˆat+1),
(13)"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.36809815950920244,"∇φiJ(φi) = Eˆτt∼ˆ
Di[∇φiµφi(ˆai,t|ˆoi,t)
(14)"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.37423312883435583,"∇ˆai,tQθi(ˆot, ˆat)|ˆai,t=µφ′
i(ˆoi,t)],"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3803680981595092,"where θ′
i and φ′
i indicate the target network parameters periodically updated as in (Hasselt et al.,
2016). Even though we use imputation, the completed data may have different value with the original
data, which harms the performance. To mitigate this, we propose the mask-based update that trains
the value function and the policy using ˆDi, which is the set of the completed data for the training of
agent i, given by"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.38650306748466257,"ˆDi = {ˆτt | mi,tmi,t+1 = 1, t ∈{1, · · · , Tmax −1}} ∪{ˆτTmax | mi,Tmax = 1},
(15)"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.39263803680981596,"where Tmax is an episode length. Note that ˆDi in equation 15 contains data collected across episodes,
and ˆτTmax is included in ˆDi when mi,Tmax = 1 to update θi and φi at time Tmax. The mask-based
update means, in the training of agent i, the completed data at time t is used only when the data of
agent i is not missed over two consecutive times, t and t + 1. An example of ˆDi is presented in Fig. 1.
As shown in Fig. 1, only the completed data at time 2 and 5 are included in ˆD1, and the completed
data at time 1 is included in ˆD2."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.3987730061349693,"In IA-MARL, the imputation and the mask-based update are used to make the policy and the
value function (i.e., µφi(ˆai,t|ˆoi,t) and Qθi(ˆot, ˆat)) similar to the ones without data missing (i.e.,
µφi(ai,t|oi,t) and Qθi(ot, at)), respectively. In case of the policy, as mi,t = 1 (i.e., the data of agent i"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4049079754601227,Under review as a conference paper at ICLR 2022
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4110429447852761,"at time t exists) by the mask-based update, we can have µφi(ˆai,t|ˆoi,t) = µφi(ai,t|oi,t). In case of the
value function, since mi,tmi,t+1 = 1 by the mask-based update, the value function can be updated
when (oi,t, ai,t, ri,t, oi,t+1, ai,t+1) exists that mainly affect the update of Qθi(ˆot, ˆat). Hence, if the
imputation on the missing data of other agents is accurate, Qθi(ˆot, ˆat) can be similar to Qθi(ot, at).
We also show the importance of the mask-based update and accurate imputation in Section 5.3."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4171779141104294,Algorithm 2 IA-MARL
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4233128834355828,"1: Initialize DGAIN and GAIN G, D
2: Initialize φi, φ′
i, θi, θ′
i, ˆDi ∀i
3: for episode = 1 to N do
4:
if episode = Npre then
5:
Initialize DGAIN, ˆDi, φi, φ′
i, θi, θ′
i ∀i
6:
end if
7:
Initialize environment and receive o
8:
for t = 1 to Tmax do
9:
For each agent i, select action ai
10:
Store o, a, r, m in DGAIN
11:
for all agent i do
12:
Train φi and θi using equation 13 and
equation 14
13:
Update target network φ′
i and θ′
i
14:
end for
15:
end for
16:
Train GAIN using DGAIN and update ˆDi, then
initialize DGAIN
17: end for"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4294478527607362,"The training algorithm for IA-MARL is
provided in Algorithm 2. In IA-MARL,
GAIN is periodically trained and outputs
completed data using Algorithm 1, where
the dataset for GAIN, DGAIN, is initialized
to prevent the overﬁtting of GAIN to the tra-
jectory of outdated policies. We pre-train
GAIN through Npre episodes for better im-
putation accuracy.
After Npre episodes,
we initialize the parameters of all agents,
i.e., φi, φ′
i, θi, θ′
i, and ˆDi, ∀i, to prevent
the agent from training with inaccurate
data. The components in Algorithm 2 can
be modiﬁed according to the MARL al-
gorithm, where we use MADDPG in this
work. Therefore, the training procedure
follows that in Lowe et al. (2017), e.g., the
action is selected as ai = µφi(oi) + N,
where N is noise for exploration."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.43558282208588955,"In IA-MARL, the number of training data
is smaller than the number of obtained data
due to the mask-based update. When the missing probability of agent i over time is equal, i.e.,
pmi,t = pmi, ∀t, and the number of obtained data is ND, the number of completed data for the
training of agent i is given as"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.44171779141104295,"E

| ˆDi|

= ND(1 −pmi)2,
(16)"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.44785276073619634,"since the condition in equation 15 is satisﬁed when the data of agent i exists during the consecutive
times. Note that E

| ˆDi|

decreases with pmi."
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4539877300613497,"When
the
imputation
is
not
applied,
the
centralized
server
can
only
use
the
data
which
is
not
missed
from
all
agents
over
consecutive
times,
i.e.,
DP
=
{τt | mi,tmi,t+1 = 1, ∀i, t ∈{1, · · · , Tmax −1}} ∪{τTmax | mi,Tmax, ∀i}.
In this case, the im-
putation is not required for the training. Hence, for given ND, the number of the training data is
given as"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4601226993865031,"E

|DP|

= ND n
Y"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4662576687116564,"i=1
(1 −pmi)2.
(17)"
WE DO NOT CONSIDER THE CASE OF MISSING ALL TRAINING DATA OF ALL AGENTS AND TIME SINCE THE IMPUTATION CANNOT,0.4723926380368098,"Therefore, the number of training data without imputation, E

|DP|

, decreases exponentially with
the number of agents n while the number of training data for IA-MARL, E

| ˆDi|

, is not affected by
n. We show the performance of MADDPG without imputation in Appendix B."
EXPERIMENTAL RESULTS,0.4785276073619632,"5
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.48466257668711654,"In this section, we provide MARL environment and hyperparameters, and then show the performance
of IA-MARL with different missing probabilities, the number of agents, and the number of pre-
training episodes for GAIN. Furthermore, we provide the ablation study which shows the importance
of the mask-based update and accurate imputation in IA-MARL.3"
EXPERIMENTAL RESULTS,0.49079754601226994,3The code is in the supplementary materials and will be published after the review.
EXPERIMENTAL RESULTS,0.49693251533742333,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.5030674846625767,"(a)
(b)
(c)
(d)"
EXPERIMENTAL RESULTS,0.50920245398773,"Figure 2: Speaker-Listener environment with a pair of speaker-listener when different training data
missing probability pm and number of pre-training episodes Npre are considered. The training data
missing probability is (a) pm = 0.1, (b) pm = 0.2, (c) pm = 0.3, and (d) pm = 0.4. Figure best
viewed in color."
ENVIRONMENT AND HYPERPARAMETERS,0.5153374233128835,"5.1
ENVIRONMENT AND HYPERPARAMETERS"
ENVIRONMENT AND HYPERPARAMETERS,0.5214723926380368,"Among widely-used MARL benchmark environments including the multi-agent particle environments
(MPE) (Lowe et al., 2017; Mordatch and Abbeel, 2018) and the starcraft multi-agent challenge
(SMAC) (Samvelyan et al., 2019), we use and modify the MPE that contains mixed cooperative and
competitive environments. Note that SMAC is not used in experiments since MADDPG, adopted in
IA-MARL, generally does not perform well in SMAC (Papoudakis et al., 2021)."
ENVIRONMENT AND HYPERPARAMETERS,0.5276073619631901,"The speaker-listener environments are composed of several pairs of agents, where each pair has a
speaker and a listener, and landmarks have unique colors. In the environment, the speaker observes
the target color and sends a message to the listener. The listener then moves after observing the
locations of landmarks and the message. Each speaker-listener pair gets shared rewards, which is
determined by the distance between the listener and the target landmark. Note that the speaker should
learn which message to send, and the listener, who does not know the target landmark, should learn
the message. We set the number of landmarks as 2 + n/2 so that the number of landmarks increases
with the number of pairs (i.e., n/2)."
ENVIRONMENT AND HYPERPARAMETERS,0.5337423312883436,"The tag environments are composed of one prey and multiple predators, where the number of predators
is set as 3, 4, and 5 (which means the number of agents are n = 4, 5, and 6, respectively). In the
environment, the objective of the prey is to run away from the predators, and the objective of the
predator is to catch the prey. Each time when the prey collides with the predator, predators get a
shared reward while the prey is penalized."
ENVIRONMENT AND HYPERPARAMETERS,0.5398773006134969,"We evaluate the performance of the algorithm across 10 runs with different random seeds and the
shade in graphs represents the conﬁdent interval. We use MADDPG for IA-MARL. As baselines,
we use MADDPG and DDPG without missing training data (i.e., pm = 0). We use the same
hyperparameters in Lowe et al. (2017), except for the training frequency of the network parameters
in IA-MARL. Speciﬁcally, while MADDPG updates every 100 samples and collects 1024 samples
before making an initial update, IA-MARL updates every 100/(1 −pm)2 samples and collects
1024/(1 −pm)2 samples before making an initial update. Accordingly, the target network parameters
are less frequently updated in IA-MARL. In the speaker-listener environment, we evaluate the
performance of IA-MARL against MADDPG speaker and listener. In the tag environment, we
evaluate IA-MARL against MADDPG predators and DDPG prey."
PERFORMANCE OF IA-MARL,0.5460122699386503,"5.2
PERFORMANCE OF IA-MARL"
PERFORMANCE OF IA-MARL,0.5521472392638037,"Figure 2 shows the average rewards as a function of the episodes with different number of pre-
training episode and training data missing probability in the speaker-listener environment with one
speaker-listener pair. In Figs. 2a-2d, pm = 0.1, 0.2, 0.3, 0.4 is used, respectively. We observe that the
performance of IA-MARL outperforms the decentralized approach, i.e., DDPG, and also can achieve
the performance of MARL without missing training data, i.e., MADDPG with pm = 0, when GAIN
is pre-trained sufﬁciently. Speciﬁcally, IA-MARL achieves the performance of MADDPG with any
Npre for pm = 0.1 and Npre ≥4 × 103 for pm = 0.2 and 0.3. When pm = 0.4, IA-MARL could
not achieve the performance of MADDPG, but it is expected to achieve the similar performance
of MADDPG with larger Npre. Note that as pm increases, mainly due to the smaller number of"
PERFORMANCE OF IA-MARL,0.558282208588957,Under review as a conference paper at ICLR 2022
PERFORMANCE OF IA-MARL,0.5644171779141104,"(a)
(b)
(c)
(d)"
PERFORMANCE OF IA-MARL,0.5705521472392638,"Figure 3: (a) Speaker-Listener environment with two pairs of agents. (b)-(d) Tag environment, where
the upper graphs show the average rewards of the prey, and the lower graphs show the average rewards
of the predator. The number of predators is 3 in (b), 4 in (c), and 5 in (d)."
PERFORMANCE OF IA-MARL,0.5766871165644172,"training data as shown in equation 16, IA-MARL slowly achieves the performance of MADDPG.
Furthermore, as pm increases, more pre-training episodes are required since GAIN is less trained due
to the larger value of imputation loss. When Npre is small, large imputation error in completed data
causes IA-MARL to hardly achieve the performance of MADDPG."
PERFORMANCE OF IA-MARL,0.5828220858895705,"Figure 3 shows the average rewards as a function of the episodes with different pre-training episode
and missing probability. In Fig. 3a, the average rewards are shown for two pairs of speaker-listener.
In Figs. 3b-3d, we show the average rewards when the number of predators is 3, 4, and 5, respectively,
where the upper graphs show the rewards of the prey and the lower graphs show those of predators.
We observe IA-MARL can achieve the performance of MADDPG, and more pre-training and training
episodes are required as pm increases. In Fig. 3a, when pm = 0.1 and 0.2, IA-MARL achieves the
performance of MADDPG slowly. Furthermore, Figs. 3b-3d also show IA-MARL achieves the
performance of MADDPG when pm = 0.1, where the rewards of predators steadily increase, and the
rewards of the prey decrease. However, as pm increases, IA-MARL cannot achieve the performance
of MADDPG due to large imputation error. For instance, in Fig. 3a, when pm = 0.3, the performance
of IA-MARL is in between the performance of MADDPG and DDPG. Similarly, in Figs. 3c and 3d,
when pm = 0.2 and 0.3, IA-MARL cannot achieve the performance of MADDPG."
PERFORMANCE OF IA-MARL,0.588957055214724,"Figure 4: Speaker-Listener environment with a
pair of agents. The average rewards with/without
the mask-based update and with GAIN/random
imputation are shown. We use Npre = 104 for
GAIN."
PERFORMANCE OF IA-MARL,0.5950920245398773,"We also observe as the number of agents in-
creases, more Npre is required to make IA-
MARL achieve the performance of MADDPG.
In the speaker-listener environment with pm =
0.3, Npre = 4 × 103 is required for n = 2 (see
Figure 2a), while Npre > 104 is required for
n = 4 (see Figure 3a). Similarly, in the tag envi-
ronment with pm = 0.2, Npre = 104 is required
for n = 4 (see Figure 3b), while Npre > 104 is
required for n = 5 and n = 6 (see Figures 3c
and 3d). The main reasons to require more Npre
for the larger number of agents are 1) environ-
ment with more agents becomes more complex
and 2) the generator and the discriminator in
GAIN require more training data as the input
and output spaces increase with the number of
agents."
PERFORMANCE OF IA-MARL,0.6012269938650306,Under review as a conference paper at ICLR 2022
ABLATION STUDY AND LIMITATION,0.6073619631901841,"5.3
ABLATION STUDY AND LIMITATION"
ABLATION STUDY AND LIMITATION,0.6134969325153374,"We compare the performance of IA-MARL with and without the mask-based update and with and
without GAIN imputation. For simplicity, we call IA-MARL without the mask-based update as
simple-IA-MARL. In simple-IA-MARL, agent i is trained using the completed data ˆD = {ˆτt | t ∈
{1, · · · , Tmax −1}}, which is different from IA-MARL that trains agent i using data ˆDi. When
GAIN is not used, we use random imputation replacing missing training data with uniform random
variables, i.e., ˆoi,t ∼U(minoi,t∈Ωi(oi,t), maxoi,t∈Ωi(oi,t)), where Ωi is the set of all oi,t. Note the
random imputation can be regarded as a simple imputation method that has low imputation accuracy.
We show the performance of IA-MARL and Simple-IA-MARL with different imputation method and
missing probability in Appendix B."
ABLATION STUDY AND LIMITATION,0.6196319018404908,"Figure 4 shows the ablation study of IA-MARL when pm = 0.1 and Npre = 104. Firstly, we observe
the average rewards of simple-IA-MARL are even lower than that of DDPG for all imputation
methods. The low performance of simple-IA-MARL is due to the effect of imputation error on the
gradient estimation, which is alleviated in IA-MARL by the mask-based update. We also observe
that the average reward of IA-MARL with random imputation is similar or lower than that of DDPG.
Since the accuracy of random imputation is low, both the MARL with/without the mask-based update
cannot achieve the performance of MARL without missing data. Similarly, when the number of
pre-training episodes for GAIN, Npre, is small, the imputation accuracy is low, so the performance
of IA-MARL is degraded as shown in Figs. 2-3. Therefore, when the imputation is not sufﬁciently
accurate, the performance of IA-MARL can be low."
CONCLUSION,0.6257668711656442,"6
CONCLUSION"
CONCLUSION,0.6319018404907976,"We propose IA-MARL for the training of agents in the presence of missing training data. The key idea
is to use the imputation for replacing the missing data and the mask-based update that selectively uses
the training data for each agent. In IA-MARL, we use GAIN for the imputation and MADDPG for
MARL algorithm. In the experimental results, we verify the performance of IA-MARL for different
training data missing probabilities, the number of agents, and the number of pre-training episodes
for GAIN. We show that IA-MARL with missing training data achieves comparable performance
with MADDPG without missing training data, when GAIN is pre-trained sufﬁciently. Through the
ablation study, we also show the importance of the mask-based update and the imputation accuracy
in IA-MARL for achieving high performance."
REFERENCES,0.6380368098159509,REFERENCES
REFERENCES,0.6441717791411042,"Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio
Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A
new frontier for ai research. Artiﬁcial Intelligence, 280:103216, 2020."
REFERENCES,0.6503067484662577,"S van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations
in R. Journal of statistical software, pages 1–68, 2010."
REFERENCES,0.656441717791411,"Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. BIRTS: Bidirectional recurrent
imputation for time series. In Advances in Neural Information Processing Systems, pages 6775–
6785, 2018."
REFERENCES,0.6625766871165644,"Luca Ferretti, Fabio Pierazzi, Michele Colajanni, Mirco Marchetti, and Marcello Missiroli. Efﬁcient
detection of unauthorized data modiﬁcation in cloud databases. In IEEE Symposium on Computers
and Communications (ISCC), pages 1–6, 2014."
REFERENCES,0.6687116564417178,"Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, pages 2137–2145, 2016."
REFERENCES,0.6748466257668712,"Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet
Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pages 1146–1155, 2017."
REFERENCES,0.6809815950920245,Under review as a conference paper at ICLR 2022
REFERENCES,0.6871165644171779,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In AAAI Conference on Artiﬁcial Intelligence, pages
2974–2982, 2018."
REFERENCES,0.6932515337423313,"Lovedeep Gondara and Ke Wang. MIDA: Multiple imputation using denoising autoencoders. In
Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, pages 260–272, 2018."
REFERENCES,0.6993865030674846,"Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pages 66–83, 2017."
REFERENCES,0.7055214723926381,"Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-
learning. In AAAI Conference on Artiﬁcial Intelligence, pages 2094–2100, 2016."
REFERENCES,0.7116564417177914,"Maximilian Hüttenrauch, Sosic Adrian, Gerhard Neumann, et al. Deep reinforcement learning for
swarm systems. Journal of Machine Learning Research, 20(54):1–31, 2019."
REFERENCES,0.7177914110429447,"Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pages 2961–2970, 2019."
REFERENCES,0.7239263803680982,"Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son,
and Yung Yi. Learning to schedule communication in multi-agent reinforcement learning. In
International Conference on Representation Learning, 2019."
REFERENCES,0.7300613496932515,"Kamakshi Lakshminarayan, Steven A Harp, and Tariq Samad. Imputation of missing data in industrial
databases. Applied intelligence, 11(3):259–275, 1999."
REFERENCES,0.7361963190184049,"Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. Journal of Machine Learning Research, 17(1):1334–1373, 2016."
REFERENCES,0.7423312883435583,"Xinge Li, Xiaoya Hu, Rongqing Zhang, and Liuqing Yang. Routing protocol design for underwater
optical wireless sensor networks: A multiagent reinforcement learning approach. IEEE Internet of
Things Journal, 7(10):9805–9818, 2020."
REFERENCES,0.7484662576687117,"Teng Liang, Yan Lin, Long Shi, Jun Li, Yijin Zhang, and Yuwen Qian. Distributed vehicle tracking in
wireless sensor network: A fully decentralized multiagent reinforcement learning approach. IEEE
Sensors Letters, 5(1):1–4, 2020."
REFERENCES,0.754601226993865,"Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
International Conference on Machine Learning, pages 157–163. 1994."
REFERENCES,0.7607361963190185,"Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pages 6379–6390, 2017."
REFERENCES,0.7668711656441718,"Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for
learning large incomplete matrices. Journal of Machine Learning Research, 11(80):2287–2322,
2010."
REFERENCES,0.7730061349693251,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529–533, 2015."
REFERENCES,0.7791411042944786,"Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. In AAAI Conference on Artiﬁcial Intelligence, pages 1495–1502, 2018."
REFERENCES,0.7852760736196319,"Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate Q-value functions
for decentralized POMDPs. Journal of Artiﬁcial Intelligence Research, 32:289–353, 2008."
REFERENCES,0.7914110429447853,"Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V Albrecht. Benchmark-
ing multi-agent deep reinforcement learning algorithms in cooperative tasks. arXiv preprint
arXiv:2006.07869, 2021."
REFERENCES,0.7975460122699386,Under review as a conference paper at ICLR 2022
REFERENCES,0.803680981595092,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pages 4295–4304, 2018."
REFERENCES,0.8098159509202454,"Donald B Rubin. Multiple Imputation for Nonresponse in Surveys, volume 81. John Wiley & Sons,
2004."
REFERENCES,0.8159509202453987,"Mikayel Samvelyan, Tabish Rashid, Christian Schröder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob N Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. In International Conference on Autonomous Agents and MultiAgent
Systems, 2019."
REFERENCES,0.8220858895705522,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354–359, 2017."
REFERENCES,0.8282208588957055,"Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pages 5887–5896, 2019."
REFERENCES,0.8343558282208589,"Daniel J Stekhoven and Peter Bühlmann. MissForest—non-parametric missing value imputation for
mixed-type data. Bioinformatics, 28(1):112–118, 2012."
REFERENCES,0.8404907975460123,"Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zam-
baldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-
decomposition networks for cooperative multi-agent learning based on team reward. In Interna-
tional Conference on Autonomous Agents and MultiAgent Systems, pages 2085–2087, 2018."
REFERENCES,0.8466257668711656,Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2011.
REFERENCES,0.852760736196319,"Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
PloS one, 12(4):e0172395, 2017."
REFERENCES,0.8588957055214724,"Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In International
Conference on Machine Learning, pages 330–337, 1993."
REFERENCES,0.8650306748466258,"Bhekisipho Twala. Robot execution failure prediction using incomplete data. In IEEE International
Conference on Robotics and Biomimetics, pages 1518–1523, 2009."
REFERENCES,0.8711656441717791,"Jun Yan, Bo Tang, and Haibo He. Detection of false data attacks in smart grid with supervised
learning. In International Joint Conference on Neural Networks, pages 1395–1402, 2016."
REFERENCES,0.8773006134969326,"Jinsung Yoon, James Jordon, and Mihaela Schaar. GAIN: Missing data imputation using generative
adversarial nets. In International Conference on Machine Learning, pages 5689–5698, 2018."
REFERENCES,0.8834355828220859,Under review as a conference paper at ICLR 2022
REFERENCES,0.8895705521472392,"A
HYPERPARAMETERS"
REFERENCES,0.8957055214723927,"We evaluate the performance of the algorithm across 10 runs with different random seeds. We use
grid search to ﬁne-tune the learning rate and the number of hidden nodes of MADDPG and DDPG.
For GAIN, we use the generator and the discriminator consisting of 2 hidden layers with 128 nodes,
where ReLU activation is used except for the last layers that use sigmoid. We use Adam optimizer
with a learning rate of 0.001 for the training of GAIN. The hint probability ph = 0.5 and αG = 100
are used, and GAIN is trained every 5 episodes. The replay buffer for GAIN is initialized after each
training."
REFERENCES,0.901840490797546,"We use MADDPG for IA-MARL. Each agent has the value function and the policy consisting
of 2 hidden layers with 64 nodes. We use ReLU for activations except for the last layer of value
function that does not use activation and the last layer of policy that uses tanh activation. We
use Adam optimizer for all agents with the learning rate of 0.01 and use target networks with
τ = 0.01. The length of the replay buffer is 106. MADDPG updates every 100 samples and collects
1024 samples before making an initial update. Since the mask-based update makes the number of
training data for IA-MARL smaller, we update IA-MARL every 100/(1 −pm)2 samples and collects
1024/(1 −pm)2 samples before making an initial update. Accordingly, the target network parameters
are less frequently updated in IA-MARL."
REFERENCES,0.9079754601226994,"B
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.9141104294478528,"In this Appendix, to show the effectiveness of IA-MARL in terms of the number of training data
discussed in Section 4.2, we show the training curves of MADDPG with missing training data and
compare them with IA-MARL. Then we show the performance IA-MARL with different imputation
methods. Furthermore, we evaluate the computational time of IA-MARL."
REFERENCES,0.9202453987730062,"(a)
(b)"
REFERENCES,0.9263803680981595,"(c)
(d)"
REFERENCES,0.9325153374233128,"Figure 5: (a)-(b) Speaker-Listener environment with a pair of agents. (c)-(d) Speaker-Listener
environment with two pairs of agents."
REFERENCES,0.9386503067484663,Under review as a conference paper at ICLR 2022
REFERENCES,0.9447852760736196,"Figure 5 shows the performance of MADDPG with different pm in Speaker-Listener environment
with n = 2 and n = 4. We update MADDPG every ND = 100/{Qn
i=1(1 −pmi)2} steps since the
number of data for the training decreases due to the data missing. When the training data is missed,
except for Fig. 5a, MADDPG has lower performance than IA-MARL due to the small number of
training data E

|DP|

as in equation 17. Note that as the number of agents becomes larger, MADDPG
is more slowly trained, while the number of training data of IA-MARL is not affected by the number
of agents."
REFERENCES,0.950920245398773,"(a)
(b)"
REFERENCES,0.9570552147239264,"(c)
(d)"
REFERENCES,0.9631901840490797,"Figure 6: IA-MARL and Simple-IA-MARL with (a)-(b) random imputation and (c)-(d) unobserved
token for the missing training data."
REFERENCES,0.9693251533742331,"Figure 6 shows the performance of IA-MARL and Simple-IA-MARL with different imputation
methods. In Figs. 6a and 6b, we replace the missing training data with uniform random variables, i.e.,
ˆoi,t ∼U(minoi,t∈Ωi(oi,t), maxoi,t∈Ωi(oi,t)), where Ωi is the set of all oi,t. In Figs. 6c and 6d, we
replace the missing training data with the unobserved token −1, i.e., ˆoi,t = −1. Note that for the
both IA-MARL and Simple-IA-MARL, the GAIN shows the better performance than the random
imputation and the use of unobserved token for the missing training data."
REFERENCES,0.9754601226993865,Figure 7: Computation time of IA-MARL across 1000 episodes.
REFERENCES,0.9815950920245399,"Figure 7 shows the computation time of IA-MARL across 1000 episodes when the numbers of agents
are 2, 4, and 5, respectively. We use Intel i7-9700K CPU and NVIDIA GeForce RTX 2080Ti GPU"
REFERENCES,0.9877300613496932,Under review as a conference paper at ICLR 2022
REFERENCES,0.9938650306748467,"for the evaluation. The time for sampling and MARL training takes a major portion compared to the
time for the imputation and GAIN training. Note that as the number of agents increases, the time
for sampling and MARL training linearly increases since each agent has the value function and the
policy that should be trained. On the other hand, GAIN has two networks, i.e., the generator and
the discriminator. Therefore, the time for imputation and GAIN training slowly increases with the
number of agents compared with the time for sampling and MARL training."
