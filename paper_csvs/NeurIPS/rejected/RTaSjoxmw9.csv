Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009671179883945841,"Phase retrieval is a fundamental problem in signal processing, where the goal is
1"
ABSTRACT,0.0019342359767891683,"to recover a (complex-valued) signal from phaseless intensity measurements. It is
2"
ABSTRACT,0.0029013539651837525,"known that natural non-convex formulations of phase retrieval do not have spurious
3"
ABSTRACT,0.0038684719535783366,"local optima. However, the theoretical analyses of such landscape results often rely
4"
ABSTRACT,0.004835589941972921,"on strong assumptions, such as the sampling vectors being Gaussian distributed.
5"
ABSTRACT,0.005802707930367505,"In this paper, we propose and study the problem of outlier robust phase retrieval.
6"
ABSTRACT,0.006769825918762089,"We seek to recover a vector x ∈Rd from n intensity measurements yi = (a⊤
i x)2,
7"
ABSTRACT,0.007736943907156673,"where the sampling vectors ai’s are initially i.i.d. Gaussian, but a small fraction of
8"
ABSTRACT,0.008704061895551257,"the (ai, yi) pairs are adversarially corrupted.
9"
ABSTRACT,0.009671179883945842,"Our main result is a near-sample-optimal and nearly-linear-time algorithm that
10"
ABSTRACT,0.010638297872340425,"provably recovers the ground-truth x in the presence of adversarial corruptions.
11"
ABSTRACT,0.01160541586073501,"We first solve a lightweight convex program to find a vector close to the ground
12"
ABSTRACT,0.012572533849129593,"truth. We then run robust gradient descent starting from this initial solution,
13"
ABSTRACT,0.013539651837524178,"leveraging recent advances in high-dimensional robust statistics. Our approach is
14"
ABSTRACT,0.014506769825918761,"conceptually simple and provides a framework for developing robust algorithms
15"
ABSTRACT,0.015473887814313346,"for other tractable non-convex problems.
16"
INTRODUCTION,0.01644100580270793,"1
Introduction
17"
INTRODUCTION,0.017408123791102514,"Phase retrieval is a fundamental problem in signal processing with applications in various fields, in-
18"
INTRODUCTION,0.0183752417794971,"cluding electron microscopy [32], crystallography [33, 36], astronomy [11], and optical imaging [37].
19"
INTRODUCTION,0.019342359767891684,"In these applications, one often has access to only the magnitudes of the Fourier transforms of a
20"
INTRODUCTION,0.020309477756286266,"complex signal. This is because measuring magnitude (e.g., by aggregating energy over time) is
21"
INTRODUCTION,0.02127659574468085,"much easier than measuring phase (which requires detecting rapid changes). We refer the reader to
22"
INTRODUCTION,0.022243713733075435,"the survey articles [37, 26] for more details about the theory and applications of phase retrieval.
23"
INTRODUCTION,0.02321083172147002,"In this paper, we focus on the real-valued generalized phase retrieval problem, where the Fourier
24"
INTRODUCTION,0.024177949709864602,"transform is replaced by a general linear operator. We first give a formal definition of this problem.
25"
INTRODUCTION,0.025145067698259187,"Definition 1.1 (Phase Retrieval). Let x ∈Rd be the ground-truth vector. Let a1 . . . an ∈Rd be n
26"
INTRODUCTION,0.02611218568665377,"sampling vectors and let yi = ⟨ai, x⟩2 ∈R be the corresponding intensity measurements. Given
27"
INTRODUCTION,0.027079303675048357,"(ai, yi)n
i=1 as input, the task is to recover x.
28"
INTRODUCTION,0.02804642166344294,"Note that it is impossible to distinguish between x and −x, so it is sufficient to recover either one.
29"
INTRODUCTION,0.029013539651837523,"Under certain assumptions (e.g., when the ai’s are Gaussian distributed), the phase retrieval problem
30"
INTRODUCTION,0.029980657640232108,"in Definition 1.1 can be solved in polynomial time with provable recovery guarantees. This was first
31"
INTRODUCTION,0.030947775628626693,"achieved via approaches based on semidefinite programming (SDP) relaxations (see, e.g., Candès
32"
INTRODUCTION,0.031914893617021274,"et al. [5]). In practice, the problem is often solved using first-order optimization algorithms such as
33"
INTRODUCTION,0.03288201160541586,"gradient descent. It is well-established that, although many natural formulations of phase retrieval
34"
INTRODUCTION,0.033849129593810444,"have nonconvex objectives, all local optima are globally optimal under certain assumptions [34, 3, 40].
35"
INTRODUCTION,0.03481624758220503,"An example of such objective function is the following:
36"
INTRODUCTION,0.035783365570599614,"minimize
f(z) = Pn
i=1(yi −⟨ai, z⟩2)2
subject to
z ∈Rd.
37"
INTRODUCTION,0.0367504835589942,"However, existing analyses of such landscape results often rely on strong assumptions, such as the
38"
INTRODUCTION,0.037717601547388784,"sampling vectors ai’s are i.i.d. Gaussian. Our work is motivated by the following questions: Can we
39"
INTRODUCTION,0.03868471953578337,"relax the assumptions used in proving landscape results in many tractable nonconvex problems? In the
40"
INTRODUCTION,0.039651837524177946,"context of phase retrieval, what happens if a small fraction of the (ai, yi)’s are changed adversarially?
41"
INTRODUCTION,0.04061895551257253,"We focus on the following strong contamination model (see, e.g., [13]).
42"
INTRODUCTION,0.041586073500967116,"Definition 1.2 (ϵ-Corruption). An algorithm first specifies the number of samples n, and n samples
43"
INTRODUCTION,0.0425531914893617,"are drawn independently from some unknown distribution D. The adversary is allowed to replace up
44"
INTRODUCTION,0.043520309477756286,"to ϵn samples with arbitrary points. The modified set of n samples is then given to the algorithm as
45"
INTRODUCTION,0.04448742746615087,"input. We say that a set of samples is ϵ-corrupted if it is generated by the above process. 1
46"
INTRODUCTION,0.045454545454545456,"Under the ϵ-corruption model for high-dimensional data, a common goal is to design efficient
47"
INTRODUCTION,0.04642166344294004,"algorithms that can achieve dimension-independent error guarantees. Early work in robust statis-
48"
INTRODUCTION,0.047388781431334626,"tics [42, 23, 25] provided sample-efficient estimators for various tasks, but with runtimes exponential
49"
INTRODUCTION,0.048355899419729204,"in the dimension. A recent line of work, initiated by [13, 28], has developed computationally efficient
50"
INTRODUCTION,0.04932301740812379,"robust algorithms for many fundamental high-dimensional tasks. There has been significant progress
51"
INTRODUCTION,0.05029013539651837,"in the algorithmic aspects of robust high-dimensional statistics (see, e.g., [12]).
52"
INTRODUCTION,0.05125725338491296,"We now formally define the main problem that we pose and study in this paper.
53"
INTRODUCTION,0.05222437137330754,"Problem 1.3 (Outlier-Robust Phase Retrieval). Let ϵ > 0. Let x ∈Rd be the ground-truth vector with
54"
INTRODUCTION,0.05319148936170213,"∥x∥2 = 1. First, n sampling vectors (ai)n
i=1 are drawn i.i.d. from N(0, I) ∈Rd. Let yi = ⟨ai, x⟩2
55"
INTRODUCTION,0.05415860735009671,"be the corresponding intensity measurements. Then, an adversary arbitrarily corrupts an ϵ-fraction of
56"
INTRODUCTION,0.0551257253384913,"the (ai, yi)’s. Finally, the corrupted (ai, yi)’s are given to the algorithm as input. The task is to find a
57"
INTRODUCTION,0.05609284332688588,"vector z ∈Rd such that min{∥z −x∥2 , ∥z + x∥2} ≤∆for some precision parameter ∆> 0.
58"
INTRODUCTION,0.05705996131528046,"Note that we allow corruption in both the sampling vectors ai ∈Rd and the intensity measurements
59"
INTRODUCTION,0.058027079303675046,"yi ∈R. We would like to answer the following algorithmic question:
60"
INTRODUCTION,0.05899419729206963,"Can we design a provably robust and near sample-optimal algorithm for the
61"
INTRODUCTION,0.059961315280464215,"ϵ-corrupted phase retrieval problem (Problem 1.3) that runs in nearly-linear time?
62"
OUR RESULTS AND CONTRIBUTIONS,0.0609284332688588,"1.1
Our Results and Contributions
63"
OUR RESULTS AND CONTRIBUTIONS,0.061895551257253385,"In this paper, we answer the above question affirmatively. We first state the main result of our paper.
64"
OUR RESULTS AND CONTRIBUTIONS,0.06286266924564797,"Theorem 1.4 (Main, Informal). Consider the outlier-robust phase retrieval problem (Problem 1.3).
65"
OUR RESULTS AND CONTRIBUTIONS,0.06382978723404255,"Let ∆> 0. Given an ϵ-corrupted set of n = eΩ(d log2(1/∆)) samples, we can compute z ∈Rd in
66"
OUR RESULTS AND CONTRIBUTIONS,0.06479690522243714,"time eO(nd) such that min(∥z −x∥2 , ∥z + x∥2) ≤∆with probability at least 0.8.
67"
OUR RESULTS AND CONTRIBUTIONS,0.06576402321083172,"Our algorithm has near-optimal sample complexity, because even without corruption, recovering the
68"
OUR RESULTS AND CONTRIBUTIONS,0.06673114119922631,"ground-truth vector x in general requires Ω(d) samples because there are d degrees of freedom in x.
69"
OUR RESULTS AND CONTRIBUTIONS,0.06769825918762089,"Moreover, our algorithm runs in time nearly-linear in the size of the input, and provably recovers
70"
OUR RESULTS AND CONTRIBUTIONS,0.06866537717601548,"the ground-truth vector x with arbitrary precision ∆. The formal version of Theorem 1.4 is stated as
71"
OUR RESULTS AND CONTRIBUTIONS,0.06963249516441006,"Theorem 3.1 in Section 3.
72"
OUR RESULTS AND CONTRIBUTIONS,0.07059961315280464,"We remark that the success probability of Theorem 1.4 can be boosted to 1 −δ for any δ > 0 by
73"
OUR RESULTS AND CONTRIBUTIONS,0.07156673114119923,"incurring an additional factor of T = O(log(1/δ)) in the sample complexity and runtime. We can
74"
OUR RESULTS AND CONTRIBUTIONS,0.0725338491295938,"randomly partition the input into T equal-sized disjoint sets and run our algorithm on each set to
75"
OUR RESULTS AND CONTRIBUTIONS,0.0735009671179884,"obtain T solutions Z = {z1, . . . , zT }. If we output a solution z⋆that has the maximum number of
76"
OUR RESULTS AND CONTRIBUTIONS,0.07446808510638298,"points in Z within distance 2∆, we can show that r(z⋆) ≤3∆with probability at least 1 −δ.
77"
OUR RESULTS AND CONTRIBUTIONS,0.07543520309477757,"Our main conceptual contribution is to propose and study the outlier-robust phase retrieval problem,
78"
OUR RESULTS AND CONTRIBUTIONS,0.07640232108317214,"where a small fraction of the input data is adversarially corrupted. Note that we allow arbitrary
79"
OUR RESULTS AND CONTRIBUTIONS,0.07736943907156674,"corruption in both the sampling vectors ai ∈Rd and the intensity measurements yi ∈R. The
80"
OUR RESULTS AND CONTRIBUTIONS,0.07833655705996131,"nonconvex optimization landscape of phase retrieval is well understood when the ai’s are Gaussian
81"
OUR RESULTS AND CONTRIBUTIONS,0.07930367504835589,"distributed, but the adversarial robustness of such landscape results is largely unexplored.
82"
OUR RESULTS AND CONTRIBUTIONS,0.08027079303675048,"Our main technical contributions include the design and analysis of a near sample-optimal and nearly-
83"
OUR RESULTS AND CONTRIBUTIONS,0.08123791102514506,"linear time algorithm that provably solves the phase retrieval problem in the presence of outliers.
84"
OUR RESULTS AND CONTRIBUTIONS,0.08220502901353965,"Our approach provides a conceptually simple two-step framework for developing outlier-robust
85"
OUR RESULTS AND CONTRIBUTIONS,0.08317214700193423,"algorithms for tractable nonconvex problems that combines the robustness of spectral initialization
86"
OUR RESULTS AND CONTRIBUTIONS,0.08413926499032882,"and the efficiency of the subsequent robust gradient descent.
87"
OUR RESULTS AND CONTRIBUTIONS,0.0851063829787234,"1We write G ⊆[n] for the (remaining) good samples, and B = [n] \ G for the corrupted samples."
OUR APPROACH AND TECHNIQUES,0.086073500967118,"1.2
Our Approach and Techniques
88"
OUR APPROACH AND TECHNIQUES,0.08704061895551257,"When there are infinite samples and no corruption, the objective function f(z) can be simplified as
89"
OUR APPROACH AND TECHNIQUES,0.08800773694390715,"f(z) =
E
a∼N(0,Id)"
OUR APPROACH AND TECHNIQUES,0.08897485493230174,"h
(⟨ai, z⟩2 −yi)2i
= 3 ∥x∥4
2 + 3 ∥z∥4
2 −2 ∥x∥2
2 ∥z∥2
2 −4 ⟨x, z⟩2 .
(1)"
OUR APPROACH AND TECHNIQUES,0.08994197292069632,"Even though f(z) is nonconvex, we know that it has no spurious local optima [34, 3, 40].
90"
OUR APPROACH AND TECHNIQUES,0.09090909090909091,"Our approach follows the general structure of Candès et al. [3], which uses a two-step procedure.
91"
OUR APPROACH AND TECHNIQUES,0.09187620889748549,"The first step uses spectral techniques to find an initial guess that is close enough to the ground truth.
92"
OUR APPROACH AND TECHNIQUES,0.09284332688588008,"The second step applies gradient descent to converge to the final solution. However, both steps are
93"
OUR APPROACH AND TECHNIQUES,0.09381044487427466,"susceptible to adversarial corruption. We develop nearly-linear time and provably robust algorithms
94"
OUR APPROACH AND TECHNIQUES,0.09477756286266925,"for both steps and combine them to get our main result.
95"
OUR APPROACH AND TECHNIQUES,0.09574468085106383,"Step 1: Robust Spectral Initialization. When there is no adversarial corruption, the empirical
96"
OUR APPROACH AND TECHNIQUES,0.09671179883945841,"second-moment matrix Y = (1/n) Pn
i=1 yiaia⊤
i has expectation E [Y ] = I + 2xx⊤, so its top
97"
OUR APPROACH AND TECHNIQUES,0.097678916827853,"eigenvector is close to x. However, the adversary can arbitrarily change the top eigenvector.
98"
OUR APPROACH AND TECHNIQUES,0.09864603481624758,"To circumvent this issue, we assign a (nonnegative) weight wi to each sample, and let Yw denote the
99"
OUR APPROACH AND TECHNIQUES,0.09961315280464217,"weighted intensity-based second-moment matrix Yw = Pn
i=1 wiyiaia⊤
i . Ideally, if the weights w are
100"
OUR APPROACH AND TECHNIQUES,0.10058027079303675,"uniformly distributed on the remaining clean samples, the top eigenvector of Yw will align with x.
101"
OUR APPROACH AND TECHNIQUES,0.10154738878143134,"We propose a novel optimization problem that can be used to find a weighting w such that Yw must
102"
OUR APPROACH AND TECHNIQUES,0.10251450676982592,"be close to the unknown unbiased expectation I + 2xx⊤. Moreover, we show that such a weight w
103"
OUR APPROACH AND TECHNIQUES,0.10348162475822051,"can be computed in nearly-linear time.
104"
OUR APPROACH AND TECHNIQUES,0.10444874274661509,"Step 2: Approximate Gradient Descent. Starting with the initial guess z1 ∈Rd produced by the
105"
OUR APPROACH AND TECHNIQUES,0.10541586073500966,"robust spectral initialization, we want to apply gradient descent to recover the ground truth x ∈Rd.
106"
OUR APPROACH AND TECHNIQUES,0.10638297872340426,"Without corruption, if the initialization is close enough to x, each iteration will bring z closer to x by
107"
OUR APPROACH AND TECHNIQUES,0.10735009671179883,"a constant factor. This convergence guarantee can be compromised by the corrupted samples.
108"
OUR APPROACH AND TECHNIQUES,0.10831721470019343,"At a high level, approximating the gradient at a specific point amounts to a robust mean estimation
109"
OUR APPROACH AND TECHNIQUES,0.109284332688588,"problem (for the underlying distribution of the gradients). When the input data is ϵ-corrupted, the
110"
OUR APPROACH AND TECHNIQUES,0.1102514506769826,"gradients of the n samples can be viewed as an ϵ-corrupted set of vectors. We can approximate the
111"
OUR APPROACH AND TECHNIQUES,0.11121856866537717,"true gradient using this ϵ-corrupted set of n gradients using robust mean estimation algorithms.
112"
RELATED AND PRIOR WORKS,0.11218568665377177,"1.3
Related and Prior Works
113"
RELATED AND PRIOR WORKS,0.11315280464216634,"Phase Retrieval. The problem of phase retrieval arises in many areas of science and engineering
114"
RELATED AND PRIOR WORKS,0.11411992263056092,"[11, 33]. Early research on this problem proposes error-reduction algorithms [22, 17, 18]. Convex
115"
RELATED AND PRIOR WORKS,0.11508704061895551,"and nonconvex optimization with various objective functions were later proposed and achieved exact
116"
RELATED AND PRIOR WORKS,0.11605415860735009,"recovery [43, 3–5, 38]. Follow-up works generalize to robust phase retrieval where the observations
117"
RELATED AND PRIOR WORKS,0.11702127659574468,"are subject to perturbations [45, 27, 7, 6, 31].
118"
RELATED AND PRIOR WORKS,0.11798839458413926,"Nonconvex Optimization. Even though optimizing a nonconvex function is NP-Hard in general,
119"
RELATED AND PRIOR WORKS,0.11895551257253385,"recent works showed that many nonconvex functions are locally optimizable due to discrete or
120"
RELATED AND PRIOR WORKS,0.11992263056092843,"rotational symmetry. Besides phase retrieval, it is known that all local optima are globally optimal
121"
RELATED AND PRIOR WORKS,0.12088974854932302,"for natural nonconvex formulations of a wide range of machine learning problems, such as matrix
122"
RELATED AND PRIOR WORKS,0.1218568665377176,"completion [21], matrix sensing [2], phase synchronization [1], dictionary learning [39], and tensor
123"
RELATED AND PRIOR WORKS,0.12282398452611218,"decomposition [20] (see also Chapter 7 of [44]). Closely related to our work, a recent line of work
124"
RELATED AND PRIOR WORKS,0.12379110251450677,"explored the robustness of these landscape results: [30] studied matrix sensing in the ϵ-corrupted
125"
RELATED AND PRIOR WORKS,0.12475822050290135,"model and [8, 19] studied matrix completion and matrix sensing in semi-random models.
126"
RELATED AND PRIOR WORKS,0.12572533849129594,"High-Dimensional Robust Statistics. Recent works in high-dimensional robust statistics developed
127"
RELATED AND PRIOR WORKS,0.12669245647969052,"nearly-linear time algorithms for the problem of robust mean estimation [9, 16, 29]. Prior works [35,
128"
RELATED AND PRIOR WORKS,0.1276595744680851,"14] developed meta-algorithms for finding first-order stationary points with dimension-independent
129"
RELATED AND PRIOR WORKS,0.1286266924564797,"accuracy guarantees, which is closely related to the robust gradient descent procedure that we use.
130"
ROADMAP,0.12959381044487428,"1.4
Roadmap
131"
ROADMAP,0.13056092843326886,"We first introduce notations and background in Section 2. Then we give an overview of our approach
132"
ROADMAP,0.13152804642166344,"in Section 3. Next, we focus on how to get an initialization that is close enough to the ground truth x
133"
ROADMAP,0.13249516441005801,"in Section 4. After the initialization, we use robust mean algorithms to estimate gradients to converge
134"
ROADMAP,0.13346228239845262,"to the desired accuracy in Section 5. Finally, we conclude in Section 6 and discuss open problems.
135"
PRELIMINARY AND BACKGROUND,0.1344294003868472,"2
Preliminary and Background
136"
PRELIMINARY AND BACKGROUND,0.13539651837524178,"Notation. We write [n] for the set of integers {1, . . . , n}. We use {e1, . . . , ed} for the standard unit
137"
PRELIMINARY AND BACKGROUND,0.13636363636363635,"vector basis in Rd and I for the identity matrix. For a vector x, we denote its ℓ1, ℓ2 and ℓ∞norm as
138"
PRELIMINARY AND BACKGROUND,0.13733075435203096,"∥x∥1, ∥x∥2 and ∥x∥∞, respectively, and write the ith coordinate in x as xi. For vectors x, y ∈Rd,
139"
PRELIMINARY AND BACKGROUND,0.13829787234042554,"we denote its inner product as ⟨x, y⟩= x⊤y. For a matrix A, we use ∥A∥2, ∥A∥∗, and ∥A∥F as its
140"
PRELIMINARY AND BACKGROUND,0.13926499032882012,"operator norm, nuclear norm, and Frobenius norm, respectively. We write λk(A) as the kth-largest
141"
PRELIMINARY AND BACKGROUND,0.1402321083172147,"eigenvalues of A, and λk(A) as the sum of the k largest eigenvalues. A symmetric n × n matrix A
142"
PRELIMINARY AND BACKGROUND,0.14119922630560927,"is said to be positive semidefinite (PSD) if for all vectors x ∈Rn, x⊤Ax ≥0. For two symmetric
143"
PRELIMINARY AND BACKGROUND,0.14216634429400388,"matrices A and B, we write A ⪯B when B −A is positive semidefinite.
144"
PRELIMINARY AND BACKGROUND,0.14313346228239845,"Packing SDP. We will use nearly-linear time solvers for the following packing SDP.
145"
PRELIMINARY AND BACKGROUND,0.14410058027079303,"max
w
∥w∥1
subject to n
X"
PRELIMINARY AND BACKGROUND,0.1450676982591876,"i=1
wiAi ⪯I,
λk n
X"
PRELIMINARY AND BACKGROUND,0.14603481624758222,"i=1
wiBi !"
PRELIMINARY AND BACKGROUND,0.1470019342359768,"≤k,
wi ≥0, ∀i.
(*)"
PRELIMINARY AND BACKGROUND,0.14796905222437137,"Lemma 2.1 ([10]). Given an instance of optimization (*) with semi-positive definite matrices
146"
PRELIMINARY AND BACKGROUND,0.14893617021276595,"Ai ∈Rd1×d1 and Bi ∈Rd2×d2 with Ai = CiC⊤
i , Bi = DiD⊤
i for all i = 1, 2, · · · , m, together
147"
PRELIMINARY AND BACKGROUND,0.14990328820116053,"with integer k > 0, error tolerance ϵ0 ≥1/m2, and failure probability δ0, there is an algorithm that
148"
PRELIMINARY AND BACKGROUND,0.15087040618955513,"runs in time eO((tC + tD + d1 + d2) poly(1/ϵ0, log 1/δ0)), where tCi and tDi are the time take to
149"
PRELIMINARY AND BACKGROUND,0.1518375241779497,"perform a matrix product with Ci and Di respectively and tC = Pn
i=1 tCi and tD = Pn
i=1 tDi, and
150"
PRELIMINARY AND BACKGROUND,0.1528046421663443,"outputs w′ with ∥w′∥1 ≥(1 −ϵ0)OPT where OPT is optimal value, with probability at least 1 −δ0.
151"
PRELIMINARY AND BACKGROUND,0.15377176015473887,"Computing the Top Eigenvector. We use power method to compute the top eigenvector of a matrix.
152"
PRELIMINARY AND BACKGROUND,0.15473887814313347,"Lemma 2.2 (Power Method for Top Eigenvector, e.g., [41]). Let A ∈Rd×d and let λ1 be its largest
153"
PRELIMINARY AND BACKGROUND,0.15570599613152805,"eigenvalue. For any δ ∈(0, 1), there exists an algorithm that takes A and outputs a unit vector
154"
PRELIMINARY AND BACKGROUND,0.15667311411992263,"x ∈Rd in time O(t log(d)/δ) such that xT Ax ≥(1 −δ)λ1 with probability at least 0.99, where t is
155"
PRELIMINARY AND BACKGROUND,0.1576402321083172,"the time required to compute Av for an arbitrary v ∈Rd.
156"
PRELIMINARY AND BACKGROUND,0.15860735009671179,"Robust Mean Estimation. Another tool we use is robust mean estimation in the ϵ-corruption model
157"
PRELIMINARY AND BACKGROUND,0.1595744680851064,"for distributions with bounded covariance. We use robust mean estimation algorithms to approximate
158"
PRELIMINARY AND BACKGROUND,0.16054158607350097,"the true gradient under adversarial corruption.
159"
PRELIMINARY AND BACKGROUND,0.16150870406189555,"Lemma 2.3 (Robust Mean Estimation, e.g., [15]). Let D be a distribution on Rd with unknown
160"
PRELIMINARY AND BACKGROUND,0.16247582205029013,"mean µ and unknown covariance matrix Σ where Σ ⪯σ2I. Let ϵ0 be a sufficiently small universal
161"
PRELIMINARY AND BACKGROUND,0.16344294003868473,"constant. Let 0 < ϵ < ϵ0 and δ > 0. Given an ϵ-corrupted set of n samples drawn from D, we can
162"
PRELIMINARY AND BACKGROUND,0.1644100580270793,"output a vector bµ ∈Rd in time eO(nd log(1/δ)) such that, with probability at least 1 −δ −exp(−nϵ),
163"
PRELIMINARY AND BACKGROUND,0.1653771760154739,"we have ∥bµ −µ∥2 = O
√ϵ +
q"
PRELIMINARY AND BACKGROUND,0.16634429400386846,"d
nδ +
q"
PRELIMINARY AND BACKGROUND,0.16731141199226304,d(log d+log 1/δ) n
PRELIMINARY AND BACKGROUND,0.16827852998065765,"
σ.
164"
OVERVIEW,0.16924564796905223,"3
Overview
165"
OVERVIEW,0.1702127659574468,"We first state a formal version of our main result.
166"
OVERVIEW,0.17117988394584138,"Theorem 3.1 (Main). Consider the setting of Problem 1.3. Let 0 < ϵ < ϵ′ for some universal
167"
OVERVIEW,0.172147001934236,"constant ϵ′ and let ∆> 0. Given an ϵ-corrupted set of n = eΩ(d log2(1/∆)) samples, we can
168"
OVERVIEW,0.17311411992263057,"compute a vector z ∈Rd in time eO(nd log(1/∆)) such that r(z) = min{∥z −x∥2 , ∥z + x∥2} ≤∆
169"
OVERVIEW,0.17408123791102514,"with probability at least 0.8.
170"
OVERVIEW,0.17504835589941972,"Theorem 3.1 requires two key technical lemmas: the robust spectral initialization (Lemma 3.2) and
171"
OVERVIEW,0.1760154738878143,"the approximate gradient descent (Lemma 3.3).
172"
OVERVIEW,0.1769825918762089,"We first show that the spectral initialization can be done in nearly linear time with high probability,
173"
OVERVIEW,0.17794970986460348,"the proof of which can be found in Section 4.
174"
OVERVIEW,0.17891682785299806,"Lemma 3.2 (Robust Spectral Initialization). Under the setting of Problem 1.3, for any 0 < ϵ < ϵ′ for
175"
OVERVIEW,0.17988394584139264,"some universal constant ϵ′ > 0, given an ϵ-corrupted set of n = eΩ(d) samples, we can compute a vec-
176"
OVERVIEW,0.18085106382978725,"tor z0 ∈Rd of the ground truth x in time eO(nd) such that r(z1) = min{∥z1 −x∥2 , ∥z1 + x∥2} ≤1 8
177"
OVERVIEW,0.18181818181818182,"with probability at least 0.95.
178"
OVERVIEW,0.1827852998065764,"Then, with such initialization results, we can proceed to show that an approximate gradient descent
179"
OVERVIEW,0.18375241779497098,"algorithm can be used to find an arbitrary approximation of the ground truth in Section 5.
180"
OVERVIEW,0.18471953578336556,"Lemma 3.3 (Robust Gradient Descent). Consider the setting of Problem 1.3. Let ∆> 0 be
181"
OVERVIEW,0.18568665377176016,"the desired precision. Let 0 < ϵ < ϵ0 for some sufficiently small universal constant ϵ0. Given
182"
OVERVIEW,0.18665377176015474,"an ϵ-corrupted set of n = eΩ(d log2(1/∆)) samples and an initial guess z1 such that r(z1) =
183"
OVERVIEW,0.18762088974854932,"min(∥z1 −x∥2 , ∥z1 + x∥2) ≤1/8, we can compute a vector z ∈Rd in time eO(nd) such that
184"
OVERVIEW,0.1885880077369439,"r(z) ≤∆with probability at least 0.95.
185"
OVERVIEW,0.1895551257253385,"For technical reasons, we cannot use the same set of samples for both the robust spectral initialization
186"
OVERVIEW,0.19052224371373308,"and the approximate gradient descent. Therefore, we partition the ϵ-corrupted set of 2n samples into
187"
OVERVIEW,0.19148936170212766,"two equally sized disjoint sets, using one set for each algorithm.
188"
OVERVIEW,0.19245647969052224,"Proof of Theorem 3.1. Let 2n = eΩ(d log2(1/∆)) be a set of ϵ/2-corrupted samples. We partition
189"
OVERVIEW,0.19342359767891681,"the input into two disjoint sets of n samples. Both sets are ϵ-corrupted. By Lemmas 3.2 and 3.3, for
190"
OVERVIEW,0.19439071566731142,"any ϵ ∈[0, ϵ′] and ∆> 0, our algorithm takes the first set of samples and output a vector z′ in time
191"
OVERVIEW,0.195357833655706,"eO(nd) such that r(z′) ≤1/8 with probability at least 0.95. Then, using z′ and the second set of
192"
OVERVIEW,0.19632495164410058,"samples, our algorithm can output z ∈Rd in time eO(nd) such that r(z) ≤∆with probability at least
193"
OVERVIEW,0.19729206963249515,"0.95. The overall success probability is at least 0.8, and the combined running time is eO(nd).
194"
ROBUST SPECTRAL INITIALIZATION,0.19825918762088976,"4
Robust Spectral Initialization
195"
ROBUST SPECTRAL INITIALIZATION,0.19922630560928434,"We dedicate this section to proving Lemma 3.2: Given an ϵ-corrupted set of (ai, yi)’s, we can compute
196"
ROBUST SPECTRAL INITIALIZATION,0.20019342359767892,"an initial guess z1 ∈Rd that is close to the ground truth x, where min(∥z1 −x∥2 , ∥z1 + x∥2) ≤
197"
ROBUST SPECTRAL INITIALIZATION,0.2011605415860735,"1/8.
To build some intuition, consider the following intensity-based covariance matrix Y
=
198"
"N
PN",0.20212765957446807,"1
n
Pn
i=1 yiaia⊤
i , where each ai is drawn independently from N(0, I) and yi = ⟨ai, x⟩2. The
199"
"N
PN",0.20309477756286268,"expectation of this matrix is E[Y ] = I + 2xx⊤. In other words, when there are enough samples and
200"
"N
PN",0.20406189555125726,"no adversarial corruption, we can obtain a good guess of the ground truth x (or −x) by computing
201"
"N
PN",0.20502901353965183,"the top eigenvector of Y . However, we cannot rely on this approach in adversarial settings.
202"
"N
PN",0.2059961315280464,"To tackle this issue, we propose a nearly-linear time preprocessing step (Algorithm 1) that can recover
203"
"N
PN",0.20696324951644102,"the true expectation of Y under adversarial corruptions. Algorithm 1 assigns a non-negative weight to
204"
"N
PN",0.2079303675048356,"each sample. For a weight vector w ∈Rn and a set of indices S ⊆[n], the weighted intensity-based
205"
"N
PN",0.20889748549323017,"covariance matrix is defined as YS,w .= P"
"N
PN",0.20986460348162475,"i∈S wiyiaia⊤
i , and we omit S when S = [n]. The feasible
206"
"N
PN",0.21083172147001933,"region for the weight vector is: ∆n,ϵ :=
n
w ∈Rn : ∥w∥1 = 1 and ∀i ∈[n], 0 ≤wi ≤
1
(1−ϵ)n
o
.
207"
"N
PN",0.21179883945841393,"A weight w defines an empirical distributions over the samples (ai, yi)n
i=1, where the largest prob-
208"
"N
PN",0.2127659574468085,"ability assigned to any point is
1
(1−ϵ)n. Ideally, we would like to find a weight vector w∗∈∆n,ϵ
209"
"N
PN",0.2137330754352031,"that assigns its weight uniformly to all the uncorrupted samples, i.e., w∗
i =
1
(1−ϵ)n · 1i∈G. To find a
210"
"N
PN",0.21470019342359767,"suitable weighting w, we use the following optimization problem (**) in which λ2 returns the sum of
211"
"N
PN",0.21566731141199227,"the top two eigenvalues (commonly known as the Ky Fan k norm for k = 2).
212"
"N
PN",0.21663442940038685,"min
w
λ2  n
X"
"N
PN",0.21760154738878143,"i=1
wiyiaia⊤
i"
"N
PN",0.218568665377176,"
subject to
0 ≤wi ≤
1
(1−ϵ)n, ∀i ∈[n], n
X"
"N
PN",0.21953578336557059,"i=1
wi = 1 .
(**)"
"N
PN",0.2205029013539652,"At a high level, our main observation is that yiaia⊤
i is always a positive semidefinite matrix as yi ≥0.
213"
"N
PN",0.22147001934235977,"Consequently, the adversary can only add extra directions with large eigenvalues, but will not be able
214"
"N
PN",0.22243713733075435,"to remove the eigendirection of x. By minimizing the Ky Fan 2 norm, we can remove any directions
215"
"N
PN",0.22340425531914893,"added by the adversary and make sure that the only remaining large eigendirection is close to x.
216"
"N
PN",0.22437137330754353,"Let δ ≥0 be some constant to be determined. We show that we can obtain a robust spectral initial-
217"
"N
PN",0.2253384912959381,"ization by solving the packing SDP problem (*), which can be solved efficiently using Lemma 2.1.
218"
"N
PN",0.2263056092843327,"In particular, to fit the reweighting problem of (**) into the framework of the generalized packing
219"
"N
PN",0.22727272727272727,"problem (*), we define the following constraint matrices for all i ∈[n] :
220"
"N
PN",0.22823984526112184,"Ai := (1 −ϵ)n · eie⊤
i ,
Bi := 1"
"N
PN",0.22920696324951645,"2(1 −δ)yiaia⊤
i .
(2)"
"N
PN",0.23017408123791103,"The matrices (Ai)n
i=1 are used to implement the constraint that w ∈∆n,ϵ. The matrices (Bi)n
i=1 help
221"
"N
PN",0.2311411992263056,"make sure the sum of the top two eigenvalues of Yw must be at most roughly 4, because λ2(Yw∗) ≈4.
222"
"N
PN",0.23210831721470018,Algorithm 1 Robust Spectral Initialization
"N
PN",0.2330754352030948,"Input: ϵ-corrupted samples (ai, yi)i∈[n]
Output: The initial guess z′ ∈Rd"
"N
PN",0.23404255319148937,"1: function ROBUSTINIT({(ai, yi)}i∈[n])
2:
{Ai, Bi} ←Constraint matrices as defined in Equation (2)
3:
w′′ ←Solution to Optimization (*) with constraints {Ai, Bi}, k = 2, and precision ϵ0 = 0.9
as in Lemma 2.1
4:
w′ ←w′′/ ∥w′′∥1
5:
z1 ←TOPEIGENVECTOR(Yw′) as in Lemma 2.2 with sufficiently small constant δ.
6:
return z1
7: end function"
"N
PN",0.23500967117988394,"First, we show that the weight w′ computed by Algorithm 1 can ensure the weighted intensity-based
223"
"N
PN",0.23597678916827852,"covariance matrix Yw′ is close enough to the unbiased expectation I + 2xx⊤.
224"
"N
PN",0.2369439071566731,"Lemma 4.1. With probability at least 0.98, the w′ outputted by Algorithm 1 satisfies:
225
Yw′ −(I + 2xx⊤)

2 = O(δ)
(3)"
"N
PN",0.2379110251450677,"In order to show Lemma 4.1, we need the following auxiliary Lemma 4.2, the proof of which can be
226"
"N
PN",0.23887814313346228,"found in Section A. Intuitively, Lemma 4.2 suggests that any weight w in the feasible region ∆n,2ϵ
227"
"N
PN",0.23984526112185686,"will not have a huge impact on the properties of uncorrupted measurements.
228"
"N
PN",0.24081237911025144,"Lemma 4.2. For any δ0 > 0, and sufficiently small ϵ ≥0, given a set of n ϵ-corrupted samples with
229"
"N
PN",0.24177949709864605,"n > eΩ(d), with probability at least 0.98, we have
YG,w −(I + 2xx⊤)

2 ≤δ0 for all w ∈∆n,2ϵ.
230"
"N
PN",0.24274661508704062,"Using Lemma 4.2, we provide a proof sketch for Lemma 4.1, and defer the details to Section A.
231"
"N
PN",0.2437137330754352,"Proof. We condition on the fact that the event of Lemma 4.2 holds (with probability at least 0.98)
232"
"N
PN",0.24468085106382978,"for δ0 = δ. Thus, for the remaining of the proof, we assume that for all w ∈∆n,2ϵ, it holds that
233
YG,w −(I + 2xx⊤)

2 ≤δ.
234"
"N
PN",0.24564796905222436,"Let λ1 and λ2 be the top two eigenvalues of Yw′, with v1 and v2 to be their corresponding eigenvectors.
235"
"N
PN",0.24661508704061896,"Note that the largest eigenvalue of I + 2xx⊤is 3, and the rest of the eigenvalues are all 1. In the
236"
"N
PN",0.24758220502901354,"proof, we show that the eigenvalues of Yw′ are also close to the ones of I + 2xx⊤. Our proof consists
237"
"N
PN",0.24854932301740812,"of two parts. We first establish lower bounds for λ1 and λ2, and then find an upper bound for λ1 + λ2.
238"
"N
PN",0.2495164410058027,"Lower Bound.
Since yiaia⊤
i ⪰0 for any i ∈[n], for any positive weight vector w ∈∆n,ϵ, we
239"
"N
PN",0.2504835589941973,"have YG,w ⪯Yw. Thus a lower bound on eigenvalues of YG,w′ will also be a lower bound on Yw′.
240"
"N
PN",0.2514506769825919,"For the top eigenvalue λ1 of Yw′, it holds
241"
"N
PN",0.2524177949709865,"λ1 = v⊤
1 Yw′v1 ≥x⊤Yw′x ≥x⊤YG,w′x ≥x⊤(I + 2xx⊤)x −δ = 3 −δ.
(4)
Similarly, for the second largest eigenvalue λ2 of Yw′, we have:
242"
"N
PN",0.25338491295938104,"λ2 = v⊤
2 Yw′v2 ≥v⊤
2 YG,w′v2 ≥v⊤
2 (I + 2xx⊤)v2 −δ = 1 + 2 ⟨v2, x⟩2 −δ ≥1 −δ.
(5)"
"N
PN",0.25435203094777564,"Upper Bound.
Through the optimization problem (*), a weight w′′ is calculated such that Yw′′
243"
"N
PN",0.2553191489361702,"are operator-norm upper-bounded by the constraint parameters. Let OPT be the value of the
244"
"N
PN",0.2562862669245648,"optimal solution of the optimization problem (*). The desired uniform weight vector over the good
245"
"N
PN",0.2572533849129594,"samples w∗∈∆n,2ϵ is also a feasible solution to this optimization problem because Yw∗satisfy the
246"
"N
PN",0.25822050290135395,"optimization constraints due to Lemma 4.2. Since w′′ is an ϵ0-approximation to the problem, we have
247"
"N
PN",0.25918762088974856,"∥w′′∥1 ≥(1 −ϵ0)OPT ≥(1 −ϵ0) ∥w∗∥1 = 1 −ϵ0
By optimization constraints, the Ky Fan 2-norm of P"
"N
PN",0.2601547388781431,"i w′′
i Bi = 1"
"N
PN",0.2611218568665377,"2(1 −δ)Yw′′ ≤2, and consequently,
248"
"N
PN",0.2620889748549323,"λ1 + λ2 =
1
∥w′′∥1"
"N
PN",0.26305609284332687,"λ2(Yw′′) ≤
4
(1−ϵ0)(1−δ) .
(6)"
"N
PN",0.2640232108317215,"By combining inequalities (4), (5), and (6), we have shown that the top two eigenvalues of Yw′ are
249"
"N
PN",0.26499032882011603,"close to 3 and 1. Since the rest of the eigenvalues of Yw′ can also be bounded, we can conclude that
250
Yw′ −(I + 2xx⊤)

2 = O(δ).
251"
"N
PN",0.26595744680851063,"We can now show the closeness between the top eigenvector of Yw′ and the ground truth.
252"
"N
PN",0.26692456479690524,"Lemma 4.3. There exists an universal constant ϵ′ such that if 0 ≤ϵ ≤ϵ′, and Algorithm 1 receives
253"
"N
PN",0.2678916827852998,"in input an ϵ-corrupted set of samples, then it outputs z1 ∈Rd such that with probability at least 0.95
254"
"N
PN",0.2688588007736944,it holds r(z1) ≤1
"N
PN",0.269825918762089,"8.
255"
"N
PN",0.27079303675048355,"Proof. We condition on the fact that the event of Lemma 4.1 holds (with probability at least 0.98).
256"
"N
PN",0.27176015473887816,Let the eigendecomposition of Yw′ be Yw′ = P
"N
PN",0.2727272727272727,"i∈[d] λiviv⊤
i , where λ1 ≥. . . ≥λd. Under
257"
"N
PN",0.2736943907156673,"the basis {v1, · · · , vd}, the ground truth x can be represented as x = P
i∈[d] αivi. Note that
258"
"N
PN",0.2746615087040619,"∥x∥2
2 = P"
"N
PN",0.27562862669245647,"i∈[d] α2
i = 1. By Lemma 4.1, we have
Yw′ −(I + 2xx⊤)

2 = O(δ). Thus, we have
259"
"N
PN",0.2765957446808511,"x⊤Yw′x ≥3 −O(δ)
and"
"N
PN",0.2775628626692456,"x⊤Yw′x ≤λ1α2
1 + λ2(1 −α2
1) ≤(3 + O(δ))α2
1 + (1 + O(δ))(1 −α2
1) ≤1 + 2α2 + O(δ)."
"N
PN",0.27852998065764023,"This implies α2
1 ≥1 −O(δ). As a result,
260"
"N
PN",0.27949709864603484,"r2(v1) =

min{∥v1 −x∥2
2 , ∥v1 + x∥2
2}

= min{(α1 −1)2, (α1 + 1)2} + d
X"
"N
PN",0.2804642166344294,"i=2
α2
i"
"N
PN",0.281431334622824,"= min{2 −2α1, 2 + 2α1} = O(δ)."
"N
PN",0.28239845261121854,The last inequality holds as long as δ is sufficiently small. Let z1 = P
"N
PN",0.28336557059961315,"i∈[d] βivi be the unit vector
261"
"N
PN",0.28433268858800775,"approximating v1 returned by the algorithm. By Lemma 2.2, we have that z⊤
1 Yw′z1 ≥(1 −δ)λ1
262"
"N
PN",0.2852998065764023,"with probability at least 0.99. Thus, we have:
263"
"N
PN",0.2862669245647969,"z⊤
1 Yw′z1 ≥(1 −δ)λ1 ≥(1 −δ)(3 −O(δ)) ≥3 −O(δ + δ)
and"
"N
PN",0.2872340425531915,"z⊤
1 Yw′z1 ≤λ1α2
1 + λ2(1 −α2
1) ≤1 + 2β2
1 + O(δ)."
"N
PN",0.28820116054158607,"Again, this implies that β2
1 ≥1 −O(δ + δ). We can show that min{∥v1 −z1∥2
2 , ∥v1 + z1∥2
2} =
264"
"N
PN",0.28916827852998067,"O(δ + δ). By the triangle inequality, we can conclude that r2(z1) = O(δ + δ) ≤1/64, where the
265"
"N
PN",0.2901353965183752,"last inequality is obtained by choosing sufficiently small δ and δ. Therefore, there exists an universal
266"
"N
PN",0.2911025145067698,"constant ϵ′ ≥0 such that for all 0 ≤ϵ ≤ϵ′, Algorithm 1 takes n = eΩ(d) samples and outputs z1
267"
"N
PN",0.29206963249516443,"such that r(z1) ≤1/8 with probability at least 0.95.
268"
"N
PN",0.293036750483559,"Lemma 4.4. Algorithm 1 runs in time eO(nd).
269"
"N
PN",0.2940038684719536,"Proof of Lemma 4.4. Since we have the factorization of the rank-two matrices Ai and rank-one
270"
"N
PN",0.29497098646034814,"matrices Bi for all i = 1, 2, . . . , n, and the time to perform a matrix-vector product with Ci and Di is
271"
"N
PN",0.29593810444874274,"O(d). Therefore, by Lemma 2.1, with tC and tD to be eO(nd), Line 3 runs in eO(nd) time. In Line 5,
272"
"N
PN",0.29690522243713735,"by Lemma 2.2, the top eigenvector of Yw′ can be computed in eO(n log d) time using power method.
273"
"N
PN",0.2978723404255319,"Scaling in Line 4 runs in O(n) time. As a result, Algorithm 1 runs in eO(nd) time.
274"
"N
PN",0.2988394584139265,"We can directly combine Lemma 4.3 and Lemma 4.4 to finish the proof of Lemma 3.2.
275"
ROBUST GRADIENT DESCENT,0.29980657640232106,"5
Robust Gradient Descent
276"
ROBUST GRADIENT DESCENT,0.30077369439071566,"After the robust spectral initialization in Section 4, we have an initial guess z1 ∈Rd that is close to
277"
ROBUST GRADIENT DESCENT,0.30174081237911027,"the ground truth x or −x. Without loss of generality, we can assume that z1 is closer to x than to −x.
278"
ROBUST GRADIENT DESCENT,0.3027079303675048,"In this section, we prove Lemma 3.3: Given an initial guess z1 with ∥z1 −x∥2 ≤1/8, we can use
279"
ROBUST GRADIENT DESCENT,0.3036750483558994,"a robust gradient descent algorithm (Algorithm 2) to recover x to any desire precision ∆> 0. It is
280"
ROBUST GRADIENT DESCENT,0.30464216634429403,"well-known that gradient descent can achieve geometric convergence rates in non-adversarial settings.
281"
ROBUST GRADIENT DESCENT,0.3056092843326886,"We show that Algorithm 2 achieves a similar convergence rate even when the input is ϵ-corrupted.
282"
ROBUST GRADIENT DESCENT,0.3065764023210832,"Consider the natural nonconvex formulation: minz∈Rd Pn
i=1 fi(z) where fi(z) .=
 
⟨ai, z⟩2 −yi
2 .
283"
ROBUST GRADIENT DESCENT,0.30754352030947774,"Let gi denote the gradient of fi with respect to z. Let Dz denote the distribution of gi(z) ∈Rd when
284"
ROBUST GRADIENT DESCENT,0.30851063829787234,"there is no adversarial corruption. Formally, g(z) ∼Dz is distributed as
285"
ROBUST GRADIENT DESCENT,0.30947775628626695,g(z) = ∂ ∂z
ROBUST GRADIENT DESCENT,0.3104448742746615,"h 
⟨a, z⟩2 −⟨a, x⟩22i
= −4
 
⟨a, z⟩2 −⟨a, x⟩2
⟨a, z⟩a where a ∼N(0, I) .
(7)"
ROBUST GRADIENT DESCENT,0.3114119922630561,"To run gradient descent, we want to estimate the expected true gradient µz .= E g(z) using samples.
286"
ROBUST GRADIENT DESCENT,0.31237911025145065,"The challenge is that the input samples {(ai, yi)}i∈[n] are ϵ-corrupted, and consequently the gradients
287"
ROBUST GRADIENT DESCENT,0.31334622823984526,"{gi(z)}i∈[n] is an ϵ-corrupted set of vectors drawn from Dz. To address this, we use robust mean
288"
ROBUST GRADIENT DESCENT,0.31431334622823986,"estimation algorithms (e.g., [16]) to approximate µz, the true mean of Dz.
289"
ROBUST GRADIENT DESCENT,0.3152804642166344,Algorithm 2 Robust Gradient Descent
ROBUST GRADIENT DESCENT,0.316247582205029,"Input:
ϵ > 0, an ϵ-corrupted set of n samples {(ai, yi)}i∈[n], initial guess z1 ∈Rd with
∥z1 −x∥≤1/8, and desired precision ∆> 0.
Output: z ∈Rd such that ∥z −x∥2 ≤∆where x is the ground truth.
1: procedure ROBUSTGD(ϵ, {(ai, yi)}i∈[n], z1, ∆)
2:
T ←O(log(1/∆)), η ←1/300
3:
{N1, N2, · · · , NT } ←a random disjoint partition of [n] such that |Nt| = n/T for all t ∈[T]
4:
for t = 1, 2, . . . , T do
5:
bµzt ←Robust mean estimation on input {gi(zt)}i∈Nt using Lemma 5.2
6:
zt+1 ←zt −η bµzt
7:
end for
8:
return zT +1
9: end procedure"
ROBUST GRADIENT DESCENT,0.31721470019342357,"The error guarantee of robust mean estimation algorithms depends on the covariance matrix Σz of
290"
ROBUST GRADIENT DESCENT,0.3181818181818182,"the distribution Dz. The next lemma upper bounds the spectral norm of Σz.
291"
ROBUST GRADIENT DESCENT,0.3191489361702128,"Lemma 5.1. Let Dz be the distribution of gradients at z as defined in Equation (7). For any z ∈Rd
292"
ROBUST GRADIENT DESCENT,0.32011605415860733,"with ∥z −x∥2 ≤1, the covariance matrix Σz of Dz satisfies Σz ⪯O(∥z −x∥2
2)I.
293"
ROBUST GRADIENT DESCENT,0.32108317214700194,"The proof of Lemma 5.1 is deferred to Appendix B. Given Lemma 5.1, we can show that robust
294"
ROBUST GRADIENT DESCENT,0.32205029013539654,"mean estimation algorithms can approximate µz with small error. For technical reasons, we randomly
295"
ROBUST GRADIENT DESCENT,0.3230174081237911,"partition the input samples (ai, yi) into T subsets, and use one subset in each iteration. With high
296"
ROBUST GRADIENT DESCENT,0.3239845261121857,"probability, each partition has at most (2ϵ)-fraction of corrupted samples
297"
ROBUST GRADIENT DESCENT,0.32495164410058025,"Lemma 5.2. Consider any z ∈Rd with ∥z −x∥2 ≤1. Let µz be the mean of Dz as defined in
298"
ROBUST GRADIENT DESCENT,0.32591876208897486,"Equation (7). Fix universal constants c > 0 and ϵ0 = Θ(c2). Let 2ϵ < ϵ0 and δ > 0. Given
299"
ROBUST GRADIENT DESCENT,0.32688588007736946,"a (2ϵ)-corrupted set of m = Ω(d log d/δ) samples drawn from Dz, we can compute bµz in time
300"
ROBUST GRADIENT DESCENT,0.327852998065764,"eO(md log(1/δ)) such that ∥bµz −µz∥2 ≤c ∥z −x∥2 with probability at least 1 −O(δ).
301"
ROBUST GRADIENT DESCENT,0.3288201160541586,"Proof of Lemma 5.2. Since 2ϵ < ϵ0, we can view the (2ϵ)-corrupted set of m samples as ϵ0-corrupted.
302"
ROBUST GRADIENT DESCENT,0.32978723404255317,"We need to replace 2ϵ with ϵ0 to reduce the failure probability of Lemma 2.3. This weakens the error
303"
ROBUST GRADIENT DESCENT,0.3307543520309478,"guarantee of Lemma 2.3, but the resulting bµz is still accurate enough for our algorithm.
304"
ROBUST GRADIENT DESCENT,0.3317214700193424,"We apply Lemma 2.3 to the ϵ0-corrupted set of m vectors drawn from Dz.
By Lemma 5.1,
305"
ROBUST GRADIENT DESCENT,0.33268858800773693,"the covariance matrix of Dz satisfies Σz ⪯O(∥z −x∥2
2)I. Consequently, for sufficiently large
306"
ROBUST GRADIENT DESCENT,0.33365570599613154,"m = Θ(d log d/δ) and sufficiently small ϵ0 = O(c2), the error guarantee of Lemma 2.3 is
307"
ROBUST GRADIENT DESCENT,0.3346228239845261,"O
√ϵ0 +
q"
ROBUST GRADIENT DESCENT,0.3355899419729207,"d
mδ +
q"
ROBUST GRADIENT DESCENT,0.3365570599613153,d(log d+log(1/δ)) m
ROBUST GRADIENT DESCENT,0.33752417794970985,"
∥z −x∥2 ≤c ∥z −x∥2. The success probability is at least
308"
ROBUST GRADIENT DESCENT,0.33849129593810445,"1 −δ −exp(−ϵ0m) = 1 −O(δ).
309"
ROBUST GRADIENT DESCENT,0.33945841392649906,"Lemma 5.2 shows that even with a (2ϵ)-corrupted set of gradients, the true gradient µz can be
310"
ROBUST GRADIENT DESCENT,0.3404255319148936,"estimated up to an additive error proportional to the distance between z and x. The next lemma shows
311"
ROBUST GRADIENT DESCENT,0.3413926499032882,"that such an approximate gradient is sufficient for gradient descent to converge, reducing the distance
312"
ROBUST GRADIENT DESCENT,0.34235976789168276,"to the ground truth x by a constant factor in each iteration.
313"
ROBUST GRADIENT DESCENT,0.34332688588007737,"Lemma 5.3. Suppose in iteration t of Algorithm 2, the current solution zt satisfies ∥zt −x∥2 ≤1/8,
314"
ROBUST GRADIENT DESCENT,0.344294003868472,"and the estimated gradient bµzt ∈Rd satisfies ∥bµzt −µzt∥2 ≤c ∥zt −x∥2 for c = 4. Then, we have
315"
ROBUST GRADIENT DESCENT,0.3452611218568665,"∥zt+1 −x∥2
2 ≤0.99 ∥zt −x∥2
2.
316"
ROBUST GRADIENT DESCENT,0.34622823984526113,"Proof Sketch of Lemma 5.3. We provide a proof sketch and defer the full proof to Appendix B. Our
317"
ROBUST GRADIENT DESCENT,0.3471953578336557,"objective function is nonconvex (even with infinitely many samples and no corruption). However,
318"
ROBUST GRADIENT DESCENT,0.3481624758220503,"when the starting point z1 is close to a global optimum, it is well-known that gradient descent is
319"
ROBUST GRADIENT DESCENT,0.3491295938104449,"well-behaved. More specifically, for any z close to the ground truth x, we can show that the (expected)
320"
ROBUST GRADIENT DESCENT,0.35009671179883944,"true gradient µz aligns with the direction toward x:
321"
ROBUST GRADIENT DESCENT,0.35106382978723405,"⟨µz, z −x⟩≥7.5 ∥z −x∥2
2 and ∥µz∥2 ≤29 ∥z −x∥2 ,"
ROBUST GRADIENT DESCENT,0.3520309477756286,"which is sufficient for proving geometric convergence. We can immediately see that this argument is
322"
ROBUST GRADIENT DESCENT,0.3529980657640232,"robust to additive error in µz that is proportional to ∥z −x∥2. When ∥bµz −µz∥2 ≤c ∥z −x∥2,
323"
ROBUST GRADIENT DESCENT,0.3539651837524178,"⟨bµz, z −x⟩≥(7.5 −c) ∥zt −x∥2
2 and ∥bµzt∥2 ≤(29 + c) ∥zt −x∥2 ."
ROBUST GRADIENT DESCENT,0.35493230174081236,"When c < 7.5, we can choose an appropriate step size η such that the distance between zt and x
324"
ROBUST GRADIENT DESCENT,0.35589941972920697,"decreases by a constant factor in each iteration.
325"
ROBUST GRADIENT DESCENT,0.3568665377176016,"We are now ready to prove Lemma 3.3, which states the performance guarantee, sample complexity,
326"
ROBUST GRADIENT DESCENT,0.3578336557059961,"runtime, and success probability of Algorithm 2. We restate Lemma 3.3 before proving it.
327"
ROBUST GRADIENT DESCENT,0.35880077369439073,"Lemma 3.3 (Robust Gradient Descent). Consider the setting of Problem 1.3. Let ∆> 0 be
328"
ROBUST GRADIENT DESCENT,0.3597678916827853,"the desired precision. Let 0 < ϵ < ϵ0 for some sufficiently small universal constant ϵ0. Given
329"
ROBUST GRADIENT DESCENT,0.3607350096711799,"an ϵ-corrupted set of n = eΩ(d log2(1/∆)) samples and an initial guess z1 such that r(z1) =
330"
ROBUST GRADIENT DESCENT,0.3617021276595745,"min(∥z1 −x∥2 , ∥z1 + x∥2) ≤1/8, we can compute a vector z ∈Rd in time eO(nd) such that
331"
ROBUST GRADIENT DESCENT,0.36266924564796904,"r(z) ≤∆with probability at least 0.95.
332"
ROBUST GRADIENT DESCENT,0.36363636363636365,"Proof of Lemma 3.3. First, we prove the success probability of Algorithm 2. Algorithm 2 can fail in
333"
ROBUST GRADIENT DESCENT,0.3646034816247582,"two ways: (i) if some Nt has more than (2ϵ)-fraction of corrupted samples, or (ii) if Lemma 5.2 fails
334"
ROBUST GRADIENT DESCENT,0.3655705996131528,"in some iteration t. The probability of event (i) is at most 0.01 for our choice of n, which follows
335"
ROBUST GRADIENT DESCENT,0.3665377176015474,"from a standard application of Hoeffding’s inequality and a union bound over T iterations. For event
336"
ROBUST GRADIENT DESCENT,0.36750483558994196,"(ii), we choose a sufficiently small δ = O(1/T) in Lemma 5.2, so each robust gradient estimation
337"
ROBUST GRADIENT DESCENT,0.36847195357833656,"fails with probability at most O(δ) = 0.01/T, and overall the probability of event (ii) is at most 0.01.
338"
ROBUST GRADIENT DESCENT,0.3694390715667311,"For the rest of the proof, we assume these bad events do not happen.
339"
ROBUST GRADIENT DESCENT,0.3704061895551257,"Next, we show the correctness of Algorithm 2. Without loss of generality, we can assume that z1 is
340"
ROBUST GRADIENT DESCENT,0.3713733075435203,"closer to the ground truth x than to −x, which implies ∥z1 −x∥2 ≤1/8. By Lemma 5.2, we can
341"
ROBUST GRADIENT DESCENT,0.3723404255319149,"obtain an approximation bµz1 of the true gradient µz1 at z1 such that ∥bµz1 −µz1∥2 ≤c ∥z1 −x∥2.
342"
ROBUST GRADIENT DESCENT,0.3733075435203095,"Then by Lemma 5.3, we know that ∥z2 −x∥2 ≤0.99 ∥z1 −x∥2 after one step of gradient descent.
343"
ROBUST GRADIENT DESCENT,0.3742746615087041,"We can iteratively apply these two lemmas to show that, after T = O(log(1/∆)) iterations, we
344"
ROBUST GRADIENT DESCENT,0.37524177949709864,"have ∥zT +1 −x∥2 ≤∆. One technical issue is that in iteration t, we need to use a fresh subset of
345"
ROBUST GRADIENT DESCENT,0.37620889748549324,"samples Nt. By the principle of deferred decisions, we can view (ai, yi)i∈Nt as being generated (and
346"
ROBUST GRADIENT DESCENT,0.3771760154738878,"corrupted) after zt is chosen, which forms a (2ϵ)-corrupted set of gradients at zt.
347"
ROBUST GRADIENT DESCENT,0.3781431334622824,"Finally, we analyze the sample complexity and runtime of Algorithm 2. Algorithm 2 requires in total
348"
ROBUST GRADIENT DESCENT,0.379110251450677,"n = mT = Ω(d log d log2(1/∆)) samples. A random partition can be computed in O(n) time by
349"
ROBUST GRADIENT DESCENT,0.38007736943907156,"shuffling the input. In each iteration, the m gradients in Nt can be computed using Equation (7) in
350"
ROBUST GRADIENT DESCENT,0.38104448742746616,"time O(md), and zt can be updated in time O(d). By Lemma 5.2, the true gradient can be robustly
351"
ROBUST GRADIENT DESCENT,0.3820116054158607,"estimated in time eO(md log T) = eO(md log log(1/∆)). The overall runtime of the algorithm is
352"
ROBUST GRADIENT DESCENT,0.3829787234042553,"eO(n + (md log log(1/∆))T) = eO(nd log log(1/∆)) = eO(nd).
353"
CONCLUSIONS AND FUTURE DIRECTIONS,0.3839458413926499,"6
Conclusions and Future Directions
354"
CONCLUSIONS AND FUTURE DIRECTIONS,0.3849129593810445,"In this paper, our main conceptual contribution is to propose and study the outlier-robust phase
355"
CONCLUSIONS AND FUTURE DIRECTIONS,0.3858800773694391,"retrieval problem, where a constant fraction of the input data is corrupted. Notably, we allow
356"
CONCLUSIONS AND FUTURE DIRECTIONS,0.38684719535783363,"corruption in both the sampled frequencies ai ∈Rd and the intensity measurements yi ∈R. Our
357"
CONCLUSIONS AND FUTURE DIRECTIONS,0.38781431334622823,"main technical contribution is the design and analysis of a near-sample-optimal and nearly-linear-time
358"
CONCLUSIONS AND FUTURE DIRECTIONS,0.38878143133462284,"algorithm that solves this problem with provably guarantees.
359"
CONCLUSIONS AND FUTURE DIRECTIONS,0.3897485493230174,"An immediate technical question is whether our sample complexity can be tightened by removing
360"
CONCLUSIONS AND FUTURE DIRECTIONS,0.390715667311412,"some log(1/∆) factors. One potential approach is to open robust mean estimation algorithms instead
361"
CONCLUSIONS AND FUTURE DIRECTIONS,0.3916827852998066,"of using them in a black-box manner. One could examine the stability conditions that these algorithms
362"
CONCLUSIONS AND FUTURE DIRECTIONS,0.39264990328820115,"require, and see if these stability conditions can be proved without partitioning the samples and using
363"
CONCLUSIONS AND FUTURE DIRECTIONS,0.39361702127659576,"fresh samples in each iteration. More broadly, we believe our framework can be used to develop
364"
CONCLUSIONS AND FUTURE DIRECTIONS,0.3945841392649903,"outlier-robust algorithms for other tractable nonconvex problems, by first finding an initial solution in
365"
CONCLUSIONS AND FUTURE DIRECTIONS,0.3955512572533849,"a saddle-free region near a global optimum and then converging to this global optimum using robust
366"
CONCLUSIONS AND FUTURE DIRECTIONS,0.3965183752417795,"gradient descent.
367"
REFERENCES,0.39748549323017407,"References
368"
REFERENCES,0.3984526112185687,"[1] A. S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank approach for semidefinite
369"
REFERENCES,0.3994197292069632,"programs arising in synchronization and community detection. In Conference on learning
370"
REFERENCES,0.40038684719535783,"theory, pages 361–382. PMLR, 2016.
371"
REFERENCES,0.40135396518375244,"[2] S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low rank
372"
REFERENCES,0.402321083172147,"matrix recovery. Advances in Neural Information Processing Systems, 29, 2016.
373"
REFERENCES,0.4032882011605416,"[3] E. J. Candès, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and
374"
REFERENCES,0.40425531914893614,"algorithms. IEEE Trans. Inf. Theory, 61(4):1985–2007, 2015a.
375"
REFERENCES,0.40522243713733075,"[4] E. J. Candès, X. Li, and M. Soltanolkotabi. Phase retrieval from coded diffraction patterns.
376"
REFERENCES,0.40618955512572535,"Applied and Computational Harmonic Analysis, 39(2):277–299, Sept. 2015b. ISSN 1063-5203.
377"
REFERENCES,0.4071566731141199,"[5] E. J. Candès, Y. C. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix completion.
378"
REFERENCES,0.4081237911025145,"SIAM Rev., 57(2):225–251, 2015c.
379"
REFERENCES,0.4090909090909091,"[6] J. Chen, L. Wang, X. Zhang, and Q. Gu. Robust Wirtinger Flow for Phase Retrieval with
380"
REFERENCES,0.41005802707930367,"Arbitrary Corruption, Jan. 2018.
381"
REFERENCES,0.41102514506769827,"[7] Y. Chen and E. Candes. Solving Random Quadratic Systems of Equations Is Nearly as Easy as
382"
REFERENCES,0.4119922630560928,"Solving Linear Systems. In Advances in Neural Information Processing Systems, volume 28.
383"
REFERENCES,0.41295938104448743,"Curran Associates, Inc., 2015.
384"
REFERENCES,0.41392649903288203,"[8] Y. Cheng and R. Ge. Non-convex matrix completion against a semi-random adversary. In
385"
REFERENCES,0.4148936170212766,"Conference On Learning Theory, pages 1362–1394. PMLR, 2018.
386"
REFERENCES,0.4158607350096712,"[9] Y. Cheng, I. Diakonikolas, and R. Ge. High-dimensional robust mean estimation in nearly-linear
387"
REFERENCES,0.41682785299806574,"time. In Proceedings of the 30th ACM-SIAM Symposium on Discrete Algorithms (SODA), pages
388"
REFERENCES,0.41779497098646035,"2755–2771. SIAM, 2019.
389"
REFERENCES,0.41876208897485495,"[10] Y. Cherapanamjeri, S. Mohanty, and M. Yau. List decodable mean estimation in nearly linear
390"
REFERENCES,0.4197292069632495,"time. In S. Irani, editor, 61st IEEE Annual Symposium on Foundations of Computer Science,
391"
REFERENCES,0.4206963249516441,"FOCS, pages 141–148. IEEE, 2020.
392"
REFERENCES,0.42166344294003866,"[11] C. Dainty and J. R. Fienup. Phase retrieval and image reconstruction for astronomy. Image
393"
REFERENCES,0.42263056092843326,"recovery: theory and application, 231:275, 1987.
394"
REFERENCES,0.42359767891682787,"[12] I. Diakonikolas and D. M. Kane. Algorithmic High-Dimensional Robust Statistics. Cambridge
395"
REFERENCES,0.4245647969052224,"University Press, 2023.
396"
REFERENCES,0.425531914893617,"[13] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust estimators in
397"
REFERENCES,0.42649903288201163,"high dimensions without the computational intractability. In 57th Annual IEEE Symposium on
398"
REFERENCES,0.4274661508704062,"Foundations of Computer Science—FOCS 2016, pages 655–664. IEEE Computer Soc., Los
399"
REFERENCES,0.4284332688588008,"Alamitos, CA, 2016.
400"
REFERENCES,0.42940038684719534,"[14] I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, and A. Stewart. Sever: A robust
401"
REFERENCES,0.43036750483558994,"meta-algorithm for stochastic optimization. In K. Chaudhuri and R. Salakhutdinov, editors, Pro-
402"
REFERENCES,0.43133462282398455,"ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
403"
REFERENCES,0.4323017408123791,"of Machine Learning Research, pages 1596–1606. PMLR, 09–15 Jun 2019.
404"
REFERENCES,0.4332688588007737,"[15] Y. Dong, S. Hopkins, and J. Li. Quantum Entropy Scoring for Fast Robust Mean Estimation and
405"
REFERENCES,0.43423597678916825,"Improved Outlier Detection. In Advances in Neural Information Processing Systems, volume 32.
406"
REFERENCES,0.43520309477756286,"Curran Associates, Inc., 2019.
407"
REFERENCES,0.43617021276595747,"[16] Y. Dong, S. B. Hopkins, and J. Li. Quantum entropy scoring for fast robust mean estimation and
408"
REFERENCES,0.437137330754352,"improved outlier detection. In Proc. 33rd Advances in Neural Information Processing Systems
409"
REFERENCES,0.4381044487427466,"(NeurIPS), 2019.
410"
REFERENCES,0.43907156673114117,"[17] J. R. Fienup. Reconstruction of an object from the modulus of its Fourier transform. Optics
411"
REFERENCES,0.4400386847195358,"Letters, 3(1):27–29, July 1978. ISSN 1539-4794.
412"
REFERENCES,0.4410058027079304,"[18] J. R. Fienup. Phase retrieval algorithms: A comparison. Applied Optics, 21(15):2758–2769,
413"
REFERENCES,0.44197292069632493,"Aug. 1982. ISSN 2155-3165.
414"
REFERENCES,0.44294003868471954,"[19] X. Gao and Y. Cheng. Robust matrix sensing in the semi-random model. Proceedings of the
415"
REFERENCES,0.44390715667311414,"37th Conference on Neural Information Processing Systems (NeurIPS), 2023.
416"
REFERENCES,0.4448742746615087,"[20] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points—online stochastic gradient
417"
REFERENCES,0.4458413926499033,"for tensor decomposition. In Conference on Learning Theory, pages 797–842, 2015.
418"
REFERENCES,0.44680851063829785,"[21] R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in
419"
REFERENCES,0.44777562862669246,"Neural Information Processing Systems, pages 2973–2981, 2016.
420"
REFERENCES,0.44874274661508706,"[22] R. W. Gerchberg.
A practical algorithm for the determination of phase from image and
421"
REFERENCES,0.4497098646034816,"diffraction plane pictures. Optik, Jan. 1972.
422"
REFERENCES,0.4506769825918762,"[23] F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. Robust statistics. The
423"
REFERENCES,0.45164410058027077,"approach based on influence functions. Wiley New York, 1986.
424"
REFERENCES,0.4526112185686654,"[24] P. Hand and V. Voroninski. Corruption robust phase retrieval via linear programming. CoRR,
425"
REFERENCES,0.45357833655706,"abs/1612.03547, 2016.
426"
REFERENCES,0.45454545454545453,"[25] P. J. Huber and E. M. Ronchetti. Robust statistics. Wiley New York, 2009.
427"
REFERENCES,0.45551257253384914,"[26] K. Jaganathan, Y. C. Eldar, and B. Hassibi. Phase retrieval: An overview of recent developments.
428"
REFERENCES,0.4564796905222437,"Optical Compressive Imaging, pages 279–312, 2016.
429"
REFERENCES,0.4574468085106383,"[27] R. Kolte and A. Özgür. Phase Retrieval via Incremental Truncated Wirtinger Flow, June 2016.
430"
REFERENCES,0.4584139264990329,"[28] K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In focs2016,
431"
REFERENCES,0.45938104448742745,"pages 665–674, 2016.
432"
REFERENCES,0.46034816247582205,"[29] G. Lecué, M. Lerasle, and T. Mathieu. Robust classification via MOM minimization. Mach.
433"
REFERENCES,0.46131528046421666,"Learn., 109(8):1635–1665, 2020.
434"
REFERENCES,0.4622823984526112,"[30] S. Li, Y. Cheng, I. Diakonikolas, J. Diakonikolas, R. Ge, and S. Wright. Robust second-order
435"
REFERENCES,0.4632495164410058,"nonconvex optimization and its application to low rank matrix sensing. In Proc. 37th Advances
436"
REFERENCES,0.46421663442940037,"in Neural Information Processing Systems (NeurIPS), 2023.
437"
REFERENCES,0.46518375241779497,"[31] J.-W. Liu, Z.-J. Cao, J. Liu, X.-L. Luo, W.-M. Li, N. Ito, and L.-C. Guo. Phase Retrieval via
438"
REFERENCES,0.4661508704061896,"Wirtinger Flow Algorithm and Its Variants. In 2019 International Conference on Machine
439"
REFERENCES,0.4671179883945841,"Learning and Cybernetics (ICMLC), pages 1–9, July 2019.
440"
REFERENCES,0.46808510638297873,"[32] J. Miao, T. Ishikawa, B. Johnson, E. H. Anderson, B. Lai, and K. O. Hodgson. High resolution
441"
REFERENCES,0.4690522243713733,"3D X-ray diffraction microscopy. Physical review letters, 89(8):088303, 2002.
442"
REFERENCES,0.4700193423597679,"[33] R. P. Millane. Phase retrieval in crystallography and optics. JOSA A, 7(3):394–411, 1990.
443"
REFERENCES,0.4709864603481625,"[34] P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. In Proc.
444"
REFERENCES,0.47195357833655704,"27th Advances in Neural Information Processing Systems (NeurIPS), pages 2796–2804, 2013.
445"
REFERENCES,0.47292069632495165,"[35] A. Prasad, A. S. Suggala, S. Balakrishnan, and P. Ravikumar. Robust estimation via robust
446"
REFERENCES,0.4738878143133462,"gradient estimation. Journal of the Royal Statistical Society. Series B. Statistical Methodology,
447"
REFERENCES,0.4748549323017408,"82(3):601–627, 2020.
448"
REFERENCES,0.4758220502901354,"[36] W. H. Robert. Phase problem in crystallography. JOSA a, 10(5):1046–1055, 1993.
449"
REFERENCES,0.47678916827852996,"[37] Y. Shechtman, Y. C. Eldar, O. Cohen, H. N. Chapman, J. Miao, and M. Segev. Phase retrieval
450"
REFERENCES,0.47775628626692457,"with application to optical imaging: a contemporary overview. IEEE signal processing magazine,
451"
REFERENCES,0.4787234042553192,"32(3):87–109, 2015.
452"
REFERENCES,0.4796905222437137,"[38] M. Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample
453"
REFERENCES,0.48065764023210833,"complexity barriers via nonconvex optimization. IEEE Trans. Inf. Theory, 65(4):2374–2400,
454"
REFERENCES,0.4816247582205029,"2019.
455"
REFERENCES,0.4825918762088975,"[39] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere i: Overview and the
456"
REFERENCES,0.4835589941972921,"geometric picture. IEEE Trans. Inf. Theor., 63(2):853–884, 2 2017.
457"
REFERENCES,0.48452611218568664,"[40] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. Found. Comput. Math.,
458"
REFERENCES,0.48549323017408125,"18(5):1131–1198, 2018.
459"
REFERENCES,0.4864603481624758,"[41] L. Trevisan. Lecture notes on graph partitioning, expanders and spectral methods. University of
460"
REFERENCES,0.4874274661508704,"California, Berkeley, https://people. eecs. berkeley. edu/luca/books/expanders-2016. pdf, 2017.
461"
REFERENCES,0.488394584139265,"[42] J. Tukey. Mathematics and picturing of data. In Proceedings of ICM, volume 6, pages 523–531,
462"
REFERENCES,0.48936170212765956,"1975.
463"
REFERENCES,0.49032882011605416,"[43] V. Voroninski. PhaseLift: A Novel Methodology for Phase Retrieval. PhD thesis, UC Berkeley,
464"
REFERENCES,0.4912959381044487,"2013.
465"
REFERENCES,0.4922630560928433,"[44] J. Wright and Y. Ma. High-dimensional data analysis with low-dimensional models: Principles,
466"
REFERENCES,0.4932301740812379,"computation, and applications. Cambridge University Press, 2022.
467"
REFERENCES,0.4941972920696325,"[45] H. Zhang, Y. Chi, and Y. Liang. Provable non-convex phase retrieval with outliers: Median
468"
REFERENCES,0.4951644100580271,"truncated wirtinger flow. In International conference on machine learning, pages 1022–1031.
469"
REFERENCES,0.4961315280464217,"PMLR, 2016.
470"
REFERENCES,0.49709864603481624,"A
Omitted Proofs in Section 4
471"
REFERENCES,0.49806576402321084,"Lemma A.1. [Lemma 4.2, Formal].
For any δ0 > 0, there exists constants ϵ0, c > 0 such that
472"
REFERENCES,0.4990328820116054,"when n > c · d log d and we are given a set of n ϵ-corrupted samples, where 0 ≤ϵ ≤ϵ0, then with
473"
REFERENCES,0.5,"probability at least 0.98, it holds
474"
REFERENCES,0.5009671179883946,"∀w ∈∆n,2ϵ,
YG,w −(I + 2xx⊤)

2 ≤δ0 .
(8)"
REFERENCES,0.5019342359767892,"Proof of Lemma A.1. We recall the definition of YG,w = P"
REFERENCES,0.5029013539651838,"i∈G wiyiaia⊤
i . Let ℓ= ϵ · n and let
475"
REFERENCES,0.5038684719535783,"{(an+i, yn+i)}ℓ
i=1 be the set of samples that were removed by the ϵ-corruption adversary. Let
476"
REFERENCES,0.504835589941973,"G′ = G ∪{n + 1, . . . , n + ℓ}, n′ = n + ℓ, and ϵ′ = ϵ/(1 + ϵ). Note that without loss of generality,
477"
REFERENCES,0.5058027079303675,"we can assume that |G| = (1 −ϵ)n and |G′| = (1 −ϵ′)n′ = n.
478"
REFERENCES,0.5067698259187621,"We define a mapping σ : ∆n,2ϵ →∆n′,3ϵ′ such that
479"
REFERENCES,0.5077369439071566,"σ(w)i =
wi
i ∈[n]
0
otherwise .
(9)"
REFERENCES,0.5087040618955513,"In other words, all the weights are the same for the samples with index in the set [n], and are equal to
480"
REFERENCES,0.5096711798839458,"0 for the samples removed by the adversary. We can verify that σ(w) ∈∆n′,3ϵ′ for all w ∈∆n,2ϵ
481"
REFERENCES,0.5106382978723404,"since σ(w)i ≤wi ≤1/(1 −2ϵ)n = 1/(1 −3ϵ′)n′ for all i ∈[n′], and ∥σ(w)∥1 = ∥w∥1 = 1.
482"
REFERENCES,0.511605415860735,"Furthermore, we have YG,w = YG′,σ(w) for all w ∈∆n,2ϵ. We denote with w∗∈∆n′,3ϵ′ the desired
483"
REFERENCES,0.5125725338491296,"uniform weighting of the samples with index in G′, i.e., w∗
i =
1
(1−ϵ′)n′ 1i∈G′.
484"
REFERENCES,0.5135396518375241,"It suffices to show both
YG′,w∗−(I + 2xx⊤)

2 ≤δ0/2 and
YG′,σ(w)−w∗

2 ≤δ0/2.
485"
REFERENCES,0.5145067698259188,"By triangle inequality, for any w ∈∆n,2ϵ, it holds
486"
REFERENCES,0.5154738878143134,"YG′,σ(w) −(I + 2xx⊤)

2 ≤
YG′,w∗−(I + 2xx⊤)

2 +
YG′,σ(w)−w∗

2 ,"
REFERENCES,0.5164410058027079,"Thus, it suffices to show both
YG′,w∗−(I + 2xx⊤)

2 ≤δ0/2 and
YG′,σ(w)−w∗

2 ≤δ0/2.
487"
REFERENCES,0.5174081237911026,"We upper bound the first term. By using the definition of w∗, note that
488"
REFERENCES,0.5183752417794971,"YG′,σ(w) −(I + 2xx⊤)

2 =  X"
REFERENCES,0.5193423597678917,"i∈G′
w∗
i yiaia⊤
i −(I + 2xx⊤) 2 =  X i∈G′"
REFERENCES,0.5203094777562862,"1
|G′|yiaia⊤
i −(I + 2xx⊤) 2
."
REFERENCES,0.5212765957446809,"Since E(yiaia⊤
i ) = I + 2xx⊤for any i ∈G′, we can use a concentration inequality to upper bound
489"
REFERENCES,0.5222437137330754,"this term. By Lemma A.2, as long as n ≥c1(δ0/2) · d log d, with probability at least 0.995, we have
490"
REFERENCES,0.52321083172147,"YG′,w∗−(I + 2xx⊤)

2 ≤δ0/2 .
(10)"
REFERENCES,0.5241779497098646,"It remains to show a high-probability upper bound to the second term
YG′,w∗−σ(w)

2 ≤δ0/2.
491"
REFERENCES,0.5251450676982592,"The first observation is that the weighting w∗and σ(w) for any w ∈∆n,2ϵ cannot too different. In
492"
REFERENCES,0.5261121856866537,"particular, we can show the following upper bound:
493 X"
REFERENCES,0.5270793036750484,"i∈G′
|w∗
i −σ(w)i| ≤ n′
X"
REFERENCES,0.528046421663443,"i=1
|w∗
i −σ(w)i| ≤
sup
w,w′∈∆n′,3ϵ′ n′
X"
REFERENCES,0.5290135396518375,"i=1
|wi −w′
i|
(11)"
REFERENCES,0.5299806576402321,"We observe that ∆n′,3ϵ′ can be seen as the convex combination of all possible uniform weighting
494"
REFERENCES,0.5309477756286267,"over subsets of n′(1 −3ϵ′) samples. Thus, the maximum distance will be between two points of the
495"
REFERENCES,0.5319148936170213,"convex hull, and we can upper bound (11) as:
496 X"
REFERENCES,0.5328820116054158,"i∈G′
|w∗
i −σ(w)i| ≤
sup
w,w′∈∆n′,3ϵ′ n′
X"
REFERENCES,0.5338491295938105,"i=1
|wi −w′
i| ≤
6ϵ′n
n′(1 −3ϵ′) ≤6ϵ .
(12)"
REFERENCES,0.534816247582205,"For a fixed unit vector z ∈Sd−1 with z = px + qu where u ∈Sd−1 and ⟨u, x⟩= 0, we have:
497"
REFERENCES,0.5357833655705996,"max
w∈∆n,2ϵ"
REFERENCES,0.5367504835589942,"z⊤YG′,w∗−σ(w)z
 = max
w  X"
REFERENCES,0.5377176015473888,"i∈G′
(w∗
i −σ(w)i) ⟨ai, x⟩2 ⟨ai, z⟩2"
REFERENCES,0.5386847195357833,"= max
w  X"
REFERENCES,0.539651837524178,"i∈G′
(w∗
i −σ(w)i) ⟨ai, x⟩2 (p ⟨ai, x⟩+ q ⟨ai, u⟩)2"
REFERENCES,0.5406189555125726,"≤2 max
w  X"
REFERENCES,0.5415860735009671,"i∈G′
(w∗
i −σ(w)i)(⟨ai, x⟩4 + ⟨ai, x⟩2 ⟨ai, u⟩2) "
REFERENCES,0.5425531914893617,"≤2 max
w X"
REFERENCES,0.5435203094777563,"i∈G′
|w∗
i −σ(w)i| ⟨ai, x⟩4"
REFERENCES,0.5444874274661509,"+ 2 max
w X"
REFERENCES,0.5454545454545454,"i∈G′
|w∗
i −σ(w)i| ⟨ai, x⟩2 ⟨ai, u⟩2 ."
REFERENCES,0.5464216634429401,"For ease of notation, let βi .= |w∗
i −σ(w)i|. Observe that 0 ≤βi ≤
1
(1−2ϵ)n for all i, and P"
REFERENCES,0.5473887814313346,"i βi ≤6ϵ
498"
REFERENCES,0.5483558994197292,"due to (11). We have:
499"
REFERENCES,0.5493230174081238,"max
w∈∆n,2ϵ"
REFERENCES,0.5502901353965184,"z⊤YG′,w∗−σ(w)z
 ≤2
max
β:P"
REFERENCES,0.5512572533849129,"i∈G′ βi≤6ϵ and 0≤βi≤
1
(1−2ϵ)n X"
REFERENCES,0.5522243713733076,"i∈G′
βi ⟨ai, x⟩4"
REFERENCES,0.5531914893617021,"+ 2
max
β:P"
REFERENCES,0.5541586073500967,"i∈G′ βi≤6ϵ and 0≤βi≤
1
(1−2ϵ)n X"
REFERENCES,0.5551257253384912,"i∈G′
βi ⟨ai, x⟩2 ⟨ai, u⟩2"
REFERENCES,0.5560928433268859,"≤
2
(1 −2ϵ)n
max
L⊆G′,|L|=6ϵn X"
REFERENCES,0.5570599613152805,"i∈L
⟨ai, x⟩4"
REFERENCES,0.558027079303675,"+
2
(1 −2ϵ)n
max
L⊆G′,|L|=6ϵn X"
REFERENCES,0.5589941972920697,"i∈L
⟨ai, x⟩2 ⟨ai, u⟩2 .
(13)"
REFERENCES,0.5599613152804642,"Inequality (13) follows by assigning the maximum possible βi to the largest entries of the sum until
500"
REFERENCES,0.5609284332688588,"we hit the budget 6ϵ due to (11).
501"
REFERENCES,0.5618955512572534,"We bound maxL
P
i∈L ⟨ai, x⟩4 first. Let Xi = ⟨ai, x⟩∼N(0, 1) for i ∈G′, and define the
502"
REFERENCES,0.562862669245648,"threshold function hr(z) =
0,
z ≤r
z,
z > r for r = C2 ·ln2(1/ϵ) with constant C > 0 to be determined.
503"
REFERENCES,0.5638297872340425,"Note that z ≤r + hr(z) for all z > 0. Therefore,
504"
REFERENCES,0.5647969052224371,"max
L⊆G′,|L|=6ϵn
1
n X"
REFERENCES,0.5657640232108317,"i∈L
X4
i ≤max
L
1
n X"
REFERENCES,0.5667311411992263,"i∈L
r + max
L
1
n X"
REFERENCES,0.5676982591876208,"i∈L
hr(X4
i )"
REFERENCES,0.5686653771760155,≤6ϵr + 1 n X
REFERENCES,0.5696324951644101,"i∈G′
hr(X4
i )."
REFERENCES,0.5705996131528046,"Then, we consider to bound exp
 P"
REFERENCES,0.5715667311411993,"i∈G′ c · hr(X4
i )

for some c > 0 to be determined. For any
505"
REFERENCES,0.5725338491295938,"i ∈G′ and z ≥1, with C = 6, for all ϵ > 0, we have
506"
REFERENCES,0.5735009671179884,"Pr

exp
 
c · hr(X4
i )

≥z

≤Pr

hr(X4
i ) ≥0

= Pr
h
Xi ≥r1/4i
≤exp(−√r/2)"
REFERENCES,0.574468085106383,≤exp(ln ϵ · C/2) ≤ϵ3.
REFERENCES,0.5754352030947776,"At the same time, with c < 1/200, for all z ≥1, we have
507"
REFERENCES,0.5764023210831721,"Pr

exp
 
c · hr(X4
i )

≥z

≤Pr

exp(c · X4
i ) ≥z

≤Pr

X4
i ≥ln z c  ≤exp  − r ln z c /2 ! ≤z−3."
REFERENCES,0.5773694390715667,"Therefore, Pr

exp
 
c · hr(X4
i )

≥z

≤min{z−3, ϵ3}, and for all ϵ < 1/2, we have
508"
REFERENCES,0.5783365570599613,"E

exp
 
c · hr(X4
i )

=
Z ∞"
PR,0.5793036750483559,"0
Pr

exp
 
c · hr(X4
i )

≥z

dz ≤
Z 1"
PR,0.5802707930367504,"0
1dz +
Z 1/ϵ"
PR,0.5812379110251451,"1
ϵ3dz +
Z ∞"
PR,0.5822050290135397,"1/ϵ
z−3dz"
PR,0.5831721470019342,= 1 + ϵ2 −ϵ3 + 1 2ϵ2
PR,0.5841392649903289,≤1 + ϵ2
PR,0.5851063829787234,≤exp(ϵ2).
PR,0.586073500967118,"Since {Xi}i∈G′ are independent, we have
509 E "" exp X"
PR,0.5870406189555126,"i∈G′
c · hr(X4
i ) !# = E "" Y"
PR,0.5880077369439072,"i∈G′
exp
 
c · hr(X4
i )

#"
PR,0.5889748549323017,≤exp(ϵ2n).
PR,0.5899419729206963,"Finally, by Markov’s inequality, for any constant δ1 > 0, as long as ϵ ≤√δ1c, we have
510 Pr ""X"
PR,0.5909090909090909,"i∈G′
hr(X4
i ) ≥2δ1n # = Pr "" exp X"
PR,0.5918762088974855,"i∈G′
c · hr(X4
i ) !"
PR,0.59284332688588,≥exp(2δ1cn) #
PR,0.5938104448742747,≤exp(ϵ2n −2δ1cn) ≤exp(−δ1cn).
PR,0.5947775628626693,"As a result, with sufficiently large n ≥c2(δ1) · d log d and sufficiently small ϵ such that 6ϵr ≤δ1,
511 Pr """
PR,0.5957446808510638,ϵr + 1 n X
PR,0.5967117988394585,"i∈G′
hr(X4
i ) ≥3δ1 #"
PR,0.597678916827853,≤exp(−δ1cn) ≤0.995 · 9−d.
PR,0.5986460348162476,"Also, maxL
P"
PR,0.5996131528046421,"i∈L ⟨ai, x⟩2 ⟨ai, u⟩2 has a similar tail bound and therefore can be bounded in the same
512"
PR,0.6005802707930368,"way. We can then bound the operator norm via an epsilon-net argument. Set δ1 = δ0/24. By an
513"
PR,0.6015473887814313,"1/4-net on Sd−1 with |N| ≤9d, we have that
514"
PR,0.6025145067698259,"Pr

max
z∈Sd−1
z⊤YG′,w∗−σ(w)z
 ≥δ0/2

≤9d · 0.99 · 9−d ≤0.99."
PR,0.6034816247582205,"By combining the above inequality with Equation (10), we can conclude that, for any δ0 > 0, there
515"
PR,0.6044487427466151,"exists ϵ0 > 0, such that when n ≥max{c1(δ0), c2(δ0)} · d log d and 0 ≤ϵ ≤ϵ0, with probability at
516"
PR,0.6054158607350096,"least 0.98,
517"
PR,0.6063829787234043,"∀w ∈∆n,2ϵ,
YG,w −(I + 2xx⊤)

2 =
YG′,σ(w) −(I + 2xx⊤)

2 ≤δ0."
PR,0.6073500967117988,"Therefore, with probability at least 0.98, we have
YG,w −(I + 2xx⊤)

2 ≤δ0 for all w ∈∆n,2ϵ.
518 519"
PR,0.6083172147001934,"A.1
Concentration Inequalities
520"
PR,0.6092843326885881,"For the undisturbed samples, we have the following concentration result.
521"
PR,0.6102514506769826,"Lemma A.2 ([3] Section A.4.2). Let x ∈Rd. For any δ > 0, there exists a constant C(δ) > 0
522"
PR,0.6112185686653772,"such that when n > C · d log d and we are given a set of n samples {(ai, yi)}n
i=1 with ai ∼N(0, I)
523"
PR,0.6121856866537717,"independently and yi = ⟨ai, x⟩2 for all i ∈[n], then with probability at least 0.99, it holds
524"
N,0.6131528046421664,"1
n n
X"
N,0.6141199226305609,"i=1
yiaia⊤
i −(I + 2xx⊤) 2
≤δ."
N,0.6150870406189555,"B
Omitted Proofs in Section 5
525"
N,0.6160541586073501,"Lemma 5.1. Let Dz be the distribution of gradients at z as defined in Equation (7). For any z ∈Rd
526"
N,0.6170212765957447,"with ∥z −x∥2 ≤1, the covariance matrix Σz of Dz satisfies Σz ⪯O(∥z −x∥2
2)I.
527"
N,0.6179883945841392,"Proof of Lemma 5.1. Recall that g ∼Dz is distributed as
528 g = ∂ ∂z"
N,0.6189555125725339,"h 
⟨a, z⟩2 −⟨a, x⟩22i"
N,0.6199226305609284,"= −4
 
⟨a, z⟩2 −⟨a, x⟩2
⟨a, z⟩a
where a ∼N(0, 1) ."
N,0.620889748549323,"Because Eg∼Dz [g] = µz, the spectral norm of Σz can be upper bounded as follows:
529"
N,0.6218568665377177,"∥Σz∥2 =
 E
g∼Dz"
N,0.6228239845261122,"
gg⊤
−µzµ⊤
z"
N,0.6237911025145068,"2
≤
 E
g∼Dz"
N,0.6247582205029013,"
gg⊤
2
."
N,0.625725338491296,"Consequently, it suffices to upper bound
Eg∼Dz

gg⊤
2. Let h = z −x.
530"
N,0.6266924564796905,"E
g∼Dz"
N,0.6276595744680851,"
gg⊤
2
= max
∥v∥2=1 v⊤
E
g∼Dz"
N,0.6286266924564797,"
gg⊤
v = max
∥v∥2=1 E
g"
N,0.6295938104448743,"h
⟨g, v⟩2i"
N,0.6305609284332688,"= 16 max
∥v∥2=1
E
a∼N(0,I)"
N,0.6315280464216635,"h
(⟨a, z⟩2 −⟨a, x⟩2)2 ⟨a, z⟩2 ⟨a, v⟩2i"
N,0.632495164410058,"= 16 max
∥v∥2=1
E
a∼N(0,I)"
N,0.6334622823984526,"h
⟨a, h⟩2 ⟨a, 2x + h⟩2 ⟨a, x + h⟩2 ⟨a, v⟩2i"
N,0.6344294003868471,"≤16 max
∥v∥2=1  E
a"
N,0.6353965183752418,"h
⟨a, h⟩8i
E
a"
N,0.6363636363636364,"h
⟨a, 2x + h⟩8i
E
a"
N,0.6373307543520309,"h
⟨a, x + h⟩8i
E
a"
N,0.6382978723404256,"h
⟨a, v⟩8i1/4"
N,0.6392649903288201,"= 16 max
∥v∥2=1"
N,0.6402321083172147,"
1054 ∥h∥8
2 ∥2x + h∥8
2 ∥x + h∥8
2 ∥v∥8
2
1/4"
N,0.6411992263056093,"= (16 · 105) ∥h∥2
2 ∥2x + h∥2
2 ∥x + h∥2
2 = O(∥h∥2
2) ."
N,0.6421663442940039,"The first inequality is due to Cauchy-Schwarz inequality. The last step uses the fact that ∥x∥2 = 1
531"
N,0.6431334622823984,"and ∥h∥2 ≤1.
532"
N,0.6441005802707931,"Lemma 5.3. Suppose in iteration t of Algorithm 2, the current solution zt satisfies ∥zt −x∥2 ≤1/8,
533"
N,0.6450676982591876,"and the estimated gradient bµzt ∈Rd satisfies ∥bµzt −µzt∥2 ≤c ∥zt −x∥2 for c = 4. Then, we have
534"
N,0.6460348162475822,"∥zt+1 −x∥2
2 ≤0.99 ∥zt −x∥2
2.
535"
N,0.6470019342359767,"Proof of Lemma 5.3. Recall that g ∼Dz is distributed as
536 g = ∂ ∂z"
N,0.6479690522243714,"h 
⟨a, z⟩2 −⟨a, x⟩22i
where a ∼N(0, I) ."
N,0.648936170212766,"We can compute the mean of Dz using moments of Gaussian:
537"
N,0.6499032882011605,"µz =
E
g∼Dz g =

12 ∥z∥2
2 −4 ∥x∥2
2

z −8⟨x, z⟩x .
(14)"
N,0.6508704061895552,"Consider one step of gradient descent in Algorithm 2: zt+1 = zt −ηbµzt, where bµzt is an approximate
538"
N,0.6518375241779497,"gradient. We have
539"
N,0.6528046421663443,"∥zt+1 −x∥2
2 = ∥zt −ηbµzt −x∥2
2 = ∥zt −x∥2
2 −2η⟨bµzt, zt −x⟩+ η2⟨bµzt, bµzt⟩"
N,0.6537717601547389,"To prove convergence, we need to lower bound ⟨bµzt, zt −x⟩and upper bound ⟨bµzt, bµzt⟩.
540"
N,0.6547388781431335,"We write z = zt and h = z −x to simplify notation. We can substitute z = x + h in Equation (14):
541"
N,0.655705996131528,"µz =

16⟨x, h⟩+ 12 ∥h∥2
2

x +

8 ∥x∥2
2 + 24⟨x, h⟩+ 12 ∥h∥2
2

h ."
N,0.6566731141199227,"Recall the assumptions of this lemma: ∥x∥2 = 1, ∥h∥2 ≤1/8, and ∥bµz −µz∥2 ≤c ∥h∥2.
542"
N,0.6576402321083172,"First we lower bound ⟨bµz, h⟩.
543"
N,0.6586073500967118,"⟨bµz, h⟩= ⟨µz, h⟩+ ⟨bµz −µz, h⟩"
N,0.6595744680851063,"= 16⟨x, h⟩2 + 36⟨x, h⟩∥h∥2
2 + 8 ∥x∥2
2 ∥h∥2
2 + 12 ∥h∥4
2 + ⟨bµz −µz, h⟩ ≥−81"
N,0.660541586073501,"4 ∥h∥4
2 + 8 ∥x∥2
2 ∥h∥2
2 + 12 ∥h∥4
2 −⟨bµz −µz, h⟩"
N,0.6615087040618955,≥(8 −33
N,0.6624758220502901,"256 −c) ∥h∥2
2
≥(7.5 −c) ∥h∥2
2 ."
N,0.6634429400386848,"The first inequality uses the fact that 16⟨x, h⟩2 + 36⟨x, h⟩∥h∥2
2 is a second-order polynomial of
544"
N,0.6644100580270793,"⟨x, h⟩, which has minimum value −81"
N,0.6653771760154739,"4 ∥h∥4
2 for all ⟨x, h⟩∈R.
545"
N,0.6663442940038685,"Next we upper bound ⟨bµz, bµz⟩using the triangle inequality.
546"
N,0.6673114119922631,∥bµz∥2 ≤∥µz∥2 + ∥bµz −µz∥2
N,0.6682785299806576,"≤

16⟨x, h⟩+ 12 ∥h∥2
2

∥x∥2 +

8 ∥x∥2
2 + 24⟨x, h⟩+ 12 ∥h∥2
2

∥h∥2 + c ∥h∥2"
N,0.6692456479690522,"≤
 
16 + 12"
N,0.6702127659574468,8 + 8 + 24
N,0.6711798839458414,8 + 12
N,0.6721470019342359,"64 + c

∥h∥2
≤(29 + c) ∥h∥2 ."
N,0.6731141199226306,"Putting everything together, we have
547"
N,0.6740812379110251,"∥zt+1 −x∥2
2 = ∥zt −x∥2
2 −2η⟨bµzt, zt −x⟩+ η2⟨bµzt, bµzt⟩"
N,0.6750483558994197,"≤

1 −2(7.5 −c)η + (29 + c)2η2
∥zt −x∥2
2 ."
N,0.6760154738878144,"Choosing c = 4 and η = 1/300 gives that ∥zt+1 −x∥2
2 ≤0.99 ∥zt −x∥2
2.
548"
N,0.6769825918762089,"C
Counter-examples
549"
N,0.6779497098646035,"Prior robust phase retrieval algorithms [24, 45] focus on the setting where the observations yi’s are sub-
550"
N,0.6789168278529981,"ject to adversarial perturbation while the measuring vectors ai’s are independently sampled from the
551"
N,0.6798839458413927,"Gaussian distribution. The Median Truncated Wirtinger Flow Algorithm [45] first initialize z(0) by the
552"
N,0.6808510638297872,"spectral method, calculating z(0) as the top eigenvector of Y := 1"
N,0.6818181818181818,"m
Pm
i=1 yiaia⊤
i 1|yi|≤α2 med({yi}m
i=1)
553"
N,0.6827852998065764,"using a truncated set of samples, where the threshold is determined by med({yi}m
i=1), the median
554"
N,0.683752417794971,"over all yi’s. As long as the fraction of of outliers is not too large and the sample complexity is large
555"
N,0.6847195357833655,"enough, the initialization is guaranteed to be within a small neighborhood of the ground truth.
556"
N,0.6856866537717602,"In this section, we present a counter-example where robust phase retrieval algorithms [24, 45] can be
557"
N,0.6866537717601547,"insufficient when directly applied to the ϵ-corruption phase retrieval problem.
558"
N,0.6876208897485493,"Let x ∈Sd−1 be the ground truth unit vector. Here we construct an ϵ-corruption adversary that can
559"
N,0.688588007736944,"manipulate the top eigenvector of the empirical covariance matrix Y = Pn
i=1 yiaia⊤
i , even when yi
560"
N,0.6895551257253385,"are accurately calculated as yi = (a⊤
i x)2.
561"
N,0.690522243713733,"Let u ∈Sd−1 be a unit vector such that x⊤u = 0. Suppose the adversary changes 1% of the ai’s
562"
N,0.6914893617021277,"to ai =
p"
N,0.6924564796905223,"d −1/25 · u + 1/5x, and suppose all the yi’s are accurate. In particular, the length of
563"
N,0.6934235976789168,"the corrupted ai’s is comparable to Gaussian vectors, and the corresponding yi = (a⊤
i x)2 = 1/25.
564"
N,0.6943907156673114,"Consequently, the median-truncated initialization in [45] will not be able to filter out such yi. However,
565"
N,0.695357833655706,"the top eigenvector of E [Y ] = E
Pn
i=1 yiaia⊤
i

= O(d)uu⊤+ O(
√"
N,0.6963249516441006,"d)(ux⊤+ xu⊤) + O(1)(I +
566"
N,0.6972920696324951,"2xx⊤) will be manipulated to u, which is far from the ground truth x.
567"
N,0.6982591876208898,"NeurIPS Paper Checklist
568"
CLAIMS,0.6992263056092843,"1. Claims
569"
CLAIMS,0.7001934235976789,"Question: Do the main claims made in the abstract and introduction accurately reflect the
570"
CLAIMS,0.7011605415860735,"paper’s contributions and scope?
571"
CLAIMS,0.7021276595744681,"Answer: [Yes]
572"
CLAIMS,0.7030947775628626,"Justification: We provide a formal discussion of our theoretical claim in the first section of
573"
CLAIMS,0.7040618955512572,"the paper, and include a formal statement of our resul in Section 3.
574"
CLAIMS,0.7050290135396519,"Guidelines:
575"
CLAIMS,0.7059961315280464,"• The answer NA means that the abstract and introduction do not include the claims
576"
CLAIMS,0.706963249516441,"made in the paper.
577"
CLAIMS,0.7079303675048356,"• The abstract and/or introduction should clearly state the claims made, including the
578"
CLAIMS,0.7088974854932302,"contributions made in the paper and important assumptions and limitations. A No or
579"
CLAIMS,0.7098646034816247,"NA answer to this question will not be perceived well by the reviewers.
580"
CLAIMS,0.7108317214700194,"• The claims made should match theoretical and experimental results, and reflect how
581"
CLAIMS,0.7117988394584139,"much the results can be expected to generalize to other settings.
582"
CLAIMS,0.7127659574468085,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
583"
CLAIMS,0.7137330754352031,"are not attained by the paper.
584"
LIMITATIONS,0.7147001934235977,"2. Limitations
585"
LIMITATIONS,0.7156673114119922,"Question: Does the paper discuss the limitations of the work performed by the authors?
586"
LIMITATIONS,0.7166344294003868,"Answer: [Yes]
587"
LIMITATIONS,0.7176015473887815,"Justification: We point out to all the assumptions used in our paper. There are a discussion
588"
LIMITATIONS,0.718568665377176,"of the limitations in the conclusion. Our algorithm can only handle a corruption ϵ < ϵ0 for
589"
LIMITATIONS,0.7195357833655706,"some universal constant ϵ0. Additionally, it is possible that the dependency on ∆for the
590"
LIMITATIONS,0.7205029013539652,"sample complexity can be removed.
591"
LIMITATIONS,0.7214700193423598,"Guidelines:
592"
LIMITATIONS,0.7224371373307543,"• The answer NA means that the paper has no limitation while the answer No means that
593"
LIMITATIONS,0.723404255319149,"the paper has limitations, but those are not discussed in the paper.
594"
LIMITATIONS,0.7243713733075435,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
595"
LIMITATIONS,0.7253384912959381,"• The paper should point out any strong assumptions and how robust the results are to
596"
LIMITATIONS,0.7263056092843327,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
597"
LIMITATIONS,0.7272727272727273,"model well-specification, asymptotic approximations only holding locally). The authors
598"
LIMITATIONS,0.7282398452611218,"should reflect on how these assumptions might be violated in practice and what the
599"
LIMITATIONS,0.7292069632495164,"implications would be.
600"
LIMITATIONS,0.730174081237911,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
601"
LIMITATIONS,0.7311411992263056,"only tested on a few datasets or with a few runs. In general, empirical results often
602"
LIMITATIONS,0.7321083172147002,"depend on implicit assumptions, which should be articulated.
603"
LIMITATIONS,0.7330754352030948,"• The authors should reflect on the factors that influence the performance of the approach.
604"
LIMITATIONS,0.7340425531914894,"For example, a facial recognition algorithm may perform poorly when image resolution
605"
LIMITATIONS,0.7350096711798839,"is low or images are taken in low lighting. Or a speech-to-text system might not be
606"
LIMITATIONS,0.7359767891682786,"used reliably to provide closed captions for online lectures because it fails to handle
607"
LIMITATIONS,0.7369439071566731,"technical jargon.
608"
LIMITATIONS,0.7379110251450677,"• The authors should discuss the computational efficiency of the proposed algorithms
609"
LIMITATIONS,0.7388781431334622,"and how they scale with dataset size.
610"
LIMITATIONS,0.7398452611218569,"• If applicable, the authors should discuss possible limitations of their approach to
611"
LIMITATIONS,0.7408123791102514,"address problems of privacy and fairness.
612"
LIMITATIONS,0.741779497098646,"• While the authors might fear that complete honesty about limitations might be used by
613"
LIMITATIONS,0.7427466150870407,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
614"
LIMITATIONS,0.7437137330754352,"limitations that aren’t acknowledged in the paper. The authors should use their best
615"
LIMITATIONS,0.7446808510638298,"judgment and recognize that individual actions in favor of transparency play an impor-
616"
LIMITATIONS,0.7456479690522244,"tant role in developing norms that preserve the integrity of the community. Reviewers
617"
LIMITATIONS,0.746615087040619,"will be specifically instructed to not penalize honesty concerning limitations.
618"
THEORY ASSUMPTIONS AND PROOFS,0.7475822050290135,"3. Theory Assumptions and Proofs
619"
THEORY ASSUMPTIONS AND PROOFS,0.7485493230174082,"Question: For each theoretical result, does the paper provide the full set of assumptions and
620"
THEORY ASSUMPTIONS AND PROOFS,0.7495164410058027,"a complete (and correct) proof?
621"
THEORY ASSUMPTIONS AND PROOFS,0.7504835589941973,"Answer: [Yes]
622"
THEORY ASSUMPTIONS AND PROOFS,0.7514506769825918,"Justification: We formally describe our problem setting and assumption used. All the proofs
623"
THEORY ASSUMPTIONS AND PROOFS,0.7524177949709865,"are in the paper, either in the main content pages or in the appendix.
624"
THEORY ASSUMPTIONS AND PROOFS,0.753384912959381,"Guidelines:
625"
THEORY ASSUMPTIONS AND PROOFS,0.7543520309477756,"• The answer NA means that the paper does not include theoretical results.
626"
THEORY ASSUMPTIONS AND PROOFS,0.7553191489361702,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
627"
THEORY ASSUMPTIONS AND PROOFS,0.7562862669245648,"referenced.
628"
THEORY ASSUMPTIONS AND PROOFS,0.7572533849129593,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
629"
THEORY ASSUMPTIONS AND PROOFS,0.758220502901354,"• The proofs can either appear in the main paper or the supplemental material, but if
630"
THEORY ASSUMPTIONS AND PROOFS,0.7591876208897486,"they appear in the supplemental material, the authors are encouraged to provide a short
631"
THEORY ASSUMPTIONS AND PROOFS,0.7601547388781431,"proof sketch to provide intuition.
632"
THEORY ASSUMPTIONS AND PROOFS,0.7611218568665378,"• Inversely, any informal proof provided in the core of the paper should be complemented
633"
THEORY ASSUMPTIONS AND PROOFS,0.7620889748549323,"by formal proofs provided in appendix or supplemental material.
634"
THEORY ASSUMPTIONS AND PROOFS,0.7630560928433269,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7640232108317214,"4. Experimental Result Reproducibility
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7649903288201161,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7659574468085106,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7669245647969052,"of the paper (regardless of whether the code and data are provided or not)?
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7678916827852998,"Answer: [NA]
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7688588007736944,"Justification: The paper is a theory paper, and it has no experiments.
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.769825918762089,"Guidelines:
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7707930367504836,"• The answer NA means that the paper does not include experiments.
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7717601547388782,"• If the paper includes experiments, a No answer to this question will not be perceived
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7727272727272727,"well by the reviewers: Making the paper reproducible is important, regardless of
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7736943907156673,"whether the code and data are provided or not.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746615087040619,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7756286266924565,"to make their results reproducible or verifiable.
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.776595744680851,"• Depending on the contribution, reproducibility can be accomplished in various ways.
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775628626692457,"For example, if the contribution is a novel architecture, describing the architecture fully
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785299806576402,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7794970986460348,"be necessary to either make it possible for others to replicate the model with the same
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7804642166344294,"dataset, or provide access to the model. In general. releasing code and data is often
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.781431334622824,"one good way to accomplish this, but reproducibility can also be provided via detailed
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7823984526112185,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7833655705996132,"of a large language model), releasing of a model checkpoint, or other means that are
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7843326885880078,"appropriate to the research performed.
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852998065764023,"• While NeurIPS does not require releasing code, the conference does require all submis-
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7862669245647969,"sions to provide some reasonable avenue for reproducibility, which may depend on the
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7872340425531915,"nature of the contribution. For example
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7882011605415861,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7891682785299806,"to reproduce that algorithm.
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7901353965183753,"(b) If the contribution is primarily a new model architecture, the paper should describe
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7911025145067698,"the architecture clearly and fully.
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7920696324951644,"(c) If the contribution is a new model (e.g., a large language model), then there should
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.793036750483559,"either be a way to access this model for reproducing the results or a way to reproduce
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7940038684719536,"the model (e.g., with an open-source dataset or instructions for how to construct
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7949709864603481,"the dataset).
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7959381044487428,"(d) We recognize that reproducibility may be tricky in some cases, in which case
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7969052224371374,"authors are welcome to describe the particular way they provide for reproducibility.
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7978723404255319,"In the case of closed-source models, it may be that access to the model is limited in
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7988394584139265,"some way (e.g., to registered users), but it should be possible for other researchers
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7998065764023211,"to have some path to reproducing or verifying the results.
673"
OPEN ACCESS TO DATA AND CODE,0.8007736943907157,"5. Open access to data and code
674"
OPEN ACCESS TO DATA AND CODE,0.8017408123791102,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
675"
OPEN ACCESS TO DATA AND CODE,0.8027079303675049,"tions to faithfully reproduce the main experimental results, as described in supplemental
676"
OPEN ACCESS TO DATA AND CODE,0.8036750483558994,"material?
677"
OPEN ACCESS TO DATA AND CODE,0.804642166344294,"Answer: [NA]
678"
OPEN ACCESS TO DATA AND CODE,0.8056092843326886,"Justification: The paper is a theory paper, and it has no experiments.
679"
OPEN ACCESS TO DATA AND CODE,0.8065764023210832,"Guidelines:
680"
OPEN ACCESS TO DATA AND CODE,0.8075435203094777,"• The answer NA means that paper does not include experiments requiring code.
681"
OPEN ACCESS TO DATA AND CODE,0.8085106382978723,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
682"
OPEN ACCESS TO DATA AND CODE,0.809477756286267,"public/guides/CodeSubmissionPolicy) for more details.
683"
OPEN ACCESS TO DATA AND CODE,0.8104448742746615,"• While we encourage the release of code and data, we understand that this might not be
684"
OPEN ACCESS TO DATA AND CODE,0.811411992263056,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
685"
OPEN ACCESS TO DATA AND CODE,0.8123791102514507,"including code, unless this is central to the contribution (e.g., for a new open-source
686"
OPEN ACCESS TO DATA AND CODE,0.8133462282398453,"benchmark).
687"
OPEN ACCESS TO DATA AND CODE,0.8143133462282398,"• The instructions should contain the exact command and environment needed to run to
688"
OPEN ACCESS TO DATA AND CODE,0.8152804642166345,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
689"
OPEN ACCESS TO DATA AND CODE,0.816247582205029,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
690"
OPEN ACCESS TO DATA AND CODE,0.8172147001934236,"• The authors should provide instructions on data access and preparation, including how
691"
OPEN ACCESS TO DATA AND CODE,0.8181818181818182,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
692"
OPEN ACCESS TO DATA AND CODE,0.8191489361702128,"• The authors should provide scripts to reproduce all experimental results for the new
693"
OPEN ACCESS TO DATA AND CODE,0.8201160541586073,"proposed method and baselines. If only a subset of experiments are reproducible, they
694"
OPEN ACCESS TO DATA AND CODE,0.8210831721470019,"should state which ones are omitted from the script and why.
695"
OPEN ACCESS TO DATA AND CODE,0.8220502901353965,"• At submission time, to preserve anonymity, the authors should release anonymized
696"
OPEN ACCESS TO DATA AND CODE,0.8230174081237911,"versions (if applicable).
697"
OPEN ACCESS TO DATA AND CODE,0.8239845261121856,"• Providing as much information as possible in supplemental material (appended to the
698"
OPEN ACCESS TO DATA AND CODE,0.8249516441005803,"paper) is recommended, but including URLs to data and code is permitted.
699"
OPEN ACCESS TO DATA AND CODE,0.8259187620889749,"6. Experimental Setting/Details
700"
OPEN ACCESS TO DATA AND CODE,0.8268858800773694,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
701"
OPEN ACCESS TO DATA AND CODE,0.8278529980657641,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
702"
OPEN ACCESS TO DATA AND CODE,0.8288201160541586,"results?
703"
OPEN ACCESS TO DATA AND CODE,0.8297872340425532,"Answer: [NA]
704"
OPEN ACCESS TO DATA AND CODE,0.8307543520309478,"Justification: The paper is a theory paper, and it has no experiments.
705"
OPEN ACCESS TO DATA AND CODE,0.8317214700193424,"Guidelines:
706"
OPEN ACCESS TO DATA AND CODE,0.8326885880077369,"• The answer NA means that the paper does not include experiments.
707"
OPEN ACCESS TO DATA AND CODE,0.8336557059961315,"• The experimental setting should be presented in the core of the paper to a level of detail
708"
OPEN ACCESS TO DATA AND CODE,0.8346228239845261,"that is necessary to appreciate the results and make sense of them.
709"
OPEN ACCESS TO DATA AND CODE,0.8355899419729207,"• The full details can be provided either with the code, in appendix, or as supplemental
710"
OPEN ACCESS TO DATA AND CODE,0.8365570599613152,"material.
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8375241779497099,"7. Experiment Statistical Significance
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8384912959381045,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.839458413926499,"information about the statistical significance of the experiments?
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8404255319148937,"Answer: [NA]
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8413926499032882,"Justification: The paper is a theory paper, and it has no experiments.
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8423597678916828,"Guidelines:
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8433268858800773,"• The answer NA means that the paper does not include experiments.
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.844294003868472,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8452611218568665,"dence intervals, or statistical significance tests, at least for the experiments that support
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8462282398452611,"the main claims of the paper.
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8471953578336557,"• The factors of variability that the error bars are capturing should be clearly stated (for
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8481624758220503,"example, train/test split, initialization, random drawing of some parameter, or overall
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8491295938104448,"run with given experimental conditions).
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8500967117988395,"• The method for calculating the error bars should be explained (closed form formula,
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.851063829787234,"call to a library function, bootstrap, etc.)
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8520309477756286,"• The assumptions made should be given (e.g., Normally distributed errors).
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8529980657640233,"• It should be clear whether the error bar is the standard deviation or the standard error
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8539651837524178,"of the mean.
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8549323017408124,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8558994197292069,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8568665377176016,"of Normality of errors is not verified.
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8578336557059961,"• For asymmetric distributions, the authors should be careful not to show in tables or
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8588007736943907,"figures symmetric error bars that would yield results that are out of range (e.g. negative
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8597678916827853,"error rates).
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8607350096711799,"• If error bars are reported in tables or plots, The authors should explain in the text how
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8617021276595744,"they were calculated and reference the corresponding figures or tables in the text.
737"
EXPERIMENTS COMPUTE RESOURCES,0.8626692456479691,"8. Experiments Compute Resources
738"
EXPERIMENTS COMPUTE RESOURCES,0.8636363636363636,"Question: For each experiment, does the paper provide sufficient information on the com-
739"
EXPERIMENTS COMPUTE RESOURCES,0.8646034816247582,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
740"
EXPERIMENTS COMPUTE RESOURCES,0.8655705996131529,"the experiments?
741"
EXPERIMENTS COMPUTE RESOURCES,0.8665377176015474,"Answer: [NA]
742"
EXPERIMENTS COMPUTE RESOURCES,0.867504835589942,"Justification: The paper is a theory paper, and it has no experiments.
743"
EXPERIMENTS COMPUTE RESOURCES,0.8684719535783365,"Guidelines:
744"
EXPERIMENTS COMPUTE RESOURCES,0.8694390715667312,"• The answer NA means that the paper does not include experiments.
745"
EXPERIMENTS COMPUTE RESOURCES,0.8704061895551257,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
746"
EXPERIMENTS COMPUTE RESOURCES,0.8713733075435203,"or cloud provider, including relevant memory and storage.
747"
EXPERIMENTS COMPUTE RESOURCES,0.8723404255319149,"• The paper should provide the amount of compute required for each of the individual
748"
EXPERIMENTS COMPUTE RESOURCES,0.8733075435203095,"experimental runs as well as estimate the total compute.
749"
EXPERIMENTS COMPUTE RESOURCES,0.874274661508704,"• The paper should disclose whether the full research project required more compute
750"
EXPERIMENTS COMPUTE RESOURCES,0.8752417794970987,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
751"
EXPERIMENTS COMPUTE RESOURCES,0.8762088974854932,"didn’t make it into the paper).
752"
CODE OF ETHICS,0.8771760154738878,"9. Code Of Ethics
753"
CODE OF ETHICS,0.8781431334622823,"Question: Does the research conducted in the paper conform, in every respect, with the
754"
CODE OF ETHICS,0.879110251450677,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
755"
CODE OF ETHICS,0.8800773694390716,"Answer: [Yes]
756"
CODE OF ETHICS,0.8810444874274661,"Justification: The paper conforms with the NeurIPS Code of Ethics.
757"
CODE OF ETHICS,0.8820116054158608,"Guidelines:
758"
CODE OF ETHICS,0.8829787234042553,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
759"
CODE OF ETHICS,0.8839458413926499,"• If the authors answer No, they should explain the special circumstances that require a
760"
CODE OF ETHICS,0.8849129593810445,"deviation from the Code of Ethics.
761"
CODE OF ETHICS,0.8858800773694391,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
762"
CODE OF ETHICS,0.8868471953578336,"eration due to laws or regulations in their jurisdiction).
763"
BROADER IMPACTS,0.8878143133462283,"10. Broader Impacts
764"
BROADER IMPACTS,0.8887814313346228,"Question: Does the paper discuss both potential positive societal impacts and negative
765"
BROADER IMPACTS,0.8897485493230174,"societal impacts of the work performed?
766"
BROADER IMPACTS,0.8907156673114119,"Answer: [NA]
767"
BROADER IMPACTS,0.8916827852998066,"Justification: The paper provides a theoretical result. We do not believe there are potential
768"
BROADER IMPACTS,0.8926499032882012,"societal consequences of our work, aside from advancing the field of Machine Learning.
769"
BROADER IMPACTS,0.8936170212765957,"Guidelines:
770"
BROADER IMPACTS,0.8945841392649904,"• The answer NA means that there is no societal impact of the work performed.
771"
BROADER IMPACTS,0.8955512572533849,"• If the authors answer NA or No, they should explain why their work has no societal
772"
BROADER IMPACTS,0.8965183752417795,"impact or why the paper does not address societal impact.
773"
BROADER IMPACTS,0.8974854932301741,"• Examples of negative societal impacts include potential malicious or unintended uses
774"
BROADER IMPACTS,0.8984526112185687,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
775"
BROADER IMPACTS,0.8994197292069632,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
776"
BROADER IMPACTS,0.9003868471953579,"groups), privacy considerations, and security considerations.
777"
BROADER IMPACTS,0.9013539651837524,"• The conference expects that many papers will be foundational research and not tied
778"
BROADER IMPACTS,0.902321083172147,"to particular applications, let alone deployments. However, if there is a direct path to
779"
BROADER IMPACTS,0.9032882011605415,"any negative applications, the authors should point it out. For example, it is legitimate
780"
BROADER IMPACTS,0.9042553191489362,"to point out that an improvement in the quality of generative models could be used to
781"
BROADER IMPACTS,0.9052224371373307,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
782"
BROADER IMPACTS,0.9061895551257253,"that a generic algorithm for optimizing neural networks could enable people to train
783"
BROADER IMPACTS,0.90715667311412,"models that generate Deepfakes faster.
784"
BROADER IMPACTS,0.9081237911025145,"• The authors should consider possible harms that could arise when the technology is
785"
BROADER IMPACTS,0.9090909090909091,"being used as intended and functioning correctly, harms that could arise when the
786"
BROADER IMPACTS,0.9100580270793037,"technology is being used as intended but gives incorrect results, and harms following
787"
BROADER IMPACTS,0.9110251450676983,"from (intentional or unintentional) misuse of the technology.
788"
BROADER IMPACTS,0.9119922630560928,"• If there are negative societal impacts, the authors could also discuss possible mitigation
789"
BROADER IMPACTS,0.9129593810444874,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
790"
BROADER IMPACTS,0.913926499032882,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
791"
BROADER IMPACTS,0.9148936170212766,"feedback over time, improving the efficiency and accessibility of ML).
792"
SAFEGUARDS,0.9158607350096711,"11. Safeguards
793"
SAFEGUARDS,0.9168278529980658,"Question: Does the paper describe safeguards that have been put in place for responsible
794"
SAFEGUARDS,0.9177949709864603,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
795"
SAFEGUARDS,0.9187620889748549,"image generators, or scraped datasets)?
796"
SAFEGUARDS,0.9197292069632496,"Answer: [NA]
797"
SAFEGUARDS,0.9206963249516441,"Justification: The paper only contains theoretical results.
798"
SAFEGUARDS,0.9216634429400387,"Guidelines:
799"
SAFEGUARDS,0.9226305609284333,"• The answer NA means that the paper poses no such risks.
800"
SAFEGUARDS,0.9235976789168279,"• Released models that have a high risk for misuse or dual-use should be released with
801"
SAFEGUARDS,0.9245647969052224,"necessary safeguards to allow for controlled use of the model, for example by requiring
802"
SAFEGUARDS,0.925531914893617,"that users adhere to usage guidelines or restrictions to access the model or implementing
803"
SAFEGUARDS,0.9264990328820116,"safety filters.
804"
SAFEGUARDS,0.9274661508704062,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
805"
SAFEGUARDS,0.9284332688588007,"should describe how they avoided releasing unsafe images.
806"
SAFEGUARDS,0.9294003868471954,"• We recognize that providing effective safeguards is challenging, and many papers do
807"
SAFEGUARDS,0.9303675048355899,"not require this, but we encourage authors to take this into account and make a best
808"
SAFEGUARDS,0.9313346228239845,"faith effort.
809"
LICENSES FOR EXISTING ASSETS,0.9323017408123792,"12. Licenses for existing assets
810"
LICENSES FOR EXISTING ASSETS,0.9332688588007737,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
811"
LICENSES FOR EXISTING ASSETS,0.9342359767891683,"the paper, properly credited and are the license and terms of use explicitly mentioned and
812"
LICENSES FOR EXISTING ASSETS,0.9352030947775629,"properly respected?
813"
LICENSES FOR EXISTING ASSETS,0.9361702127659575,"Answer: [NA]
814"
LICENSES FOR EXISTING ASSETS,0.937137330754352,"Justification: The paper only contains theoretical results. Previous work is properly cited.
815"
LICENSES FOR EXISTING ASSETS,0.9381044487427466,"Guidelines:
816"
LICENSES FOR EXISTING ASSETS,0.9390715667311412,"• The answer NA means that the paper does not use existing assets.
817"
LICENSES FOR EXISTING ASSETS,0.9400386847195358,"• The authors should cite the original paper that produced the code package or dataset.
818"
LICENSES FOR EXISTING ASSETS,0.9410058027079303,"• The authors should state which version of the asset is used and, if possible, include a
819"
LICENSES FOR EXISTING ASSETS,0.941972920696325,"URL.
820"
LICENSES FOR EXISTING ASSETS,0.9429400386847195,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
821"
LICENSES FOR EXISTING ASSETS,0.9439071566731141,"• For scraped data from a particular source (e.g., website), the copyright and terms of
822"
LICENSES FOR EXISTING ASSETS,0.9448742746615088,"service of that source should be provided.
823"
LICENSES FOR EXISTING ASSETS,0.9458413926499033,"• If assets are released, the license, copyright information, and terms of use in the
824"
LICENSES FOR EXISTING ASSETS,0.9468085106382979,"package should be provided. For popular datasets, paperswithcode.com/datasets
825"
LICENSES FOR EXISTING ASSETS,0.9477756286266924,"has curated licenses for some datasets. Their licensing guide can help determine the
826"
LICENSES FOR EXISTING ASSETS,0.9487427466150871,"license of a dataset.
827"
LICENSES FOR EXISTING ASSETS,0.9497098646034816,"• For existing datasets that are re-packaged, both the original license and the license of
828"
LICENSES FOR EXISTING ASSETS,0.9506769825918762,"the derived asset (if it has changed) should be provided.
829"
LICENSES FOR EXISTING ASSETS,0.9516441005802708,"• If this information is not available online, the authors are encouraged to reach out to
830"
LICENSES FOR EXISTING ASSETS,0.9526112185686654,"the asset’s creators.
831"
NEW ASSETS,0.9535783365570599,"13. New Assets
832"
NEW ASSETS,0.9545454545454546,"Question: Are new assets introduced in the paper well documented and is the documentation
833"
NEW ASSETS,0.9555125725338491,"provided alongside the assets?
834"
NEW ASSETS,0.9564796905222437,"Answer: [NA]
835"
NEW ASSETS,0.9574468085106383,"Justification: The paper does not introduce any new asset.
836"
NEW ASSETS,0.9584139264990329,"Guidelines:
837"
NEW ASSETS,0.9593810444874274,"• The answer NA means that the paper does not release new assets.
838"
NEW ASSETS,0.960348162475822,"• Researchers should communicate the details of the dataset/code/model as part of their
839"
NEW ASSETS,0.9613152804642167,"submissions via structured templates. This includes details about training, license,
840"
NEW ASSETS,0.9622823984526112,"limitations, etc.
841"
NEW ASSETS,0.9632495164410058,"• The paper should discuss whether and how consent was obtained from people whose
842"
NEW ASSETS,0.9642166344294004,"asset is used.
843"
NEW ASSETS,0.965183752417795,"• At submission time, remember to anonymize your assets (if applicable). You can either
844"
NEW ASSETS,0.9661508704061895,"create an anonymized URL or include an anonymized zip file.
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9671179883945842,"14. Crowdsourcing and Research with Human Subjects
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9680851063829787,"Question: For crowdsourcing experiments and research with human subjects, does the paper
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9690522243713733,"include the full text of instructions given to participants and screenshots, if applicable, as
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9700193423597679,"well as details about compensation (if any)?
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9709864603481625,"Answer: [NA]
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971953578336557,"Justification: This point does not apply to our paper.
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9729206963249516,"Guidelines:
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9738878143133463,"• The answer NA means that the paper does not involve crowdsourcing nor research with
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9748549323017408,"human subjects.
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758220502901354,"• Including this information in the supplemental material is fine, but if the main contribu-
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97678916827853,"tion of the paper involves human subjects, then as much detail as possible should be
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777562862669246,"included in the main paper.
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787234042553191,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796905222437138,"or other labor should be paid at least the minimum wage in the country of the data
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806576402321083,"collector.
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816247582205029,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825918762088974,"Subjects
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835589941972921,"Question: Does the paper describe potential risks incurred by study participants, whether
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845261121856866,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854932301740812,"approvals (or an equivalent approval/review based on the requirements of your country or
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864603481624759,"institution) were obtained?
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874274661508704,"Answer: [NA]
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988394584139265,"Justification: This point does not apply to our paper.
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893617021276596,"Guidelines:
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903288201160542,"• The answer NA means that the paper does not involve crowdsourcing nor research with
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912959381044487,"human subjects.
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922630560928434,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932301740812379,"may be required for any human subjects research. If you obtained IRB approval, you
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941972920696325,"should clearly state this in the paper.
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995164410058027,"• We recognize that the procedures for this may vary significantly between institutions
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961315280464217,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970986460348162,"guidelines for their institution.
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980657640232108,"• For initial submissions, do not include any information that would break anonymity (if
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990328820116054,"applicable), such as the institution conducting the review.
879"
