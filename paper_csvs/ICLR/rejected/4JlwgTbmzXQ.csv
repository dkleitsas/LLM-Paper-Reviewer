Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00425531914893617,"We study different notions of equivariance as an inductive bias in Reinforcement
Learning (RL) and propose new mechanisms for recovering representations that
are equivariant to both an agent’s action, and symmetry transformations of the
state-action pairs. Whereas prior work on exploiting symmetries in deep RL can
only incorporate predeﬁned linear transformations, our approach allows for non-
linear symmetry transformations of state-action pairs to be learned from the data
itself. This is achieved through an equivariant Lie algebraic parameterization of
state and action encodings, equivariant latent transition models, and the use of
symmetry-based losses. We demonstrate the advantages of our learned equivari-
ant representations for Atari games, in a data-efﬁcient setting limited to 100K
steps of interactions with the environment. Our method, which we call Equiv-
ariant representations for RL (EqR), outperforms other comparable methods on
statistically reliable evaluation metrics."
INTRODUCTION,0.00851063829787234,"1
INTRODUCTION"
INTRODUCTION,0.01276595744680851,"The recent success of deep reinforcement learning (Franc¸ois-Lavet et al., 2018) in applications to
games such as Atari (Mnih et al., 2015), Go (Silver et al., 2016) and Poker (Brown & Sandholm,
2019), to applications in robotics (Levine et al., 2016) and autonomous navigation (Bellemare et al.,
2020) has demonstrated its promise as the framework of choice for sequential decision making.
However, the use of a reward as the only signal for representation learning with high dimensional
states and actions leads to tremendous data inefﬁciency. Notably, almost all success stories of RL
rely on vast amounts of data or simulations with a huge computational overload."
INTRODUCTION,0.01702127659574468,"More data-efﬁcient representation learning (Bengio et al., 2013) requires stronger inductive biases,
though the search for a general yet strong inductive bias is still under way. One general approach
is to place a central role on transformations of the data, where invariance and equivariance to a
set of transformations imposes strong conditions on the learned representations. This viewpoint is
particularly appealing in RL, where the agent is in control of some of these transformations through
its own actions. Fig. 1 illustrates this concept using the example of a 2D pendulum. Moreover,"
INTRODUCTION,0.02127659574468085,"(a)
(b)
Figure 1: An illustration of typical symmetries in a pendulum, and the corresponding transformations of the
state and action for a group equivariant transition model: (a) shows how reﬂection of the agent’s state results in
a permutation of the action, denoted by a−1. (b) shows how rotation of the agent’s state results in invariance of
the action in the absence of gravity. The state transitions can be modeled as group actions (2D rotations in this
example), which can be captured by our symmetry transformation-based transition model. Note that rotational
symmetry can hold even when gravity is present. In this case, symmetry transformations include rotations (and
reﬂections) that preserve the Hamiltonian. Such non-linear energy-preserving transformations of state-actions
in the pixel space can become linear in the embedding space."
INTRODUCTION,0.02553191489361702,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.029787234042553193,"transformations naturally lead to a notion of disentanglement in the representations (Higgins et al.,
2018), potentially enabling better out-of-distribution generalization (Higgins et al., 2017; Thomas
et al., 2017). The recent success of self-supervised learning approaches that rely on a (predeﬁned)
set of transformations (Chen et al., 2020; Zbontar et al., 2021), and also within the context of RL
(Yarats et al., 2021; Laskin et al., 2020b), further highlights the importance of transformations in
data-efﬁcient representation learning."
INTRODUCTION,0.03404255319148936,"Motivated by these observations, this work develops a broader perspective on the notion of equiv-
ariant representation learning within RL. In particular, we integrate equivariance under the agent’s
action and equivariance under the symmetries of the environment into a single latent variable model
that is equivariant to an a priori unknown group of non-linear transformations of state-action pairs.
In contrast to the traditional approach of using symmetric Markov Decision Processes (MDPs), we
argue for modeling the larger group of state-action symmetries (separate from reward symmetries),
and show how to parameterize the latent embeddings of states and actions to make the representa-
tions equivariant to continuous transformations of the environment resulting from agent’s action. We
benchmark our approach, which we call Equivariant representations for RL (EqR), on the 26 games
in the Atari 100K benchmark (Kaiser et al., 2019), where we outperform other comparable methods
using reliable evaluation metrics (Agarwal et al., 2021). Our approach, however, is not restricted to
this domain. It is applicable in any setting where the transformations that an agent undergoes can be
expressed using matrix Lie groups, including autonomous driving, navigation, and robotics."
RELATED WORK,0.03829787234042553,"2
RELATED WORK"
RELATED WORK,0.0425531914893617,"The use of transformations, be it in data-augmentation or self-supervision, has become a common
ingredient in recent representation learning methods for deep RL. However, theoretical work on
symmetry in RL goes back to Zinkevich & Balch (2001) and Ravindran & Barto (2001), both of
which use symmetric MDPs. A more recent use of this formalism is in van der Pol et al. (2020b);
Mondal et al. (2020), where policy networks, with built-in equivariance, are shown to improve data-
efﬁciency. Closely related notions, that motivated the early work on symmetric MDPs, are model
minimization (Ravindran & Barto, 2002), state abstraction (Ravindran & Barto, 2003; Li et al.,
2006), MDP homomorphism (Ravindran & Barto, 2004) and lax bisimulations (Taylor, 2008). In
particular, MDP homomorphism, which requires equivariance under an agent’s action, encompasses
the general idea of model-based reinforcement learning. As examples, a latent MDP that matches
the state dynamics and the reward distribution of the environment is learned in (van der Pol et al.,
2020a; Gelada et al., 2019)."
RELATED WORK,0.04680851063829787,"Other works in RL that are relevant to our objective are those that attempt to increase data-efﬁciency
using a learned model of the environment. While some methods such as SimPLe (Kaiser et al.,
2019), learn this transition model at the pixel level, the majority of methods use a latent space
model. The latent space is either learned using reconstruction (Hafner et al., 2019a;b), or through
self-supervision and contrastive methods (CURL, Laskin et al., 2020b). However, there is evidence
that the improvement in sample efﬁciency is largely due to image augmentation, as seen in Laskin
et al. (2020a) and DrQ (Yarats et al., 2021). Using a reconstruction-based method is also inefﬁcient
because similar to pixel level models, one needs to learn potentially irrelevant details. The fact
that variations of model-free algorithms such as Data-Efﬁcient Rainbow (DER) (van Hasselt et al.,
2019) and OTRainbow (Kielak, 2019) can achieve a similar performance to reconstruction-based
methods without explicit representation learning components conﬁrms this intuition. More recently
SPR (Schwarzer et al., 2021) shows that data augmentation and improvements in Rainbow combined
with particular forms of self-supervision can signiﬁcantly improve the sample efﬁciency, producing
state-of-the-art results in sample-efﬁcient representation learning in RL."
BACKGROUND,0.05106382978723404,"3
BACKGROUND"
GROUPS AND THEIR REPRESENTATIONS,0.05531914893617021,"3.1
GROUPS AND THEIR REPRESENTATIONS"
GROUPS AND THEIR REPRESENTATIONS,0.059574468085106386,"A group G = {g} is a set, equipped with an associative binary operation, such that the set is closed
under this operation, and each element g ∈G has a unique inverse, such that their composition
gives the identity g−1g = e. Any subset G′ ≤G that is closed under binary operation of the groups
forms a subgroup. A group G can act on a set X by transforming its elements x ∈X through a"
GROUPS AND THEIR REPRESENTATIONS,0.06382978723404255,Under review as a conference paper at ICLR 2022
GROUPS AND THEIR REPRESENTATIONS,0.06808510638297872,"bijection. We use α ∶G ×X ↦X to denote the group action, and for brevity replace α(g,x) with g⋅x
moving forward. The action captures some of the structure of G due to two constraints – the identity
element acts trivially e ⋅x = x; and composition of actions is equal to action of the composition,
i.e., (gg′) ⋅x = g ⋅(g′ ⋅x),∀g,g′ ∈G. X is then called a G-set. Any G-action partitions X into
orbits xG = {g ⋅x ∣g ∈G}, and we denote the set of orbits under G-action using X/G. A G-action is
transitive iff its action results in a single orbit."
GROUPS AND THEIR REPRESENTATIONS,0.07234042553191489,"Parameterizing Lie Groups
In this work, we assume G is (any sub-group of) a classical Lie group
over R. These are the groups that can be represented using invertible matrices – in other words,
we consider groups whose action is linear on real vector spaces. We use ρ(G) to denote a linear
representation of G, and ρg ∶RD →RD for the action (a.k.a. the representation) of g ∈G. Two other
greek letters τ and κ are also used for this purpose. Many such Lie groups are identiﬁable by their
Lie algebra g = Lie(ρ(G)).1 This connection enables a simple parameterization of ρ(G) using a set
of linear bases for their Lie algebra – that is ρg = exp(∑i βg,i E(i)), where exp(Y) = ∑∞
j=0
Yj"
GROUPS AND THEIR REPRESENTATIONS,0.07659574468085106,"j! is the
matrix exponential. We refer to this parameterization later in Section 4. Such linear representations
in the form of invertible matrices can be used for both continuous transformations (e.g., 3D rotation)
and ﬁnite groups (×90○rotations)."
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.08085106382978724,"3.2
MDP HOMOMORPHISM AND SYMMETRIC MDPS"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.0851063829787234,"We deﬁne an MDP as the 4-tuple M = ⟨S,A,R,T⟩where S and A are respectively the set of states
and actions, R ∶S × A →R is the reward function, and T ∶S × A × S →R≥0 is the state transition
function.2 For two MDPs M = ⟨S,A,R,T⟩and ¯
M = ⟨¯S, ¯A, ¯R, ¯T⟩, MDP homomorphism can be
deﬁned as a tuple H = ⟨hS,hA⟩where hS ∶S →¯S is the state mapping and hA ∶S×A →¯A is the state
dependent action mapping. These two mappings satisfy the following invariance and equivariance
conditions:"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.08936170212765958,"1) Invariance of the reward:
¯R(hS(s),hA(s,a)) = R(s,a)
∀s,a ∈S × A
(1)"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.09361702127659574,"2) Equivariance of the deterministic transition model under the agent’s action:
¯T(hS(s),hA(s,a)) = hS(T(s,a))
∀s,a ∈S × A
(2)"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.09787234042553192,"A probabilistic variation of the above equation for a stochastic MDP (Bloem-Reddy & Teh, 2020)
is:
¯T(hS(s′) ∣hS(s),hA(s,a)) =
∑
s′′∈[s′]h
T(s′′ ∣s,a)
∀s,s′ ∈S,a ∈A,
(3)"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.10212765957446808,"where [s′]hS = hS
−1(hS(s′)) is the equivalence class of s′ under hS."
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.10638297872340426,"In related literature, MDP homomorphism is often used for minimization of the MDP, because the
optimal policy of ¯
M can be lifted to obtain the optimal counterparts for M."
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.11063829787234042,"Symmetric MDPs
The automorphism group GM = Aut(M) of an MDP identiﬁes the set of
symmetry transformations of state-actions that preserve the reward and the transition dynamics:"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.1148936170212766,"R(s,a) = R(g ⋅⟨s,a⟩))
∀g ∈GM,s ∈S,a ∈A (4)"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.11914893617021277,"T(s′ ∣s,a) = T(g ⋅s′ ∣g ⋅⟨s,a⟩) and g ⋅T(s,a) = T(g ⋅⟨s,a⟩)
∀g ∈GM,s,s′ ∈S,a ∈A (5)"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.12340425531914893,"We refer to a reward function R that satisﬁes Eq. (4) as a GM-invariant reward function and a
deterministic transition function T that satisﬁes Eq. (5) as a GM-equivariant transition function.
Note that this is a distinct notion from invariance and equivariance under agent’s action in the context
of MDP homomorphism. Here, the action refers to the action of a symmetry group, while in MDP
homomorphism, the equivariance is to the action of the agent. We use group action or G-action to
make this distinction clear when necessary."
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.1276595744680851,"For a symmetric MDP that satisﬁes both Eq. (4) and Eq. (5), both the optimal action-value and
optimal policy functions become invariant under GM action (Ravindran & Barto, 2001) – that is,"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.13191489361702127,"Q(s,a) = Q(g ⋅⟨s,a⟩)
and
π(a,s) = π(g ⋅⟨a,s⟩)
∀g ∈GM s,a ∈S × A.
(6)"
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.13617021276595745,"1This relation is bijective for ”simply connected” Lie groups.
2We ignore the discount factor for brevity."
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.14042553191489363,Under review as a conference paper at ICLR 2022
MDP HOMOMORPHISM AND SYMMETRIC MDPS,0.14468085106382977,"The connection of symmetric MDPs to MDP homomorphism is due to the fact that symmetries can
be used to deﬁne a homomorphism H ∶M ↦¯
M by collapsing the state-actions that form an orbit
under GM. Formally,the collapsed MDP
¯
M = ⟨¯S, ¯A, ¯R, ¯T⟩is deﬁned by ¯S = S/GM, ¯A = A/GM,
¯R(⟨s,a⟩GM) = R(s,a) and ¯T(s′GM ∣⟨s,a⟩GM) = T(s′ ∣s,a). This results in symmetry-based
model minimization of symmetric MDPs."
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.14893617021276595,"4
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.15319148936170213,"Separating Transition and Reward Symmetries
One important choice is between using the
symmetry group of the MDP (GM) versus the symmetry group of state-transitions (GT ), where GT is
the group of transformations of state-action pairs that leads to equivariant deterministic transitions,
as given by Equation 5. The former is a subgroup of the latter GM ≤GT , i.e, the symmetries of a
transition model contains the symmetries of the MDP. In fact it is easy to see that GM = GT ∩GR,
where GR is the group of transformations of state-action pairs that preserve the one step-reward and
only satisfy Equation 4. We observe that working with a larger symmetry group GT has two beneﬁts:
1) it creates a stronger inductive bias for the model, because in many real-world settings can involve
a range of symmetries in transitions that are not present in the reward. For example, an agent’s
navigation of a 2D map often has the symmetry of the Euclidean group, while the reward (e.g., ar-
riving at a particular location) breaks this symmetry; 2) Separate modeling of transition symmetries
facilitates transfer to new tasks, where the reward is changing."
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.1574468085106383,"Invariance/Equivariance in model-free/model-based RL
If the objective is to carry out model-
free RL, Eq. (6) motivates the need to learn action-value functions, or the policies that are invariant
to symmetries of the MDP (GM). For a deterministic policy, the invariance of Eq. (6) becomes an
equivariance constraint: g ⋅π(s) = π(g ⋅s). As it essentially leads to model minimization, van der
Pol et al. (2020b); Mondal et al. (2020) use this idea to improve sample efﬁciency when the groups
actions in the agent’s action space are known permutations. However, if our objective is just to learn
a symmetry-based model of the environment (i.e., transition and reward functions), Eq. (5) suggests
that we need to learn a GT -equivariant transition function."
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.16170212765957448,"Symmetries in a Latent Transition Model
While it is possible to learn the state transition model
in the observation space that is equivariant to the agent’s action, for high-dimensional inputs this
could be quite challenging since the model has to learn details of the environment that are irrele-
vant to the RL agent. Using state and action embeddings enables learning of the transition model
in the latent space. Indeed the constraint on the model and the embedding is that of the MDP ho-
momorphism Section 3.2. Working in the latent space has an additional beneﬁt when it comes to
symmetries: we can assume that the G action on the latent state-action pairs is linear through ρ(G)
despite having non-linear transformations in the observation space."
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.16595744680851063,"Figure 2: This ﬁgure demonstrates the relationship between
two types of equivariance in latent variable modeling for an
MDP with symmetric transition function. Green arrows (ver-
tical plane) identify a diagram for transition models in an
MDP homomorphism. A model ¯T and state embedding func-
tion hS that are equivariant under agent’s action makes this
diagram commute. Red arrows (horizontal plane) identify the
commutativity diagram for a symmetric transition function of
an MDP in the latent space. Here the state-action embedding
⟨˜s, ˜a⟩is produced through the symmetry transformation of
another state-action embedding ⟨¯s, ¯a⟩. T ¯T ¯T τg
κs g
τg s ¯s
¯a a , ,
s′￼ ¯s′￼"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.1702127659574468,State transition
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.17446808510638298,Transition Symmetry
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.17872340425531916,"Group     
equivariant"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.1829787234042553,transition
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.18723404255319148,"Action 
equivariant"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.19148936170212766,transition
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.19574468085106383,Latent Embedding
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2,"˜s , ˜a
˜s′￼"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.20425531914893616,"h𝕊
h𝕊
hs 𝔸"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.20851063829787234,"From the fact that symmetries of states GS ≤G is a subgroup of the state-action or transition symme-
try, it follows that ρg ∈ρ(G) can be divided into two parts: 1) τg ∈τ(GS) the group representation
acting on the state embedding, and; 2) κs
g ∈κ(G), the group representation for state-dependent
action embedding.3"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2127659574468085,"3This is because ρ(G) can be seen as a representation that is induced by the representation τ(GS) of its
subgroup ρ = IndG
GS τ."
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2170212765957447,Under review as a conference paper at ICLR 2022
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.22127659574468084,"At this point we can combine the requirement for an MDP homomorphism Eq. (2), with that of
the G-equivariant transition model Eq. (5) of a symmetric MDP. The result is the following two
constraints in our symmetric latent variable model ∀s,a ∈S × A and g ∈G (see Figure 2):"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.225531914893617,"¯T(hS(s),hA(s,a)) = hS(T(s,a))
(7)"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2297872340425532,"τg ¯T (hS(s),hA(s,a)) = ¯T (τghS(s),κs
ghA(s,a))
(8)"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.23404255319148937,"Matrix Embedding of States and Actions
While the design choice of this subsection is not nec-
essary, we see that it can signiﬁcantly simplify the constraints on a symmetry based model. We
propose to use group representations for our state, and state action embeddings hS ∶S →τ(GS) and
hA ∶S×A →κ(G). This choice assumes that a G action on state and state-action pairs is transitive, so
that each state, and state-action pair can be mapped to a (or at least one) group member. To empha-
size this in our notation, we use κ(s) instead of hS(s) and similarly use τ(s,a) instead of hA(s,a)
for state, and state-dependent action embedding respectively. This choice of embedding has several
beneﬁts: First, the learned embeddings are automatically equivariant to symmetry transformations
of the state, and state-actions:"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.23829787234042554,"τ(g ⋅s) = τgτ(s)
and
κ(g ⋅⟨s,a⟩) = κs
gκ(s,a)
∀s,a ∈S × A,g ∈G.
(9)"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2425531914893617,"This means that the symmetries of the state-action pairs are preserved and now take a linear form in
the latent space. Note that while the embeddings are automatically equivariant, they may be equiv-
ariant to irrelevant non-linear transformations of the input. Word modeling constraints ensure the
relevance of these non-linear transformations that are captured by the group equivariant embeddings
above. Moreover, this embedding enables the following simple choice for the transition model"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.24680851063829787,"¯T(τ(s),κ(s,a)) = κ(s,a)τ(s)
(10)"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.251063829787234,"which simply transforms the state-embedding τ(s) through the linear group action of state-
dependent action encoding κ(s,a). Using this transition model, the action equivariance constraint
of Eq. (7), and G-equivariance constraint of Eq. (8) simpliﬁes to: ∀s,a ∈S×A given a state transition
triplet {s,a,s′}"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2553191489361702,"τ(s′) = κ(s,a)τ(s)
(11)
τgκ(s,a)τ(s) = κs
gκ(s,a)τgτ(s)
(12)"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.25957446808510637,"In practice our model seeks to satisfy these two constraints via the direct minimization of appropriate
loss functions, as will be discussed in Section 5."
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.26382978723404255,"Decomposition of the Latent Space
The decomposition G = G1 ×...×GK into a direct product of
subgroups can disentangle the factors of variation in the dataset (Higgins et al., 2018). 4 This gives
us a way to represent the latent embedding space as a direct product of K subgroups of G, where
each factor varies independently by actions of a subgroup of G on the latent embedding. Intuitively
such a symmetry-based disentanglement provides an effective inductive bias particularly when there
is modularity so that temporally coherent changes in the environment are due to the change of a
(sparse) subset of factors. We impose this decomposed structure in the form of a direct sum for
state representation and the state-dependent action representation – that is τ(s) = ⊕k τk(s) and
κ(s,a) = ⊕k κk(s,a), where k ∈{1,...,K} and g = (g1,...,gK). Moreover the representation
of the symmetry group G acting on the state embedding and the state-dependent action embedding
is decomposed as τg = ⊕k τgk and κs
g = ⊕k κs
gk, where gk ∈Gk for ∈{1,...,K} and g ∈G.
Combining this block structure with the Lie parameterization of Section 3.1 we get"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2680851063829787,"τθ(s) = ⊕
k
exp(∑
i
βi,k,θ(s)E(i))
κφ(τθ(s),a) = ⊕
k
exp(∑
i
αi,k,φ(τθ(s),a)E(i))
(13)"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2723404255319149,"where we use any standard neural network to implement the αφ and βθ functions above 5. As we can
backpropagate through this function, the network parameters θ,φ can be learned end to end. The"
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2765957446808511,"4As noted by Caselles-Dupr´e et al. (2019), simply having a product structure in the latent space does not
guarantee disentanglement, and further constraints are required. In this work, we do not impose any additional
constraints for disentanglement."
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.28085106382978725,"5We denote both, the neural networks which maps to group representations, and network parameters by
lowercase greek alphabets."
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.2851063829787234,Under review as a conference paper at ICLR 2022
DESIDERATA FOR SYMMETRY-BASED REPRESENTATION IN RL,0.28936170212765955,"choice of the subgroup depends on the symmetries of the RL environment, and it only affects the
set of bases {E(i)}i in Eq. (13). For example, in Atari games, the screen often has multiple objects
undergoing 2-D translations and rotations, and one can use blocks of the 2-D Special Euclidean
(SE(2)) that comprise translation and rotations of Euclidean space (E). For more realistic 3D
environments, such as those of interest in robotics, self-driving cars and third person games, one
can use SE(3), which is the group of 3-D translations and rotations. Also, in theory, we only
need specify a group that “contains” the group of interest as a subgroup. For example, if our state-
actions only have 90○rotational symmetry, we may use a more general group for the representation
(e.g., SE(2)). The embedding function can deﬁne a homomorphism into the relevant subgroup."
LOSS FUNCTIONS,0.2936170212765957,"5
LOSS FUNCTIONS"
LOSS FUNCTIONS,0.2978723404255319,"We consider a standard RL setup where the agent interacts with its environments in episodes and we
have access to ({st,at,rt,st+1})t=1,..,T where st is the state, at is the action taken by the agent, rt
is the reward received and st+1 is the observed next state at timestep t. Below we describe three loss
functions that encode the equivariance/invariance constraints of Eqs. (1), (11) and (12)."
LOSS FUNCTIONS,0.3021276595744681,"Action Equivariant Transition Loss - Eq. (11)
Given triplets ⟨st,at,st+1⟩from our dataset we
simply apply a loss function ℓsuch as square loss6 that penalizes the difference between two sides:"
LOSS FUNCTIONS,0.30638297872340425,"LAET (θ,φ) = ℓ(τθ(st+1),κφ(st,at)τθ(st)).
(14)"
LOSS FUNCTIONS,0.31063829787234043,"The choice of the embedding space and the latent transition function ensure that state embeddings
are transformed by linear group action of the action embeddings. Minimization of LAET encourages
these symmetry transformations to capture state transitions resulting from the agent’s action."
LOSS FUNCTIONS,0.3148936170212766,"Group Equivariant Transition Loss - Eq. (12)
For this we need a st′ in addition to ⟨st,at⟩,
where t′ can be any state (at different time step in the same or a different episode.) We ﬁnd the
group transformation that maps s to s′ the latent space using τg = τθ(st′)τθ(st)−1. Using this we
can rewrite Equation Eq. (12) as"
LOSS FUNCTIONS,0.3191489361702128,"τθ(st′)τθ(st)−1
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
τg"
LOSS FUNCTIONS,0.32340425531914896,"κφ(st,at)τθ(st) = κs
gκφ(st,at) τθ(st′)
´¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¶
τgτθ(st)"
LOSS FUNCTIONS,0.3276595744680851,".
(15)"
LOSS FUNCTIONS,0.33191489361702126,"Since the state-dependent action encoding κφ(st,at) for the pair ⟨st,at⟩is also produced by a neural
network, the only missing part in the equation above is κs
g, the state-dependent action transformation.
We use a neural network ρω ∶τg ↦κs
g to infer it from state transformation τg.
Example 1. To get an intuition for what this network is doing consider the example of a pendulum
without gravity, with rotation and reﬂection symmetry O(2) as shown in Fig. 1, where inputs to the
networks (s) are image sequences and the (ideal) embeddings τθ(s),κφ(s,a) are x-y coordinates
plus angular velocity and torque respectively. If we rotate the pendulum using a rotation matrix τg,
we expect state-dependent action embedding to remain the same since the effect of torque remains
similar after rotation. However, if we transform the pendulum by reﬂection around the vertical axis,
we expect that the effect of torque will be negated. ρω parameterizes this dependence."
LOSS FUNCTIONS,0.33617021276595743,"A loss function ℓcould then measure the difference between left and the right hand side in the
equation above"
LOSS FUNCTIONS,0.3404255319148936,"LGET (θ,φ,ω) = ℓ(τθ(st′)τθ(st)−1κφ(st,at)τθ(st),ρω(τθ(st′)τθ(st)−1)κφ(st,a)τθ(st′)).
(16)"
LOSS FUNCTIONS,0.3446808510638298,"Action Invariant Reward Loss - Eq. (1)
While LAET and LGET enforce the equivariance of the
latent transition model to an agent’s action and the symmetry group, they do not encode information
of the reward in the state representations. In order for the latent model to be homomorphic to the
underlying MDP of the environment we match the reward at every state embedding using a reward
predictor network rψ ∶τθ(s) ↦R. We measure the difference between the predicted reward and the
actual reward at time step t + 1:"
LOSS FUNCTIONS,0.34893617021276596,"LR(ψ,θ,φ) = (rψ(κφ(τθ(st),at)τθ(st)) −rt+1)2 .
(17)"
LOSS FUNCTIONS,0.35319148936170214,"6In practice we use the normalized square loss ℓ(Y , Y ′) = ∥
Y
∥Y ∥2 −
Y ′"
LOSS FUNCTIONS,0.3574468085106383,"∥Y ′∥2 ∥
2 2"
LOSS FUNCTIONS,0.3617021276595745,Under review as a conference paper at ICLR 2022
APPLICATION TO MODEL-FREE RL,0.3659574468085106,"6
APPLICATION TO MODEL-FREE RL"
APPLICATION TO MODEL-FREE RL,0.3702127659574468,"Following previous success of using transition models for representation learning in model-free RL
(Gelada et al., 2019; Schwarzer et al., 2021) we add the losses discussed above to the Temporal
Difference (TD) error in Deep Q-learning. In practice, we need to make three modiﬁcations to our
model/loss."
APPLICATION TO MODEL-FREE RL,0.37446808510638296,"Target Network
A trivial solution to both equivariance enforcing losses of Section 5 is to encode
all states and actions using an identity matrix. This problem in different contexts is known as the
problem of collapse in representation learning. While using the reward signal helps in avoiding the
collapse it is often not sufﬁcient specially in sparse reward settings. Following Schwarzer et al.
(2021), we use a target network to encode state st+1 and st′ in Eq. (16) in which the network
parameters do not receive gradient and moreover are copied from the online network. We explicitly
drop the subscripts to differentiate the target from the online network in this section (e.g., τθ →τ)."
APPLICATION TO MODEL-FREE RL,0.37872340425531914,"Projection Head for Transition Losses
Strict enforcement of symmetry constraints by our model
can be overly restrictive when the environment has non-symmetric components, or when the our
transition model is too simplistic. For this reason, following the previous work, we enforce the
losses on a learnable projection of the state embedding. That is before application of the loss ℓin
Eqs. (14) and (16) we pass the embedding through a projection head."
APPLICATION TO MODEL-FREE RL,0.3829787234042553,"M-step prediction
Following the success of (Schwarzer et al., 2021) because of long-term state
embedding predictions, we predict state embeddings and rewards for M-steps."
PUTTING IT ALL TOGETHER,0.3872340425531915,"6.1
PUTTING IT ALL TOGETHER"
PUTTING IT ALL TOGETHER,0.39148936170212767,"Considering M consecutive state-actions {st∶t+M,at∶t+M} and ˆxt = xt = τθ(st), we predict the state
embeddings and the rewards of next M steps:"
PUTTING IT ALL TOGETHER,0.39574468085106385,"ˆxt+m = κφ(ˆxt+m−1,at+m−1)ˆxt+m−1
and
ˆrt+k = rψ(ˆxt+k)
∀m ∈{1,...,M}"
PUTTING IT ALL TOGETHER,0.4,"Note that we are using ˆx for M-step model prediction of the embedding to distinguish them from
the latent embedding x, and the embedding produced by the target network ¯x = τ(st+m). The same
applies to the M-step predicted reward ˆr and observed reward r. We then project these embedding
using a projection head pζ to produce ˆzt+m = pζ(ˆxt+m) and ¯zt+m = p(¯xt+m). Using this notation,
our ﬁnal expressions for LAET and LR are:"
PUTTING IT ALL TOGETHER,0.40425531914893614,"LAET =
M
∑
m=1
∥
ˆzt+m
∥ˆzt+m∥2
−
¯zt+m
∥¯zt+m∥2
∥ 2"
AND,0.4085106382978723,"2
and
LR =
M
∑
m=1
(ˆrt+m −rt+m)2
(18)"
AND,0.4127659574468085,"For LGET we need ⟨st,at,st+1⟩and another state s′. From their embedding using the notation
above we get τg = ¯xt′xt−1, the linear transformation between them, and κs
g = ρω(τg), the state-
dependent action transformation. Now for ¯xt′, we obtain the predicted next state from ¯xt′ as ¯xt′+1 =
κs
gκ(xt,at)¯xt′ = ρω(xt′x−1
t )κ(xt,at)¯xt′ and from ˆxt+1 as ˆxt′+1 = τgˆxt+1. Before penalizing the
difference between these embeddings, we project them to ˆyt′+1 = bη(ˆxt′+1) and ¯yt′+1 = b(¯xt′+1)
using projection head bη, to get the ﬁnal expression for LGET :"
AND,0.41702127659574467,"LGET = ∥
ˆyt′+1
∥ˆyt′+1∥2
−
¯yt′+1
∥¯yt′+1∥2
∥ 2"
AND,0.42127659574468085,"2
(19)"
AND,0.425531914893617,"Q-learning
We pass the representation xt to a Q-learning head qξ to learn policies based on the
output of the Q-value estimator. The Q-value estimator is learnt by minimizing:"
AND,0.4297872340425532,"LDQN(ξ,θ) = (qξ(τθ(st),at) −(rt + γ max
a
qξ(τ(st+1),a)))2
(20)"
AND,0.4340425531914894,"We use the data efﬁcient adaptation of Rainbow (van Hasselt et al., 2019; Hessel et al., 2018) which
combines many improvements over the original DQN(Mnih et al., 2013) such as Distributional
RL(Dabney et al., 2018), Dueling DQN (Wang et al., 2016), Double DQN (Van Hasselt et al., 2016).
The total loss optimized by our model is:"
AND,0.43829787234042555,"L = LDQN + λ1LR + λ2LGET + λ3LAET
(21)"
AND,0.4425531914893617,Under review as a conference paper at ICLR 2022
AND,0.44680851063829785,"where λ1, λ2 and λ3 are hyper-parameters. Motivated by the performance improvements due to
augmentation reported in recent literature (Yarats et al., 2021; Schwarzer et al., 2021), we also
augment our states by shifting and changing the pixel intensity before encoding them. Fig. 5 in
Appendix B.2 shows a detailed schematic of our model. We provide the algorithm for our model in
Appendix B.4 and details of network architectures in Appendix B.3."
EXPERIMENTS,0.451063829787234,"7
EXPERIMENTS"
EXPERIMENTS,0.4553191489361702,"We test our method on a suite of 2D Atari games, which is a popular benchmark used in RL. The full
Atari suite consists of 57 games with typically 50 million environment steps. We use the sample-
efﬁcient Atari suite introduced by Kaiser et al. (2019), which consists of 26 games with only 100,000
environment steps of training data available. In our experiments, we use three types of simple
connected Lie subgroup blocks including General Linear GL(2), Special Euclidean SE(2), and
Translation T(2), see Appendix A for more details. Unless stated otherwise, our EqR model uses
SE(2) subgroup blocks, with K = 12 blocks and M = 5 steps during training. LAET is always used
to train EqR inorder to model the non-linear transfomations in the state space resulting from agent’s
actions as linear group actions in the latent space. LGET , which makes transition model equivariant
with respect to the symmetry transformation of state-actions, and LR are optional. We build our
implementation on top of SPR’s (Schwarzer et al., 2021), which is based on rlpyt (Stooke &
Abbeel, 2019) and PyTorch (Paszke et al., 2019). We use the same underlying RL algorithm and
hyperparameters used by SPR for fair comparision."
EXPERIMENTS,0.4595744680851064,"Evaluation Metrics
We compute the average episodic return (the ‘game score’) at the end of train-
ing and normalize it with respect to human scores, as is standard practice. The human-normalized
score (HNS) is given by
agent score - random score
human score - random score. Since there is considerable variance across different
runs, the mean and the median are not very reliable metrics. Instead, Agarwal et al. (2021) pro-
pose using bootstrapped conﬁdence intervals (CI) with stratiﬁed sampling which is more suitable
for small sample sizes (10 runs per game in our case). We report the Interquartile Mean (IQM),
which is the mean across the middle 50% of the runs, as well as the Optimality Gap, which is the
amount by which the algorithm fails to meet a minimum HNS of 1.0. We also provide performance
proﬁles showing the fraction of runs above a certain normalized score, which gives a more complete
picture of the performance."
EXPERIMENTS,0.46382978723404256,"Figure 3: Performance proﬁles for different methods based
on score distributions (left), and average score distributions
(right).
Shaded regions show pointwise 95% conﬁdence
bands. The higher the curve, the better the method is."
EXPERIMENTS,0.46808510638297873,"Results We use 10 seeds for every game,
for every variation of our model. Figure 3
shows performance proﬁles for two vari-
ations of our model, EqR with LR and
with LR + LGET , along with other com-
parable methods. If one curve is strictly
above another, the better method is said
to “stochastically dominate” the other
(Agarwal et al., 2021).
The curves for
both variations of the proposed method
are almost always above the next best
method, SPR (Schwarzer et al., 2021).
Figure 4(a) provides results for different
methods on all 26 games. The two best
variations of the proposed method outperform previous methods, and the difference is statistically
signiﬁcant considering the CI. Table 2 in Appendix B.1 shows full results on all games, and our best
model achieves super-human performance on eight games and achieves higher score than any other
previous method on 13 out of the 26 games."
EXPERIMENTS,0.4723404255319149,"In order to better understand the effect of various modeling choices, loss functions and implemen-
tation details on the performance, we now consider different variations of EqR, with the same aug-
mentation as the baseline for ablation studies."
EXPERIMENTS,0.4765957446808511,"Choice of Group To understand the role of the choice of a group in the embedding space, we use
our EqR model with LR. This variation of EqR is similar to DeepMDP (Gelada et al., 2019), except
for the group structured latent embedding space and group action-based state transition. In order to
investigate the effect of the above two group-related constructs, we remove them and use an action"
EXPERIMENTS,0.4808510638297872,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.4851063829787234,"(a)
(b)"
EXPERIMENTS,0.48936170212765956,"(c)
(d)
Figure 4: Plots of Interquartile Mean (IQM) and Optimality Gap computed from human-normalized scores,
showing the point estimates along with 95% conﬁdence intervals (over 10 runs for all methods, 5 runs for
SimPLe). A higher IQM and a lower optimality gap reﬂects better performance. (a) shows different methods
for all 26 games. (b) shows different methods for 17 games. (c) shows the proposed model with different group
choices for all 26 games. (d) shows the proposed model with different loss terms for all 26 games."
EXPERIMENTS,0.49361702127659574,"encoder to predict the next states directly, referring to this as MLP. We further test models with other
subgroups including T(2) and GL(2). Figure 4(c) shows that adding a symmetry-based inductive
bias in the model by making the embeddings group representations and modeling the transitions
as group actions is indeed helpful. The success of the model which uses SE(2) blocks might be
attributed to the fact that translations and rotations are the most common types of symmetry transfor-
mations present in Atari games. However, the more restrictive T(2) slightly hurts the performance,
while the more general GL(2) performs similarly to the MLP model."
EXPERIMENTS,0.4978723404255319,"Loss functions Figure 4(d) compares the performance of EqR using SE(2) subgroup blocks with
different loss components. Using EqR with the default LAET results in a considerable improvement
over Rainbow with augmentation. Adding LGET improves the performance slightly, while adding
only LR improves the performance even further. We hypothesize that the reward loss is playing a
role in both preventing representation collapse and preserving more information of the reward dis-
tribution in the latent state embeddings. Adding both LGET and LR improves the performance only
slightly. It might be that this prior of a equivariant transition model with respect to symmetry trans-
formations of state-actions is too restrictive for some games while being beneﬁcial for others. Based
on the results in Figure 4(b), in 17 out of a total of 26 games, including this loss term leads to a sta-
tistically signiﬁcant boost in performance. These 17 games are: ‘Alien’, ‘BankHeist’, ‘BattleZone’,
‘Boxing’, ‘ChopperCommand’, ‘CrazyClimber’, ‘DemonAttack’, ‘Freeway’, ‘Hero’, ‘Jamesbond’,
’MsPacman’, ‘Pong’, ‘PrivateEye’, ‘Qbert’, ‘RoadRunner’, ‘Seaquest’, ‘UpNDown’. The full list
of game-wise scores for the ablation studies are presented in Tables 3 and 4 in Appendix B.1."
CONCLUSION,0.502127659574468,"8
CONCLUSION"
CONCLUSION,0.5063829787234042,"This paper considers three major symmetry-related constructs within a coherent framework. First,
there is the group equivariant state and state-dependent action embedding, which we achieve through
Lie parameterization. The world modeling constraints, which are discussed next, further ensure
that the transformations captured by the equivariant embedding are relevant. Second, the action
equivariant transition, which when combined with group equivariant embeddings, ensures that the
state transitions are captured by symmetry transformations in the latent space. Third, there is the
group equivariant transition, which acts as an additional bias and ensures that the latent transition
model itself is equivariant under symmetry transformations of state-action pairs."
CONCLUSION,0.5106382978723404,"We provided an extensive set of experiments to evaluate the usefulness of our approach in learning
state-embeddings for model-free RL. In future work we would like to explore the application of our
approach in model-based RL, as well as its ability to generalize across tasks. We also plan to further
investigate theoretically grounded methods for combining both symmetric and asymmetric aspects
of the environment in the model."
CONCLUSION,0.5148936170212766,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5191489361702127,"9
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.5234042553191489,"We have made every effort to ensure that our method is reproducible. Section 5 and Section 6 provide
detailed descriptions of the loss functions and various implementation-related details. We present a
step-by-step algorithm in Appendix B.4 and have provided details of the network architecture and
the list of hyperparameters used, in Sections B.3 and B.5 respectively. Section B.1 has individual
results on each game for the proposed EqR model and its variations, to enable researchers to verify
the results. Finally, we have submitted the code as part of the submission and we plan on releasing
it to the public once the reviewing process is over."
REFERENCES,0.5276595744680851,REFERENCES
REFERENCES,0.5319148936170213,"Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Informa-
tion Processing Systems, 34, 2021."
REFERENCES,0.5361702127659574,"Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Sub-
hodeep Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric bal-
loons using reinforcement learning. Nature, 588(7836):77–82, 2020."
REFERENCES,0.5404255319148936,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,
2013."
REFERENCES,0.5446808510638298,"Benjamin Bloem-Reddy and Yee Whye Teh. Probabilistic symmetries and invariant neural networks.
J. Mach. Learn. Res., 21:90–1, 2020."
REFERENCES,0.548936170212766,"Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885–890, 2019."
REFERENCES,0.5531914893617021,"Hugo Caselles-Dupr´e, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled rep-
resentation learning requires interaction with environments.
Advances in Neural Information
Processing Systems, 32:4606–4615, 2019."
REFERENCES,0.5574468085106383,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020."
REFERENCES,0.5617021276595745,"Will Dabney, Mark Rowland, Marc G Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018."
REFERENCES,0.5659574468085107,"Vincent Franc¸ois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An
introduction to deep reinforcement learning. arXiv preprint arXiv:1811.12560, 2018."
REFERENCES,0.5702127659574469,"Carles Gelada, Saurabh Kumar, Jacob Buckman, Oﬁr Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, pp. 2170–2179. PMLR, 2019."
REFERENCES,0.574468085106383,"Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a."
REFERENCES,0.5787234042553191,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555–2565. PMLR, 2019b."
REFERENCES,0.5829787234042553,"Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-second AAAI conference on artiﬁcial intelligence, 2018."
REFERENCES,0.5872340425531914,"I. Higgins, D. Amos, D. Pfau, S´ebastien Racani`ere, Lo¨ıc Matthey, Danilo Jimenez Rezende,
and Alexander Lerchner.
Towards a deﬁnition of disentangled representations.
ArXiv,
abs/1812.02230, 2018."
REFERENCES,0.5914893617021276,Under review as a conference paper at ICLR 2022
REFERENCES,0.5957446808510638,"Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot trans-
fer in reinforcement learning. In International Conference on Machine Learning, pp. 1480–1490.
PMLR, 2017."
REFERENCES,0.6,"Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Bła˙zej Osi´nski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model based
reinforcement learning for atari. In International Conference on Learning Representations, 2019."
REFERENCES,0.6042553191489362,"Kacper Piotr Kielak. Do recent advancements in model-based deep reinforcement learning really
improve data efﬁciency? 2019."
REFERENCES,0.6085106382978723,"Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020a."
REFERENCES,0.6127659574468085,"Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. In International Conference on Machine Learning, pp. 5639–
5650. PMLR, 2020b."
REFERENCES,0.6170212765957447,"Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016."
REFERENCES,0.6212765957446809,"Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a uniﬁed theory of state abstraction
for mdps. ISAIM, 4:5, 2006."
REFERENCES,0.625531914893617,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller.
Playing atari with deep reinforcement learning.
arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.6297872340425532,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.6340425531914894,"Arnab Kumar Mondal, Pratheeksha Nair, and Kaleem Siddiqi. Group equivariant deep reinforce-
ment learning. arXiv preprint arXiv:2007.03437, 2020."
REFERENCES,0.6382978723404256,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026–8037, 2019."
REFERENCES,0.6425531914893617,"Robin Quessard, Thomas D Barrett, and William R Clements. Learning group structure and disen-
tangled representations of dynamical environments. arXiv preprint arXiv:2002.06991, 2020."
REFERENCES,0.6468085106382979,"B. Ravindran and A. G. Barto. Symmetries and model minimization in markov decision processes.
Technical report, USA, 2001."
REFERENCES,0.6510638297872341,"Balaraman Ravindran and Andrew G Barto.
Model minimization in hierarchical reinforcement
learning. In International Symposium on Abstraction, Reformulation, and Approximation, pp.
196–211. Springer, 2002."
REFERENCES,0.6553191489361702,"Balaraman Ravindran and Andrew G Barto.
Smdp homomorphisms: an algebraic approach to
abstraction in semi-markov decision processes. In Proceedings of the 18th international joint
conference on Artiﬁcial intelligence, pp. 1011–1016, 2003."
REFERENCES,0.6595744680851063,"Balaraman Ravindran and Andrew G. Barto. An algebraic approach to abstraction in reinforcement
learning. 2004."
REFERENCES,0.6638297872340425,"Max Schwarzer,
Ankesh Anand,
Rishab Goel,
R Devon Hjelm,
Aaron Courville,
and
Philip Bachman.
Data-efﬁcient reinforcement learning with self-predictive represen-
tations.
In
International
Conference
on
Learning
Representations,
2021.
URL
https://openreview.net/forum?id=uCQfPZwRaUu."
REFERENCES,0.6680851063829787,Under review as a conference paper at ICLR 2022
REFERENCES,0.6723404255319149,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.676595744680851,"Adam Stooke and Pieter Abbeel. rlpyt: A research code base for deep reinforcement learning in
pytorch. arXiv preprint arXiv:1909.01500, 2019."
REFERENCES,0.6808510638297872,Jonathan Taylor. Lax probabilistic bisimulation. 2008.
REFERENCES,0.6851063829787234,"Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean
Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable features.
arXiv preprint arXiv:1708.01289, 2017."
REFERENCES,0.6893617021276596,"Elise van der Pol, Thomas Kipf, Frans A Oliehoek, and Max Welling. Plannable approximations
to mdp homomorphisms: Equivariance under actions. In Proceedings of the 19th International
Conference on Autonomous Agents and MultiAgent Systems, pp. 1431–1439, 2020a."
REFERENCES,0.6936170212765957,"Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp homo-
morphic networks: Group symmetries in reinforcement learning. Advances in Neural Information
Processing Systems, 33, 2020b."
REFERENCES,0.6978723404255319,"Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 30, 2016."
REFERENCES,0.7021276595744681,"Hado P van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in re-
inforcement learning?
Advances in Neural Information Processing Systems, 32:14322–14333,
2019."
REFERENCES,0.7063829787234043,"Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International conference on machine
learning, pp. 1995–2003. PMLR, 2016."
REFERENCES,0.7106382978723405,"Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. In International Conference on Learning Representa-
tions, 2021. URL https://openreview.net/forum?id=GY6-6sTvGaf."
REFERENCES,0.7148936170212766,"Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021."
REFERENCES,0.7191489361702128,"Martin Zinkevich and Tucker Balch. Symmetry in markov decision processes and its implications
for single agent and multi agent learning. In In Proceedings of the 18th International Conference
on Machine Learning. Citeseer, 2001."
REFERENCES,0.723404255319149,"A
SUBGROUP BLOCKS AND THEIR PARAMETERIZATION"
REFERENCES,0.7276595744680852,"A.1
CHOICE OF GROUP"
REFERENCES,0.7319148936170212,"Atari games differ in their style of play, their objectives, the symmetry transformations of both the
agent and other objects on the screen and associated symmetry transformation of the agent’s action.
But most of these games include symmetry transformations. For example, the screen often has mul-
tiple objects undergoing two dimensional(2-D) translations and rotations. In this case one can use
blocks of the 2-D Special Euclidean Group SE(2). Each such block can capture the transformation
of a particular object in the screen, including the agent. One can also use more restrictive subgroup
blocks like T(2), which capture only 2D translations. For more realistic 3D environments, such as
those of interest in robotics, self-driving cars and third person games, one can use SE(3), which
is the group of 3-D translations and rotations. This should capture both the transformations of the
objects in the environment and changes in viewpoint due to the agent’s actions."
REFERENCES,0.7361702127659574,"A.2
IMPLEMENTING PARAMETERIZATION"
REFERENCES,0.7404255319148936,"We consider three types of subgroups: GL(n) - the set of all invertible linear transformations, SE(n) -
the set of all rotations and translations and T(n) - the set of all translations in an n dimensional vector
space. We provide a general method to parameterize each of these, based on the type of group."
REFERENCES,0.7446808510638298,Under review as a conference paper at ICLR 2022
REFERENCES,0.7489361702127659,"GL(n)
As the matrix representation of GL(n) is the set of invertible matrices which has a measure
of 1 it is easy to parameterize it. We just generate n2 parameters using a network corresponding to
each element of the matrix. This gives an element from GL(n)."
REFERENCES,0.7531914893617021,"T(n)
As T(n) just denotes translation in a n-dimensional space with group action being addition,
implementing it is straightforward. We generate n parameters using a neural network and instead
of using matrix multiplication use addition for the group action. Note that we can also use a matrix
representation for T(n) but it is unnecessary and inefﬁcient."
REFERENCES,0.7574468085106383,"SE(n)
Unlike GL(n) and T(n), parameterizing SE(n) is a bit tricky because it involves pa-
rameterizing SO(n).
We use a homogeneous co-ordinate based representation of SE(n) ="
REFERENCES,0.7617021276595745,"{(R
t
0
1),R ∈SO(n) and t ∈T(n)}. So we need n parameters for the t and another D = n(n−1)"
REFERENCES,0.7659574468085106,"2
parameters for SO(n) from the neural network. As explained in Section 4, we can use a Lie pa-
rameterization to get the elements of SO(n) by R = exp(∑D
d=1 βd E(d)) where E(d) denote D bases
of the space of skew symmetric matrices and the βis are the parameters of the neural network. For"
REFERENCES,0.7702127659574468,"example, in the case of SO(2) we can use the basis E(1) = ( 0
1
−1
0) . Similarly, we can extend this"
REFERENCES,0.774468085106383,"to SO(n) by using a basis given by D n × n matrices E(ij) ∀{1 ≤i < j ≤n} whose only non-zero
elements are E(ij)
i,j = −1 and E(ij)
j,i = 1."
REFERENCES,0.7787234042553192,"Although Lie parameterization gives us a general recipe to output a representation of simple con-
nected Lie groups like SO(n), in our implementation we use Euler parameterization because it runs
faster in Pytorch. We provide the code for both. Following Quessard et al. (2020), we parameter-
ize each rotation matrix in SO(n) using the product of rotations on D orthogonal planes in Rn:
R = ∏n
i=1 ∏1≤i<j≤n Rij. Here Rij ∈Rn×n is the rotation matrix in the i −j plane, and its non-zero
elements besides the diagonal are the four values on the i,j rows and columns, which comprise the"
D ROTATION MATRIX THAT IS RIJ,0.7829787234042553,"2D rotation matrix that is Rij
i,j = [ cos(θi,j)
sin(θi,j)
−sin(θi,j)
cos(θi,j)]. We have D parameters θi,j which we can"
D ROTATION MATRIX THAT IS RIJ,0.7872340425531915,obtain from a neural network.
D ROTATION MATRIX THAT IS RIJ,0.7914893617021277,"The parameters in all the parameterization techniques mentioned here can be back-propagated. We
summarize the number of parameters required from a neural network output, representation type
and the associated group actions of different subgroups in Table 1."
D ROTATION MATRIX THAT IS RIJ,0.7957446808510639,Table 1: Group Properties
D ROTATION MATRIX THAT IS RIJ,0.8,"Subgroup block type
#Parameters
Representation type
Group action"
D ROTATION MATRIX THAT IS RIJ,0.8042553191489362,"General Linear - GL(n)
n2
Matrixn×n
Matrix multiplication
Special Euclidean - SE(n)
n(n−1)"
D ROTATION MATRIX THAT IS RIJ,0.8085106382978723,"2
+ n
Matrix(n+1)×(n+1)
Matrix multiplication
Special Orthogonal - SO(n)
n(n−1)"
D ROTATION MATRIX THAT IS RIJ,0.8127659574468085,"2
Matrixn×n
Matrix multiplication
Translation - T(n)
n
Vectorn
Addition"
D ROTATION MATRIX THAT IS RIJ,0.8170212765957446,Under review as a conference paper at ICLR 2022
D ROTATION MATRIX THAT IS RIJ,0.8212765957446808,"B
ATARI DETAILS"
D ROTATION MATRIX THAT IS RIJ,0.825531914893617,"B.1
FULL RESULTS"
D ROTATION MATRIX THAT IS RIJ,0.8297872340425532,"We provide individual results on the 26 Atari games after 100K training steps. Our results are aver-
aged over 10 seeds, and the network architectures and full list of hyperparameters used to produce
them are provided in Appendix B.3 and Appendix B.5."
D ROTATION MATRIX THAT IS RIJ,0.8340425531914893,"• Table 2 compares our two best performing EqR models using SE(2) subgroup blocks with
other methods."
D ROTATION MATRIX THAT IS RIJ,0.8382978723404255,"• Table 3 compares different choices of subgroup blocks with the reward loss, LR, included
for all EqR models (also see Figure 4 (a))."
D ROTATION MATRIX THAT IS RIJ,0.8425531914893617,"• Table 4 compares EqR using SE(2) subgroup blocks with different loss terms included in
the training objective (see Section 5 and Figure 4 (b)). The action equivariance transition
loss, LAET is always included for EqR models."
D ROTATION MATRIX THAT IS RIJ,0.8468085106382979,"Table 2: Mean game scores on the 26 Atari games after 100K environment steps. The EqR models
use SE(2) subgroup blocks along with an action equivariant transition loss, LAET , and are averaged
over 10 seeds."
D ROTATION MATRIX THAT IS RIJ,0.851063829787234,"Game
Random
Human
SimPLe
DER
CURL
DrQ
SPR
EqR, LR
EqR, LR + LGET
Alien
227.8
7127.7
616.9
739.9
558.2
771.2
801.5
774.0
872.9
Amidar
5.8
1719.5
88.0
188.6
142.1
102.8
176.3
140.9
138.4
Assault
222.4
742.0
527.2
431.2
600.6
452.4
571.0
753.8
734.3
Asterix
210.0
8503.3
1128.3
470.8
734.5
603.5
977.8
923.2
902.5
Bank Heist
14.2
753.1
34.2
51.0
131.6
168.9
380.9
395.1
397.4
BattleZone
2360.0
37187.5
5184.4
10124.6
14870.0
12954.0
16651.0
13044.0
13255.0
Boxing
0.1
12.1
9.1
0.2
1.2
6.0
35.8
37.5
39.2
Breakout
1.7
30.5
16.4
1.9
4.9
16.1
17.1
17.2
16.0
ChopperCommand
811.0
7387.8
1246.9
861.8
1058.5
780.3
974.8
1073.5
1142.2
Crazy Climber
10780.5
35829.4
62583.6
16185.3
12146.5
20516.5
42923.6
49399.0
52008.1
Demon Attack
152.1
1971.0
208.1
508.0
817.6
1113.4
545.2
531.4
532.1
Freeway
0.0
29.6
20.3
27.9
26.7
9.8
24.4
24.1
25.2
Frostbite
65.2
4334.7
254.7
866.8
1181.3
331.1
1821.5
1855.6
1699.4
Gopher
257.6
2412.5
771.0
349.5
669.3
636.3
715.2
1010.0
912.1
Hero
1027.0
30826.4
2656.6
6857.0
6279.3
3736.3
7019.2
5775.2
6118.5
Jamesbond
29.0
302.8
125.3
301.6
471.0
236.0
365.4
312.8
319.7
Kangaroo
52.0
3035.0
323.1
779.3
872.5
940.6
3276.4
3569.3
3296.0
Krull
1598.0
2665.5
4539.9
2851.5
4229.6
4018.1
3688.9
5614.5
5467.7
Kung Fu Master
258.5
22736.3
17257.2
14346.1
14307.8
9111.0
13192.7
18511.0
17510.9
Ms Pacman
307.3
6951.6
1480.0
1204.1
1465.5
960.5
1313.2
1317.1
1663.5
Pong
-20.7
14.6
12.8
-19.3
-16.5
-8.5
-5.9
-6.0
-6.1
Private Eye
24.9
69571.3
58.3
97.8
218.4
-13.6
124.0
76.6
88.9
Qbert
163.9
13455.0
1288.8
1152.9
1042.4
854.4
669.1
773.8
814.9
Road Runner
11.5
7845.0
5640.6
9600.0
5661.0
8895.1
14220.5
13385.0
13708.8
Seaquest
68.4
42054.7
683.3
354.1
384.5
301.2
583.1
650.3
697.9
Up N Down
533.4
11693.2
3350.3
2877.4
2955.2
3180.8
28138.5
44295.4
52118.4"
D ROTATION MATRIX THAT IS RIJ,0.8553191489361702,"Mean Human-Norm’d
0.000
1.000
0.443
0.285
0.381
0.357
0.704
0.859
0.886
Median Human-Norm’d
0.000
1.000
0.144
0.161
0.175
0.268
0.415
0.418
0.398"
D ROTATION MATRIX THAT IS RIJ,0.8595744680851064,"# Superhuman games
0
N/A
2
2
2
2
7
8
7"
D ROTATION MATRIX THAT IS RIJ,0.8638297872340426,Under review as a conference paper at ICLR 2022
D ROTATION MATRIX THAT IS RIJ,0.8680851063829788,"Table 3: Mean game scores on the 26 Atari games after 100K environment steps for different choices
of subgroup blocks, averaged over 10 seeds. The reward loss, LR, is included in addition to the
default loss LAET ."
D ROTATION MATRIX THAT IS RIJ,0.8723404255319149,"Game
Random
Human
MLP
EqR, T2
EqR, SE2
EqR, GL2
Alien
227.8
7127.7
780.1
846.4
774.0
881.3
Amidar
5.8
1719.5
143.3
139.7
140.9
132.2
Assault
222.4
742.0
701.5
684.0
753.8
692.3
Asterix
210.0
8503.3
973.6
1004.4
923.2
889.5
Bank Heist
14.2
753.1
402.1
353.5
395.1
430.2
BattleZone
2360.0
37187.5
12722.6
11500.0
13044.0
13114.0
Boxing
0.1
12.1
38.0
28.9
37.5
33.4
Breakout
1.7
30.5
16.2
14.8
17.2
15.4
ChopperCommand
811.0
7387.8
989.5
1028.9
1073.5
1088.1
Crazy Climber
10780.5
35829.4
43705.8
50822.1
49399.0
55018.5
Demon Attack
152.1
1971.0
518.6
544.2
531.4
510.2
Freeway
0.0
29.6
20.3
18.5
24.1
21.4
Frostbite
65.2
4334.7
1702.4
1653.8
1855.6
1797.7
Gopher
257.6
2412.5
720.2
1012.5
1010.0
894.4
Hero
1027.0
30826.4
6840.0
5779.8
5775.2
5934.7
Jamesbond
29.0
302.8
337.4
313.25
312.8
334.8
Kangaroo
52.0
3035.0
2994.8
2942.5
3569.3
3186.4
Krull
1598.0
2665.5
3801.5
5293.0
5614.5
5772.6
Kung Fu Master
258.5
22736.3
13780.4
14924.2
18511.0
16002.8
Ms Pacman
307.3
6951.6
1220.8
1166.8
1317.1
1147.7
Pong
-20.7
14.6
-6.1
-11.5
-6.0
-8.2
Private Eye
24.9
69571.3
72.4
65.1
76.6
55.9
Qbert
163.9
13455.0
678.4
763.2
773.8
635.2
Road Runner
11.5
7845.0
12765.2
13654.2
13385.0
12560.4
Seaquest
68.4
42054.7
656.9
647.3
650.3
633.3
Up N Down
533.4
11693.2
23130.6
58164.4
44295.4
43767.2"
D ROTATION MATRIX THAT IS RIJ,0.8765957446808511,"Mean Human-Norm’d
0.000
1.000
0.681
0.829
0.859
0.833
Median Human-Norm’d
0.000
1.000
0.398
0.361
0.418
0.380"
D ROTATION MATRIX THAT IS RIJ,0.8808510638297873,"# Superhuman games
0
N/A
6
6
8
7"
D ROTATION MATRIX THAT IS RIJ,0.8851063829787233,"Table 4: Mean game scores on the 26 Atari games after 100K environment steps for EqR using
SE(2) subgroup blocks with different loss terms included in the training objective. The action
equivariance transition loss, LAET , is included for all EqR models and the scores are averaged over
10 seeds."
D ROTATION MATRIX THAT IS RIJ,0.8893617021276595,"Game
Random
Human
EqR
EqR, LR
EqR, LGET
EqR, LR + LGET
Alien
227.8
7127.7
856.5
774.0
862.5
872.9
Amidar
5.8
1719.5
134.7
140.9
135.0
138.4
Assault
222.4
742.0
643.1
753.8
701.3
734.3
Asterix
210.0
8503.3
824.8
923.2
864.9
902.5
Bank Heist
14.2
753.1
407.3
395.1
335.9
397.4
BattleZone
2360.0
37187.5
12805.6
13044.0
12990.4
13255.0
Boxing
0.1
12.1
32.7
37.5
34.8
39.2
Breakout
1.7
30.5
14.6
17.2
14.8
16.0
ChopperCommand
811.0
7387.8
1015.6
1073.5
934.8
1142.2
Crazy Climber
10780.5
35829.4
38483.9
49399.0
43085.6
52008.1
Demon Attack
152.1
1971.0
523.8
531.4
504.6
532.1
Freeway
0.0
29.6
22.1
24.1
22.5
25.2
Frostbite
65.2
4334.7
1635.2
1855.6
1563.9
1699.4
Gopher
257.6
2412.5
695.3
1010.0
789.3
912.1
Hero
1027.0
30826.4
5763.9
5775.2
5603.8
6118.5
Jamesbond
29.0
302.8
388.4
312.8
344.9
319.7
Kangaroo
52.0
3035.0
2667.9
3569.3
2848.7
3296.0
Krull
1598.0
2665.5
4209.2
5614.5
4411.2
5467.7
Kung Fu Master
258.5
22736.3
12287.9
18511.0
16394.6
17510.9
Ms Pacman
307.3
6951.6
1141.3
1317.1
1514.7
1663.5
Pong
-20.7
14.6
-9.9
-6.0
-6.5
-6.1
Private Eye
24.9
69571.3
73.2
76.6
87.5
88.9
Qbert
163.9
13455.0
696.7
773.8
736.8
814.9
Road Runner
11.5
7845.0
12659.2
13385.0
13110.4
13708.8
Seaquest
68.4
42054.7
593.6
650.3
641.0
697.9
Up N Down
533.4
11693.2
29425.4
44295.4
39076.6
52118.4"
D ROTATION MATRIX THAT IS RIJ,0.8936170212765957,"Mean Human-Norm’d
0.000
1.000
0.682
0.859
0.749
0.886
Median Human-Norm’d
0.000
1.000
0.337
0.418
0.377
0.398"
D ROTATION MATRIX THAT IS RIJ,0.8978723404255319,"# Superhuman games
0
N/A
6
8
6
7"
D ROTATION MATRIX THAT IS RIJ,0.902127659574468,Under review as a conference paper at ICLR 2022
D ROTATION MATRIX THAT IS RIJ,0.9063829787234042,"B.2
MODEL SCHEMATIC"
D ROTATION MATRIX THAT IS RIJ,0.9106382978723404,"Figure 5: A schematic of the EqR model, applied to model-free RL. Green in the framework cor-
responds to learning equivariance under the agent’s action and red corresponds to learning equiv-
ariance of the transition model with respect to symmetry transformation of the state-action. This
color scheme is consistent with Figure 2. The part of the framework that corresponds to reward
matching and Q-learning is shown in blue and brown respectively. The arrows in the schematic are
differentiated by their heads and are described in the legend."
D ROTATION MATRIX THAT IS RIJ,0.9148936170212766,"B.3
NETWORK ARCHITECTURE"
D ROTATION MATRIX THAT IS RIJ,0.9191489361702128,"We follow the baseline RL implementation of DrQ (Yarats et al., 2021) and SPR (Schwarzer et al.,
2021) by using the 3-layer convolutional encoder from (Mnih et al., 2015) and then use a linear layer
to get the parameters for the Group Parameterization. The output size of this layer varies depending
on the group type, the number of blocks used and the size of the group. This deﬁnes our τθ. Note
that the output of our encoder is a matrix for GL(n) and SE(n). We ﬂatten it before we feed to
other neural network like the Q-head qξ(⋅)."
D ROTATION MATRIX THAT IS RIJ,0.9234042553191489,"For the action encoder κ(⋅) we use a simple 1 layer MLP with batchnorm, ReLU and a hidden
size of 256. We concatenate the one-hot encodings of the actions with the state representations
coming from τθ and pass it through the action encoder to get matrix representation of the group after
parameterization."
D ROTATION MATRIX THAT IS RIJ,0.9276595744680851,"For the reward predictor network rψ we use a 2-layered MLP with batchnorm, ReLU and a hidden
size of 256."
D ROTATION MATRIX THAT IS RIJ,0.9319148936170213,For the Q-head qξ(⋅) we use 2-layered MLP as well.
D ROTATION MATRIX THAT IS RIJ,0.9361702127659575,"For the projection head pζ(⋅) we share the ﬁrst layer of Q-head whereas for projection head bη(⋅)
we use a single layer MLP."
D ROTATION MATRIX THAT IS RIJ,0.9404255319148936,Under review as a conference paper at ICLR 2022
D ROTATION MATRIX THAT IS RIJ,0.9446808510638298,"B.4
ALGORITHM"
D ROTATION MATRIX THAT IS RIJ,0.948936170212766,"Algorithm 1: Equivariant Representations for RL
Denote the parameters of online networks τθ, κφ, pζ,bη as Θo
Denote the parameters of target networks τ, κ, p,b as Θc
Denote the parameters of networks ρω, qξ as Φ
Denote the dept of the prediction as M and batch size as N
Initialize the replay buffer B
while Training do"
D ROTATION MATRIX THAT IS RIJ,0.9531914893617022,"Collect {s,a,r,s′} using policy with (Θo,Φ) and add to the buffer B
Sample a minibatch of M length sequences {s0∶M,a0∶M,r0∶M} ∼B
for i in range(0,N) do"
D ROTATION MATRIX THAT IS RIJ,0.9574468085106383,if augmentation then
D ROTATION MATRIX THAT IS RIJ,0.9617021276595744,"si
0∶M ←augment(si
0∶M)
end
xi
0 ←τθ(si
0);
// state representation
ˆxi
0 ←xi
0
li ←0
for k in range(1,M + 1) do"
D ROTATION MATRIX THAT IS RIJ,0.9659574468085106,"ˆxi
k ←κφ(ˆxi
k−1,ai
k−1)ˆxi
k−1 ;
// state transition by group action
¯xi
k ←τ(si
k);
// target state representation
ˆzi
k ←pζ(ˆxi
k), ¯zi
k ←pζ(¯xi
k);
// projections
li ←li + λ2∥
ˆzt+k
∥ˆzt+k∥2 −
¯zt+k
∥¯zt+k∥2 ∥2
2;
// compute LAET at step k
ˆri
k ←rψ(ˆxi
k);
// predict rewards
li ←li + λ1∥ˆri
k −ri
k∥2
2;
// compute LR at step k
end
j ∼{0,..,N −1};
// uniformly sample an index
¯xj
0 ←τ(sj
0); // encode the state for that index from the batch
τ i
g = ¯xj
0xi
0
−1;
// find the group representation
ˆxj
1 ←τ i
gˆxi
1;
// next state by group action
¯xj
1 ←ρω(τ i
g)κ(xi
0,ai
0)¯xj
0;
// next state by action-embedding
ˆyi
1 ←bη(ˆxj
1), ¯yi
1 ←b(¯xj
1);
// projections"
D ROTATION MATRIX THAT IS RIJ,0.9702127659574468,"li ←li + λ3∥
ˆyi
1
∥ˆyi
1∥2 −
¯yi
1
∥¯yi
1∥2 ∥2
2;
// compute LGET
li ←li + RLloss(ˆxi
0,ai
0,ri
0, ¯xi
1;qξ)
end
l ←1"
D ROTATION MATRIX THAT IS RIJ,0.9744680851063829,"N ∑N
i=0 li;
// average over minibatch
Θo,Φ ←optmize((Θo,Φ),l);
// update online networks
Θc ←Θo;
// copy weights to target networks
end"
D ROTATION MATRIX THAT IS RIJ,0.9787234042553191,Under review as a conference paper at ICLR 2022
D ROTATION MATRIX THAT IS RIJ,0.9829787234042553,"B.5
HYPERPARAMETERS"
D ROTATION MATRIX THAT IS RIJ,0.9872340425531915,"In this section, we provide the full set of hyperparameters in our model. As mentioned earlier, our
baseline RL algorithm closely follows SPR’s (Schwarzer et al., 2021) implementation of Rainbow
and hence we use most of their hyperparameters setting in order to be able to compare to them. Note
that the weights of LR - λ1, LAET - λ2 and LGET - λ3 are set to one whenever they are used in the
model."
D ROTATION MATRIX THAT IS RIJ,0.9914893617021276,Table 5: Hyperparameters for ErQ (including variations) on Atari.
D ROTATION MATRIX THAT IS RIJ,0.9957446808510638,"Parameter
Setting
Gray-scaling
True
Observation down-sampling
84 × 84
Frames stacked
4
Action repetitions
4
Reward clipping
[−1,1]
Terminal on loss of life
True
Max frames per episode
108K
Update
Distributional Q
Dueling
True
Support of Q-distribution
51
Discount factor
0.99
Minibatch size
32
Optimizer
Adam
Optimizer: learning rate
0.0001
Optimizer: β1
0.9
Optimizer: β2
0.999
Optimizer: ϵ
0.00015
Max gradient norm
10
Priority exponent
0.5
Priority correction
0.4 →1
Exploration
Noisy nets
Noisy nets parameter
0.5
Training steps
100K
Evaluation trajectories
100
Min buffer size for sampling
2000
Replay period every
1 step
Updates per step
2
Multi-step return length
10
Prediction depth, M
5
λ1
1
λ2
1
λ3
1
Data Augmentation
Random shifts (±4 pixels)
Intensity(scale=0.05)
Parameter
Setting (T2)
Setting (SE2)
Setting (GL2)
Num Blocks (K)
32
12
12
Group Action
Addition
MatMul
MatMul"
