Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003703703703703704,"Meta-reinforcement learning (RL) methods can meta-train policies that adapt to
new tasks with orders of magnitude less data than standard RL, but meta-training
itself is costly and time-consuming. If we can meta-train on ofﬂine data, then we
can reuse the same static dataset, labeled once with rewards for different tasks, to
meta-train policies that adapt to a variety of new tasks at meta-test time. Although
this capability would make meta-RL a practical tool for real-world use, ofﬂine
meta-RL presents additional challenges beyond online meta-RL or standard ofﬂine
RL settings. Meta-RL learns an exploration strategy that collects data for adapting,
and also meta-trains a policy that quickly adapts to data from a new task. Since this
policy was meta-trained on a ﬁxed, ofﬂine dataset, it might behave unpredictably
when adapting to data collected by the learned exploration strategy, which differs
systematically from the ofﬂine data and thus induces distributional shift. We
propose a hybrid ofﬂine meta-RL algorithm, which uses ofﬂine data with rewards
to meta-train an adaptive policy, and then collects additional unsupervised online
data, without any reward labels to bridge this distribution shift. By not requiring
reward labels for online collection, this data can be much cheaper to collect.
We compare our method to prior work on ofﬂine meta-RL on simulated robot
locomotion and manipulation tasks and ﬁnd that using additional unsupervised
online data collection leads to a dramatic improvement in the adaptive capabilities
of the meta-trained policies, matching the performance of fully online meta-RL on
a range of challenging domains that require generalization to new tasks."
INTRODUCTION,0.007407407407407408,"1
INTRODUCTION"
INTRODUCTION,0.011111111111111112,"Reinforcement learning (RL) agents are often described as learning from reward and punishment
analogously to animals: in the same way that a person might train a dog by providing treats, we
might train RL agents by providing rewards. However, in reality, modern deep RL agents require so
many trials to learn a task that providing rewards by hand is often impractical. Meta-reinforcement
learning in principle can mitigate this, by learning to learn using a set of meta-training tasks, and
then acquiring new behaviors in just a few trials at meta-test time. Current meta-RL methods are
so efﬁcient that meta-trained policies require only a handful of trajectories (Rakelly et al., 2019),
which is reasonable for a human to provide by hand. However, the meta-training phase in these
algorithms still requires a large number of online samples, often even more than standard RL, due to
the multi-task nature of the meta-learning problem."
INTRODUCTION,0.014814814814814815,"Ofﬂine reinforcement learning methods, which use only prior experience without active data col-
lection, provide a potential solution to this issue, because a user must only annotate multi-task data
with rewards once in the ofﬂine dataset, rather than doing so in the inner loop of RL training, and
the same ofﬂine multi-task data can be reused repeatedly for many training runs. While a few recent
works have proposed ofﬂine meta-RL algorithms Dorfman & Tamar (2020); Mitchell et al. (2021),
we identify a speciﬁc problem when an agent trained with ofﬂine meta-RL is tested on a new task:
the distributional shift between the behavior policy from the ofﬂine data and the meta-test time
exploration policy means that adaptation procedures learned from ofﬂine data might not perform
well on the (differently distributed) data collected by the exploration policy at meta-test time. This
mismatch in training distribution occurs because ofﬂine meta-RL never trains on data generated by the
meta-learned exploration policy. In practice, we ﬁnd that this mismatch leads to a large degradation
in performance when adapting to new tasks. Moreover, we do not want to remove this distributional"
INTRODUCTION,0.018518518518518517,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.022222222222222223,"Figure 1: (left) In ofﬂine meta-RL, an agent uses ofﬂine data from multiple tasks T1, T2, . . . , each with reward
labels that must only be provided once. (middle) In online meta-RL, new reward supervision must be provided
with every environment interaction. (right) In semi-supervised meta-RL, an agent uses an ofﬂine dataset collected
once to learn to generate its own reward labels for new, online interactions. Similar to ofﬂine meta-RL, reward
labels must only be provided once for the ofﬂine training, and unlike online meta-RL, the additional environment
interactions require neither external reward supervision nor additional task sampling."
INTRODUCTION,0.025925925925925925,"shift by simply adopting a conservative exploration strategy, because learning an exploration strategy
enables an agent to collect better data for faster adaptation."
INTRODUCTION,0.02962962962962963,"We propose to address this challenge by collecting additional online data without any reward super-
vision, leading to a semi-supervised ofﬂine meta-RL algorithm, as illustrated in Figure 1. Online
data can be relatively cheap to collect when it does not require reward labels, but it can still make it
possible to bridge the distributional shift issue. To make it feasible to use this data for meta-training,
we can generate synthetic reward labels for it based on the labeled ofﬂine data."
INTRODUCTION,0.03333333333333333,"Based on this principle, we propose semi-supervised meta actor-critic (SMAC), which uses reward-
labeled ofﬂine data to bootstrap a semi-supervised meta-reinforcement learning procedure, in which
an ofﬂine meta-RL agent collects additional online experience without any reward labels. SMAC
uses the reward supervision from the ofﬂine dataset to learn to generate new reward functions, which
it uses to autonomously annotate rewards in these rewardless interactions and meta-train on this
new data. Our method contains two novel contributions: ﬁrst, it is the ﬁrst method to combine the
efﬁcient PEARL (Rakelly et al., 2019) amortized inference method for meta-RL with AWAC (Nair
et al., 2020), an effective approach for ofﬂine RL with online ﬁnetuning. This already yields an
effective new ofﬂine meta-RL procedure. Second, it is the ﬁrst approach to perform ofﬂine meta-RL
with self-supervised online ﬁnetuning, without ground truth rewards. We evaluate our method and
prior ofﬂine meta-RL methods on a number of benchmarks (Dorfman & Tamar, 2020; Mitchell et al.,
2021), as well as a challenging robot manipulation domain that requires generalization to new tasks,
with just a few reward-labeled trials at meta-test time. We ﬁnd that, while standard meta-RL methods
perform well at adapting to training tasks, they suffer from data-distribution shifts when adapting to
new tasks. In contrast, our method attains signiﬁcantly better performance, on par with an online
meta-RL method that receives fully labeled online interaction data."
RELATED WORKS,0.037037037037037035,"2
RELATED WORKS"
RELATED WORKS,0.040740740740740744,"Many prior meta-RL algorithms assume that reward labels are provided with each episode of online
interaction (Duan et al., 2016; Finn et al., 2017; Gupta et al., 2018b; Xu et al., 2018; Hausman et al.,
2018; Rakelly et al., 2019; Humplik et al., 2019; Kirsch et al., 2019; Zintgraf et al., 2020; Xu et al.,
2020; Zhao et al., 2020; Kamienny et al., 2020). In contrast to these prior methods, our method
only requires ofﬂine prior data with rewards, and additional online interaction does not require any
ground truth reward signal. Prior works have also studied other formulations that combine unlabeled
and labeled trials. For example, imitation and inverse reinforcement learning methods use ofﬂine
demonstrations to either learn a reward function (Abbeel & Ng, 2004; Finn et al., 2016; Ho &
Ermon, 2016; Fu et al., 2017) or to directly learn a policy (Schaal, 1999; Ross & Bagnell, 2010; Ho &
Ermon, 2016; Reddy et al., 2019; Peng et al., 2020). Semi-supervised and positive-unlabeled reward
learning (Xu & Denil, 2019; Zolna et al., 2020; Konyushkova et al., 2020) methods use reward labels
provided for some interactions to train a reward function for RL. However, all of these methods have
been studied in the context of a single task. In contrast, we focus on meta-learning an RL procedure
that can adapt to new reward functions. In other words, we do not focus on recovering a single reward
function, because there is no single test time reward or task."
RELATED WORKS,0.044444444444444446,"SMAC uses a context-based adaptation procedure similar to that proposed by Rakelly et al. (2019),
which is related to contextual policies, such as goal-conditioned reinforcement learning (Kaelbling,
1993; Schaul et al., 2015; Andrychowicz et al., 2017; Pong et al., 2018; Colas et al., 2018; Warde-"
RELATED WORKS,0.04814814814814815,Under review as a conference paper at ICLR 2022
RELATED WORKS,0.05185185185185185,"Farley et al., 2018; Péré et al., 2018; Nair et al., 2018) or successor features (Kulkarni et al., 2016;
Barreto et al., 2017; 2019; Grimm et al., 2019). In contrast, our meta-learning procedure applies
to any RL problem, does not assume that the reward is deﬁned by a single goal state or ﬁxed basis
function, and only requires reward labels for static ofﬂine data."
RELATED WORKS,0.05555555555555555,"Our method addresses a similar problem to prior ofﬂine meta-RL methods (Mitchell et al., 2021;
Dorfman & Tamar, 2020). In our comparisons, we ﬁnd that after ofﬂine meta-training, our method is
competitive to these prior approaches, but that with additional self-supervised online ﬁne-tuning, our
method signiﬁcantly outperforms these methods by mitigating the aforementioned distributional shift
issue. Our method addresses the distribution shift problem by using online interactions without reward
supervision. In our experiments, we found that SMAC greatly improves performance on both training
and held-out tasks. Lastly, SMAC is also related to unsupervised meta-learning methods (Gupta et al.,
2018a; Jabri et al., 2019), which annotate data with their own rewards. In contrast to these methods,
we assume that there exists an ofﬂine dataset with reward labels that we can use to learn to generate
similar rewards."
PRELIMINARIES,0.05925925925925926,"3
PRELIMINARIES"
PRELIMINARIES,0.06296296296296296,"Meta-reinforcement learning.
In meta-RL, we assume there is a distribution of tasks pT (·). A
task T is a Markov decision process (MDP), deﬁned by a tuple T = (S, A, r, γ, p0, pd), where S
is the state space, A is the action space, r is a reward function, γ is a discount factor, p0(s0) is the
initial state distribution, and pd(st+1 | st, at) is the environment dynamics distribution. A replay
buffer D is a set of state, action, reward, next-states tuples, D = {si, ai, ri, s′
i}Nsize
i=1 , where all the
rewards come from the same task. We will use the letter h to denote a mini-batch or “history” and
the notation h ∼D to denote that a mini-batch h is sampled from a replay buffer D. We will use the
letter τ to represent a trajectory τ = (s1, a1, s2, . . . ) without reward labels."
PRELIMINARIES,0.06666666666666667,"A meta-episode consists of sampling a task T ∼pT (·), collecting T trajectories with a policy πθ with
parameters θ, adapting the policy to the task between trajectories, and measuring the performance
on the last trajectory. Between trajectories, the adaptation procedure transforms the history of states
and actions h from the current meta-episode into a context z = Aφ(h), which is then given to the
policy πθ(a, | s, z) for adaptation. The exact representation of πθ, Aφ, and z depends on the speciﬁc
meta-RL method used. For example, the context z can be weights of a neural network (Finn et al.,
2017) outputted by a gradient update, hidden activations ouputted by a recurrent neural network (Duan
et al., 2016), or latent variables outputted by a stochastic encoder (Rakelly et al., 2019). Using this
notation, the objective in meta-RL is to learn the adaptation parameters φ and policy parameters θ to
maximize performance on a meta-episode given a new task T sampled from p(T )."
PRELIMINARIES,0.07037037037037037,"PEARL.
Since we require an off-policy meta-RL procedure for ofﬂine meta-training, we build on
probabilistic embeddings for actor-critic RL (PEARL) (Rakelly et al., 2019), an online off-policy
meta-RL algorithm. In PEARL, z is a vector and the adaptation procedure Aφ consists of sampling z
from a distribution z ∼qφe(z | h). The distribution qφe is generated by an encoder with parameters
φe. This encoder is a set-based neural network that processes the tuples in h = {si, ai, ri, s′
i}Nenc
i=1 in a
permutation-invariant manner to produce the mean and variance of a diagonal multivariate Gaussian.
The policy is a contextual policy πθ(a | s, z) conditioned on z by concatenating z to the state s."
PRELIMINARIES,0.07407407407407407,"The policy parameter θ is trained using soft-actor critic (Haarnoja et al., 2018) which involves
learning a Q-function, Qw(s, a, z), with parameter w that estimates the sum of future discounted
rewards conditioned on the current state, action, and context. The encoder parameters are trained by
back-propagating the critic loss into the encoder. The actor, critic, and encoder losses are minimized
via gradient descent with mini-batches sampled from separate replay buffers for each task."
PRELIMINARIES,0.07777777777777778,"Ofﬂine reinforcement learning.
In ofﬂine reinforcement learning, we assume that we have access
to a dataset D collected by some behavior policy πβ. An RL agent must train on this ﬁxed dataset
and cannot interact with the environment. One challenge that ofﬂine RL poses is that the distribution
of states and actions that an agent will see when deployed will likely be different from those seen in
the ofﬂine dataset as they are generated by the agent, and a number of recent methods have tackled
this distribution shift issue (Fujimoto et al., 2019b;a; Kumar et al., 2019; Wu et al., 2019; Nair et al.,
2020; Levine et al., 2020). Moreover, one can combine ofﬂine RL with meta-RL by training meta-RL"
PRELIMINARIES,0.08148148148148149,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.08518518518518518,"oﬄine buﬀer
online policy
data source 2 4 6"
PRELIMINARIES,0.08888888888888889,DKL(qφ(z | h) || p(z))
PRELIMINARIES,0.09259259259259259,Ant Direction: Online vs Oﬄine z-Posteriors
PRELIMINARIES,0.0962962962962963,"0
10000
20000
30000
40000
50000
Number of gradient updates 150 200 250"
PRELIMINARIES,0.1,Post-Adaptation Returns
PRELIMINARIES,0.1037037037037037,Ant Direction: Adapting from Online vs Oﬄine Data
PRELIMINARIES,0.10740740740740741,"data source
oﬄine buﬀer
online policy"
PRELIMINARIES,0.1111111111111111,"Figure 2: Left: The distribution of the KL-divergence between the posterior qφe(z | h) and the prior p(z) over
the course of meta-training, when conditioned on data from the ofﬂine dataset (blue) or on online data from the
learned policy (orange). Data from the learned policy results in posteriors that are substantially farther from the
prior, suggesting a signiﬁcant difference in distribution over z. Note that online data from the learned policy
is not available for meta-training, but only used for measurement. Right: The performance of the policy after
adaptation when adapted using data from the ofﬂine dataset (blue) or the learned policy (orange). Although
the meta-RL policy adapts well when conditioned on z sampled from the ofﬂine dataset, the performance does
not increase when z is sampled from the learned policy. Since the same policy is evaluated, the change in
z-distribution is likely the cause for the drop in performance."
PRELIMINARIES,0.11481481481481481,"on multiple datasets D1, . . . , DNbuff (Dorfman & Tamar, 2020; Mitchell et al., 2021), but in the next
section we describe some limitations of this combination."
PRELIMINARIES,0.11851851851851852,"4
THE PROBLEM WITH NAÏVE OFFLINE META-REINFORCEMENT LEARNING"
PRELIMINARIES,0.12222222222222222,"Ofﬂine meta-RL is the composition of meta-RL and ofﬂine RL: the objective is to maximize the
standard meta-RL objective using only a ﬁxed set of replay buffers, D = {Di}Nbuff
i=1 , where each
buffer corresponds to data for one task. Ofﬂine meta-RL methods can in principle utilize the same
constraint-based approaches that standard ofﬂine RL algorithms have used to mitigate distributional
shift. However, they must also contend with an additional distribution shift challenge that is speciﬁc
to the meta-RL scenario: distribution shift in z-space."
PRELIMINARIES,0.1259259259259259,"Distribution shift in z-space occurs because meta-learning requires learning an exploration policy πθ
that generates data for adaptation. However, ofﬂine meta-RL only trains the adaptation procedure
Aφ(h) using ofﬂine data generated by a previous behavior policy, which we denote as πβ. After
ofﬂine training, there will be a mismatch between this learned exploration policy πθ and the behavior
policy πβ, leading to a difference in the history h and in turn, in the context variables z = Aφ(h). For
example, in a robot manipulation setting, the ofﬂine dataset may contain smooth trajectories that were
collected by a human teleoperator (Kofman et al., 2005). In contrast, the learned exploration may
contain jittering due learning artifacts or the use of a stochastic policy. This jittering may not impede
the robot from exploring the environment, but may result in a trajectory distribution shift that degrades
the adaptation process, which only learned to adapt to smooth, human-generated trajectories in the
ofﬂine dataset. More formally, if p(z | hofﬂine) and p(z | honline) denote the marginal distribution of z
given histories h sampled using ofﬂine and online data, respectively, the differences between πθ and
πβ will lead to differences between p(z | hofﬂine) during ofﬂine training and p(z | honline) at meta-test
time."
PRELIMINARIES,0.12962962962962962,"To illustrate this difference, we compare p(z | hofﬂine) and p(z | honline) on the Ant Direction task
(see Section 6). We approximate these distributions by using the PEARL encoder discussed in
Section 3, with p(z | h) ≈qφe(z | h), where h is sampled either from the ofﬂine data set or using
the learned policy. We measure the KL-divergence observed at the end of ofﬂine training between the
posterior p(z | h) and a ﬁxed prior pz(z), for different samples of h. If the two distributions were the
same, then we would expect the distribution of KL divergences to also be similar. However, Figure 2
shows that the two distributions are markedly different after the ofﬂine training phase of SMAC."
PRELIMINARIES,0.13333333333333333,"We also observe that this distribution shift negatively impacts the resulting policy. In Figure 2, we plot
the performance of the learned policy when conditioned on z sampled from qφe(z | hofﬂine) compared
to qφe(z | honline). We see that the policy that uses ofﬂine data, hofﬂine, leads to improvement, while"
PRELIMINARIES,0.13703703703703704,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.14074074074074075,"Figure 3: (Left) In the ofﬂine phase, we sample a history h′ to compute the posterior qφe(z | h′). We then
use a sample from this encoder and another history batch h to train the networks. In red, we then update the
networks with h and the z sample. (Right) During the self-supervised phase, we explore by sampling z ∼p(z)
and conditioning our policy on these observations. We label rewards using our learned reward decoder, and
append the resulting data to the training data. The training procedure is equivalent to the ofﬂine phase, except
that we do not train the reward decoder or encoder since no additional ground-truth rewards are observed."
PRELIMINARIES,0.14444444444444443,"the same policy that uses data from the learned policy, honline drops in performance. Since we evaluate
the same policy πθ and only change how z is sampled, this degradation in performance suggests
that the policy suffers from distributional shift between p(z | hofﬂine) and p(z | honline). In other
words, the encoder produces z vectors that are too unfamiliar to the policy when conditioned on these
exploration trajectories."
PRELIMINARIES,0.14814814814814814,"We note that this issue arises in any method that trains non-Markovian policies with ofﬂine data. For
example, recurrent policies for partially observed MDPs (Jaakkola et al., 1995) depend both on the
current observation o and a history h. When deployed, these policies must also contend with potential
distributional shifts between the training and test-time history distributions, in addition to the change
in observation distribution o. This additional distribution shift may explain why many memory-based
recurrent policies are often trained online (Duan et al., 2016; Heess et al., 2017; Espeholt et al., 2018)
or have beneﬁted from refreshing the memory states (Kapturowski et al., 2018). In this paper, we
focus on addressing this issue speciﬁcally in the ofﬂine meta-RL setting."
PRELIMINARIES,0.15185185185185185,"Ofﬂine meta-RL with self-supervised online training.
In complex environments where many
behaviors are possible, the distribution shift in z-space will likely be inevitable, since the learned
policy is likely to deviate from the behavior policy. To address this issue, we introduce an additional
assumption: in addition to the ofﬂine dataset, we assume that the agent can autonomously interact
with the environment without observing additional reward supervision. This problem statement is
useful for scenarios where autonomously interacting with the world is relatively easy, but online
reward supervision is more expensive to obtain."
PRELIMINARIES,0.15555555555555556,"Formally, we assume that the agent can generate additional rollouts in an MDP without a reward
function, T \ r = (S, A, γ, p0, pd). These additional interactions enable the agent to explore using
the learned exploration policy. Since the resulting states and actions are from the learned exploration
policy, we can use them to construct honline and enable an agent to train on the online context
distribution, p(z | honline), thus mitigating the distribution shift issue described above. However,
meta-training requires that the history honline contain not just states and actions, but also rewards. In
the next section, we describe a method for autonomously labeling these rollouts with synthetic reward
labels to enable an agent to meta-train on this additional data."
SEMI-SUPERVISED META ACTOR-CRITIC,0.15925925925925927,"5
SEMI-SUPERVISED META ACTOR-CRITIC"
SEMI-SUPERVISED META ACTOR-CRITIC,0.16296296296296298,"In this section, we present our method, semi-supervised meta actor-critic (SMAC). SMAC consists of
ofﬂine meta-training followed by self-supervised online meta-training to mitigate the distribution
shift in z-space. The SMAC adaptation procedure consists of passing history through the encoder
described in Section 3, resulting in a posterior qφe(z | h). Below, we describe both phases."
OFFLINE META-TRAINING,0.16666666666666666,"5.1
OFFLINE META-TRAINING"
OFFLINE META-TRAINING,0.17037037037037037,"To learn from the user-provided ofﬂine data, we adapt the PEARL (Rakelly et al., 2019) to the ofﬂine
setting. Similar to PEARL, we update the critic by minimizing the Bellman error:"
OFFLINE META-TRAINING,0.17407407407407408,"Lcritic(w) = E(s,a,r,s′)∼Di,z∼qφe(z|h),a′∼πθ(a′|s′,z)

(Qw(s, a, z) −(r + γQ ¯
w(s′, a′, z)))2
,
(1)"
OFFLINE META-TRAINING,0.17777777777777778,"where ¯w are target network weights updated using soft target updates (Lillicrap et al., 2016)."
OFFLINE META-TRAINING,0.1814814814814815,"PEARL has a policy updated based on soft actor critic (SAC) (Haarnoja et al., 2018), but when
naïvely applied to the ofﬂine setting, these policy updates suffer from off-policy bootstrapping error"
OFFLINE META-TRAINING,0.18518518518518517,Under review as a conference paper at ICLR 2022
OFFLINE META-TRAINING,0.18888888888888888,"accumulation (Kumar et al., 2019), which occurs when the target Q-function for bootstrapping
Q(s′, a′) is evaluated at actions a′ outside of the training data."
OFFLINE META-TRAINING,0.1925925925925926,"To avoid error accumulation during ofﬂine training, we implicitly constrain the policy to stay close to
the actions observed in the replay buffer, following the approach in a single-task ofﬂine RL algorithm
called AWAC (Nair et al., 2020). AWAC uses the following loss to approximate a constrained
optimization problem, where the policy is constrained to stay close to the data observed in D:"
OFFLINE META-TRAINING,0.1962962962962963,"Lactor(θ) = −Es,a,s′∼D,z∼qφe(z|h)"
OFFLINE META-TRAINING,0.2,"
log πθ(a | s) exp
Q(s, a, z) −V (s′, z) λ"
OFFLINE META-TRAINING,0.2037037037037037,"
.
(2)"
OFFLINE META-TRAINING,0.2074074074074074,"We estimate the value function V (s, z) = Ea∼πθ(a|s,z)Q(s, a, z) with a single sample, and λ is the
resulting Lagrange multiplier for the optimization problem. See Nair et al. (2020) for a full derivation."
OFFLINE META-TRAINING,0.2111111111111111,"With this modiﬁed actor update, we can train the encoder, actor, and critic on the ofﬂine data
without the overestimation issues that afﬂict conventional actor-critic algorithms (Kumar et al., 2019).
However, it does not address the z-space distributional shift issue discussed in Section 4, because the
exploration policy learned via this ofﬂine procedure will still deviate signiﬁcantly from the behavior
policy πβ. As discussed previously, we will aim to address this issue by collecting additional online
data without reward labels and learning to generate reward labels if self-supervised meta-training."
OFFLINE META-TRAINING,0.21481481481481482,"Learning to generate rewards.
To continue meta-training online without ground truth reward
labels, we propose to use the ofﬂine dataset to learn a generative model over meta-training task
reward functions that we can use to label the transitions collected online. Recall that during ofﬂine
learning, we learn an encoder qφe that maps experience h to a latent context z that encodes the task.
In the same way that we train our policy πθ(a | s, z) that conditionally decodes z into actions, we
additionally train a reward decoder rφd(s, a, z) with parameter φd 1 that conditionally decodes z into
rewards. We train the reward decoder rφd to reconstruct the observed reward in the ofﬂine dataset
through a mean squared error loss."
OFFLINE META-TRAINING,0.21851851851851853,"Because we use the latent space z for reward-decoding, we back-propagate the reward decoder loss
into qφe. As visualized in Figure 3, we also regularize the posteriors qφe(z | h) against a prior
pz(z) to provide an information bottleneck in that latent space z and ensure that samples from pz(z)
represent meaningful latent variables. We found it beneﬁcial to not back-propagate the critic loss
into the encoder, in contrast to prior work such as PEARL. To summarize, we train the encoder and
reward decoder by minimizing the following loss, in which we assume that z ∼qφe(h):"
OFFLINE META-TRAINING,0.2222222222222222,"Lreward(φd, φe, h, z) =
X"
OFFLINE META-TRAINING,0.22592592592592592,"(s,a,r)∈h
∥r −rφd(s, a, z)∥2
2 + DKL

qφe(· | h)

pz(·)

.
(3)"
SELF-SUPERVISED ONLINE META-TRAINING,0.22962962962962963,"5.2
SELF-SUPERVISED ONLINE META-TRAINING"
SELF-SUPERVISED ONLINE META-TRAINING,0.23333333333333334,"We now describe the self-supervised online training procedure, during which we use the reward
decoder to provide supervision. First, we collect a trajectory τ by rolling out our exploration policy
πθ conditioned on a context sampled from the prior p(z). To emulate the ofﬂine meta-training
supervision, we would like to label τ with rewards that are in the distribution of meta-training tasks.
As such, we sample a replay buffer Di uniformly from D to get a history hofﬂine ∼Di from the ofﬂine
data. We then sample from the posterior z ∼qφe(z | hofﬂine) and label the reward rgenerated of a new
state and action, (s, a), using the reward decoder"
SELF-SUPERVISED ONLINE META-TRAINING,0.23703703703703705,"rgenerated = rφd(s, a, z),
where z ∼qφe(z | h).
(4)"
SELF-SUPERVISED ONLINE META-TRAINING,0.24074074074074073,"We then add the labeled trajectory to the buffer and perform actor and critic updates as in ofﬂine meta-
training. Lastly, since we do not observe additional ground-truth rewards, during the self-supervised
phase we do not update the reward decoder rφd or encoder qφe and instead only train the policy and
Q-function. We visualize this procedure in Figure 3."
SELF-SUPERVISED ONLINE META-TRAINING,0.24444444444444444,"We note that the distribution shift discussed in Section 4 is not an issue for the reward decoder. The
distribution shift only occurs when we sample z from the encoder using online data, i.e. z ∼qφe(z |
honline), but, we only sample from this reward decoder using z sampled from the encoder using ofﬂine"
SELF-SUPERVISED ONLINE META-TRAINING,0.24814814814814815,"1With this notation, meta-parameters are the encoder and decoder parameters (i.e., φ = {φe, φd})."
SELF-SUPERVISED ONLINE META-TRAINING,0.2518518518518518,Under review as a conference paper at ICLR 2022
SELF-SUPERVISED ONLINE META-TRAINING,0.25555555555555554,"Figure 4: We propose a new meta-learning evaluation domain based on the environment from Khazatsky et al.
(2021), in which a simulated Sawyer gripper can perform various manipulation tasks such as pushing a button,
opening drawers, and picking and placing objects. We show a subset of meta-training (blue) and meta-test
(orange) tasks. Each task contains a unique object conﬁguration, and we test the agent on held-out tasks."
SELF-SUPERVISED ONLINE META-TRAINING,0.25925925925925924,"data, i.e. z ∼qφe(z | hofﬂine). The reward decoder does need to generalize to new states and actions,
and we hypothesize that reward prediction generalization is easier than policy state, action, and z
generalization. If true, then we would expect that using the reward decoder to label rewards and train
on those labels, as in SMAC, will outperform directly using the policy on new tasks (as in Figure 2)."
ALGORITHM SUMMARY AND DETAILS,0.26296296296296295,"5.3
ALGORITHM SUMMARY AND DETAILS"
ALGORITHM SUMMARY AND DETAILS,0.26666666666666666,"We visualize SMAC in Figure 3. For ofﬂine training, we assume access to ofﬂine datasets D =
{Di}Nbuff
i=1 , where each buffer corresponds to data generated for one task. Each iteration, we sample a
buffer Di ∼D and a history from this buffer h ∼Di. We condition the stochastic encoder qφe on
this history to obtain a sample z ∼qφe(z | h). We then use this sample z and a second history sample
h′ ∼Di to update the Q-function, the policy, encoder, and decoder by minimizing Equation (1),
Equation (2), and Equation (3) respectively. During the self-supervised phase, we found it beneﬁcial
to train the actor with a combination of the loss in Equation (2) and the original PEARL actor loss,
weighted by hyperparameter λpearl. We provide pseudo-code for SMAC in Appendix A."
EXPERIMENTS,0.27037037037037037,"6
EXPERIMENTS"
EXPERIMENTS,0.2740740740740741,"We presented a method that uses self-supervised data to mitigate the z-space distribution shift that
occurs in ofﬂine meta-RL. In this section, we evaluate how well the self-supervised phase of SMAC
mitigates the resulting drop in performance, and we compare SMAC to prior ofﬂine meta-RL methods
on a range of meta-RL benchmark tasks that require generalization to unseen tasks at meta-test time."
EXPERIMENTS,0.2777777777777778,"Meta-RL tasks.
We ﬁrst evaluate our method on multiple simulated MuJoCo (Todorov et al., 2012)
meta-learning tasks that have been used in past online and ofﬂine meta-RL papers (Finn et al., 2017;
Rakelly et al., 2019; Dorfman & Tamar, 2020; Mitchell et al., 2021) (see Figure 9). Although there
are standardized benchmarks for ofﬂine RL (Fu et al., 2020) and meta-RL (Yu et al., 2020), there
are no standard benchmarks for ofﬂine meta-RL particular, so we use a combination of tasks in
prior work (Dorfman & Tamar, 2020; Mitchell et al., 2021), and introduce a more complex robotic
manipulation domain meant to push task generalization to the limit."
EXPERIMENTS,0.2814814814814815,"We ﬁrst evaluate ﬁve different domains, Cheetah Velocity, Ant Direction, Humanoid, Walker Param,
and Hopper Param, which have been used in prior work (Rothfuss et al., 2018; Rakelly et al.,
2019; Dorfman & Tamar, 2020). In the ﬁrst three domains, the agent is rewarded for moving in an
unobserved target velocity and a meta-episode consists of sampling a desired velocity. The last task,
Humanoid, is particularly challenging due to the high space dimension of 376. The last two domains,
Walker Param and Hopper Param, require adapting to different dynamics parameters, such as friction,
joint mass, and inertia. The agent must adapt to each task within T = 3 episodes, each of length 200."
EXPERIMENTS,0.2851851851851852,"We also evaluated SMAC on a signiﬁcantly more diverse robot manipulation meta-learning task
called Sawyer Manipulation, based on the goal-conditioned environment introduced by Khazatsky
et al. (2021). This is a simulated PyBullet environment Coumans & Bai (2016–2021) in which a
Sawyer robot arm can manipulate various objects. Sampling a task T ∼p(T ) involves sampling both
a new conﬁguration of the environment and the desired behavior to achieve, such as pushing a button,
opening a drawer, or lifting an object (see Figure 4). The sparse reward is 0 when the desired behavior
is achieved and −1 otherwise. The action space consists of 3 dimensions to control the end-effector
in Euclidean space and one dimension to control the gripper. The state is a 13-dimensional vector,"
EXPERIMENTS,0.28888888888888886,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.29259259259259257,"which we detail in Appendix C. The sparse reward, precise manipulation requirement, and diversity
of object conﬁguration between each task sample make this environment difﬁcult."
EXPERIMENTS,0.2962962962962963,"In all of the environments, we test the meta-RL procedure’s ability to generalize to new tasks by
evaluating the policies on held-out tasks sampled from the same distribution as in the ofﬂine datasets.
We give a complete description of environments and task distribution in Appendix C."
EXPERIMENTS,0.3,"Ofﬂine data collection.
For the MuJoCo tasks, we use the replay buffer from a single PEARL
run that uses the ground-truth reward. We limit the data collection to 1200 transitions (i.e., 6
trajectories) per task and terminate the PEARL run early, forcing the meta-RL agent to learn from
highly suboptimal data. For Sawyer Manipulation, we collect data using a scripted policy that
randomly performs one of many possible tasks in the environment. We used 50 training tasks and 50
trajectories of length 75 each. Note that the performed task (e.g. open drawer) may differ from the
task that is rewarded (e.g. push a button). As a result, in the ofﬂine dataset, the robot succeeds on
the task in 46% of the transitions, indicating that this data is highly suboptimal. In contrast to prior
work (Dorfman & Tamar, 2020; Mitchell et al., 2021), we use this limited data because it enables us
to test how well the different methods can improve over suboptimal, ofﬂine data. See Appendix C for
further discussion and details."
EXPERIMENTS,0.3037037037037037,"Comparisons and ablations.
As an upper bound, we include the performance of PEARL with
online training using oracle ground-truth rewards rather than self-generated rewards, which we label
Online Oracle. To understand the importance of using the actor loss in Equation (2), we include
an ablation that replaces Equation (2) with the actor loss from PEARL, which we label SMAC
(actor ablation). We also include a meta-imitation baseline, labeled meta behavior cloning, which
replaces the actor update in Equation (2) with simply maximizing log πθ(a | s, z). A gap between
SMAC and this imitation method would help us understand whether or not our method improves
over the (possibly sub-optimal) behavior policy. For comparisons to prior work, we include the two
previously proposed ofﬂine meta-RL methods: meta-actor critic with advantage weighting (labelled
MACAW) (Mitchell et al., 2021) and Bayesian ofﬂine RL (labelled BOReL) (Dorfman & Tamar,
2020). MACAW and BOReL assume that rewards are provided during training and cannot make
use of the self-supervised online interaction, and so we cannot compare directly to these past works.
Therefore, we report their performance only after ofﬂine training. For these prior work, we used
the open-sourced code directly from each paper (PEARL, MACAW, and BOReL), and and trained
these methods using the same ofﬂine dataset. To ensure a fair comparison, we ran the original
hyperparameters and matched SMAC hyperparameters (matching network size, learning rate, and
batch size), taking the better of the two as the result for each prior method. We include a list of all
hyperparameters and network architectures in Appendix C.3."
EXPERIMENTS,0.3074074074074074,"Comparison results.
We plot the mean post-adaptation returns and standard deviation across 4
seeds in Figure 5. We see that across all three environments, SMAC consistently improves during the
self-supervised phase, and often achieves a similar performance to the oracle that uses ground-truth
reward during the online phase of learning. SMAC also signiﬁcantly improves over meta behavior
cloning, which conﬁrms that the data in the ofﬂine dataset is far from optimal."
EXPERIMENTS,0.3111111111111111,"We also observed even before the online phase, our method is competitive with BOReL and MACAW
as a stand-alone ofﬂine meta-RL method. When self-supervised training is included, we see that
SMAC signiﬁcantly improves over the performance of BOReL on all three tasks and signiﬁcantly
improves over MACAW on two of the three tasks, including the Sawyer Manipulation environment,
which is by far the most challenging and exhibits the most variability between tasks. In this domain,
we also see the largest gains from the AWAC actor update, in contrast to the actor ablation in blue,
which corresponds to a PEARL-style update, indicating that properly handling the ofﬂine phase is
also important for good performance."
EXPERIMENTS,0.3148148148148148,"In conclusion, our method is the only one attains generalization performance close to the Online
Oracle upper bound baseline on all three tasks, indicating that self-supervised online ﬁne-tuning is
highly effective for mitigating distributional shift in meta-RL, whereas without it ofﬂine meta-RL
generally does not exceed the performance of a meta-imitation learning baseline."
EXPERIMENTS,0.31851851851851853,"Visualizing the distribution shift.
Lastly, we further investigate if the self-supervised training
helps speciﬁcally because it mitigates a distribution shift caused by the exploration policy. To
investigate this, we visualize the trajectories of the learned policy both before and after the self-
supervised phase for the Ant Direction task in Figure 6. For each plot, we show trajectories from the"
EXPERIMENTS,0.32222222222222224,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.32592592592592595,"0
25
50
75
100
Number of Environment Steps (x1000) −250 −200 −150 −100"
EXPERIMENTS,0.3296296296296296,Post-Adaptation Returns
EXPERIMENTS,0.3333333333333333,Cheetah Velocity
EXPERIMENTS,0.337037037037037,"100
200
Number of Environment Steps (x1000) −500 0 500"
EXPERIMENTS,0.34074074074074073,Post-Adaptation Returns
EXPERIMENTS,0.34444444444444444,Ant Direction
EXPERIMENTS,0.34814814814814815,"0
100
200
Number of Environment Steps (x1000) −40 −30 −20"
EXPERIMENTS,0.35185185185185186,Post-Adaptation Returns
EXPERIMENTS,0.35555555555555557,Sawyer Manipulation
EXPERIMENTS,0.3592592592592593,"25
50
75
Number of Environment Steps (x1000) 200 300 400 500"
EXPERIMENTS,0.362962962962963,Post-Adaptation Returns
EXPERIMENTS,0.36666666666666664,Walker
EXPERIMENTS,0.37037037037037035,"25
50
75
Number of Environment Steps (x1000) 100 200 300 400"
EXPERIMENTS,0.37407407407407406,Post-Adaptation Returns
EXPERIMENTS,0.37777777777777777,Hopper
EXPERIMENTS,0.3814814814814815,"25
50
75
Number of Environment Steps (x1000) 200 400 600 800 1000"
EXPERIMENTS,0.3851851851851852,Post-Adaptation Returns
EXPERIMENTS,0.3888888888888889,Humanoid
EXPERIMENTS,0.3925925925925926,"SMAC (Ours)
Online Oracle
SMAC (actor ablation)
MACAW
BOReL
meta behavior cloning"
EXPERIMENTS,0.3962962962962963,"Figure 5: We report the ﬁnal return of meta-test adaptation on unseen test tasks versus the amount of self-
supervised meta-training following ofﬂine meta-training. Our method SMAC, shown in red, consistently trains
to a reasonable performance from ofﬂine meta-RL (shown at step 0) and then steadily improves with online
self-supervised experience. The ofﬂine meta-RL methods, MACAW Mitchell et al. (2021) and BOReL are
competitive with the ofﬂine performance of SMAC but have no mechanism to improve via self-supervision. We
also compare to SMAC (SAC ablation) which uses SAC instead of AWAC as the underlying RL algorithm. This
ablation struggles to train a value function ofﬂine, and so struggles to improve on more difﬁcult tasks."
EXPERIMENTS,0.4,"policy πθ(a | s, z) when the encoder qφe(z | h) is conditioned on histories from either the ofﬂine
dataset (hofﬂine) or from the learned exploration policy (honline). Since the same policy is evaluated,
differences between the resulting trajectories represent the distribution shift caused by using history
from the learned exploration policy rather than from the ofﬂine dataset."
EXPERIMENTS,0.40370370370370373,"We see that before the self-supervised phase, there is a large difference between the two modes
that can only be attributed to the difference in h. When using honline, the post-adaptation policy
only explores one mode, but when using hofﬂine, the policy moves in all directions. This qualitative
difference explains the large performance gap observed in Figure 2 and highlights that the adaptation
procedure is sensitive to the history h used to adapt. In contrast, after the self-supervised phase,
the policy moves in all directions regardless of where the history came from. In Appendix B, we
also visualize the exploration trajectories and found that the exploration trajectories are qualitatively
similar both before and after the self-supervised phase. Together, these results illustrate the SMAC
policy learns to adapt to the exploration trajectories by using the self-supervised phase to mitigate the
distribution shift that occurs with naïve ofﬂine meta RL."
CONCLUSION,0.4074074074074074,"7
CONCLUSION"
CONCLUSION,0.4111111111111111,"In this paper, we studied a problem speciﬁc to ofﬂine meta-RL: distribution shift in the context
parameters z. This distribution shift occurs because the data collected by the meta-learned exploration
policy differs from the ofﬂine dataset. To address this problem, we assumed that an agent can sample
new trajectories without additional reward labels and presented SMAC, a method that uses these
additional interactions together with state-of-the-art ofﬂine RL techniques to provide an effective
meta-RL method. Experimentally, we found that SMAC signiﬁcantly improves over the performance
of prior ofﬂine meta-RL methods, after the self-supervised online phase."
CONCLUSION,0.4148148148148148,"A limitation of SMAC is that it assumes that the agent can gather additional unlabeled online samples.
However, this assumption may not be satisﬁed due to safety concerns and engineering needed to
ensure that automatic resets are feasible, and therefore enabling safe, autonomous environment
interactions is an important direction for future research."
CONCLUSION,0.4185185185185185,"Lastly, using self-supervised interaction to mitigate distribution shift in ofﬂine meta RL is orthogonal
to the underlying meta RL algorithm. In this paper, we built off of PEARL (Rakelly et al., 2019) due
to its simplicity, but this insight could easily be used to improve other existing (Mitchell et al., 2021;
Dorfman & Tamar, 2020) or future ofﬂine meta RL algorithms."
CONCLUSION,0.4222222222222222,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.42592592592592593,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.42962962962962964,"To ensure reproducibility, we have included the source code for our method and environments in the
supplementary ﬁle, which includes a README ﬁle that describes how to install our code and run the
experiments. We describe in detail the environments and hyperparameters used for the experiment
in Appendix C. For clarity, we have also included pseudo-code of SMAC on the ﬁrst page of the
supplementary material."
REFERENCES,0.43333333333333335,REFERENCES
REFERENCES,0.43703703703703706,"Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 1, 2004."
REFERENCES,0.44074074074074077,"Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
Mcgrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience Replay. In
Advances in Neural Information Processing Systems (NIPS), 2017. URL https://arxiv.
org/pdf/1707.01495.pdfhttp://arxiv.org/abs/1707.01495."
REFERENCES,0.4444444444444444,"André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and
David Silver. Successor features for transfer in reinforcement learning. In Advances in neural
information processing systems, pp. 4055–4065, 2017."
REFERENCES,0.44814814814814813,"André Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz,
Augustin Žídek, and Remi Munos. Transfer in deep reinforcement learning using successor features
and generalised policy improvement. arXiv preprint arXiv:1901.10964, 2019."
REFERENCES,0.45185185185185184,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.45555555555555555,"Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. Gep-pg: Decoupling exploration and
exploitation in deep reinforcement learning algorithms. International Conference on Machine
Learning (ICML), 2018."
REFERENCES,0.45925925925925926,"Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics
and machine learning. http://pybullet.org, 2016–2021."
REFERENCES,0.46296296296296297,"Ron Dorfman and Aviv Tamar. Ofﬂine meta reinforcement learning. arXiv preprint arXiv:2008.02598,
2020."
REFERENCES,0.4666666666666667,"Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016."
REFERENCES,0.4703703703703704,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron,
Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance
weighted actor-learner architectures. In International Conference on Machine Learning, pp.
1407–1416. PMLR, 2018."
REFERENCES,0.4740740740740741,"Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International conference on machine learning, pp. 49–58. PMLR, 2016."
REFERENCES,0.4777777777777778,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR, 2017."
REFERENCES,0.48148148148148145,"Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-
ment learning. arXiv preprint arXiv:1710.11248, 2017."
REFERENCES,0.48518518518518516,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.4888888888888889,"Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch
deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019a."
REFERENCES,0.4925925925925926,Under review as a conference paper at ICLR 2022
REFERENCES,0.4962962962962963,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019b."
REFERENCES,0.5,"Christopher Grimm, Irina Higgins, Andre Barreto, Denis Teplyashin, Markus Wulfmeier, Tim Her-
tweck, Raia Hadsell, and Satinder Singh. Disentangled cumulants help successor representations
transfer to new tasks. arXiv preprint arXiv:1911.10866, 2019."
REFERENCES,0.5037037037037037,"Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised meta-learning
for reinforcement learning. arXiv preprint arXiv:1806.04640, 2018a."
REFERENCES,0.5074074074074074,"Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.
Meta-
reinforcement learning of structured exploration strategies. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing Systems, 2018b."
REFERENCES,0.5111111111111111,"Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al.
Soft actor-critic algorithms and
applications. arXiv preprint arXiv:1812.05905, 2018."
REFERENCES,0.5148148148148148,"Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller.
Learning an embedding space for transferable robot skills.
In International Conference on
Learning Representations, 2018."
REFERENCES,0.5185185185185185,"Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa,
Tom Erez, Ziyu Wang, SM Eslami, et al. Emergence of locomotion behaviours in rich environments.
arXiv preprint arXiv:1707.02286, 2017."
REFERENCES,0.5222222222222223,"Jonathan Ho and Stefano Ermon.
Generative adversarial imitation learning.
arXiv preprint
arXiv:1606.03476, 2016."
REFERENCES,0.5259259259259259,"Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A Ortega, Yee Whye Teh, and
Nicolas Heess. Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424,
2019."
REFERENCES,0.5296296296296297,"Tommi Jaakkola, Satinder P Singh, and Michael I Jordan. Reinforcement learning algorithm for
partially observable markov decision problems. Advances in neural information processing systems,
pp. 345–352, 1995."
REFERENCES,0.5333333333333333,"Allan Jabri, Kyle Hsu, Ben Eysenbach, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Un-
supervised curricula for visual meta-reinforcement learning. arXiv preprint arXiv:1912.04226,
2019."
REFERENCES,0.5370370370370371,"L P Kaelbling. Learning to achieve goals. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), volume vol.2, pp. 1094 – 8, 1993."
REFERENCES,0.5407407407407407,"Pierre-Alexandre Kamienny, Matteo Pirotta, Alessandro Lazaric, Thibault Lavril, Nicolas Usunier,
and Ludovic Denoyer. Learning adaptive exploration strategies in dynamic environments through
informed policy regularization. arXiv preprint arXiv:2005.02934, 2020."
REFERENCES,0.5444444444444444,"Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent
experience replay in distributed reinforcement learning. In International conference on learning
representations, 2018."
REFERENCES,0.5481481481481482,"Alexander Khazatsky, Ashvin Nair, Daniel Jing, and Sergey Levine. What can i do here? learning new
skills by imagining visual affordances. In International Conference on Robotics and Automation.
IEEE, 2021."
REFERENCES,0.5518518518518518,"Louis Kirsch, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Improving generalization in meta
reinforcement learning using learned objectives. arXiv preprint arXiv:1910.04098, 2019."
REFERENCES,0.5555555555555556,"Jonathan Kofman, Xianghai Wu, Timothy J Luu, and Siddharth Verma. Teleoperation of a robot ma-
nipulator using a vision-based human-robot interface. IEEE transactions on industrial electronics,
52(5):1206–1219, 2005."
REFERENCES,0.5592592592592592,Under review as a conference paper at ICLR 2022
REFERENCES,0.562962962962963,"Ksenia Konyushkova, Konrad Zolna, Yusuf Aytar, Alexander Novikov, Scott Reed, Serkan Cabi,
and Nando de Freitas. Semi-supervised reward learning for ofﬂine reinforcement learning. arXiv
preprint arXiv:2012.06899, 2020."
REFERENCES,0.5666666666666667,"Tejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J Gershman. Deep successor
reinforcement learning. arXiv preprint arXiv:1606.02396, 2016."
REFERENCES,0.5703703703703704,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019."
REFERENCES,0.5740740740740741,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.5777777777777777,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2016. ISSN 10769757. doi: 10.1613/jair.301. URL https://
arxiv.org/pdf/1509.02971.pdf."
REFERENCES,0.5814814814814815,"Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Ofﬂine meta-
reinforcement learning with advantage weighting.
In International Conference on Machine
Learning, pp. 7780–7791. PMLR, 2021."
REFERENCES,0.5851851851851851,"Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual
Reinforcement Learning with Imagined Goals. In Advances in Neural Information Processing
Systems (NeurIPS), 2018."
REFERENCES,0.5888888888888889,"Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcement learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020."
REFERENCES,0.5925925925925926,"Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Lee, Jie Tan, and Sergey Levine.
Learning agile robotic locomotion skills by imitating animals. In Robotics: Science and Systems,
2020."
REFERENCES,0.5962962962962963,"Alexandre Péré, Sebastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised Learning
of Goal Spaces for Intrinsically Motivated Goal Exploration. In International Conference on
Learning Representations (ICLR), 2018. URL https://arxiv.org/pdf/1803.00781.
pdf."
REFERENCES,0.6,"Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal Difference Models: Model-
Free Deep RL For Model-Based Control. In International Conference on Learning Representations
(ICLR), 2018. URL https://arxiv.org/pdf/1802.09081.pdf."
REFERENCES,0.6037037037037037,"Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efﬁcient off-policy
meta-reinforcement learning via probabilistic context variables. In International conference on
machine learning, pp. 5331–5340. PMLR, 2019."
REFERENCES,0.6074074074074074,"Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement
learning with sparse rewards. arXiv preprint arXiv:1905.11108, 2019."
REFERENCES,0.6111111111111112,"Stéphane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In Proceedings of the
thirteenth international conference on artiﬁcial intelligence and statistics, pp. 661–668. JMLR
Workshop and Conference Proceedings, 2010."
REFERENCES,0.6148148148148148,"Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. In International Conference on Learning Representations, 2018."
REFERENCES,0.6185185185185185,"Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 3(6):
233–242, 1999."
REFERENCES,0.6222222222222222,"Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal Value Function Approxima-
tors. In International Conference on Machine Learning (ICML), 2015."
REFERENCES,0.6259259259259259,Under review as a conference paper at ICLR 2022
REFERENCES,0.6296296296296297,"Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-Time
Training with Self-Supervision for Generalization under Distribution Shifts. In International
Conference on Machine Learning (ICML), 2020. URL https://test-time-training.
github.io/."
REFERENCES,0.6333333333333333,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.
IEEE, 2012."
REFERENCES,0.6370370370370371,"David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and
Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. CoRR,
abs/1811.11359, 2018."
REFERENCES,0.6407407407407407,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.6444444444444445,"Danfei Xu and Misha Denil. Positive-unlabeled reward learning. arXiv preprint arXiv:1911.00459,
2019."
REFERENCES,0.6481481481481481,"Zhongwen Xu, Hado P van Hasselt, and David Silver.
Meta-gradient reinforcement learning.
Advances in neural information processing systems, 31:2396–2407, 2018."
REFERENCES,0.6518518518518519,"Zhongwen Xu, Hado van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, and David Sil-
ver. Meta-gradient reinforcement learning with an objective discovered online. arXiv preprint
arXiv:2007.08433, 2020."
REFERENCES,0.6555555555555556,"Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
In Conference on Robot Learning, pp. 1094–1100. PMLR, 2020."
REFERENCES,0.6592592592592592,"Tony Z Zhao, Anusha Nagabandi, Kate Rakelly, Chelsea Finn, and Sergey Levine. Meld: Meta-
reinforcement learning from images via latent state models. arXiv preprint arXiv:2010.13957,
2020."
REFERENCES,0.662962962962963,"Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson. Varibad: a very good method for bayes-adaptive deep rl via meta-learning.
Proceedings of ICLR 2020, 2020."
REFERENCES,0.6666666666666666,"Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf
Aytar, Misha Denil, Nando de Freitas, and Scott Reed. Ofﬂine learning from demonstrations and
unlabeled experience. arXiv preprint arXiv:2011.13885, 2020."
REFERENCES,0.6703703703703704,Under review as a conference paper at ICLR 2022
REFERENCES,0.674074074074074,Supplementary Material
REFERENCES,0.6777777777777778,"A
METHOD PSEUDO-CODE"
REFERENCES,0.6814814814814815,We present the pseudo-code for SMAC in Algorithm 1.
REFERENCES,0.6851851851851852,Algorithm 1 Semi-Supervised Meta Actor-Critic
REFERENCES,0.6888888888888889,"1: Input: datasets D = {Di}Nbuff
i=1 , policy πθ, Q-function Qw, encoder qφe, and decoder rφd.
2: for iteration n = 1, 2, . . . , Nofﬂine do
▷ofﬂine phase
3:
Sample buffer Di ∼D and two histories from buffer h, h′ ∼Di.
4:
Use the ﬁrst history sample to h to infer z encode it z ∼qφe(h).
5:
Update πθ, Qw, qφe, rφd by minimizing Lactor, Lcritic, Lreward with samples z, h′."
REFERENCES,0.6925925925925925,"6: for iteration n = 1, 2, . . . , Nonline do
▷self-supervised phase
7:
Collect trajectory τ with πθ(a | s, z), with zt ∼p(z).
8:
Sample buffer Di ∼D and ofﬂine history hofﬂine ∼Di.
9:
Use hofﬂine to label the rewards in τ, as in Equation (4), and add the resulting data to Di.
10:
Sample buffer Di ∼D and two histories from buffer h, h′ ∼Di.
11:
Encode ﬁrst history z = qφe(h).
12:
Update πθ, Qw by minimizing Lactor, Lcritic with samples z, h′."
REFERENCES,0.6962962962962963,"B
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.7,"Exploration and ofﬂine dataset visualization
In Figure 7, we visualize the post-adaption trajec-
tories generated when conditioning the encoder the online exploration trajectories honline and the
ofﬂine trajectories hofﬂine. Similar to Figure 6, and also visualize the online and ofﬂine trajectories
themselves. We see that the exploration trajectories honline and the ofﬂine trajectories hofﬂine are very
different (green vs red, respectively), but the self-supervised phase mitigates the negative impact that
this distribution shift has on ofﬂine meta RL. In particular, the post-adaptation trajectories conditioned
on these two data sources (blue and orange) are similar after the self-supervised training, whereas
before the self-supervised training, only the trajectories conditioned on the ofﬂine data (blue) move
in multiple directions."
REFERENCES,0.7037037037037037,"Addressing state-space distribution shift by self-supervised meta-training on test tasks.
An-
other source of distribution shift that can negatively impact a meta-policy is a distribution shift in
state space. While this distribution shift occurs in standard ofﬂine RL, we expect this issue to be"
REFERENCES,0.7074074074074074,"-20
-10
0
10
20
COM X-position -20 -10 0 10 20"
REFERENCES,0.7111111111111111,COM Y-position
REFERENCES,0.7148148148148148,After Offline Training
REFERENCES,0.7185185185185186,"-20
-10
0
10
20
COM X-position -20 -10 0 10 20"
REFERENCES,0.7222222222222222,COM Y-position
REFERENCES,0.725925925925926,After Self-Supervised Training
REFERENCES,0.7296296296296296,"post-adaptation policy,
where z
q(z
hoffline)"
REFERENCES,0.7333333333333333,"post-adaptation policy,
where z
q(z
hexplore)"
REFERENCES,0.737037037037037,"Figure 6: Example XY-coordinates visited by a learned policy on the Ant Direction task. Left: Immediately
after ofﬂine training, the post adaptation policy moves in many different directions when conditioned on hofﬂine
(blue). However, when conditioned on honline (orange), the policy only moves up and to the left, suggesting that
the post-adaptation policy is sensitive to data distribution used to collect h. Right: After the self-supervised
phase, the post-adaptation policy moves in many directions regardless of the data source, suggesting that the
self-supervised phase mitigates the distribution shift between conditioning on ofﬂine and online data."
REFERENCES,0.7407407407407407,Under review as a conference paper at ICLR 2022
REFERENCES,0.7444444444444445,"-20
-10
0
10
20
COM X-position -20 -10 0 10 20"
REFERENCES,0.7481481481481481,COM Y-position
REFERENCES,0.7518518518518519,After Offline Training
REFERENCES,0.7555555555555555,"-20
-10
0
10
20
COM X-position -20 -10 0 10 20"
REFERENCES,0.7592592592592593,COM Y-position
REFERENCES,0.762962962962963,After Self-Supervised Training
REFERENCES,0.7666666666666667,"post-adaptation policy,
where z
q(z
hoffline)"
REFERENCES,0.7703703703703704,"post-adaptation policy,
where z
q(z
hexplore)"
REFERENCES,0.774074074074074,"exploration policy,
where z
p(z)"
REFERENCES,0.7777777777777778,offline dataset
REFERENCES,0.7814814814814814,"Figure 7: We duplicate Figure 6 but include the exploration trajectories (green) and example trajectories from the
ofﬂine dataset (red). We see that the exploration policy both before and after self-supervised training primarily
moves up and to the left, whereas the ofﬂine data moves in all direction. Before the self-supervised phase, we
see that conditioning the encoder on online data (orange) rather than ofﬂine data (blue) results in very different
policies, with the online data resulting in the post-adaptation policy only moving up and to the left. However, the
self-supervised phase of SMAC mitigates the impact of this distribution shift and results in qualitatively similar
post-adaptation trajectories, despite the large difference between the exploration trajectories and ofﬂine dataset
trajectories."
REFERENCES,0.7851851851851852,"0
25
50
75
100
Number of Environment Steps (x1000) −45 −40 −35 −30 −25"
REFERENCES,0.7888888888888889,Post-Adaptation Returns
REFERENCES,0.7925925925925926,Sawyer Manipulation
REFERENCES,0.7962962962962963,"Self-Supervision on Test Tasks
Oracle
Self-Supervision on Train Tasks"
REFERENCES,0.8,"Figure 8: Learning curves when performing self-supervised training on the test environments (red) or the
meta-training environments (blue). We also compare to an oracle that trains on test environments in combination
with ground-truth rewards (black). We see that interacting with the test environment without rewards allows for
steady improvement in post-adaptation test performance and obtains a similar performance to meta-training on
those environments with ground-truth rewards."
REFERENCES,0.8037037037037037,"more prominent in meta RL, where there is a focus on generalizing to completely novel tasks. In
many real-world scenarios, experiencing the state distribution of a novel task is possible, but it is the
supervision (ie. reward signal) that is expensive to obtain. Can we mitigate state distribution shift by
allow the agent to meta-train in the test task environments, but without rewards?"
REFERENCES,0.8074074074074075,"In this experiment, we evaluate our method, SMAC, when training online on the test tasks instead
of on the meta-training tasks as in the experiments in Section 6. Prior work has explored this idea
of self-supervision with test tasks in supervised learning Sun et al. (2020) and goal-conditioned
RL Khazatsky et al. (2021). We use the Sawyer Manipulation environment to study how self-
supervised training can mitigate state distribution shifts, as these environments contain signiﬁcant
variation between tasks. To further increase the complexity of the environment, we use a version of
the environment which samples from a set of eight potential desired behaviors instead of three."
REFERENCES,0.8111111111111111,"We compare self-supervised training on test tasks to self-supervised training on the set of meta-
training tasks, which are also the tasks contained in the ofﬂine dataset. A large gap in performance
indicates that interacting with the test tasks can mitigate the resulting distribution shift even when no
reward labels are provided."
REFERENCES,0.8148148148148148,Under review as a conference paper at ICLR 2022
REFERENCES,0.8185185185185185,"We show the results in Figure 8 and ﬁnd that there is indeed a large performance gap between the
two training modes, with self-supervision on test tasks improving post-adaptation returns while
self-supervision on meta-training tasks does not improve post-adaptation returns. We also compare to
an oracle method that performs online training with the test tasks and the ground-truth reward signal.
We see that SMAC is competitive with the oracle, demonstrating that we do not need access to rewards
in order to improve on test tasks. Instead, the entire performance gain comes from experiencing the
new state distribution of test tasks. Overall, these results suggest that SMAC is effective for mitigate
distribution shifts in both z-space and state space, even when an agent can interact in the environment
without reward supervision."
REFERENCES,0.8222222222222222,"Reward Accuracy Dependence
Our method involves training a reward decoder, and so a natural
question is: How good must the reward decoder be for SMAC to work? We found that the reward loss
does not need to be particularly low for our method to work. Speciﬁcally, in the Sawyer Manipulation
environment, the reward scale is either −1 or 0, meaning that the maximum reward scale is 1. We
observed that the reward decoder loss is around 0.2 on the training task and around 0.25 on the test
tasks, indicating that the method does not need a relatively low reward decoder loss to perform well."
REFERENCES,0.825925925925926,"C
EXPERIMENTAL DETAILS"
REFERENCES,0.8296296296296296,"C.1
DATA COLLECTION DIFFERENCE FROM PRIOR WORK"
REFERENCES,0.8333333333333334,"BOReL and MACAW were both developed assuming several orders of magnitude more data than the
regime that we tested. For example, in the BOReL paper (Dorfman & Tamar, 2020), the Cheetah
Velocity was trained with an ofﬂine dataset using 400 million transitions and performs additional
reward relabeling using ground-truth information about the transitions. In contrast, our ofﬂine dataset
contains only 240 thousand transitions, roughly three orders of magnitude fewer transitions. Similarly,
MACAW uses 100M transitions for Cheetah Velocity, over 40 times more transitions than used in
our experiments. These prior methods also collect ofﬂine datasets by training task-speciﬁc policies,
which converge to near-optimal policies within the ﬁrst million time step (Haarnoja et al., 2018),
meaning that they utilize very high-quality data. In contrast, our experiments focused on the scenario
with many fewer and much lower quality trajectories, which is likely the cause of the relatively worse
performance of BOReL and MACAW than in the original papers."
REFERENCES,0.837037037037037,"C.2
ENVIRONMENT DETAILS"
REFERENCES,0.8407407407407408,"In this section, we describe the state and action space of each environment. We also describe how
reward functions were generated and how the ofﬂine data was generated."
REFERENCES,0.8444444444444444,"Figure 9: Illustrations of two evaluation domains, each
of which has a set of meta-train tasks (examples shown
in blue) and held out test tasks (orange). The domains
include (left) a half cheetah tasked with running at dif-
ferent speeds and (right) a quadruped ant locomoting to
different points on a circle."
REFERENCES,0.8481481481481481,"Ant Direction
The Ant Direction task con-
sists of controlling a quadruped “ant” robot that
can move in a plane.
Following prior work
(Rakelly et al., 2019; Dorfman & Tamar, 2020),
the reward function is the dot product between
the agent’s velocity and a direction uniformly
sampled from the unit circle. The state space
is R20, comprising the orientation of the ant
(in quaternion) as well as the angle and angu-
lar velocity of all 8 joints. The action space
is [−1, 1]8, with each dimension corresponding
to the torque applied to a respective joint. The
reward function is the negative absolute differ-
ence between the agent’s x-velocity and a target
velocity uniformly sampled from [0, 3]."
REFERENCES,0.8518518518518519,"The ofﬂine data is collected by running PEARL (Rakelly et al., 2019) on this meta RL task with
100 pre-sampled 2 target velocities. We terminate PEARL after 100 iterations, with each iteration"
REFERENCES,0.8555555555555555,"2To mitigate variance coming from this sampling procedure, we use the same sampled target velocities across
all experiments and comparisons. We similarly use a pre-sampled set of tasks for the other environments."
REFERENCES,0.8592592592592593,Under review as a conference paper at ICLR 2022
REFERENCES,0.8629629629629629,"consisting of collecting trajectories until at least 1000 new transitions have been observed. As
discussed in Appendix C.1, this results in highly suboptimal data, enabling us to test how well the
different methods can improve over the ofﬂine data. In PEARL, there are two replay buffers saved
for each task, one for sampling data for training the encoder and another for training the policy and
Q-function. We will call the former replay buffer the encoder replay buffer and the latter the RL
replay buffer. The encoder replay buffer contains data generated by only the exploration policy, in
which z ∼pz(z). The RL replay buffer contains all data generated, including both exploration and
post-adaptation, in which z ∼qφe(z | h). To make the ofﬂine dataset, we load the last 1200 samples
of the RL replay buffer and the last 400 transitions from the encoder replay buffer into corresponding
RL and encoder replay buffers for SMAC."
REFERENCES,0.8666666666666667,"Cheetah Velocity
The Cheetah Velocity task consists of controlling a two-legged “half cheetah”
that can move forwards or backwards along the x-axis. Following prior work (Rakelly et al., 2019;
Dorfman & Tamar, 2020), the reward function is the absolute difference product between the agent’s
x-velocity and a velocity uniformly sampled from [0, 3]. The state space is R20, comprising the
z-position; the cheetah’s x- and z- velocity; the angle and angular velocity of each joint and the
half-cheetah’s y-angle; and the XYZ position of the center of mass. The action space is [−1, 1]6, with
each dimension corresponding to the torque applied to a respective joint."
REFERENCES,0.8703703703703703,"The ofﬂine data is collected in the same way as in the Ant Direction task, using a run from PEARL
with 100 pre-sampled target velocities. For the ofﬂine dataset, we use the ﬁrst 1200 samples from
the RL replay buffer and last 400 samples from the encoder replay buffer after 50 PEARL iterations,
with each iteration containing at least 1000 new transitions. For only this environment, we found that
it was beneﬁcial to freeze the encoder buffer during the self-supervised phase."
REFERENCES,0.8740740740740741,"Sawyer Manipulation
The state space, action space, and reward is described in Section 6. Tasks
are generated by sampling the initial conﬁguration, and then the desired behavior. There are ﬁve
objects: a drawer opened by handle, a drawer opened by button, a button, a tray, and a graspable
object. The state is a 13-dimensional vector comprising of the 3D position of the end-effector and
positions of objects in the scene: 3 dimensions for the graspable object, and 1 dimension for each
articulated joint in the scene including the robot grippers. If an object is not present, it takes on
position 0 in the corresponding element of the state space. First, the presence or absence of each of
the ﬁve is randomized. Next, the position of the drawers (from 2 sides), initial position of the tray
(from 4 positions), and the object (from 4 positions) is randomized. Finally, the desired behavior is
randomly chosen from the following list, but only including the ones that are possible in the scene:
""move hand"", ""open top drawer with handle"", or ""open bottom drawer with button"". The ofﬂine
data is collected using a scripted controller that does not know the desired behavior and randomly
performs potential tasks in the scene, choosing another task if it ﬁnishes one task before the trajectory
ends. This data is loaded into a single replay buffer used for both the encoder and RL."
REFERENCES,0.8777777777777778,"Walker Param
This environment involves controlling a bi-pedal robot that can move along the Z-
and Y-axis. The source code is taken from the rand_param_envs repository 3, which has been
used in prior work (Rakelly et al., 2019). A task involves randomly sampling the body mass, the joint
damping coefﬁcients, the body inertia, and the friction parameters from a log-uniform distribution on
the range [1.5−3, 1.53] . The reward is the velocity plus a bonus of 1 for staying alive and a control
penalty of 10−3 × ∥a∥2, where ∥a∥is the ℓ2 norm of the action. The state space is 17 dimensions
and the action space is 6 dimensions (one for each joint)."
REFERENCES,0.8814814814814815,"Hopper Param
This environment involves controlling a one-legged robot that can move along the
Z- and Y-axis. This environment is also taken from the rand_param_envs repository. The reward
is the same as in Walker Param, and tasks are sampled the same as for the Walker Param. The state
space is 11 dimensions and the action space is 3 dimensions (one for each joint)."
REFERENCES,0.8851851851851852,"Humanoid
This environment is based on the Humanoid environment from OpenAI Gym (Brock-
man et al., 2016). We reuse the standard reward function but replace the forward-velocity reward
with a velocity reward based on target direction. Each task consists of sampling a target direction"
REFERENCES,0.8888888888888888,3https://github.com/dennisl88/rand_param_envs
REFERENCES,0.8925925925925926,Under review as a conference paper at ICLR 2022
REFERENCES,0.8962962962962963,"Hyperparameter
Value"
REFERENCES,0.9,"RL batch size
256
encoder batch size
64
meta batch size
4
Q-network hidden sizes
[300, 300, 300]
policy network hidden sizes
[300, 300, 300]
decoder network hidden sizes
[64, 64]
encoder network hidden sizes
[200, 200, 200]
z dimensionality (dz)
5
hidden activation (all networks)
ReLU
Q-network, encoder, and decoder output activation
identity
policy output activation
tanh
discount factor γ
0.99
target network soft target η
0.005
policy, Q-network, encoder, and decoder learning rate
3 × 10−4"
REFERENCES,0.9037037037037037,"policy, Q-network, encoder, and decoder optimizer
Adam
# of gradient steps per environment transition
4"
REFERENCES,0.9074074074074074,Table 1: SMAC Hyperparameters for Self-Supervised Phase
REFERENCES,0.9111111111111111,"Hyperparameter
Cheetah
Ant
Sawyer
Walker
Hopper
Humanoid"
REFERENCES,0.9148148148148149,"horizon (max # of transitions
per trajectory)
200
200
50
200
200
200"
REFERENCES,0.9185185185185185,"AWR β
100
100
0.3
100
100
100
reward scale
5
5
1
5
5
5
# of training tasks
100
100
50
50
50
50
# of test tasks
30
20
10
5
5
5
# of transitions per training task
in ofﬂine dataset
1600
1600
3750
1200
1200
1200"
REFERENCES,0.9222222222222223,"λpearl
1
1
0
1
1
1"
REFERENCES,0.9259259259259259,Table 2: Environment Speciﬁc SMAC Hyperparameters
REFERENCES,0.9296296296296296,"uniformly at random. The state space is 376 dimensions and the action space is 17 dimensions (one
for each joint)."
REFERENCES,0.9333333333333333,"C.3
HYPERPARAMETERS"
REFERENCES,0.937037037037037,"We list the hyperparameters for training the policy, encoder, decoder, and Q-network in Table 1.
If hyperparameters were different across environments, they are listed in Table 2. For pretraining,
we use the same hyperparameters and train for 50000 gradient steps. Below, we give details on
non-standard hyperparameters and architectures."
REFERENCES,0.9407407407407408,"Batch sizes.
The RL batch size is the batch size per task when sampling (s, a, r, s′) tuples to
update the policy and Q-network. The encoder batch size is the size of the history h per task used to
conditioned the encoder qφe(z | h). The meta batch size is how many tasks batches were sampled
and concatenated for both the RL and encoder batches. In other words, for each gradient update,
the policy and Q-network observe (RL batch size) × (meta batch size) transitions and the encoder
observes (RL batch size) × (encoder batch size) transitions."
REFERENCES,0.9444444444444444,Under review as a conference paper at ICLR 2022
REFERENCES,0.9481481481481482,"Encoder architecture.
The encoder uses the same architecture as in Rakelly et al. (2019). The
posterior is given as the product of independent factors"
REFERENCES,0.9518518518518518,"qφe(z | h) ∝
Y"
REFERENCES,0.9555555555555556,"s,a,r∈h
Φ(z | s, a, r),"
REFERENCES,0.9592592592592593,"where each factor is a multi-variate Gaussian over Rdz with learned mean and diagonal variance. In
other words,"
REFERENCES,0.9629629629629629,"Φφe(z | s, a, r) = N(µφe(s, a, r), σφe(s, a, r))."
REFERENCES,0.9666666666666667,"The mean and standard deviation is the output of a single MLP network with output dimensionality
2 × dz. The output of the MLP network is split into two halves. The ﬁrst half is the mean and the
second half is passed through the softplus activation to get the standard deviation."
REFERENCES,0.9703703703703703,"Self-supervised actor update.
The parameter λpearl controls the actor loss during the self-
supervised phase, which is"
REFERENCES,0.9740740740740741,"Lself-supervised
actor
(θ) = Lactor(θ) + λpearl · LPEARL
actor
(θ),"
REFERENCES,0.9777777777777777,"where LPEARL
actor
is the actor loss from PEARL (Rakelly et al., 2019). For reference, the PEARL actor
loss is"
REFERENCES,0.9814814814814815,"LPEARL
actor
(θ) = Es∼Di,z∼qφe(z|h) 
DKL"
REFERENCES,0.9851851851851852,"
πθ(a | s, z)

exp Qw(s, a, z) Z(s) 
."
REFERENCES,0.9888888888888889,"When the parameter λpearl is zero, the actor update is equivalent to the actor update in AWAC (Nair
et al., 2020)."
REFERENCES,0.9925925925925926,"Comparisons
As discussed in Section 6, we used the authors’ code for PEARL (Rakelly et al.,
2019),4 BOReL (Dorfman & Tamar, 2020),5 and MACAW (Mitchell et al., 2021).6 To ensure a fair
comparison, we ran the original hyperparameters and matched SMAC hyperparameters (matching
network size, learning rate, and batch size), taking the better of the two as the result for each prior
method. For all comparisons, we evaluated the ﬁnal policy using the same evaluation method as
SMAC, i.e. collecting exploration trajectories with the learned policy and perform adaptation using
this newly collected data."
REFERENCES,0.9962962962962963,"4https://github.com/katerakelly/oyster
5https://github.com/Rondorf/BOReL
6https://github.com/eric-mitchell/macaw"
