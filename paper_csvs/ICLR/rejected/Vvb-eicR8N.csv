Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002932551319648094,"Sketching is a dimensionality reduction technique where one compresses a matrix
by linear combinations that are chosen at random. A line of work has shown how
to sketch the Hessian to speed up each iteration in a second order method, but such
sketches usually depend only on the matrix at hand, and in a number of cases are
even oblivious to the input matrix. One could instead hope to learn a distribution on
sketching matrices that is optimized for the speciﬁc distribution of input matrices.
We show how to design learned sketches for the Hessian in the context of second
order methods. We prove that a smaller sketching dimension of the column space
of a tall matrix is possible, given an oracle that can predict the indices of the rows of
large leverage score. We design such an oracle for various datasets, and this leads to
a faster convergence of the well-studied iterative Hessian sketch procedure, which
applies to a wide range of problems in convex optimization. We show empirically
that learned sketches, compared with their “non-learned” counterparts, do improve
the approximation accuracy for important problems, including LASSO and matrix
estimation with nuclear norm constraints."
INTRODUCTION,0.005865102639296188,"1
INTRODUCTION"
INTRODUCTION,0.008797653958944282,"Large-scale optimization problems are abundant and solving them efﬁciently requires powerful tools
to make the computation practical. This is especially true of second order methods which often are
less practical than ﬁrst order ones. Although second order methods may have many fewer iterations,
each iteration could involve inverting a large Hessian, which is cubic time; in contrast, ﬁrst order
methods such as stochastic gradient descent are linear time per iteration."
INTRODUCTION,0.011730205278592375,"In order to make second order methods faster in each iteration, a large body of work has looked at
dimensionality reduction techniques, such as sampling, sketching, or approximating the Hessian by a
low rank matrix. See, for example, (Gower et al., 2016; Xu et al., 2016; Pilanci & Wainwright, 2016;
2017; Doikov & Richtárik, 2018; Gower et al., 2018; Roosta-Khorasani & Mahoney, 2019; Gower
et al., 2019; Kylasa et al., 2019; Xu et al., 2020; Li et al., 2020). Our focus is on sketching techniques,
which often consist of multiplying the Hessian by a random matrix chosen independently of the
Hessian. Sketching has a long history in theoretical computer science (see, e.g., (Woodruff, 2014) for
a survey), and we describe such methods more below. A special case of sketching is sampling, which
in practice is often uniform sampling, and hence oblivious to properties of the actual matrix. Other
times the sampling is non-uniform, and based on squared norms of submatrices of the Hessian or on
the leverage scores of the Hessian."
INTRODUCTION,0.01466275659824047,"Our focus is on sketching techniques, and in particular, we consider the framework of (Pilanci &
Wainwright, 2016; 2017) which introduces the iterative Hessian sketch and the Newton sketch, as
well as the high accuracy reﬁnement given in (van den Brand et al., 2020). If one were to run
Newton’s method to ﬁnd a point where the gradient is zero, in each iteration one needs to solve an
equation involving the current Hessian and gradient to ﬁnd the update direction. When the Hessian
can be decomposed as A⊤A for an n × d matrix A with n ≫d, then sketching is particularly
suitable. The iterative Hessian sketch was proposed in Pilanci & Wainwright (2016), where A is
replaced with S · A, for a random matrix S which could be i.i.d. Gaussian or drawn from a more
structured family of random matrices such as the Subsampled Randomized Hadamard Transforms
or COUNT-SKETCH matrices; the latter was done in (Cormode & Dickens, 2019). The Newton
sketch was proposed by Pilanci & Wainwright (2017), which extended sketching methods beyond
constrained least-squares problems to any twice differentiable function subject to a closed convex
constraint set. Using this sketch inside of interior point updates has led to much faster algorithms"
INTRODUCTION,0.017595307917888565,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020527859237536656,"for an extensive body of convex optimization problems (Pilanci & Wainwright, 2017). By instead
using sketching as a preconditioner, an application of the work of (van den Brand et al., 2020) (see
Appendix E) was able to improve the dependence on the accuracy parameter ϵ to logarithmic."
INTRODUCTION,0.02346041055718475,"In general, the idea behind sketching is the following. One chooses a random matrix S, drawn from a
certain family of random matrices, and computes SA. If A is tall-and-thin, then S is short-and-fat, and
thus SA is a small, roughly square matrix. Moreover, SA preserves important properties of A. One
typically desired property is that S is a subspace embedding, meaning that ∥SAx∥2 = (1 ± ϵ)∥Ax∥2
for all x simultaneously. An observation exploited in Cormode & Dickens (2019), building off of
the COUNT-SKETCH random matrices S introduced in randomized linear algebra in Clarkson &
Woodruff (2017), is that if S contains a single non-zero entry per column, then SA can be computed
in O(nnz(A)) time, where nnz(A) denotes the number of nonzeros in A. This is also referred to as
input-sparsity running time."
INTRODUCTION,0.026392961876832845,"Each iteration of a second order method often involves solving an equation of the form A⊤Ax = A⊤b,
where A⊤A is the Hessian and b is the gradient. For a number of problems, one has access to a
matrix A ∈Rn×d with n ≫d, which is also an assumption made in Pilanci & Wainwright (2017).
Therefore, the solution x is the minimizer to a constrained least squares regression problem:"
INTRODUCTION,0.02932551319648094,"min
x∈C
1
2 ∥Ax −b∥2
2 ,
(1)"
INTRODUCTION,0.03225806451612903,"where C is a convex constraint set in Rd. For the unconstrained case (C = Rd), various classical
sketches that attain the subspace embedding property can provably yield high-accuracy approximate
solutions (see, e.g., (Sarlos, 2006; Nelson & Nguyên, 2013; Cohen, 2016; Clarkson & Woodruff,
2017)); for the general constrained case, the Iterative Hessian Sketch (IHS) was proposed by Pilanci &
Wainwright (2016) as an effective approach and Cormode & Dickens (2019) employed sparse sketches
to achieve input-sparsity running time for IHS. All sketches used in these results are data-oblivious
random sketches."
INTRODUCTION,0.03519061583577713,"Learned Sketching.
In the last few years, an exciting new notion of learned sketching has emerged.
Here the idea is that one often sees independent samples of matrices A from a distribution D, and
can train a model to learn the entries in a sketching matrix S on these samples. When given a future
sample B, also drawn from D, the learned sketching matrix S will be such that S · B is a much
more accurate compression of B than if S had the same number of rows and were instead drawn
without knowledge of D. Moreover, the learned sketch S is often sparse, therefore allowing S · B to
be applied very quickly. For large datasets B this is particularly important, and distinguishes this
approach from other transfer learning approaches, e.g., (Andrychowicz et al., 2016), which can be
considerably slower in this context."
INTRODUCTION,0.03812316715542522,"Learned sketches were ﬁrst used in the data stream context for ﬁnding frequent items (Hsu et al.,
2019) and have subsequently been applied to a number of other problems on large data. For example,
Indyk et al. (2019) showed that learned sketches yield signiﬁcantly smaller errors for low rank
approximation. Dong et al. (2020) made signiﬁcant improvements to nearest neighbor search using
learned sketches. More recently, Liu et al. (2020) extended learned sketches to several problems in
numerical linear algebra, including least-squares regression, as well as k-means clustering."
INTRODUCTION,0.04105571847507331,"Despite the number of problems that learned sketches have been applied to, they have not been
applied to convex optimization in general. Given that such methods often require solving a large
overdetermined least squares problem in each iteration, it is hopeful that one can improve each
iteration using learned sketches. However, a number of natural questions arise: (1) how should we
learn the sketch? (2) should we apply the same learned sketch in each iteration, or learn it in the next
iteration by training on a data set involving previously learned sketches from prior iterations?"
INTRODUCTION,0.04398826979472141,"Our Contributions.
In this work we answer the above questions and develop the ﬁrst framework
of learned sketching that applies to a wide number of problems in convex optimization. Namely, we
apply learned sketches to constrained least-squares problems, including LASSO and matrix regression
with nuclear norm constraints. We show empirically that learned sketches demonstrate superior
accuracy over classical oblivious random sketches for each of these problems. All of our learned
sketches S are extremely sparse, meaning that they contain a single non-zero entry per column and
that they can be applied in input-sparsity time. For such sketches, there are two things to learn: the
position of the non-zero entry in each column and the value of the non-zero entry."
INTRODUCTION,0.0469208211143695,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04985337243401759,"Following the previous work of Indyk et al. (2019), we choose the position of the nonzero entry in
each column to be uniformly random, while the value of the nonzero entry is learned (the value is
no longer limited to −1 and 1). Here we consider a new learning objective, that is, we optimize the
subspace embedding property of the sketching matrix instead of optimizing the error in the objective
function of the optimization problem we are trying to solve. This demonstrates a signiﬁcant advantage
over non-learned sketches, and has a fast training time. Our experiments show that the convergence
rate is reduced by 44% over the nonlearned COUNT-SKETCH (a classical extremely sparse sketch)
for the LASSO problem on a real-world dataset. Recall that a smaller convergence rate means a faster
convergence."
INTRODUCTION,0.05278592375366569,"We prove theoretically that S can take fewer rows, with optimized positions of nonzero entries,
when the input matrix A has a small number of rows of heavy leverage score. More speciﬁcally,
COUNT-SKETCH takes O(d2/(δϵ2)) rows with failure probability δ, while our S requires only
O((d polylog(1/ϵ) + log(1/δ))/ϵ2) rows if A has at most d polylog(1/ϵ)/ϵ2 rows of leverage score
at least ϵ/d. This is a quadratic improvement in d and an exponential improvement in δ. Applying S
to A runs in input-sparsity time and the resulting SA may remain sparse if A is sparse. In practice, it
is not necessary to calculate the leverage scores. Instead, we show in our experiments that the indices
of the rows of heavy leverage score can be learned and the induced S achieves a comparable accuracy
for the abovementioned LASSO problem to classical dense sketches such as Gaussian matrices."
INTRODUCTION,0.05571847507331378,"Combining both aspects, the value of the nonzero entry and the indices of the rows of heavy leverage
score, we obtain even better learned sketches. For the same LASSO problem, we show empirically
that such learned sketches reduce the convergence rate by a larger 79.9% to 84.6% over non-learned
sketches. Therefore, the learned sketches attain a smaller error within the same number of iterations,
and in fact, within the same limit on the maximum runtime, since our sketches are extremely sparse."
INTRODUCTION,0.05865102639296188,"We also study the general framework of convex optimization in van den Brand et al. (2020), and show
that also for sketching-based preconditioning, learned sketches demonstrate considerable advantages.
More precisely, by using a learned sketch with the same number of rows as an oblivious sketch, we
are able to obtain a much better preconditioner with the same overall running time."
PRELIMINARIES,0.06158357771260997,"2
PRELIMINARIES"
PRELIMINARIES,0.06451612903225806,"Algorithm 1 LEARN-SKETCH: Gradient descent
algorithm for learning the sketch values"
PRELIMINARIES,0.06744868035190615,"Require: Atrain = {Ai}N
i=1 (Ai ∈Rn×d), learn-
ing rate α
1: Randomly initialize p, v
for a
COUNT-
SKETCH-type sketch as described in the text
2: for t = 0 to step do
3:
Form S using p, v
4:
Sample batch Abatch from Atrain
5:
v ←v −α ∂L(S,Abatch) ∂v"
PRELIMINARIES,0.07038123167155426,"Notation. We denote by Sn−1 the unit sphere
in the n-dimensional Euclidean space Rn. For
a matrix A ∈Rm×n we denote by ∥A∥op its
operator norm, which is deﬁned as ∥A∥op =
supx∈Sn−1 ∥Ax∥2. We also denote by σmax(A)
and σmin(A) the largest and smallest singular
values of A, respectively, and by colsp(A) the
column space of A. The condition number of A is deﬁned to be κ(A) = σmax(A)/σmin(A)."
PRELIMINARIES,0.07331378299120235,"Leverage Scores. We only consider matrices of full column rank1. Suppose that A ∈Rm×n (m ≥n)
has full column rank. It has m leverage scores, denoted by τ1(A), . . . , τm(A), which are deﬁned
as τi(A) = ∥e⊤
i A(A⊤A)−1A⊤∥2
2, where {e1, . . . , em} is the canonical basis of Rm. Equivalently,
letting A = UΣV ⊤be the singular value decomposition of A, where U ∈Rm×n, Σ, V ∈Rn×n, we
can also write τi(A) = ∥e⊤
i UU ⊤∥2
2 = ∥e⊤
i U∥2
2, which is the squared ℓ2 norm of the i-th row of U."
PRELIMINARIES,0.07624633431085044,"Classical Sketches. Below we review several classical sketches that have been used for solving
optimization problems."
PRELIMINARIES,0.07917888563049853,"• Gaussian sketch: S =
1
√mG, where G ∈Rm×n with i.i.d. N(0, 1) entries."
PRELIMINARIES,0.08211143695014662,"• COUNT-SKETCH: Each column of S has only a single non-zero entry. The position of the
non-zero entry is chosen uniformly over the m entries in the column and the value of the entry
is either +1 or −1, each with probability 1/2. Further, the columns are chosen independently.
• Sparse Johnson-Lindenstrauss Transform (SJLT): S is the vertical concatenation of s independent
COUNT-SKETCH matrices, each of dimension m/s × n."
PRELIMINARIES,0.08504398826979472,"1This can be assumed w.l.o.g. by adding artbirarily small random noise to the input, or one can ﬁrst quickly
use sketching to ﬁnd a subset of columns of maximum rank, and replace the inut with that subset of columns."
PRELIMINARIES,0.08797653958944282,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.09090909090909091,"COUNT-SKETCH-type Sketch.
A COUNT-SKETCH-type sketch is characterized by a tuple
(m, n, p, v), where m, n are positive integers and p, v are n-dimensional real vectors, deﬁned as
follows. The sketching matrix S has dimensions m × n and Spi,i = vi for all 1 ≤i ≤n, while
all the other entries of S are 0. When m and n are clear from context, we may characterize such a
sketching matrix by (p, v) only."
PRELIMINARIES,0.093841642228739,"Subspace Embeddings. For a matrix A ∈Rn×d, we say a matrix S ∈Rm×n is a (1 ± ϵ)-subspace
embedding for the column span of A if (1 −ϵ) ∥Ax∥2 ≤∥SAx∥2 ≤(1 + ϵ) ∥Ax∥2 for all x ∈Rd.
The classical sketches above, with appropriate parameters, are all subspace embedding matrices with
probability at least 1 −δ; our focus is on COUNT-SKETCH which can be applied in input sparsity
running time. We summarize the parameters needed for a subspace embedding below:"
PRELIMINARIES,0.0967741935483871,"• Gaussian sketch: m = O((d + log(1/δ))/ϵ2). It is a dense matrix and computing SA costs
O(m · nnz(A)) = O(nnz(A)(d + log(1/δ))/ϵ2) time.
• COUNT-SKETCH: m = O(d2/(δϵ2)) (Clarkson & Woodruff, 2017). Though the number of
rows is quadratic in d/ϵ, the matrix S is sparse and computing SA takes only O(nnz(A)) time.
• SJLT: m = O(d log( d"
PRELIMINARIES,0.09970674486803519,δ )/ϵ2) and has s = O(log( d
PRELIMINARIES,0.10263929618768329,"δ )/ϵ) non-zeros per column (Nelson & Nguyên,
2013; Cohen, 2016). Computing SA takes O(s nnz(A)) = O(nnz(A) log( d"
PRELIMINARIES,0.10557184750733138,δ )/ϵ) time.
PRELIMINARIES,0.10850439882697947,"Iterative Hessian Sketch. The Iterative Hessian Sketching (IHS) method (Pilanci & Wainwright,
2016) solves the constrained least-squares problem (1) by iteratively performing the update"
PRELIMINARIES,0.11143695014662756,"xt+1 = arg min
x∈C 1"
PRELIMINARIES,0.11436950146627566,"2 ∥St+1A(x −xt)∥2
2 −⟨A⊤(b −Axt), x −xt⟩

,
(2)"
PRELIMINARIES,0.11730205278592376,"where St+1 is a sketching matrix. It is not difﬁcult to see that for the unsketched version (St+1 is
the identity matrix) of the minimization above, the optimal solution xt+1 coincides with the optimal
solution to the constrained least squares problem (1). The IHS approximates the Hessian A⊤A by a
sketched version (St+1A)⊤(St+1A) to improve runtime, as St+1A typically has very few rows."
PRELIMINARIES,0.12023460410557185,"Unconstrained Convex Optimization. Consider an unconstrained convex optimization problem
minx f(x), where f is smooth and strongly convex, and its Hessian ∇2f is Lipschitz continuous.
This problem can be solved by Newton’s method, which iteratively performs the update"
PRELIMINARIES,0.12316715542521994,"xt+1 = xt −arg min
z"
PRELIMINARIES,0.12609970674486803,"(∇2f(xt)1/2)⊤(∇2f(xt)1/2)z −∇f(xt)

2 ,
(3)"
PRELIMINARIES,0.12903225806451613,"provided it is given a good initial point x0. In each step, it requires solving a regression problem of
the form minz
A⊤Az −y

2, which, with access to A, can be solved with a fast regression solver
in (van den Brand et al., 2020). The regression solver ﬁrst computes a preconditioner R via a QR
decomposition such that SAR has orthonormal columns, where S is a sketching matrix, then solves
bz = arg minz′
(AR)⊤(AR)z′ −y

2 by gradient descent and returns Rbz in the end. Here, the point
of sketching is that the QR decomposition of SA can be computed much more efﬁciently than the
QR decomposition of A, since S has only a small number of rows."
PRELIMINARIES,0.13196480938416422,"Learning a Sketch. We use the same learning algorithm in (Liu et al., 2020), given in Algorithm 1.
The algorithm aims to minimize the mean loss function L(S, A) = 1"
PRELIMINARIES,0.1348973607038123,"N
PN
i=1 L(S, Ai), where S is the
learned sketch, L(S, A) is the loss function of S applied to a data matrix A, and A = {A1, . . . , AN}
is a (random) subset of training data."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.1378299120234604,"3
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.14076246334310852,"In this section we explain two ways to optimize the subspace embedding property of the sketching
matrix. One is to optimize the non-zero positions of the COUNT-SKETCH-type sketch, based on a
trained oracle to identify a superset of the rows of large leverage score. The other is to optimize the
values of the nonzero entries, which may no longer be −1 or 1, via a learning algorithm based on
gradient descent. As we shall see in Section 4 and 5, a better subspace embedding implies a better
convergence rate in the IHS, as well as for the subroutine in unconstrained convex optimization."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.1436950146627566,"3.1
SKETCHED LEARNING: OPTIMIZING THE POSITIONS"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.1466275659824047,"In this section we consider the problem of embedding the column space of a matrix A ∈Rn×d,
provided that A has a few rows of large leverage score, as well as access to an oracle which reveals a"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.1495601173020528,Under review as a conference paper at ICLR 2022
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.15249266862170088,"superset of the indices of such rows. Formally, let τi(A) denote the leverage score of the i-th row of
A and let
I∗= {i : τi(A) ≥ν}"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.15542521994134897,"be the set of rows with large leverage score. Suppose that a superset I ⊇I∗is known to the algorithm.
In the experiments we train an oracle to predict such rows. We can maintain all rows in I explicitly
and apply a COUNT-SKETCH to the remaining rows, i.e., the rows in [n] \ I. Up to permutation of
the rows, we can write"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.15835777126099707,"A =

AI
AIc"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.16129032258064516,"
and
S =

I
0
0
S′"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.16422287390029325,"
,
(4)"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.16715542521994134,"where S′ is a random COUNT-SKETCH matrix of m rows. Clearly S has a single non-zero entry per
column. We have the following theorem, whose proof is postponed to Section A. Intuitively, the proof
for COUNT-SKETCH in (Clarkson & Woodruff, 2017) handles rows of large leverage score and rows
of small leverage score separately. The rows of large leverage score are to be perfectly hashed while
the rows of small leverage score will concentrate in the sketch by the Hanson-Wright inequality."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.17008797653958943,"Theorem 3.1. Let ν = ϵ/d. Suppose that m = O((d/ϵ2)(polylog(1/ϵ) + log(1/δ))), δ ∈(0, 1/m]
and d = Ω((1/ϵ) polylog(1/ϵ) log2(1/δ)). Then, there exists a distribution on S of the form in (4)"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.17302052785923755,"with m + |I| rows such that Pr

∀x ∈colsp(A),
∥Sx∥2
2 −∥x∥2
2
 > ϵ ∥x∥2
2
	
≤δ ."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.17595307917888564,"Hence, if there happen to be at most d polylog(1/ϵ)/ϵ2 rows of leverage score at least ϵ/d, the
overall sketch length for embedding colsp(A) can be reduced to O((d polylog(1/ϵ)+log(1/δ))/ϵ2),
a quadratic improvement in d and an exponential improvement in δ over the original sketch length
of O(d2/(ϵ2δ)) for COUNT-SKETCH. In the worst case there could be O(d2/ϵ) such rows, though
empirically we do not observe this. The following is an immediate corollary, by setting δ = 1/m."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.17888563049853373,"Corollary 3.2. Suppose that d = Ω((1/ϵ) polylog(1/ϵ)) and |I| = O((d/ϵ2) polylog(d/ϵ)) with
ν = ϵ/d. There exists a distribution on S of the form in (4) with O((d/ϵ2) polylog(d/ϵ)) rows such"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.18181818181818182,"that Pr

∀x ∈colsp(A),
∥Sx∥2
2 −∥x∥2
2
 > ϵ ∥x∥2
2
	
≤ϵ3 ."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.18475073313782991,"We remark that our S is of the COUNT-SKETCH type, which has a twofold beneﬁt. First, SA can be
applied in O(nnz(A)) time. This is faster than a chained subspace embedding of the form S2S1A,
where S1 is a COUNT-SKETCH matrix of O(d2/ϵ2) rows and S2 is a subspace embedding matrix
of O(d/ϵ2) rows. Computing S1A takes O(nnz(A)) time but computing S2(S1A) will take an
additional time of poly(d/ϵ) or O(nnz(S1A) log(d)/ϵ). The latter terms can be quite large and even
comparable to n if say, n is close to d2. Second, our S allows the sketched matrix SA to be sparse
when A is sparse, while the other designs such as Subsampled Randomized Hadamard Transforms
and Sparse Johnson-Lindentrauss Transforms either would not guarantee that SA is sparse, or would
yield a worse sparsity than a matrix of the COUNT-SKETCH type. The sparsity of SA is also important
for solving regression problems involving B = SA in intermediate steps, as algorithms such as
conjugate gradient, which use matrix-vector products, become more efﬁcient."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.187683284457478,"We note that approximate leverages scores of all rows can be found in time O(nnz(A) log n +
poly(d/ϵ)) (Clarkson & Woodruff, 2017). Hence, one can approximate the leverage score of every
row in a preprocessing step before running the IHS. This time will be amortized by the IHS iterations,
because the matrix A remains the same throughout the process. Moreover, in Section 6, we show that
for a number of real-world datasets, it is possible to learn the indices of the heavy rows. In practice,
one can shrink the size of the superset I by restricting I to the rows with large ℓ2 norms in AI. We
shall demonstrate in Section 6 that this heuristic works well on some real-world datasets."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.1906158357771261,"3.2
SKETCHED LEARNING: OPTIMIZING THE VALUES"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.1935483870967742,"As mentioned in Section 2, when we ﬁx the positions of the non-zero entries, we aim to optimize
the values by gradient descent. We propose the following objective loss function for the learning
algorithm L(S, Ai) = ∥(AiRi)⊤AiRi −I∥F , over all the training data, where Ri comes from the
QR-decomposition of SAi = QiR−1
i . We found empirically that not squaring this loss function
works better than squaring it. We think one of the reasons is that the version without squaring may
be less sensitive to outliers. The intuition for this loss function is given by the lemma below, whose
proof is deferred to Section B."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.19648093841642228,Under review as a conference paper at ICLR 2022
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.19941348973607037,"Lemma 3.3. Suppose that ϵ ∈(0, 1"
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.20234604105571846,"2), S ∈Rm×n, A ∈Rn×d has full column rank, and SA = QR
is the QR-decomposition of SA. If ∥(AR−1)⊤AR−1 −I∥op ≤ϵ, then S is a (1 ± ϵ)-subspace
embedding of the column space of A."
LEARNING-AUGMENTED SUBSPACE EMBEDDINGS,0.20527859237536658,"Lemma 3.3 implies that if the loss function over Atrain is small and the distribution of Atest is similar
to Atrain, it is reasonable to expect that S is a good subspace embedding of Atest. Here we use the
Frobenius norm rather than operator norm in the loss function because it will make the optimization
problem easier to solve, and our empirical results also show that the performance of the Frobenius
norm is better than that of the operator norm."
HESSIAN SKETCH,0.20821114369501467,"4
HESSIAN SKETCH"
HESSIAN SKETCH,0.21114369501466276,Algorithm 2 Solver for (5)
HESSIAN SKETCH,0.21407624633431085,"1: S1 ←learned sketch, S2 ←random sketch
2: ( bZi,1, bZi,2) ←ESTIMATE(Si, A), i = 1, 2"
HESSIAN SKETCH,0.21700879765395895,"3: i∗←arg mini=1,2( bZi,2/ bZi,1)
4: bx ←solution of (5) with S = Si∗
5: return bx
6: function ESTIMATE(S, A)
7:
T ←sparse (1±η)-subspace embedding
matrix for d-dimensional subspaces
8:
(Q, R) ←QR(TA)
9:
bZ1 ←σmin(SAR−1)
10:
bZ2 ←(1 ± η)-approximation to
(SAR−1)⊤(SAR−1) −I

op
11:
return ( bZ1, bZ2)"
HESSIAN SKETCH,0.21994134897360704,"In this section, we consider the minimization prob-
lem"
HESSIAN SKETCH,0.22287390029325513,"min
x∈C 1"
HESSIAN SKETCH,0.22580645161290322,"2 ∥SAx∥2
2 −⟨A⊤y, x⟩

,
(5)"
HESSIAN SKETCH,0.2287390029325513,"which is used as a subroutine for the IHS (cf. (2)).
We present an algorithm with the learned sketch
in Algorithm 2. To analyze its performance, we
deﬁne the following quantities (corresponding ex-
actly to the unconstrained case in (Pilanci & Wain-
wright, 2016))"
HESSIAN SKETCH,0.2316715542521994,"Z1(S) =
inf
v∈colsp(A)∩Sn−1 ∥Sv∥2
2 ,
Z2(S) =
sup
u,v∈colsp(A)∩Sn−1"
HESSIAN SKETCH,0.23460410557184752,"u, (S⊤S −In)v

."
HESSIAN SKETCH,0.2375366568914956,"When S is a (1 + ϵ)-subspace embedding of colsp(A), we have Z1(S) ≥1 −ϵ and Z2(S) ≤2ϵ."
HESSIAN SKETCH,0.2404692082111437,"For a general sketching matrix S, the following is the approximation guarantee of bZ1 and bZ2, which
are estimates of Z1(S) and Z2(S), respectively. The proof is postponed to Appendix C. The main
idea is that AR−1 is well-conditioned, where R is as calculated in Algorithm 2.
Lemma 4.1. Suppose that η ∈(0, 1"
HESSIAN SKETCH,0.2434017595307918,"3) is a small constant, A is of full rank and S has poly(d/η)
rows. The function ESTIMATE(S, A) returns in O((nnz(A) log 1"
HESSIAN SKETCH,0.24633431085043989,η +poly( d
HESSIAN SKETCH,0.24926686217008798,"η)) time bZ1, bZ2 which with"
HESSIAN SKETCH,0.25219941348973607,probability at least 0.99 satisfy that Z1(S)
HESSIAN SKETCH,0.25513196480938416,1+η ≤bZ1 ≤Z1(S)
HESSIAN SKETCH,0.25806451612903225,1−η and Z2(S)
HESSIAN SKETCH,0.26099706744868034,"(1+η)2 −3η ≤bZ2 ≤
Z2(S)
(1−η)2 + 3η."
HESSIAN SKETCH,0.26392961876832843,"Similar to Proposition 1 of (Pilanci & Wainwright, 2016), we have the following guarantee. The
proof is postponed to Appendix D.
Theorem 4.2. Let η ∈(0, 1"
HESSIAN SKETCH,0.2668621700879765,"3) be a small constant. Suppose that A is of full rank and S1 and S2
are both COUNT-SKETCH-type sketches with poly(d/η) rows. Algorithm 2 returns a solution bx"
HESSIAN SKETCH,0.2697947214076246,"which, with probability at least 0.98, satisﬁes that ∥A(bx −x∗)∥2 ≤(1 + η)4
min
 b
Z1,2"
HESSIAN SKETCH,0.2727272727272727,"b
Z1,1 ,
b
Z2,2"
HESSIAN SKETCH,0.2756598240469208,"b
Z2,1 o
+"
HESSIAN SKETCH,0.2785923753665689,"4η

∥Ax∗∥2 in O(nnz(A) log( 1"
HESSIAN SKETCH,0.28152492668621704,η) + poly( d
HESSIAN SKETCH,0.2844574780058651,"η)) time, where x∗= arg minx∈C ∥Ax −b∥2 is the least-
squares solution."
HESSIAN SKETCH,0.2873900293255132,"Theorem 4.2 suggests the following. If the ratio of the learned sketch Z2(S1)/Z1(S1) is a constant
smaller than that of the random sketch Z2(S2)/Z2(S2) and η is a constant fraction of the ratio
gap, then bZ1,2/ bZ1,1 is a constant smaller than bZ2,2/ bZ2,1, which means that the procedure of IHS
will converge faster with the learned sketch. In particular, if Si is a (1 + ϵi)-subspace embedding
matrix for colsp(A) with ϵi < 1/3 and η < γ min{|ϵ1 −ϵ2|, ϵ1, ϵ2} for some small constant
γ > 0, we have Z2(Si)/Z1(Si) ≤3ϵi and the guarantee in Theorem 4.2 becomes ∥A(bx −x∗)∥2 ≤
O(min{ϵ1, ϵ2}) ∥Ax∗∥2, that is, a better subspace embedding can lead to a faster convergence.
Hence, if the learned sketch is a better subspace embedding than a random sketch, theoretically we
can obtain a better convergence by setting η small enough; in practice we shall observe this."
HESSIAN SKETCH,0.2903225806451613,"Furthermore, if we know the indices of the rows of large leverage scores of A and the assumptions
in Corollary 3.2 are satisﬁed, we can use O(d2/ϵ2) rows to obtain a
 
1 + O(
ϵ
√"
HESSIAN SKETCH,0.2932551319648094,"d/ polylog(d/ϵ))

-"
HESSIAN SKETCH,0.2961876832844575,"subspace embedding using Corollary 3.2, which is almost a
√"
HESSIAN SKETCH,0.2991202346041056,d-factor better than the usual guarantee
HESSIAN SKETCH,0.3020527859237537,Under review as a conference paper at ICLR 2022
HESSIAN SKETCH,0.30498533724340177,"of a random COUNT-SKETCH matrix of the same dimension, leading to an algorithm of faster
convergence."
HESSIAN REGRESSION,0.30791788856304986,"5
HESSIAN REGRESSION"
HESSIAN REGRESSION,0.31085043988269795,Algorithm 3 Fast Regression Solver for (6)
HESSIAN REGRESSION,0.31378299120234604,"1: S1 ←learned sketch, S2 ←random sketch
2: (Qi, Ri) ←QR(SiA), i = 1, 2
3: (σi, σ′
i) ←EIG(AR−1
i ), i = 1, 2 ▷EIG(B) returns
estimates of σmax(B) and σmin(B)
4: i∗←mini=1,2(σi/σ′
i)
5: P ←R−1
i∗
6: η ←1/(σ2
i∗+ (σ′
i∗)2)
7: z0 ←0
8: while
A⊤APzt −y

2 ≥ϵ ∥y∥2 do
9:
zt+1 ←zt −η(P ⊤A⊤AP)(P ⊤A⊤APzt−P ⊤y)
10: return Pzt"
HESSIAN REGRESSION,0.31671554252199413,"In this section, we consider the minimiza-
tion problem"
HESSIAN REGRESSION,0.3196480938416422,"min
z
A⊤Az −y

2 ,
(6)"
HESSIAN REGRESSION,0.3225806451612903,"which is used as a subroutine for the un-
constrained convex optimization problem
minx f(x) with A⊤A being the Hessian
matrix ∇2f(x) (see (3)). Here A ∈Rn×d,
y ∈Rd, and we have access to A. We incorporate a learned sketch into the fast regression solver in
(van den Brand et al., 2020) and present the algorithm in Algorithm 3."
HESSIAN REGRESSION,0.3255131964809384,"Here the subroutine EIG(B) applies a (1 + η)-subspace embedding sketch T to B for some small
constant η and returns σmax(TB) and σmin(TB). Since B admits the form of AR, the sketched
matrix TB can be calculated as (TA)R and thus can be computed in O(nnz(A) + poly(d)) time if
T is a COUNT-SKETCH matrix of O(d2) rows. The extreme singular values of TB can be found by
SVD or the Lanczos algorithm."
HESSIAN REGRESSION,0.3284457478005865,"Similar to Lemma 4.2 in (van den Brand et al., 2020), we have the following guarantee of Algorithm 3.
The proof parallels the proof in (van den Brand et al., 2020) and is postponed to Appendix E.
Theorem 5.1. Suppose that S1 and S2 are both COUNT-SKETCH-type sketches with O(d2) rows.
Algorithm 3 returns a solution x′ such that ∥A⊤Ax′ −y∥2 ≤ϵ ∥y∥2 with probability at least 0.97.
The runtime is O(nnz(A)) + eO(nd · (min{σ1/σ′
1, σ2/σ′
2})2 · log(κ(A)/ϵ) + poly(d)).
Remark 5.2. In Algorithm 3, S2 can be chosen to be a subspace embedding matrix for d-dimensional
subspaces, in which case, AR−1
2
has condition number close to 1 (see, e.g., p38 of (Woodruff, 2014))
and the full algorithm would run faster than the trivial O(nd2)-time solver to (6).
Remark 5.3. For the original unconstrained convex optimization problem minx f(x), one can run
the entire optimization procedure with learned sketches versus the entire optimization procedure
with random sketches, compare the objective values at the end, and choose the better of the two.
For least-squares, f(x) = 1"
HESSIAN REGRESSION,0.3313782991202346,"2 ∥Ax −b∥2
2, and the value of f(x) can be approximated efﬁciently by a
sparse subspace embedding matrix in O(nnz(A) + nnz(b) + poly(d)) time."
EXPERIMENTS,0.3343108504398827,"6
EXPERIMENTS"
EXPERIMENTS,0.33724340175953077,"Comparison. We compare the learned sketch against three classical sketches: Gaussian, COUNT-
SKETCH, and SJLT (see Section 2) in all experiments. The quantity we compare is a certain error,
deﬁned individually for each problem, in each iteration of the IHS or the internal regression problem
in fast regression. All of our experiments are conducted on a laptop with a 1.90GHz CPU and 16GB
RAM. The ofﬂine training is done separately and the training of a single sketch matrix in our dataset
can be ﬁnished within 5 minutes using a single GPU. For the learned sketches with learned values of
nonzero entries, we take an average over three independent trials; for all other sketches, we take an
average over ﬁve independent trials. The details of the implementation are deferred to Appendix H."
EXPERIMENTS,0.34017595307917886,"We elaborate on the reason that the horizontal axes in the plots are in terms of iterations rather
than in terms of runtime. The learned matrix S is trained ofﬂine only once using the training
data. It is not computed while solving the optimization problem on the test data. Hence, no
additional computational cost is incurred in generating S other than solving the iteration step
using COUNT-SKETCH. Since Gaussian matrices and sparse JL transforms are denser than COUNT-
SKETCH matrices, they will be considerably slower in each round. Since we want to understand the
convergence behavior, an iteration count is more revealing than an overall time bound. If our learned
sketch performs no worse with respect to the total number of rounds (which our experiments show),
then it has an even greater advantage in runtime. To substantiate this claim, we show in Appendix F
an error-versus-runtime plot for the task of matrix estimation with nuclear norm constraints."
EXPERIMENTS,0.34310850439882695,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3460410557184751,"6.1
IHS EXPERIMENTS: LASSO"
EXPERIMENTS,0.3489736070381232,We deﬁne an instance of LASSO regression to be:
EXPERIMENTS,0.3519061583577713,"x∗= arg min
∥x∥1≤λ"
EXPERIMENTS,0.3548387096774194,"1
2 ∥Ax −b∥2
2 ,
(7)"
EXPERIMENTS,0.35777126099706746,where λ is a parameter. We use two real-world datasets:
EXPERIMENTS,0.36070381231671556,"• Electric2: residential electric load measurements. Each row of the matrix corresponds to
a different residence. Matrix columns are consecutive measurements from different times.
Ai ∈R370×9, bi ∈R370×1, and |(A, b)train| = 320, |(A, b)test| = 80. We set λ = 15.
• Greenhouse gas (GHG)3: time series of measured greenhouse gas concentrations in the Califor-
nia atmosphere. Each (A, b) corresponds to a different measurement location. Ai ∈R327×14,
bi ∈R327×1, and |(A, b)train| = 400, |(A, b)test| = 100. We set λ = 30."
EXPERIMENTS,0.36363636363636365,"Experiment Setting: We choose m = 6d, 8d, 10d for both datasets.
We consider the error
1
2 ∥Ax −b∥2
2 −1"
EXPERIMENTS,0.36656891495601174,"2 ∥Ax∗−b∥2
2. For the two datasets, we use both the methods proposed in Sec-
tion 3. For the heavy-row Count-Sketch, we allocate 30% of the sketch space to the rows of heavy
leverage score. For the Electric dataset, each row represents a speciﬁc residence and the indices of
the heavy rows do not vary much across the matrices in the training data. We select the heavy rows
according to the number of times each row is heavy in the training data for the heavy rows. We also
consider optimizing the non-zero values after identifying the heavy rows. For the GHG dataset, each
row represents a speciﬁc time point and the heavy rows are not very concentrated. Nevertheless, we
can ﬁnd a superset of about 30% of the rows that contains most of the heavy rows, based on the
counts on the training data. Then we prune the superset by selecting the rows with the largest ℓ2
norms, subject to the dimension budget. This will incur an additional computational cost, but the time
is almost the same as the time to read the sub-matrix of these rows, and it can be used in all iterations,
so the time of this step is negligible compared to the total runtime. We might lose a small fraction
of heavy rows, but it only negligibly affects the experiments. The distribution on the indices of the
heavy rows over the dataset is discussed in Appendix G."
EXPERIMENTS,0.36950146627565983,"Experimental Result: We plot in a logarithmic scale the mean errors of the two datasets in Figures 1
and 2. We see all methods display linear convergence, that is, letting ek denote the error in the k-th
iteration, we have ek ≈ρke1 for some convergence rate ρ. A smaller convergence rate implies a
faster convergence."
EXPERIMENTS,0.3724340175953079,"We calculate an estimated rate of convergence ρ = (ek/e1)1/k with k = 10 for the GHG dataset, and
with k = 7 for the Electric dataset. For the GHG dataset, we can see that when the sketch size is
small (m = 6d), the gradient-based learned sketch has a rate of convergence that is 56% of that of
COUNT-SKETCH, and the heavy-rows sketch has a convergence rate that is 86.9%. When the sketch
size is large (m = 10d), the gradient-based learned sketch has a convergence rate that is 63.7%, and
the heavy-rows sketch is 82.1%. For the Electric dataset, both sketches, especially the heavy-rows
sketch, show signiﬁcant improvements. When the sketch size is small, the combined-learned sketch
has a convergence rate that is just 21.1% of that of sparse JL, and when the sketch size is large, the
combined-learned sketch has a smaller convergence rate that is just 15.4%."
EXPERIMENTS,0.375366568914956,"We also conducted IHS experiments for the matrix estimation problem with a nuclear norm constraint
in Appendix F."
EXPERIMENTS,0.3782991202346041,"2
4
6
8
10
iteration round 6 4 2 0 2"
EXPERIMENTS,0.3812316715542522,log_10(error)
EXPERIMENTS,0.3841642228739003,"learned(value-only)
count-sketch
learned(heavy rows)
gaussian
sparse-jl"
EXPERIMENTS,0.3870967741935484,"2
4
6
8
10
iteration round 6 4 2 0 2"
EXPERIMENTS,0.39002932551319647,log_10(error)
EXPERIMENTS,0.39296187683284456,"learned(value-only)
count-sketch
learned(heavy rows)
gaussian
sparse-jl"
EXPERIMENTS,0.39589442815249265,"1
2
3
4
5
6
7
8
9
iteration round 6 4 2 0 2"
EXPERIMENTS,0.39882697947214074,log_10(error)
EXPERIMENTS,0.40175953079178883,"learned(value-only)
count-sketch
learned(heavy rows)
gaussian
sparse-jl"
EXPERIMENTS,0.4046920821114369,Figure 1: Test error of LASSO in the Green House Gas dataset.
EXPERIMENTS,0.40762463343108507,"2https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014
3https://archive.ics.uci.edu/ml/datasets/Greenhouse+Gas+Observing+Network"
EXPERIMENTS,0.41055718475073316,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.41348973607038125,"1
2
3
4
5
6
7
iteration round 8 6 4 2 0 2"
EXPERIMENTS,0.41642228739002934,log_10(error)
EXPERIMENTS,0.41935483870967744,"learned(value-only)
count-sketch
learned(heavy rows)
gaussian
sparse-jl
learned(combined)"
EXPERIMENTS,0.4222873900293255,"1
2
3
4
5
6
7
iteration round 10 8 6 4 2 0 2"
EXPERIMENTS,0.4252199413489736,log_10(error)
EXPERIMENTS,0.4281524926686217,"learned(value-only)
count-sketch
learned(heavy rows)
gaussian
sparse-jl
learned(combined)"
EXPERIMENTS,0.4310850439882698,"1
2
3
4
5
6
7
iteration round 10 8 6 4 2 0 2"
EXPERIMENTS,0.4340175953079179,log_10(error)
EXPERIMENTS,0.436950146627566,"learned(value-only)
count-sketch
learned(heavy rows)
gaussian
sparse-jl
learned(combined)"
EXPERIMENTS,0.4398826979472141,Figure 2: Test error of LASSO in Electric dataset.
EXPERIMENTS,0.44281524926686217,"1.00
1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00
iteration round 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 error"
EXPERIMENTS,0.44574780058651026,"gaussian(eta = 1)
gaussian(eta = 0.2)
sparse-JL(eta = 1)
sparse-JL(eta = 0.2)
learned(heavy rows)"
EXPERIMENTS,0.44868035190615835,"1.00
1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00
iteration round 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 error"
EXPERIMENTS,0.45161290322580644,"gaussian(eta = 1)
gaussian(eta = 0.2)
sparse-JL(eta = 1)
sparse-JL(eta = 0.2)
learned(heavy rows)"
EXPERIMENTS,0.45454545454545453,"1.00
1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00
iteration round 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
EXPERIMENTS,0.4574780058651026,"gaussian(eta = 1)
gaussian(eta = 0.2)
sparse-JL(eta = 1)
sparse-JL(eta = 0.2)
learned(heavy rows)"
EXPERIMENTS,0.4604105571847507,Figure 3: Test error of the subroutine in fast regression on Electric dataset.
FAST REGRESSION EXPERIMENTS,0.4633431085043988,"6.2
FAST REGRESSION EXPERIMENTS"
FAST REGRESSION EXPERIMENTS,0.4662756598240469,"2
4
6
8
10
12
14
iteration round 4 3 2 1 0 1 2 3 4"
FAST REGRESSION EXPERIMENTS,0.46920821114369504,log_10(error)
FAST REGRESSION EXPERIMENTS,0.47214076246334313,"gaussian
sparse-JL
learned(heavy rows)"
FAST REGRESSION EXPERIMENTS,0.4750733137829912,"Figure 4: Test error of fast regres-
sion on Electric dataset"
FAST REGRESSION EXPERIMENTS,0.4780058651026393,"We consider the unconstrained least squares problem minx f(x)
with f(x) = 1"
FAST REGRESSION EXPERIMENTS,0.4809384164222874,"2 ∥Ax −b∥2
2 using the Electric dataset."
FAST REGRESSION EXPERIMENTS,0.4838709677419355,"Training: Note that ∇2f(x) = A⊤A, independent of x. In
the t-th round of Newton’s method, by (3), we need to solve
a regression problem minz
A⊤Az −y
2
2 with y = ∇f(xt).
Hence, we can use the same two methods in the preceding
subsection to optimize the learned sketch Si. For a general
problem where ∇2f(x) depends on x, one can take xt to be the solution obtained from Algorithm 3
using the learned sketch St to generate A and y for the (t + 1)-st round, train a learned sketch St+1,
and repeat this process."
FAST REGRESSION EXPERIMENTS,0.4868035190615836,"Setup for Experiments: For the Electric dataset, we set m = 10d = 90. We compare the heavy-rows
COUNT-SKETCH matrix with the three classical random sketches, COUNT-SKETCH, Gaussian and
Sparse-JL. For the parameter η in Algorithm 3, we set η = 1 in all iterations for heavy-rows sketches.
For the classical random sketches, we set η in the following two ways: (a) η = 1 in all iterations and
(b) η = 1 in the ﬁrst iteration and η = 0.2 in all subsequent iterations."
FAST REGRESSION EXPERIMENTS,0.4897360703812317,"Experimental Results: We examine the accuracy of the subproblem (6) and deﬁne the error to
be
A⊤ARzt −y

2 / ∥y∥2. We consider the subproblems in the ﬁrst three iterations of the global
Newton method. The results are plotted in Figure 3. In this task, the COUNT-SKETCH causes a
terrible divergence of the subroutine and is thus omitted in the plots. Still, we observe that in setting
(a) of η, the other two classical sketches cause the subroutine to diverge. In setting (b) of η, the
other two classical sketches lead to convergence but their error is signiﬁcantly larger than that of the
heavy-rows sketches, in each of the ﬁrst three calls to the subroutine. The error of the heavy-rows
sketch is less than 0.01 in all iterations of all three subroutine calls, in both setting (a) and (b) of η."
FAST REGRESSION EXPERIMENTS,0.49266862170087977,"We also plot a ﬁgure on the convergence of the global Newton method. Here, for each subroutine, we
only run one iteration, and plot the error of the original least squares problem. The result is shown in
Figure 4, which clearly displays a signiﬁcantly faster decay with heavy-rows sketches. The rate of
convergence using heavy-rows sketches is 80.6% of that using Gaussian or sparse JL sketches.
CONCLUSION. We demonstrated the superiority of using learned sketches over classical random
sketches, for the Iterative Hessian Sketch method which is used for a number of problems in
convex optimization. Compared with random sketches, our learned sketches of the same size yield
considerably faster convergence. We also provably show a better subspace embedding property of a
sketch of the same size given an oracle for predicting a superset of rows with large leverage score.
Our experiments show the construction of such an oracle is possible for real data sets, and they
demonstrate a signiﬁcant advantage over non-learned sketches for problems in convex optimization."
FAST REGRESSION EXPERIMENTS,0.49560117302052786,Under review as a conference paper at ICLR 2022
REFERENCES,0.49853372434017595,REFERENCES
REFERENCES,0.501466275659824,"Akshay Agrawal, Brandon Amos, Shane T. Barratt, Stephen P. Boyd, Steven Diamond, and J. Zico
Kolter. Differentiable convex optimization layers. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada, pp. 9558–9570, 2019."
REFERENCES,0.5043988269794721,"Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981–3989, 2016."
REFERENCES,0.5073313782991202,"Jean Bourgain, Sjoerd Dirksen, and Jelani Nelson. Toward a uniﬁed theory of sparse dimensionality
reduction in Euclidean space. Geometric and Functional Analysis, pp. 1009–1088, 2015."
REFERENCES,0.5102639296187683,"Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004."
REFERENCES,0.5131964809384164,"Kenneth L. Clarkson and David P. Woodruff. Low-rank approximation and regression in input
sparsity time. J. ACM, 63(6), January 2017. ISSN 0004-5411. doi: 10.1145/3019134. URL
https://doi.org/10.1145/3019134."
REFERENCES,0.5161290322580645,"Michael B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities. In Proceedings
of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’16, pp.
278–287, USA, 2016. Society for Industrial and Applied Mathematics. ISBN 9781611974331."
REFERENCES,0.5190615835777126,"Graham Cormode and Charlie Dickens. Iterative hessian sketch in input sparsity time. In Proceedings
of 33rd Conference on Neural Information Processing Systems (NeurIPS), Vancouver, Canada,
2019."
REFERENCES,0.5219941348973607,"Nikita Doikov and Peter Richtárik. Randomized block cubic Newton method. In Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,
Sweden, July 10-15, 2018, pp. 1289–1297, 2018."
REFERENCES,0.5249266862170088,"Yihe Dong, Piotr Indyk, Ilya P. Razenshteyn, and Tal Wagner. Learning space partitions for nearest
neighbor search. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020, 2020."
REFERENCES,0.5278592375366569,"Robert M. Gower, Donald Goldfarb, and Peter Richtárik. Stochastic block BFGS: squeezing more
curvature out of data. In Proceedings of the 33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1869–1878, 2016."
REFERENCES,0.530791788856305,"Robert M. Gower, Filip Hanzely, Peter Richtárik, and Sebastian U. Stich. Accelerated stochastic
matrix inversion: General theory and speeding up BFGS rules for faster second-order optimiza-
tion. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pp.
1626–1636, 2018."
REFERENCES,0.533724340175953,"Robert M. Gower, Dmitry Kovalev, Felix Lieder, and Peter Richtárik. RSN: randomized subspace
Newton. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
BC, Canada, pp. 614–623, 2019."
REFERENCES,0.5366568914956011,"Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
algorithms. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019, 2019."
REFERENCES,0.5395894428152492,"Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 7400–7410,
2019."
REFERENCES,0.5425219941348973,"Sudhir B. Kylasa, Fred (Farbod) Roosta, Michael W. Mahoney, and Ananth Grama. GPU accelerated
sub-sampled Newton’s method for convex classiﬁcation problems. In Proceedings of the 2019
SIAM International Conference on Data Mining, SDM 2019, Calgary, Alberta, Canada, May 2-4,
2019, pp. 702–710, 2019."
REFERENCES,0.5454545454545454,Under review as a conference paper at ICLR 2022
REFERENCES,0.5483870967741935,"Xiang Li, Shusen Wang, and Zhihua Zhang. Do subsampled newton methods work for high-
dimensional data? In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020, pp. 4723–4730. AAAI Press, 2020."
REFERENCES,0.5513196480938416,"Simin Liu, Tianrui Liu, Ali Vakilian, Yulin Wan, and David P. Woodruff. On learned sketches for
randomized numerical linear algebra. arXiv:2007.09890 [cs.LG], 2020. URL https://arxiv.org/abs/
2007.09890."
REFERENCES,0.5542521994134897,"J. Nelson and H. L. Nguyên. Osnap: Faster numerical linear algebra algorithms via sparser subspace
embeddings. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pp.
117–126, 2013."
REFERENCES,0.5571847507331378,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, pp. 8024–8035, 2019."
REFERENCES,0.5601173020527859,"Mert Pilanci and Martin J. Wainwright. Iterative Hessian sketch: Fast and accurate solution approxi-
mation for constrained least-squares. J. Mach. Learn. Res., 17:53:1–53:38, 2016."
REFERENCES,0.5630498533724341,"Mert Pilanci and Martin J. Wainwright. Newton sketch: A near linear-time optimization algorithm
with linear-quadratic convergence. SIAM J. Optim., 27(1):205–245, 2017."
REFERENCES,0.5659824046920822,"Farbod Roosta-Khorasani and Michael W. Mahoney. Sub-sampled Newton methods. Math. Program.,
174(1-2):293–326, 2019."
REFERENCES,0.5689149560117303,"T. Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006
47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 143–152,
2006."
REFERENCES,0.5718475073313783,"Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)
neural networksin near-linear time. arXiv:2006.11648 [cs.LG], 2020."
REFERENCES,0.5747800586510264,"Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Yonina C.
Eldar and Gitta Kutyniok (eds.), Compressed Sensing: Theory and Applications, pp. 210–268.
Cambridge University Press, 2012. doi: 10.1017/CBO9780511794308.006."
REFERENCES,0.5777126099706745,"David P. Woodruff. Sketching as a tool for numerical linear algebra. 10(1–2):1–157, October 2014.
ISSN 1551-305X. doi: 10.1561/0400000060. URL https://doi.org/10.1561/0400000060."
REFERENCES,0.5806451612903226,"Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher Ré, and Michael W. Mahoney. Sub-
sampled Newton methods with non-uniform sampling. In Advances in Neural Information Process-
ing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December
5-10, 2016, Barcelona, Spain, pp. 3000–3008, 2016."
REFERENCES,0.5835777126099707,"Peng Xu, Fred Roosta, and Michael W. Mahoney. Second-order optimization for non-convex machine
learning: an empirical study. In Proceedings of the 2020 SIAM International Conference on Data
Mining, SDM 2020, Cincinnati, Ohio, USA, May 7-9, 2020, pp. 199–207, 2020."
REFERENCES,0.5865102639296188,Under review as a conference paper at ICLR 2022
REFERENCES,0.5894428152492669,"A
PROOF OF THEOREM 3.1"
REFERENCES,0.592375366568915,"First we prove the following lemma.
Lemma A.1. Let δ ∈(0, 1/m]. It holds with probability at least 1 −δ that"
REFERENCES,0.5953079178885631,"sup
x∈colsp(A)"
REFERENCES,0.5982404692082112,"∥Sx∥2
2 −∥x∥2
2
 ≤ϵ ∥x∥2
2 ,"
REFERENCES,0.6011730205278593,provided that
REFERENCES,0.6041055718475073,"m ≳ϵ−2((d + log m) min{log2(d/ϵ), log2 m} + d log(1/δ)),"
REFERENCES,0.6070381231671554,"1 ≳ϵ−2ν((log m) min{log2(d/ϵ), log2 m} + log(1/δ)) log(1/δ)."
REFERENCES,0.6099706744868035,"Proof. We shall adapt the proof of Theorem 5 in (Bourgain et al., 2015) to our setting. Let T
denote the unit sphere in colsp(A) and set the sparsity parameter s = 1. Observe that ∥Sx∥2
2 =
∥xI∥2
2 + ∥SxIc∥2
2, and so it sufﬁces to show that"
REFERENCES,0.6129032258064516,"Pr
n∥S′xIc∥2
2 −∥xIc∥2
2
 > ϵ
o
≤δ"
REFERENCES,0.6158357771260997,"for x ∈T. We make the following deﬁnition, as in (2.6) of Bourgain et al. (2015):"
REFERENCES,0.6187683284457478,"Aδ,x := m
X i=1 X"
REFERENCES,0.6217008797653959,"j∈Ic
δijxjei ⊗ej,"
REFERENCES,0.624633431085044,"and thus, S′xIc = Aδ,xσ. Also by E ∥S′xIc∥2
2 = ∥xIc∥2
2, one has"
REFERENCES,0.6275659824046921,"sup
x∈T"
REFERENCES,0.6304985337243402,"∥S′xIc∥2
2 −∥xIc∥2
2
 = sup
x∈T"
REFERENCES,0.6334310850439883,"∥Aδ,xσ∥2
2 −E ∥Aδ,xσ∥2
2
 .
(8)"
REFERENCES,0.6363636363636364,"Now, in (2.7) of Bourgain et al. (2015) we instead deﬁne a seminorm"
REFERENCES,0.6392961876832844,"∥x∥δ = max
1≤i≤m  X"
REFERENCES,0.6422287390029325,"j∈Ic
δijx2
j   1/2 ."
REFERENCES,0.6451612903225806,"Then (2.8) continues to hold, and (2.9) as well as (2.10) continue to hold if the supremum in the
left-hand side is replaced with the left-hand side of (8). At the beginning of Theorem 5, we deﬁne
U (i) to be U, but each row j ∈Ic is multiplied by δij and each row j ∈I is zeroed out. Then we
have in the ﬁrst step of (4.5) that X"
REFERENCES,0.6480938416422287,"j∈Ic
δij  d
X"
REFERENCES,0.6510263929618768,"k=1
gk⟨fk, ej⟩  2"
REFERENCES,0.6539589442815249,"≤
U (i)g

2 2 ,"
REFERENCES,0.656891495601173,"instead of equality. One can verify that the rest of (4.5) goes through. It remains true that ∥·∥δ ≤
(1/√s) ∥·∥2, and thus (4.6) holds. One can verify that the rest of the proof of Theorem 5 in Bourgain
et al. (2015) continues to hold if we replace Pn
j=1 with P
j∈Ic and max1≤j≤n with maxj∈Ic, noting
that
E
X"
REFERENCES,0.6598240469208211,"j∈Ic
δij ∥PEej∥2
2 = s m X"
REFERENCES,0.6627565982404692,"j∈Ic
⟨PEej, ej⟩≤s md"
REFERENCES,0.6656891495601173,"and
E(U (i))∗U (i) =
X"
REFERENCES,0.6686217008797654,"j∈Ic
(Eδij)uju∗
j ⪯1 m."
REFERENCES,0.6715542521994134,"Thus, the symmetrization inequalities on X"
REFERENCES,0.6744868035190615,"j∈Ic
δij ∥PEej∥2
2 Lp
δ and  X"
REFERENCES,0.6774193548387096,"j∈Ic
δijuju∗
j Lp
δ"
REFERENCES,0.6803519061583577,"continue to hold. The result then follows, observing that maxj∈Ic ∥PEej∥2 ≤ν."
REFERENCES,0.6832844574780058,Under review as a conference paper at ICLR 2022
REFERENCES,0.6862170087976539,The subspace embedding guarantee now follows as a corollary.
REFERENCES,0.6891495601173021,"Theorem 3.1. Let ν = ϵ/d. Suppose that m = Ω((d/ϵ2)(polylog(1/ϵ) + log(1/δ))), δ ∈(0, 1/m)
and d = Ω((1/ϵ) polylog(1/ϵ) log2(1/δ)). Then, there exists a distribution on S with m + |I| rows
such that
Pr
n
∀x ∈colsp(A),
∥Sx∥2
2 −∥x∥2
2
 > ϵ ∥x∥2
2
o
≤δ."
REFERENCES,0.6920821114369502,Proof. One can verify that the two conditions in Lemma A.1 are satisﬁed if m ≳d ϵ2
REFERENCES,0.6950146627565983,"
polylog(d"
REFERENCES,0.6979472140762464,"ϵ ) + log 1 δ 
, d ≳1 ϵ"
REFERENCES,0.7008797653958945,"
log 1 δ"
REFERENCES,0.7038123167155426," 
polylog(d"
REFERENCES,0.7067448680351907,"ϵ ) + log 1 δ 
."
REFERENCES,0.7096774193548387,The last condition is satisﬁed if d ≳1 ϵ
REFERENCES,0.7126099706744868,"
log2 1 δ"
REFERENCES,0.7155425219941349,"
polylog
1 ϵ 
."
REFERENCES,0.718475073313783,"B
PROOF OF LEMMA 3.3"
REFERENCES,0.7214076246334311,"Proof. On the one hand, since Q = SAR is an orthogonal matrix, we have"
REFERENCES,0.7243401759530792,"∥x∥2 = ∥Qx∥2 = ∥SARx∥2 .
(9)"
REFERENCES,0.7272727272727273,"On the other hand, the assumption implies that
(ARx)T (ARx) −xT x

2 ≤ϵ ∥x∥2
2 ,"
REFERENCES,0.7302052785923754,"that is,
(1 −ϵ) ∥x∥2
2 ≤∥ARx∥2
2 ≤(1 + ϵ) ∥x∥2
2 .
(10)"
REFERENCES,0.7331378299120235,"Combining both (9) and (10) leads to
√"
REFERENCES,0.7360703812316716,"1 −ϵ ∥SARx∥2 ≤∥ARx∥2 ≤
√"
REFERENCES,0.7390029325513197,"1 + ϵ ∥SARx∥2 ,
∀x ∈Rd"
REFERENCES,0.7419354838709677,"Equivalently, it can be written as"
REFERENCES,0.7448680351906158,"1
√1 + ϵ ∥SAy∥2 ≤∥Ay∥2 ≤
1
√1 −ϵ ∥SAy∥2 ,
∀y ∈Rd."
REFERENCES,0.7478005865102639,"The claimed result follows from the fact that 1/√1 + ϵ ≥1 −ϵ and 1/√1 −ϵ ≤1 + ϵ whenever
ϵ ∈(0,
√ 5−1 2
]."
REFERENCES,0.750733137829912,"C
PROOF OF LEMMA 4.1"
REFERENCES,0.7536656891495601,"Suppose that AR−1 = UW, where U ∈Rn×d has orthonormal columns, which form an orthonormal
basis of the column space of A. Since T is a subspace embedding of the column space of A with
probability 0.99, it holds for all x ∈Rd that"
REFERENCES,0.7565982404692082,"1
1 + η"
REFERENCES,0.7595307917888563,"TAR−1x

2 ≤
AR−1x

2 ≤
1
1 −η"
REFERENCES,0.7624633431085044,"TAR−1x

2 ."
REFERENCES,0.7653958944281525,"Since
TAR−1x

2 = ∥Qx∥2 = ∥x∥2
and
∥Wx∥2 = ∥UWx∥2 =
AR−1x

2
(11)"
REFERENCES,0.7683284457478006,"we have that
1
1 + η ∥x∥2 ≤∥Wx∥2 ≤
1
1 −η ∥x∥2 ,
x ∈Rd.
(12)"
REFERENCES,0.7712609970674487,Under review as a conference paper at ICLR 2022
REFERENCES,0.7741935483870968,It is easy to see that
REFERENCES,0.7771260997067448,"Z1(S) = min
x∈Sd−1 ∥SUx∥2 = min
y̸=0
∥SUWy∥2"
REFERENCES,0.7800586510263929,"∥Wy∥2
,"
REFERENCES,0.782991202346041,"and thus,"
REFERENCES,0.7859237536656891,"min
y̸=0(1 −η)∥SUWy∥2"
REFERENCES,0.7888563049853372,"∥y∥2
≤Z1(S) ≤min
y̸=0(1 + η)∥SUWy∥2"
REFERENCES,0.7917888563049853,"∥y∥2
."
REFERENCES,0.7947214076246334,"Recall that SUW = SAR−1. We see that
(1 −η)σmin(SAR−1) ≤Z1(S) ≤(1 + η)σmin(SAR−1)."
REFERENCES,0.7976539589442815,"By deﬁnition,
Z2(S) =
U T (S⊤S −In)U

op .
It follows from (12) that
(1 −η)2 W T U T (ST S −In)UW

op ≤Z2(S) ≤(1 + η)2 W T U T (ST S −In)UW

op ."
REFERENCES,0.8005865102639296,"and from (12), (11) and Lemma 5.36 of Vershynin (2012) that
(AR−1)⊤(AR−1) −I

op ≤3η."
REFERENCES,0.8035190615835777,"Since
W T U T (ST S −In)UW

op =
(AR−1)⊤(ST S −In)AR−1
op
and
(AR−1)⊤ST SAR−1 −I

op −
(AR−1)⊤(AR−1) −I

op
≤
(AR−1)⊤(ST S −In)AR−1
op
≤
(AR−1)⊤ST SAR−1 −I

op +
(AR−1)⊤(AR−1) −I

op ,"
REFERENCES,0.8064516129032258,"it follows that
(1 −η)2 (SAR−1)⊤SAR−1 −I

op −3(1 −η)2η"
REFERENCES,0.8093841642228738,≤Z2(S)
REFERENCES,0.8123167155425219,"≤(1 + η)2 (SAR−1)⊤SAR−1 −I

op + 3(1 + η)2η."
REFERENCES,0.8152492668621701,We have so far proved the correctness of the approximation and we shall analyze the runtime below.
REFERENCES,0.8181818181818182,"Since S and T are sparse, computing SA and TA takes O(nnz(A)) time. The QR decomposition
of TA, which is a matrix of size poly(d/η) × d, can be computed in poly(d/η) time. The matrix
SAR−1 can be computed in poly(d) time. Since it has size poly(d/η) × d, its smallest singular
value can be computed in poly(d/η) time. To approximate Z2(S), we can use the power method to
estimate
(SAR−1)T SAR−1 −I

op up to a (1 ± η)-factor in O((nnz(A) + poly(d/η)) log(1/η))
time."
REFERENCES,0.8211143695014663,"D
PROOF OF THEOREM 4.2"
REFERENCES,0.8240469208211144,"In Lemma 4.1, we have with probability at least 0.99 that
bZ2
bZ1
≥"
REFERENCES,0.8269794721407625,"1
(1+η)2 Z2(S) −3η"
REFERENCES,0.8299120234604106,"1
1−ηZ1(S)
≥
1 −η
(1 + η)2
Z2(S)
Z1(S) −
3η
Z1(S)."
REFERENCES,0.8328445747800587,"When S is random subspace embedding, it holds with probability at least 0.99 that Z1(S) ≥3/4 and
so, by a union bound, it holds with probability at least 0.98 that
bZ2
bZ1
≥
1
(1 + η)4
Z2(S)
Z1(S) −4η,"
REFERENCES,0.8357771260997068,"or,
Z2(S)
Z1(S) ≤(1 + η)4
 bZ2"
REFERENCES,0.8387096774193549,"bZ1
+ 4η ! ."
REFERENCES,0.841642228739003,"The correctness of our claim then follows from Proposition 1 of Pilanci & Wainwright (2016),
together with the fact that S2 is a random subspace embedding. The runtime follows from Lemma 4.1
and Theorem 2.2 of Cormode & Dickens (2019)."
REFERENCES,0.844574780058651,Under review as a conference paper at ICLR 2022
REFERENCES,0.8475073313782991,"E
PROOF OF THEOREM 5.1"
REFERENCES,0.8504398826979472,"The proof follows a similar argument to that in (van den Brand et al., 2020, Lemma B.1). In van den
Brand et al. (2020), it is assumed (in our notation) that 3/4 ≤σmin(AP) ≤σmax(AP) ≤5/4
and thus one can set η = 1 in Algorithm 3 and achieve a linear convergence. The only dif-
ference is that here we estimate σmin(AP) and σmax(AP) and set the step size η in the gradi-
ent descent algorithm accordingly. By standard bounds for gradient descent (see, e.g., p468 of
Boyd & Vandenberghe (2004)), with a choice of step size η = 2/(σ2
max(AP) + σ2
min(AP)), after
O((σmax(AP)/σmin(AP))2 log(1/ϵ)) iterations, we can ﬁnd zt such that
P ⊤A⊤AP(zt −z∗)

2 ≤ϵ
P ⊤A⊤AP(z0 −z∗)

2 ,"
REFERENCES,0.8533724340175953,"where z∗= arg minz
P ⊤A⊤APz −P ⊤y

2 is the optimal least-squares solution. This establishes
Eq. (11) in the proof in van den Brand et al. (2020), and the rest of the proof follows as in there."
REFERENCES,0.8563049853372434,"We use three subspace embeddings here, S1, S2 and one used in the EIG subrountine. Each subspace
embedding uses O(d2) rows with a constant distortion parameter and a failure probability of 0.01.
The overall failure probability is thus 0.03."
REFERENCES,0.8592375366568915,"F
IHS EXPERIMENTS: MATRIX ESTIMATION WITH NUCLEAR NORM
CONSTRAINT"
REFERENCES,0.8621700879765396,"In many applications, for the problem"
REFERENCES,0.8651026392961877,"X∗:= arg min
X∈Rd1×d2
∥AX −B∥2
F ,"
REFERENCES,0.8680351906158358,"it is reasonable to model the matrix X∗as having low rank. Similar to the ℓ1-minimization for
compressive sensing, a standard relaxation of the rank constraint is to minimize the nuclear norm of
X, deﬁned as ∥X∥∗:= Pmin{d1,d2}
j=1
σj(X), where σj(X) is the j-th largest singular value of X."
REFERENCES,0.8709677419354839,"Hence, the matrix estimation problem we consider here is"
REFERENCES,0.873900293255132,"X∗:= arg min
X∈Rd1×d2
∥AX −B∥2
F
such that
∥X∥∗≤ρ,"
REFERENCES,0.8768328445747801,where ρ > 0 is a user-deﬁned radius as a regularization parameter.
REFERENCES,0.8797653958944281,We conduct experiments on the following datasets:
REFERENCES,0.8826979472140762,"• Tunnel4: The data set is a time series of gas concentrations measured by eight sensors in a
wind tunnel. Each (A, B) corresponds to a different data collection trial. Ai ∈R13530×5, Bi ∈
R13530×6, |(A, B)|train = 144, |(A, B)|test = 36. In our nuclear norm constraint, we set
ρ = 10."
REFERENCES,0.8856304985337243,"Experiment Setting: We choose m = 7d, 10d for the Tunnel dataset. We consider the error
1
2 ∥AX −B∥2
2 −1"
REFERENCES,0.8885630498533724,"2 ∥AX∗−B∥2
2. The leverage scores of this dataset are very uniform. Hence, for
this experiment we only consider optimizing the values of the non-zero entries."
REFERENCES,0.8914956011730205,"Results of Our Experiments: We plot in a logarithmic scale the mean errors of the two datasets in
Figures 5. We can see that when m = 7d, the gradient-based sketch, based on the ﬁrst 6 iterations,
has a rate of convergence that is 48% of the random sketch, and when m = 10d, the gradient-based
sketch has a rate of convergence that is 29% of the random sketch."
REFERENCES,0.8944281524926686,"Runtime of Learned Sketch.
As stated in Section 2, our learned sketch matrices S are all COUNT-
SKETCH-type matrices (each column contains a single nonzero entry), the matrix product SA can
thus be computed in O(nnz(A)) time and the overall algorithm is expected to be fast. To verify this,
we plot in an error-versus-runtime plot for matrix estimation with nuclear norm constraint tasks with
m = 10d in Figures 6 (corresponding to the datasets in Figure 5). The runtime consists only of the
time for sketching and solving the optimization problem and does not include the time for loading the
data. We run the same experiment three times. Each time we take an average over all test data. From
the plot we can observe that the learned sketch and COUNT-SKETCH have the fastest runtimes, which
are slightly faster than that of the SJLT and signiﬁcantly faster than that of the Gaussian sketch."
REFERENCES,0.8973607038123167,4https://archive.ics.uci.edu/ml/datasets/Gas+sensor+array+exposed+to+turbulent+gas+mixtures
REFERENCES,0.9002932551319648,Under review as a conference paper at ICLR 2022
REFERENCES,0.9032258064516129,"2
4
6
8
10
iteration round 5 4 3 2 1 0 1 2 3"
REFERENCES,0.906158357771261,log_10(error)
REFERENCES,0.9090909090909091,"learned(value-only)
count-sketch
gaussian
sparse JL"
REFERENCES,0.9120234604105572,"1
2
3
4
5
6
7
8
9
iteration round 8 6 4 2 0 2"
REFERENCES,0.9149560117302052,log_10(error)
REFERENCES,0.9178885630498533,"learned(value-only)
count-sketch
gaussian
sparse JL"
REFERENCES,0.9208211143695014,Figure 5: Test error of matrix estimation with nuclear norm constraint on Tunnel dataset
REFERENCES,0.9237536656891495,"0.1
0.2
0.3
0.4
0.5
runtime 8 6 4 2 0 2"
REFERENCES,0.9266862170087976,log(error)
REFERENCES,0.9296187683284457,"learned
count-sketch
gaussian
sparse JL"
REFERENCES,0.9325513196480938,Figure 6: Test error of matrix estimation with nuclear norm constraint on Tunnel dataset
REFERENCES,0.9354838709677419,"G
HEAVY LEVERAGE SCORE ROWS DISTRIBUTION OVER THE DATASET"
REFERENCES,0.9384164222873901,"In our experiments, we hypothesize that in real-world data that there may be an underlying pattern
which can help us identify the heavy rows. In the Electric dataset, each row of the matrix corresponds
to a speciﬁc residence and the heavy rows are always concentrated on some speciﬁc rows; in the
GHG data set, each row corresponds to a speciﬁc time point and we can select some speciﬁc time
points to be a superset of the heavy rows and then select the heavy rows based on their ℓ2-norm in
this superset."
REFERENCES,0.9413489736070382,"To exemplify this, we study the heavy leverage score rows distribution over the Electirc dataset. For
a row i ∈[370], let fi denote the times that row i is heavy out of 320 training data points from the
Electric dataset, where we say row i is heavy if ℓi ≥5d/n. Below we list all 74 pairs (i, fi) with
fi > 0."
REFERENCES,0.9442815249266863,"(195,320), (278,320), (361,320), (207,317), (227,285), (240,284), (219,270), (275,232), (156,214),
(322,213), (193,196), (190,192), (160,191), (350,181), (63,176), (42,168), (162,148), (356,129),
(363,110), (362,105), (338,95), (215,94), (234,93), (289,81), (97,80), (146,70), (102,67), (98,58),
(48,57), (349,53), (165,46), (101,41), (352,40), (293,34), (344,29), (268,21), (206,20), (217,20),
(327,20), (340,19), (230,18), (359,18), (297,14), (357,14), (161,13), (245,10), (100,8), (85,6), (212,6),
(313,6), (129,5), (130,5), (366,5), (103,4), (204,4), (246,4), (306,4), (138,3), (199,3), (222,3), (360,3),
(87,2), (154,2), (209,2), (123,1), (189,1), (208,1), (214,1), (221,1), (224,1), (228,1), (309,1), (337,1),
(343,1)"
REFERENCES,0.9472140762463344,"Observe that the heavy rows are concentrated on a set of speciﬁc row indices. There are only 30 rows
i with fi ≥50. We view this as strong evidence for our hypothesis."
REFERENCES,0.9501466275659824,"H
IMPLEMENTATION DETAILS"
REFERENCES,0.9530791788856305,"As we state in Section 3.2, when we ﬁx the positions of the non-zero entries (uniformly chosen in
each column or sampling according to the heavy leverage score distribution), we aim to optimize the
values by gradient descent mentioned in Algorithm 1. Here the loss function is given in Section 3.2.
In our implementation, we use PyTorch (Paszke et al. (2019)), which can compute the gradient
automatically (here we can use torch.qr() and torch.svd() to deﬁne our loss function). For a more"
REFERENCES,0.9560117302052786,Under review as a conference paper at ICLR 2022
REFERENCES,0.9589442815249267,"nuanced loss function, which may be beneﬁcial, one can use the package released in Agrawal et al.
(2019), where the authors studied the problem of computing the gradient of functions which involve
the solution to certain convex optimization problem."
REFERENCES,0.9618768328445748,"As mentioned in Section 2, each column of the sketch matrix S has exact one non-zero entry.
Hence, the i-th coordinate of p can be seen as the non-zero position of the i-th column of S. In
the implementation, to sample p randomly, we can sample a random integer in {1, . . . , m} for each
coordinate of p. For the heavy rows mentioned in Section 3.1, we can allocate positions 1, . . . , k to
the k heavy rows, and for the other rows, we randomly sample an integer in {k + 1, . . . , m}. We note
that once the vector p, which contains the information of the nonzero position in each column of S, is
chosen, it will not be changed during the optimization process in Algorithm 1."
REFERENCES,0.9648093841642229,"Next, we introduce some parameters for our experiments."
REFERENCES,0.967741935483871,"• bs: batch size, the number of training samples used in one iteration."
REFERENCES,0.9706744868035191,• lr: learning rate of the gradient descent(the α in Algorithm 1).
REFERENCES,0.9736070381231672,• iter: the number of iteration for Algorithm 1.
REFERENCES,0.9765395894428153,"In our experiments, we set bs = 20, iter = 1000 for all dataset. We set lr = 10 for the Green House
Gas dataset and lr = 0.1 for the Electric dataset."
REFERENCES,0.9794721407624634,"I
ADDITIONAL EXPERIMENTS FOR LASSO"
REFERENCES,0.9824046920821115,"In this section, we consider the IHS experiments for LASSO on data of a larger size. The experiment
setting is the same as that in Section 6. We conduct our experiments on the following dataset:"
REFERENCES,0.9853372434017595,"• Gas Sensor.5
A chemical detection platform composed of 8 chemoresistive gas sen-
sors was exposed to turbulent gas mixtures generated naturally in a wind tunnel. Each
matrix represents the measurements at dense time points during a short time period.
Ai ∈R95000×19, bi ∈R95000×1, and |(A, b)train| = 30, |(A, b)test| = 9. We set λ = 10."
REFERENCES,0.9882697947214076,"The results are shown in Table 1. Here we choose m = 300. The leverage scores of the rows on this
dataset is very uniform hence we choose random positions for the nonzero entries and only optimize
the values in the learned sketch. For a matrix of such size, the gaussian sketching matrix is extremely
slow, hence, we only consider the Count-Sketch matrix and the Sparse-JL matrix. From the table
below we can see that the gradient-based learned sketch has a converge rate that is 74.6% of that of
the random sketch."
REFERENCES,0.9912023460410557,"Table 1: Error of the Sketch Matrix on Gas Senser data
Iteration
4
5
6
7
8
9
10
Learnd(value-only)
428.13
23.72
1.72
0.092
0.0060
0.00036
1.8 · 10−5"
REFERENCES,0.9941348973607038,"Count-Sketch
1864.66
122.26
12.30
1.46
0.074
0.013
0.00036
Sparse-JL
1897.32
188.31
9.60
1.30
0.16
0.0098
0.00048"
REFERENCES,0.9970674486803519,5https://archive.ics.uci.edu/ml/datasets/Gas+sensor+array+temperature+modulation
