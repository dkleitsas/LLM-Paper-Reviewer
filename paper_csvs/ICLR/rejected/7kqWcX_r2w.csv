Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005747126436781609,"Off-Policy Actor-Critic methods can effectively exploit past experiences and thus
they have achieved great success in various reinforcement learning tasks. In many
image-based and multi-source tasks, attention mechanism has been employed in
Actor-Critic methods to improve their sampling efﬁciency.
In this paper, we
propose a meta attention method for state-based reinforcement learning tasks,
which combines attention mechanism and meta-learning based on the Off-Policy
Actor-Critic framework. Unlike previous attention-based work, our meta atten-
tion method introduces attention in the actor and the critic of the typical Actor-
Critic framework rather than in multiple pixels of an image or multiple informa-
tion sources. In contrast to existing meta-learning methods, the proposed meta-
attention approach is able to function in both the gradient-based training phase
and the agent’s decision-making process. The experimental results demonstrate
the superiority of our meta-attention method in various continuous control tasks,
which are based on the Off-Policy Actor-Critic methods including DDPG, TD3,
and SAC."
INTRODUCTION,0.011494252873563218,"1
INTRODUCTION"
INTRODUCTION,0.017241379310344827,"Reinforcement Learning (RL) algorithms based on the Actor-Critic framework have achieved con-
siderable success in many areas such as games, robot control, and planning. Compared with on-
policy methods, off-policy methods possess more efﬁcient sampling since they do not require new
samples to be collected for each gradient step and make better use of experience (Haarnoja et al.,
2018a). However, even for off-policy methods, traditional reinforcement learning algorithms still
have extremely low sample efﬁciency (Yu, 2018). Recently, meta-learning (Hospedales et al., 2020)
has become topical as a paradigm to accelerate RL by learning an inductive bias from past expe-
rience. By learning aspects of the learning strategy, such as fast adaptation strategies (Finn et al.,
2017) (Rakelly et al., 2019), losses (Zhou et al., 2020) (Bechtle et al., 2020), optimization strate-
gies (Duan et al., 2016), exploration strategies (Gupta et al., 2018), hyperparameters (Bechtle et al.,
2020), and intrinsic rewards (Zheng et al., 2018), meta-learning has signiﬁcantly improved sample
efﬁciency over standard RL algorithms."
INTRODUCTION,0.022988505747126436,"Improving sample efﬁciency through attention mechanism has also been proved to be very effective
in image-based reinforcement learning(Barati & Chen, 2019; Chen et al., 2019). The application of
attention mechanism in multi-agent system(Parnika et al., 2021; Iqbal & Sha, 2019) and multi-object
task(Team et al., 2021) also shows its powerful capabilities in information processing. However, in
the existing works, attention mechanisms often have clear application scenarios, such as image-
based control or multiple sources of information (multi-agent or multi-target). The effective combi-
nation of attention mechanisms with current algorithms in a state-based single-agent environment is
still a problem to be investigated"
INTRODUCTION,0.028735632183908046,"In this paper, we propose a meta attention method based on the attention mechanism. In the human
decision-making process, people often modify their concepts based on the feedback and results to
obtain a better decision. Inspired by this decision-making process, we use meta attention to adapt
the features generated by the policy network based on the evaluation of value network. Our work
differs from current attention-based work in that our meta attention approach works only within
the Actor-Critic framework and does not depend on a speciﬁc scenario. We formalize the meta-
learning process as a bi-level optimization problem. Our approach can be ﬂexibly combined with
various algorithms by using meta attention as a meta-learner and optimizing meta attention in the
outer layer. Unlike the existing meta-learning methods, our meta attention approach can improve the"
INTRODUCTION,0.034482758620689655,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.040229885057471264,"performance of agent through gradients in the training stage and obtain better actions by adjusting
the features in the execution stage."
INTRODUCTION,0.04597701149425287,"We evaluated the proposed meta attention method in a series of continuous control tasks in Gym and
Roboschool, including three 3D robot control tasks, two 2D control tasks, and one classic control
task based on DDPG(Lillicrap et al., 2016), TD3(Fujimoto et al., 2018) and SAC(Haarnoja et al.,
2018b). Besides, we also discussed the changes and impact caused by modifying the actor features
through meta attention. Experimental results how that our meta-attention approach is not only ef-
fective in accelerating the learning progress of the agent in the training phase, but also improves the
actions in the execution phase, further enhancing the performance of the agent."
RELATED WORK,0.05172413793103448,"2
RELATED WORK"
RELATED WORK,0.05747126436781609,"Attention Mechanism
Attention is a behavioral and cognitive process of selectively attending to
a discrete aspect of information, whether subjective or objective, while ignoring other perceptible
information(de Santana Correia & Colombini, 2021). Typically, attention mechanism is mainly
applied in the ﬁelds of computational vision and natural language processing. Accordingly, although
the implementation methods are different, the application of attention mechanism in reinforcement
learning is mainly focused on video games(Wu et al., 2021; Chen et al., 2019; Barati & Chen, 2019;
Mott et al., 2019)(Manchin et al., 2019). Other works such as Peng et al. (2020) proposed a dynamic
attention model with a dynamic encoder-decoder architecture, which dynamically explores node
features to efﬁciently exploit hidden structural information at different construction steps. Li et al.
(2021) applied the attention mechanism to generate feature vectors and input them into value and
policy head during the feature extraction phase of PPO and PPG.Jiang & Lu (2018); Iqbal & Sha
(2019); Mao et al. (2019) employed a multi-head attention mechanism to make one agent selectively
pay attention to information from other agents. Team et al. (2021) used the attention mechanism to
match multiple goals with the hidden state of the current state to obtain the goal-attention hidden
state under different goals. This approach allows the agent to predict the expected return obtained
by attending to a goal at the end of a scene."
RELATED WORK,0.06321839080459771,"Meta Reinforcement Learning
Meta-learning is most often understood as learning to learn,
which refers to learning from historical information or multiple learning episodes to improve learn-
ing algorithms. Since works which fed historical trajectories into Recurrent Neural Network to ﬂesh
out task-level information(Wang et al., 2016; Duan et al., 2016), various meta-learning methods
have been proposed to strengthen agent performance. Houthooft et al. (2018); Kirsch et al. (2020);
Zhou et al. (2020) used meta-learning methods to learn a loss function rather than artiﬁcial design
to improve performance of agent in single or multiple tasks.Gupta et al. (2018); Stadie et al. (2018);
Xu et al. (2018a) employed meta-learning methods to learn exploration instead of traditional ex-
ploration methods.Finn et al. (2017) meta learned a good model initialization of the model that can
be quickly adapted to different tasks. Xu et al. (2018b) improved the performance of the agent by
meta-learning discount factors.Rakelly et al. (2019)Fakoor et al. (2020) treats meta information as an
unobservable state of Partially Observable Markov Decision Process, further improves the agent’s
performance in multi task learning. Although the attention mechanism may not have an explicit
meta-learning object, it can also be considered as the king of the meta-learning method."
RELATED WORK,0.06896551724137931,"Bi-level Optimization
Generally, meta-learning can be formalized as a bi-level optimiza-
tion(BLO) problem. However, the solution of BLO problems is often challenging. Franceschi et al.
(2018) proposed a framework for approximating the solution of BLO problems using the gradient
method. Since then, many works based on the gradient methods to optimize meta leaner have suc-
cessfully proved the feasibility of gradient method, such asLi et al. (2019); Finn & Levine (2018);
Lian et al. (2020); Flennerhag et al. (2020). For the RL problem, Zhou et al. (2020) optimize the meta
critic, the upper-level of a BLO, as an intrinsic motivation by gradient method. Kirsch et al. (2020)
enables a population of agents to use and improve a single parameterized objective function through
gradient learning on different tasks. Rajeswaran et al. (2019) proposed an implicit MAML algorithm
by drawing upon implicit differentiation that effectively decouples the meta-gradient computation
from the selection of an inner loop optimizer. Liu et al. (2019) proposed a surrogate objective
function TMAML, which incorporates control variables into gradient estimation through automatic"
RELATED WORK,0.07471264367816093,Under review as a conference paper at ICLR 2022
RELATED WORK,0.08045977011494253,"differentiation and improves the quality of the gradient estimation by reducing the variance without
introducing bias."
METHODOLOGY,0.08620689655172414,"3
METHODOLOGY"
OFF-POLICY ACTOR-CRITIC,0.09195402298850575,"3.1
OFF-POLICY ACTOR-CRITIC"
OFF-POLICY ACTOR-CRITIC,0.09770114942528736,"In general, the reinforcement learning task can be considered as ﬁnding the optimal policies in
Markov Decision Processes (MDPs). The MDP is deﬁned by a tuple (S, A, P, R), where S is a set
of states, A is a set of actions, P is a set of probabilities to switch from a state s to s′ for a given
set of action a, and R : S × A →R is a scalar reward function. In the Actor-Critic framework,
the policy network(Actor) πφ(s) and the value network(Critic) Qθ(s, a) are parameterized by a
neural network respectively. At each time t, the agent receives an observation st and takes a action
at based on its policy π : S →A, then receives a reward rt and a new state st+1. The tuple
(st, at, rt, st+1) describes a state transition and will be stored in a reply buffer D for off-policy
learning. The objective of RL is to ﬁnd the optimal policy πφ to maximizes the expected cumulative
return J:"
OFF-POLICY ACTOR-CRITIC,0.10344827586206896,"J(φ) = E "" ∞
X"
OFF-POLICY ACTOR-CRITIC,0.10919540229885058,"t=0
γtR (st, at) | at ∼πφ (· | st) # (1)"
OFF-POLICY ACTOR-CRITIC,0.11494252873563218,"Where γ is the discount factor, and J(φ) also can be written as the expected value for the Q-function:"
OFF-POLICY ACTOR-CRITIC,0.1206896551724138,"J(φ) = Es∼pπQθ(s, a)|at∼πφ(·|st),
(2)"
OFF-POLICY ACTOR-CRITIC,0.12643678160919541,"Where Q-function is the expected discounted sum of rewards following visitation at state s and
execution of action a, and pπ is the state distribution induced by policy π. For off-policy Actor-
Critic architectures such as DDPG, TD3 and SAC, the loss for the actor provided by the critic may
be different, but the Q-function will be learned by minimizing the loss of the below equation as
same:
LCritic
θ
= Est∼pπ,at∼πφ
h
(Qθ (st, at) −yt)2i"
OFF-POLICY ACTOR-CRITIC,0.13218390804597702,"yt = rt + γQθ′ (st+1, πφ′ (st+1)) ,
(3)"
OFF-POLICY ACTOR-CRITIC,0.13793103448275862,"where φ′ and θ′ represent the target network for the critic and the actor respectively. The actor loss
usually differs in details according to different algorithms, however , they all follow a form of the
following formula:
LActor
φ
= −J(φ) = −Es∼pπQθ(s, a)|a=πφ(s)
(4)"
META ATTENTION METHOD,0.14367816091954022,"3.2
META ATTENTION METHOD"
META ATTENTION METHOD,0.14942528735632185,"The attention mechanism has been widely used in image recognition and natural language processing
since it can model the human pattern recognition process. It allocates attention to the important part
of information while automatically ignores low-value features. According to (Vaswani et al., 2017),
attention function can be described as mapping a query and a set of key-value pairs to an output,
where the output is computed as a weighted sum of the values, with the weight assigned to each
value being computed by a compatibility function of the query with the corresponding key:"
META ATTENTION METHOD,0.15517241379310345,"Attention(Query, Key, Value) =
X"
META ATTENTION METHOD,0.16091954022988506,"i
Similarity (Queryi, Keyi) ∗Value i"
META ATTENTION METHOD,0.16666666666666666,"In order to introduce the attention mechanism into the Actor-Critic framework, we ﬁrst split the
actor and critic network into two parts . We take the last layer of actor network ˆπ as an action net
to produce actions and the rest as actor feature net ¯π(s) to extract features, with the entire policy
network being denoted as πφ(s) = ˆπ(¯π(s)). Similarly, the value network can also be divided
into a critic net ˆQ and a critic feature net ¯Q(s), and the whole value network can be expressed as
Qθ(s) = ˆQ( ¯Q(s, a)). We use the feature net as encoder to obtain Query, Key and Value by the
following formula:"
META ATTENTION METHOD,0.1724137931034483,"Querry = ¯π(s)
Key = ¯Q(s, πφ(s))
Value = ¯π(s)"
META ATTENTION METHOD,0.1781609195402299,Under review as a conference paper at ICLR 2022
META ATTENTION METHOD,0.1839080459770115,"漏a漐
漏b漐"
META ATTENTION METHOD,0.1896551724137931,Action Actor
META ATTENTION METHOD,0.19540229885057472,Feature net
META ATTENTION METHOD,0.20114942528735633,Action net
META ATTENTION METHOD,0.20689655172413793,Critic
META ATTENTION METHOD,0.21264367816091953,Feature net
META ATTENTION METHOD,0.21839080459770116,Critic net
META ATTENTION METHOD,0.22413793103448276,Query & Value
META ATTENTION METHOD,0.22988505747126436,Actor features
META ATTENTION METHOD,0.23563218390804597,"Features Scale
Sigmoid MLP"
META ATTENTION METHOD,0.2413793103448276,(Meta attention)
META ATTENTION METHOD,0.2471264367816092,"Key
K
Critic features"
META ATTENTION METHOD,0.25287356321839083,Attention features *
META ATTENTION METHOD,0.25862068965517243,Action net
META ATTENTION METHOD,0.26436781609195403,Critic
META ATTENTION METHOD,0.27011494252873564,Feature net
META ATTENTION METHOD,0.27586206896551724,Critic net Actor
META ATTENTION METHOD,0.28160919540229884,Feature net State State A t Fe
META ATTENTION METHOD,0.28735632183908044,Attention ti tur t eat
META ATTENTION METHOD,0.29310344827586204,Attention
META ATTENTION METHOD,0.2988505747126437,Action
META ATTENTION METHOD,0.3045977011494253,"State
State State"
META ATTENTION METHOD,0.3103448275862069,Action
META ATTENTION METHOD,0.3160919540229885,Action net Actor
META ATTENTION METHOD,0.3218390804597701,Feature net
META ATTENTION METHOD,0.3275862068965517,Critic net
META ATTENTION METHOD,0.3333333333333333,Critic
META ATTENTION METHOD,0.3390804597701149,Feature net
META ATTENTION METHOD,0.3448275862068966,"Actor
Critic
Meta Attention"
META ATTENTION METHOD,0.3505747126436782,"Figure 1: (a) Traditional Actor-Critic framework. (b) The meta attention Actor-Critic framework.
The part circled by the orange box represents the actor, the part circled by the blue box represents
the critic, and the part circled by a green dashed line represents the attention method. There is
two orange(blue) boxes in (b), but they refer to the same actor(critic). Lines with different colors
represent the update process dominated by different losses."
META ATTENTION METHOD,0.3563218390804598,"We
input
Query(actor
features)
and
Key(critic
features)
into
meta
attention
network
fψ(Query, Key) (a three-layers MLP parameterized by ψ) to calculate the similarity of each feature
dimension. To enhance or reduce the features in speciﬁc corresponding dimensions, we multiply the
output after the sigmoid function by 2 to obtain the feature scale. In this paper, we use ◦to denote the
Hadamard product, and by calculating the Hadamard product ◦of Value(actor features) and scale,
we obtain the attention features ¯π′(s) after modifying some dimensions:"
META ATTENTION METHOD,0.3620689655172414,"Attention Features = ¯π′
ψ(s) = fψ(¯π(s), ¯Q(s, πφ(s))) ◦2 ◦¯π(s)
(5)"
META ATTENTION METHOD,0.367816091954023,This calculation process corresponds to the part in the green dotted line box in Figure 1 (b).
META ATTENTION METHOD,0.3735632183908046,"There are also two critical problems in the optimization process.
1) how to affect the agent’s
decision-making and training process through attention features; and 2) how to optimize the meta
attention network to achieve the correct matching Query(actor features) and Key(critic features) to
generate the proper feature scales. To address these issues, we formalize the entire optimization
process as a bi-level optimization problem, referring to the meta attention method as the outer level,
and to the task performed by the agent as the inner level:"
META ATTENTION METHOD,0.3793103448275862,"ψ∗= arg min
ψ
LAttention
ψ
(D; φ∗; ψ)
(6)"
META ATTENTION METHOD,0.3850574712643678,"s.t. φ∗= arg min
φ
[LActor
φ
(D; φ|a ∼πφ(s)) + LActor
φ
 
D; φ; ψ|a ∼ˆπ(¯π′
ψ(s))

,
(7)"
META ATTENTION METHOD,0.39080459770114945,"where LAttention
ψ
is the meta optimization objective, and LActor
φ
is the actor loss in the form of Eq.(4)."
META ATTENTION METHOD,0.39655172413793105,"For the ﬁrst problem, we ﬁrst implement a traditional back-propagate dominated by the actor loss
LActor
φ
|a=πφ(s) on training data dtrn:"
META ATTENTION METHOD,0.40229885057471265,"φold = φ −η
∂LActor
φ
(dtrn|a ∼πφ(s))"
META ATTENTION METHOD,0.40804597701149425,"∂φ
,
(8)"
META ATTENTION METHOD,0.41379310344827586,"Where η is the learning rate, this process corresponds to the upper part of Figure 1 (b) and
the ﬁrst step of meta-training in Algorithm1.
Then, we generate a new attention action a ="
META ATTENTION METHOD,0.41954022988505746,Under review as a conference paper at ICLR 2022
META ATTENTION METHOD,0.42528735632183906,"ˆπ(¯π′
ψ(s)) from the attention features through action net and feed into the critic to get the loss
LActor
φ
(D; φ; ψ) |a=ˆπ(¯π′
ψ(s)) for back-propagation:"
META ATTENTION METHOD,0.43103448275862066,"φnew = φold −η
∂LActor
φold"
META ATTENTION METHOD,0.4367816091954023,"
dtrn; ψ|a ∼ˆπ(¯π′
ψ(s))
"
META ATTENTION METHOD,0.4425287356321839,"∂φ
(9)"
META ATTENTION METHOD,0.4482758620689655,"This allows the agent to obtain better actions by simply modifying the features compared to the orig-
inal action without increasing the batch size, strengthening the agent’s learning of good actions and
reducing the probability of producing poor actions after the gradient step. This process corresponds
to the lower part of (b) in Figure 1 and the second step of meta-training in Algorithm1."
META ATTENTION METHOD,0.4540229885057471,"For the second problem, we have two basic assumptions: 1) a good meta attention network will
inevitably generate feature scales that can generate actions with higher value since it better associates
the relationship between actor features and critic features; 2) in the process of back-propagation,
high-value actions enhance actors more than low-value actions because good actions will strengthen
the actor’s tendency to make such choices, while bad actions cause the actor to try other actions.
Furthermore, since the meta attention network is involved in the back propagation process, we use
the utility of meta attention in this process as the attention loss LAttention
ψ
and the attention loss on
validation data dval is deﬁned as:"
META ATTENTION METHOD,0.45977011494252873,"LAttention
ψ
= tanh
 
LActor
φnew (dval|a ∼πφnew(s)) −LActor
φold (dval|a ∼πφold(s))

(10)"
META ATTENTION METHOD,0.46551724137931033,"Under this deﬁnition, when performing gradient descent updates, it is ensured that the meta attention
is always updated along the direction that improves the performance of the agent. This process
corresponds to the blue line part of (b) in Figure 1 and the meta-test in Algorithm1."
BEYOND THE GRADIENT,0.47126436781609193,"3.3
BEYOND THE GRADIENT"
BEYOND THE GRADIENT,0.47701149425287354,"Figure 2: When the agent interacts with the environment, in addition to the action produced by the
traditional Actor-critic framework, meta attention also attempts to make further adjustments to the
initially produced results in the hope of achieving better results."
BEYOND THE GRADIENT,0.4827586206896552,"Unlike previous gradient-based meta-learning approaches, our proposed meta attention method can
affect the learning process of the agent through the gradient and participate in each of their decisions.
In the traditional Actor-Critic framework, the critic estimates each action’s value based on the current
state, which provides a criterion for evaluating the action produced by an actor. The Critic and the
trained meta attention make it possible for meta attention methods to participate in decision-making."
BEYOND THE GRADIENT,0.4885057471264368,"When people are given the rules for scoring in human decision-making, they often modify their
previous ideas to obtain better results. Similarly, we combine the attention method with the critic to
introduce this decision-making method into the AC framework. For the deterministic policy, when
an agent is given a state from the environment, we can get another attention action by adjusting the
actor features in addition to the actions directly produced by the traditional Actor-Critic framework.
Then the action with the higher Q value given by the critic is executed. This decision process is
shown in Figure 2 and corresponds to the environment step in Algorithm 1."
BEYOND THE GRADIENT,0.4942528735632184,Under review as a conference paper at ICLR 2022
BEYOND THE GRADIENT,0.5,"This method of action selection is like an actor dancing once more after a performance, getting
another score by adapting his or her actions based on the feedback given by the critics, and choosing
which set of actions to be performed formally by comparing the two scores."
BEYOND THE GRADIENT,0.5057471264367817,"For stochastic policies such as SAC, their actions under a continuous control task are typically
sampled from a Gaussian distribution. Although it is meaningless to compare sampled actions, we
can still compare and modify the actor features in terms of the mean value of Gaussian distribution.
Another attentional action is then sampled from the Gaussian distribution and the parameters (mean
and variance) generated by the attentional features are modiﬁed."
BEYOND THE GRADIENT,0.5114942528735632,"Although the Q-value given by the critic may be inaccurate in the early stage of training, and off-
policy algorithms typically have serious over estimation problem, these issues are beyond the scope
of this paper."
BEYOND THE GRADIENT,0.5172413793103449,"Algorithm 1 Meta attention for off-policy Actor-Critic
Initialized parameters θ, φ, ψ, D = ∅and learning rate η, λ"
BEYOND THE GRADIENT,0.5229885057471264,for each iteration do
BEYOND THE GRADIENT,0.5287356321839081,for each environment step do
BEYOND THE GRADIENT,0.5344827586206896,if the actor dances twice then
BEYOND THE GRADIENT,0.5402298850574713,"at ∼πφ (st) or at ∼ˆπ(¯π′
ψ(s))
% Select the action with higher Q-value
else"
BEYOND THE GRADIENT,0.5459770114942529,"at ∼πφ (st)
% Traditional action generation process
end if
st+1 ∼p (st+1 | st, at) , rt
% Observe reward rt and new state st+1
D ←D ∪{(st, at, rt, st+1)}
% Store the transition in the replay buffer
end for
for each gradient step do"
BEYOND THE GRADIENT,0.5517241379310345,"Sample mini-batch dtrn from D
LCritic
θ
←Eq.(3)
θ ←θ −η∇θLCritic
θ
% Critic update
meta-training:
φold ←Eq.(8)
% Vanilla Actor update
φnew ←Eq.(9)
% Actor update based on attention action
meta-test:
LAttention
ψ
←Eq.(10)
% Attention loss
meta-optimisation:
φ ←φnew
% Update the actor parameters
ψ ←ψ −λ∇ψLAttention
ψ
% Update the meta attention parameters
end for
end for"
EXPERIMENT,0.5574712643678161,"4
EXPERIMENT"
SETUP,0.5632183908045977,"4.1
SETUP"
SETUP,0.5689655172413793,"To evaluate our meta attention framework, we selected four 3D continuous robot control tasks in
Roboschool and two 2D continuous robot control tasks in Gym without modifying the original
environments or rewards. Although MuJoCo is widely used to test various reinforcement learning
algorithms, Roboschool provides a more realistic and difﬁcult robot control task than MuJoCo.
Compared with MuJoCo, Roboschool reduces the alive bonus and increases drive costs to encourage
movement in a low-energy manner. Roboschool also made other adjustments, such as encouraging
the Ant robot to have two or more legs on the ground to ensure that the robot moves in a more natural
posture."
SETUP,0.5747126436781609,"We applied our meta attention framework to DDPG, TD3 and SAC as the basic algorithms. Algo-
rithms that use meta attention methods only in the training stage will be annotated as MATT (such
as DDPG MATT), while algorithm that uses the meta attention method in both the training stage
and agent decision-making process will be annotated as MATT DT(such as DDPG MATT DT)."
SETUP,0.5804597701149425,Under review as a conference paper at ICLR 2022
SETUP,0.5862068965517241,"We also compare our MATT and MATT DT with another SOTA work (Zhou et al., 2020) which
proposed Meta Critic ( MC) to enforce the basic algorithm."
SETUP,0.5919540229885057,"All our codes are built on the code provided by Zhou et al. (2020). We ran tasks for 1 million or
5 hundred thousands game steps depending on the environment, and we evaluate our policy over
10 episodes without exploration every 1000 game steps. All results for each task are averaged over
10 random seeds (trials) and network initializations. 95% conﬁdence intervals are shown as shaded
regions over time steps. Following Fujimoto et al. (2018), curves are smoothed evenly for clarity
(window size=10)."
SETUP,0.5977011494252874,"Experiments in all environments had the same network structure and hyperparameters, with a learn-
ing rate of 0.003 for DDPG and TD3 but 0.001 for SAC. All experiments are performed on 4 servers
with 4 NVIDIA-TITAN-V GPU each and 3 servers with 6 NVIDIA-TITAN-V GPUs. Each server
is equipped by 2 Intel(R) Xeon(R) Gold 5218 CPUs."
EVALUATION OF META ATTENTION METHOD,0.603448275862069,"4.2
EVALUATION OF META ATTENTION METHOD"
EVALUATION OF META ATTENTION METHOD,0.6091954022988506,"Figure 3: The learning curves of DDPG and enhancement algorithms. The shaded region represents
the 95% conﬁdence interval."
EVALUATION OF META ATTENTION METHOD,0.6149425287356322,"Figure 4: The learning curve of TD3 and enhancement algorithms. The shaded region represents the
95% conﬁdence interval."
EVALUATION OF META ATTENTION METHOD,0.6206896551724138,"Deterministic policy
Figure 3 and Figure 4 demonstrate the comparison between the vanilla
algorithms and their enhancive versions based on DDPG and TD3.
From the ﬁgures, we"
EVALUATION OF META ATTENTION METHOD,0.6264367816091954,Under review as a conference paper at ICLR 2022
EVALUATION OF META ATTENTION METHOD,0.632183908045977,"observes that
MATT and
MATT DT achieve signiﬁcant performance improvements in all
but the RoboschoolHalfCheetah-v1 environment.
DDPG MATT perform almost as well as
DDPG MATT DT under the environments BipedalWalker-v2, LunarLanderContinuous-v2 and
RoboschoolInvertedPendulumSwingup-v1. But in these enviroment, TD3 MATT DT perform sig-
niﬁcantly better than TD3 MATT, and we speculate that this phenomenon may be related to more
accurate Q-values."
EVALUATION OF META ATTENTION METHOD,0.6379310344827587,"Similar to Meta Critic, MATT and MC only work in the training stage and inﬂuence the ac-
tor through gradients, but DDPG MC performs even worse than the original algorithm in four
environments, while DDPG MATT shows performance improvements in all environments except
RoboschoolHalfCheetah-v1. TD3 MATT also shows high performance in more environment than
TD3 MC. From the data provided in Zhou et al. (2020), we can also ﬁnd a similar situation: the
performance of MC decreases in the ﬁrst 1 million steps in deterministic policy based algorithms.
This also shows that our meta attention method completes the meta-learning process faster than
Meta Critic."
EVALUATION OF META ATTENTION METHOD,0.6436781609195402,"Figure 5: The learning curve of SAC and enhancement algorithms. The shaded region represents
the 95% conﬁdence interval."
EVALUATION OF META ATTENTION METHOD,0.6494252873563219,"Stochastic policy
Figure 5 compares the SAC and its enhancive versions.
Although
SAC MATT has only achieved performance improvement in environments BipedalWalker-v2 and
LunarLanderContinuous-v2, SAC MATT DT still achieves performance improvements in all en-
vironments other than RoboschoolAnt-v1 and RoboschoolInvertedPendulumSwingup-v1. This sug-
gests that our method is also effective in stochastic policy, but the improvement in stochastic policy
is less pronounced than in Deterministic policy. We speculate that, compared with deterministic
strategies, sampled actions in random strategies bring more uncertainty to the learning of meta at-
tention. The performance of SAC MATT and SAC MC in the four environments is similar, which
is the same as or slightly higher than the original algorithm. The performance of the ﬁrst 1 million
steps also matches the data provided in the Zhou et al. (2020)."
FURTHER ANALYSIS,0.6551724137931034,"4.3
FURTHER ANALYSIS"
FURTHER ANALYSIS,0.6609195402298851,"In order to analyze the effect of meta attention on the features, we ran 5 random seeds(trails) in
RoboschoolWalker2d-v1 environment. In each trial, we ran a total of 1 million game steps and
evaluate our policy over 10 episodes without exploring every 1000 game steps. Figure 6 (a) shows
an overview of this experiment, and the shaded region represents the 95% conﬁdence interval."
FURTHER ANALYSIS,0.6666666666666666,"We recorded the Q-values of attention and actor features during the evaluation stage to verify the
impact of meta attention on performance. In the evaluation phase, we took 1 million consecutive
Q-values of the actions generated by attention features and actor features. These 1 million samples
correspond to 1 million time steps in the evaluation stage or 10 evaluations. We used the error
between Q-values of the attention action and vanilla action as performance improvements brought"
FURTHER ANALYSIS,0.6724137931034483,Under review as a conference paper at ICLR 2022
FURTHER ANALYSIS,0.6781609195402298,(a) learning curve
FURTHER ANALYSIS,0.6839080459770115,(c) after 80000 training steps
FURTHER ANALYSIS,0.6896551724137931,(d) after 120000 training steps
FURTHER ANALYSIS,0.6954022988505747,"(b) after 40000 training steps
(e) after 160000 training steps"
FURTHER ANALYSIS,0.7011494252873564,"Figure 6: (a) Learning curves, the shaded part represents the 95% conﬁdence interval; (b-e) The
results obtained by sampling at different stages (green circle in (a)) of training. The left part represent
the difference in Q-values of actions generated by attention features and actor features, respectively
The right part represents t-SNE visualization of attention features and actor features, where red dots
represent attention features and blue dots represent actor features."
FURTHER ANALYSIS,0.7068965517241379,"by meta attention. We sampled at four different time points in the training phase, and these results
are represented in Figure 6 (b-e)."
FURTHER ANALYSIS,0.7126436781609196,"To analyze the diversity of attention features and actor features, we sampled 250 attention features
and actor features on continuous 250 time steps during evaluation on one trail. We used t-SNE to
visualize these total 500 features. Similarly, we sampled at ﬁve different time points in the training
stages, and these results are shown in the right part of Figure 6 (b-e)."
FURTHER ANALYSIS,0.7183908045977011,"Figure 6 (b-e) shows the results of sampling at approximately 40000, 80000, 120000, 160000 train-
ing steps on one trail of 1 million time steps or 200000 training steps. In terms of improvement(left
part of Figure 6 (b-e)), the actions obtained after the modiﬁcation of our meta attention usually
achieved higher Q-values at different stages of training. In terms of the distribution (right part of
Figure 6 (b-e)), attention features have changed in most cases compared with actor features, as shown
by the apparent separation of red points and blue points. This suggests that our meta-attention ap-
proach is effective in modifying features and improving performance in the current state throughout
the training phase."
CONCLUSION,0.7241379310344828,"5
CONCLUSION"
CONCLUSION,0.7298850574712644,"In this paper, we present the meta attention method, a derivative-based meta learner for off-policy
Actor-Critic reinforcement learning methods. By modifying the actor features, we speed up the
training process and enable agents to make better decisions when interacting with the environment.
The experimental results demonstrate the effectiveness of our method in both stochastic policy and
deterministic policy. Moreover, our method can be ﬂexibly integrated into a variety of contemporary
off-policy Actor-Critic methods to boost performance. In future work, we will use the multi-head
attention to improve the meta learner and decouple the meta attention method from the value function
to reduce the impact of the over estimation problem."
CONCLUSION,0.735632183908046,Under review as a conference paper at ICLR 2022
REFERENCES,0.7413793103448276,REFERENCES
REFERENCES,0.7471264367816092,"Elaheh Barati and Xuewen Chen.
An actor-critic-attention mechanism for deep reinforcement
learning in multi-view environments. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth
International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August
10-16, 2019, pp. 2002–2008. ijcai.org, 2019.
doi: 10.24963/ijcai.2019/277.
URL https:
//doi.org/10.24963/ijcai.2019/277."
REFERENCES,0.7528735632183908,"Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gau-
rav S. Sukhatme, and Franziska Meier.
Meta learning via learned loss.
In 25th Interna-
tional Conference on Pattern Recognition, ICPR 2020, Virtual Event / Milan, Italy, January
10-15, 2021, pp. 4161–4168. IEEE, 2020.
doi: 10.1109/ICPR48806.2021.9412010.
URL
https://doi.org/10.1109/ICPR48806.2021.9412010."
REFERENCES,0.7586206896551724,"Yilun Chen, Chiyu Dong, Praveen Palanisamy, Priyantha Mudalige, Katharina Muelling, and
John M. Dolan. Attention-based hierarchical deep reinforcement learning for lane change be-
haviors in autonomous driving.
In 2019 IEEE/RSJ International Conference on Intelligent
Robots and Systems, IROS 2019, Macau, SAR, China, November 3-8, 2019, pp. 3697–3703.
IEEE, 2019. doi: 10.1109/IROS40897.2019.8968565. URL https://doi.org/10.1109/
IROS40897.2019.8968565."
REFERENCES,0.764367816091954,"Alana de Santana Correia and Esther Luna Colombini. Attention, please! A survey of neural at-
tention models in deep learning. CoRR, abs/2103.16775, 2021. URL https://arxiv.org/
abs/2103.16775."
REFERENCES,0.7701149425287356,"Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl$ˆ2$:
Fast reinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016. URL
http://arxiv.org/abs/1611.02779."
REFERENCES,0.7758620689655172,"Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J. Smola. Meta-q-learning. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
SJeD3CEFPH."
REFERENCES,0.7816091954022989,"Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient
descent can approximate any learning algorithm.
In 6th International Conference on Learn-
ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=
HyjC5yWCW."
REFERENCES,0.7873563218390804,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th Interna-
tional Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,
volume 70 of Proceedings of Machine Learning Research, pp. 1126–1135. PMLR, 2017. URL
http://proceedings.mlr.press/v70/finn17a.html."
REFERENCES,0.7931034482758621,"Sebastian Flennerhag, Andrei A. Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia Had-
sell. Meta-learning with warped gradient descent. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
URL https://openreview.net/forum?id=rkeiQlBFPB."
REFERENCES,0.7988505747126436,"Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In Jennifer G. Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of
Machine Learning Research, pp. 1563–1572. PMLR, 2018. URL http://proceedings.
mlr.press/v80/franceschi18a.html."
REFERENCES,0.8045977011494253,"Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Sweden,
July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1582–1591.
PMLR, 2018. URL http://proceedings.mlr.press/v80/fujimoto18a.html."
REFERENCES,0.8103448275862069,Under review as a conference paper at ICLR 2022
REFERENCES,0.8160919540229885,"Abhishek Gupta, Russell Mendonca, Yuxuan Liu, Pieter Abbeel, and Sergey Levine.
Meta-
reinforcement learning of structured exploration strategies. In Samy Bengio, Hanna M. Wal-
lach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Ad-
vances in Neural Information Processing Systems 31: Annual Conference on Neural Informa-
tion Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp.
5307–5316, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
4de754248c196c85ee4fbdcee89179bd-Abstract.html."
REFERENCES,0.8218390804597702,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor, 2018a."
REFERENCES,0.8275862068965517,"Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-
rithms and applications. CoRR, abs/1812.05905, 2018b. URL http://arxiv.org/abs/
1812.05905."
REFERENCES,0.8333333333333334,"Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta-learning in
neural networks: A survey. CoRR, abs/2004.05439, 2020. URL https://arxiv.org/abs/
2004.05439."
REFERENCES,0.8390804597701149,"Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly C. Stadie, Filip Wolski, Jonathan Ho,
and Pieter Abbeel.
Evolved policy gradients.
In Samy Bengio, Hanna M. Wallach,
Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Ad-
vances in Neural Information Processing Systems 31: Annual Conference on Neural Informa-
tion Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp.
5405–5414, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
7876acb66640bad41f1e1371ef30c180-Abstract.html."
REFERENCES,0.8448275862068966,"Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97
of Proceedings of Machine Learning Research, pp. 2961–2970. PMLR, 2019. URL http://
proceedings.mlr.press/v97/iqbal19a.html."
REFERENCES,0.8505747126436781,"Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montr´eal, Canada, pp. 7265–7275, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html."
REFERENCES,0.8563218390804598,"Louis Kirsch, Sjoerd van Steenkiste, and J¨urgen Schmidhuber. Improving generalization in meta
reinforcement learning using learned objectives. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
URL https://openreview.net/forum?id=S1evHerYPr."
REFERENCES,0.8620689655172413,"Yiying Li, Yongxin Yang, Wei Zhou, and Timothy M. Hospedales.
Feature-critic networks for
heterogeneous domain generalization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,
pp. 3915–3924. PMLR, 2019. URL http://proceedings.mlr.press/v97/li19l.
html."
REFERENCES,0.867816091954023,"Yunfei Li, Tao Kong, Lei Li, Yifeng Li, and Yi Wu. Learning to design and construct bridge without
blueprint. CoRR, abs/2108.02439, 2021. URL https://arxiv.org/abs/2108.02439."
REFERENCES,0.8735632183908046,"Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, and
Shenghua Gao.
Towards fast adaptation of neural architectures with meta learning.
In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
r1eowANFvr."
REFERENCES,0.8793103448275862,Under review as a conference paper at ICLR 2022
REFERENCES,0.8850574712643678,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http:
//arxiv.org/abs/1509.02971."
REFERENCES,0.8908045977011494,"Hao Liu, Richard Socher, and Caiming Xiong.
Taming MAML: efﬁcient unbiased meta-
reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 4061–
4071. PMLR, 2019. URL http://proceedings.mlr.press/v97/liu19g.html."
REFERENCES,0.896551724137931,"Anthony Manchin, Ehsan Abbasnejad, and Anton van den Hengel. Reinforcement learning with
attention that works: A self-supervised approach. In Tom Gedeon, Kok Wai Wong, and Minho
Lee (eds.), Neural Information Processing - 26th International Conference, ICONIP 2019, Syd-
ney, NSW, Australia, December 12-15, 2019, Proceedings, Part V, volume 1143 of Communi-
cations in Computer and Information Science, pp. 223–230. Springer, 2019.
doi: 10.1007/
978-3-030-36802-9\ 25.
URL https://doi.org/10.1007/978-3-030-36802-9_
25."
REFERENCES,0.9022988505747126,"Hangyu Mao, Zhengchao Zhang, Zhen Xiao, and Zhibo Gong. Modelling the dynamic joint policy
of teammates with attention multi-agent DDPG. In Edith Elkind, Manuela Veloso, Noa Agmon,
and Matthew E. Taylor (eds.), Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems, AAMAS ’19, Montreal, QC, Canada, May 13-17, 2019, pp.
1108–1116. International Foundation for Autonomous Agents and Multiagent Systems, 2019.
URL http://dl.acm.org/citation.cfm?id=3331810."
REFERENCES,0.9080459770114943,"Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez Rezende.
Towards interpretable reinforcement learning using attention augmented agents. In Hanna M.
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Ro-
man Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver, BC, Canada, pp. 12329–12338, 2019. URL https://proceedings.neurips.cc/
paper/2019/hash/e9510081ac30ffa83f10b68cde1cac07-Abstract.html."
REFERENCES,0.9137931034482759,"P. Parnika, Raghuram Bharadwaj Diddigi, Sai Koti Reddy Danda, and Shalabh Bhatnagar. Atten-
tion actor-critic algorithm for multi-agent constrained co-operative reinforcement learning. In
Frank Dignum, Alessio Lomuscio, Ulle Endriss, and Ann Now´e (eds.), AAMAS ’21: 20th In-
ternational Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United
Kingdom, May 3-7, 2021, pp. 1616–1618. ACM, 2021. URL https://dl.acm.org/doi/
10.5555/3463952.3464178."
REFERENCES,0.9195402298850575,"Bo Peng, Jiahai Wang, and Zizhen Zhang. A deep reinforcement learning algorithm using dynamic
attention model for vehicle routing problems.
CoRR, abs/2002.03282, 2020.
URL https:
//arxiv.org/abs/2002.03282."
REFERENCES,0.9252873563218391,"Aravind Rajeswaran, Chelsea Finn, Sham M. Kakade, and Sergey Levine.
Meta-learning
with implicit gradients.
In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Flo-
rence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural In-
formation Processing Systems 32:
Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
113–124, 2019.
URL https://proceedings.neurips.cc/paper/2019/hash/
072b030ba126b2f4b2374f342be9ed44-Abstract.html."
REFERENCES,0.9310344827586207,"Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.
Efﬁcient off-
policy meta-reinforcement learning via probabilistic context variables.
In Kamalika Chaud-
huri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97
of Proceedings of Machine Learning Research, pp. 5331–5340. PMLR, 2019.
URL http:
//proceedings.mlr.press/v97/rakelly19a.html."
REFERENCES,0.9367816091954023,Under review as a conference paper at ICLR 2022
REFERENCES,0.9425287356321839,"Bradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya
Sutskever. Some considerations on learning to explore via meta-reinforcement learning. CoRR,
abs/1803.01118, 2018. URL http://arxiv.org/abs/1803.01118."
REFERENCES,0.9482758620689655,"Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob
Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Micha¨el Mathieu, Nat McAleese,
Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-
Fitt, Valentin Dalibard, and Wojciech Marian Czarnecki. Open-ended learning leads to generally
capable agents.
CoRR, abs/2107.12808, 2021.
URL https://arxiv.org/abs/2107.
12808."
REFERENCES,0.9540229885057471,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html."
REFERENCES,0.9597701149425287,"Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, R´emi Munos,
Charles Blundell, Dharshan Kumaran, and Matthew Botvinick. Learning to reinforcement learn.
CoRR, abs/1611.05763, 2016. URL http://arxiv.org/abs/1611.05763."
REFERENCES,0.9655172413793104,"Haiping Wu, Khimya Khetarpal, and Doina Precup. Self-supervised attention-aware reinforcement
learning. In Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-Third
Conference on Innovative Applications of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Sympo-
sium on Educational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9,
2021, pp. 10311–10319. AAAI Press, 2021. URL https://ojs.aaai.org/index.php/
AAAI/article/view/17235."
REFERENCES,0.9712643678160919,"Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore with meta-policy gradient.
CoRR, abs/1803.05044, 2018a. URL http://arxiv.org/abs/1803.05044."
REFERENCES,0.9770114942528736,"Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. In Samy
Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Ro-
man Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal,
Canada, pp. 2402–2413, 2018b. URL https://proceedings.neurips.cc/paper/
2018/hash/2715518c875999308842e3455eda2fe3-Abstract.html."
REFERENCES,0.9827586206896551,"Yang Yu. Towards sample efﬁcient reinforcement learning. In J´erˆome Lang (ed.), Proceedings of the
Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence, IJCAI 2018, July 13-19,
2018, Stockholm, Sweden, pp. 5739–5743. ijcai.org, 2018. doi: 10.24963/ijcai.2018/820. URL
https://doi.org/10.24963/ijcai.2018/820."
REFERENCES,0.9885057471264368,"Zeyu Zheng, Junhyuk Oh, and Satinder Singh.
On learning intrinsic rewards for pol-
icy gradient methods.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kris-
ten Grauman,
Nicol`o Cesa-Bianchi,
and Roman Garnett (eds.),
Advances in Neural
Information Processing Systems 31:
Annual Conference on Neural Information Pro-
cessing Systems 2018,
NeurIPS 2018,
December 3-8,
2018,
Montr´eal,
Canada,
pp.
4649–4659, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
51de85ddd068f0bc787691d356176df9-Abstract.html."
REFERENCES,0.9942528735632183,"Wei Zhou, Yiying Li, Yongxin Yang, Huaimin Wang, and Timothy M. Hospedales.
On-
line
meta-critic
learning
for
off-policy
actor-critic
methods.
In
Hugo
Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33:
Annual Conference on Neu-
ral Information Processing Systems 2020,
NeurIPS 2020,
December 6-12,
2020,
vir-
tual,
2020.
URL
https://proceedings.neurips.cc/paper/2020/hash/
cceff8faa855336ad53b3325914caea2-Abstract.html."
