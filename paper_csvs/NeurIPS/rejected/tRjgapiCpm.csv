Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00102880658436214,"We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient
1"
ABSTRACT,0.00205761316872428,"descent (DP-SGD) in the setting where only the last iterate is released and the in-
2"
ABSTRACT,0.0030864197530864196,"termediate iterates remain hidden. Namely, our heuristic assumes a linear structure
3"
ABSTRACT,0.00411522633744856,"for the model.
4"
ABSTRACT,0.0051440329218107,"We show experimentally that our heuristic is predictive of the outcome of privacy
5"
ABSTRACT,0.006172839506172839,"auditing applied to various training procedures. Thus it can be used prior to training
6"
ABSTRACT,0.00720164609053498,"as a rough estimate of the ﬁnal privacy leakage. We also probe the limitations of
7"
ABSTRACT,0.00823045267489712,"our heuristic by providing some artiﬁcial counterexamples where it underestimates
8"
ABSTRACT,0.009259259259259259,"the privacy leakage.
9"
ABSTRACT,0.0102880658436214,"The standard composition-based privacy analysis of DP-SGD effectively assumes
10"
ABSTRACT,0.01131687242798354,"that the adversary has access to all intermediate iterates, which is often unrealistic.
11"
ABSTRACT,0.012345679012345678,"However, this analysis remains the state of the art in practice. While our heuristic
12"
ABSTRACT,0.013374485596707819,"does not replace a rigorous privacy analysis, it illustrates the large gap between
13"
ABSTRACT,0.01440329218106996,"the best theoretical upper bounds and the privacy auditing lower bounds and sets a
14"
ABSTRACT,0.015432098765432098,"target for further work to improve the theoretical privacy analyses.
15"
INTRODUCTION,0.01646090534979424,"1
Introduction
16"
INTRODUCTION,0.01748971193415638,"Differential privacy (DP) [DMNS06] deﬁnes a measure of how much private information from the
17"
INTRODUCTION,0.018518518518518517,"training data leaks through the output of an algorithm. The standard differentially private algorithm
18"
INTRODUCTION,0.01954732510288066,"for deep learning is DP-SGD [BST14; ACGMMTZ16], which differs from ordinary stochastic
19"
INTRODUCTION,0.0205761316872428,"gradient descent in two ways: the gradient of each example is clipped to bound its norm and then
20"
INTRODUCTION,0.021604938271604937,"Gaussian noise is added at each iteration.
21"
INTRODUCTION,0.02263374485596708,"The standard privacy analysis of DP-SGD is based on composition [BST14; ACGMMTZ16; Mir17;
22"
INTRODUCTION,0.023662551440329218,"Ste22; KJH20]. In particular, it applies to the setting where the privacy adversary has access to
23"
INTRODUCTION,0.024691358024691357,"all intermediate iterates of the training procedure. In this setting, the analysis is known to be tight
24"
INTRODUCTION,0.0257201646090535,"[NSTPC21; NHSBTJCT23]. However, in practice, potential adversaries rarely have access to the
25"
INTRODUCTION,0.026748971193415638,"intermediate iterates of the training procedure, rather they only have access to the ﬁnal model. Access
26"
INTRODUCTION,0.027777777777777776,"to the ﬁnal model can either be through queries to an API or via the raw model weights. The key
27"
INTRODUCTION,0.02880658436213992,"question motivating our work is the following.
28"
INTRODUCTION,0.029835390946502057,"Is it possible to obtain sharper privacy guarantees for DP-SGD when the adversary
29"
INTRODUCTION,0.030864197530864196,"only has access to the ﬁnal model, rather than all intermediate iterates?
30"
BACKGROUND & RELATED WORK,0.03189300411522634,"1.1
Background & Related Work
31"
BACKGROUND & RELATED WORK,0.03292181069958848,"The question above has been studied from two angles: Theoretical upper bounds, and privacy auditing
32"
BACKGROUND & RELATED WORK,0.033950617283950615,"lower bounds. Our goal is to shed light on this question from a third angle via principled heuristics.
33"
BACKGROUND & RELATED WORK,0.03497942386831276,"A handful of theoretical analyses [FMTT18; CYS21; YS22; AT22; BSA24] have shown that asymp-
34"
BACKGROUND & RELATED WORK,0.0360082304526749,"totically the privacy guarantee of the last iterate of DP-SGD can be far better than the standard
35"
BACKGROUND & RELATED WORK,0.037037037037037035,"composition-based analysis that applies to releasing all iterates. In particular, as the number of
36"
BACKGROUND & RELATED WORK,0.03806584362139918,"iterations increases, these analyses give a privacy guarantee that converges to a constant (depending
37"
BACKGROUND & RELATED WORK,0.03909465020576132,"on the loss function and the scale of the noise), whereas the standard composition-based analysis
38"
BACKGROUND & RELATED WORK,0.040123456790123455,"would give a privacy guarantee that increases forever. Unfortunately, these theoretical analyses
39"
BACKGROUND & RELATED WORK,0.0411522633744856,"are only applicable under strong assumptions on the loss function, such as (strong) convexity and
40"
BACKGROUND & RELATED WORK,0.04218106995884774,"smoothness. We lack an understanding of how well they reﬂect the “real” privacy leakage.
41"
BACKGROUND & RELATED WORK,0.043209876543209874,"Privacy auditing [JUO20; DWWZK18; BGDCTV18; SNJ23; TTSSJC22; ZBWTSRPNK22] com-
42"
BACKGROUND & RELATED WORK,0.044238683127572016,"plements theoretical analysis by giving empirical lower bounds on the privacy leakage. Privacy
43"
BACKGROUND & RELATED WORK,0.04526748971193416,"auditing works by performing a membership inference attack [SSSS17; HSRDTMPSNC08; SOJH09;
44"
BACKGROUND & RELATED WORK,0.046296296296296294,"DSSUV15]. That is, it constructs neighbouring inputs and demonstrates that the corresponding
45"
BACKGROUND & RELATED WORK,0.047325102880658436,"output distributions can be distinguished well enough to imply a lower bound on the differential
46"
BACKGROUND & RELATED WORK,0.04835390946502058,"privacy parameters. In practice, the theoretical privacy analysis may give uncomfortably large values
47"
BACKGROUND & RELATED WORK,0.04938271604938271,"for the privacy leakage (e.g., ε > 10); in this case, privacy auditing may be used as evidence that
48"
BACKGROUND & RELATED WORK,0.050411522633744855,"the “real” privacy leakage is lower. There are settings where the theoretical analysis is matched by
49"
BACKGROUND & RELATED WORK,0.051440329218107,"auditing, such as when all intermediate results are released [NSTPC21; NHSBTJCT23]. However,
50"
BACKGROUND & RELATED WORK,0.05246913580246913,"despite signiﬁcant work on privacy auditing and membership inference [CCNSTT22; BTRKMW24;
51"
BACKGROUND & RELATED WORK,0.053497942386831275,"WBKBGGG23; LF20; SDSOJ19; ZLS23], a large gap remains between the theoretical upper bounds
52"
BACKGROUND & RELATED WORK,0.05452674897119342,"and the auditing lower bounds [AKOOMS23; NHSBTJCT23] when only the ﬁnal parameters are
53"
BACKGROUND & RELATED WORK,0.05555555555555555,"released. This observed gap is the starting point for our work.
54"
OUR CONTRIBUTIONS,0.056584362139917695,"1.2
Our Contributions
55"
OUR CONTRIBUTIONS,0.05761316872427984,"We propose a heuristic privacy analysis of DP-SGD in the setting where only the ﬁnal iterate is
56"
OUR CONTRIBUTIONS,0.05864197530864197,"released. Our experiments demonstrate that this heuristic analysis consistently provides an upper
57"
OUR CONTRIBUTIONS,0.059670781893004114,"bound on the privacy leakage measured by privacy auditing tools in realistic deep learning settings.
58"
OUR CONTRIBUTIONS,0.060699588477366256,"Our heuristic analysis corresponds to a worst-case theoretical analysis under the assumption that the
59"
OUR CONTRIBUTIONS,0.06172839506172839,"loss functions are linear. This case is simple enough to allow for an exact privacy analysis whose
60"
OUR CONTRIBUTIONS,0.06275720164609054,"parameters are can be computed numerically (Theorem 1). Our consideration of linear losses is built
61"
OUR CONTRIBUTIONS,0.06378600823045268,"on the observation that current auditing techniques achieve the highest ε values when the gradients
62"
OUR CONTRIBUTIONS,0.06481481481481481,"of the canaries – that is, the examples that are included or excluded to test the privacy leakage – are
63"
OUR CONTRIBUTIONS,0.06584362139917696,"ﬁxed and independent from the gradients of the other examples. This is deﬁnitely the case for linear
64"
OUR CONTRIBUTIONS,0.0668724279835391,"losses; the linear assumption thus allows us to capture the setting where current attacks are most
65"
OUR CONTRIBUTIONS,0.06790123456790123,"effective. Linear loss functions are also known to be the worst case for the non-subsampled (i.e., full
66"
OUR CONTRIBUTIONS,0.06893004115226338,"batch) case; see Appendix B. Assuming linearity is unnatural from an optimization perspective, as
67"
OUR CONTRIBUTIONS,0.06995884773662552,"there is no minimizer. But, from a privacy perspective, we show that it captures the state of the art.
68"
OUR CONTRIBUTIONS,0.07098765432098765,"We also probe the limitations of our heuristic and give some artiﬁcial counterexamples where it
69"
OUR CONTRIBUTIONS,0.0720164609053498,"underestimates empirical privacy leakage. One class of counterexamples exploits the presence of a
70"
OUR CONTRIBUTIONS,0.07304526748971193,"regularizer. Roughly, the regularizer partially zeros out the noise that is added for privacy. However,
71"
OUR CONTRIBUTIONS,0.07407407407407407,"the regularizer also partially zeros out the signal of the canary gradient. These two effects are almost
72"
OUR CONTRIBUTIONS,0.07510288065843622,"balanced, which makes the counterexample very delicate. In a second class of counterexamples, the
73"
OUR CONTRIBUTIONS,0.07613168724279835,"data is carefully engineered so that the ﬁnal iterate effectively encodes the entire trajectory, in which
74"
OUR CONTRIBUTIONS,0.07716049382716049,"case there is no difference between releasing the last iterate and all iterates.
75"
OUR CONTRIBUTIONS,0.07818930041152264,"Implications: Heuristics cannot replace rigorous theoretical analyses. However, our heuristic can
76"
OUR CONTRIBUTIONS,0.07921810699588477,"serve as a target for future improvements to both privacy auditing as well as theoretical analysis. For
77"
OUR CONTRIBUTIONS,0.08024691358024691,"privacy auditing, matching or exceeding our heuristic is a more reachable goal than matching the
78"
OUR CONTRIBUTIONS,0.08127572016460906,"theoretical upper bounds, although our experimental results show that even this would require new
79"
OUR CONTRIBUTIONS,0.0823045267489712,"attacks. When theoretical analyses fail to match our heuristic, we should identify why there is a gap,
80"
OUR CONTRIBUTIONS,0.08333333333333333,"which builds intuition and could point towards further improvements.
81"
OUR CONTRIBUTIONS,0.08436213991769548,"Given that privacy auditing is computationally intensive and difﬁcult to perform correctly [AZT24],
82"
OUR CONTRIBUTIONS,0.08539094650205761,"we believe that our heuristic can also be valuable in practice. In particular, our heuristic can be used
83"
OUR CONTRIBUTIONS,0.08641975308641975,"prior to training (e.g., during hyperparameter selection) to predict the outcome of privacy auditing
84"
OUR CONTRIBUTIONS,0.0874485596707819,"when applied to the ﬁnal model. (This is a similar use case to scaling laws.)
85"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.08847736625514403,"2
Linearized Heuristic Privacy Analysis
86"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.08950617283950617,"Algorithm 1 Noisy Clipped Stochastic Gradient De-
scent (DP-SGD) [BST14; ACGMMTZ16]"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09053497942386832,"function DP-SGD(x ∈X n, T ∈N, q ∈[0, 1], η ∈
(0, ∞), σ ∈(0, ∞), ℓ: Rd × X →R, r : Rd →R)"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09156378600823045,"Initialize model m0 ∈Rd.
for t = 1 · · · T do"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09259259259259259,"Sample minibatch Bt ⊆[n] including each ele-
ment independently with probability q.
Compute gradients of the loss ∇mt−1ℓ(mt−1, xi)
for all i
∈
Bt
and of the regularizer
∇mt−1r(mt−1).
Clip loss gradients: clip
 
∇mt−1ℓ(mt−1, xi)

:=
∇mt−1ℓ(mt−1,xi)
max{1,∥∇mt−1ℓ(mt−1,xi)∥2}."
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09362139917695474,"Sample noise ξt ←N(0, σ2Id).
Update"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09465020576131687,"mt =mt−1−η ·
P
i∈Bt clip
 
∇mt−1ℓ(mt−1, xi)
"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09567901234567901,"+∇mt−1r(mt−1)+ξt 
."
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09670781893004116,"end for
if last_iterate_only then"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09773662551440329,"return mT
else if intermediate_iterates then"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09876543209876543,"return m0, m1, · · · , mT −1, mT
end if
end function"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.09979423868312758,"Theorem 1 presents our heuristic differen-
87"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.10082304526748971,"tial privacy analysis of DP-SGD (which we
88"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.10185185185185185,"present in Algorithm 1 for completeness;
89"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.102880658436214,"note that we include a regularizer r whose
90"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.10390946502057613,"gradient is not clipped, because it does not
91"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.10493827160493827,"depend on the private data x). We con-
92"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.10596707818930041,"sider Poisson subsampled minibatches and
93"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.10699588477366255,"add/remove neighbours, as is standard in
94"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.10802469135802469,"the differential privacy literature.
95"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.10905349794238683,"Our analysis takes the form of a conditional
96"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.11008230452674897,"privacy guarantee. Namely, under the as-
97"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1111111111111111,"sumption that the loss and regularizer are
98"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.11213991769547325,"linear, we obtain a fully rigorous differen-
99"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.11316872427983539,"tial privacy guarantee. The heuristic is to
100"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.11419753086419752,"apply this guarantee to loss functions that
101"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.11522633744855967,"are not linear (such as those that arise in
102"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.11625514403292181,"deep learning applications). Our thesis is
103"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.11728395061728394,"that, in most cases, the conclusion of the
104"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1183127572016461,"theorem is still a good approximation, even
105"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.11934156378600823,"when the assumption does not hold.
106"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12037037037037036,"Recall that a function ℓ: Rd →R is linear
107"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12139917695473251,"if there exist α ∈Rd and β ∈R such that
108"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12242798353909465,"ℓ(m) = ⟨α, m⟩+ β for all m.
109"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12345679012345678,"Theorem 1 (Privacy of DP-SGD for linear
110"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12448559670781893,"losses). Let x, T, q, η, σ, ℓ, r be as in Algo-
111"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12551440329218108,"rithm 1. Assume r and ℓ(·, x), for every
112"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12654320987654322,"x ∈X, are linear.
113"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12757201646090535,"Letting
114"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1286008230452675,"P := Binomial(T, q) + N(0, σ2T), Q := N(0, σ2T),
(1)"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.12962962962962962,"DP-SGD with last_iterate_only satisﬁes (ε, δ)-differential privacy with ε ≥0 arbitrary and
115"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.13065843621399176,"δ = δT,q,σ(ε) := max{Heε(P, Q), Heε(Q, P)}.
(2)"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.13168724279835392,"Here, Heε denotes the eε-hockey-stick-divergence Heε(P, Q) := supS P(S) −eεQ(S).
116"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.13271604938271606,"Equation 1 gives us a value of the privacy failure probability parameter δ. But it is more natural to
117"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1337448559670782,"work with the privacy loss bound parameter ε, which can be computed by inverting the formula:
118"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.13477366255144033,"εT,q,σ(δ) := min{ε ≥0 : δT,q,σ(ε) ≤δ}.
(3)"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.13580246913580246,"Both δT,q,σ(ε) and εT,q,σ(δ) can be computed using existing open-source DP accounting libraries
119"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1368312757201646,"[Goo20]. We also provide a self-contained & efﬁcient method for computing them in Appendix A.
120"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.13786008230452676,"The proof of Theorem 1 is deferred to Appendix A, but we sketch the main ideas: Under the linearity
121"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1388888888888889,"assumption, the output of DP-SGD is just a sum of the gradients and noises. We can reduce to
122"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.13991769547325103,"dimension d = 1, since the only relevant direction is that of the gradient of the canary1 (which is
123"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.14094650205761317,"constant). We can also ignore the gradients of the other examples. Thus, by rescaling, the worst case
124"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1419753086419753,"pair of output distributions can be represented as in Equation 1. Namely, Q = PT
t=1 ξt is simply the
125"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.14300411522633744,"noise ξt ←N(0, σ2) summed over T iterations; this corresponds to the case where the canary is
126"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1440329218106996,"excluded. When the canary is included, it is sampled with probability q in each iteration and thus
127"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.14506172839506173,"the total number of times it is sampled over T iterations is Binomial(T, q). Thus P is the sum of the
128"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.14609053497942387,"contributions of the canary and the noise. Finally the deﬁnition of differential privacy lets us compute
129"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.147119341563786,"ε and δ from this pair of distributions. Tightness follows from the fact that there exists a loss function
130"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.14814814814814814,"and pair of inputs such that the corresponding outputs of DP-SGD matches the pair P and Q.
131"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.14917695473251028,"1The canary refers to the individual datapoint that is added or removed between neighbouring datasets. This
terminology is used in the privacy auditing/attacks literature inspired on the expression “canary in a coalmine.”"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.15020576131687244,"100
101
102
103"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.15123456790123457,"T = 1/q = 25/
2 100 101 102 103"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1522633744855967,"at 
= 1e
06"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.15329218106995884,"Heuristic
Standard
Full Batch"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.15432098765432098,(a) Keep Tp and Tσ2 constant.
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.15534979423868311,"100
101
102
103
104"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.15637860082304528,T = 1/q 0 2 4 6 8 10
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1574074074074074,"at 
= 1e
06"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.15843621399176955,"Heuristic
Standard
Full Batch"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.15946502057613168,(b) Keep Standard DP constant.
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.16049382716049382,"100
101
102
103
104"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.16152263374485595,"T (with q = 0.01 and 
= 0.5) 0 2 4 6 8 10 12 14"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.16255144032921812,"at 
= 1e
06"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.16358024691358025,"Heuristic
Standard
Full Batch"
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.1646090534979424,(c) Keep p and σ constant.
LINEARIZED HEURISTIC PRIVACY ANALYSIS,0.16563786008230452,"Figure 1: Comparison of our heuristic to baselines in various parameter regimes. Horizontal axis is
number of iterations T and vertical axis is ε such that we have (ε, 10−6)-DP."
BASELINES,0.16666666666666666,"2.1
Baselines
132"
BASELINES,0.16769547325102882,"In addition to privacy auditing, we compare our heuristic to two different baselines in Figure 1.
133"
BASELINES,0.16872427983539096,"The ﬁrst is the standard, composition-based analysis. We use the open-source library from Google
134"
BASELINES,0.1697530864197531,"[Goo20], which computes a tight DP guarantee for DP-SGD with intermediate_iterates. Be-
135"
BASELINES,0.17078189300411523,"cause DP-SGD with intermediate_iterates gives the adversary more information than with
136"
BASELINES,0.17181069958847736,"last_iterate_only, this will always give at least as large an estimate for ε as our heuristic.
137"
BASELINES,0.1728395061728395,"We also consider approximating DP-SGD by full batch DP-GD. That is, set q = 1 and rescale the
138"
BASELINES,0.17386831275720166,"learning rate η and noise multiplier σ to keep the expected step and privacy noise variance constant:
139"
BASELINES,0.1748971193415638,"DP-SGD(x, T, q, η, σ, ℓ, r)
|
{z
}
batch size ≈nq, T iterations, T q epochs"
BASELINES,0.17592592592592593,"≈DP-SGD(x, T, 1, η · q, σ/q, ℓ, r)
|
{z
}
batch size n, T iterations, T epochs .
(4)"
BASELINES,0.17695473251028807,"The latter algorithm is full batch DP-GD since at each step it includes each data point in the batch
140"
BASELINES,0.1779835390946502,"with probability 1. Since full batch DP-GD does not rely on privacy ampliﬁcation by subsampling,
141"
BASELINES,0.17901234567901234,"it is much easier to analyze its privacy guarantees. Interestingly, there is no difference between
142"
BASELINES,0.1800411522633745,"full batch DP-GD with last_iterate_only and with intermediate_iterates; see Appendix B.
143"
BASELINES,0.18106995884773663,"Full batch DP-GD generally has better privacy guarantees than the corresponding minibatch DP-SGD
144"
BASELINES,0.18209876543209877,"and so this baseline usually (but not always) gives smaller values for the privacy leakage ε than our
145"
BASELINES,0.1831275720164609,"heuristic. In practice, full batch DP-GD is too computationally expensive to run. But we can use it as
146"
BASELINES,0.18415637860082304,"an idealized comparison point for the privacy analysis.
147"
BASELINES,0.18518518518518517,"103
104"
BASELINES,0.18621399176954734,Number of steps 0 1 2 3 4 5 6 7 8
BASELINES,0.18724279835390947,ε at δ = 1e −05
BASELINES,0.1882716049382716,"Heuristic (σ=1.1,q=0.01)"
BASELINES,0.18930041152263374,"Standard (σ=1.1,q=0.01)"
BASELINES,0.19032921810699588,Empirical
BASELINES,0.19135802469135801,(a) q = 0.01 103
BASELINES,0.19238683127572018,Number of steps 0 2 4 6 8
BASELINES,0.1934156378600823,ε at δ = 1e −05
BASELINES,0.19444444444444445,"Heuristic (σ=2.1,q=0.08)"
BASELINES,0.19547325102880658,"Standard (σ=2.1,q=0.08)"
BASELINES,0.19650205761316872,Empirical
BASELINES,0.19753086419753085,(b) q = 0.08
BASELINES,0.19855967078189302,"Figure 2: Black-box gradient space attacks fail to achieve tight auditing when other data points are
sampled from the data distribution. Heuristic and standard bounds diverge from empirical results,
indicating the attack’s ineffectiveness. This contrasts with previous work which tightly auditing with
access to intermediate updates."
BASELINES,0.19958847736625515,"100
101
102
103"
BASELINES,0.2006172839506173,Number of steps 0 1 2 3 4 5 6 7 8
BASELINES,0.20164609053497942,ε at δ = 1e −05
BASELINES,0.20267489711934156,"Heuristic
Standard
Empirical"
BASELINES,0.2037037037037037,(a) q = 0.01
BASELINES,0.20473251028806586,"100
101
102
103"
BASELINES,0.205761316872428,Number of steps 0 1 2 3 4 5 6 7 8
BASELINES,0.20679012345679013,ε at δ = 1e −05
BASELINES,0.20781893004115226,"Heuristic
Standard
Empirical"
BASELINES,0.2088477366255144,(b) q = 0.1
BASELINES,0.20987654320987653,"Figure 3: For gradient space attacks with adversarial datasets, the empirical epsilon (ε) closely tracks
the ﬁnal epsilon except for at small step counts, where distinguishing is more challenging. This is
evident at both subsampling probability values we study (q = 0.01 and q = 0.1)."
BASELINES,0.2109053497942387,"101
102
103"
BASELINES,0.21193415637860083,Number of steps 0 1 2 3 4 5 6 7 8
BASELINES,0.21296296296296297,ε at δ = 1e −05
BASELINES,0.2139917695473251,"Heuristic 
Standard 
Empirical"
BASELINES,0.21502057613168724,(a) All zero gradient inputs
BASELINES,0.21604938271604937,"101
102
103"
BASELINES,0.21707818930041153,Number of steps 0 1 2 3 4 5 6 7 8
BASELINES,0.21810699588477367,ε at δ = 1e −05
BASELINES,0.2191358024691358,"Heuristic 
Standard 
Empirical"
BASELINES,0.22016460905349794,(b) CIFAR10 Dataset
BASELINES,0.22119341563786007,"Figure 4: Input space attacks show promising results with both natural and blank image settings,
although blank images have higher attack success. These input space attacks achieve tighter results
than gradient space attacks in the natural data setting, in contrast to ﬁndings from prior work."
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.2222222222222222,"3
Empirical Evaluation via Privacy Auditing
148"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.22325102880658437,"Setup: We follow the construction of Nasr, Song, Thakurta, Papernot, and Carlini [NSTPC21] where
149"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.2242798353909465,"we have 3 entities, adversarial crafter, model trainer, and distinguisher. In this paper, we assume
150"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.22530864197530864,"the distinguisher only has access the ﬁnal iteration of the model parameters. We use the CIFAR10
151"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.22633744855967078,"dataset [Ale09] with a WideResNet model [ZK16] unless otherwise speciﬁed; in particular, we follow
152"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.2273662551440329,"the training setup of De, Berrada, Hayes, Smith, and Balle [DBHSB22], where we train and audit
153"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.22839506172839505,"a model with 79% test accuracy and, using the standard analysis, (ε = 8, δ = 10−5)-DP. For each
154"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.2294238683127572,"experiment we trained 512 CIFAR10 models with and without the canary (1024 total). To compute
155"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.23045267489711935,"the empirical lower bounds we use the PLD approach with Clopper-Pearson conﬁdence intervals
156"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.23148148148148148,"used by Nasr, Hayes, Steinke, Balle, Tramèr, Jagielski, Carlini, and Terzis [NHSBTJCT23]. Here we
157"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.23251028806584362,"assume the adversary knows the sampling rate and the number of iterations and is only estimating the
158"
EMPIRICAL EVALUATION VIA PRIVACY AUDITING,0.23353909465020575,"noise multiplier used in DP-SGD, from which the reported privacy parameters (ε and δ) are derived.
159"
EXPERIMENTAL RESULTS,0.2345679012345679,"3.1
Experimental Results
160"
EXPERIMENTAL RESULTS,0.23559670781893005,"We implement state-of-the-art attacks from prior work [NSTPC21; NHSBTJCT23]. These attacks
161"
EXPERIMENTAL RESULTS,0.2366255144032922,"heavily rely on the intermediate steps and, as a result, do not achieve tight results. In the next
162"
EXPERIMENTAL RESULTS,0.23765432098765432,"section, we design speciﬁc attacks for our heuristic privacy analysis approach to further understand
163"
EXPERIMENTAL RESULTS,0.23868312757201646,"its limitations and potential vulnerabilities. We used Google Cloud A2-megagpu-16g machines with
164"
EXPERIMENTAL RESULTS,0.2397119341563786,"16 Nvidia A100 40GB GPUs. Overall, we use roughly 33,000 GPU hours for our experiments.
165"
EXPERIMENTAL RESULTS,0.24074074074074073,"Gradient Space Attack: The most powerful attacks in prior work are gradient space attacks where
166"
EXPERIMENTAL RESULTS,0.2417695473251029,"the adversary injects a malicious gradient directly into the training process, rather than an example;
167"
EXPERIMENTAL RESULTS,0.24279835390946503,"prior work has shown that this attack can produce tight lower bounds, independent of the dataset
168"
EXPERIMENTAL RESULTS,0.24382716049382716,"and model used for training [NHSBTJCT23]. However, these previous attacks require access to all
169"
EXPERIMENTAL RESULTS,0.2448559670781893,"intermediate training steps to achieve tight results. Here, we use canary gradients in two settings: one
170"
EXPERIMENTAL RESULTS,0.24588477366255143,"where the other data points are non-adversarial and sampled from the real training data, and another
171"
EXPERIMENTAL RESULTS,0.24691358024691357,"where the other data points are designed to have very small gradients (≈0). This last setting was
172"
EXPERIMENTAL RESULTS,0.24794238683127573,"shown by [NSTPC21] to result in tighter auditing. In all attacks, we assume the distinguisher has
173"
EXPERIMENTAL RESULTS,0.24897119341563786,"access to all adversarial gradient vectors. For malicious gradients, we use Dirac gradient canaries,
174"
EXPERIMENTAL RESULTS,0.25,"where gradient vectors consist of zeros in all but a single index. In both cases, the distinguishing test
175"
EXPERIMENTAL RESULTS,0.25102880658436216,"measures the dot product of the ﬁnal model checkpoint and the gradient canary.
176"
EXPERIMENTAL RESULTS,0.25205761316872427,"Figure 2 summarizes the results for the non-adversarial data setting, with other examples sampled
177"
EXPERIMENTAL RESULTS,0.25308641975308643,"from the true training data. In this experiment, we ﬁx noise magnitude and subsampling probability,
178"
EXPERIMENTAL RESULTS,0.25411522633744854,"and run for various numbers of training steps. While prior work has shown tight auditing in this
179"
EXPERIMENTAL RESULTS,0.2551440329218107,"setting, we ﬁnd an adversary without access to intermediate updates obtains much weaker attacks.
180"
EXPERIMENTAL RESULTS,0.25617283950617287,"Indeed, auditing with this strong attack results even in much lower values than the heuristic outputs.
181"
EXPERIMENTAL RESULTS,0.257201646090535,"Our other setting assumes the other data points are maliciously chosen. We construct an adversarial
182"
EXPERIMENTAL RESULTS,0.25823045267489714,"“dataset” of m + 1 gradients, m of which are zero, and one gradient is constant (with norm equal to
183"
EXPERIMENTAL RESULTS,0.25925925925925924,"the clipping norm), applying gradients directly rather than using any examples. As this experiment
184"
EXPERIMENTAL RESULTS,0.2602880658436214,"does not require computing gradients, it is very cheap to run more trials, so we run this procedure
185"
EXPERIMENTAL RESULTS,0.2613168724279835,"N = 100, 000 times with the gradient canary, and N times without it, and compute an empirical
186"
EXPERIMENTAL RESULTS,0.2623456790123457,"estimate for ε with these values. We plot the results of this experiment in Figure 3 together with
187"
EXPERIMENTAL RESULTS,0.26337448559670784,"the ε output by the theoretical analysis and the heuristic, ﬁxing the subsampling probability and
188"
EXPERIMENTAL RESULTS,0.26440329218106995,"varying the number of update steps. We adjust the noise parameter to ensure the standard theoretical
189"
EXPERIMENTAL RESULTS,0.2654320987654321,"analysis produces a ﬁxed ε bound. The empirical measured ε is close to the heuristic ε except for
190"
EXPERIMENTAL RESULTS,0.2664609053497942,"when training with very small step counts: we expect this looseness to be the result of statistical
191"
EXPERIMENTAL RESULTS,0.2674897119341564,"effects, as lower step counts have higher relative variance at a ﬁxed number of trials.
192"
EXPERIMENTAL RESULTS,0.26851851851851855,"Input Space Attack: In practice, adversaries typically cannot insert malicious gradients freely in
193"
EXPERIMENTAL RESULTS,0.26954732510288065,"training steps. Therefore, we also study cases where the adversary is limited to inserting malicious
194"
EXPERIMENTAL RESULTS,0.2705761316872428,"inputs into the training set. Label ﬂip attacks are one of the most successful approaches used to audit
195"
EXPERIMENTAL RESULTS,0.2716049382716049,"DP machine learning models in prior work [NHSBTJCT23; SNJ23]. For input space attacks, we use
196"
EXPERIMENTAL RESULTS,0.2726337448559671,"the loss of the malicious input as a distinguisher. Similar to our gradient space attacks, we consider
197"
EXPERIMENTAL RESULTS,0.2736625514403292,"two settings for input space attacks: one where other data points are correctly sampled from the
198"
EXPERIMENTAL RESULTS,0.27469135802469136,"dataset, and another where the other data points are blank images.
199"
EXPERIMENTAL RESULTS,0.2757201646090535,"Figure 4 summarizes the results for this setting. Comparing to Figure 2, input space attacks achieve
200"
EXPERIMENTAL RESULTS,0.2767489711934156,"tighter results than gradient space attacks. This ﬁnding is in stark contrast to prior work. The reason
201"
EXPERIMENTAL RESULTS,0.2777777777777778,"is that input space attacks do not rely on intermediate iterates, so they transfer well to our setting.
202"
EXPERIMENTAL RESULTS,0.2788065843621399,"In all the cases discussed so far, the empirical results for both gradient and input attacks fall below
203"
EXPERIMENTAL RESULTS,0.27983539094650206,"the heuristic analysis and do not violate the upper bounds based on the underlying assumptions. This
204"
EXPERIMENTAL RESULTS,0.2808641975308642,"suggests that the heuristic might serve as a good indicator for assessing potential vulnerabilities.
205"
EXPERIMENTAL RESULTS,0.28189300411522633,"However, in the next section, we delve into speciﬁc attack scenarios that exploit the assumptions used
206"
EXPERIMENTAL RESULTS,0.2829218106995885,"in the heuristic analysis to create edge cases where the heuristic bounds are indeed violated.
207"
COUNTEREXAMPLES,0.2839506172839506,"4
Counterexamples
208"
COUNTEREXAMPLES,0.28497942386831276,"We now test the limits of our heuristic by constructing some artiﬁcial counterexamples. That is, we
209"
COUNTEREXAMPLES,0.28600823045267487,"construct inputs to DP-SGD with last_iterate_only such that the true privacy loss exceeds the
210"
COUNTEREXAMPLES,0.28703703703703703,"bound given by our heuristic. While we do not expect the contrived structures of these examples to
211"
COUNTEREXAMPLES,0.2880658436213992,"manifest in realistic learning settings, they highlight the difﬁculties of formalizing settings where the
212"
COUNTEREXAMPLES,0.2890946502057613,"heuristic gives a provable upper bound on the privacy loss.
213"
COUNTEREXAMPLES,0.29012345679012347,"4.1
Warmup: Zeroing Out The Model Weights
214"
COUNTEREXAMPLES,0.2911522633744856,"We begin by noting the counterintuitive fact that our heuristic εT,q,σ(δ) is not always monotone in
215"
COUNTEREXAMPLES,0.29218106995884774,"the number of steps T when the other parameters σ, q, δ are kept constant. This is shown in Figure 1c.
216"
COUNTEREXAMPLES,0.2932098765432099,"More steps means there is both more noise and more signal from the gradients; these effects partially
217"
COUNTEREXAMPLES,0.294238683127572,"cancel out, but the net effect can be non-monotone.
218"
COUNTEREXAMPLES,0.2952674897119342,"We can use a regularizer r(m) = ∥m∥2
2/2η so that η · ∇mr(m) = m. This regularizer zeros out the
219"
COUNTEREXAMPLES,0.2962962962962963,"model from the previous step, i.e., the update of DP-SGD becomes
220"
COUNTEREXAMPLES,0.29732510288065844,mt = mt−1 −η · X
COUNTEREXAMPLES,0.29835390946502055,"i∈Bt
clip
 
∇mt−1ℓ(mt−1, xi)

+ ∇mt−1r(mt−1) + ξt ! (5)"
COUNTEREXAMPLES,0.2993827160493827,"= η ·
X"
COUNTEREXAMPLES,0.3004115226337449,"i∈Bt
clip
 
∇mt−1ℓ(mt−1, xi)

+ ξt.
(6)"
COUNTEREXAMPLES,0.301440329218107,"This means that the last iterate mT is effectively the result of only a single iteration of DP-SGD. In
221"
COUNTEREXAMPLES,0.30246913580246915,"particular, it will have a privacy guarantee corresponding to one iteration. Combining this regularizer
222"
COUNTEREXAMPLES,0.30349794238683125,"with a linear loss and a setting of the parameters T, q, σ, δ such that the privacy loss is non-monotone
223"
COUNTEREXAMPLES,0.3045267489711934,"– i.e., εT,q,σ(δ) < ε1,q,σ(δ) – yields a counterexample.
224"
COUNTEREXAMPLES,0.3055555555555556,"In light of this counterexample, in the next subsection, we benchmark our counterexample against
225"
COUNTEREXAMPLES,0.3065843621399177,"sweeping over smaller values of T. I.e., we consider maxt≤T εt,q,σ(δ) instead of simply εT,q,σ(δ).
226"
COUNTEREXAMPLES,0.30761316872427985,"4.2
Linear Loss + Quadratic Regularizer
227"
COUNTEREXAMPLES,0.30864197530864196,"Consider running DP-SGD in one dimension (i.e., d = 1) with a linear loss ℓ(m, x) = mx for
228"
COUNTEREXAMPLES,0.3096707818930041,the canary and a quadratic regularizer r(m) = 1
COUNTEREXAMPLES,0.31069958847736623,"2αm2, where α ∈[0, 1] and x ∈[−1, 1] and we
229"
COUNTEREXAMPLES,0.3117283950617284,"use learning rate η = 1. With sampling probability q, after T iterations the privacy guarantee
230"
COUNTEREXAMPLES,0.31275720164609055,"is equivalent to distinguishing Q := N(0, bσ2) and P := N(P"
COUNTEREXAMPLES,0.31378600823045266,"i∈[T ](1 −α)i−1Bernoulli(q), bσ2),
231"
COUNTEREXAMPLES,0.3148148148148148,"where bσ2 := σ2 P
i∈[T ](1 −α)2(i−1). When α = 0, this retrieves linear losses. When α = 1, this
232"
COUNTEREXAMPLES,0.31584362139917693,"corresponds to distinguishing N(0, bσ2) and N(Bernoulli(q), bσ2) or, equivalently, to distinguishing
233"
COUNTEREXAMPLES,0.3168724279835391,"linear losses after T = 1 iteration. If we maximize our heuristic over the number of iterations ≤T,
234"
COUNTEREXAMPLES,0.31790123456790126,"then our heuristic is tight for the extremes α ∈{0, 1}.
235"
COUNTEREXAMPLES,0.31893004115226337,"A natural question is whether the worst-case privacy guarantee on this quadratic is always given by
236"
COUNTEREXAMPLES,0.31995884773662553,"α ∈{0, 1}. Perhaps surprisingly, the answer is no: we found that for T = 3, q = 0.1, σ = 1, α = 0,
237"
COUNTEREXAMPLES,0.32098765432098764,"DP-SGD is (2.222, 10−6)-DP. For α = 1 instead DP-SGD is (2.182, 10−6)-DP. However, for
238"
COUNTEREXAMPLES,0.3220164609053498,"α = 0.5 instead the quadratic loss does not satisfy (ε, 10−6)-DP for ε < 2.274.
239"
COUNTEREXAMPLES,0.3230452674897119,"However, this violation is small, which suggests our heuristic is still a reasonable for this class of
240"
COUNTEREXAMPLES,0.32407407407407407,"examples. To validate this, we consider a set of values for the tuple (T, q, σ). For each setting
241"
COUNTEREXAMPLES,0.32510288065843623,"of T, q, σ, we compute maxt≤T εt,q,σ(δ) at δ = 10−6. We then compute ε for the linear loss
242"
COUNTEREXAMPLES,0.32613168724279834,"with quadratic regularizer example with α = 1/2 in the same setting. Since the support of the
243"
COUNTEREXAMPLES,0.3271604938271605,random variable P
COUNTEREXAMPLES,0.3281893004115226,"i∈[T ](1 −α)i−1Bernoulli(q) has size 2T for α = 1/2, computing exact ε for
244"
COUNTEREXAMPLES,0.3292181069958848,"even moderate T is computationally intensive. Instead, let X be the random variable equal to
245
P"
COUNTEREXAMPLES,0.33024691358024694,"i∈[T ](1 −α)i−1Bernoulli(q), except we round up values in the support which are less than .0005
246"
COUNTEREXAMPLES,0.33127572016460904,"up to .0005, and then round each value in the support up to the nearest integer power of 1.05. We then
247"
COUNTEREXAMPLES,0.3323045267489712,"compute an exact ε for distingushing N(0, bσ2) vs N(X, bσ2). By Lemma 4.5 of Choquette-Choo,
248"
COUNTEREXAMPLES,0.3333333333333333,"Ganesh, Steinke, and Thakurta [CCGST24], we know that distinguishing N(0, bσ2) vs. N(P"
COUNTEREXAMPLES,0.3343621399176955,"i∈[T ](1−
249"
COUNTEREXAMPLES,0.33539094650205764,"α)i−1Bernoulli(q), bσ2) is no harder than distingushing N(0, bσ2) vs N(X, bσ2), and since we increase
250"
COUNTEREXAMPLES,0.33641975308641975,"the values in the support by no more than 1.05 multiplicatively, we expect that our rounding does not
251"
COUNTEREXAMPLES,0.3374485596707819,"increase ε by more than 1.05 multiplicatively.
252"
COUNTEREXAMPLES,0.338477366255144,"In Figure 5, we plot the ratio of ε at δ = 10−6 for distingushing between N(0, bσ2) and N(X, bσ2)
253"
COUNTEREXAMPLES,0.3395061728395062,"divided by the maximum over i ∈[T] of ε at δ = 10−6 for distinguishing between N(0, iσ2)
254"
COUNTEREXAMPLES,0.3405349794238683,"and N(Binomial(i, q), iσ2). We sweep over T and q, and for each q In Figure 5a (resp. Figure
255"
COUNTEREXAMPLES,0.34156378600823045,"5b) we set σ such that distinguishing N(0, σ2) from N(Bernoulli(q), σ2) satisﬁes (1, 10−6)-DP
256"
COUNTEREXAMPLES,0.3425925925925926,"(a) One iteration of DP-SGD satisﬁes (1, 10−6)-
DP."
COUNTEREXAMPLES,0.3436213991769547,"(b) One iteration of DP-SGD satisﬁes (2, 10−6)-
DP."
COUNTEREXAMPLES,0.3446502057613169,"Figure 5: Ratio of upper bound on ε for quadratic loss with α = 0.5 divided by maximum ε of i
iterations on a linear loss. In Figure 5a (resp. Figure 5b), for each choice of q, σ is set so 1 iteration
of DP-SGD satisﬁes (1, 10−6)-DP (resp (2, 10−6)-DP)."
COUNTEREXAMPLES,0.345679012345679,"(resp. (2, 10−6)-DP). In the majority of settings, the linear loss heuristic provides a larger ε than
257"
COUNTEREXAMPLES,0.34670781893004116,"the quadratic with α = 1/2, and even when the quadratic provides a larger ε, the violation is small
258"
COUNTEREXAMPLES,0.3477366255144033,"(≤3%). This is evidence that our heuristic is still a good approximation for many convex losses.
259"
PATHOLOGICAL EXAMPLE,0.3487654320987654,"4.3
Pathological Example
260"
PATHOLOGICAL EXAMPLE,0.3497942386831276,"If we allow the regularizer r to be arbitrary – in particular, not even requiring continuity – then the
261"
PATHOLOGICAL EXAMPLE,0.3508230452674897,"gradient can also be arbitrary. This ﬂexibility allows us to construct a counterexample such that the
262"
PATHOLOGICAL EXAMPLE,0.35185185185185186,"standard composition-based analysis of DP-SGD with intermediate_iterates is close to tight.
263"
PATHOLOGICAL EXAMPLE,0.35288065843621397,"Speciﬁcally, choose the regularizer so that the update m′ = m −η∇mr(m) does the following:
264"
PATHOLOGICAL EXAMPLE,0.35390946502057613,"m′
1 = 0 and, for i ∈[d −1], m′
i+1 = v · mi. Here v > 1 is a large constant. We chose the loss
265"
PATHOLOGICAL EXAMPLE,0.3549382716049383,"so that, for our canary x1, we have ∇mℓ(m, x1) = (1, 0, 0, · · · , 0) and, for all other examples xi
266"
PATHOLOGICAL EXAMPLE,0.3559670781893004,"(i ∈{2, 3, · · · , n}), we have ∇mℓ(m, xi) = 0. Then the last iterate is
267"
PATHOLOGICAL EXAMPLE,0.35699588477366256,"mT = (AT + ξT,1, vAT −1 + vξT −1,1 + ξT,2, v2AT −2 + v2ξT −2,1 + vξT −1,2 + ξT,3, · · · ),
(7)"
PATHOLOGICAL EXAMPLE,0.35802469135802467,"where At ←Bernoulli(p) indicates whether or not the canary was sampled in the t-th iteration and
268"
PATHOLOGICAL EXAMPLE,0.35905349794238683,"ξt,i denotes the i-th coordinate of the noise ξt added in the t-th step. Essentially, the last iterate mT
269"
PATHOLOGICAL EXAMPLE,0.360082304526749,"contains the history of all the iterates in its coordinates. Namely, the i-th coordinate of mT gives a
270"
PATHOLOGICAL EXAMPLE,0.3611111111111111,"scaled noisy approximation to AT −i:
271"
PATHOLOGICAL EXAMPLE,0.36213991769547327,"v1−imT,i = AT −i + i−1
X"
PATHOLOGICAL EXAMPLE,0.3631687242798354,"j=0
vj+1−iξT −j,i−j ∼N

AT −i, σ2 1 −v−2i"
PATHOLOGICAL EXAMPLE,0.36419753086419754,1 −v−2
PATHOLOGICAL EXAMPLE,0.36522633744855965,"
.
(8)"
PATHOLOGICAL EXAMPLE,0.3662551440329218,"As v →∞, the variance converges to σ2. In other words, if v is large, from the ﬁnal iterate, we can
272"
PATHOLOGICAL EXAMPLE,0.36728395061728397,"obtain N(Ai, σ2) for all i. This makes the standard composition-based analysis of DP-SGD tight.
273"
MALICIOUS DATASET ATTACK,0.3683127572016461,"4.4
Malicious Dataset Attack
274"
MALICIOUS DATASET ATTACK,0.36934156378600824,"The examples above rely on the regularizer having large unclipped gradients. We now construct a
275"
MALICIOUS DATASET ATTACK,0.37037037037037035,"counterexample without a regularizer, instead using other examples to amplify the canary signal.
276"
MALICIOUS DATASET ATTACK,0.3713991769547325,"Our heuristic assumes the adversary does not have access to the intermediate iterations and that the
277"
MALICIOUS DATASET ATTACK,0.3724279835390947,"model is linear. However, we can design a nonlinear model and speciﬁc training data to directly
278"
MALICIOUS DATASET ATTACK,0.3734567901234568,"challenge this assumption. The attack strategy is to use the model’s parameters as a sort of noisy
279"
MALICIOUS DATASET ATTACK,0.37448559670781895,"storage, saving all iterations within them. Then with access only to the ﬁnal model, an adversary
280"
MALICIOUS DATASET ATTACK,0.37551440329218105,"Table 1: Previous works showed that large batch sizes achieve high performing models [DBHSB22].
Using our heuristic analysis it is possible to achieve similar performance for smaller batch sizes."
MALICIOUS DATASET ATTACK,0.3765432098765432,"Batch size
Heuristic ε
Standard ε
Accuracy
Empirical ε"
MALICIOUS DATASET ATTACK,0.3775720164609053,"4096
6.34
8
79.5%
1.7
512
7.0
12
79.1%
1.8
256
6.7
14
79.4%
1.6"
MALICIOUS DATASET ATTACK,0.3786008230452675,"can still examine the parameters, extract the intermediate steps, and break the assumption. Our
281"
MALICIOUS DATASET ATTACK,0.37962962962962965,"construction introduces a data point that changes its gradient based on the number of past iterations,
282"
MALICIOUS DATASET ATTACK,0.38065843621399176,"making it easy to identify if the point was present a given iteration of training. The rest of the
283"
MALICIOUS DATASET ATTACK,0.3816872427983539,"data points are maliciously selected to ensure the noise added during training doesn’t impact the
284"
MALICIOUS DATASET ATTACK,0.38271604938271603,"information stored in the model’s parameters. We defer the full details of the attack to Appendix C.
285"
MALICIOUS DATASET ATTACK,0.3837448559670782,"Figure 6 summarizes the results. As illustrated in the ﬁgure, this attack achieves a auditing lower
286"
MALICIOUS DATASET ATTACK,0.38477366255144035,"bound matching the standard DP-SGD analysis even in the last_iterate_only setting. As a result,
287"
MALICIOUS DATASET ATTACK,0.38580246913580246,"the attack exceeds our heuristic. However, this is a highly artiﬁcial example and it is unlikely to
288"
MALICIOUS DATASET ATTACK,0.3868312757201646,"reﬂect real-world scenarios.
289"
MALICIOUS DATASET ATTACK,0.38786008230452673,"100
101
102
103"
MALICIOUS DATASET ATTACK,0.3888888888888889,Number of steps 0 1 2 3 4 5 6 7 8
MALICIOUS DATASET ATTACK,0.389917695473251,ε at δ = 1e −05
MALICIOUS DATASET ATTACK,0.39094650205761317,"Heuristic
Standard
Empirical"
MALICIOUS DATASET ATTACK,0.39197530864197533,(a) q = 0.01
MALICIOUS DATASET ATTACK,0.39300411522633744,"100
101
102
103"
MALICIOUS DATASET ATTACK,0.3940329218106996,Number of steps 0 1 2 3 4 5 6 7 8
MALICIOUS DATASET ATTACK,0.3950617283950617,ε at δ = 1e −05
MALICIOUS DATASET ATTACK,0.39609053497942387,"Heuristic
Standard
Empirical"
MALICIOUS DATASET ATTACK,0.39711934156378603,(b) q = 0.1
MALICIOUS DATASET ATTACK,0.39814814814814814,"Figure 6: In this adversarial example, the attack encodes all training steps within the ﬁnal model
parameters, thereby violating the speciﬁc assumptions used to justify our heuristic analysis."
DISCUSSION & CONCLUSION,0.3991769547325103,"5
Discussion & Conclusion
290"
DISCUSSION & CONCLUSION,0.4002057613168724,"Both theoretical analysis and privacy auditing are valuable for understanding privacy leakage in
291"
DISCUSSION & CONCLUSION,0.4012345679012346,"machine learning, but each has limitations. Theoretical analysis is inherently conservative, while
292"
DISCUSSION & CONCLUSION,0.4022633744855967,"auditing procedures evaluate only speciﬁc attacks, and may thus underrepresent the privacy leakage.
293"
DISCUSSION & CONCLUSION,0.40329218106995884,"Our work introduces a novel heuristic analysis for DP-SGD that focuses on the privacy implications
294"
DISCUSSION & CONCLUSION,0.404320987654321,"of releasing only the ﬁnal model iterate. This approach is based in the empirical observation that
295"
DISCUSSION & CONCLUSION,0.4053497942386831,"linear loss functions accurately model the effectiveness of state of the art membership inference
296"
DISCUSSION & CONCLUSION,0.4063786008230453,"attacks. Our heuristic offers a practical and computationally efﬁcient way to estimate privacy leakage
297"
DISCUSSION & CONCLUSION,0.4074074074074074,"to complement privacy auditing and the standard composition-based analysis of DP-SGD. As shown
298"
DISCUSSION & CONCLUSION,0.40843621399176955,"in Table 1, we trained a series of CIFAR10 models with varying batch sizes that all achieved the
299"
DISCUSSION & CONCLUSION,0.4094650205761317,"similar level of heuristic epsilon, albeit with different standard epsilon values. Remarkably, these
300"
DISCUSSION & CONCLUSION,0.4104938271604938,"models exhibited similar performance and similar empirical epsilon values.
301"
DISCUSSION & CONCLUSION,0.411522633744856,"We also acknowledge the limitations of our heuristic by identifying speciﬁc counterexamples where
302"
DISCUSSION & CONCLUSION,0.4125514403292181,"the heuristic underestimates the true privacy leakage.
303"
REFERENCES,0.41358024691358025,"References
304"
REFERENCES,0.41460905349794236,"[ACGMMTZ16]
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar,
305"
REFERENCES,0.4156378600823045,"and L. Zhang. “Deep learning with differential privacy”. In: Proceedings of
306"
REFERENCES,0.4166666666666667,"the 2016 ACM SIGSAC conference on computer and communications security.
307"
REFERENCES,0.4176954732510288,"2016, pp. 308–318. URL: https://arxiv.org/abs/1607.00133 (cit. on
308"
REFERENCES,0.41872427983539096,"pp. 1, 3).
309"
REFERENCES,0.41975308641975306,"[AKOOMS23]
G. Andrew, P. Kairouz, S. Oh, A. Oprea, H. B. McMahan, and V. Suriyakumar.
310"
REFERENCES,0.4207818930041152,"“One-shot Empirical Privacy Estimation for Federated Learning”. In: arXiv
311"
REFERENCES,0.4218106995884774,"preprint arXiv:2302.03098 (2023). URL: https://arxiv.org/abs/2302.
312"
REFERENCES,0.4228395061728395,"03098 (cit. on p. 2).
313"
REFERENCES,0.42386831275720166,"[Ale09]
K. Alex. “Learning multiple layers of features from tiny images”. In: https:
314"
REFERENCES,0.42489711934156377,"// www. cs. toronto. edu/ kriz/ learning-features-2009-TR. pdf
315"
REFERENCES,0.42592592592592593,"(2009) (cit. on pp. 5, 20).
316"
REFERENCES,0.4269547325102881,"[AT22]
J. Altschuler and K. Talwar. “Privacy of noisy stochastic gradient descent:
317"
REFERENCES,0.4279835390946502,"More iterations without more privacy loss”. In: Advances in Neural Informa-
318"
REFERENCES,0.42901234567901236,"tion Processing Systems 35 (2022), pp. 3788–3800. URL: https://arxiv.
319"
REFERENCES,0.43004115226337447,"org/abs/2205.13710 (cit. on p. 2).
320"
REFERENCES,0.43106995884773663,"[AZT24]
M. Aerni, J. Zhang, and F. Tramèr. “Evaluations of Machine Learning Privacy
321"
REFERENCES,0.43209876543209874,"Defenses are Misleading”. In: arXiv preprint arXiv:2404.17399 (2024). URL:
322"
REFERENCES,0.4331275720164609,"https://arxiv.org/abs/2404.17399 (cit. on p. 2).
323"
REFERENCES,0.43415637860082307,"[BGDCTV18]
B. Bichsel, T. Gehr, D. Drachsler-Cohen, P. Tsankov, and M. Vechev. “Dp-
324"
REFERENCES,0.4351851851851852,"ﬁnder: Finding differential privacy violations by sampling and optimization”.
325"
REFERENCES,0.43621399176954734,"In: Proceedings of the 2018 ACM SIGSAC Conference on Computer and
326"
REFERENCES,0.43724279835390945,"Communications Security. 2018, pp. 508–524 (cit. on p. 2).
327"
REFERENCES,0.4382716049382716,"[BSA24]
J. Bok, W. Su, and J. M. Altschuler. “Shifted Interpolation for Differential
328"
REFERENCES,0.43930041152263377,"Privacy”. In: arXiv preprint arXiv:2403.00278 (2024). URL: https://arxiv.
329"
REFERENCES,0.4403292181069959,"org/abs/2403.00278 (cit. on p. 2).
330"
REFERENCES,0.44135802469135804,"[BST14]
R. Bassily, A. Smith, and A. Thakurta. “Private empirical risk minimization:
331"
REFERENCES,0.44238683127572015,"Efﬁcient algorithms and tight error bounds”. In: 2014 IEEE 55th annual
332"
REFERENCES,0.4434156378600823,"symposium on foundations of computer science. IEEE. 2014, pp. 464–473.
333"
REFERENCES,0.4444444444444444,"URL: https://arxiv.org/abs/1405.7085 (cit. on pp. 1, 3).
334"
REFERENCES,0.4454732510288066,"[BTRKMW24]
M. Bertran, S. Tang, A. Roth, M. Kearns, J. H. Morgenstern, and S. Z. Wu.
335"
REFERENCES,0.44650205761316875,"“Scalable membership inference attacks via quantile regression”. In: Advances
336"
REFERENCES,0.44753086419753085,"in Neural Information Processing Systems 36 (2024). URL: https://arxiv.
337"
REFERENCES,0.448559670781893,"org/abs/2307.03694 (cit. on p. 2).
338"
REFERENCES,0.4495884773662551,"[CCGST24]
C. A. Choquette-Choo, A. Ganesh, T. Steinke, and A. Thakurta. Privacy
339"
REFERENCES,0.4506172839506173,"Ampliﬁcation for Matrix Mechanisms. 2024. arXiv: 2310.15526 [cs.LG]
340"
REFERENCES,0.45164609053497945,"(cit. on p. 7).
341"
REFERENCES,0.45267489711934156,"[CCNSTT22]
N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer. “Membership
342"
REFERENCES,0.4537037037037037,"inference attacks from ﬁrst principles”. In: 2022 IEEE Symposium on Security
343"
REFERENCES,0.4547325102880658,"and Privacy (SP). IEEE. 2022, pp. 1897–1914. URL: https://arxiv.org/
344"
REFERENCES,0.455761316872428,"abs/2112.03570 (cit. on p. 2).
345"
REFERENCES,0.4567901234567901,"[CYS21]
R. Chourasia, J. Ye, and R. Shokri. “Differential privacy dynamics of langevin
346"
REFERENCES,0.45781893004115226,"diffusion and noisy gradient descent”. In: Advances in Neural Information
347"
REFERENCES,0.4588477366255144,"Processing Systems 34 (2021), pp. 14771–14781. URL: https://arxiv.
348"
REFERENCES,0.45987654320987653,"org/abs/2102.05855 (cit. on p. 2).
349"
REFERENCES,0.4609053497942387,"[DBHSB22]
S. De, L. Berrada, J. Hayes, S. L. Smith, and B. Balle. “Unlocking high-
350"
REFERENCES,0.4619341563786008,"accuracy differentially private image classiﬁcation through scale”. In: arXiv
351"
REFERENCES,0.46296296296296297,"preprint arXiv:2204.13650 (2022) (cit. on pp. 5, 9).
352"
REFERENCES,0.46399176954732513,"[DMNS06]
C. Dwork, F. McSherry, K. Nissim, and A. Smith. “Calibrating noise to
353"
REFERENCES,0.46502057613168724,"sensitivity in private data analysis”. In: Theory of Cryptography: Third Theory
354"
REFERENCES,0.4660493827160494,"of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006.
355"
REFERENCES,0.4670781893004115,"Proceedings 3. Springer. 2006, pp. 265–284. URL: https://www.iacr.
356"
REFERENCES,0.46810699588477367,"org/archive/tcc2006/38760266/38760266.pdf (cit. on p. 1).
357"
REFERENCES,0.4691358024691358,"[DRS19]
J. Dong, A. Roth, and W. J. Su. “Gaussian differential privacy”. In: arXiv
358"
REFERENCES,0.47016460905349794,"preprint arXiv:1905.02383 (2019) (cit. on p. 13).
359"
REFERENCES,0.4711934156378601,"[DSSUV15]
C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan. “Robust traceability
360"
REFERENCES,0.4722222222222222,"from trace amounts”. In: 2015 IEEE 56th Annual Symposium on Foundations
361"
REFERENCES,0.4732510288065844,"of Computer Science. IEEE. 2015, pp. 650–669 (cit. on p. 2).
362"
REFERENCES,0.4742798353909465,"[DWWZK18]
Z. Ding, Y. Wang, G. Wang, D. Zhang, and D. Kifer. “Detecting violations of
363"
REFERENCES,0.47530864197530864,"differential privacy”. In: Proceedings of the 2018 ACM SIGSAC Conference
364"
REFERENCES,0.4763374485596708,"on Computer and Communications Security. 2018, pp. 475–489 (cit. on p. 2).
365"
REFERENCES,0.4773662551440329,"[FMTT18]
V. Feldman, I. Mironov, K. Talwar, and A. Thakurta. “Privacy ampliﬁcation
366"
REFERENCES,0.4783950617283951,"by iteration”. In: 2018 IEEE 59th Annual Symposium on Foundations of
367"
REFERENCES,0.4794238683127572,"Computer Science (FOCS). IEEE. 2018, pp. 521–532. URL: https://arxiv.
368"
REFERENCES,0.48045267489711935,"org/abs/1808.06651 (cit. on p. 2).
369"
REFERENCES,0.48148148148148145,"[Goo20]
Google. Differential Privacy Accounting. https://github.com/google/
370"
REFERENCES,0.4825102880658436,"differential-privacy/tree/main/python/dp_accounting. 2020
371"
REFERENCES,0.4835390946502058,"(cit. on pp. 3, 4).
372"
REFERENCES,0.4845679012345679,"[HSRDTMPSNC08]
N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling,
373"
REFERENCES,0.48559670781893005,"J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. “Resolving
374"
REFERENCES,0.48662551440329216,"individuals contributing trace amounts of DNA to highly complex mixtures
375"
REFERENCES,0.4876543209876543,"using high-density SNP genotyping microarrays”. In: PLoS genetics 4.8
376"
REFERENCES,0.4886831275720165,"(2008), e1000167 (cit. on p. 2).
377"
REFERENCES,0.4897119341563786,"[JUO20]
M. Jagielski, J. Ullman, and A. Oprea. “Auditing differentially private ma-
378"
REFERENCES,0.49074074074074076,"chine learning: How private is private sgd?” In: Advances in Neural Informa-
379"
REFERENCES,0.49176954732510286,"tion Processing Systems 33 (2020), pp. 22205–22216 (cit. on p. 2).
380"
REFERENCES,0.492798353909465,"[KJH20]
A. Koskela, J. Jälkö, and A. Honkela. “Computing tight differential privacy
381"
REFERENCES,0.49382716049382713,"guarantees using fft”. In: International Conference on Artiﬁcial Intelligence
382"
REFERENCES,0.4948559670781893,"and Statistics. PMLR. 2020, pp. 2560–2569. URL: https://arxiv.org/
383"
REFERENCES,0.49588477366255146,"abs/1906.03049 (cit. on p. 1).
384"
REFERENCES,0.49691358024691357,"[LF20]
K. Leino and M. Fredrikson. “Stolen memories: Leveraging model memoriza-
385"
REFERENCES,0.49794238683127573,"tion for calibrated {White-Box} membership inference”. In: 29th USENIX se-
386"
REFERENCES,0.49897119341563784,"curity symposium (USENIX Security 20). 2020, pp. 1605–1622. URL: https:
387"
REFERENCES,0.5,"//arxiv.org/abs/1906.11798 (cit. on p. 2).
388"
REFERENCES,0.5010288065843621,"[Mir17]
I. Mironov. “Rényi differential privacy”. In: 2017 IEEE 30th computer secu-
389"
REFERENCES,0.5020576131687243,"rity foundations symposium (CSF). IEEE. 2017, pp. 263–275. URL: https:
390"
REFERENCES,0.5030864197530864,"//arxiv.org/abs/1702.07476 (cit. on p. 1).
391"
REFERENCES,0.5041152263374485,"[NHSBTJCT23]
M. Nasr, J. Hayes, T. Steinke, B. Balle, F. Tramèr, M. Jagielski, N. Carlini,
392"
REFERENCES,0.5051440329218106,"and A. Terzis. “Tight Auditing of Differentially Private Machine Learning”.
393"
REFERENCES,0.5061728395061729,"In: arXiv preprint arXiv:2302.07956 (2023). URL: https://arxiv.org/
394"
REFERENCES,0.507201646090535,"abs/2302.07956 (cit. on pp. 1, 2, 5, 6).
395"
REFERENCES,0.5082304526748971,"[NSTPC21]
M. Nasr, S. Song, A. Thakurta, N. Papernot, and N. Carlini. “Adversary
396"
REFERENCES,0.5092592592592593,"instantiation: Lower bounds for differentially private machine learning”. In:
397"
REFERENCES,0.5102880658436214,"2021 IEEE Symposium on security and privacy (SP). IEEE. 2021, pp. 866–
398"
REFERENCES,0.5113168724279835,"882. URL: https://arxiv.org/abs/2101.04535 (cit. on pp. 1, 2, 5, 6).
399"
REFERENCES,0.5123456790123457,"[SDSOJ19]
A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. Jégou. “White-
400"
REFERENCES,0.5133744855967078,"box vs black-box: Bayes optimal strategies for membership inference”. In:
401"
REFERENCES,0.51440329218107,"International Conference on Machine Learning. PMLR. 2019, pp. 5558–5567.
402"
REFERENCES,0.5154320987654321,"URL: https://arxiv.org/abs/1908.11229 (cit. on p. 2).
403"
REFERENCES,0.5164609053497943,"[SNJ23]
T. Steinke, M. Nasr, and M. Jagielski. “Privacy auditing with one (1) training
404"
REFERENCES,0.5174897119341564,"run”. In: Advances in Neural Information Processing Systems 36 (2023). URL:
405"
REFERENCES,0.5185185185185185,"https://arxiv.org/abs/2305.08846 (cit. on pp. 2, 6).
406"
REFERENCES,0.5195473251028807,"[SOJH09]
S. Sankararaman, G. Obozinski, M. I. Jordan, and E. Halperin. “Genomic
407"
REFERENCES,0.5205761316872428,"privacy and limits of individual detection in a pool”. In: Nature genetics 41.9
408"
REFERENCES,0.5216049382716049,"(2009), pp. 965–967 (cit. on p. 2).
409"
REFERENCES,0.522633744855967,"[SSSS17]
R. Shokri, M. Stronati, C. Song, and V. Shmatikov. “Membership inference
410"
REFERENCES,0.5236625514403292,"attacks against machine learning models”. In: 2017 IEEE symposium on
411"
REFERENCES,0.5246913580246914,"security and privacy (SP). IEEE. 2017, pp. 3–18 (cit. on p. 2).
412"
REFERENCES,0.5257201646090535,"[Ste22]
T. Steinke. “Composition of Differential Privacy & Privacy Ampliﬁcation
413"
REFERENCES,0.5267489711934157,"by Subsampling”. In: arXiv preprint arXiv:2210.00597 (2022). URL: https:
414"
REFERENCES,0.5277777777777778,"//arxiv.org/abs/2210.00597 (cit. on p. 1).
415"
REFERENCES,0.5288065843621399,"[TTSSJC22]
F. Tramer, A. Terzis, T. Steinke, S. Song, M. Jagielski, and N. Carlini. “Debug-
416"
REFERENCES,0.529835390946502,"ging differential privacy: A case study for privacy auditing”. In: arXiv preprint
417"
REFERENCES,0.5308641975308642,"arXiv:2202.12219 (2022). URL: https://arxiv.org/abs/2202.12219
418"
REFERENCES,0.5318930041152263,"(cit. on p. 2).
419"
REFERENCES,0.5329218106995884,"[WBKBGGG23]
Y. Wen, A. Bansal, H. Kazemi, E. Borgnia, M. Goldblum, J. Geiping, and
420"
REFERENCES,0.5339506172839507,"T. Goldstein. “Canary in a coalmine: Better membership inference with en-
421"
REFERENCES,0.5349794238683128,"sembled adversarial queries”. In: ICLR. 2023. URL: https://arxiv.org/
422"
REFERENCES,0.5360082304526749,"abs/2210.10750 (cit. on p. 2).
423"
REFERENCES,0.5370370370370371,"[YS22]
J. Ye and R. Shokri. “Differentially private learning needs hidden state (or
424"
REFERENCES,0.5380658436213992,"much faster convergence)”. In: Advances in Neural Information Processing
425"
REFERENCES,0.5390946502057613,"Systems 35 (2022), pp. 703–715. URL: https://arxiv.org/abs/2203.
426"
REFERENCES,0.5401234567901234,"05363 (cit. on p. 2).
427"
REFERENCES,0.5411522633744856,"[ZBWTSRPNK22]
S. Zanella-Béguelin, L. Wutschitz, S. Tople, A. Salem, V. Rühle, A. Paverd,
428"
REFERENCES,0.5421810699588477,"M. Naseri, and B. Köpf. “Bayesian estimation of differential privacy”. In:
429"
REFERENCES,0.5432098765432098,"arXiv preprint arXiv:2206.05199 (2022) (cit. on p. 2).
430"
REFERENCES,0.5442386831275721,"[ZK16]
S. Zagoruyko and N. Komodakis. “Wide residual networks”. In: arXiv preprint
431"
REFERENCES,0.5452674897119342,"arXiv:1605.07146 (2016) (cit. on pp. 5, 20).
432"
REFERENCES,0.5462962962962963,"[ZLS23]
S. Zarifzadeh, P. C.-J. M. Liu, and R. Shokri. “Low-Cost High-Power Mem-
433"
REFERENCES,0.5473251028806584,"bership Inference by Boosting Relativity”. In: (2023). URL: https://arxiv.
434"
REFERENCES,0.5483539094650206,"org/abs/2312.03262 (cit. on p. 2).
435"
REFERENCES,0.5493827160493827,"A
Proof of Theorem 1
436"
REFERENCES,0.5504115226337448,"Proof. Let xi∗be the canary, let D be the dataset with the canary and D′ be the dataset without the
437"
REFERENCES,0.551440329218107,"canary. Since ℓand r are linear, wlog we can assume r = 0 and ∇mt−1ℓ(mt−1, xi) = vi for some
438"
REFERENCES,0.5524691358024691,"set of vectors {vi}, such that ∥vi∥2 ≤1. We can also assume wlog ∥vi∗∥= 1 since, if ∥vi∗∥< 1,
439"
REFERENCES,0.5534979423868313,"the ﬁnal privacy guarantee we show only improves.
440"
REFERENCES,0.5545267489711934,"We have the following recursion for mt:
441"
REFERENCES,0.5555555555555556,mt = mt−1 −η X
REFERENCES,0.5565843621399177,"i∈Bt
vi + ξt !"
REFERENCES,0.5576131687242798,",
ξt
i.i.d
∼N(0, σ2Id)."
REFERENCES,0.558641975308642,"Unrolling the recursion:
442"
REFERENCES,0.5596707818930041,mt = m0 −η  X
REFERENCES,0.5606995884773662,t∈[T ] X
REFERENCES,0.5617283950617284,"i∈Bt
vi + ξ "
REFERENCES,0.5627572016460906,",
ξ ∼N(0, Tσ2Id)."
REFERENCES,0.5637860082304527,"By the post-processing property of DP, we can assume that in addition to the ﬁnal model mT , we
443"
REFERENCES,0.5648148148148148,"release m0 and {Bt \ {xi∗}}t∈[T ], that is we release all examples that were sampled in each batch
444"
REFERENCES,0.565843621399177,"except for the canary. The following f is a bijection, computable by an adversary using the released
445"
REFERENCES,0.5668724279835391,"information:
446"
REFERENCES,0.5679012345679012,f(mT ) := − 
REFERENCES,0.5689300411522634,"mT −m0 η
−
X"
REFERENCES,0.5699588477366255,t∈[T ] X
REFERENCES,0.5709876543209876,"i∈Bt\{xi∗}
vi.  "
REFERENCES,0.5720164609053497,"Since f is a bijection, distinguishing mT sampled using D and D′ is equivalent to distinguishing
447"
REFERENCES,0.573045267489712,"f(mT ) instead. Now we have f(mT ) = N(0, Tσ2Id) for D′, and f(mT ) = N(0, Tσ2Id) + kvi∗,
448"
REFERENCES,0.5740740740740741,"k ∼Binomial(T, q). For any vector u orthogonal to vi∗, by isotropy of the Gaussian distribution the
449"
REFERENCES,0.5751028806584362,"distribution of ⟨f(mT ), u⟩is the same for both D and D′ and independent of ⟨f(mT ), vi∗⟩, hence
450"
REFERENCES,0.5761316872427984,"distinguishing f(mT ) given D and D′ is the same as distingushing ⟨f(mT ), vi∗⟩given D and D′.
451"
REFERENCES,0.5771604938271605,"Finally, the distribution of ⟨f(mT ), vi∗⟩is exactly P for D and exactly Q for D′. By post-processing,
452"
REFERENCES,0.5781893004115226,"this gives the theorem.
453"
REFERENCES,0.5792181069958847,"We can also see that the function δT,q,σ is tight (i.e., even if we do not release Bt \ {xi∗}), by
454"
REFERENCES,0.5802469135802469,"considering the 1-dimensional setting, where vi = 0 for i ̸= i∗and vi∗= −1, η = 1, m0 = 0. Then,
455"
REFERENCES,0.581275720164609,"the distribution of mT given D is exactly P, and given D′ is exactly Q.
456"
REFERENCES,0.5823045267489712,"A.1
Computing δ from ε
457"
REFERENCES,0.5833333333333334,"Here, we give an efﬁciently computable expression for the function δT,q,σ(ε). Using P, Q as in
458"
REFERENCES,0.5843621399176955,"Theorem 1, let f(y) be the privacy loss for the output y:
459"
REFERENCES,0.5853909465020576,"f(y) = log
P(y) Q(y)"
REFERENCES,0.5864197530864198,"
= log T
X k=0 T
k"
REFERENCES,0.5874485596707819,"
qk(1 −q)n−k exp(−(y −k)2/2Tσ2)"
REFERENCES,0.588477366255144,"exp(−y2/2Tσ2) ! = log T
X k=0 T
k"
REFERENCES,0.5895061728395061,"
qk(1 −q)k exp
2ky −k2 2Tσ2 ! ."
REFERENCES,0.5905349794238683,"Then for any ε, using the fact that S = {y : f(y) ≥ε} maximizes P(S) −eεQ(S), we have:
460"
REFERENCES,0.5915637860082305,"Heε(P, Q) = P({y : f(y) ≥ε}) −eεQ({y : f(y) ≥ε})"
REFERENCES,0.5925925925925926,"= P({y : y ≥f −1(ε)}) −eεQ({y : y ≥f −1(ε)}) = T
X k=0 T
k"
REFERENCES,0.5936213991769548,"
qk(1 −q)k Pr[N(k, Tσ2) ≥f −1(ε)] −eε Pr[N(0, Tσ2) ≥f −1(ε)]."
REFERENCES,0.5946502057613169,"Similarly, S = {y : f(y) ≤−ε} maximizes Q(S) −eεP(S) so we have:
461"
REFERENCES,0.595679012345679,"Heε(Q, P) = Q({y : f(y) ≤−ε}) −eεP({y : f(y) ≤−ε})"
REFERENCES,0.5967078189300411,= Q({y : y ≤f −1(−ε)}) −eεP({y : y ≤f −1(−ε)})
REFERENCES,0.5977366255144033,"= Pr[N(0, Tσ2) ≤f −1(−ε)] −eε
T
X k=0 T
k"
REFERENCES,0.5987654320987654,"
qk(1 −q)k Pr[N(k, Tσ2) ≤f −1(−ε)]."
REFERENCES,0.5997942386831275,"These expressions can be evaluated efﬁciently. Since f is monotone, it can be inverted via binary
462"
REFERENCES,0.6008230452674898,"search. We can also use binary search to evaluate ε as a function of δ.
463"
REFERENCES,0.6018518518518519,"B
Linear Worst Case for Full Batch Setting
464"
REFERENCES,0.602880658436214,"It turns out that in the full-batch setting,
the worst-case analyses of DP-GD with
465"
REFERENCES,0.6039094650205762,"intermediate_iterates and with last_iterate_only are the same. This phenomenon arises
466"
REFERENCES,0.6049382716049383,"because there is no subsampling (because q = 1 in Algorithm 1) and thus the algorithm is “just”
467"
REFERENCES,0.6059670781893004,"the Gaussian mechanism. Intuitively, DP-GD with intermediate_iterates corresponds to T
468"
REFERENCES,0.6069958847736625,"calls to the Gaussian mechanism with noise multiplier σ, while DP-GD with last_iterate_only
469"
REFERENCES,0.6080246913580247,"corresponds to one call to the Gaussian mechanism with noise multiplier σ/
√"
REFERENCES,0.6090534979423868,"T; these are equivalent
470"
REFERENCES,0.6100823045267489,"by the properties of the Gaussian distribution.
471"
REFERENCES,0.6111111111111112,"We can formalize this using the language of Gaussian DP [DRS19]: DP-GD (Algorithm 1 with
472"
REFERENCES,0.6121399176954733,"q = 1) satisﬁes
√"
REFERENCES,0.6131687242798354,"T/σ-GDP. (Each iteration satisﬁes 1/σ-GDP and adaptive composition implies
473"
REFERENCES,0.6141975308641975,"the overall guarantee.) This means that the privacy loss is exactly dominated by that of the Gaussian
474"
REFERENCES,0.6152263374485597,"mechanism with noise multiplier σ/
√"
REFERENCES,0.6162551440329218,"T. Linear losses give an example such that DP-GD with
475"
REFERENCES,0.6172839506172839,"last_iterate_only has exactly this privacy loss, since the ﬁnal iterate reveals the sum of all the
476"
REFERENCES,0.6183127572016461,"noisy gradient estimates. The worst-case privacy of DP-GD with intermediate_iterates is no
477"
REFERENCES,0.6193415637860082,"worse than that of DP-GD with last_iterate_only. The reverse is also true (by postprocessing).
478"
REFERENCES,0.6203703703703703,"In more detail: For T iterations of (full-batch) DP-GD on a linear losses, if the losses are (wlog)
479"
REFERENCES,0.6213991769547325,"1-Lipschitz and we add noise N(0, σ2"
REFERENCES,0.6224279835390947,"n2 · I) to the gradient in every round, distinguishing the last
480"
REFERENCES,0.6234567901234568,"iterate of DP-SGD on adjacent databases is equivalent to distinguishing N(0, Tσ2) and N(T, Tσ2).
481"
REFERENCES,0.6244855967078189,"This can be seen as a special case of Theorem 1 for p = 1, so we do not a give a detailed argument
482"
REFERENCES,0.6255144032921811,"here.
483"
REFERENCES,0.6265432098765432,"If instead we are given every iteration mt, for any 1-Lipschitz loss, distinguishing the joint distribu-
484"
REFERENCES,0.6275720164609053,"tions of mt given mt−1 on adjacent databases is equivalent to distinguishing N(0, σ2) and N(1, σ2).
485"
REFERENCES,0.6286008230452675,"In turn, distinguishing the distribution of all iterates on adjacent databases is equivalent to distin-
486"
REFERENCES,0.6296296296296297,"guishing N(0T , σ2IT ) and N(1T , σ2IT ), where 0T and 1T are the all-zeros and all-ones vectors in
487"
REFERENCES,0.6306584362139918,"RT . Because the Gaussian distribution is isotropic, distinguishing N(0T , σ2IT ) and N(1T , σ2IT ) is
488"
REFERENCES,0.6316872427983539,"equivalent to distinguishing ⟨x, 1T ⟩where x ∼N(0T , σ2IT ) and ⟨x, 1T ⟩where x ∼N(1T , σ2IT ).
489"
REFERENCES,0.6327160493827161,"These distributions are N(0, Tσ2) and N(T, Tσ2), the exact pair of distributions we reduced to for
490"
REFERENCES,0.6337448559670782,"last-iterate analysis of linear losses.
491"
REFERENCES,0.6347736625514403,"C
Malicious Dataset Attack Details
492"
REFERENCES,0.6358024691358025,"Algorithms 2, 3, and 4 summarizes the construction for the attack. The attack assume the model
493"
REFERENCES,0.6368312757201646,"parameters have dimension equal to the number of iterations. It also assumes each data point can
494"
REFERENCES,0.6378600823045267,"reference which iteration of training is currently happening (this can be implemented by having
495"
REFERENCES,0.6388888888888888,"a single model parameter which increments in each step, independently of the training examples,
496"
REFERENCES,0.6399176954732511,"without impacting the privacy of the training process). Then we build our two datasets D and
497"
REFERENCES,0.6409465020576132,"D′ = D ∪{x} so that all points in dataset D (“repeaters”) run Algorithm 3 to compute gradients
498"
REFERENCES,0.6419753086419753,"and the canary point in D′ runs Algorithm 2 to compute its gradient. Our attack relies heavily on
499"
REFERENCES,0.6430041152263375,"DP-SGD’s lack of assumptions on the data distribution and any speciﬁc properties of the model or
500"
REFERENCES,0.6440329218106996,"gradients. Algorithm 2, which generates the canary data point, is straightforward. Its goal is to store
501"
REFERENCES,0.6450617283950617,"in the model parameters whether it was present in iteration i by outputting a gradient that changes
502"
REFERENCES,0.6460905349794238,"only the i-th index of the model parameters by 1 (assuming a clipping threshold of 1).
503"
REFERENCES,0.647119341563786,"All other data points, the “repeaters”, are present in both datasets (D and D′), and have three tasks:
504"
REFERENCES,0.6481481481481481,"• Cancel out any noise added to the model parameters at an index larger than the current
505"
REFERENCES,0.6491769547325102,"iteration. At iteration i, their gradients for parameters from index i onward will be the same
506"
REFERENCES,0.6502057613168725,"as the current value of the model parameter, scaled by the batch size and the learning rate to
507"
REFERENCES,0.6512345679012346,"ensure this parameter value will be 0 after the update.
508"
REFERENCES,0.6522633744855967,"• Evaluate whether the canary point was present in the previous iteration by comparing
509"
REFERENCES,0.6532921810699589,"the model parameter at index i −1 with a threshold, and rewrite the value of that model
510"
REFERENCES,0.654320987654321,"parameter to a large value if the canary was present.
511"
REFERENCES,0.6553497942386831,"• Ensure that all previous decisions are not overwritten by noise by continuing to rewrite them
512"
REFERENCES,0.6563786008230452,"with a large value based on their previous value.
513"
REFERENCES,0.6574074074074074,"To achieve all of these goals simultaneously, we require that the batch size is large enough that the
514"
REFERENCES,0.6584362139917695,"repeaters’ updates are not clipped.
515"
REFERENCES,0.6594650205761317,"Finally Algorithm 4 runs DP-SGD, with repeater points computing gradients with Algorithm 3 and
516"
REFERENCES,0.6604938271604939,"the canary point, sampled with probability p, computing its gradient using Algorithm 2. In our
517"
REFERENCES,0.661522633744856,"experiments we run Algorithm 4 100,000 times. And to evaluate if the model parameters was from
518"
REFERENCES,0.6625514403292181,"dataset D or D′ we run a hypothesis test on the values of the model parameters. All constants are
519"
REFERENCES,0.6635802469135802,"chosen to ensure all objectives of the repeaters are satisﬁed.
520"
REFERENCES,0.6646090534979424,Algorithm 2 Canary data point
REFERENCES,0.6656378600823045,"1: function ADV(x, i)
2:
Initialize a as a zero vector of the same dimension as x
3:
Set ai ←1
▷Set the i-th component to 1
4:
return −a
5: end function"
REFERENCES,0.6666666666666666,Algorithm 3 Additional data points
REFERENCES,0.6676954732510288,"Require: model parameters x, iteration number i, batch size N, learning rate η, previous history
threshold tpast, last iteration threshold tlast, history ampliﬁcation value BIG_VAL
1: function REPEATERS(x, i, N, η, tpast, tlast, BIG_VAL)
2:
h ←x0:i
▷Parameter “history” up to iteration i, not inclusive
3:
f ←xi:end
▷Future and current parameters, starting from iteration i
4:
f ←−f/(η · N)
▷Remove noise from last iteration
5:
base_history ←−x0:i/(η · N)
▷By default, zero out entire history
6:
if length(h) > 1 then
7:
h0:i−1 ←BIG_VAL/(η · N) · (21[h0:i−1 ≥tpast] −1)
▷If an old iteration is large
enough, it was a canary iteration, so amplify it
8:
end if
9:
if length(h) > 0 then
10:
hi−1 ←BIG_VAL/(η · N) · (21[hi ≥tlast] −1) ▷If the last iteration is large enough, it
was a canary iteration, so amplify it
11:
end if
12:
h ←h + base_history
▷Don’t zero out canary iterations
13:
a ←concatenate(h, f)
14:
return −a
15: end function"
REFERENCES,0.668724279835391,Algorithm 4 Encoding Attacking
REFERENCES,0.6697530864197531,"Require: add-diff, whether to add the canary, batch size N, sampling rate p, learning rate (η),
iteration count/parameter count D
1: function RUN_DPSGD(add-diff)
2:
C ←1
3:
Initialize model m ←0 of dimension D
4:
for i = 0 to D do
5:
Generate a uniform random value q ∈[0, 1]
6:
r ←repeaters(m, i)
7:
Compute norm c ←||r||
8:
if c > 0 then
9:
Normalize r ←r/ max(c, C)
10:
end if
11:
Adjusted vector z ←r × N
12:
Verify condition on mi
13:
if p ≤q and add-diff then
14:
r ←adv(m, i)
15:
Normalize and update z
16:
end if
17:
Apply Gaussian noise to z
18:
Update model m ←m −z × η
19:
end for
20:
return m
21: end function"
REFERENCES,0.6707818930041153,"NeurIPS Paper Checklist
521"
CLAIMS,0.6718106995884774,"1. Claims
522"
CLAIMS,0.6728395061728395,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
523"
CLAIMS,0.6738683127572016,"paper’s contributions and scope?
524"
CLAIMS,0.6748971193415638,"Answer: [Yes]
525"
CLAIMS,0.6759259259259259,"Justiﬁcation: The abstract and introduction state what we do and then the following sections
526"
CLAIMS,0.676954732510288,"and the appendix provide details.
527"
CLAIMS,0.6779835390946503,"Guidelines:
528"
CLAIMS,0.6790123456790124,"• The answer NA means that the abstract and introduction do not include the claims
529"
CLAIMS,0.6800411522633745,"made in the paper.
530"
CLAIMS,0.6810699588477366,"• The abstract and/or introduction should clearly state the claims made, including the
531"
CLAIMS,0.6820987654320988,"contributions made in the paper and important assumptions and limitations. A No or
532"
CLAIMS,0.6831275720164609,"NA answer to this question will not be perceived well by the reviewers.
533"
CLAIMS,0.684156378600823,"• The claims made should match theoretical and experimental results, and reﬂect how
534"
CLAIMS,0.6851851851851852,"much the results can be expected to generalize to other settings.
535"
CLAIMS,0.6862139917695473,"• It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals
536"
CLAIMS,0.6872427983539094,"are not attained by the paper.
537"
LIMITATIONS,0.6882716049382716,"2. Limitations
538"
LIMITATIONS,0.6893004115226338,"Question: Does the paper discuss the limitations of the work performed by the authors?
539"
LIMITATIONS,0.6903292181069959,"Answer: [Yes]
540"
LIMITATIONS,0.691358024691358,"Justiﬁcation: Section 4 discusses the limitations.
541"
LIMITATIONS,0.6923868312757202,"Guidelines:
542"
LIMITATIONS,0.6934156378600823,"• The answer NA means that the paper has no limitation while the answer No means that
543"
LIMITATIONS,0.6944444444444444,"the paper has limitations, but those are not discussed in the paper.
544"
LIMITATIONS,0.6954732510288066,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
545"
LIMITATIONS,0.6965020576131687,"• The paper should point out any strong assumptions and how robust the results are to
546"
LIMITATIONS,0.6975308641975309,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
547"
LIMITATIONS,0.698559670781893,"model well-speciﬁcation, asymptotic approximations only holding locally). The authors
548"
LIMITATIONS,0.6995884773662552,"should reﬂect on how these assumptions might be violated in practice and what the
549"
LIMITATIONS,0.7006172839506173,"implications would be.
550"
LIMITATIONS,0.7016460905349794,"• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
551"
LIMITATIONS,0.7026748971193416,"only tested on a few datasets or with a few runs. In general, empirical results often
552"
LIMITATIONS,0.7037037037037037,"depend on implicit assumptions, which should be articulated.
553"
LIMITATIONS,0.7047325102880658,"• The authors should reﬂect on the factors that inﬂuence the performance of the approach.
554"
LIMITATIONS,0.7057613168724279,"For example, a facial recognition algorithm may perform poorly when image resolution
555"
LIMITATIONS,0.7067901234567902,"is low or images are taken in low lighting. Or a speech-to-text system might not be
556"
LIMITATIONS,0.7078189300411523,"used reliably to provide closed captions for online lectures because it fails to handle
557"
LIMITATIONS,0.7088477366255144,"technical jargon.
558"
LIMITATIONS,0.7098765432098766,"• The authors should discuss the computational efﬁciency of the proposed algorithms
559"
LIMITATIONS,0.7109053497942387,"and how they scale with dataset size.
560"
LIMITATIONS,0.7119341563786008,"• If applicable, the authors should discuss possible limitations of their approach to
561"
LIMITATIONS,0.7129629629629629,"address problems of privacy and fairness.
562"
LIMITATIONS,0.7139917695473251,"• While the authors might fear that complete honesty about limitations might be used by
563"
LIMITATIONS,0.7150205761316872,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
564"
LIMITATIONS,0.7160493827160493,"limitations that aren’t acknowledged in the paper. The authors should use their best
565"
LIMITATIONS,0.7170781893004116,"judgment and recognize that individual actions in favor of transparency play an impor-
566"
LIMITATIONS,0.7181069958847737,"tant role in developing norms that preserve the integrity of the community. Reviewers
567"
LIMITATIONS,0.7191358024691358,"will be speciﬁcally instructed to not penalize honesty concerning limitations.
568"
THEORY ASSUMPTIONS AND PROOFS,0.720164609053498,"3. Theory Assumptions and Proofs
569"
THEORY ASSUMPTIONS AND PROOFS,0.7211934156378601,"Question: For each theoretical result, does the paper provide the full set of assumptions and
570"
THEORY ASSUMPTIONS AND PROOFS,0.7222222222222222,"a complete (and correct) proof?
571"
THEORY ASSUMPTIONS AND PROOFS,0.7232510288065843,"Answer: [Yes]
572"
THEORY ASSUMPTIONS AND PROOFS,0.7242798353909465,"Justiﬁcation: Theorem 1 is proved in Appendix A.
573"
THEORY ASSUMPTIONS AND PROOFS,0.7253086419753086,"Guidelines:
574"
THEORY ASSUMPTIONS AND PROOFS,0.7263374485596708,"• The answer NA means that the paper does not include theoretical results.
575"
THEORY ASSUMPTIONS AND PROOFS,0.727366255144033,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
576"
THEORY ASSUMPTIONS AND PROOFS,0.7283950617283951,"referenced.
577"
THEORY ASSUMPTIONS AND PROOFS,0.7294238683127572,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
578"
THEORY ASSUMPTIONS AND PROOFS,0.7304526748971193,"• The proofs can either appear in the main paper or the supplemental material, but if
579"
THEORY ASSUMPTIONS AND PROOFS,0.7314814814814815,"they appear in the supplemental material, the authors are encouraged to provide a short
580"
THEORY ASSUMPTIONS AND PROOFS,0.7325102880658436,"proof sketch to provide intuition.
581"
THEORY ASSUMPTIONS AND PROOFS,0.7335390946502057,"• Inversely, any informal proof provided in the core of the paper should be complemented
582"
THEORY ASSUMPTIONS AND PROOFS,0.7345679012345679,"by formal proofs provided in appendix or supplemental material.
583"
THEORY ASSUMPTIONS AND PROOFS,0.73559670781893,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7366255144032922,"4. Experimental Result Reproducibility
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7376543209876543,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7386831275720165,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7397119341563786,"of the paper (regardless of whether the code and data are provided or not)?
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7407407407407407,"Answer: [Yes]
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7417695473251029,"Justiﬁcation: The setup is described for each experiment we conduct and we reference prior
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.742798353909465,"work that these build on.
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7438271604938271,"Guidelines:
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7448559670781894,"• The answer NA means that the paper does not include experiments.
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7458847736625515,"• If the paper includes experiments, a No answer to this question will not be perceived
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7469135802469136,"well by the reviewers: Making the paper reproducible is important, regardless of
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7479423868312757,"whether the code and data are provided or not.
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7489711934156379,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"to make their results reproducible or veriﬁable.
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7510288065843621,"• Depending on the contribution, reproducibility can be accomplished in various ways.
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7520576131687243,"For example, if the contribution is a novel architecture, describing the architecture fully
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7530864197530864,"might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7541152263374485,"be necessary to either make it possible for others to replicate the model with the same
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7551440329218106,"dataset, or provide access to the model. In general. releasing code and data is often
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7561728395061729,"one good way to accomplish this, but reproducibility can also be provided via detailed
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.757201646090535,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7582304526748971,"of a large language model), releasing of a model checkpoint, or other means that are
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7592592592592593,"appropriate to the research performed.
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7602880658436214,"• While NeurIPS does not require releasing code, the conference does require all submis-
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7613168724279835,"sions to provide some reasonable avenue for reproducibility, which may depend on the
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7623456790123457,"nature of the contribution. For example
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7633744855967078,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76440329218107,"to reproduce that algorithm.
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7654320987654321,"(b) If the contribution is primarily a new model architecture, the paper should describe
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7664609053497943,"the architecture clearly and fully.
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7674897119341564,"(c) If the contribution is a new model (e.g., a large language model), then there should
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7685185185185185,"either be a way to access this model for reproducing the results or a way to reproduce
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7695473251028807,"the model (e.g., with an open-source dataset or instructions for how to construct
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7705761316872428,"the dataset).
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7716049382716049,"(d) We recognize that reproducibility may be tricky in some cases, in which case
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.772633744855967,"authors are welcome to describe the particular way they provide for reproducibility.
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7736625514403292,"In the case of closed-source models, it may be that access to the model is limited in
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746913580246914,"some way (e.g., to registered users), but it should be possible for other researchers
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7757201646090535,"to have some path to reproducing or verifying the results.
623"
OPEN ACCESS TO DATA AND CODE,0.7767489711934157,"5. Open access to data and code
624"
OPEN ACCESS TO DATA AND CODE,0.7777777777777778,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
625"
OPEN ACCESS TO DATA AND CODE,0.7788065843621399,"tions to faithfully reproduce the main experimental results, as described in supplemental
626"
OPEN ACCESS TO DATA AND CODE,0.779835390946502,"material?
627"
OPEN ACCESS TO DATA AND CODE,0.7808641975308642,"Answer: [No]
628"
OPEN ACCESS TO DATA AND CODE,0.7818930041152263,"Justiﬁcation: We intend to release the code eventually, but we are not able to do so at the
629"
OPEN ACCESS TO DATA AND CODE,0.7829218106995884,"moment; we refrain from providing a detailed reason, as this could violate anonymity.
630"
OPEN ACCESS TO DATA AND CODE,0.7839506172839507,"Guidelines:
631"
OPEN ACCESS TO DATA AND CODE,0.7849794238683128,"• The answer NA means that paper does not include experiments requiring code.
632"
OPEN ACCESS TO DATA AND CODE,0.7860082304526749,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
633"
OPEN ACCESS TO DATA AND CODE,0.7870370370370371,"public/guides/CodeSubmissionPolicy) for more details.
634"
OPEN ACCESS TO DATA AND CODE,0.7880658436213992,"• While we encourage the release of code and data, we understand that this might not be
635"
OPEN ACCESS TO DATA AND CODE,0.7890946502057613,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
636"
OPEN ACCESS TO DATA AND CODE,0.7901234567901234,"including code, unless this is central to the contribution (e.g., for a new open-source
637"
OPEN ACCESS TO DATA AND CODE,0.7911522633744856,"benchmark).
638"
OPEN ACCESS TO DATA AND CODE,0.7921810699588477,"• The instructions should contain the exact command and environment needed to run to
639"
OPEN ACCESS TO DATA AND CODE,0.7932098765432098,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
640"
OPEN ACCESS TO DATA AND CODE,0.7942386831275721,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
641"
OPEN ACCESS TO DATA AND CODE,0.7952674897119342,"• The authors should provide instructions on data access and preparation, including how
642"
OPEN ACCESS TO DATA AND CODE,0.7962962962962963,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
643"
OPEN ACCESS TO DATA AND CODE,0.7973251028806584,"• The authors should provide scripts to reproduce all experimental results for the new
644"
OPEN ACCESS TO DATA AND CODE,0.7983539094650206,"proposed method and baselines. If only a subset of experiments are reproducible, they
645"
OPEN ACCESS TO DATA AND CODE,0.7993827160493827,"should state which ones are omitted from the script and why.
646"
OPEN ACCESS TO DATA AND CODE,0.8004115226337448,"• At submission time, to preserve anonymity, the authors should release anonymized
647"
OPEN ACCESS TO DATA AND CODE,0.801440329218107,"versions (if applicable).
648"
OPEN ACCESS TO DATA AND CODE,0.8024691358024691,"• Providing as much information as possible in supplemental material (appended to the
649"
OPEN ACCESS TO DATA AND CODE,0.8034979423868313,"paper) is recommended, but including URLs to data and code is permitted.
650"
OPEN ACCESS TO DATA AND CODE,0.8045267489711934,"6. Experimental Setting/Details
651"
OPEN ACCESS TO DATA AND CODE,0.8055555555555556,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
652"
OPEN ACCESS TO DATA AND CODE,0.8065843621399177,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
653"
OPEN ACCESS TO DATA AND CODE,0.8076131687242798,"results?
654"
OPEN ACCESS TO DATA AND CODE,0.808641975308642,"Answer: [Yes]
655"
OPEN ACCESS TO DATA AND CODE,0.8096707818930041,"Justiﬁcation: The setup is described for each experiment we conduct and we reference
656"
OPEN ACCESS TO DATA AND CODE,0.8106995884773662,"prior work that these build on. We use the standard CIFAR10 dataset for deep learning
657"
OPEN ACCESS TO DATA AND CODE,0.8117283950617284,"experiments.
658"
OPEN ACCESS TO DATA AND CODE,0.8127572016460906,"Guidelines:
659"
OPEN ACCESS TO DATA AND CODE,0.8137860082304527,"• The answer NA means that the paper does not include experiments.
660"
OPEN ACCESS TO DATA AND CODE,0.8148148148148148,"• The experimental setting should be presented in the core of the paper to a level of detail
661"
OPEN ACCESS TO DATA AND CODE,0.815843621399177,"that is necessary to appreciate the results and make sense of them.
662"
OPEN ACCESS TO DATA AND CODE,0.8168724279835391,"• The full details can be provided either with the code, in appendix, or as supplemental
663"
OPEN ACCESS TO DATA AND CODE,0.8179012345679012,"material.
664"
OPEN ACCESS TO DATA AND CODE,0.8189300411522634,"7. Experiment Statistical Signiﬁcance
665"
OPEN ACCESS TO DATA AND CODE,0.8199588477366255,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
666"
OPEN ACCESS TO DATA AND CODE,0.8209876543209876,"information about the statistical signiﬁcance of the experiments?
667"
OPEN ACCESS TO DATA AND CODE,0.8220164609053497,"Answer: [Yes]
668"
OPEN ACCESS TO DATA AND CODE,0.823045267489712,"Justiﬁcation: The auditing results present a lower bound which can be viewed as a one-sided
669"
OPEN ACCESS TO DATA AND CODE,0.8240740740740741,"conﬁdence interval. For the other results the numbers are computed non-statistically (i.e.
670"
OPEN ACCESS TO DATA AND CODE,0.8251028806584362,"by numerically evaluating a formula); the only potential error here is due to numerical
671"
OPEN ACCESS TO DATA AND CODE,0.8261316872427984,"precision.
672"
OPEN ACCESS TO DATA AND CODE,0.8271604938271605,"Guidelines:
673"
OPEN ACCESS TO DATA AND CODE,0.8281893004115226,"• The answer NA means that the paper does not include experiments.
674"
OPEN ACCESS TO DATA AND CODE,0.8292181069958847,"• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
675"
OPEN ACCESS TO DATA AND CODE,0.8302469135802469,"dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
676"
OPEN ACCESS TO DATA AND CODE,0.831275720164609,"the main claims of the paper.
677"
OPEN ACCESS TO DATA AND CODE,0.8323045267489712,"• The factors of variability that the error bars are capturing should be clearly stated (for
678"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"example, train/test split, initialization, random drawing of some parameter, or overall
679"
OPEN ACCESS TO DATA AND CODE,0.8343621399176955,"run with given experimental conditions).
680"
OPEN ACCESS TO DATA AND CODE,0.8353909465020576,"• The method for calculating the error bars should be explained (closed form formula,
681"
OPEN ACCESS TO DATA AND CODE,0.8364197530864198,"call to a library function, bootstrap, etc.)
682"
OPEN ACCESS TO DATA AND CODE,0.8374485596707819,"• The assumptions made should be given (e.g., Normally distributed errors).
683"
OPEN ACCESS TO DATA AND CODE,0.838477366255144,"• It should be clear whether the error bar is the standard deviation or the standard error
684"
OPEN ACCESS TO DATA AND CODE,0.8395061728395061,"of the mean.
685"
OPEN ACCESS TO DATA AND CODE,0.8405349794238683,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
686"
OPEN ACCESS TO DATA AND CODE,0.8415637860082305,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
687"
OPEN ACCESS TO DATA AND CODE,0.8425925925925926,"of Normality of errors is not veriﬁed.
688"
OPEN ACCESS TO DATA AND CODE,0.8436213991769548,"• For asymmetric distributions, the authors should be careful not to show in tables or
689"
OPEN ACCESS TO DATA AND CODE,0.8446502057613169,"ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
690"
OPEN ACCESS TO DATA AND CODE,0.845679012345679,"error rates).
691"
OPEN ACCESS TO DATA AND CODE,0.8467078189300411,"• If error bars are reported in tables or plots, The authors should explain in the text how
692"
OPEN ACCESS TO DATA AND CODE,0.8477366255144033,"they were calculated and reference the corresponding ﬁgures or tables in the text.
693"
EXPERIMENTS COMPUTE RESOURCES,0.8487654320987654,"8. Experiments Compute Resources
694"
EXPERIMENTS COMPUTE RESOURCES,0.8497942386831275,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
695"
EXPERIMENTS COMPUTE RESOURCES,0.8508230452674898,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
696"
EXPERIMENTS COMPUTE RESOURCES,0.8518518518518519,"the experiments?
697"
EXPERIMENTS COMPUTE RESOURCES,0.852880658436214,"Answer: [Yes]
698"
EXPERIMENTS COMPUTE RESOURCES,0.8539094650205762,"Justiﬁcation: We used A2-megagpu-16g machines from Google cloud which have 16 Nvidia
699"
EXPERIMENTS COMPUTE RESOURCES,0.8549382716049383,"A100 40GB GPUs to run the experiments in this paper. Overall we used around 33,000
700"
EXPERIMENTS COMPUTE RESOURCES,0.8559670781893004,"hours of GPU to run all of the experiments in the paper.
701"
EXPERIMENTS COMPUTE RESOURCES,0.8569958847736625,"Guidelines:
702"
EXPERIMENTS COMPUTE RESOURCES,0.8580246913580247,"• The answer NA means that the paper does not include experiments.
703"
EXPERIMENTS COMPUTE RESOURCES,0.8590534979423868,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
704"
EXPERIMENTS COMPUTE RESOURCES,0.8600823045267489,"or cloud provider, including relevant memory and storage.
705"
EXPERIMENTS COMPUTE RESOURCES,0.8611111111111112,"• The paper should provide the amount of compute required for each of the individual
706"
EXPERIMENTS COMPUTE RESOURCES,0.8621399176954733,"experimental runs as well as estimate the total compute.
707"
EXPERIMENTS COMPUTE RESOURCES,0.8631687242798354,"• The paper should disclose whether the full research project required more compute
708"
EXPERIMENTS COMPUTE RESOURCES,0.8641975308641975,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
709"
EXPERIMENTS COMPUTE RESOURCES,0.8652263374485597,"didn’t make it into the paper).
710"
CODE OF ETHICS,0.8662551440329218,"9. Code Of Ethics
711"
CODE OF ETHICS,0.8672839506172839,"Question: Does the research conducted in the paper conform, in every respect, with the
712"
CODE OF ETHICS,0.8683127572016461,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
713"
CODE OF ETHICS,0.8693415637860082,"Answer: [Yes]
714"
CODE OF ETHICS,0.8703703703703703,"Justiﬁcation: No human subjects or sensitive data were used.
715"
CODE OF ETHICS,0.8713991769547325,"Guidelines:
716"
CODE OF ETHICS,0.8724279835390947,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
717"
CODE OF ETHICS,0.8734567901234568,"• If the authors answer No, they should explain the special circumstances that require a
718"
CODE OF ETHICS,0.8744855967078189,"deviation from the Code of Ethics.
719"
CODE OF ETHICS,0.8755144032921811,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
720"
CODE OF ETHICS,0.8765432098765432,"eration due to laws or regulations in their jurisdiction).
721"
BROADER IMPACTS,0.8775720164609053,"10. Broader Impacts
722"
BROADER IMPACTS,0.8786008230452675,"Question: Does the paper discuss both potential positive societal impacts and negative
723"
BROADER IMPACTS,0.8796296296296297,"societal impacts of the work performed?
724"
BROADER IMPACTS,0.8806584362139918,"Answer: [NA]
725"
BROADER IMPACTS,0.8816872427983539,"Justiﬁcation: This work is primarily theoretical. While it is possible that downstream uses
726"
BROADER IMPACTS,0.8827160493827161,"of our work could be societally impactful, the precise consequences are difﬁcult to foresee.
727"
BROADER IMPACTS,0.8837448559670782,"The considerations are similar to any other paper on private machine learning.
728"
BROADER IMPACTS,0.8847736625514403,"Guidelines:
729"
BROADER IMPACTS,0.8858024691358025,"• The answer NA means that there is no societal impact of the work performed.
730"
BROADER IMPACTS,0.8868312757201646,"• If the authors answer NA or No, they should explain why their work has no societal
731"
BROADER IMPACTS,0.8878600823045267,"impact or why the paper does not address societal impact.
732"
BROADER IMPACTS,0.8888888888888888,"• Examples of negative societal impacts include potential malicious or unintended uses
733"
BROADER IMPACTS,0.8899176954732511,"(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
734"
BROADER IMPACTS,0.8909465020576132,"(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc
735"
BROADER IMPACTS,0.8919753086419753,"groups), privacy considerations, and security considerations.
736"
BROADER IMPACTS,0.8930041152263375,"• The conference expects that many papers will be foundational research and not tied
737"
BROADER IMPACTS,0.8940329218106996,"to particular applications, let alone deployments. However, if there is a direct path to
738"
BROADER IMPACTS,0.8950617283950617,"any negative applications, the authors should point it out. For example, it is legitimate
739"
BROADER IMPACTS,0.8960905349794238,"to point out that an improvement in the quality of generative models could be used to
740"
BROADER IMPACTS,0.897119341563786,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
741"
BROADER IMPACTS,0.8981481481481481,"that a generic algorithm for optimizing neural networks could enable people to train
742"
BROADER IMPACTS,0.8991769547325102,"models that generate Deepfakes faster.
743"
BROADER IMPACTS,0.9002057613168725,"• The authors should consider possible harms that could arise when the technology is
744"
BROADER IMPACTS,0.9012345679012346,"being used as intended and functioning correctly, harms that could arise when the
745"
BROADER IMPACTS,0.9022633744855967,"technology is being used as intended but gives incorrect results, and harms following
746"
BROADER IMPACTS,0.9032921810699589,"from (intentional or unintentional) misuse of the technology.
747"
BROADER IMPACTS,0.904320987654321,"• If there are negative societal impacts, the authors could also discuss possible mitigation
748"
BROADER IMPACTS,0.9053497942386831,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
749"
BROADER IMPACTS,0.9063786008230452,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
750"
BROADER IMPACTS,0.9074074074074074,"feedback over time, improving the efﬁciency and accessibility of ML).
751"
SAFEGUARDS,0.9084362139917695,"11. Safeguards
752"
SAFEGUARDS,0.9094650205761317,"Question: Does the paper describe safeguards that have been put in place for responsible
753"
SAFEGUARDS,0.9104938271604939,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
754"
SAFEGUARDS,0.911522633744856,"image generators, or scraped datasets)?
755"
SAFEGUARDS,0.9125514403292181,"Answer: [NA]
756"
SAFEGUARDS,0.9135802469135802,"Justiﬁcation: Our paper uses standard datasets (CIFAR10) and standard models (WideRes-
757"
SAFEGUARDS,0.9146090534979424,"Net).
758"
SAFEGUARDS,0.9156378600823045,"Guidelines:
759"
SAFEGUARDS,0.9166666666666666,"• The answer NA means that the paper poses no such risks.
760"
SAFEGUARDS,0.9176954732510288,"• Released models that have a high risk for misuse or dual-use should be released with
761"
SAFEGUARDS,0.918724279835391,"necessary safeguards to allow for controlled use of the model, for example by requiring
762"
SAFEGUARDS,0.9197530864197531,"that users adhere to usage guidelines or restrictions to access the model or implementing
763"
SAFEGUARDS,0.9207818930041153,"safety ﬁlters.
764"
SAFEGUARDS,0.9218106995884774,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
765"
SAFEGUARDS,0.9228395061728395,"should describe how they avoided releasing unsafe images.
766"
SAFEGUARDS,0.9238683127572016,"• We recognize that providing effective safeguards is challenging, and many papers do
767"
SAFEGUARDS,0.9248971193415638,"not require this, but we encourage authors to take this into account and make a best
768"
SAFEGUARDS,0.9259259259259259,"faith effort.
769"
LICENSES FOR EXISTING ASSETS,0.926954732510288,"12. Licenses for existing assets
770"
LICENSES FOR EXISTING ASSETS,0.9279835390946503,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
771"
LICENSES FOR EXISTING ASSETS,0.9290123456790124,"the paper, properly credited and are the license and terms of use explicitly mentioned and
772"
LICENSES FOR EXISTING ASSETS,0.9300411522633745,"properly respected?
773"
LICENSES FOR EXISTING ASSETS,0.9310699588477366,"Answer: [Yes]
774"
LICENSES FOR EXISTING ASSETS,0.9320987654320988,"Justiﬁcation: We use CIFAR10 [Ale09] and a WideResNet [ZK16].
775"
LICENSES FOR EXISTING ASSETS,0.9331275720164609,"Guidelines:
776"
LICENSES FOR EXISTING ASSETS,0.934156378600823,"• The answer NA means that the paper does not use existing assets.
777"
LICENSES FOR EXISTING ASSETS,0.9351851851851852,"• The authors should cite the original paper that produced the code package or dataset.
778"
LICENSES FOR EXISTING ASSETS,0.9362139917695473,"• The authors should state which version of the asset is used and, if possible, include a
779"
LICENSES FOR EXISTING ASSETS,0.9372427983539094,"URL.
780"
LICENSES FOR EXISTING ASSETS,0.9382716049382716,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
781"
LICENSES FOR EXISTING ASSETS,0.9393004115226338,"• For scraped data from a particular source (e.g., website), the copyright and terms of
782"
LICENSES FOR EXISTING ASSETS,0.9403292181069959,"service of that source should be provided.
783"
LICENSES FOR EXISTING ASSETS,0.941358024691358,"• If assets are released, the license, copyright information, and terms of use in the
784"
LICENSES FOR EXISTING ASSETS,0.9423868312757202,"package should be provided. For popular datasets, paperswithcode.com/datasets
785"
LICENSES FOR EXISTING ASSETS,0.9434156378600823,"has curated licenses for some datasets. Their licensing guide can help determine the
786"
LICENSES FOR EXISTING ASSETS,0.9444444444444444,"license of a dataset.
787"
LICENSES FOR EXISTING ASSETS,0.9454732510288066,"• For existing datasets that are re-packaged, both the original license and the license of
788"
LICENSES FOR EXISTING ASSETS,0.9465020576131687,"the derived asset (if it has changed) should be provided.
789"
LICENSES FOR EXISTING ASSETS,0.9475308641975309,"• If this information is not available online, the authors are encouraged to reach out to
790"
LICENSES FOR EXISTING ASSETS,0.948559670781893,"the asset’s creators.
791"
NEW ASSETS,0.9495884773662552,"13. New Assets
792"
NEW ASSETS,0.9506172839506173,"Question: Are new assets introduced in the paper well documented and is the documentation
793"
NEW ASSETS,0.9516460905349794,"provided alongside the assets?
794"
NEW ASSETS,0.9526748971193416,"Answer: [NA]
795"
NEW ASSETS,0.9537037037037037,"Justiﬁcation: Our main contribution is a heuristic privacy analysis. This is fully described in
796"
NEW ASSETS,0.9547325102880658,"the paper and can be computed using existing open-source libraries.
797"
NEW ASSETS,0.9557613168724279,"Guidelines:
798"
NEW ASSETS,0.9567901234567902,"• The answer NA means that the paper does not release new assets.
799"
NEW ASSETS,0.9578189300411523,"• Researchers should communicate the details of the dataset/code/model as part of their
800"
NEW ASSETS,0.9588477366255144,"submissions via structured templates. This includes details about training, license,
801"
NEW ASSETS,0.9598765432098766,"limitations, etc.
802"
NEW ASSETS,0.9609053497942387,"• The paper should discuss whether and how consent was obtained from people whose
803"
NEW ASSETS,0.9619341563786008,"asset is used.
804"
NEW ASSETS,0.9629629629629629,"• At submission time, remember to anonymize your assets (if applicable). You can either
805"
NEW ASSETS,0.9639917695473251,"create an anonymized URL or include an anonymized zip ﬁle.
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9650205761316872,"14. Crowdsourcing and Research with Human Subjects
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9660493827160493,"Question: For crowdsourcing experiments and research with human subjects, does the paper
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670781893004116,"include the full text of instructions given to participants and screenshots, if applicable, as
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681069958847737,"well as details about compensation (if any)?
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9691358024691358,"Answer: [NA]
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970164609053498,"Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9711934156378601,"Guidelines:
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722222222222222,"• The answer NA means that the paper does not involve crowdsourcing nor research with
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732510288065843,"human subjects.
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742798353909465,"• Including this information in the supplemental material is ﬁne, but if the main contribu-
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9753086419753086,"tion of the paper involves human subjects, then as much detail as possible should be
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763374485596708,"included in the main paper.
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977366255144033,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783950617283951,"or other labor should be paid at least the minimum wage in the country of the data
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794238683127572,"collector.
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804526748971193,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814814814814815,"Subjects
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825102880658436,"Question: Does the paper describe potential risks incurred by study participants, whether
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835390946502057,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845679012345679,"approvals (or an equivalent approval/review based on the requirements of your country or
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98559670781893,"institution) were obtained?
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866255144032922,"Answer: [NA]
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876543209876543,"Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886831275720165,"Guidelines:
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897119341563786,"• The answer NA means that the paper does not involve crowdsourcing nor research with
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907407407407407,"human subjects.
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917695473251029,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992798353909465,"may be required for any human subjects research. If you obtained IRB approval, you
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938271604938271,"should clearly state this in the paper.
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948559670781894,"• We recognize that the procedures for this may vary signiﬁcantly between institutions
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958847736625515,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969135802469136,"guidelines for their institution.
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979423868312757,"• For initial submissions, do not include any information that would break anonymity (if
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989711934156379,"applicable), such as the institution conducting the review.
840"
