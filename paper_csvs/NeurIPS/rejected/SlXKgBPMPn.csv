Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017271157167530224,"Most real-world graphs exhibit a hierarchical structure, which is often overlooked
1"
ABSTRACT,0.0034542314335060447,"by existing graph generation methods. To address this limitation, we propose a
2"
ABSTRACT,0.0051813471502590676,"novel graph generative network that captures the hierarchical nature of graphs
3"
ABSTRACT,0.0069084628670120895,"and successively generates the graph sub-structures in a coarse-to-ﬁne fashion. At
4"
ABSTRACT,0.008635578583765112,"each level of hierarchy, this model generates communities in parallel, followed by
5"
ABSTRACT,0.010362694300518135,"the prediction of cross-edges between communities using a separate model. This
6"
ABSTRACT,0.012089810017271158,"modular approach results in a highly scalable graph generative network. More-
7"
ABSTRACT,0.013816925734024179,"over, we model the output distribution of edges in the hierarchical graph with
8"
ABSTRACT,0.015544041450777202,"a multinomial distribution and derive a recursive factorization for this distribu-
9"
ABSTRACT,0.017271157167530225,"tion, enabling us to generate sub-graphs with integer-valued edge weights in an
10"
ABSTRACT,0.018998272884283247,"autoregressive approach. Empirical studies demonstrate that the proposed gen-
11"
ABSTRACT,0.02072538860103627,"erative model can effectively capture both local and global properties of graphs
12"
ABSTRACT,0.022452504317789293,"and achieves state-of-the-art performance in terms of graph quality on various
13"
ABSTRACT,0.024179620034542316,"benchmarks.
14"
INTRODUCTION,0.025906735751295335,"1
Introduction
15"
INTRODUCTION,0.027633851468048358,"Graphs play a fundamental role in representing relationships and are widely applicable in various
16"
INTRODUCTION,0.02936096718480138,"domains. The task of generating graphs from data holds immense value for diverse applications but
17"
INTRODUCTION,0.031088082901554404,"also poses signiﬁcant challenges (Dai et al., 2020). Some of the applications include: the exploration
18"
INTRODUCTION,0.03281519861830743,"of novel molecular and chemical structures (Jin et al., 2020), document generation (Blei et al., 2003),
19"
INTRODUCTION,0.03454231433506045,"circuit design (Mirhoseini et al., 2021), the analysis and synthesis of realistic data networks, as well
20"
INTRODUCTION,0.03626943005181347,"as the synthesis of scene graphs in computer (Manolis Savva et al., 2019; Ramakrishnan et al., 2021).
21"
INTRODUCTION,0.037996545768566495,"In all the aforementioned domains, a common observation is the presence of locally heterogeneous
22"
INTRODUCTION,0.039723661485319514,"edge distributions in the graph representing the system, leading to the formation of clusters or
23"
INTRODUCTION,0.04145077720207254,"communities and hierarchical structures. These clusters represent groups of nodes characterized by
24"
INTRODUCTION,0.04317789291882556,"a high density of edges within the group and a comparatively lower density of edges connecting
25"
INTRODUCTION,0.044905008635578586,"the group with the rest of the graph. In a hierarchical structure that arise from graph clustering, the
26"
INTRODUCTION,0.046632124352331605,"communities in the lower levels capture the local structures and relationships within the graph. These
27"
INTRODUCTION,0.04835924006908463,"communities provide insights into the ﬁne-grained interactions among nodes. On the other hand, the
28"
INTRODUCTION,0.05008635578583765,"higher levels of the hierarchy reﬂect the broader interactions between communities and characterize
29"
INTRODUCTION,0.05181347150259067,"global properties of the graph. Therefore, in order to generate realistic graphs, it is essential for graph
30"
INTRODUCTION,0.0535405872193437,"generation models to learn this multi-scale structure, and be able to capture the cross-level relations.
31"
INTRODUCTION,0.055267702936096716,"While hierarchical multi-resolution generative models were developed for speciﬁc data types such as
32"
INTRODUCTION,0.05699481865284974,"voice (Oord et al., 2016), image (Reed et al., 2017; Karami et al., 2019) and molecular motifs Jin
33"
INTRODUCTION,0.05872193436960276,"et al. (2020), these methods rely on domain-speciﬁc priors that are not applicable to general graphs
34"
INTRODUCTION,0.06044905008635579,"with unordered nature. To the best of our knowledge, there exists no data-driven generative models
35"
INTRODUCTION,0.06217616580310881,"speciﬁcally designed for generic graphs that can effectively incorporate hierarchical structure.
36"
INTRODUCTION,0.06390328151986183,"Graph generative models have been extensively studied in the literature. Classical methods based on
37"
INTRODUCTION,0.06563039723661486,"random graph theory, such as those proposed in Erdos & Rényi (1960) and Barabási & Albert (1999),
38"
INTRODUCTION,0.06735751295336788,"can only capture a limited set of hand-engineered graph statistics. Leskovec et al. (2010) leveraged
39"
INTRODUCTION,0.0690846286701209,"the Kronecker product of matrices but the resulting generative model is very limited in modeling
40"
INTRODUCTION,0.07081174438687392,"the underlying graph distributions. With recent advances in graph neural networks, a variety of
41"
INTRODUCTION,0.07253886010362694,"deep neural network models have been introduced that are based on variational autoencoders (VAE)
42"
INTRODUCTION,0.07426597582037997,"(Kingma & Welling, 2013) or generative adversarial networks (GAN) (Goodfellow et al., 2020).
43"
INTRODUCTION,0.07599309153713299,"Some examples of such models include (De Cao & Kipf, 2018; Simonovsky & Komodakis, 2018;
44"
INTRODUCTION,0.07772020725388601,"Kipf & Welling, 2016; Ma et al., 2018; Liu et al., 2019; Bojchevski et al., 2018; Yang et al., 2019)
45"
INTRODUCTION,0.07944732297063903,"The major challenge in VAE based models is that they rely on heuristics to solve a graph matching
46"
INTRODUCTION,0.08117443868739206,"problem for aligning the VAE’s input and sampled output, limiting them to small graphs. On the other
47"
INTRODUCTION,0.08290155440414508,"hand, GAN-based methods circumvent the need for graph matching by using a permutation invariant
48"
INTRODUCTION,0.0846286701208981,"discriminator. However, they can still suffer from convergence issues and have difﬁculty capturing
49"
INTRODUCTION,0.08635578583765112,"complex dependencies in graph structures for moderate to large graphs Li et al. (2018); Martinkus
50"
INTRODUCTION,0.08808290155440414,"et al. (2022). To address these limitations, (Martinkus et al., 2022) recently proposed using spectral
51"
INTRODUCTION,0.08981001727115717,"conditioning to enhance the expressivity of GAN models in capturing global graph properties.
52"
INTRODUCTION,0.09153713298791019,"On the other hand, autoregressive models approach graph generation as a sequential decision-making
53"
INTRODUCTION,0.09326424870466321,"process. Following this paradigm, Li et al. (2018) proposed generative model based on GNN but it
54"
INTRODUCTION,0.09499136442141623,"has high complexity of O(mn2). In a distinct approach, GraphRNN (You et al., 2018) modeled graph
55"
INTRODUCTION,0.09671848013816926,"generation with a two-stage RNN architecture for generating new nodes and their links, respectively.
56"
INTRODUCTION,0.09844559585492228,"However, traversing all elements of the adjacency matrix in a predeﬁned order results in O(n2)
57"
INTRODUCTION,0.1001727115716753,"time complexity making it non-scalable to large graphs. In contrast, GRAN (Liao et al., 2019)
58"
INTRODUCTION,0.10189982728842832,"employs a graph attention network and generates the adjacency matrix row by row, resulting in a
59"
INTRODUCTION,0.10362694300518134,"O(n) complexity sequential generation process. To improve the scalability of generative models,
60"
INTRODUCTION,0.10535405872193437,"Dai et al. (2020) proposed an algorithm for sparse graphs that decreases the training complexity
61"
INTRODUCTION,0.1070811744386874,"to O(log n), but at the expense of increasing the generation time complexity to O((n + m) log n).
62"
INTRODUCTION,0.10880829015544041,"Despite their improvement in capturing complex statistics of the graphs, autoregressive models highly
63"
INTRODUCTION,0.11053540587219343,"rely on an appropriate node ordering and do not take into account the community structures of the
64"
INTRODUCTION,0.11226252158894647,"graphs. Additionally, due to their recursive nature, they are not fully parallelizable.
65"
INTRODUCTION,0.11398963730569948,"A new family of diffusion model for graphs has emerged recently. Continuous denoising diffusion
66"
INTRODUCTION,0.1157167530224525,"was developed by Jo et al. (2022), which adds Gaussian noise to the graph adjacency matrix and
67"
INTRODUCTION,0.11744386873920552,"node features during the diffusion process. However, since continuous noise destroys the sparsity
68"
INTRODUCTION,0.11917098445595854,"and structural properties of the graph, discrete denoising diffusion models have been developed as
69"
INTRODUCTION,0.12089810017271158,"a solution in (Haefeli et al., 2022; Vignac et al., 2022). These models progressively edit graphs by
70"
INTRODUCTION,0.1226252158894646,"adding or removing edges in the diffusion process, and then denoising graph neural networks are
71"
INTRODUCTION,0.12435233160621761,"trained to reverse the diffusion process. While the denoising diffusion models can offer promising
72"
INTRODUCTION,0.12607944732297063,"results, their main drawback is the requirement of a long chain of reverse diffusion, which can result
73"
INTRODUCTION,0.12780656303972365,"in relatively slow sampling.
74"
INTRODUCTION,0.12953367875647667,"In his work, we introduce HiGen, a Hierarchical Graph Generative Network to address the limitations
75"
INTRODUCTION,0.13126079447322972,"of existing generative models by incorporating community structures and cross-level interactions.
76"
INTRODUCTION,0.13298791018998274,"This approach involves generating graphs in a coarse-to-ﬁne manner, where graph generation at each
77"
INTRODUCTION,0.13471502590673576,"level is conditioned on a higher level (lower resolution) graph. The generation of communities at lower
78"
INTRODUCTION,0.13644214162348878,"levels is performed in parallel, followed by the prediction of cross-edges between communities using a
79"
INTRODUCTION,0.1381692573402418,"separate model. This parallelized approach enables high scalability. To capture hierarchical relations,
80"
INTRODUCTION,0.13989637305699482,"our model allows each node at a given level to depend not only on its neighbouring nodes but also on
81"
INTRODUCTION,0.14162348877374784,"its corresponding super-node at the higher level. Furthermore, we address the generation of integer-
82"
INTRODUCTION,0.14335060449050085,"valued edge weights of the hierarchical structure by modeling the output distribution of edges using
83"
INTRODUCTION,0.14507772020725387,"a multinomial distribution. We show that multinomial distribution can be factorized successively,
84"
INTRODUCTION,0.14680483592400692,"enabling the autoregressive generation of each community. This property makes the proposed
85"
INTRODUCTION,0.14853195164075994,"architecture well-suited for generating graphs with integer-valued edge weights. Furthermore, by
86"
INTRODUCTION,0.15025906735751296,"breaking down the graph generation process into the generation of multiple small partitions that are
87"
INTRODUCTION,0.15198618307426598,"conditionally independent of each other, HiGen reduces its sensitivity to a predeﬁned initial ordering
88"
INTRODUCTION,0.153713298791019,"of nodes.
89"
BACKGROUND,0.15544041450777202,"2
Background
90"
BACKGROUND,0.15716753022452504,"A graph G = (V, E) is a collection of nodes (vertices) V and edges E with corresponding sizes
91"
BACKGROUND,0.15889464594127806,"n = |V| and m = |E| and an adjacency matrix A⇡for the node ordering ⇡. The node set can
92"
BACKGROUND,0.16062176165803108,"be partitioned into c communities (a.k.a. cluster or modules) using a graph partitioning function
93"
BACKGROUND,0.16234887737478412,"(a)
(b)
(c)"
BACKGROUND,0.16407599309153714,"(d)
(e)"
BACKGROUND,0.16580310880829016,"Figure 1: (a) A sample hierarchical graph with 2 levels is shown. Communities are shown in different colors
and the weight of a node and the weight of an edge in a higher level, represent the sum of the edges in the
corresponding community and bipartite, respectively. Node size and edge width indicate their weights. (b) The
matrix shows corresponding adjacency of the graph G2 matrix where each of its sub-graphs corresponds to a
block in the adjacency matrix, communities are shown in different colors and bipartites are colored in gray. (c)
Decomposition of multinomial distribution as a recursive stick-breaking process where at each iteration, ﬁrst a
fraction of the remaining weights wm is allocated to the m-th row (the m-th node in the sub-graph) and then
this fraction vm is distributed among that row of lower triangular adjacency matrix, ˆA. (d) Parallel generation
of communities. (e) Parallel prediction of bipartites. Shadowed lines are the augmented edges representing
candidate edges at each step."
BACKGROUND,0.16753022452504318,"F : V ! {1, ..., c}, where each cluster of nodes forms a sub-graph denoted by Ci = (V(Ci), E(Ci))
94"
BACKGROUND,0.1692573402417962,"with adjacency matrix Ai. The cross-links between neighboring communities form a bipartite
95"
BACKGROUND,0.17098445595854922,"graph, denoted by Bij = (V(Ci), V(Cj), E(Bij)) with adjacency matrix Aij. Each community
96"
BACKGROUND,0.17271157167530224,"is aggregated to a super-node and each bipartite corresponds to a super-edge linking neighboring
97"
BACKGROUND,0.17443868739205526,"communities, which induces a coarser graph at the higher (a.k.a. parent) level. Herein, the levels are
98"
BACKGROUND,0.17616580310880828,"indexed by superscripts. Formally, each community at level l, Cl"
BACKGROUND,0.17789291882556132,"i, is mapped to a node at the higher
99"
BACKGROUND,0.17962003454231434,"level graph, also called its parent node, vl−1"
BACKGROUND,0.18134715025906736,"i
:= Pa(Cl"
BACKGROUND,0.18307426597582038,"i) and each bipartite at level l is represented by
100"
BACKGROUND,0.1848013816925734,"an edge in the higher level, also called its parent edge, el−1"
BACKGROUND,0.18652849740932642,"i
= Pa(Bl"
BACKGROUND,0.18825561312607944,ij) = (vl−1
BACKGROUND,0.18998272884283246,"i
, vl−1"
BACKGROUND,0.19170984455958548,"j
). The weights
101"
BACKGROUND,0.19343696027633853,"of the self edges and the weights of the cross-edges in the parent level are determined by the sum of the
102"
BACKGROUND,0.19516407599309155,"weights of the edges within their corresponding community and bipartite, respectively. Therefore, the
103"
BACKGROUND,0.19689119170984457,edges in the induced graphs at the higher levels have integer-valued weights: wl−1
BACKGROUND,0.19861830742659758,"ii
= P"
BACKGROUND,0.2003454231433506,e2E(Cl
BACKGROUND,0.20207253886010362,"i) we
104"
BACKGROUND,0.20379965457685664,and wl−1
BACKGROUND,0.20552677029360966,"ij
= P"
BACKGROUND,0.20725388601036268,e2E(Bl
BACKGROUND,0.20898100172711573,"ij) we, moreover sum of all edge weights remains constant in all levels so
105"
BACKGROUND,0.21070811744386875,w0 := P
BACKGROUND,0.21243523316062177,"e2E(Gl) we = |E|, 8 l 2 [0, ..., L].
106"
BACKGROUND,0.2141623488773748,"This clustering process continues recursively in a bottom-up approach until a single node graph G0 is
107"
BACKGROUND,0.2158894645941278,"obtained, producing a hierarchical graph, deﬁned by the set of graphs in all levels of abstractions,
108"
BACKGROUND,0.21761658031088082,"HG := {G0, ...., GL−1, GL}. This forms a dendrogram tree with G0 being the root and GL being the
109"
BACKGROUND,0.21934369602763384,"ﬁnal graph that is generated at the leaf level. An HG is visualized in ﬁgure 1a. The hierarchical tree
110"
BACKGROUND,0.22107081174438686,"structure enables modeling of both local and long-range interactions among nodes, as well as control
111"
BACKGROUND,0.22279792746113988,"over the ﬂow of information between them, across multiple levels of abstraction. This is a key aspect
112"
BACKGROUND,0.22452504317789293,"of our proposed generative model.
113"
HIERARCHICAL GRAPH GENERATION,0.22625215889464595,"3
Hierarchical Graph Generation
114"
HIERARCHICAL GRAPH GENERATION,0.22797927461139897,"In graph generative networks, the objective is to learn a generative model, p(G) given a set of training
115"
HIERARCHICAL GRAPH GENERATION,0.229706390328152,"graphs. This work aims to establish a hierarchical multi-resolution framework for generating graphs in
116"
HIERARCHICAL GRAPH GENERATION,0.231433506044905,"a coarse-to-ﬁne fashion. In this framework, we assume that the graphs do not have node attributes, so
117"
HIERARCHICAL GRAPH GENERATION,0.23316062176165803,"the generative model only needs to characterize the graph topology. Given a particular node ordering
118"
HIERARCHICAL GRAPH GENERATION,0.23488773747841105,"⇡, and a hierarchical graph HG := {G0, ...., GL−1, GL}, produced by recursively applying a graph
119"
HIERARCHICAL GRAPH GENERATION,0.23661485319516407,"partitioning function, F, we can factorize the generative model using the chain rule of probability as:
120"
HIERARCHICAL GRAPH GENERATION,0.23834196891191708,"p(G = GL, ⇡) = p({GL, GL−1, ..., G0}, ⇡) = p(GL, ⇡| {GL−1, ..., G0}) ... p(G1, ⇡| G0) p(G0) = L
Y l=0"
HIERARCHICAL GRAPH GENERATION,0.24006908462867013,"p(Gl, ⇡| Gl−1) ⇥p(G0)
(1)"
HIERARCHICAL GRAPH GENERATION,0.24179620034542315,"In other words, the generative process involves specifying the probability of the graph at each level
121"
HIERARCHICAL GRAPH GENERATION,0.24352331606217617,"conditioned on its parent level graph in the hierarchy. This process is iterated recursively until the
122"
HIERARCHICAL GRAPH GENERATION,0.2452504317789292,"lowest level, or leaf level, is reached. Here, the distribution of the root p(G0) = p(w0 = w0) can be
123"
HIERARCHICAL GRAPH GENERATION,0.2469775474956822,"simply estimated using the empirical distribution of the number of edges |E| of graphs in the training
124"
HIERARCHICAL GRAPH GENERATION,0.24870466321243523,"set.
125"
HIERARCHICAL GRAPH GENERATION,0.2504317789291883,"Based on the partitioned structure within each level of HG, the conditional generative probability
126"
HIERARCHICAL GRAPH GENERATION,0.25215889464594127,"p(Gl | Gl−1) can be decomposed into the probability of its communities and bipartite graphs as:
127"
HIERARCHICAL GRAPH GENERATION,0.2538860103626943,p(Gl | Gl−1) = p({Cl
HIERARCHICAL GRAPH GENERATION,0.2556131260794473,i 8i 2 V(Gl−1)} [ {Bl
HIERARCHICAL GRAPH GENERATION,0.25734024179620035,"ij 8(i, j) 2 E(Gl−1)} | Gl−1) u Y"
HIERARCHICAL GRAPH GENERATION,0.25906735751295334,i 2 V(Gl−1) p(Cl
HIERARCHICAL GRAPH GENERATION,0.2607944732297064,i | Gl−1) ⇥ Y
HIERARCHICAL GRAPH GENERATION,0.26252158894645944,"(i,j)2 E(Gl−1) p(Bl"
HIERARCHICAL GRAPH GENERATION,0.26424870466321243,"ij | Gl−1)
(2)"
HIERARCHICAL GRAPH GENERATION,0.2659758203799655,The approximation in this decomposition becomes an equivalence when each community Cl
HIERARCHICAL GRAPH GENERATION,0.26770293609671847,"i or
128"
HIERARCHICAL GRAPH GENERATION,0.2694300518134715,bipartite graph Bl
HIERARCHICAL GRAPH GENERATION,0.2711571675302245,"ij is assumed to be independent of all other components in its level conditioned on
129"
HIERARCHICAL GRAPH GENERATION,0.27288428324697755,"the parent graph Gl−1. .1 Since the integer-valued weights of the edges in each level can be modeled
130"
HIERARCHICAL GRAPH GENERATION,0.27461139896373055,"by a multinomial distribution, we can leverage the properties of multinomial distribution to prove the
131"
HIERARCHICAL GRAPH GENERATION,0.2763385146804836,"conditional independence of the components.
132"
HIERARCHICAL GRAPH GENERATION,0.27806563039723664,"Theorem 3.1. Let the random vector w := [we]e 2 E(Gl) denote the set of weights of all edges of Gl
133"
HIERARCHICAL GRAPH GENERATION,0.27979274611398963,"such that their sum is w0 = 1T w. The joint probability of w can be described by a multinomial
134"
HIERARCHICAL GRAPH GENERATION,0.2815198618307427,"distribution: w ⇠Mu(w | w0, ✓l). By observing that the sum of edge weights within each community
135 Cl"
HIERARCHICAL GRAPH GENERATION,0.28324697754749567,i and bipartite graph Bl
HIERARCHICAL GRAPH GENERATION,0.2849740932642487,"ij are determined by the weights of their parent edges in the higher level,
136 wl−1"
HIERARCHICAL GRAPH GENERATION,0.2867012089810017,"ii
and wl−1"
HIERARCHICAL GRAPH GENERATION,0.28842832469775476,"ij
respectively, we can establish that these components are conditionally independent
137"
HIERARCHICAL GRAPH GENERATION,0.29015544041450775,"and each of them follow a multinomial distribution:
138"
HIERARCHICAL GRAPH GENERATION,0.2918825561312608,p(Gl | Gl−1) ⇠ Y
HIERARCHICAL GRAPH GENERATION,0.29360967184801384,i 2 V(Gl−1)
HIERARCHICAL GRAPH GENERATION,0.29533678756476683,Mu([we]e 2 Cl
HIERARCHICAL GRAPH GENERATION,0.2970639032815199,i | wl−1
HIERARCHICAL GRAPH GENERATION,0.2987910189982729,"ii , ✓l ii) ⇥ Y"
HIERARCHICAL GRAPH GENERATION,0.3005181347150259,"(i,j)2 E(Gl−1)"
HIERARCHICAL GRAPH GENERATION,0.3022452504317789,Mu([we]e 2 Bl
HIERARCHICAL GRAPH GENERATION,0.30397236614853196,ij | wl−1
HIERARCHICAL GRAPH GENERATION,0.30569948186528495,"ij , ✓l ij) (3)"
HIERARCHICAL GRAPH GENERATION,0.307426597582038,where {✓l
HIERARCHICAL GRAPH GENERATION,0.30915371329879104,"ij[e] 2 [0, 1], s.t. 1T ✓l"
HIERARCHICAL GRAPH GENERATION,0.31088082901554404,"ij = 1 | 8 (i, j) 2 E(Gl−1)} are the multinomial model parameters.
139"
HIERARCHICAL GRAPH GENERATION,0.3126079447322971,"Proof. The detailed proof can be found in Appendix A.1.
140"
HIERARCHICAL GRAPH GENERATION,0.3143350604490501,"Therefore, given the parent graph at a higher level, the generation of graph at its subsequent level
141"
HIERARCHICAL GRAPH GENERATION,0.3160621761658031,"can be reduced to generation of its partition and bipartite sub-graphs. As illustrated in ﬁgure, this
142"
HIERARCHICAL GRAPH GENERATION,0.3177892918825561,"decomposition enables parallel generation of the communities in each level which can be followed by
143"
HIERARCHICAL GRAPH GENERATION,0.31951640759930916,"predicting all bipartite sub-graphs in that level at one pass. Each of these sub-graphs corresponds
144"
HIERARCHICAL GRAPH GENERATION,0.32124352331606215,"to a block in the adjacency matrix, as visualized in ﬁgure 1b, so the proposed hierarchical model
145"
HIERARCHICAL GRAPH GENERATION,0.3229706390328152,"generates adjacency matrix in a blocks-wise fashion and constructs the ﬁnal graph topology.
146"
HIERARCHICAL GRAPH GENERATION,0.32469775474956825,"1Indeed, this assumption implies that the cross dependency between communities are primarily encoded by
their parent abstract graph which is reasonable where the nodes’ dependencies are mostly local and are within
community rather than being global."
COMMUNITY GENERATION,0.32642487046632124,"3.1
Community Generation
147"
COMMUNITY GENERATION,0.3281519861830743,"Based on the equation (3), the edge weights within each community can be jointly modeled using
148"
COMMUNITY GENERATION,0.3298791018998273,"a multinomial distribution. Our objective is to model the generative probability of communities in
149"
COMMUNITY GENERATION,0.3316062176165803,"each level as an autoregressive process. To accomplish this, we need to factorize the multinomial
150"
COMMUNITY GENERATION,0.3333333333333333,"distribution accordingly. Toward this goal, we present two different approaches in the following.
151"
COMMUNITY GENERATION,0.33506044905008636,Lemma 3.2. A random counting vector w 2 ZE
COMMUNITY GENERATION,0.33678756476683935,"+ with a multinomial distribution can be recursively
152"
COMMUNITY GENERATION,0.3385146804835924,"decomposed into a sequence of binomial distributions as follows:
153"
COMMUNITY GENERATION,0.34024179620034545,"Mu(w1, ..., wE | w, [✓1, ..., ✓E]) = E
Y e=1"
COMMUNITY GENERATION,0.34196891191709844,Bi(we | w − X
COMMUNITY GENERATION,0.3436960276338515,"i<e wi, ˆ✓e),
(4)"
COMMUNITY GENERATION,0.3454231433506045,"where: ˆ✓e =
✓e
1 −P"
COMMUNITY GENERATION,0.3471502590673575,i<e ✓i
COMMUNITY GENERATION,0.3488773747841105,"This decomposition is known as a stick-breaking process, where ˆ✓e is the fraction of the remaining
154"
COMMUNITY GENERATION,0.35060449050086356,"probabilities we take away every time and allocate to the e-th component (Linderman et al., 2015).
155"
COMMUNITY GENERATION,0.35233160621761656,"This lemma enable us to model the generation of a community as an edge-by-edge autoregressive
156"
COMMUNITY GENERATION,0.3540587219343696,"process, similar to existing algorithms such as GraphRNN (You et al., 2018) or DeepGMG (Li et al.,
157"
COMMUNITY GENERATION,0.35578583765112265,"2018) with O(|VC|2) generation steps. However, inspired by GRAN (Liao et al., 2019), a community
158"
COMMUNITY GENERATION,0.35751295336787564,"can be generated more efﬁciently by generating one node at a time. This requires decomposing the
159"
COMMUNITY GENERATION,0.3592400690846287,"generative probability of edges in a group-wise form, where the candidate edges between the t-th
160"
COMMUNITY GENERATION,0.3609671848013817,"node and the already generated graph are grouped together. In other words, this model completes the
161"
COMMUNITY GENERATION,0.3626943005181347,"lower triangle adjacency matrix one row at a time conditioned on the already generated sub-graph and
162"
COMMUNITY GENERATION,0.3644214162348877,"the parent-level graph. The following theorem formally derives this decomposition for multinomial
163"
COMMUNITY GENERATION,0.36614853195164077,"distributions.
164"
COMMUNITY GENERATION,0.36787564766839376,Theorem 3.3. For a random counting vector w 2 ZE
COMMUNITY GENERATION,0.3696027633851468,"+ with a multinomial distribution Mu(w | w, ✓),
165"
COMMUNITY GENERATION,0.37132987910189985,"let’s split it into M disjoint groups w = [u1, ..., uM] where um 2 ZEm"
COMMUNITY GENERATION,0.37305699481865284,"+
, PM"
COMMUNITY GENERATION,0.3747841105354059,"m=1 Em = E, and
166"
COMMUNITY GENERATION,0.3765112262521589,"also split the probability vector accordingly as ✓= [✓1, ..., ✓M]. Additionally, let’s deﬁne sum of all
167"
COMMUNITY GENERATION,0.37823834196891193,variables in the m-th group by a random count variable vm := PEm
COMMUNITY GENERATION,0.3799654576856649,"e=1 um,e. Then, the multinomial
168"
COMMUNITY GENERATION,0.38169257340241797,"distribution can be factorized as a chain of binomial and multinomial distributions:
169"
COMMUNITY GENERATION,0.38341968911917096,"Mu(w = [u1, ..., uM]| w, ✓= [✓1, ..., ✓M]) = M
Y m=1"
COMMUNITY GENERATION,0.385146804835924,Bi(vm | w − X i<m
COMMUNITY GENERATION,0.38687392055267705,"vi, ⌘vm) Mu(um | vm, λm),"
COMMUNITY GENERATION,0.38860103626943004,"where: ⌘vm =
1T ✓m
1 −P"
COMMUNITY GENERATION,0.3903281519861831,i<m 1T ✓i
COMMUNITY GENERATION,0.3920552677029361,", λm =
✓m
1T ✓m .
(5)"
COMMUNITY GENERATION,0.39378238341968913,"Here, the probability of binomial, ⌘vm, is the fraction of the remaining probability mass that is
170"
COMMUNITY GENERATION,0.3955094991364421,"allocated to vm, i.e. the sum of all weights in the m-th group. The vector parameter λm is the
171"
COMMUNITY GENERATION,0.39723661485319517,"normalized multinomial probabilities of all count variables in the m-th group. Intuitively, this
172"
COMMUNITY GENERATION,0.39896373056994816,"decomposition of multinomial distribution can be viewed as a recursive stick-breaking process where
173"
COMMUNITY GENERATION,0.4006908462867012,"at each step, ﬁrst a binomial distribution is used to determine how much probability mass to allocate
174"
COMMUNITY GENERATION,0.40241796200345425,"to the current group, and a multinomial distribution is used to distribute that probability mass
175"
COMMUNITY GENERATION,0.40414507772020725,"among the variables in the group. The resulting distribution is equivalent to the original multinomial
176"
COMMUNITY GENERATION,0.4058721934369603,"distribution.
177"
COMMUNITY GENERATION,0.4075993091537133,"Proof. Refer to appendix A.2 for the proof.
178"
COMMUNITY GENERATION,0.40932642487046633,Let ˆCl
COMMUNITY GENERATION,0.4110535405872193,"i,t denote an already generated sub-graph, at the t-th step, augmented with the set of candidate
179"
COMMUNITY GENERATION,0.41278065630397237,"edges, from the new node, vt(Cl"
COMMUNITY GENERATION,0.41450777202072536,"i), to its preceding node denoted by ˆEt( ˆCl"
COMMUNITY GENERATION,0.4162348877374784,"i,t) := {(t, j) | j < t}.
180"
COMMUNITY GENERATION,0.41796200345423146,We collect the weights of these edges in the random vector ut := [we]e 2 ˆEt( ˆCl
COMMUNITY GENERATION,0.41968911917098445,"i,t) (that is the t-th
181"
COMMUNITY GENERATION,0.4214162348877375,row of the lower triangle of adjacency matrix ˆAl
COMMUNITY GENERATION,0.4231433506044905,"i), where the sum of the candidate edge weights is
182"
COMMUNITY GENERATION,0.42487046632124353,"vt. Based on theorem 3.3, the probability of ut can be characterized by the product of a binomial
183"
COMMUNITY GENERATION,0.4265975820379965,"and a multinomial distribution. This process is illustrated in ﬁgure 1c. We further increase the
184"
COMMUNITY GENERATION,0.4283246977547496,"expressiveness of the generative network by extending this probability to a mixture model with K
185"
COMMUNITY GENERATION,0.43005181347150256,"mixtures:
186"
COMMUNITY GENERATION,0.4317789291882556,"p(ut) = K
X k=1 βl"
COMMUNITY GENERATION,0.43350604490500866,"kBi(vt|wl−1 ii
− X i<t"
COMMUNITY GENERATION,0.43523316062176165,"vi, ⌘l"
COMMUNITY GENERATION,0.4369602763385147,"t,k)Mu(ut |vt, λl"
COMMUNITY GENERATION,0.4386873920552677,"t,k)
(6) λl"
COMMUNITY GENERATION,0.44041450777202074,"t,k = softmax ⇣ MLPl ✓ % h"
COMMUNITY GENERATION,0.4421416234887737,∆h ˆEt( ˆCl
COMMUNITY GENERATION,0.4438687392055268,"i,t) || hP a(Cl i) i (⌘"
COMMUNITY GENERATION,0.44559585492227977,"[k, :]
(7) ⌘l"
COMMUNITY GENERATION,0.4473229706390328,"t,k = sigmoid ⇣ MLPl ⌘ % h"
COMMUNITY GENERATION,0.44905008635578586,pool(h ˆCl
COMMUNITY GENERATION,0.45077720207253885,"i,t) || hP a(Cl i) i (⌘ [k]"
COMMUNITY GENERATION,0.4525043177892919,βl = softmax ⇣ MLPl β % h
COMMUNITY GENERATION,0.4542314335060449,pool(h ˆCl
COMMUNITY GENERATION,0.45595854922279794,"i,t) || hP a(Cl i) i (⌘"
COMMUNITY GENERATION,0.45768566493955093,Where ∆h ˆEt( ˆCl
COMMUNITY GENERATION,0.459412780656304,"i,t) is a | ˆEt( ˆCl"
COMMUNITY GENERATION,0.46113989637305697,"i,t)| ⇥dh dimensional matrix, consisting of the set of edge features
187"
COMMUNITY GENERATION,0.46286701208981,"{∆h(t,s) := ht −hs | 8 (t, s)
2 ˆEt( ˆCl"
COMMUNITY GENERATION,0.46459412780656306,"i,t)} , h ˆCl"
COMMUNITY GENERATION,0.46632124352331605,"i,t is a t ⇥dh matrix of node features in the
188"
COMMUNITY GENERATION,0.4680483592400691,"augmented community graph. The mixture weights are denoted by βl. Here, the node features are
189"
COMMUNITY GENERATION,0.4697754749568221,"learned by GNN models and the graph level representation is obtained by the addpool() aggregation
190"
COMMUNITY GENERATION,0.47150259067357514,function. In order to produce K ⇥|Et(Cl
COMMUNITY GENERATION,0.47322970639032813,"i)| dimensional matrix of multinomial probabilities, the
191 MLPl"
COMMUNITY GENERATION,0.4749568221070812,"✓() network acts at the edge level, while MLPl"
COMMUNITY GENERATION,0.47668393782383417,⌘v() and MLPl
COMMUNITY GENERATION,0.4784110535405872,"β() act at the graph level to produce
192"
COMMUNITY GENERATION,0.48013816925734026,"the binomial probabilities and K dimensional arrays for K mixture models, respectively. All of these
193"
COMMUNITY GENERATION,0.48186528497409326,"MLP networks are built by two hidden layers with ReLU() activation functions.
194"
COMMUNITY GENERATION,0.4835924006908463,During the generation process of each community Cl
COMMUNITY GENERATION,0.4853195164075993,"i, the node features of its parent node hP a(Cl"
COMMUNITY GENERATION,0.48704663212435234,"i)
195"
COMMUNITY GENERATION,0.48877374784110533,"are used as the context. This context is concatenated to the node and edge feature matrices using the
196"
COMMUNITY GENERATION,0.4905008635578584,operation ⇥
COMMUNITY GENERATION,0.49222797927461137,x || y ⇤
COMMUNITY GENERATION,0.4939550949913644,", which concatenates vector y to each row of matrix x. The purpose of this context
197"
COMMUNITY GENERATION,0.49568221070811747,"is to enrich the node and edge features by capturing long-range interactions and encoding the global
198"
COMMUNITY GENERATION,0.49740932642487046,"structure of the graph, which is important for generating local components.
199"
BIPARTITE GENERATION,0.4991364421416235,"3.2
Bipartite Generation
200"
BIPARTITE GENERATION,0.5008635578583766,"Once all the communities in level l are generated, the edges of all bipartite graphs at that level can
201"
BIPARTITE GENERATION,0.5025906735751295,"be predicted simultaneously. An augmented graph ˆGl composed of all the communities, {Cl"
BIPARTITE GENERATION,0.5043177892918825,"i 8i 2
202"
BIPARTITE GENERATION,0.5060449050086355,"V(Gl−1)}, and the candidate edges of all bipartites, {Bl"
BIPARTITE GENERATION,0.5077720207253886,"ij 8(i, j) 2 E(Gl−1)}, is used as the input
203"
BIPARTITE GENERATION,0.5094991364421416,"of a GNN to obtain node and edge features. We similarly extend the multinomial distribution of a
204"
BIPARTITE GENERATION,0.5112262521588946,"bipartite, (12), using a mixture model to express its generative probability:
205"
BIPARTITE GENERATION,0.5129533678756477,p(w := ˆE(Bl
BIPARTITE GENERATION,0.5146804835924007,"ij)) = K
X k=1 βl"
BIPARTITE GENERATION,0.5164075993091537,kMu(w | wl−1
BIPARTITE GENERATION,0.5181347150259067,"ij , ✓l ij,k) ✓l"
BIPARTITE GENERATION,0.5198618307426598,"ij,k = softmax ⇣ MLPl ✓( h"
BIPARTITE GENERATION,0.5215889464594128,∆h ˆE(Bl
BIPARTITE GENERATION,0.5233160621761658,ij) || ∆hP a(Bl ij) i ) ⌘
BIPARTITE GENERATION,0.5250431778929189,"[k, :]
(8)"
BIPARTITE GENERATION,0.5267702936096719,βl = softmax ⇣ MLPl β % h
BIPARTITE GENERATION,0.5284974093264249,pool(∆h ˆE(Bl
BIPARTITE GENERATION,0.5302245250431779,ij)) || ∆hP a(Bl ij) i (⌘
BIPARTITE GENERATION,0.531951640759931,where the random vector w := [we]e 2 ˆE(Bl
BIPARTITE GENERATION,0.533678756476684,"ij) is the set of weights of all candidate edges in bipartite
206 Bl"
BIPARTITE GENERATION,0.5354058721934369,ij and ∆hP a(Bl
BIPARTITE GENERATION,0.5371329879101899,"ij) are the parent edge features of the bipartite graph.
207"
BIPARTITE GENERATION,0.538860103626943,"Node Feature Encoding:
To encode node features, we extend GraphGPS proposed by Rampášek
208"
BIPARTITE GENERATION,0.540587219343696,"et al. (2022). GraphGPS combines local message-passing with global attention mechanism and uses
209"
BIPARTITE GENERATION,0.542314335060449,"positional and structural encoding for nodes and edges to construct a more expressive and a scalable
210"
BIPARTITE GENERATION,0.5440414507772021,"graph transformer (GT) (Dwivedi & Bresson, 2020). To apply GraphGPS on augmented graphs,
211"
BIPARTITE GENERATION,0.5457685664939551,"we use distinct initial edge features to distinguish augmented (candidate) edges from real edges.
212"
BIPARTITE GENERATION,0.5474956822107081,"Furthermore, for bipartite generation, the attention scores in the Transformers of the augmented graph
213"
BIPARTITE GENERATION,0.5492227979274611,"ˆGl are masked to restrict attention only to connected communities. The details of model architecture
214"
BIPARTITE GENERATION,0.5509499136442142,"are provided in appendix B.
215"
RELATED WORK,0.5526770293609672,"4
Related Work
216"
RELATED WORK,0.5544041450777202,"In order to deal with hierarchical structures in molecular graphs, a generative process was proposed
217"
RELATED WORK,0.5561312607944733,"by Jin et al. (2020) which recursively selects motifs, the basic building blocks, from a set and
218"
RELATED WORK,0.5578583765112263,"predicts their attachment to the emerging molecule. However, this method requires prior domain-
219"
RELATED WORK,0.5595854922279793,"speciﬁc knowledge and relies on molecule-speciﬁc graph motifs. Additionally, the graphs are
220"
RELATED WORK,0.5613126079447323,"only abstracted into two levels, and component generation cannot be performed in parallel. In
221"
RELATED WORK,0.5630397236614854,"(Kuznetsov & Polykovskiy, 2021), a hierarchical normalizing ﬂow model for molecular graphs was
222"
RELATED WORK,0.5647668393782384,"introduced, where new molecules are generated from a single node by recursively dividing each
223"
RELATED WORK,0.5664939550949913,"node into two. However, the merging and splitting of pairs of nodes in this model is based on the
224"
RELATED WORK,0.5682210708117443,"node’s neighborhood, and do not consider the diverse community structure of graphs, therefore the
225"
RELATED WORK,0.5699481865284974,"hierarchical generation of this model is inherently limited.
226"
EXPERIMENTS,0.5716753022452504,"5
Experiments
227"
EXPERIMENTS,0.5734024179620034,"In our empirical studies, we compare the proposed hierarchical graph generative network against
228"
EXPERIMENTS,0.5751295336787565,"state-of-the-art autoregressive models: GRAN and GraphRNN models, diffusion models: DiGress
229"
EXPERIMENTS,0.5768566493955095,"(Vignac et al., 2022) and GDSS (Jo et al., 2022) and a GAN-based model: SPECTRE (Martinkus
230"
EXPERIMENTS,0.5785837651122625,"et al., 2022), on a range of synthetics and real datasets of various sizes.
231"
EXPERIMENTS,0.5803108808290155,"Datasets:
We used 4 different benchmark graph datasets: (1) the synthetic Stochastic Block Model
232"
EXPERIMENTS,0.5820379965457686,"(SBM) dataset consisting of 200 graphs with 2-5 communities each with 20-40 nodes, used in a
233"
EXPERIMENTS,0.5837651122625216,"previous work (Martinkus et al., 2022); (2) the Protein including 918 protein graphs, each has 100 to
234"
EXPERIMENTS,0.5854922279792746,"500 nodes representing amino acids that are linked if they are closer than 6 Angstroms (Dobson &
235"
EXPERIMENTS,0.5872193436960277,"Doig, 2003), (3) the Enzyme that has 587 protein graphs of 10-125 nodes, representing protein tertiary
236"
EXPERIMENTS,0.5889464594127807,"structures of the enzymes from the BRENDA database (Schomburg et al., 2004) and (4) the Ego
237"
EXPERIMENTS,0.5906735751295337,"dataset containing 757 3-hop ego networks with 50-300 nodes extracted from the CiteSeer dataset,
238"
EXPERIMENTS,0.5924006908462867,"where nodes represent documents and edges represent citation relationships (Sen et al., 2008).
239"
EXPERIMENTS,0.5941278065630398,"Graph Partitioning
Different algorithms approach the problem of graph partitioning (clustering)
240"
EXPERIMENTS,0.5958549222797928,"using various clustering quality functions. Two commonly used families of such metrics are modu-
241"
EXPERIMENTS,0.5975820379965457,"larity and cut-based metrics (Tsitsulin et al., 2020). Although optimizing modularity metric is an
242"
EXPERIMENTS,0.5993091537132987,"NP-hard problem, it is well-studied in the literature and several graph partitioning algorithm based on
243"
EXPERIMENTS,0.6010362694300518,"this metric have been proposed. For example, the Louvain algorithm (Blondel et al., 2008) starts with
244"
EXPERIMENTS,0.6027633851468048,"each node as its community and then repeatedly merges communities based on the highest increase
245"
EXPERIMENTS,0.6044905008635578,"in modularity until no further improvement can be made. This heuristic algorithm is computationally
246"
EXPERIMENTS,0.6062176165803109,"efﬁcient and scalable to large graphs for community detection. Moreover, a spectral relaxation of
247"
EXPERIMENTS,0.6079447322970639,"modularity metrics has been proposed in Newman (2006a,b) which results in an analytically solution
248"
EXPERIMENTS,0.6096718480138169,"for graph partitioning. Additionally, an unsupervised GNN-based pooling method inspired by this
249"
EXPERIMENTS,0.6113989637305699,"spectral relaxation was proposed for partitioning graphs with node attributes (Tsitsulin et al., 2020).
250"
EXPERIMENTS,0.613126079447323,"As the modularity metric is based on the graph structure, it is well-suited for our problem. Therefore,
251"
EXPERIMENTS,0.614853195164076,"we employed the Louvain algorithm to hierarchically cluster the graph datasets in our experiments
252"
EXPERIMENTS,0.616580310880829,"and then spliced out the intermediate levels to achieve HGs with uniform depth of L = 2.
253"
EXPERIMENTS,0.6183074265975821,"Model Architecture
In the experiments, the GNN models consist of 8 layers of GraphGPS layers
254"
EXPERIMENTS,0.6200345423143351,"(Rampášek et al., 2022). The input node feature of GNNs is augmented with positional and structural
255"
EXPERIMENTS,0.6217616580310881,"encoding, where the ﬁrst 8 eigenvectors corresponding to the smallest non-zero eigenvalues of the
256"
EXPERIMENTS,0.6234887737478411,"Laplacian and diagonal of the random-walk matrix up to 8-steps are used. Each level has its own
257"
EXPERIMENTS,0.6252158894645942,"GNN and output models. The details of the model architecture are presented in Appendix B and C.
258"
EXPERIMENTS,0.6269430051813472,"We conducted experiments using the proposed hierarchical graph generative network (HiGen) model
259"
EXPERIMENTS,0.6286701208981001,"with two variants for the output distribution of the leaf edges: 1) HiGen: the probability of the
260"
EXPERIMENTS,0.6303972366148531,"community edges’ weights at the leaf level are modeled by mixture of Bernoulli, using sigmoid()
261"
EXPERIMENTS,0.6321243523316062,"activation in equation 7, since the leaf levels in our experiments have binary edges weights, while
262"
EXPERIMENTS,0.6338514680483592,"higher levels use mixture of multinomials. 2)HiGen-m: the model uses a mixture of multinomial
263"
EXPERIMENTS,0.6355785837651122,"distributions (6) to describe the output distribution for all levels. In this case, we observed that
264"
EXPERIMENTS,0.6373056994818653,"modeling the probability parameters of edge weights of the leaf level, denoted as λt,k in (7), by a multi-
265"
EXPERIMENTS,0.6390328151986183,"hot activation function, deﬁned as σ(z)i := sigmoid(zi)/PK"
EXPERIMENTS,0.6407599309153713,"j=1 sigmoid(zj) where σ : RK ! (K −1)-
266"
EXPERIMENTS,0.6424870466321243,"simplex, provided slightly better performance than the standard softmax() function. However, for
267"
EXPERIMENTS,0.6442141623488774,"Table 1: Comparison of generation metrics on benchmark datasets.
The baseline results for SBM and Protein graphs are obtained from
(Martinkus et al., 2022; Vignac et al., 2022), and the results for enzyme
graphs (except for GRAN, which we implemented) are obtained from
(Jo et al., 2022), while we implemented them for Ego. ""-"": not applica-
ble due to resource issue or not reported in the reference papers."
EXPERIMENTS,0.6459412780656304,"Stochastic block model
Protein"
EXPERIMENTS,0.6476683937823834,"Model
Deg. #
Clus. #
Orbit#
Spec. #
Deg. #
Clus. #
Orbit#
Spec. #"
EXPERIMENTS,0.6493955094991365,"Training set
0.0008
0.0332
0.0255
0.0063
0.0003
0.0068
0.0032
0.0009"
EXPERIMENTS,0.6511226252158895,"GraphRNN
0.0055
0.0584
0.0785
0.0065
0.0040
0.1475
0.5851
0.0152"
EXPERIMENTS,0.6528497409326425,"GRAN
0.0113
0.0553
0.0540
0.0054
0.0479
0.1234
0.3458
0.0125"
EXPERIMENTS,0.6545768566493955,"SPECTRE
0.0015
0.0521
0.0412
0.0056
0.0056
0.0843
0.0267
0.0052"
EXPERIMENTS,0.6563039723661486,"DiGress
0.0013
0.0498
0.0433
-
-
-
-
-"
EXPERIMENTS,0.6580310880829016,"HiGen-m
0.0017
0.0503
0.0604
0.0068
0.0041
0.109
0.0472
0.0061"
EXPERIMENTS,0.6597582037996546,"HiGen
0.0019
0.0498
0.0352
0.0046
0.0012
0.0435
0.0234
0.0025"
EXPERIMENTS,0.6614853195164075,"Enzyme
Ego"
EXPERIMENTS,0.6632124352331606,"Model
Deg. #
Clus. #
Orbit #
Deg. #
Clus. #
Orbit #
Spec. #"
EXPERIMENTS,0.6649395509499136,"Training set
0.0011
0.0025
3.7e-4
2.2e-4
0.010
0.012
1.4e-3"
EXPERIMENTS,0.6666666666666666,"GraphRNN
0.017
0.062
0.046
0.024
0.34
0.14
0.089"
EXPERIMENTS,0.6683937823834197,"GRAN
0.054
0.087
0.033
0.032
0.17
0.026
0.046"
EXPERIMENTS,0.6701208981001727,"GDSS
0.026
0.061
0.009
-
-
-
-"
EXPERIMENTS,0.6718480138169257,"HiGen-m
0.027
0.157
1.2e-3
0.011
0.063
0.021
0.013"
EXPERIMENTS,0.6735751295336787,"HiGen
0.012
0.038
7.2e-4
1.9e-3
0.049
0.029
0.004"
EXPERIMENTS,0.6753022452504318,"(a)
(b)"
EXPERIMENTS,0.6770293609671848,"(c)
(d)"
EXPERIMENTS,0.6787564766839378,"Figure 2: Samples from HiGen. a) SBM,
b) Protein, c) Enzyme and d) Ego. Com-
munities are distinguished with different
colors and both levels are depicted."
EXPERIMENTS,0.6804835924006909,"both HiGen and HiGen-m, the probabilities of the integer-valued edges at the higher levels are still
268"
EXPERIMENTS,0.6822107081174439,"modeled by the standard softmax() function.2
269"
EXPERIMENTS,0.6839378238341969,"For training, HiGen models used the Adam optimizer Kingma & Ba (2014) with a learning rate of
270"
EXPERIMENTS,0.6856649395509499,"5e-4 and its default settings of β1 = 0.9, β2 = 0.999 and ✏=1e-8.
271"
EXPERIMENTS,0.687392055267703,"Metrics
To evaluate the graph generative models, we adopt the approach proposed in (Liu et al.,
272"
EXPERIMENTS,0.689119170984456,"2019; Liao et al., 2019), which compares the distributions of four different graph statistics between
273"
EXPERIMENTS,0.690846286701209,"the ground truth and generated graphs: (1) degree distributions, (2) clustering coefﬁcient distributions,
274"
EXPERIMENTS,0.6925734024179621,"(3) the number of occurrences of all orbits with four nodes, and (4) the spectra of the graphs by
275"
EXPERIMENTS,0.694300518134715,"computing the eigenvalues of the normalized graph Laplacian. The ﬁrst three metrics capture local
276"
EXPERIMENTS,0.696027633851468,"graph statistics, while the spectra represents global structure. The maximum mean discrepancy
277"
EXPERIMENTS,0.697754749568221,"(MMD) score over these statistics are used as the metrics. While Liu et al. (2019) computed MMD
278"
EXPERIMENTS,0.6994818652849741,"scores using the computationally expensive Gaussian earth mover’s distance (EMD) kernel, Liao
279"
EXPERIMENTS,0.7012089810017271,"et al. (2019) proposed using the total variation (TV) distance as an alternative measure. TV distance
280"
EXPERIMENTS,0.7029360967184801,"is much faster and still consistent with the Gaussian EMD kernel. Most recently, O’Bray et al. (2021)
281"
EXPERIMENTS,0.7046632124352331,"suggested using other efﬁcient kernels such as an RBF kernel, or a Laplacian kernel, or a linear
282"
EXPERIMENTS,0.7063903281519862,"kernel. Additionally, Thompson et al. (2022) proposed new evaluation metrics for comparing graph
283"
EXPERIMENTS,0.7081174438687392,"sets using a random-GNN approach where GNNs are employed to extract meaningful graph features.
284"
EXPERIMENTS,0.7098445595854922,"However, in this work, we follow the experimental setup and evaluation metrics of (Liao et al., 2019),
285"
EXPERIMENTS,0.7115716753022453,"except for the enzyme dataset where we use a Gaussian EMD kernel to be consistent with the results
286"
EXPERIMENTS,0.7132987910189983,"reported in (Jo et al., 2022). GNN-based performance metrics of HiGen model are also reported in
287"
EXPERIMENTS,0.7150259067357513,"appendix D.2.
288"
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7167530224525043,"2As the leaf levels have binary edge weights while the sum of their weights is determined by their parent
edge, a possible extension to this work could be using the cardinality potential model (Hajimirsadeghi et al.,
2015), which is derived to model the distribution over the set of binary random variables, to model the edge
weight at the leaf level."
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7184801381692574,"The performance metrics of the proposed HiGen models are reported in Table 1, with generated graph
289"
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7202072538860104,"samples presented in Figure 2. The results demonstrate that HiGen effectively captures graph statistics
290"
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7219343696027634,"and achieves state-of-the-art on all the benchmarks graphs across various generation metrics. This
291"
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7236614853195165,"improvement in both local and global properties of the generated graphs highlights the effectiveness
292"
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7253886010362695,"of the hierarchical graph generation approach, which models communities and cross-community
293"
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7271157167530224,"interactions separately. The visual comparisons of graph samples generated by the HiGen models,
294"
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7288428324697754,"as well as the experimental evaluation of different node ordering and partitioning functions, are
295"
AS THE LEAF LEVELS HAVE BINARY EDGE WEIGHTS WHILE THE SUM OF THEIR WEIGHTS IS DETERMINED BY THEIR PARENT,0.7305699481865285,"presented in Appendix D.2.
296"
DISCUSSION,0.7322970639032815,"6
Discussion
297"
DISCUSSION,0.7340241796200345,"The GRAN model can generate graphs one block of nodes at a time in an autoregressive fashion where
298"
DISCUSSION,0.7357512953367875,"the block size is ﬁxed and nodes are assigned to blocks based on an ordering. However, the model’s
299"
DISCUSSION,0.7374784110535406,"performance deteriorates as the block size increases, since adjacent nodes in an ordering may not be
300"
DISCUSSION,0.7392055267702936,"relevant and may belong to different clusters. Additionally, intra-block connections are not modeled
301"
DISCUSSION,0.7409326424870466,"separately. In contrast, our proposed method generates blocks of nodes within each community that
302"
DISCUSSION,0.7426597582037997,"have strong relationships and then predicts the cross-links between communities using a separate
303"
DISCUSSION,0.7443868739205527,"model. As a result, this approach enables the model to capture both local relationships between
304"
DISCUSSION,0.7461139896373057,"nodes within a community and global relationships across communities, resulting in improved
305"
DISCUSSION,0.7478411053540587,"expressiveness of the graph generative model.
306"
DISCUSSION,0.7495682210708118,"The proposed hierarchical model allows for highly parallelizable training and generation. Speciﬁcally,
307"
DISCUSSION,0.7512953367875648,"let nc be the size of the largest graph cluster, then, it only requires O(nc log n) sequential steps to
308"
DISCUSSION,0.7530224525043178,"generate a graph of size n.
309"
DISCUSSION,0.7547495682210709,"Node ordering sensitivity
The predeﬁned ordering of dimensions can be crucial for training
310"
DISCUSSION,0.7564766839378239,"autoregressive (AR) models Vinyals et al. (2015), and this sensitivity to node orderings is particularly
311"
DISCUSSION,0.7582037996545768,"pronounced in autoregressive graph generative model Liao et al. (2019); Chen et al. (2021). However,
312"
DISCUSSION,0.7599309153713298,"in the proposed approach, the graph generation process is divided into the generation of multiple
313"
DISCUSSION,0.7616580310880829,"small partitions, performed sequentially across the levels, rather than generating the entire graph
314"
DISCUSSION,0.7633851468048359,"by a single AR model. Therefore, given an ordering for the parent level, the graph generation
315"
DISCUSSION,0.7651122625215889,"depends only on the permutation of the nodes within the graph communities rather than the node
316"
DISCUSSION,0.7668393782383419,"ordering of the entire graph. In other words, the proposed method is invariant to a large portion of
317"
DISCUSSION,0.768566493955095,"possible node permutations, and therefore the set of distinctive adjacency matrices is much smaller
318"
DISCUSSION,0.770293609671848,"in HiGen. For example, the node ordering ⇡1 = [v1, v2, v3, v4] with clusters VG1 = {v1, v2} and
319"
DISCUSSION,0.772020725388601,"VG2 = {v3, v4} has a similar hierarchical graph as ⇡2 = [v1, v3, v2, v4], since the node ordering
320"
DISCUSSION,0.7737478411053541,"within the communities is preserved at all levels. Formally, let {Cl"
DISCUSSION,0.7754749568221071,"i 8i 2 VGl−1} be the set of
321"
DISCUSSION,0.7772020725388601,"communities at level l produced by a deterministic partitioning function, where nl"
DISCUSSION,0.7789291882556131,i = |V(Cl
DISCUSSION,0.7806563039723662,"i)| denotes
322"
DISCUSSION,0.7823834196891192,"the size of each partition. The upper bound on the number of distinct node orderings in an HG
323"
DISCUSSION,0.7841105354058722,generated by the proposed process is then reduced to QL l=1 Q i nl
DISCUSSION,0.7858376511226253,"i!. 3
324"
CONCLUSION,0.7875647668393783,"7
Conclusion
325"
CONCLUSION,0.7892918825561313,"The proposed HiGen framework generates graphs in a hierarchical and block-wise manner, leveraging
326"
CONCLUSION,0.7910189982728842,"the inherent hierarchical structure present in real-world graphs. By decomposing the generation
327"
CONCLUSION,0.7927461139896373,"process into separate and parallel generation of communities and bipartite sub-graphs, it combines the
328"
CONCLUSION,0.7944732297063903,"beneﬁts of one-shot and AR graph generative models. Experimental results on benchmark datasets
329"
CONCLUSION,0.7962003454231433,"demonstrate that HiGen achieves state-of-the-art performance across various generation metrics. The
330"
CONCLUSION,0.7979274611398963,"hierarchical and block-wise generation strategy of HiGen enables scaling up graph generative models
331"
CONCLUSION,0.7996545768566494,"to large and complex graphs, opening up opportunities to extend it to newer generative paradigms
332"
CONCLUSION,0.8013816925734024,"such as diffusion models.
333"
IT IS WORTH NOTING THAT ALL NODE PERMUTATIONS DO NOT RESULT IN DISTINCTIVE ADJACENCY MATRICES DUE TO THE,0.8031088082901554,"3It is worth noting that all node permutations do not result in distinctive adjacency matrices due to the
automorphism property of graphs Liao et al. (2019); Chen et al. (2021). Therefore, the number of node
permutations provides an upper bound rather than an exact count."
REFERENCES,0.8048359240069085,"References
334"
REFERENCES,0.8065630397236615,"Albert-László Barabási and Réka Albert. Emergence of scaling in random networks. science, 286(5439):
335"
REFERENCES,0.8082901554404145,"509–512, 1999.
336"
REFERENCES,0.8100172711571675,"David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning
337"
REFERENCES,0.8117443868739206,"research, 3(Jan):993–1022, 2003.
338"
REFERENCES,0.8134715025906736,"Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre.
Fast unfolding of
339"
REFERENCES,0.8151986183074266,"communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008,
340"
REFERENCES,0.8169257340241797,"2008.
341"
REFERENCES,0.8186528497409327,"Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zügner, and Stephan Günnemann. Netgan: Generating graphs
342"
REFERENCES,0.8203799654576857,"via random walks. arXiv preprint arXiv:1803.00816, 2018.
343"
REFERENCES,0.8221070811744386,"Xiaohui Chen, Xu Han, Jiajing Hu, Francisco JR Ruiz, and Liping Liu. Order matters: Probabilistic modeling of
344"
REFERENCES,0.8238341968911918,"node sequence for graph generation. arXiv preprint arXiv:2106.06189, 2021.
345"
REFERENCES,0.8255613126079447,"Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,
346"
REFERENCES,0.8272884283246977,"Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.
347"
REFERENCES,0.8290155440414507,"arXiv preprint arXiv:2009.14794, 2020.
348"
REFERENCES,0.8307426597582038,"Hanjun Dai, Azade Nazi, Yujia Li, Bo Dai, and Dale Schuurmans. Scalable deep generative modeling for sparse
349"
REFERENCES,0.8324697754749568,"graphs. In International Conference on Machine Learning, pp. 2302–2312. PMLR, 2020.
350"
REFERENCES,0.8341968911917098,"Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. arXiv
351"
REFERENCES,0.8359240069084629,"preprint arXiv:1805.11973, 2018.
352"
REFERENCES,0.8376511226252159,"Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments.
353"
REFERENCES,0.8393782383419689,"Journal of molecular biology, 330(4):771–783, 2003.
354"
REFERENCES,0.8411053540587219,"Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. arXiv preprint
355"
REFERENCES,0.842832469775475,"arXiv:2012.09699, 2020.
356"
REFERENCES,0.844559585492228,"Paul Erdos and Alfréd Rényi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci, 5(1):17–60,
357"
REFERENCES,0.846286701208981,"1960.
358"
REFERENCES,0.8480138169257341,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
359"
REFERENCES,0.8497409326424871,"Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):
360"
REFERENCES,0.8514680483592401,"139–144, 2020.
361"
REFERENCES,0.853195164075993,"Kilian Konstantin Haefeli, Karolis Martinkus, Nathanaël Perraudin, and Roger Wattenhofer. Diffusion models
362"
REFERENCES,0.8549222797927462,"for graphs beneﬁt from discrete state spaces. arXiv preprint arXiv:2210.01549, 2022.
363"
REFERENCES,0.8566493955094991,"Hossein Hajimirsadeghi, Wang Yan, Arash Vahdat, and Greg Mori. Visual recognition by counting instances: A
364"
REFERENCES,0.8583765112262521,"multi-instance cardinality potential kernel. In Proceedings of the IEEE conference on computer vision and
365"
REFERENCES,0.8601036269430051,"pattern recognition, pp. 2596–2605, 2015.
366"
REFERENCES,0.8618307426597582,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs using structural
367"
REFERENCES,0.8635578583765112,"motifs. In International conference on machine learning, pp. 4839–4848. PMLR, 2020.
368"
REFERENCES,0.8652849740932642,"Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of
369"
REFERENCES,0.8670120898100173,"stochastic differential equations. In International Conference on Machine Learning, pp. 10362–10383. PMLR,
370"
REFERENCES,0.8687392055267703,"2022.
371"
REFERENCES,0.8704663212435233,"Mahdi Karami, Dale Schuurmans, Jascha Sohl-Dickstein, Laurent Dinh, and Daniel Duckworth. Invertible
372"
REFERENCES,0.8721934369602763,"convolutional ﬂow. Advances in Neural Information Processing Systems, 32, 2019.
373"
REFERENCES,0.8739205526770294,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
374"
REFERENCES,0.8756476683937824,"2014.
375"
REFERENCES,0.8773747841105354,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
376"
REFERENCES,0.8791018998272885,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv
377"
REFERENCES,0.8808290155440415,"preprint arXiv:1609.02907, 2016.
378"
REFERENCES,0.8825561312607945,"Maksim Kuznetsov and Daniil Polykovskiy. Molgrow: A graph normalizing ﬂow for hierarchical molecular
379"
REFERENCES,0.8842832469775475,"generation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 8226–8234,
380"
REFERENCES,0.8860103626943006,"2021.
381"
REFERENCES,0.8877374784110535,"Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani. Kronecker
382"
REFERENCES,0.8894645941278065,"graphs: An approach to modeling networks. Journal of Machine Learning Research, 11(Feb):985–1042,
383"
REFERENCES,0.8911917098445595,"2010.
384"
REFERENCES,0.8929188255613126,"Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative models of
385"
REFERENCES,0.8946459412780656,"graphs. arXiv preprint arXiv:1803.03324, 2018.
386"
REFERENCES,0.8963730569948186,"Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun, and
387"
REFERENCES,0.8981001727115717,"Richard Zemel. Efﬁcient graph generation with graph recurrent attention networks. Advances in neural
388"
REFERENCES,0.8998272884283247,"information processing systems, 32, 2019.
389"
REFERENCES,0.9015544041450777,"Scott Linderman, Matthew J Johnson, and Ryan P Adams. Dependent multinomial models made easy: Stick-
390"
REFERENCES,0.9032815198618307,"breaking with the pólya-gamma augmentation. Advances in Neural Information Processing Systems, 28,
391"
REFERENCES,0.9050086355785838,"2015.
392"
REFERENCES,0.9067357512953368,"Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing ﬂows, 2019.
393"
REFERENCES,0.9084628670120898,"Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via regularizing
394"
REFERENCES,0.9101899827288429,"variational autoencoders. arXiv preprint arXiv:1809.02630, 2018.
395"
REFERENCES,0.9119170984455959,"Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub,
396"
REFERENCES,0.9136442141623489,"Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI
397"
REFERENCES,0.9153713298791019,"Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.
398"
REFERENCES,0.917098445595855,"Karolis Martinkus, Andreas Loukas, Nathanaël Perraudin, and Roger Wattenhofer. Spectre: Spectral conditioning
399"
REFERENCES,0.918825561312608,"helps to overcome the expressivity limits of one-shot graph generators. In International Conference on
400"
REFERENCES,0.9205526770293609,"Machine Learning, pp. 15159–15179. PMLR, 2022.
401"
REFERENCES,0.9222797927461139,"Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon
402"
REFERENCES,0.924006908462867,"Lee, Eric Johnson, Omkar Pathak, Azade Nazi, et al. A graph placement methodology for fast chip design.
403"
REFERENCES,0.92573402417962,"Nature, 594(7862):207–212, 2021.
404"
REFERENCES,0.927461139896373,"Mark EJ Newman. Finding community structure in networks using the eigenvectors of matrices. Physical review
405"
REFERENCES,0.9291882556131261,"E, 74(3):036104, 2006a.
406"
REFERENCES,0.9309153713298791,"Mark EJ Newman. Modularity and community structure in networks. Proceedings of the national academy of
407"
REFERENCES,0.9326424870466321,"sciences, 103(23):8577–8582, 2006b.
408"
REFERENCES,0.9343696027633851,"Leslie O’Bray, Max Horn, Bastian Rieck, and Karsten Borgwardt. Evaluation metrics for graph generative
409"
REFERENCES,0.9360967184801382,"models: Problems, pitfalls, and practical solutions. arXiv preprint arXiv:2106.01098, 2021.
410"
REFERENCES,0.9378238341968912,"Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalch-
411"
REFERENCES,0.9395509499136442,"brenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint
412"
REFERENCES,0.9412780656303973,"arXiv:1609.03499, 2016.
413"
REFERENCES,0.9430051813471503,"Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg,
414"
REFERENCES,0.9447322970639033,"John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva,
415"
REFERENCES,0.9464594127806563,"Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for
416"
REFERENCES,0.9481865284974094,"embodied AI. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks
417"
REFERENCES,0.9499136442141624,"Track (Round 2), 2021.
418"
REFERENCES,0.9516407599309153,"Ladislav Rampášek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini.
419"
REFERENCES,0.9533678756476683,"Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing
420"
REFERENCES,0.9550949913644214,"Systems, 35:14501–14515, 2022.
421"
REFERENCES,0.9568221070811744,"Scott Reed, Aäron Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Yutian Chen, Dan Belov,
422"
REFERENCES,0.9585492227979274,"and Nando Freitas. Parallel multiscale autoregressive density estimation. In International Conference on
423"
REFERENCES,0.9602763385146805,"Machine Learning, pp. 2912–2921. PMLR, 2017.
424"
REFERENCES,0.9620034542314335,"Ida Schomburg, Antje Chang, Christian Ebeling, Marion Gremse, Christian Heldt, Gregor Huhn, and Dietmar
425"
REFERENCES,0.9637305699481865,"Schomburg. Brenda, the enzyme database: updates and major new developments. Nucleic acids research, 32
426"
REFERENCES,0.9654576856649395,"(suppl_1):D431–D433, 2004.
427"
REFERENCES,0.9671848013816926,"Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective
428"
REFERENCES,0.9689119170984456,"classiﬁcation in network data. AI magazine, 29(3):93–93, 2008.
429"
REFERENCES,0.9706390328151986,"Kyle Siegrist.
Probability,
Mathematical Statistics,
Stochastic Processes.
LibreTexts,
2017.
430"
REFERENCES,0.9723661485319517,"URL
https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_
431"
REFERENCES,0.9740932642487047,"Mathematical_Statistics_and_Stochastic_Processes_(Siegrist).
432"
REFERENCES,0.9758203799654577,"Martin Simonovsky and Nikos Komodakis. GraphVAE: Towards generation of small graphs using variational
433"
REFERENCES,0.9775474956822107,"autoencoders. arXiv preprint arXiv:1802.03480, 2018.
434"
REFERENCES,0.9792746113989638,"Rylee Thompson, Boris Knyazev, Elahe Ghalebi, Jungtaek Kim, and Graham W Taylor. On evaluation metrics
435"
REFERENCES,0.9810017271157168,"for graph generative models. arXiv preprint arXiv:2201.09871, 2022.
436"
REFERENCES,0.9827288428324698,"Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel Müller. Graph clustering with graph neural
437"
REFERENCES,0.9844559585492227,"networks. arXiv preprint arXiv:2006.16904, 2020.
438"
REFERENCES,0.9861830742659758,"Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress:
439"
REFERENCES,0.9879101899827288,"Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022.
440"
REFERENCES,0.9896373056994818,"Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. arXiv
441"
REFERENCES,0.9913644214162349,"preprint arXiv:1511.06391, 2015.
442"
REFERENCES,0.9930915371329879,"Carl Yang, Peiye Zhuang, Wenhan Shi, Alan Luu, and Pan Li. Conditional structure generation through graph
443"
REFERENCES,0.9948186528497409,"variational generative adversarial nets. Advances in neural information processing systems, 32, 2019.
444"
REFERENCES,0.9965457685664939,"Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generating realistic
445"
REFERENCES,0.998272884283247,"graphs with deep auto-regressive models. In ICML, pp. 5694–5703, 2018.
446"
