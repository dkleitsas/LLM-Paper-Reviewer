Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006134969325153374,"Stochastic gradient descent and other ﬁrst-order variants, such as Adam and Ada-
Grad, are commonly used in the ﬁeld of deep learning due to their computational
efﬁciency and low-storage memory requirements. However, these methods do
not exploit curvature information. Consequently, iterates can converge to sad-
dle points and poor local minima. To avoid this, directions of negative curvature
can be utilized, which requires computing the second-derivative matrix. In Deep
Neural Networks (DNNs), the number of variables (n) can be of the order of
tens of millions, making the Hessian impractical to store (O(n2)) and to invert
(O(n3)). Alternatively, quasi-Newton methods compute Hessian approximations
that do not have the same computational requirements. Quasi-Newton methods
re-use previously computed iterates and gradients to compute a low-rank struc-
tured update. The most widely used quasi-Newton update is the L-BFGS, which
guarantees a positive semi-deﬁnite Hessian approximation, making it suitable in
a line search setting. However, the loss function in DNNs are non-convex, where
the Hessian is potentially non-positive deﬁnite. In this paper, we propose using
a Limited-Memory Symmetric Rank-1 quasi-Newton approach which allows for
indeﬁnite Hessian approximations, enabling directions of negative curvature to
be exploited. Furthermore, we use a modiﬁed Adaptive Regularized Cubics ap-
proach, which generates a sequence of cubic subproblems that have closed-form
solutions. We investigate the performance of our proposed method on autoen-
coders and feed-forward neural network models and compare our approach to
state-of-the-art ﬁrst-order adaptive stochastic methods as well as L-BFGS."
INTRODUCTION,0.012269938650306749,"1
INTRODUCTION"
INTRODUCTION,0.018404907975460124,Most deep learning problems involve minimization of the empirical risk of estimation
INTRODUCTION,0.024539877300613498,"min
Θ f(x; Θ),
(1)"
INTRODUCTION,0.03067484662576687,"where Θ ∈Rn is the set of weights and f is some scalar-valued loss function. To solve (1), various
optimization approaches have been implemented, which we describe below. Throughout this paper,
we write f(Θ) and f(x; Θ) interchangeably."
INTRODUCTION,0.03680981595092025,"Gradient and adaptive gradient methods are widely used for training deep neural networks (DNN)
for their computational efﬁciency. The most common approach is Stochastic Gradient Descent
(SGD) which, despite its simplicity, performs well over a wide range of applications. However, in a
sparse training data setting, SGD performs poorly due to limited training speed (Luo et al. (2019)).
To address this problem, adaptive methods such as AdaGrad (Duchi et al. (2011)), AdaDelta (Zeiler
(2012)), RMSProp (Hinton et al. (2012)) and Adam (Kingma & Ba (2014)) have been proposed.
These methods take the root mean square of the past gradients to inﬂuence the current step. Amongst
all of these adaptive methods, Adam is arguably the most widely used in a deep learning setting due
to it rapid training speed."
INTRODUCTION,0.04294478527607362,"Newton’s method has the potential to exploit curvature information from the second-order deriva-
tive (Hessian) matrix (see e.g., Gould et al. (2000)).
Generally, the iterates are deﬁned by
Θk+1 = Θk −αk∇2f(Θk)−1∇f(Θk), where αk > 0 is a steplength deﬁned by a linesearch
criterion (Nocedal & Wright (2006)). In a DNN setting, we know that the number of parameters
(n) of the network can be of the order of millions. Thus storing the Hessian which takes O(n2)"
INTRODUCTION,0.049079754601226995,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05521472392638037,"memory, becomes impractical. In addition, the inversion of the Hessian matrix, which takes O(n3)
operations, is also impractical. Even though Newton’s method achieves convergence in fewer steps,
the method becomes computationally intractable to use on large-scale DNNs."
INTRODUCTION,0.06134969325153374,"Quasi-Newton methods are alternatives to Newton methods. They compute Hessian approxima-
tions, Bk+1, that satisfy the secant condition given by yk = Bk+1sk, where sk = Θk+1 −Θk and
yk = ∇f(Θk+1) −∇f(Θk). The most commonly used quasi-Newton method, including in the
realm of deep learning, is the limited-memory BFGS update, or L-BFGS (see e.g., Liu & Nocedal
(1989)), where the Hessian approximation is given by"
INTRODUCTION,0.06748466257668712,"Bk+1 = Bk + yky⊤
k
y⊤
k sk
−BkskskB⊤
k
s⊤
k Bksk
.
(2)"
INTRODUCTION,0.0736196319018405,"The generic L-BFGS quasi-Newton update scheme is described in Algorithm 1, and numerous vari-
ants of L-BFGS exist (see Goldfarb et al. (2020); Moritz et al. (2016); Gower et al. (2016)). One
advantage of using an L-BFGS update is that the Hessian approximation can be guaranteed to be
deﬁnite, which is highly suitable in line-search settings because the update sk is guaranteed to be a
descent direction, meaning there is some step length along this direction that results in a decrease
in the objective function (see Nocedal & Wright (2006), Algorithm 6.1). However, because the
L-BFGS update is positive deﬁnite, it does not readily detect directions of negative curvature for
avoiding saddle points. In contrast, the Symmetric-Rank One (SR1) quasi-Newton update is not
guarateed to be positive deﬁnite and can result in ascent directions for line-search methods. How-
ever, in trust-region settings where indeﬁnite Hessian approximations are an advantage because they
capture directions of negative curvature, the limited-memory SR1 (L-SR1) has been shown to out-
perform L-BFGS in DNNs for classiﬁcation (see Erway et al. (2020)). We discuss this in more detail
in Section 2 but in the context of Adaptive Regularization using Cubics."
INTRODUCTION,0.07975460122699386,"Algorithm 1 L-BFGS Quasi-Newton Method with Line Search
Require: Initial weights Θ0, batch size d, learning rate α, dataset D, loss function f(Θ)."
INTRODUCTION,0.08588957055214724,"for k = 0, 1, 2, . . . do"
INTRODUCTION,0.09202453987730061,"Sample mini-batch of size d : Dk ⊆D
Perform the forward backward pass over the current mini-batch
Compute the limited memory approximation Bk using (2)
Compute step sk = αB−1
k ∇Θf(Θk), where α is the line-search step length
end for"
INTRODUCTION,0.09815950920245399,"2
L-SR1 ADAPTIVE REGULARIZATION USING CUBICS METHOD"
INTRODUCTION,0.10429447852760736,"We begin by discussing the L-SR1 update and the adaptive regularizion using cubics methods for
large-scale optimization."
INTRODUCTION,0.11042944785276074,"Unlike the BFGS update (2), which is a rank-two update, the SR1 update is a rank-one update,
which is given by"
INTRODUCTION,0.1165644171779141,"Bk+1 = Bk +
1
s⊤
k (yk −Bksk)(yk −Bksk)(yk −Bksk)⊤
(3)"
INTRODUCTION,0.12269938650306748,"(see Khalfan et al. (1993)). As previously mentioned, Bk+1 in (3) is not guaranteed to be deﬁnite.
However, it can be shown that the SR1 matrices can converge to the true Hessian (see Conn et al.
(1991) for details). We note that the pair (sk, yk) is accepted only when |s⊤
k (yk −Bksk)| >
ε∥yk −Bksk∥2
2, for some constant ε > 0 (see Nocedal & Wright (2006), Sec. 6.2, for details). The
SR1 update can be deﬁned recursively as"
INTRODUCTION,0.12883435582822086,"Bk+1 = B0 + k
X j=0"
INTRODUCTION,0.13496932515337423,"1
s⊤
j (yj −Bjsj)(yj −Bjsj)(yj −Bjsj)⊤.
(4)"
INTRODUCTION,0.1411042944785276,"In limited-memory SR1 (L-SR1) settings, only the last m ≪n pairs of (sj, yj) are stored and used.
If Sk+1 = [ s0 s1
· · ·
sk ] and Yk+1 = [ y0 y1
· · ·
yk ], then Bk+1 admits a compact"
INTRODUCTION,0.147239263803681,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.15337423312883436,representation of the form
INTRODUCTION,0.15950920245398773,"Bk+1 = B0 + "" Ψk+1"
INTRODUCTION,0.1656441717791411,"#
Mk+1

Ψ⊤
k+1
 ,
(5) where"
INTRODUCTION,0.17177914110429449,"Ψk+1 = Yk+1−B0Sk+1 and Mk+1 = (Dk+1+Lk+1+L⊤
k+1−S⊤
k+1B0Sk+1)−1,
(6)"
INTRODUCTION,0.17791411042944785,"where Lk+1 is the strictly lower triangular part, Uk+1 is the strictly upper triangular part, and Dk+1
is the diagonal part of S⊤
k+1Yk+1 = Lk+1 + Dk+1 + Uk+1 (see Byrd et al. (1994) for further
details)."
INTRODUCTION,0.18404907975460122,"Because of the compact representation of Bk+1, its partial eigendecomposition can be computed
(see Burdakov et al. (2017)). In particular, if we compute the QR decomposition of Ψk+1, then
we can write Bk+1 = B0 = U∥ˆΛk+1U⊤
∥, where U∥∈Rn×(k+1) has orthonormal columns and
ˆΛ ∈R(k+1)×(k+1) is a diagonal matrix. If B0 = δkI (see e.g., Lemma 2.4 in Erway et al. (2020)),
where 0 < δk < δmax is some scalar and I is the identity matrix, then we obtain the eigendecom-
position Bk+1 = Uk+1Λk+1U⊤
k+1, where Uk+1 = [U∥U⊥], with U⊥∈Rn×(n−(k+1)) and
U⊤
k+1Uk+1 = I. Here, (Λk+1)i = δk + ˆλi for i ≤k + 1, where ˆλi is the ith diagonal in ˆΛk+1, and
(Λ)i = δk for i > k + 1."
INTRODUCTION,0.1901840490797546,"Since the SR1 Hessian approximation can be indeﬁnite, some safeguard must be implemented to
ensure that the resulting search direction sk is a descent direction. One such safeguard is to use a
“regularization” term."
INTRODUCTION,0.19631901840490798,"The Adaptive Regularization using Cubics (ARCs) method (Griewank (1981); Cartis et al.
(2011)) can be viewed as an alternative to line-search and trust-region methods. At each iteration,
an approximate global minimizer of a local (cubic) model,"
INTRODUCTION,0.20245398773006135,"min
s∈Rn mk(s) ≡g⊤
k s + 1"
INTRODUCTION,0.2085889570552147,2s⊤Bks + µk
INTRODUCTION,0.2147239263803681,"3 (Φk(s))3,
(7)"
INTRODUCTION,0.22085889570552147,"is determined, where gk = ∇f(Θk), µk > 0 is a regularization parameter, and Φk is a func-
tion (norm) that regularizes s. Typically, the Euclidean norm is used. In this work, we propose
an alternative “shape-changing” norm that allows us to solve each subproblem (7) exactly. This
shape-changing norm was proposed in Burdakov et al. (2017), and it is based on the partial eigende-
composition of Bk. Speciﬁcally, if Bk = UkΛkU⊤
k is the eigendecomposition of Bk, then we can"
INTRODUCTION,0.22699386503067484,"deﬁne the norm ∥s∥Uk
def
= ∥U⊤
k s∥3. Applying a change of basis with ¯s = U⊤
k s and ¯gk = U⊤
k gk,
we can redeﬁne the cubic subproblem as"
INTRODUCTION,0.2331288343558282,"min
¯s∈Rn ¯mk(s) = ¯g⊤
k ¯s + 1"
INTRODUCTION,0.2392638036809816,2¯s⊤Λk¯s + µk
INTRODUCTION,0.24539877300613497,"3 ∥¯s∥3
3 .
(8)"
INTRODUCTION,0.25153374233128833,"With this change of basis, we can ﬁnd a closed-form solution of (8) easily. The proposed Adaptive
Regularization using Cubics with L-SR1 (ARCSLSR1) algorithm is given in Algorithm 2."
CONTRIBUTIONS,0.25766871165644173,"2.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.26380368098159507,"The main contributions of this paper are as follows: 1. L-SR1 quasi-Newton methods: The most
commonly used quasi-Newton approach is the L-BFGS method.
In this work, we use the L-
SR1 update to better model potentially indeﬁnite Hessians of the non-convex loss function. 2.
Adaptive Regularization using Cubics (ARCs): Given that the quasi-Newton approximation is al-
lowed to be indeﬁnite, we use an Adaptive Regularized using Cubics approach to safeguard each
search direction. 3. Shape-changing regularizer: We use a shape-changing norm to deﬁne the cubic
regularization term, which allows us to compute the closed form solution to the cubic subproblem
(7). 4. Computational complexity: Let m be the number of previous iterates and gradients stored
in memory. The proposed LSR1 ARC approach is comparable to L-BFGS in terms of storage and
compute complexity (see Table 1)."
CONTRIBUTIONS,0.26993865030674846,Under review as a conference paper at ICLR 2022
CONTRIBUTIONS,0.27607361963190186,Algorithm 2 Limited-Memory Symmetric Rank-1 Adaptive Regularization using Cubics
CONTRIBUTIONS,0.2822085889570552,"1: Given: Θ0, γ2 ≥γ1, 1 > η2 ≥η1 > 0, and σ0 > 0
2: for k = 0, 1, 2 . . . do
3:
Obtain Sk = [ s0 · · · sk ], Yk = [ y0 · · · yk ]
4:
Solve the generalized eigenproblem S⊤
k Yku = ˆΛS⊤
k Sku and let δk = min{ˆλi}
5:
Compute Ψk = Yk −δkSk
6:
Perform QR decomposition of Ψ = QR
7:
Compute the eigendecomposition RMR⊤= PΛP⊤"
CONTRIBUTIONS,0.2883435582822086,"8:
Assign U∥= QP and U⊤
∥= P⊤Q⊤"
CONTRIBUTIONS,0.294478527607362,"9:
Deﬁne C∥= diag(c1, . . . , cm), where ci =
2
λi+√"
CONTRIBUTIONS,0.3006134969325153,"λ2
i +4µ|¯gi| and ¯g∥= U⊤
∥g"
CONTRIBUTIONS,0.3067484662576687,"10:
Compute α∗=
2
δk+√"
CONTRIBUTIONS,0.3128834355828221,"δ2
k+4µ∥g⊥∥where g⊥= g −U∥¯g∥"
CONTRIBUTIONS,0.31901840490797545,"11:
Compute step s∗= −α∗g + U∥(α∗Im −C∥)U⊤
∥
12:
Compute m(s∗) and ρk = (f(Θk) −f(Θk+1))/m(s∗)
13:
Set"
CONTRIBUTIONS,0.32515337423312884,"Θk+1 =
Θk + sk,
if ρk ≥η1,
Θk,
otherwise
and
µk+1 = 
 "
CONTRIBUTIONS,0.3312883435582822,"0.5µk
if ρk > η2,
0.5µk(1 + γ1)
if η1 ≤ρk ≤η2,
0.5µk(γ1 + γ2)
otherwise"
CONTRIBUTIONS,0.3374233128834356,14: end for
CONTRIBUTIONS,0.34355828220858897,Table 1: Storage and compute complexity of the methods used in our experiments.
CONTRIBUTIONS,0.3496932515337423,"Algorithms
Storage complexity
Compute complexity
SGD/Adaptive methods
O(n)
O(n)
L-BFGS
O(n + mn)
O(mn)
ARCs-LSR1
O(n + mn)
O(m3 + 2mn)"
IMPLEMENTATION,0.3558282208588957,"2.2
IMPLEMENTATION"
IMPLEMENTATION,0.3619631901840491,"Because full gradient computation is very expensive to perform, we impement a stochastic version
of the proposed ARCs-LSR1 method. In particular, we use the batch gradient approximation"
IMPLEMENTATION,0.36809815950920244,"˜gk ≡
1
|Bk| X"
IMPLEMENTATION,0.37423312883435583,"i∈Bk
∇fi(Θk)."
IMPLEMENTATION,0.3803680981595092,"In deﬁning the SR1 matrix, we use the quasi-Newton pairs (sk, ˜yk), where ˜yk = ˜gk+1 −˜gk (see
e.g., Erway et al. (2020))."
CONVERGENCE ANALYSIS,0.38650306748466257,"3
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.39263803680981596,"In this section, we prove convergence properties of the proposed method (ARCs-LSR1 in Algorithm
2). The following theoretical guarantees follow the ideas from Cartis et al. (2011) and Benson &
Shanno (2018)."
CONVERGENCE ANALYSIS,0.3987730061349693,"First, we make the following mild assumptions:"
CONVERGENCE ANALYSIS,0.4049079754601227,"A1. The loss function f(Θ) is continuously differentiable, i.e., f ∈C1(Rn)."
CONVERGENCE ANALYSIS,0.4110429447852761,A2. The loss function f(Θ) is bounded below.
CONVERGENCE ANALYSIS,0.4171779141104294,"Next, we prove that the matrix Bk in (4) is bounded."
CONVERGENCE ANALYSIS,0.4233128834355828,Lemma 1 The SR1 matrix Bk+1 in (4) satsiﬁes
CONVERGENCE ANALYSIS,0.4294478527607362,"∥Bk+1∥F ≤κB
for all k ≥1"
CONVERGENCE ANALYSIS,0.43558282208588955,for some κB > 0.
CONVERGENCE ANALYSIS,0.44171779141104295,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS,0.44785276073619634,"Proof: Using the limited-memory SR1 update with memory parameter m in (4), we have"
CONVERGENCE ANALYSIS,0.4539877300613497,"∥Bk+1∥F ≤∥B0∥F + k
X"
CONVERGENCE ANALYSIS,0.4601226993865031,j=k−m+1
CONVERGENCE ANALYSIS,0.4662576687116564,∥(yj −Bjsj)(yj −Bjsj)⊤∥F
CONVERGENCE ANALYSIS,0.4723926380368098,"|s⊤
j (yj −Bjsj)|
."
CONVERGENCE ANALYSIS,0.4785276073619632,"Using a property of the Frobenius norm, namely, for real matrices A, ∥A∥2
F = trace(AA⊤), we
have that ∥(yj −Bjsj)(yj −Bjsj)⊤∥F = ∥yj −Bjsj∥2
2. Since the pair (sj, yj) is accepted
only when |s⊤
j (yj −Bjsj)| > ε∥yj −Bjsj∥2
2, for some constant ε > 0, and B0 = δkI for some
0 < δk < δmax, we have
∥Bk+1∥F ≤δmax + m"
CONVERGENCE ANALYSIS,0.48466257668711654,ε ≡κB. □
CONVERGENCE ANALYSIS,0.49079754601226994,"Given the bound on ∥Bk+1∥F , we obtain the following result, which is similar to Theorem 2.5 in
Cartis et al. (2011)."
CONVERGENCE ANALYSIS,0.49693251533742333,"Theorem 1 Under Assumptions A1 and A2, if Lemma 1 holds, then"
CONVERGENCE ANALYSIS,0.5030674846625767,"lim inf
k→∞
∥gk∥= 0."
CONVERGENCE ANALYSIS,0.50920245398773,"Finally, we consider the following assumption, which can be satisﬁed when the gradient, g(Θ), is
Lipschitz continuous on Θ."
CONVERGENCE ANALYSIS,0.5153374233128835,"A3. If {Θti} and {Θli} are subsequences of {Θk}, then ∥gti −gli∥→0 whenever ∥Θti −Θli∥→0
as i →∞. If we further make Assumption A3, we have the following stronger result (which is based
on Corollary 2.6 in Cartis et al. (2011)):"
CONVERGENCE ANALYSIS,0.5214723926380368,"Corollary 1 Under Assumptions A1, A2, and A3, if Lemma 1 holds, then"
CONVERGENCE ANALYSIS,0.5276073619631901,"lim
k→∞∥gk∥= 0."
EXPERIMENTS,0.5337423312883436,"4
EXPERIMENTS"
EXPERIMENTS,0.5398773006134969,"To empirically compare the efﬁciency of the method against popular optimization methods like
SGD, ADAGRAD, ADAM, RMSProp and L-BFGS, we focus on two broad deep learning problems:
image classiﬁcation and image reconstruction. We choose these tasks due to their broad importance
and availability of reproducible model architectures. We run each experiments on an average of 5
times with a random initialization in each experiment. The number of parameters, convolutional
layers and fully connected layers are mentioned in Table 3."
EXPERIMENTS,0.5460122699386503,"Dataset: We measure the classiﬁcation performance of each optimization method on 4 image
datasets: MNIST (LeCun et al. (2010)), FashionMNIST (Xiao et al. (2017)), IRIS (Dua & Graff
(2017)) and CIFAR10 (Krizhevsky et al.). We have provided a comprehensive view of the experi-
ments in Table 2"
EXPERIMENTS,0.5521472392638037,"Dataset
Network
Type
IRIS
MLP
Classiﬁcation
MNIST
MLP
Classiﬁcation
FMNIST
Convolutional
Classiﬁcation
CIFAR10
Convolutional
Classiﬁcation
FashionMNIST
Convolutional
Reconstruction
MNIST
Convolutional
Reconstruction"
EXPERIMENTS,0.558282208588957,Table 2: List of experiments
EXPERIMENTS,0.5644171779141104,"Hyperparameter tuning: We empirically ﬁne-tune the hyperparameters and select the best for
each update scheme. We have made a comprehensive list of all the learning rates for the gradient
and adaptive gradient based algorithms in Table 4 in the Appendix. The additional parameters are
deﬁned as follows:"
EXPERIMENTS,0.5705521472392638,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.5766871165644172,"• ADAM: We apply an ϵ perturbation of 1.0 × 10−6. β0 and β1 are chosen to be 0.9 and 0.999,
respectively.
• ADAGRAD: The initial accumulator value is set to 0. The perturbation ϵ is set to 1.0 × 10−10.
• SGD: We use a momentum of 0.9.
• RMSPROP: We set α = 0.99. The perturbation ϵ is set 1.0 × 10−8.
• L-BFGS: The table 4 in Appendix A presents the initial learning rate for the stochastic step in
L-BFGS. We set the default learning rate to 1.0. We choose a history size m of 10 and max
iterations to 10. The tolerance on function value/parameter change is set to 1.0 × 10−9 and the
ﬁrst-order optimality condition for termination is deﬁned as 1.0 × 10−9.
• ARC-LSR1: We choose the same parameters as L-BFGS."
EXPERIMENTS,0.5828220858895705,"Network architecture: For each problem, we deﬁne the model architecture in Table 3 in the ap-
pendix. We deﬁne the process of the forward and backward pass of a DNN in Algorithm 3 in the
appendix."
EXPERIMENTS,0.588957055214724,"Dataset
Network
Convolution layers
Fully connected layers
Parameters
IRIS
Classiﬁer
-
3
2953
MNIST
Classiﬁer
-
3
397510
CIFAR10
Classiﬁer
2
3
62006
MNIST
Autoencoder
6
4
53415
FashionMNIST
Autoencoder
6
4
53415"
EXPERIMENTS,0.5950920245398773,Table 3: List of experiments
EXPERIMENTS,0.6012269938650306,"Testbed and software: All experiments were conducted using open-source software PyTorch
(Paszke et al. (2019)), SciPy (Virtanen et al. (2020)) and NumPy (Harris et al. (2020)). We use
an Intel Core i7-8700 CPU with a clock rate of 3.20 GHz and an NVIDIA RTX 2080 Ti graphics
card."
RESULTS,0.6073619631901841,"5
RESULTS"
RESULTS,0.6134969325153374,"We have divided the sections into two categories: classiﬁcation and image reconstruction. We
present both the training results and the testing results for all methods."
CLASSIFICATION RESULTS,0.6196319018404908,"5.1
CLASSIFICATION RESULTS"
CLASSIFICATION RESULTS,0.6257668711656442,"For each classiﬁcation problem, we deﬁne the network architecture, the corresponding hyperparam-
eters (other than the learning rate) for each optimization scheme."
CLASSIFICATION RESULTS,0.6319018404907976,"IRIS: Since this dataset is relatively small, we assume a small network for our deep-learning model.
The model is described in 3. We set the history size for the proposed approach and L-BFGS to
10 and the number of iterations to 10. Figure 1 shows the comparative performance of all the
methods. Note that our proposed method (ARCLSR1) achieves the highest classiﬁcation accuracy
in the fewest number of epochs."
CLASSIFICATION RESULTS,0.6380368098159509,"MNIST: We trained the network for 20 epochs with a batch size of 256 images each. We keep
the same history size and number of iterations as the IRIS dataset for L-BFGS and the proposed
ARCLSR1 approach. For training, it can be seen in Figure 2 that nearly all methods achieve opti-
mal training accuracy. However, closely inspecting the testing curve, we notice that the proposed
approach achieves higher accuracy than all the existing methods."
CLASSIFICATION RESULTS,0.6441717791411042,"FMNIST: We train the network for 20 epochs with a batch size of 256 images. We keep the history
size the same as the IRIS and MNIST experiments for the proposed approach and L-BFGS. For this
method, the proposed ARCLSR1 approach is comparable to L-BFGS but outperforms the adaptive
methods (see Figure 3)."
CLASSIFICATION RESULTS,0.6503067484662577,"CIFAR10: We use the same parameters presented in Table 4 in the previous section for the adaptive
methods. For ARCLSR1 and L-BFGS, we have a history size of 100 with a maximum number of
iterations of 100 and a batch size of 1024. Figure 4(a) represents the training loss (cross-entropy"
CLASSIFICATION RESULTS,0.656441717791411,Under review as a conference paper at ICLR 2022
CLASSIFICATION RESULTS,0.6625766871165644,"(a)
(b)"
CLASSIFICATION RESULTS,0.6687116564417178,"Figure 1: The classiﬁcation results on the IRIS dataset. (a) Training loss of the network. The y-axis
represents the negative log-likelihood loss and the x-axis represents the number of epochs. (b) The
classiﬁcation accuracy for each method, i.e., the percentage of testing samples correctly predicted
in the testing dataset."
CLASSIFICATION RESULTS,0.6748466257668712,"(a)
(b)"
CLASSIFICATION RESULTS,0.6809815950920245,"Figure 2: The classiﬁcation results on MNIST. The y-axis represents the classiﬁcation accuracy on
the MNIST dataset, and the x-axis represents the number of epochs. (a) Training response. (b)
Testing response."
CLASSIFICATION RESULTS,0.6871165644171779,"(a)
(b)"
CLASSIFICATION RESULTS,0.6932515337423313,"Figure 3: The plots above show the classiﬁcation results on the Fashion MNIST dataset. We run this
experiment for 20 epochs. In this experiment, the proposed method is comparable to L-BFGS but
outperforms the adaptive methods."
CLASSIFICATION RESULTS,0.6993865030674846,"loss). Figure 4(b) represents the testing accuracy, i.e., number of sample correctly predicted in
the testing set. To demonstrate the efﬁcacy of the proposed method on larger networks, additional
experimentation on the ResNet50 architecture can be found in the appendix (Figure 8)."
CLASSIFICATION RESULTS,0.7055214723926381,Under review as a conference paper at ICLR 2022
CLASSIFICATION RESULTS,0.7116564417177914,"(a)
(b)"
CLASSIFICATION RESULTS,0.7177914110429447,"Figure 4: The classiﬁcation results on CIFAR10. (a) The y-axis represents the training cross-entropy
loss, and the x-axis represents the number of batches. (b) The y-axis represents the testing response
accuracy and the x-axis represents the number of epochs."
IMAGE RECONSTRUCTION RESULTS,0.7239263803680982,"5.2
IMAGE RECONSTRUCTION RESULTS"
IMAGE RECONSTRUCTION RESULTS,0.7300613496932515,"The image reconstruction problem involves feeding a feedforward convolutional autoencoder model
(with randomly initialized weights) a batch of the dataset. It follows the same deep learning con-
vention as mentioned in Algorithm 3 in Appendix A. The loss function is deﬁned between the
reconstructed image and the original image."
IMAGE RECONSTRUCTION RESULTS,0.7361963190184049,"MNIST: An image x ∈Rn is fed to the network, compressed into a latent space z ∈Rl, where
l ≪n, and reconstructed back to its original image size ¯x ∈Rn. We compute the mean-squared
loss error between the reconstruction and the true image. The weights are initialized randomly. Each
experiment has been conducted 5 times and we considered a batch size of 256 images each with 50
epochs. The results for the image reconstruction can be seen in Figure 5."
IMAGE RECONSTRUCTION RESULTS,0.7423312883435583,"(a)
(b)"
IMAGE RECONSTRUCTION RESULTS,0.7484662576687117,"Figure 5:
This graph represents the training accuracy on the training samples. The x-axis shows
the number of epochs and the y-axis represents the accuracy (Mean-Squared error). (a) shows the
initial training error and (b) shows the ﬁnal training error."
IMAGE RECONSTRUCTION RESULTS,0.754601226993865,"One can notice that the initial descent provided by the proposed approach provides a signiﬁcant
decrease in the objective function. To understand better, we provide the details of the results during
the initial epoch (Figure 9(a)) and the ﬁnal epoch (Figure 9(b)). We notice that the ARCLSR1
method has minimized efﬁciently in the ﬁrst half of the ﬁrst epoch. This is empirical evidence that
the method converges to the minimizer in fewer steps in comparison to the adaptive methods. In
Figure 5 (b), we notice that all the adaptive methods eventually converge to the same point. For
training response results on the F-MNIST dataset, see Section B in the appendix."
IMAGE RECONSTRUCTION RESULTS,0.7607361963190185,Under review as a conference paper at ICLR 2022
IMAGE RECONSTRUCTION RESULTS,0.7668711656441718,"(a)
(b)"
IMAGE RECONSTRUCTION RESULTS,0.7730061349693251,"Figure 6: The testing accuracy on MNIST dataset. y-axis represents the Mean-Squared Error loss
on the testing set and x-axis represents the number of epochs. (a) shows the initial testing response.
(b) shows the ﬁnal testing response."
TIME COMPLEXITY ANALYSIS,0.7791411042944786,"5.3
TIME COMPLEXITY ANALYSIS"
TIME COMPLEXITY ANALYSIS,0.7852760736196319,"We understand that the proposed approach performs competitively against all existing methods.
We now analyze the time-constraints of each method. We choose to clock the computationally
demanding algorithm here - CIFAR10 classiﬁcation. We chose a maximum iterations of 100 with
a history size of 100 for L-BFGS and the ARCs LSR1, with a batch size of 1024 images. Figure 7
plots the time required by each of the methods to reach non-overtrained minima with a batch size of
1024 images. As can be seen, the proposed approach is able to reach the desired minima in much
less time than the rest of the algorithms. L-BFGS ﬁnds it hard to converge due to a very noisy loss
function and a small batch size, thus causing the algorithm to break. Ozyildirim & Kiran (2020)
argue that a large batch size is required for quasi-Newton methods to perform well. However, the
ARCLSR1 method performs well with a small batch size as well."
TIME COMPLEXITY ANALYSIS,0.7914110429447853,"Figure 7: Timing analysis for CIFAR10. (a) Evolution of model accuracy with respect to time (x-
axis is time in seconds and y-axis is accuracy of prediction in percentage). (b) Computational cost
for each epoch (the x-axis corresponds to epochs and the y-axis is time)."
CONCLUSION,0.7975460122699386,"6
CONCLUSION"
CONCLUSION,0.803680981595092,"In this paper, we proposed a novel quasi-Newton approach in a modiﬁed adaptive regularized cubic
setting. We were able to empirically and theoretically show how an L-SR1 quasi-Newton approxi-
mation in an ARCs setting was able to perform either better or comparably to most of the state of
the art optimization schemes. Even though the approach has yielded exceptional results, we need to
test the method’s efﬁcacy when the network size and dataset size is large and when availability of
data is sparse."
CONCLUSION,0.8098159509202454,Under review as a conference paper at ICLR 2022
REFERENCES,0.8159509202453987,REFERENCES
REFERENCES,0.8220858895705522,"Hande Y. Benson and David F. Shanno.
Cubic regularization in symmetric rank-1 quasi-
newton methods. Mathematical Programming Computation, 10(4):457–486, Dec 2018. ISSN
1867-2957.
doi:
10.1007/s12532-018-0136-7.
URL https://doi.org/10.1007/
s12532-018-0136-7."
REFERENCES,0.8282208588957055,"Oleg Burdakov, Lujin Gong, Spartak Zikrin, and Ya-xiang Yuan. On efﬁciently combining limited-
memory and trust-region techniques. Mathematical Programming Computation, 9(1):101–134,
2017."
REFERENCES,0.8343558282208589,"Richard. H. Byrd, Jorge. Nocedal, and Robert. B. Schnabel. Representations of quasi-Newton ma-
trices and their use in limited-memory methods. Math. Program., 63:129–156, 1994."
REFERENCES,0.8404907975460123,"Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. Adaptive cubic regularisation methods for
unconstrained optimization. part i: motivation, convergence and numerical results. Mathematical
Programming, 127(2):245–295, 2011."
REFERENCES,0.8466257668711656,"Andrew R. Conn, Nicholas. I. M. Gould, and Philippe. L. Toint. Convergence of quasi-newton
matrices generated by the symmetric rank one update. Mathematical Programming, 50(1):177–
195, Mar 1991. ISSN 1436-4646. doi: 10.1007/BF01594934. URL https://doi.org/10.
1007/BF01594934."
REFERENCES,0.852760736196319,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.8588957055214724,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011."
REFERENCES,0.8650306748466258,"Jennifer B. Erway, Joshua D. Grifﬁn, Roummel F. Marcia, and Riadh Omheni. Trust-region algo-
rithms for training responses: machine learning methods using indeﬁnite hessian approximations.
Optimization Methods and Software, 35:460 – 487, 2020."
REFERENCES,0.8711656441717791,"Donald Goldfarb, Yi Ren, and Achraf Bahamou. Practical quasi-newton methods for training deep
neural networks. arXiv preprint arXiv:2006.08877, 2020."
REFERENCES,0.8773006134969326,"Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org."
REFERENCES,0.8834355828220859,"Nicholas I. M. Gould, Stefano Lucidi, Massimo Roma, and Philippe L. Toint. Exploiting negative
curvature directions in linesearch methods for unconstrained optimization. Optimization Methods
and Software, 14(1-2):75–98, 2000. doi: 10.1080/10556780008805794."
REFERENCES,0.8895705521472392,"Robert Gower, Donald Goldfarb, and Peter Richtarik. Stochastic block bfgs: Squeezing more cur-
vature out of data. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of
The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 1869–1878, New York, New York, USA, 20–22 Jun 2016. PMLR. URL
https://proceedings.mlr.press/v48/gower16.html."
REFERENCES,0.8957055214723927,"Andreas Griewank. The modiﬁcation of Newton?s method for unconstrained optimization by bound-
ing cubic terms. Technical report, Technical report NA/12, 1981."
REFERENCES,0.901840490797546,"Charles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf Gommers, Pauli Virtanen,
David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert
Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,
Jaime Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard,
Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Ar-
ray programming with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/
s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2."
REFERENCES,0.9079754601226994,"Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. Cited on, 14(8):2, 2012."
REFERENCES,0.9141104294478528,Under review as a conference paper at ICLR 2022
REFERENCES,0.9202453987730062,"H Fayez Khalfan, Richard H Byrd, and Robert B Schnabel. A theoretical and experimental study of
the symmetric rank-one update. SIAM Journal on Optimization, 3(1):1–24, 1993."
REFERENCES,0.9263803680981595,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.9325153374233128,"Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). URL http://www.cs.toronto.edu/˜kriz/cifar.html."
REFERENCES,0.9386503067484663,"Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [On-
line]. Available: http://yann. lecun. com/exdb/mnist, 2:18, 2010."
REFERENCES,0.9447852760736196,"Dong C. Liu and Jorge Nocedal.
On the limited memory bfgs method for large scale opti-
mization.
Mathematical Programming, 45(1):503–528, Aug 1989.
ISSN 1436-4646.
doi:
10.1007/BF01589116. URL https://doi.org/10.1007/BF01589116."
REFERENCES,0.950920245398773,"Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate, 2019."
REFERENCES,0.9570552147239264,"Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic L-BFGS
algorithm.
In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th In-
ternational Conference on Artiﬁcial Intelligence and Statistics, volume 51 of Proceedings of
Machine Learning Research, pp. 249–258, Cadiz, Spain, 09–11 May 2016. PMLR.
URL
https://proceedings.mlr.press/v51/moritz16.html."
REFERENCES,0.9631901840490797,"Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA,
second edition, 2006."
REFERENCES,0.9693251533742331,"Buse Melis Ozyildirim and Mariam Kiran. Do optimization methods in deep learning applications
matter?, 2020."
REFERENCES,0.9754601226993865,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems 32, pp.
8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf."
REFERENCES,0.9815950920245399,"Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna-
peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der
Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel-
son, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore,
Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mul-
bregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing
in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2."
REFERENCES,0.9877300613496932,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. 2017."
REFERENCES,0.9938650306748467,"Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012."
