Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0032258064516129032,"Automated seizure detection and classiﬁcation from electroencephalography
(EEG) can greatly improve seizure diagnosis and treatment. However, several
modeling challenges remain unaddressed in prior automated seizure detection
and classiﬁcation studies: (1) representing non-Euclidean data structure in EEGs,
(2) accurately classifying rare seizure types, and (3) lacking a quantitative inter-
pretability approach to measure model ability to localize seizures. In this study,
we address these challenges by (1) representing the spatiotemporal dependencies
in EEGs using a graph neural network (GNN) and proposing two EEG graph
structures that capture the electrode geometry or dynamic brain connectivity, (2)
proposing a self-supervised pre-training method that predicts preprocessed sig-
nals for the next time period to further improve model performance, particu-
larly on rare seizure types, and (3) proposing a quantitative model interpretabil-
ity approach to assess a model’s ability to localize seizures within EEGs. When
evaluating our approach on seizure detection and classiﬁcation on a large pub-
lic dataset (5,499 EEGs), we ﬁnd that our GNN with self-supervised pre-training
achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure
detection and 0.749 weighted F1-score on seizure classiﬁcation, outperforming
previous methods for both seizure detection and classiﬁcation. Moreover, our
self-supervised pre-training strategy signiﬁcantly improves classiﬁcation of rare
seizure types (e.g. 47 points increase in combined tonic seizure accuracy over
baselines). Furthermore, quantitative interpretability analysis shows that our GNN
with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9
point improvement over existing CNNs. Finally, by superimposing the identiﬁed
seizure locations on both raw EEG signals and EEG graphs, our approach could
provide clinicians with an intuitive visualization of localized seizure regions."
INTRODUCTION,0.0064516129032258064,"1
INTRODUCTION"
INTRODUCTION,0.00967741935483871,"Seizures are among the most common neurological emergencies in the world (Strein et al., 2019).
Seizures can be chronic as in the case of epilepsy, a neurological disease affecting 50 million people
worldwide (WHO, 2019). Clinically, deﬁnitive detection of a seizure is only the ﬁrst step in seizure
diagnosis. An important subsequent step is to classify seizures into ﬁner-grained types––such as
focal versus generalized seizures––for identifying epilepsy syndromes, targeted therapies, and eligi-
bility for epilepsy surgery (Fisher et al., 2017)."
INTRODUCTION,0.012903225806451613,"Scalp electroencephalography (or “EEG”) plays a critical role in seizure detection and classiﬁcation.
Clinically, EEG-based seizure detection and classiﬁcation are performed by a trained EEG reader
who visually examines a patient’s EEG signals over time periods ranging from hours to days. How-
ever, this manual analysis is extremely resource- and time-intensive, and thus automated algorithms
could greatly accelerate seizure diagnosis and improve outcomes for severely ill patients."
INTRODUCTION,0.016129032258064516,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01935483870967742,"Figure 1:
(a) EEG elec-
trode
placement
in
the
standard
10-20
system.
(b) Distance-based EEG
graph.
(c) An example
correlation-based
EEG
graph.
(d) Overview of
our methods.
The inputs
to the models are the EEG
graphs, where each node
feature corresponds to the
preprocessed EEG signals
in the respective channel.
Self-edges are not shown
for better visualization."
INTRODUCTION,0.02258064516129032,"Although a large number of studies have attempted automated seizure detection (Rasheed et al.,
2020; Siddiqui et al., 2020; Shoeibi et al., 2021; O’Shea et al., 2020; Saab et al., 2020) or seizure
classiﬁcation (Raghu et al., 2020; Asif et al., 2020; Iesmantas & Alzbutas, 2020; Ahmedt-Aristizabal
et al., 2020; Roy et al., 2019), several challenges remain largely unaddressed. First, most recent
studies use convolutional neural networks (CNNs) that assume Euclidean structures in EEG signals
or spectrograms (Rasheed et al., 2020; Shoeibi et al., 2021; Raghu et al., 2020; Asif et al., 2020;
Iesmantas & Alzbutas, 2020; Ahmedt-Aristizabal et al., 2020; Roy et al., 2019; O’Shea et al., 2020;
Saab et al., 2020). However, assumption of Euclidean structure ignores the natural geometry of
EEG electrodes and the connectivity in brain networks. EEGs are measured by electrodes placed on
a manifold (i.e. patient’s scalp) (Figure 1a), and thus have inherent non-Euclidean structures. Graphs
are a data structure that can represent complex, non-Euclidean data (Chami et al., 2020; Bronstein
et al., 2017), and graph theory has been extensively used in modeling brain networks (Bullmore &
Sporns, 2009). We therefore hypothesize that graph-based modeling approaches can better represent
the inherent non-Euclidean structures in EEGs in a manner that improves both the performance and
the clinical utility of seizure detection and classiﬁcation models. Although traditional graph theory
has been used (Supriya et al., 2021), only a few deep learning studies have modeled EEGs as graphs
for seizure detection. However, these graph-based studies were limited to nonpublic (Covert et al.,
2019) or small datasets (Craley et al., 2019; Li et al., 2021), did not leverage modern self-supervised
approaches or examine seizure classiﬁcation (Cisotto et al., 2020; Zhao et al., 2021; Li et al., 2021)."
INTRODUCTION,0.025806451612903226,"Second, certain seizure types (e.g. clonic seizures) are rare by nature. Training machine learn-
ing models that perform well on these rarer seizure classes using traditional supervised learning
approaches is challenging, which could explain the performance difference between major and mi-
nority seizure types in prior studies (Raghu et al., 2020; Iesmantas & Alzbutas, 2020; Ahmedt-
Aristizabal et al., 2020). Several studies have investigated an alternative, self-supervised training
strategy (Banville et al., 2020; Mohsenvand et al., 2020; Kostas et al., 2021; Martini et al., 2021;
Xu et al., 2020), but they did not model EEGs as graphs or address automated seizure classiﬁcation.
Prior works have shown that self-supervised pre-training signiﬁcantly improves model performance
on data with imbalanced labels in the ﬁeld of computer vision (Yang & Xu, 2020; Liu et al., 2021).
Hence, we hypothesize that self-supervised pre-training can help improve our graph model per-
formance on rare seizure types. Moreover, a large portion of EEG signals generally do not have
seizures. Self-supervised pre-training strategy would allow the model to leverage the abundant non-
seizure EEGs that are readily available in the dataset."
INTRODUCTION,0.02903225806451613,"Lastly, for seizure detection and classiﬁcation models, the ability to not only provide a single pre-
diction across all EEG channels, but also to provide the interpretability and the ability to localize
seizures would be clinically useful for informing treatment strategy. While prior studies (Saab et al.,
2020; Covert et al., 2019) have shown qualitative visualization for model interpretability, none have
quantitatively assessed the model’s ability to localize seizures."
INTRODUCTION,0.03225806451612903,"In this work, we aim to address these limitations in prior automated seizure detection and classiﬁca-
tion studies. First, we propose a graph-based modeling approach for EEG-based seizure detection
and classiﬁcation. Speciﬁcally, we propose two EEG graph structures that capture EEG sensor"
INTRODUCTION,0.035483870967741936,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03870967741935484,"geometry (Figure 1b) or dynamic brain connectivity (Figure 1c), and we extend Diffusion Con-
volutional Recurrent Neural Network (DCRNN) (Li et al., 2018), an RNN with graph diffusion
convolutions, to model the spatiotemporal dependencies in EEGs (Figure 1d). Second, we improve
DCRNN performance using a self-supervised pre-training strategy of predicting the preprocessed
EEG signals for the next time period without requiring additional data or labels. Finally, we propose
quantitative metrics to assess our model’s ability to localize seizures. In summary:"
INTRODUCTION,0.041935483870967745,"• We propose two EEG graph structures that capture (1) the natural geometry of EEG sensors
or (2) dynamic connectivity in the brain, and show that building a recurrent graph neural
network (GNN) based on these representations yields models for seizure detection and
classiﬁcation that outperform previous approaches on a large public dataset (5,499 EEGs)."
INTRODUCTION,0.04516129032258064,"• We propose a self-supervised pre-training strategy to further improve our recurrent GNN
model performance, particularly on rare seizure types. To our knowledge, our study is
the ﬁrst to date that combines graph-based modeling and self-supervised pre-training for
EEGs. By leveraging graph structure and self-supervision, our method achieves 0.875
Area Under the Receiver Operating Characteristic Curve (AUROC) on seizure detection
and 0.749 weighted F1-score on seizure classiﬁcation, outperforming previous approaches
on both seizure detection and classiﬁcation on this large public dataset. Moreover, our self-
supervised pre-training method substantially improves classiﬁcation of rare seizure types
(e.g. 47 points increase in combined tonic seizure accuracy over baselines)."
INTRODUCTION,0.04838709677419355,"• We propose a quantitative model interpretability analysis that can be used to assess a
model’s ability to localize seizures, which is critical to determining the course of treat-
ment for seizures. We show that by leveraging graph structure and self-supervision, our
method precisely localizes 25.4% focal seizures, providing 21.9 points improvement over
a prior state-of-the-art CNN. Finally, by displaying the identiﬁed seizure regions on raw
EEG signals and EEG graphs, our approach could provide valuable insights that support
more effective seizure diagnosis in real-world clinical settings."
METHODS,0.05161290322580645,"2
METHODS"
SEIZURE DETECTION AND CLASSIFICATION PROBLEM FORMULATION,0.054838709677419356,"2.1
SEIZURE DETECTION AND CLASSIFICATION PROBLEM FORMULATION"
SEIZURE DETECTION AND CLASSIFICATION PROBLEM FORMULATION,0.05806451612903226,"The goal of seizure detection is to predict whether a seizure exists within an EEG clip, and the
goal of seizure classiﬁcation is to predict the seizure type given a seizure EEG clip. Following a
prior study (Saab et al., 2020), we examine our model’s capability for fast and slow detection and
classiﬁcation over 12-s and 60-s EEG clips, respectively."
GRAPH-BASED MODELING FOR EEGS,0.06129032258064516,"2.2
GRAPH-BASED MODELING FOR EEGS"
REPRESENTING EEGS AS GRAPHS,0.06451612903225806,"2.2.1
REPRESENTING EEGS AS GRAPHS"
REPRESENTING EEGS AS GRAPHS,0.06774193548387097,"We represent an EEG clip as a graph G = {V, E, W }, where V denotes the set of nodes (i.e. EEG
electrodes/channels), E denotes the set of edges, and W is the adjacency matrix. We propose the
following two methods of constructing the EEG graph."
REPRESENTING EEGS AS GRAPHS,0.07096774193548387,"Distance graph. To represent the natural geometry of EEG electrodes, we compute edge weight
Wij by applying a thresholded Gaussian kernel (Shuman et al., 2013) to the pairwise Euclidean"
REPRESENTING EEGS AS GRAPHS,0.07419354838709677,"distance between vi and vj, i.e., Wij = exp

−dist(vi, vj)2 σ2"
REPRESENTING EEGS AS GRAPHS,0.07741935483870968,"
if dist(vi, vj) ≤κ, else 0. Here,"
REPRESENTING EEGS AS GRAPHS,0.08064516129032258,"dist(vi, vj) is the Euclidean distance between electrodes vi and vj according to the standard 10-20
EEG electrode placement (Jasper, 1958), σ is the standard deviation of the distances, and κ is the
threshold for sparsity. This results in a universal undirected, weighted graph for all EEG clips. Based
on preliminary experiments and EEG domain knowledge, we chose κ = 0.9 because it results in a
reasonable graph that also resembles the EEG montage (longitudinal bipolar and transverse bipolar)
widely used clinically (Acharya et al., 2016). Figure 1b shows the distance graph with κ = 0.9.
In Appendix K, we explore different values of κ as well as constructing the distance graph using a
Gaussian kernel with a pre-speciﬁed bandwidth."
REPRESENTING EEGS AS GRAPHS,0.08387096774193549,Published as a conference paper at ICLR 2022
REPRESENTING EEGS AS GRAPHS,0.08709677419354839,"Correlation graph. To capture dynamic brain connectivity, we deﬁne the edge weight Wij as the
absolute value of the normalized cross-correlation between the preprocessed signals in vi and vj.
To introduce sparsity to the graph, only the edges whose weights are among the top-τ neighbors
of each node are kept (plus self-edges), i.e., Wij = |X:,i,: ∗X:,j,:| if vj ∈N(vi), else 0. Here,
X:,i,: and X:,j,: are preprocessed signals in vi and vj, ∗represents the normalized cross-correlation,
and N(vi) represents the top-τ neighbors of vi. This method results in a unique directed, weighted
graph for each input EEG clip. Figure 1c shows an example correlation graph with τ = 3."
GRAPH NEURAL NETWORK,0.09032258064516129,"2.2.2
GRAPH NEURAL NETWORK"
GRAPH NEURAL NETWORK,0.0935483870967742,"We adapt DCRNN (Li et al., 2018), a recurrent neural network with graph diffusion convolutions, to
model the spatiotemporal dependencies in EEG signals. DCRNN was initially developed for trafﬁc
forecasting, where the dynamics of trafﬁc ﬂow are modeled as a diffusion process. Similarly, we can
also model the spatial dependency in EEG signals as a diffusion process, because an electrode can
be inﬂuenced more by electrodes in its anatomical proximity (measured by distance) (Acharya et al.,
2016) or functional proximity (measured by correlation) (Sakkalis, 2011). Speciﬁcally, the diffusion
process is characterized by a bidirectional random walk on a directed graph G, which results in the
following diffusion convolution (Li et al., 2018):"
GRAPH NEURAL NETWORK,0.0967741935483871,"X:,m⋆Gfθ = K−1
X k=0"
GRAPH NEURAL NETWORK,0.1," 
θk,1(D−1
O W )k + θk,2(D−1
I W ⊺)k
X:,m for m ∈{1, ..., M}
(1)"
GRAPH NEURAL NETWORK,0.1032258064516129,"where X ∈RN×M is the preprocessed EEG clip at time step t ∈{1, ..., T} with N nodes and M
features, fθ is the convolution ﬁlter with parameters θ ∈RK×2, DO and DI are the out-degree and
in-degree diagonal matrices of the graph, respectively, D−1
O W and D−1
I W ⊺are the state transi-
tion matrices of the outward and inward diffusion processes, respectively, and K is the number of
maximum diffusion steps."
GRAPH NEURAL NETWORK,0.1064516129032258,"For undirected graphs, the diffusion convolution is similar to ChebNet spectral graph convolution
(Defferrard et al., 2016) up to a constant scaling factor, and thus can be computed using stable
Chebyshev polynomial bases as follows (Li et al., 2018):"
GRAPH NEURAL NETWORK,0.10967741935483871,"X:,m⋆Gfθ = Φ
 K−1
X"
GRAPH NEURAL NETWORK,0.11290322580645161,"k=0
θkΛk

Φ⊺X:,m = K−1
X"
GRAPH NEURAL NETWORK,0.11612903225806452,"k=0
θkLkX:,m = K−1
X"
GRAPH NEURAL NETWORK,0.11935483870967742,"k=0
˜θkTk(˜L)X:,m for m ∈{1, ..., M}"
GRAPH NEURAL NETWORK,0.12258064516129032,"(2)
where T0(x) = 1, T1(x) = x, and Tk(x) = 2xTk−1(x) −Tk−2(x) for k ≥2 are the bases of the
Chebyshev polynomial, L = D−1"
GRAPH NEURAL NETWORK,0.12580645161290321,2 (D −W )D−1
GRAPH NEURAL NETWORK,0.12903225806451613,"2 = ΦΛΦ⊺is the normalized graph Laplacian,
and ˜L =
2
λmax L −I is the scaled graph Laplacian mapping eigenvalues from [0, λmax] to [−1, 1].
We use Equation 1 for directed correlation graphs, and Equation 2 for undirected distance graph."
GRAPH NEURAL NETWORK,0.13225806451612904,"Next, to model the temporal dependency in EEGs, we employ Gated Recurrent Units (GRUs) (Cho
et al., 2014), a variant of RNN with a gating mechanism. Speciﬁcally, the matrix multiplications in
GRUs are replaced with diffusion convolutions (or ChebNet spectral graph convolutions for undi-
rected distance-based graph) (Li et al., 2018), allowing spatiotemporal modeling of EEG signals
(referred to as “DCGRU”):"
GRAPH NEURAL NETWORK,0.13548387096774195,"r(t) = σ
 
Θr⋆G[X(t), H(t−1)] + br

u(t) = σ
 
Θu⋆G[X(t), H(t−1)] + bu

(3)"
GRAPH NEURAL NETWORK,0.13870967741935483,"C(t) = tanh
 
ΘC⋆G[X(t), (r(t)⊙H(t−1))]+bC

H(t) = u(t)⊙H(t−1)+(1−u(t))⊙C(t) (4)"
GRAPH NEURAL NETWORK,0.14193548387096774,"Here, X(t), H(t) denote the input and output of DCGRU at time step t respectively, σ denotes
Sigmoid function, ⊙represents the Hadamard product, r(t), u(t), C(t) denote reset gate, update gate
and candidate at time step t respectively, ⋆G denotes the diffusion convolution (or ChebNet spectral
graph convolution), Θr, br, Θu, bu, ΘC and bC are the weights and biases for the corresponding
convolutional ﬁlters. Finally, for seizure detection and classiﬁcation, the models consist of several
stacked DCGRUs followed by a fully-connected layer."
SELF-SUPERVISED PRE-TRAINING,0.14516129032258066,"2.3
SELF-SUPERVISED PRE-TRAINING"
SELF-SUPERVISED PRE-TRAINING,0.14838709677419354,"To further improve DCRNN performance, we propose a self-supervised pre-training strategy for
EEGs. Speciﬁcally, we pre-train the model for predicting the next T ′ second preprocessed EEG"
SELF-SUPERVISED PRE-TRAINING,0.15161290322580645,Published as a conference paper at ICLR 2022
SELF-SUPERVISED PRE-TRAINING,0.15483870967741936,"clips given a preprocessed 12-s (60-s) EEG clip. We hypothesize that by learning to predict the EEG
signals for the next time period, the model would learn task-agnostic representations and improve
downstream seizure detection and classiﬁcation tasks. The model for self-supervised pre-training
has a sequence-to-sequence architecture with an encoder and a decoder (Sutskever et al., 2014),
each of which has several stacked DCGRUs (Figure 1d). We use mean absolute error between the
true preprocessed EEG clips and the predicted clips as the loss function. Preliminary experiments
suggest that predicting future T ′ = 12 second preprocessed EEG clips results in low regression loss
on the validation set, and thus we use T ′ = 12 in all self-supervised pre-training experiments."
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.15806451612903225,"2.4
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES"
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.16129032258064516,"We perform model interpretability analyses using an occlusion-based approach (Zeiler & Fergus,
2013). First, for seizure detection, we zero-ﬁll one second EEG signals in one channel at a time and
compute the relative change in the model’s output logit with respect to the original non-occluded
output (original −occluded). Furthermore, we scale the values in each clip to [0, 1] using the
minimum and maximum values in that clip. This results in an occlusion map M ∈RN×T , where
N is the number of EEG channels, T is the clip length, and Mij indicates the relative change in the
model output when the j-th second EEG clip in the i-th channel is occluded. A larger Mij indicates
that the occluded region is more important for predicting seizure, and vice versa. We visualize M
by superimposing it over raw EEG signals and graph structures."
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.16451612903225807,"In addition to visually analyzing the aforementioned occlusion maps, we propose quantitative met-
rics to evaluate the model’s capability of localizing seizures based on the occlusion maps. Specif-
ically, we deﬁne a coverage metric that quantiﬁes how many true seizure regions are detected, as
well as a localization metric that quantiﬁes how many detected seizure regions are true seizure re-
gions. Note that coverage and localization scores are analogous to recall and precision, respectively,
in binary classiﬁcation problems. Mathematically, let M annot ∈RN×T be detailed annotations of
seizure duration and location, where M annot
ij
= 1 if there is a seizure at the j-th second in the i-th
channel, otherwise M annot
ij
= 0. Let M ∈RN×T be the occlusion map described above. Then"
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.16774193548387098,coverage = P
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.17096774193548386,"i,j 1Mij > 0.5M annot
ij
P"
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.17419354838709677,"i,j M annot
ij
and localization = P"
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.1774193548387097,"i,j 1Mij > 0.5M annot
ij
P"
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.18064516129032257,"i,j 1Mij > 0.5
."
MODEL INTERPRETABILITY AND ABILITY TO LOCALIZE SEIZURES,0.18387096774193548,"Finally, we also perform occlusion-based interpretability analysis for seizure classiﬁcation. Since the
goal of seizure classiﬁcation is to predict the seizure type given a seizure EEG clip, we hypothesize
that the difference in signals between EEG channels are more important for predicting seizure types.
Therefore, we completely drop one EEG channel at a time, and compute the relative change in the
model output with respect to the original output. This results in an occlusion map M ′ ∈RN, where
M ′
i indicates the relative change in the model output when the i-th channel is dropped."
EXPERIMENTS,0.1870967741935484,"3
EXPERIMENTS"
EXPERIMENTAL SETUP,0.19032258064516128,"3.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.1935483870967742,"Dataset. We use the public Temple University Hospital EEG Seizure Corpus (TUSZ) v1.5.2 (Shah
et al., 2018; Obeid & Picone, 2016), the largest public EEG seizure database to date with 5,612
EEGs, 3,050 annotated seizures from clinical recordings, and eight seizure types1. We include 19
EEG channels in the standard 10-20 system (Figure 1b–1c). To evaluate model generalizability to
unseen patients, we exclude ﬁve patients from the ofﬁcial TUSZ test set who exist in both the ofﬁcial
TUSZ train and test sets. Moreover, we use the detailed annotations of seizure duration and location
available in TUSZ for our interpretability analyses. Table 1 summarizes the TUSZ data."
EXPERIMENTAL SETUP,0.1967741935483871,"Data preprocessing. Because seizures are associated with brain electrical activity in certain fre-
quency bands (Tzallas et al., 2009), we perform data preprocessing to transform raw EEG signals to
the frequency domain. Similar to prior studies (Asif et al., 2020; Ahmedt-Aristizabal et al., 2020;
Covert et al., 2019), we obtain the log-amplitudes of the fast Fourier transform of raw EEG sig-
nals. For seizure detection and self-supervised pre-training, we use both seizure and non-seizure"
EXPERIMENTAL SETUP,0.2,"1The eight seizure types are: focal, generalized non-speciﬁc, simple partial, complex partial, absence, tonic,
tonic-clonic, and myoclonic seizures."
EXPERIMENTAL SETUP,0.2032258064516129,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.2064516129032258,"EEGs, and obtain the 12-s (60-s) EEG clips using non-overlapping 12-s (60-s) sliding windows. For
seizure classiﬁcation, we use only seizure EEGs, and obtain one 12-s (60-s) EEG clip from each
seizure event, such that each EEG clip has exactly one seizure type. Appendix A presents details of
data preprocessing, and Appendix B compares results of frequency-domain vs time-domain inputs."
EXPERIMENTAL SETUP,0.20967741935483872,"Reﬁned seizure classiﬁcation scheme. Because simple partial (SP) seizures and complex partial
(CP) seizures are focal seizures characterized by a clinical behavior (consciousness during a seizure)
and are not distinguishable by EEG signals alone (Fisher et al., 2017), we combine focal non-speciﬁc
(FN), SP, and CP seizures to form a combined focal (CF) seizure class. We provide extensive
experiments in Appendix C showing that these focal seizure types cannot be distinguished by a
variety of models. We also exclude myoclonic seizures because only three myoclonic seizures are
available in TUSZ. In addition, since tonic and tonic-clonic seizures are rare in the dataset (only
18 tonic seizures and 30 tonic-clonic seizures in TUSZ train set), and tonic-clonic seizures always
start with a tonic phase (Fisher et al., 2017), we combine tonic-clonic seizures with tonic seizures to
form a combined tonic (CT) seizure class. In summary, there are four seizure classes in total: CF,
generalized non-speciﬁc (GN), absence (AB), and CT seizures (Table 1). In Appendix L, we show
seizure classiﬁcation results on the original eight seizure types in TUSZ."
EXPERIMENTAL SETUP,0.2129032258064516,"Data splits. We randomly split the ofﬁcial TUSZ train set by patients into train and validation sets
by 90/10 for model training and hyperparameter tuning, respectively, and we hold-out the ofﬁcial
TUSZ test set for model evaluation (excluding ﬁve patients who exist in both the ofﬁcial TUSZ train
and test sets). The train, validation, and test sets consist of distinct patients. See Appendix D for the
number of preprocessed EEG clips and patients in each split."
EXPERIMENTAL SETUP,0.2161290322580645,Table 1: Summary of data in train and test sets of TUSZ v1.5.2. used in our study.
EXPERIMENTAL SETUP,0.21935483870967742,"EEG Files
(% Seizure)"
EXPERIMENTAL SETUP,0.22258064516129034,"Patients
(% Seizure)"
EXPERIMENTAL SETUP,0.22580645161290322,"Total Duration
(% Seizure)"
EXPERIMENTAL SETUP,0.22903225806451613,"CF Seizures
(Patients)"
EXPERIMENTAL SETUP,0.23225806451612904,"GN Seizures
(Patients)"
EXPERIMENTAL SETUP,0.23548387096774193,"AB Seizures
(Patients)"
EXPERIMENTAL SETUP,0.23870967741935484,"CT Seizures
(Patients)"
EXPERIMENTAL SETUP,0.24193548387096775,"Train Set
4,599 (18.9%)
592 (34.1%)
45,174.72 min (6.3%)
1,868 (148)
409 (68)
50 (7)
48 (11)
Test Set
900 (25.6%)
45 (77.8%)
9,031.58 min (9.8%)
297 (24)
114 (11)
49 (5)
61 (4)"
EXPERIMENTAL SETUP,0.24516129032258063,"Baselines. To compare our DCRNN to traditional CNNs/RNNs, we include three primary base-
lines: (a) Dense-CNN (Saab et al., 2020), a previous state-of-the-art CNN for seizure detection, (b)
LSTM (Hochreiter & Schmidhuber, 1997), and (c) CNN-LSTM (implemented following Ahmedt-
Aristizabal et al. (2020)). The baselines are trained and evaluated on the same preprocessed data.
Additionally, we compare our method to the reported results of two CNNs for seizure classiﬁcation
that use TUSZ and test on unseen patients (Asif et al., 2020; Iesmantas & Alzbutas, 2020)."
EXPERIMENTAL SETUP,0.24838709677419354,"Model training. Training for all models was accomplished using the Adam optimizer (Kingma
& Ba, 2014) in PyTorch on a single NVIDIA Titan RTX GPU. Model parameters were randomly
initialized for models without self-supervised pre-training, and were initialized with the pre-trained
weights of the encoder for models with self-supervised pre-training. All models were run for ﬁve
runs with different random seeds. Detailed hyperparameter settings are shown in Appendix E. Dur-
ing training, we perform data augmentation as described in Appendix F."
EXPERIMENTAL RESULTS,0.25161290322580643,"3.2
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.25483870967741934,"Graph neural network performance.
Following recent studies (Asif et al., 2020; Ahmedt-
Aristizabal et al., 2020; O’Shea et al., 2020; Saab et al., 2020; Covert et al., 2019), we use AUROC
and weighted F1-score as the main evaluation metrics for seizure detection and classiﬁcation, re-
spectively. Table 2 (3rd–7th rows) shows the performance of our DCRNN (without self-supervised
pre-training) and the baselines. Distance graph-based DCRNN (or “Dist-DCRNN”) and correlation
graph-based DCRNN (or “Corr-DCRNN”) without self-supervised pre-training perform on par with
or better than the baselines. See Appendix G for additional evaluation scores."
EXPERIMENTAL RESULTS,0.25806451612903225,"Moreover, DCRNN outperforms the reported results of two existing CNNs (Asif et al., 2020;
Ahmedt-Aristizabal et al., 2020) on seizure classiﬁcation (Table 3). For fair comparison to Asif
et al. (2020), we conduct 7-class seizure classiﬁcation2 on the same 3-fold patient-wise split."
EXPERIMENTAL RESULTS,0.26129032258064516,2The 7 classes are the original TUSZ seizure types excluding myoclinic seizure.
EXPERIMENTAL RESULTS,0.2645161290322581,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.267741935483871,"In addition, we conduct an ablation experiment to examine the effectiveness of Fourier transform in
our preprocessing step for DCRNNs. We ﬁnd that frequency-domain inputs substantially outperform
time-domain inputs on both seizure detection and classiﬁcation (Appendix B)."
EXPERIMENTAL RESULTS,0.2709677419354839,"Table 2: Seizure detection and seizure classiﬁcation results. Mean and standard deviations are from
ﬁve random runs. Best non-pretrained and pre-trained mean results are highlighted in bold. Model"
EXPERIMENTAL RESULTS,0.27419354838709675,"Seizure Detection AUROC
Seizure Classiﬁcation Weighted F1-Score
12-s
60-s
12-s
60-s
Dense-CNN
0.812 ± 0.014
0.796 ± 0.014
0.576 ± 0.101
0.626 ± 0.073
LSTM
0.786 ± 0.014
0.715 ± 0.016
0.652 ± 0.019
0.686 ± 0.020
CNN-LSTM
0.749 ± 0.006
0.682 ± 0.003
0.633 ± 0.025
0.641 ± 0.019"
EXPERIMENTAL RESULTS,0.27741935483870966,"Corr-DCRNN w/o Pre-training
0.812 ± 0.012
0.804 ± 0.015
0.710 ± 0.023
0.701 ± 0.030
Dist-DCRNN w/o Pre-training
0.824 ± 0.020
0.793 ± 0.022
0.703 ± 0.025
0.690 ± 0.035"
EXPERIMENTAL RESULTS,0.2806451612903226,"Corr-DCRNN w/ Pre-training
0.861 ± 0.005
0.850 ± 0.014
0.723 ± 0.017
0.749 ± 0.017
Dist-DCRNN w/ Pre-training
0.866 ± 0.016
0.875 ± 0.016
0.746 ± 0.024
0.749 ± 0.028"
EXPERIMENTAL RESULTS,0.2838709677419355,"Table 3: Comparison between DCRNNs (w/o pre-training) and existing CNNs on seizure classiﬁca-
tion. Mean and standard deviations are from ﬁve random runs. Best mean results are in bold."
EXPERIMENTAL RESULTS,0.2870967741935484,"Model
7-Class Classiﬁcation
Weighted F1-Score"
EXPERIMENTAL RESULTS,0.2903225806451613,"CF Seizure
AUROC"
EXPERIMENTAL RESULTS,0.29354838709677417,"GN Seizure
AUROC"
EXPERIMENTAL RESULTS,0.2967741935483871,"AB Seizure
AUROC"
EXPERIMENTAL RESULTS,0.3,"CT Seizure
AUROC
Asif et al. (2020)
0.62
-
-
-
-
Iesmantas & Alzbutas (2020)
-
-
0.78
0.72
-"
EXPERIMENTAL RESULTS,0.3032258064516129,"Corr-DCRNN, 12-s
0.619 ± 0.006
0.907 ± 0.008
0.815 ± 0.027
0.972 ± 0.013
0.908 ± 0.005
Dist-DCRNN, 12-s
0.585 ± 0.006
0.896 ± 0.011
0.814 ± 0.027
0.983 ± 0.008
0.890 ± 0.014
Corr-DCRNN, 60-s
0.650 ± 0.008
0.914 ± 0.007
0.795 ± 0.031
0.971 ± 0.020
0.939 ± 0.008
Dist-DCRNN, 60-s
0.606 ± 0.009
0.920 ± 0.004
0.811 ± 0.032
0.973 ± 0.009
0.926 ± 0.020"
EXPERIMENTAL RESULTS,0.3064516129032258,"Self-supervised pre-training improves graph neural network performance. To assess the ef-
fectiveness of self-supervised pre-training in improving DCRNN’s performance, we compare the
results of DCRNN without and with self-supervised pre-training. As shown in Table 2 (last two
rows), DCRNN with self-supervised pre-training outperforms its non-pretrained counterpart on
both seizure detection and classiﬁcation. Notably, Dist-DCRNN with self-supervised pre-training
achieves an AUROC of 0.875 for 60-s seizure detection, matching the performance of a CNN (AU-
ROC=0.88) that was pre-trained using supervised learning on a labeled dataset that was ﬁve times
larger than TUSZ (Saab et al., 2020). Importantly, the pre-trained model weights are used for both
seizure detection and classiﬁcation, which indicates that our self-supervised pre-training method
provides good model initialization that generalizes across tasks."
EXPERIMENTAL RESULTS,0.3096774193548387,"Figure 2a shows the ROC curves of median DCRNNs and baselines for seizure detection. At a low
false positive rate (FPR), such as 25% FPR, pre-trained Dist-DCRNN achieves 84.3% true positive
rate (TPR) and pre-trained Corr-DCRNN achieves 81.7% TPR on 12-s clips. Conversely, Dense-
CNN, LSTM, and CNN-LSTM only achieve 72.8%, 67.0%, and 62.5% TPRs, respectively. Figure
2b shows the confusion matrices for Dist-DCRNNs and baselines for 12-s seizure classiﬁcation.
Dist-DCRNN without self-supervised pre-training achieves 93% accuracy on the rare AB seizures,
providing 2 points increase over the best baseline. Furthermore, Dist-DCRNN with self-supervised
pre-training achieves 74% accuracy on the rare CT seizures, providing 47 points increase in accuracy
over the best baseline (Dense-CNN) and 48 points increase over non-pretrained Dist-DCRNN."
EXPERIMENTAL RESULTS,0.31290322580645163,"Surprisingly, despite being a majority class, many GN seizures are misclassiﬁed as CF seizures
(Figure 2b). A board-certiﬁed neurologist manually analyzed the EEGs of 32 misclassiﬁed test GN
seizures. We ﬁnd that 27 seizures are in fact focal seizures but are mislabeled as GN seizures. In
contrast, only 5 seizures are indeed generalized seizures but are misclassiﬁed by our models."
EXPERIMENTAL RESULTS,0.3161290322580645,"Comparison between self-supervised pre-training and transfer learning. To compare our self-
supervised pre-training strategy to traditional transfer learning approaches, we pre-train DCRNNs
for seizure detection and self-supervised prediction, respectively, on a large in-house dataset (40,316
EEGs, Table 7) and ﬁnetune the models for seizure detection and classiﬁcation on TUSZ. We ﬁnd
that self-supervised pre-training consistently outperforms transfer learning (Appendix J)."
EXPERIMENTAL RESULTS,0.3193548387096774,"Comparison between self-supervised pre-training and auxiliary learning. We also investigate
whether using the self-supervised task as an auxiliary task can outperform self-supervised pre-"
EXPERIMENTAL RESULTS,0.3225806451612903,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.3258064516129032,"Figure 2:
(a) ROC curves of
median models for seizure de-
tection.
(b) Confusion matri-
ces (averaged across ﬁve ran-
dom runs) for the baselines and
Dist-DCRNN without and with
self-supervised pre-training for
12-s seizure classiﬁcation. Each
row of the confusion matrices is
normalized by dividing by the
number of examples in the cor-
responding class, such that each
row sums up to one."
EXPERIMENTAL RESULTS,0.32903225806451614,"Figure 3:
Occlusion maps for
seizure detection obtained from
Corr-DCRNN model for (a)–(b)
focal, and (c)–(d) generalized
seizures. In each subﬁgure, left
panel shows the occlusion map
for 12-s when a seizure occurs
and right panel shows the occlu-
sion map values averaged along
time and overlaid on the corre-
sponding correlation graph."
EXPERIMENTAL RESULTS,0.33225806451612905,"training. We ﬁnd that auxiliary learning performs comparable to self-supervised pre-training on 12-s
seizure detection, whereas self-supervised pre-training signiﬁcantly outperforms auxiliary learning
on 60-s seizure detection. See Appendix M and Table 10 for details."
EXPERIMENTAL RESULTS,0.33548387096774196,"Improved model interpretability and ability to localize seizures. We leverage occlusion-based
techniques to localize seizures using our model predictions. Figure 3 shows example occlusion maps
of correctly predicted 60-s test EEG clips from Corr-DCRNN with self-supervised pre-training. We
observe that high saliency for focal seizures (Figures 3a–3b) is localized in the more abnormal EEG
channels, whereas high saliency for generalized seizures (Figures 3c–3d) is more diffuse across
channels. These patterns reﬂect the underlying brain activity of focal seizures (i.e. localized in
one brain area) and generalized seizures (i.e. occur in all areas of the brain) (Fisher et al., 2017).
Moreover, one can also interpret the seizure locations from the occlusion map overlaid on the graphs.
In Figure 3a, Fp2, F4 and F8 are highlighted on the graph occlusion map, which correspond to the
right frontal, central frontal, and anterior frontal brain regions that show the most abnormality in the
EEG. In Figure 3b, while the seizure starts in P3, Fp1, F3, and F7, the graph occlusion map mainly
highlights the central parietal region (P3), which is likely due to the ongoing artifact in channels
Fp1-F7, Fp1-F3, and F3-C3. Figure 3c–3d show two generalized seizures that occur in all regions
of the brain, and the highlighted nodes on the graphs are indeed more spread out across channels. In
contrast, high saliency from Dense-CNN does not localize in any seizure regions (Appendix H)."
EXPERIMENTAL RESULTS,0.3387096774193548,"Additionally, we leverage detailed annotations of seizure duration and location available in TUSZ
dataset, and use coverage and localization scores to quantify the models’ ability to accurately local-
ize seizures. Figure 4 shows coverage and localization distributions for Dense-CNN and DCRNNs
(w/o and with pre-training) on correctly predicted 60-s test EEG clips. Both Dist-DCRNN and
Corr-DCRNN have many more occlusion maps with high coverage and high localization scores
than Dense-CNN. This suggests that DCRNN more accurately localizes seizures than Dense-CNN,
which is intuitive given that DCRNN captures the connectivity of EEG electrodes."
EXPERIMENTAL RESULTS,0.3419354838709677,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.34516129032258064,"Clinically, localizing focal seizure onset regions is key to epilepsy surgery for patients with focal
seizures. Thus, the model’s ability to identify focal seizure regions as precisely as possible would
be highly desirable. Notably, pre-trained Corr-DCRNN precisely localizes 25.4% focal seizures and
pre-trained Dist-DCRNN precisely localizes 21.8% focal seizures (localization score > 0.8, Figure
4c). Conversely, both non-pretrained Dist-DCRNN and Corr-DCRNN precisely localize 6.3% focal
seizures, and Dense-CNN only localizes 3.5% focal seizures precisely (localization score > 0.8).
Figures 4e–4f show example occlusion maps of test focal seizures whose localization scores > 0.9
from pre-trained Corr-DCRNN, where high saliency overlaps well with annotated seizure regions."
EXPERIMENTAL RESULTS,0.34838709677419355,"Lastly, interpretability experiments on seizure classiﬁcation show that high saliency EEG channels
correspond to where the seizures are localized in CF seizures (Appendix I). This again suggests that
our method could inform seizure locations for treatment strategy if used clinically."
EXPERIMENTAL RESULTS,0.35161290322580646,"Figure 4: Distributions of (a)–(b) coverage and (c)–(d) localization scores for Dense-CNN and
DCRNNs on correctly predicted 60-s EEG clips for focal and generalized seizures. (e)–(f) Example
occlusion maps with focal seizure whose localization >0.9 from pre-trained Corr-DCRNN. Only
regions with normalized occlusion value >0.5 are colored. Red boxes are annotated seizure regions."
EXPERIMENTAL RESULTS,0.3548387096774194,"Comparison between graph structures. While Dist-DCRNN and Corr-DCRNN perform compa-
rable to each other on seizure detection and classiﬁcation (Table 2), we observe that Corr-DCRNN
better localizes focal seizures than Dist-DCRNN, particularly when combined with self-supervised
pre-training (e.g., coverage and localization scores > 0.9 in Figure 4a, 4c). This suggests that the
correlation-based graph structure could provide better interpretability and representation of focal
seizure EEGs. Moreover, the correlation graph structure has two particular advantages: (a) it can
be used even when the physical locations of electrodes are unknown and (b) it captures dynamic
brain connectivity rather than replying purely on spatial sensor information, which is particularly
desirable for automated seizure detection and classiﬁcation models."
CONCLUSION,0.3580645161290323,"4
CONCLUSION"
CONCLUSION,0.36129032258064514,"In conclusion, we present a novel method combining graph-based modeling and self-supervised pre-
training for EEG-based seizure detection and classiﬁcation, as well as an interpretability method to
quantify model ability to localize seizures. Our method sets new state-of-the-art on both seizure
detection and classiﬁcation on a large public dataset, signiﬁcantly improves classiﬁcation of rare
seizure classes, and more accurately localizes seizures. We also ﬁnd that the correlation-based
graph more accurately localizes focal seizures than distance-based graph. The improved ability to
localize seizures and the novel graph visualizations could provide clinicians with valuable insights
about localized seizure regions in real-world clinical settings. Looking to the future, because our
methods are not conﬁned to EEG alone, our study opens exciting opportunities to build graph-based
representations for a wide variety of medical time series."
CONCLUSION,0.36451612903225805,Published as a conference paper at ICLR 2022
CONCLUSION,0.36774193548387096,ACKNOWLEDGMENTS
CONCLUSION,0.3709677419354839,"This work was supported by a Wu Tsai Neurosciences Institute Neuroscience Translate Grant. The
authors would like to thank Bibek Paudel, Liangqiong Qu, Jean Benoit Delbrouck, and Nandita
Bhaskhar for their feedback on the paper."
ETHICS STATEMENT,0.3741935483870968,ETHICS STATEMENT
ETHICS STATEMENT,0.3774193548387097,"The Temple University Hospital EEG Seizure Corpus used in our study is anonymized and publicly
available3 with full IRB approval (Shah et al., 2018; Obeid & Picone, 2016). No conﬂict of interest
is reported from the authors. No harmful insights are provided by the seizure detection and classiﬁ-
cation models described in this study. Although we show that our methods could provide improved
performance and clinical utility for seizure detection and classiﬁcation, additional model valida-
tions are needed before they can be used in real-world clinical settings, including (a) validation on
datasets from multiple institutions, (b) validation on different populations and age groups, and (c)
validation on a EEG waveform-based seizure classiﬁcation scheme that is approved by a consensus
of board-certiﬁed neurologists."
REPRODUCIBILITY STATEMENT,0.38064516129032255,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.38387096774193546,"The Temple University Hospital EEG Seizure Corpus used in our study is publicly available. De-
tailed data preprocessing steps are provided in Appendix A. Source code is publicly available at
https://github.com/tsy935/eeg-gnn-ssl."
REFERENCES,0.3870967741935484,REFERENCES
REFERENCES,0.3903225806451613,"Jayant N Acharya, Abeer J Hani, Parthasarathy Thirumala, and Tammy N Tsuchida. American
clinical neurophysiology society guideline 3: a proposal for standard montages to be used in
clinical eeg. The Neurodiagnostic Journal, 56(4):253–260, 2016."
REFERENCES,0.3935483870967742,"David Ahmedt-Aristizabal, Tharindu Fernando, Simon Denman, Lars Petersson, Matthew J Aburn,
and Clinton Fookes. Neural memory networks for seizure type classiﬁcation. In 2020 42nd Annual
International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp.
569–575. IEEE, 2020."
REFERENCES,0.3967741935483871,"Umar Asif, Subhrajit Roy, Jianbin Tang, and Stefan Harrer. Seizurenet: Multi-spectral deep feature
learning for seizure type classiﬁcation. In Seyed Mostafa Kia, Hassan Mohy-ud Din, Ahmed
Abdulkadir, Cher Bass, Mohamad Habes, Jane Maryam Rondina, Chantal Tax, Hongzhi Wang,
Thomas Wolfers, Saima Rathore, and Madhura Ingalhalikar (eds.), Machine Learning in Clinical
Neuroimaging and Radiogenomics in Neuro-oncology, pp. 77–87, Cham, 2020. Springer Interna-
tional Publishing."
REFERENCES,0.4,"Hubert Banville, Omar Chehab, Aapo Hyvarinen, Denis Engemann, and Alexandre Gramfort. Un-
covering the structure of clinical eeg signals with self-supervised learning. Journal of Neural
Engineering, 2020."
REFERENCES,0.4032258064516129,"Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
ric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42,
2017."
REFERENCES,0.4064516129032258,"Ed Bullmore and Olaf Sporns. Complex brain networks: graph theoretical analysis of structural and
functional systems. Nature reviews neuroscience, 10(3):186–198, 2009."
REFERENCES,0.4096774193548387,"Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher R´e, and Kevin Murphy.
Machine
learning on graphs: A model and comprehensive taxonomy. arXiv preprint arXiv:2005.03675,
2020."
REFERENCES,0.4129032258064516,3https://www.isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml
REFERENCES,0.4161290322580645,Published as a conference paper at ICLR 2022
REFERENCES,0.41935483870967744,"Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103–111, Doha,
Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-4012.
URL https://www.aclweb.org/anthology/W14-4012."
REFERENCES,0.42258064516129035,"Giulia Cisotto, Alessio Zanga, Joanna Chlebus, Italo Zoppis, Sara Manzoni, and Urszula
Markowska-Kaczmar. Comparison of attention-based deep learning models for eeg classiﬁca-
tion. arXiv preprint arXiv:2012.01074, 2020."
REFERENCES,0.4258064516129032,"Ian C. Covert, Balu Krishnan, Imad Najm, Jiening Zhan, Matthew Shore, John Hixson, and
Ming Jack Po. Temporal graph convolutional networks for automatic seizure detection. In Pro-
ceedings of the 4th Machine Learning for Healthcare Conference, volume 106 of Proceedings of
Machine Learning Research, pp. 160–180, Ann Arbor, Michigan, 09–10 Aug 2019. PMLR."
REFERENCES,0.4290322580645161,"Jeff Craley, Emily Johnson, and Archana Venkataraman. Integrating convolutional neural networks
and probabilistic graphical modeling for epileptic seizure detection in multichannel eeg. In In-
ternational Conference on Information Processing in Medical Imaging, pp. 291–303. Springer,
2019."
REFERENCES,0.432258064516129,"Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral ﬁltering. In Advances in neural information processing systems,
pp. 3844–3852, 2016."
REFERENCES,0.43548387096774194,"Robert S. Fisher, J. Helen Cross, Jacqueline A. French, Norimichi Higurashi, Edouard Hirsch,
Floor E. Jansen, Lieven Lagae, Solomon L. Mosh´e, Jukka Peltola, Eliane Roulet Perez, Ingrid E.
Scheffer, and Sameer M. Zuberi. Operational classiﬁcation of seizure types by the international
league against epilepsy: Position paper of the ilae commission for classiﬁcation and terminology.
Epilepsia, 58(4):522–530, 2017. doi: https://doi.org/10.1111/epi.13670."
REFERENCES,0.43870967741935485,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.44193548387096776,"Tomas Iesmantas and Robertas Alzbutas. Convolutional neural network for detection and classiﬁ-
cation of seizures in clinical data. Medical & Biological Engineering & Computing, 58, 06 2020.
doi: 10.1007/s11517-020-02208-7."
REFERENCES,0.44516129032258067,"Herbert H Jasper.
The ten-twenty electrode system of the international federation.
Electroen-
cephalogr. Clin. Neurophysiol., 10:370–375, 1958."
REFERENCES,0.4483870967741935,"Diederik Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
International
Conference on Learning Representations, 12 2014."
REFERENCES,0.45161290322580644,"Demetres Kostas, Stephane Aroca-Ouellette, and Frank Rudzicz. Bendr: using transformers and a
contrastive self-supervised learning task to learn from massive amounts of eeg data, 2021."
REFERENCES,0.45483870967741935,"Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven trafﬁc forecasting. In 6th International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceed-
ings, 2018."
REFERENCES,0.45806451612903226,"Yang Li, Yu Liu, Yu-Zhu Guo, Xiao-Feng Liao, Bin Hu, and Tao Yu. Spatio-temporal-spectral
hierarchical graph convolutional network with semisupervised active learning for patient-speciﬁc
seizure prediction. IEEE Transactions on Cybernetics, 2021."
REFERENCES,0.4612903225806452,"Hong Liu, Jeff Z HaoChen, Adrien Gaidon, and Tengyu Ma. Self-supervised learning is more robust
to dataset imbalance. arXiv preprint arXiv:2110.05025, 2021."
REFERENCES,0.4645161290322581,"Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017."
REFERENCES,0.46774193548387094,Published as a conference paper at ICLR 2022
REFERENCES,0.47096774193548385,"Michael L Martini, Aly A Valliani, Claire Sun, Anthony B Costa, Shan Zhao, Fedor Panov, Saadi
Ghatan, Kanaka Rajan, and Eric Karl Oermann. Deep anomaly detection of seizures with paired
stereoelectroencephalography and video recordings. Scientiﬁc Reports, 11(1):1–11, 2021."
REFERENCES,0.47419354838709676,"Mostafa Neo Mohsenvand, Mohammad Rasool Izadi, and Pattie Maes. Contrastive representation
learning for electroencephalogram classiﬁcation. In Machine Learning for Health, pp. 238–253.
PMLR, 2020."
REFERENCES,0.4774193548387097,"Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. Frontiers in neuro-
science, 10:196, 2016."
REFERENCES,0.4806451612903226,"Alison O’Shea, Gordon Lightbody, Geraldine Boylan, and Andriy Temko. Neonatal seizure de-
tection from raw multi-channel EEG using a fully convolutional architecture.
Neural Net-
works, 123:12–25, 2020. ISSN 08936080. doi: 10.1016/j.neunet.2019.11.023. URL https:
//linkinghub.elsevier.com/retrieve/pii/S0893608019303910."
REFERENCES,0.4838709677419355,"Shivarudhrappa Raghu, Natarajan Sriraam, Yasin Temel, Shyam Vasudeva Rao, and Pieter L
Kubben. Eeg based multi-class seizure type classiﬁcation using convolutional neural network
and transfer learning. Neural Networks, 124:202–212, 2020."
REFERENCES,0.4870967741935484,"Khansa Rasheed, Adnan Qayyum, Junaid Qadir, Shobi Sivathamboo, Patrick Kwan, Levin
Kuhlmann, Terence O’Brien, and Adeel Razi. Machine learning for predicting epileptic seizures
using eeg signals: A review. IEEE Reviews in Biomedical Engineering, 14:139–155, 2020."
REFERENCES,0.49032258064516127,"Subhrajit Roy, Umar Asif, Jianbin Tang, and Stefan Harrer.
Machine learning for seizure type
classiﬁcation: setting the benchmark. arXiv preprint arXiv:1902.01012, 2019."
REFERENCES,0.4935483870967742,"Khaled Saab, Jared Dunnmon, Christopher R´e, Daniel Rubin, and Christopher Lee-Messer. Weak
supervision as an efﬁcient approach for automated seizure detection in electroencephalography.
npj Digital Medicine, 3(1):1–12, 2020."
REFERENCES,0.4967741935483871,"Vangelis Sakkalis. Review of advanced techniques for the estimation of brain connectivity measured
with eeg/meg. Computers in biology and medicine, 41(12):1110–1117, 2011."
REFERENCES,0.5,"Vinit Shah, Eva von Weltin, Silvia Lopez, James Riley McHugh, Lillian Veloso, Meysam Golmo-
hammadi, Iyad Obeid, and Joseph Picone. The temple university hospital seizure detection corpus.
Frontiers in Neuroinformatics, 12:83, 2018. ISSN 1662-5196. doi: 10.3389/fninf.2018.00083."
REFERENCES,0.5032258064516129,"Afshin Shoeibi, Marjane Khodatars, Navid Ghassemi, Mahboobeh Jafari, Parisa Moridian, Roohal-
lah Alizadehsani, Maryam Panahiazar, Fahime Khozeimeh, Assef Zare, Hossein Hosseini-Nejad,
et al. Epileptic seizures detection using deep learning techniques: A review. International Journal
of Environmental Research and Public Health, 18(11):5780, 2021."
REFERENCES,0.5064516129032258,"David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The
emerging ﬁeld of signal processing on graphs: Extending high-dimensional data analysis to net-
works and other irregular domains. IEEE signal processing magazine, 30(3):83–98, 2013."
REFERENCES,0.5096774193548387,"Mohammad Khubeb Siddiqui, Ruben Morales-Menendez, Xiaodi Huang, and Nasir Hussain. A
review of epileptic seizure detection using machine learning classiﬁers. Brain informatics, 7:
1–18, 2020."
REFERENCES,0.5129032258064516,"Micheal Strein, John P Holton-Burke, LaTangela R Smith, and Gretchen M Brophy. Prevention,
treatment, and monitoring of seizures in the intensive care unit. Journal of Clinical Medicine, 8
(8):1177, 2019."
REFERENCES,0.5161290322580645,"Supriya Supriya, Siuly Siuly, Hua Wang, and Yanchun Zhang. Epilepsy detection from eeg using
complex network techniques: A review. IEEE Reviews in Biomedical Engineering, 2021."
REFERENCES,0.5193548387096775,"Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104–3112, 2014."
REFERENCES,0.5225806451612903,"Alexandros T Tzallas, Markos G Tsipouras, and Dimitrios I Fotiadis. Epileptic seizure detection in
eegs using time–frequency analysis. IEEE transactions on information technology in biomedicine,
13(5):703–710, 2009."
REFERENCES,0.5258064516129032,Published as a conference paper at ICLR 2022
REFERENCES,0.5290322580645161,"Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna-
peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der
Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel-
son, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore,
Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mul-
bregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing
in Python. Nature Methods, 17:261–272, 2020a. doi: 10.1038/s41592-019-0686-2."
REFERENCES,0.532258064516129,"Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau,
Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. Scipy 1.0: funda-
mental algorithms for scientiﬁc computing in python. Nature methods, 17(3):261–272, 2020b."
REFERENCES,0.535483870967742,"WHO.
Epilepsy.
https://www.who.int/news-room/fact-sheets/detail/
epilepsy, june 2019."
REFERENCES,0.5387096774193548,"Junjie Xu, Yaojia Zheng, Yifan Mao, Ruixuan Wang, and Wei-Shi Zheng. Anomaly detection on
electroencephalography with self-supervised learning. In 2020 IEEE International Conference
on Bioinformatics and Biomedicine (BIBM), pp. 363–368. IEEE, 2020."
REFERENCES,0.5419354838709678,"Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced learning.
In Conference on Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5451612903225806,"Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks, 2013."
REFERENCES,0.5483870967741935,"Yanna Zhao, Gaobo Zhang, Changxu Dong, Qi Yuan, Fangzhou Xu, and Yuanjie Zheng. Graph
attention network with focal loss for seizure detection on electroencephalography signals. Inter-
national Journal of Neural Systems, pp. 2150027, 2021."
REFERENCES,0.5516129032258065,Published as a conference paper at ICLR 2022
REFERENCES,0.5548387096774193,APPENDIX
REFERENCES,0.5580645161290323,"A
DATA PREPROCESSING"
REFERENCES,0.5612903225806452,"Because the EEG signals are sampled at different frequencies in the Temple University EEG Seizure
Corpus (TUSZ), we resample them to the same frequency of 200 Hz using the “resample” function
in SciPy python package (Virtanen et al., 2020a). We perform the following preprocessing steps to
obtain EEG clips in the frequency domain and their corresponding labels."
REFERENCES,0.5645161290322581,"First, for seizure detection, we use both seizure and non-seizure EEGs. We obtain the EEG clips by
sliding a 12-s (or 60-s) window over the EEG signals without overlaps, and ignore the last window
if it is shorter than the clip length. The label for each clip is y = 1 if at least one seizure event occurs
within this clip, otherwise y = 0."
REFERENCES,0.567741935483871,"Second, for seizure classiﬁcation, we use only seizure EEGs. We obtain one 12-s (or 60-s) EEG
clip for each seizure event starting at 2-s before the annotated seizure onset time, where a 2-s offset
accounts for tolerance in the annotations. If a seizure event is shorter than 12-s (or 60-s), the EEG
clip is truncated at the end of the seizure to prevent a clip from having multiple seizure types. The
label for each clip is the index of the corresponding seizure class, i.e. y ∈{0, 1, 2, 3}, which
corresponds to combined focal (CF) seizures, generalized non-speciﬁc (GN) seizures, absence (AB)
seizures, and combined tonic (CT) seizures."
REFERENCES,0.5709677419354838,"Third, for self-supervised pre-training, we use the same EEG clips as seizure detection."
REFERENCES,0.5741935483870968,"Fourth, for each EEG clip in each of the seizure detection/seizure classiﬁcation/self-supervised pre-
training tasks, we perform the following preprocessing steps to transform the signals in time domain
to frequency domain: (a) slide a t second window over the EEG clip without overlap, where t is
the time step size for networks involving recurrent layers; (b) apply fast Fourier transform (FFT)
to each t second window using the “fft” function in Scipy python package (Virtanen et al., 2020b),
and retain the log amplitudes of the non-negative frequency components similar to prior studies
(Asif et al., 2020; Ahmedt-Aristizabal et al., 2020; Covert et al., 2019); (c) z-normalize the EEG
clip with respect to the mean and standard deviation of the training data. Because EEG clips for
seizure classiﬁcation may have variable lengths due to short seizures, we pad the clips with 0’s to
facilitate model training in batches. We use t = 1 second as a natural choice of the time step size.
After preprocessing, each EEG clip can be denoted as X ∈RT ×N×M, where T = 12 (or T = 60)
represents the clip length, N = 19 represents the number of EEG channels/electrodes, and M = 100
represents the feature dimension after the aforementioned Fourier transform."
REFERENCES,0.5774193548387097,"B
EFFECTIVENESS OF FOURIER TRANSFORM"
REFERENCES,0.5806451612903226,"To evaluate the effectiveness of Fourier transform (Appendix A), we compare the performance of
our DCRNN (without self-supervised pre-training) on inputs without and with Fourier transform
on both seizure detection and seizure classiﬁcation. As shown in Table 4, frequency-domain inputs
(with Fourier transform) result in signiﬁcantly better performance than time-domain inputs (without
Fourier transform). This is likely because seizures are associated with electrical activity in certain
frequency bands (Tzallas et al., 2009), and thus short-time-interval frequency-domain inputs could
be more informative than time-domain inputs."
REFERENCES,0.5838709677419355,"C
DISTINGUISHABILITY OF FOCAL NON-SPECIFIC, SIMPLE PARTIAL, AND
COMPLEX PARTIAL SEIZURES"
REFERENCES,0.5870967741935483,"Because simple partial and complex partial seizures are focal seizures characterized by the clinical
behavior, consciousness during seizure (Fisher et al., 2017), they are not distinguishable from other
focal seizures from EEG signals alone. In this study, we merge focal non-speciﬁc (FN), simple
partial (SP), and complex partial (CP) seizures into a combined focal seizure class. To justify our
decision, we perform classiﬁcation of these focal seizure types using our DCRNNs and baselines.
As shown in Figure 5, SP and CP seizures are largely misclassiﬁed as FN seizures, suggesting that"
REFERENCES,0.5903225806451613,Published as a conference paper at ICLR 2022
REFERENCES,0.5935483870967742,"Table 4: Seizure detection and classiﬁcation results from DCRNNs (without self-supervised pre-
training) on time-domain inputs and frequency-domain inputs. Mean and standard deviations are
obtained from ﬁve random runs."
REFERENCES,0.5967741935483871,"Model
Input Domain"
REFERENCES,0.6,"Seizure Detection
AUROC"
REFERENCES,0.603225806451613,"Seizure Classiﬁcation
Weighted F1-Score
12-s
60-s
12-s
60-s
Corr-DCRNN
Without Pre-Training"
REFERENCES,0.6064516129032258,"Time
0.717 ± 0.003
0.704 ± 0.023
0.597 ± 0.023
0.618 ± 0.018
Frequency
0.812 ± 0.012
0.804 ± 0.015
0.710 ± 0.023
0.701 ± 0.030"
REFERENCES,0.6096774193548387,"Dist-DCRNN
Without Pre-Training"
REFERENCES,0.6129032258064516,"Time
0.733 ± 0.012
0.698 ± 0.003
0.592 ± 0.022
0.608 ± 0.015
Frequency
0.825 ± 0.019
0.793 ± 0.022
0.703 ± 0.025
0.690 ± 0.035"
REFERENCES,0.6161290322580645,"machine learning models may not be able to distinguish these focal seizure types from EEG signals
alone. This supports our decision of combining FN, SP, and CP seizures into one class."
REFERENCES,0.6193548387096774,"D
TRAIN, VALIDATION, AND TEST SETS"
REFERENCES,0.6225806451612903,"Table 5 shows the number of EEG clips and number of patients in our train, validation, and test sets
for self-supervised pre-training, seizure detection, and seizure classiﬁcation tasks. Train, validation,
and test sets consist of distinct patients."
REFERENCES,0.6258064516129033,"Table 5: Number of EEG clips and patients in the train, validation, and test sets in our study. Train,
validation, and test sets consist of distinct patients."
REFERENCES,0.6290322580645161,"EEG Clip
Length
(Secs)"
REFERENCES,0.632258064516129,"Train Set
Validation Set
Test Set
EEG Clips
(% Seizure)"
REFERENCES,0.635483870967742,"Patients
(% Seizure)"
REFERENCES,0.6387096774193548,"EEG Clips
(% Seizure)"
REFERENCES,0.6419354838709678,"Patients
(% Seizure)"
REFERENCES,0.6451612903225806,"EEG Clips
(% Seizure)"
REFERENCES,0.6483870967741936,"Patients
(% Seizure)"
REFERENCES,0.6516129032258065,"Pre-training
& Seizure
Detection"
-S,0.6548387096774193,"60-s
38,613
(9.3%)"
-S,0.6580645161290323,"530
(34.0%)"
-S,0.6612903225806451,"5,503
(11.4%)"
-S,0.6645161290322581,"61
(36.1%)"
-S,0.667741935483871,"8,848
(14.7%)"
-S,0.6709677419354839,"45
(77.8%)"
-S,0.6741935483870968,"12-s
196,646
(6.9%)"
-S,0.6774193548387096,"531
(33.9%)"
-S,0.6806451612903226,"28,057
(8.7%)"
-S,0.6838709677419355,"61
(36.1%)"
-S,0.6870967741935484,"44,959
(10.9%)"
-S,0.6903225806451613,"45
(77.8%)"
-S,0.6935483870967742,"Seizure
Classiﬁcation"
-S,0.6967741935483871,"60-s
& 12-s"
-S,0.7,"1,925
(100.0%)"
-S,0.7032258064516129,"179
(100.0%)"
-S,0.7064516129032258,"450
(100.0%)"
-S,0.7096774193548387,"22
(100.0%)"
-S,0.7129032258064516,"521
(100.0%)"
-S,0.7161290322580646,"34
(100.0%)"
-S,0.7193548387096774,"E
DETAILS OF MODEL TRAINING PROCEDURES AND HYPERPARAMETERS"
-S,0.7225806451612903,"We performed the following hyperparameter search on the validation set: (a) initial learning rate
within range [5e-5, 1e-3]; (b) τ ∈{2, 3, 4}, the number of neighbors to keep for each node in the
correlation graphs; (c) the number of Diffusion Convolutional Gated Recurrent Units (DCGRU) lay-
ers within range {2, 3, 4, 5} and hidden units within range {32, 64, 128}; (d) the maximum diffusion
step K ∈{2, 3, 4}; (e) dropout probability in the last fully connected layer. We used a batch size of
40 EEG clips, the maximum possible across all models and baselines on a single Titan RTX GPU.
The hyperparameters were selected based on the best performance on the validation set, and are
detailed in the next sections. We used the cosine annealing learning rate scheduler (Loshchilov &
Hutter, 2017) in PyTorch for all model training. We ran ﬁve runs with different random seeds for
all models. In all experiments, model training was early stopped when the validation loss did not
decrease for ﬁve consecutive epochs."
-S,0.7258064516129032,"Model training for seizure detection. For seizure detection, the plentiful negative examples in the
train set were undersampled such that the train set had 50% positive examples, which resulted in
27,292 training examples for 12-s clips and 7,188 training examples for 60-s clips. We used binary
cross-entropy as the loss function to train the seizure detection models. The models were trained
for 100 epochs with an initial learning rate of 1e-4. For the correlation graphs, the top-3 neighbors’
edges were kept for each node. The maximum number of diffusion step was 2, and the dropout
probability was 0 (i.e. no dropout). The model consists of two stacked DCGRU layers with 64
hidden units, resulting in 168,641 trainable parameters for the distance graph and 280,769 trainable"
-S,0.7290322580645161,Published as a conference paper at ICLR 2022
-S,0.7322580645161291,"Figure 5: Confusion matrices (averaged across ﬁve random runs) of focal seizure classiﬁcation on
(a) 60-s EEG clips, and (b) 12-s EEG clips. All models fail to distinguish well among these
focal seizure types, supporting our decision of merging these seizure types into one combined focal
seizure class. Note that each row of the confusion matrices is normalized by dividing by the number
of examples in the corresponding class, such that each row sums up to one."
-S,0.7354838709677419,"parameters for the correlation graphs. Model training for seizure detection took about 20-min for
12-s EEG clips, and about 30-min for 60-s EEG clips."
-S,0.7387096774193549,"To obtain the ﬁnal seizure/non-seizure prediction, we performed decision threshold search on the
validation set. More speciﬁcally, to balance between precision and recall scores, we selected the"
-S,0.7419354838709677,Published as a conference paper at ICLR 2022
-S,0.7451612903225806,"decision threshold that results in the highest F1-score on the validation set. When evaluating the
models on the test set, EEG clips with probabilities above this decision threshold are predicted as
seizures, while clips with probabilities below this decision threshold are predicted as non-seizures."
-S,0.7483870967741936,"Model training for seizure classiﬁcation. For seizure classiﬁcation, we used multi-class cross-
entropy as the loss function during training. The models were trained for 60 epochs with an initial
learning rate of 3e-4. For the correlation graphs, the top-3 neighbors’ edges were kept for each
node. The maximum number of diffusion step was 2, and the dropout probability was 0.5. The
model consists of two stacked DCGRU layers with 64 hidden units, resulting in 168,836 trainable
parameters for the distance graph and 280,964 trainable parameters for the correlation graphs. Model
training for seizure classiﬁcation took about 3-min for 12-s EEG clips, and about 7-min for 60-s EEG
clips."
-S,0.7516129032258064,"Model training for self-supervised task. Preliminary experiments suggested that predicting future
T ′ = 12 second preprocessed EEG clips results in low regression loss on the validation set given
previous 12-s (60-s) preprocessed clips, and thus we used T ′ = 12 in all self-supervised pre-training
experiments. For self-supervised pre-training, we used mean absolute error (MAE) as the loss func-
tion. The models were trained for 350 epochs with an initial learning rate of 5e-4. For the correlation
graphs, the top-3 neighbors’ edges were kept for each node. The maximum number of diffusion step
was 2. The model consists of three stacked DCGRU layers with 64 hidden units in both the encoder
and decoder, resulting in 417,572 trainable parameters for the distance graph and 690,980 trainable
parameters for the correlation graphs. Model training for self-supervised prediction took about 10-h
for 12-s EEG clips, and about 24-h for 60-s EEG clips."
-S,0.7548387096774194,"Model training for baselines. For baseline Dense-CNN, we employ the same model architecture
as that described in Saab et al. (2020). For baseline LSTM (Hochreiter & Schmidhuber, 1997), we
have the number of LSTM layers and hidden units the same as the number of DCGRU layers and
hidden units in our DCRNN model. For baseline CNN-LSTM, we use the same model architecture
described in Ahmedt-Aristizabal et al. (2020), i.e., two stacked convolutional layers (32 3 × 3 ker-
nels), one max-pooling layer (2 × 2), one fully-connected layer (output neuron = 512), two stacked
LSTM layers (hidden size = 128), and one fully connected layer."
-S,0.7580645161290323,"F
DATA AUGMENTATION"
-S,0.7612903225806451,"During training, we performed the following data augmentations based on EEG domain knowledge:
(a) randomly scaling the amplitude of the raw EEG signals by a scale within [0.8, 1.2] and (b)
randomly reﬂecting the signals along the scalp midline."
-S,0.7645161290322581,"G
ADDITIONAL EVALUATION RESULTS ON SEIZURE DETECTION"
-S,0.7677419354838709,"Table 6 shows F1-score, Area Under the Precision-Recall Curve (AUPR), sensitivity, and speciﬁcity
of seizure detection models."
-S,0.7709677419354839,"H
SEIZURE DETECTION OCCLUSION MAPS FROM BASELINE DENSE-CNN"
-S,0.7741935483870968,"Figure 6 shows example seizure detection occlusion maps obtained from baseline Dense-CNN on
correctly predicted 60-s EEG clips in the test set. Unlike our model (Figure 3), high saliency from
Dense-CNN does not localize in any seizure regions."
-S,0.7774193548387097,"I
SEIZURE CLASSIFICATION OCCLUSION MAPS FROM DCRNN"
-S,0.7806451612903226,"Figure 7 shows example seizure classiﬁcation occlusion maps from Corr-DCRNN with self-
supervised pre-training on correctly predicted 60-s EEG clips in the test set. The occlusion maps
are obtained by completely dropping one EEG channel at a time and calculating the relative change
in the model output. For CF seizures (a–b), high saliency regions correspond to brain areas where
the focal seizures are localized. For the other generalized seizure types (c–e), less salient regions
correspond to less abnormal areas."
-S,0.7838709677419354,Published as a conference paper at ICLR 2022
-S,0.7870967741935484,"Table 6: Additional evaluation scores for seizure detection on (a) 12-s EEG clips and (b) 60-s EEG
clips. Mean and standard deviations are obtained from ﬁve random runs. Best non-pretrained and
pre-trained mean results are highlighted in bold."
-S,0.7903225806451613,(a) 12-s EEG clips
-S,0.7935483870967742,"Model
F1-Score
(mean ± std)"
-S,0.7967741935483871,"AUPR
(mean ± std)"
-S,0.8,"Sensitivity
(mean ± std)"
-S,0.8032258064516129,"Speciﬁcity
(mean ± std)
Dense-CNN
0.326 ± 0.019
0.328 ± 0.043
0.293 ± 0.021
0.938 ± 0.014
LSTM
0.376 ± 0.021
0.354 ± 0.023
0.357 ± 0.045
0.934 ± 0.015
CNN-LSTM
0.337 ± 0.009
0.309 ± 0.015
0.333 ± 0.028
0.920 ± 0.021"
-S,0.8064516129032258,"Corr-DCRNN
Without Pre-training
0.392 ± 0.027
0.370 ± 0.027
0.373 ± 0.035
0.935 ± 0.012"
-S,0.8096774193548387,"Dist-DCRNN
Without Pre-training
0.437 ± 0.029
0.411 ± 0.041
0.411 ± 0.038
0.943 ± 0.006"
-S,0.8129032258064516,"Corr-DCRNN
With Pre-training
0.484 ± 0.011
0.454 ± 0.020
0.524 ± 0.012
0.922 ± 0.004"
-S,0.8161290322580645,"Dist-DCRNN
With Pre-training
0.487 ± 0.042
0.463 ± 0.048
0.592 ± 0.052
0.897 ± 0.012"
-S,0.8193548387096774,(b) 60-s EEG clips
-S,0.8225806451612904,"Model
F1-Score
(mean ± std)"
-S,0.8258064516129032,"AUPR
(mean ± std)"
-S,0.8290322580645161,"Sensitivity
(mean ± std)"
-S,0.832258064516129,"Speciﬁcity
(mean ± std)
Dense-CNN
0.404 ± 0.022
0.399 ± 0.017
0.451 ± 0.134
0.869 ± 0.071
LSTM
0.365 ± 0.009
0.287 ± 0.026
0.463 ± 0.060
0.814 ± 0.053
CNN-LSTM
0.330 ± 0.016
0.276 ± 0.009
0.363 ± 0.044
0.857 ± 0.023"
-S,0.8354838709677419,"Corr-DCRNN
Without Pre-training
0.448 ± 0.029
0.440 ± 0.021
0.457 ± 0.058
0.900 ± 0.028"
-S,0.8387096774193549,"Dist-DCRNN
Without Pre-training
0.341 ± 0.170
0.418 ± 0.046
0.326 ± 0.183
0.932 ± 0.058"
-S,0.8419354838709677,"Corr-DCRNN
With Pre-training
0.514 ± 0.028
0.539 ± 0.024
0.502 ± 0.047
0.923 ± 0.008"
-S,0.8451612903225807,"Dist-DCRNN
With Pre-training
0.571 ± 0.029
0.593 ± 0.031
0.570 ± 0.047
0.927 ± 0.012"
-S,0.8483870967741935,"J
COMPARISON BETWEEN SELF-SUPERVISED PRE-TRAINING AND
TRANSFER LEARNING"
-S,0.8516129032258064,"To compare our self-supervised pre-training strategy to traditional transfer learning, we perform
transfer learning by pre-training DCRNNs for seizure detection on an in-house dataset (40,316
EEGs, Table 7) and ﬁnetuning on TUSZ data for seizure detection and classiﬁcation. Due to the lack
of ﬁne-grained seizure type labels, we do not pre-train DCRNNs for seizure classiﬁcation on the in-
house dataset. Moreover, we pre-train DCRNNs on the in-house dataset using our self-supervised
pre-training strategy and ﬁnetuned them for seizure detection and classiﬁcation on TUSZ."
-S,0.8548387096774194,"Table 8 shows DCRNN results with self-supervised pre-training and transfer learning from the in-
house dataset. Self-supervised pre-training from the in-house dataset (3rd-4th rows) consistently
outperforms trasnfer learning from the in-house dataset (5th-6th rows). We speculate that trans-
fer learning does not perform comparably to self-supervised pre-training because it suffers from
distribution shift in the data (i.e., the in-house dataset comes from a different population and uses a
slightly different EEG acquisition protocol). In contrast, by learning to predict the EEGs for the next"
-S,0.8580645161290322,Published as a conference paper at ICLR 2022
-S,0.8612903225806452,"Figure 6: Example occlusion maps for seizure detection obtained from baseline Dense-CNN on cor-
rectly predicted 60-s clips for (a)–(b) focal seizures, and (c)–(d) generalized seizures. Red boxes
indicate the duration of the seizures. Note that the values within an occlusion map are normalized,
and thus should not be compared across different occlusion maps."
-S,0.864516129032258,"time period, self-supervised pre-training encourages the model to learn task-agnostic representations
and thus mitigates the problem of distribution shift."
-S,0.867741935483871,"Table 7: Summary of in-house dataset. Only annotations for the start of seizure are available. For
EEG ﬁles with seizures, EEG clips are obtained by taking 12-s (or 60-s) signals starting from the
annotated seizure start time. For EEG ﬁles without seizures, EEG clips are obtained by taking 12-
s (or 60-s) signals randomly from the entire signal. We apply the same data preprocessing steps
described in Appendix A."
-S,0.8709677419354839,"EEG Files
(% Seizure)
Total Duration
Patients
(% Seizure)"
-S,0.8741935483870967,"EEG Clips
(% Seizure)"
-S,0.8774193548387097,"In-House Train Set
40,316 (24.1%)
853,141.87 min
5,355 (25.7%)
46,613 (34.4%)"
-S,0.8806451612903226,"K
CHOICE OF DISTANCE-BASED GRAPH STRUCTURE"
-S,0.8838709677419355,"For the distance graph, we used a threholded Gaussian kernel (Shuman et al., 2013) to compute the
edge weight between two electrodes (Section 2.2.1). We experimented with κ, the threshold for
graph sparsity, within range [0.1, 2]. Based on preliminary experiments and EEG domain knowl-
edge, we chose κ = 0.9 because it results in a reasonable graph (e.g. no long-range connection)
that resembles the EEG montage (longitudinal bipolar and transverse bipolar) widely used clinically
(Acharya et al., 2016). Figure 8 shows distance graphs resulting from different thresholds κ. In
Figure 8, we can see that a smaller threshold (e.g., 0.7 or 0.8) results in missing edges between"
-S,0.8870967741935484,Published as a conference paper at ICLR 2022
-S,0.8903225806451613,"Table 8: Comparison of results between self-supervised pre-training and transfer learning pre-
trained on an in-house dataset. 3rd-4th rows: DCRNN results with self-supervised (SS) pre-training
on the in-house dataset. 5th-6th rows: DCRNN results with transfer learning pre-trained for seizure
detection on the in-house dataset. Mean and standard deviations are from ﬁve random runs. Best
mean results for each column are highlighted in bold. Model"
-S,0.8935483870967742,"Seizure Detection
AUROC"
-S,0.896774193548387,"Seizure Classiﬁcation
Weighted F1-Score
12-s
60-s
12-s
60-s
Corr-DCRNN w/ SS
Pre-Training
0.863 ± 0.005
0.856 ± 0.013
0.736 ± 0.007
0.723 ± 0.010"
-S,0.9,"Dist-DCRNN w/ SS
Pre-Training
0.879 ± 0.006
0.886 ± 0.006
0.767 ± 0.038
0.718 ± 0.018"
-S,0.9032258064516129,"Corr-DCRNN w/ Transfer
Learning
0.848 ± 0.018
0.850 ± 0.002
0.733 ± 0.027
0.711 ± 0.025"
-S,0.9064516129032258,"Dist-DCRNN w/ Transfer
Learning
0.866 ± 0.001
0.847 ± 0.004
0.720 ± 0.046
0.691 ± 0.038"
-S,0.9096774193548387,"nearby electrodes. For example, there is no edge between FP1 and FZ for threshold 0.8 and no edge
between T3 and C3 for threshold 0.7. In contrast, a larger threshold (e.g., 1.0, 1.1, or 1.2) results
in edges connecting electrodes that are spatially far apart. For example, there is an edge connecting
C3 and FZ, as well as an edge between F7 and T5 for threshold 1.2. Using EEG domain knowl-
edge provided by a board certiﬁed neurologist, we believe that a threshold of 0.9 results in a more
reasonable distance-based EEG graph compared to other thresholding values."
-S,0.9129032258064517,"In addition, we explore using a Gaussian kernel with a speciﬁed bandwidth for building the distance"
-S,0.9161290322580645,"graph, i.e., Wij =
1
√"
-S,0.9193548387096774,"2πh2 exp

−dist(vi, vj)2 2h2"
-S,0.9225806451612903,"
, where Wij is the edge weight between electrodes"
-S,0.9258064516129032,"vi and vj, dist(vi, vj) is the Euclidean distance between vi and vj, and h is the Gaussian kernel band-
width. Figure 9 shows the distance graph structures resulting from different values of bandwidth.
With bandwidth = 0.06, the distance graph structure resembles the original graph structure using
thresholded Gaussian kernel with a threshold of 0.9 (Figure 1b)."
-S,0.9290322580645162,"L
8-CLASS SEIZURE CLASSIFICATION"
-S,0.932258064516129,"We also perform seizure classiﬁcation on the original eight seizure types4 available in TUSZ (see
Table 9). Note that only one patient’s two myoclonic seizures are available in the ofﬁcial TUSZ
train set and only one patient’s one myoclonic seizure is available in the ofﬁcial TUSZ test set.
Hence, patient-wise train/validation splits for myoclonic seizure is not possible, and we randomly
assign one myoclonic seizure in the ofﬁcial TUSZ train set to our train split and the other to our
validation split. As shown in Table 9, DCRNNs consistently outperform the baselines on 8-class
seizure classiﬁcation."
-S,0.9354838709677419,"M
USING SELF-SUPERVISED PREDICTION AS AN AUXILIARY TASK"
-S,0.9387096774193548,"Here, instead of pre-training DCRNNs for the self-supervised prediction task (i.e., predicting pre-
processed EEG clips for the next time period), we conduct experiments using the self-supervised
prediction task as an auxiliary task. Because EEG clips for the seizure classiﬁcation task have vari-
able lengths, we only perform this experiment for seizure detection to facilitate training in batches.
Speciﬁcally, the loss function is L = Lseizure detection + λLSS prediction where λ is a hyperparameter
balancing the seizure detection loss and the self-supervised loss and is tuned on the validation set.
For 12-s (or 60-s) seizure detection, the auxiliary task is to predict the next 6-s preprocessed EEG
clips given the ﬁrst 6-s (or 30-s) EEG clips."
-S,0.9419354838709677,"4The eight seizure types are: focal seizure, generalized non-speciﬁc seizure, simple partial seizure, complex
partial seizure, absence seizure, tonic seizure, tonic-clonic seizure, and myoclonic seizure."
-S,0.9451612903225807,Published as a conference paper at ICLR 2022
-S,0.9483870967741935,"Table 9: Results (weighted F1-scores) of seizure classiﬁcation on original 8 seizure types in TUSZ.
Mean and standard deviations are from ﬁve random runs. Best mean results for both non-pretrained
and pretrained models are highlighted in bold."
-S,0.9516129032258065,"Model
12-s
60-s
Dense-CNN
0.431 ± 0.037
0.427 ± 0.047
LSTM
0.515 ± 0.025
0.525 ± 0.017
CNN-LSTM
0.489 ± 0.036
0.509 ± 0.021"
-S,0.9548387096774194,"Corr-DCRNN
Without Pre-training
0.553 ± 0.025
0.577 ± 0.028"
-S,0.9580645161290322,"Dist-DCRNN
Without Pre-training
0.570 ± 0.027
0.600 ± 0.022"
-S,0.9612903225806452,"Corr-DCRNN
With Pre-training
0.582 ± 0.014
0.591 ± 0.008"
-S,0.964516129032258,"Dist-DCRNN
With Pre-training
0.583 ± 0.009
0.607 ± 0.017"
-S,0.967741935483871,"Table 10 shows the seizure detection results for DCRNNs when the self-supervised (SS) prediction
task is used as an auxiliary task during training. Compared to self-supervised pre-training (Table
2 last two rows), auxiliary learning only marginally improves Dist-DCRNN performance on 12-
s seizure detection (no statistical signiﬁcance), whereas self-supervised pre-training signiﬁcantly
outperforms auxiliary learning for 60-s seizure detection."
-S,0.9709677419354839,"Table 10: Seizure detection results for DCRNNs with self-supervised pre-training (same as Table 2)
and auxiliary learning. Mean and standard deviations are from ﬁve random runs. Best mean results
for Dist-DCRNN/Corr-DCRNN are in bold."
-S,0.9741935483870968,"Model
Seizure Detection AUROC"
-S,0.9774193548387097,"12-s
60-s
Corr-DCRNN w/ SS Pre-Training (Table 2)
0.861 ± 0.005
0.850 ± 0.013
Corr-DCRNN w/ SS Auxiliary Task
0.851 ± 0.008
0.811 ± 0.005"
-S,0.9806451612903225,"Dist-DCRNN w/ SS Pre-Training (Table 2)
0.866 ± 0.016
0.875 ± 0.016
Dist-DCRNN w/ SS Auxiliary Task
0.875 ± 0.005
0.840 ± 0.013"
-S,0.9838709677419355,Published as a conference paper at ICLR 2022
-S,0.9870967741935484,"Figure 7: Example occlusion maps for seizure classiﬁcation obtained from pre-trained Corr-DCRNN
model for (a)–(b) combined focal (CF) seizures, (c) generalized non-speciﬁc (GN) seizure, (d)
absence (AB) seizure, and (e) combined tonic (CT) seizure. In each subﬁgure, the bottom panel
shows the occlusion values for each channel replicated along the time dimension and overlaid on
60-s EEG signals, the red boxes indicate the duration of seizures in the EEG clips, and the top panel
shows the occlusion values overlaid on the correlation graph structure. To visualize occlusion map
values in this “double banana” montage, we subtract the occlusion values between the corresponding
channels in the montage, which results in different values between occlusion maps shown on the
EEG signals (bottom panel in each subﬁgure) and that shown on the graph structures (top panel in
each subﬁgure). Note that the values within an occlusion map are normalized, and thus should not
be compared across different occlusion maps."
-S,0.9903225806451613,Published as a conference paper at ICLR 2022
-S,0.9935483870967742,"Figure 8: Distance graphs with different threshold κ in the thresholded Gaussian kernel. Small
thresholds (e.g., 0.7 and 0.8) result in missing edges between nearby electrodes, whereas large
thresholds (e.g., 1.0, 1.1, 1.2) result in edges connecting electrodes that are spatially far away."
-S,0.9967741935483871,"Figure 9: Distance-based graphs constructed using Gaussian kernels with different bandwidths.
When bandwidth = 0.06, the distance graph structure resembles the original graph structure using
thresholded Gaussian kernel with a threshold of 0.9 (Figure 1b)."
