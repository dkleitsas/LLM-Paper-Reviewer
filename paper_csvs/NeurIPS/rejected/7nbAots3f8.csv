Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0012515644555694619,"Unsigned distance fields (UDFs) provide a versatile framework for representing
1"
ABSTRACT,0.0025031289111389237,"a diverse array of 3D shapes, encompassing both watertight and non-watertight
2"
ABSTRACT,0.0037546933667083854,"geometries. Traditional UDF learning methods typically require extensive training
3"
ABSTRACT,0.0050062578222778474,"on large datasets of 3D shapes, which is costly and often necessitates hyperparame-
4"
ABSTRACT,0.006257822277847309,"ter adjustments for new datasets. This paper presents a novel neural framework,
5"
ABSTRACT,0.007509386733416771,"LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging lo-
6"
ABSTRACT,0.008760951188986232,"cal shape functions to learn UDFs. We observe that 3D shapes manifest simple
7"
ABSTRACT,0.010012515644555695,"patterns within localized areas, prompting us to create a training dataset of point
8"
ABSTRACT,0.011264080100125156,"cloud patches characterized by mathematical functions that represent a continuum
9"
ABSTRACT,0.012515644555694618,"from smooth surfaces to sharp edges and corners. Our approach learns features
10"
ABSTRACT,0.01376720901126408,"within a specific radius around each query point and utilizes an attention mecha-
11"
ABSTRACT,0.015018773466833541,"nism to focus on the crucial features for UDF estimation. This method enables
12"
ABSTRACT,0.016270337922403004,"efficient and robust surface reconstruction from point clouds without the need for
13"
ABSTRACT,0.017521902377972465,"shape-specific training. Additionally, our method exhibits enhanced resilience
14"
ABSTRACT,0.01877346683354193,"to noise and outliers in point clouds compared to existing methods. We present
15"
ABSTRACT,0.02002503128911139,"comprehensive experiments and comparisons across various datasets, including
16"
ABSTRACT,0.02127659574468085,"synthetic and real-scanned point clouds, to validate our method’s efficacy.
17"
INTRODUCTION,0.02252816020025031,"1
Introduction
18"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.023779724655819776,"3D surface reconstruction from raw point clouds is a significant and long-standing problem in
19"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.025031289111389236,"computer graphics and machine vision. Traditional techniques like Poisson Surface Reconstruction[1]
20"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.026282853566958697,"create an implicit indicator function from oriented points and reconstruct the surface by extracting
21"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.02753441802252816,"an appropriate isosurface. The advancement of artificial intelligence has led to the emergence
22"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.028785982478097622,"of numerous neural network-based methods for 3D reconstruction. Among these, neural implicit
23"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.030037546933667083,"representations have gained significant influence, which utilize signed distance fields (SDFs) [2–8]
24"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.03128911138923655,"and occupancy fields [9–12] to implicitly depict 3D geometries. SDFs and occupancy fields extract
25"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.03254067584480601,"isosurfaces by solving regression and classification problems, respectively. However, both techniques
26"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.03379224030037547,"require internal and external definitions of the surfaces, limiting their capability to reconstructing only
27"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.03504380475594493,"watertight geometries. Therefore, unsigned distance fields [13–20] have recently gained increasing
28"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.03629536921151439,"attention due to their ability to reconstruct non-watertight surfaces and complex geometries with
29"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.03754693366708386,"arbitrary topologies.
30"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.03879849812265332,"Reconstructing 3D geometries from raw point clouds using UDFs presents significant challenges due
31"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.04005006257822278,"to the non-differentiability near the surface. This characteristic complicates the development of loss
32"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.04130162703379224,"functions and undermines the stability of neural network training. Various unsupervised approaches
33"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.0425531914893617,"[17, 14, 19] have been developed to tailor loss functions that leverage the intrinsic characteristics
34"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.04380475594493116,"of UDFs, ensuring that the reconstructed geometry aligns closely with the original point clouds.
35"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.04505632040050062,"However, these methods suffer from slow convergence, necessitating an extensive network training
36"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.04630788485607009,"time to reconstruct a single geometry. As a supervised method, GeoUDF [15] learns local geometric
37"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.04755944931163955,"priors through training on datasets such as ShapeNet [21], thus achieving efficient UDF estimation.
38"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.04881101376720901,"Nonetheless, the generalizability of this approach is dependent on the training dataset, which also
39"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.05006257822277847,"leads to relatively high computational costs.
40"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.05131414267834793,"In this paper, we propose a lightweight and effective supervised learning framework, Losf-UDF, to
41"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.052565707133917394,"address these challenges. Since learning UDFs does not require determining whether a query point is
42"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.05381727158948686,"inside or outside the geometry, it is a local quantity independent of the global context. Inspired by the
43"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.05506883604505632,"observation that 3D shapes manifest simple patterns within localized areas, we synthesize a training
44"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.056320400500625784,"dataset comprising a set of point cloud patches by utilizing local shape functions. Subsequently, we
45"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.057571964956195244,"can estimate the unsigned distance values by learning local geometric features through an attention-
46"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.058823529411764705,"based network. Our approach distinguishes itself from existing methods by its novel training strategy.
47"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.060075093867334166,"Specifically, it is uniquely trained on synthetic surfaces, yet it demonstrates remarkable capability
48"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.06132665832290363,"in predicting UDFs for a wide range of common surface types. For smooth surfaces, we generate
49"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.0625782227784731,"training patches (quadratic surfaces) by analyzing principal curvatures, meanwhile, we design simple
50"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.06382978723404255,"shape functions to simulate sharp features. This strategy has three unique advantages. First, it
51"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.06508135168961202,"systematically captures the local geometries of most common surfaces encountered during testing,
52"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.06633291614518148,"effectively mitigating the dataset dependence risk that plagues current UDF learning methods. Second,
53"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.06758448060075094,"for each training patch, the ground-truth UDF is readily available, streamlining the training process.
54"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.0688360450563204,"Third, this approach substantially reduces the costs associated with preparing the training datasets.
55"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.07008760951188986,"We evaluate our framework on various datasets and demonstrates its ability to robustly reconstruct
56"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.07133917396745933,"high-quality surfaces, even for point clouds with noise and outliers. Notably, our method can serve as
57"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.07259073842302878,"a lightweight initialization that can be integrated with existing unsupervised methods to enhance their
58"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.07384230287859825,"performance. We summarize our main contributions as follows.
59"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.07509386733416772,"• We present a simple yet effective data-driven approach that learns UDFs directly from a
60"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.07634543178973717,"synthetic dataset consisting of point cloud patches, which is independent of the global shape.
61"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.07759699624530664,"• Our method is computationally efficient and requires training only once on our synthetic
62"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.07884856070087609,"dataset. Then it can be applied to reconstruct a wide range of surface types.
63"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.08010012515644556,"• Our framework achieves superior performance in surface reconstruction from both synthetic
64"
D SURFACE RECONSTRUCTION FROM RAW POINT CLOUDS IS A SIGNIFICANT AND LONG-STANDING PROBLEM IN,0.08135168961201501,"point clouds and real scans, even in the presence of noise and outliers.
65"
RELATED WORK,0.08260325406758448,"2
Related Work
66"
RELATED WORK,0.08385481852315395,"Surface reconstruction. Reconstructing 3D surfaces from point clouds is a classic and important
67"
RELATED WORK,0.0851063829787234,"topic in computer graphics. The most widely used Poisson method [1, 22] fits surfaces by solving
68"
RELATED WORK,0.08635794743429287,"PDEs. These traditional methods involve adjusting the gradient of an indicator function to align with
69"
RELATED WORK,0.08760951188986232,"a solution derived from a (screened) Poisson equation. A crucial requirement of these methods is the
70"
RELATED WORK,0.08886107634543179,"input of oriented normals. The Iterative Screened Poisson Reconstruction method[23] introduced
71"
RELATED WORK,0.09011264080100125,"an iterative approach to refine the reconstruction process, improving the ability to generate surfaces
72"
RELATED WORK,0.09136420525657071,"from point clouds without direct computation of normals. The shape of points [24] introduced a
73"
RELATED WORK,0.09261576971214018,"differentiable point-to-mesh layer by employing a differentiable formulation of PSR [1] to generate
74"
RELATED WORK,0.09386733416770963,"watertight, topology-agnostic manifold surfaces.
75"
RELATED WORK,0.0951188986232791,"Neural surface representations. Recently, the domain of deep learning has spurred significant
76"
RELATED WORK,0.09637046307884856,"advances in the implicit neural representation of 3D shapes. Some of these works trained a classifier
77"
RELATED WORK,0.09762202753441802,"neural network to construct occupancy fields [9–12] for representing 3D geometries. Poco [12]
78"
RELATED WORK,0.09887359198998748,"achieves superior reconstruction performance by introducing convolution into occupancy fields.
79"
RELATED WORK,0.10012515644555695,"Ouasfi et al. [25] recently proposed a uncertainty measure method based on margin to learn occu-
80"
RELATED WORK,0.10137672090112641,"pancy fields from sparse point clouds. Compared to occupancy fields, SDFs [2–8] offer a more
81"
RELATED WORK,0.10262828535669587,"precise geometric representation by differentiating between interior and exterior spaces through the
82"
RELATED WORK,0.10387984981226533,"assignment of signs to distances. Some recent SOTA methods, such as DeepLS [3], using volumetric
83"
RELATED WORK,0.10513141426783479,"SDFs to locally learned continuous SDFs, have achieved higher compression, accuracy, and local
84"
RELATED WORK,0.10638297872340426,"shape refinement.
85"
RELATED WORK,0.10763454317897372,"Unsigned distance fields learning. Although Occupancy fields and SDFs have undergone signif-
86"
RELATED WORK,0.10888610763454318,"icant development recently, they are hard to reconstruct surfaces with boundaries or nonmanifold
87"
RELATED WORK,0.11013767209011265,"features. G-Shell[26] developed a differentiable shell-based representation for both watertight and
88"
RELATED WORK,0.1113892365456821,"non-watertight surfaces. However, UDFs provide a simpler and more natural way to represent
89"
RELATED WORK,0.11264080100125157,"general shapes [13–20]. Various methods have been proposed to reconstruct surfaces from point
90"
RELATED WORK,0.11389236545682102,"clouds by learning UDFs. CAP-UDF [17] suggested directing 3D query points towards the surface
91 …"
RELATED WORK,0.11514392991239049,Synthetic Dataset UDF
RELATED WORK,0.11639549436795996,LoSF-UDF
RELATED WORK,0.11764705882352941,LoSF-UDF
RELATED WORK,0.11889862327909888,"DCUDF
Supervised"
RELATED WORK,0.12015018773466833,Training
RELATED WORK,0.1214017521902378,Mesh Extraction
RELATED WORK,0.12265331664580725,Training
RELATED WORK,0.12390488110137672,Reconstruction
RELATED WORK,0.1251564455569462,"Figure 1: Pipeline. First, we train a UDF prediction network UΘ on a synthetic dataset, which contains
a series of local point cloud patches that are independent of specific shapes. Given a global point
cloud P, we then extract a local patch P assigned to each query point q within a specified radius,
and obtain the corresponding UDF values U ˆΘ(P, q). Finally, we extract the mesh corresponding to
the input point cloud by incorporating the DCUDF[32] framework."
RELATED WORK,0.12640801001251564,"with a consistency constraint to develop UDFs that are aware of consistency. LevelSetUDF [14]
92"
RELATED WORK,0.1276595744680851,"learned a smooth zero-level function within UDFs through level set projections. As a supervised
93"
RELATED WORK,0.12891113892365458,"approach, GeoUDF [15] estimates UDFs by learning local geometric priors from training on many
94"
RELATED WORK,0.13016270337922403,"3D shapes. DUDF [19] formulated the UDF learning as an Eikonal problem with distinct boundary
95"
RELATED WORK,0.13141426783479349,"conditions. UODF [20] proposed unsigned orthogonal distance fields that every point in this field
96"
RELATED WORK,0.13266583229036297,"can access to the closest surface points along three orthogonal directions. Instead of reconstructing
97"
RELATED WORK,0.13391739674593242,"from point clouds, many recent works [27–30] learn high-quality UDFs from multi-view images for
98"
RELATED WORK,0.13516896120150187,"reconstructing non-watertight surfaces. Furthermore, UiDFF [31] presents a 3D diffusion model for
99"
RELATED WORK,0.13642052565707133,"UDFs to generate textured 3D shapes with boundaries.
100"
METHOD,0.1376720901126408,"3
Method
101"
METHOD,0.13892365456821026,"Motivation. Distinct from SDFs, there is no need for UDFs to determine the sign to distinguish
102"
METHOD,0.14017521902377972,"between the inside and outside of a shape. Consequently, the UDF values are solely related to the local
103"
METHOD,0.1414267834793492,"geometric characteristics of 3D shapes. Furthermore, within a certain radius for a query point, local
104"
METHOD,0.14267834793491865,"geometry can be approximated by general mathematical functions. Stemming from these insights, we
105"
METHOD,0.1439299123904881,"propose a novel UDF learning framework that focuses on local geometries. We employ local shape
106"
METHOD,0.14518147684605756,"functions to construct a series of point cloud patches as our training dataset, which includes common
107"
METHOD,0.14643304130162704,"smooth and sharp geometric features. Fig. 1 illustrates the pipeline of our proposed UDF learning
108"
METHOD,0.1476846057571965,"framework.
109"
LOCAL SHAPE FUNCTIONS,0.14893617021276595,"3.1
Local shape functions
110"
LOCAL SHAPE FUNCTIONS,0.15018773466833543,"Smooth patches. From the viewpoint of differential geometry [33], the local geometry at a specific
111"
LOCAL SHAPE FUNCTIONS,0.15143929912390489,"point on a regular surface can be approximated by a quadratic surface. Specifically, consider a regular
112"
LOCAL SHAPE FUNCTIONS,0.15269086357947434,"surface S : r = r(u, v) with a point p on it. At point p, it is possible to identify two principal
113"
LOCAL SHAPE FUNCTIONS,0.1539424280350438,"direction unit vectors, e1 and e2, with the corresponding normal n = e1 × e2. A suitable parameter
114"
LOCAL SHAPE FUNCTIONS,0.15519399249061328,"system (u, v) can be determined such that ru = e1 and rv = e2, thus obtaining the corresponding
115"
LOCAL SHAPE FUNCTIONS,0.15644555694618273,"first and second fundamental forms as
116"
LOCAL SHAPE FUNCTIONS,0.15769712140175218,"\la b
e
l
 
{
e"
LOCAL SHAPE FUNCTIONS,0.15894868585732166,"q
:
f
u
n
d
m"
LOCAL SHAPE FUNCTIONS,0.16020025031289112,"e
n
tal-1 2
}
 
[
\
m"
LOCAL SHAPE FUNCTIONS,0.16145181476846057,"a
t
h
rm
 
{
I}"
LOCAL SHAPE FUNCTIONS,0.16270337922403003,"]
_{\mathbf {p}}=\begin {bmatrix} E &F\\ F &G \end {bmatrix}=\begin {bmatrix} 1 &0\\ 0 &1 \end {bmatrix},\quad [\mathrm {II}]_{\mathbf {p}}=\begin {bmatrix} L &M\\ M &N \end {bmatrix}=\begin {bmatrix} \kappa _1 &0\\ 0 &\kappa _2 \end {bmatrix},\quad 
(1)"
LOCAL SHAPE FUNCTIONS,0.1639549436795995,"where κ1, κ2 are principal curvatures. Without loss of generality, we assume p corresponding to
117"
LOCAL SHAPE FUNCTIONS,0.16520650813516896,"u = v = 0 and expand the Taylor form at this point as
118"
LOCAL SHAPE FUNCTIONS,0.16645807259073842,\l ab e l {e q: t aylor } \ b egin {sp l i
LOCAL SHAPE FUNCTIONS,0.1677096370463079,t} \math bf {r
LOCAL SHAPE FUNCTIONS,0.16896120150187735,"}(u,v) &=\m a thbf { r}(0, 0 )+\m a thbf {r}_u(0,0)u+\mathbf {r}_v(0,0)v+\frac {1}{2}[\mathbf {r}_{uu}(0,0)u^2+ \\ & \mathbf {r}_{uv}(0,0)uv+\mathbf {r}_{vv}(0,0)v^2]+o(u^2+v^2). \end {split} (2)"
LOCAL SHAPE FUNCTIONS,0.1702127659574468,"Decomposing ruu(0, 0), ruv(0, 0), and rvv(0, 0) along the tangential and normal directions, we can
119"
LOCAL SHAPE FUNCTIONS,0.17146433041301626,"formulate Eq.(2) according to Eq.(1) as
120"
LOCAL SHAPE FUNCTIONS,0.17271589486858574,"\b eg i n {s pl i t}  \m
a"
LOCAL SHAPE FUNCTIONS,0.1739674593241552,"th b f {r}( u ,v ) &=
\"
LOCAL SHAPE FUNCTIONS,0.17521902377972465,ma t hbf {r } (
LOCAL SHAPE FUNCTIONS,0.17647058823529413,"0,0)+( u +o(\ s qrt { u^2+v^2}))\mathbf {e}_1+(v+o(\sqrt {u^2+v^2}))\mathbf {e}_2\\ &+\frac {1}{2}(\kappa _1u^2+\kappa _2v^2+o(u^2+v^2)))\mathbf {n} \end {split} (3)"
LOCAL SHAPE FUNCTIONS,0.17772215269086358,"where o(u2 + v2) ≈0 is negligible in a small local region. Consequently, by adopting {p, e1, e2, n}
121"
LOCAL SHAPE FUNCTIONS,0.17897371714643304,"as the orthogonal coordinate system, we can define the form of the local approximating surface as
122"
LOCAL SHAPE FUNCTIONS,0.1802252816020025,"\l
a b el
 { e"
LOCAL SHAPE FUNCTIONS,0.18147684605757197,"q:smoo t h} x=u,\quad y=v,\quad z=\frac {1}{2}(\kappa _1u^2+\kappa _2v^2), 
(4)"
LOCAL SHAPE FUNCTIONS,0.18272841051314143,Planar
LOCAL SHAPE FUNCTIONS,0.18397997496871088,"Parabolic
Ellipsoida"
LOCAL SHAPE FUNCTIONS,0.18523153942428036,Hyperbolic
LOCAL SHAPE FUNCTIONS,0.18648310387984982,V-Saddle
LOCAL SHAPE FUNCTIONS,0.18773466833541927,Crease Cusps
LOCAL SHAPE FUNCTIONS,0.18898623279098872,Corner
LOCAL SHAPE FUNCTIONS,0.1902377972465582,"(a) Smooth patches
(b) Sharp patches
Figure 2: Local geometries. (a) For points on a geometry that are differentiable, the local shape at
these points can be approximated by quadratic surfaces. (b) For points that are non-differentiable, we
can also construct locally approximated surfaces using functions."
LOCAL SHAPE FUNCTIONS,0.19148936170212766,which exactly are quadratic surfaces z = 1
LOCAL SHAPE FUNCTIONS,0.1927409261576971,"2(κ1x2 + κ2y2). Furthermore, in relation to Gaussian
123"
LOCAL SHAPE FUNCTIONS,0.1939924906132666,"curvatures κ1κ2, quadratic surfaces can be categorized into four types: ellipsoidal, hyperbolic,
124"
LOCAL SHAPE FUNCTIONS,0.19524405506883605,"parabolic, and planar. As shown in Fig. 2, for differentiable points on a general geometry, the local
125"
LOCAL SHAPE FUNCTIONS,0.1964956195244055,"shape features can always be described by one of these four types of quadratic surfaces.
126"
LOCAL SHAPE FUNCTIONS,0.19774718397997496,"Sharp patches. For surfaces with sharp features, they are not differentiable at some points and cannot
127"
LOCAL SHAPE FUNCTIONS,0.19899874843554444,"be approximated in the form of a quadratic surface. We categorize commonly seen sharp geometric
128"
LOCAL SHAPE FUNCTIONS,0.2002503128911139,"features into four types, including creases, cusps, corners, and v-saddles, as illustrated in Fig. 2(b).
129"
LOCAL SHAPE FUNCTIONS,0.20150187734668334,"We construct these four types of sharp features in a consistent form z = f(x, y) like smooth patches
130"
LOCAL SHAPE FUNCTIONS,0.20275344180225283,"\label  { e q : s har p } 
\"
LOCAL SHAPE FUNCTIONS,0.20400500625782228,"b e gi n  {spli t }  & \ t
e"
LOCAL SHAPE FUNCTIONS,0.20525657071339173,xt  {cr
LOCAL SHAPE FUNCTIONS,0.2065081351689612,"eases:}\ ,  z = 1 - h\cdot \ frac {|kx-y|}{\ s q r t  { 1+k ^ 2}} , ~\te"
LOCAL SHAPE FUNCTIONS,0.20775969962453067,x t  {c
LOCAL SHAPE FUNCTIONS,0.20901126408010012,"u sps:}\, z=1-h\cdot \sqrt {x^2+y^2},\\ &\text {corners:}\, z=1-h\cdot \max (|x|, |y|), ~\text {v-saddles:}\, z=1-h\cdot |x|+|y|\cdot (\frac {|x|}{x}\cdot \frac {|y|}{y}), \\ \end {split} (5)"
LOCAL SHAPE FUNCTIONS,0.21026282853566958,"where h can adjust the sharpness of the shape, and k can control the direction of the crease. Fig 3
131"
LOCAL SHAPE FUNCTIONS,0.21151439299123906,"illustrates various smooth and sharp patches with distinct parameters.
132"
LOCAL SHAPE FUNCTIONS,0.2127659574468085,"Synthetic training dataset. We utilize the mathematical functions introduced above to synthesize a
133"
LOCAL SHAPE FUNCTIONS,0.21401752190237797,"series of point cloud patches for training. As shown in Fig. 3, we first uniformly sample m points
134"
LOCAL SHAPE FUNCTIONS,0.21526908635794745,"{(xi, yi)}m
i=1 within a circle of radius r0 centered at (0, 0) in the xy-plane. Then, we substitute
135"
LOCAL SHAPE FUNCTIONS,0.2165206508135169,"the coordinates into Eq.(4-5) to obtain the corresponding z-coordinate values, resulting in a patch
136"
LOCAL SHAPE FUNCTIONS,0.21777221526908636,"P = {pm
i=1}, where pi = (xi, yi, z(xi, yi)). Subsequently, we randomly collect query points
137"
LOCAL SHAPE FUNCTIONS,0.2190237797246558,"{qi}n
i=1 distributed along the vertical ray intersecting the xy-plane at the origin, extending up to a
138"
LOCAL SHAPE FUNCTIONS,0.2202753441802253,"distance of r0. For each query point qi, we determine its UDF value U(qi), which is either |q(z)
i
| for
139"
LOCAL SHAPE FUNCTIONS,0.22152690863579474,"smooth patches or 1−|q(z)
i
| for sharp patches. Noting that for patches with excessively high curvature
140"
LOCAL SHAPE FUNCTIONS,0.2227784730913642,"or sharpness, the minimum distance of the query points may not be the distance to (0, 0, z(0, 0)), we
141"
LOCAL SHAPE FUNCTIONS,0.22403003754693368,"will exclude these patches from our training dataset. Overall, each sample in our synthetic dataset is
142"
LOCAL SHAPE FUNCTIONS,0.22528160200250313,"specifically in the form of {q, P, U(q)}.
143"
LOCAL SHAPE FUNCTIONS,0.2265331664580726,"Query points
Point cloud patch"
LOCAL SHAPE FUNCTIONS,0.22778473091364204,"Figure 3: Synthetic dataset for training. By manipulating functional parameters, we can readily create
various smooth and sharp surfaces, subsequently acquiring pairs of point cloud patches and query
points via sampling."
UDF LEARNING,0.22903629536921152,"3.2
UDF learning
144"
UDF LEARNING,0.23028785982478098,"We perform supervised training on the synthesized dataset which is independent of specific shapes.
145"
UDF LEARNING,0.23153942428035043,"The network learns the features of local geometries and utilizes an attention-based module to output
146"
UDF LEARNING,0.2327909887359199,"the corresponding UDF values from the learned features. After training, given any 3D point clouds
147"
UDF LEARNING,0.23404255319148937,"and a query point in space, we extract the local point cloud patch near the query, which has the same
148"
UDF LEARNING,0.23529411764705882,"form as the data in the training dataset. Consequently, our network can predict the UDF value at that
149"
UDF LEARNING,0.23654568210262827,"query point based on this local point cloud patch.
150"
NETWORK ARCHITECTURE,0.23779724655819776,"3.2.1
Network architecture
151"
NETWORK ARCHITECTURE,0.2390488110137672,"For a sample {q, P = {pi}m
i=1, U(q)}, we first obtain a latent code fp ∈Rlp related to the local
152"
NETWORK ARCHITECTURE,0.24030037546933666,"point cloud patch P through a Point-Net [34] Fp. To derive features related to distance, we use
153"
NETWORK ARCHITECTURE,0.24155193992490614,"relative vectors from the patch points to the query point, V = {pi −q}m
i=1, as input to a Vectors-
154"
NETWORK ARCHITECTURE,0.2428035043804756,"Net Fv, which is similar to the Point-Net Fp. This process results in an additional latent code
155"
NETWORK ARCHITECTURE,0.24405506883604505,"fv ∈Rlv. Subsequently, we apply a cross-attention module [35] to obtain the feature codes for the
156"
NETWORK ARCHITECTURE,0.2453066332916145,"local geometry,
157"
NETWORK ARCHITECTURE,0.246558197747184,"\ mathbf {f}_G= \te x t {CrossAttn}(\mathbf {f}_p, \mathbf {f}_v)\in \mathbb {R}^{l_G}, 
(6)"
NETWORK ARCHITECTURE,0.24780976220275344,"where we take fp as the Key-Value (KV) pair and fv as the Query (Q). In our experiments, we set
158"
NETWORK ARCHITECTURE,0.2490613266583229,"lp = lv = 64, and lG = 128. Based on the learned geometric features, we aim to fit the UDF values
159"
NETWORK ARCHITECTURE,0.2503128911138924,"from the distance within the local point cloud. Therefore, we concatenate the distances d ∈Rm
160"
NETWORK ARCHITECTURE,0.25156445556946183,"induced from V with the latent code fG, followed by a series of fully connected layers to output the
161"
NETWORK ARCHITECTURE,0.2528160200250313,"predicted UDF values UΘ(q). Fig. 4 illustrates the overall network architecture and data flow.
162"
NETWORK ARCHITECTURE,0.25406758448060074,"Point Cloud Patch K, V"
NETWORK ARCHITECTURE,0.2553191489361702,"Q
Vectors Pointing to Query"
NETWORK ARCHITECTURE,0.2565707133917397,Points-Net
NETWORK ARCHITECTURE,0.25782227784730916,Vectors-Net
NETWORK ARCHITECTURE,0.2590738423028786,Cross-Attn
NETWORK ARCHITECTURE,0.26032540675844806,"Full-Connected
Layers"
NETWORK ARCHITECTURE,0.2615769712140175,"Denoising
Module"
NETWORK ARCHITECTURE,0.26282853566958697,Points-Net
NETWORK ARCHITECTURE,0.2640801001251564,Self-Attn
NETWORK ARCHITECTURE,0.26533166458072593,"Full-Connected
Layers"
NETWORK ARCHITECTURE,0.2665832290362954,Denoising Module
NETWORK ARCHITECTURE,0.26783479349186484,Figure 4: Network architecture of LoSF-UDF.
NETWORK ARCHITECTURE,0.2690863579474343,"Denoising module. In our network, even if point cloud patches are subjected to a certain degree of
163"
NETWORK ARCHITECTURE,0.27033792240300375,"noise or outliers, their representations in the feature space should remain similar. However, distances
164"
NETWORK ARCHITECTURE,0.2715894868585732,"induced directly from noisy vectors V will inevitably contain errors, which can affect the accurate
165"
NETWORK ARCHITECTURE,0.27284105131414266,"prediction of UDF values. To mitigate this impact, we introduce a denoising module that predicts
166"
NETWORK ARCHITECTURE,0.27409261576971217,"displacements ∆d from local point cloud patches, as shown in Fig. 4. We then add the displacements
167"
NETWORK ARCHITECTURE,0.2753441802252816,"∆d to the distances d to improve the accuracy of the UDF estimation.
168"
TRAINING AND EVALUATION,0.2765957446808511,"3.2.2
Training and evaluation
169"
TRAINING AND EVALUATION,0.27784730913642053,"Data augmentation. During the training process, we scale all pairs of local patches P and query
170"
TRAINING AND EVALUATION,0.27909887359199,"points q to conform to the bounding box constraints of [−0.5, 0.5], and the corresponding GT UDF
171"
TRAINING AND EVALUATION,0.28035043804755944,"values U(q) are scaled by equivalent magnitudes. Given the uncertain orientation of local patches
172"
TRAINING AND EVALUATION,0.2816020025031289,"extracted from a specified global point cloud, we have applied data augmentation via random rotations
173"
TRAINING AND EVALUATION,0.2828535669586984,"to the training dataset. Furthermore, to enhance generalization to open surfaces with boundaries, we
174"
TRAINING AND EVALUATION,0.28410513141426785,"randomly truncate 20% of the smooth patches to simulate boundary cases. To address the issue of
175"
TRAINING AND EVALUATION,0.2853566958698373,"noise handling, we introduce Gaussian noise N(0, 0.1) to 30% of the data in each batch during every
176"
TRAINING AND EVALUATION,0.28660826032540676,"training epoch.
177"
TRAINING AND EVALUATION,0.2878598247809762,"Loss functions. We employ L1 loss Lu to measure the discrepancy between the predicted UDF
178"
TRAINING AND EVALUATION,0.28911138923654567,"values and the GT UDF values. Moreover, for the displacements ∆d output by the denoising module,
179"
TRAINING AND EVALUATION,0.2903629536921151,"we employ L1 regularization to encourage sparsity. Consequently, we train the network driven by the
180"
TRAINING AND EVALUATION,0.29161451814768463,"following loss function,
181"
TRAINING AND EVALUATION,0.2928660826032541,"\l a bel {
eq:lo ss } \math c al {L}= \m a thcal {L}_u+\lambda _d\mathcal {L}_r, \quad \text {where}\,\, \mathcal {L}_u=|\mathcal {U}(\mathbf {q})-\mathcal {U}_\Theta (\mathbf {q})|,\,\,\mathcal {L}_r=|\Delta \mathbf {d}|, 
(7)"
TRAINING AND EVALUATION,0.29411764705882354,"where we set λd = 0.01 in our experiments.
182"
TRAINING AND EVALUATION,0.295369211514393,"Evaluation. Given a 3D point cloud P for reconstruction, we first normalize it to fit within a bounding
183"
TRAINING AND EVALUATION,0.29662077596996245,"box with dimensions ranging from [−0.5, 0.5]. Subsequently, within the bounding box space, we
184"
TRAINING AND EVALUATION,0.2978723404255319,"uniformly sample grid points at a specified resolution to serve as query points. Finally, we extract the
185"
TRAINING AND EVALUATION,0.29912390488110135,"local geometry Pp for each query point by collecting points from the point cloud that lie within a
186"
TRAINING AND EVALUATION,0.30037546933667086,"sphere of a specified radius centered on the query point. We can obtain the predicted UDF values
187"
TRAINING AND EVALUATION,0.3016270337922403,"by the trained network UΘ∗(q, Pq), where Θ∗represents the optimized network parameters. Note
188"
TRAINING AND EVALUATION,0.30287859824780977,"that for patches Pp with fewer than 5 points, we set the UDF values as a large constant. Finally, we
189"
TRAINING AND EVALUATION,0.3041301627033792,"extract meshes from the UDFs using the DCUDF model [32].
190"
EXPERIMENTS,0.3053817271589487,"4
Experiments
191"
EXPERIMENT SETUP,0.30663329161451813,"4.1
Experiment setup
192"
EXPERIMENT SETUP,0.3078848560700876,"Datasets. To compare our method with other state-of-the-art UDF learning approaches, we tested it on
193"
EXPERIMENT SETUP,0.3091364205256571,"various datasets that include general artificial objects from the field of computer graphic. Following
194"
EXPERIMENT SETUP,0.31038798498122655,"previous works [30, 17, 14], we select the ""Car"" category from ShapeNet[21], which has a rich
195"
EXPERIMENT SETUP,0.311639549436796,"collection of multi-layered and non-closed shapes. Furthermore, we select the real-world dataset
196"
EXPERIMENT SETUP,0.31289111389236546,"DeepFashion3D[36] for open surfaces, and ScanNet[37] for large outdoor scenes. To assess our
197"
EXPERIMENT SETUP,0.3141426783479349,"model’s performance on actual noisy inputs, we conducted tests on real range scan dataset [38]
198"
EXPERIMENT SETUP,0.31539424280350437,"following the previous works[17, 14].
199"
EXPERIMENT SETUP,0.3166458072590738,"Baselines. For our validation datasets, we compared our method against the state-of-the-art UDF
200"
EXPERIMENT SETUP,0.31789737171464333,"learning models, which include unsupervised methods like CAP-UDF[17], LevelSetUDF[14], and
201"
EXPERIMENT SETUP,0.3191489361702128,"DUDF[19], as well as the supervised learning method, GeoUDF[15]. We trained GeoUDF inde-
202"
EXPERIMENT SETUP,0.32040050062578224,"pendently on different datasets to achieve optimal performance. Table. 1 shows the qualitative
203"
EXPERIMENT SETUP,0.3216520650813517,"comparison between our methods and baselines. To evaluate performance, we calculate the Chamfer
204"
EXPERIMENT SETUP,0.32290362953692114,"Distance (CD) and F1-Score (setting thresholds of 0.005 and 0.01) metrics between the ground truth
205"
EXPERIMENT SETUP,0.3241551939924906,"meshes and the meshes extracted from the UDFs out by our model and each baseline model. For a fair
206"
EXPERIMENT SETUP,0.32540675844806005,"comparison, we test all baseline models using the DCUDF[32] method. All experimental procedures
207"
EXPERIMENT SETUP,0.32665832290362956,"are executed on NVIDIA RTX 4090 and A100 GPUs.
208"
EXPERIMENT SETUP,0.327909887359199,"Methods
Input
Normal
Learning Type
Feature Type
Noise
Outlier"
EXPERIMENT SETUP,0.32916145181476847,"CAP-UDF [17]
Dense
Not required
Unsupervised
Global
✗
✗
LevelSetUDF [14]
Dense
Not required
Unsupervised
Global
✓
✗
GeoUDF [15]
Sparse
Not required
Supervised
Local
✗
✗
DUDF [19]
Dense
Required
Unsupervised
Global
✗
✗"
EXPERIMENT SETUP,0.3304130162703379,"Ours
Dense
Not required
Supervised
Local
✓
✓
Table 1: Qualitative comparison of different UDF learning methods. “Normal” indicates whether
the method requires point cloud normals during learning. “Feature Type”’ refers to whether the
information required during training is global or local. “Noise” and “Outlier” indicate whether the
method can handle the presence of noise and outliers in point clouds."
EXPERIMENTAL RESULTS,0.3316645807259074,"4.2
Experimental results
209"
EXPERIMENTAL RESULTS,0.33291614518147683,"Synthetic
data.
For
general
3D
graphic
models,
ShapeNetCars,
and
Deep-
210"
EXPERIMENTAL RESULTS,0.3341677096370463,"Fashion3D,
we
obtain
dense
point
clouds
by
randomly
samping
on
meshes.
211"
EXPERIMENTAL RESULTS,0.3354192740926158,"Considering that GeoUDF [15] is a supervised method, we
212"
EXPERIMENTAL RESULTS,0.33667083854818525,"retrain it on ShapeNetCars, and DeepFashion3D, which
213"
EXPERIMENTAL RESULTS,0.3379224030037547,"are randomly partitioned into training (70%), testing
214"
EXPERIMENTAL RESULTS,0.33917396745932415,"(20%), and validation subsets (10%). All models are eval-
215"
EXPERIMENTAL RESULTS,0.3404255319148936,"uated in the validation sets, which remain unseen by any
216"
EXPERIMENTAL RESULTS,0.34167709637046306,"of the UDF learning models prior to evaluation. The first
217"
EXPERIMENTAL RESULTS,0.3429286608260325,"three rows of Fig. 5 show the visual comparison of recon-
218"
EXPERIMENTAL RESULTS,0.344180225281602,"struction results, while Tab. 2 presents the quantitative comparison results of CD and F1-score. We
219"
EXPERIMENTAL RESULTS,0.3454317897371715,"test each method using their own mesh extraction technique, as shown in the inset figure, which
220"
EXPERIMENTAL RESULTS,0.34668335419274093,"display obvious visual artifacts such as small holes and non-smoothness. We thus apply DCUDF [32]
221"
EXPERIMENTAL RESULTS,0.3479349186483104,", the state-of-art method, to each baseline model , extracting the surfaces as significantly higher
222"
EXPERIMENTAL RESULTS,0.34918648310387984,"quality meshes. Since our method utilizes DCUDF for surface extraction, we adopt it as the default
223"
EXPERIMENTAL RESULTS,0.3504380475594493,"technique to ensure consistency and fairness in comparisons with the baselines. Our method achieves
224"
EXPERIMENTAL RESULTS,0.35168961201501875,"stable results in reconstructing various types of surfaces, including both open and closed surfaces,
225"
EXPERIMENTAL RESULTS,0.35294117647058826,"and exhibits performance comparable to that of the SOTA methods. Noting that DUDF[19] requires
226"
EXPERIMENTAL RESULTS,0.3541927409261577,"normals during training, and GeoUDF utilizes the KNN approach to determine the nearest neighbors
227"
EXPERIMENTAL RESULTS,0.35544430538172717,"of the query points. Although DUDF and GeoUDF achieve better evaluations, they are less stable
228"
EXPERIMENTAL RESULTS,0.3566958698372966,"when dealing with point clouds with noise and outliers.
229"
EXPERIMENTAL RESULTS,0.3579474342928661,"Clean
Noise
Outlier"
EXPERIMENTAL RESULTS,0.3591989987484355,"CD ↓
F1 ↑
CD ↓
F1 ↑
CD ↓
F1 ↑
method
F10.005
F10.01
F10.005
F10.01
F10.005
F10.01"
EXPERIMENTAL RESULTS,0.360450563204005,ShapeNetCars [21]
EXPERIMENTAL RESULTS,0.3617021276595745,"CAP-UDF [17]
2.432
0.523
0.888
2.602
0.194
0.381
4.982
0.183
0.314
LevelSetUDF [14]
1.534
0.561
0.908
2.490
0.209
0.401
4.177
0.199
0.363
GeoUDF [15]
1.257
0.571
0.889
1.232
0.351
0.873
4.870
0.187
0.346
DUDF [19]
0.568
0.903
0.991
3.180
0.312
0.527
4.235
0.168
0.308
Ours
1.085
0.510
0.938
1.114
0.427
0.922
1.272
0.485
0.771"
EXPERIMENTAL RESULTS,0.36295369211514394,DeepFashion3D [36]
EXPERIMENTAL RESULTS,0.3642052565707134,"CAP-UDF [17]
1.660
0.417
0.818
1.892
0.336
0.542
4.941
0.172
0.430
LevelSetUDF [14]
1.500
0.403
0.856
1.488
0.453
0.729
4.328
0.203
0.468
GeoUDF [15]
0.652
0.864
0.977
1.258
0.380
0.957
4.463
0.147
0.300
DUDF [19]
0.381
0.991
0.998
1.894
0.334
0.535
4.970
0.144
0.272
Ours
0.932
0.652
0.983
1.150
0.361
0.976
1.029
0.549
0.973
Table 2: Quantitative evaluation of UDF learning methods (CD score is multiplied by 100)."
EXPERIMENTAL RESULTS,0.36545682102628285,"Noise & outliers. To evaluate our model with noisy inputs, we added Gaussian noise N(0, 0.0025) to
230"
EXPERIMENTAL RESULTS,0.3667083854818523,"the clean data across all datasets for testing. The middle three rows in Fig. 5 display the reconstructed
231"
EXPERIMENTAL RESULTS,0.36795994993742176,"surface results from noisy point clouds, and Tab. 2 also presents the quantitative comparisons. It
232"
EXPERIMENTAL RESULTS,0.3692115143929912,"can be observed that our method can robustly reconstruct smooth surfaces from noisy point clouds.
233"
EXPERIMENTAL RESULTS,0.3704630788485607,"Additionally, we tested our method’s performance with outliers by converting 10% of the clean point
234"
EXPERIMENTAL RESULTS,0.3717146433041302,"cloud into outliers, as shown in the last three rows of Fig. 5. To further demonstrate the robustness
235"
EXPERIMENTAL RESULTS,0.37296620775969963,"of our method, we conducted experiments on point clouds with higher percentage of outliers. Our
236"
EXPERIMENTAL RESULTS,0.3742177722152691,"framework is able of reconstructing reasonable surfaces even with 50% outliers. We also tested the
237"
EXPERIMENTAL RESULTS,0.37546933667083854,"task on point clouds containing both noise and outliers. Please refer to Fig. 9 in the Appendix for the
238"
EXPERIMENTAL RESULTS,0.376720901126408,"corresponding results.
239"
EXPERIMENTAL RESULTS,0.37797246558197745,"Real-world scanned data. Dataset [38] provide several real-world scanned point clouds, as illustrated
240"
EXPERIMENTAL RESULTS,0.37922403003754696,"in Fig. 6 (Left), we evaluate our model on the dataset to demonstrate the effectiveness. Our approach
241"
EXPERIMENTAL RESULTS,0.3804755944931164,"can reconstruct smooth surfaces from scanned data containing noise and outliers. However, our
242"
EXPERIMENTAL RESULTS,0.38172715894868586,"model cannot address the issue of missing parts. This limitation is due to the local geometric training
243"
EXPERIMENTAL RESULTS,0.3829787234042553,"strategy, which is independent of the global shape. Additionally, we conduct tests on large scanned
244"
EXPERIMENTAL RESULTS,0.38423028785982477,"scenes to evaluate our algorithm, as shown in Fig. 6 (Right).
245"
ANALYSIS & ABLATION STUDIES,0.3854818523153942,"4.3
Analysis & ablation studies
246"
ANALYSIS & ABLATION STUDIES,0.3867334167709637,"Efficiency.
As
a
supervised
UDF
learning
method,
our
approach
has
a
247"
ANALYSIS & ABLATION STUDIES,0.3879849812265332,"significant
improvement
in
training
efficiency
compared
to
GeoUDF
[15].
248"
ANALYSIS & ABLATION STUDIES,0.38923654568210264,"Method
Storage (GB)
Data-prep (min)
Training (h)"
ANALYSIS & ABLATION STUDIES,0.3904881101376721,"GeoUDF
120
0.5
36"
ANALYSIS & ABLATION STUDIES,0.39173967459324155,"Ours
0.59
0.02
14.5"
ANALYSIS & ABLATION STUDIES,0.392991239048811,"As shown in the insert table, we calculate the data storage
249"
ANALYSIS & ABLATION STUDIES,0.39424280350438046,"space required by GeoUDF when using ShapeNet as a
250"
ANALYSIS & ABLATION STUDIES,0.3954943679599499,"training dataset. This includes the GT UDF values and
251"
ANALYSIS & ABLATION STUDIES,0.3967459324155194,"point cloud data needed during the training process. Our
252"
ANALYSIS & ABLATION STUDIES,0.3979974968710889,"synthetic point cloud patches training dataset occupies under 1GB, which is merely 0.5% of the storage
253"
ANALYSIS & ABLATION STUDIES,0.3992490613266583,"needed for GeoUDF. Our network is very lightweight, with only 653KB of trainable parameters and a
254"
ANALYSIS & ABLATION STUDIES,0.4005006257822278,"total parameter size of just 2MB. Additionally, we highlight time-saving benefits. The provided table
255"
ANALYSIS & ABLATION STUDIES,0.40175219023779724,"illustrates the duration required to produce a single data sample for dataset preparation (“Data-prep”),
256"
ANALYSIS & ABLATION STUDIES,0.4030037546933667,"as well as the total time for training (“Training”).
257"
ANALYSIS & ABLATION STUDIES,0.40425531914893614,"Patch radius. During the evaluation phase, the radius r used to find the nearest points for each
258"
ANALYSIS & ABLATION STUDIES,0.40550688360450565,"query point determines the size of the extracted patch and the range of effective query points in the
259"
ANALYSIS & ABLATION STUDIES,0.4067584480600751,"space. As shown in Fig. 7, we analyzed the impact of different radii on the reconstruction results.
260"
ANALYSIS & ABLATION STUDIES,0.40801001251564456,"An excessively small r will generate artifacts, while an overly large r will lose many details. In our
261"
ANALYSIS & ABLATION STUDIES,0.409261576971214,"experiments, we generally set r to 0.018.
262"
ANALYSIS & ABLATION STUDIES,0.41051314142678347,"(a) Input
(b) CAP-UDF
(c) LevelSetUDF
(d) GeoUDF
(e) DUDF
(f) Ours
(g) GT
Figure 5: Visual comparisons on the synthetic dataset. First three rows: uniformly sampled points.
Meddle three rows: point clouds with 0.25% added noise. Last three rows: point clouds with 10%
outliers. All point clouds here have 48K points, except for the Bunny model, which has 100K points.
We refer readers to the appendix for more visual results."
ANALYSIS & ABLATION STUDIES,0.4117647058823529,Missing
ANALYSIS & ABLATION STUDIES,0.4130162703379224,Figure 6: Reconstructed surfaces from real-world scanned point clouds.
ANALYSIS & ABLATION STUDIES,0.4142678347934919,"GT
r = 0.08
r = 0.10
r = 0.20
r = 0.30
Figure 7: Comparison of different radii for extracting patches from the point cloud on reconstruction
results."
ANALYSIS & ABLATION STUDIES,0.41551939924906134,"Denoising module. Our framework incorporates a denoising module to handle noisy point clouds.
263"
ANALYSIS & ABLATION STUDIES,0.4167709637046308,"We conducted ablation experiments to verify the significance of this module. Specifically, we set
264"
ANALYSIS & ABLATION STUDIES,0.41802252816020025,"λd = 0 in the loss function Eq. (7) to disable the denoising module, and then retrained the network.
265"
ANALYSIS & ABLATION STUDIES,0.4192740926157697,"As illustrated in Fig. 8, we present the reconstructed surfaces for the same set of noisy point clouds
266"
ANALYSIS & ABLATION STUDIES,0.42052565707133915,"with and without the denosing module, respectively.
267 Noise"
ANALYSIS & ABLATION STUDIES,0.42177722152690866,"W/O
W/O
W/
W/"
ANALYSIS & ABLATION STUDIES,0.4230287859824781,Outliers
ANALYSIS & ABLATION STUDIES,0.42428035043804757,"Figure 8: Ablation on denoising module: Reconstructed surfaces from the same point clouds with
noise/outliers corresponding to framework with and without the denoising module, respectively."
CONCLUSION,0.425531914893617,"5
Conclusion
268"
CONCLUSION,0.4267834793491865,"In this paper, we introduce a novel and efficient neural framework for surface reconstruction from 3D
269"
CONCLUSION,0.42803504380475593,"point clouds by learning UDFs from local shape functions. Our key insight is that 3D shapes exhibit
270"
CONCLUSION,0.4292866082603254,"simple patterns within localized regions, which can be exploited to create a training dataset of point
271"
CONCLUSION,0.4305381727158949,"cloud patches represented by mathematical functions. As a result, our method enables efficient and
272"
CONCLUSION,0.43178973717146435,"robust surfaces reconstructions without the need for shape-specific training. Extensive experiments
273"
CONCLUSION,0.4330413016270338,"on various datasets have demonstrated the efficacy of our method. Moreover, our framework achieves
274"
CONCLUSION,0.43429286608260326,"superior performance on point clouds with noise and outliers.
275"
CONCLUSION,0.4355444305381727,"Limitations & future work. Owing to its dependence solely on local geometric features, our
276"
CONCLUSION,0.43679599499374216,"approach fails to address tasks involving incomplete point cloud reconstructions. However, as a
277"
CONCLUSION,0.4380475594493116,"lightweight framework, our model can readily be integrated into other unsupervised methods to
278"
CONCLUSION,0.43929912390488113,"combine the global features with our learned local priors. Furthermore, in our future work, we
279"
CONCLUSION,0.4405506883604506,"intend to design a method that dynamically adjusts the radius based on local feature sizes [39] of 3D
280"
CONCLUSION,0.44180225281602004,"shapes when extracting local point cloud patches for queries, aiming to improve the accuracy of the
281"
CONCLUSION,0.4430538172715895,"reconstruction.
282"
REFERENCES,0.44430538172715894,"References
283"
REFERENCES,0.4455569461827284,"[1] M Kazhdan. Poisson surface reconstruction. In Eurographics Symposium on Geometry Process-
284"
REFERENCES,0.44680851063829785,"ing, 2006.
285"
REFERENCES,0.44806007509386736,"[2] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
286"
REFERENCES,0.4493116395494368,"Deepsdf: Learning continuous signed distance functions for shape representation. In The IEEE
287"
REFERENCES,0.45056320400500627,"Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
288"
REFERENCES,0.4518147684605757,"[3] Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove,
289"
REFERENCES,0.4530663329161452,"and Richard Newcombe. Deep local shapes: Learning local sdf priors for detailed 3d recon-
290"
REFERENCES,0.45431789737171463,"struction, 2020.
291"
REFERENCES,0.4555694618272841,"[4] Ma Baorui, Han Zhizhong, Liu Yu-Shen, and Zwicker Matthias. Neural-pull: Learning signed
292"
REFERENCES,0.4568210262828536,"distance functions from point clouds by learning to pull space onto surfaces. In International
293"
REFERENCES,0.45807259073842305,"Conference on Machine Learning (ICML), 2021.
294"
REFERENCES,0.4593241551939925,"[5] Peng-Shuai Wang, Yang Liu, and Xin Tong. Dual octree graph networks for learning adaptive
295"
REFERENCES,0.46057571964956195,"volumetric shape representations. ACM Transactions on Graphics, 41(4):1–15, July 2022.
296"
REFERENCES,0.4618272841051314,"[6] Hao Pan Pengshuai Wang Xin Tong Yang Liu Shi-Lin Liu, Hao-Xiang Guo. Deep implicit
297"
REFERENCES,0.46307884856070086,"moving least-squares functions for 3d reconstruction. In IEEE/CVF Conference on Computer
298"
REFERENCES,0.4643304130162703,"Vision and Pattern Recognition, 2021.
299"
REFERENCES,0.4655819774718398,"[7] Zixiong Wang, Pengfei Wang, Pengshuai Wang, Qiujie Dong, Junjie Gao, Shuangmin Chen,
300"
REFERENCES,0.4668335419274093,"Shiqing Xin, Changhe Tu, and Wenping Wang. Neural-imls: Self-supervised implicit moving
301"
REFERENCES,0.46808510638297873,"least-squares network for surface reconstruction. IEEE Transactions on Visualization and
302"
REFERENCES,0.4693366708385482,"Computer Graphics, pages 1–16, 2023.
303"
REFERENCES,0.47058823529411764,"[8] Ma Baorui, Liu Yu-Shen, and Han Zhizhong. Reconstructing surfaces for sparse point clouds
304"
REFERENCES,0.4718397997496871,"with on-surface priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and
305"
REFERENCES,0.47309136420525655,"Pattern Recognition (CVPR), 2022.
306"
REFERENCES,0.47434292866082606,"[9] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.
307"
REFERENCES,0.4755944931163955,"Occupancy networks: Learning 3d reconstruction in function space. In Proceedings IEEE Conf.
308"
REFERENCES,0.47684605757196497,"on Computer Vision and Pattern Recognition (CVPR), 2019.
309"
REFERENCES,0.4780976220275344,"[10] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. Implicit functions in feature space for
310"
REFERENCES,0.4793491864831039,"3d shape reconstruction and completion. In IEEE Conference on Computer Vision and Pattern
311"
REFERENCES,0.4806007509386733,"Recognition (CVPR). IEEE, jun 2020.
312"
REFERENCES,0.4818523153942428,"[11] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.
313"
REFERENCES,0.4831038798498123,"Convolutional occupancy networks. In European Conference on Computer Vision (ECCV),
314"
REFERENCES,0.48435544430538174,"2020.
315"
REFERENCES,0.4856070087609512,"[12] Alexandre Boulch and Renaud Marlet. Poco: Point convolution for surface reconstruction,
316"
REFERENCES,0.48685857321652065,"2022.
317"
REFERENCES,0.4881101376720901,"[13] Julian Chibane, Aymen Mir, and Gerard Pons-Moll. Neural unsigned distance fields for implicit
318"
REFERENCES,0.48936170212765956,"function learning. In Advances in Neural Information Processing Systems (NeurIPS), December
319"
REFERENCES,0.490613266583229,"2020.
320"
REFERENCES,0.4918648310387985,"[14] Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, and Zhizhong Han. Learning a more
321"
REFERENCES,0.493116395494368,"continuous zero level set in unsigned distance fields through level set projection. In Proceedings
322"
REFERENCES,0.49436795994993743,"of the IEEE/CVF international conference on computer vision, 2023.
323"
REFERENCES,0.4956195244055069,"[15] Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, and Wenping Wang. Geoudf: Surface
324"
REFERENCES,0.49687108886107634,"reconstruction from 3d point clouds via geometry-guided distance representation. In Proceedings
325"
REFERENCES,0.4981226533166458,"of the IEEE/CVF International Conference on Computer Vision, pages 14214–14224, 2023.
326"
REFERENCES,0.49937421777221525,"[16] Jianglong Ye, Yuntao Chen, Naiyan Wang, and Xiaolong Wang. Gifs: Neural implicit function
327"
REFERENCES,0.5006257822277848,"for general shape representation. In Proceedings of the IEEE/CVF Conference on Computer
328"
REFERENCES,0.5018773466833542,"Vision and Pattern Recognition, 2022.
329"
REFERENCES,0.5031289111389237,"[17] Junsheng Zhou, Baorui Ma, Yu-Shen Liu, Yi Fang, and Zhizhong Han. Learning consistency-
330"
REFERENCES,0.5043804755944932,"aware unsigned distance functions progressively from raw point clouds. In Advances in Neural
331"
REFERENCES,0.5056320400500626,"Information Processing Systems (NeurIPS), 2022.
332"
REFERENCES,0.5068836045056321,"[18] Qing Li, Huifang Feng, Kanle Shi, Yi Fang, Yu-Shen Liu, and Zhizhong Han. Neural gradient
333"
REFERENCES,0.5081351689612015,"learning and optimization for oriented point normal estimation. In SIGGRAPH Asia 2023
334"
REFERENCES,0.509386733416771,"Conference Papers, 2023.
335"
REFERENCES,0.5106382978723404,"[19] Miguel Fainstein, Viviana Siless, and Emmanuel Iarussi. Dudf: Differentiable unsigned distance
336"
REFERENCES,0.5118898623279099,"fields with hyperbolic scaling, 2024.
337"
REFERENCES,0.5131414267834794,"[20] Yujie Lu, Long Wan, Nayu Ding, Yulong Wang, Shuhan Shen, Shen Cai, and Lin Gao. Unsigned
338"
REFERENCES,0.5143929912390488,"orthogonal distance fields: An accurate neural implicit representation for diverse 3d shapes. In
339"
REFERENCES,0.5156445556946183,"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.
340"
REFERENCES,0.5168961201501877,"[21] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo
341"
REFERENCES,0.5181476846057572,"Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu.
342"
REFERENCES,0.5193992490613266,"ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012
343"
REFERENCES,0.5206508135168961,"[cs.GR], Stanford University — Princeton University — Toyota Technological Institute at
344"
REFERENCES,0.5219023779724656,"Chicago, 2015.
345"
REFERENCES,0.523153942428035,"[22] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. Acm Transac-
346"
REFERENCES,0.5244055068836045,"tions on Graphics, 32(3):1–13, 2013.
347"
REFERENCES,0.5256570713391739,"[23] Fei Hou, Chiyu Wang, Wencheng Wang, Hong Qin, Chen Qian, and Ying He. Iterative poisson
348"
REFERENCES,0.5269086357947435,"surface reconstruction (ipsr) for unoriented points. ACM Transactions on Graphics, 41(4):1–13,
349"
REFERENCES,0.5281602002503129,"July 2022.
350"
REFERENCES,0.5294117647058824,"[24] Songyou Peng, Chiyu ""Max"" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas
351"
REFERENCES,0.5306633291614519,"Geiger. Shape as points: A differentiable poisson solver. In Advances in Neural Information
352"
REFERENCES,0.5319148936170213,"Processing Systems (NeurIPS), 2021.
353"
REFERENCES,0.5331664580725908,"[25] Amine Ouasfi and Adnane Boukhayma. Unsupervised occupancy learning from sparse point
354"
REFERENCES,0.5344180225281602,"cloud, 2024.
355"
REFERENCES,0.5356695869837297,"[26] Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, and Bernhard
356"
REFERENCES,0.5369211514392991,"Schölkopf. Ghost on the shell: An expressive representation of general 3d shapes. 2024.
357"
REFERENCES,0.5381727158948686,"[27] Junkai Deng, Fei Hou, Xuhui Chen, Wencheng Wang, and Ying He. 2s-udf: A novel two-stage
358"
REFERENCES,0.5394242803504381,"udf learning method for robust non-watertight model reconstruction from multi-view images,
359"
REFERENCES,0.5406758448060075,"2024.
360"
REFERENCES,0.541927409261577,"[28] Xiaoxu Meng, Weikai Chen, and Bo Yang. Neat: Learning neural implicit surfaces with
361"
REFERENCES,0.5431789737171464,"arbitrary topologies from multi-view images. Proceedings of the IEEE/CVF Conference on
362"
REFERENCES,0.5444305381727159,"Computer Vision and Pattern Recognition, June 2023.
363"
REFERENCES,0.5456821026282853,"[29] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Yuan Liu, Peng Wang, Christian Theobalt, Taku
364"
REFERENCES,0.5469336670838548,"Komura, and Wenping Wang. Neuraludf: Learning unsigned distance fields for multi-view
365"
REFERENCES,0.5481852315394243,"reconstruction of surfaces with arbitrary topologies. In Proceedings of the IEEE/CVF Conference
366"
REFERENCES,0.5494367959949937,"on Computer Vision and Pattern Recognition, pages 20834–20843, 2023.
367"
REFERENCES,0.5506883604505632,"[30] Yu-Tao Liu, Li Wang, Jie Yang, Weikai Chen, Xiaoxu Meng, Bo Yang, and Lin Gao. Neudf:
368"
REFERENCES,0.5519399249061326,"Leaning neural unsigned distance fields with volume rendering. In Computer Vision and Pattern
369"
REFERENCES,0.5531914893617021,"Recognition (CVPR), 2023.
370"
REFERENCES,0.5544430538172715,"[31] Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, and Zhizhong Han. Udiff:
371"
REFERENCES,0.5556946182728411,"Generating conditional unsigned distance fields with optimal wavelet diffusion. In Proceedings
372"
REFERENCES,0.5569461827284106,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.
373"
REFERENCES,0.55819774718398,"[32] Fei Hou, Xuhui Chen, Wencheng Wang, Hong Qin, and Ying He. Robust zero level-set
374"
REFERENCES,0.5594493116395495,"extraction from unsigned distance fields based on double covering. ACM Trans. Graph., 42(6),
375"
REFERENCES,0.5607008760951189,"dec 2023.
376"
REFERENCES,0.5619524405506884,"[33] Manfredo P Do Carmo. Differential geometry of curves and surfaces: revised and updated
377"
REFERENCES,0.5632040050062578,"second edition. Courier Dover Publications, 2016.
378"
REFERENCES,0.5644555694618273,"[34] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical
379"
REFERENCES,0.5657071339173968,"feature learning on point sets in a metric space. Advances in neural information processing
380"
REFERENCES,0.5669586983729662,"systems, 30, 2017.
381"
REFERENCES,0.5682102628285357,"[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
382"
REFERENCES,0.5694618272841051,"Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
383"
REFERENCES,0.5707133917396746,"[36] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust
384"
REFERENCES,0.571964956195244,"clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on
385"
REFERENCES,0.5732165206508135,"Computer Vision and Pattern Recognition (CVPR), June 2016.
386"
REFERENCES,0.574468085106383,"[37] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
387"
REFERENCES,0.5757196495619524,"Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer
388"
REFERENCES,0.5769712140175219,"Vision and Pattern Recognition (CVPR), IEEE, 2017.
389"
REFERENCES,0.5782227784730913,"[38] Matthew Berger, Joshua A. Levine, Luis Gustavo Nonato, Gabriel Taubin, and Claudio T. Silva.
390"
REFERENCES,0.5794743429286608,"A benchmark for surface reconstruction. ACM Trans. Graph., 32(2), apr 2013.
391"
REFERENCES,0.5807259073842302,"[39] Yulan Guo, Mohammed Bennamoun, Ferdous Sohel, Min Lu, Jianwei Wan, and Ngai Ming
392"
REFERENCES,0.5819774718397998,"Kwok. A comprehensive performance evaluation of 3d local feature descriptors. International
393"
REFERENCES,0.5832290362953693,"Journal of Computer Vision, 116:66–89, 2016.
394"
REFERENCES,0.5844806007509387,"NeurIPS Paper Checklist
395"
CLAIMS,0.5857321652065082,"1. Claims
396"
CLAIMS,0.5869837296620776,"Question: Do the main claims made in the abstract and introduction accurately reflect the
397"
CLAIMS,0.5882352941176471,"paper’s contributions and scope?
398"
CLAIMS,0.5894868585732165,"Answer: [Yes]
399"
CLAIMS,0.590738423028786,"Justification: Our abstract and introduction accurately describe our technical contributions
400"
CLAIMS,0.5919899874843555,"to Unsigned Distance Fields learning.
401"
CLAIMS,0.5932415519399249,"Guidelines:
402"
CLAIMS,0.5944931163954944,"• The answer NA means that the abstract and introduction do not include the claims
403"
CLAIMS,0.5957446808510638,"made in the paper.
404"
CLAIMS,0.5969962453066333,"• The abstract and/or introduction should clearly state the claims made, including the
405"
CLAIMS,0.5982478097622027,"contributions made in the paper and important assumptions and limitations. A No or
406"
CLAIMS,0.5994993742177722,"NA answer to this question will not be perceived well by the reviewers.
407"
CLAIMS,0.6007509386733417,"• The claims made should match theoretical and experimental results, and reflect how
408"
CLAIMS,0.6020025031289111,"much the results can be expected to generalize to other settings.
409"
CLAIMS,0.6032540675844806,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
410"
CLAIMS,0.60450563204005,"are not attained by the paper.
411"
LIMITATIONS,0.6057571964956195,"2. Limitations
412"
LIMITATIONS,0.6070087609511889,"Question: Does the paper discuss the limitations of the work performed by the authors?
413"
LIMITATIONS,0.6082603254067585,"Answer: [Yes]
414"
LIMITATIONS,0.609511889862328,"Justification: We discuss the limitations in the conclusion section (Sec.5).
415"
LIMITATIONS,0.6107634543178974,"Guidelines:
416"
LIMITATIONS,0.6120150187734669,"• The answer NA means that the paper has no limitation while the answer No means that
417"
LIMITATIONS,0.6132665832290363,"the paper has limitations, but those are not discussed in the paper.
418"
LIMITATIONS,0.6145181476846058,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
419"
LIMITATIONS,0.6157697121401752,"• The paper should point out any strong assumptions and how robust the results are to
420"
LIMITATIONS,0.6170212765957447,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
421"
LIMITATIONS,0.6182728410513142,"model well-specification, asymptotic approximations only holding locally). The authors
422"
LIMITATIONS,0.6195244055068836,"should reflect on how these assumptions might be violated in practice and what the
423"
LIMITATIONS,0.6207759699624531,"implications would be.
424"
LIMITATIONS,0.6220275344180225,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
425"
LIMITATIONS,0.623279098873592,"only tested on a few datasets or with a few runs. In general, empirical results often
426"
LIMITATIONS,0.6245306633291614,"depend on implicit assumptions, which should be articulated.
427"
LIMITATIONS,0.6257822277847309,"• The authors should reflect on the factors that influence the performance of the approach.
428"
LIMITATIONS,0.6270337922403004,"For example, a facial recognition algorithm may perform poorly when image resolution
429"
LIMITATIONS,0.6282853566958698,"is low or images are taken in low lighting. Or a speech-to-text system might not be
430"
LIMITATIONS,0.6295369211514393,"used reliably to provide closed captions for online lectures because it fails to handle
431"
LIMITATIONS,0.6307884856070087,"technical jargon.
432"
LIMITATIONS,0.6320400500625782,"• The authors should discuss the computational efficiency of the proposed algorithms
433"
LIMITATIONS,0.6332916145181476,"and how they scale with dataset size.
434"
LIMITATIONS,0.6345431789737171,"• If applicable, the authors should discuss possible limitations of their approach to
435"
LIMITATIONS,0.6357947434292867,"address problems of privacy and fairness.
436"
LIMITATIONS,0.6370463078848561,"• While the authors might fear that complete honesty about limitations might be used by
437"
LIMITATIONS,0.6382978723404256,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
438"
LIMITATIONS,0.639549436795995,"limitations that aren’t acknowledged in the paper. The authors should use their best
439"
LIMITATIONS,0.6408010012515645,"judgment and recognize that individual actions in favor of transparency play an impor-
440"
LIMITATIONS,0.6420525657071339,"tant role in developing norms that preserve the integrity of the community. Reviewers
441"
LIMITATIONS,0.6433041301627034,"will be specifically instructed to not penalize honesty concerning limitations.
442"
THEORY ASSUMPTIONS AND PROOFS,0.6445556946182729,"3. Theory Assumptions and Proofs
443"
THEORY ASSUMPTIONS AND PROOFS,0.6458072590738423,"Question: For each theoretical result, does the paper provide the full set of assumptions and
444"
THEORY ASSUMPTIONS AND PROOFS,0.6470588235294118,"a complete (and correct) proof?
445"
THEORY ASSUMPTIONS AND PROOFS,0.6483103879849812,"Answer: [Yes]
446"
THEORY ASSUMPTIONS AND PROOFS,0.6495619524405507,"Justification: We provide the differential geometry theory employed by our method in the
447"
THEORY ASSUMPTIONS AND PROOFS,0.6508135168961201,"main text (Sec.3).
448"
THEORY ASSUMPTIONS AND PROOFS,0.6520650813516896,"Guidelines:
449"
THEORY ASSUMPTIONS AND PROOFS,0.6533166458072591,"• The answer NA means that the paper does not include theoretical results.
450"
THEORY ASSUMPTIONS AND PROOFS,0.6545682102628285,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
451"
THEORY ASSUMPTIONS AND PROOFS,0.655819774718398,"referenced.
452"
THEORY ASSUMPTIONS AND PROOFS,0.6570713391739674,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
453"
THEORY ASSUMPTIONS AND PROOFS,0.6583229036295369,"• The proofs can either appear in the main paper or the supplemental material, but if
454"
THEORY ASSUMPTIONS AND PROOFS,0.6595744680851063,"they appear in the supplemental material, the authors are encouraged to provide a short
455"
THEORY ASSUMPTIONS AND PROOFS,0.6608260325406758,"proof sketch to provide intuition.
456"
THEORY ASSUMPTIONS AND PROOFS,0.6620775969962454,"• Inversely, any informal proof provided in the core of the paper should be complemented
457"
THEORY ASSUMPTIONS AND PROOFS,0.6633291614518148,"by formal proofs provided in appendix or supplemental material.
458"
THEORY ASSUMPTIONS AND PROOFS,0.6645807259073843,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
459"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6658322903629537,"4. Experimental Result Reproducibility
460"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6670838548185232,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
461"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6683354192740926,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
462"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6695869837296621,"of the paper (regardless of whether the code and data are provided or not)?
463"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6708385481852316,"Answer: [Yes]
464"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.672090112640801,"Justification: We provide the most detailed algorithmic details possible in the main text and
465"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6733416770963705,"appendix.
466"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6745932415519399,"Guidelines:
467"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6758448060075094,"• The answer NA means that the paper does not include experiments.
468"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6770963704630788,"• If the paper includes experiments, a No answer to this question will not be perceived
469"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6783479349186483,"well by the reviewers: Making the paper reproducible is important, regardless of
470"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6795994993742178,"whether the code and data are provided or not.
471"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6808510638297872,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
472"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6821026282853567,"to make their results reproducible or verifiable.
473"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6833541927409261,"• Depending on the contribution, reproducibility can be accomplished in various ways.
474"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6846057571964956,"For example, if the contribution is a novel architecture, describing the architecture fully
475"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.685857321652065,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
476"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6871088861076345,"be necessary to either make it possible for others to replicate the model with the same
477"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.688360450563204,"dataset, or provide access to the model. In general. releasing code and data is often
478"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6896120150187734,"one good way to accomplish this, but reproducibility can also be provided via detailed
479"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.690863579474343,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
480"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6921151439299124,"of a large language model), releasing of a model checkpoint, or other means that are
481"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6933667083854819,"appropriate to the research performed.
482"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6946182728410513,"• While NeurIPS does not require releasing code, the conference does require all submis-
483"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6958698372966208,"sions to provide some reasonable avenue for reproducibility, which may depend on the
484"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6971214017521903,"nature of the contribution. For example
485"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6983729662077597,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
486"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6996245306633292,"to reproduce that algorithm.
487"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7008760951188986,"(b) If the contribution is primarily a new model architecture, the paper should describe
488"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7021276595744681,"the architecture clearly and fully.
489"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7033792240300375,"(c) If the contribution is a new model (e.g., a large language model), then there should
490"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.704630788485607,"either be a way to access this model for reproducing the results or a way to reproduce
491"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7058823529411765,"the model (e.g., with an open-source dataset or instructions for how to construct
492"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7071339173967459,"the dataset).
493"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7083854818523154,"(d) We recognize that reproducibility may be tricky in some cases, in which case
494"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7096370463078848,"authors are welcome to describe the particular way they provide for reproducibility.
495"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7108886107634543,"In the case of closed-source models, it may be that access to the model is limited in
496"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7121401752190237,"some way (e.g., to registered users), but it should be possible for other researchers
497"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7133917396745932,"to have some path to reproducing or verifying the results.
498"
OPEN ACCESS TO DATA AND CODE,0.7146433041301627,"5. Open access to data and code
499"
OPEN ACCESS TO DATA AND CODE,0.7158948685857321,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
500"
OPEN ACCESS TO DATA AND CODE,0.7171464330413017,"tions to faithfully reproduce the main experimental results, as described in supplemental
501"
OPEN ACCESS TO DATA AND CODE,0.718397997496871,"material?
502"
OPEN ACCESS TO DATA AND CODE,0.7196495619524406,"Answer: [No]
503"
OPEN ACCESS TO DATA AND CODE,0.72090112640801,"Justification: We will definitely make our code publicly available one day, but not at this
504"
OPEN ACCESS TO DATA AND CODE,0.7221526908635795,"moment.
505"
OPEN ACCESS TO DATA AND CODE,0.723404255319149,"Guidelines:
506"
OPEN ACCESS TO DATA AND CODE,0.7246558197747184,"• The answer NA means that paper does not include experiments requiring code.
507"
OPEN ACCESS TO DATA AND CODE,0.7259073842302879,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
508"
OPEN ACCESS TO DATA AND CODE,0.7271589486858573,"public/guides/CodeSubmissionPolicy) for more details.
509"
OPEN ACCESS TO DATA AND CODE,0.7284105131414268,"• While we encourage the release of code and data, we understand that this might not be
510"
OPEN ACCESS TO DATA AND CODE,0.7296620775969962,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
511"
OPEN ACCESS TO DATA AND CODE,0.7309136420525657,"including code, unless this is central to the contribution (e.g., for a new open-source
512"
OPEN ACCESS TO DATA AND CODE,0.7321652065081352,"benchmark).
513"
OPEN ACCESS TO DATA AND CODE,0.7334167709637046,"• The instructions should contain the exact command and environment needed to run to
514"
OPEN ACCESS TO DATA AND CODE,0.7346683354192741,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
515"
OPEN ACCESS TO DATA AND CODE,0.7359198998748435,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
516"
OPEN ACCESS TO DATA AND CODE,0.737171464330413,"• The authors should provide instructions on data access and preparation, including how
517"
OPEN ACCESS TO DATA AND CODE,0.7384230287859824,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
518"
OPEN ACCESS TO DATA AND CODE,0.7396745932415519,"• The authors should provide scripts to reproduce all experimental results for the new
519"
OPEN ACCESS TO DATA AND CODE,0.7409261576971214,"proposed method and baselines. If only a subset of experiments are reproducible, they
520"
OPEN ACCESS TO DATA AND CODE,0.7421777221526908,"should state which ones are omitted from the script and why.
521"
OPEN ACCESS TO DATA AND CODE,0.7434292866082604,"• At submission time, to preserve anonymity, the authors should release anonymized
522"
OPEN ACCESS TO DATA AND CODE,0.7446808510638298,"versions (if applicable).
523"
OPEN ACCESS TO DATA AND CODE,0.7459324155193993,"• Providing as much information as possible in supplemental material (appended to the
524"
OPEN ACCESS TO DATA AND CODE,0.7471839799749687,"paper) is recommended, but including URLs to data and code is permitted.
525"
OPEN ACCESS TO DATA AND CODE,0.7484355444305382,"6. Experimental Setting/Details
526"
OPEN ACCESS TO DATA AND CODE,0.7496871088861077,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
527"
OPEN ACCESS TO DATA AND CODE,0.7509386733416771,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
528"
OPEN ACCESS TO DATA AND CODE,0.7521902377972466,"results?
529"
OPEN ACCESS TO DATA AND CODE,0.753441802252816,"Answer: [Yes]
530"
OPEN ACCESS TO DATA AND CODE,0.7546933667083855,"Justification: We introduce all the training and test details in the main text and appendix.
531"
OPEN ACCESS TO DATA AND CODE,0.7559449311639549,"Guidelines:
532"
OPEN ACCESS TO DATA AND CODE,0.7571964956195244,"• The answer NA means that the paper does not include experiments.
533"
OPEN ACCESS TO DATA AND CODE,0.7584480600750939,"• The experimental setting should be presented in the core of the paper to a level of detail
534"
OPEN ACCESS TO DATA AND CODE,0.7596996245306633,"that is necessary to appreciate the results and make sense of them.
535"
OPEN ACCESS TO DATA AND CODE,0.7609511889862328,"• The full details can be provided either with the code, in appendix, or as supplemental
536"
OPEN ACCESS TO DATA AND CODE,0.7622027534418022,"material.
537"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7634543178973717,"7. Experiment Statistical Significance
538"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7647058823529411,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
539"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7659574468085106,"information about the statistical significance of the experiments?
540"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7672090112640801,"Answer: [Yes]
541"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7684605757196495,"Justification: We provide various evaluation metrics about our method.
542"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.769712140175219,"Guidelines:
543"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7709637046307884,"• The answer NA means that the paper does not include experiments.
544"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.772215269086358,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
545"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7734668335419274,"dence intervals, or statistical significance tests, at least for the experiments that support
546"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7747183979974969,"the main claims of the paper.
547"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7759699624530664,"• The factors of variability that the error bars are capturing should be clearly stated (for
548"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7772215269086358,"example, train/test split, initialization, random drawing of some parameter, or overall
549"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7784730913642053,"run with given experimental conditions).
550"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7797246558197747,"• The method for calculating the error bars should be explained (closed form formula,
551"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7809762202753442,"call to a library function, bootstrap, etc.)
552"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7822277847309136,"• The assumptions made should be given (e.g., Normally distributed errors).
553"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7834793491864831,"• It should be clear whether the error bar is the standard deviation or the standard error
554"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7847309136420526,"of the mean.
555"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.785982478097622,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
556"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7872340425531915,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
557"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7884856070087609,"of Normality of errors is not verified.
558"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7897371714643304,"• For asymmetric distributions, the authors should be careful not to show in tables or
559"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7909887359198998,"figures symmetric error bars that would yield results that are out of range (e.g. negative
560"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7922403003754693,"error rates).
561"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7934918648310388,"• If error bars are reported in tables or plots, The authors should explain in the text how
562"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7947434292866082,"they were calculated and reference the corresponding figures or tables in the text.
563"
EXPERIMENTS COMPUTE RESOURCES,0.7959949937421777,"8. Experiments Compute Resources
564"
EXPERIMENTS COMPUTE RESOURCES,0.7972465581977471,"Question: For each experiment, does the paper provide sufficient information on the com-
565"
EXPERIMENTS COMPUTE RESOURCES,0.7984981226533167,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
566"
EXPERIMENTS COMPUTE RESOURCES,0.799749687108886,"the experiments?
567"
EXPERIMENTS COMPUTE RESOURCES,0.8010012515644556,"Answer: [Yes]
568"
EXPERIMENTS COMPUTE RESOURCES,0.8022528160200251,"Justification: We provide the related information in the experimental section.
569"
EXPERIMENTS COMPUTE RESOURCES,0.8035043804755945,"Guidelines:
570"
EXPERIMENTS COMPUTE RESOURCES,0.804755944931164,"• The answer NA means that the paper does not include experiments.
571"
EXPERIMENTS COMPUTE RESOURCES,0.8060075093867334,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
572"
EXPERIMENTS COMPUTE RESOURCES,0.8072590738423029,"or cloud provider, including relevant memory and storage.
573"
EXPERIMENTS COMPUTE RESOURCES,0.8085106382978723,"• The paper should provide the amount of compute required for each of the individual
574"
EXPERIMENTS COMPUTE RESOURCES,0.8097622027534418,"experimental runs as well as estimate the total compute.
575"
EXPERIMENTS COMPUTE RESOURCES,0.8110137672090113,"• The paper should disclose whether the full research project required more compute
576"
EXPERIMENTS COMPUTE RESOURCES,0.8122653316645807,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
577"
EXPERIMENTS COMPUTE RESOURCES,0.8135168961201502,"didn’t make it into the paper).
578"
CODE OF ETHICS,0.8147684605757196,"9. Code Of Ethics
579"
CODE OF ETHICS,0.8160200250312891,"Question: Does the research conducted in the paper conform, in every respect, with the
580"
CODE OF ETHICS,0.8172715894868585,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
581"
CODE OF ETHICS,0.818523153942428,"Answer: [Yes]
582"
CODE OF ETHICS,0.8197747183979975,"Justification: We strictly adhere to the NeurIPS Code of Ethics.
583"
CODE OF ETHICS,0.8210262828535669,"Guidelines:
584"
CODE OF ETHICS,0.8222778473091364,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
585"
CODE OF ETHICS,0.8235294117647058,"• If the authors answer No, they should explain the special circumstances that require a
586"
CODE OF ETHICS,0.8247809762202754,"deviation from the Code of Ethics.
587"
CODE OF ETHICS,0.8260325406758448,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
588"
CODE OF ETHICS,0.8272841051314143,"eration due to laws or regulations in their jurisdiction).
589"
BROADER IMPACTS,0.8285356695869838,"10. Broader Impacts
590"
BROADER IMPACTS,0.8297872340425532,"Question: Does the paper discuss both potential positive societal impacts and negative
591"
BROADER IMPACTS,0.8310387984981227,"societal impacts of the work performed?
592"
BROADER IMPACTS,0.8322903629536921,"Answer: [Yes]
593"
BROADER IMPACTS,0.8335419274092616,"Justification: Our method may be applied to 3D reconstruction in daily life, demonstrating
594"
BROADER IMPACTS,0.8347934918648311,"significant social value.
595"
BROADER IMPACTS,0.8360450563204005,"Guidelines:
596"
BROADER IMPACTS,0.83729662077597,"• The answer NA means that there is no societal impact of the work performed.
597"
BROADER IMPACTS,0.8385481852315394,"• If the authors answer NA or No, they should explain why their work has no societal
598"
BROADER IMPACTS,0.8397997496871089,"impact or why the paper does not address societal impact.
599"
BROADER IMPACTS,0.8410513141426783,"• Examples of negative societal impacts include potential malicious or unintended uses
600"
BROADER IMPACTS,0.8423028785982478,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
601"
BROADER IMPACTS,0.8435544430538173,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
602"
BROADER IMPACTS,0.8448060075093867,"groups), privacy considerations, and security considerations.
603"
BROADER IMPACTS,0.8460575719649562,"• The conference expects that many papers will be foundational research and not tied
604"
BROADER IMPACTS,0.8473091364205256,"to particular applications, let alone deployments. However, if there is a direct path to
605"
BROADER IMPACTS,0.8485607008760951,"any negative applications, the authors should point it out. For example, it is legitimate
606"
BROADER IMPACTS,0.8498122653316645,"to point out that an improvement in the quality of generative models could be used to
607"
BROADER IMPACTS,0.851063829787234,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
608"
BROADER IMPACTS,0.8523153942428036,"that a generic algorithm for optimizing neural networks could enable people to train
609"
BROADER IMPACTS,0.853566958698373,"models that generate Deepfakes faster.
610"
BROADER IMPACTS,0.8548185231539425,"• The authors should consider possible harms that could arise when the technology is
611"
BROADER IMPACTS,0.8560700876095119,"being used as intended and functioning correctly, harms that could arise when the
612"
BROADER IMPACTS,0.8573216520650814,"technology is being used as intended but gives incorrect results, and harms following
613"
BROADER IMPACTS,0.8585732165206508,"from (intentional or unintentional) misuse of the technology.
614"
BROADER IMPACTS,0.8598247809762203,"• If there are negative societal impacts, the authors could also discuss possible mitigation
615"
BROADER IMPACTS,0.8610763454317898,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
616"
BROADER IMPACTS,0.8623279098873592,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
617"
BROADER IMPACTS,0.8635794743429287,"feedback over time, improving the efficiency and accessibility of ML).
618"
SAFEGUARDS,0.8648310387984981,"11. Safeguards
619"
SAFEGUARDS,0.8660826032540676,"Question: Does the paper describe safeguards that have been put in place for responsible
620"
SAFEGUARDS,0.867334167709637,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
621"
SAFEGUARDS,0.8685857321652065,"image generators, or scraped datasets)?
622"
SAFEGUARDS,0.869837296620776,"Answer: [NA]
623"
SAFEGUARDS,0.8710888610763454,"Justification: Our paper poses no such risks.
624"
SAFEGUARDS,0.8723404255319149,"Guidelines:
625"
SAFEGUARDS,0.8735919899874843,"• The answer NA means that the paper poses no such risks.
626"
SAFEGUARDS,0.8748435544430538,"• Released models that have a high risk for misuse or dual-use should be released with
627"
SAFEGUARDS,0.8760951188986232,"necessary safeguards to allow for controlled use of the model, for example by requiring
628"
SAFEGUARDS,0.8773466833541927,"that users adhere to usage guidelines or restrictions to access the model or implementing
629"
SAFEGUARDS,0.8785982478097623,"safety filters.
630"
SAFEGUARDS,0.8798498122653317,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
631"
SAFEGUARDS,0.8811013767209012,"should describe how they avoided releasing unsafe images.
632"
SAFEGUARDS,0.8823529411764706,"• We recognize that providing effective safeguards is challenging, and many papers do
633"
SAFEGUARDS,0.8836045056320401,"not require this, but we encourage authors to take this into account and make a best
634"
SAFEGUARDS,0.8848560700876095,"faith effort.
635"
LICENSES FOR EXISTING ASSETS,0.886107634543179,"12. Licenses for existing assets
636"
LICENSES FOR EXISTING ASSETS,0.8873591989987485,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
637"
LICENSES FOR EXISTING ASSETS,0.8886107634543179,"the paper, properly credited and are the license and terms of use explicitly mentioned and
638"
LICENSES FOR EXISTING ASSETS,0.8898623279098874,"properly respected?
639"
LICENSES FOR EXISTING ASSETS,0.8911138923654568,"Answer: [Yes]
640"
LICENSES FOR EXISTING ASSETS,0.8923654568210263,"Justification: The original owners of all code, data, and models in our paper are properly
641"
LICENSES FOR EXISTING ASSETS,0.8936170212765957,"credited.
642"
LICENSES FOR EXISTING ASSETS,0.8948685857321652,"Guidelines:
643"
LICENSES FOR EXISTING ASSETS,0.8961201501877347,"• The answer NA means that the paper does not use existing assets.
644"
LICENSES FOR EXISTING ASSETS,0.8973717146433041,"• The authors should cite the original paper that produced the code package or dataset.
645"
LICENSES FOR EXISTING ASSETS,0.8986232790988736,"• The authors should state which version of the asset is used and, if possible, include a
646"
LICENSES FOR EXISTING ASSETS,0.899874843554443,"URL.
647"
LICENSES FOR EXISTING ASSETS,0.9011264080100125,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
648"
LICENSES FOR EXISTING ASSETS,0.9023779724655819,"• For scraped data from a particular source (e.g., website), the copyright and terms of
649"
LICENSES FOR EXISTING ASSETS,0.9036295369211514,"service of that source should be provided.
650"
LICENSES FOR EXISTING ASSETS,0.904881101376721,"• If assets are released, the license, copyright information, and terms of use in the
651"
LICENSES FOR EXISTING ASSETS,0.9061326658322904,"package should be provided. For popular datasets, paperswithcode.com/datasets
652"
LICENSES FOR EXISTING ASSETS,0.9073842302878599,"has curated licenses for some datasets. Their licensing guide can help determine the
653"
LICENSES FOR EXISTING ASSETS,0.9086357947434293,"license of a dataset.
654"
LICENSES FOR EXISTING ASSETS,0.9098873591989988,"• For existing datasets that are re-packaged, both the original license and the license of
655"
LICENSES FOR EXISTING ASSETS,0.9111389236545682,"the derived asset (if it has changed) should be provided.
656"
LICENSES FOR EXISTING ASSETS,0.9123904881101377,"• If this information is not available online, the authors are encouraged to reach out to
657"
LICENSES FOR EXISTING ASSETS,0.9136420525657072,"the asset’s creators.
658"
NEW ASSETS,0.9148936170212766,"13. New Assets
659"
NEW ASSETS,0.9161451814768461,"Question: Are new assets introduced in the paper well documented and is the documentation
660"
NEW ASSETS,0.9173967459324155,"provided alongside the assets?
661"
NEW ASSETS,0.918648310387985,"Answer: [NA]
662"
NEW ASSETS,0.9198998748435544,"Justification: There is no new assets attached to our paper. We will make our code and data
663"
NEW ASSETS,0.9211514392991239,"public once paper is accepted.
664"
NEW ASSETS,0.9224030037546934,"Guidelines:
665"
NEW ASSETS,0.9236545682102628,"• The answer NA means that the paper does not release new assets.
666"
NEW ASSETS,0.9249061326658323,"• Researchers should communicate the details of the dataset/code/model as part of their
667"
NEW ASSETS,0.9261576971214017,"submissions via structured templates. This includes details about training, license,
668"
NEW ASSETS,0.9274092615769712,"limitations, etc.
669"
NEW ASSETS,0.9286608260325406,"• The paper should discuss whether and how consent was obtained from people whose
670"
NEW ASSETS,0.9299123904881101,"asset is used.
671"
NEW ASSETS,0.9311639549436797,"• At submission time, remember to anonymize your assets (if applicable). You can either
672"
NEW ASSETS,0.932415519399249,"create an anonymized URL or include an anonymized zip file.
673"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9336670838548186,"14. Crowdsourcing and Research with Human Subjects
674"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.934918648310388,"Question: For crowdsourcing experiments and research with human subjects, does the paper
675"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9361702127659575,"include the full text of instructions given to participants and screenshots, if applicable, as
676"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9374217772215269,"well as details about compensation (if any)?
677"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9386733416770964,"Answer: [NA]
678"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9399249061326659,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
679"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9411764705882353,"Guidelines:
680"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9424280350438048,"• The answer NA means that the paper does not involve crowdsourcing nor research with
681"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9436795994993742,"human subjects.
682"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9449311639549437,"• Including this information in the supplemental material is fine, but if the main contribu-
683"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9461827284105131,"tion of the paper involves human subjects, then as much detail as possible should be
684"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9474342928660826,"included in the main paper.
685"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9486858573216521,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
686"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9499374217772215,"or other labor should be paid at least the minimum wage in the country of the data
687"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.951188986232791,"collector.
688"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9524405506883604,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
689"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9536921151439299,"Subjects
690"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9549436795994993,"Question: Does the paper describe potential risks incurred by study participants, whether
691"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9561952440550688,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
692"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9574468085106383,"approvals (or an equivalent approval/review based on the requirements of your country or
693"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9586983729662077,"institution) were obtained?
694"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9599499374217773,"Answer: [NA]
695"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9612015018773467,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
696"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9624530663329162,"Guidelines:
697"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9637046307884856,"• The answer NA means that the paper does not involve crowdsourcing nor research with
698"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9649561952440551,"human subjects.
699"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9662077596996246,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
700"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967459324155194,"may be required for any human subjects research. If you obtained IRB approval, you
701"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9687108886107635,"should clearly state this in the paper.
702"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9699624530663329,"• We recognize that the procedures for this may vary significantly between institutions
703"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712140175219024,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
704"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724655819774718,"guidelines for their institution.
705"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737171464330413,"• For initial submissions, do not include any information that would break anonymity (if
706"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749687108886108,"applicable), such as the institution conducting the review.
707"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762202753441802,"A
Appendix
708"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774718397997497,"A.1
Network details
709"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787234042553191,"The two PointNets used in our network to extract features from point cloud patches P and vectors V
710"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799749687108886,"consist of four ResNet blocks. In addition, the two fully connected layer modules in our framework
711"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981226533166458,"consist of three layers each. To ensure non-negativity of the UDF values output by the network, we
712"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824780976220275,"employ the softplus activation function.
713"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983729662077597,"A.2
Robustness to outliers
714"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9849812265331664,"Our method can reconstruct relatively accurate geometry from point clouds with 10% added outliers
715"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986232790988736,"and reasonably smooth surfaces from point clouds with even higher outlier ratios. Furthermore, our
716"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874843554443054,"approach can reconstruct high-quality geometry from point clouds containing both noise and outliers,
717"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887359198998749,"as shown in Fig. 9.
718"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9899874843554443,Figure 9: Our model demonstrates robustness to more outliers.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912390488110138,"A.3
More results
719"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924906132665833,"As shown in Fig. 10 and Fig. 11, we provide more visual comparisons on the DeepFashion3D and
720"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937421777221527,"ShapeNetCars dataset, using point clouds containing noise and outliers.
721"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949937421777222,"(a) Input
(b) CAP-UDF
(c) LevelSetUDF
(d) GeoUDF
(e) DUDF
(f) Ours
(g) GT"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9962453066332916,"Figure 10: More visual results on the DeepFashion3D dataset. Top three rows: Reconstruction results
under noise-free conditions. Bottom three rows: Reconstruction results under noise condition."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974968710888611,"(a) Input
(b) CAP-UDF
(c) LevelSetUDF
(d) GeoUDF
(e) DUDF
(f) Ours
(g) GT"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9987484355444305,Figure 11: More visual results on the synthetic datasets with outliers.
