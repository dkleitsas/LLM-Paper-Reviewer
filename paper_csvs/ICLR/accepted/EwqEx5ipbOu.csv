Section,Section Appearance Order,Paragraph
NATIONAL UNIVERSITY OF SINGAPORE,0.0,"1National University of Singapore
2ShanghaiTech University
3AARC, Huawei Technologies
4Huawei Noah’s Ark Lab"
ABSTRACT,0.0016051364365971107,ABSTRACT
ABSTRACT,0.0032102728731942215,"Prior works on self-supervised pre-training focus on the joint training scenario,
where massive unlabeled data are assumed to be given as input all at once, and
only then is a learner trained. Unfortunately, such a problem setting is often im-
practical if not infeasible since many real-world tasks rely on sequential learning,
e.g., data are decentralized or collected in a streaming fashion. In this paper,
we conduct the ﬁrst thorough and dedicated investigation on self-supervised pre-
training with streaming data, aiming to shed light on the model behavior under
this overlooked setup. Speciﬁcally, we pre-train over 500 models on four cate-
gories of pre-training streaming data from ImageNet and DomainNet and evaluate
them on three types of downstream tasks and 12 different downstream datasets.
Our studies show that, somehow beyond our expectation, with simple data replay
or parameter regularization, sequential self-supervised pre-training turns out to be
an efﬁcient alternative for joint pre-training, as the performances of the former
are mostly on par with those of the latter. Moreover, catastrophic forgetting, a
common issue in sequential supervised learning, is much alleviated in sequential
self-supervised learning (SSL), which is well justiﬁed through our comprehensive
empirical analysis on representations and the sharpness of minima in the loss land-
scape. Our ﬁndings, therefore, suggest that, in practice, for SSL, the cumbersome
joint training can be replaced mainly by sequential learning, which in turn enables
a much broader spectrum of potential application scenarios."
INTRODUCTION,0.004815409309791332,"1
INTRODUCTION"
INTRODUCTION,0.006420545746388443,"Recent advances in self-supervised learning (SSL) (He et al., 2020; Grill et al., 2020; Caron et al.,
2020; Jure et al., 2021) demonstrate competitive or even better transfer learning performance on
various downstream tasks, compared with supervised pre-training. Although waiving the cost of
human labeling, SSL usually requires massive unlabeled data to learn a powerful representation
model and beneﬁts from signiﬁcantly large-scale pre-training data, e.g., He et al. (2020) adopted
billion-scale data to pre-train better SSL models. The common pre-training practice follows the
joint training (JT) setup, where massive unlabeled data are collected together before model training.
In reality, however, it is usually difﬁcult to access a large amount of collective unlabeled data at
once. Instead, real-world data are usually accessed in a streaming fashion, e.g., data are generated
and collected sequentially chunk by chunk (Delange et al., 2021), or even decentralized and stored
in different servers (Lange et al., 2020); such a learning setup is known as sequential training
(ST). Despite much research effort and promising results achieved by JT, it inevitably suffers from
heavy data storage, prolonged training time, and ﬁnds itself incompetent when training data volume
expands over time. For ST, on the other hand, a learner can be sequentially trained with disjoint data
chunks, making it much more efﬁcient than JT."
INTRODUCTION,0.008025682182985553,"How to effectively and efﬁciently pre-train a representation model under the ST setup has been an
open problem. Despite the high efﬁciency, some continual learning research works (Goodfellow
et al., 2013; Kirkpatrick et al., 2017; Rebufﬁet al., 2017) have shown that ST with supervised mod-
els tends to suffer from catastrophic forgetting (McCloskey & Cohen, 1989), having a signiﬁcant"
INTRODUCTION,0.009630818619582664,∗Contributed equally: dapeng.hu@u.nus.edu and yanshp@shanghaitech.edu.cn.
INTRODUCTION,0.011235955056179775,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012841091492776886,"performance degradation on the historical data chunks. Unlike the case of continual learning tasks,
in pre-training tasks, one expects the model to well generalize to downstream tasks rather than fo-
cusing only on the seen tasks (Chen et al., 2020a). Nevertheless, how well sequential self-supervised
models perform on downstream tasks remains unclear."
INTRODUCTION,0.014446227929373997,"Chunk 2
Chunk 1
Chunk 3
Chunk 4"
INTRODUCTION,0.016051364365971106,Streaming data …
INTRODUCTION,0.01765650080256822,"Class 2
Class 7
Class 8
Class 1
…
Class semantic tree"
INTRODUCTION,0.019261637239165328,Domains
INTRODUCTION,0.02086677367576244,"Instance
incremental sequence"
INTRODUCTION,0.02247191011235955,"Random class 
incremental sequence"
INTRODUCTION,0.024077046548956663,"Distant class 
incremental sequence"
INTRODUCTION,0.025682182985553772,"Domain 
incremental sequence"
INTRODUCTION,0.027287319422150885,Collective data
INTRODUCTION,0.028892455858747994,"1
2
4
3"
INTRODUCTION,0.030497592295345103,Parent Child
INTRODUCTION,0.03210272873194221,"Figure 1: Illustration of streaming data and the corresponding collective data. Different colors
denote different classes, and border types distinguish different domains.
We use the WordNet
Tree (Miller, 1998) to measure the semantic similarity of classes. Classes having the same parent or
ancestor in WordNet, marked with similar colors, share similar semantics in the class semantic tree."
INTRODUCTION,0.033707865168539325,"To ﬁll the research gap, we provide a thorough empirical study on self-supervised pre-training with
streaming data. In the pre-training stage, to mimic real-world data collection scenarios and for the
better dissection of sequential SSL, we consider streaming data with different degrees of distribution
shifts. As shown in Figure 1, we obtain four types of streaming data, including the instance incre-
mental sequence with negligible data distribution shifts, by randomly splitting ImageNet-1K (Rus-
sakovsky et al., 2015) into four independent and identically distributed (IID) data chunks, the random
class incremental sequence with moderate data distribution shifts, by randomly splitting 1K classes
of images into four disjoint chunks each with 250 classes, the distant class incremental sequence
with severe data distribution shifts, by splitting 1K classes of data into four chunks while maximiz-
ing the semantical dissimilarity among chunks, and the domain incremental sequence with severe
domain distribution shifts, by taking each domain in DomainNet (Peng et al., 2019) as a data chunk."
INTRODUCTION,0.03531300160513644,"As for the evaluation, we consider three downstream tasks following Ericsson et al. (2021), includ-
ing few-shot evaluation and linear evaluation (also named many-shot classiﬁcation) on 12 image
classiﬁcation datasets (Kornblith et al., 2019b), and the Pascal VOC (Everingham et al., 2010) de-
tection task. Through extensive experiments with more than 500 pre-trained models, we thoroughly
investigate key roles in sequential SSL, including streaming data, downstream tasks and datasets,
continual learning methods, SSL methods, and the method efﬁciency in terms of time and storage.
We also thoroughly investigate the knowledge forgetting behavior of sequential SSL and supervised
learning (SL) models and provide a comprehensive empirical analysis of the underlying reason."
INTRODUCTION,0.03691813804173355,"To the best of our knowledge, we are among the ﬁrst to explore the sequential self-supervised pre-
training setting and the ﬁrst to provide a thorough empirical study on self-supervised pre-training
with streaming data. We summarize the takeaways as well as our contributions as: i). Sequential SSL
models exhibit the on-par transfer learning performance as joint SSL models on streaming data with
negligible or mild distribution shifts. As for streaming data with severe distribution shifts or longer
sequences, i.e., the distant class incremental sequence, evident performance gaps exist between
sequential SSL and joint SSL models. Such performance gaps, however, can be mitigated effectively
and efﬁciently with unsupervised parameter regularization (Aljundi et al., 2018) and simple data
replay. ii). Based on the above ﬁnding, we conclude that the standard joint training paradigm may
be unnecessary for SSL pre-training. Instead, sequential SSL is performance-competitive but more
time-efﬁcient and storage-saving and is well worth considering as the practical practice for self-
supervised pre-training with streaming data. iii). Compared with supervised learning (SL) models,
SSL models consistently show smaller performance gaps between ST and JT. Our comprehensive
investigation of learned representations demonstrates that sequential SSL models are less prone to"
INTRODUCTION,0.038523274478330656,Published as a conference paper at ICLR 2022
INTRODUCTION,0.04012841091492777,"catastrophic forgetting than SL models. iv). Through the empirical analysis on the sharpness of
minima in the loss landscape, we ﬁnd that SSL models have wider minima than SL models, which
we hypothesize is the reason for less forgetting of SSL models."
RELATED WORK,0.04173354735152488,"2
RELATED WORK"
RELATED WORK,0.04333868378812199,"Self-supervised learning (SSL). SSL learns useful features by solving various pretext tasks using
supervisions generated from unlabeled training data, e.g., predicting rotations (Gidaris et al., 2018),
predicting cluster assignments (Caron et al., 2018), and solving instance discrimination (Wu et al.,
2018; Chen et al., 2020a; He et al., 2020; Grill et al., 2020). To achieve better performance in
the downstream task, recent studies of SSL have made efforts in either upstream pre-training or
downstream transfer (Zhang et al., 2021). Previous works (Caron et al., 2019; He et al., 2020) have
leveraged especially large datasets for pre-training, such as YFCC 100M (Thomee et al., 2016) and
Instagram 1B (Mahajan et al., 2018). Some recent works (Gururangan et al., 2020; Reed et al.,
2021) propose to pre-train with the downstream dataset for a better transfer. Our work still focuses
on the downstream-agnostic model pre-training. However, in realistic scenarios, access to massive
data is often streaming, and how to perform SSL with streaming data has not been studied before,
motivating our work."
RELATED WORK,0.0449438202247191,"Continual learning. Existing studies of continual learning (CL) (Delange et al., 2021) mainly
focus on supervised tasks and can be summarized into three categories, including regularization,
replay, and parameter-isolation. In regularization-based CL, knowledge preserving is achieved by
regularizing the parameter posterior of the new task not to deviate drastically from the prior (Aljundi
et al., 2018; Kirkpatrick et al., 2017; Zenke et al., 2017). Replay-based CL methods overcome
forgetting by saving samples of previous tasks in a replay buffer (Rebufﬁet al., 2017; Rolnick et al.,
2019; Wang et al., 2021; Yan et al., 2021b) and using them to regularize the learning of new tasks.
Last, isolation-based CL methods leverage different parameters for learning each task to preserve
the learned knowledge (Serra et al., 2018; Mallya & Lazebnik, 2018). Although works (Rao et al.,
2019; Aljundi et al., 2019) explore continual learning for some speciﬁc unsupervised tasks, few have
studied the transfer learning performance of sequential self-supervised models."
PROBLEM SETTING,0.04654895666131621,"3
PROBLEM SETTING"
PROBLEM SETTING,0.048154093097913325,"In pre-training tasks, we train representation models on large-scale datasets, such as ImageNet (Rus-
sakovsky et al., 2015), and evaluate the transferability of learned representations on various down-
stream tasks (Chen et al., 2020a). In our empirical study, we adopt the prevailing MoCo-v2 (Chen
et al., 2020c) method to pre-train SSL models with diverse streaming data."
PROBLEM SETTING,0.04975922953451043,"Types of streaming data. In pre-training, we consider streaming data with various distribution
shifts to mimic practical data collection scenarios. As shown in Figure 1, each type of streaming
data consists of sequential and disjoint data chunks, while collective data cover all available data. In
the instance incremental sequence, streaming data chunks are almost IID, which simulates the sce-
nario where data are continually collected under the same condition. In this case, there is negligible
distribution shift among sequential data chunks. In the random class incremental sequence, data in
disjoint chunks belong to different classes, which mimics the scenario where data are collected by
random keyword search on the Internet (Parisi et al., 2019). Here the distribution shift is moderate.
The distant class incremental sequence is similar to the random class incremental sequence except
that the semantic gaps between sequential data chunks in the distant sequence are larger, i.e., images
from different data chunks are semantically dissimilar. This data sequence has severe distribution
shifts between chunks. It mimics the scenario where data are crawled from websites with different
subjects. In the domain incremental sequence, data chunks are collected from different domains
with severe domain distribution shifts. A typical example is that large-scale autonomous driving
data in Han et al. (2021) are collected in different domains, such as different weather conditions
and cities, but share similar classes. The ﬁrst three types of streaming data are designed with Ima-
geNet (Russakovsky et al., 2015), while the domain incremental sequence consists of ﬁve domains
in DomainNet (Peng et al., 2019). See Appendix A.1 for a detailed description."
PROBLEM SETTING,0.051364365971107544,"Model pre-training. With these streaming data, we study both sequential training (ST) and joint
training (JT) for model pre-training. As illustrated in Figure 1, in sequential training, a model is"
PROBLEM SETTING,0.052969502407704656,Published as a conference paper at ICLR 2022
PROBLEM SETTING,0.05457463884430177,"sequentially trained with streaming data chunks, while in joint training, a model is repeatedly trained
with collective data, i.e., all seen data chunks. Moreover, we compare SSL with supervised learning
(SL) and mainly study the following pre-trained models: sequentially trained SSL models (SSL-ST),
jointly trained SSL models (SSL-JT), sequentially trained SL models (SL-ST), and jointly trained
SL models (SL-JT). See Appendix A.2 for details of the pre-training stage. 30 32 34 36 38 40 42"
PROBLEM SETTING,0.056179775280898875,Aircrafts 78 80 82 84 86 88 90
PROBLEM SETTING,0.05778491171749599,Caltech 78 80 82 84 86 88
PROBLEM SETTING,0.0593900481540931,Flowers 72 75 78 81 84 87 90 Pets 24 26 28 30 32 34 36
CARS,0.060995184590690206,"38
Cars 60 62 64 66 68 70 72 DTD"
CARS,0.06260032102728733,"1
2
3
4 54 56 58 60 62 64 66 Food"
CARS,0.06420545746388442,"1
2
3
4
84 85 86 87 88 89 90 91"
CARS,0.06581059390048154,CIFAR10
CARS,0.06741573033707865,"1
2
3
4 64 66 68 70 72"
CARS,0.06902086677367576,CIFAR100
CARS,0.07062600321027288,"1
2
3
4
24
27
30
33
36
39
42
45
48 Birds"
CARS,0.07223113964686999,"1
2
3
4 46 48 50 52 54 56 58"
CARS,0.0738362760834671,Sun397
CARS,0.0754414125200642,"1
2
3
4 74 76 78 80 82 VOC07"
CARS,0.07704654895666131,Many-shot
CARS,0.07865168539325842,"1
2
3
4
# of chunks 48 52 56 60 64 68 72 76 80"
CARS,0.08025682182985554,Average Accuracy (%)
CARS,0.08186195826645265,"SSL-ST
SSL-JT"
CARS,0.08346709470304976,"SL-ST
SL-JT 38 40 42 44 46 48 50"
CARS,0.08507223113964688,Aircrafts 88 90 92 94 96
CARS,0.08667736757624397,Caltech 86 87 88 89 90 91
CARS,0.08828250401284109,Flowers 78 81 84 87 90 93 96 Pets
CARS,0.0898876404494382,"45
48
51
54
57
60
63
66
69
72 Cars 72 73 74 75 76 77"
DTD,0.09149277688603531,"78
DTD"
DTD,0.09309791332263243,"1
2
3
4
60 62 64 66 68 70 72 74 Food"
DTD,0.09470304975922954,"1
2
3
4
62
64
66
68
70
72
74
76
78
80"
DTD,0.09630818619582665,CIFAR10
DTD,0.09791332263242375,"1
2
3
4
68
70
72
74
76
78
80
82
84"
DTD,0.09951845906902086,CIFAR100
DTD,0.10112359550561797,"1
2
3
4
57
60
63
66
69
72
75
78
81
84 Birds"
DTD,0.10272873194221509,"1
2
3
4 87 88 89 90 91 92 93"
DTD,0.1043338683788122,Sun397
DTD,0.10593900481540931,"SSL-ST
SSL-JT
SL-ST
SL-JT"
DTD,0.10754414125200643,Few-shot
DTD,0.10914927768860354,"1
2
3
4
# of chunks 60 63 66 69 72 75 78 81 84 87 90"
DTD,0.11075441412520064,Average Accuracy (%)
DTD,0.11235955056179775,"SSL-ST
SSL-JT"
DTD,0.11396468699839486,"SL-ST
SL-JT"
DTD,0.11556982343499198,"Figure 2: Linear and few-shot evaluation results of random class incremental sequence. On the
left are the results of each dataset. On the right are averaged results across all left datasets."
DTD,0.11717495987158909,"Transfer to downstream tasks. We evaluate the transfer learning performance of pre-trained mod-
els using three typical downstream tasks: many-shot classiﬁcation, few-shot classiﬁcation, and ob-
ject detection. Following Chen et al. (2020a), we consider 12 classiﬁcation datasets for downstream
evaluation. Speciﬁcally, we conduct many-shot classiﬁcation on all the above 12 datasets but con-
duct few-shot classiﬁcation on 11 datasets except VOC2007, following Ericsson et al. (2021). In
both types of classiﬁcation tasks, representations are ﬁxed for evaluation. In addition, we eval-
uate pre-trained models on the PASCAL VOC detection dataset following He et al. (2020). See
Appendix A.4 for more details of the downstream evaluation."
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1187800963081862,"4
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.12038523274478331,"We pre-train representation models on four types of streaming data and evaluate pre-trained models
on 12 downstream datasets with three downstream evaluation tasks. Note that models pre-trained
with ImageNet-based streaming data are evaluated on all three downstream tasks. Models trained
with the domain incremental sequence are only evaluated with few-shot classiﬁcation, considering
the size of DomainNet is only 1/5 ImageNet. We report downstream evaluation results of the ran-
dom class incremental sequence, the distant class incremental sequence, and the domain incremen-
tal sequence in Figure 2, Figure 3, and Figure 4, respectively. Results of the instance incremental
sequence are illustrated in Appendix B.1 since the performance of SSL models on the instance in-
cremental sequence is similar to that on the random class incremental sequence without considering
the different cases for SL models. We also evaluate three types of ImageNet-based streaming data
on object detection and illustrate results in Figure 10 in Appendix."
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.12199036918138041,"4.1
HOW DOES TRANSFER LEARNING PERFORMANCE VARY WITH STREAMING DATA?"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.12359550561797752,"We ﬁrst consider streaming data with various degrees of distribution shifts, i.e., streaming data with
negligible distribution shifts such as the instance incremental sequence, streaming data with mod-
erate distribution shifts such as the random class incremental sequence, and streaming data with
severe distribution shifts such as the distant class incremental sequence and the domain incremental
sequence. As shown in Figures 2-4, on all types of streaming data, the performance of sequential
SSL models generally increases with more streaming chunks, while sequential SL models do not"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.12520064205457465,Published as a conference paper at ICLR 2022
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.12680577849117175,"28
30
32
34
36
38
40
42"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.12841091492776885,Aircrafts
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.13001605136436598,"72
74
76
78
80
82
84
86
88
90"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.13162118780096307,Caltech 76 78 80 82 84 86 88
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1332263242375602,Flowers
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1348314606741573,"56
60
64
68
72
76
80
84
88
92 Pets"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.13643659711075443,"22
24
26
28
30
32
34
36
38 Cars 58 60 62 64 66 68 70 72 DTD"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.13804173354735153,"1
2
3
4
50
52
54
56
58
60
62
64
66 Food"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.13964686998394862,"1
2
3
4 82 84 86 88 90"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.14125200642054575,CIFAR10
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.14285714285714285,"1
2
3
4
58 60 62 64 66 68 70 72"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.14446227929373998,CIFAR100
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.14606741573033707,"1
2
3
4
21
24
27
30
33
36
39
42
45
48 Birds"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1476725521669342,"1
2
3
4"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1492776886035313,"42
44
46
48
50
52
54
56
58"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1508828250401284,Sun397
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.15248796147672553,"1
2
3
4
66
68
70
72
74
76
78
80
82 VOC07"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.15409309791332262,Many-shot
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.15569823434991975,"1
2
3
4
# of chunks 48 52 56 60 64 68 72 76 80"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.15730337078651685,Average Accuracy (%)
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.15890850722311398,"SSL-ST
SSL-JT
SL-ST"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.16051364365971107,"SL-JT
SSL-ST w/MAS
SSL-ST w/MAS+ 38 40 42 44 46 48 50"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.16211878009630817,Aircrafts 84 86 88 90 92 94 96
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1637239165329053,Caltech 84 85 86 87 88 89 90 91
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1653290529695024,Flowers
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.16693418940609953,"64
68
72
76
80
84
88
92
96 Pets 48 52 56 60 64 68 72 Cars 68 70 72 74 76 78 DTD"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.16853932584269662,"1
2
3
4"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.17014446227929375,"58
60
62
64
66
68
70
72
74 Food"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.17174959871589085,"1
2
3
4 60 63 66 69 72 75 78"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.17335473515248795,"81
CIFAR10"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.17495987158908508,"1
2
3
4
66 69 72 75 78 81 84"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.17656500802568217,CIFAR100
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1781701444622793,"1
2
3
4 56 60 64 68 72 76 80 84 Birds"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1797752808988764,"1
2
3
4"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.18138041733547353,"85
86
87
88
89
90
91
92
93"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.18298555377207062,Sun397
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.18459069020866772,"SSL-ST
SSL-JT
SL-ST
SL-JT
SSL-ST w/MAS
SSL-ST w/MAS+"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.18619582664526485,Few-shot
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.18780096308186195,"1
2
3
4
# of chunks 60 63 66 69 72 75 78 81 84 87 90"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.18940609951845908,Average Accuracy (%)
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.19101123595505617,"SSL-ST
SSL-JT
SL-ST"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1926163723916533,"SL-JT
SSL-ST w/MAS
SSL-ST w/MAS+"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1942215088282504,"Figure 3: Linear and few-shot evaluation results of distant class incremental sequence. On the left
are the results of each dataset. On the right are averaged results across all left datasets."
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.1958266452648475,"beneﬁt from increasing data chunks. As for the performance on each type of streaming data, se-
quential SSL models surprisingly perform comparably to joint SSL models on streaming data with
negligible and moderate distribution shifts. On streaming data with severe distribution shifts, the
performance of sequential SSL models is evidently inferior to that of joint SSL models. The de-
tection evaluation results in Figure 10 in Appendix further support this observation. In addition,
we provide results of streaming data with longer chunks (8) and random distribution shifts in Ap-
pendix B.2. Similarly, we ﬁnd the long sequence leads to visible but not signiﬁcant gaps between
ST and JT models. In contrast, on all types of streaming data, sequential SL models perform espe-
cially worse than joint SL models. The above observations denote that, unlike traditional continual
learning tasks (Delange et al., 2021), although faced with possible visible performance gaps between
ST models and JT models, sequential SSL is still performance-promising in pre-training tasks with
streaming data."
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.19743178170144463,"4.2
ARE RESULTS CONSISTENT ACROSS DOWNSTREAM TASKS OR DATASETS?"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.19903691813804172,"In pre-training tasks, we pay attention to the generalization of the learned representations to new
data or tasks rather than the performance on the training dataset. Taking a closer look at the re-
sults in Figures 2-4, we observe that, although joint SSL models achieve comparable performance
to joint SL models in linear evaluation, joint SL models signiﬁcantly outperform joint SSL mod-
els in few-shot evaluation. This observation is also demonstrated in (Tian et al., 2020; Ericsson
et al., 2021). The main difference between the two evaluation protocols is that linear evaluation
involves more ﬁne-tuning than few-shot evaluation, as introduced in Appendix A.4. Therefore, the
underlying reason for the observation is that supervised features are correlated with labels and more
discriminative, thus easy to directly transfer to downstream datasets similar to upstream pre-training
data (DomainNet or ImageNet). For example, SL models dominate most few-shot object or scene
classiﬁcation tasks but fail on DTD (Cimpoi et al., 2014), a texture classiﬁcation dataset sharing no
common classes with ImageNet or DomainNet. In contrast, self-supervised features are more gen-
eralized and comprehensive, thus requiring more ﬁne-tuning for desirable downstream transfer. In
addition, on some downstream datasets, we have seemingly abnormal observations that ST models
may outperform JT models and the model performance may drop with the increase of chunk num-
ber. These phenomena are due to the so-called “negative transfer” (Wang et al., 2019), which is also
discussed in other model pre-training studies (Newell & Deng, 2020; Gururangan et al., 2020). That
is, pre-training with more data chunks does not necessarily beneﬁt a speciﬁc downstream dataset if
the added training data are irrelevant to the downstream dataset. See Appendix B.3 for a concrete
example of “negative transfer” on Oxford-IIIT Pets (Parkhi et al., 2012) in pre-training with stream-
ing data. It is observed that sequential SSL models suffer less “negative transfer” than SL models
and continual learning methods largely prevent “negative transfer”."
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.20064205457463885,Published as a conference paper at ICLR 2022
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.20224719101123595,"27
30
33
36
39
42
45
Aircrafts"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.20385232744783308,"42
48
54
60
66
72
78
84
90"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.20545746388443017,Caltech
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.20706260032102727,"45
50
55
60
65
70
75
80
85
90
Flowers"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.2086677367576244,"28
32
36
40
44
48
52
56
60
64
Pets"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.2102728731942215,"27
30
33
36
39
42
45
48
51 Cars"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.21187800963081863,"30
35
40
45
50
55
60
65
70 DTD"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.21348314606741572,"1
2
3
4
5
28
32
36
40
44
48
52
56 Food"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.21508828250401285,"1
2
3
4
5
30
33
36
39
42
45
48
51
54"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.21669341894060995,CIFAR10
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.21829855537720708,"1
2
3
4
5"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.21990369181380418,"36
40
44
48
52
56
60
64
68
CIFAR100"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.22150882825040127,"1
2
3
4
5"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.2231139646869984,"33
36
39
42
45
48
51
54
57
Birds"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.2247191011235955,"1
2
3
4
5
40
45
50
55
60
65
70
75
80
85"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.22632423756019263,Sun397
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.22792937399678972,"SSL-ST
SSL-JT
SSL-ST w/Replay
SL-ST
SL-JT"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.22953451043338685,Few-shot
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.23113964686998395,"1
2
3
4
5
# of chunks 30 36 42 48 54 60 66 72 78 84"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.23274478330658105,Average Accuracy (%)
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.23434991974317818,"SSL-ST
SSL-JT
SSL -ST w/Replay
SL-ST
SL-JT"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.23595505617977527,"Figure 4: Few-shot evaluation results of domain incremental sequence. On the left are the results
of each dataset. On the right are averaged results across all left datasets."
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.2375601926163724,"4.3
DO CONTINUAL LEARNING METHODS HELP SEQUENTIAL SSL?"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.2391653290529695,"As observed in Section 4.1, there still exist obvious performance gaps between sequential SSL mod-
els and joint SSL models, on streaming data with severe distribution shifts such as the distant class
incremental sequence and the domain incremental sequence. To this end, we study whether contin-
ual learning methods can help mitigate such gaps. Speciﬁcally, we investigate two classic methods
in continual learning, i.e., data replay and MAS (Aljundi et al., 2018), which are effective to defy
knowledge forgetting in supervised classiﬁcation tasks (Delange et al., 2021). When using data re-
play, we randomly reserve 10% data from each seen data chunk and add them to the current data
chunk for model pre-training. We also consider the combination of MAS and data replay, which is
referred to as MAS+ in the experiments. We denote SSL models trained with data replay as SSL-ST
w/Replay, SSL models trained with MAS as SSL-ST w/MAS, and SSL models trained with both
methods as SSL-ST w/MAS+. We report downstream evaluation results of the distant class incre-
mental sequence in Figure 3 and results of the domain incremental sequence in Figure 4. As shown
in Figure 4, data replay can totally eliminate the performance gaps between sequential SSL models
and joint SSL models on the domain incremental sequence. Results in Figure 3 also validate the
effectiveness of both continual learning methods in improving the transfer learning performance of
sequential SSL models trained with streaming data with severe distribution shifts. In short, we ﬁnd
methods devised for supervised continual tasks are especially promising to make sequential SSL
models perform comparably to joint SSL models on challenging streaming data. See Appendix A.3
for implementations of MAS and data replay in sequential SSL."
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.24077046548956663,"4.4
HOW ABOUT SSL METHODS OTHER THAN MOCO?"
DISSECTION OF SEQUENTIAL SELF-SUPERVISED PRE-TRAINING,0.24237560192616373,"For simplicity, we choose MoCo-v2 (Chen et al., 2020c) in experiments and demonstrate that se-
quential SSL is performance-promising. To verify whether it also holds for other SSL methods,
we train BYOL (Grill et al., 2020) models on the challenging distant class incremental sequence,
both sequentially and jointly. Results of BYOL are shown in Figure 13 in Appendix B.4. Similar
to MoCo-v2, there still exist visible performance gaps between sequential SSL models and corre-
sponding joint SSL models. In contrast, SSL models exhibit much smaller performance gaps than
SL models, which further validates the potential of sequential SSL in pre-training tasks."
ANALYSIS OF METHOD EFFICIENCY,0.24398073836276082,"4.5
ANALYSIS OF METHOD EFFICIENCY"
ANALYSIS OF METHOD EFFICIENCY,0.24558587479935795,"We then discuss the time and memory consumption of different training methods of SSL, including
sequential training (SSL-ST), ST with data replay (SSL-ST w/Replay), ST with MAS (SSL-ST
w/MAS), ST with MAS and data replay (SSL-ST w/MAS+), and joint training (SSL-JT). As shown
in Table 1, JT is very time-consuming especially when the data amount is large, while ST is able
to save a large amount of time under sequential training scenarios. To be speciﬁc, ST is about
2x faster than JT when there are 2 chunks of data, and is about 4x faster when the number of
chunks is 4. Moreover, when we use MAS and data replay to improve the performance of ST, the
time consumption of SSL increases a little but is still signiﬁcantly faster than JT. As for storage
consumption, we can observe a similar phenomenon as shown in Table 1. In summary, sequential
SSL is much more time-efﬁcient and storage-saving than JT, especially when the data amount is
large or grows quickly. Such a result indicates that sequential SSL is a more favorable choice for
real-world pre-training applications, where data come in sequentially and grow daily."
ANALYSIS OF METHOD EFFICIENCY,0.24719101123595505,Published as a conference paper at ICLR 2022
ANALYSIS OF METHOD EFFICIENCY,0.24879614767255218,"Table 1: Resource efﬁciency of considered SSL
pre-training methods. We take the distant class
incremental sequence as an example and report
the training time (h) and required storage (GB)
of the model pre-trained with each data chunk.
Note that all the following statistics are recorded
under the same hardware environment.
The
lower value means better efﬁciency."
ANALYSIS OF METHOD EFFICIENCY,0.2504012841091493,"Time (Storage) / Chunk
2
3
4"
ANALYSIS OF METHOD EFFICIENCY,0.2520064205457464,"SSL-ST
16.5 (35)
16.5 (35)
16.6 (35)
SSL-ST W/Replay
17.0 (35)
18.5 (42)
20.0 (46)
SSL-ST w/MAS
18.2 (35)
18.1 (35)
18.1 (35)
SSL-ST w/MAS+
22.4 (39)
24.4 (42)
26.4 (46)
SSL-JT
31.1 (70)
46.5 (105)
66.6 (140)"
ANALYSIS OF METHOD EFFICIENCY,0.2536115569823435,"Table 2: The comparison of pre-training meth-
ods in terms of the transfer performance gap be-
tween ST and JT models. We report the aver-
aged accuracy gaps of linear evaluation across
12 downstream datasets. The lower, the better."
ANALYSIS OF METHOD EFFICIENCY,0.2552166934189406,"Accuracy gap (%) / Chunk
2
3
4"
ANALYSIS OF METHOD EFFICIENCY,0.2568218298555377,"SL-ST (Instance)
2.26
3.27
4.83
SSL-ST (Instance)
0.41
1.02
1.04"
ANALYSIS OF METHOD EFFICIENCY,0.25842696629213485,"SL-ST (Random)
5.63
8.73
10.68
SSL-ST (Random)
0.42
0.94
1.13"
ANALYSIS OF METHOD EFFICIENCY,0.26003210272873195,"SL-ST (Distant)
7.77
12.50
15.75
SSL-ST (Distant)
2.34
3.81
4.62
SSL-ST w/MAS (Distant)
1.82
2.73
3.17
SSL-ST w/MAS+ (Distant)
1.47
2.01
2.10"
ANALYSIS OF METHOD EFFICIENCY,0.26163723916532905,"Summary. We show the averaged accuracy gaps between ST models and the corresponding JT
models under linear evaluation in Table 2, for both SSL and supervised learning (SL). On streaming
data with negligible distribution shifts, SL exhibits evident accuracy gaps while SSL has negligible
gaps. On streaming data with moderate distribution shifts, SL exhibits larger accuracy gaps while
SSL still keeps the negligible gaps. On streaming data with severe distribution shifts, SL shows much
larger accuracy gaps, while SSL shows mild accuracy gaps. But such accuracy gaps of SSL can be
effectively mitigated with simple continual learning methods. To sum up, SSL exhibits signiﬁcantly
smaller performance gaps between ST models and JT models than SL. The above difference between
SL and SSL models motivates us to further investigate the forgetting property in Section 5."
SELF-SUPERVISED MODELS FORGET LESS THAN SUPERVISED MODELS,0.26324237560192615,"5
SELF-SUPERVISED MODELS FORGET LESS THAN SUPERVISED MODELS"
SELF-SUPERVISED MODELS FORGET LESS THAN SUPERVISED MODELS,0.26484751203852325,"In this section we ﬁrst analyze the knowledge forgetting of previous tasks from two perspectives.
In Section 5.1, we evaluate the transfer ability of both SL and SSL representations via the standard
backward and forward transfer analysis in continual learning (Lopez-Paz & Ranzato, 2017). In
Section 5.2, we adopt the CKA similarity (Kornblith et al., 2019a) and image reconstruction from
features (Zhao et al., 2020) to directly analyze the representation. Last but not least, in Section 5.3,
we provide our empirically justiﬁed hypothesis for why SSL models forget less than SL models."
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.2664526484751204,"5.1
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING"
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.2680577849117175,"Table 3: Backward and forward transfer analy-
sis of sequential learning."
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.2696629213483146,"Data
Method
BWT(%)
FWT(%)
Top-1
Top-5
Top-1
Top-5"
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.2712680577849117,"Instance
SL
-9.45
-5.46
8.64
2.81
SSL
3.61
3.60
7.55
8.63"
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.27287319422150885,"Random
SL
-20.63
-7.03
-0.34
0.01
SSL
-5.17
-1.36
11.05
4.52"
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.27447833065810595,"Distant
SL
-40.43
-28.66
4.90
0.47
SSL
-13.24
-11.06
11.01
3.66"
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.27608346709470305,"Following (Lopez-Paz & Ranzato, 2017), we
adopt the backward and forward transfer to assess
the knowledge transfer in sequential learning.
Backward transfer refers to the improvement of
performance on previously learned chunks when
learning new chunks, where large negative trans-
fer is also known as catastrophic forgetting. For-
ward transfer measures the improvement in per-
formance on the novel chunk with the accumu-
lation of knowledge from previous chunks. For
supervised continual learning, the performance
is deﬁned as the accuracy on the associated test set, which is meaningful due to the consis-
tency of the training and test sets. Similarly, we conduct the whole analysis based on the per-
formance on the pre-training data chunks, instead of performing the evaluation on downstream
datasets. Speciﬁcally, for the ease of comparison between SL and SSL, we measure the perfor-
mance by KNN classiﬁcation accuracy on the representations of pre-training chunks, where the
labels are provided just for evaluation, similar to Wu et al. (2018). Concretely, the backward transfer
BWT =
1
T −1
PT
i=2
1"
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.27768860353130015,"i
Pi
j=1 Ai
Yj −Aj
Yj and forward transfer FWT =
1
T −1
PT
i=2 Ai
Yi−˜AYj metrics
used in Yan et al. (2021a) where T is the sequence length, Ai
Yj refers to the accuracy on the chunk j
using model learned at step i where the label space includes all observed classes up to chunk j, and
˜AYj means the accuracy with the model learned from scratch. The results are shown in Table 3, and"
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.27929373996789725,Published as a conference paper at ICLR 2022
BACKWARD AND FORWARD TRANSFER ANALYSIS OF SEQUENTIAL LEARNING,0.2808988764044944,"the implementation details are included in the Appendix C.2. We can obtain the following observa-
tions about forgetting: i). Learning method: SSL itself is less prone to catastrophic forgetting than
SL, especially that SSL achieves positive backward transfer on the instance incremental sequence.
It illustrates that SSL is more suitable for streaming data. ii). Types of streaming data: The model
suffers progressively severe forgetting when the distribution shift increases for both SSL and SL
cases. iii). Example forgetting: It is observed that forgetting is less severe in top-5 classiﬁcation
than top-1 classiﬁcation, which indicates that the knowledge is not fully forgotten."
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.2825040128410915,"5.2
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.2841091492776886,"1
2
3
4"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.2857142857142857,Chunk 1
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.28731942215088285,Chunk 2
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.28892455858747995,Chunk 3
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.29052969502407705,Chunk 4
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.29213483146067415,"1
0.78 0.76 0.75"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.29373996789727125,"0.78
1
0.78 0.77"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.2953451043338684,"0.76 0.78
1
0.78"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.2969502407704655,"0.75 0.77 0.78
1"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.2985553772070626,SL Instance
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3001605136436597,"1
2
3
4"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3017656500802568,"1
0.9
0.82 0.75"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.30337078651685395,"0.9
1
0.92 0.86"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.30497592295345105,"0.82 0.92
1
0.92"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.30658105939004815,"0.75 0.86 0.92
1"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.30818619582664525,SSL Instance 0.0 0.2 0.4 0.6 0.8 1.0
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3097913322632424,"1
2
3
4"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3113964686998395,Chunk 1
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3130016051364366,Chunk 2
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3146067415730337,Chunk 3
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3162118780096308,Chunk 4
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.31781701444622795,"1
0.73 0.72 0.72"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.31942215088282505,"0.73
1
0.76 0.76"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.32102728731942215,"0.72 0.76
1
0.77"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.32263242375601925,"0.72 0.76 0.77
1"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.32423756019261635,SL Random Class
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3258426966292135,"1
2
3
4"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3274478330658106,"1
0.82 0.79
0.8"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3290529695024077,"0.82
1
0.89
0.9"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3306581059390048,"0.79 0.89
1
0.88"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.33226324237560195,"0.8
0.9
0.88
1"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.33386837881219905,SSL Random Class 0.0 0.2 0.4 0.6 0.8 1.0
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.33547351524879615,"1
2
3
4"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.33707865168539325,Chunk 1
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.33868378812199035,Chunk 2
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3402889245585875,Chunk 3
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3418940609951846,Chunk 4
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3434991974317817,"1
0.38 0.37 0.41"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3451043338683788,"0.38
1
0.69 0.61"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3467094703049759,"0.37 0.69
1
0.61"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.34831460674157305,"0.41 0.61 0.61
1"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.34991974317817015,SL Distant Class
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.35152487961476725,"1
2
3
4"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.35313001605136435,"1
0.68 0.59 0.65"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3547351524879615,"0.68
1
0.85 0.79"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3563402889245586,"0.59 0.85
1
0.75"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3579454253611557,"0.65 0.79 0.75
1"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3595505617977528,SSL Distant Class 0.0 0.2 0.4 0.6 0.8 1.0
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3611556982343499,"Figure 5: CKA scores between sequen-
tially trained models."
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.36276083467094705,"How do features forget in sequential training?
We
study how learned features forget in sequential training
via Centered Kernel Alignment (CKA) (Kornblith et al.,
2019a). CKA is used to measure the similarity between
two representations of the same given samples. See Ap-
pendix C.3 for details of the CKA similarity. Speciﬁ-
cally, we randomly sample 50,000 images from the ﬁrst
data chunk on each type of streaming data. We use these
samples and sequentially trained models for CKA sim-
ilarity analysis. We report the CKA similarity values on
three types of ImageNet-based streaming data in Figure 5.
Each value is obtained by ﬁrst extracting features of sam-
ples with two different models and then computing the
CKA feature similarity value between the two features.
On all streaming data, we have three consistent observa-
tions about the CKA similarity between sequential mod-
els: i). SSL models all exhibit higher features similarity
to the initial model, compared with SL models. ii). In
general, SSL models show higher features similarity be-
tween two sequential models in sequential training, com-
pared with SL models. iii). Features similarity between
two sequential models decrease on streaming data with
more severe distribution shifts, for both SSL and SSL. These observations suggest that features of
SSL models forget less and evolve more slowly than those of SL models in sequential training."
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.36436597110754415,"Image reconstruction by feature inversion for sequential models. Similar to Zhao et al. (2020), in
Figure 6, we visualize images reconstructed from both SL-ST and SSL-ST features using deep image
prior (DIP) (Ulyanov et al., 2018). To be speciﬁc, we choose four images in the ﬁrst data chunk of
the challenging distant class incremental sequence and visualize features of four sequentially learned
models for both SSL and SL, respectively. As shown in Figure 6, in sequential training, features of
SSL models can always perfectly reconstruct the main information in original images. In contrast,
features of SL models lose more detailed information with more sequential data chunks, which
indicates SSL is much better at countering the knowledge forgetting in sequential training. Recalling
the evolving CKA similarity shown in Figure 5, the perfect reconstruction results of sequential SSL
models do not mean SSL models stop learning in sequential training. Instead, it indicates that SSL
does well in learning new knowledge while keeping previous knowledge."
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.36597110754414125,"Input Image
Chunk 1
Chunk 2
Chunk 3
Chunk 4
Chunk 1
Chunk 2
Chunk 3
Chunk 4"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.36757624398073835,"Raw image
Images reconstructed from SSL models
Images reconstructed from SL models"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.36918138041733545,"Figure 6: Images reconstruction by inversing features from both SSL and SL models in sequential
pre-training."
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3707865168539326,Published as a conference paper at ICLR 2022
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3723916532905297,Table 4: Comparisons of the sharpness of minima between SL and SSL models. Lower is better.
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3739967897271268,"Instance
Random Class
Distant Class
ϵ = 0.1
ϵ = 0.3
ϵ = 0.1
ϵ = 0.3
ϵ = 0.1
ϵ = 0.3"
REPRESENTATION MEMORIZATION ANALYSIS OF SEQUENTIAL LEARNING,0.3756019261637239,"SL
0.47
0.94
0.21
0.94
0.19
0.94
SSL
0.14
0.68
0.08
0.66
0.12
0.71"
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL,0.37720706260032105,"5.3
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL"
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL,0.37881219903691815,"In this subsection, we dig into the different forgetting behavior between SL and SSL by analyzing
the sharpness of the minima in the loss landscape. Flat minima in the loss landscape are the minima
in which the change of losses is slow in its neighborhood. Note that the models having ﬂat min-
ima in the loss landscape tend to exhibit an impressive generalization ability (Keskar et al., 2016).
When starting with ﬂat minima, we expect that learning new chunks will have a minor effect on
the performance of existing chunks, as escaping the wide basin is difﬁcult. Therefore, we hypothe-
size that SSL encourages the model to seek out ﬂatter minima, which increases SSL’s resistance to
catastrophic forgetting. To verify this hypothesis, we conduct experiments to compare the sharpness
of minima between SL and SSL models, where we apply a widely-used sharpness metric (Keskar
et al., 2016; Wen et al., 2018). Concretely, we ﬁrst deﬁne the neighborhood Cϵ of mimina as:"
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL,0.38041733547351525,"Cϵ = {z ∈Rn : −ϵ||θ||2 ≤||z||2 ≤ϵ||θ||2},
(1)"
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL,0.38202247191011235,"where n denotes the number of parameters and θ refers to the model parameter after training. Be-
cause SSL and SL models are trained with different loss objectives, such as cross-entropy loss and
the contrastive loss, we cannot directly analyze the sharpness with either objective. Considering
that we aim for a representation model, we propose to directly adopt the KNN classiﬁer to evaluate
representations of both SL and SSL models. The KNN classiﬁcation loss can be a discrete proxy of
loss functions. Then, the sharpness of loss minima Φθ,f is deﬁned as follows:"
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL,0.38362760834670945,"Φθ,f(ϵ) = max
θ′∈Cϵ g(θ′, θ) = max
θ′∈Cϵ
f(θ) −f(θ′)"
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL,0.3852327447833066,"f(θ)
,
(2)"
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL,0.3868378812199037,"where g(θ′, θ) means the relative loss change from minima θ to the parameter θ′, and the loss
function f(θ) is the negative KNN classiﬁcation accuracy with model parameter θ."
HYPOTHESIS FOR DIFFERENT FORGETTING BEHAVIORS BETWEEN SL AND SSL,0.3884430176565008,"As shown in Table 4, SSL indeed discovers ﬂatter minima compared to SL, which veriﬁes our
hypothesis and provides an explanation for why SSL suffers less forgetting than SL. More imple-
mentation details are in Appendix C.4. Moreover, we also conduct the visualization of relative loss
change g over a linear path like (Mirzadeh et al., 2020) in Appendix C.4. The loss change of SSL is
slower than that of SL along the linear interpolation path, demonstrating the ﬂatter minima of SSL."
DISCUSSIONS,0.3900481540930979,"6
DISCUSSIONS"
DISCUSSIONS,0.391653290529695,"This paper has conducted the ﬁrst thorough empirical evaluation to investigate how well self-
supervised learning (SSL) performs with various streaming data types and diverse downstream
tasks. Our experimental results and the empirical analysis conclude the three main ﬁndings: i).
Joint training is unnecessary for SSL with streaming data. Instead, sequential training with suitable
continual learning strategies is performance-competitive yet more efﬁcient, well worth considering
as a good alternative. ii). Sequential self-supervised pre-training shows a better capability of over-
coming catastrophic forgetting than sequential supervised pre-training. iii). We hypothesize that
SSL models have ﬂatter minima than SL models in the loss landscape, which seems reasonable for
the different forgetting behaviors between SL and SSL models. Moreover, We demonstrate this
hypothesis by a thorough empirical analysis of the sharpness of minima."
DISCUSSIONS,0.39325842696629215,"As for future directions, we ﬁrst call for more attention to sequential self-supervised learning for
understanding its underlying theories of knowledge forgetting and devising better approaches. Also,
we recommend considering sequential self-supervised training as a more efﬁcient representation
learning paradigm for real-world applications."
DISCUSSIONS,0.39486356340288925,Published as a conference paper at ICLR 2022
DISCUSSIONS,0.39646869983948635,ACKNOWLEDGMENTS
DISCUSSIONS,0.39807383627608345,"This work is supported by NUS ARTIC Project (Project Reference: ECT-RP2) and NRF Centre for
Advanced Robotics Technology Innovation (CARTIN). Dr. Lanqing Hong and Prof. Xinchao Wang
are corresponding authors. We thank Mr. Jiawei Du for his help with the sharpness metric and Mr.
Yujun Shi for his discussion on the feature uniformity. We also thank the anonymous reviewers for
their valuable comments."
REFERENCES,0.3996789727126806,REFERENCES
REFERENCES,0.4012841091492777,"Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. In European Conference on Computer
Vision, 2018."
REFERENCES,0.4028892455858748,"Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In IEEE
Conference on Computer Vision and Pattern Recognition, 2019."
REFERENCES,0.4044943820224719,"Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N
Belhumeur. Birdsnap: Large-scale ﬁne-grained visual categorization of birds. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2014."
REFERENCES,0.406099518459069,"Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative com-
ponents with random forests. In European Conference on Computer Vision, 2014."
REFERENCES,0.40770465489566615,"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In European Conference on Computer Vision, 2018."
REFERENCES,0.40930979133226325,"Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training
of image features on non-curated data. In IEEE International Conference on Computer Vision,
2019."
REFERENCES,0.41091492776886035,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Un-
supervised learning of visual features by contrasting cluster assignments. In Advances in Neural
Information Processing Systems, 2020."
REFERENCES,0.41252006420545745,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
2020a."
REFERENCES,0.41412520064205455,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton.
Big
self-supervised models are strong semi-supervised learners. In Advances in Neural Information
Processing Systems, 2020b."
REFERENCES,0.4157303370786517,"Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020c."
REFERENCES,0.4173354735152488,"Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-
scribing textures in the wild. In IEEE Conference on Computer Vision and Pattern Recognition,
2014."
REFERENCES,0.4189406099518459,"Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021."
REFERENCES,0.420545746388443,"Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models
transfer? In IEEE Conference on Computer Vision and Pattern Recognition, 2021."
REFERENCES,0.42215088282504015,"Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The PASCAL visual object classes (VOC) challenge. International Journal of Computer Vision,
2010."
REFERENCES,0.42375601926163725,"Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training
examples: An incremental bayesian approach tested on 101 object categories. In IEEE Conference
on Computer Vision and Pattern Recognition Workshop, 2004."
REFERENCES,0.42536115569823435,Published as a conference paper at ICLR 2022
REFERENCES,0.42696629213483145,"Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. In International Conference on Learning Representations, 2018."
REFERENCES,0.42857142857142855,"Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio.
An empiri-
cal investigation of catastrophic forgetting in gradient-based neural networks.
arXiv preprint
arXiv:1312.6211, 2013."
REFERENCES,0.4301765650080257,"Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In Advances
in Neural Information Processing Systems, 2020."
REFERENCES,0.4317817014446228,"Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A Smith. Don’t stop pretraining: adapt language models to domains and tasks. In
Annual Meeting of the Association for Computational Linguistics, 2020."
REFERENCES,0.4333868378812199,"Jianhua Han, Xiwen Liang, Hang Xu, Kai Chen, Lanqing Hong, Chaoqiang Ye, Wei Zhang, Zhen-
guo Li, Chunjing Xu, and Xiaodan Liang.
SODA10M: Towards large-scale object detection
benchmark for autonomous driving. arXiv preprint arXiv:2108.12178, 2021."
REFERENCES,0.434991974317817,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016."
REFERENCES,0.43659711075441415,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern
Recognition, 2020."
REFERENCES,0.43820224719101125,"Zbontar Jure, Jing Li, Misra Ishan, LeCun Yann, and Deny Stephane. Barlow twins: Self-supervised
learning via redundancy reduction. In International Conference on Machine Learning, 2021."
REFERENCES,0.43980738362760835,"Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
International Conference on Learning Representations, 2016."
REFERENCES,0.44141252006420545,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences,
2017."
REFERENCES,0.44301765650080255,"Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In International Conference on Machine Learning, 2019a."
REFERENCES,0.4446227929373997,"Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In
IEEE Conference on Computer Vision and Pattern Recognition, 2019b."
REFERENCES,0.4462279293739968,"Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of ﬁne-
grained cars. In Workshop on Fine-Grained Visual Categorization, 2013."
REFERENCES,0.4478330658105939,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Master’s thesis, University of Tront, 2009."
REFERENCES,0.449438202247191,"Matthias De Lange, Xu Jia, Sarah Parisot, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.
Unsupervised model personalization while preserving privacy and scalability: An open problem.
In IEEE Conference on Computer Vision and Pattern Recognition, 2020."
REFERENCES,0.4510433386837881,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.45264847512038525,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,
Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised
pretraining. In European Conference on Computer Vision, 2018."
REFERENCES,0.45425361155698235,"Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained
visual classiﬁcation of aircraft. arXiv preprint arXiv:1306.5151, 2013."
REFERENCES,0.45585874799357945,Published as a conference paper at ICLR 2022
REFERENCES,0.45746388443017655,"Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning. In IEEE Conference on Computer Vision and Pattern Recognition, 2018."
REFERENCES,0.4590690208667737,"Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of Learning and Motivation. Elsevier, 1989."
REFERENCES,0.4606741573033708,"George A Miller. WordNet: An Electronic Lexical Database. MIT press, 1998."
REFERENCES,0.4622792937399679,"Seyed Iman Mirzadeh,
Mehrdad Farajtabar,
Dilan Gorur,
Razvan Pascanu,
and Hassan
Ghasemzadeh. Linear mode connectivity in multitask and continual learning. In International
Conference on Learning Representations, 2020."
REFERENCES,0.463884430176565,"Alejandro Newell and Jia Deng. How useful is self-supervised pretraining for visual tasks? In IEEE
Conference on Computer Vision and Pattern Recognition, 2020."
REFERENCES,0.4654895666131621,"Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number
of classes. In Indian Conference on Computer Vision, Graphics & Image Processing, 2008."
REFERENCES,0.46709470304975925,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.46869983948635635,"German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 2019."
REFERENCES,0.47030497592295345,"Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In IEEE
Conference on Computer Vision and Pattern Recognition, 2012."
REFERENCES,0.47191011235955055,"Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In IEEE International Conference on Computer Vision, 2019."
REFERENCES,0.47351524879614765,"Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell.
Continual unsupervised representation learning. In Advances in Neural Information Processing
Systems, 2019."
REFERENCES,0.4751203852327448,"Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classiﬁer and representation learning. In IEEE Conference on Computer Vision and
Pattern Recognition, 2017."
REFERENCES,0.4767255216693419,"Colorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna Ebrahimi, Vivek Vijaykumar, Richard Mao,
Bo Li, Shanghang Zhang, Devin Guillory, Sean Metzger, et al. Self-supervised pretraining im-
proves self-supervised pretraining. In IEEE International Conference on Computer Vision, 2021."
REFERENCES,0.478330658105939,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object
detection with region proposal networks. In Advances in Neural Information Processing Systems,
2015."
REFERENCES,0.4799357945425361,"Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for
the masses. In International Conference on Learning Representations, 2021."
REFERENCES,0.48154093097913325,"David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience
replay for continual learning. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.48314606741573035,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, 2015."
REFERENCES,0.48475120385232745,"Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In International Conference on Machine Learning,
2018."
REFERENCES,0.48635634028892455,"Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland,
Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. Communica-
tions of the ACM, 2016."
REFERENCES,0.48796147672552165,Published as a conference paper at ICLR 2022
REFERENCES,0.4895666131621188,"Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking
few-shot image classiﬁcation: a good embedding is all you need? In European Conference on
Computer Vision, 2020."
REFERENCES,0.4911717495987159,"Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In IEEE Conference on
Computer Vision and Pattern Recognition, 2018."
REFERENCES,0.492776886035313,"Liyuan Wang, Kuo Yang, Chongxuan Li, Lanqing Hong, Zhenguo Li, and Jun Zhu. Ordisco: Effec-
tive and efﬁcient usage of incremental unlabeled data for semi-supervised continual learning. In
IEEE Conference on Computer Vision and Pattern Recognition, 2021."
REFERENCES,0.4943820224719101,"Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In International Conference on Machine Learning,
2020."
REFERENCES,0.4959871589085072,"Zirui Wang, Zihang Dai, Barnab´as P´oczos, and Jaime Carbonell. Characterizing and avoiding neg-
ative transfer. In IEEE Conference on Computer Vision and Pattern Recognition, 2019."
REFERENCES,0.49759229534510435,"Wei Wen, Yandan Wang, Feng Yan, Cong Xu, Chunpeng Wu, Yiran Chen, and Hai Li. Smoothout:
Smoothing out sharp minima to improve generalization in deep learning.
arXiv preprint
arXiv:1805.07898, 2018."
REFERENCES,0.49919743178170145,"Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In IEEE Conference on Computer Vision and Pattern Recog-
nition, 2018."
REFERENCES,0.5008025682182986,"Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:
Large-scale scene recognition from abbey to zoo. In IEEE Conference on Computer Vision and
Pattern Recognition, 2010."
REFERENCES,0.5024077046548957,"Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class
incremental learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2021a."
REFERENCES,0.5040128410914928,"Shipeng Yan, Jiale Zhou, Jiangwei Xie, Songyang Zhang, and Xuming He. An em framework for
online incremental learning of semantic segmentation. In Proceedings of the 29th ACM Interna-
tional Conference on Multimedia, 2021b."
REFERENCES,0.5056179775280899,"Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in Neural Information Processing Systems, 2014."
REFERENCES,0.507223113964687,"Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
Proceedings of Machine Learning Research, 2017."
REFERENCES,0.5088282504012841,"Yifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, and Jiashi Feng. Unleashing the power of con-
trastive self-supervised visual models via contrast-regularized ﬁne-tuning. In Advances in Neural
Information Processing Systems, 2021."
REFERENCES,0.5104333868378812,"Nanxuan Zhao, Zhirong Wu, Rynson WH Lau, and Stephen Lin. What makes instance discrim-
ination good for transfer learning?
In International Conference on Learning Representations,
2020."
REFERENCES,0.5120385232744783,Published as a conference paper at ICLR 2022
REFERENCES,0.5136436597110754,APPENDIX
REFERENCES,0.5152487961476726,"A
EXPERIMENTAL SETUPS"
REFERENCES,0.5168539325842697,"A.1
TYPES OF STREAMING DATA"
REFERENCES,0.5184590690208668,"We consider four kinds of streaming data for the study, i.e., the instance incremental sequence,
the random class incremental sequence, the distant class incremental sequence, and the domain
incremental sequence. To exclude the effect of the number of images, we make sure that data chunks
in the same sequence have almost the same data amount. Here we provide more details about these
data sequences."
REFERENCES,0.5200642054574639,"Instance incremental sequence.
For the instance incremental sequence, we split the Ima-
geNet (Russakovsky et al., 2015) training data that consists of 1.28 million images with 1,000 classes
into four even chunks. We ensure that each data chunk includes the same 1,000 classes with the same
number of images for each class, which means these data chunks are independent and identically
distributed (IID)."
REFERENCES,0.521669341894061,"Random class incremental sequence. For the random class incremental sequence, we randomly
split the 1,000 classes of ImageNet into four parts where each part has 250 classes. Since each class
of ImageNet has around 1,000 images, we can directly obtain four data chunks with almost the same
amount of images."
REFERENCES,0.5232744783306581,"0
200
400
Connected component 0 25 50 75 100 125 150 175 200 225"
REFERENCES,0.5248796147672552,# of classes
REFERENCES,0.5264847512038523,Distant class
REFERENCES,0.5280898876404494,"Figure 7: The number of classes
for
each
connected
component
from the adjacent matrix of 1,000
classes in ImageNet."
REFERENCES,0.5296950240770465,"Distant class incremental sequence.
To explore the data
sequence with severe distribution shifts among data chunks,
we consider the distant class incremental sequence. Follow-
ing (Yosinski et al., 2014), rather than randomly splitting the
1,000 classes, we leverage the WordNet Tree (Miller, 1998)
to obtain four even data chunks sharing the minimal seman-
tic overlapping. We ﬁrst build a 1000*1000 adjacent matrix
among the 1,000 classes by setting the value of similar classes
as 1 and the value of dissimilar classes as 0. To be speciﬁc, we
take classes sharing the common parent node beneath the ninth
depth in the WordNet Tree as similar classes and vice versa.
Using the semantic similarity described in the adjacent ma-
trix, we then split the 1,000 classes into independent connected
components as shown in Figure 7. Finally, we merge these
imbalanced components into four almost even data chunks.
Concretely, the ﬁrst chunk ‘A’ includes 250 classes of 318,459
images. The second data chunk ‘B’ includes 250 classes of
321,488 images. The third data chunk ‘C’ includes 251 classes
of 321,533 images. The fourth data chunk ‘D’ includes 249
classes of 319,687 images."
REFERENCES,0.5313001605136437,"Domain incremental sequence. As for the domain incremental sequence, we consider a multi-
domain dataset called DomainNet (Peng et al., 2019). In our work, we adopt a domain incremental
data sequence made of four distant domains including ‘sketch’, ‘real’, ‘painting’ ‘quickdraw’, and
‘clipart’. There exist severe domain distribution shifts among data in these ﬁve domains. Speciﬁ-
cally, data in the domain ‘quickdraw’ mostly contain only lines without visual textures. As a result,
images from ‘quickdraw’ are less informative and more visually distinct, compared with images
from those four domains, as shown in Figure 8. For each domain, we randomly select 48,129 im-
ages as a data chunk, except for ‘quickdraw’ where we select 47,687 images."
REFERENCES,0.5329052969502408,"A.2
DETAILS OF PRE-TRAINING"
REFERENCES,0.5345104333868379,"MoCo-v2.
For the illustration purpose, we adopt a prevailing self-supervised learning (SSL)
method, MoCo-v2 (Chen et al., 2020c), to investigate the performance of SSL with streaming data.
MoCo-v2 uses a Siamese network consisting of two encoders. These two encoders are designed for
query images and key images, respectively, and share the same architecture where an MLP projec-
tion head fw is on top of a backbone network fθ. Only the query encoder is updated by the gradients"
REFERENCES,0.536115569823435,Published as a conference paper at ICLR 2022
REFERENCES,0.5377207062600321,Figure 8: Example images in the ﬁve domains of DomainNet.
REFERENCES,0.5393258426966292,"backpropagation while the key encoder is updated by the moving average with a momentum. MoCo-
v2 maintains an additional dictionary as a queue of features for contrastive learning. Speciﬁcally,
features in the dictionary are progressively updated. The current mini-chunk features from the key
encoder are enqueued and the same number of oldest features are dequeued. MoCo-v2 uses In-
foNCE (Oord et al., 2018), a variant of contrastive loss (CL), to maximize the similarity of features
from positive pairs and minimize the similarity of features from negative pairs. The contrastive loss
is formalized as below."
REFERENCES,0.5409309791332263,"Lcl= −1 N N
X"
REFERENCES,0.5425361155698234,"i=1
log
e(z⊤
i z+
i /τ)"
REFERENCES,0.5441412520064205,"e(z⊤
i z+
i /τ) + P"
REFERENCES,0.5457463884430177,"z−
i ∈Z−e(z⊤
i z−
i /τ) ,
(3)"
REFERENCES,0.5473515248796148,"where N is the number of samples, zi is the L2-normalized projected feature from the query encoder,
z+
i is the L2-normalized projected feature of the same input image from the key encoder, Z−are
the negative history features stored in the dictionary and τ is the temperature."
REFERENCES,0.5489566613162119,"Pre-training. For self-supervised pre-training, we follow the protocol of MoCo-v2 (Chen et al.,
2020c), i.e., using the standard ResNet50 backbone (He et al., 2016). The implementation is based
on OpenSelfSup1. For both joint training and sequential training, the number of training epochs
is 200 for each model training, where the convergence of loss is observed, as shown in Figure 14.
As for the queue in MoCo-v2 training, we adopt the same as the original MoCo-v2, i.e., the queue
size is 65,536. For data chunks fewer than 65,536 like DomainNet chunks, we store all samples
of the current chunk for contrastive learning. In sequential training, the queue is refreshed before
training on a new chunk. We never preserve previous data in the queue, to ensure the model training
is sequentially conducted on disjoint chunks. For the possibly used data replay strategy, another
special replay buffer is used to preserve a small fraction of data from each previous chunk, like 10%
in our experiments. For the instance incremental sequence, we consider one random sequence as the
data are randomly divided. While for the random class incremental sequence, distant class incre-
mental sequence, and domain incremental sequence, we experiment with different sequences of data
chunks. In particular, considered sequences are obtained through right circular shift operations. For
example, if the data sequence length is 4, after splitting all the data into four chunks A, B, C, and
D, four sequences, namely A-B-C-D, B-C-D-A, C-D-A-B, and D-A-B-C are used for the sequential
pre-training a representation model. The results from different sequences are averaged to obtain the
ﬁnal performance. For comparison, supervised pre-training is also implemented using OpenSelf-
Sup following the recommended training protocol of ImageNet. For supervised pre-training, the
classiﬁer layer is reset at a new data chunk."
REFERENCES,0.550561797752809,"A.3
DETAILS OF CONTINUAL LEARNING METHODS"
REFERENCES,0.5521669341894061,"Continual learning is assumed to suffer from catastrophic forgetting of previously learned knowledge
in supervised learning (Goodfellow et al., 2013; Kirkpatrick et al., 2017; McCloskey & Cohen,
1989), leading to signiﬁcant performance degradation of previous tasks. Here we introduce the
continual learning techniques we adopt, including data replay (Rolnick et al., 2019; Rebufﬁet al.,
2017) and regularization-based method, e.g., Memory Aware Synapses (MAS) (Aljundi et al., 2018)."
REFERENCES,0.5537720706260032,"Data replay. Data relay is a simple yet effective method for alleviating the catastrophic forgetting
problem during the continual learning process. Speciﬁcally, we need to maintain a replay buffer"
REFERENCES,0.5553772070626003,1https://github.com/open-mmlab/OpenSelfSup
REFERENCES,0.5569823434991974,Published as a conference paper at ICLR 2022
REFERENCES,0.5585874799357945,"and store a selected subset of samples from each learned task in the buffer. Then we just retrain on
samples in the replay buffer to revisit old tasks while training the model for a new task."
REFERENCES,0.5601926163723917,"To perform data replay in the sequential training process, we maintain a replay buffer consisting of
images sampled from previous data chunks. After ﬁnishing the sequential training with each data
chunk, we randomly select 10% data of this chunk and store sampled data in the replay buffer. For
the sequential training with the current data chunk, we directly mix current data with data in the
replay buffer for self-supervised pre-training e.g. MoCo-v2 training using Eq. (3)."
REFERENCES,0.5617977528089888,"MAS. Regularization-based methods aim to mitigate the catastrophic forgetting by consolidating
previous knowledge with an added regularization term in the loss function. One typical unsupervised
regularization-based method is MAS (Aljundi et al., 2018). Speciﬁcally, MAS proposes to compute
gradients the squared L2-norm of the encoder output fθ as the importance weights of parameters."
REFERENCES,0.5634028892455859,"Ωij = 1 N N
X k=1"
REFERENCES,0.565008025682183,"∂[∥fθ(x)∥2
2)]
∂θij
.
(4)"
REFERENCES,0.5666131621187801,"With the parameter regularization term added, the resulting loss function with the coefﬁcient λ is
shown as below.
L(θ) = Lcl(θ) + λ
X"
REFERENCES,0.5682182985553772,"i,j
Ωij(θij −θ∗
ij)2.
(5)"
REFERENCES,0.5698234349919743,"In MoCo-v2, the query encoder is considered as the representation network, we thus only impose
the MAS regularization term on parameters of the query encoder. Speciﬁcally, the regularization
coefﬁcient λ is ﬁxed to be 100. Following the prevailing use of MAS regularization (Aljundi et al.,
2018; 2019), we update the MAS importance weights Ωij to cover information of each data chunk in
the sequential training process. To be speciﬁc, after ﬁnishing the sequential training with each data
chunk, we leverage both the current data chunk and data in the replay buffer to estimate importance
weights for the trained model using Eq. (4). Then we update the stored sequential importance
weights by a cumulative moving average of current and previous estimated importance weights,
following (Aljundi et al., 2019). As for the model training on the current data chunk, we apply the
parameter regularization using previous importance weights and optimize the model using Eq. (5)."
REFERENCES,0.5714285714285714,"To sum up, besides the sequentially trained model, both above methods require extra storage for
sequential self-supervised pre-training. For the 10% data replay method, we need to only keep
10% data of each previous data chunk for sequential training. For the MAS regularization method,
we only require to save a set of importance weights for the model and then update the importance
weights sequentially."
REFERENCES,0.5730337078651685,"A.4
DETAILS OF DOWNSTREAM TASKS"
REFERENCES,0.5746388443017657,"We evaluate the transfer performance of the pre-trained models using three different downstream
tasks. Following (Chen et al., 2020a), we consider 12 diverse image classiﬁcation datasets including
Food-101 (Bossard et al., 2014), CIFAR10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al.,
2009), Birdsnap (Berg et al., 2014), SUN397 (Xiao et al., 2010), Standard Cars (Krause et al.,
2013), FGVC Aircraft (Maji et al., 2013), VOC2007 (Everingham et al., 2010), DTD (Cimpoi et al.,
2014), Oxford-IIIT Pets (Parkhi et al., 2012), Caltech-101 (Fei-Fei et al., 2004) and Oxford 102
Flowers (Nilsback & Zisserman, 2008). On these datasets, we evaluate the pre-trained models via
the many-shot classiﬁcation and the few-shot classiﬁcation (except VOC2007). Both classiﬁcation
protocols are the same as (Ericsson et al., 2021). In addition, we evaluate the pre-trained models on
the PASCAL VOC detection task, following the same transfer protocol of MoCo (He et al., 2020).
The training data of detection come from VOC2007 and VOC2012, and the test data come from
VOC2007."
REFERENCES,0.5762439807383628,"Many-shot classiﬁcation. Many-shot classiﬁcation is a widely used evaluation protocol (Chen
et al., 2020b; He et al., 2020). To evaluate the pre-trained representations, a linear classiﬁer is
directly added to the pre-trained feature encoder. During the downstream task evaluation, only the
added linear classiﬁer is ﬁne-tuned using a substantial amount of downstream labeled data while the
feature encoder is frozen. In this way, the downstream transfer performance can directly reﬂect the
generalization ability of the pre-trained representation models."
REFERENCES,0.5778491171749599,Published as a conference paper at ICLR 2022
REFERENCES,0.579454253611557,"Few-shot classiﬁcation. Few-shot classiﬁcation reﬂects how well the pre-trained models perform
on downstream tasks in the few-shot learning regime. Speciﬁcally, we consider 5-way 5-shot few-
shot tasks on 11 downstream classiﬁcation datasets, following the few-shot setting in (Ericsson et al.,
2021). Concretely, the pre-trained model is ﬁxed for extracting representations. In contrast to many-
shot evaluation, in few-shot evaluation, only a few downstream labeled data are provided to obtain
prototypes for different categories and then the classiﬁcation is based on the nearest prototype."
REFERENCES,0.5810593900481541,"Detection. To further evaluate the transferability of the pre-trained models on more downstream
scenarios, we consider object detection as a downstream task, where the ﬁne-grained spatial location
information is more important, compared with classiﬁcation tasks. To be speciﬁc, we follow the
settings in (He et al., 2020), i.e., adopting the Faster-RCNN (Ren et al., 2015) with a backbone of
R50-dilated-C5 and ﬁne-tuning all layers including the pre-trained representation network."
REFERENCES,0.5826645264847512,"We perform no hyper-parameter tuning for few-shot evaluation and detection evaluation. As for the
linear evaluation protocol, we adopt the logistic regression and only tune the weight decay value.
The inversed weight decay values for all downstream classiﬁcation datasets are given in Table 5."
REFERENCES,0.5842696629213483,"Table 5: The inverse of regularization strength (weight decay value) used in many-shot logistic re-
gression evaluation on 12 different downstream classiﬁcation datasets. SSL models: self-supervised
models. SL models: supervised models."
REFERENCES,0.5858747993579454,"Dataset
SSL Models
SL Models"
REFERENCES,0.5874799357945425,"Aircraft
5623.413277133687
9.99999985098839
Caltech-101
316227.7712565657
0.3162277621819913
Flowers
31622.77530666721
999.999952502551
Pets
999.999952502551
562.3413185099295
Cars
5623.413277133687
17.782794106882072
DTD
1778.2794843157246
0.0177827946252197
Food
177827.94843157247
0.0562341298247638
CIFAR10
316227.7712565657
0.0562341298247638
CIFAR100
100.00000223517424
0.0562341298247638
Birdsnap
1778.27948431572
0.1
SUN397
100.00000223517424
0.0177827946252197
VOC2007
9.99999985098839
0.005623413223739"
REFERENCES,0.5890850722311396,"B
MORE EXPERIMENTAL RESULTS"
REFERENCES,0.5906902086677368,"B.1
RESULTS OF INSTANCE INCREMENTAL SEQUENCE"
REFERENCES,0.5922953451043339,"Transfer learning results of self-supervised pre-training with the instance incremental sequence are
evaluated on all three downstream tasks. For results of both many-shot classiﬁcation and few-shot
classiﬁcation in Figure 9, we ﬁnd sequential SSL performs comparably with joint SSL on all down-
stream datasets, with the average performance gap between sequential training and joint training
less than 1%, while there exists evident gaps, more than 4%, between sequential supervised learning
and joint supervised learning."
REFERENCES,0.593900481540931,"B.2
RESULTS OF LONGER SEQUENCES"
REFERENCES,0.5955056179775281,"We also conduct experiments on a more realistic data sequence with longer chunks and random
distribution shifts. Concretely, except the 1000 classes in ILSVRC 2012, we also randomly sample
1000 classes from the remaining classes in ImageNet-21K (Ridnik et al., 2021) to build ImageNet-
2K dataset with 2.29M images. Concretely, we randomly split all samples in the ImageNet-2K
dataset into 8 chunks with 0.57M images per chunk. Figure 11 summarizes the results on 8-chunk
sequence. Besides the similar observations in Section 4.2, we have the following observations for
the longer sequence: i). With more sequential chunks, the performance gaps between ST and JT
models become larger for both SL and SSL. ii). The performance gaps of SSL are visible but not
signiﬁcant, compared with those of SL. On average, at all steps in sequential training, the gaps
of SSL are much smaller than those of SL. iii). The simple data replay with 10% previous data is
effective in mitigating the performance gaps of SSL for the longer sequence. To make the simple data"
REFERENCES,0.5971107544141252,Published as a conference paper at ICLR 2022 32 34 36 38 40
AIRCRAFTS,0.5987158908507223,"42
Aircrafts 78 80 82 84 86 88 90"
AIRCRAFTS,0.6003210272873194,Caltech 85 86 87 88 89
AIRCRAFTS,0.6019261637239165,Flowers 72 75 78 81 84 87 90 Pets 31 32 33 34 35 36 37 38 Cars 65 66 67 68 69 70 71 72 DTD
AIRCRAFTS,0.6035313001605136,"1
2
3
4
59
60
61
62
63
64
65
66
67
Food"
AIRCRAFTS,0.6051364365971108,"1
2
3
4
86 87 88 89 90 91"
AIRCRAFTS,0.6067415730337079,CIFAR10
AIRCRAFTS,0.608346709470305,"1
2
3
4"
AIRCRAFTS,0.6099518459069021,"65
66
67
68
69
70
71
72
73"
AIRCRAFTS,0.6115569823434992,CIFAR100
AIRCRAFTS,0.6131621187800963,"1
2
3
4"
AIRCRAFTS,0.6147672552166934,"24
27
30
33
36
39
42
45
48 Birds"
AIRCRAFTS,0.6163723916532905,"1
2
3
4
51 52 53 54 55 56 57 58"
AIRCRAFTS,0.6179775280898876,Sun397
AIRCRAFTS,0.6195826645264848,"1
2
3
4
74
75
76
77
78
79
80
81
82 VOC07"
AIRCRAFTS,0.6211878009630819,Many-shot
AIRCRAFTS,0.622792937399679,"1
2
3
4
# of chunks 48 52 56 60 64 68 72 76 80"
AIRCRAFTS,0.6243980738362761,Average Accuracy (%)
AIRCRAFTS,0.6260032102728732,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6276083467094703,"SL-ST
SL-JT"
AIRCRAFTS,0.6292134831460674,"36
38
40
42
44
46
48
50
52
54"
AIRCRAFTS,0.6308186195826645,Aircrafts 88 90 92 94 96
AIRCRAFTS,0.6324237560192616,Caltech 88 89 90 91 92
AIRCRAFTS,0.6340288924558587,Flowers 78 81 84 87 90 93 96 Pets 44 48 52 56 60 64 68 72 Cars 72 73 74 75 76 77 78 DTD
AIRCRAFTS,0.6356340288924559,"1
2
3
4"
AIRCRAFTS,0.637239165329053,"60
62
64
66
68
70
72
74 Food"
AIRCRAFTS,0.6388443017656501,"1
2
3
4"
AIRCRAFTS,0.6404494382022472,"64
66
68
70
72
74
76
78
80"
AIRCRAFTS,0.6420545746388443,CIFAR10
AIRCRAFTS,0.6436597110754414,"1
2
3
4"
AIRCRAFTS,0.6452648475120385,"70
72
74
76
78
80
82
84"
AIRCRAFTS,0.6468699839486356,CIFAR100
AIRCRAFTS,0.6484751203852327,"1
2
3
4
56 60 64 68 72 76 80 84 Birds"
AIRCRAFTS,0.6500802568218299,"1
2
3
4 87 88 89 90 91 92 93"
AIRCRAFTS,0.651685393258427,Sun397
AIRCRAFTS,0.6532905296950241,"SSL-ST
SSL-JT
SL-ST
SL-JT"
AIRCRAFTS,0.6548956661316212,Few-shot
AIRCRAFTS,0.6565008025682183,"1
2
3
4
# of chunks 60 63 66 69 72 75 78 81 84 87 90"
AIRCRAFTS,0.6581059390048154,Average Accuracy (%)
AIRCRAFTS,0.6597110754414125,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6613162118780096,"SL-ST
SL-JT"
AIRCRAFTS,0.6629213483146067,"Figure 9: Linear and few-shot evaluation results of instance incremental sequence. on the left are
the results of each dataset. On the right are averaged results across all left datasets."
AIRCRAFTS,0.6645264847512039,"1
2
3
4
# of chunks 53 57 61 AP"
AIRCRAFTS,0.666131621187801,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6677367576243981,"1
2
3
4
# of chunks 78 81 84 AP50"
AIRCRAFTS,0.6693418940609952,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6709470304975923,"1
2
3
4
# of chunks 59 63 67 AP75"
AIRCRAFTS,0.6725521669341894,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6741573033707865,(a) Instance incremental sequence
AIRCRAFTS,0.6757624398073836,"1
2
3
4
# of chunks 53 57 61 AP"
AIRCRAFTS,0.6773675762439807,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6789727126805778,"1
2
3
4
# of chunks 78 81 84 AP50"
AIRCRAFTS,0.680577849117175,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6821829855537721,"1
2
3
4
# of chunks 59 63 67 AP75"
AIRCRAFTS,0.6837881219903692,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6853932584269663,(b) Random class incremental sequence
AIRCRAFTS,0.6869983948635634,"1
2
3
4
# of chunks 53 57 61 AP"
AIRCRAFTS,0.6886035313001605,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6902086677367576,"1
2
3
4
# of chunks 78 81 84 AP50"
AIRCRAFTS,0.6918138041733547,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6934189406099518,"1
2
3
4
# of chunks 59 63 67 AP75"
AIRCRAFTS,0.695024077046549,"SSL-ST
SSL-JT"
AIRCRAFTS,0.6966292134831461,(c) Distant class incremental sequence
AIRCRAFTS,0.6982343499197432,Figure 10: Object detection evaluation results of three types of ImageNet-based streaming data.
AIRCRAFTS,0.6998394863563403,Published as a conference paper at ICLR 2022 32 34 36 38 40 42
AIRCRAFTS,0.7014446227929374,Aircrafts 76 78 80 82 84 86 88 90
AIRCRAFTS,0.7030497592295345,Caltech 84 86 88 90 92 94
FLOWERS,0.7046548956661316,"96
Flowers"
FLOWERS,0.7062600321027287,"66
69
72
75
78
81
84
87
90 Pets 34 36 38 40 42 Cars 60 62 64 66 68 70 DTD"
FLOWERS,0.7078651685393258,"1
2
3
4
5
6
7
8 58 60 62 64 66 68 Food"
FLOWERS,0.709470304975923,"1
2
3
4
5
6
7
8"
FLOWERS,0.7110754414125201,"85
86
87
88
89
90
91
92"
FLOWERS,0.7126805778491172,CIFAR10
FLOWERS,0.7142857142857143,"1
2
3
4
5
6
7
8 64 66 68 70 72 74"
FLOWERS,0.7158908507223114,CIFAR100
FLOWERS,0.7174959871589085,"1
2
3
4
5
6
7
8"
FLOWERS,0.7191011235955056,"24
28
32
36
40
44
48
52
56 Birds"
FLOWERS,0.7207062600321027,"1
2
3
4
5
6
7
8
46 48 50 52 54 56 58 60"
FLOWERS,0.7223113964686998,Sun397
FLOWERS,0.723916532905297,"1
2
3
4
5
6
7
8
70 72 74 76 78 80 82 VOC07"
FLOWERS,0.7255216693418941,Many-shot
FLOWERS,0.7271268057784912,"1
2
3
4
5
6
7
8
# of chunks 52 56 60 64 68 72 76 80 84 88"
FLOWERS,0.7287319422150883,Average Accuracy (%)
FLOWERS,0.7303370786516854,"SSL-ST
SSL-JT
SSL-ST w/Replay
SL-ST
SL-JT 36 39 42 45 48 51 54 57"
FLOWERS,0.7319422150882825,Aircrafts 86 88 90 92 94 96 98
FLOWERS,0.7335473515248796,Caltech 90 92 94 96 98
FLOWERS,0.7351524879614767,Flowers
FLOWERS,0.7367576243980738,"72
75
78
81
84
87
90
93
96 Pets"
FLOWERS,0.7383627608346709,"40
45
50
55
60
65
70
75 Cars 70 72 74 76 78 80 DTD"
FLOWERS,0.7399678972712681,"1
2
3
4
5
6
7
8 60 63 66 69 72 75 78 81 Food"
FLOWERS,0.7415730337078652,"1
2
3
4
5
6
7
8 63 66 69 72 75 78 81 84"
FLOWERS,0.7431781701444623,CIFAR10
FLOWERS,0.7447833065810594,"1
2
3
4
5
6
7
8
69 72 75 78 81 84 87"
FLOWERS,0.7463884430176565,CIFAR100
FLOWERS,0.7479935794542536,"1
2
3
4
5
6
7
8
56
60
64
68
72
76
80
84
88
92 Birds"
FLOWERS,0.7495987158908507,"1
2
3
4
5
6
7
8 84 86 88 90 92 94"
FLOWERS,0.7512038523274478,Sun397
FLOWERS,0.7528089887640449,"SSL-ST
SSL-JT
SSL-ST w/Replay
SL-ST
SL-JT"
FLOWERS,0.7544141252006421,Few-shot
FLOWERS,0.7560192616372392,"1
2
3
4
5
6
7
8
# of chunks 52 56 60 64 68 72 76 80 84 88"
FLOWERS,0.7576243980738363,Average Accuracy (%)
FLOWERS,0.7592295345104334,"SSL-ST
SSL-JT
SSL-ST w/Replay
SL-ST
SL-JT"
FLOWERS,0.7608346709470305,"Figure 11: Linear and few-shot evaluation results of 8-chunk ImageNet-2K sequence. on the left
are the results of each dataset. On the right are averaged results across all left datasets."
FLOWERS,0.7624398073836276,"replay sustainable for real streaming data with much longer chunks, we think setting an upper bound
on the size of the replay buffer, such as the number of images, is feasible for realistic scenarios.
To be speciﬁc, we can determine the upper bound of the replay buffer according to the physical
storage limitation. Before the replay buffer approaches the upper bound, the tradeoff is only between
the accuracy and training time efﬁciency, i.e., storing more data (larger ratio) in the replay buffer
results in better accuracy but longer training time. When the number of data in the replay buffer
exceeds the upper bound, we can discard some old samples to save space for data sampled from
new chunks (Rebufﬁet al., 2017). Generally, we believe sequential SSL with longer sequence is
promising and will beneﬁt from continual learning methods (Delange et al., 2021)."
FLOWERS,0.7640449438202247,"B.3
RESULTS OF NEGATIVE TRANSFER"
FLOWERS,0.7656500802568218,"D
A
B
C
Sequential chunks 0 10 20 30 40 50 60 70 80 90 100"
FLOWERS,0.7672552166934189,Accuracy (%) Pets
FLOWERS,0.7688603531300161,"SSL-ST
SSL-JT
SL-ST
SL-JT
SSL-ST w/MAS
SSL-ST w/MAS+"
FLOWERS,0.7704654895666132,"D
A
B
C
Sequential chunks 0 10 20 30 40 50 60 70 80 90 100"
FLOWERS,0.7720706260032103,Accuracy (%) Pets
FLOWERS,0.7736757624398074,"SSL-ST
SSL-JT
SL-ST
SL-JT
SSL-ST w/MAS
SSL-ST w/MAS+"
FLOWERS,0.7752808988764045,"(a) Many-shot
(b) Few-shot"
FLOWERS,0.7768860353130016,"Figure 12: The averaged evaluation results on Pets of models pre-trained with the distant class
incremental sequence D-A-B-C."
FLOWERS,0.7784911717495987,"We observe severe “negative transfer” when pre-training models with distant class incremental se-
quence and evaluating them on Oxford-IIIT Pets (Parkhi et al., 2012). Speciﬁcally, as shown in
Figure 12, SSL-ST slightly outperforms SSL-JT at the second chunk, i.e., chunk A. After chunk A,"
FLOWERS,0.7800963081861958,Published as a conference paper at ICLR 2022 27 30 33 36 39 42 45 48
FLOWERS,0.7817014446227929,Aircrafts
FLOWERS,0.78330658105939,"72
74
76
78
80
82
84
86
88
90"
FLOWERS,0.7849117174959872,Caltech 75 78 81 84 87 90 93
FLOWERS,0.7865168539325843,Flowers
FLOWERS,0.7881219903691814,"56
60
64
68
72
76
80
84
88
92 Pets 24 28 32 36 40 44 48 52 Cars"
FLOWERS,0.7897271268057785,"58
60
62
64
66
68
70
72
74
76 DTD"
FLOWERS,0.7913322632423756,"1
2
3
4
51
54
57
60
63
66
69
72
75
Food"
FLOWERS,0.7929373996789727,"1
2
3
4
80 82 84 86 88 90 92"
FLOWERS,0.7945425361155698,CIFAR10
FLOWERS,0.7961476725521669,"1
2
3
4
58
60
62
64
66
68
70
72
74
76"
FLOWERS,0.797752808988764,CIFAR100
FLOWERS,0.7993579454253612,"1
2
3
4
20 24 28 32 36 40 44 48 Birds"
FLOWERS,0.8009630818619583,"1
2
3
4 42 45 48 51 54 57 60"
FLOWERS,0.8025682182985554,Sun397
FLOWERS,0.8041733547351525,"1
2
3
4
66
68
70
72
74
76
78
80
82 VOC07"
FLOWERS,0.8057784911717496,Many-shot
FLOWERS,0.8073836276083467,"1
2
3
4
# of chunks 48 52 56 60 64 68 72 76 80"
FLOWERS,0.8089887640449438,Average Accuracy (%)
FLOWERS,0.8105939004815409,"BYOL-ST
BYOL-JT"
FLOWERS,0.812199036918138,"SL-ST
SL-JT 40 42 44 46 48 50"
FLOWERS,0.8138041733547352,Aircrafts 86 88 90 92 94 96
FLOWERS,0.8154093097913323,Caltech 84 86 88 90 92 94
FLOWERS,0.8170144462279294,Flowers
FLOWERS,0.8186195826645265,"64
68
72
76
80
84
88
92
96 Pets"
FLOWERS,0.8202247191011236,"48
51
54
57
60
63
66
69
72 Cars 68 70 72 74 76 78 80 DTD"
FLOWERS,0.8218298555377207,"1
2
3
4"
FLOWERS,0.8234349919743178,"58
60
62
64
66
68
70
72
74 Food"
FLOWERS,0.8250401284109149,"1
2
3
4 60 63 66 69 72 75 78 81"
FLOWERS,0.826645264847512,CIFAR10
FLOWERS,0.8282504012841091,"1
2
3
4
68
70
72
74
76
78
80
82
84"
FLOWERS,0.8298555377207063,CIFAR100
FLOWERS,0.8314606741573034,"1
2
3
4
56 60 64 68 72 76 80 84 Birds"
FLOWERS,0.8330658105939005,"1
2
3
4"
FLOWERS,0.8346709470304976,"85
86
87
88
89
90
91
92
93"
FLOWERS,0.8362760834670947,Sun397
FLOWERS,0.8378812199036918,"BYOL-ST
BYOL-JT
SL-ST
SL-JT"
FLOWERS,0.8394863563402889,Few-shot
FLOWERS,0.841091492776886,"1
2
3
4
# of chunks 60 63 66 69 72 75 78 81 84 87 90"
FLOWERS,0.8426966292134831,Average Accuracy (%)
FLOWERS,0.8443017656500803,"BYOL-ST
BYOL-JT"
FLOWERS,0.8459069020866774,"SL-ST
SL-JT"
FLOWERS,0.8475120385232745,"Figure 13: Linear and few-shot evaluation results of distant incremental sequence for BYOL. on
the left are the results of each dataset. On the right are averaged results across all left datasets."
FLOWERS,0.8491171749598716,"increasing more chunks for SSL-JT models, i.e., chunk B and chunk C, instead leads to signiﬁcant
decrease in performance. We make careful comparison between images from chunk A and images
in Pets. We ﬁnd that Oxford-IIIT Pets includes 37 categories of pets like dogs and cats. As men-
tioned in Appendix A.1, Chunk A includes many relevant classes like ‘Maltese dog’, ‘Old English
sheepdog’, ‘Shetland sheepdog’, ‘Greater Swiss Mountain dog’, ‘Bernese mountain dog’, ‘French
bulldog’, ‘Eskimo dog’, ‘African hunting dog’, ‘tabby’, ‘tiger cat’, ‘Persian cat’, ‘Siamese cat’,
‘Egyptian cat’, and ‘Madagascar cat’. The other three data chunks, i.e., chunk D, B, and C, do not
have these classes. Such observations are consistent with the widely accepted belief that “transfer-
ring knowledge from the dissimilar source can have a negative impact on the target learner” (Wang
et al., 2019). Note that SL models suffer severer ‘negative transfer’ than SSL models and continual
learning methods can help SSL models signiﬁcantly reduce the negative transfer, achieving on-par
performance to SSL-JT models."
FLOWERS,0.8507223113964687,"B.4
RESULTS OF BYOL"
FLOWERS,0.8523274478330658,"To evaluate whether sequential training performs well for other SSL methods, we conduct the chal-
lenging distant class incremental sequence experiments with BYOL (Grill et al., 2020). The results
of BYOL are shown in Figure 13. Similar to the observations with MoCo-v2 in Section 4.1, se-
quential SSL is visibly inferior to joint SSL on streaming data with severe distribution shifts, but
sequential SL performs obviously worse than joint SL. In addition, compared with SL models, SSL
models show signiﬁcantly smaller performance gaps between sequential training and joint training."
FLOWERS,0.8539325842696629,"B.5
SELF-SUPERVISED TRAINING LOSS OVER TIME"
FLOWERS,0.85553772070626,"Figure 14 shows the self-supervised training loss at each step in sequential training for various types
of streaming data. The loss curves illustrate the average MoCo-v2 training loss for each data chunk,
i.e., the contrastive loss. We have two observations. First, given a type of streaming data like the
instance incremental sequence, in the sequential training process, the training loss would spike when
moving to a new disjoint data chunk. Second, the loss spike value becomes higher with increasing
distribution shifts, e.g., training loss increases from instance incremental learning (7.1), random class
incremental learning (7.2), to distant class incremental learning (7.4) at the beginning of step 2. The
queue size is 65,536 for MoCo-v2, and the training batch size is 256. We record the training loss
value averaged across one training batch and plot the loss data point every 250 training iterations,
which means that after the ﬁrst point in the loss curve, we have a ﬁlled queue for contrastive learning.
Therefore, the loss spike is not because of the training mechanism, such as the queue’s refreshing
in early training iterations. Instead, it is reasonable to ascribe the loss spike to the distribution shift
change of training data, i.e., the new chunk. As for the ﬁrst chunk, the initial average contrastive"
FLOWERS,0.8571428571428571,Published as a conference paper at ICLR 2022
FLOWERS,0.8587479935794543,"0
200000
Iteration 7 8 9 10"
FLOWERS,0.8603531300160514,Training Loss
FLOWERS,0.8619582664526485,Chunk 1
FLOWERS,0.8635634028892456,"0
200000
Iteration 6.6 6.7 6.8 6.9 7.0"
FLOWERS,0.8651685393258427,Training Loss
FLOWERS,0.8667736757624398,Chunk 2
FLOWERS,0.8683788121990369,"0
200000
Iteration 6.6 6.7 6.8 6.9 7.0"
FLOWERS,0.869983948635634,Training Loss
FLOWERS,0.8715890850722311,Chunk 3
FLOWERS,0.8731942215088283,"0
200000
Iteration 6.6 6.7 6.8 6.9 7.0"
FLOWERS,0.8747993579454254,Training Loss
FLOWERS,0.8764044943820225,Chunk 4
FLOWERS,0.8780096308186196,(a) Instance incremental sequence
FLOWERS,0.8796147672552167,"0
200000
Iteration 7 8 9 10"
FLOWERS,0.8812199036918138,Training Loss
FLOWERS,0.8828250401284109,Chunk 1
FLOWERS,0.884430176565008,"0
200000
Iteration 6.6 6.7 6.8 6.9 7.0 7.1 7.2"
FLOWERS,0.8860353130016051,Training Loss
FLOWERS,0.8876404494382022,Chunk 2
FLOWERS,0.8892455858747994,"0
200000
Iteration 6.6 6.7 6.8 6.9 7.0 7.1"
FLOWERS,0.8908507223113965,Training Loss
FLOWERS,0.8924558587479936,Chunk 3
FLOWERS,0.8940609951845907,"0
200000
Iteration 6.6 6.7 6.8 6.9 7.0 7.1 7.2"
FLOWERS,0.8956661316211878,Training Loss
FLOWERS,0.8972712680577849,Chunk 4
FLOWERS,0.898876404494382,(b) Random class incremental sequence
FLOWERS,0.9004815409309791,"0
200000
Iteration 7 8 9 10"
FLOWERS,0.9020866773675762,Training Loss
FLOWERS,0.9036918138041734,Chunk 1
FLOWERS,0.9052969502407705,"0
200000
Iteration 6.6 6.8 7.0 7.2 7.4"
FLOWERS,0.9069020866773676,Training Loss
FLOWERS,0.9085072231139647,Chunk 2
FLOWERS,0.9101123595505618,"0
200000
Iteration 6.6 6.8 7.0 7.2 7.4 7.6"
FLOWERS,0.9117174959871589,Training Loss
FLOWERS,0.913322632423756,Chunk 3
FLOWERS,0.9149277688603531,"0
200000
Iteration 6.75 7.00 7.25 7.50 7.75"
FLOWERS,0.9165329052969502,Training Loss
FLOWERS,0.9181380417335474,Chunk 4
FLOWERS,0.9197431781701445,(c) Distant class incremental sequence
FLOWERS,0.9213483146067416,Figure 14: Self-supervised training loss at each step of various types of streaming data.
FLOWERS,0.9229534510433387,"loss is signiﬁcant (around 10) because we randomly initialize the representation model. As for
subsequent chunks, we inherit the representation model from the previous step. Therefore the initial
average contrastive loss is not signiﬁcant, i.e., around 7.2. On each chunk, we train the model
for 200 epochs and ﬁnally observe the convergence of contrastive loss. We ﬁnd that the average
contrastive loss would generally converge to the value of around 6.6 for all chunks. We adopt the
same training mechanism for each chunk, including the difﬁculty of the instance discrimination task
and hyperparameter settings. We perform contrastive learning until convergence on each chunk. As
a result, the ﬁnal loss value is similar for different chunks across different settings."
FLOWERS,0.9245585874799358,"C
MORE EMPIRICAL ANALYSIS"
FLOWERS,0.9261637239165329,"C.1
UNIFORMITY ANALYSIS OF REPRESENTATIONS."
FLOWERS,0.92776886035313,"Uniformity is an important property for good representations (Wang & Isola, 2020). We then com-
pare the uniformity of representations between sequential SL and SSL models. Speciﬁcally, we
sample images from chunk a and obtain representations M ∈Rn×d, where n denotes the number
of samples (50,000) and d denotes the dimension of features (2,048). We ﬁrst use singular values
decomposition (SVD) to extract the 2,048 singular values of representations as below:"
FLOWERS,0.9293739967897271,"M = UΣV T , Σ ∈Rn×d"
FLOWERS,0.9309791332263242,Published as a conference paper at ICLR 2022
FLOWERS,0.9325842696629213,"0
500
1000
1500
2000
# of dimension 0.0 0.2 0.4 0.6 0.8 1.0"
FLOWERS,0.9341894060995185,Cumulative percentages
FLOWERS,0.9357945425361156,"SL-ST
SSL-ST
Uniform"
FLOWERS,0.9373996789727127,"0
500
1000
1500
2000
# of dimension 0.0 0.2 0.4 0.6 0.8 1.0"
FLOWERS,0.9390048154093098,Cumulative percentages
FLOWERS,0.9406099518459069,"SL-ST
SSL-ST
Uniform"
FLOWERS,0.942215088282504,"0
500
1000
1500
2000
# of dimension 0.0 0.2 0.4 0.6 0.8 1.0"
FLOWERS,0.9438202247191011,Cumulative percentages
FLOWERS,0.9454253611556982,"SL-ST
SSL-ST
Uniform"
FLOWERS,0.9470304975922953,"(a) Instance
(b) Random Class
(c) Distant Class"
FLOWERS,0.9486356340288925,Figure 15: Uniformity of representations. SSL learns more uniform representations than SL.
FLOWERS,0.9502407704654896,"We ﬁrst rank the singular values in a descending order and then normalize all singular values to have
the sum of 1. We show the cumulative percentages of singular values with increasing dimensions in
Figure 15. Our assumption is that representations with more even singular values are more uniform.
The corner case is that all singular values are the same, then representations are distributed equally
on all dimensions. From results on three types of streaming data, we ﬁnd that SSL models have
more uniform representations than SL models."
FLOWERS,0.9518459069020867,"C.2
DETAILS OF BACKWARD TRANSFER AND FORWARD TRANSFER"
FLOWERS,0.9534510433386838,"For accuracy Ai
Yj on chunk j, we ﬁrst extract the features with the model trained on chunk i for all
the examples in the chunk j, and then perform KNN classiﬁcation in the feature space on the chunk
j. Speciﬁcally, we set the number of nearest neighbor k=200 for the KNN classiﬁcation."
FLOWERS,0.9550561797752809,"C.3
CKA SIMILARITY ANALYSIS BETWEEN ST MODELS AND JT MODELS"
FLOWERS,0.956661316211878,"CKA similarity. To further understand the sequential self-supervised pre-training, we then take a
closer look at the learned feature representations during the sequential training process. We leverage
the linear centered kernel alignment (CKA) (Kornblith et al., 2019a) to measure the similarity of
output features between two different representation networks given the same data set as input. If
we consider the size of the data set as n and the feature dimension for two networks as d1 and d2,
respectively. We use the selected data set to extract features X ∈Rn×d1 from one representation
network and features Y ∈Rn×d2 from another representation network. In our experiments, n
is 50,000 and both d1 and d2 are 2,048. We ﬁrst preprocess the two representation matrices by
centering the columns. Then the linear CKA similarity between two representations X and Y can be
computed as below:"
FLOWERS,0.9582664526484751,"CKA(X, Y ) =
∥XT Y ∥
2
F
∥XT X∥2
F ∥Y T Y ∥2
F
."
FLOWERS,0.9598715890850722,"How are ST models similar to JT models? We evaluate CKA similarity, for each data chunk,
between features from the sequentially trained model and features from the corresponding jointly
trained model. The same 50,000 samples are used for CKA features similarity analysis. For exam-
ple, as shown in Figure 16, at the step of the second data chunk, we compute the CKA similarity
value between features of the model jointly trained with the ﬁrst two data chunks and features from
the model sequentially trained after the second data chunk. The corresponding CKA similarity value
is 0.4, which indicates for SL, the difference between the ST model and the JT model is very large.
In contrast, SSL has a higher similarity of 0.7 between the ST model and the JT model. Particu-
larly, with MAS and data replay, the CKA similarity increases to about 0.9, which means the model
trained by sequential SSL extracts nearly the same features as the jointly trained model does. Since
the analysis is on the streaming data with severe distribution shifts, this further reinforces our hy-
pothesis that, with the help of suitable continual learning methods, sequential SSL pre-training is
promising to replace joint training on streaming data with various distribution shifts."
FLOWERS,0.9614767255216693,"C.4
SHARPNESS ANALYSIS"
FLOWERS,0.9630818619582665,"We ﬁrst provide more details of the sharpness metric in Eq. (2). We compute the sharpness on the
respective chunk A for different types of streaming data in ImageNet. Considering that the function
f is not differentiable, we sample θ′ from Cϵ and run 50 times to take the minimal accuracy. For"
FLOWERS,0.9646869983948636,Published as a conference paper at ICLR 2022
FLOWERS,0.9662921348314607,"1
2
3
4
# of chunks 0.0 0.2 0.4 0.6 0.8 1.0"
FLOWERS,0.9678972712680578,CKA Similarity
FLOWERS,0.9695024077046549,"SL-ST
SSL-ST"
FLOWERS,0.971107544141252,"SSL-ST w/MAS
SSL-ST w/MAS+"
FLOWERS,0.9727126805778491,"Figure 16: CKA similarity scores between the sequentially trained models and the corresponding
jointly trained models at the step of each data chunk on the distant class incremental sequence.
Given images in the ﬁrst data chunk, this ﬁgure shows the similarity of features between ST models
and the corresponding JT models w.r.t. each data chunk on the distant class incremental sequence.
The higher CKA similarity value, the more similar."
FLOWERS,0.9743178170144462,"computational efﬁciency, we randomly sample 0.1M data points to perform kNN classiﬁcation with
k=200. We need to clarify that we cannot make comparisons of sharpness values across different
sequential settings. Because in different settings, the loss values are calculated on different data
samples and the kNN classiﬁcation tasks are different such as the number of classes to be classiﬁed.
Therefore, we can only compare the sharpness value between SL and SSL models trained and tested
on the same streaming data. Concretely, for the instance incremental sequence, chunk A contains
IID samples from ImageNet-1K, classiﬁed into 1000 classes. For the random class incremental
sequence, chunk A contains 250 random classes from ImageNet-1K, classiﬁed into 250 classes.
For the distant class incremental sequence, chunk A contains 250 semantically similar classes from
ImageNet-1K, classiﬁed into 250 classes. We note that in Table 4, the instance incremental sequence
has the worst sharpness metric value compared to the other two types of streaming data. This
observation is easy to understand. The classiﬁcation task with 1,000 classes on instance incremental
data is more challenging than both classiﬁcation tasks with only 250 classes on the other two types of
streaming data. Therefore, the sharpness value of instance incremental data is the highest. Different
from the sharpness experiments, transfer learning experiments evaluate the performance of models
on downstream tasks. Therefore, there is no contradiction between the observations from both kinds
of experiments."
FLOWERS,0.9759229534510433,"For ﬂatness visualization, we show the normalized loss along the speciﬁed path by performing lin-
early interpolation between the model after chunk 1 and the model after chunk 2 for different splits,
as shown in Figure 17. We can see that compared to SL, loss change is slower for SSL along the
linear path, which reﬂects SSL’s superiority in terms of ﬂatness. Speciﬁcally, the negative value in
the SSL curve means that the performance of SSL continuously improves along the path for instance
incremental learning."
FLOWERS,0.9775280898876404,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
FLOWERS,0.9791332263242376,Interpolation value 0.2 0.0 0.2 0.4 0.6 0.8 1.0
FLOWERS,0.9807383627608347,Relative loss change
FLOWERS,0.9823434991974318,"SL-ST
SSL-ST"
FLOWERS,0.9839486356340289,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
FLOWERS,0.985553772070626,Interpolation value 0.2 0.0 0.2 0.4 0.6 0.8 1.0
FLOWERS,0.9871589085072231,Relative loss change
FLOWERS,0.9887640449438202,"SL-ST
SSL-ST"
FLOWERS,0.9903691813804173,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
FLOWERS,0.9919743178170144,Interpolation value 0.2 0.0 0.2 0.4 0.6 0.8 1.0
FLOWERS,0.9935794542536116,Relative loss change
FLOWERS,0.9951845906902087,"SL-ST
SSL-ST"
FLOWERS,0.9967897271268058,"(a) Instance
(b) Random Class
(c) Distant Class"
FLOWERS,0.9983948635634029,Figure 17: Relation loss change for different interpolations of parameters
