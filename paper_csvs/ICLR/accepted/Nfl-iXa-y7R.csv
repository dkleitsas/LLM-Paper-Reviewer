Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001221001221001221,"Overparameterized neural networks generalize well but are expensive to train. Ideally, one
would like to reduce their computational cost while retaining their generalization benefits.
Sparse model training is a simple and promising approach to achieve this, but there
remain challenges as existing methods struggle with accuracy loss, slow training runtime,
or difficulty in sparsifying all model components. The core problem is that searching
for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To
address this, our main insight is to optimize over a continuous superset of sparse matrices
with a fixed structure known as products of butterfly matrices. As butterfly matrices are
not hardware efficient, we propose simple variants of butterfly (block and flat) to take
advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed
sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most
network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly
is 3× faster than butterfly and speeds up training to achieve favorable accuracy–efficiency
tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks,
our sparse models train up to 2.5× faster than the dense MLP-Mixer, Vision Transformer,
and GPT-2 medium with no drop in accuracy."
INTRODUCTION,0.002442002442002442,"1
INTRODUCTION"
INTRODUCTION,0.003663003663003663,"Recent results suggest that overparameterized neural networks generalize well (Belkin et al., 2019), but
they are expensive to train (Kaplan et al., 2020). An ideal model should use less compute and memory
while retaining the generalization benefits of large models. The simplest and most popular direction is to
sparsify these models. This idea has a long history in machine learning (LeCun et al., 1990) and has driven
fundamental progress in other fields such as statistics (Tibshirani, 1996), neuroscience (Foldiak, 2003), and
signal processing (Candes et al., 2006). However, despite significant efforts, speeding up sparse training
in wall-clock time without degrading accuracy remains an unresolved problem."
INTRODUCTION,0.004884004884004884,"While sparse training is an active research area, it has not seen wide adoption. First, it is difficult and expensive
to find the sparsity pattern (the possible locations of the nonzeros) that could maintain the same level of
accuracy of dense models. Many methods (pruning (Lee et al., 2018), lottery tickets (Frankle and Carbin, 2018),
hashing (Kitaev et al., 2020)) maintain dynamic sparsity masks. However, the large overhead of evolving the
sparsity mask can significantly slow down training and complicate the implementation. Indeed, these methods
either require long cycles of pruning and retraining (Frankle and Carbin, 2018)1 or maintain expensive hash
tables (Chen et al., 2019). Second, most existing methods adopt unstructured sparsity, which may be efficient
in theory, but do not take into account the efficiency of training hardware such as GPUs (optimized for dense
computation)2. Finally, most methods target a single type of operation such as attention (Child et al., 2019;
Zaheer et al., 2020), whereas neural network (NN) models often compose different modules (attention, MLP),
and in many applications the MLP layers are the main training bottleneck (Wu et al., 2020)."
INTRODUCTION,0.006105006105006105,"∗Equal contribution. Order determined by coin flip.
1State-of-the-art sparse training methods require up to 5× more training epochs compared to dense models (Evci et al.,
2020)
2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model (Hooker, 2020)"
INTRODUCTION,0.007326007326007326,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008547008547008548,Pixelated Butterfly
INTRODUCTION,0.009768009768009768,Attention
INTRODUCTION,0.01098901098901099,"MLP
Flat Block Butterfly +"
INTRODUCTION,0.01221001221001221,Model Schema
INTRODUCTION,0.013431013431013432,Compute Allocation +
INTRODUCTION,0.014652014652014652,Low-rank
INTRODUCTION,0.015873015873015872,Attention Mask
INTRODUCTION,0.017094017094017096,MLP Mask
INTRODUCTION,0.018315018315018316,Sparse Masks
INTRODUCTION,0.019536019536019536,"Figure 1: Pixelfly targets GEMM-based networks (networks whose computation is dominated by matrix multiply), which
it views as a series of matrix multiplication. For each matrix multiply from Model Schema, it (1) allocates compute
budget based on dimension and layer type, (2) the budget decides a mapping (hyper-parameter) to our proposed flat block
butterfly sparsity patterns, (3) outputs a hardware-aware sparse mask. Note since the hardware is a block device, one
memory access to an element in a block leads to the access to the full block."
INTRODUCTION,0.020757020757020756,"A better sparse training method should (i) be simple yet accurate, ideally with a static sparsity pattern, (ii) be fast
by aligning sparsity pattern with available hardware, and (iii) have wide coverage of operators that applies to
most NN layers. There are three technical challenges. First, we show that given a budget (e.g., total non-zeros
in a matrix), it is NP-hard to find the optimal static sparsity pattern for a NN module to minimize the approx-
imation error to the dense model. Second, for each sparsity pattern, we need to take into account hardware
block-oriented efficiency (accessing each element in memory takes the same time as accessing the block of ad-
jacent elements (Cook, 2012), illustrated in Fig. 2). Common theoretical measures of efficiency (e.g., number
of non-zeros, FLOPs) do not map well to modern hardware designed for block computation. Last, every dif-
ferent NN module might require different sparsity patterns, which makes the problem even more complicated."
INTRODUCTION,0.02197802197802198,"In our early exploration, we empirically study many sparsity patterns proposed in the literature to find
those patterns that can closely approximate the dense model (Details in Appendix K). We found that one
sparsity pattern, namely butterfly + low-rank, consistently outperforms the others. This sparsity pattern closely
connects to two lines of work in matrix structures: (i) sparse + low-rank matrices, which can capture global
and local information (Candès et al., 2011; Udell and Townsend, 2019; Chen et al., 2021), and (ii) butterfly
matrices (Parker, 1995; Dao et al., 2019) whose products can tightly represent any sparse matrix (De Sa
et al., 2018; Dao et al., 2020). Using the fixed sparsity pattern from butterfly matrices, with the addition
of a low-rank term, would address two of the three challenges above and yield a simple way to sparsify most
NN layers (that are based on matrix multiply)."
INTRODUCTION,0.0231990231990232,"However, butterfly matrices are inefficient on modern hardware: (i) they are difficult to parallelize as they
contain sequential products of many factors, and (ii) they are not hardware-friendly because the sparsity
patterns are not block-aligned. We propose two simple changes to make Butterfly efficient while retaining
their favorable properties. Our proposal, Pixelated Butterfly (Pixelfly), combines flat block butterfly and
low-rank matrices to yield a simple and efficient sparse training method."
INTRODUCTION,0.02442002442002442,"• We design an extremely simple sparsity pattern inspired by butterfly + low-rank matrices, which takes
into account the hardware’s block-oriented efficiency. We propose block butterfly matrices that are
efficient as their sparsity patterns align with hardware blocks. We then introduce flat butterfly, a first-order
approximation of butterfly with residual connection, that turns the original product of factors into a sum.
Flat butterfly matrix multiplications are easy to parallelize. Pixelfly, uses the fixed sparsity pattern from
flat & block butterfly, along with a low-rank term, to produce a sparse network.
• We prove that block butterfly retains the expressiveness of butterfly matrices and can thus tightly capture
sparse matrices. We show that flat butterfly matrices can closely approximate large classes of matrices
that butterfly matrices capture. Moreover, we demonstrate that flat block butterfly + low-rank matrices are
strictly more expressive than sparse or low-rank matrices alone. Finally, leveraging the recent advance in
the neural tangent kernel (NTK), we adapt existing techniques to prove the global convergence of gradient
descent on training sparse and wide ReLU networks.
• Our proposed Pixelfly can be applied to all network modules that rely on matrix multiplication (e.g., linear
layer, attention, MLP). To sparsify a full network, we simply need to allocate compute budget for each
layer based on matrix and hardware block size."
INTRODUCTION,0.02564102564102564,"We empirically validate that Pixelfly can speed up the training of models (Transformers, ViT, MLP-Mixer)
without quality drop compared to baselines on a wide range of domains and tasks. On CIFAR10/100 &
ImageNet classification, Pixelfly achieve 2.3× training time speedup compared to dense ViT, MLP-Mixer
models, and other sparse training baselines, while preserving the same accuracy. On the WikiText-103
language modeling task, we speed up GPT-2 Medium training by 2.5× and achieve the same perplexity. On
the Long Range Arena benchmark, we maintain the same accuracy as Transformer with 5.2× faster training
than a dense model, 2× faster than Sparse transformer, and 6× faster than non-block-aligned sparse methods"
INTRODUCTION,0.026862026862026864,Published as a conference paper at ICLR 2022
INTRODUCTION,0.028083028083028084,"(Reformer). Our ablation studies highlight the importance of each of our components: our butterfly sparsity
improves on existing hand-crafted patterns by up to 2% of accuracy on ImageNet, our hardware-aware
block-sparsity yields up to 5× speedup, and the balanced compute budget allocation brings 2× speedup
compared to baselines that only sparsify attention.3"
PROBLEM SETTING,0.029304029304029304,"2
PROBLEM SETTING"
PROBLEM SETTING,0.030525030525030524,"We first define the problem as sparse matrix approximation with a simple hardware cost model. Then we
briefly introduce butterfly and sparse + low-rank matrices."
PROBLEM SETTING,0.031746031746031744,Memory Access
PROBLEM SETTING,0.03296703296703297,"Figure 2:
Visualization of
memory access for a hardware
with block size 4: accessing
the one (red) location means
accessing the full 4×4 block
(blue)."
PROBLEM SETTING,0.03418803418803419,"Problem Formulation: We focus on the training of GEMM-based models,
which can be viewed as a series of matrix multiplies (Given A,B ∈Rn×d,
compute C =ABT). Speeding up training while maintaining model quality can
be mapped to finding an approximation procedure f which reduces the time
T of computing C while minimizing error E[∥f(A,B)−ABT∥2
F]. Since the
hardware is a block device, accessing any individual element within a block of
memory is the same as accessing the full block (Cook, 2012) (Fig. 2). A simple
cost model of T on hardware with block size b would depend on the number of
b-blocks being accessed and compute time (formal definition in Appendix A).
Our experiment (Appendix L.5) reveals that when the non-zeros are grouped
into blocks, picking the smallest block size supported by hardware can speed up operations by 10× compared
to sparsity patterns that are not block-aligned."
PROBLEM SETTING,0.03540903540903541,"Butterfly, Sparse + Low-rank Matrices: Butterfly matrices have been used in numerical linear alge-
bra (Parker, 1995; Li et al., 2015) and machine learning (Mathieu and LeCun, 2014; Jing et al., 2017;
Munkhoeva et al., 2018; Dao et al., 2019; Choromanski et al., 2019). They encode the recursive divide-and-
conquer structure of the fast Fourier transform (FFT) algorithm (Cooley and Tukey, 1965) and provably
capture any sparse matrix with near-optimal space and time complexity. Sparse and Low-rank structures
have been studied in Robust PCA (Candès et al., 2011), graph clustering (Jalali et al., 2011), and co-variance
estimation (Luo, 2011). Recently it has been adopted in attention approximation for Transformers (Chen et al.,
2021)."
BUTTERFLY MATRICES AND PIXELATED BUTTERFLY,0.03663003663003663,"3
BUTTERFLY MATRICES AND PIXELATED BUTTERFLY"
BUTTERFLY MATRICES AND PIXELATED BUTTERFLY,0.03785103785103785,"Butterfly matrices (Parker, 1995; Dao et al., 2019) are expressive and theoretically efficient. As they contain
the set of sparse matrices, we choose to search for the sparsity pattern in this larger class due to their fixed
sparsity structure. However, there are three technical challenges. We highlight them here along with our
approaches to address them:"
BUTTERFLY MATRICES AND PIXELATED BUTTERFLY,0.03907203907203907,"1. Slow speed: butterfly matrices are not friendly to modern hardware as their sparsity patterns are not
block-aligned, thus are slow. We introduce a variant of butterfly matrices, block butterfly, which operate at
the block level, yielding a block-aligned sparsity pattern.
2. Difficulty of parallelization: the sequential nature of butterfly matrices as products of many factors makes
it hard to parallelize the multiplication. We propose another class of matrices, flat butterfly matrices, that
are the first-order approximation of butterfly with residual connections. Flat butterfly turns the product of
factors into a sum, facilitating parallelization.
3. Reduced expressiveness of flat butterfly: even though flat butterfly matrices can approximate butterfly
matrices with residual connections, they are necessarily high-rank and cannot represent low-rank matri-
ces (Udell and Townsend, 2019). We propose to add a low-rank matrix (that is also block-aligned) to flat
butterfly to increase their expressiveness."
BUTTERFLY MATRICES AND PIXELATED BUTTERFLY,0.040293040293040296,"Combining these three approaches (flat & block butterfly + low-rank), our proposal (Pixelated Butterfly) is a
very simple method to train sparse networks."
BLOCK BUTTERFLY MATRICES,0.04151404151404151,"3.1
BLOCK BUTTERFLY MATRICES"
BLOCK BUTTERFLY MATRICES,0.042735042735042736,"We propose a block version of butterfly matrices, which is more hardware-friendly than the regular butterfly.
The regular butterfly matrices Dao et al. (2019; 2020) will be a special case of block butterfly with block size
b=1. We omit b in the notation if b=1."
BLOCK BUTTERFLY MATRICES,0.04395604395604396,3Pixelfly code is available at https://github.com/HazyResearch/pixelfly
BLOCK BUTTERFLY MATRICES,0.045177045177045176,Published as a conference paper at ICLR 2022
BLOCK BUTTERFLY MATRICES,0.0463980463980464,Butterfly
BLOCK BUTTERFLY MATRICES,0.047619047619047616,Flat Butterfly + + + + +
BLOCK BUTTERFLY MATRICES,0.04884004884004884,Flat Block Butterfly
BLOCK BUTTERFLY MATRICES,0.050061050061050064,Block Butterfly + + + + +
BLOCK BUTTERFLY MATRICES,0.05128205128205128,"Figure 3: Visualization of Flat, Block, and Flat Block butterfly."
BLOCK BUTTERFLY MATRICES,0.052503052503052504,"Definition 3.1. A block butterfly factor (denoted as Bk,b) of size kb (where k≥2) and block size b is a matrix"
BLOCK BUTTERFLY MATRICES,0.05372405372405373,"of the form Bk,b=

D1
D2
D3
D4"
BLOCK BUTTERFLY MATRICES,0.054945054945054944,"
where each Di is a k 2 × k"
BLOCK DIAGONAL MATRIX OF BLOCK SIZE B OF THE FORM,0.05616605616605617,2 block diagonal matrix of block size b of the form
BLOCK DIAGONAL MATRIX OF BLOCK SIZE B OF THE FORM,0.057387057387057384,"diag
 
Di,1,...,Di,k/2

where Di,j ∈Rb×b. We restrict k to be a power of 2."
BLOCK DIAGONAL MATRIX OF BLOCK SIZE B OF THE FORM,0.05860805860805861,"Definition 3.2. A block butterfly factor matrix (denoted as B(n,b)
k
) of size nb with stride k and block size b is
a block diagonal matrix of n"
BLOCK DIAGONAL MATRIX OF BLOCK SIZE B OF THE FORM,0.05982905982905983,k (possibly different) butterfly factors of size kb and block size b:
BLOCK DIAGONAL MATRIX OF BLOCK SIZE B OF THE FORM,0.06105006105006105,"B(n,b)
k
=diag

[Bk,b]1,[Bk,b]2,...,[Bk,b] n k "
BLOCK DIAGONAL MATRIX OF BLOCK SIZE B OF THE FORM,0.06227106227106227,"Definition 3.3. A block butterfly matrix of size nb with block size b (denoted as B(n,b)) is a matrix that can
be expressed as a product of butterfly factor matrices: B(n,b)=B(n,b)
n
B(n,b)
n
2
...B(n,b)
2
. Define Bb as the set of"
BLOCK DIAGONAL MATRIX OF BLOCK SIZE B OF THE FORM,0.06349206349206349,"all matrices that can be expressed in the form B(n,b) (for some n)."
FLAT BUTTERFLY MATRICES,0.06471306471306472,"3.2
FLAT BUTTERFLY MATRICES"
FLAT BUTTERFLY MATRICES,0.06593406593406594,"In most applications of butterfly matrices to neural networks, one multiplies the O(logn) butterfly factors.
However, this operation is hard to be efficiently implemented on parallel hardware (e.g., GPUs) due to the
sequential nature of the operation4. We instead propose to use a sum of butterfly factors that can approximate
the products of the factors. This sum of factors results in one sparse matrix with a fixed sparsity pattern, which
yields up to 3× faster multiplication on GPUs (Appendix J)."
FLAT BUTTERFLY MATRICES,0.06715506715506715,"Residual connections have been proposed to connect the butterfly factors (Vahid et al., 2020). We show that
residual products of butterfly matrices have a first-order approximation as a sparse matrix with a fixed sparsity.
Let M be a matrix in the set of butterfly matrices B. In residual form, for some λ∈R:"
FLAT BUTTERFLY MATRICES,0.06837606837606838,"M =(I+λB(n)
n )(I+λB(n)
n/2)...(I+λB(n)
2 ).
(1)"
FLAT BUTTERFLY MATRICES,0.0695970695970696,"Note that this form can represent the same matrices in the class of butterfly matrices B, since any B(n)
k
contains the identity matrix I."
FLAT BUTTERFLY MATRICES,0.07081807081807082,"Assuming that λ is small, we can expand the residual and collect the terms5:"
FLAT BUTTERFLY MATRICES,0.07203907203907203,"M =I+λ(B(n)
2 +B(n)
4 +···+B(n)
n )+eO(λ2)."
FLAT BUTTERFLY MATRICES,0.07326007326007326,"Definition 3.4. Flat butterfly matrices of maximum stride k (for k a power of 2) are those of the form
I+λ(B(n)
2 +B(n)
4 +···+B(n)
k )."
FLAT BUTTERFLY MATRICES,0.07448107448107448,"Flat butterfly matrices of maximum stride n are the first-order approximation of butterfly matrices in residual
form (Eq. (1)). Notice that flat butterfly of maximum stride k are sparse matrices with O(nlogk) nonzeros
with a fixed sparsity pattern, as illustrated in Fig. 3. We call this sparsity pattern the flat butterfly pattern."
FLAT BUTTERFLY MATRICES,0.0757020757020757,"Flat block butterfly matrices are block versions of flat butterfly in Section 3.2 (shown in Fig. 3). We
empirically validate that flat block butterfly matrices are up to 3× faster than block butterfly or regular
butterfly (Appendix J)."
FLAT BUTTERFLY MATRICES,0.07692307692307693,"Since flat butterfly matrices approximate the residual form of butterfly matrices, they have high rank if λ is
small (Section 4). This is one of the motivations for the addition of the low-rank term in our method."
FLAT BUTTERFLY MATRICES,0.07814407814407814,"4Even with a very specialized CUDA kernel, butterfly matrix multiply (O(nlogn) complexity) is only faster than
dense matrix multiply (O(n2) complexity) for large values of n (around 1024) (Dao et al., 2019).
5We make the approximation rigorous in Section 4."
FLAT BUTTERFLY MATRICES,0.07936507936507936,Published as a conference paper at ICLR 2022
FLAT BUTTERFLY MATRICES,0.08058608058608059,"3.3
PIXELATED BUTTERFLY: FLAT BLOCK BUTTERFLY + LOW-RANK FOR EFFICIENT SPARSE
TRAINING"
FLAT BUTTERFLY MATRICES,0.08180708180708181,"We present Pixelated Butterfly, an efficient sparse model with a simple and fixed sparsity pattern based on
butterfly and low-rank matrices. Our method targets GEMM-based neural networks, which are networks
whose computation is dominated by general matrix multiplies (GEMM), such as Transformer and MLP-Mixer.
As a result, we can view the network as a series of matrix multiplies."
FLAT BUTTERFLY MATRICES,0.08302808302808302,"Given a model schema (layer type, number of layers, matrix dimension) and a compute budget, Pixelated
Butterfly has three steps: compute budget allocation per layer, sparsity mask selection from the flat butterfly
pattern, and model sparsification. We describe these steps in more details:"
FLAT BUTTERFLY MATRICES,0.08424908424908426,"1. Compute budget allocation: based on our cost model (Appendix A), given the layer type, number of
layers, and matrix dimension, we can find the density (fraction of nonzero weights) of each layer type
to minimize the projected compute cost. Continuing our goal for a simple method, we propose to use a
simple rule of thumb: allocate sparsity compute budget proportional to the compute fraction of the layer.
For example, if the MLP layer and attention layers are projected to takes 60% and 40% the compute time
respectively, then allocate 60% of the sparsity compute budget to MLP and 40% to attention. We verify in
Appendix I that this simple rule of thumb produces similar results to solving for the density from the cost
model.
2. Sparsity mask selection: given a layer and a sparsity compute budget for that layer, we use one-quarter to
one-third of the budget for the low-rank part as a simple rule of thumb. We pick the rank as a multiple of the
smallest supported block size of the device (e.g., 32) so that the low-rank matrices are also block-aligned.
The remaining compute budget is used to select the sparsity mask from the flat block butterfly sparsity
pattern: we choose the butterfly block size as the smallest supported block size of the device (e.g., 32), and
pick the maximum stride of the flat block butterfly (Definition 3.4) to fill up the budget.
3. Model sparsification: The resulting sparse model is simply a model whose weights or attention follow
the fixed sparsity mask chosen in step 2, with the additional low-rank terms (rank also chosen in step 2).
In particular, we parameterize each weight matrix6 as: W =γB+(1−γ)UV ⊤, where B is a flat block
butterfly matrix (which is sparse), UV ⊤is the low-rank component, and γ is a learnable parameter. We
train the model from scratch as usual."
FLAT BUTTERFLY MATRICES,0.08547008547008547,"Our method is very simple, but competitive with more complicated procedures that search for the sparsity
pattern (Appendix K). We expect more sophisticated techniques (dynamic sparsity, a better approximation of
butterfly) to improve the accuracy of the method."
THEORETICAL ANALYSIS,0.08669108669108669,"4
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.08791208791208792,"We characterize the expressiveness of the matrices used in our method. In particular, we prove that block
butterfly retains the expressiveness of butterfly, and that flat butterfly can accurately approximate the residual
form of butterfly. Moreover, flat block butterfly + low-rank (an instance of sparse + low-rank) is more expres-
sive than sparse or low-rank matrices alone. Finally, we analyze the training convergence and generalization
of networks with sparse weights. All proofs are in the Appendix."
EXPRESSIVENESS OF BLOCK BUTTERFLY,0.08913308913308914,"4.1
EXPRESSIVENESS OF BLOCK BUTTERFLY"
EXPRESSIVENESS OF BLOCK BUTTERFLY,0.09035409035409035,We first prove the expressiveness of block butterfly matrices.
EXPRESSIVENESS OF BLOCK BUTTERFLY,0.09157509157509157,"Theorem 4.1. The set B2b of n×n block butterfly matrices with block size 2b contains the set Bb of n×n
block butterfly matrices of block size b."
EXPRESSIVENESS OF BLOCK BUTTERFLY,0.0927960927960928,"By a recursive argument, the set of block butterfly matrices whose block size is a power of 2 contains the set
of regular butterfly matrices."
EXPRESSIVENESS OF BLOCK BUTTERFLY,0.09401709401709402,"Dao et al. (2020) show that butterfly matrices can tightly represent all structured matrices, such as sparse
matrices and many fast transforms. As a result, block butterfly matrices can also represent those structured
matrices. In particular,"
EXPRESSIVENESS OF BLOCK BUTTERFLY,0.09523809523809523,"Corollary 4.2. For any constant block size b that is a power of 2, any nb×nb spare matrix with s nonzeros
can be written as products of block butterfly matrices with block size b and their transposes, with O(slogn)
parameters."
WE DESCRIBE HOW TO ADD SPARSE AND LOW-RANK FOR ATTENTION IN APPENDIX I,0.09645909645909646,6We describe how to add sparse and low-rank for attention in Appendix I
WE DESCRIBE HOW TO ADD SPARSE AND LOW-RANK FOR ATTENTION IN APPENDIX I,0.09768009768009768,Published as a conference paper at ICLR 2022
EXPRESSIVENESS OF FLAT BUTTERFLY,0.0989010989010989,"4.2
EXPRESSIVENESS OF FLAT BUTTERFLY"
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10012210012210013,"We now characterize how the flat butterfly matrices approximate butterfly matrices. In particular, assuming
that each butterfly factor has bounded norm, we show that flat-butterfly matrices can accurately approximate
the residual form of butterfly with error scaling as eO(λ2)."
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10134310134310134,"Theorem 4.3. Let M be a matrix of the form in Definition 3.4 where k=n, with Bmax :=maxi
B(n)
i

F
and |λ|≤
c√ϵ
lognBmax for some constant 0<c≤1"
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10256410256410256,"2 and some ϵ>0. Then
M−

I+λ(B(n)
2 +B(n)
4 +···+B(n)
n )

F ≤ϵ."
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10378510378510379,"We show that flat butterfly matrices must have high-rank if λ is small. This is the motivation for the addition
of the low-rank term in Pixelfly (Section 3)."
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10500610500610501,"Theorem 4.4. Let M be as in Eq. (1), with Bmax:=maxi
B(n)
i

F and |λ|≤
c√ϵ
lognBmax for some constant 0<c≤1"
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10622710622710622,"4 and some ϵ>0. Let B∞
max=maxi∥Bi∥∞. Assuming B∞
max≤Bmax. Then"
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10744810744810745,"rank(I+λ(B(n)
2 +···+B(n)
n ))=Ω "
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10866910866910867,"
Bmax"
EXPRESSIVENESS OF FLAT BUTTERFLY,0.10989010989010989,"B∞
max"
EXPRESSIVENESS OF FLAT BUTTERFLY,0.1111111111111111,"2
·
logn"
EXPRESSIVENESS OF FLAT BUTTERFLY,0.11233211233211234,"ϵlog

Bmax
B∞
max   ."
EXPRESSIVENESS OF FLAT BUTTERFLY,0.11355311355311355,"4.3
EXPRESSIVENESS OF FLAT BLOCK BUTTERFLY + LOW-RANK"
EXPRESSIVENESS OF FLAT BUTTERFLY,0.11477411477411477,"Chen et al. (2021) prove that there is a natural class of input sequences (generated by a clustering process)
whose attention matrix can only be approximated well by sparse + low-rank matrices, and not sparse or
low-rank matrices alone. We adapt their technique to show a similar result for the class of matrices we use in
Pixelfly."
EXPRESSIVENESS OF FLAT BUTTERFLY,0.115995115995116,"We require an extra assumption on the clustering process compared to Chen et al. (2021): the elements in the
input sequence form clusters with the same size. Then their attention matrix will have a large block diagonal
component well-approximated by flat butterfly, while the rest of the attention matrix is of medium size and is
well-approximated by low-rank."
EXPRESSIVENESS OF FLAT BUTTERFLY,0.11721611721611722,"Theorem 4.5 (Informal). There exists a class of input sequences whose attention matrices are well-
approximated by flat block butterfly + low-rank (a special case of sparse + low-rank) but not by sparse or
low-rank alone."
EXPRESSIVENESS OF FLAT BUTTERFLY,0.11843711843711843,The formal theorem statement and proof are in Appendix B.3.
CONVERGENCE AND GENERALIZATION OF SPARSE NETWORKS,0.11965811965811966,"4.4
CONVERGENCE AND GENERALIZATION OF SPARSE NETWORKS"
CONVERGENCE AND GENERALIZATION OF SPARSE NETWORKS,0.12087912087912088,"There are natural questions about the training and generalization of sparse models: do they train similarly to
dense models, is their generalization close to that of dense models, and can one successfully train them with
gradient descent? Our analysis theoretically shows that the answers are yes."
CONVERGENCE AND GENERALIZATION OF SPARSE NETWORKS,0.1221001221001221,"Our analysis relies on the neural tangent kernel (NTK) (Jacot et al., 2018) of the network. The NTK of
two data points x and y measures the similarity between the gradient of the network when evaluated at x
compared to the gradient when evaluated at y. This kernel governs the dynamics of the neural network output
function f(·,θ) throughout the training and its generalization. We build on the great literature of NTK (Li and
Liang, 2018; Du et al., 2019; Allen-Zhu et al., 2019b). The standard result (Song and Yang, 2019) implies
the following, if the NTK of the sparse model is close to the NTK of the dense model, then (i) their training
convergence speed is similar, (ii) their generalization bounds are similar. For completeness, we state the formal
result in Appendix F."
CONVERGENCE AND GENERALIZATION OF SPARSE NETWORKS,0.12332112332112333,"Though this result does not capture the possible regularization effect of sparsity, it shows that sparse models
with small NTK difference from dense NTK preserve the generalization ability of dense models, a subject
that has been studied more extensively, both from empirical and from theoretical perspectives. We also show
that training wide and sparse networks with gradient descent converges globally, similar to the result for wide
dense networks (Du et al., 2019; Allen-Zhu et al., 2019b) in Appendix H."
EXPERIMENTS,0.12454212454212454,"5
EXPERIMENTS"
EXPERIMENTS,0.12576312576312576,"In this section, our goal is to demonstrate that an extremely simple fixed sparsity pattern can actually speed
up sparse model training in wall-clock time without degrading model quality. Specifically, we empirically"
EXPERIMENTS,0.12698412698412698,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.1282051282051282,"validate three claims that suggest Pixelfly can improve training speed of different model architectures while
retaining model quality on a wide range of domains and tasks."
EXPERIMENTS,0.12942612942612944,"1. Section 5.1: for image classification tasks, we first show the empirical NTK of flat block butterfly +
low-rank sparsity pattern is closer to dense NKT than other baselines. Then we demonstrate our superior
end-to-end performance. Specifically, we achieve training speed up on both MLP-Mixer and ViT models
by up to 2.3× wall-clock time with no drop in accuracy compared to the dense model and up to 4×
compared to RigL, BigBird and other sparse baselines.
2. Section 5.2: for language modeling and text classification tasks, we can speed up GPT-2 small dense model
training by 2.1×, achieving a perplexity of 22.5 on wikitext-103. In addition, on Long Range Arena (LRA)
benchmark, we maintain the same accuracy but have 5.2× speed-up in training.
3. Section 5.3: we show the necessity of block flat butterfly and low-rank structures, hardware-alignment and
wide coverage of most network layers with ablation studies on these three components of Pixelfly."
IMAGE CLASSIFICATION,0.13064713064713065,"5.1
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.13186813186813187,"0
25
50
75
100
Epoch 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
IMAGE CLASSIFICATION,0.1330891330891331,Accuracy
IMAGE CLASSIFICATION,0.1343101343101343,"Dense
BigBird
Pixelﬂy"
IMAGE CLASSIFICATION,0.13553113553113552,NTK Distance = 0.15
IMAGE CLASSIFICATION,0.13675213675213677,NTK Distance = 0.35
IMAGE CLASSIFICATION,0.13797313797313798,"Figure 4: NTK Comparison with
Dense Model."
IMAGE CLASSIFICATION,0.1391941391941392,"We evaluate the quality and efficiency of Pixelfly through three metrics:
(1) distance to training dynamic of the dense model: compare the distance
between empirical NTK kernel7 of the models with candidate patterns,
including BigBird (Zaheer et al., 2020), Butterfly (Dao et al., 2020), and
that of the dense model, (2) upstream accuracy: compare the accuracy and
training time of the Pixelfly, the dense counterpart, and other baselines
on same image classification tasks, (3) downstream accuracy: compare
the accuracy of our pretrained Pixelfly and dense model fine-tuned on
downstream tasks (Appendix L.4). The empirical NTK of the model with
flat block butterfly + low-rank, picked by Pixelfly, is closer to the NTK of
the dense model. Pixelfly MLP-mixer and ViT models also retain the same top-1 accuracy of the original
dense models while achieving up to 2.3× speed up."
IMAGE CLASSIFICATION,0.14041514041514042,"Setup: We use three popular vision benchmarks, CIFAR-10/100 (Krizhevsky et al., 2009) and ImageNet (Deng
et al., 2009). We choose recent popular Vision Transformer (Dosovitskiy et al., 2020), T2T-ViT (Yuan et al.,
2021) and MLP-Mixer (Tolstikhin et al., 2021) as representative base models. Their major computation
bottlenecks are in different components, e.g. MLP only, attention, or both so we can evaluate the end-to-end
applicability of Pixelfly more clearly."
IMAGE CLASSIFICATION,0.14163614163614163,"Figure 5: The performance of Pixelfly and ViT or MLP-Mixer on CIFAR10,
CIFAR100 and ImageNet benchmarks. We measure the accuracy and the training
time speedup (on ImageNet) compared to the dense model."
IMAGE CLASSIFICATION,0.14285714285714285,"Model
CIFAR10
CIFAR100
ImageNet
Speedup"
IMAGE CLASSIFICATION,0.14407814407814407,"Mixer-S/16
86.4
58.7
72.4
-
Pixelfly-Mixer-S/16
89.8
62.9
72.6
1.7×
Mixer-B/16
87.6
59.5
75.6
-
Pixelfly-Mixer-B/16
90.6
65.4
76.3
2.3×"
IMAGE CLASSIFICATION,0.1452991452991453,"ViT-S/16
89.5
65.1
77.7
-
Pixelfly-ViT-S/16
91.3
66.8
77.5
1.9×
ViT-B/16
89.9
61.9
78.5
-
Pixelfly-ViT-B/16
92.2
65.1
78.6
2.0×"
IMAGE CLASSIFICATION,0.14652014652014653,"Empirical NTK: To character-
ize the training dynamic of the
sparse networks, we compute the
empirical NTK kernels for dense
Vision Transformer on CIFAR-
100.
Then, we show the rela-
tive differences between kernels
of models with different sparsity
patterns and that of the dense one
in Fig. 4. Specifically, we pick
a popular sparsity pattern com-
bination – Bigbird pattern (Za-
heer et al., 2020) for attention
layer and random (magnitude-
based sparsity at initialization equals to random) for MLP layer, as a representative baseline. The plot
indicates that our designed pattern, flat block butterfly + low-rank is the closest one to that of the dense one
among all the patterns. Hence, we expect them to enjoy the most benefits of their dense overparameterized
counterparts in real tasks. More details on measuring empirical NTK are covered in the Appendix L.3."
IMAGE CLASSIFICATION,0.14774114774114774,"Figure 6: Comparison with a representative sparse
training baseline RigL (Evci et al., 2020)."
IMAGE CLASSIFICATION,0.14896214896214896,"Model
ImageNet (Acc)
Speedup
Mixer-S/32
58.56
-
RigL (Evci et al., 2020)
56.10
0.8×
Pixelfly (ours)
59.61
2.1×"
IMAGE CLASSIFICATION,0.15018315018315018,"Training from scratch: We validate that Pixelfly trains up
to 2.3× and 2.0× faster than dense MLP-Mixer and ViT
models from scratch, with the same accuracy under the same
setting (batch size, epochs). Specifically, we sparsify the
models with Pixelfly and train them on three commonly used
vision benchmarking datasets, CIFAR-10/100 and ImageNet.
We measure their Top-1 accuracy wall-clock training time."
THERE IS AN EMERGING CONSENSUS THAT THE NTK IS AN INFORMATIVE MEASURE OF HOW TRAINING AND CONVERGENCE BEHAVIORS,0.1514041514041514,"7There is an emerging consensus that the NTK is an informative measure of how training and convergence behaviors
of two models are similar."
THERE IS AN EMERGING CONSENSUS THAT THE NTK IS AN INFORMATIVE MEASURE OF HOW TRAINING AND CONVERGENCE BEHAVIORS,0.15262515262515264,Published as a conference paper at ICLR 2022
THERE IS AN EMERGING CONSENSUS THAT THE NTK IS AN INFORMATIVE MEASURE OF HOW TRAINING AND CONVERGENCE BEHAVIORS,0.15384615384615385,"To summarize the general trend, Fig. 5 highlights that our sparse vision models consistently retain the accuracy
of their dense counterparts in terms of accuracy and achieve training-time speed-up."
THERE IS AN EMERGING CONSENSUS THAT THE NTK IS AN INFORMATIVE MEASURE OF HOW TRAINING AND CONVERGENCE BEHAVIORS,0.15506715506715507,"Furthermore, we have discussed in Section 1 that current sparse training algorithms aim to dynamic search
what could be good sparsity for efficient inference but do not speed up training in wall-clock time. But we still
present the comparison results in Fig. 6 for completeness. For a fair comparison, we conduct the experiment
on Mixer-S/32 model for 100 epochs because RigL aims for sparsity on weights, while we aim for both
weights & attention. As expected, RigL does not speed up training (the pioneering work has unstructured
sparsity and does not achieve speed up on GPU) but surprisingly Pixelfly outperforms both dense and RigL in
terms of accuracy while achieving 2.1× speedup."
THERE IS AN EMERGING CONSENSUS THAT THE NTK IS AN INFORMATIVE MEASURE OF HOW TRAINING AND CONVERGENCE BEHAVIORS,0.1562881562881563,"Figure 7: Comparison with representative sparse attention base-
lines."
THERE IS AN EMERGING CONSENSUS THAT THE NTK IS AN INFORMATIVE MEASURE OF HOW TRAINING AND CONVERGENCE BEHAVIORS,0.1575091575091575,"Model
ImageNet (Acc)
Speedup
T2T-ViT
81.7
-
BigBird
81.5
0.9×
Sparse Transformer
81.4
1.3×
Pixelfly
81.7
1.4×"
THERE IS AN EMERGING CONSENSUS THAT THE NTK IS AN INFORMATIVE MEASURE OF HOW TRAINING AND CONVERGENCE BEHAVIORS,0.15873015873015872,"Finally, we compare Pixelfly with BigBird and
Sparse Transformer pattern. For a fair compari-
son, we choose T2T-ViT as the base model be-
cause its major bottleneck is on the T2T attention
module (our baselines are efficient attention vari-
ants). We can see from Fig. 7 that Pixelfly is the
only one that can maintain the accuracy and have
actual speed up. Further more, Pixelfly speeds
up T2T module (large attention) by 1.4× compare to dense."
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.15995115995115994,"5.2
LANGUAGE MODELING AND TEXT CLASSIFICATION"
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.16117216117216118,"In this section, we aim to evaluate the effectiveness of Pixelfly in the text domain, on a language modeling
task and Long Range Arena (LRA (Tay et al., 2020)) benchmarks. On WikiText-103 (Merity et al., 2016),
Pixelfly achieves 22.5 perplexity, which is around the same perplexity as GPT-2 small (Radford et al., 2019)
but trains 2.1× faster. On LRA, Pixelfly obtains almost the same accuracy as the full model but gains up to
5.2× speed-up."
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.1623931623931624,"Setup: We use WikiText-103 for language modeling and LRA for classification tasks. We use GPT-2 small
and vanilla Transformer as the base dense models. The computational bottleneck of GPT-2 small for moderate
sequence length, e.g. 512, would be on both attention and MLP layers, while the bottleneck of transformer on
LRA task is on attention since the benchmark is designed to evaluate models under long-context scenarios."
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.16361416361416362,"Figure 8: The performance of Pixelfly, BigBird and GPT-2-
Small, Medium on WikiText-103. We measure the perplexity
and the training speed up."
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.16483516483516483,"Model
WikiText-103 (ppl)
Speedup
GPT-2-Small
22.2
-
BigBird
23.3
0.96×
Pixelfly
22.5
2.1×
GPT-2-Medium
20.9
-
BigBird
21.5
1.1×
Pixelfly
21.0
2.5×"
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.16605616605616605,"GPT-2-Small, Medium on WikiText-103: We
show training GPT-2-Small, Medium and its Pixelfly
model from scratch on a commonly used NLP bench-
marking dataset, wikiText-103. We measure their
perplexity on that dataset, and our training speed
up. All setup and finetuning hyperparameters follow
the ones in the original paper (Radford et al., 2019).
We present the results in Fig. 8. It is not hard to
see that Pixelfly models have great advantages in
accuracy-efficiency tradeoffs since it maintains the
same perplexity as the dense model but achieve up
to 2.5× speed-up in training."
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.16727716727716727,"Figure 9: The performance of Pixelfly, Reformer and vanilla transformer on Long-
Range-Arena benchmarks. We measure the accuracy and training speed."
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.1684981684981685,"Model
ListOps
Text
Retrieval Image Pathfinder
Avg
Speedup
Transformer
36.54
63.12
80.33
41.56
73.49
59.01
-
Reformer
36.85
58.12
78.36
28.30
67.95
53.90
0.8×
Pixelfly
37.65
66.78
80.55
42.35
72.01
59.86
5.2×"
LANGUAGE MODELING AND TEXT CLASSIFICATION,0.16971916971916973,"Vanilla
Transformer
on
LRA: We compare vanilla
transformer and its Pixelfly
models trained from scratch
on LRA benchmark. We mea-
sure the accuracy, throughput,
and training time of both
models. Each task has a different sequence length varying between 1024 and 4096. We follow the
implementation and experimental setting in (Xiong et al., 2021). We compare the performance of Pixelfly
against the dense transformer and report the results in Fig. 9. We also include the numbers of other baselines
from the same repository in the appendix. We can see Pixelfly cause almost no drop in accuracy while
achieving 5.2× speed-up in time."
ABLATION STUDY,0.17094017094017094,"5.3
ABLATION STUDY"
ABLATION STUDY,0.17216117216117216,"We conduct ablation studies on each component of Pixelfly (Details in Appendix L.5). Specifically, we present
(i) how flat block butterfly and low-rank affect the model quality, (ii) how different block size would affect the
training speed, (iii) how budget allocation affects the end-to-end speed up."
ABLATION STUDY,0.17338217338217338,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.1746031746031746,"Necessity of Flat Block Butterfly and Low-rank: (i) We apply different parameter allocation of flat block
butterfly and Low-rank component in Pixelfly Mixer-S model on CIFAR-10 under the different density
varying in [0.05, 0.1, 0.2]. We found that similar to what was reported in (Chen et al., 2021), using around 1"
ABLATION STUDY,0.17582417582417584,"4
budget on Low-rank and 3"
ABLATION STUDY,0.17704517704517705,"4 on flat block butterfly achieves the best accuracy. (ii) We also compare Pixelfly
with baseline sparsity patterns and show it is 2.7× faster than dense, 3× faster than Butterfly, 3.2× faster than
BigBird under 10% density."
ABLATION STUDY,0.17826617826617827,"Block Size: We study the accuracy-efficiency trade-off for flat block butterfly and random sparsity pattern
with different block sizes from 1-32 ( Table 7). We found that first, under the same density, the same sparsity
patterns covered with different block sizes could have a big difference in efficiency. Under the same block, the
pattern with more locality can be more efficient. Last, the density can seem very small, but actually memory
access could be up to 100% of the matrix. Therefore, we always want to make full utilization of the smallest
block size that the hardware (or compiler) supported."
ABLATION STUDY,0.1794871794871795,"Budget Allocation: We sparsify different components of ViT-small separately, including attention and MLP.
We show that their compute ratio is approximately 1:2 , so if only sparsify one of them, the other one will
be the bottleneck preventing end-to-end speed up. Therefore, it is necessary to have an algorithm that can
sparsify all layers."
RELATED WORK,0.1807081807081807,"6
RELATED WORK"
RELATED WORK,0.18192918192918192,"Lottery Ticket Hypothesis. Models proposed in our work can be roughly seen as a class of manually
constructed lottery tickets. Lottery tickets (Frankle and Carbin, 2018) are a set of small sub-networks derived
from a larger dense network, which outperforms their parent networks. Many insightful studies (Morcos et al.,
2019; Orseau et al., 2020; Frankle et al., 2019; 2020; Malach et al., 2020; Pensia et al., 2020) are carried out to
analyze these tickets, but it remains difficult to generalize to large models due to training cost. In an attempt,
follow-up works (Wang et al., 2020; Tanaka et al., 2020) show that one can find tickets without training labels.
We draw inspiration from one of them, Liu and Zenke (2020), which uses the NTK to avoid using labels in
sparsifying networks. Other recent works use specialized hardware to accelerate sparse training (Goli and
Aamodt, 2020; Raihan and Aamodt, 2020)."
RELATED WORK,0.18315018315018314,"Neural Pruning. Our work is loosely related to neural network pruning. By iteratively eliminating neurons
and connections, pruning has seen great success in compressing complex models. Pioneering work (Han et al.,
2015a;b) shows that pruning can produce significantly smaller and faster models for inference. Subsequent
methods (Li et al., 2016; Lin et al., 2017; Dong et al., 2017; Sanh et al., 2020; Lagunas et al., 2021; Zhu and
Gupta, 2017) improve on the quality of the pruned models. While both our and the pruning methods aim to
produce sparse models, we target training efficiency, whereas pruning mostly focuses on inference efficiency
at the cost of sacrificing training speed."
RELATED WORK,0.18437118437118438,"Overparameterized Models and NTK. Our analysis for sparse model convergence relies heavily on
recent advance in neural tangent kernel (NTK) (Jacot et al., 2018). NTK is a tool which has been widely
used in analyzing overparameterized models’ convergence (Li and Liang, 2018; Du et al., 2019; Allen-
Zhu et al., 2019b;c; Song and Yang, 2019), generalization (Allen-Zhu et al., 2019a), connection to data
separability (Oymak and Soltanolkotabi, 2020), and cost per iteration (Brand et al., 2021)). Deep Double
Descent (Nakkiran et al., 2019; d’Ascoli et al., 2020) conjectures that the generalization error improves as the
parameter count grows. It is not surprising that the community is racing to break the record of the largest
parameter counts (Radford et al., 2019; Brown et al., 2020; Dosovitskiy et al., 2020; Tolstikhin et al., 2021;
Zhang et al., 2021; Naumov et al., 2019; Jumper et al., 2021)."
RELATED WORK,0.1855921855921856,We provide extended related work in Appendix M.
CONCLUSION,0.18681318681318682,"7
CONCLUSION"
CONCLUSION,0.18803418803418803,"In our early exploration of many sparsity patterns with complex training procedures, we found that a simple
pattern (butterfly + low-rank) consistently (though not always) performed among the best. This motivated
us to propose Pixelated Butterfly, a simple and efficient sparse training method. In our quest for simplicity
and efficiency, we have chosen to use fixed sparsity that aligns with modern hardware, which was sufficient
to yield wall-clock training time speedup without sacrificing accuracy. We are excited about several future
directions. Inspired by the remarkable success of model pruning for inference, it is possible that dynamic block
sparse mask could be made efficient yet still accurate. Our flat butterfly is a simple first order approximation
of the rich class of butterfly matrices, and there could be more sophisticated approximations that retain more
expressiveness. Our method is a first step towards the goal of making sparse models train faster than dense
models and make them more accessible to the general machine learning community."
CONCLUSION,0.18925518925518925,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.19047619047619047,"Ethics Statement. As the amount of data and model size grows, our work seeks to understand how to
train those large models more efficiently by exploiting sparsity. This potentially connects to energy savings
during large-model training. In addition, this allows the general community that has limited access to the
computational resources to train and understand those foundation models. Our method is applicable to
popular models such as MLP-based and Transformer-based architectures, which may improve a wide range of
applications, each with their own potential benefits and harms. For example, making language modeling more
efficient might simplify the process of spreading misinformation. Similarly, better image classification models
might make automatic surveillance easier. To alleviate the above risks, we need to address application-specific
issues like privacy, bias and discrimination, going beyond the accuracy metric we currently considered.
Specifically, for image classification task, while our work partially addresses the issue of environmental cost,
it does not address other issues such as fairness and bias in model and datasets."
REPRODUCIBILITY STATEMENT,0.1916971916971917,"Reproducibility Statement. To facilitate the reproducibility of our algorithms and results, (i) we include a
link to downloadable source code in supplementary materials, (ii) for our theoretical statements and results,
we include clear explanations of any assumptions and a complete proof of the claims from Appendix A
to Appendix H; for any datasets used in the experiments, a complete description of the data processing steps is
in Appendix L."
REPRODUCIBILITY STATEMENT,0.19291819291819293,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.19413919413919414,"We thank Laurel Orr, Xun Huang, Sarah Hooper, Sen Wu, Megan Leszczynski, and Karan Goel for their
helpful discussions and feedback on early drafts of the paper."
REPRODUCIBILITY STATEMENT,0.19536019536019536,"We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under
No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying
Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation,
NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture,
Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud,
Salesforce, Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative
(SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The Mobilize Center
is a Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging
and Bioengineering through Grant P41EB027060. The U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions,
findings, and conclusions or recommendations expressed in this material are those of the authors and do not
necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S.
Government. Atri Rudra’s research is supported by NSF grant CCF-1763481."
REFERENCES,0.19658119658119658,REFERENCES
REFERENCES,0.1978021978021978,"Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems, pages
6155–6166, 2019a."
REFERENCES,0.199023199023199,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pages 242–252. PMLR, 2019b."
REFERENCES,0.20024420024420025,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks.
In NeurIPS, 2019c."
REFERENCES,0.20146520146520147,"Noga Alon. Perturbed identity matrices have high rank: Proof and applications. Combinatorics, Probability
and Computing, 18(1-2):3–15, 2009."
REFERENCES,0.2026862026862027,"Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration
by overparameterization. In International Conference on Machine Learning, pages 244–253. PMLR, 2018."
REFERENCES,0.2039072039072039,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning, pages 322–332. PMLR, 2019a."
REFERENCES,0.20512820512820512,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019b."
REFERENCES,0.20634920634920634,Published as a conference paper at ICLR 2022
REFERENCES,0.20757020757020758,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):
15849–15854, 2019."
REFERENCES,0.2087912087912088,"Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258, 2021."
REFERENCES,0.21001221001221002,"Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized) neural
networks in near-linear time. In ITCS, 2021."
REFERENCES,0.21123321123321123,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.21245421245421245,"Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and
inaccurate measurements. Communications on Pure and Applied Mathematics: A Journal Issued by the
Courant Institute of Mathematical Sciences, 59(8):1207–1223, 2006."
REFERENCES,0.21367521367521367,"Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal
of the ACM (JACM), 58(3):1–37, 2011."
REFERENCES,0.2148962148962149,"Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-parameterized
deep relu networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
3349–3356, 2020."
REFERENCES,0.21611721611721613,"Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, and Anshumali Shrivastava. Slide:
In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. arXiv
preprint arXiv:1903.03129, 2019."
REFERENCES,0.21733821733821734,"Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying sparse
and low-rank attention. In NeurIPS, 2021."
REFERENCES,0.21855921855921856,"Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019."
REFERENCES,0.21978021978021978,"Krzysztof Choromanski, Mark Rowland, Wenyu Chen, and Adrian Weller. Unifying orthogonal Monte Carlo
methods. In International Conference on Machine Learning, pages 1203–1212, 2019."
REFERENCES,0.221001221001221,"DC Collins and ES Angel. The diagonal decomposition technique applied to the dynamic programming
solution of elliptic partial differential equations. Journal of Mathematical Analysis and Applications, 33(3):
467–481, 1971."
REFERENCES,0.2222222222222222,"Shane Cook. CUDA Programming: A Developer’s Guide to Parallel Computing with GPUs. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition, 2012. ISBN 9780124159334."
REFERENCES,0.22344322344322345,"James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series.
Mathematics of computation, 19(90):297–301, 1965."
REFERENCES,0.22466422466422467,"Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. Learning fast algorithms for
linear transforms using butterfly factorizations. In International conference on machine learning, pages
1517–1527. PMLR, 2019."
REFERENCES,0.2258852258852259,"Tri Dao, Nimit S Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and
Christopher Ré. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In
International conference on representation learning, 2020."
REFERENCES,0.2271062271062271,"Stéphane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting: Where &
why do they appear? arXiv preprint arXiv:2006.03509, 2020."
REFERENCES,0.22832722832722832,"Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Ré, and Atri Rudra. A two-pronged progress
in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 1060–1079. SIAM, 2018."
REFERENCES,0.22954822954822954,Published as a conference paper at ICLR 2022
REFERENCES,0.23076923076923078,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.
Ieee, 2009."
REFERENCES,0.231990231990232,"Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. arXiv preprint arXiv:1705.07565, 2017."
REFERENCES,0.23321123321123322,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."
REFERENCES,0.23443223443223443,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. In ICLR. https://arxiv.org/pdf/1810.02054, 2019."
REFERENCES,0.23565323565323565,"Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making
all tickets winners. In International Conference on Machine Learning, pages 2943–2952. PMLR, 2020."
REFERENCES,0.23687423687423687,"Peter Foldiak. Sparse coding in the primate cortex. The handbook of brain theory and neural networks, 2003."
REFERENCES,0.23809523809523808,"Dean Foster, Howard Karloff, and Justin Thaler. Variable selection is hard. In Conference on Learning Theory,
pages 696–709. PMLR, 2015."
REFERENCES,0.23931623931623933,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
arXiv preprint arXiv:1803.03635, 2018."
REFERENCES,0.24053724053724054,"Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the lottery
ticket hypothesis. arXiv preprint arXiv:1903.01611, 2019."
REFERENCES,0.24175824175824176,"Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity
and the lottery ticket hypothesis. In International Conference on Machine Learning, pages 3259–3269.
PMLR, 2020."
REFERENCES,0.24297924297924298,"Negar Goli and Tor M. Aamodt. Resprop: Reuse sparsified backpropagation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.2442002442002442,"Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. arXiv preprint
arXiv:1711.09224, 3, 2017."
REFERENCES,0.2454212454212454,"Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a."
REFERENCES,0.24664224664224665,"Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient
neural networks. arXiv preprint arXiv:1506.02626, 2015b."
REFERENCES,0.24786324786324787,"Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. Training dynamics of deep networks using stochastic
gradient descent via neural tangent kernel. arXiv preprint arXiv:1905.13654, 2019."
REFERENCES,0.2490842490842491,"Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020."
REFERENCES,0.2503052503052503,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.2515262515262515,"Ali Jalali, Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via convex
optimization. In ICML, 2011."
REFERENCES,0.25274725274725274,"Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and Marin
Soljaci´c. Tunable efficient unitary neural networks (EUNN) and their application to RNNs. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pages 1733–1741. JMLR. org,
2017."
REFERENCES,0.25396825396825395,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure
prediction with alphafold. Nature, 596(7873):583–589, 2021."
REFERENCES,0.25518925518925517,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361, 2020."
REFERENCES,0.2564102564102564,Published as a conference paper at ICLR 2022
REFERENCES,0.2576312576312576,"Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In The International
Conference on Machine Learning (ICML), 2020."
REFERENCES,0.2588522588522589,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.2600732600732601,"François Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. Block pruning for faster transformers.
arXiv preprint arXiv:2109.04838, 2021."
REFERENCES,0.2612942612942613,"Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information
processing systems, pages 598–605, 1990."
REFERENCES,0.2625152625152625,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
Advances in neural information processing systems, 32:8572–8583, 2019."
REFERENCES,0.26373626373626374,"Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on
connection sensitivity. arXiv preprint arXiv:1810.02340, 2018."
REFERENCES,0.26495726495726496,"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient
convnets. arXiv preprint arXiv:1608.08710, 2016."
REFERENCES,0.2661782661782662,"Yingzhou Li, Haizhao Yang, Eileen R. Martin, Kenneth L. Ho, and Lexing Ying. Butterfly factorization.
Multiscale Modeling & Simulation, 13(2):714–732, 2015."
REFERENCES,0.2673992673992674,"Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on
structured data. In NeurIPS, 2018."
REFERENCES,0.2686202686202686,"Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/a51fb975227d6640e4fe47854476d133-Paper.pdf."
REFERENCES,0.2698412698412698,"Tianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural tangent transfer. In
International Conference on Machine Learning, pages 6336–6347. PMLR, 2020."
REFERENCES,0.27106227106227104,"Xi Luo. High dimensional low rank and sparse covariance matrix estimation via convex minimization. arXiv
preprint arXiv:1111.1133, 199, 2011."
REFERENCES,0.27228327228327226,"Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis:
Pruning is all you need. In International Conference on Machine Learning, pages 6682–6691. PMLR,
2020."
REFERENCES,0.27350427350427353,"Michael Mathieu and Yann LeCun. Fast approximation of rotations and Hessians matrices. arXiv preprint
arXiv:1404.7195, 2014."
REFERENCES,0.27472527472527475,"Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.
arXiv preprint arXiv:1609.07843, 2016."
REFERENCES,0.27594627594627597,"Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: generalizing
lottery ticket initializations across datasets and optimizers. arXiv preprint arXiv:1906.02773, 2019."
REFERENCES,0.2771672771672772,"Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, and Ivan Oseledets. Quadrature-based features
for kernel approximation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 9165–9174. Curran
Associates, Inc., 2018."
REFERENCES,0.2783882783882784,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double
descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292, 2019."
REFERENCES,0.2796092796092796,"Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo
Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recom-
mendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091,
2019."
REFERENCES,0.28083028083028083,"Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Logarithmic pruning is all you need. Advances in
Neural Information Processing Systems, 33, 2020."
REFERENCES,0.28205128205128205,Published as a conference paper at ICLR 2022
REFERENCES,0.28327228327228327,"Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence
guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 1
(1):84–105, 2020."
REFERENCES,0.2844932844932845,D Stott Parker. Random butterfly transformations with applications in computational linear algebra. 1995.
REFERENCES,0.2857142857142857,"Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopoulos. Optimal lottery
tickets via subsetsum: Logarithmic over-parameterization is sufficient. arXiv preprint arXiv:2006.07990,
2020."
REFERENCES,0.2869352869352869,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.28815628815628813,"Md Aamir Raihan and Tor M Aamodt. Sparse weight activation training. arXiv preprint arXiv:2001.01969,
2020."
REFERENCES,0.2893772893772894,"Ilya Razenshteyn, Zhao Song, and David P Woodruff. Weighted low rank approximations with provable
guarantees. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages
250–263, 2016."
REFERENCES,0.2905982905982906,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision, 115(3):211–252, 2015."
REFERENCES,0.29181929181929184,"Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by fine-tuning.
arXiv preprint arXiv:2005.07683, 2020."
REFERENCES,0.29304029304029305,"Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. arXiv
preprint arXiv:1906.03593, 2019."
REFERENCES,0.29426129426129427,"Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks without any
data by iteratively conserving synaptic flow. arXiv preprint arXiv:2006.05467, 2020."
REFERENCES,0.2954822954822955,"Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv
preprint arXiv:2011.04006, 2020."
REFERENCES,0.2967032967032967,"Ann Taylor, Mitchell Marcus, and Beatrice Santorini. The penn treebank: an overview. Treebanks, pages
5–22, 2003."
REFERENCES,0.2979242979242979,"Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological), 58(1):267–288, 1996."
REFERENCES,0.29914529914529914,"Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica
Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture for vision.
arXiv preprint arXiv:2105.01601, 2021."
REFERENCES,0.30036630036630035,"Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal on
Mathematics of Data Science, 1(1):144–160, 2019."
REFERENCES,0.30158730158730157,"Keivan Alizadeh Vahid, Anish Prabhu, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An
efficient fft based neural architecture design. In 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 12021–12030. IEEE, 2020."
REFERENCES,0.3028083028083028,"Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving
gradient flow. arXiv preprint arXiv:2002.07376, 2020."
REFERENCES,0.304029304029304,"Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range attention.
arXiv preprint arXiv:2004.11886, 2020."
REFERENCES,0.3052503052503053,"Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas
Singh. Nystromformer: A Nystrom-based algorithm for approximating self-attention. arXiv preprint
arXiv:2102.03902, 2021."
REFERENCES,0.3064713064713065,"Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan.
Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint
arXiv:2101.11986, 2021."
REFERENCES,0.3076923076923077,Published as a conference paper at ICLR 2022
REFERENCES,0.3089133089133089,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.
Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.31013431013431014,"Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. arXiv
preprint arXiv:2106.04560, 2021."
REFERENCES,0.31135531135531136,"Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji,
Jian Guan, et al. Cpm: A large-scale generative chinese pre-trained language model. AI Open, 2:93–99,
2021."
REFERENCES,0.3125763125763126,"Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model
compression. arXiv preprint arXiv:1710.01878, 2017."
REFERENCES,0.3137973137973138,Published as a conference paper at ICLR 2022
REFERENCES,0.315018315018315,"A
PROBLEM FORMULATION"
REFERENCES,0.3162393162393162,"We formulate the problem of sparse model training as sparse matrix approximation with a simple hardware
cost model (Section 2)."
REFERENCES,0.31746031746031744,"We first describe our simple cost model for sparse matrix multiplication to reflect the fact that parallel hardware
such as GPUs are block-oriented (Cook, 2012; Gray et al., 2017): accessing one single element from memory
costs the same as accessing one whole block of elements. We then formulate the sparse matrix approximation
in the forward pass and the backward pass. The cost model necessitates narrowing the sparsity pattern
candidates to those that are block-aligned."
REFERENCES,0.31868131868131866,"Cost model
We model the time cost of an operation based on the number of floating point operations
and memory access. The main feature is that our cost model takes into account memory coalescing, where
accessing a memory location costs the same as accessing the whole block of b elements around it (typically
b=16 or 32 depending on the hardware)."
REFERENCES,0.3199023199023199,"Let Costmem be the memory access cost (either read or write) for a block of b contiguous elements. Accessing
any individual element within that block also costs Costmem time. Let Costflop be the compute cost of a
floating point operation. Let Nblockmem be the number of block memory access, and Nflop be the number of
floating point operations. Then the total cost of the operation is
Totalcost=Costmem·Nblockmem+Costflop·Nflop.
This cost model is a first order approximation of the runtime on modern hardware (GPUs), ignoring the effect
of caching."
REFERENCES,0.32112332112332115,"Block-aligned sparsity pattern, Block cover, and Memory access cost
As the memory access cost
depends on the number of block of memory being accessed, we describe how the number of nonzero elements
in a sparse matrix relates to the number of blocks being accessed. We first define a block cover of a sparse
mask.
Definition A.1. A sparse mask M ∈{0,1}m×n is (b1,b2)-block-aligned if for any index i,j where Mij =1,
we also have Mi′j′ =1 where:
i′=b1⌊i/b1⌋+r1,j′=b2⌊j/b2⌋+r2 for all r1=0,1,...,b1−1 and r2=0,1,...,b2−1."
REFERENCES,0.32234432234432236,"The (b1,b2)-block cover of a sparse mask M ∈{0,1}m×n is the (b1,b2)-block-aligned mask M′∈{0,1}m×n
with the least number of nonzeros such that Mij ≤M′
ij for all i,j."
REFERENCES,0.3235653235653236,"We omit the block size (b1,b2) if it is clear from context."
REFERENCES,0.3247863247863248,"A sparse mask M being (b1,b2) block-aligned means that if we divide M into blocks of size b1×b2, then
each block is either all zeros or all ones. To get the (b1,b2)-block cover of a sparse mask M, we simply divide
M into blocks of size b1×b2 and set each block to all ones if any location in that block is one."
REFERENCES,0.326007326007326,"For a sparse matrix with sparse mask M on a device with block size b, the number of block memory access
Nblockmem is the number of nonzero blocks in its (1,b)-block cover M′ (assuming row-major storage). This
corresponds to the fact that to access a memory location on modern hardware (GPUs), the device needs to
load a whole block of b elements around that location."
REFERENCES,0.32722832722832723,"Fast sparse matrices means block-aligned sparsity pattern
For sparsity patterns that are not block-aligned,
such as the random sparse pattern where each location is independently zero or nonzero, its (1,b)-block cover
might increase the density by a factor of close to b times (we show this more rigorously in the Appendix). As
memory access often dominates the computation time, this means that non block-aligned sparsity will often
result is b times slower execution than block-aligned ones. In other words, exploiting hardware locality is
crucial to obtain speed up."
REFERENCES,0.32844932844932845,"Therefore, this cost model indicates that instead of searching over sparsity patterns whose total cost is less
than some budget C, we can instead search over block-aligned patterns whose number of nonzeros is less
than some limit k. For our theoretical analysis, we consider sparsity patterns that are (1,b)-block-aligned. In
practice, since we need to access both the matrix and its transpose (in the forward and backward pass), we
require the sparsity pattern to be both (1,b)-block-aligned and (b,1)-block-aligned. This is equivalent to the
condition that the sparsity pattern is (b,b)-block-aligned."
REFERENCES,0.32967032967032966,"Sparse matrix approximation in the forward pass
We now formulate the sparse matrix approximation
in the forward pass. That is, we have weight matrix A with input B and we would like to sparsify A while
minimizing the difference in the output. For easier exposition, we focus on the case where number of nonzeros
in each row is the same."
REFERENCES,0.3308913308913309,Published as a conference paper at ICLR 2022
REFERENCES,0.3321123321123321,"Definition A.2 (Forward regression). Given four positive integers m≥n≥d≥k≥1, matrices A∈Rm×d
and B∈Rd×n. The goal is to find a (1,b)-block-aligned binary mask matrix M ∈{0,1}m×d that satisfies
min
M∈{0,1}m×d ∥A·B−(A◦M)·B∥1"
REFERENCES,0.3333333333333333,"s.t. ∥Mi∥0=k,∀i∈[d]
where Mi is the i-th row of M."
REFERENCES,0.33455433455433453,"Sparse matrix approximation in the backward pass
In the backward pass to compute the gradient wrt to
the weight matrix A, we would like to sparsify the gradient CB⊤while preserving as much of the gradient
magnitude as possible.
Definition A.3 (Backward regression). Given four positive integers m≥n≥d≥k≥1, matrices B∈Rd×n
and C ∈Rm×n. The goal is to find a (1,b)-block-aligned binary mask matrix M ∈{0,1}m×d such that
min
M∈{0,1}m×d ∥C·B⊤−(C·B⊤)◦M∥1"
REFERENCES,0.33577533577533575,"s.t. ∥Mi∥0=k,∀i∈[d]
where Mi is the i-th row of M."
REFERENCES,0.336996336996337,"Without making any assumptions, such problems are in general computationally hard Foster et al. (2015);
Razenshteyn et al. (2016)."
REFERENCES,0.33821733821733824,Published as a conference paper at ICLR 2022
REFERENCES,0.33943833943833945,"B
ANALYSIS OF BUTTERFLY VARIANTS"
REFERENCES,0.34065934065934067,"We present formal versions of theorems in Section 4 regarding variants of butterfly matrices. We provide full
proofs of the results here."
REFERENCES,0.3418803418803419,"B.1
BLOCK BUTTERFLY ANALYSIS"
REFERENCES,0.3431013431013431,"Proof of Theorem 4.1. Let M be an n×n block butterfly matrix with block size b. We want to show that M
also has a representation as an n×n block butterfly matrix with block size 2b."
REFERENCES,0.3443223443223443,"By Definition 3.3, M has the form:"
REFERENCES,0.34554334554334554,M =B( n
REFERENCES,0.34676434676434675,"b ,b)
n"
REFERENCES,0.34798534798534797,"b
B( n"
REFERENCES,0.3492063492063492,"b ,b)
n
2b
...B( n"
REFERENCES,0.3504273504273504,"b ,b)
4
B( n"
REFERENCES,0.3516483516483517,"b ,b)
2
."
REFERENCES,0.3528693528693529,Notice that we can combine that last two terms to form a matrix of the form B( n
REFERENCES,0.3540903540903541,"2b,2b)
2
(see Fig. 3). Moreover,"
REFERENCES,0.3553113553113553,other terms in the product of the form B( n
REFERENCES,0.35653235653235654,"b ,b)
n
2ib
can also be written as B( n"
REFERENCES,0.35775335775335776,"2b,2b)
n
2i−12b (see Fig. 3). Thus M also has
the form:"
REFERENCES,0.358974358974359,M =B( n
REFERENCES,0.3601953601953602,"2b,2b)
n
2b
B( n"
REFERENCES,0.3614163614163614,"2b,2b)
n
4b
...B( n"
REFERENCES,0.3626373626373626,"2b,2b)
2
."
REFERENCES,0.36385836385836384,"In other words, M is also an n×n block butterfly matrix with block size 2b."
REFERENCES,0.36507936507936506,"Proof of Corollary 4.2. Dao et al. (2020, Theorem 3) states that any n×n sparse matrix with s nonzeros can
be represented as products of butterfly matrices and their transposes, with O(slogn) parameters."
REFERENCES,0.3663003663003663,"For a constant block size b that is a power of 2, the set of n×n block butterfly matrices of block size b
contains the set of regular butterfly matrices by Theorem 4.1. Therefore any such n×n sparse matrix also has
a representation has products of block butterfly matrices of block size b and their transposes, with O(slogn)
parameters."
REFERENCES,0.36752136752136755,"B.2
FLAT BUTTERFLY ANALYSIS"
REFERENCES,0.36874236874236876,"We prove Theorem 4.3, which relates the first-order approximation in the form of a flat butterfly matrix with
the original butterfly matrix."
REFERENCES,0.36996336996337,"Proof of Theorem 4.3. Let n=2m and let B1,...,Bm∈Rn×n be the m butterfly factor matrices (we rename
them here for simplicity of notation). Let E= m
Y"
REFERENCES,0.3711843711843712,"i=1
(I+λBi)−  I+ m
X"
REFERENCES,0.3724053724053724,"i=1
λBi ! ."
REFERENCES,0.37362637362637363,Our goal is to show that ∥E∥F ≤ϵ.
REFERENCES,0.37484737484737485,"We first recall some properties of Frobenius norm. For any matrices A and C, we have ∥AC∥F ≤∥A∥F∥C∥F
and ∥A+C∥F ≤∥A∥F +∥C∥F."
REFERENCES,0.37606837606837606,"Expanding the terms of the product in E, we have E= m
X"
REFERENCES,0.3772893772893773,"i=2
λi
X"
REFERENCES,0.3785103785103785,"s∈[m],|s|=i Y"
REFERENCES,0.3797313797313797,"j∈s
Bj."
REFERENCES,0.38095238095238093,Published as a conference paper at ICLR 2022
REFERENCES,0.38217338217338215,"Using the above properties of Frobenius norm, we can bound E:"
REFERENCES,0.3833943833943834,"∥E∥F ≤ m
X"
REFERENCES,0.38461538461538464,"i=2
λi
X"
REFERENCES,0.38583638583638585,"s∈[m],|s|=i Y"
REFERENCES,0.38705738705738707,"j∈s
∥Bj∥F ≤ m
X"
REFERENCES,0.3882783882783883,"i=2
λi
X"
REFERENCES,0.3894993894993895,"s∈[m],|s|=i Y"
REFERENCES,0.3907203907203907,"j∈s
Bmax = m
X"
REFERENCES,0.39194139194139194,"i=2
λ2mi 
Bi
max
 = m
X"
REFERENCES,0.39316239316239315,"i=2
(λmBmax)i ≤ m
X i="
REFERENCES,0.39438339438339437," 
c√ϵ
i ≤c2ϵ ∞
X"
REFERENCES,0.3956043956043956,"i=0
(c√ϵ)i"
REFERENCES,0.3968253968253968,"≤
c2ϵ
1−c√ϵ
≤ϵ,
where in the last step we use the assumption that c≤1 2."
REFERENCES,0.398046398046398,We now bound the rank of the first-order approximation.
REFERENCES,0.3992673992673993,"Proof of Theorem 4.4. Let M∗=I+Pm
i=1λBi. Note that any entry in Pm
i=1λBi has absolute value at most"
REFERENCES,0.4004884004884005,"mλB∞
max≤c√ϵB∞
max
Bmax
≤1 4,"
REFERENCES,0.4017094017094017,"where we use the assumption that B∞
max≤Bmax and c≤1 4."
REFERENCES,0.40293040293040294,Thus any diagonal entry in M∗has absolute value at least 1−1 4 = 3
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.40415140415140416,"4 and the off-diagonal entries are at most
c√ϵB∞
max
bm
."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4053724053724054,"Alon (2009, Theorem 1.1) states that: there exists some c>0 such that for any real M ∈Rn×n, if the diagonal
elements have absolute values at least 1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4065934065934066,"2 and the off-diagonal elements have absolute values at most ϵ where
1
2√n ≤ϵ≤1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4078144078144078,"4, then rank(M)≥
clogn
ϵ2log1/ϵ."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.409035409035409,"Applying this theorem to our setting, we have that"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.41025641025641024,rank(M∗)≥Ω 
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.41147741147741146,"
Bmax"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4126984126984127,"B∞
max"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4139194139194139,"2
·
m"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.41514041514041516,"ϵlog

Bmax
B∞
max   ."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4163614163614164,"We just need to show that B∞
max
Bmax ≥
1
2c√ϵn to satisfy the condition of the theorem."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4175824175824176,"Indeed, we have that 1≤Bmax"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4188034188034188,"B∞
max ≤
√"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.42002442002442003,2n as each ∥Bi∥0≤2n. Combining the two conditions on Bmax
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.42124542124542125,"B∞
max , we have"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.42246642246642246,shown that 1≤Bmax
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4236874236874237,"B∞
max ≤2c√ϵn. This concludes the proof."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4249084249084249,"B.3
FLAT BLOCK BUTTERFLY + LOW-RANK ANALYSIS"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4261294261294261,"We show that flat butterfly + low-rank (an instance) of sparse + low-rank, is more expressive than either
sparse or low-rank alone. We adapt the argument from Chen et al. (2021) to show a generative process where
the attention matrix can be well approximated by a flat butterfly + low-rank matrix, but not by a sparse or
low-rank alone."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.42735042735042733,"We describe here a generative model of an input sequence to attention, parameterized by the inverse temperature
β∈R and the intra-cluster distance ∆∈R."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.42857142857142855,"Process 1. Let Q∈Rn×d, where d≥Ω(log3/2(n)), with every row of Q generated randomly as follows:"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4297924297924298,Published as a conference paper at ICLR 2022
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.43101343101343104,"1. For C =Ω(n), sample C number of cluster centers c1,...,cC ∈Rd independently from N(0,Id/
√"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.43223443223443225,"d).
2. For each cluster around ci, sample ni = b number of elements around ci, of the form zij = ci + rij
for j = 1,...,ni where rij ∼N(0,Id∆/
√"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.43345543345543347,"d). Assume that the total number of elements is n = cb and
∆≤O(1/log1/4n)."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4346764346764347,"Let Q be the matrix whose rows are the vectors zij where i=1,...,C and j=1,...,ni. Let A=QQ⊤and let
the attention matrix be Mβ =exp(β·A).
Theorem B.1. Let Mβ, be the attention matrix in Process 1. Fix ϵ ∈(0,1). Let R ∈Rn×n be a matrix.
Consider low-rank, sparse, and sparse + low-rank approximations to Mβ. Assume (1−∆2)logn ≤β ≤
O(logn)."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4358974358974359,"1. Flat butterfly + low-rank: There exists a flat butterfly + low-rank R with n1+o(1) parameters with
∥Mβ−R∥F ≤ϵn."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4371184371184371,"2. Low-rank: If R is such that n−rank(R)=Ω(n), then ∥Mβ−R∥F ≥Ω(n)."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.43833943833943834,"3. Sparse: If R has sparsity o(n2), then ∥Mβ−R∥F ≥Ω(n)."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.43956043956043955,"Proof sketch. As the argument is very similar to that of Chen et al. (2021, Theorem 1), we describe here the
modifications needed to adapt their proof."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.44078144078144077,"The main difference between our generative process and that of Chen et al. (2021) is that each cluster has the
same number of elements, which is the same as the block size. The resulting attention matrix will have a large
block diagonal component, similar to that Chen et al. (2021). However, all the blocks in the block diagonal
component has the same block size, which is b. Moreover, a flat block butterfly of block size b contains a
block diagonal component of block size b. Therefore, this flat block butterfly matrix plays the same role as
the sparse matrix in the proof of Chen et al. (2021). The rest of the argument follows that of theirs."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.442002442002442,Published as a conference paper at ICLR 2022
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4432234432234432,"Roadmap
The analysis of sparse networks is organized as follows. In Section C we list some basic notations
that will be used. In Section D we consider the problem of adding sparsity on W, and we achieve polynomial
solving time. In Section E we prove that the gradient descent can be done fast under the sparsity assumption.
In Section G we consider the problem of adding sparsity on a, and we show that minimizing the dropout loss
is equivalent with a kernel ridge regression problem. In Section H we analyze the dynamics of gradient flow
and prove the convergence result."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4444444444444444,"C
NOTATIONS"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4456654456654457,"For a vector x, we use ∥x∥p to denote its ℓp norm, and we mainly consider p=1,2 in this paper. For a matrix
A, we use ∥A∥0,∥A∥1,∥A∥F to denote the ℓ0 norm, entry-wise ℓ1 norm and Frobenius norm of A respectively.
For two matrices A,B ∈Rd×m, we use A◦B to denote their Hadamard product. We use Tmat(n,d,m) to
denote the time of multiplying n×d matrix with another d×m matrix. For a symmetric matrix A, we use
λmin(A) to denote its minimum eigenvalue. We also let vec(A) be the vectorization of a matrix A in column
first order. We use ⟨·,·⟩to denote standard Euclidean inner product between two vectors."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4468864468864469,"Moreover, we use N(µ,Σ) to denote the Gaussian distribution with mean µ and covariance Σ. We denote the
ReLU function by φ(z)=max{z,0}. For an event E, we use 1{E} or 1E to denote its indicator function."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4481074481074481,"D
SPARSITY ON HIDDEN LAYER WEIGHTS"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.44932844932844934,"D.1
APPLYING MASKS BEFORE MULTIPLICATION"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.45054945054945056,"Given matrix A ∈Rn×d, B ∈Rd×n, naively computing AB takes Tmat(n,d,n). Note that, we can also
consider the case where A and B have different size. For simplicity, let us consider the case where matrix A
and matrix B⊤have the same size."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4517704517704518,"Our goal is to find “optimal"" binary mask matrix W ∈{0,1}d×n such that,
min
W ∥f(A·B)−f(A·(W ◦B))∥1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.452991452991453,"s.t. ∥WB,i∥0=k,∀i∈[n]"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4542124542124542,"Remark D.1. In the practical applications we care about, the function f is the activation function of neural
network, e.g., ReLU(z)=max{z,0}."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4554334554334554,We define a sparse targeted regression problem:
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.45665445665445664,"Definition D.2 (Sparse mark regression, ℓ1 version). Given a matrix B ∈Rd×n, and a vector a∈Rd, the
goal is to find a k-sparse binary vector w∈{0,1}d to minimize the following problem:"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.45787545787545786,"min
w ∥a⊤·B−(a⊤◦w⊤)·B∥1."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4590964590964591,"Naively, the above problem can be solved in n·dO(k) via guess all the
 d
k

choices."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4603174603174603,Lemma D.3. The targeted sparse mask regression problem can be solved in n·dO(k).
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.46153846153846156,"Proof. We need to guess
 d
k

times, which becomes dO(k). Each time it takes nd operations, thus the total
time is
nd·dO(k)=n·dO(k)."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4627594627594628,"Definition D.4 (ℓ1 version). Given three positive integers m ≥n ≥d ≥k ≥1, matrices A ∈Rm×d and
B∈Rd×n. We define our problem as finding the binary matrix W ∈{0,1}m×d that satisfies
min
W ∥A·B−(A◦W)·B∥1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.463980463980464,"s.t. ∥Wi∗∥0=k,∀i∈[m].
where Wi∗is the i-th row of W."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4652014652014652,Theorem D.5. The problem being defined as Definition D.4 can be solved in mndO(k) time.
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.46642246642246643,Proof. Our problem can be decomposed into m sub-problems as follows:
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.46764346764346765,Published as a conference paper at ICLR 2022
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.46886446886446886,"∥A·B−(A◦W)·B∥1= m
X i=1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4700854700854701,"(A·B)i∗−((A◦W)·B)i∗

1 = m
X i=1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4713064713064713,"Ai∗·B−(A◦W)i∗·B

1 = m
X i=1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4725274725274725,"Ai∗·B−(Ai∗◦Wi∗)·B

1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.47374847374847373,"where Ai∗means the i-th row of matrix A. By applying Lemma D.3, each sub-problem
min
Wi∗∥Ai∗·B−(Ai∗◦Wi∗)·B∥1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.47496947496947495,"can be solved in n·dO(k) time. Then the problem defined in Definition D.4 can be solved in
m·ndO(k)=mndO(k)"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.47619047619047616,time in total. Thus we finish the proof.
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.47741147741147744,"In the above Theorem, we show that solving the sparse mask regression problem is NP-hard. However, if we
add some mild assumptions and consider minimizing ℓ1 norm, then we can solve the regression problem in
polynomial time, as the following parts show."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.47863247863247865,"Definition D.6 (ℓ1 version). Given a matrix B∈Rd×n
≥0 , and a vector a∈Rd
≥0, the goal is to find a k-sparse
binary vector w∈{0,1}d to solve
min
w ∥a⊤·B−(a⊤◦w⊤)·B∥1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.47985347985347987,"Lemma D.7. The targeted ℓ1 version sparse mask regression problem can be solved in
O(nd+nlogn)
which is polynomial time."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4810744810744811,"Proof. We first consider the situation when a∈{0,1}d. In this case, we have
∥a⊤·B−(a⊤◦w⊤)·B∥1+∥(a⊤◦w⊤)·B∥1=∥a⊤·B∥1
where ∥a⊤·B∥1 is fixed. So we only need to consider the following problem:
max
w ∥(a⊤◦w⊤)·B∥1."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4822954822954823,"For simplicity we assume ai=1,∀i∈[d], and we only need to solve
max
w ∥w⊤·B∥1
where w has k elements equal to 1 and d−k elements equal to 0. For i∈[d], we compute Si =Pn
j=1Bij
which is the summation of i-th row of B, and sort them as S(1) ≥S(2) ≥···≥S(n). Then we only need to
let w(i) =1 for i∈[k] and other elements equal to 0. Computing all Si takes O(nd) time, sorting Si takes
O(nlogn) time, thus the total time consumption is O(nd+nlogn) in this case."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4835164835164835,"Next, we consider the general case when a∈Rd
≥0. We let"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.48473748473748474,"Bi∗=aiBi∗and ai=
1,
ai>0
0,
ai=0, ∀i∈[d]"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.48595848595848595,"where Bi∗is the i-th row of B. Then our optimization problem is equivalent to
min
w ∥a⊤·B−(a⊤◦w⊤)·B∥1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.48717948717948717,"where B∈Rd×n
≥0 and a∈{0,1}d. Thus we turn this case into the first case. Constructing B and a takes O(nd)
time, thus the total time consumption is also O(nd+nlogn) in this case."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4884004884004884,"Definition D.8 (ℓ1 version). Given three positive integers m ≥n ≥d ≥k ≥1, matrices A ∈Rm×d
≥0
and
B∈Rd×n
≥0 . We define our problem as finding the binary matrix W ∈{0,1}m×d that satisfies
min
W ∥A·B−(A◦W)·B∥1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4896214896214896,"s.t. ∥Wi∗∥0=k,∀i∈[m].
where Wi∗is the i-th row of W."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4908424908424908,"Theorem D.9. The problem being defined as Definition D.8 can be solved in
O(mnd+mnlogn)
time."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.49206349206349204,Published as a conference paper at ICLR 2022
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4932844932844933,Proof. Our problem can be decomposed into m sub-problems as follows:
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4945054945054945,"∥A·B−(A◦W)·B∥1= m
X i=1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.49572649572649574,"(A·B)i∗−((A◦W)·B)i∗

1 = m
X i=1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.49694749694749696,"Ai∗·B−(A◦W)i∗·B

1 = m
X i=1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4981684981684982,"Ai∗·B−(Ai∗◦Wi∗)·B

1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.4993894993894994,"where Ai∗means the i-th row of matrix A. By applying Lemma D.7, each sub-problem"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5006105006105006,"min
Wi∗∥Ai∗·B−(Ai∗◦Wi∗)·B∥1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5018315018315018,can be solved in O(nd+nlogn) time. Then the problem defined in Definition D.8 can be solved in
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.503052503052503,m·O(nd+nlogn)=O(mnd+mnlogn)
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5042735042735043,time in total. Thus we finish the proof.
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5054945054945055,"D.2
APPLYING MASKS AFTER MULTIPLICATION"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5067155067155067,"Definition D.10. Given matrix B∈Rd×n, C ∈Rm×n. The goal is to find a mask W ∈{0,1}m×d where each
column of W is k-sparse"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5079365079365079,"min
W∈{0,1}m×d∥C·B⊤−(C·B⊤)◦W∥1"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5091575091575091,"Remark D.11. The B defined in Definition D.4 is the same as the B defined in Definition D.10. B is
corresponding to the X in the neural network setting."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5103785103785103,"E
GRADIENT COMPUTATION"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5115995115995116,"In this section we consider a neural network with one hidden layer and m neurons in this hidden layer.
Suppose x∈Rd is the input, W =(w1,···,wm)∈Rd×m is the weight matrix of the first layer, a∈Rm is the
output weight, and M ∈{0,1}d×m is the mask matrix with each column having at most k non-zero entries.
The neural network f :Rd→R is defined as"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5128205128205128,"f(x)=a⊤φ

(M◦W)⊤·x

."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.514041514041514,"For simplicity, we only optimize W and fix a. Consider the mean square loss"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5152625152625152,"L(W)= 1 2 n
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5164835164835165,"i=1
(f(xi)−yi)2= 1 2 n
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5177045177045178,"i=1
(a⊤φ((M◦W)⊤·xi)−yi)2."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.518925518925519,"In the forward computation, for a batch of data points x1,···,xn ∈Rd, let X ∈Rd×n denote the input data
points matrix. For convenience, we define"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5201465201465202,∆W(t)=W(t+1)−W(t)=−η∂L(W(t)) ∂W(t)
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5213675213675214,where η is the step size. We define function gt:Rd→Rm as
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5225885225885226,gt(x)=(f(x)−y)·diag{φ′((M◦W(t))⊤·x)}·a
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5238095238095238,"and also denote gt(X)=(gt(x1),···,gt(xn))∈Rm×n."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.525030525030525,Lemma E.1. We can express ∆W(t) as
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5262515262515263,"∆W(t)=−η(X·g⊤
t (X))◦M,"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5274725274725275,and each column of ∆W(t) has at most k non-zero entries.
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5286935286935287,Published as a conference paper at ICLR 2022
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5299145299145299,"Proof. From the definition, we know"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5311355311355311,∆W(t)= −η∂L(W(t)) ∂W(t)
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5323565323565324,"= −η
 n
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5335775335775336,"i=1
(f(xi)−yi)diag{φ′((M◦W(t))⊤·xi)}
|
{z
}
m×m"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5347985347985348,"a
|{z}
m×1
x⊤
i
|{z}
1×d"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.536019536019536,"⊤
◦M
|{z}
d×m = −η( n
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5372405372405372,"i=1
gt(xi)·x⊤
i )⊤◦M"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5384615384615384,"= −η( X
|{z}
d×n
·g⊤
t (X)
| {z }
n×m"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5396825396825397,")◦M
|{z}
d×m
."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5409035409035409,"Since each column of M has at most k non-zero entries, we easily know each column of ∆W(t) also has at
most k non-zero entries."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5421245421245421,"Lemma E.2. Suppose that matrices M ∈Rd×m, W(t) ∈Rd×m and ∆W(t) ∈Rd×m are given and
pre-computed, then we can compute ft+1(X) in
O(mnk)
time. (Here ft+1(X) is the evaluation of f at W(t+1).)"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5433455433455433,"Proof. The goal is to compute
ft+1(X)=a⊤·φ(( M
|{z}
d×m
◦W(t+1)
|
{z
}
d×m"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5445665445665445,)⊤·X).
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5457875457875457,"By using Lemma E.1, we have
(M◦W(t+1))⊤·X = (M◦(W(t)+∆W(t)))⊤·X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5470085470085471,= (M◦W(t))⊤·X+(M◦∆W(t))⊤·X
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5482295482295483,"= (M◦W(t))⊤·X−η(M◦(X·g⊤
t (X))◦M)⊤·X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5494505494505495,"= (M◦W(t))⊤·X−η((X·g⊤
t (X))◦M)⊤·X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5506715506715507,"= (M◦W(t))⊤·X+(∆W(t))⊤·X.
Notice that we have already computed (M ◦W(t))⊤·X ∈Rm×d from previous iteration, so we only need
to compute (∆W(t))⊤·X where ∆W(t) ∈Rd×m and X ∈Rd×n. By using Lemma E.1, each row of
(∆W(t))⊤has at most k non-zero entries, thus we can compute (∆W(t))⊤·X in O(mnk) time."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5518925518925519,"Lemma E.3. Suppose that matrices M ∈Rd×m,W(t)∈Rd×m and ft(X) are given and pre-computed, then
we can compute ∂L(W(t))"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5531135531135531,"∂W(t)
in O(mnk) time."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5543345543345544,"Proof. By using Lemma E.1, we have
∂L(W(t))"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5555555555555556,"∂W(t)
=(X·g⊤
t (X))◦M"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5567765567765568,"where gt(x)=(f(x)−y)·diag{φ′((M◦W(t))⊤·x)}·a∈Rm and gt(X)=(gt(x1),···,gt(xn))∈Rm×n. We
first compute M ◦W(t) in O(mk) time, then we can construct gt(X) ∈Rm×n in n·O(mk) time. Given
gt(X), since we only need to compute km entries of X·g⊤
t (X), where each entry can be computed in O(n)
time, thus we can compute ∂L(W(t))"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.557997557997558,"∂W(t)
in O(mnk) time."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5592185592185592,Published as a conference paper at ICLR 2022
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5604395604395604,Algorithm 1 The sparse training algorithm
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5616605616605617,"1: procedure SPARSE TRAINING({xi,yi}i∈[n])
2:
Initialization ar,wr(0)∼N(0,Id) for r∈[m].
3:
for t=1→T do
4:
/*forward computation*/
5:
Compute M◦W(t)
▷Takes O(mk) time.
6:
for i=1→n do
7:
ft(xi)←a⊤φ((M◦W(t))⊤·xi)
▷Takes O(mk) time.
8:
gt(xi)←(f(xi)−yi)·diagφ′((M◦W(t))⊤·xi)·a
▷Takes O(mk) time.
9:
end for
10:
/*backward computation*/
11:
gt(X)←(gt(x1),···,gt(xn))."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5628815628815629,"12:
∂L(W(t))"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5641025641025641,"∂W(t) =(X·g⊤
t (X))◦M
▷Takes O(mnk) time."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5653235653235653,"13:
W(t+1)=W(t)+∆W(t)
▷∆W(t)=−η ∂L(W(t))"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5665445665445665,"∂W(t) .
14:
end for
15: end procedure"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5677655677655677,"F
NEURAL TANGENT KERNEL, CONVERGENCE, AND GENERALIZATION"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.568986568986569,"Our analysis relies on the neural tangent kernel (NTK) (Jacot et al., 2018) of the network.
Definition F.1. Let f(·,θ): Rd →R be the function specified by a neural network with parameters θ∈Rp
and input dimension d. The parameter θ is initialized randomly from a distribution P. Then its neural tangent
kernel (NTK) (Jacot et al., 2018) is a kernel K : Rd×Rd→R defined by:"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5702075702075702,"K(x,y)= E
θ∼P"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5714285714285714,∂f(x;θ)
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5726495726495726,"∂θ
,∂f(y;θ) ∂θ 
."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5738705738705738,"We can relate the training and generalization behavior of dense and sparse models through their NTK. The
standard result (Song and Yang, 2019) implies the following.
Proposition F.2. Let fdense denote a ReLU neural network with L layers with dense weight matrices
θdense with NTK Kdense, and let fsparse be the ReLU neural network with the same architecture and with
weight matrices θsparse whose rows are k-sparse, and with NTK Ksparse. Let x1,...,xN be the inputs
sampled from some distribution PX. Suppose that the empirical NTK matrices Kd = Kdense(xi,xj) and
Ks=Ksparse(xi,xj) for (i,j)∈[N]×[N] satisfy ∥Kd−Ks∥≤δ."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.575091575091575,"Training. We knew the the number of iterations of dense network is λmin(Kd)−2n2log(1/ϵ) to reach the ϵ
training loss. For sparse network we need (λmin(Kd)−δ)−2n2log(1/ϵ)."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5763125763125763,"Generalization. We knew the the number of iterations of dense network is λmin(Kd)−2n2log(1/ϵ) to reach
the generalization error ϵ training loss. For sparse network we need (λmin(Kd)−δ)−2n2log(1/ϵ)."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5775335775335775,These results relate the generalization bound of sparse models to that of dense models.
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5787545787545788,Published as a conference paper at ICLR 2022
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.57997557997558,"G
DROPOUT NEURAL NETWORK AND KRR"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5811965811965812,"We consider a two layer neural network with ReLU activation function, and write"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5824175824175825,"f(W,x):= 1
√m m
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5836385836385837,"r=1
arφ(w⊤
r x)= 1
√m m
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5848595848595849,"r=1
arw⊤
r x1w⊤
r x≥0
(2)"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5860805860805861,"where wr(0)∼N(0,Id)∈Rd, ar ∼unif({−1,+1}) and all randomnesses are independent. We will fix ar
during the training process and use
1
√m normalization factor, both of which are in the literature of Du et al.
(2019); Song and Yang (2019); Brand et al. (2021)."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5873015873015873,"Suppose the training data are (x1,y1),...,(xn,yn)∈Rd×R, we define the classical objective function bL as
follows:"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5885225885225885,"bL(W):= 1 2 n
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5897435897435898,"i=1
(f(W,xi)−yi)2."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.590964590964591,The gradient with respect to loss function bL is
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5921855921855922,"∂bL
∂wr
= 1
√m n
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5934065934065934,"i=1
(f(W,xi)−yi)arxi1w⊤
r xi≥0."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5946275946275946,"We consider the effect of dropout on network training. For each r∈[m], we introduce the mask by defining
random variable σr as follows:"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5958485958485958,"σr =
0,
with probability 1−q;
1/q,
with probability q."
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5970695970695971,"It is easy to see that E[σr]=0·(1−q)+(1/q)·q =1 and E[σ2
r]=02·(1−q)+(1/q)2·q =1/q. We assume
σi and σj are independent for any i̸=j, then E[σiσj]=E[σi]E[σj]=1. Let σ=(σ1,···,σm), we define our
dropout neural net as"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5982905982905983,"F(W,x,σ):= 1
√m m
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.5995115995115995,"r=1
arσrφ(w⊤
r x)= 1
√m m
X"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.6007326007326007,"r=1
arσrw⊤
r x1w⊤
r x≥0.
(3)"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.6019536019536019,"Dropout explicitly change the target function, since we need to minimize the ℓ2 distance between F(W,x,σ)
and y, instead of f(W,x) and y. Formally, we define the dropout loss as"
AND THE OFF-DIAGONAL ENTRIES ARE AT MOST,0.6031746031746031,L(W):= 1
E,0.6043956043956044,"2E
σ "" n
X"
E,0.6056166056166056,"i=1
(F(W,xi,σ)−yi)2
# .
(4)"
E,0.6068376068376068,We first give an explicit formulation of L which also shows the difference between L and bL.
E,0.608058608058608,"Lemma G.1. The dropout loss defined in Eq. (4) can be expressed as the sum of classical loss bL and a
regularization term as"
E,0.6092796092796092,L(W)=bL(W)+1−q
MQ,0.6105006105006106,"2mq n
X i=1 m
X"
MQ,0.6117216117216118,"r=1
φ(w⊤
r xi)2.
(5)"
MQ,0.612942612942613,"Proof. Since E[σr]=1, we have"
MQ,0.6141636141636142,"E
σ[F(W,xi,σ)]=
1
√mE
σ[ m
X"
MQ,0.6153846153846154,"r=1
arσrφ(w⊤
r x)]= 1
√m m
X"
MQ,0.6166056166056166,"r=1
arφ(w⊤
r xi)=f(W,xi)
(6)"
MQ,0.6178266178266179,Published as a conference paper at ICLR 2022
MQ,0.6190476190476191,"holds for any i∈[n]. Next, we show the difference between L and bL:
2(L(W)−bL(W)) = E
σ "" n
X"
MQ,0.6202686202686203,"i=1
(F(W,xi,σ)−yi)2
# − n
X"
MQ,0.6214896214896215,"i=1
(f(W,xi)−yi)2 = n
X i=1 
E
σ"
MQ,0.6227106227106227,"h
(F(W,xi,σ)−yi)2i
−(f(W,xi)−yi)2 = n
X i=1"
MQ,0.6239316239316239,"
E
σ

F(W,xi,σ)2
−f(W,xi)2 = n
X i=1  1 m X"
MQ,0.6251526251526252,"r1,r2∈[m]
E[ar1ar2σr1σr2φ(w⊤
r1xi)φ(w⊤
r2xi)]−1 m X"
MQ,0.6263736263736264,"r1,r2∈[m]
ar1ar2φ(w⊤
r1xi)φ(w⊤
r2xi)   = 1"
MQ,0.6275946275946276,"m · 1−q q n
X i=1 m
X"
MQ,0.6288156288156288,"r=1
a2
rφ(w⊤
r xi)2 = 1"
MQ,0.63003663003663,"m · 1−q q n
X i=1 m
X"
MQ,0.6312576312576312,"r=1
φ(w⊤
r xi)2
(7)"
MQ,0.6324786324786325,"where the first step follows from definition, the second step follows from the linearity of expectation, the third
step follows from Eq. (6), the forth step follows from expansion, the fifth step follows from E[σr1σr2]=1 for
r1̸=r2 and E[σ2
r1]= 1"
MQ,0.6336996336996337,"q, and the last step follows from a2
r =1. Thus we have"
MQ,0.6349206349206349,L(W)=bL(W)+1−q
MQ,0.6361416361416361,"2mq n
X i=1 m
X"
MQ,0.6373626373626373,"r=1
φ(w⊤
r xi)2"
MQ,0.6385836385836385,and finish the proof.
MQ,0.6398046398046398,"Before we move on, we introduce some extra notations and definitions. We denote"
MQ,0.6410256410256411,W =vec(W)=  
MQ,0.6422466422466423,"w1
w2
...
wm "
MQ,0.6434676434676435,"∈Rmd, and Y =  "
MQ,0.6446886446886447,"y1
y2
...
yn "
MQ,0.645909645909646,∈Rn.
MQ,0.6471306471306472,"Definition G.2. We define matrix G∞∈Rn×n which can be viewed as a Gram matrix from a kernel
associated with ReLU function as follows:
G∞
ij (X)=
E
w∼N(0,I)[x⊤
i xj1w⊤xi≥0,w⊤xj≥0], ∀i,j∈[n]×[n]"
MQ,0.6483516483516484,and assume λ0=λmin(G∞)>08.
MQ,0.6495726495726496,"Definition G.3. We define the masked matrix ΦW(X,σ)∈Rn×md as"
MQ,0.6507936507936508,"ΦW(X,σ):=
1
√m  "
MQ,0.652014652014652,"Φ(x1,σ)
Φ(x2,σ)
...
Φ(xn,σ)  "
MQ,0.6532356532356532,"=
1
√m  "
MQ,0.6544566544566545,"a1σ11⟨w1,x1⟩≥0x⊤
1
a2σ21⟨w2,x1⟩≥0x⊤
1
...
amσm1⟨wm,x1⟩≥0x⊤
1
a1σ11⟨w1,x2⟩≥0x⊤
2
a2σ21⟨w2,x2⟩≥0x⊤
2
...
amσm1⟨wm,x2⟩≥0x⊤
2
...
...
...
...
a1σ11⟨w1,xn⟩≥0x⊤
n
a2σ21⟨w2,xn⟩≥0x⊤
n
...
amσm1⟨wm,xn⟩≥0x⊤
n  "
MQ,0.6556776556776557,and also define the unmasked matrix bΦW(X)∈Rn×md as
MQ,0.6568986568986569,"bΦW(X):= 1
√m  "
MQ,0.6581196581196581,"a11⟨w1,x1⟩≥0x⊤
1
a21⟨w2,x1⟩≥0x⊤
1
...
am1⟨wm,x1⟩≥0x⊤
1
a11⟨w1,x2⟩≥0x⊤
2
a21⟨w2,x2⟩≥0x⊤
2
...
am1⟨wm,x2⟩≥0x⊤
2
...
...
...
...
a11⟨w1,xn⟩≥0x⊤
n
a21⟨w2,xn⟩≥0x⊤
n
...
am1⟨wm,xn⟩≥0x⊤
n  ."
MQ,0.6593406593406593,"8According to Theorem 3.1 in Du et al. (2019), the assumption holds when xi is not parallel with xj for i̸=j, which
is reasonable in reality."
MQ,0.6605616605616605,Published as a conference paper at ICLR 2022
MQ,0.6617826617826618,"Definition G.4. We define the masked block diagonal matrix ΨW(X,σ)∈Rmd×md as"
MQ,0.663003663003663,"ΨW(X,σ):= 1"
MQ,0.6642246642246642,"mdiag

ψ1,ψ2,···,ψm

."
MQ,0.6654456654456654,"where ∀r∈[m], ψr ∈Rd×d is defined as"
MQ,0.6666666666666666,"ψr :=a2
rσ2
r n
X"
MQ,0.6678876678876678,"i=1
xix⊤
i ·12
⟨wr,xi⟩≥0=σ2
r n
X"
MQ,0.6691086691086691,"i=1
xix⊤
i ·1⟨wr,xi⟩≥0."
MQ,0.6703296703296703,We also define the unmasked block diagonal matrix bΨW(X)∈Rmd×md as
MQ,0.6715506715506715,bΨW(X):= 1
MQ,0.6727716727716728,"mdiag

bψ1,bψ2,···,bψm

."
MQ,0.673992673992674,"where ∀r∈[m], bψr ∈Rd×d is defined as"
MQ,0.6752136752136753,"bψr := n
X"
MQ,0.6764346764346765,"i=1
xix⊤
i ·1⟨wr,xi⟩≥0."
MQ,0.6776556776556777,"Lemma G.5. It is easy to verify that
ΦW(X,σ)= bΦW(X)·Dσ and ΨW(X,σ)= bΨW(X)·D2
σ
where
Dσ :=diag(σ1,···,σ1
| {z }
d"
MQ,0.6788766788766789,",···,σm,···,σm
|
{z
}
d"
MQ,0.6800976800976801,)∈Rmd×md.
MQ,0.6813186813186813,"For convenience, we will simply denote ΦW =ΦW(X,σ) and ΨW =ΨW(X,σ). Then by using the above
notations, we can express our dropout loss as L(W)= 1"
MQ,0.6825396825396826,"2Eσ[∥ΦWW −Y ∥2
2]."
MQ,0.6837606837606838,Lemma G.6. If we denote λ= 1−q
MQ,0.684981684981685,"q ≥0, then we have"
MQ,0.6862026862026862,L(W)= 1
MQ,0.6874236874236874,"2∥bΦWW −Y ∥2
2+ λ"
W,0.6886446886446886,"2W
⊤bΨWW."
W,0.6898656898656899,"Proof. As for the first term, we have"
W,0.6910866910866911,"∥bΦWW −Y ∥2
2= n
X"
W,0.6923076923076923,"i=1
( 1
√m m
X"
W,0.6935286935286935,"r=1
ar1⟨wr,xi⟩≥0x⊤
i ·wr−yi)2 = n
X"
W,0.6947496947496947,"i=1
( 1
√m m
X"
W,0.6959706959706959,"r=1
arφ(w⊤
r xi)−yi)2 = n
X"
W,0.6971916971916972,"i=1
(f(W,xi)−yi)2"
W,0.6984126984126984,= 2bL(W).
W,0.6996336996336996,"As for the second term, since bΨW is a block diagonal matrix, we have"
W,0.7008547008547008,"W
⊤bΨWW = 1 m m
X r=1"
W,0.702075702075702,"
w⊤
r ·
 
a2
r n
X"
W,0.7032967032967034,"i=1
xix⊤
i ·12
⟨wr,xi⟩≥0

·wr
 = 1 m m
X r=1 n
X i=1"
W,0.7045177045177046," 
(w⊤
r xi)·(w⊤
r xi)⊤·12
⟨wr,xi⟩≥0
 = 1 m n
X i=1 m
X"
W,0.7057387057387058,"r=1
φ(w⊤
r xi)2."
W,0.706959706959707,"Thus by using Lemma G.1, we have"
W,0.7081807081807082,L(W)= bL(W)+1−q
MQ,0.7094017094017094,"2mq n
X i=1 m
X"
MQ,0.7106227106227107,"r=1
φ(w⊤
r xi)2 = 1"
MQ,0.7118437118437119,"2∥bΦWW −Y ∥2
2+ λ"
W,0.7130647130647131,"2W
⊤bΨWW"
W,0.7142857142857143,and finish the proof.
W,0.7155067155067155,Published as a conference paper at ICLR 2022
W,0.7167277167277167,Remark G.7. A classical kernel ridge regression problem can be defined as
W,0.717948717948718,"min
W
1
2∥φ(X)⊤W −Y ∥2
2+ λ"
W,0.7191697191697192,"2∥W∥2
2
where φ:Rd→F is a feature map. Note that Lemma G.6 breaks the dropout loss into two parts: the first part
is an error term, and the second part can be seen as a regularization term. Thus the task of minimizing the
dropout loss L(W) is equivalent to a kernel ridge regression (KRR) problem."
W,0.7203907203907204,Published as a conference paper at ICLR 2022
W,0.7216117216117216,"H
DYNAMICS OF KERNEL METHODS (CONTINUOUS GRADIENT FLOW)"
W,0.7228327228327228,"The NTK also allows us to analyze the training convergence of sparse networks. We show that gradient
descent converges globally when training wide sparse networks. This convergence speed is similar to that of
dense models (Du et al., 2019; Allen-Zhu et al., 2019b)."
W,0.724053724053724,"In this section we will discuss the dynamics of kernel method under the mask σ, which adds sparsity in
the output layer. Our problem will be considered in over-parameterized scheme. First we introduce some
additional definitions and notations. We define symmetric Gram matrix G(W) as G(W):= bΦW ·bΦ⊤
W ∈Rn×n.
For all i,j∈[n]×[n], we have"
W,0.7252747252747253,"G(W)ij = 1 m m
X"
W,0.7264957264957265,"r=1
a2
r1⟨wr,xi⟩≥0,⟨wr,xj⟩≥0x⊤
i xj = 1"
W,0.7277167277167277,"mx⊤
i xj m
X"
W,0.7289377289377289,"r=1
1⟨wr,xi⟩≥0,⟨wr,xj⟩≥0."
W,0.7301587301587301,"We define block symmetric matrix H(W) as H(W)= bΦ⊤
W ·bΦW ∈Rmd×md. Then for all i,j∈[m]×[m], the
(i,j)-th block of H(W) is"
W,0.7313797313797313,"H(W)ij = 1 maiaj n
X"
W,0.7326007326007326,"k=1
xkx⊤
k ·1⟨wi,xk⟩≥0,⟨wj,xk⟩≥0∈Rd×d."
W,0.7338217338217338,"By using Lemma G.6, we consider the corresponding kernel regression problem:"
W,0.7350427350427351,"min
W Lk(W)=min
W
1
2∥bΦW −Y ∥2
2+ λ"
W,0.7362637362637363,"2W
⊤bΨW
(8)"
W,0.7374847374847375,"where bΦ∈Rn×md, W ∈Rmd×1, Y ∈Rn×1 and bΨ∈Rmd×md. The main difference from neural network is
that we assume bΦ (related to NTK, e.g., see Definition G.3) and bΨ (related to regularization term, e.g., see
Definition G.4) do not change during the training process."
W,0.7387057387057387,"The gradient of Lk can be expressed as
∇WLk(W)= bΦ⊤bΦW −bΦ⊤Y +λbΨW.
(9)
We use W ⋆to denote the optimal solution of Eq. (8), and it satisfies
∇WLk(W)"
W,0.73992673992674,"W=W ⋆=(bΦ⊤bΦ+λbΨ)W ⋆−bΦ⊤Y =0.
(10)"
W,0.7411477411477412,"Since bΨ is a positive diagonal matrix, bΦ−1"
W,0.7423687423687424,"2 exists, thus we have
W ⋆= (bΦ⊤bΦ+λbΨ)−1bΦ⊤Y.
Next, we consider the question from a continuous gradient flow aspect. In time t, we denote W(t) =
vec(W(t)),bΦ(t)= bΦW(t),bΨ(t)= bΨW(t). We also denote G(t)=G(W(t)) and H(t)=H(W(t)). Following
the literature of Du et al. (2019), we consider the ordinary differential equation defined by
dwr(t)"
W,0.7435897435897436,"dt
=−∂Lk(W(t))"
W,0.7448107448107448,"∂wr(t)
.
(11)"
W,0.746031746031746,Lemma H.1 (Lemma 3.1 in Du et al. (2019)). If m=Ω(n2
W,0.7472527472527473,"λ2
0 log(n"
W,0.7484737484737485,"δ )), we have with probability at least 1−δ,"
W,0.7496947496947497,∥G(0)−G∞∥2≤λ0
W,0.7509157509157509,4 and λmin(G(0))≥3 4λ0.
W,0.7521367521367521,"Lemma H.2 (Lemma 3.2 in Du et al. (2019)). If w1,···,wm are i.i.d generated from N(0,Id), then with
probability at least 1−δ, the following holds. For any set of weight vectors w1,···,wm ∈Rd that satisfy
for any r ∈[m],∥wr −wr(0)∥2 ≤cδλ0"
W,0.7533577533577533,"n2 for some small positive constant c, then matrix G ∈Rd×d satisfies
∥G−G(0)∥2< λ0"
W,0.7545787545787546,4 and λmin(G)> λ0 2 .
W,0.7557997557997558,"The above lemma shows that for W that is close to W(0), the Gram matrix G also stays close to the initial
Gram matrix G(0), and its minimal eigenvalue is lower bounded."
W,0.757020757020757,"Lemma H.3 (Gradient Flow). If we assume λmin(bΨ) ≥Λ0 > 0, then with probability at least 1−δ, for
w1,···,wm∈Rd that satisfy ∀r∈[m],∥wr−wr(0)∥2≤cδλ0"
W,0.7582417582417582,"n2 , we have"
W,0.7594627594627594,"d∥bΦW −bΦW ⋆∥2
2
dt
≤−γ∥bΦW −bΦW ⋆∥2
2
holds some constant γ>0."
W,0.7606837606837606,"Proof. By using Eq. (9) and Eq. (11), we can express dW"
W,0.7619047619047619,"dt as
dW"
W,0.7631257631257631,"dt =−∇WLk(W)=−(bΦ⊤bΦW −bΦ⊤Y +λbΨW).
(12)"
W,0.7643467643467643,Published as a conference paper at ICLR 2022
W,0.7655677655677655,"Then we have
d∥bΦW −bΦW ⋆∥2
2
dt"
W,0.7667887667887668,"= d∥bΦW −bΦW ⋆∥2
2
dW
· dW"
W,0.7680097680097681,"dt
= 2(bΦW −bΦW ⋆)⊤bΦ·(−(bΦ⊤bΦW −bΦ⊤Y +λbΨW))"
W,0.7692307692307693,= −2(bΦW −bΦW ⋆)⊤bΦ(bΦ⊤bΦW −bΦ⊤Y +λbΨW)
W,0.7704517704517705,= −2(bΦW −bΦW ⋆)⊤bΦ(bΦ⊤bΦW −bΦ⊤bΦW ⋆−λbΨW ⋆+λbΨW)
W,0.7716727716727717,= −2(bΦW −bΦW ⋆)⊤bΦbΦ⊤(bΦW −bΦW ⋆)−2λ(bΦW −bΦW ⋆)⊤bΦ(bΨW −bΨW ⋆)
W,0.7728937728937729,"≤−2λ0∥bΦW −bΦW ⋆∥2
2−2λ(W −W ⋆)⊤bΦ⊤bΦbΨ(W −W ⋆)
(13)
where the second step follows from Eq. (12), the fourth step follows from Eq. (10), and the last step follows
from the definition that λ0=λmin(G)=λmin(bΦbΦ⊤)."
W,0.7741147741147741,"As for the second term in the Eq. (13), we have
2λ(W −W ⋆)⊤bΦ⊤bΦbΨ(W −W ⋆)"
W,0.7753357753357754,= 2λ(W bΦ⊤bΦ−W ⋆bΦ⊤bΦ)⊤bΨ(W −W ⋆)
W,0.7765567765567766,≥2λΛ0(W −W ⋆)⊤bΦ⊤bΦ(W −W ⋆)
W,0.7777777777777778,"= 2λΛ0∥bΦW −bΦW ⋆∥2
2
(14)
Thus by Eq. (13) and Eq. (14) we have
d∥bΦW −bΦW ⋆∥2
2
dt
≤−(2λ0+2λΛ0)∥bΦW −bΦW ⋆∥2
2.
By letting γ=2λ0+2λΛ0 we finish the proof."
W,0.778998778998779,"For convenience, we denote u(t)= bΦ(t)·W(t)∈Rn. Then it is easy to verify that"
W,0.7802197802197802,"ui(t)= 1
√m m
X"
W,0.7814407814407814,"r=1
arφ(w⊤
r xi)=f(W(t),xi), ∀i∈[n],"
W,0.7826617826617827,showing that u(t) is the prediction in time t.
W,0.7838827838827839,Lemma H.4 (Convergence rate). If we assume λmin(G(s))≥λ0
W,0.7851037851037851,"2 holds for 0≤s≤t, then we have"
W,0.7863247863247863,"1. ∥u(t)−Y ∥2
2≤e−(λ0+2λ/m)t∥u(0)−Y ∥2
2;"
W,0.7875457875457875,"2. ∀r∈[m],∥wr(t)−wr(0)∥2≤
√n∥u(0)−Y ∥2"
W,0.7887667887667887,"λ0
√m
."
W,0.78998778998779,"Proof. From Eq. (9), we can express the dynamics by using u(t) as
du(t)"
W,0.7912087912087912,"dt
= −bΦ(bΦ⊤bΦW −bΦ⊤Y +λbΨW)"
W,0.7924297924297924,"= G(t)(Y −u(t))−λbΦbΨW.
(15)
Thus we have
d∥u(t)−Y ∥2
2
dt
= 2(u(t)−Y )⊤ 
G(t)(Y −u(t))−λbΦbΨW
"
W,0.7936507936507936,= −2(u(t)−Y )⊤G(t)(u(t)−Y )−2λ(u(t)−Y )⊤bΦbΨW
W,0.7948717948717948,"≤−λ0∥u(t)−Y ∥2
2−2λ(u(t)−Y )⊤bΦbΨW.
(16)
As for the second term, we have"
W,0.796092796092796,2λ(u(t)−Y )⊤bΦbΨW = 2λ
W,0.7973137973137974,"m (u(t)−Y )⊤bΦ·[bψ1·w1,···,bψm·wm]⊤ = 2λ"
W,0.7985347985347986,"m (u(t)−Y )⊤bΦ·[ n
X"
W,0.7997557997557998,"i=1
xiφ(w⊤
1 xi),···, n
X"
W,0.800976800976801,"i=1
xiφ(w⊤
mxi)]⊤ = 2λ"
W,0.8021978021978022,"m (u(t)−Y )⊤·[U1(t),···,Un(t)]⊤
(17)"
W,0.8034188034188035,Published as a conference paper at ICLR 2022
W,0.8046398046398047,"where for j∈[n], Uj(t)∈R can be expressed as"
W,0.8058608058608059,"Uj(t)=
1
√m m
X r=1"
W,0.8070818070818071," 
ar1⟨wr,xj⟩≥0x⊤
j · n
X"
W,0.8083028083028083,"i=1
xiφ(w⊤
r xi)
"
W,0.8095238095238095,"=
1
√m m
X r=1 n
X"
W,0.8107448107448108,"i=1
arx⊤
j (xix⊤
i )wr·1⟨wr,xi⟩≥0,⟨wr,xj⟩≥0"
W,0.811965811965812,"=
1
√m m
X r=1"
W,0.8131868131868132,"
arx⊤
j wr·1⟨wr,xj⟩≥0· n
X"
W,0.8144078144078144,"i=1
1⟨wr,xi⟩≥0

."
W,0.8156288156288156,"We denote U(t)=[U1(t),···,Un(t)]⊤∈Rn and have"
W,0.8168498168498168,2λ(u(t)−Y )⊤bΦbΨW = 2λ
W,0.818070818070818,"m (u(t)−Y )⊤·U(t)
(18)
and our dynamics becomes
d∥u(t)−Y ∥2
2
dt
≤−λ0∥u(t)−Y ∥2
2−2λ"
W,0.8192918192918193,m (u(t)−Y )⊤·U(t)
W,0.8205128205128205,≤−(λ0+ 2λ
W,0.8217338217338217,"m )∥u(t)−Y ∥2
2
(19)"
W,0.8229548229548229,showing that d
W,0.8241758241758241,"dt
 
e(λ0+2λ/m)t∥u(t)−Y ∥2
2

≤0. Thus e(λ0+2λ/m)t∥u(t)−Y ∥2
2 is a decreasing function with
respect to t, and we have
∥u(t)−Y ∥2
2≤e−(λ0+2λ/m)t∥u(0)−Y ∥2
2.
As for bounding ∥wr(t)−wr(0)∥2, we use the same method as in Lemma 3.3 of Du et al. (2019). Thus we
complete the proof."
W,0.8253968253968254,"Finally, by combining Lemma H.1, H.2, H.3 and H.4, we have the following convergence result."
W,0.8266178266178266,"Theorem H.5 (Convergence of gradient flow). Suppose λ0>0, m=poly(n,1/λ0,1/δ), then with probability
at least 1−δ over the randomness of initialization, we have
∥u(t)−Y ∥2
2≤e−(λ0+2λ/m)t∥u(0)−Y ∥2
2."
W,0.8278388278388278,"The above theorem shows that in the over-parameterized setting (when m is large enough), the training loss
of the kernel ridge regression problem define in Eq. (8) converges to 0 in a linear rate. By comparing our
Theorem H.5 with Theorem 3.2 in Du et al. (2019), we can find that the introducing of regularization term
makes the convergence speed faster, though the improvement is limited. Further notice that in Section G we
prove the equivalence between minimizing the dropout loss and the kernel ridge regression problem. So we
conclude our results as:"
W,0.8290598290598291,"The introducing of sparsity into neural network makes the convergence speed faster, but the improvement is
limited due to the over-parameterized scheme."
W,0.8302808302808303,Published as a conference paper at ICLR 2022
W,0.8315018315018315,"I
METHOD DETAILS"
W,0.8327228327228328,We describe some details of our method.
W,0.833943833943834,"I.1
COMPUTE BUDGET ALLOCATION"
W,0.8351648351648352,"We describe here a procedure to compute the budget allocation based on our cost model. This procedure is
more complicated than our simple rule of thumb in Section 3.3, and tend to produce the same allocation. For
completeness, we include the procedure here for the interested reader."
W,0.8363858363858364,"Given a parameter budget B, we find the density of each layer type that minimize the models’ total cost of
matrix multiplication. For example, in Transformers, let da and dm be the density of the attention and the
MLP layers. Let s be the sequence length and d be the feature size. The attention layer with density da will
cost da(n2+nd), and the fully connected layers with density dm will cost 2dmnd. We then set da and dm to
minimize the total cost while maintaining the parameter budget:
minimizeδa,δmδa(n2+nd)+2δmnd
subject to
# of trainable parameters≤B.
(20)
As this is a problem with two variables, we can solve it in closed form."
W,0.8376068376068376,"I.2
LOW-RANK IN ATTENTION"
W,0.8388278388278388,"In Section 3.3, we describe how to use the sparsity pattern from flat block butterfly and the low-rank term for
weight matrices. This applies to the linear layer in MLP and the projection steps in the attention."
W,0.8400488400488401,"We also use the sparse + low-rank structure in the attention step itself. Chen et al. (2021) describes a general
method to combine sparse and low-rank attention, where one uses the sparse component to discount the
contribution from the low-rank component to ensure accurate approximation of the attention matrix."
W,0.8412698412698413,"We follow a simpler procedure, which in practice yields similar performance. We use a restricted version of
low-rank of the form a “global” sparsity mask (as shown in Fig. 12). Indeed, a sparse matrix whose sparsity
pattern follows the “global” pattern is a sum of two sparse matrices, one containing the “horizontal” global
components and one containing the “vertical” components. Let w be the width of each of those components,
then each of them has rank at most w. Therefore, this sparse matrix has rank at most 2w, and is low-rank (for
small w)."
W,0.8424908424908425,"We also make the global component block-aligned (i.e., set w to be a multiple of the smallest supported block
size such as 32) for hardware efficiency."
W,0.8437118437118437,"I.3
COMPARISON TO OTHER SPARSITY PATTERNS FOR ATTENTION"
W,0.8449328449328449,"In the context of sparse attention, other sparsity patterns such as BigBird and Longformer also contain a
“global” component, analogous to our low-rank component. Their “local” component is contained in the block
diagonal part of the flat block butterfly sparsity pattern."
W,0.8461538461538461,"The main difference that we do not use the random components (e.g., BigBird), and the diagonal strides from
flat block butterfly are not found in BigBird or Longformer. Moreover, we apply the same sparsity pattern (+
low-rank) to the linear layers in the MLP and the projection step in attention as well, allowing our method to
target most neural network layers, not just the attention layer."
W,0.8473748473748474,"I.4
SPARSITY MASK FOR RECTANGULAR MATRICES"
W,0.8485958485958486,"We have described the sparsity masks from flat block butterfly for square matrices. For rectangular weight
matrices, we simply “stretch” the sparsity mask. The low-rank component applies to both square and
rectangular matrices (as shown in Fig. 10). We have found this to work consistently well across tasks."
W,0.8498168498168498,Published as a conference paper at ICLR 2022
W,0.851037851037851,Square Flat Block Butterfly + + + +
W,0.8522588522588522,Rectangular Flat Block Butterfly
W,0.8534798534798534,Figure 10: Sparsity Mask for Rectangular Matrices.
W,0.8547008547008547,"J
BENCHMARKING OF BUTTERFLY MULTIPLY"
W,0.8559218559218559,"We validate that flat butterfly matrices (sum of factors) can speed up multiplication on GPUs compared to
butterfly matrices (products of factors)."
W,0.8571428571428571,"Consider the matrix M ∈Rn×n that can be written as products of butterfly factors of strides of up k (a power
of 2), with residual connection:
M =(I+λB(n)
k )(I+λB(n)
k/2)...(I+λB(n)
2 ).
The first-order approximation of M has the form of a flat butterfly matrix with maximum stride k (Section 3.2):
Mflat=I+λ(B(n)
2 +···+B(n)
k/2+B(n)
k )."
W,0.8583638583638583,"Notice that M is a product of log2k factors, each has 2n nonzeros, so multiplying M by a input vector x costs
O(nlogk) operations (by sequentially multiplying x by the factors of M). The flat version Mflat is a sparse
matrix with O(nlogk) nonzeros as well, and the cost of multiplying Mflatx is also O(nlogk). However, in
practice, multiplying Mflatx is much more efficient on GPUs than multiplying Mx because of the ease of
parallelization."
W,0.8595848595848596,"We measure the total time of forward and backward passes of multiplying either Mflatx and compare to that
of multiplying Mx for different maximum strides, as shown in Fig. 11. We see that “flattening” the products
brings up to 3× speedup."
W,0.8608058608058609,"2
4
8
16
32
Maximum stride 1 2 3"
W,0.8620268620268621,Speedup (x)
W,0.8632478632478633,"Figure 11: Speedup of multiplying Mflatx compared to multiplying Mx. Flattening the products yields up
3× speedup."
W,0.8644688644688645,"We use matrix size 1024×1024 with block size 32. The input batch size is 2048. We use the block sparse
matrix multiply library from https://github.com/huggingface/pytorch_block_sparse.
The speed measurement is done on a V100 GPU."
W,0.8656898656898657,Published as a conference paper at ICLR 2022
W,0.8669108669108669,"Local
Global (Low-rank)
Butterfly
Random"
W,0.8681318681318682,"Figure 12: Sparsity pattern candidate components: Local corresponds to local interaction of neighboring
elements; Global (low-rank) involves the interaction between all elements and a small subset of elements;
Butterfly captures the interaction between elements that are some fixed distance apart; Random is common in
the pruning literature."
W,0.8693528693528694,"K
EXHAUSTED SEARCHING SPARSITY PATTERNS FOR EFFICIENT SPARSE
TRAINING"
W,0.8705738705738706,"We describe here our early exploration of searching among different sparsity patterns that has been proposed
in the literature. We use a metric derived from the NTK, which has emerged as one of the standard metric to
predict the training and generalization of the model. We consistently found the butterfly + low-rank pattern to
perform among the best."
W,0.8717948717948718,"In Appendix K.1, we describe the challenges of selecting sparsity patterns for every model components using
the a metric derived from the NTK, followed by our approaches. Then in , we describe details of empirical
NTK computation, which is an important step in our method implementation. Last, in Appendix K.3, we
highlight important properties of our method – it rediscovers several classical sparsity patterns, and the sparse
models can inherit the training hyperparamters of the dense models, reducing the need for hyperparameters
tuning."
W,0.873015873015873,"K.1
CHALLENGES AND APPROACHES"
W,0.8742368742368742,"Challenge 1: We seek sparsity patterns for each model components that can closely mimic the training
dynamics of the dense counterpart. As mentioned in Theorem D.9, it is NP-hard to find the optimal sparse
matrix approximation. Although NTK provides insights and measurement on the “right” sparse model,
bruteforcely computing NTK for one-layer models with all sparsity patterns is still infeasible."
W,0.8754578754578755,"Approach 1: Sparsity Pattern Candidates. To address the above challenge, we design our search space to be
a limited set of sparsity pattern candidates, each is either a component visualized in Fig. 12 or the combination
of any two of them. These components encompass the most common types of sparsity pattern used, and can
express We provide the intuition behind these sparsity components:"
W,0.8766788766788767,"• Local: this block-diagonal component in the matrix corresponds to local interaction of neighboring elements.
This has appeared in classical PDE discretization (Collins and Angel, 1971), and has been rediscovered in
Longformer and BigBird attention patterns.
• Global: this component involves interaction between all elements and a small subset of elements (i.e.,
“global” elements). This global pattern is low-rank, and this sparse + low-rank structure is common in data
science (Udell and Townsend, 2019), and rediscovered in Longformer and BigBird patterns as well.
• Butterfly: this component corresponds to interaction between elements that are some fixed distance apart.
The many divide-and-conquer algorithms, such as the classical fast Fourier transform (Cooley and Tukey,
1965), uses this pattern at each step. Butterfly matrices reflects this divide-and-conquer structure, and hence
this sparsity component. The sparse transformer (Child et al., 2019) also found this pattern helpful for
attention on image data.
• Random: this component is a generalization of sparsity patterns found in one-shot magnitude, gradient,
or momentum based pruning (Lee et al., 2018). Note that at network initialization, they are equivalent to
random sparsity."
W,0.8778998778998779,"Challenge 2: Even with a fixed pool of sparsity patterns for each layer, if the model has many layers, the
number of possible layer-pattern assignments is exponentially large."
W,0.8791208791208791,"Approach 2: To further reduce the search space, we constrain each layer type (attention, MLP) to have the
same sparsity pattern. For example, if there are 10 patterns and 2 layer types, the candidate pool is 102=100
combinations."
W,0.8803418803418803,Published as a conference paper at ICLR 2022
W,0.8815628815628815,Algorithm 2 Model Sparsification
W,0.8827838827838828,"1: Input: model schema Ω, compute budget B, dataset subset X, sparsity mask candidate set C.
2: Kdense←NTK(fθ,X).
▷Eq. (22)
3: output sparsity mask assignment sout, dmin←inf
4: for M1,...,M|Ω|∈C|Ω| do
▷Enumerate all sparsity mask candidate combinations
5:
Let s be the sparsity mask assignment (ti,ri,mi,ni)→Mi.
6:
if TotalCompute(s)<B then
▷Eq. (21), Check if masks satisfy budget constraint
7:
Let Ms be the flattened sparse masks
8:
Ksparse←NTK(fθ◦Ms,X)
9:
ds←DISTANCE(Kdense,Ksparse)
▷Eq. (22)
10:
if dmin>ds then
11:
dmin←ds, sout←s
12:
end if
13:
end if
14: end for
15: return sout
▷Return sparsity mask assignment"
W,0.884004884004884,"Challenge 3: Computing the empirical NTK on the whole dataset is expensive in time and space, as it scales
quadratically in the dataset size."
W,0.8852258852258852,"Approach 3: We compute the empirical NTK on a randomly chosen subset of the data (i.e., a principal
submatrix of the empirical NTK matrix). In our experiments, we verify that increasing the subset size beyond
1000 does not change the choices picked by the NTK heuristic. The subsampled empirical NTK can be
computed within seconds or minutes."
W,0.8864468864468864,"K.2
ALGORITHM DESCRIPTION"
W,0.8876678876678876,"Our method targets GEMM-based neural networks, which are networks whose computation is dominated
by general matrix multiplies (GEMM), such as Transformer and MLP-Mixer. As a result, we can view the
network as a series of matrix multiplies. We first define:"
W,0.8888888888888888,"• Model schema:
a list of layer types t (e.g., attention, linear layers in MLP), number of lay-
ers r of that type, and dimension of the matrix multiplies m × n.
We denote it as Ω=
{(t1,r1,m1,n1),...,(t|Ω|,r|Ω|,m|Ω|,n|Ω|)}.
• A mask M of dimension m×n is a binary matrix {0,1}m×n. The compute of a mask is the total number
of ones in the matrix: compute(M)=P"
W,0.8901098901098901,"i,jMij.
• A sparsity pattern Pm×n for matrix dimension m×n is a set of masks {M1,...,M|P|}, each of dimension
m×n.
• A sparsity mask assignment is a mapping from a model schema Ωto masks M belonging to some sparsity
pattern P: s: (t,r,m,n)→M.
• Given a set of sparsity patterns P1,...,Pk, the set of sparsity mask candidate C is the union of sparsity masks
in each of Pi: C =∪Pi
• A sparsity pattern assignment s satisfies the compute budget B if:
TotalCompute(s):=
X"
W,0.8913308913308914,"layer type l
compute(s(t,r,m,n))≤B.
(21)"
W,0.8925518925518926,"• Let θ be the flattened vector containing the model parameters, and let Ms be the flattened vector containing
the sparsity mask by the sparsity mask assignment s. Let fθ(x) be the output of the dense network with
parameter θ and input x. Then the output of the sparse network is fθ◦Ms(x).
• The empirical NTK of a network fθ on a data subset X ={x1,...,x|X|} is a matrix of size |X|×|X|:"
W,0.8937728937728938,"NTK(fθ,X)i,j =
∂fθ(xi)"
W,0.894993894993895,"∂θ
,∂fθ(xj) ∂θ"
W,0.8962148962148963,".
(22)"
W,0.8974358974358975,"The formal algorithm to assign the sparsity mask to each layer type is described in Algorithm 2. The main idea
is that, as the set of sparsity mask candidate is finite, we can enumerate all possible sparsity mask assignment
satisfying the budget and pick the one with the smallest NTK distance to the dense NTK. In practice, we can
use strategies to avoid explicitly enumerating all possible sparsity mask, e.g. for each sparsity pattern, we can
choose the largest sparse mask that fits under the budget."
W,0.8986568986568987,"K.3
METHOD PROPERTIES: REDISCOVERING CLASSICAL SPARSITY PATTERNS, NO ADDITIONAL
HYPERPARAMETER TUNING"
W,0.8998778998778999,"When applied to the Transformer architecture, among the sparsity components described in Appendix K.1,
the NTK-guided heuristic consistently picks the local and global components for both the attention and"
W,0.9010989010989011,Published as a conference paper at ICLR 2022
W,0.9023199023199023,"MLP layers. Moreover, the butterfly component is also consistently picked for image data, reflecting the 2D
inductive bias in this component9. While some of these patterns have been proposed for sparse attention, it is
surprising that they are also picked for the MLP layers. The most popular type of sparsity pattern in MLP
layers is top-k (in magnitude or gradient, which at initialization is equivalent to random sparsity). We have
proved that lower NTK difference results in better generalization bound for the sparse model. As expected, we
observe that this allows the sparse model to use the same hyperparamters (optimizer, learning rate, scheduler)
as the dense model (Section 5)."
W,0.9035409035409036,"9Convolution (commonly used in image data) can be written in terms of the fast Fourier transform, which has this
same sparse pattern at each step of the algorithm"
W,0.9047619047619048,Published as a conference paper at ICLR 2022
W,0.905982905982906,"L
EXPERIMENT DETAILS"
W,0.9072039072039072,"L.1
DATASETS"
W,0.9084249084249084,"• Cifar10 (Krizhevsky et al., 2009) consists of 60000 coloured images of resolution 32 × 32. Each
of them belong to one of 10 classes, including airplanes, cars, birds, cats, deer, dogs, frogs, horses,
ships, and trucks. Among these, 50000 images are allocated to be the training set and 10000 images
the testing set.
• Cifar100 (Krizhevsky et al., 2009) is similar to Cifar10. It also consists of images of resolution 32
× 32. In total, there are 60000 images, each of which belongs to one of 100 classes. Each of the
100 classes has 500 images in training set and 100 images in testing set.
• ImageNet1K (Russakovsky et al., 2015) spans 1000 object classes, containing 1,281,167 training
images, 50,000 validation images and 100,000 test images. Although images are collected in
different resolutions, in practice they are generally reshaped and cropped into 224 × 224.
• WikiText-103 (Merity et al., 2016) contains articles from the wikipedia page. It extracts verified
articles from Wikipedia, which add up to over 100 million tokens. Compared to other datasets, such
as Penn Treebank (PTB) (Taylor et al., 2003), WikiText features a larger vocabulary and preserves
original upper/lower cases, punctuation and numbers."
W,0.9096459096459096,"L.2
MODEL CONFIGURATIONS AND HYPERPARAMETER"
W,0.9108669108669109,We summarize the details required to replicate our experiments below.
W,0.9120879120879121,"Baseline Model: Except for dense model. We choose our baselines for each experiment base on the following.
RigL aims to sparsify model weights/parameters, so we use it as a baseline in MLP-based models (Mixer).
BigBird focuses on attention matrices, so we used it as a baseline in Transformer-based models (ViT, GPT-2)."
W,0.9133089133089133,"L.2.1
IMAGE CLASSIFICATION"
W,0.9145299145299145,Table 1: Configuration of the Cifar10 experiments.
W,0.9157509157509157,"Model
Optimizer
Weight Decay
Learning Rate
Drop Path
Warmup/Epoch"
W,0.9169719169719169,"ViT-Small
AdamW
0.05
0.0005
0.1
5/300
Pixelfly-ViT-Small
AdamW
0.05
0.0005
0
5/300
ViT-Base
AdamW
0.05
0.0005
0.1
5/300
Pixelfly-ViT-Base
AdamW
0.05
0.0005
0
5/300"
W,0.9181929181929182,"Mixer-Small
AdamW
0.1
0.0005
0.1
5/300
Pixelfly-Mixer-Small
AdamW
0.1
0.0005
0
5/300
Mixer-Base
AdamW
0.1
0.0005
0.1
5/300
Pixelfly-Mixer-Base
AdamW
0.1
0.0005
0
5/300"
W,0.9194139194139194,"Model
Optimizer
Weight Decay
Learning Rate
Drop Path
Warmup/Epoch"
W,0.9206349206349206,"ViT-Small
AdamW
0.05
0.0005
0.1
5/300
Pixelfly-ViT-Small
AdamW
0.05
0.0005
0
5/300
ViT-Base
AdamW
0.05
0.0005
0.1
5/300
Pixelfly-ViT-Base
AdamW
0.05
0.0005
0
5/300"
W,0.9218559218559218,"Mixer-Small
AdamW
0.1
0.0005
0.1
5/300
Pixelfly-Mixer-Small
AdamW
0.1
0.0005
0
5/300
Mixer-Base
AdamW
0.1
0.0005
0.1
5/300
Pixelfly-Mixer-Base
AdamW
0.1
0.0005
0
5/300"
W,0.9230769230769231,Table 2: Configuration of the Cifar100 experiments
W,0.9242979242979243,"We report more details on the models, including number of parameters and FLOPs, in Table 4."
W,0.9255189255189256,"We follow the naming convention in the Vision Transformer paper and MLP-Mixer paper. In particular, ViT-S
and ViT-B refers to the small and base ViT models respectively, and 16 refers to the patch size of 16x16. The
MLP-Mixer models follows the same convention."
W,0.9267399267399268,"L.2.2
LANGUAGE MODELING"
W,0.927960927960928,"We report more details on the models, including number of parameters and FLOPs, in Table 5 and Table 6."
W,0.9291819291819292,Published as a conference paper at ICLR 2022
W,0.9304029304029304,"Model
Optimizer
Weight Decay
Learning Rate
Drop Path
Warmup/Epoch"
W,0.9316239316239316,"ViT-Small
AdamW
0.05
0.001
0.1
5/300
Pixelfly-ViT-Small
AdamW
0.05
0.001
0
5/300
ViT-Base
AdamW
0.05
0.001
0.1
5/300
Pixelfly-ViT-Base
AdamW
0.05
0.001
0
5/300"
W,0.9328449328449329,"Mixer-Small
AdamW
0.1
0.001
0.1
5/300
Pixelfly-Mixer-Small
AdamW
0.1
0.001
0
5/300
Mixer-Base
AdamW
0.1
0.001
0.1
5/300
Pixelfly-Mixer-Base
AdamW
0.1
0.001
0
5/300"
W,0.9340659340659341,Table 3: Configuration of the ImageNet experiment
W,0.9352869352869353,"Table 4: The performance of Pixelfly and ViT or MLP-Mixer on the ImageNet benchmarks, including the
number of parameters and FLOPs. We measure the accuracy and the training time speedup (on ImageNet)
compared to the dense model."
W,0.9365079365079365,"Model
ImageNet top-1 acc.
Speedup
Params
FLOPs"
W,0.9377289377289377,"Mixer-S/16
72.4
-
18.5M
3.8G
Pixelfly-Mixer-S/16
72.6
1.7×
5.9M
1.3G
Mixer-B/16
75.6
-
59.9M
12.6G
Pixelfly-Mixer-B/16
76.3
2.3×
17.4M
4.3G"
W,0.938949938949939,"ViT-S/16
77.7
-
48.8M
9.9G
Pixelfly-ViT-S/16
77.5
1.9×
16.9M
3.6G
ViT-B/16
78.5
-
86.6M
17.6G
Pixelfly-ViT-B/16
78.6
2.0×
28.2M
6.1G"
W,0.9401709401709402,"Table 5: The performance of Pixelfly, BigBird and GPT-2-Small on WikiText-103, including the number of
parameters and FLOPs. We measure the perplexity and the training speed up."
W,0.9413919413919414,"Model
WikiText-103 (ppl)
Speedup
Params
FLOPS
GPT-2-Small
22.2
-
117M
48.4G
BigBird
23.3
0.96×
117M
40.2G
Pixelfly
22.5
2.1×
68M
18.5G"
W,0.9426129426129426,"GPT-2-Medium
20.9
-
345 M
168G
BigBird
21.5
1.1×
345 M
134G
Pixelfly
21.0
2.5×
203M
27G"
W,0.9438339438339438,Table 6: Configuration of the WikiText103 experiments
W,0.945054945054945,"Model
Optimizer
Weight Decay
Learning Rate
Dropout
Warmup/Epoch"
W,0.9462759462759462,"GPT-2-Small
Adam
0.1
0.0001
0.1
5/100
Pixelfly
Adam
0.1
0.0001
0.1
5/100"
W,0.9474969474969475,"L.3
MEASURING EMPIRICAL NTK"
W,0.9487179487179487,"The Empirical NTK is a rough estimation of the real NTK, in which the width of the neural net goes to infinity.
As the width grows, the kernel gets closer to its infinite-width limit. Fortunately, both our models of interest,
MLP-Mixer and Vision Transformer, are wide and overly parameterized. Therefore they are only one step
away from the real NTK domain. This allows us to use the Empirical NTK to approximately predict their
training behaviors."
W,0.9499389499389499,"As described in equation 22, we first compute the gradient of each data sample, then we compute pair-wise
product to construct the Empirical NTK. Although we use a relatively small dataset, it’s still expensive to build
a kernel for large models, such as ViTs and MLP-Mixers. In practice, we find that it’s sufficient to compute
kernels for a subsampled dataset."
W,0.9511599511599511,Published as a conference paper at ICLR 2022
W,0.9523809523809523,"MLP-Mixer and Vision Transformer each represent one type of module of interest for our sparsification. In
MLP-Mixer, we study the sparse behavior of the Linear module, whereas, in Vision Transformer, we mainly
focus on sparsifying attention. All models are first sparsified to around 10% of the original dense compute.
Then we compare their NTK kernels with their original dense kernel. We run three random seeds to eliminate
noise, i.e., three different initializations for each pair of configurations. We report the mean relative difference
between the kernels with respect to the norm of the dense kernel."
W,0.9536019536019537,"L.4
TRANSFER LEARNING EXPERIMENTS"
W,0.9548229548229549,"We conduct extended experiments to test the generalization of our pretrained sparse models on downstream
tasks. Specifically, we finetune Pixelfly pretrained model (ImageNet) on CIFAR-10 and show that it get
99.03% accuracy compared to 98.77% on our pretrained dense ViT-B/16 model. In addition, we see more
than 2× speed up on downstream task fine-tuning process as well."
W,0.9560439560439561,"L.5
MICROBENCHMARKING"
W,0.9572649572649573,"In this section, we perform microbenchmarking on a 4K× 4K sparse matrix multiplication. We aim to show
that Pixelfly patterns are far more hardware friendly than random patterns. For a 4K×4K matrix, expected
density is the number of non-zero entries/(4K×4K) ; actual density is the number of accessed entries/(4K×4K),
e.g. even if there is only one non-zero, 32×32 entries would be accessed because the hardware block size is
32×32."
W,0.9584859584859585,"When random patterns are generated with small block size, (e.g 1×1, 2×2), the resources, such as memory
access and computes(denoted by Actual Density), required to compute a random sparse matrix of density
1.25% are equivalent to computing a dense matrix multiplication. This is further reflected in the latency: As
pattern block size shrinks, deviating from the hardware block size of 32×32, the random patterns’ latency
worsens, whereas the Pixelfly remains efficient. Vanilla Butterfly is 5× slower than Pixelfly as expected,
because (1) it does not take advantage of the hardware property – not structured sparsity(2) it is a series of
products."
W,0.9597069597069597,"Pattern
Block size
Expected Density
Actual Density
Latency(ms)"
W,0.960927960927961,"1×1
1.25%
100%
9.4
2×2
2.5%
99.84%
9.3
4×4
5%
96.24%
9.04
Random
6×6
10%
93.66%
8.8
8×8
20%
81.89%
7.7
16×16
40%
34.52%
3.3
32×32
80%
10.15%
1.0"
W,0.9621489621489622,"Butterfly
1×1
10%
62.50%
5.2
1×1
1.25%
4.62%
0.48
2×2
2.5%
5.38%
0.56
4×4
5%
6.13%
0.63
Pixelfly
6×6
10%
9.64%
0.96
8×8
10%
10.58%
1.05
16×16
10%
11.30%
1.12
32×32
10%
10.58%
1.04"
W,0.9633699633699634,"Table 7: Microbenchmarking of different patterns. Given GPU processes the matrix block by block of size 32
× 32, random block pattern’s latency increases as the block size shrinks, while Pixelfly remains efficient. We
measure the latency by averaging 100 runs of batch size 4096 for each configuration."
W,0.9645909645909646,"L.6
EFFICIENT IMPLEMENTATION OF PIXELFLY"
W,0.9658119658119658,"We run all of our experiments on V100 GPUs. We rely on efficient implementation of block sparse
matrix multiply and block sparse attention from the libraries Triton (https://github.com/openai/
triton) and https://github.com/huggingface/pytorch_block_sparse. For the low-
rank part, we rely on efficient (dense) matrix multiplies from cuBLAS. In particular, to multiply the input x
by the low-rank matrix UV ⊤, we multiply U(V ⊤x)."
W,0.967032967032967,"We keep the same number of training epochs as that of the dense models (e.g., on ImageNet, the dense model
and the Pixelfly model are trained for 300 epochs). The training speedup of the Pixelfly models is due to
faster time per epoch."
W,0.9682539682539683,Published as a conference paper at ICLR 2022
W,0.9694749694749695,"We do not use 2:4 sparsity (available on Ampere GPUs such as A100). Such fine-grained sparsity is orthogonal
to our approach, and we expect that future work incorporating both 2:4 sparsity and block sparsity to yield
further speedup."
W,0.9706959706959707,"L.7
ABLATION: SPEED-ACCURACY TRADEOFF OF PIXELFLY"
W,0.9719169719169719,"We conduct an ablation experiment to examine the speed-accuracy trade of Pixelfly: on the ImageNet dataset
and the Mixer-B/16 model, we replace the dense matrices with flat block butterfly + low-rank matrices, while
varying the compute / parameter budget. We plot the speed-accuracy tradeoff in Fig. 13."
W,0.9731379731379731,"1.0
1.5
2.0
2.5
Training speedup compared to dense model 75.0 75.5 76.0 76.5"
W,0.9743589743589743,ImageNet1k top-1 accuracy
W,0.9755799755799756,"Mixer-B/16 (dense)
Pixelfly Mixer-B/16 (block sparse)"
W,0.9768009768009768,"Figure 13: Speed-accuracy tradeoff of Pixelfly on ImageNet classification, with Mixer-B/16 as the dense
model. Pixelfly maintains or exceeds the accuracy of the dense model, up to around 2.3× speedup (or around
30% of the number of parameters). Performance degrades when the Pixelfly model has fewer than 30% of the
number of parameters."
W,0.978021978021978,"L.8
COMPARISON AGAINST ORIGINAL BUTTERFLY"
W,0.9792429792429792,"We compare Pixelfly against original Butterfly matrices (Dao et al., 2020) on the ImageNet dataset and
Mixer-B/16 dense model. We present the results in Table 8. We notice another benefit of Pixelfly compared to
Butterfly: it trains more stably and requires less careful initialization. Since Butterfly is a product of many
factors, it requires careful initialization, otherwise the activation and gradient will be very large or very small."
W,0.9804639804639804,Table 8: The performance of Pixelfly and original Butterfly on MLP-Mixer on the ImageNet benchmarks.
W,0.9816849816849816,"Model
ImageNet top-1 acc.
Speedup
Params
FLOPs"
W,0.9829059829059829,"Mixer-B/16
75.6
-
59.9M
12.6G
Butterfly-Mixer-B/16
76.1
0.8×
17.4M
4.3G
Pixelfly-Mixer-B/16
76.3
2.3×
17.4M
4.3G"
W,0.9841269841269841,Published as a conference paper at ICLR 2022
W,0.9853479853479854,"M
EXTENDED RELATED WORK"
W,0.9865689865689866,"In this section, we extend the related works referenced in the main paper and discuss them in detail."
W,0.9877899877899878,"M.1
NEURAL PRUNING"
W,0.989010989010989,"Our work is loosely related to neural network pruning. By iteratively eliminating neurons and connections,
pruning has seen great success in compressing complex models.Han et al. (2015a;b) put forth two naive
but effective algorithms to compress models up to 49x and maintain comparable accuracy. Li et al. (2016)
employ filter pruning to reduce the cost of running convolution models up to 38 %, Lin et al. (2017) prunes
the network at runtime, hence retaining the flexibility of the full model. Dong et al. (2017) prunes the network
locally in a layer by layer manner. Sanh et al. (2020) prunes with deterministic first-order information, which
is more adaptive to pretrained model weights. Lagunas et al. (2021) prunes transformers models with block
sparsity pattern during fine-tuning, which leads to real hardware speed up while maintaining the accuracy.
Zhu and Gupta (2017) finds large pruned sparse network consistently outperform the small dense networks
with the same compute and memory footprints. Although both our and all the pruning methods are aiming to
produce sparse models, we differ in our emphasis on the overall efficiency, whereas pruning mostly focuses
on inference efficiency and disregards the cost in finding the smaller model."
W,0.9902319902319903,"M.2
LOTTERY TICKET HYPOTHESIS"
W,0.9914529914529915,"Models proposed in our work can be roughly seen as a class of manually constructed lottery tickets. Lottery
tickets Frankle and Carbin (2018) are a set of small sub-networks derived from a larger dense network, which
outperforms their parent networks in convergence speed and potentially in generalization. A huge number
of studies are carried out to analyze these tickets both empirically and theoretically: Morcos et al. (2019)
proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with the
specialized lottery tickets; Frankle et al. (2019) improves the stability of the lottery tickets by iterative pruning;
Frankle et al. (2020) found that subnetworks reach full accuracy only if they are stable against SGD noise
during training; Orseau et al. (2020) provides a logarithmic upper bound for the number of parameters it
takes for the optimal sub-networks to exist; Pensia et al. (2020) suggests a way to construct the lottery ticket
by solving the subset sum problem and it’s a proof by construction for the strong lottery ticket hypothesis.
Furthermore, follow-up works (Liu and Zenke, 2020; Wang et al., 2020; Tanaka et al., 2020) show that we
can find tickets without any training labels."
W,0.9926739926739927,"M.3
NEURAL TANGENT KERNEL"
W,0.9938949938949939,"Our work rely heavily on neural tangent kernel in theoretical analysis. Neural Tangent Kernel Jacot et al.
(2018) is first proposed to analyse the training dynamic of infinitely wide and deep networks. The kernel is
deterministic with respect to the initialization as the width and depth go to infinity, which provide an unique
mathematical to analyze deep overparameterized networks. Couples of theoretical works are built based upon
this: Lee et al. (2019) extend on the previous idea and prove that finite learning rate is enough for the model to
follow NTK dynamic. Arora et al. (2019b) points out that there is still a gap between NTK and the real finite
NNs. Cao and Gu (2020) sheds light on the good generalization behavior of overparameterized deep neural
networks. Arora et al. (2019a) is the first one to show generalization bound independent of the network size.
Later, some works reveal the training dynamic of models of finite width, pointing out the importance of width
in training: Hayou et al. (2019) analyzes stochastic gradient from the stochastic differential equations’ point of
view; Based on these results, we formulate and derive our theorems on sparse network training."
W,0.9951159951159951,"M.4
OVERPARAMETERIZED MODELS"
W,0.9963369963369964,"Our work mainly targets overparameterized models. In Nakkiran et al. (2019), the double descendent
phenomenon was observed. Not long after that, d’Ascoli et al. (2020) discover the triple descendent
phenomenon. It’s conjectured in both works that the generalization error improves as the parameter count
grows. On top of that, Arora et al. (2018) speculates that overparameterization helps model optimization, and
without ""enough"" width, training can be stuck at local optimum. Given these intuitions, it’s not surprising
that the practitioning community is racing to break the record of the largest parameter counts: The two large
language models, GPT-2 and GPT-3 (Radford et al., 2019; Brown et al., 2020), are pushing the boundary on
text generation and understanding; Their amazing zero-shot ability earn them the title of foundation models
(Bommasani et al., 2021). On the computer vision side, Dosovitskiy et al. (2020); Tolstikhin et al. (2021);
Zhai et al. (2021) push the top-1 accuracy on various vision benchmarks to new highs after scaling up to 50
times the parameters; Naumov et al. (2019) shows impressive results on recommendation with a 21 billion"
W,0.9975579975579976,Published as a conference paper at ICLR 2022
W,0.9987789987789988,"large embedding; Jumper et al. (2021) from DeepMind solve a 50 year old grand challenge in protein research
with a 46-layer Evoformer. In our work, we show that there is a more efficient way to scale up model training
through sparsification and double descent only implies the behavior of the dense networks."
