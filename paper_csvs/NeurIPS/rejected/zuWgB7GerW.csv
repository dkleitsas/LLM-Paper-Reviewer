Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006090133982947625,"We show that deep neural networks (DNNs) can efficiently learn any composition
1"
ABSTRACT,0.001218026796589525,"of functions with bounded F1-norm, which allows DNNs to break the curse of
2"
ABSTRACT,0.0018270401948842874,"dimensionality in ways that shallow networks cannot. More specifically, we
3"
ABSTRACT,0.00243605359317905,"derive a generalization bound that combines a covering number argument for
4"
ABSTRACT,0.0030450669914738123,"compositionality, and the F1-norm (or the related Barron norm) for large width
5"
ABSTRACT,0.0036540803897685747,"adaptivity. We show that the global minimizer of the regularized loss of DNNs can
6"
ABSTRACT,0.004263093788063338,"fit for example the composition of two functions f ∗= h ◦g from a small number
7"
ABSTRACT,0.0048721071863581,"of observations, assuming g is smooth/regular and reduces the dimensionality (e.g.
8"
ABSTRACT,0.0054811205846528625,"g could be the modulo map of the symmetries of f ∗), so that h can be learned in
9"
ABSTRACT,0.0060901339829476245,"spite of its low regularity. The measures of regularity we consider is the Sobolev
10"
ABSTRACT,0.006699147381242387,"norm with different levels of differentiability, which is well adapted to the F1 norm.
11"
ABSTRACT,0.007308160779537149,"We compute scaling laws empirically and observe phase transitions depending on
12"
ABSTRACT,0.007917174177831911,"whether g or h is harder to learn, as predicted by our theory.
13"
INTRODUCTION,0.008526187576126675,"1
Introduction
14"
INTRODUCTION,0.009135200974421437,"One of the fundamental features of DNNs is their ability to generalize even when the number of
15"
INTRODUCTION,0.0097442143727162,"neurons (and of parameters) is so large that the network could fit almost any function [46]. Actually
16"
INTRODUCTION,0.010353227771010963,"DNNs have been observed to generalize best when the number of neurons is infinite [8, 21, 20].
17"
INTRODUCTION,0.010962241169305725,"The now quite generally accepted explanation to this phenomenon is that DNNs have an implicit
18"
INTRODUCTION,0.011571254567600487,"bias coming from the training dynamic where properties of the training algorithm lead to networks
19"
INTRODUCTION,0.012180267965895249,"that generalize well. This implicit bias is quite well understood in shallow networks [11, 36], in
20"
INTRODUCTION,0.012789281364190013,"linear networks [24, 30], or in the NTK regime [28], but it remains ill-understood in the general deep
21"
INTRODUCTION,0.013398294762484775,"nonlinear case.
22"
INTRODUCTION,0.014007308160779537,"In both shallow networks and linear networks, one observes a bias towards small parameter norm
23"
INTRODUCTION,0.014616321559074299,"(either implicit [12] or explicit in the presence of weight decay [42]). Thanks to tools such as the
24"
INTRODUCTION,0.015225334957369063,"F1-norm [5], or the related Barron norm [44], or more generally the representation cost [14], it is
25"
INTRODUCTION,0.015834348355663823,"possible to describe the family of functions that can be represented by shallow networks or linear
26"
INTRODUCTION,0.016443361753958587,"networks with a finite parameter norm. This was then leveraged to prove uniform generalization
27"
INTRODUCTION,0.01705237515225335,"bounds (based on Rademacher complexity) over these sets [5], which depend only on the parameter
28"
INTRODUCTION,0.01766138855054811,"norm, but not on the number of neurons or parameters.
29"
INTRODUCTION,0.018270401948842874,"Similar bounds have been proposed for DNNs [7, 6, 39, 33, 25, 40], relying on different types of
30"
INTRODUCTION,0.018879415347137638,"norms on the parameters of the network. But it seems pretty clear that we have not yet identified
31"
INTRODUCTION,0.0194884287454324,"the ‘right’ complexity measure for deep networks, as there remains many issues: these bounds are
32"
INTRODUCTION,0.020097442143727162,"typically orders of magnitude too large [29, 23], and they tend to explode as the depth L grows [40].
33"
INTRODUCTION,0.020706455542021926,"Two families of bounds are particularly relevant to our analysis: bounds based on covering numbers
34"
INTRODUCTION,0.021315468940316686,"which rely on the fact that one can obtain a covering of the composition of two function classes from
35"
INTRODUCTION,0.02192448233861145,"covering of the individual classes [7, 25], and path-norm bounds which extend the techniques behind
36"
INTRODUCTION,0.02253349573690621,"the F1-norm bound from shallow networks to the deep case [32, 6, 23].
37"
INTRODUCTION,0.023142509135200974,"Another issue is the lack of approximation results to accompany these generalization bounds: many
38"
INTRODUCTION,0.023751522533495738,"different complexity measures R(θ) on the parameters θ of DNNs have been proposed along with
39"
INTRODUCTION,0.024360535931790498,"guarantees that the generalization gap will be small as long as R(θ) is bounded, but there are often
40"
INTRODUCTION,0.024969549330085262,"little to no result describing families of functions that can be approximated with a bounded R(θ)
41"
INTRODUCTION,0.025578562728380026,"norm. The situation is much clearer in shallow networks, where we know that certain Sobolev spaces
42"
INTRODUCTION,0.026187576126674786,"can be approximated with bounded F1-norm [5].
43"
INTRODUCTION,0.02679658952496955,"We will focus on approximating composition of Sobolev functions, and obtaining close to optimal
44"
INTRODUCTION,0.027405602923264313,"rates. This is quite similar to the family of tasks considered [39], though the complexity measure we
45"
INTRODUCTION,0.028014616321559074,"consider is quite different, and does not require sparsity of the parameters.
46"
CONTRIBUTION,0.028623629719853837,"1.1
Contribution
47"
CONTRIBUTION,0.029232643118148598,"We consider Accordion Networks (AccNets), which are the composition of multiple shallow
48"
CONTRIBUTION,0.02984165651644336,"networks fL:1 = fL ◦· · · ◦f1, we prove a uniform generalization bound L(fL:1) −˜LN(fL:1) ≲
49"
CONTRIBUTION,0.030450669914738125,"R(f1, . . . , fL) log N
√"
CONTRIBUTION,0.031059683313032885,"N , for a complexity measure
50"
CONTRIBUTION,0.031668696711327646,"R(f1, . . . , fL) = L
Y"
CONTRIBUTION,0.03227771010962241,"ℓ=1
Lip(fℓ) L
X ℓ=1"
CONTRIBUTION,0.03288672350791717,"∥fℓ∥F1
Lip(fℓ) p"
CONTRIBUTION,0.03349573690621194,dℓ+ dℓ−1
CONTRIBUTION,0.0341047503045067,"that depends on the F1-norms ∥fℓ∥F1 and Lipschitz constanst Lip(fℓ) of the subnetworks, and the
51"
CONTRIBUTION,0.034713763702801465,"intermediate dimensions d0, . . . , dL. This use of the F1-norms makes this bound independent of the
52"
CONTRIBUTION,0.03532277710109622,"widths w1, . . . , wL of the subnetworks, though it does depend on the depth L (it typically grows
53"
CONTRIBUTION,0.035931790499390985,"linearly in L which is still better than the exponential growth often observed).
54"
CONTRIBUTION,0.03654080389768575,"Any traditional DNN can be mapped to an AccNet (and vice versa), by spliting the middle weight
55"
CONTRIBUTION,0.03714981729598051,"matrices Wℓwith SVD USV T into two matrices U
√"
CONTRIBUTION,0.037758830694275276,"S and
√"
CONTRIBUTION,0.03836784409257003,"SV T to obtain an AccNet with
56"
CONTRIBUTION,0.0389768574908648,"dimensions dℓ= RankWℓ, so that the bound can be applied to traditional DNNs with bounded rank.
57"
CONTRIBUTION,0.03958587088915956,"We then show an approximation result: any composition of Sobolev functions f ∗= f ∗
L∗◦· · · ◦f ∗
1
58"
CONTRIBUTION,0.040194884287454324,"can be approximated with a network with either a bounded complexity R(θ) or a slowly growing one.
59"
CONTRIBUTION,0.04080389768574909,"Thus under certain assumptions one can show that DNNs can learn general compositions of Sobolev
60"
CONTRIBUTION,0.04141291108404385,"functions. This ability can be interpreted as DNNs being able to learn symmetries, allowing them to
61"
CONTRIBUTION,0.04202192448233861,"avoid the curse of dimensionality in settings where kernel methods or even shallow networks suffer
62"
CONTRIBUTION,0.04263093788063337,"heavily from it.
63"
CONTRIBUTION,0.043239951278928136,"Empirically, we observe a good match between the scaling laws of learning and our theory, as well as
64"
CONTRIBUTION,0.0438489646772229,"qualitative features such as transitions between regimes depending on whether it is harder to learn the
65"
CONTRIBUTION,0.044457978075517664,"symmetries of a task, or to learn the task given its symmetries.
66"
ACCORDION NEURAL NETWORKS AND RESNETS,0.04506699147381242,"2
Accordion Neural Networks and ResNets
67"
ACCORDION NEURAL NETWORKS AND RESNETS,0.045676004872107184,"Our analysis is most natural for a slight variation on the traditional fully-connected neural networks
68"
ACCORDION NEURAL NETWORKS AND RESNETS,0.04628501827040195,"(FCNNs), which we call Accordion Networks, which we define here. Nevertheless, all of our results
69"
ACCORDION NEURAL NETWORKS AND RESNETS,0.04689403166869671,"can easily be adapted to FCNNs.
70"
ACCORDION NEURAL NETWORKS AND RESNETS,0.047503045066991476,"Accordion Networks (AccNets) are simply the composition of L shallow networks, that is fL:1 =
71"
ACCORDION NEURAL NETWORKS AND RESNETS,0.04811205846528624,"fL ◦· · · ◦f1 where fℓ(z) = Wℓσ(Vℓz + bℓ) for the nonlinearity σ : R →R, the dℓ× wℓmatrix
72"
ACCORDION NEURAL NETWORKS AND RESNETS,0.048721071863580996,"Wℓ, wℓ× dℓ−1 matrix Vℓ, and wℓ-dim. vector bℓ, and for the widths w1, . . . , wL and dimensions
73"
ACCORDION NEURAL NETWORKS AND RESNETS,0.04933008526187576,"d0, . . . , dL. We will focus on the ReLU σ(x) = max{0, x} for the nonlinearity. The parameters θ are
74"
ACCORDION NEURAL NETWORKS AND RESNETS,0.049939098660170524,"made up of the concatenation of all (Wℓ, Vℓ, bℓ). More generally, we denote fℓ2:ℓ1 = fℓ2 ◦· · · ◦fℓ1
75"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05054811205846529,"for any 1 ≤ℓ1 ≤ℓ2 ≤L.
76"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05115712545676005,"We will typically be interested in settings where the widths wℓis large (or even infinitely large), while
77"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05176613885505481,"the dimensions dℓremain finite or much smaller in comparison, hence the name accordion.
78"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05237515225334957,"If we add residual connections, i.e. f res
1:L = (fL + id) ◦· · · ◦(f1 + id) for the same shallow nets
79"
ACCORDION NEURAL NETWORKS AND RESNETS,0.052984165651644335,"f1, . . . , fL we recover the typical ResNets.
80"
ACCORDION NEURAL NETWORKS AND RESNETS,0.0535931790499391,"Remark. The only difference between AccNets and FCNNs is that each weight matrix Mℓof the
81"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05420219244823386,"FCNN is replaced by a product of two matrices Mℓ= VℓWℓ−1 in the middle of the network (such a
82"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05481120584652863,"structure has already been proposed [34]). Given an AccNet one can recover an equivalent FCNN by
83"
ACCORDION NEURAL NETWORKS AND RESNETS,0.055420219244823384,"choosing Mℓ= VℓWℓ−1, M0 = V0 and ML+1 = WL. In the other direction there could be multiple
84"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05602923264311815,"ways to split Mℓinto the product of two matrices, but we will focus on taking Vℓ= U
√"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05663824604141291,"S and
85"
ACCORDION NEURAL NETWORKS AND RESNETS,0.057247259439707675,"Wℓ−1 =
√"
ACCORDION NEURAL NETWORKS AND RESNETS,0.05785627283800244,"SV T for the SVD decomposition Mℓ= USV T , along with the choice dℓ= RankMℓ.
86"
LEARNING SETUP,0.058465286236297195,"2.1
Learning Setup
87"
LEARNING SETUP,0.05907429963459196,"We consider a traditional learning setup, where we want to find a function f : Ω⊂Rdin →Rdout
88"
LEARNING SETUP,0.05968331303288672,"that minimizes the population loss L(f) = Ex∼π [ℓ(x, f(x))] for an input distribution π and a
89"
LEARNING SETUP,0.06029232643118149,"ρ-Lipschitz and ρ-bounded loss function ℓ(x, y) ∈[0, B]. Given a training set x1, . . . , xN of size N
90"
LEARNING SETUP,0.06090133982947625,we approximate the population loss by the empirical loss ˜LN(f) = 1
LEARNING SETUP,0.061510353227771014,"N
PN
i=1 ℓ(xi, f(xi)) that can be
91"
LEARNING SETUP,0.06211936662606577,"minimized.
92"
LEARNING SETUP,0.06272838002436054,"To ensure that the empirical loss remains representative of the population loss, we will prove high
93"
LEARNING SETUP,0.06333739342265529,"probability bounds on the generalization gap ˜LN(f)−L(f) uniformly over certain functions families
94"
LEARNING SETUP,0.06394640682095006,"f ∈F.
95"
LEARNING SETUP,0.06455542021924482,"For regression tasks, we assume the existence of a true function f ∗and try to minimize the distance
96"
LEARNING SETUP,0.06516443361753958,"ℓ(x, y) = ∥f ∗(x) −y∥p for p ≥1. If we assume that f ∗(x) and y are uniformly bounded then one
97"
LEARNING SETUP,0.06577344701583435,"can easily show that ℓ(x, y) is bounded and Lipschitz. We are particularly interested in the cases
98"
LEARNING SETUP,0.06638246041412911,"p ∈{1, 2}, with p = 2 representing the classical MSE, and p = 1 representing a L1 distance. The
99"
LEARNING SETUP,0.06699147381242387,"p = 2 case is amenable to ‘fast rates’ which take advantage of the fact that the loss increases very
100"
LEARNING SETUP,0.06760048721071864,"slowly around the optimal solution f ∗, We do not prove such fast rates (even though it might be
101"
LEARNING SETUP,0.0682095006090134,"possible) so we focus on the p = 1 case.
102"
LEARNING SETUP,0.06881851400730817,"For classification tasks on k classes, we assume the existence of a ‘true class’ function f ∗: Ω→
103"
LEARNING SETUP,0.06942752740560293,"{1, . . . , k} and want to learn a function f : Ω→Rk such that the largest entry of f(x) is the f ∗(k)-th
104"
LEARNING SETUP,0.07003654080389768,"entry. One can consider the hinge cost ℓ(x, y) = max{0, 1 −(yf ∗(k) −maxi̸=f ∗(x) yi)}, which is
105"
LEARNING SETUP,0.07064555420219244,"zero whenever the margin yf ∗(k) −maxi̸=f ∗(x) yi is larger than 1 and otherwise equals 1 minus the
106"
LEARNING SETUP,0.0712545676004872,"margin. The hinge loss is Lipschitz and bounded if we assume bounded outputs y = f(x). The
107"
LEARNING SETUP,0.07186358099878197,"cross-entropy loss also fits our setup.
108"
GENERALIZATION BOUND FOR DNNS,0.07247259439707673,"3
Generalization Bound for DNNs
109"
GENERALIZATION BOUND FOR DNNS,0.0730816077953715,"The reason we focus on accordion networks is that there exists generalization bounds for shallow
110"
GENERALIZATION BOUND FOR DNNS,0.07369062119366626,"networks [5, 44], that are (to our knowledge) widely considered to be tight, which is in contrast to the
111"
GENERALIZATION BOUND FOR DNNS,0.07429963459196103,"deep case, where many bounds exist but no clear optimal bound has been identified. Our strategy
112"
GENERALIZATION BOUND FOR DNNS,0.07490864799025579,"is to extend the results for shallow nets to the composition of multiple shallow nets, i.e. AccNets.
113"
GENERALIZATION BOUND FOR DNNS,0.07551766138855055,"Roughly speaking, we will show that the complexity of an AccNet fθ is bounded by the sum of the
114"
GENERALIZATION BOUND FOR DNNS,0.07612667478684532,"complexities of the shallow nets f1, . . . , fL it is made of.
115"
GENERALIZATION BOUND FOR DNNS,0.07673568818514007,"We will therefore first review (and slightly adapt) the existing generalization bounds for shallow
116"
GENERALIZATION BOUND FOR DNNS,0.07734470158343483,"networks in terms of their so-called F1-norm [5], and then prove a generalization bound for deep
117"
GENERALIZATION BOUND FOR DNNS,0.0779537149817296,"AccNets.
118"
SHALLOW NETWORKS,0.07856272838002436,"3.1
Shallow Networks
119"
SHALLOW NETWORKS,0.07917174177831912,"The complexity of a shallow net f(x) = Wσ(V x + b), with weights W
∈Rw×dout and
120"
SHALLOW NETWORKS,0.07978075517661389,"V
∈Rdin×w, can be bounded in terms of the quantity C
= Pw
i=1 ∥W·i∥
q"
SHALLOW NETWORKS,0.08038976857490865,"∥Vi·∥2 + b2
i .
121"
SHALLOW NETWORKS,0.08099878197320341,"First note that the rescaled function
1
C f can be written as a convex combination
1
C f(x) =
122"
SHALLOW NETWORKS,0.08160779537149818,"Pw
i=1
∥W·i∥√"
SHALLOW NETWORKS,0.08221680876979294,"∥Vi·∥2+b2
i
C
¯W·iσ( ¯Vi·x + ¯bi) for ¯W·i =
W·i
∥W·i∥, ¯Vi· =
Vi·
√"
SHALLOW NETWORKS,0.0828258221680877,"∥Vi·∥2+b2
i , and ¯bi =
bi
√"
SHALLOW NETWORKS,0.08343483556638245,"∥Vi·∥2+b2
i ,
123"
SHALLOW NETWORKS,0.08404384896467722,"since the coefficients
∥W·i∥√"
SHALLOW NETWORKS,0.08465286236297198,"∥Vi·∥2+b2
i
C
are positive and sum up to 1. Thus f belongs to C times the
124"
SHALLOW NETWORKS,0.08526187576126674,"convex hull
125"
SHALLOW NETWORKS,0.08587088915956151,"BF1 = Conv
n
x 7→wσ(vT x + b) : ∥w∥2 = ∥v∥2 + b2 = 1
o
."
SHALLOW NETWORKS,0.08647990255785627,"We call this the F1-ball since it can be thought of as the unit ball w.r.t. the F1-norm ∥f∥F1 which we
126"
SHALLOW NETWORKS,0.08708891595615104,define as the smallest positive scalar s such that1 1
SHALLOW NETWORKS,0.0876979293544458,"sf ∈BF1. For more details in the single output
127"
SHALLOW NETWORKS,0.08830694275274056,"case, see [5].
128"
SHALLOW NETWORKS,0.08891595615103533,"The generalization gap over any F1-ball can be uniformly bounded with high probability:
129"
SHALLOW NETWORKS,0.08952496954933009,"Theorem 1. For any input distribution π supported on the L2 ball B(0, b) with radius b, we have
130"
SHALLOW NETWORKS,0.09013398294762484,"with probability 1 −δ, over the training samples x1, . . . , xN, that for all f ∈BF1(0, R) = R · BF1
131"
SHALLOW NETWORKS,0.0907429963459196,"L(f) −˜LN(f) ≤ρbR
p"
SHALLOW NETWORKS,0.09135200974421437,"din + dout
log N
√"
SHALLOW NETWORKS,0.09196102314250913,"N
+ c0 r"
SHALLOW NETWORKS,0.0925700365408039,2 log 2/δ N
SHALLOW NETWORKS,0.09317904993909866,"This theorem is a slight variation of the one found in [5]: we simply generalize it to multiple outputs,
132"
SHALLOW NETWORKS,0.09378806333739342,"and also prove it using a covering number argument instead of a direct computation of the Rademacher
133"
SHALLOW NETWORKS,0.09439707673568819,"complexity, which will be key to obtaining a generalization bound for the deep case. But due to this
134"
SHALLOW NETWORKS,0.09500609013398295,"change of strategy we pay a log N cost here and in our later results. We know that the log N term
135"
SHALLOW NETWORKS,0.09561510353227771,"can be removed in Theorem 1 by switching to a Rademacher argument, but we do not know whether
136"
SHALLOW NETWORKS,0.09622411693057248,"it can be removed in deep nets.
137"
SHALLOW NETWORKS,0.09683313032886723,"Notice how this bound does not depend on the width w, because the F1-norm (and the F1-ball)
138"
SHALLOW NETWORKS,0.09744214372716199,"themselves do not depend on the width. This matches with empirical evidence that shows that
139"
SHALLOW NETWORKS,0.09805115712545676,"increasing the width does not hurt generalization [8, 21, 20].
140"
SHALLOW NETWORKS,0.09866017052375152,"To use Theorem 1 effectively we need to be able to guarantee that the learned function will have a
141"
SHALLOW NETWORKS,0.09926918392204628,"small enough F1-norm. The F1-norm is hard to compute exactly, but it is bounded by the parameter
142"
SHALLOW NETWORKS,0.09987819732034105,"norm: if f(x) = Wσ(V x + b), then ∥f∥F1 ≤1"
SHALLOW NETWORKS,0.10048721071863581,"2

∥W∥2
F + ∥V ∥2
F + ∥b∥2
, and this bound is tight
143"
SHALLOW NETWORKS,0.10109622411693057,"if the width w is large enough and the parameters are chosen optimally. Adding weight decay/L2-
144"
SHALLOW NETWORKS,0.10170523751522534,"regularization to the cost then leads to bias towards learning with small F1 norm.
145"
DEEP NETWORKS,0.1023142509135201,"3.2
Deep Networks
146"
DEEP NETWORKS,0.10292326431181487,"Since an AccNet is simply the composition of multiple shallow nets, the functions represented by an
147"
DEEP NETWORKS,0.10353227771010962,"AccNet is included in the set of composition of F1 balls. More precisely, if ∥Wℓ∥2 +∥Vℓ∥2 +∥bℓ∥2 ≤
148"
DEEP NETWORKS,0.10414129110840438,"2Rℓthen fL:1 belongs to the set {gL ◦· · · ◦g1 : gℓ∈BF1(0, Rℓ)} for some Rℓ, which is width
149"
DEEP NETWORKS,0.10475030450669914,"agnostic.
150"
DEEP NETWORKS,0.10535931790499391,"As already noticed in [7], the covering number number is well-behaved under composition, this
151"
DEEP NETWORKS,0.10596833130328867,"allows us to bound the complexity of AccNets in terms of the individual shallow nets it is made up of:
152"
DEEP NETWORKS,0.10657734470158343,"Theorem 2. Consider an accordion net of depth L and widths dL, . . . , d0, with corresponding set of
153"
DEEP NETWORKS,0.1071863580998782,"functions F = {fL:1 : ∥fℓ∥F1 ≤Rℓ, Lip(fℓ) ≤ρℓ}. With probability 1 −δ over the sampling of the
154"
DEEP NETWORKS,0.10779537149817296,"training set X from the distribution π supported in B(0, b), we have for all f ∈F
155"
DEEP NETWORKS,0.10840438489646773,"L(f) −˜LN(f) ≤CρbρL:1 L
X ℓ=1 Rℓ ρℓ p"
DEEP NETWORKS,0.10901339829476249,"dℓ+ dℓ−1
log N
√"
DEEP NETWORKS,0.10962241169305725,"N
(1 + o(1)) + c0 r"
DEEP NETWORKS,0.110231425091352,"2 log 2/δ N
."
DEEP NETWORKS,0.11084043848964677,"Theorem 2 can be extended to ResNets (fL + id) ◦· · · ◦(f1 + id) by simply replacing the Lipschitz
156"
DEEP NETWORKS,0.11144945188794153,"constant Lip(fℓ) by Lip(fℓ+ id).
157"
DEEP NETWORKS,0.1120584652862363,"The Lipschitz constants Lip(fℓ) are difficult to compute exactly, so it is easiest to simply bound it
158"
DEEP NETWORKS,0.11266747868453106,"by the product of the operator norms Lip(fℓ) ≤∥Wℓ∥op ∥Vℓ∥op, but this bound can often be quite
159"
DEEP NETWORKS,0.11327649208282582,"loose. The fact that our bound depends on the Lipschitz constants rather than the operator norms
160"
DEEP NETWORKS,0.11388550548112059,"∥Wℓ∥op , ∥Vℓ∥op is thus a significant advantage.
161"
DEEP NETWORKS,0.11449451887941535,"This bound can be applied to a FCNNs with weight matrices M1, . . . , ML+1, by replacing the middle
162"
DEEP NETWORKS,0.11510353227771011,"Mℓwith SVD decomposition USV T in the middle by two matrices Wℓ−1 =
√"
DEEP NETWORKS,0.11571254567600488,"SV T and Vℓ= U
√"
DEEP NETWORKS,0.11632155907429964,"S,
163"
DEEP NETWORKS,0.11693057247259439,"so that the dimensions can be chosen as the rank dℓ= RankMℓ+1. The Frobenius norm of the new
164"
DEEP NETWORKS,0.11753958587088915,"matrices equal the nuclear norm of the original one ∥Wℓ−1∥2
F = ∥Vℓ∥2
F = ∥Mℓ∥∗. Some bounds
165"
DEEP NETWORKS,0.11814859926918392,"1This construction can be used for any convex set B that is symmetric around zero (B = −B) to define a
norm whose unit ball is B. This correspondence between symmetric convex sets and norms is well known."
DEEP NETWORKS,0.11875761266747868,"Figure 1: Visualization of scaling laws. We observe that deep networks (either AccNets or DNNs)
achieve better scaling laws than kernel methods or shallow networks on certain compositional tasks,
in agreement with our theory. We also see that our new generalization bounds approximately recover
the right saling laws (even though they are orders of magnitude too large overall). We consider
a compositional true function f ∗= h ◦g where g maps from dimension 15 to 3 while h maps
from 3 to 20, and we denote νg, νh for the number of times g, h are differentiable. In the first plot
νg = 8, νh = 1 so that g is easy to learn while h is hard, whereas in the second plot νg = 9, νh = 9,
so both g and h are relatively easier. The third plot presents the decay in test error and generalization
bounds for networks evaluated using the real-world dataset, WESAD [37]."
DEEP NETWORKS,0.11936662606577345,"assuming rank sparsity of the weight matrices also appear in [41]. And several recent results have
166"
DEEP NETWORKS,0.11997563946406821,"shown that weight-decay leads to a low-rank bias on the weight matrices of the network [27, 26, 19]
167"
DEEP NETWORKS,0.12058465286236297,"and replacing the Frobenius norm regularization with a nuclear norm regularization (according to the
168"
DEEP NETWORKS,0.12119366626065774,"above mentioned equivalence) will only increase this low-rank bias.
169"
DEEP NETWORKS,0.1218026796589525,"We compute in Figure 1 the upper bound of Theorem 2 for both AccNets and DNNs, and even though
170"
DEEP NETWORKS,0.12241169305724726,"we observe a very large gap (roughly of order 103), we do observe that it captures rate/scaling of the
171"
DEEP NETWORKS,0.12302070645554203,"test error (the log-log slope) well. So this generalization bound could be well adapted to predicting
172"
DEEP NETWORKS,0.12362971985383678,"rates, which is what we will do in the next section.
173"
DEEP NETWORKS,0.12423873325213154,"Remark. Note that if one wants to compute this upper bound in practical setting, it is important to
174"
DEEP NETWORKS,0.1248477466504263,"train with L2 regularization until the parameter norm also converges (this often happens after the
175"
DEEP NETWORKS,0.12545676004872108,"train and test loss have converged). The intuition is that at initialization, the weights are initialized
176"
DEEP NETWORKS,0.12606577344701583,"randomly, and they contribute a lot to the parameter norm, and thus lead to a larger generalization
177"
DEEP NETWORKS,0.12667478684531058,"bound. During training with weight decay, these random initial weights slowly vanish, thus leading
178"
DEEP NETWORKS,0.12728380024360536,"to a smaller parameter norm and better generalization bound. It might be possible to improve our
179"
DEEP NETWORKS,0.1278928136419001,"generalization bounds to take into account the randomness at initialization to obtain better bounds
180"
DEEP NETWORKS,0.1285018270401949,"throughout training, but we leave this to future work.
181"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.12911084043848964,"4
Breaking the Curse of Dimensionality with Compositionality
182"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.12971985383678442,"In this section we study a large family of functions spaces, obtained by taking compositions of
183"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13032886723507917,"Sobolev balls. We focus on this family of tasks because they are well adapted to the complexity
184"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13093788063337394,"measure we have identified, and because kernel methods and even shallow networks do suffer from
185"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1315468940316687,"the curse of dimensionality on such tasks, whereas deep networks avoid it (e.g. Figure 1).
186"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13215590742996347,"More precisely, we will show that these sets of functions can be approximated by a AccNets with
187"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13276492082825822,"bounded (or in some cases slowly growing) complexity measure
188"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13337393422655297,"R(f1, . . . , fL) = L
Y"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13398294762484775,"ℓ=1
Lip(fℓ) L
X ℓ=1"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1345919610231425,"∥fℓ∥F1
Lip(fℓ) p"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13520097442143728,dℓ+ dℓ−1.
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13580998781973203,"This will then allow us show that AccNets can (assuming global convergence) avoid the curse of
189"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1364190012180268,"dimensionality, even in settings that should suffer from the curse of dimensionality, when the input
190"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13702801461632155,"dimension is large and the function is not very smooth (only a few times differentiable).
191"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13763702801461633,"Figure 2: A comparison of empirical and theoretical error rates. The first plot illustrates the log
decay rate of the test error with respect to the dataset size N based on our empirical simulations.
The second plot depicts the theoretical decay rate of the test error as discussed in Section 4.1,
−min{ 1 2, νg"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.13824604141291108,"din ,
νh
dmid }. The final plot on the right displays the difference between the two. The lower
left region represents the area where g is easier to learn than h, the upper right where h is easier to
learn than g, and the lower right region where both f and g are easy.
."
COMPOSITION OF SOBOLEV BALLS,0.13885505481120586,"4.1
Composition of Sobolev Balls
192"
COMPOSITION OF SOBOLEV BALLS,0.1394640682095006,"The family of Sobolev norms capture some notion of regularity of a function, as it measures the size
193"
COMPOSITION OF SOBOLEV BALLS,0.14007308160779536,"of its derivatives. The Sobolev norm of a function f : Rdin →R is defined in terms of its derivatives
194"
COMPOSITION OF SOBOLEV BALLS,0.14068209500609014,"∂α
x f for some din-multi-index α, namely the W ν,p(π)-Sobolev norm with integer ν and p ≥1 is
195"
COMPOSITION OF SOBOLEV BALLS,0.14129110840438489,"defined as
196"
COMPOSITION OF SOBOLEV BALLS,0.14190012180267966,"∥f∥p
W ν,p(π) =
X"
COMPOSITION OF SOBOLEV BALLS,0.1425091352009744,"|α|≤ν
∥∂α
x f∥p
Lp(π) ."
COMPOSITION OF SOBOLEV BALLS,0.1431181485992692,"Note that the derivative ∂α
x f only needs to be defined in the ‘weak’ sense, which means that even
197"
COMPOSITION OF SOBOLEV BALLS,0.14372716199756394,"non-differentiable functions such as the ReLU functions can actually have finite Sobolev norm.
198"
COMPOSITION OF SOBOLEV BALLS,0.14433617539585872,"The Sobolev balls BW ν,p(π)(0, R) = {f : ∥f∥W ν,p(π) ≤R} are a family of function spaces with a
199"
COMPOSITION OF SOBOLEV BALLS,0.14494518879415347,"range of regularity (the larger ν, the more regular). This regularity makes these spaces of functions
200"
COMPOSITION OF SOBOLEV BALLS,0.14555420219244825,"learnable purely from the fact that they enforce the function f to vary slowly as the input changes.
201"
COMPOSITION OF SOBOLEV BALLS,0.146163215590743,"Indeed we can prove the following generalization bound:
202"
COMPOSITION OF SOBOLEV BALLS,0.14677222898903775,"Proposition 3. Given a distribution π with support the L2 ball with radius b, we have that with
203"
COMPOSITION OF SOBOLEV BALLS,0.14738124238733252,"probability 1 −δ for all functions f ∈F = {f : ∥f∥W ν,2 ≤R, ∥f∥∞≤R}
204"
COMPOSITION OF SOBOLEV BALLS,0.14799025578562727,L(f) −˜LN(f) ≤2ρC1REν/d(N) + c0 r
COMPOSITION OF SOBOLEV BALLS,0.14859926918392205,"2 log 2/δ N
."
COMPOSITION OF SOBOLEV BALLS,0.1492082825822168,where Er(N) = N −1
COMPOSITION OF SOBOLEV BALLS,0.14981729598051158,2 if r > 1
COMPOSITION OF SOBOLEV BALLS,0.15042630937880633,"2, Er(N) = N −1"
COMPOSITION OF SOBOLEV BALLS,0.1510353227771011,2 log N if r = 1
COMPOSITION OF SOBOLEV BALLS,0.15164433617539586,"2, and Er(N) = N −r if r < 1"
COMPOSITION OF SOBOLEV BALLS,0.15225334957369063,"2.
205"
COMPOSITION OF SOBOLEV BALLS,0.15286236297198538,"But this result also illustrates the curse of dimensionality: the differentiability ν needs to scale with
206"
COMPOSITION OF SOBOLEV BALLS,0.15347137637028013,"the input dimension din to obtain a reasonable rate. If instead ν is constant and din grows, then the
207"
COMPOSITION OF SOBOLEV BALLS,0.1540803897685749,"number of datapoints N needed to guarantee a generalization gap of at most ϵ scales exponentially in
208"
COMPOSITION OF SOBOLEV BALLS,0.15468940316686966,"din, i.e. N ∼ϵ−din"
COMPOSITION OF SOBOLEV BALLS,0.15529841656516444,"ν . One way to interpret this issue is that regularity becomes less and less useful the
209"
COMPOSITION OF SOBOLEV BALLS,0.1559074299634592,"larger the dimension: knowing that similar inputs have similar outputs is useless in high dimension
210"
COMPOSITION OF SOBOLEV BALLS,0.15651644336175397,"where the closest training point xi to a test point x is typically very far away.
211"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.15712545676004872,"4.1.1
Breaking the Curse of Dimensionality with Compositionality
212"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1577344701583435,"To break the curse of dimensionality, we need to assume some additional structure on the data or task
213"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.15834348355663824,"which introduces an ‘intrinsic dimension’ that can be much lower than the input dimension din:
214"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.15895249695493302,"Manifold hypothesis: If the input distribution lies on a dsurf-dimensional manifold, the error rates
215"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.15956151035322777,"typically depends on dsurf instead of din [38, 10].
216"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16017052375152252,"Figure 3: Comparing error rates for shallow and AccNets: shallow nets vs. AccNets, and kernel
methods vs. AccNets. The left two graphs shows the empirical decay rate of test error with respect to
dataset size (N) for both shallow nets and kernel methods. In contrast to our earlier empirical findings
for AccNets, both shallow nets and kernel methods exhibit a slower decay rate in test error. The right
two graphs present the differences in log decay rates between shallow nets and AccNets, as well as
between kernel methods and AccNets. AccNets almost always obtain better rates, with a particularly
large advantage at the bottom and middle-left.
."
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1607795371498173,"Known Symmetries: If f ∗(g · x) = f ∗(x) for a group action · w.r.t. a group G, then f ∗can be
217"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16138855054811205,"written as the composition of a modulo map g∗: Rdin →Rdin/G which maps pairs of inputs which
218"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16199756394640683,"are equivalent up to symmetries to the same value (pairs x, y s.t. y = g · x for some g ∈G), and then
219"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16260657734470157,"a second function h∗: Rdin/G →Rdout, then the complexity of the task will depend on the dimension
220"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16321559074299635,"of the modulo space Rdin/G which can be much lower. If the symmetry is known, then one can for
221"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1638246041412911,"example fix g∗and only learn h∗(though other techniques exist, such as designing kernels or features
222"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16443361753958588,"that respect the same symmetries) [31].
223"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16504263093788063,"Symmetry Learning: However if the symmetry is not known then both g∗and h∗have to be learned,
224"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1656516443361754,"and this is where we require feature learning and/or compositionality. Shallow networks are able
225"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16626065773447016,"to learn translation symmetries, since they can learn so-called low-index functions which satisfy
226"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1668696711327649,"f ∗(x) = f ∗(Px) for some projection P (with a statistical complexity that depends on the dimension
227"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16747868453105969,"of the space one projects into, not the full dimension [5, 2]). Low-index functions correspond exactly
228"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16808769792935443,"to the set of functions that are invariant under translation along the kernel ker P. To learn general
229"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1686967113276492,"symmetries, one needs to learn both h∗and the modulo map g∗simultaneously, hence the importance
230"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16930572472594396,"of feature learning.
231"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.16991473812423874,"For g∗to be learnable efficiently, it needs to be regular enough to not suffer from the curse of
232"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1705237515225335,"dimensionality, but many traditional symmetries actually have smooth modulo maps, for example
233"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17113276492082827,"the modulo map g∗(x) = ∥x∥2 for rotation invariance. This can be understood as a special case of
234"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17174177831912302,"composition of Sobolev functions, whose generalization gap can be bounded:
235"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1723507917174178,"Theorem
4.
Consider
the
function
set
F
=
FL ◦· · · ◦F1
where
Fℓ
=
236

fℓ: Rdℓ−1 →Rdℓs.t. ∥fℓ∥W νℓ,2 ≤Rℓ, ∥fℓ∥∞≤bℓ, Lip(fℓ) ≤ρℓ
	
, and let rmin = minℓrℓfor
237"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17295980511571254,"rℓ=
νℓ
dℓ−1 , then with probability 1 −δ we have for all f ∈F
238"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1735688185140073,"L(f) −˜LN(f) ≤ρC0 L
X"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17417783191230207,"ℓ=1
(CℓρL:ℓ+1Rℓ)"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17478684531059682,"1
rmin+1
!rmin+1"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1753958587088916,Ermin(N) + c0 r
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17600487210718635,"2 log 2/δ N
,"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17661388550548113,"where Cℓdepends only on dℓ−1, dℓ, νℓ, bℓ−1.
239"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17722289890377588,"We see that only the smallest ratio rmin matters when it comes to the rate of learning. And actually
240"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17783191230207065,"the above result could be slightly improved to show that the sum over all layers could be replaced by
241"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1784409257003654,"a sum over only the layers where the ratio rℓleads to the worst rate Erℓ(N) = Ermin(N) (and the
242"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17904993909866018,"other layers contribute an asymptotically subdominant amount).
243"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.17965895249695493,"Coming back to the symmetry learning example, we see that the hardness of learning a function of
244"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18026796589524968,"the type f ∗= h ◦g with inner dimension dmid and regularities νg and νh, the error rate will be (up
245"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18087697929354446,to log terms) N −min{ 1
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1814859926918392,"2 ,
νg
din ,
νh
dmid }. This suggests the existence of three regimes depending on which
246"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.182095006090134,term attains the minimum: a regime where both g and h are easy to learn and we have N −1
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18270401948842874,"2 learning,
247"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18331303288672351,"a regime g is hard, and a regime where h is hard. The last two regimes differentiate between tasks
248"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18392204628501826,"where learning the symmetry is hard and those where learning the function knowing its symmetries is
249"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18453105968331304,"hard.
250"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1851400730816078,"In contrast, without taking advantage of the compositional structure, we expect f ∗to be only
251"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18574908647990257,"min{νg, νh} times differentiable, so trying to learn it as a single Sobolev function would lead to an
252"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18635809987819732,error rate of N −min{ 1
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18696711327649207,"2 ,
min{νg,νh}"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18757612667478685,"din
} = N −min{ 1"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1881851400730816,"2 ,
νg
din ,
νh
din } which is no better than the compositional
253"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18879415347137637,"rate, and is strictly worse whenever νh < νg and νh"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.18940316686967112,din < 1
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1900121802679659,"2 (we can always assume dmid ≤din since
254"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.19062119366626065,"one could always choose d = id).
255"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.19123020706455543,"Furthermore, since multiple compositions are possible, one can imagine a hierarchy of symmetries
256"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.19183922046285018,"that slowly reduce the dimensionality with less and less regular modulo maps. For example one could
257"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.19244823386114496,"imagine a composition fL ◦· · · ◦f1 with dimensions dℓ= d02−ℓand regularities νℓ= d02−ℓso that
258"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.1930572472594397,"the ratios remain constant rℓ=
d02−ℓ"
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.19366626065773446,d02−ℓ+1 = 1
BREAKING THE CURSE OF DIMENSIONALITY WITH COMPOSITIONALITY,0.19427527405602923,"2, leading to an almost parametric rate of N −1"
LOG N,0.19488428745432398,"2 log N
259"
LOG N,0.19549330085261876,"even though the function may only be d02−L times differentiable. Without compositionality, the rate
260"
LOG N,0.1961023142509135,"would only be N −2−L.
261"
LOG N,0.1967113276492083,"Remark. In the case of a single Sobolev function, one can show that the rate Eν/d(N) is in some
262"
LOG N,0.19732034104750304,"sense optimal, by giving an information theoretic lower bound with matching rate. A naive argument
263"
LOG N,0.19792935444579782,"suggests that the rate of Emin{r1,...,rL}(N) should similarly be optimal: assume that the minimum
264"
LOG N,0.19853836784409257,"rℓis attained at a layer ℓ, then one can consider the subset of functions such that the image
265"
LOG N,0.19914738124238734,"fℓ−1:1(B(0, r)) contains a ball B(z, r′) ⊂Rdℓ−1 and that the function fL:ℓ+1 is β-non-contracting
266"
LOG N,0.1997563946406821,"∥fL:ℓ+1(x) −fL:ℓ+1(y)∥≥β ∥x −y∥, then learning fL:1 should be as hard as learning fℓover the
267"
LOG N,0.20036540803897684,"ball B(z, r′) (more rigorously this could be argued from the fact that any ϵ-covering of fL:1 can be
268"
LOG N,0.20097442143727162,"mapped to an ϵ/β-covering of fℓ), thus forcing a rate of at least Erℓ(N) = Emin{r1,...,rL}(N).
269"
LOG N,0.20158343483556637,"An analysis of minimax rates in a similar setting has been done in [22].
270"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20219244823386115,"4.2
Breaking the Curse of Dimensionality with AccNets
271"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2028014616321559,"Now that we know that composition of Sobolev functions can be easily learnable, even in settings
272"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20341047503045068,"where the curse of dimensionality should make it hard to learn them, we need to find a model that can
273"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20401948842874543,"achieve those rates. Though many models are possible 2, we focus on DNNs, in particular AccNets.
274"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2046285018270402,"Assuming convergence to a global minimum of the loss of sufficiently wide AccNets with two types
275"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20523751522533495,"of regularization, one can guarantee close to optimal rates:
276"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20584652862362973,"Theorem 5. Given a true function f ∗
L∗:1 = f ∗
L∗◦· · · ◦f ∗
1 going through the dimensions d∗
0, . . . , d∗
L∗,
277"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20645554202192448,"along with a continuous input distribution π0 supported in B(0, b0), such that the distributions πℓ
278"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20706455542021923,"of f ∗
ℓ(x) (for x ∼π0) are continuous too and supported inside B(0, bℓ) ⊂Rd∗
ℓ. Further assume
279"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.207673568818514,"that there are differentiabilities νℓand radii Rℓsuch that ∥f ∗
ℓ∥W νℓ,2(B(0,bℓ)) ≤Rℓ, and ρℓsuch that
280"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20828258221680876,"Lip(f ∗
ℓ) ≤ρℓ. For an infinite width AccNet with L ≥L∗and dimensions dℓ≥d∗
1, . . . , d∗
L∗−1, we
281"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.20889159561510354,"have for the ratios ˜rℓ=
νℓ
d∗
ℓ+3:
282"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2095006090133983,"• At a global minimizer
ˆfL:1 of the regularized loss f1, . . . , fL
7→
˜LN(fL:1) +
283"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21010962241169306,"λ QL
ℓ=1 Lip(fℓ) PL
ℓ=1
∥fℓ∥F1
Lip(fℓ)
p"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21071863580998781,"dℓ−1 + dℓ, we have L( ˆfL:1) = ˜O(N −min{ 1"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2113276492082826,"2 ,˜r1,...,˜rL∗}).
284"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21193666260657734,"• At a global minimizer ˆfL:1 of the regularized loss f1, . . . , fL 7→˜LN(fL:1)+λ QL
ℓ=1 ∥fℓ∥F1,
285"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21254567600487212,we have L( ˆfL:1) = ˜O(N −1
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21315468940316687,"2 +PL∗
ℓ=1 max{0,˜rℓ−1"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21376370280146162,"2 }).
286"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2143727161997564,"There are a number of limitations to this result. First we assume that one is able to recover the global
287"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21498172959805115,"minimizer of the regularized loss, which should be hard in general3 (we already know from [5] that
288"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21559074299634592,"this is NP-hard for shallow networks and a simple F1-regularization). Note that it is sufficient to
289"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21619975639464067,"recover a network fL:1 whose regularized loss is within a constant of the global minimum, which
290"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21680876979293545,"2One could argue that it would be more natural to consider compositions of kernel method models, for
example a composition of random feature models. But this would lead to a very similar model: this would
be equivalent to a AccNet where only the Wℓweights are learned, while the Vℓ, bℓweights remain constant.
Another family of models that should have similar properties is Deep Gaussian Processes [15].
3Note that the unregularized loss can be optimized polynomially, e.g. in the NTK regime [28, 3, 16], but this
is an easier task than findinig the global minimum of the regularized loss where one needs to both fit the data,
and do it with an minimal regularization term."
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2174177831912302,"might be easier to guarantee, but should still be hard in general. The typical method of training with
291"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21802679658952498,"GD on the regularized loss is a greedy approach, which might fail in general but could recover almost
292"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21863580998781973,"optimal parameters under the right conditions (some results suggest that training relies on first order
293"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2192448233861145,"correlations to guide the network in the right direction [2, 1, 35]).
294"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.21985383678440926,"We propose two regularizations because they offer a tradeoff:
295"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.220462850182704,"First regularization: The first regularization term leads to almost optimal rates, up to the change
296"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22107186358099878,from rℓ= νℓ
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22168087697929353,"d∗
ℓto rℓ=
νℓ
d∗
ℓ+3 which is negligible for large dimensions dℓand differentiabilities νℓ. The
297"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2222898903775883,"first problem is that it requires an infinite width at the moment, because we were not able to prove
298"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22289890377588306,"that a function with bounded F1-norm and Lipschitz constant can be approximated by a sufficiently
299"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22350791717417784,"wide shallow networks with the same (or close) F1-norm and Lipschitz constant (we know from [5]
300"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2241169305724726,"that it is possible without preserving the Lipschitzness). We are quite hopeful that this condition
301"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22472594397076737,"might be removed in future work.
302"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22533495736906212,"The second and more significant problem is that the Lipschitz constants Lip(fℓ) are difficult to
303"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2259439707673569,"optimize over. For finite width networks it is in theory possible to take the max over all linear regions,
304"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22655298416565164,"but the complexity might be unreasonable. It might be more reasonable to leverage an implicit bias
305"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2271619975639464,"instead, such as a large learning rate, because a large Lipschitz constant implies that the nework is
306"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22777101096224117,"sensible to small changes in its parameters, so GD with a large learning rate should only converge to
307"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22838002436053592,"minima with a small Lipschitz constant (such a bias is described in [26]). It might also be possible to
308"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2289890377588307,"replace the Lipschitz constant in our generalization bounds, possibly along the lines of [43].
309"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.22959805115712545,"Second regularization: The second regularization term actually does not require an infinite width,
310"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23020706455542023,"only a sufficiently large one. Also its regularization term is equivalent to Q(∥Wℓ∥2 + ∥Vℓ∥2 + ∥bℓ∥2)
311"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23081607795371498,"which is much closer to the traditional L2-regularization (and actually one could prove the same
312"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23142509135200975,"or very similar rates for L2-regularization). The issue is that it lead to rates that could be far from
313"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2320341047503045,"optimal depending on the ratios ˜rℓ: it recovers the same rate as the first regularization term if no
314"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23264311814859928,more than one ratio ˜rℓis smaller than 1
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23325213154689403,"2, but if many of these ratios are above 1"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23386114494518878,"2, it can be arbitrarily
315"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23447015834348356,"smaller.
316"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2350791717417783,"In Figure 2, we compare the empirical rates (by doing a linear fit on a log-log plot of test error as a
317"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2356881851400731,"function of N) and the predicted optimal rates min{ 1 2, νg"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23629719853836784,"din ,
νh
dmid } and observe a pretty good match.
318"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23690621193666261,"Though surprisingly, it appears the the empirical rates tend to be slightly better than the theoretical
319"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23751522533495736,"ones.
320"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23812423873325214,"Remark. As can be seen in the proof of Theorem5, when the depth L is strictly larger than the true
321"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2387332521315469,"depth L∗, one needs to add identity layers, leading to a so-called Bottleneck structure, which was
322"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23934226552984167,"proven to be optimal and observed empirically in [27, 26, 45]. These identity layers add a term
323"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.23995127892813642,"that scales linearly in the additional depth (L−L∗)d∗
min
√"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.24056029232643117,"N
to the first regularization, and an exponential
324"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.24116930572472595,"prefactor (2d∗
min)L−L∗to the second. It might be possible to remove these factors by leveraging the
325"
BREAKING THE CURSE OF DIMENSIONALITY WITH ACCNETS,0.2417783191230207,"bottleneck structure, or simply by switching to ResNets.
326"
CONCLUSION,0.24238733252131547,"5
Conclusion
327"
CONCLUSION,0.24299634591961022,"We have given a generalization bound for Accordion Networks and as an extension Fully-Connected
328"
CONCLUSION,0.243605359317905,"networks. It depends on F1-norms and Lipschitz constants of its shallow subnetworks. This allows us
329"
CONCLUSION,0.24421437271619975,"to prove under certain assumptions that AccNets can learn general compositions of Sobolev functions
330"
CONCLUSION,0.24482338611449453,"efficiently, making them able to break the curse of dimensionality in certain settings, such as in the
331"
CONCLUSION,0.24543239951278928,"presence of unknown symmetries.
332"
REFERENCES,0.24604141291108406,"References
333"
REFERENCES,0.2466504263093788,"[1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property:
334"
REFERENCES,0.24725943970767356,"a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer
335"
REFERENCES,0.24786845310596833,"neural networks. In Conference on Learning Theory, pages 4782–4887. PMLR, 2022.
336"
REFERENCES,0.24847746650426308,"[2] Emmanuel Abbe, Enric Boix-Adserà, Matthew Stewart Brennan, Guy Bresler, and
337"
REFERENCES,0.24908647990255786,"Dheeraj Mysore Nagaraj. The staircase property: How hierarchical structure can guide deep
338"
REFERENCES,0.2496954933008526,"learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances
339"
REFERENCES,0.2503045066991474,"in Neural Information Processing Systems, 2021.
340"
REFERENCES,0.25091352009744217,"[3] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
341"
REFERENCES,0.2515225334957369,"over-parameterization. pages 242–252, 2019.
342"
REFERENCES,0.25213154689403167,"[4] Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit
343"
REFERENCES,0.25274056029232644,"sphere: an introduction, volume 2044. Springer Science & Business Media, 2012.
344"
REFERENCES,0.25334957369062117,"[5] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal
345"
REFERENCES,0.25395858708891594,"of Machine Learning Research, 18(1):629–681, 2017.
346"
REFERENCES,0.2545676004872107,"[6] Andrew R Barron and Jason M Klusowski. Complexity, statistical risk, and metric entropy of
347"
REFERENCES,0.2551766138855055,"deep nets using total path variation. stat, 1050:6, 2019.
348"
REFERENCES,0.2557856272838002,"[7] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds
349"
REFERENCES,0.256394640682095,"for neural networks. Advances in neural information processing systems, 30, 2017.
350"
REFERENCES,0.2570036540803898,"[8] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
351"
REFERENCES,0.25761266747868455,"learning practice and the classical bias–variance trade-off. Proceedings of the National Academy
352"
REFERENCES,0.2582216808769793,"of Sciences, 116(32):15849–15854, 2019.
353"
REFERENCES,0.25883069427527405,"[9] M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of the
354"
REFERENCES,0.25943970767356883,"classes W α
p . Mathematics of The USSR-Sbornik, 2:295–317, 1967.
355"
REFERENCES,0.26004872107186355,"[10] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on
356"
REFERENCES,0.26065773447015833,"low-dimensional manifolds using deep relu networks: Function approximation and statistical
357"
REFERENCES,0.2612667478684531,"recovery. Information and Inference: A Journal of the IMA, 11(4):1203–1253, 2022.
358"
REFERENCES,0.2618757612667479,"[11] Lénaïc Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over-
359"
REFERENCES,0.2624847746650426,"parameterized Models using Optimal Transport. In Advances in Neural Information Processing
360"
REFERENCES,0.2630937880633374,"Systems 31, pages 3040–3050. Curran Associates, Inc., 2018.
361"
REFERENCES,0.26370280146163216,"[12] Lénaïc Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural
362"
REFERENCES,0.26431181485992694,"networks trained with the logistic loss. In Jacob Abernethy and Shivani Agarwal, editors,
363"
REFERENCES,0.26492082825822166,"Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of
364"
REFERENCES,0.26552984165651644,"Machine Learning Research, pages 1305–1338. PMLR, 09–12 Jul 2020.
365"
REFERENCES,0.2661388550548112,"[13] Feng Dai. Approximation theory and harmonic analysis on spheres and balls. Springer, 2013.
366"
REFERENCES,0.26674786845310594,"[14] Zhen Dai, Mina Karzand, and Nathan Srebro. Representation costs of linear neural networks:
367"
REFERENCES,0.2673568818514007,"Analysis and design. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,
368"
REFERENCES,0.2679658952496955,"Advances in Neural Information Processing Systems, 2021.
369"
REFERENCES,0.2685749086479903,"[15] Andreas Damianou and Neil D. Lawrence. Deep Gaussian processes. In Carlos M. Carvalho
370"
REFERENCES,0.269183922046285,"and Pradeep Ravikumar, editors, Proceedings of the Sixteenth International Conference on
371"
REFERENCES,0.2697929354445798,"Artificial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research,
372"
REFERENCES,0.27040194884287455,"pages 207–215, Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR.
373"
REFERENCES,0.27101096224116933,"[16] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably
374"
REFERENCES,0.27161997563946405,"optimizes over-parameterized neural networks.
In International Conference on Learning
375"
REFERENCES,0.27222898903775883,"Representations, 2019.
376"
REFERENCES,0.2728380024360536,"[17] I. Dumer, M.S. Pinsker, and V.V. Prelov. On coverings of ellipsoids in euclidean spaces. IEEE
377"
REFERENCES,0.27344701583434833,"Transactions on Information Theory, 50(10):2348–2356, 2004.
378"
REFERENCES,0.2740560292326431,"[18] Lawrence C Evans. Partial differential equations, volume 19. American Mathematical Society,
379"
REFERENCES,0.2746650426309379,"2022.
380"
REFERENCES,0.27527405602923266,"[19] Tomer Galanti, Zachary S Siegel, Aparna Gupte, and Tomaso Poggio. Sgd and weight decay
381"
REFERENCES,0.2758830694275274,"provably induce a low-rank bias in neural networks. arXiv preprint arXiv:2206.05794, 2022.
382"
REFERENCES,0.27649208282582216,"[20] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli,
383"
REFERENCES,0.27710109622411694,"Giulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization
384"
REFERENCES,0.2777101096224117,"with number of parameters in deep learning. Journal of Statistical Mechanics: Theory and
385"
REFERENCES,0.27831912302070644,"Experiment, 2020(2):023401, 2020.
386"
REFERENCES,0.2789281364190012,"[21] Mario Geiger, Stefano Spigler, Stéphane d’Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio
387"
REFERENCES,0.279537149817296,"Biroli, and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape
388"
REFERENCES,0.2801461632155907,"of deep neural networks. Physical Review E, 100(1):012115, 2019.
389"
REFERENCES,0.2807551766138855,"[22] Matteo Giordano, Kolyan Ray, and Johannes Schmidt-Hieber. On the inability of gaussian
390"
REFERENCES,0.28136419001218027,"process regression to optimally learn compositional functions. In Alice H. Oh, Alekh Agarwal,
391"
REFERENCES,0.28197320341047505,"Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing
392"
REFERENCES,0.28258221680876977,"Systems, 2022.
393"
REFERENCES,0.28319123020706455,"[23] Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, and Rémi Gribonval. A path-norm toolkit
394"
REFERENCES,0.2838002436053593,"for modern networks: consequences, promises and challenges. In The Twelfth International
395"
REFERENCES,0.2844092570036541,"Conference on Learning Representations, 2023.
396"
REFERENCES,0.2850182704019488,"[24] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias
397"
REFERENCES,0.2856272838002436,"in terms of optimization geometry. In Jennifer Dy and Andreas Krause, editors, Proceedings of
398"
REFERENCES,0.2862362971985384,"the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
399"
REFERENCES,0.2868453105968331,"Learning Research, pages 1832–1841. PMLR, 10–15 Jul 2018.
400"
REFERENCES,0.2874543239951279,"[25] Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang. Generalization bounds via distillation.
401"
REFERENCES,0.28806333739342266,"In International Conference on Learning Representations, 2021.
402"
REFERENCES,0.28867235079171744,"[26] Arthur Jacot. Bottleneck structure in learned features: Low-dimension vs regularity tradeoff. In
403"
REFERENCES,0.28928136419001216,"A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in
404"
REFERENCES,0.28989037758830694,"Neural Information Processing Systems, volume 36, pages 23607–23629. Curran Associates,
405"
REFERENCES,0.2904993909866017,"Inc., 2023.
406"
REFERENCES,0.2911084043848965,"[27] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. In
407"
REFERENCES,0.2917174177831912,"The Eleventh International Conference on Learning Representations, 2023.
408"
REFERENCES,0.292326431181486,"[28] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence and
409"
REFERENCES,0.29293544457978077,"Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31,
410"
REFERENCES,0.2935444579780755,"pages 8580–8589. Curran Associates, Inc., 2018.
411"
REFERENCES,0.29415347137637027,"[29] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
412"
REFERENCES,0.29476248477466505,"generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.
413"
REFERENCES,0.2953714981729598,"[30] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient
414"
REFERENCES,0.29598051157125455,"descent for matrix factorization: Greedy low-rank learning. In International Conference on
415"
REFERENCES,0.2965895249695493,"Learning Representations, 2020.
416"
REFERENCES,0.2971985383678441,"[31] Stéphane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics,
417"
REFERENCES,0.2978075517661389,"65(10):1331–1398, 2012.
418"
REFERENCES,0.2984165651644336,"[32] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
419"
REFERENCES,0.2990255785627284,"networks. In Conference on learning theory, pages 1376–1401. PMLR, 2015.
420"
REFERENCES,0.29963459196102316,"[33] Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under
421"
REFERENCES,0.3002436053593179,"neural tangent kernel regime. In International Conference on Learning Representations, 2020.
422"
REFERENCES,0.30085261875761266,"[34] Greg Ongie and Rebecca Willett. The role of linear layers in nonlinear interpolating networks.
423"
REFERENCES,0.30146163215590743,"arXiv preprint arXiv:2202.00856, 2022.
424"
REFERENCES,0.3020706455542022,"[35] Leonardo Petrini, Francesco Cagnetta, Umberto M Tomasini, Alessandro Favero, and Matthieu
425"
REFERENCES,0.30267965895249693,"Wyart. How deep neural networks learn compositional data: The random hierarchy model.
426"
REFERENCES,0.3032886723507917,"arXiv preprint arXiv:2307.02129, 2023.
427"
REFERENCES,0.3038976857490865,"[36] Grant Rotskoff and Eric Vanden-Eijnden.
Parameters as interacting particles: long time
428"
REFERENCES,0.30450669914738127,"convergence and asymptotic error scaling of neural networks. In Advances in Neural Information
429"
REFERENCES,0.305115712545676,"Processing Systems 31, pages 7146–7155. Curran Associates, Inc., 2018.
430"
REFERENCES,0.30572472594397077,"[37] Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof Van Laerhoven.
431"
REFERENCES,0.30633373934226554,"Introducing wesad, a multimodal dataset for wearable stress and affect detection. In Proceedings
432"
REFERENCES,0.30694275274056027,"of the 20th ACM International Conference on Multimodal Interaction, ICMI ’18, page 400–408,
433"
REFERENCES,0.30755176613885504,"New York, NY, USA, 2018. Association for Computing Machinery.
434"
REFERENCES,0.3081607795371498,"[38] Johannes Schmidt-Hieber. Deep relu network approximation of functions on a manifold. arXiv
435"
REFERENCES,0.3087697929354446,"preprint arXiv:1908.00695, 2019.
436"
REFERENCES,0.3093788063337393,"[39] Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU
437"
REFERENCES,0.3099878197320341,"activation function. The Annals of Statistics, 48(4):1875 – 1897, 2020.
438"
REFERENCES,0.3105968331303289,"[40] Mark Sellke. On size-independent sample complexity of relu networks. Information Processing
439"
REFERENCES,0.31120584652862365,"Letters, page 106482, 2024.
440"
REFERENCES,0.3118148599269184,"[41] Taiji Suzuki, Hiroshi Abe, and Tomoaki Nishimura.
Compression based bound for non-
441"
REFERENCES,0.31242387332521315,"compressed network: unified generalization error analysis of large compressible deep neural
442"
REFERENCES,0.31303288672350793,"network. In International Conference on Learning Representations, 2020.
443"
REFERENCES,0.31364190012180265,"[42] Zihan Wang and Arthur Jacot. Implicit bias of SGD in l2-regularized linear DNNs: One-
444"
REFERENCES,0.31425091352009743,"way jumps from high to low rank. In The Twelfth International Conference on Learning
445"
REFERENCES,0.3148599269183922,"Representations, 2024.
446"
REFERENCES,0.315468940316687,"[43] Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via
447"
REFERENCES,0.3160779537149817,"lipschitz augmentation. Advances in Neural Information Processing Systems, 32, 2019.
448"
REFERENCES,0.3166869671132765,"[44] E Weinan, Chao Ma, and Lei Wu. Barron spaces and the compositional function spaces for
449"
REFERENCES,0.31729598051157126,"neural network models. arXiv preprint arXiv:1906.08039, 2019.
450"
REFERENCES,0.31790499390986604,"[45] Yuxiao Wen and Arthur Jacot. Which frequencies do cnns need? emergent bottleneck structure
451"
REFERENCES,0.31851400730816076,"in feature learning. to appear at ICML, 2024.
452"
REFERENCES,0.31912302070645554,"[46] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
453"
REFERENCES,0.3197320341047503,"deep learning requires rethinking generalization. ICLR 2017 proceedings, Feb 2017.
454"
REFERENCES,0.32034104750304504,"The Appendix is structured as follows:
455"
REFERENCES,0.3209500609013398,"1. In Section A, we describe the experimental setup and provide a few additional experiments.
456"
REFERENCES,0.3215590742996346,"2. In Section B, we prove Theorems 1 and 2 from the main.
457"
REFERENCES,0.3221680876979294,"3. In Section C, we prove Proposition 3 and Theorem 4.
458"
REFERENCES,0.3227771010962241,"4. In Section D, we prove Theorem 5 and other approximation results concerning Sobolev
459"
REFERENCES,0.3233861144945189,"functions.
460"
REFERENCES,0.32399512789281365,"5. In Section E, we prove a few technical results on the covering number.
461"
REFERENCES,0.32460414129110843,"A
Experimental Setup4
462"
REFERENCES,0.32521315468940315,"In this section, we review our numerical experiments and their setup both on synthetic and real-world
463"
REFERENCES,0.3258221680876979,"datasets in order to address theoretical results more clearly and intuitively.
464"
REFERENCES,0.3264311814859927,"A.1
Dataset
465"
REFERENCES,0.3270401948842874,"A.1.1
Emperical Dataset
466"
REFERENCES,0.3276492082825822,"The Matérn kernel is considered a generalization of the radial basis function (RBF) kernel. It
467"
REFERENCES,0.328258221680877,"controls the differentiability, or smoothness, of the kernel through the parameter ν. As ν →∞, the
468"
REFERENCES,0.32886723507917176,"Matérn kernel converges to the RBF kernel, and as ν →0, it converges to the Laplacian kernel, a
469"
REFERENCES,0.3294762484774665,"0-differentiable kernel. In this study, we utilized the Matérn kernel to generate Gaussian Process (GP)
470"
REFERENCES,0.33008526187576126,"samples based on the composition of two Matérn kernels, Kg and Kh, with varying differentiability
471"
REFERENCES,0.33069427527405604,"in the range [0.5,10]×[0.5,10]. The input dimension (din) of the kernel, the bottleneck mid-dimension
472"
REFERENCES,0.3313032886723508,"(dmid), and the output dimension (dout) are 15, 3, and 20, respectively.
473"
REFERENCES,0.33191230207064554,"This outlines the general procedure of our sampling method for synthetic data:
474"
REFERENCES,0.3325213154689403,"1. Sample the training dataset X ∈RD×din
475"
REFERENCES,0.3331303288672351,"2. From X, compute the D × D kernel Kg with given νg
476"
REFERENCES,0.3337393422655298,"3. From Kg, sample Z ∈RD×dmid with columns sampled from the Gaussian N(0, Kg).
477"
REFERENCES,0.3343483556638246,"4. From Z, compute Kg with given νh
478"
REFERENCES,0.33495736906211937,"5. From Kh, sample the test dataset Y ∈RD×dout with columns sampled from the Gaussian
479"
REFERENCES,0.33556638246041415,"N(0, Kh).
480"
REFERENCES,0.33617539585870887,"We utilized four AMD Opteron 6136 processors (2.4 GHz, 32 cores) and 128 GB of RAM to generate
481"
REFERENCES,0.33678440925700365,"our synthetic dataset. The maximum possible dataset size for 128 GB of RAM is approximately
482"
REFERENCES,0.3373934226552984,"52,500; however, we opted for a synthetic dataset size of 22,000 due to the computational expense
483"
REFERENCES,0.3380024360535932,"associated with sampling the Matérn kernel. This decision was made considering the time complexity
484"
REFERENCES,0.3386114494518879,"of O(n3)and the space complexity of O(n2) involved. Out of the 22,000 dataset points, 20,000 were
485"
REFERENCES,0.3392204628501827,"allocated for training data, and 2,000 were used for the test dataset
486"
REFERENCES,0.3398294762484775,"A.1.2
Real-world dataset: WESAD
487"
REFERENCES,0.3404384896467722,"In our study, we utilized the Wearable Stress and Affect Detection (WESAD) dataset to train our
488"
REFERENCES,0.341047503045067,"AccNets for binary classification. The WESAD dataset, which is publicly accessible, provides
489"
REFERENCES,0.34165651644336176,"multimodal physiological and motion data collected from 15 subjects using devices worn on the wrist
490"
REFERENCES,0.34226552984165654,"and chest. For the purpose of our experiment, we specifically employed the Empatica E4 wrist device
491"
REFERENCES,0.34287454323995126,"to distinguish between non-stress (baseline) and stress conditions, simplifying the classification task
492"
REFERENCES,0.34348355663824603,"to these two categories.
493"
REFERENCES,0.3440925700365408,"After preprocessing, the dataset comprised a total of 136,482 instances. We implemented a train-test
494"
REFERENCES,0.3447015834348356,"split ratio of approximately 75:25, resulting in 100,000 instances for the training set and 36,482
495"
REFERENCES,0.3453105968331303,"instances for the test set. The overall hyperparameters and architecture of the AccNets model applied
496"
REFERENCES,0.3459196102314251,"to the WESAD dataset were largely consistent with those used for our synthetic data. The primary
497"
REFERENCES,0.34652862362971987,"differences were the use of 100 epochs for each iteration of Ni from Ns, and a learning rate set to
498"
REFERENCES,0.3471376370280146,"1e-5.
499"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.34774665042630937,4The code used for experiments are publicly available here
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.34835566382460414,"Figure 4: A comparison: singular values of the weight matrices for DNN and AccNets models.
The first two plots represent cases where N = 10000 while the right two plots correspond to N =
200.The number of outliers at the top of each plot signifies the rank of each network. The plots with
N = 10000 datasets demonstrate a clearer capture of the true rank compared to those with N = 200
indicating that a higher dataset count provides more accurate rank determination
."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3489646772228989,"A.2
Model setups
500"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.34957369062119364,"To investigate the scaling law of test error for our synthetic data, we trained models using Ni
501"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3501827040194884,"datapoints from our training data, where N = [100, 200, 500, 1000, 2000, 5000, 10000, 20000]. The
502"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3507917174177832,"models employed for this analysis included the kernel method, shallow networks, fully connected
503"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.351400730816078,"deep neural networks (FC DNN), and AccNets. For FC DNN and AccNets, we configured the
504"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3520097442143727,"network depth to 12 layers, with the layer widths set as [din, 500, 500, ..., 500, dout] for DNNs, and
505"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3526187576126675,"[din, 900, 100, 900, ..., 100, 900, dout] for AccNets.
506"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.35322777101096225,"To ensure a comparable number of neurons, the width for the shallow networks was set to 50,000,
507"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.353836784409257,"resulting in dimensions of [din, 50000, dout].
508"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.35444579780755175,"We utilized ReLU as the activation function and L1-norm as the cost function, with the Adam
509"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.35505481120584653,"optimizer. The total number of batch was set to 5, and the training process was conducted over 3600
510"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3556638246041413,"epochs, divided into three phases. The detailed optimizer parameters are as follows:
511"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.35627283800243603,"1. For the first 1200 epochs: learning rate (lr) = 1.5 ∗0.001, weight decay = 0
512"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3568818514007308,"2. For the second 1200 epochs: lr = 0.4 ∗0.001, weight decay = 0.002
513"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3574908647990256,"3. For the final 1200 epochs: lr = 0.1 ∗0.001, weight decay = 0.005
514"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.35809987819732036,"We conducted experiments utilizing 12 NVIDIA V100 GPUs (each with 32GB of memory) over a
515"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3587088915956151,"period of 6.3 days to train the synthetic dataset. In contrast, training the WESAD dataset required
516"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.35931790499390986,"only one hour on a single V100 GPU.
517"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.35992691839220464,"A.3
Additional experiments
518"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.36053593179049936,"B
AccNet Generalization Bounds
519"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.36114494518879414,"The proof of generalization for shallow networks (Theorem 1) is the special case L = 1 of the proof
520"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3617539585870889,"of Theorem 2, so we only prove the second:
521"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3623629719853837,"Theorem 6. Consider an accordion net of depth L and widths dL, . . . , d0, with corresponding set
522"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3629719853836784,"of functions F = {fL:1 : ∥fℓ∥F1 ≤Rℓ, Lip(fℓ) ≤ρℓ} with input space Ω= B(0, r). For any
523"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3635809987819732,"ρ-Lipschitz loss function ℓ(x, f(x)) with |ℓ(x, y)| ≤c0, we know that with probability 1 −δ over the
524"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.364190012180268,"sampling of the training set X from the distribution π, we have for all f ∈F
525"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.36479902557856275,"L(f) −˜LN(f) ≤CρL:1r L
X ℓ′=1 Rℓ′ ρℓ′ p"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3654080389768575,"dℓ′ + dℓ′−1
log N
√"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.36601705237515225,"N
(1 + o(1)) + c0 r"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.36662606577344703,"2 log 2/δ N
."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.36723507917174175,"Proof. The strategy is: (1) prove a covering number bound on F (2) use it to obtain a Rademacher
526"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.36784409257003653,"complexity bound, (3) use the Rademacher complexity to bound the generalization error.
527"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3684531059683313,"(1) We define fℓ= Vℓ◦σ ◦Wℓso that fθ = fL:1 = fL ◦· · · ◦f1. First notice that we can write each
528"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3690621193666261,"fℓas convex combination of its neurons:
529"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3696711327649208,"fℓ(x) = wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3702801461632156,"i=1
vℓ,iσ(wT
ℓ,ix) = Rℓ wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37088915956151036,"i=1
cℓ,i¯vℓ,iσ( ¯wT
ℓ,ix)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37149817295980514,"for ¯wℓ,i =
wℓ,i
∥wℓ,i∥, ¯vℓ,i =
vℓ,i
∥vℓ,i∥, Rℓ= Pℓ
i=1 ∥vℓ,i∥∥wℓ,i∥and cℓ,i =
1
Rℓ∥vℓ,i∥∥wℓ,i∥.
530"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37210718635809986,"Let us now consider a sequence ϵk = 2−k for k = 0, . . . , K and define ˜v(k)
ℓ,i , ˜w(k)
ℓ,i to be the ϵk-covers
531"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37271619975639464,"of ¯vℓ,i, ¯wℓ,i, furthermore we may choose ˜v(0)
ℓ,i = ˜w(0)
ℓ,i = 0 since every unit vector is within a ϵ0 = 1
532"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3733252131546894,"distance of the origin. We will now show that on can approximate fθ by approximating each of the fℓ
533"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37393422655298414,"by functions of the form
534"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3745432399512789,"˜fℓ(x) = Rℓ Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3751522533495737,"1
Mk,ℓ"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37576126674786847,"Mk,ℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3763702801461632,"m=1
˜v(k)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37697929354445797,"ℓ,i(k)
ℓ,mσ( ˜w(k)T"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37758830694275275,"ℓ,i(k)
ℓ,mx) −˜v(k−1)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3781973203410475,"ℓ,i(k)
ℓ,m σ( ˜w(k−1)T"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.37880633373934225,"ℓ,i(k)
ℓ,m x)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.379415347137637,"for indices i(k)
ℓ,m = 1, . . . , wℓchoosen adequately. Notice that the number of functions of this type
535"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3800243605359318,"equals the number of Mk,ℓquadruples (˜v(k)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3806333739342265,"ℓ,i(k)
ℓ,m, ˜w(k)T"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3812423873325213,"ℓ,i(k)
ℓ,m, ˜v(k−1)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3818514007308161,"ℓ,i(k)
ℓ,m , ˜w(k−1)T"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.38246041412911086,"ℓ,i(k)
ℓ,m ) where these vectors belong
536"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3830694275274056,"to the ϵk- resp. ϵk−1-coverings of the din- resp. dout-dimensional unit sphere. Thus the number of
537"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.38367844092570036,"such functions is bounded by
538 Kℓ
Y k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.38428745432399514," 
N2(Sdin−1, ϵk)N2(Sdout−1, ϵk)N2(Sdin−1, ϵk−1)N2(Sdout−1, ϵk−1)
Mk,ℓ,"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3848964677222899,"and we have this choice for all ℓ= 1, . . . , L. We will show that with sufficiently large Mk,ℓthis set
539"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.38550548112058464,"of functions ϵ-covers F which then implies that
540"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3861144945188794,"log N2(F, ϵ) ≤2 L
X ℓ=1 Kℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3867235079171742,"k=1
Mk,ℓ
 
log N2(Sdin−1, ϵk−1) + log N2(Sdin−1, ϵk−1)

."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3873325213154689,"We will use the probabilistic method to find the right indices i(k)
ℓ,m to approximate a function fℓ=
541"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3879415347137637,"Rℓ
Pwℓ
i=1 cℓ,i¯vℓ,iσ( ¯wT
ℓ,ix) with a function ˜fℓ. We take all i(k)
ℓ,m to be i.i.d. equal to the index i =
542"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.38855054811205847,"1, · · · , wℓwith probability cℓ,i, so that in expectation
543"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.38915956151035325,"E ˜fℓ(x) = Rℓ Kℓ
X k=1 wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.38976857490864797,"i=1
cℓ,i

˜v(k)
ℓ,i σ( ˜w(k)T
ℓ,i
x) −˜v(k−1)
ℓ,i
σ( ˜w(k−1)T
ℓ,i
x)
 = Rℓ wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.39037758830694275,"i=1
cℓ,i˜v(K)
ℓ,i σ( ˜w(K)T
ℓ,i
x)."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3909866017052375,"We will show that this expectation is O(ϵKℓ)-close to fℓand that the variance of ˜fℓgoes to zero as
544"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3915956151035323,"the Mℓ,ks grow, allowing us to bound the expected error E
fL:1 −˜fL:1

2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.392204628501827,"π ≤ϵ2 which then implies
545"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3928136419001218,"that there must be at least one choice of indices i(k)
ℓ,m such that
fL:1 −˜fL:1

π ≤ϵ.
546"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3934226552984166,"Let us first bound the distance
547"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3940316686967113,"fℓ(x) −E ˜fℓ(x)
 = Rℓ  wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3946406820950061,"i=1
cℓ,i

¯vℓ,iσ( ¯wT
ℓ,ix) −˜v(K)
ℓ,i σ( ˜w(K)T
ℓ,i
x)
 ≤Rℓ wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.39524969549330086,"i=1
cℓ,i


¯vℓ,i −˜v(K)
ℓ,i

σ( ¯wT
ℓ,ix)
 +
˜v(K)
ℓ,i

σ( ¯wT
ℓ,ix) −σ( ˜w(K)T
ℓ,i
x)

 ≤Rℓ wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.39585870889159563,"i=1
cℓ,i
¯vℓ,i −˜v(K)
ℓ,i

 ¯wT
ℓ,ix
 +
˜v(K)
ℓ,i

 ¯wT
ℓ,ix −˜w(K)T
ℓ,i
x

 ≤2Rℓ wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.39646772228989036,"i=1
cℓ,iϵKℓ∥x∥"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.39707673568818513,= 2RℓϵKℓ∥x∥.
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3976857490864799,"Then we bound the trace of the covariance of ˜fℓwhich equals the expected square distance between
548"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3982947624847747,"˜fℓand its expectation:
549"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3989037758830694,"E
 ˜fℓ(x) −E ˜fℓ(x)

2 = Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.3995127892813642,"R2
ℓ
M 2
k,ℓ"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.40012180267965897,"Mk,ℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4007308160779537,"m=1
E
˜v(k)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.40133982947624847,"ℓ,i(k)
ℓ,mσ( ˜w(k)T"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.40194884287454324,"ℓ,i(k)
ℓ,mx) −˜v(k−1)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.402557856272838,"ℓ,i(k)
ℓ,m σ( ˜w(k−1)T"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.40316686967113274,"ℓ,i(k)
ℓ,m x) −E

˜v(k)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4037758830694275,"ℓ,i(k)
ℓ,mσ( ˜w(k)T"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4043848964677223,"ℓ,i(k)
ℓ,mx) −˜v(k−1)"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4049939098660171,"ℓ,i(k)
ℓ,m σ( ˜w(k−1)T"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4056029232643118,"ℓ,i(k)
ℓ,m x)
 2 ≤ Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4062119366626066,"R2
ℓ
M 2
k,ℓ"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.40682095006090135,"Mk,ℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4074299634591961,"m=1
E
˜v(k)
ℓ,mσ( ˜w(k)T
ℓ,m x) −˜v(k−1)
ℓ,m
σ( ˜w(k−1)T
ℓ,m
x)

2 = Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.40803897685749085,"R2
ℓ
Mk,ℓ wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.40864799025578563,"i=1
ci
˜v(k)
ℓ,i σ

˜w(k)T
ℓ,i
x

−˜v(k−1)
ℓ,i
σ

˜w(k−1)T
ℓ,i
x

2 ≤ Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4092570036540804,"2R2
ℓ∥x∥2 Mk,ℓ wℓ
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.40986601705237513,"i=1
ci
˜v(k)
ℓ,i

2  ˜w(k)
ℓ,i −˜w(k−1)
ℓ,i

2
+ ci
˜v(k)
ℓ,i −˜v(k−1)
ℓ,i

2  ˜w(k−1)
ℓ,i

2 ≤ Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4104750304506699,"4R2
ℓ∥x∥2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4110840438489647,"Mk,ℓ
(ϵk + ϵk−1)2 ≤ Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.41169305724725946,"36R2
ℓ∥x∥2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4123020706455542,"Mk,ℓ
ϵ2
k."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.41291108404384896,"Putting both together, we obtain
550"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.41352009744214374,"E
fℓ(x) −˜fℓ(x)

2
≤4R2
ℓϵ2
Kℓ∥x∥2 + Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.41412911084043846,"36R2
ℓ∥x∥2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.41473812423873324,"Mk,ℓ
ϵ2
k"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.415347137637028,"= 4R2
ℓ∥x∥2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4159561510353228,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4165651644336175,"ϵ2
k
Mk,ℓ ! ."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4171741778319123,"We will now use this bound, together with the Lipschitzness of fℓto bound the error
551"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4177831912302071,"E
fL:1(x) −˜fL:1(x)

2
. We will do this by induction on the distances E
fℓ:1(x) −˜fℓ:1(x)

2
.
552"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.41839220462850185,"We start by
553"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4190012180267966,"E
f1(x) −˜f1(x)

2
≤4R2
1 ∥x∥2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.41961023142509135,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42021924482338613,"ϵ2
k
Mk,1 ! ."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42082825822168085,"And for the induction step, we condition on the layers fℓ−1:1
554"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42143727161997563,"E
fℓ:1(x) −˜fℓ:1(x)

2
= E

E
fℓ:1(x) −˜fℓ:1(x)

2
| ˜fℓ−1:1 "
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4220462850182704,"= E
fℓ:1(x) −E
h
˜fℓ:1(x)| ˜fℓ−1:1
i
2
+ EE
 ˜fℓ:1(x) −E
h
˜fℓ:1(x)| ˜fℓ−1:1
i
2
| ˜fℓ−1:1 "
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4226552984165652,"= E
fℓ:1(x) −fℓ( ˜fℓ−1:1(x))

2
+ EE
 ˜fℓ:1(x) −fℓ( ˜fℓ−1:1(x))

2
| ˜fℓ−1:1 "
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4232643118148599,"≤ρ2
ℓE
fℓ−1:1(x) −˜fℓ−1:1(x)

2
+ 4R2
ℓE
 ˜fℓ−1:1(x)

2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4238733252131547,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42448233861144946,"ϵ2
k
Mk,ℓ ! ."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42509135200974424,"Now since
555"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42570036540803896,"E
 ˜fℓ−1:1(x)

2
≤∥fℓ−1:1(x)∥2 + E
fℓ−1:1(x) −˜fℓ−1:1(x)

2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42630937880633374,"≤ρ2
ℓ−1 · · · ρ2
1 ∥x∥2 + E
fℓ−1:1(x) −˜fℓ−1:1(x)

2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4269183922046285,"we obtain that
556"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42752740560292324,"E
fℓ:1(x) −˜fℓ:1(x)

2
≤ "
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.428136419001218,"ρ2
ℓ+ 4R2
ℓ "
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4287454323995128,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.42935444579780757,"ϵ2
k
Mk,ℓ !!"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4299634591961023,"E
fℓ−1:1(x) −˜fℓ−1:1(x)

2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.43057247259439707,"+ 4R2
ℓρ2
ℓ−1 · · · ρ2
1 ∥x∥2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.43118148599269185,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4317904993909866,"ϵ2
k
Mk,ℓ ! ."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.43239951278928135,"We define ˜ρ2
ℓ= ρ2
ℓ
h
1 + 4 R2
ℓ
ρ2
ℓ"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4330085261875761,"
ϵ2
Kℓ+ 9 PKℓ
k=1
ϵ2
k
Mk,ℓ"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4336175395858709,"i
and obtain
557"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4342265529841656,"E
fL:1(x) −˜fL:1(x)

2
≤4 L
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4348355663824604,"ℓ=1
˜ρ2
L:ℓ+1R2
ℓρ2
ℓ−1:1 ∥x∥2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4354445797807552,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.43605359317904996,"ϵ2
k
Mk,ℓ ! ."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4366626065773447,"Thus for any distribution π over the ball B(0, r), there is a choice of indices i(k)
ℓ,m such that
558"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.43727161997563946,"fL:1 −˜fL:1

2 π ≤4 L
X"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.43788063337393424,"ℓ=1
˜ρ2
L:ℓ+1R2
ℓρ2
ℓ−1:1r2"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.438489646772229,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.43909866017052374,"ϵ2
k
Mk,ℓ ! ."
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4397076735688185,"We now simply need to choose Kℓand Mk,ℓadequately. To reach an error of 2ϵ, we choose
559 Kℓ= &"
THE CODE USED FOR EXPERIMENTS ARE PUBLICLY AVAILABLE HERE,0.4403166869671133,−log ϵ + 1
LOG,0.440925700365408,"2 log """
LOG,0.4415347137637028,"4ρ2
L:1r2
 L
X ℓ′=1 Rℓ′ ρℓ′ p"
LOG,0.44214372716199757,dℓ′ + dℓ′−1
LOG,0.44275274056029235,"!
Rℓ
ρℓ
p"
LOG,0.44336175395858707,dℓ+ dℓ−1 #'
LOG,0.44397076735688185,"where ρL:1 = QL
ℓ=1 ρℓ. Notice that that ϵ2
Kℓ≤
1
4ρ2
L:1r2
PL
ℓ′=1 Rℓ′ ρℓ′
√"
LOG,0.4445797807551766,"dℓ′+dℓ′−1
 ρℓ√"
LOG,0.4451887941534714,dℓ+dℓ−1
LOG,0.4457978075517661,"Rℓ
ϵ2.
560"
LOG,0.4464068209500609,"Given s0 = P∞
k=1
√"
LOG,0.4470158343483557,"k2−k ≈1.3473 < ∞, we define
561 Mk,ℓ= &"
LOG,0.4476248477466504,"36ρ2
L:1r2s0 L
X ℓ′=1 Rℓ′ ρℓ′ p"
LOG,0.4482338611449452,dℓ′ + dℓ′−1
LOG,0.44884287454323996,"!
Rℓ
ρℓ
p"
LOG,0.44945188794153473,dℓ+ dℓ−1 2−k √
LOG,0.45006090133982946,"k
1
ϵ2 ' ."
LOG,0.45066991473812423,"So that for all ℓ
562"
LOG,0.451278928136419,"4R2
ℓ
ρ2
ℓ "
LOG,0.4518879415347138,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
LOG,0.4524969549330085,"ϵ2
k
Mk,ℓ ! ≤ Rℓ ρℓ
p"
LOG,0.4531059683313033,dℓ+ dℓ−1
LOG,0.45371498172959807,"ρ2
L:1r2
PL
ℓ′=1
Rℓ ρℓ
p"
LOG,0.4543239951278928,"dℓ+ dℓ−1
ϵ2 + Rℓ ρℓ
p"
LOG,0.45493300852618757,dℓ+ dℓ−1
LOG,0.45554202192448234,"ρ2
L:1r2
PL
ℓ′=1
Rℓ ρℓ
p"
LOG,0.4561510353227771,"dℓ+ dℓ−1
ϵ2
PKℓ
k′=1
√"
LOG,0.45676004872107184,"k′2−k′ s0 ≤2 Rℓ ρℓ
p"
LOG,0.4573690621193666,dℓ+ dℓ−1
LOG,0.4579780755176614,"ρ2
L:1r2
PL
ℓ′=1
Rℓ ρℓ
p"
LOG,0.4585870889159562,"dℓ+ dℓ−1
ϵ2."
LOG,0.4591961023142509,"Now this also implies that
563"
LOG,0.4598051157125457,"˜ρℓ≤ρℓexp  2 Rℓ ρℓ
p"
LOG,0.46041412911084045,dℓ+ dℓ−1
LOG,0.4610231425091352,"ρ2
L:1r2
PL
ℓ′=1
Rℓ ρℓ
p"
LOG,0.46163215590742995,"dℓ+ dℓ−1
ϵ2  "
LOG,0.46224116930572473,"and thus
564"
LOG,0.4628501827040195,˜ρL:ℓ+1 ≤ρL:ℓ+1 exp  2
LOG,0.46345919610231423,"PL
ℓ′=ℓ+1
Rℓ ρℓ
p"
LOG,0.464068209500609,dℓ+ dℓ−1
LOG,0.4646772228989038,"ρ2
L:1r2
PL
ℓ′=1
Rℓ ρℓ
p"
LOG,0.46528623629719856,"dℓ+ dℓ−1
ϵ2 "
LOG,0.4658952496954933,"≤ρL:ℓ+1 exp

2
ρ2
L:1r2 ϵ2

."
LOG,0.46650426309378806,"Putting it all together, we obtain that
565"
LOG,0.46711327649208284,"fL:1 −˜fL:1

2 π ≤4 L
X"
LOG,0.46772228989037756,"ℓ=1
˜ρ2
L:ℓ+1R2
ℓρ2
ℓ−1:1r2"
LOG,0.46833130328867234,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
LOG,0.4689403166869671,"ϵ2
k
Mk,ℓ !"
LOG,0.4695493300852619,"≤exp

2
ρ2
L:1r2 ϵ2

ρ2
L:1r2
L
X"
LOG,0.4701583434835566,"ℓ=1
4R2
ℓ
ρ2
ℓ "
LOG,0.4707673568818514,"ϵ2
Kℓ+ 9 Kℓ
X k=1"
LOG,0.4713763702801462,"ϵ2
k
Mk,ℓ !"
LOG,0.47198538367844095,"≤2 exp

2
ρ2
L:1r2 ϵ2

ϵ2"
LOG,0.4725943970767357,= 2ϵ2 + O(ϵ4).
LOG,0.47320341047503045,"Now since log N2(Sdℓ−1, ϵ) = dℓlog
  1"
LOG,0.47381242387332523,"ϵ + 1

and
566"
LOG,0.47442143727161995,"Mk,ℓ≤36ρ2
L:1r2s0 L
X ℓ′=1 Rℓ′ ρℓ′ p"
LOG,0.47503045066991473,dℓ′ + dℓ′−1
LOG,0.4756394640682095,"!
Rℓ
ρℓ
p"
LOG,0.4762484774665043,dℓ+ dℓ−1 2−k √
LOG,0.476857490864799,"k
1
ϵ2 + 1,"
LOG,0.4774665042630938,"we have
567"
LOG,0.47807551766138856,"log N2(F,
√"
EXP,0.47868453105968334,"2 exp

ϵ2"
EXP,0.47929354445797806,"ρ2
L:1r2"
EXP,0.47990255785627284,"
ϵ) ≤2 L
X ℓ=1 Kℓ
X"
EXP,0.4805115712545676,"k=1
Mk,ℓ
 
log N2(Sdℓ−1, ϵk−1) + log N2(Sdℓ−1−1, ϵk−1)
 ≤2 L
X ℓ=1 Kℓ
X"
EXP,0.48112058465286234,"k=1
Mk,ℓ(dℓ+ dℓ−1) log(
1
ϵk−1
+ 1)"
EXP,0.4817295980511571,"≤72s0ρ2
L:1r2
 L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.4823386114494519,"dℓ′ + dℓ′−1 !
L
X ℓ=1 Rℓ ρℓ p"
EXP,0.48294762484774667,"dℓ+ dℓ−1 Kℓ
X k=1"
EXP,0.4835566382460414,"2−k log(
1
ϵk−1 + 1)
√"
EXP,0.48416565164433617,"k
1
ϵ2 + 2 L
X"
EXP,0.48477466504263095,"ℓ=1
(dℓ+ dℓ−1) Kℓ
X"
EXP,0.4853836784409257,"k=1
log(
1
ϵk−1
+ 1)"
EXP,0.48599269183922045,"≤72s2
0ρ2
L:1r2
 L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.4866017052375152,dℓ′ + dℓ′−1
EXP,0.48721071863581,"!2
1
ϵ2 + o(ϵ−2)."
EXP,0.4878197320341047,"The diameter of F is smaller than ρL:1r, so for all δ ≥ρL:1r, log N2(F, δ) = 0. For all δ ≤ρL:1r
568"
EXP,0.4884287454323995,"we choose ϵ =
δ
√"
E SO THAT,0.4890377588306943,"2e so that
√"
EXP,0.48964677222898906,"2 exp

ϵ2"
EXP,0.4902557856272838,"ρ2
L:1r2

ϵ ≤δ and therefore
569"
EXP,0.49086479902557856,"log N2(F, δ) ≤144s2
0eρ2
L:1r2
 L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.49147381242387334,dℓ′ + dℓ′−1
EXP,0.4920828258221681,"!2
1
δ2 + o(δ−2)."
EXP,0.49269183922046283,"(2) Our goal now is to use chaining / Dudley’s theorem to bound the Rademacher complexity
570"
EXP,0.4933008526187576,"R(F(X)) evaluated on a set X of size N (e.g. Lemma 27.4 in [Understanding Machine Learning])
571"
EXP,0.4939098660170524,"of our set:
572"
EXP,0.4945188794153471,"Lemma 7. Let c = maxf∈F
1
√"
EXP,0.4951278928136419,"N ∥f(X)∥, then for any integer M > 0,
573"
EXP,0.49573690621193667,"R(F(X)) ≤c2−M + 6c
√ N M
X"
EXP,0.49634591961023145,"k=1
2−k
q"
EXP,0.49695493300852617,"log N(F, c2−k)."
EXP,0.49756394640682094,"To apply it to our setting, first note that for all x ∈B(0, r), ∥fL:1(x)∥≤ρL:1r so that c =
574"
EXP,0.4981729598051157,"maxf∈F
1
√"
EXP,0.4987819732034105,"N ∥f(X)∥≤ρL:1r, we then have
575"
EXP,0.4993909866017052,"R(F(X)) ≤c2−M + 6c
√ N M
X"
EXP,0.5,"k=1
2−k12s0
√eρL:1r L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.5006090133982948,dℓ′ + dℓ′−1c−12k(1 + o(1))
EXP,0.5012180267965896,"= c2−M + 72
√"
EXP,0.5018270401948843,"N
Ms0
√eρL:1r L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.502436053593179,dℓ′ + dℓ′−1(1 + o(1)).
EXP,0.5030450669914738,"Taking M =
l
−log2

72
√"
EXP,0.5036540803897686,"N s0
√e PL
ℓ′=1
Rℓ′ ρℓ′
p"
EXP,0.5042630937880633,"dℓ′ + dℓ′−1
m
, we obtain
576"
EXP,0.5048721071863581,"R(F(X)) ≤72
√"
EXP,0.5054811205846529,"N
Ms0
√eρL:1r L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.5060901339829477,dℓ′ + dℓ′−1(1 + M(1 + o(1)))
EXP,0.5066991473812423,"≤144
√"
EXP,0.5073081607795371,"N
Ms0
√eρL:1r L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.5079171741778319,"dℓ′ + dℓ′−1 & −log2 72
√"
EXP,0.5085261875761267,"N
s0
√e L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.5091352009744214,dℓ′ + dℓ′−1 !'
EXP,0.5097442143727162,(1 + o(1))
EXP,0.510353227771011,"≤CρL:1r L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.5109622411693058,"dℓ′ + dℓ′−1
log N
√"
EXP,0.5115712545676004,"N
(1 + o(1))."
EXP,0.5121802679658952,"(3) For any ρ-Lipschitz loss function ℓ(x, f(x)) with |ℓ(x, y)| ≤c0, we know that with probability
577"
EXP,0.51278928136419,"1 −δ over the sampling of the training set X from the distribution π, we have for all f ∈F
578"
EXP,0.5133982947624848,"Ex∼π [ℓ(x, f(x))] −1 N N
X"
EXP,0.5140073081607796,"i=1
ℓ(xi, f(xi)) ≤2EX′ [R(ℓ◦F(X′))] + c0 r"
EXP,0.5146163215590743,2 log 2/δ N
EXP,0.5152253349573691,"≤2CρL:1r L
X ℓ′=1 Rℓ′ ρℓ′ p"
EXP,0.5158343483556638,"dℓ′ + dℓ′−1
log N
√"
EXP,0.5164433617539586,"N
(1 + o(1)) + c0 r"
EXP,0.5170523751522533,"2 log 2/δ N
. 579"
EXP,0.5176613885505481,"C
Composition of Sobolev Balls
580"
EXP,0.5182704019488429,"Proposition 8 (Proposition 3 from the main.). Given a distribution π with support in B(0, r), we
581"
EXP,0.5188794153471377,"have that with probability 1 −δ for all functions f ∈F = {f : ∥f∥W ν,2 ≤R, ∥f∥∞≤R}
582"
EXP,0.5194884287454324,L(f) −˜LN(f) ≤2C1REν/d(N) + c0 r
EXP,0.5200974421437271,"2 log 2/δ N
."
EXP,0.5207064555420219,where Er(N) = N −1
EXP,0.5213154689403167,2 if r > 1
EXP,0.5219244823386114,"2, Er(N) = N −1"
EXP,0.5225334957369062,2 log N if r = 1
EXP,0.523142509135201,"2, and Er(N) = N −r if r < 1"
EXP,0.5237515225334958,"2.
583"
EXP,0.5243605359317906,"Proof. (1) We know from Theorem 5.2 of [9] that the Sobolev ball BW ν,2(0, R) over any d-
584"
EXP,0.5249695493300852,"dimensional hypercube Ωsatisfies
585"
EXP,0.52557856272838,"log N2(BW ν,2(0, R), π, ϵ) ≤C0 R ϵ  d ν"
EXP,0.5261875761266748,"for a constant c and any measure π supported in the hypercube.
586"
EXP,0.5267965895249695,"(2) By Dudley’s theorem we can bound the Rademacher complexity of our function class B(X)
587"
EXP,0.5274056029232643,"evaluated on any training set X:
588"
EXP,0.5280146163215591,"R(B(X)) ≤R2−M + 6R
√ N M
X"
EXP,0.5286236297198539,"k=1
2−k s C0"
EXP,0.5292326431181485,"
R
R2−k  d ν"
EXP,0.5298416565164433,"= R2−M + 6R
√ N p C0 M
X"
EXP,0.5304506699147381,"k=1
2k( d"
EXP,0.5310596833130329,2ν −1).
EXP,0.5316686967113277,"If 2ν = d, we take M = 1"
LOG N AND OBTAIN THE BOUND,0.5322777101096224,"2 log N and obtain the bound
589 R
√"
LOG N AND OBTAIN THE BOUND,0.5328867235079172,"N
+ 6R
√ N p"
LOG N AND OBTAIN THE BOUND,0.5334957369062119,"C0
1
2 log N ≤C1Rlog N
√ N
."
LOG N AND OBTAIN THE BOUND,0.5341047503045067,"If 2ν > d, we take M = ∞and obtain the bound
590"
R,0.5347137637028014,"6R
√ N p C0"
D,0.5353227771010962,"2
d
2ν −1"
D,0.535931790499391,"1 −2
d
2ν −1 !"
D,0.5365408038976858,"≤C1R 1
√ N
."
D,0.5371498172959805,"If 2ν < d, we take M = ν"
D,0.5377588306942753,"d log N and obtain the bound
591"
D,0.53836784409257,"R2−M + 6R
√ N p"
D,0.5389768574908648,"C02
d
2ν −1
 
2M( d"
D,0.5395858708891595,2ν −1) −1
D,0.5401948842874543,"2
d
2ν −1 −1 !"
D,0.5408038976857491,≤C1RN −ν d .
D,0.5414129110840439,"Putting it all together, we obtain that R(B(X)) ≤C1Eν/d(N).
592"
D,0.5420219244823387,"(3) For any ρ-Lipschitz loss function ℓ(x, f(x)) with |ℓ(x, y)| ≤c0, we know that with probability
593"
D,0.5426309378806333,"1 −δ over the sampling of the training set X from the distribution π, we have for all f ∈F
594"
D,0.5432399512789281,"Ex∼π [ℓ(x, f(x))] −1 N N
X"
D,0.5438489646772229,"i=1
ℓ(xi, f(xi)) ≤2EX′ [R(ℓ◦F(X′))] + c0 r"
D,0.5444579780755177,2 log 2/δ N
D,0.5450669914738124,≤2C1Eν/d(N) + c0 r
D,0.5456760048721072,"2 log 2/δ N
. 595"
D,0.546285018270402,"Proposition 9. Let F1, . . . , FL be set of functions mapping through the sets Ω0, . . . , ΩL, then if all
596"
D,0.5468940316686967,"functions in Fℓare ρℓ-Lipschitz, we have
597"
D,0.5475030450669914,"log N2(FL ◦· · · ◦F1, L
X"
D,0.5481120584652862,"ℓ=1
ρL:ℓ+1ϵℓ) ≤ L
X"
D,0.548721071863581,"ℓ=1
log N2(Fℓ, ϵℓ)."
D,0.5493300852618758,"Proof. For any distribution π0 on Ωthere is a ϵ1-covering ˜
F1 of F1 with
 ˜
F1
 ≤N2(F1, ϵ1) then
598"
D,0.5499390986601705,"for any ˜f1 ∈˜
F1 we choose a ϵ2-covering ˜F2 w.r.t. the measure π1 which is the measure of f1(x) if
599"
D,0.5505481120584653,"x ∼π0 of F2 with
 ˜F2
 ≤N2(F2, ϵ), and so on until we obtain coverings for all ℓ. Then the set
600"
D,0.5511571254567601,"˜F =
n
˜fL ◦· · · ◦˜f1 : ˜f1 ∈˜F1, . . . , ˜fL ∈˜FL
o
is a PL
ℓ=1 ρL:ℓ+1ϵℓ-covering of F = FL ◦· · · ◦F1,
601"
D,0.5517661388550548,"indeed for any f = fL:1 we choose ˜f1 ∈˜F1, . . . , ˜fL ∈˜FL that cover f1, . . . , fL, then ˜fL:1 covers
602"
D,0.5523751522533495,"fL:1:
603"
D,0.5529841656516443,"fL:1 −˜fL:1

π ≤ L
X ℓ=1"
D,0.5535931790499391,"fL:ℓ◦˜fℓ−1:1 −fL:ℓ+1 ◦˜fℓ:1

π ≤ L
X ℓ=1"
D,0.5542021924482339,"fL:ℓ−fL:ℓ+1 ◦˜fℓ

πℓ−1 ≤ L
X"
D,0.5548112058465287,"ℓ=1
ρL:ℓ+1ϵℓ,"
D,0.5554202192448234,"and log cardinality of the set ˜F is bounded PL
ℓ=1 log N2(Fℓ, ϵℓ).
604"
D,0.5560292326431181,"Theorem
10.
Let
F
=
FL
◦
· · ·
◦
F1
where
Fℓ
=
605

fℓ: Rdℓ−1 →Rdℓs.t. ∥fℓ∥W νℓ,2 ≤Rℓ, ∥fℓ∥∞≤bℓ, Lip(fℓ) ≤ρℓ
	
, and let r∗
=
minℓrℓ
606"
D,0.5566382460414129,"for rℓ=
νℓ
dℓ−1 , then with probability 1 −δ we have for all f ∈F
607"
D,0.5572472594397077,"L(f) −˜LN(f) ≤ρC0 L
X"
D,0.5578562728380024,"ℓ=1
(CℓρL:ℓ+1Rℓ)"
D,0.5584652862362972,"1
r∗+1
!r∗+1"
D,0.559074299634592,Er∗(N) + c0 r
D,0.5596833130328868,"2 log 2/δ N
,"
D,0.5602923264311814,"where Cℓdepends only on dℓ−1, dℓ, νℓ, bℓ−1.
608"
D,0.5609013398294762,"Proof. (1) We know from Theorem 5.2 of [9] that the Sobolev ball BW νℓ,2(0, Rℓ) over any dℓ-
609"
D,0.561510353227771,"dimensional hypercube Ωsatisfies
610"
D,0.5621193666260658,"log N2(BW ν,2(0, Rℓ), πℓ−1, ϵℓ) ≤

Cℓ
Rℓ ϵℓ  1 rℓ"
D,0.5627283800243605,"for a constant Cℓthat depends on the size of hypercube and the dimension dℓand the regularity νℓ
611"
D,0.5633373934226553,"and any measure πℓ−1 supported in the hypercube.
612"
D,0.5639464068209501,"Thus Proposition 9 tells us that the composition of the Sobolev balls satisfies
613"
D,0.5645554202192449,"log N2(FL ◦· · · ◦F1, L
X"
D,0.5651644336175395,"ℓ=1
ρL:ℓ+1ϵℓ) ≤ L
X ℓ=1"
D,0.5657734470158343,"
Cℓ
Rℓ ϵℓ  1 rℓ."
D,0.5663824604141291,"Given r∗
=
minℓrℓ, we can bound it by PL
ℓ=1

Cℓ
Rℓ ϵℓ  1"
D,0.5669914738124239,"r∗and by then choosing ϵℓ
=
614"
D,0.5676004872107187,"ρ−1
L:ℓ+1(ρL:ℓ+1CℓRℓ)
1
r∗+1
P"
D,0.5682095006090134,"ℓ(ρL:ℓ+1CℓRℓ)
1
r∗+1
ϵ, we obtain that
615"
D,0.5688185140073082,"log N2(FL ◦· · · ◦F1, ϵ) ≤ L
X"
D,0.5694275274056029,"ℓ=1
(ρL:ℓ+1CℓRℓ)"
D,0.5700365408038977,"1
r∗+1
!r∗+1 ϵ−1 r∗."
D,0.5706455542021924,"(2,3) It the follows by a similar argument as in points (2, 3) of the proof of Proposition 8 that there is
616"
D,0.5712545676004872,"a constant C0 such that with probability 1 −δ for all f ∈F
617"
D,0.571863580998782,"L(f) −˜LN(f) ≤C0 L
X"
D,0.5724725943970768,"ℓ=1
(ρL:ℓ+1CℓRℓ)"
D,0.5730816077953715,"1
r∗+1
!r∗+1"
D,0.5736906211936662,Er∗(N) + c0 r
D,0.574299634591961,2 log 2/δ N 618
D,0.5749086479902558,"D
Generalization at the Regularized Global Minimum
619"
D,0.5755176613885505,"In this section, we first give the proof of Theorem 5 and then present detailed proofs of lemmas used
620"
D,0.5761266747868453,"in the proof. The lemmas are largely inspired by [5] and may be of independent interest.
621"
D,0.5767356881851401,"D.1
Theorem 5 in Section 4.2
622"
D,0.5773447015834349,"Theorem 11 (Theorem 5 in the main). Given a true function f ∗
L∗:1 = f ∗
L∗◦· · ·◦f ∗
1 going through the
623"
D,0.5779537149817296,"dimensions d∗
0, . . . , d∗
L∗, along with a continuous input distribution π0 supported in B(0, b0), such
624"
D,0.5785627283800243,"that the distributions πℓof f ∗
ℓ(x) (for x ∼π0) are continuous too and supported inside B(0, bℓ) ⊂
625"
D,0.5791717417783191,"Rd∗
ℓ. Further assume that there are differentiabilities νℓand radii Rℓsuch that ∥f ∗
ℓ∥W νℓ,2(B(0,bℓ)) ≤
626"
D,0.5797807551766139,"Rℓ, and ρℓsuch that Lip(f ∗
ℓ) ≤ρℓ. For a infinite width AccNet with L ≥L∗and constant width
627"
D,0.5803897685749086,"d ≥d∗
1, . . . , d∗
L∗−1, we have for the ratios ˜rℓ=
νℓ
d∗
ℓ+3:
628"
D,0.5809987819732034,"• At a global minimizer ˆfL:1 of the regularized loss f1, . . . , fL 7→˜LN(fL:1)+λR(f1, . . . , fL),
629"
D,0.5816077953714982,we have L( ˆfL:1) = ˜O(N −min{ 1
D,0.582216808769793,"2 ,˜r1,...,˜rL∗}).
630"
D,0.5828258221680876,"• At a global minimizer ˆfL:1 of the regularized loss f1, . . . , fL 7→˜LN(fL:1)+λ QL
ℓ=1 ∥fℓ∥F1,
631"
D,0.5834348355663824,we have L( ˆfL:1) = ˜O(N −1
D,0.5840438489646772,"2 +PL∗
ℓ=1 max{0,˜rℓ−1"
D,0.584652862362972,"2 }).
632"
D,0.5852618757612668,"Proof. If f ∗= f ∗
L∗◦· · · ◦f ∗
1 with L∗≤L, intermediate dimensions d∗
0, . . . , d∗
L∗, along with a
633"
D,0.5858708891595615,"continuous input distribution π0 supported in B(0, b0), such that the distributions πℓof f ∗
ℓ(x) (for
634"
D,0.5864799025578563,"x ∼π0) are continuous too and supported inside B(0, bℓ) ⊂Rd∗
ℓ. Further assume that there are
635"
D,0.587088915956151,"differentiabilities ν∗
ℓand radii Rℓsuch that ∥f ∗
ℓ∥W ν∗
ℓ,2(B(0,bℓ)) ≤Rℓ.
636"
D,0.5876979293544458,"We first focus on the L = L∗case and then extend to the L > L∗case.
637"
D,0.5883069427527405,"Each f ∗
ℓcan be approximated by another function ˜fℓwith bounded F1-norm and Lipschitz constant.
638"
D,0.5889159561510353,"Actually if 2ν∗
ℓ≥d∗
ℓ−1 + 3 one can choose ˜fℓ= f ∗
ℓsince ∥f ∗
ℓ∥F1 ≤CℓRℓby Lemma 14, and by
639"
D,0.5895249695493301,"assumption Lip( ˜fℓ) ≤ρℓ. If 2ν∗
ℓ< d∗
ℓ−1 + 3, then by Lemma 13 we know that there is a ˜fℓwith
640
 ˜fℓ

F1 ≤CℓRℓϵ
−
1
2˜rℓ+1"
D,0.5901339829476249,"ℓ
and Lip( ˜fℓ) ≤CℓLip(f ∗
ℓ) ≤Cℓρℓand error
641"
D,0.5907429963459196,"f ∗
ℓ−˜fℓ

L2(πℓ−1) ≤cℓ
f ∗−˜fℓ

L2(B(0,bℓ)) ≤cℓϵℓ."
D,0.5913520097442144,"Therefore the composition ˜fL:1 satisfies
642"
D,0.5919610231425091,"f ∗
L:1 −ˆfL:1

L2(πℓ−1) ≤ L
X ℓ=1"
D,0.5925700365408039,"˜fL:ℓ+1 ◦f ∗
ℓ:1 −˜fL:ℓ◦f ∗
ℓ−1:1

L2(π) ≤ L
X"
D,0.5931790499390986,"ℓ=1
Lip( ˜fL:ℓ+1)cℓϵℓ ≤ L
X"
D,0.5937880633373934,"ℓ=1
ρL:ℓ+1CL:ℓ+1cℓϵℓ."
D,0.5943970767356882,"For any L ≥L∗, dimensions dℓ≥d∗
ℓand widths wℓ≥N, we can build an AccNet that fits eactly
643"
D,0.595006090133983,"˜fL:1, by simply adding zero weights along the additional dimensions and widths, and by adding
644"
D,0.5956151035322778,"identity layers if L > L∗, since it is possible to represent the identity on Rd with a shallow network
645"
D,0.5962241169305724,"with 2d neurons and F1-norm 2d (by having two neurons eiσ(eT
i ·) and −eiσ(−eT
i ·) for each basis
646"
D,0.5968331303288672,"ei). Since the cost in parameter norm of representing the identity scales with the dimension, it is
647"
D,0.597442143727162,"best to add those identity layers at the minimal dimension min{d∗
0, . . . , d∗
L∗}. We therefore end up
648"
D,0.5980511571254568,"with a AccNet with L −L∗identity layers (with F1 norm 2 min{d∗
0, . . . , d∗
L∗}) and L∗layers that
649"
D,0.5986601705237515,"approximate each of the f ∗
ℓwith a bounded F1-norm function ˜fℓ.
650"
D,0.5992691839220463,"Since f ∗
L:1 has zero population loss, the population loss of the AccNet ˜fL:1 is bounded by
651"
D,0.5998781973203411,"ρ PL
ℓ=1 ρL:ℓ+1CL:ℓ+1cℓϵℓ. By McDiarmid’s inequality, we know that with probability 1 −δ over the
652"
D,0.6004872107186358,"sampling of the training set, the training loss is bounded by ρ PL
ℓ=1 ρL:ℓ+1CL:ℓ+1cℓϵℓ+ B
q"
D,0.6010962241169305,2 log 2/δ
D,0.6017052375152253,"N
.
653"
D,0.6023142509135201,"(1) The global minimizer ˆfL:1 = ˆfL ◦· · · ◦ˆf1 of the regularized loss (with the first regularization
654"
D,0.6029232643118149,"term) is therefore bounded by
655 ρ L
X"
D,0.6035322777101096,"ℓ=1
ρL:ℓ+1CL:ℓ+1cℓϵℓ+ B r"
D,0.6041412911084044,"2 log 2/δ N + λ
√"
D,0.6047503045066992,2d
D,0.6053593179049939,""" L∗
Y"
D,0.6059683313032886,"ℓ=1
Cℓρℓ L∗
X ℓ=1"
D,0.6065773447015834,"1
Cℓρℓ"
D,0.6071863580998782,"(
CℓRℓ
2ν∗
ℓ≥d∗
ℓ−1 + 3"
D,0.607795371498173,"CℓRℓϵ
−
1
2˜rℓ+1"
D,0.6084043848964678,"ℓ
2ν∗
ℓ< d∗
ℓ−1 + 3
+ 2(L −L∗) min{d∗
0, . . . , d∗
L∗} # ."
D,0.6090133982947625,Taking ϵℓ= E˜rmin(N) and λ = N −1
D,0.6096224116930572,"2 log N, this is upper bounded by
656 "" ρ L
X"
D,0.610231425091352,"ℓ=1
ρL:ℓ+1CL:ℓ+1cℓ+ C
√"
DR,0.6108404384896468,"2dr L∗
Y"
DR,0.6114494518879415,"ℓ=1
Cℓρℓ L∗
X ℓ=1 Rℓ"
DR,0.6120584652862363,"ρℓ
+ 2(L −L∗) min{d∗
0, . . . , d∗
L∗} #"
DR,0.6126674786845311,E˜rmin(N)+B r
DR,0.6132764920828259,"2 log 2/δ N
."
DR,0.6138855054811205,"which implies that at the globla minimizer of the regularized loss, the (unregularized) train loss is of
657"
DR,0.6144945188794153,"order E˜rmin(N) and the complexity measure R( ˆf1, . . . , ˆfL) is of order 1"
DR,0.6151035322777101,"N E˜rmin(N) which implies
658"
DR,0.6157125456760049,"that the test error will be of order
659"
DR,0.6163215590742996,"L(f) ≤ "" 2ρ L
X"
DR,0.6169305724725944,"ℓ=1
ρL:ℓ+1CL:ℓ+1cℓ+ 2C
√"
DR,0.6175395858708892,"2dr L∗
Y"
DR,0.618148599269184,"ℓ=1
Cℓρℓ L∗
X ℓ=1 Rℓ"
DR,0.6187576126674786,"ρℓ
+ 2(L −L∗) min{d∗
0, . . . , d∗
L∗} #"
DR,0.6193666260657734,E˜rmin(N)
DR,0.6199756394640682,+ (2B + c0) r
DR,0.620584652862363,"2 log 2/δ N
."
DR,0.6211936662606578,"(2) Let us now consider adding the closer to traditional L2-regularization Lλ(fL:1) = L(fL:1) +
660"
DR,0.6218026796589525,"λ QL
ℓ=1 ∥fℓ∥F1. ,We see that the global minimizer ˆfL:1 of the L2-regularized loss is upper bounded
661"
DR,0.6224116930572473,"by
662 ρ L
X"
DR,0.623020706455542,"ℓ=1
ρL:ℓ+1CL:ℓ+1cℓϵℓ+B r"
DR,0.6236297198538368,"2 log 2/δ N
+λ"
DR,0.6242387332521315,""" L∗
Y ℓ=1"
DR,0.6248477466504263,"(
CℓRℓ
2ν∗
ℓ≥d∗
ℓ−1 + 3"
DR,0.6254567600487211,"CℓRℓϵ
−
1
2˜rℓ+1"
DR,0.6260657734470159,"ℓ
2ν∗
ℓ< d∗
ℓ−1 + 3 #"
DR,0.6266747868453106,"(2 min{d∗
0, . . . , d∗
L∗})(L−L∗)."
DR,0.6272838002436053,Which for ϵℓ= E˜rmin(N) and λ = N −1
DR,0.6278928136419001,"N is upper bounded by
663 ρ L
X"
DR,0.6285018270401949,"ℓ=1
ρL:ℓ+1CL:ℓ+1cℓE˜rmin(N)+B r"
DR,0.6291108404384896,2 log 2/δ
DR,0.6297198538367844,"N
+N −1 2"
DR,0.6303288672350792,""" L∗
Y"
DR,0.630937880633374,"ℓ=1
CℓRℓ
√"
DR,0.6315468940316687,NE˜rmin(N) #
DR,0.6321559074299634,"(2 min{d∗
0, . . . , d∗
L∗})(L−L∗)."
DR,0.6327649208282582,"Which implies that both the train error is of order N −1 2 QL∗ ℓ=1
√"
DR,0.633373934226553,"NE˜rmin(N) and the product of the
664"
DR,0.6339829476248477,"F1-norms is of order QL∗ ℓ=1
√"
DR,0.6345919610231425,"NE˜rmin(N).
665"
DR,0.6352009744214373,"Now note that the product of the F1-norms bounds the complexity measure up to a constant since
666"
DR,0.6358099878197321,"Lip(f) ≤∥f∥F1
667"
DR,0.6364190012180267,"R(f1, . . . , fL) = r L
Y"
DR,0.6370280146163215,"ℓ=1
Lip(fℓ) L
X ℓ=1"
DR,0.6376370280146163,"∥fℓ∥F1
Lip(fℓ) p"
DR,0.6382460414129111,"dℓ−1 + dℓ≤L
√"
D,0.6388550548112059,"2d L
Y"
D,0.6394640682095006,"ℓ=1
∥f∥F1 ."
D,0.6400730816077954,"And since at the global minimum the product of the F1-norms is of order QL∗ ℓ=1
√"
D,0.6406820950060901,"NE˜rmin(N) the
668"
D,0.6412911084043849,"test error will of order
QL∗ ℓ=1
√"
D,0.6419001218026796,"NE˜rℓ(N)

log N
√"
D,0.6425091352009744,"N .
669"
D,0.6431181485992692,"Note that if there is at a most one ℓwhere ˜rℓ>
1
2 then the rate is up to log term the same as
670"
D,0.643727161997564,"E˜rmin(N).
671"
D,0.6443361753958587,"D.2
Lemmas on approximating Sobolev functions
672"
D,0.6449451887941535,"Now we present the lemmas used in this proof above that concern the approximation errors and
673"
D,0.6455542021924482,"Lipschitz constants of Sobolev functions and compositions of them. We will bound the F2-norm and
674"
D,0.646163215590743,"note that the F2-norm is larger than the F1-norm, cf. [5, Section 3.1].
675"
D,0.6467722289890377,"Lemma 12 (Approximation for Sobolev function with bounded error and Lipschitz constant).
676"
D,0.6473812423873325,"Suppose g : Sd →R is an even function with bounded Sobolev norm ∥g∥2
W ν,2 ≤R with 2ν ≤d + 2,
677"
D,0.6479902557856273,"with inputs on the unit d-dimensional sphere. Then for every ϵ > 0, there is ˆg ∈G2 with small
678"
D,0.6485992691839221,"approximation error ∥g −ˆg∥L2(Sd) = C(d, ν, R)ϵ, bounded Lipschitzness Lip(ˆg) ≤C′(d)Lip(g),
679"
D,0.6492082825822169,"and bounded norm
680"
D,0.6498172959805115,"∥ˆg∥F2 ≤C′′(d, ν, R)ϵ−d+3−2ν 2ν
."
D,0.6504263093788063,"Proof. Given our assumptions on the target function g, we may decompose g(x) = P∞
k=0 gk(x)
681"
D,0.6510353227771011,"along the basis of spherical harmonics with g0(x) =
R"
D,0.6516443361753959,"Sd g(y)dτd(y) being the mean of g(x) over the
682"
D,0.6522533495736906,"uniform distribution τd over Sd. The k-th component can be written as
683"
D,0.6528623629719854,"gk(x) = N(d, k)
Z"
D,0.6534713763702802,"Sd
g(y)Pk(xT y)dτd(y)"
D,0.6540803897685749,"with N(d, k) = 2k+d−1"
D,0.6546894031668696,"k
 k+d−2
d−1

and a Gegenbauer polynomial of degree k and dimension d + 1:
684"
D,0.6552984165651644,"Pk(t) = (−1/2)k
Γ(d/2)
Γ(k + d/2)(1 −t2)(2−d)/2 dk"
D,0.6559074299634592,"dtk (1 −t2)k+(d−2)/2,"
D,0.656516443361754,"known as Rodrigues’ formula. Given the assumption that the Sobolev norm ∥g∥2
W ν,2 is upper
685"
D,0.6571254567600487,"bounded, we have ∥f∥2
L2(Sd) ≤C0(d, ν)R for f = ∆ν/2g where ∆is the Laplacian on Sd [18, 5].
686"
D,0.6577344701583435,"Note that gk are eigenfunctions of the Laplacian with eigenvalues k(k + d −1) [4], thus
687"
D,0.6583434835566383,"∥gk∥2
L2(Sd) = ∥fk∥2
L2(Sd)(k(k + d −1))−ν ≤∥fk∥2
L2(Sd)k−2ν ≤C1(d, ν, R)k−2ν−1
(1)"
D,0.658952496954933,"where the last inequality holds because ∥f∥2
L2(Sd) = P"
D,0.6595615103532277,"k≥0 ∥fk∥2
L2(Sd) converges. Note using the
688"
D,0.6601705237515225,"Hecke-Funk formula, we can also write gk as scaled pk for the underlying density p of the F1 and
689"
D,0.6607795371498173,"F2-norms:
690"
D,0.6613885505481121,gk(x) = λkpk(x)
D,0.6619975639464069,"where λk =
ωd−1"
D,0.6626065773447016,"ωd
R 1
−1 σ(t)Pk(t)(1 −t2)(d−2)/2dt = Ω(k−(d+3)/2) [5, Appendix D.2] and ωd
691"
D,0.6632155907429963,"denotes the surface area of Sd. Then by definition of ∥· ∥F2, for some probability density p,
692"
D,0.6638246041412911,"∥g∥2
F2 =
Z"
D,0.6644336175395859,"Sd
|p|2dτ(v) = ∥p∥2
L2(Sd) =
X"
D,0.6650426309378806,"0≤k
∥pk∥2
L2(Sd) =
X"
D,0.6656516443361754,"0≤k
λ−2
k ∥gk∥2
L2(Sd)."
D,0.6662606577344702,"Now to approximate g, consider function ˆg defined by truncating the “high frequencies” of g, i.e.
693"
D,0.666869671132765,"setting ˆgk = 1[k ≤m]gk for some m > 0 we specify later. Then we can bound the norm with
694"
D,0.6674786845310596,"∥ˆg∥2
F2 =
X"
D,0.6680876979293544,"0≤k:λk̸=0
λ−2
k ∥ˆgk∥2
L2(Sd) =
X"
D,0.6686967113276492,"0≤k≤m
λk̸=0"
D,0.669305724725944,"λ−2
k ∥gk∥2
L2(Sd)"
D,0.6699147381242387,"(a)
≤C2(d, ν, R)
X"
D,0.6705237515225335,"0≤k≤m
kd+2−2ν"
D,0.6711327649208283,"(b)
≤C3(d, ν, R)md+3−2ν"
D,0.6717417783191231,"where (a) uses Eq 1 and λk = Ω(k−(d+3)/2); (b) approximates by integral.
695"
D,0.6723507917174177,"To bound the approximation error,
696"
D,0.6729598051157125,"∥g −ˆg∥2
L2(Sd) =  X"
D,0.6735688185140073,"k>m
gk  2"
D,0.6741778319123021,"L2(Sd)
≤
X"
D,0.6747868453105969,"k>m
∥gk∥2
L2(Sd)"
D,0.6753958587088916,"≤C4(d, ν, R)
X"
D,0.6760048721071864,"k>m
k−2ν−1"
D,0.6766138855054811,"≤C5(d, ν, R)m−2ν
by integral approximation."
D,0.6772228989037758,"Finally, choosing m = ϵ−1"
D,0.6778319123020706,"ν , we obtain ∥g −ˆg∥L2(Sd) ≤C(d, ν, R)ϵ and
697"
D,0.6784409257003654,"∥ˆg∥F2 ≤C′(d, ν, R)ϵ−d+3−2ν 2ν
."
D,0.6790499390986602,"Then it remains to bound Lip(ˆg) for our constructed approximation. By construction and by [13,
698"
D,0.679658952496955,"Theorem 2.1.3], we have ˆg = g ∗h with now
699"
D,0.6802679658952497,"h(t) = m
X"
D,0.6808769792935444,"k=0
hkPk(t),
t ∈[−1, 1]"
D,0.6814859926918392,"by orthogonality of the Gegenbauer polynomial Pk’s and the convolution is defined as
700"
D,0.682095006090134,(g ∗h)(x) := 1 ωd Z
D,0.6827040194884287,"Sd
g(y)h(⟨x, y⟩)dy."
D,0.6833130328867235,"The coefficients for 0 ≤k ≤m given by [13, Theorem 2.1.3] are
701"
D,0.6839220462850183,"hk
(a)
= ωd+1 ωd"
D,0.6845310596833131,"Γ(d −1)
Γ(d −1 + k)Pk(1)k!(k + (d −1)/2)Γ((d −1)/2)2"
D,0.6851400730816078,π22−dΓ(d −1 + k)
D,0.6857490864799025,"(b)
= O

k
Γ(d −1 + k) "
D,0.6863580998781973,"where (a) follows from the (inverse of) weighted L2 norm of Pk; (b) plugs in the unit constant
702"
D,0.6869671132764921,Pk(1) = Γ(k+d−1)
D,0.6875761266747868,"Γ(d−1)k! and suppresses the dependence on d. Note that the constant factor
Γ(d−1)
Γ(d−1+k)
703"
D,0.6881851400730816,"comes from the difference in the definitions of the Gegenbauer polynomials here and in [13]. Then
704"
D,0.6887941534713764,"we can bound
705"
D,0.6894031668696712,"∥∇ˆg(x)∥op ≤
Z"
D,0.6900121802679658,"Sd
∥∇g(y)∥op|h(⟨x, y⟩)|dy"
D,0.6906211936662606,"≤Lip(g)
Z"
D,0.6912302070645554,"Sd
|h(⟨x, y⟩)|dy"
D,0.6918392204628502,"≤√ωdLip(g)
Z"
D,0.692448233861145,"Sd
h(⟨x, y⟩)2dy
1/2
by Cauchy-Schwartz"
D,0.6930572472594397,"= √ωdLip(g)  
m
X k,j=0 Z"
D,0.6936662606577345,"Sd
hkhjPk(⟨x, y⟩)Pj(⟨x, y⟩)dy   1/2"
D,0.6942752740560292,"= √ωdLip(g)  
m
X k,j=0 Z 1"
D,0.694884287454324,"−1
hkhjPk(t)Pj(t)(1 −t2)
d−2"
DT,0.6954933008526187,2 dt   1/2
DT,0.6961023142509135,"by [13, Eq A.5.1]"
DT,0.6967113276492083,"= √ωdLip(g) m
X"
DT,0.6973203410475031,"k=0
h2
k Z 1"
DT,0.6979293544457978,"−1
Pk(t)2(1 −t2)
d−2"
DT,0.6985383678440926,2 dt !1/2
DT,0.6991473812423873,by orthogonality of Pk’s w.r.t. this measure
DT,0.6997563946406821,"= √ωdLip(g) m
X"
DT,0.7003654080389768,"k=0
h2
k
π22−dΓ(d −1 + k)
k!(k + (d −1)/2)Γ((d −1)/2)2 !1/2"
DT,0.7009744214372716,= √ωdLip(g) 
DT,0.7015834348355664,"O(1) + m
X"
DT,0.7021924482338612,"k=1
O

k
Γ(d −1 + k)k! !1/2"
DT,0.702801461632156,= √ωdLip(g)C(d)
DT,0.7034104750304506,"for some constant C(d) that only depends on d. Hence Lip(ˆg) = C′(d)Lip(g).
706"
DT,0.7040194884287454,"The next lemma adapts Lemma 12 to inputs on balls instead of spheres following the construction in
707"
DT,0.7046285018270402,"[5, Proposition 5].
708"
DT,0.705237515225335,"Lemma 13. Suppose f : B(0, b) →R has bounded Sobolev norm ∥f∥2
W ν,2 ≤R with ν ≤(d+2)/2
709"
DT,0.7058465286236297,"even, where B(0, b) = {x ∈Rd : ∥x∥2 ≤b} is the radius-b ball. Then for every ϵ > 0 there exists
710"
DT,0.7064555420219245,"fϵ ∈F2 such that ∥f −fϵ∥L2(B(0,b)) = C(d, ν, b, R)ϵ, Lip(fϵ) ≤C′(b, d)Lip(f), and
711"
DT,0.7070645554202193,"∥fϵ∥F2 ≤C′′(d, ν, b, R)ϵ−d+3−2ν 2ν"
DT,0.707673568818514,"Proof. Define g(z, a) = f
  2bz"
DT,0.7082825822168087,"a

a on (z, a) ∈Sd with z ∈Rd and
1
√"
DT,0.7088915956151035,"2 ≤a ∈R. One may
712"
DT,0.7095006090133983,"verify that unit-norm (z, a) with a ≥
1
√"
DT,0.7101096224116931,"2 is sufficient to cover B(0, b) by setting x =
bz"
DT,0.7107186358099878,"a and
713"
DT,0.7113276492082826,"solve for (z, a). Then we have bounded ∥g∥2
W ν,2 ≤bνR and may apply Lemma 12 to get ˆg with
714"
DT,0.7119366626065774,"∥g −ˆg∥L2(Sd) ≤C(d, ν, b, R)ϵ. Letting fϵ(x) = ˆg
  ax"
DT,0.7125456760048721,"b , a

a−1 for the corresponding
  ax"
DT,0.7131546894031668,"b , a

∈Sd
715"
DT,0.7137637028014616,"gives the desired upper bounds.
716"
DT,0.7143727161997564,"Lemma 14. Suppose f : B(0, b) →R has bounded Sobolev norm ∥f∥2
W ν,2 ≤R with ν ≥(d+3)/2
717"
DT,0.7149817295980512,"even. Then f ∈F2 and ∥f∥F2 ≤C(d, ν)bνR.
718"
DT,0.715590742996346,"In particular, W ν,2 ⊆F2 for ν ≥(d + 3)/2 even.
719"
DT,0.7161997563946407,"Proof. This lemma reproduces [5, Proposition 5] to functions with bounded Sobolev L2 norm instead
720"
DT,0.7168087697929354,"of L∞norm. The proof follows that of Lemma 12 and Lemma 13 and noticing that by Eq 1,
721"
DT,0.7174177831912302,"∥g∥2
F2 =
X"
DT,0.718026796589525,"0≤k:λk̸=0
λ−2
k ∥gk∥2
L2(Sd) ≤
X"
DT,0.7186358099878197,"0≤k
kd+3−2ν∥(∆ν/2g)k∥2
L2(Sd)"
DT,0.7192448233861145,"≤∥∆ν/2g∥2
L2(Sd)
≤C1(d, ν)∥g∥2
W ν,2
≤C1(d, ν)R. 722"
DT,0.7198538367844093,"Finally, we remark that the above lemmas extend straightforward to functions f : B(0, b) →Rd′
723"
DT,0.7204628501827041,"with multi-dimensional outputs, where the constants then depend on the output dimension d′ too.
724"
DT,0.7210718635809987,"D.3
Lemma on approximating compositions of Sobolev functions
725"
DT,0.7216808769792935,"With the lemmas given above and the fact that the F2-norm upper bounds the F1-norm, we can find
726"
DT,0.7222898903775883,"infinite-width DNN approximations for compositions of Sobolev functions, which is also pointed out
727"
DT,0.7228989037758831,"in the proof of Theorem 5.
728"
DT,0.7235079171741778,"Lemma 15. Assume the target function f : Ω→Rdout, with Ω⊆B(0, b) ⊆Rdin, satisfies:
729"
DT,0.7241169305724726,"• f = gk ◦· · · ◦g1 a composition of k Sobolev functions gi : Rdi →Rdi+1 with bounded
730"
DT,0.7247259439707674,"norms ∥gi∥2
W νi,2 ≤R for i = 1, . . . , k, with d1 = din;
731"
DT,0.7253349573690622,"• f is Lipschitz, i.e. Lip(gi) < ∞for i = 1, . . . , k.
732"
DT,0.7259439707673568,"If νi ≤(di + 2)/2 for any i, i.e. less smooth than needed, for depth L ≥k and any ϵ > 0, there is an
733"
DT,0.7265529841656516,"infinite-width DNN ˜f such that
734"
DT,0.7271619975639464,"• Lip( ˜f) ≤C1
Qk
i=1 Lip(gi);
735"
DT,0.7277710109622412,"• ∥˜f −f∥L2 ≤C2ϵ;
736"
DT,0.728380024360536,"the constants C1 depends on all of the input dimensions di (to gi) and dout, and C2 depends on
737"
DT,0.7289890377588307,"di, dout, νi, b, R, k, and Lip(gi) for all i.
738"
DT,0.7295980511571255,"If otherwise νi ≥(di + 3)/2 for all i, we can have ˜f = f where each layer has a parameter norm
739"
DT,0.7302070645554202,"bounded by C3R, with C3 depending on di, dout, νi, and b.
740"
DT,0.730816077953715,"Proof. Note that by Lipschitzness,
741"
DT,0.7314250913520097,"(gi ◦· · · ◦g1)(Ω) ⊆B  0, b iY"
DT,0.7320341047503045,"j=1
Lip(gj)  ,"
DT,0.7326431181485993,"i.e. the pre-image of each component lies in a ball. By Lemma 12, for each gi, if νi ≤(di + 2)/2,
742"
DT,0.7332521315468941,"we have an approximation ˆgi on a slightly larger ball b′
i = b Qi−1
j=1 C′′(dj, dj+1)Lip(gj) such that
743"
DT,0.7338611449451888,"• ∥gi −ˆgi∥L2 ≤C(di, di+1, νi, b′
i, R)ϵ;
744"
DT,0.7344701583434835,"• ∥ˆgi∥F2 ≤C′(di, di+1, νi, b′
i, R)ϵ"
DT,0.7350791717417783,di+3−2νi
DT,0.7356881851400731,"2νi
;
745"
DT,0.7362971985383678,"• Lip(ˆgi) ≤C′′(di, di+1)Lip(gi);
746"
DT,0.7369062119366626,"where di is the input dimension of gi. Write the constants as Ci, C′
i, and C′′
i for notation simplicity.
747"
DT,0.7375152253349574,"Note that the Lipschitzness of the approximations ˆgi’s guarantees that, when they are composed,
748"
DT,0.7381242387332522,"(ˆgi−1 ◦· · · ◦ˆg1)(Ω) lies in a ball of radius b′
i = b Qi−1
j=1 C′′
j Lip(gj), hence the approximation error
749"
DT,0.738733252131547,"remains bounded while propagating. While each ˆgi is a (infinite-width) layer, for the other L −k
750"
DT,0.7393422655298416,"layers, we may have identity layers5.
751"
DT,0.7399512789281364,"Let ˜f be the composed DNN of these layers. Then we have
752"
DT,0.7405602923264312,"Lip( ˜f) ≤ k
Y"
DT,0.741169305724726,"i=1
C′′
i Lip(gi) = C′′(d1, . . . , dk, dout) k
Y"
DT,0.7417783191230207,"i=1
Lip(gi)"
DT,0.7423873325213155,"and approximation error
753"
DT,0.7429963459196103,"∥˜f −f∥L2 ≤ k
X"
DT,0.743605359317905,"i=1
Ciϵ
Y"
DT,0.7442143727161997,"j>i
C′′
j Lip(gj) = O(ϵ)"
DT,0.7448233861144945,"where the last equality suppresses the dependence on di, dout, νi, b, R, k, and Lip(gi) for i =
754"
DT,0.7454323995127893,"1, . . . , k.
755"
DT,0.7460414129110841,"In particular, by Lemma 14, if νi ≥(di + 3)/2 for any i = 1, . . . , k, we can take ˆgi = gi. If this
756"
DT,0.7466504263093788,"holds for all i, then we can have ˜f = f while each layer has a F2-norm bounded by O(R).
757"
DT,0.7472594397076736,"E
Technical results
758"
DT,0.7478684531059683,"Here we show a number of technical results regarding the covering number.
759"
DT,0.7484774665042631,"First, here is a bound for the covering number of Ellipsoids, which is a simple reformulation of
760"
DT,0.7490864799025578,"Theorem 2 of [17]:
761"
DT,0.7496954933008526,"Theorem 16. The d-dimensional ellipsoid E = {x : xT K−1x ≤1} with radii √λi for λi the i-th
762"
DT,0.7503045066991474,"eigenvalue of K satisfies log N2 (E, ϵ) = Mϵ (1 + o(1)) for
763"
DT,0.7509135200974422,"Mϵ =
X"
DT,0.7515225334957369,"i:√λi≥ϵ
log
√λi ϵ"
DT,0.7521315468940317,"if one has log
√λ1"
DT,0.7527405602923264,"ϵ
= o

M 2
ϵ
kϵ log d

for kϵ =

i : √λi ≥ϵ
	
764"
DT,0.7533495736906212,"For our purpose, we will want to cover a unit ball B = {w : ∥w∥≤1} w.r.t. to a non-isotropic norm
765"
DT,0.7539585870889159,"∥w∥2
K = wT Kw, but this is equivalent to covering E with an isotropic norm:
766"
DT,0.7545676004872107,"Corollary 17. The covering number of the ball B = {w : ∥w∥≤1} w.r.t. the norm ∥w∥2
K = wT Kw
767"
DT,0.7551766138855055,"satisfies log N (B, ∥·∥K , ϵ) = Mϵ (1 + o(1)) for the same Mϵ as in Theorem 16 and under the same
768"
DT,0.7557856272838003,"condition.
769"
DT,0.756394640682095,"Furthermore, log N (B, ∥·∥K , ϵ) ≤TrK"
DT,0.7570036540803897,"2ϵ2 (1 + o(1)) as long as log d = o
 √ TrK"
DT,0.7576126674786845,"ϵ

log
√ TrK"
DT,0.7582216808769793,"ϵ
−1
.
770"
DT,0.758830694275274,"Proof. If ˜E is an ϵ-covering of E w.r.t. to the L2-norm, then ˜B = K−1"
DT,0.7594397076735688,"2 ˜E is an ϵ-covering of B
771"
DT,0.7600487210718636,"w.r.t. the norm ∥·∥K, because if w ∈B, then
√"
DT,0.7606577344701584,"Kw ∈E and so there is an ˜x ∈˜E such that
772
x −
√"
DT,0.761266747868453,"Kw
 ≤ϵ, but then ˜w =
√"
DT,0.7618757612667478,"K
−1x covers w since ∥˜w −w∥K =
x −
√"
DT,0.7624847746650426,"Kw

K ≤ϵ.
773"
DT,0.7630937880633374,"5Since the domain is always bounded here, one can let the bias translate the domain to the first quadrant and
let the weight be the identity matrix, cf. the construction in [45, Proposition B.1.3]."
DT,0.7637028014616322,Since λi ≤TrK
DT,0.7643118148599269,"i , we have K ≤¯K for ¯K the matrix obtained by replacing the i-th eigenvalue λi of
774"
DT,0.7649208282582217,K by TrK
DT,0.7655298416565165,"i , and therefore N (B, ∥·∥K , ϵ) ≤N (B, ∥·∥¯
K , ϵ) since ∥·∥K ≤∥·∥¯
K. We now have the
775"
DT,0.7661388550548112,"a[proximation log N (B, ∥·∥¯
K , ϵ) = ¯
Mϵ (1 + o(1)) for
776"
DT,0.7667478684531059,"¯
Mϵ = ¯kϵ
X"
DT,0.7673568818514007,"i=1
log √ TrK
√ iϵ"
DT,0.7679658952496955,"¯kϵ =
TrK ϵ2 
."
DT,0.7685749086479903,"We now have the simplification
777"
DT,0.769183922046285,"¯
Mϵ = kϵ
X"
DT,0.7697929354445798,"i=1
log √ TrK
√"
DT,0.7704019488428745,"iϵ
= 1 2 ¯kϵ
X"
DT,0.7710109622411693,"i=1
log
¯kϵ"
DT,0.771619975639464,"i =
¯kϵ"
DT,0.7722289890377588,"2 (
Z 1"
DT,0.7728380024360536,"0
log 1"
DT,0.7734470158343484,"xdx + o(1)) =
¯kϵ"
DT,0.7740560292326432,2 (1 + o(1))
DT,0.7746650426309378,"where the o(1) term vanishes as ϵ ↘0. Furthermore, this allows us to check that as long as
778"
DT,0.7752740560292326,"log d = o

√"
DT,0.7758830694275274,"TrK
4ϵ log
√ TrK ϵ"
DT,0.7764920828258222,"
, the condition is satisfied
779 log √ TrK"
DT,0.7771010962241169,"ϵ
= o

¯kϵ
4 log d"
DT,0.7777101096224117,"
= o

¯
M 2
ϵ
¯kϵ log d 
. 780"
DT,0.7783191230207065,"Second we prove how to obtain the covering number of the convex hull of a function set F:
781"
DT,0.7789281364190013,"Theorem 18. Let F be a set of B-uniformly bounded functions, then for all ϵK = B2−K
782 p"
DT,0.7795371498172959,"log N2(ConvF, 2ϵK) ≤
√ 18 K
X"
DT,0.7801461632155907,"k=1
2K−k
q"
DT,0.7807551766138855,"log N2(F, B2−k)."
DT,0.7813641900121803,"Proof. Define ϵk = B2−k and the corresponding ϵk-coverings ˜Fk (w.r.t. some measure π). For any
783"
DT,0.781973203410475,"f, we write ˜fk[f] for the function ˜fk[f] ∈˜Fk that covers f. Then for any functions f in ConvF, we
784"
DT,0.7825822168087698,"have
785 f = m
X"
DT,0.7831912302070646,"i=1
βifi = m
X"
DT,0.7838002436053593,"i=1
βi

fi −˜fK[fi]

+ K
X k=1 m
X"
DT,0.784409257003654,"i=1
βi

˜fk[fi] −˜fk−1[fi]

+ ˜f0[fi]."
DT,0.7850182704019488,"We may assume that ˜f0[fi] = 0 since the zero function ϵ0-covers the whole F since ϵ0 = B.
786"
DT,0.7856272838002436,"We will now use the probabilistic method to show that the sums Pm
i=1 βi

˜fk[fi] −˜fk−1[fi]
 787"
DT,0.7862362971985384,"can be approximated by finite averages.
Consider the random functions ˜g(k)
1 , . . . , ˜g(k)
mk
788"
DT,0.7868453105968332,"sampled iid with P
h
˜g(k)
j
i
=

˜fk[fi] −˜fk−1[fi]

with probability βi.
We have E[˜g(k)
j
] =
789"
DT,0.7874543239951279,"Pm
i=1 βi

˜fk[fi] −˜fk−1[fi]

and
790 E  K
X k=1"
MK,0.7880633373934226,"1
mk mk
X"
MK,0.7886723507917174,"j=1
˜g(k)
j
− K
X k=1 m
X"
MK,0.7892813641900122,"i=1
βi

˜fk[fi] −˜fk−1[fi]
 p Lp(π) ≤ K
X k=1"
"MP
K",0.7898903775883069,"1
mp
k mk
X"
"MP
K",0.7904993909866017,"j=1
E
˜g(k)
j

p Lp(π) = K
X k=1"
MK,0.7911084043848965,"1
mk m
X"
MK,0.7917174177831913,"i=1
βi
 ˜fk[fi] −˜fk−1[fi]

p Lp(π) ≤ K
X k=1"
MK,0.792326431181486,"32ϵ2
k
mk
."
MK,0.7929354445797807,"Thus if we take mk =
1
ak ( 3ϵk"
MK,0.7935444579780755,"ϵK )2 with P ak = 1 we know that there must exist a choice of ˜g(k)
j
s such
791"
MK,0.7941534713763703,"that
792 K
X k=1"
MK,0.794762484774665,"1
mk mk
X"
MK,0.7953714981729598,"j=1
˜g(k)
j
− K
X k=1 m
X"
MK,0.7959805115712546,"i=1
βi

˜fk[fi] −˜fk−1[fi]


Lp(π) ≤ϵK."
MK,0.7965895249695494,"This implies that finite the set ˜C =
nPK
k=1
1
mk
Pmk
j=1 ˜g(k)
j
: ˜g(k)
j
∈˜Fk −˜Fk−1
o
is an 2ϵK covering
793"
MK,0.797198538367844,"of C = ConvF, since we know that for all f = Pm
i=1 βifi there are ˜g(k)
j
such that
794  K
X k=1"
MK,0.7978075517661388,"1
mk mk
X"
MK,0.7984165651644336,"j=1
˜g(k)
j
− m
X"
MK,0.7990255785627284,"i=1
βifi Lp(π) ≤  m
X"
MK,0.7996345919610232,"i=1
βi

fi −˜fK[fi]

Lp(π) + K
X k=1 "
MK,0.8002436053593179,"1
mk mk
X"
MK,0.8008526187576127,"j=1
˜g(k)
j
− m
X"
MK,0.8014616321559074,"i=1
βi

˜fk[fi] −˜fk−1[fi]


Lp(π)
≤2ϵK."
MK,0.8020706455542022,"Since
 ˜C
 = QK
k=1
 ˜Fk

mk  ˜Fk−1

mk, we have
795"
MK,0.8026796589524969,"log Np(C, 2ϵK) ≤ K
X k=1"
AK,0.8032886723507917,"1
ak
(3ϵk"
AK,0.8038976857490865,"ϵK
)2 (log Np(F, ϵk) + log Np(F, ϵk−1)) ≤18 K
X k=1"
AK,0.8045066991473813,"1
ak
22(K−k) log N2(F, ϵk)."
AK,0.805115712545676,"This is minimized for the choice
796"
AK,0.8057247259439708,"ak =
2(K−k)p"
AK,0.8063337393422655,"log N2(F, ϵk)
P 2(K−k)p"
AK,0.8069427527405603,"log N2(F, ϵk)
,"
AK,0.807551766138855,"which yields the bound
797 q"
AK,0.8081607795371498,"log Np(C, 2ϵK) ≤
√ 18 K
X"
AK,0.8087697929354446,"k=1
2K−kp"
AK,0.8093788063337394,"log N2(F, ϵk) 798"
AK,0.8099878197320342,"NeurIPS Paper Checklist
799"
CLAIMS,0.8105968331303288,"1. Claims
800"
CLAIMS,0.8112058465286236,"Question: Do the main claims made in the abstract and introduction accurately reflect the
801"
CLAIMS,0.8118148599269184,"paper’s contributions and scope?
802"
CLAIMS,0.8124238733252132,"Answer: [Yes]
803"
CLAIMS,0.8130328867235079,"Justification: The contribution section accurately describes our contributions, and all
804"
CLAIMS,0.8136419001218027,"theorems are proven in the appendix.
805"
CLAIMS,0.8142509135200975,"Guidelines:
806"
CLAIMS,0.8148599269183922,"• The answer NA means that the abstract and introduction do not include the claims
807"
CLAIMS,0.8154689403166869,"made in the paper.
808"
CLAIMS,0.8160779537149817,"• The abstract and/or introduction should clearly state the claims made, including the
809"
CLAIMS,0.8166869671132765,"contributions made in the paper and important assumptions and limitations. A No or
810"
CLAIMS,0.8172959805115713,"NA answer to this question will not be perceived well by the reviewers.
811"
CLAIMS,0.817904993909866,"• The claims made should match theoretical and experimental results, and reflect how
812"
CLAIMS,0.8185140073081608,"much the results can be expected to generalize to other settings.
813"
CLAIMS,0.8191230207064556,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
814"
CLAIMS,0.8197320341047503,"are not attained by the paper.
815"
LIMITATIONS,0.820341047503045,"2. Limitations
816"
LIMITATIONS,0.8209500609013398,"Question: Does the paper discuss the limitations of the work performed by the authors?
817"
LIMITATIONS,0.8215590742996346,"Answer: [Yes]
818"
LIMITATIONS,0.8221680876979294,"Justification: We discuss limitations of our Theorems after we state them.
819"
LIMITATIONS,0.8227771010962242,"Guidelines:
820"
LIMITATIONS,0.8233861144945189,"• The answer NA means that the paper has no limitation while the answer No means that
821"
LIMITATIONS,0.8239951278928136,"the paper has limitations, but those are not discussed in the paper.
822"
LIMITATIONS,0.8246041412911084,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
823"
LIMITATIONS,0.8252131546894031,"• The paper should point out any strong assumptions and how robust the results are to
824"
LIMITATIONS,0.8258221680876979,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
825"
LIMITATIONS,0.8264311814859927,"model well-specification, asymptotic approximations only holding locally). The authors
826"
LIMITATIONS,0.8270401948842875,"should reflect on how these assumptions might be violated in practice and what the
827"
LIMITATIONS,0.8276492082825823,"implications would be.
828"
LIMITATIONS,0.8282582216808769,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
829"
LIMITATIONS,0.8288672350791717,"only tested on a few datasets or with a few runs. In general, empirical results often
830"
LIMITATIONS,0.8294762484774665,"depend on implicit assumptions, which should be articulated.
831"
LIMITATIONS,0.8300852618757613,"• The authors should reflect on the factors that influence the performance of the approach.
832"
LIMITATIONS,0.830694275274056,"For example, a facial recognition algorithm may perform poorly when image resolution
833"
LIMITATIONS,0.8313032886723508,"is low or images are taken in low lighting. Or a speech-to-text system might not be
834"
LIMITATIONS,0.8319123020706456,"used reliably to provide closed captions for online lectures because it fails to handle
835"
LIMITATIONS,0.8325213154689404,"technical jargon.
836"
LIMITATIONS,0.833130328867235,"• The authors should discuss the computational efficiency of the proposed algorithms
837"
LIMITATIONS,0.8337393422655298,"and how they scale with dataset size.
838"
LIMITATIONS,0.8343483556638246,"• If applicable, the authors should discuss possible limitations of their approach to
839"
LIMITATIONS,0.8349573690621194,"address problems of privacy and fairness.
840"
LIMITATIONS,0.8355663824604141,"• While the authors might fear that complete honesty about limitations might be used
841"
LIMITATIONS,0.8361753958587089,"by reviewers as grounds for rejection, a worse outcome might be that reviewers
842"
LIMITATIONS,0.8367844092570037,"discover limitations that aren’t acknowledged in the paper. The authors should use
843"
LIMITATIONS,0.8373934226552984,"their best judgment and recognize that individual actions in favor of transparency play
844"
LIMITATIONS,0.8380024360535931,"an important role in developing norms that preserve the integrity of the community.
845"
LIMITATIONS,0.8386114494518879,"Reviewers will be specifically instructed to not penalize honesty concerning limitations.
846"
THEORY ASSUMPTIONS AND PROOFS,0.8392204628501827,"3. Theory Assumptions and Proofs
847"
THEORY ASSUMPTIONS AND PROOFS,0.8398294762484775,"Question: For each theoretical result, does the paper provide the full set of assumptions and
848"
THEORY ASSUMPTIONS AND PROOFS,0.8404384896467723,"a complete (and correct) proof?
849"
THEORY ASSUMPTIONS AND PROOFS,0.841047503045067,"Answer: [Yes]
850"
THEORY ASSUMPTIONS AND PROOFS,0.8416565164433617,"Justification: All assumptions are either stated in the Theorem statements, except for a few
851"
THEORY ASSUMPTIONS AND PROOFS,0.8422655298416565,"recurring assumptions that are stated in the setup section.
852"
THEORY ASSUMPTIONS AND PROOFS,0.8428745432399513,"Guidelines:
853"
THEORY ASSUMPTIONS AND PROOFS,0.843483556638246,"• The answer NA means that the paper does not include theoretical results.
854"
THEORY ASSUMPTIONS AND PROOFS,0.8440925700365408,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
855"
THEORY ASSUMPTIONS AND PROOFS,0.8447015834348356,"referenced.
856"
THEORY ASSUMPTIONS AND PROOFS,0.8453105968331304,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
857"
THEORY ASSUMPTIONS AND PROOFS,0.8459196102314251,"• The proofs can either appear in the main paper or the supplemental material, but if
858"
THEORY ASSUMPTIONS AND PROOFS,0.8465286236297198,"they appear in the supplemental material, the authors are encouraged to provide a short
859"
THEORY ASSUMPTIONS AND PROOFS,0.8471376370280146,"proof sketch to provide intuition.
860"
THEORY ASSUMPTIONS AND PROOFS,0.8477466504263094,"• Inversely, any informal proof provided in the core of the paper should be complemented
861"
THEORY ASSUMPTIONS AND PROOFS,0.8483556638246041,"by formal proofs provided in appendix or supplemental material.
862"
THEORY ASSUMPTIONS AND PROOFS,0.8489646772228989,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
863"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8495736906211937,"4. Experimental Result Reproducibility
864"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8501827040194885,"Question: Does the paper fully disclose all the information needed to reproduce the
865"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8507917174177831,"main experimental results of the paper to the extent that it affects the main claims and/or
866"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8514007308160779,"conclusions of the paper (regardless of whether the code and data are provided or not)?
867"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8520097442143727,"Answer: [Yes]
868"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8526187576126675,"Justification: The experimental setup is described in the Appendix.
869"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8532277710109623,"Guidelines:
870"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.853836784409257,"• The answer NA means that the paper does not include experiments.
871"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8544457978075518,"• If the paper includes experiments, a No answer to this question will not be perceived
872"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8550548112058465,"well by the reviewers: Making the paper reproducible is important, regardless of
873"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8556638246041413,"whether the code and data are provided or not.
874"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.856272838002436,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
875"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8568818514007308,"to make their results reproducible or verifiable.
876"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8574908647990256,"• Depending on the contribution, reproducibility can be accomplished in various ways.
877"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8580998781973204,"For example, if the contribution is a novel architecture, describing the architecture fully
878"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8587088915956151,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
879"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8593179049939099,"be necessary to either make it possible for others to replicate the model with the same
880"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8599269183922046,"dataset, or provide access to the model. In general. releasing code and data is often
881"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8605359317904994,"one good way to accomplish this, but reproducibility can also be provided via detailed
882"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8611449451887941,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
883"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8617539585870889,"of a large language model), releasing of a model checkpoint, or other means that are
884"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8623629719853837,"appropriate to the research performed.
885"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8629719853836785,"• While NeurIPS does not require releasing code, the conference does require all
886"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8635809987819733,"submissions to provide some reasonable avenue for reproducibility, which may depend
887"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8641900121802679,"on the nature of the contribution. For example
888"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8647990255785627,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
889"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8654080389768575,"to reproduce that algorithm.
890"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8660170523751523,"(b) If the contribution is primarily a new model architecture, the paper should describe
891"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.866626065773447,"the architecture clearly and fully.
892"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8672350791717418,"(c) If the contribution is a new model (e.g., a large language model), then there should
893"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8678440925700366,"either be a way to access this model for reproducing the results or a way to reproduce
894"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8684531059683313,"the model (e.g., with an open-source dataset or instructions for how to construct
895"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.869062119366626,"the dataset).
896"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8696711327649208,"(d) We recognize that reproducibility may be tricky in some cases, in which case
897"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8702801461632156,"authors are welcome to describe the particular way they provide for reproducibility.
898"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8708891595615104,"In the case of closed-source models, it may be that access to the model is limited in
899"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8714981729598051,"some way (e.g., to registered users), but it should be possible for other researchers
900"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8721071863580999,"to have some path to reproducing or verifying the results.
901"
OPEN ACCESS TO DATA AND CODE,0.8727161997563947,"5. Open access to data and code
902"
OPEN ACCESS TO DATA AND CODE,0.8733252131546894,"Question: Does the paper provide open access to the data and code, with sufficient
903"
OPEN ACCESS TO DATA AND CODE,0.8739342265529841,"instructions to faithfully reproduce the main experimental results, as described in
904"
OPEN ACCESS TO DATA AND CODE,0.8745432399512789,"supplemental material?
905"
OPEN ACCESS TO DATA AND CODE,0.8751522533495737,"Answer: [Yes]
906"
OPEN ACCESS TO DATA AND CODE,0.8757612667478685,"Justification: We use openly available data or synthetic data, with a description of how to
907"
OPEN ACCESS TO DATA AND CODE,0.8763702801461632,"build this synthetic data.
908"
OPEN ACCESS TO DATA AND CODE,0.876979293544458,"Guidelines:
909"
OPEN ACCESS TO DATA AND CODE,0.8775883069427527,"• The answer NA means that paper does not include experiments requiring code.
910"
OPEN ACCESS TO DATA AND CODE,0.8781973203410475,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
911"
OPEN ACCESS TO DATA AND CODE,0.8788063337393422,"public/guides/CodeSubmissionPolicy) for more details.
912"
OPEN ACCESS TO DATA AND CODE,0.879415347137637,"• While we encourage the release of code and data, we understand that this might not be
913"
OPEN ACCESS TO DATA AND CODE,0.8800243605359318,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
914"
OPEN ACCESS TO DATA AND CODE,0.8806333739342266,"including code, unless this is central to the contribution (e.g., for a new open-source
915"
OPEN ACCESS TO DATA AND CODE,0.8812423873325214,"benchmark).
916"
OPEN ACCESS TO DATA AND CODE,0.881851400730816,"• The instructions should contain the exact command and environment needed to run to
917"
OPEN ACCESS TO DATA AND CODE,0.8824604141291108,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
918"
OPEN ACCESS TO DATA AND CODE,0.8830694275274056,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
919"
OPEN ACCESS TO DATA AND CODE,0.8836784409257004,"• The authors should provide instructions on data access and preparation, including how
920"
OPEN ACCESS TO DATA AND CODE,0.8842874543239951,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
921"
OPEN ACCESS TO DATA AND CODE,0.8848964677222899,"• The authors should provide scripts to reproduce all experimental results for the new
922"
OPEN ACCESS TO DATA AND CODE,0.8855054811205847,"proposed method and baselines. If only a subset of experiments are reproducible, they
923"
OPEN ACCESS TO DATA AND CODE,0.8861144945188795,"should state which ones are omitted from the script and why.
924"
OPEN ACCESS TO DATA AND CODE,0.8867235079171741,"• At submission time, to preserve anonymity, the authors should release anonymized
925"
OPEN ACCESS TO DATA AND CODE,0.8873325213154689,"versions (if applicable).
926"
OPEN ACCESS TO DATA AND CODE,0.8879415347137637,"• Providing as much information as possible in supplemental material (appended to the
927"
OPEN ACCESS TO DATA AND CODE,0.8885505481120585,"paper) is recommended, but including URLs to data and code is permitted.
928"
OPEN ACCESS TO DATA AND CODE,0.8891595615103532,"6. Experimental Setting/Details
929"
OPEN ACCESS TO DATA AND CODE,0.889768574908648,"Question: Does the paper specify all the training and test details (e.g., data splits,
930"
OPEN ACCESS TO DATA AND CODE,0.8903775883069428,"hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand
931"
OPEN ACCESS TO DATA AND CODE,0.8909866017052375,"the results?
932"
OPEN ACCESS TO DATA AND CODE,0.8915956151035322,"Answer: [Yes]
933"
OPEN ACCESS TO DATA AND CODE,0.892204628501827,"Justification: In the experimental setup section in the Appendix.
934"
OPEN ACCESS TO DATA AND CODE,0.8928136419001218,"Guidelines:
935"
OPEN ACCESS TO DATA AND CODE,0.8934226552984166,"• The answer NA means that the paper does not include experiments.
936"
OPEN ACCESS TO DATA AND CODE,0.8940316686967114,"• The experimental setting should be presented in the core of the paper to a level of detail
937"
OPEN ACCESS TO DATA AND CODE,0.8946406820950061,"that is necessary to appreciate the results and make sense of them.
938"
OPEN ACCESS TO DATA AND CODE,0.8952496954933008,"• The full details can be provided either with the code, in appendix, or as supplemental
939"
OPEN ACCESS TO DATA AND CODE,0.8958587088915956,"material.
940"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8964677222898904,"7. Experiment Statistical Significance
941"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8970767356881851,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
942"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8976857490864799,"information about the statistical significance of the experiments?
943"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8982947624847747,"Answer: [No]
944"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8989037758830695,"Justification: The numerical experiments are mostly there as a visualization of the theoretical
945"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8995127892813642,"results, our main goal is therefore clarity, which would be hurt by putting error bars
946"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9001218026796589,"everywhere.
947"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9007308160779537,"Guidelines:
948"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9013398294762485,"• The answer NA means that the paper does not include experiments.
949"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9019488428745432,"• The authors should answer ""Yes"" if the results are accompanied by error bars,
950"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.902557856272838,"confidence intervals, or statistical significance tests, at least for the experiments that
951"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9031668696711328,"support the main claims of the paper.
952"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9037758830694276,"• The factors of variability that the error bars are capturing should be clearly stated (for
953"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9043848964677222,"example, train/test split, initialization, random drawing of some parameter, or overall
954"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.904993909866017,"run with given experimental conditions).
955"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9056029232643118,"• The method for calculating the error bars should be explained (closed form formula,
956"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9062119366626066,"call to a library function, bootstrap, etc.)
957"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9068209500609014,"• The assumptions made should be given (e.g., Normally distributed errors).
958"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9074299634591961,"• It should be clear whether the error bar is the standard deviation or the standard error
959"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9080389768574909,"of the mean.
960"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9086479902557856,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
961"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9092570036540804,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
962"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9098660170523751,"of Normality of errors is not verified.
963"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9104750304506699,"• For asymmetric distributions, the authors should be careful not to show in tables or
964"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9110840438489647,"figures symmetric error bars that would yield results that are out of range (e.g. negative
965"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9116930572472595,"error rates).
966"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9123020706455542,"• If error bars are reported in tables or plots, The authors should explain in the text how
967"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.912911084043849,"they were calculated and reference the corresponding figures or tables in the text.
968"
EXPERIMENTS COMPUTE RESOURCES,0.9135200974421437,"8. Experiments Compute Resources
969"
EXPERIMENTS COMPUTE RESOURCES,0.9141291108404385,"Question: For each experiment, does the paper provide sufficient information on the
970"
EXPERIMENTS COMPUTE RESOURCES,0.9147381242387332,"computer resources (type of compute workers, memory, time of execution) needed to
971"
EXPERIMENTS COMPUTE RESOURCES,0.915347137637028,"reproduce the experiments?
972"
EXPERIMENTS COMPUTE RESOURCES,0.9159561510353228,"Answer: [Yes]
973"
EXPERIMENTS COMPUTE RESOURCES,0.9165651644336176,"Justification: In the experimental setup section of the Appendix.
974"
EXPERIMENTS COMPUTE RESOURCES,0.9171741778319124,"Guidelines:
975"
EXPERIMENTS COMPUTE RESOURCES,0.917783191230207,"• The answer NA means that the paper does not include experiments.
976"
EXPERIMENTS COMPUTE RESOURCES,0.9183922046285018,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
977"
EXPERIMENTS COMPUTE RESOURCES,0.9190012180267966,"or cloud provider, including relevant memory and storage.
978"
EXPERIMENTS COMPUTE RESOURCES,0.9196102314250914,"• The paper should provide the amount of compute required for each of the individual
979"
EXPERIMENTS COMPUTE RESOURCES,0.9202192448233861,"experimental runs as well as estimate the total compute.
980"
EXPERIMENTS COMPUTE RESOURCES,0.9208282582216809,"• The paper should disclose whether the full research project required more compute
981"
EXPERIMENTS COMPUTE RESOURCES,0.9214372716199757,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
982"
EXPERIMENTS COMPUTE RESOURCES,0.9220462850182704,"didn’t make it into the paper).
983"
CODE OF ETHICS,0.9226552984165651,"9. Code Of Ethics
984"
CODE OF ETHICS,0.9232643118148599,"Question: Does the research conducted in the paper conform, in every respect, with the
985"
CODE OF ETHICS,0.9238733252131547,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
986"
CODE OF ETHICS,0.9244823386114495,"Answer: [Yes]
987"
CODE OF ETHICS,0.9250913520097442,"Justification: We have read the Code of Ethics and see no issue.
988"
CODE OF ETHICS,0.925700365408039,"Guidelines:
989"
CODE OF ETHICS,0.9263093788063338,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
990"
CODE OF ETHICS,0.9269183922046285,"• If the authors answer No, they should explain the special circumstances that require a
991"
CODE OF ETHICS,0.9275274056029232,"deviation from the Code of Ethics.
992"
CODE OF ETHICS,0.928136419001218,"• The authors should make sure to preserve anonymity (e.g., if there is a special
993"
CODE OF ETHICS,0.9287454323995128,"consideration due to laws or regulations in their jurisdiction).
994"
BROADER IMPACTS,0.9293544457978076,"10. Broader Impacts
995"
BROADER IMPACTS,0.9299634591961023,"Question: Does the paper discuss both potential positive societal impacts and negative
996"
BROADER IMPACTS,0.9305724725943971,"societal impacts of the work performed?
997"
BROADER IMPACTS,0.9311814859926918,"Answer: [NA]
998"
BROADER IMPACTS,0.9317904993909866,"Justification: The paper is theoretical in nature, so it has no direct societal impact that can
999"
BROADER IMPACTS,0.9323995127892813,"be meaningfully discussed.
1000"
BROADER IMPACTS,0.9330085261875761,"Guidelines:
1001"
BROADER IMPACTS,0.9336175395858709,"• The answer NA means that there is no societal impact of the work performed.
1002"
BROADER IMPACTS,0.9342265529841657,"• If the authors answer NA or No, they should explain why their work has no societal
1003"
BROADER IMPACTS,0.9348355663824605,"impact or why the paper does not address societal impact.
1004"
BROADER IMPACTS,0.9354445797807551,"• Examples of negative societal impacts include potential malicious or unintended uses
1005"
BROADER IMPACTS,0.9360535931790499,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1006"
BROADER IMPACTS,0.9366626065773447,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1007"
BROADER IMPACTS,0.9372716199756395,"groups), privacy considerations, and security considerations.
1008"
BROADER IMPACTS,0.9378806333739342,"• The conference expects that many papers will be foundational research and not tied
1009"
BROADER IMPACTS,0.938489646772229,"to particular applications, let alone deployments. However, if there is a direct path to
1010"
BROADER IMPACTS,0.9390986601705238,"any negative applications, the authors should point it out. For example, it is legitimate
1011"
BROADER IMPACTS,0.9397076735688186,"to point out that an improvement in the quality of generative models could be used to
1012"
BROADER IMPACTS,0.9403166869671132,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1013"
BROADER IMPACTS,0.940925700365408,"that a generic algorithm for optimizing neural networks could enable people to train
1014"
BROADER IMPACTS,0.9415347137637028,"models that generate Deepfakes faster.
1015"
BROADER IMPACTS,0.9421437271619976,"• The authors should consider possible harms that could arise when the technology is
1016"
BROADER IMPACTS,0.9427527405602923,"being used as intended and functioning correctly, harms that could arise when the
1017"
BROADER IMPACTS,0.9433617539585871,"technology is being used as intended but gives incorrect results, and harms following
1018"
BROADER IMPACTS,0.9439707673568819,"from (intentional or unintentional) misuse of the technology.
1019"
BROADER IMPACTS,0.9445797807551766,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1020"
BROADER IMPACTS,0.9451887941534713,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1021"
BROADER IMPACTS,0.9457978075517661,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1022"
BROADER IMPACTS,0.9464068209500609,"feedback over time, improving the efficiency and accessibility of ML).
1023"
SAFEGUARDS,0.9470158343483557,"11. Safeguards
1024"
SAFEGUARDS,0.9476248477466505,"Question: Does the paper describe safeguards that have been put in place for responsible
1025"
SAFEGUARDS,0.9482338611449452,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1026"
SAFEGUARDS,0.9488428745432399,"image generators, or scraped datasets)?
1027"
SAFEGUARDS,0.9494518879415347,"Answer: [NA]
1028"
SAFEGUARDS,0.9500609013398295,"Justification: Not relevant to our paper.
1029"
SAFEGUARDS,0.9506699147381242,"Guidelines:
1030"
SAFEGUARDS,0.951278928136419,"• The answer NA means that the paper poses no such risks.
1031"
SAFEGUARDS,0.9518879415347138,"• Released models that have a high risk for misuse or dual-use should be released with
1032"
SAFEGUARDS,0.9524969549330086,"necessary safeguards to allow for controlled use of the model, for example by requiring
1033"
SAFEGUARDS,0.9531059683313033,"that users adhere to usage guidelines or restrictions to access the model or implementing
1034"
SAFEGUARDS,0.953714981729598,"safety filters.
1035"
SAFEGUARDS,0.9543239951278928,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1036"
SAFEGUARDS,0.9549330085261876,"should describe how they avoided releasing unsafe images.
1037"
SAFEGUARDS,0.9555420219244823,"• We recognize that providing effective safeguards is challenging, and many papers do
1038"
SAFEGUARDS,0.9561510353227771,"not require this, but we encourage authors to take this into account and make a best
1039"
SAFEGUARDS,0.9567600487210719,"faith effort.
1040"
LICENSES FOR EXISTING ASSETS,0.9573690621193667,"12. Licenses for existing assets
1041"
LICENSES FOR EXISTING ASSETS,0.9579780755176613,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1042"
LICENSES FOR EXISTING ASSETS,0.9585870889159561,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1043"
LICENSES FOR EXISTING ASSETS,0.9591961023142509,"properly respected?
1044"
LICENSES FOR EXISTING ASSETS,0.9598051157125457,"Answer: [Yes]
1045"
LICENSES FOR EXISTING ASSETS,0.9604141291108405,"Justification: In the experimental setup section of the Appendix.
1046"
LICENSES FOR EXISTING ASSETS,0.9610231425091352,"Guidelines:
1047"
LICENSES FOR EXISTING ASSETS,0.96163215590743,"• The answer NA means that the paper does not use existing assets.
1048"
LICENSES FOR EXISTING ASSETS,0.9622411693057247,"• The authors should cite the original paper that produced the code package or dataset.
1049"
LICENSES FOR EXISTING ASSETS,0.9628501827040195,"• The authors should state which version of the asset is used and, if possible, include a
1050"
LICENSES FOR EXISTING ASSETS,0.9634591961023142,"URL.
1051"
LICENSES FOR EXISTING ASSETS,0.964068209500609,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1052"
LICENSES FOR EXISTING ASSETS,0.9646772228989038,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1053"
LICENSES FOR EXISTING ASSETS,0.9652862362971986,"service of that source should be provided.
1054"
LICENSES FOR EXISTING ASSETS,0.9658952496954933,"• If assets are released, the license, copyright information, and terms of use in the
1055"
LICENSES FOR EXISTING ASSETS,0.9665042630937881,"package should be provided. For popular datasets, paperswithcode.com/datasets
1056"
LICENSES FOR EXISTING ASSETS,0.9671132764920828,"has curated licenses for some datasets. Their licensing guide can help determine the
1057"
LICENSES FOR EXISTING ASSETS,0.9677222898903776,"license of a dataset.
1058"
LICENSES FOR EXISTING ASSETS,0.9683313032886723,"• For existing datasets that are re-packaged, both the original license and the license of
1059"
LICENSES FOR EXISTING ASSETS,0.9689403166869671,"the derived asset (if it has changed) should be provided.
1060"
LICENSES FOR EXISTING ASSETS,0.9695493300852619,"• If this information is not available online, the authors are encouraged to reach out to
1061"
LICENSES FOR EXISTING ASSETS,0.9701583434835567,"the asset’s creators.
1062"
NEW ASSETS,0.9707673568818515,"13. New Assets
1063"
NEW ASSETS,0.9713763702801461,"Question: Are new assets introduced in the paper well documented and is the documentation
1064"
NEW ASSETS,0.9719853836784409,"provided alongside the assets?
1065"
NEW ASSETS,0.9725943970767357,"Answer: [NA]
1066"
NEW ASSETS,0.9732034104750305,"Justification: We do not release any new assets.
1067"
NEW ASSETS,0.9738124238733252,"Guidelines:
1068"
NEW ASSETS,0.97442143727162,"• The answer NA means that the paper does not release new assets.
1069"
NEW ASSETS,0.9750304506699148,"• Researchers should communicate the details of the dataset/code/model as part of their
1070"
NEW ASSETS,0.9756394640682094,"submissions via structured templates. This includes details about training, license,
1071"
NEW ASSETS,0.9762484774665042,"limitations, etc.
1072"
NEW ASSETS,0.976857490864799,"• The paper should discuss whether and how consent was obtained from people whose
1073"
NEW ASSETS,0.9774665042630938,"asset is used.
1074"
NEW ASSETS,0.9780755176613886,"• At submission time, remember to anonymize your assets (if applicable). You can either
1075"
NEW ASSETS,0.9786845310596833,"create an anonymized URL or include an anonymized zip file.
1076"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792935444579781,"14. Crowdsourcing and Research with Human Subjects
1077"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799025578562729,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1078"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805115712545676,"include the full text of instructions given to participants and screenshots, if applicable, as
1079"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811205846528623,"well as details about compensation (if any)?
1080"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817295980511571,"Answer: [NA]
1081"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823386114494519,"Justification: Not relevant to this paper.
1082"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9829476248477467,"Guidelines:
1083"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835566382460414,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1084"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841656516443362,"human subjects.
1085"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847746650426309,"• Including this information in the supplemental material is fine, but if the main
1086"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853836784409257,"contribution of the paper involves human subjects, then as much detail as possible
1087"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859926918392204,"should be included in the main paper.
1088"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866017052375152,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1089"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98721071863581,"or other labor should be paid at least the minimum wage in the country of the data
1090"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878197320341048,"collector.
1091"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884287454323996,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1092"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890377588306942,"Subjects
1093"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989646772228989,"Question: Does the paper describe potential risks incurred by study participants, whether
1094"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902557856272838,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1095"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908647990255786,"approvals (or an equivalent approval/review based on the requirements of your country or
1096"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914738124238733,"institution) were obtained?
1097"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920828258221681,"Answer: [NA]
1098"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926918392204629,"Justification: Not relevant to this paper.
1099"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933008526187577,"Guidelines:
1100"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9939098660170523,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1101"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945188794153471,"human subjects.
1102"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951278928136419,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1103"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957369062119367,"may be required for any human subjects research. If you obtained IRB approval, you
1104"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963459196102314,"should clearly state this in the paper.
1105"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969549330085262,"• We recognize that the procedures for this may vary significantly between institutions
1106"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997563946406821,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1107"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981729598051157,"guidelines for their institution.
1108"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9987819732034104,"• For initial submissions, do not include any information that would break anonymity (if
1109"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9993909866017052,"applicable), such as the institution conducting the review.
1110"
