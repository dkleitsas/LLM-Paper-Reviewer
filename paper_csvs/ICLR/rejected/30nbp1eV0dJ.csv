Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00273224043715847,"We consider the lower bounds of differentially private ERM for general con-
vex functions. For approximate-DP, the well-known upper bound of DP-ERM"
ABSTRACT,0.00546448087431694,"is O(
√"
ABSTRACT,0.00819672131147541,p log(1/δ)
ABSTRACT,0.01092896174863388,"ϵn
), which is believed to be tight. However, current lower bounds
are off by some logarithmic terms, in particular Ω(
√p"
ABSTRACT,0.01366120218579235,ϵn ) for constrained case and
ABSTRACT,0.01639344262295082,"Ω(
√p
ϵn log p) for unconstrained case. We achieve tight Ω(
√"
ABSTRACT,0.01912568306010929,p log(1/δ)
ABSTRACT,0.02185792349726776,"ϵn
) lower bounds
for both cases by introducing a novel biased mean property for ﬁngerprinting
codes.
As for pure-DP, we utilize a novel ℓ2 loss function instead of linear functions
considered by previous papers, and achieve the ﬁrst (tight) Ω( p"
ABSTRACT,0.02459016393442623,"ϵn) lower bound.
We also introduce an auxiliary dimension to simplify the computation brought by
ℓ2 loss.
Our results close a gap in our understanding of DP-ERM by presenting the funda-
mental limits. Our techniques may be of independent interest, which help enrich
the tools so that it readily applies to problems that are not (easily) reducible from
one-way marginals."
INTRODUCTION,0.0273224043715847,"1
INTRODUCTION"
INTRODUCTION,0.030054644808743168,"Since the seminal work of Dwork et al. (2006), differential privacy (DP) has become the standard
and rigorous notion of privacy guarantee for machine learning algorithms, among which many fun-
damental ones are based on empirical risk minimization (ERM). Motivated by this, private ERM
becomes one of the most well-studied problem in the DP literature, e.g. Chaudhuri and Monteleoni
(2008); Rubinstein et al. (2009); Chaudhuri et al. (2011); Kifer et al. (2012); Song et al. (2013); Jain
and Thakurta (2014); Bassily et al. (2014); Talwar et al. (2015); Kasiviswanathan and Jin (2016);
Fukuchi et al. (2017); Wu et al. (2017); Zhang et al. (2017); Wang et al. (2017); Iyengar et al.
(2019); Bassily et al. (2020); Kulkarni et al. (2021); Asi et al. (2021); Bassily et al. (2021); Wang
et al. (2021)."
INTRODUCTION,0.03278688524590164,"Roughly speaking, in the ERM setting, we are given a convex function family deﬁned on a convex
set C ⊆Rp and a sample set D = {d1, · · · , dn} drawn i.i.d from some unknown distribution P with
the objective to minimize the loss function"
INTRODUCTION,0.03551912568306011,"L(θ; D) = 1 n n
X"
INTRODUCTION,0.03825136612021858,"i=1
ℓ(θ; di),"
INTRODUCTION,0.040983606557377046,"and the value L(θ; D)−minθ′∈C L(θ′; D) is called the excess empirical loss with respect to solution
θ, measuring how it compares with the best solution in C."
INTRODUCTION,0.04371584699453552,"Private ERM in the constrained case was studied ﬁrst and most of the previous literature belongs
to this case. More speciﬁcally, the constrained case considers convex loss functions deﬁned on a
bounded convex set C ⊊Rp. Assuming the functions are 1-Lipschitz over the convex set of diameter
1, the Ω(
√p"
INTRODUCTION,0.04644808743169399,"ϵn ) lower bound of private ERM is given by Bassily et al. (2014), even for (special and
simpler) generalized linear model (GLM)."
INTRODUCTION,0.04918032786885246,"However, there are still several aspects that existing works don’t cover. First, existing upper bounds
are off by at least a logarithmic term
p"
INTRODUCTION,0.05191256830601093,log(1/δ). For example in Wang et al. (2017); Bassily et al.
INTRODUCTION,0.0546448087431694,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05737704918032787,"(2019) they give upper bounds like O(
L||θ0−θ∗||2√"
INTRODUCTION,0.060109289617486336,"p log(1/δ)
ϵn
), which are believed to be tight. In
Bassily et al. (2014), they present a lower bound Ω(
√p"
INTRODUCTION,0.06284153005464481,"nϵ ) by reducing linear loss to one-way marginal
results in Hardt and Talwar (2010); Bun et al. (2018). In Steinke and Ullman (2015) they achieve the
tight lower bound for answering one-way marginals with respect to ℓ1 norm, but it does not imply
tight lower bounds for general loss functions by the methods in Bassily et al. (2014) directly."
INTRODUCTION,0.06557377049180328,"Another aspect is DP-ERM in the unconstrained case which was neglected before and gathered
people’s attention recently, see Jain and Thakurta (2014); Song et al. (2021). The unconstrained
case is interesting in that we can’t use linear loss any more which lies at the heart of the construction
in the constrained case. Moreover, previous algorithms in the constrained case suffer from the curse
of dimensionality when p is large. For example, when p = Ω(n2), the lower bound is Ω(1) and any
private algorithm can not get meaningful bounds on the excess empirical loss. Song et al. (2021)
proves an dimension-independent O(
√"
INTRODUCTION,0.06830601092896176,"rank
ϵn
) upper bound in the unconstrained setting for the special
case of GLMs, where rank denotes the rank of the feature matrix of GLM. However, it’s unknown
if similar results can be achieved for general loss functions. As for the lower bound, Asi et al.
(2021) give Ω(
√p
nϵ log p) lower bound by considering ℓ1 loss as the objective functions and reducing
the results also from one-way marginals."
OUR CONTRIBUTIONS,0.07103825136612021,"1.1
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.07377049180327869,"In this paper, we ﬁll up the two gaps together by proving an Ω(min(1,
√"
OUR CONTRIBUTIONS,0.07650273224043716,p log(1/δ)
OUR CONTRIBUTIONS,0.07923497267759563,"nϵ
)) tight lower
bound for the excess risk of unconstrained 1-Lipschitz convex loss functions for approximate dif-
ferentially private algorithms. This bound is automatically applicable in the constrained case, which
improves previous results and achieve a tight lower bound for both constrained and unconstrained
case. We summarize our main results as follows:"
OUR CONTRIBUTIONS,0.08196721311475409,"• We prove an Ω(min(1,
√"
OUR CONTRIBUTIONS,0.08469945355191257,p log(1/δ)
OUR CONTRIBUTIONS,0.08743169398907104,"nϵ
)) tight lower bound for the excess risk of unconstrained
1-Lipschitz convex loss functions for approximate differentially private algorithm. This
bound improves Asi et al. (2021) by a log(p)
p"
OUR CONTRIBUTIONS,0.09016393442622951,"log(1/δ) factor in the unconstrained case
and matches the upper bound in Kairouz et al. (2020).
• We also prove an Ω(min(1, p"
OUR CONTRIBUTIONS,0.09289617486338798,"nϵ)) lower bound for the excess risk of unconstrained 1-
Lipschitz convex loss functions for any pure differential privacy algorithm."
OUR CONTRIBUTIONS,0.09562841530054644,"Note that our main results for unconstrained case can be extended to constrained case directly, thus
our lower bound for approximate private algorithm is
p"
OUR CONTRIBUTIONS,0.09836065573770492,"log(1/δ) multiplicative better than the well-
known bound in Bassily et al. (2014) with the help of group privacy technique in Steinke and Ullman
(2015)."
OUR CONTRIBUTIONS,0.10109289617486339,"A key contribution of this paper is novel tools for private lower bound techniques.
For most
problems, accuracy lower bound in the private setting is established via reduction from one-way
marginals. Hence the tools for lower bounds is quite limited. We contribute to reﬁnement of such
tools – in particular, we propose modiﬁcations such as the additional ”biased means” property to
ﬁngerprinting codes, which is the key lower bound technique. Such modiﬁcations help enrich the
tools so that it readily applies to problems which are not (easily) reducible from one-way marginals."
OUR TECHNIQUES,0.10382513661202186,"1.2
OUR TECHNIQUES"
OUR TECHNIQUES,0.10655737704918032,"In general, the direct technical challenge of the unconstrained case lies in the choice of loss function
and the difﬁculties caused by the new loss function. The loss function is required to be convex and
Lipschitz-continuous at the same time. In the constrained case, the linear loss function is obviously
a good choice for constructing lower bounds, because it is easy to analyze and easily reducible from
one-way marginals. However, in the unconstrained case any non-trivial linear loss function can
take ‘−∞’ value thus not applicable anymore. We further observe that convexity plus Lipschitz-
continuity means ‘asymptotically linear’: the sub-gradient along any direction must converge. This
observation guides us in choosing new loss functions for unconstrained DP-ERM (which can be
extended to constrained case directly). We brieﬂy introduce the new problems caused by the new
loss functions and our method to overcome them for approximate-DP and pure-DP separately."
OUR TECHNIQUES,0.1092896174863388,Under review as a conference paper at ICLR 2022
APPROXIMATE-DP,0.11202185792349727,"1.2.1
APPROXIMATE-DP"
APPROXIMATE-DP,0.11475409836065574,"The construction of our lower bound for approximate-DP is based on the Fingerprinting Codes,
which was ﬁrst studied by Wagner (1983) and developed by Boneh and Shaw (1998); Tardos (2008)."
APPROXIMATE-DP,0.11748633879781421,"From a technical perspective, we change the previously used linear loss and use an ℓ1 norm function
instead where ℓ(θ; d) = ∥θ −d∥1. ℓ1 loss has been used in a concurrent work Asi et al. (2021)
which proves an Ω(
√p
ϵn log p) lower bound of approximate DP in the constrained case by reducing the
results from one-way marginals, and can be extended to unconstrained case directly. We improve
this bound by logarithmic terms and achieve optimality by utilizing the group privacy technique
from Steinke and Ullman (2015). We observe a novel biased mean property in the ﬁngerprinting
code to successfully combine and adjust these techniques to ﬁt the ℓ1 loss."
APPROXIMATE-DP,0.12021857923497267,"We brieﬂy describe the proof based on group privacy technique ﬁrst. In the group privacy we need
to copy some hard instances of data-set Dk of size nk := ⌊n/k⌋according to the construction of
ﬁngerprinting codes by k times, and append n −knk data points to get a ﬁnal data-set D of size n.
Fix any (ϵ, δ)-differentially private algorithm A for D, if we remove one element i∗from Dk, we
can get Dk
−i∗and D−i∗where D−i∗and D can have at most k elements different. Running A on D
and D−i∗respectively, we get an (kϵ, δ′)-differential privacy algorithm for Dk and Dk
−i∗. Setting k
appropriately, if A can lead to small error on the DP-ERM, it can be an adversary which contradicts
the properties of the ﬁngerprinting codes. Intuitively, the differential privacy means it is hard to ﬁnd
the removed element i∗, but ﬁngerprinting codes suggest the removed element is traceable as long
as Dk satisﬁes the required properties and A leads to small excess empirical loss with respect to
L(θ; Dk)."
APPROXIMATE-DP,0.12295081967213115,"The direct use of the biased mean property is in appending the n −knk points. As nearly all of
previous lower bounds in DP convex optimization are based on the results from one-way marginals,
we try to demonstrate the proof in the language of one-way marginals. Because for linear functions,
large one-way marginal errors lead to large excess empirical loss directly, which means the lower
bound of private one-way marginals can apply to DP-ERM. But it is obvious that without additional
assumption, large one-way marginal errors can not mean large excess empirical loss of the ℓ1 loss
functions anymore. Consider the toy case when p = 1 and Dk = {di}⌊n/k⌋
i=1
where di ∈{0, 1}.
Denote the mean of these ⌊n/k⌋by Dk. Similarly let D be the mean values of D constructed from
Dk by method above. For example, if Dk = 1/2 and we only append points 1/2, then D = 1/2 and
whatever the one-way marginals are, the excess empirical loss (of the n ℓ1 loss functions) can be 0, as
L(θ; D) = 1"
APPROXIMATE-DP,0.12568306010928962,"n
Pn
i=1 ∥θ−di∥1 is a constant function over [0, 1]. So we need the mean Dk to be biased.
More speciﬁcally, we need |Dk −1/2| should be larger than some value depending on k, then we
can append n−knk dummy points safely. For general p, we need the unbiased mean property holds
for a large fraction of coordinates. For any single dimension, the biased mean property serves to
ensure the prediction of the column (dimension) is unchanged during the group privacy mechanism,
in which some number of dummy points are appended that may potentially change the prediction of
unbiased column. This novel property sets more stringent conditions. Fortunately, we observe that
the previous construction of ﬁngerprinting codes in Bun et al. (2018) satisﬁes it."
PURE-DP,0.1284153005464481,"1.2.2
PURE-DP"
PURE-DP,0.13114754098360656,"Although the square loss seems tempting in the constrained case, which intuitively reduces the
unconstrained case to constrained because the loss value grows fast outside a bounded region and
also makes computation simple. It’s unfortunately non-Lipschitz in the unconstrained case thus not
applicable directly."
PURE-DP,0.13387978142076504,"We use the novel ℓ2-norm loss as a natural substitute, which is both convex and Lipschitz-
continuous. Unlike the constrained case, the ℓ2-norm loss brings the drawback that the minimizer
of the ERM problem no longer has a closed form solution or any nice property for computation.
Roughly speaking, in the analysis of Bassily et al. (2014) they have an ’adding dummy points’ pro-
cedure which will perturb the minimizer. Linear loss has this nice property that after adding these
dummy points the minimizer can only move along its direction, while for ℓ2 loss the minimizer
might be intractable. To overcome this problem, we deﬁne the Fermat point and introduce an aux-
iliary dimension to simplify the messy calculation brought by ℓ2-norm loss in our analysis. The"
PURE-DP,0.1366120218579235,Under review as a conference paper at ICLR 2022
PURE-DP,0.13934426229508196,"dummy points have support only in this auxiliary dimension and guarantee that the perturbation
of the minimizer is still along its own direction, reducing computation in any high dimension to a
two-dimension subspace spanned by the minimizer and the auxiliary dimension."
CONSTRAINED AND UNCONSTRAINED,0.14207650273224043,"1.3
CONSTRAINED AND UNCONSTRAINED"
CONSTRAINED AND UNCONSTRAINED,0.1448087431693989,"In this subsection, we brieﬂy discuss the relationships and differences between constrained case and
unconstrained case, and compare our bounds with previous bounds."
CONSTRAINED AND UNCONSTRAINED,0.14754098360655737,"Previous studies on DP-ERM mostly focus on the constrained setting, and the unconstrained case
recently attract people’s interest because Jain and Thakurta (2014); Song et al. (2021) found that an
O(
√"
CONSTRAINED AND UNCONSTRAINED,0.15027322404371585,"rank
ϵn
) upper bound can be achieved for minimizing the excess risk of GLMs, which evades the
curse of dimensionality."
CONSTRAINED AND UNCONSTRAINED,0.15300546448087432,"It has been known that the unconstrained condition is necessary for dimension independence, as
pointed out by Bassily et al. (2014) in which they prove an Ω(
√p"
CONSTRAINED AND UNCONSTRAINED,0.1557377049180328,"nϵ ) lower bound even for minimizing
constrained GLMs for the case when “rank ≤n ≪p”."
CONSTRAINED AND UNCONSTRAINED,0.15846994535519127,"We are interested in the necessity of the unconstrained condition to get rank-dependent bound. The
unconstrained GLM can be viewed as a rank-dimensional problem, as the noise added in the null
space of the feature matrix will not affect the excess empirical loss. However, this does not hold
in the constrained case. Take the dimension-independent algorithm in Song et al. (2021) which is
based on SGD as an example. The pitfall for the dimension-independent algorithm lies in projection
if SGD is modiﬁed to projected-SGD for constrained case, that running SGD in the constrained
setting requires projection which might “increase rank”. We can see there is some fundamental
difference between constrained and unconstrained case, and analyzing unconstrained case is also an
interesting and important direction."
CONSTRAINED AND UNCONSTRAINED,0.16120218579234974,"Classic methods, like Bassily et al. (2014) usually connect linear loss to one-way marginals Bun
et al. (2018), and then use lower bounds for one-way marginals to imply lower bounds for linear
loss. As Bassily et al. (2014) are using results from Hardt and Talwar (2010); Bun et al. (2018) in"
CONSTRAINED AND UNCONSTRAINED,0.16393442622950818,"one-way marginals and Steinke and Ullman (2015) achieves tight Ω(
√"
CONSTRAINED AND UNCONSTRAINED,0.16666666666666666,p log(1/δ)
CONSTRAINED AND UNCONSTRAINED,0.16939890710382513,"αϵ
) bound by using
the novel group privacy technique, one may ask whether combining Steinke and Ullman (2015) and
Bassily et al. (2014) can achieve the tight lower bound in the constrained case trivially. Because
Steinke and Ullman (2015) considers ℓ1 distance but Bassily et al. (2014) considers ℓ2 distance,
there is a √p gap between them and one can’t directly combine them. Though with some effort,
one may get tighter bounds in the constrained case by modifying the results in Steinke and Ullman
(2015) from ℓ1 norm to ℓ2 norm, then applying the analysis in Bassily et al. (2014)."
CONSTRAINED AND UNCONSTRAINED,0.1721311475409836,"As shown in Asi et al. (2021), proving a nearly tight lower bound in the unconstrained case is direct
by utilizing one-way marginals and choosing the right objective functions, but getting rid off those
extra logarithmic terms in the unconstrained case is nontrivial as the one-way marginals can not
work directly in group privacy. To the best of our knowledge, our result is the ﬁrst time that achieves
this improved tight lower bound for general loss function class in both cases. See Table 1.4 for
detailed comparisons between previous bounds and ours."
RELATED WORK,0.17486338797814208,"1.4
RELATED WORK"
RELATED WORK,0.17759562841530055,"The existing lower bounds of excess empirical loss, i.e. the constrained case in Bassily et al. (2014)
and the unconstrained case in Song et al. (2021), are all using GLM functions. The objective func-
tion used in Bassily et al. (2014) is ℓ(θ; d) = ⟨θ, d⟩which can’t be applied in the unconstrained
case, otherwise the loss value would be inﬁnite. Considering this limitation, Song et al. (2021)
adopts ℓ(θ; d) = |⟨θ, x⟩−y|. They transfer the problem of minimizing GLM to estimating one-way
marginals, and then get the lower bound by properties in the deﬁnition of the Fingerprinting Codes."
RELATED WORK,0.18032786885245902,"As mentioned before, our lower bound are based on ℓ1 norms, thus we can not transfer to one-
way marginals directly. Merely using the properties in the deﬁnition of Fingerprinting Codes is not
enough for a good lower bound. Instead, we need to make full use of the concrete structure of the
codes."
RELATED WORK,0.1830601092896175,Under review as a conference paper at ICLR 2022
RELATED WORK,0.18579234972677597,"Article
Constrained?
Loss Function
Pure DP
Approximate DP
Bassily et al. (2014)
constrained
GLM
Ω( p"
RELATED WORK,0.1885245901639344,"nϵ)
Ω(
√p"
RELATED WORK,0.1912568306010929,"nϵ )
Song et al. (2021)
unconstrained
GLM
N/A
Ω(
√"
RELATED WORK,0.19398907103825136,"rank
nϵ
)
Asi et al. (2021)
both
general
N/A
Ω(
√p
nϵ log p)"
RELATED WORK,0.19672131147540983,"Ours
both
general
Ω( p"
RELATED WORK,0.1994535519125683,"nϵ)
Ω(
√"
RELATED WORK,0.20218579234972678,"p log(1/δ) nϵ
)"
RELATED WORK,0.20491803278688525,"Table 1: Comparison on lower bounds for private convex ERM. Our lower bounds can be extended
to constrained case easily. The lower bound of Song et al. (2021) is weaker than ours in the important
p ≫n setting."
RELATED WORK,0.20765027322404372,"As for the upper bounds, the private ERM Wang et al. (2017) and private Stochastic Convex Op-
timization (SCO) Feldman et al. (2020) for convex and smooth functions are extensively studied,
where the objective is to minimize the function Ed∼P[ℓ(θ; d)] in the SCO and people only need
(nearly) linear gradient queries to get optimal excess loss. But for convex functions without any
smoothness assumption, the current best algorithms Kulkarni et al. (2021); Asi et al. (2021) will
need more queries (n1.375 in the worst case). Besides, most of the previous works are consider-
ing problems in ℓ2 norm, and there are some recent results Bassily et al. (2021); Asi et al. (2021)
studying the general ℓp norm."
ROADMAP,0.2103825136612022,"1.5
ROADMAP"
ROADMAP,0.21311475409836064,In section 2 we introduce background knowledge needed in the rest of the paper. In section 3
ROADMAP,0.21584699453551912,"we prove the main result of this paper, an Ω(min(1,
√"
ROADMAP,0.2185792349726776,p log(1/δ)
ROADMAP,0.22131147540983606,"nϵ
)) lower bound for approximate DP-
ERM in the unconstrained case. In section 4 we discuss an Ω(min(1, p"
ROADMAP,0.22404371584699453,"nϵ)) lower bound for the excess
risk of pure DP algorithms for minimizing any unconstrained 1-Lipschitz convex loss function.
Section 5 concludes this paper. All missing (technical) proofs can be found in the appendix."
PRELIMINARY,0.226775956284153,"2
PRELIMINARY"
PRELIMINARY,0.22950819672131148,"We consider minimizing the excess risk of unconstrained Lipschiz convex function with DP algo-
rithms in this paper, where we let n denote the sample size and p be the dimension of a sample.
In this section, we will introduce main background knowledge required in the rest of the paper.
Additional background knowledge such as the deﬁnition of GLM can be found in appendix.
Deﬁnition 2.1 (Differential privacy). A randomized mechanism M is (ϵ, δ)-differentially private if
for any event O ∈Range(M) and for any neighboring databases D and D′ that differ in a single
data element, one has"
PRELIMINARY,0.23224043715846995,Pr[M(D) ∈O] ≤exp(ϵ) Pr[M(D′) ∈O] + δ.
PRELIMINARY,0.23497267759562843,"When δ > 0, we refer to the above condition as approximate differential privacy. The special case
when δ = 0 is called pure differential privacy.
Deﬁnition 2.2 (Empirical Risk Minimization). Given a family of convex loss functions
{ℓ(θ, d)}d∈D of θ over K ⊆Rp and a set of samples D = {d1, · · · , dn} over the universe D,
the objective of Empirical Risk Minimization (ERM) is to minimize"
PRELIMINARY,0.23770491803278687,"L(θ; D) = 1 n n
X"
PRELIMINARY,0.24043715846994534,"i=1
ℓ(θ; di)."
PRELIMINARY,0.24316939890710382,The excess empirical loss with respect to a solution θ is deﬁned by
PRELIMINARY,0.2459016393442623,L(θ; D) −L(θ∗; D)
PRELIMINARY,0.24863387978142076,"where θ∗∈arg minθ∈K L(θ; D), measuring the performance of the solution θ compared with the
best solution in K.
Deﬁnition 2.3 (G-Lipschitz Continuity). A function f : Rp →R is G-Lipschitz continuous with
respect to ℓ2 norm if the following holds for all θ, θ′ ∈Rp:"
PRELIMINARY,0.25136612021857924,"|f(θ) −f(θ′)| ≤G∥θ −θ′∥2
(1)"
PRELIMINARY,0.2540983606557377,Under review as a conference paper at ICLR 2022
PRELIMINARY,0.2568306010928962,"The Chernoff Bound will serve to prove the ﬁngerprinting code constructed in Bun et al. (2018)
satisﬁes our modiﬁed deﬁnition of ﬁngerprinting code as well.
Proposition 2.4 (The Chernoff Bound). Let X = Pn
i=1 Xi where Xi = 1 with probability pi and
Xi = 0 with probability 1−pi. Assume all Xi are independent random variables. Let u = Pn
i=1 pi.
Then
P(|X −u| ≥δu) ≤2exp(−uδ2/2).
(2)"
APPROXIMATE DP,0.25956284153005466,"3
APPROXIMATE DP"
APPROXIMATE DP,0.26229508196721313,"In this section, we consider the lower bound for approximate differential privacy where 2−O(n) <
δ < o(1/n). Such assumption on δ is common in literature, for example in Steinke and Ullman
(2015). We brieﬂy introduce the (classic) ﬁngerprinting codes ﬁrst:"
FINGERPRINTING CODES,0.2650273224043716,"3.1
FINGERPRINTING CODES"
FINGERPRINTING CODES,0.2677595628415301,"Deﬁnition 3.1 (Fingerprinting codes). We are given n, p ∈N, ξ ∈(0, 1]. A pair of (random)
algorithms (Gen, Trace) is called an (n, p)-ﬁngerprinting code with security ξ ∈(0, 1] if Gen
outputs a code-book C ∈{0, 1}n×p and for any (possibly randomized) adversary AF P and any
subset S ⊆[n], if we set c ←R AF P (CS), then"
FINGERPRINTING CODES,0.27049180327868855,"• Pr[c ∈F(CS) V Trace(C, c) =⊥] ≤ξ"
FINGERPRINTING CODES,0.273224043715847,"• Pr [Trace (C, c) ∈[n]\S] ≤ξ"
FINGERPRINTING CODES,0.27595628415300544,"where F (CS) =

c ∈{0, 1}d | ∀j ∈[d], ∃i ∈S, cj = cij
	
, and the probability is taken over the
coins of Gen, Trace and AF P ."
FINGERPRINTING CODES,0.2786885245901639,"There is a very good motivation behind the ﬁngerprinting codes. For example, a software distributor
adds a ﬁngerprint to each copy of her software to protect the IP. A coalition of malicious users
can compare their copies and ﬁnd the digits that differ which belong to the ﬁngerprint. For other
locations they can’t decide and won’t change them, which is called the marking condition. This is
the reason that we requires c ∈F(CS)."
FINGERPRINTING CODES,0.2814207650273224,"The two properties of ﬁngerprinting codes demonstrate that one can identify at least one malicious
user among all with high probability. Bun et al. (2018) extends the deﬁnition that the codes can tol-
erate a small fraction of errors in the marking condition. We further modify this deﬁnition, requiring
the codes to have biased means, see below."
OUR RESULT,0.28415300546448086,"3.2
OUR RESULT"
OUR RESULT,0.28688524590163933,"We modify the deﬁnition of ﬁngerprinting code instead for our analysis.
Deﬁnition
3.2
(Error
Robust
Biased
Mean
Fingerprinting
Codes).
Given
n, p
∈
N, ξ, β, α1, α2, α3 ∈(0, 1].
We say a pair of (random) algorithms (Gen, Trace) is an (n, p)-
ﬁngerprinting code with security ξ and (α1, α2, α3)-biased mean, robust to a β fraction of errors if
Gen outputs a code-book C ∈{0, 1}n×p and for any (possibly randomized) adversary AF P and
any coalition S ⊆[n], if we set c ←R AF P (CS), then"
OUR RESULT,0.2896174863387978,"• Pr[c ∈Fβ(CS) V Trace(C, c) =⊥] ≤ξ"
OUR RESULT,0.2923497267759563,"• Pr [Trace (C, c) ∈[n]\S] ≤ξ"
OUR RESULT,0.29508196721311475,• Pr[Gα1(C) ≥(1 −α2)] ≤α3
OUR RESULT,0.2978142076502732,"where Fβ (CS)
=

c ∈{0, 1}p | Prj←R[p][∃i ∈S, cj = cij] ≥1 −β
	
, Gα(CS)
=
|{j
:
| P"
OUR RESULT,0.3005464480874317,"i∈S cij/|S| −1/2| ≤α}| is the number of slightly biased columns in CS and the probability is
taken over the coins of Gen, Trace and AF P ."
OUR RESULT,0.30327868852459017,"We use the ﬁngerprinting code in Bun et al. (2018) for the construction of our lower bound, see
Algorithm 1 in the appendix. We utilize an ℓ1 loss and use the ﬁngerprinting code in Bun et al."
OUR RESULT,0.30601092896174864,Under review as a conference paper at ICLR 2022
OUR RESULT,0.3087431693989071,"(2018) as our ’hard case’. To proceed, we ﬁrst introduce a few lemmas which would be of use later.
Similar to Bun et al. (2018), we have the following standard lemma which allows us to reduce any
ϵ < 1 to ϵ = 1 case without loss of generality, using the well-known ’secrecy of the sample’ lemma
from Kasiviswanathan et al. (2011).
Lemma 3.1. A condition Q has sample complexity n∗for algorithms with (1, o(1/n))-differential
privacy (n∗is the smallest sample size that there exists an (1, o(1/n))-differentially private algo-
rithm A which satisﬁes Q), if and only if it also has sample complexity Θ(n∗/ϵ) for algorithms with
(ϵ, o(1/n))-differential privacy."
OUR RESULT,0.3114754098360656,"Notice that Lemma 3.1 discusses the sample complexity of the algorithm, therefore is independent of
the (α1, α2, α3)-biased mean appeared in the above deﬁnition which only concerns the construction
of the ﬁngerprinting code. The following lemma veriﬁes that the ﬁngerprinting code Algorithm 1
indeed has biased mean as in deﬁnition 3.2. The proof is straightforward by using the Chernoff
bound multiple times.
Lemma 3.2. Algorithm 1 (the ﬁngerprinting code) has (1/100, 999/1000, exp(−Ω(p))-biased
mean."
OUR RESULT,0.31420765027322406,"Directly combining Lemma 3.2 and Theorem 3.4 from Bun et al. (2018), we have the following
lemma, which states that for the ﬁngerprinting code Algorithm 1 which we will use in proving our
main theorem to satisfy the error robust biased mean property in deﬁnition 3.2, one needs roughly
˜Ω(√p) samples.
Lemma 3.3. For every p ∈N and ξ ∈(0, 1], there exists an (n, p)-ﬁngerprinting code (Algorithm
1) with security ξ and (1/100, 999/1000, exp(−Ω(p))-biased mean, robust to a 1/75 fraction of
error for"
OUR RESULT,0.31693989071038253,"n = n(p, ξ) = ˜Ω(
p"
OUR RESULT,0.319672131147541,p/ log(1/ξ)).
OUR RESULT,0.3224043715846995,"We are ready to prove the main result of this section by using Lemma 3.3 to reach a contradiction.
Consider the following ℓ1 norm loss function. Deﬁne"
OUR RESULT,0.3251366120218579,"ℓ(θ; d) = ||θ −d||1, θ, d ∈Rp
(3)"
OUR RESULT,0.32786885245901637,"For any data-set D = {d1, ..., dn}, we deﬁne L(θ; D) = 1"
OUR RESULT,0.33060109289617484,"n
Pn
i=1 ℓ(θ; di).
Theorem 3.4 (Lower bound for (ϵ, δ)-differentially private algorithms). Let n, p be large enough
and 1 ≥ϵ > 0, 2−O(n) < δ < o(1/n). For every (ϵ, δ)-differentially private algorithm with output
θpriv ∈Rp, there is a data-set D = {d1, ..., dn} ⊂{0, 1}p ∪{ 1"
OUR RESULT,0.3333333333333333,2}p such that
OUR RESULT,0.3360655737704918,"E[L(θpriv; D) −L(θ⋆; D)] = Ω(min(1, p"
OUR RESULT,0.33879781420765026,p log(1/δ)
OUR RESULT,0.34153005464480873,"nϵ
)GC)
(4)"
OUR RESULT,0.3442622950819672,"where ℓis G-Lipschitz, θ⋆is a minimizer of L(θ; D), and C is the diameter of the set
{arg minθ L(θ; D)|D ⊂{0, 1}n×p}, which contains all possible true minimizers."
OUR RESULT,0.3469945355191257,"Due to the space limit, we leave the proof of the Theorem 3.4 in the appendix."
OUR RESULT,0.34972677595628415,"The dependence on the diameter C makes sense as one can minimize a substitute loss function
ℓ′(x) = ℓ(ax) where a ∈(0, 1) is a constant instead, which decreases Lipschitz constant G but
increases the diameter C. Note also that C > 0 whenever all possible D don’t share the same
minimizer of L, which is often the case. This bound improves a log factor over Bassily et al.
(2014) by combining the the group privacy technique in Steinke and Ullman (2015) and our modiﬁed
deﬁnition of ﬁngerprinting code."
OUR RESULT,0.3524590163934426,"We leave several remarks discussing slight generalizations of Theorem 3.4.
Remark 3.5. Our lower bound can be directly extended to the constrained setting, by setting
the constrained domain to be [0, 1]n×p which contains the convex hull of all possible minimizers
{arg minθ L(θ; D)|D ⊂{0, 1}n×p}."
OUR RESULT,0.3551912568306011,"Remark 3.6. Similarly, we can derive an Ω(min(1,
√"
OUR RESULT,0.35792349726775957,rank log(1/δ)
OUR RESULT,0.36065573770491804,"nϵ
) lower bound when we addition-
ally assume the rank of gradient subspace. The analysis remains the same except we ﬁrst apply
orthogonal transformation then set the complement of the gradient subspace to be all 0’s in D."
OUR RESULT,0.3633879781420765,Under review as a conference paper at ICLR 2022
OUR RESULT,0.366120218579235,"Remark 3.7. The third property of deﬁnition 3.2 serves the group privacy analysis to further im-
prove a log(1/δ) term over Bassily et al. (2014). One can simplify the proof by setting k = 1
and borrow the lower bound for 1-way marginals from Bun et al. (2018), at the cost of losing this
log(1/δ) term. See appendix for details."
PURE DP,0.36885245901639346,"4
PURE DP"
PURE DP,0.37158469945355194,"In this section, we give a lower bound for ϵ-(pure) differentially private algorithms for minimizing
unconstrained convex Lipschitz loss function L(θ; D). In the construction of lower bounds for
constrained DP-ERM (Bassily et al. (2014)), they chose linear function ℓ(θ; d) = ⟨θ, d⟩as their
objective function which isn’t applicable in the unconstrained setting because it could decrease to
negative inﬁnity. Instead, we use a novel ℓ2 norm loss function to over come this problem:"
PURE DP,0.3743169398907104,"ℓ(θ; d) = ||θ −d||2, θ, d ∈Rp
(5)"
PURE DP,0.3770491803278688,"For any dataset D = {d1, ..., dn}, we deﬁne L(θ; D) = 1"
PURE DP,0.3797814207650273,"n
Pn
i=1 ℓ(θ; di). Clearly, both ℓand L are
convex and 1-Lipschitz. The structure of the proof is similar to that in Bassily et al. (2014), while
technical details are quite different as we need to handle a non-linear objective function. Different
from the simple average of points in Bassily et al. (2014), we need to consider the Fermat point
instead, which is the minimizer of the ℓ2 norm loss function."
FERMAT POINT,0.3825136612021858,"4.1
FERMAT POINT"
FERMAT POINT,0.38524590163934425,"Deﬁnition 4.1 (Fermat point). The set of Fermat points P(D) of a dataset D = {d1, ..., dn} contains
points minimizing its ℓ2 distance to all points in D:"
FERMAT POINT,0.3879781420765027,"P(D) = {arg min
x∈Rp n
X"
FERMAT POINT,0.3907103825136612,"i=1
||x −di||2}
(6)"
FERMAT POINT,0.39344262295081966,"One obstacle of using ℓ2 norm as our loss is that Fermat points aren’t unique in the worst case.
Given a (ﬁnite) dataset D, we can easily see that P(D) is a compact subset of the convex hull of D,
which encourages us to deﬁne a unique “maximum” element in P(D). To do so, we introduce the
following well-order on Rp.
Deﬁnition 4.2 (Coordinate dictionary order). A point x is said to be larger than y in coordinate
dictionary order if and only if there exists an index i ∈[n] such that xi > yi, and for any j < i we
have that xj = yj."
FERMAT POINT,0.39617486338797814,"It’s straightforward to verify that CDO (coordinate dictionary order) is a well-order. Next we use
CDO to select a unique member from the set P(D) of all Fermat points.
Deﬁnition 4.3 (Ordered Fermat point). The Ordered Fermat point q(D) of a dataset D
=
{d1, ..., dn} is deﬁned as:
q(D) = arg max
x∈P (D) CDO(x)
(7)"
FERMAT POINT,0.3989071038251366,"Such q(D) must exist for a ﬁnite dataset as long as P(D) is compact and non-empty, because there
can’t be an ordered inﬁnite sequence with its limit outside of P(D) which contradicts compactness.
The technical proof of the following proposition is deferred to appendix.
Proposition 4.1. q(D) always exists for a ﬁnite dataset D."
FERMAT POINT,0.4016393442622951,"Note that q(D) is unique by deﬁnition and is always a minimizer of L(θ; D) over Rp. In the fol-
lowing subsection we are going to show that any pure DP algorithm can’t estimate q(D) with good
accuracy, then prove that a large error in estimating q(D) will lead to large error in the excess risk
of ℓ2 norm loss as well, establishing the main lower bound of this section."
LOWER BOUND,0.40437158469945356,"4.2
LOWER BOUND"
LOWER BOUND,0.40710382513661203,"In this subsection, we prove a lower bound on the excess risk incurred by any ϵ-differentially private
algorithm whose output is denoted by θpriv ∈Rp. We ﬁrst introduce the following lemma showing"
LOWER BOUND,0.4098360655737705,Under review as a conference paper at ICLR 2022
LOWER BOUND,0.412568306010929,"that it’s impossible to ﬁnd the location of the ordered Fermat point q(D) with good accuracy using
a pure DP algorithm."
LOWER BOUND,0.41530054644808745,"The proof follows the spirit of Bassily et al. (2014), constructing datasets ’far away’ from each other
such that the events of estimating the Fermat point of each dataset accurately are mutually disjoint.
Then by differential privacy as long as one can estimate one dataset accurately, one can estimate any
other one with certain probability as well. The sum of all these probabilities is no more than 1 due
to the disjointness, which leads to the desired bound."
LOWER BOUND,0.4180327868852459,"We denote e1 ≜(1, 0, ..., 0)⊤and let ⊕denote the direct sum of vectors, i.e. α ⊕β = (α, β) where
α ∈Ra, β ∈Rb are both vectors. For a vector α and a set S, we denote α ⊕S = {(α, β) : β ∈S}.
Lemma 4.4. Let n, p
≥
2 and ϵ
>
0.
There is a number M
=
Ω(min(n, p"
LOWER BOUND,0.4207650273224044,"ϵ )) such
that for any ϵ-differentially private algorithm A, there is a dataset D
=
{d1, ..., dn}
⊂

0 ⊕{
1
√p−1, −
1
√p−1}p−1
∪{e1, −e1, 0} with || Pn
i=1 di||2 ≤M such that, with probability at"
LOWER BOUND,0.42349726775956287,"least 1/2 (taken over the algorithm random coins), we have"
LOWER BOUND,0.4262295081967213,"||A(D) −q(D)||2 = Ω(min(1, p"
LOWER BOUND,0.42896174863387976,"nϵ))
(8)"
LOWER BOUND,0.43169398907103823,"The classic analysis of Bassily et al. (2014) contains an ’adding dummy points’ which will perturb
the location of the minimizer. In the constrained case, such perturbation won’t change the direction
of the minimizer (seen as a vector), but in the unconstrained case non-linear loss functions no longer
enjoy such good properties. To oversome this issue, we introduce the auxiliary dimension and the
dummy points we add have support only in this dimension. The beneﬁt of doing so is that the Fermat
point q(D) will also only change along its direction after we add dummy points, which simpliﬁes
the computation."
LOWER BOUND,0.4344262295081967,"Lemma 4.4 implies that it’s impossible to estimate the ordered Fermat point with good accuracy
using a pure DP algorithm. In the following theorem we are going to show that a bad estimate on
the ordered Fermat point leads to higher ℓ2 norm loss. As the fermat point is a minimizer of ℓ2 norm
loss, we can naturally translate the discrepancy in estimating q(D) to the excess risk.
Theorem 4.5 (Lower bound for ϵ-differentially private algorithms). Let n, p ≥2 and ϵ > 0. For ev-
ery ϵ-differentially private algorithm with output θpriv ∈Rp, there is a dataset D = {d1, ..., dn} ⊂

0 ⊕{
1
√p−1, −
1
√p−1}p−1
∪{e1, −e1, 0} such that, with probability at least 1/2 (over the algo-
rithm random coins), we must have that"
LOWER BOUND,0.4371584699453552,"L(θpriv; D) −min
θ
L(θ; D) = Ω(min(1, p"
LOWER BOUND,0.43989071038251365,"nϵ))
(9)"
LOWER BOUND,0.4426229508196721,"The proof is based on calculation in the two-dimensional subspace spanned by q(D) and the auxil-
iary dimension. By observing that q(D) is perpendicular to the auxiliary dimension, we can parame-
terize θpriv by these two unit vectors and write down the expression of L(θpriv; D) −minθ L(θ; D)
explicitly. Then by elementary inequality scaling we get the desired result.
Remark 4.6. In fact the lower bound in Theorem 4.5 also holds for the case p = 1. The only
difference is that the case p = 1 doesn’t need the auxiliary dimension because the perturbation
of the minimizer is always along its direction. We can simply use dummy points {1, −1, 0} and a
similar analysis to Bassily et al. (2014) to achieve this result."
CONCLUSION,0.4453551912568306,"5
CONCLUSION"
CONCLUSION,0.44808743169398907,"In this paper, we study differentially private convex ERM in the unconstrained case and give the ﬁrst
tight lower bounds for approximate-DP ERM for general loss functions. Our results also directly
imply a same lower bound for the constrained case, improving the classic lower bound in Bassily
et al. (2014) by log(1/δ). We also give an Ω( p"
CONCLUSION,0.45081967213114754,"nϵ) lower bound for unconstrained pure-DP ERM
which recovers the result in the constrained case. Our techniques enrich the quite limited tools in
constructing lower bounds in the private setting and we hope they can ﬁnd future use, especially
for those problems which are not (easily) reducible from one-way marginals. Designing better
algorithms for general (un)constrained DP-ERM based on our insights would also be an interesting
and meaningful direction, which we leave as future work."
CONCLUSION,0.453551912568306,Under review as a conference paper at ICLR 2022
REFERENCES,0.4562841530054645,REFERENCES
REFERENCES,0.45901639344262296,"Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization:
Optimal rates in ℓ1 geometry. arXiv preprint arXiv:2103.01516, 2021."
REFERENCES,0.46174863387978143,"Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efﬁcient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pages 464–473. IEEE, 2014."
REFERENCES,0.4644808743169399,"Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
convex optimization with optimal rates. In Advances in Neural Information Processing Systems,
pages 11282–11291, 2019."
REFERENCES,0.4672131147540984,"Raef Bassily, Vitaly Feldman, Crist´obal Guzm´an, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. arXiv preprint arXiv:2006.06914, 2020."
REFERENCES,0.46994535519125685,"Raef Bassily, Crist´obal Guzm´an, and Anupama Nandi. Non-euclidean differentially private stochas-
tic convex optimization. arXiv preprint arXiv:2103.01278, 2021."
REFERENCES,0.4726775956284153,"Dan Boneh and James Shaw. Collusion-secure ﬁngerprinting for digital data. IEEE Transactions on
Information Theory, 44(5):1897–1905, 1998."
REFERENCES,0.47540983606557374,"Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate
differential privacy. SIAM Journal on Computing, 47(5):1888–1938, 2018."
REFERENCES,0.4781420765027322,"Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In NIPS, vol-
ume 8, pages 289–296. Citeseer, 2008."
REFERENCES,0.4808743169398907,"Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical
risk minimization. Journal of Machine Learning Research, 12(3), 2011."
REFERENCES,0.48360655737704916,"Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pages 265–284. Springer, 2006."
REFERENCES,0.48633879781420764,"Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211–407, 2014."
REFERENCES,0.4890710382513661,"Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal
rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing, pages 439–449, 2020."
REFERENCES,0.4918032786885246,"Kazuto Fukuchi, Quang Khai Tran, and Jun Sakuma. Differentially private empirical risk minimiza-
tion with input perturbation. In International Conference on Discovery Science, pages 82–90.
Springer, 2017."
REFERENCES,0.49453551912568305,"Moritz Hardt and Kunal Talwar. On the geometry of differential privacy. In Proceedings of the
forty-second ACM symposium on Theory of computing, pages 705–714, 2010."
REFERENCES,0.4972677595628415,"Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, and Lun Wang. To-
wards practical differentially private convex optimization. In 2019 IEEE Symposium on Security
and Privacy (SP), pages 299–316. IEEE, 2019."
REFERENCES,0.5,"Prateek Jain and Abhradeep Guha Thakurta. (near) dimension independent risk bounds for differen-
tially private learning. In International Conference on Machine Learning, pages 476–484. PMLR,
2014."
REFERENCES,0.5027322404371585,"Peter Kairouz, M´onica Ribero, Keith Rush, and Abhradeep Thakurta. Dimension independence in
unconstrained private erm via adaptive preconditioning. arXiv preprint arXiv:2008.06570, 2020."
REFERENCES,0.505464480874317,"Shiva Prasad Kasiviswanathan and Hongxia Jin. Efﬁcient private empirical risk minimization for
high-dimensional learning. In International Conference on Machine Learning, pages 488–497.
PMLR, 2016."
REFERENCES,0.5081967213114754,"Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam
Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793–826, 2011."
REFERENCES,0.5109289617486339,Under review as a conference paper at ICLR 2022
REFERENCES,0.5136612021857924,"Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization
and high-dimensional regression. In Conference on Learning Theory, pages 25–1. JMLR Work-
shop and Conference Proceedings, 2012."
REFERENCES,0.5163934426229508,"Janardhan Kulkarni, Yin Tat Lee, and Daogao Liu. Private non-smooth empirical risk minimization
and stochastic convex optimization in subquadratic steps. arXiv preprint arXiv:2103.15352, 2021."
REFERENCES,0.5191256830601093,"Benjamin IP Rubinstein, Peter L Bartlett, Ling Huang, and Nina Taft. Learning in a large function
space: Privacy-preserving mechanisms for svm learning. arXiv preprint arXiv:0911.5708, 2009."
REFERENCES,0.5218579234972678,"Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differ-
entially private updates. In 2013 IEEE Global Conference on Signal and Information Processing,
pages 245–248. IEEE, 2013."
REFERENCES,0.5245901639344263,"Shuang Song, Thomas Steinke, Om Thakkar, and Abhradeep Thakurta. Evading the curse of dimen-
sionality in unconstrained private glms. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 2638–2646. PMLR, 2021."
REFERENCES,0.5273224043715847,"Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. arXiv
preprint arXiv:1501.06095, 2015."
REFERENCES,0.5300546448087432,"Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Nearly-optimal private lasso. In Proceedings
of the 28th International Conference on Neural Information Processing Systems-Volume 2, pages
3025–3033, 2015."
REFERENCES,0.5327868852459017,"G´abor Tardos. Optimal probabilistic ﬁngerprint codes. Journal of the ACM (JACM), 55(2):1–24,
2008."
REFERENCES,0.5355191256830601,"Neal R Wagner. Fingerprinting. In 1983 IEEE Symposium on Security and Privacy, pages 18–18.
IEEE, 1983."
REFERENCES,0.5382513661202186,"Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited:
Faster and more general. In Advances in Neural Information Processing Systems, pages 2722–
2731, 2017."
REFERENCES,0.5409836065573771,"Puyu Wang, Yunwen Lei, Yiming Ying, and Hai Zhang. Differentially private sgd with non-smooth
loss. arXiv preprint arXiv:2101.08925, 2021."
REFERENCES,0.5437158469945356,"Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on
differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the
2017 ACM International Conference on Management of Data, pages 1307–1322, 2017."
REFERENCES,0.546448087431694,"Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang. Efﬁcient private erm for smooth objec-
tives. arXiv preprint arXiv:1703.09947, 2017."
REFERENCES,0.5491803278688525,Under review as a conference paper at ICLR 2022
REFERENCES,0.5519125683060109,"A
ADDITIONAL BACKGROUND KNOWLEDGE"
REFERENCES,0.5546448087431693,"A.1
GENERALIZED LINEAR MODEL (GLM)"
REFERENCES,0.5573770491803278,"The generalized linear model (GLM) is a ﬂexible generalization of ordinary linear regression that
allows for response variables that have error distribution models other than a normal distribution. To
be speciﬁc,
Deﬁnition A.1 (Generalized linear model (GLM)). The generalized linear model (GLM) is a special
class of ERM problems where the loss function ℓ(θ, d) takes the following inner-product form:"
REFERENCES,0.5601092896174863,"ℓ(θ; d) = ℓ(⟨θ, x⟩; y)
(10)"
REFERENCES,0.5628415300546448,"for d = (x, y). Here, x ∈Rp is usually called the feature vector and y ∈R is called the response."
REFERENCES,0.5655737704918032,"A.2
PROPERTIES OF DIFFERENTIAL PRIVACY"
REFERENCES,0.5683060109289617,"In this subsection we introduce several very basic properties of differential privacy without proving
them (refer Dwork et al. (2014) for details). Readers familiar with the ﬁeld of differential privacy
can feel free to skip this section.
Proposition A.1 (Group privacy). If M : Xn →Y is (ϵ, δ)-differentially private mechanism, then
for all pairs of datasets x, x′ ∈Xn, then M(x), M(x′) are (kϵ, kδekϵ)-indistinguishable when
x, x′ differs on exact k locations.
Proposition A.2 (Post processing). If M : Xn →Y is (ϵ, δ)-differentially private and A : Y →Z
is any randomized function, then A ◦M : Xn →Z is also (ϵ, δ)-differentially private.
Proposition A.3 (Composition). Let Mi be an (ϵi, δi)-differentially private mechanism for all i ∈
[k]. If M[k] is deﬁned to be
M[k](x) = (M1(x), ..., Mk(x))
(11)"
REFERENCES,0.5710382513661202,"then M[k] is (Pk
i=1 ϵi, Pk
i=1 δi)-differentially private."
REFERENCES,0.5737704918032787,"B
FINGERPRINTING CODE"
REFERENCES,0.5765027322404371,"In this section we brieﬂy introduce the mechanism of the ﬁngerprinting code Algorithm 1. The
sub-procedure part is the original ﬁngerprinting code in Tardos (2008), with a pair of randomized
algorithms (Gen, Trace). The code generator Gen outputs a codebook C ∈{0, 1}n×p. The ith row
of C is the codeword of user i. The parameter p is called the length of the ﬁngerprinting code."
REFERENCES,0.5792349726775956,"The security property of ﬁngerprinting codes asserts that any codeword can be “traced” to a user i.
Moreover, we require that the ﬁngerprinting code can ﬁnd one of the malicious users even when they
get together and combine their codewords in any way that respects the marking condition. That is,
there is a tracing algorithm Trace that takes as inputs the codebook C and the combined codeword
c′ and outputs one of the malicious users with high probability."
REFERENCES,0.5819672131147541,"The sub-procedure Gen′ ﬁrst uses a sin2 x like distribution to generate a parameter pj (the mean)
for each column j independently, then generates C randomly by setting each element to be 1 with
probability pj according to its location. The sub-procedure Trace′ computes a threshold value Z
and a ’score function’ Si(c′) for each user i, then report i when its score is higher than the threshold."
REFERENCES,0.5846994535519126,"The main-procedure was introduced in Bun et al. (2018), where Gen adds dummy columns to the
original ﬁngerprinting code and applies a random permutation. Trace can ﬁrst ’undo’ the permu-
tation and remove the dummy columns, then use Trace′ as a black box. This procedure makes the
ﬁngerprinting code more robust in that it tolerates a small fraction of errors to the marking condition."
REFERENCES,0.587431693989071,"C
OMITTED PROOFS"
REFERENCES,0.5901639344262295,"C.1
PROOF OF LEMMA 3.1"
REFERENCES,0.592896174863388,"Proof. The proof uses a black-box reduction, therefore doesn’t depend on Q.
The direction
that O(n∗/ϵ) samples are sufﬁcient is equal to proving the assertion that given a (1, o(1/n))-"
REFERENCES,0.5956284153005464,Under review as a conference paper at ICLR 2022
REFERENCES,0.5983606557377049,"Algorithm 1 The Fingerprinting Code (Gen, Trace)"
REFERENCES,0.6010928961748634,"1: Sub-procedure Gen′:
2: Let d = 100n2 log(n/ξ) be the length of the code.
3: Let t = 1/300n be a parameter and let t′ be such that sin2t′ = t.
4: for j = 1, ..., d: do
5:
Choose random r uniformly from [t′, π/2 −t′] and let pj = sin2rj. Note that pj ∈[t, 1 −t]."
REFERENCES,0.6038251366120219,"6:
For each i = 1, ..., n, set Cij = 1 with probability pj independently.
7: end for
8: return C
9: Sub-procedure Trace′(C, c′):
10: Let Z = 20n log(n/ξ) be a parameter.
11: For each j = 1, ..., d, let qj =
p"
REFERENCES,0.6065573770491803,"(1 −pj)/pj.
12: For each j = 1, ..., d, and each i = 1, ..., n, let Uij = qj if Cij = 1 and Uij = −1/qj else wise."
REFERENCES,0.6092896174863388,"13: for each i = 1, ..., n: do
14:
Let Si(c′) = Pd
j=1 c′
jUij
15:
Output i if Si(c′) ≥Z/2.
16:
Output ⊥if Si(c′) < Z/2 for every i = 1, ..., n.
17: end for
18: Main-procedure Gen:
19: Let C be the (random) output of Gen′, C ∈{0, 1}n×d"
REFERENCES,0.6120218579234973,"20: Append 2d 0-marked columns and 2d 1-marked columns to C.
21: Apply a random permutation π to the columns of the augmented codebook.
22: Let the new codebook be C′ ∈{0, 1}n×5d.
23: return C′"
REFERENCES,0.6147540983606558,"24: Main-procedure Trace(C, c′):
25: Obtain C′ from the shared state with Gen.
26: Obtain C by applying π−1 to the columns of C′ and removing the dummy columns.
27: Obtain c by applying π−1 to c′ and removing the symbols corresponding to fake columns.
28: return i randomly from Trace′(C, c)."
REFERENCES,0.6174863387978142,"differentially private algorithm A, we can get a new algorithm A′ with (ϵ, o(1/n))-differential pri-
vacy at the cost of shrinking the size of the dataset by a factor of ϵ."
REFERENCES,0.6202185792349727,"Given input ϵ and a dataset X, we construct A′ to ﬁrst generate a new dataset T by selecting each
element of X with probability ϵ independently, then feed T to A. Fix an event S and two neighboring
datasets X1, X2 that differs by a single element i. Consider running A on X1. If i is not included
in the sample T, then the output is distributed the same as a run on X2. On the other hand, if i is
included in the sample T, then the behavior of A on T is only a factor of e off from the behavior
of A on T \ {i}. Again, because of independence, the distribution of T \ {i} is the same as the
distribution of T conditioned on the omission of i."
REFERENCES,0.6229508196721312,"For a set X, let pX denote the distribution of A(X), we have that for any event S,"
REFERENCES,0.6256830601092896,"pX1(S) = (1 −ϵ)pX1(S|i /∈T) + ϵpX1(S|i ∈T)
≤(1 −ϵ)pX2(S) + ϵ(e · pX2(S) + δ)
≤exp(2ϵ)pX2(S) + ϵδ"
REFERENCES,0.6284153005464481,"A lower bound of pX1(S) ≥exp(−ϵ)pX2(S) −ϵδ/e can be obtained similarly. To conclude,
since ϵδ = o(1/n) as the sample size n decreases by a factor of ϵ, A′ has (2ϵ, o(1/n))-differential
privacy. The size of X is roughly 1/ϵ times larger than T, combined with the fact that A has sample
complexity n∗and T is fed to A, A′ has sample complexity at least Θ(n∗/ϵ)."
REFERENCES,0.6311475409836066,"For the other direction, simply using the composability of differential privacy yields the desired
result. In particular, by the k-fold adaptive composition theorem in Dwork et al. (2006), we can
combine 1/ϵ independent copies of (ϵ, δ)-differentially private algorithms to get an (1, δ/ϵ) one and
notice that if δ = o(1/n), then δ/ϵ = o(1/n) as well because the sample size n is scaled by a factor
of ϵ at the same time, offsetting the increase in δ."
REFERENCES,0.6338797814207651,Under review as a conference paper at ICLR 2022
REFERENCES,0.6366120218579235,"C.2
PROOF OF LEMMA 3.2"
REFERENCES,0.639344262295082,"Proof. In line 5 of algorithm 1, every column j is assigned a probability pj independently where"
REFERENCES,0.6420765027322405,Pr[|pj −1
REFERENCES,0.644808743169399,"2| < 0.002] <
1
400
(12)"
REFERENCES,0.6475409836065574,"by straightforward calculation. By the Chernoff bound (with u < p/400, δ = 1), with probability at
least
1 −2 exp(−p/800)
(13)
, at least 1 −
1
200 fraction of the columns have |pj −1"
REFERENCES,0.6502732240437158,"2| ≥0.002. Denote mj to be the mean of
entries of column j, then by using the Chernoff bound again (with δ = 0.001), we have that with
probability at least
1 −2 exp(−n/8000000)
(14)
a column j actually satisﬁes |mj −1"
REFERENCES,0.6530054644808743,"2| ≥0.001.
Again by the Chernoff bound (with u ≤
2 exp(−n/8000000)p and uδ = 0.01p) together with the union bound, at least 0.99 fraction of
all columns have |mj −1"
REFERENCES,0.6557377049180327,2| ≥0.001 with probability at least
REFERENCES,0.6584699453551912,"1 −2 exp(−p/800) −2 exp(−pen/8000000/40000) = 1 −O(e−Ω(p))
(15)"
REFERENCES,0.6612021857923497,"C.3
PROOF OF THEOREM 3.4"
REFERENCES,0.6639344262295082,"Proof. Let (α1, α2, α3) = (1/100, 999/1000, exp(−Ω(p)) be the parameters in the statement of
Lemma 3.3. Let k = Θ(log(1/δ)) be a parameter to be determined later and nk = ⌊n/k⌋."
REFERENCES,0.6666666666666666,"Consider the case when p ≥pnk ﬁrst, where pnk = O(ϵ2n2
k log(1/δ)). Without loss of generality,
we assume ϵ = 1 ﬁrst, and pnk = O(n2
k log(1/δ)) corresponds to the number in Lemma 3.3 where
we set ξ = δ. We will use contradiction to prove that for any (ϵ, δ)-differentially private mechanism
M, there exists some D ∈{0, 1}n×p with Gα1−1/k(D) ≤1 −α2 such that"
REFERENCES,0.6693989071038251,"E[L(M(D); D) −L(θ⋆; D)] ≥Ω(p)
(16)"
REFERENCES,0.6721311475409836,"Assume for contradiction that M : {0, 1}n×p →[0, 1]n×p is a (randomized) (ϵ, δ)-differentially
private mechanism such that"
REFERENCES,0.674863387978142,E[L(M(D); D) −L(θ⋆; D)] < α1α2p
REFERENCES,0.6775956284153005,"1000
for all D ∈{0, 1}n×p with Gα1−1/k(D) ≤(1 −α2). We then construct a mechanism Mk =
{0, 1}nk×p with respect to M as follows: with input Dk ∈{0, 1}nk×p, Mk will copy Dk for k
times and append enough 0’s to get a dataset D ∈{0, 1}n×p. The output is Mk(Dk) = M(D).
Mk is (k, ek−1"
REFERENCES,0.680327868852459,"e−1 δ)-differentially private by the group privacy. According to the construction above,
we know that if Gα1(Dk) < 1 −α2, then Gα1−1/k(D) < 1 −α2 as well."
REFERENCES,0.6830601092896175,"We consider algorithm AF P to be the adversarial algorithm in the ﬁngerprinting codes, which rounds
the the output Mk(Dk) to the binary vector, i.e. rounding those coordinates with values no less than
1/2 to 1 and the remaining 0, and let c = AF P (M(D)) be the vector after rounding. As Mk is
(k, ek−1"
REFERENCES,0.6857923497267759,"e−1 δ)-differentially private, AF P is also (k, ek−1"
REFERENCES,0.6885245901639344,e−1 δ)-differentially private.
REFERENCES,0.6912568306010929,"If for some Dk ∈{0, 1}nk×p with Gα1(Dk) ≤1 −α2, D (constructed from Dk as above) further
satisﬁes"
REFERENCES,0.6939890710382514,E[L(M(D); D) −L(θ⋆; D)] < α1α2p
REFERENCES,0.6967213114754098,1000 .
REFERENCES,0.6994535519125683,"As we are considering the ℓ1 loss, we can consider the loss caused by each coordinate indepen-
dently. Recall that Mk(Dk) = M(D). The fraction of those nearly unbiased columns (the mean
is close to 1/2) is at most 1 −α2, and we treat the worst-case error for them. For other ’α1-biased’
columns (coordinates) which take at least α2 fraction of all, if M(D) is right for the prediction, then
appending 0’s can’t change the prediction and Mk(Dk) is also right. Thus we have that"
REFERENCES,0.7021857923497268,"E[L(Mk(Dk); Dk) −L(θ⋆; Dk)] <E[L(M(D); D) −L(θ⋆; D)] + (1 −α2)p <
p
900."
REFERENCES,0.7049180327868853,Under review as a conference paper at ICLR 2022
REFERENCES,0.7076502732240437,By Markov Inequality we know that
REFERENCES,0.7103825136612022,"Pr[L(Mk(Dk); Dk) −L(θ⋆; Dk)] ≥
p
180] ≤1/5."
REFERENCES,0.7131147540983607,"c /∈Fβ(Dk) means that there is at least βp all-one or all-zero columns in Dk, but c is inconsistent
in those coordinates. Thus if c /∈Fβ(Dk), we have that L(Mk(Dk); Dk) −L(θ⋆; Dk) ≥βp/2 =
p/150 > p/180 for the Dk by Lemma 3.3, implying"
REFERENCES,0.7158469945355191,"Pr[c ∈Fβ(Dk)] ≥4/5.
(17)"
REFERENCES,0.7185792349726776,"By the ﬁrst property of the codes, one also has"
REFERENCES,0.7213114754098361,"Pr[L(Mk(Dk); Dk) −L(θ⋆; Dk) ≤p/180
^
Trace(Dk, c) =⊥]"
REFERENCES,0.7240437158469946,"≤Pr[c ∈Fβ(Dk)
^
Trace(Dk, c) =⊥] ≤δ."
REFERENCES,0.726775956284153,"Recall that the arguments above are for those Dk ∈{0, 1}nk×p with Gα1(Dk) ≤1 −α2, which
happens with probability at least 1 −α3 by the third property of ﬁngerprinting codes. By union
bound, we can upper bound the probability Pr[Trace(Dk, c) =⊥] ≤1/5 + δ + α3 ≤1/2. As a
result, there exists i∗∈[nk] such that"
REFERENCES,0.7295081967213115,"Pr[i∗∈Trace(Dk, c)] ≥1/(2nk).
(18)"
REFERENCES,0.73224043715847,"Consider the database with i∗removed, denoted by Dk
−i∗. Let c′ = AF P (M(Dk
−i∗)) denote the
vector after rounding. By the second property of ﬁngerprinting codes, we have that"
REFERENCES,0.7349726775956285,"Pr[i∗∈Trace(Dk
−i∗, c′)] ≤δ."
REFERENCES,0.7377049180327869,"By the differential privacy and post-processing property of M,"
REFERENCES,0.7404371584699454,"Pr[i∗∈Trace(Dk, c)] ≤ek Pr[i∗∈Trace(Dk
−i∗, c′)] + ek −1"
REFERENCES,0.7431693989071039,e −1 δ.
REFERENCES,0.7459016393442623,which implies that
REFERENCES,0.7486338797814208,"1
2nk
≤ek+1δ.
(19)"
REFERENCES,0.7513661202185792,"Recall that 2−O(n) < δ < o(1/n), and Equation (19) suggests k/n ≤2ek/δ for all valid k, but it
is easy to see there exists k = Θ(log(1/δ)) to make this inequality false, which is contraction. As a
result, there exists some D ∈{0, 1}n×p with Gα1−1/k(D) ≥(1 −α2) subject to"
REFERENCES,0.7540983606557377,E[L(M(D); D) −L(θ⋆; D)] ≥α1α2p
REFERENCES,0.7568306010928961,1000 = Ω(p).
REFERENCES,0.7595628415300546,"For the (ϵ, δ)-differential privacy case, setting Q to be the condition"
REFERENCES,0.7622950819672131,E[L(M(D); D) −L(θ⋆; D)] = O(p).
REFERENCES,0.7650273224043715,"in Lemma 3.1, we have that any (ϵ, δ)-differentially private mechanism M which satisﬁes Q for all
D ∈{0, 1}n×p with Gα1−1/k(D) ≥1 −α2 must have n ≥Ω(
p"
REFERENCES,0.76775956284153,p log(1/n)/ϵ).
REFERENCES,0.7704918032786885,"Now we consider the case when p < pnk, i.e. when n > n⋆≜Ω(
p"
REFERENCES,0.773224043715847,"p log(1/δ)/ϵ). Given any
dataset D ∈{0, 1}n⋆×p with Gα1−1/k(D) ≥1 −α2, we will construct a new dataset D′ based on
D by appending dummy points to D like in Lemma 4.4. Speciﬁcally, if n −n⋆is even, we append
n −n⋆rows among which half are 0 and half are {1}p. If n −n⋆is odd, we append n−n⋆−1"
POINTS,0.7759562841530054,"2
points
0, n−n⋆−1"
POINTS,0.7786885245901639,"2
points {1}p and one point {1/2}p."
POINTS,0.7814207650273224,"Denote the new dataset after appending by D′, we will draw contradiction if there is an (ϵ, δ)-
differentially private algorithm M′ such that E[L(M(D′); D′) −L(θ⋆; D′)] = o(n⋆p/n) for all D′,
by reducing M′ to an (ϵ, δ)-differentially private algorithm M which satisﬁes E[L(M(D); D) −
L(θ⋆; D)] = o(p) for all D with Gα1−1/k(D) ≥1 −α2."
POINTS,0.7841530054644809,Under review as a conference paper at ICLR 2022
POINTS,0.7868852459016393,"We construct M by ﬁrst constructing D′, and then use M′ as a black box to get M(D) = M′(D′).
It’s clear that such algorithm for D preserves (ϵ, δ)-differential privacy. It sufﬁces to show that if
E[L(M′(D′); D′) −L(θ⋆; D′)] = o(n⋆p/n),
(20)
then L(M(D); D) −L(θ⋆; D) = o(p), which contradicts the previous conclusion for the case
n ≤n⋆. Speciﬁcally, if n −n⋆is even, we have that
n⋆E[L(M(D); D) −L(θ⋆; D)] = nE[L(M′(D′); D′) −L(θ⋆; D′)].
and if n −n⋆is odd we have that
n⋆E[L(M(D); )D −L(θ⋆; D)] ≤nE[L(M′(D′); D′) −L(θ⋆; D′)] + p/2,
both leading to the desired reduction. We try to explain the above two cases in more detail. If
n −n∗is even, then the minimizer of L(; D) and L(θ∗; D) are the same. And the distributions of
the M(D) and M′(D′) are the same and indistinguishable. Multiplying n∗or n depends on the
number of rows (recall that we normalize the objective function in ERM). The second inequality is
because we append one point {1/2}p, which can only increase the loss (∥1/2p −θ∗∥1) by p/2 in
the worst case."
POINTS,0.7896174863387978,Combining results for both cases we have the following:
POINTS,0.7923497267759563,"E[L(θpriv; D) −L(θ⋆; D)] = Ω(min(p, pn∗"
POINTS,0.7950819672131147,"n ))
(21)"
POINTS,0.7978142076502732,"To conclude, observe that G = √p and C = √p. In particular, let D = (d, ..., d)⊤∈{0, 1}n×p
contain n identical copies of rows d ∈{0, 1}p, θ∗= d. Going over all such D, we ﬁnd that the
set {arg minθ L(θ; D)|D ⊂{0, 1}n×p} contains {0, 1}p, with diameter at least √p. Meanwhile, its
diameter can’t exceed √p obviously."
POINTS,0.8005464480874317,"C.4
DETAILS OF REMARK 3.7"
POINTS,0.8032786885245902,"We give a sketch of Remark 3.7. In Bun et al. (2018) they prove the following lower bound for
1-way marginals:
Proposition C.1 (Corollary 3.6 in Bun et al. (2018)). The family of 1-way marginals on {0, 1}d"
POINTS,0.8060109289617486,"has sample complexity at least ˜Ω(
√"
POINTS,0.8087431693989071,"d) for (1/3, 1/75)-accuracy and (O(1), o(1/n))-differential
privacy."
POINTS,0.8114754098360656,"Inspecting the proof we ﬁnd that the constant 1/3 in the above proposition is chosen casually, and
can be replaced by any constant c < 1/2 for free, as the proof only requires 1 −c is rounded to 1
and c is rounded to 0 respectively."
POINTS,0.8142076502732241,"The third property of the ﬁngerprinting code implies that with high probability, at most 0.01 fraction
of all columns have mean with bias smaller than 0.001. When we assume the opposite for the sake
of contradiction, by union bound, at least 1/75 −1/100 = 1/300 fraction of columns have both
’large error on 1-way marginal’ and ’large bias on mean’."
POINTS,0.8169398907103825,"For any such column j, the algorithm is forced to predict wrongly on the question ’Is there more 0’s
than 1’s in column j’ as the range of prediction is restricted in [0, 1] and choose c + 0.001 > 1/2,
thus leading to error on ℓ1 norm loss."
POINTS,0.819672131147541,"C.5
PROOF OF PROPOSITION 4.1"
POINTS,0.8224043715846995,"Proof. We assume P(D) ̸= ∅without loss of generality. To verify P(D) is compact, we ﬁrst
observe that P(D) is bounded. To prove P(D) is closed, notice that when P(D) ̸= ∅, the function
f(x) = Pn
i=1 ||x −di||2 is continuous and non negative, which implies its image is of the form
[a, ∞). Therefore the pre-image of the open set (a, ∞) is also open, whose complement is exactly
P(D)."
POINTS,0.825136612021858,"To ﬁnd the ordered Fermat point, we reduce the dimension of P(D) one after another. Because
P(D) is compact, the largest value of the ﬁrst coordinate a1 ≜argmaxx∈P (D)x1 exists, and the
ordered Fermat point must lie on the restriction of P(D) on {x|x1 = a1} which is also compact
and non-empty. We continue this process until all dimensions are peeled and there is one point left
because the only non-empty set with zero dimension is a single point."
POINTS,0.8278688524590164,Under review as a conference paper at ICLR 2022
POINTS,0.8306010928961749,"C.6
PROOF OF LEMMA 4.4"
POINTS,0.8333333333333334,"Proof. By using a standard packing argument we can construct K = 2
p−1"
POINTS,0.8360655737704918,"2
points d(1), ..., d(K) in
0 ⊕{
1
√p−1, −
1
√p−1}p−1 such that for every distinct pair d(i), d(j) of these points, we have"
POINTS,0.8387978142076503,||d(i) −d(j)||2 ≥1
POINTS,0.8415300546448088,"8
(22)"
POINTS,0.8442622950819673,"It is easy to show the existence of such set of points using the probabilistic method (for example, the
Gilbert-Varshamov construction of a linear random binary code)."
POINTS,0.8469945355191257,"Fix ϵ > 0 and deﬁne n⋆=
p
160ϵ. Let’s ﬁrst consider the case where n ≤4n⋆. We construct K
datasets D(1), ..., D(K) where for each i ∈[K], D(i) contains n copies of d(i). Note that q(D(i)) =
d(i), we have that for all i ̸= j,"
POINTS,0.8497267759562842,||q(D(i)) −q(D(j))||2 ≥1
POINTS,0.8524590163934426,"8
(23)"
POINTS,0.855191256830601,"Let A be any ϵ-differentially private algorithm. Suppose that for every D(i), i ∈[K], with probability
at least 1/2, ||A(D(i)) −q(D(i))||2 <
1
16,i.e.,Pr[A(D(i)) ∈B(D(i))] ≥1"
POINTS,0.8579234972677595,"2 where for any dataset D,
B(D) is deﬁned as"
POINTS,0.860655737704918,B(D) = {x ∈Rp : ||x −q(D)||2 < 1
POINTS,0.8633879781420765,"16}
(24)"
POINTS,0.8661202185792349,"Note that for all i ̸= j, D(i) and D(j) differs in all their n entries. Since A is ϵ-differentially private,
for all i ∈[K], we have Pr[A(D(1)) ∈B(D(i))] ≥1"
POINTS,0.8688524590163934,"2e−ϵn. Since all B(D(i)) are mutually disjoint,
then
K"
POINTS,0.8715846994535519,"2 e−ϵn ≤ K
X"
POINTS,0.8743169398907104,"i=1
Pr[A(D(1)) ∈B(D(i))] ≤1
(25)"
POINTS,0.8770491803278688,"which implies that n > 4n⋆for sufﬁciently large p, contradicting the fact that n ≤4n⋆. Hence,
there must exist a dataset D(i) on which A makes an ℓ2-error on estimating q(D) which is at least
1/16 with probability at least 1/2. Note also that the ℓ2 norm of the sum of the entries of such D(i)
is n."
POINTS,0.8797814207650273,"Next, we consider the case where n > 4n⋆.
As before, we construct K = 2
p−1"
DATASETS,0.8825136612021858,"2
datasets
˜D(1), · · · , ˜D(K) of size n where for every i ∈[K], the ﬁrst n⋆entries of each dataset ˜D(i) are
the same as dataset D(i) from before whereas the remaining n −n⋆entries are constructed as fol-
lows. The ﬁrst ⌊n−n⋆"
DATASETS,0.8852459016393442,"2
⌋of those entries are all copies of e1 whereas the following ⌊n−n⋆"
DATASETS,0.8879781420765027,"2
⌋are copies
of −e1. The last entry is set to be 0 when n −n⋆is odd."
DATASETS,0.8907103825136612,"Note that any two distinct datasets ˜D(i), ˜D(j) in this collection differ in exactly n⋆entries. Let A
be any ϵ-differentially private algorithm for answering q. Suppose that for every i ∈[K], with
probability at least 1/2, we have that"
DATASETS,0.8934426229508197,||A( ˜D(i)) −q( ˜D(i))||2 < n⋆
N,0.8961748633879781,"32n
(26)"
N,0.8989071038251366,"Note that for all i ∈[K], we have that q( ˜D(i)) = λq(D(i)) where λ =
n⋆
√"
N,0.9016393442622951,"n2−2nn⋆if n −n⋆is even
and
λ =
n⋆−1
q"
N,0.9043715846994536,4⌊n−n⋆
N,0.907103825136612,"2
⌋2 −(n⋆−1)2
(27)"
N,0.9098360655737705,if n −n⋆is odd. We notice that n⋆
N,0.912568306010929,n ≤λ ≤2n⋆
N,0.9153005464480874,"n , and is independent of the choice of i. Now, we
deﬁne an algorithm ˜A for answering q on datasets D of size n⋆as follows. First, ˜A computes λ
and appends e1, −e1, 0 as above to get a dataset ˜D of size n. Then, it runs A on ˜D and outputs
A( ˜
D)
λ
. Hence, by the post-processing propertry of differential privacy, ˜A is ϵ-differentially private
since A is ϵ-differentially private. Thus for every i ∈[K], with probability at least 1/2, we have
that || ˜A(D(i)) −q(D(i))||2 <
1
16. However, this contradicts our result in the ﬁrst part of the proof."
N,0.9180327868852459,Under review as a conference paper at ICLR 2022
N,0.9207650273224044,"Therefore, there must exist a dataset ˜D(i) in the above collection such that, with probability at least
1/2,"
N,0.9234972677595629,||A( ˜D(i)) −q( ˜D(i))||2 ≥n⋆
N,0.9262295081967213,"32n ≥
p
5120ϵn
(28)"
N,0.9289617486338798,Note that the ℓ2 norm of the sum of entries of such ˜D(i) is always n⋆.
N,0.9316939890710383,"C.7
PROOF OF THEOREM 4.5"
N,0.9344262295081968,"Proof. Let A be an ϵ-differentially private algorithm for minimizing L and let θpriv denote its output.
We choose the dataset D (with corresponding di) constructed in Lemma 4.4. When n ≤4n⋆, D
contains only identical elements di so that minθ L(θ; D) = 0, and"
N,0.9371584699453552,"L(θpriv; D) −min
θ
L(θ; D) = L(θpriv; D) = ||θpriv −q(D)||2 = Ω(min(1, p"
N,0.9398907103825137,"nϵ))
(29)"
N,0.9426229508196722,"by Lemma 4.4. When n > 4n⋆, we denote r ≜||θpriv −q(D)||2 = Ω(min(1, p"
N,0.9453551912568307,"ϵn)). Notice that
di, e1, −e1, 0 all lie in a 2-dimensional subspace and ||di||2 = 1 is perpendicular to e1, we may
assume di = e2 without loss of generality. Because q(D) = λe2, we parameterize θpriv as follows"
N,0.9480874316939891,"θpriv = (x1, λ + x2, x3, ..., xp)
(30)"
N,0.9508196721311475,"where Pp
i=1 x2
i = r2. Now the excess loss satisﬁes"
N,0.953551912568306,"L(θpriv; D) −min
θ
L(θ; D) ≥(n⋆p"
N,0.9562841530054644,"r2 + (λ −1)2 + 2(λ −1)x2 + ⌊n −n⋆ 2
⌋
p"
N,0.9590163934426229,r2 + 1 + λ2 + 2λx2 + 2x1
N,0.9617486338797814,"+ ⌊n −n⋆ 2
⌋
p"
N,0.9644808743169399,r2 + 1 + λ2 + 2λx2 −2x1)/n
N,0.9672131147540983,(opening up the expression)
N,0.9699453551912568,"≥(⌊n −n⋆ 2
⌋
p"
N,0.9726775956284153,"r2 + (λ −1)2 + 2x1 + ⌊n −n⋆ 2
⌋
p"
N,0.9754098360655737,r2 + (λ −1)2 −2x1)/n
N,0.9781420765027322,(dropping the ﬁrst term)
N,0.9808743169398907,"≥(⌊n −n⋆ 2
⌋
p"
N,0.9836065573770492,r2 + (λ −1)2)/n
N,0.9863387978142076,"(max{x1, −x1} ≥0)"
N,0.9890710382513661,≥⌊n −n⋆
N,0.9918032786885246,"2
⌋· r"
N,0.994535519125683,"n = Ω(min(1, p nϵ))"
N,0.9972677595628415,"(n > 4n⋆, r = Ω(min(1, p ϵn)))"
