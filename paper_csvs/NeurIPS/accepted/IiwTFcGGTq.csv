Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001145475372279496,"Out-of-distribution (OOD) generalization has attracted increasing research attention
in recent years, due to its promising experimental results in real-world applications.
Interestingly, we ﬁnd that existing OOD generalization methods are vulnerable
to adversarial attacks. This motivates us to study OOD adversarial robustness.
We ﬁrst present theoretical analyses of OOD adversarial robustness in two dif-
ferent complementary settings. Motivated by the theoretical results, we design
two algorithms to improve the OOD adversarial robustness. Finally, we conduct
experiments to validate the effectiveness of our proposed algorithms. Our code is
available at https://github.com/ZouXinn/OOD-Adv."
INTRODUCTION,0.002290950744558992,"1
Introduction"
INTRODUCTION,0.003436426116838488,"Recent years have witnessed the remarkable success of modern machine learning techniques in many
applications. A fundamental assumption of most machine learning algorithms is that the training
and test data are drawn from the same underlying distribution. However, this assumption is always
violated in many practical applications. The test environment is inﬂuenced by a range of factors,
such as the distributional shifts across the photos caused by different cameras in image classiﬁcation
tasks, the voices of different persons in voice recognition tasks, and the variations between scenes in
self-driving tasks [48]. Therefore, there is now a rapidly growing body of research with a focus on
generalizing to unseen distributions, namely out-of-distribution (OOD) generalization [56]."
INTRODUCTION,0.004581901489117984,"Deep neural networks (DNNs) have achieved state-of-the-art performance in many ﬁelds. However,
several prior works [59, 25] have demonstrated that DNNs may be vulnerable to imperceptibly
changed adversarial examples, which has increased focus on the adversarial robustness of the models.
Adversarial robustness refers to the invariance of a model to small perturbations of its input [54],
while adversarial accuracy refers to a model’s prediction performance on adversarial examples
generated by an attacker. However, the adversarial robustness of OOD generalization models (OOD
adversarial robustness) is less explored, despite its importance in many systems requiring high security
such as self-driving cars. We evaluate the adversarial robustness of the models trained with the current
OOD generalization algorithms (the detailed experimental settings can be found in Appendix C.3),
and present the results in Table 1. Surprisingly, under the PGD-20 [44] attack, the algorithms achieve
nearly 0% adversarial accuracy on RotatedMNIST [21], VLCS [15], PACS [38], and OfﬁceHome
[62], and no more than 10% adversarial accuracy on ColoredMNIST [4]. These results show that
even if the OOD generalization algorithms generalize well in different scenes, they remain highly
vulnerable to adversarial attacks."
INTRODUCTION,0.0057273768613974796,"Motivated by these limitations of existing algorithms, we provide theoretical analyses for OOD
adversarial robustness in two different but complementary OOD settings. We then design two"
INTRODUCTION,0.006872852233676976,∗Corresponding author: Weiwei Liu (liuweiwei863@gmail.com).
INTRODUCTION,0.008018327605956471,"Table 1: The results (%) for some of the current OOD generalization algorithms. We present the
results in the form of a/b: here, a is the OOD adversarial accuracy under PGD-20 attack [44];
and b is the OOD adversarial accuracy under AutoAttack [12]. We conduct the ℓ∞-norm attack.
We use the perturbation radius ϵ = 0.1 for RotatedMNIST and ColoredMNIST, and ϵ =
4
255 for
VLCS. For the architecture, following [26], we use a small CNN-architecture for RotatedMNIST and
ColoredMNIST, and ResNet-50 [27] for VLCS, PACS and OfﬁceHome. Since [64] do not realize
MAT and LDAT for RotatedMNIST, we use X to denote the unrealized results. For more details
about the algorithms, please refer to Appendix C.1. We use RM, CM, and OH as the abbreviation of
RotatedMNIST, ColoredMNIST, and OfﬁceHome, respectively."
INTRODUCTION,0.009163802978235968,"Algorithm
RM
CM
VLCS
PACS
OH
Avg"
INTRODUCTION,0.010309278350515464,"ERM [61]
0.6/0.0
5.8/X
0.0/0.0
0.3/0.6
0.4/0.0
1.4/X
MLDG [39]
0.2/0.0
4.8/X
0.0/0.0
0.1/0.3
0.6/0.1
1.2/X
CDANN [42]
0.9/0.0
8.2/X
3.0/0.0
1.5/0.3
0.1/0.0
2.7/X
VREx [33]
0.2/0.0
6.4/X
0.0/0.0
0.0/0.3
0.4/0.1
1.4/X
RSC [28]
1.0/0.0
3.9/X
0.0/0.0
0.1/0.4
0.7/0.0
1.1/X
MAT [64]
X/X
10.7/X
0.0/0.0
0.7/1.4
0.8/0.1
X/X
LDAT [64]
X/X
7.9/X
0.0/0.0
0.1/0.3
0.4/0.1
X/X"
INTRODUCTION,0.011454753722794959,"baseline algorithms to improve the OOD adversarial robustness, based on the implications of our
theory, and validate the effectiveness of our proposed algorithms through experiments."
INTRODUCTION,0.012600229095074456,Our contributions can be summarized as follows:
"WE EVALUATE THE ADVERSARIAL ROBUSTNESS OF CURRENT OOD GENERALIZATION ALGORITHMS AND
EXPERIMENTALLY VERIFY THAT THE CURRENT OOD GENERALIZATION ALGORITHMS ARE VULNERABLE TO",0.013745704467353952,"1. We evaluate the adversarial robustness of current OOD generalization algorithms and
experimentally verify that the current OOD generalization algorithms are vulnerable to
adversarial attacks."
WE PRESENT THEORETICAL ANALYSES FOR THE ADVERSARIAL OOD GENERALIZATION ERROR BOUNDS IN,0.014891179839633447,"2. We present theoretical analyses for the adversarial OOD generalization error bounds in
the average case and the limited training environments case. Speciﬁcally, our bounds in
limited training environments involve a “distance"" term between the training and the test
environments. We further use a toy example to illustrate how the “distance"" term affects the
OOD adversarial robustness, which is veriﬁed by the experimental results in Section 5."
WE PRESENT THEORETICAL ANALYSES FOR THE ADVERSARIAL OOD GENERALIZATION ERROR BOUNDS IN,0.016036655211912942,"3. Inspired by our theory, we propose two algorithms to improve OOD adversarial robustness.
Extensive experiments show that our algorithms are able to achieve more than 53% average
adversarial accuracy over the datasets."
WE PRESENT THEORETICAL ANALYSES FOR THE ADVERSARIAL OOD GENERALIZATION ERROR BOUNDS IN,0.01718213058419244,"The remainder of this article is structured as follows: §2 introduces related works. §3 presents our
main theory. §4 shows our two theory-driven algorithms. §5 provides our experimental results.
Finally, the conclusions are presented in the last section."
RELATED WORK,0.018327605956471937,"2
Related Work"
ADVERSARIAL ROBUSTNESS,0.019473081328751432,"2.1
Adversarial Robustness"
ADVERSARIAL ROBUSTNESS,0.020618556701030927,"[59] show that DNNs are fragile to imperceptible distortions in the input space. One of the most
popular methods used to improve adversarial robustness is adversarial training (AT). The seminal
AT work, the fast gradient sign method [25, FGSM], perturbs a sample towards its gradient direction
to increase the loss, then uses the generated sample to train the model. Following this line of research,
[44, 47, 34, 11] propose iterative variants of the gradient attack with improved AT frameworks.
[70, 30, 41] investigates the adversarial robustness from the perspective of ordinary differential
equations. Recently, [50, 66] utilize the data from generative models as data augmentation to improve
adversarial robustness. Besides, [43] analyze the trade-off between robustness and fairness, [37]
study the worst-class adversarial robustness in adversarial training."
ADVERSARIAL ROBUSTNESS,0.021764032073310423,"For the theoretical perspective, [46] study the PAC learnability of adversarial robust learning, [69]
extend the work of [46] to multiclass case, [72, 31, 5, 18, 67] give theoretical analyses to adversarial
training by standard uniform convergence argumentation and giving a bound of the Rademacher
complexity, and [65, 78] study adversarial robustness under self-supervised learning."
OUT-OF-DISTRIBUTION GENERALIZATION,0.022909507445589918,"2.2
Out-of-distribution generalization"
OUT-OF-DISTRIBUTION GENERALIZATION,0.024054982817869417,"OOD generalization aims to train a model with data from the training environments so that it is capable
of generalizing to an unseen environment. A large number of algorithms have been developed that
aim to improve OOD generalization. One series of works focuses on minimizing the discrepancies
between the training environments [40, 17, 42, 58, 1]. The most related work among them is [1],
which measures the discrepancy between the domains by dH(S, T), while we focus on adversarial
robustness and use dB
ℓ(H)(S, T). Meta-learning domain generalization [39, MLDG] leverages the
meta-learning approach and simulates train/test distributional shift during training by synthesizing
virtual testing domains within each mini-batch. [53, GroupDRO] studies applying distributionally
robust optimization (DRO) [19, 36, 77, 13] to learn models that instead minimize the worst-case
training loss over a set of pre-deﬁned groups. Another line of works [68, 64] conducts adversarial
training to improve the OOD generalization performance. In this work, we focus on improving the
OOD adversarial robustness."
OUT-OF-DISTRIBUTION GENERALIZATION,0.025200458190148912,"From the theoretical perspective, [10] introduce a formal framework and argue that OOD generaliza-
tion can be viewed as a kind of supervised learning problem by augmenting the original feature space
with the marginal distribution of feature vectors. In [51], OOD generalization is cast into an online
game where a player (model) minimizes the risk for a “new"" distribution presented by an adversary
at each time-step. [14] propose a probabilistic framework for domain generalization called Probable
Domain Generalization, wherein the key idea is that distribution shifts seen during training should
inform us of probable shifts at test time. Notably, all these works focus on OOD generalization
performance, while we present theoretical results for OOD adversarial robustness."
WORKS RELATING DOMAIN SHIFTS AND ADVERSRIAL ROBUSTNESS,0.026345933562428408,"2.3
Works relating domain shifts and adversrial robustness"
WORKS RELATING DOMAIN SHIFTS AND ADVERSRIAL ROBUSTNESS,0.027491408934707903,"[8] focuses on improving the adversarial robustness of the models by regarding the adversarial
distribution as the target domain and then applying the domain adaptation methods, while we focus on
improving the model’s adversarial robustness on the OOD distribution. [71] studies the relationship
between adversarial robustness and OOD generalization, it shows that good adversarial robustness
implies good OOD performance when the target domain lies in a Wasserstein ball. While we study
the OOD adversarial robustness and propose algorithms to improve OOD adversarial robustness. [2]
empirically analyzes the transferability of models’ adversarial/certiﬁed robustness under distributional
shifts. It shows that adversarially trained models do not generalize better without ﬁne-tuning and
that the accuracy-robustness trade-off generalizes to the unseen domain. Its results for adversarial
robustness can also be found in our experimental results. [29] investigates how to improve the
adversarial robustness of a model against ensemble attacks or unseen attacks. [29] regards the
adversarial examples for each type of attack (such as FGSM, PGD, CW, and so on) as a domain, and
utilizes the OOD generalization methods to improve the models generalization performance under
different (maybe unseen) attacks."
WORKS RELATING DOMAIN SHIFTS AND ADVERSRIAL ROBUSTNESS,0.0286368843069874,"[45, 22] study the relationship between the dependence on spurious correlations and the adversarial
robustness of the models and show that adversarial training increases the model’s reliance on spurious
features. [60] studies the relationship between fairness and adversarial robustness and shows that
models that are fairer will be less adversarially robust. However, they do not consider the adversarial
robustness of the model on the unseen target domain, which is the topic of this paper."
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.029782359679266894,"3
Theoretical Analysis for OOD Adversarial Robustness"
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.030927835051546393,"In this section, we present two theorems in two different settings, each of which inspires an algorithm
designed to improve the OOD adversarial robustness. The proofs of all results in this section can be
found in Appendix A. We ﬁrst introduce some notations and basic setups."
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.032073310423825885,"Notations. We deﬁne [n] := {1, 2, · · · , n}. We denote scalars and vectors with lowercase letters and
lowercase bold letters respectively. We use uppercase letters to denote matrices or random variables,
and uppercase bold letters to denote random vectors or random matrices. For a vector xxx ∈Rn, we
deﬁne the ℓp-norm of xxx as ∥xxx∥p := (Pn
i=1 |xi|p)1/p for p ∈[1, ∞), where xi is the i-th element of
xxx; for p = ∞, we deﬁne ∥xxx∥∞:= max
1≤i≤n|xi|. For a matrix A ∈Rm×n, the Frobenius norm of A"
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.033218785796105384,"is deﬁned as ∥A∥F :=
Pm
i=1
Pn
j=1 A2
ij
 1"
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.03436426116838488,"2 , where Aij is the entry of A at the i-th row and j-th
column. We deﬁne the determinant of A as det(A). N(µµµ, Σ) represents the multivariable Gaussian
distribution with mean vector µµµ and covariance matrix Σ. Given f, g : R →R+, we write f = O(g)
if there exist x0, α ∈R+ such that for all x > x0, we have f(x) ≤αg(x). We use sign(·) to denote
the sign function [57]."
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.035509736540664374,"Setups. Let X ∈Rm be the input space and Y be the label space. We set Y = {±1}, Y =
{1, 2, · · · , K} (where K is the number of classes), and Y = R for the binary classiﬁcation problem,
the multi-class classiﬁcation problem, and the regression problem, respectively. We use ℓ: Y × Y −→
R+ as the loss function. We consider learning with the hypothesis class H ⊆{h : X →Y}. Given a
distribution D on X × Y, the error of h ∈H with respect to the loss function ℓunder the distribution
D is RD(ℓ, h) =
E
(xxx,y)∼D [ℓ(h(xxx), y)], where xxx ∈X and y ∈Y. We further deﬁne B(·) : X →2X"
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.03665521191294387,"as a perturbation function that maps an input xxx to a subset B(xxx) ⊆X, where 2X is the power set
of X. The adversarial error of the predictor h under the perturbation function B(·) is deﬁned as"
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.037800687285223365,"RB
D(ℓ, h) =
E
(xxx,y)∼D """
THEORETICAL ANALYSIS FOR OOD ADVERSARIAL ROBUSTNESS,0.038946162657502864,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) # ."
THE AVERAGE CASE,0.04009163802978236,"3.1
The Average Case"
THE AVERAGE CASE,0.041237113402061855,"In this section, following [20, 14, 52], we consider the average case, i.e., the case in which the
target environment follows a distribution. In this case, we aim to minimize the average target
adversarial error of the hypothesis h ∈H, where the average is taken over the distribution of the
target environment."
THE AVERAGE CASE,0.042382588774341354,"Suppose that P(X × Y) is the set of all possible probability measures (environments) on X × Y
for the task of interest. Assume there is a prior p on P(X × Y), which is the distribution of the
environments. Moreover, suppose the process of sampling training and test data is as follows:"
THE AVERAGE CASE,0.043528064146620846,"(1). We generate the training data according to the following two steps: (i) we sample t training
environments from p, i.e., D1, · · · , Dt ∼p; (ii) the examples bDi = {(xxxi1, yi1), · · · , (xxxini, yini)} ∼
Dni
i
are drawn independent and identically distributed (i.i.d.) from Di, where ni is the size of bDi and
i ∈[t]. To simplify the notations, we also use bDi to denote the empirical distribution of the dataset
{(xxxi1, yi1), · · · , (xxxini, yini)}. Let bD = 1"
THE AVERAGE CASE,0.044673539518900345,"t
Pt
i=1 bDi and ˆp = 1"
THE AVERAGE CASE,0.045819014891179836,"t
Pt
i=1 Di. To simplify the problem,
we assume n1 = n2 = · · · = nt = n, but note that with a more careful analysis, our result in the
average case can be extended to the case in which n1, · · · , nt are not necessarily the same."
THE AVERAGE CASE,0.046964490263459335,"(2). For each test example, we ﬁrst sample an environment from p, i.e., D ∼p, then sample an
example (xxx, y) ∼D."
THE AVERAGE CASE,0.048109965635738834,"Let Lp(ℓ, h) =
E
D∼p [RD(ℓ, h)] be the average risk of h on prior p. For the perturbation function"
THE AVERAGE CASE,0.049255441008018326,"B(·), we deﬁne LB
p (ℓ, h) =
E
D∼p

RB
D(ℓ, h)

as the average adversarial risk of h. The empirical"
THE AVERAGE CASE,0.050400916380297825,"Rademacher complexity [55, Chapter 26] of the hypothesis class H is deﬁned as follows:"
THE AVERAGE CASE,0.05154639175257732,"Rn(H) = E
σσσ """
THE AVERAGE CASE,0.052691867124856816,"sup
h∈H"
N,0.053837342497136315,"1
n n
X"
N,0.054982817869415807,"i=1
σih(xxxi) # ,"
N,0.056128293241695305,"where σσσ is a Rademacher random vector with i.i.d. entries, and {xxx1, · · · ,xxxn} is a set of data points.
The following theorem presents an upper bound for the average adversarial risk of h.
Theorem 3.1. Suppose the loss function ℓis bounded, i.e., ℓ∈[0, U]. Then with probability of at
least 1 −δ over the sampling of bD1, · · · , bDt, the following bound holds for all h ∈H:"
N,0.0572737686139748,"LB
p (ℓ, h) ≤LB
b
D(ℓ, h) + 2Rt( eG) + 2Rtn( eG) + 3U r ln4/δ"
T,0.058419243986254296,"2t
+ 3U r ln4/δ 2tn , where eG = ("
T,0.05956471935853379,"gh : X × Y −→R+
gh(xxx, y) =
sup
xxx′∈B(xxx)
ℓ(h(xxx′, y)), h ∈H ) ."
T,0.06071019473081329,"Theorem 3.1 presents a bound for all hypotheses h ∈H. We now consider the convergence property
of the adversarial empirical risk minimization (AERM) algorithm in the average case. We deﬁne the
output of the AERM algorithm as ˆh ∈arg inf
h∈H
LB
b
D(ℓ, h). We then deﬁne the hypothesis with the best"
T,0.061855670103092786,"adversarial generalization performance as h⋆∈arg inf
h∈H
LB
p (ℓ, h)."
T,0.06300114547537228,"The next corollary shows an upper bound for the excess risk [63, Chapter 4] of AERM:
Corollary 3.2. Suppose the loss function ℓis bounded, i.e., ℓ∈[0, U]. Then with probability of at
least 1 −δ over the sampling of bD1, · · · , bDt, the following bound holds:"
T,0.06414662084765177,"LB
p (ℓ, ˆh) ≤LB
p (ℓ, h⋆) + 4Rt( eG) + 4Rtn( eG) + 3U r ln8/δ"
T,0.06529209621993128,"2t
+ 3U r ln8/δ 2tn ."
T,0.06643757159221077,"Remark 1. The convergence rate for both Theorem 3.1 and Corollary 3.2 is O

t−1"
T,0.06758304696449026,"2

, which prompts"
T,0.06872852233676977,"us to ask: can we derive a tighter bound that has a faster convergence rate than O

t−1"
T,0.06987399770904926,"2

? The next
section provides an afﬁrmative answer to this question."
THEORY WITH LIMITED ENVIRONMENTS,0.07101947308132875,"3.2
Theory with Limited Environments"
THEORY WITH LIMITED ENVIRONMENTS,0.07216494845360824,"In this section, we provide the theoretical analysis for the limited training environment case, i.e., in the
case in which t = O(1). Suppose that we have t training environments D1, · · · , Dt and one unknown
target environment T . Our goal is to ﬁnd a predictor that obtains good adversarial robustness on the
target environment T . Assume we obtain n examples from each training distribution Di."
THEORY WITH LIMITED ENVIRONMENTS,0.07331042382588775,"First, we introduce a discrepancy between the distributions. Based on the hypothesis class H, the
seminal work for unsupervised domain adaptation (UDA), [9], deﬁnes the discrepancy between two
distributions as follows:"
THEORY WITH LIMITED ENVIRONMENTS,0.07445589919816724,"dH(D, D′)=2 sup
h∈H"
THEORY WITH LIMITED ENVIRONMENTS,0.07560137457044673,"P
xxx∼D [h(xxx) = 1]−P
xxx∼D′ [h(xxx) = 1]
 ."
THEORY WITH LIMITED ENVIRONMENTS,0.07674684994272624,"[9] analyze the generalization error bound of the target domain for UDA by dH∆H(·, ·), where
g ∈H∆H ⇐⇒g = h ⊕h′ for some h, h′ ∈H; here, ⊕is the XOR operator. However, the above
deﬁnition is limited to the binary classiﬁcation. This paper extends dH to the multi-class adversarial
case. Given the loss function ℓ, distributions P, Q on X × Y, and hypothesis class H, we deﬁne the
adversarial discrepancy as follows:"
THEORY WITH LIMITED ENVIRONMENTS,0.07789232531500573,"dB
ℓ(H)(P, Q) = sup
h∈H"
THEORY WITH LIMITED ENVIRONMENTS,0.07903780068728522,"E
(xxx,y)∼P """
THEORY WITH LIMITED ENVIRONMENTS,0.08018327605956473,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
THEORY WITH LIMITED ENVIRONMENTS,0.08132875143184422,"−
E
(xxx,y)∼Q """
THEORY WITH LIMITED ENVIRONMENTS,0.08247422680412371,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y)"
THEORY WITH LIMITED ENVIRONMENTS,0.0836197021764032,# .
THEORY WITH LIMITED ENVIRONMENTS,0.08476517754868271,"When B(xxx) = {xxx}, dB
ℓ(H)(P, Q) becomes the standard case. It can be easily veriﬁed that dB
ℓ(H)(·, ·)
is symmetric and satisﬁes the triangle inequality (the proof can be found in Appendix A.3); thus,
dB
ℓ(H)(·, ·) is a pseudometric."
THEORY WITH LIMITED ENVIRONMENTS,0.0859106529209622,"Comparison of dH(·, ·) and dB
ℓ(H)(·, ·). The theory outlined in [9, Theorem 2] presents an upper
bound with a dH∆H(·, ·) term. To align the feature distributions of the source and target domain, we
need to calculate dH∆H(P, Q), which takes the supremum over two hypotheses h, h′ ∈H:"
THEORY WITH LIMITED ENVIRONMENTS,0.08705612829324169,"dH∆H(P, Q) = 2 sup
h,h′∈H"
THEORY WITH LIMITED ENVIRONMENTS,0.0882016036655212,"P
xxx∼P [h(xxx) ̸= h′(xxx)] −P
xxx∼Q [h(xxx) ̸= h′(xxx)]
 ."
THEORY WITH LIMITED ENVIRONMENTS,0.08934707903780069,"However, the deﬁnition of dB
ℓ(H)(·, ·) shows that: dB
ℓ(H)(P, Q) takes the supremum over one hypothe-
sis h ∈H, which is easier to optimize [76] and can thus signiﬁcantly ease the minimax optimization
in Section 4.2.
Theorem 3.3. For a given but unknown target distribution T , let ∆t−1 := {(λ1, · · · , λt)|λi ≥"
THEORY WITH LIMITED ENVIRONMENTS,0.09049255441008018,"0, Pt
i=1 λi = 1} be the (t −1)-dimensional simplex, and Conv(D) :=
nPt
i=1 λiDi
λλλ ∈∆t−1o
be"
THEORY WITH LIMITED ENVIRONMENTS,0.09163802978235967,"the convex hull of D = {D1, . . . , Dt}. Let TP ∈
arg inf
D∈Conv(D)
dB
ℓ(H)(D, T ) be the “projection"" of T"
THEORY WITH LIMITED ENVIRONMENTS,0.09278350515463918,"onto Conv(D), and λλλ⋆be the weight vector where TP = Pt
i=1 λ⋆
i Di. Assume ℓ∈[0, U]. Then: with
probability of at least 1 −δ, for all h ∈H,"
THEORY WITH LIMITED ENVIRONMENTS,0.09392898052691867,"RB
T (ℓ, h) ≤1 t t
X"
THEORY WITH LIMITED ENVIRONMENTS,0.09507445589919816,"i=1
RB
b
Di(ℓ, h) + 1 t X i X"
THEORY WITH LIMITED ENVIRONMENTS,0.09621993127147767,"j
λ⋆
jdB
ℓ(H)( bDi, bDj) + dB
ℓ(H)(T , TP )"
THEORY WITH LIMITED ENVIRONMENTS,0.09736540664375716,+ 4Rtn( eG) + 2Rn( eG) + 6U r ln8/δ
THEORY WITH LIMITED ENVIRONMENTS,0.09851088201603665,2tn + 3U r
THEORY WITH LIMITED ENVIRONMENTS,0.09965635738831616,ln(16t/δ)
N,0.10080183276059565,"2n
."
N,0.10194730813287514,"Remark 2. The ﬁrst term of the bound is the average empirical adversarial robust error of the model
on training environments, and the second term is the weighted average discrepancy on the empirical
training distributions bD = { bD1, · · · , bDt}. The third term can be viewed as the distance between T
and the convex hull Conv(D), which is ﬁxed once the task, ℓ, bD and T are given. Thus, minimizing
the ﬁrst two terms of the bound is able to improve the OOD adversarial robustness."
N,0.10309278350515463,"Moreover, Rtn( eG) and Rn( eG) measure the capacity of the model, and can be regarded as an implicit
regularizer on the model. There are many existing works that present the upper bounds of the
empirical Rademacher complexity for neural networks [49, 6, 7, 23] and the adversarial Rademacher
complexity [31, 73, 18, 5, 67]. Their results can be summarized as follows: for bounded X, with
proper weak assumptions on the loss function ℓ, Rn( eG) can be upper-bounded by O( C
√n), where
C is a constant that is related to some norm of the parameters of h and increases with the norm.
Thus, we consider constraints on the norm of the parameters of h in the algorithm designing part of
Section 4."
N,0.10423825887743414,"Last but not least, different from the results in Theorem 3.1, the convergence rate is O
q ln1/δ"
N,0.10538373424971363,"tn
+
q lnt/δ n"
N,0.10652920962199312,"
. When t = O(1), O
q lnt/δ n"
N,0.10767468499427263,"
in Theorem 3.3 converges much faster"
N,0.10882016036655212,"than O
q ln1/δ t"
N,0.10996563573883161,"
in Theorem 3.1, since n ≫t. µ Qµ Q2µ α
α R"
N,0.1111111111111111,"class 1, domain 0
class 1, domain 1
class 1, domain 2
class -1, domain 0
class -1, domain 1
class -1, domain 1"
N,0.11225658648339061,"Figure 1: An intuitive visualization of the distri-
butions D(0), D(1), D(2) (red, green, blue respec-
tively). We also represent the three mean vectors
µµµ, Qµµµ, Q2µµµ using red, green, and blue arrows. The
angle between µµµ, Qµµµ is α, which is the same as the
angle between Qµµµ, Q2µµµ. More details about the
relationship between Q and α can be found in the
proof of Theorem A.4 in Appendix A.1."
N,0.1134020618556701,"The dB
ℓ(H)(T , TP ) term in Theorem 3.3 is de-
termined by the distance between the source
and target environments. A larger dB
ℓ(H)(T , TP )
may lead to worse adversarial robustness of the
model on the target domain. Next, we present
a toy example to illustrate this case."
N,0.1145475372279496,"Example 1. We consider the input space X =
Rd ⊆Rm and label space Y = {+1, −1}.
For simplicity, we consider only three dis-
tributions on X × Y, i.e., D(0), D(1) and
D(2). The marginal distributions for Y satisfy
D(0)
Y ({−1}) = D(0)
Y ({+1}) = D(1)
Y ({−1}) =
D(1)
Y ({+1}) = D(2)
Y ({−1}) = D(2)
Y ({+1}) =
1
2. For D(i), let the conditional distribution of
X
X
X(i) given Y be X
X
X(i)|Y = y ∼N(yµµµ(i), σ2I),
where µµµ(i) ∈Rd is a non-zero vector and
σ2I ∈Rd×d is the covariance matrix, i.e., the
elements of X
X
X(i) are independent. Let Q be a rotation transformation on Rd, which is a special or-
thogonal transformation. Suppose that Q ̸= I, where I is the identity transformation. Let Q ∈Rd×d
be the matrix of Q under some orthonormal basis. We then know that Q is a rotation matrix and
QT Q = QQT = I, det(Q) = 1, Q ̸= I. To model the distributional shift among the environments,
we apply the transformation Q to µµµ(0) and use µµµ(1) = Qµµµ(0),µµµ(2) = Q2µµµ(0) as the mean vectors
of D(1), D(2) respectively. Here, we consider the ℓ2-norm adversarial attack with radius ϵ, i.e.,
B(xxx) = {xxx′ : ∥xxx −xxx′∥2 ≤ϵ}. We use two environments as the training environments and the
remainder as the test environment. Theorem 3.4 shows that a larger distance between the training and
test environments leads to worse OOD adversarial robustness."
N,0.1156930126002291,"Theorem 3.4. Consider the setting in Example 1, and suppose that µµµ lies in the 2-dimensional
subspace R in Theorem A.4 (see Appendix A.1 for details about R, and see Figure 1 for an intuitive
visualization). Let ℓ01(x, y) = 1[x ̸= y], where 1[·] is the indicator function and Br
p(xxx) = {xxx′ :
∥xxx −xxx′∥p ≤r}. Consider training with hypothesis class H = {hw
w
w : hw
ww(x) = sign(wwwT x),www ∈Rd},
and denote D(ij) = 1"
N,0.11683848797250859,2D(i) + 1
N,0.11798396334478808,"2D(j), i, j ∈{0, 1, 2}, i ̸= j. Consider the ℓ2-norm adversarial attack"
N,0.11912943871706758,"with radius ϵ; for notation convenience, let eRij(www) = RBϵ
2
D(ij)(ℓ01, hww
w) and eRi(www) = RBϵ
2
D(i)(ℓ01, hww
w).
Let Φ(·) be the distribution function of the standard normal distribution. Let α denote the angle
between µµµ and Qµµµ, which is the rotation angle of Q in the subspace R. Furthermore, suppose that
0 < α ≤arccos
ϵ
∥µµµ1∥2 . We then have:"
N,0.12027491408934708,"(1) If we train with D(0) and D(1), let www(01)=µµµ+Qµµµ, which achieves the minimum of eR01(www). Then"
N,0.12142038946162657,"the adversarial accuracy of www(01) on the test distribution D(2) is eR2(www(01))=Φ

ϵ
σ −
(µµµ+Qµµµ)T Q2µµµ"
N,0.12256586483390607,"σ∥µµµ+Qµµµ∥2 
."
N,0.12371134020618557,"(2) If we train with D(0) and D(2), let www(02)=µµµ+Q2µµµ, which achieves the minimum of eR02(www). Then"
N,0.12485681557846506,"the adversarial accuracy of www(02) on the test distribution D(1) is eR1(www(02))=Φ

ϵ
σ −
(µµµ+Q2µµµ)T Qµµµ"
N,0.12600229095074456,"σ∥µµµ+Q2µµµ∥2 
."
N,0.12714776632302405,"(3) If we train with D(1) and D(2), let www(12)=Qµµµ+Q2µµµ, which achieves the minimum of eR12(www). Then"
N,0.12829324169530354,"the adversarial accuracy of www(12) on the test distribution D(0) is eR0(www(12))=Φ

ϵ
σ −
(Qµµµ+Q2µµµ)Tµµµ
σ∥Qµµµ+Q2µµµ∥2 
."
N,0.12943871706758306,(4) eR1(www(02)) < eR2(www(01)) = eR0(www(12)).
N,0.13058419243986255,"Remark 3. Let {i, j, k} = {0, 1, 2}. For task i, we use D(j), D(k) as the training environments and
D(i) as the test environment. eRi(www(jk)) is the target adversarial error of the learned classiﬁer www(jk)
in task i."
N,0.13172966781214204,"We now consider the distance between the training and test environments for each task. Intuitively,
we regard the angle as the “distance"" between two distributions. We deﬁne angle(i, j) as the angle
between the mean vector of D(i) and D(j), i ̸= j. For task i, we deﬁne the average “distance""
between the test environment and each training environment as davg(i) := angle(i,j)+angle(i,k)"
N,0.13287514318442153,"2
. We use
davg(i) as a measure of the distance between the training environments and the test environment for
task i."
N,0.13402061855670103,"From the settings in Example 1, it can be clearly seen that davg(0) = angle(0,1)+angle(0,2)"
N,0.13516609392898052,"2
= α+2α 2
= 3α"
N,0.13631156930126,"2 = davg(2) and davg(1) = angle(1,0)+angle(1,2)"
N,0.13745704467353953,"2
= α+α"
N,0.13860252004581902,"2
= α, which implies that davg(1) < davg(0) =
davg(2). Theorem 3.4 tells us that eR1(www(02)) < eR0(www(12)) = eR2(www(01)), and thus implies that
a smaller distance between the training and test environments leads to better OOD adversarial
robustness."
N,0.13974799541809851,"Moreover, the assumption of the angle between µµµ and Qµµµ in Theorem 3.4 is reasonable. Consider"
N,0.140893470790378,ϵ = ∥µµµ∥2
N,0.1420389461626575,"2 ; in this case, the attack is strong and perceptible to human eyes. Then, α ≤arccos
ϵ
∥µµµ1∥2 =
arccos 1 2 = π"
N,0.143184421534937,"3 . In this case, the maximal angle between the environments is 2π"
N,0.14432989690721648,"3 , which leads to a
strong distribution shift. When ϵ < ∥µµµ∥2"
N,0.145475372279496,"2 , the allowed rotation angle can be further enlarged, and
when ϵ = 0, it becomes the standard case."
N,0.1466208476517755,"Furthermore, the data distribution here can be regarded as a simpliﬁed data model for RotatedMNIST.
Moreover, our experimental results in Section 5 are consistent with our analysis here in both the
standard and adversarial cases. Please refer to the observation part of Section 5.2 and the table in
Appendix D.1 for further details."
ALGORITHMS,0.14776632302405499,"4
Algorithms"
ALGORITHMS,0.14891179839633448,"In this section, based on our theory, we present two algorithms that can improve the adversarial
robustness of the model on the target environment."
ALGORITHMS,0.15005727376861397,"4.1
Adversarial Empirical Risk Minimization (AERM or AT)"
ALGORITHMS,0.15120274914089346,"Based on Theorem 3.1, which shows an upper bound of the average adversarial risk over the
environments, we propose our ﬁrst algorithm: adversarial empirical risk minimization (AERM, which
corresponds to applying AT to multiple source domains). The bound in Theorem 3.1 consists of the
average adversarial empirical risk LB
b
D(ℓ, h) = 1"
ALGORITHMS,0.15234822451317298,"t
Pt
i=1 RB
b
Di(ℓ, h) and two empirical Rademacher"
ALGORITHMS,0.15349369988545247,"complexity terms Rt( eG) + Rtn( eG). As outlined in Remark 2, the empirical Rademacher complexity
implies an implicit regularizer on the model capacity. We choose ∥· ∥F as a regularizer on the model
parameters. The optimization objective of AT is as follows:"
ALGORITHMS,0.15463917525773196,"LAT(h) = 1 t t
X"
ALGORITHMS,0.15578465063001146,"i=1
RB
b
Di(ℓ, h) + λ∥W∥F ,"
ALGORITHMS,0.15693012600229095,"where λ is a trade-off hyper-parameter, and W is the parameter matrix of the model. From Remark 1,
we know that AT may not generalize well in the case where the training environments are limited.
Next, we propose another algorithm for the limited environment case."
ALGORITHMS,0.15807560137457044,"4.2
Robust DANN (RDANN)"
ALGORITHMS,0.15922107674684993,"Theorem 3.3 shows that Rn( eG), 1"
ALGORITHMS,0.16036655211912945,"t
Pt
i=1 RB
b
Di(ℓ, h), and 1 t
P i
P"
ALGORITHMS,0.16151202749140894,"j λ⋆
jdB
ℓ(H)( bDi, bDj) play the key
roles in designing the OOD adversarial training methods. However, the weights λ⋆
1, · · · , λ⋆
t are
unknown, since we have no information about the target distribution T . Since λ⋆
j ∈[0, 1], ∀j, it is
evident that 1 t
P i
P"
ALGORITHMS,0.16265750286368844,"j λ⋆
jdB
ℓ(H)( bDi, bDj) ≤1 t
P i
P"
ALGORITHMS,0.16380297823596793,"j dB
ℓ(H)( bDi, bDj). We therefore turn to optimize"
ALGORITHMS,0.16494845360824742,"the average discrepancy 1 t2
P i
P"
ALGORITHMS,0.1660939289805269,"j dB
ℓ(H)( bDi, bDj). To improve the OOD adversarial robustness, we
minimize the following: 1 t t
X"
ALGORITHMS,0.1672394043528064,"i=1
RB
b
Di(ℓ, h) + λ1
1
t2 t
X i=1 t
X"
ALGORITHMS,0.16838487972508592,"j=1
dB
ℓ(H)( bDi, bDj) + λ2∥W∥F ,"
ALGORITHMS,0.16953035509736541,"where λ1, λ2 are two hyper-parameters, and W is the parameter matrix of the model. However, the
term 1"
ALGORITHMS,0.1706758304696449,"t2
Pt
i=1
Pt
j=1 dB
ℓ(H)( bDi, bDj) is a constant. Motivated by [17], we minimize the discrepancy
of the training environments in the feature space of a feature extractor f."
ALGORITHMS,0.1718213058419244,"Speciﬁcally, we consider H = {c ◦f|f ∈F, c ∈C}; this means that the predictor h consists of a
classiﬁer c and a feature extractor f, where F = {f : X →Z}, C = {c : Z →Y} and Z ⊆Rl is
the feature space. Any feature extractor f ∈F determines a hypothesis class Hf = {c ◦f|c ∈C}.
Given a feature extractor f, we apply Theorem 3.3 to the hypothesis class Hf. Then with high
probability, for any c ∈C, the target error of h = c◦f can be controlled mainly by 1"
ALGORITHMS,0.1729667812142039,"t
Pt
i=1RB
b
Di(ℓ, c◦f) and 1"
ALGORITHMS,0.17411225658648338,"t2
Pt
i=1
Pt
j=1 dB
ℓ(Hf )( bDi, bDj). We then aim to ﬁnd c and f such that h = c ◦f has good OOD
adversarial robustness. Thus, we aim to minimize the following:"
ALGORITHMS,0.17525773195876287,"L(c◦f)
z
}|
{
1 t t
X"
ALGORITHMS,0.1764032073310424,"i=1
RB
b
Di(ℓ, c ◦f)"
ALGORITHMS,0.1775486827033219,"|
{z
}
Lcls(ℓ,c◦f)"
ALGORITHMS,0.17869415807560138,"+λ1
1
t2 t
X i=1 t
X"
ALGORITHMS,0.17983963344788087,"j=1
dB
ℓ(Hf )( bDi, bDj)"
ALGORITHMS,0.18098510882016036,"|
{z
}
Ldisc(ℓ,f)"
ALGORITHMS,0.18213058419243985,+λ2∥W∥F .
ALGORITHMS,0.18327605956471935,"To minimize L(c ◦f), we need to solve a minimax problem. For simplicity, we ﬁx B(·), ℓand deﬁne
the following:"
ALGORITHMS,0.18442153493699887,"D(h, P, Q) ="
ALGORITHMS,0.18556701030927836,"E
(xxx,y)∼P """
ALGORITHMS,0.18671248568155785,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
ALGORITHMS,0.18785796105383734,"−
E
(xxx,y)∼Q """
ALGORITHMS,0.18900343642611683,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y)"
ALGORITHMS,0.19014891179839633,# .
ALGORITHMS,0.19129438717067582,"Since D(h, P, P) = 0 and D(h, P, Q) = D(h, Q, P) for any P, Q, h, our optimization problem can
be formulated as follows:"
ALGORITHMS,0.19243986254295534,"min
c∈C,f∈F
max
cij∈C:1≤i<j≤t Lcls(ℓ, c ◦f) + λ1
2
t(t −1) X"
ALGORITHMS,0.19358533791523483,"1≤i<j≤t
D(cij ◦f, bDi, bDj) + λ2∥W∥F ."
ALGORITHMS,0.19473081328751432,"Table 2: The results (%) of the algorithms. Results are presented in the form of a/b: here, a is the
OOD adversarial accuracy under PGD-20 attack [44]; and b is the OOD adversarial accuracy under
AutoAttack [12]. We conduct the ℓ∞-norm attack. Since [64] do not realize MAT and LDAT for
RotatedMNIST, we use X to denote the unrealized results. Best results for PGD-20 attack are shown
in bold. For more details about the algorithms, please refer to Appendix C.1. We use RM, CM, and
OH as the abbreviation of RotatedMNIST, ColoredMNIST, and OfﬁceHome, respectively."
ALGORITHMS,0.1958762886597938,"Algorithm
RM
CM
VLCS
PACS
OH
Avg"
ALGORITHMS,0.1970217640320733,"ERM
0.6/0.0
5.8/X
0.0/0.0
0.3/0.6
0.4/0.0
1.4/X
MLDG
0.2/0.0
4.8/X
0.0/0.0
0.1/0.3
0.6/0.1
1.2/X
CDANN
0.9/0.0
8.2/X
3.0/0.0
1.5/0.3
0.1/0.0
2.7/X
VREx
0.2/0.0
6.4/X
0.0/0.0
0.0/0.3
0.4/0.1
1.4/X
RSC
1.0/0.0
3.9/X
0.0/0.0
0.1/0.4
0.7/0.0
1.1/X
MAT
X/X
10.7/X
0.0/0.0
0.7/1.4
0.8/0.1
X/X
LDAT
X/X
7.9/X
0.0/0.0
0.1/0.3
0.4/0.1
X/X
AT (ours)
93.4/93.3
51.6/X
42.6/41.8
48.1/47.6
30.2/29.8
53.2/X
RDANN (ours)
93.5/93.3
51.1/X
44.9/43.9
48.1/48.0
28.6/27.4
53.3/X"
ALGORITHMS,0.1981672394043528,"To solve the minimax optimization problem, we adopt an idea similar to that of adversarial neural
networks [24] and refer to cij as the discriminator. However, there are t(t−1)"
DISCRIMINATORS IN THIS,0.19931271477663232,"2
discriminators in this
case, and training with many discriminators may make the optimization process unstable [3]. We
therefore opt to use the same discriminator for all ( bDi, bDj) pairs. Note that:"
DISCRIMINATORS IN THIS,0.2004581901489118,"max
c′∈C X"
DISCRIMINATORS IN THIS,0.2016036655211913,"i<j
D(c′ ◦f, bDi, bDj) ≤max
cij∈C X"
DISCRIMINATORS IN THIS,0.2027491408934708,"i<j
D(cij ◦f, bDi, bDj),"
DISCRIMINATORS IN THIS,0.20389461626575028,"such that optimizing over a shared discriminator c′ is equivalent to optimizing a lower bound of the
original objective. Our ﬁnal optimization problem then becomes:"
DISCRIMINATORS IN THIS,0.20504009163802978,"min
c∈C,f∈F max
c′∈C Lcls(ℓ, c ◦f)
|
{z
}
Lc(w,θ)"
DISCRIMINATORS IN THIS,0.20618556701030927,"+λ1
2
t(t −1) X"
DISCRIMINATORS IN THIS,0.2073310423825888,"1≤i<j≤t
D(c′ ◦f, bDi, bDj)"
DISCRIMINATORS IN THIS,0.20847651775486828,"|
{z
}
Ld(w′,θ)"
DISCRIMINATORS IN THIS,0.20962199312714777,"+λ2 ∥W∥F
| {z }
Lreg(w,θ) ."
DISCRIMINATORS IN THIS,0.21076746849942726,"where w, w′, θ are parameters for c, c′, f respectively. We call our method robust adversarial-domain
neural network (RDANN), the pseudo-code of which is presented in Algorithm 1."
EXPERIMENTS,0.21191294387170675,"5
Experiments"
EXPERIMENTS,0.21305841924398625,Algorithm 1 RDANN
EXPERIMENTS,0.21420389461626574,"Input: the training data bD1, . . . , bDt, the num-
ber of iterations T, the number of iterations
for the inner maximization problem k, the
learning rate η, the inner max learning rate
α, the tradeoff hyper-parameters λ1, λ2.
Output: the parameters wT , θT for c, f."
EXPERIMENTS,0.21534936998854526,"1: initialize w0, θ0, w′k randomly
2: for i ←0 to T −1 do
3:
set w′0 ←w′k"
EXPERIMENTS,0.21649484536082475,"4:
for j ←0 to k −1 do
5:
w′(j+1) ←w′j + α∇w′Ld(w′j, θi)
6:
end for
7:
Li
1 ←Lc(wi, θi) + λ2Lreg(wi, θi)
8:
Li
2 ←Li
1 + λ1Ld(w′k, θi)
9:
θi+1 ←θi −η∇θLi
2
10:
wi+1 ←wi −η∇wLi
1
11: end for"
EXPERIMENTS,0.21764032073310424,"To verify the adversarial robustness of our algo-
rithms, we conduct experiments on the DomainBed
benchmark [26], a testbed for OOD generalization
that implements consistent experimental protocols
across various approaches to ensure fair compar-
isons. Our code is attached in the supplementary
material."
EXPERIMENTAL SETUP,0.21878579610538373,"5.1
Experimental Setup
Datasets. The datasets we use are RotatedMNIST
[21], ColoredMNIST [4], VLCS [15], PACS [38],
and OfﬁceHome [62]. There are several different
environments in each dataset. For the DomainBed
benchmark, we choose one environment as the test
environment and use the others as training envi-
ronments. We report the average accuracy over
different choices of the test environment. Further details about the selected datasets can be found in
Appendix C.2."
EXPERIMENTAL SETUP,0.21993127147766323,"Backbone network. Following [26], we use a small CNN architecture for RotatedMNIST, Col-
oredMNIST, and ResNet-50 [27] for VLCS, PACS, and OfﬁceHome."
EXPERIMENTAL SETUP,0.22107674684994272,"Hyper-parameters for adversarial attack and adversarial training. We use ℓ∞-norm attack
for both adversarial attack and adversarial training. We use ϵ = 0.1 for ColoredMNIST and
RotatedMNIST, and ϵ = 4/255 for VLCS, PACS, and OfﬁceHome; moreover, following [74, 75],
we use PGD-10 to generate adversarial examples at the training stage and PGD-20 at the evaluation
stage to avoid overﬁtting. The step size used to generate adversarial examples is set to be ϵ/4."
EXPERIMENTAL SETUP,0.2222222222222222,More details of the experimental settings can be found in Appendix C.4.
RESULTS,0.22336769759450173,"5.2
Results
Table 2 presents the results of our experiments. As is clear from the table, our proposed algorithms
signiﬁcantly improve the OOD adversarial robustness of the model. For example, under the attack
PGD-20, all other algorithms achieve no more than 3% average adversarial accuracy, while our
proposed algorithms achieve more than 53% average adversarial accuracy. Moreover, in our datasets,
the number of environments (t) is small. From Table 2, we can see that the overall performance of
RDANN is superior to that of AT. RDANN achieves better or comparable adversarial accuracy on
most datasets (except OfﬁceHome). The results are consistent with our claim in Remark 2: when
t = O(1), the bound in Theorem 3.3 converges faster than that in Theorem 3.1. The detailed results
for each test environment are attached in Appendix D."
RESULTS,0.22451317296678122,"Observations. According to the detailed results for RotatedMNIST and ColoredMNIST in Ap-
pendix D.1 and Appendix D.2, we can make the following observations (these phenomena occur in
both the adversarial training setting and the standard training setting):"
RESULTS,0.2256586483390607,"• For RotatedMNIST, we have six environments, each of which corresponds to a rotation
angle of the original image. The rotation angle of the i-th environment is i × 15◦, i ∈
{0, 1, 2, 3, 4, 5}. For task i, we use the i-th environment as the test environment and the
remaining environments as the training environments. Following Remark 3, we deﬁne
davg(i) := 1"
P,0.2268041237113402,"5
P"
P,0.2279495990836197,"j̸=i angle(i, j) as the average distance between the test environment and
each training environment for task i. Then, davg(0) = 45◦, davg(1) = 33◦, davg(2) = 27◦,
davg(3) = 27◦, davg(4) = 33◦and davg(5) = 45◦. We deﬁne the PGD-20 adversarial
accuracy of the model trained by RDANN for task i as a(i); then, a(0) = 90.8, a(1) = 94.7,
a(2) = 95.3, a(3) = 95.6, a(4) = 95.5 and a(5) = 89.0. As i increases, davg(i) ﬁrst
decreases and then increases, while a(i) ﬁrst increases and then decreases. The result
indicates that davg(i) is anticorrelated with a(i), which is consistent with the analysis in
Remark 3. Note that this phenomenon occurs in all algorithms."
P,0.2290950744558992,"• For ColoredMNIST, we have three environments, each of which corresponds to a correlation
between the additional channel and the label. For task i, i ∈{0, 1, 2}, we deﬁne the
correlation as cor(i) and cor(1) = 0.9, cor(2) = 0.8, cor(3) = −0.9. Here, we deﬁne"
P,0.23024054982817868,"davg(i) =
 1"
P,0.2313860252004582,"2
P"
P,0.2325315005727377,"j̸=i cor(j) −cor(i)
, then davg(1) = 0.95, davg(2) = 0.8, davg(3) = 1.75.
Similarly, we can deﬁne a(i) for a given algorithm. The detailed results in Appendix D.2
imply that davg(i) is anticorrelated with a(i). To further understand this phenomenon, we
present another toy example for ColoredMNIST with a different data model in Appendix B."
CONCLUSION,0.23367697594501718,"6
Conclusion"
CONCLUSION,0.23482245131729668,"In this paper, we focus speciﬁcally on out-of-distribution adversarial robustness. First, we show that
existing OOD generalization algorithms are easily fooled by adversarial attacks. Motivated by this, we
then study the theory of the adversarial robustness of models in two different but complementary OOD
settings. Based on our theory, we propose two algorithms, AT and RDANN. Extensive experiments
show that our proposed algorithms can signiﬁcantly improve the OOD adversarial robustness of the
model."
CONCLUSION,0.23596792668957617,Acknowledgements
CONCLUSION,0.23711340206185566,"This work is supported by the National Natural Science Foundation of China under Grant 61976161,
the Fundamental Research Funds for the Central Universities under Grant 2042022rc0016."
REFERENCES,0.23825887743413515,References
REFERENCES,0.23940435280641467,"[1] Isabela Albuquerque, João Monteiro, Mohammad Darvishi, Tiago H. Falk, and Ioannis
Mitliagkas. Generalizing to unseen domains via distribution matching, 2021."
REFERENCES,0.24054982817869416,"[2] Kumail Alhamoud, Hasan Abed Al Kader Hammoud, Motasem Alfarra, and Bernard Ghanem.
Generalizability of adversarial robustness under distribution shifts. CoRR, abs/2209.15042,
2022."
REFERENCES,0.24169530355097366,"[3] Anonymous. Fairness and accuracy under domain generalization. In Submitted to The Eleventh
International Conference on Learning Representations, 2023. under review."
REFERENCES,0.24284077892325315,"[4] Martín Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini-
mization. CoRR, abs/1907.02893, 2019."
REFERENCES,0.24398625429553264,"[5] Pranjal Awasthi, Natalie Frank, and Mehryar Mohri. Adversarial learning guarantees for linear
hypotheses and neural networks. In ICML, volume 119, pages 431–441, 2020."
REFERENCES,0.24513172966781213,"[6] Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds
for neural networks. In NeurIPS, pages 6240–6249, 2017."
REFERENCES,0.24627720504009165,"[7] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research, 3:463–482, 2002."
REFERENCES,0.24742268041237114,"[8] Pouya Bashivan, Reza Bayat, Adam Ibrahim, Kartik Ahuja, Mojtaba Faramarzi, Touraj Laleh,
Blake A. Richards, and Irina Rish. Adversarial feature desensitization. In NeurIPS, pages
10665–10677, 2021."
REFERENCES,0.24856815578465064,"[9] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
nifer Wortman Vaughan. A theory of learning from different domains. Mach. Learn., 79(1-
2):151–175, 2010."
REFERENCES,0.24971363115693013,"[10] Gilles Blanchard, Aniket Anand Deshmukh, Ürün Dogan, Gyemin Lee, and Clayton Scott.
Domain generalization by marginal transfer learning. Journal of Machine Learning Research,
22:2:1–2:55, 2021."
REFERENCES,0.2508591065292096,"[11] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks.
In SP, pages 39–57, 2017."
REFERENCES,0.2520045819014891,"[12] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an
ensemble of diverse parameter-free attacks. In ICML, pages 2206–2216, 2020."
REFERENCES,0.2531500572737686,"[13] John C. Duchi and Hongseok Namkoong. Learning models with uniform performance via
distributionally robust optimization. CoRR, abs/1810.08750, 2018."
REFERENCES,0.2542955326460481,"[14] Cian Eastwood, Alexander Robey, Shashank Singh, Julius von Kügelgen, Hamed Hassani,
George J. Pappas, and Bernhard Schölkopf. Probable domain generalization via quantile risk
minimization. CoRR, abs/2207.09944, 2022."
REFERENCES,0.2554410080183276,"[15] Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased metric learning: On the utilization of
multiple datasets and web images for softening bias. In ICCV, pages 1657–1664, 2013."
REFERENCES,0.2565864833906071,"[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta-
tion of deep networks. In ICML, volume 70, pages 1126–1135, 2017."
REFERENCES,0.25773195876288657,"[17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François
Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural
networks. Journal of Machine Learning Research, 17:59:1–59:35, 2016."
REFERENCES,0.2588774341351661,"[18] Qingyi Gao and Xiao Wang. Theoretical investigation of generalization bounds for adversarial
learning of deep neural networks. Journal of Statistical Theory and Practice, 15:1–28, 2021."
REFERENCES,0.2600229095074456,"[19] Rui Gao and Anton J. Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. Mathematics if Operations Research, 48(2):603–655, 2023."
REFERENCES,0.2611683848797251,"[20] Vikas K. Garg, Adam Tauman Kalai, Katrina Ligett, and Zhiwei Steven Wu. Learn to expect
the unexpected: Probably approximately correct domain generalization. In AISTATS, volume
130, pages 3574–3582, 2021."
REFERENCES,0.2623138602520046,"[21] Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi.
Domain
generalization for object recognition with multi-task autoencoders. In ICCV, pages 2551–2559.
IEEE Computer Society, 2015."
REFERENCES,0.2634593356242841,"[22] Tejas Gokhale, Swaroop Mishra, Man Luo, Bhavdeep Singh Sachdeva, and Chitta Baral.
Generalized but not robust? comparing the effects of data modiﬁcation methods on out-of-
domain generalization and adversarial robustness. In ACL, pages 2705–2718, 2022."
REFERENCES,0.2646048109965636,"[23] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In COLT, volume 75, pages 297–299, 2018."
REFERENCES,0.26575028636884307,"[24] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, pages
2672–2680, 2014."
REFERENCES,0.26689576174112256,"[25] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-
sarial examples. In ICLR, 2015."
REFERENCES,0.26804123711340205,"[26] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021."
REFERENCES,0.26918671248568155,"[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, pages 770–778, 2016."
REFERENCES,0.27033218785796104,"[28] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-
domain generalization. In ECCV, volume 12347, pages 124–140, 2020."
REFERENCES,0.27147766323024053,"[29] Adam Ibrahim, Charles Guille-Escuret, Ioannis Mitliagkas, Irina Rish, David Krueger, and
Pouya Bashivan. Towards out-of-distribution adversarial robustness. CoRR, abs/2210.03150,
2022."
REFERENCES,0.27262313860252,"[30] Qiyu Kang, Yang Song, Qinxu Ding, and Wee Peng Tay. Stable neural ODE with lyapunov-
stable equilibrium points for defending against adversarial attacks. In NeurIPS, pages 14925–
14937, 2021."
REFERENCES,0.27376861397479957,"[31] Justin Khim and Po-Ling Loh. Adversarial risk bounds for binary classiﬁcation via function
transformation. CoRR, abs/1810.09519, 2018."
REFERENCES,0.27491408934707906,"[32] Hoki Kim. Torchattacks : A pytorch repository for adversarial attacks. CoRR, abs/2010.01950,
2020."
REFERENCES,0.27605956471935855,"[33] David Krueger, Ethan Caballero, Jörn-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai
Zhang, Rémi Le Priol, and Aaron C. Courville. Out-of-distribution generalization via risk
extrapolation (rex). In ICML, volume 139, pages 5815–5826, 2021."
REFERENCES,0.27720504009163804,"[34] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical
world. In ICLR, 2017."
REFERENCES,0.27835051546391754,"[35] Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/
mnist/, 1998."
REFERENCES,0.27949599083619703,"[36] Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In
NeurIPS, pages 2692–2701, 2018."
REFERENCES,0.2806414662084765,"[37] Boqi Li and Weiwei Liu. WAT: improve the worst-class robustness in adversarial training. In
AAAI, pages 14982–14990, 2023."
REFERENCES,0.281786941580756,"[38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier
domain generalization. In ICCV, pages 5543–5551, 2017."
REFERENCES,0.2829324169530355,"[39] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize:
Meta-learning for domain generalization. In AAAI, pages 3490–3497, 2018."
REFERENCES,0.284077892325315,"[40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with
adversarial feature learning. In CVPR, pages 5400–5409, 2018."
REFERENCES,0.2852233676975945,"[41] Xiyuan Li, Xin Zou, and Weiwei Liu. Defending against adversarial attacks via neural dynamic
system. In NeurIPS, 2022."
REFERENCES,0.286368843069874,"[42] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In ECCV, volume
11219, pages 647–663, 2018."
REFERENCES,0.28751431844215347,"[43] Xinsong Ma, Zekai Wang, and Weiwei Liu. On the tradeoff between robustness and fairness. In
NeurIPS, 2022."
REFERENCES,0.28865979381443296,"[44] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018."
REFERENCES,0.2898052691867125,"[45] Mazda Moayeri, Kiarash Banihashem, and Soheil Feizi. Explicit tradeoffs between adversarial
and natural distributional robustness. CoRR, abs/2209.07592, 2022."
REFERENCES,0.290950744558992,"[46] Omar Montasser, Steve Hanneke, and Nathan Srebro. VC classes are adversarially robustly
learnable, but only improperly. In COLT, pages 2512–2530, 2019."
REFERENCES,0.2920962199312715,"[47] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple
and accurate method to fool deep neural networks. In CVPR, pages 2574–2582, 2016."
REFERENCES,0.293241695303551,"[48] Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure
modes of out-of-distribution generalization. In ICLR, 2021."
REFERENCES,0.2943871706758305,"[49] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In COLT, volume 40, pages 1376–1401, 2015."
REFERENCES,0.29553264604810997,"[50] Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and
Timothy A. Mann.
Fixing data augmentation to improve adversarial robustness.
CoRR,
abs/2103.01946, 2021."
REFERENCES,0.29667812142038946,"[51] Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to
interpolation and extrapolation in domain generalization. In Gustau Camps-Valls, Francisco
J. R. Ruiz, and Isabel Valera, editors, AISTATS, volume 151, pages 2641–2657, 2022."
REFERENCES,0.29782359679266895,"[52] Yangjun Ruan, Yann Dubois, and Chris J. Maddison. Optimal representations for covariate shift.
In ICLR, 2022."
REFERENCES,0.29896907216494845,"[53] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally
robust neural networks for group shifts: On the importance of regularization for worst-case
generalization. CoRR, abs/1911.08731, 2019."
REFERENCES,0.30011454753722794,"[54] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do
adversarially robust imagenet models transfer better? In NeurIPS, 2020."
REFERENCES,0.30126002290950743,"[55] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning - From Theory to
Algorithms. Cambridge University Press, 2014."
REFERENCES,0.3024054982817869,"[56] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui.
Towards out-of-distribution generalization: A survey. CoRR, abs/2108.13624, 2021."
REFERENCES,0.3035509736540664,"[57] Sign function. Sign function, 2023. [Online; last edited on 16-January-2023]."
REFERENCES,0.30469644902634596,"[58] Baochen Sun and Kate Saenko. Deep CORAL: correlation alignment for deep domain adaptation.
In ECCV, volume 9915, pages 443–450, 2016."
REFERENCES,0.30584192439862545,"[59] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J.
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014."
REFERENCES,0.30698739977090495,"[60] Cuong Tran, Keyu Zhu, Ferdinando Fioretto, and Pascal Van Hentenryck. Fairness increases
adversarial vulnerability. CoRR, abs/2211.11835, 2022."
REFERENCES,0.30813287514318444,"[61] Vladimir Vapnik. Statistical learning theory. Wiley, 1998."
REFERENCES,0.30927835051546393,"[62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan.
Deep hashing network for unsupervised domain adaptation. In CVPR, pages 5385–5394, 2017."
REFERENCES,0.3104238258877434,"[63] Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridge University Press, 2019."
REFERENCES,0.3115693012600229,"[64] Qixun Wang, Yifei Wang, Hong Zhu, and Yisen Wang. Improving out-of-distribution general-
ization by adversarial training with structured priors. CoRR, abs/2210.06807, 2022."
REFERENCES,0.3127147766323024,"[65] Zekai Wang and Weiwei Liu. Robustness veriﬁcation for contrastive learning. In ICML, pages
22865–22883, 2022."
REFERENCES,0.3138602520045819,"[66] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion
models further improve adversarial training. In ICML, pages 36246–36263, 2023."
REFERENCES,0.3150057273768614,"[67] Jiancong Xiao, Yanbo Fan, Ruoyu Sun, and Zhi-Quan Luo. Adversarial rademacher complexity
of deep neural networks. CoRR, abs/2211.14966, 2022."
REFERENCES,0.3161512027491409,"[68] Shiji Xin, Yifei Wang, Jingtong Su, and Yisen Wang. Domain-wise adversarial training for
out-of-distribution generalization, 2022."
REFERENCES,0.3172966781214204,"[69] Jingyuan Xu and Weiwei Liu. On robust multiclass learnability. In NeurIPS, 2022."
REFERENCES,0.31844215349369986,"[70] Hanshu Yan, Jiawei Du, Vincent Y. F. Tan, and Jiashi Feng. On robustness of neural ordinary
differential equations. In ICLR. OpenReview.net, 2020."
REFERENCES,0.31958762886597936,"[71] Mingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, and Zhiming Ma.
Improved OOD generalization via adversarial training and pretraing. In ICML, pages 11987–
11997, 2021."
REFERENCES,0.3207331042382589,"[72] Dong Yin, Kannan Ramchandran, and Peter L. Bartlett. Rademacher complexity for adversari-
ally robust generalization. In ICML, volume 97, pages 7085–7094, 2019."
REFERENCES,0.3218785796105384,"[73] Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John E. Hopcroft, and Liwei Wang.
Adversarially robust generalization just requires more unlabeled data. CoRR, abs/1906.00555,
2019."
REFERENCES,0.3230240549828179,"[74] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I.
Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, volume 97,
pages 7472–7482, 2019."
REFERENCES,0.3241695303550974,"[75] Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan S.
Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In ICML,
volume 119, pages 11278–11287, 2020."
REFERENCES,0.32531500572737687,"[76] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I. Jordan. Bridging theory and
algorithm for domain adaptation. In ICML, volume 97, pages 7404–7413, 2019."
REFERENCES,0.32646048109965636,"[77] Zhengyu Zhou and Weiwei Liu. Sample complexity for distributionally robust learning under
chi-square divergence. Journal of Machine Learning Research, 24(230):1–27, 2023."
REFERENCES,0.32760595647193586,"[78] Xin Zou and Weiwei Liu. Generalization bounds for adversarial contrastive learning. Journal
of Machine Learning Research, 24:114:1–114:54, 2023."
REFERENCES,0.32875143184421535,"A
Proofs"
REFERENCES,0.32989690721649484,"In this section, we show the proofs of the results in the mainscript."
REFERENCES,0.33104238258877433,"A.1
Proofs of the Toy Example"
REFERENCES,0.3321878579610538,"Lemma A.1. If X
X
X ∼N(µµµ, Σ) where X
X
X ∼Rn, then for any A ∈Rm×n, we have:"
REFERENCES,0.3333333333333333,"AX
X
X ∼N(Aµµµ, AΣAT )"
REFERENCES,0.3344788087056128,"Proof of Lemma A.1. We prove the lemma by a powerful tool, the characteristic function of random
variables. The characteristic function of a random vector X
X
X is deﬁned as:"
REFERENCES,0.3356242840778923,"φX
X
X(ωωω) = E
h
eiωωωTX
X
Xi"
REFERENCES,0.33676975945017185,"Let YYY = AX
X
X, we have:"
REFERENCES,0.33791523482245134,"φYYY (ωωω) = E
h
eiωωωTYYY i
= E
h
eiωωωT (AX
X
X)i
= E
h
ei(ATωωω)TX
X
Xi
= φX
X
X(ATωωω)."
REFERENCES,0.33906071019473083,"Since X
X
X ∼N(µµµ, Σ), we have:"
REFERENCES,0.3402061855670103,"φX
X
X(ωωω) = E
h
eiωωωTX
X
Xi
= E
h
eiωωωTµµµ−1"
REFERENCES,0.3413516609392898,"2ωωωT Σωωωi
,"
REFERENCES,0.3424971363115693,so we have:
REFERENCES,0.3436426116838488,"φYYY (ωωω) = E
h
ei(ATωωω)Tµµµ−1"
REFERENCES,0.3447880870561283,"2 (ATωωω)T Σ(ATωωω)i
= E
h
eiωωωT (Aµµµ)−1"
REFERENCES,0.3459335624284078,"2ωωωT (AΣAT )ωωωi
."
REFERENCES,0.3470790378006873,"Since the characteristic function and the distribution is one-to-one, we have: YYY obeys the multi-
variable Gaussian distribution and YYY ∼N(Aµµµ, AΣAT )."
REFERENCES,0.34822451317296677,Lemma A.2. Let DY (Y = 1) = DY (Y = −1) = 1
REFERENCES,0.34936998854524626,"2, X ∈Rd and DX|Y =y = N(yµµµ, Σ) where
µµµ ∈Rd and Σ ∈Rd×d, then for any linear classiﬁer hw
ww ∈H:"
REFERENCES,0.35051546391752575,"R
Bϵ
p
D (ℓ01, hww
w) = Φ
ϵ∥www∥p∗−wwwTµµµ
√"
REFERENCES,0.3516609392898053,"wwwT Σwww 
,"
REFERENCES,0.3528064146620848,where 1
REFERENCES,0.3539518900343643,"p +
1
p∗= 1. Further more, if p = 2 and Σ = σ2I, then www⋆= arg min
ww
w:hw
w
w∈H
RBϵ
2
D (ℓ01, hww
w), the"
REFERENCES,0.3550973654066438,"weight of the most robust hww
w ∈H under adversarial attack Bϵ
2(·) is:"
REFERENCES,0.35624284077892326,www⋆= µµµ.
REFERENCES,0.35738831615120276,"Proof of Lemma A.2. Let hw
ww(xxx) = sign(wwwTxxx), then we have:"
REFERENCES,0.35853379152348225,"R
Bϵ
p
D (ℓ01, hw
ww) =
E
(xxx,y)∼D ("
REFERENCES,0.35967926689576174,"sup
xxx′∈Bϵp(xxx)
1 [hw
w
w(xxx) ̸= y] ) = 1"
E,0.36082474226804123,"2
E
xxx∼DX|Y =1 ("
E,0.3619702176403207,"sup
xxx′∈Bϵp(xxx)
1

wwwTxxx′ < 0

) + 1"
E,0.3631156930126002,"2
E
xxx∼DX|Y =−1 ("
E,0.3642611683848797,"sup
xxx′∈Bϵp(xxx)
1

wwwTxxx′ > 0

) = 1"
P,0.3654066437571592,"2
P
xxx∼DX|Y =1 ("
P,0.3665521191294387,"inf
xxx′∈Bϵp(xxx)wwwTxxx′ < 0 ) + 1"
P,0.36769759450171824,"2
P
xxx∼DX|Y =−1 ("
P,0.36884306987399773,"sup
xxx′∈Bϵp(xxx)
wwwTxxx′ > 0 ) = 1"
P,0.3699885452462772,"2
P
xxx∼DX|Y =1"
P,0.3711340206185567,"
wwwTxxx −ϵ∥www∥p∗< 0
	
+ 1"
P,0.3722794959908362,"2
P
xxx∼DX|Y =−1"
P,0.3734249713631157,"
wwwTxxx + ϵ∥www∥p∗> 0 a= 1"
P,0.3745704467353952,"2P

N(wwwTµµµ,wwwT Σwww) < ϵ∥www∥p∗
+ 1"
P,0.3757159221076747,"2P

N(−wwwTµµµ,wwwT Σwww) > −ϵ∥www∥p∗"
P,0.3768613974799542,"= P

N(wwwTµµµ,wwwT Σwww) < ϵ∥www∥p∗
= Φ
ϵ∥www∥p∗−wwwTµµµ
√"
P,0.37800687285223367,"wwwT Σwww 
,"
P,0.37915234822451316,where a is the result of Lemma A.1 when we regard wwwT as A in Lemma A.1.
P,0.38029782359679265,"If p = 2 and Σ = σ2I, then we have:"
P,0.38144329896907214,"RBϵ
2
D (ℓ01, hww
w) = Φ
ϵ∥www∥2 −wwwTµµµ"
P,0.38258877434135163,σ∥www∥2
P,0.3837342497136312,"
= Φ
 ϵ"
P,0.3848797250859107,σ −wwwTµµµ
P,0.38602520045819017,"σ∥www∥2 
."
P,0.38717067583046966,"Since Φ(·) is increasing, we need to minimize ϵ"
P,0.38831615120274915,"σ −
w
wwTµµµ
σ∥w
w
w∥2 , i.e., to maximize
w
wwTµµµ
σ∥w
w
w∥2 , it’s easy to see
that the maximum is achieved if we choose www = µµµ, so we have www⋆= µµµ."
P,0.38946162657502864,"Lemma A.3. Let D1, D2 to be deﬁned as: D1,Y (Y = 1) = D1,Y (Y = −1) = D2,Y (Y = −1) =
D2,Y (Y = −1) 1"
P,0.39060710194730813,"2, X ∈Rd and D1,X|Y =y = N(yµµµ1, σ2I), D2,X|Y =y = N(yµµµ2, σ2I) where
µµµ1,µµµ2 ∈Rd, ∥µµµ1∥2 = ∥µµµ2∥2 and σ ∈R+, let D = 1"
P,0.3917525773195876,2D1 + 1
P,0.3928980526918671,"2D2 and suppose α is the angle between
µµµ1 and µµµ2, if 0 < α ≤2 arccos
ϵ
∥µµµ1∥2 , then www⋆= µµµ1 + µµµ2 achieves the minimal robust risk."
P,0.3940435280641466,"Proof of Lemma A.3. To simplify the notation, we denote RBϵ
2
D (ℓ01, hw
ww) by R(www). Then by Lemma
A.2 we have:"
P,0.3951890034364261,R(www) = 1
P,0.3963344788087056,"2RBϵ
2
D1(ℓ01, hw
ww) + 1"
P,0.3974799541809851,"2RBϵ
2
D2(ℓ01, hww
w) = 1"
P,0.39862542955326463,"2Φ
ϵ∥www∥2 −wwwTµµµ1"
P,0.3997709049255441,"σ∥www∥2 
+ 1"
P,0.4009163802978236,"2Φ
ϵ∥www∥2 −wwwTµµµ2"
P,0.4020618556701031,"σ∥www∥2 
,"
P,0.4032073310423826,"where Φ(u) =
1
√"
P,0.4043528064146621,"2π
R u
−∞e−y2"
P,0.4054982817869416,"2 dy. The proof is divided into two steps. In the ﬁrst step, we prove that
for any weight www, we have R(µµµP ) ≤R(µµµ), where µµµP is the project of µµµ onto the space span ⟨µµµ1,µµµ2⟩.
Then we can reduce our analysis to the weights in the space span ⟨µµµ1,µµµ2⟩."
P,0.4066437571592211,"Step 1. For any www ∈Rd, we take a direct sum decomposition as www = wwws + wwwt where wwws ∈
span ⟨µµµ1,µµµ2⟩and wwwt ∈span ⟨µµµ1,µµµ2⟩⊥, the orthogonal complement space of span ⟨µµµ1,µµµ2⟩, then we
have:
wwwTµµµ1 = wwwT
s µµµ1 + wwwT
t µµµ1 = wwwT
s µµµ1"
P,0.40778923253150057,"Similarly, we have wwwTµµµ2 = wwwT
s µµµ2. Since wwws ⊥wwwt, then we have ∥www∥2
2 = ∥wwws∥2
2 + ∥wwwt∥2
2, so for
any www ∈Rd, we have:"
P,0.40893470790378006,R(www) = 1
P,0.41008018327605955,"2Φ
 ϵ"
P,0.41122565864833904,σ −wwwTµµµ1
P,0.41237113402061853,"σ∥www∥2 
+ 1"
P,0.413516609392898,"2Φ
 ϵ"
P,0.4146620847651776,σ −wwwTµµµ2
P,0.41580756013745707,σ∥www∥2  = 1 2Φ
P,0.41695303550973656,"ϵ
σ −
wwwT
s µµµ1
σ
p"
P,0.41809851088201605,"∥wwws∥2
2 + ∥wwwt∥2
2 ! + 1 2Φ"
P,0.41924398625429554,"ϵ
σ −
wwwT
s µµµ2
σ
p"
P,0.42038946162657503,"∥wwws∥2
2 + ∥wwwt∥2
2 !"
P,0.4215349369988545,So it is obvious that R(wwws) ≤R(www).
P,0.422680412371134,"Step 2. According to the above results, we consider www ∈span ⟨µµµ1,µµµ2⟩. Suppose the angle between
µµµ1,µµµ2 is α, note that R(www) does not depend on the length of www, so without loss of generality, we
assume ∥www∥2 = 1 here. Suppose the angle between www and µµµ1 is θ, then the angle between www and µµµ2
is α −θ, then www is uniquely determined by θ, we denote R(θ) = R(www), then we have:"
P,0.4238258877434135,R(θ) = 1
P,0.424971363115693,"2Φ
 ϵ"
P,0.4261168384879725,"σ −∥µµµ1∥2cos θ σ 
+ 1"
P,0.427262313860252,"2Φ
 ϵ"
P,0.4284077892325315,σ −∥µµµ1∥2cos(α −θ) σ 
P,0.42955326460481097,"By the rule of derivation for composition function, we have:"
P,0.4306987399770905,"R′(θ) =
1
2
√"
P,0.43184421534937,"2π exp

−(ϵ −∥µµµ1∥2 cos θ)2 2σ2"
P,0.4329896907216495, ∥µµµ1∥2
P,0.434135166093929,"σ
sin θ"
P,0.4352806414662085,"−
1
2
√"
P,0.436426116838488,"2π exp

−(ϵ −∥µµµ1∥2 cos(α −θ))2 2σ2"
P,0.43757159221076747, ∥µµµ1∥2
P,0.43871706758304696,"σ
sin(α −θ)"
P,0.43986254295532645,"= ∥µµµ1∥2 2
√ 2πσ"
P,0.44100801832760594,"
exp

−(ϵ −∥µµµ1∥2 cos θ)2 2σ2"
P,0.44215349369988544,"
sin θ"
P,0.44329896907216493,"−exp

−(ϵ −∥µµµ1∥2 cos(α −θ))2 2σ2"
P,0.4444444444444444,"
sin(α −θ)

."
P,0.44558991981672397,It is easy to see that when θ = α
P,0.44673539518900346,2 or θ = α
P,0.44788087056128295,"2 + π, R′(θ) = 0, since: R′(α"
P,0.44902634593356244,"2 ) = ∥µµµ1∥2 2
√ 2πσ"
P,0.45017182130584193,"
exp

−(ϵ −∥µµµ1∥2 cos α 2 )2 2σ2"
P,0.4513172966781214,"
sin α"
P,0.4524627720504009,"2 −exp

−(ϵ −∥µµµ1∥2 cos α 2 )2 2σ2"
P,0.4536082474226804,"
sin α 2  = 0"
P,0.4547537227949599,Similarly we have R′( α
P,0.4558991981672394,2 + π) = 0. Next we prove R(θ) achieves the minimum at θ = α
P,0.4570446735395189,"2 , taking the
second order derivation of θ, we have: 2
√"
P,0.4581901489117984,"2πσ
∥µµµ1∥2
R′′(θ) = exp

−(ϵ −∥µµµ1∥2 cos θ)2 2σ2"
P,0.45933562428407787,"
·

−∥µµµ1∥2 (ϵ −∥µµµ1∥2 cos θ)sin2θ σ2 "
P,0.46048109965635736,"+ exp

−(ϵ −∥µµµ1∥2 cos θ)2 2σ2"
P,0.4616265750286369,"
cos θ"
P,0.4627720504009164,"−exp

−(ϵ −∥µµµ1∥2 cos(α −θ))2 2σ2"
P,0.4639175257731959,"
·
∥µµµ1∥2(ϵ −∥µµµ1∥2 cos(α −θ))sin2(α −θ) σ2 "
P,0.4650630011454754,"+ exp

−(ϵ −∥µµµ1∥2 cos(α −θ))2 2σ2"
P,0.4662084765177549,"
cos(α −θ)"
P,0.46735395189003437,"= exp

−(ϵ −∥µµµ1∥2 cos θ)2 2σ2"
P,0.46849942726231386," 
cos θ −∥µµµ1∥2 (ϵ −∥µµµ1∥2 cos θ)sin2θ σ2 "
P,0.46964490263459335,"+ exp

−(ϵ −∥µµµ1∥2 cos(α −θ))2 2σ2"
P,0.47079037800687284," 
cos(α −θ)−∥µµµ1∥2(ϵ −∥µµµ1∥2 cos(α −θ))sin2(α −θ) σ2 "
P,0.47193585337915234,"Then we have: 2
√"
P,0.47308132875143183,"2πσ
∥µµµ1∥2
R′′(α"
P,0.4742268041237113,"2 ) = exp

−(ϵ −∥µµµ1∥2 cos α 2 )2 2σ2  cos α"
P,0.4753722794959908,2 −∥µµµ1∥2 (ϵ −∥µµµ1∥2 cos α
P,0.4765177548682703,"2 )sin2 α 2
σ2 !"
P,0.47766323024054985,"+ exp

−(ϵ −∥µµµ1∥2 cos α 2 )2 2σ2  cos α"
P,0.47880870561282934,2 −∥µµµ1∥2(ϵ −∥µµµ1∥2 cos α
P,0.47995418098510884,"2 )sin2 α 2
σ2 ! = 2"
P,0.48109965635738833,"σ2 exp

−(ϵ −∥µµµ1∥2 cos α 2 )2 2σ2"
P,0.4822451317296678," 
σ2cosα"
P,0.4833906071019473,2 −∥µµµ1∥2(ϵ −∥µµµ1∥2 cos α
P,0.4845360824742268,2 )sin2 α 2 
P,0.4856815578465063,"Now, for simplicity, let k = ∥µµµ1∥2, x = cos α"
P,0.4868270332187858,"2 , then sin2 α"
P,0.4879725085910653,"2 = 1 −x2, then we consider f(x) =
σ2x−k(ϵ−kx)(1−x2) = σ2cos α"
P,0.48911798396334477,2 −∥µµµ1∥2(ϵ−∥µµµ1∥2 cos α
P,0.49026345933562426,2 )sin2 α
P,0.49140893470790376,"2 , where α ∈(0, π), so x ∈(0, 1).
We have:
f(x) = σ2x −k(ϵ −kx)(1 −x2) = σ2x −kϵ + k2x + kϵx2 −k2x3"
P,0.4925544100801833,= k2(x −x3) −kϵ(1 −x2) + σ2x = k2x(1 −x2) −kϵ(1 −x2) + σ2x
P,0.4936998854524628,= k(1 −x2)(kx −ϵ) + σ2x.
P,0.4948453608247423,"Since x ∈(0, 1), then σ2x > 0 and k(1 −x2) > 0, by the assumption, 0 < α ≤2 arccos
ϵ
∥µµµ1∥2 =
2 arccos ϵ"
P,0.4959908361970218,"k, so x = cos α 2 ≥ϵ"
P,0.49713631156930127,"k, so kx −ϵ ≥0, so we have f(x) > 0, which means that R′′( α"
P,0.49828178694158076,"2 ) > 0,
so we know that θ = α"
P,0.49942726231386025,"2 achieves the minimum of R(θ), which means that the direction www⋆= µµµ1 +µµµ2
achieves the minimum of R(www). Similarly we can show that R(θ) achieves its maximum at θ = α"
P,0.5005727376861397,"2 +π,
which means that the direction −(µµµ1 + µµµ2) achieves the maximum of R(www) in span ⟨µµµ1,µµµ2⟩."
P,0.5017182130584192,"Theorem 3.4. Consider the setting in Example 1 and suppose that µµµ lies in the 2-dimensional
subspace R in Theorem A.4 (see Appendix A.1 for details about R and see Figure 1 for intuitive
illustration). Let ℓ01(x, y) = 1[x ̸= y], where 1[·] is the indicator function and Br
p(xxx) = {xxx′ :
∥xxx −xxx′∥p ≤r}, consider training with hypothesis class H = {hww
w : hww
w(x) = sign(wwwT x),www ∈Rd},
and denote D(ij) = 1"
P,0.5028636884306987,2D(i) + 1
P,0.5040091638029782,"2D(j), i, j ∈{0, 1, 2}, i ̸= j. Consider the ℓ2-norm adversarial attack"
P,0.5051546391752577,"with radius ϵ, for notation convenience, let eRij(www) = RBϵ
2
D(ij)(ℓ01, hww
w) and eRi(www) = RBϵ
2
D(i)(ℓ01, hww
w),
let Φ(·) be the distribution function of the standard normal distribution, let α denote the angle between
µµµ and Qµµµ, which is the rotation angle of Q in subspace R, suppose that 0 < α ≤arccos
ϵ
∥µµµ1∥2 , then
we have:"
P,0.5063001145475372,"1. If we train with D(0) and D(1) under H, let www(01) = µµµ + Qµµµ, which achieves the minimum
of eR01(www), then the robust accuracy of www(01) on the test distribution D(2) is:"
P,0.5074455899198167,eR2(www(01)) = Φ
P,0.5085910652920962,"ϵ
σ −(µµµ + Qµµµ)T Q2µµµ"
P,0.5097365406643757,σ∥µµµ + Qµµµ∥2 ! .
P,0.5108820160366552,"2. If we train with D(0) and D(2) under H, let www(02) = µµµ + Q2µµµ, which achieves the minimum
of eR02(www), then the robust accuracy of www(02) on the test distribution D(1) is:"
P,0.5120274914089347,eR1(www(02)) = Φ
P,0.5131729667812142,"ϵ
σ −(µµµ + Q2µµµ)T Qµµµ"
P,0.5143184421534936,σ∥µµµ + Q2µµµ∥2 ! .
P,0.5154639175257731,"3. If we train with D(1) and D(2) under H, then www(12) = Qµµµ + Q2µµµ, which achieves the
minimum of eR12(www), then the robust accuracy of www(12) on the test distribution D(0) is:"
P,0.5166093928980527,eR0(www(12)) = Φ
P,0.5177548682703322,"ϵ
σ −(Qµµµ + Q2µµµ)Tµµµ"
P,0.5189003436426117,σ∥Qµµµ + Q2µµµ∥2 ! .
P,0.5200458190148912,"What’s more, we have eR1(www(02)) < eR2(www(01)) = eR0(www(12))."
P,0.5211912943871707,"Proof of Theorem 3.4. The values of eR0(www(12)), eR1(www(02)), eR2(www(01)) is a direct result of Lemma
A.2 and Lemma A.3. Next we compare the three adversarial risks."
P,0.5223367697594502,"Let C(01) =
(µµµ+Qµµµ)T Q2µµµ"
P,0.5234822451317297,"σ∥µµµ+Qµµµ∥2 , C(02) =
(µµµ+Q2µµµ)T Qµµµ"
P,0.5246277205040092,"σ∥µµµ+Q2µµµ∥2 , C(12) =
(Qµµµ+Q2µµµ)Tµµµ
σ∥Qµµµ+Q2µµµ∥2 , i.e., the subtractor in"
P,0.5257731958762887,"eR2(www(01)), eR1(www(02)), eR0(www(12)) respectively. Since Φ(u) =
1
√"
P,0.5269186712485682,"2π
R u
−∞e−y2"
DU IS MONOTONELY,0.5280641466208477,"2 du is monotonely
increasing, to judge which of the three robust errors in Theorem 3.4 is the smallest, we need only to
judge which of the corresponding C(ij) is the largest."
DU IS MONOTONELY,0.5292096219931272,"Firstly, let Qe be the matrix of Q under the orthonormal basis eee1, · · · ,eeed where (eeei)j = δij and δij ="
DU IS MONOTONELY,0.5303550973654066,"1 if i = j, δij = 0 otherwise. Then we have Qµµµ = Qeµµµ and we can see that C(12) = (Qeµµµ+Q2
eµµµ)Tµµµ
σ∥Qeµµµ+Q2eµµµ∥2 ="
DU IS MONOTONELY,0.5315005727376861,"µµµT QT
e µµµ+µµµT QT
e QT
e µµµ
σ∥Qe(µµµ+Qeµµµ)∥2
a= µµµT Qeµµµ+µµµT Q2
eµµµ
σ∥µµµ+Qeµµµ∥2
= µµµT QT
e QeQeµµµ+µµµT Q2
eµµµ
σ∥µµµ+Qeµµµ∥2
= (µµµ+Qeµµµ)T Q2
eµµµ
σ∥µµµ+Qeµµµ∥2
= C(01) (where a is"
DU IS MONOTONELY,0.5326460481099656,"from the fact that ∥Qexxx∥2 = ∥xxx∥2, ∀xxx and xxxT Axxx = xxxT ATxxx, ∀xxx, A). So eR2(w(01)) = eR0(w(12))."
DU IS MONOTONELY,0.5337915234822451,"To compare which one of eR1(www(02)) and eR2(www(01)) is smaller, we note that µµµ lies in the 2-
dimensional subspace R ∈Rd in Theorem A.4 as speciﬁed in Theorem 3.4, so there exists a
scalar k ∈R such that µµµ+Q2µµµ = k Qµµµ, and we know that Qµµµ ∈arg min
w
w
w:hw
w
w∈H
eR02(www) (by Lemma A.2),"
DU IS MONOTONELY,0.5349369988545246,"so we know that eR1(www(02)) ≤eR2(www(01)) = eR0(www(12)) since
inf
w
ww:hw
w
w∈H
eR0(www) =
inf
ww
w:hw
w
w∈H
eR1(www) ="
DU IS MONOTONELY,0.5360824742268041,"inf
ww
w:hw
w
w∈H
eR2(www) (which is obvious according to Lemma A.2). And following the same direction"
DU IS MONOTONELY,0.5372279495990836,"of the proof of Theorem A.4, it’s easy to see that www(01) = µµµ + Qµµµ is not in the same direction of
Q2µµµ (the weight attains smallest robust error on D(2)), so eR2(www(01)) >
inf
w
ww:hw
w
w∈H
eR2(www), so we have"
DU IS MONOTONELY,0.5383734249713631,eR1(www(02)) < eR2(www(01)) = eR0(www(12)).
DU IS MONOTONELY,0.5395189003436426,"Theorem A.4. Let Q be a rotation transformation on Rd and Q ̸= I, then there exists at least one
2-dimensional subspace R ∈Rd such that there exists a scalar k s.t. rrr + Q2rrr = krrr for any rrr ∈R."
DU IS MONOTONELY,0.5406643757159221,"Proof of Theorem A.4. By the results of matrix theory, we know that since Q is an orthonormal
transformation, then there exists an orthonormal basis ηηη1, . . . .ηηηd under which the corresponding
matrix of Q, Q, has the form:"
DU IS MONOTONELY,0.5418098510882016,"Q = diag {R1, . . . , Rm, λ1, . . . , λr} ,"
DU IS MONOTONELY,0.5429553264604811,"where λi = 1 or −1, i = 1, 2, · · · , r, 0 ≤r ≤d; Rj =

cos θj
−sin θj
sin θj
cos θj"
DU IS MONOTONELY,0.5441008018327605,"
, 0 < θj < π, j ="
DU IS MONOTONELY,0.54524627720504,"1, 2, · · · , m, 0 ≤m ≤d 2."
DU IS MONOTONELY,0.5463917525773195,"Since Q is a rotation transformation, Q is a rotation matrix, so λ1 = · · · = λr = 1. Once given the
basis, linear transformations on Rd and matrices in Rd×d has a one-to-one correspondence, so Q ̸= I
tells us that Q ̸= I, where I is the matrix of I under the orthonormal basis ηηη1, . . . .ηηηd. So we have
m ≥1 in our case, without loss of generality we consider the subspace R = span ⟨ηηη1,ηηη2⟩be the
subspace spanned by the vectors {ηηη1,ηηη2}."
DU IS MONOTONELY,0.5475372279495991,"For any ααα ∈R, the coordinates of ααα under the basis ηηη1, . . . .ηηηd is then [α1, α2, 0, · · · , 0]T and
the coordinates of Qααα, Q2ααα is Q[α1, α2, 0, · · · , 0]T , Q2[α1, α2, 0, · · · , 0]T respectively. We now
prove that there exists k = 2cos θ1 such that [α1, α2, 0, · · · , 0]T + Q2[α1, α2, 0, · · · , 0]T =
kQ[α1, α2, 0, · · · , 0]T . We have: Q  "
DU IS MONOTONELY,0.5486827033218786,"α1
α2
0
...
0 "
DU IS MONOTONELY,0.5498281786941581,"
=   R1"
DU IS MONOTONELY,0.5509736540664376,"
α1
α2 "
DU IS MONOTONELY,0.5521191294387171,"0
...
0 "
DU IS MONOTONELY,0.5532646048109966,"
=  "
DU IS MONOTONELY,0.5544100801832761,"α1 cos θ1 −α2 sin θ1
α1 sin θ1 + α2 cos θ1
0
...
0 "
DU IS MONOTONELY,0.5555555555555556,"
, and Q2  "
DU IS MONOTONELY,0.5567010309278351,"α1
α2
0
...
0 "
DU IS MONOTONELY,0.5578465063001146,"
=   R2
1"
DU IS MONOTONELY,0.5589919816723941,"
α1
α2 "
DU IS MONOTONELY,0.5601374570446735,"0
...
0 "
DU IS MONOTONELY,0.561282932416953,"
=  "
DU IS MONOTONELY,0.5624284077892325,"α1 cos 2θ1 −α2 sin 2θ1
α1 sin 2θ1 + α2 cos 2θ1
0
...
0 "
DU IS MONOTONELY,0.563573883161512,"
."
DU IS MONOTONELY,0.5647193585337915,"So the coordinates of ααα + Q2ααα is:
 "
DU IS MONOTONELY,0.565864833906071,"α1
α2
0
...
0 "
DU IS MONOTONELY,0.5670103092783505,"
+ Q2  "
DU IS MONOTONELY,0.56815578465063,"α1
α2
0
...
0 "
DU IS MONOTONELY,0.5693012600229095,"
=  "
DU IS MONOTONELY,0.570446735395189,"α1 (cos 2θ1 + 1) −α2 sin 2θ1
α1 sin 2θ1 + α2 (cos 2θ1 + 1)
0
...
0 "
DU IS MONOTONELY,0.5715922107674685,"
=  "
DU IS MONOTONELY,0.572737686139748,"2α1 cos2 θ1 −2α2 sin θ1 cos θ1
2α2 sin θ1 cos θ1 + 2α1 cos2 θ1
0
...
0  "
DU IS MONOTONELY,0.5738831615120275,= 2cos θ1  
DU IS MONOTONELY,0.5750286368843069,"α1 cos θ1 −α2 sin θ1
α1 sin θ1 + α2 cos θ1
0
...
0 "
DU IS MONOTONELY,0.5761741122565864,"
= k  "
DU IS MONOTONELY,0.5773195876288659,"α1 cos θ1 −α2 sin θ1
α1 sin θ1 + α2 cos θ1
0
...
0 "
DU IS MONOTONELY,0.5784650630011455,"
,"
DU IS MONOTONELY,0.579610538373425,which is just k times of the coordinates of Qααα. So we get ααα + Q2ααα = kQααα for any ααα ∈R.
DU IS MONOTONELY,0.5807560137457045,"A.2
Proofs of the Average Case"
DU IS MONOTONELY,0.581901489117984,"Theorem 3.1. Suppose the loss function is bounded, i.e., ℓ∈[0, U], then with probability at least
1 −δ over the sampling of bD1, · · · , bDt, for all h ∈H, we have:"
DU IS MONOTONELY,0.5830469644902635,"LB
p (ℓ, h) ≤LB
b
D(ℓ, h) + 2Rt( eG) + 2Rtn( eG) + 3U r ln4/δ"
T,0.584192439862543,"2t
+ 3U r ln4/δ 2tn , where eG = ("
T,0.5853379152348225,"gh : X × Y −→R+
gh(xxx, y) =
sup
xxx′∈B(xxx)
ℓ(h(xxx′, y)), h ∈H ) ."
T,0.586483390607102,"Proof of Theorem 3.1. Our proof consists of 2 steps, our 2-step uniform convergence analysis is
different from (but based on) the standard Rademacher complexity generalization error bounds."
T,0.5876288659793815,"Step 1, upper bound the difference between LB
p (ℓ, h) and LB
ˆp (ℓ, h) for any h ∈H."
T,0.588774341351661,"Let D = {D1, · · · , Dt}, we deﬁne:"
T,0.5899198167239404,"ΦB(D) = sup
h∈H"
T,0.5910652920962199,"
LB
p (ℓ, h) −LB
ˆp (ℓ, h)

."
T,0.5922107674684994,"Since ℓ∈[0, U], changing one of the Di in D will lead to at most U"
T,0.5933562428407789,"t change in ΦB(D). So by
McDiarmid’s inequality we have:"
T,0.5945017182130584,"P

ΦB(D) −E(ΦB(D))

≤exp

−2tϵ2 U 2 
,"
T,0.5956471935853379,so with probability at least 1 −δ
T,0.5967926689576174,"4 over the choice of D, we have:"
T,0.5979381443298969,ΦB(D) ≤E(ΦB(D)) + U r ln4/δ
T,0.5990836197021764,"2t
.
(A.1)"
T,0.6002290950744559,"Then we give an upper bound for E(ΦB(D)), by the symmetric technique, let S = {S1, · · · , St}
be a set of independent copy of D, we have:"
T,0.6013745704467354,"E(ΦB(D)) =
E
D∼pt"
T,0.6025200458190149,"
sup
h∈H"
T,0.6036655211912944," 
LB
p (ℓ, h) −LB
ˆp (ℓ, h)
"
T,0.6048109965635738,"=
E
D∼pt """
T,0.6059564719358533,"sup
h∈H "
T,0.6071019473081328,"E
S∼pRB
S(ℓ, h) −1 t t
X"
T,0.6082474226804123,"i=1
RB
Di(ℓ, h) !#"
T,0.6093928980526919,"=
E
D∼pt """
T,0.6105383734249714,"sup
h∈H 1 t t
X"
T,0.6116838487972509,"i=1
E
Si∼p
 
RB
Si(ℓ, h) −RB
Di(ℓ, h)

#"
T,0.6128293241695304,"a
≤
E
D∼pt """
T,0.6139747995418099,"E
S∼ptsup
h∈H 1 t t
X i=1"
T,0.6151202749140894," 
RB
Si(ℓ, h) −RB
Di(ℓ, h)

#"
T,0.6162657502863689,"b=
E
D∼pt,S∼ptE
σσσ """
T,0.6174112256586484,"sup
h∈H 1 t t
X"
T,0.6185567010309279,"i=1
σi
 
RB
Si(ℓ, h) −RB
Di(ℓ, h)

#"
T,0.6197021764032073,"c
≤2
E
D∼ptE
σσσ """
T,0.6208476517754868,"sup
h∈H 1 t t
X"
T,0.6219931271477663,"i=1
σiRB
Di(ℓ, h) #"
T,0.6231386025200458,"= 2
E
D∼ptE
σσσ """
T,0.6242840778923253,"sup
h∈H 1 t t
X"
T,0.6254295532646048,"i=1
σi
E
(xxxi,yi)∼Di """
T,0.6265750286368843,"sup
xxx′
i∈B(xxxi)
ℓ(h(xxx′
i), yi) ##"
T,0.6277205040091638,"d
≤2
E
D∼pt
E
(xxxi,yi)∼DiE
σσσ """
T,0.6288659793814433,"sup
h∈H 1 t t
X"
T,0.6300114547537228,"i=1
σi """
T,0.6311569301260023,"sup
xxx′
i∈B(xxxi)
ℓ(h(xxx′
i), yi) ##"
T,0.6323024054982818,"= 2
E
D∼pt
E
(xxxi,yi)∼Di"
T,0.6334478808705613,"h
Rt( eG)
i
,"
T,0.6345933562428407,"where:
a uses Jensen’s inequality;
b comes from the fact that the random variables
σi
 
RB
Si(ℓ, h) −RB
Di(ℓ, h)

and RB
Si(ℓ, h) −RB
Di(ℓ, h) are identically distributed, where σi ∼
Uniform({±1}) and σσσ ∈{±1}t has independent elements σ1, · · · , σt; c is from the property of
sup that sup(a + b) ≤sup(a) + sup(b) and the fact that σσσ has the same distribution as −σσσ; d uses
Jensen’s inequality."
T,0.6357388316151202,"Since ℓ∈[0, U], change in one data point (xxxi, yi) will cause at most U"
T,0.6368843069873997,"t difference in Rt( eG), we then
use McDiarmid’s inequality to get: with probability at least 1 −δ 4,"
T,0.6380297823596792,"E
D∼pt
E
(xxxi,yi)∼Di"
T,0.6391752577319587,"h
Rt( eG)
i
≤Rt( eG) + U r ln4/δ"
T,0.6403207331042382,"2t
.
(A.2)"
T,0.6414662084765178,"Combine (A.1) with (A.2) we get: with probability at least 1 −δ 2,"
T,0.6426116838487973,ΦB(D) ≤2Rt( eG) + 3U r ln4/δ
T,0.6437571592210768,"2t
.
(A.3)"
T,0.6449026345933563,"Step 2, upper bound the difference between LB
ˆp (ℓ, h) and LB
b
D(ℓ, h) for any h ∈H."
T,0.6460481099656358,"LB
ˆp (ℓ, h) −LB
b
D(ℓ, h) = 1 t t
X i=1"
T,0.6471935853379153,"
RB
Di(ℓ, h) −RB
b
Di(ℓ, h)

."
T,0.6483390607101948,Let bD = bDi ∪· · · ∪bDt and then deﬁne:
T,0.6494845360824743,"ΨB(bD) = sup
h∈H"
T,0.6506300114547537,"
LB
ˆp (ℓ, h) −LB
b
D(ℓ, h)

= sup
h∈H 1 t t
X i=1"
T,0.6517754868270332,"
RB
Di(ℓ, h) −RB
b
Di(ℓ, h)

."
T,0.6529209621993127,"Since ℓ∈[0, U] and there are tn examples in bD, changing the j-th example of bDi, (xxxij, yij), will
cause at most U"
T,0.6540664375715922,"tn change ΨB(bD). So by McDiarmid’s inequality we know: with probability at least
1 −δ"
T,0.6552119129438717,"4 over the choice of bD,"
T,0.6563573883161512,"ΨB(bD) ≤E
h
ΨB(bD)
i
+ U r ln4/δ"
T,0.6575028636884307,"2tn .
(A.4)"
T,0.6586483390607102,"Next we upper bound E
h
ΨB(bD)
i
, by the symmetric technique, let bS = { bS1, · · · , bSt} be a set of"
T,0.6597938144329897,"independent copies of bD and let (˜xxxij, ˜yij) be the j-th example in bSi, we have:"
T,0.6609392898052692,"E
h
ΨB(bD)
i
= E
b
D """
T,0.6620847651775487,"sup
h∈H 1 t t
X i=1"
T,0.6632302405498282,"
RB
Di(ℓ, h) −RB
b
Di(ℓ, h)
#"
T,0.6643757159221076,"= E
b
D 
"
T,0.6655211912943871,"sup
h∈H 1 t t
X i=1 "
T,0.6666666666666666,"
E
(˜xxxi,˜yi)∼Di "
T,0.6678121420389461,"sup
˜xxx′
i∈B(˜xxxi)
ℓ(h(˜xxx′
i), ˜yi) ! −1 n n
X"
T,0.6689576174112256,"j=1
sup
xxx′
ij∈B(xxxij)
ℓ(h(xxx′
ij), yij)   
 "
T,0.6701030927835051,"= E
b
D 
"
T,0.6712485681557846,"sup
h∈H 1 t t
X i=1"
N,0.6723940435280642,"1
n n
X j=1 """
N,0.6735395189003437,"E
(˜xxxij,˜yij)∼b
Si "
N,0.6746849942726232,"sup
˜xxx′
ij∈B(˜xxxij)
ℓ(h(˜xxx′
ij), ˜yij)−sup
xxx′
ij∈B(xxxij)
ℓ(h(xxx′
ij), yij) !#
 "
N,0.6758304696449027,"a
≤E
b
D, b
S 
"
N,0.6769759450171822,"sup
h∈H 1 t t
X i=1"
N,0.6781214203894617,"1
n n
X j=1 """
N,0.6792668957617412,"sup
˜xxx′
ij∈B(˜xxxij)
ℓ(h(˜xxx′
ij), ˜yij) −
sup
xxx′
ij∈B(xxxij)
ℓ(h(xxx′
ij), yij) #
 "
N,0.6804123711340206,"b
≤E
b
D, b
S
E
ΣΣΣ 
"
N,0.6815578465063001,"sup
h∈H 1 t t
X i=1"
N,0.6827033218785796,"1
n n
X j=1 "" Σij "
N,0.6838487972508591,"sup
˜xxx′
ij∈B(˜xxxij)
ℓ(h(˜xxx′
ij), ˜yij)−
sup
xxx′
ij∈B(xxxij)
ℓ(h(xxx′
ij), yij) !#
 "
N,0.6849942726231386,"c
≤2 E
b
D,ΣΣΣ 
"
N,0.6861397479954181,"sup
h∈H 1 t t
X i=1"
N,0.6872852233676976,"1
n n
X j=1 """
N,0.6884306987399771,"Σij
sup
xxx′
ij∈B(xxxij)
ℓ(h(xxx′
ij), yij) #
 "
N,0.6895761741122566,"d= 2 E
b
D"
N,0.6907216494845361,"h
Rtn( eG)
i
,"
N,0.6918671248568156,"where:
a
is
from
Jensen’s
inequality;
b
is
from
the
fact
that Σij "
N,0.693012600229095,"sup
˜xxx′
ij∈B(˜xxxij)
ℓ(h(˜xxx′
ij), ˜yij) −
sup
xxx′
ij∈B(xxxij)
ℓ(h(xxx′
ij), yij) !"
N,0.6941580756013745,"has
the
same
distribution
as"
N,0.695303550973654,"sup
˜xxx′
ij∈B(˜xxxij)
ℓ(h(˜xxx′
ij), ˜yij) −
sup
xxx′
ij∈B(xxxij)
ℓ(h(xxx′
ij), yij), where Σij
∼
Uniform({±1}) and ΣΣΣ has"
N,0.6964490263459335,"independent entries; c is from the property of sup that sup(a + b) ≤sup(a) + sup(b) and the fact
that ΣΣΣ has the same distribution as −ΣΣΣ."
N,0.697594501718213,"Similar as before, if we change one of the {(xxxij, yij)}n
i,j=1, the change of Rtn( eG) is at most U"
N,0.6987399770904925,"tn, so
by McDiarmid’s inequality, with probability at least 1 −δ"
N,0.699885452462772,"4, we have: E
b
D"
N,0.7010309278350515,"h
Rtn( eG)
i
≤Rtn( eG) + U r ln4/δ"
N,0.702176403207331,"2tn .
(A.5)"
N,0.7033218785796106,"Combine (A.4) and (A.5) we have: with probability at least 1 −δ 2,"
N,0.7044673539518901,ΨB(bD) ≤2Rtn( eG) + 3U r ln4/δ
N,0.7056128293241696,"2tn .
(A.6)"
N,0.7067583046964491,Combine (A.3) and (A.6) we have: with probability at least 1 −δ:
N,0.7079037800687286,ΦB(D) + ΨB(bD) ≤2Rt( eG) + 2Rtn( eG) + 3U r ln4/δ
T,0.709049255441008,"2t
+ 3U r ln4/δ 2tn ."
T,0.7101947308132875,"By the property of supremum, we have: with probability at least 1 −δ,"
T,0.711340206185567,"sup
h∈H"
T,0.7124856815578465,"
LB
p (ℓ, h) −LB
b
D(ℓ, h)

= sup
h∈H"
T,0.713631156930126,"
LB
p (ℓ, h) −LB
ˆp (ℓ, h) + LB
ˆp (ℓ, h) −LB
b
D(ℓ, h)
"
T,0.7147766323024055,"≤sup
h∈H"
T,0.715922107674685," 
LB
p (ℓ, h) −LB
ˆp (ℓ, h)

+ sup
h∈H"
T,0.7170675830469645,"
LB
ˆp (ℓ, h) −LB
b
D(ℓ, h)
"
T,0.718213058419244,= ΦB(D) + ΨB(bD)
T,0.7193585337915235,≤2Rt( eG) + 2Rtn( eG) + 3U r ln4/δ
T,0.720504009163803,"2t
+ 3U r ln4/δ 2tn ."
T,0.7216494845360825,"Proof of Corollary 3.2. The main idea of the proof is to use risk decomposition to reduce LB
p (ℓ, ˆh) −
LB
p (ℓ, h⋆) to LB
p (ℓ, h) −LB
b
D(ℓ, h) for some h. Then we have: with probability at least 1 −δ,"
T,0.722794959908362,"LB
p (ℓ, ˆh) −LB
p (ℓ, h⋆) = LB
p (ℓ, ˆh) −LB
b
D(ℓ, ˆh) + LB
b
D(ℓ, ˆh) −LB
b
D(ℓ, h⋆) + LB
b
D(ℓ, h⋆) −LB
p (ℓ, h⋆)"
T,0.7239404352806414,"a
≤LB
p (ℓ, ˆh) −LB
b
D(ℓ, ˆh) + LB
b
D(ℓ, h⋆) −LB
p (ℓ, h⋆)"
T,0.7250859106529209,"≤2sup
h∈H"
T,0.7262313860252004,"LB
p (ℓ, h) −LB
b
D(ℓ, h)"
T,0.7273768613974799,"b
≤4Rt( eG) + 4Rtn( eG) + 3U r ln8/δ"
T,0.7285223367697594,"2t
+ 3U r ln8/δ 2tn ,"
T,0.7296678121420389,"where: a is from the fact that LB
b
D(ℓ, ˆh) ≤LB
b
D(ℓ, h⋆) by the deﬁnition of ˆh; b is from Theorem"
T,0.7308132875143184,"3.1, although in Theorem 3.1, we only have bounds for sup
h∈H"
T,0.7319587628865979,"
LB
p (ℓ, h) −LB
b
D(ℓ, h)

, we can use"
T,0.7331042382588774,"a similar argumentation as in Theorem 3.1 to get the same bound for sup
h∈H"
T,0.7342497136311569,"
LB
b
D(ℓ, h) −LB
p (ℓ, h)

,"
T,0.7353951890034365,"both with probability at least 1 −δ′, using union bound for these two cases we get a bound for
sup
h∈H"
T,0.736540664375716,"LB
p (ℓ, h) −LB
b
D(ℓ, h)
 with probability at least 1 −2δ′, take δ =
δ′"
T,0.7376861397479955,"2 , we get the bound for"
T,0.738831615120275,"sup
h∈H"
T,0.7399770904925544,"LB
p (ℓ, h) −LB
b
D(ℓ, h)
 with probability at least 1 −δ, this is why we get ln4/δ in Theorem 3.1"
T,0.7411225658648339,but ln8/δ here.
T,0.7422680412371134,"A.3
Proof of the Limited Environment Case"
T,0.7434135166093929,"Proof dB
ℓ(H)(·, ·) is a pseudometric. It is obvious that dB
ℓ(H)(·, ·) is symmetric, now we proof that
dB
ℓ(H)(·, ·) satisﬁes the triangle inequality. By deﬁnition, for distribution P, Q, R:"
T,0.7445589919816724,"dB
ℓ(H)(P, Q) = sup
h∈H"
T,0.7457044673539519,"E
(xxx,y)∼P """
T,0.7468499427262314,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7479954180985109,"−
E
(xxx,y)∼Q """
T,0.7491408934707904,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7502863688430699,"= sup
h∈H"
T,0.7514318442153494,"E
(xxx,y)∼P """
T,0.7525773195876289,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7537227949599083,"−
E
(xxx,y)∼R """
T,0.7548682703321878,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7560137457044673,"+
E
(xxx,y)∼R """
T,0.7571592210767468,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7583046964490263,"−
E
(xxx,y)∼Q """
T,0.7594501718213058,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7605956471935853,"a
≤sup
h∈H"
T,0.7617411225658648,"E
(xxx,y)∼P """
T,0.7628865979381443,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7640320733104238,"−
E
(xxx,y)∼R """
T,0.7651775486827033,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7663230240549829,"+ sup
h∈H"
T,0.7674684994272624,"E
(xxx,y)∼R """
T,0.7686139747995419,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7697594501718213,"−
E
(xxx,y)∼Q """
T,0.7709049255441008,"sup
xxx′∈B(xxx)
ℓ(h(xxx′), y) #"
T,0.7720504009163803,"= dB
ℓ(H)(P, R) + dB
ℓ(H)(R, Q),"
T,0.7731958762886598,"where: a is from the subadditivity of the supremum operator. So dB
ℓ(H)(·, ·) is a pseudometric."
T,0.7743413516609393,"Before prove Thorem 3.3, we ﬁrst give a useful lemma."
T,0.7754868270332188,"Lemma A.5. For any h ∈H, any distribution P, Q on , we have:
RB
P (ℓ, h) −RB
Q(ℓ, h)
 ≤dB
ℓ(H)(P, Q)"
T,0.7766323024054983,"Proof of Lemma A.5. From the deﬁnition of dB
ℓ(H)(P, Q), we can know that:"
T,0.7777777777777778,"dB
ℓ(H)(P, Q) = sup
h∈H"
T,0.7789232531500573,"RB
P (ℓ, h) −RB
Q(ℓ, h)
 ≥
RB
P (ℓ, h) −RB
Q(ℓ, h)
 , ∀h ∈H,"
T,0.7800687285223368,which is just what we want.
T,0.7812142038946163,"Theorem 3.3. For a given but unknown target distribution T , let ∆t−1 := {(λ1, · · · , λt)|λi ≥"
T,0.7823596792668958,"0, Pt
i=1 λi = 1} be the t-dimensional simplex and Conv(D) :=
nPt
i=1 λiDi
λλλ ∈∆t−1o
be the"
T,0.7835051546391752,"convex hull of D = {D1, . . . , Dt}, deﬁne TP ∈
arg inf
D∈Conv(D)
dB
ℓ(H)(D, T ) be the “projection"" of T"
T,0.7846506300114547,"onto Conv(D) and λλλ⋆be the weight vector where TP = Pt
i=1 λ⋆
i Di, ℓ∈[0, U], then we have: with
probability at least 1 −δ, for all h ∈H,"
T,0.7857961053837342,"RB
T (ℓ, h) ≤1 t t
X"
T,0.7869415807560137,"i=1
RB
b
Di(ℓ, h) + 1 t X i X"
T,0.7880870561282932,"j
λ⋆
jdB
ℓ(H)( bDi, bDj) + dB
ℓ(H)(T , TP )"
T,0.7892325315005727,+ 4Rtn( eG) + 2Rn( eG) + 6U r ln8/δ
T,0.7903780068728522,2tn + 3U r
T,0.7915234822451317,ln(16t/δ)
N,0.7926689576174112,2n
N,0.7938144329896907,"Proof of Theorem 3.3. Our proof is divided into 2 steps, the ﬁrst step gets results for the population
distributions and the second step uses ﬁnite sample approximation to get results for empirical
distributions."
N,0.7949599083619702,Step 1. Get the relationship between the population distributions.
N,0.7961053837342497,"Lemma A.5 tells us that: for all h ∈H,"
N,0.7972508591065293,"RB
T (ℓ, h) ≤RB
¯
D(ℓ, h) + dB
ℓ(H)(T , ¯D) ≤RB
¯
D(ℓ, h) + dB
ℓ(H)(T , TP ) + dB
ℓ(H)(TP , ¯D),"
N,0.7983963344788088,"where the last inequality is from the triangle inequality of dB
ℓ(H)(·, ·) and ¯D = 1"
N,0.7995418098510882,"t
Pt
i=1 Di. Recall"
N,0.8006872852233677,that bD = 1
N,0.8018327605956472,"t
Pt
i=1 bDi, then we have:"
N,0.8029782359679267,"dB
ℓ(H)(TP , ¯D)
a
≤dB
ℓ(H)(TP , bTP ) + dB
ℓ(H)(bTP , bD) + dB
ℓ(H)( bD, ¯D)"
N,0.8041237113402062,"= dB
ℓ(H)(TP , bTP ) + dB
ℓ(H)( bD, ¯D) + dB
ℓ(H) t
X"
N,0.8052691867124857,"i=1
λ⋆
i bDi, 1 t t
X"
N,0.8064146620847652,"i=1
bDi !"
N,0.8075601374570447,"= dB
ℓ(H)(TP , bTP ) + dB
ℓ(H)( bD, ¯D) + sup
h∈H RBP"
N,0.8087056128293242,"i λ⋆
i b
Di(ℓ, h) −RBP"
N,0.8098510882016037,"i
1
t b
Di(ℓ, h)"
N,0.8109965635738832,"= dB
ℓ(H)(TP , bTP ) + dB
ℓ(H)( bD, ¯D) + sup
h∈H"
N,0.8121420389461627,"RB
1
t
P i
P"
N,0.8132875143184422,"j λ⋆
j b
Dj(ℓ, h)−RBP"
N,0.8144329896907216,"i
1
t
P"
N,0.8155784650630011,"j λ⋆
j b
Di(ℓ, h)"
N,0.8167239404352806,"b
≤dB
ℓ(H)(TP , bTP ) + dB
ℓ(H)( bD, ¯D) + 1 t X i X"
N,0.8178694158075601,"j
λ⋆
j sup
h∈H"
N,0.8190148911798396,"RB
b
Di(ℓ, h) −RB
b
Dj(ℓ, h)"
N,0.8201603665521191,"= dB
ℓ(H)(TP , bTP ) + dB
ℓ(H)( bD, ¯D) + 1 t X i X"
N,0.8213058419243986,"j
λ⋆
jdB
ℓ(H)( bDi, bDj),"
N,0.8224513172966781,"where: a is from the triangle inequality of dB
ℓ(H)(·, ·); b follows from the linearity of RB
D(ℓ, h) and
the subadditivity of the supremum operator. So we have, for any h ∈H:"
N,0.8235967926689576,"RB
T (ℓ, h) ≤RB
¯
D(ℓ, h)+dB
ℓ(H)(T , TP )+dB
ℓ(H)(TP , bTP )+dB
ℓ(H)( bD, ¯D)+ 1 t X i X"
N,0.8247422680412371,"j
λ⋆
jdB
ℓ(H)( bDi, bDj). (A.7)"
N,0.8258877434135166,"Step 2. We now show the bound of the ﬁnite sample approximation error dB
ℓ(H)(TP , bTP ) and"
N,0.827033218785796,"dB
ℓ(H)( bD, ¯D). Since we have no access to the population distribution ¯D, we also give a ﬁnite
sample approximation of RB
¯
D(ℓ, h) and bound the corresponding approximation error."
N,0.8281786941580757,The empirial distribution of ¯D is bD = 1
N,0.8293241695303551,"t
Pt
i=1 bDi, then we have:"
N,0.8304696449026346,"sup
h∈H"
N,0.8316151202749141,"
RB
¯
D(ℓ, h) −RB
b
D(ℓ, h)

= sup
h∈H 1 t t
X i=1"
N,0.8327605956471936,"
RB
Di(ℓ, h) −RB
b
Di(ℓ, h)

= ΨB(bD),"
N,0.8339060710194731,"where ΨB(bD) is deﬁned in the proof of Theorem 3.1. Then we have, with probability at least 1 −δ"
N,0.8350515463917526,"4,
for all h ∈H,"
N,0.8361970217640321,"RB
¯
D(ℓ, h) = 1 t t
X"
N,0.8373424971363116,"i=1
RB
Di(ℓ, h) ≤1 t t
X"
N,0.8384879725085911,"i=1
RB
b
Di(ℓ, h) + 2Rtn( eG) + 3U r ln8/δ"
N,0.8396334478808706,"2tn .
(A.8)"
N,0.8407789232531501,"Note that dB
ℓ(H)( bD, ¯D)
=
sup
h∈H"
N,0.8419243986254296,"RB
¯
D(ℓ, h) −RB
b
D(ℓ, h)
, similar as the bound of ΨB(bD)
="
N,0.843069873997709,"sup
h∈H"
N,0.8442153493699885,"
RB
¯
D(ℓ, h) −RB
b
D(ℓ, h)

, we have that with probability at least 1 −δ 4:"
N,0.845360824742268,"sup
h∈H"
N,0.8465063001145475,"
RB
b
D(ℓ, h) −RB
¯
D(ℓ, h)

≤2Rtn( eG) + 3U r ln8/δ 2tn ,"
N,0.847651775486827,so with probability at least 1 −δ 2:
N,0.8487972508591065,"dB
ℓ(H)( bD, ¯D) = sup
h∈H"
N,0.849942726231386,"RB
¯
D(ℓ, h) −RB
b
D(ℓ, h)
 ≤2Rtn( eG) + 3U r ln8/δ"
N,0.8510882016036655,"2tn .
(A.9)"
N,0.852233676975945,"Now we bound dB
ℓ(H)(TP , bTP ), by the deﬁnition of TP , we have:"
N,0.8533791523482245,"dB
ℓ(H)(TP , bTP ) = dB
ℓ(H) t
X"
N,0.854524627720504,"i=1
λ⋆
i Di, t
X"
N,0.8556701030927835,"i=1
λ⋆
i bDi ! ≤ t
X"
N,0.856815578465063,"i=1
λ⋆
i dB
ℓ(H)(Di, bDi)"
N,0.8579610538373424,"For each i, we deﬁne:
ΓB( bDi) = sup
h∈H"
N,0.8591065292096219,"
RB
Di(ℓ, h) −RB
b
Di(ℓ, h)

."
N,0.8602520045819015,"Since ℓ∈[0, U], changing one example in bDi will lead to at most 1"
N,0.861397479954181,"n chang in ΓB( bDi), so by
McDiarmid’s inequality we know: with probability at least 1 −
δ
16t,"
N,0.8625429553264605,"ΓB( bDi) ≤E
h
ΓB( bDi)
i
+ U r"
N,0.86368843069874,ln(16t/δ)
N,0.8648339060710195,"2n
."
N,0.865979381443299,"Next we upper bound E
h
ΓB( bDi)
i
, by similar analysis as in the proof of Theorem 3.1, we have: with"
N,0.8671248568155785,"probability at least 1 −
δ
16t,"
N,0.868270332187858,"E
b
Di"
N,0.8694158075601375,"h
ΓB( bDi)
i
≤Rn( eG) + U r"
N,0.870561282932417,ln(16t/δ)
N,0.8717067583046965,"2n
."
N,0.872852233676976,"Then with probability at least 1 −δ 8t,"
N,0.8739977090492554,"sup
h∈H"
N,0.8751431844215349,"
RB
Di(ℓ, h) −RB
b
Di(ℓ, h)

= ΓB( bDi) ≤2Rn( eG) + 3U r"
N,0.8762886597938144,ln(16t/δ)
N,0.8774341351660939,"2n
."
N,0.8785796105383734,"With similar argument, we know that with probability at least 1 −δ 8t,"
N,0.8797250859106529,"sup
h∈H"
N,0.8808705612829324,"
RB
b
Di(ℓ, h) −RB
Di(ℓ, h)

≤2Rn( eG) + 3U r"
N,0.8820160366552119,ln(16t/δ)
N,0.8831615120274914,"2n
."
N,0.8843069873997709,"So with probability at least 1 −δ 4t,"
N,0.8854524627720504,"dB
ℓ(H)( bDj, Dj) = sup
h∈H"
N,0.8865979381443299,"RB
Di(ℓ, h) −RB
b
Di(ℓ, h)
 ≤2Rn( eG) + 3U r"
N,0.8877434135166093,ln(16t/δ)
N,0.8888888888888888,"2n
."
N,0.8900343642611683,"So with probability at least 1 −δ 4,"
N,0.8911798396334479,"dB
ℓ(H)(TP , bTP ) ≤ t
X"
N,0.8923253150057274,"j=1
λ⋆
jdB
ℓ(H)( bDj, Dj) ≤2Rn( eG) + 3U r"
N,0.8934707903780069,ln(16t/δ)
N,0.8946162657502864,"2n
.
(A.10)"
N,0.8957617411225659,"Now combine (A.7), (A.8), (A.9) and (A.10), we have: with probability at least 1 −δ, for any h ∈H,"
N,0.8969072164948454,"RB
T (ℓ, h) ≤RB
¯
D(ℓ, h)+dB
ℓ(H)(T , TP )+dB
ℓ(H)(TP , bTP )+dB
ℓ(H)( bD, ¯D)+ 1 t X i X"
N,0.8980526918671249,"j
λ⋆
jdB
ℓ(H)( bDi, bDj) ≤1 t t
X"
N,0.8991981672394044,"i=1
RB
b
Di(ℓ, h) + 2Rtn( eG) + 3U r ln8/δ"
N,0.9003436426116839,"2tn + dB
ℓ(H)(T , TP ) + 2Rn( eG) + 3U r"
N,0.9014891179839634,ln(16t/δ)
N,0.9026345933562429,"2n
+ 2Rtn( eG) + 3U r ln8/δ"
N,0.9037800687285223,2tn + 1 t X i X
N,0.9049255441008018,"j
λ⋆
jdB
ℓ(H)( bDi, bDj) = 1 t t
X"
N,0.9060710194730813,"i=1
RB
b
Di(ℓ, h) + 1 t X i X"
N,0.9072164948453608,"j
λ⋆
jdB
ℓ(H)( bDi, bDj) + dB
ℓ(H)(T , TP ) + 4Rtn( eG)"
N,0.9083619702176403,+ 2Rn( eG) + 6U r ln8/δ
N,0.9095074455899198,2tn + 3U r
N,0.9106529209621993,ln(16t/δ)
N,0.9117983963344788,"2n
."
N,0.9129438717067583,"B
Toy Example to Model the ColoredMNIST"
N,0.9140893470790378,"Example 2. Consider data point (xxx, y), where xxx ∈X ⊆Rd+1 consists of invariant feature xxxinv ∈
Xinv ⊆Rd and spuriously correlated feature xsp ∈{±s} ⊊R, to simplify the example, let xxx ="
N,0.9152348224513173,"(xxxinv, xsp) be the feature observed. Consider the case there is a linear classiﬁer h(xxxinv) = wwwinv · xxxinv
such that y · h(xxxinv) > 0, i.e., xxxinv is invariant for all distributions on Xinv × {±1}. Suppose the
dataset S = {(xxxi, yi)}n
i=1 is linearly separable, and the dataset S is induced into two disjoint
groups: a majority group Smaj where xsp · y > 0 and a minority group Smin where xsp · y < 0, let
p = P[xsp · y > 0] > 0.5, which means that the spurious feature is positively related to the label y."
N,0.9163802978235968,"The next theorem considers the degree of dependence on the spurious feature of the trained linear
classiﬁer.
Theorem B.1 (Theorem 4 of [48]). Let H be the set fo linear classiﬁers, h(xxx) = wwwinvxxxinv + wspxsp.
Consider any task that satisﬁes all the constraints in Section 3.1 of [48]. Consider a dataset S drawn
from D such that the empirical distribution of xxxinv given xsp · y > 0 is identical to the empirical
distribution of xxxinv given xsp · y < 0. Let wwwinv(t)xxxinv + wspxsp be initialized to the origin, and trained
with an inﬁnitesimal learning rate gradient descent to minimize the exponential loss on a dataset S.
Then, for any (xxx, y) ∈S, we have: Ω "
N,0.9175257731958762,"
ln
c+p
c+√"
N,0.9186712485681557,"p(1−p)
Mln(t + 1) "
N,0.9198167239404352,"≤
wsp(t)s
|wwwinv(t) · xxxinv| ≤O"
N,0.9209621993127147,"ln
p
1−p
ln(t + 1) ! ,"
N,0.9221076746849943,where:
N,0.9232531500572738,"1. p denotes the empirical level of spurious correlation, p =
1
|S|
P"
N,0.9243986254295533,"(xxx,y)∈S 111[xsp · y > 0] which
without generality is assumed to satisfy p ∈[0.5, 1)."
N,0.9255441008018328,"2. M denotes the maximum value of the margin of the max-margin classiﬁer on S, i.e.,
M = maxxxx∈S ˆwww · xxx where ˆwww is the max-margin classiﬁer on S."
N,0.9266895761741123,3. c := 2(2M−1)
N,0.9278350515463918,"s2
.
Remark 4. In the proof of Theorem B.1 (please refer to [48]), the loss function is:"
N,0.9289805269186713,"L(wwwinv, wsp) = pExxxinv∼ˆ
Dinv"
N,0.9301260022909508,"h
e−(w
wwinvxxxinv+wsps)i
+ (1 −p)Exxxinv∼ˆ
Dinv"
N,0.9312714776632303,"h
e−(ww
winvxxxinv−wsps)i
.
(B.11)"
N,0.9324169530355098,"We denote L1 = Exxxinv∼ˆ
Dinv

e−(w
w
winvxxxinv+wsps)
and L2 = Exxxinv∼ˆ
Dinv

e−(w
wwinvxxxinv−wsps)
, so L = pL1 +
(1 −p)L2. To model the case in the ColoredMNIST dataset, we let S1 denote the dataset with
corruption rate 0.1, S2 the dataset with corruption rate 0.2, S3 the dataset with corruption rate 0.9.
For training environment with corruption rate ptrain, we have Ltrain = ptrainL1 + (1 −ptrain)L2 and
Ltest = ptestL1 + (1 −ptest)L2 if ptrain > 0.5. otherwise we have Ltrain = ptrainL2 + (1 −ptrain)L1
and Ltest = ptestL2 + (1 −ptest)L1. What’s more, since the lower bound > 0 for p > 1"
N,0.9335624284077892,"2, we have
L1 < L2. So in the standard training case:"
N,0.9347079037800687,"1. When we train with S1, S3, then the overall ptrain = 0.1+0.9 2
= 1"
SO ACCORDING TO THEOREM,0.9358533791523482,"2. So according to Theorem
B.1 we know that wsp(t) = 0, which means that the classiﬁer just depends on the invariant
feature, so the performance on S2 is good. Since wsp(t) = 0, L1 = L2, so we have
Ltrain = ptrainL1 + (1 −ptrain)L2 = Exxxinv∼ˆ
Dinv

e−(w
wwinvxxxinv)
= Ltest. So the mixed training
distribution here is the same as the test distribution, which means that we can generalize
well in this case, which is consistent with our experimental results."
SO ACCORDING TO THEOREM,0.9369988545246277,"2. When training with S2, S3, then the overall ptrain = 0.2+0.9"
SO ACCORDING TO THEOREM,0.9381443298969072,"2
= 0.55, which is quite close to
0.5, which means that the correlation between xsp and y is not very strong. It means that
the upper bound and the lower bound in Theorem B.1 is small, so wsp is small, the classiﬁer
mainly depends on the invariant feature and slightly depends on the spurious feature, which
is just a little negatively related to y when ptest = 0.1 in the test environment. In this case,
Ltrain = ptrainL1 + (1 −ptrain)L2 = 0.55L1 + 0.45L2 and Ltest = ptestL1 + (1 −ptest)L2 =
0.1L1 + 0.9L2, and Ltest −Ltrain = −0.45L1 + 0.45L2 = 0.45(L2 −L1) > 0, but in
this case wsp is small, so we have L2 −L1 is small according to (B.11), so the difference
between training error and test error is not too large, so the generalization performance for
this case is not bad, which is consistent with our experimental results."
SO ACCORDING TO THEOREM,0.9392898052691867,"3. Finally, it comes to the failure case in our experimental results, i.e., using S1, S2 as the
training environments and S3 as the test environment. In this case, the overall ptrain ="
SO ACCORDING TO THEOREM,0.9404352806414662,0.1+0.2
SO ACCORDING TO THEOREM,0.9415807560137457,"2
= 0.15, so the classiﬁer depends more on the spurious features compared with
the above two cases, what’s more, since ptest = 0.9, the relationship between xsp and y
is almost reversed when we switch to the test stage, so we have Ltrain = ptrainL2 + (1 −
ptrain)L1 = 0.85L1 + 0.15L2 and Ltest = ptestL2 + (1 −ptest)L1 = 0.1L1 + 0.9L2 and
Ltest −Ltrain = −0.75L1 + 0.75L2 = 0.75(L2 −L1) > 0, and the wsp here is much larger
than that in case 2, which means that L2 −L1 here is much larger than that in case 2, so
the gap between training error and test error here map be much larger than that in case two,
leading to the poor generalization performance in practice."
SO ACCORDING TO THEOREM,0.9427262313860252,"We can see that the analysing results here are consistent with our experimental results for ColoredM-
NIST in Appendix D.2."
SO ACCORDING TO THEOREM,0.9438717067583047,"C
Experimental Settings"
SO ACCORDING TO THEOREM,0.9450171821305842,"We run each algorithm 20 times and 1 trial (since adversarial training is time-consuming, we just run
1 trial rather than 3 trials as done in DomainBed). We use part of the training data as the validation
set to select the best model of the 20 runs according to the adversarial robustness of the training
environments. Following DomainBed, we use random hyper-parameters in the 20 runs."
SO ACCORDING TO THEOREM,0.9461626575028637,"C.1
Attacked Algorithms"
SO ACCORDING TO THEOREM,0.9473081328751431,"1. Empirical Risk Minimization (ERM, [61]) minimizes the errors across domains.
2. Meta-Learning for Domain Generalization (MLDG, [39]) leverages MAML [16] to meta-
learn how to generalize across domains.
3. Class-conditional DANN (C-DANN, [42]) is a variant of DANN [17] matching the condi-
tional distributions P[φ(Xd)|Y d = y] across domains, for all labels y.
4. Risk Extrapolation (VREx, [33]) approximates IRM with a variance penalty.
5. Representation Self-Challenging (RSC, [28]) learns robust neural networks by iteratively
discarding (challenging) the most activated features.
6. Domain-wise Multiple-perturbation Adversarial Training (MAT, [64]) use an universal
adversarial perturbation (UAP) with low rank along the dimension of examples (for n exam-
ples, use the convex combination of k (k < n) perturbations as the universal perturbation)
to conduct universal adversarial training (UAT) to improve the OOD generalization.
7. Adversarial Training with Low-rank Decomposed perturbations(LDAT, [64]) shares similar
idea with MAT, but it uses an UAP with low rank along the input space (for N × N images,
it constrains the N × N UAP matrix has rank l < N) to conduct universal adversarial
training (UAT) to improve the OOD generalization."
SO ACCORDING TO THEOREM,0.9484536082474226,"C.2
Datasets"
SO ACCORDING TO THEOREM,0.9495990836197021,We use the following datasets provided by the DomainBed [26]:
SO ACCORDING TO THEOREM,0.9507445589919816,"1. ColoredMNIST [4] is a variant of the MNIST handwritten digit classiﬁcation dataset [35].
Domain d ∈{0.1, 0.3, 0.9} contains a disjoint set of digits colored either red or blue. The
label is a noisy function of the digit and color, such that color bears correlation d with
the label and the digit bears correlation 0.75 with the label. This dataset contains 70000
examples of dimension (2, 28, 28) and 2 classes.
2. RotatedMNIST [21] is a variant of MNIST where domain d ∈{0, 15, 30, 45, 60, 75}
contains digits rotated by d degrees. Our dataset contains 70000 examples of dimension
(1, 28, 8) and 10 classes.
3. PACS [38] comprises four domains d ∈{ art, cartoons, photos, sketches }. This dataset
contains 9991 examples of dimension (3, 224, 224) and 7 classes.
4. VLCS [15] comprises four photographic domains d ∈{ Caltech101, LabelMe, SUN09,
VOC2007 }. This dataset contains 10729 examples of dimension (3, 224, 224) and 5 classes.
5. OfﬁceHome [62] includes domains four d ∈{ art, clipart, product, real }. This dataset
contains 15588 examples of dimension (3, 224, 224) and 65 classes."
SO ACCORDING TO THEOREM,0.9518900343642611,"C.3
Settings for attacking standard OOD algorithms"
SO ACCORDING TO THEOREM,0.9530355097365406,"We evaluate the adversarial robustness of some of the algorithms in Table 1. For adversarial attacks,
we use ℓ∞adversarial perturbation upper bound ϵ = 0.1 for RotatedMNIST and ColoredMNIST,
ϵ = 4/255 for others (it is because ϵ = 4/255 is use usually used in 224 × 224 colored images); we
use the classical FGSM and PGD-20 as the attack methods, for PGD-20, the step size α = ϵ/4, we
realize the attacks according to the package ’torchattacks’ [32]."
SO ACCORDING TO THEOREM,0.9541809851088202,"We train the models in the same hyper-parameter random search setting (except that we just train
the model for one trial) in DomainBed, and the we attack the models trained with the best hyper-
parameters choosed by the training-domain validation set model selection method in DomainBed
with the same seed as used during training. For MAT and LDAT, we use the same hyper-parameter
random search setting in [64] except that we train with 20 random hyper-parameter for 1 trial and
they train with 8 random hyper-parameter for 3 trials."
SO ACCORDING TO THEOREM,0.9553264604810997,"C.4
Settings for OOD adversarial algorithms"
SO ACCORDING TO THEOREM,0.9564719358533792,"In this subsection, we introduce the settings in our experiments in Section 5. We use the training-
domain validation set model selection method in the experiment and train the model with 20 random
hyper-parameter for 1 trial."
SO ACCORDING TO THEOREM,0.9576174112256587,Basic setting for training hyper-parameter random search.
SO ACCORDING TO THEOREM,0.9587628865979382,"1. For AT, we use the same basic random search setting as ERM."
SO ACCORDING TO THEOREM,0.9599083619702177,"2. For RDANN, we use the same basic random search setting as DANN."
SO ACCORDING TO THEOREM,0.9610538373424972,"Setting for adversarial training. We use PGD-10 as the adversarial attack to generate adversarial
examples for adversarial training, with ϵ = 0.1 for RotatedMNIST and ColoredMNIST and ϵ =
4/255 for the other 224 × 224 datasets, and we use the attack step size α = ϵ/4 as usually done in
the adversarial training community."
SO ACCORDING TO THEOREM,0.9621993127147767,"Setting for evaluating adversarial robustness. We consider two attack methods, FGSM and PGD-
20, to evaluate the OOD adversarial robustness of the trained models. We use the same ϵ as that used
for training but we do 20 iterations for PGD in the evaluation stage rather than just 10 iterations in
the training stage to avoid overﬁtting, which is also a strategy used by the adversarial robustness
community."
SO ACCORDING TO THEOREM,0.9633447880870561,"D
Additional Experimental Results"
SO ACCORDING TO THEOREM,0.9644902634593356,"In this section, we show the detailed adversarial robustness of the current OOD generalization
algorithms and our proposed algorithms, here we show the adversarial robustness in each test
environment, for the ColoredMNIST dataset and RotatedMNIST dataset, we set ϵ = 0.1, and for
other 224 × 224 datasets, we set ϵ =
4
255."
SO ACCORDING TO THEOREM,0.9656357388316151,"In Appendices D.1 to D.3, we use FGSM and PGD-20 as the attacks to evaluate the adversarial
robustness and the results are shown in the form of triple tuple (clean accuracy / accuracy under
FGSM attak / accuracy under PGD-20 attack). Each column represents the results for a test
environment. The tables show that compared with existing methods, our methods (AT and RDANN)
signiﬁcantly improve the adversarial robustness of the model on the target domains."
SO ACCORDING TO THEOREM,0.9667812142038946,"In Appendix D.6, we use AutoAttack as the attacks to evaluate the adversarial robustness. Each
column represents the results for a test environment. The results show that under AutoAttack, all
the existing methods fails (no more than 1% adversarial accuracy), and our methods signiﬁcantly
improves the adversarial robustness."
SO ACCORDING TO THEOREM,0.9679266895761741,"D.1
RotatedMNIST"
SO ACCORDING TO THEOREM,0.9690721649484536,"Algorithm
0
15
30
45
60
75
Avg"
SO ACCORDING TO THEOREM,0.9702176403207331,"ERM
93.8 / 11.9 / 1.3
98.7 / 10.0 / 1.1
99.2 / 13.5 / 0.5
99.2 / 19.2 / 0.2
99.0 / 19.1 / 0.3
95.8 / 11.8 / 0.2
97.6 / 14.2 / 0.6
MLDG
94.8 / 14.1 / 0.4
98.7 / 15.0 / 0.2
99.1 / 11.8 / 0.0
99.1 / 17.2 / 0.0
98.8 / 15.3 / 0.8
96.7 / 16.9 / 0.0
97.9 / 15.1 / 0.2
CDANN
95.7 / 11.4 / 0.0
98.7 / 12.2 / 2.7
98.8 / 16.1 / 0.5
98.9 / 12.3 / 2.1
98.9 / 16.5 / 0.0
95.9 / 22.9 / 0.3
97.8 / 15.2 / 0.9
VREx
95.1 / 8.3 / 0.3
98.6 / 9.3 / 0.1
98.6 / 11.9 / 0.1
99.0 / 13.4 / 0.0
98.8 / 14.4 / 0.4
96.3 / 9.2 / 0.0
97.7 / 11.1 / 0.2
RSC
95.5 / 11.4 / 1.5
98.4 / 11.3 / 0.0
99.3 / 11.6 / 2.3
99.1 / 12.0 / 0.7
99.0 / 21.6 / 1.0
96.0 / 9.9 / 0.3
97.9 / 13.0 / 1.0
AT
97.0 / 91.8 / 90.8
99.2 / 96.1 / 94.8
99.2 / 96.6 / 95.6
99.0 / 96.3 / 95.4
99.0 / 96.2 / 95.1
97.2 / 90.9 / 88.9
98.4 / 94.7 / 93.4
RDANN
96.1 / 91.5 / 90.8
99.2 / 96.0 / 94.7
99.2 / 96.4 / 95.3
99.2 / 96.7 / 95.6
99.1 / 96.4 / 95.5
97.3 / 91.1 / 89.0
98.4 / 94.7 / 93.5"
SO ACCORDING TO THEOREM,0.9713631156930126,"D.2
ColoredMNIST"
SO ACCORDING TO THEOREM,0.9725085910652921,"Algorithm
+90%
+80%
-90%
Avg"
SO ACCORDING TO THEOREM,0.9736540664375716,"ERM
71.7 / 20.0 / 2.2
72.8 / 29.8 / 5.5
9.8 / 9.8 / 9.8
51.5 / 19.9 / 5.8
MLDG
72.3 / 16.6 / 0.1
73.4 / 29.5 / 5.3
9.7 / 9.5 / 9.0
51.8 / 18.5 / 4.8
CDANN
71.4 / 31.3 / 11.7
72.5 / 29.5 / 4.6
10.0 / 8.9 / 8.1
51.3 / 23.2 / 8.2
VREx
72.9 / 24.3 / 3.7
73.9 / 29.8 / 5.9
10.3 / 10.1 / 9.8
52.3 / 21.4 / 6.4
RSC
72.7 / 36.7 / 1.2
72.9 / 36.3 / 3.7
10.2 / 9.1 / 6.7
51.9 / 27.3 / 3.9
MAT
72.7 / 40.6 / 21.2
73.9 / 27.9 / 1.6
9.9 / 9.6 / 9.3
52.2 / 26.0 / 10.7
LDAT
72.1 / 28.3 / 7.0
72.8 / 36.4 / 7.4
9.7 / 9.5 / 9.3
51.5 / 24.7 / 7.9
AT
73.7 / 72.4 / 72.3
73.7 / 72.6 / 72.5
10.2 / 10.2 / 10.2
52.5 / 51.7 / 51.6
RDANN
73.3 / 72.1 / 72.0
73.2 / 71.8 / 71.7
9.7 / 9.7 / 9.7
52.1 / 51.2 / 51.1"
SO ACCORDING TO THEOREM,0.9747995418098511,"D.3
VLCS"
SO ACCORDING TO THEOREM,0.9759450171821306,"Algorithm
C
L
S
V
Avg"
SO ACCORDING TO THEOREM,0.97709049255441,"ERM
98.2 / 47.4 / 0.0
62.7 / 21.7 / 0.0
72.1 / 4.4 / 0.0
71.9 / 6.0 / 0.0
76.2 / 19.9 / 0.0
MLDG
98.5 / 50.0 / 0.0
63.5 / 24.3 / 0.0
73.1 / 4.9 / 0.0
79.7 / 10.0 / 0.0
78.7 / 22.3 / 0.0
CDANN
98.1 / 58.4 / 0.0
64.8 / 11.9 / 0.0
74.6 / 10.4 / 0.0
77.1 / 25.8 / 12.1
78.7 / 26.6 / 3.0
VREx
98.4 / 56.2 / 0.0
64.9 / 16.2 / 0.0
69.7 / 2.7 / 0.0
78.4 / 13.6 / 0.0
77.9 / 22.2 / 0.0
RSC
97.9 / 35.7 / 0.0
64.9 / 12.5 / 0.0
75.5 / 3.0 / 0.0
77.8 / 4.3 / 0.0
79.0 / 13.9 / 0.0
MAT
98.9 / 52.7 / 0.0
64.8 / 18.3 / 0.0
68.7 / 2.3 / 0.0
80.7 / 9.5 / 0.0
78.3 / 20.7 / 0.0
LDAT
97.2 / 44.8 / 0.0
65.8 / 20.9 / 0.0
70.8 / 2.9 / 0.0
73.9 / 5.2 / 0.0
76.9 / 18.5 / 0.0
AT
74.2 / 64.8 / 63.5
53.0 / 40.8 / 39.5
47.2 / 36.2 / 34.9
49.6 / 35.1 / 32.4
56.0 / 44.2 / 42.6
RDANN
69.3 / 66.5 / 66.3
53.2 / 43.2 / 42.1
50.3 / 37.7 / 35.5
54.4 / 39.4 / 35.7
56.8 / 46.7 / 44.9"
SO ACCORDING TO THEOREM,0.9782359679266895,"D.4
PACS"
SO ACCORDING TO THEOREM,0.979381443298969,"Algorithm
A
C
P
S
Avg"
SO ACCORDING TO THEOREM,0.9805269186712485,"ERM
86.8/6.7/0.0
76.1/29.5/0.9
98.1/55.5/0.0
67.9/23.5/0.2
82.2/28.8/0.3
MLDG
86.8/10.3/0.0
75.2/30.0/0.2
97.6/62.5/0.0
76.1/29.6/0.1
83.9/33.1/0.1
CDANN
91.3/18.3/1.0
78.4/31.4/0.3
96.7/53.7/4.0
77.1/31.0/0.6
85.9/33.6/1.5
VREx
86.3/8.1/0.0
80.5/30.0/0.2
98.1/51.2/0.0
68.7/27.0/0.0
83.4/29.1/0.0
RSC
86.2/6.4/0.0
76.4/29.4/0.4
97.5/59.7/0.0
72.2/27.0/0.1
83.1/30.6/0.1
MAT
88.3/11.8/0.0
80.5/33.5/1.1
97.4/57.6/0.1
77.4/32.2/1.5
85.9/33.8/0.7
LDAT
87.2/8.3/0.0
76.1/26.6/0.1
97.8/54.0/0.0
74.0/33.1/0.2
83.8/30.5/0.1
AT
59.8/30.9/24.5
71.4/56.7/54.9
75.7/60.4/57.6
60.1/55.8/55.5
66.7/50.9/48.1
RDANN
63.8/31.8/25.0
63.3/50.5/48.2
76.8/62.3/58.2
68.5/61.8/61.1
68.1/51.6/48.1"
SO ACCORDING TO THEOREM,0.981672394043528,"D.5
OfﬁceHome"
SO ACCORDING TO THEOREM,0.9828178694158075,"Algorithm
A
C
P
R
Avg"
SO ACCORDING TO THEOREM,0.983963344788087,"ERM
62.1/12.6/0.0
52.9/20.9/0.3
76.4/25.7/0.5
76.0/26.3/0.6
66.9/21.4/0.4
MLDG
61.1/13.7/0.6
52.9/22.1/0.7
76.5/25.7/0.3
77.4/24.0/0.9
67.0/21.4/0.6
CDANN
61.8/8.5/0.1
53.5/19.3/0.1
75.8/24.4/0.1
77.6/23.7/0.0
67.2/19.0/0.1
VREx
58.7/8.3/0.0
52.4/20.0/0.1
75.5/22.5/0.7
76.1/23.3/1.0
65.7/18.5/0.4
RSC
59.3/9.1/0.2
52.1/18.3/0.3
75.2/20.5/1.0
74.1/20.6/1.2
65.1/17.1/0.7
MAT
57.3/11.2/0.1
54.0/20.2/0.5
75.2/26.9/0.8
77.4/24.1/1.8
66.0/20.6/0.8
LDAT
61.2/12.9/0.0
52.7/20.6/0.4
74.5/27.9/0.6
78.0/29.5/0.8
66.6/22.7/0.4
AT
29.6/16.4/14.5
44.6/36.4/35.0
53.2/40.9/38.3
53.7/36.1/32.9
45.3/32.4/30.2
RDANN
30.0/17.1/15.3
41.9/33.6/32.1
48.8/38.2/36.3
47.1/33.5/30.8
41.9/30.6/28.6"
SO ACCORDING TO THEOREM,0.9851088201603666,"D.6
Results for AutoAttack"
SO ACCORDING TO THEOREM,0.9862542955326461,"D.6.1
RotatedMNIST"
SO ACCORDING TO THEOREM,0.9873997709049256,"Algorithm
0
15
30
45
60
75
Avg"
SO ACCORDING TO THEOREM,0.9885452462772051,"ERM
0.0
0.0
0.0
0.0
0.0
0.0
0.0
MLDG
0.0
0.0
0.0
0.0
0.0
0.0
0.0
CDANN
0.0
0.0
0.0
0.0
0.0
0.1
0.0
VREx
0.0
0.0
0.0
0.0
0.0
0.0
0.0
RSC
0.0
0.0
0.0
0.0
0.0
0.0
0.0
AT
90.6
94.7
95.5
95.3
95.0
88.6
93.3
RDANN
90.6
94.6
95.2
95.6
95.5
88.6
93.3"
SO ACCORDING TO THEOREM,0.9896907216494846,"D.6.2
VLCS"
SO ACCORDING TO THEOREM,0.9908361970217641,"Algorithm
C
L
S
V
Avg"
SO ACCORDING TO THEOREM,0.9919816723940436,"ERM
0.0
0.0
0.0
0.0
0.0
MLDG
0.0
0.0
0.0
0.0
0.0
CDANN
0.0
0.0
0.0
0.0
0.0
VREx
0.0
0.0
0.0
0.0
0.0
RSC
0.0
0.0
0.0
0.0
0.0
MAT
0.0
0.0
0.0
0.0
0.0
LDAT
0.0
0.0
0.0
0.0
0.0
AT
63.2
38.7
33.9
31.4
41.8
RDANN
65.6
41.1
34.7
34.3
43.9"
SO ACCORDING TO THEOREM,0.993127147766323,"D.6.3
PACS"
SO ACCORDING TO THEOREM,0.9942726231386025,"Algorithm
A
C
P
S
Avg"
SO ACCORDING TO THEOREM,0.995418098510882,"ERM
0.0
1.8
0.0
0.7
0.6
MLDG
0.0
0.5
0.0
0.7
0.3
CDANN
0.0
0.6
0.0
0.4
0.3
VREx
0.0
0.5
0.0
0.9
0.3
RSC
0.0
0.9
0.0
0.6
0.4
MAT
0.0
1.5
0.1
4.0
1.4
LDAT
0.0
0.5
0.0
0.9
0.3
AT
23.4
54.7
57.0
55.2
47.6
ADANN
24.5
48.2
58.0
61.1
48.0"
SO ACCORDING TO THEOREM,0.9965635738831615,"D.7
OfﬁceHome"
SO ACCORDING TO THEOREM,0.997709049255441,"Algorithm
A
C
P
R
Avg"
SO ACCORDING TO THEOREM,0.9988545246277205,"ERM
0.0
0.0
0.1
0.1
0.0
MLDG
0.0
0.1
0.2
0.1
0.1
CDANN
0.0
0.1
0.0
0.1
0.0
VREx
0.0
0.1
0.2
0.1
0.1
RSC
0.0
0.1
0.1
0.0
0.0
MAT
0.0
0.3
0.2
0.1
0.1
LDAT
0.0
0.0
0.1
0.1
0.1
AT
14.2
34.6
38.1
32.4
29.8
ADANN
14.0
31.5
34.4
29.7
27.4"
