Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007451564828614009,"The pioneering work of Oono & Suzuki [ICLR, 2020] and Cai & Wang
1"
ABSTRACT,0.0014903129657228018,"[arXiv:2006.13318] analyze the smoothness of graph convolutional network (GCN)
2"
ABSTRACT,0.0022354694485842027,"features. Their results reveal an intricate empirical correlation between node clas-
3"
ABSTRACT,0.0029806259314456036,"sification accuracy and the ratio of smooth to non-smooth feature components.
4"
ABSTRACT,0.0037257824143070045,"However, the optimal ratio that favors node classification is unknown, and the
5"
ABSTRACT,0.004470938897168405,"non-smooth features of deep GCN with ReLU or leaky ReLU activation function
6"
ABSTRACT,0.005216095380029807,"diminish. In this paper, we propose a new strategy to let GCN learn node features
7"
ABSTRACT,0.005961251862891207,"with a desired smoothness to enhance node classification. Our approach has three
8"
ABSTRACT,0.0067064083457526085,"key steps: (1) We establish a geometric relationship between the input and output
9"
ABSTRACT,0.007451564828614009,"of ReLU or leaky ReLU. (2) Building on our geometric insights, we augment the
10"
ABSTRACT,0.00819672131147541,"message-passing process of graph convolutional layers (GCLs) with a learnable
11"
ABSTRACT,0.00894187779433681,"term to modulate the smoothness of node features with computational efficiency.
12"
ABSTRACT,0.009687034277198211,"(3) We investigate the achievable ratio between smooth and non-smooth feature
13"
ABSTRACT,0.010432190760059613,"components for GCNs with the augmented message passing scheme. Our extensive
14"
ABSTRACT,0.011177347242921014,"numerical results show that the augmented message passing remarkably improves
15"
ABSTRACT,0.011922503725782414,"node classification for GCN and some related models.
16"
INTRODUCTION,0.012667660208643815,"1
Introduction
17"
INTRODUCTION,0.013412816691505217,"Let G = (V, E) be an undirected graph with V = {vi}n
i=1 and E be the set of nodes and edges, resp.
18"
INTRODUCTION,0.014157973174366617,"Let A ∈Rn×n be the adjacency matrix of the graph with Aij = 1(i,j)∈E, where 1 is the indicator
19"
INTRODUCTION,0.014903129657228018,"function. Furthermore, let G be the following (augmented) normalized adjacency matrix
20"
INTRODUCTION,0.01564828614008942,G := (D + I)−1
INTRODUCTION,0.01639344262295082,2 (I + A)(D + I)−1
INTRODUCTION,0.01713859910581222,"2 = ˜
D−1"
INTRODUCTION,0.01788375558867362,"2 ˜
A ˜
D−1"
INTRODUCTION,0.018628912071535022,"2 ,
(1)"
INTRODUCTION,0.019374068554396422,"where I is the identity matrix, D is the degree matrix with Dii = Pn
j=1 Aij, and ˜
A := A + I and
21"
INTRODUCTION,0.020119225037257823,"˜D := D + I. Starting from the initial node features H0 := [(h0
1)⊤, . . . , (h0
n)⊤]⊤∈Rd×n with
22"
INTRODUCTION,0.020864381520119227,"h0
i ∈Rd being the ith node feature vector, the graph convolutional network (GCN) [20] learns node
23"
INTRODUCTION,0.021609538002980627,"representations using the following graph convolutional layer (GCL) transformation
24"
INTRODUCTION,0.022354694485842028,"Hl = σ(W lHl−1G),
(2)"
INTRODUCTION,0.023099850968703428,"where σ is the activation function, e.g. ReLU [25], and W l ∈Rd×d is learnable. GCL smooths
25"
INTRODUCTION,0.02384500745156483,"feature vectors of the neighboring nodes. The smoothness of features helps node classification; see
26"
INTRODUCTION,0.02459016393442623,"e.g. [22, 31, 5], resonating with the idea of classical semi-supervised learning approaches [41, 38].
27"
INTRODUCTION,0.02533532041728763,"Accurate node classification requires a balance between smooth and non-smooth components of GCN
28"
INTRODUCTION,0.02608047690014903,"features [27]. Besides graph convolutional networks (GCNs) stacking GCLs, many other graph neural
29"
INTRODUCTION,0.026825633383010434,"networks (GNNs) have been developed using different mechanisms, including spectral methods [3, 9],
30"
INTRODUCTION,0.027570789865871834,"spatial methods [12, 30], sampling methods [13, 36], and the attention mechanism [30]. Many other
31"
INTRODUCTION,0.028315946348733235,"GNN models can be found in recent surveys or monographs; see, e.g. [15, 1, 33, 39, 14].
32"
INTRODUCTION,0.029061102831594635,"Deep neural networks usually outperform shallow architectures, and a remarkable example is convo-
33"
INTRODUCTION,0.029806259314456036,"lutional neural networks [21, 16]. However, this does not carry to GCNs; deep GCNs tend to perform
34"
INTRODUCTION,0.030551415797317436,"significantly worse than shallow models [5]. In particular, the node feature vectors learned by deep
35"
INTRODUCTION,0.03129657228017884,"GCNs tend to be identical over each connected component of the graph; this phenomenon is referred
36"
INTRODUCTION,0.03204172876304024,"to as over-smoothing [22, 26, 27, 4, 5, 32], which not only occurs for GCN but also for many other
37"
INTRODUCTION,0.03278688524590164,"GNNs, e.g., GraphSage [13] and MPNN [12]. Intuitively, each GCL smooths neighboring node
38"
INTRODUCTION,0.03353204172876304,"features, benefiting node classification [22, 31, 5]. However, stacking these smoothing layers will in-
39"
INTRODUCTION,0.03427719821162444,"evitably homogenize node features. Algorithms have been developed to alleviate the over-smoothing
40"
INTRODUCTION,0.03502235469448584,"issue of GNNs, including decoupling prediction and message passing [11], skip connection and batch
41"
INTRODUCTION,0.03576751117734724,"normalization [18, 7, 6], graph sparsification [29], jumping knowledge [34], scattering transform
42"
INTRODUCTION,0.03651266766020864,"[24], PairNorm [37], and controlling the Dirichlet energy of node features [40].
43"
INTRODUCTION,0.037257824143070044,"From a theoretical perspective, it is proved that deep GCNs using ReLU or leaky ReLU activation
44"
INTRODUCTION,0.038002980625931444,"function learn homogeneous node features [27, 4]. In particular, [27] shows that the distance of
45"
INTRODUCTION,0.038748137108792845,"node features to the eigenspace M – corresponding to the largest eigenvalue 1 of matrix G in (1)
46"
INTRODUCTION,0.039493293591654245,"– goes to zero when the depth of GCN with ReLU goes to infinity. Meanwhile, [27] empirically
47"
INTRODUCTION,0.040238450074515646,"studies the intricate correlation between node classification accuracy and the ratio between smooth
48"
INTRODUCTION,0.040983606557377046,"and non-smooth components of GCN node features, i.e., projections of node features onto eigenspace
49"
INTRODUCTION,0.041728763040238454,"M and its orthogonal complement M⊥, resp. The empirical results of [27] indicate that both smooth
50"
INTRODUCTION,0.042473919523099854,"and non-smooth components of node features are crucial for accurate node classification, while
51"
INTRODUCTION,0.043219076005961254,"the ratio between smooth and non-smooth components to achieve optimal accuracy is unknown and
52"
INTRODUCTION,0.043964232488822655,"task-dependent. Furthermore, [4] proves that the Dirichlet energy – another smoothness measure for
53"
INTRODUCTION,0.044709388971684055,"node features – goes to zero when the depth of GCN with ReLU or leaky ReLU goes to infinity.
54"
INTRODUCTION,0.045454545454545456,"A crucial step in the proofs of [27, 4] is that ReLU and leaky ReLU reduce the distance of feature
55"
INTRODUCTION,0.046199701937406856,"vectors to M and their Dirichlet energy. However, [4] points out that over-smoothing – characterized
56"
INTRODUCTION,0.04694485842026826,"by the distance of features to eigenspace M or the Dirichlet energy – is a misnomer; the real
57"
INTRODUCTION,0.04769001490312966,"smoothness should be characterized by a normalized smoothness, e.g., normalizing the Dirichlet
58"
INTRODUCTION,0.04843517138599106,"energy by the magnitude of the features. The ratio between smooth and non-smooth components
59"
INTRODUCTION,0.04918032786885246,"of node features – studied in [27] – is closely related to the normalized smoothness. Nevertheless,
60"
INTRODUCTION,0.04992548435171386,"analyzing the normalized smoothness of node features learned by GCN with ReLU or leaky ReLU
61"
INTRODUCTION,0.05067064083457526,"remains an open problem [4]. Moreover, it is interesting to ask if analyzing the normalized smoothness
62"
INTRODUCTION,0.05141579731743666,"can result in any new understanding of GCN features and algorithms to improve GCN’s performance.
63"
OUR CONTRIBUTION,0.05216095380029806,"1.1
Our contribution
64"
OUR CONTRIBUTION,0.05290611028315946,"We aim to (1) establish a new geometric understanding of how GCL smooths GCN features and
65"
OUR CONTRIBUTION,0.05365126676602087,"(2) develop an efficient algorithm to let GCN and related models learn node features with a desired
66"
OUR CONTRIBUTION,0.05439642324888227,"normalized smoothness to improve node classification. We summarize our main contributions towards
67"
OUR CONTRIBUTION,0.05514157973174367,"achieving our goal as follows:
68"
OUR CONTRIBUTION,0.05588673621460507,"• We prove that there is a high-dimensional sphere underlying the input and output vectors of ReLU
69"
OUR CONTRIBUTION,0.05663189269746647,"or leaky ReLU. This geometric characterization not only implies theories in [27, 4] but also informs
70"
OUR CONTRIBUTION,0.05737704918032787,"that adjusting the projection of input onto eigenspace M can alter the smoothness of the output
71"
OUR CONTRIBUTION,0.05812220566318927,"vectors. See Section 3 for details.
72"
OUR CONTRIBUTION,0.05886736214605067,"• We show that both ReLU and leaky ReLU reduce the distance of node features to eigenspace M,
73"
OUR CONTRIBUTION,0.05961251862891207,"i.e., ReLU and leaky ReLU smooth their input vectors without considering their magnitude. In
74"
OUR CONTRIBUTION,0.06035767511177347,"contrast, when taking the magnitude into account, ReLU and leaky ReLU can increase, decrease, or
75"
OUR CONTRIBUTION,0.06110283159463487,"preserve the normalized smoothness of each dimension of the input vectors; see Sections 3 and 4.
76"
OUR CONTRIBUTION,0.06184798807749627,"• Inspired by our established geometric relationship between the input and output of ReLU or leaky
77"
OUR CONTRIBUTION,0.06259314456035768,"ReLU, we study how adjusting the projection of input onto eigenspace M affects both normalized
78"
OUR CONTRIBUTION,0.06333830104321908,"and unnormalized smoothness of the output vectors. We show that the distance of the output to
79"
OUR CONTRIBUTION,0.06408345752608048,"eigenspace M is no greater than that of the original input – no matter how we adjust the input by
80"
OUR CONTRIBUTION,0.06482861400894188,"changing its projection onto M. In contrast, adjusting the projection of input vectors onto M can
81"
OUR CONTRIBUTION,0.06557377049180328,"change the normalized smoothness of output to any desired value; see details in Section 4.
82"
OUR CONTRIBUTION,0.06631892697466468,"• Based on our theory, we propose a computationally efficient smoothness control term (SCT)
83"
OUR CONTRIBUTION,0.06706408345752608,"to let GCN and related models learn node features with a desired (normalized) smoothness to
84"
OUR CONTRIBUTION,0.06780923994038748,"improve node classification. We comprehensively validate the benefits of our proposed SCT in
85"
OUR CONTRIBUTION,0.06855439642324888,"improving node classification – for both homophilic and heterophilic graphs – using a few of the
86"
OUR CONTRIBUTION,0.06929955290611028,"most representative GCN-style models. See Sections 5 and 6 for details.
87"
OUR CONTRIBUTION,0.07004470938897168,"As far as we know, our work is the first thorough study of how ReLU and leaky ReLU affect the
88"
OUR CONTRIBUTION,0.07078986587183309,"smoothness of node features both with and without considering their magnitude.
89"
ADDITIONAL RELATED WORKS,0.07153502235469449,"1.2
Additional related works
90"
ADDITIONAL RELATED WORKS,0.07228017883755589,"Controlling the smoothness of node features to improve the performance of GCNs is another line of
91"
ADDITIONAL RELATED WORKS,0.07302533532041729,"related work. For instance, [37] designs a normalization layer to prevent node features from becoming
92"
ADDITIONAL RELATED WORKS,0.07377049180327869,"too similar to each other, and [40] constrains the Dirichlet energy to control the smoothness of node
93"
ADDITIONAL RELATED WORKS,0.07451564828614009,"features without considering the effects of nonlinear activation functions. While there has been effort
94"
ADDITIONAL RELATED WORKS,0.07526080476900149,"in understanding and alleviating the over-smoothing of GCNs and controlling the smoothness of
95"
ADDITIONAL RELATED WORKS,0.07600596125186289,"node features, there is a shortage of theoretical examination of how activation functions affect the
96"
ADDITIONAL RELATED WORKS,0.07675111773472429,"smoothness of node features, specifically accounting for the magnitude of features.
97"
NOTATION AND ORGANIZATION,0.07749627421758569,"1.3
Notation and Organization
98"
NOTATION AND ORGANIZATION,0.07824143070044709,"Notation. We denote the ℓ2-norm of a vector u as ∥u∥. For vectors u and v, we use ⟨u, v⟩, u ⊙v,
99"
NOTATION AND ORGANIZATION,0.07898658718330849,"and u ⊗v to denote their inner, Hadamard, and Kronecker product, resp. For a matrix A, we
100"
NOTATION AND ORGANIZATION,0.07973174366616989,"denote its (i, j)th entry, transpose, and inverse as Aij, A⊤, and A−1, resp. We denote the trace of
101"
NOTATION AND ORGANIZATION,0.08047690014903129,"A ∈Rn×n as Trace(A) = Pn
i=1 Aii. For two matrices A and B, we denote the Frobenius inner
102"
NOTATION AND ORGANIZATION,0.08122205663189269,"product as ⟨A, B⟩F := Trace(AB⊤) and the Frobenius norm of A as ∥A∥F :=
p"
NOTATION AND ORGANIZATION,0.08196721311475409,"⟨A, A⟩.
103"
NOTATION AND ORGANIZATION,0.08271236959761549,"Organization. We provide preliminaries in Section 2. In Section 3, we establish a geometric
104"
NOTATION AND ORGANIZATION,0.08345752608047691,"characterization of how ReLU and leaky ReLU affect the smoothness of their input vectors. We study
105"
NOTATION AND ORGANIZATION,0.08420268256333831,"the smoothness of each dimension of node features and take their magnitude into account in Section 4.
106"
NOTATION AND ORGANIZATION,0.08494783904619971,"Our proposed SCT is presented in Section 5. We comprehensively verify the efficacy of the proposed
107"
NOTATION AND ORGANIZATION,0.08569299552906111,"SCT in Section 6. Technical proofs and more experimental results are provided in the appendix.
108"
PRELIMINARIES AND EXISTING RESULTS,0.08643815201192251,"2
Preliminaries and Existing Results
109"
PRELIMINARIES AND EXISTING RESULTS,0.08718330849478391,"From the spectral graph theory [8], we can sort eigenvalues of matrix G in (1) as 1 = λ1 = . . . =
110"
PRELIMINARIES AND EXISTING RESULTS,0.08792846497764531,"λm > λm+1 ≥. . . ≥λn > −1, where m is the number of connected components of the graph. We
111"
PRELIMINARIES AND EXISTING RESULTS,0.08867362146050671,"decompose V = {vk}n
k=1 into m connected components V1, . . . , Vm. Let ui = (1{vk∈Vi})1≤k≤n be
112"
PRELIMINARIES AND EXISTING RESULTS,0.08941877794336811,"the indicator vector of Vi, i.e., the kth coordinate of ui is one if the kth node vk lies in the connected
113"
PRELIMINARIES AND EXISTING RESULTS,0.09016393442622951,"component Vi; zero otherwise. Moreover, let ei be the eigenvector associated with λi, then {ei}n
i=1
114"
PRELIMINARIES AND EXISTING RESULTS,0.09090909090909091,"forms an orthonormal basis of Rn. Notice that {ei}m
i=1 spans the eigenspace M – corresponding to
115"
PRELIMINARIES AND EXISTING RESULTS,0.09165424739195231,"eigenvalue 1 of matrix G, and {ei}n
i=m+1 spans the orthogonal complement of M, denoted by M⊥.
116"
PRELIMINARIES AND EXISTING RESULTS,0.09239940387481371,"The paper [27] connects the indicator vectors uis with the space M. In particular, we have
117"
PRELIMINARIES AND EXISTING RESULTS,0.09314456035767511,"Proposition 2.1 ([27]). All eigenvalues of matrix G lie in the interval (−1, 1]. Furthermore, the
118"
PRELIMINARIES AND EXISTING RESULTS,0.09388971684053651,"nonnegative vectors { ˜D
1
2 ui/∥˜D
1
2 ui∥}1≤i≤m form an orthonormal basis of M.
119"
PRELIMINARIES AND EXISTING RESULTS,0.09463487332339791,"For any matrix H := [h1, . . . , hn] ∈Rd×n, we have the decomposition H = HM + HM⊥
120"
PRELIMINARIES AND EXISTING RESULTS,0.09538002980625931,"with HM
= Pm
i=1 Heie⊤
i
and HM⊥
= Pn
i=m+1 Heie⊤
i
such that ⟨HM, HM⊥⟩F
=
121"
PRELIMINARIES AND EXISTING RESULTS,0.09612518628912071,"Trace
  Pm
i=1 Heie⊤
i (Pn
j=m+1 Heje⊤
j )⊤
= 0, implying that ∥H∥2
F = ∥HM∥2
F + ∥HM⊥∥2
F .
122"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.09687034277198212,"2.1
Existing smoothness notions of node features
123"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.09761549925484352,"Distance to the eigenspace M. Oono et al. [27] study the smoothness of features H := [h1, . . . , hn]
124"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.09836065573770492,"using their distance to the eigenspace M as an unnormalized smoothness notion.
125"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.09910581222056632,"Definition 2.2 ([27]). Let Rd ⊗M be the subspace of Rd×n consisting of the sum Pm
i=1 wi ⊗ei,
where wi ∈Rd and {ei}m
i=1 is an orthonormal basis of the eigenspace M. Then we define ∥H∥M⊥
– the distance of node features H to the eigenspace M – as follows:"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.09985096870342772,"∥H∥M⊥:=
inf
Y ∈Rd⊗M ∥H −Y ∥F =
H − m
X"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10059612518628912,"i=1
Heie⊤
i

F ."
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10134128166915052,"With the decomposition H = HM + HM⊥, ∥· ∥M⊥can be related to ∥· ∥F as follows:
126"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10208643815201192,"∥H∥M⊥= ∥H −HM∥F = ∥HM⊥∥F .
(3)"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10283159463487332,"Dirichlet energy. The paper [4] studies the unnormalized smoothness of node features using Dirichlet
127"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10357675111773472,"energy, which is defined as follows:
128"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10432190760059612,"Definition 2.3 ([4]). Let ˜∆= I −G be the (augmented) normalized Laplacian, then the Dirichlet
129"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10506706408345752,"energy ∥H∥E of node features H is defined by ∥H∥2
E := Trace(H ˜∆H⊤).
130"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10581222056631892,"Normalized Dirichlet energy. [4] points out that the real smoothness of node features H should be
131"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10655737704918032,"measured by the normalized Dirichlet energy Trace(H ˜∆H⊤)/∥H∥2
F . This normalized measurement
132"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10730253353204174,"is essential because data often originates from various sources with diverse measurement units or
133"
EXISTING SMOOTHNESS NOTIONS OF NODE FEATURES,0.10804769001490314,"scales. By normalization, we can mitigate biases resulting from these different scales.
134"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.10879284649776454,"2.2
Two existing theories of over-smoothing
135"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.10953800298062594,"Let λ = max{|λi| | λi < 1} be the second largest magnitude of G’s eigenvalues, and sl be the largest
136"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11028315946348734,"singular value of weight matrix W l. [27] shows that ∥Hl∥M⊥≤slλ∥Hl−1∥M⊥under GCL when
137"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11102831594634874,"σ is ReLU. Therefore, ∥Hl∥M⊥→0 as l →∞if slλ < 1, indicating node features converge to M
138"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11177347242921014,"and results in over-smoothing. A crucial step in the analysis in [27] is that ∥σ(Z)∥M⊥≤∥Z∥M⊥, for
139"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11251862891207154,"any matrix Z when σ is ReLU, i.e., ReLU reduces the distance to M. [27] points out that it is hard
140"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11326378539493294,"to extend the above result to other activation functions even leaky ReLU.
141"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11400894187779434,"Instead of considering ∥H∥M⊥, [4] shows that ∥Hl∥E ≤slλ∥Hl−1∥E under GCL when σ is
142"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11475409836065574,"ReLU or leaky ReLU. Hence, ∥Hl∥E →0 as l →∞, implying over-smoothing of GCNs. Note that
143"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11549925484351714,"∥H∥M⊥= 0 or ∥Hl∥E = 0 indicates homogeneous node features. The proof in [4] applies to GCN
144"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11624441132637854,"with both ReLU and leaky ReLU by establishing the inequality ∥σ(Z)∥E ≤∥Z∥E for any matrix Z.
145"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11698956780923994,"3
Effects of Activation Functions: A Geometric Characterization
146"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11773472429210134,"In this section, we present a geometric relationship between the input and output vectors of ReLU or
147"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11847988077496274,"leaky ReLU. We use ∥H∥M⊥as the unnormalized smoothness notion for all subsequent analyses
148"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11922503725782414,"since we observe that ∥H∥M⊥and ∥H∥E are equivalent as seminorms. In particular, we have
149"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.11997019374068554,"Proposition 3.1. ∥H∥M⊥and ∥H∥E are two equivalent seminorms, i.e., there exist two constants
150"
TWO EXISTING THEORIES OF OVER-SMOOTHING,0.12071535022354694,"α, β > 0 s.t. α∥H∥M⊥≤∥H∥E ≤β∥H∥M⊥, for any H ∈Rd×n.
151"
RELU,0.12146050670640834,"3.1
ReLU
152"
RELU,0.12220566318926974,"Let σ(x) = max{x, 0} be ReLU. The first main result of this paper is that there is a high-dimensional
153"
RELU,0.12295081967213115,"sphere underlying the input and output of ReLU; more precisely, we have
154"
RELU,0.12369597615499255,"Proposition 3.2 (ReLU). For any Z = ZM + ZM⊥∈Rd×n, let H = σ(Z) = HM + HM⊥.
Then HM⊥lies on the high-dimensional sphere centered at ZM⊥/2 with radius"
RELU,0.12444113263785395,"r :=
 
∥ZM⊥/2∥2
F −⟨HM, HM −ZM⟩F
1/2."
RELU,0.12518628912071536,"In particular, HM⊥lies inside the ball centered at ZM⊥/2 with radius ∥ZM⊥/2∥F and hence we
155"
RELU,0.12593144560357675,"have ∥H∥M⊥≤∥Z∥M⊥.
156"
LEAKY RELU,0.12667660208643816,"3.2
Leaky ReLU
157"
LEAKY RELU,0.12742175856929955,"Now we consider leaky ReLU σa(x) = max{x, ax}, where 0 < a < 1 is a positive scalar. Similar
158"
LEAKY RELU,0.12816691505216096,"to ReLU, we have the following result for leaky ReLU
159"
LEAKY RELU,0.12891207153502235,"Proposition 3.3 (Leaky ReLU). For any Z = ZM + ZM⊥∈Rd×n, let H = σa(Z) = HM +
HM⊥. Then HM⊥lies on the high-dimensional sphere centered at (1 + a)ZM⊥/2 with radius"
LEAKY RELU,0.12965722801788376,"ra :=
 
∥(1 −a)ZM⊥/2∥2
F −⟨HM −ZM, HM −aZM⟩F
1/2."
LEAKY RELU,0.13040238450074515,"In particular, HM⊥lies inside the ball centered at (1 + a)ZM⊥/2 with radius ∥(1 −a)ZM⊥/2∥F
160"
LEAKY RELU,0.13114754098360656,"and hence we see that a∥Z∥M⊥≤∥H∥M⊥≤∥Z∥M⊥.
161"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13189269746646795,"3.3
Implications of the above geometric characterizations
162"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13263785394932937,"Propositions 3.2 and 3.3 imply that the precise location of HM⊥(or ∥HM⊥∥F = ∥H∥M⊥) depends
163"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13338301043219075,"on the center and the radius r or ra. Given a fixed ZM⊥, the center of the spheres remains unchanged,
164"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13412816691505217,"and r and ra are only affected by changes in ZM. This observation motivates us to investigate how
165"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13487332339791355,"changes in ZM impact ∥H∥M⊥, i.e., the unnormalized smoothness of node features.
166"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13561847988077497,"Propositions 3.2 and 3.3 imply both ReLU and leaky ReLU reduce the distance of node features to
167"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13636363636363635,"eigenspace M, i.e. ∥H∥M⊥≤∥Z∥M⊥. Moreover, this inequality is independent of ZM; consider
168"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13710879284649777,"Z, Z′ ∈Rd×n s.t. ZM⊥= Z′
M⊥but ZM ̸= Z′
M. Let H and H′ be the output of Z and Z′ via
169"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13785394932935915,"ReLU or leaky ReLU, resp. Then we have ∥H∥M⊥≤∥Z∥M⊥and ∥H′∥M⊥≤∥Z′∥M⊥. Since
170"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13859910581222057,"ZM⊥= Z′
M⊥, we deduce that ∥H′∥M⊥≤∥Z∥M⊥. In other words, when ZM⊥= Z′
M⊥is fixed,
171"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.13934426229508196,"changing ZM to Z′
M can change the unnormalized smoothness of the output features but cannot
172"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.14008941877794337,"change the fact that ReLU and leaky ReLU smooth node features; we demonstrate this result in
173"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.14083457526080476,"Fig. 1a) in Section 4.1. Notice that without considering the nonlinear activation function, changing
174"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.14157973174366617,"ZM does not affect the unnormalized smoothness of node features measured by ∥H∥M⊥.
175"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.14232488822652756,"In contrast to the unnormalized smoothness, if one considers the normalized smoothness, we find
176"
IMPLICATIONS OF THE ABOVE GEOMETRIC CHARACTERIZATIONS,0.14307004470938897,"that adjusting ZM can result in a less smooth output; we will discuss this in Section 4.1.
177"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.14381520119225039,"4
How Adjusting ZM Affects the Smoothness of the Output
178"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.14456035767511177,"Throughout this section, we let Z and H be the input and output of ReLU or leaky ReLU. The
179"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1453055141579732,"smoothness notions based on the distance of feature to M or their Dirichlet energy do not account
180"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.14605067064083457,"for the magnitude of each dimension of the features; [4] points out that analyzing the normalized
181"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.146795827123696,"smoothness of features Z, given by ∥Z∥E/∥Z∥F , is an open problem. However, these two smooth-
182"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.14754098360655737,"ness notions aggregate the smoothness of node features across all dimensions; when the magnitude
183"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1482861400894188,"of some dimensions is much larger than others, the smoothness will be dominated by them.
184"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.14903129657228018,"Motivated by the discussion in Section 3.3, we study the disparate effects of adjusting ZM on the
185"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1497764530551416,"normalized and unnormalized smoothness in this section. For the sake of simplicity, we assume
186"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.15052160953800298,"the graph is connected (m = 1); all the following results can be extended to graphs with multiple
187"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1512667660208644,"connected components easily. Due to the equivalence between seminorms ∥· ∥M and ∥· ∥E, we
188"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.15201192250372578,"introduce the following definition of the dimension-wise normalized smoothness of node features.
189"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1527570789865872,"Definition 4.1. Let Z ∈Rd×n be the features over n nodes with z(i) ∈Rn being its ith row, i.e., the
ith dimension of the features over all nodes. We define the normalized smoothness of z(i) as follows:"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.15350223546944858,"s(z(i)) := ∥z(i)
M∥/∥z(i)∥,"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.15424739195231,"where we set s(z(i)) = 1 when z(i) = 0.
190"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.15499254843517138,"Remark 4.2. Notice that the normalized smoothness s(z(i)) = ∥z(i)
M∥/∥z(i)∥is closely related to the
191"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1557377049180328,"ratio between the smooth and non-smooth components of node features ∥z(i)
M∥/∥z(i)
M⊥∥.
192"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.15648286140089418,"The graph is connected implies that z(i)
M = ⟨z(i), e1⟩e1 and ∥z(i)
M∥= |⟨z(i), e1⟩|. Without ambiguity,
193"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1572280178837556,"we write z for z(i) and e for e1 – the eigenvector of G associated with the eigenvalue 1. Moreover,
194"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.15797317436661698,"we have
195"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1587183308494784,s(z) = ∥zM∥
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.15946348733233978,"∥z∥
= |⟨z, e⟩|"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1602086438152012,"∥z∥
=
|⟨z, e⟩|
∥z∥· ∥e∥⇒0 ≤s(z) ≤1,
(4)"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.16095380029806258,"It is evident that the larger s(z) is, the smoother the node feature z is1. In fact, we have"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.161698956780924,"s(z)2 +
∥z∥M⊥ ∥z∥"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.16244411326378538,"2
= ∥zM∥2"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1631892697466468,"∥z∥2
+ ∥zM⊥∥2"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.16393442622950818,"∥z∥2
= 1,"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1646795827123696,"where ∥z∥M⊥/∥z∥decreases as s(z) increases.
196"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.16542473919523099,"1.0
0.5
0.0
0.5
1.0
Parameter ( ) 0 5 10"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1661698956780924,Smoothness (s) ||z||
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.16691505216095381,|| (z )||
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1676602086438152,"||
a(z )||"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.16840536512667661,"1.0
0.5
0.0
0.5
1.0
1.5
2.0
Parameter ( ) 0.0 0.5 1.0"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.169150521609538,Smoothness (s)
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.16989567809239942,"s(z)
s( (z ))
s(
a(z ))"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1706408345752608,"a) Smoothness
b) Normalized smoothness
Figure 1: Contrasting the effects of varying parame-
ter α on the smoothness and normalized smoothness
of output features σ(zα) and σa(zα). The disconti-
nuity of s(σ(zα)) in b) comes from the definition of
normalized smoothness. Note that s(z) = 1 if z = 0,
and σ(zα) can become 0 when α is large enough."
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.17138599105812222,"To discuss how the smoothness s(h) = s(σ(z))
or s(σa(z)) can be adjusted by changing zM, we
consider the function"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1721311475409836,z(α) = z −αe.
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.17287630402384502,It is clear that
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1736214605067064,"z(α)M⊥= zM⊥and z(α)M = zM −αe,"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.17436661698956782,"where we see that α only alters zM while pre-
197"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1751117734724292,"serves zM⊥. Moreover, it is evident that
198"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.17585692995529062,s(z(α)) = s
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.176602086438152,1 −∥z(α)M⊥∥2
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.17734724292101342,"∥z(α)∥2
= s"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1780923994038748,1 −∥zM⊥∥2
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.17883755588673622,∥z(α)∥2 .
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1795827123695976,"It follows that s(z(α)) = 1 if and only if zM⊥= 0 (include the case z = 0), showing that when
199"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.18032786885245902,"zM⊥= 0, the vector z is the smoothest one.
200"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1810730253353204,"4.1
The disparate effects of α on ∥· ∥M⊥and s(·): Empirical results
201"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.18181818181818182,"Let us empirically study possible values that the unnormalized smoothness ∥σ(z(α))∥M⊥,
202"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1825633383010432,"∥σa(z(α))∥M⊥and the normalized smoothness s(σ(z(α))), s(σa(z(α))) can take when α varies.
203"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.18330849478390462,"1Here, z ∈Rn is a vector whose ith entry is the 1D feature associated with node i."
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.184053651266766,"We denote zα := z(α) = z −αe. We consider a connected synthetic graph with 100 nodes, and each
204"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.18479880774962743,"node is assigned a random degree between 2 to 10. Then we assign an initial node feature z ∈R100,
205"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1855439642324888,"sampled uniformly on the interval [−1.5, 1.5], to the graph with each node feature being a scalar.
206"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.18628912071535023,"Also, we compute e by the formula e = ˜D
1
2 u/∥˜D
1
2 u∥from Proposition 2.1, where u ∈R100 is
207"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1870342771982116,"the vector whose entries are all ones and ˜D is the (augmented) degree matrix. We examine two
208"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.18777943368107303,"different smoothness notions for the input z and the output σ(zα) and σa(zα), where the smoothness
209"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.1885245901639344,"is measured for various values of the smoothness control parameter α ∈[−1.5, 1.5]. In Fig. 1a), we
210"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.18926974664679583,"study the unnormalized smoothness measured by ∥·∥M⊥; we see that ∥σ(zα)∥M⊥and ∥σa(zα)∥M⊥
211"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19001490312965721,"are always no greater than ∥z∥M⊥. This coincides with the discussion in Section 3.3; adjusting
212"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19076005961251863,"the projection of z onto the eigenspace M can not change the fact that ∥σ(zα)∥M⊥≤∥z∥M⊥
213"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19150521609538004,"and ∥σa(zα)∥M⊥≤∥z∥M⊥. Nevertheless, an interesting result is that altering the eigenspace
214"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19225037257824143,"projection can adjust the unnormalized smoothness of the output: notice that altering the eigenspace
215"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19299552906110284,"projection does not change its distance to M, i.e., the smoothness of the input is unchanged, but the
216"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19374068554396423,"smoothness of the output after activation function can be changed.
217"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19448584202682564,"In contrast, when studying the normalized smoothness s(·) in Fig. 1b), we find that s(σ(z(α)))
218"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19523099850968703,"and s(σa(z(α))) can be adjusted by α to values smaller than s(z). More precisely, we see that by
219"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19597615499254845,"adjusting α, s(σ(z(α))) and s(σa(z(α))) can achieve most of the values in [0, 1]. In other words,
220"
HOW ADJUSTING ZM AFFECTS THE SMOOTHNESS OF THE OUTPUT,0.19672131147540983,"both smoother and less smooth features can be obtained by adjusting α.
221"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.19746646795827125,"4.2
Theoretical results on the smooth effects of ReLU and leaky ReLU
222"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.19821162444113263,"In this subsection, we build theoretical understandings of the above empirical findings on the
223"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.19895678092399405,"achievable smoothness shown in Fig. 1. Notice that if zM⊥= 0, the inequalities presented in
224"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.19970193740685543,"Propositions 3.2 and 3.3 indicate that ∥σ(z(α))∥M⊥and ∥σa(z(α))∥M⊥vanish. So we have
225"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20044709388971685,"s(σ(z(α))) = 1 for any α when zM⊥= 0. Then we may assume zM⊥̸= 0 for the following study.
226"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20119225037257824,"Proposition 4.3 (ReLU). Suppose zM⊥̸= 0. Let h(α) = σ(z(α)) with σ being ReLU, then
227"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20193740685543965,"min
α s(h(α)) = sP"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20268256333830104,"xi=max x di
Pn
j=1 dj
and max
α
s(h(α)) = 1,"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20342771982116245,where x := ˜D−1
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20417287630402384,"2 z, max x = max1≤i≤n xi, and ˜D is the augmented degree matrix with diagonals
228"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20491803278688525,"d1, d2, . . . , dn. In particular, the normalized smoothness s(h(α)) is monotone increasing as α
229"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20566318926974664,"decreases whenever α < ∥˜D
1
2 un∥max x and it has range [minα s(h(α)), 1].
230"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20640834575260805,"Proposition 4.4 (Leaky ReLU). Suppose zM⊥̸= 0. Let h(α) = σa(z(α)) with σa being leaky
231"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20715350223546944,"ReLU, then (1) minα s(h(α)) = 0, and (2) supα s(h(α)) = 1 and s(h(α)) has range [0, 1).
232"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20789865871833085,"Proposition 4.4 also holds for other variants of ReLU, e.g., ELU2 and SELU3.; see Appendix C. We
233"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20864381520119224,"summarize Propositions 3.2, 3.3, 4.3, and 4.4 in the following corollary, which qualitatively explains
234"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.20938897168405365,"the empirical results in Fig. 1.
235"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21013412816691504,"Corollary 4.5. Suppose zM⊥̸= 0. Let h(α) = σ(z(α)) or σa(z(α)) with σ being ReLU and σa
236"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21087928464977646,"being leaky ReLU. Then we have ∥z∥M⊥≥∥h(α)∥M⊥for any α ∈R; however, s(h(α)) can be
237"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21162444113263784,"smaller than, larger than, or equal to s(z) for different values of α.
238"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21236959761549926,"Propositions 4.3 and 4.4, and Corollary 4.5, provide a theoretical basis for the empirical results in
239"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21311475409836064,"Fig. 1. Moreover, our results indicate that for any given vector z, altering zM can change both the
240"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21385991058122206,"unnormalized and the normalized smoothness of the output vector h = σ(z) or σa(z). In particular,
241"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21460506706408347,"the normalized smoothness of h = σ(z) or σa(z) can be adjusted to any value in the range shown
242"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21535022354694486,"in Propositions 4.3 and 4.4. This provides us with insights to control the smoothness of features to
243"
THEORETICAL RESULTS ON THE SMOOTH EFFECTS OF RELU AND LEAKY RELU,0.21609538002980627,"improve the performance of GCN and we will discuss this in the next section.
244"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.21684053651266766,"5
Controlling Smoothness of Node Features
245"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.21758569299552907,"We do not know how smooth features are ideal for a given node classification task. Nevertheless, our
246"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.21833084947839046,"theory indicates that both normalized and unnormalized smoothness of the output of each GCL can
247"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.21907600596125187,"be adjusted by altering the input’s projection onto M. As such, we propose the following learnable
248"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.21982116244411326,"smoothness control term to modulate the smoothness of each dimension of the learned node features
249"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22056631892697467,"Bl
α = m
X"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22131147540983606,"i=1
αl
ie⊤
i ,
(5)"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22205663189269748,"2The ELU function is defined by f(x) = max(x, 0) + min(0, a · (ex −1)) where a > 0.
3The SELU function is defined by f(x) = c(max(x, 0) + min(0, a · (ex −1))) where a, c > 0."
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22280178837555886,"where l is the layer index, {ei}m
i=1 is the orthonormal basis of the eigenspace M, and αl := {αl
i}m
i=1
250"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22354694485842028,"is a collection of learnable vectors with αl
i ∈Rd being approximated by a multi-layer perceptron
251"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22429210134128166,"(MLP). The detailed configuration of αl
i will be specified in each experiment later. One can see that
252"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22503725782414308,"Bl
α always lies in Rd ⊗M. We integrate SCT into GCL, resulting in
253"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22578241430700446,"Hl = σ(W lHl−1G + Bl
α).
(6)"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22652757078986588,"We call the corresponding model GCN-SCT. Again, the idea is that we alter the component in
254"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22727272727272727,"eigenspace to control the smoothness of features. Each dimension of Hl can be smoother, less
255"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22801788375558868,"smooth, or the same as Hl−1 in normalized smoothness, though Hl gets closer to M than Hl−1.
256"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22876304023845007,"To design SCT, we introduce a learnable matrix Al ∈Rd×m for layer l, whose columns are αl
i, where
257"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.22950819672131148,"m is the dimension of the eigenspace M and d is the dimension of the features. We observe in our
258"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23025335320417287,"experiments that the SCT performs best when informed by degree pooling over the subcomponents of
259"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23099850968703428,"the graph. The matrix of the orthogonal basis vectors, denoted by Q := [e1, . . . , em] ∈Rn×m, is used
260"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23174366616989567,"to perform pooling HlQ for input Hl. In particular, we let Al = W ⊙(HlQ), where W ∈Rd×m
261"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23248882265275708,"is learnable and performs pooling over Hl using the eigenvectors Q. The second architecture uses
262"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23323397913561847,"a residual connection with hyperparameter βl = log(θ/l + 1) and learnable matrices W0, W1 ∈
263"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23397913561847988,"Rd×d and the softmax function ϕ. Resulting in Al = ϕ(HlQ) ⊙(βlW0H0Q + (1 −βl)W1HlQ). In
264"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23472429210134127,"Section 6, we use the first architecture for GCN-SCT as GCN uses only Hl information at each
265"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23546944858420268,"layer. We use the second architecture for GCNII-SCT and EGNN-SCT which use both H0 and Hl
266"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23621460506706407,"information at each layer. There are two particular advantages of the above design of SCT: (1) it can
267"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23695976154992549,"effectively change the normalized smoothness of the learned features, and (2) it is computationally
268"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23770491803278687,"efficient since we only use the eigenvectors corresponding to the eigenvalue 1 of matrix G, which is
269"
CONTROLLING SMOOTHNESS OF NODE FEATURES,0.23845007451564829,"determined based on the connectivity of the graph.
270"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2391952309985097,"5.1
Integrating SCT into other GCN-style models
271"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2399403874813711,"In this subsection, we present other usages of the proposed SCT. Due to the page limit, we carefully
272"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2406855439642325,"select two other most representative models. The first example is GCNII [6], GCNII extends GCN
273"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2414307004470939,"to express an arbitrary polynomial filter rather than the Laplacian polynomial filter and achieves
274"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2421758569299553,"state-of-the-art (SOTA) performance among GCN-style models on various tasks [6, 23], and we
275"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2429210134128167,"aim to show that SCT can even improve the accuracy of the GCN-style model that achieves SOTA
276"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2436661698956781,"performance on many node classification tasks. The second example is energetic GNN (EGNN) [40],
277"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2444113263785395,"which controls the smoothness of node features by constraining the lower and upper bounds of the
278"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2451564828614009,"Dirichlet energy of features and assuming the activation function is linear. In this case, we aim to
279"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2459016393442623,"show that our new theoretical understanding of the role of activation functions and the proposed SCT
280"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2466467958271237,"can boost the performance of EGNN with considering nonlinear activation functions.
281"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2473919523099851,"GCNII. Each GCNII layer uses a skip connection to the initial layer H0 and given as follows:
282"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2481371087928465,"Hl = σ
 
((1 −αl)Hl−1G + αlH0)((1 −βl)I + βlW l)

,"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2488822652757079,"where αl, βl ∈(0, 1) are learnable scalars. We integrate SCT Bl
α into GCNII, resulting in the
283"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2496274217585693,"following GCNII-SCT layers
284"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2503725782414307,"Hl = σ
 
((1 −αl)Hl−1G + αlH0)((1 −βl)I + βlW l) + Bl
α

,"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2511177347242921,"where the residual connection and identity mapping are consistent with GCNII.
285"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2518628912071535,"EGNN. Each EGNN layer can be written as follows:
286"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2526080476900149,"Hl = σ
 
W l(c1H0 + c2Hl−1 + (1 −cmin)Hl−1G)

,
(7)"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2533532041728763,"where c1, c2 are learnable weights that satisfy c1 + c2 = cmin with cmin being a hyperparameter. To
constrain Dirichlet energy, EGNN initializes trainable weights W l as a diagonal matrix with explicit
singular values and regularizes them to keep the orthogonality during the model training. Ignoring
the activation function σ, Hl – node features at layer l of EGNN satisfies"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2540983606557377,"cmin∥H0∥E ≤∥Hl∥E ≤cmax∥H0∥E,"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2548435171385991,"where cmax is the square of the maximal singular value of the initialization of W 1. Similarly, we
287"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2555886736214605,"modify EGNN to result in the following EGNN-SCT layer
288"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2563338301043219,"Hl = σ
 
W l((1 −cmin)Hl−1G + c1H0 + c2Hl−1) + Bl
α

,"
INTEGRATING SCT INTO OTHER GCN-STYLE MODELS,0.2570789865871833,"where everything remains the same as the EGNN layer except that we add our proposed SCT Bl
α.
289"
EXPERIMENTS,0.2578241430700447,"6
Experiments
290"
EXPERIMENTS,0.2585692995529061,"In this section, we comprehensively demonstrate the effects of SCT – in the three most representative
291"
EXPERIMENTS,0.2593144560357675,"GCN-style models discussed in Section 5 – using various node classification benchmarks. The
292"
EXPERIMENTS,0.2600596125186289,"purpose of all experiments in this section is to verify the efficacy of the proposed SCT – motivated
293"
EXPERIMENTS,0.2608047690014903,"by our theoretical results – for GCN-style models. We consider the citation datasets (Cora, Citeseer,
294"
EXPERIMENTS,0.2615499254843517,"PubMed, Coauthor-Physics, Ogbn-arxiv), web knowledge-base datasets (Cornell, Texas, Wisconsin),
295"
EXPERIMENTS,0.26229508196721313,"and Wikipedia network datasets (Chameleon, Squirrel). We provide additional dataset details in
296"
EXPERIMENTS,0.2630402384500745,"Appendix D.1. We implement baseline GCN [20] and GCNII [6] (without weight sharing) using PyG
297"
EXPERIMENTS,0.2637853949329359,"(Pytorch Geometric) [10]. Baseline EGNN [40] is implemented using the public code4.
298"
NODE FEATURE TRAJECTORY,0.26453055141579734,"6.1
Node feature trajectory
299"
NODE FEATURE TRAJECTORY,0.26527570789865873,"a) α = −0.25
b) α = 0.0
c) α = 1.0
Figure 2: Node feature trajectories, with colorized
magnitude, for varying smoothness control param-
eter α. For classical GCN b), the node features
converge to the eigenspace M (red dashed line)."
NODE FEATURE TRAJECTORY,0.2660208643815201,"We visualize the trajectory of the node features, fol-
300"
NODE FEATURE TRAJECTORY,0.2667660208643815,"lowing [27], for a graph with two nodes connected
301"
NODE FEATURE TRAJECTORY,0.26751117734724295,"by an edge and 1D node feature. In this case, (6)
302"
NODE FEATURE TRAJECTORY,0.26825633383010433,"becomes h1 = σ(wh0G + bα), where w = 1.2 in
303"
NODE FEATURE TRAJECTORY,0.2690014903129657,"our experiment, h0, h1, bα ∈R2, and G ∈R2×2.
304"
NODE FEATURE TRAJECTORY,0.2697466467958271,"We use a matrix G = [0.592, 0.194; 0.194, 0.908]
305"
NODE FEATURE TRAJECTORY,0.27049180327868855,"whose largest eigenvalue is 1. Twenty initial node
306"
NODE FEATURE TRAJECTORY,0.27123695976154993,"feature vectors h0 are sampled evenly in the domain
307"
NODE FEATURE TRAJECTORY,0.2719821162444113,"[−1, 1] × [−1, 1]. Fig. 2 shows the trajectories in
308"
NODE FEATURE TRAJECTORY,0.2727272727272727,"relation to the eigenspace M (red dashed line). In Fig 2a), one can see that some trajectories do not
309"
NODE FEATURE TRAJECTORY,0.27347242921013415,"directly converge to M. In Fig. 2b) when α = 0.0, GCL is recovered and all trajectories converge to
310"
NODE FEATURE TRAJECTORY,0.27421758569299554,"M. In Fig. 2c), large values of α enable the features to significantly deviate from M initially. We
311"
NODE FEATURE TRAJECTORY,0.2749627421758569,"observe that the parameter α can effectively change the trajectory of features.
312"
NODE FEATURE TRAJECTORY,0.2757078986587183,"Layers
2
4
16
32
Cora
GCN/GCN-SCT
81.1/82.9
80.4/82.8
64.9/71.4
60.3/67.2
GCNII/GCNII-SCT
82.2/83.8
82.6/84.3
84.6/84.8
85.4/85.5
EGNN/EGNN-SCT
83.2/84.1
84.2/84.5
85.4/83.3
85.3/82.0
Citeseer
GCN/GCN-SCT
70.3/69.9
67.6/67.7
18.3/55.4
25.0/51.0
GCNII/GCNII-SCT
68.2/72.8
68.9/72.8
72.9/73.8
73.4/73.4
EGNN/EGNN-SCT
72.0/73.1
71.9/72.0
72.4/72.6
72.3/72.9
PubMed
GCN/GCN-SCT
79.0/79.8
76.5/78.4
40.9/76.1
22.4/77.0
GCNII/GCNII-SCT
78.2/79.7
78.8/80.1
80.2/80.7
79.8/80.7
EGNN/EGNN-SCT
79.2/79.8
79.5/80.4
80.1/80.3
80.0/80.4
Coauthor-Physics
GCN/GCN-SCT
92.4/92.6 ± 1.6
92.1/92.5 ± 5.9
13.5/50.9 ± 15.0
13.1/43.6 ± 16.0
GCNII/GCNII-SCT
92.5/94.4 ± 0.4
92.9/94.2 ± 0.3
92.9/93.7 ± 0.7
92.9/94.1 ± 0.3
EGNN/EGNN-SCT
92.6/93.9 ± 0.7
92.9/94.1 ± 0.4
93.1/94.0 ± 0.7
93.3/93.8 ± 1.3
Ogbn-arxiv
GCN/GCN-SCT
70.4/72.1 ± 0.3
71.7/72.7 ± 0.3
70.6/72.3 ± 0.2
68.5/72.3 ± 0.3
GCNII/GCNII-SCT
70.1/72.0 ± 0.3
71.4/72.2 ± 0.2
71.5/72.4 ± 0.3
70.5/72.1 ± 0.3
EGNN/EGNN-SCT
68.4/68.5 ± 0.6
71.1/71.3 ± 0.5
72.7/72.8 ± 0.5
72.7/72.3 ± 0.5
Table 1: Accuracy for models of varying depth. We note vanishing gradients occur but not over-smoothing for
the accuracy drop using GCN-SCT with 16 or 32 layers. For Cora, Citeseer, and PubMed, we use a fixed split
with a single forward pass following [6]; only test accuracy is available in these experiments. For Coauthor-
Physics and Ogbn-arxiv, we use the splits from [40]; both test accuracy and standard deviation are reported. The
baseline results are copied from [6, 40] where the standard deviation was not reported. (Unit:%)"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.27645305514157975,"6.2
Baseline comparisons for node classification
313"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.27719821162444114,"Citation networks. We compare the three representative models discussed in Section 5, of different
314"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.2779433681073025,"depths, with and without SCT in Table 1. This task uses the citation datasets with fixed splits from
315"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.2786885245901639,"[35] for Cora, Citeseer, and Pubmed and splits from [40] for Coauthor-Physics and Ogbn-arxiv; a
316"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.27943368107302535,"detailed description of these datasets and splits are provided in Appendix D. Following [6], we use a
317"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28017883755588674,"single training pass to minimize the negative log-likelihood loss using the Adam optimizer [19], with
318"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.2809239940387481,"1500 maximum epochs, and 100 epochs of patience. A grid search for possible hyperparameters is
319"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.2816691505216095,"listed in Table 5 in Appendix D. We accelerate the hyperparameter search by applying a Bayesian
320"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28241430700447095,"meta-learning algorithm [2] which minimizes the validation loss, and we run the search for 200
321"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28315946348733234,"iterations per model. In particular, Table 1 presents the best test accuracy between ReLU and leaky
322"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28390461997019373,"ReLU for GCN, GCNII, and all three models with SCT5. For the baseline EGNN, we follow [40]
323"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.2846497764530551,"using SReLU, a particular activation used for EGNN in [40]. These results show that SCT can boost
324"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28539493293591656,"4https://github.com/Kaixiong-Zhou/EGNN
5A comparison of the results using ReLU and leaky ReLU is presented in Appendix D."
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28614008941877794,"the classification accuracy of baseline models; in particular, the improvement can be remarkable for
325"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28688524590163933,"GCN and GCNII. However, EGNN-SCT (using ReLU or leaky ReLU) performs occasionally worse
326"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28763040238450077,"than EGNN (using SReLU), and this is because of the choice of activation functions. In Appendix D.3,
327"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28837555886736216,"we report the results of EGNN-SCT using SReLU, showing that EGNN-SCT outperforms EGNN in
328"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28912071535022354,"all tasks. In fact, SReLU is a shifted version of ReLU, and our theory for ReLU applies to SReLU as
329"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.28986587183308493,"well. The model size and computational time are reported in Table 4 in the appendix.
330"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.2906110283159464,"Table 1 also shows that even with SCT, the accuracy of GCN drops when the depth is 16 or 32. This
331"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29135618479880776,"motivates us to investigate the smoothness of the node features learned by GCN and GCN-SCT. Fig. 3
332"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29210134128166915,"plots the heatmap of the normalized smoothness of each dimension of the learned node features
333"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29284649776453053,"learned by GCN and GCN-SCT with 32 layers for Citeseer node classification. In these plots, the
334"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.293591654247392,"horizontal and vertical dimensions denote the feature dimension and the layer of the model, resp.
335"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29433681073025336,"We notice that the normalized smoothness of each dimension of the features – from layers 14 to 32
336"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29508196721311475,"learned by GCN – closes to 1, confirming that deep GCN learns homogeneous features. In contrast,
337"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29582712369597614,"the features learned by GCN-SCT are inhomogeneous, as shown in Fig. 3b). Therefore, we believe the
338"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.2965722801788376,"performance degradation of deep GCN-SCT is due to other factors. Compared to GCNII/GCNII-SCT
339"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29731743666169896,"and EGNN/EGNN-SCT, GCN-SCT does not use skip connections, which is known to help avoid
340"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29806259314456035,"vanishing gradients in training deep neural networks [16, 17]. In Appendix D.3, we show that training
341"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.29880774962742174,"GCN and GCN-SCT do suffer from the vanishing gradient issue; however, the other models do not.
342"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.2995529061102832,"Besides Citeseer, we notice similar behavior occurs for training GCN and GCN-SCT for Cora and
343"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30029806259314457,"Coauthor-Physics node classification tasks.
344"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30104321907600595,"1
8
16
dim 1 16 32 Layer 0.0 0.2 0.4 0.6 0.8 1.0"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30178837555886734,"1
8
16
dim 1 16 32 Layer 0.0 0.2 0.4 0.6 0.8 1.0"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3025335320417288,"a) GCN
b) GCN-SCT
Figure 3: The normalized smoothness – of
each dimension of the feature vectors at a
given layer – for a) GCN and b) GCN-SCT
on the Citeseer dataset with 32 layers and
16 hidden dimensions. GCN features be-
come entirely smooth since layer 14, while
GCN-SCT controls the smoothness for each
feature at any depth. Horizontal and verti-
cal axes represent the index of the feature
dimension and the intermediate layer, resp."
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30327868852459017,"Other datasets. We further compare different models
345"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30402384500745155,"trained on different datasets using 10-fold cross-validation
346"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30476900149031294,"and fixed 48/32/20% splits following [28]. Table 2 com-
347"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3055141579731744,"pares GCN and GCNII with and without SCT, using leaky
348"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30625931445603577,"ReLU, for classifying five heterophilic node classification
349"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30700447093889716,"datasets. We exclude EGNN as these heterophilic datasets
350"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30774962742175854,"are not considered in [40]. We report the average accu-
351"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30849478390462,"racy of GCN and GCNII from [6].
We tune all other
352"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30923994038748137,"models using a Bayesian meta-learning algorithm to max-
353"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.30998509687034276,"imize the mean validation accuracy. We report the best
354"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3107302533532042,"test accuracy for each model of depth searched over the set
355"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3114754098360656,"{2, 4, 8, 16, 32}. SCT can significantly improve the clas-
356"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.312220566318927,"sification accuracy of the baseline models. Table 2 also
357"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.31296572280178836,"contrasts the computational time (on Tesla T4 GPUs from
358"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3137108792846498,"Google Colab) per epoch of models that achieve the best
359"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3144560357675112,"test accuracy; the models using SCT can even save compu-
360"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3152011922503726,"tational time to achieve the best accuracy which is because
361"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.31594634873323396,"the best accuracy is achieved at a moderate depth (Table 8 in Appendix D.4 lists the mean and
362"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3166915052160954,"standard deviation for the test accuracies on all five datasets. Table 9 in Appendix D.4 lists the
363"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3174366616989568,"computational time per epoch for each model of depth 8, showing that using SCT only takes a small
364"
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.3181818181818182,amount of computational overhead.
BASELINE COMPARISONS FOR NODE CLASSIFICATION,0.31892697466467956,"Cornell
Texas
Wisconsin
Chameleon
Squirrel
52.70/55.95 (0.7/1.8)
52.16/62.16 (0.7/0.8)
45.88/54.71 (0.7/0.8)
28.18/38.44 (0.6/0.7)
23.96/35.31 (1.6/4.0)
74.86/75.41 (2.0/2.0)
69.46/83.34 (3.1/2.0)
74.12/86.08 (2.0/1.5)
60.61/64.52 (1.5/1.3)
38.47/47.51 (5.5/3.7)
Table 2: Mean test accuracy and average computational time per epoch (in the parenthesis) for the We-
bKB and WikipediaNetwork datasets with fixed 48/32/20% splits. First row: GCN/GCN-SCT. Second row:
GCNII/GCNII-SCT. (Unit:% for accuracy and ×10−2 second for computational time.)
365"
CONCLUDING REMARKS,0.319672131147541,"7
Concluding Remarks
366"
CONCLUDING REMARKS,0.3204172876304024,"In this paper, we establish a geometric characterization of how ReLU and leaky ReLU affect the
367"
CONCLUDING REMARKS,0.3211624441132638,"smoothness of the GCN features. We further study the dimension-wise normalized smoothness of the
368"
CONCLUDING REMARKS,0.32190760059612517,"learned node features, showing that activation functions not only smooth node features but also can
369"
CONCLUDING REMARKS,0.3226527570789866,"reduce or preserve the normalized smoothness of the features. Our theoretical findings inform the
370"
CONCLUDING REMARKS,0.323397913561848,"design of a simple yet effective SCT for GCN. The proposed SCT can change the smoothness, in
371"
CONCLUDING REMARKS,0.3241430700447094,"terms of both normalized and unnormalized smoothness, of the learned node features by GCN.
372"
CONCLUDING REMARKS,0.32488822652757077,"Limitations: Our proposed SCT provides provable guarantees for controlling the smoothness of
373"
CONCLUDING REMARKS,0.3256333830104322,"features learned by GCN and related models. A key aspect to establish our theoretical results is
374"
CONCLUDING REMARKS,0.3263785394932936,"demonstrating that, without SCT, the features of the vanilla model tend to be overly smooth; without
375"
CONCLUDING REMARKS,0.327123695976155,"this condition, SCT cannot ensure performance guarantees.
376"
BROADER IMPACTS,0.32786885245901637,"8
Broader Impacts
377"
BROADER IMPACTS,0.3286140089418778,"Our paper focuses on developing new theoretical understandings of the smoothness of node features
378"
BROADER IMPACTS,0.3293591654247392,"learned by graph convolutional networks. The paper is mainly theoretical. We do not see any potential
379"
BROADER IMPACTS,0.3301043219076006,"ethical issues in our research; all experiments are carried out using existing benchmark settings and
380"
BROADER IMPACTS,0.33084947839046197,"datasets.
381"
BROADER IMPACTS,0.3315946348733234,"Our paper brings new insights into building new graph neural networks with improved performance
382"
BROADER IMPACTS,0.3323397913561848,"over existing models, which is crucial for many applications. In particular, for applications where
383"
BROADER IMPACTS,0.3330849478390462,"graph neural network is the method of choice. We expect our approach to play a role in material
384"
BROADER IMPACTS,0.33383010432190763,"science and biophysics applications.
385"
REFERENCES,0.334575260804769,"References
386"
REFERENCES,0.3353204172876304,"[1] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
387"
REFERENCES,0.3360655737704918,"Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
388"
REFERENCES,0.33681073025335323,"Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint
389"
REFERENCES,0.3375558867362146,"arXiv:1806.01261, 2018.
390"
REFERENCES,0.338301043219076,"[2] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from
391"
REFERENCES,0.3390461997019374,"wandb.com.
392"
REFERENCES,0.33979135618479883,"[3] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and
393"
REFERENCES,0.3405365126676602,"deep locally connected networks on graphs. In 2nd International Conference on Learning
394"
REFERENCES,0.3412816691505216,"Representations, ICLR 2014, 2014.
395"
REFERENCES,0.342026825633383,"[4] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. arXiv preprint
396"
REFERENCES,0.34277198211624443,"arXiv:2006.13318, 2020.
397"
REFERENCES,0.3435171385991058,"[5] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the
398"
REFERENCES,0.3442622950819672,"over-smoothing problem for graph neural networks from the topological view. In Proceedings
399"
REFERENCES,0.3450074515648286,"of the AAAI Conference on Artificial Intelligence, volume 34, pages 3438–3445, 2020.
400"
REFERENCES,0.34575260804769004,"[6] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep
401"
REFERENCES,0.3464977645305514,"graph convolutional networks. In Hal Daumé III and Aarti Singh, editors, Proceedings of the
402"
REFERENCES,0.3472429210134128,"37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
403"
REFERENCES,0.3479880774962742,"Learning Research, pages 1725–1735. PMLR, 13–18 Jul 2020.
404"
REFERENCES,0.34873323397913564,"[7] Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph
405"
REFERENCES,0.349478390461997,"neural networks. In International Conference on Learning Representations, 2019.
406"
REFERENCES,0.3502235469448584,"[8] Fan RK Chung. Spectral graph theory, volume 92. American Mathematical Soc., 1997.
407"
REFERENCES,0.3509687034277198,"[9] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
408"
REFERENCES,0.35171385991058124,"on graphs with fast localized spectral filtering. Advances in neural information processing
409"
REFERENCES,0.3524590163934426,"systems, 29, 2016.
410"
REFERENCES,0.353204172876304,"[10] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric.
411"
REFERENCES,0.3539493293591654,"In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
412"
REFERENCES,0.35469448584202684,"[11] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. Combining neural
413"
REFERENCES,0.3554396423248882,"networks with personalized pagerank for classification on graphs. In International Conference
414"
REFERENCES,0.3561847988077496,"on Learning Representations, 2019.
415"
REFERENCES,0.356929955290611,"[12] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.
416"
REFERENCES,0.35767511177347244,"Neural message passing for quantum chemistry. In Proceedings of the 34th International
417"
REFERENCES,0.35842026825633383,"Conference on Machine Learning - Volume 70, ICML’17, page 1263–1272. JMLR.org, 2017.
418"
REFERENCES,0.3591654247391952,"[13] William Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
419"
REFERENCES,0.35991058122205666,"graphs. Advances in neural information processing systems, 30, 2017.
420"
REFERENCES,0.36065573770491804,"[14] William L Hamilton. Graph representation learning. Morgan & Claypool Publishers, 2020.
421"
REFERENCES,0.36140089418777943,"[15] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
422"
REFERENCES,0.3621460506706408,"and applications. arXiv preprint arXiv:1709.05584, 2017.
423"
REFERENCES,0.36289120715350226,"[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-
424"
REFERENCES,0.36363636363636365,"age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
425"
REFERENCES,0.36438152011922503,"Recognition (CVPR), June 2016.
426"
REFERENCES,0.3651266766020864,"[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
427"
REFERENCES,0.36587183308494786,"networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The
428"
REFERENCES,0.36661698956780925,"Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 630–645. Springer, 2016.
429"
REFERENCES,0.36736214605067063,"[18] Tatsuro Kawamoto, Masashi Tsubaki, and Tomoyuki Obuchi. Mean-field theory of graph neural
430"
REFERENCES,0.368107302533532,"networks in graph partitioning. Advances in Neural Information Processing Systems, 31, 2018.
431"
REFERENCES,0.36885245901639346,"[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
432"
REFERENCES,0.36959761549925485,"arXiv:1412.6980, 2014.
433"
REFERENCES,0.37034277198211624,"[20] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
434"
REFERENCES,0.3710879284649776,"networks. In International Conference on Learning Representations, 2017.
435"
REFERENCES,0.37183308494783907,"[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
436"
REFERENCES,0.37257824143070045,"convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.
437"
REFERENCES,0.37332339791356184,"[22] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
438"
REFERENCES,0.3740685543964232,"for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence, 2018.
439"
REFERENCES,0.37481371087928467,"[23] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen
440"
REFERENCES,0.37555886736214605,"Chang, and Doina Precup. Revisiting heterophily for graph neural networks. In Alice H.
441"
REFERENCES,0.37630402384500744,"Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural
442"
REFERENCES,0.3770491803278688,"Information Processing Systems, 2022.
443"
REFERENCES,0.37779433681073027,"[24] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in
444"
REFERENCES,0.37853949329359166,"graph convolutional networks. Advances in Neural Information Processing Systems, 33:14498–
445"
REFERENCES,0.37928464977645304,"14508, 2020.
446"
REFERENCES,0.38002980625931443,"[25] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
447"
REFERENCES,0.38077496274217587,"In Proceedings of the 27th international conference on machine learning (ICML-10), pages
448"
REFERENCES,0.38152011922503726,"807–814, 2010.
449"
REFERENCES,0.38226527570789864,"[26] Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass
450"
REFERENCES,0.3830104321907601,"filters. arXiv preprint arXiv:1905.09550, 2019.
451"
REFERENCES,0.3837555886736215,"[27] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
452"
REFERENCES,0.38450074515648286,"node classification. In International Conference on Learning Representations, 2020.
453"
REFERENCES,0.38524590163934425,"[28] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo-
454"
REFERENCES,0.3859910581222057,"metric graph convolutional networks. In International Conference on Learning Representations,
455"
REFERENCES,0.3867362146050671,"2020.
456"
REFERENCES,0.38748137108792846,"[29] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep
457"
REFERENCES,0.38822652757078985,"graph convolutional networks on node classification. In International Conference on Learning
458"
REFERENCES,0.3889716840536513,"Representations, 2020.
459"
REFERENCES,0.3897168405365127,"[30] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
460"
REFERENCES,0.39046199701937406,"Bengio. Graph attention networks. In International Conference on Learning Representations,
461"
REFERENCES,0.39120715350223545,"2018.
462"
REFERENCES,0.3919523099850969,"[31] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger.
463"
REFERENCES,0.3926974664679583,"Simplifying graph convolutional networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
464"
REFERENCES,0.39344262295081966,"editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of
465"
REFERENCES,0.39418777943368105,"Proceedings of Machine Learning Research, pages 6861–6871. PMLR, 09–15 Jun 2019.
466"
REFERENCES,0.3949329359165425,"[32] Xinyi Wu, Zhengdao Chen, William Wei Wang, and Ali Jadbabaie. A non-asymptotic analysis
467"
REFERENCES,0.3956780923994039,"of oversmoothing in graph neural networks. In The Eleventh International Conference on
468"
REFERENCES,0.39642324888226527,"Learning Representations, 2023.
469"
REFERENCES,0.39716840536512665,"[33] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
470"
REFERENCES,0.3979135618479881,"comprehensive survey on graph neural networks. IEEE transactions on neural networks and
471"
REFERENCES,0.3986587183308495,"learning systems, 32(1):4–24, 2020.
472"
REFERENCES,0.39940387481371087,"[34] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
473"
REFERENCES,0.40014903129657226,"Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
474"
REFERENCES,0.4008941877794337,"International conference on machine learning, pages 5453–5462. PMLR, 2018.
475"
REFERENCES,0.4016393442622951,"[35] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning
476"
REFERENCES,0.40238450074515647,"with graph embeddings. In International conference on machine learning, pages 40–48. PMLR,
477"
REFERENCES,0.40312965722801786,"2016.
478"
REFERENCES,0.4038748137108793,"[36] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure
479"
REFERENCES,0.4046199701937407,"Leskovec. Graph convolutional neural networks for web-scale recommender systems. In
480"
REFERENCES,0.40536512667660207,"Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &
481"
REFERENCES,0.4061102831594635,"data mining, pages 974–983, 2018.
482"
REFERENCES,0.4068554396423249,"[37] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
483"
REFERENCES,0.4076005961251863,"Conference on Learning Representations, 2020.
484"
REFERENCES,0.4083457526080477,"[38] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Schölkopf.
485"
REFERENCES,0.4090909090909091,"Learning with local and global consistency. Advances in neural information processing systems,
486"
REFERENCES,0.4098360655737705,"16, 2003.
487"
REFERENCES,0.4105812220566319,"[39] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng
488"
REFERENCES,0.4113263785394933,"Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and
489"
REFERENCES,0.4120715350223547,"applications. AI Open, 1:57–81, 2020.
490"
REFERENCES,0.4128166915052161,"[40] Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi, and Xia Hu.
491"
REFERENCES,0.4135618479880775,"Dirichlet energy constrained learning for deep graph neural networks. Advances in Neural
492"
REFERENCES,0.4143070044709389,"Information Processing Systems, 34:21834–21846, 2021.
493"
REFERENCES,0.4150521609538003,"[41] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
494"
REFERENCES,0.4157973174366617,"fields and harmonic functions. In Proceedings of the 20th International conference on Machine
495"
REFERENCES,0.4165424739195231,"learning (ICML-03), pages 912–919, 2003.
496"
REFERENCES,0.4172876304023845,"Appendix for “Learning to Control the Smoothness of GCN
497"
REFERENCES,0.4180327868852459,"Features""
498"
REFERENCES,0.4187779433681073,"A
Details of Notations
499"
REFERENCES,0.4195230998509687,"For two vectors u = (u1, u2, . . . , ud) and v = (v1, v2, . . . , vd), their inner product is defined as"
REFERENCES,0.4202682563338301,"⟨u, v⟩= d
X"
REFERENCES,0.4210134128166915,"i=1
uivi,"
REFERENCES,0.4217585692995529,their Hadamard product is defined as
REFERENCES,0.4225037257824143,"u ⊙v = (u1v1, u2v2, . . . , udvd),"
REFERENCES,0.4232488822652757,and their Kronecker product is defined as
REFERENCES,0.4239940387481371,"u ⊗v = uv⊤=  

"
REFERENCES,0.4247391952309985,"u1v1
u1v2
. . .
u1vd
u2v1
u2v2
. . .
u2vd
...
...
...
...
udv1
udv2
. . .
udvd "
REFERENCES,0.4254843517138599,"

."
REFERENCES,0.4262295081967213,"The Kronecker product can be defined for two vectors of different lengths in a similar manner as
500"
REFERENCES,0.4269746646795827,"above.
501"
REFERENCES,0.4277198211624441,"B
Proofs in Section 3
502"
REFERENCES,0.4284649776453055,"First, we prove that the two smoothness notions used in [27, 4] are two equivalent seminorms, i.e.,
503"
REFERENCES,0.42921013412816694,"we prove Proposition 3.1 below.
504"
REFERENCES,0.42995529061102833,"Proof of Proposition 3.1. The matrix H can be decomposed as H = Pn
i=1 Heie⊤
i , where each ei
is the eigenvector of G associated with eigenvalue λi. This indicates that"
REFERENCES,0.4307004470938897,"H ˜∆= H(I −G) = n
X"
REFERENCES,0.4314456035767511,"i=1
Heie⊤
i (I −G) = n
X"
REFERENCES,0.43219076005961254,"i=1
(Heie⊤
i −Heie⊤
i G) = n
X"
REFERENCES,0.43293591654247393,"i=1
(Heie⊤
i −Hei(λiei)⊤) = n
X"
REFERENCES,0.4336810730253353,"i=1
(1 −λi)Heie⊤
i = n
X"
REFERENCES,0.4344262295081967,"i=m+1
(1 −λi)Heie⊤
i ."
REFERENCES,0.43517138599105815,"Then using the fact that 1 −λi ≥0 for each i, we obtain"
REFERENCES,0.43591654247391953,"∥H∥2
E = Trace(H ˜∆H⊤)"
REFERENCES,0.4366616989567809,"= Trace

n
X"
REFERENCES,0.4374068554396423,"i=m+1
(1 −λi)Heie⊤
i ( n
X"
REFERENCES,0.43815201192250375,"j=1
Heje⊤
j )⊤"
REFERENCES,0.43889716840536513,"= Trace

n
X i=m+1 n
X"
REFERENCES,0.4396423248882265,"j=1
(1 −λi)Heie⊤
i eje⊤
j H⊤"
REFERENCES,0.4403874813710879,"= Trace

n
X"
REFERENCES,0.44113263785394935,"i=m+1
(1 −λi)Heie⊤
i eie⊤
i H⊤"
REFERENCES,0.44187779433681074,"= Trace

n
X i=m+1 p"
REFERENCES,0.4426229508196721,"1 −λiHeie⊤
i eie⊤
i H⊤p"
REFERENCES,0.4433681073025335,"1 −λi
"
REFERENCES,0.44411326378539495,"= Trace

n
X i=m+1 p"
REFERENCES,0.44485842026825634,"1 −λiHeie⊤
i ( n
X j=m+1 p"
REFERENCES,0.4456035767511177,"1 −λjHeje⊤
j )⊤ = n
X i=m+1 p"
REFERENCES,0.4463487332339791,"1 −λiHeie⊤
i

2 F ."
REFERENCES,0.44709388971684055,"That is,"
REFERENCES,0.44783904619970194,"∥H∥E = n
X i=m+1 p"
REFERENCES,0.4485842026825633,"1 −λiHeie⊤
i

F ."
REFERENCES,0.4493293591654247,"On the other hand, (3) implies"
REFERENCES,0.45007451564828616,"∥H∥M⊥= ∥HM⊥∥F = n
X"
REFERENCES,0.45081967213114754,"i=m+1
Heie⊤
i

F ."
REFERENCES,0.45156482861400893,"We first show that both ∥H∥M⊥and ∥H∥E are seminorms. Since ∥cH∥F = |c| · ∥H∥F for any
c ∈R, we have ∥cH∥M⊥= |c| · ∥H∥M⊥and ∥cH∥E = |c| · ∥H∥E. Moreover, for any two
matrices H1 and H2 s.t. H = H1 + H2, we have n
X"
REFERENCES,0.4523099850968703,"i=m+1
H1eie⊤
i + n
X"
REFERENCES,0.45305514157973176,"i=m+1
H2eie⊤
i = n
X"
REFERENCES,0.45380029806259314,"i=m+1
Heie⊤
i , n
X i=m+1 p"
REFERENCES,0.45454545454545453,"1 −λiH1eie⊤
i + n
X i=m+1 p"
REFERENCES,0.455290611028316,"1 −λiH2eie⊤
i = n
X i=m+1 p"
REFERENCES,0.45603576751117736,"1 −λiHeie⊤
i ."
REFERENCES,0.45678092399403875,"Then the triangle inequality of ∥· ∥F implies that of ∥H∥M⊥and ∥H∥E, respectively.
505"
REFERENCES,0.45752608047690013,"Now since 0 < 1 −λm+1 ≤1 −λi ≤2 for any i = m + 1, . . . , n, we may take α =
p"
REFERENCES,0.4582712369597616,"1 −λm+1
and β =
√"
THEN,0.45901639344262296,2. Then
THEN,0.45976154992548435,"α∥H∥M⊥=
α n
X"
THEN,0.46050670640834573,"i=m+1
Heie⊤
i

F ≤ n
X i=m+1 p"
THEN,0.4612518628912072,"1 −λiHeie⊤
i

F"
THEN,0.46199701937406856,"≤
β n
X"
THEN,0.46274217585692995,"i=m+1
Heie⊤
i

F"
THEN,0.46348733233979134,= β∥H∥M⊥.
THEN,0.4642324888226528,"The result thus follows from ∥H∥E =
 Pn
i=m+1
√1 −λiHeie⊤
i

F .
506"
THEN,0.46497764530551416,"B.1
ReLU
507"
THEN,0.46572280178837555,"We present a crucial tool to characterize how ReLU affects its input.
508"
THEN,0.46646795827123694,"Lemma B.1. Let Z ∈Rd×n, and let Z+ = max(Z, 0) and Z−= max(−Z, 0) be the positive and
509"
THEN,0.4672131147540984,"negative parts of Z. Then (1) Z+, Z−are (component-wise) nonnegative and Z = Z+ −Z−and
510"
THEN,0.46795827123695977,"(2) ⟨Z+, Z−⟩F = 0.
511"
THEN,0.46870342771982115,"Proof of Lemma B.1. Notice that for any a ∈R, we have"
THEN,0.46944858420268254,"max(a, 0) =
a
if a ≥0
0
otherwise and max(−a, 0) =
0
if a ≥0
−a
otherwise."
THEN,0.470193740685544,"This implies that a = max(a, 0) −max(−a, 0) and max(a, 0) · max(−a, 0) = 0.
512"
THEN,0.47093889716840537,"Let Zij be the (i, j)th entry of Z. Then Z = Z+ −Z−follows from Zij = max(Zij, 0) −
max(−Zij, 0). Also, one can deduce that"
THEN,0.47168405365126675,"⟨Z+, Z−⟩F = Trace((Z+)⊤Z−) = d
X i=1 j
X"
THEN,0.47242921013412814,"j=1
max(Zij, 0) max(−Zij, 0) = 0. 513"
THEN,0.4731743666169896,"Before proving Proposition 3.2, we notice the following relation between Z and H.
514"
THEN,0.47391952309985097,"Lemma B.2. Given Z ∈Rd×n, let H = σ(Z) with σ being ReLU, then H lies on the high-
515"
THEN,0.47466467958271236,"dimensional sphere, in ∥· ∥F norm, that is centered at Z/2 and with radius ∥Z/2∥F . That is, H
516"
THEN,0.47540983606557374,"and Z satisfy the following equation
517 H −Z 2 2"
THEN,0.4761549925484352,"F =
Z 2 2"
THEN,0.47690014903129657,"F .
(8)"
THEN,0.47764530551415796,"Proof of Lemma B.2. We observe that H = σ(Z) = max(Z, 0) = Z+ is the positive part of Z.
Then
⟨H, Z⟩F = ⟨H, Z+ −Z−⟩F = ⟨H, Z+⟩F −⟨H, Z−⟩F = ⟨H, H⟩F ,"
THEN,0.4783904619970194,"where we have used Z = Z+ −Z−and ⟨H, Z−⟩F = ⟨Z+, Z−⟩F = 0 from Lemma B.1.
518"
THEN,0.4791356184798808,"Therefore, one can deduce the desired result as follows"
THEN,0.4798807749627422,"⟨H, H⟩F −⟨H, Z⟩F = 0 ⇒∥H∥2
F −2
D
H, Z 2 E"
THEN,0.48062593144560356,"F +
Z 2 2"
THEN,0.481371087928465,"F =
Z 2 2 F"
THEN,0.4821162444113264,"⇒
H −Z 2 2"
THEN,0.4828614008941878,"F =
Z 2 2 F . 519"
THEN,0.48360655737704916,"Applying ∥H∥2
F = ∥HM + HM⊥∥2
F = ∥HM∥2
F + ∥HM⊥∥2
F , to both Z"
THEN,0.4843517138599106,2 and H −Z
THEN,0.485096870342772,"2 , we obtain
Z 2 2"
THEN,0.4858420268256334,"F =
ZM⊥ 2 2"
THEN,0.48658718330849476,"F +
ZM 2 2 F ,"
THEN,0.4873323397913562,"and
H −Z 2 2"
THEN,0.4880774962742176,"F =
HM⊥−ZM⊥ 2 2"
THEN,0.488822652757079,"F +
HM −ZM 2 2 F ."
THEN,0.48956780923994037,"Then (8) becomes
520 ZM⊥ 2 2"
THEN,0.4903129657228018,"F −
HM⊥−ZM⊥ 2 2"
THEN,0.4910581222056632,"F =
HM −ZM 2 2"
THEN,0.4918032786885246,"F −
ZM 2 2 F
(9)"
THEN,0.49254843517138597,"By direct calculation, we have
521"
THEN,0.4932935916542474,HM −ZM 2 2
THEN,0.4940387481371088,"F −
ZM 2 2"
THEN,0.4947839046199702,"F = ⟨HM, HM⟩F −2
D
HM, ZM 2 E"
THEN,0.49552906110283157,"F
= ⟨HM, HM −ZM⟩F .
(10)"
THEN,0.496274217585693,"Combining (9) and (10), we obtain the following result
522"
THEN,0.4970193740685544,"Lemma B.3. For any Z = ZM + ZM⊥, let H = σ(Z) = HM + HM⊥, then
523 ZM⊥ 2 2"
THEN,0.4977645305514158,"F −
HM⊥−ZM⊥ 2 2"
THEN,0.49850968703427717,"F = ⟨Z+
M, Z−
M⟩F ."
THEN,0.4992548435171386,"where Z+
M = Pm
i=1 Z+eie⊤
i , Z−
M = Pm
i=1 Z−eie⊤
i .
524"
THEN,0.5,"Proof of Lemma B.3. Recall that H = σ(Z) = max(Z, 0) = Z+. Also, Z = Z+ −Z−implies
ZM = Z+
M −Z−
M = H+
M −Z−
M. Therefore, we see that"
THEN,0.5007451564828614,"⟨HM, HM −ZM⟩F = ⟨Z+
M, Z−
M⟩F . 525"
THEN,0.5014903129657228,"By using the fact that ⟨Z+
M, Z−
M⟩F ≥0 in Lemma B.3, we reveal a geometric relation between Z
526"
THEN,0.5022354694485842,"and H mentioned in Proposition 3.2.
527"
THEN,0.5029806259314457,"Proof of Proposition 3.2. Since Z+, Z−≥0 are nonnegative and all the eigenvectors ei are also
nonnegative, we see that Z+
M = Pm
i=1 Z+eie⊤
i and Z−
M = Pm
i=1 Z−eie⊤
i are nonnegative. This
indicates that
⟨Z+
M, Z−
M⟩F = Trace

Z+
M(Z−
M)⊤
≥0."
THEN,0.503725782414307,"Then according to Lemma B.3, we obtain
ZM⊥ 2 2"
THEN,0.5044709388971684,"F −
HM⊥−ZM⊥ 2 2"
THEN,0.5052160953800298,"F = ⟨Z+
M, Z−
M⟩F ≥0."
THEN,0.5059612518628912,"So we have
HM⊥−ZM⊥ 2 F ="
THEN,0.5067064083457526,"rZM⊥ 2 2"
THEN,0.507451564828614,"F −⟨Z+
M, Z−
M⟩F ="
THEN,0.5081967213114754,"rZM⊥ 2 2"
THEN,0.5089418777943369,"F −⟨HM, HM −ZM⟩F ,"
THEN,0.5096870342771982,"which shows that HM⊥lies on the high-dimensional sphere that we have claimed. Furthermore, we
528"
THEN,0.5104321907600596,"conclude that
529"
THEN,0.511177347242921,"0 ≤
HM⊥−ZM⊥ 2"
THEN,0.5119225037257824,"F ≤
ZM⊥ 2"
THEN,0.5126676602086438,"F .
(11)"
THEN,0.5134128166915052,"This demonstrates that HM⊥lies on the high-dimensional sphere we have stated.
530"
THEN,0.5141579731743666,"Since the sphere
HM⊥−
ZM⊥"
THEN,0.5149031296572281,"2

2"
THEN,0.5156482861400894,"F =

ZM⊥"
THEN,0.5163934426229508,"2

2"
THEN,0.5171385991058122,"F passes through the origin, the distance of any"
THEN,0.5178837555886736,"HM⊥to the origin must be no greater than the diameter of this sphere, i.e., ∥HM⊥∥F ≤∥ZM⊥∥F .
Also, this can be derived from"
THEN,0.518628912071535,"∥HM⊥∥F −
ZM⊥ 2"
THEN,0.5193740685543964,"F ≤
HM⊥−ZM⊥ 2"
THEN,0.5201192250372578,"F ≤
ZM⊥ 2 F ."
THEN,0.5208643815201193,"One can see that the maximal smoothness ∥HM⊥∥F = ∥ZM⊥∥F is attained when HM⊥= ZM⊥,
531"
THEN,0.5216095380029806,"the intersection of the surface and the line passing through the center and the origin.
532"
THEN,0.522354694485842,"After all, we complete the proof by using the fact that ∥ZM⊥∥F = ∥Z∥M⊥for any matrix Z, which
533"
THEN,0.5230998509687034,"implies ∥H∥M⊥= ∥HM⊥∥F ≤∥ZM⊥∥F = ∥Z∥M⊥.
534 535"
THEN,0.5238450074515648,"B.2
Leaky ReLU
536"
THEN,0.5245901639344263,"For the leaky ReLU activation function, we have
537"
THEN,0.5253353204172876,"Lemma B.4. If H = σa(Z) with σa being leaky ReLU, then H lies on the high-dimensional sphere
538"
THEN,0.526080476900149,"centered at (1 + a)Z/2 with radius ∥(1 −a)Z/2∥F .
539"
THEN,0.5268256333830105,Proof of Lemma B.4. Notice that
THEN,0.5275707898658718,H = σa(Z) = Z+ −aZ−.
THEN,0.5283159463487332,"Then H −Z = (1 −a)Z−and H −aZ = (1 −a)Z+. Using ⟨Z−, Z+⟩F = 0, we have
540"
THEN,0.5290611028315947,"⟨H −Z, H −aZ⟩F = 0 ⇒∥H∥2
F −2
D
H, (1 + a)Z 2 E"
THEN,0.529806259314456,"F + a∥Z∥2
F = 0"
THEN,0.5305514157973175,"⇒∥H∥2
F −2
D
H, (1 + a)Z 2 E"
THEN,0.5312965722801788,"F = −a∥Z∥2
F"
THEN,0.5320417287630402,"⇒
H −(1 + a)"
Z,0.5327868852459017,"2
Z

2"
Z,0.533532041728763,"F =
(1 + a)"
Z,0.5342771982116244,"2
Z

2"
Z,0.5350223546944859,"F −a∥Z∥2
F =
(1 −a)"
Z,0.5357675111773472,"2
Z

2 F . 541"
Z,0.5365126676602087,"Moreover, we notice that
542"
Z,0.53725782414307,"Lemma B.5. For any Z = ZM + ZM⊥, let H = σa(Z) = HM + HM⊥, then
543"
Z,0.5380029806259314,(1 −a)
Z,0.5387481371087929,"2
ZM⊥

2"
Z,0.5394932935916542,"F −
HM⊥−(1 + a)"
Z,0.5402384500745157,"2
ZM⊥

2"
Z,0.5409836065573771,"F = (1 −a)2⟨Z+
M, Z−
M⟩F"
Z,0.5417287630402384,"Proof of Lemma B.5. Similar to the proof of Lemma B.3, the orthogonal decomposition implies that
(1 −a)"
Z,0.5424739195230999,"2
ZM⊥

2"
Z,0.5432190760059612,"F −
HM⊥−(1 + a)"
Z,0.5439642324888226,"2
ZM⊥

2"
Z,0.5447093889716841,"F =
HM −(1 + a)"
ZM,0.5454545454545454,"2
ZM

2"
ZM,0.5461997019374069,"F −
(1 −a)"
ZM,0.5469448584202683,"2
ZM

2"
ZM,0.5476900149031296,"F
=⟨HM −ZM, HM −aZM⟩F
=⟨(1 −a)Z−
M, (1 −a)Z+
M⟩F
=(1 −a)2⟨Z−
M, Z+
M⟩F . 544"
ZM,0.5484351713859911,"Proof of Proposition 3.3. Similar to the proof of Proposition 3.2, we apply ⟨Z−
M, Z+
M⟩F ≥0 to
Lemma B.5 and hence obtain the geometric condition as follows"
ZM,0.5491803278688525,HM⊥−(1 + a)
ZM,0.5499254843517138,"2
ZM⊥

F ="
ZM,0.5506706408345753,"r(1 −a)"
ZM,0.5514157973174366,"2
ZM⊥

2"
ZM,0.5521609538002981,"F −⟨HM −ZM, HM −aZM⟩F ."
ZM,0.5529061102831595,Then we have the following inequality
ZM,0.5536512667660208,"0 ≤
HM⊥−(1 + a)"
ZM,0.5543964232488823,"2
ZM⊥

F ≤
(1 −a)"
ZM,0.5551415797317437,"2
ZM⊥

F ."
ZM,0.555886736214605,"Moreover, we deduce that
∥HM⊥∥F −
(1 + a)"
ZM,0.5566318926974665,"2
ZM⊥

F"
ZM,0.5573770491803278,"≤
HM⊥−(1 + a)"
ZM,0.5581222056631893,"2
ZM⊥

F ≤
(1 −a)"
ZM,0.5588673621460507,"2
ZM⊥

F ."
ZM,0.559612518628912,and hence
ZM,0.5603576751117735,"−
(1 −a)"
ZM,0.5611028315946349,"2
ZM⊥

F ≤∥HM⊥∥F −
(1 + a)"
ZM,0.5618479880774963,"2
ZM⊥

F ≤
(1 −a)"
ZM,0.5625931445603577,"2
ZM⊥

F ."
ZM,0.563338301043219,"Therefore, we obtain a∥ZM⊥∥F ≤∥HM⊥∥F ≤∥ZM⊥∥F . (Remark that HM⊥achieves its
545"
ZM,0.5640834575260805,"maximal norm when it is equal to ZM⊥, the intersection of the surface and the line passing through
546"
ZM,0.5648286140089419,"the center and the origin. )
547"
ZM,0.5655737704918032,"By using the fact that ∥ZM⊥∥F = ∥Z∥M⊥for any matrix Z, we conclude that a∥Z∥M⊥≤
548"
ZM,0.5663189269746647,"∥H∥M⊥≤∥Z∥M⊥.
549"
ZM,0.5670640834575261,"C
Proofs in Section 4
550"
ZM,0.5678092399403875,"Throughout this section, we assume that zM⊥̸= 0.
551"
ZM,0.5685543964232489,"Proof of Proposition 4.3. Recall that e = ˜D
1
2 un/c has only positive entries where ˜D is the aug-
552"
ZM,0.5692995529061102,"mented degree matrix and un = [1, . . . , 1]⊤∈Rn and c = ∥˜D
1
2 un∥. Let di be the ith diagonal
553"
ZM,0.5700447093889717,"entry of ˜D. Then we have e = [√d1/c, √d2/c, . . . , √dn/c]⊤and c =
pPn
i=1 di.
554"
ZM,0.5707898658718331,Note that z(α) = z −αe = z −α
ZM,0.5715350223546944,"c ˜D
1
2 un = ˜D
1
2 ( ˜D−1"
ZM,0.5722801788375559,2 z −α
ZM,0.5730253353204173,"c un) = ˜D
1
2 (x −α"
ZM,0.5737704918032787,"c un), where we
assume x := ˜D−1"
ZM,0.5745156482861401,"2 z. Then we observe that when σ is the ReLU activation function,"
ZM,0.5752608047690015,"h(α) = σ(z(α)) = σ

˜D
1
2 (x −α"
ZM,0.5760059612518629,"c un)

= ˜D
1
2 σ

x −α"
ZM,0.5767511177347243,"c un

,"
ZM,0.5774962742175856,"and hence
⟨h(α), e⟩=
D
˜D
1
2 σ

x −α"
ZM,0.5782414307004471,"c un

, e
E"
ZM,0.5789865871833085,"=
D
σ

x −α"
ZM,0.5797317436661699,"c un

, ˜D
1
2 e
E
=
D
σ

x −α"
ZM,0.5804769001490313,"c un

, ˜Dun
E
."
ZM,0.5812220566318927,"We may now assume x = [x1, . . . , xn]⊤is well-ordered s.t. x1 ≥x2 ≥. . . ≥xn. Indeed, there is a
collection of indices {k1, ..., kl} s.t."
ZM,0.5819672131147541,"x1 = . . . , xk1 and xk1 > xk1+1,
xkj−1+1 = . . . = xkj and xkj > xkj+1 for any j = 2, . . . , l −1,"
ZM,0.5827123695976155,xkl−1+1 = . . . = xkl and kl = n.
ZM,0.5834575260804769,"That is, x1 = x2 = . . . = xk1 > xk1+1 = . . . = xk2 > xk2+1 = . . . = xk3 > xk3+1 . . .
555"
ZM,0.5842026825633383,We first restrict the domain of α s.t. h(α) ̸= 0. Note that we have
ZM,0.5849478390461997,"h(α) = 0 ⇔σ

x −α"
ZM,0.5856929955290611,"c un

= 0"
ZM,0.5864381520119225,⇔xi −α
ZM,0.587183308494784,"c ≤0 for i = 1, . . . , n"
ZM,0.5879284649776453,⇔x1 −α c ≤0
ZM,0.5886736214605067,⇔α ≥cx1.
ZM,0.589418777943368,"So we will study the smoothness s(h(α)) when α < cx1.
556"
ZM,0.5901639344262295,"Let ϵ > 0 and consider α = c(x1 −ϵ). When ϵ ≤x1 −xk1+1 = x1 −xk2, we see that x −α"
ZM,0.5909090909090909,"c un = [ϵ, . . . , ϵ, ϵ −(x1 −xk1+1), . . . , ϵ −(x1 −xn)]⊤,"
ZM,0.5916542473919523,"where only the first k1 entries are positive since x1 −xi ≥ϵ for any i ≥k1 + 1. Therefore,"
ZM,0.5923994038748137,"h(α) = ˜D
1
2 σ

x −α"
ZM,0.5931445603576752,"c un

= ˜D
1
2 [ϵ, . . . , ϵ, 0, . . . , 0]⊤"
ZM,0.5938897168405365,"= [ϵ
p"
ZM,0.5946348733233979,"d1, . . . , ϵ
p"
ZM,0.5953800298062594,"dk1, 0, . . . , 0]⊤."
ZM,0.5961251862891207,and hence we can compute that ∥h(α)∥= ϵ
ZM,0.5968703427719821,"v
u
u
t k1
X"
ZM,0.5976154992548435,"i=1
di. Also, we have"
ZM,0.5983606557377049,"∥h(α)∥M = |⟨h(α), e⟩| = [ϵ
p"
ZM,0.5991058122205664,"d1, . . . , ϵ
p"
ZM,0.5998509687034277,"dk1, 0, . . . , 0]⊤[
p"
ZM,0.6005961251862891,"d1/c,
p"
ZM,0.6013412816691506,"d2/c, . . . ,
p dn/c] = ϵ c k1
X"
ZM,0.6020864381520119,"i=1
di."
ZM,0.6028315946348733,Then we obtain the smoothness s(h(α)) as follows
ZM,0.6035767511177347,s(h(α)) = ∥h(α)∥M
ZM,0.6043219076005961,"∥h(α)∥
="
ZM,0.6050670640834576,"ϵ
c
Pk1
i=1 di"
ZM,0.6058122205663189,"ϵ
qPk1
i=1 di
="
ZM,0.6065573770491803,"qPk1
i=1 di
c
= K1"
ZM,0.6073025335320418,"c
< 1,"
ZM,0.6080476900149031,"where K1 :=
qPk1
i=1 di. Similarly, we may denote
qPkj
i=kj−1+1 di by Kj for j = 2, . . . , l.
557"
ZM,0.6087928464977646,"Now we are going to show that the smoothness s(h(α)) is increasing as α gets smaller whenever α <
558"
ZM,0.6095380029806259,"cx1, implying K1"
ZM,0.6102831594634873,"c is the minimum of the smoothness s(h(α)). Remember that we are considering
559"
ZM,0.6110283159463488,"α = c(x1 −ϵ) and we have studied the case when 0 < ϵ ≤x1 −xk1+1 = x1 −xk2.
560"
ZM,0.6117734724292101,"Let δj := x1 −xkj for 1 ≤j ≤l. Clearly, we have δ1 = 0 and δj < δj+1 for 1 ≤j ≤l −1. Fix a
j′ ∈{2, . . . , l −1}, we see that when δj′ < ϵ ≤x1 −xkj′+1, x −α c un"
ZM,0.6125186289120715,"=
h
ϵ −δ1, . . . , ϵ −δ1, ϵ −δ2, . . . , ϵ −δ2, ϵ −δ3, . . . , ϵ −δj′, ϵ −(x1 −xkj′+1), . . . , ϵ −(x1 −xn)
i⊤
,"
ZM,0.613263785394933,"where we have ϵ −δj > 0 for 2 ≤j ≤j′ and ϵ −(x1 −xi) ≤0 for any i ≥kj′ + 1. Consequently,"
ZM,0.6140089418777943,"h(α) = ˜D
1
2 σ(x −α"
ZM,0.6147540983606558,"c un) = [(ϵ −δ1)
p"
ZM,0.6154992548435171,"d1, . . . , (ϵ −δ1)
p"
ZM,0.6162444113263785,"dk1, (ϵ −δ2)
p"
ZM,0.61698956780924,"dk1+1, . . . , (ϵ −δ2)
p dk2,"
ZM,0.6177347242921013,"(ϵ −δ3)
p"
ZM,0.6184798807749627,"dk2+1, . . . , (ϵ −δj′)
q"
ZM,0.6192250372578242,"dkj′, 0, . . . , 0]⊤."
ZM,0.6199701937406855,Then we can compute
ZM,0.620715350223547,∥h(α)∥=
ZM,0.6214605067064084,"v
u
u
u
t j′
X j=1 kj
X"
ZM,0.6222056631892697,"i=kj−1+1
di(ϵ −δj)2 ="
ZM,0.6229508196721312,"v
u
u
t j′
X"
ZM,0.6236959761549925,"j=1
K2
j (ϵ −δj)2,"
ZM,0.624441132637854,"where we set k0 := 0 for simplicity and Kj =
qPkj
i=kj−1+1 di for j = 1, . . . , j′. Also, we have"
ZM,0.6251862891207154,"∥h(α)∥M = |⟨h(α), e⟩| = j′
X j=1 kj
X"
ZM,0.6259314456035767,i=kj−1+1
ZM,0.6266766020864382,"di(ϵ −δj) c
= 1 c j′
X"
ZM,0.6274217585692996,"j=1
K2
j (ϵ −δj)."
ZM,0.6281669150521609,A careful calculation shows that ∂
ZM,0.6289120715350224,"∂ϵs(h(α)) > 0 whenever δj′ < ϵ ≤x1 −xkj′+1 which implies that
s(h(α)) is increasing as ϵ increases. Indeed, we have"
ZM,0.6296572280178837,"∂
∂ϵs(h(α)) = ∂ ∂ϵ Pj′"
ZM,0.6304023845007451,"j=1 K2
j (ϵ −δj)"
ZM,0.6311475409836066,"c
qPj′
j=1 K2
j (ϵ −δj)2 ! ="
ZM,0.6318926974664679,"
∂
∂ϵ
Pj′"
ZM,0.6326378539493294,"j=1 K2
j (ϵ −δj)
qPj′
j=1 K2
j (ϵ −δj)2 −Pj′"
ZM,0.6333830104321908,"j=1 K2
j (ϵ −δj)

∂
∂ϵ"
ZM,0.6341281669150521,"qPj′
j=1 K2
j (ϵ −δj)2
"
ZM,0.6348733233979136,"c Pj′
j=1 K2
j (ϵ −δj)2 =  Pj′"
ZM,0.6356184798807749,"j=1 K2
j
qPj′
j=1 K2
j (ϵ −δj)2 −Pj′"
ZM,0.6363636363636364,"j=1 K2
j (ϵ −δj)

∂
∂ϵ
Pj′"
ZM,0.6371087928464978,"j=1 K2
j (ϵ−δj)2"
ZM,0.6378539493293591,"2
qPj′
j=1 K2
j (ϵ−δj)2 "
ZM,0.6385991058122206,"c Pj′
j=1 K2
j (ϵ −δj)2 =  Pj′"
ZM,0.639344262295082,"j=1 K2
j
 Pj′"
ZM,0.6400894187779433,"j=1 K2
j (ϵ −δj)2 −Pj′"
ZM,0.6408345752608048,"j=1 K2
j (ϵ −δj)
 Pj′"
ZM,0.6415797317436661,"j=1 K2
j (ϵ −δj)
"
ZM,0.6423248882265276,"c Pj′
j=1 K2
j (ϵ −δj)2
qPj′
j=1 K2
j (ϵ −δj)2
."
ZM,0.643070044709389,Then to show that ∂
ZM,0.6438152011922503,"∂ϵs(h(α)) > 0, it suffices to show that the numerator is positive, i.e."
ZM,0.6445603576751118,"
j′
X"
ZM,0.6453055141579732,"j=1
K2
j

j′
X"
ZM,0.6460506706408345,"j=1
K2
j (ϵ −δj)2 −

j′
X"
ZM,0.646795827123696,"j=1
K2
j (ϵ −δj)
2
> 0,"
ZM,0.6475409836065574,since the denominator c Pj′
ZM,0.6482861400894188,"j=1 K2
j (ϵ −δj)2
qPj′
j=1 K2
j (ϵ −δj)2 > 0 is always positive. In fact, this
follows from the Cauchy inequality ∥v∥∥u∥≥⟨v, u⟩, where we set"
ZM,0.6490312965722802,"v := [K1, K2, . . . , KJ′]⊤, u := [K1(ϵ −δ1), K2(ϵ −δ2), . . . , Kj′(ϵ −δj′)]⊤."
ZM,0.6497764530551415,"Moreover, equality happens only when v is parallel to u. This is, however, impossible since
561"
ZM,0.650521609538003,"ϵ −δj > ϵ −δj+1 for any j = 1, . . . , j′ −1 and each Kj is positive.
562"
ZM,0.6512667660208644,"So we see that s(h(α)) is increasing as ϵ increases whenever 0 < ϵ, and hence the smoothness
563"
ZM,0.6520119225037257,"s(h(α)) is increasing as α decreases whenever cxn ≤α < cx1.
564"
ZM,0.6527570789865872,"For the case j′ = l where δl = x1−xn < ϵ, we have xn −α/c = xn −(x1−ϵ) = ϵ−(x1−xn) > 0,
implying α < cxn and h(α) = z(α). We have shown that the smoothness is increasing as α is going
far from ⟨z, e⟩; in particular, when α < ⟨z, e⟩and α is decreasing. One can check that"
ZM,0.6535022354694486,"cxn =
Pn
i=1 dixn"
ZM,0.65424739195231,"c
=

xnun,
˜Dun c"
ZM,0.6549925484351714,"≤

x,
˜Dun c"
ZM,0.6557377049180327,"=

˜D
1
2 x,
˜D
1
2 un c"
ZM,0.6564828614008942,"= ⟨z, e⟩,"
ZM,0.6572280178837556,"which means the smoothness is increasing as α decreases whenever α < cxn.
565"
ZM,0.657973174366617,"We conclude that the smoothness increases as α decreases provided α < cx1. Also, we have
566"
ZM,0.6587183308494784,"supα<cx1 s(h(α)) = 1 as the case in the proof of Proposition C.1. One can check that s(h(α)) is a
567"
ZM,0.6594634873323398,"continuous function for α < cx1 and thus it has range [K1/c, 1) by the mean value theorem.
568"
ZM,0.6602086438152012,"Finally, we can establish the result: K1/c =
r P"
ZM,0.6609538002980626,"xi=max x di
Pn
j=1 dj
is the minimum of s(h(α)) and 1 is the
569"
ZM,0.6616989567809239,"maximum of s(h(α)) occurring whenever α ≥cx1 =
qPn
j=1 dj maxi xi. Moreover, s(h(α)) has
570"
ZM,0.6624441132637854,"a monotone property when α <
qPn
j=1 dj maxi xi and has range
hr P"
ZM,0.6631892697466468,"xi=max x di
Pn
j=1 dj
, 1
i
.
571"
ZM,0.6639344262295082,"It is clear that the assumption on the ordering of the entries of x will not affect this result.
572"
ZM,0.6646795827123696,"To prove Proposition 4.4, we first prove an analogous result for the identity function, that is, h =
573"
ZM,0.665424739195231,"σ(z) = z.
574"
ZM,0.6661698956780924,"Proposition C.1. Suppose zM⊥̸= 0, then s(z(α)) achieves its minimum 0 if α = ⟨z, e⟩. Moreover,
575"
ZM,0.6669150521609538,"supα s(z(α)) = 1 where s(z(α)) is close to 1 when α is far away from ⟨z, e⟩.
576"
ZM,0.6676602086438153,"Notice that Proposition C.1 does not consider the activation function.
577"
ZM,0.6684053651266766,Proof of Proposition C.1. We know that 0 ≤s(z(α)) ≤1 and
ZM,0.669150521609538,s(z(α)) = s
ZM,0.6698956780923994,1 −∥zM⊥∥2
ZM,0.6706408345752608,∥z(α)∥2 = s
ZM,0.6713859910581222,"1 −
∥zM⊥∥2"
ZM,0.6721311475409836,∥zM⊥∥2 + ∥z(α)M∥2 = s
ZM,0.672876304023845,"1 −
∥zM⊥∥2"
ZM,0.6736214605067065,∥zM⊥∥2 + ∥zM −αe∥2 .
ZM,0.6743666169895678,"Suppose s(z(α)) = 1. Then we have
∥zM⊥∥2"
ZM,0.6751117734724292,"∥zM⊥∥2+∥zM−αe∥2 = 0 which forces ∥zM⊥∥= 0. However,
578"
ZM,0.6758569299552906,"this contradicts the hypothesis zM⊥̸= 0. So s(z(α)) cannot attain its maximum.
579"
ZM,0.676602086438152,"But for any 0 ≤t < 1, one can see that s(z(α)) = t if and only if
s"
ZM,0.6773472429210134,"1 −
∥zM⊥∥2"
ZM,0.6780923994038748,"∥zM⊥∥2 + ∥zM −αe∥2 = t ⇔
∥zM⊥∥2"
ZM,0.6788375558867362,∥zM⊥∥2 + ∥zM −αe∥2 = 1 −t2
ZM,0.6795827123695977,"⇔∥zM⊥∥2 = (1 −t2)
 
∥zM⊥∥2 + ∥zM −αe∥2"
ZM,0.680327868852459,⇔t2∥zM⊥∥2 = (1 −t2)∥zM −αe∥2
ZM,0.6810730253353204,⇔∥zM −αe∥= r t2
ZM,0.6818181818181818,1 −t2 · ∥zM⊥∥
ZM,0.6825633383010432,"This implies that supα s(z(α)) = 1 and s(z(α)) achieves its minimum 0 if and only if α = ⟨z, e⟩.
580"
ZM,0.6833084947839047,"It is clear that s(z(α)) get closer to 1 when α is going far away from ⟨z, e⟩. i.e., |α −⟨z, e⟩| =
581"
ZM,0.684053651266766,"∥zM −αe∥is increasing.
582"
ZM,0.6847988077496274,"Proof of Proposition 4.4. First, we notice that leaky ReLU has the following two properties
583"
ZM,0.6855439642324889,"1. σa(x) > 0 for x ≫0 and σa(x) < 0 for x ≪0.
584"
ZM,0.6862891207153502,"2. σa is a non-trivial linear map for x ≫0.
585"
ZM,0.6870342771982116,"We will use Property 1 to show that minα s(h(α)) = 0 and Property 2 to show that supα s(h(α)) = 1.
586"
ZM,0.687779433681073,"Notice that σa(x) < 0 for x ≪0 implies that there exists a sufficient small α2 < 0 s.t. all of the
587"
ZM,0.6885245901639344,"entries of h(α2) are negative and hence |⟨h(α2), e⟩| < 0. Similarly, σa(x) > 0 for x ≫0 implies
588"
ZM,0.6892697466467959,"that there exists a sufficient large α1 > 0 s.t. all of the entries of h(α1) are positive and hence
589"
ZM,0.6900149031296572,"|⟨h(α1), e⟩| > 0. Since |⟨h(α), e⟩| is a continuous function of α on [α1, α2], the Intermediate
590"
ZM,0.6907600596125186,"Value Theorem follows that there exists an α ∈(α1, α2) s.t. |⟨h(α), e⟩| = 0. Thus by definition
591"
ZM,0.6915052160953801,"s(h(α)) = |⟨h(α), e⟩|/∥h(α)∥, we see that minα s(h(α)) = 0.
592"
ZM,0.6922503725782414,"On the other hand, since σa is a non-trivial linear map for x ≫0, we may assume σa(x) = cx for
x > x0 where c ̸= 0 is some non-zero constant and x0 > 0 is some positive constant. Then we
can choose an α0 > ⟨z, e⟩s.t. for any α ≥α0, all of the entries of z(α) are greater than x0. Then
whenever α ≥α0, we have h(α) = σa(z(α)) = cz(α). This implies"
ZM,0.6929955290611028,"s(h(α)) = |⟨h(α), e⟩|"
ZM,0.6937406855439643,"∥h(α)∥
= |⟨cz(α), e⟩|"
ZM,0.6944858420268256,"∥cz(α)∥
= |⟨z(α), e⟩|"
ZM,0.6952309985096871,"∥z(α)∥
= s(z(α))."
ZM,0.6959761549925484,"Thus supα s(h(α)) = 1 follows from the Proof of Proposition C.1 where we see that supα s(z(α)) =
593"
ZM,0.6967213114754098,"1 since s(z(α)) gets closer to 1 as α increases.
594 595"
ZM,0.6974664679582713,"Remark C.2. Indeed, it holds for any continuous function f : R →R satisfying the following
596"
ZM,0.6982116244411326,"1. f(x) > 0 for x ≫0, f(x) < 0 for x ≪0 or f(x) < 0 for x ≫0, f(x) > 0 for x ≪0,
597"
ZM,0.698956780923994,"2. f is a non-trivial linear map for x ≫0 or x ≪0.
598"
ZM,0.6997019374068555,"One can check the proof above only depends on these two properties. It is worth mentioning that
599"
ZM,0.7004470938897168,"most activation functions, e.g. leaky LU, SiLU, tanh, satisfy condition 1.
600"
ZM,0.7011922503725783,"Proof of Corollary 4.5. For any α, we notice that ∥z∥M⊥= ∥zM⊥∥F = ∥z(α)∥M⊥since α
601"
ZM,0.7019374068554396,"only changes the component of z in the eigenspace M. Also, Propositions 3.2 and 3.3 show
602"
ZM,0.702682563338301,"that ∥z(α)∥M⊥≥∥h(α)∥M⊥whenever h(α) = σ(z(α)) or σa(z(α)). Therefore, we see that
603"
ZM,0.7034277198211625,"∥z∥M⊥≥∥h(α)∥M⊥holds for any α. Since zM⊥̸= 0, s(z) must lie in [0, 1).
604 605"
ZM,0.7041728763040238,"D
Experimental Details
606"
ZM,0.7049180327868853,"This part includes the missing details about experimental configurations and additional experimental
607"
ZM,0.7056631892697467,"results for Section 6. All tasks we run using Nvidia RTX 3090, GV100, and Tesla T4 GPUs. All
608"
ZM,0.706408345752608,"computational performance metrics, including timing procedures, are run using Tesla T4 GPUs from
609"
ZM,0.7071535022354695,"Google Colab.
610"
ZM,0.7078986587183308,"D.1
Dataset details
611"
ZM,0.7086438152011922,"In this section, we briefly describe the benchmark datasets used. Table 3 provides additional details
612"
ZM,0.7093889716840537,"about the underlying graph representation.
613"
ZM,0.710134128166915,"Citation Datasets: The five citation datasets considered are Cora, Citeseer PubMed, Coauthor-
614"
ZM,0.7108792846497765,"Physics, and Ogbn-arxiv. Each dataset is represented by a graph with nodes representing academic
615"
ZM,0.7116244411326379,"publications, features encoding a bag-of-words description, labels classifying the publication type,
616"
ZM,0.7123695976154992,"and edges representing citations.
617"
ZM,0.7131147540983607,"Web Knowledge-Base Datasets: The three web knowledge-base datasets are Cornell, Texas, and
618"
ZM,0.713859910581222,"Wisconsin. Each dataset is represented by a graph with nodes representing CS department webpages,
619"
ZM,0.7146050670640834,"features encoding a bag-of-words description, edges representing hyper-link connections, and labels
620"
ZM,0.7153502235469449,"classifying the webpage type.
621"
ZM,0.7160953800298062,"Wikipedia Network Datasets: The two Wikipedia network datasets are Chameleon and Squirrel.
622"
ZM,0.7168405365126677,"Each dataset is represented by a graph with nodes representing CS department webpages, features en-
623"
ZM,0.7175856929955291,"coding a bag-of-words description, edges representing hyper-link connections, and labels classifying
624"
ZM,0.7183308494783904,"the webpage type.
625"
ZM,0.7190760059612519,"# Nodes
# Edges
# Features
# Classes
Splits (Train/Val/Test)
Cornell
183
295
1, 703
5
48/32/20%
Texas
181
309
1, 703
5
48/32/20%
Wisconsin
251
499
1, 703
5
48/32/20%
Chameleon
2, 277
36, 101
2, 325
5
48/32/20%
Squirrel
5, 201
217, 073
2, 089
5
48/32/20%
Citeseer
3, 727
4, 732
3, 703
6
120/500/1000
Cora
2, 708
5, 429
1, 433
7
140/500/1000
PubMed
19, 717
44, 338
500
3
60/500/1000
Coauthor-Physics
34,493
247,962
8415
5
100/150/34,243
Ogbn-arxiv
169,343
1,166,243
128
40
90,941/29,799/48,603
Table 3: Graph statistics."
ZM,0.7198211624441133,"D.2
Model size and computational time for citation datasets
626"
ZM,0.7205663189269746,"Table 4 compares the model size and computational time for experiments on citation datasets in
627"
ZM,0.7213114754098361,"Section 6.2.
628"
ZM,0.7220566318926974,"# Parameters
Training Time (s)
Inference Time (ms)
Cora
GCN
100,423
8.4
1.6
GCNII
110,535
10.0
2.1
GCNII
708,743
57.6
12.3
GCNII-SCT
1,237,127
110.3
29.6
EGNN
712,839
65.6
14.4
EGNN-SCT
316,551
24.8
4.5
Citeseer
GCN
245,638
8.3
1.5
GCN-SCT
301,830
15.5
4.0
GCNII
999,174
57.6
12.3
GCNII-SCT
1,001,222
65.9
15.7
EGNN
739,078
39.6
7.2
EGNN-SCT
540,934
24.0
5.8
PubMed
GCN
40,451
9.0
1.8
GCN-SCT
40,707
11.1
2.2
GCNII
326,659
98.2
12.8
GCNII-SCT
590,851
71.7
17.4
EGNN
592,899
93.7
2.5
EGNN-SCT
130,563
16.0
3.1
Coauthor-Physics
GCN
547,141
35.2
8.0
GCN-SCT
547,397
33.9
8.3
GCNII
555,333
49.1
10.3
GCNII-SCT
555,461
67.0
9.5
EGNN
672,069
176.4
47.9
EGNN-SCT
572,229
51.7
14.8
Ogbn-arxiv
GCN
27,240
50.4
21.1
GCN-SCT
28,392
62.6
24.4
GCNII
76,392
205.4
94.8
GCNII-SCT
80,616
253.0
108.9
EGNN
77,416
206.8
98.0
EGNN-SCT
81,640
254.0
112.3
Table 4: Number of model parameters for varying numbers of layers using the optimal model hyperparameters.
The SCT is added at each layer and the size of the additional parameters scales with the number of eigenvectors
with an eigenvalue of one for matrix G in (2)."
ZM,0.7228017883755589,"D.3
Additional Section 6.2 details for citation datasets
629"
ZM,0.7235469448584203,"Table 5 lists the hyperparameters used in the grid search in generating the results in Table 1. Also,
630"
ZM,0.7242921013412816,"Table 7 reports the classification accuracy of different models with different depths using either ReLU
631"
ZM,0.7250372578241431,or leaky ReLU.
ZM,0.7257824143070045,"Parameter
Values
Learning Rate
{1e-4, 1e-3, 1e-2}
Weight Decay (FC)
{0, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
Weight Decay (Conv)
{0, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
Dropout
{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}
Hidden Channels
{16, 32, 64, 128}
GCNII-α
{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}
GCNII-θ
{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}
EGNN-cmax
{0.5, 1.0, 1.5, 2.0}
EGNN-α
{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}
EGNN-θ
{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}
Table 5: Hyperparameter grid search for Table 1. 632"
ZM,0.7265275707898659,"Layers
2
4
16
32
Cora
EGNN/EGNN-SCT
83.2/83.4
84.2/84.3
85.4/85.5
85.3/85.5
Citeseer
EGNN/EGNN-SCT
72.0/72.1
71.9/72.3
72.4/72.6
72.3/72.8
PubMed
EGNN/EGNN-SCT
79.2/79.4
79.5/79.8
80.1/80.1
80.0/80.2
Coauthor-Physics
EGNN/EGNN-SCT
92.6/92.8
92.9/93.0
93.1/93.3
93.3/93.3
Ogbn-arxiv
EGNN/EGNN-SCT
68.4/68.5
71.1/71.3
72.7/73.0
72.7/72.9
Table 6: Test accuracy for EGNN and EGNN-SCT using SReLU activation function of varying depth on citation
networks with the split discussed in Section 6.2. (Unit:%)"
ZM,0.7272727272727273,"D.3.1
Vanishing gradients
633"
ZM,0.7280178837555886,"Figure 4 shows the vanishing gradient problem for training deep GCN – with or without SCT – in
634"
ZM,0.7287630402384501,"comparison to models like GCNII and EGNN. This figure plots ||∂Hout/∂Hl|| for layers l ∈[0, 32]
635"
ZM,0.7295081967213115,"as the training epochs run from 0 to 100. Figures 4 (a) and (b) illustrate the vanishing gradient issue
636"
ZM,0.7302533532041728,"for GCN and that it persists for GCN-SCT. Figures 4 (c) and (e) illustrate that GCNII and EGNN
637"
ZM,0.7309985096870343,"do not suffer from vanishing gradients, and furthermore, because these models connect H0 to every
638"
ZM,0.7317436661698957,"layer, the gradient with respect to the weights in the first layer is nonzero. What is interesting about
639"
ZM,0.732488822652757,"the addition of SCT to both EGNN and GCNII is that the intermediate gradients become large as the
640"
ZM,0.7332339791356185,"training epochs progress shown in Figure 4 (d) and (f).
641"
ZM,0.7339791356184798,"D.4
Additional Section 6.2 details for other datasets
642"
ZM,0.7347242921013413,"Table 8 reports the mean test accuracy and standard deviation over ten folds of the WebKB and
643"
ZM,0.7354694485842027,"WikipediaNetwork datasets using SCT-based models.
644"
ZM,0.736214605067064,"Table 9 lists the average computational time for each epoch for different models of the same depth
645"
ZM,0.7369597615499255,"– 8 layers. These results show that integrating SCT into GNNs only results in a small amount of
646"
ZM,0.7377049180327869,"computational overhead.
647"
ZM,0.7384500745156483,"0
20
40
60
80
Epoch 0 20 Layer 0.00 0.05 0.10"
ZM,0.7391952309985097,"0
20
40
60
80
Epoch 0 20 Layer 0.00 0.05 0.10"
ZM,0.7399403874813711,"0
20
40
60
80
Epoch 0 20 Layer 0.00 0.05 0.10"
ZM,0.7406855439642325,"(a) GCN
(c) GCNII
(e) EGNN"
ZM,0.7414307004470939,"0
20
40
60
80
Epoch 0 20 Layer 0.00 0.05 0.10"
ZM,0.7421758569299552,"0
20
40
60
80
Epoch 0 20 Layer 0.00 0.05 0.10"
ZM,0.7429210134128167,"0
20
40
60
80
Epoch 0 20 Layer 0.00 0.05 0.10"
ZM,0.7436661698956781,"(b) GCN-SCT
(d) GCNII-SCT
(f) EGNN-SCT"
ZM,0.7444113263785395,"Figure 4: Training gradients for ||∂Hout/∂Hl|| for l ∈[0, 32] layers and 100 training epochs on the Citeseer
dataset. Here, all models have 32 layers and 16 hidden dimensions for each layer. We observe that (a) GCN
suffers from vanishing gradients. By contrast (c) GCNII and (e) EGNN do not suffer from vanishing gradients,
and we can observe their skip connection to H0. Because these models (GCNII/GCNII-SCT and EGNN/EGNN-
SCT) connect H0 to every layer, the gradient at the first layer is nonzero. We notice that while SCT does
not overcome vanishing gradients for (b) GCN-SCT, it is able to increase the norm of the gradients for the
intermediate layers in (d) GCNII-SCT and (f) EGNN-SCT."
ZM,0.7451564828614009,"Cora
ReLU
leaky ReLU
Layers
2
4
16
32
2
4
16
32
GCN-SCT
81.2
80.3
71.4
67.2
82.9
82.8
68.0
65.5
GCNII-SCT
83.5
83.8
82.7
83.3
83.8
84.8
84.8
85.5
EGNN-SCT
84.1
83.8
82.3
80.8
83.7
84.5
83.3
82.0"
ZM,0.7459016393442623,"Citeseer
ReLU
leaky ReLU
Layers
2
4
16
32
2
4
16
32
GCN-SCT
69.0
67.3
51.5
50.3
69.9
67.7
55.4
51.0
GCNII-SCT
72.8
72.8
72.8
73.3
72.8
72.9
73.8
72.7
EGNN-SCT
72.5
72.0
70.2
71.8
73.1
71.7
72.6
72.9"
ZM,0.7466467958271237,"PubMed
ReLU
leaky ReLU
Layers
2
4
16
32
2
4
16
32
GCN-SCT
79.4
78.2
75.9
77.0
79.8
78.4
76.1
76.9
GCNII-SCT
79.7
80.1
80.7
80.7
79.6
80.0
80.3
80.7
EGNN-SCT
79.7
80.1
80.0
80.4
79.8
80.4
80.3
80.2"
ZM,0.7473919523099851,"Coauthor-Physics
ReLU
leaky ReLU
Layers
2
4
16
32
2
4
16
32
GCN-SCT
91.8 ± 1.6
91.6 ± 3.0
44.5 ± 13.0
42.6 ± 17.0
92.6 ± 1.6
92.5 ± 5.9
50.9 ± 15.0
43.6 ± 16.0
GCNII-SCT
94.4 ± 0.4
93.5 ± 1.2
93.7 ± 0.7
93.8 ± 0.6
94.0 ± 0.4
94.2 ± 0.3
93.3 ± 0.7
94.1 ± 0.3
EGNN-SCT
93.6 ± 0.7
94.1 ± 0.4
93.4 ± 0.8
93.8 ± 1.3
93.9 ± 0.7
94.0 ± 0.7
94.0 ± 0.7
93.3 ± 0.9"
ZM,0.7481371087928465,"Ogbn-arxiv
ReLU
leaky ReLU
Layers
2
4
16
32
2
4
16
32
GCN-SCT
71.7 ± 0.3
72.6 ± 0.3
71.4 ± 0.2
71.9 ± 0.3
72.1 ± 0.3
72.7 ± 0.3
72.3 ± 0.2
72.3 ± 0.3
GCNII-SCT
71.4 ± 0.3
72.1 ± 0.3
72.2 ± 0.2
71.8 ± 0.2
72.0 ± 0.3
72.2 ± 0.2
72.4 ± 0.3
72.1 ± 0.3
EGNN-SCT
68.5 ± 0.6
71.0 ± 0.5
72.8 ± 0.5
72.1 ± 0.6
67.7 ± 0.5
71.3 ± 0.5
72.3 ± 0.5
72.3 ± 0.5
Table 7: Test accuracy results for models of varying depth with ReLU or leaky ReLU activation function on the
citation network datasets using the split discussed in Section 6.2."
ZM,0.7488822652757079,"Cornell
Texas
Wisconsin
Chameleon
Squirrel
GCN-SCT
55.95 ± 8.5
62.16 ± 5.7
54.71 ± 4.4
38.44 ± 4.3
35.31 ± 1.9
GCNII-SCT
75.41 ± 2.2
83.34 ± 4.5
86.08 ± 3.8
64.52 ± 2.2
47.51 ± 1.4
Table 8: Test mean ± standard deviation accuracy from 10 fold cross validation on five heterophilic datasets
with fixed 48/32/20% splits. The depth of each model is 8 layers with 16 hidden channels. (Unit: second)"
ZM,0.7496274217585693,"Cornell
Texas
Wisconsin
Chameleon
Squirrel
GCN [20]
0.011
0.013
0.012
0.011
0.022
GCNII [6]
0.017
0.018
0.017
0.013
0.022
GCN-SCT
0.015
0.017
0.015
0.011
0.023
GCNII-SCT
0.017
0.018
0.017
0.020
0.025
Table 9: Average computational time per epoch for five heterophilic datasets with fixed 48/32/20% splits. The
depth of each model is 8 layers with 16 hidden channels. (Unit: second)"
ZM,0.7503725782414307,"NeurIPS Paper Checklist
648"
ZM,0.7511177347242921,"The checklist is designed to encourage best practices for responsible machine learning research,
649"
ZM,0.7518628912071535,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
650"
ZM,0.7526080476900149,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
651"
ZM,0.7533532041728763,"follow the references and precede the (optional) supplemental material. The checklist does NOT
652"
ZM,0.7540983606557377,"count towards the page limit.
653"
ZM,0.7548435171385991,"Please read the checklist guidelines carefully for information on how to answer these questions. For
654"
ZM,0.7555886736214605,"each question in the checklist:
655"
ZM,0.7563338301043219,"• You should answer [Yes] , [No] , or [NA] .
656"
ZM,0.7570789865871833,"• [NA] means either that the question is Not Applicable for that particular paper or the
657"
ZM,0.7578241430700448,"relevant information is Not Available.
658"
ZM,0.7585692995529061,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
659"
ZM,0.7593144560357675,"The checklist answers are an integral part of your paper submission. They are visible to the
660"
ZM,0.7600596125186289,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
661"
ZM,0.7608047690014903,"(after eventual revisions) with the final version of your paper, and its final version will be published
662"
ZM,0.7615499254843517,"with the paper.
663"
ZM,0.7622950819672131,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
664"
ZM,0.7630402384500745,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
665"
ZM,0.763785394932936,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
666"
ZM,0.7645305514157973,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
667"
ZM,0.7652757078986587,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
668"
ZM,0.7660208643815202,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
669"
ZM,0.7667660208643815,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
670"
ZM,0.767511177347243,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
671"
ZM,0.7682563338301043,"please point to the section(s) where related material for the question can be found.
672"
ZM,0.7690014903129657,"IMPORTANT, please:
673"
ZM,0.7697466467958272,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
674"
ZM,0.7704918032786885,"• Keep the checklist subsection headings, questions/answers and guidelines below.
675"
ZM,0.7712369597615499,"• Do not modify the questions and only use the provided macros for your answers.
676"
CLAIMS,0.7719821162444114,"1. Claims
677"
CLAIMS,0.7727272727272727,"Question: Do the main claims made in the abstract and introduction accurately reflect the
678"
CLAIMS,0.7734724292101341,"paper’s contributions and scope?
679"
CLAIMS,0.7742175856929955,"Answer: [Yes]
680"
CLAIMS,0.7749627421758569,"Justification: See details in Sections 3, 4, 5, and 6.
681"
CLAIMS,0.7757078986587184,"Guidelines:
682"
CLAIMS,0.7764530551415797,"• The answer NA means that the abstract and introduction do not include the claims
683"
CLAIMS,0.7771982116244411,"made in the paper.
684"
CLAIMS,0.7779433681073026,"• The abstract and/or introduction should clearly state the claims made, including the
685"
CLAIMS,0.7786885245901639,"contributions made in the paper and important assumptions and limitations. A No or
686"
CLAIMS,0.7794336810730254,"NA answer to this question will not be perceived well by the reviewers.
687"
CLAIMS,0.7801788375558867,"• The claims made should match theoretical and experimental results, and reflect how
688"
CLAIMS,0.7809239940387481,"much the results can be expected to generalize to other settings.
689"
CLAIMS,0.7816691505216096,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
690"
CLAIMS,0.7824143070044709,"are not attained by the paper.
691"
LIMITATIONS,0.7831594634873323,"2. Limitations
692"
LIMITATIONS,0.7839046199701938,"Question: Does the paper discuss the limitations of the work performed by the authors?
693"
LIMITATIONS,0.7846497764530551,"Answer: [Yes]
694"
LIMITATIONS,0.7853949329359166,"Justification: See Section 7.
695"
LIMITATIONS,0.786140089418778,"Guidelines:
696"
LIMITATIONS,0.7868852459016393,"• The answer NA means that the paper has no limitation while the answer No means that
697"
LIMITATIONS,0.7876304023845008,"the paper has limitations, but those are not discussed in the paper.
698"
LIMITATIONS,0.7883755588673621,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
699"
LIMITATIONS,0.7891207153502235,"• The paper should point out any strong assumptions and how robust the results are to
700"
LIMITATIONS,0.789865871833085,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
701"
LIMITATIONS,0.7906110283159463,"model well-specification, asymptotic approximations only holding locally). The authors
702"
LIMITATIONS,0.7913561847988078,"should reflect on how these assumptions might be violated in practice and what the
703"
LIMITATIONS,0.7921013412816692,"implications would be.
704"
LIMITATIONS,0.7928464977645305,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
705"
LIMITATIONS,0.793591654247392,"only tested on a few datasets or with a few runs. In general, empirical results often
706"
LIMITATIONS,0.7943368107302533,"depend on implicit assumptions, which should be articulated.
707"
LIMITATIONS,0.7950819672131147,"• The authors should reflect on the factors that influence the performance of the approach.
708"
LIMITATIONS,0.7958271236959762,"For example, a facial recognition algorithm may perform poorly when image resolution
709"
LIMITATIONS,0.7965722801788375,"is low or images are taken in low lighting. Or a speech-to-text system might not be
710"
LIMITATIONS,0.797317436661699,"used reliably to provide closed captions for online lectures because it fails to handle
711"
LIMITATIONS,0.7980625931445604,"technical jargon.
712"
LIMITATIONS,0.7988077496274217,"• The authors should discuss the computational efficiency of the proposed algorithms
713"
LIMITATIONS,0.7995529061102832,"and how they scale with dataset size.
714"
LIMITATIONS,0.8002980625931445,"• If applicable, the authors should discuss possible limitations of their approach to
715"
LIMITATIONS,0.801043219076006,"address problems of privacy and fairness.
716"
LIMITATIONS,0.8017883755588674,"• While the authors might fear that complete honesty about limitations might be used by
717"
LIMITATIONS,0.8025335320417287,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
718"
LIMITATIONS,0.8032786885245902,"limitations that aren’t acknowledged in the paper. The authors should use their best
719"
LIMITATIONS,0.8040238450074516,"judgment and recognize that individual actions in favor of transparency play an impor-
720"
LIMITATIONS,0.8047690014903129,"tant role in developing norms that preserve the integrity of the community. Reviewers
721"
LIMITATIONS,0.8055141579731744,"will be specifically instructed to not penalize honesty concerning limitations.
722"
THEORY ASSUMPTIONS AND PROOFS,0.8062593144560357,"3. Theory Assumptions and Proofs
723"
THEORY ASSUMPTIONS AND PROOFS,0.8070044709388972,"Question: For each theoretical result, does the paper provide the full set of assumptions and
724"
THEORY ASSUMPTIONS AND PROOFS,0.8077496274217586,"a complete (and correct) proof?
725"
THEORY ASSUMPTIONS AND PROOFS,0.8084947839046199,"Answer: [Yes]
726"
THEORY ASSUMPTIONS AND PROOFS,0.8092399403874814,"Justification: See Sections 3 and 4 for details.
727"
THEORY ASSUMPTIONS AND PROOFS,0.8099850968703428,"Guidelines:
728"
THEORY ASSUMPTIONS AND PROOFS,0.8107302533532041,"• The answer NA means that the paper does not include theoretical results.
729"
THEORY ASSUMPTIONS AND PROOFS,0.8114754098360656,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
730"
THEORY ASSUMPTIONS AND PROOFS,0.812220566318927,"referenced.
731"
THEORY ASSUMPTIONS AND PROOFS,0.8129657228017884,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
732"
THEORY ASSUMPTIONS AND PROOFS,0.8137108792846498,"• The proofs can either appear in the main paper or the supplemental material, but if
733"
THEORY ASSUMPTIONS AND PROOFS,0.8144560357675111,"they appear in the supplemental material, the authors are encouraged to provide a short
734"
THEORY ASSUMPTIONS AND PROOFS,0.8152011922503726,"proof sketch to provide intuition.
735"
THEORY ASSUMPTIONS AND PROOFS,0.815946348733234,"• Inversely, any informal proof provided in the core of the paper should be complemented
736"
THEORY ASSUMPTIONS AND PROOFS,0.8166915052160953,"by formal proofs provided in appendix or supplemental material.
737"
THEORY ASSUMPTIONS AND PROOFS,0.8174366616989568,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8181818181818182,"4. Experimental Result Reproducibility
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8189269746646796,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.819672131147541,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8204172876304023,"of the paper (regardless of whether the code and data are provided or not)?
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8211624441132638,"Answer: [Yes]
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8219076005961252,"Justification: See Section 6 and supplementary materials for details.
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8226527570789866,"Guidelines:
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.823397913561848,"• The answer NA means that the paper does not include experiments.
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8241430700447094,"• If the paper includes experiments, a No answer to this question will not be perceived
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8248882265275708,"well by the reviewers: Making the paper reproducible is important, regardless of
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8256333830104322,"whether the code and data are provided or not.
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8263785394932935,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.827123695976155,"to make their results reproducible or verifiable.
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8278688524590164,"• Depending on the contribution, reproducibility can be accomplished in various ways.
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8286140089418778,"For example, if the contribution is a novel architecture, describing the architecture fully
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8293591654247392,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8301043219076006,"be necessary to either make it possible for others to replicate the model with the same
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.830849478390462,"dataset, or provide access to the model. In general. releasing code and data is often
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8315946348733234,"one good way to accomplish this, but reproducibility can also be provided via detailed
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8323397913561847,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8330849478390462,"of a large language model), releasing of a model checkpoint, or other means that are
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8338301043219076,"appropriate to the research performed.
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.834575260804769,"• While NeurIPS does not require releasing code, the conference does require all submis-
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8353204172876304,"sions to provide some reasonable avenue for reproducibility, which may depend on the
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8360655737704918,"nature of the contribution. For example
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8368107302533532,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8375558867362146,"to reproduce that algorithm.
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8383010432190761,"(b) If the contribution is primarily a new model architecture, the paper should describe
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8390461997019374,"the architecture clearly and fully.
767"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8397913561847988,"(c) If the contribution is a new model (e.g., a large language model), then there should
768"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8405365126676602,"either be a way to access this model for reproducing the results or a way to reproduce
769"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8412816691505216,"the model (e.g., with an open-source dataset or instructions for how to construct
770"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.842026825633383,"the dataset).
771"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8427719821162444,"(d) We recognize that reproducibility may be tricky in some cases, in which case
772"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8435171385991058,"authors are welcome to describe the particular way they provide for reproducibility.
773"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8442622950819673,"In the case of closed-source models, it may be that access to the model is limited in
774"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8450074515648286,"some way (e.g., to registered users), but it should be possible for other researchers
775"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.84575260804769,"to have some path to reproducing or verifying the results.
776"
OPEN ACCESS TO DATA AND CODE,0.8464977645305514,"5. Open access to data and code
777"
OPEN ACCESS TO DATA AND CODE,0.8472429210134128,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
778"
OPEN ACCESS TO DATA AND CODE,0.8479880774962743,"tions to faithfully reproduce the main experimental results, as described in supplemental
779"
OPEN ACCESS TO DATA AND CODE,0.8487332339791356,"material?
780"
OPEN ACCESS TO DATA AND CODE,0.849478390461997,"Answer: [Yes]
781"
OPEN ACCESS TO DATA AND CODE,0.8502235469448585,"Justification: See supplementary materials for details.
782"
OPEN ACCESS TO DATA AND CODE,0.8509687034277198,"Guidelines:
783"
OPEN ACCESS TO DATA AND CODE,0.8517138599105812,"• The answer NA means that paper does not include experiments requiring code.
784"
OPEN ACCESS TO DATA AND CODE,0.8524590163934426,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
785"
OPEN ACCESS TO DATA AND CODE,0.853204172876304,"public/guides/CodeSubmissionPolicy) for more details.
786"
OPEN ACCESS TO DATA AND CODE,0.8539493293591655,"• While we encourage the release of code and data, we understand that this might not be
787"
OPEN ACCESS TO DATA AND CODE,0.8546944858420268,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
788"
OPEN ACCESS TO DATA AND CODE,0.8554396423248882,"including code, unless this is central to the contribution (e.g., for a new open-source
789"
OPEN ACCESS TO DATA AND CODE,0.8561847988077497,"benchmark).
790"
OPEN ACCESS TO DATA AND CODE,0.856929955290611,"• The instructions should contain the exact command and environment needed to run to
791"
OPEN ACCESS TO DATA AND CODE,0.8576751117734724,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
792"
OPEN ACCESS TO DATA AND CODE,0.8584202682563339,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
793"
OPEN ACCESS TO DATA AND CODE,0.8591654247391952,"• The authors should provide instructions on data access and preparation, including how
794"
OPEN ACCESS TO DATA AND CODE,0.8599105812220567,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
795"
OPEN ACCESS TO DATA AND CODE,0.860655737704918,"• The authors should provide scripts to reproduce all experimental results for the new
796"
OPEN ACCESS TO DATA AND CODE,0.8614008941877794,"proposed method and baselines. If only a subset of experiments are reproducible, they
797"
OPEN ACCESS TO DATA AND CODE,0.8621460506706409,"should state which ones are omitted from the script and why.
798"
OPEN ACCESS TO DATA AND CODE,0.8628912071535022,"• At submission time, to preserve anonymity, the authors should release anonymized
799"
OPEN ACCESS TO DATA AND CODE,0.8636363636363636,"versions (if applicable).
800"
OPEN ACCESS TO DATA AND CODE,0.8643815201192251,"• Providing as much information as possible in supplemental material (appended to the
801"
OPEN ACCESS TO DATA AND CODE,0.8651266766020864,"paper) is recommended, but including URLs to data and code is permitted.
802"
OPEN ACCESS TO DATA AND CODE,0.8658718330849479,"6. Experimental Setting/Details
803"
OPEN ACCESS TO DATA AND CODE,0.8666169895678092,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
804"
OPEN ACCESS TO DATA AND CODE,0.8673621460506706,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
805"
OPEN ACCESS TO DATA AND CODE,0.8681073025335321,"results?
806"
OPEN ACCESS TO DATA AND CODE,0.8688524590163934,"Answer: [Yes]
807"
OPEN ACCESS TO DATA AND CODE,0.8695976154992549,"Justification: See Section 6 for details.
808"
OPEN ACCESS TO DATA AND CODE,0.8703427719821163,"Guidelines:
809"
OPEN ACCESS TO DATA AND CODE,0.8710879284649776,"• The answer NA means that the paper does not include experiments.
810"
OPEN ACCESS TO DATA AND CODE,0.8718330849478391,"• The experimental setting should be presented in the core of the paper to a level of detail
811"
OPEN ACCESS TO DATA AND CODE,0.8725782414307004,"that is necessary to appreciate the results and make sense of them.
812"
OPEN ACCESS TO DATA AND CODE,0.8733233979135618,"• The full details can be provided either with the code, in appendix, or as supplemental
813"
OPEN ACCESS TO DATA AND CODE,0.8740685543964233,"material.
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8748137108792846,"7. Experiment Statistical Significance
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.875558867362146,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8763040238450075,"information about the statistical significance of the experiments?
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8770491803278688,"Answer: [Yes]
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8777943368107303,"Justification: See Section 6 for details.
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8785394932935916,"Guidelines:
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.879284649776453,"• The answer NA means that the paper does not include experiments.
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8800298062593145,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
822"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8807749627421758,"dence intervals, or statistical significance tests, at least for the experiments that support
823"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8815201192250373,"the main claims of the paper.
824"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8822652757078987,"• The factors of variability that the error bars are capturing should be clearly stated (for
825"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.88301043219076,"example, train/test split, initialization, random drawing of some parameter, or overall
826"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8837555886736215,"run with given experimental conditions).
827"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8845007451564829,"• The method for calculating the error bars should be explained (closed form formula,
828"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8852459016393442,"call to a library function, bootstrap, etc.)
829"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8859910581222057,"• The assumptions made should be given (e.g., Normally distributed errors).
830"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.886736214605067,"• It should be clear whether the error bar is the standard deviation or the standard error
831"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8874813710879285,"of the mean.
832"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8882265275707899,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
833"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8889716840536512,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
834"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8897168405365127,"of Normality of errors is not verified.
835"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8904619970193741,"• For asymmetric distributions, the authors should be careful not to show in tables or
836"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8912071535022354,"figures symmetric error bars that would yield results that are out of range (e.g. negative
837"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8919523099850969,"error rates).
838"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8926974664679582,"• If error bars are reported in tables or plots, The authors should explain in the text how
839"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8934426229508197,"they were calculated and reference the corresponding figures or tables in the text.
840"
EXPERIMENTS COMPUTE RESOURCES,0.8941877794336811,"8. Experiments Compute Resources
841"
EXPERIMENTS COMPUTE RESOURCES,0.8949329359165424,"Question: For each experiment, does the paper provide sufficient information on the com-
842"
EXPERIMENTS COMPUTE RESOURCES,0.8956780923994039,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
843"
EXPERIMENTS COMPUTE RESOURCES,0.8964232488822653,"the experiments?
844"
EXPERIMENTS COMPUTE RESOURCES,0.8971684053651267,"Answer: [Yes]
845"
EXPERIMENTS COMPUTE RESOURCES,0.8979135618479881,"Justification: See Section 6 for details.
846"
EXPERIMENTS COMPUTE RESOURCES,0.8986587183308494,"Guidelines:
847"
EXPERIMENTS COMPUTE RESOURCES,0.8994038748137109,"• The answer NA means that the paper does not include experiments.
848"
EXPERIMENTS COMPUTE RESOURCES,0.9001490312965723,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
849"
EXPERIMENTS COMPUTE RESOURCES,0.9008941877794336,"or cloud provider, including relevant memory and storage.
850"
EXPERIMENTS COMPUTE RESOURCES,0.9016393442622951,"• The paper should provide the amount of compute required for each of the individual
851"
EXPERIMENTS COMPUTE RESOURCES,0.9023845007451565,"experimental runs as well as estimate the total compute.
852"
EXPERIMENTS COMPUTE RESOURCES,0.9031296572280179,"• The paper should disclose whether the full research project required more compute
853"
EXPERIMENTS COMPUTE RESOURCES,0.9038748137108793,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
854"
EXPERIMENTS COMPUTE RESOURCES,0.9046199701937406,"didn’t make it into the paper).
855"
CODE OF ETHICS,0.9053651266766021,"9. Code Of Ethics
856"
CODE OF ETHICS,0.9061102831594635,"Question: Does the research conducted in the paper conform, in every respect, with the
857"
CODE OF ETHICS,0.9068554396423248,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
858"
CODE OF ETHICS,0.9076005961251863,"Answer: [Yes]
859"
CODE OF ETHICS,0.9083457526080477,"Justification: We have fully complied with the NeurIPS Code of Ethics.
860"
CODE OF ETHICS,0.9090909090909091,"Guidelines:
861"
CODE OF ETHICS,0.9098360655737705,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
862"
CODE OF ETHICS,0.910581222056632,"• If the authors answer No, they should explain the special circumstances that require a
863"
CODE OF ETHICS,0.9113263785394933,"deviation from the Code of Ethics.
864"
CODE OF ETHICS,0.9120715350223547,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
865"
CODE OF ETHICS,0.912816691505216,"eration due to laws or regulations in their jurisdiction).
866"
BROADER IMPACTS,0.9135618479880775,"10. Broader Impacts
867"
BROADER IMPACTS,0.9143070044709389,"Question: Does the paper discuss both potential positive societal impacts and negative
868"
BROADER IMPACTS,0.9150521609538003,"societal impacts of the work performed?
869"
BROADER IMPACTS,0.9157973174366617,"Answer: [Yes]
870"
BROADER IMPACTS,0.9165424739195231,"Justification: See Section 8 for details.
871"
BROADER IMPACTS,0.9172876304023845,"Guidelines:
872"
BROADER IMPACTS,0.9180327868852459,"• The answer NA means that there is no societal impact of the work performed.
873"
BROADER IMPACTS,0.9187779433681073,"• If the authors answer NA or No, they should explain why their work has no societal
874"
BROADER IMPACTS,0.9195230998509687,"impact or why the paper does not address societal impact.
875"
BROADER IMPACTS,0.9202682563338301,"• Examples of negative societal impacts include potential malicious or unintended uses
876"
BROADER IMPACTS,0.9210134128166915,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
877"
BROADER IMPACTS,0.9217585692995529,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
878"
BROADER IMPACTS,0.9225037257824144,"groups), privacy considerations, and security considerations.
879"
BROADER IMPACTS,0.9232488822652757,"• The conference expects that many papers will be foundational research and not tied
880"
BROADER IMPACTS,0.9239940387481371,"to particular applications, let alone deployments. However, if there is a direct path to
881"
BROADER IMPACTS,0.9247391952309985,"any negative applications, the authors should point it out. For example, it is legitimate
882"
BROADER IMPACTS,0.9254843517138599,"to point out that an improvement in the quality of generative models could be used to
883"
BROADER IMPACTS,0.9262295081967213,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
884"
BROADER IMPACTS,0.9269746646795827,"that a generic algorithm for optimizing neural networks could enable people to train
885"
BROADER IMPACTS,0.9277198211624441,"models that generate Deepfakes faster.
886"
BROADER IMPACTS,0.9284649776453056,"• The authors should consider possible harms that could arise when the technology is
887"
BROADER IMPACTS,0.9292101341281669,"being used as intended and functioning correctly, harms that could arise when the
888"
BROADER IMPACTS,0.9299552906110283,"technology is being used as intended but gives incorrect results, and harms following
889"
BROADER IMPACTS,0.9307004470938898,"from (intentional or unintentional) misuse of the technology.
890"
BROADER IMPACTS,0.9314456035767511,"• If there are negative societal impacts, the authors could also discuss possible mitigation
891"
BROADER IMPACTS,0.9321907600596125,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
892"
BROADER IMPACTS,0.9329359165424739,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
893"
BROADER IMPACTS,0.9336810730253353,"feedback over time, improving the efficiency and accessibility of ML).
894"
SAFEGUARDS,0.9344262295081968,"11. Safeguards
895"
SAFEGUARDS,0.9351713859910581,"Question: Does the paper describe safeguards that have been put in place for responsible
896"
SAFEGUARDS,0.9359165424739195,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
897"
SAFEGUARDS,0.936661698956781,"image generators, or scraped datasets)?
898"
SAFEGUARDS,0.9374068554396423,"Answer: [Yes]
899"
SAFEGUARDS,0.9381520119225037,"Justification: The data used in this paper are all benchmark tasks established by the commu-
900"
SAFEGUARDS,0.9388971684053651,"nity.
901"
SAFEGUARDS,0.9396423248882265,"Guidelines:
902"
SAFEGUARDS,0.940387481371088,"• The answer NA means that the paper poses no such risks.
903"
SAFEGUARDS,0.9411326378539493,"• Released models that have a high risk for misuse or dual-use should be released with
904"
SAFEGUARDS,0.9418777943368107,"necessary safeguards to allow for controlled use of the model, for example by requiring
905"
SAFEGUARDS,0.9426229508196722,"that users adhere to usage guidelines or restrictions to access the model or implementing
906"
SAFEGUARDS,0.9433681073025335,"safety filters.
907"
SAFEGUARDS,0.944113263785395,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
908"
SAFEGUARDS,0.9448584202682563,"should describe how they avoided releasing unsafe images.
909"
SAFEGUARDS,0.9456035767511177,"• We recognize that providing effective safeguards is challenging, and many papers do
910"
SAFEGUARDS,0.9463487332339792,"not require this, but we encourage authors to take this into account and make a best
911"
SAFEGUARDS,0.9470938897168405,"faith effort.
912"
LICENSES FOR EXISTING ASSETS,0.9478390461997019,"12. Licenses for existing assets
913"
LICENSES FOR EXISTING ASSETS,0.9485842026825634,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
914"
LICENSES FOR EXISTING ASSETS,0.9493293591654247,"the paper, properly credited and are the license and terms of use explicitly mentioned and
915"
LICENSES FOR EXISTING ASSETS,0.9500745156482862,"properly respected?
916"
LICENSES FOR EXISTING ASSETS,0.9508196721311475,"Answer: [Yes]
917"
LICENSES FOR EXISTING ASSETS,0.9515648286140089,"Justification: We have fully acknowledged baseline models, codes, and data in our paper.
918"
LICENSES FOR EXISTING ASSETS,0.9523099850968704,"Guidelines:
919"
LICENSES FOR EXISTING ASSETS,0.9530551415797317,"• The answer NA means that the paper does not use existing assets.
920"
LICENSES FOR EXISTING ASSETS,0.9538002980625931,"• The authors should cite the original paper that produced the code package or dataset.
921"
LICENSES FOR EXISTING ASSETS,0.9545454545454546,"• The authors should state which version of the asset is used and, if possible, include a
922"
LICENSES FOR EXISTING ASSETS,0.9552906110283159,"URL.
923"
LICENSES FOR EXISTING ASSETS,0.9560357675111774,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
924"
LICENSES FOR EXISTING ASSETS,0.9567809239940388,"• For scraped data from a particular source (e.g., website), the copyright and terms of
925"
LICENSES FOR EXISTING ASSETS,0.9575260804769001,"service of that source should be provided.
926"
LICENSES FOR EXISTING ASSETS,0.9582712369597616,"• If assets are released, the license, copyright information, and terms of use in the
927"
LICENSES FOR EXISTING ASSETS,0.9590163934426229,"package should be provided. For popular datasets, paperswithcode.com/datasets
928"
LICENSES FOR EXISTING ASSETS,0.9597615499254843,"has curated licenses for some datasets. Their licensing guide can help determine the
929"
LICENSES FOR EXISTING ASSETS,0.9605067064083458,"license of a dataset.
930"
LICENSES FOR EXISTING ASSETS,0.9612518628912071,"• For existing datasets that are re-packaged, both the original license and the license of
931"
LICENSES FOR EXISTING ASSETS,0.9619970193740686,"the derived asset (if it has changed) should be provided.
932"
LICENSES FOR EXISTING ASSETS,0.96274217585693,"• If this information is not available online, the authors are encouraged to reach out to
933"
LICENSES FOR EXISTING ASSETS,0.9634873323397913,"the asset’s creators.
934"
NEW ASSETS,0.9642324888226528,"13. New Assets
935"
NEW ASSETS,0.9649776453055141,"Question: Are new assets introduced in the paper well documented and is the documentation
936"
NEW ASSETS,0.9657228017883756,"provided alongside the assets?
937"
NEW ASSETS,0.966467958271237,"Answer: [Yes]
938"
NEW ASSETS,0.9672131147540983,"Justification: We have provided details documents for the codes.
939"
NEW ASSETS,0.9679582712369598,"Guidelines:
940"
NEW ASSETS,0.9687034277198212,"• The answer NA means that the paper does not release new assets.
941"
NEW ASSETS,0.9694485842026825,"• Researchers should communicate the details of the dataset/code/model as part of their
942"
NEW ASSETS,0.970193740685544,"submissions via structured templates. This includes details about training, license,
943"
NEW ASSETS,0.9709388971684053,"limitations, etc.
944"
NEW ASSETS,0.9716840536512668,"• The paper should discuss whether and how consent was obtained from people whose
945"
NEW ASSETS,0.9724292101341282,"asset is used.
946"
NEW ASSETS,0.9731743666169895,"• At submission time, remember to anonymize your assets (if applicable). You can either
947"
NEW ASSETS,0.973919523099851,"create an anonymized URL or include an anonymized zip file.
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746646795827124,"14. Crowdsourcing and Research with Human Subjects
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754098360655737,"Question: For crowdsourcing experiments and research with human subjects, does the paper
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761549925484352,"include the full text of instructions given to participants and screenshots, if applicable, as
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769001490312966,"well as details about compensation (if any)?
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977645305514158,"Answer: [NA]
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783904619970194,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791356184798807,"Guidelines:
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798807749627422,"• The answer NA means that the paper does not involve crowdsourcing nor research with
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806259314456036,"human subjects.
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981371087928465,"• Including this information in the supplemental material is fine, but if the main contribu-
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821162444113264,"tion of the paper involves human subjects, then as much detail as possible should be
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828614008941878,"included in the main paper.
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836065573770492,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843517138599106,"or other labor should be paid at least the minimum wage in the country of the data
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850968703427719,"collector.
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858420268256334,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865871833084948,"Subjects
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873323397913562,"Question: Does the paper describe potential risks incurred by study participants, whether
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880774962742176,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988822652757079,"approvals (or an equivalent approval/review based on the requirements of your country or
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895678092399404,"institution) were obtained?
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903129657228018,"Answer: [NA]
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910581222056631,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918032786885246,"Guidelines:
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992548435171386,"• The answer NA means that the paper does not involve crowdsourcing nor research with
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932935916542474,"human subjects.
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940387481371088,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947839046199702,"may be required for any human subjects research. If you obtained IRB approval, you
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955290611028316,"should clearly state this in the paper.
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996274217585693,"• We recognize that the procedures for this may vary significantly between institutions
978"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970193740685543,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
979"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977645305514158,"guidelines for their institution.
980"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985096870342772,"• For initial submissions, do not include any information that would break anonymity (if
981"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992548435171386,"applicable), such as the institution conducting the review.
982"
