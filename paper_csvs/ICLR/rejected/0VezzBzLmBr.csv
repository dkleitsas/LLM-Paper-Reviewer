Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004081632653061225,"Ofﬂine reinforcement learning could learn effective policies from a ﬁxed dataset,
which is promising in real-world applications. However, in ofﬂine decentralized
multi-agent reinforcement learning, due to the discrepancy between the behavior
policy and learned policy, the transition dynamics in ofﬂine experiences do not
accord with the transition dynamics in online execution, which creates severe er-
rors in value estimates, leading to uncoordinated and suboptimal policies. One
way to overcome the transition bias is to bridge ofﬂine training and online tuning.
However, considering both deployment efﬁciency and sample efﬁciency, we could
only collect very limited online experiences, making it insufﬁcient to use merely
online data for updating the agent policy. To utilize both ofﬂine and online ex-
periences to tune the policies of agents, we introduce online transition correction
(OTC) to implicitly correct the biased transition dynamics by modifying sampling
probabilities. We design two types of distances, i.e., embedding-based and value-
based distance, to measure the similarity between transitions, and further propose
an adaptive rank-based prioritization to sample transitions according to the transi-
tion similarity. OTC is simple yet effective to increase data efﬁciency and improve
agent policies in online tuning. Empirically, we show that OTC outperforms base-
lines in a variety of tasks."
INTRODUCTION,0.00816326530612245,"1
INTRODUCTION"
INTRODUCTION,0.012244897959183673,"In fully decentralized multi-agent reinforcement learning (MARL) (de Witt et al., 2020a), agents
interact with the environment to obtain individual experiences and independently improve the poli-
cies to maximize the cumulative shared reward. Due to the scalability, decentralized learning would
be promising in real-world cooperative tasks. However, in many industrial applications, continu-
ously interacting with the environment to collect the experiences for learning is costly and risky,
e.g., autonomous driving. To overcome this challenge, ofﬂine decentralized MARL (Jiang & Lu,
2021) lets each agent learn its policy from a ﬁxed dataset of experiences without interacting with the
environment. The dataset of each agent contains the individual action instead of the joint action of
all agents. There is no assumption on the data collection policies and the relationship between the
datasets of agents."
INTRODUCTION,0.0163265306122449,"However, from the perspective of each individual agent, the transition dynamics depend on the
policies of other agents and will change as other agents improve the policies (Foerster et al., 2017).
Since the behavior policies of other agents during data collection would be inconsistent with their
learned policies, the transition dynamics in the dataset would be different from the real transition
dynamics in execution, which will cause extrapolation error, i.e., the error in value estimate incurred
by the mismatch between the experience distribution of the learned policy and the dataset (Fujimoto
et al., 2019). The extrapolation error makes the agent underestimate or overestimate state values,
which leads to uncoordinated and suboptimal policies (Jiang & Lu, 2021)."
INTRODUCTION,0.02040816326530612,"One way to reduce the extrapolation error caused by the mismatch of transition dynamics is to bridge
ofﬂine training and online tuning. However, since both deploying new policies and interacting with
the environment are costly and risky, we should consider the deployment efﬁciency (the number of
deployments) and sample efﬁciency (the number of interactions) in the collection of online experi-
ences (Matsushima et al., 2021). Due to the efﬁciency requirement, the collected online experiences
can be very limited. Thus, it is insufﬁcient to tune the policies of agents merely using the online"
INTRODUCTION,0.024489795918367346,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02857142857142857,"data, and the small online dataset may also cause overﬁtting. To increase data efﬁciency, it is better
to additionally exploit the ofﬂine data for online tuning. However, uniformly sampling from the
merged ofﬂine and online experiences (Nair et al., 2020) cannot address the transition mismatch
problem. Therefore, it is necessary to correct the transition dynamics in the ofﬂine data to make it
close to the online transition dynamics. Nevertheless, the requirement of efﬁciency also means it is
infeasible to accurately estimate the real transition dynamics from the limited online experiences,
thus explicit correction is impractical."
INTRODUCTION,0.0326530612244898,"In this paper, we introduce a simple yet effective method to correct the transition dynamics of of-
ﬂine data for online tuning, without explicitly modeling the transition dynamics. When sampling a
transition from the ofﬂine experiences, we ﬁrst search for the best-matched transition in the online
experiences, which has the minimum state-action distance to the sampled transition. Then, we com-
pute the next-state distance between the sampled transition and the best-matched online transition
to represent the transition similarity. After that, a probability function maps the transition similarity
to the probability of accepting the sampled transition for update, which is equivalent to modifying
the transition probability. Therefore, the objective is to ﬁnd the optimal probability function which
minimizes the KL-divergence between the online transition distribution and the modiﬁed transition
distribution, given the distance measure. We design two distance measures based on the embed-
ding and Q-value of transitions, respectively. The embedding-based distance captures the similarity
in feature space, and the value-based distance measures the isomorphism. Due to the limited on-
line experiences, it is hard to ﬁnd the optimal probability function by gradient-based optimization,
but we empirically ﬁnd that the rank-based prioritization in PER (Schaul et al., 2016) is a proper
choice of the probability function. Moreover, we propose an adaptive rank-based prioritization to
adjust the degree of the correction according to the difference between ofﬂine and online transition
distributions."
INTRODUCTION,0.036734693877551024,"The proposed method, termed OTC (Online Transition Correction), could be applied to any ofﬂine
RL method for decentralized multi-agent learning. We construct the decentralized datasets from a
variety of D4RL tasks (Fu et al., 2020) and evaluate OTC on them. Experimental results show that
OTC outperforms baselines, and ablation studies demonstrate the effectiveness of the two distance
measures, the practicability of rank-based prioritization, and the improvement of adaptive prioritiza-
tion. To the best of our knowledge, OTC is the ﬁrst method for bridging ofﬂine training and online
tuning in decentralized MARL."
RELATED WORK,0.04081632653061224,"2
RELATED WORK"
OFFLINE RL,0.044897959183673466,"2.1
OFFLINE RL"
OFFLINE RL,0.04897959183673469,"In ofﬂine RL, the agent could only access to a ﬁxed dateset of single-step transitions collected by
a behavior policy, and no interactive experience collection is allowed during learning. Ofﬂine RL
easily suffers from the extrapolation error, which is mainly caused by out-of-distribution actions in
single-agent environments. To address this issue, constraint-based methods introduce policy con-
straints to enforce the learned policy to be close to the behavior policy, e.g., direct action constraint
(Fujimoto et al., 2019), kernel MMD (Kumar et al., 2019), Wasserstein distance (Wu et al., 2019),
and KL-divergence (Peng et al., 2019). Conservative methods (Kumar et al., 2020; He & Hou,
2020; Yu et al., 2021) train a Q-function pessimistic to out-of-distribution actions. Uncertainty-
based methods quantify the uncertainty by the learned environment model (Yu et al., 2020) or by
Monte Carlo dropout (Wu et al., 2021) of Q-function, and use it as a penalty or to weight the update
of Q-function, so as to avoid the overestimation of out-of-distribution actions."
OFFLINE RL,0.053061224489795916,"In ofﬂine decentralized MARL, besides out-of-distribution actions, the extrapolation error is also
caused by the bias of transition dynamics. For each individual agent, since the transition dynamics
depend on other agents’ policies which are also updating, there will be a difference between the
transition dynamics in the ofﬂine dataset and the real transition dynamics during online deployment
(Jiang & Lu, 2021). To overcome this, MABCQ (Jiang & Lu, 2021) uses two importance weights to
modify the ofﬂine transition dynamics by normalizing the biased transition probabilities and increas-
ing the transition probabilities of high-value next states. However, the modiﬁed transition dynamics
in MABCQ are not theoretically guaranteed to be close to the real ones. Unlike MABCQ, OTC
focuses on online tuning and exploits online experiences to correct the bias of transition dynamics
to quickly adapt to the learned policies of other agents."
OFFLINE RL,0.05714285714285714,Under review as a conference paper at ICLR 2022
OFFLINE RL,0.061224489795918366,ofﬂine training Env
OFFLINE RL,0.0653061224489796,online tuning Env
OFFLINE RL,0.06938775510204082,online tuning
OFFLINE RL,0.07346938775510205,"deployment  
deployment Env "
OFFLINE RL,0.07755102040816327,online tuning
OFFLINE RL,0.08163265306122448,deployment
OFFLINE RL,0.08571428571428572,"Figure 1: Overview of ofﬂine training and online tuning. After each agent i learning its policy π0
i from ofﬂine
dataset B0
i , their policies are deployed in the environment to get the online dataset D1
i . Then, π0
i is ﬁnetuned to
obtain π1
i using the merged dataset B1
i . The online tuning is repeated for K times."
BRIDGING OFFLINE LEARNING AND ONLINE TUNING,0.08979591836734693,"2.2
BRIDGING OFFLINE LEARNING AND ONLINE TUNING"
BRIDGING OFFLINE LEARNING AND ONLINE TUNING,0.09387755102040816,"Since the ofﬂine dataset is usually insufﬁcient to cover the entire transition space, the extrapolation
error cannot be eliminated entirely in the fully ofﬂine learning. It is crucial to improve the policy
trained using ofﬂine data further with online reinforcement learning. Since the online interaction
is expensive, we must consider both the deployment efﬁciency (the number of policy deployments)
and sample efﬁciency (the number of interactions) in online tuning. The concept of deployment
efﬁciency is adopted in BREMEN (Matsushima et al., 2021) and MUSBO (Su et al., 2021), which,
however, do not aim to ﬁnetune the pre-trained policy but instead train the policy from scratch with
limited deployments. AWAC (Nair et al., 2020) employs an implicit constraint that could miti-
gate the extrapolation error while avoiding overly conservative updates in ofﬂine learning and thus
quickly performs online ﬁnetuning. Balanced Replay (Lee et al., 2021) adopts prioritized sampling
to encourage the use of near-on-policy samples from the ofﬂine dataset. However, they deploy the
policy frequently, ignoring the deployment efﬁciency. Moreover, these methods above are designed
for single-agent environments, where ofﬂine and online data follow the same transition dynamics,
thus they cannot deal with transition bias. Abiding by both deployment and sample efﬁciency, OTC
uses prioritized sampling to reduce the bias of transition dynamics in decentralized MARL, rather
than the state-action distribution shift considered in Balanced Replay."
METHOD,0.09795918367346938,"3
METHOD"
PRELIMINARIES,0.10204081632653061,"3.1
PRELIMINARIES"
PRELIMINARIES,0.10612244897959183,"In ofﬂine and decentralized cooperative settings, each agent i could only access to an ofﬂine dataset
Bi, which is collected by a behavior policy and contains the tuples ⟨s, ai, r, s′⟩, where s is the state,
ai is the individual action of agent i, r is the shared reward, and s′ is the next state. Note that Bi dose
not contain the joint actions of all agents. Each agent i independently learns its policy πi using ofﬂine
RL algorithm, without information sharing among agents. The goal of all agents is to maximize
the expected return E PT
t=0 γtrt when deploying their learned policies in the environment, where
γ is the discount factor and T is the time horizon of the episode. From the perspective of each
agent i, the transition probability in Bi, denoted by PBi (s′|s, ai), depends on other agents’ behavior
policies during the collection of Bi, while the real transition probability in execution, denoted by
PEi (s′|s, ai), depends on other agents’ learned policies. The difference between PBi and PEi would
cause severe extrapolation errors, which eventually lead to uncoordinated and suboptimal policies
(Jiang & Lu, 2021)."
PRELIMINARIES,0.11020408163265306,"To ﬁnetune the policies learned from ofﬂine datasets, we allow the agents to interact with the envi-
ronment to collect online experiences. As illustrated in Figure 1, after ofﬂine learning using initial
dataset B0
i for each agent i, their learned policies ⟨π0
i , π0
−i⟩, where π0
−i denotes the policies of all
agents except i, are deployed in the environment and interact with each other for M timesteps. Each
agent i obtains an online dataset D1
i with M transitions. D1
i still only contains the individual actions
of agent i rather than the joint actions. We merge D1
i and B0
i to get B1
i , and ﬁnetune the policy π0
i to
obtain π1
i using B1
i . Then, we deploy the updated policies ⟨π1
i , π1
−i⟩in the environment and repeat"
PRELIMINARIES,0.11428571428571428,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.11836734693877551,"the procedures until K deployments. K represents the deployment efﬁciency, and K × M repre-
sents the sample efﬁciency. Note that for presentation simplicity, we denote π−i as also updating,
but other agents may or may not be learning online, which however does not affect the following
problem formulation and our method. That said OTC does not have any assumptions on other agents."
PROBLEM FORMULATION,0.12244897959183673,"3.2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.12653061224489795,"For agent i, given s and ai, the KL-divergence between the transition distributions of next state s′ in
the online dataset Dk
i and in the merged dataset Bk
i is"
PROBLEM FORMULATION,0.1306122448979592,"KL(PDk
i ∥PBk
i ) =
X"
PROBLEM FORMULATION,0.1346938775510204,"s′
PDk
i (s′|s, ai) log
PDk
i (s′|s, ai)"
PROBLEM FORMULATION,0.13877551020408163,"PBk
i (s′|s, ai) .
(1)"
PROBLEM FORMULATION,0.14285714285714285,"However, since KL(PDk
i ∥PBk
i ) is generally large, in order to use the merged dataset to ﬁnetune the
policy, we need to modify PBk
i as ˜PBk
i to minimize the KL-divergence between PDk
i and ˜PBk
i ."
PROBLEM FORMULATION,0.1469387755102041,"Since the difference of transition distributions means the difference of next-state distributions given
the same state-action pair, we ﬁrst deﬁne two distance functions: d(s1, ai1, s2, ai2) that measures
the similarity of state-action pairs, and d(s′
1, s′
2) that measures the similarity of next states. Once
sampling a transition ⟨s, ai, s′, r⟩from Bk
i , we select the best-matched transition ⟨s∗, a∗
i , s′∗, r∗⟩
from Dk
i , which has the minimum state-action distance to ⟨s, ai, s′, r⟩,"
PROBLEM FORMULATION,0.1510204081632653,"⟨s∗, a∗
i , s′∗, r∗⟩= arg min
Dk
i
d(s, ai, s∗, a∗
i ).
(2)"
PROBLEM FORMULATION,0.15510204081632653,"For the convenience of theoretical analysis, we assume there is always ⟨s∗, a∗
i , s′∗, r∗⟩in Dk
i that
meets d(s, ai, s∗, a∗
i ) = 0, i.e., s = s∗and ai = a∗
i . If there is more than one transition, we
uniformly select one from them. Then, we adopt a probability function f which maps the dis-
tance d(s′, s′∗) to the probability of accepting the sampled transition ⟨s, ai, s′, r⟩for update, i.e.,
f(d(s′, s′∗)). Therefore, the transition probability can be modiﬁed as"
PROBLEM FORMULATION,0.15918367346938775,"˜PBk
i (s′|s, ai) = PBk
i (s′|s, ai) ∗ P"
PROBLEM FORMULATION,0.16326530612244897,"ˆs′ PDk
i (ˆs′|s, ai)f(d(s′, ˆs′))"
PROBLEM FORMULATION,0.1673469387755102,"Z(s, ai)
,
(3)"
PROBLEM FORMULATION,0.17142857142857143,"where Z(s, ai) is a normalization term to make sure P
s′ ˜PBk
i (s′|s, ai) = 1. Thus the KL-divergence
between PDk
i and ˜PBk
i is"
PROBLEM FORMULATION,0.17551020408163265,"KL(PDk
i ∥˜PBk
i ) = KL(PDk
i ∥PBk
i ) −
X"
PROBLEM FORMULATION,0.17959183673469387,"s′
PDk
i (s′|s, ai) log P"
PROBLEM FORMULATION,0.1836734693877551,"ˆs′ PDk
i (ˆs′|s, ai)f(d(s′, ˆs′))"
PROBLEM FORMULATION,0.18775510204081633,"Z(s, ai)
.
(4)"
PROBLEM FORMULATION,0.19183673469387755,"To minimize the KL-divergence, we need to design appropriate d-functions to accurately mea-
sure the distances between transitions and ﬁnd the optimal f-function which properly satisﬁes
maxf
P"
PROBLEM FORMULATION,0.19591836734693877,"s′ PDk
i (s′|s, ai) log(
P"
PROBLEM FORMULATION,0.2,"ˆs′ PDk
i (ˆs′|s,ai)f(d(s′,ˆs′))/Z(s,ai))."
D-FUNCTIONS,0.20408163265306123,"3.3
d-FUNCTIONS"
D-FUNCTIONS,0.20816326530612245,"Due to the limitations of computational complexity and representation ability, directly measuring the
distances in the original space is impractical, especially in high-dimensional environments. There-
fore, we design two types of d-functions. The ﬁrst type is the distance in the embedding space. We
employ VAE (Kingma & Welling, 2013) to encode the state-action pair and next state into e(s, ai)
and e(s′), and deﬁne the d-functions as l1 distance in the embedding space,"
D-FUNCTIONS,0.21224489795918366,"de(s, ai, s∗, a∗
i ) = ∥e(s, ai) −e(s∗, a∗
i )∥,
de(s′, s′∗) = ∥e(s′) −e(s′∗)∥.
(5)"
D-FUNCTIONS,0.2163265306122449,"Due to the requirement of sample efﬁciency, it is impossible that we could always ﬁnd the transition
from Di with the same state-action pair as the given transition. Relying on the generalization ability
of the encoder, similar inputs will be encoded into similar embeddings. We could search for the
best-matched transition with the most similar state-action pair in terms of de(s, ai, s∗, a∗
i ) and then
evaluate the transition similarity using de(s′, s′∗)."
D-FUNCTIONS,0.22040816326530613,Under review as a conference paper at ICLR 2022
D-FUNCTIONS,0.22448979591836735,"Agents 
Other agents
Landmark"
D-FUNCTIONS,0.22857142857142856,"Transition 
Transition 
Transition"
D-FUNCTIONS,0.23265306122448978,"Figure 2: In this navigation task, the agents are learning to cover all the landmarks. For the transition a from
Bk
i , the best-matched transition b selected from Dk
i using de is still much different from transition a. However,
the value-based distance dq will select the nearly isomorphic transition c, which is more helpful for evaluating
the trainsition similarity."
D-FUNCTIONS,0.23673469387755103,"The embedding-based distance measures the similarity in feature space. However, since the limited
online experiences cannot cover all state-action pairs in ofﬂine dataset, there must be some transi-
tions in the ofﬂine dataset which are still much different from the transition with the most similar
state-action feature, e.g., transition a and b in Figure 2. In such cases, we cannot accurately evalu-
ate the transition similarity using de. Inspired by state representation learning (Gelada et al., 2019;
Zhang et al., 2020a;b), we notice that some state-action pairs may have common latent structure,
e.g., transition a and c in Figure 2. Although they are different in feature space, the topologies of
agents are isomorphic. As pointed by DeepMDP (Gelada et al., 2019), nearly isomorphic state-
action pairs would have similar Q-values. Therefore, we use the difference in Q-values to evaluate
the isomorphism in state-action pairs and select the best-matched transition from the online dataset,"
D-FUNCTIONS,0.24081632653061225,"dq(s, ai, s∗, a∗
i ) = ∥Q(s, ai) −Q(s∗, a∗
i )∥.
(6)"
D-FUNCTIONS,0.24489795918367346,"On the other hand, we use the expected Q-value to measure the distance of next states,"
D-FUNCTIONS,0.24897959183673468,"dq(s′, s′∗) =

V (s′)
Es′V (s′) −
V (s′∗)
Es′∗V (s′∗))
 ,
where V (s′) = EaiQ(s′, ai).
(7)"
D-FUNCTIONS,0.2530612244897959,"V (s′)/Es′V (s′) is the deviation from the expected value, which could mitigate the inﬂuence of absolute
value. The value-based distance dq has stronger representation ability than the embedding-based
distance de. The transitions that are close in embedding space will also have small value difference,
and the value-based distance could also represent the isomorphism of transitions."
F-FUNCTION,0.2571428571428571,"3.4
f-FUNCTION"
F-FUNCTION,0.2612244897959184,The optimal f-function is to maximize P
F-FUNCTION,0.2653061224489796,"s′ PDk
i (s′|s, ai) log
P"
F-FUNCTION,0.2693877551020408,"ˆs′ PDk
i (ˆs′|s,ai)f(d(s′,ˆs′))/Z(s,ai). How-
ever, since the online experiences are limited, it is hard to solve the optimal f-function by
gradient-based optimization. Nevertheless, there must exist such f-functions which do not increase
KL(PDk
i ∥˜PBk
i ), and a trivial example is the constant function. Therefore, we try to ﬁnd a heuris-
tic and practical f-function which is able to reduce the KL-divergence and extrapolation error. An
important prior is that f-function should be monotonic, which will produce a larger acceptance
probability when fed with a smaller distance of next states. The intuition is that if the next state
of the transition from Bk
i is more similar to the next state of the online experience with the same
state-action pair, the transition is more likely to follow the transition dynamics in Dk
i . Therefore, we
should give it a larger acceptance probability, which eventually leads to a larger transition probabil-
ity. Empirically, we ﬁnd the rank-based prioritized sampling in PER (Schaul et al., 2016) is a good
solution. Concretely, the probability of accepting transition j is"
F-FUNCTION,0.27346938775510204,"P(j) =
pα
j
P"
F-FUNCTION,0.27755102040816326,"m pαm
,
(8)"
F-FUNCTION,0.2816326530612245,"where the priority pj = 1/rank(j), and rank(j) is the rank of transition j when the transitions are
sorted according to d(s′, s′∗). The exponent α determines the degree of modifying the transition"
F-FUNCTION,0.2857142857142857,Under review as a conference paper at ICLR 2022
F-FUNCTION,0.2897959183673469,Algorithm 1 OTC for Agent i
F-FUNCTION,0.2938775510204082,"1: Initialize the RL model and the modiﬁcation degree α0
i
2: Train the RL model using B0
i to obtain the policy π0
i
3: for k = 1, . . . , K do
4:
Deploy the policy πk−1
i
in the environment
5:
Collect the online dataset Dk
i with M transitions
6:
Merge the experiences Bk
i = Bk−1
i
∪Dk
i
7:
if k > 1 then
8:
Adjust αk
i by (9)
9:
end if
10:
for t = 1, . . . , max update do
11:
Sample a minibatch B from Bk
i and a minibatch D from Dk
i
12:
for each transition in B do
13:
ﬁnd the best-matched transition in D (2) and compute the transition similarity
14:
end for
15:
Sample transitions from B by rank-based prioritization (8)
16:
Update the RL model using the sampled transitions
17:
end for
18: end for"
F-FUNCTION,0.2979591836734694,"probability, with α = 0 meaning the f-function degrades into a constant function. The rank-based
prioritization ensures that the probability of being accepted is monotonic in terms of transition sim-
ilarity, and is robust as it is insensitive to outliers."
F-FUNCTION,0.3020408163265306,"Another prior is that the modiﬁcation degree should depend on the distribution difference between
Dk
i and Bk
i . We should adopt a weaker modiﬁcation degree when the transition dynamics in Bk
i is
more similar to that in Dk
i , and vice versa. Inspired by that, we propose to adaptively adjust α at
each deployment k (k > 1) as"
F-FUNCTION,0.30612244897959184,"αk
i = αk−1
i
×
Dk
i
Dk−1
i
,
where Dk
i = Es,a,s′∼Bk
i d(s′, s′∗∼arg min
Dk
i
d(s, ai, s∗, a∗
i )).
(9)"
F-FUNCTION,0.31020408163265306,"Dk
i is the expected transition similarity for agent i at deployment k. As the difference between
ofﬂine and online transition distributions would change along with the update of agents, a ﬁxed α is
not an optimal solution. On the contrary, for example, if the distribution difference grows, adaptive
α is likely to take on large value, and thus more aggressively modiﬁes the transition dynamics."
IMPLEMENTATION DETAILS,0.3142857142857143,"3.5
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.3183673469387755,"For the embedding-based distance de, we do not maintain two embeddings e(s, ai) and e(s′),
but train a conditional VAE Gi = {Ei (µ, σ|s, ai, s′) , Di (ai|s, s′, z ∼(µ, σ))} which encodes
⟨s, ai, s′⟩into the embedding µ(s, ai, s′). We take the embedding µ(s, ai, s′) as a substitute for
both e(s, ai) and e(s′), which is more effective and computationally efﬁcient in practice. Moreover,
it is costly to sample transitions from Bk
i and then search the best-match transitions from Dk
i for
every update. To reduce the complexity, for each update we uniformly sample two minibatches, B
and D respectively from Bk
i and Dk
i , and perform the rank-based prioritized sampling on the two
minibatches. The complete training procedure is summarized in Algorithm 1."
EXPERIMENTS,0.3224489795918367,"4
EXPERIMENTS"
SETTINGS,0.32653061224489793,"4.1
SETTINGS"
SETTINGS,0.3306122448979592,"We evaluate OTC in D4RL (Fu et al., 2020) datasets with three types: random, medium, and
medium-replay. Following the settings in multi-agent mujoco (de Witt et al., 2020b; Jiang & Lu,
2021), we split the original action space of three mujoco tasks (Todorov et al., 2012), i.e., HalfChee-
tah, Walker2d, and Hopper, into several sub-spaces. As illustrated in Figure 3, different colors
indicate different agents. Each agent obtains the state and reward of the robot and independently"
SETTINGS,0.3346938775510204,Under review as a conference paper at ICLR 2022
SETTINGS,0.33877551020408164,"(a) HalfCheetah
(b) Walker2d
(c) Hopper"
SETTINGS,0.34285714285714286,"Figure 3: Illustrations of multi-agent mujoco tasks. Different colors mean different agents (Jiang & Lu, 2021)."
SETTINGS,0.3469387755102041,"controls one or some joints of the robot. For each agent i, we delete the actions of other agents
from the original dataset and take the modiﬁed dataset as B0
i . During online tuning, we perform
K = 10 deployments. For each deployment k, the agent collects a very limited online dataset Dk
i ,
of which the size is only one percent of the initial ofﬂine dataset (|Dk
i | = 1%|B0
i |). After online data
collection, we ﬁnetune the agents by L updates and deploy the updated policies in the environment
for next deployment."
SETTINGS,0.3510204081632653,"We instantiate OTC respectively on two ofﬂine RL algorithms, BCQ (Fujimoto et al., 2019) and
AWAC (Nair et al., 2020), and also take them as the baselines. During online tuning, the baselines
uniformly and randomly sample transitions from the merged ofﬂine and online dataset, and they
also have the same neural network architectures and hyperparameters as OTC. All the models are
trained for ﬁve runs with different random seeds, and results are presented using mean and standard
deviation. More details about hyperparameters are available in Appendix A."
EVALUATING D-FUNCTIONS,0.3551020408163265,"4.2
EVALUATING d-FUNCTIONS"
EVALUATING D-FUNCTIONS,0.35918367346938773,"We summarize the performance of the last deployment of OTC with de (embedding-based distance)
and dq (value-based distance) on BCQ in Table 1 and on AWAC in Table 2, and plot the learning
curves along with the number of deployments for a part of tasks in Figure 4. The empirical results
show that OTC with de or dq performs more than one standard deviation better than BCQ and
AWAC, which veriﬁes that de and dq are capable of properly measuring the transition similarity.
Since the online experiences are limited, uniformly sampling from the merged dataset is not effective"
EVALUATING D-FUNCTIONS,0.363265306122449,Table 1: Performance of OTC on BCQ.
EVALUATING D-FUNCTIONS,0.3673469387755102,"de + BCQ
dq + BCQ
BCQ"
EVALUATING D-FUNCTIONS,0.37142857142857144,"halfcheetah-random
1242 ± 11
1170 ± 16
1078 ± 23
walker2d-random
446 ± 24
444 ± 24
374 ± 32
hopper-random
328 ± 4
325 ± 8
309 ± 39"
EVALUATING D-FUNCTIONS,0.37551020408163266,"halfcheetah-medium-replay
2828 ± 65
2724 ± 35
2624 ± 55
walker2d-medium-replay
634 ± 40
730 ± 72
581 ± 128
hopper-medium-replay
753 ± 63
777 ± 271
568 ± 22"
EVALUATING D-FUNCTIONS,0.3795918367346939,"halfcheetah-medium
3725 ± 134
3732 ± 34
3638 ± 97
walker2d-medium
1449 ± 247
1406 ± 89
982 ± 49
hopper-medium
1286 ± 47
1405 ± 33
1169 ± 87"
EVALUATING D-FUNCTIONS,0.3836734693877551,Table 2: Performance of OTC on AWAC.
EVALUATING D-FUNCTIONS,0.3877551020408163,"de + AWAC
dq + AWAC
AWAC"
EVALUATING D-FUNCTIONS,0.39183673469387753,"halfcheetah-random
296 ± 276
301 ± 243
59 ± 110
walker2d-random
660 ± 270
278 ± 21
262 ± 8
hopper-random
552 ± 79
459 ± 65
261 ± 61"
EVALUATING D-FUNCTIONS,0.39591836734693875,"halfcheetah-medium-replay
2990 ± 216
3263 ± 97
2578 ± 188
walker2d-medium-replay
409 ± 67
459 ± 37
368 ± 60
hopper-medium-replay
2943 ± 130
1877 ± 703
1741 ± 455"
EVALUATING D-FUNCTIONS,0.4,"halfcheetah-medium
4253 ± 56
4176 ± 115
4090 ± 76
walker2d-medium
2027 ± 310
2099 ± 588
1059 ± 673
hopper-medium
2561 ± 533
2275 ± 785
1403 ± 384"
EVALUATING D-FUNCTIONS,0.40408163265306124,Under review as a conference paper at ICLR 2022
EVALUATING D-FUNCTIONS,0.40816326530612246,"0
2
4
6
8
10
Deployment 950 1000 1050 1100 1150 1200 1250"
EVALUATING D-FUNCTIONS,0.4122448979591837,Reward
EVALUATING D-FUNCTIONS,0.4163265306122449,"de+BCQ
dq+BCQ BCQ"
EVALUATING D-FUNCTIONS,0.4204081632653061,(a) halfcheetah-random
EVALUATING D-FUNCTIONS,0.42448979591836733,"0
2
4
6
8
10
Deployment 300 325 350 375 400 425 450 475"
EVALUATING D-FUNCTIONS,0.42857142857142855,Reward
EVALUATING D-FUNCTIONS,0.4326530612244898,"de+BCQ
dq+BCQ BCQ"
EVALUATING D-FUNCTIONS,0.43673469387755104,(b) walker2d-random
EVALUATING D-FUNCTIONS,0.44081632653061226,"0
2
4
6
8
10
Deployment 220 240 260 280 300 320 340"
EVALUATING D-FUNCTIONS,0.4448979591836735,Reward
EVALUATING D-FUNCTIONS,0.4489795918367347,"de+BCQ
dq+BCQ BCQ"
EVALUATING D-FUNCTIONS,0.4530612244897959,(c) hopper-random
EVALUATING D-FUNCTIONS,0.45714285714285713,"0
2
4
6
8
10
Deployment 2300 2400 2500 2600 2700 2800 2900"
EVALUATING D-FUNCTIONS,0.46122448979591835,Reward
EVALUATING D-FUNCTIONS,0.46530612244897956,"de+BCQ
dq+BCQ BCQ"
EVALUATING D-FUNCTIONS,0.46938775510204084,(d) halfcheetah-medium-replay
EVALUATING D-FUNCTIONS,0.47346938775510206,"0
2
4
6
8
10
Deployment 400 500 600 700 800"
EVALUATING D-FUNCTIONS,0.4775510204081633,Reward
EVALUATING D-FUNCTIONS,0.4816326530612245,"de+BCQ
dq+BCQ BCQ"
EVALUATING D-FUNCTIONS,0.4857142857142857,(e) walker2d-medium-replay
EVALUATING D-FUNCTIONS,0.4897959183673469,"0
2
4
6
8
10
Deployment 400 500 600 700 800 900 1000"
EVALUATING D-FUNCTIONS,0.49387755102040815,Reward
EVALUATING D-FUNCTIONS,0.49795918367346936,"de+BCQ
dq+BCQ BCQ"
EVALUATING D-FUNCTIONS,0.5020408163265306,(f) hopper-medium-replay
EVALUATING D-FUNCTIONS,0.5061224489795918,"0
2
4
6
8
10
Deployment 0 200 400 600 800"
EVALUATING D-FUNCTIONS,0.5102040816326531,Reward
EVALUATING D-FUNCTIONS,0.5142857142857142,"de+AWAC
dq+AWAC AWAC"
EVALUATING D-FUNCTIONS,0.5183673469387755,(g) halfcheetah-random
EVALUATING D-FUNCTIONS,0.5224489795918368,"0
2
4
6
8
10
Deployment 200 400 600 800"
EVALUATING D-FUNCTIONS,0.5265306122448979,Reward
EVALUATING D-FUNCTIONS,0.5306122448979592,"de+AWAC
dq+AWAC AWAC"
EVALUATING D-FUNCTIONS,0.5346938775510204,(h) walker2d-random
EVALUATING D-FUNCTIONS,0.5387755102040817,"0
2
4
6
8
10
Deployment 200 300 400 500 600"
EVALUATING D-FUNCTIONS,0.5428571428571428,Reward
EVALUATING D-FUNCTIONS,0.5469387755102041,"de+AWAC
dq+AWAC AWAC"
EVALUATING D-FUNCTIONS,0.5510204081632653,(i) hopper-random
EVALUATING D-FUNCTIONS,0.5551020408163265,"0
2
4
6
8
10
Deployment 2000 2200 2400 2600 2800 3000 3200 3400"
EVALUATING D-FUNCTIONS,0.5591836734693878,Reward
EVALUATING D-FUNCTIONS,0.563265306122449,"de+AWAC
dq+AWAC AWAC"
EVALUATING D-FUNCTIONS,0.5673469387755102,(j) halfcheetah-medium-replay
EVALUATING D-FUNCTIONS,0.5714285714285714,"0
2
4
6
8
10
Deployment 100 200 300 400 500 600"
EVALUATING D-FUNCTIONS,0.5755102040816327,Reward
EVALUATING D-FUNCTIONS,0.5795918367346938,"de+AWAC
dq+AWAC AWAC"
EVALUATING D-FUNCTIONS,0.5836734693877551,(k) walker2d-medium-replay
EVALUATING D-FUNCTIONS,0.5877551020408164,"0
2
4
6
8
10
Deployment 1000 1500 2000 2500 3000"
EVALUATING D-FUNCTIONS,0.5918367346938775,Reward
EVALUATING D-FUNCTIONS,0.5959183673469388,"de+AWAC
dq+AWAC AWAC"
EVALUATING D-FUNCTIONS,0.6,"(l) hopper-medium-replay
Figure 4: Learning curves of OTC on BCQ and AWAC."
EVALUATING D-FUNCTIONS,0.6040816326530613,"in correcting the transition bias. The value-based distance dq has a stronger representation ability
than the embedding-based distance de, since it could represent both the similarity in feature and
isomorphism. However, dq does not commonly outperform de. The reason might be that dq would
mistakenly judge the state-action pairs, which are different in feature and isomorphism but have
similar values, as best-matched pairs. Moreover, since the Q-values are updating, dq is inconsistent
during the tuning process."
EVALUATING F-FUNCTION,0.6081632653061224,"4.3
EVALUATING f-FUNCTION"
EVALUATING F-FUNCTION,0.6122448979591837,"The hyperparameter α controls the strength of modifying the transition dynamics. Figure 5 shows
the learning curves of OTC (de) with different α. It is observed that if α is too small, OTC has
weak effects on correcting the transition dynamics. However, if α is too large, the overly modiﬁed
transition dynamics would deviate from the real transition dynamics and degrade the performance.
Since the agents are continuously updated, the difference between the transition dynamics in Bk
i and
Dk
i will also change every deployment, so a ﬁxed α cannot deal with the nonstationarity. We test the
ﬁxed α = 1.0 and our method for adaptive α where initial α0 = 1.0. Figure 6 shows that adaptive α
could improve the performance of online tuning. Figure 7 shows the curves of α during the online
tuning of OTC (de). In halfcheetah-medium-replay, α of both two agents grows, which means the
difference between Bk
i and Dk
i increases as the agents are further improved during online tuning,
and α becomes stable in the latter deployments, which means the convergence of online tuning. In
halfcheetah-random and walker2d-random, two agents have different trends of α, which means that
the agents are inﬂuenced differently by the update of other agent. Our adaptive α method could
discriminate the transition biases of agents rather than treat them equally."
EVALUATING F-FUNCTION,0.6163265306122448,Under review as a conference paper at ICLR 2022
EVALUATING F-FUNCTION,0.6204081632653061,"0
2
4
6
8
10
Deployment 950 1000 1050 1100 1150 1200 1250"
EVALUATING F-FUNCTION,0.6244897959183674,Reward
EVALUATING F-FUNCTION,0.6285714285714286,"α = 0
α = 0.4
α = 1.0
α = 1.6
α = 2.0"
EVALUATING F-FUNCTION,0.6326530612244898,(a) halfcheetah-random
EVALUATING F-FUNCTION,0.636734693877551,"0
2
4
6
8
10
Deployment 300 325 350 375 400 425 450 475"
EVALUATING F-FUNCTION,0.6408163265306123,Reward
EVALUATING F-FUNCTION,0.6448979591836734,"α = 0
α = 0.4
α = 1.0
α = 1.6
α = 2.0"
EVALUATING F-FUNCTION,0.6489795918367347,(b) walker2d-random
EVALUATING F-FUNCTION,0.6530612244897959,"0
2
4
6
8
10
Deployment 2300 2400 2500 2600 2700 2800 2900"
EVALUATING F-FUNCTION,0.6571428571428571,Reward
EVALUATING F-FUNCTION,0.6612244897959184,"α = 0
α = 0.4
α = 1.0
α = 1.6
α = 2.0"
EVALUATING F-FUNCTION,0.6653061224489796,"(c) halfcheetah-medium-replay
Figure 5: Learning curves of OTC (de) on BCQ with different α."
EVALUATING F-FUNCTION,0.6693877551020408,"0
2
4
6
8
10
Deployment 950 1000 1050 1100 1150 1200 1250"
EVALUATING F-FUNCTION,0.673469387755102,Reward
EVALUATING F-FUNCTION,0.6775510204081633,"de, adaptive α
de, α = 1.0
dq, adaptive α
dq, α = 1.0"
EVALUATING F-FUNCTION,0.6816326530612244,(a) halfcheetah-random
EVALUATING F-FUNCTION,0.6857142857142857,"0
2
4
6
8
10
Deployment 300 325 350 375 400 425 450"
EVALUATING F-FUNCTION,0.689795918367347,Reward
EVALUATING F-FUNCTION,0.6938775510204082,"de, adaptive α
de, α = 1.0
dq, adaptive α
dq, α = 1.0"
EVALUATING F-FUNCTION,0.6979591836734694,(b) walker2d-random
EVALUATING F-FUNCTION,0.7020408163265306,"0
2
4
6
8
10
Deployment 2300 2400 2500 2600 2700 2800"
EVALUATING F-FUNCTION,0.7061224489795919,Reward
EVALUATING F-FUNCTION,0.710204081632653,"de, adaptive α
de, α = 1.0
dq, adaptive α
dq, α = 1.0"
EVALUATING F-FUNCTION,0.7142857142857143,"(c) halfcheetah-medium-replay
Figure 6: Learning curves of OTC (de) on BCQ with adaptive α and ﬁxed α (1.0)."
EVALUATING F-FUNCTION,0.7183673469387755,"2
4
6
8
10
Deployment 0.5 1.0 1.5 2.0 2.5 α"
EVALUATING F-FUNCTION,0.7224489795918367,"agent 1
agent 2"
EVALUATING F-FUNCTION,0.726530612244898,(a) halfcheetah-random
EVALUATING F-FUNCTION,0.7306122448979592,"2
4
6
8
10
Deployment 0.5 1.0 1.5 2.0 2.5 3.0 α"
EVALUATING F-FUNCTION,0.7346938775510204,"agent 1
agent 2"
EVALUATING F-FUNCTION,0.7387755102040816,(b) walker2d-random
EVALUATING F-FUNCTION,0.7428571428571429,"2
4
6
8
10
Deployment 0.9 1.0 1.1 1.2 1.3 α"
EVALUATING F-FUNCTION,0.746938775510204,"agent 1
agent 2"
EVALUATING F-FUNCTION,0.7510204081632653,"(c) halfcheetah-medium-replay
Figure 7: Curves of adaptive α of OTC (de) on BCQ during online tuning. Dotted lines show mean values, and
violin plots show distributions over seeds."
EVALUATING F-FUNCTION,0.7551020408163265,"0
2
4
6
8
10
Deployment 2800 3000 3200 3400 3600 3800"
EVALUATING F-FUNCTION,0.7591836734693878,Reward
EVALUATING F-FUNCTION,0.763265306122449,"OTC
Balanced Replay
Uniform
Online only"
EVALUATING F-FUNCTION,0.7673469387755102,(a) halfcheetah-medium
EVALUATING F-FUNCTION,0.7714285714285715,"0
2
4
6
8
10
Deployment 400 600 800 1000 1200 1400 1600"
EVALUATING F-FUNCTION,0.7755102040816326,Reward
EVALUATING F-FUNCTION,0.7795918367346939,"OTC
Balanced Replay
Uniform
Online only"
EVALUATING F-FUNCTION,0.7836734693877551,(b) walker2d-medium
EVALUATING F-FUNCTION,0.7877551020408163,"0
2
4
6
8
10
Deployment 400 600 800 1000 1200 1400"
EVALUATING F-FUNCTION,0.7918367346938775,Reward
EVALUATING F-FUNCTION,0.7959183673469388,"OTC
Balanced Replay
Uniform
Online only"
EVALUATING F-FUNCTION,0.8,"(c) hopper-medium
Figure 8: Learning curves of OTC, Balanced Replay, uniformly sampling from the merged dataset, and only
sampling from the online dataset."
EVALUATING F-FUNCTION,0.8040816326530612,"We additionally provide ablation studies of different sampling strategies in Figure 8. Since the online
dataset is very limited, ﬁnetuning the agents with online samples only is insufﬁcient, and the small
dataset could cause overﬁtting, e.g., Figure 8(b) and 8(c). Balanced Replay (Lee et al., 2021) and
uniformly sampling are susceptible to the transition bias. OTC obtains the performance gain over
the other three sampling strategies."
CONCLUSION,0.8081632653061225,"5
CONCLUSION"
CONCLUSION,0.8122448979591836,"We have proposed OTC to effectively correct the transition dynamics during online interaction for
tuning decentralized multi-agent policies learned from ofﬂine datasets, given limited online experi-
ences. OTC consists of two types of distances to measure the transition similarity and an adaptive
rank-based prioritization to sample transitions for updating the agent policy according to the tran-
sition similarity. Experimental results show that OTC outperform baselines for online tuning in a
variety of tasks."
CONCLUSION,0.8163265306122449,Under review as a conference paper at ICLR 2022
REFERENCES,0.8204081632653061,REFERENCES
REFERENCES,0.8244897959183674,"Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020a."
REFERENCES,0.8285714285714286,"Christian Schroeder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip Torr, Wendelin B¨ohmer,
and Shimon Whiteson. Deep multi-agent reinforcement learning for decentralized continuous
cooperative control. arXiv preprint arXiv:2003.06709, 2020b."
REFERENCES,0.8326530612244898,"Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet
Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement
learning. In International Conference on Machine Learning (ICML), 2017."
REFERENCES,0.8367346938775511,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.8408163265306122,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning (ICML), 2019."
REFERENCES,0.8448979591836735,"Carles Gelada, Saurabh Kumar, Jacob Buckman, Oﬁr Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, pp. 2170–2179. PMLR, 2019."
REFERENCES,0.8489795918367347,"Qiang He and Xinwen Hou.
Popo: Pessimistic ofﬂine policy optimization.
arXiv preprint
arXiv:2012.13682, 2020."
REFERENCES,0.8530612244897959,"Jiechuan Jiang and Zongqing Lu. Ofﬂine decentralized multi-agent reinforcement learning. arXiv
preprint arXiv:2108.01832, 2021."
REFERENCES,0.8571428571428571,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.8612244897959184,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-
learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems
(NeurIPS), 2019."
REFERENCES,0.8653061224489796,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.8693877551020408,"Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin.
Ofﬂine-to-
online reinforcement learning via balanced replay and pessimistic q-ensemble. arXiv preprint
arXiv:2107.00591, 2021."
REFERENCES,0.8734693877551021,"Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oﬁr Nachum, and Shixiang Gu. Deployment-
efﬁcient reinforcement learning via model-based ofﬂine optimization. In International Confer-
ence on Learning Representations (ICLR), 2021."
REFERENCES,0.8775510204081632,"Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020."
REFERENCES,0.8816326530612245,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019."
REFERENCES,0.8857142857142857,"Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
International Conference on Learning Representations (ICLR), 2016."
REFERENCES,0.889795918367347,"DiJia Su, Jason D Lee, John M Mulvey, and H Vincent Poor. Musbo: Model-based uncertainty
regularized and sample efﬁcient batch optimization for deployment constrained reinforcement
learning. arXiv preprint arXiv:2102.11448, 2021."
REFERENCES,0.8938775510204081,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012."
REFERENCES,0.8979591836734694,Under review as a conference paper at ICLR 2022
REFERENCES,0.9020408163265307,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.9061224489795918,"Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and
Hanlin Goh. Uncertainty weighted actor-critic for ofﬂine reinforcement learning. arXiv preprint
arXiv:2105.08140, 2021."
REFERENCES,0.9102040816326531,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based ofﬂine policy optimization. Advances in Neural Information
Processing Systems (NeurIPS), 2020."
REFERENCES,0.9142857142857143,"Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative ofﬂine model-based policy optimization. arXiv preprint arXiv:2102.08363,
2021."
REFERENCES,0.9183673469387755,"Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learn-
ing invariant representations for reinforcement learning without reconstruction. In International
Conference on Learning Representations (ICLR), 2020a."
REFERENCES,0.9224489795918367,"Amy Zhang, Shagun Sodhani, Khimya Khetarpal, and Joelle Pineau. Learning robust state abstrac-
tions for hidden-parameter block mdps. In International Conference on Learning Representations
(ICLR), 2020b."
REFERENCES,0.926530612244898,Under review as a conference paper at ICLR 2022
REFERENCES,0.9306122448979591,"A
HYPERPARAMETERS"
REFERENCES,0.9346938775510204,"The hyperparameters are summarized in Table 3. For the results in Table 1 and Table 2, we use grid
search to ﬁnd the optimal α from [0.6, 0.8, 1.0, 1.2, 1.4, 1.6]."
REFERENCES,0.9387755102040817,Table 3: Experimental settings and hyperparameters
REFERENCES,0.9428571428571428,"Hyperparameter
BCQ
AWAC"
REFERENCES,0.9469387755102041,"discount (γ)
0.99
0.99
|B|
512
512
|D|
2000
2000
batch size
128
128
hidden sizes
(64, 64)
(256, 256)
activation
ReLU
ReLU
actor learning rate
10−4
10−4"
REFERENCES,0.9510204081632653,"critic learning rate
10−4
5 × 10−4"
REFERENCES,0.9551020408163265,"embedding dimension
10
10
ﬁnetuning updates (L)
4000
2000"
REFERENCES,0.9591836734693877,"B
OTC ON MABCQ"
REFERENCES,0.963265306122449,"As the transition distribution of the learned policy in MABCQ does not follow PBi, MABCQ is not
a suitable backbone algorithm, though we additionally provides some results of OTC on MABCQ
in Figure 9. OTC+MABCQ could outperform MABCQ in online tuning. In halfcheetah-random,
MABCQ achieves better performance than BCQ in ofﬂine learning. However, since value deviation
in MABCQ has made the agent be optimistic toward other agents in ofﬂine training, the modiﬁed
transition dynamics in MABCQ is close to the real transition dynamics during the online interaction
with improved other agents, and thus MABCQ does not beneﬁt from online tuning in halfcheetah-
random."
REFERENCES,0.9673469387755103,"0
2
4
6
8
10
Deployment 950 1000 1050 1100 1150 1200 1250"
REFERENCES,0.9714285714285714,Reward
REFERENCES,0.9755102040816327,"OTC+MABCQ
MABCQ
BCQ"
REFERENCES,0.9795918367346939,(a) halfcheetah-random
REFERENCES,0.9836734693877551,"0
2
4
6
8
10
Deployment 275 300 325 350 375 400 425 450"
REFERENCES,0.9877551020408163,Reward
REFERENCES,0.9918367346938776,"OTC+MABCQ
MABCQ
BCQ"
REFERENCES,0.9959183673469387,"(b) walker2d-random
Figure 9: Learning curves of OTC (de) on MABCQ."
