Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003401360544217687,"We introduce DictFormer with efﬁcient shared dictionary to provide a compact,
fast, and accurate transformer model. DictFormer signiﬁcantly reduces the redun-
dancy in the transformer’s parameters by replacing the prior transformer’s parame-
ters with compact, shared dictionary, few unshared coefﬁcients and indices. Also,
DictFormer enables faster computations since expensive weights multiplications
are converted into cheap shared look-ups on dictionary and few linear projections.
Training dictionary and coefﬁcients are not trivial since indices used for looking
up dictionary are not differentiable. We adopt a sparse-constraint training with
l1 norm relaxation to learn coefﬁcients and indices in DictFormer. DictFormer is
ﬂexible to support different model sizes by dynamically changing dictionary size.
Compared to existing lightweight Transformers, DictFormer consistently reduces
model size over Transformer on multiple tasks, e.g., machine translation, abstrac-
tive summarization, and language modeling. Extensive experiments show that
DictFormer reduces 3.6× to 8.9× model size with similar accuracy over multiple
tasks, compared to Transformer."
INTRODUCTION,0.006802721088435374,"1
INTRODUCTION"
INTRODUCTION,0.01020408163265306,"Transformer (Vaswani et al., 2017) has been widely used in natural language processing (NLP) for
its superior capability in capturing long-distance dependencies. However, its good performance
comes with the giant model size. For example, T5 (Raffel et al., 2019) with a hidden dimension
of 65K and GPT-3 (Brown et al., 2020) with 96 transformer blocks have 11 billion and 175 bil-
lion parameters, respectively. These large Transformers suffer from multiple severe issues, such as
complicated learning and difﬁcult deployment on mobile/IoT devices. First, during the training of
a big transformer model, large training corpora (Raffel et al., 2019; Brown et al., 2020) or careful
regularization (Merity et al., 2017; Mehta et al., 2021) are required. Furthermore, the trained model
is over-parameterized (Reid et al., 2021; Zhao et al., 2021). The large model size with 11 billion and
175 billion parameters is beyond the capabilities of many edge devices including mobile devices and
IoTs. Therefore, there is an urgent need to design parameter-efﬁcient and fast transformer model
that eliminates redundant parameters and enables real-time NLP applications on the edge."
INTRODUCTION,0.013605442176870748,"Weights sharing is a choice proved by ALBERT (Lan et al., 2020) for designing compact and ef-
ﬁcient pre-trained transformer encoders, like BERT, on self-supervised learning task. However,
directly sharing all weights in an encoder-decoder transformer for a sequence-to-sequence task like
machine translation will dramatically decrease performance and accuracy (Reid et al., 2021). Al-
though a recent framework Universal Transformer (UT) (Dehghani et al., 2019) shows that a vanilla
transformer’s accuracy can be improved by using recursive weight sharing, UT’s high accuracy
comes at the cost of deeper blocks or wider hidden dimensions, which signiﬁcantly enlarges the
computational cost and does not necessarily reduce the model size."
INTRODUCTION,0.017006802721088437,"This paper introduces a new compact, fast, and accurate transformer architecture, DictFormer, that
can be easily trained and deployed on edge devices. DictFormer depends on dictionary sharing
and unshared linear projection coefﬁcients instead of weights sharing. Speciﬁcally, a shared dic-
tionary among all encoder/decoder blocks can signiﬁcantly reduce the parameter redundancy and
therefore compresses the model size. Few unshared linear projection with coefﬁcients on the shared
dictionary enable each encoder/decoder block to have distinct layer-wise feature representations,"
INTRODUCTION,0.02040816326530612,Published as a conference paper at ICLR 2022
INTRODUCTION,0.023809523809523808,"thus improving the representation abilities compared to prior weights sharing. Also, DictFormer
provides a method to dynamically control the layer-wise representation abilities by using the group-
wise shared dictionary for layers with large-dimension features, e.g., Feed-forward Network (FFN).
Last but not least, we show that training dictionary and coefﬁcients are not trivial since indices used
for looking up dictionary are not differentiable. We convert coefﬁcients and indices into a sparse
matrix and train it with l1 norm relaxation and convert this sparse matrix into dense coefﬁcients
and indices during inference."
INTRODUCTION,0.027210884353741496,"Extensive experiments demonstrate that our DictFormer provides signiﬁcant improvements over
existing transformers on three sequence-to-sequence tasks, (i) machine translation, (ii) abstractive
summarization, and (iii) language modeling. For machine translation, on IWSLT 2014 German-
English, DictFormer attains transformer performance with 8.9× fewer parameters and 2× fewer
Multi-Adds; on WMT 2014 German-English, DictFormer brings about 4.9× model compression
and 1.9× computation reduction; on WMT 2014 English-French, DictFormer obtains consistent
performance improvements: 4.9× model compression and 1.9× less computation with a similar
score. For abstractive summarization, DictFormer reduces the model size by more than 4.7× on
CNN-DailyMail dataset. For language modeling, DictFormer matches the performance of trans-
former with 3.6× to 5.7× fewer parameters than transformer on WikiText-103 benchmark."
RELATED WORK AND MOTIVATION,0.030612244897959183,"2
RELATED WORK AND MOTIVATION"
RELATED WORK AND MOTIVATION,0.034013605442176874,"Related lightweight Transformers. Several methods have been proposed to design lightweight
transformers. The ﬁrst line of research is to reduce the transformer computation complexities by
redesigning self-attention mechanism including (Katharopoulos et al., 2020; Zhou et al., 2021; Ra-
ganato et al., 2020; You et al., 2020; Correia et al., 2019; Kaiser et al., 2018). These methods cannot
reduce model size. The second line of research is model compression, e.g., quantization (Prato
et al., 2020; Lou et al., 2020b), pruning (Behnke & Heaﬁeld, 2020), low-rank factorization (Ma
et al., 2019), and knowledge distillation (Wang et al., 2020). These two directions of research can
be combined with our Dictformer. The third line of research is efﬁcient architecture design (So
et al., 2019; Wu et al., 2020; Mehta et al., 2021) by improving the expressiveness of transformers.
The forth line of research is weights sharing (Xia et al., 2019; Ma et al., 2019; Dehghani et al.,
2019; Reid et al., 2021; Takase & Kiyono, 2021) by reusing parameters across transformer blocks.
Weights sharing cannot reduce computations. Our Dictformer falls into the third and forth category.
We show that our Dictformer with Dictionary sharing can reduce both model size and computations."
RELATED WORK AND MOTIVATION,0.03741496598639456,"Transformers with Weights Sharing. Weight sharing is surprisingly effective to compress model
size for discriminate NLP models based on Transformer encoders, e.g., BERT. For example, prior
work ALBERT (Lan et al., 2020) shows that even sharing all parameters across layers does not
introduce any accuracy reduction. However, for generative sequence-to-sequence models based on
transformer’s encoders and decoders, sharing all parameters will signiﬁcantly decrease accuracy
on multiple standard machine translation or language modelling tasks (Reid et al., 2021; Takase &
Kiyono, 2021). To match vanilla Transformer’s accuracy, multiple works (Reid et al., 2021; Takase
& Kiyono, 2021; Xia et al., 2019; Ma et al., 2019) only share weights across partial layers instead
of all layers. However, partial weights sharing remarkably brings down the model size compression
effect of weights sharing. Also, how to decide which layers should be shared in partial weights
sharing is difﬁcult due to the large and dynamic search space that is dependent on the speciﬁc tasks."
RELATED WORK AND MOTIVATION,0.04081632653061224,"Transformer with all-parameters sharing such as Universal Transformer (Dehghani et al., 2019)
matches or improves transformer’s performance at the cost of a wider or deeper transformer archi-
tecture. A wider transformer with a larger embedding dimension enlarges the model size and brings
larger computations (Mult-Adds). A deeper transformer with more encoder/decoder blocks does
not only increase model size, but also introduces more computations. Importantly, weights sharing
techniques cannot reduce Mult-Adds numbers and training/inference time. Figure 6(a) in Appendix
shows the comparisons of Transformers with weights sharing and our dictionary sharing. Weights
sharing techniques cannot solve the deployment challenges of transformers on resource-limited de-
vices for real-time NLP applications."
RELATED WORK AND MOTIVATION,0.04421768707482993,Published as a conference paper at ICLR 2022
RELATED WORK AND MOTIVATION,0.047619047619047616,"Transformer
Subformer Tied Trasnformer"
RELATED WORK AND MOTIVATION,0.05102040816326531,"Universal 
Transformer"
RELATED WORK AND MOTIVATION,0.05442176870748299,DeLighT
RELATED WORK AND MOTIVATION,0.05782312925170068,Evolved Transformer
RELATED WORK AND MOTIVATION,0.061224489795918366,Lite Transformer
RELATED WORK AND MOTIVATION,0.06462585034013606,DictFormer
RELATED WORK AND MOTIVATION,0.06802721088435375,DictFormer-tiny 24 25 26 27 28 29
RELATED WORK AND MOTIVATION,0.07142857142857142,"5
10
15
20
25
30
35
40
45
50 BLEU"
RELATED WORK AND MOTIVATION,0.07482993197278912,# Params (in million)
RELATED WORK AND MOTIVATION,0.0782312925170068,"#Mult-Adds: 500M
1500M      4500M"
RELATED WORK AND MOTIVATION,0.08163265306122448,Smaller circle means fewer operations
RELATED WORK AND MOTIVATION,0.08503401360544217,"Figure 1: DictFormer achieves similar BLEU score with prior works using fewer parameters and
Mult-Adds on WMT 2014 De-En translation. DictFormer is ﬂexible and scalable, e.g., for mo-
bile devices, DictFormer-tiny can reduce 2.2× model size of lite transformer (Wu et al., 2020)
that achieves state-of-the-art performance under mobile settings. Results of existing works come
from their own implementations and #Params does not include embedding parameters, e.g., Trans-
former (Vaswani et al., 2017) has 44M #Params."
RELATED WORK AND MOTIVATION,0.08843537414965986,Motivation of DictFormer with Dictionary Sharing.
RELATED WORK AND MOTIVATION,0.09183673469387756,"Figure 1 shows that although transformers with weights sharing including Universal Trans-
former (Dehghani et al., 2019), Tied Transformer (Xia et al., 2019), and Subformer (Reid et al.,
2021) signiﬁcantly reduce the model size over vanilla transformer and lightweight transformer De-
LighT (Mehta et al., 2021), they cannot reduce the #Mult-Adds and even suffer from larger #Mult-
Adds. Particularly, Tied Transformer (Xia et al., 2019) and Subformer (Reid et al., 2021) compress
transformer model by 1.7× ∼2.6× but cannot reduce #Mult-Adds. Universal Transformer (De-
hghani et al., 2019) achieves ∼1 BLEU score improvement with 1.4× less parameters, but the
#Mult-Adds is increased by ∼4.3×, thereby signiﬁcantly prolonging the NLP inference latency
and restraining the deployment of real-time NLP applications on edge devices. To enable the de-
ployment of transformer models on mobile devices, recent work Evolved Transformer (So et al.,
2019) and Lite Transformer (Wu et al., 2020) try to design new lightweight transformer architecture
to meet the deﬁned mobile settings, e.g., 10 million parameters, but their tiny architectures suffer
from a large accuracy decrease. As Figure 1 shows, Evolved Transformer and Lite Transformer lose
2.9 and 2.4 BLEU score compared to base transformer, respectively."
DICTFORMER,0.09523809523809523,"3
DICTFORMER"
DICTFORMER,0.09863945578231292,"Overview. We propose DictFormer with dictionary sharing to enable a fast, compact, and accurate
transformer. When matching transformer’s accuracy, DictFormer can reduce more than 3.6× param-
eters and ∼3× Mult-Adds shown in Figure 1, which outperforms prior transformers with a higher
BLEU score. Also, DictFormer is ﬂexible to compress model size given an accuracy threshold. For
instance, when DictFormer matches the accuracy of Lite Transformer, it could further compress the
model of lite transformer by ∼2.2×. Given a N-layer transformer model, we can easily transform
it into DictFormer by converting all the weights in N blocks into one shared dictionary and few
unshared look-up coefﬁcients, shown in Figure 2. For example, N-layer weights attention W A
i and
FFN W F
i , where i ∈[0, N −1], in Transformer (Figure 2(a)) are represented by smaller dictio-
naries DA, DF that are shared by N blocks, and N-layer unshared coefﬁcients CA
i and CF
i
for
DictFormer (Figure 2(b)). Meanwhile, the attention and FFN operations in vanilla Transformer are
replaced with shared-dictionary attention and group-wise shared-dictionary FFN whose details are
introduced in our following contents. Dictformer reduces Transformer’s #Params from O(d2N) to
O(d(m + tN)). This is a O(dN/(m + tN))× model size reduction since dictionary size m < d,
coefﬁcient size t << d, where the embedding size is d. Dictformer also reduces #Mult-Adds from
O(d2Nn) to O(dNn(m + t)), where n is input sequence length. The details on how to deﬁne and
calculate #Params and #Mult-Adds are shown in Appendix A.1."
DICTFORMER,0.10204081632653061,Published as a conference paper at ICLR 2022 FFN
DICTFORMER,0.1054421768707483,Attention d
DICTFORMER,0.10884353741496598,"d
 WQ
Wi
A d"
D,0.11224489795918367,"4d Wi
F Embed"
D,0.11564625850340136,Encoder Block N x FFN
D,0.11904761904761904,Attention Embed
D,0.12244897959183673,"N x
Attention"
D,0.12585034013605442,Decoder Block
D,0.1292517006802721,#Params: (d2N)         #Mult-Adds: (d2Nn)
D,0.1326530612244898,Dict-FFN
D,0.1360544217687075,Dict-Attention Embed
D,0.13945578231292516,Encoder Block N x
D,0.14285714285714285,Dict-FFN
D,0.14625850340136054,Dict-Attention Embed
D,0.14965986394557823,"N x
Dict-Attention"
D,0.15306122448979592,Decoder Block d m DA
D,0.1564625850340136,"t
d
Ci
A d m"
D,0.1598639455782313,"DF
Ci
F
t
4d"
D,0.16326530612244897,#Params: (d(m+tN))    #Mult-Adds: (dNn(m+t))
D,0.16666666666666666,"(a) Transformer : Weights        &        are not shared 
(b) Our DictFormer: Dictionaries (DA & DF) are shared by N blocks
Wi
A 
Wi
F"
D,0.17006802721088435,"Figure 2: Our DictFormer replaces N-layer unshared, large attention and FFN weights W A
i , W F
i
(i ∈[0, N −1]) in transformer with smaller, shared dictionaries DA, DF and coefﬁcients CA
i ,
CF
i . The Mult-Adds operations between weights and inputs in Transformer are also reduced by our
dictionary look-ups and few linear projections with coefﬁcients."
D,0.17346938775510204,"Shared-dictionary Attention. Given a N-layer transformer model, we deﬁne that Qi, Ki, and Vi
are the i-th layer query, key, and values. Same to Transformer (Vaswani et al., 2017), we also use
equation 1 to calculate the attention scores once we have query, key, and values."
D,0.17687074829931973,"Attention(Qi, Ki, Vi) = softmax(Qi · KT
i
√"
D,0.18027210884353742,"d
) · Vi
(1)"
D,0.1836734693877551,"Our DictFormer utilizes equation 2 instead of MultiHead(Qi, Ki, Vi) = MHi · W O
i
used in
Transformer (Vaswani et al., 2017) to compute multi-head values, where MHi is derived according
to equation 3, and DA, CO
i , and IO
i are used to linearly project MHi, instead of W O
i . The attention
value headj
i for each head j in layer i is computed by equation 4 instead of headj
i = Attention(Qi·
W Qj
i
, Ki · W Kj
i
, Vi · W Vj
i ) used in existing transformers."
D,0.1870748299319728,"MultiHead(Qi, Ki, Vi) = SD(MHi, DA, CO
i , IO
i )
(2)"
D,0.19047619047619047,"MHi = Concat(head1
i , ..., headh
i )
(3)"
D,0.19387755102040816,"headj
i = Attention(SD(Qi, DA, C
Qj
i
, I
Qj
i
), SD(Ki, DA, C
Kj
i
, I
Kj
i
), SD(Vi, DA, C
Vj
i , I
Vj
i ))
(4)"
D,0.19727891156462585,"The reason why we use the shared attention dictionary DA, indices Ii and coefﬁcients Ci to replace
the larger and unshared weights W Qj
i
, W Kj
i
, W Vj
i
and W Oj
i
used in previous transformers is that
the linear projections of lookup of DA with coefﬁcients CX
i can have similar representation abilities
as W Xj
i
, where X represents inputs type, e.g., X = Q means input is query. To be speciﬁc, DA, Ii,
and Ci have size of d × mA, tA × d, and tA × d, respectively. mA and tA are the dictionary size and
coefﬁcient size in attention. As equation 5 shows, the linear projections in transformer’s attention,
e.g., W Qj
i
· Qj, are replaced by our lightweight shared dictionary projection function (SD), e.g.,
SD(Qj, DA, CQj
i
, IQj
i
), derived by equation 6 and equation 7. SD(Qj, DA, CQj
i
, IQj
i
) replaces
W Qj
i
·Qj since W Qj
i
can be replaced by looking up DA and few linear projections with coefﬁcients
CQj
i
without accuracy decrease. The following paragraphs and Figure 3(a) introduce the details of
look-up and scaling using equation 6 and equation 7."
D,0.20068027210884354,"Qj · W
Qj
i
⇒SD(Qj, DA, C
Qj
i
, I
Qj
i
)
(5)"
D,0.20408163265306123,Published as a conference paper at ICLR 2022
D,0.20748299319727892,"In equation 6, the lookup of DA by indices IQj
i
is deﬁned by DA[:, IQj
i
[t, id]] that can fetch the
IQj
i
[t, id]]-column vector from DA; Unshared linear projection that is used to enlarge the represen-"
D,0.2108843537414966,"tation ability of shared dictionary is depicted by ]
W Qj
i
= PtA"
D,0.21428571428571427,"t=1 CQj
i
[t, id] ⊙DA[:, IQj
i
[t, id]], where
⊙represents scaling a fetched vector from a dictionary with a scalar in coefﬁcients. Therefore, lin-"
D,0.21768707482993196,"early projecting Qj with Wi according to W Qj
i
· Qj is replaced by ]
W Qj
i
· Qj in equation 6 when"
D,0.22108843537414966,"we ﬁnd proper DA, IQj
i
,CQj
i
to meet that ]
W Qj
i
and Wi
Qj have similar representation abilities,"
D,0.22448979591836735,"e.g., they have matched accuracy. Directly generating ]
Wi
Qj and multiplying it with Qj potentially
increases the computations. To tackle this problem, we compute the multiplications between Qi
and dictionary DA as Oi ﬁrst according to equation 7, and reuse Oi for the following look-ups and
scaling with CQj
i
."
D,0.22789115646258504,"SD(Qi, DA, C
Qj
i
, I
Qj
i
) = Qi · tA
X"
D,0.23129251700680273,"t=1
C
Qj
i
[t, id] ⊙DA[:, I
Qj
i
[t, id]] = ( tA
X"
D,0.23469387755102042,"t=1
Qi · DA[:, I
Qj
i
[t, id]]) ⊙C
Qj
i
[t, id] = tA
X"
D,0.23809523809523808,"t=1
Oi[:, I
Qj
i
[t, id]] ⊙C
Qj
i
[t, id], id ∈[0, d h] (6)"
D,0.24149659863945577,"Oi[:, b] = Qi · DA[:, b], b ∈[1, mA]
(7)
We use Figure 3(a) to show an example of equation 5 about how to lookup dictionary with in- d"
D,0.24489795918367346,Shared Dictionary DA d
D,0.24829931972789115,"d
 Wi
Q"
D,0.25170068027210885,"1 13
35 +
+"
D,0.25510204081632654,"0.2
0.3
0.5"
D,0.2585034013605442,N-layer Weight WQ
D,0.2619047619047619,"tA=3
d"
D,0.2653061224489796,"Index Ii
Q"
D,0.2687074829931973,"[1, 13, 35]"
D,0.272108843537415,"d
tA=3"
D,0.2755102040816326,"Coefficients Ci
Q"
D,0.2789115646258503,"[0.2, 0.3, 0.5] mA "
D,0.282312925170068,4d
D,0.2857142857142857,Group-2 Dictionary DF2
D,0.2891156462585034,"1
7
27 +
+"
D,0.2925170068027211,"0.3
0.1
0.6 d"
D,0.29591836734693877,"Index Ii
F2"
D,0.29931972789115646,"[1, 7, 27] d"
D,0.30272108843537415,Coeffi.
D,0.30612244897959184,"[0.3, 0.1, 0.6] d"
D,0.30952380952380953,"[0.1, 0.6, 0.3]"
D,0.3129251700680272,Coeffi.
D,0.3163265306122449,"0.1
0.6
0.3 d"
D,0.3197278911564626,4d
D,0.3231292517006803,"N-layer Weight WF2
mF2 Wi
F2"
D,0.32653061224489793,"Ci
F2
g=1
Ci
F2
g=2"
D,0.3299319727891156,"tF2
tF2
tF2 "
D,0.3333333333333333,"(a) Replacing weight with shared-dictionary, index, coefficients (b) Replacing weight with group-wise dictionary and coefficients"
D,0.336734693877551,"Figure 3: (a) An example of looking up dictionary with indices and scaling it with coefﬁcients. This
lookup and scaling is able to reconstruct N-layer weight. (b) An example of looking up group-wise
dictionary and its scaling for large-dimension representation projection."
D,0.3401360544217687,"dices and scaling it with coefﬁcients. This process can create a N-layer weight W Q. For example,
given the shared dictionary DA, to generate the ﬁrst column of weight W Q
i , the ﬁrst column of
index matrix IQ
i is taken out, e.g., [1, 13, 35], and is used as indices to fetch the the corresponding
columns from DA, e.g., DA[:][1], DA[:][13], DA[:][35]. Then the ﬁrst column of coefﬁcients CQ
i ,
e.g., [0.2, 0.3, 0.5], are multiplied with DA[:][1], DA[:][13], DA[:][35] respectively, and the sum of
multiplication works as the ﬁrst column of W Q
i . In this way, weights W in attention with 4d2N pa-
rameters are compressed into mA, IQ, and CQ with size dmA + 8tAdN. For example, DictFormer
can reduce 8.7× model size when d = 512, mA = 256, tA = 24, N = 6. More calculation details
can be found at appendix A.1."
D,0.3435374149659864,"Group-wise Shared-dictionary FFN. The FFN in prior transformer includes two-layer compu-
tations: (i) F1 = max(0, Xi · W F1
i
+ b1), (ii) F2 = F1 · W F2
i
+ b2. Instead of the regular
linear projections Xi · W F1
i
+ b1 and F1 · W F2
i
+ b2, DictFormer uses a new lightweight pro-
jection called group-wise shared dictionary projection (GSD), i.e., GSD(Xi, D, CF1
i , IF1
i , G) and"
D,0.3469387755102041,"Published as a conference paper at ICLR 2022 n Oi tA
d"
D,0.35034013605442177,"Index Ii
[1, 13, 35] d
tA"
D,0.35374149659863946,"Coefficients Ci
[0.2, 0.3, 0.5]"
D,0.35714285714285715,"1 13
35
0.2
0.3
0.5 +
+ d n"
D,0.36054421768707484,Attention Results S d mA
D,0.36394557823129253,"[0.2, 0.3, 0.5]"
D,0.3673469387755102,Sparse Coefficients Z n
D,0.3707482993197279,Shared Oi . mA mA 
D,0.3741496598639456,"Figure 4: Since index I in DictFormer is not differentiable, we train sparse coefﬁcients Z instead of
jointly training I and C. After training, sparse Z is converted to I and C for deployment."
D,0.37755102040816324,"GSD(F1, D, CF2
i , IF2
i , G), in equation 8 and equation 9 to compute the FFN. Here GSD projection
is same as SD in shared-dictionary attention when G = 1. In SD projection, it works well to replace
a N weights of d × d with a d × m dictionary where each column of dictionary is multiplied with
a scalar in coefﬁcients. However, when the column of dictionary is large, e.g., 4 × d in the second
layer of FFN, SD projection is difﬁcult to achieve an accurate model since an vector of dictionary
with 4 × d, e.g. 2048, multiplied by a same scalar is not ﬂexible enough. To increase the ﬂexibility
of SD projection and improve performance, GSD ﬁrstly divides each column of dictionary into G
groups equally and assign different scalars to multiply the numbers in each group."
D,0.38095238095238093,"F1 = max(0, GSD(Xi, D, CF1
i , IF1
i , G) + b1)
(8)"
D,0.3843537414965986,"F2 = GSD(F1, D, CF2
i , IF2
i , G) + b2
(9)"
D,0.3877551020408163,"Equation 10 and equation 11 show the computations of GSD. Dictionary DF2[:, :] is divided
equally into G groups and the g-th group is deﬁned as DF2[Indexg, :], where Indexg = [(g −
1) 4d"
D,0.391156462585034,"G , g 4d"
D,0.3945578231292517,"G ) and g ∈[1, G]. Also, the multiplication result between g-th group dictionary and input
is deﬁned as Og[:, b] shown in equation 11, where b ∈[1, mF2]. Different groups of dictionary
share the same look-up indices IF2
i . The tF2 queried vectors in each group will be scaled by its
corresponding coefﬁcients C
F g
2
i
and the scaled results are then accumulated to derive the g-th GSD
result shown in equation 10. In addition, G group-wise GSD are further summed to compute the
ﬁnal GSD result."
D,0.3979591836734694,"GSD(X, DF2, CF2
i , IF2
i , G) = G
X g=1 tF2
X"
D,0.4013605442176871,"t=1
Og[:, IF2
i [t, id]] ⊙C
F g
2
i
[t, id]
(10)"
D,0.40476190476190477,"Og[:, b] = X[:, Indexg] · DF2[Indexg, b], g ∈[1, G], b ∈[1, mF2], Indexg = [(g −1)4d"
D,0.40816326530612246,"G , g 4d"
D,0.41156462585034015,"G )
(11)"
D,0.41496598639455784,"Figure 3(b) shows an example of group-2 shared dictionary for the the second layer of FFN. The
dictionary DF2 is equally split into two parts that share the same indices IF2
i
but each group has"
D,0.41836734693877553,"unique coefﬁcients, e.g., C
F g=1
2
i
and C
F g=2
2
i
. The ﬁrst group and the second group use the corre-
sponding coefﬁcients, e.g., [0.3, 0.1, 0.6] and [0.1, 0.6, 0.3], to scale the queried dictionary vectors,
respectively. The accumulation of scaled vectors can be used to represent the N-layer weight W F2."
D,0.4217687074829932,"Training DictFormer via Constraints and Relaxation. DictFormer replaces transformer weights
with linear projections of a dictionary, thereby having three-step computations including small pro-
jections, lookup and scale. DictFormer computes a small multiplication between input and dictio-
nary, generating intermediate variable O, looking up O and scaling it with coefﬁcients. To train the
DictFormer, one should jointly optimize dictionary, index I and coefﬁcients C. Directly training
the Dictformer leads to a combinatorial optimization problem since index I is non-continuous. Al-
though one could use AutoML, such as evolutionary method and Reinforcement learning to jointly
learn dictionary, I and C, these methods suffer from a large training time with low performance. To
workaround the training of index I, we reformulate the method to a regular linear projection with
sparse constraints, which can efﬁciently train the DictFormer."
D,0.42517006802721086,Published as a conference paper at ICLR 2022
D,0.42857142857142855,"We convert the index Ii and coefﬁcients Ci into sparse coefﬁcients Z, so that the training of index
Ii and coefﬁcients Ci is replaced with training sparse coefﬁcients Z. During the deployment phase,
the sparse coefﬁcients Z are reversely transformed into index Ii and coefﬁcients Ci shown in Figure
4. In particular, the shape of coefﬁcients C and index I is tA × d × N. The shape of sparse Z
is mA × d × N. There are two steps to derive Z: Initializing all elements as 0 and copying C
elements to Z according to the index values in Ii. For example, since the ﬁrst column of I1 and C1
is [1, 13, 35] and [0.2, 0.3, 0.5], all entries of the ﬁrst column in Z are zeros except the 1st entry, 13th
entry, and the 35th entry are 0.2, 0.3, and 0.5. Therefore, the lookup and scale of Oi can be converted
into the matrix multiplication between Oi and Zi shown in equation 12. The new coefﬁcient C has a
sparsity constraint that requires that the non-zero elements (l0 norm) of each column is tA << mA"
D,0.43197278911564624,"shown in equation 13. Now we can train Z with ||ZQj
i
[:, id]||l0 = tA, instead of training I and C. tA
X"
D,0.43537414965986393,"t=1
Oi[:, I[t, id]] ⊙C
Qj
i
[t, id] = Oi · Z
Qj
i
(12)"
D,0.4387755102040816,"||Z
Qj
i
[:, id]||l0 = tA
(13)"
D,0.4421768707482993,"The l0 norm sparsity constraint in equation 13 is non-differentiable and we cannot directly train Z.
To achieve the training with sparsity, we are inspired by existing sparse training methods Tsuruoka
et al. (2009); Li et al. (2016) to ﬁrstly loose this constraint to a l1 norm constraint shown in equa-
tion 14 to penalize the non-zero parameters and clip the unimportant values that are near to zeros
using equation 16. To be speciﬁc, the gradients of coefﬁcients are calculated by equation 15, where
parameter λ is used to control the trade-off between the gradient of loss L, δL"
D,0.445578231292517,"δZ , and l1 norm sparsity
constraint."
D,0.4489795918367347,"||Z||l1 = d
X"
D,0.4523809523809524,"id
||Z
Qj
i
[:, id]||l1
(14)"
D,0.4557823129251701,δ(L + λ||Z||l1)
D,0.45918367346938777,"δZ
= δL"
D,0.46258503401360546,"δZ + λsign(Z)
(15)"
D,0.46598639455782315,"Equation 16 is a threshold function during training. During the backpropagation, the derivative of
threshold function is 1 if |x| > value(ρ), 0 otherwise. During the forward phase, Equation 16 can
be used to globally clip the near-zero values to zero given a ratio ρ, where value(ρ) derives the value
at ratio ρ in the ascending order. One also can force that at most t of the elements in each column of
Z are non-zero while the rest are set to 0 by letting value(ρ) is the t-th largest value of each column."
D,0.46938775510204084,µ(x) =
D,0.47278911564625853,"(
x,
if |x| > value(ρ).
0
otherwise.
(16)"
EXPERIMENTAL METHODOLOGY,0.47619047619047616,"4
EXPERIMENTAL METHODOLOGY"
EXPERIMENTAL METHODOLOGY,0.47959183673469385,"Machine Translation Dataset.
Three machine translation benchmarks are tested: IWSLT’14
German-English (De-En), WMT’14 English to German (En-De), and WMT’14 English to France
(En-Fr). For IWSLT’14 De-En, we adopt the same settings in Wu et al. (2020) with 160K training
sentence pairs and 10K joint byte pair encoding (BPE) vocabulary in lower case. For WMT’14
En-De, our models are trained with 4.5M sentence pairs, validated on newstest2013, and tested on
newstest2014. Also, a 32K joint source and target BPE is used in the vocabulary. For WMT’14 En-
Fr, we follow the setup in Wu et al. (2020) by training models on 36M training sentence pairs from
WMT’14, validating on newstest2012 and 2013, and testing on newstest2014. The vocabulary has a
size of 40K and is based on a joint source and target BPE factorization. The evaluation setting used
in Vaswani et al. (2017) is utilized with a beam size of 4 and a length penalty of 0.6. We replicate
the same BLEU calculations in Wu et al. (2020) with case-sensitive tokenization. The last 10 model
checkpoints are averaged for testing and the lowest-perplexity model is picked for validation."
ABSTRACT,0.48299319727891155,"Abstractive Summarization Dataset. We evaluate DictFormer on CNN-DailyMail dataset (Chen
et al., 2016) that has 280K news articles with multi-sentence summaries. We follow the settings"
ABSTRACT,0.48639455782312924,Published as a conference paper at ICLR 2022
ABSTRACT,0.4897959183673469,"in (Wu et al., 2020), truncate the articles to 3000 tokens, use a 30K BPE vocabulary and F1-Rouge
as the metric."
ABSTRACT,0.4931972789115646,"Language Modeling Dataset. We also evaluate DictFormer on WIKITEXT-103 (Merity et al.,
2016) that has a 260K BPE vocabulary and contains 103M/217K/245K tokens for training, valida-
tion, and testing. Language modeling performance about perplexity (ppl) is reported."
ABSTRACT,0.4965986394557823,"Architecture and Evaluation. For machine translation tasks, we follow the sequence-to-sequence
transformer (Vaswani et al., 2017) with encoder and decoder to develop our model. For IWSLT
dataset, the general settings are same as (So et al., 2019). For WMT dataset, the setting is based
on (Wu et al., 2020). Also, the same model for machine translation is used for summarization task.
For language modelling, we use the transformer-based decoder architecture of (Baevski & Auli,
2019) but with 512 model dimensions and 12 blocks that is same to (Wu et al., 2020). Fairseq’s
transformer implementation (Ott et al., 2019) is used as the backbone for the baseline model."
ABSTRACT,0.5,"In our DictFormer architecture, encoder and decoder have three dictionaries, respectively, i.e., en-
coder attention dictionary DA
enc, encoder F1 dictionary DF1
enc, encoder F2 dictionary DF2
enc, and
decoder attention dictionary DA
dec, decoder F1 dictionary DF1
dec, decoder F2 dictionary DF2
dec. Each
of them is shared by all blocks in encoder or decoder. For example, DA
enc is shared by all attention
blocks in encoder. We study the effects of DictFormer with various dictionary size from 80 to 240
and different coefﬁcient size from 12 to 84 shown in Figure 8. One can pick up a dictionary size and
coefﬁcient size to control the trade-off between accuracy and model size according to the require-
ments. For example, one could get the results in our tables when dictionary size is set as mA =
d
2.5,
mF1 =
d
2.5, mF2 = d"
ABSTRACT,0.5034013605442177,"2, and tA =
d
10, tF1 = tF2 =
d
15."
ABSTRACT,0.5068027210884354,"In this paper, #Params means parameter numbers of transformer without including embedding lay-
ers; #TParams shows parameter numbers of model and embedding layers. The Mult-Adds calcu-
lation is tested when a model is used to translating two sentences with the length of 30 in default
(same to previous work Wu et al. (2020))."
ABSTRACT,0.5102040816326531,"Table 1: Comparison with state-of-the-art transformers on machine translation tasks. For a fair com-
parison with existing tiny transformer, We follow Lite Transformer (Wu et al., 2020) to scaling down
the hidden size to meet the deployment requirements on mobile settings. For example, Transformer
uses a 128 hidden size and 2.8M #Params; #Ops represents Mult-Adds and Entries with ± shows
the average across three independent runs."
ABSTRACT,0.5136054421768708,"IWSLT’14 De-En
WMT’14 En-De
WMT’14 En-Fr
#Params Ratios
BLEU
∆BLEU #Ops #Params Ratios
BLEU
∆BLEU #Ops
BLEU
∆BLEU"
ABSTRACT,0.5170068027210885,"Transformer (Vaswani et al., 2017)
2.8M
1.0×
27.8
-
63M
2.8M
1.0×
21.3
-
87M
33.6
-
Universal Trans (Dehghani et al., 2019) 2.1 M
1.5×
30.3
+2.5
152M
2.6M
1.1 ×
22.4
+1.1
193M
35.8
+2.2
Tensorized. Trans (Ma et al., 2019)
1.4 M
2×
28.0
0.2
-
1.7M
1.6×
21.2
-0.1
-
33.4
-0.2
Lite Transformer (Wu et al., 2020)
2.3 M
1.2× 29.6±0.4
+1.8
46M
1.8 M
1.6× 21.8±0.3
+0.6
47M 35.3±0.4
+1.7
DeLighT (Mehta et al., 2021)
1.1 M
2.5× 28.9±0.2
+1.1
49M
1.6M
1.8× 21.8±0.1
+0.5
64 M 34.2±0.2
+0.6
Subformer (Reid et al., 2021)
1.3M
2.2× 27.6±0.3
-0.2
61M
1.9M
1.5 × 21.0±0.2
-0.3
85M 33.5±0.4
-0.1
DictFormer
0.3 M
8.9× 30.0±0.3
+2.2
32M
0.6 M
4.9× 22.0±0.2
+0.7
46M 35.6±0.3
+2.0"
RESULTS,0.5204081632653061,"5
RESULTS"
RESULTS,0.5238095238095238,"Machine Translation. We ﬁrst report the machine translation results and compare them with prior
works. Our baseline transformer model settings are in line with Lite Trans. (Wu et al., 2020) which
provides the best results under mobile settings. Our DictFormer generally outperforms the state-of-
the-art transformers on IWSLT’14 De-En, WMT’14 En-De, and WMT’14 En-Fr. Table 1 represents
the quantitative results of our DictFormer. Compared to Transformer (Vaswani et al., 2017), Dict-
Former compresses 8.9× model size, reduces 1.9× #Mult-Adds, and improves 2.2 BLEU score on
IWSLT’14 De-En, which achieves better trade-off between performance and model size than pre-
vious works. Similar to IWSLT’14 De-En, our experiments on larger corpora including WMT’14
En-De and En-Fr also show DictFormer obtains more compact and accurate model than existing
Transformers. For instance, only our DictFormer can compress more than 6× model size over
Transformer without decreasing any model performance under mobile settings."
RESULTS,0.5272108843537415,Published as a conference paper at ICLR 2022
RESULTS,0.5306122448979592,"Table 2: Comparison with state-of-the-art transformers on abstractive summarization and Language
modeling tasks. #Ops is calculated by longer sequence with the input length of 100. Entries with ±
represents the average across three independent runs."
RESULTS,0.5340136054421769,"CNN-DailyMail for abstractive summarization
WIKITEXT103 for language modeling
R1
R2
RL
#Ops
#Params
Ratios
Valid ppl.
Test ppl.
#Ops
#Params
Ratios"
RESULTS,0.5374149659863946,"Trans./Adaptive
41.4
18.9
38.3
3.6G
41.4M
1.0 ×
23.2
24.0
50.3G
37.8M
1.0×
Lite Transformer
41.3
18.8
38.3
1.5G
17.3M
2.4 ×
21.4
22.2
48.7G
37.2M
1.01×
DelighT
-
-
-
-
-
-
23.5
24.1
-
∼33M
1.1×
Subformer
41.6
19.2
38.4
-
41M
1.01 ×
20.8
21.1
∼50G
∼18M
2.1×
DictFormer
41.3±0.2
18.9±0.1
38.3±0.1
0.9G
8.9M
4.7×
21.3±0.2
22.2±0.2
20.3G
10.0M
3.8×"
ABSTRACT,0.5408163265306123,"Abstractive Summarization and Language Modeling. We then report the abstractive summariza-
tion and language modeling results and compare them with prior works. Abstractive summarization
model follows the transformer model, but language modeling model follows the adaptive inputs
model (Baevski & Auli, 2019). Our DictFormer consistently outperforms prior transformers on
both summarization and language modeling tasks. Table 2 represents the quantitative results of our
DictFormer. For abstractive summarization on CNN-DailyMail, DictFormer achieves similar F1-
Rouge score with Transformer but requires 4.7× less model size and ∼4× less computations. For
language modeling on WIKITEXT103, compared to adaptive inputs model (Baevski & Auli, 2019),
DictFormer compresses 3.8× model size with less computations and matched testing perplexity."
ABSTRACT,0.54421768707483,"Sensitive Study and Ablation Study. We study the DictFormer’s performance under different
model size and hyper-parameters in Appendix. We also use table 3 in Appendix to show the ablation
study of DictFormer’s techniques. The performance of DictFormer improves with an increase in the
number of model parameters, across different corpora. DictFormer achieves similar BLEU score
with Transformer using fewer model parameters. Although DictFormer’s performance improves
with an increase in the size of dictionary and coefﬁcients, the model size is also enlarged. So one
should pick up a proper dictionary or coefﬁcient size according to accuracy requirements."
CONCLUSION,0.5476190476190477,"6
CONCLUSION"
CONCLUSION,0.5510204081632653,"We propose DictFormer with dictionary sharing to enable a fast, compact, and accurate transformer.
DictFormer signiﬁcantly reduces both model size and computation parameters of prior transformers
with similar performance. Also, DictFormer can consistently improve the performance on multiple
tasks, such as machine translation, abstractive summarization, and language modeling with similar
model size. DictFormer code is available at https://github.com/SamNLP/DictFormer."
CONCLUSION,0.5544217687074829,Published as a conference paper at ICLR 2022
REFERENCES,0.5578231292517006,REFERENCES
REFERENCES,0.5612244897959183,"Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=ByxZX20qFQ."
REFERENCES,0.564625850340136,"Maximiliana Behnke and Kenneth Heaﬁeld. Losing heads in the lottery: Pruning transformer atten-
tion in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 2664–2674, 2020."
REFERENCES,0.5680272108843537,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR,
abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165."
REFERENCES,0.5714285714285714,"Danqi Chen, Jason Bolton, and Christopher D. Manning. A thorough examination of the cnn/daily
mail reading comprehension task. CoRR, abs/1606.02858, 2016. URL http://arxiv.org/
abs/1606.02858."
REFERENCES,0.5748299319727891,"Gonc¸alo M Correia, Vlad Niculae, and Andr´e FT Martins. Adaptively sparse transformers. arXiv
preprint arXiv:1909.00015, 2019."
REFERENCES,0.5782312925170068,"Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
transformers. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=HyzdRiR9Y7."
REFERENCES,0.5816326530612245,"Bo Feng, Qian Lou, Lei Jiang, and Geoffrey Fox. CRYPTOGRU: Low latency privacy-preserving
text analysis with GRU. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing, pp. 2052–2057, Online and Punta Cana, Dominican Republic, November
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.156."
REFERENCES,0.5850340136054422,"Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam
Shazeer. Fast decoding in sequence models using discrete latent variables. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 2390–2399. PMLR, 10–15 Jul
2018. URL https://proceedings.mlr.press/v80/kaiser18a.html."
REFERENCES,0.5884353741496599,"Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International Conference on Ma-
chine Learning, pp. 5156–5165. PMLR, 2020."
REFERENCES,0.5918367346938775,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=H1eA7AEtvS."
REFERENCES,0.5952380952380952,"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient convnets. arXiv preprint arXiv:1608.08710, 2016."
REFERENCES,0.5986394557823129,"Qian Lou and Lei Jiang. She: A fast and accurate deep neural network for encrypted data. In
Advances in Neural Information Processing Systems, pp. 10035–10043, 2019."
REFERENCES,0.6020408163265306,"Qian Lou, Bo Feng, Geoffrey C. Fox, and Lei Jiang. Glyph: Fast and accurately training deep neural
networks on encrypted data. CoRR, abs/1911.07101, 2019."
REFERENCES,0.6054421768707483,"Qian Lou, Song Bian, and Lei Jiang. Autoprivacy: Automated layer-wise parameter selection for
secure neural network inference. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 8638–8647.
Curran Associates, Inc., 2020a."
REFERENCES,0.608843537414966,Published as a conference paper at ICLR 2022
REFERENCES,0.6122448979591837,"Qian Lou, Feng Guo, Minje Kim, Lantao Liu, and Lei Jiang. Autoq: Automated kernel-wise neural
network quantization. In International Conference on Learning Representations, 2020b."
REFERENCES,0.6156462585034014,"Qian Lou, Yilin Shen, Hongxia Jin, and Lei Jiang. Safenet: A secure, accurate and fast neural
network inference. In International Conference on Learning Representations, 2021."
REFERENCES,0.6190476190476191,"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song.
A tensorized transformer for language modeling. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.
cc/paper/2019/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf."
REFERENCES,0.6224489795918368,"Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
Delight: Deep and light-weight transformer. In International Conference on Learning Represen-
tations, 2021. URL https://openreview.net/forum?id=ujmgfuxSLrO."
REFERENCES,0.6258503401360545,"Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843."
REFERENCES,0.6292517006802721,"Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-
guage models. arXiv preprint arXiv:1708.02182, 2017."
REFERENCES,0.6326530612244898,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint
arXiv:1904.01038, 2019."
REFERENCES,0.6360544217687075,"Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh. Fully quantized transformer for machine
translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1–
14, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
ﬁndings-emnlp.1. URL https://aclanthology.org/2020.findings-emnlp.1."
REFERENCES,0.6394557823129252,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683."
REFERENCES,0.6428571428571429,"Alessandro Raganato, Yves Scherrer, and J¨org Tiedemann. Fixed encoder self-attention patterns in
transformer-based machine translation. CoRR, abs/2002.10260, 2020. URL https://arxiv.
org/abs/2002.10260."
REFERENCES,0.6462585034013606,"Machel Reid, Edison Marrese-Taylor, and Yutaka Matsuo. Subformer: Exploring Weight Sharing
for Parameter Efﬁciency in Generative Transformers. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021, Punta Cana, Dominican Republic, November 2021. Association
for Computational Linguistics."
REFERENCES,0.6496598639455783,"David R. So, Chen Liang, and Quoc V. Le. The evolved transformer. CoRR, abs/1901.11117, 2019.
URL http://arxiv.org/abs/1901.11117."
REFERENCES,0.6530612244897959,"Sho Takase and Shun Kiyono. Lessons on parameter sharing across layers in transformers. CoRR,
abs/2104.06022, 2021. URL https://arxiv.org/abs/2104.06022."
REFERENCES,0.6564625850340136,"Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. Stochastic gradient descent training
for L1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pp. 477–485, Suntec, Singapore, August 2009. Association
for Computational Linguistics. URL https://aclanthology.org/P09-1054."
REFERENCES,0.6598639455782312,"Ashish Vaswani,
Noam Shazeer,
Niki Parmar,
Jakob Uszkoreit,
Llion Jones,
Aidan N
Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf."
REFERENCES,0.6632653061224489,Published as a conference paper at ICLR 2022
REFERENCES,0.6666666666666666,"Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-
attention distillation for task-agnostic compression of pre-trained transformers. arXiv preprint
arXiv:2002.10957, 2020."
REFERENCES,0.6700680272108843,"Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short
range attention. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ByeMPlHKPH."
REFERENCES,0.673469387755102,"Yingce Xia, Tianyu He, Xu Tan, Fei Tian, Di He, and Tao Qin. Tied transformers: Neural machine
translation with shared encoder and decoder. In The Thirty-Third AAAI Conference on Artiﬁ-
cial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence
Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial In-
telligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 5466–5473.
AAAI Press, 2019. ISBN 978-1-57735-809-1. URL https://aaai.org/ojs/index.
php/AAAI/article/view/4487."
REFERENCES,0.6768707482993197,"Weiqiu You, Simeng Sun, and Mohit Iyyer.
Hard-coded gaussian attention for neural machine
translation. arXiv preprint arXiv:2005.00742, 2020."
REFERENCES,0.6802721088435374,"Changsheng Zhao, Ting Hua, Yilin Shen, Qian Lou, and Hongxia Jin. Automatic mixed-precision
quantization search of bert. Proceedings of the Thirtieth International Joint Conference on Artiﬁ-
cial Intelligence, Aug 2021. doi: 10.24963/ijcai.2021/472. URL http://dx.doi.org/10.
24963/ijcai.2021/472."
REFERENCES,0.6836734693877551,"Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efﬁcient transformer for long sequence time-series forecasting. In Proceedings
of AAAI, 2021."
REFERENCES,0.6870748299319728,Published as a conference paper at ICLR 2022
REFERENCES,0.6904761904761905,"A
APPENDIX"
REFERENCES,0.6938775510204082,"A.1
DICTFORMER BENEFITS WITH DICTIONARY SHARING"
REFERENCES,0.6972789115646258,The block comparison of Transformer and our DictFormer.
REFERENCES,0.7006802721088435,(a) Transformer block
REFERENCES,0.7040816326530612,"Multi-head Attention
FFN"
REFERENCES,0.7074829931972789,"Wi
Qj
Wi
Kj
Wi
Vj"
REFERENCES,0.7108843537414966,Attention
REFERENCES,0.7142857142857143,"Concat Add Wi
O Wi
F1 Wi
F2 Add"
REFERENCES,0.717687074829932,"(b) Our DictFormer block d
d
d d d d"
REFERENCES,0.7210884353741497,dF = 4d d
REFERENCES,0.7244897959183674,Paras #: 8Nd2
REFERENCES,0.7278911564625851,Ops: 8Nnd2
REFERENCES,0.7312925170068028,Paras #: 4Nd2
REFERENCES,0.7346938775510204,Ops: 4Nnd2+dn2
REFERENCES,0.7380952380952381,"SD Attention
GSD FFN D"
REFERENCES,0.7414965986394558,Attention
REFERENCES,0.7448979591836735,Concat Add D Add D
REFERENCES,0.7482993197278912,Lookup GSD (Eq. 9)
REFERENCES,0.7517006802721088,Lookup SD (Eq. 5)
REFERENCES,0.7551020408163265,"SD (Eq. 5)
SD (Eq. 5)"
REFERENCES,0.7585034013605442,"Paras #: 
8NdtA+mAd"
REFERENCES,0.7619047619047619,"Ops: 
Nnd(mA+tA)"
REFERENCES,0.7653061224489796,"Paras #: 
3dmF+32NdtF"
REFERENCES,0.7687074829931972,"Ops: 
Nnd(3mF+32tF+
dn2 d mA d d d F1 F2 A mF"
D,0.7721088435374149,4d mF
D,0.7755102040816326,Figure 5: The block comparison of (a) transformer and (b) our DictFormer.
D,0.7789115646258503,"Figure 5 further depicts the comparison of transformer encoder block and DictFormer encoder block.
The i-th Transformer block including attention and FFN has four weights W A
i
and W F
i . W A
i
has
four weights including query W Qj
i
, key W Kj
i
, value W Vj
i , and output W Oj
i
, thereby having 4Nd2"
D,0.782312925170068,"parameters and 4Nnd2 Mult-Adds given a sequence of size n. W F
i has two parts: W F1
i
and W F2
i
.
N-layer FFN blocks have 8Nd2 parameters and 8Nnd2 Mult-Adds operations. In contrast, Dict-
Former signiﬁcantly reduces the parameters and Mult-Adds of Transformer. Speciﬁcally, each Dict-
Former block replaces the attention and FFN with shared dictionary (SD) attention and group-wise
shared dictionary (GSD) FFN. Linear projection is performed by looking up dictionary DA using
Eq.5. SD Attention has 8NdtA + mAd parameters and Nnd(mA + tA) operations. GSD FFN
has 3dmF + 32NdtF parameters and Nnd(3mF + 32tF ) operations, where t, m are the size of
coefﬁcient and dictionary. To simplify the calculations of #params and #Mult-Adds, we can let
tA = tF , mA = mF . Dictformer reduces Transformer’s #Params from O(d2N) to O(d(m + tN)).
This is a O(dN/(m + tN))× model size reduction since dictionary size m < d, coefﬁcient size
t << d, where the embedding size is d. Dictformer also reduces #Mult-Adds from O(d2Nn) to
O(dNn(m + t))."
D,0.7857142857142857,"A.2
THE COMPARISON BETWEEN WEIGHT SHARING AND DICTIONARY SHARING"
D,0.7891156462585034,"Weight sharing is surprisingly effective to compress model size for discriminate NLP models based
on Transformer encoders, e.g., BERT. For example, prior work ALBERT (Lan et al., 2020) shows
that even sharing all parameters across layers does not introduce any accuracy reduction. However,
for generative sequence-to-sequence models based on transformer’s encoders and decoders, sharing
all parameters will signiﬁcantly decrease accuracy on multiple standard machine translation or lan-
guage modelling tasks (Reid et al., 2021; Takase & Kiyono, 2021). To match vanilla Transformer’s
accuracy, multiple works (Reid et al., 2021; Takase & Kiyono, 2021; Xia et al., 2019; Ma et al.,
2019) only share weights across partial layers instead of all layers. However, partial weights sharing
remarkably brings down the model size compression effect of weights sharing. Also, how to decide
which layers should be shared in partial weights sharing is difﬁcult due to the large and dynamic
search space that is dependent on the speciﬁc tasks."
D,0.7925170068027211,Published as a conference paper at ICLR 2022 FFN
D,0.7959183673469388,Attention
D,0.7993197278911565,"ds
d
 WQ
Wi
A ds"
DS,0.8027210884353742,"4ds Wi
F Embed"
DS,0.8061224489795918,Encoder Block Ns x FFN
DS,0.8095238095238095,Attention Embed
DS,0.8129251700680272,"Ns x
Attention"
DS,0.8163265306122449,Decoder Block
DS,0.8197278911564626,"Wider: ds > d  large model size & Mult-Adds#
Deeper: Ns > N  Mult-Adds#"
DS,0.8231292517006803,Dict-FFN
DS,0.826530612244898,Dict-Attention Embed
DS,0.8299319727891157,Encoder Block N x
DS,0.8333333333333334,Dict-FFN
DS,0.8367346938775511,Dict-Attention Embed
DS,0.8401360544217688,"N x
Dict-Attention"
DS,0.8435374149659864,Decoder Block d m DA
DS,0.8469387755102041,"t
d
Ci
A d m"
DS,0.8503401360544217,"DF
Ci
F
t
4d"
DS,0.8537414965986394,m < d & tiny t & N blocks  Tiny model & Mult-Adds#
DS,0.8571428571428571,"(a) Transformer with  weight sharing: wider or deeper (inefficient) 
(b) DictFormer: efficient transformer with dictionary sharing"
DS,0.8605442176870748,"Figure 6: (a) Transformer with weights sharing. It contains three parts including embedding, N s
encoder blocks and N s decoder blocks. Each encoder/decoder block contains attention and Feed-
Forward Network (FFN). The embedding size is ds and FFN feature size is 4 × ds. Weights in the
i-th Attention and FFN are denoted as W A
i and W F
i . To match or improve accuracy of transformer
w/o weights sharing, transformer w/ wights sharing should be wider (ds > d) or deeper (N s > N),
where d and N are the embedding size and blocks number of transformer w/o wight sharing. (b)
Our DictFormer with dictionary sharing. W A
i and W F
i are represented by smaller dictionaries DA,
DF , and coefﬁcients CA
i and CF
i , where dictionary size m < d, coefﬁcient size t << d."
DS,0.8639455782312925,"Transformer with all-parameters sharing such as Universal Transformer (Dehghani et al., 2019)
matches or improves transformer’s performance at the cost of a wider or deeper transformer archi-
tecture. A wider transformer with a larger embedding dimension enlarges the model size and brings
larger computations (Mult-Adds). A deeper transformer with more encoder/decoder blocks does
not only increase model size, but also introduces more computations. Importantly, weights shar-
ing techniques cannot reduce Mult-Adds numbers and training/inference time. Figure 6(a) shows
the comparisons of Transformers with weights sharing and our DictFormer with dictionary sharing.
Weights sharing techniques cannot solve the deployment challenges of transformers on resource-
limited devices and obtain real-time NLP applications. To reduce both model size and computations
of Transformers, we introduce DictFormer with dictionary sharing instead of previous weights shar-
ing. In particular, DictFormer shares dictionary across all layers so there is no need to decide which
layers should be shared. Also, DictFormer with few unshared look-up coefﬁcients does not require
a wider embedding size or deeper encoder/decoder to improve accuracy. Rather than compressing
model size by parameters sharing, DictFormer with dictionary sharing can also enable computations
sharing to reduce running latency. Therefore, our DictFormer provides a compact, fast, and accurate
transformer model for sequence-to-sequence NLP tasks."
DS,0.8673469387755102,"A.3
TRAINING SETTINGS"
DS,0.8707482993197279,"To fairly compare with our baselines, all of our training settings are in line with (Wu et al., 2020).
For machine translation tasks, a dropout of 0.3 is used and the dropout ratio is linearly scaled down
when we shrink the dimension of the embeddings for WMT datasets. The learning rate linearly
warms up from 10−7 to 10−3 followed by a cosine annealing with a single cycle and Adam optimizer
same as (So et al., 2019) and (Wu et al., 2020). Inverse linear square root learning rate scheduling
is used for IWSLT De-En. The summarization training settings are the same as machine translation.
To train models on language modeling task, we follow the training setting in (Baevski & Auli,
2019), but decrease the dropout rate by half in FFN layers. The training experiments of WMT,
summarization, and language modeling are conducted on 8 NVIDIA Tesla V100 GPUs. IWSLT
De-En is trained on two GPUs. Machine translations tasks are trained for 50K steps, but language
modelling tasks are trained for 286K steps. We further describe deployment settings in Appendix."
DS,0.8741496598639455,Published as a conference paper at ICLR 2022
DS,0.8775510204081632,"27
28
29
30
31
32
33
34
35"
DS,0.8809523809523809,0 2 4 6 8 10 12 14 16 BLEU
DS,0.8843537414965986,Parameters (in million)
DS,0.8877551020408163,Transformer
DS,0.891156462585034,"DictFormer
30
32
34
36
38
40
42"
DS,0.8945578231292517,"0
10
20
30
40
50 BLEU"
DS,0.8979591836734694,Parameters (in million)
DS,0.9013605442176871,"Transformer
DictFormer
20 25 30 35"
DS,0.9047619047619048,0 10 20 30 40 50 60 70
DS,0.9081632653061225,Test PPL
DS,0.9115646258503401,Parameters (in million)
DS,0.9149659863945578,"Transformer
DictFormer"
DS,0.9183673469387755,"(a) IWSLT’14 De-En
(b) WMT’14 En-Fr
(c) WIKITEXT-103"
DS,0.9217687074829932,"Figure 7: The performance of DictFormer improves with an increase in the number of model pa-
rameters, across different corpora."
DS,0.9251700680272109,"A.4
DEPLOYMENT SETTINGS."
DS,0.9285714285714286,"Deploying deep learning models including transformer on powerful cloud-based servers as a ser-
vice suffers from data privacy issues when users’ data is private or conﬁdential (Lou & Jiang, 2019;
Lou et al., 2019). In addition, the inference cost of privacy-sensitive applications that performed
over encrypted data using fully homomorphic encryption is prohibitively expensive (Feng et al.,
2021; Lou et al., 2021; 2020a). (i) Mobile settings. Scaling up transformer models leads to higher
machine translation performance, but those large architectures are not suitable for real-world appli-
cations, especially when we should deploy them on edge devices where computation and memory
size are highly constrained. Multiple prior works like (Wu et al., 2020) formalize the deployment
of lightweight transformers on mobile devices by deﬁning the mobile settings regarding the amount
of computation and parameter numbers. Particularly, given the ARM Cortex-A72 mobile CPU with
computation performance of 48G FLOPS, the computation constraint for machine translation should
be under 500M Mult-Adds and the parameter number should be around 10M with a sequence of 30
tokens that are the general length for machine translation. (ii) System-independent settings. Other
than deployed on mobile devices, DictFormer also supports system-independent deployment. In
this setting, we directly choose prior transformer architectures like DelighT (Mehta et al., 2021) and
Subformer (Reid et al., 2021) as baselines and compare our DictFormer with them under the the
same performance constraint."
DS,0.9319727891156463,"A.5
SENSITIVITY STUDY"
DS,0.935374149659864,". We study the DictFormer’s performance under different model size as Figure 7 shows. This is
important since system-independent settings require scalable DictFormer. The performance of Dict-
Former improves with an increase in the number of model parameters, across different corpora.
DictFormer achieves similar BLEU score with Transformer using 3× to 10× less model parame-
ters. In Figure 8, we also report the hyper-parameter’s effects on DictFormer’s performance with
WMT’14 En-Fr dataset with embedding size d = 512. Figure 8(a) shows the dictionary size study
when the coefﬁcient size is ﬁxed to 60. All dictionaries share the same size, and all coefﬁcients also
share same size. The Figure 8(b) shows the coefﬁcient size study when the dictionary size is ﬁxed
240. Although DictFormer’s performance improves with an increase in the size of dictionary and
coefﬁcients, the model size is also enlarged. So one should pick up a proper dictionary or coefﬁcient
size according to accuracy requirements."
DS,0.9387755102040817,"A.6
ABLATION STUDY"
DS,0.9421768707482994,"We also use table 3 to show the ablation study of DictFormer’s techniques. Transformer Vaswani
et al. (2017) adopts the base architecture with hidden size 512. Transformer-Flatten ﬂattens the
feed-forward hidden size of transformer from 2048 to 512, thus reducing the parameters from 44
millions to 25 millions without hurting the accuracy. Only-Dictionary is based on Transformer-
Flatten and uses unshared dictionary architecture. This method achieves 33.2 BLEU with 9.7 mil-
lion model parameters. The #TParams is 14.3 million and 27.4 million when adding embedding
parameters on IWSLT’14 De-En and WMT’14 En-Fr, respectively. When sharing dictionaries in at-
tention, Shared-Attention achieves similar BLEU score with 4.2 million model parameters. Mean-"
DS,0.9455782312925171,Published as a conference paper at ICLR 2022
DS,0.9489795918367347,"(a) Dictionary study
(b) Coefficient study 37 38 39 40 1 2 3 4"
DS,0.9523809523809523,"80
100 140 180 200 220 240 BLEU"
DS,0.95578231292517,Parameters (in million)
DS,0.9591836734693877,Dictionary size (m)
DS,0.9625850340136054,"Model Size
BLEU 37 38 39 40"
DS,0.9659863945578231,"1
2
3
4
5
6"
DS,0.9693877551020408,"12
24
36
48
60
72
84 BLEU"
DS,0.9727891156462585,Parameters (in million)
DS,0.9761904761904762,Coefficient size (t)
DS,0.9795918367346939,"Model Size
BLEU"
DS,0.9829931972789115,"Figure 8: Increasing the size of dictionary (a) and coefﬁcients (b) improves DictFormer’s perfor-
mance. (a) and (b) are tested on WMT’14 En-Fr dataset for machine translation task."
DS,0.9863945578231292,"while, Shared-FFN only shares FFN using group-wise shared dictionaries (G is group number).
DictFormer that shares both FFN and attention with G=2, denoted by Shared-both, requires 2.6M
model parameters and matches the accuracy of unshared DictFormer. Improved-Embed represents
that the embedding layers of Shared-both are further compressed by using existing method Mehta
et al. (2021) so that the #TParams is reduced to 5.1M from 7.1M."
DS,0.9897959183673469,"Table 3: Ablation study of DictFormer techniques on IWLS’14 De-En and WMT’14 En-Fr. Entries
with ± represents the average across three independent runs."
DS,0.9931972789115646,"IWSLT’14 De-En
WMT’14 En-Fr
#Params
#TParams
BLEU
#Params
#TParams
BLEU"
DS,0.9965986394557823,"Transformer (Vaswani et al., 2017)
44M
48.6M
34.4
44M
64M
40.7
Transformer-Flatten
25M
29.6M
34.4
25M
45M
40.7
Only-Dictionary
9.7M
14.3M
33.5 ±0.3
10.6M
27.4M
39.5 ±0.3
Shared-Attention
4.2M
8.7M
33.3 ±0.4
4.5M
21.3M
39.5 ±0.4
Shared-FFN (G=1)
7.8M
12.2M
33.2 ±0.5
8.6M
25.4M
39.2 ±0.5
Shared-FFN (G=2)
8.1M
12.5M
33.2 ±0.3
8.9M
25.7M
39.6 ±0.4
Shared-FFN (G=3)
8.4M
12.8M
33.3 ±0.3
9.1M
25.9M
39.5 ±0.3
Shared-both (G=2)
2.6M
7.1M
33.2 ±0.4
2.5M
19.3M
39.6 ±0.4
Improved-Embed
2.6M
5.1M
33.1 ±0.4
2.5M
9.8M
39.5 ±0.3"
