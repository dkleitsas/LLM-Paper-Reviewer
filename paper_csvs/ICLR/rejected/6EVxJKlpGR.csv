Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024390243902439024,"Multi-Agent Reinforcement Learning (MARL) has demonstrated signiﬁcant suc-
cess by virtue of collaboration across agents. Recent work, on the other hand,
introduces surprise which quantiﬁes the degree of change in an agent’s environ-
ment. Surprise-based learning has received signiﬁcant attention in the case of
single-agent entropic settings but remains an open problem for fast-paced dynamics
in multi-agent scenarios. A potential alternative to address surprise may be realized
through the lens of free-energy minimization. We explore surprise minimization
in multi-agent learning by utilizing the free energy across all agents in a multi-
agent system. A temporal Energy-Based Model (EBM) represents an estimate of
surprise which is minimized over the joint agent distribution. Our formulation of
the EBM is theoretically akin to the minimum conjugate entropy objective and
highlights suitable convergence towards minimum surprising states. We further
validate our theoretical claims in an empirical study of multi-agent tasks demanding
collaboration in the presence of fast-paced dynamics."
ABSTRACT,0.004878048780487805,sites.google.com/view/surprise-web/
INTRODUCTION,0.007317073170731708,"1
INTRODUCTION"
INTRODUCTION,0.00975609756097561,"The rise of RL has led to an increasing interest in the study of multi-agent systems (Lowe et al., 2017;
Vinyals et al., 2019), commonly known as Multi-Agent Reinforcement Learning (MARL). In the
case of partially observable settings, MARL enables the learning of policies with centralised training
and decentralised control (Kraemer & Banerjee, 2016). This has proven to be useful for exploiting
value-based methods which motivate collaboration across large number of agents. But how do agents
behave in the presence of sudden environmental changes?"
INTRODUCTION,0.012195121951219513,"Consider the problem of autonomous driving wherein a driver (agent) autonomously operates a
vehicle in real-time. The driver learns to optimize the reward function by maintaining constant
speed and covering more distance in different trafﬁc conditions. Whenever the vehicle approaches
an obstacle, the driver acts to avoid it by utilizing the brake and directional steering commands.
However, due to the fast-paced dynamics of the environment, say fast-moving trafﬁc, the agent may
abruptly encounter an obstacle (a person running across the street) which may result in a collision.
Irrespective of the optimal action (pushing of brakes) executed by the agent, the vehicle may fail to
evade the collision as a result of the abrupt temporal change."
INTRODUCTION,0.014634146341463415,"The above arises as a consequence of surprise, which is deﬁned as a statistical measure of uncertainty.
Surprise minimization (Berseth et al., 2019) is a recent phenomenon observed in the case of single-
agent RL methods which deals with environments consisting of rapidly changing states. In the case
of model-based RL (Kaiser et al., 2019), surprise minimization is used as an effective planning tool in
the agent’s model (Berseth et al., 2019) whereas in the case of model-free RL, surprise minimization
is witnessed as an intrinsic motivation (Achiam & Sastry, 2017; Macedo et al., 2004) or generalization
problem (Chen, 2020). On the other hand, MARL does not account for surprise across agents as a
result of which agents remain unaware of drastic changes in the environment (Macedo & Cardoso,
2005). Thus, surprise minimization in the case of multi-agent settings requires attention from a
critical standpoint."
INTRODUCTION,0.01707317073170732,"A potential pathway to treat surprising states may be realized in light of free-energy minimization.
The free-energy principle depicts convergence to local niches and provides a general recipe for
cognitive stability among agents. Through this lens, we unify surprise with free-energy in the
multi-agent setting. We construct a temporal EBM which represents an estimate of surprise agents"
INTRODUCTION,0.01951219512195122,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02195121951219512,"may face in the environment. All agents jointly minimize this estimate utilizing temporal difference
learning upon their value functions and the EBM. Our formulation of free-energy minimization
is theoretically akin to minimizing the entropy in conjugate gradient space. This insight provides
a suitable convergence result towards minimum surprising states (or niches) of the agent state
distributions. In an empirical study of multi-agent tasks which present signiﬁcant collaboration
bottlenecks and fast-paced dynamics, we validate our theoretical claims and motivate the practical
usage of EBMs in MARL."
RELATED WORK,0.024390243902439025,"2
RELATED WORK"
RELATED WORK,0.026829268292682926,"Surprise Minimization: Despite the recent success of value-based methods (Mnih et al., 2016;
Hessel et al., 2017) RL agents suffer from spurious state spaces and encounter sudden changes in
trajectories. Quantitatively, surprise has been studied as a measure of deviation (Berseth et al., 2019;
Chen, 2020) among states encountered by the agent during its interaction with the environment. While
exploring (Burda et al., 2019; Thrun, 1992) the environment, agents tend to have higher deviation
among states which is gradually reduced by gaining a signiﬁcant understanding of state-action
transitions. In the case of model-based RL, agents can leverage spurious experiences (Berseth et al.,
2019) and plan effectively for future steps. On the other hand, in the case of model-free RL, surprise
results in sample-inefﬁcient learning (Achiam & Sastry, 2017). This is primarily addressed by
making use of rigorous exploration strategies (Stadie et al., 2015; Lee et al., 2019). High-dimensional
exploration further requires extrinsic feature engineering (Kulkarni et al., 2016) and meta models
(Gupta et al., 2018). A suitable way to tackle high-dimensional dynamics is by utilizing surprise
as a penalty on the reward (Chen, 2020). This leads to improved generalization for single-agent
interactions (Ren et al., 2005). Our proposed approach is orthogonal to the aforesaid methods."
RELATED WORK,0.02926829268292683,"Energy-based Models: EBMs have been successfully implemented in single-agent RL methods
(O’Donoghue et al., 2016; Haarnoja et al., 2017). These typically make use of Boltzmann distributions
to approximate policies (Levine & Abbeel, 2014). Such a formulation results in the minimization of
free energy within the agent. While policy approximation depicts promise in the case of unknown
dynamics, inference methods (Toussaint, 2009) play a key role in optimizing goal-oriented behavior."
RELATED WORK,0.03170731707317073,"A second type of usage of EBMs follows the maximization of entropy (Ziebart et al., 2008). The max-
imum entropy framework (Haarnoja et al., 2018b) highlighted in Soft Q-Learning (SQL) (Haarnoja
et al., 2017) allows the agent to obey a policy which maximizes its reward and entropy concurrently.
Maximization of agent’s entropy results in diverse and adaptive behaviors (Ziebart, 2010) which may
be difﬁcult to accomplish using standard exploration techniques (Burda et al., 2019; Thrun, 1992).
The maximum entropy framework is akin to approximate inference in the case of policy gradient
methods (Schulman et al., 2017). Such a connection between likelihood ratio gradient techniques and
energy-based formulations leads to diverse and robust policies (Haarnoja, 2018) and their hierarchical
extensions (Haarnoja et al., 2018a) which preserve the lower levels of hierarchies. In the case of
MARL, EBMs have witnessed limited applicability as a result of the increasing number of agents and
complexity within each agent (Bus¸oniu et al., 2010). While the probabilistic framework is readily
transferable to opponent-aware multi-agent systems (Wen et al., 2019), cooperative settings consisting
of coordination between agents require a ﬁrm formulation of energy which is scalable in the number
of agents (Grau-Moya et al., 2018) and accounts for environments consisting of spurious states (Wei
et al., 2018). Our theoretical formulation is motivated by these methods in literature."
PRELIMINARIES,0.03414634146341464,"3
PRELIMINARIES"
MULTI-AGENT LEARNING,0.036585365853658534,"3.1
MULTI-AGENT LEARNING"
MULTI-AGENT LEARNING,0.03902439024390244,"We review the cooperative MARL setup.
The problem is modeled as a Dec-Partially Ob-
servable Markov Decision Process (POMDP) (Oliehoek & Amato, 2016) deﬁned by the tuple
(S, A, r, N, P, Z, O, γ) where the state space S and action space A are discrete, r : S × A →
[rmin, rmax] presents the reward observed by agents a ∈N where N is the set of all agents,
P : S × S × A →[0, ∞) presents the unknown transition model consisting of the transition proba-
bility to the next state s′ ∈S given the current state s ∈S and joint action u ∈A (a combination
of each agent’s action ua ∈Aa) at time step t and γ is the discount factor. We consider a partially
observable setting in which each agent n draws individual observations z ∈Z according to the
observation function O(s, u) : S × A →Z. We consider a joint policy πθ(u|s) as a function of"
MULTI-AGENT LEARNING,0.041463414634146344,Under review as a conference paper at ICLR 2022
MULTI-AGENT LEARNING,0.04390243902439024,"model parameters θ. Standard RL deﬁnes the agent’s objective to maximize the expected discounted
reward Eπθ[PT
t=0 γtr(st, ut)] as a function of the parameters θ. The joint action-value function
for agents is represented as Q(u, s; θ) = Eπθ[PT
t=1 γtr(s, u)|s = st, u = ut] which is the ex-
pected sum of payoffs obtained in state s upon performing action u by following the policy πθ. We
denote the optimal policy πθ∗( shorthand π∗) such that Q(u, s; θ∗) ≥Q(u, s; θ)∀s ∈S, u ∈A.
In the case of multiple agents, the joint optimal policy can be expressed as the Nash Equi-
librium (Nash, 1950) of the Stochastic Markov Game as π∗= (π1,∗, π2,∗, ...πN,∗) such that
Q(ua, s; θ∗) ≥Q(ua, s; θ)∀s ∈S, u ∈A, a ∈N. Q-Learning is an off-policy, model-free al-
gorithm suitable for continuous and episodic tasks. The algorithm uses semi-gradient descent to
minimize the Temporal Difference (TD) error in Equation 1."
MULTI-AGENT LEARNING,0.046341463414634146,"L(θ) =
E
s,u,s′∼R"
MULTI-AGENT LEARNING,0.04878048780487805,"""
r + γmax
u′∈AQ(u′, s′; θ−) −Q(u, s; θ)
2# (1)"
MULTI-AGENT LEARNING,0.05121951219512195,"where y = r + γmax
u′∈AQ(u′, s′; θ−) is the TD target consisting of θ−as the target parameters and R"
MULTI-AGENT LEARNING,0.05365853658536585,denotes the replay buffer.
ENERGY-BASED MODELS,0.05609756097560976,"3.2
ENERGY-BASED MODELS"
ENERGY-BASED MODELS,0.05853658536585366,"EBMs (LeCun et al., 2006; 2007) have been successfully applied in the ﬁeld of machine learning
(Teh et al., 2003) and probabilistic inference (MacKay, 2002). A typical EBM E formulates the
equilibrium probabilities (Sallans & Hinton, 2004) P(v, h) =
exp (−E(v,h))
P"
ENERGY-BASED MODELS,0.06097560975609756,"ˆv,ˆh[exp (−E(ˆv,ˆh))] via a Boltzmann"
ENERGY-BASED MODELS,0.06341463414634146,"distribution (Levine & Abbeel, 2014) where v and h are the values of the visible and hidden variables
and ˆv and ˆh are all the possible conﬁgurations of the visible and hidden variables respectively. The
probability distribution over all the visible variables can be obtained by summing over all possible
conﬁgurations of the hidden variables. This is mathematically expressed in Equation 2."
ENERGY-BASED MODELS,0.06585365853658537,"P(v) =
P"
ENERGY-BASED MODELS,0.06829268292682927,"h exp (−E(v, h))
P"
ENERGY-BASED MODELS,0.07073170731707316,"ˆv,ˆh exp (−E(ˆv, ˆh))
(2)"
ENERGY-BASED MODELS,0.07317073170731707,"Here, E(v, h) is called the equilibrium free energy which is the minimum of the variational free
energy and P"
ENERGY-BASED MODELS,0.07560975609756097,"ˆv,ˆh exp (−E(ˆv, ˆh)) is the partition function."
ENERGY-BASED SURPRISE MINIMIZATION,0.07804878048780488,"4
ENERGY-BASED SURPRISE MINIMIZATION"
ENERGY-BASED SURPRISE MINIMIZATION,0.08048780487804878,"We begin by constructing surprise minimization as an energy-based problem in the temporal setting.
The motivation behind an energy-based formulation stems from rapidly changing states as an unde-
sired niche among agents in the case of partially-observed settings. To steer agents away from this
niche, we further construct a method which incorporates the theoretical aspect of the study."
THE SURPRISE MINIMIZATION OBJECTIVE,0.08292682926829269,"4.1
THE SURPRISE MINIMIZATION OBJECTIVE"
THE SURPRISE MINIMIZATION OBJECTIVE,0.08536585365853659,"To make analysis tractable towards valid function spaces and surprising states, we take into account
two assumptions which form the central basis of surprise minimization among multiple agents."
THE SURPRISE MINIMIZATION OBJECTIVE,0.08780487804878048,"Assumption 1. (Completeness of value function space) The space Π : S × A of all Q value
functions Q(s, u) ∈Π, ∀s ∈S, ∀u ∈A is a nonempty complete metric space."
THE SURPRISE MINIMIZATION OBJECTIVE,0.09024390243902439,"Assumption 1 restricts the formulation of individual agent value functions Qa to the nonempty
complete metric space. A nonempty space conﬁrms the presence of candidate functions Qa upper
bounded by the optimal function Q∗, i.e.- Qa ≤Q∗, ∀a ∈N (Bertsekas & Tsitsiklis, 1995). The
completeness counterpart, on the other hand, provisions a ﬁxed interior int Π for optimization (Boyd
& Vandenberghe, 2004)."
THE SURPRISE MINIMIZATION OBJECTIVE,0.09268292682926829,Under review as a conference paper at ICLR 2022
THE SURPRISE MINIMIZATION OBJECTIVE,0.0951219512195122,"Assumption 2. (Constant surprise at Equilibrium) In the limit of convergence
lim
πa→π∗to an"
THE SURPRISE MINIMIZATION OBJECTIVE,0.0975609756097561,"optimal policy π∗, all agents a ∈N incur a ﬁnite surprise ζ > 0 between consecutive states
s and s′ until termination state sT ."
THE SURPRISE MINIMIZATION OBJECTIVE,0.1,"Assumption 2 is directly based on the constant and continuous temporal aspect of surprise minimiza-
tion (Schwartenbeck et al., 2013; Friston, 2010). Corresponding to the lifetime of each agent a ∈N,
a desired ecological niche bakes in the optimal distribution of actions which correspond to minimum
yet ﬁnite instantaneous surprise."
THE SURPRISE MINIMIZATION OBJECTIVE,0.1024390243902439,"We formulate the energy-based objective consisting of surprise as a function of states s, joint actions u
and standard deviation σ of observations for each agent a. In the case of high-dimensional state spaces
(such as multiple opponents), σ informs agents of the abrupt statistical change that would take place
upon executing action u. We formulate surprise as T V a
surp(s, u, σ) which serves as an uncertainty
quantiﬁer Unc(s,a) of the state-action distribution. Here V a
surp(s, u, σ) denotes the surprise value
function which serves as a mapping from agent and environment dynamics to surprise. Deﬁne an
operator presented in Equation 3 which sums surprising conﬁgurations across all agents."
THE SURPRISE MINIMIZATION OBJECTIVE,0.1048780487804878,"T V a
surp(s, u, σ) = log N
X"
THE SURPRISE MINIMIZATION OBJECTIVE,0.1073170731707317,"a=1
exp
 
V a
surp(s, u, σ)

(3)"
THE SURPRISE MINIMIZATION OBJECTIVE,0.10975609756097561,"Remark 1. T V a
surp(s, u, σ) intuitively provides a global estimate of surprise. If all agents are equally
likely to face a surprising state, then T V a
surp(s, u, σ) captures their individual contributions.
The formulation makes use of the soft-maximum operator (Asadi & Littman, 2017). The operator
T V a
surp(s, u, σ) is similar to prior energy formulations (Haarnoja et al., 2017) where the energy
across different actions is evaluated. In our case, inference is carried out across all agents with actions
as prior variables. However, in the special case of using an EBM as a Q-function, our approach
suitable generalizes to the above methods (details in Appendix B)."
THE SURPRISE MINIMIZATION OBJECTIVE,0.11219512195121951,"Our choice of T V a
surp(s, u, σ) is based on its unique mathematical properties which result in better
convergence. Of these properties, the most useful result is that T forms a contraction on the surprise
value function V a
surp(s, u, σ) indicating a guaranteed minimization of surprise within agents. This
is formally stated in Theorem 1 while utilizing the completeness criterion of Assumption 1 which
provides a tractable value function space. All proofs are deferred to Appendix A."
THE SURPRISE MINIMIZATION OBJECTIVE,0.11463414634146342,"Theorem 1. Given a surprise value function V a
surp(s, u, σ) ∀a ∈N, the energy operator
T V a
surp(s, u, σ) = log PN
a=1 exp (V a
surp(s, u, σ)) forms a contraction on V a
surp(s, u, σ)."
THE SURPRISE MINIMIZATION OBJECTIVE,0.11707317073170732,"Theorem 1 provides a suitable guarantee of T V a
surp(s, u, σ) converging to a ﬁxed point niche. The
contraction result is directly based on Banach’s ﬁxed point property and suggests the generalization
of convergence in any nonempty complete metric space (X, d) (Bertsekas & Tsitsiklis, 1995)."
THE SURPRISE MINIMIZATION OBJECTIVE,0.11951219512195121,"We now consider a weighted combination of Q(s, u) with T V a
surp(s, u, σ) wherein we denote β as a
temperature parameter,ˆQ(u, s; θ) = Q(u, s; θ) + β log N
X"
THE SURPRISE MINIMIZATION OBJECTIVE,0.12195121951219512,"a=1
exp (V a
surp(s, u, σ)))
(4)"
THE SURPRISE MINIMIZATION OBJECTIVE,0.12439024390243902,"Remark 2. Equation 4 is an instance of value function regularization wherein the Q values are
subject to a joint penalty while observing surprising states."
THE SURPRISE MINIMIZATION OBJECTIVE,0.12682926829268293,"Interestingly, upon considering the Legendre transform f ∗(x) (Boyd & Vandenberghe, 2004; Gao &
Pavel, 2017) (convex conjugate function corresponding to the conjugate space X of a differentiable
function f(z)) of T V a
surp(s, u, σ), we obtain the following,
f ∗(x) =
sup
z∈dom f"
THE SURPRISE MINIMIZATION OBJECTIVE,0.12926829268292683," 
xTz −f(z)

, f(z) = T V a
surp(s, u, σ)
(5)"
THE SURPRISE MINIMIZATION OBJECTIVE,0.13170731707317074,"f ∗(x) =
X"
THE SURPRISE MINIMIZATION OBJECTIVE,0.13414634146341464,"x
x log(x) , x = ∇zf(z) ∈X
(6)"
THE SURPRISE MINIMIZATION OBJECTIVE,0.13658536585365855,"Remark 3. The Legendre Transform of T V a
surp(s, u, σ) given by f ∗(x) = P"
THE SURPRISE MINIMIZATION OBJECTIVE,0.13902439024390245,"x x log(x) when utilized
as value function regularization ˆQ = Q −f ∗(x) corresponds to the minimum entropy formulation in"
THE SURPRISE MINIMIZATION OBJECTIVE,0.14146341463414633,"conjugate space Eπθ
hPT
t=0 γt(r(st, ut) −λH(x))
i
for x = ∇zf(z) ∈X."
THE SURPRISE MINIMIZATION OBJECTIVE,0.14390243902439023,Under review as a conference paper at ICLR 2022
THE SURPRISE MINIMIZATION OBJECTIVE,0.14634146341463414,"Figure 1: Agent pop-
ulations (robots) tra-
verse the energy land-
scape (in grey) during
update steps ( ) to
seek energy minima
(darker shade at cen-
ter). This results in
surprise minimization
from high ( ) to low
energy ( ) niches."
THE SURPRISE MINIMIZATION OBJECTIVE,0.14878048780487804,"Based on the above insight, minimizing entropy to express ∇zf(z) in con-
jugate space is akin to minimizing uncertainty among all agents in the value
function space Π. Intuitively, H(x) denotes the uncertainty for each agent
a ∈N in the multi-agent population which is directly related to its ability
of efﬁcaciously interpreting the environment. Minimizing H(x) leads to
an increase in the expressiveness of value function. This in turn, induces
an expressive state visitation distribution which steers the agent away from
sudden changes in its environment. Note that the setting does not minimize
entropy in value function space which would stand contrary to the maximum
entropy formulation Haarnoja et al. (2018b) (see Appendix B)."
THE SURPRISE MINIMIZATION OBJECTIVE,0.15121951219512195,"Figure 1 presents an intuitive illustration of the objective. The joint agent
population aims to minimize surprise corresponding to minimum energy
conﬁgurations. Agents collaborate in partially-observed worlds to attain
a joint niche. This local niche implicitly corresponds to a ﬁxed point of
T V a
surp(s, u, σ) on the energy landscape. Note that agents act locally with
actions conditioned on their own action observation histories. It is by virtue
of preconditioned values estimations that the surprise minimization scheme
informs agents of joint surprise. Upon population’s convergence to a suitable
conﬁguration, agents continue to experience minimum (yet ﬁnite) surprise
arising from evironment dynamics."
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.15365853658536585,"4.2
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.15609756097560976,"We utilize the above insights as surprise-based regularization in the TD learning setting. Upon
replacing Q(u, s; θ) with ˆQ(u, s, ; θ) in the RL construction of Equation 1 one obtains the following,"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.15853658536585366,"L(θ) =
E
s,u,s′∼R  1 2 "
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.16097560975609757,"ˆy −(Q(u, s; θ) + β log N
X"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.16341463414634147,"a=1
exp (V a
surp(s, u, σ))) !2 "
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.16585365853658537,"where ˆy = r + γmax
u′ Q(u′, s′; θ−) + β log PN
a=1 exp (V a
surp(s′, u′, σ′)). Collecting the log terms"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.16829268292682928,"yields the following,"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.17073170731707318,"=
E
s,u,s′∼R  1 2 "
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.17317073170731706,"r + γmax
u′ Q(u′, s′; θ−) + β log"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.17560975609756097,"PN
a=1 exp (V a
surp(s′, u′, σ′))
PN
a=1 exp (V a
surp(s, u, σ)) !"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.17804878048780487,"−Q(u, s; θ) !2 "
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.18048780487804877,"L(θ) =
E
s,u,s′∼R 1 2"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.18292682926829268,"
r + γmax
u′ Q(u′, s′; θ−) + βE −Q(u, s; θ)
2
(7)"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.18536585365853658,"Here, E is deﬁned as the surprise ratio. The surprise value function V a
surp(s′, u′, σ′) is expressed as
the negative free energy and PN
a=1 exp (V a
surp(s, u, σ)) as the partition function of a conventional
EBM described in Equation 2. Alternatively, V a
surp(s, u, σ) can be formulated as the negative free
energy with PN
a=1 exp (V a
surp(s′, u′, σ′)) as the partition function. The TD objective incorporates
the minimization of surprise across all agents as minimizing the energy in spurious states."
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.1878048780487805,"Remark 4. The above formulation of βE can be realized as intrinsic motivation steering the agent
towards subgoals with reduced surprise."
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.1902439024390244,"The energy formulation E provides a tractable distribution over all surprising conﬁgurations in the
state space S. This guarantees convergence to minimum surprise at optimal policy π∗and is formally
expressed in Theorem 2 (see Appendix C for a detailed convergence analysis)."
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.1926829268292683,"Theorem 2. Upon agent’s convergence to an optimal policy π∗, total energy of π∗, expressed
by E∗will reach a thermal equilibrium consisting of minimum surprise among consecutive
states s and s′."
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.1951219512195122,Under review as a conference paper at ICLR 2022
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.1975609756097561,"Theorem 2 demonstrates an intuitive convergence result of agent populations collaborating to reside
in a mutual ecological niche (Friston, 2010). The multi-agent population with minimum surprise
exhibits the optimal policy π∗which results in minimum energy corresponding to each surprising
state in the state distribution S. Orthogonally, agents may continue to experience ﬁnite and constant
surprise in the long-horizon while acting optimally to visit non-surprising and rewarding states. This
presents surprise minimization as a secondary surrogate objective in MARL."
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.2,"4.3
ENERGY-BASED MIXER (EMIX)"
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.20243902439024392,Figure 2: The EMIX architecture for learning surprise across global states.
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.2048780487804878,"Based on our theoretical analysis, we incorporate learning of surprise as global intrinsic motivation
across all agents in the multi-agent system. A global estimate of surprise, following the energy
operator T V a
surp(s, u, σ), is beﬁtting from a computational perspective as well. An individual
estimate of surprise for each agent may be intractable to obtain due to the non-stationarity of the
environment. Instead, we seek to minimize surprise jointly across all agents using an expressive
Energy-based MIXer (EMIX) architecture which is compatible with any multi-agent RL algorithm.
Figure 2 illustrates our learning scheme."
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.2073170731707317,"Learning of surprise in the high-dimensional value function space is cumbersome with the number
of actions scaling linearly in the number of agents. This imposes an inherent restriction to learn
global surprise efﬁcaciously across all agents at a given timestep. Towards this goal, EMIX encodes
individual value functions Q1, Q2, ... Qn corresponding to each agent using local value encoders.
These encoders capture the local change in value functions arising over subsequent TD learning
iterations (Wang et al., 2021). A global state encoder maps environment states s1, s2, ... sT to
a low dimensional representation. Further, a state deviation encoder encodes deviations across all
states s1, s2, ... sT within the given batch. Akin to a model-based method (Janner et al., 2019),
the state deviation encoder accounts for uncertainty in an agent’s state visitation distribution. Note
that the encoder does not construct an explicit model of states, but only represents their variation
in the agent’s environment. This insight is essential to account for abrupt dynamics encountered
by agents. Representations obtained from state and value function encoders are concatenated and
compressed using a ﬁnal surprise encoder which estimates a distribution of surprise values. The
distribution implicitly represents the density of states wherein an agent may encounter most surprise.
A value estimate V a
surp(s, u, σ) sampled from the surprise distribution depicts the variational free
energy conﬁguration upon application of T which serves as global intrinsic motivation. Practical
training of EMIX proceeds with backpropagation (Rumelhart et al., 1986) using gradient descent and
the reparameterization trick (Kingma & Welling, 2014) for sampling of V a
surp(s, u, σ)."
SURPRISE MINIMIZATION WITH FUNCTION APPROXIMATION,0.2097560975609756,Under review as a conference paper at ICLR 2022
PRACTICAL IMPLEMENTATION,0.2121951219512195,"4.4
PRACTICAL IMPLEMENTATION"
PRACTICAL IMPLEMENTATION,0.2146341463414634,Algorithm 1 Energy-based MIXer (EMIX)
PRACTICAL IMPLEMENTATION,0.21707317073170732,"1: Initialize φ, θ, θ−
1 ..., θ−
m, agent and hypernetwork parameters.
2: Initialize learning rate α, temperature β and replay buffer R.
3: for environment step do
4:
u ←−(u1, u2..., uN)
5:
R ←−R ∪{(s, u, r, s′)}
6:
if |R| > batch-size then
7:
for random batch do
8:
Qθ
tot ←−Mixer-Network(Q1, Q2..., QN, s)
9:
Qθ−
i
←−Target-Mixeri(Q1, Q2..., QN, s′), ∀i = 1, 2.., m
10:
Calculate σ and σ′ using s and s′"
PRACTICAL IMPLEMENTATION,0.21951219512195122,"11:
V a
surp(s, u, σ) ←−Surprise-Mixer(s, u, σ)
12:
V a
surp(s′, u′, σ′) ←−Target-Surprise-Mixer(s′, u′, σ′)"
PRACTICAL IMPLEMENTATION,0.22195121951219512,"13:
E ←−log
 PN
a=1 exp (V a
surp(s′,u′,σ′))
PN
a=1 exp (V a
surp(s,u,σ)) "
PRACTICAL IMPLEMENTATION,0.22439024390243903,"14:
Calculate L(θ) using E in Equation 7
15:
θ ←−θ −α∇θL(θ)
16:
end for
17:
end if
18:
if update-interval steps have passed then
19:
θ−
i ←−θ, ∀i = 1, 2.., m
20:
end if
21: end for"
PRACTICAL IMPLEMENTATION,0.22682926829268293,"Algorithm 1 presents the EMIX framework (in green) combined with QMIX Rashid et al. (2018),
an off-the-shelf MARL algorithm. The total Q-value Qθ
tot is computed by the mixer network with
its inputs as the Q-values of all the agents conditioned on s via the hypernetworks. Similarly, the
target mixers approximate Qθ−
i
conditioned on s′. In order to evaluate surprise within agents, we
compute the standard deviations σ and σ′ across all observations z and z′ for each agent using s and s′
respectively. The surprise value function, called the Surprise-Mixer, estimates surprise V a
surp(s, u, σ)
conditioned on s, u and σ. The same computation is repeated using the Target-Surprise-Mixer for
estimating surprise V a
surp(s′, u′, σ′) within next-states in the batch. Application of the energy operator
along the non-singleton agent dimension for V a
surp(s, u, σ) and V a
surp(s′, u′, σ′) yields the energy
ratio E which is used in Equation 7 to evaluate L(θ). We then use batch gradient descent to update
parameters of the mixer θ. Target parameters θ−
i are updated every update −interval steps."
EXPERIMENTS,0.22926829268292684,"5
EXPERIMENTS"
EXPERIMENTS,0.23170731707317074,"Our experiments aim to evaluate the theoretical claims presented by EMIX along with its performance
to prior MARL methods. Speciﬁcally, we aim to answer the following questions- (1) How does the
provision of an EBM for surprise minimization compare to current MARL methods?, and (2) Does
the algorithm validate the theoretical claims corresponding to its components?"
ENERGY-BASED SURPRISE MINIMIZATION,0.23414634146341465,"5.1
ENERGY-BASED SURPRISE MINIMIZATION"
ENERGY-BASED SURPRISE MINIMIZATION,0.23658536585365852,"We assess the validity of EMIX, when combined with QMIX, on multi-agent StarCraft II microman-
agement scenarios (Samvelyan et al., 2019) as these consist of a larger number of agents with different
action spaces. This in turn motivates a greater deal of coordination. Additionally, micromanagement
scenarios in StarCraft II consist of multiple opponents which introduce a greater degree of surprise
within consecutive states."
ENERGY-BASED SURPRISE MINIMIZATION,0.23902439024390243,"We compare our method to prior methods namely; (1) QMIX (Rashid et al., 2018), constituting
of nonlinear value function factorization with monotonicity constraints; (2) Value Decomposition
Networks (VDN) (Sunehag et al., 2018), consisting of linear additive factorization of Q function;
(3) Counterfactual Multi-Agent Policy Gradients (COMA) (Foerster et al., 2017), which consist of"
ENERGY-BASED SURPRISE MINIMIZATION,0.24146341463414633,Under review as a conference paper at ICLR 2022
ENERGY-BASED SURPRISE MINIMIZATION,0.24390243902439024,"Scenarios
EMIX
SMiRL-QMIX
QMIX
VDN
COMA
IQL
2s vs 1sc
90.33 ± 0.72
88.41 ± 1.31
89.19 ± 3.23
91.42 ± 1.23
96.90 ± 0.54
86.07 ± 0.98
2s3z
95.40±0.45
94.93±0.32
95.30±1.28
92.03±2.08
43.33±2.70
55.74±6.84
3m
94.90±0.39
93.94±0.22
93.43±0.20
94.58±0.58
84.75±7.93
94.79±0.50
3s vs 3z
99.58±0.07
97.63±1.08
99.43±0.20
97.90±0.58
0.21±0.54
92.32±2.83
3s vs 4z
97.22±0.73
0.24±0.11
96.01±3.93
94.29±2.13
0.00±0.00
59.75±12.22
3s vs 5z
52.91±11.80
0.00±0.00
43.44±7.09
68.51±5.60
0.00±0.00
18.14±2.34
3s5z
88.88±1.07
88.53±1.03
88.49±2.32
63.58±3.99
0.25±0.11
7.05±3.52
8m
94.47±1.38
89.96±1.42
94.30±2.90
90.26±1.12
92.82±0.53
83.53±1.62
8m vs 9m
71.03±2.69
69.90±1.94
68.28±2.30
58.81±4.68
4.17±0.58
28.48±22.38
10m vs 11m
75.35±2.30
77.85±2.02
70.36±2.87
71.81±6.50
4.55±0.73
32.27±25.68
so many baneling
95.87±0.16
93.61±0.94
93.35±0.78
92.26±1.06
91.65±2.26
74.97±6.52
5m vs 6m
37.07±2.42
33.27±2.79
34.42±2.63
35.63±3.32
0.52±0.13
14.78±2.72"
ENERGY-BASED SURPRISE MINIMIZATION,0.24634146341463414,"Table 1: Comparison of success rate percentages between EMIX and prior MARL methods on
StarCraft II micromanagement scenarios. EMIX is comparable to or improves over QMIX agent. In
comparison to SMiRL-QMIX, EMIX demonstrates improved minimization of surprise. Results are
averaged over 5 random seeds."
ENERGY-BASED SURPRISE MINIMIZATION,0.24878048780487805,"counterfactual actor-critic updates in a centralized critic; and (4) Independent Q Learning (IQL)
(Tan, 1993), wherein each agent acts independent of other agents. (5) In order to compare our
surprise minimization scheme against pre-existing mechanisms, we compare EMIX additionally to a
model-free implementation of SMiRL (Berseth et al., 2019) in QMIX. We use the generalized version
of SMiRL as it demonstrates reduced variance across batches (Chen, 2020). This implementation
is denoted as SMiRL-QMIX for comparisons. Details related to the implementation of EMIX are
presented in Appendix D."
ENERGY-BASED SURPRISE MINIMIZATION,0.25121951219512195,"Table 1 presents the comparison of success rate percentages between EMIX and prior MARL algo-
rithms on 12 StarCraft II micromanagement scenarios. Corresponding to each scenario, algorithms
demonstrating higher success rate values in comparison to other methods have their entries high-
lighted in bold (see Appendix E.2 for a statistical analysis). Out of the 12 scenarios considered,
EMIX presents higher success rates on 9 of these scenarios depicting the suitability of the proposed
approach. In cases of so many baneling and 5m vs 6m having large number of opponents and a
greater level of surprise, EMIX aptly improves over prior methods."
ENERGY-BASED SURPRISE MINIMIZATION,0.25365853658536586,"When compared to QMIX, EMIX depicts improved success rates on all of the 12 scenarios. On
comparing EMIX with SMiRL-QMIX, we note that EMIX demonstrates a higher average success
rate. This highlights the suitability of the energy-based scheme in the case of a larger number of
agents and complex environment dynamics for surprise minimization."
ABLATION STUDY,0.25609756097560976,"5.2
ABLATION STUDY"
ABLATION STUDY,0.25853658536585367,"We now present the ablation study for the various components of EMIX. Our experiments aim to
determine the effectiveness of the energy-based surprise minimization method. Additionally, we
also aim to evaluate the utility of dual approximators for surprise estimation in accordance with the
precept from RL literature (Hasselt et al., 2016; Fujimoto et al., 2018; Haarnoja et al., 2018b)."
EMIX OBJECTIVE,0.26097560975609757,"5.2.1
EMIX OBJECTIVE"
EMIX OBJECTIVE,0.2634146341463415,"Figure 3: Ablations for each of EMIX’s component. When
compared to QMIX, EMIX and TwinQMIX depict im-
provements in performance and sample efﬁciency."
EMIX OBJECTIVE,0.2658536585365854,"To weigh the effectiveness of energy-
based scheme, we ablate the energy op-
erator T and only utilize V a
surp. Since
this implementation employs dual ap-
proximators V a
surp,(i) i ∈{1, 2} for sta-
bility, we call this implementation as
TwinQMIX. Thus, we compare between
QMIX, TwinQMIX and EMIX to assess
the contributions of each of the proposed
methods."
EMIX OBJECTIVE,0.2682926829268293,"Figure 3 presents the comparison of aver-
age success rates for QMIX, TwinQMIX and EMIX on 3 different scenarios. Agents were evaluated
for a total of 2 million timesteps with the lines in the plot indicating average success rates and the"
EMIX OBJECTIVE,0.2707317073170732,Under review as a conference paper at ICLR 2022
EMIX OBJECTIVE,0.2731707317073171,"shaded area as the deviation across 5 random seeds. In comparison to QMIX, TwinQMIX adds
stability to the original objective by incorporating surprising estimates in the initial QMIX objective.
On comparing TwinQMIX to EMIX we note that dual approximators play little role in improving
convergence. Thus, the energy-based surprise minimization scheme is the main facet for signiﬁcant
performance improvement in the modiﬁed EMIX objective. This is demonstrated in the 5m vs 6m
scenario wherein the EMIX implementation improves the performance of TwinQMIX in comparison
to QMIX by utilizing a surprise-robust policy. In the case of so many baneling scenario which
consists of a large number of opponents (27 banelings), EMIX tackles surprise effectively by pre-
venting a signiﬁcant drop in performance which is observed in cases of QMIX and TwinQMIX. We
conjecture that this is a direct consequence of underestimations arising from V a
surp,(i) estimates."
SURPRISE MINIMIZATION WITH TEMPERATURE,0.275609756097561,"5.2.2
SURPRISE MINIMIZATION WITH TEMPERATURE"
SURPRISE MINIMIZATION WITH TEMPERATURE,0.2780487804878049,"The importance of β can be validated by assessing its usage in surprise minimization. However, it is
difﬁcult to evaluate surprise minimization directly as surprise value function estimates V a
surp(s, u, σ)
vary from state-to-state across different agents and thus, they present high variance during agent’s
learning. We instead observe the variation of E as it is a collection of surprise-based sample estimates
across the batch. Additionally, E consists of prior samples V a
surp(s, u, σ) for V a
surp(s′, u′, σ′) which
makes inference across different agents tractable."
SURPRISE MINIMIZATION WITH TEMPERATURE,0.2804878048780488,"Figure 4: Variation of surprise minimization with temperature β. Learning of surprise is achieved by
making use of a suitable value of temperature parameter (β = 0.01) which controls the stability in
surprise minimization by utilizing E as intrinsic motivation."
SURPRISE MINIMIZATION WITH TEMPERATURE,0.28292682926829266,"Figure 4 presents the variation of Energy ratio E with the temperature parameter β during learning.
We compare two stable variations of E at β = 0.001 and β = 0.01. The objective minimizes
E over the course of learning and attains thermal equilibrium with minimum energy. Intuitively,
equilibrium corresponds to convergence to optimal policy π∗which validates the claim in Theorem 2.
With β = 0.01, EMIX presents improved convergence and surprise minimization for 5 out of the 6
considered scenarios, hence validating the suitable choice of β. On the other hand, a lower value of
β = 0.001 does little to minimize surprise across agents."
DISCUSSION,0.28536585365853656,"6
DISCUSSION"
DISCUSSION,0.28780487804878047,"In this paper, we presented an energy-based perspective towards surprise minimization in multi-
agent RL. Towards this goal we introduce EMIX, an energy-based intrinsic motivation framework
for surprise minimization in MARL algorithms. EMIX utilizes a temporal EBM to estimate and
minimize surprise jointly across all agents. Our theoretical claims on the formulation of minimization
of temporal energy with surprise are corroborated upon utilizing EMIX on a suite of challenging
MARL tasks requiring signiﬁcant collaboration under fast-paced dynamics."
DISCUSSION,0.29024390243902437,"While EMIX serves as a practical example of EBMs in cooperative MARL, it presents several new
avenues for future work. We shed light on 3 such aspects,"
DISCUSSION,0.2926829268292683,"(1) Provision of an energy-based model naturally raises the question of how can we efﬁciently sample
from the surprise distribution? Advances in sampling methods depict promise towards this aspect."
DISCUSSION,0.2951219512195122,"(2) Although suitable for lower dimensions, the scalability of EBMs towards high dimensional
action spaces remains an open question. We conjecture that the utility of density-based methods and
generative models can address the scalability gap."
DISCUSSION,0.2975609756097561,"(3) Lastly, the extension of an EBM framework to opponent-aware and competitive MARL settings
presents a suitable tangent for learning multi-agent roles. Provision of an EBM for learning minimum
energy role conﬁgurations would do away with the need for multi-stage training and complex
exploration strategies. We leave the aforesaid as potential directions for future work."
DISCUSSION,0.3,Under review as a conference paper at ICLR 2022
REFERENCES,0.3024390243902439,REFERENCES
REFERENCES,0.3048780487804878,"Joshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement
learning, 2017."
REFERENCES,0.3073170731707317,"Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.
In International Conference on Machine Learning, 2017."
REFERENCES,0.3097560975609756,"Glen Berseth, Daniel Geng, Coline Devin, Dinesh Jayaraman, Chelsea Finn, and Sergey Levine.
Smirl: Surprise minimizing rl in entropic environments. 2019."
REFERENCES,0.3121951219512195,"Dimitri P Bertsekas. Abstract dynamic programming. Athena Scientiﬁc Nashua, NH, USA, 2018."
REFERENCES,0.3146341463414634,"Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic Programming, volume 1. Athena Scientiﬁc,
1995."
REFERENCES,0.3170731707317073,"Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004."
REFERENCES,0.3195121951219512,"Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros.
Large-scale study of curiosity-driven learning. In ICLR, 2019."
REFERENCES,0.32195121951219513,"Lucian Bus¸oniu, Robert Babuˇska, and Bart De Schutter. Multi-agent reinforcement learning: An
overview. In Innovations in multi-agent systems and applications-1. 2010."
REFERENCES,0.32439024390243903,"Jerry Zikun Chen. Reinforcement learning generalization with surprise minimization, 2020."
REFERENCES,0.32682926829268294,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients, 2017."
REFERENCES,0.32926829268292684,"Karl Friston. The free-energy principle: a uniﬁed brain theory? Nature reviews neuroscience, 11(2):
127–138, 2010."
REFERENCES,0.33170731707317075,"Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods, 2018."
REFERENCES,0.33414634146341465,"Bolin Gao and Lacra Pavel. On the properties of the softmax function with application in game theory
and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017."
REFERENCES,0.33658536585365856,"Jordi Grau-Moya, Felix Leibfried, and Haitham Bou-Ammar. Balancing two-player stochastic games
with soft q-learning. arXiv preprint arXiv:1802.03216, 2018."
REFERENCES,0.33902439024390246,"Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.
Meta-
reinforcement learning of structured exploration strategies. In Advances in Neural Information
Processing Systems 31. 2018."
REFERENCES,0.34146341463414637,"Tuomas Haarnoja. Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement
Learning. PhD thesis, UC Berkeley, 2018."
REFERENCES,0.3439024390243902,"Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017."
REFERENCES,0.3463414634146341,"Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for
hierarchical reinforcement learning. arXiv preprint arXiv:1804.02808, 2018a."
REFERENCES,0.348780487804878,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018b."
REFERENCES,0.35121951219512193,"Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016."
REFERENCES,0.35365853658536583,"Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017."
REFERENCES,0.35609756097560974,Under review as a conference paper at ICLR 2022
REFERENCES,0.35853658536585364,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.36097560975609755,"Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model-based reinforcement learning for
atari, 2019."
REFERENCES,0.36341463414634145,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, 2014."
REFERENCES,0.36585365853658536,"Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for
decentralized planning. Neurocomputing, 190, 02 2016."
REFERENCES,0.36829268292682926,"Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep
reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in
neural information processing systems, 2016."
REFERENCES,0.37073170731707317,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. NeurIPS 2020, 2020."
REFERENCES,0.37317073170731707,"Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. Predicting structured data, 1, 2006."
REFERENCES,0.375609756097561,"Yann LeCun, Sumit Chopra, M Ranzato, and F-J Huang.
Energy-based models in document
recognition and computer vision. In Ninth International Conference on Document Analysis and
Recognition (ICDAR 2007), volume 1, 2007."
REFERENCES,0.3780487804878049,"Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov.
Efﬁcient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019."
REFERENCES,0.3804878048780488,"Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under
unknown dynamics. In Advances in Neural Information Processing Systems, 2014."
REFERENCES,0.3829268292682927,"Michael A Lones. How to avoid machine learning pitfalls: a guide for academic researchers. arXiv
preprint arXiv:2108.02497, 2021."
REFERENCES,0.3853658536585366,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments, 2017."
REFERENCES,0.3878048780487805,"Luis Macedo and Amilcar Cardoso. The role of surprise, curiosity and hunger on exploration
of unknown environments populated with entities. In 2005 portuguese conference on artiﬁcial
intelligence, 2005."
REFERENCES,0.3902439024390244,"Luis Macedo, Rainer Reisezein, and Amilcar Cardoso. Modeling forms of surprise in artiﬁcial agents:
empirical and theoretical study of surprise functions. In Proceedings of the Annual Meeting of the
Cognitive Science Society, volume 26, 2004."
REFERENCES,0.3926829268292683,"David J. C. MacKay. Information Theory, Inference & Learning Algorithms. Cambridge University
Press, 2002."
REFERENCES,0.3951219512195122,"Henry B Mann and Donald R Whitney. On a test of whether one of two random variables is
stochastically larger than the other. Annals of Mathematical Statistics, 18, 1947."
REFERENCES,0.3975609756097561,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, 2016."
REFERENCES,0.4,"John F. Nash. Equilibrium points in n-person games. Proceedings of the National Academy of
Sciences, 36(1), 1950."
REFERENCES,0.4024390243902439,"Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006."
REFERENCES,0.40487804878048783,Under review as a conference paper at ICLR 2022
REFERENCES,0.4073170731707317,"Brendan O’Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy
gradient and q-learning. arXiv preprint arXiv:1611.01626, 2016."
REFERENCES,0.4097560975609756,"Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs. Springer,
2016."
REFERENCES,0.4121951219512195,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In ICML 2018: Proceedings of the Thirty-Fifth International Conference
on Machine Learning, 2018."
REFERENCES,0.4146341463414634,"Wei Ren, Randal W Beard, and Ella M Atkins. A survey of consensus problems in multi-agent
coordination. In Proceedings of the 2005, American Control Conference, 2005., 2005."
REFERENCES,0.4170731707317073,"David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning Representations by
Back-propagating Errors. Nature, 323:533–536, 1986."
REFERENCES,0.4195121951219512,"Brian Sallans and Geoffrey E Hinton. Reinforcement learning with factored states and actions.
Journal of Machine Learning Research, 5, 2004."
REFERENCES,0.4219512195121951,"Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge, 2019."
REFERENCES,0.424390243902439,"John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning.
arXiv preprint arXiv:1704.06440, 2017."
REFERENCES,0.4268292682926829,"Philipp Schwartenbeck, Thomas FitzGerald, Ray Dolan, and Karl Friston. Exploration, novelty,
surprise, and free energy minimization. Frontiers in psychology, 4:710, 2013."
REFERENCES,0.4292682926829268,"Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015."
REFERENCES,0.4317073170731707,"Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel.
Value-decomposition networks for cooperative multi-agent learning based on team reward. In
Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems,
AAMAS ’18, pp. 2085–2087, 2018."
REFERENCES,0.43414634146341463,"Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In In Proceedings
of the Tenth International Conference on Machine Learning, 1993."
REFERENCES,0.43658536585365854,"Yee Whye Teh, Max Welling, Simon Osindero, and Geoffrey E Hinton. Energy-based models for
sparse overcomplete representations. Journal of Machine Learning Research, 4, 2003."
REFERENCES,0.43902439024390244,Sebastian B Thrun. Efﬁcient exploration in reinforcement learning. 1992.
REFERENCES,0.44146341463414634,"Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the
26th annual international conference on machine learning, 2009."
REFERENCES,0.44390243902439025,"Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Junyoung
Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John Agapiou, Max Jaderberg,
and David Silver. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature,
575, 11 2019."
REFERENCES,0.44634146341463415,"Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang.
{RODE}: Learning roles to decompose multi-agent tasks. In International Conference on Learning
Representations, 2021."
REFERENCES,0.44878048780487806,"Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. arXiv preprint
arXiv:1804.09817, 2018."
REFERENCES,0.45121951219512196,"Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. arXiv preprint arXiv:1901.09207, 2019."
REFERENCES,0.45365853658536587,Under review as a conference paper at ICLR 2022
REFERENCES,0.4560975609756098,"Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal
entropy. 2010."
REFERENCES,0.4585365853658537,"Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In AAAI, 2008."
REFERENCES,0.4609756097560976,Under review as a conference paper at ICLR 2022
REFERENCES,0.4634146341463415,"A
PROOFS"
REFERENCES,0.4658536585365854,"Theorem 1. Given a surprise value function V a
surp(s, u, σ) ∀a
∈
N, the energy operator
T V a
surp(s, u, σ) = log PN
a=1 exp (V a
surp(s, u, σ)) forms a contraction on V a
surp(s, u, σ)."
REFERENCES,0.4682926829268293,"Proof. We follow the process of (Asadi & Littman, 2017). Let us ﬁrst deﬁne a norm on surprise
values ||V1 −V2|| ≡max
s,u,σ|V1(s, u, σ) −V2(s, u, σ)|. Suppose ϵ = ||V1 −V2||, log N
X"
REFERENCES,0.47073170731707314,"a=1
exp (V1(s, u, σ)) ≤log N
X"
REFERENCES,0.47317073170731705,"a=1
exp (V2(s, u, σ) + ϵ) = log N
X"
REFERENCES,0.47560975609756095,"a=1
exp (V1(s, u, σ)) ≤log exp (ϵ) N
X"
REFERENCES,0.47804878048780486,"a=1
exp (V2(s, u, σ)) = log N
X"
REFERENCES,0.48048780487804876,"a=1
exp (V1(s, u, σ)) ≤ϵ + log N
X"
REFERENCES,0.48292682926829267,"a=1
exp (V2(s, u, σ)) = log N
X"
REFERENCES,0.4853658536585366,"a=1
exp (V1(s, u, σ)) −log N
X"
REFERENCES,0.4878048780487805,"a=1
exp (V2(s, u, σ)) ≤||V1 −V2||
(8)"
REFERENCES,0.4902439024390244,"Similarly, using ϵ with log PN
a=1 exp (V1(s, u, σ)), log N
X"
REFERENCES,0.4926829268292683,"a=1
exp (V1(s, u, σ) + ϵ) ≥log N
X"
REFERENCES,0.4951219512195122,"a=1
exp (V2(s, u, σ))"
REFERENCES,0.4975609756097561,"= log exp (ϵ) N
X"
REFERENCES,0.5,"a=1
exp (V1(s, u, σ)) ≥log N
X"
REFERENCES,0.5024390243902439,"a=1
exp (V2(s, u, σ))"
REFERENCES,0.5048780487804878,"= ϵ + log N
X"
REFERENCES,0.5073170731707317,"a=1
exp (V1(s, u, σ)) ≥log N
X"
REFERENCES,0.5097560975609756,"a=1
exp (V2(s, u, σ))"
REFERENCES,0.5121951219512195,"= ||V1 −V2|| ≥log N
X"
REFERENCES,0.5146341463414634,"a=1
exp (V2(s, u, σ)) −log N
X"
REFERENCES,0.5170731707317073,"a=1
exp (V1(s, u, σ))
(9)"
REFERENCES,0.5195121951219512,Results in Equation 8 and Equation 9 prove that the energy operation is a contraction.
REFERENCES,0.5219512195121951,"Theorem 2. Upon agent’s convergence to an optimal policy π∗, total energy of π∗, expressed by E∗
will reach a thermal equilibrium consisting of minimum surprise among consecutive states s and s′."
REFERENCES,0.524390243902439,"Proof. We begin by initializing a set of M policies {π1, π2..., πM} having energy ratios
{E1, E2..., EM}. Consider a policy π1 with surprise value function V1. E1 can then be expressed as"
REFERENCES,0.526829268292683,E1 = log
REFERENCES,0.5292682926829269,"""PN
a=1 exp (V a
1 (s′, u′, σ′))
PN
a=1 exp (V a
1 (s, u, σ)) #"
REFERENCES,0.5317073170731708,"Invoking Assumption 2 for s and s′, we can express V a
1 (s′, u′, σ′) = V a
1 (s, u, σ) + ζ1 where ζ1 is a
constant. Using this expression in E1 we get,"
REFERENCES,0.5341463414634147,E1 = log
REFERENCES,0.5365853658536586,"""PN
a=1 exp (V a
1 (s, u, σ) + ζ1)
PN
a=1 exp (V a
1 (s, u, σ)) #"
REFERENCES,0.5390243902439025,E1 = log
REFERENCES,0.5414634146341464,"""
exp (ζ1) PN
a=1 exp (V a
1 (s, u, σ))
PN
a=1 exp (V a
1 (s, u, σ)) #"
REFERENCES,0.5439024390243903,E1 = ζ1
REFERENCES,0.5463414634146342,Under review as a conference paper at ICLR 2022
REFERENCES,0.5487804878048781,"Similarly, E2 = ζ2,E3 = ζ3...,EM = ζM. Thus, the energy residing in policy π is proportional to the
surprise between consecutive states s and s′. Clearly, an optimal policy π∗is the one with minimum
surprise. Mathematically,"
REFERENCES,0.551219512195122,"π∗≥π1, π2..., πM =⇒ζ∗≤ζ1, ζ2..., ζM
= π∗≥π1, π2..., πM =⇒E∗≤E1, E2..., EM"
REFERENCES,0.5536585365853659,"Thus, proving that the optimal policy consists of minimum surprise at thermal equilibrium."
REFERENCES,0.5560975609756098,"B
RELATION TO MAXIMUM ENTROPY FRAMEWORK"
REFERENCES,0.5585365853658537,"B.1
SIMILARITIES & DIFFERENCES"
REFERENCES,0.5609756097560976,We conceptually compare EMIX to the maximum entropy framework.
REFERENCES,0.5634146341463414,"Similarities: Both methods utilize an auxilary objective as intrinsic motivation to tackle uncertainty.
While the maximum entropy formulation assigns low energy to uncertain actions, our method assigns
low energy to uncertain encoded representations od states (as presented in Fig. 2)."
REFERENCES,0.5658536585365853,"Differences: Our method differs from maximum entropy in its optimization process and learning
scheme. The maximum entropy formulation aims to maximize entropy in the value function space so
as to motivate exploration. Our proposed scheme, on the other hand, aims to minimize surprise in the
low-dimensional representation space to obtain dynamics-aware robust policies."
REFERENCES,0.5682926829268292,"B.2
CONNECTION TO SOFT Q-LEARNING"
REFERENCES,0.5707317073170731,"The Soft Q-Learning objective with V θ−
soft(s′) and Qsoft(u, s; θ) as state and action value functions
respectively is given by-"
REFERENCES,0.573170731707317,"JQ(θ) = Es,u∼R 1 2"
REFERENCES,0.5756097560975609,"
r + γEs′∼R[V θ−
soft(s′)] −Qsoft(u, s; θ)
2"
REFERENCES,0.5780487804878048,"= JQ(θ) = Es,u∼R  1 2 "
REFERENCES,0.5804878048780487,"r + γEs′∼R "" log
X"
REFERENCES,0.5829268292682926,"u∈A
exp Qsoft(u′, s′; θ−) #"
REFERENCES,0.5853658536585366,"−Qsoft(u, s; θ) !2 "
REFERENCES,0.5878048780487805,The gradient of this objective can be expressed as-
REFERENCES,0.5902439024390244,"∇θJQ(θ) = Es,u∼R """
REFERENCES,0.5926829268292683,"r + γEs′∼R "" log
X"
REFERENCES,0.5951219512195122,"u∈A
exp Q(u′, s′; θ−) #"
REFERENCES,0.5975609756097561,"−Qsoft(u, s; θ) !#"
REFERENCES,0.6,"∇θQsoft(u, s; θ) (10)"
REFERENCES,0.6024390243902439,And the gradient of the EMIX objective is obtained as-
REFERENCES,0.6048780487804878,"L(θ) = Es,u,s′∼R  1 2 "
REFERENCES,0.6073170731707317,"r + γmax
u′ Q(u′, s′; θ−) + β log"
REFERENCES,0.6097560975609756,"PN
a=1 exp (V a
surp(s′, u′, σ′))
PN
a=1 exp (V a
surp(s, u, σ)) !"
REFERENCES,0.6121951219512195,"−Q(u, s; θ) !2 "
REFERENCES,0.6146341463414634,"∇θL(θ) = Es,u,s′∼R """
REFERENCES,0.6170731707317073,"r + γmax
u′ Q(u′, s′; θ−)"
REFERENCES,0.6195121951219512,"+ β log
PN
a=1 exp (V a
surp(s′, u′, σ′))
PN
a=1 exp (V a
surp(s, u, σ))"
REFERENCES,0.6219512195121951,"
−Q(u, s; θ) !#"
REFERENCES,0.624390243902439,"∇θQ(u, s; θ)
(11)"
REFERENCES,0.6268292682926829,"Comparing Equation 10 to Equation 11 we notice that Soft Q-Learning and EMIX are related to
each other as they utilize EBMs. Soft Q-Learning makes use of a discounted energy function
which downweights the energy values over longer horizons. Actions consisting of lower energy
conﬁgurations are given preference by making use of Qsoft(u, s; θ) as the negative energy. On
the other hand, EMIX makes use of a constant energy function weighed by β which minimizes"
REFERENCES,0.6292682926829268,Under review as a conference paper at ICLR 2022
REFERENCES,0.6317073170731707,"surprise-based energy between consecutive states. Both the objectives can be thought of as energy
minimizing models which search for an optimal energy conﬁguration. Soft Q-Learning searches for
an optimal conﬁguration in the action space whereas EMIX favours optimal behavior on spurious
states. In fact, EMIX can be realized as a special case of Soft Q-Learning if the mixer agent utilizes
an energy-based policy and attains thermal equilibrium. This leads us to express Theorem 3.
Theorem
3.
Given
an
energy-based
policy
π
with
its
target
function
V (s′)
=
log P"
REFERENCES,0.6341463414634146,"u∈A exp Q(u′, s′; θ−), the surprise minimization objective L(θ) reduces to the Soft Q-
Learning objective L(θsoft) in the special case surprise absent between consecutive states,
PN
a=1 exp (V a
surp(s′, u′, σ′)) = PN
a=1 exp (V a
surp(s, u, σ))."
REFERENCES,0.6365853658536585,Proof. We know that the EMIX objective is given by-
REFERENCES,0.6390243902439025,"L(θ) = Es,u,s′∼R  1 2 "
REFERENCES,0.6414634146341464,"r + γmax
u′ Q(u′; s′, θ−) + β log"
REFERENCES,0.6439024390243903,"PN
a=1 exp (V a
surp(s′, u′, σ′))
PN
a=1 exp (V a
surp(s, u, σ)) !"
REFERENCES,0.6463414634146342,"−Q(u, s; θ) !2 "
REFERENCES,0.6487804878048781,"(12)
Replacing the greedy policy term max
u′ Q(u′, s′; θ−) with the energy-based value function V (s′) = log P"
REFERENCES,0.651219512195122,"u′∈A exp Q(u′, s′; θ−), we get,"
REFERENCES,0.6536585365853659,"L(θ) = Es,u,s′∼R  1 2 "
REFERENCES,0.6560975609756098,r + γEs′∼R[V (s′)] + β log
REFERENCES,0.6585365853658537,"PN
a=1 exp (V a
surp(s′, u′, σ′))
PN
a=1 exp (V a
surp(s, u, σ)) !"
REFERENCES,0.6609756097560976,"−Q(u, s; θ) !2  (13)"
REFERENCES,0.6634146341463415,"= L(θ) = Es,u,s′∼R ""
1
2 "
REFERENCES,0.6658536585365854,r + γEs′∼R
REFERENCES,0.6682926829268293,"
log
X"
REFERENCES,0.6707317073170732,"u′∈A
exp Q(u′, s′; θ−)
"
REFERENCES,0.6731707317073171,"+ β log
PN
a=1 exp (V a
surp(s′, u′, σ′))
PN
a=1 exp (V a
surp(s, u, σ))"
REFERENCES,0.675609756097561,"
−Q(u, s; θ) !2#"
REFERENCES,0.6780487804878049,"At thermal equilibrium, PN
a=1 exp (V a
surp(s, u, σ)) = PN
a=1 exp (V a
surp(s′, u′, σ′)),"
REFERENCES,0.6804878048780488,"= L(θ) = Es,u,s′∼R ""
1
2 "
REFERENCES,0.6829268292682927,r + γEs′∼R
REFERENCES,0.6853658536585366,"
log
X"
REFERENCES,0.6878048780487804,"u′∈A
exp Q(u′, s′; θ−)
"
REFERENCES,0.6902439024390243,"+ β log
PN
a=1 exp (V a
surp(s′, u′, σ′))
PN
a=1 exp (V a
surp(s′, u′, σ′))"
REFERENCES,0.6926829268292682,"
−Q(u, s; θ) !2#"
REFERENCES,0.6951219512195121,"= L(θ) = Es,u,s′∼R  1 2 "
REFERENCES,0.697560975609756,"r + γEs′∼R "" log
X"
REFERENCES,0.7,"u′∈A
exp Q(u′, s′; θ−) #"
REFERENCES,0.7024390243902439,"+ β log(1) −Q(u, s; θ) !2  (14)"
REFERENCES,0.7048780487804878,"= L(θ) = Es,u,s′∼R  1 2 "
REFERENCES,0.7073170731707317,"r + γEs′∼R "" log
X"
REFERENCES,0.7097560975609756,"u′∈A
exp Q(u′, s′; θ−) #"
REFERENCES,0.7121951219512195,"−Q(u, s; θ) !2"
REFERENCES,0.7146341463414634,"
(15)"
REFERENCES,0.7170731707317073,"Equation 15 represents the Soft Q-Learning objective, hence proving the result."
REFERENCES,0.7195121951219512,"C
CONVERGENCE ANALYSIS"
REFERENCES,0.7219512195121951,"We now analyze convergence of the surprise minimization scheme during policy optimization. For
brevity, our notation denotes the modiﬁed Bellman operator as B obeying the standard assumptions
of monotonicity and contraction (Bertsekas, 2018). Additionally, we consider the cumulative value
ˆVk = rk + Gk + β log PN
a=1 exp(V a
surp,(k)(s, u, σ)) as the sum of state values Vk = rk + Gk and"
REFERENCES,0.724390243902439,"surprise energy values β log PN
a=1 exp(V a
surp,(k)(s, u, σ)) at kth Bellman update."
REFERENCES,0.7268292682926829,Under review as a conference paper at ICLR 2022
REFERENCES,0.7292682926829268,"Consider
Vk −V ∗

2
with V ∗being the optimal value at convergence,"
REFERENCES,0.7317073170731707,"ˆVk −V ∗

2
≤
B ˆVk−1 + β log N
X"
REFERENCES,0.7341463414634146,"a=1
exp(V a
surp,(k)) −V ∗

2
(16)"
REFERENCES,0.7365853658536585,"≤
B2 ˆVk−2 + β log N
X"
REFERENCES,0.7390243902439024,"a=1
exp(V a
surp,(k−1)) + β log N
X"
REFERENCES,0.7414634146341463,"a=1
exp(V a
surp,(k)) −V ∗

2
(17)"
REFERENCES,0.7439024390243902,"≤
B2 ˆVk−2 + β  log N
X"
REFERENCES,0.7463414634146341,"a=1
exp(V a
surp,(k−1)) + log N
X"
REFERENCES,0.748780487804878,"a=1
exp(V a
surp,(k)) !"
REFERENCES,0.751219512195122,"−V ∗

2
(18)"
REFERENCES,0.7536585365853659,"≤
B2 ˆVk−2 + β  log "" N
X"
REFERENCES,0.7560975609756098,"a=1
exp(V a
surp,(k−1))"
REFERENCES,0.7585365853658537,"# "" N
X"
REFERENCES,0.7609756097560976,"a=1
exp(V a
surp,(k)) #!"
REFERENCES,0.7634146341463415,"−V ∗

2
(19)"
REFERENCES,0.7658536585365854,"Thus, for k iterations, we have,"
REFERENCES,0.7682926829268293,"≤
BkV0 + β  log k
Y i=1 "" N
X"
REFERENCES,0.7707317073170732,"a=1
exp(V a
surp,(i)) #!"
REFERENCES,0.7731707317073171,"−V ∗

2
(20)"
REFERENCES,0.775609756097561,"=
BkV0 + β  log N
X a=1 "" k
Y"
REFERENCES,0.7780487804878049,"i=1
exp(V a
surp,(i)) #!"
REFERENCES,0.7804878048780488,"−V ∗

2
(21)"
REFERENCES,0.7829268292682927,"=
BkV0 + β  log N
X a=1 "" exp( k
X"
REFERENCES,0.7853658536585366,"i=1
V a
surp,(i)) #!"
REFERENCES,0.7878048780487805,"−V ∗

2
(22)"
REFERENCES,0.7902439024390244,"We now absorb the sum of surprise values from time index i = 1, .., k in a single variable V a
tot. Thus,
using V a
tot = Pk
i=1 V a
surp,(i) and utilizing the Triangle Inequality, we get,"
REFERENCES,0.7926829268292683,"=
BkV0 −V ∗

2
+
β  log N
X"
REFERENCES,0.7951219512195122,"a=1
[exp(V a
tot)]"
REFERENCES,0.7975609756097561,"! 
2
(23)"
REFERENCES,0.8,"We now bound the two terms separately. Considering the ﬁrst term and following the results of value
iteration convergence (Bertsekas & Tsitsiklis, 1995),
BkV −V ∗

2
≤γk
V −V ∗

2
(24)"
REFERENCES,0.802439024390244,"= γk
V + Vµ −Vµ −V ∗

2
(25)"
REFERENCES,0.8048780487804879,"wherein Vµ denotes an approximation to V . Utilizing the triangle inequality yields,"
REFERENCES,0.8073170731707318,"≤γk
V −Vµ"
REFERENCES,0.8097560975609757,"2
+ γk
Vµ −V ∗

2
(26)"
REFERENCES,0.8121951219512196,"The two terms are bounded using the convergence result of (Bertsekas, 2018)."
REFERENCES,0.8146341463414634,"= γk√rmax + γk
s"
REFERENCES,0.8170731707317073,rmax|S|
REFERENCES,0.8195121951219512,"1 −γ
(27)"
REFERENCES,0.8219512195121951,"Now, considering the second term in Equation 23,"
REFERENCES,0.824390243902439,"≤β
 log N
X"
REFERENCES,0.8268292682926829,"a=1
exp(V a
tot)

2
(28)"
REFERENCES,0.8292682926829268,"= β
 log N
X"
REFERENCES,0.8317073170731707,"a=1
exp(V a
tot) −log N
X"
REFERENCES,0.8341463414634146,"a=1
exp(V ∗
tot) + log N
X"
REFERENCES,0.8365853658536585,"a=1
exp(V ∗
tot)

2
(29)"
REFERENCES,0.8390243902439024,Under review as a conference paper at ICLR 2022
REFERENCES,0.8414634146341463,"using the triangle inequality,"
REFERENCES,0.8439024390243902,"≤β
 log N
X"
REFERENCES,0.8463414634146341,"a=1
exp(V a
tot) −log N
X"
REFERENCES,0.848780487804878,"a=1
exp(V ∗
tot)

2
+ β
 log N
X"
REFERENCES,0.8512195121951219,"a=1
exp(V ∗
tot)

2
(30)"
REFERENCES,0.8536585365853658,"Since T = log PN
a=1 exp(V a
tot) is a contraction following Theorem 1, for the ﬁrst term we have,"
REFERENCES,0.8560975609756097,"≤βγ
V a
tot −V ∗
tot"
REFERENCES,0.8585365853658536,"2
+ β
 log N
X"
REFERENCES,0.8609756097560975,"a=1
exp(V ∗
tot)

2
(31)"
REFERENCES,0.8634146341463415,"The second term in the above relation is bounded due to the completeness assumption,
 log PN
a=1 exp(V ∗
tot)

2
."
REFERENCES,0.8658536585365854,"≤βγ
V a
tot −V ∗
tot"
REFERENCES,0.8682926829268293,"2
+ βζ , ζ > 0
(32)"
REFERENCES,0.8707317073170732,"Finally, combining Equation 27 and Equation 32 in Equation 23, we obtain the desired convergence
bound.
Vk −V ∗

2
≤γk
 
√rmax + s"
REFERENCES,0.8731707317073171,rmax|S| 1 −γ !
REFERENCES,0.875609756097561,"+ β

γ
V a
tot −V ∗
tot"
REFERENCES,0.8780487804878049,"2
+ ζ

(33)"
REFERENCES,0.8804878048780488,"While the ﬁrst term in Equation 33 denotes the convergence of policy optimization, the second
term indicates the bounded convergence of surprise to ecological niches with ﬁnite (yet nonzero)
surprising elements. The policy optimization process converges at a geometric rate O(γk) towards its
stable ﬁxed points. The surprise minimization process, on the other hand, demonstrates an annealing
behavior which depends on the temperature parameter β. Furthermore, convergence to stable ﬁxed
point V a
tot is bounded in respect to each agents individual surprise values V a
tot. This insight indicates
that different agents converge towards different locally optimal values of surprise. Finally, the
presence of constant ζ corroborates prior claims (Schwartenbeck et al., 2013; Friston, 2010) that
agents continue to experience surprise irrespective of their convergence to minimum energy niches."
REFERENCES,0.8829268292682927,"To further develop intuition for this claim, consider the special case wherein
V a
tot −V ∗
tot 2
→0."
REFERENCES,0.8853658536585366,"Irrespective of global convergence among all agents, a ﬁnite yet small ζ continues to contribute to the"
REFERENCES,0.8878048780487805,"upper bound of
Vk −V ∗

2
."
REFERENCES,0.8902439024390244,"Role of β: We further discuss the role of β which is of balancing the terms at successive iterations.
While the ﬁrst term geometrically decays with O(γk) rate, the second term approaches a ﬁnite
constant βζ as V a
tot →V ∗
tot. Irrespective of our choice of β, the LHS ∥Vk −V ∗∥2 is upper bounded
by a constant which validates the claims of minimum yet ﬁnite surprise values. We do note that a
small β is still desirable to remove any approximation errors in order to push Vk →V ∗. However,
this comes at the cost of increased surprise if β is not selected appropriately."
REFERENCES,0.8926829268292683,"D
IMPLEMENTATION DETAILS"
REFERENCES,0.8951219512195122,"D.1
MODEL SPECIFICATIONS"
REFERENCES,0.8975609756097561,"Architecture: This section highlights model architecture for the surprise value function. At the lower
level, the architecture consists of 3 independent networks called state net, q net and surp net. Each of
these networks consist of a single layer of 256 units with ReLU non-linearity as activations. Similar
to the mixer-network, we use the ReLU non-linearity in order to provide monotonicity constraints
across agents. Using a modular architecture in combination with independent networks leads to a
richer extraction of joint latent transition space. Outputs from each of the networks are concatenated
and are provided as input to the main net consisting of 256 units with ReLU activations. The main net
yields a single output as the surprise value V a
surp(s, u, σ) which is reduced along the agent dimension
by the energy operator. Alternatively, deeper versions of networks can be used in order to make the
extracted embeddings increasingly expressive. However, increasing the number of layers does little
in comparison to additional computational expense."
REFERENCES,0.9,Under review as a conference paper at ICLR 2022
REFERENCES,0.9024390243902439,"Computation of σ: The deviation σ corresponds to the standard deviation across each dimension of
the state s. Considering the state as a tensor of size B × A × M with B as the batch size, A as the
number of agents and M as the observation dimension, we compute σ by calculating the standard
deviation across the M dimension. This yields σ as a B × A × 1 dimensional array."
REFERENCES,0.9048780487804878,"Computation of surprise estimates: Vsurp denotes the surprise value function which quantiﬁes the
amount of surprise experienced by agents. Analogous to a Q value function which provides estimates
of returns, Vsurp provides estimate of surprise. Our framework learns Vsurp much like any other
value function (using a neural network), but by additionally undergoing a log P exp transformation
to obey the ﬁxed point property. This is achieved by realizing log-sum-exp as an energy operator
T = log P exp which can be computed using standard computation libraries. Since our code is im-
plemented in PyTorch, we implement this as T_V = torch.logsumexp(V_surp, dim=1)."
REFERENCES,0.9073170731707317,"D.2
HYPERPARAMETERS"
REFERENCES,0.9097560975609756,"Table 2 presents hyperparameter values for EMIX. A total of 2 target Q-functions were used as the
model is found to be robust to any greater values."
REFERENCES,0.9121951219512195,"Hyperparameters
Values
batch size
b = 32
learning rate
α = 0.0005
discount factor
γ = 0.99
target update interval
200 episodes
gradient clipping
10
exploration schedule
1.0 to 0.01 over 50000 steps
mixer embedding size
32
agent hidden size
64
temperature
β = 0.01
target Q-functions
2"
REFERENCES,0.9146341463414634,Table 2: Hyperparameter values for EMIX agents
REFERENCES,0.9170731707317074,"D.3
SELECTION & TUNING OF β"
REFERENCES,0.9195121951219513,"One can manually tune β using a ﬁne-grained hyperparameter search. We tune β between 0.001 and
1 in intervals of 0.01 with best performance observed at β = 0.01. However, we ﬁnd two additional
methods helpful for obtaining more accurate values. These are described as follows-"
REFERENCES,0.9219512195121952,"Armijo’s Line Search: One can borrow from optimization theory and utilize Armijo’s line search
Nocedal & Wright (2006) by setting a termination condition. The method starts with a constant
value of β which is iteratively incremented/decremented until a termination criterion (example-
∥∇L(θ)∥< ϵ with ϵ a constant) is reached. While line search is proven to converge towards globally
optimal values, its O(n2) convergence may be computationally expensive that too in the MARL
setting. Thus, we turn to the more efﬁcient automatic tuning."
REFERENCES,0.9243902439024391,Algorithm 2 Armijo’s Line Search
REFERENCES,0.926829268292683,"1: Initialize β, δ ∈(0, 1], EMIX & T V a
surp;
2: while EMIX(Q+β∗T V a
surp) > EMIX(Q)
+ α ∗β ∗∇EMIX(Q)TT V a
surp do
3:
β = δ ∗β
4: end while
5: return β"
REFERENCES,0.9292682926829269,Algorithm 3 Automatic Tuning
REFERENCES,0.9317073170731708,"1: Initialize β, δ ∈(0, 1], EMIX & T V a
surp;
2: EMIX(Q + β ∗T V a
surp)
3: beta loss = β ∗0.5 ∗(T V a
surp −0)2"
REFERENCES,0.9341463414634147,"4: beta loss.backward()
5: return β"
REFERENCES,0.9365853658536586,"Automatic Tuning: We choose to automatically tune β following single-agent RL literature Haarnoja
et al. (2018b); Kumar et al. (2020). This is achieved by treating β as a parameter and adaptively"
REFERENCES,0.9390243902439024,Under review as a conference paper at ICLR 2022
REFERENCES,0.9414634146341463,"optimizing over it using Adam. We treat a surprise value of 0 as our target value. The method works
well in practice and provides β values closer to 0.01 (our manual selection)."
REFERENCES,0.9439024390243902,"E
ADDITIONAL RESULTS"
REFERENCES,0.9463414634146341,"E.1
QUALITATIVE ANALYSIS"
REFERENCES,0.948780487804878,"Figure 5: Task- so many baneling, (left) Behaviors learned by EMIX agents, (right) Behaviors
learned by QMIX agents"
REFERENCES,0.9512195121951219,"Figure 6: Task- 2s vs 1sc, (left) Behaviors learned by EMIX agents, (right) Behaviors learned by
QMIX agents"
REFERENCES,0.9536585365853658,"We visualize and compare behaviors learned by surprise minimizing agents to the prior method of
QMIX. Fig. 5 presents the comparison of EMIX and QMIX agent trajectories (in yellow arrows) on
the challenging so many baneling task. The task consists of 27 baneling opponents which rapidly
attack the agent team on a bridge. QMIX agents naively move to the central alley of the bridge and
start attacking enemies early on. While QMIX agents naively maximize returns, EMIX agents learn a
different strategy. EMIX agents rearrange themselves ﬁrst at the corners of the bridge. Note that these
corners provide cover from enemy’s ﬁre. Thus, EMIX agents learn to take cover before approaching
the enemy head-on. This indicates that the surprise-robust policy is aware of the incoming fast-paced
assault."
REFERENCES,0.9560975609756097,"As another example, Fig. 6 presents behaviors on the 2s vs 1sc task wherein two agents must
collaborate together to defeat a SpineCrawler enemy. The enemy, having a long tentacle pointing to
the front, chooses to attack any one of the agents randomly in front of it. Additionally, the tentacle
has a ﬁxed length and cannot extend beyond this range. Random intermittent attacks indicate that
the agents face a greater degree of surprise with no prior knowledge of the enemy’s movement. We
observe that QMIX agents take turns to attack the enemy by moving back and forth to minimize
damage. EMIX agents, on the other hand, learn a different strategy. One of the EMIX agents stands at
a distance to attack th enemy while the other agent goes around to attack from behind. This indicates
that the policy is aware of enemy’s limited movement."
REFERENCES,0.9585365853658536,"E.2
STATISTICAL SIGNIFICANCE"
REFERENCES,0.9609756097560975,"We follow the recommendation of Lones (2021) and evaluate the statistical signiﬁcance of our results
by carrying out the Mann-Whitney U test Mann & Whitney (1947). All 5 seeds of an algorithm"
REFERENCES,0.9634146341463414,Under review as a conference paper at ICLR 2022
REFERENCES,0.9658536585365853,"Scenarios
EMIX
SMiRL-QMIX
QMIX
VDN
COMA
IQL
2s vs 1sc
14
7
-
21
25
4
2s3z
15
9
-
6
0
0
3m
17
0
-
0
2
12
3s vs 3z
11
3
-
0
0
1
3s vs 4z
21
0
-
2
0
0
3s vs 5z
5
0
-
25
0
0
3s5z
7
13
-
0
0
0
8m
15
1
-
1
3
0
8m vs 9m
7
11
-
0
0
0
10m vs 11m
14
25
-
6
0
0
so many baneling
24
14
-
9
4
0
5m vs 6m
21
15
-
18
0
0"
REFERENCES,0.9682926829268292,"Table 3: Comparison of the U statistic on StarCraft II benchmark. U here denotes the statistical
signiﬁcance of an algorithm against QMIX (higher is better)."
REFERENCES,0.9707317073170731,"(on each task) are compared to that of QMIX to yield the U statistic. U here denotes the statistical
signiﬁcance of performance with higher values being desirable."
REFERENCES,0.973170731707317,"Table 3 presents the comparison of U statistic on the StartCraft II benchmark. EMIX demonstrates
consistently high values of U across a diverse set of tasks when compared to SMiRL and prior MARL
agents. This highlights the consistent surprise-minimizing performance of EMIX across random
seeds."
REFERENCES,0.975609756097561,"E.3
ADDITIONAL TASKS"
REFERENCES,0.9780487804878049,"This section compares EMIX and TwinQMIX to prior MARL methods on the Predator-Prey tasks. In
addition to the difﬁculty of task, we vary the number of opponents. This helps quantify the variation
in performance against increasing level of surprise under ﬁxed dynamics. Table 4 presents average
returns. While all agents present comparable performance on the easier tasks, EMIX improves over
QMIX and TwinQMIX on the more challenging punish and hard tasks. In the case of punish, EMIX
is the only method to achieve greater than 20 returns outperforming baselines by a signiﬁcant margin."
REFERENCES,0.9804878048780488,"Scenarios
EMIX
TwinQMIX
SMiRL-QMIX
QMIX
VDN
COMA
IQL
predator prey easy
40.00 ± 0.13
40.00 ± 0.34
40.00 ± 0.98
40.00 ± 0.22
38.74 ± 0.64
27.49 ± 4.26
34.73 ± 2.92
predator prey
40.00 ± 0.72
40.00 ± 1.92
40.00 ± 0.27
40.00 ± 0.16
36.23 ± 3.19
25.13 ± 0.92
31.59 ± 0.74
predator prey punish
24.17 ± 3.29
20.32 ± 4.15
19.31 ± 1.12
14.33 ± 3.81
17.21 ± 2.31
10.92 ± 4.35
7.86 ± 3.21
predator prey hard
12.34 ± 3.11
10.19 ± 1.15
10.47 ± 0.83
8.76 ± 4.33
5.19 ± 3.97
-4.37 ± 1.53
-9.26 ± 4.84"
REFERENCES,0.9829268292682927,"Table 4: Comparison of average returns between EMIX, its ablations and prior MARL methods on
Predator-Prey tasks. EMIX improves over QMIX agent. In comparison to SMiRL-QMIX, EMIX
demonstrates improved minimization of surprise. Results are averaged over 5 random seeds."
REFERENCES,0.9853658536585366,"Figure 7:
Variation
in performance with
increasing number of
agents."
REFERENCES,0.9878048780487805,"We consider a simple toy task from the Predator-Prey benchmark to
demonstrate the importance of surprise minimization. We select preda-
tor prey easy due to its simplicity and convenient dynamics. The task
consists of 3 agents and 3 opponents. We increase the number of opponents
while keeping the task ﬁxed. This way the dynamics of the MDP remain
unchanged and the only changing factor is opponent behaviors."
REFERENCES,0.9902439024390244,"Fig. 7 presents the variation of average returns for EMIX and QMIX
over 5 random seeds. While QMIX agents undergo a steady decrease in
performance, EMIX agents are found robust to this fast degradation. Even
after the addition of 20 opponents (against only 3 agents), EMIX is able
to retain positive returns. The algorithm acquires a surprise robust-policy
early on during training to tackle fast-paced changes introduced by the large
number of agents."
REFERENCES,0.9926829268292683,Under review as a conference paper at ICLR 2022
REFERENCES,0.9951219512195122,"E.4
ADDITIONAL ABLATIONS"
REFERENCES,0.9975609756097561,Figure 8: Variation in success rates with temperature β. A value of β = 0.01 is found to work best.
