Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005076142131979695,"Our goal is to understand why the robustness accuracy would abruptly drop to zero,
after conducting FGSM-style adversarial training for too long. While this phe-
nomenon is commonly explained as overÔ¨Åtting, we observe that it is a twin process:
not only does the model catastrophic overÔ¨Åts to one type of perturbation, but also
the perturbation deteriorates into random noise. For example, at the same epoch
when the FGSM-trained model catastrophically overÔ¨Åts, its generated perturbations
deteriorate into random noise. Intuitively, once the generated perturbations become
weak and inadequate, models would be misguided to overÔ¨Åt those weak attacks
and fail to defend strong ones. In the light of our analyses, we propose APART, an
adaptive adversarial training method, which parameterizes perturbation generation
and progressively strengthens them. In our experiments, APART successfully
prevents perturbation deterioration and catastrophic overÔ¨Åtting. Also, APART
signiÔ¨Åcantly improves the model robustness while maintaining the same efÔ¨Åciency
as FGSM-style methods, e.g., on the CIFAR-10 dataset, APART achieves 53.89%
accuracy under the PGD-20 attack and 49.05% accuracy under the AutoAttack1."
INTRODUCTION,0.01015228426395939,"1
INTRODUCTION"
INTRODUCTION,0.015228426395939087,"While neural networks keep advancing the state of the arts, their vulnerability to adversarial at-
tacks (Szegedy et al., 2013) casts a shadow over their applications‚Äîsubtle, human-imperceptible
input shifts can fool these models and alter their predictions. Adversarial training adds perturbations
to model inputs during training, and is one of the most successful approaches to establish model
robustness (Goodfellow et al., 2014; Madry et al., 2017; Kurakin et al., 2017; Tram√®r et al., 2017;
Zhang et al., 2019b; Liu et al., 2018)."
INTRODUCTION,0.02030456852791878,"One crucial and common challenge in adversarial training is the signiÔ¨Åcant computation overhead,
e.g., it may take 3-30 times longer to conduct adversarial training than the vanilla training. In
response to this challenge, there has been a recent surge in work aiming to reduce the computation
overhead (Goodfellow et al., 2014; Shafahi et al., 2019a; Zhang et al., 2019a; Wong et al., 2020).
Although these methods successfully accelerates adversarial training, they lead to an unexpected
phenomenon‚Äîwhen conducting FGSM-style adversarial training, the model robustness would
abruptly drop to zero (Rice et al., 2020) at certain epoch. This phenomenon is referred as catastrophic
overÔ¨Åtting, and the robustness drop is usually viewed as model overÔ¨Åtting‚Äîthe model overÔ¨Åts to one
speciÔ¨Åc type of perturbation (Rice et al., 2020; Wong et al., 2020; Kim et al., 2021)."
INTRODUCTION,0.025380710659898477,"Here, we show that the robustness drop is a twin process. Besides the model overÔ¨Åtting to one
type of perturbation, we observe perturbations becoming too weak to establish model robustness.
For example, as visualized in Figure 1, both the model and the perturbation change dramatically
between Epoch 15 and Epoch 16‚Äîthe robust accuracy of the model drops from 45 to almost zero,
the perturbation strength deteriorates dramatically. As implied by this phenomenon, we suggest that
there exists strong correlations between the catastrophic overÔ¨Åtting and perturbation deterioration.
Intuitively, without perturbation deterioration, even if the model overÔ¨Åts to one type of perturbation
with a reasonable strength, it would lead to a sub-optimal robustness instead of entirely diminished
robust accuracy. Meanwhile, once the perturbation deteriorates into random noise, overÔ¨Åtting to that
random noise-like perturbation could cause the model robustness drop to zero."
INTRODUCTION,0.030456852791878174,1Code will be released under the Apache-2.0 license for future studies.
INTRODUCTION,0.03553299492385787,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04060913705583756,Table 1: Notation Table (Elaborated in Section 2)
INTRODUCTION,0.04568527918781726,"x is input
y is label
Œ± is step size
L is objective
‚àÜx = ‚àÇL/‚àÇx
‚àÜŒ∏ = ‚àÇL/‚àÇŒ∏"
INTRODUCTION,0.050761421319796954,"Œ∏(i)
A
model parameter Œ∏ trained for i epochs by method A"
INTRODUCTION,0.05583756345177665,"fA(Œ∏, x, y)
perturbation generated by method A to attack model Œ∏ on x, y"
INTRODUCTION,0.06091370558375635,"œâx
perturbation initialization as parameterized by FGSM+ and APART"
INTRODUCTION,0.06598984771573604,"GB,i(A)
perturbation strength of fA(Œ∏(i)
A ), calculated as its gap to fB(Œ∏(i)
A ) on model Œ∏(i)
A"
INTRODUCTION,0.07106598984771574,"Acc(Œ∏(i1)
A , fB(Œ∏(i2)
C
, ¬∑))
accuracy of Œ∏(i1)
A
when attacked by perturbations generated by fB(¬∑) for Œ∏(i2)
C"
INTRODUCTION,0.07614213197969544,"Truck
Bird
Airplane                  Ship"
INTRODUCTION,0.08121827411167512,ùüèùüìùíïùíâepoch
INTRODUCTION,0.08629441624365482,ùüèùüîùíïùíâepoch
INTRODUCTION,0.09137055837563451,Original Image
INTRODUCTION,0.09644670050761421,"Perturbation 
distinguishes the 
object and the 
background"
INTRODUCTION,0.10152284263959391,"Perturbations is 
in a random 
manner"
INTRODUCTION,0.1065989847715736,(a) FGSM-Generated Perturbations.
INTRODUCTION,0.1116751269035533,"15!"" 16!"""
INTRODUCTION,0.116751269035533,"FGSM 
APART-Simple
APART"
INTRODUCTION,0.1218274111675127,(b) Test Accuracy under PGD-10.
INTRODUCTION,0.12690355329949238,Figure 1: Analyses of FGSM-generated perturbations (for Pre-ResNet18 on the CIFAR-10 dataset).
INTRODUCTION,0.1319796954314721,"From this perspective, the key to prevent catastropic overÔ¨Åtting falls upon shielding models from per-
turbation deterioration. Correspondingly, we design an adaptive adversarial training method, APART,
which parameterizes perturbation generation and updates its parameters to progressively strengthen
the perturbation generator with gradient ascent. SpeciÔ¨Åcally, we Ô¨Årst treat perturbation initialization
as parameters‚Äîinstead of starting from scratch every time, APART improve the initialization with
gradient ascent. Then, APART factorize the input perturbation as a series of perturbations, which
are integrated with learnable step sizes and can self-adapt to different scenarios. In our experi-
ments, APART leads to consistent performance improvements over FGSM-style algorithms while
maintaining roughly the same efÔ¨Åciency."
PRELIMINARIES AND NOTATIONS,0.13705583756345177,"2
PRELIMINARIES AND NOTATIONS"
PRELIMINARIES AND NOTATIONS,0.14213197969543148,"Given a neural network with n convolution blocks, we denote the input of the i-th block as xi, the
input and output of the entire network as x and y. Note that, x and x1 are the same in conventional
residual networks, such as ResNet (He et al., 2016a), Wide ResNet (Zagoruyko & Komodakis, 2016),
and Pre-Act ResNet (He et al., 2016b). Adversarial training aims to establish the model robustness by
solving the optimization problem as below (Œ∏ is the network parameter, Œ¥ is the perturbation, (x, y) is
a data-label pair, and f(¬∑) is the perturbation generation function)."
PRELIMINARIES AND NOTATIONS,0.14720812182741116,"min
Œ∏
L(Œ∏, x + Œ¥, y) s.t. Œ¥ = f(Œ∏, x, y).
(1)"
PRELIMINARIES AND NOTATIONS,0.15228426395939088,"Different adversarial training methods generate perturbations differently. Ideally, adversarial training
should use the most effective (i.e., strongest) perturbation within the same norm constraint, i.e.,
f ‚àó(¬∑) = argmax||Œ¥||‚â§œµ L(Œ∏, x+Œ¥, y). In practice, as an approximation, f(¬∑) is typically implemented
as gradient ascent with Ô¨Åxed iteration and step size. For example, the FGSM algorithm directly
calculates perturbations as fFGSM(Œ∏FGSM, x, y) = œµ ¬∑ sign(‚àÜx). Also, we use Œ∏(i)
method to refer to
model parameters that are trained by a speciÔ¨Åc adversarial training algorithm for i epochs. We
use Acc(Œ∏(i1)
A , fB(Œ∏(i2)
C
, ¬∑)) to indicate the performance of model Œ∏(i1)
A , under fB(Œ∏(i2)
C
, ¬∑)), i.e., the
perturbation generated by method fB(¬∑) to attack Œ∏(i2)
C
. These notations are summarized in Table 1."
PRELIMINARIES AND NOTATIONS,0.15736040609137056,Under review as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATIONS,0.16243654822335024,"15!"" 16!"""
PRELIMINARIES AND NOTATIONS,0.16751269035532995,"Random 
FGSM
APART-Simple
APART"
PRELIMINARIES AND NOTATIONS,0.17258883248730963,"(a) Accuracy of Œ∏(30)
PGD-10 on transferred attacks (smaller
value indicates stronger attacks). Line named as method
refers to Acc(Œ∏(30)
PGD-10, fFGSM(Œ∏i"
PRELIMINARIES AND NOTATIONS,0.17766497461928935,"method, ¬∑)), while Random
refers to Acc(Œ∏(30)
PGD-10, frandom(¬∑))."
PRELIMINARIES AND NOTATIONS,0.18274111675126903,"15!"" 16!"""
PRELIMINARIES AND NOTATIONS,0.18781725888324874,"FGSM
APART-Simple APART"
PRELIMINARIES AND NOTATIONS,0.19289340101522842,"(b) Strength gap between method and PGD-10, i.e.,
L(Œ∏(i)
method; x + fmethod(Œ∏(i)
method, x, y), y) ‚àíL(Œ∏(i)
method; x +
fPGD-10(Œ∏(i)
method, x, y), y).
Smaller value indicates
stronger attacks."
PRELIMINARIES AND NOTATIONS,0.19796954314720813,"Figure 2: Perturbation strength in different epochs. In part (a), perturbation strength is estimated by
transfer the adversary image to attack a model trained by PGD-10 separately for 30 epochs. In part
(b), perturbation strength is estimated as the gap to a stronger attack (PGD-10 here)"
PERTURBATION DETERIORATION AND CATASTROPHIC OVERFITTING,0.20304568527918782,"3
PERTURBATION DETERIORATION AND CATASTROPHIC OVERFITTING"
PERTURBATION DETERIORATION AND CATASTROPHIC OVERFITTING,0.20812182741116753,"Typically, the robustness drop is viewed as model overÔ¨Åtting (Rice et al., 2020; Wong et al., 2020;
Kim et al., 2021) and this phenomenon is referred as catastrophic overÔ¨Åtting. Meanwhile, it has been
observed that catastrophic overÔ¨Åtting only happens to simple methods like FGSM (Goodfellow et al.,
2014) 2, indicating this phenomenon is not only about model overÔ¨Åtting, but also the strength of
perturbation. Inspired by this observation, we aim to explore the other side of catastophic overÔ¨Åtting,
i.e., perturbation deterioration."
PERTURBATION STRENGTH,0.2131979695431472,"3.1
PERTURBATION STRENGTH"
PERTURBATION STRENGTH,0.2182741116751269,"To verify our intuition, we try to empirically estimate the perturbation strength and analyze its
dynamics during the adversarial training."
PERTURBATION STRENGTH,0.2233502538071066,"First, we try to estimate the perturbation strength of fA as Acc(Œ∏30
PGD-10, fA(Œ∏i
A, ¬∑)), i.e., using fA(Œ∏i
A)
to attack a model trained separately with PGD-10 for 30 epochs. For comparisons, we also list the
performance of Œ∏30
PGD-10 under random noise. As visualized in Figure 2(a), in the Ô¨Årst 15 epochs, the
perturbation strength of FGSM keeps increasing, while at the 16 epoch, its strength dramatically
drops to the random noise level. This shows that, after the perturbation deterioration, the perturbations
not only look like random noise, but also behaves like random noise."
PERTURBATION STRENGTH,0.22842639593908629,"Alternatively, we estimate the perturbation strength of fA as its gap to a more powerful method fB.
SpeciÔ¨Åcally, we calculate the strength for fA(Œ∏(i)
A , ¬∑) as GB,i(A) in Equation 2. Intuitively, the weaker
the perturbation fA is, the larger GB,i(A) would be."
PERTURBATION STRENGTH,0.233502538071066,"GB,i(A) = L(Œ¥A) ‚àíL(Œ¥B) where L(Œ¥method) = L(Œ∏(i)
A ; x + fmethod(Œ∏(i)
A , x, y), y).
(2)"
PERTURBATION STRENGTH,0.23857868020304568,"We conduct experiments with Pre-Act ResNet18 on the CIFAR-10 dataset and visualize
GPGD-10,i(FGSM) in Figure 2(b). It shows that the strength gap between PGD-10 and FGSM is
small in the early stage, dramatically explodes at the 16th epoch, and keeps a large value since then
(i.e., the perturbation strength mostly vanishes at the 16th epoch)."
PERTURBATION STRENGTH,0.2436548223350254,"Perturbation Deterioration. In both cases, the timing of perturbation strength deterioration co-
incides with the timing of the robustness drop, thus supporting our intuition that the other side of"
PERTURBATION STRENGTH,0.24873096446700507,"2For a FGSM trained Pre-ResNet18 (on CIFAR-10), its accuracy under PGD-20 attacks drops from 45%
(epoch 15) to 0% (epoch 16); For a PGD-10 trained model, it drops from 49% (epoch 30) to 38% (epoch 200)."
PERTURBATION STRENGTH,0.25380710659898476,Under review as a conference paper at ICLR 2022
PERTURBATION STRENGTH,0.25888324873096447,"Algorithm 1: APART (the Ô¨Årst and the second round propagations are marked with red and blue;
œµ is the perturbation bound; ¬µŒ∏, ¬µœâ, and ¬µŒ± are learning rates for Œ∏, œâ, and Œ±; Table 1 summarizes
others notations)."
WHILE NOT CONVERGED DO,0.2639593908629442,1 while not converged do
WHILE NOT CONVERGED DO,0.26903553299492383,"2
for x, y in the training set do"
WHILE NOT CONVERGED DO,0.27411167512690354,"3
Œ¥1 ‚ÜêŒ±œâ ¬∑ œâx //initialize the perturbation for the model input."
WHILE NOT CONVERGED DO,0.27918781725888325,"4
Œ¥1 ‚Üêmax(min(Œ¥1 + Œ±1 ¬∑ sign( ‚àÇL(Œ∏;x+Œ¥1,y)"
WHILE NOT CONVERGED DO,0.28426395939086296,"‚àÇx1
), ‚àíœµ), +œµ) //calculate Œ¥1."
WHILE NOT CONVERGED DO,0.2893401015228426,"5
for each residual block with index i > 1 do"
WHILE NOT CONVERGED DO,0.29441624365482233,"6
Œ¥i ‚ÜêŒ±i ¬∑ sign( ‚àÇL(Œ∏;x+Œ¥1,y)"
WHILE NOT CONVERGED DO,0.29949238578680204,"‚àÇxi
) //calculate perturbations for block i."
WHILE NOT CONVERGED DO,0.30456852791878175,"7
Œ∏ = Œ∏ ‚àí¬µŒ∏ ¬∑ ‚àÇL(Œ∏;{xi}n
i=1+{Œ¥i}n
i=1,y)
‚àÇŒ∏
//update parameters."
WHILE NOT CONVERGED DO,0.3096446700507614,"8
œâx = max(min(œâx + ¬µœâ ¬∑ sign( ‚àÇL(Œ∏;{xi}n
i=1+{Œ¥i}n
i=1,y)
‚àÇœâx
), ‚àí1), 1) //update œâx."
WHILE NOT CONVERGED DO,0.3147208121827411,"9
Œ±i = Œ±i + ¬µŒ± ¬∑ ( ‚àÇL(Œ∏;{xi}n
i=1+{Œ¥i}n
i=1,y)
‚àÇŒ±i
‚àíŒª ¬∑ ‚àÇ|Œ±i|2
2
‚àÇŒ±i ) //update step sizes."
WHILE NOT CONVERGED DO,0.3197969543147208,10 return Œ∏
WHILE NOT CONVERGED DO,0.3248730964467005,"catastrophic overÔ¨Åtting is perturbation deterioration. Intuitively, adversarial training cannot establish
satisfying model robustness without strong enough perturbations, and strong perturbations require
a small gap G(¬∑). However, one local optima of Equation 1 is the parameter Œ∏ that deteriorates
f(Œ∏, ¬∑) into random noise, which advances the optimization of Equation 1 at the cost of deteriorated
perturbations. Also, since all parameter updates are made to decrease L, most existing methods have
no regularization to keep the perturbation strength."
ADAPTIVITY HELPS PREVENT CATASTROPHIC OVERFITTING,0.3299492385786802,"3.2
ADAPTIVITY HELPS PREVENT CATASTROPHIC OVERFITTING"
ADAPTIVITY HELPS PREVENT CATASTROPHIC OVERFITTING,0.3350253807106599,"Here, we further verify our intuition by showing that it prevents the catastrophic overÔ¨Åtting by
alleviating the perturbation deterioration. Intuitively, one straightforward way to strengthen generators
is to parameterize them and update them together with model parameters. SpeciÔ¨Åcally, we treat the
perturbation initialization for the input (denoted as œâx) and the step size (referred as Œ±) as parameters
of FGSM, and change the objective from Equation 1 to:"
ADAPTIVITY HELPS PREVENT CATASTROPHIC OVERFITTING,0.3401015228426396,"max
Œ±,œâ min
Œ∏
L(Œ∏; x + fFGSM(Œ±, œâx; Œ∏, x, y), y).
(3)"
ADAPTIVITY HELPS PREVENT CATASTROPHIC OVERFITTING,0.34517766497461927,"During model training, we update Œ∏ with gradient descent and update Œ± and œâx with gradient ascent.
We refer this variant as APART-Simple. Note that its only difference to FGSM is that APART-
Simple is parameterized and can be enhanced during training, thus suffering less from perturbation
deterioration."
ADAPTIVITY HELPS PREVENT CATASTROPHIC OVERFITTING,0.350253807106599,"We conduct experiments with Pre-Act ResNet18 on the CIFAR-10 dataset, and visualize
Acc(Œ∏(30)
PGD-10, fFGSM(Œ∏(i)
APART-Simple, ¬∑)) in Figure 2(a) and GPGD-10,i(APART-Simple) in Figure 2(b). It shows
that APART-Simple does not suffer from the catastrophic overÔ¨Åtting. This not only supports our
intuition that the perturbation deterioration is one cause of the catastrophic overÔ¨Åtting, but motivates
us to add more adaptivity to the perturbation generator."
ADAPTIVE ADVERSARIAL TRAINING,0.3553299492385787,"4
ADAPTIVE ADVERSARIAL TRAINING"
ADAPTIVE ADVERSARIAL TRAINING,0.3604060913705584,"Guided by our analyses, we propose to improve FGSM by further improving the perturbation
generator during the training. Since the algorithm features the ability to adapt itself, we refer our
method as adaptive adversarial training (APART). SpeciÔ¨Åcally, it Ô¨Årst factorizes the perturbation
for the input image into a series of perturbations, one for each residual block. Moreover, it employ
different step sizes for different perturbations, treat them as learnable parameters, update them to
integrate perturbations adaptively, and strengthen the generator during the model training."
ADAPTIVE ADVERSARIAL TRAINING,0.36548223350253806,"Factorize the Input Perturbation. For a multi-layer network, the perturbation generated at the
input attacks not only the Ô¨Årst layer, but also all following layers. Intuitively, existing methods like
PGD-N implicitly blender these attacks with N additional forward- and backward-propagations on"
ADAPTIVE ADVERSARIAL TRAINING,0.37055837563451777,Under review as a conference paper at ICLR 2022
ADAPTIVE ADVERSARIAL TRAINING,0.3756345177664975,Table 2: Model Performance of WideResNet34-10 on the CIFAR-10 dataset.
ADAPTIVE ADVERSARIAL TRAINING,0.38071065989847713,"EfÔ¨Åcient
Methods
PGD-20
AA
C&W
Gaussian
Clean Data
Time/Epoch"
ADAPTIVE ADVERSARIAL TRAINING,0.38578680203045684,"√ó
ATTA-10
54.33%
49.10%
59.11%
77.05%
83.80%
706 secs
√ó
PGD-10
55.41%
52.08%
58.77%
77.70%
86.43%
680 secs"
ADAPTIVE ADVERSARIAL TRAINING,0.39086294416243655,"‚úì
Free-8
47.68%
46.21%
56.31%
75.98%
85.54%
252 secs
‚úì
S+FGSM
36.71%
33.15%
44.50%
81.25%
85.15%
232 secs
‚úì
F+FGSM
46.37%
44.27%
56.21%
75.10%
85.10%
122 secs
‚úì
APART
53.89%
49.05%
58.50%
77.31%
84.65%
162 secs"
ADAPTIVE ADVERSARIAL TRAINING,0.39593908629441626,"the input perturbation, which signiÔ¨Åcantly inÔ¨Çates the computation cost. Here, we factorize the
input perturbation as a series of perturbations and explicitly learn to combine them. SpeciÔ¨Åcally,
we refer to the input of i-th residual block as xi and the output of i-th residual block as xi +
CNNs(xi). Then, we add the perturbation ‚àÜxi to the input of CNNs(¬∑) to establish its robustness,
i.e., xi + CNNs(xi + Œ¥i) where Œ¥i = Œ±i‚àÜxi. Similar to PGD-N, this approach also involves multiple
perturbations; different from PGD-N, these perturbations can be calculated with the same forward-
and backward-propagations."
ADAPTIVE ADVERSARIAL TRAINING,0.4010152284263959,"Initialization Parameterization. Similar to APART-Simple, we also treat perturbation initializations
as learnable parameters to better defense the deterioration. Since it consumes additional memory,
we only parameterize the perturbation initialization at the input, and keep all other perturbations
zero-initialized. In this way, the additional storage has roughly the same size with the dataset."
ADAPTIVE ADVERSARIAL TRAINING,0.40609137055837563,"APART Algorithm. We summarize APART in Algorithm 1. Same with FGSM, it contains two
rounds of forward- and backward-propagations. In the Ô¨Årst round, it initializes the input perturbation
and calculates gradients for both the input of the Ô¨Årst layer and the input of all the following blocks.
In the second round, it applies the generated perturbations to the input of the corresponding blocks,
i.e., change xi + CNNs(xi) to xi + CNNs(xi + Œ¥i). Then, besides updating model parameters with
gradient descent, we enhance the generator with gradient ascent (i.e., updating step sizes Œ±i and the
perturbation initialization œâx). Note that, to control the magnitude of step sizes Œ±i, we add a L2
regularization to its updates and use Œª to control it (as line 9 in Algorithm 1)."
ADAPTIVE ADVERSARIAL TRAINING,0.41116751269035534,"Note that, calculating the exact gradients of œâx or Œ±œâ requires a second order derivation (‚àÜœâx
and ‚àÜŒ±œâ are based on Œ¥i, and the calculation of Œ¥i includes some Ô¨Årst order derivations involving
œâx and Œ±œâ). Due to the success of the First-order MAML (FOMAML) (Finn et al., 2017), we
simpliÔ¨Åes the calculation by omitting higher order derivations. SpeciÔ¨Åcally, FOMAML demonstrates
the effectiveness to ignore higher order derivations and approximate the exact gradient with only
Ô¨Årst order derivations. Here, we have a similar objective with FOMAML‚ÄîFOMAML aims to Ô¨Ånd a
good model initialization, and we try to Ô¨Ånd a good perturbation initialization. Thus, we also restrict
gradient calculations to Ô¨Årst-order derivations. In this way, APART has roughly the same computation
complexity with FGSM and it is signiÔ¨Åcantly faster than PGD-N."
EXPERIMENTS,0.41624365482233505,"5
EXPERIMENTS"
EXPERIMENTS,0.4213197969543147,"As in Figures 2(a) and Figure 2(b), APART shields adversarial training from catastrophic overÔ¨Åtting
and largely alleviates the robustness drop. Systematic evaluations are further conducted as below."
EXPERIMENTAL SETTINGS,0.4263959390862944,"5.1
EXPERIMENTAL SETTINGS"
EXPERIMENTAL SETTINGS,0.43147208121827413,"Datasets. We conduct experiments on the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) as
well as the ImageNet dataset (Krizhevsky et al., 2012)."
EXPERIMENTAL SETTINGS,0.4365482233502538,"Neural Architectures. We conduct experiments with ResNet He et al. (2016a), Pre-ResNet (He
et al., 2016b) and WideResNet (Zagoruyko & Komodakis, 2016). SpeciÔ¨Åcally, we use Pre-ResNet18
and WideResNet34-10 on the CIFAR-10 dataset, Pre-ResNet18 on the CIFAR-100 dataset, and
ResNet50 (He et al., 2016a) on the ImageNet dataset."
EXPERIMENTAL SETTINGS,0.4416243654822335,"Data Augmentation. Following the previous work, we apply the standard data augmentation. For
the CIFAR datasets, we apply random Ô¨Çipping as a data augmentation procedure and take a random"
EXPERIMENTAL SETTINGS,0.4467005076142132,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETTINGS,0.4517766497461929,Table 3: Model Performance of Pre-ResNet18 on the CIFAR-10 dataset.
EXPERIMENTAL SETTINGS,0.45685279187817257,"EfÔ¨Åcient
Methods
PGD-20
AA
C&W
Gaussian
Clean Data
Time/Epoch"
EXPERIMENTAL SETTINGS,0.4619289340101523,"√ó
ATTA-10
49.03%
45.70%
58.30%
73.10%
82.10%
140 secs
√ó
PGD-10
52.16%
48.50%
58.97%
72.33%
82.05%
133 secs"
EXPERIMENTAL SETTINGS,0.467005076142132,"‚úì
Free-8
47.37%
44.53%
56.10%
72.56%
81.64%
62 secs
‚úì
S+FGSM
34.47%
32.15%
42.21%
73.45%
89.28%
61 secs
‚úì
F+FGSM
46.06%
42.37%
55.34%
72.25%
83.81%
20 secs
‚úì
APART
51.30%
45.92%
58.73%
73.65%
82.80%
29 secs"
EXPERIMENTAL SETTINGS,0.4720812182741117,"crop with 32 √ó 32 from images padded by 4 pixels on each side (Lee et al., 2015). For the ImageNet
dataset, we divide the training into three phases, where phases 1 and 2 use images resized to 160 and
352 pixels and the third phase uses the original images (Wong et al., 2020)."
EXPERIMENTAL SETTINGS,0.47715736040609136,"Optimizer. For all experiments, we use SGD with momentum as the optimizer. The momentum
factor is set to 0.9 and the training is conducted for 60 epochs with cyclic learning rate (Smith, 2017),
where the maximum learning rate is set to 0.2 and minimum learning rate is set to 0. For the ImageNet
dataset, we adopt a setting similar to Wong et al. (2020) and train the model for 15 epochs; The
maximum learning rate of cyclic learning rate schedule is set to 0.4 and the learning rate is set to 0."
EXPERIMENTAL SETTINGS,0.48223350253807107,"Other Hyper-parameters. For all experiments, we apply the cyclic learning rate scheduler for ¬µŒ±.
On the CIFAR datasets, the maximum learning rate and Œª are set as 5 √ó 10‚àí8 and 200, respectively;
on the ImageNet dataset, they are set as 4 √ó 10‚àí9 and 5000, respectively. Due to the similarity
between line 4 and line 7 in Algorithm 1, we set ¬µœâ as Œ±1"
EXPERIMENTAL SETTINGS,0.4873096446700508,"Œ±œâ , which makes the update on œâ has a similar
impact with the update in line 4 of Algorithm 1."
EXPERIMENTAL SETTINGS,0.49238578680203043,"Robustness Evaluation. We adopt PGD-20, AutoAttack (Croce & Hein, 2020), Gaussian random
noise, and C&W (Carlini & Wagner, 2017) as the attack methods for evaluation. For both adversarial
training and evaluation, we restrict perturbations to |Œ¥|‚àû‚â§8/255 on the CIFAR datasets, |Œ¥|‚àû‚â§
2/255 on the ImageNet dataset."
EXPERIMENTAL SETTINGS,0.49746192893401014,"Infrastructure. Our experiments are conducted with NVIDIA Quadro RTX 8000 GPUs; mixed-
precision arithmetic (Micikevicius et al., 2018) is adopted to accelerate model training; the training
speed of APART or baselines is evaluated on an idle GPU."
COMPARED METHODS,0.5025380710659898,"5.2
COMPARED METHODS"
COMPARED METHODS,0.5076142131979695,"For comparison, we select three state-of-the-art adversarial training methods, which features efÔ¨Åcient
training. Also, we list two other adversarial training methods that are signiÔ¨Åcant slower. On the
CIFAR datasets, we report accuracy and training time based on our experiments. As to ImageNet, we
directly refer to the number reported in the original papers."
COMPARED METHODS,0.5126903553299492,"‚Ä¢ PGD-N (Madry et al., 2017) is a classical, sophisticated adversarial training method. PGD is an
iterative version of FGSM with uniform random noise as initialization and N is the number of
iterations.
‚Ä¢ ATTA-K (Zheng et al., 2019) uses the adversarial examples from neighboring epochs. K is the
number of iterations and denotes the strength of attack.
‚Ä¢ Free-m (Shafahi et al., 2019a) uses the same backward propagation to update both the model and
trains on the same minibatch m times in a row. Here we set m = 8.
‚Ä¢ F+FGSM (Wong et al., 2020) uses a large step size and random initialization to improve FGSM. It
achieves comparable performance with PGD-10, but still suffers from the robustness drop.
‚Ä¢ S+FGSM (Kim et al., 2021) uses several checkpoints to validate the inner interval of perturbation
direction to determine the appropriate magnitude of the perturbation of each image."
PERFORMANCE COMPARISON,0.5177664974619289,"5.3
PERFORMANCE COMPARISON"
PERFORMANCE COMPARISON,0.5228426395939086,"Generally, we observe that PGD and ATTA achieves better performance than other methods, at
the cost of signiÔ¨Åcant training overheads. Meanwhile, APART achieves consistent performance
improvements over FGSM-style methods, while maintaining roughly the same training speed."
PERFORMANCE COMPARISON,0.5279187817258884,Under review as a conference paper at ICLR 2022
PERFORMANCE COMPARISON,0.5329949238578681,Table 4: Model Performance of Pre-ResNet18 on the CIFAR-100 dataset.
PERFORMANCE COMPARISON,0.5380710659898477,"EfÔ¨Åcient
Methods
PGD-20
AA
C&W
Gaussian
Clean Data
Time/Epoch"
PERFORMANCE COMPARISON,0.5431472081218274,"√ó
ATTA-10
25.60%
22.90%
30.75%
42.10%
56.20%
140 secs
√ó
PGD-10
28.10%
25.11%
33.35%
42.41%
57.23%
133 secs"
PERFORMANCE COMPARISON,0.5482233502538071,"‚úì
Free-8
25.88%
22.15%
30.55%
42.15%
55.13%
62 secs
‚úì
S+FGSM
10.15%
8.91%
15.11%
50.00%
71.10%
61 secs
‚úì
F+FGSM
25.31%
21.32%
30.06%
42.03%
57.95%
20 secs
‚úì
APART
27.56%
23.38%
32.36%
44.37%
58.40%
29 secs"
PERFORMANCE COMPARISON,0.5532994923857868,"Table 5: Model Performance of ResNet50 on the ImageNet dataset. + indicates single precision
training."
PERFORMANCE COMPARISON,0.5583756345177665,"EfÔ¨Åcient
Methods
Clean Data
PGD-10 Attack
Time/Epoch"
PERFORMANCE COMPARISON,0.5634517766497462,"√ó
ATTA-2 (Zheng et al., 2019)
60.70%
44.57%
4.85+ hrs"
PERFORMANCE COMPARISON,0.5685279187817259,"‚úì
Free-4 (Shafahi et al., 2019a)
64.44%
43.52%
3.46 hrs
‚úì
F+FGSM (Wong et al., 2020)
60.90%
43.46%
0.8 hrs
‚úì
APART
60.52%
44.30%
1 hrs"
PERFORMANCE COMPARISON,0.5736040609137056,Table 6: Ablation study of APART on the CIFAR-10 dataset with Pre-ResNet20.
PERFORMANCE COMPARISON,0.5786802030456852,"Training Methods
Clean Data
PGD-20 Attack
AA"
PERFORMANCE COMPARISON,0.583756345177665,"APART
82.80%
51.30%
45.92%
APART (w/o layer-wise perturbation)
83.64%
47.28%
43.10%
APART (w/o perturbation initialization)
82.42%
50.10%
45.10%"
PERFORMANCE COMPARISON,0.5888324873096447,"SpeciÔ¨Åcally, we summarize results on the CIFAR-10 in Table 2 and Table 3, CIFAR-100 in Table 4,
and ImageNet in Table 5. Comparing to Free-8, S+FGSM, and F+FGSM, APART achieves consistent
performance improvements against PGD-20, AutoAttack, and C&W attack for both Pre-ResNet18
and WideResNet34-10. As to ATTA-10 and PGD-10, APART achieves slightly worse performance
with 4+ times speedup. Among all methods, F+FGSM is the fastest method, and APART signiÔ¨Åcantly
improves the model robustness without signiÔ¨Åcant computation overheads. For example, F+FGSM
takes 122 secs/epoch for training WideResNet34-10 on the CIFAR-10 dataset, and APART takes 162
secs/epoch to achieve a 7.52 absolute accuracy improvement under the PGD-20 attack and a 4.78
absolute accuracy improvement under the AutoAttack."
PERFORMANCE COMPARISON,0.5939086294416244,"It is worth mentioning that, on CIFAR-10 and CIFAR-100, S-FGSM has the best performance under
Gaussian noise attack due to its high clean accuracy while the performances under PGD-20 and C&W
attack are much poorer than other methods. This trade-off is further discussed in Section 5.4."
BALANCING CLEAN ACCURACY AND ROBUST ACCURACY,0.5989847715736041,"5.4
BALANCING CLEAN ACCURACY AND ROBUST ACCURACY"
BALANCING CLEAN ACCURACY AND ROBUST ACCURACY,0.6040609137055838,"As shown in Tables 3, 2, and 4, the robustness improvement usually comes at the cost of accuracy
on clean images. Meanwhile, the performances on corrupted images have consistent trends, e.g., if
method A outperforms B under PGD-20 attack, A likely also outperforms B under AutoAttack or
C&W attack. To better understand the trade-off between the clean accuracy and the model robustness,
we employ different œµ values during training (i.e., [ 2"
BALANCING CLEAN ACCURACY AND ROBUST ACCURACY,0.6091370558375635,"255,
3
255, ¬∑ ¬∑ ¬∑ , 10"
BALANCING CLEAN ACCURACY AND ROBUST ACCURACY,0.6142131979695431,"255]), train multiple models for each
method, and visualize their performance of Pre-ResNet20 on the CIFAR-10 dataset in Figure 4. Points
of APART locate in the right top corner of the Ô¨Ågure and signiÔ¨Åcantly outperform other methods.
This further veriÔ¨Åes the effectiveness of APART."
ABLATION STUDIES,0.6192893401015228,"5.5
ABLATION STUDIES"
ABLATION STUDIES,0.6243654822335025,"APART employs two techniques to parameterize the perturbation generator. The Ô¨Årst is to learn an
initialization for the perturbation, and the second is to factorize input perturbations into a series of"
ABLATION STUDIES,0.6294416243654822,Under review as a conference paper at ICLR 2022
ABLATION STUDIES,0.6345177664974619,"5
10
15
20
25
30
Index of epochs"
ABLATION STUDIES,0.6395939086294417,0.0000
ABLATION STUDIES,0.6446700507614214,0.0025
ABLATION STUDIES,0.649746192893401,0.0050
ABLATION STUDIES,0.6548223350253807,0.0075
ABLATION STUDIES,0.6598984771573604,0.0100
ABLATION STUDIES,0.6649746192893401,0.0125
ABLATION STUDIES,0.6700507614213198,0.0150
ABLATION STUDIES,0.6751269035532995,0.0175 Value 0 1 2 3 4 5 6 7 8
ABLATION STUDIES,0.6802030456852792,"Figure 3: Step sizes in different epochs
(with Pre-ResNet18 on CIFAR-10)"
ABLATION STUDIES,0.6852791878172588,"70
75
80
85
90
Clean Accuracy 10 20 30 40 50"
ABLATION STUDIES,0.6903553299492385,Robust Test Accuracy
ABLATION STUDIES,0.6954314720812182,"Free(m=8)
F+FGSM
S+FGSM
APART(Ours)"
ABLATION STUDIES,0.700507614213198,"Figure 4: Pre-ResNet18 performance on
CIFAR-10 (œµtrain = [ 2"
ABLATION STUDIES,0.7055837563451777,"255, ¬∑ ¬∑ ¬∑ , 10 255])"
ABLATION STUDIES,0.7106598984771574,"0
25
50
75
100
125
150
175
200
Index of epochs 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
ABLATION STUDIES,0.7157360406091371,Accuracy
ABLATION STUDIES,0.7208121827411168,"PGD-10
APART"
ABLATION STUDIES,0.7258883248730964,(a) Training Accuracy
ABLATION STUDIES,0.7309644670050761,"0
25
50
75
100
125
150
175
200
Index of epochs 0.0 0.1 0.2 0.3 0.4 0.5"
ABLATION STUDIES,0.7360406091370558,Accuracy
ABLATION STUDIES,0.7411167512690355,"PGD-10
APART"
ABLATION STUDIES,0.7461928934010152,(b) Test Accuracy w. PGD-20
ABLATION STUDIES,0.751269035532995,"0
25
50
75
100
125
150
175
200
Index of epochs 0.1 0.2 0.3 0.4 0.5 0.6"
ABLATION STUDIES,0.7563451776649747,Loss value
ABLATION STUDIES,0.7614213197969543,"PGD 5
PGD 20(
(i)
PGD 5)"
ABLATION STUDIES,0.766497461928934,"APART
PGD 20(
(i)
APARt)"
ABLATION STUDIES,0.7715736040609137,(c) Perturbation Gap to PGD-20.
ABLATION STUDIES,0.7766497461928934,Figure 5: APART can alleviate the robust overÔ¨Åtting (note that APART is ‚àº4x faster than PGD-10).
ABLATION STUDIES,0.7817258883248731,"perturbations. To understand the effectiveness of them, we conduct an ablation study and summarized
the results in Table 6. Removing layer-wise perturbations leads to a 4.02% drop and a 2.48% drop on
accuracy under PGD-20 attack and AutoAttack respectively; Removing perturbation initialization
leads to a 1.20% drop and a 0.82% drop on accuracy under PGD-20 attack and AutoAttack respectively.
Therefore, both techniques are helpful and necessary to achieve a better model robustness."
EVOLUTION OF STEP SIZES,0.7868020304568528,"5.6
EVOLUTION OF STEP SIZES"
EVOLUTION OF STEP SIZES,0.7918781725888325,"After factorizing the input perturbation as a series of perturbations, we employ learnable step sizes
to compose perturbations more effectively. To better understand these step sizes, we visualize their
values during the training of Pre-ResNet20 on the CIFAR-10 dataset in Figure 3. It shows that the
Ô¨Årst-layer perturbation is more important than others. Also, the step sizes of perturbation at the Ô¨Årst
and the second layers decrease after 10 epochs, while other step sizes keep increasing across the
training. This phenomenon veriÔ¨Åes that APART is able to adapt the generator setting to different
training stages."
ALLEVIATION OF ROBUST OVERFITTING,0.7969543147208121,"5.7
ALLEVIATION OF ROBUST OVERFITTING"
ALLEVIATION OF ROBUST OVERFITTING,0.8020304568527918,"Besides shielding the model from catastrophic overÔ¨Åtting, we observe that APART can also alleviate
the robust overÔ¨Åtting. SpeciÔ¨Åcally, as visualize the training curve of APART and PGD-10 in Figure 5,
both APART and PGD-10 consistently get better training accuracy in the Ô¨Årst 200 epochs. Meanwhile,
after the Ô¨Årst 30 epochs, both methods suffer from robustness drop against the PGD-20 attack. This
phenomenon is referred as robust overÔ¨Åtting, and we can observe that APART suffers less from
the robust overÔ¨Åtting. To better understand this phenomenon, we also calculated their perturbation
strength gap to PGD-20, i.e., GPGD-20,i(method) = L(Œ∏(i)
method; x+fmethod(Œ∏(i)
method, x, y), y)‚àíL(Œ∏(i)
method; x+
fPGD-20(Œ∏(i)
method, x, y), y). We can Ô¨Ånd that both methods get larger perturbation strength gap in the
later stage of training, and APART consistently gets a smaller gap. Intuitively, adaptive perturbation
generator can prevents the model overÔ¨Åtting to a single type of perturbation, thus alleviating the
robust overÔ¨Åtting."
ALLEVIATION OF ROBUST OVERFITTING,0.8071065989847716,Under review as a conference paper at ICLR 2022
RELATED WORK,0.8121827411167513,"6
RELATED WORK"
ADVERSARIAL TRAINING,0.817258883248731,"6.1
ADVERSARIAL TRAINING"
ADVERSARIAL TRAINING,0.8223350253807107,"Goodfellow et al. (2014) Ô¨Årst recognize the cause of the adversarial vulnerability to be the extreme
nonlinearity of deep neural networks and introduced the fast gradient sign method (FGSM) to generate
adversarial examples with a single gradient step. Madry et al. (2017) propose an iterative method
based on FGSM with random starts, Projected Gradient Descent (PGD). PGD adversarial training
is effective but time-consuming, and thus some recent work also pays attention to the efÔ¨Åciency of
adversarial training. For example, Shafahi et al. (2019b) propose to update both the model parameters
and image perturbations using one simultaneous backward pass. Zhang et al. (2019a) show that
the Ô¨Årst layer of the neural network is more important than other layers and make the adversary
computation focus more on the Ô¨Årst layer. Zheng et al. (2019) also improve the utilization of gradients
to reuse perturbations across epochs. Wong et al. (2020) use uniform random initialization to improve
the performance of FGSM adversarial training. APART improves the efÔ¨Åciency and effectiveness
of adversarial training by factorizing the input perturbation as a series of perturbations. Previous
methods only added the perturbation to input images, while APART adds perturbation to the input
of residual blocks. Perturbations added to intermediate variables help improve the robustness, as
discussed in Section 5.5."
ROBUSTNESS DROP,0.8274111675126904,"6.2
ROBUSTNESS DROP"
ROBUSTNESS DROP,0.8324873096446701,"Wong et al. (2020) mention the robustness drop as overÔ¨Åtting and Ô¨Årst identify a failure mode named
as ‚Äúcatastrophic overÔ¨Åtting‚Äù, which caused FGSM adversarial training to fail against PGD attacks.
Rice et al. (2020) further explore the overÔ¨Åtting in other adversarial training methods, such as PGD
adversarial training and TRADES. They observe that the best test set performance was achieved after
a certain epochs and further training would lead to a consistent decrease in the robust test accuracy,
and therefore explain it as ‚Äúrobust overÔ¨Åtting‚Äù. Rice et al. (2020) show that robustness drop is a
general phenomenon but they did not analyze its cause. Kim et al. (2021) asserts that catastrophic
overÔ¨Åtting is caused by the Ô¨Åxed perturbation step size in single-step adversarial training, while we
found PGD-2 may also suffer from catastrophic overÔ¨Åtting, even it does not Ô¨Åx the perturbation step
size. In this work, we explore the nature of robustness drop in adversarial training and further propose
APART to address the perturbation deterioration issue."
CONCLUSIONS AND FUTURE WORK,0.8375634517766497,"7
CONCLUSIONS AND FUTURE WORK"
CONCLUSIONS AND FUTURE WORK,0.8426395939086294,"In this paper, we attempt to explore the mechanism behind catastrophic overÔ¨Åtting. As the common
wisdom views the robustness drop as model overÔ¨Åtting, our analyses in Section 3 present a novel
perspective and suggest that the other side of catastrophic overÔ¨Åtting is perturbation deterioration.
Guided by our analyses, we propose APART, an adaptive adversarial training framework. APART
parameterizes the perturbation initialization, factorizes the input perturbation into a series of per-
turbations (one for each layer in the neural networks), and progressively strengthens them during
the training. In our experiments, APART not only successfully shields the model from catastrophic
overÔ¨Åtting, but also achieves consistently performance improvements while maintaining roughly the
same training efÔ¨Åciency with FGSM-style methods."
CONCLUSIONS AND FUTURE WORK,0.8477157360406091,"The major limitations of our method is that it can only be applied to residual networks, and it achieves
faster training at the cost of some model robustness. There are several interesting directions to
pursue in future work, including applying APART to general neural models, further improve the
performance of APART by further strengthing the perturbation generator. Besides, we plan to explore
the underlying mechanism of other phenomenons, like the trade-off between clean accuracy and
robust accuracy."
CONCLUSIONS AND FUTURE WORK,0.8527918781725888,"Reproducibility. In this study, we conduct experiments on three public datasets, i.e., CIFAR-10,
CIFAR-100, and ImageNet. We will release implementations for all methods and scripts for all
experiments on GitHub, under the Apache-2.0 license."
CONCLUSIONS AND FUTURE WORK,0.8578680203045685,Under review as a conference paper at ICLR 2022
REFERENCES,0.8629441624365483,REFERENCES
REFERENCES,0.868020304568528,"Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. 2017
IEEE Symposium on Security and Privacy (SP), pp. 39‚Äì57, 2017."
REFERENCES,0.8730964467005076,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020."
REFERENCES,0.8781725888324873,"Chelsea Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. ArXiv, abs/1703.03400, 2017."
REFERENCES,0.883248730964467,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.8883248730964467,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770‚Äì778, 2016a."
REFERENCES,0.8934010152284264,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630‚Äì645. Springer, 2016b."
REFERENCES,0.8984771573604061,"Hoki Kim, Woojin Lee, and J. Lee. Understanding catastrophic overÔ¨Åtting in single-step adversarial
training. In AAAI, 2021."
REFERENCES,0.9035532994923858,Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.9086294416243654,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097‚Äì1105,
2012."
REFERENCES,0.9137055837563451,"Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
ArXiv, abs/1607.02533, 2017."
REFERENCES,0.9187817258883249,"Chen-Yu Lee, Saining Xie, P. W. Gallagher, Z. Zhang, and Zhuowen Tu. Deeply-supervised nets.
ArXiv, abs/1409.5185, 2015."
REFERENCES,0.9238578680203046,"Xuanqing Liu, Minhao Cheng, Huan Zhang, and C. Hsieh. Towards robust neural networks via
random self-ensemble. In ECCV, 2018."
REFERENCES,0.9289340101522843,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ArXiv, abs/1706.06083, 2017."
REFERENCES,0.934010152284264,"P. Micikevicius, Sharan Narang, J. Alben, G. Diamos, E. Elsen, D. Garc√≠a, B. Ginsburg, Michael
Houston, O. Kuchaiev, G. Venkatesh, and H. Wu. Mixed precision training. ArXiv, abs/1710.03740,
2018."
REFERENCES,0.9390862944162437,"Leslie Rice, Eric Wong, and J. Z. Kolter. OverÔ¨Åtting in adversarially robust deep learning. ArXiv,
abs/2002.11569, 2020."
REFERENCES,0.9441624365482234,"Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, Larry S.
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! ArXiv, abs/1904.12843,
2019a."
REFERENCES,0.949238578680203,"Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer,
Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in
Neural Information Processing Systems, pp. 3353‚Äì3364, 2019b."
REFERENCES,0.9543147208121827,"Leslie N. Smith. Cyclical learning rates for training neural networks. 2017 IEEE Winter Conference
on Applications of Computer Vision (WACV), pp. 464‚Äì472, 2017."
REFERENCES,0.9593908629441624,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
fellow, and Rob Fergus. Intriguing properties of neural networkstheoratical. arXiv preprint
arXiv:1312.6199, 2013."
REFERENCES,0.9644670050761421,Under review as a conference paper at ICLR 2022
REFERENCES,0.9695431472081218,"Florian Tram√®r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017."
REFERENCES,0.9746192893401016,"Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training.
ArXiv, abs/2001.03994, 2020."
REFERENCES,0.9796954314720813,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. ArXiv, abs/1605.07146, 2016."
REFERENCES,0.9847715736040609,"Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. In NeurIPS, 2019a."
REFERENCES,0.9898477157360406,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019b."
REFERENCES,0.9949238578680203,"Haizhong Zheng, Ziqi Zhang, Juncheng Gu, Honglak Lee, and Atul Prakash. EfÔ¨Åcient adversarial
training with transferable adversarial examples. ArXiv, abs/1912.11969, 2019."
