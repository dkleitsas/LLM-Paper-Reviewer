Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011135857461024498,"Encoder-decoder architectures have recently gained popularity in sequence
to sequence modelling, featuring in state-of-the-art models such as trans-
formers. However, a mathematical understanding of their working princi-
ples still remains limited. In this paper, we study the approximation prop-
erties of recurrent encoder-decoder architectures. Prior work established
theoretical results for RNNs in the linear setting, where approximation
capabilities can be related to smoothness and memory of target temporal
relationships. Here, we uncover that the encoder and decoder together form
a particular “temporal product structure” which determines the approxi-
mation eﬃciency. Moreover, the encoder-decoder architecture generalises
RNNs with the capability to learn time-inhomogeneous relationships. Our
results provide the theoretical understanding of approximation properties
of the recurrent encoder-decoder architecture, which precisely characterises,
in the considered setting, the types of temporal relationships that can be
eﬃciently learned."
INTRODUCTION,0.0022271714922048997,"1
Introduction"
INTRODUCTION,0.0033407572383073497,"Encoder-decoder is an increasingly popular architecture for sequence to sequence modelling
problems (Sutskever et al., 2014; Chiu et al., 2018; Venugopalan et al., 2015). The core
of this architecture is to ﬁrst encode the input sequence into a vector using the encoder
and then map the vector into the output sequence through the decoder.
In particular,
such architecture forms the main component in the transformer network (Vaswani et al.,
2017), which has become a powerful method for modelling sequence to sequence relationships
(Parmar et al., 2018; Beltagy et al., 2020; Li et al., 2019)."
INTRODUCTION,0.004454342984409799,"The encoder-decoder family of structures diﬀer signiﬁcantly from direct application of recur-
rent neural networks (RNNs, Elman (1990)) and its generalisations (Hochreiter & Schmid-
huber, 1997; Cho et al., 2014b) for processing sequences. However, both architectures can
be considered as modelling mappings between sequences, albeit with diﬀerent underlying
structures. Hence, a natural but unresolved question is: how are these approaches funda-
mentally diﬀerent? Answering this question is not only of theoretical importance but also
of practical interest. Currently, architectural selection for diﬀerent time series modelling
tasks is predominantly empirical. Thus, it is desirable to develop a concrete mathemati-
cal framework to understand the key diﬀerences between separate architectures in order to
guide practitioners in a principled way."
INTRODUCTION,0.005567928730512249,"∗Equal contribution
†Corresponding author"
INTRODUCTION,0.0066815144766146995,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0077951002227171495,"In this paper, we investigate the approximation properties of encoder-decoder architectures.
Approximation is one of the most basic and important problems for supervised learning. It
considers to what extent the model can ﬁt a target. In particular, we prove a general approx-
imation result in the linear setting, which characterises the types of temporal input-output
relationships that can be eﬃciently approximated by encoder-decoder architectures. These
results reveal that such architectures essentially generalise RNNs by lifting the requirement
of time-homogeneity (see Remark 3.2) in the target relationships. Hence, it can be used
to tackle a broader class of sequence to sequence problems. Furthermore, of particular in-
terest is the identiﬁcation of a “temporal product structure” — a precise property of the
target temporal relationship that highlights another intrinsic diﬀerence between recurrent
encoder-decoders and RNNs."
INTRODUCTION,0.008908685968819599,Our main contributions can be summarised as follows.
WE PROVE A UNIVERSAL APPROXIMATION RESULT FOR RECURRENT ENCODER-DECODER ARCHITEC-,0.01002227171492205,"1. We prove a universal approximation result for recurrent encoder-decoder architec-
tures in the linear setting, including the approximation rates.
2. We show that in the considered setting, the recurrent encoder-decoder generalises
the RNNs and can approximate time-inhomogeneous relationships, which further
adapt to additional temporal product structures in the target relationship. This
answers precisely how encoder-decoders are diﬀerent from RNNs, at least in the
considered setting."
WE PROVE A UNIVERSAL APPROXIMATION RESULT FOR RECURRENT ENCODER-DECODER ARCHITEC-,0.011135857461024499,"Organisation.
In Section 2, we review the related work on encoder-decoder architectures
and general approximation theories of sequence modelling. The approximation problem is
formulated in Section 3. Our main results, their consequences and numerical illustrations
are presented in Section 4. All the proofs and numerical details are included in appendices."
WE PROVE A UNIVERSAL APPROXIMATION RESULT FOR RECURRENT ENCODER-DECODER ARCHITEC-,0.012249443207126948,"Notations.
For consistency, we adhere to the following notations. Boldfaced letters are
reserved for sequences or paths, which can be understood as functions of time. Lower case
letters can mean vectors or scalars. Matrices are denoted by capital letters. For α ∈N, Cα
denotes the space of functions with continuous derivatives up to order-α."
RELATED WORK,0.013363028953229399,"2
Related work"
RELATED WORK,0.014476614699331848,"We ﬁrst review some previous works on sequence to sequence modelling.
The encoder-
decoder architecture ﬁrst appeared in Kalchbrenner & Blunsom (2013), where they map the
input sequence into a vector using convolutional neural networks (CNNs), and then using
a recurrent structure to map the vector to the output sequence. With the ﬂexibility of
manipulating the underlying structure of encoder and decoder, numerous models based on
this architecture have come out thereafter. For instance, Cho et al. (2014b) used gated RNNs
as both the encoder and decoder, while in the later work (Cho et al., 2014a), they proposed
a CNN-based decoder. In Sutskever et al. (2014), they proposed a deep LSTM for both
the encoder and decoder. Bahdanau et al. (2015) ﬁrst introduced the attention mechanism,
which was further developed in the well-known transformer networks (Vaswani et al., 2017).
However, most of the research on encoder-decoder architectures focused on applications. A
theoretical understanding is helpful for its further improvement and development."
RELATED WORK,0.015590200445434299,"From the theoretical point of view, Ye & Sung (2019) studied several theoretical properties of
CNN encoder-decoders, including expressiveness, generalisation capability and optimisation
landscape. Of particular relevance to the current work is expressiveness, which considers
the relationships that can be generated from the architecture. However, this is not approx-
imation. Yun et al. (2020) proved the universal approximation property of transformers for
certain classes of functions, for example, permutation equivariant functions, but they did not
consider the actual dynamical properties of target relationships that aﬀect approximation.
Dynamical proprieties such as memory, smoothness and low rank structures are essential,
because they can precisely characterise diﬀerent temporal relationships and aﬀect the ap-
proximation capabilities of models. Assuming the target generated from a hidden dynamical
system is one approach, which is widely applied (Maass et al., 2007; Sch¨afer & Zimmermann,
2007; Doya, 1993; Funahashi & Nakamura, 1993). In contrast, a functional-based approach is"
RELATED WORK,0.01670378619153675,Published as a conference paper at ICLR 2022
RELATED WORK,0.017817371937639197,"recently introduced, where the target temporal relationships are generated from functionals
satisfying speciﬁc properties such as linearity, continuity, regularity and time-homogeneity
(Li et al., 2021). In Li et al. (2021), the approximation properties of linear RNN models
are studied, and the results therein show that the approximation eﬃciency is related to the
memory structure. In Jiang et al. (2021), similar formulations are applied to investigate
convolutional architectures, where the results suggest that targets with certain spectrum
regularity can be well approximated by dilated CNNs. Under this framework, the target
temporal relationship that can be eﬃciently approximated is characterised by properties
such as memory, smoothness and sparsity. This enables us to make precise mathemati-
cal comparisons between diﬀerent architectures. Our results in this work reveal that the
encoder-decoders have a special temporal product structure which is intrinsically diﬀerent
from other sequence modelling architectures."
PROBLEM FORMULATION,0.01893095768374165,"3
Problem formulation"
PROBLEM FORMULATION,0.0200445434298441,"In this section, we precisely deﬁne the input space, output space, concept space and hy-
pothesis space, respectively."
PROBLEM FORMULATION,0.021158129175946547,"Functional formulation of temporal modelling.
First, we deﬁne the input and output
space precisely. A temporal sequence can be viewed as a function of time t. The input space
is deﬁned by X = C0((−∞, 0], Rd). This is the space of continuous functions from (−∞, 0]
to Rd vanishing at inﬁnity, where d ∈N+ is the dimension. Denote the element in X by
x = {xt ∈Rd : t ≤0}, we equip X with the supremum norm ∥x∥X := supt≤0 ∥xt∥∞. We
take the outputs space as Y = Cb([0, ∞), R), the space of bounded continuous functions
from [0, ∞) to R. We consider real-valued outputs, since each dimension can be handled
individually for vector-valued outputs."
PROBLEM FORMULATION,0.022271714922048998,"The mapping between input and output sequences can be formulated as a sequence of
functionals, i.e. yt = Ht(x), t ≥0. The output yt at the time step t depends on the input
sequence x. The ground truth relation between inputs and outputs is formulated by the
sequence of functionals H := {Ht : t ≥0}."
PROBLEM FORMULATION,0.02338530066815145,"We provide an example to illustrate the above formulation. Given an input x, the output
y is a smoothed version of x, resulting from convolving x with the Gaussian kernel g(s) =
1
√"
PROBLEM FORMULATION,0.024498886414253896,2π exp(−s2
PROBLEM FORMULATION,0.025612472160356347,"2 ). This relation can be formulated as yt = Ht(x) =
R ∞
0
g(t + s)x−sds."
PROBLEM FORMULATION,0.026726057906458798,"The RNN encoder-decoder model.
For the supervised learning problem, our goal is
to use a model to learn the target relationship H. First, we deﬁne the model. Among all
diﬀerent variants of the encoder-decoder architectures, the RNN encoder-decoder introduced
in Cho et al. (2014b) can be considered as the most simple and representative model, where
the encoder and decoder are both RNNs.
We study this particular model as we try to
eliminate other factors and only focus on the encoder-decoder architecture itself."
PROBLEM FORMULATION,0.02783964365256125,"Under our setting, the simpliﬁed model of Cho et al. (2014b) with RNNs as both encoder
and decoder can be formulated as
hs = σE(WEhs−1 + UExs + bE),
v = hτ,
gt = σD(WDgt−1 + bD),
g0 = v,
ot = WOgt + bO,
(1)"
PROBLEM FORMULATION,0.028953229398663696,"where ht, gt are hidden states of the encoder and decoder respectively. Recurrent activation
functions are denoted by σE and σD. Here, τ denotes the terminating time step of the
encoder, and v is the summary of the input sequence, which is called as the coding vector.
The model prediction is denoted as ot ∈R. All the other notations are model parameters.
Equation (1) describes the following model dynamics. First, the encoder reads the entire
input x, and then summarises the input into a ﬁxed size coding vector v, which is also the
last hidden state of the encoder. Next, the coding vector is passed into the decoder as the
initial state, and then the decoder produces an output at each time step. Note that the
encoder has a terminating time, and the decoder has a starting time. This is the reason
why we take the input and output as semi-inﬁnite sequences."
PROBLEM FORMULATION,0.030066815144766147,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.031180400890868598,"We study a linear, residual and continuous-time idealisation of the model dynamics (1):"
PROBLEM FORMULATION,0.03229398663697105,"˙hs = Whs + Uxs,
v = Qh0,
s ≤0
˙gt = V gt,
g0 = Pv,"
PROBLEM FORMULATION,0.0334075723830735,"ot = c⊤gt,
t ≥0, (2)"
PROBLEM FORMULATION,0.034521158129175944,"where W ∈RmE×mE, U ∈RmE×d, Q ∈RN×mE, V ∈RmD×mD, P ∈RmD×N and c ∈RmD
are parameters. mE and mD denote the width of encoder and decoder, respectively. The
coding vector v has dimension N, where we apply linear transformations to control it. We
assume h−∞= 0, which is the usual choice for the initial condition of RNN hidden states."
PROBLEM FORMULATION,0.035634743875278395,"Since our goal is to investigate approximation problems over large time horizons, we are
supposed to consider the stable RNN encoder-decoders, where"
PROBLEM FORMULATION,0.036748329621380846,"W ∈WmE := {W ∈RmE×mE : eigenvalues of W have negative real parts},
(3)"
PROBLEM FORMULATION,0.0378619153674833,"V ∈VmD := {V ∈RmD×mD : eigenvalues of V have negative real parts}.
(4)"
PROBLEM FORMULATION,0.03897550111358575,"The hypothesis space of RNN encoder-decoder models with arbitrary widths and coding
vector dimension is deﬁned as ˆ
H := S"
PROBLEM FORMULATION,0.0400890868596882,"mE,mD,N∈N+ˆ
HmE,mD,N, where"
PROBLEM FORMULATION,0.04120267260579064,"ˆ
HmE,mD,N :=
"
PROBLEM FORMULATION,0.042316258351893093,"ˆ
H := {ˆ
Ht : t ≥0} : ˆ
Ht(x) = c⊤eV tP
Z ∞"
PROBLEM FORMULATION,0.043429844097995544,"0
QeW sUx−sds, with"
PROBLEM FORMULATION,0.044543429844097995,"(W, U, Q, V, P, c) ∈WmE × RmE×d × RN×mE × VmD × RmD×N × RmD

.
(5)"
PROBLEM FORMULATION,0.045657015590200446,"The widths mE, mD and the coding vector dimension N together control the capac-
ity/complexity of the hypothesis space. Note that the assumptions on eigenvalues of W
and V ensure that the parameterized linear functionals are continuous."
PROBLEM FORMULATION,0.0467706013363029,"Due to the mathematical form (5), not all functionals can be represented by RNN encoder-
decoders. To achieve a good approximation, the target functionals must possess certain
structures. We introduce the following deﬁnitions to clarify these structures.
Deﬁnition 3.1. Let H = {Ht : t ≥0} be a sequence of functionals."
PROBLEM FORMULATION,0.04788418708240535,"1. For any t ≥0, the functional Ht is linear and continuous if for any λ1, λ2 ∈R
and x1, x2 ∈X, we have Ht(λ1x1 + λ2x2) = λ1Ht(x1) + λ2Ht(x2), and ∥Ht∥:=
supx∈X,∥x∥X ≤1 |Ht(x)| < ∞, where ∥Ht∥denotes the induced functional norm."
PROBLEM FORMULATION,0.04899777282850779,"2. For any t ≥0, the functional Ht is regular if for any sequence {x(n)}∞
n=1 ⊂X
such that limn→∞x(n)
s
= 0 for almost every s ≤0 (Lebesgue measure), we have
limn→∞Ht(x(n)) = 0."
PROBLEM FORMULATION,0.05011135857461024,"For a sequence of functionals H, we deﬁne its norm by ∥H∥:=
Z ∞"
PROBLEM FORMULATION,0.051224944320712694,"0
∥Ht∥dt."
PROBLEM FORMULATION,0.052338530066815145,"Remark 3.1. The deﬁnitions of linear and continuous functionals are standard. One can
view regular functionals as those not determined by inputs on arbitrarily small time intervals,
e.g. an inﬁnitely thin spike (i.e. δ-functions)."
PROBLEM FORMULATION,0.053452115812917596,"Given the above deﬁnitions, we immediately have the following observation."
PROBLEM FORMULATION,0.05456570155902005,"Proposition 3.1. Let ˆ
H ∈ˆ
H be a sequence of functionals in the RNN encoder-decoder
hypothesis space (see (5)). Then for any t ≥0, ˆ
Ht ∈ˆ
H is a linear, continuous and regular
functional. Furthermore, ∥ˆ
Ht∥decays exponentially as a function of t."
PROBLEM FORMULATION,0.0556792873051225,"The proof is found in Appendix A. This proposition characterises properties of the encoder-
decoder hypothesis space.
In particular, it is diﬀerent from the RNN hypothesis space
discussed in Li et al. (2021), since the encoder-decoder is not necessarily time-homogeneous.
Remark 3.2. A sequence of functionals H is time-homogeneous if for any t, τ ≥0, Ht(x) =
Ht+τ(x(τ)), with x(τ)s = xs−τ for all s ∈R. That is, if the input is shifted to the right by"
PROBLEM FORMULATION,0.05679287305122494,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.05790645879732739,"τ, the output is also shifted by τ. Temporal convolution is an example of time-homogeneous
operation (recall the Gaussian convolution discussed in Section 3.
An example of time-
inhomogeneous relationship is video captioning: shifts in the sequence of input video frames
do not necessarily lead to corresponding shifts in the caption text sequence."
PROBLEM FORMULATION,0.05902004454342984,"Relation with RNNs.
Here, we emphasise the diﬀerences between the encoder-decoder
hypothesis space and the RNN hypothesis space discussed in Li et al. (2021), where
ˆ
H(RNN)
t
(x) =
R ∞
0
c⊤eW (t+s)Ux−sds. A key diﬀerence is that the encoder-decoder has a
structure involving two temporal parameters t and s, while the RNN only has one depend-
ing on t + s, due to the time-homogeneity.
Owing to this diﬀerence and the fact that
ˆ
H(RNN) ⊂ˆ
H, the encoder-decoder hypothesis space (5) is more general, with the extra ca-
pability to learn time-inhomogeneous relationships. Furthermore, eV t and eW s adapt to a
temporal product structure, which is an intrinsic diﬀerence between encoder-decoders and
other architectures. We will discuss this in detail in the next section."
APPROXIMATION RESULTS,0.060133630289532294,"4
Approximation results"
APPROXIMATION RESULTS,0.061247216035634745,"One of the most fundamental problems for supervised learning is the approximation prob-
lem. It basically concerns the capacity of the hypothesis space to ﬁt the concept space. In
general, there are two levels of approximation problems that can be discussed. The ﬁrst is
known as the universal approximation, which considers the density of the hypothesis space
in the concept space. The second is the approximation rate, which aims to characterise
quantitatively the approximation accuracy concerning the capacity/complexity of the hy-
pothesis space (e.g. the number of trainable parameters). In this section, both of them are
developed for RNN encoder-decoders."
UNIVERSAL APPROXIMATION,0.062360801781737196,"4.1
Universal approximation"
UNIVERSAL APPROXIMATION,0.06347438752783964,"We ﬁrst present the most basic density result, which states that any linear, continuous,
and regular temporal relationship can be approximated by RNN encoder-decoders up to
arbitrary accuracy. The proof is found in Appendix B.
Theorem 4.1. Let H be a sequence of linear, continuous and regular functionals deﬁned
on X, and satisfy ∥H∥< ∞. Then for any ϵ > 0, there exists ˆ
H ∈ˆ
H such that"
UNIVERSAL APPROXIMATION,0.0645879732739421,"∥H −ˆ
H∥≡
Z ∞"
UNIVERSAL APPROXIMATION,0.06570155902004454,"0
∥Ht −ˆ
Ht∥dt < ϵ.
(6)"
UNIVERSAL APPROXIMATION,0.066815144766147,"Here, we highlight two important observations while deriving Theorem 4.1.
First, one
can show that each sequence of functionals H ∈H can be associated with a unique two-
parameter “representation” ρ(t, s), such that Ht(x) =
R ∞
0
x⊤
−sρ(t, s)ds. Recall the model
form ˆ
Ht(x) =
R ∞
0
x⊤
−sˆρ(t, s)ds, where ˆρ(t, s) := [c⊤eV tPQeW sU]⊤denotes the correspond-
ing representation. The functional approximation is then reduced to function approximation
in the sense of representations, i.e. ∥H −ˆ
H∥≤∥ρ−ˆρ∥L1([0,∞)2). It turns out that ρ directly
aﬀects the rate of approximation and gives rise to intrinsic properties. We will discuss this
in detail in Section 4.3. In addition, we again emphasise the diﬀerences between the present
work and Li et al. (2021). In Li et al. (2021), the target relationships are assumed to be
time-homogeneous with the representation Ht(x) =
R ∞
0
ρ(t + s)x−sds, which only depends
on t+s. However, the setting here does not assume time-homogeneity, hence implies a more
general representation ρ depending on the two temporal directions t and s simultaneously."
GENERAL APPROXIMATION RATES,0.06792873051224944,"4.2
General approximation rates"
GENERAL APPROXIMATION RATES,0.06904231625835189,"While the density result (Theorem 4.1) ensures the universal approximation property of the
RNN encoder-decoder, it does not identify targets that can be eﬃciently approximated. To
achieve this, we focus on approximation rates next. We characterise the temporal structure
of a target relationship by observing its responses to “constant” input signals. Here, we
consider the approximation rates for the model with “large size” coding vector, where the"
GENERAL APPROXIMATION RATES,0.07015590200445435,Published as a conference paper at ICLR 2022
GENERAL APPROXIMATION RATES,0.07126948775055679,"dimension N ≥¯m := min{mE, mD}. This is the scenario where we ﬁx the widths but take
an oversized coding vector.
Theorem 4.2. Let H be a sequence of linear, continuous and regular functionals deﬁned
on X, and satisfy ∥H∥< ∞. Consider the output of piece-wise constant signals yc
i (t, s) =
Ht(ei1(−∞,−s]), t, s ≥0, i = 1, 2, . . . , d, where {ei}d
i=1 denotes the standard basis of Rd.
Assume that there exist α ∈N+, β > 0 such that for any i = 1, 2, . . . , d,"
GENERAL APPROXIMATION RATES,0.07238307349665925,"yc
i ∈Cα+1([0, ∞)2),
(7)"
GENERAL APPROXIMATION RATES,0.07349665924276169,eβ(t+s) ∂k+l
GENERAL APPROXIMATION RATES,0.07461024498886415,"∂tk∂sl yc
i (t, s) = o(1) as ∥(t, s)∥→∞,
(k, l) ∈N × N+, k + l ≤α + 1.
(8)"
GENERAL APPROXIMATION RATES,0.0757238307349666,"Then for any mE, mD, N ∈N+, there exists ˆ
H ∈ˆ
HmE,mD,N such that"
GENERAL APPROXIMATION RATES,0.07683741648106904,"∥H −ˆ
H∥≤C(α)γd β2  1"
GENERAL APPROXIMATION RATES,0.0779510022271715,"mα
E
+
1
mα
D"
GENERAL APPROXIMATION RATES,0.07906458797327394,"
,
(9)"
GENERAL APPROXIMATION RATES,0.0801781737193764,"where C(α), γ > 0 are both universal constants with dependence only on α and (α, β),"
GENERAL APPROXIMATION RATES,0.08129175946547884,"respectively, and γ :=
max
i∈N+, i≤d
max
k,l∈N, k+l≤α+1 sup
t,s≥0
β−(k+l)eβ(t+s)  ∂k+l"
GENERAL APPROXIMATION RATES,0.08240534521158129,"∂tk∂sl yc
i (t, s)
 < ∞. Here,"
GENERAL APPROXIMATION RATES,0.08351893095768374,the number of trainable parameters is dN(mE + mD) with N ≥¯m.
GENERAL APPROXIMATION RATES,0.08463251670378619,"The proof is found in Appendix C. First, note that the error bound does not depend on the
coding vector size N, as long as N ≥¯m. This is because further increasing N beyond ¯m
only increases the number of trainable parameters, but does not increase the model capacity
(see Remark C.2). Only the model widths mE, mD aﬀect the approximation capabilities."
GENERAL APPROXIMATION RATES,0.08574610244988864,"Next, we focus on the classes of target relationships that can be well approximated. Here,
α characterises the smoothness of H, and β characterises the temporal decay rates of the
output responding to a constant signal under H. This is a notion of memory in the target
relationship. The error bound (9) indicates that a sequence of target functionals can be
eﬃciently approximated by the encoder-decoder if it is smooth (large α), and has fast
decayed memory (large β)."
GENERAL APPROXIMATION RATES,0.08685968819599109,"The characterisation in smoothness and memory decay also appears in the approxima-
tion results of RNNs (Li et al., 2021), where the upper bound is C(α)γd"
GENERAL APPROXIMATION RATES,0.08797327394209355,"βmα . However, our
results for encoder-decoders suggest extra structures, where the bound involves two (in-
stead of one) temporal parameters together with smoothness and decay memories in both.
The two-parameter temporal dependence allows the encoder-decoder to approximate time-
inhomogeneous relationships, which generalises the RNN. This two-parameter structure
further leads to adaptation to a speciﬁc low rank type of target relationships, resulting in
ﬁner approximation rates as we discuss next."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.08908685968819599,"4.3
Approximation rates via temporal product structure"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09020044543429843,"Motivation of temporal product structure.
In contrast with Theorem 4.2, we next
consider the model with N < ¯m = min{mE, mD}. In this situation, the model has fewer
parameters, and we aim to characterise the target relationships by further exploiting the
structure of the two-parameter representation ρ(t, s). This leads to a ﬁner approximation
rate by considering mE, mD, N together."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09131403118040089,"We ﬁrst motivate how the “temporal product structure” arises, and how it relates to the
approximation. Detailed discussions and proofs are found in Appendix D. For the illustration
purpose, we set the input dimension d = 1. Recall Q ∈RN×mE, P ∈RmD×N, then the
representation ˆρ of the encoder-decoder functional can be rewritten as"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09242761692650334,"ˆρ(t, s) = c⊤eV tP · QeW su = N
X n=1 mD
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.0935412026726058,"i,j=1
ciPjn

eV t ij"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09465478841870824,"! mE
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.0957683741648107,"i,j=1
uiQnj

eW s ji ! = N
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09688195991091314,"n=1
ˆϕn(t)ˆφn(s).
(10)"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09799554565701558,Published as a conference paper at ICLR 2022
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.09910913140311804,"This is a tensor product structure over the (t, s) time domain (determined by the encoder
{ˆφn} and decoder { ˆϕn} successively).
We call it the temporal product structure.
As is
shown later, this structure signiﬁcantly aﬀects approximation rates. When {ˆφn} and { ˆϕn}
are selected as the “bases” along s, t direction, respectively, N is considered as the rank of
the temporal product. We also deﬁne N as the rank of the model, which is understood as
the maximum rank of temporal products that the encoder-decoder model can represent."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10022271714922049,"The rank concept of temporal relationships.
Recall that the given number of train-
able parameters is dN(mE + mD). Hence, a low rank model may achieve fewer trainable
parameters. When investigating relationships that can be well approximated by low rank
models, a natural conjecture would be “low rank” targets."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10133630289532294,"What is the meaning of “low rank” for a temporal relationship? It is well-known that in linear
algebra, an operator is low rank means that its range space is low-dimensional. This idea
can be also applied to temporal relationships. For a “low rank” temporal relationship, the
output sequence is more “regular”, meaning that the output sequences (viewed as functions)
are in a low-dimensional function space. We provide an intuitive numerical illustration for
better understanding."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10244988864142539,"0
5
10
15
20
25
30
t −3 −2 −1 0 1 2 xt"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10356347438752785,"0
5
10
15
20
25
30
t −2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 2.0 Ht(x)"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10467706013363029,(a) high rank relationship
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10579064587973273,"0
5
10
15
20
25
30
t −3 −2 −1 0 1 2 xt"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10690423162583519,"0
5
10
15
20
25
30
t −10 0 10 20 30 40 Ht(x)"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.10801781737193764,(b) low rank relationship
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1091314031180401,"Figure 1: We construct a high rank and a low rank target from the temporal product. For
both (1a) and (1b), we plot the inputs xt together with the corresponding outputs Ht(x).
Detailed settings are found in Appendix E.1."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11024498886414254,"Figure 1 shows the outputs of a high rank (a) and a low rank (b) target relationship on
the same set of random input sequences. Diﬀerent colours refer to diﬀerent instances of
inputs. In the ﬁrst case (high rank), the temporal structure of the outputs is very complex
and depends sensitively on the inputs. However, in the second case (low rank), the output
sequences are much more regular, and only macroscopic structures (e.g. scale/oﬀset) appear."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.111358574610245,"Remark 4.1. In the research of approximation theories for temporal sequences, prior works
also related a notion of rank to approximation properties of the dilated convolutional structure
(Jiang et al., 2021). Here, we emphasise that the notion of rank considered in our work is
very diﬀerent from that in Jiang et al. (2021), which mainly concerns the tensorisation of
a discrete-time sequence according to the width of convolution ﬁlters."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11247216035634744,"POD as an analogue of SVD.
Now, we characterise low rank and high rank temporal
relationships in a mathematical way. We will introduce the concepts informally, and rigorous
deﬁnitions and arguments can be found in Appendix D."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11358574610244988,"For a matrix, we can assess its rank by performing the singular value decomposition
(SVD). This method can be extended to the temporal relationships using proper orthogo-
nal decomposition (POD; Liang et al. (2002), Berkooz et al. (1993), Chatterjee (2000)).
The basic insight is that the function ρ can be decomposed into the following form:
ρ(t, s) = PN0
n=1 σnϕn(t)φn(s), where N0 ≤∞, {ϕn} and {φn} are orthonormal bases, and
σ1 ≥σ2 ≥· · · ≥0 denote the singular values. This procedure can be viewed as apply-
ing SVD to an inﬁnite-dimensional space (when N0 = ∞). An analogue of Eckart–Young
theorem (Eckart & Young, 1936), which characterises the best low rank approximation,
also holds for POD. It roughly states that infrank(ˆρ)=N∥ρ −ˆρ∥2
L2 = PN0
n=N+1 σ2
n. That is,"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11469933184855234,Published as a conference paper at ICLR 2022
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11581291759465479,"any target ρ has a rank-N best approximation, with error equalling to the tail sum of the
squared singular values. In other words, a target with fast decayed σn (low “eﬀective rank”)
has smaller approximation errors. This forms the basis of our next result, which states that
if the target relationship possesses an eﬀective low rank structure in terms of the decay
of singular values, then one can achieve an eﬃcient approximation using encoder-decoder
structures by limiting the size of coding vectors. Detailed deﬁnitions for {σn} and proofs
are found in Appendix D."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11692650334075724,"Theorem 4.3. Assume the same conditions as in Theorem 4.2. Then for any mE, mD, N ∈
N+ with N ≤¯m, there exists ˆ
H ∈ˆ
HmE,mD,N such that"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11804008908685969,"∥H −ˆ
H∥≲C(α)γd β2"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.11915367483296214,"( 
1 +
√"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12026726057906459,"¯m −N

·
 1"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12138084632516703,"mα
E
+
1
mα
D 
+ ¯m
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12249443207126949,"n=N+1
σ2
n !1/2 + ¯m
X"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12360801781737193,"n=N+1
σn !1/2 · 1"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12472160356347439,"mα/2
E
+
1"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12583518930957685,"mα/2
D ! )"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12694877505567928,",
(11)"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.12806236080178174,"where ≲hides universal positive constants, and ¯m = min{mE, mD}. Here, the number of
trainable parameters is dN(mE + mD) with N ≤¯m."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1291759465478842,"This is a ﬁner approximation rate compared to Theorem 4.2, where both the widths mE, mD
and the coding vector size N aﬀect the model capacity for approximation.
Besides the
smoothness and memory decay, we have the additional rank structure of the target rela-
tionship, which is characterised by its singular values {σn}. We again focus on the class of
functionals that can be well approximated. Smoothness α and decay rate β is the same as
Theorem 4.2. The diﬀerence lies in the rank structure indicated by {σn}: the error bound is
small if {σn} has a small tail P ¯m
n=N+1 σ2
n. It suggests that a target with fast decayed {σn}
or low “eﬀective rank” can be well approximated by the RNN encoder-decoder with fewer
parameters. Due to the Eckart–Young-like low rank approximation, we can appropriately
select N based on the decay rate of singular values."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13028953229398663,"Here, we emphasise that the temporal product is an intrinsic structure arising from the
encoder-decoder architecture. Recall the dynamics of the encoder-decoder: it ﬁrst encodes
the input sequence into a coding vector, and then decodes an entire output sequence from
it. In this sense, the coding vector is the only interaction between the input and output.
Thus, the coding vector size N is an essential measure of the model capacity concerning
the dependence of outputs on inputs. Here, we show that this concept can be formalised
as a notion of rank, which can pinpoint the precise types of input-output relationships that
encoder-decoder architectures are well adapted to."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13140311804008908,"Numerical illustrations.
Here, we utilise numerical examples to illustrate the above
discussions. We observe how the decay rate of singular values, the rank N0 of the target
relationships, and the model rank N aﬀect the approximation error ∥H −ˆ
H∥. However,
it is not always possible to construct the best approximation. Instead, we perform some
training steps to achieve an upper bound of the approximation error, which is consistent
with our theoretical results."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13251670378619154,"In Figure 2, we train linear encoder-decoder models to learn three relationships of diﬀerent
ranks determined by various decay patterns of singular values, given in (a), (b) and (c).
Diﬀerent colours denote targets with diﬀerent ranks. From Figure 2, we have the following
observations consistent with previous discussions. First, observe that increasing the model
rank N makes approximation errors smaller, as expected. Moreover, note that when increas-
ing N, the speeds of error decrements are diﬀerent. If the singular values decay fast, the
approximation errors also decay fast. This implies that a target with fast decayed singular
values can be approximated eﬃciently with fewer parameters (smaller N). In addition, for
each experiment, we are able to achieve low approximation errors by choosing N ≪m. The
errors will remain unchanged or decrease much more slowly when further increasing N. This
suggests that in practice, one can choose N such that it covers the major singular values of
the target in order to improve the approximation eﬃciency."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.133630289532294,Published as a conference paper at ICLR 2022
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13474387527839643,"2
4
6
8
10
N 0.0 0.3 0.5 0.7 0.9 1.1 Error ×10−2"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1358574610244989,Target Rank
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13697104677060135,"N0 = 2
N0 = 4
N0 = 6
N0 = 8"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13808463251670378,"(a) σn =

n−1"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.13919821826280623,"8 ,
n ≤N0
0,
n > N0"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1403118040089087,"2
4
6
8
10
N 0.0 0.8 1.3 1.8 2.3 2.8 Error ×10−3"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14142538975501115,Target Rank
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14253897550111358,"N0 = 2
N0 = 4
N0 = 6
N0 = 8"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14365256124721604,"(b) σn =

n−1,
n ≤N0
0,
n > N0"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1447661469933185,"0
10
20
30
N 0.0 0.2 0.4 0.6 0.8 1.0 Error ×10−3"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14587973273942093,Target Rank
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14699331848552338,N0 = ∞
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.14810690423162584,(c) σn = n−2
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1492204899777283,"Figure 2: In (a), (b), (c) we consider target relationships with diﬀerent singular values
indicated in the respective caption. For (a), (b) we also consider targets with diﬀerent rank,
where N0 = 2, 4, 6, 8. We use models with ﬁxed width m = mE = mD = 128 and coding
vector size N. Detailed settings are found in Appendix E.2."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15033407572383073,"In Figure 3, we perform experiments on the forced Lorentz 96 system (Lorenz, 1996), which
parameterises a high-dimensional and nonlinear relationship between input forcing and
model states. The parameters K, J in the Lorenz 96 system control the overall complexity
of the target (see Appendix E.3 for details). We use the RNN encoder-decoder with tanh
activations to learn this target. Although our theories are developed in the linear regime,
the low rank approximation phenomenon also appears in this nonlinear setting. The error
decrements saturate when increasing the coding vector size N beyond a threshold, suggest-
ing the existence of some implicit notion of “rank” of the target nonlinear functional. This
“rank” increases with the target complexity (mainly K)."
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.1514476614699332,"0
10
20
30
N 0 2 4 6 8 Error ×10−4"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15256124721603564,Target Parameters
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15367483296213807,"K = 1, J = 6
K = 5, J = 6
K = 10, J = 6
K = 20, J = 6"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15478841870824053,"0
10
20
30
N 0 2 4 6 Error ×10−4"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.155902004454343,Target Parameters
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15701559020044542,"K = 5, J = 5
K = 5, J = 15
K = 5, J = 25
K = 5, J = 100"
APPROXIMATION RATES VIA TEMPORAL PRODUCT STRUCTURE,0.15812917594654788,"Figure 3: K, J are the parameters of the Lorenz 96 system. They describe the number
of independent and coupled variables in the system, which can be viewed as a complexity
measure. Detailed settings are found in Appendix E.3."
CONCLUSION,0.15924276169265034,"5
Conclusion"
CONCLUSION,0.1603563474387528,"We theoretically study the approximation properties of the RNN encoder-decoder in a lin-
ear setting. We prove a universal approximation result for linear temporal relationships
utilising encoder-decoder architectures, and show that they generalise RNNs to the time-
inhomogeneous setting. Moreover, we discover an important temporal product structure
that characterises the types of input-output relationships especially suited for the eﬃcient
approximation using encoder-decoders. This elucidates the key diﬀerences between these
novel architectures and classical methods for temporal modelling, and forms a basic step
towards understanding the intricacies of modern deep learning."
CONCLUSION,0.16146993318485522,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.16258351893095768,"Reproducibility statements.
Detailed proofs for theoretical results, and complete set-
tings of numerical examples are found in the appendix. The source code for numerical tests
can be made available upon request."
REPRODUCIBILITY STATEMENT,0.16369710467706014,Here is a quick reference:
REPRODUCIBILITY STATEMENT,0.16481069042316257,"Proposition 3.1
Properties of RNN encoder-decoder functionals
Appendix A
Theorem 4.1
Universal approximation theorem
Appendix B
Theorem 4.2
General approximation rates
Appendix C
Theorem 4.3
Approximation rates concerning temporal product structure
Appendix D
Figure 1
Illustration of high/low rank temporal relationships
Appendix E.1
Figure 2
Numerical examples on singular values
Appendix E.2
Figure 3
Numerical examples on Lorenz 96 systems
Appendix E.3"
REPRODUCIBILITY STATEMENT,0.16592427616926503,Acknowledgements
REPRODUCIBILITY STATEMENT,0.16703786191536749,"ZL is supported by Peking University under BICMR mathematical scholarship. HJ is sup-
ported by National University of Singapore under PGF scholarship. QL is supported by the
National Research Foundation, Singapore, under the NRF fellowship (NRF-NRFF13-2021-
0005)."
REPRODUCIBILITY STATEMENT,0.16815144766146994,Published as a conference paper at ICLR 2022
REFERENCES,0.16926503340757237,References
REFERENCES,0.17037861915367483,"Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. In International Conference on Learning Repre-
sentations, pp. 1–15, 2015."
REFERENCES,0.1714922048997773,"Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document trans-
former. arXiv preprint arXiv:2004.05150, 2020."
REFERENCES,0.17260579064587972,"G. Berkooz, P. Holmes, and J. L. Lumley. The proper orthogonal decomposition in the
analysis of turbulent ﬂows.
Annual Review of Fluid Mechanics, 25(1):539–575, 1993.
doi: 10.1146/annurev.ﬂ.25.010193.002543.
URL https://doi.org/10.1146/annurev.
fl.25.010193.002543."
REFERENCES,0.17371937639198218,"Vladimir I. Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007."
REFERENCES,0.17483296213808464,"Ching-Hua Chang and Chung-Wei Ha.
On eigenvalues of diﬀerentiable positive deﬁnite
kernels. Integral Equations and Operator Theory, 33:1–7, 1999. doi: 10.1007/BF01203078."
REFERENCES,0.1759465478841871,"Anindya Chatterjee. An introduction to the proper orthogonal decomposition. Current
Science, 78(7):808–817, 2000. ISSN 00113891. URL http://www.jstor.org/stable/
24103957."
REFERENCES,0.17706013363028952,"Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen,
Z. Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly,
Bo Li, Jan Chorowski, and Michiel Bacchiani. State-of-the-art speech recognition with
sequence-to-sequence models. In IEEE International Conference on Acoustics, Speech and
Signal Processing, pp. 4774–4778, 2018."
REFERENCES,0.17817371937639198,"Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio.
On
the properties of neural machine translation: Encoder–decoder approaches. In Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103–111.
Association for Computational Linguistics, 2014a.
doi: 10.3115/v1/W14-4012.
URL
https://aclanthology.org/W14-4012."
REFERENCES,0.17928730512249444,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using
RNN encoder-decoder for statistical machine translation. Conference on Empirical Meth-
ods in Natural Language Processing, pp. 1724–1734, 2014b. doi: 10.3115/v1/d14-1179."
REFERENCES,0.18040089086859687,"Kenji Doya. Universality of fully-connected recurrent neural networks. IEEE Transactions
on Neural Networks, 1993."
REFERENCES,0.18151447661469933,"Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.
Psychometrika, 1(3):211–218, 1936."
REFERENCES,0.18262806236080179,"Jeﬀrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990."
REFERENCES,0.18374164810690424,"Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by con-
tinuous time recurrent neural networks. Neural Networks, 6(6):801 – 806, 1993. ISSN
0893-6080."
REFERENCES,0.18485523385300667,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735–1780, 1997."
REFERENCES,0.18596881959910913,"Haotian Jiang, Zhong Li, and Qianxiao Li. Approximation theory of convolutional archi-
tectures for time series modelling. In Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4961–
4970. PMLR, 2021. URL https://proceedings.mlr.press/v139/jiang21d.html."
REFERENCES,0.1870824053452116,"Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Confer-
ence on Empirical Methods in Natural Language Processing, pp. 1700–1709, 2013. ISBN
9781937284978."
REFERENCES,0.18819599109131402,Published as a conference paper at ICLR 2022
REFERENCES,0.18930957683741648,"Peter D. Lax. Functional Analysis. John Wiley & Sons, Inc., 2002."
REFERENCES,0.19042316258351893,"Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis
with transformer network. In AAAI Conference on Artiﬁcial Intelligence, volume 33, pp.
6706–6713, 2019."
REFERENCES,0.1915367483296214,"Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. On the curse of memory in recurrent neu-
ral networks: Approximation and optimization analysis. In International Conference on
Learning Representations, 2021. URL https://openreview.net/forum?id=8Sqhl-nF50."
REFERENCES,0.19265033407572382,"Y. C. Liang, H. P. Lee, S. P. Lim, W. Z. Lin, K. H. Lee, and C. G. Wu. Proper orthogonal
decomposition and its applications—Part I: Theory. Journal of Sound and Vibration, 252
(3):527–544, 2002. ISSN 0022-460X. doi: https://doi.org/10.1006/jsvi.2001.4041. URL
https://www.sciencedirect.com/science/article/pii/S0022460X01940416."
REFERENCES,0.19376391982182628,"G. G. Lorentz. Approximation of Functions. AMS Chelsea Publishing Series. Holt, Rinehart
and Winston, 2005. ISBN 9780821840504. URL https://books.google.com.sg/books?
id=8VMrOmTKSe0C."
REFERENCES,0.19487750556792874,"Edward N. Lorenz. Predictability: A problem partly solved. In Proc. Seminar on Pre-
dictability, volume 1, pp. 40–58, 1996."
REFERENCES,0.19599109131403117,"Wolfgang Maass, Prashant Joshi, and Eduardo D. Sontag. Computational aspects of feed-
back in neural circuits. PLOS Computational Biology, 3(1):e165, 2007."
REFERENCES,0.19710467706013363,"Charles Bradﬁeld Morrey. Multiple Integrals in the Calculus of Variations. Springer-Verlag,
1966."
REFERENCES,0.19821826280623608,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexan-
der Ku, and Dustin Tran. Image transformer. In International Conference on Machine
Learning, pp. 4055–4064. PMLR, 2018."
REFERENCES,0.19933184855233854,"Walter Rudin. Real and Complex Analysis. Higher Mathematics Series. McGraw-Hill Edu-
cation, 1987. ISBN 9780070542341. URL https://books.google.com.sg/books?id=Z_
fuAAAAMAAJ."
REFERENCES,0.20044543429844097,"Anton Maximilian Sch¨afer and Hans-Georg Zimmermann. Recurrent neural networks are
universal approximators. International Journal of Neural Systems, 17(4):253–263, 2007."
REFERENCES,0.20155902004454343,"Martin H. Schultz. L∞-multivariate approximation theory. SIAM Journal on Numerical
Analysis, 6(2):161–183, 1969. doi: 10.1137/0706017. URL https://doi.org/10.1137/
0706017."
REFERENCES,0.2026726057906459,"Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems, volume 4, pp. 3104–
3112, 2014."
REFERENCES,0.20378619153674832,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez,  Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In Advances
in Neural Information Processing Systems, pp. 5999–6009, 2017."
REFERENCES,0.20489977728285078,"Subhashini Venugopalan, Marcus Rohrbach, Jeﬀrey Donahue, Raymond Mooney, Trevor
Darrell, and Kate Saenko. Sequence to sequence-video to text. In Proceedings of the
IEEE International Conference on Computer Vision, pp. 4534–4542, 2015."
REFERENCES,0.20601336302895323,"Jong Chul Ye and Woon Kyoung Sung.
Understanding geometry of encoder-decoder
CNNs. In International Conference on Machine Learning, pp. 12245–12254, 2019. ISBN
9781510886988."
REFERENCES,0.2071269487750557,"Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? In Interna-
tional Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=ByxRM0Ntvr."
REFERENCES,0.20824053452115812,Published as a conference paper at ICLR 2022
REFERENCES,0.20935412026726058,"A
Properties of model functionals"
REFERENCES,0.21046770601336304,"In this section, we prove observations of the hypothesis space reported in Proposition 3.1."
REFERENCES,0.21158129175946547,Proof of Proposition 3.1. Recall that
REFERENCES,0.21269487750556793,"ˆ
Ht(x; θ) =
Z ∞"
REFERENCES,0.21380846325167038,"0
c⊤eV tMeW sUx−sds.
(12)"
REFERENCES,0.21492204899777284,"Fix any θ = (W, V, U, M, c). The linearity is obvious. Since both W ∈WmE and V ∈VmD
have eigenvalues with negative real parts, there exist c1, c2, c′
1, c′
2 > 0, such that ∥eV t∥∞≤
c1e−c2t and ∥eW s∥∞≤c′
1e−c′
2s for any t, s ≥0, hence"
REFERENCES,0.21603563474387527,"|c⊤eV tMeW sUx−s| ≤∥c∥1∥eV tMeW sUx−s∥∞
≤∥c∥1∥eV t∥∞∥M∥∞∥eW s∥∞∥U∥∞∥x−s∥∞"
REFERENCES,0.21714922048997773,"≲e−c2te−c′
2s∥x∥X ,
(13)"
REFERENCES,0.2182628062360802,where ≲hides universal positive constants depending on parameters θ. Therefore
REFERENCES,0.21937639198218262,"|ˆ
Ht(x; θ)| ≲e−c2t∥x∥X ⇒∥ˆ
Ht(·; θ)∥≲e−c2t.
(14)"
REFERENCES,0.22048997772828507,"That is, the functional ˆ
Ht(·; θ) is bounded (i.e. continuous) with an exponentially-decayed
norm (as a function of t). Finally, by (13) and Lebesgue’s dominated convergence theorem,
the functional ˆ
Ht(·; θ) is also regular. The proof is completed."
REFERENCES,0.22160356347438753,"B
Universal approximation"
REFERENCES,0.22271714922049,"In this section, we provide the proof of Theorem 4.1, i.e.
the universal approximation
property of RNN encoder-decoders. As is stated in the main text, the key step is to utilise
the classical representation theorem, which helps us to reduce the approximation problem
of functionals to functions."
REFERENCES,0.22383073496659242,"B.1
Preliminaries"
REFERENCES,0.22494432071269488,"First, we list the background deﬁnitions and notations used in the following theorems. Let
(X, A) be a measure space (with A as the σ-algebra of subsets of X)."
REFERENCES,0.22605790645879734,"• The space X is called locally compact if for any x ∈X, x has a compact neighbour-
hood.
• X is a Hausdorﬀspace if all distinct points in X are pair-wisely separable by neigh-
bourhoods. That is, for any x, y ∈X, there exists a neighbourhood ∆x of x and a
neighbourhood ∆y of y, such that ∆x ∩∆y = ∅.
• The measure µ is called a ﬁnite measure, if it satisﬁes µ(X) < ∞. The measure
ν is called a σ-ﬁnite measure, if X can be covered with at most countably many
measurable sets with ﬁnite measure. That is, there are measurable sets {An}∞
n=1 ⊂
A with ν(An) < ∞for all n ∈N+, such that S∞
n=1 An = X. Obviously, a ﬁnite
measure is also σ-ﬁnite.
• Let X = (−∞, 0]. For a measure µ on the measure space ((−∞, 0], A), µ is absolutely
continuous with respect to the Lebesgue measure ν, if for every measurable set A,
ν(A) = 0 implies µ(A) = 0, which is written as µ ≪ν."
REFERENCES,0.22717149220489977,"Denote by C0(X) the linear space of continuous functions deﬁned on X vanishing at inﬁnity.
We have the following classical representation theorem.
Theorem B.1 (Riesz-Markov-Kakutani representation theorem). Let X be a locally com-
pact Hausdorﬀspace. For any continuous linear functional ψ on C0(X), there is a unique,
regular, countably additive and signed measure µ on X, such that"
REFERENCES,0.22828507795100222,"ψ(f) =
Z"
REFERENCES,0.22939866369710468,"X
f(x)dµ(x),
∀f ∈C0(X),
(15)"
REFERENCES,0.23051224944320714,Published as a conference paper at ICLR 2022
REFERENCES,0.23162583518930957,"with ∥ψ∥= |µ|(X). Here, |µ|(X) denotes the total variation of (the signed measure) µ,
which is deﬁned as |µ|(X) := supP
Pk
i=1 |µ(Ai)|, where P : X = Sk
i=1 Ai is a partition over
X with Ai ∈A for all i = 1, 2, · · · , k."
REFERENCES,0.23273942093541203,"Proof. Well-known, see e.g. Bogachev (2007) (CH 7.10.4)."
REFERENCES,0.23385300668151449,"Remark B.1. It is straightforward to verify that |µ|(X) = supA∈A(|µ(A)| + |µ(Ac)|). Fur-
thermore, if µ has a density dµ/dν with respect to a countably additive, nonnegative measure
ν, then we have |µ|(X) = ∥dµ/dν∥L1(ν)."
REFERENCES,0.23496659242761692,"To handle signed measures, the following Jordan decomposition theorem (Bogachev, 2007)
is necessary."
REFERENCES,0.23608017817371937,"Theorem B.2. Let µ be a signed measure on the measure space (X, A). Then, there are two
mutually singular (non-negative) measures µ+ and µ−on (X, A), such that µ = µ+ −µ−.
Moreover, such a pair (µ+, µ−) is unique."
REFERENCES,0.23719376391982183,"Based on this, we have the following proposition to characterise absolutely continuous signed
measures."
REFERENCES,0.2383073496659243,"Proposition B.1. If µ and ν are signed measures, then we have µ ≪ν ⇔µ+ ≪ν and
µ−≪ν."
REFERENCES,0.23942093541202672,"We also need the following Radon-Nikodym theorem (Bogachev, 2007)."
REFERENCES,0.24053452115812918,"Theorem B.3. Let (X, A, ν) be a σ-ﬁnite measure space, and let µ be a σ-ﬁnite signed
measure, such that µ ≪ν. Then there exists a unique measurable function f, such that
µ(A) =
R"
REFERENCES,0.24164810690423164,A fdν for every measurable set A.
REFERENCES,0.24276169265033407,"B.2
Proofs"
REFERENCES,0.24387527839643652,"Before we prove the universal approximation theorem (Theorem 4.1), we need some lemmas."
REFERENCES,0.24498886414253898,"Lemma B.1. Let {Ht : t ≥0} be a family of linear, continuous and regular functionals
deﬁned on X. Then there exists a integrable function ρ : [0, ∞)2 →Rd, i.e."
REFERENCES,0.24610244988864144,"∥ρ∥L1([0,∞)2) := d
X"
REFERENCES,0.24721603563474387,"i=1
∥ρi∥L1([0,∞)2) < ∞,
(16)"
REFERENCES,0.24832962138084633,such that
REFERENCES,0.24944320712694878,"Ht(x) =
Z ∞"
REFERENCES,0.2505567928730512,"0
x⊤
−sρ(t, s)ds,
∀x ∈X.
(17)"
REFERENCES,0.2516703786191537,"In particular, we have ∥H∥=
R ∞
0
∥Ht∥dt = ∥ρ∥L1([0,∞)2)."
REFERENCES,0.25278396436525613,"Proof. Obviously, (−∞, 0] is a locally compact Hausdorﬀspace.
For any t ≥0, since
Ht is linear continuous, according to the Riesz-Markov-Kakutani representation theorem
(Theorem B.1), there exists a unique, regular, countably additive and signed measure µt,
such that"
REFERENCES,0.25389755011135856,"Ht(x) =
Z 0"
REFERENCES,0.25501113585746105,"−∞
x⊤
s dµt(s),
∀x ∈X,
(18)"
REFERENCES,0.2561247216035635,"with Pd
i=1 |µt,i|((−∞, 0]) = ∥Ht∥.
We show that for any t ≥0, i = 1, 2, · · · , d, µt,i is
absolutely continuous with respect to ν (the Lebesgue measure), i.e. µt,i ≪ν. According
to Theorem B.2 and Proposition B.1, one can assume µt,i to be non-negative without loss
of generality.
Take a measurable set A ⊂(−∞, 0] with ν(A) = 0, the aim is to show
µt,i(A) = 0.
Let A′ = (−∞, 0] \ A.
Since both A and A′ are measurable, there exist
Kn ⊂A, K′
n ⊂A′ with Kn, K′
n closed, such that µt,i(A \ Kn) ≤1/n, µt,i(A′ \ K′
n) ≤1/n"
REFERENCES,0.2572383073496659,Published as a conference paper at ICLR 2022
REFERENCES,0.2583518930957684,"and ν(A′ \ K′
n) ≤1/n for any n ∈N+. Fix any i ∈{1, 2, . . . , d}, we construct the sequence
of input signals {x(n)}∞
n=1 as"
REFERENCES,0.2594654788418708,"x(n)
s,j = 
 "
REFERENCES,0.26057906458797325,"0,
s ≤0, j ̸= i,
0,
s ∈K′
n, j = i,
j = 1, 2, . . . , d,
1,
s ∈Kn, j = i,
(19)"
REFERENCES,0.26169265033407574,"which can then be continuously extended to (−∞, 0] by deﬁning x(n)
s,i :=
d(s,K′
n)
d(s,Kn)+d(s,K′n) ∈
[0, 1]. 1"
REFERENCES,0.26280623608017817,"We deduce that limn→∞x(n)
s,i = 0 for ν-a.e. s ≤0. In fact, let S := {s ≤0 : limn→∞x(n)
s,i ="
REFERENCES,0.2639198218262806,"0}, we have K′
n ⊂S since for any s ∈K′
n, x(n)
s,i = 0. Hence, (−∞, 0] \ S ⊂A ∪(A′ \ K′
n),
which gives ν((−∞, 0]\S) ≤ν(A)+ν(A′ \K′
n) ≤1/n →0 as n →∞. Due to the regularity
of Ht, we get limn→∞Ht(x(n)) = 0. By (18) and (19), we have"
REFERENCES,0.2650334075723831,"Ht(x(n)) = d
X j=1 Z 0"
REFERENCES,0.2661469933184855,"−∞
x(n)
s,j dµt,j(s) =
Z 0"
REFERENCES,0.267260579064588,"−∞
x(n)
s,i dµt,i(s) =
Z"
REFERENCES,0.26837416481069043,"Kn
+
Z"
REFERENCES,0.26948775055679286,"A\Kn
+
Z"
REFERENCES,0.27060133630289535,"A′\K′n
x(n)
s,i dµt,i(s) = µt,i(Kn) + I1,n + I2,n,
(20)"
REFERENCES,0.2717149220489978,"where µt,i(Kn)
=
µt,i(A) −µt,i(A \ Kn)
∈
[µt,i(A) −1/n, µt,i(A)],
and |I1,n| +
|I2,n| ≤
R"
REFERENCES,0.2728285077951002,"A\Kn +
R"
REFERENCES,0.2739420935412027,"A′\K′n 1dµt,i(s) = µt,i(A \ Kn) + µt,i(A′ \ K′
n) ≤2/n, which gives"
REFERENCES,0.2750556792873051,"limn→∞Ht(x(n)) = µt,i(A). Therefore, µt,i(A) = 0."
REFERENCES,0.27616926503340755,"Notice that |µt,i((−∞, 0])| ≤|µt,i|((−∞, 0]) ≤∥Ht∥< ∞for a.e. t ≥0 (since ∥H∥=
R ∞
0 ∥Ht∥dt < ∞), we get that ((−∞, 0], A, µt,i) is a ﬁnite measure space, and hence σ-ﬁnite.
Obviously, ((−∞, 0], A, ν) is a σ-ﬁnite measure space. According to the Radon-Nikodym
theorem (Theorem B.3), there exists a unique measurable function ρt,i : (−∞, 0] →R, such
that µt,i(A) =
R"
REFERENCES,0.27728285077951004,"A ρt,i(s)dν(s) for every measurable set A. Hence, we have"
REFERENCES,0.27839643652561247,"Ht(x) =
Z 0"
REFERENCES,0.2795100222717149,"−∞
x⊤
s ρt(s)ds =
Z ∞"
REFERENCES,0.2806236080178174,"0
x⊤
−sρ(t, s)ds,
∀x ∈X,
(21)"
REFERENCES,0.2817371937639198,"with ρ(t, s)
:=
ρt(−s).
In addition, by Remark B.1, we have |µt,i|((−∞, 0])
=
R 0
−∞|ρt,i(s)|ds, which gives"
REFERENCES,0.2828507795100223,"∥H∥=
Z ∞"
REFERENCES,0.28396436525612473,"0
∥Ht∥dt = d
X i=1 Z ∞"
REFERENCES,0.28507795100222716,"0
|µt,i|((−∞, 0])dt = d
X"
REFERENCES,0.28619153674832964,"i=1
∥ρi∥L1([0,∞)2) = ∥ρ∥L1([0,∞)2). (22)"
REFERENCES,0.2873051224944321,The proof is completed.
REFERENCES,0.2884187082405345,"Based on this representation theorem, the problem of functional approximation is reduced
as function approximation. That is,"
REFERENCES,0.289532293986637,"∥H −ˆ
H∥=
Z ∞"
REFERENCES,0.2906458797327394,"0
∥Ht −ˆ
Ht∥dt =
Z ∞"
SUP,0.29175946547884185,"0
sup
∥x∥X ≤1"
SUP,0.29287305122494434,"Ht(x) −ˆ
Ht(x; θ)
dt =
Z ∞"
SUP,0.29398663697104677,"0
sup
∥x∥X ≤1  Z ∞"
SUP,0.2951002227171492,"0
x⊤
−s(ρ(t, s) −ˆρ(t, s))ds
dt ≤
Z ∞"
SUP,0.2962138084632517,"0
sup
∥x∥X ≤1 Z ∞"
SUP,0.2973273942093541,"0
∥x−s∥∞∥ρ(t, s) −ˆρ(t, s)∥1dsdt ≤ d
X i=1 Z ∞ 0 Z ∞"
SUP,0.2984409799554566,"0
|ρi(t, s) −ˆρi(t, s)|dsdt,
(23)"
SUP,0.29955456570155903,"1Here, d(s, B) := inf{|s −a| : a ∈B} is the distance between a point s and a set B."
SUP,0.30066815144766146,Published as a conference paper at ICLR 2022 i.e.
SUP,0.30178173719376394,"∥H −ˆ
H∥≤∥ρ −ˆρ∥L1([0,∞)2) := d
X"
SUP,0.3028953229398664,"i=1
∥ρi −ˆρi∥L1([0,∞)2).
(24)"
SUP,0.3040089086859688,"Lemma B.2. Let ρ(t, s) : [0, ∞)2 →R with ∥ρ∥L1([0,∞)2) < ∞. Then for any ϵ > 0, there
exists a polynomial p(u, v) = Pm
j=1,k=1 cjkujvk, such that
Z ∞ 0 Z ∞"
SUP,0.3051224944320713,"0
|ρ(t, s) −p(e−t, e−s)|dtds < ϵ.
(25)"
SUP,0.3062360801781737,Proof. Fix any ϵ > 0. Consider the following transformation
SUP,0.30734966592427615,"R(u, v) =
 1"
SUP,0.30846325167037864,"uvρ(−ln u, −ln v),
u, v ∈(0, 1],
0,
uv = 0.
(26)"
SUP,0.30957683741648107,"This transformation preserves the norm with ∥ρ∥L1([0,∞)2) = ∥R∥L1([0,1]2)."
SUP,0.3106904231625835,"First, according to the density of continuous functions in Lp space (Rudin, 1987, Theorem
3.14), there exists ˜R ∈C([0, 1]2), such that ∥R −˜R∥L1([0,1]2) < ϵ/2. Next, by the density of
polynomials in the space of continuous functions (Lorentz, 2005, Theorem 6), there exists
a polynomial q(u, v) = Pm
j,k=0 cjkujvk, such that ∥˜R −q∥L∞([0,1]2) < ϵ/2.
Finally, let
p(u, v) = uvq(u, v), we have
Z ∞ 0 Z ∞"
SUP,0.311804008908686,"0
|ρ(t, s) −p(e−t, e−s)|dtds =
Z 1 0 Z 1 0"
SUP,0.3129175946547884,"R(u, v) −1"
SUP,0.31403118040089084,"uv p(u, v)
dudv"
SUP,0.3151447661469933,"= ∥R −q∥L1([0,1]2)
≤∥R −˜R∥L1([0,1]2) + ∥˜R −q∥L∞([0,1]2)
< ϵ/2 + ϵ/2 = ϵ,
(27)"
SUP,0.31625835189309576,which completes the proof.
SUP,0.31737193763919824,Now we are ready to prove the universal approximation theorem.
SUP,0.3184855233853007,"Proof of Theorem 4.1. According to the representation theorem (Lemma B.1), we have"
SUP,0.3195991091314031,"Ht(x) =
Z ∞"
SUP,0.3207126948775056,"0
x⊤
−sρ(t, s)ds,
∀x ∈X,
(28)"
SUP,0.321826280623608,"where ∥ρ∥L1([0,∞)2) = ∥H∥< ∞.
Therefore, by Lemma B.2, there exists pi(u, v) =
Pm
j,k=1 c(i)
jk ujvk, i = 1, 2, . . . , d, where m is the maximal degree of {pi}d
i=1, such that d
X i=1 Z ∞ 0 Z ∞"
SUP,0.32293986636971045,"0
|ρi(t, s) −pi(e−t, e−s)|dtds < ϵ.
(29) Let"
SUP,0.32405345211581293,"c = u = 1m, V = ˜W = −diag (1, 2, . . . , m),"
SUP,0.32516703786191536,"W = diag( ˜W, ˜W, · · · , ˜W) ∈Rdm×dm, U = diag(u, u, · · · , u) ∈Rdm×d,
(30)"
SUP,0.3262806236080178,"M = PQ = (M1, M2, · · · , Md) ∈Rm×dm, [Mi]jk = c(i)
jk ,"
SUP,0.3273942093541203,we get
SUP,0.3285077951002227,"ˆρ(t, s)⊤= c⊤eV tPQeW sU = c⊤eV tMeW sU"
SUP,0.32962138084632514,"= c⊤eV t · (M1, M2, · · · , Md) · diag(eW s, eW s, · · · , eW s) · diag(u, u, · · · , u)"
SUP,0.3307349665924276,"= (c⊤eV tM1, c⊤eV tM2, · · · , c⊤eV tMd) · diag(eW su, eW su, · · · , eW su)"
SUP,0.33184855233853006,"= (c⊤eV tM1eW su, c⊤eV tM2eW su, · · · , c⊤eV tMdeW su),
(31)"
SUP,0.33296213808463254,Published as a conference paper at ICLR 2022 with
SUP,0.33407572383073497,"ˆρi(t, s) = c⊤eV tMieW su = pi(e−t, e−s),
i = 1, 2, . . . , d.
(32)"
SUP,0.3351893095768374,"Therefore, by (24) and (29), we have"
SUP,0.3363028953229399,"∥H −ˆ
H∥≤ d
X"
SUP,0.3374164810690423,"i=1
∥ρi −ˆρi∥L1([0,∞)2) = d
X i=1 Z ∞ 0 Z ∞ 0"
SUP,0.33853006681514475,"ρi(t, s) −pi(e−t, e−s)
dsdt < ϵ,
(33)"
SUP,0.33964365256124723,which completes the proof.
SUP,0.34075723830734966,"C
General approximation rates"
SUP,0.3418708240534521,"In this section, the proof of Theorem 4.2 is given.
Again, by (24), the aim now is to
investigate the function approximation ∥ρ−ˆρ∥. Since one can handle each spatial dimension
separately (similarly with (30) and (31)), we ﬁrstly derive the estimates by assuming d = 1,
and then extend the obtained results to the case of multi-dimensional inputs (for general
d ∈N+)."
SUP,0.3429844097995546,"Conditions on representation.
To characterise the accuracy of using the model
c⊤eV tMeW su (with M := PQ) to approximate the target ρ(t, s), the ﬁrst stuﬀis to trans-
late the conditions on the output (of piece-wise constant signals) to the representation.
Recall that yc(t, s) = Ht(1(−∞,−s]), t, s ≥0, we get ρ(t, s) = −d"
SUP,0.344097995545657,"dsHt(1(−∞,−s]). Hence, the
assumptions on yc in Theorem 4.2 is equivalent to the following smoothness and exponential
decay conditions on ρ. That is, there exist α ∈N+, β > 0 such that"
SUP,0.34521158129175944,"ρ ∈Cα([0, +∞)2),
(34)"
SUP,0.3463251670378619,eβ(t+s) ∂k+l
SUP,0.34743875278396436,"∂tk∂sl ρ(t, s) = o(1) as ∥(t, s)∥→∞,
k, l ∈N, k + l ≤α.
(35)"
SUP,0.34855233853006684,Note that the last decay condition implies
SUP,0.34966592427616927,"sup
t,s≥0
β−(k+l)eβ(t+s)

∂k+l"
SUP,0.3507795100222717,"∂tk∂sl ρ(t, s)
 ≤γ,
k, l ∈N, k + l ≤α
(36)"
SUP,0.3518930957683742,for some γ > 0.
SUP,0.3530066815144766,"C.1
Basics"
SUP,0.35412026726057905,Let Ω∈Rd be a bounded set. Deﬁne the spaces
SUP,0.35523385300668153,"Cα(Ω) := {f ∈C(¯Ω) : Dif ∈C(¯Ω) for all |i| ≤α},
α ∈N,
(37)"
SUP,0.35634743875278396,"Cα,µ(Ω) := {f ∈Cα(Ω) : |Dif(x) −Dif(y)| ≤K∥x −y∥µ
2 for some K > 0,
for all x, y ∈Ωand |i| = α},
(38)"
SUP,0.3574610244988864,and the “norm”
SUP,0.3585746102449889,"|f|α,µ,Ω:= sup
|i|=α
sup
x,y∈Ω"
SUP,0.3596881959910913,|Dif(x) −Dif(y)|
SUP,0.36080178173719374,"∥x −y∥µ
2
,
∀f ∈Cα,µ(Ω),
(39)"
SUP,0.3619153674832962,"with the shorthand ∥· ∥Ω:= | · |0,0,Ω."
SUP,0.36302895322939865,"Theorem C.1 (Multivariate Jackson’s theorem (Schultz, 1969), Theorem 4.10). Let Ω∈Rd"
SUP,0.36414253897550114,"be a regular, 2 bounded and open set, and f ∈Cα,µ(¯Ω) for some α ∈N, µ ∈[0, 1]. Then for
any n ∈N+, we have"
SUP,0.36525612472160357,"inf
p∈Pd
n
∥f −p∥¯Ω≤C(α, µ)"
SUP,0.366369710467706,"nα+µ |f|α,µ,¯Ω,
(40)"
SUP,0.3674832962138085,"2It is proved that every bounded, open and convex set is regular. See Morrey (1966) (Lemma
3.4.1)."
SUP,0.3685968819599109,Published as a conference paper at ICLR 2022
SUP,0.36971046770601335,"where Pd
n denotes the set of all polynomials with the degree of no more than n in each
variable, C(α, µ) > 0 is a universal constant only depending on α, µ and Ω."
SUP,0.37082405345211583,"A commonly used case is when µ = 0. That is, for f ∈Cα(¯Ω), we get"
SUP,0.37193763919821826,"|f|α,0,¯Ω≤2 max
|i|=α ∥Dif∥L∞(¯Ω) < ∞.
(41)"
SUP,0.3730512249443207,"For any x, x0 ∈¯Ω, let ˜p(x) := p(x) + f(x0) −p(x0), and ˜e(x) := f(x) −˜p(x). Then ˜p ∈Pd
n,
and"
SUP,0.3741648106904232,|f(x) −˜p(x)| ≤|˜e(x0)| + |˜e(x) −˜e(x0)|
SUP,0.3752783964365256,"≤sup
x,y∈¯Ω
|˜e(x) −˜e(y)| = |˜e|0,0,¯Ω= ∥f −˜p∥¯Ω,
∀x ∈¯Ω.
(42)"
SUP,0.37639198218262804,This gives the following convenient corollary.
SUP,0.3775055679287305,"Corollary C.1. Let Ω∈Rd be a regular, bounded and open set, and f ∈Cα(¯Ω) for some
α ∈N. Then for any n ∈N+, there exists p ∈Pd
n such that"
SUP,0.37861915367483295,∥f −p∥L∞(¯Ω) ≤Cα
SUP,0.37973273942093544,"nα max
|i|=α ∥Dif∥L∞(¯Ω),
(43)"
SUP,0.38084632516703787,"where Pd
n denotes the set of all polynomials with the degree of no more than n in each
variable, Cα > 0 is a universal constant only depending on α and Ω."
SUP,0.3819599109131403,"C.2
Proofs"
SUP,0.3830734966592428,Now we are ready to present the proof.
SUP,0.3841870824053452,"Proof of Theorem 4.2. Step 1: domain transform. Consider the transform from the inﬁnite
domain [0, ∞)2 to the compact one [0, 1]2:"
SUP,0.38530066815144765,"R(u, v) =
 1"
SUP,0.38641425389755013,"uvρ(−c0 ln u, −c0 ln v),
u, v ∈(0, 1],
0,
uv = 0,
(44)"
SUP,0.38752783964365256,"where c0 := (α + 1)/β > 0 is a ﬁxed constant. A straightforward computation by induction
shows that, for any k, l ∈N, k + l ≤α, and any u, v ∈(0, 1], ∂k+l"
SUP,0.388641425389755,"∂uk∂vl R(u, v) = (−1)k+l"
SUP,0.3897550111358575,"uk+1vl+1 k
X i=0 l
X"
SUP,0.3908685968819599,"j=0
C(k, i)C′(l, j)ci+j
0
∂i+j"
SUP,0.39198218262806234,"∂ti∂sj ρ(−c0 ln u, −c0 ln v),
(45)"
SUP,0.3930957683741648,"where C(k, i), C′(l, j) are some integer constants, and (t, s) = (−c0 ln u, −c0 ln v) is a one-
to-one mapping between (0, 1]2 and [0, ∞)2. By (36), we get ∂k+l"
SUP,0.39420935412026725,"∂uk∂vl R(u, v)
 ≤
1
uk+1vl+1 k
X i=0 l
X"
SUP,0.39532293986636974,"j=0
|C(k, i)||C′(l, j)|ci+j
0 ∂i+j"
SUP,0.39643652561247217,"∂ti∂sj ρ(−c0 ln u, −c0 ln v)
 , ∂k+l"
SUP,0.3975501113585746,∂uk∂vl R(e−t
SUP,0.3986636971046771,"c0 , e−s"
SUP,0.3997772828507795,"c0 )
 ≤e (k+1) c0
te (l+1)"
SUP,0.40089086859688194,"c0
s
k
X i=0 l
X"
SUP,0.40200445434298443,"j=0
|C(k, i)||C′(l, j)|ci+j
0 ∂i+j"
SUP,0.40311804008908686,"∂ti∂sj ρ(t, s) ≤ k
X i=0 l
X"
SUP,0.4042316258351893,"j=0
|C(k, i)||C′(l, j)|(α + 1)i+j · β−(i+j)eβ(t+s)

∂i+j"
SUP,0.4053452115812918,"∂ti∂sj ρ(t, s) ≤ k
X i=0 l
X"
SUP,0.4064587973273942,"j=0
|C(k, i)||C′(l, j)|(α + 1)i+jγ ≤C(α)γ,
(46)"
SUP,0.40757238307349664,Published as a conference paper at ICLR 2022
SUP,0.4086859688195991,"where C(α) > 0 is a universal constant only depending on α.
According to the decay
condition (35), we get"
SUP,0.40979955456570155,"lim
(u,v)→(0,0)"
SUP,0.410913140311804,"∂i+j
∂ti∂sj ρ(−c0 ln u, −c0 ln v)"
SUP,0.41202672605790647,"uk+1vl+1
=
lim
(t,s)→(+∞,+∞) e (k+1) c0
te (l+1)"
SUP,0.4131403118040089,"c0
s ∂i+j"
SUP,0.4142538975501114,"∂ti∂sj ρ(t, s)"
SUP,0.4153674832962138,"=
lim
(t,s)→(+∞,+∞) e (k−α)"
SUP,0.41648106904231624,α+1 βte (l−α)
SUP,0.41759465478841873,α+1 βs · eβ(t+s) ∂i+j
SUP,0.41870824053452116,"∂ti∂sj ρ(t, s)"
SUP,0.4198218262806236,"= 0,
(47)"
SUP,0.4209354120267261,"and similarly for u0, v0 ∈(0, 1],"
SUP,0.4220489977728285,"lim
(u,v)→(u0,0)"
SUP,0.42316258351893093,"∂i+j
∂ti∂sj ρ(−c0 ln u, −c0 ln v)"
SUP,0.4242761692650334,"uk+1vl+1
=
lim
(t,s)→(−c0 ln u0,+∞) e (k+1) c0
te (l+1)"
SUP,0.42538975501113585,"c0
s ∂i+j"
SUP,0.4265033407572383,"∂ti∂sj ρ(t, s)"
SUP,0.42761692650334077,"=
lim
∥(t,s)∥→+∞e (k−α)"
SUP,0.4287305122494432,α+1 βte (l−α)
SUP,0.4298440979955457,α+1 βs · eβ(t+s) ∂i+j
SUP,0.4309576837416481,"∂ti∂sj ρ(t, s)"
SUP,0.43207126948775054,"= 0,
(48)"
SUP,0.43318485523385303,"lim
(u,v)→(0,v0)"
SUP,0.43429844097995546,"∂i+j
∂ti∂sj ρ(−c0 ln u, −c0 ln v)"
SUP,0.4354120267260579,"uk+1vl+1
= 0.
(49)"
SUP,0.4365256124721604,This gives ∂k+l
SUP,0.4376391982182628,"∂uk∂vl R(u, v) = 0,
(u, v) ∈[0, 1] × {0} ∪{0} × [0, 1],
(50) and"
SUP,0.43875278396436523,"M0 :=
max
k,l∈N, k+l≤α
max
(u,v)∈[0,1]2 ∂k+l"
SUP,0.4398663697104677,"∂uk∂vl R(u, v)
 ≤C(α)γ
(51)"
SUP,0.44097995545657015,"by (46) and (50). Hence, R(u, v) ∈Cα([0, 1]2) with bounded derivatives."
SUP,0.4420935412026726,"Step 2: polynomial approximation. According to Corollary C.1 and (51), we obtain that
there exists ˜Rn ∈P2
n, such that"
SUP,0.44320712694877507,"∥R −˜Rn∥L∞([0,1]2) ≤Cα"
SUP,0.4443207126948775,"nα
max
k,l∈N, k+l=α ∂k+l"
SUP,0.44543429844098,"∂uk∂vl R(u, v)

L∞([0,1]2)"
SUP,0.4465478841870824,≤C(α)γ
SUP,0.44766146993318484,"nα
,
∀n ∈N+,
(52)"
SUP,0.4487750556792873,"where C(α) > 0 is a universal constant only related to α. Furthermore, let ˆRn(u, v) :=
˜Rn(u, v) −˜Rn(u, 0) −˜Rn(0, v) + ˜Rn(0, 0), we get ˆRn ∈P2
n with ˆRn(u, 0) = ˆRn(0, v) = 0 for
any u, v ∈[0, 1]. By (50), we have R(u, v) = 0 for any (u, v) ∈[0, 1] × {0} ∪{0} × [0, 1], then"
SUP,0.44988864142538976,"∥R −ˆRn∥L∞([0,1]2) ≤∥R −˜Rn∥L∞([0,1]2) + ∥˜Rn −ˆRn∥L∞([0,1]2)
≤∥R −˜Rn∥L∞([0,1]2) + ∥˜Rn(u, 0) −R(u, 0)∥L∞([0,1]2)
+ ∥˜Rn(0, v) −R(0, v)∥L∞([0,1]2) + ∥˜Rn(0, 0) −R(0, 0)∥L∞([0,1]2)
≲∥R −˜Rn∥L∞([0,1]2),
(53)"
SUP,0.4510022271714922,"i.e. we can further require the approximator satisfying the zero half-boundary condition
(i.e. vanishing on (u, v) ∈[0, 1] × {0} ∪{0} × [0, 1]) without eﬀecting the approximation
accuracy. Let ¯m := min{mE, mD}, we get"
SUP,0.4521158129175947,"∥R −˜R∥L∞([0,1]2) ≤C(α)γ"
SUP,0.4532293986636971,"¯mα
≤C(α)γ
 1"
SUP,0.45434298440979953,"mα
E
+
1
mα
D"
SUP,0.455456570155902,"
,
(54) where"
SUP,0.45657015590200445,"˜R := ˜R ¯m ∈P2
¯m,
˜R(u, v) := ¯m
X i=1 ¯m
X"
SUP,0.4576837416481069,"j=1
˜rijuivj.
(55)"
SUP,0.45879732739420936,Published as a conference paper at ICLR 2022 Let
SUP,0.4599109131403118,"c = 1 ¯m,
u = 1 ¯m,
M = [˜rij] ∈R ¯m× ¯m,
(56)
V = −diag(2, 3, · · · , ¯m + 1)/c0,
W = −diag(2, 3, · · · , ¯m + 1)/c0,
(57)"
SUP,0.4610244988864143,"then by (24) and (54), we have"
SUP,0.4621380846325167,"∥H −ˆ
H∥≤∥ρ −ˆρ∥L1([0,∞)2)
(58)"
SUP,0.46325167037861914,"=
ρ(t, s) −c⊤eV tMeW su

L1([0,∞)2)"
SUP,0.4643652561247216,"=
ρ(t, s) −e−t"
SUP,0.46547884187082406,c0 e−s
SUP,0.4665924276169265,c0 ˜R(e−t
SUP,0.46770601336302897,"c0 , e−s"
SUP,0.4688195991091314,"c0 )

L1([0,∞)2)
= c2
0
R −˜R

L1([0,1]2)
(59)"
SUP,0.46993318485523383,"≤c2
0
R −˜R

L∞([0,1]2) ≤C(α)γ"
SUP,0.4710467706013363,β2 ¯mα ≤C(α)γ β2  1
SUP,0.47216035634743875,"mα
E
+
1
mα
D"
SUP,0.4732739420935412,"
.
(60)"
SUP,0.47438752783964366,"That is, one can achieve an approximation accuracy scaling like (1/ ¯m)α with ¯m2 parameters.
The proof is completed."
SUP,0.4755011135857461,"Remark C.1. The extension to multi-dimensional inputs (general d ∈N+) is found in the
last paragraph of Appendix D.2."
SUP,0.4766146993318486,"Remark C.2. Recall M = PQ ∈RmD×mE with P ∈RmD×N, Q ∈RN×mE, we only need
to investigate the case of N ≤min{mE, mD} = ¯m, since rank(M) ≤¯m."
SUP,0.477728285077951,"D
Approximation rates via temporal product structure"
SUP,0.47884187082405344,"In this section, the proof of Theorem 4.3 is provided."
SUP,0.4799554565701559,"D.1
Proper Orthogonal Decomposition"
SUP,0.48106904231625836,"Proper orthogonal decomposition (POD; (Liang et al., 2002), (Berkooz et al., 1993), (Chat-
terjee, 2000)) is a method for model reduction, which is commonly applied to numerical
PDEs and ﬂuids mechanics. It can be viewed as an extension of singular value decomposi-
tion (SVD) and principal component analysis (PCA) to inﬁnite-dimensional spaces."
SUP,0.4821826280623608,"Fix any R ∈L∞([0, 1]2). 3 Deﬁne the POD operator"
SUP,0.48329621380846327,"K : φ(v) 7→
Z 1 0 Z 1"
SUP,0.4844097995545657,"0
R(u, v)φ(v)dv · R(u, v)du,
φ(v) ∈L2[0, 1].
(61)"
SUP,0.48552338530066813,"Proposition D.1. The operator K is linear, bounded, compact, self-adjoint and non-
negative."
SUP,0.4866369710467706,Proof. (i) The linearity is obvious.
SUP,0.48775055679287305,(ii) Let
SUP,0.4888641425389755,"R : φ(v) 7→
Z 1"
SUP,0.48997772828507796,"0
R(u, v)φ(v)dv,
φ(v) ∈L2[0, 1],
(62) then"
SUP,0.4910913140311804,"(Kφ)(v) =
Z 1"
SUP,0.4922048997772829,"0
R(u, v)(Rφ)(u)du,
φ(v) ∈L2[0, 1].
(63)"
SUP,0.4933184855233853,"By the Cauchy-Schwartz inequality, we get"
SUP,0.49443207126948774,"∥Rφ∥L2[0,1] ≤∥R∥L2([0,1]2)∥φ∥L2[0,1],
(64)"
SUP,0.4955456570155902,"3Here, we use the same notation as (44), since the function deﬁned there is also bounded."
SUP,0.49665924276169265,Published as a conference paper at ICLR 2022
SUP,0.4977728285077951,which gives
SUP,0.49888641425389757,"∥Kφ∥L2[0,1] ≤∥R∥L2([0,1]2)∥Rφ∥L2[0,1] ≤∥R∥2
L2([0,1]2)∥φ∥L2[0,1].
(65)"
SUP,0.5,"Note that R(u, v) ∈L∞([0, 1]2) ⊂L2([0, 1]2), hence both K and R are bounded operator
from L2[0, 1] to itself."
SUP,0.5011135857461024,(iii) It is well-known that the Hilbert–Schmidt integral operator
SUP,0.5022271714922049,"(Cψ)(v) =
Z 1"
SUP,0.5033407572383074,"0
R(u, v)ψ(u)du,
ψ(u) ∈L2[0, 1]
(66)"
SUP,0.5044543429844098,"is a compact operator from L2[0, 1] to itself. Therefore"
SUP,0.5055679287305123,"Kφ = CRφ,
φ ∈L2[0, 1],
(67)"
SUP,0.5066815144766147,"which gives K = CR. Since R is bounded and C is compact, we get K is also compact."
SUP,0.5077951002227171,"(iv) By Fubini’s theorem, it is straightforward to verify that"
SUP,0.5089086859688196,"⟨Kφ, ψ⟩L2[0,1] =
Z 1 0 Z 1"
SUP,0.5100222717149221,"0
R(u, w)
Z 1"
SUP,0.5111358574610245,"0
R(u, v)φ(v)dv

du · ψ(w)dw =
Z 1 0 Z 1 0 Z 1"
SUP,0.512249443207127,"0
R(u, w)R(u, v)φ(v)ψ(w)dvdudw =
Z 1 0 Z 1"
SUP,0.5133630289532294,"0
R(u, v)
Z 1"
SUP,0.5144766146993318,"0
R(u, w)ψ(w)dw

du · φ(v)dv"
SUP,0.5155902004454342,"= ⟨φ, Kψ⟩L2[0,1],
φ, ψ ∈L2[0, 1].
(68)"
SUP,0.5167037861915368,"(v) By Fubini’s theorem, it is straightforward to verify that"
SUP,0.5178173719376392,"⟨Kφ, φ⟩L2[0,1] =
Z 1 0 Z 1"
SUP,0.5189309576837416,"0
(Rφ)(u)R(u, w)du · φ(w)dw =
Z 1 0 Z 1"
SUP,0.5200445434298441,"0
(Rφ)(u)R(u, w)φ(w)dudw =
Z 1"
SUP,0.5211581291759465,"0
(Rφ)(u)
Z 1"
SUP,0.522271714922049,"0
R(u, w)φ(w)dw

du =
Z 1"
SUP,0.5233853006681515,"0
(Rφ)2(u)du ≥0,
φ ∈L2[0, 1].
(69)"
SUP,0.5244988864142539,The proof is completed.
SUP,0.5256124721603563,"Combining (i)—(iv) and applying Hilbert–Schmidt’s expansion theorem, we obtain that
L2[0, 1] has an orthonormal basis {φn}n∈N
S{ψξ}ξ∈Ξ, such that"
SUP,0.5267260579064588,"• Kφn = λnφn, λn ̸= 0 for n ∈N, and Kψξ = 0 for ξ ∈Ξ, where N is a ﬁnite or
countable set. If N is not ﬁnite, we have limn→∞λn = 0;
• For any ψ ∈L2[0, 1], we have ψ =
X"
SUP,0.5278396436525612,"n∈N
⟨ψ, φn⟩L2[0,1]φn +
X"
SUP,0.5289532293986637,"ξ∈Ξ
⟨ψ, ψξ⟩L2[0,1]ψξ,
(70)"
SUP,0.5300668151447662,"where the second summation has at most countable non-zero terms, and"
SUP,0.5311804008908686,"Kψ =
X"
SUP,0.532293986636971,"n∈N
λn⟨ψ, φn⟩L2[0,1]φn.
(71)"
SUP,0.5334075723830735,"Here, all the series converge under the norm ∥· ∥L2[0,1]. Without loss of generality, N =
{1, 2, · · · , N0} for N0 ∈N+ or N0 = +∞(i.e. N = N+). By (69), we get"
SUP,0.534521158129176,"0 ≤⟨Kφn, φn⟩L2[0,1] = λn⟨φn, φn⟩2
L2[0,1] = λn,
∀n ∈N,
(72)"
SUP,0.5356347438752784,Published as a conference paper at ICLR 2022
SUP,0.5367483296213809,"i.e.
all the eigenvalues of K are non-negative, and λn ̸= 0 for n ∈N implies λn > 0,
∀n ∈N. In addition, limn→∞λn = 0 implies that one can index all the eigenvalues in a
non-increasing sequence: λ1 ≥λ2 ≥· · · ≥λn ≥· · · ≥0."
SUP,0.5378619153674833,"Then, we can present the POD estimate.
Theorem D.1. For any R ∈L∞([0, 1]2), we have Z 1 0"
SUP,0.5389755011135857,"R(u, v) − N
X"
SUP,0.5400890868596881,"n=1
⟨R(u, v), φn(v)⟩L2[0,1]φn(v)  2"
SUP,0.5412026726057907,"L2[0,1]
du = N0
X"
SUP,0.5423162583518931,"n=N+1
λn,
∀N ∈N.
(73)"
SUP,0.5434298440979956,Proof. Combining (69) and (72) gives
SUP,0.544543429844098,"λn = ⟨Kφn, φn⟩L2[0,1] =
Z 1"
SUP,0.5456570155902004,"0
(Rφn)2(u)du,
∀n ∈N.
(74)"
SUP,0.5467706013363028,"Similarly,
Z 1"
SUP,0.5478841870824054,"0
(Rψξ)2(u)du = ⟨Kψξ, ψξ⟩L2[0,1] =
X"
SUP,0.5489977728285078,"n∈N
λn⟨ψξ, φn⟩2
L2[0,1] = 0,
∀ξ ∈Ξ,
(75)"
SUP,0.5501113585746102,which gives
SUP,0.5512249443207127,"(Rψξ)(u) =
Z 1"
SUP,0.5523385300668151,"0
R(u, v)ψξ(v)dv = 0,
a.e. u ∈[0, 1].
(76)"
SUP,0.5534521158129176,"Notice that Ru(v) := R(u, v) ∈Cα[0, 1] ⊂L2[0, 1] for any u ∈[0, 1]. By (70) and (76), we
get"
SUP,0.5545657015590201,"∥Ru∥2
L2[0,1] = * X"
SUP,0.5556792873051225,"n∈N
⟨Ru, φn⟩L2[0,1]φn +
X"
SUP,0.5567928730512249,"ξ∈Ξ
⟨Ru, ψξ⟩L2[0,1]ψξ, X"
SUP,0.5579064587973274,"n∈N
⟨Ru, φn⟩L2[0,1]φn +
X"
SUP,0.5590200445434298,"ξ∈Ξ
⟨Ru, ψξ⟩L2[0,1]ψξ +"
SUP,0.5601336302895323,"L2[0,1] =
X"
SUP,0.5612472160356348,"n∈N
⟨Ru, φn⟩2
L2[0,1] +
X"
SUP,0.5623608017817372,"ξ∈Ξ
⟨Ru, ψξ⟩2
L2[0,1] =
X"
SUP,0.5634743875278396,"n∈N
(Rφn)2(u),
a.e. u ∈[0, 1].
(77)"
SUP,0.5645879732739421,"Hence for any N ∈N, we have
Ru − N
X"
SUP,0.5657015590200446,"n=1
⟨Ru, φn⟩L2[0,1]φn  2"
SUP,0.566815144766147,"L2[0,1]
= ∥Ru∥2
L2[0,1] − N
X"
SUP,0.5679287305122495,"n=1
⟨Ru, φn⟩2
L2[0,1]
(78) =
X"
SUP,0.5690423162583519,"n∈N
(Rφn)2(u) − N
X"
SUP,0.5701559020044543,"n=1
(Rφn)2(u) = N0
X"
SUP,0.5712694877505567,"n=N+1
(Rφn)2(u),
(79)"
SUP,0.5723830734966593,"where the summation is zero by convention if the subscript is larger than the superscript.
This by (74) implies Z 1 0"
SUP,0.5734966592427617,"R(u, v) − N
X"
SUP,0.5746102449888641,"n=1
⟨R(u, v), φn(v)⟩L2[0,1]φn(v)  2"
SUP,0.5757238307349666,"L2[0,1]
du = N0
X"
SUP,0.576837416481069,"n=N+1
λn.
(80)"
SUP,0.5779510022271714,"Here, the equality holds as a consequence of Beppo Levi’s monotone convergence lemma
and Lebesgue’s dominated convergence theorem, and one has P
n∈N λn < +∞. In fact, for"
SUP,0.579064587973274,Published as a conference paper at ICLR 2022
SUP,0.5801781737193764,"N0 = +∞, let Sn = Pn
k=1 λk, we get Sn increasing (since λk ≥0 for all k ∈N). By (74)
and (78), we have Sn = n
X k=1 Z 1"
SUP,0.5812917594654788,"0
(Rφk)2(u)du =
Z 1 0 n
X"
SUP,0.5824053452115813,"k=1
⟨Ru, φk⟩2
L2[0,1]du ≤
Z 1"
SUP,0.5835189309576837,"0
∥Ru∥2
L2[0,1]du = ∥R∥2
L2([0,1]2), (81)"
SUP,0.5846325167037862,which gives that Sn converges as n →∞. The proof is completed.
SUP,0.5857461024498887,"Remark D.1. Let ϕn(u) := ⟨R(u, v), φn(v)⟩L2[0,1], then we have the POD estimate
R(u, v) ≈P"
SUP,0.5868596881959911,"n ϕn(u)φn(v), where the error is characterised by the tail sum of eigenvalues of
the POD operator."
SUP,0.5879732739420935,Recall the POD operator deﬁned in (61). We similarly deﬁne
SUP,0.589086859688196,"˜K : φ(v) 7→
Z 1 0 Z 1"
SUP,0.5902004454342984,"0
˜R(u, v)φ(v)dv · ˜R(u, v)du,
φ(v) ∈L2[0, 1],
(82)"
SUP,0.5913140311804009,"where ˜R is deﬁned as (55), i.e. the approximator constructed in the general approximation
theorem before. Obviously, as a polynomial, ˜R ∈L∞([0, 1]2). Hence, by Proposition D.1,
˜K is also linear, bounded, compact, self-adjoint and non-negative. In addition, according to
Theorem D.1, we have the following POD estimate
Z 1 0"
SUP,0.5924276169265034,"˜R(u, v) − N
X n=1"
SUP,0.5935412026726058,"˜R(u, v), ˜φn(v)"
SUP,0.5946547884187082,"L2[0,1] ˜φn(v)  2"
SUP,0.5957683741648107,"L2[0,1]
du ="
SUP,0.5968819599109132,"˜
N0
X"
SUP,0.5979955456570156,"n=N+1
˜λn,
(83)"
SUP,0.5991091314031181,"where {˜λn}
˜
N0
n=1 are eigenvalues of ˜K satisfying ˜λ1 ≥˜λ2 ≥· · · ≥˜λn ≥· · · ≥0, and {˜φn}
˜
N0
n=1 ⊂
L2[0, 1] are the corresponding orthonormal eigenfunctions, i.e.
˜K˜φn = ˜λn ˜φn, ˜λn > 0 for
n ∈{1, 2, · · · , ˜N0}."
SUP,0.6002227171492205,"Lemma D.1. ˜K is a ﬁnite-rank operator. That is, ˜N0 ≤¯m = min{mE, mD} < ∞."
SUP,0.6013363028953229,"Proof. Let ˜ϕn(u) :=

 ˜R(u, v), ˜φn(v)"
SUP,0.6024498886414253,"L2[0,1]. We ﬁrst show that both ˜ϕn(u) and ˜φn(v) are"
SUP,0.6035634743875279,"polynomials. In fact, since ˜R(u, v) = P ¯m
i=1
P ¯m
j=1 ˜rijuivj, we have ∂k"
SUP,0.6046770601336303,"∂uk ˜R(u, v)
 =  ¯m
X i=k ¯m
X"
SUP,0.6057906458797327,"j=1
˜rij
i!
(i −k)!ui−kvj  ≤ ¯m
X i=k ¯m
X"
SUP,0.6069042316258352,"j=1
|˜rij|
i!
(i −k)! ≜C1(k, ¯m),
k = 1, 2, · · · ,
(84)"
SUP,0.6080178173719376,"with the convention that the summation is zero if the subscript is larger than the superscript,
i.e. C1(k, ¯m) = 0 for any k > ¯m. Let C1( ¯m) := max{∥˜R∥L∞([0,1]2), max1≤k≤¯m C1(k, ¯m)},
then we have

∂k"
SUP,0.60913140311804,"∂uk ˜R(u, v)˜φn(v)
 ≤C1( ¯m)|˜φn(v)| ∈L2[0, 1] ⊂L1[0, 1],
k = 0, 1, · · · .
(85)"
SUP,0.6102449888641426,"According to Lebesgue’s dominated convergence theorem, we get by induction that dk"
SUP,0.611358574610245,duk ˜ϕn(u) = dk duk Z 1
SUP,0.6124721603563474,"0
˜R(u, v)˜φn(v)dv =
Z 1 0 ∂k"
SUP,0.6135857461024499,"∂uk ˜R(u, v)˜φn(v)dv,
k = 0, 1, · · · .
(86)"
SUP,0.6146993318485523,"Similarly, we get

∂k"
SUP,0.6158129175946548,"∂vk ˜R(u, v)
 =  ¯m
X i=1 ¯m
X"
SUP,0.6169265033407573,"j=k
˜rijui
j!
(j −k)!vj−k  ≤ ¯m
X i=1 ¯m
X"
SUP,0.6180400890868597,"j=k
|˜rij|
j!
(j −k)! ≜C2(k, ¯m),
k = 1, 2, · · · ,
(87)"
SUP,0.6191536748329621,Published as a conference paper at ICLR 2022
SUP,0.6202672605790646,"and C2(k, ¯m) = 0 for any k > ¯m. Let C2( ¯m) := max{∥˜R∥L∞([0,1]2), max1≤k≤¯m C2(k, ¯m)},
then for k = 0, 1, · · · , we have

∂k"
SUP,0.621380846325167,"∂vk ˜R(u, v)
Z 1"
SUP,0.6224944320712695,"0
˜R(u, v)˜φn(v)dv
 ≤C2( ¯m)∥˜R∥L∞([0,1]2)∥˜φn∥L1[0,1]"
SUP,0.623608017817372,"≤C2
2( ¯m)∥˜φn∥L2[0,1] = C2
2( ¯m) ⊂L1[0, 1].
(88)"
SUP,0.6247216035634744,"According to Lebesgue’s dominated convergence theorem, we get by induction that dk"
SUP,0.6258351893095768,dvk ˜φn(v) = 1 ˜λn Z 1 0 ∂k
SUP,0.6269487750556793,"∂vk ˜R(u, v)
Z 1"
SUP,0.6280623608017817,"0
˜R(u, v)˜φn(v)dvdu,
k = 0, 1, · · · .
(89)"
SUP,0.6291759465478842,"That is, ˜ϕn, ˜φn ∈C∞[0, 1] for any n = 1, 2, · · · , ˜N0.
Since ˜R(u, 0) = ˜R(0, v) = 0 for
u, v ∈[0, 1], we get ˜ϕn(0) = ˜φn(0) = 0. Furthermore, we have
dk"
SUP,0.6302895322939867,"duk ˜ϕn(u) =
dk"
SUP,0.6314031180400891,dvk ˜φn(v) = 0
SUP,0.6325167037861915,"for k > ¯m, hence ˜ϕn, ˜φn ∈P ¯m. Since {˜φn}
˜
N0
n=1 ⊂L2[0, 1] are orthonormal, we must have
˜N0 ≤¯m < ∞. 4 The proof is completed."
SUP,0.6336302895322939,"D.2
Approximation rates"
SUP,0.6347438752783965,"Perturbation of eigenvalues.
First, we need to bound the gap between the eigenvalues
{˜λn}
˜
N0
n=1 and {λn}N0
n=1 (corresponding to the function R deﬁned in (44)). The following
theorem is necessary.
Theorem D.2 (Courant–Fischer–Weyl min-max principle; Lax (2002) (Chapter 28, The-
orem 4)). Let B be a compact, self-adjoint operator on a Hilbert space H, whose positive
eigenvalues are listed in a decreasing order µ1 ≥µ2 ≥· · · ≥µk ≥· · · > 0. Then"
SUP,0.6358574610244989,"max
Sk
min
x∈Sk, ∥x∥H=1⟨Bx, x⟩H = µk,
(90)"
SUP,0.6369710467706013,where Sk ⊂H is any k-dimensional linear subspace.
SUP,0.6380846325167038,"Based on it, we have the following lemma to characterise the perturbation of singular values.
Lemma D.2. For any R1, R2 ∈L∞([0, 1]2), we have the estimate q"
SUP,0.6391982182628062,"λR1
k
−
q λR2
k"
SUP,0.6403118040089086,"≤∥R1 −R2∥L2([0,1]2).
(91)"
SUP,0.6414253897550112,"Proof. According to Theorem D.2 and by (69), we have"
SUP,0.6425389755011136,"λR
k = max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1⟨KRφ, φ⟩L2[0,1] = max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1 ∥RRφ∥2
L2[0,1].
(92)"
SUP,0.643652561247216,"Note that RR1 −RR2 = RR1−R2 and by (64), we have
q"
SUP,0.6447661469933185,"λR1
k
= max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1 ∥RR1φ∥L2[0,1]"
SUP,0.6458797327394209,"≤max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1"
SUP,0.6469933184855234," 
∥(RR1 −RR2)φ∥L2[0,1] + ∥RR2φ∥L2[0,1]
"
SUP,0.6481069042316259,"≤max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1"
SUP,0.6492204899777283," 
∥RR1−R2∥+ ∥RR2φ∥L2[0,1]
"
SUP,0.6503340757238307,"≤max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1 ∥RR2φ∥L2[0,1] + ∥R1 −R2∥L2([0,1]2) =
q"
SUP,0.6514476614699332,"λR2
k
+ ∥R1 −R2∥L2([0,1]2),
(93)"
SUP,0.6525612472160356,"4In fact, for any p ∈P ¯
m with p(0) = 0, we have p ∈span{v, v2, · · · , v ¯
m}. Through a standard
Schmidt-orthogonalization, we can get ek(v) ∈Pk, k = 1, 2, · · · , ¯m, such that ⟨ei, ej⟩L2[0,1] =
δij, and p ∈span{e1(v), e2(v), · · · , e ¯
m(v)}. That is, if ⟨p, q⟩L2[0,1] = 0 for some p, q ∈P ¯
m with
p(0) = 0, q(0) = 0, then their coordinates under the basis {ek} ¯
m
k=1 are orthogonal. Hence, the ˜
N0
orthogonal ¯m-dimensional coordinates here leads to at most ¯m non-zeros."
SUP,0.6536748329621381,Published as a conference paper at ICLR 2022
SUP,0.6547884187082406,"and similarly,
q"
SUP,0.655902004454343,"λR2
k
= max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1 ∥RR2φ∥L2[0,1]"
SUP,0.6570155902004454,"≤max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1"
SUP,0.6581291759465479," 
∥(RR2 −RR1)φ∥L2[0,1] + ∥RR1φ∥L2[0,1]
"
SUP,0.6592427616926503,"≤max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1"
SUP,0.6603563474387528," 
∥RR2−R1∥+ ∥RR1φ∥L2[0,1]
"
SUP,0.6614699331848553,"≤max
Sk
min
φ∈Sk, ∥φ∥L2[0,1]=1 ∥RR1φ∥L2[0,1] + ∥R2 −R1∥L2([0,1]2) =
q"
SUP,0.6625835189309577,"λR1
k
+ ∥R1 −R2∥L2([0,1]2),
(94)"
SUP,0.6636971046770601,which completes the proof.
SUP,0.6648106904231625,"Proofs.
Now we are ready to derive the ﬁnal estimate."
SUP,0.6659242761692651,"Proof of Theorem 4.3. By Lemma D.2 and (54), we get

p"
SUP,0.6670378619153675,"λk −
q ˜λk"
SUP,0.6681514476614699,"≤∥R −˜R∥L2([0,1]2) ≤∥R −˜R∥L∞([0,1]2) ≤C(α)γ"
SUP,0.6692650334075724,"¯mα
.
(95)"
SUP,0.6703786191536748,"Combining (54), (83) and (95) gives that"
SUP,0.6714922048997772,"1
c2
0"
SUP,0.6726057906458798,"ρ(t, s) − N
X"
SUP,0.6737193763919822,"n=1
e−t"
SUP,0.6748329621380846,c0 ˜ϕn(e−t
SUP,0.6759465478841871,c0 ) · e−s
SUP,0.6770601336302895,c0 ˜φn(e−s c0 )
SUP,0.678173719376392,"L1([0,∞)2) ="
SUP,0.6792873051224945,"R(u, v) − N
X"
SUP,0.6804008908685969,"n=1
˜ϕn(u)˜φn(v)"
SUP,0.6815144766146993,"L1([0,1]2)"
SUP,0.6826280623608018,"≤
R(u, v) −˜R(u, v)

L1([0,1]2) +"
SUP,0.6837416481069042,"˜R(u, v) − N
X"
SUP,0.6848552338530067,"n=1
˜ϕn(u)˜φn(v)"
SUP,0.6859688195991092,"L1([0,1]2)"
SUP,0.6870824053452116,"≤
R(u, v) −˜R(u, v)

L∞([0,1]2) +"
SUP,0.688195991091314,"˜R(u, v) − N
X"
SUP,0.6893095768374164,"n=1
˜ϕn(u)˜φn(v)"
SUP,0.6904231625835189,"L2([0,1]2)"
SUP,0.6915367483296214,"≤C(α)γ ¯mα
+"
SUP,0.6926503340757239,"v
u
u
t"
SUP,0.6937639198218263,"˜
N0
X"
SUP,0.6948775055679287,"n=N+1
˜λn ≤C(α)γ ¯mα
+"
SUP,0.6959910913140311,"v
u
u
t"
SUP,0.6971046770601337,"˜
N0
X"
SUP,0.6982182628062361,"n=N+1
|˜λn −λn| +"
SUP,0.6993318485523385,"˜
N0
X"
SUP,0.700445434298441,"n=N+1
λn"
SUP,0.7015590200445434,"≤C(α)γ ¯mα
+"
SUP,0.7026726057906458,"v
u
u
t"
SUP,0.7037861915367484,"˜
N0
X"
SUP,0.7048997772828508,"n=N+1
λn +"
SUP,0.7060133630289532,"v
u
u
t"
SUP,0.7071269487750557,"˜
N0
X n=N+1  q"
SUP,0.7082405345211581,"˜λn −
p λn   q"
SUP,0.7093541202672605,"˜λn +
p λn "
SUP,0.7104677060133631,"≤C(α)γ ¯mα
+"
SUP,0.7115812917594655,"v
u
u
t"
SUP,0.7126948775055679,"˜
N0
X"
SUP,0.7138084632516704,"n=N+1
λn +"
SUP,0.7149220489977728,"v
u
u
t"
SUP,0.7160356347438753,"˜
N0
X n=N+1  q"
SUP,0.7171492204899778,"˜λn −
p λn  2
+
√ 2"
SUP,0.7182628062360802,"v
u
u
t"
SUP,0.7193763919821826,"˜
N0
X n=N+1 p λn  q"
SUP,0.720489977728285,"˜λn −
p λn "
SUP,0.7216035634743875,"≲C(α)γ 
 "
SUP,0.72271714922049,"
1 +
q"
SUP,0.7238307349665924,"˜N0 −N

·
1
¯mα +"
SUP,0.7249443207126949,"v
u
u
t"
SUP,0.7260579064587973,"˜
N0
X"
SUP,0.7271714922048997,"n=N+1
λn +"
SUP,0.7282850779510023,"v
u
u
t"
SUP,0.7293986636971047,"˜
N0
X n=N+1 p"
SUP,0.7305122494432071,"λn ·
1
¯mα/2 
"
SUP,0.7316258351893096,",
(96)"
SUP,0.732739420935412,where ≲hides universal positive constants.
SUP,0.7338530066815144,"For the corresponding parameters, recall that ˜ϕn, ˜φn ∈P ¯m with ˜ϕn(0) = ˜φn(0) = 0, we can
write"
SUP,0.734966592427617,"˜ϕn(u) = ¯m
X"
SUP,0.7360801781737194,"i=1
Pinui,
˜φn(v) = ¯m
X"
SUP,0.7371937639198218,"j=1
Qnjvj
(97)"
SUP,0.7383073496659243,Published as a conference paper at ICLR 2022
SUP,0.7394209354120267,"for any n = 1, 2, · · · , ˜N0. Let"
SUP,0.7405345211581291,"c = 1 ¯m,
u = 1 ¯m,
(98)
V = −diag(2, 3, · · · , ¯m + 1)/c0,
W = −diag(2, 3, · · · , ¯m + 1)/c0,
(99)"
SUP,0.7416481069042317,"M = PQ with P = [Pin] ∈R ¯m×N, Q = [Qnj] ∈RN× ¯m,
(100)"
SUP,0.7427616926503341,then we have
SUP,0.7438752783964365,"c⊤eV tMeW su = c⊤eV tP · QeW su = N
X n=1 ¯m
X"
SUP,0.744988864142539,"i=1
e−i+1"
SUP,0.7461024498886414,"c0 tPin · ¯m
X"
SUP,0.7472160356347439,"j=1
Qnje−j+1 c0 s = N
X"
SUP,0.7483296213808464,"n=1
e−t"
SUP,0.7494432071269488,c0 ˜ϕn(e−t
SUP,0.7505567928730512,c0 ) · e−s
SUP,0.7516703786191536,c0 ˜φn(e−s
SUP,0.7527839643652561,"c0 ).
(101)"
SUP,0.7538975501113586,Plugging this into (96) gives
SUP,0.755011135857461,"∥H −ˆ
H∥≤∥ρ −ˆρ∥L1([0,∞)2)
(102)"
SUP,0.7561247216035635,"=
ρ(t, s) −c⊤eV tMeW su

L1([0,∞)2)"
SUP,0.7572383073496659,"≲C(α)γ β2 
 "
SUP,0.7583518930957683,"
1 +
q"
SUP,0.7594654788418709,"˜N0 −N

·
1
¯mα +"
SUP,0.7605790645879733,"v
u
u
t"
SUP,0.7616926503340757,"˜
N0
X"
SUP,0.7628062360801782,"n=N+1
λn +"
SUP,0.7639198218262806,"v
u
u
t"
SUP,0.765033407572383,"˜
N0
X n=N+1 p"
SUP,0.7661469933184856,"λn ·
1
¯mα/2 
 , (103)"
SUP,0.767260579064588,"and the number of trainable parameters is 2N ¯m. Together with Lemma D.1, the proof is
completed."
SUP,0.7683741648106904,"Extension to multi-dimensional inputs.
The above results can be naturally extended
to the general case where a d-dimensional input is given (∀d ∈N+). In fact, let
ρi(t, s) −c⊤eV tMieW su

L1([0,∞)2) ≲ϵ,
i = 1, 2, · · · , d,
(104)"
SUP,0.7694877505567929,"for some 0 < ϵ ≪1, then we take"
SUP,0.7706013363028953,"M = (M1, M2, · · · , Md) ∈R ¯m×d ¯m,
(105)"
SUP,0.7717149220489977,"W = diag(W, W, · · · , W) ∈Rd ¯m×d ¯m,
U = diag(u, u, · · · , u) ∈Rd ¯m×d,
(106)"
SUP,0.7728285077951003,and have
SUP,0.7739420935412027,"c⊤eV tMeW sU = c⊤eV t · (M1, M2, · · · , Md) · diag(eW s, eW s, · · · , eW s) · diag(u, u, · · · , u)"
SUP,0.7750556792873051,"= (c⊤eV tM1, c⊤eV tM2, · · · , c⊤eV tMd) · diag(eW su, eW su, · · · , eW su)"
SUP,0.7761692650334076,"= (c⊤eV tM1eW su, c⊤eV tM2eW su, · · · , c⊤eV tMdeW su).
(107)"
SUP,0.77728285077951,"If the form PQ(= M) is required, it is suﬃcient to take Mi = PiQi, i = 1, 2, · · · , d, and"
SUP,0.7783964365256125,"P = (P1, P2, · · · , Pd) ∈R ¯m×dN, Q = diag(Q1, Q2, · · · , Qd) ∈RdN×d ¯m.
(108)"
SUP,0.779510022271715,"Therefore, we obtain d
X i=1"
SUP,0.7806236080178174,"ρi(t, s) −

c⊤eV tMeW sU
"
SUP,0.7817371937639198,"i

L1([0,∞)2) ≲dϵ,
(109)"
SUP,0.7828507795100222,"with the number of parameters increased by d-times compared to the corresponding one-
dimensional setting."
SUP,0.7839643652561247,"D.3
Case analysis"
SUP,0.7850779510022272,"Bounds under diﬀerent cases.
Now we make the comparison between (102) and (58).
Recall that {λn}N0
n=1 is a positive decreased sequence (with limn→∞λn = 0 and P∞
n=1 λn ≤
∥R∥2
L2([0,1]2) by (81), if N0 = +∞), and ˜N0 ≤¯m, we have the following cases."
SUP,0.7861915367483296,Published as a conference paper at ICLR 2022
SUP,0.7873051224944321,"• if ˜N0 = o( ¯m), we set N = ˜N0 in (102) and get the same bound as (58), but the
number of parameters is only O( ˜N0 ¯m) = o( ¯m2);"
SUP,0.7884187082405345,"• if ˜N0 = O( ¯m), then (102) implies that
ρ(t, s) −c⊤eV tMeW su

L1([0,∞)2)"
SUP,0.7895322939866369,"≲C(α)γ β2 
 "
SUP,0.7906458797327395,"
1 +
√"
SUP,0.7917594654788419,"¯m −N

·
1
¯mα +"
SUP,0.7928730512249443,"v
u
u
t ¯m
X"
SUP,0.7939866369710468,"n=N+1
λn +"
SUP,0.7951002227171492,"v
u
u
t ¯m
X n=N+1 p"
SUP,0.7962138084632516,"λn ·
1
¯mα/2 
 "
SUP,0.7973273942093542,"≲C(α)γ β2 
"
SUP,0.7984409799554566,"
1
¯mα−1 2 +"
SUP,0.799554565701559,"v
u
u
t ¯m
X"
SUP,0.8006681514476615,"n=N+1
λn +"
SUP,0.8017817371937639,"v
u
u
t ¯m
X n=N+1 p"
SUP,0.8028953229398663,"λn ·
1
¯mα/2 
"
SUP,0.8040089086859689,".
(110)"
SUP,0.8051224944320713,We are supposed to require that
SUP,0.8062360801781737,"1
¯mα−1"
SUP,0.8073496659242761,"2 ≳max 
 "
SUP,0.8084632516703786,"v
u
u
t ¯m
X"
SUP,0.8095768374164811,"n=N+1
λn,"
SUP,0.8106904231625836,"v
u
u
t ¯m
X n=N+1 p"
SUP,0.811804008908686,"λn ·
1
¯mα/2 
  ⇔ ¯m
X"
SUP,0.8129175946547884,"n=N+1
λn ≲
1
¯m2α−1 , ¯m
X n=N+1 p"
SUP,0.8140311804008908,"λn ≲
1
¯mα−1 .
(111)"
SUP,0.8151447661469933,"We give the following typical examples to illustrate suﬃcient conditions to guarantee
(111)."
SUP,0.8162583518930958,"– If λn = O(n−r) with r > 2α + 1 ≥3 (α ∈N+), since ¯m
X"
SUP,0.8173719376391982,"n=N+1
n−r ≤
Z
¯m"
SUP,0.8184855233853007,"N
x−rdx =
1
r −1"
SUP,0.8195991091314031,"
1
N r−1 −
1
¯mr−1"
SUP,0.8207126948775055,"
,
∀r > 1,
(112)"
SUP,0.821826280623608,"we get by (111) that ¯m
X"
SUP,0.8229398663697105,"n=N+1
λn ≲ ¯m
X"
SUP,0.8240534521158129,"n=N+1
n−r ≲
1
N r−1 ≲
1
¯m2α−1 ⇔N ≳¯m 2α−1"
SUP,0.8251670378619154,"r−1 ,
(113) ¯m
X n=N+1 p λn ≲ ¯m
X"
SUP,0.8262806236080178,"n=N+1
n−r"
SUP,0.8273942093541202,"2 ≲
1
N
r
2 −1 ≲
1
¯mα−1 ⇔N ≳¯m α−1"
SUP,0.8285077951002228,"r
2 −1 .
(114)"
SUP,0.8296213808463252,"Assume that N ∼¯mδ with δ ∈[0, 1), then by (113) and (114), we require
δ ≥max{ 2α−1"
SUP,0.8307349665924276,"r−1 , α−1"
SUP,0.8318485523385301,"r
2 −1}, i.e."
SUP,0.8329621380846325,"r ≥max
2α −1"
SUP,0.8340757238307349,"δ
+ 1, 2
α −1"
SUP,0.8351893095768375,"δ
+ 1

= 2α −1"
SUP,0.8363028953229399,"δ
+ 1.
(115)"
SUP,0.8374164810690423,"Meanwhile, the POD-estimate (110) achieves an accuracy scaling like (1/ ¯m)α−1"
SUP,0.8385300668151447,"2
with O( ¯m1+δ) parameters, while under the same capacity, the accuracy of
(58) scales like (1/ ¯m)
α(1+δ)"
SUP,0.8396436525612472,"2
. The former beats the latter if α −1"
SUP,0.8407572383073497,2 > α(1+δ)
SUP,0.8418708240534521,"2
,
i.e.
δ < 1 −1/α (α ≥2).
When δ = 1 −1/α, (115) becomes r ≥
max{ 2α2−1"
SUP,0.8429844097995546,"α−1 , 2(α + 1)} = 2α2−1"
SUP,0.844097995545657,"α−1 . That is to say, r∗:= 2α2−1"
SUP,0.8452115812917594,"α−1
can be viewed
as an upper bound of the critical point where the two estimates are compa-
rable. When r > r∗, the POD-estimate outperforms the non-POD-estimate,
and this eﬀect gets more notable with r increasing. In fact, when r > r∗, we"
SUP,0.8463251670378619,take N = ¯m 2α−1
SUP,0.8474387527839644,r−1 > ¯m α−1
SUP,0.8485523385300668,"r
2 −1 (hence satisfying (113) and (114)), which gives an
O((1/ ¯m)α−1"
SUP,0.8496659242761693,2 ) approximation error with O( ¯m1+ 2α−1
SUP,0.8507795100222717,"r−1 ) parameters for the POD-
estimate, while O( ¯m2−1"
SUP,0.8518930957683741,"α ) trainable parameters are needed to achieve the same
accuracy using the non-POD-estimate. Since 2α−1"
SUP,0.8530066815144766,r−1 < 1 −1
SUP,0.8541202672605791,"α, we get that the
POD-estimate outperforms the non-POD-estimate. When r →+∞, the num-
ber of trainable parameters for the POD-estimate is O( ¯m), much better than
the non-POD-estimate."
SUP,0.8552338530066815,Published as a conference paper at ICLR 2022
SUP,0.856347438752784,Remark D.2. Let
SUP,0.8574610244988864,"K(v, w) :=
Z 1"
SUP,0.8585746102449888,"0
R(u, v)R(u, w)du,
v, w ∈[0, 1],
(116)"
SUP,0.8596881959910914,we get
SUP,0.8608017817371938,"(Kφ)(w) =
Z 1"
SUP,0.8619153674832962,"0
K(v, w)φ(v)dv,
φ ∈L2[0, 1].
(117)"
SUP,0.8630289532293987,"Recall that R ∈Cα([0, 1]2) ⊂L∞([0, 1]2), we get K ∈L∞([0, 1]2) ⊂L2([0, 1]2),
and hence K can be also viewed as a Hilbert-Schmidt integral operator with the
kernel K, where K is obviously symmetric as K(v, w) = K(w, v), and positive
deﬁnite since 0 ≤⟨Kφ, φ⟩L2[0,1] =
R 1
0
R 1
0 K(v, w)φ(v)φ(w)dvdw, φ ∈L2[0, 1]
by (69). Since the derivatives of R (up to α-order) are continuous on [0, 1]2
(hence bounded and integrable), we get K ∈Cα,α([0, 1]2) (α-diﬀerentiable for
both arguments). According to Chang & Ha (1999) (Theorem 1), we have ∞
X"
SUP,0.8641425389755011,"n=N+2α+1
λn ≲N −α,
∀N ∈N+ ⇒λn = O(n−(α+1)),
n →∞.
(118)"
SUP,0.8652561247216035,"That is to say, this general setting (only assume smoothness of the kernel)
can not guarantee the suﬃcient condition provided here (λn = O(n−r) with
r > 2α2−1"
SUP,0.8663697104677061,"α−1 ), i.e. the point that the POD-estimate outperforms the non-POD-
estimate requires a faster decay of the eigenvalues.
– If λn = O(e−ωn) with ω > 0, we get by (111) that ¯m
X"
SUP,0.8674832962138085,"n=N+1
λn ≲ ∞
X"
SUP,0.8685968819599109,"n=N+1
e−ωn = e−ω(N+1)"
SUP,0.8697104677060133,"1 −e−ω ≲
1
¯m2α−1 ⇔N ≳1"
SUP,0.8708240534521158,"ω ln
 ¯m2α−1"
SUP,0.8719376391982183,"1 −e−ω 
−1, (119) ¯m
X n=N+1 p λn ≲ ∞
X"
SUP,0.8730512249443207,"n=N+1
e−ω"
SUP,0.8741648106904232,2 n = e−ω
SUP,0.8752783964365256,2 (N+1)
SUP,0.876391982182628,1 −e−ω
SUP,0.8775055679287305,"2
≲
1
¯mα−1 ⇔N ≳2"
SUP,0.878619153674833,"ω ln

¯mα−1"
SUP,0.8797327394209354,"1 −e−ω 2 
−1. (120)"
SUP,0.8808463251670379,"That is to say, for any α ∈N+, ω > 0, we have N ∼(2α −1) ln ¯m. This
implies an O((1/ ¯m)α−1"
SUP,0.8819599109131403,"2 ) approximation error with O( ¯m ln ¯m) parameters for
the POD-estimate, while O( ¯m2−1"
SUP,0.8830734966592427,"α ) parameters are needed to achieve the same
accuracy using the non-POD-estimate.
– If N0 < +∞(i.e. K is a ﬁnite rank operator by (71)), one can just take N = N0
and get by (110) an O((1/ ¯m)α−1"
SUP,0.8841870824053452,"2 ) approximation error. For ¯m ∈N+ suﬃciently
large such that ¯m ∼N κ
0 for some κ ≫1, the number of trainable parameters for"
SUP,0.8853006681514477,"the POD-estimate is O(N ¯m) = O(N κ+1
0
), while O

N
κ(2−1"
SUP,0.8864142538975501,"α )
0

parameters are
needed to achieve the same accuracy using the non-POD-estimate. Obviously,
if α ≥2, we get κ(2 −1 α) ≥3"
SUP,0.8875278396436526,2κ ≫κ + 1.
SUP,0.888641425389755,"Eigenvalue approximation.
One can apply (91) in Lemma D.2 to estimate the eigen-
values {λn}N0
n=1, where R1 = R and R2 is taken as some approximator of R, say ˆR. Let
ˆλ := λ ˆ
R, we recall (91) as

p"
SUP,0.8897550111358574,"λk −
q ˆλk"
SUP,0.89086859688196,"≤∥R −ˆR∥L2([0,1]2).
(121)"
SUP,0.8919821826280624,"We provide a naive method here. That is, one can take ˆR as a piece-wise constant ap-
proximation of R. Fix any n ∈N+. Let I0 := {0}, Ii := ( i−1 n , i"
SUP,0.8930957683741648,"n] for i = 1, 2, · · · , n, then
{Ii×Ij}n
i,j=0 is the uniform partition over [0, 1]2. Let ˆR(u, v) := Pn
i,j=0 R( i n, j"
SUP,0.8942093541202673,"n)1Ii×Ij(u, v)."
SUP,0.8953229398663697,Published as a conference paper at ICLR 2022
SUP,0.8964365256124721,"Then for any φ ∈L2[0, 1], if w ∈Ik, k = 0, 1, · · · , n, we have"
SUP,0.8975501113585747,"(K ˆ
Rφ)(w) =
Z 1 0 Z 1"
SUP,0.8986636971046771,"0
ˆR(u, v)φ(v)dv · ˆR(u, w)du = n
X i=0 n
X j=0 n
X i′=0 n
X"
SUP,0.8997772828507795,"j′=0
R
 i n, j n"
SUP,0.9008908685968819,"
R
i′ n, j′ n  Z 1 0 Z 1"
SUP,0.9020044543429844,"0
1Ii×Ij(u, v)φ(v)dv · 1Ii′×Ij′(u, w)du = n
X i=0 n
X"
SUP,0.9031180400890868,"j=0
R
 i n, j n"
SUP,0.9042316258351893,"
R
 i n, k n  Z Ii Z"
SUP,0.9053452115812918,"Ij
1Ii×Ij(u, v)φ(v)dv · 1Ii×Ik(u, w)du = n
X i=0 n
X"
SUP,0.9064587973273942,"j=0
R
 i n, j n"
SUP,0.9075723830734966,"
R
 i n, k n 
· 1 n Z"
SUP,0.9086859688195991,"Ij
φ(v)dv,
(122)"
SUP,0.9097995545657016,"which is a constant only related to k.
That is, K ˆ
Rφ is a piece-wise constant func-
tion, i.e.
K ˆ
Rφ ∈span {1Ik}n
k=0, or range(K ˆ
R) ⊂span {1Ik}n
k=0.
Obviously, {1Ik}n
k=0
is an orthogonal set, which implies that the operator K ˆ
R is of ﬁnite rank at most
n + 1. Let ˆRij :=
1
nR( i−1"
SUP,0.910913140311804,"n , j−1"
SUP,0.9120267260579065,"n ), i, j = 1, 2, · · · , n + 1, {σk}n+1
k=1 be the singular value of
ˆR := [ ˆRij] ∈R(n+1)×(n+1) and V ∈R(n+1)×(n+1) with columns as the corresponding right
singular vectors. Set [e1, e2, · · · , en+1] := [1I0, 1I1, · · · , 1In] V , then by (122), we get for
l = 1, 2, · · · , n + 1,"
SUP,0.9131403118040089,"(K ˆ
Rel)(w) = n
X i=0 n
X"
SUP,0.9142538975501113,"j=0
R
 i n, j n"
SUP,0.9153674832962138,"
R
 i n, k n 
· 1"
SUP,0.9164810690423163,"n2 Vj+1,l = n+1
X i=1 n+1
X"
SUP,0.9175946547884187,"j=1
ˆRij ˆRi,k+1Vjl =
h
ˆR⊤ˆRV:,l
i"
SUP,0.9187082405345212,"k+1 ,
(123)"
SUP,0.9198218262806236,which gives
SUP,0.920935412026726,"K ˆ
Rel = n
X k=0"
SUP,0.9220489977728286,"h
ˆR⊤ˆRV:,l
i"
SUP,0.923162583518931,"k+1 1Ik = σ2
l n
X"
SUP,0.9242761692650334,"k=0
Vk+1,l1Ik = σ2
l el,
(124)"
SUP,0.9253897550111359,"i.e. the set {σ2
k}n+1
k=1 collects the all the eigenvalues of K ˆ
R, which can be obtained by the
SVD of ˆR."
SUP,0.9265033407572383,"For the error estimate, it is straightforward to have
R(u, v) −ˆR(u, v)

2"
SUP,0.9276169265033407,"L2([0,1]2) = n
X i=1 n
X j=1 Z
i
n i−1 n Z
j
n j−1 n"
SUP,0.9287305122494433,"R(u, v) −R
 i n, j n "
DUDV,0.9298440979955457,"2
dudv ≤ n
X i=1 n
X j=1 Z
i
n i−1 n Z
j
n j−1"
DUDV,0.9309576837416481,"n
max
(u,v)∈[0,1]2 ∥∇R(u, v)∥2
2 ·"
DUDV,0.9320712694877505,"
u −i"
DUDV,0.933184855233853,"n, v −j n  2"
DUDV,0.9342984409799554,"2
dudv ≤ n
X i=1 n
X j=1 Z
i
n i−1 n Z
j
n j−1"
DUDV,0.9354120267260579,"n
2C2(α)γ2 · 2"
DUDV,0.9365256124721604,n2 dudv = 4C2(α)γ2
DUDV,0.9376391982182628,"n2
,
(125)"
DUDV,0.9387527839643652,"hence by (121),
√λk −σk
 ≤2C(α)γ"
DUDV,0.9398663697104677,"n
for any n ∈N+."
DUDV,0.9409799554565702,"E
Numerical settings"
DUDV,0.9420935412026726,"According to Lemma B.1, the target input-output temporal relationship has a Riesz repre-
sentation form. Under the discrete-time regime, we are supposed to set"
DUDV,0.9432071269487751,"Ht(x) = T
X"
DUDV,0.9443207126948775,"s=1
ρ(t, s)xs,
(126)"
DUDV,0.9454342984409799,where T ∈N+ is the path length.
DUDV,0.9465478841870824,Published as a conference paper at ICLR 2022
DUDV,0.9476614699331849,"E.1
Settings of Figure 1"
DUDV,0.9487750556792873,"For the input, we generate 6 sequences using Gaussian i.i.d. random variables with the path
length T = 30. Hence, the output (target) is Ht(x) = P30
s=1 ρ(t, s)xs."
DUDV,0.9498886414253898,The high rank target has the representation
DUDV,0.9510022271714922,"ρhigh(t, s) =
cos(t),
t = s,
0,
t ̸= s,
(127)"
DUDV,0.9521158129175946,while the low rank target has the representation
DUDV,0.9532293986636972,"ρlow(t, s) ="
X,0.9543429844097996,"99
X n=0"
X,0.955456570155902,"1
n + 1 cos(nπt) cos(nπs).
(128)"
X,0.9565701559020044,"E.2
Settings of Figure 2"
X,0.9576837416481069,Consider the target with the following representation
X,0.9587973273942093,"ρ(t, s) = e−t"
X,0.9599109131403119,c0 e−s
X,0.9610244988864143,c0 R(e−t
X,0.9621380846325167,"c0 , e−s"
X,0.9632516703786191,"c0 ),
(129) where"
X,0.9643652561247216,"R(u, v) = ∞
X"
X,0.965478841870824,"n=1
σnϕn(u)φn(v),
(130)"
X,0.9665924276169265,"with both {ϕk} and {φk} as orthonormal bases. In this way, we construct a target with
singular values {σk}. Under the discrete-time setting, we are supposed to use the following
linear, width-m RNN encoder-decoder"
X,0.967706013363029,"ˆ
Ht(x) = τ
X"
X,0.9688195991091314,"s=1
c⊤V tPQW s−1Ux(τ −s),
(131)"
X,0.9699331848552338,"where P ∈Rm×N, Q ∈RN×m with m = mD = mE."
X,0.9710467706013363,Recall that the approximation error is derived as
X,0.9721603563474388,"∥H −ˆ
H∥≲∥R −ˆR∥L1([0,1]2) ≤∥R −ˆR∥L2([0,1]2),
(132)"
X,0.9732739420935412,"where ˆR(u, v) := PN0
i=1 ˜ϕi(u)˜φi(v) ∈P2
m with ˜ϕn, ˜φn ∈Pm. In the numerical experiments,
we ﬁrst construct an R(u, v), and then ﬁt it with the polynomial ˆR(u, v) using the method
of least squares. The norm ∥R −ˆR∥L2([0,1]2) is used to evaluate the approximation error."
X,0.9743875278396437,"In the particular example reported in Figure 2, we set m = 128, ϕn(u) =
√"
X,0.9755011135857461,2 sin(nπu)
X,0.9766146993318485,"and φn(v) =
√"
X,0.977728285077951,"2 sin(nπv). The singular values are taken as: (a) σn =
n−1"
X,0.9788418708240535,"8 ,
n ≤N0
0,
n > N0
;"
X,0.9799554565701559,"(b) σn =
n−1,
n ≤N0
0,
n > N0
; (c) σn = n−2, with N0 = 2, 4, 6, 8. As an inﬁnite sum, R is"
X,0.9810690423162584,constructed under a ﬁnite truncation with the ﬁrst 50 terms.
X,0.9821826280623608,"E.3
Settings of Figure 3"
X,0.9832962138084632,"We perform experiments on nonlinear targets to show that the insight of low rank approxi-
mation also holds in the nonlinear case."
X,0.9844097995545658,"Nonlinear target.
We consider the forced Lorenz 96 system (Lorenz, 1996), which is an
important example of reduced order modelling for convection dynamics, with applications
in weather forecasting."
X,0.9855233853006682,"Mathematically, the system has K output variables {yk} and JK hidden variables {zj,k}
with k = 1, 2, . . . , K and j = 1, 2, . . . , J.
The parameters K, J control the number of"
X,0.9866369710467706,Published as a conference paper at ICLR 2022
X,0.987750556792873,"variables in the system, and can be viewed as a complexity measure. The input {xk} is an
external temporal forcing. The system satisﬁes the following dynamics dyk"
X,0.9888641425389755,"dt = −yk−1(yk−2 −yk+1) −yk + xk −1 J J
X"
X,0.9899777282850779,"j=1
zj,k,
(133) dzj,k"
X,0.9910913140311804,"dt
= −zj+1,k(zj+2,k −zj−1,k) −zj,k + yk,
(134)"
X,0.9922048997772829,"with cyclic indices yk+K = yk, zj,k+K = zj,k and zj+J,k = zj,k. Here, we take {xk} as
randomly generated input sequences with the path length 64. We have tested for several
cases with diﬀerent parameters: i) J = 6 with K = 1, 5, 10, 20; ii) K = 5 with J =
5, 15, 25, 100."
X,0.9933184855233853,Note that the forced Lorenz 96 system parameterizes a highly nonlinear functional.
X,0.9944320712694877,"Nonlinear model.
We learn the above system using RNN encoder-decoders with nonlin-
ear activations, i.e."
X,0.9955456570155902,"hs = σ(WEhs−1 + UExs + bE),
v = σ(Qhτ + b1),
gt = σ(WDgt−1 + bD),
g0 = σ(Pv + b2),
ot = WOgt + bO,
(135)"
X,0.9966592427616926,"where σ is the element-wise tanh activation. Let m = 128 be the hidden dimension, N =
1, 2, . . . , 32 be the size of the coding vector v, we have xs, ot, bO ∈RK, hs, bE, bD, b2 ∈Rm,
WE, WD ∈Rm×m, b1 ∈RN, WO ∈Rm×K, Q ∈Rm×N, P ∈RN×m. Note that we construct
the model with a ﬁxed hidden dimension m but diﬀerent N, thus only sizes of Q, P, b1, b2
are varying, while sizes of other parameters remain unchanged."
X,0.9977728285077951,"Training and initialisation.
We denote the model with the coding vector size N as
EncDec(N). We utilise the Adam optimiser and train from EncDec(1) to EncDec(32). For
EncDec(1), we use a normal random initialisation, and train for 3000 epoches until a stable
error. For EncDec(N) with N > 1, we use the parameters trained from EncDec(N−1) as the
initialisation. For the parameters Q, P, b1, b2, we pad them to match the size of EncDec(N)
with normal distributions as initialisations."
X,0.9988864142538976,"It is shown that the low rank approximation phenomena discovered in the linear setting also
appears in this nonlinear case."
