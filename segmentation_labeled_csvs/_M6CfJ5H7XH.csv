Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.010101010101010102,"Deep learning classifiers are prone to latching onto dominant confounders present in a
dataset rather than on the causal markers associated with the target class, leading to poor
generalization and biased predictions. Although explainability via counterfactual image
generation has been successful at exposing the problem, bias mitigation strategies that
permit accurate explainability in the presence of dominant and diverse artifacts remain
unsolved. In this work, we propose the DeCoDEx framework and show how an external,
pre-trained binary artifact detector can be leveraged during inference to guide a diffusion-
based counterfactual image generator towards accurate explainability. Experiments on the
CheXpert dataset, using both synthetic artifacts and real visual artifacts (support devices),
show that the proposed method successfully synthesizes the counterfactual images that
change the causal pathology markers associated with Pleural Effusion while preserving
or ignoring the visual artifacts. Augmentation of ERM and Group-DRO classifiers with
the DeCoDEx generated images substantially improves the results across underrepresented
groups that are out of distribution for each class. The code is made publicly available at
https://github.com/NimaFathi/DeCoDEx.
Keywords: Bias Mitigation, Causality, Counterfactual Image Synthesis, Diffusion, Ex-
plainability, Spurious Correlations"
INTRODUCTION,0.020202020202020204,1. Introduction
INTRODUCTION,0.030303030303030304,"Deep learning (DL) methods have shown tremendous success in a wide variety of medical
image tasks, including disease classification, due to their ability to learn generalizable,
discriminative features across subjects. However, DL models are prone to learning shortcuts
in order to obtain high overall accuracies, including any prevalent visual artifacts (e.g. marks
in the image (DeGrave et al., 2021)) that are correlated with, but not causal of, the target
outcome. Models that have not learned the relevant causal visual markers (Jia and Liang,
2017; Zech et al., 2018) are right for the wrong reasons (Sun et al., 2023b,a), and fail to"
INTRODUCTION,0.04040404040404041,∗Contributed equally
INTRODUCTION,0.050505050505050504,Fathi Kumar Nichyporuk Havaei Arbel
INTRODUCTION,0.06060606060606061,"generalize across out-of-distribution subgroups
(Geirhos et al., 2018, 2020). Explainable
DL models that not only expose these biases but mitigate them are required in order to
ensure their trustworthiness for safe clinical deployment.
Counterfactual (CF) image generation methods (e.g. Gifsplanation (Cohen et al., 2021)
and Attri-Net (Sun et al., 2023b,a)) have recently been successful at exposing when the
classifier is latching onto spurious correlations in order to obtain high performance. These
methods employ conditional generation of the counterfactual image when the classifier has
the opposing target outcome. Differences between the factual and counterfactual images
should reflect the predictive local markers indicative of the class label, but also expose
the classifier’s reliance on spurious correlations. These methods do not mitigate the biases
nor address their poor generalization. A number of debiasing methods (Sagawa et al., 2020;
Wang et al., 2020; Sarhan et al., 2020) have recently been successful in several medical imag-
ing contexts. Recent work (Kumar et al., 2023) combined Cycle-GAN counterfactual image
generation and a Group-DRO (Sagawa et al., 2020) classifier to expose and mitigate the
biases. The results showed improvement for minority subgroups, and classification based on
disease-specific features. However, debiasing techniques have a number of known drawbacks,
including improved fairness at the expense of a reduction in the performance in the ma-
jority subgroup. Integrating them into the counterfactual models requires sub-group labels
for each class during training (which is often unavailable), and GANs require retraining the
generative model with the pre-trained debiasing classifier in order to provide supervision for
the counterfactual synthesis, which makes the process inflexible. Recently, an unconditional
DDPM (Denoising Diffusion Probabilistic Model) (Ho et al., 2020), DiME (Jeanneret et al.,
2022), has been proposed to generate classifier-guided counterfactual explanations, however,
with the classifier latching onto shortcuts present in the dataset. Overall, developing bias
mitigation strategies that permit accurate explainability in the presence of dominant and
diverse visual artifacts remain open research questions.
This paper introduces DeCoDEx, a diffusion-based (DDPM) counterfactual image gen-
erator for debiased classifier explainability in the presence of dominant and diverse visual
artifacts. DeCoDEx overcomes a number of limitations of current approaches: Rather than
requiring training specialized debiasing classifiers for known subgroups, and then retraining
the counterfactual generator that uses them (e.g. GANs), the framework provides debiased
explainability of the classifier in question at inference time by leveraging the flexibility and
explicit inference procedure (Dhariwal and Nichol, 2021; Wang et al., 2022; Kazerouni et al.,
2022) of DDPMs, is generalizable to any subgroups, and shows stable training as compared
to GANs. The framework can make use of any pre-trained, binary detector trained to in-
dicate the presence or absence of the visual artifact in question (for any classes). During
inference, the detector guides the diffusion-based counterfactual image generator towards
accurate explainability, as the gradients from the detector counter the gradients from the
classifier away from spurious correlations.
Extensive experiments are performed on the publicly available CheXpert dataset (Irvin
et al., 2019), using both synthetic artifacts and real visual artifacts (support devices). Qual-
itative results show that the proposed method successfully synthesizes the counterfactual
images by making changes in the pathology associated with Pleural Effusion while preserv-
ing or ignoring the visual artifacts. The quality of the counterfactual images was measured
via several metrics, such as L1 distance, Counterfactual Prediction Gain (CPG) and Spuri-"
INTRODUCTION,0.0707070707070707,DeCoDEx
INTRODUCTION,0.08080808080808081,"ous Correlation Latching Score (Kumar et al., 2023). The results indicated the strength of
framework over a baseline (without a detector). Augmentation of the dataset with counter-
factual images synthesized with DeCoDEx improves ERM and Group-DRO classification
for the minority subgroups."
IMPLEMENTATION/METHODS,0.09090909090909091,2. Methodology
IMPLEMENTATION/METHODS,0.10101010101010101,"The DeCoDEX framework involves a training strategy for explainability via counterfactual
image synthesis, while ensuring robustness to spurious correlations. The model consists of a
classifier and a trained unconditional denoising diffusion probabilistic model (DDPM). The
framework leverages a pre-trained visual artifact detector that guides a DDPM to synthesize
counterfactuals while ignoring the artifact."
IMPLEMENTATION/METHODS,0.1111111111111111,"Figure 1: CF explanations for a subject with Pleural Effusion in the presence of an artifact:
(a) Chest radiograph of a sick patient: dot artifact, disease pathology; (b) CF
image from biased classifier using DDPM (i.e. DeCoDEx without detector) main-
tains the diseased area but modifies the dot; (c) DeCoDEx CF image modifies
the Pleural Effusion area to look healthy as expected (Huang et al., 2022; Wang
et al., 2017) while ignoring the dot artifact."
IMPLEMENTATION/METHODS,0.12121212121212122,"One of the advantages of our approach is the flexibility to use any pre-trained detector
that can identify spurious correlations in the input sample. During the counterfactual image
generation, when the biased classifier relies on spurious non-causal features in the dataset,
the detector’s gradient reversal signal readjusts the generation process, steering it back
toward focusing on relevant pathological features (Fig. 1). It should be noted here that if
there are several spurious correlations in the dataset that are difficult to detect, the detector
may only block only some of them. Therefore, for some difficult cases, even after using a
detector the counterfactual images may fail to make changes in the area correlated with the
target class. An overview of our method is shown in Fig. 2.
Counterfactual Image Generation: The counterfactual image generation is designed to
adhere to a set of constraints (Mothilal et al., 2020; Nemirovsky et al., 2020) through the
following loss functions: (i)Identity preservation loss, Lperc: counterfactual images preserves
the identity of the factual image, Lperc; (ii) Classifier consistency loss, Lclass: counterfactual
image belongs to the correct target class. An additional Detector loss term, Ldet, is intro-"
IMPLEMENTATION/METHODS,0.13131313131313133,Fathi Kumar Nichyporuk Havaei Arbel
IMPLEMENTATION/METHODS,0.1414141414141414,"Figure 2: DeCoDEx Framework: Generating the counterfactuals (CFs) involves several in-
ference steps. At each step, there are several components: (1) Denoising via un-
conditional DDPM, (2) pretrained classifier and detector loss, (3) gradient of the
classifier, detector and perceptual loss, (4) counterfactual synthesis via sampling
and backpropagating loss from black-box classifier and detector. The classifier,
detector and unconditional DDPM are all pre-trained components. The resulting
CF makes changes to the disease markers while disregarding visual artifacts."
IMPLEMENTATION/METHODS,0.15151515151515152,"duced to help guide the generation away from the spurious correlations that the classifier
would have otherwise latched onto.
The generative model for synthesizing counterfactual images use a pre-trained DDPM
with classifier guidance. DDPMs operate through a forward diffusion process (Eq. 1), which
incrementally corrupts the original image x (or z0) by adding Gaussian noise, culminating
in a highly noised image zT over T timesteps."
IMPLEMENTATION/METHODS,0.16161616161616163,"zt ∼N
p"
IMPLEMENTATION/METHODS,0.1717171717171717,"1 −βtzt−1, βtI

,
(1)"
IMPLEMENTATION/METHODS,0.18181818181818182,"where βt are predefined noise levels. The reverse diffusion process (Eq. 2) aims to recover
the original image from zT , using a neural network to predict and subtract the added noise
iteratively."
IMPLEMENTATION/METHODS,0.1919191919191919,"xt−1 =
1
√αt"
IMPLEMENTATION/METHODS,0.20202020202020202,"
xt −1 −αt
√1 −¯αt
ϵθ(xt, t)

,
(2)"
IMPLEMENTATION/METHODS,0.21212121212121213,"where αt := Qt
k=1(1 −βk) and ϵθ(xt, t) is the noise estimated by the network at step t.
Guided-diffusion sampling (Dhariwal and Nichol, 2021) is used to denoise the image at any
time step t, given by zt−1 ∼N (µ(zt) −Σ(zt)∇ztL(zt; yc, ys, xt), Σ(zt)). The complete loss
function is given by:"
IMPLEMENTATION/METHODS,0.2222222222222222,"L(xt; yc, ys, x) = λcLclass (C(yc|xt)) + λdLdet (D(ys|xt)) + λpLperc(xt, x),
(3)"
IMPLEMENTATION/METHODS,0.23232323232323232,"where C and D refer to our classifier and detector modules, yc and ys refer to the target
labels of the class and the presence of spurious correlation and λc, λd, λp are hyperparmeters."
IMPLEMENTATION/METHODS,0.24242424242424243,DeCoDEx
IMPLEMENTATION/METHODS,0.25252525252525254,"Figure 3: Majority and Minority subgroup samples from the Dot dataset (top row) and
Device dataset (bottom row). Red boxes show the location of the artifacts."
IMPLEMENTATION/METHODS,0.26262626262626265,"Finally, the gradient of the complete loss function can be expressed as follows:"
IMPLEMENTATION/METHODS,0.2727272727272727,"∇ztL(zt; yc, ys, xt) =
1
√αt
∇xtL(xt; yc, ys, x).
(4)"
RESULTS/EXPERIMENTS,0.2828282828282828,3. Experiments and Results
RESULTS/EXPERIMENTS,0.29292929292929293,3.1. Dataset and Implementation Details
RESULTS/EXPERIMENTS,0.30303030303030304,"We perform experiments on the publicly available CheXpert dataset (Irvin et al., 2019) that
contains over 200,000 chest X-ray images, with binary labels for 14 diseases (e.g. Pleural
Effusion, Cardiomegaly, Pneumonia), as well as binary labels indicating the presence of
support devices (visual artifacts). We create two variants of the CheXpert dataset: (i) Dot
dataset: We introduce a synthetic artifact, a black dot of radius 9 pixels, in the center of
the image for evaluating the quality of counterfactual images in the presence of spurious
correlations. This also helps to compare the behaviour of counterfactual images synthesized
from the baseline method: DeCoDEx without a detector and the proposed DeCoDEx. (ii)
Device dataset: A subset of the CheXpert data is used to demonstrate the performance of
DeCoDEx in the presence of real artifacts (Support devices). In both datasets, the artifacts
are present in the majority of images of subjects with Pleural Effusion and in the minority
of images of healthy subjects. In contrast, the majority of the images of healthy subjects
and the minority of the images of subjects with Pleural Effusion do not contain these
artifacts. Fig. 3 shows the sample images from both the datasets and all four subgroups.
For both datasets, the ratio of the number of samples in majority to minority is 90:10 and
the dataset is divided into training/validation/testing with a 70/15/15 random split. The
details of number of samples in different split is included in Appendix A.
The DenseNet-121 (Huang et al., 2017) architecture is used to train the classifier and
detector.
The classifier is trained separately on Dot and Device datasets.
We use the
standard Empirical Risk Minimization (ERM) (Sagawa et al., 2020) as the optimization"
RESULTS/EXPERIMENTS,0.31313131313131315,Fathi Kumar Nichyporuk Havaei Arbel
RESULTS/EXPERIMENTS,0.32323232323232326,"Figure 4: Qualitative comparison of counterfactual images synthesized via Baseline (i.e.
DeCoDEx without detector) and DeCoDEx. For the baseline, most of the changes
were made to the spurious correlation but for DeCoDEx visual artifacts were
ignored and changes pertained to disease pathology."
RESULTS/EXPERIMENTS,0.3333333333333333,"method. The binary detector indicating the presence/absence of support devices is pre-
trained on the entire CheXpert dataset (except the held out test set). DeCoDEx is capable
of handling images with multiple support devices, each varying in type, shape, size, location
and intensities. An analysis showing the performance of the detector via counterfactual
image generation is discussed in Appendix B."
RESULTS/EXPERIMENTS,0.3434343434343434,3.2. Metrics and Experiments to evaluate counterfactuals
RESULTS/EXPERIMENTS,0.35353535353535354,"Several metrics are used to evaluate the quality of counterfactual images: (i) Subject Identity
Preservation: L1 Score as depicted by the L1 distance between the counterfactual and the
factual (original) image (as in (Mothilal et al., 2020; Nemirovsky et al., 2020)); (ii) Coun-
terfactual Prediction Gain (CPG) (Nemirovsky et al., 2020) which measures the absolute
value of the difference in the prediction of the classifier on the factual and counterfactual
images (a higher score indicates a maximal change in the classifier decision boundary); (iii)
Spurious Correlation Latching Score (SCLS)) (Kumar et al., 2023) assesses whether the
spurious correlation was preserved in the counterfactual image (a lower SCLS score is desir-
able); (iv) Classifier Flip Rate (CFR) which represent the number of samples that flipped
their class as per the classifier and (v) Detector Robustness Rate (DRR) showing number
of samples that were robust to the detector.
In order to show that the synthesized counterfactuals learn useful features associated
with the disease, the training data for the original classifier was augmented with synthesized
images. The ERM classifier is retrained with the augmented data. An increase in perfor-
mance indicates that synthesized images learned discriminative features generalizable to the"
RESULTS/EXPERIMENTS,0.36363636363636365,0. Only the training dataset is augmented while the validation and test split remains the same.
RESULTS/EXPERIMENTS,0.37373737373737376,DeCoDEx
RESULTS/EXPERIMENTS,0.3838383838383838,"CFR↑
DRR↑
L1↓
CPG↑
SCLS↓
Dot dataset
Baseline
0.8975
0.2575
0.038
0.592
0.7394
DeCoDEx
1
0.9775
0.036
0.559
0.058
Device dataset
Baseline
0.97
0.7625
0.040
0.377
0.201
DeCoDEx
0.89
0.99
0.035
0.529
0.068"
RESULTS/EXPERIMENTS,0.3939393939393939,"Table 1: Quantitative results comparing the scores for the counterfactuals generated by
the Baseline and DeCoDEx on both datasets.
Notice the high DRR and low
SCLS values for DeCoDEx showing the spurious correlation were ignored in the
counterfactual images."
RESULTS/EXPERIMENTS,0.40404040404040403,"subgroups. We augment the Dot dataset with 200 counterfactual images and the Device
dataset with 600 counterfactual images synthesized using basline method and DeCoDEx.
For completeness, the augmentation experiments are repeated for a debiasing Group-DRO
classifier."
RESULTS/EXPERIMENTS,0.41414141414141414,3.3. Results
RESULTS/EXPERIMENTS,0.42424242424242425,"Classifier and Detector evaluation The classifier’s performance on both datasets is
shown in Table 2 on the row labeled ‘ERM’. Note that the performance on the minority
subgroup samples is significantly lower than the majority for both datasets. The dot and
device detectors perform very well with subgroups [majorityS, minorityS, minorityH,
majorityH] accuracies of [100, 99.9, 100, 99.8] and [91.8, 88.8, 79.2, 88.7]. Perfect accuracies
for the dot detector can be expected given that the position and size of the dot remain fixed
for all the subjects. However, the variability including size, position, intensity of support
devices can be large, making detection much more challenging.
Qualitative evaluation Pleural effusion (PE) is characterized by the rounding of the
costophrenic angle, augmented lung opacity, and reduced clarity of the diaphragm and lung
fissures (Light, 2002). It is observed in the lower corner of the lungs (Wang et al., 2017;
Huang et al., 2022). Qualitative results for counterfactual generation from the models can
be seen in Fig. 4. For the baseline method on the Dot dataset, the counterfactual for a
healthy subject simply added dots to the image, indicating that the classifier has latched
onto the dot artifact. However, DeCoDEx ignored the dot artifact (and maintained the
original text artifact) and made changes reflective of PE disease. For the Device dataset,
the baseline generated a counterfactual for a sick patient from the majority subgroup by
simply removing the support device(s) while DeCoDEx makes (correct) changes in the area
associated with the disease. Therefore, DeCoDEx indeed ignores the spurious correlation.
Quantitative evaluation In Table 1, CF images generated by the baseline and DeCoDEx
show similar results for the metric L1 indicating CF images are similar to the factual im-
age. CF synthesized by DeCoDEx have SCLS score close to zero indicating that the artifact
was preserved in the counterfactual images. Table 2 shows the results of the augmentation
experiments. Augmented training significantly improves the performance over the minority"
RESULTS/EXPERIMENTS,0.43434343434343436,Fathi Kumar Nichyporuk Havaei Arbel
RESULTS/EXPERIMENTS,0.4444444444444444,Dot dataset
RESULTS/EXPERIMENTS,0.45454545454545453,"Pleural Effusion
Healthy
Dot
No Dot
Dot
No Dot
ERM
97
2.2
8
100
ERM augmented with Baseline CFs
98.5
0
16.6
100
ERM augmented with DeCoDEx CFs
90.6
12.1
53.1
98.0
Group-DRO
90
61.8
56.0
88.0
Group-DRO augmented with Baseline CFs
97
8.0
33.0
98.9
Group-DRO augmented with DeCoDEx CFs
91.0
70.2
60.9
80.3
Device dataset"
RESULTS/EXPERIMENTS,0.46464646464646464,"Pleural Effusion
Healthy
Support Device
No Support Device
Support Device
No Support Device
ERM
92.7
75.2
84.9
87.0
ERM augmented with Baseline CFs
92.5
74.8
83.4
87.0
ERM augmented with DeCoDEx CFs
92.6
76.3
85.9
86.8
Group-DRO
92.7
83.4
77.3
88.4
Group-DRO augmented with Baseline CFs
97.8
85.4
55.9
72.0
Group-DRO augmented with DeCoDEx CFs
93.2
84.7
79.0
88.4"
RESULTS/EXPERIMENTS,0.47474747474747475,"Table 2: Augmented Classifier Accuracies: CFs generated by DeCoDEx and the Baseline
are used to augment the imbalanced datasets. Both ERM and Group-DRO are
retrained on these augmented datasets and the effects are examined. The accu-
racies (percentages) of all classifiers are shown on the held out test set. Green
indicates majority subgroups (90%) and red are the minority subgroups (10%).
The best results are in bold. Note the increase in performance for the minority
classes when both the methods, ERM and Group-DRO are augmented with De-
CoDEx counterfactual images, illustrating the power of our method on extracting
disease pathology and generating better counterfactual explanations."
RESULTS/EXPERIMENTS,0.48484848484848486,"groups for the Dot dataset, indicating that CF samples have learned discriminative fea-
tures common to this subgroup. Among both ERM and Group-DRO based techniques,
our method outperforms the Baseline along with augmented CFs. CF synthesized from
DeCoDEx when augmented with ERM performs as well as Group-DRO for some minority
classes which is a positive result, particularly when debiasing cannot be performed due to
the lack of annotations."
CONCLUSION/DISCUSSION,0.494949494949495,4. Conclusions
CONCLUSION/DISCUSSION,0.5050505050505051,"In medical image analysis, explainable models are needed to expose and mitigate the bias to
improve the trustworthiness of complex models. This paper presents DeCoDEx, an explain-
ability framework that leverages a pre-trained classifier and detector to guide diffusion-based
counterfactual synthesis towards accurate disease markers while ignoring spurious correla-
tions. Qualitative and quantitative analysis of our extensive experiments indicate that the
proposed method outperforms the baseline model that does not use a detector. Further-
more, the flexibility of our method allows it to be used with any pre-trained detector, does
not require retraining a debiasing classifier and associated generative architecture, and pro-
vides guidance during inference. One of the current limitations of the model is resulting
minor changes throughout the generated CF images. Future work will explore conditional
score-based generative models."
CONCLUSION/DISCUSSION,0.5151515151515151,DeCoDEx
CONCLUSION/DISCUSSION,0.5252525252525253,Acknowledgments
CONCLUSION/DISCUSSION,0.5353535353535354,"The authors are grateful for funding provided by the Natural Sciences and Engineering
Research Council of Canada, the Canadian Institute for Advanced Research (CIFAR) Ar-
tificial Intelligence Chairs program, Mila - Quebec AI Institute, Google Research, Calcul
Quebec, and the Digital Research Alliance of Canada."
REFERENCES,0.5454545454545454,References
REFERENCES,0.5555555555555556,"Joseph Paul Cohen, Rupert Brooks, Sovann En, Evan Zucker, Anuj Pareek, Matthew P
Lungren, and Akshay Chaudhari. Gifsplanation via latent shift: a simple autoencoder
approach to counterfactual generation for chest x-rays. In Medical Imaging with Deep
Learning, pages 74–104. PMLR, 2021."
REFERENCES,0.5656565656565656,"Alex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detection
selects shortcuts over signal. Nature Machine Intelligence, 3(7):610–619, 2021."
REFERENCES,0.5757575757575758,"Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.
Advances in neural information processing systems, 34:8780–8794, 2021."
REFERENCES,0.5858585858585859,"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann,
and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape
bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018."
REFERENCES,0.5959595959595959,"Robert Geirhos, J¨orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks.
Nature Machine Intelligence, 2(11):665–673, 2020."
REFERENCES,0.6060606060606061,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
Advances in Neural Information Processing Systems, volume 33, pages 6840–6851, 2020."
REFERENCES,0.6161616161616161,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely
connected convolutional networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4700–4708, 2017."
REFERENCES,0.6262626262626263,"Tao Huang, Rui Yang, Longbin Shen, Aozi Feng, Li Li, Ningxia He, Shuna Li, Liying Huang,
and Jun Lyu. Deep transfer learning to quantify pleural effusion severity in chest x-rays.
BMC Medical Imaging, 22(1):1–11, 2022."
REFERENCES,0.6363636363636364,"Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute,
Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert:
A large chest radiograph dataset with uncertainty labels and expert comparison.
In
Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 590–597,
2019."
REFERENCES,0.6464646464646465,"Guillaume Jeanneret, Lo¨ıc Simon, and Fr´ed´eric Jurie. Diffusion models for counterfactual
explanations. In Proceedings of the Asian Conference on Computer Vision, pages 858–
876, 2022."
REFERENCES,0.6565656565656566,Fathi Kumar Nichyporuk Havaei Arbel
REFERENCES,0.6666666666666666,"Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension
systems. arXiv preprint arXiv:1707.07328, 2017."
REFERENCES,0.6767676767676768,"Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen
Fayyaz, Ilker Hacihaliloglu, and Dorit Merhof. Diffusion models for medical image anal-
ysis: A comprehensive survey. arXiv preprint arXiv:2211.07804, 2022."
REFERENCES,0.6868686868686869,"Amar Kumar, Nima Fathi, Raghav Mehta, Brennan Nichyporuk, Jean-Pierre R Falet,
Sotirios Tsaftaris, and Tal Arbel. Debiasing counterfactuals in the presence of spurious
correlations. In Workshop on Clinical Image-Based Procedures, pages 276–286. Springer,
2023."
REFERENCES,0.696969696969697,"Richard W Light. Pleural effusion. New England Journal of Medicine, 346(25):1971–1977,
2002."
REFERENCES,0.7070707070707071,"Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning
classifiers through diverse counterfactual explanations. In Proceedings of the 2020 con-
ference on fairness, accountability, and transparency, pages 607–617, 2020."
REFERENCES,0.7171717171717171,"Daniel Nemirovsky, Nicolas Thiebaut, Ye Xu, and Abhishek Gupta. Countergan: Gener-
ating realistic counterfactuals with residual generative adversarial nets. arXiv preprint
arXiv:2009.05199, 2020."
REFERENCES,0.7272727272727273,"Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally
robust neural networks for group shifts: On the importance of regularization for worst-
case generalization, 2020."
REFERENCES,0.7373737373737373,"Mhd Hasan Sarhan, Nassir Navab, Abouzar Eslami, and Shadi Albarqouni. Fairness by
learning orthogonal disentangled representations. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIX 16,
pages 746–761. Springer, 2020."
REFERENCES,0.7474747474747475,"Susu Sun, Lisa M Koch, and Christian F Baumgartner. Right for the wrong reason: Can
interpretable ml techniques detect spurious correlations? In International Conference on
Medical Image Computing and Computer-Assisted Intervention, pages 425–434. Springer,
2023a."
REFERENCES,0.7575757575757576,"Susu Sun, Stefano Woerner, Andreas Maier, Lisa M Koch, and Christian F Baumgartner. In-
herently interpretable multi-label classification using class-specific counterfactuals. arXiv
preprint arXiv:2303.00500, 2023b."
REFERENCES,0.7676767676767676,"Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M
Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-
supervised classification and localization of common thorax diseases. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 2097–2106, 2017."
REFERENCES,0.7777777777777778,"Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji
Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies
for bias mitigation. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 8919–8928, 2020."
REFERENCES,0.7878787878787878,DeCoDEx
REFERENCES,0.797979797979798,"Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou.
Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022."
REFERENCES,0.8080808080808081,"John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and
Eric Karl Oermann. Variable generalization performance of a deep learning model to
detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11):
e1002683, 2018."
REFERENCES,0.8181818181818182,Fathi Kumar Nichyporuk Havaei Arbel
OTHER,0.8282828282828283,Appendix A. Detailed dataset description
OTHER,0.8383838383838383,"We elaborate on the datasets and their variants used in our experiments in this appendix,
complemented by statistical data on participant distribution across groups as detailed in
Table 3."
OTHER,0.8484848484848485,Dot dataset
OTHER,0.8585858585858586,"Spurious Correlation
Pleural Effusion
Healthy
Train
Validation
Test
Train
Validation
Test
Dot
1359
191
242
56
5
49
No Dot
166
28
242
340
51
49
Device dataset"
OTHER,0.8686868686868687,"Spurious Correlation
Pleural Effusion
Healthy
Train
Validation
Test
Train
Validation
Test
Support Device(s)
6653
1432
1389
665
143
138
No Support Device(s)
665
143
138
6653
1432
1389"
OTHER,0.8787878787878788,"Table 3:
Summary of the number of samples for both dataset variants"
OTHER,0.8888888888888888,Appendix B. Explainability: Providing insight into the detector result
OTHER,0.898989898989899,"We wish to provide some insights into the workings of the artifact/medical device detector.
Fig. 5 shows two examples of explainability via counterfactual image generation illustrating
the correct working of the detector. In both examples, the binary classifier was correctly
focusing on the support devices. These are removed in the counterfactual images in order
to flip the decision of the binary classifier."
OTHER,0.9090909090909091,"Figure 5: CF explanations for the detector: Removing medical devices from the original
images while explaining the detector. Note the disease state is maintained in the
counterfactual image."
OTHER,0.9191919191919192,DeCoDEx
OTHER,0.9292929292929293,"Appendix C. Extensive augmentation of the minority subgroup with
synthesized CFs"
OTHER,0.9393939393939394,Dot dataset
OTHER,0.9494949494949495,"Pleural Effusion
Healthy
Dot
No Dot
Dot
No Dot
ERM augmented with DeCoDEx CFs [200]
90.6
12.1
53.1
98.0
ERM augmented with DeCoDEx CFs [400]
93.8
26.5
61.2
98.3
Group-DRO augmented with DeCoDEx CFs [200]
91.0
70.2
60.9
80.3
Group-DRO augmented with DeCoDEx CFs [400]
93.1
81.6
65.3
85.9
Device dataset"
OTHER,0.9595959595959596,"Pleural Effusion
Healthy
Support Device
No Support Device
Support Device
No Support Device
ERM augmented with DeCoDEx CFs [600]
92.6
76.3
85.9
86.8
ERM augmented with DeCoDEx CFs [1600]
92.8
76.8
86.8
82.3
Group-DRO augmented with DeCoDEx CFs [600]
93.2
84.7
79.0
88.4
Group-DRO augmented with DeCoDEx CFs [1600]
93.5
86.5
79.9
90.1"
OTHER,0.9696969696969697,"Table 4: Improved accuracy at subgroup level through extensive classifier augmentation:
Using DeCoDEx, we expanded on counterfactual generation to demonstrate im-
provement in the subgroup accuracy. We synthesise 400 counterfactual samples
for the Dot dataset and 1600 for the Device dataset (the number in the square
bracket refers to the total number of augmentation samples added to both major-
ity and minority subgroups). First row is the original results discussed in Table 2
and the second row shows the result after extensive augmentation. Notice the
improvement in the accuracy of minority subgroups across both Dot and Device
datasets.
Notably, 90% of these counterfactuals represent minority subgroups,
thereby achieving a more equalized distribution in the dataset. Our findings indi-
cate improvement in the accuracy of minority groups across all scenarios."
OTHER,0.9797979797979798,"Appendix D. Validating the Preservation of Patient Sex in the
Synthesized Counterfactual Images"
OTHER,0.98989898989899,"The identity preservation loss in Equation 3 does not guarantee that all the other attributes
of the patients are maintained in the counterfactual images. A quick experiment was per-
formed to validate that the sex of the patients is maintained in the counterfactual images
generated by DeCoDEx.
To this end, a sex classifier, G, is trained on the real images,
and then tested on real and synthesized counterfactual images. The sex classifier had an
AUC-ROC of 0.98 on the real (factual) images. The differences in the sex classifier results
based on the factual (F) and the counterfactual (CF) images, |G(F) −G(CF)|, were 0.08 on
average, indicating that the sex attribute was maintained in the counterfactual images."
