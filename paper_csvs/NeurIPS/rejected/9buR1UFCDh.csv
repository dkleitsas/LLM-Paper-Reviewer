Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022471910112359553,"Value-based reinforcement learning (RL) methods strive to obtain accurate approxi-
1"
ABSTRACT,0.0044943820224719105,"mations of optimal action-value functions. Notoriously, these methods heavily rely
2"
ABSTRACT,0.006741573033707865,"on the application of the optimal Bellman operator, which needs to be approximated
3"
ABSTRACT,0.008988764044943821,"from samples. Most approaches consider only a single Bellman iteration, which
4"
ABSTRACT,0.011235955056179775,"limits their power. In this paper, we introduce Iterated Deep Q-Network (iDQN), a
5"
ABSTRACT,0.01348314606741573,"new DQN-based algorithm that incorporates several consecutive Bellman iterations
6"
ABSTRACT,0.015730337078651686,"into the training loss. iDQN leverages the online network of DQN to build a target
7"
ABSTRACT,0.017977528089887642,"for a second online network, which in turn serves as a target for a third online
8"
ABSTRACT,0.020224719101123594,"network, and so forth, thereby taking into account future Bellman iterations. While
9"
ABSTRACT,0.02247191011235955,"using the same number of gradient steps, iDQN allows for better learning of the
10"
ABSTRACT,0.024719101123595506,"Bellman iterations compared to DQN. We evaluate iDQN against relevant baselines
11"
ABSTRACT,0.02696629213483146,"on 54 Atari 2600 games to showcase its benefit in terms of approximation error and
12"
ABSTRACT,0.029213483146067417,"performance. iDQN outperforms its closest baselines, DQN and Random Ensemble
13"
ABSTRACT,0.03146067415730337,"Mixture, while being orthogonal to more advanced DQN-based approaches.
14"
INTRODUCTION,0.033707865168539325,"1
Introduction
15"
INTRODUCTION,0.035955056179775284,"Deep value-based Reinforcement Learning algorithms have achieved remarkable success in various
16"
INTRODUCTION,0.038202247191011236,"fields, from nuclear physics (Degrave et al., 2022) to construction assembly tasks (Funk et al., 2022).
17"
INTRODUCTION,0.04044943820224719,"These algorithms aim at learning a function as close as possible to the optimal action-value function,
18"
INTRODUCTION,0.04269662921348315,"on which they can build a policy to solve the task at hand. To obtain an accurate estimate of the
19"
INTRODUCTION,0.0449438202247191,"optimal action-value function, the optimal Bellman operator is used to guide the learning procedure in
20"
INTRODUCTION,0.04719101123595506,"the space of Q-functions (Bertsekas, 2019) through successive iterations, starting from any Q-function
21"
INTRODUCTION,0.04943820224719101,"to the optimal action-value function. In Reinforcement Learning, as opposed to Dynamic Programing,
22"
INTRODUCTION,0.051685393258426963,"the reward function and system dynamics are not assumed to be known (Bertsekas, 2015). This forces
23"
INTRODUCTION,0.05393258426966292,"us to approximate the optimal Bellman operator with an empirical Bellman operator. This problem
24"
INTRODUCTION,0.056179775280898875,"has received a lot of attention from the community (Fellows et al. (2021), Van Hasselt et al. (2016)).
25"
INTRODUCTION,0.058426966292134834,"On top of that, the use of function approximation results in the necessity of learning the projection of
26"
INTRODUCTION,0.060674157303370786,"the empirical Bellman operator’s iteration on the space of approximators. In this work, we focus on
27"
INTRODUCTION,0.06292134831460675,"the projection step.
28"
INTRODUCTION,0.0651685393258427,"We propose a way to improve the accuracy of the learned projection by increasing the number of
29"
INTRODUCTION,0.06741573033707865,"gradient steps and samples that each Q-function estimate has been trained on. This idea, implemented
30"
INTRODUCTION,0.0696629213483146,"in the training loss function, uses the same total number of gradient steps and samples than the
31"
INTRODUCTION,0.07191011235955057,"classical approaches. At a given timestep of the learning process, this new loss is composed of the
32"
INTRODUCTION,0.07415730337078652,"consecutive temporal differences corresponding to the following Bellman iterations needed to be
33"
INTRODUCTION,0.07640449438202247,"learned, as opposed to DQN (Mnih et al., 2015), where only one temporal difference related to the
34"
INTRODUCTION,0.07865168539325842,"first projection step is considered. Each temporal difference is learned by a different neural network,
35"
INTRODUCTION,0.08089887640449438,"making this method part of the DQN variants using multiple Q estimates during learning. Those
36"
INTRODUCTION,0.08314606741573034,"consecutive temporal differences are computed in a telescopic manner, where the online network of
37"
INTRODUCTION,0.0853932584269663,"the first temporal difference is used to build a target for the second temporal difference and so on.
38"
INTRODUCTION,0.08764044943820225,"This loss implicitly incurs a hierarchical order between the Q estimates by forcing each Q estimate to
39"
INTRODUCTION,0.0898876404494382,"be the projection of the Bellman iteration corresponding to the previous Q estimate, hence the name
40"
INTRODUCTION,0.09213483146067415,"iterated Deep Q-Network (iDQN). In the following, we start by reviewing algorithms built on top of
41"
INTRODUCTION,0.09438202247191012,"DQN, highlighting their behavior in the space of Q-functions. We then introduce a new approach to
42"
INTRODUCTION,0.09662921348314607,"Q-learning that emerges naturally from a graphical representation of DQN. In Section 5, we show
43"
INTRODUCTION,0.09887640449438202,"the benefit of our method on the Arcade Learning Environment benchmark (Bellemare et al., 2013).
44"
INTRODUCTION,0.10112359550561797,"Our approach outperforms DQN and Random Ensemble Mixture (REM, Agarwal et al. (2020)), its
45"
INTRODUCTION,0.10337078651685393,"closest baselines, establishing iDQN as a relevant method to consider when aggregating significant
46"
INTRODUCTION,0.10561797752808989,"advances to design a powerful value-based agent such as Rainbow (Hessel et al., 2018). We also
47"
INTRODUCTION,0.10786516853932585,"perform further experimental studies to bring evidence of the intuition on which iDQN is built. We
48"
INTRODUCTION,0.1101123595505618,"conclude the paper by discussing the limits of iDQN and pointing at some promising follow-up ideas.
49"
PRELIMINARIES,0.11235955056179775,"2
Preliminaries
50"
PRELIMINARIES,0.1146067415730337,"We consider discounted Markov decision processes (MDPs) defined as M = ⟨S, A, P, R, γ⟩,
51"
PRELIMINARIES,0.11685393258426967,"where S is the state space, A is the action space, P : S × A × S →R is the transition kernel of
52"
PRELIMINARIES,0.11910112359550562,"the dynamics of the system, R : S × A →R is a reward function, and γ ∈[0, 1) is a discount
53"
PRELIMINARIES,0.12134831460674157,"factor (Puterman, 1990). A deterministic policy π : S →A is a function mapping a state to
54"
PRELIMINARIES,0.12359550561797752,"an action, inducing a value function V π(s) ≜E
hP+∞
t=0 γtR(St, π(St))|S0 = s
i
representing the
55"
PRELIMINARIES,0.1258426966292135,"expected cumulative discounted reward starting in state s and following policy π thereafter. Similarly,
56"
PRELIMINARIES,0.12808988764044943,"the action-value function Qπ(s, a) ≜E
hP+∞
t=0 γtR(St, At)|S0 = s, A0 = a, At = π(St)
i
is the
57"
PRELIMINARIES,0.1303370786516854,"expected discounted cumulative reward executing action a in state s, following policy π thereafter.
58"
PRELIMINARIES,0.13258426966292136,"Q-learning aims to find a function Q from which the greedy policy πQ(s) = arg maxa Q(·, a) yields
59"
PRELIMINARIES,0.1348314606741573,"the optimal value function V ∗(·) ≜maxπ:S→A V π(·) (Puterman, 1990). The (optimal) Bellman
60"
PRELIMINARIES,0.13707865168539327,"operator Γ∗is a fundamental tool in RL for obtaining optimal policies, and it is defined as:
61"
PRELIMINARIES,0.1393258426966292,"(Γ∗Q)(s, a) ≜R(s, a) + γ
Z"
PRELIMINARIES,0.14157303370786517,"S
P(s, a, s′) max
a′∈A Q(s′, a′)ds′,
(1)"
PRELIMINARIES,0.14382022471910114,"for all (s, a) ∈S × A. It is well-known that Bellman operators are contraction mappings in L∞-
62"
PRELIMINARIES,0.14606741573033707,"norm, such that their iterative application leads to the fixed point Γ∗Q∗= Q∗in the limit (Bertsekas,
63"
PRELIMINARIES,0.14831460674157304,"2015). We consider using function approximation to represent value functions and denote Θ the
64"
PRELIMINARIES,0.15056179775280898,"space of their parameters. Thus, we define QΘ = {Q(·|θ) : S × A →R|θ ∈Θ} as the set of value
65"
PRELIMINARIES,0.15280898876404495,"functions representable by parameters of Θ.
66"
RELATED WORK,0.1550561797752809,"3
Related Work
67"
RELATED WORK,0.15730337078651685,"To provide an overview of the related work, we propose to view the related algorithms from the
68"
RELATED WORK,0.15955056179775282,"perspective of their behavior in the space of Q-functions, which we denote by Q. Due to the curse of
69"
RELATED WORK,0.16179775280898875,"dimensionality, covering the whole space of Q-functions with function approximators is practically
70"
RELATED WORK,0.16404494382022472,"infeasible, as it requires a large number of parameters. Therefore, the space of representable Q-
71"
RELATED WORK,0.1662921348314607,"functions QΘ only covers a small part of the whole space Q. We illustrate this in Figure 1a by
72"
RELATED WORK,0.16853932584269662,"depicting the space of representable Q-functions as a subspace of Q. One can deduce two properties
73"
RELATED WORK,0.1707865168539326,"from this gap in dimensionality. First, the optimal Q-function Q∗is a priori not representable by any
74"
RELATED WORK,0.17303370786516853,"chosen function approximator. Second, the same is true for the optimal Bellman operator Γ∗applied
75"
RELATED WORK,0.1752808988764045,"to a representable Q-function. That is why in Figure 1a, both functions Q∗and Γ∗Q are drawn
76"
RELATED WORK,0.17752808988764046,"outside of QΘ. Additionally, thanks to the contracting property of the optimal Bellman operator
77"
RELATED WORK,0.1797752808988764,"||Γ∗Q −Q∗||∞≤γ||Q −Q∗||∞, we know that the iterated Q given by Γ∗Q is at least γ closer to
78"
RELATED WORK,0.18202247191011237,"the optimal Q∗than the initial Q (Bertsekas, 2015). The goal of most value-based methods is to learn
79"
RELATED WORK,0.1842696629213483,"a Q-function that is as close as possible to the projection of the optimal Q-function on the space of
80"
RELATED WORK,0.18651685393258427,"representable Q-functions, shown with a dotted line in Figure 1a.
81"
RELATED WORK,0.18876404494382024,"This perspective allows us to represent a variety of Q-learning algorithms proposed so far in an
82"
RELATED WORK,0.19101123595505617,"intuitive way in a single picture. For example, Figure 1a depicts how Deep Q-Network (DQN) by
83"
RELATED WORK,0.19325842696629214,"Mnih et al. (2015) works. With a target network ¯Q0, DQN aims at learning the iterated target network
84"
RELATED WORK,0.19550561797752808,"Γ∗¯Q0, also called “target”, using an online network Q1. The loss used during training is shown in red.
85 loss"
RELATED WORK,0.19775280898876405,"(a) DQN learns the optimal Bellman iteration of the
Q-function represented by the target network ¯Q0. It
uses an online network Q1 for that purpose. loss"
RELATED WORK,0.2,"(b) DQN updates the target network to make the target
change. This means moving one Bellman iteration
forward in the space of Q-functions."
RELATED WORK,0.20224719101123595,Figure 1: Graphical representation of DQN in the space of Q-functions.
RELATED WORK,0.20449438202247192,"(a) Two common ways of improving DQN is to develop
a more efficient empirical Bellman operator, noted ˜Γ ¯Q,
or to modify the space of representable Q-functions,
noted ˜QΘ. loss"
RELATED WORK,0.20674157303370785,"(b) REM uses multiple Q-functions to explore areas
in the space of Q-functions. A random convex com-
bination of the stored Q-functions is sampled at each
gradient step."
RELATED WORK,0.20898876404494382,Figure 2: Graphical representations of DQN variants in the space of Q-functions.
RELATED WORK,0.21123595505617979,"In the optimal case, after a pre-defined number of training steps, the online network should represent
86"
RELATED WORK,0.21348314606741572,"the projection of the iterated target network onto the space of representable Q-functions (shown with
87"
RELATED WORK,0.2157303370786517,"a dotted line). This perspective also gives a way to understand the hyper-parameter related to the
88"
RELATED WORK,0.21797752808988763,"frequency at which the target network is updated. It is the number of training steps before moving to
89"
RELATED WORK,0.2202247191011236,"the next Bellman iteration. When the target network is updated, it will be equal to the online network,
90"
RELATED WORK,0.22247191011235956,"and the next Bellman iteration will be computed from there, as shown in Figure 1b. It is important to
91"
RELATED WORK,0.2247191011235955,"note that in DQN, the empirical Bellman operator is used instead of the optimal Bellman operator.
92"
RELATED WORK,0.22696629213483147,"The term included in the loss at every gradient step is a stochastic estimation of the optimal Bellman
93"
RELATED WORK,0.2292134831460674,"iteration.
94"
DQN VARIANTS,0.23146067415730337,"3.1
DQN Variants
95"
DQN VARIANTS,0.23370786516853934,"The DQN paper has inspired the community to develop further methods which improve its efficiency.
96"
DQN VARIANTS,0.23595505617977527,"A large number of those algorithms focuses on using a better empirical Bellman operator (Van Hasselt
97"
DQN VARIANTS,0.23820224719101124,"et al. (2016), Fellows et al. (2021), Sutton (1988)). For instance, double DQN (Van Hasselt et al.,
98"
DQN VARIANTS,0.24044943820224718,"2016) uses an empirical Bellman operator designed to avoid overestimating the return. This results
99"
DQN VARIANTS,0.24269662921348314,"in a different location of the Bellman iteration, as shown in Figure 2a. Other approaches consider
100"
DQN VARIANTS,0.2449438202247191,"changing the space of representable Q-functions (Wang et al. (2016), Osband et al. (2016)). The hope
101"
DQN VARIANTS,0.24719101123595505,"is that the projection of Q∗on the space of representable Q-function is closer than for the classical
102"
DQN VARIANTS,0.24943820224719102,"neural network architecture chosen in DQN. It is important to note that adding a single neuron to
103"
DQN VARIANTS,0.251685393258427,"one architecture layer can significantly change the space of representable Q-function. Wang et al.
104"
DQN VARIANTS,0.2539325842696629,"(2016) showed that performance can be increased by including inductive bias in the neural network
105"
DQN VARIANTS,0.25617977528089886,"architecture. This idea can be understood as a modification of the space of Q-functions, as shown
106"
DQN VARIANTS,0.25842696629213485,"in Figure 2a where the new space of representable Q-function is colored in yellow. Furthermore,
107"
DQN VARIANTS,0.2606741573033708,"algorithms such as Rainbow (Hessel et al., 2018) leverage both ideas. Other approaches, however,
108"
DQN VARIANTS,0.26292134831460673,"such as prioritized replay buffer (Schaul et al., 2015), cannot be represented in the picture.
109 loss loss"
DQN VARIANTS,0.2651685393258427,"(a) iDQN learns two Bellman iterations at once. It
uses a second target network ¯Q1 to learn the second
Bellman iteration. This second target network stays
close to the first online network Q1. loss loss"
DQN VARIANTS,0.26741573033707866,"(b) iDQN updates the online and target networks to
move one Bellman iteration ahead. Each network takes
the value of the network corresponding to the following
iteration."
DQN VARIANTS,0.2696629213483146,Figure 3: Graphical representation of iDQN in the space of Q-functions.
RANDOM ENSEMBLE MIXTURE,0.27191011235955054,"3.2
Random Ensemble Mixture
110"
RANDOM ENSEMBLE MIXTURE,0.27415730337078653,"Among the variants of DQN, ideas involving learning several Q-functions (Osband et al. (2016),
111"
RANDOM ENSEMBLE MIXTURE,0.27640449438202247,"Agarwal et al. (2020)) are particularly close to our method. Even if they are close, they remain
112"
RANDOM ENSEMBLE MIXTURE,0.2786516853932584,"orthogonal in the sense that they can be combined with our idea to create a more powerful agent.
113"
RANDOM ENSEMBLE MIXTURE,0.2808988764044944,"Random Ensemble Mixture (REM, Agarwal et al. (2020)) has been shown to be state-of-the-art for
114"
RANDOM ENSEMBLE MIXTURE,0.28314606741573034,"DQN variants with several Q-functions. Instead of exploring the space of Q-functions point by point
115"
RANDOM ENSEMBLE MIXTURE,0.2853932584269663,"as DQN does, REM moves in this space by exploring area by area, where the areas are the convex
116"
RANDOM ENSEMBLE MIXTURE,0.2876404494382023,"hull of the Q-functions stored in memory. As represented by the red line in Figure 2b, the loss used
117"
RANDOM ENSEMBLE MIXTURE,0.2898876404494382,"by REM is
118"
RANDOM ENSEMBLE MIXTURE,0.29213483146067415,"L(θ) = E(s,a,r,s′)∼D [Eα∼∆[l(δα(s, a, r, s′|θ))]] ,"
RANDOM ENSEMBLE MIXTURE,0.2943820224719101,"with δα(s, a, r, s′|θ) = Qα
1 (s, a|θ) −r −γ max
a′
¯Qα
0 (s′, a′|¯θ)"
RANDOM ENSEMBLE MIXTURE,0.2966292134831461,"where θ denotes the parameters of the online network and ¯θ the target parameters, D is the replay
119"
RANDOM ENSEMBLE MIXTURE,0.298876404494382,"buffer, ∆is the standard simplex and l is the Huber loss (Huber, 1992), and Qα
i = P"
RANDOM ENSEMBLE MIXTURE,0.30112359550561796,"k αkQk
i , i ∈
120"
RANDOM ENSEMBLE MIXTURE,0.30337078651685395,"{0, 1}. For a Bellman iteration i, the kth learnt Q-function is noted Qk
i . Figure 4a shows how this
121"
RANDOM ENSEMBLE MIXTURE,0.3056179775280899,"loss is computed with the neural network’s architecture used in REM.
122"
ITERATED DEEP Q-NETWORKS,0.30786516853932583,"4
Iterated Deep Q-Networks
123"
ITERATED DEEP Q-NETWORKS,0.3101123595505618,"We propose an approach built on top of DQN. The main idea emerges naturally from the representation
124"
ITERATED DEEP Q-NETWORKS,0.31235955056179776,"developed in Section 3. In DQN, Figure 1 illustrates that to learn two Bellman iterations, we first
125"
ITERATED DEEP Q-NETWORKS,0.3146067415730337,"need to wait until the first iteration is learned, and then we need to update the target before starting to
126"
ITERATED DEEP Q-NETWORKS,0.31685393258426964,"learn the second iteration. Conversely, we propose to use a second online network that learns the
127"
ITERATED DEEP Q-NETWORKS,0.31910112359550563,"second Bellman iteration while the first Bellman iteration is being learned. The target for the second
128"
ITERATED DEEP Q-NETWORKS,0.32134831460674157,"online network is created from a second target network that is frequently updated to be equal to the
129"
ITERATED DEEP Q-NETWORKS,0.3235955056179775,"first online network. Figure 3a shows how iDQN behaves in the space of Q-function. It is important
130"
ITERATED DEEP Q-NETWORKS,0.3258426966292135,"to understand that in iDQN, both online networks are learned at the same time. This idea can be
131"
ITERATED DEEP Q-NETWORKS,0.32808988764044944,"further applied, as more Q-functions can be considered for learning the following Bellman iterations.
132"
ITERATED DEEP Q-NETWORKS,0.3303370786516854,"Repeating the same schema, we can learn a following Bellman iteration by adding a new online
133"
ITERATED DEEP Q-NETWORKS,0.3325842696629214,"network that would use a new target network set to be equal to the last online network. In Figure 3a,
134"
ITERATED DEEP Q-NETWORKS,0.3348314606741573,"it would mean adding an online network Q3, learnt to be the projection of the target Γ∗¯Q2, where ¯Q2
135"
ITERATED DEEP Q-NETWORKS,0.33707865168539325,"is a target network periodically updated to the value of Q2. This iterative process can be continued
136"
ITERATED DEEP Q-NETWORKS,0.3393258426966292,"until memory usage becomes an issue. Once again, the loss can be observed on the representation in
137"
ITERATED DEEP Q-NETWORKS,0.3415730337078652,"Figure 3a (see red lines), it writes
138"
ITERATED DEEP Q-NETWORKS,0.3438202247191011,"L(s, a, r′, s′|θ) = K
X k=1 "
ITERATED DEEP Q-NETWORKS,0.34606741573033706,"Qk(s, a|θ) −r −γ max
a′
¯Qk−1(s′, a′|¯θ)
2
(2)"
ITERATED DEEP Q-NETWORKS,0.34831460674157305,"where θ is the online parameters and ¯θ the target parameters, K is the number of Bellman iterations
139"
ITERATED DEEP Q-NETWORKS,0.350561797752809,"considered at once. The kth learned Q-function corresponds to the kth Bellman iteration and is noted
140"
ITERATED DEEP Q-NETWORKS,0.35280898876404493,"Qk. The way the loss is computed from the neural network’s architecture is presented in Figure 4a.
141"
ITERATED DEEP Q-NETWORKS,0.3550561797752809,(a) Losses and neural networks architectures.
ITERATED DEEP Q-NETWORKS,0.35730337078651686,"(b) After the same number of gradi-
ent steps, iDQN has already started
to learn the second Bellman itera-
tion, noted Q2."
ITERATED DEEP Q-NETWORKS,0.3595505617977528,"Figure 4: Understanding the loss of iDQN. In Figure 4a, the dotted lines link the outputs of the neural
networks to the mathematical objects they represent. The flash signs stress how the information
flows from the target(s) bΓ ¯Q, considered fixed, to the online network(s) Q, on which the optimization
is being done. For any Q-function Q, bΓQ = Es,a,s′[R(s, a) + γ maxa′ Q(s′, a′)] is the empirical
Bellman operator."
ITERATED DEEP Q-NETWORKS,0.36179775280898874,"In iDQN, updating the target networks does not bring the target parameters to the next Bellman
142"
ITERATED DEEP Q-NETWORKS,0.36404494382022473,"iteration like in DQN. It simply refines their positions to be closer to the online networks to allow
143"
ITERATED DEEP Q-NETWORKS,0.36629213483146067,"better estimates of the iterated Q-functions. To be able to go further in the Bellman iterations, we
144"
ITERATED DEEP Q-NETWORKS,0.3685393258426966,"periodically consider a new online Q-function to learn and discard the first online and target network,
145"
ITERATED DEEP Q-NETWORKS,0.3707865168539326,"as shown in Figure 3b. For this example, Q3 is now considered and Q1 and ¯Q0 are left aside. We call
146"
ITERATED DEEP Q-NETWORKS,0.37303370786516854,"this procedure a rolling step. In practice, the rolling step is simple to implement. A new head to the
147"
ITERATED DEEP Q-NETWORKS,0.3752808988764045,"neural network of Figure 4a is added, with the index K + 1 and the first head is removed. It leads us
148"
ITERATED DEEP Q-NETWORKS,0.3775280898876405,"to introduce a new hyper-parameter that indicates at which frequency the rolling step is performed. It
149"
ITERATED DEEP Q-NETWORKS,0.3797752808988764,"is worth noticing that if K is set to 1 and if the rolling step frequency is synchronized with the target
150"
ITERATED DEEP Q-NETWORKS,0.38202247191011235,"update frequency in DQN, then we recover DQN, i.e., iDQN with K = 1 is equal to DQN.
151"
ITERATED DEEP Q-NETWORKS,0.3842696629213483,"In DQN, the actions are drawn from the online network. For iDQN, one needs to choose from which
152"
ITERATED DEEP Q-NETWORKS,0.3865168539325843,"of the multiple online networks to sample. One could stick to DQN and choose the first online
153"
ITERATED DEEP Q-NETWORKS,0.3887640449438202,"network. One could also use the last online network since it is supposed to be the one that is closer
154"
ITERATED DEEP Q-NETWORKS,0.39101123595505616,"to the optimal Q-function, or one could pick an online neural network at random as it is done in
155"
ITERATED DEEP Q-NETWORKS,0.39325842696629215,"Bootstrapped DQN (Osband et al., 2016). We do not consider taking the mean as REM proposes
156"
ITERATED DEEP Q-NETWORKS,0.3955056179775281,"because the online Q-functions are expected to follow a specific arrangement in space. Taking their
157"
ITERATED DEEP Q-NETWORKS,0.39775280898876403,"mean could lead to unwanted behavior. We investigate these sampling strategies in Section 5.1. One
158"
ITERATED DEEP Q-NETWORKS,0.4,"can see how iDQN uses the same algorithm as DQN through its pseudo-code shown in Algorithm 1
159"
ITERATED DEEP Q-NETWORKS,0.40224719101123596,"in the supplementary material. The only two changing parts are the behavioral policy and the loss.
160"
UNDERSTANDING THE LOSS OF IDQN,0.4044943820224719,"4.1
Understanding the Loss of iDQN
161"
UNDERSTANDING THE LOSS OF IDQN,0.4067415730337079,"The crucial advantage of iDQN is coming from the loss. In addition to the loss used in DQN, it
162"
UNDERSTANDING THE LOSS OF IDQN,0.40898876404494383,"contains K −1 more terms. Those terms concern the future Bellman iterations, hence the fact that
163"
UNDERSTANDING THE LOSS OF IDQN,0.41123595505617977,"iDQN allows for a better learning of the Bellman iterations. In practice, each Bellman iteration
164"
UNDERSTANDING THE LOSS OF IDQN,0.4134831460674157,"is learned with K times more gradient steps than in DQN while having the same overall number
165"
UNDERSTANDING THE LOSS OF IDQN,0.4157303370786517,"of gradient steps. This means that each selected sample is used K times more or, in other words,
166"
UNDERSTANDING THE LOSS OF IDQN,0.41797752808988764,"that each network sees K times more samples. As mentioned earlier, updating the target in DQN
167"
UNDERSTANDING THE LOSS OF IDQN,0.4202247191011236,"moves the learning procedure one Bellman iteration further. The same goes for the rolling step for
168"
UNDERSTANDING THE LOSS OF IDQN,0.42247191011235957,"iDQN. From the fact that each Bellman iteration is learned with K times more gradient steps than in
169"
UNDERSTANDING THE LOSS OF IDQN,0.4247191011235955,"DQN, we can allow iDQN to perform the rolling step more frequently than DQN updates the target,
170"
UNDERSTANDING THE LOSS OF IDQN,0.42696629213483145,"which means that iDQN will do more Bellman iterations at the end of the training, while still learning
171"
UNDERSTANDING THE LOSS OF IDQN,0.42921348314606744,"each iteration better than DQN. Figure 4b pinpoints the advantage of iDQN with K = 2 over DQN.
172"
UNDERSTANDING THE LOSS OF IDQN,0.4314606741573034,"There we assume that the update target frequency of DQN is equal to the rolling step frequency in
173"
UNDERSTANDING THE LOSS OF IDQN,0.4337078651685393,"iDQN. When DQN updates its target network Q0 to ¯Q1, the new online network Q2 is located at ¯Q1.
174"
UNDERSTANDING THE LOSS OF IDQN,0.43595505617977526,"When the rolling step is performed for iDQN, ¯Q1 is located at the same position as the ¯Q1 of DQN1
175"
UNDERSTANDING THE LOSS OF IDQN,0.43820224719101125,"but Q2 has already been learnt and is closer to the optimal Q-function than the Q2 of DQN. This
176"
UNDERSTANDING THE LOSS OF IDQN,0.4404494382022472,"phenomenon is even stronger when K increases. Another way to understand iDQN is to see iDQN as
177"
UNDERSTANDING THE LOSS OF IDQN,0.44269662921348313,"a way to pre-train the next online Q-functions instead of taking them equal to the online network as it
178"
UNDERSTANDING THE LOSS OF IDQN,0.4449438202247191,"is done in DQN.
179"
EXPERIMENTS,0.44719101123595506,"5
Experiments
180"
EXPERIMENTS,0.449438202247191,"We evaluate our proposed algorithm on 54 Atari 2600 Games (Bellemare et al., 2013)2. Many
181"
EXPERIMENTS,0.451685393258427,"implementations of Atari environments along with classical baselines are available online (Castro
182"
EXPERIMENTS,0.45393258426966293,"et al. (2018), D’Eramo et al. (2021), Raffin et al. (2021), Huang et al. (2022)). We choose to mimic
183"
EXPERIMENTS,0.45617977528089887,"the implementation choices made in Dopamine (Castro et al., 2018) since it is the only one to release
184"
EXPERIMENTS,0.4584269662921348,"the evaluation metric for all relevant baselines to our work and the only one to use the evaluation
185"
EXPERIMENTS,0.4606741573033708,"metric recommended by Machado et al. (2018). Namely, we use the game over signal to terminate
186"
EXPERIMENTS,0.46292134831460674,"an episode instead of the life signal. The input given to the neural network is a concatenation of 4
187"
EXPERIMENTS,0.4651685393258427,"frames in gray scale of dimension 84 by 84. To get a new frame, we sample 4 frames from the Gym
188"
EXPERIMENTS,0.46741573033707867,"environment (Brockman et al., 2016) configured with no frame skip, and we apply a max pooling
189"
EXPERIMENTS,0.4696629213483146,"operation on the 2 last gray scale frames. We use sticky actions to make the environment stochastic
190"
EXPERIMENTS,0.47191011235955055,"(with p = 0.25). The training performance is the one obtained during learning. By choosing an
191"
EXPERIMENTS,0.47415730337078654,"identical setting as Castro et al. (2018) does, we can take the baselines’ training performance of
192"
EXPERIMENTS,0.4764044943820225,"Dopamine without the need to train them again ourselves. To certify that the comparison is fair,
193"
EXPERIMENTS,0.4786516853932584,"we compared our version of DQN to their version and concluded positively (see Figure A of the
194"
EXPERIMENTS,0.48089887640449436,"supplementary material).
195"
EXPERIMENTS,0.48314606741573035,"Hyperparameter tuning.
The hyperparameters shared with DQN are kept untouched. The two
196"
EXPERIMENTS,0.4853932584269663,"additional hyperparameters (rolling step frequency and target parameters update frequency) were
197"
EXPERIMENTS,0.48764044943820223,"set to follow our intuition on their impact on the performance. As a reminder, the frequency at
198"
EXPERIMENTS,0.4898876404494382,"which the rolling step is performed is comparable to the target update frequency in DQN. Since
199"
EXPERIMENTS,0.49213483146067416,"iDQN allows more gradient steps per iteration, we set this hyperparameter to be 25% lower than
200"
EXPERIMENTS,0.4943820224719101,"the target update frequency in DQN (6000 compared to 8000). To further ensure that our code
201"
EXPERIMENTS,0.4966292134831461,"is trustworthy, Figure A in the supplementary material shows that DQN achieves similar training
202"
EXPERIMENTS,0.49887640449438203,"performances than iDQN with K = 1 and the rolling step frequency is set to be equal to the target
203"
EXPERIMENTS,0.501123595505618,"update frequency of DQN. It is important to note that decreasing the target parameters update results
204"
EXPERIMENTS,0.503370786516854,"in a more stable training but also a higher delay with the online networks which can harm the overall
205"
EXPERIMENTS,0.5056179775280899,"performance. We set it to 30, allowing 200 target updates per rolling step. We choose K = 5. This
206"
EXPERIMENTS,0.5078651685393258,"choice is further discussed in Section 5.1. To make the experiments run faster, we designed the
207"
EXPERIMENTS,0.5101123595505618,"Q-functions to share the convolutional layers. Additionally, we consider the first layers of the neural
208"
EXPERIMENTS,0.5123595505617977,"network useful for extracting a feature representation of the state space. This is why this choice can
209"
EXPERIMENTS,0.5146067415730337,"potentially be beneficial to our algorithm. Further details about the hyperparameters can be found in
210"
EXPERIMENTS,0.5168539325842697,"the supplementary material.
211"
EXPERIMENTS,0.5191011235955056,"Performance metric.
As recommended by Agarwal et al. (2021), we choose the interquartile
212"
EXPERIMENTS,0.5213483146067416,"mean (IQM) of the human normalized score to report the results of our experiments with shaded
213"
EXPERIMENTS,0.5235955056179775,"regions showing pointwise 95% percentile stratified bootstrap confidence intervals. IQM is a trade-off
214"
EXPERIMENTS,0.5258426966292135,"between the mean and the median where the tail of the score distribution is removed on both sides to
215"
EXPERIMENTS,0.5280898876404494,"consider only 50% of the runs. 5 seeds are used for each game.
216"
EXPERIMENTS,0.5303370786516854,"Main result.
iDQN greatly outperforms DQN (Adam) on the aggregation metric, proposed in
217"
EXPERIMENTS,0.5325842696629214,"Agarwal et al. (2021). Figure 5a shows the IQM human normalized score over 54 Atari games
218"
EXPERIMENTS,0.5348314606741573,"according to the number of frames sampled during the training. In the last millions of frames, iDQN
219"
EXPERIMENTS,0.5370786516853933,"reaches a higher IQM human normalized score than DQN (Adam). iDQN performs better than REM
220"
EXPERIMENTS,0.5393258426966292,"as well, showing that our approach should be preferred when using multi-head Q networks. We do not
221"
EXPERIMENTS,0.5415730337078651,"consider other variants of DQN to be relevant baselines to compare with. The ideas used in Rainbow,
222"
EXPERIMENTS,0.5438202247191011,"IQN or Munchausen DQN (Vieillard et al., 2020) can be included in iDQN algorithm to build an
223"
EXPERIMENTS,0.5460674157303371,"1The loss in iDQN is additive and includes the DQN loss. Thus both Q1 are located at the same position.
2We excluded the game Atlantis due to the significantly higher training time."
EXPERIMENTS,0.5483146067415731,"iDQN K=5
DQN (Nature)
DQN (Adam)
C51
REM"
EXPERIMENTS,0.550561797752809,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
EXPERIMENTS,0.5528089887640449,IQM Human Normalized Score
EXPERIMENTS,0.5550561797752809,(a) Training performance.
EXPERIMENTS,0.5573033707865168,"0
2
4
6
8
Human Normalized Score ( ) 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.5595505617977528,Fraction of runs with score >
EXPERIMENTS,0.5617977528089888,(b) Performance profile.
EXPERIMENTS,0.5640449438202247,"Figure 5: iDQN outperforms DQN (Nature), DQN (Adam), C51 and REM. DQN (Nature) uses the
RMSProp optimizer (Tieleman et al., 2012) while DQN (Adam) uses Adam (Kingma & Ba, 2015)."
EXPERIMENTS,0.5662921348314607,"iDQN K=5
DQN (Nature)
DQN (Adam)
C51
REM"
EXPERIMENTS,0.5685393258426966,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.5 1.0 1.5"
EXPERIMENTS,0.5707865168539326,IQM Human Normalized Score
EXPERIMENTS,0.5730337078651685,BankHeist
EXPERIMENTS,0.5752808988764045,"0
50
100
150
200
Number of Frames (in millions) 0 1 2 3 4"
EXPERIMENTS,0.5775280898876405,Enduro
EXPERIMENTS,0.5797752808988764,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.2 0.4 0.6 0.8"
EXPERIMENTS,0.5820224719101124,ChopperCommand
EXPERIMENTS,0.5842696629213483,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.5 1.0 1.5"
EXPERIMENTS,0.5865168539325842,Frostbite
EXPERIMENTS,0.5887640449438202,"0
50
100
150
200
Number of Frames (in millions) 1.0 0.5 0.0"
EXPERIMENTS,0.5910112359550562,Skiing
EXPERIMENTS,0.5932584269662922,Figure 6: 5 Atari games where different behaviors can be observed.
EXPERIMENTS,0.5955056179775281,"even more powerful agent. We further discuss it in Section 5.1. To visualize the distribution of final
224"
EXPERIMENTS,0.597752808988764,"scores, we plot the performance profile in Figure 5b. It shows the fraction of runs with a higher final
225"
EXPERIMENTS,0.6,"score than a certain threshold given by the X axis. In some ranges of human normalized score, iDQN
226"
EXPERIMENTS,0.6022471910112359,"statistically dominates DQN. For example, there are more games in which iDQN achieves 1.5 times a
227"
EXPERIMENTS,0.604494382022472,"human performance (τ = 1.5) than DQN. It is harder to distinguish iDQN and REM because the gap
228"
EXPERIMENTS,0.6067415730337079,"in final performance between iDQN and REM is smaller than between iDQN and DQN. In Figure 6,
229"
EXPERIMENTS,0.6089887640449438,"we selected 5 games where different behaviors can be observed. On some games like BankHeist and
230"
EXPERIMENTS,0.6112359550561798,"Enduro, iDQN overtakes all its baselines. Interestingly, in ChopperCommand, DQN (Adam) and
231"
EXPERIMENTS,0.6134831460674157,"REM fail at outperforming DQN (Nature), while iDQN is comparable with C51 in performance. This
232"
EXPERIMENTS,0.6157303370786517,"shows that efficiently learning the Bellman iterations plays an important role in some environments.
233"
EXPERIMENTS,0.6179775280898876,"In Frostbite, REM is more efficient than DQN (Adam). iDQN outperforms both algorithms in this
234"
EXPERIMENTS,0.6202247191011236,"game, being the only one achieving superhuman performances. Finally, in Skiing, REM and C51
235"
EXPERIMENTS,0.6224719101123596,"failed to be better than a random agent, while iDQN sticks to the performance of DQN (Adam). We
236"
EXPERIMENTS,0.6247191011235955,"believe this behavior comes from the fact that iDQN is close to DQN in principle, which minimizes
237"
EXPERIMENTS,0.6269662921348315,"the risk of failing when DQN succeeds. We present the training performance of all the remaining
238"
EXPERIMENTS,0.6292134831460674,"games in Figure B of the supplementary material.
239"
ABLATION STUDIES,0.6314606741573033,"5.1
Ablation Studies
240"
ABLATION STUDIES,0.6337078651685393,"We perform several ablation studies to showcase the different behaviors of iDQN. We first investigate
241"
ABLATION STUDIES,0.6359550561797753,"the importance of the number of Bellman iterations K taken into account in the loss. As shown in
242"
ABLATION STUDIES,0.6382022471910113,"Figure 7 for the games Asteroids and Asterix, increasing K to 10 iterations could be beneficial. In
243"
ABLATION STUDIES,0.6404494382022472,"Qbert, the gain seems not certain. We believe further tuning of hyperparameters should bring iDQN
244"
ABLATION STUDIES,0.6426966292134831,"with K = 10 to yield better performances than iDQN with K = 5. We insist that no hyperparameter
245"
ABLATION STUDIES,0.6449438202247191,"tuning has been performed to generate this plot. In order to have the same number of gradient steps
246"
ABLATION STUDIES,0.647191011235955,"per Bellman iteration for K = 5 and K = 10, we simply halved the frequency at which the rolling
247"
ABLATION STUDIES,0.6494382022471911,"step is performed for K = 10, bringing it to 3000 since we doubled K. Interestingly, the performance
248"
ABLATION STUDIES,0.651685393258427,"iDQN K=5
iDQN K=10
DQN (Adam)"
ABLATION STUDIES,0.6539325842696629,"0
50
100
150
200
Number of Frames (in millions)"
ABLATION STUDIES,0.6561797752808989,0.0000
ABLATION STUDIES,0.6584269662921348,0.0025
ABLATION STUDIES,0.6606741573033708,0.0050
ABLATION STUDIES,0.6629213483146067,0.0075
ABLATION STUDIES,0.6651685393258427,0.0100
ABLATION STUDIES,0.6674157303370787,0.0125
ABLATION STUDIES,0.6696629213483146,0.0150
ABLATION STUDIES,0.6719101123595506,IQM Human Normalized Score
ABLATION STUDIES,0.6741573033707865,Asteroids
ABLATION STUDIES,0.6764044943820224,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.5 1.0 1.5 2.0 2.5"
ABLATION STUDIES,0.6786516853932584,Asterix
ABLATION STUDIES,0.6808988764044944,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.2 0.4 0.6 0.8 1.0 Qbert"
ABLATION STUDIES,0.6831460674157304,Figure 7: Ablation study on the number of Bellman iterations K taken into account in the loss.
ABLATION STUDIES,0.6853932584269663,"iDQN uniform sampling
iDQN first online Q sampling
iDQN last online Q sampling"
ABLATION STUDIES,0.6876404494382022,"0
50
100
150
200
Number of Frames (in millions) 0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014"
ABLATION STUDIES,0.6898876404494382,IQM Human Normalized Score
ABLATION STUDIES,0.6921348314606741,Asteroids
ABLATION STUDIES,0.6943820224719102,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.5 1.0 1.5 2.0 2.5"
ABLATION STUDIES,0.6966292134831461,Asterix
ABLATION STUDIES,0.698876404494382,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.2 0.4 0.6 0.8 1.0 Qbert"
ABLATION STUDIES,0.701123595505618,"Figure 8: Ablation study on the way actions are sampled to interact with the environment. Actions
can be sampled from an online Q-function taken at random (in blue), from the first online Q-function
(in green), or from the last Q-function (in pink)."
ABLATION STUDIES,0.7033707865168539,"never drops in the 3 considered games. Therefore, we recommend increasing K as much as the
249"
ABLATION STUDIES,0.7056179775280899,"computational resources allow it.
250"
ABLATION STUDIES,0.7078651685393258,"In Section 4, we mentioned several sampling strategies. Figure 8 illustrates the different possibilities
251"
ABLATION STUDIES,0.7101123595505618,"mentioned earlier: sampling from a uniform online Q-function, sampling from the first online Q-
252"
ABLATION STUDIES,0.7123595505617978,"function like DQN does, or sampling from the last online Q-function, it is supposed to be the closest
253"
ABLATION STUDIES,0.7146067415730337,"to the optimal Q-function. No significant difference exists except for the game Asteroids, where
254"
ABLATION STUDIES,0.7168539325842697,"sampling from a uniform online Q-function seems to yield better performance throughout the training.
255"
ABLATION STUDIES,0.7191011235955056,"We believe that the increase in performance comes from the fact that the online Q-functions generate
256"
ABLATION STUDIES,0.7213483146067415,"a wider variety of samples compared to only sampling from the first or last online Q-function. We
257"
ABLATION STUDIES,0.7235955056179775,"recommend sampling actions from an online Q-function chosen at random.
258"
ABLATION STUDIES,0.7258426966292135,"iDQN heavily relies on the fact that the learned Q functions are located at different areas in the space
259"
ABLATION STUDIES,0.7280898876404495,"of Q-functions. We computed the standard deviation of the output of the learned Q-functions during
260"
ABLATION STUDIES,0.7303370786516854,"the training in Figure 9 to verify this assumption. The figure shows that the standard deviation among
261"
ABLATION STUDIES,0.7325842696629213,"the Q-function is indeed greater than zero across the 3 studied games. Furthermore, we can observe
262"
ABLATION STUDIES,0.7348314606741573,"that the standard deviation decreases during training, hinting that they become increasingly closer.
263"
ABLATION STUDIES,0.7370786516853932,"This matches the intuition that at the end of the training, the Q-functions should be close to the
264"
ABLATION STUDIES,0.7393258426966293,"projection of the optimal Q-function, hence being close to each other.
265"
DISCUSSION,0.7415730337078652,"6
Discussion
266"
DISCUSSION,0.7438202247191011,"In Figure 10, we compare iDQN with other powerful baselines to show the gap between those
267"
DISCUSSION,0.7460674157303371,"baselines and our approach, which does not use the benefit of a prioritized replay buffer and a n-step
268"
DISCUSSION,0.748314606741573,"return. The curves for other algorithms shown in Figure 10 depict the publicly available metrics for
269"
DISCUSSION,0.750561797752809,"those algorithms.3 The training performance of IQN and Munchausen DQN without the n-step return
270"
DISCUSSION,0.7528089887640449,"would be interesting to analyze, but our limited resources do not allow us to run those baselines.
271"
DISCUSSION,0.755056179775281,"The major improvement of Rainbow over C51 is made by using a prioritized replay buffer and
272"
DISCUSSION,0.7573033707865169,"3Often different metrics are used for different algorithms, making the comparison not straightforward."
DISCUSSION,0.7595505617977528,"0
50
100
150
200
Number of Frames (in millions) 0.02 0.04 0.06 0.08"
DISCUSSION,0.7617977528089888,IQM inter-head standard deviation
DISCUSSION,0.7640449438202247,Asteroids
DISCUSSION,0.7662921348314606,"0
50
100
150
200
Number of Frames (in millions) 0.04 0.06 0.08 0.10 0.12"
DISCUSSION,0.7685393258426966,Asterix
DISCUSSION,0.7707865168539326,"0
50
100
150
200
Number of Frames (in millions) 0.01 0.02 0.03 0.04 0.05 0.06"
DISCUSSION,0.7730337078651686,BankHeist
DISCUSSION,0.7752808988764045,"Figure 9: Standard deviation of the online networks’ output averaged over 3200 samples at each
iteration."
DISCUSSION,0.7775280898876404,"iDQN K=5
REM"
DISCUSSION,0.7797752808988764,"DQN (Nature)
Rainbow (C51 + 3-step return + PER)"
DISCUSSION,0.7820224719101123,"QR-DQN + 3-step return
IQN + 3-step return"
DISCUSSION,0.7842696629213484,"DQN (Adam)
Munchausen + IQN + 3-step return C51"
DISCUSSION,0.7865168539325843,"0
50
100
150
200
Number of Frames (in millions) 0.0 0.5 1.0 1.5 2.0"
DISCUSSION,0.7887640449438202,IQM Human Normalized Score
DISCUSSION,0.7910112359550562,"0
2
4
6
8
Human Normalized Score ( ) 0.0 0.2 0.4 0.6 0.8 1.0"
DISCUSSION,0.7932584269662921,Fraction of runs with score >
DISCUSSION,0.7955056179775281,"Figure 10: Training performance (left) and performance profile (right) of iDQN and other orthogonal
methods. QR-DQN (Dabney et al., 2018) and DQN (Adam) have been removed from the performance
profile for clarity."
DISCUSSION,0.797752808988764,"adding a 3-step return which gives hope for following works to study the potential strength of any
273"
DISCUSSION,0.8,"Rainbow-like iDQN approach. The training performances of iDQN on 54 Atari games along with
274"
DISCUSSION,0.802247191011236,"those more advanced methods are available in Figure C of the supplementary materials.
275"
DISCUSSION,0.8044943820224719,"Our approach introduces two new hyperparameters, rolling step frequency and target parameters
276"
DISCUSSION,0.8067415730337079,"update frequency, that need to be tuned. However, we provide a thorough understanding of their
277"
DISCUSSION,0.8089887640449438,"effects to mitigate this drawback. While extreme values of some hyperparameters were found to be
278"
DISCUSSION,0.8112359550561797,"harmful to the performance of DQN, e.g., changing the target update frequency, little variation of
279"
DISCUSSION,0.8134831460674158,"the presented values was found to have only a small impact on the overall performance. Regarding
280"
DISCUSSION,0.8157303370786517,"the resources needed to train an iDQN agent, more computations are required to get the gradient of
281"
DISCUSSION,0.8179775280898877,"the loss compared to DQN. Thanks to the ability of JAX (Bradbury et al., 2018) to parallelize the
282"
DISCUSSION,0.8202247191011236,"computation, iDQN with K = 5 only requires 1 to 2 times more time to run. With the released code
283"
DISCUSSION,0.8224719101123595,"base, each run presented in this paper can be run under 3 days on an NVIDIA RTX 3090.
284"
CONCLUSION,0.8247191011235955,"7
Conclusion
285"
CONCLUSION,0.8269662921348314,"In this paper, we have presented a way to learn the Bellman iterations more efficiently than DQN.
286"
CONCLUSION,0.8292134831460675,"The underlying idea of iDQN comes from an intuitive understanding of DQN’s behavior in the space
287"
CONCLUSION,0.8314606741573034,"of Q-functions. It allows each Q estimate to be learned with more gradient steps without increasing
288"
CONCLUSION,0.8337078651685393,"the overall number of gradient steps. iDQN outperforms its closest baselines, DQN and REM, on the
289"
CONCLUSION,0.8359550561797753,"Atari 2600 benchmark. While we proposed an approach to Q-learning that focuses on the projection
290"
CONCLUSION,0.8382022471910112,"step of the Bellman iterations, an interesting direction for future work would be to investigate which
291"
CONCLUSION,0.8404494382022472,"other past improvements of DQN, in combination with iDQN, would lead to a new state-of-the-art
292"
CONCLUSION,0.8426966292134831,"for value-based methods.
293"
REFERENCES,0.8449438202247191,"References
294"
REFERENCES,0.8471910112359551,"Agarwal, R., Schuurmans, D., and Norouzi, M. An optimistic perspective on offline reinforcement
295"
REFERENCES,0.849438202247191,"learning. In International Conference on Machine Learning, pp. 104–114. PMLR, 2020.
296"
REFERENCES,0.851685393258427,"Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. Deep reinforcement
297"
REFERENCES,0.8539325842696629,"learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems,
298"
REFERENCES,0.8561797752808988,"34, 2021.
299"
REFERENCES,0.8584269662921349,"Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An
300"
REFERENCES,0.8606741573033708,"evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279,
301"
REFERENCES,0.8629213483146068,"2013.
302"
REFERENCES,0.8651685393258427,"Bertsekas, D. Reinforcement learning and optimal control. Athena Scientific, 2019.
303"
REFERENCES,0.8674157303370786,"Bertsekas, D. P. Dynamic Programming and Optimal Control 4 th Edition , Volume II. Athena
304"
REFERENCES,0.8696629213483146,"Scientific, 2015.
305"
REFERENCES,0.8719101123595505,"Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke,
306"
REFERENCES,0.8741573033707866,"A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of
307"
REFERENCES,0.8764044943820225,"Python+NumPy programs, 2018. URL http://github.com/google/jax.
308"
REFERENCES,0.8786516853932584,"Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W.
309"
REFERENCES,0.8808988764044944,"Openai gym, 2016.
310"
REFERENCES,0.8831460674157303,"Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. Dopamine: A research
311"
REFERENCES,0.8853932584269663,"framework for deep reinforcement learning. arXiv preprint arXiv:1812.06110, 2018.
312"
REFERENCES,0.8876404494382022,"Dabney, W., Rowland, M., Bellemare, M., and Munos, R. Distributional reinforcement learning with
313"
REFERENCES,0.8898876404494382,"quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,
314"
REFERENCES,0.8921348314606742,"2018.
315"
REFERENCES,0.8943820224719101,"Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R.,
316"
REFERENCES,0.8966292134831461,"Abdolmaleki, A., de Las Casas, D., et al. Magnetic control of tokamak plasmas through deep
317"
REFERENCES,0.898876404494382,"reinforcement learning. Nature, 602(7897):414–419, 2022.
318"
REFERENCES,0.9011235955056179,"D’Eramo, C., Tateo, D., Bonarini, A., Restelli, M., and Peters, J.
Mushroomrl: Simplifying
319"
REFERENCES,0.903370786516854,"reinforcement learning research. Journal of Machine Learning Research, 22(131):1–5, 2021.
320"
REFERENCES,0.9056179775280899,"Fellows, M., Hartikainen, K., and Whiteson, S. Bayesian bellman operators. Advances in Neural
321"
REFERENCES,0.9078651685393259,"Information Processing Systems, 34:13641–13656, 2021.
322"
REFERENCES,0.9101123595505618,"Funk, N., Chalvatzaki, G., Belousov, B., and Peters, J. Learn2assemble with structured representations
323"
REFERENCES,0.9123595505617977,"and search for robotic architectural construction. In Conference on Robot Learning, pp. 1401–1411.
324"
REFERENCES,0.9146067415730337,"PMLR, 2022.
325"
REFERENCES,0.9168539325842696,"Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B.,
326"
REFERENCES,0.9191011235955057,"Azar, M., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning. In
327"
REFERENCES,0.9213483146067416,"Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.
328"
REFERENCES,0.9235955056179775,"Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., and Araújo, J. G. Cleanrl:
329"
REFERENCES,0.9258426966292135,"High-quality single-file implementations of deep reinforcement learning algorithms. Journal
330"
REFERENCES,0.9280898876404494,"of Machine Learning Research, 23(274):1–18, 2022. URL http://jmlr.org/papers/v23/
331"
REFERENCES,0.9303370786516854,"21-1342.html.
332"
REFERENCES,0.9325842696629213,"Huber, P. J. Robust estimation of a location parameter. Breakthroughs in statistics: Methodology and
333"
REFERENCES,0.9348314606741573,"distribution, pp. 492–518, 1992.
334"
REFERENCES,0.9370786516853933,"Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference
335"
REFERENCES,0.9393258426966292,"on Learning Representations, 2015.
336"
REFERENCES,0.9415730337078652,"Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M.
337"
REFERENCES,0.9438202247191011,"Revisiting the arcade learning environment: Evaluation protocols and open problems for general
338"
REFERENCES,0.946067415730337,"agents. Journal of Artificial Intelligence Research, 61:523–562, 2018.
339"
REFERENCES,0.9483146067415731,"Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Ried-
340"
REFERENCES,0.950561797752809,"miller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement
341"
REFERENCES,0.952808988764045,"learning. nature, 518(7540):529–533, 2015.
342"
REFERENCES,0.9550561797752809,"Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped dqn.
343"
REFERENCES,0.9573033707865168,"Advances in neural information processing systems, 29, 2016.
344"
REFERENCES,0.9595505617977528,"Puterman, M. L. Markov decision processes. Handbooks in operations research and management
345"
REFERENCES,0.9617977528089887,"science, 2:331–434, 1990.
346"
REFERENCES,0.9640449438202248,"Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N. Stable-baselines3:
347"
REFERENCES,0.9662921348314607,"Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22
348"
REFERENCES,0.9685393258426966,"(268):1–8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.
349"
REFERENCES,0.9707865168539326,"Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. arXiv preprint
350"
REFERENCES,0.9730337078651685,"arXiv:1511.05952, 2015.
351"
REFERENCES,0.9752808988764045,"Sutton, R. S. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):
352"
REFERENCES,0.9775280898876404,"9–44, August 1988. URL http://www.cs.ualberta.ca/~sutton/papers/sutton-88.pdf.
353"
REFERENCES,0.9797752808988764,"Tieleman, T., Hinton, G., et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
354"
REFERENCES,0.9820224719101124,"recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.
355"
REFERENCES,0.9842696629213483,"Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In
356"
REFERENCES,0.9865168539325843,"Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.
357"
REFERENCES,0.9887640449438202,"Vieillard, N., Pietquin, O., and Geist, M. Munchausen reinforcement learning. Advances in Neural
358"
REFERENCES,0.9910112359550561,"Information Processing Systems, 33:4235–4246, 2020.
359"
REFERENCES,0.9932584269662922,"Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N. Dueling network
360"
REFERENCES,0.9955056179775281,"architectures for deep reinforcement learning. In International conference on machine learning,
361"
REFERENCES,0.9977528089887641,"pp. 1995–2003. PMLR, 2016.
362"
