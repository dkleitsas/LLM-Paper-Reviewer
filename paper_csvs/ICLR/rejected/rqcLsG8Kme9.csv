Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004,"rQdia (pronounced “Arcadia”) regularizes Q-value distributions with augmented
images in pixel-based deep reinforcement learning. This simple idea, to equalize
Q-values across statistical distributions of actions and states, affords image aug-
mentation techniques like DrQ additional sample efﬁciency and better ﬁnal per-
formance, while propelling discrete-action DER to nearly 1.5x the performance
of DrQ and nearly 2x that of base DER. Representation learning is a major hur-
dle in deep RL, which notoriously requires far too many environment interactions
for real-world use cases. rQdia decreases this data hunger and increases overall
scores, bringing deep RL closer to real-world applicability."
INTRODUCTION,0.008,"1
INTRODUCTION"
INTRODUCTION,0.012,"Human perception is invariant to and remarkably robust against many perturbations, like discol-
oration, obfuscation, and low exposure. On the other hand, artiﬁcial neural networks do not intrin-
sically carry these invariance properties, not without regularizers or hand-crafted inductive biases
like convolution, kernel rotation, dilation, attention, and recurrence. In deep reinforcement learning
(RL) from pixels, an agent must learn to visually interpret a scene in order to decide on an action.
Thus, recent approaches in RL have turned to the self-supervision and data augmentation techniques
found in computer vision. Indeed, such contrastive learning auxiliary losses (Srinivas et al., 2020)
or data augmentation regularizers (Yarats et al., 2021b) have afforded greater sample efﬁciency and
ﬁnal scores in both the DeepMind Continuous Control Suite (Tassa et al., 2018) from pixels and
Atari Arcade Learning Environments (Bellemare et al., 2013)."
INTRODUCTION,0.016,"Nevertheless, pixel-based approaches continue to lag behind models that learn directly from state
embeddings, not just in terms of sample efﬁciency, but also in longer-term asymptotic performance.
For example, the recent DrQ (Yarats et al., 2021b), an image augmentation-based regularizer added
to SAC (Haarnoja et al., 2018), reports falling 14.5% short of its state embedding-based SAC coun-
terpart on the Cheetah Run task. Such discrepancies indicate that visual representations are not yet
up to par with state embeddings, at least not for locomotive continuous control. State embeddings
have many properties that facilitate generalization, such as location invariance, and to a degree, in-
variance between morphological relations. If a subset of the dimensions of a state embedding always
indicates feet position, then the relation “one foot in front of the other” will be represented by those
dimensions invariant to the placement of the robot’s arms, head, or other body parts. Parametric vi-
sual encodings are not guaranteed to learn such invariances with respect to the robot’s morphology."
INTRODUCTION,0.02,"What other signals in deep RL can guide visual representation learning toward more invariant en-
codings? We propose Q-value distributions, sets of cumulative discounted rewards, as such a signal.
For state s, Q-function Q, and actions a(0), ..., a(m) ∼D(A) from some statistical distribution D
over action space A, we deﬁne Q-value distribution simply as:"
INTRODUCTION,0.024,"Q(s, a(0)), ..., Q(s, a(m))."
INTRODUCTION,0.028,"Since Q-values are optimally proportional to action probabilities, this “distribution” is representative
of the actual policy distribution when D = π. Furthermore, it is a measure of the current and future
value of each action for that state. We review the MDP framework in Section 2.1. This signal is
amenable to many of the same invariances afforded by state embeddings, if not additional ones. For"
INTRODUCTION,0.032,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.036,Figure 1: “rQdia in a nutshell” rQdia regularizes Q-value distributions across augmentations.
INTRODUCTION,0.04,"example, if the optimal action is “put one foot in front of the other,” then the Q-value distribution
reﬂects this action’s relation to other actions regardless of (invariant to) where the agent is located."
INTRODUCTION,0.044,"While recent work shows individual Q-values beneﬁt from regularizing across augmented im-
ages (Yarats et al., 2021b), we consider that Q-value distributions are also important, as they contain
information not just about one action in isolation, but multiple actions in relation to one another."
BACKGROUND,0.048,"2
BACKGROUND"
DEEP RL FROM PIXELS,0.052,"2.1
DEEP RL FROM PIXELS"
DEEP RL FROM PIXELS,0.056,"A Markov Decision Process (MDP) consists of an action a ∈IRda, state s ∈IRds, and reward
r ∈IR. “From pixels” assumes that state s is an image frame or multiple image frames stacked
together. The action is sampled from a policy at any t time step at ∼π(st). Taking such actions
yields a trajectory τ = (s0, a0, s1, a1, ..., sT ) via the dynamics model st+1 ∼f(st, at) of the
environment and its rewards rt+1 = R(st, at). The goal is to maximize cumulative discounted
reward R(τ) = PT
t=0 rtγt where γ is the temporal discount factor. The optimal action for a state
a∗(s) = argmaxaQ∗(s, a) thus depends on the state-action value function, also known as the Q-
value, Qπ(s, a) = Eτ∼π[R(τ)|s0 = s, a0 = a]."
SOFT ACTOR-CRITIC & DRQ,0.06,"2.2
SOFT ACTOR-CRITIC & DRQ"
SOFT ACTOR-CRITIC & DRQ,0.064,"Soft-Actor Critic (SAC) (Haarnoja et al., 2018) is an RL algorithm which learns a min-reduced
ensemble of Q-value functions Qφ(s, a) = mini=1,2 Qφi(s, a) optimized with one-step Bellman
error, and a stochastic Gaussian policy πθ(s, a) optimized by maximizing Qφ(s, a) and entropy
H(s) = −πθ(s)log(πθ(s)), made differentiable via the reparameterization trick. To further en-
courage exploration and avoid premature policy collapse, entropy H(s) is also added as part of the
agent’s reward. In visual domains, Qφ and πθ are typically equipped with a shared convolutional
neural net encoder. DrQ (Yarats et al., 2021b) sets the Bellman target as the average of the aug-
mented and non-augmented next-state targets, and minimizes Bellman error for both Qφ(s, a) and
Qφ(aug(s), a) where aug is the random augmentation."
SOFT ACTOR-CRITIC & DRQ,0.068,"2.3
DATA-EFFICIENT RAINBOW (DER)"
SOFT ACTOR-CRITIC & DRQ,0.072,"Rainbow (Hessel et al., 2018) maps directly to Q-value estimates for discrete actions. Compared
to vanilla DQN (Mnih et al., 2013), several reﬁnements are used: Q-values are sampled from a
multivariate Gaussian probabilistically (Dabney et al., 2018), noise is injected into network parame-
ters (Plappert et al., 2017), double Q networks (Van Hasselt et al., 2016), dueling DQNs (Wang et al.,
2016), n-step returns (Watkins, 1989), and mini-batches are sampled from a prioritized experience
replay (Schaul et al., 2015). “Data-efﬁcient” refers to the Atari sample limit of 100k environmental
interactions, a much more challenging setting for notoriously inefﬁcient RL."
SOFT ACTOR-CRITIC & DRQ,0.076,Under review as a conference paper at ICLR 2022
RELATED WORK,0.08,"3
RELATED WORK"
DATA-EFFICIENT RL,0.084,"3.1
DATA-EFFICIENT RL"
DATA-EFFICIENT RL,0.088,"Data-efﬁcient RL is a paramount concern to the practicality of RL in real-world use cases. Image
augmentation has proven an extremely effective regularizer for improving the sample efﬁciency of
model-free off-policy RL algorithms (Yarats et al., 2019a; Srinivas et al., 2020; Sermanet et al.,
2018; Dwibedi et al., 2018), so much so that basic augmentation techniques sufﬁce to rival or sur-
pass model-based RL algorithms (Hafner et al., 2019c; Lee et al., 2019b; Hafner et al., 2019b) in
the sample efﬁciency metric. Curiously, these methods have become progressively simpler. CURL
(Srinivas et al., 2020) employed contrastive learning, using positive and negative samples extracted
from the mini-batch, requiring a computation of quadratic complexity w.r.t. the mini-batch size.
RAD (Laskin et al., 2020) and DrQ (Yarats et al., 2021b) showed that simpler methods work just
as well or better, either by augmenting images naively, or augmenting and averaging their Q-values,
respectively. Even more recently (contemporaneously), the as-yet unpublished DrQv2 (Yarats et al.,
2021a) found basic augmentation alone sufﬁces under a DDPG-based algorithm, with signiﬁcant
efﬁciency improvements over prior methods despite the exceptional simplicity. While these recent
methods have traded CURL’s mini-batch statistics for mere augmentation, rQdia marks the ﬁrst
combination of the two that uses mini-batch statistics to enforce consistency across Q-value distri-
butions, in a manner both simple and complementary to the above implements."
MINI-BATCH REGULARIZATION IN RL,0.092,"3.2
MINI-BATCH REGULARIZATION IN RL"
MINI-BATCH REGULARIZATION IN RL,0.096,"Figure 2: Contrastive sampling has disadvan-
tages that Q-value-based equalization does not."
MINI-BATCH REGULARIZATION IN RL,0.1,"Mini-batch-based regularization has not com-
monly been used in RL. For example, Batch
Norm (Ioffe & Szegedy, 2015), a common reg-
ularizer in computer vision, is not as notably
employed in RL. This type of regularization re-
mained unexploited in RL until CURL (Srinivas
et al., 2020) showed that contrastive learning via
image augmentation (Chen et al., 2020; He et al.,
2020; Misra & Maaten, 2020; Henaff, 2020)
greatly improved RL data efﬁciency. CURL con-
trasts a state’s “positive” augmentation sample
with the rest of the mini-batch’s “negative” aug-
mentation samples. These negative samples can
be thought of as sampled from a Uniform dis-
tribution over the agent’s experience replay, in-
spiring rQdia. However, contrastive learning en-
forces a non-guaranteed ground truth, disassociating negative samples regardless of actual similarity.
See Figure 2 for an example of this negative samples problem, where similar states are contrasted to
have dissimilar encodings as an inadvertent byproduct of the uniform randomness. rQdia bypasses
this ﬂaw by only enforcing a guaranteed constraint: that the Q-value for any sampled action, regard-
less of statistical distribution, be consistent across the same states invariant to augmentation. This
indeed should always be the case, thus yielding gains over CURL while remaining complementary
to methods like DrQ and DrQv2."
IMAGE AUGMENTATION,0.104,"3.3
IMAGE AUGMENTATION"
IMAGE AUGMENTATION,0.108,"Image augmentation is commonly used to counter over-ﬁtting in computer vision. Techniques in-
clude color shift, afﬁnity translation, scale, etc. (Ciregan et al., 2012; Cires¸an et al., 2011; Simard
et al., 2003; Krizhevsky et al., 2012; Chen et al., 2020). In Yarats et al. (2021b), the authors in-
vestigated several common image transformations and concluded that random shifts strike a good
balance between simplicity and performance for the MuJoCo environments. A variety of differ-
ent augmentations are useful for different games in the Procgen benchmark (Raileanu et al., 2020),
and Yarats et al. (2021b) used Intensity variation for the Atari environments. These techniques have
proven critical to MuJoCo from pixels."
IMAGE AUGMENTATION,0.112,Under review as a conference paper at ICLR 2022
METHODS,0.116,"4
METHODS"
METHODS,0.12,"rQdia is the ﬁrst RL Q-value regularizer that does not necessarily depend on either the on-policy or
the off-policy states and actions. n, m such states and actions are instead sampled from arbitrary
distributions, let’s call them D1 and D2, of state space S and action space A, respectively:"
METHODS,0.124,"s(0), ..., s(n−1) ∼D1(S)"
METHODS,0.128,"a(0), ..., a(m−1) ∼D2(A).
(1)"
METHODS,0.132,"Then, the following constraint is enforced for Qφ(·), the neural network that models Qπ(·) the
Q-value function:"
METHODS,0.136,"Qφ(s(i), a(j)) = Qφ(aug(s(i)), a(j)) ∀i, j,
(2)"
METHODS,0.14,where aug(·) is an arbitrary augmentation transform.
METHODS,0.144,"aug(·) and D1, D2 could vary."
METHODS,0.148,"For aug(·), in line with Yarats et al. (2021b), we use translation for MuJoCo and intensity jittering
for Atari. Speciﬁcally, to translate, we pad by 4 pixels, then crop randomly inward by 4 pixels; to
intensity jitter, each image is multiplied by some random noise s = 1.1×clip(r, −2, 2), r ∼N(0, 1)."
METHODS,0.152,"For D1, D2 in MuJoCo continuous action spaces, we implement a simple approach analogous to
CURL’s negative-sample sampling, except sampling both states and actions rather than just states.
That is, states and actions are directly lifted from the mini-batch B (n = m = |B|), in effect sampled
from a Uniform distribution over the agent’s experience replay."
METHODS,0.156,"By using historical states and actions as opposed to random noise, we compute Q-value distributions
over state-action pairs that could more feasibly be encountered in a deployed roll-out."
METHODS,0.16,"In discrete Atari, actions may be lifted from the action space directly, that is, a(0), ..., a(m−1) = A."
METHODS,0.164,"Thus,
given mini-batch states s(0), ..., s(n−1)
and mini-batch (or action space) actions
a(0), ..., a(m−1), the following auxiliary loss is proposed to constitute a basic implement of rQdia:"
METHODS,0.168,"LrQdia =
1
nm X"
METHODS,0.172,"i<n,j<m
(Qφ(s(i), a(j)) −Qφ(aug(s(i)), a(j)))2.
(3)"
METHODS,0.176,"Then this auxiliary loss is simply added to the RL agent’s standard loss term. Voila, rQdia (visualized
“in a nutshell” in Figure 1). This is applied in parallel for each s(i), a(j) pair. If mini-batches or
actions spaces are very large, it is possible to convolve a smaller subset of n states and m actions."
METHODS,0.18,"In MuJoCo, we note that while D1, D2 are treated as Uniform distributions over an agent’s history
similar to CURL’s negative sampling, D1, D2 could be more sophisticated. For example, the proba-
bility of sampling an action could be proportional to state similarities. Or, like MPO (Abdolmaleki
et al., 2018), actions could be sampled directly from the policy itself."
METHODS,0.184,"Algorithm 1 provides pseudo-code for rQdia in continuous control algorithms like SAC-AE and
DrQ. All code for rQdia will be released open-source."
EXPERIMENTS,0.188,"5
EXPERIMENTS"
EXPERIMENTS,0.192,"To measure the data-efﬁciency and overall performance of rQdia, we conducted experiments at
100k and 500k steps in the DeepMind Continuous Control Suite (MuJoCo) from pixels and 100k
interaction steps in the Atari Arcade Learning Environments."
EXPERIMENTS,0.196,"In the following, a(j) are all of the actions in the mini-batch (m = |B|) for MuJoCo, a(j) are all of
the actions in the action space (m = |A|) for Atari, with s(i) as all of the states in the mini-batch
(n = |B|) and the same MSE-based loss as deﬁned in Equation 3 for both."
EXPERIMENTS,0.2,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.204,"Algorithm 1 rQdia (blue) added to Soft Actor-Critic (SAC), pseudocode courtesy of Achiam (2018).
SAC is a good base framework and can be expanded easily to DrQ (see Section 2.2)."
EXPERIMENTS,0.208,"Input: initial policy params θ, Q-function params φ1, φ2, empty replay buffer D. Set target params
equal to main params φtarg,1 ←φ1, φtarg,2 ←φ2.
Denote augmentation function aug(·).
repeat"
EXPERIMENTS,0.212,"(1) Observe state s and select action a ∼πθ(·|s), (2) Execute a in the environment, (3) Ob-
serve next state s′, reward r, and done signal d to indicate whether s′ is terminal, (4) Store
(s, a, r, s′, d) in replay buffer D, (5) If s′ is terminal, reset environment state.
if it’s time to update then"
EXPERIMENTS,0.216,for j in range(however many updates) do
EXPERIMENTS,0.22,"Randomly sample a batch of transitions, B = {(s, a, r, s′, d)} from D. Compute targets
for the Q functions:"
EXPERIMENTS,0.224,"y(r, s′, d) = r + γ(1 −d)

min
i=1,2 Qφtarg,i(s′, ˜a′) −α log πθ(˜a′|s′)

,
˜a′ ∼πθ(·|s′)"
EXPERIMENTS,0.228,Update Q-functions (minimize Bellman error) by one step of gradient descent using:
EXPERIMENTS,0.232,"∇φi
1
|B| X"
EXPERIMENTS,0.236,"(s,a,r,s′,d)∈B
(Qφi(s, a) −y(r, s′, d))2
for i = 1, 2"
EXPERIMENTS,0.24,Update rQdia by one step of gradient descent using:
EXPERIMENTS,0.244,"∇φk
1
|B|2
X"
EXPERIMENTS,0.248,"(s(i),a(i),...),(s(j),a(j),...)∈B"
EXPERIMENTS,0.252,"
min
k=1,2 Qφk(s(i), a(j)) −min
k=1,2 Qφk(aug(s(i)), a(j))
2"
EXPERIMENTS,0.256,"Note that s(i) and a(j) are not necessarily the historically paired state and action.
Update policy by one step of gradient ascent using:"
EXPERIMENTS,0.26,"∇θ
1
|B| X s∈B"
EXPERIMENTS,0.264,"
min
i=1,2 Qφi(s, ˜aθ(s)) −α log πθ (˜aθ(s)| s)

,"
EXPERIMENTS,0.268,"where ˜aθ(s) is a sample from πθ(·|s) which is differentiable wrt θ via the reparametriza-
tion trick. Update target networks with:"
EXPERIMENTS,0.272,"φtarg,i ←ρφtarg,i + (1 −ρ)φi
for i = 1, 2"
EXPERIMENTS,0.276,until convergence
EXPERIMENTS,0.28,"Most deep RL benchmarks report point estimates of aggregate performance such as mean and me-
dian scores across task runs, ignoring the statistical uncertainties that are a natural consequence of
training with a ﬁnite number of random seeds. In the recent analysis by Agarwal et al. (2021),
the authors observe that viewing reported mean scores as random quantities that depend on a small
number of sample runs exhibits substantial variability, and demonstrate that a lot of the reported
improvements from previous works disproportionately beneﬁted from randomness in the experi-
mental protocol. To account for the variability of results in RL, they propose a number of statistical
best-practice protocols. We followed these best recommended practices as closely as possible and
report results in accord with their measured benchmark performances using their open-source library
for RL statistical analysis, rliable (https://github.com/google-research/rliable).
Compared to previous works, we evaluate our method with this more thorough statistical analysis
and prove rQdia with the recommended robust and efﬁcient aggregate metrics in Figures 3 and 4."
EXPERIMENTS,0.284,"Sans stats, raw results show rQdia boosts DrQ’s 100k MuJoCo sample efﬁciency on 4/6 tasks by
wide margins, despite a smaller batch size, also surpassing ground truth state embeddings on 4/6. In
the already-saturated 500k setting, rQdia boosts DrQ on 4/6 tasks, and additionally surpasses state
embeddings on the Cheetah Run task. In Atari 100k, rQdia affords DER clear gains over DrQ and
CURL, superseding mean baseline human-norm scores especially by near 200%."
EXPERIMENTS,0.288,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.292,"5.1
DEEPMIND CONTINUOUS CONTROL 500K & 100K"
STATISTICAL ANALYSIS,0.296,"5.1.1
STATISTICAL ANALYSIS"
STATISTICAL ANALYSIS,0.3,"0.0
0.2
0.4
0.6
0.8
1.0"
STATISTICAL ANALYSIS,0.304,"0.00
0.25
0.50
0.75
1.00"
K STEPS,0.308,100k steps
K STEPS,0.312,"0.0
0.2
0.4
0.6
0.8
1.0
Normalized Score ( )"
K STEPS,0.316,"0.00
0.25
0.50
0.75
1.00"
K STEPS,0.32,500k steps
K STEPS,0.324,Fraction of runs with score >
K STEPS,0.328,a) Performance Proﬁle
K STEPS,0.332,"0.4
0.6
0.8
Normalized Scores"
K STEPS,0.336,"SLAC
SAC+AE
Dreamer PISAC RAD"
K STEPS,0.34,"DrQ
DrQ+rQdia"
K STEPS,0.344,100k steps
K STEPS,0.348,"0.8
0.9
Normalized Scores"
K STEPS,0.352,500k steps
K STEPS,0.356,b) Interval Estimates
K STEPS,0.36,1 2 3 4 5 6 7
K STEPS,0.364,Ranking
K STEPS,0.368,Distribution
K STEPS,0.372,ball_in_cup_catch
K STEPS,0.376,1 2 3 4 5 6 7
K STEPS,0.38,Ranking
K STEPS,0.384,cartpole_swingup
K STEPS,0.388,1 2 3 4 5 6 7
K STEPS,0.392,Ranking
K STEPS,0.396,cheetah_run
K STEPS,0.4,1 2 3 4 5 6 7
K STEPS,0.404,Ranking
K STEPS,0.408,finger_spin
K STEPS,0.412,1 2 3 4 5 6 7
K STEPS,0.416,Ranking
K STEPS,0.42,reacher_easy
K STEPS,0.424,1 2 3 4 5 6 7
K STEPS,0.428,Ranking
K STEPS,0.432,walker_walk
K STEPS,0.436,"SLAC
SAC+AE
Dreamer
PISAC
RAD
DrQ
DrQ+rQdia"
K STEPS,0.44,c) Rank Comparisons On Individual Tasks
K STEPS,0.444,"Figure 3: 20 seeds DM Suite, aggregated across 6 tasks, on 100k and 500k benchmarks.
We compare DrQ+rQdia with SAC+AE (Yarats et al., 2019b), SLAC (Lee et al., 2019a),
Dreamer (Hafner et al., 2019a), CURL (Srinivas et al., 2020), RAD (Laskin et al.), DrQ (Yarats
et al., 2021b), and PISAC (Lee et al., 2020). Due to our own computational limits, we used a batch
size of 128, while other methods used 512. According to Yarats et al. (2021a), bigger batch sizes
yield bigger improvements. At 1/4 the batch size of other methods, rQdia still surpasses or matches
benchmarked scores. (a) Although pure stochastic dominance is rarely observed (Dror et al., 2019),
rQdia outperforms others at 100 steps, meaning we achieve higher scores per number of runs. At
500k steps, we still reach SOTA performance despite the hampered, more-efﬁcient batch size. (b) If
the lower bound of an algorithm’s interval is higher than another algorithm’s upper bound, there is
a high conﬁdence that the algorithm is better. rQdia not only outperforms the others but also has a
relatively small interval, meaning the result is better and also more consistent across different runs.
(c) rQdia not only ranks ﬁrst, but has a high probability of being ranked ﬁrst on 5/6 tasks, indicating
the holistic statistical performance exceeds baselines by a decisive margin."
K STEPS,0.448,"In line with recent works, we evaluate 6 tasks in the DeepMind Continuous Control Suite at 100k
and 500k steps to measure data efﬁciency and asymptotic performance respectively. The reported
improvements are based on higher mean scores per task, with large variability across random seeds.
When accounting for this variability, it turns out that many previous algorithms do not consistently
rank above the algorithms they claim to improve on (Agarwal et al., 2021). Therefore, the results and
metrics aggregated by Agarwal et al. (2021) and reported in Figure 3 are not necessarily consistent
with the ones reported in previous papers. We followed the protocol of Agarwal et al. (2021) and
instead stick to their benchmarked curves for our analysis. Unlike prior works, our computational
constraints required that we use a smaller batch size. All of our rQdia results are reported with this
smaller batch size. Yarats et al. (2021a) report that smaller batch sizes are disadvantaged."
K STEPS,0.452,"Performance Proﬁle The performance proﬁle shows the tail distribution of scores on combined
runs across tasks, thus allowing us to compare different methods at a glance. If one curve is strictly
higher than another, it is said to “stochastically dominate” (Dror et al., 2019). Figure 3a) indicates
that at 100k, rQdia can achieve the same score as other algorithms with fewer runs and a higher
score with equal runs. At 500k, rQdia matches SOTA performance despite 1/4 the batch size."
K STEPS,0.456,Under review as a conference paper at ICLR 2022
K STEPS,0.46,"Table 1: rQdia is robust to batch size. At 1/4 the batch size, rQdia surpasses or matches baseline
models in data-efﬁciency (100k) and asymptotic performance (500k) given by mean episode reward
averaged over 20 seeds. It is reported that larger batch sizes of 512 were necessary to achieving the
performances of prior works (Yarats et al., 2021a), while DrQ+rQdia uses a batch size of only 128."
K STEPS,0.464,"From Pixels
State Emb"
K STEP SCORES,0.468,"500k Step Scores
DrQ+rQdia-128
DrQ-512
CURL
RAD
SAC+AE
SAC State"
K STEP SCORES,0.472,"Ball In Cup Catch
919.69
963.94
958
970.36
810.85
979
Cartpole Swingup
864.75
868.82
861
858.09
730.94
870
Cheetah Run
777.29
679.91
500
774.96
544.3
772
Finger Spin
939.05
938.77
874
907.4
914.3
929
Reacher Easy
950.01
945.4
904
930.44
601.36
975
Walker Walk
934.47
924.16
906
917.58
858.16
964"
K STEP SCORES,0.476,100k Step Scores
K STEP SCORES,0.48,"Ball In Cup Catch
910
913.8
772
950.22
338.42
957
Cartpole Swingup
867.42
759.37
592
863.69
276.63
812
Cheetah Run
502.77
360.97
307
499.06
240.58
228
Finger Spin
842.47
901.41
779
813.05
747.01
672
Reacher Easy
905.34
600.42
517
772.44
225.14
919
Walker Walk
721.78
633.57
344
644.78
395.87
604"
K STEP SCORES,0.484,"Interval Estimates We resampled with replacement independently for each task to construct an em-
pirical bootstrap sample in which we computed 95% stratiﬁed bootstrap conﬁdence intervals (CIs).
This process is repeated 50000 times to approximate the real sampling distributions. Normalized
scores are computed by dividing by the maximum score (= 1000). rQdia yields a high conﬁdence
with a small interval, meaning its performance is both statistically better and more consistent."
K STEP SCORES,0.488,"Rank Comparisons show the probability that a given method is assigned rank i, averaged across
all tasks. The ranks are estimated using 200,000 stratiﬁed bootstrap re-samples. rQdia ranks highest
on 5/6 tasks with high probability."
ROBUSTNESS TO BATCH SIZE,0.492,"5.1.2
ROBUSTNESS TO BATCH SIZE"
ROBUSTNESS TO BATCH SIZE,0.496,"We report tabular results in Table 1. rQdia excels at sample efﬁciency, achieving SOTA results at the
100k benchmark, even rivaling the state embedding ground truth. At 500k, where results are already
saturated, rQdia yields less pronounced improvements in terms of ﬁnal score, but notably attains
SOTA scores despite a smaller batch size. Yarats et al. (2021a) report that performance hinged on
the whopping 512 batch size. Due to computational limits, we could not reproduce this, but achieved
competitive results nevertheless at 1/4 the batch size, robustly using just 128."
ROBUSTNESS TO BATCH SIZE,0.5,"5.2
ATARI ARCADE 100K"
ROBUSTNESS TO BATCH SIZE,0.504,"To conduct a thorough statistical analysis of Atari 100k performance, we evaluated rQdia with the
performance proﬁle described in Section 5.1. We further scaled the x-axis such that the space
between any τ1 and τ2 is proportional to the fraction of runs averaged across algorithms between τ1
and τ2. This scaling shows the regions of the score distribution where most of the runs lie as opposed
to comparing tail ends of the distribution. In addition to median and mean, we further report two
additional aggregate metrics as recommended in (Dror et al., 2019), where all scores are computed
with 95% stratiﬁed bootstrap conﬁdence intervals. Our case study compares the performance of
ﬁve recent deep RL algorithms, namely: (1) DER (van Hasselt et al., 2019), OTR(Kielak, 2020),
SimPle (Kaiser et al., 2019), DrQ (Yarats et al., 2021b), and CURL (Srinivas et al., 2020). We
also include SPR (Schwarzer et al., 2020), a slightly-apples-to-oranges-baseline which learns a self-
supervised environment dynamics model, to which rQdia is orthogonal to and amazingly approaches
the performance of despite the substantial difference in simplicity. Raw tabular results for Atari 100k
are presented in Table 2, with benchmarks likewise pulled from rliable."
ROBUSTNESS TO BATCH SIZE,0.508,Under review as a conference paper at ICLR 2022
ROBUSTNESS TO BATCH SIZE,0.512,"Table 2: DER + rQdia rQdia augmented to Data-Efﬁcient Rainbow (DER) yields performance gains
competitive with SOTA models in the 100k data-efﬁcient Atari benchmark (mean per 10 random
seeds, scores pulled from rliable (Agarwal et al., 2021)). The most apples-to-apples comparison
is rQdia and DER, since we build on top of DER. rQdia is also orthogonal to the other reported
methods, and could feasibly yield even more striking improvements augmented to those."
ROBUSTNESS TO BATCH SIZE,0.516,"Atari Arcade Environments
DER + rQdia
DrQ
CURL
DER
Random
Human"
ROBUSTNESS TO BATCH SIZE,0.52,"Alien
1188
734.076
711.033
802.346
227.8
7127.7
Amidar
208.9
94.195
113.743
125.905
5.8
1719.5
Assault
649.9
479.536
500.927
561.46
222.4
742
Asterix
890
535.645
567.24
535.44
210
8503.3
BankHeist
64
153.412
65.299
185.479
14.2
753.1
BattleZone
19000
10563.6
8997.8
8977
2360
37187.5
Boxing
12.3
6.631
0.95
-0.309
0.1
12.1
Breakout
8
15.406
2.555
9.214
1.7
30.5
ChopperCommand
1500
792.39
783.53
925.87
811
7387.8
CrazyClimber
23970
21991.55
9154.36
34508.57
10780.5
35829.4
DemonAttack
1833
1142.448
646.467
627.599
152.1
1971
Freeway
26.8
17.778
28.268
20.855
0
29.6
Frostbite
2874
508.08
1226.494
870.975
65.2
4334.7
Gopher
896
618.014
400.856
467.02
257.6
2412.5
Hero
7261
3722.64
4987.682
6226.044
1027
30826.4
Jamesbond
985
251.765
331.05
275.66
29
302.8
Kangaroo
670
974.45
740.24
581.67
52
3035
Krull
4193
4131.377
3049.225
3256.886
1598
2665.5
KungFuMaster
16310
7154.51
8155.56
6580.07
258.5
22736.3
MsPacman
1598
1002.926
1064.012
1187.431
307.3
6951.6
Pong
-14.8
-14.251
-18.487
−9.711
-20.7
14.6
PrivateEye
12.9
24.844
81.855
72.751
24.9
69571.3
Qbert
2112.5
934.242
727.01
1773.54
163.9
13455
RoadRunner
8840
8724.66
5006.11
11843.35
11.5
7845
Seaquest
386
310.494
315.186
304.581
68.4
42054.7
UpNDown
4154
3619.133
2646.372
3075.004
533.4
11693.2"
ROBUSTNESS TO BATCH SIZE,0.524,"Mean Human-Norm Score
59.146%
36.912%
26.149%
30.03%
0%
100%
Med Human-Norm Score
25.750%
21.198%
9.235%
18.9%
0%
100%"
ROBUSTNESS TO BATCH SIZE,0.528,"Interquartile Mean (IQM) discards the bottom and top 25% of the runs and calculates the mean
score of the remaining 50% runs, which serves as an interpolation between mean and median across
runs. IQM is robust to outliers compared to mean and has considerably less bias than median."
ROBUSTNESS TO BATCH SIZE,0.532,"Optimality Gap is the amount by which the algorithm fails to meet a minimum score (human
score), which serves as a robust alternative to mean. This metric assumes that a score of human-
level performance is a desirable target beyond which improvements are not very important."
ROBUSTNESS TO BATCH SIZE,0.536,"Probability Of Improvement is designed to measure how likely it is for X to outperform Y on
a randomly selected task, which is computed by the Mann-Whitney U-statistic (Mann & Whitney,
1947). The interval reported estimates are based on stratiﬁed bootstrap with independent sampling
with 2000 bootstrap re-samples."
DISCUSSION,0.54,"6
DISCUSSION"
DISCUSSION,0.544,"Limitations One limitation of rQdia is that it assumes a beneﬁt to a certain augmentation invariance.
Translation invariance for example might not be as useful in environments where most objects are
held within a consistent axis."
DISCUSSION,0.548,"Moreover, rQdia ensures consistency between Q-value distributions across such perturbations,
which means that models in environments that do not require such visual invariances are needlessly
expected to learn a more complex, more general Q function."
DISCUSSION,0.552,Under review as a conference paper at ICLR 2022
DISCUSSION,0.556,"0.0
0.5
1.0
1.5
2.0
Human Normalized Score ( ) 0.00 0.25 0.50 0.75 1.00"
DISCUSSION,0.56,Fraction of runs with score >
DISCUSSION,0.564,Score Distributions
DISCUSSION,0.568,"0.0
0.1
0.2
0.5
1.0 2.0
Human Normalized Score ( ) 0.00 0.25 0.50 0.75 1.00"
DISCUSSION,0.572,Fraction of tasks with score >
DISCUSSION,0.576,Score Distributions with Non Linear Scaling
DISCUSSION,0.58,"SimPLe
DER
OTR
CURL
DrQ
SPR
DER+rQdia"
DISCUSSION,0.584,a) Performance proﬁle w/ linear and non-linear scaling
DISCUSSION,0.588,0.300.450.600.75
DISCUSSION,0.592,SimPLe
DISCUSSION,0.596,"DER
OTR
CURL"
DISCUSSION,0.6,"DrQ
SPR
DER+rQdia"
DISCUSSION,0.604,P(DER > X)
DISCUSSION,0.608,0.15 0.30 0.45
DISCUSSION,0.612,P(OTR > X)
DISCUSSION,0.616,0.15 0.30 0.45
DISCUSSION,0.62,P(CURL > X)
DISCUSSION,0.624,"0.4
0.6
0.8"
DISCUSSION,0.628,SimPLe
DISCUSSION,0.632,"DER
OTR
CURL"
DISCUSSION,0.636,"DrQ
SPR
DER+rQdia"
DISCUSSION,0.64,P(DrQ > X)
DISCUSSION,0.644,0.60 0.75
DISCUSSION,0.648,P(SPR > X)
DISCUSSION,0.652,0.45 0.60 0.75
DISCUSSION,0.656,P(DER+rQdia > X)
DISCUSSION,0.66,b) Probability of Improvement
DISCUSSION,0.664,"0.15
0.30
0.45"
DISCUSSION,0.668,SimPLe
DISCUSSION,0.672,"DER
OTR
CURL"
DISCUSSION,0.676,"DrQ
SPR
DER+rQdia"
DISCUSSION,0.68,Median
DISCUSSION,0.684,"0.1
0.2
0.3
0.4 IQM"
DISCUSSION,0.688,"0.2
0.4
0.6
0.8 Mean"
DISCUSSION,0.692,0.56 0.64 0.72 0.80
DISCUSSION,0.696,Optimality Gap
DISCUSSION,0.7,Human Normalized Score
DISCUSSION,0.704,c) Aggregate metrics on Atari 100k
DISCUSSION,0.708,"Figure 4: 10 seeds Atari ALE, aggregated across 26 environments. (a) Performance proﬁles with
pointwise 95% conﬁdence bands show rQdia outperforms others with a large margin especially when
τ ∈[0, 1], namely relative to human-level performance. After non-linear scaling, the improvement
of our algorithm is more pronounced. The gap between DER+rQdia and DER suggests rQdia can
majorly improve learning. (b) The bottom-right subplot shows that rQdia has a very high chance of
improvement over all baselines, and no other baseline can have a > 50% chance of outperforming
rQdia. (c) Higher mean, median, and IQM scores and lower optimality gap are better. rQdia has
the best performance across all four metrics, indicating a more certain and substantial improvement.
All results are based on 10 runs per environment, except SimPLe, for which we use their reported 5.
Notably, DER+rQdia, with simple image augmentation, rivals the SOTA results of the orthogonal-
potentially-complementary SPR (Schwarzer et al., 2020), which learns a computationally intensive
environment dynamics model. We include SPR just for reference."
DISCUSSION,0.712,"On the other hand, such environments where these invariances are not useful or important may
leverage rQdia to learn more invariant representations that could potentially better generalize to
different, more complex environments."
DISCUSSION,0.716,"Ethics rQdia is a simple regularizer that contributes to the generalization of deep reinforcement
learning models. While we hope deep RL continues to improve and its applications and abilities ex-
pand, we would be remiss not to note the destructive potential of the ﬁeld, ranging from autonomous
weaponry to economic exploitation. However, we are optimistic that RL can do much more good
than bad for society. Autonomous agents that can interact with the real world via RL-based, more-
streamlined robotics opens the door for countless medical, social, and economic beneﬁts as well."
DISCUSSION,0.72,"Reproducability We will release all code open-source; we have submitted code together with the
paper; relevant code snippets are shared in Appendix C; continuous control pseudocode provided in
Algorithm 1; hyperparams speciﬁed in Appendix B."
CONCLUSION,0.724,"7
CONCLUSION"
CONCLUSION,0.728,"We presented a simple regularizer for model-free reinforcement learning that may easily be inte-
grated into existing reinforcement learning frameworks. With the inclusion of this auxiliary loss, we
attain strong performance compared to baseline models, including recent state of the arts. By reg-
ularizing Q-value distributions, we further enforce the invariances afforded by image augmentation
techniques such that Q-value distributions are preserved under these perturbations. Consequently,
we observe improvements in sample efﬁciency and ﬁnal reward in the DeepMind Continuous Con-
trol Suite and environments in the Atari Arcade Learning Environments."
CONCLUSION,0.732,Under review as a conference paper at ICLR 2022
REFERENCES,0.736,REFERENCES
REFERENCES,0.74,"Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,
2018."
REFERENCES,0.744,Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.
REFERENCES,0.748,"Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Informa-
tion Processing Systems, 2021."
REFERENCES,0.752,"Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:
253–279, 2013."
REFERENCES,0.756,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020."
REFERENCES,0.76,"Dan Ciregan, Ueli Meier, and J¨urgen Schmidhuber. Multi-column deep neural networks for image
classiﬁcation. In 2012 IEEE conference on computer vision and pattern recognition, pp. 3642–
3649. IEEE, 2012."
REFERENCES,0.764,"Dan C Cires¸an, Ueli Meier, Jonathan Masci, Luca M Gambardella, and J¨urgen Schmidhuber. High-
performance neural networks for visual object classiﬁcation. arXiv preprint arXiv:1102.0183,
2011."
REFERENCES,0.768,"Will Dabney, Mark Rowland, Marc Bellemare, and R´emi Munos.
Distributional reinforcement
learning with quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, volume 32, 2018."
REFERENCES,0.772,"Rotem Dror, Segev Shlomov, and Roi Reichart. Deep dominance-how to properly compare deep
neural models. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pp. 2773–2785, 2019."
REFERENCES,0.776,"Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, and Pierre Sermanet. Learning actionable rep-
resentations from visual observations. In 2018 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pp. 1577–1584. IEEE, 2018."
REFERENCES,0.78,"Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018."
REFERENCES,0.784,"Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a."
REFERENCES,0.788,"Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019b."
REFERENCES,0.792,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555–2565. PMLR, 2019c."
REFERENCES,0.796,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729–9738, 2020."
REFERENCES,0.8,"Olivier Henaff. Data-efﬁcient image recognition with contrastive predictive coding. In International
Conference on Machine Learning, pp. 4182–4192. PMLR, 2020."
REFERENCES,0.804,"Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 32, 2018."
REFERENCES,0.808,Under review as a conference paper at ICLR 2022
REFERENCES,0.812,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.816,"Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019."
REFERENCES,0.82,"Kacper Piotr Kielak. Do recent advancements in model-based deep reinforcement learning really
improve data efﬁciency? arXiv preprint arXiv:2003.10181, 2020."
REFERENCES,0.824,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.828,"Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learming with augmented data. arXiv:2004.14990."
REFERENCES,0.832,"Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020."
REFERENCES,0.836,"Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model.
arXiv preprint arXiv:1907.00953,
2019a."
REFERENCES,0.84,"Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model.
arXiv preprint arXiv:1907.00953,
2019b."
REFERENCES,0.844,"Kuang-Huei
Lee,
Ian
Fischer,
Anthony
Liu,
Yijie
Guo,
Honglak
Lee,
John
Canny,
and
Sergio
Guadarrama.
Predictive
information
accelerates
learning
in
rl.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 11890–11901. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
89b9e0a6f6d1505fe13dea0f18a2dcfa-Paper.pdf."
REFERENCES,0.848,"Henry B Mann and Donald R Whitney. On a test of whether one of two random variables is stochas-
tically larger than the other. The annals of mathematical statistics, pp. 50–60, 1947."
REFERENCES,0.852,"Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representa-
tions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 6707–6717, 2020."
REFERENCES,0.856,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller.
Playing atari with deep reinforcement learning.
arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.86,"Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
arXiv preprint arXiv:1706.01905, 2017."
REFERENCES,0.864,"Roberta Raileanu, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus.
Auto-
matic data augmentation for generalization in deep reinforcement learning.
arXiv preprint
arXiv:2006.12862, 2020."
REFERENCES,0.868,"Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015."
REFERENCES,0.872,"Max Schwarzer, Ankesh Anand, Rishab Goel, R. Devon Hjelm, Aaron C. Courville, and Philip
Bachman.
Data-efﬁcient reinforcement learning with momentum predictive representations.
CoRR, abs/2007.05929, 2020. URL https://arxiv.org/abs/2007.05929."
REFERENCES,0.876,Under review as a conference paper at ICLR 2022
REFERENCES,0.88,"Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey
Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In
2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 1134–1141. IEEE,
2018."
REFERENCES,0.884,"Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural
networks applied to visual document analysis. In Icdar, volume 3. Citeseer, 2003."
REFERENCES,0.888,"Aravind Srinivas, Michael Laskin, and Pieter Abbeel. CURL: contrastive unsupervised representa-
tions for reinforcement learning. CoRR, abs/2004.04136, 2020. URL https://arxiv.org/
abs/2004.04136."
REFERENCES,0.892,"Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018."
REFERENCES,0.896,"Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016."
REFERENCES,0.9,"Hado van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforce-
ment learning? arXiv preprint arXiv:1906.05243, 2019."
REFERENCES,0.904,"Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International conference on machine
learning, pp. 1995–2003. PMLR, 2016."
REFERENCES,0.908,Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
REFERENCES,0.912,"Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Im-
proving sample efﬁciency in model-free reinforcement learning from images.
arXiv preprint
arXiv:1910.01741, 2019a."
REFERENCES,0.916,"Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Im-
proving sample efﬁciency in model-free reinforcement learning from images.
arXiv preprint
arXiv:1910.01741, 2019b."
REFERENCES,0.92,"Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous con-
trol: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021a."
REFERENCES,0.924,"Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. In International Conference on Learning Representa-
tions, 2021b. URL https://openreview.net/forum?id=GY6-6sTvGaf."
REFERENCES,0.928,"A
ARCHITECTURE"
REFERENCES,0.932,"The SAC-AE base architecture we use for DrQ is the same as [38], consisting of a shared encoder
and distinct policy and Q-function heads. The CNN encoder is shared by the actor and critic; the
critic consists of two ReLU-activated 3-layer MLP Q-networks; and the actor is a single ReLU-
activated 3-layer MLP Gaussian policy head. We modify the code provided by [30]: https:
//github.com/MishaLaskin/curl."
REFERENCES,0.936,"Atari environments were tested with a Rainbow architecture inspired by [35] and built on the
variant implemented in tandem with CURL in [30]. We added the rQdia auxiliary loss to their
code sans the CURL-related portion. This code may be found here: https://github.com/
aravindsrinivas/curl_rainbow."
REFERENCES,0.94,"B
TRAINING"
REFERENCES,0.944,"All hyperparameters were preserved from the original implementations discussed above. They are
reviewed in Tables 3 and 4, except for the substitution of batch size since we used a batch size of
128 while [30, 37] used 512, reportedly giving those models a decent advantage."
REFERENCES,0.948,Under review as a conference paper at ICLR 2022
REFERENCES,0.952,"Param
Value"
REFERENCES,0.956,"Observation Size
(84, 84)
Replay Buffer Size
100000
Initial Steps
1000
Stacked Frames
3
Action Repeat
2 ﬁnger, spin;
walker, walk
8 cartpole, swingup
4 otherwise
MLP Hidden Units
1024
Evaluation Episodes
10
Optimizer
ADAM
Learning Rate (fθ, πψ, Qφ)
0.001
Learning Rate (α)
0.0001
Batch Size
128
Q Function EMA τ
0.01
Critic Target Update Freq
2
Conv Layers
4
Number of Filters
32
Non-Linearity
ReLU
Encoder EMA τ
0.05
Latent Dimension
50
Discount γ
0.99
Initial Temperature
0.1"
REFERENCES,0.96,Table 3: Hyperparameters for rQdia-DrQ
REFERENCES,0.964,"Param
Value"
REFERENCES,0.968,"Observation Size
(84, 84)
Replay Buffer Size
100000
Frame Skip
4
Action repeat
4
Q-network Channels
32, 64
Q-network Filter Size
5 × 5, 5 × 5
Q-network Stride
5, 5
Q-network Hidden Units
256
Momentum τ
0.001
Non-Linearity
ReLU
Reward Clipping
[−1, 1]
Multi Step Return
20
Min replay size
for sampling
1600
Max Frames Per Episode
108K
Target network
update period
2000 updates
Support Of Q-dist
51 bins
Discount γ
0.99
Batch Size
32
Optimizer
ADAM
Learning Rate
0.9
(β1, β2)
(0.9, 0.999)
Optimizer ϵ
0.000015
Max Grad Norm
10
Noisy Nets Parameter
0.1
Priority Exponent
0.5
Priority Correction
0.4 →1"
REFERENCES,0.972,"Table
4:
Hyperparameters
for
rQdia-
Rainbow"
REFERENCES,0.976,"C
CODE"
REFERENCES,0.98,"Code for continuous control and discrete Atari will be released on GitHub and is provided in the
supplementary material."
REFERENCES,0.984,Figure 5: Pytorch code for rQdia in Rainbow Atari.
REFERENCES,0.988,"The rQdia loss in Rainbow is a sim-
ple mean squared error between the
anchor and augmentation’s respective
Q-value distributions (Figure 5)."
REFERENCES,0.992,Figure 6: Pytorch code for rQdia in continuous control.
REFERENCES,0.996,"Continuous control involves a
bit more handiwork, but is also
simple to tweak into an exist-
ing RL library (Figure 6). First,
mini-batch actions and states
have to be convolved in pairs
with one another.
A scaling
factor ∈(0, 1] can be modiﬁed
for efﬁciency to determine how
many such pairs should be used.
Then the double-critics predict
a Q-value distribution for the
convolved pairs, which is mini-
mized w.r.t. the augmentation."
