Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003952569169960474,"The lack of adversarial robustness has been recognized as an important issue for
state-of-the-art machine learning (ML) models, e.g., deep neural networks (DNNs).
Thereby, robustifying ML models against adversarial attacks is now a major fo-
cus of research. However, nearly all existing defense methods, particularly for
robust training, made the white-box assumption that the defender has the access
to the details of an ML model (or its surrogate alternatives if available), e.g., its
architectures and parameters. Beyond existing works, in this paper we aim to
address the problem of black-box defense: How to robustify a black-box model
using just input queries and output feedback? Such a problem arises in practical
scenarios, where the owner of the predictive model is reluctant to share model
information in order to preserve privacy. To this end, we propose a general no-
tion of defensive operation that can be applied to black-box models, and design
it through the lens of denoised smoothing (DS), a ﬁrst-order (FO) certiﬁed de-
fense technique. To allow the design of merely using model queries, we further
integrate DS with the zeroth-order (gradient-free) optimization. However, a di-
rect implementation of zeroth-order (ZO) optimization suffers a high variance of
gradient estimates, and thus leads to ineffective defense. To tackle this problem,
we next propose to prepend an autoencoder (AE) to a given (black-box) model
so that DS can be trained using variance-reduced ZO optimization. We term the
eventual defense as ZO-AE-DS. In practice, we empirically show that ZO-AE-
DS can achieve improved accuracy, certiﬁed robustness, and query complexity
over existing baselines. And the effectiveness of our approach is justiﬁed under
both image classiﬁcation and image reconstruction tasks. Codes are available at
https://github.com/damon-demon/Black-Box-Defense."
INTRODUCTION,0.007905138339920948,"1
INTRODUCTION"
INTRODUCTION,0.011857707509881422,"ML models, DNNs in particular, have achieved remarkable success owing to their superior predictive
performance. However, they often lack robustness. For example, imperceptible but carefully-crafted
input perturbations can fool the decision of a well-trained ML model. These input perturbations
refer to adversarial perturbations, and the adversarially perturbed (test-time) examples are known
as adversarial examples or adversarial attacks (Goodfellow et al., 2015; Carlini & Wagner, 2017;
Papernot et al., 2016). Existing studies have shown that it is not difﬁcult to generate adversarial
attacks. Numerous attack generation methods have been designed and successfully applied to (i)
different use cases from the digital world to the physical world, e.g., image classiﬁcation (Brown
et al., 2017; Li et al., 2019; Xu et al., 2019; Yuan et al., 2021), object detection/tracking (Eykholt
et al., 2017; Xu et al., 2020; Sun et al., 2020), and image reconstruction (Antun et al., 2020; Raj
et al., 2020; Vasiljevi´c et al., 2021), and (ii) different types of victim models, e.g., white-box models
whose details can be accessed by adversaries (Madry et al., 2018; Carlini & Wagner, 2017; Tramer
et al., 2020; Croce & Hein, 2020; Wang et al., 2021), and black-box models whose information is not
disclosed to adversaries (Papernot et al., 2017; Tu et al., 2019; Ilyas et al., 2018a; Liang et al., 2021)."
INTRODUCTION,0.015810276679841896,Published as a conference paper at ICLR 2022
INTRODUCTION,0.019762845849802372,"Given the prevalence of adversarial attacks, methods to robustify ML models are now a major focus
in research. For example, adversarial training (AT) (Madry et al., 2018), which has been poised one
of the most effective defense methods (Athalye et al., 2018), employed min-max optimization to
minimize the worst-case (maximum) training loss induced by adversarial attacks. Extended from AT,
various empirical defense methods were proposed, ranging from supervised learning, semi-supervised
learning, to unsupervised learning (Madry et al., 2018; Zhang et al., 2019b; Shafahi et al., 2019; Zhang
et al., 2019a; Carmon et al., 2019; Chen et al., 2020; Zhang et al., 2021). In addition to empirical
defense, certiﬁed defense is another research focus, which aims to train provably robust ML models
and provide certiﬁcates of robustness (Wong & Kolter, 2017; Raghunathan et al., 2018; Katz et al.,
2017; Salman et al., 2019; 2020; 2021). Although exciting progress has been made in adversarial
defense, nearly all existing works ask a defender to perform over white-box ML models (assuming
non-conﬁdential model architectures and parameters). However, the white-box assumption may
restrict the defense application in practice. For example, a model owner may refuse to share the model
details, since disclosing model information could hamper the owner’s privacy, e.g., model inversion
attacks lead to training data leakage (Fredrikson et al., 2015). Besides the privacy consideration, the
white-box defense built upon the (end-to-end) robust training (e.g., AT) is computationally intensive,
and thus is difﬁcult to scale when robustifying multiple models. For example, in the medical domain,
there exist massive pre-trained ML models for different diseases using hundreds of neuroimaging
datasets (Sisodiya et al., 2020). Thus, robustly retraining all models becomes impractical. Taking the
model privacy and the defense efﬁciency into consideration, we ask:"
INTRODUCTION,0.023715415019762844,Is it possible to design an adversarial defense over black-box models using only model queries?
INTRODUCTION,0.02766798418972332,"Black-box model
Defensive 
operation
Inference"
INTRODUCTION,0.03162055335968379,Ostrich
INTRODUCTION,0.03557312252964427,Output feedback
INTRODUCTION,0.039525691699604744,"Input
query"
INTRODUCTION,0.043478260869565216,Adversarial
INTRODUCTION,0.04743083003952569,attack
INTRODUCTION,0.05138339920948617,"Figure 1: Illustration of defense against adversarial attacks for
entirely black-box models."
INTRODUCTION,0.05533596837944664,"Extending adversarial defense to the
black-box regime (that we call ‘black-
box defense’) is highly non-trivial
due to the challenge of black-box op-
timization (i.e., learning over black-
box models).
To tackle this prob-
lem, the prior work (Salman et al.,
2020) leveraged surrogate models as
approximations of the black-box models, over which defense can be conducted following the white-
box setup. Yet, this still requires to have access to the information on the victim model type and
its function. In practice, those conditions could be difﬁcult to achieve. For example, if the domain
knowledge related to medicine or healthcare is lacking (Qayyum et al., 2020; Finlayson et al., 2019),
then it will be difﬁcult to determine a proper surrogate model of a medical ML system. Even if a
black-box model estimate can be obtained using the model inversion technique (Kumar & Levine,
2019), a signiﬁcantly large number of model queries are needed even just for tackling a MNIST-level
prediction task (Oh et al., 2019). Different from (Salman et al., 2020), we study an authentic black-
box scenario, in which the interaction between defender and model is only based on input-output
function queries (see Fig. 1). To our best knowledge, this is the ﬁrst work to tackle the problem of
query-based black-box defense."
INTRODUCTION,0.05928853754940711,"Contributions.
We summarize our contributions below."
INTRODUCTION,0.06324110671936758,"x (Formulation-wise) We formulate the problem of black-box defense and investigate it through the
lens of zeroth-order (ZO) optimization. Different from existing works, our paper aims to design the
restriction-least black-box defense and our formulation is built upon a query-based black-box setting,
which avoids the use of surrogate models."
INTRODUCTION,0.06719367588932806,"y (Methodology-wise) We propose a novel black-box defense approach, ZO AutoEncoder-based
Denoised Smoothing (ZO-AE-DS), which is able to tackle the challenge of ZO optimization in high
dimensions and convert a pre-trained non-robust ML model into a certiﬁably robust model using only
function queries."
INTRODUCTION,0.07114624505928854,"z (Experiment-wise) We verify the efﬁcacy of our method through an extensive experimental study.
In the task of image classiﬁcation, the proposed ZO-AE-DS signiﬁcantly outperforms the ZO baseline
built upon (Salman et al., 2020). For instance, we can improve the certiﬁed robust accuracy of ResNet-
110 on CIFAR-10 from 19.16% (using baseline) to 54.87% (using ZO-AE-DS) under adversarial
perturbations with ℓ2 norm less than 64/255. We also empirically show that our proposal stays
effective even in the task of image reconstruction."
INTRODUCTION,0.07509881422924901,Published as a conference paper at ICLR 2022
RELATED WORK,0.07905138339920949,"2
RELATED WORK"
RELATED WORK,0.08300395256916997,"Empirical defense.
An immense number of defense methods have been proposed, aiming to
improve model robustness against adversarial attacks. Examples include detecting adversarial
attacks (Guo et al., 2017; Meng & Chen, 2017; Gong et al., 2017; Grosse et al., 2017; Metzen et al.,
2017) and training robust ML models (Madry et al., 2018; Zhang et al., 2019b; Shafahi et al., 2019;
Wong et al., 2020; Zhang et al., 2019a; Athalye et al., 2018; Cheng et al., 2017; Wong & Kolter,
2017; Salman et al., 2019; Raghunathan et al., 2018; Katz et al., 2017). In this paper, we focus
on advancing the algorithm foundation of robust training over black-box models. Robust training
can be broadly divided into two categories: empirical defense and certiﬁed defense. In the former
category, the most representative method is AT (adversarial training) that formulates adversarial
defense as a two-player game (between attacker and defender) (Madry et al., 2018). Spurred by
AT, empirical defense has developed rapidly. For example, in (Zhang et al., 2019b), TRADES was
proposed to seek the optimal trade-off between accuracy and robustness. In (Stanforth et al., 2019;
Carmon et al., 2019), unlabeled data and self-training were shown effective to improve adversarial
defense in both robustness and generalization. In (Shafahi et al., 2019; Wong et al., 2020; Zhang
et al., 2019a; Andriushchenko & Flammarion, 2020), to improve the scalability of adversarial defense,
computationally-light alternatives of AT were developed. Despite the effectiveness of empirical
defense against adversarial attacks (Athalye et al., 2018), it lacks theoretical guarantee (known as
‘certiﬁcate’) for the achieved robustness. Thus, the problem of certiﬁed defense arises."
RELATED WORK,0.08695652173913043,"Certiﬁed defense.
Certiﬁed defense seeks to provide a provably guarantee of ML models. One
line of research focuses on post-hoc formal veriﬁcation of a pre-trained ML model. The certiﬁed
robustness is then given by a ‘safe’ input perturbation region, within which any perturbed inputs
will not fool the given model (Katz et al., 2017; Ehlers, 2017; Bunel et al., 2018; Dutta et al., 2017).
Since the exact veriﬁcation is computationally intensive, a series of work (Raghunathan et al., 2018;
Dvijotham et al., 2018; Wong & Kolter, 2017; Weng et al., 2018a;b; Wong et al., 2018) proposed
‘incomplete’ veriﬁcation, which utilizes convex relaxation to over-approximate the output space of
a predictive model when facing input perturbations. Such a relaxation leads to fast computation in
the veriﬁcation process but only proves a lower bound of the exact robustness guarantee. Besides
the post-hoc model veriﬁcation with respect to each input example, another line of research focuses
on in-processing certiﬁcation-aware training and prediction. For example, randomized smoothing
(RS) transforms an empirical classiﬁer into a provably robust one by convolving the former with an
isotropic Gaussian distribution. It was shown in (Cohen et al., 2019) that RS can provide formal
guarantees for adversarial robustness. Different types of RS-oriented provable defenses have been
developed, such as adversarial smoothing (Salman et al., 2019), denoised smoothing (Salman et al.,
2020), smoothed ViT (Salman et al., 2021), and feature smoothing (Addepalli et al., 2021)."
RELATED WORK,0.09090909090909091,"Zeroth-order (ZO) optimization for adversarial ML.
ZO optimization methods are gradient-
free counterparts of ﬁrst-order (FO) optimization methods (Liu et al., 2020b). They approximate the
FO gradients through function value based gradient estimates. Thus, ZO optimization is quite useful
to solve black-box problems when explicit expressions of their gradients are difﬁcult to compute or
infeasible to obtain. In the area of adversarial ML, ZO optimization has become a principled approach
to generate adversarial examples from black-box victim ML models (Chen et al., 2017; Ilyas et al.,
2018a;b; Tu et al., 2019; Liu et al., 2019; 2020a; Huang & Zhang, 2020; Cai et al., 2020; 2021). Such
ZO optimization-based attack generation methods can be as effective as state-of-the-art white-box
attacks, despite only having access to the inputs and outputs of the targeted model. For example,
the work (Tu et al., 2019) leveraged the white-box decoder to map the generated low-dimension
perturbations back to the original input dimension. Inspired by (Tu et al., 2019), we leverage the
autoencoder architecture to tackle the high-dimension challenge of ZO optimization in black-box
defense. Despite the widespread application of ZO optimization to black-box attack generation, few
work studies the problem of black-box defense."
RELATED WORK,0.09486166007905138,"3
PROBLEM FORMULATION: BLACK-BOX DEFENSE"
RELATED WORK,0.09881422924901186,"In this section, we formulate the problem of black-box defense, i.e., robustifying black-box ML
models without having any model information such as architectures and parameters."
RELATED WORK,0.10276679841897234,Published as a conference paper at ICLR 2022
RELATED WORK,0.1067193675889328,"Problem statement.
Let fθbb(x) denote a pre-deﬁned black-box (bb) predictive model, which can
map an input example x to a prediction. In our work, fθbb can be either an image classiﬁer or an
image reconstructor. For simplicity of notation, we will drop the model parameters θbb when referring
to a black-box model. The threat model of our interest is given by norm-ball constrained adversarial
attacks (Goodfellow et al., 2015). To defend against these attacks, existing approaches commonly
require the white-box assumption of f (Madry et al., 2018) or have access to white-box surrogate
models of f (Salman et al., 2020). Different from the prior works, we study the problem of black-box
defense when the owner of f is not able to share the model details. Accordingly, the only mode
of interaction with the black-box system is via submitting inputs and receiving the corresponding
predicted outputs. The formal statement of black-box defense is given below:"
RELATED WORK,0.11067193675889328,"(Black-box defense) Given a black-box base model f, can we develop a defensive operation
R using just input-output function queries so as to produce the robustiﬁed model R(f) against
adversarial attacks?"
RELATED WORK,0.11462450592885376,"Defensive operation.
We next provide a concrete formulation of the defensive operation R. In the
literature, two principled defensive operations were used: (R1) end-to-end AT (Madry et al., 2018;
Zhang et al., 2019b; Cohen et al., 2019), and (R2) prepending a defensive component to a base model
(Meng & Chen, 2017; Salman et al., 2020; Aldahdooh et al., 2021). The former (R1) has achieved
the state-of-the-art robustness performance (Athalye et al., 2018; Croce & Hein, 2020) but is not
applicable to black-box defense. By contrast, the latter (R2) is more compatible with black-box
models. For example, denonised smoothing (DS), a recently-developed R2-type approach (Salman
et al., 2020), gives a certiﬁed defense by prepending a custom-trained denoiser to the targeted model.
In this work, we choose DS as the backbone of our defensive operation (Fig. 2)."
RELATED WORK,0.11857707509881422,Black-box predictor Input
RELATED WORK,0.1225296442687747,Denoised
RELATED WORK,0.12648221343873517,"example
Prediction"
RELATED WORK,0.13043478260869565,White-box denoiser
RELATED WORK,0.13438735177865613,"Robustification:
Figure 2: DS-based black-box defense."
RELATED WORK,0.1383399209486166,"In DS, a denoiser is integrated with a base model
f so that the augmented system becomes resilient
to Gaussian noise and thus plays a role similar
to the RS-based certiﬁed defense (Cohen et al.,
2019). That is, DS yields"
RELATED WORK,0.1422924901185771,"R(f(x)) := f(Dθ(x)),
(1)"
RELATED WORK,0.14624505928853754,"where Dθ denotes the learnable denoiser (with
parameters θ) prepended to the (black-box) predictor f. Once Dθ is learned, then the DS-based
smooth classiﬁer, arg maxc Pδ∈N(0,σ2I)[R(f(x + δ)) = c], can achieve certiﬁed robustness, where
c is a class label, δ ∈N(0, σ2I) denotes the standard Gaussian noise with variance σ2, and
arg maxc Pδ∈N(0,σ2I)[f(x + δ) = c] signiﬁes a smooth version of f."
RELATED WORK,0.15019762845849802,"Based on (1), the goal of black-box defense becomes to ﬁnd the optimal denoiser Dθ so as to achieve
satisfactory accuracy as well as adversarial robustness. In the FO learning paradigm, Salman et al.
(2020) proposed a stability regularized denoising loss to train Dθ:"
RELATED WORK,0.1541501976284585,"minimize
θ
Eδ∈N(0,σ2I),x∈U ∥Dθ(x + δ) −x∥2
2
|
{z
}
:= ℓDenoise(θ)"
RELATED WORK,0.15810276679841898,"+γEδ,x ℓCE(R(f(x + δ)), f(x))
|
{z
}
:= ℓStab(θ) ,
(2)"
RELATED WORK,0.16205533596837945,"where U denotes the training dataset, the ﬁrst objective term ℓDenoise(θ) corresponds to the mean
squared error (MSE) of image denoising, the second objective term ℓStab(θ) measures the prediction
stability through the cross-entropy (CE) between the outputs of the denoised input and the original
input, and γ > 0 is a regularization parameter that strikes a balance between ℓDenoise and ℓStab."
RELATED WORK,0.16600790513833993,"We remark that problem (2) can be solved using the FO gradient descent method if the base model
f is fully disclosed to the defender. However, the black-box nature of f makes the gradients of
the stability loss ℓStab(θ) infeasible to obtain. Thus, we will develop a gradient-free DS-oriented
defense."
RELATED WORK,0.16996047430830039,"4
METHOD: A SCALABLE ZEROTH-ORDER OPTIMIZATION SOLUTION"
RELATED WORK,0.17391304347826086,"In this section, we begin by presenting a brief background on ZO optimization, and elaborate on the
challenge of black-box defense in high dimensions. Next, we propose a novel ZO optimization-based
DS method that can not only improve model query complexity but also lead to certiﬁed robustness."
RELATED WORK,0.17786561264822134,Published as a conference paper at ICLR 2022
RELATED WORK,0.18181818181818182,"ZO optimization.
In ZO optimization, the FO gradient of a black-box function ℓ(w) (with a
d-dimension variable w) is approximated by the difference of two function values along a set of
random direction vectors. This leads to the randomized gradient estimate (RGE) (Liu et al., 2020b):"
RELATED WORK,0.1857707509881423,"ˆ∇wℓ(w) = 1 q q
X i=1  d"
RELATED WORK,0.18972332015810275,µ (ℓ(w + µui) −ℓ(w)) ui
RELATED WORK,0.19367588932806323,"
,
(3)"
RELATED WORK,0.1976284584980237,"where {ui}q
i=1 are q random vectors drawn independently and uniformly from the sphere of a unit
ball, and µ > 0 is a given small step size, known as the smoothing parameter. The rationale behind
(3) is that it provides an unbiased estimate of the FO gradient of the Gaussian smoothing version
of ℓ(Gao et al., 2018), with variance in the order of O( d"
RELATED WORK,0.2015810276679842,"q ) (Liu et al., 2020b). Thus, a large-scale
problem (with large d) yields a large variance of RGE (3). To reduce the variance, a large number
of querying directions (i.e., q) is then needed, with the worst-case query complexity in the order of
O(d). If q = d, then the least estimation variance can be achieved by the coordinatewise gradient
estimate (CGE) (Lian et al., 2016; Liu et al., 2018):"
RELATED WORK,0.20553359683794467,"ˆ∇wℓ(w) = d
X i=1"
RELATED WORK,0.20948616600790515,"ℓ(w + µei) −ℓ(w) µ
ei"
RELATED WORK,0.2134387351778656,"
,
(4)"
RELATED WORK,0.21739130434782608,"where ei ∈Rd denotes the ith elementary basis vector, with 1 at the ith coordinate and 0s elsewhere.
For any off-the-shelf FO optimizers, e.g., stochastic gradient descent (SGD), if we replace the FO
gradient estimate with the ZO gradient estimate, then we obtain the ZO counterpart of a FO solver,
e.g., ZO-SGD (Ghadimi & Lan, 2013)."
RELATED WORK,0.22134387351778656,"Warm-up: A direct application of ZO optimization.
A straightforward method to achieve the
DS-based black-box defense is to solve problem (2) using ZO optimization directly. However, it will
give rise to the difﬁculty of ZO optimization in high dimensions. Speciﬁcally, DS requires to calculate
the gradient of the defensive operation (1). With the aid of ZO gradient estimation, we obtain"
RELATED WORK,0.22529644268774704,∇θR(f(x)) = dDθ(x)
RELATED WORK,0.22924901185770752,"dθ
df(z)"
RELATED WORK,0.233201581027668,"dz
|z=Dθ(x) ≈dDθ(x)"
RELATED WORK,0.23715415019762845,"dθ
ˆ∇zf(z) |z=Dθ(x) ,
(5)"
RELATED WORK,0.24110671936758893,"where with an abuse of notation, let d denote the dimension of x (yielding Dθ(x) ∈Rd and z ∈Rd)
and dθ denote the dimension of θ, dDθ(x)"
RELATED WORK,0.2450592885375494,"dθ
∈Rdθ×d is the Jacobian matrix of the vector-valued
function Dθ(x), and ˆ∇zf(z) denotes the ZO gradient estimate of f, following (3) or (4). Since the
dimension of an input is typically large for image classiﬁcation (e.g., d = 3072 for a CIFAR-10"
RELATED WORK,0.2490118577075099,"Method
Certiﬁed robustness (%)
(ℓ2 radius: ϵ = 0.5)
Standard accuracy (%)"
RELATED WORK,0.25296442687747034,"FO-DS
30.22
71.80
ZO-DS
(RGE, q = 192)
5.06 (↓25.16)
44.81 (↓26.99)"
RELATED WORK,0.25691699604743085,"Table 1: Performance comparison between FO-
DS (Salman et al., 2020) and its direct ZO imple-
mentation ZO-DS on (CIFAR-10, ResNet-110)."
RELATED WORK,0.2608695652173913,"image), it imposes two challenges: (a) The variance
of RGE (3) will be ultra-large if the query complexity
stays low, i.e., a small query number q is used; And (b)
the variance-least CGE (4) becomes impracticable due
to the need of ultra-high querying cost (i.e., q = d).
Indeed, Table 1 shows that the direct application of (5)
into the existing FO-DS solver (Salman et al., 2020),
which we call ZO-DS, yields over 25% degradation in
both standard accuracy and certiﬁed robustness evaluated at input perturbations with ℓ2 norm less
than 128/255, where pixels of an input image are normalized to [0, 1]. We refer readers to Sec. 5 for
more details."
RELATED WORK,0.2648221343873518,"Black-box predictor 
White-box denoiser"
RELATED WORK,0.26877470355731226,"White-box 
Black-box"
RELATED WORK,0.2727272727272727,Figure 3: Model architecture for ZO-AE-DS.
RELATED WORK,0.2766798418972332,"ZO
autoencoder-based
DS
(ZO-AE-
DS): A scalable solution to black-box de-
fense.
The difﬁculty of ZO optimization
in high dimensions prevents us from devel-
oping an effective DS-oriented provable
defense for black-box ML models.
To
tackle such problem, we introduce an Au-
toencoder (AE) to connect the front-end
denoiser Dθ with the back-end black-box
predictive model f so that ZO optimization can be conducted in a (low-dimension) feature embedding
space. To be concrete, let ψθDec ◦φθEnc denote AE consisting of the encoder (Enc) ψθEnc and the"
RELATED WORK,0.28063241106719367,Published as a conference paper at ICLR 2022
RELATED WORK,0.2845849802371542,"decoder (Dec) φθDec, where ◦denotes the function composition operation. Plugging φθAE between
the denoiser Dθ and the black-box predictor f, we then extend the defensive operation (1) to the
following (see Fig. 3 for illustration):
Rnew(f(x)) := f (φθDec (z))
|
{z
}
new black box"
RELATED WORK,0.2885375494071146,",
z = ψθEnc (Dθ(x))
|
{z
}
new white box ,
(6)"
RELATED WORK,0.2924901185770751,"where z ∈Rdz denotes the low-dimension feature embedding with dz < d. In (6), we integrate
the decoder ψθDec with the black-box predictor f to construct a new black-box model f ′(z) :=
f(ψθDec(z)), which enables us to derive a ZO gradient estimate of reduced dimension:"
RELATED WORK,0.2964426877470356,∇θRnew(f(x)) ≈dφθEnc(Dθ(x))
RELATED WORK,0.30039525691699603,"dθ
ˆ∇zf ′(z) |z=φθEnc(Dθ(x)) .
(7)"
RELATED WORK,0.30434782608695654,"Assisted by AE, RGE of ˆ∇zf ′ has a reduced variance from O( d"
RELATED WORK,0.308300395256917,q ) to O( dz
RELATED WORK,0.31225296442687744,"q ). Meanwhile, the
least-variance CGE (4) also becomes feasible by setting the query number as q = dz."
RELATED WORK,0.31620553359683795,"Note that the eventual ZO estimate (7) is a function of the dθ ×dz Jacobian matrix ∇θ[φθEnc(Dθ(x))].
For ease of storing and computing the Jacobian matrix, we derive the following computationally-light
alternative of (7) (see derivation in Appendix A):"
RELATED WORK,0.3201581027667984,"∇θℓStab(θ) ≈∇θ[a⊤φθEnc(Dθ(x + δ))], a = ˆ∇zℓCE(f ′(z), f(x)) |z=φθEnc(Dθ(x+δ)) ,
(8)"
RELATED WORK,0.3241106719367589,"where recall that f ′(z) = f(ψθDec(z)), and ˆ∇denotes the ZO gradient estimate given by (3) or (4).
The computation advantage of (8) is that the derivative operation ∇θ can be applied to a scalar-valued
inner product built upon a pre-calculated ZO gradient estimate a."
RELATED WORK,0.32806324110671936,"Training ZO-AE-DS.
Recall from Fig. 3 that the proposed defensive system involves three compo-
nents: denoiser Dθ, AE ψθDec ◦φθEnc, and pre-deﬁned black-box predictor f. Thus, the parameters
to be optimized include θ, θDec and θEnc. To train ZO-AE-DS, we adopt a two-stage training
protocol. x White-box pre-training on AE: At the ﬁrst stage, we pre-train the AE model by calling a
standard FO optimizer (e.g., Adam) to minimize the reconstruction loss Ex∥φθDec(ψθEnc(x)) −x∥2
2.
The resulting AE will be used as the initialization of the second-stage training. We remark that the
denoising model Dθ can also be pre-trained. However, such a pre-training could hamper optimization,
i.e., making the second-stage training over θ easily trapped at a poor local optima. y End-to-end
training: At the second stage, we keep the pre-trained decoder φθDec intact and merge it into the
black-box system as shown in Fig. 3. We then optimize θ and θEnc by minimizing the DS-based
training loss (2), where the denoiser Dθ and the defensive operation R are replaced by ψθEnc ◦Dθ
and Rnew (6), respectively. In (2), minimization over the stability loss ℓStab(θ) calls the ZO estimate
of ∇θℓStab(θ), given by (7). In Appendix C.2, different training schemes are discussed."
EXPERIMENTS,0.33201581027667987,"5
EXPERIMENTS"
EXPERIMENTS,0.3359683794466403,"In this section, we demonstrate the effectiveness of our proposal through extensive experiments.
We will show that the proposed ZO-AE-DS outperforms a series of baselines when robustifying
black-box neural networks for secure image classiﬁcation and image reconstruction."
EXPERIMENT SETUP,0.33992094861660077,"5.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.3438735177865613,"Datasets and model architectures.
In the task of image classiﬁcation, we focus on CIFAR-10
and STL-10 datasets. In Appendix C.3, we demonstrate the effectiveness of ZO-AE-DS on the
high-dimension ImageNet images. In the task of image reconstruction, we consider the MNIST
dataset. To build ZO-AE-DS and its variants and baselines, we specify the prepended denoiser
Dθ as DnCNN (Zhang et al., 2017). We then implement task-speciﬁc AE for different datasets.
Superﬁcially, the dimension of encoded feature embedding, namely, dz in (6), is set as 192, 576
and 192 for CIFAR-10, STL-10 and MNIST, respectively. The architectures of AE are conﬁgured
following (Mao et al., 2016), and ablation study on the choice of AE is shown in Appendix C.1. To
specify the black-box image classiﬁcation model, we choose ResNet-110 for CIFAR-10 following
(Salman et al., 2020), and ResNet-18 for STL-10. It is worth noting that STL-10 contains 500 labeled
96 × 96 training images, and the pre-trained ResNet-18 achieves 76.6% test accuracy that matches to
state-of-the-art performance. For image reconstruction, we adopt a reconstruction network consisting
of convolution, deconvolution and ReLU layers, following (Raj et al., 2020)."
EXPERIMENT SETUP,0.34782608695652173,Published as a conference paper at ICLR 2022
EXPERIMENT SETUP,0.35177865612648224,"Baselines.
We will consider two variants of our proposed ZO-AE-DS: i) ZO-AE-DS using RGE
(3), ii) ZO-AE-DS using CGE (4). In addition, we will compare ZO-AE-DS with i) FO-AE-DS,
i.e., the ﬁrst-order implementation of ZO-AE-DS, ii) FO-DS, which developed in (Salman et al.,
2020), iii) RS-based certiﬁed training, proposed in (Cohen et al., 2019), and iv) ZO-DS, i.e., the ZO
implementation of FO-DS using RGE. Note that CGE is not applicable to ZO-DS due to the obstacle
of high dimensions. To our best knowledge, ZO-DS is the only query-based black-box defense
baseline that can be directly compared with ZO-AE-DS."
EXPERIMENT SETUP,0.3557312252964427,"Training setup.
We build the training pipeline of the proposed ZO-AE-DS following ‘Training
ZO-AE-DS’ in Sec. 4. To optimize the denoising model Dθ, we will cover two training schemes:
training from scratch, and pre-training & ﬁne-tuning. In the scenario of training from scratch, we
use Adam optimizer with learning rate 10−3 to train the model for 200 epochs and then use SGD
optimizer with learning rate 10−3 drop by a factor of 10 at every 200 epoch, where the total number
of epochs is 600. As will be evident later, training from scratch over Dθ leads to better performance
of ZO-AE-DS. In the scenario of pre-training & ﬁne-tuning, we use Adam optimizer to pre-train the
denoiser Dθ with the MSE loss ℓDenoise in (2) for 90 epochs and ﬁne-tune the denoiser with ℓStab for
200 epochs with learning rate 10−5 drop by a factor of 10 every 40 epochs. When implementing the
baseline FO-DS, we use the best training setup provided by (Salman et al., 2020). When implementing
ZO-DS, we reduce the initial learning rate to 10−4 for training from scratch and 10−6 for pre-training
& ﬁne-tuning to stabilize the convergence of ZO optimization. Furthermore, we set the smoothing
parameter µ = 0.005 for RGE and CGE. And to achieve a smooth predictor, we set the Gaussian
smoothing noise as δ ∈N(0, σ2I) with σ2 = 0.25. With the help of matrix operations and the
parallel computing power of the GPU, we optimize the training time to an acceptable range. The
averaged one-epoch training time on a single Nvidia RTX A6000 GPU is about ∼1min and ∼29min
for FO-DS and our proposed ZO method, ZO-AE-DS (CGE, q = 192), on the CIFAR-10 dataset."
EXPERIMENT SETUP,0.35968379446640314,"Evaluation metrics.
In the task of robust image classiﬁcation, the performance will be evaluated at
standard test accuracy (SA) and certiﬁed accuracy (CA). Here CA is a provable robust guarantee of the
Gaussian smoothing version of a predictive model. Let us take ZO-AE-DS as an example, the resulting
smooth image classiﬁer is given by fsmooth(x) := arg maxc Pδ∈N(0,σ2I)[Rnew(f(x + δ)) = c],
where Rnew is given by (6). Further, a certiﬁed radius of ℓ2-norm perturbation ball with respect
to an input example can be calculated following the RS approach provided in (Cohen et al., 2019).
As a result, CA at a given ℓ2-radius r is the percentage of the correctly classiﬁed data points whose
certiﬁed radii are larger than r. Note that if r = 0, then CA reduces to SA."
EXPERIMENT SETUP,0.36363636363636365,"FO
ZO-DS
ZO-AE-DS (Ours)"
EXPERIMENT SETUP,0.3675889328063241,"ℓ2-radius r
RS
FO-DS
FO-AE-DS
q = 20
(RGE)
q = 100
(RGE)
q = 192
(RGE)"
EXPERIMENT SETUP,0.3715415019762846,"q = 20
(RGE)
q = 100
(RGE)
q = 192
(RGE)
q = 192
(CGE)
0.00 (SA)
76.44
71.80
75.97
19.50
41.38
44.81
42.72
58.61
63.13
72.23
0.25
60.64
51.74
59.12
3.89
18.05
19.16
29.57
40.96
45.69
54.87
0.50
41.19
30.22
38.50
0.60
4.78
5.06
17.85
24.28
27.84
35.50
0.75
21.11
11.87
18.18
0.03
0.32
0.30
8.52
9.45
10.89
16.37"
EXPERIMENT SETUP,0.37549407114624506,"Table 2: SA (standard accuracy, %) and CA (certiﬁed accuracy, %) versus different values of ℓ2-radius r. Note
that SA corresponds to the case of r = 0. In both FO and ZO blocks, the best accuracies for each ℓ2-radius are
highlighted in bold."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.3794466403162055,"5.2
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION"
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.383399209486166,"Performance on CIFAR-10.
In Table 2, we present certiﬁed accuracies of ZO-AE-DS and its
variants/baselines versus different ℓ2-radii in the setup of (CIFAR-10, ResNet-110). Towards a
comprehensive comparison, different RGE-based variants of ZO-AE-DS and ZO-DS are demonstrated
using the query number q ∈{20, 100, 192}. First, the comparison between ZO-AE-DS and ZO-DS
shows that our proposal signiﬁcantly outperforms ZO-DS ranging from the low query number q = 20
to the high query number q = 192 when RGE is applied. Second, we observe that the use of CGE
yields the best CA and SA (corresponding to r = 0). The application of CGE is beneﬁted from
AE, which reduces the dimension from d = 32 × 32 × 3 to dz = 192. In particular, CGE-based
ZO-AE-DS improves the case studied in Table 1 from 5.06% to 35.5% at the ℓ2-radius r = 0.5. Third,
although FO-AE-DS yields CA improvement over FO-DS in the white-box context, the improvement
achieved by ZO-AE-DS (vs. ZO-DS) for black-box defense is much more signiﬁcant. This implies"
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.38735177865612647,Published as a conference paper at ICLR 2022
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.391304347826087,"that the performance of black-box defense relies on a proper solution (namely, ZO-AE-DS) to tackle
the challenge of ZO optimization in high dimensions. Fourth, RS outperforms the ZO methods. This
is not surprising since RS is a known white-box certiﬁably robust training approach. In Appendix B,
we demonstrate the consistent effectiveness of ZO-AE-DS under different denoisier and classiﬁers."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.3952569169960474,"Performance on STL-10.
In Table 3, we evaluate the performance of ZO-AE-DS for STL-10
image classiﬁcation. For comparison, we also represent the performance of FO-DS, FO-AE-DS, and
ZO-DS. Similar to Table 2, the improvement brought by our proposal over ZO-DS is evident, with at
least 10% SA/CA improvement across different ℓ2-radii."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.39920948616600793,STL-10
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4031620553359684,"ℓ2-radius r
FO-DS
FO-AE-DS
ZO-DS
(RGE, q = 576)
ZO-AE-DS
(CGE, q = dz = 576)
0.00 (SA)
53.36
54.26
38.60
45.67
0.25
35.83
43.99
21.50
35.78
0.50
21.61
34.85
9.58
26.70
0.75
9.86
25.56
3.29
17.91"
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.40711462450592883,"Table 3: CA (certiﬁed accuracy, %) vs. different ℓ2-radii for image
classiﬁcation on STL-10."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.41106719367588934,"When
comparing
ZO-AE-DS
with FO-DS, we observe that ours
introduces a 7% degradation in
SA (at r = 0). This is differ-
ent from CIFAR-10 classiﬁcation.
There might be two reasons for
the degradation of SA in STL-10.
First, the size of a STL-10 image
is 9× larger than a CIFAR-10 im-
age. Thus, the over-reduced feature dimension could hamper SA. In this example, we set dz = 576,
which is only 3× larger than dz = 192 used for CIFAR-10 classiﬁcation. Second, the variance of ZO
gradient estimates has a larger effect on the performance of STL-10 than that of CIFAR-10, since
the former only contains 500 labeled images, leading to a challenging training task. Despite the
degradation of SA, ZO-AE-DS outperforms FO-DS in CA, especially when facing a large ℓ2-radius.
This is consistent with Table 2. The rationale is that AE can be regarded as an extra smoothing
operation for the image classiﬁer, and thus improves certiﬁed robustness over FO-DS, even if the
latter is designed in a white-box setup. If we compare ZO-AE-DS with FO-AE-DS, then the FO
approach leads to the best performance due to the high-accuracy of gradient estimates."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4150197628458498,"Figure 4: Comparison between non-AE-based and
AE-based methods in CA vs.
different ℓ2-radii.
Dashed lines: Models obtained by non-AE-based
methods; Solid lines: Models obtained by AE-based
methods."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4189723320158103,"Advantage of AE on ZO optimization.
Ex-
tended from Table 2, Fig. 4 presents the complete
CA curve of non-AE-based and AE-based methods
vs. the value of ℓ2-radius in the example of (CIFAR-
10, ResNet-110). As we can see, ZO-AE-DS using
RGE with the smallest query number q = 20 has
outperformed ZO-DS using RGE with the largest
query number q = 192. This shows the vital role
of AE on ZO optimization. Meanwhile, consistent
with Table 2 and Table 3, the best model achieved
by ZO-AE-DS using CGE could be even better than
the FO baseline FO-DS since AE could play a sim-
ilar role on the smoothing operation. Furthermore,
as the query number q increases, the improvement
of ZO-AE-DS grows, towards the performance of
FO-AE-DS."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.42292490118577075,"ZO-AE-DS (CGE, q = 192)
ℓ2-radius r
Training from scratch
Pre-training + ﬁne-tuning
0.00
72.23
59.74
0.25
54.87
42.61
0.50
35.50
26.26
0.75
16.37
11.13"
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4268774703557312,"Table 4: ZO-AE-DS using different denoiser training
schemes under (CIFAR-10, ResNet-110)."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4308300395256917,"Effect of training scheme on ZO-AE-DS.
In
Table 4, we present the impact of training
scheme (over the denoiser Dθ) on the CA perfor-
mance of ZO-AE-DS versus different ℓ2-radii.
Two training schemes, training from scratch and
pre-training + ﬁne-tuning, are considered. As
we can see, training from scratch for Dθ leads to
the better performance of ZO-AE-DS than pre-
training + ﬁne-tuning. This is because the application of pre-training to Dθ could make optimization
easily get trapped at a local optima. We list other ablation studies in Appendix C."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.43478260869565216,"5.3
EXPERIMENT RESULTS ON IMAGE RECONSTRUCTION."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.43873517786561267,"In what follows, we apply the proposed ZO-AE-DS to robustifying a black-box image reconstruction
network. The goal of image reconstruction is to recover the original image from a noisy measurement."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4426877470355731,Published as a conference paper at ICLR 2022
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.44664031620553357,"Following (Antun et al., 2020; Raj et al., 2020), we generate the noisy measurement following a
linear observation model y = Ax, where A is a sub-sampling matrix (e.g., Gaussian sampling),
and x is an original image. A pre-trained image reconstruction network (Raj et al., 2020) then
takes A⊤y as the input to recover x. To evaluate the reconstruction performance, we adopt two
metrics (Antun et al., 2020), the root mean squared error (RMSE) and structural similarity (SSIM).
SSIM is a supplementary metric to RMSE, since it gives an accuracy indicator when evaluating the
similarity between the true image and its estimate at ﬁne-level regions. The vulnerability of image
reconstruction networks to adversarial attacks, e.g., PGD attacks (Madry et al., 2018), has been shown
in (Antun et al., 2020; Raj et al., 2020; Wolf, 2019)."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4505928853754941,"When the image reconstructor is given as a black-box model, spurred by above, Table 5 presents the
performance of image reconstruction using various training methods against adversarial attacks with
different perturbation strengths. As we can see, compared to the normally trained image reconstructor
(i.e., ‘Standard’ in Table 5), all robustiﬁcation methods lead to degraded standard image reconstruction
performance in the non-adversarial context (i.e., ∥δ∥2 = 0). But the worst performance is provided
by ZO-DS. When the perturbation strength increases, the model achieved by standard training
becomes over-sensitive to adversarial perturbations, yielding the highest RMSE and the lowest SSIM.
Furthermore, we observe that the proposed black-box defense ZO-AE-DS yields very competitive
and even better performance with respect to FO defenses. In Fig. 5, we provide visualizations of
the reconstructed images using different approaches at the presence of reconstruction-evasion PGD
attacks. For example, the comparison between Fig. 5-(f) and (b)/(d) clearly shows the robustness
gained by ZO-AE-DS."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.45454545454545453,Image reconstruction on MNIST
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.45849802371541504,"Method
∥δ∥2 = 0
∥δ∥2 = 1
∥δ∥2 = 2
∥δ∥2 = 3
∥δ∥2 = 4
RMSE
SSIM
RMSE
SSIM
RMSE
SSIM
RMSE
SSIM
RMSE
SSIM
Standard
0.112
0.888
0.346
0.417
0.493
0.157
0.561
0.057
0.596
0.014
FO-DS
0.143
0.781
0.168
0.703
0.221
0.544
0.278
0.417
0.331
0.337
ZO-DS
0.197
0.521
0.217
0.474
0.262
0.373
0.313
0.284
00.356
0.225
FO-AE-DS
0.139
0.792
0.162
0.717
0.215
0.554
0.274
0.421
0.329
0.341
ZO-AE-DS
0.141
0.79
0.164
0.718
0.217
0.551
0.277
0.42
0.33
0.339"
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4624505928853755,"Table 5:
Performance of image reconstruction using different methods at various attack scenarios. Here
‘standard’ refers to the original image reconstructor without making any robustiﬁcation. Four robustiﬁcation
methods are presented including FO-DS, ZO-DS (RGE, q = 192), FO-AE-DS, and ZO-AE-DS (CGE, q = 192).
The performance metrics RMSE and SSIM are measured by adversarial example (x + δ), generated by 40-step
ℓ2 PGD attacks under different values of ℓ2 perturbation norm ∥δ∥2."
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.466403162055336,"(a) Ground truth
(b) Standard
(c) FO-DS"
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.47035573122529645,"(d) ZO-DS
(e) FO-AE-DS
(f) ZO-AE-DS"
EXPERIMENT RESULTS ON IMAGE CLASSIFICATION,0.4743083003952569,"Figure 5: Visualization for Image Reconstruction under ℓ2 PGD attack (Step = 40, ϵ = 1.0 ). Original: base
reconstruction network. ZO-DS: RGE with q = 192. ZO-AE-DS: CGE with q = 192"
CONCLUSION,0.4782608695652174,"6
CONCLUSION"
CONCLUSION,0.48221343873517786,"In this paper, we study the problem of black-box defense, aiming to secure black-box models
against adversarial attacks using only input-output model queries. The proposed black-box learning
paradigm is new to adversarial defense, but is also challenging to tackle because of the black-box
optimization nature. To solve this problem, we integrate denoised smoothing (DS) with ZO (zeroth-
order) optimization to build a feasible black-box defense framework. However, we ﬁnd that the direct
application of ZO optimization makes the defense ineffective and difﬁcult to scale. We then propose
ZO-AE-DS, which leverages autoencoder (AE) to bridge the gap between FO and ZO optimization.
We show that ZO-AE-DS reduces the variance of ZO gradient estimates and improves the defense
and optimization performance in a signiﬁcant manner. Lastly, we evaluate the superiority of our
proposal to a series of baselines in both image classiﬁcation and image reconstruction tasks."
CONCLUSION,0.48616600790513836,Published as a conference paper at ICLR 2022
CONCLUSION,0.4901185770750988,ACKNOWLEDGMENT
CONCLUSION,0.49407114624505927,"Yimeng Zhang, Yuguang Yao, Jinghan Jia, and Sijia Liu are supported by the DARPA RED program."
REFERENCES,0.4980237154150198,REFERENCES
REFERENCES,0.5019762845849802,"Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, and R Venkatesh Babu. Boosting adversarial robustness
using feature level stochastic smoothing. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 93–102, 2021."
REFERENCES,0.5059288537549407,"Ahmed Aldahdooh, Wassim Hamidouche, Sid Ahmed Fezza, and Olivier Deforges. Adversarial example
detection for dnn models: A review. arXiv preprint arXiv:2105.00203, 2021."
REFERENCES,0.5098814229249012,"Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial training.
arXiv preprint arXiv:2007.02617, 2020."
REFERENCES,0.5138339920948617,"Vegard Antun, Francesco Renna, Clarice Poon, Ben Adcock, and Anders C Hansen. On instabilities of deep
learning in image reconstruction and the potential costs of ai. Proceedings of the National Academy of
Sciences, 117(48):30088–30095, 2020."
REFERENCES,0.5177865612648221,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018."
REFERENCES,0.5217391304347826,"Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint
arXiv:1712.09665, 2017."
REFERENCES,0.525691699604743,"Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda. A uniﬁed view of
piecewise linear neural network veriﬁcation. In Advances in Neural Information Processing Systems, pp.
4790–4799, 2018."
REFERENCES,0.5296442687747036,"HanQin Cai, Daniel Mckenzie, Wotao Yin, and Zhenliang Zhang. Zeroth-order regularized optimization (zoro):
Approximately sparse gradients and adaptive sampling. SIOPT, 2020."
REFERENCES,0.5335968379446641,"HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order block coordinate descent algorithm
for huge-scale black-box optimization. In International Conference on Machine Learning, pp. 1193–1203.
PMLR, 2021."
REFERENCES,0.5375494071146245,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium
on S&P, 2017."
REFERENCES,0.541501976284585,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data improves
adversarial robustness. arXiv preprint arXiv:1905.13736, 2019."
REFERENCES,0.5454545454545454,"Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based
black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th
ACM Workshop on Artiﬁcial Intelligence and Security, pp. 15–26. ACM, 2017."
REFERENCES,0.549407114624506,"T. Chen, S. Liu, S. Chang, Y. Cheng, L. Amini, and Z. Wang. Adversarial robustness: From self-supervised
pretraining to ﬁne-tuning. In CVPR, 2020."
REFERENCES,0.5533596837944664,"Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural networks.
In International Symposium on Automated Technology for Veriﬁcation and Analysis, pp. 251–268. Springer,
2017."
REFERENCES,0.5573122529644269,"Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certiﬁed adversarial robustness via randomized smoothing.
arXiv preprint arXiv:1902.02918, 2019."
REFERENCES,0.5612648221343873,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In International Conference on Machine Learning, pp. 2206–2216. PMLR, 2020."
REFERENCES,0.5652173913043478,"Souradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan, and Ashish Tiwari. Output range analysis for deep
neural networks. arXiv preprint arXiv:1709.09130, 2017."
REFERENCES,0.5691699604743083,"Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. A dual approach
to scalable veriﬁcation of deep networks. UAI, 2018."
REFERENCES,0.5731225296442688,"Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In International
Symposium on Automated Technology for Veriﬁcation and Analysis, pp. 269–286. Springer, 2017."
REFERENCES,0.5770750988142292,Published as a conference paper at ICLR 2022
REFERENCES,0.5810276679841897,"Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Ta-
dayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning models. arXiv preprint
arXiv:1707.08945, 2017."
REFERENCES,0.5849802371541502,"Samuel G Finlayson, John D Bowers, Joichi Ito, Jonathan L Zittrain, Andrew L Beam, and Isaac S Kohane.
Adversarial attacks on medical machine learning. Science, 363(6433):1287–1289, 2019."
REFERENCES,0.5889328063241107,"Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit conﬁdence informa-
tion and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security, pp. 1322–1333, 2015."
REFERENCES,0.5928853754940712,"Xiang Gao, Bo Jiang, and Shuzhong Zhang. On the information-adaptive variants of the admm: an iteration
complexity perspective. Journal of Scientiﬁc Computing, 76(1):327–363, 2018."
REFERENCES,0.5968379446640316,"S. Ghadimi and G. Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming. SIAM
Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.6007905138339921,"Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. Adversarial and clean data are not twins. arXiv preprint
arXiv:1704.04960, 2017."
REFERENCES,0.6047430830039525,"Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
ICLR, 2015."
REFERENCES,0.6086956521739131,"Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the
(statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017."
REFERENCES,0.6126482213438735,"Chuan Guo, Mayank Rana, Moustapha Cissé, and Laurens van der Maaten. Countering adversarial images using
input transformations. arXiv preprint arXiv:1711.00117, 2017."
REFERENCES,0.616600790513834,"Zhichao Huang and Tong Zhang. Black-box adversarial attack with transferable model-based embedding. In
International Conference on Learning Representations, 2020."
REFERENCES,0.6205533596837944,"Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited
queries and information. In International Conference on Machine Learning, pp. 2137–2146. PMLR, 2018a."
REFERENCES,0.6245059288537549,"Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial attacks with
bandits and priors. arXiv preprint arXiv:1807.07978, 2018b."
REFERENCES,0.6284584980237155,"Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efﬁcient smt solver
for verifying deep neural networks. In International Conference on Computer Aided Veriﬁcation, pp. 97–117.
Springer, 2017."
REFERENCES,0.6324110671936759,"Aviral Kumar and Sergey Levine. Model inversion networks for model-based optimization. arXiv preprint
arXiv:1912.13464, 2019."
REFERENCES,0.6363636363636364,"Juncheng Li, Frank Schmidt, and Zico Kolter. Adversarial camera stickers: A physical camera-based attack on
deep learning systems. In International Conference on Machine Learning, pp. 3896–3904, 2019."
REFERENCES,0.6403162055335968,"X. Lian, H. Zhang, C.-J. Hsieh, Y. Huang, and J. Liu. A comprehensive linear speedup analysis for asynchronous
stochastic parallel optimization from zeroth-order to ﬁrst-order. In Advances in Neural Information Processing
Systems, pp. 3054–3062, 2016."
REFERENCES,0.6442687747035574,"Siyuan Liang, Baoyuan Wu, Yanbo Fan, Xingxing Wei, and Xiaochun Cao. Parallel rectangle ﬂip attack:
A query-based black-box attack against object detection. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 7697–7707, 2021."
REFERENCES,0.6482213438735178,"S. Liu, B. Kailkhura, P.-Y. Chen, P. Ting, S. Chang, and L. Amini. Zeroth-order stochastic variance reduction for
nonconvex optimization. Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.6521739130434783,"S. Liu, P.-Y. Chen, X. Chen, and M. Hong. signSGD via zeroth-order oracle. In International Conference on
Learning Representations, 2019."
REFERENCES,0.6561264822134387,"S. Liu, S. Lu, X. Chen, Y. Feng, K. Xu, A. Al-Dujaili, M. Hong, and U.-M. O’Reilly. Min-max optimization
without gradients: Convergence and applications to adversarial ML. In ICML, 2020a."
REFERENCES,0.6600790513833992,"Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K Varshney. A
primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances,
and applications. IEEE Signal Processing Magazine, 37(5):43–54, 2020b."
REFERENCES,0.6640316205533597,Published as a conference paper at ICLR 2022
REFERENCES,0.6679841897233202,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. ICLR, 2018."
REFERENCES,0.6719367588932806,"Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang. Image restoration using very deep convolutional encoder-
decoder networks with symmetric skip connections. Advances in neural information processing systems, 29:
2802–2810, 2016."
REFERENCES,0.6758893280632411,"Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of
the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135–147. ACM, 2017."
REFERENCES,0.6798418972332015,"Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial perturbations.
arXiv preprint arXiv:1702.04267, 2017."
REFERENCES,0.6837944664031621,"Seong Joon Oh, Bernt Schiele, and Mario Fritz. Towards reverse-engineering black-box neural networks. In
Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 121–144. Springer, 2019."
REFERENCES,0.6877470355731226,"Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The
limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European
Symposium on, pp. 372–387. IEEE, 2016."
REFERENCES,0.691699604743083,"Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference
on Computer and Communications Security, pp. 506–519. ACM, 2017."
REFERENCES,0.6956521739130435,"Adnan Qayyum, Junaid Qadir, Muhammad Bilal, and Ala Al-Fuqaha. Secure and robust machine learning for
healthcare: A survey. IEEE Reviews in Biomedical Engineering, 14:156–180, 2020."
REFERENCES,0.6996047430830039,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
Certiﬁed defenses against adversarial examples.
In International Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=Bys4ob-Rb."
REFERENCES,0.7035573122529645,"Ankit Raj, Yoram Bresler, and Bo Li. Improving robustness of deep-learning-based image reconstruction. In
International Conference on Machine Learning, pp. 7932–7942. PMLR, 2020."
REFERENCES,0.7075098814229249,"Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien Bubeck.
Provably robust deep learning via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584,
2019."
REFERENCES,0.7114624505928854,"Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Denoised smoothing: A provable
defense for pretrained classiﬁers. NeurIPS, 2020."
REFERENCES,0.7154150197628458,"Hadi Salman, Saachi Jain, Eric Wong, and Aleksander M ˛adry. Certiﬁed patch robustness via smoothed vision
transformers. arXiv preprint arXiv:2110.07719, 2021."
REFERENCES,0.7193675889328063,"Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural Information
Processing Systems, pp. 3353–3364, 2019."
REFERENCES,0.7233201581027668,"Sanjay M Sisodiya, Christopher D Whelan, Sean N Hatton, Khoa Huynh, Andre Altmann, Mina Ryten,
Annamaria Vezzani, Maria Eugenia Caligiuri, Angelo Labate, and Antonio Gambardella. The enigma-
epilepsy working group: Mapping disease from large data sets. Human brain mapping, 2020."
REFERENCES,0.7272727272727273,"Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli, et al. Are labels required for improving adversarial
robustness? arXiv preprint arXiv:1905.13725, 2019."
REFERENCES,0.7312252964426877,"Jiachen Sun, Yulong Cao, Qi Alfred Chen, and Z Morley Mao. Towards robust lidar-based perception in
autonomous driving: General black-box adversarial sensor attack and countermeasures. In 29th {USENIX}
Security Symposium, pp. 877–894, 2020."
REFERENCES,0.7351778656126482,"Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial
example defenses. arXiv preprint arXiv:2002.08347, 2020."
REFERENCES,0.7391304347826086,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness
may be at odds with accuracy. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=SyxAb30cY7."
REFERENCES,0.7430830039525692,"Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming
Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural
networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 742–749, 2019."
REFERENCES,0.7470355731225297,Published as a conference paper at ICLR 2022
REFERENCES,0.7509881422924901,"Jelica Vasiljevi´c, Friedrich Feuerhake, Cédric Wemmert, and Thomas Lampert. Self adversarial attack as an
augmentation method for immunohistochemical stainings. In 2021 IEEE 18th International Symposium on
Biomedical Imaging (ISBI), pp. 1939–1943. IEEE, 2021."
REFERENCES,0.7549407114624506,"Yixiang Wang, Jiqiang Liu, Xiaolin Chang, Jianhua Wang, and Ricardo J Rodríguez. Di-aa: An interpretable
white-box attack for fooling deep neural networks. arXiv preprint arXiv:2110.07305, 2021."
REFERENCES,0.758893280632411,"Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S Dhillon, and
Luca Daniel. Towards fast computation of certiﬁed robustness for relu networks. ICML, 2018a."
REFERENCES,0.7628458498023716,"Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel.
Evaluating the robustness of neural networks: An extreme value theory approach. ICLR, 2018b."
REFERENCES,0.766798418972332,"Adva Wolf. Making medical image reconstruction adversarially robust, 2019."
REFERENCES,0.7707509881422925,"Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial
polytope. arXiv preprint arXiv:1711.00851, 2017."
REFERENCES,0.7747035573122529,"Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial defenses. In
Advances in Neural Information Processing Systems, pp. 8400–8409, 2018."
REFERENCES,0.7786561264822134,"Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In ICLR,
2020."
REFERENCES,0.782608695652174,"Kaidi Xu, Sijia Liu, Pu Zhao, Pin-Yu Chen, Huan Zhang, Quanfu Fan, Deniz Erdogmus, Yanzhi Wang, and Xue
Lin. Structured adversarial attack: Towards general implementation and better interpretability. In ICLR, 2019."
REFERENCES,0.7865612648221344,"Kaidi Xu, Gaoyuan Zhang, S. Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, and
Xue Lin. Adversarial T-Shirt! evading person detectors in a physical world. In ECCV, 2020."
REFERENCES,0.7905138339920948,"Zheng Yuan, Jie Zhang, Yunpei Jia, Chuanqi Tan, Tao Xue, and Shiguang Shan. Meta gradient adversarial attack.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7748–7757, 2021."
REFERENCES,0.7944664031620553,"Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate once:
Accelerating adversarial training via maximal principle. arXiv preprint arXiv:1905.00877, 2019a."
REFERENCES,0.7984189723320159,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoretically
principled trade-off between robustness and accuracy. ICML, 2019b."
REFERENCES,0.8023715415019763,"Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual
learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142–3155, 2017."
REFERENCES,0.8063241106719368,"Shudong Zhang, Haichang Gao, and Qingxun Rao. Defense against adversarial attacks by reconstructing images.
IEEE Transactions on Image Processing, 30:6117–6129, 2021."
REFERENCES,0.8102766798418972,Published as a conference paper at ICLR 2022
REFERENCES,0.8142292490118577,"A
DERIVATION OF (8)"
REFERENCES,0.8181818181818182,"First, based on (2) and (6), the stability loss corresponding to ZO-AE-DS is given by"
REFERENCES,0.8221343873517787,"ℓStab(θ) = ℓCE (f ′(z), f(x)) := g(z), where f ′(z) = f (φθDec (z)) , z = ψθEnc (Dθ(x + δ)) .
(9)"
REFERENCES,0.8260869565217391,We then take the derivative of ℓStab(θ) w.r.t. θ. This yields
REFERENCES,0.8300395256916996,∇θℓStab(θ) = dz
REFERENCES,0.83399209486166,"dθ
dg(z)"
REFERENCES,0.8379446640316206,"dz
|z=ψθEnc(Dθ(x+δ)) ,
(10)"
REFERENCES,0.841897233201581,where dz
REFERENCES,0.8458498023715415,dθ ∈Rdθ×d and dg(z)
REFERENCES,0.849802371541502,"dz
∈Rd."
REFERENCES,0.8537549407114624,"Since g(z) involves the black-box function f, we ﬁrst compute its ZO gradient estimate following (3)
or (4) and obtain dg(z)"
REFERENCES,0.857707509881423,"dz
|z=ψθEnc(Dθ(x+δ)) ≈ˆ∇zg(z) |z=ψθEnc(Dθ(x+δ)) := a.
(11)"
REFERENCES,0.8616600790513834,"Substituting the above into (10), we obtain"
REFERENCES,0.8656126482213439,∇θℓStab(θ) = dz
REFERENCES,0.8695652173913043,dθ a =   da⊤z
REFERENCES,0.8735177865612648,"dθ1
da⊤z"
REFERENCES,0.8774703557312253,"dθ2...
da⊤z dθdθ "
REFERENCES,0.8814229249011858,"
= da⊤z"
REFERENCES,0.8853754940711462,"dθ
= ∇θ[a⊤φθEnc(Dθ(x + δ))],
(12)"
REFERENCES,0.8893280632411067,where the last equality holds based on (9). This completes the derivation.
REFERENCES,0.8932806324110671,"B
COMBINATION OF DIFFERENT DENOISERS AND CLASSIFIERS"
REFERENCES,0.8972332015810277,"Table A1 presents the certiﬁed accuracies of our proposal using different denoiser models (Wide-
DnCnn vs. DnCnn) and image classiﬁer (Vgg-16)."
REFERENCES,0.9011857707509882,"DnCnn & VGG-16
Wide-DnCnn & VGG-16"
REFERENCES,0.9051383399209486,"ℓ2-radius r
FO-DS
FO-AE-DS
ZO-AE-DS
(CGE, q = dz = 192)
FO-DS
FO-AE-DS
ZO-AE-DS
(CGE, q = dz = 192)
0.00 (SA)
71.37
73.75
71.92
66.57
75.14
72.97
0.25
51.37
54.74
54.33
50.1
57.45
54.92
0.50
30.21
34.6
34.39
31.52
37.59
34.2
0.75
11.72
15.45
15.36
13.94
17.64
15.7"
REFERENCES,0.9090909090909091,"Table A1: CA (certiﬁed accuracy, %) vs. different ℓ2-radii for different combinations of denoisers and classiﬁer."
REFERENCES,0.9130434782608695,"C
ADDITIONAL EXPERIMENTS AND ABLATION STUDIES"
REFERENCES,0.9169960474308301,"In what follows, we will show the ablation study on the choice of AE architectures in Appendix C.1.
Afterwards, we will show the performance of FO-AE-DS versus different training schemes in
Appendix C.2. Finally, we will show the performance of our proposal on the high-dimension
ImageNet images in Appendix C.3."
REFERENCES,0.9209486166007905,"C.1
THE PERFORMANCE OF FO-AE-DS WITH DIFFERENT AUTOENCODERS."
REFERENCES,0.924901185770751,"Table. A2 presents the certiﬁed accuracy performance of FO-AE-DS with different autoencoders
(AE). As we can see, if AE-96 is used (namely, the encoded dimension is half of AE-192 used in
the paper), then we observe a slight performance drop. This is a promising result as we can further
reduce the query complexity by choosing a different autoencoder since the use of CGE has to be
matched with the encoded dimension."
REFERENCES,0.9288537549407114,Published as a conference paper at ICLR 2022
REFERENCES,0.932806324110672,"ℓ2-radius r
AE-96
AE-192
0.00 (SA)
75.57
75.97
0.25
58.07
59.12
0.50
37.09
38.50
0.75
17.05
18.18"
REFERENCES,0.9367588932806324,"Table A2: CA (certiﬁed accuracy, %) vs. different ℓ2-radii for FO-AE-DS with different AutoEncoders."
REFERENCES,0.9407114624505929,"C.2
THE PERFORMANCE OF FO-AE-DS WITH DIFFERENT TRAINING SCHEMES."
REFERENCES,0.9446640316205533,"Table. A3 presents the certiﬁed accuracy of FO-AE-DS (ﬁrst-order implementation of ZO-AE-DS)
with different training schemes. Training both denoiser and encoder is the default setting. As we
can see, only training the denoiser would bring performance degradation, and training both denoiser
and AE does boost the performance. It is worth noting that FO-AE-DS with ""train the denoiser
and AE"" training scheme can be regarded as the FO-DS treating the combination of the original
denoiser and the same AE used in FO-AE-DS as a new denoiser, which cannot be implemented for
ZO-AE-DS since the decoder of ZO-AE-DS is merged into the black-box classiﬁer and its parameters
cannot be updated. Furthermore, the key of the introduced AE is to reduce the variable dimension for
Zeroth-Order (ZO) gradient estimation."
REFERENCES,0.9486166007905138,"ℓ2-radius r
FO-DS"
REFERENCES,0.9525691699604744,"FO-AE-DS
(only train
the denoiser)"
REFERENCES,0.9565217391304348,"FO-AE-DS
(train the denoiser
and encoder)"
REFERENCES,0.9604743083003953,"FO-AE-DS
(train the denoiser
and the AE)
0.00 (SA)
71.80
73.34
75.97
75.76
0.25
51.74
55.61
59.12
58.14
0.50
30.22
35.68
38.50
38.88
0.75
11.87
15.92
18.18
18.48"
REFERENCES,0.9644268774703557,"Table A3: CA (certiﬁed accuracy, %) vs. different ℓ2-radii for FO-AE-DS with different training schemes."
REFERENCES,0.9683794466403162,"C.3
THE PERFORMANCE OF ZO-AE-DS ON IMAGENET IMAGES."
REFERENCES,0.9723320158102767,"To evaluate the performance of ZO-AE-DS on the Restricted ImageNet (R-ImageNet) dataset, a
10-class subset of ImageNet with 38472 images for training and 1500 images for testing, similar
to (Tsipras et al., 2019). Due to our limited computing resources, we are not able to scale up our
experiment to the full ImageNet dataset, but the purpose of evaluating on high-dimension images
remains the same. In the implementation of ZO-AE-DS, we choose an AE with an aggressive
compression (130:1), which is to compress the original 3 × 224 × 224 images into the 1152 × 1 × 1
feature dimension. We compare the certiﬁed accuracy (CA) performance of our proposed ZO-AE-DS
(using CGE) with the black-box baseline ZO-DS, and the white-box baselines FO-DS and FO-AE-DS.
Results are summarized in the following table."
REFERENCES,0.9762845849802372,"As we can see, (1) when considering the black-box classiﬁer, the proposed ZO-AE-DS still signif-
icantly outperforms the direct ZO implementation of DS. This shows the importance of variance
reduction of query-based gradient estimates. (2) Since ZO-AE-DS and FO-AE-DS used an aggressive
AE structure, the performance drops compared to FO-DS. (3) the use of high-resolution images
would make the black-box defense much more challenging. However, ZO-AE-DS is still a principled
black-box defense method that can achieve reasonable performance."
REFERENCES,0.9802371541501976,"ℓ2-radius r
FO-DS
FO-AE-DS"
REFERENCES,0.9841897233201581,"ZO-AE-DS
(RGE, q =1152
and encoder)"
REFERENCES,0.9881422924901185,"ZO-AE-DS
(CGE, q=1152)"
REFERENCES,0.9920948616600791,"0.00 (SA)
89.33
71.07
26.93
63.60
0.25
81.67
63.40
18.40
52.80
0.50
68.87
53.60
11.67
43.13
0.75
49.80
42.87
5.53
32.73"
REFERENCES,0.9960474308300395,"Table A4: CA (certiﬁed accuracy, %) vs. different ℓ2-radii for FO-AE-DS on ImageNet Images."
