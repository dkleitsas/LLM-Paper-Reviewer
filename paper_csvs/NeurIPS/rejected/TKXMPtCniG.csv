Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018796992481203006,"As models for nature language processing (NLP), computer vision (CV) and
1"
ABSTRACT,0.0037593984962406013,"recommendation systems (RS) require surging computation, a large number of
2"
ABSTRACT,0.005639097744360902,"GPUs/TPUs are paralleled with a large batch (LB) to improve training throughput.
3"
ABSTRACT,0.007518796992481203,"Training such LB tasks often converges to sharp minimum and downgrades final
4"
ABSTRACT,0.009398496240601503,"precision. Adversarial learning (ConAdv) and LANS method scales ImageNet
5"
ABSTRACT,0.011278195488721804,"and BERT pretraining up to 96k batch size. In this work, we develop the variance
6"
ABSTRACT,0.013157894736842105,"reduced gradient descent technique (VRGD) based on the gradient signal to noise ra-
7"
ABSTRACT,0.015037593984962405,"tio (GSNR) and apply it onto popular optimizers such as SGD/Adam/LARS/LAMB.
8"
ABSTRACT,0.016917293233082706,"We carry out a theoretical analysis of VR-SGD’s convergence rate to explain its
9"
ABSTRACT,0.018796992481203006,"fast training dynamics, and a generalization analysis to demonstrate its smaller
10"
ABSTRACT,0.020676691729323307,"generalization gap on LB training. Comprehensive experiments demonstrate that
11"
ABSTRACT,0.022556390977443608,"VRGD can remarkably accelerate training (1.7 ∼4×), narrow the generalization
12"
ABSTRACT,0.02443609022556391,"gap and improve final accuracy. We push the batch size limit of BERT pretraining
13"
ABSTRACT,0.02631578947368421,"up to 128k/64k and DLRM to 512k without noticeable accuracy loss. We improve
14"
ABSTRACT,0.02819548872180451,"ImageNet Top-1 accuracy at 96k by 0.52pp than LARS and significantly reduce
15"
ABSTRACT,0.03007518796992481,"generalization gap by 68.3%.
16"
INTRODUCTION,0.03195488721804511,"1
Introduction
17"
INTRODUCTION,0.03383458646616541,"Recent machine learning models have grown wider and deeper in their architectures (e.g., GPT-3
18"
INTRODUCTION,0.03571428571428571,"[Floridi and Chiriatti, 2020], M6 [Lin et al., 2021], Switch Transformer [Fedus et al., 2021]). Training
19"
INTRODUCTION,0.03759398496240601,"complex models may consume more training data to converge, which needs a surge in computing
20"
INTRODUCTION,0.039473684210526314,"capacity and efficiency. However, hardware improvement can not keep pace with the expansion of
21"
INTRODUCTION,0.041353383458646614,"model calculations [Bommasani et al., 2021].
22"
INTRODUCTION,0.043233082706766915,"Several techniques to speed up training are proposed. The aggregation and scattering of gradients
23"
INTRODUCTION,0.045112781954887216,"among massive workers requires an efficient synchronization algorithm. Since the communication
24"
INTRODUCTION,0.046992481203007516,"bandwidth between GPUs/TPUs is much higher than CPU-GPU (e.g., NVLink, Foley and Danskin
25"
INTRODUCTION,0.04887218045112782,"[2017]), several efficient synchronization strategies such as Ring-All-Reduce [Gibiansky, 2017] and
26"
INTRODUCTION,0.05075187969924812,"software toolkits like Horovod [Sergeev and Del Balso, 2018] are proposed to replace the traditional
27"
INTRODUCTION,0.05263157894736842,"PS-Worker framework [Li et al., 2014b,a]. In addition, training with LB can notably improve
28"
INTRODUCTION,0.05451127819548872,"throughput [You et al., 2017b; Hoffer et al., 2019]. You et al. [2020] successfully train BERT using
29"
INTRODUCTION,0.05639097744360902,"1024 TPUs and a LB (64k) within 76 minutes. It demonstrates the efficiency of GPUs/TPUs in large
30"
INTRODUCTION,0.05827067669172932,"scale parallel tasks. Small batch (SB) is not able to fully utilize those powerful GPUs/TPUs.
31"
INTRODUCTION,0.06015037593984962,"However, Keskar et al. [2017] theoretically analyze the LB training and finds that it can be easily
32"
INTRODUCTION,0.06203007518796992,"trapped into sharp local minimum, leading to strong generalization gap. Hoffer et al. [2017] indicate
33"
INTRODUCTION,0.06390977443609022,"that the generalization gap can be attributed to the fewer update steps in LB training compared with
34"
INTRODUCTION,0.06578947368421052,"SB when using identical epochs. Dai and Zhu [2018] theoretically demonstrate that training with
35"
INTRODUCTION,0.06766917293233082,"more steps or expanding the learning rate to batch size ratio helps to converge to a flatter local
36"
INTRODUCTION,0.06954887218045112,"minimum. Although these issues can be partly resolved by layer-wise adaptive rate scaling (LARS,
37"
INTRODUCTION,0.07142857142857142,"You et al. [2017a]) and layer-wise adaptive large batch (LAMB, You et al. [2020]), the batch size
38"
INTRODUCTION,0.07330827067669173,"limit still exists.
39"
INTRODUCTION,0.07518796992481203,"To push the batch size limit and reduce generalization gap, we propose the element-wise adaptive
40"
INTRODUCTION,0.07706766917293233,"techniques called variance reduced gradient descent technique (VRGD) based on GSNR of parameters.
41"
INTRODUCTION,0.07894736842105263,"Our contributions are listed below:
42"
INTRODUCTION,0.08082706766917293,"• We carry out theoretical derivations of convergence rate and generalization analysis to explain
43"
INTRODUCTION,0.08270676691729323,"why VRGD can accelerate LB training and achieve dramatically smaller generalization gap.
44"
INTRODUCTION,0.08458646616541353,"• We perform comprehensive LB experiments and find that VRGD can remarkably accelerate
45"
INTRODUCTION,0.08646616541353383,"training (1.7 ∼4×), narrow the generalization gap and improve final precision than previous
46"
INTRODUCTION,0.08834586466165413,"SOTA (e.g., LAMB, LARS).
47"
INTRODUCTION,0.09022556390977443,"• VR-LAMB pushes the batch size limit of BERT pretraining up to 128k/64k without any
48"
INTRODUCTION,0.09210526315789473,"accuracy loss, while LAMB stops scaling at 64k/32k. VR-LARS improves the ImageNet
49"
INTRODUCTION,0.09398496240601503,"Top-1 accuracy to 74.82% at 96k, 0.52pp higher than LARS. The generalization gap of
50"
INTRODUCTION,0.09586466165413533,"ImageNet trained with VR-LARS is dramatically reduced by 68.3% comparing with LARS
51"
INTRODUCTION,0.09774436090225563,"at 96k. VR-SGD pushes the batch size limit of DLRM from 64k to 512k without noticeable
52"
INTRODUCTION,0.09962406015037593,"accuracy loss.
53"
RELATED WORK,0.10150375939849623,"2
Related Work
54"
LARGE BATCH TRAINING,0.10338345864661654,"2.1
Large Batch Training
55"
LARGE BATCH TRAINING,0.10526315789473684,"Several techniques are proposed to improve the optimization and generalization ability in LB training.
56"
LARGE BATCH TRAINING,0.10714285714285714,"Goyal et al. [2017] propose a linear scaling rule on learning rate (LR) to achieve the same accuracy
57"
LARGE BATCH TRAINING,0.10902255639097744,"as SB and push the batch size limit of ImageNet to 8k. EXTRAP-SGD uses the extra-gradient to
58"
LARGE BATCH TRAINING,0.11090225563909774,"stabilize the optimization trajectory and smooth training [Lin et al., 2020]. SWAP quickly trains the
59"
LARGE BATCH TRAINING,0.11278195488721804,"model with LB in the first stage and refines it by averaging the weights of multiple SB models in
60"
LARGE BATCH TRAINING,0.11466165413533834,"the second stage [Gupta et al., 2020]. Batch Augmentation replicates multiple instances with the
61"
LARGE BATCH TRAINING,0.11654135338345864,"same batch size to improve generalization [Hoffer et al., 2019]. The batch size of the experiments in
62"
LARGE BATCH TRAINING,0.11842105263157894,"EXTRAP-SGD/SWAP/Batch-Augmentation are less than 8k and are not compared in our experiments.
63"
LARGE BATCH TRAINING,0.12030075187969924,"DecentLaM removes the growing momentum-incurred bias observed in DmSGD and pushes Im-
64"
LARGE BATCH TRAINING,0.12218045112781954,"ageNet to 32k [Yuan et al., 2021]. Layer-wise LRs adjustment optimizers such as LARS [You et
65"
LARGE BATCH TRAINING,0.12406015037593984,"al., 2017a], complete layer-wise adaptive rate scaling (CLARS, Huo et al. [2021]), LAMB [You et
66"
LARGE BATCH TRAINING,0.12593984962406016,"al., 2020] successfully improve the batch size up to 64k both for ImageNet and BERT pretraining
67"
LARGE BATCH TRAINING,0.12781954887218044,"without accuracy loss. Recently, the concurrent adversarial learning (ConAdv) method pushes the
68"
LARGE BATCH TRAINING,0.12969924812030076,"batch size limit of ImageNet training up to 96k [Liu et al., 2021]. LANS replaces the layer-wise LR
69"
LARGE BATCH TRAINING,0.13157894736842105,"adjustment in LAMB with block-wise style [Zheng et al., 2020] and also pushes BERT training up to
70"
LARGE BATCH TRAINING,0.13345864661654136,"96k. Adasum adds those gradients after scaling with proper scalars and even pushes the batch size
71"
LARGE BATCH TRAINING,0.13533834586466165,"limit of BERT up to 128k/32k [Maleki et al., 2021].
72"
GRADIENT VARIANCE AND GSNR,0.13721804511278196,"2.2
Gradient Variance and GSNR
73"
GRADIENT VARIANCE AND GSNR,0.13909774436090225,"Unlike gradient mean, which is widely used in optimizers, gradient variance and its successor GSNR
74"
GRADIENT VARIANCE AND GSNR,0.14097744360902256,"are less used. But gradient variance is frequently discussed in generalization gap. Johnson and Zhang
75"
GRADIENT VARIANCE AND GSNR,0.14285714285714285,"[2013a] propose the stochastic variance reduced gradient (SVRG) with the explicit gradient variance
76"
GRADIENT VARIANCE AND GSNR,0.14473684210526316,"reduction method. Other variants of SVRG like SRVR-NPG, SVRPG and Control Variate methods
77"
GRADIENT VARIANCE AND GSNR,0.14661654135338345,"are also proposed to reduce the gradient variance during training [Liu et al., 2020b; Wang et al.,
78"
GRADIENT VARIANCE AND GSNR,0.14849624060150377,"2013; Papini et al., 2018; Miller et al., 2017]. Rainforth et al. [2018] use GSNR to analyze the
79"
GRADIENT VARIANCE AND GSNR,0.15037593984962405,"variational bounds in variational auto-encoder (VAE). McCandlish et al. [2018] use GSNR to predict
80"
GRADIENT VARIANCE AND GSNR,0.15225563909774437,"the useful upper bound of batch size. Smith et al. [2018]; Devarakonda et al. [2017] adaptively
81"
GRADIENT VARIANCE AND GSNR,0.15413533834586465,"increase the batch size during training to achieve acceleration without accuracy loss. Liu et al. [2020a]
82"
GRADIENT VARIANCE AND GSNR,0.15601503759398497,"theoretically derive a quantitative relationship between GSNR and generalization gap and prove that
83"
GRADIENT VARIANCE AND GSNR,0.15789473684210525,"larger GSNR leads to better generalization performance. Therefore, gradient variance and GSNR are
84"
GRADIENT VARIANCE AND GSNR,0.15977443609022557,"potentially useful to train deep neural networks.
85"
PRELIMINARIES,0.16165413533834586,"3
Preliminaries
86"
GSNR,0.16353383458646617,"3.1
GSNR
87"
GSNR,0.16541353383458646,"Figure 1: Schematic of VRGD’s mechanism: up-
dating parameters with larger GSNR (left panel)
and smaller GSNR (right panel)."
GSNR,0.16729323308270677,"Given a data distribution Z = X × Y, a model
88"
GSNR,0.16917293233082706,"ˆy = f(x, θ) parameterized by θ and the loss
89"
GSNR,0.17105263157894737,"function L. The parameters’ gradient w.r.t. sam-
90"
GSNR,0.17293233082706766,"ple (xi, yi) can be written as (Refer to all ""nota-
91"
GSNR,0.17481203007518797,"tions"" in the Appendix.C):
92"
GSNR,0.17669172932330826,"gi(θ) := ∂L(yi, f(xi, θ))"
GSNR,0.17857142857142858,"∂θ
(1)"
GSNR,0.18045112781954886,"Then j-th parameter’ (θj) gradient computed using (xi, yi) is gi(θj). Here we use i to index the
93"
GSNR,0.18233082706766918,"data samples and j to index the parameters of θ. We denote the sample-wise gradient mean as
94"
GSNR,0.18421052631578946,"˜g(θ) = E(x,y)∼Z(g(x, y, θ)) and variance of gi(θ) as ρ2(θ) = Var(x,y)∼Z(g(x, y, θ)). The GSNR
95"
GSNR,0.18609022556390978,"for each model parameter θj is defined as:
96"
GSNR,0.18796992481203006,r(θj) := ˜g2(θj)
GSNR,0.18984962406015038,"ρ2(θj)
(2)"
GSNR,0.19172932330827067,"Intuitively, GSNR measures the consistency of the gradient direction of each parameter across a batch
97"
GSNR,0.19360902255639098,"of data samples. The gradient space of the parameters tends to converge in the same direction when
98"
GSNR,0.19548872180451127,"the GSNR is large, but diverge if the GSNR is small (Figure.1).
99"
GSNR AND GENERALIZATION GAP,0.19736842105263158,"3.2
GSNR and Generalization Gap
100"
GSNR AND GENERALIZATION GAP,0.19924812030075187,"Consider a training set D = {(x1, y1), ..., (xn, yn)} ∼Z(n), where n samples come from Z, and
101"
GSNR AND GENERALIZATION GAP,0.20112781954887218,"a test set of dataset size (n′) from Z′(n′) denoted by D′ = {(x′
1, y′
1), ..., (x′
n′, y′
n′)} ∼Z′(n′). The
102"
GSNR AND GENERALIZATION GAP,0.20300751879699247,"empirical training and test losses can be denoted as:
103"
GSNR AND GENERALIZATION GAP,0.20488721804511278,"L[D] = 1 n n
X"
GSNR AND GENERALIZATION GAP,0.20676691729323307,"i=1
L(yi, f(xi, θ));
L[D′] = 1 n′ n′
X"
GSNR AND GENERALIZATION GAP,0.20864661654135339,"i=1
L(y′
i, f(x′
i, θ))
(3)"
GSNR AND GENERALIZATION GAP,0.21052631578947367,"respectively. Then the empirical generalization gap is given by L[D′] −L[D]. Both the training loss
104"
GSNR AND GENERALIZATION GAP,0.212406015037594,"L[D] and the test loss L[D′] would decrease after one training step and can be denoted as ∆L[D]
105"
GSNR AND GENERALIZATION GAP,0.21428571428571427,"and ∆L[D′] respectively. The ratio between the expectations of ∆L[D] and ∆L[D′] for one training
106"
GSNR AND GENERALIZATION GAP,0.2161654135338346,"step can be denoted as:
107"
GSNR AND GENERALIZATION GAP,0.21804511278195488,"R(Z, n) := ED,D′∼Zn(∆L[D′])"
GSNR AND GENERALIZATION GAP,0.2199248120300752,"ED∼Zn(∆L[D])
(4)"
GSNR AND GENERALIZATION GAP,0.22180451127819548,"Assumption 1 (Non-overfitting limit approximation of Liu et al. [2020a]). The parameters’ gradient
108"
GSNR AND GENERALIZATION GAP,0.2236842105263158,"over the training set and test set i.e., gD(θ) and gD′(θ) obey the same distribution.
109"
GSNR AND GENERALIZATION GAP,0.22556390977443608,"Based on Assumption 1 and using a small learning rate λ →0, Liu et al. [2020a] derive the
110"
GSNR AND GENERALIZATION GAP,0.2274436090225564,"relationship between the one-step generalization ratio (eq.4) and GSNR:
111"
GSNR AND GENERALIZATION GAP,0.22932330827067668,"R(Z, n) = 1 −1 n X"
GSNR AND GENERALIZATION GAP,0.231203007518797,"j
Wj
1
rj + 1"
GSNR AND GENERALIZATION GAP,0.23308270676691728,"n
, where Wj := ED∼Zn(∆Lj[D])"
GSNR AND GENERALIZATION GAP,0.2349624060150376,"ED∼Zn(∆L[D])
with
X"
GSNR AND GENERALIZATION GAP,0.23684210526315788,"j
Wj = 1
(5)"
GSNR AND GENERALIZATION GAP,0.2387218045112782,"where ∆Lj[D] is the training loss reduction caused by updating θj. A more detailed mathematical
112"
GSNR AND GENERALIZATION GAP,0.24060150375939848,"derivation can be found in Liu et al. [2020a]. This relationship (eq.5) demonstrates that GSNR
113"
GSNR AND GENERALIZATION GAP,0.2424812030075188,"(rj) plays a crucial role in determining the generalization performance of the model. Updating the
114"
GSNR AND GENERALIZATION GAP,0.24436090225563908,"model parameters with smaller GSNR leads to generalization gap growth. Also note that we have
115"
GSNR AND GENERALIZATION GAP,0.2462406015037594,"R(Z, n) →1 when n →∞, which means that training with a larger dataset helps generalization.
116"
PROPOSED ALGORITHMS,0.24812030075187969,"4
Proposed Algorithms
117"
PROPOSED ALGORITHMS,0.25,"In this section, we propose VRGD with their general updating rules (taking VR-SGD as an example
118"
PROPOSED ALGORITHMS,0.2518796992481203,"in Algorithm.1). The SGD is shown in Appendix.D for comparison.
119"
PROPOSED ALGORITHMS,0.25375939849624063,"Algorithm 1: V R −SGD
Input: require device number k ≥2
Input: B = GlobalBatchSize/k
Input: γ = 0.1"
PROPOSED ALGORITHMS,0.2556390977443609,1 while θt not converged do
PROPOSED ALGORITHMS,0.2575187969924812,for device d = 1 to k do
PROPOSED ALGORITHMS,0.2593984962406015,˜gd(θt) ←1
PROPOSED ALGORITHMS,0.26127819548872183,"B
PB
i=1 ∇θL(yi, f(xi, θt−1)) (Get gradient on each GPU/TPU)
˜g2
d(θt) ←˜gd(θt) ⊗˜gd(θt) (Element-wise multiply, so as square terms below)"
PROPOSED ALGORITHMS,0.2631578947368421,˜g(θt) ←1
PROPOSED ALGORITHMS,0.2650375939849624,"k
Pk
d=1 ˜gd(θt) (Reduce gradient over all devices)
σ2
t ←1"
PROPOSED ALGORITHMS,0.2669172932330827,"k
Pk
d=1 ˜g2
d(θt) −˜g2(θt) (Compute gradient variance)"
PROPOSED ALGORITHMS,0.26879699248120303,r(θt) ←˜g2(θt)
PROPOSED ALGORITHMS,0.2706766917293233,"σ2
t
(Compute GSNR)
for layer l = 0 to h do"
PROPOSED ALGORITHMS,0.2725563909774436,"r(θ(l)
t ) ←
r(θ(l)
t
)"
"J
PJ",0.2744360902255639,"1
J
PJ
j=1 r(θ(l)
t,j) (Normalize GSNR so that r(θ(l)
t ) = 1)"
"J
PJ",0.27631578947368424,"r(θ(l)
t ) ←"
"J
PJ",0.2781954887218045,"(
γ,
if r(θ(l)
t ) < γ
1,
if r(θ(l)
t ) > 1
(Confine the max/min ratio within 1 γ )"
"J
PJ",0.2800751879699248,θt ←θt−1 −λ · r(θt) · ˜g(θt) (Update weights)
"J
PJ",0.2819548872180451,"4.1
VR-SGD’s Updating Rules
120"
"J
PJ",0.28383458646616544,"Consider the simple updating rule for SGD as follows:
121"
"J
PJ",0.2857142857142857,"θt = θt−1 −λ · ˜g(θt)
(6)"
"J
PJ",0.287593984962406,"where λ is the learning rate. Previous section demonstrates that updating the weights with larger
122"
"J
PJ",0.2894736842105263,"GSNR confines the model’s generalization gap growth during training. Therefore, GSNR can be
123"
"J
PJ",0.29135338345864664,"used in the optimizer for better generalization. In the mathematical derivation of GSNR’s role on the
124"
"J
PJ",0.2932330827067669,"generalization gap, all sample-wise gradients for the entire dataset are used to compute the gradient
125"
"J
PJ",0.2951127819548872,"variance, which is less efficient. However, in the LB training training, where each batch is large
126"
"J
PJ",0.29699248120300753,"enough to accurately estimate the gradient variance, we replace the entire dataset with a LB and the
127"
"J
PJ",0.29887218045112784,"sample-wise with device-wise gradient computation. Gradients on each GPU/TPU device can be
128"
"J
PJ",0.3007518796992481,"synchronized using Ring-AllReduce, thus perfectly avoiding the inefficiency of gradient variance
129"
"J
PJ",0.3026315789473684,"computation. The simplified gradient variance computation is as follows:
130"
"J
PJ",0.30451127819548873,"σ2
t = 1 k k
X"
"J
PJ",0.30639097744360905,"d=1
˜g2
d(θt) −˜g2(θt)
(7)"
"J
PJ",0.3082706766917293,"where k devices are used, each of which computes 1/k part of the gradient ˜gd(θt), the same as what
131"
"J
PJ",0.3101503759398496,"data parallel does. The GSNR can then be easily calculated based on eq.2 (ρ2(θj) is replaced by σ2
j ).
132"
"J
PJ",0.31203007518796994,"The mean values of GSNR are removed at each layer before applying gradient to the parameters.
133"
"J
PJ",0.31390977443609025,"This normalization of GSNR ensures that the global averaged GSNR remains at 1.0:
134"
"J
PJ",0.3157894736842105,"r(θ(l)
t ) =
r(θ(l)
t )"
"J
PJ",0.3176691729323308,"1
J
PJ
j=1 r(θ(l)
t,j)
(8)"
"J
PJ",0.31954887218045114,"where lth layer contains J parameters. We constrain the max/min of GSNR within 1/γ so that
135"
"J
PJ",0.32142857142857145,"those neurons with very small GSNR remain active:
136"
"J
PJ",0.3233082706766917,"r(θ(l)
t ) ="
"J
PJ",0.325187969924812,"(
γ,
if r(θ(l)
t ) < γ
1,
if r(θ(l)
t ) > 1
(9)"
"J
PJ",0.32706766917293234,"where γ is a hyper-parameter used here. For simplicity, we don’t tune γ but set it to 0.1 in all of our
137"
"J
PJ",0.32894736842105265,"experiments by default. Finally, we element-wisely adapt λ according to GSNR of each parameter
138"
"J
PJ",0.3308270676691729,"and get the updating rule for VR-SGD:
139"
"J
PJ",0.33270676691729323,"θt = θt−1 −λ · r(θt) · ˜g(θt)
(10)"
"J
PJ",0.33458646616541354,"Figure.1 shows the mechanism of VRGD. As for a good estimation of gradient mean (left panel),
140"
"J
PJ",0.33646616541353386,"optimizer should be confident to move along the direction of gradient mean or even further. However,
141"
"J
PJ",0.3383458646616541,"when gradients on the devices are scatteredly distributed (right panel), updating weights with gradient
142"
"J
PJ",0.34022556390977443,"mean may bring noises and slow down convergence, which should be avoided.
143"
"J
PJ",0.34210526315789475,"Differences compared with existing LB methods:
144"
"J
PJ",0.34398496240601506,"• The linear scaling rule uses the same large LR for all parameters, which tends to diverge
145"
"J
PJ",0.3458646616541353,"when some gradients are too large; LARS/LAMB/LANS use large LRs for some layers but
146"
"J
PJ",0.34774436090225563,"layer-wisely or block-wisely limit LRs when ||θt|| is compatible with its updating quantity,
147"
"J
PJ",0.34962406015037595,"i.e., ||θt|| ∼||λ · ˜g(θt)||; VRGD that we propose here element-wisely limit the updating
148"
"J
PJ",0.35150375939849626,"quantity for those parameters without confident gradient estimation (Fig.1b, large gradient
149"
"J
PJ",0.3533834586466165,"variance or small GSNR).
150"
"J
PJ",0.35526315789473684,"• GSNR and its relationship with generalization gap is discussed in Liu et al. [2020a], but
151"
"J
PJ",0.35714285714285715,"further work to embed such GSNR into the optimizers is missing. In our work, we apply
152"
"J
PJ",0.35902255639097747,"GSNR in the SGD/LARS/LAMB and demonstrate that GSNR helps the model maintain a
153"
"J
PJ",0.3609022556390977,"small generalization gap in LB training based on the derivations of the generalization gap
154"
"J
PJ",0.36278195488721804,"and ImageNet experiments.
155"
"J
PJ",0.36466165413533835,"• VRGD does not need extra-gradient used in EXTRAP-SGD or the two-stage training like
156"
"J
PJ",0.36654135338345867,"SWAP. Sub gradients used in Batch Augmentation have different transforms each while
157"
"J
PJ",0.3684210526315789,"VRGD uses the same transforms. Adasum adaptively sums two gradients scaled by a
158"
"J
PJ",0.37030075187969924,"constant while VRGD still uses the mean gradient.
159"
"J
PJ",0.37218045112781956,"4.2
VR-Adam, VR-LAMB and other VRGD optimizers
160"
"J
PJ",0.37406015037593987,"GSNR can be easily applied on any optimizer using the general updating rules shown above. Here we
161"
"J
PJ",0.37593984962406013,"discuss those popular optimizers frequently used in the research community, e.g., SGD, Adam, LARS
162"
"J
PJ",0.37781954887218044,"and LAMB. As for VR-Adam, GSNR is calculated directly based on ˜g(θt) and then used to adapt the
163"
"J
PJ",0.37969924812030076,"gradient mean before gradients’ momentum estimation. Similar with the gradients’ momentum, we
164"
"J
PJ",0.3815789473684211,"apply the momentum mechanism on GSNR (ˆpt) for faster convergence. If we adapt the final update
165"
"J
PJ",0.38345864661654133,"term, i.e. θt ←θt−1 −λ · r(θt) · ˆmt/(√ˆvt + ε), the 1st and 2nd order momentum estimation (mt
166"
"J
PJ",0.38533834586466165,"and vt) for the next training step would be biased (meaning that the update term cannot be inferred
167"
"J
PJ",0.38721804511278196,"merely on ˆmt and ˆvt since r(θt) ̸= 1).
168"
"J
PJ",0.3890977443609023,"VR-LAMB is similar to VR-Adam, except that VR-LAMB layer-wisely adapt the LRs for stable
169"
"J
PJ",0.39097744360902253,"convergence when using very large LRs. VR-Adam and VR-LAMB are shown in Appendix.D.
170"
"J
PJ",0.39285714285714285,"VR-LARS and VR-Momentum, which are based on LARS and Momentum, are similar to VR-SGD
171"
"J
PJ",0.39473684210526316,"that it uses GSNR to adapt the gradient means before applying them to the model weights (algorithms
172"
"J
PJ",0.3966165413533835,"omitted).
173"
THEORITICAL ANALYSIS,0.39849624060150374,"5
Theoritical Analysis
174"
CONVERGENCE ANALYSIS,0.40037593984962405,"5.1
Convergence Analysis
175"
CONVERGENCE ANALYSIS,0.40225563909774437,"Assumption 2 (bounded gradient). ||∇L(θ)|| ≤G
176"
CONVERGENCE ANALYSIS,0.4041353383458647,"Assumption 3 (l-smooth). ∃l > 0 satisfies ||∇L(x) −∇L(y)|| ≤l||x −y||
177"
CONVERGENCE ANALYSIS,0.40601503759398494,"We mathematically derive the convergence rate of VR-SGD under nonconvex settings and assume
178"
CONVERGENCE ANALYSIS,0.40789473684210525,"the training process satisfies Assumption.2 and Assumption.3, which are widely used in convergence
179"
CONVERGENCE ANALYSIS,0.40977443609022557,"analysis [Shamir and Zhang, 2013; Ghadimi and Lan, 2013; Allen-Zhu and Hazan, 2016; Allen-Zhu
180"
CONVERGENCE ANALYSIS,0.4116541353383459,"et al., 2019; You et al., 2020]. Table.1 of Appendix compares the assumptions of ours and those
181"
CONVERGENCE ANALYSIS,0.41353383458646614,"popular optimizers. It shows that our assumptions are weaker than LARS/LAMB/DecentLaM and
182"
CONVERGENCE ANALYSIS,0.41541353383458646,"similar with SGD. Detailed derivations can be found in Appendix.A. Then we have Theorem.1.
183"
CONVERGENCE ANALYSIS,0.41729323308270677,"Theorem 1. Let λt =
q"
CONVERGENCE ANALYSIS,0.4191729323308271,L(θ1)−L(θ∗)
CONVERGENCE ANALYSIS,0.42105263157894735,"T ||ℓ||1
and
1
√"
CONVERGENCE ANALYSIS,0.42293233082706766,"ˆT =
q"
CONVERGENCE ANALYSIS,0.424812030075188,[(L(θ1)−L(θ∗)]||ℓ||1
CONVERGENCE ANALYSIS,0.4266917293233083,"T
, VR-SGD is bounded by:
184"
CONVERGENCE ANALYSIS,0.42857142857142855,E||∇L(θt)||2 ≤O 
CONVERGENCE ANALYSIS,0.43045112781954886,"(1 + r2
uG2 2
)
1"
CONVERGENCE ANALYSIS,0.4323308270676692,"r2
l
p ˆT ! (11)"
CONVERGENCE ANALYSIS,0.4342105263157895,"where rl and ru are the lower and upper bound of GSNR.
185"
CONVERGENCE ANALYSIS,0.43609022556390975,"Convergence rates discussion: 1) The convergence rate O(
1
√"
CONVERGENCE ANALYSIS,0.43796992481203006,"ˆT ) of VR-SGD is the same as SGD
186"
CONVERGENCE ANALYSIS,0.4398496240601504,"[Johnson and Zhang, 2013b]; 2) VR-SGD’s bound depends on the lower (rl) and upper bound (ru) of
187"
CONVERGENCE ANALYSIS,0.4417293233082707,"GSNR. Larger batch size brings smaller gradient variance (eq.43 of Appendix.B) and larger GSNR
188"
CONVERGENCE ANALYSIS,0.44360902255639095,"(both bigger rl and ru), then may result in a tighter bound with quicker convergence (verified by
189"
CONVERGENCE ANALYSIS,0.44548872180451127,"experiments shown in Figure.2).
190"
GENERALIZATION GAP,0.4473684210526316,"5.2
Generalization Gap
191"
GENERALIZATION GAP,0.4492481203007519,"This section derives the generalization gap of SGD and VR-SGD during SB and LB scenarios.
192"
GENERALIZATION GAP,0.45112781954887216,"Detailed derivations can be found in Appendix.B. Citing eq.14 of Liu et al. [2020a] below, i.e.,
193"
GENERALIZATION GAP,0.45300751879699247,"when training satisfies Assumption.1 and λ →0, after one training step the expectation of empirical
194"
GENERALIZATION GAP,0.4548872180451128,"generalization gap at tth step is:
195"
GENERALIZATION GAP,0.4567669172932331,"ED,D′∼Zn(∆tL[D] −∆tL[D′]) = λ
X"
GENERALIZATION GAP,0.45864661654135336,"j
σ2
t,j + O(λ2)
(12)"
GENERALIZATION GAP,0.4605263157894737,"where we use σ2
t,j and rt,j to denote σ2(θt,j) and r(θt,j) for simplicity. Next, we assume that the
196"
GENERALIZATION GAP,0.462406015037594,"batch size of LB is k times than that of SB. λ0 (λ) represents the learning rate of SB (LB). The
197"
GENERALIZATION GAP,0.4642857142857143,"accumulated generalization gap after training T steps for SB using SGD and T/k steps for LB can be
198"
GENERALIZATION GAP,0.46616541353383456,"derived as follows:
199"
GENERALIZATION GAP,0.4680451127819549,"E(GAPSB,SGD) ≈λ0 T
X t=1 X"
GENERALIZATION GAP,0.4699248120300752,"j
σ2
t,j;
E(GAPLB,SGD) ≈λ k T/k
X t=1 X"
GENERALIZATION GAP,0.4718045112781955,"j
σ2
t,j
(13)"
GENERALIZATION GAP,0.47368421052631576,"If we assume ""σt,j is t-independent"", eq.13 are simplified as E(GAPSB,SGD) ≈λ0T P
j σ2
j and
200"
GENERALIZATION GAP,0.4755639097744361,"E(GAPLB,SGD) ≈λT k2
P"
GENERALIZATION GAP,0.4774436090225564,"j σ2
j respectively. Taking λ = k2λ0, E(GAPLB,SGD) will have the
201"
GENERALIZATION GAP,0.4793233082706767,"same accumulated generalization gap as SB. This is known as the linear/square scaling rules. However,
202"
GENERALIZATION GAP,0.48120300751879697,"the assumption that ""σt,j is t-independent"" is unrealistic. Similarly, the accumulated generalization
203"
GENERALIZATION GAP,0.4830827067669173,"gap of VR-SGD in LB training can be written as:
204"
GENERALIZATION GAP,0.4849624060150376,"E(GAPLB,V R−SGD) ≈ T/k
X t=1 X j"
GENERALIZATION GAP,0.4868421052631579,"λrt,jσ2
t,j
k
= λ k T/k
X t=1 X"
GENERALIZATION GAP,0.48872180451127817,"j
g2
t,j
(14)"
GENERALIZATION GAP,0.4906015037593985,"The generalization gap of SGD and VR-SGD in LB training:
205"
GENERALIZATION GAP,0.4924812030075188,"When training converges (gt,j →0), we have g2
t,j < σ2
t,j because rt,j = g2
t,j/σ2
t,j →0 (ver-
206"
GENERALIZATION GAP,0.4943609022556391,"ified experimentally by Figure.4 of Liu et al. [2020a]). Therefore, we have λ"
GENERALIZATION GAP,0.49624060150375937,"k
PT/k
t=1
P"
GENERALIZATION GAP,0.4981203007518797,"j g2
t,j <
207"
GENERALIZATION GAP,0.5,"λ
k
PT/k
t=1
P"
GENERALIZATION GAP,0.5018796992481203,"j σ2
t,j, i.e., E(GAPLB,V R−SGD) < E(GAPLB,SGD). This inequality demonstrates
208"
GENERALIZATION GAP,0.5037593984962406,"that VR-SGD has a much smaller generalization gap than SGD in LB training (verified by our
209"
GENERALIZATION GAP,0.5056390977443609,"ImageNet experiments shown in Table.3 ).
210"
EXPERIMENTS,0.5075187969924813,"6
Experiments
211"
EXPERIMENTS,0.5093984962406015,"In this section, we show comprehensive experiments on commonly used LB benchmarks such as
212"
EXPERIMENTS,0.5112781954887218,"BERT Pretraining [Devlin et al., 2019], ImageNet-2012 [Russakovsky et al., 2015] and DLRM
213"
EXPERIMENTS,0.5131578947368421,"[Naumov and Mudigere, 2020]. We mainly adopt the square root rules to scale LRs. We set the
214"
EXPERIMENTS,0.5150375939849624,"hyper-parameters of VRGD as γ = 0.1 and k to the minimum GPU devices that can hold the LB
215"
EXPERIMENTS,0.5169172932330827,"without out of memory for resource efficiency (but satisfy k ≥8) in all experiments. Similar with
216"
EXPERIMENTS,0.518796992481203,"other optimizers, VRGD can generate a generally good training curve using default sets. The 1st and
217"
EXPERIMENTS,0.5206766917293233,"2nd order decay rates are set to β1 = β3 = 0.9, β2 = 0.999 by default. Experiments are performed
218"
EXPERIMENTS,0.5225563909774437,"with TensorFlow on 96 DGX-A100 nodes (768-GPUs).
219"
BERT PRETRAINING,0.5244360902255639,"6.1
BERT Pretraining
220"
BERT PRETRAINING,0.5263157894736842,"BERT pretraining is a common NLP task needs speeding up with LB training. For a fair comparison,
221"
BERT PRETRAINING,0.5281954887218046,"we use the same settings as LAMB [You et al., 2020] except optimizer and learning rate: (1) BERT
222"
BERT PRETRAINING,0.5300751879699248,"large pretrains using Wikipedia and BooksCorpus and then finetunes on SQuAD(v1.1) to evaluate its
223"
BERT PRETRAINING,0.5319548872180451,"precision with F1 score; (2) A two-phase training strategy is used. First 90% steps use a sequence
224"
BERT PRETRAINING,0.5338345864661654,"length of 128 (phase-1) and last 10% use a sequence length of 512 (phase-2). Mixed-Batch Training
225"
BERT PRETRAINING,0.5357142857142857,"is used when batch size is set to 64k/32k, 96k/32k and 128k/64k.
226"
BERT PRETRAINING,0.5375939849624061,"Table 1: Dev set F1 score of BERT pretraining and then finetuning on SQuAD(v1.1). Each
score is the median result of 3 repeated experiments. The baseline of BERT-large on SQuAD(v1.1)
is 90.395 [You et al., 2020]."
BERT PRETRAINING,0.5394736842105263,"Batch Size
16k
32k
64k/32k
64k
96k/32k
96k
128k/32k
128k/64k
Steps
31250
15625
8599
7820
6256
5214
6137
4301
LAMB∗[You et al., 2020]
91.35
91.48
90.58
-
-
-
-
-
Adam∗[Nado et al., 2021]
-
91.58
91.04
90.46
-
-
-
-
LANS∗[Zheng et al., 2020]
-
-
-
-
90.60
-
-
-
Adasum∗[Maleki et al., 2021]
-
-
-
-
-
-
90.50
-
VR-LAMB
91.42
91.58
91.49
91.30
91.23
90.70
-
90.85
(ours)
(+0.07pp)
(+0.00pp)
(+0.45pp)
(+0.84pp)
(+0.63pp)"
BERT PRETRAINING,0.5413533834586466,∗means the F1 scores are cited from their work.
BERT PRETRAINING,0.543233082706767,Using median of repeated experiments is the same as Nado et al. [2021].
BERT PRETRAINING,0.5451127819548872,"We use NVIDIA’s best practise1 to carry out VR-LAMB experiments and tune nothing of the down-
227"
BERT PRETRAINING,0.5469924812030075,"stream SQuAD(v1.1) tasks (same as LAMB). Detailed hyper-parameters are listed in Appendix.D.
228"
BERT PRETRAINING,0.5488721804511278,"Results shown in Table.1 indicate that:
229"
BERT PRETRAINING,0.5507518796992481,"• VR-LAMB outperforms LAMB (widely used in BERT LB pretraining) in all batch sizes
230"
BERT PRETRAINING,0.5526315789473685,"from 16k to 64k/32k. F1 score is improved up to 91.49 at 64k/32k, 0.91pp higher than
231"
BERT PRETRAINING,0.5545112781954887,"LAMB.
232"
BERT PRETRAINING,0.556390977443609,"• VR-LAMB also outperforms Adam (with standard bias correction and LR discontinuity
233"
BERT PRETRAINING,0.5582706766917294,"removal) and LANS by an improvement of 0.84pp at 64k and 0.63pp at 96k/32k respectively.
234"
BERT PRETRAINING,0.5601503759398496,"• VR-LAMB pushes the batch size limit up to 128k/64k using just 4301 steps and maintains a
235"
BERT PRETRAINING,0.5620300751879699,"F1 score of 90.85. Although Adasum achieves a F1 score of 90.50 at 128k/32k, but it needs
236"
BERT PRETRAINING,0.5639097744360902,"6137 steps to converge (30% extra steps than VR-LAMB). VR-LAMB achieves 50% less
237"
BERT PRETRAINING,0.5657894736842105,"steps than LAMB at 64k/32k and even 0.45pp higher of F1 score than baseline.
238"
BERT PRETRAINING,0.5676691729323309,"6.2
ImageNet with ResNet50
239"
BERT PRETRAINING,0.5695488721804511,"ImageNet training with ResNet50 v1.5 [He et al., 2016a] is a standard CV benchmark for LB training.
240"
BERT PRETRAINING,0.5714285714285714,"We use the default sets of official best practise of Google Tensorflow2 with linear LR warm-up, label
241"
BERT PRETRAINING,0.5733082706766918,"smoothing and cosine LR decay (to 0). It is the same setup as LARS [Liu et al., 2021]. We merely
242"
BERT PRETRAINING,0.575187969924812,"adjust the optimizers and learning rate for a fair comparison. We find some successful LB applications
243"
BERT PRETRAINING,0.5770676691729323,"using Momentum, LAMB and LARS, but not for Adam, AdaGrad or AdamW optimizers [Goyal et
244"
BERT PRETRAINING,0.5789473684210527,"al., 2017; You et al., 2020; Liu et al., 2021]. LARS based on Momentum is more fitful on CV tasks.
245"
BERT PRETRAINING,0.5808270676691729,"Therefore, we merely apply VR-LARS on ImageNet. Detailed hyper-parameters are listed in the
246"
BERT PRETRAINING,0.5827067669172933,"appendix.D.
247"
BERT PRETRAINING,0.5845864661654135,"Table 2: Top-1 test accuracy of ImageNet using ResNet50. Each test accuracy of VR-LARS(ours)
is averaged over 5 repeated experiments. The standard Top-1 accuracy of MLPerf-v0.5 is 74.9%."
BERT PRETRAINING,0.5864661654135338,"Batch Size
2k
4k
8k
16k
32k
64k
96k
Momentum∗[Goyal et al., 2017]
76.51%
76.44%
76.26%
-
-
-
-
DecentLaM∗[Yuan et al., 2021]
76.43%
-
76.19%
76.73%
76.22%
-
-
LAMB∗[You et al., 2020]
77.11%
76.92%
76.89%
76.66%
76.42%
-
-
LARS∗[Liu et al., 2021]
-
76.90%
76.60%
76.60%
76.60%
75.30%
74.30%
VR-LARS
77.14%
77.23%
77.36%
77.27%
76.81%
75.86%
74.82%
(ours)
(+0.03pp)
(+0.31pp)
(+0.47pp)
(+0.54pp)
(+0.21pp)
(+0.56pp)
(+0.52pp)"
BERT PRETRAINING,0.5883458646616542,∗means the results are cited from their work.
BERT PRETRAINING,0.5902255639097744,"The results shown in Table.2 indicate that:
248"
BERT PRETRAINING,0.5921052631578947,"• VR-LARS outperforms Momentum, DecentLaM, LAMB and LARS (previous SOTA) in all
249"
BERT PRETRAINING,0.5939849624060151,"batch sizes (from 0.03pp to 0.56pp). The improvements are higher for larger batch size.
250"
BERT PRETRAINING,0.5958646616541353,"1https://github.com/NVIDIA/DeepLearningExamples/tree/master
2https://github.com/tensorflow/models/tree/r1.13.0"
BERT PRETRAINING,0.5977443609022557,"• VR-LARS achieves 75.86% accuracy at 64k batch size, 0.56pp higher than LARS. When
251"
BERT PRETRAINING,0.599624060150376,"batch size reaches up to 96k, VR-LARS maintains 74.82% accuracy, close to the MLPerf-
252"
BERT PRETRAINING,0.6015037593984962,"v0.5 standard (74.9%).
253"
BERT PRETRAINING,0.6033834586466166,"Generalization Gap: Table.3 demonstrates that VR-LARS can dramatically narrow the generalization
254"
BERT PRETRAINING,0.6052631578947368,"gap in LB training. The generalization gap is only 1.46 for VR-LARS at 96k (68.3% smaller than
255"
BERT PRETRAINING,0.6071428571428571,"LARS), even smaller than ConAdv+AA (2.2; Liu et al. [2021]). Note that VR-LARS can be used
256"
BERT PRETRAINING,0.6090225563909775,"together with ConAdv+AA and other techniques for further improvement.
257"
BERT PRETRAINING,0.6109022556390977,"Table 3: Generalization Gap of large batch train-
ing on ImageNet."
BERT PRETRAINING,0.6127819548872181,"LARS∗
VR-LARS (ours)
32k
64k
96k
32k
64k
96k
Train
82.50 79.60 78.90
80.00
78.06
76.28
Accuracy"
BERT PRETRAINING,0.6146616541353384,"Test
76.60 75.30 74.30
76.81
75.86
74.82
Accuracy
Generalization 5.90
4.30
4.60
3.12
2.20
1.46
Gap
(-47.1%) (-48.8%) (-68.3%)"
BERT PRETRAINING,0.6165413533834586,"∗means the results are cited from [Liu et al., 2021]."
BERT PRETRAINING,0.618421052631579,"Similar phenomenon that train accuracy becomes smaller in VR-
LARS is also observed in ConAdv+AA [Liu et al., 2021]."
BERT PRETRAINING,0.6203007518796992,"Table 4: Test AUC of DLRM trained with SGD
and VR-SGD in 1 epoch. The reported results
are averaged over 5 repeated experiments. The
baseline AUC is 0.8014 for SGD at 32k batch
size."
BERT PRETRAINING,0.6221804511278195,"Batch Size
32k
64k
128k
256k
512k
SGD†
0.8014
0.8025
0.8021
0.7827
0.7787
VR-SGD
0.8026
0.8048
0.8042
0.8023
0.8013
(ours)
(+0.12pp) (+0.23pp) (+0.21pp) (+1.96pp) (+2.26pp)"
BERT PRETRAINING,0.6240601503759399,† means we reproduce based on NVIDIA’s best practise. 258
DLRM TRAINING,0.6259398496240601,"6.3
DLRM Training
259"
DLRM TRAINING,0.6278195488721805,"Criteo Terabyte click logs dataset (4 billion records) trained with DLRM is a standard CTR prediction
260"
DLRM TRAINING,0.6296992481203008,"benchmark newly added in MLPerf-v0.7. DLRM is used following NVIDIA’s best practise1. For a
261"
DLRM TRAINING,0.631578947368421,"fair comparison, we merely modify LRs and optimizers (hyper-parameters are listed in Appendix.D).
262"
DLRM TRAINING,0.6334586466165414,"Settings of Linear LR warm up, polynomial decay and training with 1 epoch are used by their default
263"
DLRM TRAINING,0.6353383458646616,"set up. Results in Table.4 indicates that:
264"
DLRM TRAINING,0.6372180451127819,"• VR-SGD outperforms SGD in all batch size settings. Similar with experiments shown above,
265"
DLRM TRAINING,0.6390977443609023,"the improvement of VR-SGD w.r.t SGD increases along with larger batch sizes (from 0.12pp
266"
DLRM TRAINING,0.6409774436090225,"to 2.26pp).
267"
DLRM TRAINING,0.6428571428571429,"• VR-SGD pushes the batch size limit up to 512k and maintains a high AUC of 0.8013, close
268"
DLRM TRAINING,0.6447368421052632,"to the baseline of 0.8014. Note that Google’s submission of MLPerf v0.7 merely uses a
269"
DLRM TRAINING,0.6466165413533834,"maximum batch size of 64k [Kumar et al., 2021].
270"
ABLATION STUDIES,0.6484962406015038,"7
Ablation Studies
271"
ORTHOGONAL EXPERIMENTS,0.650375939849624,"7.1
Orthogonal Experiments
272"
ORTHOGONAL EXPERIMENTS,0.6522556390977443,"Table 5: Top-1 test accuracy of CIFAR10 trained with Momen-
tum/Adam/LAMB/LARS optimizers and their corresponding
VRGD optimizers using ResNet56. Each test accuracy is
averaged over 5 repeated experiments. The reported target
accuracy for ResNet56 is 93.03% [He et al., 2016a]."
ORTHOGONAL EXPERIMENTS,0.6541353383458647,"Batch Size
256
512
1k
2k
4k
8k
Momentum†
93.68%
93.56%
93.17%
92.19%
17.40%
14.57%
VR-Momentum 93.79%
93.71%
93.50%
93.28%
92.70%
90.57%
(ours)
(+0.11pp) (+0.15pp) (+0.33pp) (+1.09pp) (+75.30pp) (+76.00pp)
Adam†
91.88%
92.24%
92.02%
91.98%
59.38%
20.74%
VR-Adam
92.46%
92.40%
92.43%
92.10%
91.74%
90.86%
(ours)
(+0.58pp) (+0.16pp) (+0.41pp) (+0.12pp) (+32.36pp) (+70.12pp)
LAMB†
92.08%
92.03%
91.90%
92.13%
58.35%
15.13%
VR-LAMB
92.29%
92.34%
92.05%
92.43%
92.04%
91.07%
(ours)
(+0.21pp) (+0.31pp) (+0.15pp) (+0.30pp) (+33.69pp) (+75.94pp)
LARS†
92.30%
92.29%
92.34%
82.39%
27.50%
12.21%
VR-LARS
92.35%
92.53%
92.44%
92.79%
92.35%
91.86%
(ours)
(+0.05pp) (+0.24pp) (+0.10pp) (+10.40pp) (+64.85pp) (+79.65pp)"
ORTHOGONAL EXPERIMENTS,0.6560150375939849,† means we reproduce based on Google TensorFlow’s best practise.
ORTHOGONAL EXPERIMENTS,0.6578947368421053,"Figure 2: Composite averaged test
accuracy or AUC curves of each op-
timizer for CIFAR10 experiments.
The abrupt surging of accuracy at
91th and 136th epoch is caused by
decaying LR with a rate of 0.1. 273"
ORTHOGONAL EXPERIMENTS,0.6597744360902256,"In this section, we demonstrate that GSNR is important in optimization and VRGD can be applicable
274"
ORTHOGONAL EXPERIMENTS,0.6616541353383458,"to most popular optimizers using CIFAR10. During CIFAR10 training with ResNet56 [He et al.,
275"
ORTHOGONAL EXPERIMENTS,0.6635338345864662,"2016a,b], we use the default sets of the official best practice for Google Tensorflow2 and mainly
276"
ORTHOGONAL EXPERIMENTS,0.6654135338345865,"add square-root LR scaling rules to perform the 216 composite experiments shown in Figure.2.
277"
ORTHOGONAL EXPERIMENTS,0.6672932330827067,"Additional linear LR warm-up, label smoothing and cosine LR decay (to 0) techniques are used
278"
ORTHOGONAL EXPERIMENTS,0.6691729323308271,"to stabilize LB training experiments shown in Table.5, the same as ImageNet training. Detailed
279"
ORTHOGONAL EXPERIMENTS,0.6710526315789473,"hyper-parameters are listed in Appendix.D. As for the test accuracy curves, Figure.2 shows the
280"
ORTHOGONAL EXPERIMENTS,0.6729323308270677,"averaged composite test accuracy curve of all 216 experiments for the LR-batch size pairs. Training
281"
ORTHOGONAL EXPERIMENTS,0.674812030075188,"with VR-Momentum/VR-Adam/VR-LAMB converge much faster (1.7 ∼4×). As for the final
282"
ORTHOGONAL EXPERIMENTS,0.6766917293233082,"precision, Table.5 demonstrate that VR-Momentum/VR-Adam/VR-LAMB/VR-LARS dramatically
283"
ORTHOGONAL EXPERIMENTS,0.6785714285714286,"outperform Momentum/Adam/LAMB/LARS when batch size is larger than 4k, which demonstrates
284"
ORTHOGONAL EXPERIMENTS,0.6804511278195489,"that VRGD is applicable to most popular optimizers in LB training. The improvements of VRGD
285"
ORTHOGONAL EXPERIMENTS,0.6823308270676691,"comparing with their base optimizers grow with the increase of batch size. VRGD optimizers remains
286"
ORTHOGONAL EXPERIMENTS,0.6842105263157895,"convergent when batch size reaches 8k.
287"
ORTHOGONAL EXPERIMENTS,0.6860902255639098,"7.2
GSNR’s Behaviour
288"
ORTHOGONAL EXPERIMENTS,0.6879699248120301,"To understand GSNR’s behaviour in VRGD optimizers, we perform the linear regression experiments.
289"
ORTHOGONAL EXPERIMENTS,0.6898496240601504,"The true weights are set to Wi = i, i ∈[1, 10] and the corresponding parameters wi are initialized to
290"
ORTHOGONAL EXPERIMENTS,0.6917293233082706,"zero. Given randomly generated inputs X, we have the true labels as Y = WX and the MSE loss as
291"
ORTHOGONAL EXPERIMENTS,0.693609022556391,"L = ||Y −wX||2. Finally, optimize w with 100 steps.
292"
ORTHOGONAL EXPERIMENTS,0.6954887218045113,"Training about 50 (half) steps, VR-SGD is able to converge to the test loss where SGD requires 100
293"
ORTHOGONAL EXPERIMENTS,0.6973684210526315,"steps (Figure.1a of Appendix.D). The weights of VR-SGD (dashed lines of Figure.1b of Appendix.D)
294"
ORTHOGONAL EXPERIMENTS,0.6992481203007519,"converge faster to their ground truth. We find that w5, w6 converge firstly, then w3, w8 and finally
295"
ORTHOGONAL EXPERIMENTS,0.7011278195488722,"w1, w10. Consistently, the GSNR of w5, w6 arise firstly (updating w5, w6 with larger LRs), then
296"
ORTHOGONAL EXPERIMENTS,0.7030075187969925,"w3, w8 while the GSNR of w5, w6 decrease slowly (no need to update the converged weights using
297"
ORTHOGONAL EXPERIMENTS,0.7048872180451128,"large LRs). Finally after step 60, the GSNR of w1, w10 begin to arise. Intuitively, GSNR helps
298"
ORTHOGONAL EXPERIMENTS,0.706766917293233,"element-wisely fine-tune the LRs for different weights.
299"
HYPER-PARAMETERS SENSITIVITY,0.7086466165413534,"7.3
Hyper-parameters Sensitivity
300"
HYPER-PARAMETERS SENSITIVITY,0.7105263157894737,"Figure 3: Hyper parameter sen-
sitivity experiments: test loss of
various γ (Upper panel) and k
(Bottom panel)."
HYPER-PARAMETERS SENSITIVITY,0.7124060150375939,"There are two main hyper-parameters in VRGD, i.e., normal-
301"
HYPER-PARAMETERS SENSITIVITY,0.7142857142857143,"ization strength factor γ and the equivalent GPU device number
302"
HYPER-PARAMETERS SENSITIVITY,0.7161654135338346,"k. We take use of linear regression trained with VR-SGD
303"
HYPER-PARAMETERS SENSITIVITY,0.7180451127819549,"using batchsize = 2048 shown above to examine the hyper-
304"
HYPER-PARAMETERS SENSITIVITY,0.7199248120300752,"parameter sensitivity.
305"
HYPER-PARAMETERS SENSITIVITY,0.7218045112781954,"Figure.3 shows that the optimal γ is around (0.04, 0.2) for lin-
306"
HYPER-PARAMETERS SENSITIVITY,0.7236842105263158,"ear regression. Test loss would be larger if γ →1, which means
307"
HYPER-PARAMETERS SENSITIVITY,0.7255639097744361,"VR-SGD is reduced to SGD. It again demonstrates that GSNR
308"
HYPER-PARAMETERS SENSITIVITY,0.7274436090225563,"is valuable to improve final precision. On the other hand, the
309"
HYPER-PARAMETERS SENSITIVITY,0.7293233082706767,"optimal k is around [32, 256]. This means that each gradient
310"
HYPER-PARAMETERS SENSITIVITY,0.731203007518797,"mean calculated using [8, 64] samples on each GPU/TPU de-
311"
HYPER-PARAMETERS SENSITIVITY,0.7330827067669173,"vice, and gradient variance calculated using [32, 256] values
312"
HYPER-PARAMETERS SENSITIVITY,0.7349624060150376,"of the gradient mean will return a good evaluation of GSNR.
313"
HYPER-PARAMETERS SENSITIVITY,0.7368421052631579,"In fact, we do not use the optimal hyper-parameters. Instead,
314"
HYPER-PARAMETERS SENSITIVITY,0.7387218045112782,"above experiments use γ = 0.1 and set k to the minimum GPU
315"
HYPER-PARAMETERS SENSITIVITY,0.7406015037593985,"devices that can hold the LB without out of memory (but sat-
316"
HYPER-PARAMETERS SENSITIVITY,0.7424812030075187,"isfy k ≥8, refer all of the hyper-parameters in Appendix.D).
317"
HYPER-PARAMETERS SENSITIVITY,0.7443609022556391,"Fine-tuning γ and k may further improve the results.
318"
SUMMARY,0.7462406015037594,"8
Summary
319"
SUMMARY,0.7481203007518797,"In this paper, we propose the VRGD for large batch training using GSNR. We carry out theoretical
320"
SUMMARY,0.75,"derivations of convergence rate and generalization analysis to explain why VRGD can accelerate
321"
SUMMARY,0.7518796992481203,"large batch training and reduce generalization gap. Comprehensive experiments on BERT-pretraining,
322"
SUMMARY,0.7537593984962406,"ImageNet and DLRM verify that VRGD can push the batch size limit than previous SOTA optimizers
323"
SUMMARY,0.7556390977443609,"in LB training and perform better. Codes will be released when published.
324"
REFERENCES,0.7575187969924813,"References
325"
REFERENCES,0.7593984962406015,"Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
326"
REFERENCES,0.7612781954887218,"International conference on machine learning, pages 699–707. PMLR, 2016.
327"
REFERENCES,0.7631578947368421,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
328"
REFERENCES,0.7650375939849624,"parameterization. In International Conference on Machine Learning, pages 242–252. PMLR,
329"
REFERENCES,0.7669172932330827,"2019.
330"
REFERENCES,0.768796992481203,"Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
331"
REFERENCES,0.7706766917293233,"Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-
332"
REFERENCES,0.7725563909774437,"ties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
333"
REFERENCES,0.7744360902255639,"Xiaowu Dai and Yuhua Zhu. Towards theoretical understanding of large batch training in stochastic
334"
REFERENCES,0.7763157894736842,"gradient descent. CoRR, abs/1812.00542, 2018.
335"
REFERENCES,0.7781954887218046,"Aditya Devarakonda, Maxim Naumov, and Michael Garland. Adabatch: Adaptive batch sizes for
336"
REFERENCES,0.7800751879699248,"training deep neural networks. arXiv preprint arXiv:1712.02029, 2017.
337"
REFERENCES,0.7819548872180451,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
338"
REFERENCES,0.7838345864661654,"bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar
339"
REFERENCES,0.7857142857142857,"Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the
340"
REFERENCES,0.7875939849624061,"Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, pages
341"
REFERENCES,0.7894736842105263,"4171–4186. Association for Computational Linguistics, 2019.
342"
REFERENCES,0.7913533834586466,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
343"
REFERENCES,0.793233082706767,"models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.
344"
REFERENCES,0.7951127819548872,"Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds
345"
REFERENCES,0.7969924812030075,"and Machines, 30(4):681–694, 2020.
346"
REFERENCES,0.7988721804511278,"Denis Foley and John Danskin. Ultra-performance pascal gpu and nvlink interconnect. IEEE Micro,
347"
REFERENCES,0.8007518796992481,"37(2):7–17, 2017.
348"
REFERENCES,0.8026315789473685,"Saeed Ghadimi and Guanghui Lan.
Stochastic first- and zeroth-order methods for nonconvex
349"
REFERENCES,0.8045112781954887,"stochastic programming. SIAM J. Optim., 23(4):2341–2368, 2013.
350"
REFERENCES,0.806390977443609,"Andrew Gibiansky. Bringing hpc techniques to deep learning. Baidu Research, Tech. Rep., 2017.
351"
REFERENCES,0.8082706766917294,"Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
352"
REFERENCES,0.8101503759398496,"Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
353"
REFERENCES,0.8120300751879699,"in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
354"
REFERENCES,0.8139097744360902,"Vipul Gupta, Santiago Akle Serrano, and Dennis DeCoste. Stochastic weight averaging in par-
355"
REFERENCES,0.8157894736842105,"allel: Large-batch training that generalizes well. In 8th International Conference on Learning
356"
REFERENCES,0.8176691729323309,"Representations, ICLR. OpenReview.net, 2020.
357"
REFERENCES,0.8195488721804511,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
358"
REFERENCES,0.8214285714285714,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
359"
REFERENCES,0.8233082706766918,"pages 770–778, 2016.
360"
REFERENCES,0.825187969924812,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
361"
REFERENCES,0.8270676691729323,"networks. In European conference on computer vision, pages 630–645. Springer, 2016.
362"
REFERENCES,0.8289473684210527,"Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
363"
REFERENCES,0.8308270676691729,"ization gap in large batch training of neural networks. In Isabelle Guyon, Ulrike von Luxburg,
364"
REFERENCES,0.8327067669172933,"Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors,
365"
REFERENCES,0.8345864661654135,"Advances in Neural Information Processing Systems, pages 1731–1741, 2017.
366"
REFERENCES,0.8364661654135338,"Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment
367"
REFERENCES,0.8383458646616542,"your batch: better training with larger batches. arXiv preprint arXiv:1901.09335, 2019.
368"
REFERENCES,0.8402255639097744,"Zhouyuan Huo, Bin Gu, and Heng Huang. Large batch optimization for deep learning using new
369"
REFERENCES,0.8421052631578947,"complete layer-wise adaptive rate scaling. In Thirty-Fifth AAAI Conference on Artificial Intelligence,
370"
REFERENCES,0.8439849624060151,"AAAI, pages 7883–7890. AAAI Press, 2021.
371"
REFERENCES,0.8458646616541353,"Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
372"
REFERENCES,0.8477443609022557,"reduction. Advances in neural information processing systems, 26, 2013.
373"
REFERENCES,0.849624060150376,"Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
374"
REFERENCES,0.8515037593984962,"reduction. In Christopher J. C. Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger,
375"
REFERENCES,0.8533834586466166,"editors, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural
376"
REFERENCES,0.8552631578947368,"Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake
377"
REFERENCES,0.8571428571428571,"Tahoe, Nevada, United States, pages 315–323, 2013.
378"
REFERENCES,0.8590225563909775,"Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
379"
REFERENCES,0.8609022556390977,"Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In 5th
380"
REFERENCES,0.8627819548872181,"International Conference on Learning Representations, ICLR. OpenReview.net, 2017.
381"
REFERENCES,0.8646616541353384,"Sameer Kumar, Yu Emma Wang, Cliff Young, James Bradbury, Naveen Kumar, Dehao Chen, and
382"
REFERENCES,0.8665413533834586,"Andy Swing. Exploring the limits of concurrency in ML training on google TPUS. In Alex Smola,
383"
REFERENCES,0.868421052631579,"Alex Dimakis, and Ion Stoica, editors, Proceedings of Machine Learning and Systems 2021, MLSys
384"
REFERENCES,0.8703007518796992,"2021, virtual, April 5-9, 2021. mlsys.org, 2021.
385"
REFERENCES,0.8721804511278195,"Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James
386"
REFERENCES,0.8740601503759399,"Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter
387"
REFERENCES,0.8759398496240601,"server. In 11th USENIX Symposium on Operating Systems Design and Implementation, pages
388"
REFERENCES,0.8778195488721805,"583–598, 2014.
389"
REFERENCES,0.8796992481203008,"Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. Communication efficient distributed
390"
REFERENCES,0.881578947368421,"machine learning with the parameter server. Advances in Neural Information Processing Systems,
391"
REFERENCES,0.8834586466165414,"27, 2014.
392"
REFERENCES,0.8853383458646616,"Tao Lin, Lingjing Kong, Sebastian Stich, and Martin Jaggi. Extrapolation for large-batch training in
393"
REFERENCES,0.8872180451127819,"deep learning. In International Conference on Machine Learning, pages 6094–6104. PMLR, 2020.
394"
REFERENCES,0.8890977443609023,"Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang
395"
REFERENCES,0.8909774436090225,"Wang, Le Jiang, Xianyan Jia, et al. M6: A chinese multimodal pretrainer. arXiv preprint
396"
REFERENCES,0.8928571428571429,"arXiv:2103.00823, 2021.
397"
REFERENCES,0.8947368421052632,"Jinlong Liu, Guoqing Jiang, Yunzhi Bai, Ting Chen, and Huayan Wang. Understanding why neural
398"
REFERENCES,0.8966165413533834,"networks generalize well through GSNR of parameters. In 8th International Conference on
399"
REFERENCES,0.8984962406015038,"Learning Representations, ICLR. OpenReview.net, 2020.
400"
REFERENCES,0.900375939849624,"Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced)
401"
REFERENCES,0.9022556390977443,"policy gradient and natural policy gradient methods. Advances in Neural Information Processing
402"
REFERENCES,0.9041353383458647,"Systems, 33:7624–7636, 2020.
403"
REFERENCES,0.9060150375939849,"Yong Liu, Xiangning Chen, Minhao Cheng, Cho-Jui Hsieh, and Yang You. Concurrent adversarial
404"
REFERENCES,0.9078947368421053,"learning for large-batch training. arXiv preprint arXiv:2106.00221, 2021.
405"
REFERENCES,0.9097744360902256,"Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Olli Saarikivi, Tianju Xu, Vadim Eksarevskiy,
406"
REFERENCES,0.9116541353383458,"Jaliya Ekanayake, and Emad Barsoum. Scaling distributed training with adaptive summation. In
407"
REFERENCES,0.9135338345864662,"Alex Smola, Alex Dimakis, and Ion Stoica, editors, Proceedings of Machine Learning and Systems
408"
REFERENCES,0.9154135338345865,"2021, MLSys 2021, virtual, April 5-9, 2021. mlsys.org, 2021.
409"
REFERENCES,0.9172932330827067,"Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of
410"
REFERENCES,0.9191729323308271,"large-batch training. CoRR, abs/1812.06162, 2018.
411"
REFERENCES,0.9210526315789473,"Andrew Miller, Nick Foti, Alexander D’Amour, and Ryan P Adams. Reducing reparameterization
412"
REFERENCES,0.9229323308270677,"gradient variance. Advances in Neural Information Processing Systems, 30, 2017.
413"
REFERENCES,0.924812030075188,"Zachary Nado, Justin M Gilmer, Christopher J Shallue, Rohan Anil, and George E Dahl. A large
414"
REFERENCES,0.9266917293233082,"batch optimizer reality check: Traditional, generic optimizers suffice across batch sizes. arXiv
415"
REFERENCES,0.9285714285714286,"preprint arXiv:2102.06356, 2021.
416"
REFERENCES,0.9304511278195489,"Maxim Naumov and Dheevatsa Mudigere. Dlrm: An advanced, open source deep learning recom-
417"
REFERENCES,0.9323308270676691,"mendation model, 2020.
418"
REFERENCES,0.9342105263157895,"Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochas-
419"
REFERENCES,0.9360902255639098,"tic variance-reduced policy gradient. In International conference on machine learning, pages
420"
REFERENCES,0.9379699248120301,"4026–4035. PMLR, 2018.
421"
REFERENCES,0.9398496240601504,"Tom Rainforth, Adam R. Kosiorek, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood,
422"
REFERENCES,0.9417293233082706,"and Yee Whye Teh. Tighter variational bounds are not necessarily better. In Jennifer G. Dy and
423"
REFERENCES,0.943609022556391,"Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning,
424"
REFERENCES,0.9454887218045113,"ICML, volume 80 of Proceedings of Machine Learning Research, pages 4274–4282. PMLR, 2018.
425"
REFERENCES,0.9473684210526315,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
426"
REFERENCES,0.9492481203007519,"Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
427"
REFERENCES,0.9511278195488722,"Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV),
428"
REFERENCES,0.9530075187969925,"115(3):211–252, 2015.
429"
REFERENCES,0.9548872180451128,"Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in
430"
REFERENCES,0.956766917293233,"tensorflow. arXiv preprint arXiv:1802.05799, 2018.
431"
REFERENCES,0.9586466165413534,"Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence
432"
REFERENCES,0.9605263157894737,"results and optimal averaging schemes. In International conference on machine learning, pages
433"
REFERENCES,0.9624060150375939,"71–79. PMLR, 2013.
434"
REFERENCES,0.9642857142857143,"Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don’t decay the learning
435"
REFERENCES,0.9661654135338346,"rate, increase the batch size. In 6th International Conference on Learning Representations, ICLR.
436"
REFERENCES,0.9680451127819549,"OpenReview.net, 2018.
437"
REFERENCES,0.9699248120300752,"Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
438"
REFERENCES,0.9718045112781954,"gradient optimization. Advances in Neural Information Processing Systems, 26, 2013.
439"
REFERENCES,0.9736842105263158,"Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv
440"
REFERENCES,0.9755639097744361,"preprint arXiv:1708.03888, 2017.
441"
REFERENCES,0.9774436090225563,"Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training.
442"
REFERENCES,0.9793233082706767,"arXiv preprint arXiv:1708.03888, 6(12):6, 2017.
443"
REFERENCES,0.981203007518797,"Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
444"
REFERENCES,0.9830827067669173,"Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning:
445"
REFERENCES,0.9849624060150376,"Training BERT in 76 minutes. In 8th International Conference on Learning Representations, ICLR.
446"
REFERENCES,0.9868421052631579,"OpenReview.net, 2020.
447"
REFERENCES,0.9887218045112782,"Kun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin.
448"
REFERENCES,0.9906015037593985,"Decentlam: Decentralized momentum SGD for large-batch deep training. In 2021 IEEE/CVF
449"
REFERENCES,0.9924812030075187,"International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17,
450"
REFERENCES,0.9943609022556391,"2021, pages 3009–3019. IEEE, 2021.
451"
REFERENCES,0.9962406015037594,"Shuai Zheng, Haibin Lin, Sheng Zha, and Mu Li. Accelerated large batch optimization of BERT
452"
REFERENCES,0.9981203007518797,"pretraining in 54 minutes. CoRR, abs/2006.13484, 2020.
453"
