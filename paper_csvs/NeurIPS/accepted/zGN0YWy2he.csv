Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002583979328165375,"There has been exciting progress in generating images from natural language or lay-
out conditions. However, these methods struggle to faithfully reproduce complex
scenes due to the insufficient modeling of multiple objects and their relationships.
To address this issue, we leverage the scene graph, a powerful structured representa-
tion, for complex image generation. Different from the previous works that directly
use scene graphs for generation, we employ the generative capabilities of varia-
tional autoencoders and diffusion models in a generalizable manner, compositing
diverse disentangled visual clues from scene graphs. Specifically, we first propose
a Semantics-Layout Variational AutoEncoder (SL-VAE) to jointly derive (layouts,
semantics) from the input scene graph, which allows a more diverse and reasonable
generation in a one-to-many mapping. We then develop a Compositional Masked
Attention (CMA) integrated with a diffusion model, incorporating (layouts, se-
mantics) with fine-grained attributes as generation guidance. To further achieve
graph manipulation while keeping the visual content consistent, we introduce a
Multi-Layered Sampler (MLS) for an “isolated” image editing effect. Extensive
experiments demonstrate that our method outperforms recent competitors based on
text, layout, or scene graph, in terms of generation rationality and controllability.
Code is available at https://github.com/wangyunnan/DisCo."
INTRODUCTION,0.00516795865633075,"1
Introduction"
INTRODUCTION,0.007751937984496124,"Text-to-image (T2I) generation with diffusion models (DMs) [1, 2] has yielded remarkable advance-
ments [3, 4, 5] in recent years, benefiting from the developments of vision-language foundation
models [6, 7, 8, 9]. However, textual conditions with linear structure struggle to delineate the intri-
cacies of complex scenes precisely. For example, as shown in the failure cases of DALL·E 3 [3] in
Figure 1 (a), given the intricate text prompt “A sheep by another sheep ... a boat on the grass.”, the
T2I model may have difficulty accurately generating object relationships or quantities. Consequently,
some studies [10, 11, 12, 13] strive to improve spatial relationship (e.g., “by” and “on”) control
by incorporating additional layout conditions. Nevertheless, as illustrated in the failure cases of
LayoutDiffusion [10] in Figure 1 (b), layout-to-image (L2I) methods inevitably encounter challenges
in representing certain non-spatial interactions, such as depicting “playing” within spatial topology."
INTRODUCTION,0.0103359173126615,"To efficiently depict complex scenes for guiding generative models, recent methods [14, 15, 16]
utilize structured scene graphs as conditions instead of text or layout prompts. Scene graphs [17]"
INTRODUCTION,0.012919896640826873,*Xin Jin is the corresponding author.
INTRODUCTION,0.015503875968992248,"A sheep by 
another sheep 
on the grass with"
INTRODUCTION,0.01808785529715762,the ocean under
INTRODUCTION,0.020671834625323,"the sky; 
the ocean by a tree;"
INTRODUCTION,0.023255813953488372,a boat on the grass
INTRODUCTION,0.025839793281653745,Relationship and Quantity Confusion of T2I
INTRODUCTION,0.028423772609819122,(a) DALL·E 3 (middle) and Ours (right).
INTRODUCTION,0.031007751937984496,Non-Spatial Interaction Dilemma of L2I beach man man kite beach man man kite
INTRODUCTION,0.03359173126614987,(b) LayoutDiffusion (middle) and Ours (right).
INTRODUCTION,0.03617571059431524,Independent Nodes Missing of Semantics-based SG2I
INTRODUCTION,0.03875968992248062,"Missing 
lamp and stone"
INTRODUCTION,0.041343669250646,(c) R3CD (middle) and Ours (right).
INTRODUCTION,0.04392764857881137,Attribute Control of our SG2I method
INTRODUCTION,0.046511627906976744,(d) Results w/o (middle) and w/ (right) AC.
INTRODUCTION,0.04909560723514212,"Figure 1: Failure cases generated by (a) text-to-image (T2I) (DALL·E 3 [3]), (b) layout-to-image
(L2I) (LayoutDiffusion [10]), and (c) semantics-based scene-graph-to-image (SG2I) (R3CD [14])
methods . (d) Generalizable object Attribute Control (AC) under consistency achieved by our DisCo."
INTRODUCTION,0.05167958656330749,"represent scenes with a structured graph format, where objects within the scene are denoted as
nodes and the relationships between objects are represented as edges. Scene-Graph-to-Image (SG2I)
generation is a challenging task due to the frequent ambiguous alignment between graph edges and
relationships/interactions among visual objects. To address this issue, layout-based SG2I methods
[15, 17, 18, 19] explicitly predict the spatial arrangements of objects in scenes by additional layout
predictors, followed by L2I synthesis according to the layout (as demonstrated in Figure 2 (a)). These
methods commonly employ one-to-one mapping, i.e., a single scene graph only corresponds to one
layout, which severely limits the generation diversity. Besides, they also inherit the limitation of
the L2I approach in modeling non-spatial interactions, whereby each object is typically generated
independently. In contrast, as shown in Figure 2 (b), semantic-based SG2I methods implicitly encode
graph edges into node embeddings by graph convolutional networks (GCNs), which effectively aligns
object semantics with non-spatial interactions. Nonetheless, these methods are weak in logically
determining the spatial positions of independent nodes, which might cause the absence of independent
nodes (e.g., the “lamp” and “stone” shown in Figure 1 (c)) in the generated image."
INTRODUCTION,0.05426356589147287,"In this paper, we propose DisCo, a Compositional image generation framework that integrates
the Disentangled layout and semantics derived from scene graph representations (as depicted in
Figure 2 (c)). To boost the representational capacity of scene graphs for complex scenes, we augment
the node and edge representations with CLIP [6] text embeddings, and incorporate extra spatial
information (i.e., bounding box embeddings) for nodes during training. Once the textual scene graph
is constructed, we propose a Semantics-Layout Variational AutoEncoder (SL-VAE) based on triplet-
GCN [18] to jointly model the spatial relationships and non-spatial interactions in the scene. SL-VAE
allows the one-to-many disentanglement for spatial layout and interactive semantics that match the"
INTRODUCTION,0.056847545219638244,"(b) Semantic-based SG2I O1
O2 O3 O4
O5 O1
O2 O3 O4
O5"
INTRODUCTION,0.059431524547803614,"Layout
Predictor O1
O2 O3 O4
O5 O1
O2 O3 O4
O5"
INTRODUCTION,0.06201550387596899,Semantic
INTRODUCTION,0.06459948320413436,"Encoder
Diffusion
Diffusion"
INTRODUCTION,0.06718346253229975,(a) Layout-based SG2I
INTRODUCTION,0.06976744186046512,Diffusion
INTRODUCTION,0.07235142118863049,Layout
INTRODUCTION,0.07493540051679587,"Semantic 
Embeddings"
INTRODUCTION,0.07751937984496124,Object-level fusion
INTRODUCTION,0.08010335917312661,"Latent 
Space O1
O2 O3 O4
O5 O1
O2 O3 O4
O5"
INTRODUCTION,0.082687338501292,"Union
Encoder
Diffusion
Diffusion"
INTRODUCTION,0.08527131782945736,"Layout 
Decoder"
INTRODUCTION,0.08785529715762273,Semantic
INTRODUCTION,0.09043927648578812,"Decoder
Semantic"
INTRODUCTION,0.09302325581395349,Decoder
INTRODUCTION,0.09560723514211886,GCN-based VAE ... ...
INTRODUCTION,0.09819121447028424,(c) Our method
INTRODUCTION,0.10077519379844961,Semantic Embeddings
INTRODUCTION,0.10335917312661498,"Layouts
paired
paired"
INTRODUCTION,0.10594315245478036,"Figure 2: Comparison between the previous SG2I architectures and ours. (a) Layout-based SG2I
model [15] generate a spatial arrangement with an object layout; (b) Semantic-based SG2I models
[14, 16] build interactive semantic embedding between objects; (c) Our method leverages scene graph
representation by jointly deriving the disentangled layout and semantics with the proposed SL-VAE."
INTRODUCTION,0.10852713178294573,"input scene graph. This is achieved by sampling from a Gaussian distribution, offering object-level
(layouts, semantics) conditions for the diffusion process [20]. Given that the layout and semantics
encapsulate global and local relational information, we further introduce the Compositional Masked
Attention (CMA) mechanism to inject object-level graph information with fine-grained attributes
into the diffusion model, thereby preventing relational confusion and attribute leakage.Finally, we
present a Multi-Layered Sampler (MLS) technique that leverages the diverse conditions generated by
SL-VAE, achieving generalizable generation for object-level graph manipulation (i.e., node addition
and attribute control) in the SG2I task, as depicted by the color change of two “sheep” in Figure 1 (d)."
INTRODUCTION,0.1111111111111111,"In summary, our key contributions are as follows: (i) We apply the textual scene graph as a structured
scene representation and introduce the Semantics-Layout Variational AutoEncoder (SL-VAE) to
disentangle diverse spatial layouts and interactive semantics from the scene graph; (ii) We present the
Compositional Masked Attention (CMA) to inject extracted object-level graph information with fine-
grained attributes into the diffusion model, which avoids relational confusion and attribute leakage;
(iii) We introduce the Multi-Layered Sampler (MLS), a technique that leverages the diverse conditions
produced by SL-VAE to implement object-level graph manipulation while keeping the visual content
consistent; (iv) Our method outperforms current text/layout-based methods in relationship generation
and achieves significantly superior generation performance compared to state-of-the-art SG2I models,
thus showcasing the generalization of textual scene graphs in depicting complex scenes."
PRELIMINARY,0.11369509043927649,"2
Preliminary"
TEXT-TO-IMAGE DIFFUSION MODELS,0.11627906976744186,"2.1
Text-to-Image Diffusion Models"
TEXT-TO-IMAGE DIFFUSION MODELS,0.11886304909560723,"Diffusion models (DMs) [1] are generative models that learn the data distribution p(x) by gradually
performing T-step noise reduction from the variables xT sampled from the Gaussian distribution
N(0, 1). Thus the training process of DMs can be regarded as the reverse process of a Markov
chain with a fixed length T. To generate high-resolution images with less computational resources,
Latent Diffusion Models (LDMs) [20] encode the image x into the latent space z with the pre-trained
Vector Quantized Variational AutoEncoder (VQ-VAE). Subsequently, the LDMs aim to predict the
distribution p(z) rather than p(x). For the text-to-image LDMs, the text is encoded with the CLIP [6]
text encoder ECLIP. Then the objective function of a text-guided LDM can be formulated as follows:"
TEXT-TO-IMAGE DIFFUSION MODELS,0.12144702842377261,"LLDM = Ez,ϵ∼N(0,1),t[∥ϵ −ϵθ(zt, ECLIP(c), t)∥2
2],
(1)"
TEXT-TO-IMAGE DIFFUSION MODELS,0.12403100775193798,"where ECLIP(c) is the text embedding of the text condition c, t is the diffusion step, ϵθ is a model for
estimating the noise ϵ, and zt = αtzt−1 + σtϵ is the t-step noised latent code from the ground-truth
z0. During inference, the model ϵθ with various samplers [1, 21] gradually denoises the initial noise
zT ∼N(0, 1). Finally, the predicted latent code is decoded into the image space."
TEXT-TO-IMAGE DIFFUSION MODELS,0.12661498708010335,"Node i: 
Node i:"
TEXT-TO-IMAGE DIFFUSION MODELS,0.12919896640826872,"Self-Attention
MLP"
TEXT-TO-IMAGE DIFFUSION MODELS,0.13178294573643412,"Mask-Attention
Fusion Feature
Fusion Feature
Fusion Feature"
TEXT-TO-IMAGE DIFFUSION MODELS,0.1343669250645995,Attention Layer
TEXT-TO-IMAGE DIFFUSION MODELS,0.13695090439276486,Subsequent Layer
TEXT-TO-IMAGE DIFFUSION MODELS,0.13953488372093023,Node i:
TEXT-TO-IMAGE DIFFUSION MODELS,0.1421188630490956,"Self-Attention
MLP"
TEXT-TO-IMAGE DIFFUSION MODELS,0.14470284237726097,"Mask-Attention
Fusion Feature"
TEXT-TO-IMAGE DIFFUSION MODELS,0.14728682170542637,Attention Layer
TEXT-TO-IMAGE DIFFUSION MODELS,0.14987080103359174,Subsequent Layer
TEXT-TO-IMAGE DIFFUSION MODELS,0.1524547803617571,IV. Details of CMA Mechanism
TEXT-TO-IMAGE DIFFUSION MODELS,0.15503875968992248,"Inference 
Starting Point"
TEXT-TO-IMAGE DIFFUSION MODELS,0.15762273901808785,Graph Union
TEXT-TO-IMAGE DIFFUSION MODELS,0.16020671834625322,Encoder
TEXT-TO-IMAGE DIFFUSION MODELS,0.16279069767441862,Semantic
TEXT-TO-IMAGE DIFFUSION MODELS,0.165374677002584,Decoder
TEXT-TO-IMAGE DIFFUSION MODELS,0.16795865633074936,"Layout
Decoder o3 o1
o2 o4
o5 o3 o1
o2 o4
o5"
TEXT-TO-IMAGE DIFFUSION MODELS,0.17054263565891473,Object-level
TEXT-TO-IMAGE DIFFUSION MODELS,0.1731266149870801,Fusion
TEXT-TO-IMAGE DIFFUSION MODELS,0.17571059431524547,"I. Graph Encoding for Semantics-Layout VAE
II. Graph Decoding for Semantics-Layout VAE"
TEXT-TO-IMAGE DIFFUSION MODELS,0.17829457364341086,III. Visual Diffusion Process with Semantic and Layout Condition
TEXT-TO-IMAGE DIFFUSION MODELS,0.18087855297157623,"Image
Node i:"
TEXT-TO-IMAGE DIFFUSION MODELS,0.1834625322997416,Diffusion Process
TEXT-TO-IMAGE DIFFUSION MODELS,0.18604651162790697,CMA Mechanism
TEXT-TO-IMAGE DIFFUSION MODELS,0.18863049095607234,Visual VAE
TEXT-TO-IMAGE DIFFUSION MODELS,0.19121447028423771,Encoder
TEXT-TO-IMAGE DIFFUSION MODELS,0.1937984496124031,Visual VAE
TEXT-TO-IMAGE DIFFUSION MODELS,0.19638242894056848,Decoder
TEXT-TO-IMAGE DIFFUSION MODELS,0.19896640826873385,Visual VAE
TEXT-TO-IMAGE DIFFUSION MODELS,0.20155038759689922,Encoder
TEXT-TO-IMAGE DIFFUSION MODELS,0.2041343669250646,Visual VAE
TEXT-TO-IMAGE DIFFUSION MODELS,0.20671834625322996,"Decoder o1
o2 o3 o4
o5 o1
o2 o3 o4
o5"
TEXT-TO-IMAGE DIFFUSION MODELS,0.20930232558139536,"Box embedding
Box embedding
CLIP embedding
Learnable embedding"
TEXT-TO-IMAGE DIFFUSION MODELS,0.21188630490956073,"Latent Space
Layout
Semantcs"
TEXT-TO-IMAGE DIFFUSION MODELS,0.2144702842377261,"Figure 3: Framework overview. (I) We parameterize the node embeddings into the Gaussian
distribution with the Graph Union Encoder, which jointly models the spatial relationships and non-
spatial interactions in scene graphs; (II) The Semantic and Layout Decoders generate spatial layouts
and interactive semantics sampled from Gaussian distribution, respectively; (III) A diffusion model
with the proposed Compositional Masked Attention (CMA) incorporates object-level conditions to
generate visual images following the scene graph description; (IV) Detailed structure of CMA Layer."
SCENE GRAPH REPRESENTATION,0.21705426356589147,"2.2
Scene Graph Representation"
SCENE GRAPH REPRESENTATION,0.21963824289405684,"The scene graph G = (O, E) [17] presents a structured scene representation. Nodes O = {oi}No
i=1
denotes No objects within the scene, while edges E = {eij}1≤i,j≤No,i̸=j denotes relationships
between objects. All nodes and edges come with a semantic label, denoted as co
i ∈Co and ce
ij ∈Ce,
where Co and Ce are category vocabularies of nodes and edges, respectively. In practice, the nodes
O = {oi}No
i=1 and the triples T = {tij = (oi, eij, oj)}1≤i,j≤No,i̸=j representing connections from
oi to oj serve as inputs for graph convolutional networks (GCNs). Moreover, nodes and edges are
typically converted into learnable embeddings using embedding layers denoted as Eo
emb and Ee
emb."
METHODOLOGY,0.2222222222222222,"3
Methodology"
METHODOLOGY,0.2248062015503876,"As illustrated in Figure 3, we present a novel SG2I synthesis framework known as DisCo. The DisCo
comprises three primary components: (1) Semantics-Layout Variational AutoEncoder (SL-VAE)
that disentangle diverse spatial layouts and interactive semantics from the scene graph (Section
3.1); (2) Compositional Masked Attention (CMA) that injects object-level (layouts, semantics) with
fine-grained attributes into the diffusion model (Section 3.2); and (3) Multi-Layered Sampler (MLS)
that implements generalizable generation for object-level graph manipulation (Section 3.3)."
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.22739018087855298,"3.1
Semantics-Layout Variational AutoEncoder"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.22997416020671835,"Textual Scene Graph Construction. For constructing the scene graph representation, we employ the
visual-language model to fully leverage the inherent disentangled semantics of the language, while
simultaneously facilitating the alignment between images and scene graphs. Specifically, we augment
the node and edge embeddings of the scene graph with CLIP [6] text embeddings. During training,
we also incorporate spatial information for node embeddings by including bounding box coordinates
(i.e., top-left corner and box size denoted as bi = (xi, yi, wi, hi)). Then the node embeddings O and
edge embeddings E can be formulated as follows:"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.23255813953488372,"O = {Eo
emb(co
i ) ⊗ECLIP(oi) ⊗Ebox(bi)}No
i=1, E = {Ee
emb(ce
ij) ⊗ECLIP(tij)}1≤i,j≤No,i̸=j (2)"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.2351421188630491,"where ECLIP denotes the frozen pre-trained text encoder, Ebox is the spatial encoder for bounding
box coordinates using Multi-Layer Perceptions (MLPs), and ⊗denotes concatenate operation."
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.23772609819121446,"Graph Union Encoding. Although layout-based SG2I methods are superior in modeling spatial
topology compared to the semantics-based method, they fall short in capturing object interactions
(i.e., non-spatial relationships) within the scene. Accordingly, after obtaining the node and edge
embeddings mentioned above, we apply a Conditional Variational Autoencoder (CVAE) [22] based
on triplet-GCN [18] to jointly model the layout and semantics information. As shown in Figure 3.I,
the L-layer Graph Union Encoder Eu takes node and edge embeddings as inputs:"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.24031007751937986,"(ϕl+1
i
, ϕl+1
ij , ϕl+1
j
) = GCNl(ϕl
i, ϕl
ij, ϕl
j), l ∈{0, . . . , L −1}
(3)"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.24289405684754523,"where l denotes the layer index of Graph Union Encoder, and ϕ denotes intermediate features. Here we
initialize (ϕ0
i , ϕ0
ij, ϕ0
j) = (Oi, Eij, Oj). Please refer to the Appendix for more details about the triplet-
GCN. Given that the last node embedding ϕL
i integrates both topology and interaction information, we
conduct layout-semantic modeling by parameterizing it into Gaussian spaces Z ∼N(µ, σ). In this
context, the means µ ∈RDz and variances σ ∈RDz are estimated individually by two supplementary
MLPs, where Dz denotes the dimensional of latent space for node embedding. Hence, we jointly
model the layout and semantics through the following minimization:"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.2454780361757106,"Lunion = KL (Eu(u|y, O, E) ∥p(u|y)) ,
(4)"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.24806201550387597,"where KL denotes the Kullback-Liebler divergence, y denotes condition and the prior p(u|y) is the
standard Gaussian distribution N(u | 0, 1). Specifically, we condition the latent space of the graph
structure using the edge embedding following Equation 2 alongside the updated node embedding
{Eo
emb(co
i ) ⊗ECLIP(oi) ⊗ui}No
i=1, where ui is a random vector sampled from Z. This architecture
ensures that layout is solely necessary for training, with no need for hand-crafted layout in inference."
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.25064599483204136,"Disentangled Semantics-Layout Decoding. As illustrated in Figure 3.II, we disentangle the explicit
spatial layout and implicit interactive semantics from the latent space using two separate triplet-GCN-
based decoders, i.e., layout decoder Dl and semantic decoder Ds. The proposed Semantics-Layout"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.2532299741602067,"Variational AutoEncode (SL-VAE) comprises these two decoders and the graph union encoder
mentioned above, which derives the spatial topology and object interactions from the scene graph
representation. During training, the layout decoder is optimized by the following objective function:"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.2558139534883721,"Llayout = 1 No No
X"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.25839793281653745,"i=1
|bi −ˆbi|1,
(5)"
SEMANTICS-LAYOUT VARIATIONAL AUTOENCODER,0.26098191214470284,"where ˆbi denotes the predicted coordinates. We only incorporate the ground truth layout B = {bi}No
i=1
during training, while generating Nl diverse layouts { ˆBn = {ˆbn,i}No
i=1}Nl
n=1 by sampling Gaussian
noise at inference time. For simplicity, we omit the superscriptˆin the following description. The
semantic decoder Ds generates semantics embeddings S = {si}No
i=1 to facilitate subsequent diffusion
processes, and its parameters are iteratively updated with the diffusion loss in the next Section 3.2."
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.26356589147286824,"3.2
Diffusion with Compositional Masked Attention"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.2661498708010336,"Object-level Fusion Tokenizer. We integrate the spatial layout B = {bi}No
i=1 and interactive
semantics S = {si}No
i=1 at object level, as illustrated in Figure 3.III. The single-object embeddings
C = {ci}No
i=1 = {si ⊗F(bi)}No
i=1 are acquired by directly applying semantic embeddings, while
encoding box information using a Fourier mapping F [23]. We define a learnable null embedding to
pad the embedding length to Nmax, thereby accommodating varying numbers of objects:"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.268733850129199,"ci =
si ⊗F(bi),
i ≤No
cnull,
otherwise
(6)"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.2713178294573643,"where cnull denotes the learnable null embedding for padding. We optionally add attribute embedding
A = {ai}Nmax
i=1
to construct updated C = {ci ⊗ai}Nmax
i=1
, where ci and ai are separately processed
by two MLPs before concatenation. Note that ai is obtained similarly to edge embedding in Equation
2. We also define a learnable null embedding anull for cases where no attribute is specified."
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.2739018087855297,(a) Compositional masked attention
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.27648578811369506,"A1
A2
B1"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.27906976744186046,"object embedding
visual token A
B"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.28165374677002586,"A1
A2
B1
A
B
(b) Attention mask"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.2842377260981912,"A1
1
1
-inf
-inf
1
1
1
-inf
-inf
1"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.2868217054263566,"1
1
-inf
-inf
1
1
1
-inf
-inf
1"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.28940568475452194,"-inf
-inf
1
1
-inf
-inf
-inf
1
1
-inf"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.29198966408268734,"1
1
-inf
-inf
1
1
1
-inf
-inf
1"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.29457364341085274,"-inf
-inf
1
1
-inf A2 B B1 A B1 A"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.2971576227390181,"object 
embedding
visual token"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.2997416020671835,"visual token
object 
embedding"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3023255813953488,"A1
1
1
-inf
-inf
1"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3049095607235142,"1
1
-inf
-inf
1"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.30749354005167956,"-inf
-inf
1
1
-inf"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.31007751937984496,"1
1
-inf
-inf
1"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.31266149870801035,"-inf
-inf
1
1
-inf A2 B B1 A"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3152454780361757,"object 
embedding
visual token"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3178294573643411,"visual token
object 
embedding"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.32041343669250644,"Figure 4: Toy example of (a)
compositional masked atten-
tion, and (b) its correspond-
ing attention mask. We use vi-
sual tokens and object embed-
dings of objects A and B for
demonstration. A and B have
1 and 2 visual tokens, respec-
tively, whose attribution is de-
termined by bounding boxes."
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.32299741602067183,"Compositional Masked Attention. The cross-attention mechanism
in diffusion bridges the visual and textual information, while self-
attention captures self-related information within visual tokens [24].
Therefore, we insert our proposed Compositional Masked Atten-
tion (CMA) between self-attention and cross-attention layers. This
technique effectively injects graph information into the diffusion
process at the object level, preventing semantic confusion and at-
tribute leakage through the attention mask. Specifically, we denote
the visual token output by the vanilla self-attention as V ∈RNv×Dv,
where Nv and Dv represent the number and dimensions of tokens,
respectively. Then the CMA layer can be expressed as:"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.32558139534883723,"ˆV = SAmask(V ⊗ˆC, M)[: Nv],
(7)"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3281653746770026,"where ˆC = {ˆci}Nmax
i=1
denotes object embeddings whose dimensions
are aligned with the visual token V using MLPs. The matrix M ∈
R(Nv+Nmax)×(Nv+Nmax) denotes the attention mask that depends
on layout B, which can be constructed as follows:"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.330749354005168,"Mi,j =
1, if i, j fall into the same object
−inf, otherwise
(8)"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3333333333333333,"where “i, j fall into the same object” means that i and j index the
visual tokens or object embeddings of the same object. Figure 4
illustrates the mechanism of CMA through a toy example. In con-
trast to vanilla self-attention, the proposed CMA prevents relational
confusion and attribute leakage between different objects through
the well-designed object-level masks mentioned above. As shown in Figure 3.IV, we forward the
output of the CMA layer into the subsequent layers, serving as the updated visual token."
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3359173126614987,"Diffusion Loss. Based on the object semantics output from the proposed SL-VAE, we optimize
the evidence lower bound between sampling noise and prediction noise conditioned on object-level"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3385012919896641,"Table 1: Performance comparison on COCO-Stuff and Visual Genome datasets using Inception
Score (IS) and Fréchet Inception Distance (FID) metrics. We report the results of methods with two
generator structures, namely GAN- and Diffusion-based. The architecture of these methods is based
on the layout (L) or semantics (S), while our approach includes both. The best results are bolded."
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.34108527131782945,"Method
Type
COCO [26]
Visual Genome [27]"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.34366925064599485,"IS ↑
FID ↓
IS ↑
FID ↓"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3462532299741602,"Real Image
-
30.7
-
27.3
-"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3488372093023256,GAN-based
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.35142118863049093,"SG2Im [17]
L
8.2
99.1
7.9
90.5
PasteGAN [28]
L
12.3
79.1
8.1
66.5
SOAP [18]
L
14.5
81.0
-
-
WSGC [19]
L
6.5
121.7
9.8
84.1
KCGM [29]
S
-
-
11.6
27.4"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.35400516795865633,Diffusion-based
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.35658914728682173,"LDM [20]
S
22.2
63.8
16.5
45.7
SGDiff [16]
S
17.8
36.2
16.4
26.0
SceneGenie [15]
L
22.2
63.3
20.3
42.2
R3CD [14]
S
19.5
32.9
18.9
23.4
DisCo (ours)
L+S
23.1
30.8
22.3
21.9"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.35917312661498707,"information (i.e., ground truth spatial layout and generated interactive semantics). Then the training
loss of the diffusion model equipped with CMA can be summarized as follows:"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.36175710594315247,"LLDM = Ez,ϵ∼N(0,1),t[∥ϵ −ϵθ(zt, ECLIP(O), B, S, A, t)∥2
2].
(9)"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3643410852713178,"Finally, we employ an end-to-end joint training pipeline for the whole proposed DisCo framework.
The total objective function is presented as follows:"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3669250645994832,"Ltotal = λ1LLDM + λ2Lunion + λ3Llayout,
(10)"
DIFFUSION WITH COMPOSITIONAL MASKED ATTENTION,0.3695090439276486,"where λ1, λ2, and λ3 are hyperparameters, which are typically set to 1.0, 0.1 and 1.0, respectively."
MULTI-LAYERED SAMPLER,0.37209302325581395,"3.3
Multi-Layered Sampler"
MULTI-LAYERED SAMPLER,0.37467700258397935,"Manipulations in the input scene graph, such as node addition and attribute adjustment, pose chal-
lenges for maintaining visual consistency in the generated images, ultimately compromising general-
izability. To achieve an “isolated” image editing effect, we also provide the Multi-Layered Sampler
(MLS) motivated by SceneDiffusion [25]. The scheme defines each object as a layer, thus allowing
independent object-level Gaussian sampling. In contrast to SceneDiffusion which scrambles the
reference layouts randomly, we sample additional Nl (layouts, semantics) by the SL-VAE. Note that
Nl fixed seeds exist for the same scene. Then we aggregate latent codes from various layers into zn
and utilize layout-converted non-overlapping masks {Mn = {mn,i}No
i=1}Nl
n=1 for locally conditioned
diffusion. During the inference, the noise estimation for Nl scenes is calculated as follows:"
MULTI-LAYERED SAMPLER,0.3772609819121447,"ˆϵ(t)
n = No
X"
MULTI-LAYERED SAMPLER,0.3798449612403101,"i=1
mn,i ⊙ϵθ(z(t)
n , ECLIP(oi), bn,i, sn,i, ai, t),
(11)"
MULTI-LAYERED SAMPLER,0.38242894056847543,"Subsequently, the latent code for each object is computed as the weighted average of the Nl cropped
denoised views. Please refer to the Appendix for more details about MLS."
EXPERIMENTS,0.3850129198966408,"4
Experiments"
EXPERIMENTS,0.3875968992248062,"Dataset. We conduct scene-graph-to-image (SG2I) generation experiments on the Visual Genome
(VG) [27] and COCO-Stuff (COCO) [26] datasets. The VG dataset comprises 108, 077 image-scene
graph pairs, accompanied by the bounding box coordinates and object attributes. Following previous
work [17], we select objects and relationships that appear at least 2, 000 and 500 times respectively
in VG, resulting in 178 objects and 45 unique relationship types. Also, we ignore small objects and
use images containing 5 to 30 objects along with a minimum of 3 relationships. Based on the above"
EXPERIMENTS,0.39018087855297157,"filtering, we have 62, 565 images available for training, each containing an average of 10 objects
and 5 relationships. While the original COCO-Stuff dataset [26] lacks scene graph annotations,
it consists of 40, 000 images annotated with bounding box coordinates and captions, essential for
synthesizing geometric scene graphs [15, 16]. All images in the COCO-Stuff dataset are labeled as
80 item categories and 91 stuff categories."
EXPERIMENTS,0.39276485788113696,"Implementation Details. We fine-tune the pre-trained Stable-Diffusion 1.51 with the modified
Attention module on 4 NVIDIA A100 GPUs, each with 80GB of memory. We apply the CLIP text
encoder (vit-large-patch14 ) to construct the textual scene graph. We train the model with a batch
size of 64 using the AdamW optimizer [30] with an initial learning rate of 1.0 × 10−4, which is
adjusted linearly over 50, 000 steps. During inference, we use the 50-step PNDMScheduler [21] with
a classifiers-free scale [31] of 7.5. The sample number Nl in the multi-layered sampler is set to 5."
EXPERIMENTS,0.3953488372093023,"Evaluation Metrics. Following previous works [14, 15, 16], we evaluate the performance of our
method with the Inception Score (IS) [32] and the Fréchet Inception Distance (FID) [33]. The IS score
is derived from a pre-trained Inception Net [34], assessing both the quality and diversity of synthesized
images. The FID score quantifies the dissimilarity between the generated image and the real image
distribution, which evaluates the fidelity of the generated images. To measure the effectiveness of
compositional generation, we further evaluate our method on the T2I-CompBench [35]. Besides, we
apply CLIP for zero-shot attribute classification of the controlled object cropped by the bounding box,
and subsequently evaluate the attribute control performance by the classification accuracy ACCattr."
EXPERIMENTS,0.3979328165374677,"Table 2: Relationship and attribute generation compared
with text-to-image methods on T2I-CompBench [35]."
EXPERIMENTS,0.4005167958656331,"Method
UniDet
CLIP
B-VQA
3-in-1"
EXPERIMENTS,0.40310077519379844,"SD-v1.4 [20]
0.1246
0.3079
0.3765
0.3080
SD-v2 [20]
0.1342
0.3127
0.5065
0.3386
Composable [36]
0.0800
0.2980
0.4063
0.2898
Structured [37]
0.1386
0.3111
0.4990
0.3355
Attn-Exct [38]
0.1455
0.3109
0.6400
0.3401
GORS [35]
0.1815
0.3193
0.6603
0.3328"
EXPERIMENTS,0.40568475452196384,"DisCo (ours)
0.2376
0.3217
0.6959
0.4143"
EXPERIMENTS,0.4082687338501292,"Quantitative Comparisons.
To
demonstrate the effectiveness of the
proposed DisCo, we compare it with
current state-of-the-art SG2I meth-
ods on the COCO-Stuff and Visual
Genome datasets, which are summa-
rized in Table 1. Our DisCo outper-
forms other methods in both IS and
FID scores, revealing its superior per-
formance in both fidelity and diver-
sity of image generation. Compared
with previous methods, the primary
architectural advantage of DisCo is
its innovative approach of simultaneously integrating disentangled layout and semantics extracted
from scene graph representations. Moreover, the proposed SL-VAE achieves the diverse generation of
layouts and semantics from a single scene graph through Gaussian distribution sampling. Therefore,
our DisCo integrates the benefits of both layout-based and semantic-based methods, which is further
ablated in detail in Table 4 of the ablation study. We proceed to assess the compositional generation
on the T2I-CompBench [35], as shown in Table 2. The benchmark evaluates the competency of the
text-to-image model in responding to compositional prompts. We report UniDet, CLIP, B-VQA,
and 3-in-1 scores for measuring the generation of spatial/non-spatial relationships, attributes, and
complex scenes, respectively. Following [35], we use the UniDet [39], CLIP [6], and BLIP [7] to
evaluate these results. Our DisCo surpasses all compared T2I methods, confirming the efficacy of
scene graphs in depicting complex scenes."
EXPERIMENTS,0.4108527131782946,"Table 3: User study. The score quantifies the user evaluation (i.e., relationships, quantities, and
generation quality) of the alignment between the given prompt and the generated image."
EXPERIMENTS,0.4134366925064599,"Method
SD-XL
[40]
DALL·E 3
[3]
Imagen 2
[41]
GLIGEN
[12]
LD
[10]
MIGC
[13]
SG2Im
[17]
SGDiff
[16]
R3CD
[14]
DisCo
(ours)"
EXPERIMENTS,0.4160206718346253,"Score
0.6684
0.5944
0.5637
0.6549
0.6200
0.7055
0.3783
0.4717
0.6928
0.8533"
EXPERIMENTS,0.4186046511627907,"User Study. We conduct a user study by recruiting 50 participants from Amazon Mechanical Turk.
We randomly select 8 prompts for each method, resulting in 80 generated images. We ask participants
to score each generated image independently based on the image-prompt alignment. The worker
can choose a score from {1, 2, 3, 4, 5} and we normalize the scores by dividing them by 5. We then
compute the average score across all images and all workers. The results are presented in the Table 3.
Our method is favored by most participants in terms of generation rationality and controllability."
EXPERIMENTS,0.42118863049095606,1https://huggingface.co/runwayml/stable-diffusion-v1-5
EXPERIMENTS,0.42377260981912146,"A sheep by another sheep on the grass with the ocean 
under the sky; the ocean by a tree; a boat on the grass"
EXPERIMENTS,0.4263565891472868,A building with a window on side of a bus has a tire
EXPERIMENTS,0.4289405684754522,Three turkeys on top of the pasture
EXPERIMENTS,0.4315245478036176,"SD-XL
DALL·E 3
Imagen 2
Ours
(a) Comparison with T2I methods in
spatial relationships and object quantities."
EXPERIMENTS,0.43410852713178294,Two men standing on a beach playing a kite
EXPERIMENTS,0.43669250645994834,A dog chasing a cat on the grass
EXPERIMENTS,0.4392764857881137,A building with a window on side of a bus has a tire
EXPERIMENTS,0.4418604651162791,"GLIGEN
LayoutDiffusion
MIGC
Ours
(b) Comparison with L2I methods in
non-spatial interactions and rationality."
EXPERIMENTS,0.4444444444444444,"SG2Im
SGDiff
R3CD
Ours
SG2Im
SGDiff
R3CD
Ours
(c) Comparison with SG2I methods in independent node inference and generation quality."
EXPERIMENTS,0.4470284237726098,"Figure 5: Qualitative Comparisons with (a) text-to-image (T2I) (SableDiffusion-XL [40], DALL·E
3 [3], and Imagen 2 [41]), (b) layout-to-image (L2I) (GLIGEN [12], LayoutDiffusion [10], and MIGC
[13]), and (c) scene-graph-to-image (SG2I) (SG2Im [17], SGDiff [16], and R3CD [14]) methods."
EXPERIMENTS,0.4496124031007752,"Table 4: Ablation study for overall architecture.
SL-VAE w/o Ds means independent use of O."
EXPERIMENTS,0.45219638242894056,"Method
G2I-ACC ↑I2G-ACC ↑"
EXPERIMENTS,0.45478036175710596,"Layout (Dl)
70.3
70.5
Semantics (Ds)
71.1
71.5
SL-VAE (w/o Ds)
72.9
72.8
SL-VAE (Dl + Ds)
73.9
74.3"
EXPERIMENTS,0.4573643410852713,"Table 5: Ablation study for attention mechanism.
Vanilla attention means off-the-shelf T2I attention."
EXPERIMENTS,0.4599483204134367,"Attention Type
IS ↑
FID ↓"
EXPERIMENTS,0.4625322997416021,"Vanilla attention
17.2
29.1
CMA (w/o mask M)
17.9
28.4
CMA (union MLP)
19.8
22.0
CMA (separate MLP)
22.3
21.9"
EXPERIMENTS,0.46511627906976744,"Qualitative Comparisons. Figure 5 visualizes the results of the methods conditioned by text,
layout, or scene graph, showcasing our advantages in generating rationality and controllability: (i)
Comparison with the text-to-image (T2I) methods. In Figure 5 (a), we present the superiority of"
EXPERIMENTS,0.46770025839793283,"(a) unmodified
(d) w/ AC (red)
(c) w/ AC (blue)
(b) w/ NA"
EXPERIMENTS,0.4702842377260982,"Figure 6: Illustration of object-level Node Addition (NA) and
Attribute Control (AC) in the scene. From left to right: (a) the
image generated by the unmodified scene graph; (b) the chair
addition; (c) the blue-colored wall; and (d) the red-colored wall."
EXPERIMENTS,0.4728682170542636,"the disentangled structured scene
graph over linear text for repre-
senting complex scenes. Firstly,
we resolve ambiguity in textual
relationships and semantics by
employing layout and semantic
disentanglement within the scene
graph. For example, our DisCo
clarifies the relationship between
“boat” and “grass” in the first line,
as well as the semantics of “bus”
and “building” in the following
line. Additionally, the samples in the first and last lines also showcase our capacity to generate the
specified quantities of objects precisely. (ii) Comparison with the layout-to-image (L2I) methods.
We also demonstrate that our DisCo outperforms the diffusion methods relying on manually crafted
scene layout representations, as illustrated in Figure 5 (b). While the L2I method struggles to model
non-spatial interactions (such as “playing” in the first line and “chasing” in the second line), our
DisCo addresses this challenge using disentangled object interactive semantics. Furthermore, by
establishing semantics among objects derived from their relationships, we prevent independent gen-
eration instances that rely solely on the layout, exemplified by the “bus” and “tire” in the last line.
(iii) Comparison with the scene-graph-to-image (SG2I) methods. The SG2I visualization results of
different methods are showcased in Figure 5 (c). Our DisCo significantly improves the quality of
SG2I generation, particularly for independent nodes. The proposed layout and semantics disentan-
glement technique effectively capture both the spatial and interactive information of independent
nodes. Taking the “lamp” and “stone” in the first image and the “shadow” in the second image as
illustrations, these entities are neglected by previous methods, whereas our DisCo not only retains
their semantic relevance but also infers their appropriate spatial placement within the scene. We
also demonstrate the generalizable generation under consistency for graph manipulation (i.e., node
addition and attribute control) in SG2I tasks, as shown in Figure 6."
EXPERIMENTS,0.4754521963824289,Table 6: Ablation study for Multi-Layer Sampler (MLS).
EXPERIMENTS,0.4780361757105943,"Method
IS ↑
FID ↓"
EXPERIMENTS,0.4806201550387597,"Baseline (w/o MLS)
20.5
23.0"
EXPERIMENTS,0.48320413436692505,"w/ LSD [25]
21.1
22.7
w/ MLS (Ours)
22.3
21.9 45 50 55 60"
EXPERIMENTS,0.48578811369509045,ACCattr
EXPERIMENTS,0.4883720930232558,"Baseline
LSD
MLS"
EXPERIMENTS,0.4909560723514212,"Ablation Study. Table 4 explores the
overall architecture by evaluating the
alignment of the generated image with
the objects and relationships depicted
in the input scene graph. Following
SGDiff [16], we conduct this analysis
by graph-to-image (G2I) and image-
to-graph (I2G) retrieval experiments.
Note that “w/o Ds” means processing each node embedding by MLP independently,instead of
obtaining object interactive semantics through Ds. We observe that the spatial layout and interactive
semantics collaborate to boost both retrieval tasks. These results demonstrate the effectiveness of
integrating explicit spatial relations with implicit interactive semantics. In Table 5, we study the
impact of different attention mechanisms. We inject graph conditions using different mechanisms: (a)
Vanilla attention mechanism in the T2I diffusion model without our CMA; (b) CMA without attention
mask M; (c) CMA with a union MLP after concatenating object and attribute embeddings; and (d)
CMA with two separate MLPs before concatenating object and attribute embeddings. We found that
CMA, which fuses separate encoding of object and attribute embeddings, significantly enhances the
overall generation performance. Table 6 presents the Multi-Layer Sampler (MLS) ablation results,
confirming its enhancement over the baseline and LSD [29]. In contrast to LSD, which randomly
scrambles layouts, the proposed MLS naturally leverages a variety of coherent layouts and semantics
produced by SL-VAE. Moreover, the increase in ACCattr scores also indicates that MLS facilitates
controllability, especially in attribute control, while ensuring generation quality."
RELATED WORKS,0.4935400516795866,"5
Related Works"
RELATED WORKS,0.49612403100775193,"Diffusion Models. Diffusion models (DMs) [2, 20, 31, 42] have achieved great success in high-quality
image generation. The essence of DMs lies in estimating image distributions by iterative denoising
noise-corrupted image, showcasing the superiority over VAEs [43, 44] and GANs [45] in training
stability and likelihood estimation. To further explore the controllability of DMs, considerable efforts"
RELATED WORKS,0.49870801033591733,"have been devoted to conditional generation based on DMs. Benefiting from the naturalness of
language [6] and the advancements of vision-language foundation models [6, 7, 8, 9], numerous text-
to-image DMs [40, 41, 46] are beginning to emerge, facilitating explicit control of the corresponding
semantics and style. However, the expressive capacity of linear text is limited. Therefore, many studies
also endeavor to bolster global control through supplementary conditions, such as depth [46, 47],
layout [10, 11, 12, 13], segmentation map [46, 48], and scene graph [14, 15]."
RELATED WORKS,0.5012919896640827,"Image Generation from Scene Graphs. Scene graphs are structured scene representations, where
nodes represent objects and edges represent relationships between objects [17, 27]. Given the
superiority of scene graphs over linear text in delineating multiple objects and their intricate relation-
ships [27, 49, 50], many studies investigate image generation from scene graphs. These approaches
typically fall into two categories: layout-based and semantics-based methods. Layout-based meth-
ods [17, 18, 19, 28, 36, 51] initially map scene graphs to coarse scene layouts comprising multiple
bounding boxes and further refine these layouts to images with a layout-to-image model (e.g., Layout-
Diffusion [10]). While the layout depicts spatial relationships, it fails to capture abstract relationships
within the scene, leading to a lack of object interaction. Another branch is semantics-based methods
[14, 16, 29, 52, 53], which focus on graph understanding by directly encoding semantic information
from the scene graph. Nevertheless, these methods have limitations in addressing independent scene
nodes, leading to issues like entity loss and unreasonable placement. In this paper, we propose a
compositional image generation that leverages the layout and semantics derived from the scene graph
representation. We complement explicit layout and implicit semantics to enhance the understanding
of the diffusion model for scene graphs. Additionally, to improve the controllability in the scene-
graph-to-image task, we also attain generalizable generation for object-level graph manipulation (i.e.,
node addition and attribute control)."
LIMITATIONS,0.5038759689922481,"6
Limitations"
LIMITATIONS,0.5064599483204134,"Figure 7: Qualitative limitations
on attribute leakage of overlapping."
LIMITATIONS,0.5090439276485789,"The proposed CMA injects object-level information into the
diffusion model via masks from the layout, effectively miti-
gating semantic ambiguity and limiting attribute leakage. In
scenarios involving object overlap, the proposed CMA inhibits
direct interaction between the visual token and the object em-
bedding along with its attributes. Nonetheless, the attribute
information from the visual token inadvertently leaks into the
overlapping region in subsequent layers. Hence, there may be
attribute leakage among the objects, as shown in Figure 7."
CONCLUSION,0.5116279069767442,"7
Conclusion"
CONCLUSION,0.5142118863049095,"In this study, we leverage the disentangled textual scene graph representation to condition the
diffusion process for generating complex scene images. The innovation of our framework lies in
utilizing the VAE for scene relationship modeling and diffusion model (DM) for the composite visual
generation. To comprehensively capture spatial relationships and non-spatial interactions within
scenes, we introduce the Semantics-Layout Variational AutoEncoder (SL-VAE) for deriving diverse
layouts and semantics from a single scene graph. Building upon them, we propose the Compositional
Masked Attention (CMA) integrated with DM, which guides the de-noising trajectory by compositing
extracted object-level graph information with fine-grained attributes. We also introduce a Multi-Layer
Sampler (MLS) to preserve the main visual content while modifying the input scene graph. Extensive
experiments demonstrate that our framework outperforms current methods conditioned by text, layout,
or scene graph in relationship modeling and controllability."
CONCLUSION,0.5167958656330749,Acknowledgments
CONCLUSION,0.5193798449612403,"This research is supported by the National Natural Science Foundation of China [Grant 62302246]
and the Zhejiang Provincial Natural Science Foundation of China [Grant LQ23F010008]. We also
express our sincere gratitude to the AI Computing Center at the Eastern Institute of Technology for
their valuable support and assistance."
REFERENCES,0.5219638242894057,References
REFERENCES,0.524547803617571,"[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
in Neural Information Processing Systems (NeurIPS), 33:6840–6851, 2020. 1, 3
[2] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.
Advances in Neural Information Processing Systems (NeurIPS), 34:8780–8794, 2021. 1, 9
[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang,
Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions.
Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2:8, 2023. 1, 2, 7, 8
[4] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing
with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1
[5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1
[6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In Proceedings of the International Conference on
Machine Learning (ICML), pages 8748–8763, 2021. 1, 2, 3, 4, 7, 10
[7] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image
pre-training for unified vision-language understanding and generation. In Proceedings of the
International Conference on Machine Learning (ICML), pages 12888–12900, 2022. 1, 7, 10
[8] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models. In Proceedings of the
International Conference on Machine Learning (ICML), pages 19730–19742, 2023. 1, 10
[9] Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, and Yujun Shen.
Dreamlip: Language-image pre-training with long captions. arXiv preprint arXiv:2403.17007,
2024. 1, 10
[10] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Lay-
outdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
22490–22499, 2023. 1, 2, 7, 8, 10
[11] Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, and Mu Li. Layoutdif-
fuse: Adapting foundational diffusion models for layout-to-image generation. arXiv preprint
arXiv:2302.08908, 2023. 1, 10
[12] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan
Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
22511–22521, 2023. 1, 7, 8, 10
[13] Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc: Multi-instance generation
controller for text-to-image synthesis. Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), 2024. 1, 7, 8, 10
[14] Jinxiu Liu and Qi Liu. R3cd: Scene graph to image generation with relation-aware compositional
contrastive control diffusion. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI), volume 38, pages 3657–3665, 2024. 1, 2, 6, 7, 8, 10
[15] Azade Farshad, Yousef Yeganeh, Yu Chi, Chengzhi Shen, Böjrn Ommer, and Nassir Navab.
Scenegenie: Scene graph guided diffusion models for image synthesis. In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV), pages 88–98, 2023. 1, 2, 6, 7,
10
[16] Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui,
Bernard Ghanem, and Ming-Hsuan Yang. Diffusion-based scene graph to image generation
with masked contrastive pre-training. arXiv preprint arXiv:2211.11138, 2022. 1, 2, 6, 7, 8, 9, 10
[17] Justin Johnson, Agrim Gupta, and Li Fei-Fei.
Image generation from scene graphs.
In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pages 1219–1228, 2018. 1, 2, 4, 6, 7, 8, 10"
REFERENCES,0.5271317829457365,"[18] Oron Ashual and Lior Wolf. Specifying object attributes and relations in interactive scene
generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV), pages 4561–4569, 2019. 2, 4, 6, 10"
REFERENCES,0.5297157622739018,"[19] Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor Darrell, and Amir Globerson. Learning
canonical representations for scene graph to image generation. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 210–227. Springer, 2020. 2, 6, 10"
REFERENCES,0.5322997416020672,"[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684–10695, 2022.
3, 6, 7, 9, 14, 16"
REFERENCES,0.5348837209302325,"[21] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models
on manifolds. arXiv preprint arXiv:2202.09778, 2022. 3, 7"
REFERENCES,0.537467700258398,"[22] Andrew Luo, Zhoutong Zhang, Jiajun Wu, and Joshua B Tenenbaum. End-to-end optimization
of scene layout. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3754–3763, 2020. 4"
REFERENCES,0.5400516795865633,"[23] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,
Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let
networks learn high frequency functions in low dimensional domains. Advances in Neural
Information Processing Systems (NeurIPS), 33:7537–7547, 2020. 5"
REFERENCES,0.5426356589147286,"[24] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual
conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 5"
REFERENCES,0.5452196382428941,"[25] Jiawei Ren, Mengmeng Xu, Jui-Chieh Wu, Ziwei Liu, Tao Xiang, and Antoine Toisoul. Move
anything with layered scene diffusion. arXiv preprint arXiv:2404.07178, 2024. 6, 9, 14, 15"
REFERENCES,0.5478036175710594,"[26] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes
in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1209–1218, 2018. 6, 7"
REFERENCES,0.5503875968992248,"[27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting
language and vision using crowdsourced dense image annotations. International Journal of
Computer Vision (IJCV), 123:32–73, 2017. 6, 10"
REFERENCES,0.5529715762273901,"[28] Yikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, and Xiaogang Wang. Pastegan: A
semi-parametric method to generate image from scene graph. Advances in Neural Information
Processing Systems (NeurIPS), 32, 2019. 6, 10"
REFERENCES,0.5555555555555556,"[29] Yang Wu, Pengxu Wei, and Liang Lin. Scene graph to image synthesis via knowledge consensus.
In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 37, pages
2856–2865, 2023. 6, 9, 10"
REFERENCES,0.5581395348837209,"[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. 7"
REFERENCES,0.5607235142118863,"[31] Jonathan Ho and Tim Salimans.
Classifier-free diffusion guidance.
arXiv preprint
arXiv:2207.12598, 2022. 7, 9"
REFERENCES,0.5633074935400517,"[32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. Advances in Neural Information Processing Systems
(NeurIPS), 29, 2016. 7"
REFERENCES,0.5658914728682171,"[33] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
Neural Information Processing Systems (NeurIPS), 30, 2017. 7"
REFERENCES,0.5684754521963824,"[34] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, 2016. 7"
REFERENCES,0.5710594315245479,"[35] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A compre-
hensive benchmark for open-world compositional text-to-image generation. Advances in Neural
Information Processing Systems (NeurIPS), 36:78723–78747, 2023. 7"
REFERENCES,0.5736434108527132,"[36] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional
visual generation with composable diffusion models. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 423–439. Springer, 2022. 7, 10
[37] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato
Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for
compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 7
[38] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite:
Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on
Graphics (TOG), 42(4):1–10, 2023. 7
[39] Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Simple multi-dataset detection. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pages 7571–7580, 2022. 7
[40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe
Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image
synthesis. arXiv preprint arXiv:2307.01952, 2023. 7, 8, 10
[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,
Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.
Photorealistic text-to-image diffusion models with deep language understanding. Advances in
Neural Information Processing Systems (NeurIPS), 35:36479–36494, 2022. 7, 8, 10
[42] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
preprint arXiv:2011.13456, 2020. 9
[43] Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013. 9
[44] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation
using deep conditional generative models. Advances in Neural Information Processing Systems
(NeurIPS), 28, 2015. 9
[45] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications
of the ACM, 63(11):139–144, 2020. 9
[46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV), pages 3836–3847, 2023. 10
[47] Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, and Siavash Arjomand Bigdeli. Stere-
odiffusion: Training-free stereo image generation using latent diffusion models. arXiv preprint
arXiv:2403.04965, 2024. 10
[48] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-
modal face generation and editing. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 6080–6090, 2023. 10
[49] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein,
and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages 3668–3678, 2015. 10
[50] Subarna Tripathi, Anahita Bhiwandiwalla, Alexei Bastidas, and Hanlin Tang. Using scene graph
context to improve image generation. arXiv preprint arXiv:1901.03762, 2019. 10
[51] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus,
Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle:
Compositional generation with energy-based diffusion models and mcmc. In Proceedings of
the International Conference on Machine Learning (ICML), pages 8489–8510, 2023. 10
[52] Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang, and Quan Bai. Swinv2-imagen: Hi-
erarchical vision transformer diffusion models for text-to-image generation. Neural Computing
and Applications, pages 1–16, 2023. 10
[53] Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. Composi-
tional text-to-image synthesis with attention map control of diffusion models. In Proceedings of
the AAAI Conference on Artificial Intelligence (AAAI), volume 38, pages 5544–5552, 2024. 10"
REFERENCES,0.5762273901808785,"A
Appendix"
REFERENCES,0.5788113695090439,"This appendix is organized as follows: Section A.1 provides more details about Semantics-Layout
Variation AutoEncoder; Section A.2 introduces the Stable Diffusion and its attention mechanism;
Section A.3 describes the implementation of the Multi-Layer Sampler in detail; Section A.4 covers
more ablation studies; Section A.5 presents more qualitative results, including comparison visualiza-
tion and graph manipulation; Section A.6 delves into the broader societal impacts of this work. The
core script is zipped and attached to the supplementary material."
REFERENCES,0.5813953488372093,"A.1
Semantics-Layout Variation AutoEncoder
Recall that we apply the triplet-GCN-based CVAE architecture in Section 3.1. Each triplet-GCN layer
in the encoder and decoder takes the node and edge embeddings. Specifically, the GCNl mentioned
in the paper uses two cascading MLPs {mlp1, mlp2} to deal with node and edge embeddings:"
REFERENCES,0.5839793281653747,"(ψl
i, ϕl+1
ij , ψl
j) = mlp1(ϕl
i, ϕl
ij, ϕl
j), l ∈{0, . . . , L −1},
(12)"
REFERENCES,0.58656330749354,"ϕl+1
i
= ψl
i + mlp2(avg(ψl
j | j ∈NE(oi))),
(13)"
REFERENCES,0.5891472868217055,"where l denotes the layer index of encoder or decoder, NE denotes the neighbor index set for each
node, avg denotes the average pooling operation, ϕ and ψ denote intermediate features. Hence, mlp1
conducts message passing among interconnected nodes and updates the edge features, while mlp2
aggregates features from all neighboring nodes and updates its features. For graph union encoder,
we let (ϕ0
i , ϕ0
ij, ϕ0
j) = (Oi, Eij, Oj). The last embedding ϕL
i is parameterized to the Gaussian
distribution Z ∼N(µ, σ), where µ, σ ∈RDz output by two additional MLPs and Dz denotes the
dimensional of latent space for node embedding."
REFERENCES,0.5917312661498708,"A.2
Diffusion with Compositional Masked Attention
Stable Diffusion [20] is one of most popular text-to-image model. As described in Section 2.1,
Stable Diffusion uses a U-Net ϵθ composed of convolution and transformer to estimate noise. The
transformer includes two attention mechanisms, namely Cross-Attention, and Self-Attention."
REFERENCES,0.5943152454780362,"Cross-Attention Layer. Text prompts are mapped to sequence embeddings by CLIP text encoder
and integrated into UNet via Cross-Attention to guide the de-noising trajectory:"
REFERENCES,0.5968992248062015,"Attention(Qvisual, Ktext, Vtext) = softmax(QvisualKT
text
√"
REFERENCES,0.599483204134367,"d
) · Vtext
(14)"
REFERENCES,0.6020671834625323,"where Qvisual denotes the Query from the visual token of the UNet, Ktext and Vtext denotes Key
and Value from text embeddings, all of which are projected by linear layers, d denotes the dimension
of Qvisual, Ktext, and Vtext."
REFERENCES,0.6046511627906976,Self-Attention Layer. Self-Attention captures self-related information within visual tokens:
REFERENCES,0.6072351421188631,"Attention(Qvisual, Kvisual, Vvisual) = softmax(QvisualKT
visual
√"
REFERENCES,0.6098191214470284,"d
) · Vvisual
(15)"
REFERENCES,0.6124031007751938,"where Qvisual, Kvisual, and Vvisual separately represent the Query, Key, and Value in self-attention
layers, which are projected by linear layers. The self-attention mechanism isolates the information
flow between specific tokens by multiplying a mask M to the QvisualKT
visual. Since M is applied
before softmax, the value of the isolated position is set to negative infinity −inf."
REFERENCES,0.6149870801033591,"Compositional Masked Attention Layer. Based on the attention mask M that depends on layout B,
the Compositional Masked Attention can be expressed as:"
REFERENCES,0.6175710594315246,"Attention(QCMA, KCMA, VCMA) = softmax(QCMAKT
CMA ⊙M
√"
REFERENCES,0.6201550387596899,"d
) · VCMA
(16)"
REFERENCES,0.6227390180878553,"where QCMA, KCMA, and VCMA individually represent the Query, Key, and Value derived from
V ⊗ˆC, achieved through linear layer projections. We insert our proposed Compositional Masked
Attention (CMA) between self-attention and cross-attention layers."
REFERENCES,0.6253229974160207,"A.3
Multi-Layer Sampler
Layered Scene Representation. We decompose a controllable scene containing No objects into No
layers. Different from SceneDiffusion [25], our approach involves each layer incorporating not only"
REFERENCES,0.627906976744186,"separate latent code zi and spatial layout bi, but also integrating the interactive semantics si produced
by the SL-VAE. Here we convert the layout parameter bi to two parts: (1) a fixed object-centric binary
mask mi ∈{0, 1}c×w×h to solely show the geometric property of the object, and (2) a two-element
offset pi = {µi, υi} to solely indicate its spatial locations, with µi and υi defining the horizontal and
vertical movement range. We sample Gaussian noise individually for the initial latent code of each
layer, i.e., Z = {z(T )
i
∼N(0, 1)}No
i=1. Then we utilize the layout-converted non-overlapping masks
{li}No
i=1 to derive the aggregated latent code z from various layers:"
REFERENCES,0.6304909560723514,"z(t) = No
X"
REFERENCES,0.6330749354005168,"i=1
li ⊙shift(z(t)
i , pi)
(17)"
REFERENCES,0.6356589147286822,"li = shift(mi, pi)"
REFERENCES,0.6382428940568475,"Ni−1
Y"
REFERENCES,0.6408268733850129,"j=1
(1 −shift(mj, pj)),
(18)"
REFERENCES,0.6434108527131783,"where ⊙denotes element-wise multiplication, and shift(x, p) denotes spatially shifting the values
of x in the direction of p."
REFERENCES,0.6459948320413437,"Multi-Layer Generation. We introduce the Multi-Layer Sampler that matches our diverse layout
and semantic simulation. In contrast to SceneDiffusion [25] which scrambles the reference layouts
randomly, we sample additional Nl layouts and semantics by the proposed SL-VAE. On the one hand,
the SL-VAE ensures that the generated scene layout is reasonable. On the other hand, we take full
advantage of the paired object-level (layouts, semantics). Specifically, the denoising scheme consists
of four steps:"
REFERENCES,0.648578811369509,"(a) Sampling additional Nl layouts {Bn = {bn,i}No
i=1}Nl
n=1 and semantics {Sn = {sn,i}No
i=1}Nl
n=1 by
the proposed SL-VAE. Note that Nl fixed seeds exist for the same scene graph. According to the
description of the layered representation, we convert the layout to get offset {Pn = {pn,i}No
i=1}Nl
n=1."
REFERENCES,0.6511627906976745,(b) Aggregating latent codes from various layers in each scene:
REFERENCES,0.6537467700258398,"z(t)
n
= No
X"
REFERENCES,0.6563307493540051,"i=1
li ⊙shift(z(t)
i , pn,i)
(19)"
REFERENCES,0.6589147286821705,"(c) Estimating the noise ˆϵ(t)
n from each aggregated latent code z(t)
n
and gets denoised aggregated
latent code ˆz(t−1)
n
∈{ˆz(t−1)
1
, . . . , ˆz(t−1)
Nl
}:"
REFERENCES,0.661498708010336,"ˆϵ(t)
n = No
X"
REFERENCES,0.6640826873385013,"i=1
mn,i ⊙ϵθ(z(t)
n , ECLIP(oi), bn,i, sn,i, ai, t),
(20)"
REFERENCES,0.6666666666666666,"where mn,i is the non-overlapping mask converted by the layout bn,i."
REFERENCES,0.6692506459948321,"(d) Updating the latent code of each layer by computing the weighted average of the Nl aggregated
latent code"
REFERENCES,0.6718346253229974,"z(t−1)
i
=
PNl
n=1 shift(li ⊙ˆz(t−1)
n
, −pn,i)
PNl
n=1 shift(li, −pn,i)
(21)"
REFERENCES,0.6744186046511628,"where shift(x, −p) denotes spatially shifting the values of x in the reverse direction of p."
REFERENCES,0.6770025839793282,"A.4
More Ablation Studies"
REFERENCES,0.6795865633074936,Table 7: Ablation study for graph construction.
REFERENCES,0.6821705426356589,"Graph Type
IS ↑
FID ↓"
REFERENCES,0.6847545219638242,"No CLIP Emb.
20.6
23.9
No Box Emb.
21.7
22.5
No Learnable Emb.
21.9
22.2"
REFERENCES,0.6873385012919897,"Graph Construction. We conduct ablation for
graph construction in Table 7. We investigate the
impact of different graph components (i.e., CLIP,
Box, and Learnable Embeddings) by turning off
each independently. We observe that each compo-
nent improves the performance, all of which are
crucial components presented in our DisCo."
REFERENCES,0.689922480620155,"Computing Consumption. We demonstrate the impact of our proposed CMA on the computational
complexity of the U-Net within the Stable Diffusion, as presented in Table 8. We use Floating Point"
REFERENCES,0.6925064599483204,Table 8: Ablation study for computing consumption.
REFERENCES,0.6950904392764858,"Method
FLOPs (G)
Params (M)
Time (ms)"
REFERENCES,0.6976744186046512,"SD-v1.5 [20]
677.5
859.4
37.9
DisCo
724.1
875.8
108.3"
REFERENCES,0.7002583979328165,"Operations (FLOPs), the number of parameters (Params), and inference time (Time) to measure
computing consumption. The FLOPS and Time metrics are conducted by processing the tensor with a
resolution of 2 × 4 × 64 × 64 on an NVIDIA A100 GPU. Our proposed DisCo significantly improves
the controllability of the Stable Diffusion with a tolerable increase in computational cost."
REFERENCES,0.7028423772609819,"A.5
More Visualization Results"
REFERENCES,0.7054263565891473,"Figure 8 showcases more generalizable generation results under consistency for graph manipulation
(i.e., node addition and attribute control) in SG2I task. In Figure 9, 10, and 11, we present more
visualization comparisons with the methods conditioned by text, layout, or scene graph, which
demonstrates the superiority of our DisCo in terms of generation rationality and controllability."
REFERENCES,0.7080103359173127,"A.6
Broader Impacts"
REFERENCES,0.710594315245478,"We demonstrate the superiority of our DisCo over existing generation methods based on text, layout,
and scene graphs, suggesting a potential beneficial influence on the realms of art creation and data
synthesis. Nevertheless, there remains a concern regarding the possibility of generating malicious
images or infringing copyright."
REFERENCES,0.7131782945736435,Graph Manipulation (Node Addition and Attribute Control)
REFERENCES,0.7157622739018088,Figure 8: Generalizable Generation Samples under Consistency for Graph Manipulation.
REFERENCES,0.7183462532299741,A teddy bear sit on the ground; another two teddy bear besides the table
REFERENCES,0.7209302325581395,The street by the building; the car next to the truck
REFERENCES,0.7235142118863049,Two cow on top of grass; one near the tree; sky above the grass; a building near the tree
REFERENCES,0.7260981912144703,Tree zebras standing on the grass; two near the tree; one in front of the house
REFERENCES,0.7286821705426356,"SD-XL
DALL·E 3
Imagen 2
Ours"
REFERENCES,0.7312661498708011,A building in front of a mountain; a tree in front of the building
REFERENCES,0.7338501291989664,Figure 9: Qualitative Comparison with Text-to-Image methods.
REFERENCES,0.7364341085271318,"GLIGEN
LayoutDiffusion
MIGC
Ours
GLIGEN
LayoutDiffusion
MIGC
Ours"
REFERENCES,0.7390180878552972,A child with the hair on the grass
REFERENCES,0.7416020671834626,A boy riding a dog on the grass
REFERENCES,0.7441860465116279,A boy holding a apple on the grass
REFERENCES,0.7467700258397932,A man looking at the cloud in sky
REFERENCES,0.7493540051679587,A man walking on the sidewalk
REFERENCES,0.751937984496124,Figure 10: Qualitative Comparison with Layout-to-Image methods.
REFERENCES,0.7545219638242894,"Scene Graph
SGDiff
R3CD
Ours"
REFERENCES,0.7571059431524548,"Scene Graph
SG2Im
SceneGenie
Ours"
REFERENCES,0.7596899224806202,Figure 11: Qualitative Comparison with Scene-Graph-to-Image methods.
REFERENCES,0.7622739018087855,NeurIPS Paper Checklist
CLAIMS,0.7648578811369509,1. Claims
CLAIMS,0.7674418604651163,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.7700258397932817,"Answer: [Yes]
Justification: We claim in the abstract and introduction that this paper studies generating
complex images with structured scene graphs."
CLAIMS,0.772609819121447,Guidelines:
CLAIMS,0.7751937984496124,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.7777777777777778,2. Limitations
LIMITATIONS,0.7803617571059431,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.7829457364341085,"Answer: [Yes]
Justification: The limitations are addressed in the Appendix, which will be included in the
camera-ready version."
LIMITATIONS,0.7855297157622739,Guidelines:
LIMITATIONS,0.7881136950904393,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.7906976744186046,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.7932816537467701,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.7958656330749354,"Answer: [Yes]
Justification: All the theorems and formulas are clearly stated in Section 2 and 3 of the
main paper, and complemented in Section A.1, A.2 and A.3 of the Supplemental Material.
Besides, all theorems and assumptions related to this work are properly referenced."
THEORY ASSUMPTIONS AND PROOFS,0.7984496124031008,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8010335917312662,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8036175710594315,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8062015503875969,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8087855297157622,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8113695090439277,"Justification: We provide the implementation details for reproducibility in Section 4, and the
project will be open-sourced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.813953488372093,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8165374677002584,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8191214470284238,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8217054263565892,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8242894056847545,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8268733850129198,"Justification: We attach the core script and data preparation to the Supplemental Material.
The complete code will be released in the camera-ready version, accompanied by detailed
instructions for reproducibility."
OPEN ACCESS TO DATA AND CODE,0.8294573643410853,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8320413436692506,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.834625322997416,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8372093023255814,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8397932816537468,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8423772609819121,Justification: We provide the training and test details in Section 4.
OPEN ACCESS TO DATA AND CODE,0.8449612403100775,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8475452196382429,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8501291989664083,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8527131782945736,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8552971576227391,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8578811369509044,"Justification: We report error bars suitably and correctly to achieve statistical significance in
the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8604651162790697,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8630490956072352,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8656330749354005,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8682170542635659,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8708010335917312,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.8733850129198967,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.875968992248062,Justification: We provide sufficient information on the computer resources in Section 4.
EXPERIMENTS COMPUTE RESOURCES,0.8785529715762274,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.8811369509043928,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.8837209302325582,9. Code Of Ethics
CODE OF ETHICS,0.8863049095607235,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.8888888888888888,Answer: [Yes]
CODE OF ETHICS,0.8914728682170543,Justification: Our work conforms with the NeurIPS Code of Ethics in every respect.
CODE OF ETHICS,0.8940568475452196,Guidelines:
CODE OF ETHICS,0.896640826873385,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.8992248062015504,10. Broader Impacts
BROADER IMPACTS,0.9018087855297158,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9043927648578811,Answer: [Yes]
BROADER IMPACTS,0.9069767441860465,"Justification: We discuss both potential positive and negative societal impacts in Section A.6
of the Appendix."
BROADER IMPACTS,0.9095607235142119,Guidelines:
BROADER IMPACTS,0.9121447028423773,• The answer NA means that there is no societal impact of the work performed.
BROADER IMPACTS,0.9147286821705426,"• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.917312661498708,11. Safeguards
SAFEGUARDS,0.9198966408268734,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9224806201550387,Answer: [Yes]
SAFEGUARDS,0.9250645994832042,"Justification: Our model is heavily based on existing StableDiffusion, so our safeguards are
the same as theirs."
SAFEGUARDS,0.9276485788113695,Guidelines:
SAFEGUARDS,0.9302325581395349,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9328165374677002,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9354005167958657,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.937984496124031,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9405684754521964,"Justification: In Section 4, we properly credit all the public baselines and datasets utilized in
this paper."
LICENSES FOR EXISTING ASSETS,0.9431524547803618,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9457364341085271,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset."
LICENSES FOR EXISTING ASSETS,0.9483204134366925,"• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9509043927648578,13. New Assets
NEW ASSETS,0.9534883720930233,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9560723514211886,Answer: [Yes]
NEW ASSETS,0.958656330749354,Justification: The assets introduced in the paper are well documented.
NEW ASSETS,0.9612403100775194,Guidelines:
NEW ASSETS,0.9638242894056848,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9664082687338501,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9689922480620154,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9715762273901809,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9741602067183462,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767441860465116,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979328165374677,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819121447028424,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844961240310077,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870801033591732,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896640826873385,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922480620155039,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948320413436692,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974160206718347,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
