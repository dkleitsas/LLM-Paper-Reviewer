Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002044989775051125,"Toeplitz Neural Networks (TNNs) [1] are a recent impressive sequence model
2"
ABSTRACT,0.00408997955010225,"requiring O(n log n) computational complexity and O(n) relative positional en-
3"
ABSTRACT,0.006134969325153374,"coder (RPE) multi-layer perceptron (MLP) and decay bias calls. We aim to reduce
4"
ABSTRACT,0.0081799591002045,"both. We ﬁrst note that the RPE is a non symmetric positive deﬁnite kernel and
5"
ABSTRACT,0.010224948875255624,"the Toeplitz matrices are pseudo-Gram matrices. Further 1) the learned kernels
6"
ABSTRACT,0.012269938650306749,"display spiky behavior near the main diagonals with otherwise smooth behavior;
7"
ABSTRACT,0.014314928425357873,"2) the RPE MLP is slow. For bidirectional models, this motivates a sparse plus
8"
ABSTRACT,0.016359918200409,"low-rank Toeplitz matrix decomposition. For the sparse component’s action, we
9"
ABSTRACT,0.018404907975460124,"do a small 1D convolution. For the low rank component, we replace the RPE
10"
ABSTRACT,0.02044989775051125,"MLP with linear interpolation and use Structured Kernel Interpolation (SKI) [2]
11"
ABSTRACT,0.022494887525562373,"for O(n) complexity. For causal models, “fast” causal masking [3] negates SKI’s
12"
ABSTRACT,0.024539877300613498,"beneﬁts. Working in frequency domain, we avoid an explicit decay bias. To enforce
13"
ABSTRACT,0.026584867075664622,"causality, we represent the kernel via the real part of its frequency response using
14"
ABSTRACT,0.028629856850715747,"the RPE and compute the imaginary part via a Hilbert transform. This maintains
15"
ABSTRACT,0.03067484662576687,"O(n log n) complexity but achieves an absolute speedup. Modeling the frequency
16"
ABSTRACT,0.032719836400818,"response directly is also competitive for bidirectional training, using one fewer FFT.
17"
ABSTRACT,0.034764826175869123,"We improve on speed and sometimes score on the Long Range Arena (LRA) [4].
18"
ABSTRACT,0.03680981595092025,"(a) Long Range Arena (LRA).
(b) Pre-training speedups using Fourier Domain."
ABSTRACT,0.03885480572597137,"Figure 1: (a) In LRA, our approaches, SKI and FD-TNN are faster than TNNs for 1d tasks with strong
LRA scores. Bubble sizes denote training model memory. (b) Our approach, FD-TNN, achieves
substantial speed ups in iterations/sec for pre-training both causal and bidirectional models. Note that
we do not include SKI-TNN in this plot as it does not use an MLP based RPE."
INTRODUCTION,0.0408997955010225,"1
Introduction
19"
INTRODUCTION,0.04294478527607362,"Sequence modeling is important in natural language processing, where sentences are represented
20"
INTRODUCTION,0.044989775051124746,"as a sequence of tokens. Successful sequence modeling typically involves token and channel
21"
INTRODUCTION,0.04703476482617587,"mixing. Token mixing combines representations of different sequence parts, while channel mixing
22"
INTRODUCTION,0.049079754601226995,"combines the information across different dimensions of embedding vectors used to encode tokens.
23"
INTRODUCTION,0.05112474437627812,"Transformers [5] are arguably the most successful technique for sequence modeling, and variants
24"
INTRODUCTION,0.053169734151329244,"including [6, 7] have achieved state of the art performance on natural language tasks. They use
25"
INTRODUCTION,0.05521472392638037,"self-attention for token mixing and feedforward networks for channel mixing.
26"
INTRODUCTION,0.05725971370143149,"Recently, [1] proposed Toeplitz Neural Networks (TNN) using Toeplitz matrices for token mixing.
27"
INTRODUCTION,0.05930470347648262,"They use a learned neural similarity function, the Relative Positional Encoder (RPE), to form
28"
INTRODUCTION,0.06134969325153374,"the Toeplitz matrices. Toeplitz matrix vector multiplication can be performed with sub-quadratic
29"
INTRODUCTION,0.06339468302658487,"complexity using the Fast Fourier Transform (FFT), giving the TNN token mixing layer a total
30"
INTRODUCTION,0.065439672801636,"O(dn log n) computational complexity, where d is the embedding dimension and n is the sequence
31"
INTRODUCTION,0.06748466257668712,"length. This achieved state of the art predictive performance and nearly state of the art speed for
32"
INTRODUCTION,0.06952965235173825,"the long range arena (LRA) benchmark [4]. They also showed strong performance pre-training
33"
INTRODUCTION,0.07157464212678936,"wikitext-103 [8] and on the GLUE benchmark[9]. Despite strong empirical speed performance,
34"
INTRODUCTION,0.0736196319018405,"TNNs have two fundamental efﬁciency limitations: 1) super-linear computational complexity 2)
35"
INTRODUCTION,0.07566462167689161,"many calls to the RPE: for each layer, one call per relative position.
36"
INTRODUCTION,0.07770961145194274,"In this paper, we interpret the RPE as a non-SPD kernel and note 1) the learned kernels are discontin-
37"
INTRODUCTION,0.07975460122699386,"uous near the main diagonals but otherwise smooth globally; 2) the ReLU RPE learns 1D piecewise
38"
INTRODUCTION,0.081799591002045,"linear functions: an MLP is slower than necessary. For bidirectional models, this motivates a sparse
39"
INTRODUCTION,0.08384458077709611,"plus low-rank decomposition. We apply the sparse component’s action via a small 1D convolution.
40"
INTRODUCTION,0.08588957055214724,"For the low rank component, we replace the RPE MLP with linear interpolation at a set of inducing
41"
INTRODUCTION,0.08793456032719836,"points and an asymmetric extension of Structured Kernel Interpolation (SKI) [2] for O(n) complexity.
42"
INTRODUCTION,0.08997955010224949,"Further, using an inverse time warp, we can extrapolate beyond sequence lengths observed during
43"
INTRODUCTION,0.09202453987730061,"training. For causal models, even “fast” causal masking [3] negates the speed and memory beneﬁts
44"
INTRODUCTION,0.09406952965235174,"from SKI. Thus, we instead represent the real part of the kernel’s frequency response using the RPE
45"
INTRODUCTION,0.09611451942740286,"MLP, and evaluate the RPE with ﬁner frequency resolution to extrapolate to longer sequence lengths
46"
INTRODUCTION,0.09815950920245399,"in the time domain. From the real part, we compute the imaginary part via a Hilbert transform during
47"
INTRODUCTION,0.10020449897750511,"the forward pass to enforce causality. In the bidirectional setting, we remove the causality constraint
48"
INTRODUCTION,0.10224948875255624,"and represent the complex frequency response of the kernel with the RPE MLP. Levels of smoothness
49"
INTRODUCTION,0.10429447852760736,"in frequency response imply decay rates in the time domain: thus we model the decay bias implicitly.
50"
INTRODUCTION,0.10633946830265849,"This maintains O(n log n) complexity but achieves an absolute speedup. Further, it often leads to
51"
INTRODUCTION,0.1083844580777096,"better predictive performance on LRA tasks.
52"
INTRODUCTION,0.11042944785276074,"This paper has three primary contributions: 1) a TNN sparse plus low rank decomposition, extending
53"
INTRODUCTION,0.11247443762781185,"SKI to TNNs for the low rank part. We replace the RPE MLP with linear interpolation and apply
54"
INTRODUCTION,0.11451942740286299,"inverse time warping to efﬁciently train bidirectional TNNs. We provide rigorous error analysis
55"
INTRODUCTION,0.1165644171779141,"for our asymmetric SKI application; 2) alternatively, for both causal and bidirectional models, we
56"
INTRODUCTION,0.11860940695296524,"work directly in the frequency domain and use the Hilbert transform to enforce causality in the
57"
INTRODUCTION,0.12065439672801637,"autoregressive setting. We prove that different activation choices for an MLP modeling the discrete
58"
INTRODUCTION,0.12269938650306748,"time Fourier transform (DTFT) lead to different decay rates in the original kernel. 3) Empirical
59"
INTRODUCTION,0.12474437627811862,"results: we demonstrate that our approaches show dramatically improved computational efﬁciency,
60"
INTRODUCTION,0.12678936605316973,"setting a new speed state of the art on LRA [10] on the 1d tasks, with strong LRA score. In section
61"
INTRODUCTION,0.12883435582822086,"2 we describe related work. In section 3 we propose our new modeling approaches. In 4 we state
62"
INTRODUCTION,0.130879345603272,"several theoretical results regarding our modeling approaches. In 5 we extend the empirical results of
63"
INTRODUCTION,0.1329243353783231,"[1], showing our speed gains with minimal prediction deterioration. We conclude in section 6.
64"
RELATED,0.13496932515337423,"2
Related
65"
RELATED,0.13701431492842536,"The most related papers use Toeplitz matrices for sequence modeling [1, 11, 12]. We build off of [1]
66"
RELATED,0.1390593047034765,"and introduce several techniques to improve on their speed results. [11] took a similar approach, but
67"
RELATED,0.1411042944785276,"applied Toeplitz matrices to self-attention rather than departing from it. [12] is also similar, using
68"
RELATED,0.14314928425357873,"alternating Toeplitz and diagonal matrices as a replacement for self-attention within a Transformer.
69"
RELATED,0.14519427402862986,"While we focus on the setting of [1] as it was released ﬁrst, our approach is applicable to [12].
70"
RELATED,0.147239263803681,"Also related are kernel based xFormers, particularly those using the Nyström method [13, 14]. The
71"
RELATED,0.1492842535787321,"most related work is [15], which adapts a matrix Nyström method for asymmetric matrices [16] to
72"
RELATED,0.15132924335378323,"self-attention. We instead adapt this along with SKI [2] to Toeplitz matrices. [17] extends [15] by
73"
RELATED,0.15337423312883436,"embedding the self-attention matrix into a larger PSD kernel matrix and approximating the larger
74"
RELATED,0.1554192229038855,"matrix instead. Their ﬁnal approximate matrix has lower spectral error compared to [15] and higher
75"
RELATED,0.1574642126789366,"average validation accuracy on LRA [4]. However, their method is slightly slower. Also somewhat
76"
RELATED,0.15950920245398773,"related are random feature self-attention approximations[18, 19]. These extend [20], but use different
77"
RELATED,0.16155419222903886,"random features that better approximate self-attention than random Fourier or binning features.
78"
RELATED,0.16359918200409,"Sparse transformers are also relevant. [21] proposed using strided and ﬁxed patterns. [22] alternated
79"
RELATED,0.1656441717791411,"between sparse locally banded and dense attention. Finally, [23] proposed combining random
80"
RELATED,0.16768916155419222,"attention, window attention and global attention. Our use of a short convolutional ﬁlter is most similar
81"
RELATED,0.16973415132924335,"to window attention. The space of efﬁcient transformers is huge and there are many models that we
82"
RELATED,0.17177914110429449,"haven’t covered that may be relevant. [10] provides an excellent survey.
83"
RELATED,0.1738241308793456,"Other successful long sequence approaches include state space models [24, 25, 26], long convolution
84"
RELATED,0.17586912065439672,"[27, 28], adding moving averages to gated attention [29] and more [30].
85"
MODELING APPROACH,0.17791411042944785,"3
Modeling Approach
86"
MODELING APPROACH,0.17995910020449898,"We review Toeplitz neural networks (TNNs) in section 3.1. We next speed up the TNN’s Toeplitz
87"
MODELING APPROACH,0.18200408997955012,"neural operator (TNO). We discuss using Nyström and SKI approaches to bidirectional training in
88"
MODELING APPROACH,0.18404907975460122,"3.2. We discuss frequency based approaches, particularly for causal training in 3.3.
89"
MODELING APPROACH,0.18609406952965235,"3.1
Preliminaries: Toeplitz matrices and Toeplitz Neural Networks
90"
MODELING APPROACH,0.18813905930470348,"TNNs [1] replace self-attention, which computes the action of self-attention matrices that encode
91"
MODELING APPROACH,0.1901840490797546,"the similarity between both observation values and absolute positions, with the action of Toeplitz
92"
MODELING APPROACH,0.19222903885480572,"matrices that encode similarity only based on relative positions. Toeplitz matrices have, for each
93"
MODELING APPROACH,0.19427402862985685,"diagonal, the same entries from left to right. That is, Tij = ti−j, T 2 Rn⇥n. Unlike self-attention
94"
MODELING APPROACH,0.19631901840490798,"matrices, which require O(n2) memory, a Toeplitz matrix has 2n −1 unique elements and requires
95"
MODELING APPROACH,0.1983640081799591,"O(n) memory. Due to close connections with discrete-time convolution, Tx can be computed in
96"
MODELING APPROACH,0.20040899795501022,"O(n log n) time by embedding T in a circulant matrix and applying FFT.
97"
MODELING APPROACH,0.20245398773006135,"A TNN [1] has multiple sequence modeling blocks, which we show in Figure 3 in Appendix A. Each
98"
MODELING APPROACH,0.20449897750511248,"block has a Gated Toeplitz Unit (GTU), which does both token and channel mixing, followed by a
99"
MODELING APPROACH,0.2065439672801636,"Gated Linear Unit (GLU) [31], which does channel mixing. The core of the GTU is the Toeplitz
100"
MODELING APPROACH,0.2085889570552147,"Neural Operator (TNO), which does token mixing and is the part of the architecture that we modify.
101"
MODELING APPROACH,0.21063394683026584,"We now describe the TNO, shown in Figure 3b of Appendix A. Given a sequence X 2 Rn⇥d of
102"
MODELING APPROACH,0.21267893660531698,"length n and dimension d in discrete time, there are 2n −1 unique relative positions/times i −j for
103"
MODELING APPROACH,0.2147239263803681,"i, j = 1, . . . , n. An RPE : Z ! Rd neural network maps each relative position to a d-dimensional
104"
MODELING APPROACH,0.2167689161554192,"embedding. These embeddings are used to construct Toeplitz matrices Tl for l = 1, . . . , d using
105 Tl"
MODELING APPROACH,0.21881390593047034,ij = λ|i−j|RPEl(i −j).
MODELING APPROACH,0.22085889570552147,"RPEl(i −j) is a learned similarity between positions for dimension l, while λ|i−j| with λ 2 (0, 1)
106"
MODELING APPROACH,0.2229038854805726,is an exponential decay bias penalizing far away tokens to be dissimilar. We can interpret Tl
MODELING APPROACH,0.2249488752556237,"ij as
107"
MODELING APPROACH,0.22699386503067484,"evaluating a stationary non-SPD kernel kl(i −j) = λ|i−j|RPEl(i −j). Thus Tl can be interpreted
108"
MODELING APPROACH,0.22903885480572597,"as a pseudo or generalized Gram matrix. Letting xl be the lth column of X, the TNO outputs
109"
MODELING APPROACH,0.2310838445807771,TNO(X) = (T1x1 . . . Tdxd) 2 Rn⇥d
MODELING APPROACH,0.2331288343558282,"where each Tlxl is computed via the FFT as described above.
110"
MODELING APPROACH,0.23517382413087934,"The main costs are the RPE’s MLP, the FFT, and the decay bias. We aim to eliminate the MLP and
111"
MODELING APPROACH,0.23721881390593047,"decay bias when possible. In the bidirectional setting, we use SKI to apply the FFT using a much
112"
MODELING APPROACH,0.2392638036809816,"smaller Toeplitz matrix. In a separate model we learn the RPE’s frequency response directly. In the
113"
MODELING APPROACH,0.24130879345603273,"bidirectional setting, this allows us to both avoid explicitly modeling the decay bias and use one fewer
114"
MODELING APPROACH,0.24335378323108384,"FFT. In the causal setting, it allows us to avoid explicitly modeling the decay bias.
115"
MODELING APPROACH,0.24539877300613497,SKI TNO (Concept)
MODELING APPROACH,0.2474437627811861,Smooth
MODELING APPROACH,0.24948875255623723,Output Input x d
MODELING APPROACH,0.25153374233128833,"Interpolate
RPE 2r-1"
MODELING APPROACH,0.25357873210633947,Toeplitz
MODELING APPROACH,0.2556237218813906,Sparse x d
MODELING APPROACH,0.25766871165644173,?m/2?+1
MODELING APPROACH,0.25971370143149286,Banded
MODELING APPROACH,0.261758691206544,(a) SKI-TNO.
MODELING APPROACH,0.26380368098159507,SKI TNO (Computation)
MODELING APPROACH,0.2658486707566462,Approx. smooth
MODELING APPROACH,0.26789366053169733,Output Input x d
MODELING APPROACH,0.26993865030674846,"Interpolate
RPE
(
)
, 2r-1"
MODELING APPROACH,0.2719836400817996,"Toeplitz (A)
Sparse (W) Conv. x d m *"
MODELING APPROACH,0.2740286298568507,"(b) Fast implementation of
SKI-TNO."
MODELING APPROACH,0.27607361963190186,Output Input x d
MODELING APPROACH,0.278118609406953,"MLP
RPE FFT IFFT"
MODELING APPROACH,0.28016359918200406,".
Hilbert Transform"
MODELING APPROACH,0.2822085889570552,TNO FD (Causal) 2n-1
MODELING APPROACH,0.2842535787321063,(c) FD-TNO causal.
MODELING APPROACH,0.28629856850715746,TNO FD
MODELING APPROACH,0.2883435582822086,Output Input x d
MODELING APPROACH,0.2903885480572597,"MLP
RPE FFT IFFT ."
MODELING APPROACH,0.29243353783231085,Freq. responses 2n-1
MODELING APPROACH,0.294478527607362,"(d) FD-TNO bidirec-
tional."
MODELING APPROACH,0.2965235173824131,"Figure 2: Our SKI-TNO and FD-TNO modiﬁcations: (a) We decompose Toeplitz matrices into
sums of sparse + smooth components. Additionally, we use interpolation instead of an MLP to
learn the RPE. (b) We use a 1D convolution to apply the sparse component and SKI as a low-rank
approximation to the smooth component. (c) For the causal case, we use frequency domain RPE with
a Hilbert Transform to enforce causality. (d) Our FD-TNO also is competitive in the bidirectional
case, with one fewer FFT than TNO."
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.2985685071574642,"3.2
SKI Based Approaches for Bidirectional Training
116"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3006134969325153,"For a given Toeplitz matrix T, we assume it admits a decomposition that we can approximate with a
117"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.30265848670756645,"sparse+low-rank representation, T = Tsparse + Tsmooth ⇡Tsparse + Tlow. Our bidirectional training
118"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3047034764826176,"thus consists of three primary components. The ﬁrst, the sparse component Tsparse is straightforward.
119"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3067484662576687,"Applying the action Tsparsex of Tsparse 2 Rn⇥n with m non-zero diagonals is equivalent to applying
120"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.30879345603271985,"a 1D convolution layer with ﬁlter size m. We then discuss our asymmetric SKI for Tlow in section
121"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.310838445807771,"3.2.1. Finally, we discuss how we handle sequence lengths not observed in training for Tlow via an
122"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3128834355828221,"inverse time warp in section 3.2.2. Algorithm 1 summarizes our TNO based on these techniques.
123"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3149284253578732,Algorithm 1 Sparse Plus Low Rank Bidirectional TNO with Asymmetric SKI
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3169734151329243,"Given sequence X 2 Rn⇥d with columns xl
Hyperparameters rank r ⌧n, sparse ﬁlter size m, interpolation degree N, decay parameter λ
Compute inducing points p1, . . . , pr evenly spaced on [0, n]
for l = 1, . . . , d do"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.31901840490797545,Compute Tl
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3210633946830266,"sparsexl with a 1D convolutional ﬁlter, size m.
Let x(t) = sign(t)λ|t|.
Form Al 2 Rr⇥r with entries Al"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3231083844580777,"ij = kl(pi −pj) = RPEl(x(pi −pj))
Form Wl 2 Rn⇥r degree N polynomial interpolation matrix
Compute Tl"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.32515337423312884,lowxl with Tl
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.32719836400818,"low = WlAlWl>
end for
Return TNO(X) = (T1"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3292433537832311,sparsex1 + T1
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3312883435582822,"lowx1, . . . , Td"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3333333333333333,sparsexd + Td
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.33537832310838445,lowxd)
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3374233128834356,"3.2.1
SKI For Asymmetric Nyström
124"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3394683026584867,"Given an asymmetric stationary kernel k : R ⇥R ! R, we wish to approximate the (pseudo) Gram
125"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.34151329243353784,"matrix T 2 Rn⇥n using a low-rank approximation based on a smaller Gram matrix A 2 Rr⇥r, with
126"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.34355828220858897,"r ⌧n. In context, A is formed using relative positions between a set of inducing points p1, . . . , pr
127"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3456032719836401,"instead of the full set 1, . . . , n that is used for T. That is,
128"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3476482617586912,"Tij = k(i −j)
and
Aij = k(pi −pj)."
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3496932515337423,"In our case, the inducing points are uniformly spaced. Some submatrices of A may be submatrices of
129"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.35173824130879344,"T (if inducing points are also observation points). To derive the Nyström approximation, we form an
130"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3537832310838446,"augmented Gram matrix K 2 R(n+r)⇥(n+r) in block form as
131 K = ✓"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3558282208588957,"A
B
F
T ◆ ,"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.35787321063394684,"where B 2 Rr⇥n and F 2 Rn⇥r are respectively the upper right and lower left partitions of the large
132"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.35991820040899797,"Gram matrix K. Explicitly,
133"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3619631901840491,"Bij = k(pi −j)
and
Fij = k(i −pj)."
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.36400817995910023,"Extending [16] to allow singular A,
134 bK = ✓ A F ◆"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3660531697341513,"A† (A
B) = ✓"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.36809815950920244,"A
AA†B
FA†A
FA†B ◆"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.37014314928425357,"where A† is the Moore-Penrose pseudo-inverse satisfying AA†A = A (but not necessarily AA† = I
135"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3721881390593047,"as in [16], which shows up in our different expressions for off-diagonal blocks of bK). Following
136"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.37423312883435583,"structured kernel interpolation (SKI) [2], we approximate F and B using interpolation. Speciﬁcally,
137"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.37627811860940696,"F ⇡WA
and
B ⇡AW>"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3783231083844581,"where W 2 Rn⇥r is a matrix of sparse interpolation weights with up to two non-zero entries per row
138"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3803680981595092,"for linear interpolation or up to four for cubic. These weights can be computed in closed form from
139"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3824130879345603,"the inducing points pi and the observation points i. Thus we have
140"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.38445807770961143,T ⇡FA†B ⇡WAA†AW> = WAW>
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.38650306748466257,) eT = WAW>
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3885480572597137,"as desired. We can set Tlow = ˜T and compute ˜Tx by ﬁrst applying W>x, which is an O(n)
141"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.39059304703476483,"operation due to W 2 Rn⇥r having sparse rows. Next, we apply A(W>x). Since A is a Toeplitz
142"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.39263803680981596,"matrix, this is O(r log r) as per Section 3.1. Finally, W(AW>x), the action of W, is again an O(n)
143"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3946830265848671,"operation. Thus computing ˜Tx is O(n + r log r) computation. On a GPU, this factorization achieves
144"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3967280163599182,"a speedup from having small r and being able to leverage efﬁcient parallelized matrix multiplication
145"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.3987730061349693,"on specialized hardware. However, in PyTorch [32], we note that for medium sized matrices up to
146"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.40081799591002043,"n = 512, the time required for data movement in order to perform sparse-dense matrix multiplications
147"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.40286298568507156,"can be higher than that of simply performing dense matrix multiplication. This means that in practice,
148"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.4049079754601227,"we may instead choose to perform batched dense matrix multiplication, which yields an absolute
149"
SKI BASED APPROACHES FOR BIDIRECTIONAL TRAINING,0.4069529652351738,"speedup but a worse asymptotic complexity of O(nr2 + r log r).
150"
INVERSE TIME WARP,0.40899795501022496,"3.2.2
Inverse Time Warp
151"
INVERSE TIME WARP,0.4110429447852761,"TNNs use kl(i −j) = λ|i−j|RPEl(i −j), where RPEl(i −j) is an MLP. There are two issues: 1) the
152"
INVERSE TIME WARP,0.4130879345603272,"sequential computations required for an MLP are slow, and we only need to evaluate at 2r −1 points
153"
INVERSE TIME WARP,0.41513292433537835,"using SKI instead of 2n −1 to produce the full matrix; 2) extrapolation is used in extending to longer
154"
INVERSE TIME WARP,0.4171779141104294,"sequence lengths than the MLP was trained on, which is generally less reliable than interpolation.
155"
INVERSE TIME WARP,0.41922290388548056,"In Proposition 1, we note that an MLP f : R ! Rd with ReLU activations and layer normalization is
156"
INVERSE TIME WARP,0.4212678936605317,"d piecewise linear functions. As we only need to evaluate at 2r −1 points, we could let RPEl be a
157"
INVERSE TIME WARP,0.4233128834355828,"piecewise linear function with r grid points. However, we still need to handle extrapolation. We use
158"
INVERSE TIME WARP,0.42535787321063395,"an inverse time warp and let RPEl linearly interpolate on [−1, 1] with the constraint RPEl(0) = 0
159"
INVERSE TIME WARP,0.4274028629856851,"and deﬁne x(t) = sign(t)λ|t| for some 0 < λ < 1. We then let kl(i −j) = RPEl(x(i −j)).
160"
FREQUENCY BASED APPROACHES,0.4294478527607362,"3.3
Frequency Based Approaches
161"
CAUSAL TRAINING,0.43149284253578735,"3.3.1
Causal Training
162"
CAUSAL TRAINING,0.4335378323108384,"The SKI approach allows training bidirectional TNNs with linear complexity. However, fast causal
163"
CAUSAL TRAINING,0.43558282208588955,"masking negates SKI’s beneﬁts (see Appendix B). Thus we need an alternate causal speedup. We
164"
CAUSAL TRAINING,0.4376278118609407,"use an MLP in the Fourier domain to avoid an explicit time domain decay bias, and use the Hilbert
165"
CAUSAL TRAINING,0.4396728016359918,"transform to enforce causality. We now describe how we can learn a causal kernel when working in
166"
CAUSAL TRAINING,0.44171779141104295,"frequency domain (FD). We ﬁrst deﬁne the discrete Hilbert transform, the key tool for achieving this.
167"
CAUSAL TRAINING,0.4437627811860941,"Deﬁnition 1. The discrete Hilbert transform of the discrete Fourier transform ˆk is given by
168"
CAUSAL TRAINING,0.4458077709611452,H{ˆk} = ˆk ⇤h
CAUSAL TRAINING,0.44785276073619634,"where ⇤denotes convolution and
169"
CAUSAL TRAINING,0.4498977505112474,h[l] =
CAUSAL TRAINING,0.45194274028629855,"⇢0, l even"
CAUSAL TRAINING,0.4539877300613497,"2
⇡l, l odd"
CAUSAL TRAINING,0.4560327198364008,"The real and imaginary parts of the Fourier transform of a causal function are related to each other
170"
CAUSAL TRAINING,0.45807770961145194,"through the Hilbert transform. Thus, in order to represent a causal signal, we can model only the real
171"
CAUSAL TRAINING,0.4601226993865031,"part and compute the corresponding imaginary part. That is, we ﬁrst estimate an even real function ˆk
172"
CAUSAL TRAINING,0.4621676891615542,"(symmetric about 0) using an MLP. We then take ˆkcausal(!) = ˆk(!) −iH{ˆk}(!).
173"
CAUSAL TRAINING,0.46421267893660534,"The inverse Fourier transform kcausal of ˆkcausal will thus be causal. For a discussion of why this ensures
174"
CAUSAL TRAINING,0.4662576687116564,"causality, see [33]. See Algorithm 2 for TNO pseudocode using this approach. Different choices for
175"
CAUSAL TRAINING,0.46830265848670755,"the smoothness of the frequency domain MLP will lead to different decay rates in time domain, so
176"
CAUSAL TRAINING,0.4703476482617587,"that smoothness in frequency domain essentially serves the same purpose as the decay bias in [1]. We
177"
CAUSAL TRAINING,0.4723926380368098,"discuss this theoretically in Section 4.2. Note that we also ﬁnd that working directly in the frequency
178"
CAUSAL TRAINING,0.47443762781186094,"domain for bidirectional models (without the Hilbert transform) is often competitive with SKI for
179"
CAUSAL TRAINING,0.47648261758691207,"speed (despite being O(n log n) instead of O(n + r log r)) due to needing one fewer FFT.
180"
CAUSAL TRAINING,0.4785276073619632,Algorithm 2 Causal TNO via Discrete Hilbert Transform
CAUSAL TRAINING,0.48057259713701433,"Given sequence X 2 Rn⇥d with columns xl
Hyperparameters activation function
for l = 1, . . . , d do"
CAUSAL TRAINING,0.48261758691206547,"ˆxl  F{xl}, where F is the rFFT.
Compute even real function ˆkl = RPEl(!), ! = m⇡"
CAUSAL TRAINING,0.48466257668711654,"n , m = 0, . . . , n.
Take discrete Hilbert transform H{ˆkl} via the rFFT and irFFT.
Compute ˆkl"
CAUSAL TRAINING,0.4867075664621677,causal(!) = ˆkl(!) −iH{ˆkl}(!) for ! = m⇡
CAUSAL TRAINING,0.4887525562372188,"n , m = 0, . . . , n.
yl  F−1{ˆkl"
CAUSAL TRAINING,0.49079754601226994,"causal ⊙ˆxl}, where F−1 is the irFFT and ⊙denotes an element-wise product.
end for
Return TNO(X) = (y1, . . . , yd)"
BIDIRECTIONAL TRAINING WITH FD TNN,0.49284253578732107,"3.3.2
Bidirectional Training with FD TNN
181"
BIDIRECTIONAL TRAINING WITH FD TNN,0.4948875255623722,"We extend the FD approach to bidirectional training by removing the causality constraint and model
182"
BIDIRECTIONAL TRAINING WITH FD TNN,0.49693251533742333,"the complex frequency response of real valued time domain kernels directly. To do so we simply
183"
BIDIRECTIONAL TRAINING WITH FD TNN,0.49897750511247446,"double the output width of the RPE and allocate each half for the real and imaginary parts of the
184"
BIDIRECTIONAL TRAINING WITH FD TNN,0.5010224948875256,"kernel frequency responses, while explicitly forcing real-valued responses at ! = 0 and ⇡. While
185"
BIDIRECTIONAL TRAINING WITH FD TNN,0.5030674846625767,"increasing the complexity of the RPE slightly, we achieve the speed ups in Figure 1 by eliminating
186"
BIDIRECTIONAL TRAINING WITH FD TNN,0.5051124744376279,"the FFTs for the kernels and causality constraint, in addition to the decay bias.
187"
THEORY,0.5071574642126789,"4
Theory
188"
THEORY,0.50920245398773,"We show in Proposition 1 that an MLP mapping from scalars with layer norm and ReLU activations
189"
THEORY,0.5112474437627812,"is piecewise linear and continuous, suggesting that using an MLP that we only need to evaluate at a
190"
THEORY,0.5132924335378323,"small number of points may be overparametrized, justifying the use of interpolated piecewise linear
191"
THEORY,0.5153374233128835,"functions. In section 4.1 we analyze the spectral norm of the matrix approximation error for SKI. We
192"
THEORY,0.5173824130879345,"assume the sparse component is exactly identiﬁable and bound the error of approximating the smooth
193"
THEORY,0.5194274028629857,"term via a low-rank SKI factorization. We leave the problem of relaxing this assumption to future
194"
THEORY,0.5214723926380368,"work. In section 4.2, we analyze how by using different activations with different smoothness when
195"
THEORY,0.523517382413088,"learning the DTFT of the kernel, we obtain corresponding decay rates for the time domain signal.
196"
THEORY,0.5255623721881391,"Proposition 1. A ReLU MLP f : R ! Rd with layer norm and no activation on its output is d
197"
THEORY,0.5276073619631901,"piecewise linear continuous functions.
198"
THEORY,0.5296523517382413,"Proof. See Appendix C.
199"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5316973415132924,"4.1
Matrix Approximation Spectral Norm Error
200"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5337423312883436,"We give our main error bound for our SKI based low rank approximation. Note that this requires
201"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5357873210633947,"that our kernel is N + 1 times continuously differentiable, while the kernel we use in practice uses a
202"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5378323108384458,"piecewise linear function and is thus non-differentiable. In theory, we would need a smoother kernel,
203"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5398773006134969,"adding additional computation overhead. However, we ﬁnd that empirical performance is still strong
204"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5419222903885481,"and thus we simply use piecewise linear kernels but include the error bound for completeness. Our
205"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5439672801635992,"results depends on the Nyström error Enyst: its l2 norm is bounded in [16].
206"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5460122699386503,"Theorem 1. Assume that A is non-singular and k : [p1, pr] ! R is an N + 1 times continuously
207"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5480572597137015,"differentiable function, where p1 is the smallest inducing point and pr is the largest. Let Tr,opt be
208"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5501022494887525,"the optimal rank r approximation to T and let
209"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5521472392638037,"ESKI = WAW> −Tr,opt"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5541922290388548,"be the difference between the SKI approximation using linear interpolation and the optimal one, while
210"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.556237218813906,"Enyst = FA−1B −Tr,opt"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.558282208588957,"is the difference between the Nyström approximation and the optimal one. Then
211"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5603271983640081,"kESKIk2 pnr
max
pn1ipnN"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5623721881390593,"| N(i)|
(N + 1)!L ✓"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5644171779141104,"(N + 1)pn + min(σ1(F), σ1(B)) σr(A) ◆"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5664621676891616,+ kEnystk2.
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5685071574642127,where  N(i) = QN
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5705521472392638,"j=1(i −pnj) with pnj being the jth closest inducing point to i, L is an upper
212"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5725971370143149,"bound on the N + 1th derivative of k, and σi(M) denotes the ith largest singular value of matrix M.
213"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5746421267893661,"Proof. See Appendix D.1.
214"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5766871165644172,For linear interpolation | N(i)|
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5787321063394683,(N+1)! h2
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5807770961145194,"8 , where h is the spacing between two neighboring inducing
215"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5828220858895705,"points. We have considered the sparse component of the Toeplitz matrix to be identiﬁable and focused
216"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5848670756646217,"on the error of approximating the smooth component. While there are potential approaches to relaxing
217"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.5869120654396728,"this assumption [34, 35, 36, 37, 38, 39, 40], they must be adapted properly to the Toeplitz setting.
218"
MATRIX APPROXIMATION SPECTRAL NORM ERROR,0.588957055214724,"Thus, this additional analysis is outside the scope of this paper and a fruitful direction for future work.
219"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.591002044989775,"4.2
Smoothness in Fourier Domain Implies Decay in Time Domain
220"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.5930470347648262,"We now discuss activation function choices when directly learning the discrete time Fourier transform
221"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.5950920245398773,"(DTFT) ˆk as an MLP. In practice, we sample the DTFT to obtain the actually computable discrete
222"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.5971370143149284,"Fourier transform (DFT) by evaluating the MLP with uniform spacing. Different levels of smoothness
223"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.5991820040899796,"of the MLP ˆk imply different decay rates of the signal k. One can think of the choice of activation
224"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.6012269938650306,"function as a parametric form for the decay bias. For an MLP, using a GeLU activation implies
225"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.6032719836400818,"super-exponential time domain decay. Using SiLU implies super-polynomial time domain decay. For
226"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.6053169734151329,"ReLU the signal is square summable. While this subsection focuses on the theoretical relationship
227"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.6073619631901841,"between smoothness and decay, in Appendix E.3 we show visualizations demonstrating that these
228"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.6094069529652352,"relationships are observed in practice. We ﬁrst deﬁne the DTFT and its inverse.
229"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.6114519427402862,"Deﬁnition 2. The discrete time Fourier transform [41, 33] ˆk or F{k} of k is given by
230"
SMOOTHNESS IN FOURIER DOMAIN IMPLIES DECAY IN TIME DOMAIN,0.6134969325153374,ˆk(!) ⌘
X,0.6155419222903885,"1
X m=−1"
X,0.6175869120654397,k[m] exp(−i!m)
X,0.6196319018404908,"Deﬁnition 3. The inverse discrete time Fourier transform of the DTFT ˆk is given by
231"
X,0.621676891615542,F−1{ˆk}[n] ⌘1 2⇡ Z ⇡ −⇡
X,0.623721881390593,ˆk(!) exp(i!n)d!
X,0.6257668711656442,"We now give three theorems relating smoothness of the DTFT to decay of the signal (its inverse).
232"
X,0.6278118609406953,"Theorem 2. Using a GeLU MLP for the DTFT ˆk, for all a > 0, the signal k[n] will have decay
233"
X,0.6298568507157464,k[n] = O(exp(−an)).
X,0.6319018404907976,"Proof. See Appendix E.1.
234"
X,0.6339468302658486,"Theorem 3. Using a SiLU MLP for the DTFT ˆk, the signal k[n] will have decay
235"
X,0.6359918200408998,"|k[n]| 
1
2⇡|n|N"
X,0.6380368098159509,))ˆk(N))) 1
X,0.6400817995910021,"for all n 6= 0, N 2 N.
236"
X,0.6421267893660532,"Proof. See Appendix E.2.
237"
X,0.6441717791411042,"Theorem 4. Using a ReLU MLP for the DTFT ˆk implies kkk2 < 1 (the signal is square summable).
238"
X,0.6462167689161554,"Proof. Note that ˆk 2 L2[−⇡, ⇡] since it is continuous. Then apply Parseval’s theorem.
239"
EXPERIMENTS,0.6482617586912065,"5
Experiments
240"
EXPERIMENTS,0.6503067484662577,"We perform experiments in two areas: pre-training a causal language model on Wikitext-103 [8] and
241"
EXPERIMENTS,0.6523517382413088,"training bidirectional models on Long-Range Arena. We start with the repositories of the TNN paper1
242"
EXPERIMENTS,0.65439672801636,"and use their training and hyper-parameter settings unless indicated otherwise. We use A100 and
243"
EXPERIMENTS,0.656441717791411,"V100s for training, and a single A100 for timing experiments.
244"
EXPERIMENTS,0.6584867075664622,"5.1
Pre-training on Wikitext-103
245"
EXPERIMENTS,0.6605316973415133,"In the causal case we aim to predict the next token, conditional on a ﬁxed length sequence of previous
246"
EXPERIMENTS,0.6625766871165644,"tokens. Table 1 compares FD-TNN’s causal pre-training perplexity [8] to existing models: it almost
247"
EXPERIMENTS,0.6646216768916156,"exactly matches that of TNNs. Our approach is faster for the same capacity: at sequence length 512
248"
EXPERIMENTS,0.6666666666666666,"with 6 layer RPEs (as in the TNN paper), FD TNN is 15% faster than the baseline TNN on a single
249"
EXPERIMENTS,0.6687116564417178,"A100 GPU. When both use a three layer RPE, FD TNN is 10% faster. We provide some additional
250"
EXPERIMENTS,0.6707566462167689,"details for this experiment as well as for bidirectional pre-training (we see larger speed gains) in
251"
EXPERIMENTS,0.6728016359918201,"Appendix F.
252"
LONG-RANGE ARENA,0.6748466257668712,"5.2
Long-Range Arena
253"
LONG-RANGE ARENA,0.6768916155419223,"The Long-Range Arena (LRA) is a benchmark with several long sequence datasets. The goal is to
254"
LONG-RANGE ARENA,0.6789366053169734,"achieve both high LRA score (predictive performance) and training steps per second. Following [1],
255"
LONG-RANGE ARENA,0.6809815950920245,"we take the TNN architecture and their tuned hyperparameter (HP) conﬁgurations2, simply replacing
256"
LONG-RANGE ARENA,0.6830265848670757,"their TNO module with our SKI-TNO module with r = 64 and m = 32. We use λ = 0.99 where
257"
LONG-RANGE ARENA,0.6850715746421268,"they set λ = 1, but otherwise perform no additional HP tuning on 1D tasks and use smaller layers
258"
LONG-RANGE ARENA,0.6871165644171779,"r = 32 and m = 16 for the 2D tasks. For FD-TNN, we simply use a same-sized RPE for all tasks
259"
LONG-RANGE ARENA,0.689161554192229,"except a 3-layer RPE for the CIFAR task. We could potentially achieve even higher accuracy with
260"
LONG-RANGE ARENA,0.6912065439672802,"more comprehensive tuning on the 2D tasks or any tuning for the 1D tasks. We select the checkpoint
261"
LONG-RANGE ARENA,0.6932515337423313,"with the highest validation accuracy and report the corresponding test accuracy. SKI-TNN achieves
262"
LONG-RANGE ARENA,0.6952965235173824,"similar average accuracy than TNN at lower size, while FD-TNN achieves higher accuracy. We
263"
LONG-RANGE ARENA,0.6973415132924335,"suspect that for some of these problems, the square summable signal implied by ReLU in frequency
264"
LONG-RANGE ARENA,0.6993865030674846,"domain is a better parametric form than applying exponential decay bias. We show our results in
265"
LONG-RANGE ARENA,0.7014314928425358,"Table 2.
266"
LONG-RANGE ARENA,0.7034764826175869,"We additionally perform timing and memory proﬁling tests on a single 1x A100 instance, keeping
267"
LONG-RANGE ARENA,0.7055214723926381,"the per-GPU batch size constant as in the training runs. In Figure 1a, we plot for each 1D task the
268"
LONG-RANGE ARENA,0.7075664621676891,"percentage of TNN accuracy achieved vs the percentage speedup relative to TNN, with the size of
269"
LONG-RANGE ARENA,0.7096114519427403,"the marker corresponding to the peak memory usage measured. We highlight the 1D tasks because
270"
LONG-RANGE ARENA,0.7116564417177914,"they required no tuning, and they represent the longest sequences at lengths ranging from 1024 to
271"
LONG-RANGE ARENA,0.7137014314928425,"4096, whereas the 2D tasks are treated as separate 1D sequences in each dimension, so that a 32 ⇥32
272"
LONG-RANGE ARENA,0.7157464212678937,"image is seen as alternating length 32 sequences. We note that because the effective sequence lengths
273"
LONG-RANGE ARENA,0.7177914110429447,"are shorter, there is less beneﬁt from using our methods over the baseline TNN.
274"
LONG-RANGE ARENA,0.7198364008179959,"1https://github.com/OpenNLPLab/Tnn
2https://github.com/OpenNLPLab/lra"
LONG-RANGE ARENA,0.721881390593047,"Architecture
PPL (val)
PPL (test)
Params (m)
(Attn-based)
Trans
24.40
24.78
44.65
LS
23.56
24.05
47.89
Flash
25.92
26.70
42.17
1+elu
27.44
28.05
44.65
Performer
62.50
63.16
44.65
Cosformer
26.53
27.06
44.65
(MLP-based)
Syn(D)
31.31
32.43
46.75
Syn(R)
33.68
34.78
44.65
gMLP
28.08
29.13
47.83
(SS-based)
S4
38.34
39.66
45.69
DSS
39.39
41.07
45.73
GSS
29.61
30.74
43.84
(TNN-based)
TNN (reproduced, 3 layers)
23.98 (23.96)
24.67 (24.61)
48.68 (48.59)
FD-TNN: Ours, 3 layers
23.97
24.56
48.58
Table 1: Performance on Wikitext-103, Causal Language Model. We reproduce [1]’s table except
for the bottom two rows corresponding to the baseline TNN and our FD-TNN. For both we use the
same RPE conﬁg with 3 layers. We add in parenthesis the baseline TNN results that we reproduced.
We have nearly the same perplexity as the baseline TNN. Our approach is faster: at sequence length
512 with a six layer RPE (as in the TNN paper), FD TNN is 15% faster than the baseline TNN. For a
three layer RPE, it is 10% faster."
LONG-RANGE ARENA,0.7239263803680982,"Architecture
Text
ListOps
Retrieval
Pathﬁnder
Image
Avg
TNN
86.39
47.33
89.40
73.89
77.84
74.97
SKI-TNN
83.19
45.31
88.73
68.30
76.46
72.40
FD-TNN
85.00
55.21
90.26
69.45
84.12
76.81
Table 2: Performance on Long Range Arena. We reproduce experiments and train our proposed
variants using tuned hyperparameters from [1]. We bold the best and underline the second in each
task. Our proposed SKI-TNN and FD-TNN achieve similar overall performance with no additional
hyperparameter tuning on 1D LRA tasks and a minimal amount of tuning on 2D tasks."
CONCLUSION,0.7259713701431493,"6
Conclusion
275"
CONCLUSION,0.7280163599182005,"In this paper, we note that [1]’s Toeplitz neural networks essentially apply the action of a generalized
276"
CONCLUSION,0.7300613496932515,"Gram matrix (the Toeplitz matrix) for an asymmetric kernel (the RPE times decay bias) as their main
277"
CONCLUSION,0.7321063394683026,"computationally expensive operation. The visualized learned Gram matrices motivate a sparse and
278"
CONCLUSION,0.7341513292433538,"low rank decomposition. We thus propose two different approaches to improve efﬁciency. In the
279"
CONCLUSION,0.7361963190184049,"bidirectional setting, we extend SKI to the asymmetric setting and use linear interpolation over a
280"
CONCLUSION,0.7382413087934561,"small set of inducing points to avoid the MLP entirely, while using an inverse time warp to handle
281"
CONCLUSION,0.7402862985685071,"extrapolation to time points not observed during training. This approach reduces the mathematical
282"
CONCLUSION,0.7423312883435583,"complexity from O(n log n) to O(n + r log r), where r is the number of inducing points. However
283"
CONCLUSION,0.7443762781186094,"in practice, we do not actually use O(n + r log r) code due to a reshape required for sparse tensors
284"
CONCLUSION,0.7464212678936605,"leading to them actually being slower than dense tensors. Thus we actually use O(nr2 + r log r) in
285"
CONCLUSION,0.7484662576687117,"code: still much faster than Baseline TNN for small r. For causal training, as causal masking negates
286"
CONCLUSION,0.7505112474437627,"SKI’s beneﬁts, we instead eliminate the explicit decay bias. We do this by working directly in the
287"
CONCLUSION,0.7525562372188139,"frequency domain, enforcing causality via the Hilbert transform and enforcing decay in time domain
288"
CONCLUSION,0.754601226993865,"via smoothness. For the bidirectional case, we eliminate the FFT applied to the kernels. While this
289"
CONCLUSION,0.7566462167689162,"maintains O(n log n) computational complexity, it leads to a substantial speedup in practice and
290"
CONCLUSION,0.7586912065439673,"beats TNNs on LRA score.
291"
REFERENCES,0.7607361963190185,"References
292"
REFERENCES,0.7627811860940695,"[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng
293"
REFERENCES,0.7648261758691206,"Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh
294"
REFERENCES,0.7668711656441718,"International Conference on Learning Representations, 2023.
295"
REFERENCES,0.7689161554192229,"[2] Andrew Wilson and Hannes Nickisch. Kernel interpolation for scalable structured Gaussian
296"
REFERENCES,0.7709611451942741,"processes (KISS-GP). In International conference on machine learning, pages 1775–1784.
297"
REFERENCES,0.7730061349693251,"PMLR, 2015.
298"
REFERENCES,0.7750511247443763,"[3] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers
299"
REFERENCES,0.7770961145194274,"are RNNs: Fast autoregressive transformers with linear attention. In International Conference
300"
REFERENCES,0.7791411042944786,"on Machine Learning, pages 5156–5165. PMLR, 2020.
301"
REFERENCES,0.7811860940695297,"[4] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
302"
REFERENCES,0.7832310838445807,"Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A Benchmark for Efﬁcient
303"
REFERENCES,0.7852760736196319,"Transformers. In International Conference on Learning Representations, 2020.
304"
REFERENCES,0.787321063394683,"[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
305"
REFERENCES,0.7893660531697342,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
306"
REFERENCES,0.7914110429447853,"processing systems, 30, 2017.
307"
REFERENCES,0.7934560327198364,"[6] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
308"
REFERENCES,0.7955010224948875,"Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
309"
REFERENCES,0.7975460122699386,"Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia
310"
REFERENCES,0.7995910020449898,"Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and
311"
REFERENCES,0.8016359918200409,"Laurent Sifre. An empirical analysis of compute-optimal large language model training. In
312"
REFERENCES,0.803680981595092,"Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in
313"
REFERENCES,0.8057259713701431,"Neural Information Processing Systems, 2022.
314"
REFERENCES,0.8077709611451943,"[7] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-
315"
REFERENCES,0.8098159509202454,"training text encoders as discriminators rather than generators. In International Conference on
316"
REFERENCES,0.8118609406952966,"Learning Representations, 2020.
317"
REFERENCES,0.8139059304703476,"[8] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel Mixture
318"
REFERENCES,0.8159509202453987,"Models. In International Conference on Learning Representations, 2016.
319"
REFERENCES,0.8179959100204499,"[9] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
320"
REFERENCES,0.820040899795501,"Glue: A multi-task benchmark and analysis platform for natural language understanding. In
321"
REFERENCES,0.8220858895705522,"International Conference on Learning Representations.
322"
REFERENCES,0.8241308793456033,"[10] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey.
323"
REFERENCES,0.8261758691206544,"ACM Computing Surveys, 55(6):1–28, 2022.
324"
REFERENCES,0.8282208588957055,"[11] Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei
325"
REFERENCES,0.8302658486707567,"Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional
326"
REFERENCES,0.8323108384458078,"encoding. Advances in Neural Information Processing Systems, 34:22795–22807, 2021.
327"
REFERENCES,0.8343558282208589,"[12] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
328"
REFERENCES,0.83640081799591,"Bengio, Stefano Ermon, and Christopher Ré. Hyena Hierarchy: Towards Larger Convolutional
329"
REFERENCES,0.8384458077709611,"Language Models. arXiv preprint arXiv:2302.10866, 2023.
330"
REFERENCES,0.8404907975460123,"[13] Evert J Nyström. Über die praktische auﬂösung von integralgleichungen mit anwendungen auf
331"
REFERENCES,0.8425357873210634,"randwertaufgaben. Acta Mathematica, 54(1):185–204, 1930.
332"
REFERENCES,0.8445807770961146,"[14] Christopher TH Baker. The numerical treatment of integral equations. Oxford University Press,
333"
REFERENCES,0.8466257668711656,"1977.
334"
REFERENCES,0.8486707566462167,"[15] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin
335"
REFERENCES,0.8507157464212679,"Li, and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-
336"
REFERENCES,0.852760736196319,"attention. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages
337"
REFERENCES,0.8548057259713702,"14138–14148, 2021.
338"
REFERENCES,0.8568507157464212,"[16] Arik Nemtsov, Amir Averbuch, and Alon Schclar. Matrix compression using the Nyström
339"
REFERENCES,0.8588957055214724,"method. Intelligent Data Analysis, 20(5):997–1019, 2016.
340"
REFERENCES,0.8609406952965235,"[17] Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian
341"
REFERENCES,0.8629856850715747,"kernel and nyström method. Advances in Neural Information Processing Systems, 34:2122–
342"
REFERENCES,0.8650306748466258,"2135, 2021.
343"
REFERENCES,0.8670756646216768,"[18] H Peng, N Pappas, D Yogatama, R Schwartz, N Smith, and L Kong. Random Feature Attention.
344"
REFERENCES,0.869120654396728,"In International Conference on Learning Representations, 2021.
345"
REFERENCES,0.8711656441717791,"[19] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, An-
346"
REFERENCES,0.8732106339468303,"dreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz
347"
REFERENCES,0.8752556237218814,"Kaiser, et al. Rethinking Attention with Performers. In International Conference on Learning
348"
REFERENCES,0.8773006134969326,"Representations, 2021.
349"
REFERENCES,0.8793456032719836,"[20] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances
350"
REFERENCES,0.8813905930470347,"in neural information processing systems, 20:1177–1184, 2007.
351"
REFERENCES,0.8834355828220859,"[21] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
352"
REFERENCES,0.885480572597137,"sparse transformers. arXiv preprint arXiv:1904.10509, 2019.
353"
REFERENCES,0.8875255623721882,"[22] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
354"
REFERENCES,0.8895705521472392,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
355"
REFERENCES,0.8916155419222904,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
356"
REFERENCES,0.8936605316973415,"[23] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
357"
REFERENCES,0.8957055214723927,"Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big Bird:
358"
REFERENCES,0.8977505112474438,"Transformers for Longer Sequences.
Advances in neural information processing systems,
359"
REFERENCES,0.8997955010224948,"33:17283–17297, 2020.
360"
REFERENCES,0.901840490797546,"[24] Albert Gu, Karan Goel, and Christopher Re. Efﬁciently modeling long sequences with structured
361"
REFERENCES,0.9038854805725971,"state spaces. In International Conference on Learning Representations.
362"
REFERENCES,0.9059304703476483,"[25] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simpliﬁed state space layers for
363"
REFERENCES,0.9079754601226994,"sequence modeling. In The Eleventh International Conference on Learning Representations,
364"
REFERENCES,0.9100204498977505,"2023.
365"
REFERENCES,0.9120654396728016,"[26] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré.
366"
REFERENCES,0.9141104294478528,"Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint
367"
REFERENCES,0.9161554192229039,"arXiv:2212.14052, 2022.
368"
REFERENCES,0.918200408997955,"[27] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogen-
369"
REFERENCES,0.9202453987730062,"doorn. Ckconv: Continuous kernel convolution for sequential data. In International Conference
370"
REFERENCES,0.9222903885480572,"on Learning Representations.
371"
REFERENCES,0.9243353783231084,"[28] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri
372"
REFERENCES,0.9263803680981595,"Rudra, and Christopher Ré. Simple hardware-efﬁcient long convolutions for sequence modeling.
373"
REFERENCES,0.9284253578732107,"arXiv preprint arXiv:2302.06646, 2023.
374"
REFERENCES,0.9304703476482618,"[29] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
375"
REFERENCES,0.9325153374233128,"May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh
376"
REFERENCES,0.934560327198364,"International Conference on Learning Representations, 2023.
377"
REFERENCES,0.9366053169734151,"[30] Ruslan Khalitov, Tong Yu, Lei Cheng, and Zhirong Yang. Chordmixer: A scalable neural
378"
REFERENCES,0.9386503067484663,"attention model for sequences with different length. In The Eleventh International Conference
379"
REFERENCES,0.9406952965235174,"on Learning Representations, 2023.
380"
REFERENCES,0.9427402862985685,"[31] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
381"
REFERENCES,0.9447852760736196,"[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
382"
REFERENCES,0.9468302658486708,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
383"
REFERENCES,0.9488752556237219,"style, high-performance deep learning library. Advances in neural information processing
384"
REFERENCES,0.950920245398773,"systems, 32, 2019.
385"
REFERENCES,0.9529652351738241,"[33] Alan V Oppenheim and Schafer R W. Discrete Time Signal Processing. Prentice-Hall, 2010.
386"
REFERENCES,0.9550102249488752,"[34] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of
387"
REFERENCES,0.9570552147239264,"linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.
388"
REFERENCES,0.9591002044989775,"[35] Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE,
389"
REFERENCES,0.9611451942740287,"98(6):925–936, 2010.
390"
REFERENCES,0.9631901840490797,"[36] Tianyi Zhou and Dacheng Tao. Godec: Randomized low-rank & sparse matrix decomposition in
391"
REFERENCES,0.9652351738241309,"noisy case. In Proceedings of the 28th International Conference on Machine Learning, 2011.
392"
REFERENCES,0.967280163599182,"[37] Jonathan Mei and José M F Moura. SILVar: Single Index Latent Variable Models. IEEE
393"
REFERENCES,0.9693251533742331,"Transactions on Signal Processing, 66:2790 – 2803, 3 2018.
394"
REFERENCES,0.9713701431492843,"[38] Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-sparsity
395"
REFERENCES,0.9734151329243353,"incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572–596, 2011.
396"
REFERENCES,0.9754601226993865,"[39] Venkat Chandrasekaran, Pablo A. Parrilo, and Alan S. Willsky. Latent variable graphical model
397"
REFERENCES,0.9775051124744376,"selection via convex optimization. Ann. Stat., 40:1935–1967, 8 2012.
398"
REFERENCES,0.9795501022494888,"[40] Teng Zhang and Yi Yang. Robust PCA by manifold optimization. The Journal of Machine
399"
REFERENCES,0.9815950920245399,"Learning Research, 19(1):3101–3139, 2018.
400"
REFERENCES,0.983640081799591,"[41] John G Proakis and Dimitris G Manolakis. Introduction to digital signal processing. Prentice
401"
REFERENCES,0.9856850715746421,"Hall Professional Technical Reference, 1988.
402"
REFERENCES,0.9877300613496932,"[42] Jeffrey Wong. Math 563 lecture notes, polynomial interpolation: the fundamentals, 2020.
403"
REFERENCES,0.9897750511247444,"URL:https://services.math.duke.edu/ jtwong/math563-2020/lectures/Lec1-polyinterp.pdf.
404"
REFERENCES,0.9918200408997955,"[43] Mhenni Benghorbal (https://math.stackexchange.com/users/35472/mhenni benghorbal). How
405"
REFERENCES,0.9938650306748467,"to prove error function erf is entire (i.e., analytic everywhere)? Mathematics Stack Exchange,
406"
REFERENCES,0.9959100204498977,"2017. URL:https://math.stackexchange.com/q/203920 (version: 2017-04-13).
407"
REFERENCES,0.9979550102249489,"[44] Christopher Heil. Introduction to Real Analysis, volume 280. Springer, 2019.
408"
