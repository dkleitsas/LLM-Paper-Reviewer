Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005780346820809248,"Camera-based physiological measurement is a growing ﬁeld with neural models
providing state-the-art-performance. Prior research have explored various “end-
to-end” models; however these methods still require several preprocessing steps.
These additional operations are often non-trivial to implement making replication
and deployment difﬁcult and can even have a higher computational budget than
the “core” network itself. In this paper, we propose two novel and efﬁcient neural
models for camera-based physiological measurement called EfﬁcientPhys that
remove the need for face detection, segmentation, normalization, color space
transformation or any other preprocessing steps. Using an input of raw video
frames, our models achieve state-of-the-art accuracy on three public datasets. We
show that this is the case whether using a transformer or convolutional backbone.
We further evaluate the latency of the proposed networks and show that our most
light weight network also achieves a 33% improvement in efﬁciency."
INTRODUCTION,0.011560693641618497,"1
INTRODUCTION"
INTRODUCTION,0.017341040462427744,"Camera-based physiological measurement is a non-contact approach for capturing cardiac signals
via light reﬂected from the body. The most common such signal is the blood volume pulse (BVP)
measured via the photoplethysmogram (PPG). From this, heart rate (Takano & Ohta, 2007; Verkruysse
et al., 2008), respiration rate (Poh et al., 2010) and pulse transit times Shao et al. (2014) can be derived.
Furthermore, there is promising evidence that the PPG signals be be used to measure signs of arterial
disease Takazawa et al. (1998). Neural models are the current state-of-the-art in this domain (Chen &
McDuff, 2018a; Liu et al., 2020a; 2021d). These networks can learn strong feature representations
and effectively disentangle the subtle changes in pixels due to underlying physiological processes
from those due to body motions, lighting changes and other sources of “noise”."
INTRODUCTION,0.023121387283236993,"While prior research has framed architectures as “end-to-end” methods, those that achieve state-of-
the-art performance actually require several preprocessing steps before data is input into the network.
For example, Chen & McDuff (2018a) and Liu et al. (2020a) use hand-crafted normalized difference
frames and normalized appearance frames as input to their convolutional attention network. Niu et al.
(2020) and Lu et al. (2021) use a complex schema to create feature maps called “MSTmap”. This
process includes facial landmark detection, extractions of several regions of interest (ROI) using
these landmarks, and then averaging pixel values in both the RGB and YUV color spaces."
INTRODUCTION,0.028901734104046242,"These preprocessing steps have several drawbacks: 1) They make assumptions about optimal nor-
malization or representation without allowing the network to learn these features in a data-driven
manner. 2) They are computationally costly and in many cases add a signiﬁcant number of operations
to the video processing pipeline. There are several reasons why we would prefer to run camera-based
physiological sensing on-device: preserving privacy, analyzing raw video (i.e., not compressed) and
saving data costs and bandwidth. Therefore, any additional computation needs to be justiﬁed by
improving model accuracy, otherwise it is considerably disadvantageous. Moreover, since camera-
based physiological sensing is a privacy sensitive application, it is preferred to store the data at local
devices instead of streaming the video and physiological data to cloud. The overhead from processing
is not affordable if we aim to make the system accessible to low-end mobile devices. 3) Many of
these steps are non-trivial to implement and optimize in and of themselves. This makes it harder to
deploy real-time systems and to replicate the implementation on different platforms. For instance,
implementing existing methods on Android, iOS, or in JavaScript requires a signiﬁcant amount of"
INTRODUCTION,0.03468208092485549,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04046242774566474,Video Sequence
EFFICIENTPHYS,0.046242774566473986,2. EfficientPhys
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.05202312138728324,1. Existing Deep Learning Methods for Camera-Based Vitals Measurements
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.057803468208092484,Raw Frames 
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.06358381502890173,"A. Face Detection
B. ROI Segmentation
& Landmark Detection"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.06936416184971098,"Diff 
Layer"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.07514450867052024,BatchNorm
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.08092485549132948,"Normalization 
Module"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.08670520231213873,"Diff
Layer
BatchNorm
+"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.09248554913294797,"BatchNorm
Conv2D"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.09826589595375723,"BatchNorm
Transfomer
Block 2D OR"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.10404624277456648,Temporal Shift Module
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.10982658959537572,Efficient Spatial Temporal Modeling
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.11560693641618497,"Fully 
Connected"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.12138728323699421,"C.Color Augmentation
RGB + YUV
High Dimensional 
Multi-Scale 
Spatial Temporal Map"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.12716763005780346,"Deep Neural Netowrk
with High Complexity"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.1329479768786127,Video Frames
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.13872832369942195,"Spatial
Averaging"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.14450867052023122,"All ROI
Combinations"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.15028901734104047,"Figure 1: High-level comparison between EfﬁcientPhys and existing deep learning approaches for
camera-based vitals measurement"
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.15606936416184972,"effort. Some libraries, such as facial landmark detection, are not even available on every platform.
Thus, the last mile engineering using the existing methods becomes especially challenging."
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.16184971098265896,"Ideally, a video-based physiological measurement method would be able to run at a high frame rate
even on mobile devices, be simple to implement across different platforms, and achieve state-of-the-
art performance. Addressing the aforementioned challenges would help achieve these properties. We
propose a truly end-to-end network, EfﬁcientPhys, for which the input is unprocessed video frames
without requiring accurate face cropping (see Fig. 1). Due to recent interest in visual transformers,
we propose both a convolutional and visual transformer architecture and compare and contrast the
performance of these two."
EXISTING DEEP LEARNING METHODS FOR CAMERA-BASED VITALS MEASUREMENTS,0.1676300578034682,"In summary, our key contributions are to: 1) propose two novel one-stop neural architectures, a visual
transformer and a convolutional network, which do not require any preprocessing steps, 2) evaluate
the proposed methods on three popular benchmark datasets, 3) evaluate on-device latency across both
state-of-the-art machine learning based approaches as well as signal processing based techniques. To
the best of our knowledge, this is the ﬁrst paper that explores the visual transformer in camera-based
physiological measurement and its comparison with convolutional networks. This is also the ﬁrst
paper exploring a completely end-to-end neural architecture. Our code and project page is available
at here 1 and supplementary materials."
RELATED WORK,0.17341040462427745,"2
RELATED WORK"
RELATED WORK,0.1791907514450867,"Camera-based Vital Measurement. There is a growing community studying the use of cameras
to sense physiological vitals signs (Wu et al., 2000; Takano & Ohta, 2007; Verkruysse et al., 2008).
Prior work established the fundamentals of how RGB images could be used to extract the pulse
signal using signal source separation techniques (e.g., ICA) (Poh et al., 2010). Other methods derived
these parameters from physically-based models to achieve elegant and fast demixing (e.g., Plane
Orthogonal-to-Skin (POS))(Wang et al., 2017). By calculating a projection plane orthogonal to the
skin-tone based on optical and physiological principles, the authors were able to achieve a stronger
BVP signal-to-noise ratio (SNR)."
RELATED WORK,0.18497109826589594,"Since the underlying relationship between the pulse and skin pixels is complex, deep convolutional
neural networks have shown superior performance over the traditional source separation algorithms.
DeepPhys (Chen & McDuff, 2018b) was the ﬁrst paper that demonstrated that a deep neural network
outperforms all the traditional signal processing approaches. Liu et al. have also proposed an on-
device efﬁcient neural architecture called MTTS-CAN for on camera-based physiological sensing,
which leverages a tensor-shift module and 2D-convolutional operations to perform efﬁcient spatial-
temporal modeling (Liu et al., 2020b). More recently, an adversarial learning approach, called Dual-
GAN, has also been studied to learn noise-resistant mappings from video frames to pulse waveform
and noise distributions (Lu et al., 2021). With two generative-adversarial networks, they can promote"
RELATED WORK,0.1907514450867052,1https://sites.google.com/view/iclr22-efficientphys
RELATED WORK,0.19653179190751446,Under review as a conference paper at ICLR 2022
RELATED WORK,0.2023121387283237,"Diff 
Layer"
RELATED WORK,0.20809248554913296,BatchNorm
RELATED WORK,0.2138728323699422,"Normalization 
Module"
RELATED WORK,0.21965317919075145,"Self-Attention
Mask
Self-Attention
Mask"
RELATED WORK,0.2254335260115607,Convolution based EfficientPhys
RELATED WORK,0.23121387283236994,Nx72x72x3
RELATED WORK,0.23699421965317918,Transformer based EfficientPhys
RELATED WORK,0.24277456647398843,Nx72x72x3
RELATED WORK,0.24855491329479767,Patch Partition
RELATED WORK,0.2543352601156069,Linear Embedding
RELATED WORK,0.26011560693641617,"Swin 
Transformer 
Block"
RELATED WORK,0.2658959537572254,Stage 1 x2
RELATED WORK,0.27167630057803466,"Diff 
Layer"
RELATED WORK,0.2774566473988439,BatchNorm
RELATED WORK,0.2832369942196532,"Normalization 
Module"
RELATED WORK,0.28901734104046245,Raw Frames
RELATED WORK,0.2947976878612717,Raw Frames
RELATED WORK,0.30057803468208094,Patch Merging
RELATED WORK,0.3063583815028902,"Swin 
Transformer 
Block"
RELATED WORK,0.31213872832369943,Stage 2 x2
RELATED WORK,0.3179190751445087,Patch Merging
RELATED WORK,0.3236994219653179,"Swin 
Transformer 
Block"
RELATED WORK,0.32947976878612717,Stage 3 x6
RELATED WORK,0.3352601156069364,Patch Merging
RELATED WORK,0.34104046242774566,"Swin 
Transformer 
Block"
RELATED WORK,0.3468208092485549,Stage 4 x2
RELATED WORK,0.35260115606936415,"Conv
2D
Conv
2D"
RELATED WORK,0.3583815028901734,"Conv
2D
Conv
2D
Avg
Pool"
RELATED WORK,0.36416184971098264,"Avg
Pool"
RELATED WORK,0.3699421965317919,"+ Dropout
+ Dropout
Blood Volume 
Pulse"
RELATED WORK,0.37572254335260113,"Blood Volume 
Pulse"
RELATED WORK,0.3815028901734104,"Temporal Shift 
Module Dense Dense"
RELATED WORK,0.3872832369942196,"Figure 2: We present two novel architectures to enable simple, fast and accurate camera-based
vitals measurement: Convolution based EfﬁcientPhys and Transformer based EfﬁcientPhys. N is the
number frames of video clip inputting to the network."
RELATED WORK,0.3930635838150289,"each adversarial network’s representation and further improve the feature disentanglement between
pulse and various noise sources."
RELATED WORK,0.3988439306358382,"However, DeepPhys and MTTS-CAN both require a few preprocessing steps including calculating
difference frames and performing image normalization. Dual-GAN has a even more complex
preprocessing module called MSTMaps proposed by Niu et al. (2020). The MSTMaps is a multi-
scale spatial temporal map by 1) ﬁne-grained facial cropping, 2) landmarker extraction, 3) performing
average pooling for every color channel and every ROI combination for each frame, 4) generating
ROI combinations using all the detected ROI regions and landmarkers, 5) multiplications of each item
in all ROI combinations with six channel respectively. The ﬁnal size of MSTMap is (2n −1) × T × 6
where T is the number of frames and n is the number of ROI regions. Such a preprocessing module
not only takes large memory size but also could signiﬁcant introduces large computational burden
to the entire pipeline. Moreover, stacking all of these extra procedures makes development and
deployment much more difﬁcult. Unlike these previous work, our the goal of proposed EfﬁcientPhys
is to create a preprocessing-free neural architecture that is simple to use and deploy, efﬁcient on
mobile devices, and accurate on settings with various noises."
RELATED WORK,0.4046242774566474,"Visual Transformers. Although convolutional neural networks have been widely studied and used in
many computer vision applications, vision transformer started showing its superior performance on the
task image classiﬁcation task. By training on larger datasets, vision transformer (ViT) attains excellent
performance and can be used in downstream ﬁne-tuning with fewer amount of data (Dosovitskiy
et al., 2020). More recently, the state-of-the-art vision transformer, called Swin Transformer, was
proposed to construct hierarchical feature maps and improve computational efﬁciency by using a
hierarchical representation and limiting self-attention computation to non-overlapping local windows
while allowing for cross-window connection (Liu et al., 2021b). However, transformer architectures
have been merely studied in the ﬁeld of camera-based vital measurement. The closest work is using
transformer to detect remote photoplethysmography (rPPG) for for attack/spooﬁng detection (Yu
et al., 2021). However, this paper did not have evaluation on the proposed vision transformer in
the task of heart rate estimation using any public datasets, which is considered as the gold-standard
benchmark for the ﬁeld of camera-based vital measurement. To our best knowledge, our proposed
vision transformer is the ﬁrst architecture in camera-based heart rate measurement with detailed
evaluation on various public datasets."
METHOD,0.41040462427745666,"3
METHOD"
CONVOLUTION BASED EFFICIENTPHYS,0.4161849710982659,"3.1
CONVOLUTION BASED EFFICIENTPHYS"
CONVOLUTION BASED EFFICIENTPHYS,0.42196531791907516,"To enable simple, fast and accurate real-time on-device camera-based vitals measurement, we propose
a one-stop-solution architecture that takes raw video frames as the input to the network and outputs
a ﬁrst-derivative PPG signal. The convolution based EfﬁcientPhys is a one-branch network that
contains a custom normalization layer, self-attention module, tensor-shift module and 2D convolution
operation to perform efﬁcient and accurate spatial-temporal modeling while making it simple to
deploy."
CONVOLUTION BASED EFFICIENTPHYS,0.4277456647398844,Under review as a conference paper at ICLR 2022
CONVOLUTION BASED EFFICIENTPHYS,0.43352601156069365,"Normalization Module. Existing neural methods all require different levels of preprocessing before
providing the visual representation to the network to learn the underlying relationship between skin
pixels and cardiac pulse signal. For instance, The state-of-the-art networks Dual-GAN (Lu et al., 2021)
and CVD (Niu et al., 2020) proposed a hand-crafted spatial-temporal representations called STMaps.
These preprocessed representations are generated for each video frame and includes steps of detecting
81 facial landmark points, extracting a set of region of interest (ROI) combinations (2n −1 where n
is the number of ROIs, n=6) using these landmarks, and averaging pixel values in both the RGB and
YUV color spaces, multiplying the 63 ROI combinations with the six channels. These modules not
only add signiﬁcant computational burden (Table 2 shows that Dual-GAN’s preprocessing module
takes 275ms per frame) but also make the system more challenging to implement and deploy on
real-world computing systems such as mobile devices."
CONVOLUTION BASED EFFICIENTPHYS,0.4393063583815029,"One of the goals of EfﬁcientPhys is to remove these preprocessing modules entirely and provide a
one-stop solution. To achieve such simplicity and deployability, we propose a custom normalization
module, which can perform motion modeling between every two consecutive frames and normaliza-
tion to reduce the lighting and motion noise. More speciﬁcally, the proposed normalization module
includes a difference layer and a batchnorm layer. The difference layer (e.g., torch.diff) computes the
ﬁrst forward difference along the temporal axis of the raw video frames, by subtracting every two
adjacent frames. To provide optical basis in our work, equation 1 illustrates the optical grounding of
difference frame where D
D
Dk(t) of every two consecutive frames where I(t) is the luminance intensity
which is modulated by the specular reﬂection vvvs(t) and the diffuse reﬂection vvvd(t) as well as optical
sensor’s quantization noise vvvn(t)."
CONVOLUTION BASED EFFICIENTPHYS,0.44508670520231214,"D
D
Dk(t) = (I(t) · (vvvs(t) + vvvd(t)) + vvvn(t)) −(I(t −1) · (vvvs(t −1) + vvvd(t −1)) + vvvn(t −1)) (1)"
CONVOLUTION BASED EFFICIENTPHYS,0.4508670520231214,"However, difference frames could be dramatically different in scale and make it hard for the network
to learn meaningful feature representations, especially when the signal of interest is hidden in subtle
pixel changes along the temporal axis and noise artifacts can cause signiﬁcantly larger relative
changes. To address this, we add a batch-normalization layer following by the difference layer.
Adding a batchnorm layer provides two beneﬁts: 1) it normalizes the difference frames to the same
scale within the batch during training, 2) unlike ﬁxed normalization in previous work (Chen &
McDuff, 2018b; Liu et al., 2020a), batchnorm provides two learnable parameters β and γ for scaling
(to a different variance) and shifting (to a different mean) and two constant parameters which are
the mean µ and the standard deviation σ. Through the learning process, the batchnorm layer can
learn the best parameters for removing noise as the Equation 2 shows. Without a batchnorm layer,
directly applying a difference layer means the frames appear “black”; because the subtle changes of
skin pixels in every two consecutive frames are relatively very small. On the other hand, adding a
follow-up batchnorm layer will help it learn the normalization function to magnify the subtle changes
of skin pixels substantially. The result is not simply a magniﬁcation of values but a normalization and
magniﬁcation. Moreover, we also compare the output batchnorm layer to the hand-crafted normalized
frame as shown in Fig.3. The output of batchnorm layer contains more information and qualitative
analysis suggests it should be a better tool for skin segmentation after the learning process."
CONVOLUTION BASED EFFICIENTPHYS,0.45664739884393063,"NNN k(t) = (βt ∗D
D
Dk(t) + γt) −µD
D
Dk
σD
D
Dk
(2)"
CONVOLUTION BASED EFFICIENTPHYS,0.4624277456647399,"Self-Attention-Shifted Network. To efﬁciently capture the rich spatial-temporal information, we
propose a self-attention-shifted network (SASN). SASN is built on top of the previous state-of-the-
art method for on-device spatial-temporal modeling in optical cardiac measurement - tensor-shift
convolutional attention network (TS-CAN) (Liu et al., 2020b). TS-CAN has two convolutional
branches, one of which takes a preprocessed difference frame representation and one of which takes a
normalized appearance frame. The motion branch performs the main spatial-temporal modeling and
estimation, and the appearance branch provides attention masks to guide the motion branch to better
isolate the pixels of interest (e.g., skin pixels). However, we argue that the attention masks do not have
to be obtained through a separate appearance branch and they can be also learned with a single branch
end-to-end network. As Fig. 2 illustrates, our proposed self-attention-shifted network starts with the
custom normalization module discussed in the previous section then continues with two tensor-shifted
convolutional operations. After the second and fourth tensor-shifted 2D convolutional layers, we add"
CONVOLUTION BASED EFFICIENTPHYS,0.4682080924855491,Under review as a conference paper at ICLR 2022
CONVOLUTION BASED EFFICIENTPHYS,0.47398843930635837,"Raw Frame
Output of Diff"
CONVOLUTION BASED EFFICIENTPHYS,0.4797687861271676,"Output of 
BatchNorm Layer"
CONVOLUTION BASED EFFICIENTPHYS,0.48554913294797686,"HandCrafted 
Normalized Frame vs"
CONVOLUTION BASED EFFICIENTPHYS,0.4913294797687861,"Figure 3: Outputs of Diff and batchnorm layers and comparison with normalized frames generated
via the hand-crafted process in prior work Chen & McDuff (2018b). The output from the diff layer is
almost black because the difference in skin pixels of consecutive frames is very subtle."
CONVOLUTION BASED EFFICIENTPHYS,0.49710982658959535,"a self-attention module respectively to help the network minimize the negative effects introduced by
tensor shifting as well as motion and lighting noises. The self-attention layers are softmax attention
layers with 1D convolutions followed by a sigmoid activation function. Then, normalization is
applied to remove the outlying values in the attention mask, and the ﬁnal normalized attention mask is
element-wise multiplied with the output from the tensor-shifted convolution. Equation 3 summarize
how our self-attention mechanism works where ts(.) denotes tensor shift operation, ωt
c denotes the
2D convolutional kernel followed by the tensor shift module, and ωt
a is the 1 × 1 convolutional kernel
for self attention."
CONVOLUTION BASED EFFICIENTPHYS,0.5028901734104047,"(ωt
cts(NNN k(t)) + bt
c) ⊙HtWt · σ(ωt
aXt
α + bt
a)
2 ∥σ(ωtaXtα + bta) ∥1
(3)"
TRANSFORMER BASED EFFICIENTPHYS,0.5086705202312138,"3.2
TRANSFORMER BASED EFFICIENTPHYS"
TRANSFORMER BASED EFFICIENTPHYS,0.5144508670520231,"Efﬁcient Spatial-Temporal Video Transformer. Due to the recent success of visual transformers
for image and video understanding and the importance of attention mechanisms for this task (Chen
& McDuff, 2018a; Yu et al., 2019; Qi et al., 2020; Liu et al., 2020b), we also present a visual
transformer version of EfﬁcientPhys. For this task, we need a visual transformer to learn both
spatial and temporal representations. Several existing video-based visual transformers are based
on 3D-embedding tokens and input all the frames into 3D encoder and spatial-temporal attention
modules (Arnab et al., 2021; Liu et al., 2021d). However, the computational complexity makes that
unfavourable for real-time efﬁcient modeling on mobile devices. In the convolutional version we
used tensor-shifted 2D convolutions which have been shown to achieve comparable performance as
3D convolutions (Liu et al., 2020b). Inspired by this, our proposed transformer based EfﬁcientPhys is
based on a 2D visual transformer, Swin transformer (Liu et al., 2021c), but with added components
that we will describe below."
TRANSFORMER BASED EFFICIENTPHYS,0.5202312138728323,"Since the 2D Swin transformer is only able to learn spatial features that map raw RGB values to
latent representations between a single frame and the target signal (pulse) and does not have ability
to model temporal relationships beyond consecutive frames. One of the main contributions of the
Swin transformer is the shifted window module which has linear computation complexity and allows
cross-window connection by shifting the window partition and limiting self-attention computation
to non-overlapping local windows. Inspired by the idea of shifting of spatial window partitions, we
propose to add a tensor-shift module (TSM) (Lin et al., 2019) before every Swin transformer block to
facilitate information exchange across the temporal axis. The TSM ﬁrst splits the input tensor into
three chunks, shifts the ﬁrst chunk to the left by one place (advancing time by one frame) and shifts
the second chunk to right by one place (delaying time by one frame). All the shifting operations are
along temporal axis and performed before the tensor is fed into each transformer block as shown in
Fig. 2. By adding the TSM module to the Swin transformer, the new transformer architecture now
has the ability to perform efﬁcient spatial-temporal modeling and attention by combining shifting
window partitions spatially and shifting frames temporally. It is worth noting that TSM does not
introduce any learnable parameters thus the proposed transformer architecture has the same number
of parameters as the original Swin transformer. Finally, to enable truly end-to-end inference and"
TRANSFORMER BASED EFFICIENTPHYS,0.5260115606936416,Under review as a conference paper at ICLR 2022
TRANSFORMER BASED EFFICIENTPHYS,0.5317919075144508,"learning, we also add the same normalization module proposed in the convolution EfﬁcientPhys to
this architecture."
TRANSFORMER BASED EFFICIENTPHYS,0.5375722543352601,"In summary, the transformer-based EfﬁcientPhys is the ﬁrst end-to-end transformer architecture for
camera-based cardiac pulse measurement that leverages tensor-shift modules and window-partition
shift modules to perform efﬁcient spatial-temporal modeling and attention to learn the underlying
physiological signal from skin pixels."
EXPERIMENTS,0.5433526011560693,"4
EXPERIMENTS"
EXPERIMENTS,0.5491329479768786,"Training Data. To help create a robust and generalizable model for cross-dataset evaluation we use
two datasets. The ﬁrst is AFRL (Estepp et al., 2014), which includes 300 videos from 25 subjects
(17 males and 8 females). For each video, the raw resolution is 658x492 and the sampling rate of
the synchronized pulse measurement is 30Hz. The dataset includes videos with a range of head
motions. Every participant was instructed to maintain stationary for the ﬁrst two tasks, and then to
perform head motions with increasing rotational velocity in the next four tasks (turning from left
to right). Along with AFRL dataset, we also leverage a synthetic avatar video dataset introduced
by McDuff et al. (2020) where each synthetic video is parameterized and generated with a custom
pulse signal, background, facial appearance, and motion. More speciﬁcally, the input pulse signal is
used to augment skin color and the subsurface radius of skin pixels to mimic the effect of the blood
volume pulse on the skin’s appearance. Synthetic data such as this introduces greater diversity into
the training set and has been shown to effectively help reduce disparities in performance by skin type."
EXPERIMENTS,0.5549132947976878,"Testing Data. We use three popular benchmark datasets to evaluate the accuracy of the proposed
EfﬁcientPhys. UBFC (Bobbia et al., 2019) is a dataset of 42 videos from 42 subjects, and the
raw resolution of each video is 640x480 in a uncompressed 8-bit RGB format. The sampling for
synchronized pulse signal is 30 Hz. All of the tasks collected in UBFC are stationary. MMSE
(Zhang et al., 2016) is a dataset including 102 videos from 40 subjects, and the raw resolution of each
video is at 1040x1392. The ground-truth waveform for MMSE is blood pressure signal instead of
blood-volume pulse signal, and the sampling rate is 25hz. It is worth noting that MMSE contains a
diverse distribution of skin types in Fitzpatrick scale (II=8, III=11, IV=17, V+VI=4). PURE (Stricker
et al., 2014) is a containing 60 videos from 10 subjects. The raw resolution of each video is 640x480,
and the sampling rate of ground-truth pulse signal is 60 Hz. PURE includes a diverse set of motion
tasks such as steady, talking, slow/fast translation between head movements and the camera plane,
small/medium head rotation."
EXPERIMENTS,0.5606936416184971,"Implementation & Experiment Details. We implemented both convolution based and transformer
based EfﬁcientPhys in PyTorch (Paszke et al., 2019). We used an AdamW optimizer to train both
networks instead of Adam by introducing additional regularization to reduce the effects of over-ﬁtting
through weight decay (Loshchilov & Hutter, 2017). The learning rate we used for Convolutional
model was 0.001 while the one for transformer model was 0.0001. Based on empirical studies,
we used mean squared error(MSE) loss for training the transformer models and negative pearson
loss (Tsou et al., 2020) for the convolutional model. We trained both models for ﬁve epochs. We
implemented TS-CAN based on the open-sourced code (Liu et al., 2021a; 2020b). To calculate the
performance metrics, we ﬁrst applied a band-pass ﬁlter to the signal with a cutoff frequency of 0.75
and 2.5Hz (45 beats/minute to 150 beats/minute). We then followed Dual-GAN’s evaluation scheme
to run peak detection and FFT to get estimated heart rate on each video of UBFC and PURE datasets
(Lu et al.) and MetaPhys’s evaluation schema on MMSE (Liu et al., 2021a). We calculated three
standard metrics for each video: mean absolute error (MAE), root mean squared error (RMSE) and
Pearson correlation (ρ) in heart rate estimations and the corresponding ground-truth heart rates from
the blood volume pulse collected via contact oximeter sensor."
EXPERIMENTS,0.5664739884393064,"To explore the efﬁciency of different architectures on mobile devices, we also conducted experiments
on a quad-core Cortex-A72 Raspberry Pi 4B to evaluate the model’s performance on an edge
device. We performed inference 10 times to get a reliable averaged on-device inference latency for
EfﬁcientPhys and TS-CAN. Due to the lack of open-source implementation from Dual-GAN, we were
only able to ﬁnd the implementation of STMaps which is the preprocessing module of Dual-GAN.
Thus, we only evaluated the on-device latency for the preprocessing module in Dual-GAN. We also
evaluated the latency of POS, CHROM, and ICA as they are traditional signal processing methods
and don’t have a separate preprocessing module."
EXPERIMENTS,0.5722543352601156,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.5780346820809249,"Table 1: Cross-dataset heart rate evaluation on three public datasets: UBFC, MMSE and PURE (beats
per minute)"
EXPERIMENTS,0.5838150289017341,"UBFC
PURE
MMSE
Method MAE↓RMSE↓
ρ ↑
MAE↓RMSE↓
ρ ↑
MAE↓RMSE↓
ρ ↑
EfﬁcientPhys-C
1.14
1.81
0.99
1.33
5.99
0.97
2.91
5.43
0.92
EfﬁcientPhys-T1
1.53
2.27
0.99
2.15
7.98
0.94
3.48
7.21
0.86
EfﬁcientPhys-T2
3.07
4.78
0.96
7.71
9.95
0.85
3.51
6.98
0.88
TS-CAN(Liu et al., 2020b)
1.70
2.72
0.99
2.48
9.01
0.92
3.85
7.21
0.86
Dual-GAN(Lu et al., 2021)
0.74
1.02
0.99
N/A
N/A
N/A
N/A
N/A
N/A
PulseGAN(Song et al., 2021)
2.09
4.42
0.99
N/A
N/A
N/A
N/A
N/A
N/A
POS(Wang et al., 2017)
3.52
8.38
0.90
3.14
10.57
0.95
3.98
6.66
0.67
CHROM(De Haan & Jeanne, 2013)
3.10
6.84
0.93
2.07
9.92
0.99
7.31
9.85
0.57
ICA(Poh et al., 2010)
4.39
11.60
0.82
N/A
N/A
N/A
10.2
14.4
0.50"
EXPERIMENTS,0.5895953757225434,"MAE = Mean Absolute Error in HR estimation, RMSE = Root Mean Square Error in HR estimation, ρ = Pearson Correlation in HR estimation."
EXPERIMENTS,0.5953757225433526,Table 2: On-Device Data Preprocessing Latency and Model Inference Latency Per Frame (ms)
EXPERIMENTS,0.6011560693641619,"Preprocessing Model
Total
Method
(ms) ↓
(ms) ↓(ms) ↓
EfﬁcientPhys-C
0
40
40
EfﬁcientPhys-T1
0
300
300
EfﬁcientPhys-T2
0
40
40
TS-CAN(Liu et al., 2020b)
3
60
63
Dual-GAN(Lu et al., 2021)
275
N/A
> 275
POS(Wang et al., 2017)
0
27
27
CHROM(De Haan & Jeanne, 2013)
0
28
28
ICA(Poh et al., 2010)
0
31
31"
EXPERIMENTS,0.6069364161849711,ms = Preprocessing and model latency on Raspberry Pi 4B per frame.
RESULTS AND DISCUSSION,0.6127167630057804,"5
RESULTS AND DISCUSSION"
RESULTS AND DISCUSSION,0.6184971098265896,"EfﬁcientPhys vs. State-of-the-Art. In the Table 1, we present results from our proposed Efﬁcient-
Phys models and the current state-of-the-art neural and signal processing methods. The learning
models are all trained on the same datasets and tested on a separate dataset (e.g., UBFC) to test if the
model can be generalize to videos with a different facial appearance, background and lighting. To
investigate how the depth of the network impacts the Transformer architecture, we created two version
of Transformer-based EfﬁcientPhys: T1 and T2. T1 uses the same depth as the Swin Transformer
reported in Liu et al. (2021c) ([2, 2, 6, 2]). Each number indicates the number of Swin Transformer
blocks as illustrated in Fig. 2. T2 has a much lighter architecture to enable real-time on-device
inference which has a depth of [2, 1]. EfﬁcientPhys-C denotes the Convolution-based EfﬁcientPhys
as shown in the Fig.2. EfﬁcientPhys-C and EfﬁcientPhys-T1 outperform almost all the existing
method except Dual-GAN. However, we argue that the margin is relatively small as both Dual-GAN
and EfﬁcientPhys-C achieve a Pearson correlation of 0.99. EfﬁcientPhys-C again surpasses all the
state-of-the-art methods by 46% of MAE in PURE and 26% of MAE in MMSE. Unfortunately,
Dual-GAN did not perform cross-dataset evaluation on other datasets. Due to the lack of open source
implementation or released models, we could not successfully replicate their complicated model
architecture."
RESULTS AND DISCUSSION,0.6242774566473989,"Computational Cost and On-Device Latency. Fig. 4 and the Table 2 summarize the computational
cost of the existing neural methods. Again, due to the lack of open source implementation and complex
algorithm design, we were not able to replicate every architecture to benchmark its on-device latency.
The results show that EfﬁcientPhys-C only takes 40ms to process a single frame and it does not take
any extra computational time to perform preprocessing. On the other hand, due to the complexity
model architecture and extra time for calculating hand-crafted normalized raw and difference frames,
TS-CAN takes 63ms per frame. As mentioned earlier, Dual-GAN has a complicated preprocessing
procedure for facial landmark detection, segmentation, color transformation and augmentation. We
implemented this and benchmark the preprocessing module on our platform, and it takes 275ms per
frame, which is already 7x than the entire computational time of EfﬁcientPhys-C. The estimation"
RESULTS AND DISCUSSION,0.630057803468208,Under review as a conference paper at ICLR 2022
RESULTS AND DISCUSSION,0.6358381502890174,"network in Dual-GAN also includes 12 2D convolution operations, numerous 1D convolution
operations. Thus, we believe it would add a signiﬁcant amount of computational time on top of
the 275ms preprocessing time per frame. The default Transformer-based EfﬁcientPhys (T1) has a
unfavorable inference due to its deep architecture design and takes 300ms to process every single
frame. After reducing the depth to EfﬁcientPhys-T2, it can achieve same inference time as the
EfﬁcientPhys-C. However, EfﬁcientPhys-T2 has a much worse performance on all three benchmark
datasets."
RESULTS AND DISCUSSION,0.6416184971098265,"50
100
150
200
250
300 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5"
RESULTS AND DISCUSSION,0.6473988439306358,"EfficientPhys-C
40ms/1.14bmp"
RESULTS AND DISCUSSION,0.653179190751445,"TS-CAN
63ms/1.70bmp"
RESULTS AND DISCUSSION,0.6589595375722543,"ICA
31ms/4.39bmp"
RESULTS AND DISCUSSION,0.6647398843930635,"EfficientPhys-T2
40ms/3.07bmp"
RESULTS AND DISCUSSION,0.6705202312138728,"POS
27ms/3.52bmp"
RESULTS AND DISCUSSION,0.6763005780346821,"CHROM
28ms/3.10bmp"
RESULTS AND DISCUSSION,0.6820809248554913,"Dual-GAN
285ms/0.74bmp
Latency of 
Preprocessing Module Only"
RESULTS AND DISCUSSION,0.6878612716763006,"EfficientPhys-T1
300ms/3.07bmp"
RESULTS AND DISCUSSION,0.6936416184971098,Latency (ms)
RESULTS AND DISCUSSION,0.6994219653179191,Mean Absolute Error (BMP) in UBFC
RESULTS AND DISCUSSION,0.7052023121387283,"Figure 4: Accuracy-Latency Trade-off in eight dif-
ferent methods. Y-axis denotes the MAE error,
and X-axis denotes the latency. The methods in
the left-top corner have the best accuracy-latency
Trade-off."
RESULTS AND DISCUSSION,0.7109826589595376,"Convolution vs.
Transformer in Camera-
Based Vitals Measurement. Although visual
transformers have begun to achieve state-of-the-
art performance on some vision tasks, it is not
the case in the task of video based vitals mea-
surement. Based on the results shown in Table
1, Efﬁcient-C outperforms both Efﬁcient-T1 by
25% of MAE in UBFC, 38% of MAE in PURE,
16% of MAE in MMSE, while Efﬁcient-C is
more than 7x faster in terms of latency. When
we shrink the Transformer-based EfﬁcientPhys
to a similar complexity as Convolution-based Ef-
ﬁcientPhys, the accuracy performance is signif-
icantly diminished. The errors from lightweight
Transformer-based EfﬁcientPhys-T2 increased
106% of MAE in UBFC, 478% of MAE in
PURE and 21% of MAE in MMSE. These re-
sults indicate a shallow transformer architecture
struggles to model subtle changes of skin pixels
in the video. These ﬁnding suggest two poten-
tial insights. First, further optimizations will
be necessary for transformers to outperform,
even relatively shallow, convolutional models
in this domain, this is possibly especially true
when there is not a large amount of high-quality
data available. As previous studies have shown
(Dosovitskiy et al., 2020), Transformers usually require more pre-training samples to obtain state-
of-the-art accuracy. Unfortunately, currently the amount of data in the ﬁeld of camera-based vital
measurement is limited compared to other visual tasks. We believe synthetic data is one way to help
address this issue. Second, the good accuracy-efﬁciency trade-off for visual transformer might not
be scaled to on-device architectures without further work. Since many on-device neural networks
require signiﬁcantly less amount of computing resources to perform real-time operations, scaling
the Transformer architecture down is not ideal as our experimental results of EfﬁcientPhys-T2 have
shown."
RESULTS AND DISCUSSION,0.7167630057803468,"Simplifying Last-Mile ML Deployment. Numerous real-world applications are driven by novel
machine learning algorithms. However, deploying these algorithms on different computing platforms
has been extremely challenging for various reasons. One of these is researchers sometimes only
pay attention to the accuracy of the model and ignore the complexity of the last-mile engineering
efforts. In this paper, we address this important issue through our one-stop architecture which takes
the unprocessed raw frames and directly outputs the desired signal. This elegant and simple design
not only will reduce the burden of engineering required for cross-platform implementations, but also
will help the research community to replicate and reproduce results."
RESULTS AND DISCUSSION,0.7225433526011561,"Extensible to Other Signal. Finally, another potential upside of our end-to-end design and the
low latency, we envision EfﬁcientPhys could be applied to various other video-based applications.
Since the input of our model is raw frame, we believe EfﬁcientPhys can be easily extended to other
physiological signals and formats of data such as video-based blood pressure measurement and video
understanding & recognition etc. On the other hand, most of the baseline methods we compared (e.g.,
Dual-GAN, PulseGAN) require many custom preprocessing operations for video-based measurement
which are less useful in other applications."
RESULTS AND DISCUSSION,0.7283236994219653,Under review as a conference paper at ICLR 2022
BROADER IMPACTS AND ETHICS STATEMENT,0.7341040462427746,"6
BROADER IMPACTS AND ETHICS STATEMENT"
BROADER IMPACTS AND ETHICS STATEMENT,0.7398843930635838,"Recent Information and Communications Technologies for Development (ICT4D) work reconceptu-
alize technology development to empower the people it serves. Machine learning based applications
such as health interventions also tend to focus more on the development of the technology itself rather
than the people and problems they address. Although many large-scale deep neural network models
are trained on the data created by the public, they are often not freely available to the public. In this
work we only used data that was collected under informed consent for the purposes of physiological
analysis. We also make the trained models accessible and available to more diverse communities as
described below."
BROADER IMPACTS AND ETHICS STATEMENT,0.7456647398843931,"During the development of EfﬁcientPhys, we intended that the innovations do not create larger
disparities between different populations. By achieving the state-of-the-art accuracy and efﬁciency as
well as our simple and elegant design, we believe EfﬁcientPhys will help make camera-based vitals
measurement more widely available to the medical research community and broader community in
computing. We also believe that this technology can have a particular impact in low-resource settings
there are greater barriers to access healthcare. We envision our proposed method could, with the
appropriate clinical validation and regulatory approval, eventually be used in healthcare applications
(e.g., real-time vitals measurement in telehealth appointments). During the COVID-19 pandemic the
need for such technology has been clearly highlighted. We contextualize our contributions within
the scope of democratizing technology for social good and helping to reduce health disparities with
advanced AI technology. However, we are aware that machine learning systems are biased and can
propagate inequalities. Before technology such as that presented in this paper is ready for deployment
we need to make sure that that is not the case."
REPRODUCIBILITY STATEMENT,0.7514450867052023,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.7572254335260116,"We described detailed experimental details in the section of 4. We also release our code in a
Github anonymous repo and provide a detailed project overview in an anonymous website: https:
//sites.google.com/view/iclr22-efficientphys."
CONCLUSION,0.7630057803468208,"8
CONCLUSION"
CONCLUSION,0.7687861271676301,"In this paper, we present a novel method called EfﬁcientPhys to enable simple, fast, accurate camera-
based contactless vitals measuremnt. We achieved the state-of-the-art performance with using
signiﬁcant less computational power. With the simple and elegant one-stop design, EfﬁcientPhys also
help address the issue of last-time machine learning deployment and reduce health disparity."
REFERENCES,0.7745664739884393,REFERENCES
REFERENCES,0.7803468208092486,"Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid.
ViViT: A Video Vision Transformer. arXiv:2103.15691 [cs], March 2021. URL http://arxiv.
org/abs/2103.15691. arXiv: 2103.15691."
REFERENCES,0.7861271676300579,"Serge Bobbia, Richard Macwan, Yannick Benezeth, Alamin Mansouri, and Julien Dubois. Unsu-
pervised skin tissue segmentation for remote photoplethysmography. Pattern Recognition Letters,
124:82–90, 2019."
REFERENCES,0.791907514450867,"Weixuan Chen and Daniel McDuff. Deepphys: Video-based physiological measurement using
convolutional attention networks. In Proceedings of the European Conference on Computer Vision
(ECCV), pp. 349–365, 2018a."
REFERENCES,0.7976878612716763,"Weixuan Chen and Daniel McDuff. DeepPhys: Video-Based Physiological Measurement Using
Convolutional Attention Networks. arXiv:1805.07888 [cs], August 2018b. URL http://
arxiv.org/abs/1805.07888. arXiv: 1805.07888."
REFERENCES,0.8034682080924855,"Gerard De Haan and Vincent Jeanne. Robust pulse rate from chrominance-based rppg. IEEE
Transactions on Biomedical Engineering, 60(10):2878–2886, 2013."
REFERENCES,0.8092485549132948,Under review as a conference paper at ICLR 2022
REFERENCES,0.815028901734104,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.8208092485549133,"Justin R. Estepp, Ethan B. Blackford, and Christopher M. Meier. Recovering pulse rate during motion
artifact with a multi-imager array for non-contact imaging photoplethysmography. In 2014 IEEE
International Conference on Systems, Man, and Cybernetics (SMC), pp. 1462–1469, October 2014.
doi: 10.1109/SMC.2014.6974121. ISSN: 1062-922X."
REFERENCES,0.8265895953757225,"Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efﬁcient video understanding.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7083–7093,
2019."
REFERENCES,0.8323699421965318,"Xin Liu, Josh Fromm, Shwetak Patel, and Daniel McDuff. Multi-task temporal shift attention
networks for on-device contactless vitals measurement. arXiv preprint arXiv:2006.03790, 2020a."
REFERENCES,0.838150289017341,"Xin Liu, Josh Fromm, Shwetak Patel, and Daniel McDuff. Multi-Task Temporal Shift Attention
Networks for On-Device Contactless Vitals Measurement. October 2020b."
REFERENCES,0.8439306358381503,"Xin Liu, Ziheng Jiang, Josh Fromm, Xuhai Xu, Shwetak Patel, and Daniel McDuff. MetaPhys:
Few-Shot Adaptation for Non-Contact Physiological Measurement. arXiv:2010.01773 [cs], March
2021a. URL http://arxiv.org/abs/2010.01773. arXiv: 2010.01773."
REFERENCES,0.8497109826589595,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021b."
REFERENCES,0.8554913294797688,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv:2103.14030
[cs], March 2021c. URL http://arxiv.org/abs/2103.14030. arXiv: 2103.14030."
REFERENCES,0.861271676300578,"Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin
transformer. arXiv preprint arXiv:2106.13230, 2021d."
REFERENCES,0.8670520231213873,"Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017."
REFERENCES,0.8728323699421965,"Hao Lu, Hu Han, and S Kevin Zhou. Dual-GAN: Joint BVP and Noise Modeling for Remote
Physiological Measurement. pp. 10."
REFERENCES,0.8786127167630058,"Hao Lu, Hu Han, and S Kevin Zhou. Dual-gan: Joint bvp and noise modeling for remote physiological
measurement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 12404–12413, 2021."
REFERENCES,0.884393063583815,"Daniel McDuff, Javier Hernandez, Erroll Wood, Xin Liu, and Tadas Baltrusaitis. Advancing Non-
Contact Vital Sign Measurement using Synthetic Avatars. arXiv:2010.12949 [cs], October 2020.
URL http://arxiv.org/abs/2010.12949. arXiv: 2010.12949."
REFERENCES,0.8901734104046243,"Xuesong Niu, Zitong Yu, Hu Han, Xiaobai Li, Shiguang Shan, and Guoying Zhao. Video-based re-
mote physiological measurement via cross-veriﬁed feature disentangling. In European Conference
on Computer Vision, pp. 295–310. Springer, 2020."
REFERENCES,0.8959537572254336,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026–8037, 2019."
REFERENCES,0.9017341040462428,"Ming-Zher Poh, Daniel J McDuff, and Rosalind W Picard. Advancements in noncontact, multiparam-
eter physiological measurements using a webcam. IEEE transactions on biomedical engineering,
58(1):7–11, 2010."
REFERENCES,0.9075144508670521,Under review as a conference paper at ICLR 2022
REFERENCES,0.9132947976878613,"Hua Qi, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Wei Feng, Yang Liu, and Jianjun Zhao.
Deeprhythm: Exposing deepfakes with attentional visual heartbeat rhythms. In Proceedings of the
28th ACM International Conference on Multimedia, pp. 4318–4327, 2020."
REFERENCES,0.9190751445086706,"Dangdang Shao, Yuting Yang, Chenbin Liu, Francis Tsow, Hui Yu, and Nongjian Tao. Noncontact
monitoring breathing pattern, exhalation ﬂow rate and pulse transit time. IEEE Transactions on
Biomedical Engineering, 61(11):2760–2767, 2014."
REFERENCES,0.9248554913294798,"Rencheng Song, Huan Chen, Juan Cheng, Chang Li, Yu Liu, and Xun Chen. Pulsegan: Learning to
generate realistic pulse waveforms in remote photoplethysmography. IEEE Journal of Biomedical
and Health Informatics, 25(5):1373–1384, 2021."
REFERENCES,0.930635838150289,"Ronny Stricker, Steffen M¨uller, and Horst-Michael Gross. Non-contact video-based pulse rate
measurement on a mobile service robot. In The 23rd IEEE International Symposium on Robot and
Human Interactive Communication, pp. 1056–1062. IEEE, 2014."
REFERENCES,0.9364161849710982,"Chihiro Takano and Yuji Ohta. Heart rate measurement based on a time-lapse image. Medical
engineering & physics, 29(8):853–857, 2007."
REFERENCES,0.9421965317919075,"Kenji Takazawa, Nobuhiro Tanaka, Masami Fujita, Osamu Matsuoka, Tokuyu Saiki, Masaru Aikawa,
Sinobu Tamura, and Chiharu Ibukiyama. Assessment of Vasoactive Agents and Vascular Aging by
the Second Derivative of Photoplethysmogram Waveform. Hypertension, 32(2):365–370, August
1998. doi: 10.1161/01.HYP.32.2.365. URL https://www.ahajournals.org/doi/10.
1161/01.HYP.32.2.365."
REFERENCES,0.9479768786127167,"Yun-Yun Tsou, Yi-An Lee, Chiou-Ting Hsu, and Shang-Hung Chang. Siamese-rppg network: Remote
photoplethysmography signal estimation from face videos. In Proceedings of the 35th annual
ACM symposium on applied computing, pp. 2066–2073, 2020."
REFERENCES,0.953757225433526,"Wim Verkruysse, Lars O Svaasand, and J Stuart Nelson. Remote plethysmographic imaging using
ambient light. Optics express, 16(26):21434–21445, 2008."
REFERENCES,0.9595375722543352,"Wenjin Wang, Albertus C. den Brinker, Sander Stuijk, and Gerard de Haan. Algorithmic Principles
of Remote PPG. IEEE Transactions on Biomedical Engineering, 64(7):1479–1491, July 2017.
ISSN 1558-2531. doi: 10.1109/TBME.2016.2609282. Conference Name: IEEE Transactions on
Biomedical Engineering."
REFERENCES,0.9653179190751445,"Ting Wu, Vladimir Blazek, and Hans Juergen Schmitt. Photoplethysmography imaging: a new
noninvasive and noncontact method for mapping of the dermal perfusion changes. In Optical
Techniques and Instrumentation for the Measurement of Blood Composition, Structure, and
Dynamics, volume 4163, pp. 62–70. International Society for Optics and Photonics, 2000."
REFERENCES,0.9710982658959537,"Zitong Yu, Xiaobai Li, and Guoying Zhao. Remote Photoplethysmograph Signal Measurement
from Facial Videos Using Spatio-Temporal Networks. arXiv:1905.02419 [cs], July 2019. URL
http://arxiv.org/abs/1905.02419. arXiv: 1905.02419."
REFERENCES,0.976878612716763,"Zitong Yu, Xiaobai Li, Pichao Wang, and Guoying Zhao. Transrppg: Remote photoplethysmography
transformer for 3d mask face presentation attack detection. IEEE Signal Processing Letters, 2021."
REFERENCES,0.9826589595375722,"Zheng Zhang, Jeff M Girard, Yue Wu, Xing Zhang, Peng Liu, Umur Ciftci, Shaun Canavan, Michael
Reale, Andy Horowitz, Huiyuan Yang, et al. Multimodal spontaneous emotion corpus for human
behavior analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3438–3446, 2016."
REFERENCES,0.9884393063583815,"A
APPENDIX"
REFERENCES,0.9942196531791907,Project and Code Link: https://sites.google.com/view/iclr22-efficientphys
