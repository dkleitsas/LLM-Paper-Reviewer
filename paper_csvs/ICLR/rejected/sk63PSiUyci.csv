Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013227513227513227,"We present AI-SARAH, a practical variant of SARAH. As a variant of SARAH, this
algorithm employs the stochastic recursive gradient yet adjusts step-size based on
local geometry. AI-SARAH implicitly computes step-size and efﬁciently estimates
local Lipschitz smoothness of stochastic functions. It is fully adaptive, tune-free,
straightforward to implement, and computationally efﬁcient. We provide technical
insight and intuitive illustrations on its design and convergence. We conduct
extensive empirical analysis and demonstrate its strong performance compared
with its classical counterparts and other state-of-the-art ﬁrst-order methods in
solving convex machine learning problems."
INTRODUCTION,0.0026455026455026454,"1
INTRODUCTION"
INTRODUCTION,0.003968253968253968,We consider the unconstrained ﬁnite-sum optimization problem
INTRODUCTION,0.005291005291005291,"minw∈Rd[P(w)
def
= 1"
INTRODUCTION,0.006613756613756613,"n
Pn
i=1fi(w)].
(1)"
INTRODUCTION,0.007936507936507936,"This problem is prevalent in machine learning tasks where w corresponds to the model parameters,
fi(w) represents the loss on the training point i, and the goal is to minimize the average loss
P(w) across the training points. In machine learning applications, (1) is often considered the loss
function of Empirical Risk Minimization (ERM) problems. For instance, given a classiﬁcation or
regression problem, fi can be deﬁned as logistic regression or least square by (xi, yi) where xi is
a feature representation and yi is a label. Throughout the paper, we assume that each function fi,
i ∈[n]
def
= {1, ..., n}, is smooth and convex, and there exists an optimal solution w∗of (1)."
MAIN CONTRIBUTIONS,0.009259259259259259,"1.1
MAIN CONTRIBUTIONS"
MAIN CONTRIBUTIONS,0.010582010582010581,"We propose AI-SARAH, a practical variant of stochastic recursive gradient methods (Nguyen et al.,
2017) to solve (1). This practical algorithm explores and adapts to local geometry. It is adaptive at
full scale yet requires zero effort of tuning hyper-parameters. The extensive numerical experiments
demonstrate that our tune-free and fully adaptive algorithm is capable of delivering a consistently
competitive performance on various datasets, when comparing with SARAH, SARAH+ and other
state-of-the-art ﬁrst-order method, all equipped with ﬁne-tuned hyper-parameters (which are selected
from ≈5, 000 runs for each problem). This work provides a foundation on studying adaptivity (of
stochastic recursive gradient methods) and demonstrates that a truly adaptive stochastic recursive
algorithm can be developed in practice."
RELATED WORK,0.011904761904761904,"1.2
RELATED WORK"
RELATED WORK,0.013227513227513227,"Stochastic gradient descent (SGD) (Robbins & Monro, 1951; Nemirovski & Yudin, 1983; Shalev-
Shwartz et al., 2007; Nemirovski et al., 2009; Gower et al., 2019) is the workhorse for training
supervised machine learning problems that have the generic form (1).
In its generic form, SGD deﬁnes the new iterate by subtracting a multiple of a stochastic gradient
g(wt) from the current iterate wt. That is,"
RELATED WORK,0.01455026455026455,"wt+1 = wt −αtg(wt).
In most algorithms, g(w) is an unbiased estimator of the gradient (i.e., a stochastic gradient),
E[g(w)] = ∇P(w), ∀w ∈Rd. However, in several algorithms (including the ones from this paper),"
RELATED WORK,0.015873015873015872,Under review as a conference paper at ICLR 2022
RELATED WORK,0.017195767195767195,"g(w) could be a biased estimator, and convergence guarantees can still be well obtained.
Adaptive step-size selection. The main parameter to guarantee the convergence of SGD is the
step-size. In recent years, several ways of selecting the step-size have been proposed. For example,
an analysis of SGD with constant step-size (αt = α) or decreasing step-size has been proposed
in Moulines & Bach (2011); Ghadimi & Lan (2013); Needell et al. (2016); Nguyen et al. (2018);
Bottou et al. (2018); Gower et al. (2019; 2020b) under different assumptions on the properties of
problem (1).
More recently, adaptive / parameter-free methods (Duchi et al., 2011; Kingma & Ba, 2015; Bengio,
2015; Li & Orabona, 2018; Vaswani et al., 2019; Liu et al., 2019a; Ward et al., 2019; Loizou et al.,
2020) that adapt the step-size as the algorithms progress have become popular and are particularly
beneﬁcial when training deep neural networks. Normally, in these algorithms, the step-size does not
depend on parameters that might be unknown in practical scenarios, like the smoothness parameter
or the strongly convex parameter.
Random vector g(wt) and variance reduced methods. One of the most remarkable algorithmic
breakthroughs in recent years was the development of variance-reduced stochastic gradient algorithms
for solving ﬁnite-sum optimization problems. These algorithms, by reducing the variance of the
stochastic gradients, are able to guarantee convergence to the exact solution of the optimization
problem with faster convergence than classical SGD. In the past decade, many efﬁcient variance-
reduced methods have been proposed. Some popular examples of variance reduced algorithms are
SAG (Schmidt et al., 2017), SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013) and SARAH
(Nguyen et al., 2017). For more examples of variance reduced methods, see Defazio (2016); Koneˇcný
et al. (2016); Gower et al. (2020a); Khaled et al. (2020); Horváth et al. (2020).
Among the variance reduced methods, SARAH is of our interest in this work. Like the popular SVRG,
SARAH algorithm is composed of two nested loops. In each outer loop k ≥1, the gradient estimate
v0 = ∇P(wk−1) is set to be the full gradient. Subsequently, in the inner loop, at t ≥1, a biased
estimator vt is used and deﬁned recursively as"
RELATED WORK,0.018518518518518517,"vt = ∇fi(wt) −∇fi(wt−1) + vt−1,
(2)"
RELATED WORK,0.01984126984126984,"where i ∈[n] is a random sample selected at t.
A common characteristic of the popular variance reduced methods is that the step-size α in their
update rule wt+1 = wt −αvt is constant (or diminishing with predetermined rules) and that depends
on the characteristics of problem (1). An exception to this rule are the variance reduced methods
with Barzilai-Borwein step size, named BB-SVRG and BB-SARAH proposed in Tan et al. (2016)
and Li & Giannakis (2019) respectively. These methods allow to use Barzilai-Borwein (BB) step
size rule to update the step-size once in every epoch; for more examples, see Li et al. (2020); Yang
et al. (2021). There are also methods proposing approach of using local Lipschitz smoothness to
derive an adaptive step-size (Liu et al., 2019b) with additional tunable parameters or leveraging BB
step-size with averaging schemes to automatically determine the inner loop size (Li et al., 2020).
However, these methods do not fully take advantage of the local geometry, and a truly adaptive
algorithm: adjusting step-size at every (inner) iteration and eliminating need of tuning any
hyper-parameters, is yet to be developed in the stochastic variance reduced framework. This
is exactly the main contribution of this work, as we mentioned in previous section."
MOTIVATION,0.021164021164021163,"2
MOTIVATION"
MOTIVATION,0.022486772486772486,"With our primary focus on the design of a stochastic recursive algorithm with adaptive step-size, we
discuss our motivation in this chapter.
A standard approach of tuning the step-size involves the painstaking grid search on a wide range
of candidates. While more sophisticated methods can design a tuning plan, they often struggle for
efﬁciency and/or require a considerable amount of computing resources.
More importantly, tuning step-size requires knowledge that is not readily available at a starting
point w0 ∈Rd, and choices of step-size could be heavily inﬂuenced by the curvature provided
∇2P(w0). What if a step-size has to be small due to a ""sharp"" curvature initially, which becomes
""ﬂat"" afterwards?
To see this is indeed the case for many machine learning problems, let us consider logistic regression
for a binary classiﬁcation problem, i.e., fi(w) = log(1 + exp(−yixT
i w)) + λ"
MOTIVATION,0.023809523809523808,"2 ∥w∥2, where xi ∈Rd
is a feature vector, yi ∈{−1, +1} is a ground truth, and the ERM problem is in the form of (1). It is"
MOTIVATION,0.02513227513227513,Under review as a conference paper at ICLR 2022
MOTIVATION,0.026455026455026454,"0
5
10
15
20
25
30
Effective Pass 10
7 10
5 10
3 10
1"
MOTIVATION,0.027777777777777776,"P(w)
P"
MOTIVATION,0.0291005291005291,"AI-SARAH
SARAH, step-size 0.25
SARAH, step-size 0.50
SARAH, step-size 1.00
SARAH, step-size 2.00
SARAH, step-size 4.00
SARAH, step-size 8.00
SARAH, step-size 16.00 (a)"
MOTIVATION,0.03042328042328042,"0
5
10
15
20
25
30
Effective Pass 10
8 10
6 10
4 10
2 100"
MOTIVATION,0.031746031746031744,||vt||2
MOTIVATION,0.03306878306878307,"AI-SARAH
SARAH, step-size 0.25
SARAH, step-size 0.50
SARAH, step-size 1.00
SARAH, step-size 2.00
SARAH, step-size 4.00
SARAH, step-size 8.00
SARAH, step-size 16.00 (b)"
MOTIVATION,0.03439153439153439,"0
5
10
15
20
25
30
Effective Pass 0 10 20 30 40"
MOTIVATION,0.03571428571428571,Step-Size
MOTIVATION,0.037037037037037035,"AI-SARAH - step-size
AI-SARAH - step-size upper-bound"
MOTIVATION,0.03835978835978836,"1/
max(
2P(w))"
MOTIVATION,0.03968253968253968,AI-SARAH - step-size (moving avg.) (c)
MOTIVATION,0.041005291005291,"1.0
2.0
3.9
6.3
8.9 12.1 15.2 17.6 21.0 24.6 27.5 31.1"
MOTIVATION,0.042328042328042326,Effective Pass 0.00 0.05 0.10 0.15 0.20 0.25 si(w) (d)
MOTIVATION,0.04365079365079365,"Figure 1: AI-SARAH vs. SARAH: (a) evolution of the optimality gap P(w) −¯P and (b) the squared
norm of stochastic recursive gradient ∥vt∥2; AI-SARAH: (c) evolution of the step-size, upper-bound,
local Lipschitz smoothness and (d) distribution of si of stochastic functions. Note: in (a), ¯P is a
lower bound of P(w∗); in (c), the white spaces suggest full gradient computations at outer iterations;
in (d), bars represent medians of si’s."
MOTIVATION,0.04497354497354497,"easy to derive the local curvature of P(w), deﬁned by its Hessian in the form"
MOTIVATION,0.046296296296296294,∇2P(w) = 1
MOTIVATION,0.047619047619047616,"n
Pn
i=1
exp(−yixT
i w)
[1+exp(−yixT
i w)]2
|
{z
}
si(w)"
MOTIVATION,0.04894179894179894,"xixT
i + λI.
(3)"
MOTIVATION,0.05026455026455026,"Given that
a
(1+a)2 ≤0.25 for any a ≥0, one can immediately obtain the global bound on Hessian,
i.e. ∀w ∈Rd we have ∇2P(w) ⪯1"
MOTIVATION,0.051587301587301584,"4
1
n
Pn
i=1 xixT
i + λI. Consequently, the parameter of global
Lipschitz smoothness is L = 1"
MOTIVATION,0.05291005291005291,4λmax( 1
MOTIVATION,0.05423280423280423,"n
Pn
i=1 xixT
i ) + λ. It is well known that, with a constant
step-size less than (or equal to) 1"
MOTIVATION,0.05555555555555555,"L, a convergence is guaranteed by many algorithms."
MOTIVATION,0.056878306878306875,"However, suppose the algorithm starts at a random w0 (or at 0 ∈Rd), this bound can be very tight.
With more progress being made on approaching an optimal solution (or reducing the training error),
it is likely that, for many training samples, −yixT
i wt ≪0. An immediate implication is that si(wt)
deﬁned in (3) becomes smaller and hence the local curvature will be smaller as well. It suggests
that, although a large initial step-size could lead to divergence, with more progress made by the
algorithm, the parameter of local Lipschitz smoothness tends to be smaller and a larger step-size can
be used. That being said, such a dynamic step-size cannot be well deﬁned in the beginning, and a
fully adaptive approach needs to be developed."
MOTIVATION,0.0582010582010582,"For illustration, we present the inspiring results of an experiment on real-sim dataset1 with
ℓ2-regularized logistic regression. Figures 1(a) and 1(b) compare the performance of classical
SARAH with AI-SARAH in terms of the evolution of the optimality gap and the squared norm of
recursive gradient. As is clear from the ﬁgure, AI-SARAH displays a signiﬁcantly faster convergence
per effective pass2."
MOTIVATION,0.05952380952380952,"Now, let us discuss why this could happen. The distribution of si as shown in Figured 1(d) indicates
that: initially, all si’s are concentrated at 0.25; the median continues to reduce within a few effective
passes on the training samples; eventually, it stabilizes somewhere below 0.05. Correspondingly, as
presented in Figure 1(c), AI-SARAH starts with a conservative step-size dominated by the global
Lipschitz smoothness, i.e., 1/λmax(∇2P(w0)) (red dots); however, within 5 effective passes, the
moving average (magenta dash) and upper-bound (blue line) of the step-size start surpassing the red
dots, and eventually stablize above the conservative step-size."
MOTIVATION,0.06084656084656084,"For classical SARAH, we conﬁgure the algorithm with different values of the ﬁxed step-size, i.e.,
{2−2, 2−1, ..., 24}, and notice that 25 leads to a divergence. On the other hand, AI-SARAH starts
with a small step-size, yet achieves a faster convergence per effective pass with an eventual (moving
average) step-size larger than 25."
"THE
DATASET
IS
AVAILABLE
AT",0.062169312169312166,"1The
dataset
is
available
at
https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
datasets/
2The effective pass is deﬁned as a complete pass on the training dataset. Each data sample is selected once
per effective pass on average."
"THE
DATASET
IS
AVAILABLE
AT",0.06349206349206349,Under review as a conference paper at ICLR 2022
ALGORITHM,0.06481481481481481,"3
ALGORITHM"
ALGORITHM,0.06613756613756613,"We present AI-SARAH in Algorithm 1. This algorithm implicitly computes αt−1 at any t ≥1 through
approximately solving the sub-problem, i.e., minα>0 ξt(α), and estimating the local Lipschitz
smoothness of a stochastic function; the upper-bound of step-size makes the algorithm stable, and
it is updated with exponential smoothing on harmonic mean (of approximate solutions to the sub-
problems), which also keep tracks of the local Lipschitz smoothness of a ﬁnite sum function, i.e.,
P(w).
This algorithm is fully adaptive and requires no efforts of tuning, and can be implemented
easily. Notice that β is treated as a smoothing factor in updating the upper-bound of the step-size,
and the default setting is β = 0.999. There exists one hyper-parameter in Algorithm 1, γ, which
deﬁnes the early stopping criterion on Line 8, and the default setting is γ =
1
32. We will show later
in this chapter that, the performance of this algorithm is not sensitive to the choices of γ, and this
is true regardless of the problems (i.e., regularized/non-regularized logistic regression and different
datasets.)"
ALGORITHM,0.06746031746031746,Algorithm 1 AI-SARAH
ALGORITHM,0.06878306878306878,"1: Parameter: 0 < γ < 1 (default
1
32), β = 0.999
2: Initialize: ˜w0
3: Set: αmax = ∞
4: for k = 1, 2, ... do
5:
w0 = ˜wk−1
6:
v0 = ∇P(w0)
7:
t = 1
8:
while ∥vt∥2 ≥γ∥v0∥2 do
9:
Select random mini-batch St from [n] uniformly with |St| = b
10:
˜αt−1 ≈arg minα>0 ξt(α)
11:
if k = 0 and t = 1 then
12:
δk
t =
1
˜αt−1
13:
else
14:
δk
t = βδk
t−1 + (1 −β)
1
˜αt−1
15:
end if
16:
αmax =
1
δk
t
17:
αt−1 = min{˜αt−1, αmax}
18:
wt = wt−1 −αt−1vt−1
19:
vt = ∇fSt(wt) −∇fSt(wt−1) + vt−1
20:
t = t + 1
21:
end while
22:
Set ˜wk = wt.
23: end for"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.0701058201058201,"3.1
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.07142857142857142,"In the previous chapter, we showed that AI-SARAH adapts to local Lipschitz smoothness and yields
a faster convergence than classical SARAH. Then, the question is how to estimate the parameter of
local Lipschitz smoothness in practice.
Can we use line-search? The standard approach to estimate local Lipschitz smoothness is to
use backtracking line-search. Recall SARAH’s update rule, i.e., wt = wt−1 −αt−1vt−1, where
vt−1 is a stochastic recursive gradient. The standard procedure is to apply line-search on function
fit(wt−1 −αvt−1). However, the main issue is that −vt−1 is not necessarily a descent direction.
AI-SARAH sub-problem. Deﬁne the sub-problem (as shown on line 10 of Algorithm 13) as
min
α>0 ξt(α) = min
α>0 ∥∇fit(wt−1 −αvt−1) −∇fit(wt−1) + vt−1∥2,
(4)"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.07275132275132275,"where t ≥1 denotes an inner iteration and it indexes a random sample selected at t. We argue that,
by (approximately) solving (4), we can have good estimate of the parameters of the local Lipschitz"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.07407407407407407,"3For sake of simplicity, we use fit instead of fSt."
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.07539682539682539,Under review as a conference paper at ICLR 2022
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.07671957671957672,"smoothness.
To illustrate this setting, we denote Li
t the parameter of local Lipschitz smoothness prescribed by
fit at wt−1. Let us focus on a simple quadratic function fit(w) = 1"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.07804232804232804,"2(xT
itw −yit)2. Let ˜α be the
optimal step-size along direction −vt−1, i.e. ˜α = arg minα fit(wt−1 −αvt−1). Then, the closed"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.07936507936507936,"form solution of ˜α can be easily derived as ˜α =
xT
itwt−1−yit"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.08068783068783068,"xT
itvt−1
, whose value can be positive, negative,"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.082010582010582,bounded or unbounded.
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.08333333333333333,"On the other hand, one can compute the step-size implicitly by solving (4) and obtain αi
t−1, i.e.,
αi
t−1 = arg minα ξt(α). Then, we have"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.08465608465608465,"αi
t−1 =
1
xT
itxit ,"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.08597883597883597,"which is exactly
1
Li
t and recall Li
t is the parameter of local Lipschitz smoothness of fit."
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.0873015873015873,"Simply put, as quadratic function has a constant Hessian, solving (4) gives exactly
1
Li
t . For"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.08862433862433862,"general (strongly) convex functions, if ∇2fit(wt−1), does not change too much locally, we can
still have a good estimate of Li
t by solving (4) approximately.
Based on a good estimate of Li
t, we can then obtain the estimate of the local Lipschitz smoothness of
P(wt−1). And, that is
¯Lt = 1"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.08994708994708994,"n
Pn
i=1Li
t = 1"
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.09126984126984126,"n
Pn
i=1
1
αi
t−1 ."
ESTIMATE LOCAL LIPSCHITZ SMOOTHNESS,0.09259259259259259,"Clearly, if a step-size in the algorithm is selected as 1/¯Lt, then a harmonic mean of the sequence of the
step-size’s, computed for various component functions could serve as a good adaptive upper-bound
on the step-size computed in the algorithm. More details of intuition for the adaptive upper-bound
can be found in Appendix A.2."
COMPUTE STEP-SIZE AND UPPER-BOUND,0.09391534391534391,"3.2
COMPUTE STEP-SIZE AND UPPER-BOUND"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.09523809523809523,"On Line 10 of Algorithm 1, the sub-problem is a one-dimensional minimization problem, which
can be approximately solved by Newton method. Speciﬁcally in Algorithm 1, we compute one-step
Newton at α = 0, and that is"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.09656084656084656,"˜αt−1 = −ξ′
t(0)
|ξ′′
t (0)|.
(5)"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.09788359788359788,"Note that, for convex function in general, (5) gives an approximate solution; for functions in particular
forms such as quadratic ones, (5) gives an exact solution.
The procedure prescribed in (5) can be implemented very efﬁciently, and it does not require any
extra (stochastic) gradient computations if compared with classical SARAH. The only extra cost
per iteration is to perform two backward passes, i.e., one pass for ξ′
t(0) and the other for ξ′′
t (0); see
Appendix A.2 for implementation details.
As shown on Lines 11-16 of Algorithm 1, αmax is updated at every inner iteration. Speciﬁcally, the
algorithm starts without an upper bound (i.e., αmax = ∞on Line 3); as ˜αt−1 being computed at
every t ≥1, we employs the exponential smoothing on the harmonic mean of {˜αt−1} to update the
upper-bound. For k ≥0 and t ≥1, we deﬁne αmax =
1
δk
t , where"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.0992063492063492,"δk
t ="
COMPUTE STEP-SIZE AND UPPER-BOUND,0.10052910052910052,"(
1
˜αt−1 ,
k = 0, t = 1
βδk
t−1 + (1 −β)
1
˜αt−1 ,
otherwise"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.10185185185185185,"and 0 < β < 1. We default β = 0.999 in Algorithm 1; see Appendix A.2 for details on the design of
the adaptive upper-bound."
COMPUTE STEP-SIZE AND UPPER-BOUND,0.10317460317460317,"3.3
CHOICE OF γ"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.10449735449735449,"We perform a sensitivity analysis on different choices of γ. Figures 2 shows the evolution of
the squared norm of full gradient, i.e., ∥∇P(w)∥2, for logistic regression on binary classiﬁcation
problems; see extended results in Appendix A. It is clear that the performance of γ’s, where,
γ ∈{1/8, 1/16, 1/32, 1/64}, is consistent with only marginal improvement by using a smaller value.
We default γ = 1/32 in Algorithm 1."
COMPUTE STEP-SIZE AND UPPER-BOUND,0.10582010582010581,Under review as a conference paper at ICLR 2022
COMPUTE STEP-SIZE AND UPPER-BOUND,0.10714285714285714,"0
5
10
15
20 10
12 10
10 10
8 10
6 10
4 10
2"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.10846560846560846,|| P(w)||2
COMPUTE STEP-SIZE AND UPPER-BOUND,0.10978835978835978,ijcnn1
COMPUTE STEP-SIZE AND UPPER-BOUND,0.1111111111111111,"1/64
1/32
1/16
1/8
1/4
1/2"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.11243386243386243,"0
5
10
15
20 10
10 10
9 10
8 10
7 10
6 10
5 10
4 10
3 rcv1"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.11375661375661375,"0
5
10
15
20 10
10 10
8 10
6 10
4 10
2"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.11507936507936507,real-sim
COMPUTE STEP-SIZE AND UPPER-BOUND,0.1164021164021164,"0
5
10
15
20 10
8 10
7 10
6 10
5 10
4"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.11772486772486772,news20
COMPUTE STEP-SIZE AND UPPER-BOUND,0.11904761904761904,"0
5
10
15
20 10
10 10
8 10
6 10
4"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.12037037037037036,covtype
COMPUTE STEP-SIZE AND UPPER-BOUND,0.12169312169312169,"0
5
10
15
20
Effective Pass 10
10 10
8 10
6 10
4 10
2"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.12301587301587301,|| P(w)||2
COMPUTE STEP-SIZE AND UPPER-BOUND,0.12433862433862433,"1/64
1/32
1/16
1/8
1/4
1/2"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.12566137566137567,"0
5
10
15
20
Effective Pass 10
7 10
6 10
5 10
4 10
3"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.12698412698412698,"0
5
10
15
20
Effective Pass 10
8 10
7 10
6 10
5 10
4 10
3 10
2"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.1283068783068783,"0
5
10
15
20
Effective Pass 10
6 10
5 10
4"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.12962962962962962,"0
5
10
15
20
Effective Pass 10
9 10
8 10
7 10
6 10
5 10
4 10
3"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.13095238095238096,"Figure 2: Evolution of ∥∇P(w)∥2 for γ ∈{ 1 64, 1 32, 1 16, 1 8, 1 4, 1"
COMPUTE STEP-SIZE AND UPPER-BOUND,0.13227513227513227,"2}: regularized (top row) and non-
regularized (bottom row) logistic regression on ijcnn1, rcv1, real-sim, news20 and covtype."
CONVERGENCE ANALYSIS,0.1335978835978836,"3.4
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.1349206349206349,"In this section, we provide a convergence analysis of AI-SARAH (Algorithm 1) with a i) modiﬁed
line 10 to ˜αt−1 ≈arg minα∈[αk
min,αkmax] ξt(α), and ii) replacing the while loop with a for loop
t ∈[m], where αk
min and αk
max are step-size bounds picked in each outer iteration and m be a
hyper-parameter.
For brevity, let us just hint that for many problems (e.g., the one mentioned in Chapter 2), as the
solution wt is approaching w∗the local curvature of P(w) is getting ﬂatter and hence αk
max could be
chosen as a fraction of reciprocal of a smoothness parameter of fi(w)’s over set"
CONVERGENCE ANALYSIS,0.13624338624338625,"Wk := {w ∈Rd | ∥˜wk−1 −w∥≤m · αk
max∥v0∥},
(6)"
CONVERGENCE ANALYSIS,0.13756613756613756,"which can be much larger than the fraction of reciprocal of a global smoothness parameter. Let us
just remark that for k-th outer loop, all iterates wt will stay inside of Wk and hence αk
max is well
deﬁned. The parameter 0 < αk
min ≤αk
max can be chosen arbitrary (e.g., αk
min = αk
max/2 or even
αk
min = αk
max). We defer the technical details and proofs in Appendix B and C. Let us just state the
main convergence theorem here.
Theorem 3.1. Suppose that the functions fi(w) are convex and smooth with parameter Lmax
k
over
Wk and P is µ-strongly convex. Let us deﬁne"
CONVERGENCE ANALYSIS,0.1388888888888889,"σk
m =
1
µαk
min(m+1) + αk
max
αk
min ·
αk
maxLmax
k
2−αkmaxLmax
k
,"
CONVERGENCE ANALYSIS,0.1402116402116402,"and select m and αk
max such that σk
m < 1, ∀k ≥1. Then, modiﬁed Algorithm 1 converges as follows:"
CONVERGENCE ANALYSIS,0.14153439153439154,"E[∥∇P( ˜wk)∥2] ≤
Qk
ℓ=1σℓ
m

∥∇P( ˜w0)∥2."
CONVERGENCE ANALYSIS,0.14285714285714285,"Remark: SARAH algorithm is a special case of the modiﬁed Algorithm 1 when ∀k: αk
min = αk
max =
α ≤
1
2L."
NUMERICAL EXPERIMENT,0.14417989417989419,"4
NUMERICAL EXPERIMENT"
NUMERICAL EXPERIMENT,0.1455026455026455,"In this chapter, we present the empirical study on the performance of AI-SARAH. For brevity, we
present a subset of experiments in the main paper, and defer the full experimental results and imple-
mentation details4 in Appendix A.
The problems we consider in the experiment are ℓ2-regularized logistic regression for binary classiﬁ-
cation problems; see Appendix A for non-regularized case. Given a training sample (xi, yi) indexed
by i ∈[n], the component function fi is in the form fi(w) = log(1 + exp(−yixT
i w)) + λ"
NUMERICAL EXPERIMENT,0.14682539682539683,"2 ∥w∥2,"
NUMERICAL EXPERIMENT,0.14814814814814814,4Code will be made available upon publication.
NUMERICAL EXPERIMENT,0.14947089947089948,Under review as a conference paper at ICLR 2022
NUMERICAL EXPERIMENT,0.15079365079365079,"Table 1: Summary of Datasets from Chang & Lin (2011).
Dataset
# features
n (# Train)
# Test
% Sparsity
ijcnn11
22
49,990
91,701
40.91
rcv11
47,236
20,242
677,399
99.85
real-sim2
20,958
54,231
18,078
99.76
news202
1,355,191
14,997
4,999
99.97
covtype2
54
435,759
145,253
77.88"
NUMERICAL EXPERIMENT,0.15211640211640212,"1 dataset has default training/testing sanples.
2 dataset is randomly split by 75%-training & 25%-testing."
NUMERICAL EXPERIMENT,0.15343915343915343,where λ = 1
NUMERICAL EXPERIMENT,0.15476190476190477,"n for the ℓ2-regularized case and λ = 0 for the non-regularized case.
The datasets chosen for the experiments are ijcnn1, rcv1, real-sim, news20 and covtype. Table 1 shows
the basic statistics of the datasets. More details and additional datasets can be found in Appendix A.
We compare AI-SARAH with SARAH, SARAH+, SVRG (Johnson & Zhang, 2013), ADAM (Kingma
& Ba, 2015) and SGD with Momentum (Sutskever et al., 2013; Loizou & Richtárik, 2020; 2017).
While AI-SARAH does not require hyper-parameter tuning, we ﬁne-tune each of the other al-
gorithms, which yields ≈5, 000 runs in total for each dataset and case.
To be speciﬁc, we perform an extensive search on hyper-parameters: (1) ADAM and SGD with
Momentum (SGD w/m) are tuned with different values of the (initial) step-size and schedules to
reduce the step-size; (2) SARAH and SVRG are tuned with different values of the (constant) step-size
and inner loop size; (3) SARAH+ is tuned with different values of the (constant) step-size and early
stopping parameter. (See Appendix A for detailed tuning plan and the selected hyper-parameters.)
Figure 3 shows the average and total wall clock running time of AI-SARAH and the other algorithms.
While any individual run of AI-SARAH could be 2-5x more time consuming than the other algorithms,
its running time is negligible if comparing the total wall clock time. The reason is that AI-SARAH
does not require any tuning effort, but we have ≈5, 000 runs to ﬁne-tune the other algorithms. Figure
4 shows the minimum ∥∇P(w)∥2 achieved at a few points of effective passes and wall clock time
horizon. It is clear that, AI-SARAH’s practical speed of convergence is faster than the other algorithms
in most cases. Here, we argue that, if given an optimal implementation of AI-SARAH (just as that of
ADAM and other built-in optimizer in Pytorch5 ), it is likely that our algorithm can be accelerated.
By selecting the ﬁne-tuned hyper-parameters of all other algorithms, we compare them with AI-
SARAH and show the results in Figures 5-7. For these experiments, we use 10 distinct random seeds
to initialize w and generate stochastic mini-batches. And, we use the marked dashes to represent the
average and ﬁlled areas for 97% conﬁdence intervals.
Figure 5 presents the evolution of ∥∇P(w)∥2. Obviously from the ﬁgure, AI-SARAH exhibits the
strongest performance in terms of converging to a stationary point: by effective pass, the consis-
tently large gaps are displayed between AI-SARAH and the rest; by wall clock time, we notice that
AI-SARAH achieves the smallest ∥∇P(w)∥2 at the same time point. This validates our design, that is
to leverage local Lipschitz smoothness and achieve a faster convergence than SARAH and SARAH+.
In terms of minimizing the ﬁnite-sum functions, Figure 6 shows that, by effective pass, AI-SARAH
consistently outperforms SARAH and SARAH+ on all of the datasets with a possible exception on
covtype dataset. By wall clock time, AI-SARAH yields a competitive performance on all of the
datasets, and it delivers a stronger performance on ijcnn1 and real-sim than SARAH.
For completeness of illustration on the performance, we show the testing accuracy in Figure 7. Clearly,
ﬁne-tuned ADAM dominates the competition. However, AI-SARAH outperforms the other variance
reduced methods on most of the datasets from both effective pass and wall clock time perspectives,
and achieves the similar levels of accuracy as ADAM does on rcv1, real-sim and covtype datasets.
Having illustrated the strong performance of AI-SARAH, we continue the presentation by showing
the trajectories of the adaptive step-size and upper-bound in Figure 8.
This ﬁgure clearly shows that why AI-SARAH can achieve such a strong performance, especially on
the convergence to a stationary point. As mentioned in previous chapters, the adaptivity is driven by
the local Lipschitz smoothness. As shown in Figure 8, AI-SARAH starts with conservative step-size
and upper-bound, both of which continue to increase while the algorithm progresses towards a
stationary point. After a few effective passes, we observe: the step-size and upper-bound are stablized"
NUMERICAL EXPERIMENT,0.15608465608465608,5Please see https://pytorch.org/docs/stable/optim.html for Pytorch built-in optimizers.
NUMERICAL EXPERIMENT,0.1574074074074074,Under review as a conference paper at ICLR 2022
NUMERICAL EXPERIMENT,0.15873015873015872,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m 20 30 40 50 60"
NUMERICAL EXPERIMENT,0.16005291005291006,Avg. Wall Clock (sec)
NUMERICAL EXPERIMENT,0.16137566137566137,ijcnn1 40 50 60 70 80
NUMERICAL EXPERIMENT,0.1626984126984127,"90
rcv1 20 30 40 50 60 70 80 90 100"
NUMERICAL EXPERIMENT,0.164021164021164,real-sim 40 60 80 100 120 140
NUMERICAL EXPERIMENT,0.16534391534391535,news20 200 400 600 800
COVTYPE,0.16666666666666666,"1000
covtype"
COVTYPE,0.167989417989418,Algorithm 103 104
COVTYPE,0.1693121693121693,Total Wall Clock (sec)
COVTYPE,0.17063492063492064,Algorithm 103 104 105
COVTYPE,0.17195767195767195,Algorithm 103 104
COVTYPE,0.17328042328042328,Algorithm 104
COVTYPE,0.1746031746031746,Algorithm 104 105
COVTYPE,0.17592592592592593,"Figure 3: Average (top row) and total (bottom row) running time of AI-SARAH and other algorithms
for the regularized case."
COVTYPE,0.17724867724867724,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
COVTYPE,0.17857142857142858,"5
10
20
Effective Pass 10
14 10
12 10
10 10
8 10
6 10
4 10
2"
COVTYPE,0.17989417989417988,|| P(w)||2
COVTYPE,0.18121693121693122,ijcnn1
COVTYPE,0.18253968253968253,"5
15
30
Effective Pass 10
12 10
10 10
8 10
6 10
4 rcv1"
COVTYPE,0.18386243386243387,"5
10
20
Effective Pass 10
11 10
9 10
7 10
5 10
3"
COVTYPE,0.18518518518518517,real-sim
COVTYPE,0.1865079365079365,"5
20
40
Effective Pass 10
11 10
9 10
7 10
5"
COVTYPE,0.18783068783068782,"10
3
news20"
COVTYPE,0.18915343915343916,"5
10
20
Effective Pass 10
10 10
8 10
6 10
4"
COVTYPE,0.19047619047619047,covtype
COVTYPE,0.1917989417989418,"10.0
29.0
64.0
Wall Clock (sec) 10
14 10
12 10
10 10
8 10
6 10
4"
COVTYPE,0.1931216931216931,|| P(w)||2
COVTYPE,0.19444444444444445,"10.0
42.0
88.0
Wall Clock (sec) 10
12 10
10 10
8 10
6 10
4"
COVTYPE,0.19576719576719576,"10.0
45.0
103.0
Wall Clock (sec) 10
11 10
9 10
7 10
5 10
3"
COVTYPE,0.1970899470899471,"10.0
66.0
147.0
Wall Clock (sec) 10
11 10
9 10
7 10
5 10
3"
COVTYPE,0.1984126984126984,"10.0
245.0
957.0
Wall Clock (sec) 10
10 10
8 10
6 10
4"
COVTYPE,0.19973544973544974,"Figure 4: Running minimum per effective pass (top row) and wall clock time (bottom row) of
∥∇P(w)∥2 between other algorithms with all hyper-parameters conﬁgurations and AI-SARAH for the
regularized case. Note: the horizontal dashes in blue represent the minimum ∥∇P(w)∥2 achieved
by AI-SARAH at certain effective pass or time point."
COVTYPE,0.20105820105820105,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
COVTYPE,0.20238095238095238,"0
5
10
15
20
Effective Pass 10
13 10
11 10
9 10
7 10
5 10
3 10
1"
COVTYPE,0.2037037037037037,|| P(w)||2
COVTYPE,0.20502645502645503,ijcnn1
COVTYPE,0.20634920634920634,"0
10
20
30
Effective Pass 10
12 10
10 10
8 10
6 10
4"
COVTYPE,0.20767195767195767,"10
2
rcv1"
COVTYPE,0.20899470899470898,"0
5
10
15
20
Effective Pass 10
11 10
9 10
7 10
5 10
3"
COVTYPE,0.21031746031746032,"10
1
real-sim"
COVTYPE,0.21164021164021163,"0
10
20
30
40
Effective Pass 10
11 10
9 10
7 10
5 10
3"
COVTYPE,0.21296296296296297,news20
COVTYPE,0.21428571428571427,"0
5
10
15
20
Effective Pass 10
10 10
8 10
6 10
4"
COVTYPE,0.2156084656084656,"10
2
covtype"
COVTYPE,0.21693121693121692,"0
25
50
75
100
125
Wall Clock (sec) 10
13 10
11 10
9 10
7 10
5 10
3 10
1"
COVTYPE,0.21825396825396826,|| P(w)||2
COVTYPE,0.21957671957671956,"0
20
40
60
80
Wall Clock (sec) 10
12 10
10 10
8 10
6 10
4 10
2"
COVTYPE,0.2208994708994709,"0
25
50
75
100
Wall Clock (sec) 10
11 10
9 10
7 10
5 10
3 10
1"
COVTYPE,0.2222222222222222,"0
50
100
150
Wall Clock (sec) 10
11 10
9 10
7 10
5 10
3"
COVTYPE,0.22354497354497355,"0
200
400
600
800
1000
Wall Clock (sec) 10
10 10
8 10
6 10
4 10
2"
COVTYPE,0.22486772486772486,"Figure 5: Evolution of ∥∇P(w)∥2 for the regularized case by effective pass (top row) and wall
clock time (bottom row)."
COVTYPE,0.2261904761904762,Under review as a conference paper at ICLR 2022
COVTYPE,0.2275132275132275,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
COVTYPE,0.22883597883597884,"0
5
10
15
20
Effective Pass 0.20 0.22 0.24 0.26 0.28 0.30 P(w)"
COVTYPE,0.23015873015873015,ijcnn1
COVTYPE,0.23148148148148148,"0
10
20
30
Effective Pass 0.20 0.22 0.24 0.26 0.28"
COVTYPE,0.2328042328042328,"0.30
rcv1"
COVTYPE,0.23412698412698413,"0
5
10
15
20
Effective Pass 0.16 0.18 0.20 0.22 0.24 0.26 0.28"
REAL-SIM,0.23544973544973544,"0.30
real-sim"
REAL-SIM,0.23677248677248677,"0
10
20
30
40
Effective Pass 0.34 0.36 0.38 0.40"
REAL-SIM,0.23809523809523808,"0.42
news20"
REAL-SIM,0.23941798941798942,"0
5
10
15
20
Effective Pass 0.520 0.525 0.530 0.535"
COVTYPE,0.24074074074074073,"0.540
covtype"
COVTYPE,0.24206349206349206,"0
25
50
75
100
125
Wall Clock (sec) 0.20 0.22 0.24 0.26 0.28 0.30 P(w)"
COVTYPE,0.24338624338624337,"0
20
40
60
80
Wall Clock (sec) 0.20 0.22 0.24 0.26 0.28 0.30"
COVTYPE,0.2447089947089947,"0
25
50
75
100
Wall Clock (sec) 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30"
COVTYPE,0.24603174603174602,"0
50
100
150
Wall Clock (sec) 0.34 0.36 0.38 0.40 0.42"
COVTYPE,0.24735449735449735,"0
200
400
600
800
1000
Wall Clock (sec) 0.520 0.525 0.530 0.535 0.540"
COVTYPE,0.24867724867724866,"Figure 6: Evolution of P(w) for the regularized case by effective pass (top row) and wall clock time
(bottom row)."
COVTYPE,0.25,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
COVTYPE,0.25132275132275134,"0
5
10
15
20
Effective Pass 0.900 0.905 0.910 0.915 0.920 0.925"
COVTYPE,0.2526455026455027,Accuracy
COVTYPE,0.25396825396825395,ijcnn1
COVTYPE,0.2552910052910053,"0
10
20
30
Effective Pass 0.940 0.945 0.950 0.955"
COVTYPE,0.2566137566137566,"0.960
rcv1"
COVTYPE,0.25793650793650796,"0
5
10
15
20
Effective Pass 0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97"
COVTYPE,0.25925925925925924,real-sim
COVTYPE,0.2605820105820106,"0
10
20
30
40
Effective Pass 0.900 0.905 0.910 0.915 0.920 0.925 0.930 0.935"
COVTYPE,0.2619047619047619,news20
COVTYPE,0.26322751322751325,"0
5
10
15
20
Effective Pass 0.70 0.71 0.72 0.73 0.74 0.75"
COVTYPE,0.26455026455026454,"0.76
covtype"
COVTYPE,0.26587301587301587,"0
25
50
75
100
125
Wall Clock (sec) 0.900 0.905 0.910 0.915 0.920 0.925"
COVTYPE,0.2671957671957672,Accuracy
COVTYPE,0.26851851851851855,"0
20
40
60
80
Wall Clock (sec) 0.940 0.945 0.950 0.955 0.960"
COVTYPE,0.2698412698412698,"0
25
50
75
100
Wall Clock (sec) 0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97"
COVTYPE,0.27116402116402116,"0
50
100
150
Wall Clock (sec) 0.900 0.905 0.910 0.915 0.920 0.925 0.930 0.935"
COVTYPE,0.2724867724867725,"0
200
400
600
800
1000
Wall Clock (sec) 0.70 0.71 0.72 0.73 0.74 0.75 0.76"
COVTYPE,0.27380952380952384,"Figure 7: Running maximum of testing accuracy for the regularized case by effective pass (top row)
and wall clock time (bottom row)."
COVTYPE,0.2751322751322751,"0
5
10
15
20
Effective Pass 10 20 30 40"
COVTYPE,0.27645502645502645,Step-Size
COVTYPE,0.2777777777777778,ijcnn1 max
COVTYPE,0.2791005291005291,"0
10
20
30
Effective Pass 5 10 15 20 25"
COVTYPE,0.2804232804232804,Step-Size rcv1
COVTYPE,0.28174603174603174,"0
5
10
15
20
Effective Pass 5 10 15 20 25 30 35 40"
COVTYPE,0.2830687830687831,Step-Size
COVTYPE,0.2843915343915344,real-sim
COVTYPE,0.2857142857142857,"0
10
20
30
40
Effective Pass 6 8 10 12 14"
COVTYPE,0.28703703703703703,Step-Size
COVTYPE,0.28835978835978837,news20
COVTYPE,0.2896825396825397,"0
5
10
15
20
Effective Pass 3 4 5 6 7 8 9"
COVTYPE,0.291005291005291,Step-Size
COVTYPE,0.2923280423280423,covtype
COVTYPE,0.29365079365079366,Figure 8: Evolution of AI-SARAH’s step-size α and upper-bound αmax for the regularized case.
COVTYPE,0.294973544973545,"due to λ (and hence strong convexity). In Appendix A, we can see that, as a result of the function
being unregularized, the step-size and upper-bound could be continuously increasing due to the fact
that the function is likely non-strongly convex."
CONCLUSION,0.2962962962962963,"5
CONCLUSION"
CONCLUSION,0.2976190476190476,"In this paper, we propose AI-SARAH, a practical variant of stochastic recursive gradient methods.
The idea of design is simple yet powerful: by taking advantage of local Lipschitz smoothness, the
step-size can be dynamically determined. With intuitive illustration and implementation details,
we show how AI-SARAH can efﬁciently estimate local Lipschitz smoothness and how it can be
easily implemented in practice. Our algorithm is tune-free and adaptive at full scale. With extensive
numerical experiment, we demonstrate that, without (tuning) any hyper-parameters, it delivers
a competitive performance compared with SARAH(+), ADAM and other ﬁrst-order methods, all
equipped with ﬁne-tuned hyper-parameters."
CONCLUSION,0.29894179894179895,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3002645502645503,ETHICS STATEMENT
ETHICS STATEMENT,0.30158730158730157,"This work presents a new algorithm for training machine learning models. We do not foresee any
ethical concerns. All datasets used in this work are from the public domain and are commonly used
benchmarks in ML papers."
REPRODUCIBILITY STATEMENT,0.3029100529100529,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.30423280423280424,"We uploaded all the codes used to make all the experiments presented in this paper. We have used
random seeds to ensure that one can start optimizing the ML models from the same initial starting
point as was used in the experiments. We have used only datasets that are in the public domain,
and one can download them from the following website https://www.csie.ntu.edu.tw/
~cjlin/libsvmtools/datasets/. After acceptance, we will include a link to the GitHub
repository where we will host the source codes."
REPRODUCIBILITY STATEMENT,0.3055555555555556,Under review as a conference paper at ICLR 2022
REFERENCES,0.30687830687830686,REFERENCES
REFERENCES,0.3082010582010582,"Yoshua Bengio. Rmsprop and equilibrated adaptive learning rates for nonconvex optimization. corr
abs/1502.04390, 2015."
REFERENCES,0.30952380952380953,"Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223–311, 2018."
REFERENCES,0.31084656084656087,"Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM
transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011."
REFERENCES,0.31216931216931215,"A. Defazio. A simple practical accelerated method for ﬁnite sums. In NeurIPS, 2016."
REFERENCES,0.3134920634920635,"Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, volume 27, pp. 1646–1654. Curran Associates, Inc., 2014."
REFERENCES,0.3148148148148148,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(Jul):2121–2159, 2011."
REFERENCES,0.31613756613756616,"Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.31746031746031744,"Robert M Gower, Peter Richtárik, and Francis Bach. Stochastic quasi-gradient methods: Variance
reduction via jacobian sketching. Mathematical Programming, pp. 1–58, 2020a."
REFERENCES,0.3187830687830688,"Robert M Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions:
Learning rates, minibatching and interpolation. arXiv preprint arXiv:2006.10311, 2020b."
REFERENCES,0.3201058201058201,"Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtárik. Sgd: General analysis and improved rates. In International Conference on Machine
Learning, pp. 5200–5209, 2019."
REFERENCES,0.32142857142857145,"Samuel Horváth, Lihua Lei, Peter Richtárik, and Michael I. Jordan. Adaptivity of stochastic gradient
methods for nonconvex optimization. arXiv preprint arXiv:2002.05359, 2020."
REFERENCES,0.32275132275132273,"Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, volume 26, pp. 315–323. Curran
Associates, Inc., 2013."
REFERENCES,0.32407407407407407,"Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M. Gower, and Peter Richtárik. Uniﬁed
analysis of stochastic gradient methods for composite convex and smooth optimization. arXiv
preprint arXiv:2006.11573, 2020."
REFERENCES,0.3253968253968254,"Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."
REFERENCES,0.32671957671957674,"J. Koneˇcný, J. Liu, P. Richtárik, and M. Takáˇc. Mini-batch semi-stochastic gradient descent in the
proximal setting. IEEE Journal of Selected Topics in Signal Processing, 10(2):242–255, 2016."
REFERENCES,0.328042328042328,"Bingcong Li and Georgios B Giannakis. Adaptive step sizes in variance reduction via regularization.
arXiv preprint arXiv:1910.06532, 2019."
REFERENCES,0.32936507936507936,"Bingcong Li, Lingda Wang, and Georgios B. Giannakis. Almost tune-free variance reduction.
In Proceedings of the 37th International Conference on Machine Learning, volume 119, pp.
5969–5978. PMLR, 2020."
REFERENCES,0.3306878306878307,"Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018."
REFERENCES,0.33201058201058203,"Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019a."
REFERENCES,0.3333333333333333,"Yan Liu, Congying Han, and Tiande Huo. A class of stochastic variance reduced methods with an
adaptive stepsize. 2019b. URL http://www.optimization-online.org/DB_FILE/
2019/04/7170.pdf."
REFERENCES,0.33465608465608465,Under review as a conference paper at ICLR 2022
REFERENCES,0.335978835978836,"Nicolas Loizou and Peter Richtárik. Linearly convergent stochastic heavy ball method for minimizing
generalization error. arXiv preprint arXiv:1710.10737, 2017."
REFERENCES,0.3373015873015873,"Nicolas Loizou and Peter Richtárik. Momentum and stochastic momentum for stochastic gradi-
ent, newton, proximal point and subspace descent methods. Computational Optimization and
Applications, 77(3):653–710, 2020."
REFERENCES,0.3386243386243386,"Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-
size for sgd: An adaptive learning rate for fast convergence. arXiv preprint arXiv:2002.10542,
2020."
REFERENCES,0.33994708994708994,"Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation algorithms
for machine learning. In Advances in Neural Information Processing Systems, pp. 451–459, 2011."
REFERENCES,0.3412698412698413,"D. Needell, N. Srebro, and R. Ward. Stochastic gradient descent, weighted sampling, and the
randomized kaczmarz algorithm. Mathematical Programming, Series A, 155(1):549–573, 2016."
REFERENCES,0.3425925925925926,"Arkadi Nemirovski and David B. Yudin. Problem complexity and method efﬁciency in optimization.
Wiley Interscience, 1983."
REFERENCES,0.3439153439153439,"Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–
1609, 2009."
REFERENCES,0.34523809523809523,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.34656084656084657,"Lam Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richtárik, Katya Scheinberg, and Martin
Takáˇc. SGD and hogwild! Convergence without the bounded gradients assumption. In Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 3750–3758. PMLR, 2018."
REFERENCES,0.3478835978835979,"Lam M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Takáˇc. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In Proceedings of the 34th International Con-
ference on Machine Learning (ICML 2000), volume 70, pp. 2613–2621, International Convention
Centre, Sydney, Australia, 2017. PMLR."
REFERENCES,0.3492063492063492,"H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics,
pp. 400–407, 1951."
REFERENCES,0.3505291005291005,"M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient.
Math. Program., 162(1-2):83–112, 2017."
REFERENCES,0.35185185185185186,"Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: primal estimated subgradient
solver for SVM. In 24th International Conference on Machine Learning, pp. 807–814, 2007."
REFERENCES,0.3531746031746032,"Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pp. 1139–1147.
PMLR, 2013."
REFERENCES,0.3544973544973545,"Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian. Barzilai-borwein step size for stochastic
gradient descent. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, pp. 685–693, 2016."
REFERENCES,0.3558201058201058,"Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 32, pp. 3732–3745. Curran Associates, Inc., 2019."
REFERENCES,0.35714285714285715,"Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes. In International Conference on Machine Learning, pp. 6677–6686, 2019."
REFERENCES,0.3584656084656085,"Zhuang Yang, Zengping Chen, and Cheng Wang. Accelerating mini-batch sarah by step size rules.
Information Sciences, 2021. ISSN 0020-0255. doi: https://doi.org/10.1016/j.ins.2020.12.075."
REFERENCES,0.35978835978835977,Under review as a conference paper at ICLR 2022
REFERENCES,0.3611111111111111,APPENDIX
REFERENCES,0.36243386243386244,"The Appendix is organized as follows. In Chapter A, we present extended details on the design,
implementation and results of our numerical experiments. In Chapter B, we present the theoretical
analysis of AI-SARAH. In Chapter C, we provide the basic deﬁnitions, some existing technical
preliminaries that are used in our results, and the proofs of the main lemmas and theorems from
Chapter B."
REFERENCES,0.3637566137566138,"A
EXTENDED DETAILS ON NUMERICAL EXPERIMENT"
REFERENCES,0.36507936507936506,"In this chapter, we present the extended details of the design, implementation and results of the
numerical experiments."
REFERENCES,0.3664021164021164,"A.1
PROBLEM AND DATA"
REFERENCES,0.36772486772486773,"The machine learning tasks studied in the experiment are binary classiﬁcation problems. As a
common practice in the empirical research of optimization algorithms, the LIBSVM datasets6 are
chosen to deﬁne the tasks. Speciﬁcally, we selected 10 popular binary class datasets: ijcnn1, rcv1,
news20, covtype, real-sim, a1a, gisette, w1a, w8a and mushrooms (see Table 2 for basic statistics
of the datasets)."
REFERENCES,0.36904761904761907,"Table 2: Summary of Datasets.
Dataset
d −1 (# feature)
n (# Train)
ntest (# Test)
% Sparsity
ijcnn11
22
49,990
91,701
40.91
rcv11
47,236
20,242
677,399
99.85
news202
1,355,191
14,997
4,999
99.97
covtype2
54
435,759
145,253
77.88
real-sim2
20,958
54,231
18,078
99.76
a1a1
123
1,605
30,956
88.73
gisette1
5,000
6,000
1,000
0.85
w1a1
300
2,477
47,272
96.11
w8a1
300
49,749
14,951
96.12
mushrooms2
112
6,093
2,031
81.25"
REFERENCES,0.37037037037037035,"1 dataset has default training/testing samples.
2 dataset is randomly split by 75%-training & 25%-testing."
REFERENCES,0.3716931216931217,"A.1.1
DATA PRE-PROCESSING"
REFERENCES,0.373015873015873,"Let (χi, yi) be a training (or testing) sample indexed by i ∈[n] (or i ∈[ntest]), where χi ∈Rd−1
is a feature vector and yi is a label. We pre-processed the data such that χi is of a unit length in
Euclidean norm and yi ∈{−1, +1}."
REFERENCES,0.37433862433862436,"A.1.2
MODEL AND LOSS FUNCTION"
REFERENCES,0.37566137566137564,"The selected model, hi : Rd 7→R, is in the linear form"
REFERENCES,0.376984126984127,"hi(ω, ε) = χT
i ω + ε,
∀i ∈[n],
(7)"
REFERENCES,0.3783068783068783,where ω ∈Rd−1 is a weight vector and ε ∈R is a bias term.
REFERENCES,0.37962962962962965,"For simplicity of notation, from now on, we let xi
def
= [χT
i 1]T ∈Rd be an augmented feature vector,
w
def
= [ωT ε]T ∈Rd be a parameter vector, and hi(w) = xT
i w for i ∈[n]."
REFERENCES,0.38095238095238093,"6LIBSVM datasets are available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
datasets/."
REFERENCES,0.38227513227513227,Under review as a conference paper at ICLR 2022
REFERENCES,0.3835978835978836,"Given a training sample indexed by i ∈[n], the loss function is deﬁned as a logistic regression"
REFERENCES,0.38492063492063494,fi(w) = log(1 + exp(−yihi(w)) + λ
REFERENCES,0.3862433862433862,"2 ∥w∥2.
(8)"
REFERENCES,0.38756613756613756,"In (8), λ"
REFERENCES,0.3888888888888889,"2 ∥w∥2 is the ℓ2-regularization of a particular choice of λ > 0, where we used λ = 1"
REFERENCES,0.39021164021164023,"n in the
experiment; for the non-regularized case, λ was set to 0. Accordingly, the ﬁnite-sum minimization
problem we aimed to solve is deﬁned as"
REFERENCES,0.3915343915343915,"min
w∈Rd"
REFERENCES,0.39285714285714285,"
P(w)
def
= 1 n n
X"
REFERENCES,0.3941798941798942,"i=1
fi(w)

.
(9)"
REFERENCES,0.3955026455026455,"Note that (9) is a convex function.
For the ℓ2-regularized case, i.e., λ = 1/n in (8), (9) is
µ-strongly convex and µ = 1"
REFERENCES,0.3968253968253968,"n. However, without the λ, i.e., λ = 0 in (8), (9) is µ-strongly convex if
and only if there there exists µ > 0 such that ∇2P(w) ⪰µI for w ∈Rd (provided ∇P(w) ∈C)."
REFERENCES,0.39814814814814814,"A.2
ALGORITHMS"
REFERENCES,0.3994708994708995,"This section provides the implementation details7 of the algorithms, practical consideration, and
discussions."
REFERENCES,0.4007936507936508,"A.2.1
TUNE-FREE AI-SARAH"
REFERENCES,0.4021164021164021,"In Chapter 3 of the main paper, we introduced AI-SARAH, a tune-free and fully adaptive algo-
rithm. The implementation of Algorithm 1 was quite straightforward, and we highlight the
implementation of Line 10 with details: for logistic regression, the one-dimensional (constrained
optimization) sub-problem minα>0 ξt(α) can be approximately solved by computing the Newton
step at α = 0, i.e., ˜αt−1 = −ξ′
t(0)
|ξ′′
t (0)|. This can be easily implemented with automatic differentiation in
Pytorch8, and only two additional backward passes w.r.t α is needed. For function in some particular
form, such as a linear least square loss function, an exact solution in closed form can be easily derived."
REFERENCES,0.40343915343915343,"As mentioned in Chapter 3, we have an adaptive upper-bound, i.e., αmax, in the algorithm. To be
speciﬁc, the algorithm starts without an upper-bound, i.e., αmax = ∞on Line 3 of Algorithm 1.
Then, αmax is updated per (inner) iteration. Recall in Chapter 3, αmax is computed as a harmonic
mean of the sequence, i.e., {˜αt−1}, and an exponential smoothing is applied on top of the simple
harmonic mean."
REFERENCES,0.40476190476190477,"Having an upper-bound stabilizes the algorithm from stochastic optimization perspective. For
example, when the training error of the randomly selected mini-batch at wt is drastically reduced
or approaching zero, the one-step Newton solution in (5) could be very large, i.e. ˜αt−1 ≫0,
which could be too aggressive to other mini-batch and hence Problem (1) prescribed by the batch.
On the other hand, making the upper-bound adaptive allows the algorithm to adapt to the
local geometry and avoid restrictions on using a large step-size when the algorithm tries to make
aggressive progress with respect to Problem (1). With the adaptive upper-bound being derived by an
exponential smoothing of the harmonic mean, the step-size is determined by emphasizing the
current estimate of local geometry while taking into account the history of the estimates. The
exponential smoothing further stabilizes the algorithm by balancing the trade-off of being locally
focused (with respect to fSt) and globally focused (with respect to P)."
REFERENCES,0.4060846560846561,"It is worthwhile to mention that Algorithm 1 does not require computing extra gradient of
fSt with respect to w if compared with SARAH and SARAH+. At each inner iteration, t ≥1,
Algorithm 1 computes ∇fSt(wt−1−αvt−1) with α = 0 just as SARAH and SARAH+ would compute
∇fSt(wt−1), and the only difference is that α is speciﬁed as a variable in Pytorch. After the adaptive
step-size αt−1 is determined (Line 17), Algorithm 1 computes ∇fSt(wt−1 −αt−1vt−1) just as
SARAH and SARAH+ would compute ∇fSt(wt)."
REFERENCES,0.4074074074074074,"In Chapter 3 of the main paper, we discussed the sensitivity of Algorithm 1 on the choice of γ.
Here, we present the full results (on 10 chosen datasets for both ℓ2-regularized and non-regularized"
REFERENCES,0.4087301587301587,"7Code will be made available upon publication.
8For detailed description of the automatic differentiation engine in Pytorch, please see https://pytorch.
org/tutorials/beginner/blitz/autograd_tutorial.html."
REFERENCES,0.41005291005291006,Under review as a conference paper at ICLR 2022 0.2 0.3 0.4 0.5 0.6 0.7 P(w)
REFERENCES,0.4113756613756614,ijcnn1
REFERENCES,0.4126984126984127,"1/64
1/32
1/16
1/8
1/4
1/2 0.2 0.3 0.4 0.5 0.6 0.7 rcv1 0.2 0.3 0.4 0.5 0.6 0.7"
REFERENCES,0.414021164021164,real-sim 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70
REFERENCES,0.41534391534391535,news20 0.525 0.550 0.575 0.600 0.625 0.650 0.675 0.700
REFERENCES,0.4166666666666667,"covtype 10
12 10
10 10
8 10
6 10
4 10
2"
REFERENCES,0.41798941798941797,"|| P(w)||2 10
10 10
9 10
8 10
7 10
6 10
5 10
4 10
3 10
10 10
8 10
6 10
4 10
2 10
8 10
7 10
6 10
5 10
4 10
10 10
8 10
6 10
4"
REFERENCES,0.4193121693121693,"0
5
10
15
20
Effective Pass 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.42063492063492064,Accuracy
REFERENCES,0.421957671957672,"0
5
10
15
20
Effective Pass 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.42328042328042326,"0
5
10
15
20
Effective Pass 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.4246031746031746,"0
5
10
15
20
Effective Pass 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.42592592592592593,"0
5
10
15
20
Effective Pass 0.50 0.55 0.60 0.65 0.70 0.75"
REFERENCES,0.42724867724867727,"Figure 9:
ℓ2-regularized case ijcnn1,
rcv1,
real-sim,
news20 and covtype with γ
∈
{ 1 64, 1 32, 1 16, 1 8, 1 4, 1"
REFERENCES,0.42857142857142855,"2}: evolution of P(w) (top row) and ∥∇P(w)∥2 (middle row) and running maxi-
mum of testing accuracy (bottom row). 0.40 0.45 0.50 0.55 0.60 0.65 0.70 P(w) a1a"
REFERENCES,0.4298941798941799,"1/64
1/32
1/16
1/8
1/4
1/2 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70"
REFERENCES,0.4312169312169312,gisette 0.1 0.2 0.3 0.4 0.5 0.6 0.7 w1a 0.1 0.2 0.3 0.4 0.5 0.6 0.7 w8a 0.1 0.2 0.3 0.4 0.5 0.6 0.7
REFERENCES,0.43253968253968256,"mushrooms 10
6 10
5 10
4 10
3 10
2 10
1"
REFERENCES,0.43386243386243384,"|| P(w)||2 10
7 10
6 10
5 10
4 10
3 10
6 10
5 10
4 10
3 10
2 10
1 10
11 10
9 10
7 10
5 10
3 10
1 10
8 10
7 10
6 10
5 10
4 10
3 10
2"
REFERENCES,0.4351851851851852,"0
5
10
15
20
Effective Pass 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85"
REFERENCES,0.4365079365079365,Accuracy
REFERENCES,0.43783068783068785,"0
5
10
15
20
Effective Pass 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.43915343915343913,"0
5
10
15
20
Effective Pass 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.44047619047619047,"0
5
10
15
20
Effective Pass 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.4417989417989418,"0
5
10
15
20
Effective Pass 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.44312169312169314,"Figure 10:
ℓ2-regularized case of a1a, gisette, w1a, w8a and mushrooms with γ
∈
{ 1 64, 1 32, 1 16, 1 8, 1 4, 1"
REFERENCES,0.4444444444444444,"2}: evolution of P(w) (top row) and ∥∇P(w)∥2 (middle row) and running maxi-
mum of testing accuracy (bottom row)."
REFERENCES,0.44576719576719576,"cases) in Figures 9, 10, 11, and 12. Note that, in this experiment, we chose γ ∈{ 1 64, 1 32, 1 16, 1 8, 1 4, 1"
REFERENCES,0.4470899470899471,"2},
and for each γ, dataset and case, we used 10 distinct random seeds and ran each experiment for 20
effective passes."
REFERENCES,0.44841269841269843,"A.2.2
OTHER ALGORITHMS"
REFERENCES,0.4497354497354497,"In our numerical experiment, we compared the performance of TUNE-FREE AI-SARAH (Algorithm
1) with that of 5 FINE-TUNED state-of-the-art (stochastic variance reduced or adaptive) ﬁrst-order"
REFERENCES,0.45105820105820105,Under review as a conference paper at ICLR 2022 0.2 0.3 0.4 0.5 0.6 0.7 P(w)
REFERENCES,0.4523809523809524,ijcnn1
REFERENCES,0.4537037037037037,"1/64
1/32
1/16
1/8
1/4
1/2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 rcv1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
REFERENCES,0.455026455026455,real-sim 0.1 0.2 0.3 0.4 0.5 0.6 0.7
REFERENCES,0.45634920634920634,news20 0.525 0.550 0.575 0.600 0.625 0.650 0.675 0.700
REFERENCES,0.4576719576719577,"covtype 10
10 10
8 10
6 10
4 10
2"
REFERENCES,0.458994708994709,"|| P(w)||2 10
7 10
6 10
5 10
4 10
3 10
8 10
7 10
6 10
5 10
4 10
3 10
2 10
6 10
5 10
4 10
9 10
8 10
7 10
6 10
5 10
4 10
3"
REFERENCES,0.4603174603174603,"0
5
10
15
20
Effective Pass 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.46164021164021163,Accuracy
REFERENCES,0.46296296296296297,"0
5
10
15
20
Effective Pass 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.4642857142857143,"0
5
10
15
20
Effective Pass 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.4656084656084656,"0
5
10
15
20
Effective Pass 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.4669312169312169,"0
5
10
15
20
Effective Pass 0.50 0.55 0.60 0.65 0.70 0.75"
REFERENCES,0.46825396825396826,"Figure 11:
Non-regularized case ijcnn1, rcv1, real-sim, news20 and covtype with γ
∈
{ 1 64, 1 32, 1 16, 1 8, 1 4, 1"
REFERENCES,0.4695767195767196,"2}: evolution of P(w) (top row) and ∥∇P(w)∥2 (middle row) and running maxi-
mum of testing accuracy (bottom row). 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 P(w) a1a"
REFERENCES,0.4708994708994709,"1/64
1/32
1/16
1/8
1/4
1/2 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
REFERENCES,0.4722222222222222,gisette 0.1 0.2 0.3 0.4 0.5 0.6 0.7 w1a 0.1 0.2 0.3 0.4 0.5 0.6 0.7 w8a 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
REFERENCES,0.47354497354497355,"mushrooms 10
5 10
4 10
3 10
2 10
1"
REFERENCES,0.4748677248677249,"|| P(w)||2 10
5 10
4 10
3 10
5 10
4 10
3 10
2 10
1 10
8 10
6 10
4 10
2 10
6 10
5 10
4 10
3 10
2"
REFERENCES,0.47619047619047616,"0
5
10
15
20
Effective Pass 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85"
REFERENCES,0.4775132275132275,Accuracy
REFERENCES,0.47883597883597884,"0
5
10
15
20
Effective Pass 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.4801587301587302,"0
5
10
15
20
Effective Pass 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.48148148148148145,"0
5
10
15
20
Effective Pass 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.4828042328042328,"0
5
10
15
20
Effective Pass 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.48412698412698413,"Figure 12:
Non-regularized case a1a,
gisette,
w1a,
w8a and mushrooms with γ
∈
{ 1 64, 1 32, 1 16, 1 8, 1 4, 1"
REFERENCES,0.48544973544973546,"2}: evolution of P(w) (top row) and ∥∇P(w)∥2 (middle row) and running maxi-
mum of testing accuracy (bottom row)."
REFERENCES,0.48677248677248675,Under review as a conference paper at ICLR 2022
REFERENCES,0.4880952380952381,Table 3: Tuning Plan - Choice of Hyper-parameters.
REFERENCES,0.4894179894179894,"Method
# Conﬁguration
Step-Size
Schedule (%)1
Inner Loop Size (# Effective Pass)
Early Stopping (γ)
SARAH
160
{0.1, 0.2, ..., 1}/L
n/a
{0.5, 0.6, ..., 2}
n/a
SARAH+
50
{0.1, 0.2, ..., 1}/L
n/a
n/a
1/{2, 4, 8, 16, 32}
SVRG
160
{0.1, 0.2, ..., 1}/L
n/a
{0.5, 0.6, ..., 2}
n/a
ADAM2
300
[10−3, 10]
{0, 1, 5, 10, 15}
n/a
n/a
SGD w/m3
300
[10−3, 10]
{0, 1, 5, 10, 15}
n/a
n/a"
REFERENCES,0.49074074074074076,"1 Step-size is scheduled to decrease by X% every effective pass over the training samples.
2 β1 = 0.9, β2 = 0.999.
3 β = 0.9."
REFERENCES,0.49206349206349204,Table 4: Running Budget (# Effective Pass).
REFERENCES,0.4933862433862434,"Dataset
Regularized
Non-regularized
ijcnn1
20
20
rcv1
30
40
news20
40
50
covtype
20
20
real-sim
20
30
a1a
30
40
gisette
30
40
w1a
40
50
w8a
30
40
mushrooms
30
40"
REFERENCES,0.4947089947089947,"methods: SARAH, SARAH+, SVRG, ADAM and SGD with Momentum (SGD w/m). These algorithms
were implemented in Pytorch, where ADAM and SGD w/m are built-in optimizers of Pytorch."
REFERENCES,0.49603174603174605,"Hyper-parameter tuning.
For ADAM and SGD w/m, we selected 60 different values of the (initial)
step-size on the interval [10−3, 10] and 5 different schedules to decrease the step-size after every
effective pass on the training samples; for SARAH and SVRG, we selected 10 different values of
the (constant) step-size and 16 different values of the inner loop size; for SARAH+, the values of
step-size were selected in the same way as that of SARAH and SVRG. In addition, we chose 5 different
values of the inner loop early stopping parameter. Table 3 presents the detailed tuning plan for these
algorithms."
REFERENCES,0.4973544973544973,Selection criteria:
REFERENCES,0.49867724867724866,"We deﬁned the best hyper-parameters as the ones yielding the minimum ending value of the loss
function, where the running budget is presented in Table 4. Speciﬁcally, the criteria are: (1) ﬁltering
out the ones exhibited a ""spike"" of the loss function, i.e., the initial value of the loss function is
surpassed at any point within the budget; (2) selecting the ones achieved the minimum ending value
of the loss function."
REFERENCES,0.5,Hightlights of the hyper-parameter search:
REFERENCES,0.5013227513227513,"• To take into account the randomness in the performance of these algorithms provided different
hyper-parameters, we ran each conﬁguration with 5 distinct random seeds. The total number of
runs for each dataset and case is 4, 850.
• Tables 5 and 6 present the best hyper-parameters selected from the candidates for the regularized
and non-regularized cases.
• Figures 13, 14, 15 and 16 show the performance of different hyper-parameters for all tuned
algorithms; it is clearly that, the performance is highly dependent on the choices of hyper-
parameter for SARAH, SARAH+, and SVRG. And, the performance of ADAM and SGD
w/m are very SENSITIVE to the choices of hyper-parameter."
REFERENCES,0.5026455026455027,"Global Lipschitz smoothness of P(w).
Tuning the (constant) step-size of SARAH, SARAH+ and
SVRG requires the parameter of (global) Lipschitz smoothness of P(w), denoted the (global) Lipschitz"
REFERENCES,0.503968253968254,Under review as a conference paper at ICLR 2022
REFERENCES,0.5052910052910053,"Table 5: Fine-tuned Hyper-parameters - ℓ2-regularized Case.
Dataset
ADAM
SGD w/m
SARAH
SARAH+
SVRG
(α0, x%)
(α0, x%)
(α, m)
(α, γ)
(α, m)
ijcnn1
(0.07, 15%)
(0.4, 15%)
(3.153, 1015)
(3.503, 1/32)
(3.503, 1562)
rcv1
(0.016, 10%)
(4.857, 10%)
(3.924, 600)
(3.924, 1/32)
(3.924, 632)
news20
(0.028, 15%)
(6.142, 10%)
(3.786, 468)
(3.786, 1/32)
(3.786, 468)
covtype
(0.07, 15%)
(0.4, 15%)
(2.447, 13616)
(2.447, 1/32)
(2.447, 13616)
real-sim
(0.16, 15%)
(7.428, 15%)
(3.165, 762)
(3.957, 1/32)
(3.957, 1694)
a1a
(0.7, 15%)
(4.214, 15%)
(2.758, 50)
(2.758, 1/32)
(2.758, 50)
gisette
(0.028, 15%)
(8.714, 10%)
(2.320, 186)
(2.320, 1/16)
(2.320, 186)
w1a
(0.1, 10%)
(3.571, 10%)
(3.646, 60)
(3.646, 1/32)
(3.646, 76)
w8a
(0.034, 15%)
(2.285, 15%)
(2.187, 543)
(3.645, 1/32)
(3.645, 1554)
mushrooms
(0.220, 15%)
(3.571, 0%)
(2.682, 190)
(2.682, 1/32)
(2.682, 190)"
REFERENCES,0.5066137566137566,"Table 6: Fine-tuned Hyper-parameters - Non-regularized Case.
Dataset
ADAM
SGD w/m
SARAH
SARAH+
SVRG
(α0, x%)
(α0, x%)
(α, m)
(α, γ)
(α, m)
ijcnn1
(0.1, 15%)
(0.58, 15%)
(3.153, 1015)
(3.503, 1/32)
(3.503, 1562)
rcv1
(5.5, 10%)
(10.0, 0%)
(3.925, 632)
(3.925, 1/32)
(3.925, 632)
news20
(1.642, 10%)
(10.0, 0%)
(3.787, 468)
(3.787, 1/32)
(3.787, 468)
covtype
(0.16, 15%)
(2.2857, 15%)
(2.447, 13616)
(2.447, 1/32)
(2.447, 13616)
real-sim
(2.928, 15%)
(10.0, 0%)
(3.957, 1609)
(3.957, 1/16)
(3.957, 1694)
a1a
(1.642, 15%)
(6.785, 1%)
(2.763, 50)
(2.763, 1/32)
(2.763, 50)
gisette
(2.285, 1%)
(10.0, 0%)
(2.321, 186)
(2.321, 1/32)
(2.321, 186)
w1a
(8.714, 10%)
(10.0, 0%)
(3.652, 76)
(3.652, 1/32)
(3.652, 76)
w8a
(0.16, 10%)
(10.0, 5%)
(2.552, 543)
(3.645, 1/32)
(3.645, 1554)
mushrooms
(10.0, 0%)
(10.0, 0%)
(2.683, 190)
(2.683, 1/32)
(2.683, 190) 0.2 0.4 0.6 0.8 1.0 P(w)"
REFERENCES,0.5079365079365079,ijcnn1 0.0 2.5 5.0 7.5 10.0 12.5 15.0 rcv1 0 2 4 6 8 10 12 14
REFERENCES,0.5092592592592593,real-sim 0 10 20 30 40 50 60
REFERENCES,0.5105820105820106,"70
news20 0.6 0.8 1.0 1.2 1.4 1.6"
REFERENCES,0.5119047619047619,"covtype 10
8 10
6 10
4 10
2"
REFERENCES,0.5132275132275133,"||P(w)||2 10
8 10
7 10
6 10
5 10
4 10
3 10
2 10
7 10
6 10
5 10
4 10
3 10
2 10
7 10
5 10
3 10
1 10
9 10
7 10
5 10
3 10
1 SARAH"
REFERENCES,0.5145502645502645,SARAH+ SVRG Adam
REFERENCES,0.5158730158730159,SGD w/m 0.905 0.910 0.915 0.920 0.925 0.930 0.935
REFERENCES,0.5171957671957672,Accuracy SARAH
REFERENCES,0.5185185185185185,SARAH+ SVRG Adam
REFERENCES,0.5198412698412699,SGD w/m 0.6 0.7 0.8 0.9 SARAH
REFERENCES,0.5211640211640212,SARAH+ SVRG Adam
REFERENCES,0.5224867724867724,SGD w/m 0.70 0.75 0.80 0.85 0.90 0.95 SARAH
REFERENCES,0.5238095238095238,SARAH+ SVRG Adam
REFERENCES,0.5251322751322751,SGD w/m 0.775 0.800 0.825 0.850 0.875 0.900 0.925 SARAH
REFERENCES,0.5264550264550265,SARAH+ SVRG Adam
REFERENCES,0.5277777777777778,SGD w/m 0.70 0.71 0.72 0.73 0.74 0.75 0.76
REFERENCES,0.5291005291005291,"Figure 13: Ending loss (top row), ending squared norm of full gradient (middle row), maximum
testing accuracy (bottom row) of different hyper-paramters and algorithms for the ℓ2-regularized
case on ijcnn1, rcv1, real-sim, news20 and covtype datasets."
REFERENCES,0.5304232804232805,Under review as a conference paper at ICLR 2022 0.4 0.6 0.8 1.0 1.2 P(w) a1a 0 10 20 30 40
REFERENCES,0.5317460317460317,gisette 0.1 0.2 0.3 0.4 0.5 0.6
REFERENCES,0.533068783068783,"0.7
w1a 0.2 0.4 0.6 0.8 1.0 1.2 w8a 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5343915343915344,"mushrooms 10
7 10
6 10
5 10
4 10
3 10
2 10
1"
REFERENCES,0.5357142857142857,"||P(w)||2 10
7 10
5 10
3 10
1 10
7 10
5 10
3 10
1 10
9 10
8 10
7 10
6 10
5 10
4 10
3 10
8 10
6 10
4 10
2 SARAH"
REFERENCES,0.5370370370370371,SARAH+ SVRG Adam
REFERENCES,0.5383597883597884,SGD w/m 0.76 0.78 0.80 0.82 0.84
REFERENCES,0.5396825396825397,Accuracy SARAH
REFERENCES,0.541005291005291,SARAH+ SVRG Adam
REFERENCES,0.5423280423280423,SGD w/m 0.84 0.86 0.88 0.90 0.92 0.94 0.96 SARAH
REFERENCES,0.5436507936507936,SARAH+ SVRG Adam
REFERENCES,0.544973544973545,SGD w/m 0.970 0.971 0.972 0.973 0.974 0.975 0.976 0.977 SARAH
REFERENCES,0.5462962962962963,SARAH+ SVRG Adam
REFERENCES,0.5476190476190477,SGD w/m
REFERENCES,0.548941798941799,0.9700
REFERENCES,0.5502645502645502,0.9725
REFERENCES,0.5515873015873016,0.9750
REFERENCES,0.5529100529100529,0.9775
REFERENCES,0.5542328042328042,0.9800
REFERENCES,0.5555555555555556,0.9825
REFERENCES,0.5568783068783069,0.9850 SARAH
REFERENCES,0.5582010582010583,SARAH+ SVRG Adam
REFERENCES,0.5595238095238095,SGD w/m 0.88 0.90 0.92 0.94 0.96 0.98 1.00
REFERENCES,0.5608465608465608,"Figure 14: Ending loss (top row), ending squared norm of full gradient (middle row), maximum
testing accuracy (bottom row) of different hyper-paramters and algorithms for the ℓ2-regularized
case on a1a, gisette, w1a, w8a and mushrooms datasets. 0.2 0.4 0.6 0.8 1.0 P(w)"
REFERENCES,0.5621693121693122,ijcnn1 0.0 0.1 0.2 0.3 0.4 0.5 0.6
REFERENCES,0.5634920634920635,"0.7
rcv1 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
REFERENCES,0.5648148148148148,real-sim 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
REFERENCES,0.5661375661375662,news20 0.6 0.8 1.0 1.2 1.4 1.6
COVTYPE,0.5674603174603174,"1.8
covtype 10
8 10
7 10
6 10
5 10
4 10
3 10
2"
COVTYPE,0.5687830687830688,"||P(w)||2 10
9 10
7 10
5 10
3 10
9 10
8 10
7 10
6 10
5 10
4 10
3 10
10 10
9 10
8 10
7 10
6 10
5 10
4 10
8 10
6 10
4 10
2 SARAH"
COVTYPE,0.5701058201058201,SARAH+ SVRG Adam
COVTYPE,0.5714285714285714,SGD w/m 0.905 0.910 0.915 0.920 0.925 0.930 0.935
COVTYPE,0.5727513227513228,Accuracy SARAH
COVTYPE,0.5740740740740741,SARAH+ SVRG Adam
COVTYPE,0.5753968253968254,SGD w/m 0.6 0.7 0.8 0.9 SARAH
COVTYPE,0.5767195767195767,SARAH+ SVRG Adam
COVTYPE,0.578042328042328,SGD w/m 0.70 0.75 0.80 0.85 0.90 0.95 SARAH
COVTYPE,0.5793650793650794,SARAH+ SVRG Adam
COVTYPE,0.5806878306878307,SGD w/m 0.5 0.6 0.7 0.8 0.9 1.0 SARAH
COVTYPE,0.582010582010582,SARAH+ SVRG Adam
COVTYPE,0.5833333333333334,SGD w/m 0.70 0.71 0.72 0.73 0.74 0.75 0.76
COVTYPE,0.5846560846560847,"Figure 15: Ending loss (top row), ending squared norm of full gradient (middle row), maximum
testing accuracy (bottom row) of different hyper-paramters and algorithms for the non-regularized
case on ijcnn1, rcv1, real-sim, news20 and covtype datasets."
COVTYPE,0.5859788359788359,Under review as a conference paper at ICLR 2022 0.4 0.6 0.8 1.0 1.2 P(w) a1a 0.0 0.2 0.4 0.6 0.8 1.0 1.2
COVTYPE,0.5873015873015873,gisette 0.0 0.1 0.2 0.3 0.4 w1a 0.0 0.2 0.4 0.6 0.8 1.0 w8a 0.0 0.1 0.2 0.3 0.4 0.5 0.6
COVTYPE,0.5886243386243386,"mushrooms 10
7 10
6 10
5 10
4 10
3 10
2"
COVTYPE,0.58994708994709,"||P(w)||2 10
28 10
24 10
20 10
16 10
12 10
8 10
4 10
7 10
5 10
3 10
1 10
9 10
8 10
7 10
6 10
5 10
4 10
14 10
12 10
10 10
8 10
6 10
4 10
2 SARAH"
COVTYPE,0.5912698412698413,SARAH+ SVRG Adam
COVTYPE,0.5925925925925926,SGD w/m 0.76 0.78 0.80 0.82 0.84
COVTYPE,0.593915343915344,Accuracy SARAH
COVTYPE,0.5952380952380952,SARAH+ SVRG Adam
COVTYPE,0.5965608465608465,SGD w/m 0.850 0.875 0.900 0.925 0.950 0.975 SARAH
COVTYPE,0.5978835978835979,SARAH+ SVRG Adam
COVTYPE,0.5992063492063492,SGD w/m 0.970 0.972 0.974 0.976 0.978 0.980 SARAH
COVTYPE,0.6005291005291006,SARAH+ SVRG Adam
COVTYPE,0.6018518518518519,SGD w/m 0.970 0.975 0.980 0.985 SARAH
COVTYPE,0.6031746031746031,SARAH+ SVRG Adam
COVTYPE,0.6044973544973545,SGD w/m 0.88 0.90 0.92 0.94 0.96 0.98 1.00
COVTYPE,0.6058201058201058,"Figure 16: Ending loss (top row), ending squared norm of full gradient (middle row), maximum
testing accuracy (bottom row) of different hyper-paramters and algorithms for the non-regularized
case on a1a, gisette, w1a, w8a and mushrooms datasets."
COVTYPE,0.6071428571428571,"Table 7: Global Lipschitz Constant L
Dataset
Regularized
Non-regularized
ijcnn1
0.285408
0.285388
rcv1
0.254812
0.254763
news20
0.264119
0.264052
covtype
0.408527
0.408525
real-sim
0.252693
0.252675
a1a
0.362456
0.361833
gisette
0.430994
0.430827
w1a
0.274215
0.273811
w8a
0.274301
0.274281
mushrooms
0.372816
0.372652"
COVTYPE,0.6084656084656085,"constant L, and it can be computed as, given (8) and (9), L = 1"
COVTYPE,0.6097883597883598,"4λmax( 1 n n
X"
COVTYPE,0.6111111111111112,"i=1
xixT
i ) + λ,"
COVTYPE,0.6124338624338624,"where λmax(A) denotes the largest eigenvalue of A and λ is the penalty term of the ℓ2-regularization
in (8). Table 7 shows the values of L for the regularized and non-regularized cases on the chosen
datasets."
COVTYPE,0.6137566137566137,Under review as a conference paper at ICLR 2022
COVTYPE,0.6150793650793651,"0
20
40
60
80
100
Percentile 10 13 10 10 10 7 10 4"
COVTYPE,0.6164021164021164,|| P(w)||2
COVTYPE,0.6177248677248677,ijcnn1
COVTYPE,0.6190476190476191,"AI­SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
COVTYPE,0.6203703703703703,"0
20
40
60
80
100
Percentile 10 11 10 8 10 5"
COVTYPE,0.6216931216931217,"10 2
rcv1"
COVTYPE,0.623015873015873,"0
20
40
60
80
100
Percentile 10 11 10 9 10 7 10 5 10 3"
COVTYPE,0.6243386243386243,real­sim
COVTYPE,0.6256613756613757,"0
20
40
60
80
100
Percentile 10 11 10 8 10 5 10 2"
COVTYPE,0.626984126984127,news20
COVTYPE,0.6283068783068783,"0
20
40
60
80
100
Percentile 10 10 10 8 10 6 10 4 10 2"
COVTYPE,0.6296296296296297,covtype
COVTYPE,0.6309523809523809,"0
20
40
60
80
100
Percentile 10 6 10 4 10 2"
COVTYPE,0.6322751322751323,|| P(w)||2 a1a
COVTYPE,0.6335978835978836,"0
20
40
60
80
100
Percentile 10 8 10 6 10 4 10 2"
GISETTE,0.6349206349206349,"100
gisette"
GISETTE,0.6362433862433863,"0
20
40
60
80
100
Percentile 10 9 10 7 10 5 10 3"
GISETTE,0.6375661375661376,"10 1
w1a"
GISETTE,0.6388888888888888,"0
20
40
60
80
100
Percentile 10 14 10 11 10 8 10 5 w8a"
GISETTE,0.6402116402116402,"0
20
40
60
80
100
Percentile 10 13 10 10 10 7 10 4"
GISETTE,0.6415343915343915,"10 1
mushrooms"
GISETTE,0.6428571428571429,"Figure 17: Average ending ∥∇P(w)∥2 for ℓ2-regularized case - AI-SARAH vs. Other Algorithms:
AI-SARAH is shown as the horizontal lines; for each of the other algorithms, the average ending
∥∇P(w)∥2 from different conﬁgurations of hyper-parameters are indexed from 0 percentile (the
worst choice) to 100 percentile (the best choice); see Section A.2.2 for details of the selection criteria."
GISETTE,0.6441798941798942,"0
20
40
60
80
100
Percentile 10 11 10 9 10 7 10 5 10 3"
GISETTE,0.6455026455026455,|| P(w)||2
GISETTE,0.6468253968253969,ijcnn1
GISETTE,0.6481481481481481,"AI­SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
GISETTE,0.6494708994708994,"0
20
40
60
80
100
Percentile 10 7 10 5"
GISETTE,0.6507936507936508,"10 3
rcv1"
GISETTE,0.6521164021164021,"0
20
40
60
80
100
Percentile 10 8 10 7 10 6 10 5 10 4"
GISETTE,0.6534391534391535,real­sim
GISETTE,0.6547619047619048,"0
20
40
60
80
100
Percentile 10 8 10 7 10 6 10 5 10 4"
GISETTE,0.656084656084656,news20
GISETTE,0.6574074074074074,"0
20
40
60
80
100
Percentile 10 8 10 6 10 4 10 2"
GISETTE,0.6587301587301587,covtype
GISETTE,0.66005291005291,"0
20
40
60
80
100
Percentile 10 6 10 4 10 2"
GISETTE,0.6613756613756614,|| P(w)||2 a1a
GISETTE,0.6626984126984127,"0
20
40
60
80
100
Percentile 10 12 10 10 10 8 10 6 10 4"
GISETTE,0.6640211640211641,gisette
GISETTE,0.6653439153439153,"0
20
40
60
80
100
Percentile 10 7 10 5 10 3"
GISETTE,0.6666666666666666,"10 1
w1a"
GISETTE,0.667989417989418,"0
20
40
60
80
100
Percentile 10 9 10 7 10 5 w8a"
GISETTE,0.6693121693121693,"0
20
40
60
80
100
Percentile 10 13 10 10 10 7 10 4"
GISETTE,0.6706349206349206,mushrooms
GISETTE,0.671957671957672,Figure 18: Average ending ∥∇P(w)∥2 for non-regularized case - AI-SARAH vs. Other Algorithms.
GISETTE,0.6732804232804233,"A.3
EXTENDED RESULTS OF EXPERIMENT"
GISETTE,0.6746031746031746,"In Chapter 4, we compared tune-free & fully adaptive AI-SARAH (Algorithm 1) with ﬁne-tuned
SARAH, SARAH+, SVRG, ADAM and SGD w/m. In this section, we present the extended results of
our empirical study on the performance of AI-SARAH."
GISETTE,0.6759259259259259,"Figures 17 and 18 compare the average ending ∥∇P(w)∥2 achieved by AI-SARAH with the other
algorithms, conﬁgured with all candidate hyper-parameters."
GISETTE,0.6772486772486772,"It is clear that,"
GISETTE,0.6785714285714286,"• without tuning, AI-SARAH achieves the best convergence (to a stationary point) in practice on
most of the datasets for both cases;
• while ﬁne-tuned ADAM achieves a better result for the non-regularized case on a1a, gisette, w1a
and mushrooms, AI-SARAH outperforms ADAM for at least 80% (a1a), 55% (gisette), 50% (w1a),
and 50% (mushrooms) of all candidate hyper-parameters."
GISETTE,0.6798941798941799,"Figure 19 shows the results of the non-regularized case for ijcnn1, rcv1, real-sim, news20 and covtype
datasets. Figures 20 and 21 present the results of the ℓ2-regularized case and non-regularized case
respectively on a1a, gisette, w1a, w8a and mushrooms datasets. For completeness of presentation,
we present the evolution of AI-SARAH’s step-size and upper-bound on a1a, gisette, w1a, w8a and
mushrooms datasets in Figures 22 and 23. Consistent with the results shown in Chapter 4 of the main
paper, AI-SARAH delivers a competitive performance in practice."
GISETTE,0.6812169312169312,Under review as a conference paper at ICLR 2022
GISETTE,0.6825396825396826,"0
5
10
15
20
Effective Pass 0.18 0.20 0.22 0.24 0.26 0.28 0.30 P(w)"
GISETTE,0.6838624338624338,ijcnn1
GISETTE,0.6851851851851852,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
GISETTE,0.6865079365079365,"0
10
20
30
40
Effective Pass 0.00 0.05 0.10 0.15 0.20 0.25"
GISETTE,0.6878306878306878,"0.30
rcv1"
GISETTE,0.6891534391534392,"0
10
20
30
Effective Pass 0.00 0.05 0.10 0.15 0.20 0.25"
REAL-SIM,0.6904761904761905,"0.30
real-sim"
REAL-SIM,0.6917989417989417,"0
10
20
30
40
50
Effective Pass 0.00 0.05 0.10 0.15 0.20 0.25"
REAL-SIM,0.6931216931216931,"0.30
news20"
REAL-SIM,0.6944444444444444,"0
5
10
15
20
Effective Pass 0.52 0.53 0.54 0.55 0.56 0.57"
COVTYPE,0.6957671957671958,"0.58
covtype"
COVTYPE,0.6970899470899471,"0
5
10
15
20
Effective Pass 10
11 10
9 10
7 10
5 10
3 10
1"
COVTYPE,0.6984126984126984,|| P(w)||2
COVTYPE,0.6997354497354498,ijcnn1
COVTYPE,0.701058201058201,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
COVTYPE,0.7023809523809523,"0
10
20
30
40
Effective Pass 10
8 10
7 10
6 10
5 10
4 10
3 rcv1"
COVTYPE,0.7037037037037037,"0
10
20
30
Effective Pass 10
8 10
7 10
6 10
5 10
4 10
3 10
2"
COVTYPE,0.705026455026455,real-sim
COVTYPE,0.7063492063492064,"0
10
20
30
40
50
Effective Pass 10
8 10
7 10
6 10
5 10
4 10
3"
COVTYPE,0.7076719576719577,"10
2
news20"
COVTYPE,0.708994708994709,"0
5
10
15
20
Effective Pass 10
8 10
6 10
4 10
2"
COVTYPE,0.7103174603174603,covtype
COVTYPE,0.7116402116402116,"0
5
10
15
20
Effective Pass 0.900 0.905 0.910 0.915 0.920 0.925 0.930"
COVTYPE,0.7129629629629629,Accuracy
COVTYPE,0.7142857142857143,ijcnn1
COVTYPE,0.7156084656084656,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m"
COVTYPE,0.716931216931217,"0
10
20
30
40
Effective Pass 0.940 0.945 0.950 0.955 0.960"
COVTYPE,0.7182539682539683,"0.965
rcv1"
COVTYPE,0.7195767195767195,"0
10
20
30
Effective Pass 0.88 0.90 0.92 0.94 0.96 0.98"
COVTYPE,0.7208994708994709,real-sim
COVTYPE,0.7222222222222222,"0
10
20
30
40
50
Effective Pass 0.86 0.88 0.90 0.92 0.94 0.96"
COVTYPE,0.7235449735449735,news20
COVTYPE,0.7248677248677249,"0
5
10
15
20
Effective Pass 0.70 0.71 0.72 0.73 0.74 0.75"
COVTYPE,0.7261904761904762,"0.76
covtype"
COVTYPE,0.7275132275132276,"Figure 19: Non-regularized case: evolution of P(w) (top row), ∥∇P(w)∥2 (middle row), and running
maximum of testing accuracy (bottom row). 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44 0.45 P(w) a1a"
COVTYPE,0.7288359788359788,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44"
COVTYPE,0.7301587301587301,gisette 0.10 0.11 0.12 0.13 0.14
COVTYPE,0.7314814814814815,"0.15
w1a 0.06 0.08 0.10 0.12 0.14 0.16 0.18"
COVTYPE,0.7328042328042328,"0.20
w8a 0.10 0.12 0.14 0.16 0.18"
MUSHROOMS,0.7341269841269841,"0.20
mushrooms 10
7 10
6 10
5 10
4 10
3 10
2 10
1"
MUSHROOMS,0.7354497354497355,"|| P(w)||2 10
8 10
6 10
4 10
2 10
8 10
6 10
4 10
2 10
15 10
13 10
11 10
9 10
7 10
5 10
3 10
1 10
11 10
9 10
7 10
5 10
3"
MUSHROOMS,0.7367724867724867,"0
10
20
30
Effective Pass 0.76 0.78 0.80 0.82 0.84"
MUSHROOMS,0.7380952380952381,Accuracy
MUSHROOMS,0.7394179894179894,"0
10
20
30
Effective Pass 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96"
MUSHROOMS,0.7407407407407407,"0
10
20
30
40
Effective Pass 0.960 0.962 0.964 0.966 0.968 0.970 0.972 0.974"
MUSHROOMS,0.7420634920634921,"0
10
20
30
Effective Pass 0.950 0.955 0.960 0.965 0.970 0.975 0.980 0.985"
MUSHROOMS,0.7433862433862434,"0
10
20
30
Effective Pass 0.90 0.92 0.94 0.96 0.98 1.00"
MUSHROOMS,0.7447089947089947,"Figure 20: ℓ2-regularized case: evolution of P(w) (top row), ∥∇P(w)∥2 (middle row), and running
maximum of testing accuracy (bottom row)."
MUSHROOMS,0.746031746031746,Under review as a conference paper at ICLR 2022 0.300 0.325 0.350 0.375 0.400 0.425 0.450 0.475 0.500 P(w) a1a
MUSHROOMS,0.7473544973544973,"AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m 0.0 0.1 0.2 0.3 0.4 0.5"
GISETTE,0.7486772486772487,"0.6
gisette 0.025 0.050 0.075 0.100 0.125 0.150 0.175"
GISETTE,0.75,"0.200
w1a 0.06 0.08 0.10 0.12 0.14 0.16 0.18"
GISETTE,0.7513227513227513,"0.20
w8a 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175"
MUSHROOMS,0.7526455026455027,"0.200
mushrooms 10
6 10
5 10
4 10
3 10
2 10
1"
MUSHROOMS,0.753968253968254,"|| P(w)||2 10
12 10
10 10
8 10
6 10
4 10
2 10
7 10
6 10
5 10
4 10
3 10
2 10
1 10
9 10
7 10
5 10
3 10
1 10
14 10
12 10
10 10
8 10
6 10
4 10
2"
MUSHROOMS,0.7552910052910053,"0
10
20
30
40
Effective Pass 0.76 0.78 0.80 0.82 0.84"
MUSHROOMS,0.7566137566137566,Accuracy
MUSHROOMS,0.7579365079365079,"0
10
20
30
40
Effective Pass 0.88 0.90 0.92 0.94 0.96 0.98"
MUSHROOMS,0.7592592592592593,"0
10
20
30
40
50
Effective Pass 0.950 0.955 0.960 0.965 0.970 0.975 0.980 0.985"
MUSHROOMS,0.7605820105820106,"0
10
20
30
40
Effective Pass 0.950 0.955 0.960 0.965 0.970 0.975 0.980 0.985 0.990"
MUSHROOMS,0.7619047619047619,"0
10
20
30
40
Effective Pass 0.90 0.92 0.94 0.96 0.98 1.00"
MUSHROOMS,0.7632275132275133,"Figure 21: Non-regularized case: evolution of P(w) (top row), ∥∇P(w)∥2 (middle row), and running
maximum of testing accuracy (bottom row)."
MUSHROOMS,0.7645502645502645,"0
10
20
30
Effective Pass 3.0 3.5 4.0 4.5 5.0 a1a max"
MUSHROOMS,0.7658730158730159,"0
10
20
30
Effective Pass 5 10 15 20 25 30"
GISETTE,0.7671957671957672,"35
gisette"
GISETTE,0.7685185185185185,"0
10
20
30
40
Effective Pass 4 6 8 10 12 w1a"
GISETTE,0.7698412698412699,"0
10
20
30
Effective Pass 0 20 40 60 80 100 120 140 w8a"
GISETTE,0.7711640211640212,"0
10
20
30
Effective Pass 5 10 15 20 25 30 35 40"
GISETTE,0.7724867724867724,mushrooms
GISETTE,0.7738095238095238,Figure 22: ℓ2-regularized case: evolution of AI-SARAH’s step-size α and upper-bound αmax.
GISETTE,0.7751322751322751,"0
10
20
30
40
Effective Pass 3 4 5 6 a1a max"
GISETTE,0.7764550264550265,"0
10
20
30
40
Effective Pass 5 10 15 20 25 30 35 40"
GISETTE,0.7777777777777778,gisette
GISETTE,0.7791005291005291,"0
10
20
30
40
50
Effective Pass 5.0 7.5 10.0 12.5 15.0 17.5 20.0 w1a"
GISETTE,0.7804232804232805,"0
10
20
30
40
Effective Pass 0 50 100 150 200 w8a"
GISETTE,0.7817460317460317,"0
10
20
30
40
Effective Pass 0 50 100 150 200"
GISETTE,0.783068783068783,mushrooms
GISETTE,0.7843915343915344,Figure 23: Non-regularized case: evolution of AI-SARAH’s step-size α and upper-bound αmax.
GISETTE,0.7857142857142857,Under review as a conference paper at ICLR 2022
GISETTE,0.7870370370370371,"B
THEORETICAL ANALYSIS"
GISETTE,0.7883597883597884,"In this chapter, we provide a convergence analysis of AI-SARAH (Algorithm 1) with a i) modiﬁed
line 10 to ˜αt−1 ≈arg minα∈[αk
min,αkmax] ξt(α), and ii) replacing the while loop with a for loop
t ∈[m], where αk
min and αk
max are step-size bounds picked in each outer iteration and m be a
hyper-parameter."
GISETTE,0.7896825396825397,"B.1
AI-SARAH AND SARAH"
GISETTE,0.791005291005291,"Like SVRG and SARAH, AI-SARAH’s loop structure of this algorithm is divided into the outer loop,
where a full gradient is computed, and the inner loop, where only stochastic gradient is computed.
However, unlike SVRG and SARAH, the step-size is computed implicitly. In particular, at each
iteration t ∈[m] of the inner loop, the step-size is chosen by approximately solving a simple one-
dimensional constrained optimization problem. Deﬁne the (modiﬁed) sub-problem (optimization
problem) at t ≥1 as"
GISETTE,0.7923280423280423,"min
α∈[αk
min,αkmax] ξt(α),
(10)"
GISETTE,0.7936507936507936,"where ξt(α) := ∥vt∥2 = ∥∇fSt(wt−1 −αvt−1) −∇fSt(wt−1)+ vt−1∥2, αk
min and αk
max are lower-
bound and upper-bound of the step-size respectively. These bounds do not allow large ﬂuctuations of
the (adaptive) step-size. We denote αt−1 the approximate solution of (10). Now, let us present some
remarks regarding AI-SARAH.
Remark B.1. As we will explain with more details in the following subsections, the values of αk
min
and αk
max cannot be arbitrarily large. To guarantee convergence, we will need to assume that
αk
max ≤
2
Lmax
k
, where Lmax
k
= maxi∈[n] Li
k. Here, Li
k is the local smoothness parameter of fi
deﬁned on a working-set for each outer loop (see Deﬁnition B.6).
Remark B.2. SARAH Nguyen et al. (2017) can be seen as a special case of AI-SARAH, where
αk
min = αk
max = α for all outer loops (k ≥1). In other words, a constant step-size is chosen for the
algorithm. However, if αk
min < αk
max, then the selection of the step-size in AI-SARAH allows a faster
convergence of ∥vt∥2 than SARAH in each inner loop.
Remark B.3. At t ≥1, let us select a mini-batch of size n, i.e., |St| = n. In this case, AI-SARAH is
equivalent to deterministic gradient descent with a very particular way of selecting the step-size, i.e.
by solving the following problem
min
α∈[αk
min,αkmax] ξt(α),"
GISETTE,0.794973544973545,"where ξt(α) = ∥∇P (wt−1 −α∇P(wt−1)) ∥2. In other words, the step-size is selected to minimize
the squared norm of the full gradient with respect to wt."
GISETTE,0.7962962962962963,"B.2
DEFINITIONS / ASSUMPTIONS"
GISETTE,0.7976190476190477,"First, we present the main deﬁnitions and assumptions that are used in our convergence analysis.
Deﬁnition B.4. Function f : Rd →R is L-smooth if: f(x) ≤f(y) + ⟨∇f(y), x −y⟩+ L"
GISETTE,0.798941798941799,"2 ∥x −
y∥2, ∀x, y ∈Rd,
and it is LC-smooth if:"
GISETTE,0.8002645502645502,"f(x) ≤f(y) + ⟨∇f(y), x −y⟩+ LC"
GISETTE,0.8015873015873016,"2 ∥x −y∥2, ∀x, y ∈C."
GISETTE,0.8029100529100529,"Deﬁnition B.5. Function f : Rd →R is µ-strongly convex if: f(x) ≥f(y) + ⟨∇f(y), x −y⟩+
µ
2 ∥x −y∥2, ∀x, y ∈Rd. If µ = 0 then function f is a (non-strongly) convex function."
GISETTE,0.8042328042328042,"Having presented the two main deﬁnitions for the class of problems that we are interested in, let us
now present the working-set Wk which contains all iterates produced in the k-th outer loop of (the
modiﬁed) Algorithm 1.
Deﬁnition B.6 (Working-Set Wk). For any outer loop k ≥1 in (the modiﬁed) Algorithm 1, starting
at ˜wk−1 we deﬁne
Wk := {w ∈Rd | ∥˜wk−1 −w∥≤m · αk
max∥v0∥}.
(11)"
GISETTE,0.8055555555555556,Under review as a conference paper at ICLR 2022
GISETTE,0.8068783068783069,"Note that the working-set Wk can be seen as a ball of all vectors w’s, which are not further away
from ˜wk−1 than m · αk
max∥v0∥. Here, recall that m is the total number of iterations of an inner loop,
αk
max is an upper bound of the step-size αt−1, ∀t ∈[m], and ∥v0∥is simply the norm of the full
gradient evaluated at the starting point ˜wk−1 in the outer loop.
By combining Deﬁnition B.4 with the working-set Wk, we are now ready to provide the main
assumption used in our analysis.
Assumption 1. Functions fi, i ∈[n], of problem (1) are Li
Wk-smooth. Since we only focus on the
working-set Wk, we simply write Li
k-smooth."
GISETTE,0.8082010582010583,"Let us denote Li the smoothness parameter of function fi, i ∈[n], in the domain Rd. Then, it is
easy to see that Li
k ≤Li, ∀i ∈[n]. In addition, under Assumption 1, it holds that function P is
¯Lk-smooth in the working-set Wk, where ¯Lk = 1"
GISETTE,0.8095238095238095,"n
Pn
i=1 Li
k.
As we will explain with more details in the next section for our theoretical results, we will assume
that αk
max ≤
2
Lmax
k
, where Lmax
k
= maxi∈[n] Li
k."
GISETTE,0.8108465608465608,"B.3
CONVERGENCE GUARANTEES"
GISETTE,0.8121693121693122,"Now, we can derive the convergence rate of AI-SARAH. Here, we highlight that, all of our theoretical
results can be applied to SARAH. We also note that, some quantities involved in our results, such as
Lk and Lmax
k
, are dependent upon the working set Wk (deﬁned for each outer loop k ≥1). Similar
to Nguyen et al. (2017), we start by presenting two important lemmas, serving as the foundation of
our theory."
GISETTE,0.8134920634920635,"The ﬁrst lemma provides an upper bound on the quantity Pm
t=0 E[∥∇P(wt)∥2]. Note that it does not
require any convexity assumption.
Lemma B.7. Fix a outer loop k ≥1 and consider Algorithm 1 with αk
max ≤1/¯Lk. Under"
GISETTE,0.8148148148148148,"Assumption 1, Pm
t=0 E[∥∇P(wt)∥2] ≤
2
αk
min E[P(w0) −P(w∗)] + αk
max
αk
min
Pm
t=0 E[∥∇P(wt) −vt∥2]."
GISETTE,0.8161375661375662,"The second lemma provides an informative bound on the quantity E[∥∇P(wt) −vt∥2]. Note that it
requires convexity of component functions fi, i ∈[n].
Lemma B.8. Fix a outer loop k ≥1 and consider Algorithm 1 with αk
max < 2/Lmax
k
. Suppose fi is
convex for all i ∈[n]. Then, under Assumption 1, for any t ≥1 :"
GISETTE,0.8174603174603174,"E[∥∇P(wt) −vt∥2] ≤

αk
maxLmax
k
2−αkmaxLmax
k"
GISETTE,0.8187830687830688,"
E[∥v0∥2]."
GISETTE,0.8201058201058201,"Equipped with the above lemmas, we can then present our main theorem and show the linear
convergence of (the modiﬁed) Algorithm 1 for solving strongly convex smooth problems.
Theorem B.9. Suppose that Assumption 1 holds and P is strongly convex with convex component
functions fi, i ∈[n]. Let us deﬁne"
GISETTE,0.8214285714285714,"σk
m =
1
µαk
min(m+1) + αk
max
αk
min ·
αk
maxLmax
k
2−αkmaxLmax
k
,"
GISETTE,0.8227513227513228,"and select m and αk
max such that σk
m < 1, ∀k ≥1. Then, Algorithm 1 converges as follows:"
GISETTE,0.8240740740740741,"E[∥∇P( ˜wk)∥2] ≤
Qk
ℓ=1σℓ
m

∥∇P( ˜w0)∥2."
GISETTE,0.8253968253968254,"As a corollary of our main theorem, it is easy to see that we can also obtain the convergence of
SARAH Nguyen et al. (2017). Recall, from Remark B.2, that SARAH can be seen as a special case of
AI-SARAH if, for all outer loops, αk
min = αk
max = α. In this case, we can have"
GISETTE,0.8267195767195767,"σk
m =
1
µα(m+1) +
αLmax
k
2−αLmax
k
."
GISETTE,0.828042328042328,"If we further assume that all functions fi, i ∈[n], are L-smooth and do not take advantage of the
local smoothness (in other words, do not use the working-set Wk), then Lmax
k
= L for all k ≥1.
Then, with these restrictions, we have
σm = σk
m =
1
µα(m+1) +
αL
2−αL < 1."
GISETTE,0.8293650793650794,"As a result, Theorem B.9 guarantees the following linear convergence:
E[∥∇P( ˜wk)∥2] ≤
(σm)k∥∇P( ˜w0)∥2, which is exactly the convergence of classical SARAH provided in Nguyen et al.
(2017)."
GISETTE,0.8306878306878307,Under review as a conference paper at ICLR 2022
GISETTE,0.832010582010582,"C
TECHNICAL PRELIMINARIES & PROOFS OF MAIN RESULTS"
GISETTE,0.8333333333333334,Let us start by presenting some important technical lemmas that will be later used for our main proofs.
GISETTE,0.8346560846560847,"C.1
TECHNICAL PRELIMINARIES"
GISETTE,0.8359788359788359,"Lemma C.1. Nesterov (2003) Suppose that function f is convex and L-Smooth in C ⊆Rn. Then
for any w, w′ ∈C:"
GISETTE,0.8373015873015873,"⟨∇f(w) −∇f(w′), (w −w′)⟩≥1"
GISETTE,0.8386243386243386,"L∥∇f(w) −∇f(w′)∥2.
(12)"
GISETTE,0.83994708994709,"Lemma C.2. Let Assumption 1 hold for all functions fi of problem (1). That is, let us assume that
function fi is Li
k-smooth ∀i ∈[n]. Then, function P(w)
def
=
1
n
Pn
i=1 fi(w) is ¯Lk-smooth, where
¯Lk = 1"
GISETTE,0.8412698412698413,"n
Pn
i=1 Li
k."
GISETTE,0.8425925925925926,"Proof. For each function fi, we have by deﬁnition of Li
k-local smoothness,"
GISETTE,0.843915343915344,"fi(x) ≤fi(y) + ⟨∇fi(y), x −y⟩+ Li
k
2 ∥x −y∥2, ∀x, y ∈Wk."
GISETTE,0.8452380952380952,"Summing through all i′s and dividing by n, we get"
GISETTE,0.8465608465608465,"P(x) ≤P(y) + ⟨∇P(y), x −y⟩+
¯Lk"
GISETTE,0.8478835978835979,"2 ∥x −y∥2, ∀x, y ∈Wk."
GISETTE,0.8492063492063492,"The next Lemma was ﬁrst proposed in Nguyen et al. (2017). We add it here with its proof for
completeness and will use it later for our main theoretical result."
GISETTE,0.8505291005291006,"Lemma C.3. Nguyen et al. (2017) Consider vt deﬁned in (2). Then for any t ≥1 in Algorithm 1, it
holds that:"
GISETTE,0.8518518518518519,"E[∥∇P(wt) −vt∥2] = t
X"
GISETTE,0.8531746031746031,"j=1
E[∥vj −vj−1∥2] − t
X"
GISETTE,0.8544973544973545,"j=1
E[∥∇P(wj) −∇P(wj−1)∥2].
(13)"
GISETTE,0.8558201058201058,"Proof. Let Ej denote the expectation by conditioning on the information w0, w1, . . . , wj as well as
v0, v1, . . . , vj−1. Then,"
GISETTE,0.8571428571428571,"Ej[∥∇P(wj) −vj∥2] = Ej

∥(∇P(wj−1) −vj−1) + (∇P(wj) −∇P(wj−1)) −(vj −vj−1)∥2"
GISETTE,0.8584656084656085,= Ej[∥∇P(wj−1) −vj−1∥2] + Ej[∥∇P(wj) −∇P(wj−1)∥2] + Ej[∥vj −vj−1∥2]
GISETTE,0.8597883597883598,+ 2 (∇P(wj−1) −vj−1)T (∇P(wj) −∇P(wj−1))
GISETTE,0.8611111111111112,−2 (∇P(wj−1) −vj−1)T Ej[vj −vj−1]
GISETTE,0.8624338624338624,−2 (∇P(wj) −∇P(wj−1))T Ej[vj −vj−1]
GISETTE,0.8637566137566137,"= Ej[∥∇P(wj−1) −vj−1∥2] −Ej[∥∇P(wj) −∇P(wj−1)∥2] + Ej[∥vj −vj−1∥2],"
GISETTE,0.8650793650793651,where the last equality follows from
GISETTE,0.8664021164021164,Ej[vj −vj−1] = Ej[∇fij(wj) −∇fij(wj−1)] = ∇P(wj) −∇P(wj−1).
GISETTE,0.8677248677248677,"By taking expectation in the above expression, using the tower property, and summing over j =
1, ..., t, we obtain"
GISETTE,0.8690476190476191,"E[∥∇P(wt) −vt∥2] = t
X"
GISETTE,0.8703703703703703,"j=1
E[∥vj −vj−1∥2] − t
X"
GISETTE,0.8716931216931217,"j=1
E[∥∇P(wj) −∇P(wj−1)∥2]."
GISETTE,0.873015873015873,Under review as a conference paper at ICLR 2022
GISETTE,0.8743386243386243,"C.2
PROOFS OF LEMMAS AND THEOREMS"
GISETTE,0.8756613756613757,"For simplicity of notation, we use |S| = 1 in the following proofs, and a generalization to |S| > 1 is
straightforward."
GISETTE,0.876984126984127,"C.2.1
PROOF OF LEMMA B.7"
GISETTE,0.8783068783068783,"By Assumption 1, Lemma C.2 and the update rule wt = wt−1 −αt−1vt−1 of Algorithm 1, we obtain:"
GISETTE,0.8796296296296297,"P(wt)
≤
P(wt−1) −αt−1⟨∇P(wt−1), vt−1⟩+
¯Lk"
GISETTE,0.8809523809523809,"2 α2
t−1∥vt−1∥2"
GISETTE,0.8822751322751323,"=
P(wt−1) −αt−1"
GISETTE,0.8835978835978836,"2
∥∇P(wt−1)∥2 + αt−1"
GISETTE,0.8849206349206349,"2
∥∇P(wt−1) −vt−1∥2 −
αt−1"
GISETTE,0.8862433862433863,"2
−
¯Lk"
GISETTE,0.8875661375661376,"2 α2
t−1"
GISETTE,0.8888888888888888,"
∥vt−1∥2,"
GISETTE,0.8902116402116402,"where, in the equality above, we use the fact that ⟨a, b⟩= 1"
GISETTE,0.8915343915343915,2(∥a∥2 + ∥b∥2 −∥a −b∥2).
GISETTE,0.8928571428571429,"By rearranging and using the lower and upper bounds of the step-size αt−1 in the outer loop k
(αk
min ≤αt−1 ≤αk
max), we get:
αk
min"
GISETTE,0.8941798941798942,"2
∥∇P(wt−1)∥2
≤
[P(wt−1) −P(wt)] + αk
max"
GISETTE,0.8955026455026455,"2
∥∇P(wt−1) −vt−1∥2 −αt−1"
GISETTE,0.8968253968253969,"2
 
1 −¯Lkαt−1

∥vt−1∥2."
GISETTE,0.8981481481481481,"By assuming that αk
max ≤
1
¯Lk , it holds that αt−1 ≤
1
¯Lk and
 
1 −¯Lkαt−1

≥0, ∀t ∈[m]. Thus,"
GISETTE,0.8994708994708994,"αk
min"
GISETTE,0.9007936507936508,"2
∥∇P(wt−1)∥2
≤
[P(wt−1) −P(wt)] + αk
max"
GISETTE,0.9021164021164021,"2
∥∇P(wt−1) −vt−1∥2 −αk
min"
GISETTE,0.9034391534391535,"2
 
1 −¯Lkαk
max

∥vt−1∥2."
GISETTE,0.9047619047619048,"By taking expectations and multiplying both sides with
2
αk
min :"
GISETTE,0.906084656084656,"E[∥∇P(wt−1)∥2] ≤
2
αk
min
[E[P(wt−1)] −E[P(wt)]] + αk
max
αk
min
E[∥∇P(wt−1) −vt−1∥2] −
 
1 −¯Lkαk
max

E[∥vt−1∥2]"
GISETTE,0.9074074074074074,"≤
2
αk
min
[E[P(wt−1)] −E[P(wt)]] + αk
max
αk
min
E[∥∇P(wt−1) −vt−1∥2],"
GISETTE,0.9087301587301587,"where the last inequality holds as αk
max ≤
1
¯Lk . Summing over t = 1, 2, . . . , m + 1, we have m+1
X"
GISETTE,0.91005291005291,"t=1
E[∥∇P(wt−1)∥2]
≤
2
αk
min m+1
X"
GISETTE,0.9113756613756614,"t=1
E[P(wt−1) −P(wt)] + αk
max
αk
min m+1
X"
GISETTE,0.9126984126984127,"t=1
E[∥∇P(wt−1) −vt−1∥2]"
GISETTE,0.9140211640211641,"=
2
αk
min
E[P(w0) −P(wm+1)] + αk
max
αk
min m+1
X"
GISETTE,0.9153439153439153,"t=1
E[∥∇P(wt−1) −vt−1∥2"
GISETTE,0.9166666666666666,"≤
2
αk
min
E[P(w0) −P(w∗)] + αk
max
αk
min m+1
X"
GISETTE,0.917989417989418,"t=1
E[∥∇P(wt−1) −vt−1∥2],"
GISETTE,0.9193121693121693,where the last inequality holds since w∗is the global minimizer of P.
GISETTE,0.9206349206349206,"The last expression can be equivalently written as:
m
X"
GISETTE,0.921957671957672,"t=0
E[∥∇P(wt)∥2]
≤
2
αk
min
E[P(w0) −P(w∗)] + αk
max
αk
min m
X"
GISETTE,0.9232804232804233,"t=0
E[∥∇P(wt) −vt∥2],"
GISETTE,0.9246031746031746,which completes the proof.
GISETTE,0.9259259259259259,"C.2.2
PROOF OF LEMMA B.8"
GISETTE,0.9272486772486772,"Ej

∥vj∥2
≤
Ej

∥vj−1 −
 
∇fij(wj−1) −∇fij(wj)

∥2"
GISETTE,0.9285714285714286,"=
∥vj−1∥2 + Ej

∥∇fij(wj−1) −∇fij(wj)∥2 −Ej"
GISETTE,0.9298941798941799,"
2
αj−1"
GISETTE,0.9312169312169312,"∇fit(wj−1) −∇fij(wj), wj−1 −wj
"
GISETTE,0.9325396825396826,"(12)
≤
∥vj−1∥2 + Ej

∥∇fij(wj−1) −∇fij(wj)∥2
−Ej ""
2"
GISETTE,0.9338624338624338,"αj−1Lij
k
∥∇fij(wj−1) −∇fij(wj)∥2
# ."
GISETTE,0.9351851851851852,Under review as a conference paper at ICLR 2022
GISETTE,0.9365079365079365,"For each outer loop k, it holds that αj−1 ≤αk
max and Li
k ≤Lmax
k
. Thus,"
GISETTE,0.9378306878306878,"Ej[∥vj∥2]
≤
∥vj−1∥2 + Ej

∥∇fij(wj−1) −∇fij(wj)∥2
−
2
αkmaxLmax
k
Ej

∥∇fij(wj−1) −∇fij(wj)∥2"
GISETTE,0.9391534391534392,"=
∥vj−1∥2 +

1 −
2
αkmaxLmax
k"
GISETTE,0.9404761904761905,"
Ej

∥∇fij(wj−1) −∇fij(wj)∥2"
GISETTE,0.9417989417989417,"=
∥vj−1∥2 +

1 −
2
αkmaxLmax
k"
GISETTE,0.9431216931216931,"
Ej

∥vj −vj−1∥2
."
GISETTE,0.9444444444444444,"By rearranging, taking expectations again, and assuming that αk
max < 2/Lmax
k
:"
GISETTE,0.9457671957671958,"E[∥vj −vj−1∥2]
≤

αk
maxLmax
k
2 −αkmaxLmax
k"
GISETTE,0.9470899470899471," 
E[∥vj−1∥2] −E[∥vj∥2]

."
GISETTE,0.9484126984126984,"By summing the above inequality over j = 1, . . . , t (t ≥1), we have: t
X"
GISETTE,0.9497354497354498,"j=1
E[∥vj −vj−1∥2]
≤

αk
maxLmax
k
2 −αkmaxLmax
k 
t
X j=1"
GISETTE,0.951058201058201,"
∥vj−1∥2 −∥vj∥2"
GISETTE,0.9523809523809523,"≤

αk
maxLmax
k
2 −αkmaxLmax
k"
GISETTE,0.9537037037037037," 
E[∥v0∥2] −E[∥vt∥2]

.
(14)"
GISETTE,0.955026455026455,"Now, by using Lemma C.3, we obtain:"
GISETTE,0.9563492063492064,"E[∥∇P(wt) −vt∥2]
(13)
≤ t
X"
GISETTE,0.9576719576719577,"j=1
E

∥vj −vj−1∥2"
GISETTE,0.958994708994709,"(14)
≤

αk
maxLmax
k
2 −αkmaxLmax
k"
GISETTE,0.9603174603174603," 
E[∥v0∥2] −E[∥vt∥2]
"
GISETTE,0.9616402116402116,"≤

αk
maxLmax
k
2 −αkmaxLmax
k"
GISETTE,0.9629629629629629,"
E[∥v0∥2].
(15)"
GISETTE,0.9642857142857143,"C.2.3
PROOF OF THEOREM B.9"
GISETTE,0.9656084656084656,"Proof. Since v0 = ∇P(w0) implies ∥∇P(w0) −v0∥2 = 0, then by Lemma B.8, we obtain: m
X"
GISETTE,0.966931216931217,"t=0
E[∥∇P(wt) −vt∥2] ≤
 mαk
maxLmax
k
2 −αkmaxLmax
k"
GISETTE,0.9682539682539683,"
E[∥v0∥2].
(16)"
GISETTE,0.9695767195767195,"Combine this with Lemma B.7, we have that: m
X"
GISETTE,0.9708994708994709,"t=0
E[∥∇P(wt)∥2]
≤
2
αk
min
E[P(w0) −P(w∗)] + αk
max
αk
min m
X"
GISETTE,0.9722222222222222,"t=0
E[∥∇P(wt) −vt∥2]"
GISETTE,0.9735449735449735,"(16)
≤
2
αk
min
E[P(w0) −P(w∗)] + αk
max
αk
min"
GISETTE,0.9748677248677249," mαk
maxLmax
k
2 −αkmaxLmax
k"
GISETTE,0.9761904761904762,"
E[∥v0∥2].(17)"
GISETTE,0.9775132275132276,"Since we are considering one outer iteration, with k ≥1, we have v0 = ∇P(w0) = ∇P( ˜wk−1) and
˜wk = wt, where t is drawn uniformly at random from {0, 1, . . . , m}. Therefore, the following holds,"
GISETTE,0.9788359788359788,"E[∥∇P( ˜wk)∥2]
=
1
m + 1 m
X"
GISETTE,0.9801587301587301,"t=0
E[∥∇P(wt)∥2]"
GISETTE,0.9814814814814815,"(17)
≤
2
αk
min(m + 1)E[P( ˜wk−1) −P(w∗)] + αk
max
αk
min"
GISETTE,0.9828042328042328,"
αk
maxLmax
k
2 −αkmaxLmax
k"
GISETTE,0.9841269841269841,"
E[∥∇P( ˜wk−1)∥2]"
GISETTE,0.9854497354497355,"≤

1
µαk
min(m + 1) + αk
max
αk
min"
GISETTE,0.9867724867724867,"
αk
maxLmax
k
2 −αkmaxLmax
k"
GISETTE,0.9880952380952381,"
E[∥∇P( ˜wk−1)∥2]."
GISETTE,0.9894179894179894,Under review as a conference paper at ICLR 2022
GISETTE,0.9907407407407407,"Let us use σk
m =
1
µαk
min(m+1) + αk
max
αk
min ·
αk
maxLmax
k
2−αkmaxLmax
k
, then the above expression can be written as:"
GISETTE,0.9920634920634921,"E[∥∇P( ˜wk)∥2]
≤
σk
mE[∥∇P( ˜wk−1)∥2]."
GISETTE,0.9933862433862434,"By expanding the recurrence, we obtain:"
GISETTE,0.9947089947089947,"E[∥∇P( ˜wk)∥2]
≤ k
Y"
GISETTE,0.996031746031746,"ℓ=1
σℓ
m !"
GISETTE,0.9973544973544973,∥∇P( ˜w0)∥2.
GISETTE,0.9986772486772487,This completes the proof.
