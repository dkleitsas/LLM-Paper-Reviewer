Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004366812227074236,"Most neural text-to-speech (TTS) models require ‚ü®speech, transcript‚ü©paired data
from the desired speaker for high-quality speech synthesis, which limits the us-
age of large amounts of untranscribed data for training. In this work, we present
Guided-TTS, a high-quality TTS model that learns to generate speech from un-
transcribed speech data. Guided-TTS combines an unconditional diffusion prob-
abilistic model with a separately trained phoneme classiÔ¨Åer for text-to-speech.
By modeling the unconditional distribution for speech, our model can utilize the
untranscribed data for training. For text-to-speech synthesis, we guide the gener-
ative process of the unconditional DDPM via phoneme classiÔ¨Åcation to produce
mel-spectrograms from the conditional distribution given transcript. We show that
Guided-TTS achieves comparable performance with the existing methods with-
out any transcript for LJSpeech. Our results further show that a single speaker-
dependent phoneme classiÔ¨Åer trained on multispeaker large-scale data can guide
unconditional DDPMs for various speakers to perform TTS."
INTRODUCTION,0.008733624454148471,"1
INTRODUCTION"
INTRODUCTION,0.013100436681222707,"Neural text-to-speech (TTS) models have been achieved to generate high-quality human-like speech
given text (van den Oord et al. (2016); Shen et al. (2018)). In general, these TTS models are condi-
tional generative models that encode text into the hidden representation and generate speech from
the encoded representation. Early TTS models are autoregressive generative models which generate
high-quality speech but suffer from slow synthesis speed due to the sequential sampling procedure
(Shen et al. (2018); Li et al. (2019)). Owing to the development of non-autoregressive generative
models, recent TTS models are capable of generating high-quality speech with faster inference speed
(Ren et al. (2019); Ren et al. (2021); Kim et al. (2020); Popov et al. (2021)). Recently, high-quality
end-to-end TTS models have been proposed that generate raw waveform from the text at once (Kim
et al. (2021); Weiss et al. (2021); Chen et al. (2021b))."
INTRODUCTION,0.017467248908296942,"Despite the high-quality and fast inference speed of speech synthesis, most TTS models can be
trained only if the transcribed data of the desired speaker is given. While long-form untranscribed
data, such as audiobooks or podcasts, is available on various websites, it is challenging to use these
unpaired speech data to train existing TTS models. To utilize these untranscribed data, long-form
untranscribed speech data has to be segmented into sentence-level, and then each segmented speech
should be transcribed accurately. Since the existing TTS models must directly model the conditional
distribution of speech given text, the direct usage of untranscribed data remains challenging to solve."
INTRODUCTION,0.021834061135371178,"In this work, we propose Guided-TTS, an unconditional diffusion-based generative model trained
on untranscribed data that leverages a phoneme classiÔ¨Åer for text-to-speech synthesis. Trained on
untranscribed speech data for the desired speaker, our unconditional diffusion probabilistic model
learns to generate mel-spectrograms of the speaker without any context. As training data does not
have to be aligned with text sequence for unconditional speech modeling, we simply use random
chunks of untranscribed speech to train our unconditional generative model. This allows us to build
training data without extra effort in modeling the speech of speakers for which only long-form
untranscribed speech data is available."
INTRODUCTION,0.026200873362445413,"To guide the unconditional DDPM for TTS, we train a framewise phoneme classiÔ¨Åer on transcribed
data and use the gradients of the classiÔ¨Åer during sampling. Although our unconditional generative"
INTRODUCTION,0.03056768558951965,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.034934497816593885,"model is trained without any transcript, Guided-TTS effectively generates mel-spectrograms given
the transcript by guiding the generative process of unconditional DDPM with the phoneme classiÔ¨Åer."
INTRODUCTION,0.039301310043668124,"We demonstrate that the proposed method, TTS by guiding the unconditional DDPM, matches the
performance of the existing conditional TTS models on LJSpeech. We further show that by training
the phoneme classiÔ¨Åer on multi-speaker paired dataset, Guided-TTS also shows comparable perfor-
mance without seeing any transcript of LJSpeech, which shows the possibility to build a high-quality
text-to-speech model without a transcript for the desired speaker. We encourage the readers to listen
to samples of Guided-TTS trained on various untranscribed datasets on our demo page. 1"
BACKGROUND,0.043668122270742356,"2
BACKGROUND"
BACKGROUND,0.048034934497816595,"2.1
DENOISING DIFFUSION PROBABLISTIC MODELS (DDPM) AND ITS VARIANT"
BACKGROUND,0.05240174672489083,"DDPM (Ho et al. (2020)), which is recently proposed as a kind of probabilistic generative model,
has been applied to various domains such as image (Dhariwal & Nichol (2021)) and audio (Chen
et al. (2021a); Popov et al. (2021)). DDPM Ô¨Årst deÔ¨Ånes a forward process that gradually corrupts
data X0 to a random noise XT across T timesteps. The model learns the reverse process that follows
the reverse trajectory of the predeÔ¨Åned forward process to generate data from random noise."
BACKGROUND,0.056768558951965066,"Recently, there have been approaches to formulate the trajectory between data and noise as a contin-
uous stochastic differential equation (SDE) instead of a discrete-time Markov process (Song et al.
(2021b)). Grad-TTS (Popov et al. (2021)) introduces SDE formulation to TTS, which we have fol-
lowed and used. According to the formulation of Grad-TTS, the forward process that corrupts data
X0 into the standard Gaussian noise XT is as follows:"
BACKGROUND,0.0611353711790393,dXt = ‚àí1
BACKGROUND,0.06550218340611354,"2XtŒ≤tdt +
p"
BACKGROUND,0.06986899563318777,"Œ≤tdWt,
(1)"
BACKGROUND,0.07423580786026202,"where Œ≤t is a predeÔ¨Åned noise schedule, Œ≤t = Œ≤0+(Œ≤T ‚àíŒ≤0)t, and Wt is a Wiener process. Anderson
(1982) showed that the reverse process, which represents the trajectory from noise XT to X0, can
be formulated in SDE, which is deÔ¨Åned as follows:"
BACKGROUND,0.07860262008733625,dXt = (‚àí1
BACKGROUND,0.08296943231441048,"2Xt ‚àí‚àáXt log pt(Xt))Œ≤tdt +
p"
BACKGROUND,0.08733624454148471,"Œ≤tdf
Wt,
(2)"
BACKGROUND,0.09170305676855896,"where f
Wt is a reverse time Wiener process. Given the score, the gradient of log density with respect
to data (i.e., ‚àáXt log pt(Xt)), for t ‚àà[0, T], we can sample data X0 from random noise XT by
solving Eq. (2). To generate data, the DDPM learns to estimate the score with the neural network sŒ∏
parameterized by Œ∏."
BACKGROUND,0.09606986899563319,"To estimate the score, Xt is sampled from the distribution derived from Eq. (1) given data X0, which
is as follows:
Xt|X0 ‚àºN(œÅ(X0, t), Œª(t)),
(3)"
BACKGROUND,0.10043668122270742,"where œÅ(X0, t) = e‚àí1"
R T,0.10480349344978165,"2
R t
0 Œ≤sdsX0, and Œª(t) = I‚àíe‚àí
R t
0 Œ≤sds. Then, the score can be derived from Eq.
(3); ‚àáXt log pt(Xt|X0) = ‚àíŒª(t)‚àí1œµt, given X0 (Popov et al. (2021)). To train the model sŒ∏(Xt, t)
for ‚àÄt ‚àà[0, T], the following loss is used:"
R T,0.1091703056768559,"L(Œ∏) = EtEX0Eœµt
 sŒ∏(Xt, t) + Œª(t)‚àí1œµt
2
2

(4)"
R T,0.11353711790393013,"which is L2 loss as in previous works (Ho et al. (2020), Song et al. (2021b))."
R T,0.11790393013100436,"Using the model sŒ∏(Xt, t), we can generate sample X0 from noise by solving Eq. (2). Grad-TTS
generates data X0 from XT by setting T = 1 and using a Ô¨Åxed discretization strategy (Song et al.
(2021b)): Xt‚àí1"
R T,0.1222707423580786,N = Xt + Œ≤t N (1
R T,0.12663755458515283,2Xt + ‚àáXt log pt(Xt)) + r
R T,0.13100436681222707,"Œ≤t
N zt
(5)"
R T,0.13537117903930132,"where N is the number of steps to solve SDE, t ‚àà{ 1 N , 2"
R T,0.13973799126637554,"N , ..., 1} and zt is a standard Gaussian noise."
R T,0.14410480349344978,1Demo : https://bit.ly/3oWhVJg
R T,0.14847161572052403,Under review as a conference paper at ICLR 2022
CLASSIFIER GUIDANCE,0.15283842794759825,"2.2
CLASSIFIER GUIDANCE"
CLASSIFIER GUIDANCE,0.1572052401746725,"DDPM can be guided to generate samples with the desired condition without Ô¨Åne-tuning with the
introduction of a classiÔ¨Åer. Song et al. (2021b) used the unconditional DDPM to generate class-
conditional images using a separately trained image classiÔ¨Åer. Not only unconditional DDPM but
also conditional DDPM can be guided using the classiÔ¨Åer, which contributes to achieving state-of-
the-art performance for class-conditional image generation (Dhariwal & Nichol (2021))."
CLASSIFIER GUIDANCE,0.1615720524017467,"For conditional generation, the classiÔ¨Åer pt(y|Xt) is trained to classify the noisy data Xt as the
condition y. If Eq. (5) is modiÔ¨Åed, discretized SDE for the conditional generation can be obtained. Xt‚àí1"
CLASSIFIER GUIDANCE,0.16593886462882096,N = Xt + Œ≤t N (1
CLASSIFIER GUIDANCE,0.1703056768558952,2Xt + ‚àáXt log pt(Xt|y)) + r
CLASSIFIER GUIDANCE,0.17467248908296942,"Œ≤t
N zt
(6)"
CLASSIFIER GUIDANCE,0.17903930131004367,"‚àáXt log pt(Xt|y) = ‚àáXt log pt(Xt) + ‚àáXt log pt(y|Xt)
(7)"
CLASSIFIER GUIDANCE,0.18340611353711792,"If the unconditional score and the classiÔ¨Åer for the condition are given, the sample X0 with the
condition y can be generated using Eq. (6)."
GUIDED-TTS,0.18777292576419213,"3
GUIDED-TTS"
GUIDED-TTS,0.19213973799126638,"In this section, we present Guided-TTS, which aims to build a high-quality text-to-speech model
without the transcript of the target speaker. While other TTS models directly learn to generate speech
from text, Guided-TTS models the unconditional distribution of speech to utilize speech-only data
and guides the unconditional model to generate speech with a given text. To the best of our knowl-
edge, Guided-TTS is the Ô¨Årst TTS model that generates speech with the unconditional generative
model."
GUIDED-TTS,0.1965065502183406,"Both unconditional speech modeling and controllable generation with the unconditional generative
model are well known to be challenging. To tackle these challenges, we adopt a diffusion-based gen-
erative model for unconditional speech generation, which has the advantages of modeling complex
distributions and easy controllability. Additionally, we introduce a phoneme classiÔ¨Åer to guide the
unconditional DDPM for TTS, which requires transcribed data for training."
GUIDED-TTS,0.20087336244541484,"In Guided-TTS, the generative model and the phoneme classiÔ¨Åer are trained separately so that we can
utilize different datasets to train each module. For instance, assume only the untranscribed speech
data of the target speaker for TTS is available. With Guided-TTS, the phoneme classiÔ¨Åer can still be
trained on other transcribed datasets that contain rich, large-scale multi-speaker data. The phoneme
classiÔ¨Åer trained in such a manner effectively guides the generative model, trained only using the
untranscribed speech data of the target speaker. By doing so, with the two modules independent of
each other, we can achieve the text-to-speech model without any transcript of the target speaker."
GUIDED-TTS,0.2052401746724891,"Guided-TTS consists of 4 modules: the unconditional DDPM, the phoneme classiÔ¨Åer, the duration
predictor, and the speaker encoder, as shown in Fig. 1. Unconditional DDPM is a module that learns
to generate mel-spectrogram unconditionally, and the remaining three modules are for TTS synthesis
by guidance. We will explain the unconditional DDPM in Section 3.1, followed by the method of
guiding the unconditional model for TTS in Section 3.2."
UNCONDITIONAL DDPM,0.2096069868995633,"3.1
UNCONDITIONAL DDPM"
UNCONDITIONAL DDPM,0.21397379912663755,"Our unconditional DDPM models the unconditional distribution of speech PX without any tran-
script. We assume that the training data for the diffusion-based model has tens of hours of untran-
scribed speech data from the target speaker S for TTS. As our generative model learns only with
speech data, training samples do not need to be aligned with the text. Thus, we use random chunks
of untranscribed speech as training data to reduce the burden of not only speech transcription but
also segmentation when only the long-form speech data is available for the target speaker."
UNCONDITIONAL DDPM,0.2183406113537118,"Given a mel-spectrogram X = X0, we deÔ¨Åne a forward process as in Eq. (1), which gradually
corrupts data into noise, and approximate the reverse process in Eq. (2) by estimating the uncon-
ditional score ‚àáXtlog p(Xt) for each timestep t. At each iteration, Xt, t ‚àà[0, 1] is sampled from
the mel-spectrogram X0 as in Eq. (3), and score is estimated with the neural network sŒ∏(Xt, t)
parameterized by Œ∏. The training objective of our unconditional model is in Eq. (4)."
UNCONDITIONAL DDPM,0.22270742358078602,Under review as a conference paper at ICLR 2022 ùë¶ ùëéùëèùëê
UNCONDITIONAL DDPM,0.22707423580786026,"ùëéùëéùëèùëè
‡∑úùë¶ ùëèùëê"
UNCONDITIONAL DDPM,0.2314410480349345,ùõªùëãùë°ùëôùëúùëîùëùùúÉ(ùëãùë°)
UNCONDITIONAL DDPM,0.23580786026200873,"ùõªùëãùë°
Duration
Predictor"
UNCONDITIONAL DDPM,0.24017467248908297,1.8 2.3 0.7
UNCONDITIONAL DDPM,0.2445414847161572,"Speaker
Encoder [ùëã0"
UNCONDITIONAL DDPM,0.24890829694323144,"1, ‚Ä¶ , ùëã0 ùëÅ] ùëíùëÜ ùëíùëÜ ùëãùë°"
UNCONDITIONAL DDPM,0.25327510917030566,Phoneme
UNCONDITIONAL DDPM,0.2576419213973799,Classifier ùë°
UNCONDITIONAL DDPM,0.26200873362445415,"log ùëùùúô(‡∑úùë¶|ùëãùë°, ùëÜ) ùëãùë°"
UNCONDITIONAL DDPM,0.2663755458515284,Unconditional DDPM ùë°
UNCONDITIONAL DDPM,0.27074235807860264,"Figure 1: The overall architecture of Guided-TTS. The unconditional DDPM learns to generate
speech X0 without the transcript. The other modules, the phoneme classiÔ¨Åer, duration predictor, and
speaker encoder are for guiding the unconditional DDPM to generate conditional samples given y.
The speaker embedding eS is only used for training speaker-dependent modules."
UNCONDITIONAL DDPM,0.27510917030567683,"Similar to Grad-TTS (Popov et al. (2021)), we regard mel-spectrogram as a 2D image with a single
channel and use the U-Net architecture (Ronneberger et al. (2015)) as sŒ∏. We use the same size of
the architecture used to model 32 √ó 32 sized images in Ho et al. (2020) to capture long-term depen-
dencies without any text information, while Grad-TTS uses smaller architecture for the conditional
distribution modeling."
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.2794759825327511,"3.2
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE"
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.2838427947598253,"For TTS synthesis, we use a framewise phoneme classiÔ¨Åer to guide the unconditional DDPM. As
shown in Fig. 1, in order to generate mel-spectrogram given text, our duration predictor outputs the
duration for each text token and expands the transcript y to frame-level phoneme label ÀÜy. Then, we
sample a random noise XT of the same length as ÀÜy from the standard normal distribution, and we
can generate conditional samples by replacing the unconditional score in Eq. (5) with the conditional
score. As in Eq. (8), we can estimate the conditional score on the left side by adding the two terms
on the right side: the Ô¨Årst term is obtained from the unconditional DDPM, and the second term can
be computed with the phoneme classiÔ¨Åer. That is, we achieve to build a text-to-speech model with
the unconditional generative model for the speech by adding the gradient of the phoneme classiÔ¨Åer
during the generative process."
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.28820960698689957,"‚àáXtlog p(Xt|ÀÜy, spk = S) = ‚àáXtlog pŒ∏(Xt|spk = S) + ‚àáXtlog pœÜ(ÀÜy|Xt, spk = S)
(8)"
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.2925764192139738,"If the transcribed speech dataset of the target speaker S is available, we train all the unconditional
DDPM, the phoneme classiÔ¨Åer, and the duration predictor on the dataset of the speaker S for TTS.
Otherwise, if we only have the untranscribed speech data for the target speaker S, we Ô¨Årst train
unconditional DDPM on the untranscribed speech data and leverage a large-scale multi-speaker
transcribed speech dataset to train the phoneme classiÔ¨Åer and the duration predictor. In this case, to
guide unconditional DDPM for the target speaker S, our phoneme classiÔ¨Åer and duration predictor
are designed to be speaker-dependent modules and to generalize well to the unseen speaker S dur-
ing training. We provide the speaker embedding extracted from the pre-trained speaker veriÔ¨Åcation
network as condition to both modules, as described in Fig. 1. We describe each module required for
guidance below."
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.29694323144104806,"Phoneme ClassiÔ¨Åer The phoneme classiÔ¨Åer is a network trained on transcribed data that recog-
nizes the phoneme corresponding to each frame of the input mel-spectrogram. To train the frame-
wise phoneme classiÔ¨Åer, we align transcript and speech using a forced alignment tool, Montreal
Forced Aligner (MFA) (McAuliffe et al. (2017)), and extract the frame-level phoneme label ÀÜy. The
phoneme classiÔ¨Åer is trained to classify the corrupted mel-spectrogram Xt sampled from Eq. (3) as
the frame-level phoneme label ÀÜy. The training objective of the phoneme classiÔ¨Åer is to maximize the"
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.30131004366812225,Under review as a conference paper at ICLR 2022
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.3056768558951965,"expectation of cross-entropy between the phoneme label ÀÜy and the output probability with respect to
t ‚àà[0, 1]."
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.31004366812227074,"We use a WaveNet-like architecture (van den Oord et al. (2016)) as a phoneme classiÔ¨Åer, and time
embedding et, which is extracted in the same way as in Popov et al. (2021), is used as a global
condition in WaveNet to provide information about the noise level of the corrupted input Xt at
timestep t. Additionally, for speaker-dependent classiÔ¨Åcation, we use the speaker embedding eS
from the speaker encoder as the global condition."
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.314410480349345,"Duration Predictor Duration predictor is a module that predicts the duration of each text token for
a given text sequence y. We extract the duration label of each text token using MFA for the same
data on which the phoneme classiÔ¨Åer is trained. The duration predictor is trained to minimize L2
loss between the duration label and the estimated duration in the log-domain, and we round up the
estimated duration during inference. The architecture of the duration predictor is the same as that
of Glow-TTS (Kim et al. (2020)) with the text encoder. We concatenate the text embedding and the
speaker embedding eS for training a speaker-dependent module."
TEXT-TO-SPEECH VIA CLASSIFIER GUIDANCE,0.31877729257641924,"Speaker Encoder Speaker encoder encodes the speaker information from the input mel-
spectrogram and outputs the speaker embedding eS. Similar to Jia et al. (2018), we train a speaker
encoder with GE2E loss (Wan et al. (2018)) on the speaker veriÔ¨Åcation dataset and use speaker en-
coder to condition speaker-dependent modules. To train speaker-dependent modules, we use speaker
embedding eS extracted from the clean mel-spectrogram X0 for each training data. For guidance,
we average and normalize the speaker embeddings of untranscribed speech for the desired speaker
S to extract eS."
NORM-BASED GUIDANCE,0.3231441048034934,"3.2.1
NORM-BASED GUIDANCE"
NORM-BASED GUIDANCE,0.32751091703056767,Algorithm 1 Norm-based Guidance
NORM-BASED GUIDANCE,0.3318777292576419,"ÀÜy: framewise phoneme label, s: gradient scale, œÑ: temperature
Œ∏: parameter of unconditional DDPM:, œÜ: parameter of phoneme classiÔ¨Åer
X1 ‚àºN(0, œÑ ‚àí1I)
for i = N to 1 do"
NORM-BASED GUIDANCE,0.33624454148471616,"t ‚Üê
i
N
Œ±t ‚Üê‚à•‚àáXt log pŒ∏(Xt)‚à•/ ‚à•‚àáXt log pœÜ(ÀÜy|Xt)‚à•
zt ‚àºN(0, œÑ ‚àí1I) Xt‚àí1"
NORM-BASED GUIDANCE,0.3406113537117904,N ‚ÜêXt + Œ≤t N ( 1
NORM-BASED GUIDANCE,0.34497816593886466,"2Xt + ‚àáXt log pŒ∏(Xt) + s ¬∑ Œ±t‚àáXt log pœÜ(ÀÜy|Xt))) +
q"
NORM-BASED GUIDANCE,0.34934497816593885,"Œ≤t
N zt
end for
return X0"
NORM-BASED GUIDANCE,0.3537117903930131,"Initially, we scaled classiÔ¨Åer gradient ‚àáXt log pœÜ(ÀÜy|Xt, spk = S) in Eq. (8) using gradient scale s,
which is the scaled version of Song et al. (2021b). However, when guiding the unconditional DDPM
with the framewise phoneme classiÔ¨Åer, we found that the norm of the unconditional score suddenly
increases near the data. That is, the closer to the data, the phoneme classiÔ¨Åer has little effect on
the generative process of DDPM. The norm of unconditional score ‚à•‚àáXt log pŒ∏(Xt)‚à•according to
timestep t is shown in Fig. 4 of Appendix A.4. Here, we propose norm-based guidance to guide
the unconditional DDPM better in terms of generating speech conditioned on frame-level phoneme
label ÀÜy. Norm-based guidance is a method of scaling the norm of the classiÔ¨Åer gradient in proportion
to the norm of the score in order to prevent the effect of the gradient from being insigniÔ¨Åcant as the
score rises steeply. The ratio between the norm of the scaled gradient and the norm of the score is
deÔ¨Åned as the gradient scale s. By adjusting s, we can determine how much the classiÔ¨Åer gradient
contributes to the guidance of unconditional DDPM. We also introduced the temperature parameter
œÑ when guiding the DDPM. We observed that tuning œÑ to a value greater than 1 helps generate
high-quality mel-spectrograms."
EXPERIMENTS,0.35807860262008734,"4
EXPERIMENTS"
EXPERIMENTS,0.3624454148471616,"Datasets For experiments using a transcribed single dataset, we train our unconditional DDPM and
phoneme classiÔ¨Åer respectively on speech-only LJSpeech (Ito (2017)) and speech-transcript paired"
EXPERIMENTS,0.36681222707423583,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.37117903930131,"LJSpeech to match the settings of baseline TTS models. In addition, since our method makes use of
a set of separately trained unconditional DDPM and phoneme classiÔ¨Åer, they can also be trained with
different datasets respectively. Here, by training the unconditional DDPM with a speech-only target
speaker dataset and the phoneme classiÔ¨Åer with a transcribed large-scale multi-speaker dataset, we
can build a high-quality TTS model from any speciÔ¨Åc speaker dataset without any transcript. For
the corresponding experiments, we use only the speech data from LJSpeech, Hi-Fi TTS (Bakhturina
et al. (2021)), and Blizzard 2013 (King & Karaiskos (2013)) for training unconditional DDPMs,
and LibriTTS (Zen et al. (2019)), a large-scale multi-speaker dataset with approximately 585 hours
of speech uttered by 2456 speakers with corresponding texts, for training the speaker-dependent
phoneme classiÔ¨Åer to guide the various unconditional generative models. To extract the speaker
embedding eS from each utterance, we train a speaker encoder on VoxCeleb2 (Chung et al. (2018)),
which is a speaker veriÔ¨Åcation dataset that contains more than 1M utterances for 6112 speakers."
EXPERIMENTS,0.37554585152838427,"LJSpeech is a 24-hour single female speaker dataset consisting of 13,100 audio clips. Dataset is
randomly split; 12,500 samples for the training set, 100 samples for the validation set, and 500
samples for the test set. Hi-Fi TTS is a multi-speaker dataset with 6 females and 4 males, and each
speaker‚Äôs data consists of at least 17 hours of speech. We select three of them (two males and one
female) and use them for training three unconditional DDPMs, respectively. Blizzard 2013 is an 147
hours segmented and unsegmented audiobook data read by a single female speaker. We only use
5-second randomly clipped audio of unsegmented data to show that our model does not require text
labeling or aligning for untranscribed data. Only audio Ô¨Åles are used to train unconditional DDPM."
EXPERIMENTS,0.3799126637554585,"Training Details We convert text into International Phonetic Alphabet (IPA) phoneme sequences us-
ing open-source software (Bernard (2021)). To extract the mel-spectrogram, we use the same hyper-
parameters as Glow-TTS (Kim et al. (2020)). All modules are trained using Adam optimizer with a
learning rate of 0.0001. For the unconditional model and the phoneme classiÔ¨Åer, Œ≤0 = 0.05, Œ≤1 = 20
are used for beta schedule. For unconditional model, the base model has the same architecture and
hyperparameters as DDPM (Ho et al. (2020)) used for 32 √ó 32 sized image modeling, and for ab-
lation, we use the same architecture and hyperparameters for the small model as Grad-TTS (Popov
et al. (2021)). Other details and hyperparameters are described in the Appendix A.2."
EXPERIMENTS,0.38427947598253276,"Evaluation To compare the performance of models, we use pre-trained models and the ofÔ¨Åcial
implementations of Glow-TTS and Grad-TTS.2 For Glow-TTS, we use a pre-trained model with
blank tokens between phonemes and use œÑ = 1.5. We use the same hyperparameters as the ofÔ¨Åcial
implementation, œÑ = 1.5, and the number of reverse steps N = 50 for Grad-TTS. For our model,
we set œÑ = 1.5, gradient scale s = 0.3, and the number of reverse steps N = 50. We use ofÔ¨Åcial
implementation and pre-trained models of HiFi-GAN as a vocoder.3"
EXPERIMENTS,0.388646288209607,"To show whether Guided-TTS with the norm-based guidance generates the sentences of the given
text accurately, we measure the character error rate (CER) and word error rate (WER) for each
model, which are metrics commonly used in automatic speech recognition (ASR). We use the pre-
trained CTC-based conformer (Graves et al. (2006), Gulati et al. (2020)) model provided by NEMO
toolkit (Kuchaiev et al. (2019)) to compute the metrics."
RESULTS,0.3930131004366812,"5
RESULTS"
MODEL COMPARISON,0.39737991266375544,"5.1
MODEL COMPARISON"
MODEL COMPARISON,0.4017467248908297,"We compare the performance of audio samples by measuring the 5-scale mean opinion score (MOS)
on LJSpeech using Amazon Mechanical Turk. In addition, through CER and WER, we check
whether the generated sample of Guided-TTS faithfully reÔ¨Çects the text. To calculate CER and
WER, we Ô¨Årst synthesize the speech of a given text for each model and provide it to the ASR model
to extract the text corresponding to the generated sample. We then measure the CER and WER
between the ground truth text and the text obtained from the ASR model. For evaluation, we ran-
domly select 50 samples drawn from the test set of LJSpeech, and the results are shown in Table
1.4 In Table 1, Guided-TTS-T (T:Transcribed) is speciÔ¨Åed for the case of training the unconditional"
MODEL COMPARISON,0.40611353711790393,"2Glow-TTS: https://bit.ly/3kS315K, Grad-TTS: https://bit.ly/3qTCmcJ
3HiFi-GAN: https://bit.ly/3FxBv5x
4Demo : https://bit.ly/3oWhVJg"
MODEL COMPARISON,0.4104803493449782,Under review as a conference paper at ICLR 2022
MODEL COMPARISON,0.4148471615720524,"DDPM and the other modules on the speech data of the target speaker. The quality of Guided-TTS-T
on LJSpeech shows that our model achieves TTS performance comparative to the baseline models.
Furthermore, the ASR metrics of Guided-TTS-T indicate that our model generates speech samples
according to given text stably like the existing TTS models do. Unlike Grad-TTS, which leverages
the conditional DDPM for TTS, our unconditional DDPM requires a large model size for model-
ing long-term dependency of speech without text. The qualitative results of Guided-TTS-T with the
small unconditional model (Guided-TTS-T (small)), whose architecture is the same as the decoder
of Grad-TTS, demonstrate the importance of modeling capability of unconditional DDPM for the
naturalness of the samples and accurate pronunciation."
MODEL COMPARISON,0.4192139737991266,"Guided-TTS-U (U:Untranscribed) is the case that we train unconditional DDPM using only the tar-
get speaker‚Äôs text-free speech and guide it using a speaker-dependent phoneme classiÔ¨Åer separately
trained on LibriTTS. Guided-TTS-U shows performance and ASR metrics comparable to other TTS
models, which require transcribed data of the desired speaker for training. This demonstrates that
Guided-TTS enables building a high-quality TTS model using the untranscribed speech of the target
speaker. Samples of all models are available on the demo page."
MODEL COMPARISON,0.42358078602620086,"Table 1: Mean Opinion Score (MOS) with 95% conÔ¨Ådence intervals and Automated Speech Recog-
nition (ASR) metrics of TTS models for LJSpeech. The ASR metric indicates character error rate
(CER) and word error rate (WER) measured by the pre-trained ASR model. The unconditional
DDPM in Guided-TTS uses only untranscribed speech of LJSpeech for training. Guided-TTS-T is a
model whose phoneme classiÔ¨Åer and duration predictor are trained using the transcript of LJSpeech.
Guided-TTS-U uses a multi-speaker dataset instead of the transcript of LJSpeech to train phoneme
classiÔ¨Åer and duration predictor."
MODEL COMPARISON,0.4279475982532751,"Method
Transcript of LJSpeech
5-scale MOS
CER/WER(%)"
MODEL COMPARISON,0.43231441048034935,"Ground Truth
4.46 ¬± 0.05
0.79 / 3.77
Ground Truth (Mel + HiFi-GAN)
4.25 ¬± 0.07
1.05 / 4.08
Glow-TTS

4.10 ¬± 0.10
1.09 / 5.03
Grad-TTS

4.25 ¬± 0.09
1.31 / 5.55
Guided-TTS-T

4.18 ¬± 0.07
1.20 / 4.71
Guided-TTS-T (small)

4.01 ¬± 0.07
1.65 / 5.97
Guided-TTS-U

4.18 ¬± 0.07
1.26 / 4.61"
MODEL COMPARISON,0.4366812227074236,"Table 2: Mean Opinion Score (MOS) of Guided-TTS-U with various untranscribed datasets with
95% conÔ¨Ådence intervals. In Guided-TTS-U, the phoneme classiÔ¨Åer and the duration predictor are
trained on LibriTTS dataset, and the speaker encoder is trained on VoxCeleb2 dataset."
MODEL COMPARISON,0.4410480349344978,"Method
Untranscribed Data
5-scale MOS
Total duration (hrs)
Gender"
MODEL COMPARISON,0.44541484716157204,"LJSpeech
4.18 ¬± 0.07
24
Female
Hi-Fi TTS ID: 92
3.97 ¬± 0.08
27.3
Female
Guided-TTS-U
Hi-Fi TTS ID: 6097
3.82 ¬± 0.08
30.1
Male
Hi-Fi TTS ID: 9017
3.72 ¬± 0.08
58.0
Male
Blizzard 2013
3.85 ¬± 0.08
149.4
Female"
GENERALIZATION TO DIVERSE DATASETS,0.4497816593886463,"5.2
GENERALIZATION TO DIVERSE DATASETS"
GENERALIZATION TO DIVERSE DATASETS,0.45414847161572053,"We show that our model can synthesize high-quality speech of LJSpeech using untranscribed speech
in the previous section. Since our model trains the unconditional model and the classiÔ¨Åer separately,
TTS for various untranscribed datasets is possible with a single classiÔ¨Åer trained with a large-scale
paired multi-speaker dataset. The performance of our multiple TTS models using the untranscribed
speech of target speaker is presented in Table 2. For evaluation, we used the same 50 randomly cho-
sen sentences from the test set of LJSpeech. In Table 2, we show that if we have the phoneme clas-
siÔ¨Åer, duration predictor, and speaker encoder trained on a large-scale multi-speaker dataset, we can"
GENERALIZATION TO DIVERSE DATASETS,0.4585152838427948,Under review as a conference paper at ICLR 2022
GENERALIZATION TO DIVERSE DATASETS,0.462882096069869,"build a TTS model using untranscribed speech data of any speaker without additional burdens such
as transcription and segmentation. In particular, Guided-TTS-U synthesizes high-quality samples
even when trained on a randomly cropped unsegmented Blizzard 2013 dataset. Our method enables
TTS for diverse untranscribed datasets with various characteristics (e.g., gender, American/British
accent, prosody). There are samples on the demo page for diverse datasets."
ANALYSIS ON NORM-BASED GUIDANCE,0.4672489082969432,"5.3
ANALYSIS ON NORM-BASED GUIDANCE"
ANALYSIS ON NORM-BASED GUIDANCE,0.47161572052401746,"We also compare the proposed norm-based classiÔ¨Åer guidance with the classiÔ¨Åer guidance used
in previous works (Song et al. (2021b); Dhariwal & Nichol (2021)). A model that performs the
conditional generation task with classiÔ¨Åer guidance sometimes generates samples of conditions
other than the target condition (Song et al. (2021b)). Similarly, we observe that Guided-TTS with
the previous guidance method produces mispronounced samples given text. To show the effect of
norm-based guidance and adjustment of gradient scale, we measure CER of Guided-TTS-T and
Guided-TTS-U for LJSpeech according to the gradient scale s. We explore the gradient scale s
within [0.5, 1.0, ..., 4.5] for the scaled version of the previous method (Song et al. (2021b)), and
[0.1, 0.2, ..., 1.0] for the norm-based guidance."
ANALYSIS ON NORM-BASED GUIDANCE,0.4759825327510917,"1
2
3
4
Gradient scale s 0 2 4 6 8 10 12 14"
ANALYSIS ON NORM-BASED GUIDANCE,0.48034934497816595,CER (%)
ANALYSIS ON NORM-BASED GUIDANCE,0.4847161572052402,"Guided-TTS-T
Guided-TTS-U
Glow-TTS
Grad-TTS"
ANALYSIS ON NORM-BASED GUIDANCE,0.4890829694323144,(a) ClassiÔ¨Åer guidance in Song et al. (2021b)
ANALYSIS ON NORM-BASED GUIDANCE,0.49344978165938863,"0.2
0.4
0.6
0.8
1.0
Gradient scale s 0 2 4 6 8 10 12 14"
ANALYSIS ON NORM-BASED GUIDANCE,0.4978165938864629,CER (%)
ANALYSIS ON NORM-BASED GUIDANCE,0.5021834061135371,"Guided-TTS-T
Guided-TTS-U
Glow-TTS
Grad-TTS"
ANALYSIS ON NORM-BASED GUIDANCE,0.5065502183406113,(b) Norm-based classiÔ¨Åer guidance
ANALYSIS ON NORM-BASED GUIDANCE,0.5109170305676856,"Figure 2: CER according to gradient scales, (a) CER of Guided-TTS with the classiÔ¨Åer guidance
(scaled version of Song et al. (2021b)), (b) CER of Guided-TTS with the norm-based guidance"
ANALYSIS ON NORM-BASED GUIDANCE,0.5152838427947598,"Fig. 2 presents the CER of the Guided-TTS with the classiÔ¨Åer guidance in the previous works and the
proposed norm-based guidance. As shown in Fig. 2a, the sample generated by the existing guidance
method shows a far worse CER than the existing TTS models, which indicates that it is not suitable
for TTS. On the other hand, Fig. 2b shows that the proposed guidance method with the appropriate
gradient scale helps generate samples given text sentences accurately like the existing TTS models."
ANALYSIS ON NORM-BASED GUIDANCE,0.519650655021834,"If the gradient scale is too small, the effect of the classiÔ¨Åer gradient is negligible, and generated
samples do not reÔ¨Çect the given text. On the other hand, we observed that guidance with a gradient
scale too large deteriorates the sample quality. For the proposed norm-based guidance, we set the
default gradient scale s to 0.3, which generates high-quality samples that exactly match the given
text. Samples for multiple gradient scales with each guidance method are on the demo page."
RELATED WORK,0.5240174672489083,"6
RELATED WORK"
RELATED WORK,0.5283842794759825,"Unconditional Speech Generation In general, the unconditional speech generative model (van den
Oord et al. (2016); Vasquez & Lewis (2019)), which models audio without any information, is
more challenging than the conditional generative model that synthesizes speech using text or mel-
spectrograms. Several works attempt to unconditionally generate raw waveforms (van den Oord
et al. (2016); Donahue et al. (2019)) or to model the unconditional distribution of latent code or
mel-spectrogram of audio (van den Oord et al. (2017); Vasquez & Lewis (2019)) instead of directly
modeling raw waveforms. Most existing unconditional models have only been used for uncondi-
tional audio modeling and no other purpose. To the best of our knowledge, this is the Ô¨Årst applica-"
RELATED WORK,0.5327510917030568,Under review as a conference paper at ICLR 2022
RELATED WORK,0.537117903930131,"tion of an unconditional model for TTS with appropriate guidance to enable speech synthesis using
untranscribed data of the target speaker."
RELATED WORK,0.5414847161572053,"Text-to-Speech Models Most text-to-speech (TTS) models are composed of two parts: a model
that generates intermediate features (e.g., mel-spectrogram) from text (Shen et al. (2018)) and the
vocoder, which synthesizes raw waveforms from intermediate features (van den Oord et al. (2016)).
The autoregressive model is used for the text-to-intermediate feature model (Wang et al. (2017);
Shen et al. (2018); Ping et al. (2018); Li et al. (2019)) and vocoder (van den Oord et al. (2016);
Kalchbrenner et al. (2018)) to perform high-quality TTS. Since the autoregressive models generate
samples in a sequential manner, their inference speed is slow. Thus, various parallel TTS mod-
els have been proposed to improve the sampling speed. Flow-based generative models (Kingma &
Dhariwal (2018)) and feed-forward models have been proposed for text-to-mel-spectrogram models
(Ren et al. (2019); Ren et al. (2021); Kim et al. (2020); Shih et al. (2021)) and vocoders (Oord et al.
(2018); Prenger et al. (2019); Kim et al. (2019)). Also, variational autoencoder (Kingma & Welling
(2014)) based models (Lee et al. (2020); Liu et al. (2021)), diffusion (Ho et al. (2020)) based models
(Chen et al. (2021a); Kong et al. (2021); Popov et al. (2021); Jeong et al. (2021)), GAN (Goodfellow
et al. (2014)) based models (Kumar et al. (2019); Bi¬¥nkowski et al. (2019); Kong et al. (2020)) have
been proposed as high-quality speech synthesis models with parallel sampling schemes."
RELATED WORK,0.5458515283842795,"Recently, end-to-end TTS models have been proposed, such as Ren et al. (2021), Donahue et al.
(2021), Weiss et al. (2021), Kim et al. (2021), and Chen et al. (2021b). Most previous TTS models
perform conditional generation tasks using transcribed speech data of the target speaker. We propose
a new TTS model that learns to generate speech of the target speaker using untranscribed data and
guides it through a phoneme classiÔ¨Åer trained with other transcribed datasets. By guiding the uncon-
ditional model with the independently trained classiÔ¨Åer, we achieve a high-performance TTS model
using untranscribed data of the desired speaker without sentence-level segmentation or transcription."
RELATED WORK,0.5502183406113537,"Diffusion-based Generative Models DDPM (Ho et al. (2020)) has undergone several theoretical
developments (Song et al. (2021b)) and produces high quality samples in many domains (Ho et al.
(2020); Dhariwal & Nichol (2021); Chen et al. (2021a); Popov et al. (2021); Luo & Hu (2021)).
Many theoretical and practical breakthroughs (Nichol & Dhariwal (2021); Lam et al. (2021); Chen
et al. (2021a); Song et al. (2021a); Watson et al. (2021); Kong & Ping (2021); Jolicoeur-Martineau
et al. (2021)) have been proposed, and a continuous version of DDPM, an SDE-based model (Song
et al. (2021b); Popov et al. (2021)) is also presented. DDPM has also shown strong performance in
speech synthesis (Chen et al. (2021a); Kong et al. (2021); Popov et al. (2021); Jeong et al. (2021))."
RELATED WORK,0.5545851528384279,"A pre-trained unconditional DDPM can be used for various tasks such as imputation (Song et al.
(2021b)), and controllable generation (Song et al. (2021b)). In particular, the controllable genera-
tion allows Dhariwal & Nichol (2021) to achieve state-of-the-art performance in class-conditional
image generation by guiding the DDPM using a gradient from the classiÔ¨Åer trained on the same
dataset as DDPM. We introduce the classiÔ¨Åer guidance method of unconditional DDPM to text-to-
speech synthesis. Our unconditional DDPM and the phoneme classiÔ¨Åer can be trained using different
datasets, making it possible to build a TTS model with the target speaker‚Äôs untranscribed speech."
CONCLUSION,0.5589519650655022,"7
CONCLUSION"
CONCLUSION,0.5633187772925764,"In this work, we present Guided-TTS, a new type of TTS model that generates speech given tran-
script by guiding the unconditional diffusion-based model for speech. As Guided-TTS models un-
conditional distribution for speech, the proposed model can construct a TTS model using the desired
speaker‚Äôs untranscribed data. Thanks to the properties of diffusion-based generative models, our
unconditional generative model can generate a speech when a transcript is given by introducing
the separately trained phoneme classiÔ¨Åer. To the best of our knowledge, Guided-TTS is the Ô¨Årst
TTS model to leverage the unconditional generative model for speech. We showed that Guided-TTS
matches the performance of the previous TTS models on LJSpeech. Moreover, Guided-TTS without
the transcript of the target speaker generates samples comparable to existing models using tran-
scripts by training its phoneme classiÔ¨Åer on a large-scale multi-speaker dataset. We also showed that
the single well-performed phoneme classiÔ¨Åer can guide various unconditional DDPMs to generate
high-quality sample. We believe that Guided-TTS can reduce the burden of constructing training
datasets for high-quality TTS."
CONCLUSION,0.5676855895196506,Under review as a conference paper at ICLR 2022
REFERENCES,0.5720524017467249,REFERENCES
REFERENCES,0.5764192139737991,"Brian D O Anderson. Reverse-time diffusion equation models. Stochastic Process. Appl., 12(3):
313‚Äì326, May 1982."
REFERENCES,0.5807860262008734,"Evelina Bakhturina, Vitaly Lavrukhin, Boris Ginsburg, and Yang Zhang. Hi-Ô¨Åmulti-speaker english
tts dataset. arXiv preprint arXiv:2104.01497, 2021."
REFERENCES,0.5851528384279476,"Mathieu Bernard. Phonemizer. https://github.com/bootphon/phonemizer, 2021."
REFERENCES,0.5895196506550219,"Miko≈Çaj Bi¬¥nkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman
Casagrande, Luis C Cobo, and Karen Simonyan. High Fidelity Speech Synthesis with Adver-
sarial Networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.5938864628820961,"Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wave-
Grad: Estimating Gradients for Waveform Generation. In International Conference on Learning
Representations, 2021a."
REFERENCES,0.5982532751091703,"Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, Najim Dehak, and William
Chan. WaveGrad 2: Iterative ReÔ¨Ånement for Text-to-Speech Synthesis. In Proc. Interspeech 2021,
pp. 3765‚Äì3769, 2021b. doi: 10.21437/Interspeech.2021-1897."
REFERENCES,0.6026200873362445,"J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker recognition. In INTER-
SPEECH, 2018."
REFERENCES,0.6069868995633187,"Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint
arXiv:2105.05233, 2021."
REFERENCES,0.611353711790393,"C. Donahue, J. McAuley, and M. Puckette. Adversarial audio synthesis. International Conference
on Learning Representations (ICLR), 2019."
REFERENCES,0.6157205240174672,"Jeff Donahue, Sander Dieleman, Mikolaj Binkowski, Erich Elsen, and Karen Simonyan. End-to-
end Adversarial Text-to-Speech. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=rsf1z-JSj87."
REFERENCES,0.6200873362445415,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014."
REFERENCES,0.6244541484716157,"Alex Graves, Santiago Fern¬¥andez, Faustino Gomez, and J¬®urgen Schmidhuber. Connectionist tem-
poral classiÔ¨Åcation: labelling unsegmented sequence data with recurrent neural networks. In Pro-
ceedings of the 23rd international conference on Machine learning, pp. 369‚Äì376, 2006."
REFERENCES,0.62882096069869,"Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo
Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented
Transformer for Speech Recognition. In Proc. Interspeech 2020, pp. 5036‚Äì5040, 2020. doi:
10.21437/Interspeech.2020-3015."
REFERENCES,0.6331877729257642,"Jonathan Ho, Ajay Jain, and Pieter Abbeel.
Denoising Diffusion Probabilistic Models.
In Ad-
vances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, volume 33. Curran As-
sociates, Inc., 2020."
REFERENCES,0.6375545851528385,"Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017."
REFERENCES,0.6419213973799127,"Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-
TTS: A Denoising Diffusion Model for Text-to-Speech. In Proc. Interspeech 2021, pp. 3605‚Äì
3609, 2021. doi: 10.21437/Interspeech.2021-469."
REFERENCES,0.6462882096069869,"Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, zhifeng Chen, Patrick Nguyen,
Ruoming Pang, Ignacio Lopez Moreno, and Yonghui Wu. Transfer learning from speaker veriÔ¨Åca-
tion to multispeaker text-to-speech synthesis. In S. Bengio, H. Wallach, H. Larochelle, K. Grau-
man, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.
cc/paper/2018/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf."
REFERENCES,0.6506550218340611,Under review as a conference paper at ICLR 2022
REFERENCES,0.6550218340611353,"Alexia Jolicoeur-Martineau, Ke Li, R¬¥emi Pich¬¥e-Taillefer, Tal Kachman, and Ioannis Mitliagkas.
Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080,
2021."
REFERENCES,0.6593886462882096,"Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lock-
hart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. EfÔ¨Åcient neural
audio synthesis. In International Conference on Machine Learning, pp. 2410‚Äì2419. PMLR, 2018."
REFERENCES,0.6637554585152838,"Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-TTS: A Generative Flow for
Text-to-Speech via Monotonic Alignment Search. Advances in Neural Information Processing
Systems, 33, 2020."
REFERENCES,0.6681222707423581,"Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial
learning for end-to-end text-to-speech. arXiv preprint arXiv:2106.06103, 2021."
REFERENCES,0.6724890829694323,"Sungwon Kim, Sang-Gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon. Flowavenet: A
generative Ô¨Çow for raw audio. In International Conference on Machine Learning, pp. 3370‚Äì3378,
2019."
REFERENCES,0.6768558951965066,"Simon J. King and Vasilis Karaiskos. The blizzard challenge 2013. In In Blizzard Challenge Work-
shop, 2013."
REFERENCES,0.6812227074235808,"Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative Ô¨Çow with invertible 1x1 convolutions.
In Advances in Neural Information Processing Systems, pp. 10236‚Äì10245, 2018."
REFERENCES,0.6855895196506551,"Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference
on Learning Representations (ICLR), 2014."
REFERENCES,0.6899563318777293,"Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.
HiFi-GAN: Generative Adversarial networks
for EfÔ¨Åcient and High Fidelity Speech Synthesis. Advances in Neural Information Processing
Systems, 33, 2020."
REFERENCES,0.6943231441048034,"Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In ICML Workshop
on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021. URL
https://openreview.net/forum?id=agj4cdOfrAP."
REFERENCES,0.6986899563318777,"Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A Versatile
Diffusion Model for Audio Synthesis. In International Conference on Learning Representations,
2021."
REFERENCES,0.7030567685589519,"Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel
Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al. Nemo: a toolkit for building ai
applications using neural modules. arXiv preprint arXiv:1909.09577, 2019."
REFERENCES,0.7074235807860262,"Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Br¬¥ebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. In Advances in Neural Information Processing
Systems 32, pp. 14910‚Äì14921, 2019."
REFERENCES,0.7117903930131004,"Max WY Lam, Jun Wang, Rongjie Huang, Dan Su, and Dong Yu. Bilateral denoising diffusion
models. arXiv preprint arXiv:2108.11514, 2021."
REFERENCES,0.7161572052401747,"Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/."
REFERENCES,0.7205240174672489,"Yoonhyung Lee, Joongbo Shin, and Kyomin Jung.
Bidirectional variational inference for non-
autoregressive text-to-speech. In International Conference on Learning Representations, 2020."
REFERENCES,0.7248908296943232,"Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with trans-
former network. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 33, pp.
6706‚Äì6713, 2019."
REFERENCES,0.7292576419213974,"Peng Liu, Yuewen Cao, Songxiang Liu, Na Hu, Guangzhi Li, Chao Weng, and Dan Su. Vara-tts:
Non-autoregressive text-to-speech synthesis based on very deep vae with residual attention. arXiv
preprint arXiv:2102.06431, 2021."
REFERENCES,0.7336244541484717,Under review as a conference paper at ICLR 2022
REFERENCES,0.7379912663755459,"Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.
2837‚Äì2845, June 2021."
REFERENCES,0.74235807860262,"Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger.
Montreal forced aligner: Trainable text-speech alignment using kaldi. In INTERSPEECH, 2017."
REFERENCES,0.7467248908296943,"Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic mod-
els.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Confer-
ence on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
8162‚Äì8171. PMLR, 18‚Äì24 Jul 2021. URL https://proceedings.mlr.press/v139/
nichol21a.html."
REFERENCES,0.7510917030567685,"Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast
high-Ô¨Ådelity speech synthesis. In International conference on machine learning, pp. 3918‚Äì3926.
PMLR, 2018."
REFERENCES,0.7554585152838428,"Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan ¬®Omer Arik, Ajay Kannan, Sharan Narang,
Jonathan Raiman, and John Miller. Deep voice 3: Scaling text-to-speech with convolutional se-
quence learning. In International Conference on Learning Representations, 2018."
REFERENCES,0.759825327510917,"Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS:
A Diffusion Probabilistic Model for Text-to-Speech. In Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research, pp. 8599‚Äì8608. PMLR, 2021."
REFERENCES,0.7641921397379913,"Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A Ô¨Çow-based generative network for
speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 3617‚Äì3621. IEEE, 2019."
REFERENCES,0.7685589519650655,"Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech:
Fast, Robust and Controllable Text to Speech. volume 32, pp. 3171‚Äì3180, 2019."
REFERENCES,0.7729257641921398,"Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech 2:
Fast and High-Quality End-to-End Text to Speech. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=piLPYqxtWuA."
REFERENCES,0.777292576419214,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234‚Äì241. Springer, 2015."
REFERENCES,0.7816593886462883,"Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang,
Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by con-
ditioning wavenet on mel spectrogram predictions. In 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 4779‚Äì4783. IEEE, 2018."
REFERENCES,0.7860262008733624,"Kevin J Shih, Rafael Valle, Rohan Badlani, Adrian Lancucki, Wei Ping, and Bryan Catanzaro. Rad-
tts: Parallel Ô¨Çow-based tts with robust alignment learning and diverse synthesis. In ICML Work-
shop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021."
REFERENCES,0.7903930131004366,"Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-
tional Conference on Learning Representations, 2021a. URL https://openreview.net/
forum?id=St1giarCHLP."
REFERENCES,0.7947598253275109,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Interna-
tional Conference on Learning Representations, 2021b. URL https://openreview.net/
forum?id=PxTIG12RRHS."
REFERENCES,0.7991266375545851,"A¬®aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016."
REFERENCES,0.8034934497816594,Under review as a conference paper at ICLR 2022
REFERENCES,0.8078602620087336,"Aaron van den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in
Neural Information Processing Systems, pp. 6306‚Äì6315, 2017."
REFERENCES,0.8122270742358079,"Sean Vasquez and Mike Lewis. Melnet: A generative model for audio in the frequency domain.
arXiv preprint arXiv:1906.01083, 2019."
REFERENCES,0.8165938864628821,"Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno.
Generalized end-to-end loss for
speaker veriÔ¨Åcation. In 2018 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 4879‚Äì4883, 2018. doi: 10.1109/ICASSP.2018.8462665."
REFERENCES,0.8209606986899564,"Yuxuan Wang, R.J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly,
Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis,
Rob Clark, and Rif A. Saurous. Tacotron: Towards End-to-End Speech Synthesis. In Proc. Inter-
speech 2017, pp. 4006‚Äì4010, 2017. doi: 10.21437/Interspeech.2017-1452."
REFERENCES,0.8253275109170306,"Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efÔ¨Åciently sam-
ple from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021."
REFERENCES,0.8296943231441049,"Ron J Weiss, RJ Skerry-Ryan, Eric Battenberg, Soroosh Mariooryad, and Diederik P Kingma. Wave-
tacotron: Spectrogram-free end-to-end text-to-speech synthesis. In ICASSP 2021-2021 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5679‚Äì5683.
IEEE, 2021."
REFERENCES,0.834061135371179,"Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu.
LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. In Proc. Interspeech 2019,
pp. 1526‚Äì1530, 2019. doi: 10.21437/Interspeech.2019-2441."
REFERENCES,0.8384279475982532,Under review as a conference paper at ICLR 2022
REFERENCES,0.8427947598253275,"A
APPENDIX"
REFERENCES,0.8471615720524017,"A.1
INPAINTING"
REFERENCES,0.851528384279476,"We perform the inpainting task to show how well the unconditional DDPM learns the dependen-
cies in mel-spectrogram. The pre-trained unconditional DDPM Ô¨Ålls out the masked part of the
mel-spectrogram. We use samples from three speakers; one female speaker (ID: 92) and one male
speaker (ID: 6097) from Hi-Fi TTS and a female speaker from LJSpeech. Two cross-shaped masks
(LJSpeech, Hi-Fi TTS male) and one binarized MNIST (LeCun & Cortes (2010)) mask (Hi-Fi TTS
female) are used for masking. We set 1000 as the number of reverse steps N and œÑ = 1.5 for
inpainting. The method of inpainting is the same as Song et al. (2021b), and the algorithm is as
follows:"
REFERENCES,0.8558951965065502,Algorithm 2 Inpainting Mel-spectrogram
REFERENCES,0.8602620087336245,"Binary Mask: M, Original mel-spectrogram: ÀÜ
X0
Œ∏: parameter of unconditional DDPM
X1 ‚àºN(0, œÑ ‚àí1I)
for i = N to 1 do"
REFERENCES,0.8646288209606987,"t ‚Üê
i
N
œÅ( ÀÜ
X0, t) ‚Üêe‚àí1"
R T,0.868995633187773,"2
R t
0 Œ≤sds ÀÜ
X0
Œª(t) ‚ÜêI ‚àíe‚àí
R t
0 Œ≤sds
ÀÜ
Xt ‚àºN(œÅ( ÀÜ
X0, t), Œª(t))
Xt ‚ÜêXt ‚äôM + ÀÜ
Xt ‚äô(1 ‚àíM)
zt ‚àºN(0, œÑ ‚àí1I) Xt‚àí1"
R T,0.8733624454148472,N ‚ÜêXt + Œ≤t N ( 1
R T,0.8777292576419214,"2Xt + ‚àáXt log pŒ∏(Xt)) +
q"
R T,0.8820960698689956,"Œ≤t
N zt
end for
return X0 ‚äôM + ÀÜ
X0 ‚äô(1 ‚àíM)"
R T,0.8864628820960698,"(a) Ground Truth
(b) Masked Input
(c) Inpainting"
R T,0.8908296943231441,LJ Speech
R T,0.8951965065502183,Hi-Fi TTS
R T,0.8995633187772926,(ID: 92)
R T,0.9039301310043668,Hi-Fi TTS
R T,0.9082969432314411,(ID: 6097)
R T,0.9126637554585153,"Figure 3: Mel-spectrogram inpainting results of unconditional DDPM trained on LJSpeech, and two
speakers (Speaker ID: 92, 6097) from Hi-Fi TTS."
R T,0.9170305676855895,"The inpainting results are shown in Fig. 3, where (a) is the original mel-spectrogram, (b) is the
masked mel-spectrogram, and (c) is the result of inpainting on the masked part. As shown in Fig. 3,
we show that our unconditional DDPM learns the adjacent frequency and temporal dependencies of
the mel-spectrogram. Samples of inpainting results are provided on the demo page."
R T,0.9213973799126638,"A.2
TRAINING DETAILS AND HYPERPARAMTERS"
R T,0.925764192139738,"In this section, we cover the training details and detailed hyperparameters. In Guided-TTS-T, all
modules are trained on LJSpeech. In Guided-TTS-U, we use only untranscribed data of the various
target speakers for training unconditional DDPMs and train a phoneme classiÔ¨Åer and a duration"
R T,0.9301310043668122,Under review as a conference paper at ICLR 2022
R T,0.9344978165938864,"predictor on LibriTTS. Alignment labels are required for training the phoneme classiÔ¨Åer and the
duration predictor, and we train MFA to extract the alignment using the same dataset used to train
the phoneme classiÔ¨Åer and duration predictor."
R T,0.9388646288209607,"For both Guided-TTS-T and Guided-TTS-U, the unconditional models are trained with batch size
16 for all datasets. The duration predictor is trained for 20 epochs with batch size 64. The phoneme
classiÔ¨Åer of Guided-TTS-T uses a WaveNet-like structure with 256 residual channels, 6 residual
blocks stacks of 3 dilated convolution layers, and is trained for 1000 epochs with batch size 64. For
Guided-TTS-U, we use a WaveNet-like structure with 512 residual channels, 3 residual blocks stacks
of 6 dilated convolution layers for the phoneme classiÔ¨Åer, and it is trained for 140 epochs with batch
size 64. The speaker encoder is a two-layer LSTM with 768 channels followed by a linear projection
layer to extract 256-dimensional speaker embedding eS, and trained for 300K iterations."
R T,0.9432314410480349,"For sampling, we use the last checkpoint for the unconditional DDPM and the speaker encoder. We
use the checkpoint of the epoch with the best metric for the phoneme classiÔ¨Åer (validation accuracy)
and the duration predictor (validation loss)."
R T,0.9475982532751092,"A.3
HARDWARE AND SAMPLING SPEED"
R T,0.9519650655021834,"We conduct all experiments and evaluations using NVIDIA‚Äôs RTX 8000 with 48GB memory. Al-
though our model is not focused on fast inference, Guided-TTS can perform real-time speech syn-
thesis on GPU for N = 50, which is the number of reverse steps we use for evaluation. We measure
the sampling speed of Guided-TTS using a real-time factor (RTF). We also measure how much
time it takes to compute the unconditional score (‚àáXt log pŒ∏(Xt)) and gradient of the classiÔ¨Åer
(‚àáXt log pœÜ(ÀÜy|Xt)). Guided-TTS-T achieves an RTF of 0.56, of which 0.39 is used to calculate the
score and 0.16 is used for classiÔ¨Åer gradient calculation. Similarly, Guided-TTS-U achieves an RTF
of 0.63, 0.39 for computing the score and 0.23 for computing the gradient of the classiÔ¨Åer."
R T,0.9563318777292577,"A.4
NORM OF THE UNCONDITIONAL SCORE AND CLASSIFIER GRADIENT"
R T,0.9606986899563319,"0.0
0.2
0.4
0.6
0.8
1.0
Time t 250 500 750 1000 1250 1500 1750"
R T,0.9650655021834061,"2000
||
Xtlogp (Xt)||"
R T,0.9694323144104804,(a) The norm of the unconditional score
R T,0.9737991266375546,"0.0
0.2
0.4
0.6
0.8
1.0
Time t 20 40 60 80 100 120"
R T,0.9781659388646288,"||
Xtlogp (y|Xt)||"
R T,0.982532751091703,(b) The gradient norm of the classiÔ¨Åer
R T,0.9868995633187773,Figure 4: The norm of the unconditional score and the classiÔ¨Åer gradient for each timestep
R T,0.9912663755458515,"The norm of the unconditional score and the gradient norm of the classiÔ¨Åer for each timestep are
shown in Fig. 4. We sample Xt at a total of 1000 timesteps (t ‚àà(
1
2000,
3
2000, ..., 1999"
R T,0.9956331877729258,"2000)) using the
Eq. (3) for all 500 samples from the test set of the LJSpeech. We then obtain the norm of the
unconditional score and the gradient of the classiÔ¨Åer using the sampled Xt with the modules of
Guided-TTS-T. Each norm is averaged over 500 samples for each timestep. As shown in Fig. 4a,
the norm of the unconditional score rises steeply around t = 0. This is about 70 times larger than
the norm of the classiÔ¨Åer gradient near t = 0 (Fig. 4b), which signiÔ¨Åcantly reduces the effect of
the classiÔ¨Åer guidance. To alleviate this problem, we propose the norm-based guidance in Section
3.2.1, which helps prevent both the gradient of the classiÔ¨Åer from being ignored and the issue of
synthesized speech not matching the text."
