Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.007751937984496124,"We propose a new approach for training medical image classification models using seg-
mentation masks, particularly effective in small dataset scenarios. By guiding the model’s
attention with segmentation masks toward relevant features, we significantly improve ac-
curacy for diagnosing Hydronephrosis. Evaluation of our model on identically distributed
data showed either the same or better performance with improvement up to 0.28 in AUROC
and up to 0.33 in AUPRC. Our method showed better generalization ability than baselines,
improving from 0.02 to 0.75 in AUROC and from 0.09 to 0.47 in AUPRC for four different
out-of-distribution datasets. The results show that models trained on smaller datasets using
our approach can achieve comparable results to those trained on datasets 25 times larger.
The source code is available at github.com/MeriDK/segmentation-guided-attention.
Keywords: Medical Image Classification, Domain Generalization, Hydronephrosis."
ABSTRACT,0.015503875968992248,Rizhko et al.
INTRODUCTION,0.023255813953488372,1. Introduction
INTRODUCTION,0.031007751937984496,"Can a model trained to predict medical diagnoses for young children maintain the same
level of accuracy for older children? If a model is trained with data from one hospital, will
it perform just as efficiently with data from another? How precise will a model be when
analyzing images produced by different machines? Machine Learning (ML) models struggle
with these scenarios since they are trained on identically distributed (i.i.d.) data. However,
their accuracy can vary significantly when these models are tested on out-of-distribution
(OOD) data. The problem is known as domain shift between i.i.d source and OOD target
domains. It occurs when models are not trained to deal with the domain shift in mind.
This issue is significant for the medical field, where labeled data is limited, and training
different models for each scenario is impractical.
Domain shift is a challenge that extends beyond healthcare. The task of addressing
domain shift is known as Domain Generalization (DG), a problem that exists in almost
every application of ML (Zhou et al., 2022). For example, in the semantic segmentation task
in autonomous driving, a model trained on urban data may fail in rural settings (Hoffman
et al., 2018; Ros et al., 2016), potentially leading to accidents. In personal identification
systems, the model trained on well-illuminated images may not recognize a person in dim
lighting (Sun et al., 2019; Li et al., 2020), potentially preventing access to their home if the
lights are broken. Even with seemingly simple tasks like recognizing handwritten digits,
ML models can underperform due to minor variations like ink color (Ganin and Lempitsky,
2015).
Similarly, in the medical domain, a model trained on images collected with one
protocol might be ineffective for images collected through another (Liu et al., 2020). These
examples underline the significance of the problem across different domains.
An excellent survey (Zhou et al., 2022) categorizes various DG methodologies.
The
Domain Alignment (Li et al., 2018b,d) methods focus on learning a mapping function
between the domain and target datasets. Meta-learning (Li et al., 2018a; Balaji et al., 2018)
approaches divide data into meta-train and meta-test sets, where a model is trained on the
meta-train set and evaluated on the meta-test set. The methods separate domain-specific
and domain-agnostic features within datasets in the Learning Disentangled Representations
(Li et al., 2017; Ilse et al., 2020) category. While Domain Alignment, Meta-learning, and
Learning Disentangled Representations offer promising approaches, they require a labeled
target dataset during training on a domain dataset. The target dataset is usually unavailable
during training in the medical domain, so other approaches should be used.
The DG survey (Zhou et al., 2022) also covers the methodologies that do not require
a target dataset during the training. Data augmentations (Volpi et al., 2018; Volpi and
Murino, 2019; Xu et al., 2020) simulate a domain shift by changing images.
Ensemble
learning (Xu et al., 2014; Cha et al., 2021) trains the same model with a different random
seed for weight initialization or data split. Self-supervised learning (Carlucci et al., 2019;
Bucci et al., 2021) lets a model learn generic features of your data first and then fine-tune
the model for a downstream task. Regularization Strategies (Wang et al., 2019; Huang et al.,
2020) learn generalized features by focusing on global structure instead of local patterns or
by masking out over dominant features. All of these approaches are generally considered to
make more robust models. However, when trained on small datasets, which is usually the
case for the medical domain, their performance might suffer on i.i.d and OOD data."
INTRODUCTION,0.03875968992248062,Segmentation Masks for Image Classification
INTRODUCTION,0.046511627906976744,"before
after
before
after"
INTRODUCTION,0.05426356589147287,"Figure 1: Illustration of our approach. Before: focus on both organ and background noise.
After: targeted focus on the critical organ."
INTRODUCTION,0.06201550387596899,"To address this limitation we utilize Gradient-weighted Class Activation Mapping (Grad-
CAM) (Selvaraju et al., 2017) to create attention maps. GradCAM is a visual explanation
method in computer vision that highlights the regions in an input image that influence
the model’s outcome the most. Prior work showed that this attention mechanism could
be learned, resulting in a better performance for image segmentation (Li et al., 2018c)
and classification tasks (Fukui et al., 2019). This idea has been adapted for the medical
domain: it showed improved accuracy for thyroid nodules (Lu et al., 2022), for chest X-ray
abnormality localization and diagnosis (Ouyang et al., 2020b), for diagnosis of COVID-19
(Ouyang et al., 2020a) and dementia (Lian et al., 2020, 2019). However, these prior studies
used large datasets, ranging from 2,000 MRI scans to as many as 1.2 million images from
ImageNet, and they focused only on i.i.d. data. In contrast, we apply the idea to DG tasks
and demonstrate its effectiveness on small datasets with less than 100 images.
Our study addresses the typical scenario in the medical field where models are trained on
small datasets. Typically, these models learn specific ”useful” noise patterns, leading to high
performance on similar (i.i.d) test datasets. However, their accuracy declines when applied
to new images without these noise patterns. Beyond basic classification tasks, models should
also be trained to disregard features known a priori to be irrelevant. For instance, in kidney
ultrasound classification (see Figure 1), the model should focus only on the kidney, ignoring
background noise.
Often, in medical imaging, additional information like segmentation
masks is available. We adapt gradient-based techniques to utilize segmentation masks for
medical imaging with small dataset scenarios. This adaptation allows us to effectively train
models on small datasets and improve their performance when tested on i.i.d and OOD
data without having target datasets during training."
IMPLEMENTATION/METHODS,0.06976744186046512,2. Method
IMPLEMENTATION/METHODS,0.07751937984496124,"Let X be the input image space and Y the label space. A domain is defined as a joint
distribution D = (X, Y), which contains image-label pairs {(x(n), y(n))}N
n=1, where N is the
number of samples. Our goal is to learn a classification model Fθ : X →Y using the source
domain D for generalization across unseen target domains {D1
tg, D2
tg, . . . , DK
tg} set of K
target domains. In a source domain for input images X, we have corresponding segmentation
masks M that will be utilized for our method. Note that there are no requirements for
masks in target domains. The core idea of our method is to force the model to learn two"
IMPLEMENTATION/METHODS,0.08527131782945736,Rizhko et al.
IMPLEMENTATION/METHODS,0.09302325581395349,"things simultaneously: the attention mechanism learning task and the classification task
itself.
Attention Map Calculation. Given an input image, a classification model processes
it up to a target layer. Let Ak represent the activation of the k-th feature map at this
layer. The gradient of the score for class c, denoted yc, with respect to the activations Ak"
IMPLEMENTATION/METHODS,0.10077519379844961,of the feature map is computed. This gradient is represented as ∂yc
IMPLEMENTATION/METHODS,0.10852713178294573,"∂Ak . To obtain the neuron
importance weights αc
k we apply Global Average Pooling (GAP) to these gradients. This is
given by:"
IMPLEMENTATION/METHODS,0.11627906976744186,"αc
k = GAP
 ∂yc ∂Ak 
(1)"
IMPLEMENTATION/METHODS,0.12403100775193798,"The Class Activation Map (CAM) for class c, denoted as Lc
Grad−CAM, is a weighted sum
of the feature maps, weighted by αc
k, and passed through a ReLU function:"
IMPLEMENTATION/METHODS,0.13178294573643412,"Lc
Grad−CAM = ReLU X"
IMPLEMENTATION/METHODS,0.13953488372093023,"k
αc
kAk
! (2)"
IMPLEMENTATION/METHODS,0.14728682170542637,"The final Attention Map A is achieved by resizing Lc
Grad−CAM to the dimensions of the
input image.
Attention Loss. It is a custom loss function, denoted as LAttention, that incorporates
the difference between the Grad-CAM Attention Map A and a given ground truth attention
mask M by calculating the mean squared error (MSE) between A and M:"
IMPLEMENTATION/METHODS,0.15503875968992248,"LAttention = 1 N N
X"
IMPLEMENTATION/METHODS,0.16279069767441862,"i=1
(Ai −Mi)2
(3)"
IMPLEMENTATION/METHODS,0.17054263565891473,"where N is the total number of pixels in the image, and i indexes these pixels.
This loss function measures the alignment between the regions highlighted by the Grad-
CAM and those indicated by the attention mask. The objective of the training is to minimize
this loss, thereby encouraging the model to focus more on areas marked as important by
the mask.
Overall Loss. The Binary Cross-Entropy Loss LBCE, given the predicted outputs ypred"
IMPLEMENTATION/METHODS,0.17829457364341086,"and the true labels ytrue, is defined as:"
IMPLEMENTATION/METHODS,0.18604651162790697,"LBCE = −1 N N
X i=1"
IMPLEMENTATION/METHODS,0.1937984496124031,"h
ytrue
i
log(ypred
i
) + (1 −ytrue
i
) log(1 −ypred
i
)
i
(4)"
IMPLEMENTATION/METHODS,0.20155038759689922,"where N is the number of samples and i indexes these samples.
The overall loss is a weighted combination of the Binary Cross-Entropy Loss LBCE and
the Attention Loss LAttention."
IMPLEMENTATION/METHODS,0.20930232558139536,"L = αLBCE + βLAttention
(5)"
IMPLEMENTATION/METHODS,0.21705426356589147,"where α and β are weighting coefficients that balance the two components of the loss.
In all our experiments α = β = 1.
By combining these two losses, the model not only focuses on minimizing the prediction
error but also emphasizes the alignment of the attention maps with the important regions
as marked by the attention masks."
IMPLEMENTATION/METHODS,0.2248062015503876,Segmentation Masks for Image Classification
RESULTS/EXPERIMENTS,0.23255813953488372,3. Experiments
RESULTS/EXPERIMENTS,0.24031007751937986,"Hydronephrosis (HN) is a medical condition characterized by the swelling of one or both
kidneys due to a urine buildup. It can affect people of any age and is spotted in up to 5%
of babies during routine pregnancy ultrasound scans. However, surgical intervention is only
required in 20% of these cases (Dos Santos et al., 2015). To determine which cases need
intervention, patients receive repeated invasive scans and ultrasounds to monitor whether
the HN is causing functional damage or resolving without the need for surgery. Previously,
deep learning models have used postnatal ultrasound images to predict surgical intervention
in HN from the first ultrasound (Erdman et al., 2020), further investigated predicting HN
grades (Smail et al., 2020) and risk scores (Tabrizi et al., 2021). While prior models (Erdman
et al., 2020; Smail et al., 2020; Tabrizi et al., 2021) worked well for i.i.d data, they showed
lower performance on smaller and OOD datasets.
Datasets. We use five datasets from four pediatric hospitals in North America con-
taining ultrasounds of kidneys. The variation of the data comes not only from its collection
across various hospitals but also from differences in patient demographics and imaging
equipment, attributing to its OOD characteristics. For example, the average patient age
in the Hospital for Sick Children (SickKids) is 53 weeks, while in the Children’s Hospi-
tal of Philadelphia (CHOP), it is 313 weeks. These variances provide a robust setting for
evaluating models for DG.
The source domain dataset D from SickKids has 2542 ultrasounds. 20% of D is held
out to create i.i.d. test dataset Dtest. The rest of the 2048 images are used for training and
validation. Only 83 out of 2048 images have corresponding kidney segmentation masks. We
will utilize these 83 images for training baseline models. The same 83 images with their
segmentation masks will also be used to train our model. To assess the robustness of our
model, we will further use the complete set of 2048 images, which is bigger by 25 times,
to train additional baseline models. We split D into train Dtrain and validation Dval sets,
ensuring each patient’s images appear in only one set. Dtrain has only 66 images with kidney
segmentation masks, creating Dseg
train with 51 non-surgical and 15 surgical cases. Similarly,
the validation set Dseg
val is a subset of Dval and has 10 non-surgical and 7 surgical cases.
We evaluate models on four distinct target domain datasets. The first, TSickKids, in-
cludes data from 202 patients at SickKids, having 711 images, of which 75 are positive.
Despite being collected at the same hospital as the training dataset, patient demographics
and imaging equipment variations make this dataset OOD. The second dataset, TStanford,
is from the Stanford Children’s Hospital (Stanford) and includes data from 103 patients,
with 551 images (27 positive). The third, TUIowa, is from the University of Iowa Children’s
Hospital (UIowa) with 91 patients and 97 images (56 positives). Lastly, TCHOP comes from
CHOP with 89 patients and 89 images, 60 of which are positive. The datasets summary is
shown in Table 1.
Baselines. ResNet-18, ResNet-50 (He et al., 2016), ViT-Tiny, and ViT-Base (Doso-
vitskiy et al., 2020) were trained on Dseg
train and validated on Dseg
val, utilizing Binary Cross
Entropy Loss LBCE only. We tested two weights initialization methods: Kaiming uniform
initialization (random) (He et al., 2015) and using weights pre-trained on ImageNet (Deng
et al., 2009). Hyperparameters were tuned via Bayesian optimization (Snoek et al., 2012)
to minimize the loss on Dseg
val, more details in Appendix A.Consistent image transformations"
RESULTS/EXPERIMENTS,0.24806201550387597,Rizhko et al.
RESULTS/EXPERIMENTS,0.2558139534883721,Table 1: Datasets Summary
RESULTS/EXPERIMENTS,0.26356589147286824,"Name
Hospital
Domain Used for
Masks
Patients Images
Pos
Neg
Dtrain
SickKids
source
training
×
266
1549
185
1364
Dseg
train
SickKids
source
training
✓
35
66
15
51
Dval
SickKids
source
validation
×
89
499
67
432
Dseg
val
SickKids
source
validation
✓
7
17
7
10
Dtest
SickKids
source
i.i.d. test
×
89
494
71
423
TSickKids SickKids
target
OOD test
×
202
711
75
636
TStanford Stanford
target
OOD test
×
103
551
27
524
TUIowa
UIowa
target
OOD test
×
91
97
56
41
TCHOP
CHOP
target
OOD test
×
89
89
60
29"
RESULTS/EXPERIMENTS,0.2713178294573643,"are applied across all experiments. Rotation, cropping, horizontal flipping, and normaliza-
tion are used for training, while resizing and normalization are used for validation. We train
all models for 30 epochs with early stopping based on validation AUROC. One NVIDIA
RTX 2080 Ti was used for all experiments. For further analysis, we also trained additional
baseline models on the larger datasets Dtrain and Dval; all experimental setups were the
same."
RESULTS/EXPERIMENTS,0.27906976744186046,"Our model.
We trained the ResNet-18 model on Dseg
train and validated it on Dseg
val,
utilizing our proposed loss function as described in Equation 5. The model’s weights were
initialized using pre-trained ImageNet weights. All other experimental setups, including
hyperparameters search, image transformations, and training duration, were consistent with
those used in the baseline models."
RESULTS/EXPERIMENTS,0.2868217054263566,3.1. Baselines vs. Our Model trained on the Small Datasets.
RESULTS/EXPERIMENTS,0.29457364341085274,"The results in Table 2 and Table 3 show Area Under the Receiver Operating Characteristic
(AUROC) and Area Under the Precision-Recall Curve (AUPRC) of the baseline models
and our model, all trained on Dseg
train and validated on Dseg
val."
RESULTS/EXPERIMENTS,0.3023255813953488,I.i.d. Comparison.
RESULTS/EXPERIMENTS,0.31007751937984496,"Table 2 presents a comparative analysis of the models performance on i.i.d. test dataset
Dtest. Note, Dtest is a held-out test dataset from the whole dataset D and has 494 images,
while the models are trained on the small subsets Dseg
train and Dseg
val with 66 and 17 images
respectively. This comparison reflects each model’s ability to generalize to new data with a
similar distribution to the training set. Interestingly, only three models, including our own,
were able to effectively generalize to Dtest, showing 0.81-0.83 AUROC and 0.48 AUPRC."
RESULTS/EXPERIMENTS,0.3178294573643411,OOD Comparison.
RESULTS/EXPERIMENTS,0.32558139534883723,"Table 3 presents the performance of the models across four different OOD datasets
TSickKids, TStanford, TUIowa, and TCHOP . Notably, our model consistently outperformed all
baselines across all OOD datasets."
RESULTS/EXPERIMENTS,0.3333333333333333,Segmentation Masks for Image Classification
RESULTS/EXPERIMENTS,0.34108527131782945,"Table 2: Comparison of models trained on the small dataset Dseg
train for performance on
held-out i.i.d. test dataset Dtest"
RESULTS/EXPERIMENTS,0.3488372093023256,"Model Name
Backbone
Weights Init.
AUROC
AUPRC
R18-random-small
ResNet-18
Random
0.79
0.43
R18-imagenet-small
ResNet-18
ImageNet
0.70
0.27
R50-random-small
ResNet-50
Random
0.68
0.30
R50-imagenet-small
ResNet-50
ImageNet
0.71
0.28
ViT-T-random-small
ViT-Tiny
Random
0.55
0.20
ViT-T-imagenet-small
ViT-Tiny
ImageNet
0.81
0.48
ViT-B-random-small
ViT-Base
Random
0.54
0.15
ViT-B-imagenet-small
ViT-Base
ImageNet
0.83
0.48
R18-attention (Ours)
ResNet-18
ImageNet
0.82
0.48"
RESULTS/EXPERIMENTS,0.35658914728682173,"Table 3: Comparison of models trained on the small dataset Dseg
train for performance on four
different OOD datasets TSickKids, TStanford, TUIowa, and TCHOP"
RESULTS/EXPERIMENTS,0.3643410852713178,"AUROC
AUPRC
Model Name
TSickKids TStanford TUIowa TCHOP TSickKids TStanford TUIowa TCHOP
R18-random-small
0.47
0.19
0.33
0.34
0.09
0.04
0.48
0.59
R18-imagenet-small
0.52
0.35
0.72
0.54
0.10
0.04
0.77
0.73
R50-random-small
0.52
0.23
0.39
0.27
0.20
0.04
0.51
0.58
R50-imagenet-small
0.37
0.13
0.18
0.21
0.08
0.03
0.43
0.52
ViT-T-random-small
0.52
0.29
0.76
0.56
0.10
0.03
0.73
0.72
ViT-T-imagenet-small
0.80
0.72
0.80
0.69
0.35
0.22
0.85
0.82
ViT-B-random-small
0.46
0.31
0.71
0.50
0.10
0.03
0.74
0.68
ViT-B-imagenet-small
0.84
0.84
0.72
0.68
0.46
0.33
0.79
0.81
R18-attention (Ours)
0.86
0.88
0.90
0.81
0.53
0.42
0.90
0.92"
RESULTS/EXPERIMENTS,0.37209302325581395,"3.2. Baselines trained on the Big Datasets vs. Our Model trained on the
Small Datasets."
RESULTS/EXPERIMENTS,0.3798449612403101,"To further analyze our model, we trained additional baselines on the whole train dataset
Dtrain and validation dataset Dval with a total of 2078 images. We compared the baselines
to our model trained on Dseg
train and validated on Dseg
val with a total of 83 images.
I.i.d. Comparison.
Table 4 shows the overall performance of baselines and our model on the held-out i.i.d.
test dataset Dtest. All models, including our trained only on 4% of the data, have com-
parable AUROC (0.82 - 0.87) and AUPRC (0.47 - 0.54), which means all models generalize
well to unseen images from the same i.i.d. distribution.
OOD Comparison.
Table 5 shows models’ performance on OOD datasets TSickKids, TStanford, TCHOP , and
TUIowa. Out of 9 models that perform well on i.i.d. data, only three models, including ours,
transfer well to all OOD datasets. It demonstrates the effectiveness of using our approach,
considering that our model trained on 25 times less data could generalize well to OOD data."
RESULTS/EXPERIMENTS,0.3875968992248062,Rizhko et al.
RESULTS/EXPERIMENTS,0.3953488372093023,"Table 4: Comparison of baselines trained on the big dataset Dtrain and our model trained
on the small dataset Dseg
train for performance on held-out i.i.d. test dataset Dtest"
RESULTS/EXPERIMENTS,0.40310077519379844,"Model Name
Backbone
Weights Init.
Images
AUROC AUPRC
R18-random
ResNet-18
Random
2078
0.85
0.50
R18-imagenet
ResNet-18
ImageNet
2078
0.87
0.52
R50-random
ResNet-50
Random
2078
0.82
0.47
R50-imagenet
ResNet-50
ImageNet
2078
0.83
0.52
ViT-T-random
ViT-Tiny
Random
2078
0.84
0.50
ViT-T-imagenet
ViT-Tiny
ImageNet
2078
0.86
0.54
ViT-B-random
ViT-Base
Random
2078
0.84
0.49
ViT-B-imagenet
ViT-Base
ImageNet
2078
0.85
0.52
R18-attention (Ours)
ResNet-18
ImageNet
83
0.82
0.48"
RESULTS/EXPERIMENTS,0.4108527131782946,"Table 5: Comparison of baselines trained on the big dataset Dtrain and our model trained
on the small dataset Dseg
train for performance on four OOD datasets TSickKids, TStanford,
TUIowa, and TCHOP"
RESULTS/EXPERIMENTS,0.4186046511627907,"AUROC
AUPRC
Model Name
TSickKids TStanford TUIowa TCHOP TSickKids TStanford TUIowa TCHOP
R18-random
0.59
0.35
0.43
0.36
0.19
0.04
0.52
0.59
R18-imagenet
0.88
0.88
0.82
0.85
0.55
0.40
0.88
0.94
R50-random
0.61
0.49
0.66
0.55
0.21
0.06
0.67
0.71
R50-imagenet
0.74
0.66
0.80
0.78
0.23
0.07
0.84
0.84
ViT-T-random
0.53
0.17
0.23
0.27
0.17
0.03
0.43
0.55
ViT-T-imagenet
0.77
0.62
0.66
0.62
0.35
0.12
0.72
0.72
ViT-B-random
0.57
0.22
0.23
0.24
0.24
0.04
0.44
0.54
ViT-B-imagenet
0.89
0.91
0.88
0.85
0.55
0.48
0.88
0.93
R18-attention (Ours)
0.86
0.88
0.90
0.81
0.53
0.42
0.90
0.92"
CONCLUSION/DISCUSSION,0.4263565891472868,4. Conclusion
CONCLUSION/DISCUSSION,0.43410852713178294,"This paper presented a new method for improving medical image classification models using
segmentation masks, especially effective in small dataset scenarios (less than 100 images).
By utilizing a specialized loss function, our model demonstrated remarkable performance
on both i.i.d. and OOD datasets despite limited training data. It matched or exceeded
the performance of other models trained on similar-sized datasets in i.i.d. scenarios and
consistently outperformed all baselines in OOD settings. Notably, our model, trained on
just 4% of the data, showed the same or even better performance as baselines trained on
significantly larger datasets in i.i.d. and OOD settings. The implications of these results
are promising. Creating segmentation masks, which our method relies on, could be more
feasible than gathering extensive data on rare diseases. Additionally, our model’s ability to
transfer across different hospitals could reduce the need for unique models for each medical
setting."
CONCLUSION/DISCUSSION,0.4418604651162791,Segmentation Masks for Image Classification
REFERENCES,0.4496124031007752,References
REFERENCES,0.4573643410852713,"Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain
generalization using meta-regularization. Advances in neural information processing sys-
tems, 31, 2018."
REFERENCES,0.46511627906976744,"Silvia Bucci, Antonio D’Innocente, Yujun Liao, Fabio M Carlucci, Barbara Caputo, and Ta-
tiana Tommasi. Self-supervised learning across domains. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 44(9):5516–5528, 2021."
REFERENCES,0.4728682170542636,"Fabio M Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tom-
masi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 2229–2238, 2019."
REFERENCES,0.4806201550387597,"Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung
Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances
in Neural Information Processing Systems, 34:22405–22418, 2021."
REFERENCES,0.4883720930232558,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database.
In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009."
REFERENCES,0.49612403100775193,"Joana Dos Santos, Rulan S Parekh, Tino D Piscione, Tarek Hassouna, Victor Figueroa,
Paula Gonima, Isis Vargas, Walid Farhat, and Norman D Rosenblum. A new grading
system for the management of antenatal hydronephrosis. Clinical Journal of the American
Society of Nephrology, 10(10):1783–1790, 2015."
REFERENCES,0.5038759689922481,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain
Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929, 2020."
REFERENCES,0.5116279069767442,"Lauren Erdman, Marta Skreta, Mandy Rickard, Carson McLean, Aziz Mezlini, Daniel T
Keefe, Anne-Sophie Blais, Michael Brudno, Armando Lorenzo, and Anna Goldenberg.
Predicting obstructive hydronephrosis based on ultrasound alone. In International Con-
ference on Medical Image Computing and Computer-Assisted Intervention, pages 493–
503. Springer, 2020."
REFERENCES,0.5193798449612403,"Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi. Attention
branch network: Learning of attention mechanism for visual explanation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition, pages 10705–
10714, 2019."
REFERENCES,0.5271317829457365,"Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropaga-
tion. In International conference on machine learning, pages 1180–1189. PMLR, 2015."
REFERENCES,0.5348837209302325,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification.
In Proceedings of the
IEEE international conference on computer vision, pages 1026–1034, 2015."
REFERENCES,0.5426356589147286,Rizhko et al.
REFERENCES,0.5503875968992248,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for
image recognition. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016."
REFERENCES,0.5581395348837209,"Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei
Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In
International conference on machine learning, pages 1989–1998. Pmlr, 2018."
REFERENCES,0.5658914728682171,"Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-
domain generalization. In Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 124–140. Springer,
2020."
REFERENCES,0.5736434108527132,"Maximilian Ilse, Jakub M Tomczak, Christos Louizos, and Max Welling. Diva: Domain
invariant variational autoencoders. In Medical Imaging with Deep Learning, pages 322–
348. PMLR, 2020."
REFERENCES,0.5813953488372093,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier
domain generalization. In Proceedings of the IEEE international conference on computer
vision, pages 5542–5550, 2017."
REFERENCES,0.5891472868217055,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-
learning for domain generalization. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018a."
REFERENCES,0.5968992248062015,"Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with
adversarial feature learning. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 5400–5409, 2018b."
REFERENCES,0.6046511627906976,"Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me where to look:
Guided attention inference network. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 9215–9223, 2018c."
REFERENCES,0.6124031007751938,"Wei Li, Xiatian Zhu, and Shaogang Gong. Scalable person re-identification by harmonious
attention. International Journal of Computer Vision, 128(6):1635–1653, 2020."
REFERENCES,0.6201550387596899,"Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng
Tao. Deep domain generalization via conditional invariant adversarial networks. In Pro-
ceedings of the European conference on computer vision (ECCV), pages 624–639, 2018d."
REFERENCES,0.627906976744186,"Chunfeng Lian, Mingxia Liu, Li Wang, and Dinggang Shen.
End-to-end dementia sta-
tus prediction from brain mri using multi-task weakly-supervised attention network. In
Medical Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd In-
ternational Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part IV 22,
pages 158–167. Springer, 2019."
REFERENCES,0.6356589147286822,"Chunfeng Lian, Mingxia Liu, Yongsheng Pan, and Dinggang Shen. Attention-guided hy-
brid network for dementia diagnosis with structural mr images. IEEE transactions on
cybernetics, 52(4):1992–2003, 2020."
REFERENCES,0.6434108527131783,Segmentation Masks for Image Classification
REFERENCES,0.6511627906976745,"Quande Liu, Qi Dou, and Pheng-Ann Heng. Shape-aware meta-learning for generalizing
prostate mri segmentation to unseen domains. In Medical Image Computing and Com-
puter Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru,
October 4–8, 2020, Proceedings, Part II 23, pages 475–485. Springer, 2020."
REFERENCES,0.6589147286821705,"Jintao Lu, Xi Ouyang, Xueda Shen, Tianjiao Liu, Zhiming Cui, Qian Wang, and Dinggang
Shen. Gan-guided deformable attention network for identifying thyroid nodules in ul-
trasound images. IEEE Journal of Biomedical and Health Informatics, 26(4):1582–1590,
2022."
REFERENCES,0.6666666666666666,"Xi Ouyang, Jiayu Huo, Liming Xia, Fei Shan, Jun Liu, Zhanhao Mo, Fuhua Yan, Zhongx-
iang Ding, Qi Yang, Bin Song, et al. Dual-sampling attention network for diagnosis of
covid-19 from community acquired pneumonia. IEEE Transactions on Medical Imaging,
39(8):2595–2605, 2020a."
REFERENCES,0.6744186046511628,"Xi Ouyang, Srikrishna Karanam, Ziyan Wu, Terrence Chen, Jiayu Huo, Xiang Sean Zhou,
Qian Wang, and Jie-Zhi Cheng. Learning hierarchical attention for weakly-supervised
chest x-ray abnormality localization and diagnosis. IEEE transactions on medical imag-
ing, 40(10):2698–2710, 2020b."
REFERENCES,0.6821705426356589,"German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez.
The synthia dataset: A large collection of synthetic images for semantic segmentation
of urban scenes. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 3234–3243, 2016."
REFERENCES,0.689922480620155,"Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi
Parikh, and Dhruv Batra.
Grad-cam:
Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE international conference on com-
puter vision, pages 618–626, 2017."
REFERENCES,0.6976744186046512,"Lauren C Smail, Kiret Dhindsa, Luis H Braga, Suzanna Becker, and Ranil R Sonnadara. Us-
ing deep learning algorithms to grade hydronephrosis severity: toward a clinical adjunct.
Frontiers in pediatrics, 8:1, 2020."
REFERENCES,0.7054263565891473,"Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of
machine learning algorithms.
Advances in neural information processing systems, 25,
2012."
REFERENCES,0.7131782945736435,"Yifan Sun, Liang Zheng, Yali Li, Yi Yang, Qi Tian, and Shengjin Wang. Learning part-
based convolutional features for person re-identification. IEEE transactions on pattern
analysis and machine intelligence, 43(3):902–917, 2019."
REFERENCES,0.7209302325581395,"Pooneh R Tabrizi, Jonathan Zember, Bruce Michael Sprague, Steven Hoefer, Ramon
Sanchez-Jacob, James Jago, Dorothy Bulas, Hans G Pohl, and Marius George Lingu-
raru. Pediatric hydronephrosis severity assessment using convolutional neural networks
with standardized ultrasound images. In 2021 IEEE 18th International Symposium on
Biomedical Imaging (ISBI), pages 1803–1806. IEEE, 2021."
REFERENCES,0.7286821705426356,Rizhko et al.
REFERENCES,0.7364341085271318,"Riccardo Volpi and Vittorio Murino. Addressing model vulnerability to distributional shifts
over image transformation sets. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pages 7980–7989, 2019."
REFERENCES,0.7441860465116279,"Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and
Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. Ad-
vances in neural information processing systems, 31, 2018."
REFERENCES,0.751937984496124,"Haohan Wang, Zexue He, Zachary C Lipton, and Eric P Xing. Learning robust representa-
tions by projecting superficial statistics out. arXiv preprint arXiv:1903.06256, 2019."
REFERENCES,0.7596899224806202,"Zheng Xu, Wen Li, Li Niu, and Dong Xu. Exploiting low-rank structure from latent domains
for domain generalization. In Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part III 13, pages 628–643.
Springer, 2014."
REFERENCES,0.7674418604651163,"Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer.
Robust and
generalizable visual representation learning via random convolutions.
arXiv preprint
arXiv:2007.13003, 2020."
REFERENCES,0.7751937984496124,"Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain general-
ization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence,
2022."
OTHER,0.7829457364341085,Segmentation Masks for Image Classification
OTHER,0.7906976744186046,Appendix A. Hyperparameterms Search
OTHER,0.7984496124031008,"In our study, we used Bayesian optimization to systematically explore and identify optimal
hyperparameters for training all models. We focused on tuning batch size, gamma, learning
rate, and weight decay, aiming to minimize the validation loss. We tested batch sizes of 16,
32, 64, and 128; gamma values ranging from 0.99 to 0.85 in decrements of 0.02; learning
rates of 0.1, 0.01, 0.001, 0.0001, 1e-05, and 1e-06; and weight decay parameters of 0.3, 0.1,
0.03, 0.01, 0.003, and 0.001. The best set of hyperparameters for each model is reported in
the Table 6."
OTHER,0.8062015503875969,Table 6: Hyperparameter Selection for Models
OTHER,0.813953488372093,"Model Name
Batch Size
Gamma Learning Rate
Weight Decay
R18-random-small
32
0.91
0.001
0.1
R18-imagenet-small
16
0.91
0.001
0.1
R50-random-small
32
0.85
0.01
0.01
R50-imagenet-small
64
0.85
0.01
0.01
ViT-T-random-small
64
0.85
0.01
0.003
ViT-T-imagenet-small
128
0.95
0.0001
0.01
ViT-B-random-small
16
0.85
0.000001
0.3
ViT-B-imagenet-small
128
0.91
0.00001
0.001
R18-random
64
0.89
0.001
0.001
R18-imagenet
64
0.93
0.00001
0.01
R50-random
64
0.87
0.001
0.001
R50-imagenet
16
0.93
0.0001
0.001
ViT-T-random
32
0.87
0.0001
0.001
ViT-T-imagenet
16
0.99
0.00001
0.01
ViT-B-random
16
0.91
0.00001
0.001
ViT-B-imagenet
16
0.87
0.000001
0.03
R18-attention (Ours)
128
0.85
0.001
0.1"
OTHER,0.8217054263565892,Appendix B. Attention Score
OTHER,0.8294573643410853,"To quantify how much different models actually pay attention to the region of interest, we
create a new metric Attention Score, which has two components Overlap Score and Coverage
Score. Overlap Score (OS) measures the proportion of the important areas, as defined by
the ground truth mask M, that is successfully captured by the attention map A:"
OTHER,0.8372093023255814,"OS(A, M) =
PN
i=1 min(Ai, Mi)
PN
i=1 Mi
(6)"
OTHER,0.8449612403100775,"where N is the total number of pixels, and i indexes these pixels. Coverage Score (CS)
assesses the concentration and specificity of the model’s attention, evaluating how much of
the attention map’s activation A is meaningfully focused on the target areas M:"
OTHER,0.8527131782945736,"CS(A, M) =
PN
i=1 min(Ai, Mi)
PN
i=1 Ai
(7)"
OTHER,0.8604651162790697,Rizhko et al.
OTHER,0.8682170542635659,"where N is the total number of pixels, and i indexes these pixels.
You can think
about the Overlap Score as a Recall metric and the Coverage Score as Precision but for
Attention maps instead of classification labels. The final Attention Score is computed as
the harmonic mean of the Overlap Score and Coverage Score, providing a balanced measure
of both overlap and coverage:"
OTHER,0.875968992248062,"AttentionScore(A, M) = 2 × OS(A, M) × CS(A, M)"
OTHER,0.8837209302325582,"OS(A, M) + CS(A, M)
(8)"
OTHER,0.8914728682170543,"Table 7: Attention Scores on i.i.d. Dtest and OOD datasets TSickKids, TStanford, TUIowa,
and TCHOP"
OTHER,0.8992248062015504,"Model Name
Dtest
TSickKids TStanford TUIowa
TCHOP
R18-random
0.38
0.51
0.34
0.46
0.44
R18-imagenet
0.43
0.56
0.52
0.57
0.52
R50-random
0.40
0.49
0.40
0.40
0.51
R50-imagenet
0.41
0.48
0.43
0.45
0.45
ViT-T-random
0.44
0.57
0.10
0.23
0.09
ViT-T-imagenet
0.26
0.41
0.32
0.43
0.42
ViT-B-random
0.45
0.59
0.50
0.67
0.58
ViT-B-imagenet
0.27
0.36
0.38
0.39
0.39
R18-attention (Ours)
0.57
0.60
0.61
0.59
0.62"
OTHER,0.9069767441860465,"In Table 7, we show that our model generally outperforms other models in terms of
Attention Score. The interesting exception is the Attention Score for the ViT-B-random
model on TUIowa dataset, where it shows a higher score than our model. Considering the low
performance of the ViT-B-random model in terms of AUROC and AUPRC on that dataset,
we conclude that the attention score, even though it is a useful indicator of the model’s
performance, is only a part of the evaluation and should be considered in combination with
other metrics."
OTHER,0.9147286821705426,Appendix C. Datasets Information
OTHER,0.9224806201550387,"Training Dataset
Sex distribution: 2027 M, 515 F.
Kidney side distribution: 1289 Left, 1253 Right.
Ultrasound machine distribution:
philips-medical-systems: 992, toshiba-mec: 497, NA:
376, ToshibaST: 258, PhilipsST: 112, SamsungST: 97, ge-medical-systems: 45, samsung-
medison-co-ltd: 36, OutsideST: 26, acuson: 25, atl: 22, toshiba-mec-us: 20, TreeST: 17,
GEST: 13, siemens: 4, ge-healthcare: 2.
The age varies from 0.14 weeks to 720 weeks, with an average of 53 weeks."
OTHER,0.9302325581395349,"OOD dataset TSickKids
Sex distribution: 599 M, 112 F.
Kidney side distribution: 475 Left, 236 Right."
OTHER,0.937984496124031,Segmentation Masks for Image Classification
OTHER,0.9457364341085271,"Ultrasound machine distribution: ToshibaST: 294, PhilipsST: 247, SamsungST: 158, Out-
sideST: 12.
The age varies from 0.29 weeks to 92 weeks, with an average of 17 weeks."
OTHER,0.9534883720930233,"OOD dataset TStanford
Sex distribution: 413 M, 138 F.
Kidney side distribution: 275 Left, 276 Right.
Ultrasound machine distribution: Stanford: 551.
The age varies from 104.0 weeks to 988 weeks, with an average of 190 weeks."
OTHER,0.9612403100775194,"OOD dataset TUIowa
Sex distribution: 80 M, 17 F.
Kidney side distribution: 59 Left, 38 Right.
Ultrasound machine distribution: UIowa: 97.
The age varies from 0.14 weeks to 266 weeks, with an average of 28 weeks."
OTHER,0.9689922480620154,"OOD dataset TCHOP
Sex distribution: 55 M, 34 F.
Kidney side distribution: 56 Left, 33 Right.
Ultrasound machine distribution: Philips: 51, GE: 16, Phillips: 7, HDI 5000: 3, Siemens: 2,
Acuson: 2, General electric: 1, MRI abd w/wo, RBUS 7/19/2010: 1, Cineloop: 1, Mindray:
1, Toshiba: 1.
The age varies from 1.43 weeks to 1001 weeks, with an average of 313 weeks."
OTHER,0.9767441860465116,Appendix D. Limitations and Future Work
OTHER,0.9844961240310077,"Limitations.
Even though our model performed well on multiple out-of-distribution
datasets, it’s worth noting that all the data came from hospitals in the USA and Canada.
In real-world scenarios, particularly in areas with substantially different demographics or
medical equipment, our model might show diminished performance."
OTHER,0.9922480620155039,"Future Work. We plan to conduct a series of comprehensive ablation studies to pre-
cisely quantify the impact of attention loss on the performance of each model separately.
Additionally, we aim to broaden the applicability and robustness of our model by collect-
ing and incorporating data from hospitals outside North America. This effort will test the
model’s ability to generalize across diverse demographics, addressing potential biases and
enhancing its global applicability. Furthermore, we intend to explore the potential of our
approach in other clinical settings, such as the diagnosis of pneumonia in lung ultrasound
images. By extending our domain generalization efforts to various medical imaging tasks,
we hope to contribute further to the advancement of AI in healthcare, ensuring models are
both effective and equitable across different populations and conditions."
