Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013908205841446453,"This work studies the question of Representation Learning in RL: how can we
learn a compact low-dimensional representation such that on top of the represen-
tation we can perform RL procedures such as exploration and exploitation, in a
sample efﬁcient manner. We focus on the low-rank Markov Decision Processes
(MDPs) where the transition dynamics correspond to a low-rank transition matrix.
Unlike prior works that assume the representation is known (e.g., linear MDPs),
here we need to learn the representation for the low-rank MDP. We study both the
online RL and ofﬂine RL settings. For the online setting, operating with the same
computational oracles used in FLAMBE(Agarwal et al., 2020b)—-the state-of-art
algorithm for learning representations in low-rank MDPs, we propose an algo-
rithm REP-UCB—Upper Conﬁdence Bound driven REPresentation learning for
RL, which signiﬁcantly improves the sample complexity from eO(A9d7/(ϵ10(1 −
γ)22)) for FLAMBE to eO(d4A2/(ϵ2(1 −γ)5)) with d being the rank of the transi-
tion matrix (or dimension of the ground truth representation), A being the number
of actions, and γ being the discount factor. Notably, REP-UCB is simpler than
FLAMBE, as it directly balances the interplay between representation learning,
exploration, and exploitation, while FLAMBE is an explore-then-commit style ap-
proach and has to perform reward-free exploration step-by-step forward in time.
For the ofﬂine RL setting, we develop an algorithm that leverages pessimism to
learn under a partial coverage condition: our algorithm is able to compete against
any policy as long as it is covered by the ofﬂine data distribution."
INTRODUCTION,0.0027816411682892906,"1
INTRODUCTION"
INTRODUCTION,0.004172461752433936,"When applying Reinforcement Learning (RL) to large-scale problems where data is complex and
high-dimensional, learning effective transformations of the data, i.e., representation learning, can
often signiﬁcantly improve the sample and computation efﬁciency of the RL procedure. Indeed,
several empirical works have shown that leveraging representation learning techniques developed
in supervised or unsupervised learning settings can accelerate the search for good decision-making
strategies (Silver et al., 2018; Stooke et al., 2021; Srinivas et al., 2020; Yang & Nachum, 2021).
However, representation learning in RL is far more subtle than it is for non-sequential and non-
interactive learning tasks (e.g., supervised learning). Prior works have shown that even if one is
given the magic representation that exactly linearizes the optimal policy (Du et al., 2019b) or the
optimal value functions (Wang et al., 2020; Weisz et al., 2021), RL is still challenging (i.e., one may
still need exponentially many samples to learn). This indicates that an effective representation that
permits efﬁcient RL needs to encode more information about the underlying Markov Decision Pro-
cesses (MDPs). Despite the recent empirical success of representation learning in RL , its statistical
guarantee and theoretical properties remain under-investigated."
INTRODUCTION,0.005563282336578581,"In this work, we study the representation learning question under the low-rank MDP assumption.
Concretely, a low-rank MDP assumes that the MDP transition matrix admits a low-rank factoriza-
tion, i.e., there exists two unknown mappings µ(s′), φ(s, a), such that P(s′|s, a) = µ(s′)⊤φ(s, a)
for all s, a, s′, where P(s′|s, a) is the probability of transiting to the next state s′ under the current"
INTRODUCTION,0.006954102920723227,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008344923504867872,"state and action (s, a). The representation φ in a low-rank MDP not only linearizes the optimal
state-action value function of the MDP (Jin et al., 2020a), but also linearizes the transition operator.
A low-rankness assumption on large stochastic matrices is a common and natural assumption
and has enabled successful development of algorithms for real world applications such as movie
recommendation systems (Koren et al., 2009). We note that a low-rank MDP strictly generalizes
the linear MDP model (Yang & Wang, 2020; Jin et al., 2020a) which assumes φ is known a priori.
The unknown representation φ makes learning in low-rank MDPs much more challenging than that
in linear MDPs since one can no longer directly use linear function approximations. On the other
hand, the fact that linear MDPs can be solved statistical and computational efﬁciently if φ is known
a priori implies that if one could learn the representation of the low-rank MDP, one could then
efﬁciently learn the optimal policy."
INTRODUCTION,0.009735744089012517,"Indeed, prior works have shown that learning in low-rank MDPs is statistically feasible (Jiang et al.,
2017; Sun et al., 2019; Du et al., 2021) via leveraging rich function approximators. However, these
algorithms are version space algorithms and are not computationally efﬁcient. Recent work FLAMBE
proposes an oracle-efﬁcient algorithm1 that learns in low-rank MDPs with a polynomial sample
complexity, where the computation oracle is Maximum Likelihood Estimation (MLE) operating
under the standard supervised learning style Empirical Risk Minimization (ERM) setting. In this
work, we follow the same setup from FLAMBE (Agarwal et al., 2020b), and propose a new algo-
rithm — Upper Conﬁdence Bound driven Representation Learning, Exploration and Exploitation
(REP-UCB), which can learn a near optimal policy for a low-rank MDP with a polynomial sample
complexity and is oracle-efﬁcient. Comparing to FLAMBE, our algorithm signiﬁcantly improves the
sample complexity from O(d7A9/(ϵ10(1 −γ)22) for FLAMBE to O(d4A2/(ϵ2(1 −γ)5), where d
is the rank of the transition matrix (or dimension of the true representation), A is the number of
actions, ϵ is the suboptimality gap and γ ∈[0, 1) is the discount factor in the MDP. Our algorithm
is also arguably much simpler than FLAMBE: FLAMBE is an explore-then-commit algorithm, has
to explore in a layer-by-layer forward way, and does not permit data sharing across different time
steps. In contrast, REP-UCB carefully trades exploration versus exploitation by combining the re-
ward signal and exploration bonus (constructed using the latest learned representation), and enables
data sharing across all time steps.2 Our sample complexity nearly matches the ones from those
computationally inefﬁcient algorithms (Jiang et al., 2017; Sun et al., 2019; Du et al., 2021). We
summarize the comparison with the prior works that study representation learning in Table 1."
INTRODUCTION,0.011126564673157162,"In addition to the online exploration setting, we also show that our new techniques can be directly
used for designing ofﬂine RL algorithms for low-rank MDPs under partial coverage. More specif-
ically, we propose an algorithm REP-LCB—Lower Conﬁdence Bound driven Reprepresentation
Learning for ofﬂine RL, that given an ofﬂine dataset, can learn to compete against any policy
(including history-dependent policies) as long as it is covered by the ofﬂine data where the coverage
is measured using the relative condition number (Agarwal et al., 2021) associated with the ground
truth representation. Thus, our ofﬂine RL result generalizes prior ofﬂine RL works on linear MDPs
(Jin et al., 2020b; Zhang et al., 2021b) which assume representation is known a priori and use linear
function approximation. Computation-wise, our approach uses one call to the MLE computation
oracle, and hence is oracle-efﬁcient. REP-LCB is the ﬁrst oracle efﬁcient ofﬂine algorithm for
low-rank MDP enjoying the aforementioned statistical guarantee. See Section 2 for a more detailed
comparison with the existing literature on representation learning in ofﬂine RL."
INTRODUCTION,0.012517385257301807,"Our contributions.
We develop new representation learning RL algorithms that enable sample
efﬁcient learning in low-rank MDPs under both online and ofﬂine settings:"
INTRODUCTION,0.013908205841446454,"1. In the online episodic learning setting, our new algorithm REP-UCB integrates representa-
tion learning, exploration, and exploitation together, and signiﬁcantly improves the sample
complexity of the prior state-of-art algorithm FLAMBE;"
INTRODUCTION,0.015299026425591099,"1The oracle generally refers to supervised learning style empirical risk minimization oracle. We seek to
design an algorithm that runs in polynomial time with each oracle call counting as O(1). The reduction to
supervised learning has lead to many successful provable and practical algorithms in contextual bandit (Agarwal
et al., 2014; Dud´ık et al., 2017; Foster & Rakhlin, 2020) and RL (Du et al., 2019a; Misra et al., 2020).
2Our algorithm and analysis can be easily extended to ﬁnite horizon non-stationary setting. We choose
the discounted inﬁnite horizon setting to contrast our results to FLAMBE: FLAMBE is not capable of learning
stationary policies under the discounted inﬁnite horizon setting."
INTRODUCTION,0.016689847009735744,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01808066759388039,"Methods
Setting
Sample Complexity
Computation"
INTRODUCTION,0.019471488178025034,"OLIVE (Jiang et al., 2017)
Low Bellman rank
d2A
ϵ2(1−γ)4
Inefﬁcient"
INTRODUCTION,0.02086230876216968,"Witness rank (Sun et al., 2019)
Low Witness rank
d2A
ϵ2(1−γ)4
Inefﬁcient"
INTRODUCTION,0.022253129346314324,"BLin-UCB (Du et al., 2021)
Bilinear Class
d2A
ϵ2(1−γ)7
Inefﬁcient"
INTRODUCTION,0.02364394993045897,"Mofﬂe (Modi et al., 2021)
Low-nonnegative-rank MDP
d6A13
ϵ2η5(1−γ)5
Oracle efﬁcient"
INTRODUCTION,0.025034770514603615,"FLAMBE Agarwal et al. (2020b)
Low-rank MDP
d7A9
ϵ10(1−γ)22
Oracle efﬁcient"
INTRODUCTION,0.02642559109874826,"REP-UCB (Ours)
Low-rank MDP
d4A2
ϵ2(1−γ)5
Oracle efﬁcient"
INTRODUCTION,0.027816411682892908,"Table 1: Comparison among different provable representation learning algorithms in online RL.
Algorithms such as OLIVE, Witness rank, and BLin-UCB work for settings which are more gen-
eral than low-rank MDPs and have tight sample complexity. However, these algorithms are version
space algorithms and thus are not computationally efﬁcient. Mofﬂe is an oracle-efﬁcient algorithm
(with a much stronger oracle than the one in FLAMBE and ours), but the assumptions under which
Mofﬂe operates essentially imply that the MDP’s transition has low non-negative matrix rank (nnr).
Note that a nnr is at least as large as and could be exponentially larger than the rank (Agarwal et al.,
2020b). Finally, FLAMBE operates under the same function approximation setting and the computa-
tion oracle as ours. Our algorithm signiﬁcantly improves the sample complexity from FLAMBE in all
parameters. Note the horizon dependence is not exactly comparable as these prior works originally
considered the ﬁnite horizon setting with nonstationary transition, and we convert their results to the
discounted setting by simply replacing the ﬁnite horizon H by Θ(1/(1 −γ))."
INTRODUCTION,0.02920723226703755,"2. In the ofﬂine learning setting, we propose a natural concentrability coefﬁcient (i.e., relative
condition number under the true representation) that captures the partial coverage condition in
low-rank MDP, and our algorithm REP-LCB learns to compete against any policy (including
history-dependent ones) under such a partial coverage condition."
RELATED WORK,0.030598052851182198,"2
RELATED WORK"
RELATED WORK,0.031988873435326845,"Online Setting
We list the comparison as follows, which is summarized in Table 1. Additional
related works are discussed in Section A."
RELATED WORK,0.03337969401947149,"FLAMBE (Agarwal et al., 2020b) was a state-of-the-art oracle-efﬁcient algorithm for low-rank
MDPs. In all parameters, the statistical complexity is much worse than REP-UCB . Our algorithm
and FLAMBE operate under the same computation oracle. FLAMBE does not balance exploration
and exploitation, and uses explore-then-committee style techniques (i.e., constructions of absorbing
MDPs (Brafman & Tennenholtz, 2002)) which results in its worse sample complexity."
RELATED WORK,0.03477051460361613,"With a more complex oracle, Mofﬂe (Modi et al., 2021) is a model-free algorithm for low-rank
MDPs, with two additional assumptions: (1) the transition has low non-negative rank (nnr), and
(2) reachability in latent states. The ﬁrst assumption signiﬁcantly restricts the scope of low-rank
MDPs as there are matrices whose nnr is exponentially larger than the rank (Agarwal et al., 2020b).
The sample complexity of Mofﬂe can scale O(d6|A|13/(ϵ2η5(1 −γ)5)), where η is the reachability
probability, and 1/η could be as large as nnr1/2 (Proposition 4 in Agarwal et al. (2020b)), which
essentially means that Mofﬂe has a polynomial dependence on the nnr."
RELATED WORK,0.03616133518776078,"OLIVE (Jiang et al., 2017), Witness rank (Sun et al., 2019) and Bilinear-UCB (Du et al., 2021),
when specialized to low-rank MDPs, have slightly tighter dependence on d (e.g., O(d2/ϵ2)). But
these algorithms are computationally inefﬁcient as they are version space algorithms. Dann et al.
(2021) shows that with a policy class, solving a low-rank MDP can take Ω(2d) samples. In this work,
similar to Witness rank (Sun et al., 2019) and FLAMBE, we use function approximators to model the
transition. Thus our positive result is not in contradiction to the result from Dann et al. (2021)."
RELATED WORK,0.037552155771905425,"VALOR (Dann et al., 2018), PCID (Du et al., 2019a), HOMER (Misra et al., 2020), RegRL (Foster
et al., 2020), and the approach from Feng et al. (2020) are algorithms for block MDPs which is a
more restricted setting than low-rank MDPs. These works require additional assumptions such as
deterministic transitions (Dann et al., 2018), reachability (Misra et al., 2020; Du et al., 2019a), strong
Bellman closure (Foster et al., 2020), and strong unsupervised learning oracles (Feng et al., 2020)."
RELATED WORK,0.03894297635605007,"Ofﬂine Setting
We discuss related works in ofﬂine RL. Additional related works are discussed in
Section A."
RELATED WORK,0.04033379694019471,Published as a conference paper at ICLR 2022
RELATED WORK,0.04172461752433936,"Uehara & Sun (2021) obtained similar statistical results for ofﬂine RL on low-rank MDPs. Though
the sample complexity in their algorithm is slightly tighter, our algorithm is oracle-efﬁcient, while
the CPPO algorithm from Uehara & Sun (2021) is a version space algorithm."
RELATED WORK,0.043115438108484005,"Xie et al. (2021) propose a (general) pessimistic model-free algorithm in the ofﬂine setting. We can
also apply their algorithm to low-rank MDPs and show some ﬁnite-sample guarantee. However, it is
unclear whether the ﬁnal bounds in their results can be characterized by the relative condition num-
ber only using the true representation, and whether they can compete with history-dependent poli-
cies. Thus, our result is still considered superior on low-rank MDPs. The detail is given in Section E."
PRELIMINARIES,0.04450625869262865,"3
PRELIMINARIES"
PRELIMINARIES,0.0458970792767733,"We consider an episodic discounted inﬁnite horizon Markov Decision Process M
=
⟨S, A, P, r, γ, d0⟩speciﬁed by a state space S, a discrete action space A, a transition model
P : S × A →∆(S), a reward function r : S × A →R, a discount factor γ ∈[0, 1), and an
initial distribution d0 ∈∆(S). To simplify the presentation, we assume r(s, a) and d0 are known
(e.g., when d0 is a probability mass only on s0, agent always starts from a ﬁxed initial state s0)3.
Following prior work (Jiang et al., 2017; Sun et al., 2019), we assume trajectory reward is normal-
ized, i.e., for any trajectory {sh, ah}∞
h=0, we have P∞
h=0 γhr(sh, ah) ∈[0, 1]. Since the ground
truth P ⋆is unknown, we need to learn it by interacting with environments in an online manner or
utilizing ofﬂine data at hand. We remark that the extension of our all results to the ﬁnite horizon
nonstationary case is straightforward. For example, refer to Zhang et al. (2022)."
PRELIMINARIES,0.04728789986091794,"We use the following notation. Given a policy π : S →∆(A), we deﬁne the value function
V π
P (s) = E
P∞
h=0 γhr(sh, ah)|s0 = s, P, π

to represent the expected total discounted reward of
π under P starting at s. Similarly, we deﬁne the state-action Q function Qπ
P (s, a) := r(s, a) +
γEs′∼P (·|s,a)V π
P (s′). The expected total discounted reward of a policy π under transition P and
reward r is denoted as V π
P,r := Es0∼d0V π
P (s0). We deﬁne the discounted state-action occupancy
distribution dπ
P (s, a) = (1 −γ) P∞
t=0 γtdπ
P,t(s, a), where dπ
P,t(s, a) is the probability of π visiting
(s, a) at time step t under π and P. We slightly abuse the notation, and denote dπ
P (s) as the state
visitation, which is equal to P"
PRELIMINARIES,0.048678720445062586,"a∈A dπ
P (s, a). When P is the true transition model P ⋆, we drop
the subscript and simply use dπ(·). Unless otherwise noted, Π denotes the class of all polices
{S →∆(A)}. We denote total variation distance of P1 and P2 by ∥P1 −P2∥1. Finally, given a
vector a, we deﬁne ∥a∥B =
√"
PRELIMINARIES,0.05006954102920723,"a⊤Ba. c0, c1, · · · are universal constants."
PRELIMINARIES,0.05146036161335188,"We study low-rank MDPs deﬁned as follows (Jiang et al., 2017; Agarwal et al., 2020b). The condi-
tions on the upper bounds of the norm of φ⋆, µ⋆are just for normalization.
Deﬁnition 1 (Low-rank MDP). A transition model P ⋆: S × A →∆(A) admits a low rank
decomposition with rank d ∈N if there exists two embedding functions φ⋆µ⋆such that"
PRELIMINARIES,0.05285118219749652,"∀s, s′ ∈S, a ∈A : P ⋆(s′ | s, a) = µ⋆(s′)⊤φ⋆(s, a)"
PRELIMINARIES,0.054242002781641166,"where ∥φ∗(s, a)∥2 ≤1 for all (s, a) and for any function g : S →[0, 1], ∥
R
µ⋆(s)g(s)d(s)∥2 ≤
√"
PRELIMINARIES,0.055632823365785816,"d.
An MDP is a low rank MDP if P ⋆admits such a low rank decomposition."
PRELIMINARIES,0.05702364394993046,"Low-rank MDPs capture the latent variable model (Agarwal et al., 2020b) where φ⋆(s, a) is a dis-
tribution over a discrete latent state space Z. The block-MDP model (Du et al., 2019a) is a special
instance of the latent variable model with φ⋆(s, a) being a one-hot encoding vector. Note the linear
MDPs (Yang & Wang, 2020; Jin et al., 2020a) assume φ⋆is known."
PRELIMINARIES,0.0584144645340751,"Next, we explain two settings: the online learning setting and the ofﬂine learning setting. Then, we
present our function approximation setup and computational oracles."
PRELIMINARIES,0.059805285118219746,"Episodic Online learning
In online learning, our overall goal is to learn a stationary policy ˆπ so
that it maximizes V ˆπ
P ⋆,r, where P ⋆is the ground truth transition. We assume that we operate under
the episodic learning setting where we can only reset to states sampled from the initial distribution
d0 (e.g., to emphasize the challenge from exploration, we can consider the special case where we
can only reset to a ﬁxed s0). In the episodic setting, given a policy π, sampling a state s from the"
PRELIMINARIES,0.061196105702364396,"3Extension to the unknown case is straightforward. Recall the major challenging of RL is due to the un-
known transition model."
PRELIMINARIES,0.06258692628650904,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.06397774687065369,"state visitation dπ
P is done by the following roll-in procedure: starting at s0 ∼d0, at every time step
t, we terminate and return st with probability 1−γ, and otherwise we execute at ∼π(st) and move
to t + 1, i.e., st+1 ∼P(·|st, at). Such a sampling procedure is widely used in the policy gradient
and policy optimization literature (e.g., (Kakade & Langford, 2002; Agarwal et al., 2021; 2020a))."
PRELIMINARIES,0.06536856745479833,"Ofﬂine learning
In the ofﬂine RL, we are given a static dataset in the form of quadruples:"
PRELIMINARIES,0.06675938803894298,"D = {s(i), a(i), r(i), s′(i)}n
i=1 ∼ρ(s, a)δ(r = r(s, a))P ⋆(s′ | s, a)."
PRELIMINARIES,0.06815020862308763,"For simplicity, we assume ρ = dπb
P ⋆, where πb ∈[S →∆(A)] is a ﬁxed behavior policy. We denote
ED[f(s, a, s′)] = 1/n P"
PRELIMINARIES,0.06954102920723226,"(s,a,s′)∈D f(s, a, s′). To succeed in ofﬂine RL, we in general need some
coverage property of ρ. One common assumption is that ρ globally covers every possible policies’
state-action distribution, i.e., maxπ,s,a dπ
P ⋆(s, a)/ρ(s, a) < ∞(Antos et al., 2008). In this work, we
relax such a global coverage assumption and work under the partial coverage condition where ρ may
not cover distributions of all possible policies. Instead of competing against the optimal policy under
the global coverage, we aim to compete against any policies covered by the ofﬂine data. In section 5,
we precisely deﬁne the partial coverage condition using the concept of the relative condition number."
PRELIMINARIES,0.07093184979137691,"Function approximation setup and computational oracles
Since µ⋆and φ⋆are unknown, we
use function classes to capture them. Our function approximation and computational oracles are ex-
actly the same as the ones used in FLAMBE. For completeness, we state the function approximation
and computational oracles below."
PRELIMINARIES,0.07232267037552156,"Assumption 2. We have a model class M = {(µ, φ) : µ ∈Ψ, φ ∈Φ}, where µ⋆∈Ψ, φ⋆∈Φ."
PRELIMINARIES,0.0737134909596662,"Following the norm bounds on µ⋆, φ⋆we similarly assume that the same norm bounds hold
for our function approximator, i.e., for any µ ∈Ψ, φ ∈Φ, ∥φ(s, a)∥2 ≤1, ∀(s, a) and
∥
R
µ(s)g(s)d(s)∥2 ≤
√"
PRELIMINARIES,0.07510431154381085,"d, ∀g : S →[0, 1], and
R
µ⊤(s′)φ(s, a)d(s′) = 1, ∀(s, a)."
PRELIMINARIES,0.07649513212795549,"As for computational oracles, we use a supervised learning style MLE oracle."
PRELIMINARIES,0.07788595271210014,"Deﬁnition 3 (Maximum Likelihood Oracle (MLE)). Consider the model class M and a dataset D
in the form of (s, a, s′), the MLE oracle returns the maixmum likelihood estimator ˆP := (ˆµ, ˆφ) =
arg max(µ,φ)∈M ED ln(µ(s′)⊤φ(s, a))."
PRELIMINARIES,0.07927677329624479,"We also invoke a planning procedure for known linear MDPs with potentially nonlinear rewards,
which can be done in polynomial time (we know that online learning in linear MDPs can be
done statistically and computationally efﬁcient). Given a reward r and a model P := (µ, φ) with
P(s′|s, a) = µ(s′)⊤φ(s, a) (i.e., a known linear transition with a known feature φ), we can compute
the optimal policy arg maxπ V π
P,r by standard least square value iteration which uses linear regres-
sion. A planning procedure for a known linear MDP is also used in FLAMBE, see Section 5.1 in
Agarwal et al. (2020b) how to implement this procedure with polynomial computation complexity."
REPRESENTATION LEARNING IN ONLINE SETTING,0.08066759388038942,"4
REPRESENTATION LEARNING IN ONLINE SETTING"
REPRESENTATION LEARNING IN ONLINE SETTING,0.08205841446453407,"We consider the online episodic learning setting where the agent can only reset based on the initial
state distribution d0. To ﬁnd a near-optimal policy for a low-rank MDP efﬁciently, we need to
carefully interleave representation learning, exploration, and exploitation."
ALGORITHM,0.08344923504867872,"4.1
ALGORITHM"
ALGORITHM,0.08484005563282336,"We present our algorithm in the online setting in Algorithm 1. We ﬁrst describe the data collection
process. Every iteration, Algorithm 1 rollouts its current policy π to collect a tuple (s, a, s′, a′, ˜s)
where s ∼dπ
P ⋆, a ∼U(A), s′ ∼P ⋆(·|s, a), a′ ∼U(A), ˜s ∼P ⋆(· | s, a) where U(A) is a
uniform distribution over actions (note that we take two uniform actions here). Recall that to sample
s ∼dπ
P ⋆, we start at s0 ∼d0, at every time step t, we terminate and return st with probability 1 −γ,
and otherwise we execute at ∼π(st) and move to t + 1, i.e., st+1 ∼P ⋆(·|st, at). Thus collecting
one tuple requires exactly one roll-in, i.e., one trajectory. We can verify that with high probability
the roll-in terminates with ˜O((1 −γ)−1) steps which is often called the effective horizon."
ALGORITHM,0.08623087621696801,"After collecting new data and concatenating it with the existing data, we perform representation
learning, i,e, learning a factorization and a representation by MLE (line 6), set the bonus based on"
ALGORITHM,0.08762169680111266,Published as a conference paper at ICLR 2022
ALGORITHM,0.0890125173852573,"Algorithm 1 UCB-driven representation learning, exploration, and exploitation (REP-UCB)"
ALGORITHM,0.09040333796940195,"1: Input: Regularizer λn, parameter αn, Models M = {(µ, φ) : µ ∈Ψ, φ ∈Φ}, Iteration N
2: Initialize π0(· | s) to be uniform; set D0 = ∅, D′
0 = ∅
3: for episode n = 1, · · · , N do
4:
Collect a tuple (s, a, s′, a′, ˜s) with"
ALGORITHM,0.0917941585535466,"s ∼dπn−1
P ⋆
, a ∼U(A), s′ ∼P ⋆(·|s, a), a′ ∼U(A), ˜s ∼P ⋆(·|s′, a′)"
ALGORITHM,0.09318497913769123,"5:
Update datasets by adding triples (s, a, s′) and (s′, a′, ˜s):"
ALGORITHM,0.09457579972183588,"Dn = Dn−1 + {(s, a, s′)},
D′
n = D′
n−1 + {(s′, a′, ˜s)}"
ALGORITHM,0.09596662030598054,"6:
Learn representation via ERM (i.e., MLE):"
ALGORITHM,0.09735744089012517,"ˆPn := (ˆµn, ˆφn) = arg max(µ,φ)∈M EDn+D′n

ln µ⊤(s′)φ(s, a)
"
ALGORITHM,0.09874826147426982,"7:
Update empirical covariance matrix ˆΣn = P"
ALGORITHM,0.10013908205841446,"s,a∈Dn ˆφn(s, a)ˆφn(s, a)⊤+ λnI
8:
Set the exploration bonus:"
ALGORITHM,0.10152990264255911,"ˆbn(s, a) := min

αn
q"
ALGORITHM,0.10292072322670376,"ˆφn(s, a)⊤ˆΣ−1
n ˆφn(s, a), 2

(1)"
ALGORITHM,0.1043115438108484,"9:
Update policy πn = arg maxπ V π
ˆ
Pn,r+ˆbn
10: end for
11: Return π1, · · · , πN"
ALGORITHM,0.10570236439499305,"the learned feature (Eq. 1), and update the policy via planning inside the learned model with the
bonus-enhanced reward (Line 9). Note the learned transition ˆP from MLE is linear with respect to
the learned feature ˆφ, and planning in a known linear MDP is known as computationally efﬁcient
(Jin et al., 2020a) (see the explanation after Deﬁnition 3 as well)."
ALGORITHM,0.1070931849791377,"Computation of the MLE oracle
The MLE oracle in general could be a non-convex optimization
procedure when φ and µ are general nonlinear function approximators. However, this is a standard
supervised learning ERM oracle and one can easily optimize it via stochastic gradient descent style
approaches if µ and φ are differentiable. For special cases where the MDP is a tabular MDP, the MLE
objective is convex and the optimal solution has closed-form. For linear MDPs (Yang & Wang, 2020)
where P ⋆(s′|s, a) = (ψ⋆(s′))⊤M ⋆φ⋆(s, a) with known µ⋆and ψ⋆but unknown M ⋆, the MLE
objective again is convex with respect to parameter M ⋆. Thus when specializing to speciﬁc settings
such as tabular MDPs and linear MDPs where computationally efﬁcient approaches exist, Rep-UCB
is also provably computationally efﬁcient. In contrast, more general approaches such as Olive (Jiang
et al., 2017) are provably computationally inefﬁcient even when specialized to tabular MDPs. We
note that our setting does not directly capture the linear MDP setting from Jin et al. (2020a) since
there µ⋆might not be captured by a model class Ψ that has bounded statistical complexity."
ANALYSIS,0.10848400556328233,"4.2
ANALYSIS"
ANALYSIS,0.10987482614742698,"Theorem 4 (PAC Bound for REP-UCB). Fix δ ∈(0, 1), ϵ ∈(0, 1). Let ˆπ be a uniform mixture of
π1, · · · , πN and π⋆:= arg maxπ V π
P ⋆,r as the optimal policy. By setting parameters as follows:"
ANALYSIS,0.11126564673157163,"αn = O
p"
ANALYSIS,0.11265646731571627,"(|A| + d2) γ ln(|M|n/δ)

,
λn = O (d ln(|M|n/δ)) ,"
ANALYSIS,0.11404728789986092,"with probability at least 1 −δ, we have"
ANALYSIS,0.11543810848400557,"V π⋆
P ⋆,r −V ˆπ
P ⋆,r ≤ϵ,
where the number of collected samples is at most"
ANALYSIS,0.1168289290681502,"O
d4|A|2 ln(|M|/δ)"
ANALYSIS,0.11821974965229486,"(1 −γ)5ϵ2
· ν

,"
ANALYSIS,0.11961057023643949,where ν only contains log terms and the dependence on |M| is at most ln(ln(|M|)).
ANALYSIS,0.12100139082058414,"The theorem shows that REP-UCB learns in low-rank MDPs in a statistically efﬁcient and oracle-
efﬁcient manner. To the best of our knowledge, this algorithm has the best sample complexity among"
ANALYSIS,0.12239221140472879,Published as a conference paper at ICLR 2022
ANALYSIS,0.12378303198887343,"all oracle efﬁcient algorithms for low-rank MDPs. Extension to continuous function class Ψ and Φ
using statistical complexities such as covering dimension is possible since our analysis only uses
standard uniform convergence analysis on Ψ and Φ."
ANALYSIS,0.12517385257301808,"Highlight of the analysis
Below we highlight our key lemmas and proof techniques."
ANALYSIS,0.12656467315716272,"First, why is learning in a low-rank MDP harder than learning in models with linear structures?
Unlike standard linear models such as linear MDPs (Yang & Wang, 2020; Jin et al., 2020a),
KNRs (Kakade et al., 2020; Abbasi-Yadkori & Szepesv´ari, 2011; Mania et al., 2020; Song &
Sun, 2021), and GP / kernel models (Chowdhury & Gopalan, 2019; Curi et al., 2020), we can-
not get uncertainty quantiﬁcation on the model in a point-wise manner. When models are linear,
one can get the following style of point-wise uncertainty quantiﬁcation for the learned model ˆP:
∀s, a : ℓ( ˆP(·|s, a), P ⋆(·|s, a)) ≤σ(s, a) where σ(s, a) is the uncertainty measure, and ℓis some
distance metric (e.g., ℓ1 norm). With proper scaling, the uncertainty measure σ(s, a) is then used
for the bonus. For example, in linear MDPs (i.e., low-rank MDP with known feature φ⋆), given a
dataset D = {s, a, s′}, we can learn a non-parametric model ˆP(s′|s, a) := ˆµ(s′)⊤φ⋆(s, a) , and get
point-wise uncertainty quantiﬁcation:"
ANALYSIS,0.12795549374130738,"∀(s, a), |
Z
f(s′)ˆµ⊤(s′)φ⋆(s, a)d(s′) −
Z
f(s′)µ⋆⊤(s′)φ⋆(s, a)d(s′)| ≤c∥φ⋆(s, a)∥Σ−1
φ⋆
(2)"
ANALYSIS,0.12934631432545202,"for some family of functions f : S →R with Σφ⋆= P
s,a∈D φ⋆(s, a)φ⋆(s, a)+λI (Lykouris et al.,
2021; Neu & Pike-Burke, 2020). To set the scaling c properly, since φ⋆is known a priori, the linear
regression analysis applies here, and one can apply Cauchy-Schwarz inequality to the LHS of (2) to
pull out φ⋆and get an upper bound in the form of"
ANALYSIS,0.13073713490959665,"∥φ⋆(s, a)∥Σ−1
φ⋆
|
{z
}
(a)"
ANALYSIS,0.13212795549374132,"∥
Z
f(s){ˆµ(s) −µ⋆(s)}d(s)∥Σφ⋆
|
{z
}
(b)"
ANALYSIS,0.13351877607788595,where c is set to be the linear regression training error measured in the term (b) above.
ANALYSIS,0.1349095966620306,"However, when we jointly learn µ and φ, since nonlinear function approximation is used, we can-
not get point-wise uncertainty quantiﬁcation via linear regression-based analysis. We stress that
our bonus is not designed to capture the uncertainty quantiﬁcation on the model error between
ˆP(·|s, a) = ˆµ⊤ˆφ(s, a) and P ⋆(·|s, a) = µ⋆⊤φ⋆(s, a) in a point-wise way, which is not tractable as
ˆP and P ⋆does not even share the same representation. Instead, the bonus is carefully designed so
that it only provides near-optimism at the initial state distribution. This is formalized as follows."
ANALYSIS,0.13630041724617525,"Lemma 5 (Almost Optimism at the Initial State Distribution). Set the parameters as in Theorem 4.
With probability 1 −δ,"
ANALYSIS,0.1376912378303199,"∀n ∈[1, · · · , N], ∀π ∈Π, V π
ˆ
Pn,r+ˆbn −V π
P ⋆,r ≥−c1
q"
ANALYSIS,0.13908205841446453,"|A| ln(|M|n/δ)(1−γ)−1 n
."
ANALYSIS,0.1404728789986092,"We remark that the idea of optimism with respect to the initial state distribution has been used in
prior works (Jiang et al., 2017; Sun et al., 2019; Du et al., 2021; Zanette et al., 2020). However, these
algorithms are not computationally efﬁcient (i.e., they use version space instead of reward bonus),
and their version-space based analysis is different from ours."
ANALYSIS,0.14186369958275383,"Proof sketch for Lemma 5
We start by using the simulation lemma (Lemma 20) inside the learned
model which is important since our bonus ˆbn uses ˆφn associated with the learned model ˆPn:"
ANALYSIS,0.14325452016689846,"V π
ˆ
Pn,r+ˆbn −V π
P ⋆,r ≥(1 −γ)−1Es,a∼dπ
ˆ
Pn"
ANALYSIS,0.14464534075104313,"h
ˆbn(s, a) −∥ˆPn(·|s, a) −P ⋆(·|s, a)∥1
i
,"
ANALYSIS,0.14603616133518776,"from where we show that Es,a∼dπ
ˆ
Pn∥ˆP(·|s, a) −P ⋆(·|s, a)∥1 as a whole nearly lower bounds the"
ANALYSIS,0.1474269819193324,"average bonus Es,a∼dπ
ˆ
Pn[ˆbn(s, a)]. Thus the proof of optimism is fundamentally different from the
proofs in tabular and linear MDPs which are done via induction in a point-wise manner. The detailed
procedure is illustrated in Lemma 7 in Appendix B."
ANALYSIS,0.14881780250347706,Published as a conference paper at ICLR 2022
ANALYSIS,0.1502086230876217,"Second, our bonus is using representation ˆφn that is being updated every episode, and our empirical
covariance matrix ˆΣn is also updated whenever we update ˆφn, which means that standard elliptical
potential based analysis (i.e., analysis used in linear bandits/MDPs with known features) cannot
work here as our feature changes every episode. Instead, in our analysis, we have to keep tracking a
potential function that is deﬁned using the unknown ground truth representation φ⋆, i.e., the elliptical
potential ∥φ⋆(s, a)∥2
Σ−1
ρn,φ⋆, where"
ANALYSIS,0.15159944367176634,"Σρn,φ⋆= nE(s,a)∼ρnφ⋆(s, a)φ⋆(s, a)⊤+ λnI,"
ANALYSIS,0.15299026425591097,"and ρn(s, a) = Pn−1
i=0 dπi
P ⋆(s, a)/n. Since this potential function uses the ﬁxed representation φ⋆,
we can apply the standard elliptical potential argument to track the progress that our algorithm makes
during learning. Below we illustrate the procedure of linking the bonus under ˆφn to the potential
function ∥φ⋆(s, a)∥2
Σ−1
ρn,φ⋆deﬁned with respect to the true feature φ⋆."
ANALYSIS,0.15438108484005564,"Linking bonus under ˆφn to the elliptical potential function under φ⋆
With near optimism,
using the simulation lemma (Lemma 20) inside the real model, we can upper bound the per-iteration
regret as follows:"
ANALYSIS,0.15577190542420027,"V π⋆
P ⋆,r −V πn
P ⋆,r ≤(1 −γ)−1E(s,a)∼dπn
P ⋆[ˆbn(s, a) + (1 −γ)−1fn(s, a)] +
p"
ANALYSIS,0.1571627260083449,"|A|ζn(1 −γ)−1,"
ANALYSIS,0.15855354659248957,"where ζn = ˜O(1/n), and fn(s, a) := ∥ˆPn(· | s, a) −P ⋆(· | s, a)∥1. To connect the ﬁrst term in the
right-hand side of the above inequality to the elliptical potential under the ﬁxed feature φ⋆, we show
that for any function g ∈S × A →[0, B] for B ∈R+,"
ANALYSIS,0.1599443671766342,"E(s,a)∼dπn
P ⋆[g(s, a)] ≤(1 −γ)−1E(s,a)∼dπn
P ⋆
h
∥φ⋆(s, a)∥Σ−1
ρn,φ⋆ i q"
ANALYSIS,0.16133518776077885,"nγ|A|Eρ′n[g2(s, a)] + γλndB2 +
q"
ANALYSIS,0.1627260083449235,"(1 −γ)|A|Eρ′n[g2(s, a)],"
ANALYSIS,0.16411682892906815,"where ρ′
n(s, a) = 1/n Pn−1
i=0 dπi(s)u(a) and u(a) = 1/|A|. See Lemma 12 in Appendix B. By
substituting g with ˆbn + fn/(1 −γ), the ﬁrst term of the RHS of the above inequality can be upper
bounded as:"
ANALYSIS,0.16550764951321278,"2(1 −γ)−1 E(s,a)∼dπn
P ⋆
h
∥φ⋆(s, a)∥Σ−1
ρn,φ⋆ i"
ANALYSIS,0.16689847009735745,"|
{z
}
(G1) s"
ANALYSIS,0.16828929068150209,n|A|Eρ′n
ANALYSIS,0.16968011126564672," f 2n(s, a)"
ANALYSIS,0.17107093184979139,"(1 −γ)2 + ˆb2n(s, a)

+ λnd"
ANALYSIS,0.17246175243393602,"|
{z
}
(G2) ."
ANALYSIS,0.17385257301808066,"In the term (G2), we expect nEρ′n[f 2
n(s, a)] to be O(1) as Eρ′n[f 2
n(s, a)] is in order of 1/n due to
the fact that it is the generalization bound of the MLE estimator ˆPn which is trained on the data
drawn from ρ′
n . For nEρ′n[ˆb2
n(s, a)], we expect it to be in the order of d as the (unnormalized)
data covariance matrix ˆΣn in the bonus ˆbn uses training data from ρ′
n, i.e., we are measuring the
expected bonus under the training distribution. In other words, the term (G2) scales in order of
poly(d). For the term (G1), since it contains the potential function based on φ⋆, the sum of the term
(G1) over all episodes can be controlled by the standard elliptical potential argument (see Lemma 18
and Lemma 19). This concludes the proof sketch of our main theorem."
ANALYSIS,0.17524339360222532,"In summary, our analysis relies on the standard idea of optimism in the face of uncertainty, but
with novel techniques to achieve optimism under nonlinear function approximation with the MLE
supervised learning style generalization bound, and to track regret under changing representations."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.17663421418636996,"5
REPRESENTATION LEARNING IN OFFLINE SETTING"
REPRESENTATION LEARNING IN OFFLINE SETTING,0.1780250347705146,"In this section, we study representation learning in the ofﬂine setting. We consider the setting where
the ofﬂine data does not have a full global coverage. We present our algorithm Lower Conﬁdence
Bound driven Representation Learning in ofﬂine RL (REP-LCB) in Algorithm 2. Our proposed
algorithm consists of three parts. The ﬁrst part is MLE which learns a model ˆP and a representation
ˆφ. The second part is the construction of a penalty term ˆb. Using the learned representation ˆφ, we
use a standard bonus in linear bandits as the penalty term as if ˆφ were the true feature. The third part
is planning with the learned model ˆP and reward r −ˆb."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.17941585535465926,Published as a conference paper at ICLR 2022
REPRESENTATION LEARNING IN OFFLINE SETTING,0.1808066759388039,Algorithm 2 LCB-driven Representation Learning in ofﬂine RL (REP-LCB)
REPRESENTATION LEARNING IN OFFLINE SETTING,0.18219749652294853,"1: Input: Regularizer λ, Parameter α, Model classes M = {µ⊤φ : µ ∈Ψ, φ ∈Φ}, Dataset D.
2: Learn a model ˆP by MLE: ˆP = ˆµ⊤ˆφ = arg maxP ∈M ED[ln P(s′ | s, a)]."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.1835883171070932,3: Set the empirical covariance matrix ˆΣ = P
REPRESENTATION LEARNING IN OFFLINE SETTING,0.18497913769123783,"(s,a)∈D ˆφ(s, a)ˆφ⊤(s, a) + λI.
4: Set the reward penalty:
ˆb(s, a) = min

α
q"
REPRESENTATION LEARNING IN OFFLINE SETTING,0.18636995827538247,"ˆφ(s, a)⊤ˆΣ−1 ˆφ(s, a), 2

."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.18776077885952713,"5: Solve ˆπ = arg maxπ V π
ˆ
P ,r−ˆb."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.18915159944367177,"We present the PAC guarantee of REP-LCB. Before proceeding, we deﬁne a relative condition
number as a mean to measure the deviation between a comparator policy π and the ofﬂine data:"
REPRESENTATION LEARNING IN OFFLINE SETTING,0.1905424200278164,"C⋆
π = sup
x∈Rd"
REPRESENTATION LEARNING IN OFFLINE SETTING,0.19193324061196107,"x⊤Edπ
P ⋆[φ⋆(s, a)φ⋆⊤(s, a)]x"
REPRESENTATION LEARNING IN OFFLINE SETTING,0.1933240611961057,"x⊤Eρ[φ⋆(s, a)φ⋆⊤(s, a)]x
."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.19471488178025034,"In the special case where the MDP is just a tabular MDP (i.e., φ⋆is a one-hot encoding vector), this
is reduced to a density ratio C⋆
∞= maxs,a dπ
P ⋆(s, a)/ρ(s, a). The relative condition number C⋆
π is
always no larger than the density ratio and could be much smaller for MDPs with large state spaces.
Note that we quantify the relative condition number using the unknown true representation φ⋆. With
the above setup, now we are ready to state the main theorem for REP-LCB."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.19610570236439498,"Theorem 6 (PAC Bound for REP-LCB). Let ω = maxa,s(1/πb(a | s)). Denote ˆπ as the output of
REP-LCB. There exists a set of parameters such that with probability at least 1 −δ, for any policy
π (including history-dependent non-Markovian policies),"
REPRESENTATION LEARNING IN OFFLINE SETTING,0.19749652294853964,"V π
P ⋆,r −V ˆπ
P ⋆,r ≤c s"
REPRESENTATION LEARNING IN OFFLINE SETTING,0.19888734353268428,d4ω2C⋆π log(|M|/δ)
REPRESENTATION LEARNING IN OFFLINE SETTING,0.20027816411682892,"(1 −γ)4n
."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.20166898470097358,"See Theorem 14 in Appendix B for the detailed parameters. We explain several implications. First of
all, this theorem shows that we can uniformly compete with any policy including history-dependent
non-Markovian policies 4 satisfying the partial coverage C⋆
π < ∞. Particularly, if the optimal policy
π⋆is covered by the ofﬂine data, i.e., C⋆
π⋆< ∞, then our algorithm is able to compete against it
5. Note that assuming ofﬂine data covers π⋆is still a weaker assumption than the global coverage
such as supπ sup(s,a) dπ
P ⋆(s, a)/ρ(s, a) in prior ofﬂine RL works (Antos et al., 2008; Chen & Jiang,
2019). Second, our coverage condition is measured by a relative condition number deﬁned using the
unknown ground truth representation φ⋆but not depending on other features. Prior works that use
relative condition numbers as measures of coverage are restricted to the settings where the ground
truth representation φ⋆is known (Jin et al., 2020b; Chang et al., 2021; Zanette et al., 2021b)."
REPRESENTATION LEARNING IN OFFLINE SETTING,0.20305980528511822,"To sum up, our algorithm is the ﬁrst oracle efﬁcient algorithm which does not need to know φ⋆,
and requires partial coverage only in terms of φ⋆. Note while Uehara & Sun (2021) has a similar
guarantee on low-rank MDPs, their algorithm is not oracle-efﬁcient as it is a version space algorithm."
CONCLUSION,0.20445062586926285,"6
CONCLUSION"
CONCLUSION,0.20584144645340752,"We study online/ofﬂine RL on low-rank MDPs, where the ground truth feature is not known a pri-
ori. For online RL, our new algorithm REP-UCB signiﬁcantly improves the sample complexity of
the piror state-of-the-art algorithm FLAMBE in all parameters while using the same computational
oracles. REP-UCB has the best sample complexity among existing oracle efﬁcient algorithms for
low-rank MDPs by a margin. Comparing to prior representation learning works on low-rank MDPs
and block MDPs that rely on a forward step-by-step reward-free exploration framework, our algo-
rithm interleaves representation learning, exploration, and exploitation together, and learns a single
stationary policy. For ofﬂine RL, our new algorithm REP-LCB is the ﬁrst oracle efﬁcient algorithm
for low-rank MDPs that has a PAC guarantee under a partial coverage condition measured by the
relative condition number deﬁned with the true feature representation."
CONCLUSION,0.20723226703755215,"4Given π = {πi}∞
i=0 where πi depends on s0, a0, . . . si, V π
P ⋆,r and dπ
P ⋆(s, a) are still well-deﬁned.
5We also require ω < ∞, which is a mild assumption since it does not involve P ⋆. Indeed, it is much
weaker than the global coverage type assumption 1/ρ(s, a) < ∞, ∀(s, a)."
CONCLUSION,0.2086230876216968,Published as a conference paper at ICLR 2022
CONCLUSION,0.21001390820584145,ACKNOWLEDGMENTS
CONCLUSION,0.2114047287899861,"The authors would like to thank Alekh Agarwal, Praneeth Netrapalli and Ming Yin for valuable
feedback."
CONCLUSION,0.21279554937413073,Masatoshi Uehara is partially supported by Masason foundation.
REFERENCES,0.2141863699582754,REFERENCES
REFERENCES,0.21557719054242003,"Yasin Abbasi-Yadkori and Csaba Szepesv´ari.
Regret bounds for the adaptive control of linear
quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, pp. 1–
26. JMLR Workshop and Conference Proceedings, 2011."
REFERENCES,0.21696801112656466,"Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning, pp. 1638–1646. PMLR, 2014."
REFERENCES,0.21835883171070933,"Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-
ration for provable policy gradient learning. NeurIPS, 2020a."
REFERENCES,0.21974965229485396,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complex-
ity and representation learning of low rank mdps. In Advances in Neural Information Processing
Systems, volume 33, pp. 20095–20107, 2020b."
REFERENCES,0.2211404728789986,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1–76, 2021."
REFERENCES,0.22253129346314326,"Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning,
71:89–129, 2008."
REFERENCES,0.2239221140472879,"Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213–231, 2002."
REFERENCES,0.22531293463143254,"Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
covariate shift in imitation learning via ofﬂine data without great coverage. 2021."
REFERENCES,0.2267037552155772,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 1042–
1051, 2019."
REFERENCES,0.22809457579972184,"Sayak Ray Chowdhury and Aditya Gopalan. Online learning in kernelized markov decision pro-
cesses. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pp. 3197–
3205. PMLR, 2019."
REFERENCES,0.22948539638386647,"Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efﬁcient model-based reinforcement learn-
ing through optimistic policy search and planning. arXiv preprint arXiv:2006.08684, 2020."
REFERENCES,0.23087621696801114,"Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E
Schapire. On oracle-efﬁcient pac rl with rich observations. arXiv preprint arXiv:1803.00606,
2018."
REFERENCES,0.23226703755215578,"Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Ag-
nostic reinforcement learning with low-rank mdps and rich observations.
arXiv preprint
arXiv:2106.11519, 2021."
REFERENCES,0.2336578581363004,"Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efﬁcient rl with rich observations via latent state decoding. In International Conference
on Machine Learning, pp. 1665–1674. PMLR, 2019a."
REFERENCES,0.23504867872044508,"Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufﬁcient
for sample efﬁcient reinforcement learning? arXiv preprint arXiv:1910.03016, 2019b."
REFERENCES,0.2364394993045897,Published as a conference paper at ICLR 2022
REFERENCES,0.23783031988873435,"Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and
Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. ICML,
2021."
REFERENCES,0.23922114047287898,"Miroslav Dud´ık, Nika Haghtalab, Haipeng Luo, Robert E Schapire, Vasilis Syrgkanis, and Jen-
nifer Wortman Vaughan. Oracle-efﬁcient online learning and auction design. In 2017 ieee 58th
annual symposium on foundations of computer science (focs), pp. 528–539. IEEE, 2017."
REFERENCES,0.24061196105702365,"Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin F Yang. Provably efﬁcient exploration
for reinforcement learning using unsupervised learning. arXiv preprint arXiv:2003.06898, 2020."
REFERENCES,0.24200278164116829,"Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efﬁcient contextual bandits with
regression oracles. In International Conference on Machine Learning, pp. 3199–3210. PMLR,
2020."
REFERENCES,0.24339360222531292,"Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu.
Instance-dependent
complexity of contextual bandits and reinforcement learning: A disagreement-based perspective.
arXiv preprint arXiv:2010.03104, 2020."
REFERENCES,0.24478442280945759,"Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesv´ari, and Mengdi Wang. Sparse feature selec-
tion makes batch reinforcement learning more sample efﬁcient. In International Conference on
Machine Learning, pp. 4063–4073. PMLR, 2021a."
REFERENCES,0.24617524339360222,"Botao Hao, Tor Lattimore, Csaba Szepesv´ari, and Mengdi Wang. Online sparse reinforcement learn-
ing. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 316–324. PMLR,
2021b."
REFERENCES,0.24756606397774686,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low bellman rank are pac-learnable. In International Conference
on Machine Learning, 2017."
REFERENCES,0.24895688456189152,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.
PMLR, 2020a."
REFERENCES,0.25034770514603616,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efﬁcient for ofﬂine rl? arXiv
preprint arXiv:2012.15085, 2020b."
REFERENCES,0.2517385257301808,"Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002."
REFERENCES,0.25312934631432543,"Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Infor-
mation theoretic regret bounds for online nonlinear control. In Advances in Neural Information
Processing Systems, volume 33, pp. 15312–15325, 2020."
REFERENCES,0.2545201668984701,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofﬂine reinforcement learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21810–21823. Curran Associates, Inc., 2020."
REFERENCES,0.25591098748261476,"Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30–37, 2009."
REFERENCES,0.2573018080667594,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.25869262865090403,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy
reinforcement learning without great exploration. In Advances in Neural Information Processing
Systems, volume 33, pp. 1264–1274, 2020."
REFERENCES,0.26008344923504867,"Rui Lu, Gao Huang, and Simon S Du. On the power of multitask representation learning in linear
mdp. arXiv preprint arXiv:2106.08053, 2021."
REFERENCES,0.2614742698191933,"Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration
in episodic reinforcement learning. In Conference on Learning Theory, pp. 3242–3245. PMLR,
2021."
REFERENCES,0.26286509040333794,Published as a conference paper at ICLR 2022
REFERENCES,0.26425591098748263,"Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identiﬁ-
cation with guarantees. arXiv preprint arXiv:2006.10277, 2020."
REFERENCES,0.26564673157162727,"Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state ab-
straction and provably efﬁcient rich-observation reinforcement learning. In International confer-
ence on machine learning, pp. 6961–6971. PMLR, 2020."
REFERENCES,0.2670375521557719,"Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps.
arXiv preprint arXiv:2102.07035,
2021."
REFERENCES,0.26842837273991654,"Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning.
arXiv preprint arXiv:2007.01891, 2020."
REFERENCES,0.2698191933240612,"Chengzhuo Ni, Anru Zhang, Yaqi Duan, and Mengdi Wang. Learning good state and action repre-
sentations via tensor decomposition. arXiv preprint arXiv:2105.01136, 2021."
REFERENCES,0.2712100139082058,"Matteo Papini, Andrea Tirinzoni, Marcello Restelli, Alessandro Lazaric, and Matteo Pirotta. Lever-
aging good representations in linear contextual bandits. arXiv preprint arXiv:2104.03781, 2021."
REFERENCES,0.2726008344923505,"Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofﬂine rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021."
REFERENCES,0.27399165507649514,"Devavrat Shah, Dogyoon Song, Zhi Xu, and Yuzhe Yang. Sample efﬁcient reinforcement learning
via low-rank matrix estimation. Advances in Neural Information Processing Systems, 33:12092–
12103, 2020."
REFERENCES,0.2753824756606398,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–
1144, 2018."
REFERENCES,0.2767732962447844,"Yuda Song and Wen Sun. Pc-mlp: Model-based reinforcement learning with policy cover guided
exploration. In International Conference on Machine Learning, pp. 9801–9811. PMLR, 2021."
REFERENCES,0.27816411682892905,"Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020."
REFERENCES,0.2795549374130737,"Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning
from reinforcement learning. In International Conference on Machine Learning, pp. 9870–9879.
PMLR, 2021."
REFERENCES,0.2809457579972184,"Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on learning theory, pp. 2898–2933. PMLR, 2019."
REFERENCES,0.282336578581363,"Masatoshi Uehara and Wen Sun.
Pessimistic model-based ofﬂine rl: Pac bounds and posterior
sampling under partial coverage. arXiv preprint arXiv:2107.06226, 2021."
REFERENCES,0.28372739916550765,"Ruosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of ofﬂine rl with
linear function approximation? arXiv preprint arXiv:2010.11895, 2020."
REFERENCES,0.2851182197496523,"Gell´ert Weisz, Philip Amortila, and Csaba Szepesv´ari. Exponential lower bounds for planning in
mdps with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory,
pp. 1237–1264. PMLR, 2021."
REFERENCES,0.2865090403337969,"Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for ofﬂine reinforcement learning. arXiv preprint arXiv:2106.06926, 2021."
REFERENCES,0.28789986091794156,"Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In Proceedings of the 37th International Conference on Machine Learning, pp.
10746–10756, 2020."
REFERENCES,0.28929068150208626,Published as a conference paper at ICLR 2022
REFERENCES,0.2906815020862309,"Mengjiao Yang and Oﬁr Nachum. Representation matters: Ofﬂine pretraining for sequential deci-
sion making. arXiv preprint arXiv:2102.05815, 2021."
REFERENCES,0.29207232267037553,"Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal ofﬂine reinforcement learning via double
variance reduction. arXiv preprint arXiv:2102.01748, 2021."
REFERENCES,0.29346314325452016,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based ofﬂine policy optimization. In Advances in Neural
Information Processing Systems, volume 33, pp. 14129–14142, 2020."
REFERENCES,0.2948539638386648,"Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning,
pp. 10978–10989. PMLR, 2020."
REFERENCES,0.29624478442280944,"Andrea Zanette, Ching-An Cheng, and Alekh Agarwal. Cautiously optimistic policy optimization
and exploration with linear function approximation. arXiv preprint arXiv:2103.12923, 2021a."
REFERENCES,0.29763560500695413,"Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable beneﬁts of actor-critic meth-
ods for ofﬂine reinforcement learning. arXiv preprint arXiv:2108.08812, 2021b."
REFERENCES,0.29902642559109877,"Weitong Zhang, Jiafan He, Dongruo Zhou, Amy Zhang, and Quanquan Gu.
Provably efﬁcient
representation learning in low-rank markov decision processes. arXiv preprint arXiv:2106.11935,
2021a."
REFERENCES,0.3004172461752434,"Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust ofﬂine reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021b."
REFERENCES,0.30180806675938804,"Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Wen Sun, and Alekh Agarwal.
Efﬁcient reinforcement learning in block mdps: A model-free representation learning approach.
arXiv preprint arXiv:2202.00063, 2022."
REFERENCES,0.3031988873435327,Published as a conference paper at ICLR 2022
REFERENCES,0.3045897079276773,"A
MORE RELATED WORK"
REFERENCES,0.30598052851182195,"Here, we mention several additional related works."
REFERENCES,0.30737134909596664,"Online setting
We mention works that tackle representation learning in quite different settings."
REFERENCES,0.3087621696801113,"Hao et al. (2021b) consider feature selection in sparse linear MDPs with a given exploratory distri-
bution. Zhang et al. (2021a) consider how to choose the best representation among correct repre-
sentations inspired by Papini et al. (2021) (i.e., the MDP is a linear MDP under any representation
in the function class Φ). Thus, it still falls into the linear function approximation setting. In contrast,
we only assume the MDP is linear under some unknown φ⋆∈Φ."
REFERENCES,0.3101529902642559,"Ofﬂine setting
In addition to the two work we mentioned, the pessimistic approach in ofﬂine RL
has been extensively investigated. Empirically, it can work on simulation control tasks (Kidambi
et al., 2020; Yu et al., 2020; Kumar et al., 2020; Liu et al., 2020; Chang et al., 2021). On the theo-
retical side, pessimism allows us to obtain the PAC guarantee on various models when a comparator
policy is covered by ofﬂine data in some forms (Jin et al., 2020b; Rashidinejad et al., 2021; Yin et al.,
2021; Zanette et al., 2021b; Zhang et al., 2021b; Chang et al., 2021). However, these algorithms and
their analysis rely on a known representation and linear function approximation."
REFERENCES,0.31154381084840055,"We mention works that tackle representation learning from different viewpoints and settings. Lu
et al. (2021) consider multitask representation learning under a generative model assumption. Hao
et al. (2021a) study the feature selection problem in sparse linear MDPs and Ni et al. (2021) study
dimensionality reduction in a given kernel space, under the assumption that the ofﬂine data admits
some form of full coverage condition. Shah et al. (2020) studies learning on the assumption that the
optimal Q-function admits a low-rank structure on the generative model setting."
REFERENCES,0.3129346314325452,"B
PROOF OF THE THEORETICAL PROPERTY OF REP-UCB"
REFERENCES,0.3143254520166898,"Notation
We summarize the notations we frequently use.
First of all, hereafter, we assume
c0, c1, · · · , are some universal constants, and the notation"
REFERENCES,0.3157162726008345,"f(1/(1 −γ), |A|, ln(1/δ), ln(|M|), d, n) ≲g(1/(1 −γ), |A|, ln(1/δ), ln(|M|), d, n)"
REFERENCES,0.31710709318497915,"means there exists some constant c1 > 0, such that"
REFERENCES,0.3184979137691238,"f(1/(1 −γ), |A|, ln(1/δ), ln(|M|), d, n) ≤c1g(1/(1 −γ), |A|, ln(1/δ), ln(|M|), d, n)"
REFERENCES,0.3198887343532684,"for any 0 ≤γ < 1, |A|, ln(1/δ), ln(|M|), d, n."
REFERENCES,0.32127955493741306,We deﬁne
REFERENCES,0.3226703755215577,"ρn(s) := 1 n n−1
X"
REFERENCES,0.3240611961057024,"i=0
dπi
P ⋆(s)."
REFERENCES,0.325452016689847,"With slight abuse of notation, we overload the above notation and use ρn for 1/n Pn−1
i=0 dπi
P ⋆(s, a).
Next, deﬁne ρ′
n ∈[S →R] as a marginal distribution of s′ for a triple"
REFERENCES,0.32684283727399166,"(s, a, s′) ∼ρn(s)U(a)P ⋆(s′ | s, a)."
REFERENCES,0.3282336578581363,We deﬁne three matrices as follows:
REFERENCES,0.32962447844228093,"Σρn×U(A),φ = nEs∼ρn,a∼U(A)[φ(s, a)φ⊤(s, a)] + λnI,"
REFERENCES,0.33101529902642557,"Σρn,φ = nE(s,a)∼ρn[φ(s, a)φ⊤(s, a)] + λnI,
ˆΣn,φ = nE(s,a)∼Dn[φφ⊤] + λnI."
REFERENCES,0.33240611961057026,"Note that for a ﬁxed φ, ˆΣn,φ is an unbiased estimate of Σρn×U(A),φ."
REFERENCES,0.3337969401947149,Published as a conference paper at ICLR 2022
REFERENCES,0.33518776077885953,"Optimism
First, we prove the optimism at the initial distribution. This is proved by using a sim-
ulation lemma inside the learned model which is important since both the bonus and the learned
model use ˆφ. In high level, we will show that the expected bonus Es,a∼dπ
ˆ
Pn
ˆbn(s, a) is in the same"
REFERENCES,0.33657858136300417,"order of the expected model error Es,a∼dπ
ˆ
Pn∥ˆPn(·|s, a) −P ⋆(·|s, a)∥1. Note that the expectation is
with respect to dπ
ˆ
Pn."
REFERENCES,0.3379694019471488,"Lemma 7 (Almost Optimism at the Initial Distribution). Consider an episode n (1 ≤n ≤N) and
set"
REFERENCES,0.33936022253129344,"αn = O(
p"
REFERENCES,0.34075104311543813,"(|A| + d2) γ ln(|M|n/δ)),
λn = O (d ln(|M|n/δ)) , ζn = O
ln(|M|n/δ) n 
."
REFERENCES,0.34214186369958277,"With probability 1 −δ, we have"
REFERENCES,0.3435326842837274,"∀n ∈[1, · · · , N], ∀π ∈Π, V π
ˆ
Pn,r+ˆbn −V π
P ⋆,r ≥−
p"
REFERENCES,0.34492350486787204,(1 −γ)−1|A|ζn.
REFERENCES,0.3463143254520167,"Proof. In this proof, letting fn(s, a) = ∥ˆPn(· | s, a) −P ⋆(· | s, a)∥1, we condition on the event"
REFERENCES,0.3477051460361613,"∀n,
Es∼ρn,a∼U(A)[f 2
n(s, a)] ≤ζn,
Es∼ρ′n,a∼U(A)[f 2
n(s, a)] ≤ζn,"
REFERENCES,0.34909596662030595,"∀n, ∀φ, ∥φ(s, a)∥ˆΣ−1
n ,φ = Θ(∥φ(s, a)∥Σ−1
ρn×U(A),φ)."
REFERENCES,0.35048678720445064,"From Lemma 10 and Lemma 17, this event happens with probability 1 −δ. Then, for any policy π,
from simulation lemma 20,"
REFERENCES,0.3518776077885953,"(1 −γ)(V π
ˆ
Pn,r+ˆbn −V π
P ⋆,r)"
REFERENCES,0.3532684283727399,"= E(s,a)∼dπ
ˆ
Pn"
REFERENCES,0.35465924895688455,"h
ˆbn(s, a) + γEs′∼ˆ
Pn(s,a)

V π
P ⋆,r(s′)

−γEs′∼P ⋆(s,a)

V π
P ⋆,r(s′)
i"
REFERENCES,0.3560500695410292,"≳E(s,a)∼dπ
ˆ
Pn"
REFERENCES,0.3574408901251738,"
min

αn∥ˆφn(s, a)∥Σ−1
ρn×U(A), ˆ
φn, 2

+ γEs′∼ˆ
Pn(s,a)

V π
P ⋆,r(s′)

−γEs′∼P ⋆(s,a)

V π
P ⋆,r(s′)
 (3)"
REFERENCES,0.3588317107093185,"where in the last step, we replaced the empirical covariance by the population covariance. Note the
notation ≲is up to universal constants. Here, since ∥V π
P ⋆,r∥∞≤1 (since we assume trajectory-wise
total reward is normalized between [0, 1]), we have:
E(s,a)∼dπ
ˆ
Pn"
REFERENCES,0.36022253129346316,"n
Es′∼ˆ
Pn(s,a)

V π
P ⋆,r(s′)

−Es′∼P ⋆(s,a)

V π
P ⋆,r(s′)
o ≤E(s,a)∼dπ
ˆ
Pn {fn(s, a)} ."
REFERENCES,0.3616133518776078,The above is further bounded by Lemma 11:
REFERENCES,0.36300417246175243,"|E(s,a)∼dπ
ˆ
Pn {fn(s, a)} | ≤E(˜s,˜a)∼dπ
ˆ
Pn∥ˆφn(˜s, ˜a)∥Σ−1
ρn×U(A), ˆ
φn
√γ
q
n|A|Es∼ρ′n,a∼U(A) [f 2n(s, a)]
	
+ 4λnd + 4nζn +
q"
REFERENCES,0.36439499304589706,"(1 −γ)|A|Es∼ρn,a∼U(A) [f 2n(s, a)]. Then,"
REFERENCES,0.3657858136300417,"E(s,a)∼dπ
ˆ
Pn {fn(s, a)} ≲
p"
REFERENCES,0.3671766342141864,"α′nE(˜s,˜a)∼dπ
ˆ
Pn∥ˆφn(˜s, ˜a)∥Σ−1
ρn×U(A), ˆ
φn +
p"
REFERENCES,0.36856745479833103,"|A|ζn(1 −γ).
(4) where"
REFERENCES,0.36995827538247567,"α′
n = γ{n|A|ζn + λnd + nζn} ≲γ
 
|A| + d2
ln(|M|n/δ)."
REFERENCES,0.3713490959666203,"Note we here use fn(s, a) ≤2, Es∼ρn,a∼U(A)[fn(s, a)2] ≤ζn and Es∼ρ′
n,a∼U(A)[fn(s, a)2] ≤ζn."
REFERENCES,0.37273991655076494,"Combining all things together,
E(s,a)∼dπ
ˆ
Pn"
REFERENCES,0.3741307371349096,"n
Es′∼ˆ
Pn(s,a)

V π
P ⋆,r(s′)

−Es′∼P ⋆(s,a)

V π
P ⋆,r(s′)
o ≤2E(s,a)∼dπ
ˆ
Pn {fn(s, a)} ≲
p"
REFERENCES,0.37552155771905427,"α′nE(˜s,˜a)∼dπ
ˆ
Pn∥ˆφn(˜s, ˜a)∥Σ−1
ρn×U(A), ˆ
φn +
p"
REFERENCES,0.3769123783031989,(1 −γ)|A|ζn
REFERENCES,0.37830319888734354,"≤αnE(˜s,˜a)∼dπ
ˆ
Pn∥ˆφn(˜s, ˜a)∥Σ−1
ρn×U(A), ˆ
φn +
p"
REFERENCES,0.3796940194714882,"(1 −γ)|A|ζn,
where αn :=
p"
REFERENCES,0.3810848400556328,"α′n.
(5)"
REFERENCES,0.38247566063977745,Published as a conference paper at ICLR 2022
REFERENCES,0.38386648122392214,"Going back to (3), we have
(1 −γ)(V π
ˆ
Pn,r+ˆbn −V π
P ⋆,r)"
REFERENCES,0.3852573018080668,"≳E(s,a)∼dπ
ˆ
Pn"
REFERENCES,0.3866481223922114,"
min

αn∥ˆφn(s, a)∥Σ−1
ρn×U(A), ˆ
φn, 2

+ γEs′∼ˆ
Pn(s,a)

V π
P ⋆,r(s′)

−γEs′∼P ⋆(s,a)

V π
P ⋆,r(s′)
"
REFERENCES,0.38803894297635605,"≥E(s,a)∼dπ
ˆ
Pn"
REFERENCES,0.3894297635605007,"
min

αn∥ˆφn(s, a)∥Σ−1
ρn×U(A), ˆ
φn, 2

−min

αn∥ˆφn(s, a)∥Σ−1
ρn×U(A), ˆ
φn +
p"
REFERENCES,0.3908205841446453,"(1 −γ)|A|ζn, 2
 ≥−
p"
REFERENCES,0.39221140472878996,(1 −γ)|A|ζn.
REFERENCES,0.39360222531293465,"From the second line to the third line, we again use ∥V π
P ⋆,r∥∞= O(1) and (4). This concludes the
proof."
REFERENCES,0.3949930458970793,"Next, we obtain the upper bound of PN
n=0 V π⋆
P ⋆,r −V πn
P ⋆,r. Recall π⋆is the optimal policy. Though
this form is the same as a standard regret form, since we are not exactly deploying πn in episode n
(recall that we play a uniform action at the end of the episode), we cannot get the regret guarantee.
However, it sufﬁces for the PAC guarantee.
Lemma 8 (Regret). With probability 1 −δ, we have N
X"
REFERENCES,0.3963838664812239,"n=1
V π⋆
P ⋆,r −V πn
P ⋆,r ≲ s"
REFERENCES,0.39777468706536856,"N ln

1 + N λ1"
REFERENCES,0.3991655076495132,"
ln(N|M|/δ) |A|d2"
REFERENCES,0.40055632823365783,(1 −γ).
REFERENCES,0.4019471488178025,"Proof. Similar to Lemma 7, letting fn(s, a) = ∥ˆPn(· | s, a) −P ⋆(· | s, a)∥1, we condition on the
event
∀n,
Es∼ρn,a∼U(A)[f 2
n(s, a)] ≤ζn,
∀φ, ∥φ(s, a)∥ˆΣ−1
n ,φ = Θ(∥φ(s, a)∥Σ−1
ρn×U,φ).
(6)"
REFERENCES,0.40333796940194716,"From Lemma 10 and Lemma 17, this event happens with probability 1 −δ."
REFERENCES,0.4047287899860918,"For any ﬁxed episode n and any policy π, we have"
REFERENCES,0.40611961057023643,"V π⋆
P ⋆,r −V πn
P ⋆,r"
REFERENCES,0.40751043115438107,"≤V π⋆
ˆ
Pn,r+ˆbn −V πn
P ⋆,r +
p"
REFERENCES,0.4089012517385257,"|A|ζn(1 −γ)−1
(Lemma 7)"
REFERENCES,0.4102920723226704,"≤V πn
ˆ
Pn,r+ˆbn −V πn
P ⋆,r +
p"
REFERENCES,0.41168289290681503,"|A|ζn(1 −γ)−1
(πn = arg maxπ V π
ˆ
Pn,r+ˆbn)"
REFERENCES,0.41307371349095967,"= (1 −γ)−1E(s,a)∼dπn
P ⋆[ˆbn(s, a) + γE ˆ
Pn(s′|s,a)[V πn
ˆ
Pn,r+ˆbn(s′)] −γEP ⋆(s′|s,a)[V πn
ˆ
Pn,r+ˆbn(s′)]] +
p"
REFERENCES,0.4144645340751043,|A|ζn(1 −γ)−1.
REFERENCES,0.41585535465924894,We use the 2nd form of simulation Lemma 20 in the last display.
REFERENCES,0.4172461752433936,"Then, noting ∥ˆbn∥∞≤2, we have ∥V πn
ˆ
Pn,r+ˆbn∥∞≤2/(1 −γ). Combining this fact with the above
expansion, we have"
REFERENCES,0.41863699582753827,"(V π⋆
P ⋆,r −V πn
P ⋆,r)"
REFERENCES,0.4200278164116829,"≤(1 −γ)−1 E(s,a)∼dπn
P ⋆[ˆbn(s, a)]
|
{z
}
(a)"
REFERENCES,0.42141863699582754,"+

2
(1 −γ)2"
REFERENCES,0.4228094575799722,"
E(s,a)∼dπn
P ⋆[fn(s, a)]
|
{z
}
(b) +
p"
REFERENCES,0.4242002781641168,|A|ζn(1 −γ)−1. (7)
REFERENCES,0.42559109874826145,"First, we calculate the ﬁrst term (a) in Inequality 7. Following Lemma 12 and noting the bonus ˆbn
is O(1), we have"
REFERENCES,0.42698191933240615,"E(s,a)∼dπn
P ⋆
h
ˆbn(s, a)
i"
REFERENCES,0.4283727399165508,"≲E(s,a)∼dπn
P ⋆"
REFERENCES,0.4297635605006954,"
min

αn∥ˆφn(s, a)∥Σ−1
ρn×U(A), ˆ
φn, 2

( From (6))"
REFERENCES,0.43115438108484005,"≲E(˜s,˜a)∼dπn
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆ s"
REFERENCES,0.4325452016689847,"nγ|A|α2nEs∼ρn,a∼U(A)"
REFERENCES,0.4339360222531293,"
∥ˆφn(s, a)∥2
Σ−1
ρn×U(A), ˆ
φn"
REFERENCES,0.43532684283727396,"
+ dγλn + s"
REFERENCES,0.43671766342141866,"|A|α2nEs∼ρn,a∼U(A)"
REFERENCES,0.4381084840055633,"
∥ˆφn(s, a)∥2
Σ−1
ρn×U(A), ˆ
φn"
REFERENCES,0.43949930458970793,"
(1 −γ)."
REFERENCES,0.44089012517385257,Published as a conference paper at ICLR 2022
REFERENCES,0.4422809457579972,"Note that we use the fact that B = 2 when applying Lemma 12. In addition, we have"
REFERENCES,0.44367176634214184,"nEs∼ρn,a∼U(A)"
REFERENCES,0.44506258692628653,"
∥ˆφn(s, a)∥2
Σ−1
ρn×U(A), ˆ
φn"
REFERENCES,0.44645340751043117,"
= n Tr(Eρn×U(A)[ˆφn ˆφ⊤
n ]{nEρn×U(A)[ˆφn ˆφ⊤
n ] + λnI}−1) ≤d. Then,"
REFERENCES,0.4478442280945758,"E(s,a)∼dπn
P ⋆
h
ˆbn(s, a)
i
≤E(˜s,˜a)∼dπn
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆
p"
REFERENCES,0.44923504867872044,"γd|A|α2n + γdλn +
p"
REFERENCES,0.4506258692628651,d|A|α2n(1 −γ)/n.
REFERENCES,0.4520166898470097,"Second, we calculate the term (b) in inequality 7. Following Lemma 12 and noting f 2
n(s, a) is
upper-bounded by 4 (i.e., B = 4 in Lemma 12), we have"
REFERENCES,0.4534075104311544,"E(s,a)∼dπn
P ⋆[fn(s, a)]"
REFERENCES,0.45479833101529904,"≤E(˜s,˜a)∼dπn
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆"
REFERENCES,0.4561891515994437,"q
n|A|γEs∼ρn,a∼U(A) [f 2n(s, a)]
	
+ 4γλnd +
q"
REFERENCES,0.4575799721835883,"|A|Es∼ρn,a∼U(A) [f 2n(s, a)(1 −γ)]"
REFERENCES,0.45897079276773295,"≤E(˜s,˜a)∼dπn
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆
p"
REFERENCES,0.4603616133518776,"n|A|γζn + 4γλnd +
p"
REFERENCES,0.4617524339360223,|A|ζn(1 −γ)
REFERENCES,0.4631432545201669,"≤E(˜s,˜a)∼dπn
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆αn +
p"
REFERENCES,0.46453407510431155,"|A|ζn(1 −γ),"
REFERENCES,0.4659248956884562,"where in the second inequality, we use Es∼ρn,a∼U(A)[f 2
n(s, a)] ≤ζn, and in the last line, recall
√γ
p"
REFERENCES,0.4673157162726008,n|A|ζn + λnd + nζn ≲αn.
REFERENCES,0.46870653685674546,"Then, by combining the above calculation of the term (a) and term (b) in inequality 7, we have:"
REFERENCES,0.47009735744089015,"V π⋆
P ⋆,r −V πn
P ⋆,r ≲
1
(1 −γ) "
REFERENCES,0.4714881780250348,"E(˜s,˜a)∼dπn
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆
p"
REFERENCES,0.4728789986091794,d|A|α2n + dλn + r
REFERENCES,0.47426981919332406,d|A|α2n(1 −γ) n !
REFERENCES,0.4756606397774687,"+
1
(1 −γ)2"
REFERENCES,0.47705146036161333,"
E(˜s,˜a)∼dπn
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆αn +
p"
REFERENCES,0.47844228094575797,"|A|ζn(1 −γ)

."
REFERENCES,0.47983310152990266,"Hereafter, we take the dominating term out. First, recall"
REFERENCES,0.4812239221140473,"αn ≲
p"
REFERENCES,0.48261474269819193,"{|A| + d2} ln(N|M|/δ)) ≲
p"
REFERENCES,0.48400556328233657,|A|d2 ln(N|M|/δ).
REFERENCES,0.4853963838664812,"Second, we also use N
X"
REFERENCES,0.48678720445062584,"n=1
E(˜s,˜a)∼dπn
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆≤"
REFERENCES,0.48817802503477054,"v
u
u
tN N
X"
REFERENCES,0.48956884561891517,"n=1
E(˜s,˜a)∼dπn
P ⋆[φ⋆(˜s, ˜a)⊤Σ−1
ρn,φ⋆φ⋆(˜s, ˜a)]"
REFERENCES,0.4909596662030598,(CS inequality) ≲
REFERENCES,0.49235048678720444,"v
u
u
tN "
REFERENCES,0.4937413073713491,"ln det( N
X"
REFERENCES,0.4951321279554937,"n=1
E(˜s,˜a)∼dπn
P ⋆[φ⋆(˜s, ˜a)φ⋆(˜s, ˜a)⊤]) −ln det(λ1I) !"
REFERENCES,0.4965229485396384,(Lemma 18 and λ1 ≤· · · ≤λN) ≤ s
REFERENCES,0.49791376912378305,"dN ln

1 + N dλ1 
."
REFERENCES,0.4993045897079277,"(Potential function bound, Lemma 19 noting ∥φ⋆(s, a)∥2 ≤1 for any (s, a).)"
REFERENCES,0.5006954102920723,Published as a conference paper at ICLR 2022
REFERENCES,0.502086230876217,"Finally, N
X"
REFERENCES,0.5034770514603616,"n=1
V π
P ⋆,r −V πn
P ⋆,r ≲
1
(1 −γ) s"
REFERENCES,0.5048678720445062,"dN ln

1 + N dλ1 q"
REFERENCES,0.5062586926286509,"d|A|α2
N + dλN + N
X n=1 r"
REFERENCES,0.5076495132127955,d|A|α2n(1 −γ) n !
REFERENCES,0.5090403337969402,"+
1
(1 −γ)2 s"
REFERENCES,0.5104311543810849,"dN ln

1 + N dλ1"
REFERENCES,0.5118219749652295,"
αN + N
X n=1 p"
REFERENCES,0.5132127955493742,|A|ζn(1 −γ) !
REFERENCES,0.5146036161335188,"≲
1
(1 −γ) s"
REFERENCES,0.5159944367176634,"dN ln

1 + N dλ1 q"
REFERENCES,0.5173852573018081,"d|A|α2
N +
1
(1 −γ)2 s"
REFERENCES,0.5187760778859527,"dN ln

1 + N dλ1 
αN"
REFERENCES,0.5201668984700973,(Some algebra. We take the dominating term out.) ≲ s
REFERENCES,0.521557719054242,"dN ln

1 + N dλ1"
REFERENCES,0.5229485396383866,|A|d3/2 ln1/2(N|M|/δ)
REFERENCES,0.5243393602225312,"(1 −γ)2
."
REFERENCES,0.5257301808066759,This concludes the proof.
REFERENCES,0.5271210013908206,"Using Lemma 8, we can immediately obtain the PAC guarantee."
REFERENCES,0.5285118219749653,"Theorem 9 (PAC guarantee of REP-UCB). By interacting with the environment for a number of
steps at most"
REFERENCES,0.5299026425591099,"N log(N/δ),
N := O
d4|A|2 ln(|M|/δ)"
REFERENCES,0.5312934631432545,"(1 −γ)5ϵ2
ln2

1 + d4|A|2 ln(|M|/δ)"
REFERENCES,0.5326842837273992,"(1 −γ)5ϵ2 
."
REFERENCES,0.5340751043115438,"with probability 1 −δ, we can ensure V π⋆
P ⋆,r −V ˆπ
P ⋆,r ≤ϵ."
REFERENCES,0.5354659248956884,"Proof. From Lemma 8 and Lemma 22, when N is"
REFERENCES,0.5368567454798331,"O
d4|A|2 ln(|M|/δ)"
REFERENCES,0.5382475660639777,"(1 −γ)4ϵ2
ln2

1 + d4|A|2 ln(|M|/δ)"
REFERENCES,0.5396383866481224,"(1 −γ)4ϵ2 
,"
REFERENCES,0.541029207232267,"with probability 1 −δ, we can ensure"
N,0.5424200278164116,"1
N N
X"
N,0.5438108484005564,"n=1
V π⋆
P ⋆,r −V πn
P ⋆,r ≤ϵ."
N,0.545201668984701,"With probability 1−δ, we need (1−γ)−1 ln(1/δ) interactions with the environment to get one tuple
(s, a, s′, a′, ˜s) from one roll-in of π. Thus, the total sample complexity is O(N(1 −γ)−1 ln(N/δ))."
N,0.5465924895688457,"Next, we provide an important lemma to ensure the concentration of the bonus term. The version
for ﬁxed φ is proved in Zanette et al. (2021a, Lemma 39). Here, we take a union bound over the
whole feature φ ∈Φ. Recall"
N,0.5479833101529903,"ρn(·) = 1 n n−1
X"
N,0.5493741307371349,"i=0
dπi
P ⋆(·)."
N,0.5507649513212796,Lemma 10 (Concentration of the bonus term). Set λn = Θ(d ln(n|Φ|/δ)) for any n. Deﬁne
N,0.5521557719054242,"Σρn,φ = nEs∼ρn,a∼U(A)[φ(s, a)φ⊤(s, a)] + λnI,
ˆΣn,φ = n−1
X"
N,0.5535465924895688,"i=0
φ(s(i), a(i))φ⊤(s(i), a(i)) + λnI."
N,0.5549374130737135,"With probability 1 −δ, we have"
N,0.5563282336578581,"∀n ∈N+, ∀φ ∈Φ, c1∥φ(s, a)∥Σ−1
ρn×U(A),φ ≤∥φ(s, a)∥ˆΣ−1
n,φ ≤c2∥φ(s, a)∥Σ−1
ρn×U(A),φ."
N,0.5577190542420027,Published as a conference paper at ICLR 2022
N,0.5591098748261474,"For any g ∈S × A →R, The next lemma shows that E(s,a)∼dπ
ˆ
Pn {g(s, a)} can be upper-bounded"
N,0.5605006954102921,"using E(˜s,˜a)∼dπ
ˆ
Pn∥ˆφn(˜s, ˜a)∥Σ−1
ρn, ˆ
φn as long as we have the convergence guarantee for"
N,0.5618915159944368,"Es∼ρn,a∼U(A)

g2(s, a)

and Es∼ρ′n,a∼U(A)

g2(s, a)

.
Lemma 11 (One-step back inequality for the learned model). Take any g ∈S × A →R such that
∥g∥∞≤B. We condition on the event where the MLE guarantee (17):
Es∼ρn,a∼U(A)[fn(s, a)] ≲ζn,
holds. Then, for any policy π, we have
|E(s,a)∼dπ
ˆ
Pn {g(s, a)} |"
N,0.5632823365785814,"≤E(˜s,˜a)∼dπ
ˆ
Pn∥ˆφn(˜s, ˜a)∥Σ−1
ρn×U(A), ˆ
φn"
N,0.564673157162726,"q
n|A|Es∼ρ′n,a∼U(A) [g2(s, a)]
	
+ B2λnd + nB2ζn +
q"
N,0.5660639777468707,"(1 −γ)|A|Es∼ρn,a∼U(A) [g2(s, a)]."
N,0.5674547983310153,"Recall Σρn×U(A), ˆφn = nEs∼ρn,a∼U(A)[ˆφn(s, a)ˆφ⊤
n (s, a)] + λnI."
N,0.56884561891516,"Proof. First, we have an equality:
E(s,a)∼dπ
ˆ
Pn {g(s, a)} = γE(˜s,˜a)∼dπ
ˆ
Pn,s∼ˆ
Pn(˜s,˜a),a∼π(s) {g(s, a)} + (1 −γ)Es∼d0,a∼π(s0) {g(s, a)} , (8)"
N,0.5702364394993046,The second term in (8) is upper-bounded by
N,0.5716272600834492,(1 −γ) s
N,0.5730180806675939,"max
(s,a)
d0(s)π(a | s)"
N,0.5744089012517385,"ρn(s)u(a) Es∼ρn,a∼U(A) [g2(s, a)]"
N,0.5757997218358831,≤(1 −γ) s
N,0.5771905424200278,"max
(s,a)
d0(s)π(a | s)
(1 −γ)d0(s)u(a)Es∼ρn,a∼U(A) [g2(s, a)] ≤
q"
N,0.5785813630041725,"(1 −γ)|A|Es∼ρn,a∼U(A) [g2(s, a)]."
N,0.5799721835883171,"Next we consider the ﬁrst term in (8). By CS inequality, we have"
N,0.5813630041724618,"E(˜s,˜a)∼dπ
ˆ
Pn,s∼ˆ
Pn(˜s,˜a),a∼π(s) {g(s, a)} = E(˜s,˜a)∼dπ
ˆ
Pn
ˆφn(˜s, ˜a)⊤
Z X"
N,0.5827538247566064,"a
ˆµn(s)π(a | s)g(s, a)d(s)"
N,0.5841446453407511,"≤E(˜s,˜a)∼dπ
ˆ
Pn∥ˆφn(˜s, ˜a)∥Σ−1
ρn×U(A), ˆ
φn  Z X"
N,0.5855354659248957,"a
ˆµn(s)π(a | s)g(s, a)d(s)"
N,0.5869262865090403,"Σρn×U(A), ˆ
φn . Then, ∥
Z X"
N,0.588317107093185,"a
ˆµn(s)π(a | s)g(s, a)d(s)∥2
Σρn×U(A), ˆ
φn ≤ (Z X"
N,0.5897079276773296,"a
ˆµn(s)π(a | s)g(s, a)d(s)"
N,0.5910987482614742,")⊤n
nEs∼ρn,a∼U(A)[ˆφn ˆφ⊤
n ] + λnI
o (Z X"
N,0.5924895688456189,"a
ˆµn(s)π(a | s)g(s, a)d(s) )"
N,0.5938803894297635,"≤nE˜s∼ρn,˜a∼U(A) 
  ""Z X"
N,0.5952712100139083,"a
ˆµn(s)⊤ˆφn(˜s, ˜a)π(a | s)g(s, a)d(s) #2
"
N,0.5966620305980529,+ B2λnd
N,0.5980528511821975,(Use the assumption ∥P
N,0.5994436717663422,"a π(a | s)g(s, a)∥∞≤B and
R
∥ˆµn(s)h(s)d(s)∥2 ≤
√"
N,0.6008344923504868,"d for any h : S →[0, 1].)"
N,0.6022253129346314,"= nE˜s∼ρn,˜a∼U(A)"
N,0.6036161335187761,"n
Es∼ˆ
Pn(˜s,˜a),a∼π(s) [g(s, a)]
o2
+ B2λnd"
N,0.6050069541029207,"≤nEs∼ρn,a∼U(A)
h
Es∼P ⋆(˜s,˜a),a∼π(s) [g(s, a)]
	2i
+ B2λnd + nB2ζn
(MLE guarantee)"
N,0.6063977746870653,"≤nE˜s∼ρn,˜a∼U(A),s∼P ⋆(˜s,˜a),a∼π(s)

g2(s, a)

+ B2λnd + B2nζn.
(Jensen)"
N,0.60778859527121,"≤n|A|

E˜s∼ρn,˜a∼U(A),s∼P ⋆(˜s,˜a),a∼U(A)

g2(s, a)
	
+ B2λnd + B2nζn
(Importance sampling)"
N,0.6091794158553546,"≤n|A|Es∼ρ′
n,a∼U(A)

g2(s, a)

+ B2λnd + B2nζn.
(Deﬁnition of ρ′
n)"
N,0.6105702364394993,Published as a conference paper at ICLR 2022
N,0.6119610570236439,"Then, the ﬁnal statement is immediately concluded."
N,0.6133518776077886,"Below, we show a similar lemma as Lemma 11.
The difference is we aim for calculating
E(s,a)∼dπ
P ⋆{g(s, a)} instead of E(s,a)∼dπ
ˆ
Pn {g(s, a)} . For any g ∈S × A →R, this lemma shows
that E(s,a)∼dπ
P ⋆{g(s, a)} can be upper-bounded using E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn, ˆ
φ⋆as long as we"
N,0.6147426981919333,"have the convergence guarantee for Es∼ρn,a∼U(A)

g2(s, a)

. Note comparing to Lemma 11, this
is not a probabilistic statement. Note that ∥φ⋆(s, a)∥Σ−1
ρn,φ⋆is the usual elliptical potential function
under the ﬁxed representation φ⋆.
Lemma 12 (One-step back inequality for the true model ). Take any g ∈S × A →R such that
∥g∥∞≤B. Then,"
N,0.6161335187760779,"E(s,a)∼dπ
P ⋆{g(s, a)} ≤E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆
√γ
q"
N,0.6175243393602226,"n|A|Es∼ρn,a∼U(A) [g2(s, a)] + λndB2 +
q"
N,0.6189151599443672,"(1 −γ)|A|Es∼ρn,a∼U(A) [g2(s, a)]."
N,0.6203059805285118,"Recall Σρn,φ⋆= nE(s,a)∼ρn[φ⋆(s, a)φ⋆(s, a)⊤] + λnI."
N,0.6216968011126565,"Proof. First, we have"
N,0.6230876216968011,"E(s,a)∼dπ
P ⋆{g(s, a)} = γE(˜s,˜a)∼dπ
P ⋆,s∼P ⋆(˜s,˜a),a∼π(s) {g(s, a)} + (1 −γ)Es∼d0,a∼π(s0) {g(s, a)} .
(9)"
N,0.6244784422809457,The second term in (9) is upper-bounded by
N,0.6258692628650904,(1 −γ) s
N,0.627260083449235,"max
(s,a)
d0(s)π(a | s)"
N,0.6286509040333796,"ρn(s)u(a) Es∼ρn,a∼U(A) [g2(s, a)] ≤
q"
N,0.6300417246175244,"|A|Es∼ρn,a∼U(A) [g2(s, a)] (1 −γ)."
N,0.631432545201669,"By CS inequality, the ﬁrst term in (9) is further bounded as follows:"
N,0.6328233657858137,"E(˜s,˜a)∼dπ
P ⋆,s∼P ⋆(˜s,˜a),a∼π(s) {g(s, a)} = E(˜s,˜a)∼dπ
P ⋆φ⋆(˜s, ˜a)⊤
Z X"
N,0.6342141863699583,"a
µ⋆(s)π(a | s)g(s, a)d(s)"
N,0.6356050069541029,"≤E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆  Z X"
N,0.6369958275382476,"a
µ⋆(s)π(a | s)g(s, a)d(s)"
N,0.6383866481223922,"Σρn,φ⋆
."
N,0.6397774687065368,"Here, we have ∥
Z X"
N,0.6411682892906815,"a
µ⋆(s)π(a | s)g(s, a)d(s)∥2
Σρn,φ⋆ ≤ (Z X"
N,0.6425591098748261,"a
µ⋆(s)π(a | s)g(s, a)d(s)"
N,0.6439499304589708,")⊤
nE(s,a)∼ρn[φ⋆(s, a){φ⋆(s, a)}⊤] + λnI
	
(Z X"
N,0.6453407510431154,"a
µ⋆(s)π(a | s)g(s, a)d(s) )"
N,0.6467315716272601,"≤nE(˜s,˜a)∼ρn 
  ""Z X"
N,0.6481223922114048,"a
µ⋆(s)⊤φ⋆(˜s, ˜a)π(a | s)g(s, a)d(s) #2
"
N,0.6495132127955494,+ λndB2
N,0.650904033379694,"≤n

E(˜s,˜a)∼ρn,s∼P ⋆(˜s,˜a),a∼π(s)

g2(s, a)
	
+ λndB2.
(Jensen)"
N,0.6522948539638387,"Therefore,"
N,0.6536856745479833,"n

E(˜s,˜a)∼ρn,s∼P ⋆(˜s,˜a),a∼π(s)

g2(s, a)
	
+ λndB"
N,0.655076495132128,"≤n|A|

E(˜s,˜a)∼ρn,s∼P ⋆(˜s,˜a),a∼U(A)

g2(s, a)
	
+ λndB2
(Importance sampling)"
N,0.6564673157162726,"≤n|A|
 1"
N,0.6578581363004172,"γ Es∼ρn,a∼U(A)

g2(s, a)

+ λndB2."
N,0.6592489568845619,Published as a conference paper at ICLR 2022
N,0.6606397774687065,"In the last line, we use the following inequality:"
N,0.6620305980528511,"Es∼ρn,a∼U(A)

g2(s, a)
"
N,0.6634214186369958,"= γE(˜s,˜a)∼ρn,s∼P ⋆(˜s,˜a),a∼U(A)

g2(s, a)

+ (1 −γ)Es0∼d0,a∼U(A)

g2(s, a)
"
N,0.6648122392211405,"≥γE(˜s,˜a)∼ρn,s∼P ⋆(˜s,˜a),a∼U(A)

g2(s, a)

."
N,0.6662030598052852,"Finally, we have"
N,0.6675938803894298,"E(s,a)∼dπ
P ⋆{g(s, a)} ≤E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρn,φ⋆
√γ
q
n|A|Es∼ρn,a∼U(A) [g2(s, a)]
	
+ λndB2 +
q"
N,0.6689847009735744,"|A|Es∼ρn,a∼U(A) [g2(s, a)] (1 −γ)."
N,0.6703755215577191,This concludes the proof.
N,0.6717663421418637,"C
PROOF OF THE THEORETICAL PROPERTY OF REP-LCB"
N,0.6731571627260083,This section provides the detailed proofs for our results in the ofﬂine setting.
N,0.674547983310153,"Below we ﬁrst prove that V π
ˆ
P ,r−ˆb is an almost pessimistic estimator of V π
P ⋆,r."
N,0.6759388038942976,"Lemma 13 (Almost Pessimism at the Initial Distribution). Let ω = maxa,s 1/πb(a | s). Set"
N,0.6773296244784422,"α = c1
p"
N,0.6787204450625869,"(ω + d2) γ ln(|M|/δ),
λ = O(d ln(|M|/δ)), ζ = O
ln(|M|/δ) n 
."
N,0.6801112656467315,"With probability 1 −δ, for any policy π, we have"
N,0.6815020862308763,"V π
ˆ
P ,r−ˆb −V π
P ⋆,r ≤ r"
N,0.6828929068150209,"ω(1 −γ)−1 ln(|M|/δ) n
."
N,0.6842837273991655,Proof. We deﬁne
N,0.6856745479833102,"Σρ,φ = nE(s,a)∼ρ[φφ⊤] + λI,
ˆΣφ = nED[φφ⊤] + λI."
N,0.6870653685674548,"where λ = O(d ln(|M|/δ)). In this proof, letting f(s, a) = ∥ˆP(· | s, a) −P ⋆(· | s, a)∥1, we
condition on the events:"
N,0.6884561891515995,"E(s,a)∼ρ[f 2(s, a)] ≤ζ,
∀φ ∈Φ : ∥φ(s, a)∥ˆΣ−1
φ
= Θ(∥φ(s, a)∥Σ−1
ρ,φ).
(10)"
N,0.6898470097357441,"where ζ = O(ln(|M|/δ)/n). From the ofﬂine version of Lemma 17 and Lemma 10 6, this event
happens with probability 1 −δ."
N,0.6912378303198887,"Then, from simulation lemma (Lemma 20),"
N,0.6926286509040334,"(1 −γ)(V π
ˆ
P ,r−ˆb −V π
P ⋆,r)"
N,0.694019471488178,"= E(s,a)∼dπ
ˆ
P"
N,0.6954102920723226,"h
−ˆb(s, a) + γEs′∼ˆ
P (s,a)

V π
P ⋆,r(s′)

−γEs′∼P ⋆(s,a)

V π
P ⋆,r(s′)
i"
N,0.6968011126564673,"≲E(s,a)∼dπ
ˆ
P"
N,0.6981919332406119,"
−min

α∥ˆφ(s, a)∥Σ−1
ρ, ˆ
φ, 2

+ γEs′∼ˆ
P (s,a)

V π
P ⋆,r(s′)

−γEs′∼P ⋆(s,a)

V π
P ⋆,r(s′)

."
N,0.6995827538247567,(From (10))
N,0.7009735744089013,"Here, we have
E(s,a)∼dπ
ˆ
P"
N,0.7023643949930459,"n
Es′∼ˆ
P (s,a)

V π
P ⋆,r(s′)

−Es′∼P ⋆(s,a)

V π
P ⋆,r(s′)
o ≤E(s,a)∼dπ
ˆ
P {f(s, a)} ,"
N,0.7037552155771906,"noting ∥V π
P ⋆,r∥∞≤1. This is further bounded by Lemma 15:"
N,0.7051460361613352,"E(s,a)∼dπ
ˆ
P {f(s, a)} ≲
√"
N,0.7065368567454798,"α′E(˜s,˜a)∼dπ
ˆ
P ∥ˆφ(˜s, ˜a)∥Σ−1
ρ, ˆ
φ +
p"
N,0.7079276773296245,"ωζ(1 −γ).
(11)"
N,0.7093184979137691,6We can remove ln n since n is ﬁxed in the ofﬂine setting.
N,0.7107093184979137,Published as a conference paper at ICLR 2022 where
N,0.7121001390820584,"α′ = nγωζ + γ2λd + γ2nζ ≲
 
ω + d2
γ ln(|M|/δ)."
N,0.713490959666203,"Here, we use f(s, a) ≤2 in Lemma 15 and E(s,a)∼ρ[f 2(s, a)] ≤ζ."
N,0.7148817802503477,"Thus,
E(s,a)∼dπ
ˆ
P"
N,0.7162726008344924,"n
Es′∼ˆ
P (s,a)

V π
P ⋆,r(s′)

−Es′∼P ⋆(s,a)

V π
P ⋆,r(s′)
o ≤E(s,a)∼dπ
ˆ
P {f(s, a)} ≤
√"
N,0.717663421418637,"α′E(˜s,˜a)∼dπ
ˆ
P ∥ˆφ(˜s, ˜a)∥Σ−1
ρ, ˆ
φ +
p"
N,0.7190542420027817,ωζ(1 −γ)
N,0.7204450625869263,"= αE(˜s,˜a)∼dπ
ˆ
P ∥ˆφ(˜s, ˜a)∥Σ−1
ρ, ˆ
φ +
p"
N,0.721835883171071,"ωζ(1 −γ),
α =
√ α′."
N,0.7232267037552156,"Going back to the simulation lemma 20, we have"
N,0.7246175243393602,"(1 −γ)(V π
ˆ
P ,r−ˆb −V π
P ⋆,r)"
N,0.7260083449235049,"≲E(s,a)∼dπ
ˆ
P"
N,0.7273991655076495,"
−min

α∥ˆφ(s, a)∥Σ−1
ρ, ˆ
φ, 2

+ Es′∼ˆ
P (s,a)

V π
P ⋆,r(s′)

−Es′∼P ⋆(s,a)

V π
P ⋆,r(s′)
"
N,0.7287899860917941,"≤E(s,a)∼dπ
ˆ
P"
N,0.7301808066759388,"
−min

α∥ˆφ(s, a)∥Σ−1
ρ, ˆ
φ, 2

+ min

α∥ˆφ(s, a)∥Σ−1
ρ, ˆ
φ +
p"
N,0.7315716272600834,"ωζ(1 −γ), 2
 ≤
p"
N,0.7329624478442281,ωζ(1 −γ).
N,0.7343532684283728,This concludes the proof.
N,0.7357440890125174,"With the above lemma, now we can proceed to prove the main theorem.
Theorem 14 (PAC guarantee of REP-LCB). Set the parameters as in Lemma 13. With probability
1 −δ, for any comparator policy π including history-dependent non-Markovian policies, we have"
N,0.7371349095966621,"V π
P ⋆,r −V ˆπ
P ⋆,r ≲
ωd2"
N,0.7385257301808067,(1 −γ)2 r
N,0.7399165507649513,"C⋆π ln(|M|/δ) n
,"
N,0.741307371349096,"where C⋆
π is the relative condition number under φ⋆:"
N,0.7426981919332406,"C⋆
π := sup
x∈R"
N,0.7440890125173852,"x⊤E(s,a)∼dπ
P ⋆[φ⋆(s, a){φ⋆(s, a)}⊤]x"
N,0.7454798331015299,"x⊤E(s,a)∼ρ[φ⋆(s, a){φ⋆(s, a)}⊤]x ."
N,0.7468706536856745,"Proof. In this proof, letting f(s, a) = ∥ˆP(· | s, a) −P ⋆(· | s, a)∥1 we condition on the events:"
N,0.7482614742698191,"E(s,a)∼ρ[f 2(s, a)] ≤ζ,
∀φ ∈Φ : ∥φ(s, a)∥ˆΣ−1
φ
= Θ(∥φ(s, a)∥Σ−1
ρ,φ).
(12)"
N,0.7496522948539638,"From Lemma 10 and Lemma 17, this event happens with probability 1 −δ."
N,0.7510431154381085,"For any policy π, we have"
N,0.7524339360222532,"V π
P ⋆,r −V ˆπ
P ⋆,r"
N,0.7538247566063978,"≤V π
P ⋆,r −V ˆπ
ˆ
P ,r−ˆb +
p"
N,0.7552155771905424,"ωζ(1 −γ)−1
(Lemma 13)"
N,0.7566063977746871,"≤V π
P ⋆,r −V π
ˆ
P ,r−ˆb +
p"
N,0.7579972183588317,ωζ(1 −γ)−1
N,0.7593880389429764,"≲(1 −γ)−1 E(s,a)∼dπ
P ⋆[ˆb(s, a)]
|
{z
}
(a)"
N,0.760778859527121,"+

1
1 −γ"
N,0.7621696801112656,"2
E(s,a)∼dπ
P ⋆[f(s, a)]
|
{z
}
(b) +
p"
N,0.7635605006954103,ωζ(1 −γ)−1.
N,0.7649513212795549,"Recall f(s, a) = ∥ˆP(· | s, a) −P ⋆(· | s, a)∥1."
N,0.7663421418636995,"From the second line to the third line, note though ˆπ is the argmax over Markoovian polices, ˆπ is
also the argmax over all history-dependent polices. In the last line, we use a simulation lemma 20,
which is tailored to a time-inhomogeneous policy. We here use ∥V π
ˆ
P ,r−ˆb∥∞≤2/((1 −γ)). noting"
N,0.7677329624478443,∥ˆb∥∞= O(1).
N,0.7691237830319889,Published as a conference paper at ICLR 2022
N,0.7705146036161336,"We further calculate the ﬁrst term (a). Considering 16 and noting ∥ˆb∥∞≤2, we have"
N,0.7719054242002782,"E(s,a)∼dπ
P ⋆[ˆb(s, a)] ≲E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆ r"
N,0.7732962447844228,"nω
n
γE(s,a)∼ρ
h
ˆb2(s, a)
io
+ γλd +
p"
N,0.7746870653685675,"ω(1 −γ){Eρ[ˆb2(s, a)]}1/2."
N,0.7760778859527121,"From (12), we have"
N,0.7774687065368567,"nE(s,a)∼ρ
h
ˆb2(s, a)
i
≤nE(s,a)∼ρ"
N,0.7788595271210014,"
min

α2∥ˆφ(s, a)∥2
Σ−1
ρ, ˆ
φ, 4

≤nE(s,a)∼ρ"
N,0.780250347705146,"
α2∥ˆφ(s, a)∥2
Σ−1
ρ, ˆ
φ  (13)"
N,0.7816411682892906,"≤Tr[nE(s,a)∼ρ[ˆφˆφ⊤]{nE(s,a)∼ρ[ˆφˆφ⊤] + λI}−1
(14)"
N,0.7830319888734353,"≤Tr[n(E(s,a)∼ρ[ˆφˆφ⊤] + λI){nE(s,a)∼ρ[ˆφˆφ⊤] + λI}−1] ≤d.
(15) Thus,"
N,0.7844228094575799,"E(s,a)∼dπ
P ⋆[ˆb(s, a)] ≤E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆
p"
N,0.7858136300417247,ωdα2γ + γλd + r
N,0.7872044506258693,"ωdα2(1 −γ) n
."
N,0.7885952712100139,"Second, we further calculate the second term (b). Considering the ofﬂine version of Lemma 12 and
noting f 2(s, a) is upper-bounded by 4,"
N,0.7899860917941586,"E(s,a)∼dπ
P ⋆[f(s, a)]"
N,0.7913769123783032,"= E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆ q"
N,0.7927677329624478,"nω

γE(s,a)∼ρ [f 2(s, a)]
	
+ 4γλd +
q"
N,0.7941585535465925,"ωE(s,a)∼ρ [f 2(s, a)] (1 −γ)"
N,0.7955493741307371,"≲E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆
p"
N,0.7969401947148818,"ω {nγζ} + γλd +
p"
N,0.7983310152990264,ωζ(1 −γ)
N,0.799721835883171,"≲E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆α +
p"
N,0.8011126564673157,ωζ(1 −γ).
N,0.8025034770514604,"In the ﬁnal line, recall
p"
N,0.803894297635605,ω {nγζ} + γλd + γnζ ≤α.
N,0.8052851182197497,"Finally, by combining the calculation of the ﬁrst term (a) and the second term (b), we have"
N,0.8066759388038943,"V π
P ⋆,r −V ˆπ
P ⋆,r ≲
1
(1 −γ)E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆
p"
N,0.808066759388039,dα2ωγ + γλd + r
N,0.8094575799721836,ωα2d(1 −γ)−1 n
N,0.8108484005563282,"+
α
(1 −γ)2 E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆+ s"
N,0.8122392211404729,"ωζ
(1 −γ)3"
N,0.8136300417246175,"Now, we use the fact E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆is upper-bounded as"
N,0.8150208623087621,"E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆≤
r"
N,0.8164116828929068,"E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥2
Σ−1
ρ,φ⋆≤
r"
N,0.8178025034770514,"C⋆E(˜s,˜a)∼ρ∥φ⋆(˜s, ˜a)∥2
Σ−1
ρ,φ⋆
(Refer to Lemma 21) ≤
p"
N,0.8191933240611962,"C⋆d/n.
(From (13))"
N,0.8205841446453408,"Finally, we have"
N,0.8219749652294854,"V π
P ⋆,r −V ˆπ
P ⋆,r"
N,0.8233657858136301,"≲(1 −γ)−1
(r C⋆d n p"
N,0.8247566063977747,"dα2ωγ + γλd +
α
(1 −γ) r C⋆d n
+ r"
N,0.8261474269819193,"ωα2d(1 −γ) n
+ s"
N,0.827538247566064,"ωζ
(1 −γ) )"
N,0.8289290681502086,"≲(1 −γ)−1
(r C⋆d n p"
N,0.8303198887343533,"dα2ωγ +
α
(1 −γ) r C⋆d n )"
N,0.8317107093184979,"(Take out two dominating terms) ≲
ωd2"
N,0.8331015299026425,(1 −γ)2 r
N,0.8344923504867872,"C⋆ln(|M|/δ) n
."
N,0.8358831710709318,Published as a conference paper at ICLR 2022
N,0.8372739916550765,"The lemma below is a key technical lemma for our proof. It shows that one can relate the expected
value of any function f(s, a) with respect to dπ
ˆ
P (i.e., inside the learned model ˆP) to the potential"
N,0.8386648122392212,"function with respect to dπ
ˆ
P , i.e., E(˜s,˜a)∼dπ
ˆ
P ∥ˆφ(˜s, ˜a)∥Σ−1
ρ, ˆ
φ. Pairing ˆφ and ˆP is important since ˆP is"
N,0.8400556328233658,"the low-rank transition model deﬁned using ˆφ. As we have seen in the above analysis, when using
the lemma below, we instantiate f(s, a) := ∥ˆP(·|s, a) −P ⋆(·|s, a)∥1.
Lemma 15 (One-step back inequality for the learned model in ofﬂine setting). Take any f ⊂S ×
A →R s.t. ∥f∥∞≤B. We condition on the event where the MLE guarantee holds:"
N,0.8414464534075105,"E(s,a)∼ρ∥ˆP(· | s, a) −P ⋆(· | s, a)∥2
1 ≲ζ."
N,0.8428372739916551,"Then, letting ω = maxs,a(1/πb(a | s)), for any policy π, we have"
N,0.8442280945757997,"|E(s,a)∼dπ
ˆ
P {f(s, a)} | ≤E(˜s,˜a)∼dπ
ˆ
P ∥ˆφ(˜s, ˜a)∥Σ−1
ρ, ˆ
φ"
N,0.8456189151599444,"q
nωE(s,a)∼ρ [f 2(s, a)]
	
+ γ2λdB2 + nγ2ζB2 +
q"
N,0.847009735744089,"ωE(s,a)∼ρ [f 2(s, a)] (1 −γ)."
N,0.8484005563282336,"Proof. First, we have an equality:"
N,0.8497913769123783,"E(s,a)∼dπ
ˆ
P {f(s, a)} = γE(˜s,˜a)∼dπ
ˆ
P ,s∼ˆ
P (˜s,˜a),a∼π(s) {f(s, a)} + (1 −γ)Es∼d0,a∼π(s0) {f(s, a)} . (16)"
N,0.8511821974965229,The second term in (16) is upper-bounded by
N,0.8525730180806675,"Es∼d0,a∼π(s0) {f(s, a)} ≤Es∼d0,a∼π(s0)

f 2(s, a)
	
}1/2 =
q"
N,0.8539638386648123,"ωE(s,a)∼ρ [f 2(s, a)] /(1 −γ)."
N,0.8553546592489569,"Next we consider the ﬁrst term in (16). By CS inequality, we have
E(˜s,˜a)∼dπ
ˆ
P ,s∼ˆ
P (˜s,˜a),a∼π(s) {f(s, a)}
 ="
N,0.8567454798331016,"E(˜s,˜a)∼dπ
ˆ
P
ˆφ(˜s, ˜a)⊤
Z X"
N,0.8581363004172462,"a
ˆµ(s)π(a | s)f(s, a)d(s) "
N,0.8595271210013908,"≤E(˜s,˜a)∼dπ
ˆ
P ∥ˆφ(˜s, ˜a)∥Σ−1
ρ, ˆ
φ∥
Z X"
N,0.8609179415855355,"a
ˆµ(s)π(a | s)f(s, a)d(s)∥Σρ, ˆ
φ. Then,"
N,0.8623087621696801,"∥
Z
ˆµ(s)π(a | s)f(s, a)d(s, a)∥2
Σρ, ˆ
φ ≤ (Z X"
N,0.8636995827538247,"a
ˆµ(s)π(a | s)f(s, a)d(s)"
N,0.8650904033379694,")⊤n
nE(s,a)∼ρ[ˆφˆφ⊤] + λI
o (Z X"
N,0.866481223922114,"a
ˆµ(s)π(a | s)f(s, a)d(s) )"
N,0.8678720445062587,"≤nE(˜s,˜a)∼ρ 
  ""Z X"
N,0.8692628650904033,"a
ˆµ(s)⊤ˆφ(˜s, ˜a)π(a | s)f(s, a)d(s) #2
"
N,0.8706536856745479,+ B2λd
N,0.8720445062586927,(Use the assumption ∥P
N,0.8734353268428373,"a f(s, a)∥∞≤B and ∥
R
ˆµ(s)h(s)d(s)∥2 ≤
√"
N,0.874826147426982,"d for h : S →[0, 1].)"
N,0.8762169680111266,"= nE(˜s,˜a)∼ρ{Es∼ˆ
P (˜s,˜a),a∼π(s) [f(s, a)]2} + B2λd"
N,0.8776077885952712,"= nE(˜s,˜a)∼ρ{Es∼P ⋆(˜s,˜a),a∼π(s) [f(s, a)]2} + B2λd + nB2ζ
(MLE guarantee and ∥Ea∼π(·)[f 2(·, a)]∥∞≤B2.)"
N,0.8789986091794159,"≤n

E(˜s,˜a)∼ρ,s∼P ⋆(˜s,˜a),a∼π(s)

f 2(s, a)
	
+ B2λd + nB2ζ.
(Jensen)"
N,0.8803894297635605,"Finally, the ﬁrst term in (16) is upper-bounded by"
N,0.8817802503477051,"n

E(˜s,˜a)∼ρ,s∼P ⋆(˜s,˜a),a∼π(s)

f 2(s, a)
	
+ λdB2 + nB2ζ"
N,0.8831710709318498,"≤nω

E(˜s,˜a)∼ρ,s∼P ⋆(˜s,˜a),a∼πb(s)

f 2(s, a)
	
+ λdB2 + nB2ζ
(Importance sampling)"
N,0.8845618915159944,"≤nω
 1"
N,0.885952712100139,"γ E(s,a)∼ρ

f 2(s, a)

+ λdB2 + nB2ζ.
(Deﬁnition of ρ)"
N,0.8873435326842837,Published as a conference paper at ICLR 2022
N,0.8887343532684284,"In the last line, we use the following equality:"
N,0.8901251738525731,"E(s,a)∼ρ

f 2(s, a)

= γE(˜s,˜a)∼ρ,s∼P ⋆(˜s,˜a),a∼πb(s)

f 2(s, a)

+ (1 −γ)Es∼d0,a∼πb

f 2(s, a)

."
N,0.8915159944367177,"Based on the above discussion, the ﬁnal statement is immediately concluded."
N,0.8929068150208623,"We can prove the similar inequality for the true model. The proof is omitted since it is quite similar
to the one of Lemma 15.
Lemma 16 (One-step back inequality for the true model in ofﬂine setting). Take any f ⊂S × A →
R s.t. ∥f∥∞≤B. Then, letting ω = maxs,a(1/πb(a | s)), for any policy π, we have"
N,0.894297635605007,"|E(s,a)∼dπ
P ⋆{f(s, a)} | ≤E(˜s,˜a)∼dπ
P ⋆∥φ⋆(˜s, ˜a)∥Σ−1
ρ,φ⋆"
N,0.8956884561891516,"q
nωE(s,a)∼ρ [f 2(s, a)]
	
+ γ2λdB2 +
q"
N,0.8970792767732962,"ωE(s,a)∼ρ [f 2(s, a)] (1 −γ)."
N,0.8984700973574409,"D
AUXILIARY LEMMAS"
N,0.8998609179415855,"First, we present the MLE guarantee. Regarding the proof, refer to Agarwal et al. (2020b, Theorem
21). Note ˆPn and ¯πn are the quantities appearing in the proposed online algorithm. We can also
immediately obtain the statement to the ofﬂine case.
Lemma 17 (MLE guarantee). For a ﬁxed episode n, with probability 1 −δ,"
N,0.9012517385257302,"Es∼{0.5ρn+0.5ρ′n},a∼U(A)[∥ˆPn(· | s, a) −P ⋆(· | s, a)∥2
1] ≲ζ,
ζ := ln(|M|/δ) n
."
N,0.9026425591098748,"As a straightforward corollary, with probability 1 −δ,"
N,0.9040333796940194,"∀n ∈N+, Es∼{0.5ρn+0.5ρ′n},a∼U(A)[∥ˆPn(· | s, a) −P ⋆(· | s, a)∥2
1] ≲0.5ζn,
ζn := ln(|M|n/δ) n
. (17)"
N,0.9054242002781642,"The following is a standard inequality to prove regret bounds for linear models. Refer to Agarwal
et al. (2020a, Lemma G.2.)
Lemma 18. Consider the following process. For n = 1, · · · , N, Mn = Mn−1+Gn with M0 = λ0I
and Gn being a positive semideﬁnite matrix with eigenvalues upper-bounded by 1. We have that:"
N,0.9068150208623088,"2 ln det(MN) −2 ln det(λ0I) ≥ N
X"
N,0.9082058414464534,"n=1
Tr(GnM −1
n−1)."
N,0.9095966620305981,Lemma 19 (Potential function lemma). Suppose Tr(Gn) ≤B2.
N,0.9109874826147427,"2 ln det(MN) −2 ln det(λ0I) ≤d ln

1 + NB2 dλ0 
."
N,0.9123783031988874,"Proof. Let σ1, · · · , σd be the set of singular values of MN recalling MN is a positive semideﬁnite
matrix. Then, by the AM-GM inequality,"
N,0.913769123783032,"ln det(MN)/ det(λ0I) = ln d
Y"
N,0.9151599443671766,"i=1
(σi/λ0) ≤ln d"
D,0.9165507649513213,"1
d d
X"
D,0.9179415855354659,"i=1
(σi/λ0)) !"
D,0.9193324061196105,Since we have P
D,0.9207232267037552,"i σi = Tr(MN) ≤dλ0 + NB2, the statement is concluded."
D,0.9221140472878998,"Lemma 20 (Simulation lemma). Given two MDPs (P ′, r+b) and (P, r), for any policy π, we have:"
D,0.9235048678720446,"V π
P ′,r+b −V π
P,r =
1
1 −γ E(s,a)∼dπ
P ′[b(s, a) + γEP ′(s′|s,a)[Qπ
P,r(s′, π)] −γEP (s′|s,a)[Qπ
P,r(s′, π)]] and"
D,0.9248956884561892,"V π
P ′,r+b −V π
P,r =
1
1 −γ E(s,a)∼dπ
P [b(s, a) + γEP ′(s′|s,a)[Qπ
P,r+b(s′, π)] −γEP (s′|s,a)[Qπ
P ′,r+b(s′, π)]]."
D,0.9262865090403338,Published as a conference paper at ICLR 2022
D,0.9276773296244785,Proof. We use
D,0.9290681502086231,"V π
P −f(s0, π) =
1
1 −γ Edπ
P [r(s, a) + γEP (s′|s,a)[f(s′, π)] −f(s, a)]] Then,"
D,0.9304589707927677,"V π
P ′,r+b −V π
P,r =
1
1 −γ E(s,a)∼dπ
P ′[r(s, a) + b(s, a) + γEP ′(s′|s,a)[Qπ
P,r(s′, π)] −Qπ
P,r(s, a)]]"
D,0.9318497913769124,"=
1
1 −γ E(s,a)∼dπ
P ′[b(s, a) + γEP ′(s′|s,a)[Qπ
P,r(s′, π)] −γEP (s′|s,a)[Qπ
P,r(s′, π)]]."
D,0.933240611961057,"Similarly,"
D,0.9346314325452016,"V π
P,r −V π
P ′,r+b =
1
1 −γ E(s,a)∼dπ
P [r(s, a) + γEP (s′|s,a)[Qπ
P ′,r+b(s′, π)] −Qπ
P ′,r+b(s, a)]]"
D,0.9360222531293463,"=
1
1 −γ E(s,a)∼dπ
P [−b(s, a) + γEP (s′|s,a)[Qπ
P ′,r+b(s′, π)] −γEP ′(s′|s,a)[Qπ
P,r(s′, π)]]."
D,0.9374130737134909,"The following lemma is used to deal with the distribution shift in the ofﬂine setting. For the proof,
refer to Chang et al. (2021).
Lemma 21 (Distribution shift lemma). Consider any policy π and state-action distribution ρ, and
any representation φ⋆, we have:"
D,0.9388038942976356,"E(s,a)∼dπ
P ⋆[φ⋆(s, a){φ⋆(s, a)}⊤] ≤C⋆Eρ[φ⋆(s, a){φ⋆(s, a)}⊤],
C⋆:= sup
x∈R"
D,0.9401947148817803,"x⊤E(s,a)∼dπ
P ⋆[φ⋆{φ⋆}⊤]x"
D,0.9415855354659249,"x⊤E(s,a)∼ρ[φ⋆{φ⋆}⊤]x ."
D,0.9429763560500696,"This is some auxiliary lemma to convert the ﬁnite sample error bound into the sample complexity.
Lemma 22 (Conversion of ﬁnite sample error bounds into sample complexities). By taking"
D,0.9443671766342142,"N = 1/ϵ′2 × ln2(1 + 1/ϵ′2), ϵ′ =
ϵ"
D,0.9457579972183588,"a1 ln1/2(e + a2) ln1/2(e + a3)
."
D,0.9471488178025035,"It satisﬁes a1
p"
D,0.9485396383866481,"1/N ln1/2(1 + a2N) ln1/2(1 + a3N) < cϵ.
where c is a constant independent of a1, a2, a3."
D,0.9499304589707928,"Proof. We ﬁrst have a1
p"
D,0.9513212795549374,"1/N ln1/2(1 + a2N) ln1/2(1 + a3N) ≤a1 max(ln1/2(1 + a2) ln1/2(1 + a3), 1)
p"
D,0.952712100139082,"1/N ln(1 + N).
Here, we use"
D,0.9541029207232267,"ln1/2(1 + a2N) ≤{ln(1 + a2) + ln(1 + N)}1/2 ≤
p"
D,0.9554937413073713,"max(1, ln(1 + a2)) ln(1 + N)."
D,0.9568845618915159,"Then, we prove when N = 1/ϵ2 × ln2(1 + 1/ϵ2).
p"
D,0.9582753824756607,"1/N ln(1 + N) < ϵ.
This is proved by
p"
D,0.9596662030598053,1/N ln(1 + N) ≤ϵ × ln(1 + 1/ϵ2 × ln2(1 + 1/ϵ2))
D,0.96105702364395,ln(1 + 1/ϵ2)
D,0.9624478442280946,≤ϵ × ln(1 + 1/ϵ2) + ln(1 + ln2(1 + 1/ϵ2))
D,0.9638386648122392,ln(1 + 1/ϵ2)
D,0.9652294853963839,≤ϵ + ϵ × ln(1 + ln2(1 + 1/ϵ2))
D,0.9666203059805285,ln(1 + 1/ϵ2)
D,0.9680111265646731,≤ϵ + ϵ × 0.5{1 + ln2(1 + 1/ϵ2))}1/2 −1
D,0.9694019471488178,"ln(1 + 1/ϵ2)
≲ϵ."
D,0.9707927677329624,"From the third line to the fourth line, we use ln(x) ≤0.5(x1/2 −1) for x > 0. Then, the ﬁnal
statement is concluded."
D,0.972183588317107,Published as a conference paper at ICLR 2022
D,0.9735744089012517,"E
MORE COMPARISON TO XIE ET AL. (2021)"
D,0.9749652294853964,"We brieﬂy explain the guarantee when we use Algorithm 1 (Xie et al., 2021). For a given reward r,
we ﬁrst deﬁne a new feature class Φ+
r .
Deﬁnition 23 (Augmented feature). Let φ = [φ1, · · · , φd]."
D,0.9763560500695411,"Φ+
r = {φ+
r ; φ ∈Φ},
φ+
r = [φ1, · · · , φd, r]."
D,0.9777468706536857,"Then, we set"
D,0.9791376912378303,"F = {a⊤φ+
r | ∥a∥2 ≤c
√"
D,0.980528511821975,"d + 1, φ+
r ∈Φ+
r }."
D,0.9819193324061196,"where c is some suitable constant. Given the hypothesis class F for the Q-function, we can run
Algorithm 1 in (Xie et al., 2021). We denote the output policy as ˆπ."
D,0.9833101529902643,"We check two assumptions to ensure the algorithm works. The ﬁrst assumption is realizability. This
is satisﬁed since for any policy π ∈Π (Π is the class of all Markovian polices), we have Qπ
P ⋆,r ∈F.
The second assumption is completeness. This is also satisﬁed since T π
P ⋆,rF ⊂F for any policy
π ∈Π where T π
P ⋆,r is a Bellman-operator s.t."
D,0.9847009735744089,"T π
P ⋆,r : {S × A →R} ∋f 7→r(s, a) + γEs′∼P ⋆(s,a)[f(s′, π)] ∈{S × A →R},"
D,0.9860917941585535,"where we denote f(s, π) = Ea∼π(s)f(s, a). Then, by invoking their Corollary 5, we have"
D,0.9874826147426982,"Theorem 24 (PAC bound based on Xie et al. (2021)). With probability 1 −δ,"
D,0.9888734353268428,"∀π ∈Π : V π
P ⋆,r −V ˆπ
P ⋆,r ≤c q"
D,0.9902642559109874,"C†
π,r
(1 −γ)2"
D,0.9916550764951322,(d + 1) log(1/δ) log |A| n
D,0.9930458970792768,"1/5
. where"
D,0.9944367176634215,"C†
π,r =
sup
φ+
r ∈Φ+
r
sup
a∈Rd+1"
D,0.9958275382475661,"a⊤Edπ
P ⋆[φ+
r {φ+
r }⊤]a"
D,0.9972183588317107,"a⊤Eρ[φ+
r {φ+
r }⊤]a ."
D,0.9986091794158554,"We compare the above result with our result in Theorem 6. First, since C†
r includes r and all pos-
sible features in Φ, this partial coverage condition is stronger than ours (recall our partial coverage
condition is only related to the true representation φ⋆), and we always have C⋆
π ≤C†
π,r. Secondly,
the dependence on n is much worse. Third, it is unclear whether the learned policy can compete
against any history-dependent policy. Recall in Theorem 6, we show that our algorithm can compete
with any history-dependent policies."
