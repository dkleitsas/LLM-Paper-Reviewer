Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005291005291005291,"Classical chest X-ray analysis has designed radiomic features to indicate the
characteristics of abnormality of the chest X-rays. However, extracting reliable
radiomic features heavily hinges on pathology localization, which is often absent
in real-world image data. Although the past decade has witnessed the promising
performance of convolutional neural networks (CNNs) in analyzing chest X-rays,
most of them ignored domain knowledge such as radiomics. Recently, the surge of
Transformers in computer vision has suggested a promising substitute for CNNs.
It can encode highly expressive and generalizable representations and avoid costly
manual annotations via a unique implementation of the self-attention mechanism.
Moreover, Transformers naturally suit the feature extraction and fusion from
different input modalities. Inspired by its recent success, this paper proposes
CheXT, the Ô¨Årst Transformer-based chest X-ray model. CheXT targets (semi-
supervised) abnormality classiÔ¨Åcation and localization from chest X-rays, enhanced
by baked-in auxiliary knowledge guidance using radiomics. SpeciÔ¨Åcally, CheXT
consists of an image branch and a radiomics branch, interacted by cross-attention
layers. During training, the image branch leverages its learned attention to estimate
pathology localization, which is then utilized to extract radiomic features from
images in the radiomics branch. Therefore, the two branches in CheXT are deeply
fused and constitute an end-to-end optimization loop that can bootstrap accurate
pathology localization from image data without any bounding box used for training.
Extensive experiments on the NIH chest X-ray dataset demonstrate that CheXT
signiÔ¨Åcantly outperforms existing baselines in disease classiÔ¨Åcation (by 1.1% in
average AUCs) and localization (by a signiÔ¨Åcant average margin of 3.6% over
different IoU thresholds). Codes and models will be publicly released."
INTRODUCTION,0.010582010582010581,"1
INTRODUCTION"
INTRODUCTION,0.015873015873015872,"In medical study, handcrafted radiomics (Zwanenburg et al., 2016) refers to the process of extracting
several quantitative and semiquantitative features from medical images for improved decision support.
It has the potential to uncover disease characteristics that are difÔ¨Åcult to identify by viewing raw
images alone. Given their advantages, researchers have explored the performance of radiomic features
for chest X-ray analysis. For example, (Shi et al.; Saygƒ±lƒ±, 2021) extracted a set of radiomic features
to diagnose different types of pneumonia. (Bai et al., 2020) proposed a hybrid model to encode the
combination of radiomic features and clinical information. (Ghosh et al., 2020) presented a new
handcrafted feature to distinguish between severe and nonsevere patients. However, all the above
methods rely on accurate pathology localization annotations to extract radiomic features from the
correct ‚Äúregion of interest‚Äù (aka bounding boxes) but not other irrelevant parts(Van Griethuysen et al.,
2017). Such bounding boxes are usually expensive and time-consuming to acquire by humans and, if
inaccurate, will tremendously degrade the reliability of radiomic features. There is thus an unmet
need to automatically localize pathologies on chest X-rays to facilitate radiomic features extraction."
INTRODUCTION,0.021164021164021163,"Under the rapid development of deep learning, many researchers have made their efforts to utilize the
Convolutional Neural Networks (CNNs) in building automated systems of chest X-ray abnormality
classiÔ¨Åcation and localization (Rajpurkar et al., 2017; Wang et al., 2017; Li et al., 2018; Liu et al.,
2019b; Rozenberg et al., 2020; Wang et al., 2021). However, CNN methods witness several limitations."
INTRODUCTION,0.026455026455026454,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.031746031746031744,"CheXT
Localization"
INTRODUCTION,0.037037037037037035,Disease distribution
INTRODUCTION,0.042328042328042326,Cardiomegaly
INTRODUCTION,0.047619047619047616,Radiomics BYOA
INTRODUCTION,0.05291005291005291,"Figure 1: General overview of our CheXT frame-
work for Cardiopulmonary Disease ClassiÔ¨Åcation
and Localization from chest X-rays. CheXT takes
the chest X-ray image as the input and outputs
a heatmap for pathology localization, based on
which the bounding box could be obtained. Ra-
diomic features are further extracted from the
bounded region and are fed to predict the disease."
INTRODUCTION,0.0582010582010582,"First, chest X-rays own valuable domain knowl-
edge and domain-speciÔ¨Åc features, such as ra-
diomic features. Thus, they could have blessed
better recognition but are unfortunately over-
looked by most CNNs. Second, chest X-rays
have more subtle discriminative features com-
pared to natural images, making their recogni-
tion more challenging. Finally, CNNs are often
criticized for being non-transparent and their
predictions not traceable by humans, hence hin-
ders their acceptance and adoption by clinicians."
INTRODUCTION,0.06349206349206349,"Why Transformers for Chest X-rays?
The
latest surge of transformers provides a promis-
ing alternative to model chest X-rays. Trans-
former was Ô¨Årst prevailing to Natural Language
Processing (Vaswani et al., 2017; Devlin et al.,
2018; Brown et al., 2020), followed by its recent
success in computer vision (Dosovitskiy et al.,
2020; Carion et al., 2020; Zhu et al., 2020) and
multi-modal learning (Ying et al., 2021). It is an
‚Äúuniversal modeling tool‚Äù that can unify the fea-
ture extraction and fusion from different input
modalities within one model, without domain-
speciÔ¨Åc model tweaks. For example, (Akbari
et al., 2021) demonstrated to learn powerful multi-modal representations from unlabeled video, audio,
and text data, using one Transformer architecture."
INTRODUCTION,0.06878306878306878,"Bringing that into the context of chest X-rays, we see the tantalizing potential that a Transformer
could organically and jointly learn from two views of chest X-rays: (i) raw X-ray images that contain
the richest details, hence beneÔ¨Åting from the data-driven learning capacity; and (ii) radiomic features
that encode critical domain prior knowledge, hence effectively guiding and regularizing the learning
process. The appeal of Transformers is, however, blocked by a ‚Äúchicken-and-egg‚Äù problem: as
aforementioned, the extraction of reliable radiomic features hinge on the pathology localization, but
then the pathology localization is often absent in images and also needs to be learned Ô¨Årst."
INTRODUCTION,0.07407407407407407,"This paper presents a holistic framework of Knowledge-Guided Cross-Attention Transformer for
Chest X-ray analysis, named CheXT (Figure 1). CheXT consists of two Transformer-based branches
that learn from two data formats characterizing the same patient: the image and radiomics branches.
Both are deeply fused and interacted by cross-attention layers (Chen et al., 2021a). Notably, the
radiomic features need be extracted from the learned pathology localizations, which are not readily
available. The key enabling technology to resolve this hurdle, is to construct a ‚Äúfeedback loop‚Äù during
training: the image branch leverages its learned attention to estimating pathology localization, which
is then utilized to extract radiomic features from images in the radiomics branch. Training under
a uniÔ¨Åed contrastive loss, such an end-to-end optimization loop can bootstrap accurate pathology
localization from image data, with no bounding boxes used for training. Our contributions are
outlined as follows:"
INTRODUCTION,0.07936507936507936,"‚Ä¢ We leverage the radiomic feature as an ‚Äúauxiliary input modality‚Äù correlated with the raw
image modality and encoded with domain knowledge. We then propose a novel knowledge-
guided cross-attention Transformer, CheXT, to jointly extract and fuse image and radiomic
feature representations for chest X-ray analysis."
INTRODUCTION,0.08465608465608465,"‚Ä¢ To resolve the key ‚Äúchicken-and-egg‚Äù problem of extracting radiomic features without
available pathology localization, we construct an innovative optimization loop with the
image and radiomic branches deeply interacting via attention. Such end-to-end loop can
bootstrap accurate pathology localization from images without using human-annotated
bounding boxes."
INTRODUCTION,0.08994708994708994,"‚Ä¢ Our approach achieve superior classiÔ¨Åcation and localization results against several com-
petitive baselines, on the NIH chest X-ray benchmark. Notably, compared to existing"
INTRODUCTION,0.09523809523809523,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.10052910052910052,"approaches, CheXT generates more accurate disease localization for extracting radiomic
features, by a signiÔ¨Åcant average margin of 3.6% over different IoU thresholds."
RELATED WORK,0.10582010582010581,"2
RELATED WORK"
RELATED WORK,0.1111111111111111,"Radiomics in Medical Diagnosis. The design of radiomics involves biological and medical data
and prior knowledge. Thus, radiomics vastly enriches images and expands the horizons of the image
toward in-vivo biologic information extraction (Gillies et al., 2016). In image-based biomarkers
for cancer staging and prognostication, radiomics had shown promising power (Nasief et al., 2019).
Radiomics extracts quantitative data from medical images to represent tumor phenotypes, such as
spatial heterogeneity of a tumor and spatial response variations. (Eilaghi et al., 2017) demonstrated
that radiomic of CT texture features are associated with the overall survival rate of pancreatic cancer.
(Chen et al., 2017) revealed that the Ô¨Årst-order radiomic features (e.g., mean, skewness, and kurtosis)
are correlated with pathological responses to cancer treatment. (Huang et al., 2018) showed that
radiomics could increase the positive predictive value and reduce the false-positive rate in lung
cancer screening for small nodules compared with radiologists. (Zhang et al., 2018) found that
multiparametric MRI-based radiomics nomograms provided improved prognostic ability in advanced
nasopharyngeal carcinoma."
RELATED WORK,0.1164021164021164,"In comparison, deep learning is often criticized for being a ‚Äúblack box‚Äù and lacks interpretability
despite high predictive accuracy. This limitation has motivated many interpretable learning techniques
including activation maximization (Erhan et al., 2009), network inversion (Mahendran & Vedaldi,
2015), GradCAM (Selvaraju et al., 2016), and network dissection (Bau et al., 2017). We believe that
the joint utilization of radiomics and interpretable learning techniques in our framework can further
advance accurate yet interpretable learning in the medical image domain."
RELATED WORK,0.12169312169312169,"Transformers for Medical Images. Recently, Vision Transformer (ViT) (Dosovitskiy et al., 2020)
achieved state-of-the-art classiÔ¨Åcation on ImageNet by directly applying Transformers with global
self-attention to full-sized images. Inspired by the promising performance of ViT, researchers have
recently applied the idea to medical images. For example, (Oktay et al., 2018; Wang et al., 2019; Chen
et al., 2021b) used the attention mechanism to boost the performance of medical image segmentation.
(Valanarasu et al., 2021) proposed a gated axial-attention model to introduce an additional control
mechanism in the self-attention module. (Park et al., 2021) utilized a hybrid framework of CNN and
Transformer for Covid-19 prediction. However, those methods did not consider any domain prior
knowledge. (Han et al., 2021) applied pre-extracted radiomic features to guide pneumonia detection
from chest X-ray images. However, they adopted a convolutional backbone for image encoder, while
using a another speciÔ¨Åcally crafted radiomic features encoder. Therefore, the method involves no
joint interaction between image and radiomic features, and need to use accurate bounding boxes
during training in order to extract radiomic features. Hence, their method dramatically limits usability
in clinical practice."
METHOD,0.12698412698412698,"3
METHOD"
METHOD,0.13227513227513227,"An overview of CheXT is illustrated in Figure 2. In the following subsections, we will Ô¨Årst present
Cross-attention Vision Transformer (CrossViT), a recent two-branch ViT backbone on which CheXT
is built, and then describe our many unique improvements customized for Chest X-ray analysis."
METHOD,0.13756613756613756,"3.1
PRELIMINARY: VIT AND CROSS-ATTENTION"
METHOD,0.14285714285714285,"ViT Ô¨Årst converts an image into a sequence of patch tokens by dividing the image with certain
patch size and linearly projecting each patch into tokens. A special token (CLS) is added in front
of the sequence, as in the original BERT (Devlin et al., 2018). Then, all tokens are passed through
stacked Transformer encoders. Finally, the hidden state corresponding to the CLS token is used as
the aggregate sequence representation for image classiÔ¨Åcation."
METHOD,0.14814814814814814,"A Transformer encoder is composed of a sequence of blocks where each block contains multiheaded
self-attention with a feed-forward network. Layer normalization and residual shortcuts are applied
before and after every block, respectively. The granularity of the patch size affects the accuracy and
complexity of ViT. Therefore, ViT can perform better with Ô¨Åne-grained patch size but with higher"
METHOD,0.15343915343915343,Under review as a conference paper at ICLR 2022
METHOD,0.15873015873015872,"Conv
Block"
METHOD,0.164021164021164,Transformer
METHOD,0.1693121693121693,"Encoder ‚Ä¶
‚Ä¶ BYOA"
METHOD,0.1746031746031746,"Cross-
Attention
feedback loop"
METHOD,0.17989417989417988,Image Branch ‚Ä¶ ùëîùëñ
METHOD,0.18518518518518517,"Unsupervised 
Contrastive loss ùëîùëü ùëìùëñ ùëìùëü"
METHOD,0.19047619047619047,"Supervised 
classification"
METHOD,0.19576719576719576,focal loss ùëÖùëêùëôùë†
METHOD,0.20105820105820105,Transformer
METHOD,0.20634920634920634,Encoder
METHOD,0.21164021164021163,Transformer
METHOD,0.21693121693121692,Encoder ùêºùëêùëôùë†
METHOD,0.2222222222222222,ùëÖùëùùëéùë°ùëê‚Ñé
METHOD,0.2275132275132275,ùêºùëùùëéùë°ùëê‚Ñé
METHOD,0.2328042328042328,Transformer
METHOD,0.23809523809523808,Encoder
METHOD,0.24338624338624337,Radiomic Branch
METHOD,0.24867724867724866,"‚Ä¶
Transformer"
METHOD,0.25396825396825395,Encoder
METHOD,0.25925925925925924,Transformer
METHOD,0.26455026455026454,Encoder
METHOD,0.2698412698412698,"Figure 2: Overview of our proposed CheXT. It contains two branches, the Image branch and the
Radiomics branch, to process the image and radiomic features (generated by the BYOA module
shown in Figure 3), respectively. The output tokens are then fused by an efÔ¨Åcient module via cross
attention of the CLS tokens. Finally, the output of two CLS tokens (Icls and Rcls) are used for disease
classiÔ¨Åcation. We minimize the classiÔ¨Åcation errors via a focal loss. Since we train with both labeled
and unlabeled images, we leverage a contrastive learning strategy. SpeciÔ¨Åcally, we generate the image
view zi = gi(Icls) by a projection head gi. Similarily, we generate the radiomic view zr = gr(Rcls)
by another projection head gr. We maximize the agreement between zi and zr via a contrastive loss
(NT-Xent). Of note, the contrastive loss is only active during the training."
METHOD,0.2751322751322751,"FLOPS and memory consumption (Chen et al., 2021a). To relieve this problem, CrossViT (Chen
et al., 2021a) was proposed with a dual-branch ViT where either branch operates at a different patch
size, as its own ‚Äúview‚Äù of the image. The cross-attention module is then used to fuse information
between the branches to balance the patch sizes and complexity. Similar to ViT, the Ô¨Ånal hidden
vector of CLS from two branches are used for image classiÔ¨Åcation."
OUR PROPOSED CHEXT MODEL,0.2804232804232804,"3.2
OUR PROPOSED CHEXT MODEL"
OUR PROPOSED CHEXT MODEL,0.2857142857142857,"CrossViT supplies a graceful framework to simultaneously tackle and fuse two different ‚Äúviews‚Äù from
the same input data, e.g., different-size image patches in (Chen et al., 2021a). In CheXT, we extend
their idea by treating image itself as one ‚Äúview‚Äù and the radiomic feature extracted from the same
image as another ‚Äúview‚Äù (Figure 2). The two views are then jointly learned by interacting through
cross-attention. Transformer serves as the modality-agnostic backbone for both."
OUR PROPOSED CHEXT MODEL,0.291005291005291,"SpeciÔ¨Åcally, we introduce a dual-branch cross-attention Transformer where the Ô¨Årst (primary) branch
operates the image part, while the second (complementary) branch handles the radiomic features. To
resolve the ‚Äúchicken-and-egg‚Äù dilemma in extracting reliable radiomic features without bounding
boxes, we have designed a novel Bootstrap Your Own Attention (BYOA) module, using feedbacks
to learn region localization for extracting radiomic features. A simple yet effective module is also
utilized to fuse information between the branches. In the subsequent sections, we will describe the
two branches, the BOYA module, and the fusion module."
OUR PROPOSED CHEXT MODEL,0.2962962962962963,"Image Branch. The primary image branch uses a Progress-Sampling ViT (PS-ViT) (Yue et al., 2021)
as its backbone. Unlike the vanilla ViT that splits images into Ô¨Åxed-size tokens, PS-ViT utilizes an
iterative and progressive sampling strategy to locate discriminative regions and avoid over-partition
object structures. We experimentally observed PS-ViT outperforms ViT and other variants in our
framework because it generates higher-quality and more structure-aware attention maps, which are
crucial for estimating the pathology localization during training."
OUR PROPOSED CHEXT MODEL,0.30158730158730157,"Radiomics Branch. The complementary radiomics branch is for processing radiomic features.
Handcrafted features usually cover a wide range of categories, such as Ô¨Årst-order (basic intensity and
shaped-based features), second-order (texture features extracted from various matrices), and more
advanced features including those calculated from Fourier and wavelet transforms. SpeciÔ¨Åcally, the
radiomic features are composed of the following categories:"
OUR PROPOSED CHEXT MODEL,0.30687830687830686,"‚Ä¢ First-Order statistics features to measure the distribution of voxel intensities within the bound-
ing boxes. The features include energy (the measurement of the magnitude of voxel values),"
OUR PROPOSED CHEXT MODEL,0.31216931216931215,Under review as a conference paper at ICLR 2022
OUR PROPOSED CHEXT MODEL,0.31746031746031744,Attention map
OUR PROPOSED CHEXT MODEL,0.32275132275132273,"generator
Cross-
Attention"
OUR PROPOSED CHEXT MODEL,0.328042328042328,Radiomics
OUR PROPOSED CHEXT MODEL,0.3333333333333333,Extractor
OUR PROPOSED CHEXT MODEL,0.3386243386243386,Radiomic
OUR PROPOSED CHEXT MODEL,0.3439153439153439,Features
OUR PROPOSED CHEXT MODEL,0.3492063492063492,"Figure 3: Overview of our BYOA module. For the input chest X-rays, we look at the self-attention
of the CLS token of the Image branch on the heads of the Ô¨Ånal output of the cross attention module.
Then we apply a threshold (0.1) (which means we only keep the top 10% active pixels in the whole
attention map) to generate the bounding boxes. Then with the generated bounding boxes, we use the
Pyradiomic tool as the radiomic extractor to extract the radiomic features."
OUR PROPOSED CHEXT MODEL,0.3544973544973545,"entropy (the measurement of uncertainty in the image values), and max/mean/median gray
level intensity within the region of interest."
OUR PROPOSED CHEXT MODEL,0.35978835978835977,"‚Ä¢ Shape-based features, such as Mesh Surface, Pixel Surface, and Perimeter."
OUR PROPOSED CHEXT MODEL,0.36507936507936506,"‚Ä¢ Gray-level features, such as Gray Level Co-occurrence Matrix (GLCM), Gray Level Size
Zone (GLSZM), Gray Level Run Length Matrix (GLRLM), Neighboring Gray Tone Differ-
ence Matrix (NGTDM), and Gray Level Dependence Matrix (GLDM) features."
OUR PROPOSED CHEXT MODEL,0.37037037037037035,"In short, radiomic features are a set of quantitative features that can describe the characteristics of
medical images. In our framework, we aim to make the hidden features of the CLS similar to the
radiomic features to learn the localization of pathologies in the chest X-rays. For this branch, we
use the vanilla Transformer (Liu et al., 2017) as the radiomic features encoder. Please note that the
only difference is that the positional encoding module is discarded, since there does not exist any
positional relationship between the radiomic features."
OUR PROPOSED CHEXT MODEL,0.37566137566137564,"Bootstrap Your Own Attention (BYOA): A Feedback Loop Module. Our main roadblock is how
to generate robust radiomic features without pathology localization. On one hand, radiomic features
are dependent on and highly sensitive to the local image regions of interest, for which we have no
bounding box annotation. On the other hand, image features would beneÔ¨Åt from the guidance from
radiomic features that encode important domain knowledge. The learning of image and radiomic
features are fully entangled, forming a challenging ‚Äúchicken-and-egg‚Äù loop."
OUR PROPOSED CHEXT MODEL,0.38095238095238093,"To address this issue, we design BYOA to constitute an end-to-end feedback loop that can bootstrap
accurate pathology localization from image data, without any bounding box used for training (Fig-
ure 3). BYOA contains two components: attention map generation and radiomic feature extraction."
OUR PROPOSED CHEXT MODEL,0.3862433862433862,"‚Ä¢ Attention Map Generation. Similar to the approach in (Caron et al., 2021), we look at the
self-attention of the CLS token on the heads of the last layer. Here, we have two CLS tokens
from two branches, but the attention maps only come from the Image branch. Then we
apply a threshold on the self-attention maps to generate bounding boxes for the extraction
of radiomic features. The choice of the threshold will inÔ¨Çuence the quality of the radiomic
features. SpeciÔ¨Åcally, the threshold is designed as how much active pixels to keep in the
generated attention map. The more active pixels we keep, the larger the generated bounding
box is. Please see 4.3.3 for more details."
OUR PROPOSED CHEXT MODEL,0.3915343915343915,"‚Ä¢ Radiomic Features Extraction. Given the original images and generated bounding boxes,
we used the Pyradiomic tool to extract radiomic features (Van Griethuysen et al., 2017)."
OUR PROPOSED CHEXT MODEL,0.3968253968253968,"Cross-Attention Fusion Module. This fusion involves the CLS token of one branch and patch
tokens of the other branch. As the CLS token is the aggregate representation of the branch, this
interaction helps include information at different scale. Please refer to (Chen et al., 2021a) for more
details about the cross-attention mechanism."
SEMI-SUPERVISED LOSS FUNCTION,0.4021164021164021,"3.3
SEMI-SUPERVISED LOSS FUNCTION"
SEMI-SUPERVISED LOSS FUNCTION,0.4074074074074074,"As shown in Figure 2, CheXT is trained using the linear combination of the supervised focal
classiÔ¨Åcation and unsupervised contrastive losses. For the supervised classiÔ¨Åcation, considering that
the chest X-ray dataset is usually highly imbalanced, we adopt the focal loss (Pasupa et al., 2020).
For unsupervised contrastive learning, we use the cross-view contrastive loss (Chen et al., 2020)."
SEMI-SUPERVISED LOSS FUNCTION,0.4126984126984127,Under review as a conference paper at ICLR 2022
SEMI-SUPERVISED LOSS FUNCTION,0.41798941798941797,"Supervised ClassiÔ¨Åcation Focal Loss. We feed the output of the CLS tokens of two branches Icls
and Rcls to a simple linear classiÔ¨Åer. The supervised classiÔ¨Åcation focal loss Lfl is deÔ¨Åned as"
SEMI-SUPERVISED LOSS FUNCTION,0.42328042328042326,"Lfl =

‚àíŒ± (1 ‚àíy‚Ä≤)Œ≥ log y‚Ä≤,
y = 1
‚àí(1 ‚àíŒ±)y‚Ä≤Œ≥ log (1 ‚àíy‚Ä≤) ,
y = 0
(1)"
SEMI-SUPERVISED LOSS FUNCTION,0.42857142857142855,"Œ± allows us to give different importance to positive and negative examples. Œ≥ is used to distinguish
easy and hard samples and force the model to learn more from difÔ¨Åcult examples."
SEMI-SUPERVISED LOSS FUNCTION,0.43386243386243384,"Unsupervised Cross-View Contrastive Loss. Our contrastive loss extends the normalized tempera-
ture scaled cross-entropy loss (NT-Xent). The difference is that we maximize agreement between
two feature views extracted from different input formats, one the image and the other radiomics."
SEMI-SUPERVISED LOSS FUNCTION,0.43915343915343913,"Given an anchor chest X-ray in a minibatch, the positive sample will be its radiomic feature view, and
the negative samples will be other chest X-rays (either image or radiomics). Since the CLS could be
regarded as the representation of all other tokens, we only need to maximize the agreement between
them. Suppose Icls,k and Rcls,k are the k‚àíth image features and radiomic features in the minibatch,
respectively, and sim(Àô) the cosine similarity, the loss function Lcl is deÔ¨Åned as"
SEMI-SUPERVISED LOSS FUNCTION,0.4444444444444444,"Lcl = ‚àílog
exp(sim(gi(Icls,k), gr(Rcls,k))/œÑ)
PN
k=1 exp(sim(gi(Icls,k), gr(Rcls,k))/œÑ)
(2)"
SEMI-SUPERVISED LOSS FUNCTION,0.4497354497354497,where œÑ is the temperature. The Ô¨Ånal contrastive loss is summed over all instances in the minibatch.
SEMI-SUPERVISED LOSS FUNCTION,0.455026455026455,"Overall, we treat CheXT training as a semi-supervised multi-task learning. Since for chest x-rays,
there exists two labels, disease class labels and pathology bounding box annotations. In our case,
we only have access to the disease labels, however, bounding box annotations are more important
than the disease class labels for chest x-rays. Here, when we say ‚Äúsemi-supervised‚Äù, we refer to the
bounding box annotations, not disease class labels. For multi-task, one task is supervised disease
classiÔ¨Åcation. The other is unsupervised cross-view contrastive learning. The total loss is:
L = (1 ‚àíŒª) √ó Lcl + Œª √ó Lfl
(3)"
EXPERIMENTS,0.4603174603174603,"4
EXPERIMENTS"
DATASET AND PROTOCOL SETTING,0.4656084656084656,"4.1
DATASET AND PROTOCOL SETTING"
DATASET AND PROTOCOL SETTING,0.4708994708994709,"The NIH Chest X-ray dataset (Wang et al., 2017) consists of 112,120 chest X-rays collected from
30,805 patients, and each image is labeled with 8 cardiopulmonary disease labels (Atelectasis,
Cardiomegaly, Effusion, InÔ¨Åltration, Mass, Nodule, Pneumonia, and Pneumothorax). The labels are
extracted from the associated radiology report using an automatic labeler (Peng et al., 2018) with
a reported accuracy of 90%. We use the extracted labels as ground-truth for training CheXT. The
NIH dataset also includes high-quality bounding box annotations for 880 images by radiologists.
We separated these 880 images from our entire dataset, and they are used only to evaluate disease
localization. A signiÔ¨Åcant difference between our method and existing baseline methods (Liu et al.,
2019a; Li et al., 2018) is that our method does not require any training data related to the bounding
box while others use some percentage of these images for training."
DATASET AND PROTOCOL SETTING,0.47619047619047616,"In our experiments, we followed the same protocol as in the studies of (Wang et al., 2017; Li et al.,
2018), to shufÔ¨Çe our dataset (excluding images with Bounding Boxes annotations) into three subsets:
70% for training, 10% for validation, and 20% for testing. In order to prevent data leakage across
patients, we make sure that there is no patient overlap between our train, validation, and test set."
IMPLEMENTATION DETAILS,0.48148148148148145,"4.2
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.48677248677248675,"We build our image branch encoder based on PS-ViT (Yue et al., 2021) and apply their default
hyperparameters for training. The only difference is that we use a more shallow PS-ViT with 6 layers.
For the radiomic branch encoder, since the radiomic features are already informative features, we
use a more shallow Transformer (1-layers) to encode the radiomic features. And we add one more
cross-attention layer to fuse these two format features. We set the batch size as 128 and train the
model for 50 epochs (5 warm-up epochs). Other setup includes a cosine linear-rate scheduler with a
linear warm-up, an initial learning rate of 0.004, and a weight decay of 0.05. During the evaluation,
we resize the image to 256√ó256 and take the center crop 224√ó224 as the input."
IMPLEMENTATION DETAILS,0.49206349206349204,Under review as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.4973544973544973,"Method
Atelectasis
Cardiomegaly
Effusion
InÔ¨Åltration
Mass
Nodule
Pneumonia Pneumothorax Mean"
IMPLEMENTATION DETAILS,0.5026455026455027,"CNN
(Wang et al., 2017)
0.72
0.81
0.78
0.61
0.71
0.67
0.63
0.81
0.718
(Wang et al., 2018)
0.73
0.84
0.79
0.67
0.73
0.69
0.72
0.85
0.753
(Yao et al., 2017)
0.77
0.90
0.86
0.70
0.79
0.72
0.71
0.84
0.786
(Rajpurkar et al., 2017)
0.82
0.91
0.88
0.72
0.86
0.78
0.76
0.89
0.828
(Kumar et al., 2018)
0.76
0.91
0.86
0.69
0.75
0.67
0.72
0.86
0.778
(Liu et al., 2019b)
0.79
0.87
0.88
0.69
0.81
0.73
0.75
0.89
0.801
(Seyyed et al., 2020)
0.81
0.92
0.87
0.72
0.83
0.78
0.76
0.88
0.821
(Han et al., 2020)
0.83
0.92
0.87
0.76
0.85
0.76
0.77
0.86
0.828"
IMPLEMENTATION DETAILS,0.5079365079365079,"Transformer
ViT
0.74
0.78
0.81
0.72
0.70
0.66
0.65
0.76
0.728
CrossViT
0.69
0.71
0.72
0.72
0.74
0.79
0.82
0.88
0.759
PS-ViT
0.75
0.81
0.82
0.73
0.79
0.73
0.69
0.81
0.766
CheXT
0.80
0.92
0.78
0.86
0.88
0.88
0.79
0.81
0.839
(¬±0.02)
(¬±0.00)
(¬±0.01)
(¬±0.01)
(¬±0.02) (¬±0.00)
(¬±0.01)
(¬±0.02)
‚Äì"
IMPLEMENTATION DETAILS,0.5132275132275133,"Table 1: Comparison with the baseline models for AUC of each class and average AUC. For each
column, bold values denote the best results."
ABNORMALITY CLASSIFICATION,0.5185185185185185,"4.3
ABNORMALITY CLASSIFICATION"
ABNORMALITY CLASSIFICATION,0.5238095238095238,"Abnormality classiÔ¨Åcation is a multi-label classiÔ¨Åcation problem. It assigns one or more labels
among 8 cardiopulmonary diseases (Atelectasis, Cardiomegaly, Effusion, InÔ¨Åltration, Mass, Nodule,
Pneumonia, and Pneumothorax) to each input image at inference time. We conducted 3-fold cross-
validation (Table 1). We compared CheXT with reference models, which have published state-of-the-
art performance of disease classiÔ¨Åcation on the NIH Chest X-ray dataset."
EVALUATION METRIC,0.5291005291005291,"4.3.1
EVALUATION METRIC"
EVALUATION METRIC,0.5343915343915344,"We used Area under the Receiver Operating Characteristics (AUC) to estimate the performance of
our model (FawcettTom, 2006). A higher AUC score implies a better classiÔ¨Åcation model. We also
provide mean AUC across all the classes to highlight the overall performance of our model."
COMPARISON WITH THE SOTA MODELS,0.5396825396825397,"4.3.2
COMPARISON WITH THE SOTA MODELS"
COMPARISON WITH THE SOTA MODELS,0.544973544973545,"AUC scores for each disease and mean AUC score across 8 diseases are presented in Table 1. Not only
we compared CheXT with previous CNN-based SOTA models, but also several Transformer-based
models. From the table, we can Ô¨Ånd that CheXT outperformed the baselines in the majority of
diseases. We report the average AUC of 3 runs to show the robustness of our model. Compared to all
baselines, CheXT achieves a mean AUC score of 0.839 using a dual-branch shallow Transformer
across the 8 different diseases, which is 0.011 higher than the SOTA (uses DenseNet-121) (Rajpurkar
et al., 2017) on disease classiÔ¨Åcation. SpeciÔ¨Åcally, our results signiÔ¨Åcantly outperformed the best
baseline models by 0.13, 0.09, and 0.02 in AUC for detecting InÔ¨Åltration, Nodule, and Mass higher
respectively. Besides, compared to the Transformer-based models, the key difference is that we utilize
the extracted radiomic features for disease prediction, which improves the accuracy and adds the
model‚Äôs interpretability due to the utilization of self-designed handcrafted radiomic features."
EFFECT OF DIFFERENT THRESHOLDS OF ATTENTION MAPS,0.5502645502645502,"4.3.3
EFFECT OF DIFFERENT THRESHOLDS OF ATTENTION MAPS"
EFFECT OF DIFFERENT THRESHOLDS OF ATTENTION MAPS,0.5555555555555556,"We investigate the impact of the thresholds (T) in the process of attention map generation, on the
performance of CheXT for disease classiÔ¨Åcation. Figure 4A summarizes the AUC comparison of
CheXT for different values of T. Higher values of T imply smaller bounding boxes to extract
radiomic features. During our experiments, we found that CheXT performs worse when very large
bounding boxes are generated. This observation meets our expectations since radiomic features are
very subtle to bounding boxes. SpeciÔ¨Åcally, the extracted radiomic features are more robust if more
focal and accurate bounding boxes are given."
EFFECT OF CONTRASTIVE LEARNING,0.5608465608465608,"4.3.4
EFFECT OF CONTRASTIVE LEARNING"
EFFECT OF CONTRASTIVE LEARNING,0.5661375661375662,"We also investigated the impact of contrastive learning. SpeciÔ¨Åcally, we check the performance
of CheXT for disease classiÔ¨Åcation by varying Œª of equation (3). Figure 4B summarizes the AUC"
EFFECT OF CONTRASTIVE LEARNING,0.5714285714285714,Under review as a conference paper at ICLR 2022
EFFECT OF CONTRASTIVE LEARNING,0.5767195767195767,"0.1
0.3
0.5
0.7
0.9
0.74 0.76 0.78 0.80 0.82 0.84 0.86 T AUC"
EFFECT OF CONTRASTIVE LEARNING,0.582010582010582,"0.5 0.6 0.7
0.8 0.9 1.0
0.74 0.76 0.78 0.80 0.82 0.84 0.86 Œª AUC"
EFFECT OF CONTRASTIVE LEARNING,0.5873015873015873,"(A)
(B)"
EFFECT OF CONTRASTIVE LEARNING,0.5925925925925926,Figure 4: AUC comparison for varying T in (A) attention map generation and (B) Œª in Equation 3.
EFFECT OF CONTRASTIVE LEARNING,0.5978835978835979,"comparison of CheXT for different values of Œª. Higher values of Œª implies lower weight to contrastive
loss. During our experiments, we found that CheXT performs worse when small weight (1%) is given
to the contrastive loss. CheXT‚Äôs performance improves when we increase contrastive loss weight,
but after a certain point, it starts decreasing. This proves our hypothesis that both contrastive and
focal losses are important. In a calculated ratio, they help CheXT to learn both disease-level and
patient-level discriminative visual features."
ABNORMALITY LOCALIZATION,0.6031746031746031,"4.4
ABNORMALITY LOCALIZATION"
ABNORMALITY LOCALIZATION,0.6084656084656085,"The NIH Chest X-ray dataset has 880 images labeled by radiologists with the bounding box informa-
tion. We have used this dataset to evaluate the performance of CheXT for abnormality localization.
Many prior works (Li et al., 2018; Liu et al., 2019a) have used a fraction of ground truth (GT) bound-
ing boxes for training and evaluated their system on the remaining. To ensure a robust evaluation,
we do not use any GT for training. Table 2 presents our evaluation results on all 880 images. We
used (Wang et al., 2017) as our baseline to compare our localization results since it has the same
experimental settings."
EVALUATION METRIC,0.6137566137566137,"4.4.1
EVALUATION METRIC"
EVALUATION METRIC,0.6190476190476191,"For localization, we evaluated our detected regular rectangular regions against the annotated bounding
boxes, using intersection over union ratio (IoU). Our localization results are only calculated for 880
images which have ground truth annotation for 8 diseases. The localization is deÔ¨Åned as correct only
if IoU > T(IoU). We evaluated CheXT for different thresholds ranging from {0.1, 0.2, 0.3, 0.4, 0.5,
0.6, 0.7} as shown in Table 2. A higher IoU threshold is preferred for disease localization because
clinical usage requires high accuracy."
COMPARISON WITH THE SOTA MODELS,0.6243386243386243,"4.4.2
COMPARISON WITH THE SOTA MODELS"
COMPARISON WITH THE SOTA MODELS,0.6296296296296297,"We compared disease localization accuracy under varying IoU, with baselines having similar settings
as CheXT (Table 2). Unlike other baselines(Li et al., 2018; Liu et al., 2019a) that use a portion
of 880 images for evaluation (because they need the remaining data for training), we used all 880
annotated images for evaluation. Therefore, no k-fold cross-validation for localization was performed.
CheXT average performance across 8 diseases is signiÔ¨Åcantly better than the baseline under all IoU
thresholds. When the thresholds of IoU is set to 0.1, CheXT outperforms the baseline (Wang et al.,
2017) in Cardiomegaly, InÔ¨Åltration, Mass, and Pneumonia. Even with higher thresholds, our model
performs superior to baseline. For example, when evaluated at T(IoU) = 0.5, our ‚ÄúCardiomegaly‚Äù
accuracy is still 32%, while the reference model achieves only 18%. Our ‚ÄúPneumonia‚Äù accuracy
is 12%, while the reference model has only 3% accuracy. Note that some diseases can appear at
multiple places, but ground truth might have mentioned only one location. This can signiÔ¨Åcantly
impact the accuracy at high thresholds. More importantly, we also add the ViT as our additional
baseline here, the results show that the radiomics branch and BYOA module can help the model
learn more important regions, which also improves the model‚Äôs interpretability. Few examples of the
localization results of ViT and CheXT are shown in Figure 5."
COMPARISON WITH THE SOTA MODELS,0.6349206349206349,Under review as a conference paper at ICLR 2022
COMPARISON WITH THE SOTA MODELS,0.6402116402116402,"Atlectasis
Cardiomegaly
Effusion
Infiltration"
COMPARISON WITH THE SOTA MODELS,0.6455026455026455,"Mass
Nodule
Pneumonia
Pneumothorax"
COMPARISON WITH THE SOTA MODELS,0.6507936507936508,"ViT
CheXT
ViT
CheXT
ViT
CheXT
ViT
CheXT"
COMPARISON WITH THE SOTA MODELS,0.656084656084656,"Figure 5: Examples of visualization of localization on the test images. The attention maps are
generated from the self-attention maps of the CLS token. The ground-truth bounding boxes are shown
in blue. The left image in each pair is the localization result of ViT (Dosovitskiy et al., 2020). The
right one is our localization results. All examples are positive for corresponding disease labels. Best
viewed in color."
COMPARISON WITH THE SOTA MODELS,0.6613756613756614,"T(IoU)
Model
Atelectasis Cardiomegaly Effusion InÔ¨Åltration Mass Nodule Pneumonia
Pneumothorax Mean"
COMPARISON WITH THE SOTA MODELS,0.6666666666666666,"0.1
(Wang et al., 2017)
0.69
0.94
0.66
0.71
0.40
0.14
0.63
0.38
0.569
ViT
0.58
0.91
0.61
0.77
0.44
0.11
0.75
0.25
0.553
CheXT
0.61
0.95
0.65
0.82
0.50
0.13
0.79
0.28
0.591"
COMPARISON WITH THE SOTA MODELS,0.671957671957672,"0.2
(Wang et al., 2017)
0.47
0.68
0.45
0.48
0.26
0.05
0.35
0.23
0.371
ViT
0.38
0.85
0.39
0.55
0.24
0.01
0.51
0.15
0.385
CheXT
0.41
0.91
0.41
0.59
0.26
0.05
0.57
0.19
0.424"
COMPARISON WITH THE SOTA MODELS,0.6772486772486772,"0.3
(Wang et al., 2017)
0.24
0.46
0.30
0.28
0.15
0.04
0.17
0.13
0.221
ViT
0.20
0.45
0.19
0.32
0.06
0.00
0.21
0.02
0.181
CheXT
0.28
0.79
0.22
0.38
0.12
0.01
0.41
0.05
0.283"
COMPARISON WITH THE SOTA MODELS,0.6825396825396826,"0.4
(Wang et al., 2017)
0.09
0.28
0.20
0.12
0.07
0.01
0.08
0.07
0.115
ViT
0.10
0.21
0.03
0.05
0.02
0.00
0.04
0.00
0.056
CheXT
0.17
0.54
0.13
0.18
0.07
0.01
0.26
0.02
0.173"
COMPARISON WITH THE SOTA MODELS,0.6878306878306878,"0.5
(Wang et al., 2017)
0.05
0.18
0.11
0.07
0.01
0.01
0.03
0.03
0.061
ViT
0.05
0.15
0.01
0.04
0.02
0.00
0.03
0.00
0.034
CheXT
0.08
0.32
0.05
0.09
0.05
0.00
0.12
0.01
0.090"
COMPARISON WITH THE SOTA MODELS,0.6931216931216931,"0.6
(Wang et al., 2017)
0.02
0.08
0.05
0.02
0.00
0.01
0.02
0.03
0.029
ViT
0.01
0.03
0.01
0.01
0.01
0.00
0.01
0.00
0.010
CheXT
0.02
0.15
0.03
0.04
0.03
0.00
0.06
0.00
0.041"
COMPARISON WITH THE SOTA MODELS,0.6984126984126984,"0.7
(Wang et al., 2017)
0.01
0.03
0.02
0.00
0.00
0.00
0.01
0.02
0.011
ViT
0.00
0.00
0.00
0.01
0.00
0.00
0.00
0.00
0.001
CheXT
0.01
0.04
0.01
0.02
0.01
0.00
0.03
0.00
0.015"
COMPARISON WITH THE SOTA MODELS,0.7037037037037037,"Table 2:
Disease localization under varying IoU on the NIH Chest X-ray dataset. Please note
that since our model doesn‚Äôt use any ground truth bounding box information, to fairly evaluate the
performance of our model, we only consider the previous methods‚Äô results under the same setting,"
CONCLUSION,0.708994708994709,"5
CONCLUSION"
CONCLUSION,0.7142857142857143,"In this paper, we propose an end-to-end knowledge-guided cross-attention Transformer, named
CheXT that can jointly model abnormality classiÔ¨Åcation and localization without the supervision
of localization annotation of chest X-rays. Our approach differs from previous studies in the
choice of universal modeling transformer, the use of radiomic features as prior knowledge, and a
feedback loop for image and radiomic features to mutually interact with each other. Additionally,
the project aims to mitigate current gaps in radiology by making prior knowledge more accessible to
image data analytic and diagnostic assisting tools, with the hope that this will increase the model‚Äôs
interpretability. Experimental results demonstrate that our method outperforms the state-of-the-art
algorithms, especially for the disease localization task, where our method can generate more accurate
bounding boxes."
CONCLUSION,0.7195767195767195,Under review as a conference paper at ICLR 2022
REFERENCES,0.7248677248677249,REFERENCES
REFERENCES,0.7301587301587301,"Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing
Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text,
2021."
REFERENCES,0.7354497354497355,"Xiang Bai, Cong Fang, Yu Zhou, Song Bai, Zaiyi Liu, Liming Xia, Qianlan Chen, Yongchao Xu,
Tian Xia, Shi Gong, et al. Predicting covid-19 malignant progression with ai techniques. 2020."
REFERENCES,0.7407407407407407,"David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:
Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 6541‚Äì6549, 2017."
REFERENCES,0.746031746031746,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.7513227513227513,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer
Vision, pp. 213‚Äì229. Springer, 2020."
REFERENCES,0.7566137566137566,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.7619047619047619,"Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision
transformer for image classiÔ¨Åcation. arXiv preprint arXiv:2103.14899, 2021a."
REFERENCES,0.7671957671957672,"Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille,
and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation.
arXiv preprint arXiv:2102.04306, 2021b."
REFERENCES,0.7724867724867724,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597‚Äì1607. PMLR, 2020."
REFERENCES,0.7777777777777778,"Xiaojian Chen, Kiyoko Oshima, Diane Schott, Hui Wu, William Hall, Yingqiu Song, Yalan Tao,
Dingjie Li, Cheng Zheng, Paul Knechtges, et al. Assessment of treatment response during
chemoradiation therapy for pancreatic cancer based on quantitative radiomic analysis of daily cts:
An exploratory study. PLoS One, 12(6):e0178961, 2017."
REFERENCES,0.783068783068783,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.7883597883597884,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.7936507936507936,"Armin Eilaghi, Sameer Baig, Yucheng Zhang, Junjie Zhang, Paul Karanicolas, Steven Gallinger,
Farzad Khalvati, and Masoom A Haider. Ct texture features are associated with overall survival
in pancreatic ductal adenocarcinoma‚Äìa quantitative analysis. BMC medical imaging, 17(1):1‚Äì7,
2017."
REFERENCES,0.798941798941799,"Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer
features of a deep network. University of Montreal, 1341(3):1, 2009."
REFERENCES,0.8042328042328042,"FawcettTom. An introduction to roc analysis. Pattern Recognition Letters, 2006."
REFERENCES,0.8095238095238095,"Biswajoy Ghosh, Nikhil Kumar, Nitisha Singh, Anup K Sadhu, Nirmalya Ghosh, Pabitra Mitra, and
Jyotirmoy Chatterjee. A quantitative lung computed tomography image feature for multi-center
severity assessment of covid-19. medRxiv, 2020."
REFERENCES,0.8148148148148148,Under review as a conference paper at ICLR 2022
REFERENCES,0.8201058201058201,"Robert J Gillies, Paul E Kinahan, and Hedvig Hricak. Radiomics: images are more than pictures,
they are data. Radiology, 278(2):563‚Äì577, 2016."
REFERENCES,0.8253968253968254,"Yan Han, Chongyan Chen, Liyan Tang, Mingquan Lin, Ajay Jaiswal, Song Wang, Ahmed TewÔ¨Åk,
George Shih, Ying Ding, and Yifan Peng. Using radiomics as prior knowledge for thorax disease
classiÔ¨Åcation and localization in chest x-rays. arXiv preprint arXiv:2011.12506, 2020."
REFERENCES,0.8306878306878307,"Yan Han, Chongyan Chen, Ahmed H TewÔ¨Åk, Ying Ding, and Yifan Peng. Pneumonia detection on
chest x-ray using radiomic features and contrastive learning. arXiv preprint arXiv:2101.04269,
2021."
REFERENCES,0.8359788359788359,"Peng Huang, Seyoun Park, Rongkai Yan, Junghoon Lee, Linda C Chu, Cheng T Lin, Amira Hussien,
Joshua Rathmell, Brett Thomas, Chen Chen, et al. Added value of computer-aided ct image
features for early lung cancer diagnosis with small pulmonary nodules: a matched case-control
study. Radiology, 286(1):286‚Äì295, 2018."
REFERENCES,0.8412698412698413,"Pulkit Kumar, Monika Grewal, and Muktabh Mayank Srivastava. Boosted cascaded convnets for
multilabel classiÔ¨Åcation of thoracic diseases in chest radiographs. In International Conference
Image Analysis and Recognition, pp. 546‚Äì552. Springer, 2018."
REFERENCES,0.8465608465608465,"Zhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei, Li-Jia Li, and Li Fei-Fei. Thoracic disease
identiÔ¨Åcation and localization with limited supervision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 8290‚Äì8299, 2018."
REFERENCES,0.8518518518518519,"Chenxi Liu, Junhua Mao, Fei Sha, and Alan Yuille. Attention correctness in neural image captioning.
In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 31, 2017."
REFERENCES,0.8571428571428571,"Jingyu Liu, Gangming Zhao, Yu Fei, Ming Zhang, Yizhou Wang, and Yizhou Yu. Align, attend
and locate: Chest x-ray diagnosis via contrast induced attention network with limited supervision.
In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October
2019a."
REFERENCES,0.8624338624338624,"Jingyu Liu, Gangming Zhao, Yu Fei, Ming Zhang, Yizhou Wang, and Yizhou Yu. Align, attend and
locate: Chest x-ray diagnosis via contrast induced attention network with limited supervision. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10632‚Äì10641,
2019b."
REFERENCES,0.8677248677248677,"Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5188‚Äì5196, 2015."
REFERENCES,0.873015873015873,"Haidy Nasief, Cheng Zheng, Diane Schott, William Hall, Susan Tsai, Beth Erickson, and X Allen Li.
A machine learning based delta-radiomics process for early prediction of treatment response of
pancreatic cancer. NPJ precision oncology, 3(1):1‚Äì10, 2019."
REFERENCES,0.8783068783068783,"Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa,
Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net:
Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018."
REFERENCES,0.8835978835978836,"Sangjoon Park, Gwanghyun Kim, Yujin Oh, Joon Beom Seo, Sang Min Lee, Jin Hwan Kim, Sungjun
Moon, Jae-Kwang Lim, and Jong Chul Ye. Vision transformer using low-level chest x-ray feature
corpus for covid-19 diagnosis and severity quantiÔ¨Åcation. arXiv preprint arXiv:2104.07235, 2021."
REFERENCES,0.8888888888888888,"Kitsuchart Pasupa, Supawit Vatathanavaro, and Suchat Tungjitnob. Convolutional neural networks
based focal loss for class imbalance problem: A case study of canine red blood cells morphology
classiÔ¨Åcation. Journal of Ambient Intelligence and Humanized Computing, pp. 1‚Äì17, 2020."
REFERENCES,0.8941798941798942,"Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri, Ronald Summers, and Zhiyong Lu.
NegBio: a high-performance tool for negation and uncertainty detection in radiology reports. In
AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational
Science, volume 2017, pp. 188‚Äì196, 2018."
REFERENCES,0.8994708994708994,Under review as a conference paper at ICLR 2022
REFERENCES,0.9047619047619048,"Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding,
Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist-level pneumonia
detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225, 2017."
REFERENCES,0.91005291005291,"Eyal Rozenberg, Daniel Freedman, and Alex Bronstein. Localization with limited annotation for
chest x-rays. In Machine Learning for Health Workshop, pp. 52‚Äì65. PMLR, 2020."
REFERENCES,0.9153439153439153,"Ahmet Saygƒ±lƒ±. A new approach for computer-aided detection of coronavirus (covid-19) from ct and
x-ray images using machine learning methods. Applied Soft Computing, 105:107323, 2021."
REFERENCES,0.9206349206349206,"Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh,
and Dhruv Batra. Grad-cam: Why did you say that? arXiv preprint arXiv:1611.07450, 2016."
REFERENCES,0.9259259259259259,"F Shi, L Xia, F Shan, et al. Large-scale screening of covid-19 from community acquired pneumonia
using infection size-aware classiÔ¨Åcation. arxiv e-prints [preprint] 2020."
REFERENCES,0.9312169312169312,"Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer:
Gated axial-attention for medical image segmentation. arXiv preprint arXiv:2102.10662, 2021."
REFERENCES,0.9365079365079365,"Joost JM Van Griethuysen, Andriy Fedorov, Chintan Parmar, Ahmed Hosny, Nicole Aucoin, Vivek
Narayan, Regina GH Beets-Tan, Jean-Christophe Fillion-Robin, Steve Pieper, and Hugo JWL
Aerts. Computational radiomics system to decode the radiographic phenotype. Cancer research,
77(21):e104‚Äìe107, 2017."
REFERENCES,0.9417989417989417,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998‚Äì6008, 2017."
REFERENCES,0.9470899470899471,"Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.
Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classi-
Ô¨Åcation and localization of common thorax diseases. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2097‚Äì2106, 2017."
REFERENCES,0.9523809523809523,"Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M Summers. Tienet: Text-image
embedding network for common thorax disease classiÔ¨Åcation and reporting in chest x-rays. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9049‚Äì9058,
2018."
REFERENCES,0.9576719576719577,"Xudong Wang, Shizhong Han, Yunqiang Chen, Dashan Gao, and Nuno Vasconcelos. Volumetric
attention for 3d medical image segmentation and detection. In International Conference on Medical
Image Computing and Computer-Assisted Intervention, pp. 175‚Äì184. Springer, 2019."
REFERENCES,0.9629629629629629,"Yirui Wang, Kang Zheng, Chi-Tung Cheng, Xiao-Yun Zhou, Zhilin Zheng, Jing Xiao, Le Lu, Chien-
Hung Liao, and Shun Miao. Knowledge distillation with adaptive asymmetric label sharpening for
semi-supervised fracture detection in chest x-rays. In International Conference on Information
Processing in Medical Imaging, pp. 599‚Äì610. Springer, 2021."
REFERENCES,0.9682539682539683,"Li Yao, Eric Poblenz, Dmitry Dagunts, Ben Covington, Devon Bernard, and Kevin Lyman. Learning to
diagnose from scratch by exploiting dependencies among labels. arXiv preprint arXiv:1710.10501,
2017."
REFERENCES,0.9735449735449735,"Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,
and Tie-Yan Liu. Do transformers really perform bad for graph representation? arXiv preprint
arXiv:2106.05234, 2021."
REFERENCES,0.9788359788359788,"Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei, Philip Torr, Wayne Zhang, and Dahua Lin.
Vision transformer with progressive sampling. arXiv preprint arXiv:2108.01684, 2021."
REFERENCES,0.9841269841269841,"Yuhao Zhang, Daisy Yi Ding, Tianpei Qian, Christopher D Manning, and Curtis P Langlotz. Learning
to summarize radiology Ô¨Åndings. arXiv preprint arXiv:1809.04698, 2018."
REFERENCES,0.9894179894179894,"Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:
Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020."
REFERENCES,0.9947089947089947,"Alex Zwanenburg, Stefan Leger, Martin Valli√®res, and Steffen L√∂ck. Image biomarker standardisation
initiative. arXiv preprint arXiv:1612.07003, 2016."
