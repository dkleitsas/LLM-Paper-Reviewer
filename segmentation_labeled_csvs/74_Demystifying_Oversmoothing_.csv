Section,Section Appearance Order,Paragraph
ABSTRACT,0.0033783783783783786,Abstract
ABSTRACT,0.006756756756756757,"Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon
where increasing network depth leads to homogeneous node representations.
While previous work has established that Graph Convolutional Networks (GCNs)
exponentially lose expressive power, it remains controversial whether the graph
attention mechanism can mitigate oversmoothing. In this work, we provide a
definitive answer to this question, by viewing attention-based GNNs as nonlinear
time-varying dynamical systems and incorporating tools and techniques from the
theory of products of inhomogeneous matrices and the joint spectral radius. We
establish that, contrary to popular belief, the graph attention mechanism cannot
prevent oversmoothing and loses expressive power exponentially. The proposed
framework extends the existing results on oversmoothing for symmetric GCNs
to a significantly broader class of GNN models, including random walk GCNs,
Graph Attention Networks (GATs) and (graph) transformers. In particular, our
analysis accounts for asymmetric, state-dependent and time-varying aggregation
operators and a wide range of common nonlinear activation functions, such as
ReLU, LeakyReLU, GELU and SiLU."
INTRODUCTION,0.010135135135135136,"1
Introduction"
INTRODUCTION,0.013513513513513514,"Graph neural networks (GNNs) have emerged as a powerful framework for learning with graph-
structured data [1â€“7]. Most GNN models follow the message-passing paradigm [8], where the
representation of each node is computed by recursively aggregating and transforming the representa-
tions of its neighboring nodes."
INTRODUCTION,0.016891891891891893,"One notable drawback of repeated message-passing is oversmoothing, which refers to the phenomenon
that stacking message-passing GNN layers makes node representations of the same connected
component converge to the same vector [6, 9â€“14]. As a result, most GNNs used in practice remain
relatively shallow and often only have few layers [6, 7, 15]. On the theory side, while previous works
have shown that the symmetric Graph Convolution Networks (GCNs) with ReLU and LeakyReLU
nonlinearities exponentially lose expressive power, analyzing the oversmoothing phenomenon in
other types of GNNs is still an open question [10, 11]. In particular, the question of whether the
graph attention mechanism can prevent oversmoothing has not been settled yet. Motivated by the
capacity of graph attention to distinguish the importance of different edges in the graph, some works
claim that oversmoothing is alleviated in Graph Attention Networks (GATs), heuristically crediting
to GATsâ€™ ability to learn adaptive node-wise aggregation operators via the attention mechanism [16].
On the other hand, it has been empirically observed that similar to the case of GCNs, oversmoothing
seems inevitable for attention-based GNNs such as GATs or (graph) transformers [14, 17]."
INTRODUCTION,0.02027027027027027,"In this work, we provide a definitive answer to this question â€” attention-based GNNs also lose
expressive power exponentially, albeit potentially at a slower exponential rate compared to GCNs.
Given that attention-based GNNs can be viewed as nonlinear time-varying dynamical systems, our
analysis is built on the theory of products of inhomogeneous matrices [18, 19] and the concept
of joint spectral radius [20], as these methods have been long proved effective in the analysis of"
INTRODUCTION,0.02364864864864865,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
INTRODUCTION,0.02702702702702703,"time-inhomogeneous markov chains and ergodicity of dynamical systems [18, 19, 21]. Our approach
generalizes the existing results on oversmoothing for symmetric GCNs to a significantly broader
class of GNN models with asymmetric, state-dependent and time-varying aggregation operators
and nonlinear activation functions under general conditions. In particular, our analysis accounts for
asymmetric, state-dependent and time-varying aggregation operators and a wide range of common
nonlinearities such as ReLU, LeakyReLU, and even non-monotone ones like GELU and SiLU."
IMPLEMENTATION/METHODS,0.030405405405405407,"2
Problem Setup"
IMPLEMENTATION/METHODS,0.033783783783783786,"2.1
Graph attention mechanism and attention-based GNNs"
IMPLEMENTATION/METHODS,0.037162162162162164,"Let G be a graph with N nodes. Given node representation vectors Xi, Xj âˆˆRd, we use an attention
function Î¨ : Rdâ€² Ã— Rdâ€² â†’R to compute a raw attention coefficient eij = Î¨(W âŠ¤Xi, W âŠ¤Xj), W âˆˆ
RdÃ—dâ€² that indicates the importance of node jâ€™s features to node i. Then the graph structure is injected
into the mechanism by performing masked attention normalized using the softmax function, where for
each node i, we only compute its attention to its neighbors: Pij = softmaxj(eij) =
exp(eij)
P"
IMPLEMENTATION/METHODS,0.04054054054054054,kâˆˆNi exp(eik) .
IMPLEMENTATION/METHODS,0.04391891891891892,"The matrix P, where the ijth entry is Pij, is a row stochastic matrix. We refer to P as an aggregation
operator in message-passing."
IMPLEMENTATION/METHODS,0.0472972972972973,"The update rule of a single graph attentional layer can be written as Xâ€² = Ïƒ(PXW) , where Ïƒ(Â·) is
a pointwise nonlinearity function, and the aggregation operator P is a function of XW. As a result,
the output of the tth graph attentional layers can be written as X(t+1) = Ïƒ(P (t)X(t)W (t)), where
X(0) = X âˆˆRNÃ—d is the input node features, W (t) âˆˆRdâ€²Ã—dâ€² for t âˆˆN and W (0) âˆˆRdÃ—dâ€². For the
rest of this work, without loss of generality, we assume that d = dâ€²."
IMPLEMENTATION/METHODS,0.05067567567567568,"2.2
Measure of Oversmoothing"
IMPLEMENTATION/METHODS,0.05405405405405406,"We establish our results on oversmoothing for attention-based GNNs using the following node
similarity measure Âµ : RNÃ—d â†’Râ‰¥0, satisfying the criteria proposed in Rusch et al. [14]: Âµ(X) :=
âˆ¥X âˆ’1Î³Xâˆ¥F , where 1 âˆˆRN is the all-one vector, Î³X = 1âŠ¤X"
IMPLEMENTATION/METHODS,0.057432432432432436,"N
. In particular, Âµ(X) = 0 if and only
if all node representations converge to the same vector. Then oversmoothing with respect to Âµ is
defined as the layer-wise exponential convergence of the node-similarity measure Âµ to zero, i.e. for
t âˆˆN, with constants C1, C2 > 0,
Âµ(X(t)) â‰¤C1eâˆ’C2t.
(1)
We note that our analysis directly applies to any Lipschitz node similarity measure, including the
popular Dirichlet energy [11, 14]."
IMPLEMENTATION/METHODS,0.060810810810810814,"2.3
Assumptions"
IMPLEMENTATION/METHODS,0.06418918918918919,"We make the following assumptions (in fact, quite minimal) in deriving our results:"
IMPLEMENTATION/METHODS,0.06756756756756757,"A1 The graph G is connected and has a self-loop at each node.
A2 The attention function Î¨(Â·, Â·) is continuous.
A3 The sequence {âˆ¥Qk
t=0 |W (t)|âˆ¥max}âˆž
k=0 is bounded.
A4 The point-wise nonlinear activation function Ïƒ(Â·) satisfies 0 â‰¤Ïƒ(x)"
IMPLEMENTATION/METHODS,0.07094594594594594,"x
â‰¤1 for x Ì¸= 0 and Ïƒ(0) = 0."
IMPLEMENTATION/METHODS,0.07432432432432433,"We note that all of these assumptions are either standard or quite general. The assumptions on the
GNN architecture A2 and A4 can be easily verified for commonly used GNN designs. For example,
the attention functions used in the GAT [7], GATv2 [22], and transformers [23] are specific cases
that all satisfy A2. As for A4, one way to satisfy it is to have Ïƒ be 1-Lipschitz and Ïƒ(x) â‰¤0 for
x < 0 and Ïƒ(x) â‰¥0 for x > 0. Then it is easy to verify that most of the commonly used nonlinear
activation functions such as ReLU, LeakyReLU, GELU, SiLU, ELU, tanh all satisfy A4."
RESULTS/EXPERIMENTS,0.0777027027027027,"3
Main Results"
RESULTS/EXPERIMENTS,0.08108108108108109,"3.1
Common connectivity structure among aggregation operators across different layers"
RESULTS/EXPERIMENTS,0.08445945945945946,Through writing the ith column of X(t+1) as
RESULTS/EXPERIMENTS,0.08783783783783784,"X(t+1)
Â·i
=
X"
RESULTS/EXPERIMENTS,0.09121621621621621,"jt+1=i, (jt,...,j0)âˆˆ[d]t+1 tY"
RESULTS/EXPERIMENTS,0.0945945945945946,"k=0
W (k)
jkjk+1 !"
RESULTS/EXPERIMENTS,0.09797297297297297,"D(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 ,
(2)"
RESULTS/EXPERIMENTS,0.10135135135135136,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
RESULTS/EXPERIMENTS,0.10472972972972973,"where D(t)
i
is a diagonal matrix representing the effect of Ïƒ(Â·) to the ith column of P (t)X(t)W (t), we
can show the boundedness of the node representationsâ€™ trajectories X(t) for all t â‰¥0. We define D
to be the set of all possible diagonal matrices D(t)
i
satisfying A4: D := {diag(d) : d âˆˆRN, 0 â‰¤ew
d â‰¤ew 1} . Then we establish the following key lemma suggesting that the graph attention mechanism
cannot fundamentally change the connectivity pattern of the graph.
Lemma 1. Under A2-A4, there exists Ïµ > 0 such that for all t â‰¥0 and for any nodes i, j that are
connected, we have P (t)
ij â‰¥Ïµ."
RESULTS/EXPERIMENTS,0.10810810810810811,"We define the family of row-stochastic matrices satisfying Lemma 1 below.
Definition 1. Let Ïµ > 0. We define PG,Ïµ to be the set of row-stochastic matrices satisfying the
following conditions: 1. Ïµ â‰¤Pij â‰¤1, for i, j connected; 2. Pij = 0, for i, j not connected."
RESULTS/EXPERIMENTS,0.11148648648648649,"3.2
Ergodicity of infinite products of matrices"
RESULTS/EXPERIMENTS,0.11486486486486487,"Ergodicity, in its most general form, deals with the long-term behavior of dynamical systems. The
oversmoothing phenomenon in GNNs defined in the sense of (1) concerns the convergence of all
rows of X(t) to a common vector at an exponential rate. To this end, we define ergodicity in our
analysis as the convergence of infinite matrix products to a rank-one matrix with identical rows.
Definition 2 (Ergodicity). Let B âˆˆR(Nâˆ’1)Ã—N be the orthogonal projection onto the space or-
thogonal to span{1}. A sequence of matrices {M (n)}âˆž
n=0 is ergodic if lim
tâ†’âˆžB Qt
n=0 M (n) = 0 ."
RESULTS/EXPERIMENTS,0.11824324324324324,"We will take advantage of the following properties of the projection matrix B already established in
Blondel et al. [21]:1. B1 = 0; 2. âˆ¥Bxâˆ¥2 = âˆ¥xâˆ¥2 for x âˆˆRN if xâŠ¤1 = 0; 3. For M âˆˆRNÃ—N, there
exists a unique matrix Ëœ
M âˆˆR(Nâˆ’1)Ã—(Nâˆ’1) such that BM = Ëœ
MB ."
RESULTS/EXPERIMENTS,0.12162162162162163,"Let MG,Ïµ := {DP : D âˆˆD, P âˆˆPG,Ïµ} . Then any infinite product of matrices in MG,Ïµ is ergodic.
Lemma 2. Any sequence {D(t)P (t)}âˆž
t=0 in MG,Ïµ is ergodic."
RESULTS/EXPERIMENTS,0.125,"3.3
Joint spectral radius"
RESULTS/EXPERIMENTS,0.12837837837837837,"Finally, we make use of the concept of the joint spectral radius for a set of matrices [20] and employ
it to deduce exponential convergence of node representations from our ergodicity result, Lemma 2.
Definition 3 (Joint Spectral Radius). For a collection of matrices M, the joint spectral radius
JSR(M) is defined to be"
RESULTS/EXPERIMENTS,0.13175675675675674,"JSR(M) = lim sup
kâ†’âˆž
sup
M1,M2,...,MkâˆˆM
âˆ¥M1M2...Mkâˆ¥
1
k ,"
RESULTS/EXPERIMENTS,0.13513513513513514,and it is independent of the norm used.
RESULTS/EXPERIMENTS,0.13851351351351351,"To analyze the convergence rate of products of matrices in MG,Ïµ to a rank-one matrix with identical
rows, we investigate the dynamics induced by the matrices on the subspace orthogonal to span{1}.
More precisely, with the notion of ergodicity in Definition 2 and the goal of studying the convergence
rate of a matrix product BM1M2 . . . Mk where each Mi âˆˆMG,Ïµ, we use the third property of the
orthogonal projection B to write"
RESULTS/EXPERIMENTS,0.14189189189189189,"BM1M2 . . . Mk = Ëœ
M1 Ëœ
M2... Ëœ
MkB ,"
RESULTS/EXPERIMENTS,0.14527027027027026,"where each Ëœ
Mi is the unique matrix in R(Nâˆ’1)Ã—(Nâˆ’1) that satisfies BMi =
Ëœ
MiB. To analyze
products of such matrices Ëœ
Mi, let us define Ëœ
MG,Ïµ := { Ëœ
M : BM = Ëœ
MB, M âˆˆMG,Ïµ}. We can use
the ergodictiy result developed in Lemma 2 to show that the joint spectral radius of Ëœ
MG,Ïµ is strictly
less than 1.
Lemma 3. Let 0 < Ïµ < 1. Under assumptions A1-A4, JSR( Ëœ
MG,Ïµ) < 1."
RESULTS/EXPERIMENTS,0.14864864864864866,"It follows from the definition of the joint spectral radius that if JSR( Ëœ
MG,Ïµ) < 1, for any
JSR( Ëœ
MG,Ïµ) < q < 1, there exists a C for which"
RESULTS/EXPERIMENTS,0.15202702702702703,"âˆ¥Ëœ
M1 Ëœ
M2... Ëœ
Mkyâˆ¥â‰¤Cqkâˆ¥yâˆ¥
(3)"
RESULTS/EXPERIMENTS,0.1554054054054054,"for all y âˆˆRNâˆ’1 and Ëœ
M1, Ëœ
M2, ..., Ëœ
Mk âˆˆËœ
MG,Ïµ."
RESULTS/EXPERIMENTS,0.15878378378378377,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
RESULTS/EXPERIMENTS,0.16216216216216217,"3.4
Main Theorem"
RESULTS/EXPERIMENTS,0.16554054054054054,"Applying (3) to the recursive expansion of X(t+1)
Â·i
in (2) using the 2-norm, we can prove the
exponential convergence of Âµ(X(t)) to zero for the similarity measure Âµ(Â·) defined in (1), which in
turn implies the convergence of node representations to a common representation at an exponential
rate. This completes the proof of the main result of this paper, which states that oversmoothing
defined in (1) is unavoidable for attention-based GNNs.
Theorem 1 (Oversmoothing happens exponentially in attention-based GNNs). Under assump-
tions A1-A4, JSR( Ëœ
MG,Ïµ) < 1 and for any q satisfying JSR( Ëœ
MG,Ïµ) < q < 1, there exists C1(q) > 0
such that
Âµ(X(t)) â‰¤C1qt , âˆ€t â‰¥0 ,"
RESULTS/EXPERIMENTS,0.16891891891891891,where Âµ(X) = âˆ¥X âˆ’11âŠ¤X
RESULTS/EXPERIMENTS,0.17229729729729729,"N
âˆ¥F . As a result, node representations X(t) exponentially converge to the
same value as the model depth t â†’âˆž."
RESULTS/EXPERIMENTS,0.17567567567567569,"3.5
Comparison with the GCN"
RESULTS/EXPERIMENTS,0.17905405405405406,"Computing or approximating the joint spectral radius for a given set of matrices is known to be hard
in general [24], yet it is straightforward to lower bound JSR( Ëœ
MG,Ïµ)."
RESULTS/EXPERIMENTS,0.18243243243243243,"Proposition 1. Let Î» be the second largest eigenvalue of Dâˆ’1/2
deg ADâˆ’1/2
deg . Then under assumptions
A1-A4, it holds that Î» â‰¤JSR( Ëœ
MG,Ïµ).
A direct consequence of the above result is that the upper bound q on the convergence rate that we get
for graph attention in Theorem 1 is at least as large as Î». On the other hand, previous work has already
established that in the graph convolution case, the convergence rate of Âµ(X(t)) is O(Î»t) [10, 11]. It
is thus natural to expect attention-based GNNs to potentially have better expressive power at finite
depth than GCNs, even though they both inevitably suffer from oversmoothing. This is also evident
from the numerical experiments that we present in the next section."
RESULTS/EXPERIMENTS,0.1858108108108108,"4
Numerical Experiments"
RESULTS/EXPERIMENTS,0.1891891891891892,"We validate our theoretical results on three real-world datasets: Cora, CiteSeer and PubMed [25]
with two attention-based GNN architectures (GAT [7] and random walk GCN (constant attention
function) [12, 13]) and five common nonlinearities. We ran each experiment 10 times. Figure 1
shows the evolution of Âµ(X(t)) in log-log scale on the largest connected component of each graph as
we forward pass the input X into a trained model. The solid curve is the average over 10 runs and the
band indicates one standard deviation around the average. We observe that oversmoothing happens
exponentially in both GCNs and GATs with the rates varying depending on the choice of activation
function. Notably, GCNs demonstrate faster rates of oversmoothing compared to GATs."
RESULTS/EXPERIMENTS,0.19256756756756757,"Figure 1: Evolution of Âµ(X(t)) (in log-log scale) on the largest connected component of three
benchmark datasets: Cora, Citeseer, and PubMed."
RESULTS/EXPERIMENTS,0.19594594594594594,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
RESULTS/EXPERIMENTS,0.19932432432432431,Acknowledgements
RESULTS/EXPERIMENTS,0.20270270270270271,"This research has been supported in part by ARO MURI W911NF-19-0217, ONR N00014-20-1-2394,
and the MIT-IBM Watson AI Lab."
REFERENCES,0.20608108108108109,References
REFERENCES,0.20945945945945946,"[1] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In
IJCNN, 2005. 1
[2] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20:61â€“80, 2009.
[3] Joan Bruna, Wojciech Zaremba, Arthur D. Szlam, and Yann LeCun. Spectral networks and
locally connected networks on graphs. In ICLR, 2014.
[4] David Kristjanson Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael GÃ³mez-
Bombarelli, Timothy D. Hirzel, AlÃ¡n Aspuru-Guzik, and Ryan P. Adams. Convolutional
networks on graphs for learning molecular fingerprints. In NeurIPS, 2015.
[5] MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In NeurIPS, 2016.
[6] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In ICLR, 2017. 1
[7] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua
Bengio. Graph attention networks. In ICLR, 2018. 1, 2, 4, 13
[8] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.
Neural message passing for quantum chemistry. In ICML, 2017. 1
[9] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In AAAI, 2018. 1
[10] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
node classification. In ICLR, 2020. 1, 4
[11] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. In ICML
Graph Representation Learning and Beyond (GRL+) Workshop, 2020. 1, 2, 4
[12] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing.
In NeurIPS, 2022. 4
[13] Xinyi Wu, Zhengdao Chen, William Wang, and Ali Jadbabaie. A non-asymptotic analysis of
oversmoothing in graph neural networks. In ICLR, 2023. 4
[14] T.Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A survey on oversmoothing
in graph neural networks. ArXiv, abs/2303.10993, 2023. 1, 2
[15] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 32:4â€“24, 2019. 1
[16] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in
graph convolutional networks. In NeurIPS, 2020. 1
[17] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S.
Lee, and James Tin-Yau Kwok. Revisiting over-smoothing in bert from the perspective of graph.
In ICLR, 2022. 1
[18] Darald J. Hartfiel. Nonhomogeneous Matrix Products. 2002. 1, 2, 8
[19] Eugene Seneta. Non-negative Matrices and Markov Chains. 2008. 1, 2, 8
[20] Gian-Carlo Rota and W. Gilbert Strang. A note on the joint spectral radius. 1960. 1, 3
[21] Vincent D. Blondel, Julien M. Hendrickx, Alexander Olshevsky, and John N. Tsitsiklis. Con-
vergence in multiagent coordination, consensus, and flocking. Proceedings of the 44th IEEE
Conference on Decision and Control, pages 2996â€“3000, 2005. 2, 3
[22] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In ICLR,
2022. 2"
REFERENCES,0.21283783783783783,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
REFERENCES,0.21621621621621623,"[23] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2"
REFERENCES,0.2195945945945946,"[24] John N. Tsitsiklis and Vincent D. Blondel. The Lyapunov exponent and joint spectral radius of
pairs of matrices are hardâ€”when not impossibleâ€”to compute and to approximate. Mathematics
of Control, Signals and Systems, 10:31â€“40, 1997. 4"
REFERENCES,0.22297297297297297,"[25] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. In ICML, 2016. 4"
REFERENCES,0.22635135135135134,"[26] David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times. 2008. 8"
REFERENCES,0.22972972972972974,"[27] Jacques Theys. Joint spectral radius: theory and approximations. Ph. D. dissertation, 2005. 11"
REFERENCES,0.23310810810810811,[28] Peter D. Lax. Functional Analysis. 2002. 13
REFERENCES,0.23648648648648649,"[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
KÃ¶pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In NeurIPS, 2019. 13"
REFERENCES,0.23986486486486486,"[30] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric.
In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. 13"
OTHER,0.24324324324324326,"A
Basic Facts about Matrix Norms"
OTHER,0.24662162162162163,"In this section, we list some basic facts about matrix norms that will be helpful in comprehending the
subsequent proofs."
OTHER,0.25,"A.1
Matrix norms induced by vector norms"
OTHER,0.2533783783783784,"Suppose a vector norm âˆ¥Â·âˆ¥Î± on Rn and a vector norm âˆ¥Â·âˆ¥Î² on Rm are given. Any matrix M âˆˆRmÃ—n
induces a linear operator from Rn to Rm with respect to the standard basis, and one defines the
corresponding induced norm or operator norm by"
OTHER,0.25675675675675674,"âˆ¥Mâˆ¥Î±,Î² = sup
âˆ¥Mvâˆ¥Î²"
OTHER,0.26013513513513514,"âˆ¥vâˆ¥Î±
, v âˆˆRn, v Ì¸= 0

."
OTHER,0.2635135135135135,"If the p-norm for vectors (1 â‰¤p â‰¤âˆž) is used for both spaces Rn and Rm, then the corresponding
operator norm is"
OTHER,0.2668918918918919,"âˆ¥Mâˆ¥p = sup
vÌ¸=0 âˆ¥Mvâˆ¥p"
OTHER,0.2702702702702703,"âˆ¥vâˆ¥p
."
OTHER,0.27364864864864863,The matrix 1-norm and âˆž-norm can be computed by
OTHER,0.27702702702702703,"âˆ¥Mâˆ¥1 = max
1â‰¤j m
X"
OTHER,0.28040540540540543,"i=1
|Mij| ,"
OTHER,0.28378378378378377,"that is, the maximum absolute column sum of the matrix M;"
OTHER,0.28716216216216217,"âˆ¥Mâˆ¥âˆž= max
1â‰¤m n
X"
OTHER,0.2905405405405405,"j=1
|Mij| ,"
OTHER,0.2939189189189189,"that is, the maximum absolute row sum of the matrix M."
OTHER,0.2972972972972973,"Remark.
In the special case of p = 2, the induced matrix norm âˆ¥Â· âˆ¥2 is called the spectral norm,
and is equal to the largest singular value of the matrix."
OTHER,0.30067567567567566,"For square matrices, we note that the name â€œspectral norm"" does not imply the quantity is directly
related to the spectrum of a matrix, unless the matrix is symmetric."
OTHER,0.30405405405405406,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.30743243243243246,"Example.
We give the following example of a stochastic matrix P, whose spectral radius is 1, but
its spectral norm is greater than 1."
OTHER,0.3108108108108108,"P =

0.9
0.1
0.25
0.75"
OTHER,0.3141891891891892,"
âˆ¥Pâˆ¥2 â‰ˆ1.0188"
OTHER,0.31756756756756754,"A.2
Matrix (p, q)-norms"
OTHER,0.32094594594594594,The Frobenius norm of a matrix M âˆˆRmÃ—n is defined as
OTHER,0.32432432432432434,âˆ¥Mâˆ¥F =
OTHER,0.3277027027027027,"v
u
u
t n
X j=1 m
X"
OTHER,0.3310810810810811,"i=1
|Mij|2 ,"
OTHER,0.3344594594594595,"and it belongs to a family of entry-wise matrix norms: for 1 â‰¤p, q â‰¤âˆž, the matrix (p, q)-norm is
defined as"
OTHER,0.33783783783783783,"âˆ¥Mâˆ¥p,q = ï£« ï£­
n
X j=1 m
X"
OTHER,0.34121621621621623,"i=1
|Mij|p
!q/pï£¶ ï£¸ 1/q ."
OTHER,0.34459459459459457,"The special case p = q = 2 is the Frobenius norm âˆ¥Â· âˆ¥F , and p = q = âˆžyields the max norm
âˆ¥Â· âˆ¥max."
OTHER,0.34797297297297297,"A.3
Equivalence of norms"
OTHER,0.35135135135135137,"For any two matrix norms âˆ¥Â· âˆ¥Î± and âˆ¥Â· âˆ¥Î², we have that for all matrices M âˆˆRmÃ—n,
râˆ¥Mâˆ¥Î± â‰¤âˆ¥Mâˆ¥Î² â‰¤sâˆ¥Mâˆ¥Î±
for some positive numbers r and s. In particular, the following inequality holds for the 2-norm âˆ¥Â· âˆ¥2
and the âˆž-norm âˆ¥Â· âˆ¥âˆž:
1
âˆšnâˆ¥Mâˆ¥âˆžâ‰¤âˆ¥Mâˆ¥2 â‰¤âˆšmâˆ¥Mâˆ¥âˆž."
OTHER,0.3547297297297297,"B
Proof of Lemma 1"
OTHER,0.3581081081081081,"We can use the formulation in (2) to show the boundedness of the node representationsâ€™ trajectories
X(t) for all t âˆˆNâ‰¥0, which in turn implies the boundedness of the input to graph attention in each
layer, X(t)W (t).
Lemma 4. Under assumptions A3-A4, there exists C > 0 such that âˆ¥X(t)âˆ¥max â‰¤C for all t âˆˆNâ‰¥0."
OTHER,0.3614864864864865,Proof of Lemma 4. According to the formulation (2):
OTHER,0.36486486486486486,"X(t+1)
Â·i
=
X"
OTHER,0.36824324324324326,"jt+1=i, (jt,...,j0)âˆˆ[d]t+1 tY"
OTHER,0.3716216216216216,"k=0
W (k)
jkjk+1 !"
OTHER,0.375,"D(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 ,"
OTHER,0.3783783783783784,we thus obtain that
OTHER,0.38175675675675674,"âˆ¥X(t+1)
Â·i
âˆ¥âˆž=  X"
OTHER,0.38513513513513514,"jt+1=i ,(jt,...,j0)âˆˆ[d]t+1 tY"
OTHER,0.3885135135135135,"k=0
W (k)
jkjk+1 !"
OTHER,0.3918918918918919,"D(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 âˆž â‰¤
X"
OTHER,0.3952702702702703,"jt+1=i ,(jt,...,j0)âˆˆ[d]t+1 tY k=0"
OTHER,0.39864864864864863,"W (k)
jkjk+1 "
OTHER,0.40202702702702703,"! 


D(t)
jt+1P (t)...D(0)
j1 P (0)


âˆž"
OTHER,0.40540540540540543,"X(0)
j0 âˆž â‰¤
X"
OTHER,0.40878378378378377,"jt+1=i ,(jt,...,j0)âˆˆ[d]t+1 tY k=0"
OTHER,0.41216216216216217,"W (k)
jkjk+1 "
OTHER,0.4155405405405405,"! 


X(0)
j0 âˆž â‰¤C0 ï£« ï£­
X"
OTHER,0.4189189189189189,"jt+1=i ,(jt,...,j0)âˆˆ[d]t+1 tY k=0"
OTHER,0.4222972972972973,"W (k)
jkjk+1  !ï£¶ ï£¸"
OTHER,0.42567567567567566,"= C0âˆ¥(|W (0)|...|W (t)|)Â·iâˆ¥1 ,"
OTHER,0.42905405405405406,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.43243243243243246,where C0 equals the maximal entry in |X(0)|.
OTHER,0.4358108108108108,"The assumption A3 implies that there exists Câ€² > 0 such that for all t âˆˆNâ‰¥0 and i âˆˆ[d],"
OTHER,0.4391891891891892,âˆ¥(|W (0)|...|W (t)|)Â·iâˆ¥1 â‰¤Câ€²N .
OTHER,0.44256756756756754,"Hence there exists Câ€²â€² > 0 such that for all t âˆˆNâ‰¥0 and i âˆˆ[d], we have"
OTHER,0.44594594594594594,"âˆ¥X(t)
Â·i âˆ¥âˆžâ‰¤Câ€²â€² ,"
OTHER,0.44932432432432434,proving the existence of C > 0 such that âˆ¥X(t)âˆ¥max â‰¤C for all t âˆˆNâ‰¥0.
OTHER,0.4527027027027027,"For a continuous Î¨(Â·, Â·)1, the following lemma is a direct consequence of Lemma 4, suggesting that
the graph attention mechanism cannot fundamentally change the connectivity pattern of the graph."
OTHER,0.4560810810810811,"C
Proof of Lemma 2"
OTHER,0.4594594594594595,"C.1
Ergodicity of infinite products of matrices in PG,Ïµ"
OTHER,0.46283783783783783,"In this section, we make use the existing results on the ergodicity of infinite products of inhomoge-
neous stochastic matrices [18, 19] to show that any sequence of matrices in PG,Ïµ is ergodic.
Lemma 5. Fix Ïµ > 0. Consider a sequence of matrices {P (t)}âˆž
t=0 in PG,Ïµ. That is, P (t) âˆˆPG,Ïµ for
all t âˆˆNâ‰¥0. Then {P (t)}âˆž
t=0 is ergodic."
OTHER,0.46621621621621623,"Proof of Lemma 5. The following sufficient condition guarantees the ergodicity of the infinite prod-
ucts of row-stochastic matrices."
OTHER,0.46959459459459457,"Lemma 6 (Corollary 5.1 [18]). Consider a sequence of row-stochastic matrices {S(t)}âˆž
t=0. Let at
and bt be the smallest and largest entries in S(t), respectively. If Pâˆž
t=0
at
bt = âˆž, then {S(t)}âˆž
t=0 is
ergodic."
OTHER,0.47297297297297297,"In order to make use of the above result, we first show that long products of P (t)â€™s from PG,Ïµ will
eventually become strictly positive. For t0 â‰¤t1, we denote"
OTHER,0.47635135135135137,P (t1:t0) = P (t1) . . . P (t0) .
OTHER,0.4797297297297297,"Lemma 7. Under the assumption A1, there exist T âˆˆN and c > 0 such that for all t0 â‰¥0,"
OTHER,0.4831081081081081,"c â‰¤P (t0+T :t0)
ij
â‰¤1 , âˆ€1 â‰¤i, j â‰¤N ."
OTHER,0.4864864864864865,"Proof of Lemma 7. Fix any T âˆˆNâ‰¥0. Since âˆ¥P (t)âˆ¥âˆžâ‰¤1 for any P (t) âˆˆPG,Ïµ, it follows that
âˆ¥P (t0+T :t0)âˆ¥âˆžâ‰¤1 and hence P (t0+T :t0)
ij
â‰¤1, for all 1 â‰¤i, j â‰¤N."
OTHER,0.48986486486486486,"To show the lower bound, without loss of generality, we will show that there exist T âˆˆN and c > 0
such that
P (T :0)
ij
â‰¥c , âˆ€1 â‰¤i, j â‰¤N ."
OTHER,0.49324324324324326,"Since each P (t) has the same connectivity pattern as the original graph G, it follows from the
assumption A1 that there exists T âˆˆN such that P (T :0) is a positive matrix, following a similar
argument as the one for Proposition 1.7 in [26]: For each pair of nodes i, j, since we assume that the
graph G is connected, there exists r(i, j) such that P (r(i,j):0)
ij
> 0. on the other hand, since we also"
OTHER,0.4966216216216216,"assume each node has a self-loop, P (t:0)
ii
> 0 for all t â‰¥0 and hence for t â‰¥r(i, j),"
OTHER,0.5,"P (t:0)
ij
â‰¥P (tâˆ’r(i,j))
ii
P (r(i,j):0)
ij
> 0 ."
OTHER,0.5033783783783784,"1More generally, for Î¨(Â·, Â·) that outputs bounded attention scores for bounded inputs."
OTHER,0.5067567567567568,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.5101351351351351,"For t â‰¥t(i) := max
jâˆˆG r(i, j), we have P (t:0)
ij
> 0 for all node j in G. Finally, if t â‰¥T := max
iâˆˆG t(i),"
OTHER,0.5135135135135135,"then P (t:0)
ij
> 0 for all pairs of nodes i, j in G. Notice that P (T :0)
ij
is a weighted sum of walks of"
OTHER,0.5168918918918919,"length T between nodes i and j, and hence P (T :0)
ij
> 0 if and only if there exists a walk of length"
OTHER,0.5202702702702703,"T between nodes i and j. Since for all t âˆˆNâ‰¥0, P (t)
ij
â‰¥Ïµ if (i, j) âˆˆE(G), we conclude that"
OTHER,0.5236486486486487,"P (T :0)
ij
â‰¥ÏµT := c."
OTHER,0.527027027027027,"Given the sequence {P (t)}âˆž
t=0, we use T âˆˆN from Lemma 7 and define"
OTHER,0.5304054054054054,Â¯P (k) := P ((k+1)T :kT ) .
OTHER,0.5337837837837838,"Then {P (t)}âˆž
t=0 is ergodic if and only if { Â¯P (k)}âˆž
k=0 is ergodic. Notice that by Lemma 7, for all
k âˆˆNâ‰¥0, there exists c > 0 such that c â‰¤Â¯P (k)
ij
â‰¤1 , âˆ€1 â‰¤i, j â‰¤N. Then Lemma 5 is a direct
consequence of Lemma 6."
OTHER,0.5371621621621622,"C.2
Notations and auxiliary results"
OTHER,0.5405405405405406,"Consider a sequence {D(t)P (t)}âˆž
t=0 in MG,Ïµ. For t0 â‰¤t1, define"
OTHER,0.543918918918919,"Qt0,t1 := D(t1)P (t1)...D(t0)P (t0)"
OTHER,0.5472972972972973,"and
Î´t = âˆ¥D(t) âˆ’INâˆ¥âˆž,
where IN denotes the N Ã— N identity matrix. It is also useful to define"
OTHER,0.5506756756756757,"Ë†Qt0,t1 :=P (t1)Qt0,t1âˆ’1
:=P (t1)D(t1âˆ’1)P (t1âˆ’1)...D(t0)P (t0)."
OTHER,0.5540540540540541,"We start by proving the following key lemma, which states that long products of matrices in MG,Ïµ
eventually become a contraction in âˆž-norm.
Lemma 8. There exist 0 < c < 1 and T âˆˆN such that for all t0 â‰¤t1,"
OTHER,0.5574324324324325,"âˆ¥Ë†Qt0,t1+T âˆ¥âˆžâ‰¤(1 âˆ’cÎ´t1)âˆ¥Ë†Qt0,t1âˆ¥âˆž."
OTHER,0.5608108108108109,"Proof of Lemma 8. First observe that for every T â‰¥0,"
OTHER,0.5641891891891891,"âˆ¥Ë†Qt0,t1+T âˆ¥âˆžâ‰¤âˆ¥P (t1+T )D(t1+T âˆ’1)P (t1+T âˆ’1)...D(t1+1)P (t1+1)D(t1)âˆ¥âˆžâˆ¥Ë†Qt0,t1âˆ¥âˆž"
OTHER,0.5675675675675675,"â‰¤âˆ¥P (t1+T )P (t1+T âˆ’1)...P (t1+1)D(t1)âˆ¥âˆžâˆ¥Ë†Qt0,t1âˆ¥âˆž,"
OTHER,0.5709459459459459,where the second inequality is based on the following element-wise inequality:
OTHER,0.5743243243243243,P (t1+T )P (t1+T âˆ’1)...D(t1+1)P (t1+1) â‰¤ew P (t1+T )P (t1+T âˆ’1)...P (t1+1) .
OTHER,0.5777027027027027,"By Lemma 7, there exist T âˆˆN and 0 < c < 1 such that"
OTHER,0.581081081081081,"(P (t1+T )...P (t1+1))ij â‰¥c, âˆ€1 â‰¤i, j â‰¤N ."
OTHER,0.5844594594594594,"Since the matrix product P (t1+T )P (t1+T âˆ’1)...P (t1+1) is row-stochastic, multiplying it with the
diagonal matrix D(t1) from right decreases the row sums by at least c(1 âˆ’D(t1)
min) = cÎ´t1, where
D(t1)
min here denotes the smallest diagonal entry of the diagonal matrix D(t1). Hence,"
OTHER,0.5878378378378378,âˆ¥P (t1+T )P (t1+T âˆ’1)...P (t1+1)D(t1)âˆ¥âˆžâ‰¤1 âˆ’cÎ´t1 .
OTHER,0.5912162162162162,"Now define Î²k := Qk
t=0(1 âˆ’cÎ´t) and let Î² := lim
kâ†’âˆžÎ²k. Note that Î² is well-defined because the"
OTHER,0.5945945945945946,"partial product is non-increasing and bounded from below. Then we present the following result,
which is stated as Lemma 9 in the main paper and from which the ergodicity of any sequence in
MG,Ïµ is an immediate result."
OTHER,0.597972972972973,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.6013513513513513,"Lemma 9. Let Î²k := Qk
t=0(1 âˆ’cÎ´t) and Î² := lim
kâ†’âˆžÎ²k."
OTHER,0.6047297297297297,"1. If Î² = 0, then lim
kâ†’âˆžQ0,k = 0 ;"
OTHER,0.6081081081081081,"2. If Î² > 0, then lim
kâ†’âˆžBQ0,k = 0 ."
OTHER,0.6114864864864865,Proof of Lemma 9. We will prove the two cases separately.
OTHER,0.6148648648648649,"[Case Î² = 0].
We will show that Î² = 0 implies
lim
kâ†’âˆžâˆ¥Ë†Q0,kâˆ¥âˆž= 0, and as a result,"
OTHER,0.6182432432432432,"lim
kâ†’âˆžâˆ¥Q0,kâˆ¥âˆž= 0. For 0 â‰¤j â‰¤T âˆ’1, let us define Î²j := âˆž
Y"
OTHER,0.6216216216216216,"k=0
(1 âˆ’Î´j+kT ) ."
OTHER,0.625,"Then by Lemma 8, we get that"
OTHER,0.6283783783783784,"lim
kâ†’âˆžâˆ¥Ë†Q0,kT âˆ¥âˆžâ‰¤Î²jâˆ¥Ë†Q0,jâˆ¥âˆž."
OTHER,0.6317567567567568,"By construction, Î² = Î T âˆ’1
j=0 Î²j. Hence, if Î² = 0 then Î²j0 = 0 for some 0 â‰¤j0 â‰¤T âˆ’1, which
yields lim
kâ†’âˆžâˆ¥Ë†Q0,kâˆ¥âˆž= 0. Consequently, lim
kâ†’âˆžâˆ¥Q0,kâˆ¥âˆž= 0 implies that lim
kâ†’âˆžQ0,k = 0."
OTHER,0.6351351351351351,"[Case Î² > 0].
First observe that if Î² > 0, then âˆ€0 < Î· < 1, there exist m âˆˆNâ‰¥0 such that âˆž
Y"
OTHER,0.6385135135135135,"t=m
(1 âˆ’cÎ´t) > 1 âˆ’Î· .
(4)"
OTHER,0.6418918918918919,"Using 1 âˆ’x â‰¤eâˆ’x for all x âˆˆR, we deduce âˆž
Y"
OTHER,0.6452702702702703,"t=m
eâˆ’cÎ´t > 1 âˆ’Î· ."
OTHER,0.6486486486486487,"It also follows from (4) that 1 âˆ’cÎ´t > 1 âˆ’Î·, or equivalently Î´t < Î·"
OTHER,0.652027027027027,c for t â‰¥m. Choosing Î· < c
OTHER,0.6554054054054054,"2
thus ensures that Î´t < 1"
OTHER,0.6587837837837838,"2 for t â‰¥m. Putting this together with the fact that, there exists2 b > 0 such
that 1 âˆ’x â‰¥eâˆ’bx for all x âˆˆ[0, 1"
OTHER,0.6621621621621622,"2], we obtain âˆž
Y"
OTHER,0.6655405405405406,"t=m
(1 âˆ’Î´t) â‰¥ âˆž
Y"
OTHER,0.668918918918919,"t=m
eâˆ’bÎ´t > (1 âˆ’Î·)
b
c := 1 âˆ’Î·â€² .
(5)"
OTHER,0.6722972972972973,"Define the product of row-stochastic matrices P (M:m) := P (M) . . . P (m). It is easy to verify the
following element-wise inequality:
 M
Y"
OTHER,0.6756756756756757,"t=m
(1 âˆ’cÎ´t) !"
OTHER,0.6790540540540541,"P (M:m) â‰¤ew Qm,M â‰¤ew P (M:m) ,"
OTHER,0.6824324324324325,which together with (5) leads to
OTHER,0.6858108108108109,"(1 âˆ’Î·â€²)P (M:m) â‰¤ew Qm,M â‰¤ew P (M:m) .
(6)"
OTHER,0.6891891891891891,"Therefore,"
OTHER,0.6925675675675675,"âˆ¥BQm,Mâˆ¥âˆž= âˆ¥B(Qm,M âˆ’P (M:m)) + BP (M:m)âˆ¥âˆž"
OTHER,0.6959459459459459,"â‰¤âˆ¥B(Qm,M âˆ’P (M:m))âˆ¥âˆž+ âˆ¥BP (M:m)âˆ¥âˆž"
OTHER,0.6993243243243243,"= âˆ¥B(Qm,M âˆ’P (M:m))âˆ¥âˆž"
OTHER,0.7027027027027027,"â‰¤âˆ¥Bâˆ¥âˆžâˆ¥Qm,M âˆ’P (M:m)âˆ¥âˆž
â‰¤Î·â€²âˆ¥Bâˆ¥âˆž â‰¤Î·â€²âˆš N ,"
OTHER,0.706081081081081,"2Choose, e.g., b = 2 log 2."
OTHER,0.7094594594594594,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.7128378378378378,"where the last inequality is due to the fact that âˆ¥Bâˆ¥2 = 1. By definition, Q0,M = Qm,MQ0,mâˆ’1,
and hence"
OTHER,0.7162162162162162,"âˆ¥BQ0,Mâˆ¥âˆžâ‰¤âˆ¥BQm,Mâˆ¥âˆžâˆ¥Q0,mâˆ’1âˆ¥âˆžâ‰¤âˆ¥BQm,Mâˆ¥âˆžâ‰¤Î·â€²âˆš"
OTHER,0.7195945945945946,"N .
(7)
The above inequality (7) holds when taking M â†’âˆž. Then taking Î· â†’0 implies Î·â€² â†’0 and
together with (7), we conclude that
lim
Mâ†’âˆžâˆ¥BQ0,Mâˆ¥âˆž= 0 ,"
OTHER,0.722972972972973,"and therefore,
lim
Mâ†’âˆžBQ0,M = 0 ."
OTHER,0.7263513513513513,"C.3
Proof of Lemma 2"
OTHER,0.7297297297297297,"Notice that both cases Î² = 0 and Î² > 0 in Lemma 9 imply the ergodicity of {D(t)P (t)}âˆž
t=0. Hence
the statement is a direct corollary of Lemma 9."
OTHER,0.7331081081081081,"D
Proof of Lemma 3"
OTHER,0.7364864864864865,"In order to show that JSR( Ëœ
MG,Ïµ) < 1, we start by making the following observation.
Lemma 10. A sequence {M (n)}âˆž
n=0 is ergodic if and only if Qt
n=0 Ëœ
M (n) converges to the zero
matrix."
OTHER,0.7398648648648649,"Proof of Lemma 10. For any t âˆˆNâ‰¥0, it follows from the third property of the orthogonal projection
B (see, Page 6 of the main paper) that B tY"
OTHER,0.7432432432432432,"n=0
M (n) = tY"
OTHER,0.7466216216216216,"n=0
Ëœ
M (n)B . Hence"
OTHER,0.75,"{M (n)}âˆž
n=0 is ergodic â‡â‡’lim
tâ†’âˆžB tY"
OTHER,0.7533783783783784,"n=0
M (n) = 0"
OTHER,0.7567567567567568,"â‡â‡’lim
tâ†’âˆž tY"
OTHER,0.7601351351351351,"n=0
Ëœ
M (n)B = 0"
OTHER,0.7635135135135135,"â‡â‡’lim
tâ†’âˆž tY"
OTHER,0.7668918918918919,"n=0
Ëœ
M (n) = 0 ."
OTHER,0.7702702702702703,"Next, we utilize the following result, as a means to ensure a joint spectral radius strictly less than 1
for a bounded set of matrices.
Lemma 11 (Proposition 3.2 in [27]). For any bounded set of matrices M, JSR(M) < 1 if and only
if for any sequence {M (n)}âˆž
n=0 in M, Qt
n=0 M (n) converges to the zero matrix."
OTHER,0.7736486486486487,"Here, â€œbounded"" means that there exists an upper bound on the norms of the matrices in the set. Note
that MG,Ïµ is bounded because âˆ¥DPâˆ¥âˆžâ‰¤1, DP âˆˆMG,Ïµ. To show that Ëœ
MG,Ïµ is also bounded, let
Ëœ
M âˆˆËœ
MG,Ïµ, then by definition, we have
Ëœ
MB = BM, M âˆˆMG,Ïµ â‡’Ëœ
M = BMBT ,"
OTHER,0.777027027027027,"since BBT = INâˆ’1. As a result,"
OTHER,0.7804054054054054,"âˆ¥Ëœ
Mâˆ¥2 = âˆ¥BMBT âˆ¥2 â‰¤âˆ¥Mâˆ¥2 â‰¤
âˆš"
OTHER,0.7837837837837838,"N ,
where the first inequality is due to âˆ¥Bâˆ¥2 = âˆ¥BâŠ¤âˆ¥2 = 1, and the second ineuality follows from
âˆ¥Mâˆ¥âˆžâ‰¤1."
OTHER,0.7871621621621622,"Combining Lemma 2, Lemma 10 and Lemma 11, we conclude that JSR( Ëœ
MG,Ïµ) < 1."
OTHER,0.7905405405405406,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.793918918918919,"E
Proof of Theorem 1"
OTHER,0.7972972972972973,"Recall the formulation of X(t+1)
Â·i
in (2):"
OTHER,0.8006756756756757,"X(t+1)
Â·i
= Ïƒ(P (t)(X(t)W (t))Â·i) =
X"
OTHER,0.8040540540540541,"jt+1=i, (jt,...,j0)âˆˆ[d]t+1 tY"
OTHER,0.8074324324324325,"k=0
W (k)
jkjk+1 !"
OTHER,0.8108108108108109,"D(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 ."
OTHER,0.8141891891891891,Then it follows that
OTHER,0.8175675675675675,"âˆ¥BX(t+1)
Â·i
âˆ¥2 =  X"
OTHER,0.8209459459459459,"jt+1=i, (jt,...,j0)âˆˆ[d]t+1 tY"
OTHER,0.8243243243243243,"k=0
W (k)
jkjk+1 !"
OTHER,0.8277027027027027,"BD(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 2 â‰¤
X"
OTHER,0.831081081081081,"jt+1=i, (jt,...,j0)âˆˆ[d]t+1 tY k=0"
OTHER,0.8344594594594594,"W (k)
jkjk+1 "
OTHER,0.8378378378378378,"! 


BD(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 2 =
X"
OTHER,0.8412162162162162,"jt+1=i, (jt,...,j0)âˆˆ[d]t+1 tY k=0"
OTHER,0.8445945945945946,"W (k)
jkjk+1 "
OTHER,0.847972972972973,"! 


 ËœD(t)
jt+1 ËœP (t)... ËœD(0)
j1 ËœP (0)BX(0)
j0 2 â‰¤
X"
OTHER,0.8513513513513513,"jt+1=i, (jt,...,j0)âˆˆ[d]t+1 tY k=0"
OTHER,0.8547297297297297,"W (k)
jkjk+1  !"
OTHER,0.8581081081081081,"Cqt+1 


BX(0)
j0 2"
OTHER,0.8614864864864865,"â‰¤Câ€²qt+1 ï£« ï£­
X"
OTHER,0.8648648648648649,"jt+1=i, (jt,...,j0)âˆˆ[d]t+1 tY k=0"
OTHER,0.8682432432432432,"W (k)
jkjk+1  !ï£¶ ï£¸"
OTHER,0.8716216216216216,"= Câ€²qt+1âˆ¥(|W (0)|...|W (t)|)Â·iâˆ¥1 ,"
OTHER,0.875,"where Câ€² = Cmax
jâˆˆ[d]âˆ¥BX(0)
j
âˆ¥2 and âˆ¥Â· âˆ¥1 denotes the 1-norm. Specifically, the first inequality follows"
OTHER,0.8783783783783784,"from the triangle inequality, and the second inequality is due to the property of the joint spectral
radius in (3), where JSR( Ëœ
MG,Ïµ) < q < 1."
OTHER,0.8817567567567568,"Since âˆ¥Bxâˆ¥2 = âˆ¥xâˆ¥2 if xâŠ¤1 = 0 for x âˆˆRN, we also have that if XâŠ¤1 = 0 for X âˆˆRNÃ—d, then"
OTHER,0.8851351351351351,"âˆ¥BXâˆ¥F = âˆ¥Xâˆ¥F ,"
OTHER,0.8885135135135135,using which we obtain that
OTHER,0.8918918918918919,Âµ(X(t+1)) = âˆ¥X(t+1) âˆ’1Î³X(t+1)âˆ¥F = âˆ¥BX(t+1)âˆ¥F =
OTHER,0.8952702702702703,"v
u
u
t d
X"
OTHER,0.8986486486486487,"i=1
âˆ¥BX(t+1)
Â·i
âˆ¥2
2"
OTHER,0.902027027027027,â‰¤Câ€²qt+1
OTHER,0.9054054054054054,"v
u
u
t d
X"
OTHER,0.9087837837837838,"i=1
âˆ¥(|W (0)|...|W (t)|)Â·iâˆ¥2
1"
OTHER,0.9121621621621622,â‰¤Câ€²qt+1
OTHER,0.9155405405405406,"v
u
u
t d
X"
OTHER,0.918918918918919,"i=1
âˆ¥|(W (0)|...|W (t)|)Â·iâˆ¥1 !2"
OTHER,0.9222972972972973,"= Câ€²qt+1âˆ¥|(W (0)|...|W (t)|âˆ¥1,1 ,"
OTHER,0.9256756756756757,"where âˆ¥Â· âˆ¥1,1 denotes the matrix (1, 1)-norm (recall from Section A.2 that for a matrix M âˆˆRmÃ—n,
we have âˆ¥Mâˆ¥1,1 = Pm
i=1
Pn
j=1 |Mij|). The assumption A3 implies that there exists Câ€²â€² such that
for all t âˆˆNâ‰¥0,
âˆ¥(|W (0)|...|W (t)|)âˆ¥1,1 â‰¤Câ€²â€²d2 ."
OTHER,0.9290540540540541,"Thus we conclude that there exists C1 such that for all t âˆˆNâ‰¥0,"
OTHER,0.9324324324324325,Âµ(X(t)) â‰¤C1qt .
OTHER,0.9358108108108109,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.9391891891891891,"F
Proof of Proposition 1"
OTHER,0.9425675675675675,"Since Dâˆ’1
degA is similar to Dâˆ’1/2
deg ADâˆ’1/2
deg , they have the same spectrum. For Dâˆ’1
degA, the smallest
nonzero entry has value 1/dmax, where dmax is the maximum node degree in G. On the other hand,
it follows from the definition of PG,Ïµ that"
OTHER,0.9459459459459459,Ïµdmax â‰¤1 .
OTHER,0.9493243243243243,"Therefore, Ïµ â‰¤1/dmax and thus Dâˆ’1
degA âˆˆPG,Ïµ."
OTHER,0.9527027027027027,"We proceed by proving the following result.
Lemma 12. For any M in M, the spectral radius of M denoted by Ï(M), satisfies"
OTHER,0.956081081081081,Ï(M) â‰¤JSR(M) .
OTHER,0.9594594594594594,"Proof of Lemma 12. Gelfandâ€™s formula states that Ï(M) =
lim
kâ†’âˆžâˆ¥M kâˆ¥
1
k , where the quantity is"
OTHER,0.9628378378378378,"independent of the norm used [28]. Then comparing with the definition of the joint spectral radius,
we can immediately conclude the statement."
OTHER,0.9662162162162162,"Let B(Dâˆ’1
degA) = ËœPB. By definition, ËœP âˆˆËœ
MG,Ïµ since Dâˆ’1
degA âˆˆPG,Ïµ as shown before the lemma.
Moreover, the spectrum of ËœP is the spectrum of Dâˆ’1
degA after reducing the multiplicity of eigenvalue 1
by one. Under the assumption A1, the eigenvalue 1 of Dâˆ’1
degA has multiplicity 1, and hence Ï( ËœP) = Î»,
where Î» is the second largest eigenvalue of Dâˆ’1
degA. Putting this together with Lemma 12, we conclude
that
Î» â‰¤JSR( Ëœ
MG,Ïµ)
as desired."
OTHER,0.9695945945945946,"G
Numerical Experiments"
OTHER,0.972972972972973,"Here we provide more details on the numerical experiments. All models were implemented with
PyTorch [29] and PyTorch Geometric [30]."
OTHER,0.9763513513513513,"Datasets.
We used torch_geometric.datasets.planetoid provided in PyTorch Geometric
for all the three datasets: Cora, CiteSeer, and PubMed with their default training and test splits."
OTHER,0.9797297297297297,Model details.
OTHER,0.9831081081081081,"â€¢ For GAT, we consider the architecture proposed in VeliË‡ckoviÂ´c et al. [7] with each attentional
layer sharing the parameter a in LeakyReLU(aâŠ¤[W âŠ¤Xi||W âŠ¤Xj]), a âˆˆR2dâ€² to compute the
attention scores.
â€¢ For GCN, we consider the standard random walk graph convolution Dâˆ’1
degA. That is, the update
rule of each graph convolutional layer can be written as"
OTHER,0.9864864864864865,"Xâ€² = Dâˆ’1
degAXW ,"
OTHER,0.9898648648648649,"where X and Xâ€² are the input and output node representations, respectively, and W is the shared
learnable weight matrix in the layer."
OTHER,0.9932432432432432,"Compute.
We trained all of our models on a Telsa V100 GPU."
OTHER,0.9966216216216216,"Training details.
In all experiments, we used the Adam optimizer using a learning rate of 0.00001
and 0.0005 weight decay and trained for 1000 epoch."
