Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019230769230769232,"Neural operators (NOs) have become popular for learning partial differential equa-
1"
ABSTRACT,0.0038461538461538464,"tion (PDE) operators. As a mapping between infinite-dimensional function spaces,
2"
ABSTRACT,0.0057692307692307696,"each layer of NO contains a kernel operator and a linear transform, followed by
3"
ABSTRACT,0.007692307692307693,"nonlinear activation. NO can accurately simulate the operator and conduct super-
4"
ABSTRACT,0.009615384615384616,"resolution, i.e., train and test on grids with different resolutions. Despite its success,
5"
ABSTRACT,0.011538461538461539,"NO’s design of kernel operator, choice of grids, the capability of generalization
6"
ABSTRACT,0.013461538461538462,"and super-resolution, and applicability to general problems on irregular domains
7"
ABSTRACT,0.015384615384615385,"are poorly understood. To this end, we systematically analyze NOs from a unified
8"
ABSTRACT,0.01730769230769231,"perspective, considering the orthogonal bases in their kernel operators. This analy-
9"
ABSTRACT,0.019230769230769232,"sis facilitates a better understanding and enhancement of NOs in the following: (1)
10"
ABSTRACT,0.021153846153846155,"Generalization bounds of NOs, (2) Construction of NOs on arbitrary domains, (3)
11"
ABSTRACT,0.023076923076923078,"Enhancement of NOs’ performance by designing proper orthogonal bases that align
12"
ABSTRACT,0.025,"with the operator and domain, (4) Improvement of NOs’ through the allocation
13"
ABSTRACT,0.026923076923076925,"of suitable grids, and (5) Investigation of super-resolution error. Our theory has
14"
ABSTRACT,0.028846153846153848,"multiple implications in practice: choosing the orthogonal basis and grid points to
15"
ABSTRACT,0.03076923076923077,"accelerate training, improving the generalization and super-resolution capabilities,
16"
ABSTRACT,0.032692307692307694,"and adapting NO to irregular domains. Corresponding experiments are conducted
17"
ABSTRACT,0.03461538461538462,"to verify our theory. Our paper provides a new perspective for studying NOs.
18"
INTRODUCTION,0.03653846153846154,"1
Introduction
19"
INTRODUCTION,0.038461538461538464,"Partial differential equation (PDE) operators are widespread in science and engineering. However,
20"
INTRODUCTION,0.04038461538461539,"traditional numerical methods are known to be slow and ill-suited for high-dimensional problems.
21"
INTRODUCTION,0.04230769230769231,"As a result, there has been a surge in the popularity of utilizing deep learning techniques for
22"
INTRODUCTION,0.04423076923076923,"operator learning. Neural operators (NOs) [20, 19, 8, 21] are among the most important models.
23"
INTRODUCTION,0.046153846153846156,"As a mapping between infinite-dimensional function spaces, each layer of NO contains a kernel
24"
INTRODUCTION,0.04807692307692308,"operator and a linear transform to convert the input function, followed by nonlinear activation,
25"
INTRODUCTION,0.05,"conducted numerically based on the discretization of the input function on a grid. With appropriate
26"
INTRODUCTION,0.051923076923076926,"kernels, e.g., shift-invariant kernels in Fourier NO (FNO) [20], facilitates the construction of complex
27"
INTRODUCTION,0.05384615384615385,"operators. Stacking multiple NO layers further enhances the operator’s complexity and demonstrates
28"
INTRODUCTION,0.05576923076923077,"its universal approximation capabilities [15]. Moreover, empirical evidence reveals that NO exhibits
29"
INTRODUCTION,0.057692307692307696,"fast convergence and excellent generalization, making it a practical choice. In addition to its operator
30"
INTRODUCTION,0.05961538461538462,"fitting and generalization abilities, NO can also perform super-resolution tasks. This involves training
31"
INTRODUCTION,0.06153846153846154,"the model on a low-resolution grid and accurately predicting outcomes on a high-resolution grid. This
32"
INTRODUCTION,0.06346153846153846,"capability expands the utility of NO beyond precise operator fitting and generalization, showcasing
33"
INTRODUCTION,0.06538461538461539,"its versatility and accuracy in super-resolution applications.
34"
INTRODUCTION,0.0673076923076923,"Insufficient understanding surrounds the design of kernel operators, choice of grid points, and
35"
INTRODUCTION,0.06923076923076923,"NOs’ capabilities, despite their notable features in generalization and super-resolution tasks. For
36"
INTRODUCTION,0.07115384615384615,"instance, while the Fourier basis-based FNO has been established, alternative approaches employing
37"
INTRODUCTION,0.07307692307692308,"polynomial basis [21] and wavelet basis [8, 28] have been proposed to enhance NO’s performance in
38"
INTRODUCTION,0.075,"handling operators related to non-periodic functions and multiscale functions, respectively. However,
39"
INTRODUCTION,0.07692307692307693,"the underlying reasons for their effectiveness through basis changes remain elusive. Besides, the
40"
INTRODUCTION,0.07884615384615384,"effectiveness of the super-resolution effect in NO is currently based on empirical observations, and
41"
INTRODUCTION,0.08076923076923077,"the factors influencing its efficacy remain unknown. Furthermore, NO is commonly trained on a
42"
INTRODUCTION,0.08269230769230769,"predefined uniform grid with a predetermined sparsity level. The impact of utilizing grids with
43"
INTRODUCTION,0.08461538461538462,"varying sparsity levels or randomly sampled grid points on NO’s performance remains unexplored.
44"
INTRODUCTION,0.08653846153846154,"On the other hand, the applicability of NOs to general problems on irregular domains poses a
45"
INTRODUCTION,0.08846153846153847,"significant challenge, mainly because the kernel operators utilized in popular models [20, 21, 28]
46"
INTRODUCTION,0.09038461538461538,"are typically defined on regular bounded domains. Extending NO, based on orthogonal bases, to
47"
INTRODUCTION,0.09230769230769231,"encompass general irregular domains remains a formidable task. The difficulty lies in effectively
48"
INTRODUCTION,0.09423076923076923,"incorporating the physical information of the domain into the design of NO’s basis and grid.
49"
INTRODUCTION,0.09615384615384616,"In this paper, we provide a novel perspective for studying NOs by examining the role of orthogonal
50"
INTRODUCTION,0.09807692307692308,"bases within their kernel operators. The kernel operators in NOs are constructed such that their
51"
INTRODUCTION,0.1,"eigenfunctions are predefined orthogonal bases, and eigenvalues are trainable parameters. This unified
52"
INTRODUCTION,0.10192307692307692,"view enables the analysis of NOs in various aspects. Firstly, we establish generalization bounds
53"
INTRODUCTION,0.10384615384615385,"for NOs, considering them mappings between infinite-dimensional function spaces. Moreover, by
54"
INTRODUCTION,0.10576923076923077,"carefully designing orthogonal bases for the input domain and functions, NOs can be constructed
55"
INTRODUCTION,0.1076923076923077,"on irregular domains, improving generalization. The impact of grid points on NO convergence and
56"
INTRODUCTION,0.10961538461538461,"generalization is also investigated. Additionally, we analyze factors influencing the super-resolution
57"
INTRODUCTION,0.11153846153846154,"error in NOs. Our theory carries practical implications, such as selecting appropriate orthogonal
58"
INTRODUCTION,0.11346153846153846,"bases and grid points to accelerate convergence, enhance generalization and super-resolution abilities,
59"
INTRODUCTION,0.11538461538461539,"and adapt NOs to irregular domains. Extensive experiments are conducted to validate our theory,
60"
INTRODUCTION,0.11730769230769231,"which sheds new light on understanding NOs and improving their properties in practical applications.
61"
PRELIMINARY,0.11923076923076924,"2
Preliminary
62"
NOTATION & PROBLEM DEFINITION,0.12115384615384615,"2.1
Notation & Problem Definition
63"
NOTATION & PROBLEM DEFINITION,0.12307692307692308,"Notation. We use ∥·∥2 to denote vector 2-norm or matrix spectral norm, while ∥·∥l2 and ∥·∥L2 are the
64"
NOTATION & PROBLEM DEFINITION,0.125,"norms in l2 and L2 spaces, respectively. We use |·| to denote the cardinality of a set. For a metric space
65"
NOTATION & PROBLEM DEFINITION,0.12692307692307692,"(S, ρ) and T ⊂S we say that ˆT ⊂S is an ϵ-cover of T, if ∀t ∈T, there ∃ˆt ∈ˆT such that ρ(t, ˆt) ≤ϵ.
66"
NOTATION & PROBLEM DEFINITION,0.12884615384615383,"The ϵ-covering number of T is defined as [13, 29]: N(ϵ, T, ρ) = min{| ˆT| : ˆT is an ϵ −cover of T}.
67"
NOTATION & PROBLEM DEFINITION,0.13076923076923078,"Problem Definition. We consider the operator learning problem in [20] on the (possibly irregular
68"
NOTATION & PROBLEM DEFINITION,0.1326923076923077,"or unbounded) domain Ω⊂Rd, with the input function space A = A(Ω) and output function
69"
NOTATION & PROBLEM DEFINITION,0.1346153846153846,"space H = H(Ω), and the operator to be learned G : A →H. Adding a project can extend the
70"
NOTATION & PROBLEM DEFINITION,0.13653846153846153,"model to different input and output domains. We are given training data {fj, G(fj)}Ntrain
j=1 , where
71"
NOTATION & PROBLEM DEFINITION,0.13846153846153847,"fj ∼µ are i.i.d. samples from an unknown distribution µ over the functions supported on A. We
72"
NOTATION & PROBLEM DEFINITION,0.14038461538461539,"aim to approximate G by a neural operator (NO) Gθ, which requires discretization. Thus, the input
73"
NOTATION & PROBLEM DEFINITION,0.1423076923076923,"functions are represented by their pointwise values on a discrete grid {⃗x, fj(⃗x)} = {xi, fj(xi)}Ngrid
i=1 .
74"
NOTATION & PROBLEM DEFINITION,0.14423076923076922,"Labels are also discretized on the same grid {⃗x, G(fj)(⃗x)} = {xi, G(fj)(xi)}Ngrid
i=1 , and train Gθ
75"
NOTATION & PROBLEM DEFINITION,0.14615384615384616,"via minimizing Ltrain(θ) =
1
Ntrain
PNtrain
j=1
1
Ngrid ∥G(fj)(⃗x) −Gθ(fj)(⃗x)∥2
2 . The corresponding (regular)
76"
NOTATION & PROBLEM DEFINITION,0.14807692307692308,"test loss on the same grid ⃗x is Ltest-reg(θ) = Ef∼µ
1
Ngrid ∥G(f)(⃗x) −Gθ(f)(⃗x)∥2
2 . We also consider
77"
NOTATION & PROBLEM DEFINITION,0.15,"the super-resolution task, i.e., the model can make predictions on all x ∈Ω, which is approximated by
78"
NOTATION & PROBLEM DEFINITION,0.1519230769230769,"taking another different and usually finer grid ⃗xtest as input and yields the output function pointwise
79"
NOTATION & PROBLEM DEFINITION,0.15384615384615385,"values on the new grid. Note that the characteristic of NO is to take in an arbitrary grid and output the
80"
NOTATION & PROBLEM DEFINITION,0.15576923076923077,"values of the target function on this grid, and the grid size can be arbitrary [19, 20]. This reflects that
81"
NOTATION & PROBLEM DEFINITION,0.1576923076923077,"NO is a mapping between infinite-dimensional spaces. The super-resolution test loss on a different
82"
NOTATION & PROBLEM DEFINITION,0.1596153846153846,"grid ⃗xtest is Ltest-sr(θ) = Ef∼µ ∥G(f) −Gθ(f)∥2
L2(Ω) ≈Ef∼µ
1
Ngrid,test ∥G(f)(⃗xtest) −Gθ(f)(⃗xtest)∥2
2 .
83"
UNDERSTANDING NEURAL OPERATOR,0.16153846153846155,"2.2
Understanding Neural Operator
84"
UNDERSTANDING NEURAL OPERATOR,0.16346153846153846,"Given an input function f : Ω⊂Rd →Rh where h is the hidden/output dim, and the complete
85"
UNDERSTANDING NEURAL OPERATOR,0.16538461538461538,"orthogonal basis set {ϕi}∞
i=0 on L2(Ω; R) arranged with increasing frequencies/degrees/orders, each
86"
UNDERSTANDING NEURAL OPERATOR,0.1673076923076923,"layer of NOs contains three operations: (1) the kernel transform, (2) the linear transform, and (3) the
87"
UNDERSTANDING NEURAL OPERATOR,0.16923076923076924,"nonlinear activation. The overall model structure can be summarized as follows.
88"
UNDERSTANDING NEURAL OPERATOR,0.17115384615384616,"Overall Model. Denote the input function as ˆu0(x), then the recursive formulation of NO is
89"
UNDERSTANDING NEURAL OPERATOR,0.17307692307692307,"ˆul(x) = σ (ul(x)) , l ≥1; ˆu0(x) = ˆu0(x),"
UNDERSTANDING NEURAL OPERATOR,0.175,"ul+1(x) =
Z
K(Bl, x, y)ˆul(y)dy + W lˆul(x),"
UNDERSTANDING NEURAL OPERATOR,0.17692307692307693,"v(x) = uL(x), (1)"
UNDERSTANDING NEURAL OPERATOR,0.17884615384615385,"where v(x) is the output, l is the layer index, and L is the total number of layers. ˆu and u are the
90"
UNDERSTANDING NEURAL OPERATOR,0.18076923076923077,"post-activation and pre-activation input functions, respectively.
91"
UNDERSTANDING NEURAL OPERATOR,0.18269230769230768,"Kernel Transform κ. We set up trainable kernels such that the basis set with Nmodes lowest
92"
UNDERSTANDING NEURAL OPERATOR,0.18461538461538463,"frequencies form the eigenfunctions of the kernels, i.e., K(B; x, y) = PNmodes
i=0
Biϕi(x)ϕi(y) where
93"
UNDERSTANDING NEURAL OPERATOR,0.18653846153846154,"Bi ∈Rh×h and B ∈RNmodes×h×h are trainable, and h is the hidden dim. Thanks to the low-
94"
UNDERSTANDING NEURAL OPERATOR,0.18846153846153846,"rank property of the kernel, i.e., the dimension of its kernel spectra are only Nmodes, κ(f)(x) =
95
R
K(B; x, y)f(y)dy = PNmodes
i=0
Biciϕi(x) ∈Rh, where ci = ⟨f, ϕi⟩∈Rh is the dimension-wise
96"
UNDERSTANDING NEURAL OPERATOR,0.19038461538461537,"inner product between the input f and the basis ϕi. This form is exactly the same as the kernel in
97"
UNDERSTANDING NEURAL OPERATOR,0.19230769230769232,"FNO if the basis ϕi is Fourier series. For implementation, we usually (1) project the input function
98"
UNDERSTANDING NEURAL OPERATOR,0.19423076923076923,"onto the basis with Nmodes lowest frequencies to get {ci}Nmodes
i=1 ; (2) linear transform the coefficients
99"
UNDERSTANDING NEURAL OPERATOR,0.19615384615384615,"ci with the matrices Bi; (3) finally project from the coefficient space back to the function space by
100"
UNDERSTANDING NEURAL OPERATOR,0.19807692307692307,"multiplying ϕi(x) to each Bici.
101"
UNDERSTANDING NEURAL OPERATOR,0.2,"Linear Transform ω.
The linear transform ω is a straightforward operation ω[f](x)
=
102"
UNDERSTANDING NEURAL OPERATOR,0.20192307692307693,"W f(x), W ∈Rh×h, f(x) ∈Rh. Unlike the kernel operator, the linear transform can be con-
103"
UNDERSTANDING NEURAL OPERATOR,0.20384615384615384,"ducted in the original function space without projection. The final output is obtained by combining
104"
UNDERSTANDING NEURAL OPERATOR,0.20576923076923076,"the results from the kernel transform and the linear transform: T[f](x) = κ(f)(x) + ω[f](x).
105"
UNDERSTANDING NEURAL OPERATOR,0.2076923076923077,"Finally, the transformer function is passed through a nonlinear activation, i.e., σ (T[f](x)).
106"
UNDERSTANDING NEURAL OPERATOR,0.20961538461538462,"Examples of Bases. In FNO [20], the orthogonal basis used is the Fourier basis, specifically over a
107"
UNDERSTANDING NEURAL OPERATOR,0.21153846153846154,"bounded regular domain. However, alternative orthogonal bases are employed in other variations
108"
UNDERSTANDING NEURAL OPERATOR,0.21346153846153845,"of FNO, such as those discussed in [8, 21], including polynomial and wavelet bases. Regarding the
109"
UNDERSTANDING NEURAL OPERATOR,0.2153846153846154,"implementation of the most popular FNO [20]: (1) is the fast Fourier transform (FFT); (2) is its
110"
UNDERSTANDING NEURAL OPERATOR,0.2173076923076923,"coefficient transform; (3) is the inverse FFT.
111"
UNDERSTANDING NEURAL OPERATOR,0.21923076923076923,"Numerical Integration. In practice, input functions are discretized on a grid. Transformations and
112"
UNDERSTANDING NEURAL OPERATOR,0.22115384615384615,"activations can be applied pointwise, but kernel transforms relying on the inner product with bases
113"
UNDERSTANDING NEURAL OPERATOR,0.2230769230769231,"require numerical integration on the grid. However, the linear transform does not need numerical
114"
UNDERSTANDING NEURAL OPERATOR,0.225,"integration and can be implemented more easily in the original function space.
115"
UNDERSTANDING NEURAL OPERATOR,0.22692307692307692,"Efficiency of Numerical Integration. In FNO, the Fast Fourier Transform (FFT) is used to compute
116"
UNDERSTANDING NEURAL OPERATOR,0.22884615384615384,"integrals, with a time complexity of O(Ngrid log Ngrid). However, in practice, only the first Nmodes
117"
UNDERSTANDING NEURAL OPERATOR,0.23076923076923078,"integrations between the basis and input need to be calculated. This reduces the complexity to O(Ngrid)
118"
UNDERSTANDING NEURAL OPERATOR,0.2326923076923077,"since Nmodes ≪Ngrid. This approach, adopted in Geo-FNO [19], ensures that the integration step
119"
UNDERSTANDING NEURAL OPERATOR,0.23461538461538461,"does not become a bottleneck in the NO model’s time complexity. As a result, NO models are
120"
UNDERSTANDING NEURAL OPERATOR,0.23653846153846153,"generally faster than Transformers.
121"
THEORY,0.23846153846153847,"3
Theory
122"
THEORY,0.2403846153846154,"We have examined the traditional function perspective of NOs. However, machine learning often
123"
THEORY,0.2423076923076923,"overlooks the complexity of mappings between infinite-dimensional functions. To address this, we
124"
THEORY,0.24423076923076922,"propose studying NOs in the coefficient space. By representing NOs as mappings between infinite
125"
THEORY,0.24615384615384617,"sequences of real numbers, derived from the expansion of input functions on an orthogonal basis, we
126"
THEORY,0.24807692307692308,"can leverage the extensive literature on mappings between finite-dimensional vectors and extend it to
127"
THEORY,0.25,"our context. This enables a comprehensive analysis of NOs from a new perspective.
128"
THEORY,0.2519230769230769,"Given a complete orthogonal basis {ϕi}∞
i=0, the input function f and the output function G(f) can be
129"
THEORY,0.25384615384615383,"expanded as f = P∞
i=0 ciϕi, G(f) = P∞
i=0 diϕi where ci = ⟨f, ϕi⟩and di = ⟨G(f), ϕi⟩. For the
130"
THEORY,0.25576923076923075,"infinite sums to converge, the infinite sequences {ci}, {di} ∈l2. So, the operator learning problem
131"
THEORY,0.25769230769230766,"on G can be abstracted to a mapping between infinite sequences between numbers in the l2 space, i.e.,
132"
THEORY,0.25961538461538464,"NOs aim to learn the mapping {ci}∞
i=0 7→{di}∞
i=0. Thus, we can define the input space from the
133"
THEORY,0.26153846153846155,"viewpoint of coefficients B := {(⟨f, ϕi⟩)∞
i=1, f ∈A}. where A is the space of input functions.
134"
THEORY,0.26346153846153847,"We reinterpreted the operations in NOs from the sequences mapping perspective in l2.
135"
THEORY,0.2653846153846154,"Kernel Transform. It maps f = P∞
i=0 ciϕi to κ(f) =
R
K(B; x, y)f(y)dy = PNmodes
i=0
Biciϕi,
136"
THEORY,0.2673076923076923,"which can be abstracted to: c≤Nmodes 7→B≤Nmodesc≤Nmodes, c>Nmodes 7→0, due to the kernel’s local
137"
THEORY,0.2692307692307692,"rankness and truncation at the Nmodes-lowest frequency.
138"
THEORY,0.27115384615384613,"Linear Transform. For an input function f(x), the linear transform maps f(x) to W f(x). Also,
139"
THEORY,0.27307692307692305,"considering the channel-wise operation, from the sequence space point of view, it is ci 7→W ci.
140"
THEORY,0.275,"Compared to the previous kernel transform, the linear coefficient is the same for all elements in the
141"
THEORY,0.27692307692307694,"sequence, but those are different and truncated in the previous kernel transform.
142"
THEORY,0.27884615384615385,"Nonlinear Activation. The nonlinear activation σ is an abstract and fixed mapping between l2
143"
THEORY,0.28076923076923077,"to itself, denoted Σ : l2 →l2, c 7→Σc. In the original function space, the mapping is from the
144"
THEORY,0.2826923076923077,"input function f(x) to σ(f)(x) where σ : C(Ω) →C(Ω). However, in the sequence space, the
145"
THEORY,0.2846153846153846,"original ci is ci = ⟨f, ϕi⟩, and (Σc)i = ⟨σ(f), ϕi⟩. The ith entry of Σc may depend on all ci for
146"
THEORY,0.2865384615384615,"every i since the activation is performed on the whole input function. For instance, for the input
147"
THEORY,0.28846153846153844,"function f(x) = cos(kx) and the cosine basis, only ck = 0 while other entries equal zero. But
148"
THEORY,0.2903846153846154,"σ(f)(x) = σ(cos(kx)) is a very complicated function even for simple σ like ReLU, Sigmoid, and
149"
THEORY,0.2923076923076923,"Tanh activations, with the post-transformed coefficients (Σc)i ̸= 0 for all i. Although it is abstract,
150"
THEORY,0.29423076923076924,"the Lipschitz continuity is kept:
151"
THEORY,0.29615384615384616,"Proposition 3.1. If σ is L-Lipschitz, i.e., |σ(x) −σ(y)| ≤L|x −y|, then the mapping Σ is also
152"
THEORY,0.2980769230769231,"L-Lipschitz in the l2 space.
153"
THEORY,0.3,"Model Summary. Denote the input function as ˆu0(x), and its expansion over the orthogonal basis to
154"
THEORY,0.3019230769230769,"be {ˆc0,i}∞
i=1, i.e., ˆu0 = P ˆc0,iϕi, then the recursive formulation of NO in the coefficient space is
155"
THEORY,0.3038461538461538,"ˆcl,i = Σcl,i,
l ≥1;
cl+1,≤Nmodes = (Bl,≤Nmodes + W l)ˆcl,≤Nmodes,
cl+1,>Nmodes = W lˆcl,>Nmodes;
vi = cL,i;
(2)"
THEORY,0.3057692307692308,"where vi is the coefficient for the output function, i.e., the output function v(x) = P∞
i=1 viϕi(x),
156"
THEORY,0.3076923076923077,"l is the layer index, and L is the total number of layers. For all the indices cl,i, the first l is for the
157"
THEORY,0.3096153846153846,"layer, while the second i is for the index of the orthogonal basis. ˆc and c are the post-activation and
158"
THEORY,0.31153846153846154,"pre-activation input coefficients, respectively.
159"
GENERALIZATION OF NOS,0.31346153846153846,"3.1
Generalization of NOs
160"
GENERALIZATION OF NOS,0.3153846153846154,"From the sequence perspective, we can derive the generalization bound of NOs via the robustness
161"
GENERALIZATION OF NOS,0.3173076923076923,"bound [29, 13]. The generalization gap of the model given in equation (2) can be bounded as follows.
162"
GENERALIZATION OF NOS,0.3192307692307692,"Theorem 3.1. (Generalization bound of NOs) For any δ ∈(0, 1), with probability at least 1 −δ over
163"
GENERALIZATION OF NOS,0.3211538461538462,"the choice of random samples S = {fj}Ntrain
j=1 ∼µ, let the model parameters after optimization to be
164"
GENERALIZATION OF NOS,0.3230769230769231,"θS = {{Bl,i}Nmodes
i=1 , W l}L
l=0, the following bound holds:
165"
GENERALIZATION OF NOS,0.325,"|Ltest-reg(θS) −Ltrain(θS)| ≤ L
Y l=0"
GENERALIZATION OF NOS,0.3269230769230769,"
max
i
{∥Bl,i + W l∥2, ∥W l∥2}

γ + M r"
GENERALIZATION OF NOS,0.32884615384615384,2K log 2 + 2 log(1/δ)
GENERALIZATION OF NOS,0.33076923076923076,"Ntrain
, (3)"
GENERALIZATION OF NOS,0.3326923076923077,"for all γ > 0, where K = N (γ/2, B, ∥· ∥l2) is the γ/2-covering number of the input space B under
166"
GENERALIZATION OF NOS,0.3346153846153846,"the norm ∥· ∥l2. M is the upper bound of the loss function.
167"
GENERALIZATION OF NOS,0.33653846153846156,"All proofs are presented in the Appendix. The generalization bounds, similar to vanilla neural nets,
168"
GENERALIZATION OF NOS,0.3384615384615385,"rely on the products of multilayer parameter norms [2, 29]. Theorem 3.1 offers a more detailed
169"
GENERALIZATION OF NOS,0.3403846153846154,"characterization of the generalization bounds compared to the findings in [14], and it guides selecting
170"
GENERALIZATION OF NOS,0.3423076923076923,"orthogonal bases in NO. We will delve into this topic further in Section 4.
171"
GENERALIZATION OF NOS,0.34423076923076923,"Extension to Discretized NOs. In Theorem 3.1, we primarily focus on continuous NOs. However,
172"
GENERALIZATION OF NOS,0.34615384615384615,"the presented theory can be extended to discrete NOs by substituting the infinite-dimensional l2
173"
GENERALIZATION OF NOS,0.34807692307692306,"space with a finite-dimensional vector space. In this context, inner products and orthogonal bases
174"
GENERALIZATION OF NOS,0.35,"can still be defined. The modification lies in the term involving the covering number in the bound
175"
GENERALIZATION OF NOS,0.35192307692307695,"N(γ/2, B, ∥· ∥l2). Here, we replace the l2 space and norm with the finite-dimensional Euclidean
176"
GENERALIZATION OF NOS,0.35384615384615387,"space corresponding to the discrete NO and its vector 2-norm.
177"
SUPER-RESOLUTION ERROR,0.3557692307692308,"3.2
Super-resolution Error
178"
SUPER-RESOLUTION ERROR,0.3576923076923077,"Super-resolution involves training a model on a low-resolution grid and evaluating it on a high-
179"
SUPER-RESOLUTION ERROR,0.3596153846153846,"resolution grid, with the expectation of comparable performance. While FNO [20] demonstrates
180"
SUPER-RESOLUTION ERROR,0.36153846153846153,"excellent super-resolution capabilities, the underlying reasons remain poorly understood. This
181"
SUPER-RESOLUTION ERROR,0.36346153846153845,"understanding is crucial for two reasons: (1) enabling training on sparse grids, leading to reduced
182"
SUPER-RESOLUTION ERROR,0.36538461538461536,"training time, and (2) ensuring NOs can effectively handle inputs of the same function on different
183"
SUPER-RESOLUTION ERROR,0.36730769230769234,"grids and produce satisfactory results.
184"
SUPER-RESOLUTION ERROR,0.36923076923076925,"Before delving into the analysis, it is important to address the numerical integration errors that
185"
SUPER-RESOLUTION ERROR,0.37115384615384617,"arise when training NOs on low-resolution grids compared to high-resolution grids during super-
186"
SUPER-RESOLUTION ERROR,0.3730769230769231,"resolution. Intuitively, if the integration error is significant, there will be notable discrepancies in the
187"
SUPER-RESOLUTION ERROR,0.375,"integral values obtained from the sparse training grid and the high-resolution testing grid, resulting in
188"
SUPER-RESOLUTION ERROR,0.3769230769230769,"inconsistent model performance between training and testing.
189"
SUPER-RESOLUTION ERROR,0.37884615384615383,"As ul, ˆul, and v represent variables in continuous NOs, we use Ul, ˆUl, and V to represent variables
190"
SUPER-RESOLUTION ERROR,0.38076923076923075,"in the discrete NOs using numerical integration rule ˆR
g ≈PNgrid
i=1 wig(xi) for any integrand g:
191"
SUPER-RESOLUTION ERROR,0.38269230769230766,"ˆUl(x) = σ (Ul(x)) , l ≥1;
V = UL;
U0 = f;"
SUPER-RESOLUTION ERROR,0.38461538461538464,"Ul+1(x) =
ˆZ
K(Bl, x, y) ˆUl(y)dy + W l ˆUl(x) ="
SUPER-RESOLUTION ERROR,0.38653846153846155,"Ngrid
X"
SUPER-RESOLUTION ERROR,0.38846153846153847,"i=1
wiK(Bl, x, xi) ˆUl(xi) + W l ˆUl(x),"
SUPER-RESOLUTION ERROR,0.3903846153846154,"where f is the input function, wi is the weight for numerical integral at the grid point xi. The error in
192"
SUPER-RESOLUTION ERROR,0.3923076923076923,"numerical integration generally depends on two factors: the grid size (defined as egrid(Ngrid)) and the
193"
SUPER-RESOLUTION ERROR,0.3942307692307692,"smoothness of the integrand function (defined as efunc(f)).
194"
SUPER-RESOLUTION ERROR,0.39615384615384613,"For instance, on a uniform grid over the interval [a, b], the integral is approximated using the Darboux
195"
SUPER-RESOLUTION ERROR,0.39807692307692305,"method
R b
a f ≈1/Ngrid
PNgrid
i=1 f(xi) where xi = a+(i−1)(b−a)/Ngrid, which yields an integration
196"
SUPER-RESOLUTION ERROR,0.4,"error of O(f ′′(ξ)/Ngrid) where ξ ∈(a, b), i.e., egrid(Ngrid) = 1/N 2
grid, efunc(f) = f ′′(ξ). However,
197"
SUPER-RESOLUTION ERROR,0.40192307692307694,"using the trapezoidal rule instead, the error can be reduced to O(f ′′(ξ)/N 2
grid) without requiring
198"
SUPER-RESOLUTION ERROR,0.40384615384615385,"additional computations. FNO [20] assumes that the input function is periodic, so the error of the
199"
SUPER-RESOLUTION ERROR,0.40576923076923077,"uniform grid decreases to O(f ′′(ξ)/N 2
grid). For the Gaussian quadrature on the interval [a, b], the error
200"
SUPER-RESOLUTION ERROR,0.4076923076923077,"can be reduced to O
 
(Ngrid!)4f 2(Ngrid)(ξ)/[(2Ngrid)!]3
for ξ ∈(a, b) [10]. With the background, we
201"
SUPER-RESOLUTION ERROR,0.4096153846153846,"can write the discretization error of NOs:
202"
SUPER-RESOLUTION ERROR,0.4115384615384615,"Theorem 3.2. (Discretization error of NOs) Suppose the numerical integration’s error is
203"
SUPER-RESOLUTION ERROR,0.41346153846153844,"egrid(Ngrid)efunc(f) where Ngrid is the grid size, and f is the integrand, then the discretization
204"
SUPER-RESOLUTION ERROR,0.4153846153846154,"error of discrete NOs compared with continuous ones due to numerical integral is upper bounded by
205"
SUPER-RESOLUTION ERROR,0.4173076923076923,"∥v −V ∥L2 ≤ L
X l=0 L
Y"
SUPER-RESOLUTION ERROR,0.41923076923076924,"k=l
max
i
{∥Bk,i + W k∥2, ∥W k∥2}"
SUPER-RESOLUTION ERROR,0.42115384615384616,"Nmodes
X"
SUPER-RESOLUTION ERROR,0.4230769230769231,"i=0
∥Bi,l∥2egrid(Ngrid)efunc

ˆUl · ϕi
! .
(4)"
SUPER-RESOLUTION ERROR,0.425,"The discretization error in discrete NOs relies on the norm of model parameters and the accuracy of
206"
SUPER-RESOLUTION ERROR,0.4269230769230769,"the integration method employed for integrating intermediate output functions. Building upon this,
207"
SUPER-RESOLUTION ERROR,0.4288461538461538,"we can derive the super-resolution error of discrete NOs. Firstly, we bound the prediction error of
208"
SUPER-RESOLUTION ERROR,0.4307692307692308,"continuous NOs across all points in the domain Ω(i.e., the super-resolution error of continuous NOs).
209"
SUPER-RESOLUTION ERROR,0.4326923076923077,"During the training phase, NOs are trained on a finite training grid, leading to this error. Subsequently,
210"
SUPER-RESOLUTION ERROR,0.4346153846153846,"we bound the discrepancy between continuous and discrete NOs, corresponding to the discretization
211"
SUPER-RESOLUTION ERROR,0.43653846153846154,"error stated in Theorem 3.2. These two terms are reflected in the following theorem.
212"
SUPER-RESOLUTION ERROR,0.43846153846153846,"Theorem 3.3. (Super-resolution error of NOs) Assuming a uniform grid on a bounded regular
213"
SUPER-RESOLUTION ERROR,0.4403846153846154,"domain with Ngrid points in the FNO [20] setting, under the same notation as Theorem 3.1. Then, the
214"
SUPER-RESOLUTION ERROR,0.4423076923076923,"super-resolution error of the discrete NO model for the input function f with intermediate output ˆUl
215"
SUPER-RESOLUTION ERROR,0.4442307692307692,"(as detailed in equation (3.2)) can be bounded as follows:
216"
SUPER-RESOLUTION ERROR,0.4461538461538462,"|Ltest-sr(θS) −Ltest-reg(θS)| ≤ L
X l=0 L
Y"
SUPER-RESOLUTION ERROR,0.4480769230769231,"k=l
max
i
{∥Bk,i + W k∥2, ∥W k∥2}"
SUPER-RESOLUTION ERROR,0.45,"Nmodes
X"
SUPER-RESOLUTION ERROR,0.4519230769230769,"i=0
∥Bl,i∥2egrid(Ngrid)efunc

ˆUl · ϕi
! +"
SUPER-RESOLUTION ERROR,0.45384615384615384,"(Nmodes
X i=0 L−1
X l=0 L−1
Y"
SUPER-RESOLUTION ERROR,0.45576923076923076,"k=l+1
∥W k∥2 !"
SUPER-RESOLUTION ERROR,0.4576923076923077,"∥Bl,i⟨ˆul, ϕi⟩∥2Lip(ϕi) + L−1
Y"
SUPER-RESOLUTION ERROR,0.4596153846153846,"l=0
∥W l∥2Lip(f) )"
SUPER-RESOLUTION ERROR,0.46153846153846156,/Ngrid. (5)
SUPER-RESOLUTION ERROR,0.4634615384615385,"The first term pertains to the integral error in Theorem 3.2, while the second term relates to the
217"
SUPER-RESOLUTION ERROR,0.4653846153846154,"interpolation and generalization ability of NOs across the entire domain. These factors significantly
218"
SUPER-RESOLUTION ERROR,0.4673076923076923,"influence the super-resolution of NOs. Understanding and addressing these factors is crucial for
219"
SUPER-RESOLUTION ERROR,0.46923076923076923,"enhancing super-resolution accuracy in NOs, which will be thoroughly discussed in Section 4.
220"
IMPLICATION AND APPLICATION OF THE THEORY,0.47115384615384615,"4
Implication and Application of the Theory
221"
IMPLICATION AND APPLICATION OF THE THEORY,0.47307692307692306,"In this subsection, we introduce the implications and applications of the proposed theory and
222"
IMPLICATION AND APPLICATION OF THE THEORY,0.475,"correspond them to the following numerical experiments.
223"
IMPLICATION AND APPLICATION OF THE THEORY,0.47692307692307695,"Tighter Bound in Theorem 3.1. Our bound’s proof and form are much more general and tighter
224"
IMPLICATION AND APPLICATION OF THE THEORY,0.47884615384615387,"than previous work [14]. In particular, in terms of parameter matrix norm contributed by each layer,
225"
IMPLICATION AND APPLICATION OF THE THEORY,0.4807692307692308,"the bound in [14] depends on ∥W l∥F + ∥Bl∥F N d/2
modes where ∥· ∥denotes the Frobienus norm.
226"
IMPLICATION AND APPLICATION OF THE THEORY,0.4826923076923077,"So, our reliance of maxi {∥Bl,i + W l∥2, ∥W l∥2} is a significant improvement. Additionally, our
227"
IMPLICATION AND APPLICATION OF THE THEORY,0.4846153846153846,"bound suggests that increasing the number of modes does not necessarily increase the complexity
228"
IMPLICATION AND APPLICATION OF THE THEORY,0.48653846153846153,"of the model. However, it is important to note that selecting high-frequency basis functions to fit
229"
IMPLICATION AND APPLICATION OF THE THEORY,0.48846153846153845,"high-frequency noise can adversely impact generalization, as it leads to larger values of ∥Bl,i+W l∥2.
230"
IMPLICATION AND APPLICATION OF THE THEORY,0.49038461538461536,"We shall verify the advantage of our bound in Experiment 6.1.
231"
IMPLICATION AND APPLICATION OF THE THEORY,0.49230769230769234,"Super-Resolution Error. From Theorem 3.3, super-resolution in NOs depends on two critical
232"
IMPLICATION AND APPLICATION OF THE THEORY,0.49423076923076925,"factors: (1) the accuracy of integration trained on low-resolution grids, and (2) the density of the
233"
IMPLICATION AND APPLICATION OF THE THEORY,0.49615384615384617,"low-resolution grid to facilitate generalization to other points. It is important to note that numerical
234"
IMPLICATION AND APPLICATION OF THE THEORY,0.4980769230769231,"integration accuracy does not necessarily imply grid density, especially in the case of low-precision
235"
IMPLICATION AND APPLICATION OF THE THEORY,0.5,"integration formats. We shall experiment on the super-resolution error in Experiment 6.2.
236"
IMPLICATION AND APPLICATION OF THE THEORY,0.5019230769230769,"Choice of Grid. The choice of grids depends on the specific function and its domain. In the case
237"
IMPLICATION AND APPLICATION OF THE THEORY,0.5038461538461538,"of a finite interval, a uniform grid is commonly used, and integration can be performed using the
238"
IMPLICATION AND APPLICATION OF THE THEORY,0.5057692307692307,"trapezoidal rule. Furthermore, selecting a grid that allows for accurate integration effectively captures
239"
IMPLICATION AND APPLICATION OF THE THEORY,0.5076923076923077,"the function’s characteristics at a finite number of points. For instance, in density functional theory
240"
IMPLICATION AND APPLICATION OF THE THEORY,0.5096153846153846,"(DFT), where the domain is R3, the function typically involves a multi-center Gaussian mixture. In
241"
IMPLICATION AND APPLICATION OF THE THEORY,0.5115384615384615,"such cases, designing an integral scheme tailored to multi-center functions is more reasonable. For
242"
IMPLICATION AND APPLICATION OF THE THEORY,0.5134615384615384,"example, selecting points in the vicinity of each Gaussian center enables better characterization and
243"
IMPLICATION AND APPLICATION OF THE THEORY,0.5153846153846153,"integration accuracy for the input function. We will conduct the corresponding DFT experiment on
244"
IMPLICATION AND APPLICATION OF THE THEORY,0.5173076923076924,"molecules in 6.3.
245"
IMPLICATION AND APPLICATION OF THE THEORY,0.5192307692307693,"Extending NOs to Irregular Domains. The limitation of FNO [19, 23] is its reliance on Fourier
246"
IMPLICATION AND APPLICATION OF THE THEORY,0.5211538461538462,"bases, which restricts its application to regular domains and limits its usage in real-world complex
247"
IMPLICATION AND APPLICATION OF THE THEORY,0.5230769230769231,"geometries. Geo-FNO [19] assumes that irregular domains can be mapped to regular ones through a
248"
IMPLICATION AND APPLICATION OF THE THEORY,0.525,"bijection, which is not applicable for arbitrarily irregular domains. Fortunately, under our framework
249"
IMPLICATION AND APPLICATION OF THE THEORY,0.5269230769230769,"in equation (2), we can overcome this limitation by utilizing random Fourier features (RFFs) and
250"
IMPLICATION AND APPLICATION OF THE THEORY,0.5288461538461539,"polynomials on irregular domains. By employing Gram-Schmidt orthogonalization, we can obtain an
251"
IMPLICATION AND APPLICATION OF THE THEORY,0.5307692307692308,"orthogonal basis and perform numerical integration on a discrete grid for the inner product. Moreover,
252"
IMPLICATION AND APPLICATION OF THE THEORY,0.5326923076923077,"we can select a suitable orthogonal basis based on the domain, such as Gauss-Hermite polynomials
253"
IMPLICATION AND APPLICATION OF THE THEORY,0.5346153846153846,"for the unbounded whole space R. As a result, we successfully extend NOs to irregular domains,
254"
IMPLICATION AND APPLICATION OF THE THEORY,0.5365384615384615,"enhancing their applicability in various scenarios. In the first setting of Experiment 6.3, we validate
255"
IMPLICATION AND APPLICATION OF THE THEORY,0.5384615384615384,"our general NOs on unbounded domains.
256"
IMPLICATION AND APPLICATION OF THE THEORY,0.5403846153846154,"Guiding the Choice of Orthogonal Basis. The choice of basis in NOs significantly impacts their
257"
IMPLICATION AND APPLICATION OF THE THEORY,0.5423076923076923,"expressiveness, as highlighted by Theorem 3.1. If the target function can be well represented by a
258"
IMPLICATION AND APPLICATION OF THE THEORY,0.5442307692307692,"finite combination of basis functions, a small number of modes (Nmodes) is sufficient. This leads to
259"
IMPLICATION AND APPLICATION OF THE THEORY,0.5461538461538461,"fewer parameters and increased model efficiency. Conversely, if an infinite series of basis functions is
260"
IMPLICATION AND APPLICATION OF THE THEORY,0.5480769230769231,"needed to expand the target function, the model tends to generalize poorly. For example, Fourier bases
261"
IMPLICATION AND APPLICATION OF THE THEORY,0.55,"are suitable for periodic functions, wavelets excel in capturing rapid changes and discontinuities, and
262"
IMPLICATION AND APPLICATION OF THE THEORY,0.551923076923077,"polynomials (e.g., orthogonal Legendre polynomials) provide a versatile basis for all functions. These
263"
IMPLICATION AND APPLICATION OF THE THEORY,0.5538461538461539,"bases have distinct capabilities and cannot efficiently represent each other. Additionally, wavelets
264"
IMPLICATION AND APPLICATION OF THE THEORY,0.5557692307692308,"are effective in handling multi-scale and multi-physics problems. Combining multiple basis sets can
265"
IMPLICATION AND APPLICATION OF THE THEORY,0.5576923076923077,"yield superior results by leveraging complementary effects. The selection of the basis is guided by
266"
IMPLICATION AND APPLICATION OF THE THEORY,0.5596153846153846,"the characteristics of the function and the operator in the dataset. For instance, if the function exhibits
267"
IMPLICATION AND APPLICATION OF THE THEORY,0.5615384615384615,"periodicity in certain subdomains but not in others, a combination of Fourier and polynomial basis
268"
IMPLICATION AND APPLICATION OF THE THEORY,0.5634615384615385,"functions can effectively model both parts simultaneously, which is verified in Experiment 6.4.
269"
RELATED WORK,0.5653846153846154,"5
Related Work
270"
RELATED WORK,0.5673076923076923,"Orthogonal Basis in NOs. Various orthogonal bases are adopted for kernel transform in neural
271"
RELATED WORK,0.5692307692307692,"operators, e.g., Fourier NO (FNO) [20] and its variant [25, 27, 30] adopt Fourier bases, Geo-FNO
272"
RELATED WORK,0.5711538461538461,"[19] utilizes Fourier bases under deformation, [21] uses orthogonal Legendre polynomials, and [28, 8]
273"
RELATED WORK,0.573076923076923,"use multi-wavelets.
274"
RELATED WORK,0.575,"NOs on Arbitrary Domains. Some operator networks different from NO are grid-free. DeepONet
275"
RELATED WORK,0.5769230769230769,"[22] encodes the input function and the grid, respectively, and then combines them by dot product as
276"
RELATED WORK,0.5788461538461539,"the operator network output. MIONet [12] extends DeepONet to multiple input functions. Trans-
277"
RELATED WORK,0.5807692307692308,"former operators [3, 18, 9] directly process the inputs by the attention mechanism [26]. The NO
278"
RELATED WORK,0.5826923076923077,"we proposed can also be applied to any domain and incorporates its prior knowledge, so our NO
279"
RELATED WORK,0.5846153846153846,"generally outperforms these two approaches.
280"
RELATED WORK,0.5865384615384616,"Theory of Neural Operators. DeepONet and FNO are analyzed theoretically in the literature.
281"
RELATED WORK,0.5884615384615385,"Theory on DeepONet [16, 7] relies on the discretization over function and the input grid to transform
282"
RELATED WORK,0.5903846153846154,"the model into mapping between finite-dimensional vector space. The generalization theory on FNO
283"
RELATED WORK,0.5923076923076923,"[14] relies on discretization and proposes Rademacher complexity bounds. [15] proves the universal
284"
RELATED WORK,0.5942307692307692,"approximation and errors bound for approximating Darcy type elliptic PDE and the incompressible
285"
RELATED WORK,0.5961538461538461,"Navier-Stokes equations. [5, 6, 23] proposes theories for NO’s approximation. [4] proves convergence
286"
RELATED WORK,0.5980769230769231,"rates for linear operators.
287"
EXPERIMENT,0.6,"6
Experiment
288"
EXPERIMENT,0.6019230769230769,"6.1
Validation of Theorem 3.1
289"
EXPERIMENT,0.6038461538461538,"This section provides empirical evidence demonstrating the superior tightness and quality of our
290"
EXPERIMENT,0.6057692307692307,"generalization bound presented in Theorem 3.1 compared to related works such as [14]. It is
291"
EXPERIMENT,0.6076923076923076,"customary in the literature to compare the numerical values of various generalization bounds as a
292"
EXPERIMENT,0.6096153846153847,"means to showcase their tightness, e.g., [24, 1, 11, 2]. In Figure 1, we provide numerical values
293"
EXPERIMENT,0.6115384615384616,"of the generalization bounds for FNO models trained on four distinct datasets (1D Burgers, 2D
294"
EXPERIMENT,0.6134615384615385,"Darcy Flow, 2D+time Navier-Stokes equation, 3D Navier-Stokes equation) as provided by FNO [20].
295"
EXPERIMENT,0.6153846153846154,"Following FNO’s experimental setup, we normalize the value of our proposed bound to 1 for clarity.
296"
EXPERIMENT,0.6173076923076923,"Remarkably, our robustness-based bound outperforms existing bounds by 2-3 orders of magnitude,
297"
EXPERIMENT,0.6192307692307693,"underscoring its superior tightness and reliability.
298"
D BURGERS,0.6211538461538462,"1D Burgers
2D Darcy
2D+Time NS
3D NS
Datasets 100 101 102 103 104"
D BURGERS,0.6230769230769231,Bound Values
D BURGERS,0.625,1789.59
D BURGERS,0.6269230769230769,5623.29
D BURGERS,0.6288461538461538,"626.22
732.52"
D BURGERS,0.6307692307692307,"1.00
1.00
1.00
1.00"
D BURGERS,0.6326923076923077,Our bound is much tighter than previous bound.
D BURGERS,0.6346153846153846,"Rademacher Complexity Bound
Our Robustness Bound"
D BURGERS,0.6365384615384615,"Figure 1: In the four datasets, our generaliza-
tion bounds (green) are tighter by 2-3 orders
of magnitude compared to [14] (blue)."
D BURGERS,0.6384615384615384,"Ngrid = 26
Ngrid = 28
Ngrid = 210 10
4 10
3 10
2 10
1"
D BURGERS,0.6403846153846153,"Super-res Gap: Ltest
reg
Ltest
sr"
D BURGERS,0.6423076923076924,"1e-01
9e-02 6e-02 2e-02 6e-03 2e-03 4e-03 9e-05 5e-05"
D BURGERS,0.6442307692307693,Super-Resolution Error and Integral Scheme
D BURGERS,0.6461538461538462,Random: O(1/ N)
D BURGERS,0.6480769230769231,Darboux: O(1/N)
D BURGERS,0.65,Trapezoid: O(1/N2)
D BURGERS,0.6519230769230769,"Figure 2: The relationship between super-
resolution error, integration format, and the
number of integration grid points."
SUPER-RESOLUTION ERROR,0.6538461538461539,"6.2
Super-resolution Error
299"
SUPER-RESOLUTION ERROR,0.6557692307692308,"We validate Theorem 3.3 that super-resolution error is affected by both the integration scheme and
300"
SUPER-RESOLUTION ERROR,0.6576923076923077,"the grid size. We conduct experiments on the previously mentioned Burgers dataset from FNO
301"
SUPER-RESOLUTION ERROR,0.6596153846153846,"Table 1: Relative L2 error results on DFT datasets with unbounded domains. ANO achieves the best
result and can conduct SR."
SUPER-RESOLUTION ERROR,0.6615384615384615,"FNO
Geo-FNO
DeepONet
LT
OFormer
NO-Ours
QHO-vanilla
/
1.22E-2
2.63E-3
3.04E-3
3.31E-3
1.50E-3
QHO-superres
/
1.32E-1
1.42E+0
7.15E-3
7.91E-3
2.02E-3
CO2-vanilla
/
4.97E-1
5.82E-1
2.46E-1
2.41E-1
2.22E-1
CO2-superres
/
5.44E-1
1.44E+0
2.51E-1
2.47E-1
2.23E-1
Water-vanilla
/
2.41E-1
3.22E-1
3.03E-1
2.86E-1
1.52E-1
Water-superres
/
5.07E-1
1.75E+0
4.52E-1
3.07E-1
1.58E-1
CH4-vanilla
/
3.16E-1
5.19E-1
2.76E-1
2.62E-1
2.05E-1
CH4-superres
/
3.95E-1
1.82E+0
2.81E-1
2.79E-1
2.07E-1"
SUPER-RESOLUTION ERROR,0.6634615384615384,"[20], where we use the random scheme (error: 1/
√"
SUPER-RESOLUTION ERROR,0.6653846153846154,"N, blue), Darboux rule based on a uniform grid
302"
SUPER-RESOLUTION ERROR,0.6673076923076923,"(error: 1/N, green), and trapezoidal rule based on a uniform grid (error: 1/N 2, red) as integration
303"
SUPER-RESOLUTION ERROR,0.6692307692307692,"schemes in FNO [20]. The grid sizes are chosen to be 26 (first column), 28 (second column), 210
304"
SUPER-RESOLUTION ERROR,0.6711538461538461,"(third column) points. The super-resolution performance is evaluated by the super-resolution gap
305"
SUPER-RESOLUTION ERROR,0.6730769230769231,"Ltest-reg −Ltest-sr originally defined in Theorem 3.3. Figure 2 illustrates the following findings: (1)
306"
SUPER-RESOLUTION ERROR,0.675,"increasing the grid size for the same integration scheme improves super-resolution performance, and
307"
SUPER-RESOLUTION ERROR,0.676923076923077,"(2) for the same grid size, integration schemes with higher accuracy yield lower super-resolution
308"
SUPER-RESOLUTION ERROR,0.6788461538461539,"error. These results confirm the validity of our theory. It is important to note that the original FNO
309"
SUPER-RESOLUTION ERROR,0.6807692307692308,"framework cannot operate on random grids. Our reinterpretation of NOs, utilizing random grids as
310"
SUPER-RESOLUTION ERROR,0.6826923076923077,"the integration format and Fourier bases, enabled this capability.
311"
NOS ON UNBOUNDED DOMAIN,0.6846153846153846,"6.3
NOs on Unbounded Domain
312"
NOS ON UNBOUNDED DOMAIN,0.6865384615384615,"In this subsection, we select several Density Functional Theory (DFT) Hamiltonian operators to test
313"
NOS ON UNBOUNDED DOMAIN,0.6884615384615385,"our NOs with suitable orthogonal bases with other strong baselines.
314"
NOS ON UNBOUNDED DOMAIN,0.6903846153846154,"Baselines. (1) FNO [20]: cannot be adopted on an arbitrary domain. (2) Geo-FNO [19]: uses a
315"
NOS ON UNBOUNDED DOMAIN,0.6923076923076923,"bijection to map the irregular domain to a regular one and perform FNO. Here the unbounded domains
316"
NOS ON UNBOUNDED DOMAIN,0.6942307692307692,"can be mapped to the regular domain by the bijection tan−1, which can conduct super-resolution. (3)
317"
NOS ON UNBOUNDED DOMAIN,0.6961538461538461,"DeepONet [22] can operate on arbitrary domains but accepts fixed-length discretized input function,
318"
NOS ON UNBOUNDED DOMAIN,0.698076923076923,"which restricts its super-resolution performance. (4) Linear Transformer (LT) [3] and (5) OFormer
319"
NOS ON UNBOUNDED DOMAIN,0.7,"[18] are all transformers for operator learning, they can handle arbitrary domains and grids.
320"
NOS ON UNBOUNDED DOMAIN,0.7019230769230769,"In Density Functional Theory (DFT), the Hamiltonian operator plays a crucial role in characterizing
321"
NOS ON UNBOUNDED DOMAIN,0.7038461538461539,"the ground state energy through its spectra. In this subsection, we evaluate different NOs’ performance
322"
NOS ON UNBOUNDED DOMAIN,0.7057692307692308,"in learning Hamiltonian operators defined on unbounded domains across various dimensions.
323"
NOS ON UNBOUNDED DOMAIN,0.7076923076923077,"QHO. In the quantum harmonic oscillator (QHO), the Hamiltonian operator, given the wave function
324"
NOS ON UNBOUNDED DOMAIN,0.7096153846153846,"ϕ, is

ˆHQHOϕ

(x) = −1"
NOS ON UNBOUNDED DOMAIN,0.7115384615384616,2∇2ϕ(x) + 1
NOS ON UNBOUNDED DOMAIN,0.7134615384615385,"2x2ϕ(x), x ∈R. We use random linear combinations of the
325"
NOS ON UNBOUNDED DOMAIN,0.7153846153846154,"1D Hermite polynomial for data generation: ϕi(x) =
1"
NOS ON UNBOUNDED DOMAIN,0.7173076923076923,"π
1
4 2
i
2 √"
NOS ON UNBOUNDED DOMAIN,0.7192307692307692,"i!
Hi(x)e−x2"
NOS ON UNBOUNDED DOMAIN,0.7211538461538461,"2 , x ∈R, The grid for
326"
NOS ON UNBOUNDED DOMAIN,0.7230769230769231,"training is the points in the Gauss-Hermite quadrature with 32 points, and the super-resolution testing
327"
NOS ON UNBOUNDED DOMAIN,0.725,"grid is with degree 64. Geo-FNO adopts the grid generated by the tan−1 transform of uniform grid
328"
NOS ON UNBOUNDED DOMAIN,0.7269230769230769,"over (−π 2 , π"
NOS ON UNBOUNDED DOMAIN,0.7288461538461538,"2 ). The orthogonal basis in our NO is Gauss-Hermite polynomials.
329"
NOS ON UNBOUNDED DOMAIN,0.7307692307692307,"Molecules. Following D4FT [17], we consider Hamiltonian operators in real-world 3D molecules,
330"
NOS ON UNBOUNDED DOMAIN,0.7326923076923076,"which take the wave function ϕ(r), and the density ρ(r) as inputs is given by four different terms
331"
NOS ON UNBOUNDED DOMAIN,0.7346153846153847,"(kinetic, external potential, Hartree and exchange-correlation):
332"
NOS ON UNBOUNDED DOMAIN,0.7365384615384616,"
ˆHKS-DFT(ϕ, ρ)

(r) = −1"
NOS ON UNBOUNDED DOMAIN,0.7384615384615385,"2∇2ϕ(r) + vext(r)ϕ(r) +
Z"
NOS ON UNBOUNDED DOMAIN,0.7403846153846154,"R3
ρ(r′)
|r −r′|dr′ϕ(r) + vxc(r)ϕ(r), r ∈R3.
(6)"
NOS ON UNBOUNDED DOMAIN,0.7423076923076923,"KS-DFT offers natural basis sets that can be linearly combined to generate the training functions.
333"
NOS ON UNBOUNDED DOMAIN,0.7442307692307693,"Additionally, grids with varying resolutions, characterized by levels, are provided based on the
334"
NOS ON UNBOUNDED DOMAIN,0.7461538461538462,"multi-center Gaussian nature of the functions in DFT. In our experiments, we select level 1 for
335"
NOS ON UNBOUNDED DOMAIN,0.7480769230769231,"training and level 2, which has more points, for testing. In Geo-FNO, the grid is generated using
336"
NOS ON UNBOUNDED DOMAIN,0.75,"the tan−1 transform of a uniform grid over the domain (−π 2 , π"
NOS ON UNBOUNDED DOMAIN,0.7519230769230769,"2 )3. To ensure a fair comparison, the
337"
NOS ON UNBOUNDED DOMAIN,0.7538461538461538,Table 2: Relative L2 error results for the advection case.
NOS ON UNBOUNDED DOMAIN,0.7557692307692307,"NO-Sin/Sin
NO-Poly/Poly
NO-Sin/Poly
Advection (1)
8.34E-3
1.96E-2
1.01E-2
Advection (2)
1.00E-2
1.76E-2
7.66E-3"
NOS ON UNBOUNDED DOMAIN,0.7576923076923077,"number of grid points in Geo-FNO is similar to the quadrature used in D4FT. For the D4FT setting,
338"
NOS ON UNBOUNDED DOMAIN,0.7596153846153846,"we consider three molecules: CO2, water, and CH4.
339"
NOS ON UNBOUNDED DOMAIN,0.7615384615384615,"Results. Relative L2 errors for DFT experiments are shown in Table 1. (1) NO-Ours exhibits slightly
340"
NOS ON UNBOUNDED DOMAIN,0.7634615384615384,"superior performance on regular testing and significantly outperforms in super-resolution tasks.
341"
NOS ON UNBOUNDED DOMAIN,0.7653846153846153,"(2) The uniform grid deformation in Geo-FNO is less efficient compared to quadrature, resulting
342"
NOS ON UNBOUNDED DOMAIN,0.7673076923076924,"in NO-Ours surpassing Geo-FNO. (3) As the molecule size increases, atom distribution extends
343"
NOS ON UNBOUNDED DOMAIN,0.7692307692307693,"throughout the R3 domain. Consequently, the input wave function and density function disperse
344"
NOS ON UNBOUNDED DOMAIN,0.7711538461538462,"near the atoms rather than concentrating near the unit box. Thus, using a unit box grid in Geo-FNO
345"
NOS ON UNBOUNDED DOMAIN,0.7730769230769231,"becomes inefficient. (4) NO-Ours adapts to various grids based on the problem, utilizing efficient
346"
NOS ON UNBOUNDED DOMAIN,0.775,"quadrature points in D4FT to accurately represent the input wave function and density. As a result,
347"
NOS ON UNBOUNDED DOMAIN,0.7769230769230769,"NO-Ours achieves super-resolution with minimal additional error. This highlights the significance of
348"
NOS ON UNBOUNDED DOMAIN,0.7788461538461539,"selecting appropriate integral points based on the characteristics of the input functions.
349"
CONBINING MULTIPLE BASES,0.7807692307692308,"6.4
Conbining Multiple Bases
350"
CONBINING MULTIPLE BASES,0.7826923076923077,"We try the advection equation ut + ux = 0, x ∈[0, 1], t ∈[0, 1] with/without periodic bound-
351"
CONBINING MULTIPLE BASES,0.7846153846153846,"ary conditions taken from [23].
Given the initial condition u0(x), we aim to learn the non-
352"
CONBINING MULTIPLE BASES,0.7865384615384615,"linear operator G : u0(x) 7→u(x, t)2, (x, t) ∈[0, 1] × [0, 1] with the hybrid input functions
353"
CONBINING MULTIPLE BASES,0.7884615384615384,u0(x) = h11{c1−w
CONBINING MULTIPLE BASES,0.7903846153846154,"2 ,c1+ w"
CONBINING MULTIPLE BASES,0.7923076923076923,"2 } +
p"
CONBINING MULTIPLE BASES,0.7942307692307692,"max(h2
2 −a2(x −c2)2, 0) where c1, c2, w, h1, h2 are randomly
354"
CONBINING MULTIPLE BASES,0.7961538461538461,"chosen to generate samples. The full problem is named Advection (1), which is periodic in both t
355"
CONBINING MULTIPLE BASES,0.7980769230769231,"and x axis, i.e., u(x, 0) = u(x, 1) and u(0, t) = u(1, t). To construct a non-periodic problem, we
356"
CONBINING MULTIPLE BASES,0.8,"truncate the target function on (x, t) ∈[0, 1] × [0, 0.5], so that it is periodic on the x-axis but not the
357"
CONBINING MULTIPLE BASES,0.801923076923077,"t-axis. We use sinusoidal and/or polynomial bases. Specifically, there are two axes, for NO-Sin/Sin,
358"
CONBINING MULTIPLE BASES,0.8038461538461539,"we use sinusoidal bases on both axes; for NO-Poly/Poly, we use Legendre polynomial bases on both
359"
CONBINING MULTIPLE BASES,0.8057692307692308,"axes; for NO-Sin/Poly, we use sinusoidal on the x-axis and polynomial on t-axis. More specifically,
360"
CONBINING MULTIPLE BASES,0.8076923076923077,"if we have a set of basis functions {ϕi(x)}∞
i=0 on the domain Ωx along the x-axis, and another set of
361"
CONBINING MULTIPLE BASES,0.8096153846153846,"basis functions {ψj(t)}∞
j=0 on the domain Ωt along the t-axis, then the set of tensor product basis
362"
CONBINING MULTIPLE BASES,0.8115384615384615,"functions {ϕi(x)ψj(t)}∞
i,j=0 forms a basis for the two-dimensional space Ωx × Ωt. We train all
363"
CONBINING MULTIPLE BASES,0.8134615384615385,"models with 1000 epochs.
364"
CONBINING MULTIPLE BASES,0.8153846153846154,"Table 2 verifies the effectiveness of sinusoidal bases for periodic functions and polynomials for
365"
CONBINING MULTIPLE BASES,0.8173076923076923,"general non-periodic functions. Additionally, the combination of multiple different bases has been
366"
CONBINING MULTIPLE BASES,0.8192307692307692,"shown to be effective, taking into account the properties of the data. This approach deviates from
367"
CONBINING MULTIPLE BASES,0.8211538461538461,"previous works that typically focus on utilizing a single basis.
368"
CONCLUSION,0.823076923076923,"7
Conclusion
369"
CONCLUSION,0.825,"This paper proposes a novel perspective for studying NOs. We have provided a comprehensive
370"
CONCLUSION,0.8269230769230769,"understanding of NO through a detailed analysis of the infinite sequence space under orthogonal
371"
CONCLUSION,0.8288461538461539,"basis projection. Based on this versatile framework, we have proposed a method to adapt NO to
372"
CONCLUSION,0.8307692307692308,"arbitrary complex domains. We have also analyzed the generalization bound of NO, demonstrating
373"
CONCLUSION,0.8326923076923077,"its superiority over previous works. Furthermore, we have explained the importance of selecting
374"
CONCLUSION,0.8346153846153846,"the type and quantity of basis functions in NO, emphasizing the benefits of using multiple bases in
375"
CONCLUSION,0.8365384615384616,"a complementary manner based on operator characteristics. Additionally, we have examined the
376"
CONCLUSION,0.8384615384615385,"impact of grid points on super-resolution error, highlighting the crucial role of the integration format
377"
CONCLUSION,0.8403846153846154,"associated with the grid and the density of the grid itself. All the theoretical analyses have been
378"
CONCLUSION,0.8423076923076923,"extensively validated through experiments on multiple data sources, including numerical PDE and
379"
CONCLUSION,0.8442307692307692,"DFT. We shed new light on understanding NOs and improving them in practical applications.
380"
REFERENCES,0.8461538461538461,"References
381"
REFERENCES,0.8480769230769231,"[1] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds
382"
REFERENCES,0.85,"for deep nets via a compression approach. In International Conference on Machine Learning,
383"
REFERENCES,0.8519230769230769,"pages 254–263. PMLR, 2018.
384"
REFERENCES,0.8538461538461538,"[2] Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
385"
REFERENCES,0.8557692307692307,"neural networks. arXiv preprint arXiv:1706.08498, 2017.
386"
REFERENCES,0.8576923076923076,"[3] Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in Neural Information
387"
REFERENCES,0.8596153846153847,"Processing Systems, 34:24924–24940, 2021.
388"
REFERENCES,0.8615384615384616,"[4] Maarten V de Hoop, Nikola B Kovachki, Nicholas H Nelsen, and Andrew M Stuart. Conver-
389"
REFERENCES,0.8634615384615385,"gence rates for learning linear operators from noisy data. arXiv preprint arXiv:2108.12515,
390"
REFERENCES,0.8653846153846154,"2021.
391"
REFERENCES,0.8673076923076923,"[5] Tim De Ryck and Siddhartha Mishra. Generic bounds on the approximation error for physics-
392"
REFERENCES,0.8692307692307693,"informed (and) operator learning. arXiv preprint arXiv:2205.11393, 2022.
393"
REFERENCES,0.8711538461538462,"[6] Nicola Rares Franco, Stefania Fresca, Andrea Manzoni, and Paolo Zunino. Approximation
394"
REFERENCES,0.8730769230769231,"bounds for convolutional neural networks in operator learning. Neural Networks, 161:129–141,
395"
REFERENCES,0.875,"2023.
396"
REFERENCES,0.8769230769230769,"[7] Pulkit Gopalani, Sayar Karmakar, and Anirbit Mukherjee. Capacity bounds for the deeponet
397"
REFERENCES,0.8788461538461538,"method of solving differential equations. arXiv preprint arXiv:2205.11359, 2022.
398"
REFERENCES,0.8807692307692307,"[8] Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for
399"
REFERENCES,0.8826923076923077,"differential equations. Advances in neural information processing systems, 34:24048–24062,
400"
REFERENCES,0.8846153846153846,"2021.
401"
REFERENCES,0.8865384615384615,"[9] Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu,
402"
REFERENCES,0.8884615384615384,"Ze Cheng, Jun Zhu, and Jian Song. Gnot: A general neural operator transformer for operator
403"
REFERENCES,0.8903846153846153,"learning. arXiv preprint arXiv:2302.14376, 2023.
404"
REFERENCES,0.8923076923076924,"[10] Francis Begnaud Hildebrand. Introduction to numerical analysis. Courier Corporation, 1987.
405"
REFERENCES,0.8942307692307693,"[11] Zheyuan Hu, Ameya D Jagtap, George Em Karniadakis, and Kenji Kawaguchi. When do
406"
REFERENCES,0.8961538461538462,"extended physics-informed neural networks (xpinns) improve generalization? SIAM Journal on
407"
REFERENCES,0.8980769230769231,"Scientific Computing (SISC), 2022.
408"
REFERENCES,0.9,"[12] Pengzhan Jin, Shuai Meng, and Lu Lu. Mionet: Learning multiple-input operators via tensor
409"
REFERENCES,0.9019230769230769,"product. SIAM Journal on Scientific Computing, 44(6):A3490–A3514, 2022.
410"
REFERENCES,0.9038461538461539,"[13] Kenji Kawaguchi, Zhun Deng, Kyle Luh, and Jiaoyang Huang. Robustness implies general-
411"
REFERENCES,0.9057692307692308,"ization via data-dependent generalization bounds. In International Conference on Machine
412"
REFERENCES,0.9076923076923077,"Learning (ICML), 2022.
413"
REFERENCES,0.9096153846153846,"[14] Taeyoung Kim and Myungjoo Kang. Bounding the rademacher complexity of fourier neural
414"
REFERENCES,0.9115384615384615,"operator. arXiv preprint arXiv:2209.05150, 2022.
415"
REFERENCES,0.9134615384615384,"[15] Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation
416"
REFERENCES,0.9153846153846154,"and error bounds for fourier neural operators. The Journal of Machine Learning Research,
417"
REFERENCES,0.9173076923076923,"22(1):13237–13312, 2021.
418"
REFERENCES,0.9192307692307692,"[16] Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets:
419"
REFERENCES,0.9211538461538461,"A deep learning framework in infinite dimensions.
Transactions of Mathematics and Its
420"
REFERENCES,0.9230769230769231,"Applications, 6(1):tnac001, 2022.
421"
REFERENCES,0.925,"[17] Tianbo Li, Min Lin, Zheyuan Hu, Kunhao Zheng, Giovanni Vignale, Kenji Kawaguchi,
422"
REFERENCES,0.926923076923077,"A.H. Castro Neto, Kostya S. Novoselov, and Shuicheng YAN. D4FT: A deep learning approach
423"
REFERENCES,0.9288461538461539,"to kohn-sham density functional theory. In The Eleventh International Conference on Learning
424"
REFERENCES,0.9307692307692308,"Representations, 2023.
425"
REFERENCES,0.9326923076923077,"[18] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential
426"
REFERENCES,0.9346153846153846,"equations’ operator learning. arXiv preprint arXiv:2205.13671, 2022.
427"
REFERENCES,0.9365384615384615,"[19] Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural oper-
428"
REFERENCES,0.9384615384615385,"ator with learned deformations for pdes on general geometries. arXiv preprint arXiv:2207.05209,
429"
REFERENCES,0.9403846153846154,"2022.
430"
REFERENCES,0.9423076923076923,"[20] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,
431"
REFERENCES,0.9442307692307692,"Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differen-
432"
REFERENCES,0.9461538461538461,"tial equations. International Conference on Learning Representations (ICLR), 2021.
433"
REFERENCES,0.948076923076923,"[21] Ziyuan Liu, Haifeng Wang, Kaijuna Bao, Xu Qian, Hong Zhang, and Songhe Song. Render
434"
REFERENCES,0.95,"unto numerics: Orthogonal polynomial neural operator for pdes with non-periodic boundary
435"
REFERENCES,0.9519230769230769,"conditions. arXiv preprint arXiv:2206.12698, 2022.
436"
REFERENCES,0.9538461538461539,"[22] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning
437"
REFERENCES,0.9557692307692308,"nonlinear operators via DeepONet based on the universal approximation theorem of operators.
438"
REFERENCES,0.9576923076923077,"Nature Machine Intelligence, 3(3):218–229, 2021.
439"
REFERENCES,0.9596153846153846,"[23] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang,
440"
REFERENCES,0.9615384615384616,"and George Em Karniadakis. A comprehensive and fair comparison of two neural operators
441"
REFERENCES,0.9634615384615385,"(with practical extensions) based on fair data. Computer Methods in Applied Mechanics and
442"
REFERENCES,0.9653846153846154,"Engineering, 393:114778, 2022.
443"
REFERENCES,0.9673076923076923,"[24] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring
444"
REFERENCES,0.9692307692307692,"generalization in deep learning. arXiv preprint arXiv:1706.08947, 2017.
445"
REFERENCES,0.9711538461538461,"[25] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural
446"
REFERENCES,0.9730769230769231,"operators. In International Conference on Learning Representations, 2023.
447"
REFERENCES,0.975,"[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
448"
REFERENCES,0.9769230769230769,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
449"
REFERENCES,0.9788461538461538,"processing systems, 30, 2017.
450"
REFERENCES,0.9807692307692307,"[27] Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson.
451"
REFERENCES,0.9826923076923076,"U-fno—an enhanced fourier neural operator-based deep-learning model for multiphase flow.
452"
REFERENCES,0.9846153846153847,"Advances in Water Resources, 163:104180, 2022.
453"
REFERENCES,0.9865384615384616,"[28] Xiongye Xiao, Defu Cao, Ruochen Yang, Gaurav Gupta, Gengshuo Liu, Chenzhong Yin, Radu
454"
REFERENCES,0.9884615384615385,"Balan, and Paul Bogdan. Coupled multiwavelet operator learning for coupled differential
455"
REFERENCES,0.9903846153846154,"equations. In The Eleventh International Conference on Learning Representations, 2023.
456"
REFERENCES,0.9923076923076923,"[29] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391–423,
457"
REFERENCES,0.9942307692307693,"2012.
458"
REFERENCES,0.9961538461538462,"[30] Jiawei Zhao, Robert Joseph George, Yifei Zhang, Zongyi Li, and Anima Anandkumar. Incre-
459"
REFERENCES,0.9980769230769231,"mental fourier neural operator. arXiv preprint arXiv:2211.15188, 2022.
460"
