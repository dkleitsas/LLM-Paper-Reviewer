Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005291005291005291,"Despite substantial efforts from the deep learning system community to relieve
researchers and practitioners from the burden of implementing models with ever-
growing complexity, a considerable lingual gap remains between developing mod-
els in the language of mathematics and implementing them in the languages of
computer. The mission of KOKOYI is to close this gap by enabling automatic
translation of mathematics into efﬁcient implementations, thereby making math-in-
codes and math-in-model consistent. This paper presents our ﬁrst step towards the
goal: kokoyi-lang, a programming language with the syntax of LATEX and the
semantics of deep learning mathematics, and a prototype kokoyi-lang compiler
and runtime supporting advanced optimizations such as auto-batching. KOKOYI is
integrated with Jupyter Notebook, and will be released in open-source."
INTRODUCTION,0.010582010582010581,"1
INTRODUCTION"
INTRODUCTION,0.015873015873015872,"The success of deep learning is a tale of two ends. At one end is model development, which leverages
the language of mathematics to deﬁne models. At the other end is model implementation, which relies
on programming languages such as Python and CUDA to unleash the power of big data and efﬁcient
computation. Despite the success, the language gap between the two ends is arguably the source of
many evils, such as slow prototyping, wrong/inefﬁcient/unreadable implementations, irreproducible
results in publications, and learning barriers to name but a few. A model is often implemented and
re-implemented in different, incompatible deep learning frameworks, an unnecessary fragmentation
from a model point of view."
INTRODUCTION,0.021164021164021163,"The mission of KOKOYI is to close the language gap between the two ends, making math-in-codes and
math-in-model consistent. KOKOYI advocates a simple principle: your model is your code. KOKOYI
introduces a programming language called kokoyi-lang with a programming model elevated
to mathematical equations written in LATEX, a rendering language popular among deep learning
researchers (Lamport, 1994). Syntactically, kokoyi-lang is a LATEX subset that is prevalent in
mathematical deﬁnitions of deep learning models, making kokoyi-lang programs renderable
just like LATEX documents. Semantically, kokoyi-lang formalizes the consensus of the semantics
of mathematics among deep learning researchers and practitioners. KOKOYI aims at boosting
productivity at a community level: researchers code a model once instead of twice (one in a program
and another in the publication), kokoyi-lang is designed to be framework agnostic, the model
written in kokoyi-lang in a paper is executable by all and can be ported to different frameworks."
INTRODUCTION,0.026455026455026454,"As a teaser, the following (rendered) line of kokoyi-lang code deﬁnes the mean squared error
loss function ℓfor a linear regression model with weight w ∈Rd and bias b ∈R given a dataset D:"
INTRODUCTION,0.031746031746031744,"ℓ(w, b, D) ←
1
|D| X"
INTRODUCTION,0.037037037037037035,"(x,y)∈D
((w · x + b) −y)2"
INTRODUCTION,0.042328042328042326,"Given kokoyi-lang programs, KOKOYI can generate executables using various backends. In its
current prototype, KOKOYI generates PyTorch code because of its widespread adoption. However,
compared to PyTorch’s deep learning frameworks that rely on Python for usability, KOKOYI can po-
tentially avoid Python’s overheads without degrading user experience by translating kokoyi-lang
programs into more efﬁcient deep learning IRs such as Relay (Roesch et al., 2018) instead of Python."
INTRODUCTION,0.047619047619047616,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05291005291005291,IDE (Jupyter Notebook Environment)
INTRODUCTION,0.0582010582010582,"Kokoyi
Compiler"
INTRODUCTION,0.06349206349206349,"Type
inference"
INTRODUCTION,0.06878306878306878,"VMap
Optimizations"
INTRODUCTION,0.07407407407407407,"Code
Generation"
INTRODUCTION,0.07936507936507936,Kokoyi Auto-batching Runtime
INTRODUCTION,0.08465608465608465,"(a)
DE, CSE"
INTRODUCTION,0.08994708994708994,Tensor Backend (PyTorch) (c) (b)
INTRODUCTION,0.09523809523809523,"Figure 1: (a) A typical work ﬂow when using KOKOYI; (b) Jupyter notebook example of a multi-layer
RNN encoder; (c) The corresponding auto-generated module in PyTorch."
INTRODUCTION,0.10052910052910052,"The level of abstraction offered by the language of mathematics can enable various code optimizations
deemed hard for existing deep learning frameworks. An example is auto-batching, which can relieve
users from the burden of manually vectorizing per-sample code, an often counter-intuitive endeavor
mandated solely by efﬁciency. Previous attempts of auto-batching like TensorFlow-Fold (Looks
et al., 2017), DyNet (Neubig et al., 2017) and JAX (Bradbury et al., 2018) heavily rely on Just-
in-time compilation, which are limited to statically shaped samples. By contrast, KOKOYI takes a
holistic design, constructs built during compilation facilitate auto-batching during execution. As
a result, KOKOYI can auto-batch complex models, such as the auto-regressive attention module in
Transformer (Vaswani et al., 2017)."
INTRODUCTION,0.10582010582010581,"KOKOYI focuses on model speciﬁcation. Our informal survey of sampled papers from several
machine learning conferences, showing KOKOYI can impact about 15% to 66% of the work where
model development is the focus (Appendix A). As such, we do not attempt to reinvent all the wheels,
compiled kokoyi-lang programs are callable Python objects and integrate with other stages (e.g.
data preparation, model tuning, and model serving) of the deep learning pipeline; whenever possible,
KOKOYI reuses efﬁcient existing modules."
INTRODUCTION,0.1111111111111111,"We implemented a Jupyter Notebook frontend to facilitate interactive rendering and execution of
kokoyi-lang programs ((see HTML examples in the supplementary materials). There is also an
interactive Web server where one can prototype a model and download both the PyTorch module and
the LATEX snippet. We envision supporting compilation of kokoyi-lang code embedded in LATEX
documents in the future to enable executable research papers."
INTRODUCTION,0.1164021164021164,"We implemented in kokoyi-lang several important deep learning models with various complexity,
and ﬁnd most of our per-sample implementations, with PyTorch as backend, achieve performance
comparable to manually tensorized PyTorch implementations. At the same time, models implemented
in kokoyi-lang are substantially more succinct, for instance the standard Transformer (Vaswani
et al., 2017) needs only 65 lines of code, saving users from the tedious and often error-prone process
of manually batching computation (Appendix B). Although a large-scale user experience study is
yet to be conducted, initial feedbacks from a group of deep learning researchers and data scientists
indicate that compared to existing deep learning frameworks, kokoyi-lang is much closer to their
mathematical formulations of deep learning models. KOKOYI will open source soon."
KOKOYI OVERVIEW,0.12169312169312169,"2
KOKOYI OVERVIEW"
KOKOYI OVERVIEW,0.12698412698412698,"KOKOYI aims to be pragmatic, while adhering to the “your model is your code” principle. It is built
around and leverages the existing Python ecosystem, yet still reserves the ﬂexibility to revert to more
efﬁcient execution when possible. Fig. 1(a) depicts the work ﬂow when using KOKOYI. Development
starts in an IDE and we currently choose the Jupyter Notebook, a popular interactive environment for
live coding and data rendering."
KOKOYI OVERVIEW,0.13227513227513227,"Users can write a KOKOYI program – one or more mathematical equations – in a Jupyter Notebook
cell, led by the %kokoyi marker to register a hook function that renders the equation via MathJax."
KOKOYI OVERVIEW,0.13756613756613756,Under review as a conference paper at ICLR 2022
KOKOYI OVERVIEW,0.14285714285714285,"Programming in KOKOYI uses kokoyi-lang, a LATEX dialect that is extended with syntax familiar
to DL researchers. Running the cell triggers the compilation of the equation to generate a callable
python object. In this example (Fig. 1(b)), the returned object represents the module deﬁnition of an
L-layer RNN to compute the representation of a sentence S. We hook the program execution with a
proper error handler to highlight the portion of the equations that causes the error."
KOKOYI OVERVIEW,0.14814814814814814,"KOKOYI translates tensors and functions deﬁned in kokoyi-lang into tensors in underly-
ing DL frameworks and Python functions, and makes them accessible in a Python dictionary
(named kokoyi.symbols in Jupyter notebooks). For example, the RNN PyTorch module calls
kokoyi.symbols[""RNN""](Fig. 1(c)) in its forward function. KOKOYI also ports many Py-
Torch’s commonly used modules (e.g. \ReLu); the porting is mostly straightforward, with the
only complexities arise from supporting auto-batching. Our current prototype enables one-click
auto-generation of a Pytorch module (Fig. 1(c)) once it is speciﬁed in kokoyi-lang. This capacity
is simplistic for now and the user is required to complete the rest of the initialization (e.g. selecting
the initializer). It is relatively straightforward to generate more initialization code in the future."
KOKOYI OVERVIEW,0.15343915343915343,"The KOKOYI compiler generates PyTorch code directly and thus the execution is just like any other
PyTorch programs. Note that KOKOYI can in principle completely by-pass Python by translating
the program into low-level IRs such as Relay IR (Roesch et al., 2018) and LLVM IR (Lattner
& Adve, 2004) to enable efﬁcient execution. Nevertheless, we choose to focus on Python code
generation mainly due to its dominating ecosystem within the deep learning community. For example,
a user can easily replace the model deﬁnition in Python with KOKOYI-generated codes, while
still utilizing the rich code base from the Python community for data preparation, model tuning,
serving and visualization, as all the examples in the supplementary do. KOKOYI itself leverages
compilers specialized for Python deep learning program such as TorchScript (PyTorch, 2021) to
further accelerate the generated code."
KOKOYI OVERVIEW,0.15873015873015872,"The design of KOKOYI compiler is covered in Section 4. KOKOYI performs auto-batching, where the
model is written out entirely from a single sample point of view, but processes batches of them after
compilation (see Section 5)."
PROGRAMMING IN KOKOYI,0.164021164021164,"3
PROGRAMMING IN KOKOYI"
PROGRAMMING IN KOKOYI,0.1693121693121693,"Our goal is to design KOKOYI in such a way that it requires almost no learning for researchers
who have written papers. In a sense, kokoyi-lang is a dialect of LATEX. KOKOYI can typeset
mathematics written in kokoyi-lang as if they were written in LATEX. All mathematics in this
section are typeset, parsed, and compiled by KOKOYI, thereby removing the gap between math-in-
paper and math-in-code. kokoyi-lang is also more than a LATEX dialect. Mathematics written
in kokoyi-lang are executable programs carrying the semantics that most members of the DL
community will reasonably expect when reading them."
PROGRAMMING IN KOKOYI,0.1746031746031746,"The immediate challenge we face is that math formulations are immensely ﬂexible. Repurposing
LATEX as the programming interface must therefore allow practical and efﬁcient code generation
without losing expressiveness. In contrast to programming languages that require alphanumeric
variable names, kokoyi-lang allows programmers to denote variables by math symbols typeset
in LATEX. The “real estate” of the sub- and superscription needs particular attention. After several
iterations, we decide to leave them as part of variable name and impose reasonable constraints
elsewhere. For instance, we choose f(x; θ) instead of fθ(x) for learnable functions (i.e. modules),
and x[i] (typeset as x[i]) for indexing. We also introduce common Python syntax where there is
no ambiguity such as raising a power and various forms of multiplication, see supplementary for
concrete examples."
PROGRAMMING IN KOKOYI,0.17989417989417988,"We now proceed to introduce kokoyi-lang’s key constructs: tensors, functions and modules."
PROGRAMMING IN KOKOYI,0.18518518518518517,"Tensor deﬁnition
A key construct in kokoyi-lang is tensor deﬁnition, which enables one to
deﬁne (←) a tensor as the value of an expression, with the command \gets:
[tensor] ←[expression]
Expressions in tensor deﬁnitions can be applications of built-in tensor operations, such as transpose
(⊤), element-wise arithmetic (including element-wise reductions such as sum P and product Q), dot
product (·), and indexing (·[·])."
PROGRAMMING IN KOKOYI,0.19047619047619047,Under review as a conference paper at ICLR 2022
PROGRAMMING IN KOKOYI,0.19576719576719576,"The expression in a tensor deﬁnition can also be a tensor constructor, which gives an entry-by-entry
deﬁnition of a tensor. For example, one can deﬁne the M × N attention matrix in Transformer
(Vaswani et al., 2017) as

  ("
PROGRAMMING IN KOKOYI,0.20105820105820105,softmax
PROGRAMMING IN KOKOYI,0.20634920634920634,"q⊤
[i] · k[j]
√ d !)N j=0 
  M i=0 
  ("
PROGRAMMING IN KOKOYI,0.21164021164021163,softmax
PROGRAMMING IN KOKOYI,0.21693121693121692,"q⊤
[i] · k[j]
√ d !)i j=0 
  M i=0 (1)"
PROGRAMMING IN KOKOYI,0.2222222222222222,"where d is embedding dimension, and q and k are M × d and N × d matrices consisting of query and
key vectors, respectively. The one on the left is full self-attention whereas the right is auto-regressive,
i.e. only attentive to tokens behind the current one; in a sequence-to-sequence model, the ﬁrst is often
used in encoder and the second in decoder. One can similarly deﬁne a multi-head attention matrix, by
adding another index that ranges over [0, H), where H is the number of heads (see supplementary
for the full Transformer model)."
PROGRAMMING IN KOKOYI,0.2275132275132275,"One can also deﬁne a tensor in kokoyi-lang by giving a recurrence relation among its parts, such
as rows and columns. For example, the hidden state matrix of a vanilla recurrent neural network
(RNN) can be given by"
PROGRAMMING IN KOKOYI,0.2328042328042328,"h[0≤t≤L] ←
h0
t = 0
tanh(Wh · h[t−1] + Wx · x[t] + b)
otherwise
(2)"
PROGRAMMING IN KOKOYI,0.23809523809523808,"where x is a matrix with L rows, each being the embedding of an input token, and Wh, Wx, and
h0 are learnable parameters. This is an example of KOKOYI module that we will describe next. To
facilitate compiler optimization, programmers need to annotate indices where recurrence relations
exist with a range (e.g. h[0≤t≤L]), which can depend on the dimensions of other tensors as in the case
of tensor constructor. Also notice the use of \begin{cases} and \end{cases} (rendered as
the big left brace {) in Eq. (2) for branching (Fig. 1(b))"
PROGRAMMING IN KOKOYI,0.24338624338624337,"It is common for a recurrence relation to involve more than one tensor. For example, the hidden state
and memory cell of a long short-term memory (LSTM) network (Hochreiter & Schmidhuber, 1997)
depends on the network’s forget, input, and output gates, which in turn depends on the previous hidden
state and memory cell. This recursive mutual dependency can be expressed in kokoyi-lang as"
PROGRAMMING IN KOKOYI,0.24867724867724866,"f[1≤t≤L] ←σ(Uf · h + Vf · x[t] + bf)
i[1≤t≤L] ←σ(Ui · h + Vi · x[t] + bi)"
PROGRAMMING IN KOKOYI,0.25396825396825395,"o[1≤t≤L] ←σ(Uo · h + Vo · x[t] + bo)
˜c[1≤t≤L] ←σ(Uc · h + Vc · x[t] + bc)"
PROGRAMMING IN KOKOYI,0.25925925925925924,"c[1≤t≤L] ←
c0
t = 0
f[t] ◦c[t−1] + i[t] ◦˜c[t]
otherwise
h[1≤t≤L] ←
h0
t = 0
o[t] ◦tanh(c[t])
otherwise"
PROGRAMMING IN KOKOYI,0.26455026455026454,"Function and module
The syntax for tensor deﬁnition applies to function deﬁnitions as well. The
following is a deﬁnition of the sigmoid function:"
PROGRAMMING IN KOKOYI,0.2698412698412698,"σ(x) ←
1
1 + e−x"
PROGRAMMING IN KOKOYI,0.2751322751322751,"kokoyi-lang borrows the concept of module from popular DL frameworks such as PyTorch
(Paszke et al., 2019) to enable encapsulation of learnable parameters. Conceptually, a module is a
function parameterized by learnable parameters that are updated iteratively, for example, by gradient
descent. Consistent with the DL practice, kokoyi-lang separates learnable parameters from
parameters that the function can be applied to using a semicolon: f(x; θ). Similarly, if f uses a
component module g that itself contains learnable parameters, it is written as f(x; g). The syntax of
module deﬁnition resembles those of LATEX packages for pseudocode."
PROGRAMMING IN KOKOYI,0.2804232804232804,"Fig. 1(b) demonstrates the major concepts of kokoyi-lang with a L-layer RNN, where the ﬁnal
hidden representation h[L] embeds the input sentence S that contains the indices of embedding of
the tokens. Wx, Wh, b, h0 and the embedding table D are all trainable parameters. Note that |S| is a
syntactic sugar to retrieve the ﬁrst dimension of a multi-dimensional tensor, whereas the more general
GetShape function returns the complete shape. {0}1||S combines a tensor construction (of a zero
input) and concatenation to right-shift the input. The rest of the codes are all self-explanatory."
PROGRAMMING IN KOKOYI,0.2857142857142857,Under review as a conference paper at ICLR 2022
PROGRAMMING IN KOKOYI,0.291005291005291,"e := v
(variable)
| 0, −2, 1.0, . . .
(constant)"
PROGRAMMING IN KOKOYI,0.2962962962962963,"| let v = e in e′
(let binding)"
PROGRAMMING IN KOKOYI,0.30158730158730157,"| letrec v1 = e1 , . . . , in e′
(letrec binding)
| λ (v1, . . . , vk). e
(function abstraction)
| v(v1, . . . , vk)
(function application)
| if e then e1 else e2
(branch)
| λ [v1 ≤v ≤v2]. e
(functional array)"
PROGRAMMING IN KOKOYI,0.30687830687830686,"| vmap v on v′
(vectorized map)."
PROGRAMMING IN KOKOYI,0.31216931216931215,"Figure 2: Grammar of KOKOYI IR. e and vrepresents expression and variable, respectively."
THE KOKOYI COMPILER,0.31746031746031744,"4
THE KOKOYI COMPILER"
THE KOKOYI COMPILER,0.32275132275132273,"Intermediate representation specialized for deep learning
Deep learning compilers have been a
popular research topic in the recent years which leaves several choices of Intermediate Representation
(IR) for our compiler. Among them, Relay (Roesch et al., 2018) uses functional IR due to its power in
expressing function closure; TorchScript (PyTorch, 2021) uses Static Single Assignment (SSA) for its
simplicity; JAX (Bradbury et al., 2018) relies on XLA which translates a user program ﬁrst into High
Level Optimizer (HLO) IR for deep learning speciﬁc optimizations and then lowers it to LLVM IR."
THE KOKOYI COMPILER,0.328042328042328,"The KOKOYI compiler chooses functional IR because the mathematical language is inherently very
functional, e.g., functions are pure with no side-effect, there is almost no symbol redeﬁnition, etc.
This makes it easy for KOKOYI compiler to translate a program into a functional IR by traversing
the abstract syntax tree and applying syntactic rules. The full grammar of KOKOYI IR is in Figure 2.
Generally speaking, KOKOYI IR follows the A-normal form with two additional rules to capture the
common loop patterns in deep learning programs, namely the vectorized map (as in Transformer
Eq.(1)) and functional array (as in RNN Eq.(2)). Having these patterns in IR makes it very convenient
to perform optimizations on them later. They demonstrate the necessity of lifting the program interface
to a math-like language. While some tools have identiﬁed the importance of these patterns and have
provided specialized Python APIs for them (e.g., jax.vmap, jax.lax.scan), they typically
require complicated speciﬁcations from users due to Python’s notorious versatility. By contrast,
expressing them in math equations is almost out of natural instinct which KOKOYI utilizes to achieve
both high usability and efﬁciency. The translated IR of the attention module is shown in Figure 3(c)."
THE KOKOYI COMPILER,0.3333333333333333,"Strong type or weak type?
The choice of type system has a signiﬁcant impact on a programming
language and its compiler. The debate on whether a language should embrace a stronger or weaker
type system prolongs into the era of deep learning. On one hand, Python’s dynamically-typed
design saves users from annotating types of variables and functions, which boosts the productivity
of model development. On the other hand, lacking static type checking leads to elusive bugs,
causing issues like unrunnable codes, buggy codes and reproducibility. Furthermore, deep learning
compilers continuously push for stronger type semantics to achieve high performance. For example,
TorchScript’s script mode requires users to provide type hints for the argument and return values of
every method. For more complicated programs, it also provides a trace mode that executes the program
once and records the data ﬂow as well as the runtime type information for further optimizations.
Program tracing cannot deal with control ﬂows that may diverge during runtime. To circumvent
this, AutoGraph (Moldovan et al., 2018) (which is integrated into tensorflow.function)
adopts source code transformation to convert control ﬂows to data ﬂow operators. Nevertheless, the
transformation rules are limited and can fail in certain cases."
THE KOKOYI COMPILER,0.3386243386243386,"KOKOYI’s principle is usability oriented. As such, no type hint is required by default. KOKOYI
tries to infer the expression type from intrinsic language semantics. Some operators (e.g., matmul)
have constraints on the types of inputs and outputs they can be applied to (e.g., tensors). Similarly,
the two new IR expressions introduced by KOKOYI, functional array and vectorized map, have list
and tensor type respectively. We can propagate these type constraints among the IR nodes to reﬁne"
THE KOKOYI COMPILER,0.3439153439153439,Under review as a conference paper at ICLR 2022 (a)
THE KOKOYI COMPILER,0.3492063492063492,"(b)
(c)"
THE KOKOYI COMPILER,0.3544973544973545,batch scope: ()
THE KOKOYI COMPILER,0.35978835978835977,batch scope: (M)
THE KOKOYI COMPILER,0.36507936507936506,"batch scope: (M, N)"
THE KOKOYI COMPILER,0.37037037037037035,"Figure 3: (a) Attention module in kokoyi-lang; (b) Python code generated by KOKOYI; (c)
Intermediate representation of the program."
THE KOKOYI COMPILER,0.37566137566137564,"types that are unknown. Also note that it is not uncommon to see type notations when describing
a model during paper writing. Examples are x : Rd1×d2 to specify a matrix-type variable and
f : (Rd1, Rd2) 7→Rd1×d2 to specify a function. It is straightforward for KOKOYI to parse them into
extra type constraints in the future, which will also help to auto-generate more initialization codes."
THE KOKOYI COMPILER,0.38095238095238093,"The viable types in KOKOYI are scalar (i.e., integer or ﬂoating number), list, tensor, function and
tuple. To support type polymorphism (e.g., x + y can be between two scalars, or two tensors, or one
scalar and one tensor), KOKOYI iteratively reﬁnes expression types by incorporating more precise
type constraints. Speciﬁcally, the ﬁrst pass only collects the most general types of each operator,
type hints from user annotations as well as from IR requirements. Once the constraints are resolved,
we reﬁne the type of each operator according to their signatures, which will introduce more type
constraints for further reﬁnement. Type constraints are collected using the let-polymorphism (Milner,
1978) algorithm and are resolved by uniﬁcation. If an expression type cannot be determined statically,
KOKOYI will generate code with the most general type and defer the type dispatch to runtime.
Otherwise, KOKOYI will generate more precise implementation. For instance, although both RNN
(below left) and ReLU (below right) uses the case expression to represent branching, KOKOYI can
generate if...else... statement for RNN while the vectorized torch.where operator for
ReLU in most cases."
THE KOKOYI COMPILER,0.3862433862433862,"h[1≤t≤L] ←
h0
t = 0
o[t] ◦tanh(c[t])
otherwise
ReLU(x) ←
x
x ≤0
0
otherwise
(3)"
THE AUTO-BATCHING RUNTIME,0.3915343915343915,"5
THE AUTO-BATCHING RUNTIME"
THE AUTO-BATCHING RUNTIME,0.3968253968253968,"KOKOYI promotes the concept of “think in one sample, run in multiple"". Users express a model as
a parameterized function (or a module) over one input sample, from which KOKOYI automatically
converts to an implementation that executes multiple samples in a batch. The need for auto-batching
not only appears at the scope of an entire program but also at the level of one statement or one
expression, or even multiple places that are nested. The code to compute attention in Eq. (1) shows
such an example. It calculates attentions between all positions of array q and k by nesting two array
deﬁnitions and one will wish to compute them in one batch. Moreover, the inner tensor expression
can have dynamic length as shown in the decoder attention example."
THE AUTO-BATCHING RUNTIME,0.4021164021164021,Under review as a conference paper at ICLR 2022
THE AUTO-BATCHING RUNTIME,0.4074074074074074,"Another design consideration is debuggability. The best way to avoid bugs is to not having them
in the ﬁrst place, expressing a model in equations already helps to some extent, as we experienced
ourselves in prototyping models in KOKOYI. Nevertheless, we cannot realistically hope no bugs
creep into the implementation. The success of PyTorch has proven that the run-and-see experience is
favored by a large population of users, which we wish to keep in KOKOYI. Unfortunately, none of the
previous auto-batching solutions can satisfy all the requirements."
THE AUTO-BATCHING RUNTIME,0.4126984126984127,"In KOKOYI, we build a runtime system that supports nested, dynamic and eagerly executed auto-
batching on the top of tensor-based deep learning framework (e.g., PyTorch). Our design is centered
around an operational semantics called General Universal Function (GUFunc) which speciﬁes how a
function over one sample can be generalized to multi-sample inputs."
THE AUTO-BATCHING RUNTIME,0.41798941798941797,"Operational semantics
GUFunc1 is broadly adopted by NumPy to generalize functions originally
over scalar elements to sub-arrays. A GUFunc speciﬁes the element shape, called core dimensions,
of its arguments and return values. For example, matrix multiplication is annotated as (a, b),
(b, c) -> (a, c) meaning that the element shape of the two arguments and the return value
is matrix. To support batch processing, a GUFunc allows prepending arbitrary number of batch
dimensions to its arguments as long as the dimensions satisfy the broadcasting semantics 2. For
example, NumPy’s matmul supports inputs of shapes (32, 1, 10, 5) and (4, 5, 6) respectively because
the element shapes (underlined) are valid for matrix multiplication and the batch dimensions (32, 1)
and (4) are broadcastable. Conceptually, one kernel conducts 32 × 4 = 128 multiplications."
THE AUTO-BATCHING RUNTIME,0.42328042328042326,"Hence, the auto-batching problem becomes how to make the entire program a GUFunc where its core
dimensions specify the shape of an individual sample. To achieve that, KOKOYI ﬁrst needs to know
the number of batch and core dimensions. At the moment, we ask users to provide such information
when invoking a KOKOYI-generated function while leaving automatic inference as future work.
KOKOYI then ensures that each invoked operator satisﬁes the GUFunc property with regard to the
batch and core dimensionality passed on during execution. For most operators, this is straight-forward
as most PyTorch operators have supported broadcasting. For operators working along a certain axis
such as reduction or normalization (e.g., \softmax), KOKOYI adjusts the working axis according to
the batch and core dimensionality. Since the GUFunc property is composable (a function consisting
of only GUFuncs is a GUFunc), the entire program can execute all samples in one batch."
THE AUTO-BATCHING RUNTIME,0.42857142857142855,"Eager-mode vectorized map
KOKOYI’s vmap(f, x) is a high-order function with its ﬁrst
argument being a function and the second one being a tensor. Conceptually, it applies f over each
element of x and returns the transformed results in a batched tensor. We call f the mapped function
and x the mapped domain. The mapped function acts as a transformation routine over one element.
As is shown in Figure 3(b), the mapped function _2 (Line 5-16) computes for one position i the
attention against all positions j. The vmap call (Line 17) applies it over domain variable _3 which is
calculated by arange(0,M) (Line 4). This function further contains another nested function _4
that is mapped by the vmap at Line 15, over the mapped domain arange(0, N)."
THE AUTO-BATCHING RUNTIME,0.43386243386243384,"During execution, KOKOYI maintains a global batching scope. Initially, the batching scope has shape
(), meaning no batch dimensions. vmap lifts the current batching scope by appending the batch
size of the mapped domain. Whenever a tensor created outside of the current batching scope is
referenced inside a mapped function, its batching dimension is expanded by appending the batch size
of the current scope. For instance, in Figure 3(a), scalar variable N is created at Line 3 outside of
any mapped functions. When it is referenced at Line 6 by kokoyi.arange, it is automatically
expanded to a vector of shape (M,). Because all KOKOYI operators are GUFuncs, they automatically
work on a batch of inputs so the computation is vectorized. One subtlety is that the vmap itself must
be a GUFunc too to allow nested invocation. This is guaranteed by appending the batch size so that
the original batch dimensions are retained."
THE AUTO-BATCHING RUNTIME,0.43915343915343913,"To support dynamically shaped batching as in the decoder attention module, KOKOYI associates
each tensor variable with a mask tensor, a boolean tensor indicating whether the corresponding
entry is valid. Masks are created by operators that may lead to dynamic shapes within a batch
(e.g., kokoyi.arange). KOKOYI operators will utilize the mask tensor during computation, e.g.,
skipping invalid entries during reduction, calculating the result mask or simply dropping it."
THE AUTO-BATCHING RUNTIME,0.4444444444444444,"1https://numpy.org/doc/stable/reference/c-api/generalized-ufuncs.html
2https://numpy.org/doc/stable/user/basics.broadcasting.html"
THE AUTO-BATCHING RUNTIME,0.4497354497354497,Under review as a conference paper at ICLR 2022
THE AUTO-BATCHING RUNTIME,0.455026455026455,"Naive runtime auto-batching can lead to several performance issues. First of all, expanding batch
dimensions may duplicate tensor data. To give an example, suppose matrix q (Figure 3(a), Line
1) has shape (M, D), referencing it inside the nested mapped function (Line 8) expands it to be
(M, N, M, D) where each (M, D) sub-tensors are the same. Our implementation resolves the
problem by taking advantage of the broadcasting semantics. We unsqueeze q’s dimension to (1,
1, M, D) and let broadcasting take affect during subsequent operations. Another issue is with the
index lookup q[i] (Line 8). Although the lookup is vectorized for every element in i, the particular
case has a more optimized solution using tensor slicing q[0:M]. We hereby implement a compiler
optimization pass that makes such transformation automatically when detecting the index variable i
being a range domain."
EVALUATION,0.4603174603174603,"6
EVALUATION"
EVALUATION,0.4656084656084656,"Diversity and Coverage
To measure the ﬂexibility of kokoyi-lang, we have implemented a
variety of popular models, including Multi-layer Perceptron (MLP), LeNet (LeCun et al., 1998),
UNet (Ronneberger et al., 2015) for images segmentation, sequence models for sentimental classi-
ﬁcation, neural machine translation (with and without attention (Sutskever et al., 2014)) using
LSTM (Hochreiter & Schmidhuber, 1997), Bi-LSTM (Huang et al., 2015), and ﬁnally Trans-
former (Vaswani et al., 2017). For generative model, we implemented Generative Adversarial
Networks (GAN (Goodfellow et al., 2014)) and Variational Auto-Encoder (VAE (Kingma & Welling,
2013)). Finally, in the context of reinforcement learning, we implemented Policy Gradient (PG)
and Deep Q-learing Networks (DQN). We have included HTML ﬁles of these notebooks in the
supplementary."
EVALUATION,0.4708994708994709,"Figure 4: Relative training speed
v.s. PyTorch
Figure 5: Per-iteration training time under different batch sizes
for MLP (left) and Transformer (right)."
EVALUATION,0.47619047619047616,"Performance
We tested KOKOYI’s system efﬁciency on a variety of deep neural network models
and compared them with the state-of-the-art well-optimized implementations in native PyTorch (see
Appendix C for the model details). We measured the running time of training these models for
one iteration, including a forward and a backward propagation pass and the parameter update, and
reported the average time of 20 runs. We observed little variance. All the experiments are conducted
on an AWS EC2 g4dn.2xlarge instance equipped with a 16GB NVIDIA T4 GPU card. The software
used are PyTorch 1.10.0 and CUDA 11.0."
EVALUATION,0.48148148148148145,"The training speed of these models generally match their PyTorch versions (≤6%) as shown in
Figure 4, and is stable with larger batch sizes (as in Figure 5), showing the effectiveness of KOKOYI’s
auto-batching."
EVALUATION,0.48677248677248675,"Usability
Coding in math equations is concise and succinct, especially when the model is compli-
cated. For example, Transformer in kokoyi-lang has only 62 lines of code, as compared with
155 in the PyTorch implementation (we count only the forward functions). The key that KOKOYI
codes are much compact has to do with the fact that it speciﬁes the computation at a ﬁner granularity
and lets auto-batching handles batching, whereas the PyTorch implementation must make sure all
operators are batched and aligned, see Appendix-B for a detailed inspection."
EVALUATION,0.49206349206349204,Under review as a conference paper at ICLR 2022
EVALUATION,0.4973544973544973,"Model
LoC (KOKOYI)
LoC (PyTorch)
MLP
10
10
LSTM
25
22
Seq2Seq
30
34
Transformer
62
155"
EVALUATION,0.5026455026455027,Table 1: Comparison on line-of-code (LoC) between KOKOYI and PyTorch
RELATED WORK,0.5079365079365079,"7
RELATED WORK"
RELATED WORK,0.5132275132275133,"Compilers for deep learning frameworks
are generally divided into two categories: Tensor
compilers such as Halide (Ragan-Kelley et al., 2013), TVM (Chen et al., 2018) and Tensor Com-
prehension (Vasilache et al., 2018) aims at generating efﬁcient codes for tensor operation; Program
translators like AutoGraph (Moldovan et al., 2018) and JAX (Bradbury et al., 2018) converts Python
into low-level IRs for more efﬁcient execution. KOKOYI can leverage both of them: speeding up
individual tensor operator by plugging in functions produced by the tensor compilers; replacing the
code generation stage and the runtime with the program translator’s for faster overall execution."
RELATED WORK,0.5185185185185185,"Auto-batching
is a technique that transforms the computation for a single instance to the one for a
batch. TensorFlow-Fold (Looks et al., 2017) provides domain-speciﬁc languages for users to express
sample-wise computation and performs batching on the computation graph, but has limited support
for control ﬂow. DyNet (Neubig et al., 2017) generates and batches computation graphs dynamically,
but with signiﬁcant runtime overhead. DGL (Wang et al., 2019) utilizes domain knowledge from the
graph neural network family for auto-batching, but cannot generalize to arbitrary programs. (Radul
et al., 2019) proposed a local static auto-batching solution that could handle control-ﬂow intensive
programs. None of these solutions can handle dynamic shapes."
FUTURE WORK,0.5238095238095238,"8
FUTURE WORK"
FUTURE WORK,0.5291005291005291,"There are many possible improvements to the current KOKOYI prototype, including:"
FUTURE WORK,0.5343915343915344,"A better kokoyi-lang editor.
Users of kokoyi-lang may frequently write mathematical
notations that are compact after rendering but cumbersome to LATEX, which necessitates a better editor.
There have been many efforts in developing What You See Is What You Mean (WYSIWYM) editors for
mathematical equations and even LATEX (Kastrup, 2002), which we believe are good bases to start
with. Early users have also expressed making KOKOYI available in other IDEs."
FUTURE WORK,0.5396825396825397,"Effective debugging tools.
Another question is how to present debugging information. Compilers
for traditional programming languages associate error messages with the locations to source code.
As for KOKOYI, the error messages must be further rendered alongside the highly compact symbol
deﬁnitions, since the generated backend code has less readability."
FUTURE WORK,0.544973544973545,"More compiler optimizations.
Many traditional code optimizations for the functional language
family are applicable to kokoyi-lang, including control-ﬂow analysis, lambda lifting, inlining
and so on. Recent advancements such as kernel fusion (Chen et al., 2018) and graph substitution (Jia
et al., 2019) that optimize the computation graph of deep learning programs are also promising to
integrate."
FUTURE WORK,0.5502645502645502,"Towards a web of executable papers.
Our ultimate vision is to build an ecosystem where papers
are executable. This can be accomplished by supporting compilation of KOKOYI code snippets
inside LATEX documents. From that perspective, a research paper is nothing more than an elaborated
documentation explaining an executable model. Since these snippets are executable, they are
guaranteed to be syntactically correct, removing the error-prone process to reproduce their correct
implementations. In fact, snippets of KOKOYI codes in different research papers can be cross-linked,
making them reusable for the wide community to consume."
FUTURE WORK,0.5555555555555556,Under review as a conference paper at ICLR 2022
REFERENCES,0.5608465608465608,REFERENCES
REFERENCES,0.5661375661375662,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy programs,
2018. URL http://github.com/google/jax."
REFERENCES,0.5714285714285714,"Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan
Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. {TVM}: An automated end-to-end optimizing
compiler for deep learning. In 13th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 18), pp. 578–594, 2018."
REFERENCES,0.5767195767195767,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014."
REFERENCES,0.582010582010582,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.5873015873015873,"Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991, 2015."
REFERENCES,0.5925925925925926,"Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and Alex Aiken. Taso:
optimizing deep learning computation with automatic generation of graph substitutions.
In
Proceedings of the 27th ACM Symposium on Operating Systems Principles, pp. 47–62, 2019."
REFERENCES,0.5978835978835979,"David Kastrup. Revisiting wysiwyg paradigms for authoring latex. COMMUNICATIONS OF THE
TEX USERS GROUP TUGBOAT EDITOR BARBARA BEETON PROCEEDINGS EDITORS KAJA
CHRISTIANSEN, 23(1):57, 2002."
REFERENCES,0.6031746031746031,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.6084656084656085,"Leslie Lamport. LATEX: a document preparation system: user’s guide and reference manual.
Addison-wesley, 1994."
REFERENCES,0.6137566137566137,"Chris Lattner and Vikram Adve. Llvm: A compilation framework for lifelong program analysis &
transformation. In International Symposium on Code Generation and Optimization, 2004. CGO
2004., pp. 75–86. IEEE, 2004."
REFERENCES,0.6190476190476191,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.6243386243386243,"Moshe Looks, Marcello Herreshoff, DeLesley Hutchins, and Peter Norvig. Deep learning with
dynamic computation graphs. arXiv preprint arXiv:1702.02181, 2017."
REFERENCES,0.6296296296296297,"Robin Milner. A theory of type polymorphism in programming. Journal of computer and system
sciences, 17(3):348–375, 1978."
REFERENCES,0.6349206349206349,"Dan Moldovan, James M Decker, Fei Wang, Andrew A Johnson, Brian K Lee, Zachary Nado,
D Sculley, Tiark Rompf, and Alexander B Wiltschko. Autograph: Imperative-style coding with
graph-based performance. arXiv preprint arXiv:1810.08061, 2018."
REFERENCES,0.6402116402116402,"Graham Neubig, Yoav Goldberg, and Chris Dyer. On-the-ﬂy operation batching in dynamic computa-
tion graphs. In Advances in Neural Information Processing Systems, pp. 3971–3981, 2017."
REFERENCES,0.6455026455026455,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems,
pp. 8024–8035, 2019."
REFERENCES,0.6507936507936508,"PyTorch. Torchscript, 2021. URL https://pytorch.org/docs/stable/jit.html."
REFERENCES,0.656084656084656,"Alexey Radul, Brian Patton, Dougal Maclaurin, Matthew D Hoffman, and Rif A Saurous.
Automatically batching control-intensive programs for modern accelerators.
arXiv preprint
arXiv:1910.11141, 2019."
REFERENCES,0.6613756613756614,Under review as a conference paper at ICLR 2022
REFERENCES,0.6666666666666666,"Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman
Amarasinghe. Halide: a language and compiler for optimizing parallelism, locality, and recompu-
tation in image processing pipelines. Acm Sigplan Notices, 48(6):519–530, 2013."
REFERENCES,0.671957671957672,"Jared Roesch, Steven Lyubomirsky, Logan Weber, Josh Pollock, Marisa Kirisame, Tianqi Chen, and
Zachary Tatlock. Relay: A new ir for machine learning frameworks. In Proceedings of the 2nd
ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pp.
58–68, 2018."
REFERENCES,0.6772486772486772,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.6825396825396826,"Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104–3112, 2014."
REFERENCES,0.6878306878306878,"Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal, Zachary DeVito,
William S Moses, Sven Verdoolaege, Andrew Adams, and Albert Cohen. Tensor comprehen-
sions: Framework-agnostic high-performance machine learning abstractions. arXiv preprint
arXiv:1802.04730, 2018."
REFERENCES,0.6931216931216931,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.6984126984126984,"Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma,
Lingfan Yu, Yu Gai, et al. Deep graph library: A graph-centric, highly-performant package for
graph neural networks. arXiv preprint arXiv:1909.01315, 2019."
REFERENCES,0.7037037037037037,Under review as a conference paper at ICLR 2022
REFERENCES,0.708994708994709,"A
ESTIMATION OF KOKOYI’S APPLICABILITY"
REFERENCES,0.7142857142857143,"KOKOYI as it stands can mend the gap between math-in-codes and math-in-paper and help researchers
who focus on new model development. Of course, not all research fall into such category. We sampled
papers from ﬁve top machine learning conferences and classiﬁed them into the following categories:"
REFERENCES,0.7195767195767195,"1. Model-NN: Papers that proposed new model architectures, new loss functions or new neural
network building blocks."
REFERENCES,0.7248677248677249,"2. Algorithm: Papers that proposed new algorithm/models but they generally are not imple-
mented with neural networks."
REFERENCES,0.7301587301587301,3. Optim: Papers with focus on optimization techniques.
REFERENCES,0.7354497354497355,4. Theory: Papers about theoretical analysis.
REFERENCES,0.7407407407407407,5. Other: Papers not in the above categories.
REFERENCES,0.746031746031746,"Figure 6 shows the result. KOKOYI can potentially beneﬁt most papers in the Model-NN category,
ranging from 15% to 66%."
REFERENCES,0.7513227513227513,"Other
15.4%"
REFERENCES,0.7566137566137566,"Theory
10.3%"
REFERENCES,0.7619047619047619,"Optim
2.6%"
REFERENCES,0.7671957671957672,Model-NN 15.4%
REFERENCES,0.7724867724867724,Algorithm 56.4%
REFERENCES,0.7777777777777778,(a) ICLR 2020
REFERENCES,0.783068783068783,"Other
15.6%"
REFERENCES,0.7883597883597884,"Theory
15.6%"
REFERENCES,0.7936507936507936,"Optim
12.5%"
REFERENCES,0.798941798941799,Model-NN 37.5%
REFERENCES,0.8042328042328042,Algorithm 18.8%
REFERENCES,0.8095238095238095,(b) ICML 2019
REFERENCES,0.8148148148148148,"Other
8.5%"
REFERENCES,0.8201058201058201,"Theory
24.4%"
REFERENCES,0.8253968253968254,"Optim
7.3%"
REFERENCES,0.8306878306878307,Model-NN 24.4%
REFERENCES,0.8359788359788359,Algorithm 35.4%
REFERENCES,0.8412698412698413,(c) NIPS 2019
REFERENCES,0.8465608465608465,"Other
33.3%"
REFERENCES,0.8518518518518519,Model-NN 66.7%
REFERENCES,0.8571428571428571,(d) ACL 2019
REFERENCES,0.8624338624338624,"Other
22.2%"
REFERENCES,0.8677248677248677,"Theory
2.8%
Algorithm
8.3%
Model-NN 66.7%"
REFERENCES,0.873015873015873,(e) CVPR 2019
REFERENCES,0.8783068783068783,Figure 6: Classiﬁcation of the papers sampled from several recent conferences
REFERENCES,0.8835978835978836,Under review as a conference paper at ICLR 2022
REFERENCES,0.8888888888888888,"B
STUDY OF TRANSFORMER CODE"
REFERENCES,0.8941798941798942,"To quantify the actual Line-of-code saving of Kokoyi on the Transformer model, we compare the
implementation in PyTorch and KOKOYI side-by-side in Figure 7. Overall, a large portion of
the PyTorch implementation is for manipulating shapes of the tensors to align with the batching
requirement of the subsequent operations. By contrast, these non-math parts are completely absent
in the KOKOYI implementation thanks to the ﬂexible and succinct tensor constructor syntax. Yet,
KOKOYI can still achieve similar training speed by its auto-batching design."
REFERENCES,0.8994708994708994,"Reshape the weight matrices to 
align with the shape requirement of 
batched matrix multiplication."
REFERENCES,0.9047619047619048,Recover the num_heads dimension.
REFERENCES,0.91005291005291,"Swap axis so that the follow-up 
attention can be computed for 
multiple heads at once."
REFERENCES,0.9153439153439153,"Recover the manipulated 
shapes and align with the 
shape requirement of the 
last matrix multiplication."
REFERENCES,0.9206349206349206,"Swap axis so that attention can be 
computed between all pairs"
REFERENCES,0.9259259259259259,Calculate mask
REFERENCES,0.9312169312169312,Specify axis
REFERENCES,0.9365079365079365,Swap axis so that attention can be
REFERENCES,0.9417989417989417,computed between all pairs
REFERENCES,0.9470899470899471,Specify axis
REFERENCES,0.9523809523809523,"Figure 7: Comparison of the multi-head attention code in PyTorch and KOKOYI. We hightlight the
parts of shape manipulation that are saved by KOKOYI."
REFERENCES,0.9576719576719577,Under review as a conference paper at ICLR 2022
REFERENCES,0.9629629629629629,"C
MODEL BENCHMARK DETAILS"
REFERENCES,0.9682539682539683,"Synthetic inputs are used for measuring training speed. For all the models, we have veriﬁed their
accuracy on real datasets (see the supplementary). Below lists the model conﬁgurations in Figure 4."
REFERENCES,0.9735449735449735,"• For MLP, we use an input size of 784, a batch size of 512 and all the hidden sizes are also 512."
REFERENCES,0.9788359788359788,"• For LeNet, we use an input image size of 32 × 32 with one channel, a batch size of 64 and all the
hidden feature maps have 512 channels."
REFERENCES,0.9841269841269841,"• For LSTM and BiLSTM, we use an input sequence of length 50, a word vocabulary of size 5000
and a batch size of 512. Both the word embedding size and hidden sizes are set to 512."
REFERENCES,0.9894179894179894,"• For GAN and VAE, we set the size of the latent vector to be 512 and test them on input images of
size 32 × 32 with one channel. The batch size is 1024."
REFERENCES,0.9947089947089947,"• For Seq2Seq and Transformer, both of their source and target sequences have 50 words drawn from
a vocabulary of size 5000. Both the batch size and hidden sizes are set to 512."
