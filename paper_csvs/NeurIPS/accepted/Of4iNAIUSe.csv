Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001976284584980237,"Due to the heterogeneous architectures and class skew, the global representation
models training in resource-adaptive federated self-supervised learning face with
tricky challenges: deviated representation abilities and inconsistent representation
spaces. In this work, we are the first to propose a multi-teacher knowledge distil-
lation framework, namely FedMKD, to learn global representations with whole
class knowledge from heterogeneous clients even under extreme class skew. Firstly,
the adaptive knowledge integration mechanism is designed to learn better repre-
sentations from all heterogeneous models with deviated representation abilities.
Then the weighted combination of the self-supervised loss and the distillation
loss can support the global model to encode all classes from clients into a unified
space. Besides, the global knowledge anchored alignment module can make the
local representation spaces close to the global spaces, which further improves the
representation abilities of local ones. Finally, extensive experiments conducted
on two datasets demonstrate the effectiveness of FedMKD which outperforms
state-of-the-art baselines 4.78% under linear evaluation on average."
INTRODUCTION,0.003952569169960474,"1
Introduction"
INTRODUCTION,0.005928853754940711,"The federated self-supervised learning (Fed-SSL) has emerged as a highly promising paradigm due
to the extremely limited labeled data in real-world scenarios [4, 25, 30]. The Fed-SSL mechanism
can learn common representations collaboratively across all the clients without labeled data [7, 8],
which could enable the aggregation of knowledge from diverse unlabeled data sources and overcome
the limitations caused by the high cost and scarcity of labeled data [24, 28, 31]."
INTRODUCTION,0.007905138339920948,"Traditional Fed-SSL methods usually assume that each client should train the identical architecture
model, such as FedU [33], FedEMA [34], FedCA [30]. But it would not be easy in resource-limited
scenarios, especially for the existing arsing large-scale models [1, 26]. As shown in Fig.1, client A,
client B and client C might train heterogeneous representation models due to the varying system
resources. In addition, real-world data often exhibits skewed class distributions across clients."
INTRODUCTION,0.009881422924901186,"∗Xiao Zhang and Dongxiao Yu are corresponding authors. Email:{xiaozhang, dxyu}@sdu.edu.cn"
INTRODUCTION,0.011857707509881422,"Therefore, how to learn global class representations under the heterogeneous architectures and class
skew in resource-aware Fed-SSL paradigm is challenging, particularly comparing with existing
FedU 2 [17], FedX [5] with identical architectures."
INTRODUCTION,0.01383399209486166,"Although both Hetero-SSFL [20] and FedFoA [19] consider the heterogeneous client models, they
can not learn a global representation model. In order to aggregate the knowledge from the clients to
form global class representations, some tricky challenges arise. (1) Deviated representation abilities.
Even for the same data samples, the different models might encode them into different latent spaces
with deviated representation abilities. For example, client A and client B all have images with dog,
cat, tiger, but client model A can encode cat, tiger well into different clusters, client model B can only
learn better representations of dog. So how could global representation models take advantage of the
best of both client models? (2) Inconsistent representation spaces. The skewed class distributions
across clients lead to inconsistent representation spaces. For example in Fig. 1, comparing with
client A, client C has different kinds of images with dog, cat, airplane. Thus how to make global
representation models encode the whole classes from all the clients well in a unified space? Therefore,
different from the existing works, our goal is to break the gaps caused by the hybrid heterogeneity,
which can learn the high-quality global representation model in federated self-supervised learning."
INTRODUCTION,0.015810276679841896,"Global 
representation 
space"
INTRODUCTION,0.017786561264822136,"Client A
Client B
Client C"
INTRODUCTION,0.019762845849802372,Server Cat
INTRODUCTION,0.021739130434782608,"Representation Space
Representation Space
Representation Space dog plane cat"
INTRODUCTION,0.023715415019762844,"Inconsistent
representation dog cat dog tiger"
INTRODUCTION,0.025691699604743084,"Deviated 
representation 
abilities Dog Tiger Plane Cat Dog Tiger Plane Cat Dog Tiger Plane"
INTRODUCTION,0.02766798418972332,"Figure 1:
Illustrations of main challenges in
resource-aware Fed-SSL."
INTRODUCTION,0.029644268774703556,"Along this line, we propose FedMKD, a multi-
teacher knowledge distillation based resource-
adaptive Fed-SSL framework, which can learn
global representations over all classes from het-
erogeneous clients. First, an adaptive knowl-
edge integration module is introduced to learn
high-quality representations from all the het-
erogeneous models with deviated representa-
tion abilities.
Then in order to encode all
classes from clients in a unified space, the global
model uses the weighted combination of self-
supervised loss and distillation loss to update.
Besides, the global knowledge anchored align-
ment module is applied within the server to elim-
inate the inconsistency in representation spaces
and reduce the burden on the clients. It uses
global knowledge to additionally update the lo-
cal models, which can not only make the lo-
cal representation spaces close to the global
space but also improve the representation ca-
pability of both local models and the global
ones. Code is available at https://github.
com/limee-sdu/FedMKD. The main contribu-
tions of this paper can be summarized as follows."
INTRODUCTION,0.03162055335968379,"• In resource-aware Fed-SSL, we are the first to delve into global class representation learning
through revealing the deviated representation abilities and inconsistent representation spaces
caused by the heterogeneous architectures and class skew."
INTRODUCTION,0.03359683794466403,"• We design a multi-teacher knowledge distillation framework, namely FedMKD, to adaptively
aggregate positive knowledge from heterogeneous models with deviated representation
abilities. Through combining the self-supervised loss and the distillation loss, FedMKD can
encode skewed classes into a unified space."
INTRODUCTION,0.03557312252964427,"• Extensive experiments conducted on CIFAR-10 and CIFAR-100 show the representation
abilities over all classes of the FedMKD perform better than state-of-the-art baselines. Our
algorithm can improve 4.22% and 5.31% separately on the two chosen datasets."
RELATED WORK,0.037549407114624504,"2
Related work"
RELATED WORK,0.039525691699604744,"The federated self-supervised learning aims to learn high-quality representations from clients without
large labeled datasets [11]. From the beginning, several works [12, 23] simply combine federated
learning with self-supervised methods. Besides, FedU [33] designs a communication-efficient mech-"
RELATED WORK,0.041501976284584984,Table 1: Comparison of federated self-supervised learning methods.
RELATED WORK,0.043478260869565216,"Method
Global
Global
Model
Deviated
Inconsistent
Theoretical
model
model size
Heterogeneity
representation ability
representation space
Analysis"
RELATED WORK,0.045454545454545456,"FedU [33]
✓
= Client
✗
✗
✗
✗
FedEMA [34]
✓
= Client
✗
✗
✗
✗
L-DAWA [21]
✓
= Client
✗
✗
✗
✗
FedX [5]
✓
= Client
✗
✗
✗
✗
FedCA [30]
✓
= Client
✗
✗
✓
✗
FLPD [29]
✓
= Client
✓
✗
✗
✗
FedU 2 [17]
✓
= Client
✗
✗
✓
✓
FedFoA [19]
✗
-
✓
✗
✓
✗
Hetero-SSFL [20]
✗
-
✓
✗
✓
✓
FedMKD(ours)
✓
≥Client
✓
✓
✓
✓"
RELATED WORK,0.04743083003952569,"anism by only aggregating the online encoders under non-IID data. FedUTN [16] is proposed to use
the aggregated online networks for the target network updating in the self-supervised framework.
L-DAWA [21] proposes the layer-wise divergence aware weight aggregation to mitigate the influence
of client bias. FedEMA [34] considers the divergence-aware moving average updating in clients,
measuring the divergence between local models and global model. FedX [5] proposes a unsupervised
federated learning framework to learn representations through a two-sided distillation method. How-
ever, all the above works intuitively gain the global model through parameters average due to the
identical client models, which can not be applied in the heterogeneous clients setting directly. In
addition, although FedCA [30] address the misaligned and inconsistent representation challenges
by gathering features from clients, inducing potential privacy problems. FLPD [27, 29] introduces
distillation method based similarity between prototypes from a labeled public dataset to update the
local model. FedU 2 [17] focuses on mitigating representation collapse entanglement and obtaining
unified representation spaces."
RELATED WORK,0.04940711462450593,"Considering heterogeneous client models in federated self-supervised learning, Hetero-SSFL [6]
introduces linear-CKA to align lower-dimensional representations between the local model and
global model without architectural constraints. FedFoA [19] designs a factorization-based method
to extract the cross-feature relation matrix from the local representations for aggregation. However,
both Hetero-SSFL and FedFoA can not learn a global representation model considering the hybrid
heterogeneity, which is the main focus of our work. The comparison details are shown in Table 1."
PRELIMINARIES,0.05138339920948617,"3
Preliminaries"
PRELIMINARIES,0.0533596837944664,"The goal of federated unsupervised learning is to learn the generalized representation for some
downstream tasks from several distributed unlabeled data sources. A federated learning setting
consists of a central server and N clients. Each client k contains a local unlabeled dataset Dk, and
the server contains a public unlabeled dataset Dpub. The local objective at k-th client is"
PRELIMINARIES,0.05533596837944664,"min
θk F(θk) = Eξk∼Dk[Lk(θk, ξk)],
(1)"
PRELIMINARIES,0.05731225296442688,"to minimize the expected local loss of client k on local dataset Dk and ξk is the unlabeled data.
In traditional FL settings, it’s assumed that {θk} are identical and gain the global model using
θ = PN
k=1 pkθk, where pk is the weight of k-th client. But in real-world cross-device scenarios, each
client might have a unique model and the architecture of the model might be different, which means
that traditional aggregation methods are not available. We assume that θk is not similar to others, and
use {θk} to collaborate in training the larger global model θ in server. Here we define the global
update function is θt = G(θt−1; θ1, · · · , θN). Our final aim is to optimize the global goal"
PRELIMINARIES,0.05928853754940711,"min
θ
Fglobal(θ) = Eξ∼Dglobal[L(θ, ξ)],
(2)"
PRELIMINARIES,0.06126482213438735,"where L(θ, ξ) is global loss function in server and ξ is the unlabeled data sampled from global dataset."
DESIGNED FEDMKD METHOD,0.06324110671936758,"4
Designed FedMKD Method"
DESIGNED FEDMKD METHOD,0.06521739130434782,"We propose a multi-teacher knowledge distillation based federated self-supervised learning framework
FedMKD, which is shown in Fig. 2. In FedMKD, besides the local self-supervised learning (Sec. 4.1),"
DESIGNED FEDMKD METHOD,0.06719367588932806,"Online
encoder"
DESIGNED FEDMKD METHOD,0.0691699604743083,"Online
 encoder"
DESIGNED FEDMKD METHOD,0.07114624505928854,"Encoder
 Encoder"
DESIGNED FEDMKD METHOD,0.07312252964426877,"Online
 encoder"
DESIGNED FEDMKD METHOD,0.07509881422924901,"Encoder
 Encoder"
DESIGNED FEDMKD METHOD,0.07707509881422925,"Encoder
 Encoder"
DESIGNED FEDMKD METHOD,0.07905138339920949,"Online
 encoder"
DESIGNED FEDMKD METHOD,0.08102766798418973,"Target
 encoder"
DESIGNED FEDMKD METHOD,0.08300395256916997,"Online
 encoder"
DESIGNED FEDMKD METHOD,0.08498023715415019,"Target
 encoder"
DESIGNED FEDMKD METHOD,0.08695652173913043,"Initialization
Local training"
DESIGNED FEDMKD METHOD,0.08893280632411067,"Clients
Server"
DESIGNED FEDMKD METHOD,0.09090909090909091,Attention
DESIGNED FEDMKD METHOD,0.09288537549407115,Contrastive
DESIGNED FEDMKD METHOD,0.09486166007905138,Global anchored alignment Align ~ ~ ~
DESIGNED FEDMKD METHOD,0.09683794466403162,"Target
 encoder"
DESIGNED FEDMKD METHOD,0.09881422924901186,"Online
 encoder p(·) p(·) p(·)"
DESIGNED FEDMKD METHOD,0.1007905138339921,"Online
 encoder
p(·)"
DESIGNED FEDMKD METHOD,0.10276679841897234,"Online
encoder Align"
DESIGNED FEDMKD METHOD,0.10474308300395258,Multi-teacher knowledge distillation
DESIGNED FEDMKD METHOD,0.1067193675889328,"Global
     model"
DESIGNED FEDMKD METHOD,0.10869565217391304,"Global
     model"
DESIGNED FEDMKD METHOD,0.11067193675889328,"Global
     model"
DESIGNED FEDMKD METHOD,0.11264822134387352,"Figure 2: The overall framework of FedMKD. Clients initialize the model architecture based on the
local resource, then self-supervised train the local model using unlabeled local data. The server uses
the multi-teacher adaptive knowledge integration distillation to aggregate positive local knowledge to
train the global model and then updates local models again according to the alignment module."
DESIGNED FEDMKD METHOD,0.11462450592885376,"we design a multi-teacher adaptive knowledge integration distillation module to adaptive determine
the weight of the representations from heterogeneous local models with deviated representation
abilities. The distilled loss combined with the global self-supervised loss, we can gain the weighted
combined loss to update the global model, so that the global model can encode all classes from
clients in a unified space (Sec. 4.2). And the global knowledge anchored alignment could improve
the representation capability of clients and further benefit the global model training (Sec. 4.3). In
addition, we provide the theoretical analysis of our algorithm in Appendix B."
SELF-SUPERVISED MODEL TRAINING,0.116600790513834,"4.1
Self-supervised model training"
SELF-SUPERVISED MODEL TRAINING,0.11857707509881422,"Each client performs self-supervised contrastive learning using an asymmetric Siamese network,
inspired by BYOL [3]. The model M comprises an online encoder θ and a target encoder ϕ, both
sharing the same architecture, with the online network incorporating an additional predictor p. That
is M = {p(ϕ(·)), θ(·)}. Given an unlabeled image x, we can obtain two augmented views, v and v′,
serving as inputs to online and target networks, respectively. The loss function is defined as follows:"
SELF-SUPERVISED MODEL TRAINING,0.12055335968379446,"Lself =

p(r)
∥p(r)∥−
r′ ∥r′∥ "
SELF-SUPERVISED MODEL TRAINING,0.1225296442687747,"2
,
(3)"
SELF-SUPERVISED MODEL TRAINING,0.12450592885375494,"where r = θ(v) and r′ = ϕ(v′). This loss encourages the online network to produce representation
p(r) that is similar to the positive sample generated by the target network r′. We then exchange
the views, feeding v′ to the online network and v to the target network, to compute L′
self. At each
training step, we use stochastic gradient descent to minimize ˜Lself = Lself + L′
self to update the
online network ϕ alone,
θ ←θ −η∇θ ˜Lself.
(4)
The target network helps to provide regression targets to train the online network. Choosing α ∈[0, 1]
as the target decay rate, we employ the exponential moving average (EMA) of the online network to
update ϕ:
ϕ ←αϕ + (1 −α)θ.
(5)"
SELF-SUPERVISED MODEL TRAINING,0.12648221343873517,"Using this self-supervised training method, the model learns intricate representations from unlabeled
data, capturing high-level features and patterns inherent in the dataset."
SELF-SUPERVISED MODEL TRAINING,0.12845849802371542,"4.2
Multi-teacher adaptive knowledge integration distillation."
SELF-SUPERVISED MODEL TRAINING,0.13043478260869565,"In contrast to homogeneous federated learning, the presence of model heterogeneity poses a challenge:
direct aggregation of local models into a global model is not feasible. To overcome this, we design a
multi-teacher knowledge distillation mechanism to transfer local knowledge to the server."
SELF-SUPERVISED MODEL TRAINING,0.1324110671936759,"Given a batch of data B, the representation from the teacher model is denoted as rt and that from the
student model as rs, the knowledge distillation loss is defined as follows:"
SELF-SUPERVISED MODEL TRAINING,0.13438735177865613,"Ldistill = −log
exp(sim(rs,i, rt,i)/τ)
exp(sim(rs,i, rt,i)/τ) +
P"
SELF-SUPERVISED MODEL TRAINING,0.13636363636363635,"k∈{B−i}
exp(sim(rs,i, rs,k)/τ),
(6)"
SELF-SUPERVISED MODEL TRAINING,0.1383399209486166,"where τ is the temperature parameter controlling entropy and sim(·) is the similarity function
between two representations."
SELF-SUPERVISED MODEL TRAINING,0.14031620553359683,"Then we extend this knowledge distillation learning method to multi-teacher. Although the data is
heterogeneous, the knowledge of each local model is valuable, each local model captures the unique
characteristics of local data. Our goal is to integrate the positive knowledge of all clients to guide
the global model in learning a general representation of unlabeled data. We design a multi-teacher
adaptive knowledge integration distillation that can adaptively weigh the representations from clients."
SELF-SUPERVISED MODEL TRAINING,0.1422924901185771,"Given a sample xi, representation from the n-th local model is Rn,i ∈Rd where d is the dimension
of the representation. Following [3], a fully connected layer is employed to project the representation
into a lower-dimensional space, enhancing the discriminate power of the learned representations. So,
we map the representation from the global model Rs,i into the same lower latent space, obtaining"
SELF-SUPERVISED MODEL TRAINING,0.1442687747035573,"rs,i = g(Rs,i), rn,i = g(Rn,i),
(7)"
SELF-SUPERVISED MODEL TRAINING,0.14624505928853754,"where rn,i, rs,i ∈Rk, k is the dimension of the new latent space and g(·) is the projector."
SELF-SUPERVISED MODEL TRAINING,0.1482213438735178,"In addition, we introduce an adapter module to learn instance-level teacher importance weights for
knowledge integration. After getting rn,i, an attention block is used to generate the weighted sum
of them. In this context, representation from global model rs,i is treated as the query, while those
from local models ˜R = [r1,i, r2,i, . . . , rN,i]T is treated as the key and value. Treating representations
from the global model as query ensures consistency in knowledge transfer. The attention mechanism
computes attention scores to understand the relevance of each local model’s representation to the
global model’s query. The aggregated representation is:"
SELF-SUPERVISED MODEL TRAINING,0.15019762845849802,"¯ri = Attn(rs,i, ˜R) = softmax(rs,i · ˜R
√"
SELF-SUPERVISED MODEL TRAINING,0.15217391304347827,"k
) ˜R,
(8)"
SELF-SUPERVISED MODEL TRAINING,0.1541501976284585,"where ¯ri means the aggregated representation, and Attn(·) denotes the attention block."
SELF-SUPERVISED MODEL TRAINING,0.15612648221343872,"Returning to the knowledge distillation for unlabeled data proposed earlier, we treat the aggregate
representation as the positive sample, and the remaining samples in the batch as the negative sample.
The adaptive weight multi-teacher knowledge distillation loss function is expressed as follows:"
SELF-SUPERVISED MODEL TRAINING,0.15810276679841898,"Ldistill = −log
exp(sim(rs,i, ¯ri)/τ)
exp(sim(rs,i, ¯ri)/τ) + P"
SELF-SUPERVISED MODEL TRAINING,0.1600790513833992,"j̸=i
exp(sim(rs,i, rs,j)/τ).
(9)"
SELF-SUPERVISED MODEL TRAINING,0.16205533596837945,"Above all, the weighted combined loss for the global model is presented as:"
SELF-SUPERVISED MODEL TRAINING,0.16403162055335968,"Lserver = Lself + γLdistill,
(10)"
SELF-SUPERVISED MODEL TRAINING,0.16600790513833993,where γ is a hyper-parameter controlling the weight of the distillation process.
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.16798418972332016,"4.3
Global knowledge anchored alignment"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.16996047430830039,"As we mentioned, the representations from different models are inconsistent and the representation
abilities of models are also deviated. So we introduce the global knowledge anchored alignment
mechanism that each local model uses the global model as an anchor. It ensures that the local
representation spaces are closer to the global ones."
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.17193675889328064,"Unlike methods such as FedX [5] and MOON [15], which align local models to the global model
locally, our approach aims to train a better global encoder tailored for resource-constrained federated
learning scenarios. Those methods are not available when clients cannot afford to store or infer the
global model locally. So we transfer this alignment process to the server."
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.17391304347826086,"After finishing the global model training, we construct local twin models in the server to realize the
alignment under global view. Here, we use the global online network and local online network to"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.17588932806324112,"airplane
automobile"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.17786561264822134,"bird
cat"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.17984189723320157,"deer
dog"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.18181818181818182,"frog
horse"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.18379446640316205,"ship
truck
(a) Standalone
training
ResNet18 on Partial public
dataset."
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.1857707509881423,"airplane
automobile"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.18774703557312253,"bird
cat"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.18972332015810275,"deer
dog"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.191699604743083,"frog
horse"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.19367588932806323,"ship
truck
(b) MOON on Partial pub-
lic datset."
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.1956521739130435,"airplane
automobile"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.1976284584980237,"bird
cat"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.19960474308300397,"deer
dog"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.2015810276679842,"frog
horse"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.20355731225296442,"ship
truck
(c) FedMKD on IID public
dataset"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.20553359683794467,"airplane
automobile"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.2075098814229249,"bird
cat"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.20948616600790515,"deer
dog"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.21146245059288538,"frog
horse"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.2134387351778656,"ship
truck
(d) FedMKD on Partial
public dataset"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.21541501976284586,"Figure 3: T-SNE visualizations of hidden vectors from different models on CIFAR-10, the data
distribution of clients is IID."
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.21739130434782608,"construct a new asymmetric siamese network called the twin of the original local model. The local
online network θn is the online model, and the global online network θs is the target model, that is,"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.21936758893280633,"{θ′
n, ϕ′
n} ←{θn, θs}
(11)"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.22134387351778656,"and ˜
M = {θ′
n, ϕ′
n}. The training loss is updated to"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.22332015810276679,"Lalign = −log
exp(sim(θ′
n(i), ϕ′
n(i))/τ)
P"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.22529644268774704,"i∈B exp(sim(θ′n(i), ϕ′n(i))/τ).
(12)"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.22727272727272727,"According to the idea of contrastive learning, the representations learned by the online network
become more consistent with the knowledge captured by the target network. This global knowledge
anchored contrastive learning suggests that the global model’s knowledge is used as a positive
example for the local model to train itself, thus making it more consistent with the target global
network. Aligning local models with the global view representation helps create a comprehensive
understanding of the overall data distribution. The process refines the knowledge acquired locally,
ensuring that it contributes meaningfully to the overall federated learning process. Then, we use
stochastic gradient descent to minimize Lalign to update the θ′
n,"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.22924901185770752,"θ′
n ←θ′
n −η∇Lalign.
(13)"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.23122529644268774,"Once the global model anchored alignment is finished, the server will send the online network of the
local twin network θ′ to the corresponding client to update the local model. The local online network
θn is replaced by the server-updated network θ′
n. The target network is not replaced to retain more
local knowledge and stabilize model training:"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.233201581027668,"{θq,n, ϕq,n} ←{θ′
q−1,n, ϕq−1,n}
(14)"
GLOBAL KNOWLEDGE ANCHORED ALIGNMENT,0.23517786561264822,"so that, the local model can benefit from the alignment process and align to the representation under
global view, which can further use the local data to train the model."
EXPERIMENTS,0.23715415019762845,"5
Experiments"
EXPERIMENTS,0.2391304347826087,"In this section, we evaluate the representations learned from our proposed global model FedMKD on
CIFAR-10 and CIFAR-100. We first describe the experimental setup and baselines, and then analyze
the performance in comparison to other methods. Due to the space limitation, further hyperparameter
analysis and communication cost analysis are represented in Appendix D."
EXPERIMENTAL SETUP,0.24110671936758893,"5.1
Experimental setup"
EXPERIMENTAL SETUP,0.24308300395256918,"We use CIFAR-10 and CIFAR-100 [13] datasets to train all the models. Both of them contain 50,000
training images and 10,000 testing images. To construct the public dataset, we sample 4000 data
samples from the training set, then divide the remaining data into N partitions to simulate N clients."
EXPERIMENTAL SETUP,0.2450592885375494,"Table 2: Top-1 accuracy comparison under linear probing on CIFAR datasets with best model
performance in bold and second-best results with underlines. ’-’ means this method is not suitable for
the experiment setting."
EXPERIMENTAL SETUP,0.24703557312252963,"Method
Pub.
CIFAR-10 (%)
CIFAR-100 (%)
Class
Dir(β=0.5)
IID
Class
Dir(β=0.5)
IID
Std. ResNet18 IID"
EXPERIMENTAL SETUP,0.2490118577075099,"51.09 ± 0.04
25.35 ± 0.02
FedMD
45.28 ± 0.02
45.93 ± 0.02
46.21 ± 0.02
23.25 ± 0.05
22.46 ± 0.04
23.20 ± 0.03
FedDF
46.94 ± 0.04
48.04 ± 0.02
48.74 ± 0.08
23.07 ± 0.03
22.73 ± 0.03
21.57 ± 0.01
MOON-KL
44.93 ± 0.05
45.84 ± 0.03
46.51 ± 0.03
21.26 ± 0.03
21.34 ± 0.01
21.82 ± 0.02
MOON
53.35 ± 0.03
53.71 ± 0.04
55.14 ± 0.02
27.82 ± 0.01
26.84 ± 0.03
26.70 ± 0.03
FedET
56.42 ± 0.02
59.38 ± 0.03
61.43 ± 0.02
29.11 ± 0.03
26.98 ± 0.01
24.48 ± 0.02
FedU/FedEMA
-
-
-
-
-
-
Hetero-SSFL
59.13 ± 0.02
64.04 ± 0.04
65.61 ± 0.07
30.84 ± 0.10
29.63 ± 0.06
28.89 ± 0.06
FedMKD
64.81 ± 0.02
66.98 ± 0.06
69.07 ± 0.04
36.33 ± 0.01
35.59 ± 0.07
35.94 ± 0.02
Std. ResNet18 Par."
EXPERIMENTAL SETUP,0.2509881422924901,"50.15 ± 0.02
24.97 ± 0.01
FedMD
47.16 ± 0.03
46.39 ± 0.02
45.93 ± 0.03
23.95 ± 0.05
23.14 ± 0.03
22.47 ± 0.01
FedDF
52.59 ± 0.05
53.50 ± 0.03
54.17 ± 0.05
27.21 ± 0.02
27.31 ± 0.04
27.05 ± 0.04
MOON-KL
46.41 ± 0.03
46.81 ± 0.03
45.89 ± 0.01
21.73 ± 0.01
20.97 ± 0.03
22.27 ± 0.04
MOON
54.31 ± 0.04
54.54 ± 0.02
52.94 ± 0.04
27.00 ± 0.04
27.27 ± 0.01
28.26 ± 0.02
FedET
57.75 ± 0.01
57.08 ± 0.01
58.59 ± 0.01
29.38 ± 0.01
28.12 ± 0.02
29.61 ± 0.01
FedU/FedEMA
-
-
-
-
-
-
Hetero-SSFL
63.20 ± 0.08
61.93 ± 0.04
61.15 ± 0.07
30.94 ± 0.05
29.92 ± 0.03
29.56 ± 0.03
FedMKD
66.39 ± 0.09
67.60 ± 0.04
65.88 ± 0.03
35.82 ± 0.02
35.55 ± 0.05
34.38 ± 0.02"
EXPERIMENTAL SETUP,0.25296442687747034,"To assess the validity of the public dataset, we use two sampling methods to construct it. First, we
use a random sampling method over all classes to generate public dataset ’IID’. And for the public
dataset ’Partial’, data is selected randomly from 40% classes in two datasets."
EXPERIMENTAL SETUP,0.2549407114624506,"We utilize three settings to simulate heterogeneous data distributions among all the clients. For the
IID setting, each client contains the same number of samples from all classes. For the class setting,
each client only has 10/N and 100/N classes on two datasets and the classes between clients have no
overlap. For the non-IID setting, data heterogeneity levels are described by the Dirichlet distribution
Dir(β) [10], where smaller β represents stronger heterogeneity levels, here we choose β = 0.5."
EXPERIMENTAL SETUP,0.25691699604743085,"Regarding the self-supervised learning framework design within each client, we use ResNet18 [9]
and VGG9 [22] as the encoder network and Multi-Layer Perception (MLP) as the predictor. In order
to construct the model heterogeneous setting, 2 clients train the Resnet18 encoder while 3 clients use
the VGG9. And for the global representation model, Resnet18 is selected as the encoder in server."
BASELINES AND EVALUATION METHODS,0.25889328063241107,"5.2
Baselines and evaluation methods"
BASELINES AND EVALUATION METHODS,0.2608695652173913,"Firstly, we select several federated knowledge distillation frameworks FedMD [14], FedDF [18],
FedET [2], MOON [15], MOON-KL that use unlabeled public dataset for distillation. We then replaced
the local model with a self-supervised model to evaluate the process of knowledge distillation in our
method. And several federated self-supervised learning frameworks FedU [33], FedEMA [34], Hetero-
SSFL [20] are also chosen as baselines. To verify the client’s knowledge can improve the global
model, we also train the global model separately on the public dataset, denoted as Std. ResNet18.
Following FedEMA [30], we evaluate the performance of learned representations using linear and
semi-supervised evaluation. Due to limited space, please refer to Appendix C for more details."
PERFORMANCE EVALUATION,0.2628458498023715,"5.3
Performance Evaluation"
PERFORMANCE EVALUATION,0.2648221343873518,"Table 2 and 3 shows the linear evaluation results and semi-supervised evaluation results of FedMKD
compared with all the baselines on CIFAR-10 and CIFAR-100. We can gain the following observation."
PERFORMANCE EVALUATION,0.26679841897233203,"On the whole, our FedMKD outperforms all baselines under different public dataset settings and
different data heterogeneity level settings on both two datasets. Compared to the second-best results,
FedMKD achieves significant improvement. On average, our model improves CIFAR-10 by 4.22%
and CIFAR-100 by 5.31% under linear evaluation and gain 3.66% and 2.07% improvement on two
dataset under semi-supervised evaluation."
PERFORMANCE EVALUATION,0.26877470355731226,"The effectiveness of our multi-teacher adaptive knowledge integration distillation can be approved
when compared with FedMD, FedDF, MOON-KL and MOON. Although these methods all designed"
PERFORMANCE EVALUATION,0.2707509881422925,"Table 3: Top-1 accuracy comparison on 1% of labeled data for semi-supervised learning on CIFAR
datasets with best model performance in bold and second-best results with underlines. ’-’ means this
method doesn’t apply for the experiment setting."
PERFORMANCE EVALUATION,0.2727272727272727,"Method
Pub.
CIFAR-10 (%)
CIFAR-100 (%)
Class
Dir(β=0.5)
IID
Class
Dir(β=0.5)
IID
Std. ResNet18 IID"
PERFORMANCE EVALUATION,0.274703557312253,"46.84 ± 0.25
15.19 ± 0.20
FedMD
43.32 ± 0.22
44.20 ± 0.18
44.66 ± 0.20
15.88 ± 0.17
14.94 ± 0.19
15.34 ± 0.12
FedDF
43.60 ± 0.44
44.13 ± 0.16
44.80 ± 0.40
14.39 ± 0.20
13.06 ± 0.14
12.90 ± 0.07
MOON-KL
45.42 ± 0.26
46.61 ± 0.21
46.72 ± 0.15
16.25 ± 0.06
17.22 ± 0.25
16.07 ± 0.04
MOON
49.96 ± 0.24
50.21 ± 0.10
51.78 ± 0.28
19.23 ± 0.12
17.21 ± 0.13
17.07 ± 0.18
FedET
52.37 ± 0.24
56.57 ± 0.17
57.44 ± 0.13
19.70 ± 0.08
16.82 ± 0.20
15.68 ± 0.18
FedU/FedEMA
-
-
-
-
-
-
Hetero-SSFL
54.30 ± 0.15
58.73 ± 0.54
60.50 ± 0.12
20.04 ± 0.40
19.19 ± 0.17
18.82 ± 0.17
FedMKD
59.65 ± 0.28
61.78 ± 0.40
64.06 ± 0.32
22.57 ± 0.12
22.13 ± 0.11
22.07 ± 0.15
Std. ResNet18 Par."
PERFORMANCE EVALUATION,0.2766798418972332,"46.42 ± 0.12
14.12 ± 0.09
FedMD
44.54 ± 0.26
43.61 ± 0.13
42.52 ± 0.19
17.32 ± 0.17
16.47 ± 0.14
16.31 ± 0.07
FedDF
48.14 ± 0.27
48.74 ± 0.12
48.56 ± 0.18
17.01 ± 0.04
17.14 ± 0.01
16.95 ± 0.01
MOON-KL
46.76 ± 0.05
46.92 ± 0.05
46.49 ± 0.22
16.21 ± 0.27
16.10 ± 0.12
16.94 ± 0.12
MOON
50.43 ± 0.18
51.99 ± 0.34
49.86 ± 0.19
18.64 ± 0.21
18.92 ± 0.25
19.29 ± 0.08
FedET
52.75 ± 0.07
52.61 ± 0.03
54.64 ± 0.14
18.49 ± 0.12
18.16 ± 0.13
18.01 ± 0.22
FedU/FedEMA
-
-
-
-
-
-
Hetero-SSFL
59.95 ± 0.34
58.31 ± 0.50
58.35 ± 0.20
20.72 ± 0.14
20.30 ± 0.32
19.62 ± 0.08
FedMKD
61.55 ± 0.19
63.10 ± 0.28
61.08 ± 0.19
22.44 ± 0.19
22.21 ± 0.17
20.94 ± 0.25"
PERFORMANCE EVALUATION,0.27865612648221344,"new federated knowledge distillation frameworks based on unlabeled public dataset, since the original
local model is supervised, they prefer using the class information from logits to distill. When
observing the result of FedET, we find that although it also designs a larger global model in server
which improves the model a lot, the final result is not satisfied. This is also because it designs a
distillation method based on knowledge of probability distribution over classes. Next, compared
with the federated self-supervised method, our FedMKD also achieves better performance. FedU and
FedEMA are not applicable in model heterogeneity setting, so we cannot evaluate their effectiveness.
Hetero-SSFL gain the best performance among all baselines but is worse than ours. That’s because it
aims to train personalised client models. Only the alignment module cannot hold the inconsistent
representation space perfectly. But our global model can directly generate representation from the
global model, it avoids using the representation from inconsistent clients."
PERFORMANCE EVALUATION,0.28063241106719367,"Apart from the client data heterogeneity, we consider the influence of the public dataset distribution.
Here, we construct two public datasets, one is ’IID’ to the whole data distribution and the other only
has partial classes. In both two settings, our FedMKD also gets the best performance. The overall
performance of ’IID’ public dataset is better than ’Partial’ setting. That’s because the global model
adapts the self-supervised learning on public dataset, and the diversity of the sample is important,
which can help the model explore a wide range of features and patterns present in the data. And it’s
observed that when the public dataset is ’IID’, the performance increases with the decrease of the
data heterogeneity level for CIFAR-10, but it doesn’t apply to the CIFAR-100. It’s because there are
too many classes in CIFAR-100 and the number of samples in each class is not efficient."
PERFORMANCE EVALUATION,0.2826086956521739,"In order to evaluate the effectiveness of FedMKD, we use the dimensionality reduction algorithm t-sne
to visualize the representation on the test dataset of CIFAR-10 from different encoders. As shown
in Fig. 3 (b)(c)(d), the global models in FedMKD trained on both ’IID’ and ’Partial’ public datasets
both achieve better clustering results than Standalone training and MOON. These results further
verify our model can gain better generalized representations although the representation spaces of
clients are inconsistent. There’s also an averaged global model in MOON, but it cannot tackle the
problem of inconsistent spaces well using the average method, so it only gains a poor clustering
performance. Additionally, the performance on ’IID’ public dataset is better than ’paritial’ ones from
the observation of cluster performance. This suggests that the number of classes seen by the global
model also affects how well the global model can encode all classes in a unified space. Comparing
the class distributions in Fig. 3 (c) and (d), we can find that although these two global models are
trained on different public datasets, the final cluster layout is similar, which can further prove that our
global model can encode all classes from clients even if it never sees some classes during the training."
PERFORMANCE EVALUATION,0.2845849802371542,Table 4: Experimental results on ablation studies of FedMKD with best model performance in bold.
PERFORMANCE EVALUATION,0.2865612648221344,"Method
CIFAR-10 (%)
CIFAR-100 (%)
Class
IID
Class
IID"
PERFORMANCE EVALUATION,0.2885375494071146,"Standalone training
51.09 ± 0.04
25.35 ± 0.02
FedMKDKL
43.88 ± 0.19
46.24 ± 0.08
19.56 ± 0.18
15.45 ± 0.05
FedMKDw/o adaptive
46.14 ± 0.05
47.29 ± 0.08
21.77 ± 0.02
22.76 ± 0.03
FedMKDw/o alignment
61.85 ± 0.04
62.34 ± 0.09
34.97 ± 0.04
29.91 ± 0.02
FedMKD
64.81 ± 0.02
69.07 ± 0.04
36.33 ± 0.01
35.94 ± 0.02"
IMPROVEMENT OF CLIENTS,0.29051383399209485,"5.4
Improvement of clients"
IMPROVEMENT OF CLIENTS,0.2924901185770751,"Client 1
Client 2
Client 3
Client 4
Client 5 30 40 50 60 70 80"
IMPROVEMENT OF CLIENTS,0.29446640316205536,Testing Acc.(%)
IMPROVEMENT OF CLIENTS,0.2964426877470356,"standalone
FedMKD"
IMPROVEMENT OF CLIENTS,0.2984189723320158,"Figure 4: Improvement of clients after involving
our proposed FedMKD."
IMPROVEMENT OF CLIENTS,0.30039525691699603,"In FedMKD, the global knowledge anchored
alignment module is used to align the client
model in the server which can further transfer
the global knowledge to the client and incen-
tivize the clients to participate in the federated
learning. To evaluate the improvement of the
clients, the local models which are standalone
training locally are compared with our local
models. As shown in Fig. 4, the client perfor-
mance in our FedMKD framework is better than
local standalone training, regardless of the archi-
tecture of the local model. Especially for clients with ResNet18, it improves 6.73% on average. It’s
concluded that clients benefit from federated training by contributing to global training."
VALIDATION OF INCONSISTENT REPRESENTATION SPACES,0.30237154150197626,"5.5
Validation of inconsistent representation spaces"
VALIDATION OF INCONSISTENT REPRESENTATION SPACES,0.30434782608695654,ClientA
VALIDATION OF INCONSISTENT REPRESENTATION SPACES,0.30632411067193677,ClientB
VALIDATION OF INCONSISTENT REPRESENTATION SPACES,0.308300395256917,Global
VALIDATION OF INCONSISTENT REPRESENTATION SPACES,0.3102766798418972,"Figure 5: LDA visualizations of hidden vectors
from different models on CIFAR-10."
VALIDATION OF INCONSISTENT REPRESENTATION SPACES,0.31225296442687744,"As we mentioned, the representation spaces be-
tween different clients are inconsistent because
of data heterogeneity. To validate this opinion,
we use Linear Discriminant Analysis (LDA) to
reduce dimensionality in order to visualize the
distribution of the representations. Here the data
distribution of clients is Class. In Fig. 5 left,
’◦’ and ’×’ denote representations of Client A
and Client B, respectively. And different colors
denote different classes. We can observe that
in Fig. 5 left the classes ’cat’ and ’dog’ almost
overlap while they are from different clients,
which verifies that the inconsistent representa-
tion spaces did exactly exist. And Fig. 5 right
shows the visualization result of the represen-
tation from the global model. It’s clear that the classes ’cat’ and ’dog’ are embedded in different
positions in global space, which proves that although the local representations are inconsistent, our
global model can learn a good representation."
ABLATION EXPERIMENT,0.3142292490118577,"5.6
Ablation experiment"
ABLATION EXPERIMENT,0.31620553359683795,"In order to investigate the effectiveness of different parts of FedMKD, we design these comparison
experiments:"
ABLATION EXPERIMENT,0.3181818181818182,"• Standalone training: The global model is trained alone using the public dataset, without the
knowledge from client models."
ABLATION EXPERIMENT,0.3201581027667984,"• FedMKDKL: The global distillation function is replaced by the KL-divergence function to
measure the similarity of the aggregated representation ¯r and global representation rs."
ABLATION EXPERIMENT,0.3221343873517787,"• FedMKDw/o adaptive: The adaptive knowledge integration module is removed, each local
representation has the same weight to generate the aggregated representation."
ABLATION EXPERIMENT,0.3241106719367589,Table 5: Experimental results on scalability studies of FedMKD.
ABLATION EXPERIMENT,0.32608695652173914,"Method
Client number
5
10
30"
ABLATION EXPERIMENT,0.32806324110671936,"Linear result
67.79 ± 0.04
57.39 ± 0.60
53.85 ± 0.03
Semi result
63.37 ± 0.51
53.30 ± 0.31
48.97 ± 0.86"
ABLATION EXPERIMENT,0.3300395256916996,"• FedMKDw/o alignment: The global knowledge anchored alignment module is removed from
FedMKD."
ABLATION EXPERIMENT,0.33201581027667987,"Above experiment is conducted on the both CIFAR-10 and CIFAR-100 dataset, using the IID public
dataset. The results of the ablation study experiment are shown in Table 4. When comparing FedMKD
with FedMKDKL, significant performance drop can be observed when the knowledge distillation
function is replaced by the KL-divergence function. Thus we can conclude that for self-supervised
learning, we prefer performing knowledge distillation based on representation, but the KL-divergence
cannot capture the distribution characteristics from them. Therefore, the appropriate distillation
method is critically important in self-supervised learning due to the lack of labels. And a worse
performance on both two datasets can be observed when we use the equal weight instead of the
adaptive weight. Because the client models are heterogeneous, their representation capabilities are
different and the representation spaces are also inconsistent, so intuitively average representations
may reduce the information contained in the representation. Finally, when we remove the alignment
module, the performance under each setting all decreases, which demonstrates that the alignment is
not only beneficial to the local models, but also improves the whole training process."
SCALABILITY OF ALGORITHM,0.3339920948616601,"5.7
Scalability of algorithm"
SCALABILITY OF ALGORITHM,0.3359683794466403,"In order to explore the scalability of our proposed algorithm FedMKD, we add the experiment that
the number of clients is 5, 10, 30 on CIFAR-10, and 40% clients use the VGG model and 60% use
ResNet18. And we repartition the data for each client under Dir(β = 0.5) and set the public dataset
distribution as ’IID’. The results are shown in Table 5. We can find that as the number of clients
increasing, the performance decreases. The reason is that the total number of data is fixed, if the
number of clients increases, the number of data in each client will decrease, which further affect the
performance of the local model."
CONCLUSION,0.33794466403162055,"6
Conclusion"
CONCLUSION,0.33992094861660077,"In this work, we focused on how to solve the deviated representation abilities and inconsistent
representation spaces caused by the heterogeneous architectures and class skew in federated self-
supervised learning. We proposed a multi-teacher knowledge based federated self-supervised learning
framework FedMKD to learn a global model. Firstly, the adaptive knowledge integration module
could learn high-quality representation knowledge from heterogeneous models. And the combination
of the self-supervised loss and the distillation loss enabled the global model to encode all classes from
clients in a unified space. Then a global knowledge anchored alignment module improved the local
representation models in server and fed it back to corresponding clients. The experiments conducted
on two datasets demonstrated that our proposed FedMKD was state-of-the-art and outperformed
existing methods."
CONCLUSION,0.34189723320158105,Acknowledgement
CONCLUSION,0.3438735177865613,"This work was supported in part by the National Natural Science Foundation of China under Grant
62202273, 62176014, 92370204, in part by National Science Fund for Excellent Young Scholars of
China under Grant 62122042, in part by Shandong Provincial Natural Science Foundation of China
under Grant ZR2021QF044, in part by the Fundamental Research Funds for the Central Universities,
in part by the Major Basic Research Program of Shandong Provincial Natural Science Foundation
under Grant ZR2022ZD02, in part by the Joint Key Funds of National Natural Science Foundation of
China under Grant U23A20302, in part by Guangzhou-HKUST(GZ) Joint Funding Program under
Grant 2023A03J0008, in part by the Education Bureau of Guangzhou Municipality."
REFERENCES,0.3458498023715415,References
REFERENCES,0.34782608695652173,"[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020."
REFERENCES,0.34980237154150196,"[2] Yae Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, and Dimitrios Dimitriadis. Heterogeneous ensemble
knowledge transfer for training large models in federated learning. arXiv preprint arXiv:2204.12703, 2022."
REFERENCES,0.35177865612648224,"[3] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray
Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-
supervised learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.35375494071146246,"[4] Wei Guo, Fuzhen Zhuang, Xiao Zhang, Yiqi Tong, and Jin Dong. A comprehensive survey of federated
transfer learning: challenges, methods and applications. Frontiers Comput. Sci., 18(6):186356, 2024."
REFERENCES,0.3557312252964427,"[5] Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Chuhan Wu, Xing Xie, and Meeyoung Cha.
Fedx: Unsupervised federated learning with cross knowledge distillation. In European Conference on
Computer Vision, pages 691–707. Springer, 2022."
REFERENCES,0.3577075098814229,"[6] Chaoyang He, Zhengyu Yang, Erum Mushtaq, Sunwoo Lee, Mahdi Soltanolkotabi, and Salman Avestimehr.
Ssfl: Tackling label deficiency in federated learning via personalized self-supervision. arXiv preprint
arXiv:2110.02470, 2021."
REFERENCES,0.35968379446640314,"[7] Jingxuan He, Lechao Cheng, Chaowei Fang, Zunlei Feng, Tingting Mu, and Mingli Song. Progressive
feature self-reinforcement for weakly supervised semantic segmentation. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 38, pages 2085–2093, 2024."
REFERENCES,0.3616600790513834,"[8] Jingxuan He, Lechao Cheng, Chaowei Fang, Dingwen Zhang, Zhangye Wang, and Wei Chen. Mitigating
undisciplined over-smoothing in transformer for weakly supervised semantic segmentation. arXiv preprint
arXiv:2305.03112, 2023."
REFERENCES,0.36363636363636365,"[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016."
REFERENCES,0.36561264822134387,"[10] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribu-
tion for federated visual classification. CoRR, abs/1909.06335, 2019."
REFERENCES,0.3675889328063241,"[11] Yilun Jin, Yang Liu, Kai Chen, and Qiang Yang. Federated learning without full labels: A survey. arXiv
preprint arXiv:2303.14453, 2023."
REFERENCES,0.3695652173913043,"[12] Yilun Jin, Xiguang Wei, Yang Liu, and Qiang Yang. Towards utilizing unlabeled data in federated learning:
A survey and prospective. arXiv preprint arXiv:2002.11545, 2020."
REFERENCES,0.3715415019762846,"[13] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.37351778656126483,"[14] Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv
preprint arXiv:1910.03581, 2019."
REFERENCES,0.37549407114624506,"[15] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 10713–10722, 2021."
REFERENCES,0.3774703557312253,"[16] Simou Li, Yuxing Mao, Jian Li, Yihang Xu, Jinsen Li, Xueshuo Chen, Siyang Liu, and Xianping Zhao.
Fedutn: federated self-supervised learning with updating target network. Applied Intelligence, 53(9):10879–
10892, 2023."
REFERENCES,0.3794466403162055,"[17] Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu, Huabin Zhu, Binhui Yao, Tao
Wang, Xiaolin Zheng, and Yanchao Tan. Rethinking the representation in federated unsupervised learning
with non-iid data. ArXiv, abs/2403.16398, 2024."
REFERENCES,0.3814229249011858,"[18] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model
fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351–2363, 2020."
REFERENCES,0.383399209486166,"[19] Yi Liu, Song Guo, Jie Zhang, Qihua Zhou, Yingchun Wang, and Xiaohan Zhao. Feature correlation-guided
knowledge transfer for federated self-supervised learning. arXiv preprint arXiv:2211.07364, 2022."
REFERENCES,0.38537549407114624,"[20] Disha Makhija, Nhat Ho, and Joydeep Ghosh. Federated self-supervised learning for heterogeneous clients.
arXiv preprint arXiv:2205.12493, 2022."
REFERENCES,0.38735177865612647,"[21] Yasar Abbas Ur Rehman, Yan Gao, Pedro Porto Buarque de Gusmao, Mina Alibeigi, Jiajun Shen, and
Nicholas D Lane. L-dawa: Layer-wise divergence aware weight aggregation in federated self-supervised
visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 16464–16473, 2023."
REFERENCES,0.3893280632411067,"[22] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.391304347826087,"[23] Bram van Berlo, Aaqib Saeed, and Tanir Ozcelebi. Towards federated unsupervised representation learning.
In Proceedings of the third ACM international workshop on edge systems, analytics and networking, pages
31–36, 2020."
REFERENCES,0.3932806324110672,"[24] Kun Wang, Hao Liu, Lirong Jie, Zixu Li, Yupeng Hu, and Liqiang Nie. Explicit granularity and implicit
scale correspondence learning for point-supervised video moment localization. In Proceedings of the 32nd
ACM International Conference on Multimedia, MM ’24, page 9214–9223, New York, NY, USA, 2024.
Association for Computing Machinery."
REFERENCES,0.3952569169960474,"[25] Lirui Wang, Kaiqing Zhang, Yunzhu Li, Yonglong Tian, and Russ Tedrake. Does learning from decentral-
ized non-iid unlabeled data benefit from self supervision? In The Eleventh International Conference on
Learning Representations, 2022."
REFERENCES,0.39723320158102765,"[26] Yangyang Wang, Xiao Zhang, Mingyi Li, Tian Lan, Huashan Chen, Hui Xiong, Xiuzhen Cheng, and
Dongxiao Yu. Theoretical convergence guaranteed resource-adaptive federated learning with mixed
heterogeneity. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, KDD ’23, page 2444–2455, New York, NY, USA, 2023. Association for Computing Machinery."
REFERENCES,0.39920948616600793,"[27] Yuzhu Wang, Lechao Cheng, Manni Duan, Yongheng Wang, Zunlei Feng, and Shu Kong. Improving
knowledge distillation via regularizing feature norm and direction. arXiv preprint arXiv:2305.17007, 2023."
REFERENCES,0.40118577075098816,"[28] Ziming Ye, Xiao Zhang, Xu Chen, Hui Xiong, and Dongxiao Yu. Adaptive clustering based personalized
federated learning framework for next poi recommendation with location noise. IEEE Transactions on
Knowledge and Data Engineering, 36(5):1843–1856, 2024."
REFERENCES,0.4031620553359684,"[29] Chen Zhang, Yu Xie, Tingbin Chen, Wenjie Mao, and Bin Yu. Prototype similarity distillation for
communication-efficient federated unsupervised representation learning. IEEE Transactions on Knowledge
and Data Engineering, pages 1–13, 2024."
REFERENCES,0.4051383399209486,"[30] Fengda Zhang, Kun Kuang, Long Chen, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Fei
Wu, Yueting Zhuang, et al. Federated unsupervised representation learning. Frontiers of Information
Technology & Electronic Engineering, 24(8):1181–1193, 2023."
REFERENCES,0.40711462450592883,"[31] Xiao Zhang, Ziming Ye, Jianfeng Lu, Fuzhen Zhuang, Yanwei Zheng, and Dongxiao Yu. Fine-grained
preference-aware personalized federated poi recommendation with data sparsity. In Proceedings of the
46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR
’23, page 413–422, New York, NY, USA, 2023. Association for Computing Machinery."
REFERENCES,0.4090909090909091,"[32] Weiming Zhuang, Xin Gan, Yonggang Wen, and Shuai Zhang. Easyfl: A low-code federated learning
platform for dummies. IEEE Internet of Things Journal, 2022."
REFERENCES,0.41106719367588934,"[33] Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang, and Shuai Yi. Collaborative unsupervised visual
representation learning from decentralized data. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 4912–4921, 2021."
REFERENCES,0.41304347826086957,"[34] Weiming Zhuang, Yonggang Wen, and Shuai Zhang. Divergence-aware federated self-supervised learning.
In International Conference on Learning Representations, 2022."
REFERENCES,0.4150197628458498,Appendix
REFERENCES,0.41699604743083,We provide more details about our work and experiments in the appendices:
REFERENCES,0.4189723320158103,"• Appendix A: the additional details of the proposed algorithm FedMKD.
• Appendix B: the detailed proof of the convergence analysis of our proposed algorithm
FedMKD.
• Appendix C: the details of experimental settings including datasets and federated simulations,
reproduction details, baselines, evaluation methods and ablation experiment setting.
• Appendix D: additional experimental results including Appendix D.1, the overall perfor-
mance under two evaluation methods; Appendix D.2, the communication and storage
efficiency analysis; Appendix D.3 discusses the impact of hyperparameters.
• Appendix E: the details of limitations and broader impacts of this work."
REFERENCES,0.4209486166007905,"A
Algorithm description"
REFERENCES,0.42292490118577075,"We outline the FedMKD algorithm in Algorithm 1. In round q, the clients and server perform the
following updates:"
REFERENCES,0.424901185770751,"• Starting from the resource-adaptive model θq,n,0, we update the local parameters for t ∈[T]"
REFERENCES,0.4268774703557312,"θq,n,t ←θq,n,t−1 −η∇lq,n,t."
REFERENCES,0.4288537549407115,"ϕq,n,t ←αϕq,n,t−1 + (1 −α)θq,n,t."
REFERENCES,0.4308300395256917,"• After T times local updates, upload the local parameter θq,n,T .
• Server uses all local model to process multi-teacer adaptive knowledge integration distilla-
tion, update the global model for t ∈[T ′],"
REFERENCES,0.43280632411067194,"θq,s ←θq,s −η∇lq,s.
(15)"
REFERENCES,0.43478260869565216,"ϕq,s ←αϕq−1,s + (1 −α)θq,s.
(16)"
REFERENCES,0.4367588932806324,"• Server aligns all client models to the global model, for n ∈N,"
REFERENCES,0.43873517786561267,"θ′
q,n ←θ′
q,n −η∇l′
q,n.
(17)"
REFERENCES,0.4407114624505929,"B
Convergence Analysis"
REFERENCES,0.4426877470355731,"In this section, we show the convergence analysis of our FedMKD. Firstly, we give some commonly
used assumptions in federated learning:"
REFERENCES,0.44466403162055335,"Assumption 1 (Lipschitz Condition). Every function F(·) is with L-Lipschitz gradient: ∀n ∈
[N], θ, φ ∈Rd
∥∇F(θ) −∇F(φ)∥≤L∥θ −φ∥
(18)"
REFERENCES,0.44664031620553357,"Assumption 2 (Bounded variance). The stochastic gradients ∇Fn(θq,n,t; ξn,t) is an unbiased
estimator of the gradient, with the variance bounded by σ > 0:"
REFERENCES,0.44861660079051385,"Eξn,t∼Dn∥∇Fn(θq,n,t; ξn,t) −∇Fn(θq,n,t)∥2 ≤σ2,
∀q, n, t
(19)"
REFERENCES,0.4505928853754941,"In order to analyze the convergence rate of our proposed FedMKD, we firstly state some preliminary
lemmas as follows:"
REFERENCES,0.4525691699604743,"Lemma 1 (Jensen’s inequality). For any convex function h and any variable x1, . . . , xn we have h( 1 n n
X"
REFERENCES,0.45454545454545453,"i=1
xi) ≤1 n n
X"
REFERENCES,0.45652173913043476,"i=1
h(xi).
(20)"
REFERENCES,0.45849802371541504,"Especially, when h(x) = ∥x∥2, we can get ∥1 n n
X"
REFERENCES,0.46047430830039526,"i=1
xi∥2 ≤1 n n
X"
REFERENCES,0.4624505928853755,"i=1
∥xi∥2.
(21)"
REFERENCES,0.4644268774703557,Algorithm 1: Algorithm of FedMKD
REFERENCES,0.466403162055336,"1 Initialize: total number of clients N; Number of rounds T; Local data in clients{D1, . . . , DN};
Public data in server Dpub; Local online network {θ1, . . . , θN}, target network {ϕ1, . . . , ϕN};
Server online network θs, target network ϕs; Learning rate η; Adapter network Attn(·). for
q = 1 to Q do"
REFERENCES,0.4683794466403162,"2
LocalUpdate: for n = 1 to N (all clients in parallel) do"
REFERENCES,0.47035573122529645,"3
if q = 1 then"
REFERENCES,0.4723320158102767,"4
Random initialize θq,n,0."
ELSE,0.4743083003952569,"5
else"
ELSE,0.4762845849802372,"6
{θq,n,0, ϕq,n,0} ←{θ′
q−1,n, ϕq−1,n,T }."
END,0.4782608695652174,"7
end"
END,0.48023715415019763,"8
for epoch t = 1 to T do"
END,0.48221343873517786,"9
lq,n,t = Lself(θq,n,t−1; ϕq,n,t−1; Dn)."
END,0.4841897233201581,"10
θq,n,t ←θq,n,t−1 −η∇lq,n,t."
END,0.48616600790513836,"11
ϕq,n,t ←αϕq,n,t−1 + (1 −α)θq,n,t."
END,0.4881422924901186,"12
end"
END,0.4901185770750988,"13
Upload θq,n,t."
END,0.49209486166007904,"14
end"
END,0.49407114624505927,"15
ServerExecution:"
END,0.49604743083003955,"16
# Multi-teacher adaptive knowledge integration distillation."
END,0.4980237154150198,"17
for epoch t = 1 to T ′ do"
END,0.5,"18
for batch b ∈Dpub do"
END,0.5019762845849802,"19
for n = 1 to N do"
END,0.5039525691699605,"20
rq,n = g(θq,n,t(b))."
END,0.5059288537549407,"21
end"
END,0.5079051383399209,"22
˜R = [rq,1, · · · , rq,N]"
END,0.5098814229249012,"23
rs = g(θq,s(b))."
END,0.5118577075098815,"24
¯r = Attn(rs, ˜R)"
END,0.5138339920948617,"25
lq,s = Lself(θq,s; ϕq,s; b) + γLdistill(θq,s; rs; ¯r)."
END,0.5158102766798419,"26
θq,s ←θq,s −η∇lq,s."
END,0.5177865612648221,"27
ϕq,s ←αϕq−1,s + (1 −α)θq,s."
END,0.5197628458498024,"28
end"
END,0.5217391304347826,"29
end"
END,0.5237154150197628,"30
# Alignment client models in server."
END,0.525691699604743,"31
for n = 1 to N do"
END,0.5276679841897233,"32
{θ′
q,n, ϕ′
q,n} ←{θq,n,T , θq,s}"
END,0.5296442687747036,"33
l′
q,n = Lalign(θ′
q,n; ϕ′
q,n; Dpub)."
END,0.5316205533596838,"34
θ′
q,n ←θ′
q,n −η∇l′
q,n."
END,0.5335968379446641,"35
end"
END,0.5355731225296443,"36
Send θ′
q,n to client n."
END,0.5375494071146245,37 end
END,0.5395256916996047,"Lemma 2 For random variable x1, . . . , xn we have"
END,0.541501976284585,"E[∥x1 + · · · + xn∥2] ≤nE[∥x1∥2 + · · · + ∥xn∥2].
(22)"
END,0.5434782608695652,"Lemma 3 For independent random variables x1, . . . , xn whose mean is 0, we have"
END,0.5454545454545454,"E[∥x1 + · · · + xn∥2] = E[∥x1∥2 + · · · + ∥xn∥2].
(23)"
END,0.5474308300395256,"Based on the above assumptions, we present the theoretical results for the non-convex problem."
END,0.549407114624506,"Lemma 4 (Deviation bound of the optimization function) In each communication round, the function
value in server reduce after T ′ epochs and is bounded as:"
END,0.5513833992094862,"E[F(θq,T ′)] ≤E[F(θq,0)] −(η −Lη2 2 ) T ′
X"
END,0.5533596837944664,"t=1
∥∇F(θq,t)∥2 + LT ′η2"
END,0.5553359683794467,"2
σ2
(24)"
END,0.5573122529644269,"Lemma 4 indicates the deviation bound of the optimization function of the server in each global
round."
END,0.5592885375494071,"Theorem 1 (Non-convex divergence for FedMKD) Let Assumption 1 to 3 hold and ∆= F(θ0) −
F(θT ′), given any δ > 0, suppose that the learning rates satisfy 0 ≤η ≤2/L, after"
END,0.5612648221343873,"Q =
2∆
T ′δ(2η −Lη2) −T ′Lη2σ2
(25)"
END,0.5632411067193676,"communication round, we have"
END,0.5652173913043478,"1
QT ′ Q−1
X q=0"
END,0.567193675889328,"T ′−1
X"
END,0.5691699604743083,"t=0
E[∇Lq,t)] ≤δ
(26)"
END,0.5711462450592886,the convergence can be guaranteed.
END,0.5731225296442688,Proof. Let’s start the proof from L-Lipschitz condition:
END,0.575098814229249,"F(θq,t+1)
(a)
≤F(θq,t) + ⟨∇F(θq,t), θq,t+1 −θq,t⟩+ L"
END,0.5770750988142292,"2 ∥θq,t+1 −θq,t∥2"
END,0.5790513833992095,"= F(θq,t) + ⟨∇F(θq,t), −η∇F(θq,t, ξq,t)⟩+ L"
END,0.5810276679841897,"2 ∥−η∇F(θq,t, ξq,t)∥2"
END,0.5830039525691699,"where (a) is from Assumption 1. Taking expectation of both sides, we obtain"
END,0.5849802371541502,"E[F(θq,t+1)] ≤E[F(θq,t)] −ηE[∥∇F(θq,t)∥2] + Lη2"
END,0.5869565217391305,"2 E[∥F(θq,t, ξq,t)∥2]"
END,0.5889328063241107,"= E[F(θq,t)] −ηE[∥∇F(θq,t)∥2] + Lη2"
END,0.5909090909090909,"2 E[∥F(θq,t∥2 + σ2]"
END,0.5928853754940712,"(b)
≤E[F(θq,t)] −(η −Lη2"
END,0.5948616600790514,"2 )E[∥∇F(θq,t)∥2] + Lη2 2 σ2,"
END,0.5968379446640316,"where (b) follows from Assumption 2. Let’s set the learning step at the start of training to T ′,"
END,0.5988142292490118,"E[F(θq,T ′)] ≤E[F(θq,0)] −(η −Lη2 2 ) T ′
X"
END,0.6007905138339921,"t=1
∥∇F(θq,t)∥2 + LT ′η2"
END,0.6027667984189723,"2
σ2
(27)"
END,0.6047430830039525,"Proof of Theorem 1. According to Lemma 4,"
END,0.6067193675889329,"1
QT ′ (η −η2L 2 ) Q−1
X q=0"
END,0.6086956521739131,"T ′−1
X"
END,0.6106719367588933,"t=0
E[∇Lq,t)] ≤
1
QT ′ Q−1
X"
END,0.6126482213438735,"q=0
E[F(θq,T ′)] −
1
QT ′ Q−1
X"
END,0.6146245059288538,"q=0
E[F(θq,0)] + LT ′η2 2
σ2"
END,0.616600790513834,≤δ(η −η2L 2 )
END,0.6185770750988142,"Therefore,"
END,0.6205533596837944,"∆
Q ≤δ(η −η2L"
END,0.6225296442687747,"2 ) −LT ′η2 2
σ2,"
END,0.6245059288537549,which is equal to
END,0.6264822134387352,"Q =
2∆
T ′δ(2η −Lη2) −T ′(Lη2σ2)."
END,0.6284584980237155,"C
Experiment supplements"
END,0.6304347826086957,"C.1
Datasets and federated simulations"
END,0.6324110671936759,"We use CIFAR-10 and CIFAR-100 [13] datasets to train all the models. Both of them contain 50,000
training images and 10,000 testing images. To construct the public dataset, we sample 4000 data"
END,0.6343873517786561,airplane
END,0.6363636363636364,automobile bird cat deer dog frog horse ship truck
END,0.6383399209486166,Classes 1 2 3 4 5
END,0.6403162055335968,Client ID
END,0.642292490118577,"Client 1
Client 2
Client 3
Client 4
Client 5"
END,0.6442687747035574,(a) Class
END,0.6462450592885376,airplane
END,0.6482213438735178,automobile bird cat deer dog frog horse ship truck
END,0.650197628458498,Classes 1 2 3 4 5
END,0.6521739130434783,Client ID
END,0.6541501976284585,(b) Dir(β = 0.1)
END,0.6561264822134387,airplane
END,0.658102766798419,automobile bird cat deer dog frog horse ship truck
END,0.6600790513833992,Classes 1 2 3 4 5
END,0.6620553359683794,Client ID
END,0.6640316205533597,(c) Dir(β = 0.5)
END,0.66600790513834,airplane
END,0.6679841897233202,automobile bird cat deer dog frog horse ship truck
END,0.6699604743083004,Classes 1 2 3 4 5
END,0.6719367588932806,Client ID
END,0.6739130434782609,(d) IID
END,0.6758893280632411,"Figure 6: Illustrations of # of samples per class allocated to each client, for different distributions."
END,0.6778656126482213,"samples from the training set, then divide the remaining data into N partitions to simulate N clients.
To assess the validity of the public dataset, we use two sampling methods to construct it. First, we
use a random sampling method over all classes to generate public dataset ’IID’. And for the public
dataset ’Partial’, data is selected randomly from 40% classes in two datasets."
END,0.6798418972332015,"We utilise three settings to simulate heterogeneous data distributions among all the clients. For the
IID setting, each client contains the same number of samples from all classes. For the class setting,
each client only has 10/N and 100/N classes on two datasets and the classes between clients have no
overlap. For the Non-IID setting, data heterogeneity levels are described by the Dirichlet distribution
Dir(β) [10], where smaller β represents stronger heterogeneity levels. A value of β = 0.1 is chosen
to simulate a high degree of heterogeneity, and β = 0.5 for a lower level. Fig. 6 shows the data
distribution among clients on CIFAR-10 dataset. The x-axis represents 10 classes and the y-axis is
the total of 5 clients. The size of each circle denotes the number of samples for the specific class in
the respective client."
END,0.6818181818181818,"C.2
Reproduction details"
END,0.6837944664031621,"Regarding the self-supervised learning framework design within each client, we use ResNet18 [9]
and VGG9 [22] as the encoder network and Multi-Layer Perception (MLP) as the predictor. To
compare with other baselines, we set the number of clients N = 5, and conduct the experiments for
R = 100 rounds. In order to construct the model heterogeneous setting, 2 clients train the Resnet18
encoder while 3 clients use the VGG9. And for the global representation model, Resnet18 is selected
as the encoder in server. The hyper-parameter γ in the loss of global model is set to 0.9. During the
training process, each client trains locally for T = 5 epochs while the server also distills for T ′ = 5
epochs. Finally, we set the target decay rate α = 0.99, with a batch size of B = 128, and utilize
SGD for optimization with a learning rate of η = 0.032. We implement all the methods in Python
using EasyFL[32] based on PyTorch."
END,0.6857707509881423,"C.3
Baselines"
END,0.6877470355731226,"Firstly, we select several federated knowledge distillation frameworks that use unlabeled public
dataset for distillation. We then replaced the local model with a self-supervised model to evaluate the
process of knowledge distillation in our method."
END,0.6897233201581028,"• FedMD[14]: Each client trains convergence on the public dataset and then on local data.
In each round, clients upload the embedding of the public dataset to the server. The server
averages the embeddings and sends averaged embedding to clients. Clients first use average
embedding to update local models on public dataset, then train on local data for a few epochs.
There’s no global model, the local models are evaluated to make comparison."
END,0.691699604743083,"• FedDF[18]: Clients in FedDF train locally and upload the model parameter to server. The
server uses all client models to compute the embedding of the public dataset and uses the
average embedding to train the global model. But it has no global model, so we only test the
performance of local models."
END,0.6936758893280632,"• FedET[2]: There is a global large model and several alternative small models on the server.
Each client selects the appropriate small model for training locally and then uploads the
parameter to the server. The server uses all the small models to compute representations of"
END,0.6956521739130435,"the public dataset and updates the global model with the averaged representation. Then the
server utilizes the representation from the global model to update all the small models.
• MOON[15]: Moon introduces a model-contrastive learning approach, treating the represen-
tation from the global model as positive knowledge to update the local models.
• MOON-KL: Instead of using the NT-Xent loss in MOON, MOON-KL utilises the KL-
divergence function to measure the similarity between global representation and local
representation. To make comparisons, both MOON and MOON-KL distribute the global
model to each client, and then the client uses the local model to update the global model.
The server aggregates the updated global models as the final global model."
END,0.6976284584980237,And several federated self-supervised learning frameworks are also chosen as baselines.
END,0.6996047430830039,"• FedU[33]: Clients in FedU upload the local online encoder and predictor for server aggrega-
tion. Then they update the local online encoder using the aggregate ones, and dynamically
update the predictor based on the divergence. But it cannot applied to the model heterogene-
ity setting.
• FedEMA[34]: FedEMA uses the aggregated local online network as a global model and
measures the divergence between global and local networks. The divergence is then applied
to the local exponential moving average update. It’s also not applicable when models are
heterogeneous.
• Hetero-SSFL[20]: Each client trains locally and uploads the kernel matrix over the public
dataset. The server aggregates them and sends to clients. Then client uses linear-CKA
normalization term to align to the averaged kernel matrix. There’s no global model, so we
just test local models’ performance."
END,0.7015810276679841,"To verify that the client’s knowledge can improve the global model, we also train the global model
separately on the public dataset, denoted as Std. ResNet18."
END,0.7035573122529645,"C.4
Evaluation methods"
END,0.7055335968379447,"Following [33] and [30], we evaluate the performance of learned representations using linear and
semi-supervised evaluation. For linear evaluation, all the models are trained without any supervised
labels. Subsequently, the model encoder is then frozen and the representations are utilized for training
a new classifier over 200 epochs. For semi-supervised evaluation, we consider the scenario that only
a small subset of the data has a label, here only 1% data are labeled. Then, different from freezing the
encoder, we fine-tune the whole model with a new classifier using the labeled data for 100 epochs.
For FedMD, FedDF and Hetero-SSFL, only local models can be evaluated. So we use the averaged
representation from local models to do the linear evaluation. But in semi-supervised evaluation,
we need to fine-tune the whole model, so we evaluate each client’s performance alone and use the
average result as the final result."
END,0.7075098814229249,"D
Additional Experiments"
END,0.7094861660079052,"D.1
Numerical Results"
END,0.7114624505928854,"Table 6, Table 7 and Table 8, Table 9 shows the linear evaluation results and semi-supervised evalua-
tion results of FedMKD compared with all the baselines on CIFAR-10 and CIFAR-100. Comparing
to the result in main text, we extra add the result on Dir(β = 0.1) to prove the efficiency of our
algorithm totally."
END,0.7134387351778656,"D.2
Communication and storage efficiency"
END,0.7154150197628458,"Considering the experimental setting, the communication and storage capabilities are limited, so we
analyze the communication and computation efficiency of FedMKD compared to Hetero-SSFL and
all the experiments maintain the original settings. Firstly, we calculate that the memory of ResNet18
and VGG9 is 42.63MB and 16.38MB separately. We use S(θ) to denote the storage cost of model θ."
END,0.717391304347826,"About the communication cost, the client in FedMKD only needs to upload the online encoder to the
server, and download the updated encoder, so the cost per communication is equal to the ResNet18 or"
END,0.7193675889328063,"Table 6: Top-1 accuracy comparison under linear probing on CIFAR-10 datasets with best model
performance in bold and second-best results with underlines. ’-’ means this method is not suitable for
the experiment setting."
END,0.7213438735177866,"Method
Public
dataset
CIFAR-10 (%)
Class
Dir(β=0.1)
Dir(β=0.5)
IID
Std. ResNet18 IID"
END,0.7233201581027668,"51.09 ± 0.04
FedMD
45.28 ± 0.02
45.48 ± 0.01
45.93 ± 0.02
46.21 ± 0.02
FedDF
46.94 ± 0.04
47.05 ± 0.06
48.04 ± 0.02
48.74 ± 0.08
MOON-KL
44.93 ± 0.05
45.59 ± 0.02
45.84 ± 0.03
46.51 ± 0.03
MOON
53.35 ± 0.03
53.52 ± 0.03
53.71 ± 0.04
55.14 ± 0.02
FedET
56.42 ± 0.02
57.48 ± 0.01
59.38 ± 0.03
61.43 ± 0.02
FedU/FedEMA
-
-
-
-
Hetero-SSFL
59.13 ± 0.02
62.62 ± 0.14
64.04 ± 0.04
65.61 ± 0.07
FedMKD
64.81 ± 0.02
65.96 ± 0.03
66.98 ± 0.06
69.07 ± 0.04
Std. ResNet18"
END,0.7252964426877471,Partial
END,0.7272727272727273,"50.15 ± 0.02
FedMD
47.16 ± 0.03
46.87 ± 0.04
46.39 ± 0.02
45.93 ± 0.03
FedDF
52.59 ± 0.05
52.71 ± 0.02
53.50 ± 0.03
54.17 ± 0.05
MOON-KL
46.41 ± 0.03
47.03 ± 0.02
46.81 ± 0.03
45.89 ± 0.01
MOON
54.31 ± 0.04
54.59 ± 0.02
54.54 ± 0.02
52.94 ± 0.04
FedET
57.75 ± 0.01
56.70 ± 0.02
57.08 ± 0.01
58.59 ± 0.01
FedU/FedEMA
-
-
-
-
Hetero-SSFL
63.20 ± 0.08
63.61 ± 0.02
61.93 ± 0.04
61.15 ± 0.07
FedMKD
66.39 ± 0.09
68.35 ± 0.05
67.60 ± 0.04
65.88 ± 0.03"
END,0.7292490118577075,"Table 7: Top-1 accuracy comparison under linear probing on CIFAR-100 dataset with best model
performance in bold and second-best results with underlines. ’-’ means this method is not suitable for
the experiment setting."
END,0.7312252964426877,"Method
Public
dataset
CIFAR-100 (%)
Class
Dir(β=0.1)
Dir(β=0.5)
IID
Std. ResNet18 IID"
END,0.733201581027668,"25.35 ± 0.02
FedMD
23.25 ± 0.05
23.08 ± 0.02
22.46 ± 0.04
23.20 ± 0.03
FedDF
23.07 ± 0.03
22.44 ± 0.06
22.73 ± 0.03
21.57 ± 0.01
MOON-KL
21.26 ± 0.03
20.81 ± 0.04
21.34 ± 0.01
21.82 ± 0.02
MOON
27.82 ± 0.01
27.63 ± 0.01
26.84 ± 0.03
26.70 ± 0.03
FedET
29.11 ± 0.03
28.55 ± 0.02
26.98 ± 0.01
24.48 ± 0.02
FedU/FedEMA
-
-
-
-
Hetero-SSFL
30.84 ± 0.10
29.74 ± 0.03
29.63 ± 0.06
28.89 ± 0.06
FedMKD
36.33 ± 0.01
34.37 ± 0.05
35.59 ± 0.07
35.94 ± 0.02
Std. ResNet18"
END,0.7351778656126482,Partial
END,0.7371541501976284,"24.97 ± 0.01
FedMD
23.95 ± 0.05
23.28 ± 0.02
23.14 ± 0.03
22.47 ± 0.01
FedDF
27.21 ± 0.02
27.83 ± 0.02
27.31 ± 0.04
27.05 ± 0.04
MOON-KL
21.73 ± 0.01
22.13 ± 0.02
20.97 ± 0.03
22.27 ± 0.04
MOON
27.00 ± 0.04
27.05 ± 0.06
27.27 ± 0.01
28.26 ± 0.02
FedET
29.38 ± 0.01
29.56 ± 0.02
28.12 ± 0.02
29.61 ± 0.01
FedU/FedEMA
-
-
-
-
Hetero-SSFL
30.94 ± 0.05
30.78 ± 0.07
29.92 ± 0.03
29.56 ± 0.03
FedMKD
35.82 ± 0.02
34.70 ± 0.02
35.55 ± 0.05
34.38 ± 0.02"
END,0.7391304347826086,"VGG9. But for Hetero-SSFL, each client needs to upload the kernel metric K ∈RL×L, L = |Dpub|
is the size of the public dataset. In order to make comparison, we fix the communication rounds.
Compared to the Hetero-SSFL, when S(θn) ∗2 ≤|Dpub|2, our FedMKD is better in communication
cost. And compared to the MOON, for larger server model, S(θn) ∗2 ≤S(θs) ∗2, MOON will cost
more when the server model is larger."
END,0.741106719367589,"About local storage for each client, we compute the summary of the model and data. For FedMKD
and Hetero-SSFL, each client runs the siamese network, so the model storage is double of the model
memory, that is 2 ∗S(θn). But for the MOON, each client processes the local contrastive learning
and the model contrastive learning, so they must store two local models and one global model, that is
2 ∗S(θn) + S(θs). Here, we neglect the storage of the predictor which is usually a 2-layer MLP. We
use S(|D|) to denote the storage of the dataset. And each client in FedMKD only needs to store the"
END,0.7430830039525692,"Table 8: Top-1 accuracy comparison on 1% of labeled data for semi-supervised learning on CIFAR-10
dataset with best model performance in bold and second-best results with underlines. ’-’ means this
method doesn’t apply for the experiment setting."
END,0.7450592885375494,"Method
Public
dataset
CIFAR-10 (%)
Class
Dir(β=0.1)
Dir(β=0.5)
IID
Std. ResNet18 IID"
END,0.7470355731225297,46.84 ± 0.25
END,0.7490118577075099,"FedMD
43.32 ± 0.22
43.83 ± 0.25
44.20 ± 0.18
44.66 ± 0.20
FedDF
43.60 ± 0.44
43.73 ± 0.36
44.13 ± 0.16
44.80 ± 0.40
MOON-KL
45.42 ± 0.26
46.18 ± 0.38
46.61 ± 0.21
46.72 ± 0.15
MOON
49.96 ± 0.24
49.90 ± 0.32
50.21 ± 0.10
51.78 ± 0.28
FedET
52.37 ± 0.24
54.29 ± 0.25
56.57 ± 0.17
57.44 ± 0.13
FedU/FedEMA
-
-
-
-
Hetero-SSFL
54.30 ± 0.15
56.19 ± 0.21
58.73 ± 0.54
60.50 ± 0.12
FedMKD
59.65 ± 0.28
60.71 ± 0.05
61.78 ± 0.40
64.06 ± 0.32
Std. ResNet18"
END,0.7509881422924901,Partial
END,0.7529644268774703,"46.42 ± 0.12
FedMD
44.54 ± 0.26
44.01 ± 0.19
43.61 ± 0.13
42.52 ± 0.19
FedDF
48.14 ± 0.27
47.95 ± 0.17
48.74 ± 0.12
48.56 ± 0.18
MOON-KL
46.76 ± 0.05
47.52 ± 0.20
46.92 ± 0.05
46.49 ± 0.22
MOON
50.43 ± 0.18
51.06 ± 0.20
51.99 ± 0.34
49.86 ± 0.19
FedET
52.75 ± 0.07
52.12 ± 0.10
52.61 ± 0.03
54.64 ± 0.14
FedU/FedEMA
-
-
-
-
-
-
-
-
Hetero-SSFL
59.95 ± 0.34
59.34 ± 0.08
58.31 ± 0.50
58.35 ± 0.20
FedMKD
61.55 ± 0.19
62.99 ± 0.17
63.10 ± 0.28
61.08 ± 0.19"
END,0.7549407114624506,"Table 9: Top-1 accuracy comparison on 1% of labeled data for semi-supervised learning on CIFAR-
100 dataset with best model performance in bold and second-best results with underlines. ’-’ means
this method doesn’t apply for the experiment setting."
END,0.7569169960474308,"Method
Public
dataset
CIFAR-100 (%)
Class
Dir(β=0.1)
Dir(β=0.5)
IID
Std. ResNet18 IID"
END,0.758893280632411,"15.19 ± 0.20
FedMD
15.88 ± 0.17
15.01 ± 0.14
14.94 ± 0.19
15.34 ± 0.12
FedDF
14.39 ± 0.20
14.22 ± 0.03
13.06 ± 0.14
12.90 ± 0.07
MOON-KL
16.25 ± 0.06
16.48 ± 0.13
17.22 ± 0.25
16.07 ± 0.04
MOON
19.23 ± 0.12
18.74 ± 0.13
17.21 ± 0.13
17.07 ± 0.18
FedET
19.70 ± 0.08
17.53 ± 0.09
16.82 ± 0.20
15.68 ± 0.18
FedU/FedEMA
-
-
-
-
Hetero-SSFL
20.04 ± 0.40
19.05 ± 0.09
19.19 ± 0.17
18.82 ± 0.17
FedMKD
22.57 ± 0.12
21.28 ± 0.24
22.13 ± 0.11
22.07 ± 0.15
Std. ResNet18"
END,0.7608695652173914,Partial
END,0.7628458498023716,"14.12 ± 0.09
FedMD
17.32 ± 0.17
16.91 ± 0.11
16.47 ± 0.14
16.31 ± 0.07
FedDF
17.01 ± 0.04
17.60 ± 0.10
17.14 ± 0.01
16.95 ± 0.01
MOON-KL
16.21 ± 0.27
16.87 ± 0.11
16.10 ± 0.12
16.94 ± 0.12
MOON
18.64 ± 0.21
19.39 ± 0.28
18.92 ± 0.25
19.29 ± 0.08
FedET
18.49 ± 0.12
19.71 ± 0.16
18.16 ± 0.13
18.01 ± 0.22
FedU/FedEMA
-
-
-
-
Hetero-SSFL
20.72 ± 0.14
20.83 ± 0.23
20.30 ± 0.32
19.62 ± 0.08
FedMKD
22.44 ± 0.19
21.45 ± 0.16
22.21 ± 0.17
20.94 ± 0.25"
END,0.7648221343873518,Table 10: Communication and storage cost comparison of FedMKD and several baselines.
END,0.766798418972332,"Methods
Communication cost
Storage cost
Resnet18
VGG9
Resnet18
VGG9"
END,0.7687747035573123,"FedMKD
85.26MB
32.76MB
193.07MB
140.57MB
MOON
85.25MB
235.70MB
183.20MB
Hetero-SSFL
122.08MB
239.95MB
187.45MB"
END,0.7707509881422925,"1000
2000
3000
4000
5000
Public Dataset Size 40 50 60 70"
END,0.7727272727272727,Testing Acc.(%)
END,0.7747035573122529,"IID
Class"
END,0.7766798418972332,"(a) The performance of FedMKD
with different public dataset size
on Cifar-10."
END,0.7786561264822134,"0
50
100
150
200
Rounds 40 50 60 70 80"
END,0.7806324110671937,Testing Acc.(%)
END,0.782608695652174,"IID
CLASS"
END,0.7845849802371542,"(b) The performance of FedMKD
with different global rounds on
Cifar-10."
END,0.7865612648221344,"1
3
5
7
10
Server Epochs 40 50 60 70"
END,0.7885375494071146,Testing Acc.(%)
END,0.7905138339920948,"IID
Class"
END,0.7924901185770751,"(c) The performance of FedMKD
with different server epochs on
Cifar-10."
END,0.7944664031620553,"1000
2000
3000
4000
5000
Public Dataset Size 20 30 40"
END,0.7964426877470355,Testing Acc.(%)
END,0.7984189723320159,"IID
Class"
END,0.8003952569169961,"(d) The performance of FedMKD
with different public dataset size
on Cifar-100."
END,0.8023715415019763,"0
50
100
150
200
Rounds 20 30 40 50"
END,0.8043478260869565,Testing Acc.(%)
END,0.8063241106719368,"IID
CLASS"
END,0.808300395256917,"(e) The performance of FedMKD
with different global rounds on
Cifar-100."
END,0.8102766798418972,"1
3
5
7
10
Server Epochs 20 30 40"
END,0.8122529644268774,Testing Acc.(%)
END,0.8142292490118577,"IID
Class"
END,0.8162055335968379,"(f) The performance of FedMKD
with different server epochs on
Cifar-100."
END,0.8181818181818182,Figure 7: The top-1 test accuracy of different hyperparameter settings on CIFAR-10 and CIFAR100
END,0.8201581027667985,"local training data without the public data, that is S(|Dn|) and for Hetero-SSFL client needs to store
both two S(|Dn|) + S(|Dpub|). MOON doesn’t use the public dataset, so the client only needs to
store the local data S(|Dn|)also. Totally, we can get"
END,0.8221343873517787,"2 ∗S(θn) + S(|D|) ≤2 ∗S(θn) + S(|Dn|) + S(|Dpub|),
2 ∗S(θn) + S(|D|) ≤2 ∗S(θn) + S(θs) + S(|D|)."
END,0.8241106719367589,"D.3
Hyperparameter analysis"
END,0.8260869565217391,"To explore the influence of different hyperparameters, we conduct experiments on several key
parameters of FedMKD."
END,0.8280632411067194,"Impact of public dataset size. As mentioned in [20], the size and construct method of the public
dataset are both critically important, especially since our global model needs training based on it.
About the latter one we’ve given an analysis in the last section, so we explore the influence of the size
on the model performance. Considering the construct method of the public dataset, we choose part of
the dataset as a public dataset and set it aside to be used. The performance change with respect to its
size is shown in Fig. 7 (a)(d). We can observe that in both two datasets, when the public dataset is
small, the performance of FedMKD is quite worse, and as the size of the public dataset increases, the
performance gets better. But when the size is larger than 4000, the gain of performance is small, and
because of the design of the model, the time cost of the server computation is larger, so we choose
4000 as our experiment setting."
END,0.8300395256916996,"Impact of the number of global rounds. In order to investigate the impact of the training rounds to
FedMKD, we fix the other hyperparameters and train the model for 200 rounds. The results showed
in Fig. 7 (b)(e) demonstrate that while the total training rounds increase, the performance of the
FedMKD gets better. For both two kinds clients data distribution in CIFAR-10, the global model
converges at nearly 150 rounds. For CIFAR-100, the global model converges at 200 rounds. There’s
no denying that the increase of the global rounds will result in better performance, especially when
the global round is small, the improvement of the performance is significant."
END,0.8320158102766798,"Impact of the number of server epochs. Since our global model is trained on the public dataset,
through standalone training and knowledge distillation. The lower server epoch means the global
model may under-fitting not only the distribution of the public dataset but also the client knowledge.
On the other hand, large server epoch will lead large computation cost, and over-fit the distribution
of the public dataset, which is harmful to the generalization of the model, so the epoch in server is
important. We choose 5 different settings T ′ = {1, 3, 5, 7, 10} to validate the influence of the server
epochs. Based on the experiment results shown in Fig. 7(c)(f), we can see that the performance of the
global model improves when the server epoch increases with the distribution of the public is IID. But
the change of improvement gets smaller when the server epoch is larger than 5."
END,0.83399209486166,"E
Additional discussion"
END,0.8359683794466403,"In this section, we discuss the limitations and broader impacts of the work."
END,0.8379446640316206,"Limitations. Although we provide detailed explanations of the proposed algorithm and extensive
experiments analysis, the theoretical proof of FedMKD is not rigorous enough. The characteristic of
multi-teacher distillation of the global model has not been sufficiently theoretically justified and we
only combined it with the self-training loss as one whole loss to analyze."
END,0.8399209486166008,"Broader Impacts. FedMKD offers significant societal and technological benefits, which is crucial
in kinds of domains like healthcare and finance. It promotes inclusivity by leveraging diverse data
from various sources, thereby reducing biases and improving model generalization. Technologically,
FedMKD lowers the dependency on labeled data, making federated self-supervised learning more
efficient and scalable, and drives innovation in heterogeneous resource-limited devices. By carefully
navigating these challenges: deviated representation abilities and inconsistent representation spaces,
FedMKD can lead to responsible and equitable advancements in distributed AI technology."
END,0.841897233201581,NeurIPS Paper Checklist
CLAIMS,0.8438735177865613,1. Claims
CLAIMS,0.8458498023715415,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8478260869565217,Answer: [Yes]
CLAIMS,0.849802371541502,"Justification: The abstract and introduction sections of our paper clearly articulate the main
claims and contributions of our research. These include explicit statements of the proposed
framework innovations, theoretical advancements, and empirical findings is summarized in
lines 62-73. Furthermore, the theoretical arguments in Sec.B and the experimental evidence
in Sec.5 of the main body of the paper fully support the claims made in this paper."
CLAIMS,0.8517786561264822,Guidelines:
CLAIMS,0.8537549407114624,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8557312252964426,2. Limitations
LIMITATIONS,0.857707509881423,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8596837944664032,Answer: [Yes]
LIMITATIONS,0.8616600790513834,"Justification: In this work, we are the first to propose a multi-teacher knowledge distillation
framework, namely FedMKD, to learn global representations with whole class knowledge
from heterogeneous clients even under extreme class skew. However, the paper still exists
some limitations about the privacy preservation. We discuss the limitations of this work in
Appendix E."
LIMITATIONS,0.8636363636363636,Guidelines:
LIMITATIONS,0.8656126482213439,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best"
LIMITATIONS,0.8675889328063241,"judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8695652173913043,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8715415019762845,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8735177865612648,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8754940711462451,"Justification: In this work, we provide the full set of theoretical assumptions and the complete
process process of the proposed multi-teacher knowledge distillation framework FedMKD
in the Appendix.B."
THEORY ASSUMPTIONS AND PROOFS,0.8774703557312253,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8794466403162056,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8814229249011858,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.883399209486166,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8853754940711462,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8873517786561265,"Justification: We propose a multi-teacher knowledge distillation framework FedMKD and de-
scribe this framework in detail in Sec.4. Furthermore, this paper disclose all the information
needed to reproduce the main experimental results of it in Sec.5 and Appendix. C."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8893280632411067,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8913043478260869,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8932806324110671,"(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8952569169960475,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8972332015810277,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8992094861660079,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9011857707509882,"Justification: In this work, we use the public dataset CIFAR-10 and CIFAR-100 as the
datasets. And the code of our work is available at https://github.com/limee-sdu/
FedMKD, as stated in Sec.5."
OPEN ACCESS TO DATA AND CODE,0.9031620553359684,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9051383399209486,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9071146245059288,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9090909090909091,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9110671936758893,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9130434782608695,Justification: We provide the training and test details in Sec.5 and Appendix. C.
OPEN ACCESS TO DATA AND CODE,0.9150197628458498,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9169960474308301,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9189723320158103,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9209486166007905,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9229249011857708,"Answer: [Yes]
Justification: We provide the error bars of the experiments in Sec. 5 and Appendix. D.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.924901185770751,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9268774703557312,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: In this work, we use the NVIDIA GeForce RTX 3090 cards with 24GB
memory as the server and the clients.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9288537549407114,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9308300395256917,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We have reviewed the NeurIPS Code of Ethics. And we ensure that our work
does not involve any inference of personal information and that it will only be used for
academic purposes.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.932806324110672,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9347826086956522,10. Broader Impacts
BROADER IMPACTS,0.9367588932806324,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9387351778656127,Answer: [Yes]
BROADER IMPACTS,0.9407114624505929,"Justification: We discuss the broader impacts of this work in the last paragraph of Ap-
pendix.E."
BROADER IMPACTS,0.9426877470355731,Guidelines:
BROADER IMPACTS,0.9446640316205533,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9466403162055336,11. Safeguards
SAFEGUARDS,0.9486166007905138,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.950592885375494,Answer: [NA]
SAFEGUARDS,0.9525691699604744,Justification: The paper poses no such risks.
SAFEGUARDS,0.9545454545454546,Guidelines:
SAFEGUARDS,0.9565217391304348,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.958498023715415,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9604743083003953,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9624505928853755,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9644268774703557,"Justification: In this work, we introduce the public datasets CIFAR-10 and CIFAR-100 and
the ResNet18 [9] and VGG9 [22] as the backbones in the Sec.5.1. And we properly cite the
related work in the main body and the appendix of this paper."
LICENSES FOR EXISTING ASSETS,0.9664031620553359,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9683794466403162,"• The answer NA means that the paper does not use existing assets.
• The authors should citep the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9703557312252964,13. New Assets
NEW ASSETS,0.9723320158102767,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.974308300395257,Answer: [NA]
NEW ASSETS,0.9762845849802372,Justification: The paper does not release new assets.
NEW ASSETS,0.9782608695652174,Guidelines:
NEW ASSETS,0.9802371541501976,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822134387351779,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841897233201581,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861660079051383,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881422924901185,Justification: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901185770750988,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920948616600791,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940711462450593,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960474308300395,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980237154150198,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
