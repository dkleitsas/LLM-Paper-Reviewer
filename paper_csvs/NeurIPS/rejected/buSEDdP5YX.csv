Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010604453870625664,"We consider the problem of computing tight privacy guarantees for the composition
1"
ABSTRACT,0.0021208907741251328,"of subsampled differentially private mechanisms. Recent algorithms can numeri-
2"
ABSTRACT,0.003181336161187699,"cally compute the privacy parameters to arbitrary precision but must be carefully
3"
ABSTRACT,0.0042417815482502655,"applied.
4"
ABSTRACT,0.005302226935312832,"Our main contribution is to address two common points of confusion. First, some
5"
ABSTRACT,0.006362672322375398,"privacy accountants assume that the privacy guarantees for the composition of a
6"
ABSTRACT,0.007423117709437964,"subsampled mechanism are determined by self-composing the worst-case datasets
7"
ABSTRACT,0.008483563096500531,"for the uncomposed mechanism. We show that this is not true in general. Second,
8"
ABSTRACT,0.009544008483563097,"Poisson subsampling is sometimes assumed to have similar privacy guarantees
9"
ABSTRACT,0.010604453870625663,"compared to sampling without replacement. We show that the privacy guarantees
10"
ABSTRACT,0.01166489925768823,"may in fact differ significantly between the two sampling schemes. In particular, we
11"
ABSTRACT,0.012725344644750796,"give an example of hyperparameters that result in ε ≈1 for Poisson subsampling
12"
ABSTRACT,0.013785790031813362,"and ε > 10 for sampling without replacement. This occurs for some parameters
13"
ABSTRACT,0.014846235418875928,"that could realistically be chosen for DP-SGD.
14"
INTRODUCTION,0.015906680805938492,"1
Introduction
15"
INTRODUCTION,0.016967126193001062,"A fundamental property of differential privacy is that the composition of multiple differentially
16"
INTRODUCTION,0.018027571580063628,"private mechanisms still satisfies differential privacy. This property allows us to design complicated
17"
INTRODUCTION,0.019088016967126194,"mechanisms with strong formal privacy guarantees such as differentially private stochastic gradient
18"
INTRODUCTION,0.02014846235418876,"descent (DP-SGD, [SCS13, BST14, ACG+16]).
19"
INTRODUCTION,0.021208907741251327,"The privacy guarantees of a mechanism inevitably deteriorate with the number of compositions.
20"
INTRODUCTION,0.022269353128313893,"Accurately quantifying the privacy parameters under composition is highly non-trivial and is an
21"
INTRODUCTION,0.02332979851537646,"important area within the field of differential privacy. A common approach is to find the privacy
22"
INTRODUCTION,0.024390243902439025,"parameters for each part of a mechanism and apply a composition theorem [DRV10, KOV15] to find
23"
INTRODUCTION,0.02545068928950159,"the privacy parameters of the full mechanism. In recent years, several alternatives to the traditional
24"
INTRODUCTION,0.026511134676564158,"definition of differential privacy with cleaner results for composition have gained popularity (see,
25"
INTRODUCTION,0.027571580063626724,"e.g., [DR16, BS16, Mir17, DRS19]).
26"
INTRODUCTION,0.02863202545068929,"Another important concept is privacy amplification by subsampling (see, e.g., [BBG18, Ste22]). The
27"
INTRODUCTION,0.029692470837751856,"general idea is to improve privacy guarantees by only using a randomly sampled subset of the full
28"
INTRODUCTION,0.030752916224814422,"dataset as input to a mechanism. In this work we consider the problem of computing tight privacy
29"
INTRODUCTION,0.031813361611876985,"parameters for subsampled mechanisms under composition.
30"
INTRODUCTION,0.032873806998939555,"One of the primary motivations for studying privacy accounting of subsampled mechanisms is DP-
31"
INTRODUCTION,0.033934252386002124,"SGD. DP-SGD achieves privacy by clipping gradients and adding Gaussian noise to each batch.
32"
INTRODUCTION,0.03499469777306469,"As such, we can find the privacy parameters by analyzing the subsampled Gaussian mechanism
33"
INTRODUCTION,0.036055143160127257,"under composition. One of the key contributions of [ACG+16] was the moments accountant,
34"
INTRODUCTION,0.03711558854718982,"which gives tighter bounds for the mechanism than the generic composition theorems. Later work
35"
INTRODUCTION,0.03817603393425239,"improved the accountant by giving improved bounds on the Rényi Differential Privacy guarantees
36"
INTRODUCTION,0.03923647932131495,"of the subsampled Gaussian mechanism under both Poisson subsampling and sampling without
37"
INTRODUCTION,0.04029692470837752,"replacement [MTZ19, WBK20].
38"
INTRODUCTION,0.041357370095440084,"Even small constant factors in an (ε, δ)-DP budget are important. First, from the definition, such
39"
INTRODUCTION,0.042417815482502653,"constant factors manifest exponentially in the privacy guarantee. Furthermore, when training a model
40"
INTRODUCTION,0.043478260869565216,"privately with DP-SGD, it has been observed that they can lead to significant differences in the
41"
INTRODUCTION,0.044538706256627786,"downstream utility, see, e.g., Figure 1 of [DBH+22]. Consequently, “saving” such a factor in the
42"
INTRODUCTION,0.04559915164369035,"value of ε through tighter analysis can be very valuable. While earlier approximate techniques for
43"
INTRODUCTION,0.04665959703075292,"privacy accounting (e.g., moments accountant of [ACG+16] and related methods) were lossy, a
44"
INTRODUCTION,0.04772004241781548,"more recent line of work focuses on exact computation of privacy loss by numerically estimating
45"
INTRODUCTION,0.04878048780487805,"the privacy parameters [SMM19, KJH20, KJPH21, GLW21, ZDW22]. These accountants generally
46"
INTRODUCTION,0.04984093319194061,"look at the “worst case” for a single iteration for a privacy mechanism, and then use a fast Fourier
47"
INTRODUCTION,0.05090137857900318,"transform (FFT) to compose the privacy loss over multiple iterations. They often rely on an implicit
48"
INTRODUCTION,0.051961823966065745,"assumption that the worst-case dataset for a single execution of a privacy mechanism remains the
49"
INTRODUCTION,0.053022269353128315,"worst case for a self-composition of the mechanism.
50"
INTRODUCTION,0.05408271474019088,"Most privacy accounting techniques for DP-SGD assume a version of the algorithm that employs
51"
INTRODUCTION,0.05514316012725345,"amplification by Poisson subsampling. That is, the batch for each iteration is formed by including each
52"
INTRODUCTION,0.05620360551431601,"point independently with sampling probability γ. Other privacy accountants consider a variant where
53"
INTRODUCTION,0.05726405090137858,"random batches of a fixed size are selected for each step. Note that both of these are inconsistent with
54"
INTRODUCTION,0.05832449628844114,"the standard method in the non-private setting, where batches are formed by randomly permuting and
55"
INTRODUCTION,0.05938494167550371,"then partitioning the dataset. Indeed, the latter approach is much more efficient, and highly-optimized
56"
INTRODUCTION,0.060445387062566275,"in most libraries. Consequently, many works in private machine learning implement a method with
57"
INTRODUCTION,0.061505832449628844,"the conventional shuffle-and-partition method of batch formation, but employ privacy accountants
58"
INTRODUCTION,0.06256627783669141,"that assume some other method of sampling batches. The hope is that small modifications of this
59"
INTRODUCTION,0.06362672322375397,"sort would have negligible impact on the privacy analysis, thus justifying privacy accountants for a
60"
INTRODUCTION,0.06468716861081654,"setting which is technically not matching. Concurrent work to this paper by [CGK+24] compares the
61"
INTRODUCTION,0.06574761399787911,"shuffle-and-partition technique with Poisson subsampling. Similar to our results they find that the
62"
INTRODUCTION,0.06680805938494168,"batching method can significantly impact the privacy parameters.
63"
INTRODUCTION,0.06786850477200425,"The central aim of our paper is to highlight and clarify some common problems with privacy
64"
INTRODUCTION,0.0689289501590668,"accounting techniques. Towards the goal of more faithful comparisons between private algorithms
65"
INTRODUCTION,0.06998939554612937,"that rely upon such accountants, we make the following contributions:
66"
INTRODUCTION,0.07104984093319194,"• In Sections 4 and 5, we establish that a worst-case dataset may exist for a single execution
67"
INTRODUCTION,0.07211028632025451,"of a privacy mechanism but may fail to exist when looking at the self-composition of the
68"
INTRODUCTION,0.07317073170731707,"same mechanism. Some popular privacy accountants incorrectly assume otherwise. Our
69"
INTRODUCTION,0.07423117709437964,"counterexample involves the subsampled Laplace mechanism, and stronger analysis is
70"
INTRODUCTION,0.07529162248144221,"needed to demonstrate the soundness of privacy accountants for specific mechanisms, e.g.,
71"
INTRODUCTION,0.07635206786850478,"the subsampled Gaussian mechanism.
72"
INTRODUCTION,0.07741251325556733,"• In Section 6, we show that rigorous privacy accounting is significantly affected by the method
73"
INTRODUCTION,0.0784729586426299,"of sampling batches, e.g., Poisson versus fixed-size. This results in sizeable differences in the
74"
INTRODUCTION,0.07953340402969247,"resulting privacy guarantees for settings which were previously treated as interchangeable
75"
INTRODUCTION,0.08059384941675504,"by prior works. Consequently, we caution against the common practice of using one method
76"
INTRODUCTION,0.0816542948038176,"of batch sampling and employing the privacy accountant for another.
77"
INTRODUCTION,0.08271474019088017,"• In Section 7, we discuss issues that arise in tight privacy accounting under the “substitution”
78"
INTRODUCTION,0.08377518557794274,"relation for neighbouring datasets, which make this setting even more challenging than under
79"
INTRODUCTION,0.08483563096500531,"the traditional “add/remove” relation. Once again we consider the subsampled Laplace
80"
INTRODUCTION,0.08589607635206786,"mechanism and show that there may be several worst-case datasets one must consider when
81"
INTRODUCTION,0.08695652173913043,"doing accounting, exposing another important gap in existing analyses.
82"
PRELIMINARIES,0.088016967126193,"2
Preliminaries
83"
PRELIMINARIES,0.08907741251325557,"Differential privacy is a rigorous privacy framework introduced by [DMNS06]. Differential privacy
84"
PRELIMINARIES,0.09013785790031813,"is a restriction on how much the output distribution of a mechanism can change between any pair of
85"
PRELIMINARIES,0.0911983032873807,"datasets that differ only in a single individual. Such datasets are called neighboring, and we denote a
86"
PRELIMINARIES,0.09225874867444327,"pair of neighboring datasets as D ∼D′. We formally define neighboring datasets below.
87"
PRELIMINARIES,0.09331919406150584,"Definition 1 ((ε, δ)-Differential Privacy). A randomized mechanism M satisfies (ε, δ)-DP under
88"
PRELIMINARIES,0.09437963944856839,"neighboring relation ∼if and only if for all D ∼D′ and all measurable sets of outputs Z we have
89"
PRELIMINARIES,0.09544008483563096,Pr[M(D) ∈Z] ≤eε Pr[M(D′) ∈Z] + δ.
PRELIMINARIES,0.09650053022269353,"In this work, we consider problems where we want to estimate a sum for k queries where each
90"
PRELIMINARIES,0.0975609756097561,"datapoint holds a single-dimensional real value in the interval [−1, 1] for each query. The mechanisms
91"
PRELIMINARIES,0.09862142099681867,"we consider apply more generally to multi-dimensional real-valued queries. Since we demonstrate
92"
PRELIMINARIES,0.09968186638388123,"issues already present in the former more restrictive setting, these pitfalls are present in the more
93"
PRELIMINARIES,0.1007423117709438,"general case as well. We focus on single-dimensional inputs for simplicity of presentation. Likewise,
94"
PRELIMINARIES,0.10180275715800637,"by considering mechanisms defined on [−1, 1], our privacy analysis immediately extends to any
95"
PRELIMINARIES,0.10286320254506894,"mechanism defined on R that clips to [−1, 1]. After the appropriate rescaling, our privacy analysis
96"
PRELIMINARIES,0.10392364793213149,"extends to any mechanism used in practice for DP-SGD. Note that in all but one example in Section 7
97"
PRELIMINARIES,0.10498409331919406,"the datapoints hold the same value for all k queries for the datasets we consider. We abuse notation
98"
PRELIMINARIES,0.10604453870625663,"and represent each data point as a single real value rather than a vector.
99"
PRELIMINARIES,0.1071049840933192,"On the domain [−1, 1]∗×k := S∞
m=0[−1, 1]m×k, we define the neighboring definitions of add,
100"
PRELIMINARIES,0.10816542948038176,"remove, and substitution (replacement). We typically want the neighboring relation to be symmetric,
101"
PRELIMINARIES,0.10922587486744433,"which is why add and remove are typically included in a single definition. However, as noted by
102"
PRELIMINARIES,0.1102863202545069,"previous work we need to analyze the add and remove cases separately to get tight results (see, e.g.,
103"
PRELIMINARIES,0.11134676564156946,"[ZDW22]).
104"
PRELIMINARIES,0.11240721102863202,"Definition 2 (Neighboring Datasets). Let D and D′ be datasets. If D′ can be obtained by adding a
105"
PRELIMINARIES,0.11346765641569459,"datapoint to D, then we write D ∼A D′. Likewise, if D′ can be obtained by removing a datapoint
106"
PRELIMINARIES,0.11452810180275716,"from D, then we write D ∼R D′. Combining these, write D ∼A/R D′ if D ∼A D′ or D ∼R D′.
107"
PRELIMINARIES,0.11558854718981973,"Finally, we write D ∼S D′ if D can be obtained from D′ by swapping one datapoint for another.
108"
PRELIMINARIES,0.11664899257688228,"Note that differential privacy under add and remove implies differential privacy under substitution,
109"
PRELIMINARIES,0.11770943796394485,"with appropriate translation of the privacy parameters.
110"
PRELIMINARIES,0.11876988335100742,"Definition 1 can be restated in terms of the hockey-stick divergence.
111"
PRELIMINARIES,0.11983032873807,"Definition 3 (Hockey-stick Divergence). For any α ≥0 the hockey-stick divergence between two
112"
PRELIMINARIES,0.12089077412513255,"distributions P and Q is defined as
113"
PRELIMINARIES,0.12195121951219512,Hα(P||Q) := Ey∼Q
PRELIMINARIES,0.12301166489925769,"
max
dP"
PRELIMINARIES,0.12407211028632026,"dQ(y) −α, 0
"
PRELIMINARIES,0.12513255567338283,where dP
PRELIMINARIES,0.1261930010604454,"dQ is the Radon–Nikodym derivative.
114"
PRELIMINARIES,0.12725344644750794,"Specifically, a randomized mechanism M satisfies (ε, δ)-DP if and only if Heε(M(D)||M(D′)) ≤δ
115"
PRELIMINARIES,0.1283138918345705,"for all pairs of neighboring datasets D ∼D′. This restated definition is the basis for the privacy
116"
PRELIMINARIES,0.12937433722163308,"accounting tools we consider in this paper. If we know what choice of neighboring datasets D ∼D′
117"
PRELIMINARIES,0.13043478260869565,"maximizes the expression then we can get optimal parameters by computing Heε(M(D)||M(D′)).
118"
PRELIMINARIES,0.13149522799575822,"The full range of privacy guarantees for a mechanism can be captured by the privacy curve.
119"
PRELIMINARIES,0.1325556733828208,"Definition 4 (Privacy Curves). The privacy curve of a randomized mechanism M under neighboring
120"
PRELIMINARIES,0.13361611876988336,"relation ∼is the function δ∼
M : R →[0, 1] given by
121"
PRELIMINARIES,0.13467656415694593,"δ∼
M(ε) := min{δ ∈[0, 1] : M is (ε, δ)-DP}."
PRELIMINARIES,0.1357370095440085,"If there is a single pair of neighboring datasets D ∼D′ such that δ∼
M(ε) = Heε(M(D)||M(D′))
122"
PRELIMINARIES,0.13679745493107104,"for all ε ≥0, we say that the privacy curve of M under ∼is realized by the worst-case dataset pair
123"
PRELIMINARIES,0.1378579003181336,"(D, D′).
124"
PRELIMINARIES,0.13891834570519618,"Unfortunately, a worst-case dataset pair does not always exist. A broader tool that is now frequently
125"
PRELIMINARIES,0.13997879109225875,"used in the computation of privacy curves is the privacy loss distribution (PLD) formalism [DR16,
126"
PRELIMINARIES,0.14103923647932132,"SMM19].
127"
PRELIMINARIES,0.1420996818663839,"Definition 5 (Privacy Loss Distribution). Given a mechanism M and a pair of neighboring datasets
128"
PRELIMINARIES,0.14316012725344646,"D ∼D′, the privacy loss distribution of M with respect to (D, D′) is
129"
PRELIMINARIES,0.14422057264050903,"LM(D||D′) := ln(dM(D)/dM(D′))(y),"
PRELIMINARIES,0.14528101802757157,"where y ∼M(D) and dM(D)/dM(D′) means the density of M(D) with respect to M(D′).
130"
PRELIMINARIES,0.14634146341463414,"An important caveat is that the privacy loss distribution is defined with respect to a specific pair of
131"
PRELIMINARIES,0.1474019088016967,"datasets, whereas the privacy curve implicitly involves taking a maximum over all neighboring pairs
132"
PRELIMINARIES,0.14846235418875928,"of datasets. Nonetheless, the PLD formalism can be used to recover the hockey-stick divergence via
133"
PRELIMINARIES,0.14952279957582185,"Heε(M(D)||M(D′)) = EY ∼LM(D||D′)[1 −eε−Y ],"
PRELIMINARIES,0.15058324496288442,"from which we can reconstruct the privacy curve as
134"
PRELIMINARIES,0.15164369034994699,"δ∼
M(ε) = max
D∼D′ EY ∼LM(D||D′)[1 −eε−Y ]."
PRELIMINARIES,0.15270413573700956,"Lastly, we define the two subsampling procedures we consider in this work: sampling without
135"
PRELIMINARIES,0.1537645811240721,"replacement (WOR) and Poisson sampling. Given a dataset D = (x1, . . . , xn) and a set I ⊆
136"
PRELIMINARIES,0.15482502651113467,"{1, . . . , n}, we denote the restriction of D to I = {i1, . . . , ib} by D|I := (xi1, . . . , xib).
137"
PRELIMINARIES,0.15588547189819724,"Definition 6 (Subsampling). Let M take datasets of size1 b ≥1. The
 n
b

-subsampled mechanism
138"
PRELIMINARIES,0.1569459172852598,"MW OR is defined on datasets of size n ≥b as
139"
PRELIMINARIES,0.15800636267232238,"MW OR(D) := M(D|I),"
PRELIMINARIES,0.15906680805938495,"where I is a uniform random b-subset of {1, . . . , n}.
140"
PRELIMINARIES,0.16012725344644752,"On the other hand, given a mechanism M taking datasets of any size, the γ-subsampled mechanism
141"
PRELIMINARIES,0.16118769883351008,"MP oisson is defined on datasets of arbitrary size as
142"
PRELIMINARIES,0.16224814422057265,"MP oisson(D) := M(D|I),"
PRELIMINARIES,0.1633085896076352,"where I includes each element of {1, . . . , |D|} independently with probability γ.
143"
RELATED WORK,0.16436903499469777,"3
Related Work
144"
RELATED WORK,0.16542948038176034,"After [DR16] introduced privacy loss distributions, a number of works used the formalism to estimate
145"
RELATED WORK,0.1664899257688229,"the privacy curve to arbitrary precision, beginning with [SMM19]. [KJH20, KJPH21] developed an
146"
RELATED WORK,0.16755037115588547,"efficient accountant that efficiently computes the convolution of PLDs by leveraging the fast Fourier
147"
RELATED WORK,0.16861081654294804,"transform. [GLW21] fine-tuned the application of FFT to speed up the accountant by several orders
148"
RELATED WORK,0.16967126193001061,"of magnitude.
149"
RELATED WORK,0.17073170731707318,"The most relevant related paper for our work is by [ZDW22]. They introduce the concept of a
150"
RELATED WORK,0.17179215270413573,"dominating pair of distributions. Dominating pairs generalize worst-case datasets, which for some
151"
RELATED WORK,0.1728525980911983,"problems can be difficult to find and may not even exist.
152"
RELATED WORK,0.17391304347826086,"Definition 7 (Dominating Pair of Distributions [ZDW22]). The ordered pair (P, Q) is a dominating
153"
RELATED WORK,0.17497348886532343,"pair of distributions for a mechanism M (under some neighboring relation ∼) if for all α ≥0 it
154"
RELATED WORK,0.176033934252386,"holds that
155"
RELATED WORK,0.17709437963944857,"sup
D∼D′ Hα(M(D)||M(D′)) ≤Hα(P||Q)."
RELATED WORK,0.17815482502651114,"The hockey-stick divergence of the dominating pair P and Q gives an upper bound on the value δ for
156"
RELATED WORK,0.1792152704135737,"any ε. Note that the distributions P and Q do not need to be output distributions of the mechanism.
157"
RELATED WORK,0.18027571580063625,"However, if there exists a pair of neighboring datasets such that P = M(D) and Q = M(D′) then
158"
RELATED WORK,0.18133616118769882,"we can find tight privacy parameters by analyzing the mechanisms with inputs D and D′ because
159"
RELATED WORK,0.1823966065747614,"Heε(M(D)||M(D′)) is also a lower bound on δ for any ε. We refer to such D ∼D′ as a dominating
160"
RELATED WORK,0.18345705196182396,"pair of datasets.
161"
RELATED WORK,0.18451749734888653,"The definition of dominating pairs of distributions is useful for analyzing the privacy guarantees of
162"
RELATED WORK,0.1855779427359491,"composed mechanisms. In this work, we focus on the special case where a mechanism consists of k
163"
RELATED WORK,0.18663838812301167,"self-compositions. This is, for example, the case in DP-SGD, in which we run several iterations of the
164"
RELATED WORK,0.18769883351007424,"subsampled Gaussian mechanism. The property we need for composition is presented in Theorem 8.
165"
RELATED WORK,0.18875927889713678,"Theorem 8 (Following Theorem 10 of [ZDW22]). If (P, Q) is a dominating pair for a mechanism
166"
RELATED WORK,0.18981972428419935,"M then (P k, Qk) is a dominating pair for k iterations of M.
167"
RELATED WORK,0.19088016967126192,"When studying differential privacy parameters in terms of the hockey-stick divergence, we usually
168"
RELATED WORK,0.1919406150583245,"focus on the case of α ≥1. Recall that the hockey-stick divergence of order α can be used to bound
169"
RELATED WORK,0.19300106044538706,1We treat the sample size and batch size as public knowledge in line with prior work [ZDW22].
RELATED WORK,0.19406150583244963,"the value of δ for an (ε, δ)-DP mechanism where ε = ln(α). We typically do not care about the region
170"
RELATED WORK,0.1951219512195122,"of α < 1 because it corresponds to negative values of ε. However, the definition of dominating pairs
171"
RELATED WORK,0.19618239660657477,"of distributions must include these values as well. This is because outputs with negative privacy loss
172"
RELATED WORK,0.19724284199363734,"are important for composition and Theorem 8 would not hold if the definition only considered α ≥1.
173"
RELATED WORK,0.19830328738069988,"In Sections 5 and 7 we consider mechanisms where the distributions that bound the hockey-stick
174"
RELATED WORK,0.19936373276776245,"divergence for α ≥1 without composition do not bound the divergence for α ≥1 under composition.
175"
RELATED WORK,0.20042417815482502,"[ZDW22] studied general mechanisms in terms of dominating pairs of distributions under Poisson
176"
RELATED WORK,0.2014846235418876,"subsampling and sampling without replacement. Their work gives upper bounds on the privacy
177"
RELATED WORK,0.20254506892895016,"parameters based on the dominating pair of distributions of the non-subsampled mechanism. We use
178"
RELATED WORK,0.20360551431601273,"some of their results which we introduce later throughout this paper.
179"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2046659597030753,"4
Dominating Pair of Datasets under Add and Remove Relations
180"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.20572640509013787,"In this section we give pairs of neighboring datasets with provable worst-case privacy parameters
181"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2067868504772004,"under the add and remove neighboring relations separately. We use these datasets as examples of the
182"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.20784729586426298,"pitfalls to avoid in the subsequent section, where we discuss the combined add/remove neighboring
183"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.20890774125132555,"relation.
184"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.20996818663838812,"Proposition 9. Let M be either the Gaussian mechanism M(x1, . . . , xn) := Pn
i=1 xi + N(0, σ2)
185"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2110286320254507,"or the Laplace mechanism M(x1, . . . , xn) := Pn
i=1 xi + Lap(0, s).
186"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.21208907741251326,"1. The datasets D := (0, . . . , 0) and D′ := (0, . . . , 0, 1) form a dominating pair of datasets
187"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.21314952279957583,"for MP oisson under the add relation and (D′, D) is a dominating pair of datasets under
188"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2142099681866384,"the remove relation.
189"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.21527041357370094,"2. Likewise, the datasets D := (−1, . . . , −1) and D′ := (−1, . . . , −1, 1) form a dominating
190"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2163308589607635,"pair of datasets for MW OR under the add relation and (D′, D) is a dominating pair of
191"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.21739130434782608,"datasets under the remove relation.
192"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.21845174973488865,"The proposition implies that the hockey-stick divergence of the mechanisms with said datasets as
193"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.21951219512195122,"input describes the privacy curves of the composed mechanisms under the add and remove relations,
194"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2205726405090138,"respectively. We contrast this good behavior of composed and subsampled mechanisms under add
195"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.22163308589607636,"and remove separately with the Laplace mechanism, which, as we will see in Section 5, does not
196"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.22269353128313893,"behave well when composed under the combined add/remove relation.
197"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.22375397667020147,"Our dominating pair of datasets can be found by reduction to one of the main results of [ZDW22].
198"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.22481442205726404,"Theorem 10 (Theorem 11 of [ZDW22]). Let M be a randomized mechanism, let MP oisson be
199"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2258748674443266,"the γ-subsampled version of the mechanism, and let MW OR be the
 n
b

-subsampled version of the
200"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.22693531283138918,"mechanism on datasets of size n and n −1 with γ = b/n.
201"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.22799575821845175,"1. If (P, Q) dominates M for add neighbors then (P, (1 −γ)P + γQ) dominates MP oisson
202"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.22905620360551432,"for add neighbors and ((1 −γ)Q + γP, P) dominates MP oisson for removal neighbors.
203"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2301166489925769,"2. If (P, Q) dominates M for substitution neighbors then (P, (1 −γ)P + γQ) dominates
204"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.23117709437963946,"MW OR for add neighbors and ((1 −γ)P + γQ, P) dominates MW OR for removal
205"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.23223753976670203,"neighbors.
206"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.23329798515376457,"In Appendix A we prove that Proposition 9 holds by showing that the hockey-stick divergence between
207"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.23435843054082714,"the mechanism with the dominating pairs of datasets matches the upper bound from Theorem 10.
208"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2354188759278897,"Crucially, Proposition 9 implies that under the add and remove relations, we must add noise with
209"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.23647932131495228,"twice the magnitude when sampling without replacement compared to Poisson subsampling! The
210"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.23753976670201485,"intuition behind this difference is that the subroutine behaves similarly to the add/remove neighboring
211"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.23860021208907742,"relation when using Poisson subsampling, whereas it resembles the substitution neighborhood when
212"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.23966065747614,"sampling without replacement. When D′
i is included in the batch another datapoint is ’pushed out’ of
213"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.24072110286320256,"the batch under sampling without replacement. Due to this parallel one might hope that the difference
214"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2417815482502651,"in privacy parameters between Poisson subsampling and sampling without replacement only differ
215"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.24284199363732767,"by a small constant similar to the difference between the add/remove and substitution neighboring
216"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.24390243902439024,"relations. That is indeed the case for many parameters, but as we show in Section 7 this assumption
217"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2449628844114528,"unfortunately does not always hold.
218"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.24602332979851538,"0.0
0.5
1.0
1.5 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 k = 1"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.24708377518557795,"Remove
Add"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.24814422057264052,"0.0
0.5
1.0
1.5
2.0 0.0 0.1 0.2 0.3 0.4 k = 2"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2492046659597031,"Remove
Add"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.25026511134676566,"0
2
4
6
8 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 k = 8"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2513255567338282,"Remove
Add"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2523860021208908,"Privacy Curve of the (
10
9 )-subsampled Laplace(2) Mechanism (k Iterations)"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.25344644750795337,"Figure 1: The privacy curves for the subsampled Laplace mechanism under the remove and add
neighboring relations respectively."
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2545068928950159,"5
No Worst-case Pair of Datasets under Add/Remove Relation
219"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.25556733828207845,"So far, we have considered the entire privacy curve for all ε ∈R. This is a necessary subtlety for PLD
220"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.256627783669141,"privacy accounting tools under composition (e.g., Theorem 8). Here we focus only on the privacy
221"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2576882290562036,"curve for ε ≥0. Our main result of this section is to give a minimal example of a mechanism M that
222"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.25874867444326616,"admits a worst-case dataset pair under ∼A/R yet Mk does not admit any worst-case dataset pair for
223"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2598091198303287,"some k > 1. This violates an implicit assumption made by some privacy accountants.
224"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2608695652173913,"Proposition 11. For some mechanism M, the privacy curve of the
 n
b

-subsampled mechanism
225"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.26193001060445387,"MW OR is realized by a pair of datasets under ∼A/R, yet no pair of datasets realizes the privacy
226"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.26299045599151644,"curve of Mk
W OR for all k > 1.
227"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.264050901378579,"A proof of this proposition for a simple mechanism can be found in Appendix B.1. However, it
228"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2651113467656416,"is more illustrative to demonstrate the proposition informally for the Laplace mechanism M. In
229"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.26617179215270415,"this case, note that the proposition can be extended to MP oisson as well. The proposition stands in
230"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2672322375397667,"contrast to the case of the add and remove relations discussed in Proposition 9. That is, we can find
231"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2682926829268293,"datasets D ∼A D′ such that δ∼A
MW OR is realized by (D, D′) and δ∼R
MW OR is realized by (D′, D), but
232"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.26935312831389185,"no such (ordered) pair realizes the privacy curve under ∼A/R.
233"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2704135737009544,"Moreover, it is generally the case that the privacy curve of a subsampled mechanism without
234"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.271474019088017,"composition under ∼R dominates the privacy curve under ∼A when ε ≥0 (see, e.g., Proposition 30
235"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2725344644750795,"of [ZDW22] or Theorem 5 of [MTZ19]). Specifically, it follows from Proposition 30 of [ZDW22]
236"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2735949098621421,"that in the case of the subsampled Laplace mechanism and ε ≥0, we have that
237"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.27465535524920465,"δ
∼A/R
MW OR(ε) = δ∼R
MW OR(ε) ≥δ∼A
MW OR(ε)."
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2757158006362672,"Here we visualize the counter-example by plotting privacy curves for the add and remove relation in
238"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2767762460233298,"Figure 1. Note that δ
∼A/R
MW OR(ε) = max{δ∼A
MW OR(ε), δ∼R
MW OR(ε)}. Figure 1 shows several variations
239"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.27783669141039236,"of the curves δ∼A
Mk
W OR and δ∼R
Mk
W OR, which we estimated numerically by Monte Carlo simulation (as
240"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2788971367974549,"in, e.g., [WMW+23]). Appendix B.2 has the methodological details. These curves are seen to cross
241"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2799575821845175,"in the region ε ≥0 for k = 2 compositions.
242"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.28101802757158006,"The phenomenon is most apparent for k = 2. There is a clear break in the curve for the remove relation.
243"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.28207847295864263,"Under many compositions, however, it is known that both PLDs converge to a Gaussian distribution
244"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2831389183457052,"[DRS19], which explains why this break vanishes as the number of compositions increases.
245"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2841993637327678,"Avoiding incorrect upper bounds
As shown in this section we cannot assume that the privacy
246"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.28525980911983034,"curve for the remove relation dominates the add relation for composed subsampled mechanisms under
247"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2863202545068929,"∼A/R even though it is the case without composition. Luckily, this particular issue can be easily
248"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2873806998939555,"resolved by computing the privacy parameters for the add and remove relation separately and taking
249"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.28844114528101805,"the maximum. This technique is already used in practice in, e.g., the Google DP library [Goo20].
250"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.28950159066808057,"We conjecture that this workaround is unnecessary for the Gaussian mechanism—the natural choice
251"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.29056203605514314,"for DP-SGD. We searched a wide range of parameters and were unable to produce a counterexample.
252"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2916224814422057,"Conjecture 12. Let M be the Gaussian mechanism with any σ. Then for all k > 0, γ ∈[0, 1], and
253"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.2926829268292683,"ε ≥0 we have
254"
DOMINATING PAIR OF DATASETS UNDER ADD AND REMOVE RELATIONS,0.29374337221633084,"δ
∼A/R
Mk
P oisson(ε) = δ∼R
Mk
P oisson(ε) ≥δ∼A
Mk
P oisson(ε)."
COMPARISON OF SAMPLING SCHEMES,0.2948038176033934,"6
Comparison of Sampling Schemes
255"
COMPARISON OF SAMPLING SCHEMES,0.295864262990456,"In this section we explore the difference in privacy parameters between Poisson subsampling and
256"
COMPARISON OF SAMPLING SCHEMES,0.29692470837751855,"sampling without replacement. We focus on the subsampled Gaussian mechanism which is the
257"
COMPARISON OF SAMPLING SCHEMES,0.2979851537645811,"mechanism of choice for DP-SGD. We show that for some parameters the privacy guarantees of the
258"
COMPARISON OF SAMPLING SCHEMES,0.2990455991516437,"mechanism differ significantly between the two sampling schemes.
259"
COMPARISON OF SAMPLING SCHEMES,0.30010604453870626,"There are several different techniques one might use when selecting privacy-specific hyperparameters
260"
COMPARISON OF SAMPLING SCHEMES,0.30116648992576883,"for DP-SGD. One approach is to fix the value of δ and the number of iterations. Given a sampling
261"
COMPARISON OF SAMPLING SCHEMES,0.3022269353128314,"rate γ and a value for ε, we can compute the smallest value for the noise multiplier σ such that the
262"
COMPARISON OF SAMPLING SCHEMES,0.30328738069989397,"mechanism satisfies (ε, δ)-differential privacy. We use this approach to showcase our findings. We
263"
COMPARISON OF SAMPLING SCHEMES,0.30434782608695654,"fix δ = 10−6 and the number of iterations to 10, 000. We then vary the sampling rate between 10−4
264"
COMPARISON OF SAMPLING SCHEMES,0.3054082714740191,"to 1 and use the PLD accountant implemented in the Opacus library [YSS+21] to compute σ.
265"
COMPARISON OF SAMPLING SCHEMES,0.3064687168610817,"Figure 2: Plots of the smallest noise multiplier σ required to achieve certain privacy parameters
for the subsampled Gaussian mechanism with varying sampling rates under add/remove. Each line
shows a specific value of ε for either Poisson subsampling or sampling without replacement. The
parameter δ is fixed to 10−6 for all lines."
COMPARISON OF SAMPLING SCHEMES,0.3075291622481442,"In Figure 2 we plot the noise multiplier required to achieve (ε, 10−6)-DP with Poisson subsampling
266"
COMPARISON OF SAMPLING SCHEMES,0.30858960763520676,"for ε ∈{1, 2, 5, 10}. For comparison, we plot the noise multiplier that achieves (10, 10−6)-DP
267"
COMPARISON OF SAMPLING SCHEMES,0.30965005302226933,"when sampling without replacement. Recall from Section 4 that the noise magnitude required when
268"
COMPARISON OF SAMPLING SCHEMES,0.3107104984093319,"sampling without replacement is exactly twice that required for Poisson subsampling. The plots are
269"
COMPARISON OF SAMPLING SCHEMES,0.3117709437963945,"clearly divided into two regions. For large sampling rate, the noise multiplier scales roughly linearly
270"
COMPARISON OF SAMPLING SCHEMES,0.31283138918345704,"in the sampling rate. However, for sufficiently low sampling rates the noise multiplier decreases
271"
COMPARISON OF SAMPLING SCHEMES,0.3138918345705196,"much slower. This effect has been observed previously for setting hyperparameters (see Figure 1 of
272"
COMPARISON OF SAMPLING SCHEMES,0.3149522799575822,"[PHK+23] for a similar plot).
273"
COMPARISON OF SAMPLING SCHEMES,0.31601272534464475,"δ
ε (Poisson)
ε (WOR)
10−7
1.19
17.48
10−6
0.96
15.26
10−5
0.80
12.98
10−4
0.64
10.62
Table 1: The table contrasts the privacy parameter ε for the subsampled Gaussian mechanism with
10, 000 iterations, sampling rate γ = 0.001, and noise multiplier σ = 0.8 for multiple values of δ."
COMPARISON OF SAMPLING SCHEMES,0.3170731707317073,"Avoiding problematic parameters
It is generally advised to select parameters that fall into the
274"
COMPARISON OF SAMPLING SCHEMES,0.3181336161187699,"right-hand regime of the plots in Figure 2 [PHK+23]. However, one might select parameters close to
275"
COMPARISON OF SAMPLING SCHEMES,0.31919406150583246,"the transition point. This can be especially problematic if the wrong privacy accountant is used. The
276"
COMPARISON OF SAMPLING SCHEMES,0.32025450689289503,"transition point happens when σ is slightly less than 1 for Poisson sampling and therefore it happens
277"
COMPARISON OF SAMPLING SCHEMES,0.3213149522799576,"when it is slightly less than 2 for sampling without replacement. The consequence can be seen for
278"
COMPARISON OF SAMPLING SCHEMES,0.32237539766702017,"the plot for sampling without replacement in Figure 2. When the sampling rates are high the noise
279"
COMPARISON OF SAMPLING SCHEMES,0.32343584305408274,"required roughly matches that for ε = 5 with Poisson subsampling. But when the sampling rate is
280"
COMPARISON OF SAMPLING SCHEMES,0.3244962884411453,"small we have to add more noise than is required for ε = 1 with Poisson subsampling. As such, if we
281"
COMPARISON OF SAMPLING SCHEMES,0.3255567338282078,"use a privacy accountant for Poisson subsampling and have a target of ε = 1 but our implementation
282"
COMPARISON OF SAMPLING SCHEMES,0.3266171792152704,"uses sampling without replacement the actual value of ε could be above 10! We might hope that
283"
COMPARISON OF SAMPLING SCHEMES,0.32767762460233296,"this increase would be offset if we allow for some slack in δ as well. However, as seen in the table
284"
COMPARISON OF SAMPLING SCHEMES,0.32873806998939553,"of Figure 1 there can still be a big gap in ε between the sampling schemes even when we allow a
285"
COMPARISON OF SAMPLING SCHEMES,0.3297985153764581,"difference of several orders of magnitude in δ.
286"
SUBSTITUTION NEIGHBORING RELATION,0.33085896076352067,"7
Substitution Neighboring Relation
287"
SUBSTITUTION NEIGHBORING RELATION,0.33191940615058324,"In this section, we consider both sampling schemes under the substitution neighboring relation.
288"
SUBSTITUTION NEIGHBORING RELATION,0.3329798515376458,"In their work on computing tight differential privacy guarantees, [KJH20] considered worst-case
289"
SUBSTITUTION NEIGHBORING RELATION,0.3340402969247084,"distributions for the subsampled Gaussian mechanism under multiple sampling techniques and
290"
SUBSTITUTION NEIGHBORING RELATION,0.33510074231177095,"neighboring relations. In the substitution case, they compute the hockey-stick divergence between
291"
SUBSTITUTION NEIGHBORING RELATION,0.3361611876988335,"(1 −γ)N(0, σ2) + γN(−1, σ2) and (1 −γ)N(0, σ2) + γN(1, σ2). These distributions correspond
292"
SUBSTITUTION NEIGHBORING RELATION,0.3372216330858961,"to running the mechanism with neighboring datasets where all but one entry is 0. We first consider
293"
SUBSTITUTION NEIGHBORING RELATION,0.33828207847295866,"Poisson subsampling in the proposition below and later discuss sampling without replacement.
294"
SUBSTITUTION NEIGHBORING RELATION,0.33934252386002123,"Proposition 13. Consider the Gaussian mechanism M(x1, . . . , xn) := Pn
i=1 xi + N(0, σ2) and
295"
SUBSTITUTION NEIGHBORING RELATION,0.3404029692470838,"let MP oisson be the γ-subsampled mechanism. Then D := (0, . . . , 0, 1) and D′ := (0, . . . , 0, −1)
296"
SUBSTITUTION NEIGHBORING RELATION,0.34146341463414637,"form a dominating pair of datasets under the substitution neighboring relation.
297"
SUBSTITUTION NEIGHBORING RELATION,0.3425238600212089,"Proposition 13 simply confirms that the pair of distributions considered by [KJH20] does indeed give
298"
SUBSTITUTION NEIGHBORING RELATION,0.34358430540827145,"correct guarantees as it is a dominating pair of distributions. However, as far as we are aware, no
299"
SUBSTITUTION NEIGHBORING RELATION,0.344644750795334,"formal proof existed anywhere. Our proof of the proposition is in Appendix C.
300"
SUBSTITUTION NEIGHBORING RELATION,0.3457051961823966,"In the rest of the section we focus on sampling without replacement. We start by restating another
301"
SUBSTITUTION NEIGHBORING RELATION,0.34676564156945916,"result from [ZDW22] which we use throughout the section.
302"
SUBSTITUTION NEIGHBORING RELATION,0.34782608695652173,"Theorem 14 (Proposition 30 of [ZDW22]). If (P, Q) dominates M under substitution for datasets
303"
SUBSTITUTION NEIGHBORING RELATION,0.3488865323435843,"of size γn, then under the substitution neighborhood for datasets of size n, we have
304"
SUBSTITUTION NEIGHBORING RELATION,0.34994697773064687,"δ(α) ≤
Hα((1 −γ)Q + γP||P)
if α ≥1;
Hα(P||(1 −γ)P + γQ)
if 0 < α < 1,"
SUBSTITUTION NEIGHBORING RELATION,0.35100742311770944,"where δ(α) is the largest hockey-stick divergence of order α for MW OR on neighboring datasets.
305"
SUBSTITUTION NEIGHBORING RELATION,0.352067868504772,"Next, we address a mistake made in related work. We introduced the distributions considered
306"
SUBSTITUTION NEIGHBORING RELATION,0.3531283138918346,"by [KJH20] for Poisson subsampling above and we show in Proposition 13 that it is a dominating
307"
SUBSTITUTION NEIGHBORING RELATION,0.35418875927889715,"pair of distributions. However, [KJH20] claimed in their paper that the privacy curves are identical
308"
SUBSTITUTION NEIGHBORING RELATION,0.3552492046659597,"for the two sampling schemes under the substitution relation which is unfortunately incorrect.
309"
SUBSTITUTION NEIGHBORING RELATION,0.3563096500530223,"They considered datasets where all but one entry has a value of 0. This results in correct distri-
310"
SUBSTITUTION NEIGHBORING RELATION,0.35737009544008486,"butions for Poisson subsampling but for sampling without replacement, we instead consider the
311"
SUBSTITUTION NEIGHBORING RELATION,0.3584305408271474,"datasets D := (−1, . . . , −1, 1) and D′ := (−1, . . . , −1, −1). With these datasets the values of
312"
SUBSTITUTION NEIGHBORING RELATION,0.35949098621421,"Hα(MW OR(D)||MW OR(D′)) and Hα(MW OR(D′)||MW OR(D)) match the cases of the upper
313"
SUBSTITUTION NEIGHBORING RELATION,0.3605514316012725,"bound in Theorem 14 for α ≥1 and α < 1, respectively. This can be easily verified by following the
314"
SUBSTITUTION NEIGHBORING RELATION,0.3616118769883351,"steps of the proof of Proposition 9 for sampling without replacement.
315"
SUBSTITUTION NEIGHBORING RELATION,0.36267232237539765,"We can use the datasets above to compute tight privacy guarantees for a single iteration. However,
316"
SUBSTITUTION NEIGHBORING RELATION,0.3637327677624602,"composition is more complicated since neither of the two directions corresponds to a dominating
317"
SUBSTITUTION NEIGHBORING RELATION,0.3647932131495228,"pair of distributions. One might hope that we could simply compute the hockey-stick divergence of
318"
SUBSTITUTION NEIGHBORING RELATION,0.36585365853658536,"the self-composed distributions in both directions and use the maximum similar to the add/remove
319"
SUBSTITUTION NEIGHBORING RELATION,0.3669141039236479,"case. However, for some mechanisms that is not sufficient because we can combine the directions
320"
SUBSTITUTION NEIGHBORING RELATION,0.3679745493107105,"unlike with the add and remove cases. Next we give a minimal counterexample using the Laplace
321"
SUBSTITUTION NEIGHBORING RELATION,0.36903499469777307,"mechanism to showcase this challenge.
322"
SUBSTITUTION NEIGHBORING RELATION,0.37009544008483564,"We consider datasets of size 2 and sample batches with a single element such that γ = 0.5. Let
323"
SUBSTITUTION NEIGHBORING RELATION,0.3711558854718982,"x1 and x2 denote the two data points in D and without loss of generality assume that x1 = x′
1
324"
SUBSTITUTION NEIGHBORING RELATION,0.3722163308589608,"and x2 ̸= x′
2, where x′
1 and x′
2 are the corresponding data points in D′. We apply the subsampled
325"
SUBSTITUTION NEIGHBORING RELATION,0.37327677624602335,"Laplace mechanism with a scale of 2 and perform 2 queries where x1 has the value −1 for both
326"
SUBSTITUTION NEIGHBORING RELATION,0.3743372216330859,"queries. Let P := 0.5 · Lap(−1, 2) + 0.5 · Lap(1, 2) and Q := Lap(−1, 2). That is, P and Q are
327"
SUBSTITUTION NEIGHBORING RELATION,0.3753976670201485,"the distributions for running one query of MW OR(D) with x2 having value 1 or −1, respectively.
328"
SUBSTITUTION NEIGHBORING RELATION,0.37645811240721105,"Then Heε(P × P||Q × Q) is the hockey-stick divergence for the mechanism if x2 has value 1 for
329"
SUBSTITUTION NEIGHBORING RELATION,0.37751855779427357,"Figure 3: Hockey-stick divergence of the Laplace mechanism when sampling without replacement
under ∼S. The worst-case pair of datasets depends on the value of ε."
SUBSTITUTION NEIGHBORING RELATION,0.37857900318133614,"both queries and x′
2 has value −1 for both queries. Similarly, Heε(Q × Q||P × P) is the divergence
330"
SUBSTITUTION NEIGHBORING RELATION,0.3796394485683987,"when x2 has value −1 for both queries and x′
2 has value 1 for both queries.
331"
SUBSTITUTION NEIGHBORING RELATION,0.3806998939554613,"The two hockey-stick divergences above are similar to those for the remove and add neighboring
332"
SUBSTITUTION NEIGHBORING RELATION,0.38176033934252385,"relations. However, we also have to consider Heε(P × Q||P × Q) in the case of substitution. These
333"
SUBSTITUTION NEIGHBORING RELATION,0.3828207847295864,"distributions correspond to the case when x2 has a value of 1 for the first query and −1 for the
334"
SUBSTITUTION NEIGHBORING RELATION,0.383881230116649,"second query, and x′
2 has a value of −1 for the first query and 1 for the second query. Figure 3
335"
SUBSTITUTION NEIGHBORING RELATION,0.38494167550371156,"shows the hockey-stick divergence as a function of ε for the three pairs of neighboring datasets.
336"
SUBSTITUTION NEIGHBORING RELATION,0.3860021208907741,"The largest divergence depends on the value of ε with all three divergences being the maximum for
337"
SUBSTITUTION NEIGHBORING RELATION,0.3870625662778367,"some interval. This counterexample shows that we cannot upper bound the hockey-stick divergence
338"
SUBSTITUTION NEIGHBORING RELATION,0.38812301166489926,"for the subsampled Laplace mechanism as max{Heε(P k||Qk), Heε(Qk||P k)} for k > 1. For k
339"
SUBSTITUTION NEIGHBORING RELATION,0.38918345705196183,"compositions, we have to consider k + 1 ways of combining P and Q. This significantly slows down
340"
SUBSTITUTION NEIGHBORING RELATION,0.3902439024390244,"the accountants in contrast to the 2 cases required for add/remove. Worse still, we do not have a proof
341"
SUBSTITUTION NEIGHBORING RELATION,0.391304347826087,"that one of k + 1 cases is the worst-case pair of datasets for all ε ≥0.
342"
SUBSTITUTION NEIGHBORING RELATION,0.39236479321314954,"In Appendix D we use an alternative technique for bounding the privacy curve under the substitution
343"
SUBSTITUTION NEIGHBORING RELATION,0.3934252386002121,"relation based on [DGK+22]. We show that this accountant does not generally outperform the RDP
344"
SUBSTITUTION NEIGHBORING RELATION,0.3944856839872747,"accountant. This demonstrates the need to strengthen the theory for sampling without replacement
345"
SUBSTITUTION NEIGHBORING RELATION,0.3955461293743372,"under the substitution relation for the purposes of tight privacy accounting.
346"
DISCUSSION,0.39660657476139977,"8
Discussion
347"
DISCUSSION,0.39766702014846234,"We have highlighted two issues that arise in the practice of privacy accounting.
348"
DISCUSSION,0.3987274655355249,"First, we have given a concrete example where the worst-case dataset (for ε ≥0) of a subsampled
349"
DISCUSSION,0.3997879109225875,"mechanism fails to be a worst-case dataset once that mechanism is composed. Care should therefore
350"
DISCUSSION,0.40084835630965004,"be taken to ensure that the privacy accountant computes privacy guarantees with respect to a true
351"
DISCUSSION,0.4019088016967126,"worst-case dataset for a given choice of ε.
352"
DISCUSSION,0.4029692470837752,"Secondly, we have shown that the privacy parameters for a subsampled and composed mechanism
353"
DISCUSSION,0.40402969247083775,"can differ significantly for different subsampling schemes. This can be problematic if the privacy
354"
DISCUSSION,0.4050901378579003,"accountant is assuming a different subsampling procedure from the one actually employed. We have
355"
DISCUSSION,0.4061505832449629,"shown this in the case of Poisson sampling and sampling without replacement but the phenomenon
356"
DISCUSSION,0.40721102863202546,"is likely to occur when comparing Poisson sampling to shuffling as well. Computing tight privacy
357"
DISCUSSION,0.40827147401908803,"guarantees for the shuffled Gaussian mechanism remains an important open problem. It is best
358"
DISCUSSION,0.4093319194061506,"practice to ensure that the implemented subsampling method matches the accounting method. When
359"
DISCUSSION,0.41039236479321317,"this is not practical, the discrepancy should be disclosed.
360"
DISCUSSION,0.41145281018027574,"We conclude with two recommendations for practitioners applying privacy accounting in the DP-
361"
DISCUSSION,0.41251325556733826,"SGD setting. We recommend disclosing the privacy accounting hyperparameters for the sake of
362"
DISCUSSION,0.4135737009544008,"reproducibility (see Section 5.3.3 of [PHK+23] for a list of suggestions). Finally, we also recommend
363"
DISCUSSION,0.4146341463414634,"that, when comparisons are made between DP-SGD mechanisms, the privacy accounting for both
364"
DISCUSSION,0.41569459172852596,"should be re-run for the sake of fairness.
365"
REFERENCES,0.41675503711558853,"References
366"
REFERENCES,0.4178154825026511,"[ACG+16] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
367"
REFERENCES,0.4188759278897137,"Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the
368"
REFERENCES,0.41993637327677624,"2016 ACM Conference on Computer and Communications Security, CCS ’16, pages
369"
REFERENCES,0.4209968186638388,"308–318, New York, NY, USA, 2016. ACM.
370"
REFERENCES,0.4220572640509014,"[BBG18] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling:
371"
REFERENCES,0.42311770943796395,"Tight analyses via couplings and divergences. In Advances in Neural Information
372"
REFERENCES,0.4241781548250265,"Processing Systems 31, NeurIPS ’18, pages 6277–6287. Curran Associates, Inc., 2018.
373"
REFERENCES,0.4252386002120891,"[BS16] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications,
374"
REFERENCES,0.42629904559915166,"extensions, and lower bounds. In Proceedings of the 14th Conference on Theory of
375"
REFERENCES,0.42735949098621423,"Cryptography, TCC ’16-B, pages 635–658, Berlin, Heidelberg, 2016. Springer.
376"
REFERENCES,0.4284199363732768,"[BST14] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimiza-
377"
REFERENCES,0.42948038176033937,"tion: Efficient algorithms and tight error bounds. In Proceedings of the 55th Annual
378"
REFERENCES,0.4305408271474019,"IEEE Symposium on Foundations of Computer Science, FOCS ’14, pages 464–473,
379"
REFERENCES,0.43160127253446445,"Washington, DC, USA, 2014. IEEE Computer Society.
380"
REFERENCES,0.432661717921527,"[BW18] Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differen-
381"
REFERENCES,0.4337221633085896,"tial privacy: Analytical calibration and optimal denoising. In ICML, volume 80 of
382"
REFERENCES,0.43478260869565216,"Proceedings of Machine Learning Research, pages 403–412. PMLR, 2018.
383"
REFERENCES,0.43584305408271473,"[CGK+24] Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha,
384"
REFERENCES,0.4369034994697773,"and Chiyuan Zhang. How private is dp-sgd?, 2024.
385"
REFERENCES,0.43796394485683987,"[DBH+22] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Un-
386"
REFERENCES,0.43902439024390244,"locking high-accuracy differentially private image classification through scale. arXiv
387"
REFERENCES,0.440084835630965,"preprint arXiv:2204.13650, 2022.
388"
REFERENCES,0.4411452810180276,"[DGK+22] Vadym Doroshenko, Badih Ghazi, Pritish Kamath, Ravi Kumar, and Pasin Manurangsi.
389"
REFERENCES,0.44220572640509015,"Connect the dots: Tighter discrete approximations of privacy loss distributions. Proc.
390"
REFERENCES,0.4432661717921527,"Priv. Enhancing Technol., 2022(4):552–570, 2022.
391"
REFERENCES,0.4443266171792153,"[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise
392"
REFERENCES,0.44538706256627786,"to sensitivity in private data analysis. In Proceedings of the 3rd Conference on Theory
393"
REFERENCES,0.44644750795334043,"of Cryptography, TCC ’06, pages 265–284, Berlin, Heidelberg, 2006. Springer.
394"
REFERENCES,0.44750795334040294,"[DR16] Cynthia Dwork and Guy N. Rothblum. Concentrated differential privacy. arXiv preprint
395"
REFERENCES,0.4485683987274655,"arXiv:1603.01887, 2016.
396"
REFERENCES,0.4496288441145281,"[DRS19] Jinshuo Dong, Aaron Roth, and Weijie J. Su. Gaussian differential privacy. arXiv
397"
REFERENCES,0.45068928950159065,"preprint arXiv:1905.02383, 2019.
398"
REFERENCES,0.4517497348886532,"[DRV10] Cynthia Dwork, Guy N. Rothblum, and Salil Vadhan. Boosting and differential privacy.
399"
REFERENCES,0.4528101802757158,"In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer
400"
REFERENCES,0.45387062566277836,"Science, FOCS ’10, pages 51–60, Washington, DC, USA, 2010. IEEE Computer
401"
REFERENCES,0.45493107104984093,"Society.
402"
REFERENCES,0.4559915164369035,"[GLW21] Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differ-
403"
REFERENCES,0.45705196182396607,"ential privacy. In Advances in Neural Information Processing Systems 34, NeurIPS ’21,
404"
REFERENCES,0.45811240721102864,"pages 11631–11642. Curran Associates, Inc., 2021.
405"
REFERENCES,0.4591728525980912,"[Goo20] Google’s differential privacy libraries. dp accounting library, 2020.
406"
REFERENCES,0.4602332979851538,"[KJH20] Antti Koskela, Joonas Jälkö, and Antti Honkela. Computing tight differential privacy
407"
REFERENCES,0.46129374337221635,"guarantees using fft. In Silvia Chiappa and Roberto Calandra, editors, Proceedings
408"
REFERENCES,0.4623541887592789,"of the Twenty Third International Conference on Artificial Intelligence and Statistics,
409"
REFERENCES,0.4634146341463415,"volume 108 of Proceedings of Machine Learning Research, pages 2560–2569. PMLR,
410"
REFERENCES,0.46447507953340406,"26–28 Aug 2020.
411"
REFERENCES,0.46553552492046657,"[KJPH21] Antti Koskela, Joonas Jälkö, Lukas Prediger, and Antti Honkela. Tight differential
412"
REFERENCES,0.46659597030752914,"privacy for discrete-valued mechanisms and for the subsampled gaussian mechanism
413"
REFERENCES,0.4676564156945917,"using FFT. In AISTATS, volume 130 of Proceedings of Machine Learning Research,
414"
REFERENCES,0.4687168610816543,"pages 3358–3366. PMLR, 2021.
415"
REFERENCES,0.46977730646871685,"[KOV15] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for
416"
REFERENCES,0.4708377518557794,"differential privacy. In Proceedings of the 32nd International Conference on Machine
417"
REFERENCES,0.471898197242842,"Learning, ICML ’15, pages 1376–1385. JMLR, Inc., 2015.
418"
REFERENCES,0.47295864262990456,"[Mir17] Ilya Mironov. Rényi differential privacy. In Proceedings of the 30th IEEE Computer
419"
REFERENCES,0.4740190880169671,"Security Foundations Symposium, CSF ’17, pages 263–275, Washington, DC, USA,
420"
REFERENCES,0.4750795334040297,"2017. IEEE Computer Society.
421"
REFERENCES,0.47613997879109227,"[MTZ19] Ilya Mironov, Kunal Talwar, and Li Zhang. Rényi differential privacy of the sampled
422"
REFERENCES,0.47720042417815484,"gaussian mechanism. arXiv preprint arXiv:1908.10530, 2019.
423"
REFERENCES,0.4782608695652174,"[PHK+23] Natalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Deni-
424"
REFERENCES,0.47932131495228,"son, H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Guha
425"
REFERENCES,0.48038176033934255,"Thakurta. How to dp-fy ML: A practical guide to machine learning with differential
426"
REFERENCES,0.4814422057264051,"privacy. J. Artif. Intell. Res., 77:1113–1201, 2023.
427"
REFERENCES,0.48250265111346763,"[SCS13] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent
428"
REFERENCES,0.4835630965005302,"with differentially private updates. In Proceedings of the 2013 IEEE Global Conference
429"
REFERENCES,0.48462354188759277,"on Signal and Information Processing, GlobalSIP ’13, pages 245–248, Washington,
430"
REFERENCES,0.48568398727465534,"DC, USA, 2013. IEEE Computer Society.
431"
REFERENCES,0.4867444326617179,"[SMM19] David M. Sommer, Sebastian Meiser, and Esfandiar Mohammadi. Privacy loss classes:
432"
REFERENCES,0.4878048780487805,"The central limit theorem in differential privacy. Proc. Priv. Enhancing Technol.,
433"
REFERENCES,0.48886532343584305,"2019(2):245–269, 2019.
434"
REFERENCES,0.4899257688229056,"[Ste22] Thomas Steinke. Composition of differential privacy & privacy amplification by
435"
REFERENCES,0.4909862142099682,"subsampling. arXiv preprint arXiv:2210.00597, 2022.
436"
REFERENCES,0.49204665959703076,"[War65] Stanley L. Warner. Randomized response: A survey technique for eliminating evasive
437"
REFERENCES,0.4931071049840933,"answer bias. Journal of the American Statistical Association, 60(309):63–69, 1965.
438"
REFERENCES,0.4941675503711559,"[WBK20] Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled rényi
439"
REFERENCES,0.49522799575821846,"differential privacy and analytical moments accountant. J. Priv. Confidentiality, 10(2),
440"
REFERENCES,0.49628844114528103,"2020.
441"
REFERENCES,0.4973488865323436,"[WMW+23] Jiachen T Wang, Saeed Mahloujifar, Tong Wu, Ruoxi Jia, and Prateek Mittal. A
442"
REFERENCES,0.4984093319194062,"randomized approach for tight privacy accounting. arXiv preprint arXiv:2304.07927,
443"
REFERENCES,0.49946977730646874,"2023.
444"
REFERENCES,0.5005302226935313,"[YSS+21] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik
445"
REFERENCES,0.5015906680805938,"Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao,
446"
REFERENCES,0.5026511134676565,"Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library
447"
REFERENCES,0.503711558854719,"in PyTorch. arXiv preprint arXiv:2109.12298, 2021.
448"
REFERENCES,0.5047720042417816,"[ZDW22] Yuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang. Optimal accounting of differential
449"
REFERENCES,0.5058324496288441,"privacy via characteristic function. In Gustau Camps-Valls, Francisco J. R. Ruiz, and
450"
REFERENCES,0.5068928950159067,"Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial
451"
REFERENCES,0.5079533404029692,"Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research,
452"
REFERENCES,0.5090137857900318,"pages 4782–4817. PMLR, 28–30 Mar 2022.
453"
REFERENCES,0.5100742311770944,"A
Proof of Proposition 9
454"
REFERENCES,0.5111346765641569,"Without loss of generality, we show both parts for the Gaussian mechanism under the add neighboring
455"
REFERENCES,0.5121951219512195,"relation only.
456"
REFERENCES,0.513255567338282,"We first note that any pair of neighboring datasets with maximum ℓ2-distance is a dominating pair of
457"
REFERENCES,0.5143160127253447,"datasets for the Gaussian mechanism [BW18]. Since the datapoints in our setting are from [−1, 1]
458"
REFERENCES,0.5153764581124072,"this implies that (N(0, σ2), N(1, σ2)) is a dominating pair of distributions for M under ∼A and
459"
REFERENCES,0.5164369034994698,"(N(r, σ2), N(r + 2, σ2)) is a dominating pair of distributions for M under ∼S for any r ∈R. The
460"
REFERENCES,0.5174973488865323,"distance of 2 is obtained by substituting −1 with 1.
461"
REFERENCES,0.5185577942735949,"Now, let us prove part 1 of the proposition. To that end, let D be the all zeros dataset and let D′ be D
462"
REFERENCES,0.5196182396606575,"with a 1 appended to the end. The sum of the subsampled dataset is 1 if the last datapoint is included
463"
REFERENCES,0.5206786850477201,"in the sample and 0 otherwise. As such, we have that
464"
REFERENCES,0.5217391304347826,"MP oisson(D′) = (1 −γ)N(0, σ2) + γN(1, σ2)"
REFERENCES,0.5227995758218452,"Since (N(0, σ2), N(1, σ2)) is a dominating pair of distributions for M under ∼A from Theorem 10
465"
REFERENCES,0.5238600212089077,"we have that
466"
REFERENCES,0.5249204665959704,"(N(0, σ2), (1 −γ)N(0, σ2) + γN(1, σ2)) = (MP oisson(D), MP oisson(D′))"
REFERENCES,0.5259809119830329,"dominates MP oisson under ∼A.
467"
REFERENCES,0.5270413573700954,"As for part 2, let γ := b/n for convenience, let D be the all −1 dataset, let D′ be D with a single −1
468"
REFERENCES,0.528101802757158,"substituted for a 1. We can describe MW OR(D′) by considering the two cases where the 1 is either
469"
REFERENCES,0.5291622481442205,"excluded or included in the batch of size b
470"
REFERENCES,0.5302226935312832,"MW OR(D′) = (1−γ)M(−1, . . . , −1, −1
|
{z
}
b
)+γM(−1, . . . , −1, 1
|
{z
}
b
) = (1−γ)N(−b, σ2)+γN(−b+2, σ2)"
REFERENCES,0.5312831389183457,"Since (N(−b, σ2), N(−b + 2, σ2)) is a dominating pair of distributions for M under ∼S from
471"
REFERENCES,0.5323435843054083,"Theorem 10 we have that
472"
REFERENCES,0.5334040296924708,"(N(−b, σ2), (1 −γ)N(−b, σ2) + γN(−b + 2, σ2)) = (MW OR(D), MW OR(D′))"
REFERENCES,0.5344644750795334,"dominates MW OR under ∼A.
473"
REFERENCES,0.5355249204665959,"The proof for the remove direction is symmetric and the proof for the Laplace mechanism follows
474"
REFERENCES,0.5365853658536586,"from replacing the normal distribution with the Laplace distribution.
475"
REFERENCES,0.5376458112407211,"B
Details for Section 5
476"
REFERENCES,0.5387062566277837,"B.1
Proof of Proposition 11 for Randomized Response
477"
REFERENCES,0.5397667020148462,"Here we show that Proposition 11 holds using a simple mechanism. The mechanism is similar to
478"
REFERENCES,0.5408271474019088,"randomized response [War65] which is used in differential privacy to privately release bits. The
479"
REFERENCES,0.5418875927889714,"mechanism takes a dataset as input and randomly outputs a single bit. The output is weighted towards
480"
REFERENCES,0.542948038176034,"0 if all entries of the dataset are 0 and towards 1 otherwise. Here we use this mechanism for the proof
481"
REFERENCES,0.5440084835630965,"because the calculations and presentation are particularly clean and simple since there are only two
482"
REFERENCES,0.545068928950159,"outputs. A similar proof can be used to verify the accuracy of the estimated plots for the Laplace
483"
REFERENCES,0.5461293743372216,"mechanism presented in Section 5 by calculating the exact hockey-stick divergence at, e.g., ε = 0.25
484"
REFERENCES,0.5471898197242842,"and ε = 1.5.
485"
REFERENCES,0.5482502651113468,"M(D) =
b
with probability 3"
REFERENCES,0.5493107104984093,"4
1 −b
with probability 1"
REFERENCES,0.5503711558854719,"4
where b ∈{0, 1} is 0 if all entries in D are 0 and 1 otherwise.
486"
REFERENCES,0.5514316012725344,"We use the dataset D that consists of all zeroes and D′ is obtained from D by adding a single 1.
487"
REFERENCES,0.5524920466595971,"We will present the proof using MP oisson, but it is the same for MW OR since the only effect on
488"
REFERENCES,0.5535524920466596,"the output distribution is whether or not the 1 is sampled in a batch. We use a sampling probability
489"
REFERENCES,0.5546129374337222,"of γ = 1/2. Since the output distribution of M is symmetric this means that the probability for
490"
REFERENCES,0.5556733828207847,"MP oisson(D′) to output either bit is 1/2 · 3/4 + 1/2 · 1/4 = 1/2. The counterexample occurs when
491"
REFERENCES,0.5567338282078473,"running the mechanism for 2 iterations. There are 4 possible outcomes of the two iterations. The
492"
REFERENCES,0.5577942735949099,"probability of any of these outcomes for MP oisson(D′) is 1/2 · 1/2 = 1/4. For MP oisson(D) the
493"
REFERENCES,0.5588547189819725,"probability we can find the output distribution by considering each distinct outcome
494"
REFERENCES,0.559915164369035,"Pr[MP oisson(D) × MP oisson(D) = (0, 0)] = Pr[MP oisson(D) = 0] · Pr[MP oisson(D) = 0] = 3/4 · 3/4 = 9/16
Pr[MP oisson(D) × MP oisson(D) = (0, 1)] = Pr[MP oisson(D) = 0] · Pr[MP oisson(D) = 1] = 3/4 · 1/4 = 3/16
Pr[MP oisson(D) × MP oisson(D) = (1, 0)] = Pr[MP oisson(D) = 1] · Pr[MP oisson(D) = 0] = 1/4 · 3/4 = 3/16
Pr[MP oisson(D) × MP oisson(D) = (1, 1)] = Pr[MP oisson(D) = 1] · Pr[MP oisson(D) = 1] = 1/4 · 1/4 = 1/16"
REFERENCES,0.5609756097560976,"Now, we find the hockey-stick divergence in both directions for α = 4/3 and α = 2. We denote
495"
REFERENCES,0.5620360551431601,"the two distributions for running the mechanism as P = MP oisson(D) × MP oisson(D) and
496"
REFERENCES,0.5630965005302226,"Q = MP oisson(D′) × MP oisson(D′).
497"
REFERENCES,0.5641569459172853,"H4/3(P||Q) = Pr[P = (0, 0)] −4/3 · Pr[Q = (0, 0)]
= 9/16 −4/3 · 1/4 = 11/48"
REFERENCES,0.5652173913043478,"H4/3(Q||P) = Pr[Q ∈{(0, 1), (1, 0), (1, 1)}] −4/3 · Pr[P ∈{(0, 1), (1, 0), (1, 1)}]
= 3/4 −4/3 · 7/16 = 1/6"
REFERENCES,0.5662778366914104,"H2(P||Q) = Pr[P = (0, 0)] −2 · Pr[Q = (0, 0)]
= 9/16 −2 · 1/4 = 1/16
H2(Q||P) = Pr[Q = (1, 1)] −2 · Pr[P = (1, 1)]
= 1/4 −2 · 1/16 = 1/8"
REFERENCES,0.5673382820784729,"As such, we have that H4/3(P||Q) > H4/3(Q||P) and H2(P||Q) < H2(Q||P).
498"
REFERENCES,0.5683987274655355,"B.2
Details of Monte Carlo Simulation
499"
REFERENCES,0.5694591728525981,"To produce Figure 1, we leverage the PLD framework and apply Monte Carlo simulation.
500"
REFERENCES,0.5705196182396607,"By Proposition 9 and Theorem 8, the privacy curve of the composed and subsampled Laplace
501"
REFERENCES,0.5715800636267232,"mechanism under add (remove) is given by Heε(MP oisson(D)k||MP oisson(D′)k) (vice-versa for
502"
REFERENCES,0.5726405090137858,"remove) where
503"
REFERENCES,0.5737009544008483,"D := (0, . . . , 0)
D′ := (0, . . . , 0, 1)."
REFERENCES,0.574761399787911,"On the other hand, a standard result (e.g. Theorem 3.5 of [GLW21]) asserts that the PLD of a
504"
REFERENCES,0.5758218451749735,"composed mechanism is obtained by self-convolving the PLD of the uncomposed mechanism,
505"
REFERENCES,0.5768822905620361,"namely
506"
REFERENCES,0.5779427359490986,"Heε(MP oisson(D)k||MP oisson(D′)k) = EY ∼LMk
P oisson(D||D′)[1 −eε−Y ]"
REFERENCES,0.5790031813361611,= EY ∼LMP oisson(D||D′)⊕k[1 −eε−Y ].
REFERENCES,0.5800636267232238,"We estimate this expectation via sampling. We know the densities of MP oisson(D) = N(0, σ2) and
507"
REFERENCES,0.5811240721102863,"MP oisson(D′) = (1 −γ)N(0, σ2) + γN(1, σ2), so we can quickly sample LMP oisson(D||D′). By
508"
REFERENCES,0.5821845174973489,"drawing k samples and summing them, we can sample LMP oisson(D||D′)⊕k as well. Therefore, we
509"
REFERENCES,0.5832449628844114,"can draw Yi ∼LMP oisson(D||D′)k for 1 ≤i ≤N, then compute the Monte Carlo estimate
510"
N,0.584305408271474,"1
N N
X"
N,0.5853658536585366,"i=1
(1 −eε−Yi)."
N,0.5864262990455992,"As for the error, the quantity inside the expectation is bounded in [0, 1], so we can apply Höffding as
511"
N,0.5874867444326617,"well as the union bound. In this case,
512"
N,0.5885471898197243,"N =
ln(2|E|/β) 2α2 "
N,0.5896076352067868,"samples will suffice to ensure that the Monte Carlo estimate of Heε(MP oisson(D)||MP oisson(D′))
513"
N,0.5906680805938495,"is accurate within α, with probability 1 −β, for all ε ∈E simultaneously.
514"
N,0.591728525980912,"For Figure 1, we chose α = 0.001 and β = 0.01 and considered |E| = 40 values of ε, which required
515"
N,0.5927889713679746,"N = 3, 342, 306 samples. This value of α is small enough relative to the plot that our conclusion
516"
N,0.5938494167550371,"holds with probability 99%.
517"
N,0.5949098621420997,"C
Proof of Proposition 13
518"
N,0.5959703075291622,"The proof relies mainly on the following data-processing inequality, which can also be seen as closure
519"
N,0.5970307529162248,"of privacy under post-processing.
520"
N,0.5980911983032874,"Lemma 15. Let P and Q be any distributions on X and let Proc : X →Y be a randomized
521"
N,0.5991516436903499,"procedure. Denote by Proc P the distribution of Proc(X) for X ∼P. Then, for any α ≥0,
522"
N,0.6002120890774125,Hα(Proc P|| Proc Q) ≤Hα(P||Q).
N,0.601272534464475,"Proof. For any event E ⊆Y,
523"
N,0.6023329798515377,(Proc P)(E) −α(Proc Q)(E) = EProc[PX∼P (Proc(X) ∈E)] −αEProc[PX∼Q(Proc(X) ∈E)]
N,0.6033934252386002,= EProc[P(Proc−1(E))] −αEProc[Q(Proc−1(E)]
N,0.6044538706256628,"= EProc[P(Proc−1(E)) −αQ(Proc−1(E)]
≤EProc[Hα(P||Q)]
= Hα(P||Q)"
N,0.6055143160127253,"and the result holds since
524"
N,0.6065747613997879,"Hα(Proc P|| Proc Q) = sup
E⊆Y
(Proc P)(E) −α(Proc Q)(E). 525"
N,0.6076352067868505,"We now prove the proposition. Our main goal is to argue that D := (0, . . . , 0, 1) and D′ :=
526"
N,0.6086956521739131,"(0, . . . , 0, −1) form a dominating pair of datasets for MP oisson.
To that end, consider any
527"
N,0.6097560975609756,"∼S-neighbors that differ, without loss of generality, in the last entry, say (x, a) and (x, a′).
528"
N,0.6108165429480382,"We leverage postprocessing to show that (MP oisson(x, a), MP oisson(x, a′)) is dominated by
529"
N,0.6118769883351007,"(MP oisson(0, a), MP oisson(0, a′)). Indeed, consider
530"
N,0.6129374337221634,Proc(y) := y +
N,0.6139978791092259,"|ˆx|
X"
N,0.6150583244962884,"i=1
ˆxi"
N,0.616118769883351,"where ˆx is randomly drawn from x by Poisson(γ)-subsampling. Now, sampling MP oisson(0, a) is
531"
N,0.6171792152704135,"equivalent to drawing ˆa from the singleton dataset (a) via Poisson(γ) and returning a sample from
532"
N,0.6182396606574762,"N(P|ˆa|
i=1 ˆai, σ2). Since the normal distribution satisfies N(a, σ2) + b = N(a + b, σ2), sampling
533"
N,0.6193001060445387,"Proc(MP oisson(0, a)) is equivalent to sampling
534 N  "
N,0.6203605514316013,"|ˆx|
X"
N,0.6214209968186638,"i=1
ˆxi +"
N,0.6224814422057264,"|ˆa|
X"
N,0.623541887592789,"i=1
ˆai, σ2  "
N,0.6246023329798516,"where ˆx is Poisson(γ)-subsampled from x and ˆa is Poisson(γ)-subsampled from (a).
But,
535"
N,0.6256627783669141,"by independence, (ˆx, ˆa) is a Poisson(γ)-subsample drawn from (x, a), so, in conclusion,
536"
N,0.6267232237539767,"Proc(MP oisson(0, a))
=
MP oisson(x, a).
By an analogous argument, we have that
537"
N,0.6277836691410392,"Proc(MP oisson(0, a′)) = MP oisson(x, a′) and hence
538"
N,0.6288441145281018,"Hα(MP oisson(x, a)||MP oisson(x, a′)) = Hα(Proc(MP oisson(0, a))|| Proc(MP oisson(0, a′)))"
N,0.6299045599151644,"≤Hα(MP oisson(0, a)||MP oisson(0, a′)) (Lemma 15)
≤Hα(MP oisson(0, 1)||MP oisson(0, −1))."
N,0.630965005302227,"D
Constructing a Dominating Pair of Distributions for the Gaussian
539"
N,0.6320254506892895,"Mechanism
540"
N,0.633085896076352,"In this section we consider the problem of computing privacy curves for the Gaussian mechanism
541"
N,0.6341463414634146,"under ∼S when sampling without replacement. As shown in Section 7 computing tight parameters is
542"
N,0.6352067868504772,"challenging in this setting because we do not know which datasets result in the largest hockey-stick
543"
N,0.6362672322375398,"divergence. However, we can still compute an upper bound on the privacy curve using a dominating
544"
N,0.6373276776246023,"pair of distributions.
545"
N,0.6383881230116649,"We modified the implementation of the algorithm introduced by [DGK+22] in the Google DP library
546"
N,0.6394485683987274,"to construct the PLDs (Privacy Loss Distribution object). The algorithm constructs an approximation
547"
N,0.6405090137857901,"of the PLD from the hockey-stick divergence between the pair of distributions at a range of values
548"
N,0.6415694591728526,"for ε. From Theorem 14 we know that the direction of the pair of distributions yielding the largest
549"
N,0.6426299045599152,"hockey-stick divergence for the mechanism of a single iteration differs for α below and above 1. We
550"
N,0.6436903499469777,"construct a new PLD by combining the two directions at α = 1 or ε = 0.
551"
N,0.6447507953340403,"See the left-side plot of Figure 4 for a visualization of how our construction uses the point-wise
552"
N,0.6458112407211029,"maximum of the hockey-stick divergence for a single iteration. This construction represents a
553"
N,0.6468716861081655,"dominating pair of distributions and as such it is sufficient to find a dominating pair of distributions
554"
N,0.647932131495228,"for the composed mechanism using self-composition by Theorem 8.
555"
N,0.6489925768822906,"The right-side plot of Figure 4 shows the privacy curve obtained from self-composing the PLD for
556"
N,0.6500530222693531,"the dominating pair of distributions with parameters σ = 4, γ = 0.05, and 1000 iterations. The blue
557"
N,0.6511134676564156,"line is the privacy curve under ∼R and also serves as a lower bound for the true privacy curve. Note
558"
N,0.6521739130434783,"that the orange line would also be the privacy curve achieved by this technique under the add/remove
559"
N,0.6532343584305408,"relation if we did not consider the add and remove relations separately.
560"
N,0.6542948038176034,"The gap between the upper and lower bound motivates future work for understanding the worst-case
561"
N,0.6553552492046659,"datasets. Similar to the add/remove case we conjecture that the subsampled Gaussian mechanism
562"
N,0.6564156945917285,"behaves well under composision. Specifically, we conjecture that the privacy curve of the composed
563"
N,0.6574761399787911,"subsampled Gaussian mechanism under ∼S matches the curve under ∼R for ε ≥0. It seems likely
564"
N,0.6585365853658537,"that this is the case if Conjecture 12 holds. However, if Conjecture 12 does not hold the above
565"
N,0.6595970307529162,"statement also does not hold.
566"
N,0.6606574761399788,"Figure 4: Hockey-stick divergence for the Gaussian mechanism under substitution when sampling
without replacement using a dominating pair of distributions. The dominating pair of distributions
is constructed using a point-wise maximum of the privacy curve for a single iteration as seen in
the left plot. The right plot compares the privacy curve from self-composing the dominating pair
of distributions with a lower bound obtained from self-composing the PLD that corresponds to the
blue line in the left plot. The dotted line for the RDP accountant is used for reference of scale. The
difference between the blue and the dotted line corresponds to the difference between using the PLD
and RDP accountants for Poisson subsampling under add/remove."
N,0.6617179215270413,"NeurIPS Paper Checklist
567"
CLAIMS,0.662778366914104,"1. Claims
568"
CLAIMS,0.6638388123011665,"Question: Do the main claims made in the abstract and introduction accurately reflect the
569"
CLAIMS,0.6648992576882291,"paper’s contributions and scope?
570"
CLAIMS,0.6659597030752916,"Answer: [Yes]
571"
CLAIMS,0.6670201484623541,"Justification: A provide a comprehensive list of contribitions at the end of the introduction.
572"
CLAIMS,0.6680805938494168,"A summary is given in the abstract.
573"
CLAIMS,0.6691410392364793,"Guidelines:
574"
CLAIMS,0.6702014846235419,"• The answer NA means that the abstract and introduction do not include the claims
575"
CLAIMS,0.6712619300106044,"made in the paper.
576"
CLAIMS,0.672322375397667,"• The abstract and/or introduction should clearly state the claims made, including the
577"
CLAIMS,0.6733828207847296,"contributions made in the paper and important assumptions and limitations. A No or
578"
CLAIMS,0.6744432661717922,"NA answer to this question will not be perceived well by the reviewers.
579"
CLAIMS,0.6755037115588547,"• The claims made should match theoretical and experimental results, and reflect how
580"
CLAIMS,0.6765641569459173,"much the results can be expected to generalize to other settings.
581"
CLAIMS,0.6776246023329798,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
582"
CLAIMS,0.6786850477200425,"are not attained by the paper.
583"
LIMITATIONS,0.679745493107105,"2. Limitations
584"
LIMITATIONS,0.6808059384941676,"Question: Does the paper discuss the limitations of the work performed by the authors?
585"
LIMITATIONS,0.6818663838812301,"Answer: [Yes]
586"
LIMITATIONS,0.6829268292682927,"Justification: The main limitation of our work is expressed in Conjecture 12.
587"
LIMITATIONS,0.6839872746553552,"Guidelines:
588"
LIMITATIONS,0.6850477200424178,"• The answer NA means that the paper has no limitation while the answer No means that
589"
LIMITATIONS,0.6861081654294804,"the paper has limitations, but those are not discussed in the paper.
590"
LIMITATIONS,0.6871686108165429,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
591"
LIMITATIONS,0.6882290562036055,"• The paper should point out any strong assumptions and how robust the results are to
592"
LIMITATIONS,0.689289501590668,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
593"
LIMITATIONS,0.6903499469777307,"model well-specification, asymptotic approximations only holding locally). The authors
594"
LIMITATIONS,0.6914103923647932,"should reflect on how these assumptions might be violated in practice and what the
595"
LIMITATIONS,0.6924708377518558,"implications would be.
596"
LIMITATIONS,0.6935312831389183,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
597"
LIMITATIONS,0.694591728525981,"only tested on a few datasets or with a few runs. In general, empirical results often
598"
LIMITATIONS,0.6956521739130435,"depend on implicit assumptions, which should be articulated.
599"
LIMITATIONS,0.6967126193001061,"• The authors should reflect on the factors that influence the performance of the approach.
600"
LIMITATIONS,0.6977730646871686,"For example, a facial recognition algorithm may perform poorly when image resolution
601"
LIMITATIONS,0.6988335100742312,"is low or images are taken in low lighting. Or a speech-to-text system might not be
602"
LIMITATIONS,0.6998939554612937,"used reliably to provide closed captions for online lectures because it fails to handle
603"
LIMITATIONS,0.7009544008483564,"technical jargon.
604"
LIMITATIONS,0.7020148462354189,"• The authors should discuss the computational efficiency of the proposed algorithms
605"
LIMITATIONS,0.7030752916224814,"and how they scale with dataset size.
606"
LIMITATIONS,0.704135737009544,"• If applicable, the authors should discuss possible limitations of their approach to
607"
LIMITATIONS,0.7051961823966065,"address problems of privacy and fairness.
608"
LIMITATIONS,0.7062566277836692,"• While the authors might fear that complete honesty about limitations might be used by
609"
LIMITATIONS,0.7073170731707317,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
610"
LIMITATIONS,0.7083775185577943,"limitations that aren’t acknowledged in the paper. The authors should use their best
611"
LIMITATIONS,0.7094379639448568,"judgment and recognize that individual actions in favor of transparency play an impor-
612"
LIMITATIONS,0.7104984093319194,"tant role in developing norms that preserve the integrity of the community. Reviewers
613"
LIMITATIONS,0.711558854718982,"will be specifically instructed to not penalize honesty concerning limitations.
614"
THEORY ASSUMPTIONS AND PROOFS,0.7126193001060446,"3. Theory Assumptions and Proofs
615"
THEORY ASSUMPTIONS AND PROOFS,0.7136797454931071,"Question: For each theoretical result, does the paper provide the full set of assumptions and
616"
THEORY ASSUMPTIONS AND PROOFS,0.7147401908801697,"a complete (and correct) proof?
617"
THEORY ASSUMPTIONS AND PROOFS,0.7158006362672322,"Answer: [Yes]
618"
THEORY ASSUMPTIONS AND PROOFS,0.7168610816542949,"Justification: Each theoretical result is indicated as a proposition (theorems indicate prior
619"
THEORY ASSUMPTIONS AND PROOFS,0.7179215270413574,"work). A proof for each result can be found in the appropriate appendix section (references
620"
THEORY ASSUMPTIONS AND PROOFS,0.71898197242842,"given in main body).
621"
THEORY ASSUMPTIONS AND PROOFS,0.7200424178154825,"Guidelines:
622"
THEORY ASSUMPTIONS AND PROOFS,0.721102863202545,"• The answer NA means that the paper does not include theoretical results.
623"
THEORY ASSUMPTIONS AND PROOFS,0.7221633085896076,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
624"
THEORY ASSUMPTIONS AND PROOFS,0.7232237539766702,"referenced.
625"
THEORY ASSUMPTIONS AND PROOFS,0.7242841993637328,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
626"
THEORY ASSUMPTIONS AND PROOFS,0.7253446447507953,"• The proofs can either appear in the main paper or the supplemental material, but if
627"
THEORY ASSUMPTIONS AND PROOFS,0.7264050901378579,"they appear in the supplemental material, the authors are encouraged to provide a short
628"
THEORY ASSUMPTIONS AND PROOFS,0.7274655355249204,"proof sketch to provide intuition.
629"
THEORY ASSUMPTIONS AND PROOFS,0.7285259809119831,"• Inversely, any informal proof provided in the core of the paper should be complemented
630"
THEORY ASSUMPTIONS AND PROOFS,0.7295864262990456,"by formal proofs provided in appendix or supplemental material.
631"
THEORY ASSUMPTIONS AND PROOFS,0.7306468716861082,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7317073170731707,"4. Experimental Result Reproducibility
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7327677624602333,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7338282078472959,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7348886532343585,"of the paper (regardless of whether the code and data are provided or not)?
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.735949098621421,"Answer: [Yes]
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7370095440084835,"Justification: Details for Monte Carlo simulation results (Figures 1 and 3) are in the appendix.
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7380699893955461,"Other experimental results can be obtained by straightforward modification of publicly
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7391304347826086,"available privacy accounting software.
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7401908801696713,"Guidelines:
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7412513255567338,"• The answer NA means that the paper does not include experiments.
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7423117709437964,"• If the paper includes experiments, a No answer to this question will not be perceived
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7433722163308589,"well by the reviewers: Making the paper reproducible is important, regardless of
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7444326617179216,"whether the code and data are provided or not.
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7454931071049841,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7465535524920467,"to make their results reproducible or verifiable.
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7476139978791092,"• Depending on the contribution, reproducibility can be accomplished in various ways.
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7486744432661718,"For example, if the contribution is a novel architecture, describing the architecture fully
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497348886532343,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.750795334040297,"be necessary to either make it possible for others to replicate the model with the same
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7518557794273595,"dataset, or provide access to the model. In general. releasing code and data is often
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7529162248144221,"one good way to accomplish this, but reproducibility can also be provided via detailed
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7539766702014846,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7550371155885471,"of a large language model), releasing of a model checkpoint, or other means that are
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7560975609756098,"appropriate to the research performed.
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7571580063626723,"• While NeurIPS does not require releasing code, the conference does require all submis-
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7582184517497349,"sions to provide some reasonable avenue for reproducibility, which may depend on the
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7592788971367974,"nature of the contribution. For example
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76033934252386,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7613997879109226,"to reproduce that algorithm.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7624602332979852,"(b) If the contribution is primarily a new model architecture, the paper should describe
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7635206786850477,"the architecture clearly and fully.
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7645811240721103,"(c) If the contribution is a new model (e.g., a large language model), then there should
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7656415694591728,"either be a way to access this model for reproducing the results or a way to reproduce
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7667020148462355,"the model (e.g., with an open-source dataset or instructions for how to construct
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.767762460233298,"the dataset).
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7688229056203606,"(d) We recognize that reproducibility may be tricky in some cases, in which case
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7698833510074231,"authors are welcome to describe the particular way they provide for reproducibility.
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7709437963944857,"In the case of closed-source models, it may be that access to the model is limited in
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7720042417815483,"some way (e.g., to registered users), but it should be possible for other researchers
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7730646871686108,"to have some path to reproducing or verifying the results.
672"
OPEN ACCESS TO DATA AND CODE,0.7741251325556734,"5. Open access to data and code
673"
OPEN ACCESS TO DATA AND CODE,0.7751855779427359,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
674"
OPEN ACCESS TO DATA AND CODE,0.7762460233297985,"tions to faithfully reproduce the main experimental results, as described in supplemental
675"
OPEN ACCESS TO DATA AND CODE,0.777306468716861,"material?
676"
OPEN ACCESS TO DATA AND CODE,0.7783669141039237,"Answer: [No]
677"
OPEN ACCESS TO DATA AND CODE,0.7794273594909862,"Justification: See previous justification. Instructions to reproduce Monte Carlo simulation
678"
OPEN ACCESS TO DATA AND CODE,0.7804878048780488,"results are included in the appendix. Other results rely on open-source code.
679"
OPEN ACCESS TO DATA AND CODE,0.7815482502651113,"Guidelines:
680"
OPEN ACCESS TO DATA AND CODE,0.782608695652174,"• The answer NA means that paper does not include experiments requiring code.
681"
OPEN ACCESS TO DATA AND CODE,0.7836691410392365,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
682"
OPEN ACCESS TO DATA AND CODE,0.7847295864262991,"public/guides/CodeSubmissionPolicy) for more details.
683"
OPEN ACCESS TO DATA AND CODE,0.7857900318133616,"• While we encourage the release of code and data, we understand that this might not be
684"
OPEN ACCESS TO DATA AND CODE,0.7868504772004242,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
685"
OPEN ACCESS TO DATA AND CODE,0.7879109225874867,"including code, unless this is central to the contribution (e.g., for a new open-source
686"
OPEN ACCESS TO DATA AND CODE,0.7889713679745494,"benchmark).
687"
OPEN ACCESS TO DATA AND CODE,0.7900318133616119,"• The instructions should contain the exact command and environment needed to run to
688"
OPEN ACCESS TO DATA AND CODE,0.7910922587486744,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
689"
OPEN ACCESS TO DATA AND CODE,0.792152704135737,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
690"
OPEN ACCESS TO DATA AND CODE,0.7932131495227995,"• The authors should provide instructions on data access and preparation, including how
691"
OPEN ACCESS TO DATA AND CODE,0.7942735949098622,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
692"
OPEN ACCESS TO DATA AND CODE,0.7953340402969247,"• The authors should provide scripts to reproduce all experimental results for the new
693"
OPEN ACCESS TO DATA AND CODE,0.7963944856839873,"proposed method and baselines. If only a subset of experiments are reproducible, they
694"
OPEN ACCESS TO DATA AND CODE,0.7974549310710498,"should state which ones are omitted from the script and why.
695"
OPEN ACCESS TO DATA AND CODE,0.7985153764581124,"• At submission time, to preserve anonymity, the authors should release anonymized
696"
OPEN ACCESS TO DATA AND CODE,0.799575821845175,"versions (if applicable).
697"
OPEN ACCESS TO DATA AND CODE,0.8006362672322376,"• Providing as much information as possible in supplemental material (appended to the
698"
OPEN ACCESS TO DATA AND CODE,0.8016967126193001,"paper) is recommended, but including URLs to data and code is permitted.
699"
OPEN ACCESS TO DATA AND CODE,0.8027571580063627,"6. Experimental Setting/Details
700"
OPEN ACCESS TO DATA AND CODE,0.8038176033934252,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
701"
OPEN ACCESS TO DATA AND CODE,0.8048780487804879,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
702"
OPEN ACCESS TO DATA AND CODE,0.8059384941675504,"results?
703"
OPEN ACCESS TO DATA AND CODE,0.806998939554613,"Answer: [Yes]
704"
OPEN ACCESS TO DATA AND CODE,0.8080593849416755,"Justification: Simulation results rely on a choice of sample size, which is explained in the
705"
OPEN ACCESS TO DATA AND CODE,0.809119830328738,"appendix.
706"
OPEN ACCESS TO DATA AND CODE,0.8101802757158006,"Guidelines:
707"
OPEN ACCESS TO DATA AND CODE,0.8112407211028632,"• The answer NA means that the paper does not include experiments.
708"
OPEN ACCESS TO DATA AND CODE,0.8123011664899258,"• The experimental setting should be presented in the core of the paper to a level of detail
709"
OPEN ACCESS TO DATA AND CODE,0.8133616118769883,"that is necessary to appreciate the results and make sense of them.
710"
OPEN ACCESS TO DATA AND CODE,0.8144220572640509,"• The full details can be provided either with the code, in appendix, or as supplemental
711"
OPEN ACCESS TO DATA AND CODE,0.8154825026511134,"material.
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8165429480381761,"7. Experiment Statistical Significance
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8176033934252386,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8186638388123012,"information about the statistical significance of the experiments?
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8197242841993637,"Answer: [Yes]
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8207847295864263,"Justification: An analysis of sample size and the associated error is included in the appendix.
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8218451749734889,"The error is very small compared to the plots due to the high sample size, so we did not
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8229056203605515,"explicitly include them in simulation plots.
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.823966065747614,"Guidelines:
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8250265111346765,"• The answer NA means that the paper does not include experiments.
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8260869565217391,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8271474019088016,"dence intervals, or statistical significance tests, at least for the experiments that support
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8282078472958643,"the main claims of the paper.
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8292682926829268,"• The factors of variability that the error bars are capturing should be clearly stated (for
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8303287380699894,"example, train/test split, initialization, random drawing of some parameter, or overall
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8313891834570519,"run with given experimental conditions).
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8324496288441146,"• The method for calculating the error bars should be explained (closed form formula,
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8335100742311771,"call to a library function, bootstrap, etc.)
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8345705196182397,"• The assumptions made should be given (e.g., Normally distributed errors).
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8356309650053022,"• It should be clear whether the error bar is the standard deviation or the standard error
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8366914103923648,"of the mean.
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8377518557794273,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.83881230116649,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8398727465535525,"of Normality of errors is not verified.
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8409331919406151,"• For asymmetric distributions, the authors should be careful not to show in tables or
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8419936373276776,"figures symmetric error bars that would yield results that are out of range (e.g. negative
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8430540827147401,"error rates).
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8441145281018028,"• If error bars are reported in tables or plots, The authors should explain in the text how
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8451749734888653,"they were calculated and reference the corresponding figures or tables in the text.
740"
EXPERIMENTS COMPUTE RESOURCES,0.8462354188759279,"8. Experiments Compute Resources
741"
EXPERIMENTS COMPUTE RESOURCES,0.8472958642629904,"Question: For each experiment, does the paper provide sufficient information on the com-
742"
EXPERIMENTS COMPUTE RESOURCES,0.848356309650053,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
743"
EXPERIMENTS COMPUTE RESOURCES,0.8494167550371156,"the experiments?
744"
EXPERIMENTS COMPUTE RESOURCES,0.8504772004241782,"Answer: [NA]
745"
EXPERIMENTS COMPUTE RESOURCES,0.8515376458112407,"Justification: Experiments required minimal compute resources, so we do not report details.
746"
EXPERIMENTS COMPUTE RESOURCES,0.8525980911983033,"Guidelines:
747"
EXPERIMENTS COMPUTE RESOURCES,0.8536585365853658,"• The answer NA means that the paper does not include experiments.
748"
EXPERIMENTS COMPUTE RESOURCES,0.8547189819724285,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
749"
EXPERIMENTS COMPUTE RESOURCES,0.855779427359491,"or cloud provider, including relevant memory and storage.
750"
EXPERIMENTS COMPUTE RESOURCES,0.8568398727465536,"• The paper should provide the amount of compute required for each of the individual
751"
EXPERIMENTS COMPUTE RESOURCES,0.8579003181336161,"experimental runs as well as estimate the total compute.
752"
EXPERIMENTS COMPUTE RESOURCES,0.8589607635206787,"• The paper should disclose whether the full research project required more compute
753"
EXPERIMENTS COMPUTE RESOURCES,0.8600212089077413,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
754"
EXPERIMENTS COMPUTE RESOURCES,0.8610816542948038,"didn’t make it into the paper).
755"
CODE OF ETHICS,0.8621420996818664,"9. Code Of Ethics
756"
CODE OF ETHICS,0.8632025450689289,"Question: Does the research conducted in the paper conform, in every respect, with the
757"
CODE OF ETHICS,0.8642629904559915,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
758"
CODE OF ETHICS,0.865323435843054,"Answer: [Yes]
759"
CODE OF ETHICS,0.8663838812301167,"Justification: We reviewed the guidelines and found no violations in our work.
760"
CODE OF ETHICS,0.8674443266171792,"Guidelines:
761"
CODE OF ETHICS,0.8685047720042418,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
762"
CODE OF ETHICS,0.8695652173913043,"• If the authors answer No, they should explain the special circumstances that require a
763"
CODE OF ETHICS,0.870625662778367,"deviation from the Code of Ethics.
764"
CODE OF ETHICS,0.8716861081654295,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
765"
CODE OF ETHICS,0.8727465535524921,"eration due to laws or regulations in their jurisdiction).
766"
BROADER IMPACTS,0.8738069989395546,"10. Broader Impacts
767"
BROADER IMPACTS,0.8748674443266172,"Question: Does the paper discuss both potential positive societal impacts and negative
768"
BROADER IMPACTS,0.8759278897136797,"societal impacts of the work performed?
769"
BROADER IMPACTS,0.8769883351007424,"Answer: [No]
770"
BROADER IMPACTS,0.8780487804878049,"Justification: The aim of the work is to bring attention among practitioners and theoreticians
771"
BROADER IMPACTS,0.8791092258748674,"to the limitations of privacy accountants. There is no foreseeable path to negative broad
772"
BROADER IMPACTS,0.88016967126193,"societal impact. On the other hand improving privacy accountants may lead to wider
773"
BROADER IMPACTS,0.8812301166489925,"deployment of private machine learning, which can be expected to have a positive societal
774"
BROADER IMPACTS,0.8822905620360552,"impact. We briefly discuss this outcome in the introduction in order to motivate our work.
775"
BROADER IMPACTS,0.8833510074231177,"Guidelines:
776"
BROADER IMPACTS,0.8844114528101803,"• The answer NA means that there is no societal impact of the work performed.
777"
BROADER IMPACTS,0.8854718981972428,"• If the authors answer NA or No, they should explain why their work has no societal
778"
BROADER IMPACTS,0.8865323435843054,"impact or why the paper does not address societal impact.
779"
BROADER IMPACTS,0.887592788971368,"• Examples of negative societal impacts include potential malicious or unintended uses
780"
BROADER IMPACTS,0.8886532343584306,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
781"
BROADER IMPACTS,0.8897136797454931,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
782"
BROADER IMPACTS,0.8907741251325557,"groups), privacy considerations, and security considerations.
783"
BROADER IMPACTS,0.8918345705196182,"• The conference expects that many papers will be foundational research and not tied
784"
BROADER IMPACTS,0.8928950159066809,"to particular applications, let alone deployments. However, if there is a direct path to
785"
BROADER IMPACTS,0.8939554612937434,"any negative applications, the authors should point it out. For example, it is legitimate
786"
BROADER IMPACTS,0.8950159066808059,"to point out that an improvement in the quality of generative models could be used to
787"
BROADER IMPACTS,0.8960763520678685,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
788"
BROADER IMPACTS,0.897136797454931,"that a generic algorithm for optimizing neural networks could enable people to train
789"
BROADER IMPACTS,0.8981972428419936,"models that generate Deepfakes faster.
790"
BROADER IMPACTS,0.8992576882290562,"• The authors should consider possible harms that could arise when the technology is
791"
BROADER IMPACTS,0.9003181336161188,"being used as intended and functioning correctly, harms that could arise when the
792"
BROADER IMPACTS,0.9013785790031813,"technology is being used as intended but gives incorrect results, and harms following
793"
BROADER IMPACTS,0.9024390243902439,"from (intentional or unintentional) misuse of the technology.
794"
BROADER IMPACTS,0.9034994697773064,"• If there are negative societal impacts, the authors could also discuss possible mitigation
795"
BROADER IMPACTS,0.9045599151643691,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
796"
BROADER IMPACTS,0.9056203605514316,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
797"
BROADER IMPACTS,0.9066808059384942,"feedback over time, improving the efficiency and accessibility of ML).
798"
SAFEGUARDS,0.9077412513255567,"11. Safeguards
799"
SAFEGUARDS,0.9088016967126193,"Question: Does the paper describe safeguards that have been put in place for responsible
800"
SAFEGUARDS,0.9098621420996819,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
801"
SAFEGUARDS,0.9109225874867445,"image generators, or scraped datasets)?
802"
SAFEGUARDS,0.911983032873807,"Answer: [NA]
803"
SAFEGUARDS,0.9130434782608695,"Justification: N/A
804"
SAFEGUARDS,0.9141039236479321,"Guidelines:
805"
SAFEGUARDS,0.9151643690349947,"• The answer NA means that the paper poses no such risks.
806"
SAFEGUARDS,0.9162248144220573,"• Released models that have a high risk for misuse or dual-use should be released with
807"
SAFEGUARDS,0.9172852598091198,"necessary safeguards to allow for controlled use of the model, for example by requiring
808"
SAFEGUARDS,0.9183457051961824,"that users adhere to usage guidelines or restrictions to access the model or implementing
809"
SAFEGUARDS,0.9194061505832449,"safety filters.
810"
SAFEGUARDS,0.9204665959703076,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
811"
SAFEGUARDS,0.9215270413573701,"should describe how they avoided releasing unsafe images.
812"
SAFEGUARDS,0.9225874867444327,"• We recognize that providing effective safeguards is challenging, and many papers do
813"
SAFEGUARDS,0.9236479321314952,"not require this, but we encourage authors to take this into account and make a best
814"
SAFEGUARDS,0.9247083775185578,"faith effort.
815"
LICENSES FOR EXISTING ASSETS,0.9257688229056203,"12. Licenses for existing assets
816"
LICENSES FOR EXISTING ASSETS,0.926829268292683,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
817"
LICENSES FOR EXISTING ASSETS,0.9278897136797455,"the paper, properly credited and are the license and terms of use explicitly mentioned and
818"
LICENSES FOR EXISTING ASSETS,0.9289501590668081,"properly respected?
819"
LICENSES FOR EXISTING ASSETS,0.9300106044538706,"Answer: [Yes]
820"
LICENSES FOR EXISTING ASSETS,0.9310710498409331,"Justification: Credit is given as needed to open-source software repositories.
821"
LICENSES FOR EXISTING ASSETS,0.9321314952279958,"Guidelines:
822"
LICENSES FOR EXISTING ASSETS,0.9331919406150583,"• The answer NA means that the paper does not use existing assets.
823"
LICENSES FOR EXISTING ASSETS,0.9342523860021209,"• The authors should cite the original paper that produced the code package or dataset.
824"
LICENSES FOR EXISTING ASSETS,0.9353128313891834,"• The authors should state which version of the asset is used and, if possible, include a
825"
LICENSES FOR EXISTING ASSETS,0.936373276776246,"URL.
826"
LICENSES FOR EXISTING ASSETS,0.9374337221633086,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
827"
LICENSES FOR EXISTING ASSETS,0.9384941675503712,"• For scraped data from a particular source (e.g., website), the copyright and terms of
828"
LICENSES FOR EXISTING ASSETS,0.9395546129374337,"service of that source should be provided.
829"
LICENSES FOR EXISTING ASSETS,0.9406150583244963,"• If assets are released, the license, copyright information, and terms of use in the
830"
LICENSES FOR EXISTING ASSETS,0.9416755037115588,"package should be provided. For popular datasets, paperswithcode.com/datasets
831"
LICENSES FOR EXISTING ASSETS,0.9427359490986215,"has curated licenses for some datasets. Their licensing guide can help determine the
832"
LICENSES FOR EXISTING ASSETS,0.943796394485684,"license of a dataset.
833"
LICENSES FOR EXISTING ASSETS,0.9448568398727466,"• For existing datasets that are re-packaged, both the original license and the license of
834"
LICENSES FOR EXISTING ASSETS,0.9459172852598091,"the derived asset (if it has changed) should be provided.
835"
LICENSES FOR EXISTING ASSETS,0.9469777306468717,"• If this information is not available online, the authors are encouraged to reach out to
836"
LICENSES FOR EXISTING ASSETS,0.9480381760339343,"the asset’s creators.
837"
NEW ASSETS,0.9490986214209968,"13. New Assets
838"
NEW ASSETS,0.9501590668080594,"Question: Are new assets introduced in the paper well documented and is the documentation
839"
NEW ASSETS,0.9512195121951219,"provided alongside the assets?
840"
NEW ASSETS,0.9522799575821845,"Answer: [NA]
841"
NEW ASSETS,0.953340402969247,"Justification: N/A
842"
NEW ASSETS,0.9544008483563097,"Guidelines:
843"
NEW ASSETS,0.9554612937433722,"• The answer NA means that the paper does not release new assets.
844"
NEW ASSETS,0.9565217391304348,"• Researchers should communicate the details of the dataset/code/model as part of their
845"
NEW ASSETS,0.9575821845174973,"submissions via structured templates. This includes details about training, license,
846"
NEW ASSETS,0.95864262990456,"limitations, etc.
847"
NEW ASSETS,0.9597030752916225,"• The paper should discuss whether and how consent was obtained from people whose
848"
NEW ASSETS,0.9607635206786851,"asset is used.
849"
NEW ASSETS,0.9618239660657476,"• At submission time, remember to anonymize your assets (if applicable). You can either
850"
NEW ASSETS,0.9628844114528102,"create an anonymized URL or include an anonymized zip file.
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9639448568398727,"14. Crowdsourcing and Research with Human Subjects
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9650053022269353,"Question: For crowdsourcing experiments and research with human subjects, does the paper
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9660657476139979,"include the full text of instructions given to participants and screenshots, if applicable, as
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9671261930010604,"well as details about compensation (if any)?
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.968186638388123,"Answer: [NA]
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9692470837751855,"Justification: N/A
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703075291622482,"Guidelines:
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713679745493107,"• The answer NA means that the paper does not involve crowdsourcing nor research with
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724284199363733,"human subjects.
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9734888653234358,"• Including this information in the supplemental material is fine, but if the main contribu-
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9745493107104984,"tion of the paper involves human subjects, then as much detail as possible should be
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975609756097561,"included in the main paper.
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766702014846236,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777306468716861,"or other labor should be paid at least the minimum wage in the country of the data
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787910922587487,"collector.
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798515376458112,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809119830328739,"Subjects
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819724284199364,"Question: Does the paper describe potential risks incurred by study participants, whether
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830328738069989,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840933191940615,"approvals (or an equivalent approval/review based on the requirements of your country or
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985153764581124,"institution) were obtained?
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862142099681867,"Answer: [NA]
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872746553552492,"Justification: N/A
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883351007423118,"Guidelines:
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893955461293743,"• The answer NA means that the paper does not involve crowdsourcing nor research with
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904559915164369,"human subjects.
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915164369034994,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925768822905621,"may be required for any human subjects research. If you obtained IRB approval, you
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936373276776246,"should clearly state this in the paper.
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946977730646872,"• We recognize that the procedures for this may vary significantly between institutions
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957582184517497,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968186638388123,"guidelines for their institution.
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978791092258749,"• For initial submissions, do not include any information that would break anonymity (if
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989395546129375,"applicable), such as the institution conducting the review.
885"
