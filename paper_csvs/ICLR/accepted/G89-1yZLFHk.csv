Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003003003003003003,"Traditional computer vision models are trained to predict a fixed set of predefined
categories. Recently, natural language has been shown to be a broader and richer
source of supervision that provides finer descriptions to visual concepts than
supervised ""gold"" labels. Previous works, such as CLIP, use InfoNCE loss to train
a model to predict the pairing between images and text captions. CLIP, however,
is data hungry and requires more than 400M image-text pairs for training. The
inefficiency can be partially attributed to the fact that the image-text pairs are
noisy. To address this, we propose OTTER (Optimal TransporT distillation for
Efficient zero-shot Recognition), which uses online entropic optimal transport to
find a soft image-text match as labels for contrastive learning. Based on pretrained
image and text encoders, models trained with OTTER achieve strong performance
with only 3M image text pairs. Compared with InfoNCE loss, label smoothing,
and knowledge distillation, OTTER consistently outperforms these baselines in
zero-shot evaluation on Google Open Images (19,958 classes) and multi-labeled
ImageNet 10K (10032 classes) from Tencent ML-Images. Over 42 evaluations on
7 different dataset/architecture settings x 6 metrics, OTTER outperforms (32) or
ties (2) all baselines in 34 of them. Our source code is open sourced at https:
//github.com/facebookresearch/OTTER."
INTRODUCTION,0.006006006006006006,"1
INTRODUCTION"
INTRODUCTION,0.009009009009009009,"In real-world image recognition tasks, input images come from a broad range of distributions,
spanning tens of thousands of object categories unknown during training. It is thus important for
computer vision models to generalize to a large number of visual concepts that may or may not be
present in the training data. This problem is called zero-shot learning (ZSL), which aims to transfer
knowledge from some known classes with training data to a much larger number of unfamiliar classes."
INTRODUCTION,0.012012012012012012,"Previous works on ZSL have explored using attributes (Romera-Paredes & Torr, 2015; Akata et al.,
2015; 2013), class hierarchy (Wang et al., 2018; Kampffmeyer et al., 2019), and pretrained word
embeddings (Frome et al., 2013; Norouzi et al., 2014) to transfer knowledge from pretrained image
representations to recognize new classes. Recently, natural language has been used as a powerful
source of supervision for visual representation learning. (Desai & Johnson, 2020; Sariyildiz et al.,
2020; Zhang et al., 2020; Jia et al., 2020) demonstrate the effectiveness of pretraining on image-text
data. Among them, CLIP (Radford et al., 2021) applies natural language supervision to zero-shot
image recognition. It collects an enormous dataset with over 400M image caption pairs from the
Internet, and trains an image encoder and a text encoder jointly with a contrastive loss to maximize
the cosine similarity of paired image and text embeddings. CLIP demonstrates good zero-shot
classification results on a wide range of downstream image classification datasets. However, a main
constraint of CLIP is that it requires over 400M image-text pairs for training. Collecting and training
on such a huge dataset is very expensive. The inefficiency can be partially attributed to the fact that
the training labels from image-text pairs are noisy. As shown in Figure 1, in a typical image-text
dataset, we observe that images and captions are loosely correlated. It is very common that one
caption (image) can potentially match several other images (captions), and the ground-truth pairing is"
INTRODUCTION,0.015015015015015015,∗Equal contribution
INTRODUCTION,0.018018018018018018,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021021021021021023,"one of the 
most 
dramatic 
mountain 
ranges I have 
seen"
INTRODUCTION,0.024024024024024024,"aerial:  
woman 
waving her 
arms on the 
rock"
INTRODUCTION,0.02702702702702703,"cinematic 
aerial shot of 
the dramatic 
coastline at 
the cliffs"
INTRODUCTION,0.03003003003003003,"view of a lake 
and pine 
forest"
INTRODUCTION,0.03303303303303303,"a forest of 
stunted trees 
that stand in 
sharp 
contrast …"
INTRODUCTION,0.036036036036036036,"landscape 
with mown 
grass and a 
haystack"
INTRODUCTION,0.03903903903903904,"newly built 
small house 
next to the 
sea and the 
beach"
INTRODUCTION,0.042042042042042045,"the public 
house 
traditional 
pub in old 
building on 
corner"
INTRODUCTION,0.04504504504504504,"a cottage in 
the 
picturesque 
village"
INTRODUCTION,0.04804804804804805,"Original
Captions"
INTRODUCTION,0.05105105105105105,"one of the 
most 
dramatic 
mountain 
ranges I have 
seen"
INTRODUCTION,0.05405405405405406,"aerial:  
woman 
waving her 
arms on the 
rock"
INTRODUCTION,0.057057057057057055,"cinematic 
aerial shot of 
the dramatic 
coastline at 
the cliffs"
INTRODUCTION,0.06006006006006006,"Permuted
Captions"
INTRODUCTION,0.06306306306306306,"landscape 
with mown 
grass and a 
haystack"
INTRODUCTION,0.06606606606606606,"a forest of 
stunted trees 
that stand in 
sharp 
contrast …"
INTRODUCTION,0.06906906906906907,"view of a lake 
and pine 
forest"
INTRODUCTION,0.07207207207207207,"the public 
house 
traditional 
pub in old 
building on 
corner"
INTRODUCTION,0.07507507507507508,"a cottage in 
the 
picturesque 
village"
INTRODUCTION,0.07807807807807808,"newly built 
small house 
next to the 
sea and the 
beach"
INTRODUCTION,0.08108108108108109,"Figure 1: Images and captions are only loosely correlated in many image-text datasets. The ground-
truth pairing is not the only sensible match between texts and images. In the example above, we can
find permutations of text captions that can still match with original images."
INTRODUCTION,0.08408408408408409,"not the only sensible match. Note that examples in Figure 1 are not hand-picked special cases. In
fact, such noisy image-text matching is prevalent in image-text datasets."
INTRODUCTION,0.08708708708708708,"To quantitatively analyze this, we use a CLIP(Radford et al., 2021) VIT-B/32 pretrained on OpenAI’s
400M dataset to estimate the matching probabilities between a batch of paired image-text samples.
Specifically, we randomly sample 1000 batches from the CC3M (Sharma et al., 2018) and YFCC15M
(subset of YFCC100M (Thomee et al., 2016)) datasets, and use the pretrained CLIP model to compute
the image-to-text matching probabilities by taking the dot-product of the feature embeddings and
taking a softmax along each row. For each batch, we compute three statistics (averaged across rows):
default probability, non-default max probability, and non-default average probability. Note in both
datasets, the matching probability between paired samples are far smaller than 1.0, and the probability
decreases with the batch size. This indicates that there exist image and text samples that are not
paired, but have nontrivial matching probabilities. This is further confirmed by the max matching
probabilities between unpaired samples. In the extreme cases (CC 3M, 2048 batch size), the average
of max matching probability between unpaired image-text samples is very close to the average of
probability of paired samples. Despite prevalent noisy matching between images and texts, CLIP uses
the InfoNCE loss (Hadsell et al., 2006) for training and uses the ground-truth pairings as hard labels.
This ignores the many-to-many relationship within a batch of images and text captions, leading to
noisy training signals and lower data efficiency."
INTRODUCTION,0.09009009009009009,Table 1: Matching probabilities estimated by CLIP on Conceptual Captions and YFCC
INTRODUCTION,0.09309309309309309,"Dataset
Batch Size
Paired
Unpaired Avg
Unpaired Max
512
0.565
0.001
0.215
1024
0.480
0.001
0.230
CC 3M"
INTRODUCTION,0.0960960960960961,"2048
0.398
0.000
0.238
512
0.628
0.001
0.197
1024
0.551
0.000
0.219
YFCC 15M"
INTRODUCTION,0.0990990990990991,"2048
0.469
0.000
0.239"
INTRODUCTION,0.1021021021021021,"To address this, we propose OTTER, or Optimal TransporT distillation for Efficient zero-shot
Recognition. We improve InfoNCE to consider the many-to-many relationship between unpaired
images and texts. Specifically, given a batch of image and text tuples {(vi, ti)}i=1:N, we first use
image/text encoders to estimate a similarity matrix whose elements denotes similarity from image
vi to text caption tj. Based on the similarity matrix, we use optimal transport to find a matching
probability between each possible image-text combination. To model the many-to-many relationship,
we add an entropic regularization to the optimal transport so that the match is softly assigned.
Entropic-regularized optimal transport can be solved efficiently with the iterative Sinkhorn-Knopp
algorithm (Cuturi, 2013). Finally, we use the match as soft label to train the image and text encoders."
INTRODUCTION,0.10510510510510511,"Based on pretrained image and text models, we use OTTER to train zero-shot models on the
Conceptual Captions (CC) (Sharma et al., 2018), (subset of) Wikipedia-based Image Text (Srinivasan
et al., 2021), and YFCC 15M (Thomee et al., 2016) datasets, which contain 3M, 5M, and 15M
image-caption pairs, respectively. We evaluate the image encoder’s zero-shot recognition of common
visual concepts on Google Open Images (GOI) (Kuznetsova et al., 2020) (19,958 categories) and"
INTRODUCTION,0.10810810810810811,Published as a conference paper at ICLR 2022
INTRODUCTION,0.1111111111111111,"multi-labeled ImageNet 10K (10032 categories) from Tencent-ML-Images (Wu et al., 2019a). Over
42 evaluations on 7 different dataset-architecture settings × 6 metrics, OTTER outperforms (32) or
ties (2) all baselines in 34 of them. We also propose a quantitative vision-language compositionality
benchmark and show comparable results to CLIP in Appendix D."
RELATED WORKS,0.11411411411411411,"2
RELATED WORKS"
RELATED WORKS,0.11711711711711711,"Zero-Shot Learning in Computer Vision: Zero-shot learning (ZSL) studies the generalization of
knowledge to unseen classes. Previous methods for zero-shot recognition in computer vision mainly
follow three paradigms. The first type, including DeViSE (Frome et al., 2013) and ConSE (Norouzi
et al., 2014), uses pretrained word embedding vectors to represent different categories and implicitly
model their relationships. However, word embedding is a preliminary and limited representation
of class relationships, which hurts performance. The second paradigm, including GCNZ (Wang
et al., 2018), DPGZ (Kampffmeyer et al., 2019), and HZSL (Liu et al., 2020), explicitly models
class relationships as a graph, and uses a graph convolutional network (GCN), or a predefined class
hierarchy, such as WordNet (Feinerer & Hornik, 2020), to learn the knowledge propagation between
classes. However, real-world class relationships are complicated and simple graph structures such as
WordNet are too limited to model such relationships. Lastly, (Romera-Paredes & Torr, 2015; Akata
et al., 2015; 2013) rely on human-labeled attributes to model semantics of classes. The scalability of
these methods are limited by the need for attribute annotations. More recently, CLIP (Radford et al.,
2021) applies language-supervision to ZSL by training on image caption pairs. Our work is based on
CLIP and we generalize the InfoNCE loss to improve its data efficiency."
RELATED WORKS,0.12012012012012012,"Vision and Language: Natural language has long been used as a source of supervision in fields
like image-text retrieval Hironobu et al. (1999), object classification Wang et al. (2009), and video
understanding (Ramanathan et al., 2013). Socher et al. (2014); Karpathy et al. (2014); Li et al.
(2019); Chen et al. (2021a) have proposed methods of learning visual and language representations
in a joint embedding space. More recently, (Lu et al., 2019; Chen et al., 2020c; Qi et al., 2020)
propose using a cross-modal attention mechanism to increase performance in image-text matching.
In addition, (Joulin* et al., 2016; Li et al., 2017; Desai & Johnson, 2020; Sariyildiz et al., 2020)
demonstrate that good visual representations can be learned by predicting image captions. To scale
up vision-language joint training, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2020) both
collect their own image-text datasets with 400M and 1B image-caption pairs."
RELATED WORKS,0.12312312312312312,"Optimal transport (OT) is a theory that enables comparison of two probability distributions whose
supports may not overlap. OT has been applied to many areas such as domain adaptation (Courty
et al., 2016), generative models (Salimans et al., 2018), and self-supervised vision representation
learning (Caron et al., 2020; Asano et al., 2019). In vision and language, (Chen et al., 2020a) uses
OT to align objects in images and words in texts. The problem formulation of our work is similar to
(Damodaran et al., 2018), where OT is used to mitigate the label noise in remote-sensing data under
supervised learning. In our paper, we extend the method from supervised learning to contrastive
learning, where OT is a natural way to estimate pairings between images and texts. In another related
work, (Chen et al., 2021b) adds an additional OT-based Wasserstein loss to contrastive representation
distillation (Tian et al., 2019). The loss matches student representations to teacher representations in a
batch. (Chen et al., 2021b) is different from our method since it directly minimizes the Wassertein loss
between two models’ representations, while our method uses OT to estimate the pairing probability
and use the probability for knowledge distillation. Directly minimizing Wasserstein loss between
image/text embeddings in our case will lead to collapsed representations, where models generate
constant output regardless of inputs."
RELATED WORKS,0.12612612612612611,"Other related works: Our work is also related to areas including learning with noisy labels,
contrastive learning, and knowledge distillation. Our method uses OT to estimate the matching
probability of unpaired images and texts. This is reminiscent to estimating label transition probability
under noisy labels (Song et al., 2020). Our method is based on contrastive learning, which is
commonly used in self-supervised visual representation learning (Chen et al., 2020b; He et al., 2020).
For vision representation learning, (Robinson et al., 2020) argues that sampling hard negative pairs
can improve learning efficiency. For language-supervised representation learning, however, it is
important to mitigate the noise of widely spread hard negative samples, since positive image-text
pairs are usually only loosely correlated. Our method is also an extension to knowledge distillation
(KD) (Hinton et al., 2015). Typical KD directly relies on a teacher model to directly generate a
target distribution (Xie et al., 2020; Bagherinezhad et al., 2018; Caron et al., 2021). Our method is"
RELATED WORKS,0.12912912912912913,Published as a conference paper at ICLR 2022
RELATED WORKS,0.13213213213213212,"different since our target distribution is computed by OT based on the pairwise similarity estimated
by a teacher model. Experiments show that this works better for image-text contrastive learning."
RELATED WORKS,0.13513513513513514,Fly over sea
RELATED WORKS,0.13813813813813813,Dog on boat
RELATED WORKS,0.14114114114114115,Mount & sea
RELATED WORKS,0.14414414414414414,Cliff & sea
RELATED WORKS,0.14714714714714713,EMA update
RELATED WORKS,0.15015015015015015,EMA update
RELATED WORKS,0.15315315315315314,"Cross 
Entropy"
RELATED WORKS,0.15615615615615616,"Image
similarity"
RELATED WORKS,0.15915915915915915,"Text 
similarity"
RELATED WORKS,0.16216216216216217,Image-text
RELATED WORKS,0.16516516516516516,"similarity +
+"
RELATED WORKS,0.16816816816816818,"Optimal 
Transport"
RELATED WORKS,0.17117117117117117,"Target 
distribution"
RELATED WORKS,0.17417417417417416,Image encoder
RELATED WORKS,0.17717717717717718,Text encoder
RELATED WORKS,0.18018018018018017,"Predicted 
distribution"
RELATED WORKS,0.1831831831831832,"Figure 2: Architecture of OTTER. We use image and text embeddings to compute similarity matrices
Sv (and St), which is then used to solve for matching probabilities Mv∗(and Mt∗) as targets."
METHODS,0.18618618618618618,"3
METHODS"
METHODS,0.1891891891891892,"We introduce OTTER in this section. Let {(vi, ti)}i=1:N be a batch of paired image-text tuples
sampled from data distribution p(v, t). Our model contains an image encoder fv(·) and a text encoder
ft(·) that map image vi and text ti to ℓ2-normalized embeddings zv
i and zt
i respectively."
CONTRASTIVE LEARNING WITH INFONCE LOSS,0.1921921921921922,"3.1
CONTRASTIVE LEARNING WITH INFONCE LOSS"
CONTRASTIVE LEARNING WITH INFONCE LOSS,0.19519519519519518,"CLIP (Radford et al., 2021) trains the image and text encoders with contrastive learning to pull the
paired image and text embeddings closer, and push the unpaired embeddings farther. This is achieved
by miminizing the InfoNCE loss LInfoNCE = Lv + Lt. Lv is the loss for matching images to text
captions, and Lt is for text-to-image matching. Lv is defined as"
CONTRASTIVE LEARNING WITH INFONCE LOSS,0.1981981981981982,"Lv = −1 N N
X i=1 N
X"
CONTRASTIVE LEARNING WITH INFONCE LOSS,0.2012012012012012,"j=1
Iij log pv(zv
i , zt
j; τ) = −1 N N
X i=1 N
X"
CONTRASTIVE LEARNING WITH INFONCE LOSS,0.2042042042042042,"j=1
Iij log
exp((zv⊤
i
zt
j)/τ)
PN
k=1 exp((zv⊤
i
zt
k)/τ)
,
(1)"
CONTRASTIVE LEARNING WITH INFONCE LOSS,0.2072072072072072,"where (zv⊤
i
zt
j) is the cosine similarity between two ℓ2-normalized embedding vectors. τ is a
(trainable) temperature parameter. Iij is the element of an identity matrix IN with Iii = 1, ∀i
and Iij = 0, ∀i ̸= j. Note pv is normalized across zt
k for k = 1, · · · , N in the denominator.
Symmetrically, we define Lt and pt in the same way as Equation (1) except we normalize across zv
k."
CONTRASTIVE LEARNING WITH INFONCE LOSS,0.21021021021021022,"Equation (1) is a rather redundant way of writing the InfoNCE loss, as Iij are all zeros for unpaired
image-text samples. However, this shows that InfoNCE is essentially the cross entropy between a
one-hot distribution Iij and the estimated probability pv(zv
i , zt
j; τ). One-hot distribution assumes
that within a batch of images and text captions, the only match for image vi is its paired text caption
ti. However, as shown in Figure 1 and Table 1, this assumption is not true. Paired images and text
captions are only loosely correlated. It is common that one image can match with several other texts
and vice versa. The ground-truth match provided by the dataset is not the only sensible match between
images and texts. One-hot labels are therefore noisy, leading to degraded learning performance."
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING,0.2132132132132132,"3.2
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING"
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING,0.21621621621621623,"To better capture the many-to-many relationship in image-text datasets, we modify InfoNCE in
Equation (1) to consider the matching probability of unpaired images and texts. For a batch of N
image-text pairs, we define Yi ∈{1, . . . , N} as a random variable, and let qv(Yi = j|v1:N, t1:N) be
the probability that image vi should be matched with text caption tj in the batch. We model this as"
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING,0.21921921921921922,"qv(Yi = j|v1:N, t1:N) = qv(Yi = i)Iij + qv(Yi ̸= i)Mij = αIij + (1 −α)Mij
(2)"
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING,0.2222222222222222,Published as a conference paper at ICLR 2022
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING,0.22522522522522523,"where Mij := qv(Yi = j|Yi ̸= i, v1:N, t1:N), Mii = 0 ∀i is the conditional probability of image vi
being matched to tj given that it is not matched to text ti. For simplicity, we write qv
i (j) := qv(Yi =
j|v1:N, t1:N). α ∈[0, 1] is the prior probability that image vi is matched with its paired text caption
ti. α reflects the noise level in the dataset. In an ideal noiseless dataset, α = 1, so qv
i (j) = Iij.
This is the case where we should use the one-hot labels Iij for contrastive learning. However, in
image-text datasets, it is common for an unpaired text caption tj to be a better match for image vi, as
shown in Table 1. In this case, α < 1 and using Iij as the target distribution is no longer accurate. So
we generalize the InfoNCE loss in Equation (1) by replacing Iij with the more generic qv
i (j) as"
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING,0.22822822822822822,"Lv = −1 N N
X i=1 N
X"
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING,0.23123123123123124,"j=1
[αIij + (1 −α)M v
ij] log pv(zv
i , zt
j; τ),
(3)"
MODELING THE PROBABILITY OF UNPAIRED IMAGE-TEXT MATCHING,0.23423423423423423,"αIij provides supervision on paired image-text samples and (1−α)M v
ij supervises unpaired samples.
The question is how do we estimate M v
ij. A simple estimation is to let M v
ij = (1−Iij)/(N −1) ∀i, j
be a uniform distribution. This is equivalent to the label smoothing method proposed in (Szegedy
et al., 2016). However, this completely ignores the contents of images v1:N and texts t1:N."
MODELING WITH OPTIMAL TRANSPORT,0.23723723723723725,"3.3
MODELING WITH OPTIMAL TRANSPORT"
MODELING WITH OPTIMAL TRANSPORT,0.24024024024024024,"To design a better method of estimating M v
ij, we start from two intuitions: first, in a reasonable
image-text dataset, there are no bad images or texts. We assume all the images and texts are equally
matchable so they should have equal matching probabilities. Second, the matching probability from
image vi to caption tj should depend on their similarity estimation Sij. A relatively higher similarity
Sij should lead to higher matching probability M v
ij. An estimation for M v
ij that satisfies the two
intuitions can be obtained by solving the following entropic optimal transport problem (Cuturi, 2013)
Mv∗= arg max
M∈M
⟨M, Sv⟩F + λH(M).
(4)"
MODELING WITH OPTIMAL TRANSPORT,0.24324324324324326,"Sv ∈RN×N is a similarity matrix whose elements Sv
ij denotes the similarity from vi to tj. We
discuss how to compute Sv in Section 3.4. ⟨M, Sv⟩F = P"
MODELING WITH OPTIMAL TRANSPORT,0.24624624624624625,"ij MijSv
ij is the Frobenius inner product
between the similarity matrix Sv and the matching plan M. Maximizing this term ensures M is
similar to Sv, i.e., larger Sv
ij leads to larger Mij and vice versa. Meanwhile, we add an entropy
regularization on M as H(M) = −P"
MODELING WITH OPTIMAL TRANSPORT,0.24924924924924924,"ij Mij log Mij. This ensures that M does not over concentrate
on a few elements. We constrain the solution of Equation (4) to be a transportation polytope"
MODELING WITH OPTIMAL TRANSPORT,0.25225225225225223,"M = {M ∈RN×N
+
| M1N = 1"
MODELING WITH OPTIMAL TRANSPORT,0.2552552552552553,"N 1N, M⊤1N = 1"
MODELING WITH OPTIMAL TRANSPORT,0.25825825825825827,"N 1N}.
(5)"
MODELING WITH OPTIMAL TRANSPORT,0.26126126126126126,"This constraint ensures that the solution Mv∗satisfies the first intuition – all images and texts are
equally important and should be matched with equal probabilities. Moreover, as proven in (Cuturi,
2013), the solution to Equation (4) takes the form of a normalized exponential matrix
Mv∗= Diag(r) exp(Sv/λ)Diag(c),
(6)
where r, c ∈RN are row and column normalization vectors and can be calculated through the
iterative Sinkhorn-Knopp algorithm (Cuturi, 2013). The Sinkhorn-Knopp algorithm can be efficiently
implemented on GPU and we provide a pseudo-code implementation in Appendix B."
MODELING WITH OPTIMAL TRANSPORT,0.26426426426426425,"From Equation (6), it is clear that Mv∗satisfies our second intuition that a similarity Sij leads to
higher matching probability since M v∗
ij ∼exp(Sv
ij/λ). The role of the entropic regularization is also
clear. A larger λ or higher entropy regularization and leads to ""softer"" distribution for M v∗
ij . On the
other hand, a smaller λ or lower entropy regularization leads to ""harder"" distribution for M v∗
ij ."
COMPUTING THE SIMILARITY MATRIX,0.2672672672672673,"3.4
COMPUTING THE SIMILARITY MATRIX"
COMPUTING THE SIMILARITY MATRIX,0.2702702702702703,"To compute the similarity from image vi to text tj, we can use a pair of teacher encoders ˜fv(·), ˜ft(·)
to compute ℓ2-normalized embeddings ˜zv
i , ˜zt
j. Denoting ˜Zv, ˜Zt ∈Rd×N as matrcies whose columns
are ˜zv
1:N, ˜zt
1:N respectively, we compute the similarity matrix as"
COMPUTING THE SIMILARITY MATRIX,0.2732732732732733,"Sv = γv˜Zv⊤˜Zv + γt˜Zt⊤˜Zt + ˜Zv⊤˜Zt −ηIN.
(7)"
COMPUTING THE SIMILARITY MATRIX,0.27627627627627627,"The first term ˜Zv⊤˜Zv ∈RN×N compares the image similarities, as (˜Zv⊤˜Zv)ij = ˜zv⊤
i
˜zv
j is the
cosine similarity between image embeddings. Intuitively, it assumes that for a pair of similar images,
it is likely that we can exchange their text captions. Similarly, ˜Zt⊤˜Zt compares the text similarities."
COMPUTING THE SIMILARITY MATRIX,0.27927927927927926,Published as a conference paper at ICLR 2022
COMPUTING THE SIMILARITY MATRIX,0.2822822822822823,"It assumes that if a pair of text captions are similar, it is more likely that one text caption can match
the other image. The term ˜Zv⊤˜Zt considers the similarity between the image and text embeddings.
Finally, ηIN with η →∞ensures the diagonal terms of Sv are infinitely small. This effectively sets
the diagonal terms of Mv∗to 0, which is necessary since Mij is conditioned on Yi ̸= i."
COMPUTING THE SIMILARITY MATRIX,0.2852852852852853,"There are several options to instantiate ˜fv(·) and ˜ft(·). The simplest option is to use the original
image and text encoder fv(·), ft(·) as ˜fv(·), ˜ft(·). Alternatively, following recent works (He et al.,
2020; Caron et al., 2021; Liu et al., 2021), ˜fv(·), ˜ft(·) can share the same model architecture with
fv(·), ft(·), but their weights are updated as an exponential moving average as ˜θ ←m˜θ + (1 −m)θ,
where ˜θ is the weight for ˜fv(·), ˜ft(·), θ is the weight for fv(·), ft(·), and m is a momentum parameter
set to 0.999. Of course, we can also use trained image and text encoders such as CLIP for ˜fv(·) and
˜ft(·). We adopt the first two options in our paper, since we want to avoid using extra image-text pairs."
RELATIONSHIP WITH KNOWLEDGE DISTILLATION,0.2882882882882883,"3.5
RELATIONSHIP WITH KNOWLEDGE DISTILLATION"
RELATIONSHIP WITH KNOWLEDGE DISTILLATION,0.2912912912912913,"OTTER is an extension of conventional knowledge distillation (KD) (Hinton et al., 2015). Equation
(3) computes the cross entropy H(qv
i , pv
i ) between qv
i (j) and pv
i (j) := pv(zv
i , zt
j; τ), where qv
i (j) is
the teacher distribution solved by OT and pv
i (j) is the student distribution with logits (ziv⊤zjt)/τ
computed by f(·)v, f(·)t. A more conventional way to compute KD’s teacher distribution is"
RELATIONSHIP WITH KNOWLEDGE DISTILLATION,0.29429429429429427,"qv(˜zv
i ,˜zt
j; τ) =
exp((˜zv⊤
i ˜zt
j)/τ)
PN
k=1 exp((˜zv⊤
i ˜zt
k)/τ)
,
(8)"
RELATIONSHIP WITH KNOWLEDGE DISTILLATION,0.2972972972972973,"where ˜zv
i ,˜zt
j are computed by the teacher ˜fv(·), ˜ft(·). We can re-write Equation (8) in the matrix form
as Qv = Diag(r) exp(˜Zv⊤˜Zt/τ)Diag(c), where ri = 1, and ci = 1/ PN
k=1 exp((˜zv⊤
i ˜zt
k)/τ). Note
this teacher distribution has the same form as OTTER in Equation (6), but with two differences. First,
OTTER’s similarity matrix Sv in Equation (7) have three more terms: γv˜Zv⊤˜Zv, γt˜Zt⊤˜Zt, ηIN. In
comparison, KD ignores image-image, text-text similarities and does not exclude diagonal terms. By
setting γv = γt = η = 0, their similarity matrices are equivalent. Second, OTTER’s normalization
vectors r, c in Equation (6) are solved with Sinkhorn-Knopp while for KD r, c are computed by a
Softmax function. In fact, if we set the #iteration to 0 in Algorithm 2 (Appendix B), Sinkhorn-Knopp
is equivalent to Softmax, as also noted by (Caron et al., 2021)."
EXPERIMENTS,0.3003003003003003,"4
EXPERIMENTS"
EXPERIMENTS,0.3033033033033033,"In this section, we discuss our experiments validating the effectiveness of OTTER. We open-sourced
our code at https://github.com/facebookresearch/OTTER. To setup a baseline, we
follow CLIP (Radford et al., 2021) to train an image and a text encoder to predict the pairing of
image and text samples using the infoNCE loss. Since the dataset used by CLIP is not released,
we train on three publicly available datasets, Conceptual Captions 3M (CC) (Sharma et al., 2018),
Wikipedia-base Image-Text Dataset (WIT), and YFCC 15M (Thomee et al., 2016). We only train on
images with English captions in all 10 partitions of the WIT dataset, resulting in 5M image-text pairs
in total. Since the datasets we use are small (∼100x smaller than the one used by CLIP), we have to
use pre-trained models to initialize the image and text encoders. Also, due to the datasets’ limited
scale and concept coverage, models trained on CC or WIT do not perform well on domain-specific
datasets such as Stanford Cars (Krause et al., 2013) and FGVC Aircraft (Maji et al., 2013). To test
zero-shot recognition on common visual concepts, we evaluate our models on the test set of Google
Open Image (GOI) (Kuznetsova et al., 2020), which contains 19,958 classes. We also evaluate on
the test set of multi-labeled ImageNet 10K (10032 classes) dataset whose labels come from Tencent
ML-Images (Wu et al., 2019a). Each image in ImageNet 10K is auto-labeled with highly-correlated
class labels from GOI, alleviating the single-label issue of ImageNet 21K and 1K. To compare with
previous ZSL methods (Norouzi et al., 2014; Frome et al., 2013; Wang et al., 2018; Liu et al., 2020),
we report the ZSL performance of one of our models on ImageNet21K+1K."
EXPERIMENTS,0.3063063063063063,"Training: We adopt a training recipe similar to BiT’s finetuning strategy (Kolesnikov et al., 2020):
We use SGD with an initial learning rate of 3e-3, a cosine annealing scheduler, momentum 0.9, and
no weight decay. Input images are resized to 256x256 and randomly cropped to 224x224 while test
images are resized to 256x256 and center-cropped to 224x224. We train on 8 V100 GPUs using
Pytorch (Paszke et al., 2019) distributed data parallel with a total batch size of 512 (64 per GPU) for"
EXPERIMENTS,0.30930930930930933,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.3123123123123123,"10 epochs. While CLIP (Radford et al., 2021) computes InfoNCE using sub-batches on each GPU,
we gather logits from all GPUs for OTTER and baselines."
EXPERIMENTS,0.3153153153153153,"Inference: For inference, we follow CLIP to compute the text embeddings for the target classes
using the trained text encoder, and we use a prompt template of “a photo of {label}"" to augment the
label texts. Next, we fit a KNN using the text embeddings. Given an image, we find the top K nearest
label embedding neighbors to the image embedding based on cosine similarity."
EXPERIMENTS,0.3183183183183183,"Evaluation: GOI (Kuznetsova et al., 2020) and ImageNet 10K from Tencent-ML-Images (Wu et al.,
2019a) are multi-labeled. Following previous work on ZSL (Norouzi et al., 2014; Frome et al., 2013;
Wang et al., 2018; Liu et al., 2020), we use flat hit @ k (FH@K) for evaluation. FH@K is the
percentage of test images such that the top K predictions of the model overlap with true labels and is
formally defined as 1"
EXPERIMENTS,0.3213213213213213,"N
PN
i=1 1({{f(vi)}K ∩Li ̸= ∅}), where {f(vi)}K is the top K predictions
for the i-th image and Li is the set of true labels."
EXPERIMENTS,0.32432432432432434,Table 2: FH@K on test sets of Google Open Images and ImageNet10K from Tencent-ML-Images.
EXPERIMENTS,0.32732732732732733,"Data
Image
encoder"
EXPERIMENTS,0.3303303303303303,"Text
encoder
Method
GOI FH@K (%)
IN10K FH@K (%)
1
5
10
1
5
10"
EXPERIMENTS,0.3333333333333333,"CLIP
(400M)"
EXPERIMENTS,0.33633633633633636,"ResNet50
CLIP
Transformer
InfoNCE
26.5
54.0
64.3
20.1
44.8
56.4
ViT-B/32
27.5
55.3
65.4
22.5
49.1
60.7"
EXPERIMENTS,0.33933933933933935,"CC
(3M)"
EXPERIMENTS,0.34234234234234234,Wide ResNet50x2
EXPERIMENTS,0.34534534534534533,"DeCLUTR
-Sci-base"
EXPERIMENTS,0.3483483483483483,"InfoNCE
28.6
58.6
69.8
11.0
29.9
40.6"
EXPERIMENTS,0.35135135135135137,ResNet50
EXPERIMENTS,0.35435435435435436,"InfoNCE
26.8
55.1
66.4
10.9
29.4
40.5
LS
26.3
55.9
67.5
10.1
29.6
39.8
KD
26.7
55.3
67.1
10.0
27.5
38.5
OTTER
29.1
59.6
70.9
12.0
31.8
42.1"
EXPERIMENTS,0.35735735735735735,ResNet34
EXPERIMENTS,0.36036036036036034,"InfoNCE
22.8
50.0
61.5
7.9
23.7
33.0
LS
19.8
46.9
59.2
6.7
21.9
31.9
KD
21.1
47.9
59.8
7.3
23.0
32.5
OTTER
24.2
52.6
64.4
9.0
25.6
35.4"
EXPERIMENTS,0.3633633633633634,FBNetV3-A
EXPERIMENTS,0.3663663663663664,"InfoNCE
27.2
57.0
69.0
10.0
27.9
38.5
LS
24.2
53.9
65.7
8.9
26.7
38.0
KD
26.9
56.7
68.4
10.7
28.9
39.7
OTTER
27.5
57.2
69.0
10.4
29.4
39.9"
EXPERIMENTS,0.36936936936936937,FBNetV3-C
EXPERIMENTS,0.37237237237237236,"InfoNCE
25.7
54.3
66.1
8.7
25.8
35.8
LS
24.8
54.0
66.1
9.7
26.8
37.6
KD
26.6
55.8
67.6
10.5
28.2
38.9
OTTER
27.5
57.6
69.1
10.4
28.7
39.4"
EXPERIMENTS,0.37537537537537535,"ResNet50
Sentence
-BERT-base"
EXPERIMENTS,0.3783783783783784,"InfoNCE
25.5
52.2
62.8
9.5
26.1
35.9
LS
24.5
50.8
61.6
9.3
26.7
37.0
KD
25.6
52.3
62.4
9.8
26.2
36.0
OTTER
26.1
53.1
63.4
9.9
26.6
36.6"
EXPERIMENTS,0.3813813813813814,"WIT
(5M)
ResNet50
DeCLUTR
-Sci-base"
EXPERIMENTS,0.3843843843843844,"InfoNCE
13.5
34.0
44.8
6.3
19.2
27.8
LS
14.3
35.5
46.2
6.4
19.8
28.9
KD
14.4
35.0
45.9
6.2
19.3
28.0
OTTER
14.5
36.4
47.7
6.2
19.8
29.0"
EXPERIMENTS,0.38738738738738737,"YFCC
(15M)
ResNet50
DeCLUTR
-Sci-base"
EXPERIMENTS,0.39039039039039036,"InfoNCE
18.8
42.9
53.6
8.9
26.3
36.9
LS
19.6
44.9
55.7
9.8
28.2
38.8
KD
19.5
43.5
54.2
8.9
26.0
36.7
OTTER
20.6
45.4
55.9
9.3
27.4
38.1"
COMPARING OTTER WITH BASELINES,0.3933933933933934,"4.1
COMPARING OTTER WITH BASELINES"
COMPARING OTTER WITH BASELINES,0.3963963963963964,"To compare with OTTER, we include three baselines: 1) InfoNCE with hard labels; 2) InfoNCE with
label-smoothing (LS) (Szegedy et al., 2016), as described in Section 3.2; 3) InfoNCE with knowledge
distillation (KD) (Hinton et al., 2015), as described in Section 3.5. In addition to the experimental
setting described above, we use the following OTTER hyper-parameters: we set the loss coefficient
α = 0.5, set γv = γt = 1 for the similarity matrix. We use the exponential-moving average (EMA)
of the image/text encoders as teachers and set the EMA decay to 0.999. For Sinkhorn-Knopp, we set"
COMPARING OTTER WITH BASELINES,0.3993993993993994,Published as a conference paper at ICLR 2022
COMPARING OTTER WITH BASELINES,0.4024024024024024,Table 3: Flat hit @K on ImageNet 21K+1K.
COMPARING OTTER WITH BASELINES,0.40540540540540543,"Dataset
Image Encoder
Text Encoder
Method
Flat Hit@k(%)
1
2
5
10"
COMPARING OTTER WITH BASELINES,0.4084084084084084,"IN1k
(1.2M)
ResNet50"
COMPARING OTTER WITH BASELINES,0.4114114114114114,"skip-gram
DeViSE
0.3
0.9
2.2
3.6
skip-gram
ConSE
0.1
1.5
3.5
4.9
GloVe
GCNZ
1.0
2.3
5.3
8.1
GloVe
HZSL
2.2
4.6
9.2
12.7"
COMPARING OTTER WITH BASELINES,0.4144144144144144,"CC
(3M)
FBNetV3-C
DeCLUTR-Sci-base"
COMPARING OTTER WITH BASELINES,0.4174174174174174,"InfoNCE
3.2
4.8
8.8
12.9
LS
3.4
5.1
9.4
13.7
KD
3.6
5.4
9.7
14.0
OTTER
3.7
5.5
9.9
14.3"
COMPARING OTTER WITH BASELINES,0.42042042042042044,"CLIP
(400M)"
COMPARING OTTER WITH BASELINES,0.42342342342342343,"ResNet50
CLIP
Transformer
CLIP
13.5
19.7
30.5
39.4
ViT-B/32
15.3
22.2
33.9
43.3"
COMPARING OTTER WITH BASELINES,0.4264264264264264,"λ = 0.15 and the number of iterations to 5. For the KD baseline, we also use EMA teacher and set
α = 0.5. For the label-smoothing baseline, we set α = 0.9, which yields better results than α = 0.5."
COMPARING OTTER WITH BASELINES,0.4294294294294294,"On CC, we train the image-text models based on four different pretrained image encoders: ResNet-
{50, 34} (He et al., 2016), FBNetV3-{A, C} (Wu et al., 2019b; Wan et al., 2020; Dai et al., 2020),
and two pretrained text encoders: DeCLUTR-Sci-base (Giorgi et al., 2020) pretrained on S2ORC
(Lo et al., 2020) and Sentence BERT (Reimers & Gurevych, 2019) pretrained on SNLI (Bowman
et al., 2015) and MultiNLI (Williams et al., 2018). We also train ResNet50 + DeCLUTR-Sci-base
on the (partial) WIT (Srinivasan et al., 2021) and the YFCC15M (subset of YFCC 100M) (Thomee
et al., 2016) datasets. We report FH@K=1, 5, 10 on the test sets of both GOI and multi-labeled
ImageNet 10K (Wu et al., 2019a). As shown in Table 2, over the 42 evaluations on 7 different
dataset-architecture settings x 6 metrics, OTTER outperforms (32) or ties (2) all other baselines
in 34 of them. Compared with CLIP’s performance on the GOI test set, a ResNet50 trained by
OTTER outperforms CLIP-RN50 by 2.6 pts FH@1 and by 6.6 pts FH@10. To further illustrate the
significance of the performance gain, we show that a ResNet50 (25.6M params) trained with OTTER
outperforms a Wide ResNet50x2 (68.4M params) trained with InfoNCE under the same setting."
COMPARING OTTER WITH BASELINES,0.43243243243243246,"For reference, to put OTTER in the context of traditional ZSL methods, we present FH@K results on
zero-shot transfer to the ImageNet 21K+1K (Deng et al., 2009) dataset, which contains 21,841 classes
in total. The result is reported in Table 3. With 400M image-text pairs, CLIP (Radford et al., 2021)
vastly outperforms all other methods. ImageNet22K’s classes contain many uncommon words, such
as scientific names of animals or medical terms. While not directly comparable with traditional ZSL
methods due to differences in datasets used and model architectures, OTTER is significantly better
than previous ZSL methods, beating the previous SotA, HZSL(Liu et al., 2020), by 68% relatively."
VISUALIZING OTTER,0.43543543543543545,"4.2
VISUALIZING OTTER
In order to check if the image/text matching found by OTTER is sensible, we provide visualizations
of OTTER’s matching results. In Figure 3, we visualize the matching results on a small batch of 9
image-text pairs. We set α = 0.5 for paired image-text samples, as shown in the diagonal elements
in Figure 3. The off-diagonal elements are estimated by OTTER. Since the interpretation of the
matching results are highly subjective, we leave the interpretation to readers."
VISUALIZING OTTER,0.43843843843843844,"Next, we use OTTER to process a larger batch of 512 image-text pairs. This is our batch size for
training. We pick the top-8 largest off-diagonal pairs from the optimal tranport result and show them
in Figure 4. As we can see, in a large batch, we can easily find unpaired images and captions that turn
out to be good matches. InfoNCE will simply regard these pairs as negative examples and push them
away from each other while OTTER can better handle this by treating them as semi-positive pairs."
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.44144144144144143,"4.3
IMPORTANCE OF SIMILARITY MATRIX AND EMA
In Equation 7, we design the similarity matrix Sv as the composition of image, text, and image-text
similarity matrices. In Table 4, we show experiments to validate the effectiveness of this composition
and the necessity of using EMA. There are various levels of performance drop when we don’t use
the image or text similarities, or when EMA is turned off. Note that our baseline hyper-parameters
are different from Table 2, so the accuracy is also different. We compare different settings using
FH@K=1 on the GOI test set. More in-depth ablation studies are shown in Appendix A."
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4444444444444444,Published as a conference paper at ICLR 2022
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.44744744744744747,"one of the 
most 
dramatic 
mountain 
ranges I 
have seen"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.45045045045045046,"aerial:  
woman 
waving her 
arms on the 
rock"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.45345345345345345,"cinematic 
aerial shot 
of the 
dramatic 
coastline at 
the cliffs"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.45645645645645644,"view of a 
loch and 
pine forest"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4594594594594595,"a forest of 
stunted 
trees that 
stand in 
sharp 
contrast …"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4624624624624625,"landscape 
with mown 
grass and a 
haystack"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.46546546546546547,"newly built 
small house 
next to the 
sea and the 
beach"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.46846846846846846,"the public 
house 
traditional 
pub in old 
building on 
corner"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.47147147147147145,"a cottage in 
the 
picturesque 
village"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4744744744744745,Figure 3: Visualization of OTTER’s matching on a batch of 9 image/text pairs.
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4774774774774775,"ice hockey 
player # of the 
skates against 
sports team"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4804804804804805,"american
football player 
celebrates his 
touchdown run"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.48348348348348347,"ice hockey 
player blocks a 
shot during the 
second period …"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4864864864864865,"vector illustration 
with a cup of 
coffee and hand 
drawn …"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4894894894894895,"a cute cat 
starring with 
sharp eyes"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4924924924924925,"seamless sketch 
of a cup of hot 
coffee and 
steam"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4954954954954955,"close-up of a 
calico cat 
playing with a 
toy"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.4984984984984985,"profession cuts 
the hair of the 
client with 
clipper"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.5015015015015015,"Figure 4: Visualization of top-8 image-text pairs matched by OTTER in a batch of 512 samples.
These image/text pairs are regarded as negative samples by InfoNCE."
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.5045045045045045,Table 4: Validation of Similarity Matrix and EMA.
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.5075075075075075,"α
γv
γt
EMA
λ
#iter
batch
FH@K=1"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.5105105105105106,"baseline
0.5
1.0
1.0
✓
0.1
4
512
31.0"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.5135135135135135,"0.5
0.0
1.0
✓
0.1
4
512
28.8 (↓2.2)
0.5
1.0
0.0
✓
0.1
4
512
27.8 (↓3.2)
similarity
matrix
0.5
0.0
0.0
✓
0.1
4
512
26.1 (↓4.9)"
IMPORTANCE OF SIMILARITY MATRIX AND EMA,0.5165165165165165,"EMA
0.5
1.0
1.0
✗
0.1
4
512
30.4 (↓0.6)"
CONCLUSION,0.5195195195195195,"5
CONCLUSION"
CONCLUSION,0.5225225225225225,"Image-text datasets collected from the Internet are noisy, and the InfoNCE loss used by previous
works such as CLIP fails to recognize the potential matches between unpaired images and captions in
a batch. As a solution, OTTER extends the InfoNCE loss to consider the many-to-many relationship
between unpaired images and texts by computing a pair-wise similarity matrix and using entropic
optimal transport to solve for the off-diagonal matching probabilities. OTTER outperforms (32) or
ties (2) all other baselines on Google Open Images and ImageNet 10K in 34 out of 42 comparisons.
In future research, we want to test the effectiveness of OTTER on larger datasets, such as CLIP 400M."
CONCLUSION,0.5255255255255256,Published as a conference paper at ICLR 2022
REFERENCES,0.5285285285285285,REFERENCES
REFERENCES,0.5315315315315315,"Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for
attribute-based classification. CVPR, 2013."
REFERENCES,0.5345345345345346,"Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output
embeddings for fine-grained image classification. CVPR, 2015."
REFERENCES,0.5375375375375375,"Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous
clustering and representation learning. arXiv preprint arXiv:1911.05371, 2019."
REFERENCES,0.5405405405405406,"Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, and Ali Farhadi. Label refinery:
Improving ima- genet classification through label progression. arXiv preprint arXiv:1805.02641,
2018."
REFERENCES,0.5435435435435435,"Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated
corpus for learning natural language inference. 2015."
REFERENCES,0.5465465465465466,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020."
REFERENCES,0.5495495495495496,"Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.5525525525525525,"Jiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, and Changhu Wang. Learning the best pooling
strategy for visual semantic embedding. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2021a."
REFERENCES,0.5555555555555556,"Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. Graph optimal
transport for cross-domain alignment. ICML, 2020a."
REFERENCES,0.5585585585585585,"Liqun Chen, Zhe Gan, Dong Wang, Jingjing Liu, Ricardo Henao, and Lawrence Carin. Wasserstein
contrastive representation distillation. CVPR, 2021b."
REFERENCES,0.5615615615615616,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. ICML, 2020b."
REFERENCES,0.5645645645645646,"Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020c."
REFERENCES,0.5675675675675675,"Nicolas Courty, Rémi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain
adaptation. arXiv preprint arXiv:1507.00504v2, 2016."
REFERENCES,0.5705705705705706,"Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. NeurIPS, 2013."
REFERENCES,0.5735735735735735,"Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong
Tian, Matthew Yu, Peter Vajda, et al. Fbnetv3: Joint architecture-recipe search using neural
acquisition function. arXiv preprint arXiv:2006.02049, 2020."
REFERENCES,0.5765765765765766,"Bharath Bhushan Damodaran, Rémi Flamary, Vivien Seguy, and Nicolas Courty. An entropic optimal
transport loss for learning deep neural networks under label noise in remote sensing images. arXiv
preprint arXiv:1810.01163, 2018."
REFERENCES,0.5795795795795796,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. pp. 248–255, 2009."
REFERENCES,0.5825825825825826,"Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations.
arXiv preprint arXiv:2006.06666, 2020."
REFERENCES,0.5855855855855856,"Ingo Feinerer and Kurt Hornik.
wordnet: WordNet Interface, 2020.
URL https://CRAN.
R-project.org/package=wordnet. R package version 0.1-15."
REFERENCES,0.5885885885885885,Published as a conference paper at ICLR 2022
REFERENCES,0.5915915915915916,"Andrea Frome, Greg S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. Devise: A deep visual-semantic embedding model. NIPS, 2013."
REFERENCES,0.5945945945945946,"John M Giorgi, Osvald Nitski, Gary D. Bader, and Bo Wang. Declutr: Deep contrastive learning for
unsupervised textual representations. arXiv preprint arXiv:2006.03659, 2020."
REFERENCES,0.5975975975975976,"Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. CVPR, 2006."
REFERENCES,0.6006006006006006,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CVPR, 2016."
REFERENCES,0.6036036036036037,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. CVPR, 2020."
REFERENCES,0.6066066066066066,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015."
REFERENCES,0.6096096096096096,"Yasuhide Mori Hironobu, Hironobu Takahashi, and Ryuichi Oka. Image-to-word transformation
based on dividing and vector quantizing images with words. In in Boltzmann machines”, Neural
Networks, pp. 405409, 1999."
REFERENCES,0.6126126126126126,"Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. arXiv preprint arXiv:2102.05918, 2020."
REFERENCES,0.6156156156156156,"Armand Joulin*, Laurens van der Maaten*, Allan Jabri, and Nicolas Vasilache. Learning visual
features from large weakly supervised data. In ECCV, 2016."
REFERENCES,0.6186186186186187,"Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P. Xing.
Rethinking knowledge graph propagation for zero-shot learning. CVPR, 2019."
REFERENCES,0.6216216216216216,"Andrej Karpathy, Armand Joulin, and Li Fei-Fei. Deep fragment embeddings for bidirectional image
sentence mapping. In Proceedings of the 27th International Conference on Neural Information
Processing Systems - Volume 2, NIPS’14, pp. 1889–1897, Cambridge, MA, USA, 2014. MIT Press."
REFERENCES,0.6246246246246246,"Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
arXiv:1912.11370, 2020."
REFERENCES,0.6276276276276276,"Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013."
REFERENCES,0.6306306306306306,"Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab
Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.
The open images dataset v4: Unified image classification, object detection, and visual relationship
detection at scale. IJCV, 2020."
REFERENCES,0.6336336336336337,"Ang Li, Allan Jabri, Armand Joulin, and Laurens van der Maaten. Learning visual n-grams from web
data. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 4193–4202, 2017.
doi: 10.1109/ICCV.2017.449."
REFERENCES,0.6366366366366366,"Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. Visual semantic reasoning for
image-text matching. In ICCV, 2019."
REFERENCES,0.6396396396396397,"Shaoteng Liu, Jingjing Chen, Liangming Pan, Chong-Wah Ngo, Tat-Seng Chua, and Yu-Gang Jiang.
Hyperbolic visual embedding learning for zero-shot recognition. CVPR, 2020."
REFERENCES,0.6426426426426426,"Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu,
Zsolt Kira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. arXiv preprint
arXiv:2102.09480, 2021."
REFERENCES,0.6456456456456456,Published as a conference paper at ICLR 2022
REFERENCES,0.6486486486486487,"Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic
scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 4969–4983, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://www.aclweb.org/
anthology/2020.acl-main.447."
REFERENCES,0.6516516516516516,"Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic
representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019."
REFERENCES,0.6546546546546547,"S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of
aircraft. Technical report, 2013."
REFERENCES,0.6576576576576577,"Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome,
Greg S. Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of semantic
embeddings. ICLR, 2014."
REFERENCES,0.6606606606606606,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf."
REFERENCES,0.6636636636636637,"Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal
pre-training with large-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966,
2020."
REFERENCES,0.6666666666666666,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint
arXiv:2103.00020, 2021."
REFERENCES,0.6696696696696697,"Vignesh Ramanathan, Percy Liang, and Li Fei-Fei. Video event understanding using natural language
descriptions. In 2013 IEEE International Conference on Computer Vision, pp. 905–912, 2013. doi:
10.1109/ICCV.2013.117."
REFERENCES,0.6726726726726727,"Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.
10084."
REFERENCES,0.6756756756756757,"Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with
hard negative samples. arXiv preprint arXiv:2010.04592, 2020."
REFERENCES,0.6786786786786787,"Bernardino Romera-Paredes and Philip H. S. Torr. An embarrassingly simple approach to zero-shot
learning. ICML, 2015."
REFERENCES,0.6816816816816816,"Tim Salimans, Han Zhang, Alec Radford, and Dimitris N. Metaxas. Improving gans using optimal
transport. ICLR, 2018."
REFERENCES,0.6846846846846847,"Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption
annotations. arXiv preprint arXiv:2008.01392, 2020."
REFERENCES,0.6876876876876877,"Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018."
REFERENCES,0.6906906906906907,"Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. Grounded
compositional semantics for finding and describing images with sentences. Transactions of the
Association for Computational Linguistics, 2:207–218, 2014. doi: 10.1162/tacl_a_00177. URL
https://www.aclweb.org/anthology/Q14-1017."
REFERENCES,0.6936936936936937,Published as a conference paper at ICLR 2022
REFERENCES,0.6966966966966966,"Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy
labels with deep neural networks: A survey. arXiv preprint arXiv:2007.08199, 2020."
REFERENCES,0.6996996996996997,"Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit:
Wikipedia-based image text dataset for multimodal multilingual machine learning. arXiv preprint
arXiv:2103.01913, 2021."
REFERENCES,0.7027027027027027,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.7057057057057057,"Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland,
Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Commun. ACM,
59(2):64–73, jan 2016. ISSN 0001-0782. doi: 10.1145/2812802. URL https://doi.org/
10.1145/2812802."
REFERENCES,0.7087087087087087,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv
preprint arXiv:1910.10699, 2019."
REFERENCES,0.7117117117117117,"Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu,
Matthew Yu, Tao Xu, Kan Chen, et al. Fbnetv2: Differentiable neural architecture search for
spatial and channel dimensions. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 12965–12974, 2020."
REFERENCES,0.7147147147147147,"Josiah Wang, Katja Markert, and Mark Everingham. Learning models for object recognition from
natural language descriptions. In Proceedings of the British Machine Vision Conference, 2009."
REFERENCES,0.7177177177177178,"Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and
knowledge graphs. CVPR, 2018."
REFERENCES,0.7207207207207207,"P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010."
REFERENCES,0.7237237237237237,"Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 1112–1122. Association for Computational Linguistics, 2018.
URL http://aclweb.org/anthology/N18-1101."
REFERENCES,0.7267267267267268,"Baoyuan Wu, Weidong Chen, Yanbo Fan, Yong Zhang, Jinlong Hou, Jie Liu, and Tong Zhang.
Tencent ml-images: A large-scale multi-label image database for visual representation learning.
IEEE Access, 7, 2019a."
REFERENCES,0.7297297297297297,"Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian,
Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via
differentiable neural architecture search. CVPR, 2019b."
REFERENCES,0.7327327327327328,"Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. Self-training with noisy student
improves imagenet classification. CVPR, 2020."
REFERENCES,0.7357357357357357,"Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Con-
trastive learning of medical visual representations from paired images and texts. arXiv preprint
arXiv:2010.00747, 2020."
REFERENCES,0.7387387387387387,Published as a conference paper at ICLR 2022
REFERENCES,0.7417417417417418,"A
ABLATION STUDIES"
REFERENCES,0.7447447447447447,"In this section, we analyze the impact of hyper-parameters on the performance of OTTER in Table 5.
Note that our baseline hyper-parameters are different from Table 2, so the accuracy is also different.
We compare different settings using FH@K=1 on the GOI test set."
REFERENCES,0.7477477477477478,Table 5: Ablation studies. ResNet50 + DeCLUTR-Sci-base evaluated on GOI test set.
REFERENCES,0.7507507507507507,"α
γv
γt
EMA
λ
#iter
batch
FH@K=1"
REFERENCES,0.7537537537537538,"baseline
0.5
1.0
1.0
✓
0.1
4
512
31.0
0.1
1.0
1.0
✓
0.1
4
512
29.9 (↓1.1)
α
0.9
1.0
1.0
✓
0.1
4
512
28.4 (↓2.6)"
REFERENCES,0.7567567567567568,"0.5
0.0
1.0
✓
0.1
4
512
28.8 (↓2.2)
0.5
1.0
0.0
✓
0.1
4
512
27.8 (↓3.2)
similarity
matrix
0.5
0.0
0.0
✓
0.1
4
512
26.1 (↓4.9)"
REFERENCES,0.7597597597597597,"EMA
0.5
1.0
1.0
✗
0.1
4
512
30.4 (↓0.6)"
REFERENCES,0.7627627627627628,"0.5
1.0
1.0
✓
0.05
4
512
29.0 (↓2.0)
0.5
1.0
1.0
✓
0.3
4
512
28.2 (↓2.8)
0.5
1.0
1.0
✓
0.1
0
512
29.1 (↓1.9)
0.5
1.0
1.0
✓
0.1
2
512
29.3 (↓1.7)
Sinkhorn"
REFERENCES,0.7657657657657657,"0.5
1.0
1.0
✓
0.1
6
512
30.0 (↓1.0)"
REFERENCES,0.7687687687687688,"0.5
1.0
1.0
✓
0.1
4
256
25.6 (↓5.4)
batch
size
0.5
1.0
1.0
✓
0.1
4
768
28.1 (↓2.9)"
REFERENCES,0.7717717717717718,"Confidence in the image-text pairs: In Section 3.2, we define α = qv
i (i) as the probability that the
paired text caption is the correct match with the image. This reflects the confidence, or the noise
level, in the ground truth pairs. We set α = 0.1, 0.5, 0.9 in our experiment, and found that both lack
of confidence (0.1) or over-confidence (0.9) can hurt the performance. Relatively, α = 0.9 leads to
worse performance, validating the necessity of mitigating label noise."
REFERENCES,0.7747747747747747,"Image-to-image, text-to-text similarity: We included the image and text similarity when computing
the pair-wise similarities for OT. The assumption is that samples with similar images or text captions
are likely to share labels. To test this, we set γv, γt to 0 in the experiments. We found that both
image-to-image and text-to-text similarity are helpful. Relatively, text similarity seems to be more
important than image similarity, as removing text similarity leads to a larger performance drop."
REFERENCES,0.7777777777777778,"Do we need EMA teacher? In the default setting, we used the exponential moving average of the
image/text encoders to compute the similarity estimation. We test the alternative option of using the
image/text encoders themselves, and found that this leads to a small accuracy drop (0.6 points)."
REFERENCES,0.7807807807807807,"Impact of optimal transport: One key component of OTTER is to use optimal transport to match
images with text captions within a batch. However, do we really need optimal transport? To compute
the teacher distillation, a simple alternative is to use a Softmax function. As we discussed in Section
3.5, Softmax is equivalent to our Sinkhorn-Knopp implementation when we set the number of
iterations to 0. So we validate the necessity of optimal transport by setting the #iteration to 0, 2, 4, 6.
Experiments show that using Softmax (0 iteration of Sinkhorn) leads to the worst performance (-1.9).
This validates the necessity of using optimal transport to ensure all images and texts within a batch
are matchable. Besides, using 2 (fewer) and 6 (more) iterations also lead to accuracy drops (-1.7,
-1.0). Using more iterations of Sinkhorn leads to a more converged solution to the optimal transport
problem, but this does not seem to be positively correlated with better performance. We also explored
the impact of entropy regularization controlled by λ in Equation (6). Experiments show that the target
distribution being too ""hard"" (λ = 0.05) or too ""soft"" (λ = 0.30) can hurt the performance."
REFERENCES,0.7837837837837838,"Batch size: Previous works on contrastive learning show that a larger batch size raises the lower
bound of mutual information (Hadsell et al., 2006) and leads to better performance (Chen et al.,
2020b; He et al., 2020). However, for noisy image-text pairs, larger batch sizes can potentially bring
more unpaired matches. We study the impact of batch sizes by setting it to 256, 512, 768 in the
experiments. We find that both smaller (256) and larger (768) batch sizes lead to worse performance.
We hypothesize that batch size needs to be co-adapted with hyper-parameter settings of α and λ."
REFERENCES,0.7867867867867868,Published as a conference paper at ICLR 2022
REFERENCES,0.7897897897897898,"α estimates the noise level, which is positively correlated with the batch size. λ yields different
""softness"" with different batch sizes. However, further investigation is to required to validate this."
REFERENCES,0.7927927927927928,"B
PSEUDOCODE FOR OTTER"
REFERENCES,0.7957957957957958,"Algorithm 1: PyTorch Pseudocode for OTTER
# fs, ft: student and teacher model.
# tpi, tpd: learnable inverse temperature.
# eta: a large constant, e.g., 100.
# alpha: loss coefficient.
# I_N: NxN identity matrix.
# xent: cross entropy function."
REFERENCES,0.7987987987987988,"for img, txt in loader:"
REFERENCES,0.8018018018018018,"# Regular InfoNCE loss
emb_v, emb_t = fs(img, txt) # normalized embeddings.
logits = emb_v @ emb_t.T
prob_v = Softmax(logits * tpi) # normalize over t.
prob_t = Softmax(logits.T * tpi) # normalize over v.
L_infoNCE = xent(prob_v, I_N) + xent(prob_t, I_N)"
REFERENCES,0.8048048048048048,"# Similarity estimation
emb_v_t, emb_t_t = ft(img, txt).detach() # stop gradient.
sim_vv, sim_tt = emb_v_t @ emb_v_t.T, emb_t_t @ emb_t_t.T
sim_vt, sim_tv = emb_v_t @ emb_t_t.T, emb_t_t @ emb_v_t.T
S_v = sim_vv + sim_tt + sim_vt - eta * I_N
S_t = sim_tt + sim_vv + sim_tv - eta * I_N"
REFERENCES,0.8078078078078078,"# Optimal Transport Distillation
M_v = sinkhorn(S_v)
M_t = sinkhorn(S_t)
L_d = xent(prob_v, M_v) + xent(prob_t, M_t)"
REFERENCES,0.8108108108108109,"# Final loss
loss = alpha * L_infoNCE + (1-alpha) * L_d
update(fs, ft, tpi, tpd)"
REFERENCES,0.8138138138138138,"Algorithm 2: PyTorch Pseudocode for Sinkhorn-Knopp
def sinkhorn(S, lambda=0.15, niter=5):
T = exp(S / lambda)
T = T / T.sum()
N = T.shape[0]"
REFERENCES,0.8168168168168168,"# iterative row/column normalization
for _ in range(niter):
T /= (T.sum(dim=1, keepdim=True) * N)
# row normalization
T /= (T.sum(dim=0, keepdim=True) * N)
# column normalization"
REFERENCES,0.8198198198198198,"# Note if niter=0, this is equivalent to Softmax
return T /= T.sum(dim=1, keepdim=True)
# row normalization"
REFERENCES,0.8228228228228228,"C
VARIANCE ANALYSIS"
REFERENCES,0.8258258258258259,"In our experiments, we noticed variance of experimental results with identical settings. To study this,
we repeat the experiments in Table 2 with a ResNet50 image encoder and a DeCLUTR-Sci-base text
encoder for 3 times each using different random seed to analyze the variance of the experiments. We
noticed higher variance on GOI experiments. For example, for the FH@10 metric, the variance can be
up to 1.88 pts. Note that the mean accuracy’s gap between OTTER and baselines are all significantly
larger than the standard deviation, indicating that the performance improvement of OTTER is not a"
REFERENCES,0.8288288288288288,Published as a conference paper at ICLR 2022
REFERENCES,0.8318318318318318,"result of randomness. However, such high variance is worth noting and requires future investigation
on how to reduce it."
REFERENCES,0.8348348348348348,"Table 6: Flat hit @K on test sets of Google Open Images and ImageNet10K from Tencent-ML-
Images."
REFERENCES,0.8378378378378378,"Data
Method
GOI FH@K (%)
IN10K FH@K (%)
1
5
10
1
5
10"
REFERENCES,0.8408408408408409,"CC
(3M)"
REFERENCES,0.8438438438438438,"InfoNCE
27.1 ± 0.23
56.1 ± 0.92
66.4 ± 1.05
10.9 ± 0.38
29.4 ± 0.75
40.5 ± 0.76
LS
26.7 ± 1.00
55.9 ± 1.31
67.5 ± 1.31
10.1 ± 0.67
29.6 ± 0.81
39.8 ± 1.03
KD
26.7 ± 0.81
55.3 ± 1.67
67.1 ± 1.71
10.0 ± 0.75
27.5 ± 1.42
38.5 ± 1.14
OTTER
28.6 ± 1.17
59.6 ± 1.71
70.9 ± 1.88
12.0 ± 0.31
31.8 ± 0.40
42.1 ± 0.26"
REFERENCES,0.8468468468468469,"D
QUANTITATIVE ANALYSIS ON THE IMAGE-TEXT COMPOSITIONALITY ON
CUB"
REFERENCES,0.8498498498498499,"ALIGN Jia et al. (2020) presents an interesting demonstration of the compositionality of image and
text embeddings generated by language supervised vision models. On an image retrieval task, a
query is formed by adding an image embedding vector to a text embedding vector. The returned
image is expected to be similar to the image query and the text query. ALIGN demonstrates
the compositionality by qualitatively showing several retrieval results, but does not provide any
quantitative evaluations. In this paper, we design a preliminary benchmark based on the CUB dataset
(Welinder et al. (2010)) to evaluate the image-text compositionality."
REFERENCES,0.8528528528528528,",PDJHTXHU\HPE"
REFERENCES,0.8558558558558559,7H[WTXHU\HPE
REFERENCES,0.8588588588588588,7H[WTXHU\HPE 
REFERENCES,0.8618618618618619,7H[WTXHU\HPE
REFERENCES,0.8648648648648649,\HOORZIRUHKHDGFRORU
REFERENCES,0.8678678678678678,EODFNH\HFRORU
REFERENCES,0.8708708708708709,ZKLWHEUHDVWFRORU
REFERENCES,0.8738738738738738,4XHU\HPEHGGLQJ 
REFERENCES,0.8768768768768769,",PDJHHPEHGGLQJ"
REFERENCES,0.8798798798798799,",PDJHHPEHGGLQJ"
REFERENCES,0.8828828828828829,",PDJHHPEHGGLQJ"
REFERENCES,0.8858858858858859,",PDJHHPEHGGLQJ
    "
REFERENCES,0.8888888888888888,"&RVLQH
6LPLODULW\
'DWDVHWLPDJHHPEHGGLQJ
4XHU\HPEHGGLQJV
,PDJHWH[WTXHULHV"
REFERENCES,0.8918918918918919,Figure 5: Illustration of the image + text -> image retrieval.
REFERENCES,0.8948948948948949,"CUB (Welinder et al. (2010)) consists of 6033 bird images, and each image-i is annotated with a
set of bird attributes, which we denote as Ai. In this dataset, there are in total 288 unique attributes.
Given an image and several CUB bird attributes in text, we generate image and text embeddings
using our pretrained models and add the embeddings together to form a query embedding vector.
We use the query vector to match images in CUB, and choose the nearest neighbor, based on cosine
similarity, as the retrieved image. This process is illustrated in Figure 5. To evaluate the retrieval
quality, we compare the overlap between the retrieved image’s attributes with the image-text query’s
attributes."
REFERENCES,0.8978978978978979,"We now describe how we generate text queries in addition to an image query. Randomly adding
text attributes to image queries may result in unmatchable queries. To ensure that the added text
query is sensible and the combined image-text query can be matched to an image in the dataset,
we obtain a text query with the following approach: We first select a pair of images, denoted as
image-i and image-j. We use image-i as the image query, and let Qv = Ai be the image query’s
attribute set. Then, to form the text query, we compare the differences of image-j and i, and let
Qt = Aj −Ai be the text query’s attribute set. The combined image-text query should contain
attributes Q = Qv S Qt. We show an example of image query, text query, and combined query in
Figure 6. Following this process, we generate 1,000,000 image-text queries from randomly sampled
image pairs from CUB where each image pair shares at least 10 common attributes. The image-text
queries used in our experiment are provided in the attached file image_text_query_list.txt
in the supplementary material."
REFERENCES,0.9009009009009009,Published as a conference paper at ICLR 2022
REFERENCES,0.9039039039039038,Figure 6: Image and text query example.
REFERENCES,0.9069069069069069,"Table 7: Quantitative Vision-Language Compositionality Benchmark. OR represents Overlapping
Rate, IOR represents Image Overlapping Rate, and TOR represents Text Overlapping Rate."
REFERENCES,0.9099099099099099,"Model
Image Encoder
Text Encoder
Method
OR (%)
IOR (%)
TOR (%)"
REFERENCES,0.9129129129129129,"Baseline
-
-
-
27.7
31.3
23.2"
REFERENCES,0.9159159159159159,"CLIP
ResNet50
CLIP Transformer
InfoNCE
35.7
36.2
34.5"
REFERENCES,0.918918918918919,"Ours
ResNet50
DeCLUTR
-Sci-base"
REFERENCES,0.9219219219219219,"InfoNCE
34.5
33.6
36.8
LS
34.2
33.6
35.8
KD
33.2
32.9
34.4
OTTER
34.7
33.9
36.7"
REFERENCES,0.924924924924925,"Our evaluation measures the overlap between the retrieved image’s attribute set Rk with the image-
text query set Qk, image query set Qv
k, text query set Qt
k. Specifically, we compute: 1) the attributes
overlapping rate (OR)
1
N
PN
k=1
|Qk∩Rk|"
REFERENCES,0.9279279279279279,"|Qk|
. This measures the retrieval quality for the combined"
REFERENCES,0.9309309309309309,image-text query. 2) the average image attributes overlapping rate (IOR) 1
REFERENCES,0.933933933933934,"N
PN
k=1
|Qv
k∩Rk|
|Qv
k|
. This
evaluates if the returned image hits/misses attributes in the image query. 3) the average text attributes
overlapping rate (TOR) 1"
REFERENCES,0.9369369369369369,"N
PN
k=1
|Qt
k∩Rk|
|Qt
k|
. This evaluates if the retrieved image hits/misses attributes
of text queries. An ideal match should achieve higher scores in OR, IOR, and TOR simultaneously."
REFERENCES,0.93993993993994,"We report our results comparing with CLIP and other baselines in Table 7. First, we report the
measurement of a random baseline. With this random baseline, for any image-text query, we return a
random image from the dataset. We can see that this gives a non-trivial OR (27.7%), IOR (31.3%),
and TOR (23.2%). This is not surprising because images in CUB have many overalapping attributes.
However, although both CLIP and our models are never directly trained on CUB nor trained to
predict bird attributes, their overlapping rates are significantly higher than the random baseline. CLIP
achieves comparable accuracy with our models, with 1% higher OR, 2.3% higher IOR, and -2.2%
lower TOR. Among different training methods, OTTER outperforms all baselines in OR and IOR, and
achieves slightly worse (-0.1%) TOR than the InfoNCE baseline. Also note that our models achieve
similar IOR and TOR, which shows both image and text queries are considered. This demonstrates
good compositionality."
REFERENCES,0.9429429429429429,"In Figure 7, we show qualitative results of our model trained by OTTER. For example, in the first
row, we add yellow forehead color, black eye color, yellow upper-parts color and yellow breast"
REFERENCES,0.9459459459459459,Published as a conference paper at ICLR 2022
REFERENCES,0.948948948948949,"Figure 7: Qualitative results of text + image retrieval. Blue lines indicate texts attributes, and red
lines indicate image attributes. For the demonstration purpose, we only label parts of the attributes
that are easy to recognize."
REFERENCES,0.9519519519519519,"color as text attributes to an image of a black bird. The retrieved bird image contains attributes
from both image and text queries – it has yellow body colors while maintains black leg and black
wing colors. From both quantitative and qualitative results, our model demonstrates good image-text
compositionality."
REFERENCES,0.954954954954955,Published as a conference paper at ICLR 2022
REFERENCES,0.9579579579579579,"E
IMAGE ATTRIBUTIONS"
REFERENCES,0.960960960960961,"1. Figure 1, Paul Bica, Coast of Kauai, Hawaii, CC BY 2.0"
REFERENCES,0.963963963963964,"2. Figure 1, James St. John, Columnar-jointed rhyolitic obsidian lava flow, CC BY 2.0"
REFERENCES,0.9669669669669669,"3. Figure 1, Mordaka, QK9A1397, CC BY-SA 4.0"
REFERENCES,0.96996996996997,"4. Figure 1, Alan Reid, Heathery moor on the flank of Stone Saul, CC BY-SA 2.0"
REFERENCES,0.972972972972973,"5. Figure 1, Jimmy Emmerson, The Tormented Valley, CC BY-NC-ND 2.0"
REFERENCES,0.975975975975976,"6. Figure 1, Roman Boed from The Netherlands, Black Forest- Meadow (10561897306), CC
BY 2.0"
REFERENCES,0.978978978978979,"7. Figure 1, Daniel Clerc / CC-BY-SA-3.0, 2013 bois herpin 013, CC BY-SA 3.0"
REFERENCES,0.9819819819819819,"8. Figure 1, Dave Bevis, 22 and 24 High Street, Newcastle-under-Lyme, CC BY-SA 2.0"
REFERENCES,0.984984984984985,"9. Figure 1, Nilfanion, Thatched cottages in Coverack (8379), CC BY-SA 4.0"
REFERENCES,0.987987987987988,"10. Figure 4, TheAHL, Chuck Kobasew (cropped), CC BY 2.0"
REFERENCES,0.990990990990991,"11. Figure 4, Mark Mauno, Jasper Fitzi, CC BY 2.0"
REFERENCES,0.993993993993994,"12. Figure 4, Famartin, 2020-04-27 18 55 23 A Calico cat looking for food in a kitchen in the
Franklin Farm section of Oak Hill, Fairfax County, Virginia, CC BY-SA 4.0"
REFERENCES,0.996996996996997,"13. Figure 4, Beth, Haircut-4, CC BY 2.0"
