Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003067484662576687,"The scope of this paper is generative modeling through diffusion processes. An
approach falling within this paradigm is the work of Song et al. (2021), which
relies on a time-reversal argument to construct a diffusion process targeting the
desired data distribution. We show that the time-reversal argument, common to all
denoising diffusion probabilistic modeling proposals, is not necessary. We obtain
diffusion processes targeting the desired data distribution by taking appropriate
mixtures of diffusion bridges. The resulting transport is exact by construction,
allows for greater ﬂexibility in choosing the dynamics of the underlying diffusion,
and can be approximated by means of a neural network via novel training objectives.
We develop a unifying view of the drift adjustments corresponding to our and
to time-reversal approaches and make use of this representation to inspect the
inner workings of diffusion-based generative models. Finally, we leverage on
scalable simulation and inference techniques common in spatial statistics to move
beyond fully factorial distributions in the underlying diffusion dynamics. The
methodological advances contained in this work contribute toward establishing a
general framework for generative modeling based on diffusion processes."
INTRODUCTION,0.006134969325153374,"1
INTRODUCTION"
INTRODUCTION,0.009202453987730062,"Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020;
Song et al., 2021) is a recent generative modeling paradigm exhibiting strong empirical performance.
Consider a dataset of N samples D = {x(n)}N
n=1 with empirical distribution PD. The unifying
key steps underlying DDPM approaches are: (i) the deﬁnition of a stochastic process with initial
distribution PD, whose forward-time (noising) dynamics progressively transform PD toward a simple
data-independent distribution PZ; (ii) the derivation of the backward-time (denoising / sampling)
dynamics transforming PZ toward PD; (iii) the approximation of the backward-time transitions by
means of a neural network. Following the training step (iii), a sample whose distribution approximates
PD is drawn by (iv) simulating from the approximated backward-time transitions starting with a
sample from PZ. Both discrete-time (Ho et al., 2020) and continuous-time (Song et al., 2021)
formulations of DDPM have been pursued. This work focuses on the latter case, to which we refer
as diffusion time-reversal transport (DTRT). As in DTRT, dynamics are speciﬁed through diffusion
processes, i.e. solutions to stochastic differential equations (SDE) with associated drift f(·) and
diffusion g(·) coefﬁcients. A number of approximations are involved in the aforementioned steps.
Firstly, as the dynamics are deﬁned on a ﬁnite time interval, a dependency from PD is retained
through the noising process. Hence, starting with a sample from the data-independent distribution
PZ in (iv) introduces an approximation. Secondly, while the backward-time dynamics of (ii) are
directly available for diffusions, they are approximated by means of a neural network in (iii). Thirdly,
sampling in (iv) is achieved through a discretization on a time-grid, which introduces a discretization
error. De Bortoli et al. (2021, Theorem 1) links these approximations to the total variation distance
between the distribution of the generated samples from (iv) and PD."
INTRODUCTION,0.012269938650306749,"In our ﬁrst methodological contribution we develop a procedure for constructing diffusion processes
targeting PD without relying on time-reversal arguments. The proposed transport (coupling) between
PZ and PD is achieved by: (1) specifying a diffusion process X on [0, τ] starting from a generic x0;
(2) conditioning X on hitting a generic xτ at time τ, thus obtaining a diffusion bridge; (3) taking a
bivariate mixture Π0,τ of diffusion bridges over (x0, xτ) with marginals Π0 = PZ and Πτ = PD,
obtaining a mixture process M; (4) matching the marginal distribution of M over [0, τ] with a
diffusion process, resulting in a diffusion with initial distribution PZ and terminal distribution PD."
INTRODUCTION,0.015337423312883436,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.018404907975460124,"The realized diffusion bridge mixture transport (DBMT) between PZ and PD is exact by construction.
We thus sidestep the approximation common to all DDPM approaches due to the dependency from
PD retained through the noising process. Moreover, the DBMT can be realized for almost arbitrary
PZ, f(·) and g(·). This increased ﬂexibility is a departure from the DTRT where f(·) and g(·)
need to be chosen to obtain convergence toward a simple distribution PZ."
INTRODUCTION,0.02147239263803681,"Similarly to the DTRT, achieving the DBMT requires the computation of a drift adjustment term
which depends on D. For a SDE class of interest, we develop a uniﬁed and interpretable representation
of DTRT and DBMT drift adjustments as simple transformations of conditional expectations over
D. This novel result provides insights on the target mapping that we aim to approximate and on
the quality of approximation achieved by the trained score models of Song et al. (2021). Having
deﬁned for the DBMT a Fisher divergence objective similarly to Song et al. (2021), we leverage on
this uniﬁed representation to deﬁne two additional training objectives featuring appealing properties."
INTRODUCTION,0.024539877300613498,"In our last methodological contribution we extend the class of SDEs that can be realistically employed
in computer vision applications. Speciﬁcally, computational considerations have so far restricted the
transitions of the stochastic processes employed in DDPM to be fully factorials. We view images
at a given resolution as taking values over a 2D lattice which discretizes the continuous coordinate
system [0, 1]2 representing heights and widths. Diffusion processes are viewed as spatio-temporal
processes with spatial support [0, 1]2. Doing so, it is possible to leverage on scalable simulation and
inference techniques from spatial statistics and consider more realistic diffusion transitions."
INTRODUCTION,0.027607361963190184,"This paper is structured as follows. In Section 2 we review the DTRT of Song et al. (2021) and in
Section 3 we introduce the DBMT. In order to implement the DTRT and the DBMT it is necessary
to specify the underlying SDE, i.e. the coefﬁcients f(·) and g(·). We study a class of interest in
Section 4. The uniﬁed view of drift adjustments is introduced in Section 5. Section 6 develops the
training objectives and Section 7 reviews the obtained results and ﬁnalizes the DBMT construction.
In Section 8 we establish the connection with spatio-temporal processes. We conclude in Section 9.
Appendices A to D contain the theoretical framework, assumptions, proofs, and additional material."
INTRODUCTION,0.03067484662576687,"Notation and conventions: we use uppercase notation for probability distributions (measures, laws)
and lowercase notation for densities; each probability distribution, and corresponding density, is
uniquely identiﬁed by its associated letter not by its arguments (which are muted); for example
P(dx) is a distribution, p(x) is its corresponding density; random elements are always uppercase (an
exception is made for times, always lowercase for typographical reasons); if P is the distribution of a
stochastic process, we use subscript notation to refer to its ﬁnite dimensional distributions (densities
with p), conditional or not, for some collection of times; for example pt′|t denotes a transition density,
which is understood to be a function of four arguments pt′|t(y|x) = f(t, t′, x, y); δx is the delta
distribution at x and ⊗is used for product distributions; we refer directly to a given SDE instead of
referring to the diffusion process satisfying such SDE when no ambiguity arises; we use [a]i and
[A]i,j for vector and matrix indexing, A⊤for matrix transposition."
DIFFUSION TIME-REVERSAL TRANSPORT,0.03374233128834356,"2
DIFFUSION TIME-REVERSAL TRANSPORT"
DIFFUSION TIME-REVERSAL TRANSPORT,0.03680981595092025,"The starting point of Song et al. (2021) is a diffusion process Y satisfying a generic D-dimensional
time-inhomogenous SDE with initial distribution Y0 ∼PD
dYr = f(Yr, r)dr + g(Yr, r)dWr,
(1)
over noising time r ∈[0, τ]. Thorough this paper we denote with Q the law of the diffusion solving
(1) and with q the corresponding densities. Thus, let qr′|r(y|x), 0 ≤r < r′ ≤τ, be the transition
density of (1), and let qr(y), 0 < r ≤τ, be the marginal density of (1). As Y0 ∼PD, we have"
DIFFUSION TIME-REVERSAL TRANSPORT,0.03987730061349693,"qr(y) = 1 N N
X"
DIFFUSION TIME-REVERSAL TRANSPORT,0.04294478527607362,"n=1
qr|0(y|x(n)).
(2)"
DIFFUSION TIME-REVERSAL TRANSPORT,0.046012269938650305,"The dynamics of (1) over the reversed, i.e. sampling, time t = τ −r, t ∈[0, τ], are given by
(Anderson, 1982; Haussmann & Pardoux, 1986; Millet et al., 1989)
dXt = [−f(Xt, r) + ∇· G(Xt, r) + G(Xt, r) ∇Xt ln qr(Xt)] dt + g(Xt, r)dWt,
(3)
where r = τ −t is the remaining sampling time, G(x, r) = g(x, r)g(x, r)⊤and the D-dimensional
vector ∇· G(x, r) is deﬁned by [∇· G(x, r)]i = PD
j=1 ∇xj[G(x, r)]i,j. That is the processes Xt"
DIFFUSION TIME-REVERSAL TRANSPORT,0.049079754601226995,Under review as a conference paper at ICLR 2022
DIFFUSION TIME-REVERSAL TRANSPORT,0.05214723926380368,"and Yr = Yτ−t have the same distribution. Approximating the terminal distribution Qτ of (1), i.e.
the initial distribution of (3), with PZ, X0 is sampled from PZ and (3) is discretized and integrated
over t to produce a sample Xτ approximately distributed as PD."
DIFFUSION TIME-REVERSAL TRANSPORT,0.05521472392638037,"The computation of the multiplicative drift adjustment ∇y ln qr(y) entering (3), i.e. the score of the
marginal density (2), requires in principle O(N) operations. Let sφ(y, r) be a neural network for
which we would like sφ(y, r) ≈∇y ln qr(y). It remains to ﬁnd a suitable training objective for which
unbiased gradients with respect to φ can be obtained at O(1) cost with respect to the dataset size N.
As qr(y) has a mixture representation, the identity of Vincent (2011) for Fisher divergences provides
us with the desired objective for a ﬁxed r ∈(0, τ]"
DIFFUSION TIME-REVERSAL TRANSPORT,0.05828220858895705,"LFD,DTRT(φ, r) =
E
Yr∼Qr"
DIFFUSION TIME-REVERSAL TRANSPORT,0.06134969325153374,"h ∇Yr ln qr(Yr) −sφ(Yr, r)
2i"
DIFFUSION TIME-REVERSAL TRANSPORT,0.06441717791411043,"=
E
(Y0,Yr)∼Q0,r"
DIFFUSION TIME-REVERSAL TRANSPORT,0.06748466257668712,"h ∇Yr ln qr|0(Yr|Y0) −sφ(Yr, r)
2i
.
(4)"
DIFFUSION TIME-REVERSAL TRANSPORT,0.0705521472392638,"The key point is that an unbiased, O(1) with respect to N, mini-batch Monte Carlo (MC) estimator
for the expectation (4) can be trivially obtained by sampling a batch Y0 ∼PD, Yr ∼Qr|0(dyr|Y0),
and evaluating the average loss over the batch. In order to achieve a global approximation over the
whole time interval (0, τ], Song et al. (2021) proposes uniform sampling of time r"
DIFFUSION TIME-REVERSAL TRANSPORT,0.0736196319018405,"LFD,DTRT(φ) =
E
r∼U(0,τ],(Y0,Yr)∼Q0,r"
DIFFUSION TIME-REVERSAL TRANSPORT,0.07668711656441718,"h
Rr
 ∇Yr ln qr|0(Yr|Y0) −sφ(Yr, r)
2i
,
(5)"
DIFFUSION TIME-REVERSAL TRANSPORT,0.07975460122699386,"where Rr = E[∥∇Yr ln qr|0(Yr|Y0)∥2]
−1 is a regularization term. A MC estimator for (5) is con-
structed by augmenting the MC estimator for (4) with the additional sampling step r ∼U(0, τ]."
DIFFUSION BRIDGE MIXTURE TRANSPORT,0.08282208588957055,"3
DIFFUSION BRIDGE MIXTURE TRANSPORT"
DIFFUSION BRIDGE MIXTURE TRANSPORT,0.08588957055214724,"Our starting point is a generic D-dimensional time-inhomogenous SDE which, in contrast to Song
et al. (2021), is directly deﬁned on the sampling time t ∈[0, τ]
dXt = f(Xt, t)dt + g(Xt, t)dWt.
(6)
We reserve P·|0(·|x0) to denote the law of the diffusion solving (6) for a given starting value x0 and
p·|0(·|x0) to denote the corresponding densities."
DIFFUSION BRIDGES,0.08895705521472393,"3.1
DIFFUSION BRIDGES"
DIFFUSION BRIDGES,0.09202453987730061,"Diffusion bridges are central to the proposed methodology, in this Section we cover their basic theory.
A diffusion bridge is a diffusion process starting from a given value which is conditioned on hitting a
terminal value. It is a deep result, and consequence of Doob h-transforms (Särkkä & Solin (2019,
Chapter 7.9), Rogers & Williams (2000, Chapter IV.6.39)), that a diffusion processes pinned down on
both ends is still a diffusion process. In particular the Markov property is preserved. More precisely,
(6) with initial value x0 conditioned on hitting a terminal value xτ at time τ is characterized the
following SDE on [0, τ] with initial value x0 (Särkkä & Solin, 2019, Theorem 7.11)"
DIFFUSION BRIDGES,0.0950920245398773,"dXt =

f(Xt, t) + G(Xt, t) ∇Xt ln pτ|t(xτ|Xt)

dt + g(Xt, t)dWt,
(7)"
DIFFUSION BRIDGES,0.09815950920245399,"where G(x, t) = g(x, t)g(x, t)⊤. The multiplicative adjustment factor ∇xt ln pτ|t(xτ|xt) forces the
process to hit xτ at time τ and the diffusion process solving (7) is known as the diffusion bridge from
(x0, 0) to (xτ, τ). As previously noted, pτ|t(xτ|xt) in (7) refers to the transition density of (6)."
DIFFUSION MIXTURES,0.10122699386503067,"3.2
DIFFUSION MIXTURES"
DIFFUSION MIXTURES,0.10429447852760736,"The proposed transport construction relies on a representation result for diffusion mixtures. We
present here an informal version and report the precise statement, the required assumptions, and the
proof in Appendix A.
Theorem 1 (Diffusion mixture representation — informal). Let {Xλ}, λ ∈Λ be a collection of
diffusions with associated SDEs {dXλ
t } and marginal densities {πλ
t }. Let L be a mixing distribution
on Λ, πt be the L-mixture of {πλ
t }. Then there exists a diffusion process X with marginal πt. X
follows a SDE whose drift and diffusion coefﬁcients are weighted averages of the corresponding
coefﬁcients in {dXλ
t }, where the weights are proportional to {πλ
t } and to the mixing density."
DIFFUSION MIXTURES,0.10736196319018405,Under review as a conference paper at ICLR 2022
DIFFUSION MIXTURES,0.11042944785276074,"Theorem 1 is ﬁrst established in Brigo (2002, Corollary 1.3) limitedly to ﬁnite mixtures and 1-
dimensional diffusions. The proof of Theorem 1 in Appendix A is more direct and extends the result
to the required multivariate setting. In Section 3.1 we introduced diffusion bridges mapping arbitrary
initial values x0 to arbitrary ﬁnal values xτ. Let Π0,τ denote a generic bivariate distribution on
RD × RD with marginals Π0, Πτ. We deﬁne the diffusion mixture M as the mixture of diffusion
bridges corresponding to (X0, Xτ) ∼Π0,τ. That is, we apply Theorem 1 to the collection of
diffusion bridges (7) indexed by their initial and terminal values, λ = (x0, xτ), Λ = RD × RD, with
mixing distribution L(dλ) = Π0,τ(dx0, dxτ). By Theorem 1 the following SDE on [0, τ] with initial
distribution Π0 has the same marginal distribution as M, in particular its terminal distribution is Πτ
dXt = µ(Xt, t)dt + g(Xt, t)dWt,"
DIFFUSION MIXTURES,0.11349693251533742,"µ(xt, t) = f(xt, t) + G(xt, t)
Z
∇xt ln pτ|t(xτ|xt)pt|0,τ(xt|x0, xτ)"
DIFFUSION MIXTURES,0.1165644171779141,"πt(xt)
Π0,τ(dx0, dxτ)
|
{z
}
A(xt,t) ,"
DIFFUSION MIXTURES,0.1196319018404908,"πt(xt) =
Z
pt|0,τ(xt|x0, xτ)Π0,τ(dx0, dxτ). (8)"
DIFFUSION MIXTURES,0.12269938650306748,"In (8), A(xt, t) gives the multiplicative drift adjustment factor for (6). A case of particular interest
occurs when Π0 puts all the mass on a single value x0. In the following we refer to A(xt, t, x0)
in stance of A(xt, t), and to πt|0(xt|x0) in stance of πt(xt), when it is necessary to distinguish
this speciﬁc case. We also extend the scope of Π to indicate the law of M. Indeed, we already
denoted with Π0,τ its initial-terminal distribution, and with πt its marginal density. Accordingly,
A(xt, t) = EXτ ∼Π(dxτ |xt)[∇xt ln pτ|t(Xτ|xt)]. The transport from PZ to PD is then achieved by
Πτ = PD and PZ = Π0 (PZ can be arbitrarily deﬁned). As PD is an empirical distribution, the
integral in (8) with respect to xτ reduces to averages over D. In summary, the diffusion X solution
of (8) realizes the proposed transport from PZ to PD by matching the marginal distribution of M."
SDE CLASS,0.12576687116564417,"4
SDE CLASS"
SDE CLASS,0.12883435582822086,"The starting point of the proposed transport is the unconstrained SDE (6). In this Section we deﬁne
SDEs which are realized through a time-change of simpler SDEs and which are general enough to
subsume the SDEs introduced in Song et al. (2021). Consider the D-dimensional SDEs"
SDE CLASS,0.13190184049079753,"dZt = Γ1/2dWt,
(9)"
SDE CLASS,0.13496932515337423,"dZt = αtZtdt + Γ1/2dWt,
(10)
where αt ̸= 0 is a scalar function and G(Xt, t) = Γ introduces an arbitrary covariance structure. (9)
is the SDE of a correlated and scaled Brownian motion and (10) is the SDE of an Ornstein-Uhlenbeck
process driven by a correlated and scaled Brownian motion. The transition densities of (9) and (10)
are Gaussian (Appendix B). We denote both with ept′|t, informally (9) is a special case of (10) with
αt = 0. SDE (10) in the time-homogenous case αt = −1/2 has stationary distribution ND(0, Γ). We
now introduce the time-change. Let βt > 0 be a continuous function on [0, τ]. Then bt =
R t
0 βudu
deﬁnes a monotonically (strictly) increasing function bt : [0, τ] →[0, bτ]. The following SDEs on
[0, τ] represent the class of dynamics for (1) and (6) on which we focus on the rest of this paper"
SDE CLASS,0.13803680981595093,"dXt =
p"
SDE CLASS,0.1411042944785276,"βtΓ1/2dWt,
(11)"
SDE CLASS,0.1441717791411043,"dXt = αtβtXtdt +
p"
SDE CLASS,0.147239263803681,"βtΓ1/2dWt,
(12)
and G(x, t) = βtΓ. The standard time-change result for diffusions (Øksendal, 2003, Theorem 8.5.1)
establishes that the processes Xt respectively from (11) and (12) are equivalent in law to their time-
scaled counterparts Zbt from (9) and (10). That is, SDEs (11) and (12) correspond to the evolution of
the simpler SDEs (9) and (10) under a non-linear time wrapping where time ﬂows with instantaneous
intensity βt. For both (11) and (12) the time-change argument yields pτ|t(y|x) = epbτ |bt(y|x) for the
transition density of (6), and equivalently for the transition density qτ|t of (1). We thus obtain"
SDE CLASS,0.15030674846625766,"pτ|t(xτ|xt) = ND(xτ; xta(t, τ), Γv(t, τ))
(13)"
SDE CLASS,0.15337423312883436,"for appropriate scalar functions a(t, τ), v(t, τ) with v(t, τ) > 0 (Appendix B). By direct computation"
SDE CLASS,0.15644171779141106,"∇xt ln pτ|t(xτ|xt) = Γ−1

xτ
a(t, τ) −xt"
SDE CLASS,0.15950920245398773,"a2(t, τ)"
SDE CLASS,0.16257668711656442,"v(t, τ) ,
(14)"
SDE CLASS,0.1656441717791411,Under review as a conference paper at ICLR 2022
SDE CLASS,0.1687116564417178,"∇xτ ln pτ|t(xτ|xt) = Γ−1

xta(t, τ) −xτ"
SDE CLASS,0.17177914110429449,"
1
v(t, τ).
(15)"
SDE CLASS,0.17484662576687116,From Bayes theorem and the Markov property we have
SDE CLASS,0.17791411042944785,"pt|0,τ(xt|x0, xτ) = ND (xt; x0abr(0, t, τ) + xτabr(0, t, τ), Γvbr(0, t, τ)) ,
(16)"
SDE CLASS,0.18098159509202455,"where once again abr(0, t, τ), abr(0, t, τ) and vbr(0, t, τ) > 0 are scalar functions given in Ap-
pendix B. Finally, by direct computation"
SDE CLASS,0.18404907975460122,"∇xt ln pt|0,τ(xt|x0, xτ) = Γ−1 x0abr(0, t, τ) + xτabr(0, t, τ) −xt"
SDE CLASS,0.18711656441717792,"vbr(0, t, τ)
.
(17)"
SDE CLASS,0.1901840490797546,"These results provide all the analytical formulas required for the computation of the adjustment
factors A(xt, t), A(xt, t, x0) and of the training objectives used to approximate them (Section 6)."
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.19325153374233128,"4.1
INTERPRETATION OF DENOISING TIME-REVERSED SDES"
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.19631901840490798,"Song et al. (2021) introduces two speciﬁcations of (1), named VESDE and VPSDE, which are
respectively given by"
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.19938650306748465,"dYr =
p"
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.20245398773006135,"βve,rdWr,
(18)"
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.20552147239263804,dYr = −1
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.2085889570552147,"2βvp,rYrdr +
p"
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.2116564417177914,"βvp,rdWr.
(19)"
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.2147239263803681,"See Appendix B for the functional form of βve,r and βvp,r. We thus recover (18) and (19) from (11)
and (12) with Γ = I and αt = −1/2. That is, VESDE and VPSDE correspond to a time change of
the much simpler SDEs for the standard Brownian motion and for the standard Langevin SDE"
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.21779141104294478,"dZr = dWr,"
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.22085889570552147,dZr = −1
INTERPRETATION OF DENOISING TIME-REVERSED SDES,0.22392638036809817,2Zrdr + dWr.
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.22699386503067484,"5
UNIFIED VIEW OF DRIFT ADJUSTMENTS"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.23006134969325154,"The linearity of SDEs (11) and (12), underlying our and Song et al. (2021) works, has the important
consequence that (14) and (15) are linear in xt. This in turn allow us to derive an alternative
representation for the drift adjustment in (8). Indeed, substituting (14) in (8) gives (Appendix A)"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2331288343558282,"G(x, t)A(x, t) = βt"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2361963190184049,"
1
a(t, τ)
E
Xτ ∼Πτ|t(dxτ |x)[Xτ] −x
 a2(t, τ)"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2392638036809816,"v(t, τ) .
(20)"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.24233128834355827,"Similarly, for the time-reversal drift adjustment term in (3) we have (Appendix A)"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.24539877300613497,"G(x, r) ∇x ln qr(x) = βr"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.24846625766871167,"
a(0, r)
E
Xτ ∼Q0|r(dxτ |x)[Xτ] −x

1
v(0, r).
(21)"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.25153374233128833,"The relations (20) and (21) provide a uniﬁed view of the inner workings of the DTRT and of the
DBMT targeting PD. In the following we always refer to sampling time t. Remember that r = τ −t
is the remaining sampling time. For ease of exposition we assume βt = 1, as shown in Section 4 the
term βt corresponds to a time-warping. The terms a(t, τ), a(0, r) are “integrated scalings”. They are
equal to 1 for (11) and the same holds for (12) as r →0. The terms v(t, τ), v(0, r) are “integrated
variances”. They are equal to r for (11) and the same holds for (12) as r →0. We commonly refer
to E[Xτ|x, t] for expectation terms in (20) and (21). Both drift adjustments (20) and (21) are thus
essentially of the form (E[Xτ|x, t] −x)v−1
r
where the term v−1
r
diverges as r →0."
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.254601226993865,"The expectations E[Xτ|x, t] are convex linear combinations of the samples x(n) from D. Explicitly,
E[Xτ|x, t] = PN
n=1 ω(x, t)(n)x(n), where the weights ω(x, t)(n) are the probabilities, under the
distributions Q (time-reversal sampling process (3)) and Π (mixture of diffusions process M from
Section 3.2), of reaching each state x(n) at terminal time τ from x at time t. By construction, the
initial weights entering expectation (20) are all equal to 1/N when X starts from a ﬁxed value x0,
and are so on average when X0 is stochastic. The initial weights entering expectation (21) are"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.25766871165644173,Under review as a conference paper at ICLR 2022
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2607361963190184,"on average approximately equal to 1/N, depending on the quality of the approximation PZ ≈Qτ.
Thus, E[Xτ|X0, 0] is an averaging of many samples x(n). As time progresses, changes in Xt
correspond to changes in E[Xτ|Xt, t] through changes in the weights ω(x, t)(n). Eventually all
mass concentrates on a single weight ω(x, t)(∗) corresponding to a dataset sample x(∗). Ultimately,
the attractor dynamics implied by (E[Xτ|x, t] −x)v−1
r
drive Xt to x(∗). We provide an inspection
in Figure 1, where D(CIFAR) stands for the training portion of the CIFAR10 dataset, and Euler(T)
corresponds to the Euler scheme (Kloeden & Platen, 1992) applied with T discretization steps."
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.26380368098159507,"In the VESDE and VPSDE of Song et al. (2021) we have G(x, r) = βrI and reversing (21) gives"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2668711656441718,"E
Xτ ∼Q0|r(dxτ |x)[Xτ] = v(0, r) ∇x ln qr(x) + x"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.26993865030674846,"a(0, r)
,
(22)"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.27300613496932513,"where ∇y ln qr(y) is the true score. We can thus take a trained score model sφ(x, r) ≈∇x ln qr(y),
plug it in (22), and verity the extent to which E[Xτ|x, t] has been approximated, see Figure 1."
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.27607361963190186,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8 1.0"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2791411042944785,"Figure 1: VPSDE model — 2nd cells’ row: evolution of a trajectory of X over sampling time (its
terminal value Xτ is the generated sample) via the Euler(1000) discretization of (3) using the true
score ∇y ln qr(y) for D(CIFAR); line-plot: weights’ evolution ω(Xt, t)(n) for all x(n) in D(CIFAR)
for the same X (cyclical palette, many weights cannot be distinguished as they remain close to 0); 1st
cells’ row: E[Xτ|Xt, t] evolution for the same X; 3rd and 4th cells’ rows: same as 1st and 2nd cells’
rows for another trajectory X, using the trained score model; 5th and 6th cells’ rows: same as 3rd and
4th cells’ rows for another trajectory X, using Euler(100)."
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2822085889570552,"We pause for a moment to review the ﬁndings of Figure 1 (see Appendix D for additional related
plots). Firstly, we can classify the dynamics of E[Xτ|Xt, t] and of the associated weights in three
stages. In the 1st stage the weights do not move much. During the 2nd stage, roughly t ∈[0.4, 0.6],
the weights’ mass gets distributed over a limited number of samples. Interestingly, the weights’
dynamics are not monotonic. As time progresses the weights’ mass shifts between different objects
from different classes. From the beginning of the 3rd stage all mass gets allocated to a single weight,
the terminal image is decided well in advance of the terminal time. These dynamics are suboptimal.
We would like to shorten the 1st stage, but it is associated with large values of βt (i.e. quick time
passing) which are required to decouple Qτ from PD. This is an intrinsic limitation of time-reversal
approaches. It is also dubious that (partially) sampling multiple objects over t is beneﬁcial for efﬁcient
generative modeling when we make use only of the terminal sample. This issue applies to trained
models as well, as the 3rd row of Figure 1 shows. An interesting open question is how to obtain more
suitable dynamics, where perhaps class transitions happen rarely. Secondly, E[Xτ|Xt, t] provides
a denoised representation of Xt across the whole 3rd stage. An alternative to the noise removal
step applied to Xτ in Song et al. (2021) is to consider E[Xτ|Xt, t] as the sampling process instead.
Thirdly, Figure 1 makes it clear that lowering the number of discretization steps affects generative"
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2852760736196319,Under review as a conference paper at ICLR 2022
UNIFIED VIEW OF DRIFT ADJUSTMENTS,0.2883435582822086,"sampling in multiple ways. On the one hand the terminal sample Xτ is more noisy. This is not very
surprising: close to τ the drift adjustment is approximately (x(∗) −Xt)v−1
r , which is the drift of
a Brownian bridge. Bridge sampling is notoriously problematic (Bladt et al., 2016). On the other
hand larger discretization errors also signiﬁcantly affect the dynamics of E[Xτ|Xt, t] resulting in
less coherent samples. We remark that none of these insights could have been gained by observing
Xt alone, i.e. the even cells’ rows of Figure 1. To conclude, (20) and (21) give an additional meaning
to “denoising”. Neural network approximators need to map from a noisy input Xt to an adjustment
toward a smoother superimposition of samples. The desire to minimize the discrepancy between the
smoothness properties of Xt and that of E[Xτ|Xt, t] motivates the developments of Section 8."
TRANSPORTS APPROXIMATION,0.29141104294478526,"6
TRANSPORTS APPROXIMATION"
TRANSPORTS APPROXIMATION,0.294478527607362,"As in Song et al. (2021), computing the multiplicative drift adjustment A(xt, t) requires O(N)
operations. In this Section we introduce three training objectives for which unbiased and scalable, i.e.
O(1) with respect to N, MC estimators can be immediately derived."
TRANSPORTS APPROXIMATION,0.29754601226993865,"The ﬁrst training objective applies only to A(xt, t, x0). It relies on the identity (Appendix A)"
TRANSPORTS APPROXIMATION,0.3006134969325153,"A(xt, t, x0) = ∇xt ln πt|0(xt|x0) −∇xt ln pt|0(xt|x0).
(23)"
TRANSPORTS APPROXIMATION,0.30368098159509205,"It is advantageous to consider the right-hand side of (23) because from (8) we know that πt|0(xt|x0)
has mixture representation. As in Song et al. (2021), we can rely on Vincent (2011) to obtain a
scalable objective to train a neural network approximator sφ(xt, t) ≈∇xt ln πt|0(xt|x0), i.e."
TRANSPORTS APPROXIMATION,0.3067484662576687,"LFD,DBMT(φ) =
E
t∼U(0,τ),Xt∼Πt|0"
TRANSPORTS APPROXIMATION,0.3098159509202454,"h
Jt
 ∇Xt ln πt|0(Xt|x0) −sφ(Xt, t)
2i"
TRANSPORTS APPROXIMATION,0.3128834355828221,"=
E
t∼U(0,τ),(Xt,Xτ )∼Πt,τ|0"
TRANSPORTS APPROXIMATION,0.3159509202453988,"h
Jt
 ∇Xt ln pt|0,τ(Xt|x0, Xτ) −sφ(Xt, t)
2i
,
(24)"
TRANSPORTS APPROXIMATION,0.31901840490797545,"where Jt = E[∥∇Xt ln pt|0,τ(Xt|x0, Xτ)∥2]
−1 is a regularization term."
TRANSPORTS APPROXIMATION,0.3220858895705521,"The remaining training objectives rely on the identities (20) and (21). The goal is directly approximate
the expectations of (20) and (21) which, as in Section 5, we denote with a generic E[Xτ|x, t]. That is,
we aim to train a neural network approximator sφ(x, t) ≈E[Xτ|x, t]. As conditional expectations
are mean squared error minimizers, suitable objectives for the expectation terms of (20) and (21) are"
TRANSPORTS APPROXIMATION,0.32515337423312884,"LCE,DBMT(φ) =
E
t∼U[0,τ),(Xt,Xτ )∼Πt,τ"
TRANSPORTS APPROXIMATION,0.3282208588957055,"hXτ −sφ(Xt, t)
2i
,
(25)"
TRANSPORTS APPROXIMATION,0.3312883435582822,"LCE,DTRT(φ) =
E
r∼U[0,τ),(Y0,Yr)∼Q0,r"
TRANSPORTS APPROXIMATION,0.3343558282208589,"hY0 −sφ(Yr, r)
2i
.
(26)"
TRANSPORTS APPROXIMATION,0.3374233128834356,"In Table 1 we summarize the operations needed to implement the plain MC estimators for the four
objectives considered in this work. We reference where to ﬁnd the required quantities for SDEs
(11) and (12). The MC estimators for the Fisher divergence losses LFD,∗involve multiplications
by Γ−1 (by (15) and (17)). Moreover, computing the drift adjustment at generation time requires
multiplications by Γ. In Section 8 we discuss how to manage the computational burden. An appealing
property of LCE,∗is that computing the drift adjustment only requires the application of simple
scalar functions (see (20) and (21)), and that their MC estimators only requires sampling operations.
A further advantage of LCE,∗is that no regularization is required. In contrast, in the absence of
regularization terms, LFD,∗are divergent for t ≈τ due to the term v−1
r
(Section 5)."
TRANSPORTS APPROXIMATION,0.34049079754601225,"L
Sampling (r∼U(0,τ],t∼U[0,τ))
Evaluation"
TRANSPORTS APPROXIMATION,0.34355828220858897,"LFD,DTRT
Y0∼PD, Yr∼Qr|0(dyr|Y0)(13)
∇Yr ln qr|0(Yr|Y0)(15)
LFD,DBMT
Xτ ∼PD, (X0=x0), Xt∼Pt|0,τ (dxt|x0,Xτ )(16)
∇Xt ln pt|0,τ (Xt|X0,Xτ )(17)
LCE,DTRT
Y0∼PD, Yr∼Qr|0(dyr|Y0)(13)
LCE,DBMT
Xτ ∼PD, X0∼Π0|τ (dx0|Xτ ), Xt∼Pt|0,τ (dxt|X0,Xτ )(16)"
TRANSPORTS APPROXIMATION,0.34662576687116564,Table 1: Sampling and evaluation operations required to implement the proposed MC estimators.
TRANSPORTS APPROXIMATION,0.3496932515337423,Under review as a conference paper at ICLR 2022
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.35276073619631904,"7
DBMT OVERVIEW AND NUMERICAL EXPERIMENT"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.3558282208588957,"In this section we ﬁnalize the DBMT construction, putting together the results of Sections 3, 4 and 6.
The unconstrained SDE follows (11) or (12). It remains to choose the mixing distribution Π0,τ. The
marginal Πτ needs to match PD, but there is ﬂexibility in the choice of Π0|τ. Song et al. (2021)
derived a random ordinary differential equation (RODE) matching the marginal distribution of a
generative SDE, leading to faster sampling and to likelihood evaluation. RODE-matching requires
Π0 to have density. A natural implementation is given by the factorial distribution Π0,τ = PZ ⊗PD
with PZ = ND(0, Γ) and the unconstrained SDE following (12) with αt = 1/2 which preserves
PZ. If instead the DBMT starts from a ﬁxed value x0, i.e. Π0,τ = δx0 ⊗PD, we can choose
x0 = 1/N PN
n=1 x(n)a(0, τ)−1 to remove the drift adjustment at t = 0 (see (20)) and reduce the
work required to transport x0 to PD. Finally, the use of non-factorial distributions can lead to a more
efﬁcient implementation, by linking the initial distribution to PD."
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.3588957055214724,"The training steps for the simplest objective (25) of Section 6 are reported in Algorithm 1. Batch
size is assumed to be 1 to ease the description. It is also assumed that Π0,τ is factorial, otherwise
the obvious modiﬁcation applies to line 2 (Algorithm 2 is unaffected) where the endpoints are
sampled. At line 3 a random central time and the corresponding state are sampled. The function
optimizationstep implements a step of stochastic gradient descent update based on the loss L.
The corresponding sampling algorithm is reported in Algorithm 2 where the Euler(T) discretization is
assumed in line 6. Pt|0,τ(dxt|X0, Xτ), a(t, τ), v(t, τ), βt are deﬁned in Section 4. Section 8 shows
how to sample efﬁciently from Pt|0,τ(dxt|X0, Xτ) and ND(0, Γ) in computer vision applications."
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.3619631901840491,"Algorithm 1 DBMT training (LCE,DBMT)
Input: PD, PZ, SDE (11) or (12), NN sφ(x, t)
Output: trained sφ(x, t)"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.36503067484662577,"1: repeat
2:
Xτ ∼PD, X0 ∼PZ
3:
t ∼U[0, τ), Xt ∼Pt|0,τ(dxt|X0, Xτ)"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.36809815950920244,"4:
L ←
Xτ −sφ(Xt, t)
2"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.37116564417177916,"5:
φ ←optimizationstep(φ, L)
6: until convergence"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.37423312883435583,"Algorithm 2 DBMT sampling (LCE,DBMT)
Input: PZ, SDE (11) or (12), trained sφ(x, t)
Output: Discretized path X0:T"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.3773006134969325,"1: X0 ∼PZ
2: for s = 1, . . . , T do
3:
t ←(s −1) τ"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.3803680981595092,"T , x ←Xs−1"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.3834355828220859,"4:
us ←βt

1
a(t,τ)sφ(x, t) −x

a2(t,τ)"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.38650306748466257,"v(t,τ)
5:
Es ∼ND(0, Γ)
6:
Xs ←x+(f(x, t)+us) τ"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.3895705521472393,"T +g(x, t)p τ"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.39263803680981596,"T Es
7: end for"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.39570552147239263,"We consider a toy numerical example with PZ = PD = 1/3(δ−2 + δ0 + δ2), D = τ = 1. The
unconstrained SDE follows the standard Brownian motion. We consider two mixing distributions:
independent mixing Π⊥⊥
0,1 where X0 and X1 are independent and fully dependent mixing Π=
0,1 where
X0 = X1. The results are reported in Figure 2. For both couplings the correct terminal distribution
PD is recovered, as can be seen by taking the row-wise sum of the transition matrices. The initial-
terminal distribution of X solving (8), which realizes the DBMT, is different from the corresponding
mixing distribution Π0,1 which is realized by the mixture process M. Π⊥⊥
0,1 results in a transition
matrix of equal entries 1/9, Π⊥⊥
0,1 results in a diagonal transition matrix of equal diagonal entries 1/3."
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.3987730061349693,"0.0
0.2
0.4
0.6
0.8
1.0
3 2 1 0 1 2 3"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.401840490797546,"0.0
0.2
0.4
0.6
0.8
1.0
3 2 1 0 1 2 3"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.4049079754601227,"-2.0
0.0
2.0
x0"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.40797546012269936,"-2.0
0.0
2.0
x1"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.4110429447852761,"0.24
0.07
0.01"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.41411042944785276,"0.08
0.19
0.08"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.4171779141104294,"0.01
0.08
0.25"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.42024539877300615,"-2.0
0.0
2.0
x0"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.4233128834355828,"-2.0
0.0
2.0
x1"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.4263803680981595,"0.30
0.03
0.00"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.4294478527607362,"0.03
0.28
0.03"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.4325153374233129,"0.00
0.03
0.30"
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.43558282208588955,"Figure 2: (1st (Π⊥⊥
0,1), 2nd (Π=
0,1) plots): marginal density of the diffusion mixture M in yellow, which
matches the marginal density of X solving (8), 5 sample paths of X started at 0 in black; (3rd (Π⊥⊥
0,1),
4th (Π=
0,1) plots): transition matrix of X from t = 0 to t = 1 estimated from 2000 samples."
DBMT OVERVIEW AND NUMERICAL EXPERIMENT,0.4386503067484663,Under review as a conference paper at ICLR 2022
NON-DENOISING DIFFUSIONS,0.44171779141104295,"8
NON-DENOISING DIFFUSIONS"
NON-DENOISING DIFFUSIONS,0.4447852760736196,"In computer vision applications, images of resolution H×W corresponds to D = 3HW. The use of
an arbitrary covariance matrix Γ in (11) and (12) requires its Cholesky (or equivalent) decomposition
with cost O(D3). As the resolution increases the computational burden gets intractable very quickly.
Indeed, to the best of the authors’ knowledge, all prior DDPM literature only considers independent
transitions, that is Γ = I. We suggest to view SDEs (11) and (12) as corresponding to the space
discretization on an H×W grid of a spatio-temporal process deﬁned over the spatial domain [0, 1]2.
Consider the Euler discretization of (12): Xt+∆t = Xt+αtβtXt∆t+
√"
NON-DENOISING DIFFUSIONS,0.44785276073619634,"∆tEt, where Et ∼ND(0, Γ),
the idea is to adopt a functional perspective: X(t+∆t, s) = X(t, s)+αtβtX(t, s)∆t+
√"
NON-DENOISING DIFFUSIONS,0.450920245398773,"∆tE(t, s)
where s ∈[0, 1]2 deﬁnes space coordinates. That is, both X(t) and E(t) at each time t are random
processes over [0, 1]2. We assume the innovations E(t) to be a Gaussian process (GP) for each t."
NON-DENOISING DIFFUSIONS,0.4539877300613497,"As the GPs E(t) are deﬁned on a 2D domain we can leverage on scalable inference techniques from
spatial statistics. As an example, we consider the circulant embedding method (CEM) (Wood &
Chan, 1994; Dietrich & Newsam, 1997) which exploits a connection with the fast Fourier transform
(FFT). See Appendix C for a cursory review of the CEM. Consider an H×W uniform grid S of
size S = HW discretizing [0, 1]2, i.e. the support of images. For a stationary covariance function
the CEM samples E(t) on S with cost O(D ln(D)). This is close to the O(D) cost of sampling
from a pure white-noise process, and compares very favorably to the O(D3) cost of a Cholesky
decomposition. One limitation of CEM is that generated samples, while always Gaussian, might not
have the correct covariances. Whether this happens, and in that case the quality of the approximation,
depends on the covariance function. In Appendix C we select and ﬁt an isotropic GP to the microscale
properties of D(CIFAR). For this estimated GP sampling is exact. Figure 3 shows samples from a
pure white-noise GP, i.e. Γ = I, (1st row) and from the ﬁtted GP using CEM (2nd row). As noted in
Section 6, sampling is enough to implement the MC estimators for LCE,∗, but the MC estimators and
drift adjustments for LCE,∗involve additional matrix multiplications by Γ and Γ−1. The CEM allows
to compute these at the same O(D ln(D)) cost if we deﬁne the GP E(t) on a 2D torus (Rue & Held,
2005, Chapter 2.1). This corresponds to introducing dependencies between “opposing” boundaries of
[0, 1]2. Figure 3 (3rd row) shows some samples, in the highlighted patch the opposing-boundaries
dependency is evident. Either way, all samples from the 2nd and 3rd rows of Figure 3 match the
smoothness properties of D(CIFAR)."
NON-DENOISING DIFFUSIONS,0.4570552147239264,"Figure 3: Spatial GP samples, see the main text for the description."
CONCLUSIONS,0.4601226993865031,"9
CONCLUSIONS"
CONCLUSIONS,0.46319018404907975,"The DBMT construction of Section 3 is exact. The SDE class of Section 4 is tractable as it results in
linear diffusion bridges. The time-space factorization of the diffusion coefﬁcient g(x, t) = √βtΓ1/2
separates modeling concerns: βt corresponds to a time-wrapping, Γ can be efﬁciently modeled
by ﬁtting the microscale properties of PD. Availability of GPU-accelerated FFT implementations
motivates our focus on the CEM. Alternative scalable approaches abound, from Gaussian Markov
Random Fields (Rue & Tjelmeland, 2002; Rue, 2001) to Karhunen–Loève expansions (Betz et al.,
2014). It remains to apply the results of this work to perform an empirical benchmarking. Section 6
develops three novel training objectives, two of which with desirable properties compared to the
objective of Song et al. (2021), especially for non-factorial transitions. We remark the simplicity of the
proposed DBMT approach (Algorithms 1 and 2) compared to alternatives grounded in the Schrödinger
bridge problem (De Bortoli et al., 2021; Wang et al., 2021; Vargas et al., 2021). The understanding
of the target mappings ((20) and (21)) can guide the development of neural networks more closely
matching the target structure compared to the U-Net default choice. https://github.com/?
links to the code accompanying this paper which is made available under the MIT license."
CONCLUSIONS,0.4662576687116564,Under review as a conference paper at ICLR 2022
REFERENCES,0.46932515337423314,REFERENCES
REFERENCES,0.4723926380368098,"Brian D.O. Anderson. Reverse-Time Diffusion Equation Models. Stochastic Processes and their
Applications, 12(3):313–326, May 1982."
REFERENCES,0.4754601226993865,"Wolfgang Betz, Iason Papaioannou, and Daniel Straub. Numerical Methods for the Discretization
of Random Fields by Means of the Karhunen–Loève Expansion. Computer Methods in Applied
Mechanics and Engineering, 271:109–129, April 2014."
REFERENCES,0.4785276073619632,"Mogens Bladt, Samuel Finch, and Michael Sørensen. Simulation of Multivariate Diffusion Bridges.
Journal of the Royal Statistical Society. Series B (Statistical Methodology), 78(2):343–369, 2016."
REFERENCES,0.4815950920245399,"Damiano Brigo. The General Mixture-Diffusion SDE and Its Relationship with an Uncertain-Volatility
Option Model with Volatility-Asset Decorrelation, December 2002."
REFERENCES,0.48466257668711654,"Noel Cressie. Statistics for Spatial Data. John Wiley & Sons, 1993."
REFERENCES,0.48773006134969327,"Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger
Bridge with Applications to Score-Based Generative Modeling, June 2021."
REFERENCES,0.49079754601226994,"C. R. Dietrich and G. N. Newsam. Fast and Exact Simulation of Stationary Gaussian Processes
through Circulant Embedding of the Covariance Matrix. SIAM Journal on Scientiﬁc Computing,
18(4):1088–1107, July 1997."
REFERENCES,0.4938650306748466,"U. G. Haussmann and E. Pardoux. Time Reversal of Diffusions. The Annals of Probability, 14(4):
1188–1205, October 1986."
REFERENCES,0.49693251533742333,"Jonathan Ho, Ajay Jain, and Pieter Abbeel.
Denoising Diffusion Probabilistic Models.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 6840–6851, 2020."
REFERENCES,0.5,"Ioannis Karatzas and Steven E. Shreve. Brownian Motion and Stochastic Calculus. Number 113
in Graduate Texts in Mathematics. Springer, New York, 2nd ed edition, 1996. ISBN 978-0-387-
97655-6 978-3-540-97655-4."
REFERENCES,0.5030674846625767,"Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations.
Springer Berlin Heidelberg, Berlin, Heidelberg, 1992. ISBN 978-3-642-08107-1 978-3-662-12616-
5."
REFERENCES,0.5061349693251533,"NV Krylov. Introduction to the Theory of Diffusion Processes, volume 142. Providence, 1995."
REFERENCES,0.50920245398773,"Annie Millet, David Nualart, and Marta Sanz. Integration by Parts and Time Reversal for Diffusion
Processes. The Annals of Probability, pp. 208–238, 1989."
REFERENCES,0.5122699386503068,"L Chris G Rogers and David Williams. Diffusions, Markov Processes and Martingales: Volume 2:
Itô Calculus, volume 2. Cambridge university press, 2000."
REFERENCES,0.5153374233128835,"Havard Rue. Fast Sampling of Gaussian Markov Random Fields. Journal of the Royal Statistical
Society. Series B (Statistical Methodology), 63(2):325–338, 2001."
REFERENCES,0.5184049079754601,"Håvard Rue and Leonhard Held. Gaussian Markov Random Fields: Theory and Applications.
Number 104 in Monographs on Statistics and Applied Probability. Chapman & Hall/CRC, Boca
Raton, 2005. ISBN 978-1-58488-432-3."
REFERENCES,0.5214723926380368,"Hååvard Rue and Hååkon Tjelmeland. Fitting Gaussian Markov Random Fields to Gaussian Fields.
Scandinavian Journal of Statistics, 29(1):31–49, 2002."
REFERENCES,0.5245398773006135,"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised
Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
2256–2265. PMLR, 2015."
REFERENCES,0.5276073619631901,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In
International Conference on Learning Representations, 2021."
REFERENCES,0.5306748466257669,Under review as a conference paper at ICLR 2022
REFERENCES,0.5337423312883436,"Simo Särkkä and Arno Solin. Applied Stochastic Differential Equations. Cambridge University Press,
ﬁrst edition, April 2019. ISBN 978-1-108-18673-5 978-1-316-51008-7 978-1-316-64946-6."
REFERENCES,0.5368098159509203,"Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving Schrödinger
Bridges via Maximum Likelihood. Entropy, 23(9):1134, September 2021."
REFERENCES,0.5398773006134969,"Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural
Computation, 23(7):1661–1674, July 2011."
REFERENCES,0.5429447852760736,"Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep Generative Learning via
Schrödinger Bridge, June 2021."
REFERENCES,0.5460122699386503,"Andrew T. A. Wood and Grace Chan. Simulation of Stationary Gaussian Processes in [0,1]d. Journal
of Computational and Graphical Statistics, 3(4):409–432, 1994."
REFERENCES,0.549079754601227,"B. K. Øksendal. Stochastic Differential Equations: An Introduction with Applications. Universitext.
Springer, Berlin ; New York, 6th ed. edition, 2003. ISBN 978-3-540-04758-2."
REFERENCES,0.5521472392638037,Under review as a conference paper at ICLR 2022
REFERENCES,0.5552147239263804,"A
THEORETICAL FRAMEWORK"
REFERENCES,0.558282208588957,"A.1
ASSUMPTIONS"
REFERENCES,0.5613496932515337,"Assumption 1 (SDE solution). A given D-dimensional SDE(f, g) with associated initial distribution
V0 and integration interval [0, τ] admits a unique strong solution on [0, τ]."
REFERENCES,0.5644171779141104,"Assumption 1 can be checked through the application of the standard existence and uniqueness
theorems for SDE solutions. Of particular relevance to our setting is the formulation of Krylov (1995,
Chapter 5, Theorem 1) that limits the monotonic requirement to, informally speaking, drifts that pull
the process toward inﬁnities."
REFERENCES,0.5674846625766872,"Assumption 2 (SDE density). A given D-dimensional SDE(f, g) with associated initial distribution
V0 and integration interval [0, τ] admits a marginal / transition density on (0, τ) with respect to the
D-dimensional Lebesgue measure that uniquely satisﬁes the Fokker-Plank / Kolmogorov-forward
partial differential equation (PDE)."
REFERENCES,0.5705521472392638,"We refer to Särkkä & Solin (2019, Chapter 5) and to Karatzas & Shreve (1996, Chapter 5.7) for
connections between SDEs and PDEs."
REFERENCES,0.5736196319018405,"All theoretical results of this work rely on simple algebraic manipulations and re-arrangements of
quantities of interest. The main complication stems from the need to justify differentiation and
integration exchanges, i.e. exchange of limits."
REFERENCES,0.5766871165644172,"Assumption 3 (exchange of limits). We assume that limits exchanges are justiﬁed in the steps marked
with (⋆) and (⋆⋆)."
REFERENCES,0.5797546012269938,"Similarly, various steps in the derivations involve considering fractional quantities with densities
appearing in the denominators."
REFERENCES,0.5828220858895705,"Assumption 4 (positivity). For a given stochastic process, all ﬁnite-dimensional densities, condi-
tional or not, are strictly positive."
REFERENCES,0.5858895705521472,"Assumption 4 is easy to verify. We resorted to the practical but somewhat unsatisfactory formulation
of Assumption 3 because in full generality it is complicated to give easy to check conditions. We just
note that when ΠT = PD, the limit exchange marked with (⋆) is always justiﬁed. So are the limits
exchanges marked with (⋆⋆) when in addition Π0 puts all the mass to a ﬁxed initial value, or (by
direct veriﬁcation) when Π0 is Gaussian for the SDE class of Section 4. Thorough this paper, both in
the main text and in the proofs that follow, it is supposed that Assumptions 1, 2 and 4 are satisﬁed by
SDEs (1), (6) and (7). This is the case for the SDE class of Section 4, i.e. (11) and (12), for any Π0
with ﬁnite variance."
REFERENCES,0.588957055214724,"Remark: For ease of exposition it is assumed thorough this paper that all diffusions take values in the
state space RD. There is no impediment in extending the presented results to the case of diffusions
taking values in a subset X ⊂RD. The obvious changes to Assumptions 1 to 4 apply, the proofs
carry over without substantial modiﬁcations. This extension could be of practical interest as images
are often represented as ﬂoating point values in [0, 1]."
REFERENCES,0.5920245398773006,"A.2
STATEMENT AND PROOF OF DIFFUSION MIXTURE REPRESENTATION THEOREM"
REFERENCES,0.5950920245398773,"Theorem 2 (Diffusion mixture representation). Consider the family of D-dimensional SDEs on
t ∈[0, τ] indexed by λ ∈Λ"
REFERENCES,0.598159509202454,"dXλ
t = µλ(Xλ
t , t)dt + σλ(Xλ
t , t)dW λ
t ,"
REFERENCES,0.6012269938650306,"Xλ
0 ∼Vλ
0 ,
(27)"
REFERENCES,0.6042944785276073,"where the initial distributions Vλ
0 and the BMs W λ
t are all independent. Let νλ
t , t ∈(0, τ) denote
the marginal density of Xλ
t . For a generic mixing distribution L on Λ, deﬁne the mixture marginal
density νt for t ∈(0, τ) and the mixture initial distribution V0 by"
REFERENCES,0.6073619631901841,"νt(x) =
Z"
REFERENCES,0.6104294478527608,"Λ
νλ
t (x)L(dλ),
V0(dx) =
Z"
REFERENCES,0.6134969325153374,"Λ
Vλ
0 (dx)L(dλ).
(28)"
REFERENCES,0.6165644171779141,Under review as a conference paper at ICLR 2022
REFERENCES,0.6196319018404908,"Consider the D-dimensional SDE on t ∈[0, τ] deﬁned by"
REFERENCES,0.6226993865030674,"µ(x, t) = R"
REFERENCES,0.6257668711656442,"Λ µλ(x, t)νλ
t (x)L(dλ)
νt(x)
,"
REFERENCES,0.6288343558282209,"σ(x, t) = R"
REFERENCES,0.6319018404907976,"Λ σλ(x, t)νλ
t (x)L(dλ)
νt(x)
,"
REFERENCES,0.6349693251533742,"dXt = µ(Xt, t)dt + σ(Xt, t)dWt,
Y0 ∼V0. (29)"
REFERENCES,0.6380368098159509,"It is assumed that all diffusion processes Xλ and the diffusion process X solving (29) satisfy the
regularity assumptions Assumptions 1, 2 and 4 and that Assumption 3 holds. Then the marginal
distribution of the diffusion X is νt."
REFERENCES,0.6411042944785276,"Proof of Theorem 2. We start by establishing that the law of X is indeed given by the solution of
(29). In this proof we make use of the following notation: for f scalar-valued (f)t =
d
dtf, for a
vector-valued (a)x = PD
i=1
d
dxi a, for A matrix-valued (A)xx = PD
i,j=1
d2
dxidxj A. This notation
allows for a compact representation of PDEs reminiscent of the 1-dimensional setting. Then, for
0 < t < τ we have that"
REFERENCES,0.6441717791411042,"(ν(x, t))t =
Z"
REFERENCES,0.647239263803681,"Λ
νλ(x, t)L(dλ)
 t =
Z Λ"
REFERENCES,0.6503067484662577," 
νλ(x, t)
"
REFERENCES,0.6533742331288344,"tL(dλ)
(⋆⋆) =
Z Λ"
REFERENCES,0.656441717791411," 
µλ(x, t)νλ(x, t)
 x + 1"
REFERENCES,0.6595092024539877,"2
 
σλ(x, t)νλ(x, t)
"
REFERENCES,0.6625766871165644,"xxL(dλ) =
Z Λ"
REFERENCES,0.6656441717791411,"µλ(x, t)νλ(x, t)"
REFERENCES,0.6687116564417178,"ν(x, t)
ν(x, t)
 x
+ 1 2"
REFERENCES,0.6717791411042945,"σλ(x, t)νλ(x, t)"
REFERENCES,0.6748466257668712,"ν(x, t)
ν(x, t)
"
REFERENCES,0.6779141104294478,"xx
L(dλ) =
Z Λ"
REFERENCES,0.6809815950920245,"µλ(x, t)νλ(x, t)"
REFERENCES,0.6840490797546013,"ν(x, t)
L(dλ)ν(x, t)
 x
+ 1 2 Z Λ"
REFERENCES,0.6871165644171779,"σλ(x, t)νλ(x, t)"
REFERENCES,0.6901840490797546,"ν(x, t)
L(dλ)ν(x, t)
"
REFERENCES,0.6932515337423313,"xx
.
(⋆⋆)"
REFERENCES,0.696319018404908,"The second line is an exchange of limits, the third line is the application of the Fokker-Plank PDEs
for the collection of processes Xλ, the fourth line is a rewriting in terms of ν(y, t), the last line is
another exchange of limits. The result follows by noticing that the last line gives the Fokker-Plank
representation of (29)."
REFERENCES,0.6993865030674846,"A.3
DRIFT ADJUSTMENTS"
REFERENCES,0.7024539877300614,"Limitedly to this section, we lighten the notation by removing subscripts from probability measures
and densities. The missing time points can be inferred without ambiguity from the variables."
REFERENCES,0.7055214723926381,"A.3.1
DRIFT ADJUSTMENT IDENTITIES FOR CONSTANT INITIAL VALUE"
REFERENCES,0.7085889570552147,First identity:
REFERENCES,0.7116564417177914,"∇xt ln
Z p(xτ|xt)"
REFERENCES,0.7147239263803681,p(xτ|x0)Π(dxτ)
REFERENCES,0.7177914110429447,"=
Z ∇xtp(xτ|xt)"
REFERENCES,0.7208588957055214,"p(xτ|x0)
p(xt|x0)Π(dxτ)
 Z p(xτ|xt)"
REFERENCES,0.7239263803680982,"p(xτ|x0)p(xt|x0)Π(dxτ)
(⋆)"
REFERENCES,0.7269938650306749,"=
Z
∇xt ln p(xτ|xt)p(xτ, xt|x0)"
REFERENCES,0.7300613496932515,"p(xτ|x0)
Π(dxτ)
 Z p(xτ, xt|x0)"
REFERENCES,0.7331288343558282,"p(xτ|x0)
Π(dxτ)"
REFERENCES,0.7361963190184049,"=
Z
∇xt ln p(xτ|xt)p(xt|x0, xτ)Π(dxτ)

π(xt|x0)"
REFERENCES,0.7392638036809815,"= A(xt, t, x0)."
REFERENCES,0.7423312883435583,Under review as a conference paper at ICLR 2022
REFERENCES,0.745398773006135,Second identity:
REFERENCES,0.7484662576687117,"∇xt ln
Z p(xτ|xt)"
REFERENCES,0.7515337423312883,p(xτ|x0)Π(dxτ)
REFERENCES,0.754601226993865,"= ∇xt ln
Z p(xτ|xt)"
REFERENCES,0.7576687116564417,p(xτ|x0)p(xt|x0)Π(dxτ) −∇xt ln p(xt|x0)
REFERENCES,0.7607361963190185,= ∇xt ln π(xt|x0) −∇xt ln p(xt|x0).
REFERENCES,0.7638036809815951,"A.3.2
DRIFT ADJUSTMENTS AS EXPECTATIONS"
REFERENCES,0.7668711656441718,To establish (20) notice that from (14) we have
REFERENCES,0.7699386503067485,"G(xt, t)A(xt, t)"
REFERENCES,0.7730061349693251,"= βtΓΓ−1
Z 
xτ
a(t, τ) −xt"
REFERENCES,0.7760736196319018," a2(t, τ)"
REFERENCES,0.7791411042944786,"v(t, τ)
p(xt|x0, xτ)"
REFERENCES,0.7822085889570553,"π(xt)
Π0,τ(dx0, dxτ) = βt"
REFERENCES,0.7852760736196319,"
1
a(t, τ)"
REFERENCES,0.7883435582822086,"Z
xτ
π(xt|x0, xτ)"
REFERENCES,0.7914110429447853,"π(xt)
Π0,τ(dx0, dxτ) −xt"
REFERENCES,0.7944785276073619," a2(t, τ)"
REFERENCES,0.7975460122699386,"v(t, τ) = βt"
REFERENCES,0.8006134969325154,"
1
a(t, τ)
E
Xτ ∼Π(dxτ |xt)[Xτ] −xt"
REFERENCES,0.803680981595092," a2(t, τ)"
REFERENCES,0.8067484662576687,"v(t, τ) ."
REFERENCES,0.8098159509202454,To establish (21) note that from (15) we have
REFERENCES,0.8128834355828221,"G(yr, r) ∇yr ln q(yr) = Γ
Z
∇yr ln q(yr|y0)q(yr|y0)"
REFERENCES,0.8159509202453987,q(yr) PD(dy0)
REFERENCES,0.8190184049079755,"= βrΓΓ−1
Z a(0, r)y0 −yr"
REFERENCES,0.8220858895705522,"v(0, r)"
REFERENCES,0.8251533742331288, q(yr|y0)
REFERENCES,0.8282208588957055,q(yr) PD(dy0) = βr
REFERENCES,0.8312883435582822,"
a(0, r)
Z
y0
q(yr|y0)"
REFERENCES,0.8343558282208589,q(yr) PD(dy0) −yr
REFERENCES,0.8374233128834356,"
1
v(0, r) = βr"
REFERENCES,0.8404907975460123,"
a(0, r)
E
Y0∼Q(dx0|yr)[Y0] −yr"
REFERENCES,0.843558282208589,"
1
v(0, r)."
REFERENCES,0.8466257668711656,"B
SDES CLASS FORMULAS"
REFERENCES,0.8496932515337423,The transition densities of (9) and (10) are given respectively by
REFERENCES,0.852760736196319,"epbm,τ|t(zτ|zt) = ND (zτ; zt, Γ(τ −t)) ,"
REFERENCES,0.8558282208588958,"epou,τ|t(zτ|zt) = ND"
REFERENCES,0.8588957055214724,"
zτ; zteαt:τ (τ−t), Γ
 1"
REFERENCES,0.8619631901840491,"2αt
e2αt:τ (τ−t) −
1
2ατ 
."
REFERENCES,0.8650306748466258,"Here we used the notation f t:τ =
1
τ−t
R τ
t fudu, i.e. f t:τ is the average value of a function fu on the
interval [t, τ]. The time-homogenous case of (10), where αt:τ = α, is thus immediately recovered."
REFERENCES,0.8680981595092024,"The scalar functions abm(t, τ), aou(t, τ), vbm(t, τ) and vou(t, τ) are given by"
REFERENCES,0.8711656441717791,"abm(t, τ) = 1,
vbm(t, τ) = bτ −bt,"
REFERENCES,0.8742331288343558,"aou(t, τ) = eαbt:bτ (bτ −bt),
vou(t, τ) =
1
2αbt
e2αbt:bτ (bτ −bt) −
1
2αbτ
."
REFERENCES,0.8773006134969326,"The scalar functions vbr(0, t, τ), abr(0, t, τ) and abr(0, t, τ) are given by"
REFERENCES,0.8803680981595092,"vbr(0, t, τ) =
v(0, t)v(t, τ)
v(0, t)a2(t, τ) + v(t, τ),"
REFERENCES,0.8834355828220859,"abr(0, t, τ) =
v(t, τ)a(0, t)
v(0, t)a2(t, τ) + v(t, τ),"
REFERENCES,0.8865030674846626,"abr(0, t, τ) =
v(0, t)a(t, τ)
v(0, t)a2(t, τ) + v(t, τ)."
REFERENCES,0.8895705521472392,Under review as a conference paper at ICLR 2022
REFERENCES,0.8926380368098159,"The scalar functions βve,r and βvp,r are given by"
REFERENCES,0.8957055214723927,"βve,r = σ2
min σmax σmin"
REFERENCES,0.8987730061349694,"2r
2 log σmax"
REFERENCES,0.901840490797546,"σmin
,
βvp,r =
 ¯βmin + r
 ¯βmax −¯βmin

."
REFERENCES,0.9049079754601227,"The constants σmin, σmax, ¯βmin, ¯βmax depend in part on the dataset considered, but are consistently
chosen to have βve,r, βvp,r small for r ≈0 and large for r ≈τ."
REFERENCES,0.9079754601226994,"The approximating distributions PZ in Song et al. (2021) are Pve
Z = ND(0, Iσ2
max) for VESDE,
Pvp
Z = ND(0, I) for VPSDE."
REFERENCES,0.911042944785276,"C
ADDITIONAL MATERIAL"
REFERENCES,0.9141104294478528,"C.1
CLOSELY RELATED WORK"
REFERENCES,0.9171779141104295,"A work closely related to the present paper is that of Wang et al. (2021) as it similarly avoids the
time-reversal construction. Wang et al. (2021) construct a 2-stages diffusion process from a constant
initial value x0 to PD by relying on the theory of Schrödinger bridges. The most notable differences
with respect to the DBMT transport are: (i) the dynamics considered in Wang et al. (2021) are less
general, in our notation they correspond to f(·) = 0, g(·) = σI for a ﬁxed scalar σ; (ii) the transport
proposed in Wang et al. (2021) necessarily starts from x0, the general result of (8) allows for (almost)
arbitrary initial distributions and initial-terminal dependencies. For an initial x0, i.e. for case of
A(xt, tx0) in Section 3.2 and for the more limited dynamics considered in Wang et al. (2021), the
achieved transport is the same. In this sense the DBMT generalizes the ﬁrst stage diffusion of Wang
et al. (2021). It is interesting to note that in the case of a constant x0 the DBMT can also be obtained
by an application of Doob h-transforms as we show in the following section."
REFERENCES,0.9202453987730062,"We now review two additional works grounded in the Schrödinger bridge problem: De Bortoli et al.
(2021); Vargas et al. (2021). Both works rely on the Iterative Proportional Fitting (IPF) procedure
to solve the (dynamic) Schrödinger bridge problem. Both works leverage on time-reversal results
to carry out the alternated Schrödinger half-bridge IPF iterations. The main difference between
the two works is that De Bortoli et al. (2021) estimates the optimal SDE drifts via neural network
approximations and score-matching, while Vargas et al. (2021) relies on Gaussian Processes and
maximum likelihood ﬁtting. The work of De Bortoli et al. (2021) can be seen as an extension of Song
et al. (2021), and similarly to our work allows the use of shorter time intervals. Compared to our
proposal, it solves a harder problem but also presents additional difﬁculties. Training is more involved
as all the neural network approximations, one for each IPF iterate, need to converge. Moreover, there
is limited guidance on how to optimally choose the number of integration steps over the number of
IPF iterates."
REFERENCES,0.9233128834355828,"C.2
CONNECTION WITH DOOB H-TRANSFORMS"
REFERENCES,0.9263803680981595,The previously established identity
REFERENCES,0.9294478527607362,"A(xt, t, x0) = ∇xt ln
Z
pτ|t(xτ|xt)
pτ|0(xτ|x0)Πτ(dxτ),"
REFERENCES,0.9325153374233128,shows that the drift adjustment can be equivalently expressed as
REFERENCES,0.9355828220858896,"µ(xt, t) = f(xt, t) + G(xt, t) ∇xth(xt, t),
h(xt, t) = ln
Z
pτ|t(xτ|xt)
pτ|0(xτ|x0)Πτ(dxτ),"
REFERENCES,0.9386503067484663,"as x0 is a constant. It can be veriﬁed that the h function satisﬁes the required space-time regularity
property (Särkkä & Solin, 2019, Eq. (7.73)). As such, it is a genuine Doob h-transform. That
ph
t′|t(xt′|xt) = pt′|t(xt′|xt)h(xt′, t′)/h(xt, t) is the transition density of the DBMT transport from
δx0 to Πτ follows by direct computation."
REFERENCES,0.941717791411043,"C.3
GP MODELLING ON CIFAR10"
REFERENCES,0.9447852760736196,"For simplicity, we assume a factorial distribution over the channels and an isotopic stationary
covariance function. We rely on the semivariogram approach (Cressie, 1993) to compare how"
REFERENCES,0.9478527607361963,Under review as a conference paper at ICLR 2022
REFERENCES,0.950920245398773,"different covariance functions ﬁt D(CIFAR). A semivariogram is a measure of dependency across
space. In the case of an isotropic stationary covariance it simpliﬁes to a scalar function of the
Euclidean distance between points: γ(∥∆s∥) = E[(∆xs)2]/2 with ∆xs = xs+∆s −xs. The
rate of decrease of γ(∥∆s∥) toward 0 as ∥∆s∥→0 gives a measure of the inﬁnitesimal spatial
dependency, i.e. the smoothness of the spatial process. Semivariograms corresponding to different
covariance functions are here ﬁtted to their empirical counterparts via a weighted minimum-least-
squares procedure (Cressie, 1993). Figure 4 illustrates the exponential and RBF semivariogram
ﬁts for two images of D(CIFAR). The exponential covariance, which corresponds to rougher paths,
provides a much better ﬁt to the shown samples. This result is consistent across D(CIFAR). We
remark that Γ = I corresponds to a pure white-noise process with a perfectly ﬂat semivariogram
which would clearly result in a very poor ﬁt to the empirical semivariograms shown in Figure 4.
Based on these ﬁndings, we model the innovations of each image channel as a GP with exponential
covariance function with length-scale θ = 0.205, the median estimated value (Figure 4 (right)). We
match the marginal variance to that of D(CIFAR), σ2 = 0.063."
REFERENCES,0.9539877300613497,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 p( )"
REFERENCES,0.9570552147239264,"Figure 4: Empirical semivariograms (dots) and ﬁtted exponential (solid lines) and RBF (doted lines)
semivariograms for the 1st (left) and 2nd (center) image of D(CIFAR); histograms (bins) and medians
(lines) of the distributions of the length-scale parameters in the exponential variogram model over
D(CIFAR) (right); colors represent the RGB channels."
REFERENCES,0.9601226993865031,"C.4
CIRCULANT EMBEDDING METHOD"
REFERENCES,0.9631901840490797,"Figure 5: Circulant embedding covariance matrices, see the appendix’s main text for the description."
REFERENCES,0.9662576687116564,"We start by providing a cursory explanation leading to efﬁcient sampling in the 1D case, before
giving the intuition behind the extension to the 2D case. We refer to Wood & Chan (1994); Dietrich
& Newsam (1997) for a complete explanation and to Rue & Held (2005) for the results underlying
efﬁcient density (likelihood) computation. Let [0, 1] be the spatial domain of interest. Let S =
{si}M
i=1 be a uniform grid (regular lattice) discretizing [0, 1], where the points si are assumed to be
ordered. The ﬁrst key observation is that for a stationary covariance function ρ(·, ·) the covariance
matrix C with entries Ci,j = ρ(sj, sj) is symmetric and Toeplitz, i.e. with constant-diagonals. See
Figure 5 (leftmost) for an example where M = 9. A property of symmetric Toeplitz matrices is
that they can always be embedded in larger symmetric circulant matrices. A circulant matrix of size
M ′×M ′ is deﬁned by the property that all its rows (and columns) are obtained by cycling through
the same M ′-dimensional vector. Circulant matrices correspond to covariance matrices of GPs
deﬁned on a (here 1D) torus (Rue & Held, 2005, Chapter 2.1). The circulant embedding matrix just
introduced corresponds to an artiﬁcial enlargement of the spatial domain [0, 1] to a larger interval
leading to a torus. See Figure 5 (2nd from left) for a circulant embedding of C. The second key
observation is that a circulant matrix is diagonalized by the 1D FFT matrix. Having obtained the
eigenvalues of C, efﬁcient sampling on the enlarged domain is achieved by the 1D FFT applied to"
REFERENCES,0.9693251533742331,Under review as a conference paper at ICLR 2022
REFERENCES,0.9723926380368099,"complex standard random numbers multiplied by the (square root of the) eigenvalues. The real and
imaginary part of the generated samples are independent. The main issue with the CEM is that the
circulant matrix embedding might fail to be positive deﬁnite. The issue can be avoided by considering
progressively larger embeddings, see the theoretical and empirical ﬁndings of Dietrich & Newsam
(1997). Otherwise, a level of approximation can be accepted by modifying the covariance function or
by truncating the eigenvalues to be positive."
REFERENCES,0.9754601226993865,"The development of the 2D CEM follows very similar steps. The domain of interest is now [0, 1]2,
the uniform grid is S = {si,j}M
i,j=1 and the points si,j are assumed to be lexicographically ordered.
The stationarity of the covariance functions results in a symmetric block-Toeplitz covariance matrix,
as shown in Figure 5 (3rd from left). Again, symmetric block-Toeplitz matrices can be embedded
in symmetric block-circulant matrices, as exempliﬁed by Figure 5 (rightmost, zooming might be
required to see the block structure). Block-circulant matrices can be shown to be diagonalized by the
2D FFT matrix, and efﬁcient sampling follows from similar steps to the ones seen in the 1D case."
REFERENCES,0.9785276073619632,"D
ADDITIONAL FIGURES"
REFERENCES,0.9815950920245399,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9846625766871165,Figure 6: Same as Figure 1 for VESDE model.
REFERENCES,0.9877300613496932,Under review as a conference paper at ICLR 2022
REFERENCES,0.99079754601227,"Figure 7: Additional samples from the trained VPSDE model of Song et al. (2021), E[Xτ|Xt, t] and
Xt (interleaved rows) over sampling time t, Euler(1000) (top 16 rows) and Euler(100) (bottom 16
rows)."
REFERENCES,0.9938650306748467,Under review as a conference paper at ICLR 2022
REFERENCES,0.9969325153374233,"Figure 8: Additional samples from the trained VESDE model of Song et al. (2021), E[Xτ|Xt, t] and
Xt (interleaved rows) over sampling time t, Euler(1000) (top 16 rows) and Euler(100) (bottom 16
rows)."
