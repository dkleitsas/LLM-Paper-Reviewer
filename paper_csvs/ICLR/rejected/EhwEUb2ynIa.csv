Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035335689045936395,"Pre-training large-scale vision and language models (e.g. CLIP) has shown promis-
ing results in representation and transfer learning. We investigate the question of
how to efﬁciently adapt these models to downstream tasks. For image classiﬁca-
tion, linear probes have been the standard for ease of use and efﬁciency, while
for language, other approaches like prompt tuning have emerged. We analyze
several ﬁne-tuning methods across a diverse set of image classiﬁcation tasks across
two spectra investigating the amount and similarity of downstream data to that of
pretraining one. We ﬁnd that just tuning LayerNorm parameters is a surprisingly
effective baseline across the board. We further demonstrate a simple yet effective
strategy that combines LayerNorm-tuning with general ﬁne-tuning methods to
improve their performance and benchmark them on few-shot adaption and distribu-
tion shift tasks. Finally, we provide an empirical analysis and recommend general
recipes for efﬁcient transfer learning of vision and language models 1."
INTRODUCTION,0.007067137809187279,"1
INTRODUCTION"
INTRODUCTION,0.01060070671378092,"Large-scale deep network models pretrained on ultra large-scale data on the internet, whether text or
images, have shown impressive performance recently (Radford et al., 2019; 2021; Brown et al., 2020;
Devlin et al., 2018; Jia et al., 2021). Training such models with billions of parameters on a large
internet scale data is an expensive and time consuming process often costing over millions of dollars.
Hence, replicating such models is not only difﬁcult but also undesirable for every downstream task.
Fortunately, the information gathered by these large-scale models using raw internet data seems to
transfer well to several downstream tasks with little to no ﬁnetuning at all using natural language as a
way for zero-shot evaluation (Brown et al., 2020; Radford et al., 2021)."
INTRODUCTION,0.014134275618374558,"While zero-shot transfer performs well, it is generally better to adapt the model itself if there are any
labeled examples available for the downstream task. Traditionally, the go-to strategy in the computer
vision community has either been to ﬁnetune the whole network or an additional MLP layer at the
end. With the use of raw language, adaptation techniques such as prompt tuning have surfaced (Li
& Liang, 2021; Lester et al., 2021). Alternative methods include new parameters in between the
network instead of adding a layer at the end (Houlsby et al., 2019; Mahabadi et al., 2021). However,
it remains unclear as to which approach is preferred under which scenarios."
INTRODUCTION,0.0176678445229682,"We ask what are the general guidelines one should adopt while ﬁnetuning a large-scale pretrained
model on downstream datasets. To scope this question, we choose CLIP (Radford et al., 2021) as the
base pretrained model and adapt it to several downstream problems. CLIP is a vision-and-language
model trained on over 400M pairs of image and text descriptions collected off the internet. There are
several reasons to choose CLIP for this study. First, CLIP is one of the few vision models trained
on ultra-large scale, unﬁltered and varied raw visual data on the internet. Second, the multi-modal
nature of CLIP enables use of more general ways of adaptation like using natural language prompts
for “zero-shot” transfer to new categories – techniques previously popular mostly in NLP."
INTRODUCTION,0.02120141342756184,"We ﬁnd that merely tuning the parameters of LayerNorm (Ba et al., 2016) turns out to be a surprisingly
effective approach that is competitive or better than all other adaptation methods across the board. The
effectiveness of normalization techniques has been observed by prior work for generalization (Perez
et al., 2018; Lu et al., 2021) as well as training from scratch (Frankle et al., 2020). Inspired by this,"
INTRODUCTION,0.024734982332155476,1Website at https://sites.google.com/view/adapt-large-scale-models
INTRODUCTION,0.028268551236749116,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03180212014134275,“{class}”
INTRODUCTION,0.0353356890459364,"“cat”
“dog”
...
“sun” ..."
INTRODUCTION,0.038869257950530034,"!!
!!"" #!
. . .
!!"" #"""
INTRODUCTION,0.04240282685512368,". . .
##
#!
#$
#"""
INTRODUCTION,0.045936395759717315,"!!"" ##
!!"" #$"
INTRODUCTION,0.04946996466431095,learnable
INTRODUCTION,0.053003533568904596,"prompt +
+"
INTRODUCTION,0.05653710247349823,"Multi-headed 
Attention"
INTRODUCTION,0.06007067137809187,Layer Norm MLP
INTRODUCTION,0.0636042402826855,Layer Norm
INTRODUCTION,0.06713780918727916,"Multi-headed 
Attention"
INTRODUCTION,0.0706713780918728,Layer Norm MLP
INTRODUCTION,0.07420494699646643,Layer Norm
INTRODUCTION,0.07773851590106007,"Adapter +
+"
INTRODUCTION,0.0812720848056537,Image Encoder
INTRODUCTION,0.08480565371024736,Text Encoder
INTRODUCTION,0.08833922261484099,Linear
INTRODUCTION,0.09187279151943463,"Layer Norm Tuning
Adapter
Linear Probe
Prompt Tuning “cat”"
INTRODUCTION,0.09540636042402827,"Figure 1: Illustration of multiple methods for adapting CLIP to downstream image classiﬁcation tasks. Each
labeled approach can be used separately for ﬁne-tuning the CLIP model. We analyze a variety of ﬁne-tuning
methods such as prompt tuning by prepending a learnable prompt, tuning Layer Normalization parameters,
inserting adapter and compacter modules in-between the Transformer layers, and using a linear probe on top of
visual features. The CLIP model can be also used for inference on a downstream task in a zero-shot manner."
INTRODUCTION,0.0989399293286219,"we further look into different ways of combining LayerNorm-tuning with other adaptation methods
that ﬁnetune new parameters. We devise an effective scheme that ﬁrst ﬁnetunes the CLIP model using
only LayerNorm tuning and uses it as initialization for adapting new parameters. We evaluate our
adaptation techniques across 12 downstream tasks spread along two spectra: size of downstream task
dataset as well as the similarity of downstream data to the pretraining data. Across both spectra, we
ﬁnd that our two-stage LayerNorm tuning approach is most competitive and show its effectiveness
for general-purpose adaptation of CLIP to downstream image-classiﬁcation tasks."
INTRODUCTION,0.10247349823321555,"To summarize, our paper’s contributions are as follows:"
INTRODUCTION,0.10600706713780919,"• We show the effectiveness of LayerNorm-tuning for adaptation to downstream tasks.
• We devise a simple yet effective scheme to combine LayerNorm-tuning with other methods of"
INTRODUCTION,0.10954063604240283,"ﬁnetuning to obtain competitive performance across the board.
• We show a thorough comparison of different adaptation methods in four scenarios across two"
INTRODUCTION,0.11307420494699646,"spectra (amount of downstream data and its similarity to pretraining data) studied on numerous
downstream classiﬁcation tasks."
INTRODUCTION,0.1166077738515901,"We believe our ﬁndings will encourage more research and put existing research in perspective of
what works best while ﬁnetuning large-scale vision-language models to downstream tasks."
INTRODUCTION,0.12014134275618374,"2
BACKGROUND: VISION-AND-LANGUAGE PRETRAINED MODELS"
INTRODUCTION,0.12367491166077739,"Vision-and-language pre-training methods have recently shown promise on diverse tasks across
images and text (Radford et al., 2021; Jia et al., 2021). While many such approaches have emerged,
we focus on CLIP (Contrastive Language-Image Pre-training), a large-scale model with strong
zero-shot performance on downstream classiﬁcation tasks (Radford et al., 2021)."
INTRODUCTION,0.127208480565371,"Contrastive Language-Image Pre-training (CLIP)
CLIP consists of two parallel encoders for
processing images and text, whose outputs are then projected into a shared embedding space. The text
encoder is a Transformer (Vaswani et al., 2017) following the architecture described in Radford et al.
(2019), while the image encoder is a Vision Transformer (ViT) with a patch size of 16 (Dosovitskiy
et al., 2020). For our experiments, we utilize the open-sourced pretrained CLIP models."
INTRODUCTION,0.13074204946996468,"Training
Because the image and text features live in the same embedding space, the cosine
similarity between any embedded image and text description can be computed. CLIP uses these as
prediction probabilities for classifying an image with the correct text caption (or vice versa) across
batches. Formally, denote I and T as the set of image and text features in a single batch. The
prediction probability for the ith image and jth caption in the batch is given by"
INTRODUCTION,0.13427561837455831,"p(Tj | Ii) =
exp (cos(Tj, Ii)/⌧)
exp (cos(Tj, Ii)/⌧) + P"
INTRODUCTION,0.13780918727915195,"k6=j exp (cos(Tk, Ii)/⌧)"
INTRODUCTION,0.1413427561837456,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.14487632508833923,"Layer Norm Tuning
Prompt Tuning + MLP MLP +"
INTRODUCTION,0.14840989399293286,Layer Norm
INTRODUCTION,0.1519434628975265,Multi-headed
INTRODUCTION,0.15547703180212014,Attention
INTRODUCTION,0.15901060070671377,Layer Norm + MLP +
INTRODUCTION,0.1625441696113074,Layer Norm
INTRODUCTION,0.16607773851590105,Multi-headed
INTRODUCTION,0.1696113074204947,Attention
INTRODUCTION,0.17314487632508835,Layer Norm + MLP +
INTRODUCTION,0.17667844522968199,Layer Norm
INTRODUCTION,0.18021201413427562,Multi-headed
INTRODUCTION,0.18374558303886926,Attention
INTRODUCTION,0.1872791519434629,Layer Norm
INTRODUCTION,0.19081272084805653,"Adapter
Linear Probe"
INTRODUCTION,0.19434628975265017,"Text Encoder
Text Encoder
Image Encoder
Text Encoder"
INTRODUCTION,0.1978798586572438,"&
Image Encoder"
INTRODUCTION,0.20141342756183744,"“{class}”
learnable"
INTRODUCTION,0.2049469964664311,prompt
INTRODUCTION,0.20848056537102475,Multi-headed
INTRODUCTION,0.21201413427561838,Attention
INTRODUCTION,0.21554770318021202,Layer Norm MLP
INTRODUCTION,0.21908127208480566,Layer Norm + +
INTRODUCTION,0.2226148409893993,Non-linearity +
INTRODUCTION,0.22614840989399293,Down-projection
INTRODUCTION,0.22968197879858657,Up-projection
INTRODUCTION,0.2332155477031802,"Figure 2: Parameter count and architectures of ﬁne-tuning methods. All of the ﬁne-tuning methods we consider
tune only a small fraction of the total number of parameters and act in different ways on the model. LayerNorm-
tuning only trains existing Layer Normalization parameters across all Transformer layers. The remaining
approaches inject additional parameters which act on different parts of the model: the input, intermediate
activations, and output. Prompt tuning prepends a learnable prompt to the input text embeddings of classes.
Adapter modules are composed of a linear down-projection, non-linearity, and linear up-projection, and are
inserted inside the Transformer layers of the text encoder after the attention block. Linear probe directly classiﬁes
classes from the output of the image encoder."
INTRODUCTION,0.23674911660777384,"where ⌧is a learnable temperature parameter. CLIP is trained with a contrastive loss accordingly
across 400 million pairs of image and text captions collected online (Radford et al., 2021)."
INTRODUCTION,0.24028268551236748,"Inference
For a downstream classiﬁcation task at test time, CLIP ﬁrst embeds the textual descrip-
tions of all classes. These descriptions may range from a phrase like “a photo of a <class>” to heavily
engineered embeddings ensembled over 80 different templates (Radford et al., 2021). Each image
is then classiﬁed using the embedded classes as labels and the prediction probabilities described
above. Notably, this inference scheme allows CLIP to be transferred zero-shot to any downstream
image classiﬁcation task. Radford et al. (2021) show that zero-shot CLIP is competitive with a fully
supervised ResNet (He et al., 2016) baseline on a suite of image classiﬁcation tasks."
INTRODUCTION,0.24381625441696114,"3
METHODOLOGY: FINE-TUNING LARGE-SCALE PRETRAINED MODEL"
INTRODUCTION,0.24734982332155478,"Although zero-shot CLIP performs well on natural images and general object classiﬁcation datasets,
its performance degrades quickly on more abstract tasks from out-of-distribution data. Even on a
simple dataset like MNIST (LeCun, 1998), the zero-shot CLIP model (ViT-B/16) we test attains an
accuracy of only 55%. Substantial gains can be achieved by ﬁne-tuning the pre-trained model, but
many such strategies have emerged across tasks in vision and language and it’s unclear which to
use on diverse downstream settings. For this reason, we provide an extensive study of adaptation
approaches. Figure 1 illustrates the ﬁne-tuning methods we consider in context of CLIP while
Figure 2 shows more detailed information regarding each approach."
INTRODUCTION,0.2508833922261484,"We propose a general taxonomy of ﬁne-tuning approaches and consider three major classes: (a)
methods which only ﬁne-tune existing parameters, (b) methods which freeze existing parameters and
add new parameters, and (c) methods which combine (a) and (b). We ﬁrst consider two methods in
(a) which only ﬁne-tune existing parameters."
FINE-TUNING EXISTING PARAMETERS,0.254416961130742,"3.1
FINE-TUNING EXISTING PARAMETERS"
FINE-TUNING EXISTING PARAMETERS,0.2579505300353357,"Full Model Fine-tuning
The simplest approach to ﬁne-tuning is to train all of the model parameters
on the downstream task. However, this is unstable and doesn’t scale well to CLIP-size models with
hundreds of millions of parameters. Our empirical results show this behavior as well."
FINE-TUNING EXISTING PARAMETERS,0.26148409893992935,Under review as a conference paper at ICLR 2022
FINE-TUNING EXISTING PARAMETERS,0.26501766784452296,"LayerNorm Tuning
Instead of full model ﬁne-tuning for large-scale models, we can tune a small
subset of chosen parameters when the downstream data is scarce. In fact, Frankle et al. (2020)
show that just tuning Batch Normalization (Ioffe & Szegedy, 2015) parameters from a random
initialization can be highly expressive. In a similar vein, we investigate tuning the parameters of
Layer Normalization (LayerNorm) layers (Ba et al., 2016). Unlike Batch Normalization, LayerNorm
applies per-element normalization across mini-batches. Given a mini batch of inputs x, LayerNorm
transforms this as"
FINE-TUNING EXISTING PARAMETERS,0.26855123674911663,"y =
x −E[x]
p"
FINE-TUNING EXISTING PARAMETERS,0.27208480565371024,Var[x] + ✏
FINE-TUNING EXISTING PARAMETERS,0.2756183745583039,· γ + β
FINE-TUNING EXISTING PARAMETERS,0.2791519434628975,"where the mean and variance are calculated over the normalized dimensions and γ, β are learned
parameters. Because the image and text encoders in CLIP share the same underlying Transformer
architecture, in LayerNorm Tuning, we ﬁne-tune the Layer Normalization parameters γ, β across all
layers of both encoders. These parameters are 768-dimensional and 512-dimensional for the image
and text encoders respectively."
FINE-TUNING NEW PARAMETERS,0.2826855123674912,"3.2
FINE-TUNING NEW PARAMETERS"
FINE-TUNING NEW PARAMETERS,0.2862190812720848,"An alternative paradigm is to inject new parameters which can more effectively adapt to downstream
tasks. These new parameters can act at various stages of a pre-trained model: on the output, input, or
intermediate activations."
FINE-TUNING NEW PARAMETERS,0.28975265017667845,"Linear Probe
The classic method of training a linear probe on top of frozen features is an example
of adding new parameters which act on the model output. Given a pre-trained CLIP model, we discard
the text encoder, freeze the image encoder, and learn a linear layer on top of the image features
before they’re projected to the shared embedding space. The linear layer maps the penultimate image
features to logits from which class predictions are made. While this simple method is popular and
effective, it’s parameter-inefﬁcient for tasks with higher number of classes and fails to leverage any
of the language information contained in CLIP."
FINE-TUNING NEW PARAMETERS,0.29328621908127206,"Prompt Tuning
Alternatively, we can consider adding parameters which act on the model input.
Such an approach known as prompt tuning has emerged as a parameter-efﬁcient ﬁne-tuning method in
language (Li & Liang, 2021; Lester et al., 2021). A ﬁxed number of continuous vectors (a “prompt”)
is prepended to the model input and optimized throughout training. Similar to concurrent work by
Zhou et al. (2021), we apply prompt tuning to image classiﬁcation with CLIP. For the model input,
we embed the raw text of the classes without a template and prepend a continuous prompt of ﬁxed
length. During training, the prompt is learned using a cross-entropy loss according to the prediction
probabilities detailed in Section 3.1. Although prompt tuning can be applied in the same way for
transformer-based visual encoders, we ﬁnd that only applying it for the text encoder produces better
and more stable results."
FINE-TUNING NEW PARAMETERS,0.2968197879858657,"Prompt tuning is parameter-efﬁcient and removes the need for manual prompt engineering e.g.
specifying “a photo of a <class>, a type of ﬂower” for a downstream task on ﬂower classiﬁcation.
Ideally, the learned prompts would contain such domain-speciﬁc information. However, prompt
tuning suffers from high variance during training and is sensitive to initialization."
FINE-TUNING NEW PARAMETERS,0.3003533568904594,"Adapter and Compacter Networks
The above two approaches inject parameters which act either
at the end of the network (linear probe) or at the beginning (prompt tuning). A third option is to
inject new parameters for the downstream task within the layers of the network itself. This idea has
been popularized as an efﬁcient transfer learning method in language (Houlsby et al., 2019). For
Transformer-based architectures, a common strategy is to insert a block of learnable parameters after
feed forward layers or the attention mechanism."
FINE-TUNING NEW PARAMETERS,0.303886925795053,"Adapter networks insert learnable adapter blocks after the feed forward layers in each Transformer
layer (Houlsby et al., 2019). Each block follows a bottleneck architecture and is composed of a
linear down-projection, non-linearity, and linear up-projection as shown in Figure 2. However, for
architectures with many stacked Transformer layers and larger hidden dimensions, adapter modules
are parameter-inefﬁcient."
FINE-TUNING NEW PARAMETERS,0.30742049469964666,"To alleviate this issue, Mahabadi et al. (2021) introduce compacter modules which follow the same
architecture but use low-rank parameters and hypercomplex multiplication to improve parameter
efﬁciency. Speciﬁcally, if the down-projection layer maps x 2 Rm ! W x + b 2 Rd where"
FINE-TUNING NEW PARAMETERS,0.31095406360424027,Under review as a conference paper at ICLR 2022
FINE-TUNING NEW PARAMETERS,0.31448763250883394,Amount of Training Data →
FINE-TUNING NEW PARAMETERS,0.31802120141342755,CLIP Zero-Shot Performance → 21.98 76.01 83.74
FINE-TUNING NEW PARAMETERS,0.3215547703180212,"73.08
77.05
79.03
82.83 20 30 40 50 60 70 80 90"
FINE-TUNING NEW PARAMETERS,0.3250883392226148,Low Data & High Similarity 21.76 78.11
FINE-TUNING NEW PARAMETERS,0.3286219081272085,"90.63
89.29
88.07
88.94
90.40 20 30 40 50 60 70 80 90"
FINE-TUNING NEW PARAMETERS,0.3321554770318021,High Data & High Similarity
FINE-TUNING NEW PARAMETERS,0.33568904593639576,"27.57
32.65"
FINE-TUNING NEW PARAMETERS,0.3392226148409894,"75.60
74.57
72.26
74.12
73.11 25 35 45 55 65 75"
FINE-TUNING NEW PARAMETERS,0.34275618374558303,High Data & Low Similarity 12.61 34.93
FINE-TUNING NEW PARAMETERS,0.3462897526501767,"53.98
52.00 46.35"
FINE-TUNING NEW PARAMETERS,0.3498233215547703,"50.69
53.48 10 20 30 40 50 60"
FINE-TUNING NEW PARAMETERS,0.35335689045936397,Low Data & Low Similarity
FINE-TUNING NEW PARAMETERS,0.3568904593639576,"Figure 3: Comparison of ﬁne-tuning methods across different regimes of training data and CLIP zero shot
performance. Within each quadrant, results are averaged over all corresponding datasets. LayerNorm tuning is
the strongest baseline and performs the best in all regimes. All ﬁne-tuning methods generally provide a large
beneﬁt over zero-shot CLIP."
FINE-TUNING NEW PARAMETERS,0.36042402826855124,"W 2 Rm⇥d, b 2 Rd are learned parameters and d ⌧m, compacter modules represent W as W = n
X i=1"
FINE-TUNING NEW PARAMETERS,0.36395759717314485,Ai ⌦(sitT i )
FINE-TUNING NEW PARAMETERS,0.3674911660777385,"where Ai are global weights shared across Transformer layers and si, ti are local, rank-1 weights.
We insert Adapter and Compacter modules across the Transformer layers in the text encoder."
COMBINING LAYERNORM-TUNING WITH FINE-TUNING METHODS,0.3710247349823322,"3.3
COMBINING LAYERNORM-TUNING WITH FINE-TUNING METHODS"
COMBINING LAYERNORM-TUNING WITH FINE-TUNING METHODS,0.3745583038869258,"While we ﬁnd that just LayerNorm tuning by itself is a strong baseline, an additional beneﬁt is that
it can be combined with any other ﬁne-tuning method given that the underlying model architecture
contains Layer Normalization parameters. The parameters of the alternative method can simply be
ﬁne-tuned simultaneously with the Layer Normalization parameters. For example, Houlsby et al.
(2019) combine LayerNorm tuning with Adapter modules in their Adapter network for language
tasks."
COMBINING LAYERNORM-TUNING WITH FINE-TUNING METHODS,0.37809187279151946,"Fine-tuned LayerNorm as Initialization
We propose an additional approach for combining Layer-
Norm Tuning with other ﬁne-tuning methods. We ﬁrst ﬁnetune a CLIP model using only LayerNorm
tuning. The weights of this model can then be used as initialization for any arbitrary subsequent
ﬁne-tuning method. In this multi-stage process, we effectively distill the ﬁne-tuned LayerNorm model
through the LayerNorm parameters to the secondary ﬁne-tuning method."
EXPERIMENTS,0.38162544169611307,"4
EXPERIMENTS"
EXPERIMENTS,0.38515901060070673,"Setup The goal of this work is to study transfer learning to downstream vision tasks. However, the
downstream transfer performance depends on two key factors: the amount of training data present as
well as the distribution of that training data relative to what the model was pre-trained on. We aim to
investigate transfer learning across both these dimensions. To do so, we create 4 different benchmark
suites across these two factors: low data and high similarity, low data and low similarity, high data
and high similarity, and high data and low similarity. Results along these axes are shown in Figure 3."
EXPERIMENTS,0.38869257950530034,"Datasets We select a diverse set of 12 image classiﬁcation datasets. We consider a subset of 7
datasets that Radford et al. (2021) use for zero-shot CLIP evaluation: MNIST (LeCun, 1998),
EuroSAT (Helber et al., 2019), CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al.,
2009), Flowers102 (Nilsback & Zisserman, 2008), DTD (Cimpoi et al., 2014), and Food101 (Bossard
et al., 2014). We then test on 3 distribution shift datasets from the WILDS benchmark (Koh et al.,
2021): FMoW (Christie et al., 2018), Camelyon17 (Bandi et al., 2018), and iWildCam (Beery et al.,
2021). We ﬁnally benchmark on 2 few-shot adaptation tasks: MiniImageNet (Vinyals et al., 2016)
and CUB (Wah et al., 2011)."
EXPERIMENTS,0.392226148409894,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3957597173144876,"Spectrum 1: Amount of Downstream Data We control for the amount of downstream training
data by considering k-shot settings where k samples from each class are made available during
the ﬁne-tuning training phase. For the low-data regime, we follow the few-shot evaluation scheme
described in Radford et al. (2021) and train with 1, 2, 4, 8, and 16-shots, while for the high-data
regime, we train with 256 and 512-shots. For each setting, we evaluate our methods on the full tests
sets and average our results across all corresponding shots and three random seeds."
EXPERIMENTS,0.3992932862190813,"Spectrum 2: Distribution of Downstream Data We measure the similarity of downstream data to
the one that CLIP was pretrained on by measuring the performance of purely transferring CLIP zero-
shot. Our 12 datasets cover a range of image classiﬁcation tasks including ﬁne-grained classiﬁcation,
distribution shift, and few-shot learning, and they encompass diverse domains and varying downstream
distributions. We split them into high and low similarity regimes according to zero-shot accuracy
using a threshold accuracy of 55%. Under this scheme, MNIST, EuroSAT, DTD, FMoW, and
iWildCam fall under the low similarity regime while CIFAR10, CIFAR100, Flowers, Camelyon17,
and Food101 fall under the high similarity regime. For the high data, low similarity regime and high
data, high similarity regime, we exclude DTD and Flowers respectively due to a lack of data."
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4028268551236749,"4.1
EFFECTIVENESS OF INTERMEDIATE WEIGHTS 0 10 20 30 40 50 60 70 80 90 100 FMoW"
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.40636042402826855,iWildCam
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4098939929328622,CIFAR100
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4134275618374558,Camelyon17 MNIST
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4169611307420495,EuroSAT
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4204946996466431,CIFAR10
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.42402826855123676,Food101
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4275618374558304,High Data 0 10 20 30 40 50 60 70 80 90 100 FMoW
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.43109540636042404,iWildCam DTD
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.43462897526501765,CIFAR100
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4381625441696113,EuroSAT
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4416961130742049,Camelyon17 MNIST
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4452296819787986,Food101
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.44876325088339225,Flowers
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.45229681978798586,CIFAR10
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4558303886925795,Low Data
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.45936395759717313,"Figure 4: Accuracy of baseline ﬁne-tuning
methods across datasets in the low and high-
data regimes. Datasets are ordered along the
x-axis by average performance of ﬁne-tuning
methods. Flowers and Food101 datasets are
ommited from the high data regime due to
lack of data. We observe that LayerNorm
tuning is a strong baseline across all regimes
and datasets and provides substantial gains
over zero-shot CLIP."
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4628975265017668,"Given this characterization of 4 downstream regimes, we
evaluate the performance of 5 baseline ﬁne-tuning meth-
ods: linear probe, prompt tuning, LayerNorm tuning,
adapter networks, and compacter networks. For a fair
comparison and to isolate the effect of each method, we
don’t simultaneously tune LayerNorm parameters in any
of the other 4 ﬁne-tuning approaches. Additionally, we
compare with a sixth baseline: zero-shot CLIP using the
prompt engineering detailed in Radford et al. (2021). The
addition of a strong zero-shot baseline allows us to eval-
uate how much our ﬁne-tuning methods help across the
different settings. Full-model ﬁne-tuning is included as a
seventh baseline for a complete, fair comparison."
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.4664310954063604,"Our results from Figure 3 show that LayerNorm tuning is a
simple but highly effective baseline across all four regimes.
It performs the best in all four regimes including the most
difﬁcult quadrant: the low-data, hard zero-shot regime.
Across individual datasets, Figure 4 shows that LayerNorm
tuning is consistently among the best as well. This points
towards the importance of cross-modal interaction when
ﬁne-tuning as LayerNorm tuning is the only method which
trains parameters in both CLIP encoders. More generally,
it suggests that transfer performance on downstream vision
tasks can beneﬁt from ﬁne-tuning grounded in language."
EFFECTIVENESS OF INTERMEDIATE WEIGHTS,0.46996466431095407,"Furthermore, the strong performance of LayerNorm tuning
as well as adapter and compacter networks in the low-
data regime suggests that either ﬁne-tuning or injecting
parameters among intermediate layers in a network is key
for efﬁcient adaptation. Intuitively, acting only on the model input or output like prompt tuning
or linear probe has limited expressivity compared to modifying the intermediate layers themselves,
particularly in low-data regimes."
LEVERAGING LAYERNORM TUNING,0.4734982332155477,"4.2
LEVERAGING LAYERNORM TUNING"
LEVERAGING LAYERNORM TUNING,0.47703180212014135,"The results of the previous section demonstrate that just LayerNorm tuning is a competitive baseline to
many of the existing ﬁne-tuning methods. We now examine how much LayerNorm tuning can beneﬁt
existing methods. We consider two ways of incorporating LayerNorm parameters: by simultaneously
tuning them or by applying ﬁne-tuned LayerNorm as initialization as described in Section 3.2. For"
LEVERAGING LAYERNORM TUNING,0.48056537102473496,Under review as a conference paper at ICLR 2022
LEVERAGING LAYERNORM TUNING,0.4840989399293286,"Low-Data Regime
High-Data Regime"
LEVERAGING LAYERNORM TUNING,0.4876325088339223,"Type of LN Tuning
None
Normal
As Initialization
None
Normal
As Initialization"
LEVERAGING LAYERNORM TUNING,0.4911660777385159,"Linear Probe
62.54
63.55
66.61
81.93
83.84
84.36
Prompt Tuning
61.70
62.82
63.95
80.26
83.69
83.17
Adapter
64.82
66.23
66.63
81.53
83.31
83.63
Compacter
68.15
69.69
68.76
81.75
83.61
83.17
Table 1: Effect of combining LayerNorm tuning with ﬁne-tuning methods. Normal refers to LN tuning by
simultaneously tuning LayerNorm parameters with the speciﬁed ﬁne-tuning method. For all approaches, the
addition of LayerNorm tuning in either form provides a signiﬁcant performance boost. Linear probe and prompt
tuning receive the largest beneﬁt when combined with any form of LayerNorm tuning."
LEVERAGING LAYERNORM TUNING,0.49469964664310956,"each of the remaining ﬁne-tuning methods (linear probe, prompt tuning, adapter networks, compacter
networks), we compare the baseline method to these two variants."
LEVERAGING LAYERNORM TUNING,0.49823321554770317,"We average the results over the corresponding datasets in the low and high-data regimes respectively.
As Table 1 shows, incorporating any form of LayerNorm tuning increases performance compared
to the normal baselines across all methods and regimes. Because the process of incorporating
LayerNorm tuning is method-agnostic, we recommend this as a simple approach to improve transfer
performance."
LEVERAGING LAYERNORM TUNING,0.5017667844522968,"Across speciﬁc ﬁne-tuning methods, linear probe receives the largest beneﬁts from applying Lay-
erNorm tuning ﬁrst before ﬁnetuning the linear probe. We posit that this is the case as ﬁnetuning
LayerNorm ﬁrst effectively distills the information from the pre-trained text encoder to the Lay-
erNorm parameters on the vision side. This is privileged information that a classical linear probe
doesn’t have access to and provides further evidence towards the beneﬁt of leveraging both vision
and text on downstream, unimodal tasks."
LEVERAGING LAYERNORM TUNING,0.5053003533568905,"Finally, we observe that for adapter and compacter networks, using ﬁne-tuned LayerNorm as initial-
ization performs equivalently or slightly worse compared to simultaneously tuning the LayerNorm
parameters across both data regimes. This suggests that LayerNorm tuning and inserting adapter and
compacter modules may serve similar roles as ﬁne-tuning mechanisms."
GENERAL RECIPES,0.508833922261484,"4.3
GENERAL RECIPES"
GENERAL RECIPES,0.5123674911660777,"From the previous two sections, we’ve seen that just LayerNorm tuning is a surprisingly effective
baseline and applying it on top of other ﬁne-tuning methods provides performance gains. We now
investigate the question of what the best performing combination of ﬁne-tuning methods across the
four regimes are. We follow the experimental setup in Section 4 but compare the performance of our
ﬁne-tuning methods when combined with LayerNorm tuning ﬁrst."
GENERAL RECIPES,0.5159010600706714,"Figure 7 indicates that there is no clear best baseline across all four regimes but linear probe
and compacter have the strongest performance when combined with ﬁne-tuned LayerNorm as
initialization. Across regimes, we observe similar results to Figure 3. In both low-data regimes,
we ﬁnd that prompt tuning generally perform worse than the remaining ﬁne-tuning methods while
compacter performs better. Across both high-data regimes, linear probe performs quite strongly.
Notably, these trends hold despite the addition of our initialization scheme. We recommend these, as
well as just Layer Norm tuning, as general recipes when selecting a ﬁne-tuning method to use on a
downstream task depending on the setting."
DISTRIBUTION SHIFT,0.519434628975265,"4.4
DISTRIBUTION SHIFT"
DISTRIBUTION SHIFT,0.5229681978798587,"While we test on the WILDS benchmark in our analysis above, we benchmark our methods to evaluate
how robust they are to distribution shift in downstream tasks. We focus on domain generalization
where the train and test distributions come from disjoint domains (Koh et al., 2021). For example,
the Camelyon17 dataset contains training and testing images of tumor tissues coming from distinct
hospitals. We compare our results to those on the public leaderboard which contains techniques for
domain generalization while our models are simply trained according to an empirical risk minimiza-
tion objective. For performance metrics, we use average accuracy for FMoW and Camelyon17, and
Macro F1 for iWildCam."
DISTRIBUTION SHIFT,0.5265017667844523,Under review as a conference paper at ICLR 2022
DISTRIBUTION SHIFT,0.5300353356890459,Amount of Training Data →
DISTRIBUTION SHIFT,0.5335689045936396,CLIP Zero-Shot Performance → 76.93 78.73 79.96 84.07 75 77 79 81 83 85
DISTRIBUTION SHIFT,0.5371024734982333,Low Data & High Similarity 91.30
DISTRIBUTION SHIFT,0.5406360424028268,"90.97
90.80
90.57 88 89 90 91 92"
DISTRIBUTION SHIFT,0.5441696113074205,High Data & High Similarity 56.29 49.16
DISTRIBUTION SHIFT,0.5477031802120141,"53.31
53.46 45 47 49 51 53 55 57"
DISTRIBUTION SHIFT,0.5512367491166078,Low Data & Low Similarity 77.43 75.36 76.46 75.77 70 72 74 76 78
DISTRIBUTION SHIFT,0.5547703180212014,High Data & Low Similarity
DISTRIBUTION SHIFT,0.558303886925795,"Figure 5: Comparison of ﬁne-tuning methods initialized with ﬁne-tuned LayerNorm across different regimes
of training data and CLIP zero shot performance. Although there is no clear best combination across all four
regimes, we recommend general recipes of using Linear Probe with ﬁne-tuned initialization in the high data
regimes and Compacter with ﬁne-tuned initialization in the low data regimes."
DISTRIBUTION SHIFT,0.5618374558303887,"FMoW
Camelyon17
iWildCam"
DISTRIBUTION SHIFT,0.5653710247349824,"Zero-shot
19.71
67.46
3.73
LayerNorm Tuning
47.59
90.47
18.52
Linear Probe + LN as Initialization
48.98
89.98
23.80
Best leaderboard result
55.5
91.6
38.5"
DISTRIBUTION SHIFT,0.568904593639576,"Table 2: Results on the WILDS benchmark. We benchmark
our ﬁne-tuning methods on three image classiﬁcation datasets
and ﬁnd that our ﬁne-tuning methods improve upon zero-shot
CLIP performance signiﬁcantly."
DISTRIBUTION SHIFT,0.5724381625441696,"As shown in Table 2, our methods don’t
quite match the best reported results, but
close the gap from zero-shot CLIP signiﬁ-
cantly. We are competitive with other do-
main generalization speciﬁc approaches on
the leaderboard despite the simplicity of
our ﬁne-tuning methods. Our results show
that our ﬁne-tuning methods are effective in
adapting CLIP to difﬁcult downstream tasks
and relatively robust to factors in distribution shift."
FEW-SHOT LEARNING,0.5759717314487632,"4.5
FEW-SHOT LEARNING"
FEW-SHOT LEARNING,0.5795053003533569,"Our previous experiments show results in the low-data regime, but we also apply our ﬁne-tuning
methods with CLIP to more standard meta few-shot tasks to evaluate class generalization. In particular,
we consider the setting where given a labeled dataset of base classes, the objective is to identify novel
classes only using a few samples. Formally, we are given a large dataset of B base classes. At test
time, in a single episode of a N-way K-shot few-shot task, we are given a support set with N test
classes and K samples per class as well as a query set with N test classes and Q samples per class.
We measure the accuracy of classifying the N ⇥Q query images into N classes (Chen et al., 2020)."
FEW-SHOT LEARNING,0.5830388692579506,"We adopt our ﬁne-tuning methods to this setting in two stages. First, we pre-train on the full dataset
of the base classes in the same way we ﬁne-tune to any of the prior downstream tasks. Then, we
use the image encoder of the ﬁne-tuned model within the Classiﬁer Baseline proposed by Chen et al.
(2020). For a given few-shot task, we compute a representative for each of the N classes by averaging
the embeddings of the K support examples. We classify each of the N ⇥Q query-set examples
according to the cosine similarity of their embeddings to the representatives."
FEW-SHOT LEARNING,0.5865724381625441,"Using this approach, we test our ﬁne-tuning methods on MiniImageNet and CUB in the 5-way
1-shot and 5-way 5-shot settings averaged over 600 episodes. Table 4 shows that simply applying"
FEW-SHOT LEARNING,0.5901060070671378,"zero-shot CLIP in this way does remarkably well, outperforming the best reported results on the
public leaderboards for these tasks."
FEW-SHOT LEARNING,0.5936395759717314,"All of our ﬁne-tuning methods provide further improvement with Linear Probe and LayerNorm tuning
reaching 6% higher accuracy than the current SOTA on 5-way 1-shot MiniImageNet as shown in
Tables 3 and 4. The strong performance of linear probe in this setting is expected as the Classiﬁer
Baseline doesn’t utilize the CLIP text encoder at all. Of the ﬁne-tuning methods we evaluate, Linear
Probe with LayerNorm tuning is the only method which trains parameters solely in the image encoder,
so its visual representations transfer the best."
FEW-SHOT LEARNING,0.5971731448763251,Under review as a conference paper at ICLR 2022
FEW-SHOT LEARNING,0.6007067137809188,"Mini-ImageNet (1-shot)
Mini-ImageNet (5-shot)"
FEW-SHOT LEARNING,0.6042402826855123,"Zero-shot
86.20
96.56
LayerNorm
89.24
96.46
Prompt Tuning + LN
89.61
97.05
Adapter + LN
91.17
97.39
Linear Probe + LN
92.08
97.94
Best leaderboard result
82.99
91.50"
FEW-SHOT LEARNING,0.607773851590106,"Table 3:
Few-shot classiﬁcation accuracy on Mini-
ImageNet. Just zero-shot CLIP performs strongly on
few-shot adaptation, and our ﬁne-tuning methods pro-
vide additional performance gains. A combination of
Linear Probe with LayerNorm tuning performs the best,
exceeding the current reported SOTA on Mini-ImageNet."
FEW-SHOT LEARNING,0.6113074204946997,"CUB (1-shot)
CUB (5-shot)"
FEW-SHOT LEARNING,0.6148409893992933,"Zero-shot
87.04
97.28
LayerNorm
91.40
98.16
Adapter + LN as Initialization
91.73
98.20
Prompt Tuning + LN as Initialization
92.21
98.20
Linear Probe + LN as Initialization
93.73
98.50
Best leaderboard result
94.73
96.28"
FEW-SHOT LEARNING,0.6183745583038869,"Table 4: Few-shot classiﬁcation accuracy on CUB.
Similar to our results on Mini-ImageNet, we see that
zero-shot CLIP performs strongly but ﬁne-tuning
with LayerNorm on top can produce signiﬁcant im-
provements in accuracy. Linear Probe combined
with LayerNorm performs the best again, exceeding
the current SOTA in the 5-shot setting."
RELATED WORK,0.6219081272084805,"5
RELATED WORK"
RELATED WORK,0.6254416961130742,"Large-Scale Transformer-Based Models: Unsupervised pre-training for language typically takes
advantage of the sequential nature of text through a self-supervised prediction task. Initially, recurrent
neural networks (RNNs) (Hochreiter & Schmidhuber, 1997) were the predominant deep learning
architectures for unsupervised language learning and were particularly successful for machine
translation (Sutskever et al., 2014). In the context of language modeling, RNNs were superseded by
attention architectures (Bahdanau et al., 2015) and speciﬁcally masked self-attention Transformer
architectures (Vaswani et al., 2017; Devlin et al., 2018; Radford et al., 2019). Over the last few years,
Transformers have produced impressive generalization results through unsupervised pre-training of
increasingly larger models on larger datasets (Brown et al., 2020)."
RELATED WORK,0.6289752650176679,"Multimodal Learning: Multimodal models (Ngiam et al., 2011) are trained through tasks that
leverage multiple data modalities simultaneously, such as vision and language. Examples include text
to image synthesis (Reed et al., 2016) and text descriptions of visual inputs (Krishna et al., 2017).
Recently, multi-modal architectures have been combined with large-scale unsupervised pre-training
to achieve impressive text-to-image generation (Ramesh et al., 2021) as well as learning joint image
and language embeddings (Radford et al., 2021). In particular, this work investigates how do adapt
a pre-trained CLIP model, which uses noise contrastive estimation and transformers to maximize
similarity between images and their text captions, to downstream tasks."
RELATED WORK,0.6325088339222615,"Finetuning Pre-trained Models: With the emergence of large-scale pre-trained vision and language
models, it’s becoming increasingly important to adapt such models efﬁciently to downstream tasks.
For transfer learning in vision, the most common approach is to use a linear probe on top of pretrained
image features while for language, a variety of ﬁne-tuning approaches have emerged including
variations of prompt tuning (Liu et al., 2021; Li & Liang, 2021; Lester et al., 2021; Zhou et al.,
2021), adapter and compacter networks (Houlsby et al., 2019; Mahabadi et al., 2021), and multimodal
approaches applicable to vision and language (Tsimpoukelli et al., 2021; Shen et al., 2021). We hope
our analysis provides insight into some of these methods."
RELATED WORK,0.6360424028268551,"6
DISCUSSION: GENERAL GUIDELINES"
RELATED WORK,0.6395759717314488,"Our work analyzes relevant questions in transfer learning of large-scale pretrained vision-and-
language model to several downstream classiﬁcation tasks. We evaluate 5 different ﬁne-tuning
baseline methods across 12 total image classiﬁcation datasets and ﬁnd that just tuning Layer Normal-
ization parameters is a surprisingly effective, parameter-efﬁcient baseline, and propose an effective
approach to combine it with other ﬁnetuning methods."
RELATED WORK,0.6431095406360424,"We analyze our best-performing ﬁne-tuning methods over different settings to ﬁnd general guidelines.
For all of our methods, we combine them with ﬁne-tuned LayerNorm as initialization. For the
low-data regime, we recommend using ﬁne-tuning approaches which inject or modify intermediate
parameters like LayerNorm tuning, Adapter networks, and Compacter networks. For the high-data
regime, we recommend using linear probe or prompt tuning. For generic settings, we recommend
simply LayerNorm tuning. Code to reproduce the experiments will be made available. We hope
that this work will lead to a broader future research in efﬁcient adaptation of large-scale pre-trained
models, not just limited to vision-and-language models."
RELATED WORK,0.6466431095406361,Under review as a conference paper at ICLR 2022
REFERENCES,0.6501766784452296,REFERENCES
REFERENCES,0.6537102473498233,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint"
REFERENCES,0.657243816254417,"arXiv:1607.06450, 2016. 1, 4"
REFERENCES,0.6607773851590106,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly"
REFERENCES,0.6643109540636042,"learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473. 9"
REFERENCES,0.6678445229681979,"Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke"
REFERENCES,0.6713780918727915,"Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al.
From detection of individual metastases to classiﬁcation of lymph node status at the patient level:
the camelyon17 challenge. IEEE transactions on medical imaging, 38(2):550–560, 2018. 5"
REFERENCES,0.6749116607773852,"Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar. The iwildcam 2021 competition"
REFERENCES,0.6784452296819788,"dataset. arXiv preprint arXiv:2105.03494, 2021. 5"
REFERENCES,0.6819787985865724,"Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative compo-"
REFERENCES,0.6855123674911661,"nents with random forests. In European conference on computer vision, pp. 446–461. Springer,
2014. 5"
REFERENCES,0.6890459363957597,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,"
REFERENCES,0.6925795053003534,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1, 9"
REFERENCES,0.696113074204947,"Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell. A new meta-baseline for"
REFERENCES,0.6996466431095406,"few-shot learning. arXiv preprint arXiv:2003.04390, 2020. 8"
REFERENCES,0.7031802120141343,"Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In"
REFERENCES,0.7067137809187279,"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6172–6180,
2018. 5"
REFERENCES,0.7102473498233216,"Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-"
REFERENCES,0.7137809187279152,"scribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3606–3613, 2014. 5"
REFERENCES,0.7173144876325088,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep"
REFERENCES,0.7208480565371025,"bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1,
9"
REFERENCES,0.7243816254416962,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas"
REFERENCES,0.7279151943462897,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020. 2"
REFERENCES,0.7314487632508834,"Jonathan Frankle, David J Schwab, and Ari S Morcos. Training batchnorm and only batchnorm: On"
REFERENCES,0.734982332155477,"the expressive power of random features in cnns. arXiv preprint arXiv:2003.00152, 2020. 1, 4"
REFERENCES,0.7385159010600707,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image"
REFERENCES,0.7420494699646644,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016. 3"
REFERENCES,0.7455830388692579,"Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset"
REFERENCES,0.7491166077738516,"and deep learning benchmark for land use and land cover classiﬁcation. IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sensing, 12(7):2217–2226, 2019. 5"
REFERENCES,0.7526501766784452,"Sepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9"
REFERENCES,0.7561837455830389,"(8):1735–1780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https:
//doi.org/10.1162/neco.1997.9.8.1735. 9"
REFERENCES,0.7597173144876325,"Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,"
REFERENCES,0.7632508833922261,"Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for
nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. 1, 4, 5, 9"
REFERENCES,0.7667844522968198,Under review as a conference paper at ICLR 2022
REFERENCES,0.7703180212014135,Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
REFERENCES,0.773851590106007,"reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015. 4"
REFERENCES,0.7773851590106007,"Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan"
REFERENCES,0.7809187279151943,"Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021. 1, 2"
REFERENCES,0.784452296819788,"Pang Wei Koh, Shiori Sagawa, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu,"
REFERENCES,0.7879858657243817,"Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, et al. Wilds: A benchmark of
in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664.
PMLR, 2021. 5, 7"
REFERENCES,0.7915194346289752,"Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning"
REFERENCES,0.7950530035335689,"events in videos. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy,
October 22-29, 2017, pp. 706–715. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.83.
URL https://doi.org/10.1109/ICCV.2017.83. 9"
REFERENCES,0.7985865724381626,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 5"
REFERENCES,0.8021201413427562,"Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 3, 5"
REFERENCES,0.8056537102473498,"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt"
REFERENCES,0.8091872791519434,"tuning. arXiv preprint arXiv:2104.08691, 2021. 1, 4, 9"
REFERENCES,0.8127208480565371,Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv
REFERENCES,0.8162544169611308,"preprint arXiv:2101.00190, 2021. 1, 4, 9"
REFERENCES,0.8197879858657244,"Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig."
REFERENCES,0.823321554770318,"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing. arXiv preprint arXiv:2107.13586, 2021. 9"
REFERENCES,0.8268551236749117,"Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal"
REFERENCES,0.8303886925795053,"computation engines. arXiv preprint arXiv:2103.05247, 2021. 1"
REFERENCES,0.833922261484099,"Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank"
REFERENCES,0.8374558303886925,"hypercomplex adapter layers. arXiv preprint arXiv:2106.04647, 2021. 1, 4, 9"
REFERENCES,0.8409893992932862,"Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multi-"
REFERENCES,0.8445229681978799,"modal deep learning. In ICML, 2011. 9"
REFERENCES,0.8480565371024735,Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number
REFERENCES,0.8515901060070671,"of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
pp. 722–729. IEEE, 2008. 5"
REFERENCES,0.8551236749116607,"Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual"
REFERENCES,0.8586572438162544,"reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 32, 2018. 1"
REFERENCES,0.8621908127208481,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language"
REFERENCES,0.8657243816254417,"models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1, 2, 9"
REFERENCES,0.8692579505300353,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,"
REFERENCES,0.872791519434629,"Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 1, 2, 3, 5, 6, 9,
13"
REFERENCES,0.8763250883392226,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,"
REFERENCES,0.8798586572438163,"and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.),
Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8821–8831.
PMLR, 2021. URL http://proceedings.mlr.press/v139/ramesh21a.html. 9"
REFERENCES,0.8833922261484098,Under review as a conference paper at ICLR 2022
REFERENCES,0.8869257950530035,"Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak"
REFERENCES,0.8904593639575972,"Lee. Generative adversarial text to image synthesis. In Maria-Florina Balcan and Kilian Q.
Weinberger (eds.), Proceedings of the 33nd International Conference on Machine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference
Proceedings, pp. 1060–1069. JMLR.org, 2016. URL http://proceedings.mlr.press/
v48/reed16.html. 9"
REFERENCES,0.8939929328621908,"Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei"
REFERENCES,0.8975265017667845,"Yao, and Kurt Keutzer. How much can clip beneﬁt vision-and-language tasks? arXiv preprint"
REFERENCES,0.901060070671378,"arXiv:2107.06383, 2021. 9"
REFERENCES,0.9045936395759717,"Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks."
REFERENCES,0.9081272084805654,"In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger
(eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural
Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp.
3104–3112, 2014. URL https://proceedings.neurips.cc/paper/2014/hash/
a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html. 9"
REFERENCES,0.911660777385159,"Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multi-"
REFERENCES,0.9151943462897526,"modal few-shot learning with frozen language models. arXiv preprint arXiv:2106.13884, 2021.
9"
REFERENCES,0.9187279151943463,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz"
REFERENCES,0.9222614840989399,"Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017. 2, 9"
REFERENCES,0.9257950530035336,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one"
REFERENCES,0.9293286219081273,"shot learning. Advances in neural information processing systems, 29:3630–3638, 2016. 5"
REFERENCES,0.9328621908127208,"Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd"
REFERENCES,0.9363957597173145,birds-200-2011 dataset. 2011. 5
REFERENCES,0.9399293286219081,"Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-"
REFERENCES,0.9434628975265018,"language models. arXiv preprint arXiv:2109.01134, 2021. 4, 9"
REFERENCES,0.9469964664310954,Under review as a conference paper at ICLR 2022
REFERENCES,0.950530035335689,"A
APPENDIX"
REFERENCES,0.9540636042402827,"A.1
DATASET INFORMATION"
REFERENCES,0.9575971731448764,"Dataset
Classes
Train
Validation
Test"
REFERENCES,0.9611307420494699,"MNIST
10
58,000
2,000
10,000
EuroSAT
10
15,000
2,000
10,000
CIFAR-10
10
48,000
2,000
10,000
CIFAR-100
100
48,000
2,000
10,000
DTD
47
1,880
1,880
1,880
Flowers102
102
1,632
408
6,149
Food-101
101
60,600
15,150
25,250
FMoW
62
76,863
19,915
22,108
iWildCam
182
129,809
14,961
42,791
Camelyon17
2
302,436
34,904
85,054"
REFERENCES,0.9646643109540636,"CUB
200
5,891
2,932
2,965
MiniImageNet
100
38,400
9,600
12,000"
REFERENCES,0.9681978798586572,Table 5: Dataset information.
REFERENCES,0.9717314487632509,"Table 5 shows information about the number of classes and the train, validation, and test split for
each downstream dataset we evaluate. For MiniImageNet, the 100 classes are split into 64, 16, and
20 classes for training, validation, and testing respectively. For CUB, the 200 classes are split into
100, 50, and 50 classes for training, validation, and testing respectively."
REFERENCES,0.9752650176678446,"A.2
CLIP IMPLEMENTATION"
REFERENCES,0.9787985865724381,"We implement our adaptation methods on top of the PyTorch implementation provided by the authors
of CLIP at https://github.com/openai/CLIP. For the image encoder, we use a Vision
Transformer with patch size 16 (ViT-B/16) and initialize all of our models with the corresponding
pre-trained weights. For zero-shot CLIP evaluation, we perform the prompt engineering described in
Radford et al. (2021) and ensemble the features from the text encoder across 80 templates 2."
REFERENCES,0.9823321554770318,"A.3
TRAINING DETAILS"
REFERENCES,0.9858657243816255,"Input Processing
For the raw images, we preprocess them with normalization, resizing, and
random cropping to size 224 by 224. For the textual descriptions, we encode them using a byte pair
encoding like Radford et al. (2021)."
REFERENCES,0.9893992932862191,"Hyperparameters of ﬁne-tuning methods
For prompt tuning, we use a prompt of 8 512-
dimensional vectors. For adapter networks, we use a bottleneck dimension of size 24. For compacter
networks, we use the same bottleneck dimension and have 4 global weights of dimension 4 by 4."
REFERENCES,0.9929328621908127,"Training Procedure
For all ﬁne-tuning methods, we train with a cross-entropy loss. We use the
AdamW optimizer with an initial learning rate of 5e-4 and weight decay of 0.02 as well as a cosine
annealing scheduler. We evaluate our model on the validation set after every epoch and keep the
best-performing checkpoint. In the low-data regime, we train for 100 epochs and use a validation
set of size # of shots except for Flowers where we use min{# of shots, 4}. In the high-data regime,
we train for 50 epochs and use a validation set of size (# of shots)/10. Due to data constraints, for
CIFAR-100, we train with 256 and 450-shots (instead of 256 and 512-shots) in the high-data regime.
For CUB and MiniImageNet, we use the same hyperparameters and train for 100 epochs on the base
classes."
REFERENCES,0.9964664310954063,"2https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_
for_ImageNet.ipynb"
