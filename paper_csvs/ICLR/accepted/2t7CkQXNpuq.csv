Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003952569169960474,"Being able to predict the mental states of others is a key factor to effective social
interaction. It is also crucial for distributed multi-agent systems, where agents
are required to communicate and cooperate. In this paper, we introduce such an
important social-cognitive skill, i.e. Theory of Mind (ToM), to build socially intel-
ligent agents who are able to communicate and cooperate effectively to accomplish
challenging tasks. With ToM, each agent is capable of inferring the mental states
and intentions of others according to its (local) observation. Based on the inferred
states, the agents decide “when” and with “whom” to share their intentions. With
the information observed, inferred, and received, the agents decide their sub-goals
and reach a consensus among the team. In the end, the low-level executors indepen-
dently take primitive actions to accomplish the sub-goals. We demonstrate the idea
in two typical target-oriented multi-agent tasks: cooperative navigation and multi-
sensor target coverage. The experiments show that the proposed model not only
outperforms the state-of-the-art methods on reward and communication efﬁciency,
but also shows good generalization across different scales of the environment."
INTRODUCTION,0.007905138339920948,"1
INTRODUCTION"
INTRODUCTION,0.011857707509881422,"Cooperation is a key component of human society, which enables people to divide labor and achieve
common goals that could not be accomplished independently. In particular, humans are able to
form an ad-hoc team with partners and communicate cooperatively with one another (Tomasello,
2014). Cognitive studies (Sher et al., 2014; Sanfey et al., 2015; Etel & Slaughter, 2019) show that
the ability to model others’ mental states (intentions, beliefs, and desires), called Theory of Mind
(ToM) (Premack & Woodruff, 1978), is important for such social interaction. Consider a simple
real-world scenario (Fig. 1), where three people (Alice, Bob and Carol) are required to take the fruits
(apple, orange and pear) following the shortest path. To achieve it, the individual should take four
steps sequentially: 1) observe their surrounding; 2) infer the observation and intention of others; 3)
communicate with others to share the local observation or intention if necessary; 4) make a decision
and take action to get the chosen fruits without conﬂict. In this process, the ToM is naturally adopted
in inferring others (Step 2) and also guides the communication (Step 3)."
INTRODUCTION,0.015810276679841896,"In this paper, we focus on the Target-oriented Multi-Agent Cooperation (ToMAC) problem, where
agents need to cooperatively adjust the relations among the agents and targets to reach the expectation,
e.g., covering all the targets (Xu et al., 2020). Such problem setting widely exists in real-world
applications, e.g., collecting multiple objects (Fig. 1), navigating to multiple landmarks (Lowe et al.,
2017), following pedestrian (Zhong et al., 2021b), and transporting objects (Tuci et al., 2018). While
running, the distributed agents are required to concurrently choose a subset of interesting targets
and optimize the relation to them to contribute to the team goal. In this case, the key to realizing
high-quality cooperation is to reach a consensus among agents to avoid the inner-team conﬂict.
However, it is still difﬁcult for the off-the-shelf multi-agent reinforcement learning methods, as they
only implicitly model others in the hidden state and are inefﬁcient in communication."
INTRODUCTION,0.019762845849802372,"Here we propose a Target-oriented Multi-agent Communication and Cooperation mechanism
(ToM2C) using Theory of Mind. Shown as Fig. 2, each agent is of a two-level hierarchy. The"
INTRODUCTION,0.023715415019762844,* indicates equal contribution
INTRODUCTION,0.02766798418972332,Published as a conference paper at ICLR 2022 A C B A C B A C B A C B
INTRODUCTION,0.03162055335968379,"Carol
apple pear"
INTRODUCTION,0.03557312252964427,"Bob 
pear"
INTRODUCTION,0.039525691699604744,orange apple
INTRODUCTION,0.043478260869565216,"Alice
apple
Carol
Carol
apple"
INTRODUCTION,0.04743083003952569,"Bob
pear"
INTRODUCTION,0.05138339920948617,"Bob
apple apple"
INTRODUCTION,0.05533596837944664,"orange
pear apple"
INTRODUCTION,0.05928853754940711,"Bob
pear"
INTRODUCTION,0.06324110671936758,"Carol
apple"
INTRODUCTION,0.06719367588932806,Step 1. Observing
INTRODUCTION,0.07114624505928854,Step 3. Communicating
INTRODUCTION,0.07509881422924901,Step 2. Inferring
INTRODUCTION,0.07905138339920949,Step 4. Decision making
INTRODUCTION,0.08300395256916997,"Figure 1: A fruits collection example. The agents are required to cooperatively collect the three target
objects (apple, pear, and orange) in the room as fast as possible. The whole process can be divided
into 4 steps. In Step 1, 3 agents observe the environment and obtain the state of the visible targets. In
Step 2, each agent tries to infer what other agents have seen, and which targets they shall choose as
goals. In Step 3, each agent decides whom to communicate with according to the previous inference.
In Step 4, each agent decides its own goal of target based on what it observed, inferred, and received."
INTRODUCTION,0.08695652173913043,"high-level policy (planner) needs to cooperatively choose certain interesting targets as a sub-goal to
deal with, such as tracking certain moving objects or navigating to a speciﬁc landmark. Then, the
low-level policy (executor) takes primitive actions to reach the selected goals for k steps. Concretely,
each agent receives local observation and estimates the observation of others in the Theory of Mind
Network (ToM Net). Combining the observed and inferred states, the ToM Net will predict/infer the
target choices (intentions) of other agents. After that, each agent decides ‘whom’ to communicate
with according to the local observation ﬁltered by the inferred goals of others. The message is the
predicted goals of the message receiver, inferred by the sender. In the end, all the agents decide their
own goals by leveraging the observed, inferred, and received information. With the inferring and
sharing of intentions, the agents can easily reach a consensus to cooperatively adjust the target-agent
relations by taking a sequence of actions."
INTRODUCTION,0.09090909090909091,"Furthermore, we also introduce a communication reduction method to remove the redundant
message passing among agents. Thanks to the Centralized Training and Decentralized Execution
(CTDE) paradigm, we measure the effect of the received messages on each agent, by comparing
the output of the planner with and without messages. Hence, we can ﬁgure out the unnecessary
connection among agents. Then we train the connection choice network to cut these dispensable
channels in a supervised manner. Eventually, we argue that ToM2C systemically solves the problem
of ‘when’, ‘who’ and ‘what’ in multi-agent communication, providing an efﬁcient and interpretable
communication protocol for multi-agent cooperation."
INTRODUCTION,0.09486166007905138,"The experiments are conducted in two environments. First, in the cooperative navigation sce-
nario (Lowe et al., 2017), the team goal is to occupy landmarks (static targets) and avoid collision.
Then we evaluate our method in a more complex scenario, multi-sensor multi-target covering sce-
nario (Xu et al., 2020). The team goal of the sensors is to adjust their orientation to cover as many
moving targets as possible. The results show that our method achieves the best performance (the
highest reward and the lowest communication cost) among the state-of-the-art MARL methods, e.g.,
HiT-MAC (Xu et al., 2020), I2C (Ding et al., 2020), MAPPO (Yu et al., 2021) and TarMAC (Das
et al., 2019). Moreover, we further show the good scalability of ToM2C and conduct an ablation
study to evaluate the contribution of each key component in ToM2C."
INTRODUCTION,0.09881422924901186,"Our contributions are in three-folds: 1) We introduce a cognition-inspired social agent with the ability
to infer the mental states of others for enhancing multi-agent cooperation in target-oriented tasks.
2) We provide a ToM-based communication mechanism and a communication reduction method to
improve the efﬁciency of multi-agent communication. 3) We conduct experiments in two typical
target-oriented tasks: the cooperative navigation and multi-sensor multi-target coverage problem."
INTRODUCTION,0.10276679841897234,Published as a conference paper at ICLR 2022
INTRODUCTION,0.1067193675889328,Observation Encoder
INTRODUCTION,0.11067193675889328,Critic
INTRODUCTION,0.11462450592885376,"Planner
+ ∑𝑔?,𝑖 ∗
𝑔𝑖"
INTRODUCTION,0.11857707509881422,"(𝜂𝑗, 𝜂k, 𝜂𝑙)"
INTRODUCTION,0.1225296442687747,Executor
INTRODUCTION,0.12648221343873517,Primitive Action 𝑎𝑖 𝜂𝑖 𝑣𝑖
INTRODUCTION,0.13043478260869565,Theory of Mind Net ToM 𝒍
INTRODUCTION,0.13438735177865613,"𝜙𝑗, 𝐸𝑖"
INTRODUCTION,0.1383399209486166,Observation
INTRODUCTION,0.1422924901185771,"Estimation 𝜙𝑗 𝜖𝑖,𝑗"
INTRODUCTION,0.14624505928853754,"Goal 
Inference 𝐸𝑖 𝑔𝑖,𝑗 ∗"
INTRODUCTION,0.15019762845849802,"ToM 𝒌
ToM 𝒋 𝐸𝑖"
INTRODUCTION,0.1541501976284585,Decision Maker
INTRODUCTION,0.15810276679841898,Connection
INTRODUCTION,0.16205533596837945,"Choice i
k l
j"
INTRODUCTION,0.16600790513833993,"Message Sender 𝑔𝑖,𝑗"
INTRODUCTION,0.16996047430830039,"∗
𝑔𝑖,𝑘"
INTRODUCTION,0.17391304347826086,"∗
𝑔𝑖,𝑙 ∗ 𝑜𝑖 𝐺𝑖 ∗ 𝒊 𝒋 𝒌 𝒍"
INTRODUCTION,0.17786561264822134,"𝜙𝑘, 𝐸𝑖
𝜙𝑙, 𝐸𝑖 + i
k"
INTRODUCTION,0.18181818181818182,"l
j 𝑔𝑖,𝑗 ∗ 𝐸𝑖,𝑗"
INTRODUCTION,0.1857707509881423,"′ = 𝐹𝑖𝑙𝑡𝑒𝑟(𝑔𝑖,𝑗"
INTRODUCTION,0.18972332015810275,"∗, 𝐸𝑖)
𝑢𝑖,𝑗= (𝐸𝑖,𝑗"
INTRODUCTION,0.19367588932806323,"′ , 𝜖𝑖,𝑗)"
INTRODUCTION,0.1976284584980237,+ Concatenate 𝑜𝑖
INTRODUCTION,0.2015810276679842,Target
INTRODUCTION,0.20553359683794467,"Figure 2: The architecture of ToM2C for each individual. There are four key components: Observation
encoder, Theory of Mind net, Message sender, and Decision maker. Each agent ﬁrst receives a local
observation and encodes it with the encoder. Then it performs Theory of Mind inference to estimate
the observation of others and predict their goals. Next, it decides ‘whom’ to communicate with
according to local observation ﬁltered by the inferred goals of others. In the end, the planner in
decision maker outputs the sub-goal according to what it observes, infers, and receives. The low-level
executor takes primitive actions to reach the chosen sub-goal independently.
2
RELATED WORK"
INTRODUCTION,0.20948616600790515,"Multi-agent Cooperation and Communication. The cooperation of multiple agents is crucial yet
challenging in distributed systems. Agents’ policies continue to shift during training, leading to a
non-stationary environment and difﬁculty in model convergence. To mitigate the non-stationarity,
the centralized training decentralized execution (CTDE) paradigm is widely employed in the recent
multi-agent learning works (Lowe et al., 2017; Foerster et al., 2018; Sunehag et al., 2018; Rashid et al.,
2018; Iqbal & Sha, 2019). However, these methods only implicitly guide agents to overﬁt certain
policy patterns of others. Without communication mechanism, agents lack the ability to negotiate
with each other and avoid conﬂicts. As a result, it is hard for the individual agent to quickly adapt to
unseen cooperators/environments. Learning to communicate (Sukhbaatar et al., 2016) is a feasible
way to promote efﬁcient multi-agent cooperation. Unfortunately, most previous works (Sukhbaatar
et al., 2016; Das et al., 2019; Singh et al., 2019) require a broadcast communication channel, leading
to huge pressure on bandwidth. Besides, even though I2C (Ding et al., 2020) proposes an individual
communication method, the message is just the encoding of observation, which is not only costly but
also uninterpretable. (Li et al., 2020a) designs a mechanism to share the camera poses according to
the visibility of the target, by exploiting the behavior consistency among agents. In this paper, we
will investigate a more efﬁcient peer-to-peer communication, based on theory-of-mind. Hierarchical
frameworks (Yang et al., 2019; Kim et al., 2020; Xu et al., 2020) are also investigated to promote
multi-agent cooperation/coordination. HiT-MAC (Xu et al., 2020) is the closest work to ours. It
proposes a hierarchical multi-agent coordination framework to decompose the target coverage problem
into two-level tasks: assigning targets by the centralized coordinator and tracking assigned targets
by decentralized executors. The agents in ToM2C are also of a two-level hierarchy. Considering
the natural structure of ToMAC, we also decompose the cooperation tasks in this way, rather than
learning skills in an unsupervised manner (Yang et al., 2019). Differently, both levels in ToM2C are
enabled to perform distributively, thanks to the use of ToM and communication mechanism."
INTRODUCTION,0.2134387351778656,"Theory of Mind. Theory of Mind is a long-studied concept in cognitive science (Sher et al., 2014;
Sanfey et al., 2015; Etel & Slaughter, 2019). However, how to apply the discovery in cognitive science
to build cooperative multi-agent systems still remains a challenge. Most previous works make use of
Theory of Mind to interpret agent behaviors, but fail to take a step forward to enhance cooperation.
For example, Machine Theory of Mind (Rabinowitz et al., 2018) proposes a meta-learning method
to learn a ToMnet that predicts the behaviors or characteristics of a single agent. Besides, (Shum
et al., 2019) studies how to apply Bayesian inference to understand the behaviors of a group and
predict the group structure. (Track et al., 2018) introduces the concept of Satisﬁcing Theory of Mind,
which refers to the sufﬁcing and satisfying model of others. (Shu et al., 2021) and (Gandhi et al.,"
INTRODUCTION,0.21739130434782608,Published as a conference paper at ICLR 2022
INTRODUCTION,0.22134387351778656,"2021) introduce benchmarks for evaluating machine mental reasoning. These works mainly focus on
how to accurately infer the mental state of others, rather than interactive cooperation. (Puig et al.,
2021; Carroll et al., 2019; Netanyahu et al., 2021) studies the human-AI cooperation with mental
inference. (Lim et al., 2020) considers a 2-player scenario and employs Bayesian Theory of Mind to
promote collaboration. Nevertheless, the task is relatively simple and it requires the model of other
agents to do the inference. M3RL (Shu & Tian, 2019) proposes to train a manager that infers the
minds of agents and assigns sub-tasks to them, which beneﬁts from the centralized mechanism and
is not comparable with decentralized methods. (Wu et al., 2021) introduces Bayesian approach for
ToM-based cooperation, yet assumes that the environment has a partially ordered set of sub-tasks.
Differently, we study how to leverage ToM to guide efﬁcient decentralized communication to further
enhance multi-agent cooperation."
INTRODUCTION,0.22529644268774704,"Opponent Modeling. Opponent modeling (He et al., 2016; Raileanu et al., 2018; Grover et al.,
2018; Zhong et al., 2021a) is another kind of method comparable with Theory of Mind. Agents
endowed with opponent modeling can explicitly represent the model of others, and therefore plan
with awareness of the current status of others. Nevertheless, these methods rely on the access to
the observation of others, which means they are not truly decentralized paradigms. Compared with
existing methods, ToM2C applies ToM not only to explicitly model intentions and mental states but
also to improve the efﬁciency of communication to further promote cooperation."
METHODS,0.22924901185770752,"3
METHODS"
METHODS,0.233201581027668,"In this section, we will explain how to build a target-oriented social agent to achieve efﬁcient
multi-agent communication and cooperation. We formulate the target-oriented cooperative task as a
Dec-POMDP (Bernstein et al., 2002). The aim of all agents is to maximize the team reward. The
overall network architecture is shown in Fig. 2, from the perspective of agent i. The model is mainly
composed of four functional networks: observation encoder, Theory of Mind network (ToM Net),
message sender, and decision maker. First, the raw observation oi, indicating the states of observed
targets, will be encoded into Ei by an attention-based encoder. After that, the ToM Net takes Theory
of Mind inference ToMi(G∗
i |Ei, Φ) to estimate the joint intention (sub-goals) of others G∗
i , according
to the encoded observation Ei and the poses of agents Φ = (φ1, ..., φn). In details, taking ToMi,j
as an example, it uses the estimated observation ϵi,j infers the probability of agent j choosing these
targets as its goals, denoted as g∗
i,j. The estimation of ϵi,j is based on the pose φj. In general, pose
φj indicates the location and rotation of the agent j. Speciﬁcally, it can be represented as a 6D
vector (x, y, z, roll, yaw, pitch) in 3D space and a 3D vector (x, y, yaw) in 2D plane. After the ToM
inference, the message sender decides whom to communicate with. Here we employ a graph neural
network to model the connection among agents. The node feature of agent j is the concatenation
of ϵi,j and Ei ﬁltered by g∗
i,j. The ﬁnal communication connection is sampled according to the
computed graph edge features. Agent i will send g∗
i,j to agent j if there exists a communication edge
from i to j. In the end, we aggregate G∗
i , Ei and received messages P g∗
?,j as ηi for the decision
making. The planner πH
i (gi|ηi), guided by the team reward, chooses the sub-goal gi to the low-level
executor πL
i (ai|oi, gi), which takes K steps primitive actions to reach the sub-goal."
METHODS,0.23715415019762845,"In the following sections, we will illustrate the key components of ToM2C in detail."
OBSERVATION ENCODER,0.24110671936758893,"3.1
OBSERVATION ENCODER"
OBSERVATION ENCODER,0.2450592885375494,"We employ an attention module (Vaswani et al., 2017) to encode the local observation. There are two
prominent advantages of this module. On one hand, it is population-invariant and order-invariant,
which is crucial for scalability. On the other hand, multi-target information can be encoded into a
single feature due to the weighted sum mechanism. In this paper, we use scaled dot-product self-
attention similar to (Xu et al., 2020). The local observation oi is ﬁrst transformed to key Ki, query
Qi and value Vi through 3 different neural networks. Then the output Ei = softmax( QiKi
T
√dk ) ⊙Vi,
where dk is the dimension of one key. oi,q and Ei,q represent the raw and encoded feature of target q
to agent i respectively."
OBSERVATION ENCODER,0.2490118577075099,"3.2
THEORY OF MIND NETWORK (TOM NET)"
OBSERVATION ENCODER,0.25296442687747034,"Inspired by the Machine Theory of Mind (Rabinowitz et al., 2018), we introduce the ToM Net that
enables agents to infer the observation and intentions of others. For agent i, the ToM Net takes the
poses of agents Φ and the encoded local observation Ei as input. Then it infers the observation ϵi"
OBSERVATION ENCODER,0.25691699604743085,Published as a conference paper at ICLR 2022
OBSERVATION ENCODER,0.2608695652173913,"and goals Gi of others. Most previous work (He et al., 2016; Raileanu et al., 2018; Lim et al., 2020)
consider two-player scenarios, where the agent only needs to model one other agent. Instead, we take
a step forward to run our model in a more complex multi-agent scenario consisting of n (>3) agents.
Therefore, the entire ToM Net of agent i is conceptually composed of n −1 separate sub-modules,
which focus on modeling the mind of one agent respectively. Shown as Fig. 2, ToMi is an ensemble
of ToMi,j, ToMi,k, ToMi,l,. In practice, we simply let these models share parameters since all the
agents are homogeneous. In this way, we can accordingly adjust the number of sub-modules in ToM
Nets with the change of agent number. A single ToM Net is made up of two functional modules:
Observation Estimation and Goal Inference."
OBSERVATION ENCODER,0.2648221343873518,"Observation Estimation. The ﬁrst step of ToM inference is to estimate the observation representation
of the other agent. Intuitively, when an agent tries to infer the intention of others, it should ﬁrst infer
which targets are seen by them. Here, we take Bob in Fig. 1 as an example. Before he tries to infer the
goals of Alice and Carol, he ﬁrst infers that Alice cannot observe the apple but Carol can. Similarly,
agent i infers the observation of agent j, denoted as ϵi,j, with the pose φj. Note that ϵi,j is only a
representation of the estimated observation. To better learn this representation, we further introduce
an auxiliary task, where agents should infer the relation between the targets and other agents, e.g.,
which fruits are visible or closest to Alice. In practice, we employ the Gated Recurrent Units (Cho
et al., 2014) (GRUs) to model the observation of others on time series. In target coverage task, agent i
infers which targets are in the observation ﬁeld of agent j and in cooperative navigation task it infers
which landmark is closest to agent j."
OBSERVATION ENCODER,0.26877470355731226,"Goal Inference. After agent i ﬁnishes the observation estimation of others, it is able to predict which
targets will be chosen by them at this step. Just like humans, the agent infers the intentions of others
based on what it sees and what it thinks that others see. If we denote this goal inference network as
a function GI, then the process can be formulated as g∗
i,j,q = GI(Ei,q, ϵi,j), where g∗
i,j,q stands for
the probability of agent j choosing target q, inferred by i. Since there are a total of n agents and
m targets in the environment, G∗
i ∈R(n−1)×m. With ToM Net, each agent holds a belief on the
observation ϵ and goal intentions G∗of others. Such belief is not only taken into account for decision
making, but also serves as an indispensable component in communication choice."
OBSERVATION ENCODER,0.2727272727272727,"Learning ToM Net. We introduce two classiﬁcation tasks to learn the ToM Net, which is param-
eterized by θToM. First, the ToM Net infers the goals G∗
i of others. Note that g∗
i,j,q indicates the
probability of agent j choosing target q, inferred by i. Meanwhile, agent j decides its real goals gj.
Therefore, gj can be the label of g∗
i,j. The Goal Inference loss is the binary cross entropy loss of this
classiﬁcation task:"
OBSERVATION ENCODER,0.2766798418972332,LGI = −1 N X i X j̸=i X
OBSERVATION ENCODER,0.28063241106719367,"q
[gj,q · log(g∗
i,j,q) + (1 −gj,q) · log(1 −g∗
i,j,q)]
(1)"
OBSERVATION ENCODER,0.2845849802371542,"Secondly, the estimated observation ϵ is additionally guided by the auxiliary task mentioned before.
The agent i infers the relation between the targets and agent j, denoted as c∗
i,j. The ground truth is
the real observation ﬁeld cj. cj,q = 1 indicates that agent j observes target q or landmark q is closest
to agent j. Similar to the previous Goal Inference task, this Observation Estimation learning also
adopts binary cross entropy loss:"
OBSERVATION ENCODER,0.2885375494071146,LOE = −1 N X i X j̸=i X
OBSERVATION ENCODER,0.2924901185770751,"q
[cj,q · log(c∗
i,j,q) + (1 −cj,q) · log(1 −c∗
i,j,q)]
(2)"
OBSERVATION ENCODER,0.2964426877470356,"L(θToM) = LGI + LOE
(3)"
MESSAGE SENDER,0.30039525691699603,"3.3
MESSAGE SENDER"
MESSAGE SENDER,0.30434782608695654,"The message sender leverages the inferred mental state of others from ToM Net to decide ‘when’ and
with ‘whom’ to communicate, independently. During communication, the message sent from agent
i to j is just the inferred intention gi,j. Moreover, we further propose a communication reduction
method, which can remove useless connections to improve communication efﬁciency."
MESSAGE SENDER,0.308300395256917,"In practice, we use Graph Neural Network (GNN), similar to (Li et al., 2020b; Battaglia et al., 2016),
to model the communication network in an end-to-end manner. Each agent computes its own graph
based on its observation and inferred information of others. Speciﬁcally, in the perspective of agent i,
we make use of the inferred intention g∗
i to ﬁlter the observation Ei to generate graph node features.
Edge features are computed based on node features, which are further transferred into a probabilistic"
MESSAGE SENDER,0.31225296442687744,Published as a conference paper at ICLR 2022
MESSAGE SENDER,0.31620553359683795,"distribution over the type of edges(cut or retain). Communication connections are ﬁnally sampled
according to the probabilistic distribution."
MESSAGE SENDER,0.3201581027667984,"Inferred-goal Filter. The feature of agent j in the graph of agent i is the target features ﬁltered
by the inferred goals g∗
i,j as follows. δ is a probability threshold, which we set to 0.5 in this
paper. If g∗
i,j,q > δ, then agent i will consider target q as the goal that will be chosen by agent
j. Then we concatenate the ﬁltered feature E′
i,j = Pm
q=1 (g∗
i,j,q > δ) · Ei,q with the estimated
observation representation ϵi,j, to form the estimated node feature ui,j = (E′
i,j, ϵi,j). For agent i
itself, ui,i = (P"
MESSAGE SENDER,0.3241106719367589,"q Ei,q, ϵi), where ϵi is also computed by Observation Estimation with the pose of i."
MESSAGE SENDER,0.32806324110671936,"Connection Choice. For a scenario consisting of n agents, there is a total of n directed graphs
G = (G1, G2, ...Gn). Gi = (Vi, Ei) is the local graph for agent i to compute the communication
connection from agent i. The vertices Vi = {f(ui,j)}, where f is a node feature encoder. Edges Ei =
{σ(ui,j, ui,k)}, where σ is an edge feature encoder. Like the Interaction Networks (IN) (Battaglia
et al., 2016), we propagate the node and edge features spatially to obtain node and edge effects. For
convenience, we will describe only graph Gi in the following formula and omit the index i. Let Vj
be the encoded node feature of j, and hj be the node effect. Similarly, let Ej,k be the encoded edge
feature, hj,k be the edge effect. Initially, hj = Vj, hj,k = Ej,k. Then the graph iterates several times
to propagate the effect:
hj = Ψnode(Vj, hj,
X"
MESSAGE SENDER,0.33201581027667987,"k
hk,j)
(4)"
MESSAGE SENDER,0.3359683794466403,"hj,k = Ψedge(hj, hk, hj,k)
(5)"
MESSAGE SENDER,0.33992094861660077,"In the end, we obtain the ﬁnal edge feature (Ei,j, hi,j), and compute the probabilistic distribution over
the type of the edge (pi,j
cut + pi,j
retain = 1). Here we apply the Gumbel-Softmax trick (Jang et al., 2016;
Maddison et al., 2016) to sample the discrete edge type, so the gradients can be back-propagated in
end-to-end training. Considering that it is the local communication graph of agent i, only the types of
Ei,−i are sampled. If edge Ei,j is retained, agent i will send g∗
i,j to j."
MESSAGE SENDER,0.3438735177865613,"Communication Reduction (CR). In practice, the GNN tends to learn a densely connected graph,
even if applying a sparsity regularization while learning. However, we observe that the decisions made
by receivers are not always inﬂuenced by the received messages, indicating that some connections
are actually redundant in the GNN. Therefore, it is necessary for us to ﬁgure out the really valuable
connections from the densely connected networks. To this end, we measure the effectiveness of
each connection by taking an ablative analysis. To be speciﬁc, we observe the non-communicative
sub-goals g−
i of agent i by removing the received message in ηi. We can estimate the effect of the
received messages to agent i by measuring the KL-divergence, referred as χ = DKL(g−
i ||gi). Here
we set a constant threshold τ to generate a binary pseudo label for learning to remove redundant
connections. Speciﬁcally, if χ < τ, we regard that the messages are redundant to agent i. Thus the
edges pointing at i will be labeled as ‘cut’, l∗,i = 0. Otherwise (χ > τ), labeled as ‘retain’, l∗,i = 1.
Then the tuning of message-sender network follows the binary cross entropy loss:"
MESSAGE SENDER,0.34782608695652173,LCR = −1 M X i X
MESSAGE SENDER,0.35177865612648224,"j
[li,j · log(pi,j
retain) + (1 −li,j) · log(pi,j
cut)]
(6)"
DECISION MAKER,0.3557312252964427,"3.4
DECISION MAKER
Once the agent receives all the messages, it can decide its own sub-goals of targets based on
its observation, inferred intentions of others and received messages. Therefore, the actor feature
ηi = (Ei, maxj g∗
i,j, P"
DECISION MAKER,0.35968379446640314,"s g∗
s,i) is the input to the actor network. The second term maxj g∗
i,j refers to
the max inferred probability of a target to be chosen by another agent. The third term P"
DECISION MAKER,0.36363636363636365,"s g∗
s,i refers
to the sum of the messages from others, indicating how much certain others infer that agent i should
choose the target. The actor decides its goals gi according to ηi. The centralized critic obtain global
feature (η1, ...ηn) to compute value. The low-level executor πL
i (ai|oi, gi) takes primitive actions to
accomplish the sub-goal. Similar to HiT-MAC (Xu et al., 2020), the high-level planner is guided by
the team reward. The low-level executor is guided by the goal-conditioned reward, measuring the
quality of the achievement of the sub-goals."
TRAINING,0.3675889328063241,"3.5
TRAINING"
TRAINING,0.3715415019762846,"Following the Centralized Training Decentralized Execution (CTDE) paradigm (Rashid et al., 2018),
we also train our model in a centralized manner, i.e., feeding the global observation to the critic and"
TRAINING,0.37549407114624506,Published as a conference paper at ICLR 2022
TRAINING,0.3794466403162055,"using the actual observation and goals of all agents to supervise the ToM Net. The overall network is
trained by Reinforcement Learning (RL) in an end-to-end manner. We adopt standard A2C (Mnih
et al., 2016) as the RL algorithm, while any MARL method with CTDE framework is also applicable,
such as PPO (Schulman et al., 2017; Yu et al., 2021). Besides, as we mentioned in Sec. 3.2, we
provide the actual observation and goals of other agents to supervise the training of the ToM net. We
also apply communication reduction(sec. 3.3) to tune the Message-Sender network."
TRAINING,0.383399209486166,"Training Strategy. We ﬁnd that it is hard for an agent to learn long-term planning from scratch.
Therefore, we set the initial episode length L and discount factor γ to a low value, forcing agents to
learn short-term planning ﬁrst. During training, the episode length and discount factor γ increase
gradually, leading the agents to estimate the value on a longer horizon. Furthermore, we freeze the
ToM net while the other parts of the model (parameterized by θother) are updated through RL. The
reason is that the ToM net infers the goals of others, and the policy network is continuously updated
during RL training. Meanwhile, the output of ToM net is a part of the input to the policy network. If
we train them simultaneously, they are likely to inﬂuence each other in a nested loop. Therefore, we
only collect the ToM inferred data into a batch during RL training. Once the batch is large enough,
we stop RL and start ToM training to minimize ToM loss in Eq. 3. More details about the model and
training strategy can be found in Appendix. B.
4
EXPERIMENTS
We evaluate ToM2C in two typical target-oriented multi-agent tasks: cooperative navigation
(CN) (Lowe et al., 2017) and multi-sensor multi-target coverage (MSMTC) (Xu et al., 2020). CN is
the simplest environment, where targets (landmarks) are static and the agent only needs to choose
one target as sub-goal. In CN, n agents need to occupy n landmarks (targets) so as to minimize the
distances of all landmarks to their nearest agents. All agents share a team reward, which is the sum of
the negative distance of each landmark to its nearest agent. If two agents collide, the team will get a
penalty -1. In MSMTC, n agents need to cooperatively adjust the sensing orientation to maximize
the coverage rate of m moving targets. As is shown in Fig.3, each sensor can only see the targets that
are within the radius and not blocked by any obstacle. The reward is the coverage rate of targets. If
there is no target covered by sensors, we punish the team with a reward r = −0.1. For MSMTC, all
targets are dynamic and each agent chooses multiple targets concurrently as its sub-goals. Therefore,
CN and MSMTC respectively represent two typical settings of ToMAC: static target & one-choice
sub-goal and dynamic targets & multi-choice sub-goal. More details can be found in Appendix. A."
TRAINING,0.38735177865612647,"In the following, we compare ToM2C with baselines in the two scenarios. Then, in MSMTC, we
conduct an ablation study, analyze the communication, and evaluate the scalability of ToM2C.
4.1
COMPARE WITH BASELINES"
TRAINING,0.391304347826087,"𝑆ensor! 𝑆"" 𝑆# 𝑆$"
TRAINING,0.3952569169960474,"𝑇𝑎𝑟𝑔𝑒𝑡%
𝑇"" 𝑇# 𝑇$ 𝑇& 𝑇' 𝑇( 𝑆& 𝑆("
TRAINING,0.39920948616600793,𝑂𝑏𝑠𝑡𝑎𝑐𝑙𝑒)
TRAINING,0.4031620553359684,"Figure 3: An example of the target cov-
erage environment with obstacles."
TRAINING,0.40711462450592883,"We compare our methods with 4 baselines. TarMAC (Das
et al., 2019) is a multi-agent communication method that
requires a broadcast channel. I2C (Ding et al., 2020) pro-
poses an individual communication mechanism, which is
also achieved by ToM2C. HiT-MAC (Xu et al., 2020) is a
hierarchical method that uses a centralized coordinator to
organize the agents. MAPPO (Yu et al., 2021) is a variant
of PPO which is specialized for multi-agent settings, run-
ning without communication. we also implement a global
heuristic search algorithm in MSMTC , as a reference.
This policy searches in one step for the primitive actions
of all the sensors to minimize the sum of minimum angle distance of a target to a sensor. Note that the
heuristic search policy exploits the global state to do the global search, while all the other methods
are restricted to local observation. Therefore the heuristic search policy can serve as a reference
‘upper bound’ that evaluates all the MARL baselines. Please refer to Appendix. C for more details."
TRAINING,0.41106719367588934,"ToM2C is a hierarchical method, while some of the baselines are not. In MSMTC, we let all of the
methods share the low-level rule-based policy adopted in ToM2C. Therefore, these baselines are only
used for training the high-level policy, same as ToM2C. In CN, ToM2C and HiT-MAC share the
low-level policy and other methods keep non-hierarchical."
TRAINING,0.4150197628458498,"Quantitative Results. In CN, as Tab. 1 shows, ToM2C and HiT-MAC are far better than other
methods. Here we also add MADDPG as a baseline method, because this environment is early
adopted in the original paper of MADDPG (Lowe et al., 2017). The performance of HiT-MAC is"
TRAINING,0.4189723320158103,Published as a conference paper at ICLR 2022
M,0.42292490118577075,"0.0M
0.5M
1.0M
1.5M
2.0M
2.5M
3.0M
# of Iterations 40.0 50.0 60.0 70.0 80.0"
M,0.4268774703557312,Rewards
M,0.4308300395256917,"ToM2C
HiT-MAC
I2C
MAPPO
TarMAC
Heuristic Search (a)"
M,0.43478260869565216,"0.0M
0.5M
1.0M
1.5M
2.0M
2.5M
3.0M
# of Iterations 30.0 40.0 50.0 60.0 70.0 80.0"
M,0.43873517786561267,Rewards
M,0.4426877470355731,"ToM2C
ToM2C-Comm
ToM2C-ToM
A2C"
M,0.44664031620553357,"(b)
Figure 4: The learning curve of our method with baselines and reference policies in the MSMTC
scenario. The learning-based methods are all trained in the environment with 4 sensors and 5 targets.
(a) comparing ours with baselines; (b) comparing ours with its ablations."
M,0.4505928853754941,Table 1: Quantitative Results in Cooperative Navigation (n = 7).
M,0.45454545454545453,"ToM2C
TarMAC
I2C
HiT-MAC
MADDPG
MAPPO"
M,0.45849802371541504,"Reward
-0.79 ± 0.39
-2.14 ± 0.24
-1.59 ± 0.40
-0.61 ± 0.14
-2.75 ± 0.61
-2.47 ± 0.21"
M,0.4624505928853755,"slightly better than ToM2C. It is because the hierarchical structures of ToM2C and HiT-MAC are
different. For HiT-MAC, there is a centralized coordinator that collects all the local observations and
assigns the goals for each agent. On the contrary, ToM2C agents run in a fully decentralized manner.
Agents only have access to local observations and decide their own goals. The hierarchy structure is
only used for separate goal selections and primitive actions. Due to the centralized manner, HiT-MAC
is likely to degrade in more complex scenarios (e.g., large-scale MSMTC). Moreover, HiT-MAC is
also more costly in communication (Sec. 4.3) and weaker in scalability (Sec. 4.4). Therefore, ToM2C
is not inferior to HiT-MAC."
M,0.466403162055336,"In MSMTC, as Fig.4(a) shows, ToM2C achieves the second highest reward (75) in the setting of 4
sensors and 5 targets, only lower than the searching policy (80). The reward performance of I2C,
MAPPO and HiT-MAC are all around 66. TarMAC reaches a reward of 71, which is superior to the
other three baselines. This could be attributed to the broadcast channel adopted by TarMAC. Since
all the methods leverage the hierarchy structure, the superiority of ToM2C comes from the model and
training strategies."
ABLATION STUDY,0.47035573122529645,"4.2
ABLATION STUDY"
ABLATION STUDY,0.4743083003952569,"We conduct this study to evaluate the contribution of two key components in our model: ToM net and
Message sender. The ToM2C-Comm model abandons communication, so the actor makes decisions
only based on local observation and inferred goals of others. The ToM2C-ToM abandons ToM
net, but keeps the Messages sender. However, as explained before, the local graph node feature is
computed based on the ToM net output. To deal with this problem, we use the encoded observation
Ej to replace the original node feature ui,j. In this way, the n local graphs degrade into one global
graph, so the ToM2C-ToM model actually breaks the local communication mechanism. If we remove
both the ToM net and Message sender, ToM2C will become pure A2C. We show in Fig.4(b) that if
we abandon one of the key components, the performance will drop. Considering that ToM2C-Comm
outperforms ToM2C-ToM and ToM net is actually essential for communication, we argue that ToM
net mainly contributes to our method."
COMMUNICATION ANALYSIS,0.4782608695652174,"4.3
COMMUNICATION ANALYSIS
We compare our method with several candidates in regard to communication expenses. There are
2 metrics here: the number of communication edges and communication bandwidth. The latter
metric considers both the count of edges and the length of a single message. TarMAC utilizes
a broadcast channel, so there are two communication edges between each pair. The communi-
cation in HiT-MAC is between the executors and the coordinator. ToM2C w/o CR refers to the
ToM2C model without communication reduction. Apparently, communication bandwidth is more
signiﬁcant in real applications, so we place the ﬁgure of communication bandwidth in the main text."
COMMUNICATION ANALYSIS,0.48221343873517786,Published as a conference paper at ICLR 2022
COMMUNICATION ANALYSIS,0.48616600790513836,"TarMAC
I2C
HiT-MAC
ToM2C w/o CR
ToM2C
0 200 400 600 800 1000 1200 1400"
COMMUNICATION ANALYSIS,0.4901185770750988,Communication bandwidth (a)
COMMUNICATION ANALYSIS,0.49407114624505927,"TarMAC
I2C
HiT-MAC
ToM2C w/o CR
ToM2C
0 50 100 150 200 250 300 350 400"
COMMUNICATION ANALYSIS,0.4980237154150198,Communication bandwidth
COMMUNICATION ANALYSIS,0.5019762845849802,"(b)
Figure 5: The communication bandwidth of models in (a)
Cooperative navigation and (b) MSMTC."
COMMUNICATION ANALYSIS,0.5059288537549407,"The statistics of communication edges
can be found in Appendix. D. As is
shown in Fig.5, the communication
bandwidth of ToM2C and ToM2C
without CR are much lower than Tar-
MAC, I2C and HiT-MAC. It is be-
cause in ToM2C the message is only
the inferred goals, while TarMAC,
I2C and HiT-MAC have to send the
local observation. Therefore, the sin-
gle message in ToM2C is much simpler than that of TarMAC, I2C and HiT-MAC. As a result, the
communication cost of ToM2C is extremely less than existing methods."
SCALABILITY,0.5098814229249012,"4.4
SCALABILITY"
SCALABILITY,0.5138339920948617,"2
3
4
5
6
7
8
9
10
# of targets"
SCALABILITY,0.5177865612648221,"2
3
4
5
6
7
8
9
10
# of cameras ToM2C"
SCALABILITY,0.5217391304347826,"2
3
4
5
6
7
8
9
10
# of targets"
SCALABILITY,0.525691699604743,"2
3
4
5
6
7
8
9
10
# of cameras"
SCALABILITY,0.5296442687747036,HiT-MAC 0.64 0.72 0.80 0.88 0.96 1.04
SCALABILITY,0.5335968379446641,"Figure 6: Analyzing the scalability in scenarios with different
sizes of cameras and targets. The left heatmap shows RToM.
The right heatmap shows RHM."
SCALABILITY,0.5375494071146245,"We evaluate the scalability of our
method to a different number of sen-
sors and targets in the MSMTC task.
Note that the model is only trained in
the setting of 4 sensors and 5 targets,
so this could be regarded as zero-shot
transfer. Since the difﬁculty of the
task changes with the scale of cam-
eras and targets, we make use of the
heuristic search policy, whose target
coverage rate in each setting can serve
as a referential value. We adopt such
a metric that divides the coverage rate of the model by the coverage rate of the search policy:
RToM
p,q
= CToM
p,q /CHS
p,q, where CToM
p,q
refers to the coverage rate of ToM2C in the setting of p cameras
and q targets and CHS
p,q refers to the coverage rate of heuristic search. In this way, we may reduce the
ﬂuctuation induced by the change of task difﬁculty. To better show the advantage of our method,
we further test the scalability of HiT-MAC. The experiments are conducted in a total of 81 settings,
where cameras and targets both range from 2 to 10. We compute RToM and RHM(ratio of HiT-MAC
and heuristic search) in all the settings and draw the heatmap in Fig. 6. It is shown in the left ﬁgure
that the ratio RToM stays near 1, which means that the performance of ToM2C is close to heuristic
search in all the settings. Furthermore, if we compare the heatmap of ToM2C and HiT-MAC, it is
obvious that ToM2C is more stable than HiT-MAC when transferred to different scales. In this way,
we show that ToM2C has a rather strong generalization."
CONCLUSION AND DISCUSSION,0.541501976284585,"5
CONCLUSION AND DISCUSSION"
CONCLUSION AND DISCUSSION,0.5454545454545454,"In this work, we study the target-oriented multi-agent cooperation (ToMAC) problem. Inspired by
the cognitive study in Theory of Mind (ToM), we propose an effective Target-orient Multi-agent
Cooperation and Communication mechanism (ToM2C) for ToMAC. For each agent, ToM2C is
composed of an observation encoder, ToM net, message sender, and decision maker. The ToM net
is designed for estimating the observation and inferring the goals (intentions) of others. It is also
deeply used by the message sender and decision maker. Besides, a communication reduction method
is proposed to further improve the efﬁciency of the communication. Empirical results demonstrated
that our method can deal with challenging cases and outperform the state-of-the-art MARL methods."
CONCLUSION AND DISCUSSION,0.549407114624506,"Although impressive improvements have been achieved, there are still some limitations of this work
leaving for addressed by future works. 1) It is necessary to further evaluate the model on other
applications. As each component in the model is general, we are conﬁdent to apply ToM2C to
other target-oriented tasks (e.g. SMAC (Samvelyan et al., 2019)) in the future. It is also interesting
to extend ToM2C to non-target environments (e.g. Hanabi (Bard et al., 2020)), which requires a
further deﬁnition of sub-goals or automatic goal generation. 2) Besides, the communication reduction
method can also be further optimized, as the pseudo labels we generated for communication reduction
are noisy in some cases."
CONCLUSION AND DISCUSSION,0.5533596837944664,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5573122529644269,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.5612648221343873,"The code is available at https://github.com/UnrealTracking/ToM2C. The details of the environments
can be found in Appendix. A. The hyper-parameters used in ToM2C are listed in Tab. 2."
REPRODUCIBILITY STATEMENT,0.5652173913043478,ACKNOWLEDGEMENTS
REPRODUCIBILITY STATEMENT,0.5691699604743083,"This work was supported by MOST-2018AAA0102004, NSFC-62061136001, China National Post-
doctoral Program for Innovative Talents (Grant No. BX2021008), Qualcomm University Research
Grant."
REFERENCES,0.5731225296442688,REFERENCES
REFERENCES,0.5770750988142292,"Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio
Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A
new frontier for ai research. Artiﬁcial Intelligence, 280:103216, 2020."
REFERENCES,0.5810276679841897,"Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray kavukcuoglu.
Interaction networks for learning about objects, relations and physics. In Proceedings of the 30th
International Conference on Neural Information Processing Systems, pp. 4509–4517, 2016."
REFERENCES,0.5849802371541502,"Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of
decentralized control of markov decision processes. Mathematics of operations research, 27(4):
819–840, 2002."
REFERENCES,0.5889328063241107,"Micah Carroll, Rohin Shah, Mark K Ho, Tom Grifﬁths, Sanjit Seshia, Pieter Abbeel, and Anca
Dragan. On the utility of learning about humans for human-ai coordination. Advances in Neural
Information Processing Systems, 32:5174–5185, 2019."
REFERENCES,0.5928853754940712,"Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078, 2014."
REFERENCES,0.5968379446640316,"Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle
Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine
Learning, pp. 1538–1546. PMLR, 2019."
REFERENCES,0.6007905138339921,"gang Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for
multi-agent cooperation. In Advances in Neural Information Processing Systems, volume 33, pp.
22069–22079, 2020."
REFERENCES,0.6047430830039525,"Evren Etel and Virginia Slaughter. Theory of mind and peer cooperation in two play contexts. Journal
of Applied Developmental Psychology, 60:87–95, 2019."
REFERENCES,0.6086956521739131,"Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, 2018."
REFERENCES,0.6126482213438735,"Kanishk Gandhi, Gala Stojnic, Brenden M Lake, and Moira R Dillon. Baby intuitions benchmark
(bib): Discerning the goals, preferences, and actions of others. arXiv preprint arXiv:2102.11938,
2021."
REFERENCES,0.616600790513834,"Aditya Grover, Maruan Al-Shedivat, Jayesh Gupta, Yuri Burda, and Harrison Edwards. Learning
policy representations in multiagent systems. In International Conference on Machine Learning,
pp. 1802–1811. PMLR, 2018."
REFERENCES,0.6205533596837944,"He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daumé III. Opponent modeling in deep rein-
forcement learning. In International Conference on Machine Learning, pp. 1804–1813. PMLR,
2016."
REFERENCES,0.6245059288537549,"Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 2961–2970. PMLR, 2019."
REFERENCES,0.6284584980237155,Published as a conference paper at ICLR 2022
REFERENCES,0.6324110671936759,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016."
REFERENCES,0.6363636363636364,"Dong-Ki Kim, Miao Liu, Shayegan Omidshaﬁei, Sebastian Lopez-Cot, Matthew Riemer, Golnaz
Habibi, Gerald Tesauro, Sami Mourad, Murray Campbell, and Jonathan P How. Learning hierar-
chical teaching in cooperative multiagent reinforcement learning. In AAMAS, 2020."
REFERENCES,0.6403162055335968,"Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu Qiao, and Yizhou Wang. Pose-assisted multi-
camera collaboration for active object tracking. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 34, pp. 759–766, 2020a."
REFERENCES,0.6442687747035574,"Yunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, and Animesh Garg. Causal discovery
in physical systems from videos. Advances in Neural Information Processing Systems, 33, 2020b."
REFERENCES,0.6482213438735178,"Terence X Lim, Sidney Tio, and Desmond C Ong. Improving multi-agent cooperation using theory
of mind. arXiv preprint arXiv:2007.15703, 2020."
REFERENCES,0.6521739130434783,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379–6390, 2017."
REFERENCES,0.6561264822134387,"Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016."
REFERENCES,0.6600790513833992,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928–1937, 2016."
REFERENCES,0.6640316205533597,"Aviv Netanyahu, Tianmin Shu, Boris Katz, Andrei Barbu, and Joshua B Tenenbaum.
Phase:
Physically-grounded abstract social events for machine social perception.
arXiv preprint
arXiv:2103.01933, 2021."
REFERENCES,0.6679841897233202,"David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and
brain sciences, 1(4):515–526, 1978."
REFERENCES,0.6719367588932806,"Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja
Fidler, and Antonio Torralba. Watch-and-help: A challenge for social perception and human-ai
collaboration. In International conference on learning representations, 2021."
REFERENCES,0.6758893280632411,"Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick.
Machine theory of mind. In International conference on machine learning, pp. 4218–4227. PMLR,
2018."
REFERENCES,0.6798418972332015,"Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself
in multi-agent reinforcement learning. In International Conference on Machine Learning, pp.
4257–4266. PMLR, 2018."
REFERENCES,0.6837944664031621,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018."
REFERENCES,0.6877470355731226,"Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019."
REFERENCES,0.691699604743083,"Alan G Sanfey, Claudia Civai, and Peter Vavra. Predicting the other in cooperative interactions.
Trends in cognitive sciences, 19(7):364–365, 2015."
REFERENCES,0.6956521739130435,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.6996047430830039,"Itai Sher, Melissa Koenig, and Aldo Rustichini. Children’s strategic theory of mind. Proceedings of
the National Academy of Sciences, 111(37):13307–13312, 2014."
REFERENCES,0.7035573122529645,Published as a conference paper at ICLR 2022
REFERENCES,0.7075098814229249,"Tianmin Shu and Yuandong Tian.
M3rl: Mind-aware multi-agent management reinforcement
learnings. In International conference on learning representations, 2019."
REFERENCES,0.7114624505928854,"Tianmin Shu, Abhishek Bhandwaldar, Chuang Gan, Kevin A Smith, Shari Liu, Dan Gutfreund,
Elizabeth Spelke, Joshua B Tenenbaum, and Tomer D Ullman. Agent: A benchmark for core
psychological reasoning. arXiv preprint arXiv:2102.12321, 2021."
REFERENCES,0.7154150197628458,"Michael Shum, Max Kleiman-Weiner, Michael L Littman, and Joshua B Tenenbaum. Theory of
minds: Understanding behavior in groups through inverse planning. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 33, pp. 6163–6170, 2019."
REFERENCES,0.7193675889328063,"Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Individualized controlled continuous com-
munication model for multiagent cooperative and competitive tasks. In International conference
on learning representations, 2019."
REFERENCES,0.7233201581027668,"Sainbayar Sukhbaatar, arthur szlam, and Rob Fergus. Learning multiagent communication with
backpropagation. 29, 2016."
REFERENCES,0.7272727272727273,"Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zam-
baldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-
decomposition networks for cooperative multi-agent learning based on team reward. In AAMAS,
pp. 2085–2087, 2018."
REFERENCES,0.7312252964426877,"Michael Tomasello. A natural history of human thinking. Harvard University Press, 2014."
REFERENCES,0.7351778656126482,"Socially Interactive Agents Track, Jan Pöppel, and Stefan Kopp. Satisﬁcing models of bayesian
theory of mind for explaining behavior of differently uncertain agents. In Proceedings of the 17th
International Conference on Autonomous Agents and Multiagent Systems, Stockholm, Sweden, pp.
10–15, 2018."
REFERENCES,0.7391304347826086,"Elio Tuci, Muhanad HM Alkilabi, and Otar Akanyeti. Cooperative object transport in multi-robot
systems: A review of the state-of-the-art. Frontiers in Robotics and AI, 5:59, 2018."
REFERENCES,0.7430830039525692,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017."
REFERENCES,0.7470355731225297,"Sarah A Wu, Rose E Wang, James A Evans, Joshua B Tenenbaum, David C Parkes, and Max
Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent collaboration.
Topics in Cognitive Science, 13(2):414–432, 2021."
REFERENCES,0.7509881422924901,"Jing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent coordination for enhancing target
coverage in directional sensor networks. In Advances in Neural Information Processing Systems,
volume 33, pp. 10053–10064, 2020."
REFERENCES,0.7549407114624506,"Jiachen Yang, Igor Borovikov, and Hongyuan Zha. Hierarchical cooperative multi-agent reinforce-
ment learning with skill discovery. In AAMAS, 2019."
REFERENCES,0.758893280632411,"Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021."
REFERENCES,0.7628458498023716,"Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. Ad-vat+: An asymmetric
dueling mechanism for learning and understanding visual active tracking. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 43(5):1467–1482, 2021a."
REFERENCES,0.766798418972332,"Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. Towards distraction-robust
active visual tracking. In International Conference on Machine Learning, pp. 12782–12792.
PMLR, 2021b."
REFERENCES,0.7707509881422925,Published as a conference paper at ICLR 2022
REFERENCES,0.7747035573122529,"A
DETAILS OF ENVIRONMENT"
REFERENCES,0.7786561264822134,"A.1
MULTI-SENSOR TARGET COVERAGE"
REFERENCES,0.782608695652174,"The multi-sensor multi-target tracking environment is developed based on the previous version
in HiT-MAC(Xu et al., 2020), and it inherits most of the characters. It is a 2D environment that
simulates the real target coverage problem in directional sensor networks. Each sensor can only
see the targets that are within the radius and not blocked by any obstacle. There 2 types of target:
destination-navigation and random walking. The former one moves in the shortest path to reach a
previously sampled destination. The latter one moves randomly at each time step. At the beginning
of each episode, the location of sensors, targets and obstacles are randomly sampled. Besides, the
types of the targets are also sampled according to a pre-deﬁned probability. The length of an episode
is 100 steps."
REFERENCES,0.7865612648221344,"Observation Space. At each time step, the local observation oi is a set of agent-target pairs:
(oi,1, ...oi,m). If target q is visible to agent i, then oi,q = (i, q, di,q, αi,q), where di,q is the distance
and αi,q is the relative angle. If target q is not visible to i, then oi,q = (0, 0, 0, 0). Therefore,
oi ∈Rm×4."
REFERENCES,0.7905138339920948,"Action Space. The primitive action for a sensor is to stay or rotate +5/-5 degrees. For our method,
the high-level action is the chosen goals gi, which is a binary vector of length m. gi,q = 1 means the
agent chooses target q as one of its goals. gi,q = 0 means not. Although the low-level executor can
be trained by reinforcement learning (RL) as HiT-MAC, we ﬁnd a simple rule-based policy can also
work well in most cases. Therefore we only train the high-level policy."
REFERENCES,0.7944664031620553,"Reward. Reward is the coverage rate of targets: r = 1 m
P"
REFERENCES,0.7984189723320159,"q Iq, where Iq = 1 if q is covered by any
sensor. If there is no target covered by sensors, we punish the team with a reward r = −0.1."
REFERENCES,0.8023715415019763,"Compared with the version in HiT-MAC, we add some new features to make it more complex and
realistic:"
REFERENCES,0.8063241106719368,"• First, HiT-MAC assumes that all the agents can see all the targets in the environment, which
is unreasonable for real applications. Therefore, we change this setting into a Dec-POMDP,
in which an agent can only obtain the information in its local observation. For those targets
that are out of view, the corresponding observation will be a zero vector."
REFERENCES,0.8102766798418972,"• Secondly, we add another kind of objects, obstacles, into the environment. The obstacles are
all circles in this 2D plain simulator, varying in the radius. The targets within the observation
radius will still be invisible if it is shadowed by an obstacle."
REFERENCES,0.8142292490118577,"• Finally, in the original environment all the targets move in a goal-oriented manner. The
targets sample their destinations at the beginning of an episode and navigate themselves to
the destinations. Nevertheless, not all targets in real world follow the same action pattern.
Therefore, we ﬁll the environment with a mixed-type population of targets. The target can
either be goal-oriented or random-walking. When the target is random-walking, it will
randomly sample a primitive action to take at each step. In this way, the movement of targets
is harder to predict, raising the difﬁculty of the planning. A B
C D 1 2 3 A
B C
D 1 2 3
4 5 6 7 A B 1 2 3 4 5 A
B C D E F 1 2 3 4 5 6 7 8 9 10 11
12"
REFERENCES,0.8181818181818182,"Figure 7: Snapshots of the multi-sensor multi-target tracking environments with different scales.
From the upper left ﬁgure, in the clockwise direction, they are 4 vs 3, 4 vs 7, 2 vs 5, and 6 vs 12,
respectively."
REFERENCES,0.8221343873517787,Published as a conference paper at ICLR 2022
REFERENCES,0.8260869565217391,"A.2
COOPERATIVE NAVIGATION"
REFERENCES,0.8300395256916996,"The environment is similar to the one used in MADDPG (Lowe et al., 2017). n agents need to
cooperatively reach n landmarks. In this task, we set n = 7. The length of each episode is 100 steps.
For hierarchical methods, the high-level episode length is set to K = 10. Thus, each high-level step
consists of 10 low-level steps."
REFERENCES,0.83399209486166,"Observation Space. At each time step, the observation oi is the concatenation of self location, self
velocity, location and velocity of visible agents, location of visible landmarks."
REFERENCES,0.8379446640316206,"Action Space. The primitive action for an agent is to move up/down/left/right. For our method, the
high level action is the chosen goal gi. Similar to the multi-sensor target coverage task, ToM2C and
HiT-MAC again make use of a rule-based low-level policy."
REFERENCES,0.841897233201581,"Reward. The team reward is the sum of the negative distance of each landmark to its nearest agent.
If two agents collide, the team will get a penalty r = −1."
REFERENCES,0.8458498023715415,"B
TOM2C DETAILS"
REFERENCES,0.849802371541502,"Network Architecture and Hyper-parameters for ToM2C. The observation encoder consists of
2-layer multilayer perceptron (MLP) and an attention module: att1. The ToM net consists of a Gated
Recurrent Unit (GRU) and 2-layer MLP. The message sender is a Graph Neural Network (GNN) and
the actor consists of one fully connected layer. The critic consists of an attention module att2 that
can handle different numbers of agents. As mentioned before, the basic RL training algorithm is
A2C, and the hyper-parameters are detailed in Tab. 2."
REFERENCES,0.8537549407114624,Table 2: Hyper-parameters for ToM2C
REFERENCES,0.857707509881423,"Hyper-parameters
#
Description"
REFERENCES,0.8616600790513834,"GRU hidden units
32
the # of hidden units for GRU
att1 hidden units
64
the # of hidden units for att1
att2 hidden units
192
the # of hidden units for att1
max steps
3M
maximum environment steps sampled in workers
episode length
100
maximum time steps per episode
discount factor
0.9
discount factor for rewards
entropy weight
0.005
parameter for entropy regularization
learning rate
1e-3
learning rate for all networks
workers
6
the # of workers for sampling
update frequency
20
the network updates every # steps in A2C
ToM Frozen
5
the ToM net is frozen for every # times of RL training
gamma rate
0.002
the increasing rate of discounting factor γ"
REFERENCES,0.8656126482213439,"Training Strategy. There are two training strategies adopted to accelerate training and stabilize the
result. As mentioned in Sec.3.5, one is to increase episode length L and γ factor gradually during
training, the other one is to split the optimization of the ToM and RL model."
REFERENCES,0.8695652173913043,"In this paper, we propose this curriculum learning strategy that gradually increases episode length
L and discounting factor γ. Usually, the discounting factor γ is set larger than 0.9 to encourage
long-term planning in RL algorithms. Furthermore, the length of an episode is usually determined
by the environment. We notice that if using the default hyper-parameters, the agents are sample
inefﬁcient and unstable while learning. In our experiments, we set L = 20 and γ = 0.1 initially.
After 2000 episodes of warm-up, the γ factor will be updated according to a pre-set rate β. Each time
the network is optimized through reinforcement learning, γ = γ ∗(1 + β), where β = 0.002 in this
paper. Simultaneously, the episode length L is updated with γ. In fact, L = ⌊γ+0.1"
REFERENCES,0.8735177865612648,"0.2 ⌋× 20. In the
end, γ = 0.9 and L = 100. By doing so, the agents learn short-term planning ﬁrst, and then adapt to
a longer horizon. We ﬁnd in experiments that such strategy accelerates the training process, leading
to a faster convergence and a better performance."
REFERENCES,0.8774703557312253,Published as a conference paper at ICLR 2022
REFERENCES,0.8814229249011858,"Furthermore, we separate the optimizations of the ToM and RL model in implementation. Before
the training process starts, the parameters of our model are split into two parts: θT oM and θother.
Each part is optimized individually by a different optimizer. Since we adopt A2C as the base RL
training algorithm, we collect trajectories data from different worker processes and send them to the
training process when all the running episodes end. After that, θother is optimized with regard to
the A2C loss. Meanwhile, the trajectories data for ToM training are saved instead of being used for
training ToM net immediately. In this way, the ToM net is ‘frozen’. θT oM will be optimized with
regard to ToM loss after θother has been optimized for TF times. Here we choose TF = 5. Just like
the discussion before, the separation of ToM and RL training avoids the nested loop of inﬂuence
among the ToM net and the policy network."
REFERENCES,0.8853754940711462,"The environment and model are implemented in Python. The model is built on PyTorch and is trained
on a machine with 7 Nvidia GPUs (Titan Xp) and 72 Intel CPU Cores."
REFERENCES,0.8893280632411067,"C
BASELINES"
REFERENCES,0.8932806324110671,"Heuristic Search Method. To evaluate the performance of our ToM2C model, we choose to
implement a heuristic search policy to serve as a reference. This search policy is applied to select low-
level sensor action(Stay, Turn Left/Right). At each step, the policy searches all the 3n possibilities of
combination of actions, where n is the number of sensors. The goal is to ﬁnd the action combination
that minimizes the angle distance of targets to sensors. Speciﬁcally, we denote the angle distance
of target j to sensor i as αij. Then the objective is to minimize Pm
j=1 mini{αij}. It is obvious that
such searching policy only considers one step, thus not the optimal policy. However, we show that
this naive heuristic search can reach 80% target coverage. As a result, it can serve as a reference
‘upper bound’ that evaluates all the MARL baselines."
REFERENCES,0.8972332015810277,"MARL Baselines. The code of HiT-MAC and I2C are from their ofﬁcial repositories. We follow the
default hyper-parameters in their code, except that we change the learning rate, discounting factor γ
and episode length to be the same as ToM2C. TarMAC is implemented on our own because no ofﬁcial
code is released. Moreover, HiT-MAC is a hierarchical method, so we simply train the high-level
coordinator and use the same rule-based low-level policy utilized in ToM2C. On the other hand, I2C
is not a hierarchical method and it is not target-oriented. As a result, we concatenate all the target
information into one vector as the observation for I2C. The action space is modiﬁed as the set of
choice of all the targets, so the space size is 2m, where m is the number of targets. In this way, the
output action of I2C agent is the selection of goal targets, same as HiT-MAC and ToM2C. Once the
goal target is selected, the primitive actions will be chosen by the rule-based policy."
REFERENCES,0.9011857707509882,"D
QUANTITATIVE RESULTS"
REFERENCES,0.9051383399209486,"For MSMTC task, We list the coverage rate achieved by different methods in Tab. 3. The mean and
standard deviation are computed based on the data collected in 1000 episodes. The performance in
cooperative navigation is listed in Tab. 1."
REFERENCES,0.9090909090909091,Table 3: Coverage Rate in 4 sensors vs 5 targets scenario
REFERENCES,0.9130434782608695,"Methods
Coverage Rate(%)↑"
REFERENCES,0.9169960474308301,"A2C
38.44 ± 0.54
HiT-MAC
61.48 ± 1.45
I2C
66.29 ± 1.40
MAPPO
66.87 ± 0.69
ToM2C-ToM
67.66 ± 0.63
TarMAC
70.56 ± 0.81
ToM2C-Comm
71.61 ± 0.31
ToM2C(Ours)
75.38 ± 0.57"
REFERENCES,0.9209486166007905,"Apart from the coverage rate, we analyze the communication efﬁciency of different methods. There
are 2 metrics introduced in this paper. Communication edges refer to the count of directed communi-"
REFERENCES,0.924901185770751,Published as a conference paper at ICLR 2022
REFERENCES,0.9288537549407114,Table 4: Communication Statistics of MSMTC
REFERENCES,0.932806324110672,"Methods
Communication Edges
Communication Bandwidth↓"
REFERENCES,0.9367588932806324,"TarMAC
12.00 ± 0.00
384.00 ± 0.00
I2C
7.19 ± 0.13
258.84 ± 4.16
HiT-MAC
8.00 ± 0.00
164.00 ± 0.00
ToM2C w/o CR
9.39 ± 0.19
46.93 ± 0.97
ToM2C(Ours)
6.03 ± 0.20
30.15 ± 1.02"
REFERENCES,0.9407114624505929,Table 5: Communication Statistics of CN
REFERENCES,0.9446640316205533,"Methods
Communication Edges
Communication Bandwidth↓"
REFERENCES,0.9486166007905138,"TarMAC
42.00 ± 0.00
1344.00 ± 0.00
I2C
14.21 ± 1.65
511.41 ± 59.44
HiT-MAC
14.00 ± 0.00
231.00 ± 0.00
ToM2C w/o CR
42.00 ± 0.00
126.00 ± 0.00
ToM2C(Ours)
22.48 ± 0.91
67.44 ± 2.74"
REFERENCES,0.9525691699604744,"cation pairs. One edge from i to j means that agent i sends a message to agent j. Communication
bandwidth refers to the total volume of messages. As we mentioned in the experiment section, it is
the volume of messages that has a decisive effect on the cost of communication. Since the messages
are all ﬂoat-type vectors, we use the length of a message instead of the number of bits to represent the
volume of a single message. For I2C, the message from agent i is the local observation oi, containing
the information of all the targets. For HiT-MAC, communication happens between the executors and
the coordinator. The executors send their local observation to the coordinator, and the coordinator
returns the goal assignment. For ToM2C w/o CR and ToM2C, the message is simply the inferred
goals of the receiver. ToM2C w/o CR means that the trained ToM2C model is not further optimized
to reduce communication."
REFERENCES,0.9565217391304348,"The experiments are conducted in the 4-sensors-and-5-targets scenario and 7v7 cooperative navigation.
As shown in Tab. 4 and Tab. 5, our method achieves the lowest communication cost."
REFERENCES,0.9604743083003953,"ToM Accuracy. We further tested the accuracy of ToM prediction, including goal inference and
observation estimation. In MSMTC, the accuracy of goal inference is 80.2% ± 1.4%. The accuracy
of observation estimation is 98.1% ± 0.3%. For comparison, the accuracy of random prediction are
50.1% ± 0.3% and 50.4% ± 0.6% respectively."
REFERENCES,0.9644268774703557,"Pose Accessibility. In this paper, we enable the agents to have access to the poses of all the other
agents, while restricting the observability of targets. To further validate the performance of ToM2C
in the case of restricted pose access, we add an observation distance between agents. If agent j is
out of the view of agent i, we mask the ToM inference and communication to agent j from agent i.
Under such partially observable agent poses, we test our model in MSMTC. The average rewards
among 100 episodes are listed in Tab. 6, comparing with the case of using all agent poses. It is shown
that the performance of ToM2C is still comparable to the original version (use all agent poses) when
the poses of agents are partially observable."
REFERENCES,0.9683794466403162,Table 6: Comparison between partially and fully observable poses in MSMTC.
REFERENCES,0.9723320158102767,"sensors vs. targets
Partially observable agent poses
All agent poses"
REFERENCES,0.9762845849802372,"10 vs. 10
75.96 ± 0.81
77.26 ± 0.52
10 vs. 5
85.06 ± 0.32
84.69 ± 0.77
4 vs. 5
74.73 ± 0.95
75.38 ± 0.57"
REFERENCES,0.9802371541501976,Published as a conference paper at ICLR 2022
REFERENCES,0.9841897233201581,"E
DEMO SEQUENCE"
REFERENCES,0.9881422924901185,"To better understand the learned behavior, we render the target coverage environment and show a
typical demo sequence in Fig. 8. It consists of 4 consecutive keyframes in one episode. The arrows
between sensors indicate communication connections. Note that communication only happens every
10 steps. In step 16, sensor D can track target 1, 2 and 4. However when it comes to step 22, sensor
D can no longer track all the three targets, so it starts to hesitate about which targets to track. Then
in step 24, A sends a message to D, and D inferred that A would track target 1 and 2. Therefore, it
re-plans its own goal to be target 4. In the end, we can see that sensor D really abandons target 1 and
2, and focuses on target 4."
REFERENCES,0.9920948616600791,"Step 16
Step 22
Step 24
Step 29"
REFERENCES,0.9960474308300395,"Figure 8: An exemplar sequence in 4 sensors and 5 targets MSMTC environment. The gray circle
indicates the obstacle. The arrows are rendered as solid only when the communication happens, and
transparent at other times."
