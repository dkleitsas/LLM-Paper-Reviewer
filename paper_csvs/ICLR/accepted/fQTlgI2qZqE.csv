Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0012531328320802004,"The ability of discovering feature interactions in a black-box model is vital to
explainable deep learning. We propose a principled, global interaction detection
method by casting our target as a multi-arm bandits problem and solving it swiftly
with the UCB algorithm. This adaptive method is free of ad-hoc assumptions
and among the cutting-edge methods with outstanding detection accuracy and
stability. Based on the detection outcome, a lightweight and interpretable deep
learning model (called ParaACE) is further built using the alternating conditional
expectation (ACE) method. Our proposed ParaACE improves the prediction per-
formance by 26% and reduces the model size by 100+ times as compared to
its Teacher model over various datasets. Furthermore, we show the great po-
tential of our method for scientiﬁc discovery through interpreting various real
datasets in the economics and smart medicine sectors. The code is available at
https://github.com/zhangtj1996/ParaACE."
INTRODUCTION,0.002506265664160401,"1
INTRODUCTION"
INTRODUCTION,0.0037593984962406013,"Explainable machine learning is an active research ﬁeld that focuses on providing interpretable models,
transparent explanations, and conﬁdent decisions to practical AI systems. Investigating feature
interaction is vital to model interpretability. Interaction detection should be able to reveal which
subset of features inﬂuence the output jointly, and what the corresponding nonlinear transformation is.
We aim to ﬁnd the underlying interactions from data, such that we can interpret the model properly.
To go one step further, we hope that new models can be built with the aid of the detected interaction
knowledge economically to avoid heavy parameterization."
INTRODUCTION,0.005012531328320802,"In this paper, we ﬁrst restrict ourselves to interaction detection. A novel method is derived directly
from the most acknowledged deﬁnition of feature interaction (Friedman et al., 2008). We further
apply the obtained interaction knowledge to design a transparent and reﬁned neural network (NN).
This approach ﬁts well into the data science life cycle (Yu & Kumbier, 2019), which was applied, for
instance in Tsang et al. (2020a), successfully for interpretable recommender system design."
INTRODUCTION,0.006265664160401002,"The main contributions of this paper include: 1) a fast and principled interaction detection method,
2) a lightweight and interpretable neural network model that can surpass its Teacher, 3) thorough
theoretical analysis and performance evaluations with real datasets. More details are given below."
INTRODUCTION,0.007518796992481203,"1. We propose a generic interaction detection method based on a global statistical metric,
namely the expected Hessian, Hij := Ex
h
∂2F (x)
∂xi∂xj"
INTRODUCTION,0.008771929824561403,"i
. Notably, our method is model-agnostic
and applicable to any pre-trained learning model, F(x), being for instance, a deep neural
network model or a tree model. It is also ﬂexible to use for multi-way interaction detection.
2. To speed up the detection process, we evaluate the expected Hessian via adaptive sampling
using the Upper Conﬁdence Bound (UCB) algorithm (Lai & Robbins, 1985), which can sig-
niﬁcantly reduce the computational complexity. Besides, thorough analysis of the proposed
interaction detection method are conducted.
3. Using the detected interaction pairs, we further design a compressed but interpretable
Student model which can surpass its Teacher by 26% in terms of data ﬁtting performance"
INTRODUCTION,0.010025062656641603,Published as a conference paper at ICLR 2022
INTRODUCTION,0.011278195488721804,"averaged over various datasets. The compressed model reduces its size over 100 times
compared to the baseline fully-connected, over-parameterized neural network (OverparaFC)."
WE DEMONSTRATE THE LINKAGES BETWEEN OUR COMPRESSED MODEL AND THE CLASSIC ALTERNATING,0.012531328320802004,"4. We demonstrate the linkages between our compressed model and the classic alternating
conditional expectation (ACE) model (Breiman & Friedman, 1985)."
WE CONDUCT LARGE-SCALE PERFORMANCE EVALUATIONS AND FURTHER EXPLAIN THE OBTAINED MODEL,0.013784461152882205,"5. We conduct large-scale performance evaluations and further explain the obtained model
interpretability with some real datasets."
WE CONDUCT LARGE-SCALE PERFORMANCE EVALUATIONS AND FURTHER EXPLAIN THE OBTAINED MODEL,0.015037593984962405,"The remainder of this paper is organized as follows. Section 2 introduces all related works. Our
proposed interaction detection method is introduced in Section 3. By exploiting the detection
outcome, a new variant of lightweight and interpretable deep learning model is introduced in Section 4.
Experimental results are given in Section 5. Finally, we conclude the paper in Section 6."
RELATED WORKS,0.016290726817042606,"2
RELATED WORKS"
RELATED WORKS,0.017543859649122806,"Interaction Detection: Early works adopt pure statistics for detecting feature interactions, and
representatives include ANOVA and GUIDE (Wonnacott & Wonnacott, 1990; Fisher, 1992; Loh,
2002). These works have motivated a plethora of new methods with the aim to enhance the detection
accuracy and/or efﬁciency. The ﬁrst class of methods centered around the GA2M and tree models, see
for instance Lou et al. (2013); Sorokina et al. (2008); Friedman et al. (2008); Lundberg et al. (2020).
The second class of methods were built on the so-called factorization machines (Rendle, 2010) as
well as its new variants (Xiao et al., 2017; Song et al., 2019) with attention mechanism. The third
class exploits the most recent advances in deep learning, including the Neural Interaction Detection
(NID) and some new variants (Tsang et al., 2018a;b; Cui et al., 2019): Persistence Interaction
Detection (PID) (Liu et al., 2020), Integrated Hessians (IH) (Janizek et al., 2020), Shapley interaction
(Zhang et al., 2020; Sundararajan et al., 2020), etc. Although we have witnessed well improved
interaction detection performance for many datasets over the decades, the above methods still lead to
inconsistent detection results for some other datasets. The reasons are twofold. Firstly, the interaction
strength is empirically deﬁned, for instance, NID method computes the interaction strength via
summarizing the neural network weights. Secondly, a speciﬁc deep learning model is required, for
instance, an ℓ1-regularized ReLU network is required by the latest NID and PID methods to maintain
high accuracy. In contrast, our proposed method is derived directly from the deﬁnition of feature
interaction (Friedman et al., 2008) and moreover is not conﬁned to any speciﬁc learning model."
RELATED WORKS,0.018796992481203006,"Model Interpretability: There are two categories of approaches to address model interpretability,
namely the transparency-based and post-hoc approaches (Doˇsilovi´c et al., 2018). Transparency-based
approaches require the model itself to be simple and interpretable, like linear models, decision
trees, etc. One can directly read off the interpretations from their coefﬁcients or decision rules. But
often, they are less accurate due to limited representation power. In contrast, post-hoc approaches
extract useful information from a pre-trained model, which is often complex and hard to interpret.
Well-known methods such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg & Lee, 2017) fall
in this category, but they did not take feature interaction into account. Recently proposed symbolic
metamodel (Alaa & van der Schaar, 2019) captures nonlinear interactions by approximating the
black-box model with explicit symbolic expressions. There are also some other post-hoc approaches
based on sensitivity analysis (Cortez & Embrechts, 2013), which return a quantiﬁcation of fea-
ture importance νi = E[( ∂F (x)"
RELATED WORKS,0.020050125313283207,"∂xi )2] (Kucherenko et al., 2009) and interactions νij = E[| ∂2F (x)"
RELATED WORKS,0.021303258145363407,"∂xi∂xj |2]
(Roustant et al., 2014) by input perturbation. Our work aims to combine the strengths of the two
categories. Concretely, we ﬁrst extract the interaction knowledge by a post-hoc method, and then
build a transparent and interpretable learning model as illustrated in Figure 3."
RELATED WORKS,0.022556390977443608,"Model Compression and Knowledge Distillation (KD): Model compression (Bucilua et al., 2006)
aims to learn a small compressed model (Student) from a large complex model (Teacher) with
augmented training data produced by the Teacher. Compared to the Teacher, the Student can make
similar or even better predictions. Knowledge distillation (Hinton et al., 2015) mainly deals with
multi-class classiﬁcation problems and extracts “valuable information that deﬁnes a rich similarity
structure over the data”. Our work introduces a novel viewpoint of knowledge (the interacted
relationships) and targets a lightweight but more accurate Student model."
RELATED WORKS,0.023809523809523808,Published as a conference paper at ICLR 2022
PROPOSED INTERACTION DETECTION METHOD,0.02506265664160401,"3
PROPOSED INTERACTION DETECTION METHOD"
PROPOSED INTERACTION DETECTION METHOD,0.02631578947368421,"Before diving into in-depth discussions of interaction detection, we need to formally deﬁne what
feature interaction is. The textbook deﬁnition according to Friedman et al. (2008) is given below.
Deﬁnition 3.1 (Friedman & Popescu 2008). A function F : Rp →R is said to exhibit an interaction
between two of its variables xi and xj if the difference in the value of F(x) as a result of changing
the value of xi depends on the value of xj."
PROPOSED INTERACTION DETECTION METHOD,0.02756892230576441,"Equivalently, if Ex
h
| ∂2F (x)"
PROPOSED INTERACTION DETECTION METHOD,0.02882205513784461,"∂xi∂xj |2i
> 0, namely the partial derivative w.r.t xj turns out to be dependent"
PROPOSED INTERACTION DETECTION METHOD,0.03007518796992481,"on xi, then we say xi and xj are interacted. Otherwise, xi and xj have no interaction, if F(x) can be
expressed as the sum of two functions f\i and f\j (Sorokina et al., 2008), namely,"
PROPOSED INTERACTION DETECTION METHOD,0.03132832080200501,"F(x) = f\i (x1, . . . , xi−1, xi+1, . . . , xp) + f\j (x1, . . . , xj−1, xj+1, . . . , xp) ,"
PROPOSED INTERACTION DETECTION METHOD,0.03258145363408521,"where f\i(j) is irrespective to xi(j). Similarly, higher-order (multi-way) interaction can be deﬁned.
In this paper, we mainly focus on pairwise interaction, while multi-way interaction is only brieﬂy
discussed due to space limitations."
INTERACTION STRENGTH MEASURE,0.03383458646616541,"3.1
INTERACTION STRENGTH MEASURE"
INTERACTION STRENGTH MEASURE,0.03508771929824561,"Motivated by the above deﬁnition of interaction, it is natural to take advantage of the Hessian matrix
H := ∇2
xxF(x). The magnitude of its entry | ∂2F (x)"
INTERACTION STRENGTH MEASURE,0.03634085213032581,"∂xi∂xj | contains rich information about the local
curvature at a data point x. Note that F(x) is a regression function here, not a loss function. This
idea was originally considered in the economics community (Ai & Norton, 2003) and rediscovered
for sensitivity analysis in Roustant et al. (2014). The goodness of F(x) as an approximator can
essentially inﬂuence the interaction detection performance, see our Theorem M.2 in the supplement."
INTERACTION STRENGTH MEASURE,0.03759398496240601,"We deﬁne g(x, i, j) := | ∂2F (x)"
INTERACTION STRENGTH MEASURE,0.03884711779448621,∂xi∂xj |2 to measure the local interaction strength of the i-th and j-th
INTERACTION STRENGTH MEASURE,0.040100250626566414,"features at point x. We then use f(i, j) := Ex [g(x, i, j)] = Ex
h
| ∂2F (x)"
INTERACTION STRENGTH MEASURE,0.041353383458646614,"∂xi∂xj |2i
as a measure of
the global interaction strength. If f(i, j) ≈0, we say feature xi and xj have weak interaction;
otherwise, if f(i, j) is signiﬁcantly larger than zero, then xi and xj have strong interaction. In this
paper, we focus on the global interaction."
INTERACTION STRENGTH EVALUATION,0.042606516290726815,"3.2
INTERACTION STRENGTH EVALUATION"
INTERACTION STRENGTH EVALUATION,0.043859649122807015,"Figure 1: The landscape of a ReLU
network with two inputs (X1, X2)."
INTERACTION STRENGTH EVALUATION,0.045112781954887216,"The above deﬁned global interaction strength f(i, j)
="
INTERACTION STRENGTH EVALUATION,0.046365914786967416,"Ex
h
| ∂2F (x)"
INTERACTION STRENGTH EVALUATION,0.047619047619047616,"∂xi∂xj |2i
is mostly unavailable due to the unknown in-
put distribution x ∼P(x). The Monte Carlo method can be
used to approximate it by computing the sample mean over the
training data(Roustant et al., 2014)."
INTERACTION STRENGTH EVALUATION,0.04887218045112782,"Analytical Evaluation: The Hessian matrix ∇2
xxF(x) for neu-
ral networks at a certain data point can be calculated analytically
and efﬁciently, by using the automatic differentiation (Paszke
et al., 2017). However, using this analytical solution is prob-
lematic for some learning models, such as the ReLU network,
Random Forest (RF), etc., see supplement A. For example, the
landscape of a ReLU network is piece-wise linear as shown in
Figure 1, thus the exact Hessian is a zero matrix at almost every
point. So, we turn to the following numerical evaluation for
broader horizons."
INTERACTION STRENGTH EVALUATION,0.05012531328320802,"Numerical Evaluation: Finite difference method is a common way to approximate the Hessian on a
given data point (Campolongo & Braddock, 1999), i.e.,"
INTERACTION STRENGTH EVALUATION,0.05137844611528822,∂2F(x)
INTERACTION STRENGTH EVALUATION,0.05263157894736842,"∂xi∂xj
≈
1
4hihj
[F(x + eihi + ejhj) −F(x + eihi −ejhj)"
INTERACTION STRENGTH EVALUATION,0.05388471177944862,"−F(x + ejhj −eihi) + F(x −eihi −ejhj)],
(1)"
INTERACTION STRENGTH EVALUATION,0.05513784461152882,Published as a conference paper at ICLR 2022
INTERACTION STRENGTH EVALUATION,0.05639097744360902,"where ei is a one-hot vector with the i-th element being equal to one and the rest of elements being
zeros. We also note that if the computation of gradient ∂F"
INTERACTION STRENGTH EVALUATION,0.05764411027568922,"∂x is cheap (e.g., F is a neural network), then"
INTERACTION STRENGTH EVALUATION,0.05889724310776942,"Hij can be approximated as
1
2h
h
∂F (x+ejh)"
INTERACTION STRENGTH EVALUATION,0.06015037593984962,"∂xi
−∂F (x−ejh) ∂xi"
INTERACTION STRENGTH EVALUATION,0.06140350877192982,"i
to reduce computation, which is similar
to the feature interaction score deﬁned in Greenside et al. (2018). The choice of perturbation size hi
or hj (abbr. hi(j)) is critical. Generally, we do not want hi(j) to be too small (incurring round-off
error) or too large (incurring truncation error) to get a good overall approximation of the derivative
(Jerrell, 1997; Baydin et al., 2018). For our problem, we particularly do not want hi(j) to be too small
so that the four evaluated points (shown in the numerator of Equation 1) lie on the same hyperplane
(e.g., region A in Figure 1), which makes the quantity in Equation 1 always zero. The following
theorem reveals that choosing a sufﬁciently small hi(j) is not necessary."
INTERACTION STRENGTH EVALUATION,0.06265664160401002,"Theorem 3.1. For any x and y, function F shows no interaction between them, i.e., it can be
decomposed as F(x, y) = a(x) + b(y) if and only if, for any h, k > 0, F(x + h, y + k) −F(x +
h, y −k) −F(x −h, y + k) + F(x −h, y −k) = 0."
INTERACTION STRENGTH EVALUATION,0.06390977443609022,"The magnitude of the numerator in Equation 1 tells us whether F is locally separable for variables xi
and xj at point x, see our proof in supplement B."
INTERACTION STRENGTH EVALUATION,0.06516290726817042,"The main issue of the ﬁnite difference method lies in the computational complexity for approximating
the global interaction strength f(i, j), especially when the function evaluation itself is expensive.
Using all the training samples for ﬁnding just a few strongest interaction pairs can be a total waste of
computation resources. In general, for a dataset with N samples in p-dimensional feature spaces,
a total number of 4Np(p −1)/2 arithmetic evaluations of the surrogate regression function F are
needed. If the Hessian happens to be sparse, that is, there are only a few interactions existing in the
ground truth function, massive evaluations on those feature pairs with zero interaction strength should
be avoided."
IDENTIFICATION OF THE K-STRONGEST PAIRWISE INTERACTIONS,0.06641604010025062,"3.3
IDENTIFICATION OF THE k-STRONGEST PAIRWISE INTERACTIONS"
IDENTIFICATION OF THE K-STRONGEST PAIRWISE INTERACTIONS,0.06766917293233082,"In practice, we do not have to obtain the interaction strengths for all interaction pairs, because many
of them are simply too weak to have an impact on the output, and the top k strongest pairwise
interactions are well sufﬁcient for data modeling and prediction. Finding the top k strongest pairwise
interactions ﬁts perfectly into the best k-arms identiﬁcation problem in the context of multi-armed
bandits."
IDENTIFICATION OF THE K-STRONGEST PAIRWISE INTERACTIONS,0.06892230576441102,"Inspired by the recent work (Bagaria et al., 2018b), we can treat each entry of the Hessian Hij
as an arm Ar (see Figure 2). Since Hessian is a symmetric matrix, we only need to consider
n = p(p −1)/2 arms in the set {Ar : r ∈[n]}. For example, if we choose to pull the arm for the
i-th row and j-th column of the Hessian, we will evaluate the local interaction strength g(xξ, i, j)
on a randomly selected training data point xξ. The strength g(xξ, i, j) can be regarded as a random
reward, where xξ is uniformly drawn from the training data {x1, . . . , xN}. For ease of notation, we
let f(r; ξ) := g(xξ, i, j) as the random reward of the arm r, and deﬁne µr := Eξ[f(r; ξ)] as the true
mean value. Our goal is to ﬁnd the best k arms with the highest mean reward."
IDENTIFICATION OF THE K-STRONGEST PAIRWISE INTERACTIONS,0.07017543859649122,"This reformulation is reasonable since the following three essential assumptions for multi-armed
bandits (Slivkins et al., 2019) are satisﬁed."
IDENTIFICATION OF THE K-STRONGEST PAIRWISE INTERACTIONS,0.07142857142857142,a. Only the reward will be observed after each pull;
IDENTIFICATION OF THE K-STRONGEST PAIRWISE INTERACTIONS,0.07268170426065163,b. The reward for each arm is drawn independently and identically from its reward distribution;
IDENTIFICATION OF THE K-STRONGEST PAIRWISE INTERACTIONS,0.07393483709273183,"c. The reward for each round is bounded. Since we only consider functions F(x) deﬁned on a
compact set, if F is a continuous function and twice differentiable, its ﬁrst- and second-order
derivatives are bounded accordingly."
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.07518796992481203,"3.4
DETECTING THE k-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.07644110275689223,"In this section, we brieﬂy introduce the UCB algorithm (Lai & Robbins, 1985) for ﬁnding the
k-strongest interactions. Here, we design a sequence of estimators { ˆfℓ(r)} for the mean reward of
the arm Ar and construct conﬁdence interval C(ℓ). We let ˆfℓ(r) denote the estimator after ℓpulls of
the arm Ar. Several assumptions need to be made before we give out the primary outcome."
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.07769423558897243,Published as a conference paper at ICLR 2022 … … … … … … … …
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.07894736842105263,"Algorithm 1 UCB ({Ar : r ∈[n]}, m, k)"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.08020050125313283,"1: For each arm {Ar : r ∈[n]}, compute an initial (1 −δ) conﬁdence interval by
running log(n) steps of the estimator to obtain: [ˆµr(1) −Cr(1), ˆµr(1) + Cr(1)]
2: B = {} // Set of k best arms
3: S = {Ar : r ∈[n]}
// Set of arms under consideration
4: while TRUE do
5:
At iteration t, pick arm Ar that maximizes ˆµr(t −1) + Cr(t −1) among arms
in S.
6:
if arm Ar is evaluated less than m times then
7:
Reﬁne the conﬁdence interval and mean estimate of the arm Ar by updating the
estimator one more time.
8:
else
9:
Set ˆµr(t) to be the current mean of the arm and set Cr(t) = 0.
10:
end if
11:
if Arm Ar is such that ∀i ̸= r, ˆµi(t) + Ci(t) < ˆµr(t) −Cr(t) then
12:
Add Ar to B
13:
Remove Ar from the set of arms under consideration, that is S = S −{Ar}.
14:
if the size of B, |B| = k, then
15:
return B
16:
end if
17:
end if
18: end while"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.08145363408521303,"Figure 2: Top Left: The entries of the Hessian are treated as arms. Bottom Left: Illustration of the
notations used in Algorithm 1. This is a snapshot taken at iteration t, where the colored solid bar
and the red dashed bar are the true mean reward and the current estimate for each arm, respectively.
At each iteration, the arm with the highest UCB will be pulled. Right: Pseudo-code for the UCB
algorithm."
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.08270676691729323,Assumption 1. Finite m evaluations of one arm are sufﬁcient to obtain an accurate reward.
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.08395989974937343,Assumption 2. Estimators ˆfℓ(r) are σr-subgaussian.
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.08521303258145363,"Assumption 2 is satisﬁed because the reward is bounded (Hoeffding’s lemma, 1963). Here, σrs are
pre-deﬁned parameters. In the following analysis, we will choose σ = maxr{σr}, such that all ˆfℓ(r)
are σ-subgaussian."
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.08646616541353383,Let ˆfℓ(r) := 1
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.08771929824561403,"ℓ
Pℓ
i=1 f(r; ξi), and we construct the 1 −δ conﬁdence intervals (see supplement C) of
ˆfℓ(r) as,"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.08897243107769423,C(ℓ) := ( q
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.09022556390977443,2σ2 log 2
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.09147869674185463,"δ
ℓ
if ℓ≤m
0
if ℓ> m
,
(2)"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.09273182957393483,"µr ∈
h
ˆfℓ(r) −C(ℓ), ˆfℓ(r) + C(ℓ)
i
, w.p. 1 −δ,
(3)"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.09398496240601503,"where µr denotes the true expectation. Increasing number of pulls of one arm, the uncertainty
of the reward for that arm will decrease. When the number of pulls reaches m, we can remove
the uncertainty of the arm and set C(ℓ) = 0. Let ℓr(t) count the number of pulls of arm Ar till
iteration t. To simplify the notations, we let ˆµr(t) = ˆfℓr(t)(r) and Cr(t) = C(ℓr(t)) be the sample
means and conﬁdence intervals at iteration t. Now, we formally introduce the complete procedure in
Algorithm 1."
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.09523809523809523,"The Algorithm 1 is initialized by evaluating all arms O(log(n)) times, then at each subsequent
iteration, it picks the arm with the highest UCB, i.e., maxj{ˆµj(t)+Cj(t)}, to evaluate (see Figure 2).
If one arm has been evaluated for m times, we will its true mean value and set the uncertainty to zero.
At the end of each iteration, if there exists one arm whose Lower Conﬁdence Bound (LCB) is higher
than all the other’s UCB, we will put it into the set of k best arms."
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.09649122807017543,"In the following, we give a theoretical analysis on the complexity of this algorithm. We have to further
deﬁne ∆(k)
i
:= max(0, µi∗
k −µi), where i∗
k is the index for the k-th best arm and ∆(k)
i
indicates the
gap of true values between the top-k arms and the rest.
Theorem 3.2. With probability 1−Θ( 1"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.09774436090225563,"n2 ), given maximal pulling times m for each arm, Algorithm 1"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.09899749373433583,"returns the k-strongest interaction pairs in O
Pn
i=1 log(n)

σ2 log(nm)"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.10025062656641603,"(∆(k)
i
)2
∧m

time, where (· ∧·)"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.10150375939849623,"is short for min(·, ·)."
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.10275689223057644,Published as a conference paper at ICLR 2022
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.10401002506265664,"Remark 3.1. When k ≪n, the complexity of Algorithm 1 for ﬁnding the top k interactions is
O(n log(mn) log(n) + km log(n)) under some natural assumptions on the distribution of ∆i, being
superior to the naive algorithm (every arm is pulled m times) with complexity O(nm).
Remark 3.2. For k = 1, namely the strongest interaction pair identiﬁcation, it requires at most
Pn
i=1((
8σ2"
DETECTING THE K-STRONGEST PAIRWISE INTERACTIONS WITH UCB ALGORITHM,0.10526315789473684,"(∆(1)
i
)2 log(n3m)) ∧m) pulls. We leave the complete proof in supplement C."
EXTENSION TO MULTI-WAY INTERACTION,0.10651629072681704,"3.5
EXTENSION TO MULTI-WAY INTERACTION"
EXTENSION TO MULTI-WAY INTERACTION,0.10776942355889724,"Similarly, the third-order derivatives
∂3F
∂xi∂xj∂xk , ∀(i, j, k), and even the higher-order ones can be
approximated by the ﬁnite difference method as well, but the computational complexity will increase
geometrically. For a remedy, one can detect higher-order interactions from the obtained pairwise
ones. For instance, based on the k-strongest pairwise interaction strengths, we can further construct
an undirected graph G, in which each node represents a feature. We develop Algorithm 2 to detect
the multi-way interactions through checking all the cliques in the graph G, and more details are given
in supplement G. Despite the computation complexity, our proposed algorithm is more advantageous
to use because the associated learning model is agnostic and the form of interaction is adaptive as
compared with some existing methods, such as Min et al. (2014), which relies on a logistic regression
model and multiplicative interactions of a particular form."
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.10902255639097744,"4
MODEL COMPRESSION WITH DETECTED INTERACTIONS"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.11027568922305764,"The detected interactions can be used for designing structured and more transparent models. A similar
idea was mentioned in Tsang et al. (2020b). But we propose a new architecture called Parametric
ACE (ParaACE), inspired by several seminal works on Generalized Additive Model (GAM) (Hastie,
2017) and (nonparametric) Alternating Conditional Expectation (ACE) method (Breiman & Friedman,
1985). The original ACE method is based on the following model,"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.11152882205513784,"θ(Y ) = p
X"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.11278195488721804,"k=1
φk(Xk) + ϵ,
(4)"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.11403508771929824,"and solves for the optimal transformation functions {θ, φ1, · · · , φp} nonparametrically in the Hilbert
space. However, it only considers transformation of each single feature and is time consuming for
large feature dimension p. We build the ParaACE model with the detected interactions as"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.11528822055137844,"h(x; {θsi}, {θri}, β) := β0 + p
X"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.11654135338345864,"i=1
βisi (xi; θsi) + R
X"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.11779448621553884,"i=1
βp+iri (xIi; θri) ,
(5)"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.11904761904761904,"where {Ii}R
i=1 is a set of indices for the detected interaction pairs, si and ri are the transformation
functions for a single feature and interacting features with the corresponding parameters {θsi} and
{θri}, β is a weight vector for the output of all the transformation functions. …"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.12030075187969924,"Candidate
interactions … …"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.12155388471177944,"Main effects …
…"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.12280701754385964,Identity
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.12406015037593984,"ResNet …
… Input"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.12531328320802004,Output
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.12656641604010024,"Optimal 
transformation layer"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.12781954887218044,"Optional
fix-up 
layer"
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.12907268170426064,Figure 3: Parametric ACE model.
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.13032581453634084,"The new architecture is shown in Figure 3 with two meta
layers: the optimal feature transformation layer (Equa-
tion 5, boxed in red and green) and an optional ﬁx-up layer
(boxed in yellow). The optimal transformation layer gives
the transformation functions of both the main effects and
interacting features explicitly. The ﬁx-up layer can be un-
derstood as the inverse function for the optimal transforma-
tion of the output, θ(Y ), in the original ACE (Equation 4).
Another functionality of the ﬁx-up layer is to alleviate the
negative impacts of the wrongly detected interactions on
the output. More explanations are given in supplement H."
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.13157894736842105,"We highlight the advantages of this model as follows: 1)
interpretable, unlike many other models, ParaACE model
gives explicit transformations of features, making it interpretable. 2) lightweight, this model can
signiﬁcantly reduce the number of parameters from the original one, and unlike the pruning technique
(Han et al., 2015), it does not need extra memory to store the indices of the preserved weights."
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.13283208020050125,Published as a conference paper at ICLR 2022
MODEL COMPRESSION WITH DETECTED INTERACTIONS,0.13408521303258145,"3) ﬂexible, the structure of NN is modularized by blocks, which is easy to plug in/off for other
applications. The ﬁx-up layer can be redesigned as block-wise subnetworks to capture high-level
interactions, end up in the shape of a hierarchical deep network."
EXPERIMENTS,0.13533834586466165,"5
EXPERIMENTS"
EXPERIMENTAL SETUP,0.13659147869674185,"5.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.13784461152882205,"Datasets: We generated 10 synthetic datasets (p = 10) as was used in Tsang et al. (2018a) to
test the accuracy of different interaction detection methods. We added a scaled Gaussian noise
η ∼0.1 × N(0, 1) to get the noisy data. We also selected 5 real datasets, namely the Elevators for
controlling an F16 aircraft (Itorgo, 2019), Parkinsons for predicting the total UPDRS scores (Tsanas
et al., 2009), Skillcraft for game player behavior analysis (Thompson et al., 2013), Cal housing for
house price prediction (Pace & Barry, 1997), and Bike sharing for predicting hourly bike rental count
(Fanaee-T & Gama, 2014). The datasets are preprocessed, and details are shown in supplement D."
EXPERIMENTAL SETUP,0.13909774436090225,"Neural Network Conﬁguration: For the baseline OverparaFC (Teacher), we used the architecture
of p-5000-900-400-100-30-1 (indicating a deep neural network with p inputs, one output, and 5
hidden layers with 5000, 900, 400, 100, 30 neurons, respectively), which is a standard ReLU network
with the last hidden layer being linear. For the ParaACE model (Student), the size of the network
mainly depends on the number of candidate interactions we choose; more interactions imply more
blocks. However, the conﬁguration for each subnetwork is ﬁxed, the structure of which is 1-50-8-1
for main effects and 2-50-8-1 for pairwise interactions. These subnetworks are ReLU networks with
linear output. The ﬁx-up layer is chosen as a single layer ResNet with the number of neurons equal to
15. All the above networks were initialized with Kaiming’s strategy (He et al., 2015)."
EXPERIMENTAL SETUP,0.14035087719298245,"Optimizer Setup: We chose adam (Kingma & Ba, 2014) as the optimizer, with the default setup in
MXNet (Chen et al., 2015), and the batch size is set to be 500 for all the datasets."
PAIRWISE INTERACTION DETECTION,0.14160401002506265,"5.2
PAIRWISE INTERACTION DETECTION"
PAIRWISE INTERACTION DETECTION,0.14285714285714285,"We ﬁrst test on 10 different synthetic datasets (p = 10) (Tsang et al., 2018a) to show the superiority
of our proposed method. Datasets with higher input dimensions (p = 50, 100) were compared as
well, see supplement F. To compare the detection performance of different methods, we adopt the
ROC-AUC metric (see supplement E). In Table 1, we compared our proposed method with various
existing methods, including ANOVA (Wonnacott & Wonnacott, 1990), AG (Sorokina et al., 2008),
IH (Janizek et al., 2020), NID (Tsang et al., 2018a), and PID (Liu et al., 2020). HierLasso (Bien et al.,
2013) and RuleFit (Friedman et al., 2008) have been omitted due to their weak performances. For
our method, we pick the same number of data samples n = 10000 for the training of OverparaFC
and run 5 independent Monte Carlo tests. In Algorithm 1, we pull each arm 3 times for initialization.
Each arm will be pulled at maximally m = 100 times, and we terminate when k = 20 strongest
interactions stand out. We take ˆµr(t) upon termination as the ﬁnal scores for our method. In this
experiment, our proposed method needs around 1500 pulls of arms to pick out the top 20 interactions,
however, naively pulling each arm 100 times needs 100C2
10 = 4500 pulls in total."
PAIRWISE INTERACTION DETECTION,0.14411027568922305,"Table 1: Interaction detection methods in comparison. ROC-AUCs of pairwise interaction strengths
proposed by us (with hi(j) = 0.8) versus various baselines on a test suite of synthetic functions (see
Table 4 in the supplement)."
PAIRWISE INTERACTION DETECTION,0.14536340852130325,"ANOVAa,b
AG
IHa
NIDa
PIDa
Our methoda,b"
PAIRWISE INTERACTION DETECTION,0.14661654135338345,"F1(x)
0.992
1 ± 0.0
0.989
0.970 ± 9.2e−3
0.986 ± 4.1e−3
0.947 ± 2.5e−2
F2(x)
0.468
0.88 ± 1.4e−2
0.968
0.79 ± 3.1e−2
0.804 ± 5.7e−2
0.944 ± 3.2e−2
F3(x)
0.657
1 ± 0.0
1
0.999 ± 2.0e−3
1 ± 0.0
1 ± 0.0
F4(x)
0.563
0.999 ± 1.4e−3
1
0.85 ± 6.7e−2
0.935 ± 3.9e−2
0.997 ± 2.7e−3
F5(x)
0.544
0.67 ± 5.7e−2
1
1 ± 0.0
1 ± 0.0
0.988 ± 2.3e−2
F6(x)
0.780
0.64 ± 1.4e−2
0.932
0.98 ± 6.7e−2
1 ± 0.0
0.925 ± 3.7e−2
F7(x)
0.726
0.81 ± 4.9e−2
0.611
0.84 ± 1.7e−2
0.888 ± 2.8e−2
0.714 ± 7.0e−2
F8(x)
0.929
0.937 ± 1.4e−3
0.954
0.989 ± 4.4e−3
1 ± 0.0
0.984 ± 4.2e−3
F9(x)
0.783
0.808 ± 5.7e−3
0.831
0.83 ± 5.3e−2
0.972 ± 2.9e−2
0.948 ± 4.6e−2
F10(x)
0.765
1 ± 0.0
1
0.995 ± 9.5e−3
0.987 ± 3.5e−2
1 ± 0.0
average
0.721
0.87 ± 1.4e−2
0.931
0.92 ± 2.3e−2
0.957 ± 6.2e−2
0.945 ± 5.6e−31
Time (s)
-
-
132
0.007
-
14.7"
PAIRWISE INTERACTION DETECTION,0.14786967418546365,"1 Small standard deviation means high stability, a post-hoc method, b model agnostic method."
PAIRWISE INTERACTION DETECTION,0.14912280701754385,Published as a conference paper at ICLR 2022
PAIRWISE INTERACTION DETECTION,0.15037593984962405,"From the results, we can conclude the following: (1) PID outperforms all others; (2) our proposed
method is as competitive as the PID method; (3) while the average performance of our method is
only slightly worse than the PID method due to the poor result on dataset 7; this is due to the term
1
1+(x4x5x6x7x8)2 in F7(x) has little impact on the output, and such interactions are hard to detect.
Apart from the above facts, both NID and PID work by analyzing the connectivity of NN, which
strongly relies on speciﬁc sparse ReLU networks and empirical interaction strength computations,
and thus are risky to generalize and may fail for some other datasets. For instance, for the function
F11(x) = x2
1x2x2
3x4 + x2
5x6x2
7x8, the ROC-AUC of NID degrades to around 0.69, while our method
maintains 1.0 robustly. Moreover, our method is model-agnostic and can be directly applied to any
pre-trained learning models, such as tree models, kernel regression models, etc., which is impossible
for other methods."
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.15162907268170425,"5.3
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE"
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.15288220551378445,"In this section, we construct a Student model with the learned interactions. With the interaction
knowledge extracted from the Teacher model, we ﬁrst show that the Student model trained under the
same setup can achieve well-improved prediction accuracy (on average). We use normalized RMSE
(NRMSE), which is deﬁned by
RMSE
ymax−ymin to measure the performances of all competing models such
that the comparison across different datasets is fair and convenient."
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.15413533834586465,"For synthetic data, we use 800 training samples and 200 test samples. The noise η is injected
as described earlier. We choose the top 20 pairwise interactions with our new detection method
to build the ParaACE model. We compared our proposed ParaACE with OverparaFC, KD for
regression (Chen et al., 2017), and two recent pruning methods known as Lottery Ticket Hypothesis
(LTH) (Frankle & Carbin, 2019) and SynFlow (Tanaka et al., 2020). The results are shown in
Table 2. As we expected, the KD method shows similar performance as the OverparaFC, because
the Student model tries hard to approximate its Teacher. However, our ParaACE model can improve
the performance of the OverparaFC by 26.0% on average, and the model compression ratio (CR :=
# parameters in the uncompressed model"
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.15538847117794485,"# parameters in the compressed model ) is around 300. More details can be found in supplement L. We also
found that the ParaACE model is sample efﬁcient. To be concrete, the ParaACE trained on a small
training set can surpass the over-parameterized NN trained with substantially more data, see Figure 13
in supplement K."
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.15664160401002505,"Table 2: NRMSE of the baselines: OverparaFC, KD, and LTH versus our proposed ParaACE tested
on the synthetic datasets in Table 4. (The results were averaged over 5 folds.)"
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.15789473684210525,"F1
F2
F3
F4
F5
F6
F7
F8
F9
F10
Average
CR"
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.15914786967418545,"OverparaFC
0.026
0.054
0.059
0.062
0.042
0.041
0.018
0.024
0.032
0.023
0.038
1
KD (student)
0.029
0.055
0.061
0.064
0.041
0.042
0.019
0.024
0.032
0.024
0.039
278
LTH
0.027
0.044
0.032
0.032
0.039
0.033
0.018
0.021
0.029
0.025
0.030
18
SynFlow
0.025
0.048
0.034
0.035
0.039
0.031
0.015
0.018
0.026
0.022
0.029
280
ParaACE
0.025
0.035
0.031
0.030
0.038
0.025
0.016
0.019
0.023
0.021
0.026
283"
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.16040100250626566,"For real-world datasets, we built the ParaACE model with the top 50 pairwise interactions for
Elevators, Parkinsons, Skillcraft, Bike sharing, and the top 20 pairwise interactions for Cal housing
(since the number of features is smaller). For most datasets, the detected interactions can help improve
the predictive performance and signiﬁcantly reduce the model size, see Table 3 for detailed results.
The comparison with KD is omitted as it is supposed to be close to the Teacher."
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.16165413533834586,"Table 3: Performance comparison between OverparaFC, LTH, SynFlow, and ParaACE on real-world
datasets. (The results were averaged over 5 folds.)"
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.16290726817042606,"OverparaFC
LTH
SynFlow
ParaACE
Datasets
N
p
NRMSE
Parameters
NRMSE
CR
NRMSE
CR
NRMSE
Parameters
CR"
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.16416040100250626,"Elevators
16599
18
0.0483
4999461
0.0523
87
0.0479
120
0.0475
39848
125
Parkinsons
5875
20
0.0251
5009461
0.0180
87
0.0229
120
0.0204
40946
122
Skillcraft
3338
19
0.0937
5004461
0.1228
87
0.0968
120
0.0929
40397
124
Bike sharing
17379
15
0.0403
4984461
0.0404
87
0.0405
120
0.0420
38201
130
Cal housing
20640
8
0.1059
4949461
0.1038
87
0.1026
120
0.1022
16388
302"
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.16541353383458646,"Due to space limitations, we give more results for classiﬁcation tasks in supplement I. Furthermore,
we show the performance gain induced by interaction detection via ablation study in supplement N."
MODEL COMPRESSION WITH INTERACTION KNOWLEDGE,0.16666666666666666,Published as a conference paper at ICLR 2022
MODEL INTERPRETATION,0.16791979949874686,"5.4
MODEL INTERPRETATION"
MODEL INTERPRETATION,0.16917293233082706,"For synthetic data, we take F10(x) = sinh (x1 + x2) + arccos (tanh(x3 + x5 + x7)) + cos(x4 +
x5) + sec(x7x9) as an example. Three dominant pairwise interactions {x1, x2}, {x4, x5}, {x7, x9}
have been detected successfully and visualized in Figure 4 (left). The ground truth transformations
are shown in the upper part of the ﬁgure. Besides, the transformations for the single features can
also be learned accurately. Taking F3(x) = exp |x1 −x2| + |x2x3| −(x2
3)|x4| + log(x2
4 + x2
5 + x2
7 +
x2
8) + x9 +
1
1+x2
10 for example, wherein feature x6 is not involved, x9 is linearly transformed, and
x10 is transformed by an even function. All of them are correctly learned as shown in Figure 4 (right). x1"
MODEL INTERPRETATION,0.17042606516290726,"1.5
1.0
0.5
0.0
0.5
1.0
1.5 x2 1.5 1.0 0.5 0.0 0.5 1.0 1.5 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 x4"
MODEL INTERPRETATION,0.17167919799498746,"1.5
1.0
0.5
0.0
0.5
1.0
1.5 x5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 x7"
MODEL INTERPRETATION,0.17293233082706766,"1.5
1.0
0.5
0.0
0.5
1.0
1.5 x9 1.5 1.0 0.5 0.0 0.5 1.0 1.5 200 150 100 50 0 50 100 x1"
MODEL INTERPRETATION,0.17418546365914786,"1.5
1.0
0.5
0.0
0.5
1.0
1.5 x2 1.5 1.0 0.5 0.0 0.5 1.0 1.5 0.2 0.0 0.2 0.4 0.6 0.8 x4"
MODEL INTERPRETATION,0.17543859649122806,"1.5
1.0
0.5
0.0
0.5
1.0
1.5 x5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 0.6 0.5 0.4 0.3 0.2 0.1 x7"
MODEL INTERPRETATION,0.17669172932330826,"1.5
1.0
0.5
0.0
0.5
1.0
1.5 x9 1.5 1.0 0.5 0.0 0.5 1.0 1.5"
MODEL INTERPRETATION,0.17794486215538846,"0.005
0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035"
MODEL INTERPRETATION,0.17919799498746866,"sinh(x1 + x2)
cos(x4 + x5)
sec(x7x9)"
MODEL INTERPRETATION,0.18045112781954886,"1.6
0.8 0.0
0.8
1.6 0.095 0.100 0.105 0.110 0.115 0.120"
MODEL INTERPRETATION,0.18170426065162906,"1.6
0.8 0.0
0.8
1.6 0.1 0.0 0.1 0.2 0.3"
MODEL INTERPRETATION,0.18295739348370926,"s6(x6)
s9(x9)"
MODEL INTERPRETATION,0.18421052631578946,"1.6
0.8 0.0
0.8
1.6 0.20 0.15 0.10 0.05 0.00"
MODEL INTERPRETATION,0.18546365914786966,"1.6
0.8 0.0
0.8
1.6 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
MODEL INTERPRETATION,0.18671679197994986,"s10(x10)
ground truth s10(·)"
MODEL INTERPRETATION,0.18796992481203006,Figure 4: Visualization of the transformations in F10 (left) and F3 (right).
MODEL INTERPRETATION,0.18922305764411027,"For real applications, we ﬁrst take the Cal housing dataset for further analysis as less expert
knowledge is required. From Figure 5, we can easily recognize that {longitude, latitude} and
{totalRooms,totalBedrooms} show strong interactions. This makes sense, because {longitude, lati-
tude} indicates the location, and {totalRooms,totalBedrooms} indicates the fraction of bedrooms.
Also in the Parkinsons dataset, we ﬁnd {Age, Sex} are interacted, which is consistent with the result
that “age at onset (of Parkinson’s disease) was 2.1 years later in women than in men” in Haaxma
et al. (2007). Interaction detection may also have a signiﬁcant impact on drug combination discovery
(Julkunen et al., 2020). We applied our interaction detection method on an RF model trained with the
Drug combination dataset. Among the top 34 detected drug-drug interactions, 15 pairwise interac-
tions have been veriﬁed in the DrugBank database (Wishart et al., 2018) (see supplement J). These
results conﬁrmed the effectiveness of our interaction detection method and strongly call for careful
investigation of the unexplored drug pairs, potentially of great value."
MODEL INTERPRETATION,0.19047619047619047,longitude
MODEL INTERPRETATION,0.19172932330827067,"latitude
housingMedianAge"
MODEL INTERPRETATION,0.19298245614035087,totalRooms
MODEL INTERPRETATION,0.19423558897243107,totalBedrooms
MODEL INTERPRETATION,0.19548872180451127,population
MODEL INTERPRETATION,0.19674185463659147,households
MODEL INTERPRETATION,0.19799498746867167,medianIncome
MODEL INTERPRETATION,0.19924812030075187,longitude
MODEL INTERPRETATION,0.20050125313283207,latitude
MODEL INTERPRETATION,0.20175438596491227,housingMedianAge
MODEL INTERPRETATION,0.20300751879699247,totalRooms
MODEL INTERPRETATION,0.20426065162907267,totalBedrooms
MODEL INTERPRETATION,0.20551378446115287,population
MODEL INTERPRETATION,0.20676691729323307,households
MODEL INTERPRETATION,0.20802005012531327,medianIncome
MODEL INTERPRETATION,0.20927318295739347,Cal housing
MODEL INTERPRETATION,0.21052631578947367,subject# age
MODEL INTERPRETATION,0.21177944862155387,"sex
test_time"
MODEL INTERPRETATION,0.21303258145363407,Jitter(%)
MODEL INTERPRETATION,0.21428571428571427,Jitter(Abs)
MODEL INTERPRETATION,0.21553884711779447,Jitter:RAP
MODEL INTERPRETATION,0.21679197994987467,Jitter:PPQ5
MODEL INTERPRETATION,0.21804511278195488,Jitter:DDP
MODEL INTERPRETATION,0.21929824561403508,"Shimmer
Shimmer(dB)"
MODEL INTERPRETATION,0.22055137844611528,Shimmer:APQ3
MODEL INTERPRETATION,0.22180451127819548,Shimmer:APQ5
MODEL INTERPRETATION,0.22305764411027568,Shimmer:APQ11
MODEL INTERPRETATION,0.22431077694235588,Shimmer:DDA NHR HNR RPDE DFA PPE
MODEL INTERPRETATION,0.22556390977443608,subject#
MODEL INTERPRETATION,0.22681704260651628,"age
sex
test_time"
MODEL INTERPRETATION,0.22807017543859648,"Jitter(%)
Jitter(Abs)"
MODEL INTERPRETATION,0.22932330827067668,"Jitter:RAP
Jitter:PPQ5"
MODEL INTERPRETATION,0.23057644110275688,Jitter:DDP
MODEL INTERPRETATION,0.23182957393483708,"Shimmer
Shimmer(dB)
Shimmer:APQ3
Shimmer:APQ5
Shimmer:APQ11"
MODEL INTERPRETATION,0.23308270676691728,Shimmer:DDA
MODEL INTERPRETATION,0.23433583959899748,"NHR
HNR
RPDE DFA PPE"
MODEL INTERPRETATION,0.23558897243107768,Parkinsons
MODEL INTERPRETATION,0.23684210526315788,"0
10
20
30
40
50 FDA approved drugs 0 10 20 30 40"
MODEL INTERPRETATION,0.23809523809523808,Drug combination
MODEL INTERPRETATION,0.23934837092731828,"Figure 5: Heat maps of interaction strengths for Cal housing, Parkinsons, and Drug combination data.
Note that darker color indicates stronger interaction.
6
CONCLUSION"
MODEL INTERPRETATION,0.24060150375939848,"This paper proposed a fast, generic, and model-agnostic interaction detection method. A thorough
analysis of the computational complexity of our adaptive method proved its superior efﬁcacy. The
detected interaction knowledge can be further exploited to build a novel Student model, called
ParaACE, which is compact, more interpretable, and mostly more accurate than its Teacher and other
competing methods. We also showed experimentally that our ParaACE model is sample efﬁcient
owing to its simple architecture. Its model-agnostic property enables tremendous applications in
different domains of ﬁnance, medicine, biology, wireless communication, where a signiﬁcant number
of well-performed black-box models have been built but not interpreted yet."
MODEL INTERPRETATION,0.24185463659147868,Published as a conference paper at ICLR 2022
MODEL INTERPRETATION,0.24310776942355888,ACKNOWLEDGMENTS
MODEL INTERPRETATION,0.24436090225563908,"This work was supported by Guangdong Provincial Key Laboratory of Big Data Computing and by
NSFC under Grant 61731018. The corresponding author is Feng Yin."
REFERENCES,0.24561403508771928,REFERENCES
REFERENCES,0.24686716791979949,"Rishabh Agarwal, Nicholas Frosst, Xuezhou Zhang, Rich Caruana, and Geoffrey E Hinton. Neural
additive models: Interpretable machine learning with neural nets. arXiv preprint arXiv:2004.13912,
2020."
REFERENCES,0.24812030075187969,"Chunrong Ai and Edward C Norton. Interaction terms in logit and probit models. Economics letters,
80(1):123–129, 2003."
REFERENCES,0.24937343358395989,"Ahmed M Alaa and Mihaela van der Schaar.
Demystifying black-box models with symbolic
metamodels. In NeurIPS, pp. 11301–11311, 2019."
REFERENCES,0.2506265664160401,"Vivek Bagaria, Govinda Kamath, Vasilis Ntranos, Martin Zhang, and David Tse. Medoids in almost-
linear time via multi-armed bandits. In Amos Storkey and Fernando Perez-Cruz (eds.), Proceedings
of the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics, volume 84
of Proceedings of Machine Learning Research, pp. 500–509, Playa Blanca, Lanzarote, Canary
Islands, 09–11 Apr 2018a. PMLR."
REFERENCES,0.2518796992481203,"Vivek Bagaria, Govinda M. Kamath, and David N. Tse. Adaptive Monte-Carlo optimization, 2018b."
REFERENCES,0.2531328320802005,"Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy
physics with deep learning. Nature communications, 5(1):1–9, 2014."
REFERENCES,0.2543859649122807,"Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. Journal of machine learning research, 18
(153), 2018."
REFERENCES,0.2556390977443609,"Jacob Bien, Jonathan Taylor, and Robert Tibshirani. A lasso for hierarchical interactions. Annals of
statistics, 41(3):1111, 2013."
REFERENCES,0.2568922305764411,"Leo Breiman and Jerome H Friedman. Estimating optimal transformations for multiple regression
and correlation. Journal of the American statistical Association, 80(391):580–598, 1985."
REFERENCES,0.2581453634085213,"Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings
of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
535–541. ACM, 2006."
REFERENCES,0.2593984962406015,"Francesca Campolongo and Roger Braddock. The use of graph theory in the sensitivity analysis of
the model output: a second order screening method. Reliability Engineering & System Safety, 64
(1):1–12, 1999."
REFERENCES,0.2606516290726817,"Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efﬁcient
object detection models with knowledge distillation. In Advances in Neural Information Processing
Systems, pp. 742–751, 2017."
REFERENCES,0.2619047619047619,"Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. MXNet: A ﬂexible and efﬁcient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015."
REFERENCES,0.2631578947368421,"Paulo Cortez and Mark J Embrechts. Using sensitivity analysis and visualization techniques to open
black box data mining models. Information Sciences, 225:1–17, 2013."
REFERENCES,0.2644110275689223,"Tianyu Cui, Pekka Marttinen, and Samuel Kaski. Learning pairwise interactions with Bayesian
neural networks, 2019."
REFERENCES,0.2656641604010025,"Filip Karlo Doˇsilovi´c, Mario Brˇci´c, and Nikica Hlupi´c. Explainable artiﬁcial intelligence: A survey.
In 2018 41st International convention on information and communication technology, electronics
and microelectronics (MIPRO), pp. 0210–0215. IEEE, 2018."
REFERENCES,0.2669172932330827,Published as a conference paper at ICLR 2022
REFERENCES,0.2681704260651629,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.2694235588972431,"Hadi Fanaee-T and Joao Gama. Event labeling combining ensemble detectors and background
knowledge. Progress in Artiﬁcial Intelligence, 2(2-3):113–127, 2014."
REFERENCES,0.2706766917293233,"Ronald Aylmer Fisher. Statistical methods for research workers. In Breakthroughs in statistics, pp.
66–70. Springer, 1992."
REFERENCES,0.2719298245614035,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.2731829573934837,"Jerome H Friedman, Bogdan E Popescu, et al. Predictive learning via rule ensembles. The Annals of
Applied Statistics, 2(3):916–954, 2008."
REFERENCES,0.2744360902255639,"Peyton Greenside, Tyler Shimko, Polly Fordyce, and Anshul Kundaje. Discovering epistatic feature
interactions from neural network models of regulatory DNA sequences. Bioinformatics, 34(17):
i629–i637, 09 2018. ISSN 1367-4803. doi: 10.1093/bioinformatics/bty575. URL https:
//doi.org/10.1093/bioinformatics/bty575."
REFERENCES,0.2756892230576441,"Charlotte A Haaxma, Bastiaan R Bloem, George F Borm, Wim JG Oyen, Klaus L Leenders, Silvia
Eshuis, Jan Booij, Dean E Dluzen, and Martin WIM Horstink. Gender differences in parkinson’s
disease. Journal of Neurology, Neurosurgery & Psychiatry, 78(8):819–824, 2007."
REFERENCES,0.2769423558897243,"Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015."
REFERENCES,0.2781954887218045,"Trevor J Hastie. Generalized additive models. In Statistical models in S, pp. 249–307. Routledge,
2017."
REFERENCES,0.2794486215538847,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.2807017543859649,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015."
REFERENCES,0.2819548872180451,"Giles Hooker. Generalized functional anova diagnostics for high-dimensional functions of dependent
variables. Journal of Computational and Graphical Statistics, 16(3):709–732, 2007."
REFERENCES,0.2832080200501253,"Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
251–257, 1991."
REFERENCES,0.2844611528822055,"Luis Itorgo. Regression datasets (accessed: February 12, 2020), 2019. URL https://www.dcc.
fc.up.pt/˜ltorgo/Regression/DataSets.html."
REFERENCES,0.2857142857142857,"Joseph D. Janizek, Pascal Sturmfels, and Su-In Lee. Explaining explanations: Axiomatic feature
interactions for deep networks, 2020."
REFERENCES,0.2869674185463659,"Max E Jerrell. Automatic differentiation and interval arithmetic for estimation of disequilibrium
models. Computational Economics, 10(3):295–316, 1997."
REFERENCES,0.2882205513784461,"Heli Julkunen, Anna Cichonska, Prson Gautam, Sandor Szedmak, Jane Douat, Tapio Pahikkala,
Tero Aittokallio, and Juho Rousu. Leveraging multi-way interactions for systematic prediction of
pre-clinical drug combination effects. Nature communications, 11(1):1–11, 2020."
REFERENCES,0.2894736842105263,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.2907268170426065,"S Kucherenko et al. Derivative based global sensitivity measures and their link with global sensitivity
indices. Mathematics and Computers in Simulation, 79(10):3009–3017, 2009."
REFERENCES,0.29197994987468673,"Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in
applied mathematics, 6(1):4–22, 1985."
REFERENCES,0.2932330827067669,Published as a conference paper at ICLR 2022
REFERENCES,0.29448621553884713,"Benjamin Lengerich, Sarah Tan, Chun-Hao Chang, Giles Hooker, and Rich Caruana. Purifying
interaction effects with the functional anova: An efﬁcient algorithm for recovering identiﬁable
additive models. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 2402–
2412. PMLR, 2020."
REFERENCES,0.2957393483709273,"Zirui Liu, Qingquan Song, Kaixiong Zhou, Ting Hsiang Wang, Ying Shan, and Xia Hu. Towards inter-
action detection using topological analysis on neural networks. arXiv preprint arXiv:2010.13015,
2020."
REFERENCES,0.29699248120300753,"Wei-Yin Loh. Regression trees with unbiased variable selection and interaction detection. Statistica
sinica, pp. 361–386, 2002."
REFERENCES,0.2982456140350877,"Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intelligible models with
pairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 623–631. ACM, 2013."
REFERENCES,0.29949874686716793,"Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 30, pp. 4765–4774. Curran Associates, Inc., 2017."
REFERENCES,0.3007518796992481,"Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair, Ronit
Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global
understanding with explainable AI for trees. Nature Machine Intelligence, 2(1):2522–5839, 2020."
REFERENCES,0.30200501253132833,"Martin Renqiang Min, Xia Ning, Chao Cheng, and Mark Gerstein. Interpretable sparse high-order
boltzmann machines. In Artiﬁcial Intelligence and Statistics, pp. 614–622. PMLR, 2014."
REFERENCES,0.3032581453634085,"Thomas Muehlenstaedt, Olivier Roustant, Laurent Carraro, and Sonja Kuhnt. Data-driven kriging
models based on fanova-decomposition. Statistics and Computing, 22(3):723–738, 2012."
REFERENCES,0.30451127819548873,"R Kelley Pace and Ronald Barry. Sparse spatial autoregressions. Statistics & Probability Letters, 33
(3):291–297, 1997."
REFERENCES,0.3057644110275689,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017."
REFERENCES,0.30701754385964913,"William JE Potts. Generalized additive neural networks. In Proceedings of the ﬁfth ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 194–200, 1999."
REFERENCES,0.3082706766917293,"S. Rendle. Factorization machines. In 2010 IEEE International Conference on Data Mining, pp.
995–1000, 2010."
REFERENCES,0.30952380952380953,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the
predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135–1144, 2016."
REFERENCES,0.3107769423558897,"Olivier Roustant, Jana Fruth, Bertrand Iooss, and Sonja Kuhnt. Crossed-derivative based sensitivity
measures for interaction screening. Mathematics and Computers in Simulation, 105:105–118,
2014."
REFERENCES,0.31203007518796994,"Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 3145–3153. JMLR. org, 2017."
REFERENCES,0.3132832080200501,"Aleksandrs Slivkins et al. Introduction to multi-armed bandits. Foundations and Trends® in Machine
Learning, 12(1-2):1–286, 2019."
REFERENCES,0.31453634085213034,"Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang.
Autoint: Automatic feature interaction learning via self-attentive neural networks. In Proceedings
of the 28th ACM International Conference on Information and Knowledge Management, CIKM
’19, pp. 1161–1170, 2019."
REFERENCES,0.3157894736842105,Published as a conference paper at ICLR 2022
REFERENCES,0.31704260651629074,"Daria Sorokina, Rich Caruana, Mirek Riedewald, and Daniel Fink. Detecting statistical interactions
with additive groves of trees. In Proceedings of the 25th international conference on Machine
learning, pp. 1000–1007. ACM, 2008."
REFERENCES,0.3182957393483709,"Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. The shapley taylor interaction index.
In International Conference on Machine Learning, pp. 9259–9268. PMLR, 2020."
REFERENCES,0.31954887218045114,"Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks with-
out any data by iteratively conserving synaptic ﬂow. Advances in Neural Information Processing
Systems, 33, 2020."
REFERENCES,0.3208020050125313,"Joseph J Thompson, Mark R Blair, Lihan Chen, and Andrew J Henrey. Video game telemetry as a
critical tool in the study of complex skill learning. PloS one, 8(9), 2013."
REFERENCES,0.32205513784461154,"Athanasios Tsanas, Max A Little, Patrick E McSharry, and Lorraine O Ramig. Accurate telemon-
itoring of parkinson’s disease progression by noninvasive speech tests. IEEE transactions on
Biomedical Engineering, 57(4):884–893, 2009."
REFERENCES,0.3233082706766917,"Michael Tsang, Dehua Cheng, and Yan Liu. Detecting statistical interactions from neural network
weights. In International Conference on Learning Representations, 2018a."
REFERENCES,0.32456140350877194,"Michael Tsang, Hanpeng Liu, Sanjay Purushotham, Pavankumar Murali, and Yan Liu. Neural
interaction transparency (NIT): Disentangling learned interactions for improved interpretability. In
NeurIPS, pp. 5809–5818, 2018b."
REFERENCES,0.3258145363408521,"Michael Tsang, Dehua Cheng, Hanpeng Liu, Xue Feng, Eric Zhou, and Yan Liu. Feature interaction
interpretability: A case for explaining ad-recommendation systems via neural interaction detection.
In International Conference on Learning Representations, 2020a."
REFERENCES,0.32706766917293234,"Michael Tsang, Dehua Cheng, Hanpeng Liu, Xue Feng, Eric Zhou, and Yan Liu. Extracting
and leveraging feature interaction interpretations.
In International Conference on Learning
Representations, 2020b."
REFERENCES,0.3283208020050125,"Peter D Turney. Cost-sensitive classiﬁcation: Empirical evaluation of a hybrid genetic decision tree
induction algorithm. Journal of artiﬁcial intelligence research, 2:369–409, 1994."
REFERENCES,0.32957393483709274,"Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019."
REFERENCES,0.3308270676691729,"David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed,
Daniel Johnson, Carin Li, Zinat Sayeeda, et al. Drugbank 5.0: a major update to the drugbank
database for 2018. Nucleic acids research, 46(D1):D1074–D1082, 2018."
REFERENCES,0.33208020050125314,"Thomas H Wonnacott and Ronald J Wonnacott. Introductory statistics, volume 5. Wiley New York,
1990."
REFERENCES,0.3333333333333333,"Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.
Attentional
factorization machines: Learning the weight of feature interactions via attention networks. In
Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, IJCAI’17, pp.
3119–3125, 2017."
REFERENCES,0.33458646616541354,"Bin Yu and Karl Kumbier. Three principles of data science: predictability, computability, and stability
(PCS). arXiv preprint arXiv:1901.08152, 2019."
REFERENCES,0.3358395989974937,"Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, and Quanshi Zhang. Interpreting multivariate
interactions in dnns. arXiv preprint arXiv:2010.05045, 2020."
REFERENCES,0.33709273182957394,Published as a conference paper at ICLR 2022
REFERENCES,0.3383458646616541,"Supplementary Document
Fast Generic Interaction Detection for Model Interpretability
and Compression"
REFERENCES,0.33959899749373434,"A
PROBLEMS WITH ANALYTICAL EVALUATION"
REFERENCES,0.3408521303258145,"In this section, we ﬁrst illustrate the conceptual difference between local and global interaction and
then state the problems of analytical evaluation."
REFERENCES,0.34210526315789475,"Illustration of Local vs. Global Interaction. A quick example showing the difference of lo-
cal/global interaction is the MATLAB symbol1 (Figure 6(a)). There is no local interaction inside the
ﬂat region, but two input variables globally interact."
REFERENCES,0.3433583959899749,"Analytical evaluation works for neural networks with differentiable activation functions, e.g.,
sigmoid, tanh, etc. However, it is problematic when we use neural networks with piece-wise
linear activation functions (PLNN), such as ReLU, Leaky ReLU, and so on. The Hessian will be
a zero matrix at every second-order differentiable point, and it will no longer provide information
about local interaction. Imagine the function landscape is joint with many facets, and the interaction
information is hidden in the boundary of those facets (the set of non-differentiable points)."
REFERENCES,0.34461152882205515,"A simple example is checkerboard-like function, e.g., XOR function F(x1, x2) = 1{x1x2 > 0},
where x1, x2 are uniformly drawn from [−1, 1]. In this case, x1 and x2 are clearly interacted,
however, the second-order derivatives
∂F
∂x1∂x2 will be zero for almost all sampled points. The
interaction information is hidden at the line x1 = 0 and x2 = 0."
REFERENCES,0.3458646616541353,"(a) origin function
(b) approximated function"
REFERENCES,0.34711779448621555,"Figure 6: The MATLAB symbol. (a)Imagine the MATLAB symbol as the landscape of a function
F(x1, x2), there is no interaction locally in the ﬂat region, but x1 and x2 interact globally, i.e., there
is no global decomposition as F(x1, x2) = a(x1) + b(x2). (b) (Problems with analytical evaluation)
The function is approximated by a PLNN, and the landscape is spliced by ﬂat facets. The Hessian
matrix is a zero matrix on almost every point (no local interaction), but x1 and x2 still interact
globally."
REFERENCES,0.3483709273182957,"B
PROOF OF THEOREM 3.1 AND DISCUSSION ON THE CHOICE OF h"
REFERENCES,0.34962406015037595,"Theorem 3.1.
For any x and y, function F shows no interaction between them, i.e., it can be
decomposed as F(x, y) = a(x) + b(y) if and only if, ∀h, k > 0, F(x + h, y + k) −F(x + h, y −
k) −F(x −h, y + k) + F(x −h, y −k) = 0."
REFERENCES,0.3508771929824561,"Proof. (⇒) is trivial. (⇐) We will directly have, ∀h, k > 0 and ∀x, y,"
REFERENCES,0.35213032581453635,"F(x + h, y + k) −F(x + h, y −k)"
REFERENCES,0.3533834586466165,"k
= F(x −h, y + k) −F(x −h, y −k) k
,"
REFERENCES,0.35463659147869675,"F(x + h, y + k) −F(x −h, y + k)"
REFERENCES,0.3558897243107769,"h
= F(x + h, y −k) −F(x −h, y −k) h
."
REFERENCES,0.35714285714285715,1With permission of MathWorks.
REFERENCES,0.3583959899749373,Published as a conference paper at ICLR 2022
REFERENCES,0.35964912280701755,"Let k →0, we will have ∂F"
REFERENCES,0.3609022556390977,"∂y (x + h, y) = ∂F"
REFERENCES,0.36215538847117795,"∂y (x −h, y) for any positive h; let h →0, we will have ∂F"
REFERENCES,0.3634085213032581,"∂x (x, y + k) = ∂F"
REFERENCES,0.36466165413533835,"∂x (x, y −k) for any positive k. Thus, ∂F"
REFERENCES,0.3659147869674185,∂y is irrespective of x and ∂F
REFERENCES,0.36716791979949875,"∂x is irrespective
of y."
REFERENCES,0.3684210526315789,Remark B.1. Theorem 3.1 can be extended for higher-order interaction with the same machinery.
REFERENCES,0.36967418546365916,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
hi 0.75 0.80 0.85 0.90 0.95"
REFERENCES,0.37092731829573933,Ave. AUC score
REFERENCES,0.37218045112781956,Ave. AUC score versus hi on synthetic datasets
REFERENCES,0.37343358395989973,"PID
NID
Our method"
REFERENCES,0.37468671679197996,"Figure 7: Average ROC-AUC score versus hi(j) on synthetic datasets. The error bar represents one
standard deviation (5 folds)."
REFERENCES,0.37593984962406013,"A guide for the choice of hi(j). We empirically test the sensitivity of hi(j), see Figure 7. For
standardized features, we suggest using hi(j) ∈[0.6, 0.9] as the rule-of-thumb."
REFERENCES,0.37719298245614036,"Further discussions on the choice of hi(j). People may argue that for the interaction detection task,
a small number of evaluations for each feature pair are already enough to get pretty good results,
and the beneﬁts of using the UCB algorithm is trivial. This is not always true. We address this
from two perspectives in the following. First, in many scenarios, the function can only be evaluated
sequentially, and the evaluation may be very expensive. The beneﬁts of the UCB algorithm are
revealed then. Second, we empirically study the performance of evaluating each arm evenly under
different conﬁgurations (perturbation size hi(j), and the number of evaluations for each arm), see
Figure 8. The ROC-AUC scores were obtained from the OverparaFC model trained as described in
Section 5.2. We expect that given the ﬁxed perturbation size h, the ROC-AUC scores will rise as the
number of evaluations increases. This is consistent with the observation that the bricks are brighter
from top to bottom. Also, we observed that with a properly chosen h ∈[0.6, 0.9], the interactions
can be correctly detected with a relatively small number of evaluations (the bricks are brighter in the
middle, see dataset 3, 4, and 5). The corresponding reasons can be that (a) if h is too small, four
evaluated points lie in the same ﬂat region with high probability, which is harmful to interaction
detection; (b) if h is too large, some evaluated points may be out of the distribution of the training
data, which makes the detected interaction strength less representative. Even though the suggestion
of using hi(j) ∈[0.6, 0.9] is purely empirical, the plots here help us better understand the essence of
statistical interactions and their detection process."
REFERENCES,0.37844611528822053,"C
PROOF OF THEOREM 3.2"
REFERENCES,0.37969924812030076,"Recall that we construct the 1 −δ conﬁdence intervals of ˆfℓ(r) as,"
REFERENCES,0.38095238095238093,C(ℓ) = ( q
REFERENCES,0.38220551378446116,2σ2 log 2
REFERENCES,0.38345864661654133,"δ
ℓ
if ℓ≤m
0
if ℓ> m
,"
REFERENCES,0.38471177944862156,"µr ∈
h
ˆfℓ(r) −C(ℓ), ˆfℓ(r) + C(ℓ)
i
, w.p. 1 −δ."
REFERENCES,0.38596491228070173,"This is derived from concentration inequality (Inequality 2.9 in Wainwright (2019)). We set δ =
2
n3m,
which is a small number and easy to analyze. We have,
Lemma C.1 (high probability event/clean event). With probability (w.p.) 1 −
2
n2 , all true values µr
lie in the their conﬁdence intervals during the run of the algorithm."
REFERENCES,0.38721804511278196,Published as a conference paper at ICLR 2022
REFERENCES,0.38847117794486213,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.38972431077694236,"1
2
4
8
32
64
128
256"
REFERENCES,0.39097744360902253,# evaluations
REFERENCES,0.39223057644110276,Average AUC on the dataset 1 0.7 0.8 0.9
REFERENCES,0.39348370927318294,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.39473684210526316,"1
2
4
8
32
64
128
256"
REFERENCES,0.39598997493734334,# evaluations
REFERENCES,0.39724310776942356,Average AUC on the dataset 2 0.7 0.8 0.9
REFERENCES,0.39849624060150374,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.39974937343358397,"1
2
4
8
32
64
128
256"
REFERENCES,0.40100250626566414,# evaluations
REFERENCES,0.40225563909774437,Average AUC on the dataset 3 0.8 1.0
REFERENCES,0.40350877192982454,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.40476190476190477,"1
2
4
8
32
64
128
256"
REFERENCES,0.40601503759398494,# evaluations
REFERENCES,0.40726817042606517,Average AUC on the dataset 4 0.8 0.9 1.0
REFERENCES,0.40852130325814534,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.40977443609022557,"1
2
4
8
32
64
128
256"
REFERENCES,0.41102756892230574,# evaluations
REFERENCES,0.41228070175438597,Average AUC on the dataset 5 0.8 1.0
REFERENCES,0.41353383458646614,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.41478696741854637,"1
2
4
8
32
64
128
256"
REFERENCES,0.41604010025062654,# evaluations
REFERENCES,0.41729323308270677,Average AUC on the dataset 6 0.6 0.8
REFERENCES,0.41854636591478694,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.4197994987468672,"1
2
4
8
32
64
128
256"
REFERENCES,0.42105263157894735,# evaluations
REFERENCES,0.4223057644110276,Average AUC on the dataset 7 0.6 0.7
REFERENCES,0.42355889724310775,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.424812030075188,"1
2
4
8
32
64
128
256"
REFERENCES,0.42606516290726815,# evaluations
REFERENCES,0.4273182957393484,Average AUC on the dataset 8 0.8 0.9
REFERENCES,0.42857142857142855,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.4298245614035088,"1
2
4
8
32
64
128
256"
REFERENCES,0.43107769423558895,# evaluations
REFERENCES,0.4323308270676692,Average AUC on the dataset 9 0.7 0.8 0.9
REFERENCES,0.43358395989974935,"0.05
0.35
0.65
0.95
1.25
1.55
h"
REFERENCES,0.4348370927318296,"1
2
4
8
32
64
128
256"
REFERENCES,0.43609022556390975,# evaluations
REFERENCES,0.43734335839599,Average AUC on the dataset 10 0.9 1.0
REFERENCES,0.43859649122807015,"Figure 8: The ROC-AUC scores obtained for 10 synthetic datasets under different conﬁgurations of
perturbation size h (horizontal axis), and the number of evaluations on each interaction pair (vertical
axis). We expect that given the ﬁxed perturbation size h, the ROC-AUC scores will rise as the number
of evaluations increases. This is consistent with the observation that the bricks are brighter from
top to bottom. Also, we observed that with a properly chosen h ∈[0.6, 0.9], the interactions can be
correctly detected with a relatively small number of evaluations (the bricks are brighter in the middle,
see dataset 3, 4, and 5)."
REFERENCES,0.4398496240601504,Proof. It’s equivalent to proof:
REFERENCES,0.44110275689223055,"P({ ∀t, ∀r ∈[n], |µr −ˆµr(t)| ≤Cr(t)}) ≥1 −2 n2"
REFERENCES,0.4423558897243108,The opposite of this event is
REFERENCES,0.44360902255639095,"∃t and r, s.t., |µr −ˆµr(t)| > Cr(t)."
REFERENCES,0.4448621553884712,"Since we have,"
REFERENCES,0.44611528822055135,"P({|µr −ˆµr(t)| > Cr(t)}) ≤δ =
2
n3m,"
REFERENCES,0.4473684210526316,"and we evaluate at most nm times (n arms, each arm is pulled m times),"
REFERENCES,0.44862155388471175,"P({∃t and r, s.t., |µr −ˆµr(t)| > Cr(t)}) ≤δnm = 2"
REFERENCES,0.449874686716792,"n2
(union bound)"
REFERENCES,0.45112781954887216,"P({ ∀t, ∀r ∈[n], |µr −ˆµr(t)| ≤Cr(t)}) ≥1 −2"
REFERENCES,0.4523809523809524,"n2
(clean event)"
REFERENCES,0.45363408521303256,Published as a conference paper at ICLR 2022
REFERENCES,0.4548872180451128,"Using Lemma C.1, we restate Theorem 3.2 as follows."
REFERENCES,0.45614035087719296,Theorem C.2 (Restating Theorem 3.2). With probability 1 −Θ( 1
REFERENCES,0.4573934837092732,"n2 ), Algorithm 1 returns the k-
strongest interactions with at most M ≤ n
X i=1"
REFERENCES,0.45864661654135336,24σ2
REFERENCES,0.4598997493734336,"∆2
i
log(nm)

∧m
"
REFERENCES,0.46115288220551376,"local interaction strength evaluations. This takes O
Pn
i=1 log(n)

σ2 log(nm) ∆2
i"
REFERENCES,0.462406015037594,"
∧m

time."
REFERENCES,0.46365914786967416,"Proof. We ﬁrst analyze the running of algorithm till the ﬁrst strongest interaction comes out. If we
choose to update arm i ̸= i∗
1 at time t, then we have"
REFERENCES,0.4649122807017544,"ˆµi(t) + Ci(t) ≥ˆµi∗
1(t) + Ci∗
1(t).
(6)"
REFERENCES,0.46616541353383456,"For equation 6 to occur, at least one of the following three events must occur:"
REFERENCES,0.4674185463659148,"E1 =

ˆµi∗
1(t) ≤µi∗
1 −Ci∗
1(t)
	
,"
REFERENCES,0.46867167919799496,"E2 = {ˆµi(t) ≥µi + Ci(t)} ,"
REFERENCES,0.4699248120300752,"E3 =
n
∆(1)
i
= µi∗
1 −µi ≤2Ci(t)
o
."
REFERENCES,0.47117794486215536,"To see this, note that if none of E1, E2, E3 occurs, we have"
REFERENCES,0.4724310776942356,"ˆµi(t) + Ci(t)
(¬E2)
<
µi + 2Ci(t)
(¬E3)
<
µi∗
1"
REFERENCES,0.47368421052631576,"(¬E1)
<
ˆµi∗
1 + Ci∗
1(t)."
REFERENCES,0.474937343358396,"From Lemma C.1, E1 and E2 do not occur during the run of the algorithm with probability 1 −
2
n2 ,
because"
REFERENCES,0.47619047619047616,"w.p.

1 −2 n2"
REFERENCES,0.4774436090225564,"
: |µi −ˆµi(t)| ≤Ci(t), ∀i ∈[n], ∀t.
(7)"
REFERENCES,0.47869674185463656,It also implies w.p. 1 −Θ( 1
REFERENCES,0.4799498746867168,"n2 ), the algorithm does not stop pulling arm i until event E3 stops
occurring."
REFERENCES,0.48120300751879697,"Let ζ(w)
i
index the iteration in which Algorithm 1 evaluates arm Ai for the last time before declaring
it to be the w-th strongest interaction. Let’s ﬁrst consider how many pulls are needed for each arm to"
REFERENCES,0.4824561403508772,"pick out the ﬁrst strongest interaction. If Ci(ζ(1)
i
) ≤∆(1)
i
2
for arm Ai, then we can stop evaluating
arm Ai, that is,"
REFERENCES,0.48370927318295737,"∆(1)
i
2
≥ s"
REFERENCES,0.4849624060150376,2σ2 log n3m
REFERENCES,0.48621553884711777,"Ti(ζ(1)
i
)
or Ci(ζ(1)
i
) = 0,"
REFERENCES,0.487468671679198,"=⇒Ti(ζ(1)
i
) ≥
8σ2"
REFERENCES,0.48872180451127817,"(∆(1)
i )2 log(n3m) or Ti(ζ(1)
i
) ≥m."
REFERENCES,0.4899749373433584,"Note that Ti(t) denotes for the number of pulls of arm Ai at iteration t. We will evaluate arm Ai at
most
8σ2"
REFERENCES,0.49122807017543857,"(∆(1)
i
)2 log(n3m) ∧m times. To pick out the ﬁrst strongest interaction, we need at most M"
REFERENCES,0.4924812030075188,"evaluations satisfying M ≤ n
X i=1 8σ2"
REFERENCES,0.49373433583959897,"(∆(1)
i )2 log(n3m) ! ∧m ! ."
REFERENCES,0.4949874686716792,Published as a conference paper at ICLR 2022
REFERENCES,0.49624060150375937,"This result can be extend to ﬁnd all top k strongest interactions concretely, M ≤ n
X i=1 8σ2"
REFERENCES,0.4974937343358396,"(∆(k)
i
)2 log(n3m) ! ∧m ! , = n
X i=1 24σ2"
REFERENCES,0.49874686716791977,"(∆(k)
i
)2 log(n 3√m) ! ∧m ! , ≤ n
X i=1 24σ2"
REFERENCES,0.5,"(∆(k)
i
)2 log(nm) ! ∧m ! ."
REFERENCES,0.5012531328320802,"The Algorithm 1 takes O
Pn
i=1 log(n)

σ2 log(nm) ∆2
i"
REFERENCES,0.5025062656641605,"
∧m

time, where O(log(n)) is the time
for maintaining a priority queue(to ﬁnd the minimal LCB or maximal UCB) in each iteration."
REFERENCES,0.5037593984962406,"Theorem C.3 (Remark 3.1). If we further assume that ∆i ∼N(γ, 1), for some γ, and m = cn, for
c ∈[0, 1], then the expected pulls of arms (over randomness in ∆i) satisﬁes"
REFERENCES,0.5050125313283208,E[M] ≤O(n log(nm) + km)
REFERENCES,0.506265664160401,with probability 1 −Θ( 1
REFERENCES,0.5075187969924813,"n2 ) over randomness in Algorithm 1. Thus, the expected running time is well
bounded by O(n log(nm) log(n) + km log(n))."
REFERENCES,0.5087719298245614,Proof. This follows directly from the Appendix 2 of Bagaria et al. (2018a).
REFERENCES,0.5100250626566416,"D
DATASETS PRE-PROCESSING"
REFERENCES,0.5112781954887218,"D.1
SYNTHETIC TEST SUITE"
REFERENCES,0.5125313283208021,"To make our experimental results convincing, we follow the test suite ever used in Tsang et al. (2018a)
with the details given in Table 4. Among others, F1 is a widely used test function for interaction
detection, which can be generated as described in Sorokina et al. (2008). For all the other functions,
the input dimension is set to 10, and x1, . . . , x10
i.i.d.
∼U(−1, 1)."
REFERENCES,0.5137844611528822,Table 4: Test suite of data-generating functions
REFERENCES,0.5150375939849624,"F1(x)
πx1x2√"
REFERENCES,0.5162907268170426,2x3 −sin−1(x4) + log(x3 + x5) −x9 x10 rx7
REFERENCES,0.5175438596491229,"x8
−x2x7"
REFERENCES,0.518796992481203,"F2(x)
πx1x2p"
REFERENCES,0.5200501253132832,"2|x3| −sin−1(0.5x4) + log(|x3 + x5| + 1) +
x9
1 + |x10|"
REFERENCES,0.5213032581453634,"r
x7
1 + |x8| −x2x7"
REFERENCES,0.5225563909774437,"F3(x)
exp |x1 −x2| + |x2x3| −(x2
3)|x4| + log(x2
4 + x2
5 + x2
7 + x2
8) + x9 +
1
1 + x2
10
F4(x)
exp |x1 −x2| + |x2x3| −(x2
3)|x4| + (x1x4)2 + log(x2
4 + x2
5 + x2
7 + x2
8) + x9 +
1
1 + x2
10
F5(x)
1
1 + x2
1 + x2
2 + x2
3
+
p"
REFERENCES,0.5238095238095238,exp(x4 + x5) + |x6 + x7| + x8x9x10
REFERENCES,0.525062656641604,"F6(x)
exp (|x1x2| + 1) −exp(|x3 + x4| + 1) + cos(x5 + x6 −x8) +
q"
REFERENCES,0.5263157894736842,"x2
8 + x2
9 + x2
10"
REFERENCES,0.5275689223057645,"F7(x)
(arctan(x1) + arctan(x2))2 + max(x3x4 + x6, 0) −
1
1 + (x4x5x6x7x8)2 +

|x7|
1 + |x9| 5
+"
X,0.5288220551378446,"10
X"
X,0.5300751879699248,"i=1
xi"
X,0.531328320802005,"F8(x)
x1x2 + 2x3+x5+x6 + 2x3+x4+x5+x7 + sin(x7 sin(x8 + x9)) + arccos(0.9x10)"
X,0.5325814536340853,"F9(x)
tanh(x1x2 + x3x4)
p"
X,0.5338345864661654,"|x5| + exp(x5 + x6) + log
 
(x6x7x8)2 + 1

+ x9x10 +
1
1 + |x10|
F10(x)
sinh (x1 + x2) + arccos (tanh(x3 + x5 + x7)) + cos(x4 + x5) + sec(x7x9)"
X,0.5350877192982456,"D.2
REAL DATASETS"
X,0.5363408521303258,"All the real datasets are publicly available. We preprocessed the datasets as follows. For Parkinsons
data, we remove the column motor UPDRS and predict the total UPDRS. For SkillCraft data, the"
X,0.5375939849624061,Published as a conference paper at ICLR 2022
X,0.5388471177944862,"target is set to be LeagueIndex. For the Bike sharing data, the feature weather is converted into a one-
hot representation. Before feeding the data into neural networks, we performed data normalization
ﬁrst for all datasets."
X,0.5401002506265664,"The Drug combination dataset is processed as follows. There are 110 features extracted, among
which the ﬁrst 50 features are the concentration of 50 unique FDA-approved drugs, and the last 60
features are the one-hot encodings for the cell lines."
X,0.5413533834586466,"E
ROC-AUC FOR PAIRWISE INTERACTION DETECTION"
X,0.5426065162907269,"We calculate the ROC-AUC scores for the synthetic datasets, where the ground truth interaction pairs
are known. To obtain the ROC-AUC value, two vectors are needed, namely the pairwise interaction
score ∈R45
+ and ground truth ∈{0, 1}45, both of them are of dimension p(p−1)"
X,0.543859649122807,"2
= 45. By setting
different thresholds for the pairwise interaction score ranking, different classiﬁers with False Positive
(FP) rate and True Positive (TP) rate can be obtained. The ROC-AUC value is approximated from
those (FP, TP) pairs by the trapezoidal rule."
X,0.5451127819548872,"F
PAIRWISE INTERACTION DETECTION ON HIGH- DIMENSIONAL DATASETS"
X,0.5463659147869674,"We created two new high-dimensional datasets by simply combining the 10 synthetic datasets
considered in the paper. The new datasets have either 50 (using F1-F5) or 100 features (using F1-F10).
The labels of the two datasets are the sum of the original labels in each dataset of input dimension
p = 10. The training sample size is increased to 500,000 to mimic big data. We compared 5 different
model architectures and our method consistently outperforms NID, see the Table 5 below for the
results."
X,0.5476190476190477,Table 5: ROC-AUCs comparison for high-dimensional datasets
X,0.5488721804511278,"ROC-AUC score (NID/Ours)
data size: 500,000 * 100
data size: 500,000 * 50"
X,0.550125313283208,"Big network1 with main effect + L1reg
0.743/0.768
0.831/0.864
Big network without main effect + L1reg
0.699/0.700
0.803/0.855
Small network2 with main effect + L1reg
0.731/0.744
0.836/0.859
Small network without main effect + L1reg
0.701/0.742
0.796/0.840
Standard big network without main effect
0.646/0.653
0.583/0.793"
X,0.5513784461152882,"1 Big network: p-5000-900-400-100-30-1
2 Small network: p-140-100-60-20-1"
X,0.5526315789473685,"Here we also report the computational complexity of our UCB-based interaction detection method.
Due to the randomness of the UCB algorithm, the number of pulls ﬂuctuates. Note that we select
top k = 200 interaction pairs for 100-dimensional case, and the total number of pulls is around
40000, while the naive approach needs 100 × 100 × 99/2 = 495000 pulls. We select top k = 100
interaction pairs for 50-dimensional case, and the total number of pulls is around 21000, while the
naive approach needs 100 × 50 × 49/2 = 122500 pulls."
X,0.5538847117794486,"Discussion on High Dimensional Data: For high dimensional data, the Hessian matrix will be
too huge to handle. One possible solution is to focus on the important features. We may ﬁrst take
advantage of DeepLIFT(Shrikumar et al., 2017) or SHAP (Lundberg & Lee, 2017) to screen out the
most important features, and then apply our interaction detection method. If the number of important
features is still too large to handle, we may use ANOVA and F-test to select an affordable number of
interaction candidates to pull log(n) times, and explore the remaining pairs with a small probability,
say 5%, like the common strategy used in reinforcement learning.."
X,0.5551378446115288,"G
EXTENSION TO HIGHER-ORDER INTERACTIONS"
X,0.556390977443609,"We deﬁne three-way interaction as follows, and higher-order interactions can be deﬁned similarly."
X,0.5576441102756893,Published as a conference paper at ICLR 2022
X,0.5588972431077694,"Deﬁnition G.1 (three-way interaction). A function F : Rp →R is said to exhibit an interaction
among three of its variables xi, xj and xk if, Ex"
X,0.5601503759398496,"
∂3F(x)
∂xi∂xj∂xk"
X,0.5614035087719298,"2
> 0."
X,0.5626566416040101,"The naive method to detect higher-order interactions is quite similar to FANOVA graph (Muehlen-
staedt et al., 2012). We give three examples of detecting higher-order interactions from the synthetic
data. We detect the pairwise interaction strength ﬁrst, then build a weighted graph (the edge weight
corresponds to the interaction strength). By setting a proper number of clusters, higher-order interac-
tions can be discovered correctly, see Figure 9."
X,0.5639097744360902,(a) y = x0x1 + x2x3x4 + x5x6x7 + x8x9
X,0.5651629072681704,"(b) y = e|x0−x1| ∗x9 + |x2 ∗x3| −x2|x5|
4
+ (x6x7)2 + x8
(c) y = x0x1 + |x1 + x2x3| + x4x5 −(x2
5)x6 −ex7+x8x9 0 1
2 3 4 5 6 7 8 9 0
1 2
3 4 5 6 7 8 9 0 1 2 3 4 5 6 7
8 9"
X,0.5664160401002506,"(a)
(b)
(c)"
X,0.5676691729323309,"Figure 9: The nodes (features) are clustered correctly. The color of edges indicates pairwise interaction
strength. Nodes with the same color belong to the same cluster."
X,0.568922305764411,Algorithm 2 Hierarchical Higher-Order Interaction Detection
X,0.5701754385964912,"Require: The target number of i-way interactions {k(i)}p
i=2.
1: Detect top-k(2) pairwise interaction via UCB algorithm.
2: Construct an undirected graph G based on the detected pairwise interactions.
3: Enumerate all the cliques C in the graph G.
4: for i = 3, 4, · · · , p do
5:
if C(i) = ∅then
6:
break
7:
end if
8:
Find C(i)
reﬁne = {C ∈C(i) | every i −1 complete subgraph of C admits a detected interaction}"
X,0.5714285714285714,"9:
Detect the top k(i)-strongest i-way interactions in C(i)
reﬁne via the UCB algorithm.
10: end for
11: return All the detected interactions with their strengths."
X,0.5726817042606517,"One drawback of the above naive clustering method is that it doesn’t allow for the overlap of nodes,
i.e. one variable may appear in several interactions (F8 in the test suite). To address the above
problem, we propose the Algorithm 2, which detects the higher-order interactions hierarchically.
We ﬁrst construct an undirected graph G from the detected pairwise interactions. To shrink our
search space for higher-order interactions, we restrict ourselves to the set of all cliques C of the
graph G, where a clique C ∈C in a graph is a subset of vertices that are all joined by edges. We
use C(i) ⊆C to denote the collection of the i-cliques (the cliques with i vertices). For example,
to detect 3-way interactions, one only needs to search in the C(3), which contains the cliques with
three vertices. Furthermore, we can shrink the search space to C(i)
reﬁne for i-way interactions, based
on the detected (i −1)-way interactions (see line 8 in Algorithm 2). For example, while detecting
4-way interactions, we put 4-clique {2, 3, 4, 6} into consideration only if all the 3-way interactions
{3, 4, 6}, {2, 4, 6}, {2, 3, 6}, {3, 4, 6} exist. We then use the UCB algorithm to verify if the cliques"
X,0.5739348370927319,Published as a conference paper at ICLR 2022
X,0.575187969924812,"in C(i)
reﬁne admit the interaction relationship. In this way, we detect the i-way interactions from the
detected (i −1)-way interactions information."
X,0.5764411027568922,"The 3-way interaction strength can be calculated from Equation 8. However, with the order of
interaction increasing, the number of function evaluations for one gradient computation increases
geometrically. We do not recommend detecting the interaction whose order is higher than four,
which is uneconomical to compute and hard to interpret. We found our proposed Algorithm 2 can
successfully detect the higher-order interactions for F8 in the test suite 2."
X,0.5776942355889725,"∂2F(x)
∂xi∂xj∂xk
≈
1
8h3 [+F(x + h(ei + ej + ek)) + F(x + h(−ei −ej + ek))"
X,0.5789473684210527,"+ F(x + h(ei −ej −ek)) + F(x + h(−ei + ej −ek))
−F(x + h(−ei + ej + ek)) −F(x + h(ei −ej + ek))
−F(x + h(ei + ej −ek)) −F(x + h(−ei −ej −ek))] (8)"
X,0.5802005012531328,"H
MORE ON THE PARAMETRIC ACE"
X,0.581453634085213,"In section 4, we introduced a parametric ACE model based on the following generalized linear
additive model with main effects and pairwise feature interactions, namely,"
X,0.5827067669172933,"θ(Y ) = p
X"
X,0.5839598997493735,"i=1
βisi (xui; θsi) + R
X"
X,0.5852130325814536,"i=1
βp+iri (xIi; θri) + ϵ.
(9)"
X,0.5864661654135338,"The consideration of interactions improves the model performance signiﬁcantly, compare to General-
ized Additive Neural Networks (GANN)(Potts, 1999; Agarwal et al., 2020)3, see Table 6."
X,0.5877192982456141,"Table 6: NRMSE of the GANN versus our proposed ParaACE network on the synthetic datasets in
(Table 4). (The results were averaged over 5 folds.)"
X,0.5889724310776943,"F1
F2
F3
F4
F5
F6
F7
F8
F9
F10
average
CR"
X,0.5902255639097744,"GANN
0.033
0.063
0.064
0.068
0.053
0.052
0.028
0.032
0.040
0.033
0.046
959
ParaACE
0.025
0.035
0.031
0.030
0.038
0.025
0.016
0.019
0.023
0.021
0.026
283"
X,0.5914786967418546,"The idea behind the original nonparametric ACE algorithm (Breiman & Friedman, 1985) is to
alternate between an inner process for ﬁnding the optimal transformation functions of the inputs and
an outer process for ﬁnding the optimal transformation function of the output."
X,0.5927318295739349,"Here, we made several modiﬁcations. First, all the transformation functions are represented by small
subnetworks. Second, we reformulate the above regression equation approximately as"
X,0.5939849624060151,"Y ≈θ−1
 p
X"
X,0.5952380952380952,"i=1
βisi (xui; θsi) + R
X"
X,0.5964912280701754,"i=1
βp+iri (xIi; θri) !"
X,0.5977443609022557,".
(10)"
X,0.5989974937343359,"Therefore, the ﬁx-up layer shown in Figure 3 can be understood as the inverse optimal transformation
of the output θ−1(·). To be more general, θ−1(·) takes all input transformations as individuals instead
of their high-level summary (weighted sum). Our parametric ACE then alternately tunes the ﬁx-up
layer conditioned on the current optimal transformation layer (outer iteration) and tunes the optimal
transformation layer conditioned on the current ﬁx-up layer until some convergence condition is met."
X,0.600250626566416,"Another important function of the ﬁx-up layer is to alleviate the negative impacts of wrongly detected
pairwise interactions and/or undetected higher-order interactions altogether on the output. For
this purpose, we could make this ﬁx-up layer a network of small subnetworks, so that the whole
architecture is a network of small subnetworks. Each subnetwork can be regarded as a meta-neuron
that is expected to be more competent than any SOTA activation function. With such a nice structure,"
X,0.6015037593984962,"2A demo can be found at https://github.com/zhangtj1996/ParaACE.
3GANN is a parametric version of GAM, which only considers the transformations of univariates (single
features)."
X,0.6027568922305765,Published as a conference paper at ICLR 2022
X,0.6040100250626567,"we have divided the hyper-parameters naturally into blocks and the whole network can hopefully be
made adaptive to new tasks more rapidly by tuning just a few blocks."
X,0.6052631578947368,"It is possible for ParaACE to learn “contradictory” models, which present the same function
(Lengerich et al., 2020). For example, the main effect might be absorbed into the interaction
effects. Thus the direct interpretation through subnetworks may not sound that convincing. Lengerich
et al. (2020) proposed an algorithm to purify the interaction effect by calculating the functional
ANOVA (Hooker, 2007), based on a piecewise-constant function ˆF. This algorithm is applicable
to any function F by ﬁrst constructing a piecewise-constant approximation ˆF. Particularly, for our
ParaACE model, the overall computation will be much easier since we explicitly have the univariate
and bi-variate feature transformation. The number of bins required to approximate ParaACE with ˆF
reduces signiﬁcantly, and so does the complexity of the follow-up purifying algorithm."
X,0.606516290726817,"I
EXTENSION TO CLASSIFICATION TASK"
X,0.6077694235588973,"We generated the datasets for binary classiﬁcation from the synthetic regression datasets (1000
samples with injected noise). We choose the median of the target as a threshold to separate each
synthetic regression the dataset into two classes. The comparison between the baseline OverparaFC
and our proposed ParaACE in terms of classiﬁcation accuracy is shown in Table 7. For real-world
datasets, we picked Higgs Boson (Baldi et al., 2014), Spambase (Dua & Graff, 2017), and Diabetes
(Turney, 1994).Both the classiﬁcation accuracy and the compression ratio are reported in Table 8."
X,0.6090225563909775,"Table 7: Accuracy of the baseline OverparaFC versus our proposed ParaACE network on the synthetic
classiﬁcation datasets. The results were averaged over 5 folds."
X,0.6102756892230576,"F1
F2
F3
F4
F5
F6
F7
F8
F9
F10
Average
CR"
X,0.6115288220551378,"OverparaFC
0.955
0.896
0.887
0.872
0.907
0.939
0.964
0.971
0.932
0.953
0.927
1
ParaACE
0.949
0.908
0.94
0.946
0.916
0.94
0.96
0.935
0.919
0.942
0.936
283"
X,0.6127819548872181,"Table 8: Performance comparison between OverparaFC and ParaACE on various real-world classiﬁ-
cation datasets."
X,0.6140350877192983,"OverparaFC
ParaACE
Datasets
N
p
Accuracy
Parameters
Accuracy
CR"
X,0.6152882205513784,"Higgs Boson
98050
28
0.698 ± 3.0e −3
5049461
0.730 ± 2.8e −3
184
Spambase
4601
57
0.950 ± 8.0e −3
5194461
0.950 ± 7.9e −2
120
Diabetes
768
8
0.741 ± 2.5e −2
4949461
0.789 ± 1.7e −2
302"
X,0.6165413533834586,"J
DETECTED PAIRWISE INTERACTIONS FOR SYNTHETIC AND REAL DATA"
X,0.6177944862155389,Synthetic data: Figure 10 shows the pairwise interaction strength produced by our proposed method.
X,0.6190476190476191,"Real data: We provide the detected interactions for ﬁve real datasets in Figure 11. For the Cal
housing dataset, we terminate the UCB algorithm when k = 20 interactions stood out; for the other
four datasets, we terminate at k = 50. The green, orange, and blue points represent the UCB,
estimated mean, and LCB respectively. For the drug combination dataset, the results are shown in
Figure 12."
X,0.6203007518796992,"K
EMPIRICAL RESULT FOR SAMPLE EFFICIENCY"
X,0.6215538847117794,"Figure 13 shows our proposed framework is sample efﬁcient. The data are from the test suite (Table 4),
and noise η is injected. Increasing the training sample size, both over-parameterized neural nets and
our proposed ParaACE perform better. But ParaACE is less data-hungry since fewer training samples"
X,0.6228070175438597,Published as a conference paper at ICLR 2022
X,0.6240601503759399,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X
X X X X X X X
X X"
X,0.62531328320802,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X
X X X X X X X
X X"
X,0.6265664160401002,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X X X X
X X
X
X"
X,0.6278195488721805,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X X X X
X X
X
X X"
X,0.6290726817042607,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X
X X X X X
X"
X,0.6303258145363408,"F1
F2
F3
F4
F5"
X,0.631578947368421,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X X X
X X X
X"
X,0.6328320802005013,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X X X
X
X X
X
X"
X,0.6340852130325815,"X
X
X
X X"
X,0.6353383458646616,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X X
X X
X X
X
X X"
X,0.6365914786967418,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X
X X
X
X"
X,0.6378446115288221,"X
X
X
X X X X
X X"
X,0.6390977443609023,"x1 x2 x3 x4 x5 x6 x7 x8 x9x10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 X X
X X
X X"
X,0.6403508771929824,"F6
F7
F8
F9
F10"
X,0.6416040100250626,"Figure 10: Heat maps of pairwise interaction strength proposed by our method for functions F1-F10
(Table 4). Cross-marks indicate the ground truth interactions."
X,0.6428571428571429,"are needed for ParaACE to achieve a similar performance of the over-parameterized neural nets. The
experiments show that ParaACE still performs well, even if with small training samples."
X,0.6441102756892231,"L
DETAILS ON THE COMPARISON WITH KD, LTH, AND SYNFLOW"
X,0.6453634085213033,"L.1
COMPARISON WITH KD"
X,0.6466165413533834,"KD is widely used for multi-class classiﬁcation problems, which extracts knowledge from the ”soft
label” by controlling the temperature, but fewer KD methods are there for regression problems.
Currently, people mainly use the Teacher bounded regression loss (Chen et al., 2017) and the hint loss
to deal with the objective detection problem. Passing unlabeled data to the Teacher model to produce
pseudo labels can also help. In these ways, the Student is expected to have a similar performance to
the Teacher."
X,0.6478696741854637,"We let an over-parameterized FC (10-5000-900-400-100-30-1) be the Teacher, and a ReLU network
with architecture (10-70-70-70-70-30-1) be the Student. The Student is trained with 800 original
data and 4000 pseudo data (labeled by the Teacher). The loss function we adopted is L2 + Hint loss,
where the Hint loss is used on the layer with 30 neurons."
X,0.6491228070175439,"It shows that KD trained with pseudo data and carefully designed loss function achieves similar
performance as the Teacher, while ParaACE trained with original data and simple L2 loss achieves
signiﬁcantly better performance."
X,0.650375939849624,"L.2
COMPARISON WITH LTH"
X,0.6516290726817042,"We used the pruning technique proposed in the lottery tickets hypothesis (LTH) paper (Frankle &
Carbin, 2019). We pruned the fully connected neural network after every 20 epochs. Every time 20
% weights were pruned in each layer except for the last layer. We trained 500 epochs, and set the
batch size to be 400. We report the best test performance in each round of pruning in Figure 14."
X,0.6528822055137845,"Figure 14 shows that, in most cases pruning the network properly can improve the model performance.
But if the network was over-pruned, the performance may drop."
X,0.6541353383458647,"L.3
COMPARISON WITH SYNFLOW"
X,0.6553884711779449,"We implemented the single shot SynFlow for regression tasks based on the ofﬁcial code repository
https://github.com/ganguli-lab/Synaptic-Flow (Tanaka et al., 2020). The sparsity is set to 10−2.447
for synthetic datasets, thus the neural network is compressed 280 times. For real-world datasets, we"
X,0.656641604010025,Published as a conference paper at ICLR 2022
X,0.6578947368421053,"climbRate
Sgzpq 
curRoll
absRoll
diffClb
diffRollRate
diffDiffClb 
SaTime1
SaTime2
SaTime3
SaTime4
diffSaTime1
diffSaTime2
diffSaTime3
diffSaTime4Sa"
X,0.6591478696741855,climbRate Sgz
X,0.6604010025062657,"p
q 
curRoll
absRoll"
X,0.6616541353383458,"diffClb
diffRollRate"
X,0.6629072681704261,diffDiffClb
X,0.6641604010025063,"SaTime1
SaTime2
SaTime3
SaTime4
diffSaTime1
diffSaTime2
diffSaTime3
diffSaTime4 Sa"
X,0.6654135338345865,Elevators
X,0.6666666666666666,"0
20
40
60
80
100
120
140
160
Arms' index 0.0 0.1 0.2 0.3 0.4 0.5"
X,0.6679197994987469,Confidence interval for each arm
X,0.6691729323308271,subject# age
X,0.6704260651629073,"sex
test_time"
X,0.6716791979949874,Jitter(%)
X,0.6729323308270677,Jitter(Abs)
X,0.6741854636591479,Jitter:RAP
X,0.6754385964912281,Jitter:PPQ5
X,0.6766917293233082,Jitter:DDP
X,0.6779448621553885,"Shimmer
Shimmer(dB)"
X,0.6791979949874687,Shimmer:APQ3
X,0.6804511278195489,Shimmer:APQ5
X,0.681704260651629,Shimmer:APQ11
X,0.6829573934837093,Shimmer:DDA NHR HNR RPDE DFA PPE
X,0.6842105263157895,subject#
X,0.6854636591478697,"age
sex
test_time"
X,0.6867167919799498,"Jitter(%)
Jitter(Abs)"
X,0.6879699248120301,"Jitter:RAP
Jitter:PPQ5"
X,0.6892230576441103,Jitter:DDP
X,0.6904761904761905,"Shimmer
Shimmer(dB)
Shimmer:APQ3
Shimmer:APQ5
Shimmer:APQ11"
X,0.6917293233082706,Shimmer:DDA
X,0.6929824561403509,"NHR
HNR
RPDE DFA PPE"
X,0.6942355889724311,Parkinsons
X,0.6954887218045113,"0
25
50
75
100
125
150
175
200
Arms' index 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175"
X,0.6967418546365914,Confidence interval for each arm
X,0.6979949874686717,"id
Age
HoursPerWeek
TotalHours
APM
SelectByHotkeys
AssignToHotkeys
UniqueHotkeys
MinimapAttacks
MinimapRightClicks
NumberOfPACs
GapBetweenPACs
ActionLatency
ActionsInPAC
TotalMapExplored
WorkersMade
UniqueUnitsMade
ComplexUnitsMade
ComplexAbilitiesUsed"
X,0.6992481203007519,"id
Age
HoursPerWeek"
X,0.7005012531328321,TotalHours
X,0.7017543859649122,"APM
SelectByHotkeys
AssignToHotkeys"
X,0.7030075187969925,"UniqueHotkeys
MinimapAttacks
MinimapRightClicks"
X,0.7042606516290727,"NumberOfPACs
GapBetweenPACs"
X,0.7055137844611529,ActionLatency
X,0.706766917293233,"ActionsInPAC
TotalMapExplored"
X,0.7080200501253133,"WorkersMade
UniqueUnitsMade
ComplexUnitsMade
ComplexAbilitiesUsed"
X,0.7092731829573935,SkillCraft
X,0.7105263157894737,"0
25
50
75
100
125
150
175
Arms' index 0.015 0.010 0.005 0.000 0.005 0.010 0.015"
X,0.7117794486215538,Confidence interval for each arm
X,0.7130325814536341,season
X,0.7142857142857143,"yr
mnth"
X,0.7155388471177945,"hr
holiday"
X,0.7167919799498746,weekday
X,0.7180451127819549,workingday temp atemp
X,0.7192982456140351,"hum
windspeed"
X,0.7205513784461153,weathersit_1
X,0.7218045112781954,weathersit_2
X,0.7230576441102757,weathersit_3
X,0.7243107769423559,weathersit_4
X,0.7255639097744361,season
X,0.7268170426065163,"yr
mnth"
X,0.7280701754385965,"hr
holiday
weekday
workingday"
X,0.7293233082706767,"temp
atemp"
X,0.7305764411027569,"hum
windspeed
weathersit_1
weathersit_2
weathersit_3
weathersit_4"
X,0.731829573934837,Bike sharing
X,0.7330827067669173,"0
20
40
60
80
100
Arms' index 0.02 0.00 0.02 0.04 0.06 0.08"
X,0.7343358395989975,Confidence interval for each arm
X,0.7355889724310777,longitude
X,0.7368421052631579,"latitude
housingMedianAge"
X,0.7380952380952381,totalRooms
X,0.7393483709273183,totalBedrooms
X,0.7406015037593985,population
X,0.7418546365914787,households
X,0.7431077694235589,medianIncome
X,0.7443609022556391,longitude
X,0.7456140350877193,latitude
X,0.7468671679197995,housingMedianAge
X,0.7481203007518797,totalRooms
X,0.7493734335839599,totalBedrooms
X,0.7506265664160401,population
X,0.7518796992481203,households
X,0.7531328320802005,medianIncome
X,0.7543859649122807,Cal housing
X,0.7556390977443609,"0
5
10
15
20
25
Arms' index 0.025 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175"
X,0.7568922305764411,Confidence interval for each arm
X,0.7581453634085213,"Figure 11: Left: Heat maps of pairwise interaction strengths on real datasets. Right: Conﬁdence
interval for each interaction pair. The green, orange, blue points denote the UCB, estimated mean,
and LCB respectively."
X,0.7593984962406015,Published as a conference paper at ICLR 2022
FDA APPROVED DRUGS,0.7606516290726817,"50 FDA approved drugs
60 cell lines"
FDA APPROVED DRUGS,0.7619047619047619,"Amifostine; Cabazitaxel
Estramustine phosphate sodium; Mitoxantrone
Thioguanine; Gefitinib
Thioguanine; Mitoxantrone
Thioguanine; Cytarabine hydrochloride
Thioguanine; Cabazitaxel
Thioguanine; Floxuridine
Gefitinib; Mitoxantrone
Gefitinib; Cytarabine hydrochloride
Gefitinib; Cabazitaxel
Ruxolitinib; Vinorelbine tartrate
Teniposide; Cytarabine hydrochloride
Teniposide; Cabazitaxel
Vinorelbine tartrate; Axitinib
Vinorelbine tartrate; Vandetanib
Vinorelbine tartrate; Cytarabine hydrochloride
Crizotinib; Mitoxantrone"
FDA APPROVED DRUGS,0.7631578947368421,"Vandetanib; Mitoxantrone
Vismodegib; Cabazitaxel
Megestrol acetate; Mitoxantrone
Tamoxifen citrate; Cabazitaxel
Quinacrine hydrochloride; Mitoxantrone
Vincristine sulfate; Cabazitaxel
Uracil mustard; Cytarabine hydrochloride
Mitoxantrone; Cytarabine hydrochloride
Mitoxantrone; Cabazitaxel
Mitoxantrone; Floxuridine
Cytarabine hydrochloride; Arsenic trioxide
Cytarabine hydrochloride; Cabazitaxel
Cytarabine hydrochloride; Everolimus
Cytarabine hydrochloride; Floxuridine
Cabazitaxel; Everolimus
Cabazitaxel; Floxuridine
Everolimus; Floxuridine"
FDA APPROVED DRUGS,0.7644110275689223,"Figure 12: Heat maps of pairwise interaction strengths for drug combination data. The top 34 detected
interactions are listed, among which 15 of them (marked in red) are veriﬁed in the DrugBank (Wishart
et al., 2018)."
FDA APPROVED DRUGS,0.7656641604010025,"compressed the model by 120 times. And we retrained the neural network after the single-shot prune
(post-training). Note that we did not prune the biases here."
FDA APPROVED DRUGS,0.7669172932330827,"We set the hyper parameters for the post-training process as follows. The batch size is set to be 500,
and the number of epochs is 100. We use Adam as the optimizer with lr = 0.001, betas = (0.9, 0.99)."
FDA APPROVED DRUGS,0.768170426065163,"M
ACCURATE APPROXIMATION FUNCTION BENEFITS INTERACTION
DETECTION"
FDA APPROVED DRUGS,0.7694235588972431,"Suppose the underlying ground truth function is f(x), and our approximation function is
gθ(x), we omit the θ in the following.
We show that the estimation error for Hessian
Ex
h
∂2g(x)
∂xi∂xj"
FDA APPROVED DRUGS,0.7706766917293233,"i
−Ex
h
∂2f(x)
∂xi∂xj"
FDA APPROVED DRUGS,0.7719298245614035,"i is bounded by o(ϵ), when |f(x) −g(x)| < ϵ for all x. This im-
plies more accurate pre-trained model leads to better interaction detection accuracy. According to the
following theorem, deep neural networks are good surrogate functions due to the universal function
approximation capability (Hornik, 1991)."
FDA APPROVED DRUGS,0.7731829573934837,"Theorem M.1. Assume xi are uniformly drawn from [−1, 1] independently, if there exist an ϵ > 0,
such that |f(x) −g(x)| ≤ϵ for all x ∈Rp, then Ex"
FDA APPROVED DRUGS,0.7744360902255639, ∂2g(x)
FDA APPROVED DRUGS,0.7756892230576441,"∂xi∂xj 
−Ex"
FDA APPROVED DRUGS,0.7769423558897243,∂2f(x)
FDA APPROVED DRUGS,0.7781954887218046,"∂xi∂xj 
≤ϵ."
FDA APPROVED DRUGS,0.7794486215538847,Proof. Ex
FDA APPROVED DRUGS,0.7807017543859649, ∂2g(x)
FDA APPROVED DRUGS,0.7819548872180451,∂xi∂xj 
FDA APPROVED DRUGS,0.7832080200501254,"=
Z ∂2g(x)"
FDA APPROVED DRUGS,0.7844611528822055,"∂xi∂xj
p(x)dx"
FDA APPROVED DRUGS,0.7857142857142857,"=
Z ∂2g(x)"
FDA APPROVED DRUGS,0.7869674185463659,"∂xi∂xj
p(xi, xj)p(x\ij)dx"
FDA APPROVED DRUGS,0.7882205513784462,"=
Z
p(x\ij)
Z 1 −1 Z 1 −1"
FDA APPROVED DRUGS,0.7894736842105263,"∂2g(x)
∂xi∂xj
p(xi, xj)dxidxjdx\ij"
FDA APPROVED DRUGS,0.7907268170426065,"[where p(xi, xj) = 1/4] =1 4"
FDA APPROVED DRUGS,0.7919799498746867,"Z
p(x\ij)
Z 1 −1 Z 1 −1"
FDA APPROVED DRUGS,0.793233082706767,"∂2g(x)
∂xi∂xj
dxidxjdx\ij,"
FDA APPROVED DRUGS,0.7944862155388471,Published as a conference paper at ICLR 2022
FDA APPROVED DRUGS,0.7957393483709273,"where
Z 1 −1 Z 1 −1"
FDA APPROVED DRUGS,0.7969924812030075,"∂2g(x)
∂xi∂xj
dxidxj =
Z 1 −1"
FDA APPROVED DRUGS,0.7982456140350878,"∂g(x\ij, xi = 1, xj) −∂g(x\ij, xi = −1, xj)"
FDA APPROVED DRUGS,0.7994987468671679,"∂xj
dxj"
FDA APPROVED DRUGS,0.8007518796992481,"=
g(x\ij, xi = 1, xj = 1) −g(x\ij, xi = 1, xj = −1)"
FDA APPROVED DRUGS,0.8020050125313283,"−(g(x\ij, xi = −1, xj = 1) −g(x\ij, xi = −1, xj = −1))."
FDA APPROVED DRUGS,0.8032581453634086,"We can derive Ex
h
∂2f(x)
∂xi∂xj"
FDA APPROVED DRUGS,0.8045112781954887,"i
in a similar fashion. Thus, Ex"
FDA APPROVED DRUGS,0.8057644110275689, ∂2g(x)
FDA APPROVED DRUGS,0.8070175438596491,"∂xi∂xj 
−Ex"
FDA APPROVED DRUGS,0.8082706766917294,∂2f(x)
FDA APPROVED DRUGS,0.8095238095238095,∂xi∂xj  =1
FDA APPROVED DRUGS,0.8107769423558897,"4Ex\ij[g(x\ij, xi = 1, xj = 1) −f(x\ij, xi = 1, xj = 1)"
FDA APPROVED DRUGS,0.8120300751879699,"−g(x\ij, xi = −1, xj = 1) + f(x\ij, xi = −1, xj = 1)"
FDA APPROVED DRUGS,0.8132832080200502,"−g(x\ij, xi = 1, xj = −1) + f(x\ij, xi = 1, xj = −1)"
FDA APPROVED DRUGS,0.8145363408521303,"+ g(x\ij, xi = −1, xj = −1) −f(x\ij, xi = −1, xj = −1)] ≤1"
FDA APPROVED DRUGS,0.8157894736842105,4Ex\ij[4ϵ] =ϵ.
FDA APPROVED DRUGS,0.8170426065162907,"Theorem M.2. Assume xi are uniformly drawn from [−1, 1] independently, and further assume
 ∂2f(x)"
FDA APPROVED DRUGS,0.818295739348371,∂xi∂xj
FDA APPROVED DRUGS,0.8195488721804511,",
 ∂2g(x)"
FDA APPROVED DRUGS,0.8208020050125313,∂xi∂xj
FDA APPROVED DRUGS,0.8220551378446115,"≤M, ∀(xi, xj). If there exist an ϵ > 0, such that |f(x) −g(x)| ≤ϵ for all
x ∈Rp, then Ex"
FDA APPROVED DRUGS,0.8233082706766918,"""
∂2g(x)
∂xi∂xj  2# −Ex"
FDA APPROVED DRUGS,0.8245614035087719,"""
∂2f(x)
∂xi∂xj  2# ≤2Mϵ."
FDA APPROVED DRUGS,0.8258145363408521,Proof. Ex
FDA APPROVED DRUGS,0.8270676691729323,"""
∂2g(x)
∂xi∂xj  2# −Ex"
FDA APPROVED DRUGS,0.8283208020050126,"""
∂2f(x)
∂xi∂xj  2# =Ex"
FDA APPROVED DRUGS,0.8295739348370927,"""
∂2g(x)
∂xi∂xj "
FDA APPROVED DRUGS,0.8308270676691729,"2
−

∂2f(x)
∂xi∂xj  2# =Ex"
FDA APPROVED DRUGS,0.8320802005012531, ∂2g(x)
FDA APPROVED DRUGS,0.8333333333333334,"∂xi∂xj
−∂2f(x)"
FDA APPROVED DRUGS,0.8345864661654135,∂xi∂xj
FDA APPROVED DRUGS,0.8358395989974937,  ∂2g(x)
FDA APPROVED DRUGS,0.8370927318295739,"∂xi∂xj
+ ∂2f(x)"
FDA APPROVED DRUGS,0.8383458646616542,∂xi∂xj 
FDA APPROVED DRUGS,0.8395989974937343,"≤
Ex"
FDA APPROVED DRUGS,0.8408521303258145, ∂2g(x)
FDA APPROVED DRUGS,0.8421052631578947,"∂xi∂xj
−∂2f(x)"
FDA APPROVED DRUGS,0.843358395989975,∂xi∂xj
FDA APPROVED DRUGS,0.8446115288220551," ·
Ex"
FDA APPROVED DRUGS,0.8458646616541353, ∂2g(x)
FDA APPROVED DRUGS,0.8471177944862155,"∂xi∂xj
+ ∂2f(x)"
FDA APPROVED DRUGS,0.8483709273182958,∂xi∂xj 
FDA APPROVED DRUGS,0.849624060150376,"≤ϵ · 2M
(Using Theorem M.1)"
FDA APPROVED DRUGS,0.8508771929824561,"Note that M can be interpreted as the magnitude of the strongest local interaction. For the functions
with strong interactions, higher approximation quality is desired."
FDA APPROVED DRUGS,0.8521303258145363,Published as a conference paper at ICLR 2022
FDA APPROVED DRUGS,0.8533834586466166,"Table 9: Ablation study to show the effectiveness of interaction detection on the synthetic datasets in
(Table 4). (The results were averaged over 5 folds.)"
FDA APPROVED DRUGS,0.8546365914786967,"F1
F2
F3
F4
F5
F6
F7
F8
F9
F10
average
CR"
FDA APPROVED DRUGS,0.8558897243107769,"ParaACE (with random interaction)
0.026
0.042
0.035
0.034
0.060
0.032
0.017
0.025
0.044
0.031
0.035
283
ParaACE (with detected interaction)
0.025
0.035
0.031
0.030
0.038
0.025
0.016
0.019
0.023
0.021
0.026
283"
FDA APPROVED DRUGS,0.8571428571428571,"N
ABLATION STUDY FOR THE EFFECTIVENESS OF INTERACTION DETECTION"
FDA APPROVED DRUGS,0.8583959899749374,"To show how much performance gain we are able to obtain from the interaction detection procedure.
We conduct an experiment that input random pairwise interactions to ParaACE to compare with the
one with detected interactions on synthetic datasets, see Table 9."
FDA APPROVED DRUGS,0.8596491228070176,Published as a conference paper at ICLR 2022
FDA APPROVED DRUGS,0.8609022556390977,"400
600
800
1000
1200
1400
1600
number of training samples 0.018 0.019 0.020 0.021 0.022 0.023"
FDA APPROVED DRUGS,0.8621553884711779,test NRMSE
FDA APPROVED DRUGS,0.8634085213032582,Synthetic dataset 1
FDA APPROVED DRUGS,0.8646616541353384,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.8659147869674185,"400
600
800
1000
1200
1400
1600
number of training samples 0.020 0.025 0.030 0.035 0.040 0.045 0.050"
FDA APPROVED DRUGS,0.8671679197994987,test NRMSE
FDA APPROVED DRUGS,0.868421052631579,Synthetic dataset 2
FDA APPROVED DRUGS,0.8696741854636592,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.8709273182957393,"400
600
800
1000
1200
1400
1600
number of training samples 0.02 0.03 0.04 0.05 0.06 0.07"
FDA APPROVED DRUGS,0.8721804511278195,test NRMSE
FDA APPROVED DRUGS,0.8734335839598998,Synthetic dataset 3
FDA APPROVED DRUGS,0.87468671679198,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.8759398496240601,"400
600
800
1000
1200
1400
1600
number of training samples 0.02 0.03 0.04 0.05 0.06 0.07"
FDA APPROVED DRUGS,0.8771929824561403,test NRMSE
FDA APPROVED DRUGS,0.8784461152882206,Synthetic dataset 4
FDA APPROVED DRUGS,0.8796992481203008,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.8809523809523809,"400
600
800
1000
1200
1400
1600
number of training samples 0.025 0.030 0.035 0.040 0.045 0.050"
FDA APPROVED DRUGS,0.8822055137844611,test NRMSE
FDA APPROVED DRUGS,0.8834586466165414,Synthetic dataset 5
FDA APPROVED DRUGS,0.8847117794486216,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.8859649122807017,"400
600
800
1000
1200
1400
1600
number of training samples 0.010 0.015 0.020 0.025 0.030 0.035 0.040 0.045"
FDA APPROVED DRUGS,0.8872180451127819,test NRMSE
FDA APPROVED DRUGS,0.8884711779448622,Synthetic dataset 6
FDA APPROVED DRUGS,0.8897243107769424,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.8909774436090225,"400
600
800
1000
1200
1400
1600
number of training samples 0.010 0.012 0.014 0.016 0.018 0.020 0.022"
FDA APPROVED DRUGS,0.8922305764411027,test NRMSE
FDA APPROVED DRUGS,0.893483709273183,Synthetic dataset 7
FDA APPROVED DRUGS,0.8947368421052632,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.8959899749373433,"400
600
800
1000
1200
1400
1600
number of training samples 0.008 0.010 0.012 0.014 0.016 0.018"
FDA APPROVED DRUGS,0.8972431077694235,test NRMSE
FDA APPROVED DRUGS,0.8984962406015038,Synthetic dataset 8
FDA APPROVED DRUGS,0.899749373433584,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.9010025062656641,"400
600
800
1000
1200
1400
1600
number of training samples"
FDA APPROVED DRUGS,0.9022556390977443,0.0175
FDA APPROVED DRUGS,0.9035087719298246,0.0200
FDA APPROVED DRUGS,0.9047619047619048,0.0225
FDA APPROVED DRUGS,0.9060150375939849,0.0250
FDA APPROVED DRUGS,0.9072681704260651,0.0275
FDA APPROVED DRUGS,0.9085213032581454,0.0300
FDA APPROVED DRUGS,0.9097744360902256,0.0325
FDA APPROVED DRUGS,0.9110275689223057,test NRMSE
FDA APPROVED DRUGS,0.9122807017543859,Synthetic dataset 9
FDA APPROVED DRUGS,0.9135338345864662,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.9147869674185464,"400
600
800
1000
1200
1400
1600
number of training samples 0.014 0.016 0.018 0.020 0.022 0.024"
FDA APPROVED DRUGS,0.9160401002506265,test NRMSE
FDA APPROVED DRUGS,0.9172932330827067,Synthetic dataset 10
FDA APPROVED DRUGS,0.918546365914787,"OverparaFC
ParaACE"
FDA APPROVED DRUGS,0.9197994987468672,"Figure 13: Performance comparison on synthetic datasets while reducing the number of training
samples. For each dataset, we tried different training data 5 times in one experiment, and the
performances (of ten experiments) were all tested on the same test set. The error bar shows the
maximal and minimal test NRMSE in 5 folds."
FDA APPROVED DRUGS,0.9210526315789473,Published as a conference paper at ICLR 2022
FDA APPROVED DRUGS,0.9223057644110275,"Synthetic data
Real data"
FDA APPROVED DRUGS,0.9235588972431078,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.924812030075188,Percents of Weights Remaining 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16
FDA APPROVED DRUGS,0.9260651629072681,Test NRMSE
FDA APPROVED DRUGS,0.9273182957393483,"Synthetic dataset 1, LTH"
FDA APPROVED DRUGS,0.9285714285714286,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9298245614035088,Percents of Weights Remaining 0.04 0.05 0.06 0.07
FDA APPROVED DRUGS,0.931077694235589,Test NRMSE
FDA APPROVED DRUGS,0.9323308270676691,"Synthetic dataset 2, LTH"
FDA APPROVED DRUGS,0.9335839598997494,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9348370927318296,Percents of Weights Remaining 0.046 0.048 0.050 0.052 0.054 0.056 0.058
FDA APPROVED DRUGS,0.9360902255639098,Test NRMSE
FDA APPROVED DRUGS,0.9373433583959899,"Elevators, LTH"
FDA APPROVED DRUGS,0.9385964912280702,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9398496240601504,Percents of Weights Remaining 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18
FDA APPROVED DRUGS,0.9411027568922306,Test NRMSE
FDA APPROVED DRUGS,0.9423558897243107,"Synthetic dataset 3, LTH"
FDA APPROVED DRUGS,0.943609022556391,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9448621553884712,Percents of Weights Remaining 0.04 0.06 0.08 0.10 0.12 0.14 0.16
FDA APPROVED DRUGS,0.9461152882205514,Test NRMSE
FDA APPROVED DRUGS,0.9473684210526315,"Synthetic dataset 4, LTH"
FDA APPROVED DRUGS,0.9486215538847118,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.949874686716792,Percents of Weights Remaining 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08
FDA APPROVED DRUGS,0.9511278195488722,Test NRMSE
FDA APPROVED DRUGS,0.9523809523809523,"Parkinsons, LTH"
FDA APPROVED DRUGS,0.9536340852130326,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9548872180451128,Percents of Weights Remaining 0.04 0.06 0.08 0.10 0.12 0.14 0.16
FDA APPROVED DRUGS,0.956140350877193,Test NRMSE
FDA APPROVED DRUGS,0.9573934837092731,"Synthetic dataset 5, LTH"
FDA APPROVED DRUGS,0.9586466165413534,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9598997493734336,Percents of Weights Remaining 0.03 0.04 0.05 0.06 0.07 0.08 0.09
FDA APPROVED DRUGS,0.9611528822055138,Test NRMSE
FDA APPROVED DRUGS,0.9624060150375939,"Synthetic dataset 6, LTH"
FDA APPROVED DRUGS,0.9636591478696742,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9649122807017544,Percents of Weights Remaining 0.10 0.15 0.20 0.25 0.30
FDA APPROVED DRUGS,0.9661654135338346,Test NRMSE
FDA APPROVED DRUGS,0.9674185463659147,"Skillcraft, LTH"
FDA APPROVED DRUGS,0.968671679197995,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9699248120300752,Percents of Weights Remaining 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
FDA APPROVED DRUGS,0.9711779448621554,Test NRMSE
FDA APPROVED DRUGS,0.9724310776942355,"Synthetic dataset 7, LTH"
FDA APPROVED DRUGS,0.9736842105263158,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.974937343358396,Percents of Weights Remaining 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
FDA APPROVED DRUGS,0.9761904761904762,Test NRMSE
FDA APPROVED DRUGS,0.9774436090225563,"Synthetic dataset 8, LTH"
FDA APPROVED DRUGS,0.9786967418546366,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9799498746867168,Percents of Weights Remaining 0.038 0.040 0.042 0.044 0.046 0.048
FDA APPROVED DRUGS,0.981203007518797,Test NRMSE
FDA APPROVED DRUGS,0.9824561403508771,"Bike sharing, LTH"
FDA APPROVED DRUGS,0.9837092731829574,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9849624060150376,Percents of Weights Remaining 0.026 0.028 0.030 0.032 0.034 0.036 0.038
FDA APPROVED DRUGS,0.9862155388471178,Test NRMSE
FDA APPROVED DRUGS,0.9874686716791979,"Synthetic dataset 9, LTH"
FDA APPROVED DRUGS,0.9887218045112782,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9899749373433584,Percents of Weights Remaining 0.02 0.04 0.06 0.08 0.10 0.12 0.14
FDA APPROVED DRUGS,0.9912280701754386,Test NRMSE
FDA APPROVED DRUGS,0.9924812030075187,"Synthetic dataset 10, LTH"
FDA APPROVED DRUGS,0.993734335839599,"0.5
1
5
20
50
100"
FDA APPROVED DRUGS,0.9949874686716792,Percents of Weights Remaining 0.102 0.104 0.106 0.108 0.110 0.112 0.114
FDA APPROVED DRUGS,0.9962406015037594,Test NRMSE
FDA APPROVED DRUGS,0.9974937343358395,"Cal housing, LTH"
FDA APPROVED DRUGS,0.9987468671679198,"Figure 14: Performance comparison on both the synthetic and real datasets while reducing the percent
of weights retained by the LTH. The error bar shows the maximal and minimal test NRMSE in 5
folds."
