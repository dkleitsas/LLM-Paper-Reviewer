Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029239766081871343,"Virtually all high-energy-physics (HEP) simulations for the LHC rely on Monte
Carlo using importance sampling by means of the VEGAS algorithm.
How-
ever, complex high-precision calculations have become a challenge for the stan-
dard toolbox. As a result, there has been keen interest in HEP for modern ma-
chine learning to power adaptive sampling. Despite previous work proving that
normalizing-ﬂow-powered neural importance sampling (NIS) sometimes outper-
forms VEGAS, existing research has still left major questions open, which we
intend to solve by introducing ZüNIS, a fully automated NIS library. We ﬁrst
show how to extend the original formulation of NIS to reuse samples over multi-
ple gradient steps, yielding a signiﬁcant improvement for slow functions. We then
benchmark ZüNIS over a range of problems and show high performance with
limited ﬁne-tuning. The library can be used by non-experts with minimal effort,
which is crucial to become a mature tool for the wider HEP public."
INTRODUCTION,0.005847953216374269,"1
INTRODUCTION"
INTRODUCTION,0.008771929824561403,"High-Energy-Physics (HEP) simulations are at the heart of the Large Hadron Collider (LHC) pro-
gram for studying the fundamental laws of nature. Most HEP predictins are expressed as expectation
values, evaluated numerically as Monte Carlo (MC) integrals. This permits both the integration of
the very complex functions and the reproduction of the data selection process by experiments."
INTRODUCTION,0.011695906432748537,"Most HEP simulations tools (Alwall et al., 2014) perform MC integrals using importance sampling,
which allows to adaptively sample points to speed up convergence while keeping independent and
identically distributed samples, crucial to reproduce experimental analyses which can only ingest
uniformly-weighted data, typically produced by rejection sampling (see appendix A)."
INTRODUCTION,0.014619883040935672,"The most popular tool to optimize importance sampling in HEP is by far the VEGAS algorithm (Lep-
age, 1980; 2021), which ﬁghts the curse of dimensionality by assuming no correlations between the
variables. While this is rarely the case in general, a good understanding of the integrand func-
tion can help signiﬁcantly. Indeed optimized parametrizations using multichannelling (Kleiss et al.,
1986; Ohl, 1999; Kleiss & Pittau, 1994) have become bread-and-butter tools for HEP event genera-
tion simulators, with good success for leading-order (LO) calculations. However, as simulations get
more complex, either by having more complex ﬁnal states or by including higher orders in pertur-
bation theory, performance degrades fast."
INTRODUCTION,0.017543859649122806,"While newer approaches to adaptive importance sampling have been developped, like Population
Monte Carlo (Bugallo et al., 2017; 2015; Cappé et al., 2004; Iba, 2001) and its extensions (Cappé
et al., 2008; Koblents & Míguez, 2015; Elvira et al., 2017; Douc et al., 2007; Cornuet et al., 2012),
and have been successfully applied to other ﬁelds, VEGAS remains the standard tool used by essen-
tially all modern simulation tools (Alwall et al., 2014; Bothmann et al., 2019; Bellm et al., 2016).
We suspect that the reasons for this are twofold. First of all the lack of exchange between the two
scientiﬁc communities is probably at cause and should probably not be neglected. Second of all
however, the impetus in the HEP community is to move toward more ""black box"" approaches where
little is known of the structure of the integrand, which seems to be in tension with the situation with
PMC, which is sensitive to to the initial proposal density (Beaujean & Caldwell, 2013; Cappé et al.,
2008). As the main practical goal of this paper is to reduce the reliance of HEP simulations on the
careful tuning of integrators, we will focus on comparing our work with the de facto HEP standard:
the VEGAS algorithm."
INTRODUCTION,0.02046783625730994,"There is much room for investing computational time into improving sampling (ATLAS Collabora-
tion, 2020): modern HEP theoretical calculations are taking epic proportions and can require hours
for a single function evaluation (Jones, 2018). Furthermore, unweighting samples can be extremely
inefﬁcient, with upwards of 90% sampled points discarded (HEP Software Foundation, 2020). More
powerful importance sampling algorithms would therefore be a welcome improvement (Buckley,
2020; WG et al., 2021)."
INTRODUCTION,0.023391812865497075,"First attempts to use machine learning (ML) to address this challenge explored using classical neural
networks to sample (Bendavid, 2017; Klimek & Perelstein, 2020; Chen et al., 2021) but typically
suffer from excessive computational costs. Another avenue of research has been to leverage gener-
ative models successful in other ﬁelds such as generative adversarial networks (Butter et al., 2019;
Di Sipio et al., 2019; Butter et al., 2020; Ahdida et al., 2019; Hashemi et al., 2019; Carrazza &
Dreyer, 2019). While such approaches do improve sampling speed by a large factor, they have ma-
jor limitations. In particular, they have no theoretical guarantees of providing a correct answer on
average (Matchev et al., 2021) and poor control of uncertainties."
INTRODUCTION,0.02631578947368421,"To avoid these disadvantages, our work exploits Neural Importance Sampling (NIS) (Müller et al.,
2019; Zheng & Zwicker, 2019), which relies on normalizing ﬂows and has strong theoretical guar-
antees."
INTRODUCTION,0.029239766081871343,"A number of exploratory papers have been published on using NIS for LHC simulations (Gao et al.,
2020b; Bothmann et al., 2020; Gao et al., 2020a), as well as closely related variations (Bellagente
et al., 2021; Stienen & Verheyen, 2021), but most studies have focused on preliminary investigation
of performance without much concern for the practical usability of the method. Indeed, training
requires function evaluations, which we are trying to minimize and data-efﬁciency training is there-
fore an important but under-appreciated concern. Furthermore, few authors have provided usable
open source code, making the adoption of the technique in the HEP community difﬁcult."
INTRODUCTION,0.03216374269005848,Our contribution to improve this situation can be summarized in two items:
INTRODUCTION,0.03508771929824561,"• The introduction of a new loss function and an associated new training algorithm for NIS.
This permits the re-use of sampled points over multiple gradient descent steps, therefore
making NIS much more data efﬁcient.
• The introduction of ZÜNIS, a PyTorch-based library providing robust and usable NIS
tools, usable by non-experts. It implements previously-developped ideas as well as our
new training procedure and is extensively documented."
BACKGROUND,0.038011695906432746,"2
BACKGROUND"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.04093567251461988,"2.1
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.043859649122807015,"Importance sampling relies on the interpretation of integrals as expectation values. Indeed, let us
consider an integral over a ﬁnite volume: I =
Z"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.04678362573099415,"Ω
dxf(x),
where V (Ω) =
Z"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.049707602339181284,"Ω
dx is ﬁnite.
(1)"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.05263157894736842,"Let p be a strictly positive probability distribution over Ω, we can re-express our integral as the
expectation of a random variable I =
Z"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.05555555555555555,"Ω
p(x)dxf(x)"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.05847953216374269,"p(x) =
E
Xi∼p
1
N N
X i=1"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.06140350877192982,"f(Xi)
p(Xi) ,
(2)"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.06432748538011696,"whose mean is indeed I and whose standard deviation is σ(f, p)
√"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.06725146198830409,"N
, where σ(f, p) is the standard"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.07017543859649122,deviation of f(X)/p(X) for X ∼p:
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.07309941520467836,"σ2(f, p) = E
x∼p f(x) p(x) 2!"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.07602339181286549,"−I2.
(3)"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.07894736842105263,"The problem statement of importance sampling is to ﬁnd the probability distribution function p that
minimizes the variance of our estimator for a given N. In Neural Importance Sampling, we rely on"
IMPORTANCE SAMPLING AS AN OPTIMIZATION PROBLEM,0.08187134502923976,"Normalizing Flows to approximate the optimal distribution, which we can optimize using stochastic
gradient descent."
NORMALIZING FLOWS AND COUPLING CELLS,0.0847953216374269,"2.2
NORMALIZING FLOWS AND COUPLING CELLS"
NORMALIZING FLOWS AND COUPLING CELLS,0.08771929824561403,"Normalizing ﬂows (Tabak & Vanden-Eijnden, 2010; Tabak & Turner, 2013; Rippel & Adams, 2013;
Rezende & Mohamed, 2015) provide a way to generate complex probability distribution functions
from simpler ones using parametric changes of variables that can be learned to approximate a tar-
get distribution. As such, normalizing ﬂows are diffeomorphisms: invertible, (nearly-everywhere)
differentiable mappings with a differentiable inverse."
NORMALIZING FLOWS AND COUPLING CELLS,0.09064327485380116,"Indeed, if u ∼p(u), then T(u) = x ∼q(x) where"
NORMALIZING FLOWS AND COUPLING CELLS,0.0935672514619883,"q (x = T(u)) = p(u) |JT ( u)|−1,
(4)"
NORMALIZING FLOWS AND COUPLING CELLS,0.09649122807017543,where JT is the Jacobian determinant of T:
NORMALIZING FLOWS AND COUPLING CELLS,0.09941520467836257,JT (u) = det ∂Ti
NORMALIZING FLOWS AND COUPLING CELLS,0.1023391812865497,"∂uj
(u).
(5)"
NORMALIZING FLOWS AND COUPLING CELLS,0.10526315789473684,"In the world of machine learning, T is called a normalizing ﬂow and is typically part of a parametric
family of diffeomorphisms (T(·, θ)) such that gradients ∇θJT are tractable."
NORMALIZING FLOWS AND COUPLING CELLS,0.10818713450292397,"Coupling cell mappings perfectly satisfy this requirement (Dinh et al., 2015; 2017; Müller et al.,
2018): they are neural-network-parametrized bijections whose Jacobian factor can be obtained an-
alytically without backpropagation or expensive determinant calculation. As such, they provide a
good candidate for importance sampling as long as they can be trained to learn an unnormalized
target function, which is exactly what neural importance sampling proposes."
NEURAL IMPORTANCE SAMPLING,0.1111111111111111,"2.3
NEURAL IMPORTANCE SAMPLING"
NEURAL IMPORTANCE SAMPLING,0.11403508771929824,"Neural importance sampling was introduced in the context of computer graphics (Müller et al., 2018)
and proposes to use normalizing ﬂows as a family of probability distributions over which to solve
the minimization problem of importance sampling."
NEURAL IMPORTANCE SAMPLING,0.11695906432748537,"L(θ) =
Z"
NEURAL IMPORTANCE SAMPLING,0.11988304093567251,"Ω
dx f 2(x)"
NEURAL IMPORTANCE SAMPLING,0.12280701754385964,"p (x, θ).
(6)"
NEURAL IMPORTANCE SAMPLING,0.12573099415204678,"Of course, to actually do so, one needs to ﬁnd a way to explicitly evaluate L(θ) and the original
neural importance sampling approach proposes to approximate it using importance sampling. One
needs to be careful that the gradient of the estimator of the loss need not be the estimator of the
gradient of the loss. The gradient of the loss can be expressed as"
NEURAL IMPORTANCE SAMPLING,0.1286549707602339,"∇θL(θ) = −
Z"
NEURAL IMPORTANCE SAMPLING,0.13157894736842105,"Ω
dx f 2(x)"
NEURAL IMPORTANCE SAMPLING,0.13450292397660818,"p (x, θ)∇θ log p (x, θ) ,
(7)"
NEURAL IMPORTANCE SAMPLING,0.13742690058479531,for which an estimator is proposed as
NEURAL IMPORTANCE SAMPLING,0.14035087719298245,"b∇θL(θ) = − N
X i=0"
NEURAL IMPORTANCE SAMPLING,0.14327485380116958, f(Xi)
NEURAL IMPORTANCE SAMPLING,0.14619883040935672,"p (Xi, θ)"
NEURAL IMPORTANCE SAMPLING,0.14912280701754385,"2
∇θ log p (Xi, θ) ,
Xi ∼p.
(8)"
NEURAL IMPORTANCE SAMPLING,0.15204678362573099,"The authors also observed that other loss functions are possible which share the same global mini-
mum as the variance based loss: for example, the Kullback-Leibler divergence DKL between two
functions is also minimized when they are equal. Such alternative loss functions are not guaranteed
to work for importance sampling, but they prove quite successful in practice. After training to mini-
mize the loss estimator of eq. (8), the normalizing ﬂows provides a tractable probability distribution
function from which to sample points and estimate the integral."
CONCEPTS AND ALGORITHMS,0.15497076023391812,"3
CONCEPTS AND ALGORITHMS"
CONCEPTS AND ALGORITHMS,0.15789473684210525,"In this section we describe the original contributions of this paper. The major conceptual innova-
tion we provide in ZÜNIS is a more ﬂexible and data-efﬁcient way of training normalizing ﬂows in"
CONCEPTS AND ALGORITHMS,0.1608187134502924,"the context of importance sampling. This relies on a more rigorous formulation of the connection
between the theoretical expression of ideal loss functions in terms of integrals and their practi-
cal realizations as random estimators than in previous literature. We describe this improvement in
section 3.1. We also give a high-level overview of the organization of the ZÜNIS library, which
implements this new training procedure."
EFFICIENT TRAINING FOR IMPORTANCE SAMPLING,0.16374269005847952,"3.1
EFFICIENT TRAINING FOR IMPORTANCE SAMPLING"
EFFICIENT TRAINING FOR IMPORTANCE SAMPLING,0.16666666666666666,"In this section, we describe how we train probability distributions within ZÜNIS using gradient-
based optimizers. While the solution proposed in the original formulation of NIS deﬁned eq. (8)
works, its main drawback is that it samples points from the same distribution that it tries to optimize.
As a result, new points Xi must be sampled from p after every gradient step, which is very inefﬁcient
for slow integrands."
EFFICIENT TRAINING FOR IMPORTANCE SAMPLING,0.1695906432748538,"Our solution to this problem is to remember that the loss function is an integral, which can be
evaluated by importance sampling using any PDF, not only p. We will therefore deﬁne an auxiliary
probability distribution function q(x), independent from θ, from which we sample to estimate our
loss function:
Z
dx f(x)2"
EFFICIENT TRAINING FOR IMPORTANCE SAMPLING,0.17251461988304093,"p(x, θ) = E
x∼q
f(x)2"
EFFICIENT TRAINING FOR IMPORTANCE SAMPLING,0.17543859649122806,"p(x, θ)q(x).
(9)"
EFFICIENT TRAINING FOR IMPORTANCE SAMPLING,0.1783625730994152,"This is the basis for the general method we use for training probability distributions within ZÜNIS,
described in algorithm 2. Because the sampling distribution is separated from the model to train,
the same point sample can be reused for multiple training steps, which is not possible when using
eq. (8). This is particularly important for high-precision particle physics predictions that involve
high-order perturbative calculations or complex detector simulations because function evaluations
can be extremely costly. We show in section 4, in particular in ﬁg. 4 that reusing data indeed has a
very signiﬁcant impact on data efﬁciency."
EFFICIENT TRAINING FOR IMPORTANCE SAMPLING,0.18128654970760233,"Algorithm 1: Backward sampling training in ZÜNIS
Data: Parametric PDF p(x, θ)
Result: Trained PDF p(x, θ)"
FOR M STEPS DO,0.18421052631578946,1 for M steps do
FOR M STEPS DO,0.1871345029239766,"2
Sample a batch x1, . . . , xnbatch from q;"
FOR M STEPS DO,0.19005847953216373,"3
Compute the sampling PDF values q(xi);"
FOR M STEPS DO,0.19298245614035087,"4
Compute function values f(xi);"
FOR M STEPS DO,0.195906432748538,"5
Start tracking gradients with respect to θ;"
FOR N STEPS DO,0.19883040935672514,"6
for N steps do"
FOR N STEPS DO,0.20175438596491227,"7
Compute the PDF values from the parametric PDF p(xi, θ);"
FOR N STEPS DO,0.2046783625730994,"8
Estimate the loss ˆL = 1 N"
FOR N STEPS DO,0.20760233918128654,"nbatch
X i=1"
FOR N STEPS DO,0.21052631578947367,f(xi)2
FOR N STEPS DO,0.2134502923976608,"p(x, θ)q(x);"
FOR N STEPS DO,0.21637426900584794,"9
Compute ∇θ ˆL using backpropagation;"
FOR N STEPS DO,0.21929824561403508,"10
Set θ ←θ −η∇θ ˆL;"
FOR N STEPS DO,0.2222222222222222,"11
Reset gradients;"
END,0.22514619883040934,"12
end"
END,0.22807017543859648,13 end
END,0.2309941520467836,"14 return p(x, θ);"
END,0.23391812865497075,"After training, q is discarded and the integral is estimated from the optimized p."
END,0.23684210526315788,"The only constraint on q is that it yields a good enough estimate so that gradient steps improve the
model. Much like in other applications of neural networks, we have found that stochastic gradient
descent can yield good results despite noisy loss estimates. We propose three schemes for q:"
END,0.23976608187134502,"• A uniform distribution (survey_strategy=""flat"")"
END,0.24269005847953215,"• A frozen copy of the model, which can be updated once in a while1 (survey_strategy="
END,0.24561403508771928,"""forward"")
• An adaptive scheme starting with a uniform distribution and switching to sampling
from a frozen model when it is expected to yield a more accurate loss estimate
(survey_strategy=""adaptive_variance"")."
END,0.24853801169590642,"An important point to notice is that the original NIS formulation in eq. (8) can be restated as a
limiting case of our approach. Indeed, if we take q to be a frozen version of the model p(x, θ0),
which we update everytime we sample points (setting N = 1 in algorithm 2), the gradient update in
line 9 is ∇θ"
END,0.25146198830409355,"
E
x∼p(x,θ0)
f(x)2"
END,0.2543859649122807,"p(x, θ)p(x, θ0)"
END,0.2573099415204678,"
θ0=θ
= −
E
x∼p(x,θ)
f(x)2"
END,0.260233918128655,"p(x, θ)2 ∇θ log p(x, θ).
(10)"
END,0.2631578947368421,"3.2
THE ZÜNIS LIBRARY"
END,0.26608187134502925,"On the practical side, ZÜNIS is a PyTorch-based library which implements many ideas formulated
in previous work but organizes them in the form of a modular software library with an easy-to-use
interface and well-deﬁned building blocks. We believe this structure will help non-specialist use it
without understanding all the nuts and bolts, while experts can easily add new features to responds
to their needs. The ZÜNIS library relies on three layers of abstractions which steer the different
aspects of using normalizing ﬂows to learn probability distributions from un-normalized functions
and compute their integrals:"
END,0.26900584795321636,"• Flows, which implement a bijective mapping which transforms points and computes the
corresponding Jacobian are described in appendix F.1
• Trainers, which provide the infrastructure to perform training steps and sample from
ﬂow models are described in appendix F.2
• Integrators, which use trainers to steer the training of a model and compute integrals
are described in appendix F.3"
RESULTS,0.2719298245614035,"4
RESULTS"
RESULTS,0.27485380116959063,"In this section, we evaluate ZÜNIS on a variety of test functions to assess its performance and com-
pare it to the commonly used VEGAS algorithm (Peter Lepage, 1978; Ohl, 1999). We ﬁrst produce
a few low dimensional examples for illustrative purposes, then move on to integrating paramet-
ric functions in various dimensions and ﬁnally evaluate performance on particle scattering matrix
elements."
LOW-DIMENSIONAL EXAMPLES,0.2777777777777778,"4.1
LOW-DIMENSIONAL EXAMPLES"
LOW-DIMENSIONAL EXAMPLES,0.2807017543859649,"Let us start by illustrating the effectiveness of ZÜNIS in a low dimensional setting where we can
readily visualize results. We deﬁne three functions on the 2D unit hypercube which each illustrate
some failure mode of VEGAS (see appendix C)."
LOW-DIMENSIONAL EXAMPLES,0.28362573099415206,"We ran the ZÜNIS Integrator with default arguments over ten repetitions for each function and
report the performance of the trained integral estimator compared to a ﬂat estimator and to VEGAS
in table 1. Overall, ZÜNIS Integrators learn to sample from their target function extremely
well: we outperform VEGAS by a factor 100 for the camel and the slashed circle functions and a
factor 30 for the sinusoidal function and VEGAS itself provides no advantage over uniform sampling
for the latter two."
LOW-DIMENSIONAL EXAMPLES,0.28654970760233917,"We further illustrate the performance of our trained models by comparing the target functions and
density histogram for points sampled from the normalizing ﬂows in ﬁg. 1, which shows great quali-
tative agreement."
LOW-DIMENSIONAL EXAMPLES,0.2894736842105263,"1This is inspired by deep-Q learning, where two copies of the value model are used: a frozen one used to
sample actions, and a trainable one used to estimate values in the loss function. Here the frozen copy is used to
sample points, and the trainable model is used to compute PDF values used in the loss function"
LOW-DIMENSIONAL EXAMPLES,0.29239766081871343,"Variance Reduction
Camel
Slashed Circle
Sinusoidal
vs. uniform
1.8 ± 0.4 × 103
8.9 ± 0.9 × 101
2.0 ± 0.5 × 102"
LOW-DIMENSIONAL EXAMPLES,0.2953216374269006,"vs. VEGAS
7.0 ± 1.4 × 102
8.8 ± 0.9 × 101
1.6 ± 0.5 × 102"
LOW-DIMENSIONAL EXAMPLES,0.2982456140350877,"Table 1: Variance reduction (high is good) for the camel, slashed circle and sinusoidal functions
compared to uniform sampling and to VEGAS over 10 repetitions."
LOW-DIMENSIONAL EXAMPLES,0.30116959064327486,"(a) Camel function
(b) Sinusoidal function
(c) Slashed circle function"
LOW-DIMENSIONAL EXAMPLES,0.30409356725146197,"Figure 1: Comparison between target functions and point sampling densities for 1a the camel func-
tion, 1b the sinusoidal function, 1c the slashed circle function. Supplementary ﬁg. 7 shows how
points are mapped from latent to target space."
SYSTEMATIC BENCHMARKS,0.30701754385964913,"4.2
SYSTEMATIC BENCHMARKS"
SYSTEMATIC BENCHMARKS,0.30994152046783624,"Let us now take a more systematic approach to benchmarking ZÜNIS. We compare ZÜNIS Inte-
grators against uniform integration and VEGAS using the following metrics: integrand variance (a
measure of convergence speed, see section 2.1), unweighting efﬁciency (a measure of the efﬁciency
of exact sampling with rejection, see appendix A) and wall-clock training and sampling2."
SYSTEMATIC BENCHMARKS,0.3128654970760234,"ZÜNIS improves convergence rate compared to VEGAS
For this experiment, we focus on the
camel function deﬁned in eq. (12) and scan a 35 conﬁgurations spanning from 2 to 32 dimensions
over function variances between 10−2 and 102 as shown in table 3."
SYSTEMATIC BENCHMARKS,0.3157894736842105,"Except in the low variance limit, ZÜNIS can reduce the required number of points sampled to attain
a given precision on integral estimates without any parameter tuning, attaining speed-ups of up to
×1000 both compared to uniform sampling and VEGAS-based importance sampling, as shown in
ﬁg. 2a-2b and table 4. Unweighting efﬁciencies are also boosted signiﬁcantly, although more mildly
than variances, as shown in ﬁg. 2c-2d, which we could attribute to PDF underestimation in regions
with low point density; the nature of the veto algorithm makes it very sensitive to a few bad behaving
points in the whole dataset."
SYSTEMATIC BENCHMARKS,0.31871345029239767,"ZÜNIS is slower than VEGAS
ZÜNIS does not, however, outclass VEGAS on all metrics by
far: as shown in ﬁg. 3, training is a few hundred times slower than VEGAS and sampling is 10-50
times slower, all while ZÜNIS runs on GPUs. This is to be expected given the much increased com-"
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.3216374269005848,2We provide details on hardware in appendix G
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.32456140350877194,"(a)
(b)"
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.32748538011695905,"(c)
(d)"
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.3304093567251462,"Figure 2: Benchmarking ZÜNIS against uniform sampling and VEGAS with default settings. In
(2a-2b), we show the sampling speed-up (ratio of integrand variance) as a function of the relative
standard deviation of the integrand, while we show the unweighting speed-up (ratio of unweighting
efﬁciencies) in (2c-2d)."
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.3333333333333333,"putational complexity of normalizing ﬂows compared to the VEGAS algorithm. As such, ZÜNIS is
not a general replacement for VEGAS, but provides a clear advantage for integrating time-intensive
functions, where sampling is a negligible overhead, such as precision high-energy-physics simula-
tions."
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.3362573099415205,"(a)
(b)"
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.3391812865497076,"Figure 3: Comparison of the training and sampling speed of ZÜNIS and VEGAS. As can be ex-
pected, ZÜNIS is much slower than VEGAS, both for training and sampling, although larger batch
sizes can better leverage the power of hardware accelerators."
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.34210526315789475,"The new loss function introduced in ZÜNIS improves data efﬁciency
We have shown that
ZÜNIS is a very performant importance sampling and event generation tool and provides signiﬁcant
improvements over existing tools, while requiring little ﬁne tuning from users. Another key result
is that the new approach to training we introduced in section 3.1 has a large positive impact on"
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.34502923976608185,"performance. Indeed, as can be seen in ﬁg. 4, re-using samples for training over multiple epochs
provides a 2- to 10-fold increase in convergence speed, making training much more data-efﬁcient."
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.347953216374269,"For this experiment, we use forward sampling, where the frozen model is used to sample a batch of
points which are then used for training over multiple epochs before resampling from an update of
the frozen model. As a result, we reproduce the original formulation of NIS in eq. (8) when we use
a single epoch as shown in eq. (10)."
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.3508771929824561,"(a)
(b)"
WE PROVIDE DETAILS ON HARDWARE IN APPENDIX G,0.3538011695906433,"Figure 4: Figure 4a: Effect of repeatedly training on the same sample of points over multiple epochs.
For all settings, there is a large improvement when going from one to moderate epoch counts, with
a peak around 5-10. Larger number of epochs lead to overﬁtting, which impacts performance nega-
tively. Figure 4b: Comparison between optimal data reuse (5 epochs) and the original NIS algorithm
(1 epoch)."
MADGRAPH CROSS SECTION INTEGRALS,0.3567251461988304,"4.3
MADGRAPH CROSS SECTION INTEGRALS"
MADGRAPH CROSS SECTION INTEGRALS,0.35964912280701755,"Cross-sections are integrals of quantum transition matrix elements for a a scattering process such
as a LHC collision and express the probability that speciﬁc particles are produced. Matrix ele-
ments themselves are un-normalized probability distributions for the conﬁguration of the outgoing
particles: it is therefore both valuable to integrate them to know the overall frequency of a given
scattering process, and to sample from them to understand how particles will be spatially distributed
as they ﬂy off the collision point."
MADGRAPH CROSS SECTION INTEGRALS,0.36257309941520466,"We study the performance of ZÜNIS in comparison to VEGAS by studying three simple processes
at leading order in perturbation theory, e−µ →e−µ via Z, d ¯d →d ¯d via Z and uc →ucg (with 3-jet
cuts based on ∆R), see table 2 and ﬁg. 5. We use the ﬁrst process as a very easy reference while the
two other, quark-initiated processes are used to illustrate speciﬁc points. Indeed, both feature narrow
regions of their integration space with large enhancements, due respectively to Z-boson resonances
and infra-red divergences."
MADGRAPH CROSS SECTION INTEGRALS,0.3654970760233918,"(a)
(b)
(c)"
MADGRAPH CROSS SECTION INTEGRALS,0.3684210526315789,"Figure 5: Sample Feynman Diagrams for e−µ →e−µ via Z, d ¯d →d ¯d via Z and uc →ucg ."
MADGRAPH CROSS SECTION INTEGRALS,0.3713450292397661,"We evaluate the matrix elements for these three processes by using the FORTRAN standalone in-
terface of MADGRAPH5_AMC@NLO (Alwall et al., 2014). The two hadronic processes are con-
volved with parton-distribution functions from LHAPDF6 (Buckley et al., 2015). We parametrize"
MADGRAPH CROSS SECTION INTEGRALS,0.3742690058479532,"e−µ →e−µ via Z
d ¯d →d ¯d via Z
uc →ucg
dimensions
2
4
7
normalized standard deviation
1.45 × 10−2
6.57 × 10−2
0.96"
MADGRAPH CROSS SECTION INTEGRALS,0.37719298245614036,Table 2: Comparison of the three test processes.
MADGRAPH CROSS SECTION INTEGRALS,0.38011695906432746,"phase space (the particle conﬁguration space) using the RAMBO on diet algorithm (Plätzer, 2013)
implemented for PyTorch in TORCHPS (Götz, 2021)."
MADGRAPH CROSS SECTION INTEGRALS,0.3830409356725146,"We report benchmark results in ﬁg. 6, in which we trained over 500,000 points for each process
using near-default conﬁguration, scanning only over variance and Kullback-Leibler losses."
MADGRAPH CROSS SECTION INTEGRALS,0.38596491228070173,"(a)
(b)"
MADGRAPH CROSS SECTION INTEGRALS,0.3888888888888889,"Figure 6: Average performance of ZÜNIS over 20 runs relative to VEGAS, measured by the relative
speed-up in ﬁg. 6a and by the relative unweighting efﬁciency in ﬁg. 6b."
MADGRAPH CROSS SECTION INTEGRALS,0.391812865497076,"As previously observed, little convergence acceleration is achieved for low variance integrands like
e−µ →e−µ, but unweighting still beneﬁts from NIS. The two hadronic processes illustrate typical
features for cross sections: training performance is variable and different processes are optimized
by different loss function choices3."
MADGRAPH CROSS SECTION INTEGRALS,0.39473684210526316,"The performance of d ¯d →d ¯d shows nice improvement with ZÜNIS while that of uc →ucg is more
limited. This can be understood by comparing to importance sampling (see appendix D.3): it is in
fact VEGAS, which performs signiﬁcantly better on uc →ucg compared to d ¯d →d ¯d because the
parametrization of RAMBO is based on splitting invariant masses, making them aligned with the
enhancements in the ucg phase space and allowing great VEGAS performance. This drives a key
conclusion for the potential role of ZÜNIS in the HEP simulation landscape: not to replace VEGAS,
but to ﬁll in the gaps where it fails due to inadequate parametrizations, as we illustrate here by using
non-multichanneled d ¯d →d ¯d as a proxy for more complex processes."
CONCLUSION,0.39766081871345027,"5
CONCLUSION"
CONCLUSION,0.40058479532163743,"We have showed that ZÜNIS can outperform VEGAS both in terms of integral convergence rate
and unweighting efﬁciency on speciﬁc cases, at the cost of a signiﬁcant increase in training and
sampling time, which is an acceptable tradeoff for high-precision HEP computations with high costs.
In this context, the introduction of efﬁcient training is a key element to making the most of the
power of neural importance sampling where function evaluation costs are a major concern. While
further testing is required to ascertain how far NIS can ﬁll the gaps left by VEGAS for integrating
complex functions, there is already good evidence that ZÜNIS can provide needed improvements
in speciﬁc cases. We hope that the publication of a usable toolbox for NIS such as ZÜNIS will stir
a wider audience within the HEP community to apply the method so that the exact boundaries its
applicability can be more clearly ascertained."
CONCLUSION,0.40350877192982454,"3Generally, smoother functions are better optimized with the Kullback-Leibler loss while functions with
peaks beneﬁt from using the variance loss. As we show in appendix D.4, choosing an adaptive strategy is
generally advisable whatever the loss"
REPRODUCIBILITY STATEMENT,0.4064327485380117,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.4093567251461988,An anonymized version of the library is available on Github.
REPRODUCIBILITY STATEMENT,0.41228070175438597,"The recommended procedure to reproduce the experiments is to clone the repository and install the
Python requirements using pip install -r requirements.txt."
REPRODUCIBILITY STATEMENT,0.4152046783625731,"The data to reproduce the experiments can be generated using scripts provided in the repository
at experiments/benchmarks, in which Jupyter notebooks are also available to reproduce the
ﬁgures of the paper. The following scripts are available:"
REPRODUCIBILITY STATEMENT,0.41812865497076024,"• benchmarks_03/camel/run_benchmark_defaults.sh to generate camel integration
data"
REPRODUCIBILITY STATEMENT,0.42105263157894735,"• benchmarks_04/camel/run_benchmark_defaults.sh to generate camel sampling
speed data"
REPRODUCIBILITY STATEMENT,0.4239766081871345,"• benchmark_madgraph/ex_benchmark_emu.sh to generate e−µ →e−µ via Z integra-
tion data"
REPRODUCIBILITY STATEMENT,0.4269005847953216,"• benchmark_madgraph/ex_benchmark_dd.sh to generate d ¯d →d ¯d via Zintegration
data"
REPRODUCIBILITY STATEMENT,0.4298245614035088,• benchmark_madgraph/ex_benchmark_ucg.sh to generate uc →ucg integration data
REPRODUCIBILITY STATEMENT,0.4327485380116959,"These scripts assume that 5 CUDA GPUs are available and run 5 benchmarks in parallel. If fewer
GPUs are available, it is recommended to modify the scripts to run the benchmarking scripts se-
quentially (by removing the ampersand) and to adapt the --cuda=N option."
REFERENCES,0.43567251461988304,REFERENCES
REFERENCES,0.43859649122807015,"C. Ahdida, R. Albanese, A. Alexandrov, A. Anokhina, S. Aoki, G. Arduini, E. Atkin, N. Azorskiy,
J.J. Back, A. Bagulya, and et al. Fast simulation of muons produced at the ship experiment using
generative adversarial networks. Journal of Instrumentation, 14(11):P11028–P11028, Nov 2019.
ISSN 1748-0221. doi: 10.1088/1748-0221/14/11/p11028. URL http://dx.doi.org/10.
1088/1748-0221/14/11/P11028."
REFERENCES,0.4415204678362573,"J. Alwall, R. Frederix, S. Frixione, V. Hirschi, F. Maltoni, O. Mattelaer, H.-S. Shao, T. Stelzer,
P. Torrielli, and M. Zaro. The automated computation of tree-level and next-to-leading order
differential cross sections, and their matching to parton shower simulations. Journal of High
Energy Physics, 2014(7), Jul 2014. ISSN 1029-8479. doi: 10.1007/jhep07(2014)079. URL
http://dx.doi.org/10.1007/JHEP07(2014)079."
REFERENCES,0.4444444444444444,The ATLAS Collaboration. ATLAS HL-LHC Computing Conceptual Design Report. 2020.
REFERENCES,0.4473684210526316,"Frederik Beaujean and Allen Caldwell.
Initializing adaptive importance sampling with markov
chains. arXiv preprint arXiv:1304.7808, 2013."
REFERENCES,0.4502923976608187,"Marco Bellagente, Manuel Haußmann, Michel Luchmann, and Tilman Plehn. Understanding Event-
Generation Networks via Uncertainties. arXiv:2104.04543 [hep-ph], April 2021."
REFERENCES,0.45321637426900585,"Johannes Bellm et al. Herwig 7.0/Herwig++ 3.0 release note. Eur. Phys. J. C, 76(4):196, 2016. doi:
10.1140/epjc/s10052-016-4018-8."
REFERENCES,0.45614035087719296,"Joshua Bendavid. Efﬁcient Monte Carlo Integration Using Boosted Decision Trees and Generative
Deep Neural Networks. 2017."
REFERENCES,0.4590643274853801,"Enrico Bothmann, Timo Janßen, Max Knobbe, Tobias Schmale, and Steffen Schumann. Exploring
phase space with Neural Importance Sampling. SciPost Physics, 8(4):069, April 2020. ISSN
2542-4653. doi: 10.21468/SciPostPhys.8.4.069."
REFERENCES,0.4619883040935672,"Enrico Bothmann et al. Event Generation with Sherpa 2.2. SciPost Phys., 7(3):034, 2019. doi:
10.21468/SciPostPhys.7.3.034."
REFERENCES,0.4649122807017544,"Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density estima-
tion. 2020."
REFERENCES,0.4678362573099415,"Andy Buckley. Computational challenges for MC event generation. Journal of Physics: Conference
Series, 1525:012023, April 2020. ISSN 1742-6596. doi: 10.1088/1742-6596/1525/1/012023."
REFERENCES,0.47076023391812866,"Andy Buckley, James Ferrando, Stephen Lloyd, Karl Nordström, Ben Page, Martin Rüfe-
nacht, Marek Schönherr, and Graeme Watt.
LHAPDF6: parton density access in the LHC
precision era.
The European Physical Journal C, 75(3), Mar 2015.
ISSN 1434-6052.
doi:
10.1140/epjc/s10052-015-3318-8.
URL http://dx.doi.org/10.1140/epjc/
s10052-015-3318-8."
REFERENCES,0.47368421052631576,"Monica F. Bugallo, Victor Elvira, Luca Martino, David Luengo, Joaquin Miguez, and Petar M.
Djuric. Adaptive importance sampling: The past, the present, and the future. IEEE Signal Pro-
cessing Magazine, 34(4):60–79, 2017. doi: 10.1109/MSP.2017.2699226."
REFERENCES,0.4766081871345029,"Mónica F. Bugallo, Luca Martino, and Jukka Corander. Adaptive importance sampling in signal
processing.
Digital Signal Processing, 47:36–49, 2015.
ISSN 1051-2004.
doi: https://doi.
org/10.1016/j.dsp.2015.05.014.
URL https://www.sciencedirect.com/science/
article/pii/S1051200415001864. Special Issue in Honour of William J. (Bill) Fitzger-
ald."
REFERENCES,0.47953216374269003,"Anja Butter, Tilman Plehn, and Ramon Winterhalder. How to gan lhc events. SciPost Physics, 7
(6), Dec 2019. ISSN 2542-4653. doi: 10.21468/scipostphys.7.6.075. URL http://dx.doi.
org/10.21468/SciPostPhys.7.6.075."
REFERENCES,0.4824561403508772,"Anja Butter, Tilman Plehn, and Ramon Winterhalder. How to gan event subtraction. SciPost Physics
Core, 3(2), Nov 2020. ISSN 2666-9366. doi: 10.21468/scipostphyscore.3.2.009. URL http:
//dx.doi.org/10.21468/SciPostPhysCore.3.2.009."
REFERENCES,0.4853801169590643,"Olivier Cappé, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Population monte carlo.
Journal of Computational and Graphical Statistics, 13(4):907–929, 2004."
REFERENCES,0.48830409356725146,"Olivier Cappé, Randal Douc, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Adaptive
importance sampling in general mixture classes. Statistics and Computing, 18(4):447–459, 2008."
REFERENCES,0.49122807017543857,"Stefano Carrazza and Frédéric A. Dreyer. Lund jet images from generative and cycle-consistent
adversarial networks.
The European Physical Journal C, 79(11), Nov 2019.
ISSN 1434-
6052.
doi: 10.1140/epjc/s10052-019-7501-1.
URL http://dx.doi.org/10.1140/
epjc/s10052-019-7501-1."
REFERENCES,0.49415204678362573,"I.-Kai Chen, Matthew D. Klimek, and Maxim Perelstein. Improved Neural Network Monte Carlo
Simulation.
SciPost Physics, 10(1):023, January 2021.
ISSN 2542-4653.
doi: 10.21468/
SciPostPhys.10.1.023."
REFERENCES,0.49707602339181284,"Jean-Marie Cornuet, Jean-Michel Marin, Antonietta Mira, and Christian Robert. Adaptive multiple
importance sampling. Scandinavian Journal of Statistics, 39(4):798–812, 2012. doi: https://doi.
org/10.1111/j.1467-9469.2011.00756.x.
URL https://onlinelibrary.wiley.com/
doi/abs/10.1111/j.1467-9469.2011.00756.x."
REFERENCES,0.5,"Riccardo Di Sipio, Michele Faucci Giannelli, Sana Ketabchi Haghighat, and Serena Palazzo.
Dijetgan: a generative-adversarial network approach for the simulation of qcd dijet events
at the lhc.
Journal of High Energy Physics, 2019(8), Aug 2019.
ISSN 1029-8479.
doi:
10.1007/jhep08(2019)110. URL http://dx.doi.org/10.1007/JHEP08(2019)110."
REFERENCES,0.5029239766081871,"Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Components
Estimation, 2015."
REFERENCES,0.5058479532163743,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP, 2017."
REFERENCES,0.5087719298245614,"Randal Douc, Arnaud Guillin, J-M Marin, and Christian P Robert. Minimum variance importance
sampling via population monte carlo. ESAIM: Probability and Statistics, 11:427–447, 2007."
REFERENCES,0.5116959064327485,"Víctor Elvira, Luca Martino, David Luengo, and Mónica F. Bugallo.
Improving population
monte carlo: Alternative weighting and resampling schemes.
Signal Processing, 131:77–91,
2017.
ISSN 0165-1684.
doi: https://doi.org/10.1016/j.sigpro.2016.07.012.
URL https:
//www.sciencedirect.com/science/article/pii/S0165168416301633."
REFERENCES,0.5146198830409356,"Christina Gao, Stefan Höche, Joshua Isaacson, Claudius Krause, and Holger Schulz. Event genera-
tion with normalizing ﬂows. Physical Review D, 101(7):076002, April 2020a. ISSN 2470-0010,
2470-0029. doi: 10.1103/PhysRevD.101.076002."
REFERENCES,0.5175438596491229,"Christina Gao, Joshua Isaacson, and Claudius Krause. I-ﬂow: High-dimensional Integration and
Sampling with Normalizing Flows. Machine Learning: Science and Technology, 1(4):045023,
November 2020b. ISSN 2632-2153. doi: 10.1088/2632-2153/abab62."
REFERENCES,0.52046783625731,"Niklas Götz.
NGoetz/TorchPS:-v1.0.1, March 2021.
URL https://doi.org/10.5281/
zenodo.4639109.
Available at https://github.com/NGoetz/TorchPS/tree/
v1.0.1, version 1.0.1."
REFERENCES,0.5233918128654971,"Bobak Hashemi, Nick Amin, Kaustuv Datta, Dominick Olivito, and Maurizio Pierini. Lhc analysis-
speciﬁc datasets with generative adversarial networks, 2019."
REFERENCES,0.5263157894736842,"HEP Software Foundation. HL-LHC Computing Review: Common Tools and Community Soft-
ware. arXiv:2008.13636 [hep-ex, physics:physics], August 2020. doi: 10.5281/zenodo.4009114."
REFERENCES,0.5292397660818714,"Yukito Iba. Population based Monte Carlo algorithms. Trans. Jap. Soc. Artif. Intell., 16:279, 2001.
doi: 10.1527/tjsai.16.279."
REFERENCES,0.5321637426900585,"F. James. Monte Carlo Theory and Practice. Rept. Prog. Phys., 43:1145, 1980. doi: 10.1088/
0034-4885/43/9/002."
REFERENCES,0.5350877192982456,"S.P. Jones. Higgs Boson Pair Production: Monte Carlo Generator Interface and Parton Shower. Acta
Physica Polonica B Proceedings Supplement, 11(2):295, 2018. ISSN 1899-2358, 2082-7865. doi:
10.5506/APhysPolBSupp.11.295."
REFERENCES,0.5380116959064327,"R. Kleiss and R. Pittau. Weight optimization in multichannel Monte Carlo. Computer Physics Com-
munications, 83(2-3):141–146, December 1994. ISSN 00104655. doi: 10.1016/0010-4655(94)
90043-4."
REFERENCES,0.5409356725146199,"R Kleiss, W. J Stirling, and S. D Ellis. A new Monte Carlo treatment of multiparticle phase space at
high energies. Computer Physics Communications, 40(2):359–373, June 1986. ISSN 0010-4655.
doi: 10.1016/0010-4655(86)90119-0."
REFERENCES,0.543859649122807,"Matthew Klimek and Maxim Perelstein. Neural network-based approach to phase space integration.
SciPost Physics, 9(4):053, October 2020. ISSN 2542-4653. doi: 10.21468/SciPostPhys.9.4.053."
REFERENCES,0.5467836257309941,"Eugenia Koblents and Joaquín Míguez. A population monte carlo scheme with transformed weights
and its application to stochastic kinetic models. Statistics and Computing, 25(2):407–425, 2015."
REFERENCES,0.5497076023391813,"G. Peter Lepage. VEGAS: AN ADAPTIVE MULTIDIMENSIONAL INTEGRATION PROGRAM.
March 1980."
REFERENCES,0.5526315789473685,"G. Peter Lepage. Adaptive Multidimensional Integration: VEGAS Enhanced. Journal of Computa-
tional Physics, 439:110386, August 2021. ISSN 00219991. doi: 10.1016/j.jcp.2021.110386."
REFERENCES,0.5555555555555556,"Konstantin T. Matchev, Alexander Roman, and Prasanth Shyamsundar. Uncertainties associated
with GAN-generated datasets in high energy physics. arXiv:2002.06307 [hep-ex, physics:hep-
ph, physics:physics], June 2021."
REFERENCES,0.5584795321637427,"Thomas Müller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Novák. Neural Im-
portance Sampling, 2018."
REFERENCES,0.5614035087719298,"Thomas Müller, Brian Mcwilliams, Fabrice Rousselle, Markus Gross, and Jan Novák. Neural Impor-
tance Sampling. ACM Transactions on Graphics, 38(5):1–19, November 2019. ISSN 0730-0301,
1557-7368. doi: 10.1145/3341156."
REFERENCES,0.564327485380117,"Thorsten Ohl. Vegas revisited: Adaptive Monte Carlo integration beyond factorization. Comput.
Phys. Commun., 120:13–19, 1999. doi: 10.1016/S0010-4655(99)00209-X."
REFERENCES,0.5672514619883041,"G Peter Lepage.
A new algorithm for adaptive multidimensional integration.
Journal of
Computational Physics, 27(2):192–203, 1978.
ISSN 0021-9991.
doi:
https://doi.org/10.
1016/0021-9991(78)90004-9.
URL https://www.sciencedirect.com/science/
article/pii/0021999178900049."
REFERENCES,0.5701754385964912,"Simon Plätzer. RAMBO on diet, 2013."
REFERENCES,0.5730994152046783,"Danilo Jimenez Rezende and Shakir Mohamed.
Variational inference with normalizing ﬂows.
32nd International Conference on Machine Learning, ICML 2015, 2:1530–1538, 2015. ISSN
9781510810587."
REFERENCES,0.5760233918128655,"Oren Rippel and Ryan Prescott Adams. High-dimensional probability estimation with deep density
models. arXiv preprint arXiv:1302.5125, 2013."
REFERENCES,0.5789473684210527,"Bob Stienen and Rob Verheyen. Phase Space Sampling and Inference from Weighted Events with
Autoregressive Flows. SciPost Physics, 10(2):038, February 2021. ISSN 2542-4653. doi: 10.
21468/SciPostPhys.10.2.038."
REFERENCES,0.5818713450292398,"Esteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms.
Communications on Pure and Applied Mathematics, 66(2):145–164, 2013."
REFERENCES,0.5847953216374269,"Esteban G Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood.
Communications in Mathematical Sciences, 8(1):217–233, 2010."
REFERENCES,0.5877192982456141,"The HSF Physics Event Generator WG, Andrea Valassi, Efe Yazgan, Josh McFayden, Simone
Amoroso, Joshua Bendavid, Andy Buckley, Matteo Cacciari, Taylor Childers, Vitaliano Ciulli,
Rikkert Frederix, Stefano Frixione, Francesco Giuli, Alexander Grohsjean, Christian Gütschow,
Stefan Höche, Walter Hopkins, Philip Ilten, Dmitri Konstantinov, Frank Krauss, Qiang Li, Leif
Lönnblad, Fabio Maltoni, Michelangelo Mangano, Zach Marshall, Olivier Mattelaer, Javier Fer-
nandez Menendez, Stephen Mrenna, Servesh Muralidharan, Tobias Neumann, Simon Plätzer,
Stefan Prestel, Stefan Roiser, Marek Schönherr, Holger Schulz, Markus Schulz, Elizabeth Sexton-
Kennedy, Frank Siegert, Andrzej Siódmok, and Graeme A. Stewart. Challenges in Monte Carlo
event generator software for High-Luminosity LHC. Computing and Software for Big Science, 5
(1):12, December 2021. ISSN 2510-2036, 2510-2044. doi: 10.1007/s41781-021-00055-1."
REFERENCES,0.5906432748538012,"Quan Zheng and Matthias Zwicker. Learning to Importance Sample in Primary Sample Space.
Computer Graphics Forum, 38(2):169–179, 2019. ISSN 1467-8659. doi: 10.1111/cgf.13628."
REFERENCES,0.5935672514619883,"A
EVENT GENERATION WITH THE VETO ALGORITHM"
REFERENCES,0.5964912280701754,"Generating i.i.d points following an arbitrary probability distributions in high dimensions is not a
priori a trivial task. A straightforward way to obtain such data is to use rejection sampling, which can
be based on any distribution q from which we can sample. Given an i.i.d sample x1, . . . , xN ∼q,
we can deﬁne weights w(xi) = p(xi)/q(xi) and keep/reject points with probability w(xi)/wmax."
REFERENCES,0.5994152046783626,"The main metric for evaluating the performance of this algorithm is the unweighting efﬁciency: how
much data is kept from an original sample of size N on average, which is expressed as"
REFERENCES,0.6023391812865497,"ϵunw = E
x∼q
w(x)"
REFERENCES,0.6052631578947368,"wmax
.
(11)"
REFERENCES,0.6081871345029239,"B
BACKWARD SAMPLING ALGORITHM"
REFERENCES,0.6111111111111112,"Algorithm 2: Backward sampling training in ZÜNIS
Data: Parametric PDF p(x, θ)
Result: Trained PDF p(x, θ)"
FOR M STEPS DO,0.6140350877192983,1 for M steps do
FOR M STEPS DO,0.6169590643274854,"2
Sample a batch x1, . . . , xnbatch from q;"
FOR M STEPS DO,0.6198830409356725,"3
Compute the sampling PDF values q(xi);"
FOR M STEPS DO,0.6228070175438597,"4
Compute function values f(xi);"
FOR M STEPS DO,0.6257309941520468,"5
Start tracking gradients with respect to θ;"
FOR N STEPS DO,0.6286549707602339,"6
for N steps do"
FOR N STEPS DO,0.631578947368421,"7
Compute the PDF values from the parametric PDF p(xi, θ);"
FOR N STEPS DO,0.6345029239766082,"8
Estimate the loss ˆL = 1 N"
FOR N STEPS DO,0.6374269005847953,"nbatch
X i=1"
FOR N STEPS DO,0.6403508771929824,f(xi)2
FOR N STEPS DO,0.6432748538011696,"p(x, θ)q(x);"
FOR N STEPS DO,0.6461988304093568,"9
Compute ∇θ ˆL using backpropagation;"
FOR N STEPS DO,0.6491228070175439,"10
Set θ ←θ −η∇θ ˆL;"
FOR N STEPS DO,0.652046783625731,"11
Reset gradients;"
END,0.6549707602339181,"12
end"
END,0.6578947368421053,13 end
END,0.6608187134502924,"14 return p(x, θ);"
END,0.6637426900584795,"C
FUNDAMENTAL LIMITATIONS OF THE VEGAS ALGORITHM"
END,0.6666666666666666,We deﬁne three functions over the two dimensional hypercube:
END,0.6695906432748538,fcamel(x) = exp 
END,0.672514619883041,"−
x −µ1) σ 2! + exp "
END,0.6754385964912281,"−
x −µ2) σ 2!"
END,0.6783625730994152,",
(12)"
END,0.6812865497076024,"f∅(x) = min """
END,0.6842105263157895,"1, exp "
END,0.6871345029239766,"−
|x| −r σ∅ 2! + exp "
END,0.6900584795321637,"−
a · x σ∅ 2!# (13)"
END,0.6929824561403509,"fsin(x) = cos (k · x)2 ,
(14)"
END,0.695906432748538,"to which we refer respectively as the camel, slashed circle and sinusoidal target functions. We set
their parameters as follows"
END,0.6988304093567251,"µ1 =

0.25
0.25"
END,0.7017543859649122,"
, µ2 =

0.75
0.75"
END,0.7046783625730995,"
, a =

1
−1"
END,0.7076023391812866,"
, k =

6
6"
END,0.7105263157894737,"
, σ = 0.1, σ∅= 0.1, r = 0.3
(15)"
END,0.7134502923976608,We chose these functions because they illustrate different failure modes of the VEGAS algorithm:
END,0.716374269005848,"• Because of the factorized PDF of VEGAS, the camel function leads to ’phantom peaks’ in
the off diagonal. This problem grows exponentially with the number of dimensions but can
be addressed by a change of variable to align the peaks with an axis of integration."
END,0.7192982456140351,"• The sinuoidal function makes it nearly impossible for VEGAS to provide any improvement:
the marginal PDF over each variable of integration is nearly constant. Again this type of
issue can be addressed by a change of variable provided one knows how to perform it."
END,0.7222222222222222,"• The slashed circle function is an example of a hard-for-VEGAS function that cannot be
improved signiﬁcantly by a change of variables. One can instead use multi-channeling, but
this requires a lot of prior knowledge and has a computational costs since each channel is
its own integral."
END,0.7251461988304093,"D
SUPPLEMENTARY RESULTS"
END,0.7280701754385965,"D.1
QUALITATIVE EXAMPLES"
END,0.7309941520467836,"Figure 7: Mapping between the uniform point density in the latent space and the target distribution
for the camel function, the sinusoidal function, the slashed circle function. Points are colored based
on their position in latent space."
END,0.7339181286549707,"D.2
SYSTEMATIC BENCHMARK DETAILS"
END,0.7368421052631579,"Dimension
σ1d
rel. st.d
2
4
8
16
32"
END,0.7397660818713451,Camel param. σ
END,0.7426900584795322,"2.00 × 10−2
8.93 × 10−2
2.04 × 10−1
3.61 × 10−1
5.06 × 10−1
0.001
14.1
6.32 × 10−2
1.64 × 10−1
3.14 × 10−1
4.64 × 10−1
6.06 × 10−1
0.01
4.41
2.21 × 10−1
3.78 × 10−1
5.23 × 10−1
6.67 × 10−1
8.25 × 10−1
0.1
1.22
4.51 × 10−1
5.93 × 10−1
7.43 × 10−1
9.11 × 10−1
1.10
0.3
0.56
6.44 × 10−1
7.99 × 10−1
9.74 × 10−1
1.18
1.41
0.5
0.32
8.62 × 10−1
1.05
1.26
1.51
1.81
0.7
0.19
1.21
1.45
1.73
2.07
2.47
1.0
0.10"
END,0.7456140350877193,"Table 3: Setup of the 35 different camel functions considered to benchmark ZÜNIS. We scan over
function relative standard deviations, which correspond to different σ parameters for each dimen-
sion(eq. (12)). We provide the corresponding width of a 1D gaussian (σ1d) with the same variance
for reference."
END,0.7485380116959064,"Dimension
σ1d
rel. st.d
2
4
8
16
32"
END,0.7514619883040936,VEGAS speed-up
END,0.7543859649122807,"7.6+2.4
−2.4 × 101
6.0+1.2
−1.0 × 102
4.3+1.4
−1.0 × 103
9.0+4.7
−3.1 × 102
3.5+2.7
−1.8 × 100
0.001 14.11
2.0+0.4
−0.4 × 102
5.8+0.8
−1.0 × 102
5.1+0.8
−0.9 × 102
2.8+0.3
−0.3 × 102
4.5+0.6
−0.8 × 101
0.01
4.41
3.4+0.5
−0.3 × 102
1.0+0.1
−0.1 × 102
3.5+0.1
−0.2 × 101
1.9+0.1
−0.1 × 101
1.2+0.1
−0.1 × 101
0.1
1.22
6.3+0.4
−0.3 × 101
2.0+0.0
−0.1 × 101
7.2+0.3
−0.3 × 100
2.8+0.1
−0.1 × 100
1.3+0.1
−0.0 × 100
0.3
0.56
1.2+0.1
−0.1 × 101
3.5+0.1
−0.2 × 100 9.1+0.6
−0.4 × 10−1 3.7+0.2
−0.2 × 10−1 2.0+0.1
−0.1 × 10−1
0.5
0.33
2.4+0.2
−0.2 × 100 5.3+0.5
−0.4 × 10−1 1.3+0.1
−0.1 × 10−1 5.5+0.4
−0.3 × 10−2 3.0+0.1
−0.2 × 10−2
0.7
0.20
4.0+0.6
−0.5 × 10−1 7.8+0.6
−0.5 × 10−2 1.6+0.1
−0.1 × 10−2 7.8+0.8
−0.6 × 10−3 4.8+0.3
−0.4 × 10−3
1.0
0.11"
END,0.7573099415204678,"Table 4: Variance reduction factor compared to VEGAS for each of the 35 different camel setups
deﬁned in table 3."
END,0.7602339181286549,"D.3
COMPARING ZÜNIS WITH UNIFORM SAMPLING ON MATRIX ELEMENTS"
END,0.7631578947368421,"(a)
(b)"
END,0.7660818713450293,"Figure 8: Average performance of ZÜNIS over 20 runs relative to ﬂat sampling, measured by the
relative speed-up in 8a and by the relative unweighting efﬁciency in 8b."
END,0.7690058479532164,"D.4
EFFECT OF SURVEY STRATEGIES"
END,0.7719298245614035,"In the following, we want to investigate how the integration of the three example processes in 4.3
with ZÜNIS behaves relative to VEGAS in dependence of the choice of the loss function, the survey
strategy and the number of epochs during training. For all other options, the default values are chosen
again, except for the number of points during the survey phase, which is set to 500,000."
END,0.7748538011695907,"Figure 9a shows that for a simple process like e−µ →e−µ via Z , where no correlations exist, ZÜ-
NIS cannot reach the speed-up achieved by VEGAS. Variance loss seems to lead to higher variance
improvements than DKL loss. Contrary, 9b shows that ZÜNIS can greatly improve the unweighting
efﬁciency for this process. The effect is again consistently stronger when using variance loss. Using
a ﬂat survey strategy suffers for both loss functions from overﬁtting, whereas adaptive sampling in
average performs slightly better and does not show overﬁtting."
END,0.7777777777777778,"d ¯d →d ¯d via Z presents a more realistic use case, as the parton distribution functions introduce
correlations between the integration variables which present a challenge to the VEGAS algorithm."
END,0.7807017543859649,"For this process, both the speed-up and the unweighting efﬁciency ratio clearly favor the variance
loss again, which outperforms in both metrics the DKL loss when multiple epochs are used, as can
be seen in 10. The richer structure of the integrand reduces the effect of overﬁtting. Therefore, the
performance increases or stays approximately constant except for the combination of DKL loss and
the forward survey strategy. For the variance loss, it becomes apparent that the ﬂat survey strategy
increases much slower in performance than alternative strategies as a function of the number of
epochs. However, for combination of losses and survey strategies, VEGAS could be outperformed
substantially in terms of integration speed. (a) (b)"
END,0.783625730994152,"Figure 9: Median of the performance of ZÜNIS over 20 runs relative to VEGAS for the process
e−µ →e−µ via Z depending on the loss function, the survey strategy and number of epochs,
measured by the relative speed-up in 9a and by the relative unweighting efﬁciency in 9b. (a) (b)"
END,0.7865497076023392,"Figure 10: Median of the performance of ZÜNIS over 20 runs relative to VEGAS for the process
d ¯d →d ¯d via Z depending on the loss function, the survey strategy and number of epochs, measured
by the relative speed-up in 10a and by the relative unweighting efﬁciency in 10b."
END,0.7894736842105263,"An opposite picture is drawn by the process uc →ucg in ﬁgure 11, for which, apart from the ﬂat
survey strategy, DKL loss is in general favored both for integration speed as well as unweighting
efﬁciency ratio. The adaptive survey strategy is here giving the best results, although for a high
number of epochs causes overﬁtting for the unweighting efﬁciency. (a) (b)"
END,0.7923976608187134,"Figure 11: Median of the performance of ZÜNIS over 20 runs relative to VEGAS for the process
uc →ucg depending on the loss function, the survey strategy and number of epochs, measured by
the relative speed-up in 11a and by the relative unweighting efﬁciency in 11b."
END,0.7953216374269005,"The take-home message of this section is, one the one hand, that the ﬂat survey strategy is in general
not recommended. Apart from this, the most important mean to improve the quality of importance
sampling are testing whether, independent of the survey strategy, the loss function should be chosen
differently."
END,0.7982456140350878,"E
EXACT MINIMZATION OF THE NEURAL IMPORTANCE SAMPLING
ESTIMATOR VARIANCE"
END,0.8011695906432749,"Let us show that the optimal probability distribution for importance sampling is the function itself.
Explicitly, as discussed in section 2.1, we want to ﬁnd the probability distribution p deﬁned over
some domain Ωwhich minimizes the variance of the estimator f(X)/p(X), for X ∼p(X). We
showed that this amounts to solving the following optimization problem:"
END,0.804093567251462,"min
p L(p) =
Z"
END,0.8070175438596491,"Ω
dxf(x)2"
END,0.8099415204678363,"p(x) ,
such that
Z
dx p(x) = 1,
(16)"
END,0.8128654970760234,which we can encode using Lagrange multipliers
END,0.8157894736842105,"p = arg min L(p, λ) =
Z"
END,0.8187134502923976,"Ω
dxf(x)2"
END,0.8216374269005848,"p(x) + λ

p(x) −
1
V (Ω)"
END,0.8245614035087719,"
.
(17)"
END,0.827485380116959,We can now solve this problem by ﬁnding extrema with functional derivatives
END,0.8304093567251462,"δL(p, λ)"
END,0.8333333333333334,"δp(x)
= λ −f(x)2"
END,0.8362573099415205,"p(x)2 ,
(18)"
END,0.8391812865497076,"which indeed is zero if p(x) ∝|f(x)|. Furthermore, this extremum is certainly a minimum because
the loss function is positive and unbounded. Indeed, if we separate Ωinto two disjoint measurable
subdomains Ω1 and Ω2, and deﬁne pα(x) such that points are drawn uniformly over Ω1 with prob-
ability α and uniformly over Ω2 with probability 1 −α, then the resulting loss function would be"
END,0.8421052631578947,L(pα) = V (Ω1) α Z
END,0.8450292397660819,"Ω1
dx f(x)2 + V (Ω2) 1 −α Z"
END,0.847953216374269,"Ω2
dx f(x)2,
(19)"
END,0.8508771929824561,which can be made arbitrarily large by sending α to 0.
END,0.8538011695906432,"F
HIGH-LEVEL CONCEPTS OF THE ZÜNIS API"
END,0.8567251461988304,"F.1
NORMALIZING FLOWS WITH FL O W CLASSES"
END,0.8596491228070176,"Flows map batches of points and their densities.
The ZÜNIS library implements normalizing
ﬂows by specifying a general interface deﬁned as a Python abstract class: GeneralFlow. All
ﬂow models in ZÜNIS are child classes of GeneralFlow, which itself inherits from the Pytorch
nn.Module interface."
END,0.8625730994152047,"As deﬁned in section 2.2, a normalizing ﬂow in ZÜNIS is a bijective mapping between two d
dimensional spaces, which in practice are always the unit hypercube [0, 1]d or Rd, with a tractable
Jacobian so that it can be used to map probability distributions. To this end, the GeneralFlow
interface deﬁnes normalizing ﬂows as a callable Python object which acts on batches of point drawn
from a known PDF p. A batch of points xi with their PDF values is encoded as a Pytorch Tensor
object X organized as follows"
END,0.8654970760233918,"X = (X1, . . . , Xbatch) ∈Rbatch × Rd+1,
(20)
where each Xi corresponds to a points stacked with its negative log density Xi =  

"
END,0.868421052631579,"xi,1
...
xi,d
−log p(xi) "
END,0.8713450292397661,"

.
(21)"
END,0.8742690058479532,"Encoding point densities by their negative logarithm makes their transformation under normalizing
ﬂows additive. Indeed if we have a mapping Q with Jacobian determinant jQ, then x ∼p(x) is
mapped to y = Q(x) ∼˜p(y) such that
−log ˜p(y) = −log p(x) + log jQ(x).
(22)"
END,0.8771929824561403,"Coupling Cells are ﬂows deﬁned by an element-wise transform and a mask.
All ﬂow mod-
els used in ZÜNIS in practice are implemented as a sequence of coupling cell transformations
acting on a subset of the variables. The abstract class GeneralCouplingCell and its child In-
vertibleCouplingCell speciﬁes the general organization of coupling cells as needing to be
instantiated with"
END,0.8801169590643275,"• a dimension d
• a mask deﬁned as a list of boolean specifying which coordinates are transformed or not
• a transform that implements the mapping of the non-masked variables"
END,0.8830409356725146,"In release v1.0 of ZÜNIS two such classes are provided:
PWLinearCoupling and
PWQuadraticCoupling, which respectively implement the piecewise linear and piecewise
quadratic coupling cells proposed in (Müller et al., 2018). New coupling cells can easily be im-
plemented, as explained in appendix F.4. Both existing classes rely on dense neural networks for the
prediction of the shape of their one-dimensional piecewise-polynomial mappings, whose parameters
are set at instantiation."
END,0.8859649122807017,Here is how one can use a piecewise-linear coupling cell for sampling points
END,0.8888888888888888,"import torch
from zunis.models.flows.coupling_cells.piecewise_coupling."
END,0.8918128654970761,"piecewise_quadratic import PWQuadraticCoupling
d=2
N_batch=10
mask = [True,False]
x = torch.zeros((N_batch,d+1))
# Sample the d first entries uniformly, keep 0. for the negative log"
END,0.8947368421052632,"jacobian
x[...,:-1].uniform_()
print(x[0]) # [0.3377, 0.4362, 0.]
f = PWQuadraticCoupling(d=d,mask=mask)
y = f(x)
print(y[0]) # [0.3377, 0.4411, -0.0314]"
END,0.8976608187134503,"We provide further details of the use and possible parameters of ﬂows in the documentation of
ZÜNIS: https://zunis-anonymous.github.io/zunis/."
END,0.9005847953216374,"F.2
TRAINING WITH THE TRAINE R CLASS"
END,0.9035087719298246,"The design of the ZÜNIS library intentionally restricts Flow models to being bijective mappings
instead of being ways to evaluate and sample from PDFs so as not to restrict their applicability
(see Brehmer & Cranmer (2020) for an example). The speciﬁc application in which one uses a
normalizing ﬂow, and in our case how precisely one samples from it, is intimately linked to how
such a model is trained. As a result, ZÜNIS bundles together the low-level training tools for Flow
models together with sampling tools inside the Trainer classes."
END,0.9064327485380117,"The general blueprint for such classes is deﬁned in the GenericTrainerAPI abstract class while
the main implementation for users is provided as StatefulTrainer. At instantiation, all trainers
expect a Flow model and flow_prior which samples point from a ﬁxed PDF in latent space.
These two elements together deﬁne a probability distribution over the target space from which one
can sample."
END,0.9093567251461988,There are two main ways one interacts with Trainers:
END,0.9122807017543859,"• One can sample points from the PDF deﬁned by the model and the prior using the sam-
ple_forward method.
• One can train over a pre-sample batch of points, their sampling PDF and the corresponding
function values using train_on_batch(self, x, px, fx)"
END,0.9152046783625731,"In practice, we expect that the main way users will use Trainers is for sampling pre-trained
models. In the context of particle physics simulations for example, unweighting is a common task,
which aims at sampling exactly from a known function f. A common approach is the Hit-or-miss
algorithm (James, 1980), whose efﬁciency is improved by sampling from a PDF approaching f.
This is how one would use a trainer trained on f:"
END,0.9181286549707602,"# [...]
# import or train a trainer ‘pretrained_trainer‘
import torch"
END,0.9210526315789473,"# Sampling points
xj = pretrained_trainer.sample_forward(100)
x = x[:, :-1]
px = (-x[:,-1]).exp()
fx = f(x)"
END,0.9239766081871345,"# Applying the veto algorithm
fmax = fx.max()
veto = (fx/fmax - torch.zeros_like(fx).uniform_(0.,1.)) > 0.
x_unweighted = x[veto]
# x_unweighted follows the PDF obtained by normalizing f."
END,0.9269005847953217,"F.3
INTEGRATING WITH THE INT E G R A T O R CLASS"
END,0.9298245614035088,"Integrators are intended as the main way for standard users to interact with ZÜNIS. They provide a
high-level interface to the functionalities of the library and only optionally require users to know to
what lower levels of abstractions really entail and to what their options correspond. From a practical
point of view, the main interface of ZÜNIS for integration is implemented as the Integrator,
which is a factory function that instantiates the appropriate integrator class based on a choice of
options."
END,0.9327485380116959,"All integrators follow the same pattern, deﬁned in the SurveyRefineIntegratorAPI and
BaseIntegrator abstract classes. Integration start by performing a survey phase, in which it
optimizes the way it samples points and then a reﬁne phase, in which it computes the integral by
using its learned sampler. Each phase proceeds through a number of steps, which can be set at
instantiation or when integrating:"
END,0.935672514619883,"# Setting values at instantiation time
integrator = Integrator(d=d, f=f, n_iter_survey=3, n_iter_refine=5)
# Override at integration time
integral_data = integrator.integrate(n_survey=10, n_refine=10)"
END,0.9385964912280702,"For both the survey and the reﬁne phases, using multiple steps is useful to monitor the stability of
the training and of the integration process: if one step is not within a few standard deviations of the
next, either the sampling statistics are too low, or something is wrong. For the reﬁne stage, this is
the main real advantage of using multiple steps. On the other hand, at each new survey step, a new
batch of points is re-sampled, which can be useful to mitigate overﬁtting."
END,0.9415204678362573,"By default, only the integral estimates obtained during the reﬁne stage are combined to compute
the ﬁnal integral estimate, and their combination is performed by taking their average. Indeed,
because the model is trained during the survey step, the points sampled during the reﬁne stage are
correlated in an uncontrolled way with the points used during training. Ignoring the survey stage
makes all estimates used in the combination independent random variables, which permits us to
build a formally correct estimator of the variance of the ﬁnal result."
END,0.9444444444444444,"F.4
IMPLEMENTING NEW COUPLING CELLS"
END,0.9473684210526315,"To implement a new invertible coupling cell inheriting from InvertibleCouplingCell, one
must provide an InvertibleTransform object and deﬁne a callable attribute T computing the
parameters of the transform. For example, consider a very simple linear coupling cell over R"
END,0.9502923976608187,"y = Q(x) :

yA = xA"
END,0.9532163742690059,"yB = exp
 
T(xA)

× xB,
(23)"
END,0.956140350877193,where T(xA) is a scalar strictly positive value. This can be deﬁned in the following way in ZÜNIS
END,0.9590643274853801,"import torch
from zunis.models.flows.coupling_cells.general_coupling import"
END,0.9619883040935673,"InvertibleCouplingCell
from zunis.models.flows.coupling_cells.transforms import"
END,0.9649122807017544,"InvertibleTransform
from zunis.models.layers.trainable import ArbitraryShapeRectangularDNN"
END,0.9678362573099415,class LinearTransform(InvertibleTransform):
END,0.9707602339181286,"def forward(self,x,T):"
END,0.9736842105263158,"alpha = torch.exp(T)
logj = T*x.shape[-1]
return x*alpha, logj
def backward(self,x,T):"
END,0.9766081871345029,"alpha = torch.exp(-T)
logj = -T*x.shape[-1]
return x*alpha, logj"
END,0.97953216374269,class LinearCouplingCell(InvertibleCouplingCell):
END,0.9824561403508771,"def __init__(self, d, mask, nn_width, nn_depth):"
END,0.9853801169590644,"transform = LinearTransform()
super(LinearCouplingCell, self).__init__(d=d, mask=mask,transform="
END,0.9883040935672515,"transform)
d_in = sum(mask)
self.T = ArbitraryShapeRectangularDNN(d_in=d_in,"
END,0.9912280701754386,"out_shape=(1,),
d_hidden=nn_width,
n_hidden=nn_depth)"
END,0.9941520467836257,"G
HARDWARE SETUP DETAILS"
END,0.9970760233918129,"The computations presented in this work were performed on a computing cluster using a Intel(R)
Xeon(R) Gold 5218 CPU @ 2.30GHz with 376 GB RAM. Processes which could be performed on
the GPU were done on a GeForce RTX 2080 having 12 GB memory and running on CUDA 11.0."
