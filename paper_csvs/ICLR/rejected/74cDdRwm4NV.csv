Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001059322033898305,"Reward shaping (RS) is a powerful method in reinforcement learning (RL) for
overcoming the problem of sparse or uninformative rewards. However, RS typically
relies on manually engineered shaping-reward functions whose construction is
time-consuming and error-prone. It also requires domain knowledge which runs
contrary to the goal of autonomous learning. We introduce Reinforcement Learning
Optimising Shaping Algorithm (ROSA), an automated RS framework in which the
shaping-reward function is constructed in a novel Markov game between two agents.
A reward-shaping agent (Shaper) uses switching controls to determine which states
to add shaping rewards and their optimal values while the other agent (Controller)
learns the optimal policy for the task using these shaped rewards. We prove that
ROSA, which easily adopts existing RL algorithms, learns to construct a shaping-
reward function that is tailored to the task thus ensuring efﬁcient convergence
to high performance policies. We demonstrate ROSA’s congenial properties in
three carefully designed experiments and show its superior performance against
state-of-the-art RS algorithms in challenging sparse reward environments."
INTRODUCTION,0.00211864406779661,"1
INTRODUCTION"
INTRODUCTION,0.0031779661016949155,"Reinforcement learning (RL) offers the potential for autonomous agents to learn complex behaviours
without the need for human intervention [39]. Despite the notable success of RL in a variety domains
[8, 31, 35, 24], enabling RL algorithms to learn successfully in numerous real-world tasks remains
a challenge [41]. A key obstacle to the success of RL algorithms is the requirement of a rich reward
signal that can guide the agent towards an optimal policy [7]. In many settings of interest such as
physical tasks and video games, rich informative signals of the agent’s performance are not readily
available [14]. For example, in the video game Super Mario [33], the agent must perform sequences
of hundreds of actions while receiving no rewards for it to successfully complete its task. In this
setting, the sparse reward provides infrequent feedback of the agent’s performance. This leads to
RL algorithms requiring large numbers of samples (and high expense) for solving problems [14].
Consequently, there is great need for RL techniques that solve these problems efﬁciently."
INTRODUCTION,0.00423728813559322,"Various biological systems have mechanisms that produce rewards for activities that present little
or no direct biological utility. Such mechanisms, which have been fashioned over millions of years
are designed to provide intrinsic rewards to promote behaviours that improve chances of outcomes
with biological utility [1]. In RL, reward shaping (RS) is a tool to introduce intrinsic rewards known
as shaping rewards that supplement environment reward signals. These rewards can encourage explo-
ration and insert structural knowledge in the absence of informative environment rewards which can
improve learning outcomes [10]. In general however, RS algorithms assume hand-crafted and domain-
speciﬁc shaping functions whose construction is typically highly labour intensive. This runs contrary
to the aim of autonomous learning. Moreover, poor choices of shaping rewards can worsen the agent’s
performance [9]. To resolve these issues, a useful shaping reward must be obtained autonomously."
INTRODUCTION,0.005296610169491525,"Inspired by naturally occurring systems, we tackle the problem of sparse and uninformative rewards
by developing a framework that autonomously constructs shaping rewards during learning. Our
framework, ROSA, works by introducing an additional RL agent, Shaper, that adaptively learns to
construct shaping rewards by observing Controller (the agent whose goal is to solve the environment
task) while Controller learns to solve its task. This generates tailored shaping rewards without the"
INTRODUCTION,0.006355932203389831,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.007415254237288136,"need for domain knowledge or manual engineering. The shaping rewards supplement the environment
reward and promote effective learning, our framework therefore addresses the key challenges in RS."
INTRODUCTION,0.00847457627118644,"The resulting framework is a two-player nonzero-sum Markov game (MG) [34] – an extension of
Markov decision process (MDP) that involves two independent learners with distinct objectives. In
our framework, the two agents that have distinct learning agendas, cooperate to achieve Controller’s
objective. This MG formulation confers various advantages:
1) The shaping-reward function is constructed fully autonomously. The game also ensures the shaping
reward improves Controller’s performance unlike RS methods that can lower performance.
2) By learning the shaping-reward function while Controller learns its optimal policy, Shaper learns
to adaptively facilitate Controller’s learning and improve Controller’s performance.
3) Both learning processes converge so Controller learns the optimal value function for its task.
4) ROSA learns subgoals [28] to decompose complex tasks and promote complex exploration patterns."
INTRODUCTION,0.009533898305084746,"An integral component of ROSA is a novel combination of RL and switching controls [2, 22]. This
enables Controller to quickly determine useful states to learn to add and calibrate shaping rewards
(i.e. the states in which adding shaping rewards improve Controller’s performance) and disregard
others. This is in contrast with an RL controller (i.e. Controller) which must learn its best actions at
every state. This leads to Shaper quickly ﬁnding shaping rewards that guide Controller’s learning
process toward optimal trajectories (and away from suboptimal trajectories, c.f. Experiment 1)."
INTRODUCTION,0.01059322033898305,"For our two-player framework to succeed we have to overcome several obstacles. Solving MGs
involves ﬁnding a stable point in which each player responds optimally to the actions of the other.
In our MG, this stable point describes a pair of policies for which Shaper introduces a shaping
reward that improves performance and, with that, Controller executes an optimal policy for the
task. Tractable methods for solving MGs are rare with convergence of MG methods being seldom
guaranteed except in a few special cases [42]. Nevertheless, using special features in the design of our
game, we prove the existence of a stable point solution of our MG. We then prove the convergence of
our learning method to the solution of the game and show that the solution coincides with the solution
of the Controller’s problem. This ensures Shaper learns a shaping-reward function that improves
Controller’s performance and that Controller learns the optimal value function for the task."
RELATED WORK,0.011652542372881356,"2
RELATED WORK"
RELATED WORK,0.012711864406779662,"Reward Shaping (RS) adds a shaping function F to supplement the agent’s reward to boost learning.
RS however has some critical limitations. First, RS does not offer a means of ﬁnding F. Second,
poor choices of F can worsen the agent’s performance [9]. Last, adding shaping rewards can change
the underlying problem therefore generating policies that are completely irrelevant to the task [20].
In [27] it was established that potential-based reward shaping (PBRS) which adds a shaping function
of the form F(st+1, st) = γφ(st+1) −φ(st) preserves the optimal policy of the problem. Recent
variants of PBRS include potential-based advice which deﬁnes F over the state-action space [13]
and approaches that include time-varying shaping functions [11]. Although the last issue can be
addressed using potential-based reward shaping (PBRS) [27], the ﬁrst two issues remain. To avoid
manual engineering of F, useful shaping rewards must be obtained autonomously. Towards this [45]
introduce an RS method that adds a shaping-reward function prior which ﬁts a distribution from data
obtained over many tasks. Recently, [16] use a bilevel technique to learn a scalar coefﬁcient for an
already-given shaping-reward function. Nevertheless, constructing F while training can produce
convergence issues since the reward function now changes during training. [17]. Moreover, while F
is being learned the reward can be corrupted by inappropriate signals that hinder learning.
Curiosity based reward shaping aims to encourage the agent to explore states by rewarding the
agent for novel state visitations using exploration heuristics. One approach is to use state visitation
counts [29]. More elaborate approaches such as [6] introduce a measure of state novelty using the
prediction error of features of the visited states from a random network. [30] use the prediction error
of the next state from a learned dynamics model and [15] maximise the information gain about the
agent’s belief of the system dynamics. In general, these methods provide no performance guarantees
nor do they ensure the optimal policy (of the underlying MDP) is preserved. Moreover, they naively
reward exploration to unvisited states without consideration of the environment reward. This can lead
to spurious objectives being maximised (see Experiment 3 in §6)."
RELATED WORK,0.013771186440677966,"Within these two categories, closest to our work are bilevel approaches for learning the shaping
function [16, 36]. Unlike [16] which requires a useful shaping reward to begin with, ROSA constructs"
RELATED WORK,0.014830508474576272,Under review as a conference paper at ICLR 2022
RELATED WORK,0.015889830508474576,"a shaping reward function from scratch leading to a fully autonomous method. Moreover, in [16,
36], the agent’s policy and shaping rewards are learned with consecutive updates. In contrast, ROSA
performs these operations concurrently leading to a faster, more efﬁcient procedure. Also in contrast
to [16, 36], ROSA learns shaping rewards only at relevant states, this confers high computational
efﬁciency (see Experiment 2, §6)). As we describe, ROSA, which successfully learns the shaping-
reward function F, uses a similar form as PBRS. However in ROSA, F is augmented to include
the actions of another RL agent to learn the shaping rewards online. Lastly, unlike curiosity-based
methods e.g., [6, 30], our method preserves the agent’s optimal policy for the task (see Experiment 3,
§6) and introduces intrinsic rewards that promote complex learning behaviour (see Experiment 1, §6) ."
PRELIMINARIES & NOTATIONS,0.01694915254237288,"3
PRELIMINARIES & NOTATIONS"
PRELIMINARIES & NOTATIONS,0.018008474576271187,"In RL, an agent sequentially selects actions to maximise its expected returns. The underlying problem
is typically formalised as a MDP ⟨S, A, P, R, γ⟩where S is the set of states, A is the discrete set of
actions, P : S × A × S →[0, 1] is a transition probability function describing the system’s dynamics,
R : S × A →R is the reward function measuring the agent’s performance, and the factor γ ∈[0, 1)
speciﬁes the degree to which the agent’s rewards are discounted over time [39]. At time t the system
is in state st ∈S and the agent must choose an action at ∈A which transitions the system to a
new state st+1 ∼P(·|st, at) and produces a reward R(st, at). A policy π : S × A →[0, 1] is a
probability distribution over state-action pairs where π(a|s) represents the probability of selecting
action a ∈A in state s ∈S. The goal of an RL agent is to ﬁnd a policy π⋆∈Π that maximises its
expected returns given by the value function: vπ(s) = E[P∞
t=0 γtR(st, at)|at ∼π(·|st)] where Π is
the agent’s policy set. We denote this MDP by M."
PRELIMINARIES & NOTATIONS,0.019067796610169493,"A two-player Markov game (MG) is an augmented MDP involving two agent that simultaneously
take actions over many rounds [34]. In the classical MG framework, each agent’s rewards and the
system dynamics are now inﬂuenced by the actions of both agents. Therefore, each agent i ∈{1, 2}
has its reward function Ri : S × A1 × A2 →R and action set Ai and its goal is to maximise its own
expected returns. The system dynamics, now inﬂuenced by both agents, are described by a transition
probability P : S × A1 × A2 × S →[0, 1]. As we discuss in the next section, ROSA induces a
speciﬁc MG in which the dynamics are inﬂuenced by only Controller."
PRELIMINARIES & NOTATIONS,0.020127118644067795,"In RS the question of which φ to insert has not been addressed. Moreover it has been shown that
poor choices of φ hinder learning [9]. Consequently, in general RS methods rely on hand-crafted
shaping-reward functions that are constructed using domain knowledge (whenever available). In
the absence of a useful shaping-reward function F, the challenge is to learn a shaping-reward
function that leads to more efﬁcient learning while preserving the optimal policy. Naturally, we can
formalise the problem of learning such an F by constructing F as a parametric function of θ ∈Rm:
ˆF(st+1, st; θ) := γ ˆφ(st+1, θ) −ˆφ(st, θ). Now the problem is to ﬁnd θ⋆∈Rm for φ(s) = ˆφ(s, θ⋆)
such that F(st+1, st) = ˆF(st+1, st; θ⋆), i.e., we aim to ﬁnd θ⋆that yields a useful shaping-reward
function. Determining this function is a signiﬁcant challenge; poor choices can hinder the learning
process, moreover attempting to learn the shaping-function while learning the RL agent’s policy
presents convergence issues given the two concurrent learning processes [44]. Another issue is that
using an optimisation procedure to ﬁnd θ⋆directly does not make use of information generated by
intermediate state-action-reward tuples of the RL problem which can help to guide the optimisation."
OUR FRAMEWORK,0.0211864406779661,"4
OUR FRAMEWORK"
OUR FRAMEWORK,0.022245762711864406,"We now describe the problem setting, details of our framework, and how it learns the shaping-reward
function. We then describe Controller’s and Shaper’s objectives. We also describe the switching
control mechanism used by Shaper and the learning process for both agents."
OUR FRAMEWORK,0.023305084745762712,"To tackle the challenges described above, we introduce Shaper an adaptive agent with its own
objective that determines the best shaping rewards to give to Controller. Using observations of
the actions taken Controller, Shaper’s goal is to construct shaping rewards (which the RL learner
Controller cannot generate itself) to guide the Controller towards quickly learning its optimal policy.
To do this, Shaper learns how to choose the values of an shaping-reward at each state. Simultaneously,
Controller performs actions to maximise its rewards using its own policy. Crucially, the two agents
tackle distinct but complementary set problems. The problem for Controller is to learn to solve the"
OUR FRAMEWORK,0.024364406779661018,Under review as a conference paper at ICLR 2022
OUR FRAMEWORK,0.025423728813559324,"task by ﬁnding its optimal policy, the problem for Shaper is to learn how to add shaping rewards to
aid Controller. The objective for Controller is given by:"
OUR FRAMEWORK,0.026483050847457626,"vπ,π2
1
(s) = E "" ∞
X"
OUR FRAMEWORK,0.02754237288135593,"t=0
γt 
R(st, at) + ˆF(st, a2
t, st−1, a2
t−1)
 s = s0 # ,"
OUR FRAMEWORK,0.028601694915254237,"where a ∼π is Controller’s action, ˆF is the shaping-reward function which is given by ˆF(·) ≡
φ(st, a2
t) −γ−1φ(st−1, a2
t−1) for any st, st−1 ∈S and a2
t ∼π2 is chosen by Shaper (and a2
t ≡
0, ∀t < 0) using the policy π2 : S × A2 →[0, 1] where A2 ⊂Rp is the action set for Shaper. The
function φ : S × A2 →R is a continuous map that satisﬁes the condition φ(s, 0) ≡0 for any s ∈S
(φ can be, for example, a neural network with ﬁxed weights with input (s, a2), A2 can be for example
a set of integers {1, . . . , K}). Therefore, Shaper determines the output of ˆF (which it does through
its choice of a2
t). With this, Shaper constructs a shaping-reward function which is tailored for the
speciﬁc setting. The transition probability P : S×A×S →[0, 1] takes the state and only Controller’s
actions as inputs. Formally, the MG is deﬁned by a tuple G = ⟨N, S, A, A2, P, ˆR1, ˆR2, γ⟩where
the new elements are N = {1, 2} which is the set of agents, ˆR1 := R + ˆF is the new Controller
reward function which now contains a shaping reward ˆF, the function ˆR2 : S × A × A2 →R is
the one-step reward for Shaper (we give the details of this function later) and lastly the transition
probability P : S × A × S →[0, 1] takes the state and only Controller action as inputs."
OUR FRAMEWORK,0.029661016949152543,"As Controller’s policy can be learned using any RL method, ROSA easily adopts any existing RL
algorithm for Controller. Note that unlike reward-shaping methods e.g. [27], the function φ now
contains an action term a2 which is chosen by Shaper which enables the best shaping-reward function
to be learned online. The presence of an action term may spoil the policy invariance result in [27].
We however prove an policy invariance result (Prop. 1) analogous to that in [27] and show RIGA
preserves the optimal policy for M."
SWITCHING CONTROLS,0.03072033898305085,"4.1
SWITCHING CONTROLS"
SWITCHING CONTROLS,0.03177966101694915,"So far Shaper’s problem involves learning to construct shaping rewards at every state including
those that are irrelevant for guiding Controller. In order to increase the (computational) efﬁciency
of Shaper’s learning process, we now replace the policy space for Shaper with a form of policies
known as switching controls. This enables Shaper to decide at which states to learn the value of
shaping rewards it would like to add. Therefore, now Shaper is tasked with learning how to shape
Controller’s rewards only at states that are important for guiding Controller to its optimal policy.
This enables Shaper to quickly determine its policy π2 and how to choose the values of F unlike
Controller whose policy must learned for all states."
SWITCHING CONTROLS,0.03283898305084746,"Now at each state Shaper ﬁrst makes a binary decision to decide to switch on its shaping reward
F for Controller affecting a switch It which takes values in {0, 1}. This leads to an MG in which,
unlike classical MGs, Shaper now uses switching controls to perform its actions."
SWITCHING CONTROLS,0.03389830508474576,"The
new
Controller
objective
is:
vπ,π2
1
(s0, I0)
="
SWITCHING CONTROLS,0.034957627118644065,"E
hP∞
t=0 γt n
R(st, at) + ˆF(st, a2
t; st−1, a2
t−1)It
oi
, where Iτk+1
=
1 −Iτk, which is the"
SWITCHING CONTROLS,0.036016949152542374,"switch for the shaping rewards which is 0 or 1 (and It ≡0, ∀t ≤0) and {τk} are times that a switch
takes place for example if the switch is ﬁrst turned on at state s5 then turned off at s7, then τ1 = 5
and τ2 = 7 (we will shortly describe these in detail). The switch It is managed by Shaper, therefore
by switching It between 0 or 1, Shaper activates or deactivates the shaping reward."
SWITCHING CONTROLS,0.037076271186440676,"We now describe how at each state both the decision to activate a shaping reward and their magnitudes
are determined. Recall that a2
t ∼π2 determines the shaping reward through F. At any st, the decision
to turn on It and shape rewards is decided by a (categorical) policy g2 : S →{0, 1}. Therefore, g2
determines whether a (or no) Shaper policy π2 should be used to execute an action a2
t ∼π2. It can
now be seen the sequence of times τk = inf{t > τk−1|st ∈S, g2(st) = 1} are rules that depend on
the state. Hence, by learning an optimal g2, Shaper learns the best states to activate F."
SWITCHING CONTROLS,0.038135593220338986,Summary of events:
SWITCHING CONTROLS,0.03919491525423729,"At a time t ∈0, 1 . . ."
SWITCHING CONTROLS,0.04025423728813559,Under review as a conference paper at ICLR 2022
SWITCHING CONTROLS,0.0413135593220339,"•
Both players make an observation of the state st ∈S.
•
Controller takes an action at sampled from its policy π.
•
Shaper decides whether or not to activate the shaping reward using g2 : S →{0, 1}
•
If g2(st) = 0:
X◦The switch is not activated (It = 0). Controller receives a reward r ∼R(st, at) and
the system transitions to the next state st+1.
•
If g2(st) = 1:
X◦Shaper takes an action a2
t sampled from its policy π2.
X◦The switch is activated (It
=
1), Controller receives a reward R(st, at) +
ˆF(st, a2
t; st−1, a2
t−1) × 1 and the system transitions to the next state st+1."
SWITCHING CONTROLS,0.0423728813559322,"We set τk ≡0∀k ≤0 and a2
τk ≡0, ∀k ∈N (a2
τk+1, . . . , a2
τk+1−1 remain non-zero) and a2
k ≡
0 ∀k ≤0 and use the shorthand I(t) ≡It ."
SWITCHING CONTROLS,0.04343220338983051,"4.2
THE SHAPER’S OBJECTIVE"
SWITCHING CONTROLS,0.04449152542372881,"The goal of Shaper is to guide Controller to efﬁciently learn to maximise its own objective (given
in M). The shaping reward F is activated by switches controlled by Shaper. As we later describe,
the termination times occur according to some external (probabilistic) rule. To induce Shaper to
selectively choose when to switch on the shaping reward, each switch activation incurs a ﬁxed cost
for Shaper. The cost has two effects: ﬁrst it reduces the complexity of Shaper problem since its
decision space is to determine which subregions of S it should activate the shaping rewards (and their
magnitudes). Second, it ensures that the information-gain from Shaper encouraging Controller to
explore a given set of states is sufﬁciently high to merit activating the stream of rewards. Given these
remarks the objective for Shaper is given by vπ,π2"
SWITCHING CONTROLS,0.045550847457627115,"2
(s0, I0) = Eπ,π2   ∞
X"
SWITCHING CONTROLS,0.046610169491525424,"t=0
γt "
SWITCHING CONTROLS,0.04766949152542373,"ˆR1(st, It, at, a2
t, a2
t−1) + ∞
X"
SWITCHING CONTROLS,0.048728813559322036,"k≥1
c(It, It−1)δt
τ2k−1 + L(st)   "
SWITCHING CONTROLS,0.04978813559322034,".
(1)"
SWITCHING CONTROLS,0.05084745762711865,"The objective encodes Shaper agenda, namely to maximise the expected return.1 Therefore, using
its shaping rewards, Shaper seeks to guide Controller towards optimal trajectories (potentially away
from suboptimal trajectories, c.f. Experiment 1) and enable Controller to learn faster (c.f. Cartpole
experiment in Sec. 6). The function c : {0, 1}2 →R<0 is a strictly negative cost function which
imposes a cost for each switch and is modulated by the Kronecker-delta function δt
τ2k−1 which is 1
whenever t = τ2k−1 and 0 otherwise (this restricts the costs to only the points at which the shaping
reward is activated). Lastly, the term L : S →R is a Shaper bonus reward for when Controller
visits infrequently visited states and tends to 0 as the states are revisited. For this there are various
possibilities e.g. model prediction error [37], count-based exploration bonus [38]."
SWITCHING CONTROLS,0.05190677966101695,"With this, Shaper constructs a shaping-reward function that supports Controller’s learning which
is tailored for the speciﬁc setting. This avoids inserting hand-designed exploration heuristics into
Controller’s objective as in curiosity-based methods [6, 30] and classical reward shaping [27]. We
later prove that with this objective, Shaper’s optimal policy maximises Controller’s (extrinsic) return
(Prop. 1). Additionally, we show that the framework preserves the optimal policy of M."
SWITCHING CONTROLS,0.05296610169491525,"There are various possibilities for the termination times {τ2k} (recall that {τ2k+1} are the times
which the shaping reward F is switched on using g2). One is for Shaper to determine the sequence.
Another is to build a construction of {τ2k} that directly incorporates the information gain that a state
visit provides — we defer the details of this arrangement to Sec. 10 of the Appendix."
THE OVERALL LEARNING PROCEDURE,0.05402542372881356,"4.3
THE OVERALL LEARNING PROCEDURE"
THE OVERALL LEARNING PROCEDURE,0.05508474576271186,"The game G is solved using our multi-agent RL algorithm (ROSA). In the next section, we show the
convergence properties of ROSA. Here, we ﬁrst give a description of ROSA (the full code is in Sec. 8
of the Appendix). The ROSA algorithm consists of two independent procedures: Controller learns its
own policy while Shaper learns which states to perform a switch and the shaping reward magnitudes.
In our implementation, we used proximal policy optimization (PPO) [32] as the learning algorithm"
THE OVERALL LEARNING PROCEDURE,0.05614406779661017,"1Note that we can now see that ˆR2 ≡R(st, at) + ˆF(st, a2
t; st−1, a2
t−1)It + P∞
k≥1 c(It, It−1)δt
τ2k−1."
THE OVERALL LEARNING PROCEDURE,0.057203389830508475,Under review as a conference paper at ICLR 2022
THE OVERALL LEARNING PROCEDURE,0.05826271186440678,"for all policies: Controller’s policy, switching control policy, and the reward magnitude policy. For
Shaper L term we used L(st) := ∥ˆh(st) −h(st)∥2
2 as in RND [6] where h is a random initialised,
ﬁxed target network while ˆh is the predictor network that seeks to approximate the target network.
We constructed ˆF using a ﬁxed neural network f : Rd 7→Rm and a one-hot encoding of the action
of Shaper. Speciﬁcally, ˆφ(st, a2
t) := f(st) · i(a2
t) where i(a2
t) is a one-hot encoding of the action a2
t
picked by Shaper. Thus, ˆF(st, a2
t; st−1, a2
t−1) = f(st) · i(a2
t) −γ−1f(st−1) · i(a2
t−1). The action
set of Shaper is thus A2 := {0, 1, ..., m} where each element is an element of N, and π2 is a MLP
π2 : Rd 7→Rm. Precise details are in the Supplementary Materials Section 8."
THE OVERALL LEARNING PROCEDURE,0.059322033898305086,Algorithm 1: Reinforcement Learning Optimising Shaping Algorithm (ROSA)
THE OVERALL LEARNING PROCEDURE,0.06038135593220339,"Input: Initial Controller policy π0, Shaper policies g20, π2
0, RL learning algorithm ∆
Output: Optimised Controller policy π∗"
THE OVERALL LEARNING PROCEDURE,0.0614406779661017,"1 for t = 1, T do"
THE OVERALL LEARNING PROCEDURE,0.0625,"2
Given environment state st, sample at from π(st) and obtain st+1, rt+1 by applying at to
environment"
THE OVERALL LEARNING PROCEDURE,0.0635593220338983,"3
Evaluate g2(st) according to Prop. 2"
THE OVERALL LEARNING PROCEDURE,0.0646186440677966,"4
if g2(st) = 1 then"
THE OVERALL LEARNING PROCEDURE,0.06567796610169492,"5
Shaper samples an action a2
t+1 ∼π2(·|st+1)"
SHAPER COMPUTES RI,0.06673728813559322,"6
Shaper computes ri
t+1 = ˆF(st, a2
t, st+1, a2
t+1),"
SHAPER COMPUTES RI,0.06779661016949153,"7
Set shaped reward r = rt+1 + ri
t+1"
ELSE,0.06885593220338983,"8
else"
ELSE,0.06991525423728813,"9
Set r = rt+1"
ELSE,0.07097457627118645,"10
Update π, g2, π2 using (st, at, r, st+1) and ∆// Learn the individual policies"
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.07203389830508475,"5
CONVERGENCE AND OPTIMALITY OF OUR METHOD"
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.07309322033898305,"The ROSA framework enables Shaper to learn a shaping-reward function with which Controller
can learn the optimal policy for the task. The interaction between the two RL agents induces two
concurrent learning processes which can occasion convergence issues [44]. We now show that
our method converges and the solution ensures higher performing Controller policy than would be
achieved by solving M directly. To do this, we ﬁrst study the stable point solutions of G."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.07415254237288135,"Unlike MDPs, the existence of a stable point solution in Markov policies is not guaranteed for MGs
[5] and is rarely computable.2 MGs also often have multiple stable points that can be inefﬁcient
[25]; in G the outcome of such stable point proﬁles would be a poor performing Controller policy. To
ensure the framework is useful, we must verify that the solution of G corresponds to M. We solve
these challenges with the following scheme: [A] The method preserves the optimal policy of M. [B]
A stable point of the game G in Markov policies exists and is the convergence point of ROSA. [C]
The convergence point of ROSA yields a payoff that is (weakly) greater than that from solving M
directly. [D] ROSA converges to the stable point solution of G."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.07521186440677965,We now prove [A] which shows the solution to M is preserved under the inﬂuence of Shaper:
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.07627118644067797,Proposition 1 The following statements hold:
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.07733050847457627,"i) max
π∈Π vπ,π2
1
(s) = max
π∈Π vπ
1 (s), ∀s ∈S, ∀π2 ∈Π2, where vπ
1 (s) = E [P∞
t=0 γtR(st, at)]."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.07838983050847458,"ii) The Shaper’s optimal policy maximises vπ
1 (s) for any s ∈S."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.07944915254237288,"Result (i) says that the Controller’s problem is preserved under the inﬂuence of the Shaper. Moreover
the (expected) total return received by the agents is that from the environment (extrinsic rewards).
Result (ii) establishes that Shaper’s optimal policy induces Shaper to maximise its (Controller’s)
extrinsic total return. The result is established by a careful adaptation of the policy invariance result
in [27] to our multi-agent switching control framework in which the shaping reward is no longer
added at all states. Building on Prop. 1, we deduce the following result:"
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.08050847457627118,2Special exceptions are team MGs where agents share an objective and zero-sum MGs [34].
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.0815677966101695,Under review as a conference paper at ICLR 2022
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.0826271186440678,"Corollary 1 ROSA preserves the MDP M. In particular, let (ˆπ1, ˆπ2) be a stable point policy proﬁle3
of the MG induced by ROSA G then ˆπ1 is a solution to the MDP, M."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.0836864406779661,"Therefore, the introduction of Shaper does not alter the fundamentals of the problem."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.0847457627118644,"We now show that G belongs to a special class of MGs which possess a stable point Markov policies
which can be computed as a limit point of a sequence of Bellman operations. We later exploit this
result to prove the convergence of ROSA."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.0858050847457627,"We begin by deﬁning some objects which are central to the analysis.
For any π
∈
Π
and π2
∈
Π2, given a function V π,π2
:
S × N
→
R, we deﬁne the intervention
operator Mπ,π2 by Mπ,π2V π,π2(sτk, I(τk))
:=
ˆR1(sτk, I(τk), aτk, a2
τk, ·) + c(Ik, Ik−1) +
γ P"
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.08686440677966102,"s′∈S P(s′; aτk, s)V π,π2(s′, I(τk+1)) for any sτk ∈S and ∀τk where aτk ∼π(·|sτk) and where
a2
τk ∼π2(·|sτk). We deﬁne the Bellman operator T of the game G by TV π,π2(sτk, I(τk)) :="
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.08792372881355932,"max
n
Mπ,π2Λ(sτk, I(τk)), R(sτk, a) + γmax
a∈A
P"
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.08898305084745763,"s′∈S P(s′; a, sτk)Λ(s′, I(τk))
o
. Given a value"
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09004237288135593,"function {Vi}i∈{1,2}, the quantity MVi measures the expected future stream of rewards for agent i
after an immediate switch minus the cost of switching."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09110169491525423,We now show that a stable point of G can be computed using dynamic programming:
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09216101694915255,"Theorem 1 Let V : S×N →R then the game G has a stable point which is a given by lim
k→∞T kV π ="
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09322033898305085,"sup
ˆπ∈Π
V ˆπ = V π⋆, where ˆπ is a stable policy proﬁle for the MG, G."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09427966101694915,"Theorem 1 proves that the MG G (which is the game that is induced when Shaper inﬂuences
Controller) has a stable point which is the limit of a dynamic programming method. In particular,
it proves the that the stable point of G is the limit point of the sequence T 1V, T 2V, . . . ,. Crucially,
(by Corollary 1) the limit point corresponds to the solution of the MDP M. Theorem 1 is proven by
ﬁrstly proving that G has a dual representation as an MDP whose solution corresponds to the stable
point of the MG. Theorem 1 enables a distributed Q-learning method [4] to tractably solve the MG."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09533898305084745,"Having constructed a procedure to ﬁnd the optimal Controller policy, our next result characterises
Shaper policy g2 and the optimal times to activate F."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09639830508474577,"Proposition 2 The policy g2 is given by the following expression: g2(st) = H(Mπ,π2V π,π2 −
V π,π2)(st, It), ∀(st, It) ∈S ×{0, 1}, where V is the solution in Theorem 1 and H is the Heaviside
function, moreover Shaper’s switching times are τk = inf{τ > τk−1|Mπ,π2V π,π2 = V π,π2}."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09745762711864407,"Hence, Prop. 2 also characterises the (categorical) distribution g2. Moreover, given the function V ,
the times {τk} can be determined by evaluating if MV = V holds. In general, introducing shaping
rewards may undermine learning and worsen overall performance. We now prove that the ROSA
framework introduces an shaping rewards that yield higher total (environment) returns for Controller
as compared to solving M directly ([C])."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09851694915254237,"Proposition 3 Controller’s (extrinsic) expected return vπ,π2
1
whilst playing G is (weakly) higher
than vπ
1 , the (extrinsic) expected return for M i.e. vπ,π2
1
(s, ·) ≥vπ
1 (s), ∀s ∈S."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.09957627118644068,"Prop. 3 shows that the stable point of G improves outcomes for Controller. Unlike reward shaping
methods in general, the stable points generated never lead to a reduction to the total (environment)
return for Controller as compared to its total return without F. Note that by Prop. 1, Theorem 3
compares the environment (extrinsic) rewards accrued by the agents so that the presence of Shaper
increases the total expected environment rewards."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.10063559322033898,"We now prove the convergence to the solution with (linear) function approximators. In what follows,
we deﬁne a projection Π on a function Λ by: ΠΛ :=
arg min
¯Λ∈{Ψr|r∈Rp}"
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.1016949152542373,"¯Λ −Λ
."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.1027542372881356,"Theorem 2 ROSA converges to the stable point of G, moreover, given a set of linearly independent
basis functions Ψ = {ψ1, . . . , ψp} with ψk ∈L2, ∀k, ROSA converges to a limit point r⋆∈Rp which"
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.1038135593220339,"3By stable point proﬁle we mean a conﬁguration in which no agent can increase their expected return by
deviating unilaterally from their policy given the other agents’ policies, i.e. a Markov perfect equilibrium [12]."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.1048728813559322,Under review as a conference paper at ICLR 2022
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.1059322033898305,"is the unique solution to ΠF(Ψr⋆) = Ψr⋆where F is deﬁned by: FΛ := ˆR1 + γP max{MΛ, Λ}
where r⋆satisﬁes: ∥Ψr⋆−Q⋆∥≤(1 −γ2)−1/2 ∥ΠQ⋆−Q⋆∥."
CONVERGENCE AND OPTIMALITY OF OUR METHOD,0.10699152542372882,"Theorem 2 establishes the solution to G can be computed using ROSA. This means that Shaper
converges to a shaping-reward function that necessarily improves Controller’s performance and
Controller learns the optimal value function for the task. Secondly, the theorem establishes the
convergence of ROSA to the solution using (linear) function approximators. Lastly, the approximation
error is bounded by the smallest error that can be achieved given the basis functions."
EXPERIMENTS,0.10805084745762712,"6
EXPERIMENTS"
EXPERIMENTS,0.10911016949152542,"We performed a series of experiments to test if ROSA (1) learns a beneﬁcial shaping-reward function
(2) decomposes complex tasks into sub-goals, and (3) tailors shaping rewards to encourage Controller
to capture environment rewards (as opposed to merely pursuing novelty). In these tasks, we com-
pared the performance of our method to random network distillation (RND) [6], intrinsic curiosity
module (ICM) [30], learning intrinsic reward policy gradient (LIRPG) [43], bi-level optimization
of parameterized reward shaping (BiPaRS-IMGL) [16]4 and vanilla PPO [32]. We then compared
our method against these baselines on performance benchmarks including Sparse Cartpole, Gravitar,
Solaris, and Super Mario. Lastly, we ran a detailed suite of ablation studies (supplementary material)."
EXPERIMENTS,0.11016949152542373,"0
1
2
3
4
5
0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.11122881355932203,"0.25
0.5
0.75
1."
EXPERIMENTS,0.11228813559322035,"Suboptimal
Optimal"
EXPERIMENTS,0.11334745762711865,"RND
ICM
Count Based"
EXPERIMENTS,0.11440677966101695,% Arrivals at Suboptimal / Optimal Goal
EXPERIMENTS,0.11546610169491525,"Steps (1e3)
0           25            50           75           100           125 ROSA"
EXPERIMENTS,0.11652542372881355,"Figure 1: Left. Proportion of optimal and suboptimal goal
arrivals. Our method has a marked inﬂection (arrow) where
arrivals at the sub-optimal goal decrease and arrivals at the op-
timal goal increase. Shaper has learned to guide Controller
to forgo the suboptimal goal in favour of the optimal one.
Right. Heatmap showing where our method adds rewards."
EXPERIMENTS,0.11758474576271187,"1. Beneﬁcial shaping reward. Our
method is able to learn a shaping-
reward function that leads to improved
Controller performance. In particu-
lar, it is able to learn to shape rewards
that encourage the RL agent to avoid
suboptimal – but easy to learn – poli-
cies in favour of policies that attain
the maximal return. To demonstrate
this, we designed a Maze environment
with two terminal states: a suboptimal
goal state that yields a reward of 0.5
and an optimal goal state which yields
a reward of 1. In this maze design,
the sub-optimal goal is more easily
reached. A good shaping-reward function discourages the agent from visiting the sub-optimal goal.
As shown in Fig. 15 our method achieves this by learning to place high shaping rewards (dark green)
on the path that leads to the optimal goal. ROSA RND"
EXPERIMENTS,0.11864406779661017,"Episode Return
  .25          .5           .75          1. ICM"
EXPERIMENTS,0.11970338983050847,"ROSA
Count Based RND"
EXPERIMENTS,0.12076271186440678,"Steps (1e3)
0
50
100
150 Start Goal"
EXPERIMENTS,0.12182203389830508,"Figure 2: Discovering subgoals on Subgoal
Maze. Left. Learning curves. Right. Heatmap
of shaping rewards guiding Controller to gate."
EXPERIMENTS,0.1228813559322034,"2. Subgoal discovery. We used the Subgoal Maze
introduced in [21] to test if ROSA can discover sub-
goals. The environment has two rooms separated by
a gateway. To solve this, the agent has to discover the
subgoal of reaching the gateway before it can reach
the goal. Rewards are −0.01 everywhere except at
the goal state where the reward is 1. As shown in Fig.
2, our method successfully solves this environment
whereas other methods fail. Our method assigns im-
portance to reaching the gateway, depicted by the
heatmap of added shaped rewards."
EXPERIMENTS,0.1239406779661017,"3. Ignoring non-beneﬁcial shaping reward. Switching control gives our method the power to
learn when to attend to shaping rewards and when to ignore them. This allows us to learn to ignore
“red-herrings”, i.e., unexplored parts of the state space where there is no real environment reward, but
where surprise or novelty metrics would place high shaping reward. To verify this claim, we use a
modiﬁed Maze environment called Red-Herring Maze which features a large part of the state space
that has no environment reward, but with the goal (and accompanying real reward) in a different part"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.125,"4BiPaRS-IMGL requires a manually crafted shaping-
reward (only available in Cartpole).
5The sum of curves for each method may be less that 1 if the agent fails to arrive at either goal."
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1260593220338983,Under review as a conference paper at ICLR 2022
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1271186440677966,"of the state space. Ideally, we expect that the reward shaping method can learn to quickly ignore the
large part of the state space. Fig. 3 shows that our method outperforms all other baselines. Moreover,
the heatmap shows that while RND is easily dragged to reward exploring novel but non rewarding
states our method learns to ignore them. ROSA RND"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1281779661016949,".25       .5        .75       1.
Return"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1292372881355932,Count Based
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13029661016949154,"0                      50                      100                       150     
Steps (1e3) ROSA Start Goal"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13135593220338984,"Figure 3: Red-Herring Maze. Ignoring non-beneﬁcial shap-
ing reward. Left. Learning curves. Right. Heatmap of added
shaping rewards. ROSA ignores the RHS of the maze, while
RND incorrectly adds unuseful shaping rewards there."
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13241525423728814,Episode Length
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13347457627118645,50     100    150   200
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13453389830508475,"250
500
Episode Return"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13559322033898305,"500
1000 1500 2000"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13665254237288135,"Steps
0
200
400
600
(1e3) 750"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13771186440677965,Gravitar
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13877118644067796,"0
5
10
15
20"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.13983050847457626,Cartpole
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1408898305084746,Steps(1e6)
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1419491525423729,1000          2000
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1430084745762712,"0
5
10
15
20
Steps(1e6)"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1440677966101695,Solaris
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1451271186440678,Episode Return
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1461864406779661,"0
5
10
15
20
Steps(1e6)"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1472457627118644,Episode Return
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1483050847457627,Super Mario
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.149364406779661,"BiPars-IMGL
(Harmful shaping reward)
BiPaRS-IMGL
(Good shaping reward) RND PPO ICM LIRPG ROSA"
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.1504237288135593,Figure 4: Benchmark performance.
BIPARS-IMGL REQUIRES A MANUALLY CRAFTED SHAPING-,0.15148305084745764,"Learning Performance.
We com-
pared our method with the baselines
in four challenging sparse rewards en-
vironments: Cartpole, Gravitar, So-
laris, and Super Mario. These envi-
ronments vary in state representation,
transition dynamics and reward spar-
sity. In Cartpole, a penalty of −1 is
received only when the pole collapses;
in Super Mario Brothers the agent can
go for 100s of steps without encoun-
tering a reward. Fig. 4 shows learn-
ing curves. In terms of performance,
ROSA either markedly outperforms
the best competing baseline (Cartpole,
Gravitar) or is on par with them (So-
laris, Super Mario) showing that it is
robust to the nature of the environ-
ment and underlying sparse reward.
Moreover, ROSA does not exhibit the
failure modes where after good initial
performance it deteriorates. E.g., in Solaris both ICM and RND have good initial performance but
deteriorate sharply while ROSA’s performance remains satisfactory."
CONCLUSION,0.15254237288135594,"7
CONCLUSION"
CONCLUSION,0.15360169491525424,"In this paper, we presented a novel solution method to solve the problem of reward shaping. Our
Markov game framework of a primary Controller and a secondary reward shaping agent is guar-
anteed to preserve the underlying learning task for Controller whilst guiding Controller to higher
performance policies. Moreover, our method is able to decompose complex learning tasks into
subgoals and to adaptively guide Controller by selectively choosing the states to add shaping rewards.
By presenting a theoretically sound and empirically robust approach to solving the reward shaping
problem, our method opens up the applicability of RL to a range of real-world control problems.
The most signiﬁcant contribution of this paper, however, is the novel construction that marries RL,
multi-agent RL and game theory which leads to new solution method in RL. We believe this powerful
approach can be adopted to solve other open challenges in RL."
CONCLUSION,0.15466101694915255,Under review as a conference paper at ICLR 2022
REFERENCES,0.15572033898305085,REFERENCES
REFERENCES,0.15677966101694915,"[1]
Gianluca Baldassarre. “What are intrinsic motivations? A biological perspective”. In: 2011
IEEE international conference on development and learning (ICDL). Vol. 2. IEEE. 2011,
pp. 1–8.
[2]
Erhan Bayraktar and Masahiko Egami. “On the one-dimensional optimal switching problem”.
In: Mathematics of Operations Research 35.1 (2010), pp. 140–159.
[3]
Albert Benveniste, Michel Métivier, and Pierre Priouret. Adaptive algorithms and stochastic
approximations. Vol. 22. Springer Science & Business Media, 2012.
[4]
Dimitri P Bertsekas. Approximate dynamic programming. Athena scientiﬁc Belmont, 2012.
[5]
David Blackwell and Tom S Ferguson. “The big match”. In: The Annals of Mathematical
Statistics 39.1 (1968), pp. 159–163.
[6]
Yuri Burda et al. “Exploration by random network distillation”. In: arXiv preprint
arXiv:1810.12894 (2018).
[7]
Henry Charlesworth and Giovanni Montana. “PlanGAN: Model-based Planning With Sparse
Rewards and Multiple Goals”. In: arXiv preprint arXiv:2006.00900 (2020).
[8]
Marc Peter Deisenroth, Carl Edward Rasmussen, and Dieter Fox. “Learning to control a
low-cost manipulator using data-efﬁcient reinforcement learning”. In: Robotics: Science and
Systems VII (2011), pp. 57–64.
[9]
Sam Devlin and Daniel Kudenko. “Theoretical considerations of potential-based reward
shaping for multi-agent systems”. In: The 10th International Conference on Autonomous
Agents and Multiagent Systems-Volume 1. International Foundation for Autonomous Agents
and Multiagent Systems. 2011, pp. 225–232.
[10]
Sam Devlin, Daniel Kudenko, and Marek Grze´s. “An empirical study of potential-based reward
shaping and advice in complex, multi-agent systems”. In: Advances in Complex Systems 14.02
(2011), pp. 251–278.
[11]
Sam Michael Devlin and Daniel Kudenko. “Dynamic potential-based reward shaping”. In:
Proceedings of the 11th International Conference on Autonomous Agents and Multiagent
Systems. IFAAMAS. 2012, pp. 433–440.
[12]
Drew Fudenberg and Jean Tirole. “Tirole: Game Theory”. In: MIT Press 726 (1991), p. 764.
[13]
Anna Harutyunyan et al. “Expressing arbitrary reward functions as potential-based advice”.
In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Vol. 29. 2015.
[14]
Ionel-Alexandru Hosu and Traian Rebedea. “Playing atari games with deep reinforcement
learning and human checkpoint replay”. In: arXiv preprint arXiv:1607.05077 (2016).
[15]
Rein Houthooft et al. “Vime: Variational information maximizing exploration”. In: arXiv
preprint arXiv:1605.09674 (2016).
[16]
Yujing Hu et al. “Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping”.
In: Advances in Neural Information Processing Systems 33 (2020).
[17]
Maximilian Igl et al. “The Impact of Non-stationarity on Generalisation in Deep Reinforcement
Learning”. In: arXiv preprint arXiv:2006.05826 (2020).
[18]
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. “Convergence of stochastic iterative
dynamic programming algorithms”. In: Advances in neural information processing systems.
1994, pp. 703–710.
[19]
Sergio Valcarcel Macua, Javier Zazo, and Santiago Zazo. “Learning Parametric Closed-Loop
Policies for Markov Potential Games”. In: arXiv preprint arXiv:1802.00899 (2018).
[20]
Patrick Mannion et al. “Policy invariance under reward transformations for multi-objective
reinforcement learning”. In: Neurocomputing 263 (2017), pp. 60–73.
[21]
Amy McGovern and Andrew G Barto. “Automatic discovery of subgoals in reinforcement
learning using diverse density”. In: (2001).
[22]
David Mguni. “A Viscosity Approach to Stochastic Differential Games of Control and Stop-
ping Involving Impulsive Control”. In: arXiv preprint arXiv:1803.11432 (2018).
[23]
David Mguni. “Cutting Your Losses: Learning Fault-Tolerant Control and Optimal Stopping
under Adverse Risk”. In: arXiv preprint arXiv:1902.05045 (2019)."
REFERENCES,0.15783898305084745,Under review as a conference paper at ICLR 2022
REFERENCES,0.15889830508474576,"[24]
David Mguni, Joel Jennings, and Enrique Munoz de Cote. “Decentralised learning in sys-
tems with many, many strategic agents”. In: Thirty-Second AAAI Conference on Artiﬁcial
Intelligence. 2018.
[25]
David Mguni et al. “Coordinating the crowd: Inducing desirable equilibria in non-cooperative
systems”. In: arXiv preprint arXiv:1901.10923 (2019).
[26]
David Mguni et al. “Learning in Nonzero-Sum Stochastic Games with Potentials”. In: arXiv
preprint arXiv:2103.09284 (2021).
[27]
Andrew Y Ng, Daishi Harada, and Stuart Russell. “Policy invariance under reward trans-
formations: Theory and application to reward shaping”. In: ICML. Vol. 99. 1999, pp. 278–
287.
[28]
David C Noelle. “Unsupervised methods for subgoal discovery during intrinsic motivation in
model-free hierarchical reinforcement learning”. In: KEG@ AAAI. 2019.
[29]
Georg Ostrovski et al. “Count-based exploration with neural density models”. In: arXiv
preprint arXiv:1703.01310 (2017).
[30]
Deepak Pathak et al. “Curiosity-driven Exploration by Self-supervised Prediction”. In: Inter-
national Conference on Machine Learning (ICML). 2017, pp. 2778–2787.
[31]
Peng Peng et al. “Multiagent bidirectionally-coordinated nets: Emergence of human-level
coordination in learning to play starcraft combat games”. In: arXiv preprint arXiv:1703.10069
(2017).
[32]
John Schulman et al. “Proximal Policy Optimization Algorithms”. In: CoRR abs/1707.06347
(2017).
[33]
Kun Shao et al. “A survey of deep reinforcement learning in video games”. In: arXiv preprint
arXiv:1912.10944 (2019).
[34]
Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic,
and logical foundations. Cambridge University Press, 2008.
[35]
David Silver et al. “A general reinforcement learning algorithm that masters chess, shogi, and
Go through self-play”. In: Science 362.6419 (2018), pp. 1140–1144.
[36]
Bradly Stadie, Lunjun Zhang, and Jimmy Ba. “Learning Intrinsic Rewards as a Bi-Level
Optimization Problem”. In: Conference on Uncertainty in Artiﬁcial Intelligence. PMLR. 2020,
pp. 111–120.
[37]
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. “Incentivizing exploration in reinforcement
learning with deep predictive models”. In: arXiv preprint arXiv:1507.00814 (2015).
[38]
Alexander L Strehl and Michael L Littman. “An analysis of model-based interval estimation
for Markov decision processes”. In: Journal of Computer and System Sciences 74.8 (2008),
pp. 1309–1331.
[39]
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018.
[40]
John N Tsitsiklis and Benjamin Van Roy. “Optimal stopping of Markov processes: Hilbert
space theory, approximation algorithms, and an application to pricing high-dimensional
ﬁnancial derivatives”. In: IEEE Transactions on Automatic Control 44.10 (1999), pp. 1840–
1851.
[41]
ICML Workshop. Reinforcement Learning for Real Life, ICML 2021 Workshop. https:
//sites.google.com/view/RL4RealLife.
[42]
Kaiqing Zhang, Zhuoran Yang, and Tamer Ba¸sar. “Multi-agent reinforcement learning: A
selective overview of theories and algorithms”. In: arXiv preprint arXiv:1911.10635 (2019).
[43]
Zeyu Zheng, Junhyuk Oh, and Satinder Singh. “On Learning Intrinsic Rewards for Policy
Gradient Methods”. In: Advances in Neural Information Processing Systems (NeurIPS). 2018.
[44]
Martin Zinkevich, Amy Greenwald, and Michael Littman. “Cyclic equilibria in Markov
games”. In: Advances in Neural Information Processing Systems 18 (2006), p. 1641.
[45]
Haosheng Zou et al. “Reward shaping via meta-learning”. In: arXiv preprint arXiv:1901.09330
(2019)."
REFERENCES,0.15995762711864406,Under review as a conference paper at ICLR 2022
REFERENCES,0.16101694915254236,Appendix
REFERENCES,0.1620762711864407,Table of Contents
ALGORITHM,0.163135593220339,"8
Algorithm
2"
FURTHER IMPLEMENTATION DETAILS,0.1641949152542373,"9
Further Implementation Details
3"
SHAPER TERMINATION TIMES,0.1652542372881356,"10 Shaper Termination Times
3"
EXPERIMENTAL DETAILS,0.1663135593220339,"11 Experimental Details
4
11.1 Environments & Preprocessing Details . . . . . . . . . . . . . . . . . . . . . .
4
11.2 Hyperparameter Settings
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4"
ABLATION STUDIES,0.1673728813559322,"12 Ablation Studies
5"
FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION BONUS TERMS L,0.1684322033898305,"13 Flexibility of ROSA to Accommodate different Exploration Bonus Terms L
7"
FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION BONUS TERMS L,0.1694915254237288,"14 Robustness to choices of φ Parameters
8"
NOTATION & ASSUMPTIONS,0.1705508474576271,"15 Notation & Assumptions
9"
PROOF OF TECHNICAL RESULTS,0.1716101694915254,"16 Proof of Technical Results
9"
PROOF OF TECHNICAL RESULTS,0.17266949152542374,"17 Additional Experiment 1 - Replacing RND for Bonus Reward
28"
PROOF OF TECHNICAL RESULTS,0.17372881355932204,"18 Additional Experiment 2 - Robustness to initialisation of φ
28"
PROOF OF TECHNICAL RESULTS,0.17478813559322035,Under review as a conference paper at ICLR 2022
ALGORITHM,0.17584745762711865,"8
ALGORITHM"
ALGORITHM,0.17690677966101695,"Algorithm 2: Reinforcement Learning Optimising Shaping Algorithm ROSA
Input: Environment E
Initial Controller policy π0 with parameters θπ0
Initial Shaper switch policy g20 with parameters θg20
Initial Shaper action policy π2
0 with parameters θπ2
0
Randomly initialised ﬁxed neural network φ(·, ·)
Neural networks h (ﬁxed) and ˆh for RND with parameter θˆh
Buffer B
Number of rollouts Nr, rollout length T
Number of mini-batch updates Nu
Switch cost c(·), Discount factor γ, learning rate α
Output: Optimised Controller policy π∗"
ALGORITHM,0.17796610169491525,"1 π, π2, g2 ←π0, π2
0, g20
2 for n = 1, Nr do"
ALGORITHM,0.17902542372881355,"3
// Collect rollouts"
ALGORITHM,0.18008474576271186,"4
for t = 1, T do"
GET ENVIRONMENT STATES ST FROM E,0.18114406779661016,"5
Get environment states st from E"
GET ENVIRONMENT STATES ST FROM E,0.18220338983050846,"6
Sample at from π(st)"
GET ENVIRONMENT STATES ST FROM E,0.1832627118644068,"7
Apply action at to environment E, and get reward rt and next state st+1"
GET ENVIRONMENT STATES ST FROM E,0.1843220338983051,"8
Sample gt from g2(st) // Switching control"
GET ENVIRONMENT STATES ST FROM E,0.1853813559322034,"9
if gt = 1 then"
GET ENVIRONMENT STATES ST FROM E,0.1864406779661017,"10
Sample a2
t from π2(st)"
GET ENVIRONMENT STATES ST FROM E,0.1875,"11
Sample a2
t+1 from π2(st+1)"
RI,0.1885593220338983,"12
ri
t = γφ(st+1, a2
t+1) −φ(st, a2
t) // Calculate F(st, at, st+1, at+1)"
ELSE,0.1896186440677966,"13
else"
ELSE,0.1906779661016949,"14
a2
t, ri
t = 0, 0 // Dummy values"
ELSE,0.1917372881355932,"15
Append (st, at, gt, a2
t, rt, ri
t, st+1) to B"
ELSE,0.19279661016949154,"16
for u = 1, Nu do"
ELSE,0.19385593220338984,"17
Sample data (st, at, gt, a2
t, rt, ri
t, st+1) from B"
ELSE,0.19491525423728814,"18
if gt = 1 then"
SET SHAPED REWARD TO RS,0.19597457627118645,"19
Set shaped reward to rs
t = rt + ri
t
20
else"
SET SHAPED REWARD TO RS,0.19703389830508475,"21
Set shaped reward to rs
t = rt"
SET SHAPED REWARD TO RS,0.19809322033898305,"22
// Update RND"
SET SHAPED REWARD TO RS,0.19915254237288135,"23
LossRND = ||h(st) −ˆh(st)||2"
SET SHAPED REWARD TO RS,0.20021186440677965,"24
θˆh ←θˆh −α∇LossRND"
SET SHAPED REWARD TO RS,0.20127118644067796,"25
// Update Shaper"
SET SHAPED REWARD TO RS,0.20233050847457626,"26
lt = ||h(st) −ˆh(st)||2 // Compute L(st)"
SET SHAPED REWARD TO RS,0.2033898305084746,"27
ct = c(·)gt"
SET SHAPED REWARD TO RS,0.2044491525423729,"28
Compute Lossπ2 using (st, at, gt, ct, rt, ri
t, lt, st+1) using PPO loss // Section 4.2"
SET SHAPED REWARD TO RS,0.2055084745762712,"29
Compute Lossg2 using (st, at, gt, ct, rt, ri
t, lt, st+1) using PPO loss // Section 4.2"
SET SHAPED REWARD TO RS,0.2065677966101695,"30
θπ2 ←θπ2 −α∇Lossπ2"
SET SHAPED REWARD TO RS,0.2076271186440678,"31
θg2 ←θg2 −α∇Lossg2
32
// Update Controller"
SET SHAPED REWARD TO RS,0.2086864406779661,"33
Compute Lossπ using (st, at, rs
t , st+1) using PPO loss // Section 4"
SET SHAPED REWARD TO RS,0.2097457627118644,"34
θπ ←θπ −α∇Lossπ"
SET SHAPED REWARD TO RS,0.2108050847457627,Under review as a conference paper at ICLR 2022
FURTHER IMPLEMENTATION DETAILS,0.211864406779661,"9
FURTHER IMPLEMENTATION DETAILS"
FURTHER IMPLEMENTATION DETAILS,0.2129237288135593,"Details of Shaper and F (shaping reward)
Object
Description
f
Fixed feed forward NN that maps Rd 7→Rm
[512, ReLU, 512, ReLU, 512, m]
A2
Discrete integer action set which is size of output of f,
i.e.,A2 is set of integers {1, ..., m}
π2
Fixed feed forward NN that maps Rd 7→Rm
[512, ReLU, 512, ReLU, 512, m]
Potential function φ
φ(s, a2) = f(s) · a2"
FURTHER IMPLEMENTATION DETAILS,0.21398305084745764,"F
γφ(st+1, a2
t+1) - φ(st, a2
t),
γ = 0.95"
FURTHER IMPLEMENTATION DETAILS,0.21504237288135594,d=Dimensionality of states; m ∈N - tunable free parameter.
FURTHER IMPLEMENTATION DETAILS,0.21610169491525424,"In all experiments we used the above form of F as follows: a state st is input to the π2 network and
the network outputs logits pt. We softmax and sample from pt to obtain the action a2
t. This action
is one-hot encoded. Then, the action a2
t is multiplied with f(st) to compute the second term of F.
A similar process is used to compute the ﬁrst term. In this way the policy of Shaper chooses the
shaping reward."
SHAPER TERMINATION TIMES,0.21716101694915255,"10
SHAPER TERMINATION TIMES"
SHAPER TERMINATION TIMES,0.21822033898305085,"There are various possibilities for the termination times {τ2k} (recall that {τ2k+1} are the times
which the shaping reward F is switched on using g2). One is for Shaper to determine the sequence.
Another is to build a construction of {τ2k} that directly incorporates the information gain that a state
visit provides: let w : Ω→{0, 1} be a random variable with Pr(w = 1) = p and Pr(w = 0) = 1−p
where p ∈]0, 1]. Then for any k = 1, 2, . . . , and denote by ∆L(sτk) := L(sτk) −L(sτk−1), then we
can set:"
SHAPER TERMINATION TIMES,0.21927966101694915,"I(sτ2k+1+j) =
I(sτ2k+1),
if w∆L(sτk+j) > 0,
I(sτ2k+2),
w∆L(sτk+j) ≤0.
(2)"
SHAPER TERMINATION TIMES,0.22033898305084745,"To explain, since {τ2k}k≥0 are the times at which F is switched off then if F is deactivated at exactly
after j time steps then I(sτ2k+1+l) = I(sτ2k+1) for any 0 ≤l < j and I(sτ2k+1+j) = I(sτ2k+2) . We
now see that (2) terminates F when either the random variable w attains a 0 or when ∆L(sτk+j) ≤0
which occurs when the exploration bonus in the current state is lower than that of the previous state."
SHAPER TERMINATION TIMES,0.22139830508474576,Under review as a conference paper at ICLR 2022
EXPERIMENTAL DETAILS,0.22245762711864406,"11
EXPERIMENTAL DETAILS"
ENVIRONMENTS & PREPROCESSING DETAILS,0.22351694915254236,"11.1
ENVIRONMENTS & PREPROCESSING DETAILS"
ENVIRONMENTS & PREPROCESSING DETAILS,0.2245762711864407,The table below shows the provenance of environments used in our experiments.
ENVIRONMENTS & PREPROCESSING DETAILS,0.225635593220339,"Atari & Cartpole
https://github.com/openai/gym
Maze
https://github.com/MattChanTK/gym-maze
Super Mario Brothers
https://github.com/Kautenja/gym-super-mario-bros"
ENVIRONMENTS & PREPROCESSING DETAILS,0.2266949152542373,"Furthermore, we used preprocessing settings as indicated in the following table."
ENVIRONMENTS & PREPROCESSING DETAILS,0.2277542372881356,"Setting
Value"
ENVIRONMENTS & PREPROCESSING DETAILS,0.2288135593220339,"Max frames per episode
Atari & Mario →18000 / Maze & Cartpole →200
Observation concatenation
Preceding 4 observations
Observation preprocessing
Standardization followed by clipping to [-5, 5]
Observation scaling
Atari & Mario →(84, 84, 1) / Maze & Cartpole →None
Reward (extrinsic and intrinsic) preprocessing
Standardization followed by clipping to [-1, 1]"
HYPERPARAMETER SETTINGS,0.2298728813559322,"11.2
HYPERPARAMETER SETTINGS"
HYPERPARAMETER SETTINGS,0.2309322033898305,"In the table below we report all hyperparameters used in our experiments. Hyperparameter values in
square brackets indicate ranges of values that were used for performance tuning."
HYPERPARAMETER SETTINGS,0.2319915254237288,"Clip Gradient Norm
1
γE
0.99
λ
0.95
Learning rate
1x10−4
Number of minibatches
4
Number of optimization epochs
4
Policy architecture
CNN (Mario/Atari) or MLP (Cartpole/Maze)
Number of parallel actors
2 (Cartpole/Maze) or 20 (Mario/Atari)
Optimization algorithm
Adam
Rollout length
128
Sticky action probability
0.25
Use Generalized Advantage Estimation
True"
HYPERPARAMETER SETTINGS,0.2330508474576271,"Coefﬁcient of extrinsic reward
[1, 5]
Coefﬁcient of intrinsic reward
[1, 2, 5, 10, 20, 50]
γI
0.99
Probability of terminating option
[0.5, 0.75, 0.8, 0.9, 0.95]
RND output size
[2, 4, 8, 16, 32, 64, 128, 256]"
HYPERPARAMETER SETTINGS,0.2341101694915254,Under review as a conference paper at ICLR 2022
HYPERPARAMETER SETTINGS,0.23516949152542374,"0
50
100
150
200
250
Steps (x 1000)"
HYPERPARAMETER SETTINGS,0.23622881355932204,"0.5
1.0
Episode Return"
HYPERPARAMETER SETTINGS,0.23728813559322035,"Ours
Ours sans Switching Control"
HYPERPARAMETER SETTINGS,0.23834745762711865,(a) Ablating Switching Controls
HYPERPARAMETER SETTINGS,0.23940677966101695,"0
2
4
6 0 1 2 3 4 5 6"
HYPERPARAMETER SETTINGS,0.24046610169491525,"0
2
4
6 0 1 2 3 4 5"
HYPERPARAMETER SETTINGS,0.24152542372881355,"6
Standard Policy P1
High Entropy
Policy P1"
HYPERPARAMETER SETTINGS,0.24258474576271186,Suboptimal
HYPERPARAMETER SETTINGS,0.24364406779661016,"Optimal
Origin"
HYPERPARAMETER SETTINGS,0.24470338983050846,(b) Responsiveness to Controller policies
HYPERPARAMETER SETTINGS,0.2457627118644068,Figure 5: Ablation Experiments
ABLATION STUDIES,0.2468220338983051,"12
ABLATION STUDIES"
ABLATION STUDIES,0.2478813559322034,"Our reward-shaping method features a mechanism to selectively pick states to which intrinsic rewards
are added. It also adapts its shaping rewards according to Controller’s learning process. In this
section, we present the results of experiments in which we ablated each of these components. In
particular, we test the performance of our method in comparison to a version of our method with the
switching mechanism removed. We then present the result of an experiment in which we investigated
the ability of our method to adapt to different behaviour of Controller."
ABLATION STUDIES,0.2489406779661017,ABLATION STUDY 1: SWITCHING CONTROLS
ABLATION STUDIES,0.25,"Switching controls enable our method to be selective of states to which intrinsic rewards are added.
This improves learnability (speciﬁcally, by reducing the computational complexity) of the learning
task for Shaper as there are fewer states where it must learn the optimal intrinsic reward to add to
Controller objective."
ABLATION STUDIES,0.2510593220338983,"To test the effect of this feature on the performance of our method, we compared our method to a
modiﬁed version in which Shaper must add intrinsic rewards to all states. That is, for this version
of our method we remove the presence of the switching control mechanism for Shaper. Figure 5
(a) shows learning curves on the Maze environment used in the ""Optimality of shaping reward""
experiments in Section 6. As expected, the agent with the version of our method with switching
controls learns signiﬁcantly faster than the agent that uses the version of our method sans the switching
control mechanism. For example, it takes the agent that has no switching control mechanism almost
50,000 more steps to attain an average episode return of 0.5 as compared against the agent that uses
the version of our algorithm with switching controls."
ABLATION STUDIES,0.2521186440677966,"This illustrates a key beneﬁt of switching controls which is to reduce the computational burden on
Shaper (as it does not need to model the effects of adding intrinsic rewards in all states) which in turn
leads to both faster computation of solutions and improved performance by Controller. Moreover,
Maze is a relatively simple environment, expectedly the importance of the switching control is
ampliﬁed in more complex environments."
ABLATION STUDIES,0.2531779661016949,ABLATION STUDY 2: ADAPTION OF OUR METHOD TO DIFFERENT CONTROLLER POLICIES
ABLATION STUDIES,0.2542372881355932,"We claimed Shaper can design a reward-shaping scheme that can adapt its shaping reward guidance
of Controller (to achieve the optimal policy) according to Controller’s (RL) policy."
ABLATION STUDIES,0.2552966101694915,"To test this claim, we tested two versions of our agent in a corridor Maze. The maze features two
goal states that are equidistant from the origin, one is a suboptimal goal with a reward of 0.5 and
the other is an optimal goal which has a reward 1. There is also a ﬁxed cost for each non-terminal
transition. We tested this scenario with two versions of our controller: one with a standard RL
Controller policy and another version in which the actions of Controller are determined by a high"
ABLATION STUDIES,0.2563559322033898,Under review as a conference paper at ICLR 2022
ABLATION STUDIES,0.2574152542372881,"entropy policy, we call this version of Controller the high entropy controller.6 The high entropy
policy induces actions that may randomly push Controller towards the suboptimal goal. Therefore, in
order to guide Controller to the optimal goal state, we expect Shaper to strongly shape the rewards of
Controller to guide Controller away from the suboptimal goal (and towards the optimal goal)."
ABLATION STUDIES,0.2584745762711864,"Figure 5 (b) shows heatmaps of the added intrinsic reward (darker colours indicate higher intrinsic
rewards) for the two versions of Controller. With the standard policy controller, the intrinsic reward
is maximal in the state to the right of the origin indicating that Shaper determines that these shaping
rewards are sufﬁcient to guide Controller towards the optimal goal state. For the high entropy
controller, Shaper introduces high intrinsic rewards to the origin state as well as states beneath the
origin. These rewards serve to counteract the random actions taken by the high-entropy policy that
lead Controller towards the suboptimal goal state. It can therefore be seen that Shaper adapts the
shaping rewards according to the type of Controller it seeks to guide."
ABLATION STUDIES,0.2595338983050847,"6To generate this policy, we artiﬁcially increased the entropy by adjusting the temperature of a softmax
function on the policy logits."
ABLATION STUDIES,0.2605932203389831,Under review as a conference paper at ICLR 2022
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2616525423728814,"13
FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L"
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2627118644067797,"To demonstrate the robustness of our method to different choices of exploration bonus terms in
Shaper’s objective, we conducted an Ablation study on the L-term (c.f. Equation 1) where we
replaced the RND L term with a basic count-based exploration bonus. To exemplify the high degree
of ﬂexibility, we replaced the RND with a simple exploration bonus term L(s) =
1
Count(s)+1 for any
given state s ∈S where Count(s) refers to a simple count of the number of times the state s has been
visited. We conducted the Ablation study in the environment in Experiment 1 presented in Sec. 6.
We note that despite the simplicity of the count-based measure, generally the performance of both
versions of ROSA is comparable and in fact the count-based variant is slightly superior to the RND
version."
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.263771186440678,"2
4
6
8
10
Steps (x 10000) 0.0 0.2 0.4 0.6 0.8 1.0"
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2648305084745763,Return
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2658898305084746,"ROSA (L = RND)
ROSA (L = Count-based)"
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2669491525423729,"Figure 6: Performance of ROSA compared with the exploration bonus replaced by count-based
method."
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2680084745762712,Under review as a conference paper at ICLR 2022
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2690677966101695,"14
ROBUSTNESS TO CHOICES OF φ PARAMETERS"
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2701271186440678,"To demonstrate the robustness of φ to different choices of weight parameters, we conducted a study
with 3 sets of randomly sampled values of weight parameters for the feed forward NN that constructs
the φ function (c.f. Sec. 9)."
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2711864406779661,"As is shown in Fig. 7, the performance across all 3 values is very comparable demonstrating the
robustness of ROSA to different values of weight parameters for the φ function."
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2722457627118644,"2
4
6
8
10
Steps (x 10000) 0.0 0.2 0.4 0.6 0.8 1.0"
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2733050847457627,Return
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.274364406779661,"Run 0
Run 1
Run 2"
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2754237288135593,Figure 7: Performance of ROSA compared with different values of φ parameters.
"FLEXIBILITY OF ROSA TO ACCOMMODATE DIFFERENT EXPLORATION
BONUS TERMS L",0.2764830508474576,Under review as a conference paper at ICLR 2022
NOTATION & ASSUMPTIONS,0.2775423728813559,"15
NOTATION & ASSUMPTIONS"
NOTATION & ASSUMPTIONS,0.2786016949152542,"We assume that S is deﬁned on a probability space (Ω, F, P) and any s ∈S is measurable with
respect to the Borel σ-algebra associated with Rp. We denote the σ-algebra of events generated by
{st}t≥0 by Ft ⊂F. In what follows, we denote by (V, ∥∥) any ﬁnite normed vector space and by H
the set of all measurable functions."
NOTATION & ASSUMPTIONS,0.2796610169491525,"The results of the paper are built under the following assumptions which are standard within RL and
stochastic approximation methods:"
NOTATION & ASSUMPTIONS,0.2807203389830508,"Assumption 1 The stochastic process governing the system dynamics is ergodic, that is the process
is stationary and every invariant random variable of {st}t≥0 is equal to a constant with probability 1."
NOTATION & ASSUMPTIONS,0.2817796610169492,"Assumption 2 The constituent functions of the players’ objectives R, F and L are in L2."
NOTATION & ASSUMPTIONS,0.2828389830508475,"Assumption 3 For any positive scalar c, there exists a scalar µc such that for all s ∈S and for any
t ∈N we have: E [1 + ∥st∥c|s0 = s] ≤µc(1 + ∥s∥c)."
NOTATION & ASSUMPTIONS,0.2838983050847458,"Assumption 4 There exists scalars C1 and c1 such that for any function J satisfying |J(s)| ≤
C2(1 + ∥s∥c2) for some scalars c2 and C2 we have that: P∞
t=0 |E [J(st)|s0 = s] −E[J(s0)]| ≤
C1C2(1 + ∥st∥c1c2)."
NOTATION & ASSUMPTIONS,0.2849576271186441,"Assumption 5 There exists scalars c and C such that for any s ∈S we have that: |J(z, ·)| ≤
C(1 + ∥z∥c) for J ∈{R, F, L}."
NOTATION & ASSUMPTIONS,0.2860169491525424,We also make the following ﬁniteness assumption on set of switching control policies for Shaper:
NOTATION & ASSUMPTIONS,0.2870762711864407,"Assumption 6 For any policy gc, the total number of interventions is given by K < ∞."
NOTATION & ASSUMPTIONS,0.288135593220339,We lastly make the following assumption on L which can be made true by construction:
NOTATION & ASSUMPTIONS,0.2891949152542373,"Assumption 7 Let n(s) be the state visitation count for a given state s ∈S. For any a ∈A, the
function L(s, a) = 0 for any n(s) ≥M where 0 < M ≤∞."
PROOF OF TECHNICAL RESULTS,0.2902542372881356,"16
PROOF OF TECHNICAL RESULTS"
PROOF OF TECHNICAL RESULTS,0.2913135593220339,"We begin the analysis with some preliminary lemmata and deﬁnitions which are useful for proving
the main results."
PROOF OF TECHNICAL RESULTS,0.2923728813559322,"Deﬁnition 1 A.1 An operator T : V →V is said to be a contraction w.r.t a norm ∥· ∥if there exists
a constant c ∈[0, 1[ such that for any V1, V2 ∈V we have that:"
PROOF OF TECHNICAL RESULTS,0.2934322033898305,"∥TV1 −TV2∥≤c∥V1 −V2∥.
(3)"
PROOF OF TECHNICAL RESULTS,0.2944915254237288,"Deﬁnition 2 A.2 An operator T : V →V is non-expansive if ∀V1, V2 ∈V we have:"
PROOF OF TECHNICAL RESULTS,0.2955508474576271,"∥TV1 −TV2∥≤∥V1 −V2∥.
(4)"
PROOF OF TECHNICAL RESULTS,0.2966101694915254,"Lemma 1 For any f : V →R, g : V →R, we have that:
max
a∈V f(a) −max
a∈V g(a)
 ≤max
a∈V ∥f(a) −g(a)∥.
(5)"
PROOF OF TECHNICAL RESULTS,0.2976694915254237,Proof 1 We restate the proof given in [23]:
PROOF OF TECHNICAL RESULTS,0.298728813559322,"f(a) ≤∥f(a) −g(a)∥+ g(a)
(6)
=⇒max
a∈V f(a) ≤max
a∈V {∥f(a) −g(a)∥+ g(a)} ≤max
a∈V ∥f(a) −g(a)∥+ max
a∈V g(a).
(7)"
PROOF OF TECHNICAL RESULTS,0.2997881355932203,"Deducting max
a∈V g(a) from both sides of (7) yields:"
PROOF OF TECHNICAL RESULTS,0.3008474576271186,"max
a∈V f(a) −max
a∈V g(a) ≤max
a∈V ∥f(a) −g(a)∥.
(8)"
PROOF OF TECHNICAL RESULTS,0.3019067796610169,"After reversing the roles of f and g and redoing steps (6) - (7), we deduce the desired result since the
RHS of (8) is unchanged."
PROOF OF TECHNICAL RESULTS,0.3029661016949153,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.3040254237288136,"Lemma 2 A.4 The probability transition kernel P is non-expansive, that is:"
PROOF OF TECHNICAL RESULTS,0.3050847457627119,"∥PV1 −PV2∥≤∥V1 −V2∥.
(9)"
PROOF OF TECHNICAL RESULTS,0.3061440677966102,"Proof 2 The result is well-known e.g. [40]. We give a proof using the Tonelli-Fubini theorem and
the iterated law of expectations, we have that:"
PROOF OF TECHNICAL RESULTS,0.3072033898305085,"∥PJ∥2 = E

(PJ)2[s0]

= E

[E [J[s1]|s0])2i
≤E

E

J2[s1]|s0

= E

J2[s1]

= ∥J∥2,"
PROOF OF TECHNICAL RESULTS,0.3082627118644068,where we have used Jensen’s inequality to generate the inequality. This completes the proof.
PROOF OF TECHNICAL RESULTS,0.3093220338983051,PROOF OF PROPOSITION 1
PROOF OF TECHNICAL RESULTS,0.3103813559322034,"Proof 3 (Proof of Prop 1) To prove (i) of the proposition it sufﬁces to prove that the term
PT
t=0 γtF(θt, θt−1)I(t) converges to 0 in the limit as T →∞. As in classic potential-based
reward shaping [27], central to this observation is the telescoping sum that emerges by construction
of F."
PROOF OF TECHNICAL RESULTS,0.3114406779661017,"First recall vπ,π2
1
(s, I0), for any (s, I0) ∈S × {0, 1} is given by:"
PROOF OF TECHNICAL RESULTS,0.3125,"vπ,π2
1
(s, I0) = Eπ,π2 "" ∞
X"
PROOF OF TECHNICAL RESULTS,0.3135593220338983,"t=0
γt n
Ri(st, at) + ˆF(st, a2
t; st−1, a2
t−1)It
o# (10)"
PROOF OF TECHNICAL RESULTS,0.3146186440677966,"= Eπ,π2 "" ∞
X"
PROOF OF TECHNICAL RESULTS,0.3156779661016949,"t=0
γtRi(st, at) + ∞
X"
PROOF OF TECHNICAL RESULTS,0.3167372881355932,"t=0
γt ˆF(st, a2
t; st−1, a2
t−1)It # (11)"
PROOF OF TECHNICAL RESULTS,0.3177966101694915,"= Eπ,π2 "" ∞
X"
PROOF OF TECHNICAL RESULTS,0.3188559322033898,"t=0
γtRi(st, at) #"
PROOF OF TECHNICAL RESULTS,0.3199152542372881,"+ Eπ,π2 "" ∞
X"
PROOF OF TECHNICAL RESULTS,0.3209745762711864,"t=0
γt ˆF(st, a2
t; st−1, a2
t−1))It #"
PROOF OF TECHNICAL RESULTS,0.3220338983050847,".
(12)"
PROOF OF TECHNICAL RESULTS,0.3230932203389831,"Hence it sufﬁces to prove that Eπ,π2
hP∞
t=0 γt ˆF(st, a2
t; st−1, a2
t−1))It
i
= 0."
PROOF OF TECHNICAL RESULTS,0.3241525423728814,"Recall there a number of time steps that elapse between τk and τk+1, now ∞
X"
PROOF OF TECHNICAL RESULTS,0.3252118644067797,"t=0
γt ˆF(st, a2
t; st−1, a2
t−1))I(t) = τ2
X"
PROOF OF TECHNICAL RESULTS,0.326271186440678,"t=τ1+1
γtφ(st, a2
t) −γt−1φ(st−1, a2
t−1) + γτ1φ(sτ1, a2
τ1) + τ4
X"
PROOF OF TECHNICAL RESULTS,0.3273305084745763,"t=τ3+1
γtφ(st, a2
t) −γt−1φ(st−1, a2
t−1) + γτ3φ(sτ3, a2
τ3)"
PROOF OF TECHNICAL RESULTS,0.3283898305084746,"+ . . . + τ2k
X"
PROOF OF TECHNICAL RESULTS,0.3294491525423729,"t=τ(2k−1)+1
γtφ(st, a2
t) −γt−1φ(st−1, a2
t−1) + γτ1φ(sτ2k+1, a2
τ2k+1) + . . . + ="
PROOF OF TECHNICAL RESULTS,0.3305084745762712,"τ2−1
X"
PROOF OF TECHNICAL RESULTS,0.3315677966101695,"t=τ1
γt+1φ(st+1, a2
t+1) −γtφ(st, a2
t) + γτ1φ(sτ1, a2
τ1) +"
PROOF OF TECHNICAL RESULTS,0.3326271186440678,"τ4−1
X"
PROOF OF TECHNICAL RESULTS,0.3336864406779661,"t=τ3
γt+1φ(st+1, a2
t+1) −γtφ(st, a2
t) + γτ3φ(sτ3, a2
τ3)"
PROOF OF TECHNICAL RESULTS,0.3347457627118644,+ . . . +
PROOF OF TECHNICAL RESULTS,0.3358050847457627,"τ2K−1
X"
PROOF OF TECHNICAL RESULTS,0.336864406779661,"t=τ(2k−1)
γtφ(st, a2
t) −γt−1φ(st−1, a2
t−1) + γτ2k−1φ(sτ2k−1, a2
τ2k−1) + . . . + = ∞
X k=1"
PROOF OF TECHNICAL RESULTS,0.3379237288135593,"τ2K−1
X"
PROOF OF TECHNICAL RESULTS,0.3389830508474576,"t=τ2k−1
γt+1φ(st+1, a2
t+1) −γtφ(st, a2
t) − ∞
X"
PROOF OF TECHNICAL RESULTS,0.3400423728813559,"k=1
γτ2k−1φ(sτ2k−1, a2
τ2k−1)"
PROOF OF TECHNICAL RESULTS,0.3411016949152542,"Under review as a conference paper at ICLR 2022 = ∞
X"
PROOF OF TECHNICAL RESULTS,0.3421610169491525,"k=1
γτ2kφ(sτ2k, a2
τ2k) − ∞
X"
PROOF OF TECHNICAL RESULTS,0.3432203389830508,"k=1
γτ2k−1φ(sτ2k−1, a2
τ2k−1) = ∞
X"
PROOF OF TECHNICAL RESULTS,0.3442796610169492,"k=1
γτ2kφ(sτ2k, 0) − ∞
X"
PROOF OF TECHNICAL RESULTS,0.3453389830508475,"k=1
γτ2k−1φ(sτ2k−1, 0) = 0,"
PROOF OF TECHNICAL RESULTS,0.3463983050847458,"where we have used the fact that by construction a2
t ≡0 whenever t = τ1, τ2, . . . and by construction
φ(s, 0) ≡0 for any s."
PROOF OF TECHNICAL RESULTS,0.3474576271186441,"With this we readily deduce that vπ,π2"
PROOF OF TECHNICAL RESULTS,0.3485169491525424,"i
(s) = Eπ,π2 [P∞
t=0 γtRi(st, at)] which is a measure of
environment rewards only from which statement (i) can be readily deduced."
PROOF OF TECHNICAL RESULTS,0.3495762711864407,"For part (ii) we note ﬁrst that it is easy to see that vπ,π2
2
(s0, I0) is bounded above, indeed using the
key result in the proof of part (i) and the properties of c we have that"
PROOF OF TECHNICAL RESULTS,0.350635593220339,"vπ,π2
2
(s0, I0) = Eπ,π2  
∞
X"
PROOF OF TECHNICAL RESULTS,0.3516949152542373,"t=0
γt "
PROOF OF TECHNICAL RESULTS,0.3527542372881356,"ˆR +
X"
PROOF OF TECHNICAL RESULTS,0.3538135593220339,"k≥1
c(It, It−1)δt
τ2k−1 + Ln(st)   "
PROOF OF TECHNICAL RESULTS,0.3548728813559322,"
(13)"
PROOF OF TECHNICAL RESULTS,0.3559322033898305,"= Eπ,π2  
∞
X"
PROOF OF TECHNICAL RESULTS,0.3569915254237288,"t=0
γt "
PROOF OF TECHNICAL RESULTS,0.3580508474576271,"R +
X"
PROOF OF TECHNICAL RESULTS,0.3591101694915254,"k≥1
c(It, It−1)δt
τ2k−1 + Ln(st)  + ∞
X"
PROOF OF TECHNICAL RESULTS,0.3601694915254237,"t=0
γt ˆFIt "
PROOF OF TECHNICAL RESULTS,0.361228813559322,"
(14)"
PROOF OF TECHNICAL RESULTS,0.3622881355932203,"≤Eπ,π2 "" ∞
X"
PROOF OF TECHNICAL RESULTS,0.3633474576271186,"t=0
γt (R + Ln(st)) # (15) ≤ Eπ,π2 "" ∞
X"
PROOF OF TECHNICAL RESULTS,0.3644067796610169,"t=0
γt (R + Ln(st))"
PROOF OF TECHNICAL RESULTS,0.3654661016949153,"#
(16)"
PROOF OF TECHNICAL RESULTS,0.3665254237288136,"≤Eπ,π2 "" ∞
X"
PROOF OF TECHNICAL RESULTS,0.3675847457627119,"t=0
γt ∥R + Ln∥ # (17) ≤ ∞
X"
PROOF OF TECHNICAL RESULTS,0.3686440677966102,"t=0
γt (∥R∥+ ∥Ln∥)
(18)"
PROOF OF TECHNICAL RESULTS,0.3697033898305085,"=
1
1 −γ (∥R∥+ ∥L∥) ,
(19)"
PROOF OF TECHNICAL RESULTS,0.3707627118644068,"using the triangle inequality, the deﬁnition of ˆR and the (upper-)boundedness of L and R (Assumption
5). We now note that by the dominated convergence theorem we have that ∀(s0, I0) ∈S × {0, 1}"
PROOF OF TECHNICAL RESULTS,0.3718220338983051,"lim
n→∞vπ,π2
2
(s0, I0) = lim
n→∞Eπ,π2  
∞
X"
PROOF OF TECHNICAL RESULTS,0.3728813559322034,"t=0
γt "
PROOF OF TECHNICAL RESULTS,0.3739406779661017,"ˆR +
X"
PROOF OF TECHNICAL RESULTS,0.375,"k≥1
c(It, It−1)δt
τ2k−1 + Ln(st)   "
PROOF OF TECHNICAL RESULTS,0.3760593220338983,"
(20)"
PROOF OF TECHNICAL RESULTS,0.3771186440677966,"= Eπ,π2 lim
n→∞  
∞
X"
PROOF OF TECHNICAL RESULTS,0.3781779661016949,"t=0
γt "
PROOF OF TECHNICAL RESULTS,0.3792372881355932,"ˆR +
X"
PROOF OF TECHNICAL RESULTS,0.3802966101694915,"k≥1
c(It, It−1)δt
τ2k−1 + Ln(st)   "
PROOF OF TECHNICAL RESULTS,0.3813559322033898,"
(21)"
PROOF OF TECHNICAL RESULTS,0.3824152542372881,"= Eπ,π2  
∞
X"
PROOF OF TECHNICAL RESULTS,0.3834745762711864,"t=0
γt "
PROOF OF TECHNICAL RESULTS,0.3845338983050847,"ˆR +
X"
PROOF OF TECHNICAL RESULTS,0.3855932203389831,"k≥1
c(It, It−1)δt
τ2k−1   "
PROOF OF TECHNICAL RESULTS,0.3866525423728814,"
(22)"
PROOF OF TECHNICAL RESULTS,0.3877118644067797,"= Eπ,π2  
∞
X"
PROOF OF TECHNICAL RESULTS,0.388771186440678,"t=0
γt "
PROOF OF TECHNICAL RESULTS,0.3898305084745763,"R +
X"
PROOF OF TECHNICAL RESULTS,0.3908898305084746,"k≥1
c(It, It−1)δt
τ2k−1   "
PROOF OF TECHNICAL RESULTS,0.3919491525423729,"=
K
1 −γ + vπ
1 (s0),
(23)"
PROOF OF TECHNICAL RESULTS,0.3930084745762712,"again using the key result in the proof of (i) and Assumption 6 in the last step, after which we deduce
(ii)."
PROOF OF TECHNICAL RESULTS,0.3940677966101695,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.3951271186440678,Note that by (ii) we heron may consider the quantity for the Shaper expected return:
PROOF OF TECHNICAL RESULTS,0.3961864406779661,"ˆvπ,π2
2
(s0, I0) = Eπ,π2  
∞
X"
PROOF OF TECHNICAL RESULTS,0.3972457627118644,"t=0
γt "
PROOF OF TECHNICAL RESULTS,0.3983050847457627,"R +
X"
PROOF OF TECHNICAL RESULTS,0.399364406779661,"k≥1
c(It, It−1)δt
τ2k−1   "
PROOF OF TECHNICAL RESULTS,0.4004237288135593,".
(24)"
PROOF OF TECHNICAL RESULTS,0.4014830508474576,PROOF OF THEOREM 1
PROOF OF TECHNICAL RESULTS,0.4025423728813559,"Proof 4 Theorem 1 is proved by ﬁrstly showing that when the players jointly maximise the same
objective there exists a ﬁxed point equilibrium of the game when all players use Markov policies and
Shaper uses switching control. The proof then proceeds by showing that the MG G admits a dual
representation as an MG in which jointly maximise the same objective which has a stable point that
can be computed by solving an MDP. Thereafter, we use both results to prove the existence of a ﬁxed
point for the game as a limit point of a sequence generated by successively applying the Bellman
operator to a test function."
PROOF OF TECHNICAL RESULTS,0.4036016949152542,"Therefore, the scheme of the proof is summarised with the following steps:"
PROOF OF TECHNICAL RESULTS,0.4046610169491525,"I) Prove that the solution to Markov Team games (that is games in which both players maximise
identical objectives) in which one of the players uses switching control is the limit point of a
sequence of Bellman operators (acting on some test function)."
PROOF OF TECHNICAL RESULTS,0.4057203389830508,"II) Prove that for the MG G that is there exists a function Bπ,π2 : S × {0, 1} →R such that7 vπ,π2"
PROOF OF TECHNICAL RESULTS,0.4067796610169492,"i
(z) −vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.4078389830508475,"i
(z) = Bπ,π2(z) −Bπ′,π2(z), ∀z ≡(s, I0) ∈S × {0, 1}, ∀i ∈{1, 2}."
PROOF OF TECHNICAL RESULTS,0.4088983050847458,"III) Prove that the MG G has a dual representation as a Markov Team Game which admits a
representation as an MDP."
PROOF OF TECHNICAL RESULTS,0.4099576271186441,PROOF OF PART I
PROOF OF TECHNICAL RESULTS,0.4110169491525424,"Our ﬁrst result proves that the operator T is a contraction operator. First let us recall that the
switching time τk is deﬁned recursively τk = inf{t > τk−1|st ∈A, τk ∈Ft} where A = {s ∈
S, m ∈M|g2(m|st) > 0}. To this end, we show that the following bounds holds:"
PROOF OF TECHNICAL RESULTS,0.4120762711864407,"Lemma 3 The Bellman operator T is a contraction, that is the following bound holds:"
PROOF OF TECHNICAL RESULTS,0.413135593220339,∥Tψ −Tψ′∥≤γ ∥ψ −ψ′∥.
PROOF OF TECHNICAL RESULTS,0.4141949152542373,Proof 5 Recall we deﬁne the Bellman operator Tψ of G acting on a function Λ : S × N →R by
PROOF OF TECHNICAL RESULTS,0.4152542372881356,"TψΛ(sτk, I(τk)) := max ("
PROOF OF TECHNICAL RESULTS,0.4163135593220339,"Mπ,π2Λ(sτk, I(τk)), """
PROOF OF TECHNICAL RESULTS,0.4173728813559322,"ψ(sτk, a) + γmax
a∈A X"
PROOF OF TECHNICAL RESULTS,0.4184322033898305,"s′∈S
P(s′; a, sτk)Λ(s′, I(τk)) #) (25)"
PROOF OF TECHNICAL RESULTS,0.4194915254237288,"In what follows and for the remainder of the script, we employ the following shorthands:"
PROOF OF TECHNICAL RESULTS,0.4205508474576271,"Pa
ss′ =:
X"
PROOF OF TECHNICAL RESULTS,0.4216101694915254,"s′∈S
P(s′; a, s),
Pπ
ss′ =:
X"
PROOF OF TECHNICAL RESULTS,0.4226694915254237,"a∈A
π(a|s)Pa
ss′,
Rπ(zt) :=
X"
PROOF OF TECHNICAL RESULTS,0.423728813559322,"at∈A
π(at|s) ˆR(zt, at, θt, θt−1)"
PROOF OF TECHNICAL RESULTS,0.4247881355932203,"To prove that T is a contraction, we consider the three cases produced by (25), that is to say we prove
the following statements:"
PROOF OF TECHNICAL RESULTS,0.4258474576271186,"i)
Θ(zt, a, a2
t, a2
t−1) + γmax
a∈A Pa
s′stψ(s′, ·) −

Θ(zt, a, a2
t, a2
t−1) + γmax
a∈A Pa
s′stψ′(s′, ·)
 ≤"
PROOF OF TECHNICAL RESULTS,0.4269067796610169,γ ∥ψ −ψ′∥
PROOF OF TECHNICAL RESULTS,0.4279661016949153,"ii)
Mπ,π2ψ −Mπ,π2ψ′ ≤γ ∥ψ −ψ′∥,
(and hence M is a contraction)."
PROOF OF TECHNICAL RESULTS,0.4290254237288136,"7This property is analogous to the condition in Markov potential games [19, 26]"
PROOF OF TECHNICAL RESULTS,0.4300847457627119,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.4311440677966102,"iii)
Mπ,π2ψ −

Θ(·, a) + γmax
a∈A Paψ′
 ≤γ ∥ψ −ψ′∥. where zt ≡(st, It) ∈S ×"
PROOF OF TECHNICAL RESULTS,0.4322033898305085,"{0, 1}."
PROOF OF TECHNICAL RESULTS,0.4332627118644068,We begin by proving i).
PROOF OF TECHNICAL RESULTS,0.4343220338983051,"Indeed, for any a ∈A and ∀zt ∈S × {0, 1}, ∀θt, θt−1 ∈Θ, ∀s′ ∈S we have that
Θ(zt, a, a2
t, a2
t−1) + γPπ
s′stψ(s′, ·) −

Θ(zt, a, a2
t, a2
t−1) + γmax
a∈A Pa
s′stψ′(s′, ·)
"
PROOF OF TECHNICAL RESULTS,0.4353813559322034,"≤max
a∈A
γPa
s′stψ(s′, ·) −γPa
s′stψ′(s′, ·)"
PROOF OF TECHNICAL RESULTS,0.4364406779661017,≤γ ∥Pψ −Pψ′∥
PROOF OF TECHNICAL RESULTS,0.4375,"≤γ ∥ψ −ψ′∥,"
PROOF OF TECHNICAL RESULTS,0.4385593220338983,again using the fact that P is non-expansive and Lemma 1.
PROOF OF TECHNICAL RESULTS,0.4396186440677966,We now prove ii).
PROOF OF TECHNICAL RESULTS,0.4406779661016949,"For any τ ∈F, deﬁne by τ ′ = inf{t > τ|st ∈A, τ ∈Ft}. Now using the deﬁnition of M we have
that for any sτ ∈S
(Mπ,π2ψ −Mπ,π2ψ′)(sτ, I(τ))"
PROOF OF TECHNICAL RESULTS,0.4417372881355932,"≤
max
aτ ,a2τ ,a2
τ−1∈A×Θ2"
PROOF OF TECHNICAL RESULTS,0.4427966101694915,"Θ(zτ, aτ, a2
τ, a2
τ−1) + c(Iτ, Iτ−1) + γPπ
s′sτ Paψ(sτ, I(τ ′))"
PROOF OF TECHNICAL RESULTS,0.4438559322033898,"−
 
Θ(zτ, aτ, a2
τ, a2
τ−1) + c(Iτ, Iτ−1) + γPπ
s′sτ Paψ′(sτ, I(τ ′))
"
PROOF OF TECHNICAL RESULTS,0.4449152542372881,"= γ
Pπ
s′sτ Paψ(sτ, I(τ ′)) −Pπ
s′sτ Paψ′(sτ, I(τ ′))"
PROOF OF TECHNICAL RESULTS,0.4459745762711864,≤γ ∥Pψ −Pψ′∥
PROOF OF TECHNICAL RESULTS,0.4470338983050847,"≤γ ∥ψ −ψ′∥,"
PROOF OF TECHNICAL RESULTS,0.4480932203389831,"using the fact that P is non-expansive. The result can then be deduced easily by applying max on
both sides."
PROOF OF TECHNICAL RESULTS,0.4491525423728814,We now prove iii). We split the proof of the statement into two cases:
PROOF OF TECHNICAL RESULTS,0.4502118644067797,Case 1:
PROOF OF TECHNICAL RESULTS,0.451271186440678,"Mπ,π2ψ(sτ, I(τ)) −

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ))

< 0.
(26)"
PROOF OF TECHNICAL RESULTS,0.4523305084745763,We now observe the following:
PROOF OF TECHNICAL RESULTS,0.4533898305084746,"Mπ,π2ψ(sτ, I(τ)) −Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ))"
PROOF OF TECHNICAL RESULTS,0.4544491525423729,"≤max
n
Θ(zτ, aτ, a2
τ, a2
τ−1) + γPπ
s′sτ Paψ(s′, I(τ)), Mπ,π2ψ(sτ, I(τ))
o"
PROOF OF TECHNICAL RESULTS,0.4555084745762712,"−Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ)) ≤"
PROOF OF TECHNICAL RESULTS,0.4565677966101695,"max
n
Θ(zτ, aτ, a2
τ, a2
τ−1) + γPπ
s′sτ Paψ(s′, I(τ)), Mπ,π2ψ(sτ, I(τ))
o"
PROOF OF TECHNICAL RESULTS,0.4576271186440678,"−max

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ)), Mπ,π2ψ(sτ, I(τ))
"
PROOF OF TECHNICAL RESULTS,0.4586864406779661,"+ max

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ)), Mπ,π2ψ(sτ, I(τ))
"
PROOF OF TECHNICAL RESULTS,0.4597457627118644,"−Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ)) "
PROOF OF TECHNICAL RESULTS,0.4608050847457627,Under review as a conference paper at ICLR 2022 ≤
PROOF OF TECHNICAL RESULTS,0.461864406779661,"max

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ(s′, I(τ)), Mπ,π2ψ(sτ, I(τ))
"
PROOF OF TECHNICAL RESULTS,0.4629237288135593,"−max

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ)), Mπ,π2ψ(sτ, I(τ))
 +"
PROOF OF TECHNICAL RESULTS,0.4639830508474576,"max

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ)), Mπ,π2ψ(sτ, I(τ))
"
PROOF OF TECHNICAL RESULTS,0.4650423728813559,"−Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ)) "
PROOF OF TECHNICAL RESULTS,0.4661016949152542,"≤γmax
a∈A
Pπ
s′sτ Paψ(s′, I(τ)) −Pπ
s′sτ Paψ′(s′, I(τ))"
PROOF OF TECHNICAL RESULTS,0.4671610169491525,"+
max

0, Mπ,π2ψ(sτ, I(τ)) −

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ))
"
PROOF OF TECHNICAL RESULTS,0.4682203389830508,≤γ ∥Pψ −Pψ′∥
PROOF OF TECHNICAL RESULTS,0.4692796610169492,"≤γ∥ψ −ψ′∥,"
PROOF OF TECHNICAL RESULTS,0.4703389830508475,"where we have used the fact that for any scalars a, b, c we have that |max{a, b} −max{b, c}| ≤
|a −c| and the non-expansiveness of P."
PROOF OF TECHNICAL RESULTS,0.4713983050847458,Case 2:
PROOF OF TECHNICAL RESULTS,0.4724576271186441,"Mπ,π2ψ(sτ, I(τ)) −

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ))

≥0."
PROOF OF TECHNICAL RESULTS,0.4735169491525424,"For this case, ﬁrst recall that for any τ ∈F, −c(Iτ, Iτ−1) > λ for some λ > 0."
PROOF OF TECHNICAL RESULTS,0.4745762711864407,"Mπ,π2ψ(sτ, I(τ)) −

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ))
"
PROOF OF TECHNICAL RESULTS,0.475635593220339,"≤Mπ,π2ψ(sτ, I(τ)) −

Θ(zτ, aτ, a2
τ, a2
τ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ))

−c(Iτ, Iτ−1)"
PROOF OF TECHNICAL RESULTS,0.4766949152542373,"≤Θ(zτ, aτ, a2
τ, a2
τ−1) + c(Iτ, Iτ−1) + γPπ
s′sτ Paψ(s′, I(τ ′))"
PROOF OF TECHNICAL RESULTS,0.4777542372881356,"−

Θ(zτ, aτ, a2
τ, a2
τ−1) + c(Iτ, Iτ−1) + γmax
a∈A Pa
s′sτ ψ′(s′, I(τ))
"
PROOF OF TECHNICAL RESULTS,0.4788135593220339,"≤γmax
a∈A
Pπ
s′sτ Pa (ψ(s′, I(τ ′)) −ψ′(s′, I(τ)))"
PROOF OF TECHNICAL RESULTS,0.4798728813559322,"≤γ |ψ(s′, I(τ ′)) −ψ′(s′, I(τ))|"
PROOF OF TECHNICAL RESULTS,0.4809322033898305,"≤γ ∥ψ −ψ′∥,"
PROOF OF TECHNICAL RESULTS,0.4819915254237288,"again using the fact that P is non-expansive. Hence we have succeeded in showing that for any
Λ ∈L2 we have that
Mπ,π2Λ −max
a∈A [ψ(·, a) + γPaΛ′]
 ≤γ ∥Λ −Λ′∥.
(27)"
PROOF OF TECHNICAL RESULTS,0.4830508474576271,Gathering the results of the three cases gives the desired result.
PROOF OF TECHNICAL RESULTS,0.4841101694915254,PROOF OF PART II
PROOF OF TECHNICAL RESULTS,0.4851694915254237,"To prove Part II, we prove the following result:"
PROOF OF TECHNICAL RESULTS,0.486228813559322,"Proposition 4 For any π ∈Π and for any Shaper policy π2, there exists a function Bπ,π2 : S ×
{0, 1} →R such that vπ,π2"
PROOF OF TECHNICAL RESULTS,0.4872881355932203,"i
(z) −vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.4883474576271186,"i
(z) = Bπ,π2(z) −Bπ′,π2(z), ∀z ≡(s, I0) ∈S × {0, 1}
(28)"
PROOF OF TECHNICAL RESULTS,0.4894067796610169,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.4904661016949153,where in particular the function B is given by:
PROOF OF TECHNICAL RESULTS,0.4915254237288136,"Bπ,π2(s0, I0) = Eπ,π2  
∞
X"
PROOF OF TECHNICAL RESULTS,0.4925847457627119,"t=0
γt "
PROOF OF TECHNICAL RESULTS,0.4936440677966102,"R +
X"
PROOF OF TECHNICAL RESULTS,0.4947033898305085,"k≥1
c(It, It−1)δt
τ2k−1   "
PROOF OF TECHNICAL RESULTS,0.4957627118644068,",
(29)"
PROOF OF TECHNICAL RESULTS,0.4968220338983051,"for any (s0, I0) ∈S × {0, 1}."
PROOF OF TECHNICAL RESULTS,0.4978813559322034,"Proof 6 Note that by the deduction of (ii) in Prop 1, we immediately observe that"
PROOF OF TECHNICAL RESULTS,0.4989406779661017,"ˆvπ,π2
2
(s0, I0) = Bπ,π2(s0, I0), ∀(s0, I0) ∈S × {0, 1}.
(30)"
PROOF OF TECHNICAL RESULTS,0.5,"We therefore immediately deduce that for any two Shaper policies π2 and π′2 the following expression
holds ∀(s0, I0) ∈S × {0, 1}:"
PROOF OF TECHNICAL RESULTS,0.5010593220338984,"ˆvπ,π2
2
(s0, I0) −ˆvπ,π′2
2
(s0, I0) = Bπ,π2(s0, I0) −Bπ,π′2(s0, I0).
(31)"
PROOF OF TECHNICAL RESULTS,0.5021186440677966,"Our aim now is to show that the following expression holds ∀(s0, I0) ∈S × {0, 1}:"
PROOF OF TECHNICAL RESULTS,0.503177966101695,"vπ,π2
1
(I0, s0) −vπ′,π2
1
(I0, s0) = Bπ,π2(I0, s0) −Bπ′,π2(I0, s0), ∀i ∈N"
PROOF OF TECHNICAL RESULTS,0.5042372881355932,"For the ﬁnite horizon case, the result is proven by induction on the number of time steps until the end
of the game. Unlike the inﬁnite horizon case, for the ﬁnite horizon case the value function and policy
have an explicit time dependence."
PROOF OF TECHNICAL RESULTS,0.5052966101694916,"We consider the case of the proposition at time T −1 that is we evaluate the value functions at the
penultimate time step. In this case, we have that:"
PROOF OF TECHNICAL RESULTS,0.5063559322033898,"EsT −1∼dθ
h
Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.5074152542372882,"T −1(IT −1, sT −1) −Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.5084745762711864,"T −1 (IT −1, sT −1)
i"
PROOF OF TECHNICAL RESULTS,0.5095338983050848,"= EsT −1∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.510593220338983,"aT −1∈A
π(aT −1; sT −1) "
PROOF OF TECHNICAL RESULTS,0.5116525423728814,"R(sT −1, aT −1) +
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5127118644067796,"j=T −1
c(Ij, Ij−1)δj
τk   + γ
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.513771186440678,"aT −1∈A
π(aT −1; sT −1)P(sT ; aT −1)Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.5148305084745762,"T
(IT , sT ) − X"
PROOF OF TECHNICAL RESULTS,0.5158898305084746,"a′T −1∈A
π′(a′
T −1; sT −1) "
PROOF OF TECHNICAL RESULTS,0.5169491525423728,"R(sT −1, a′
T −1) +
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5180084745762712,"j=T −1
c(Ij, Ij−1)δj
τk   + γ
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5190677966101694,"a′T −1∈A
π′(a′
T −1; sT −1)P(sT ; a′
T −1)Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.5201271186440678,"T
(IT , sT ) !#"
PROOF OF TECHNICAL RESULTS,0.5211864406779662,"= EsT −1∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.5222457627118644,"aT −1∈A
π(aT −1; sT −1)R(sT −1, aT −1) −
X"
PROOF OF TECHNICAL RESULTS,0.5233050847457628,"a′T −1∈A
π′(a′
T −1; sT −1)R(sT −1, a′
T −1) + γ "" X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.524364406779661,"aT −1∈A
π(aT −1; sT −1)P(sT ; aT −1)Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.5254237288135594,"T
(IT , sT ) −
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5264830508474576,"a′T −1∈A
π′(a′
T −1; sT −1)P(sT ; a′
T −1)Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.527542372881356,"T
(IT , sT ) ## . (32)"
PROOF OF TECHNICAL RESULTS,0.5286016949152542,"We now observe that for any π
∈
Π and for any π2 we have that Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.5296610169491526,"T
(IT , sT )
= E
hP"
PROOF OF TECHNICAL RESULTS,0.5307203389830508,"aT ∈A π(aT ; sT )
h
R(sT , aT ) + P"
PROOF OF TECHNICAL RESULTS,0.5317796610169492,"k≥0
P∞
j=T c(Ij, Ij−1)δj
τk
ii
, moreover we have that for any"
PROOF OF TECHNICAL RESULTS,0.5328389830508474,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.5338983050847458,"π ∈Π and for any π2 E "" X"
PROOF OF TECHNICAL RESULTS,0.534957627118644,"aT ∈A
π(aT ; sT ) "
PROOF OF TECHNICAL RESULTS,0.5360169491525424,"R(sT , aT ) +
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5370762711864406,"j=T
c(Ij, Ij−1)δj
τk   −
X"
PROOF OF TECHNICAL RESULTS,0.538135593220339,"a′T ∈A
π′(aT ; sT ) "
PROOF OF TECHNICAL RESULTS,0.5391949152542372,"R(sT , a′
T ) +
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5402542372881356,"j=T
c(Ij, Ij−1)δj
τk   # =E "" X"
PROOF OF TECHNICAL RESULTS,0.5413135593220338,"aT ∈A
π(aT ; sT )R(sT , aT ) −
X"
PROOF OF TECHNICAL RESULTS,0.5423728813559322,"a′T ∈A
π′(a′
T ; sT )R(sT , aT ) # + E  X"
PROOF OF TECHNICAL RESULTS,0.5434322033898306,"aT ∈A
π(aT ; sT )
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5444915254237288,"j=T
c(Ij, Ij−1)δj
τk −
X"
PROOF OF TECHNICAL RESULTS,0.5455508474576272,"a′T ∈A
π′(a′
T ; sT )
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5466101694915254,"j=T
c(Ij, Ij−1)δj
τk  ."
PROOF OF TECHNICAL RESULTS,0.5476694915254238,Hence we ﬁnd that
PROOF OF TECHNICAL RESULTS,0.548728813559322,"EsT −1∼dθ
h
Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.5497881355932204,"T −1(IT −1, sT −1) −Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.5508474576271186,"T −1 (IT −1, sT −1)
i"
PROOF OF TECHNICAL RESULTS,0.551906779661017,"= EsT −1∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.5529661016949152,"aT −1∈A
π(aT −1; sT −1)R(sT −1, aT −1) −
X"
PROOF OF TECHNICAL RESULTS,0.5540254237288136,"a′T −1∈A
π′(a′
T −1; sT −1)R(sT −1, a′
T −1)
(33) + γ X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5550847457627118,aT −1∈A X
PROOF OF TECHNICAL RESULTS,0.5561440677966102,"aT ∈A
π(aT −1; sT −1)P(sT ; aT −1)π(aT ; sT )R(sT , aT )
(34) −
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5572033898305084,a′T −1∈A X
PROOF OF TECHNICAL RESULTS,0.5582627118644068,"a′T ∈A
π′(a′
T −1; sT −1)P(sT ; a′
T −1)π′(a′
T ; sT )R(sT , aT )
(35) +
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.559322033898305,"aT −1∈A X aT ∈A X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5603813559322034,"j=T
π(aT −1; sT −1)P(sT ; aT −1)π(aT ; sT )c(Ij, Ij−1)δj
τk
(36) −
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5614406779661016,a′T −1∈A X
PROOF OF TECHNICAL RESULTS,0.5625,"a′T ∈A X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5635593220338984,"j=T
π′(a′
T −1; sT −1)P(sT ; a′
T −1)π′(a′
T ; sT )c(Ij, Ij−1)δj
τk !# . (37) Now"
PROOF OF TECHNICAL RESULTS,0.5646186440677966,"EsT −1∼dθ "" X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.565677966101695,"aT −1∈A X aT ∈A X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5667372881355932,"j=T
π(aT −1; sT −1)P(sT ; aT −1)π(aT ; sT )c(Ij, Ij−1)δj
τk (38) −
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5677966101694916,a′T −1∈A X
PROOF OF TECHNICAL RESULTS,0.5688559322033898,"a′T ∈A X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5699152542372882,"j=T
π′(a′
T −1; sT −1)P(sT ; a′
T −1)π′(a′
T ; sT )c(Ij, Ij−1)δj
τk # (39)"
PROOF OF TECHNICAL RESULTS,0.5709745762711864,"= EsT −1∼dθ "" X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5720338983050848,aT −1∈A X
PROOF OF TECHNICAL RESULTS,0.573093220338983,"aT ∈A
π(aT −1; sT −1)P(sT ; aT −1)π(aT ; sT )
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5741525423728814,"j=T
c(Ij, Ij−1)δj
τk (40) −
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5752118644067796,a′T −1∈A X
PROOF OF TECHNICAL RESULTS,0.576271186440678,"a′T ∈A
π′(a′
T −1; sT −1)P(sT ; a′
T −1)π′(a′
T ; sT )
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5773305084745762,"j=T
c(Ij, Ij−1)δj
τk # (41)"
PROOF OF TECHNICAL RESULTS,0.5783898305084746,"= EsT −1∼dθ "" X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5794491525423728,aT −1∈A X
PROOF OF TECHNICAL RESULTS,0.5805084745762712,"aT ∈A
π(aT −1; sT −1)P(sT ; aT −1)π(aT ; sT )
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5815677966101694,"j=T
c(Ij, Ij−1)δj
τk (42)"
PROOF OF TECHNICAL RESULTS,0.5826271186440678,"Under review as a conference paper at ICLR 2022 −
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5836864406779662,a′T −1∈A X
PROOF OF TECHNICAL RESULTS,0.5847457627118644,"a′T ∈A
π′(a′
T −1; sT −1)P(sT ; a′
T −1)π′(a′
T ; sT )
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.5858050847457628,"j=T
c(Ij, Ij−1)δj
τk # (43)"
PROOF OF TECHNICAL RESULTS,0.586864406779661,"= KEsT −1∼dθ "" X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.5879237288135594,aT −1∈A X
PROOF OF TECHNICAL RESULTS,0.5889830508474576,"aT ∈A
π(aT −1; sT −1)P(sT ; aT −1)π(aT ; sT )
(44) −
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.590042372881356,a′T −1∈A X
PROOF OF TECHNICAL RESULTS,0.5911016949152542,"a′T ∈A
π′(a′
T −1; sT −1)P(sT ; a′
T −1)π′(a′
T ; sT ) # (45) = K X"
PROOF OF TECHNICAL RESULTS,0.5921610169491526,"aT ∈A
π(aT ) −
X"
PROOF OF TECHNICAL RESULTS,0.5932203389830508,"a′T ∈A
π′(a′
T ) !"
PROOF OF TECHNICAL RESULTS,0.5942796610169492,"= 0.
(46)"
PROOF OF TECHNICAL RESULTS,0.5953389830508474,"Hence, we ﬁnd that"
PROOF OF TECHNICAL RESULTS,0.5963983050847458,"EsT −1∼dθ
h
Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.597457627118644,"T −1(IT −1, sT −1) −Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.5985169491525424,"T −1 (IT −1, sT −1)
i"
PROOF OF TECHNICAL RESULTS,0.5995762711864406,"= EsT −1∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.600635593220339,"aT −1∈A
π(aT −1; sT −1)R(sT −1, aT −1) −
X"
PROOF OF TECHNICAL RESULTS,0.6016949152542372,"a′T −1∈A
π′(a′
T −1; sT −1)R(sT −1, a′
T −1)
(47) + γ X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.6027542372881356,aT −1∈A X
PROOF OF TECHNICAL RESULTS,0.6038135593220338,"aT ∈A
π(aT −1; sT −1)P(sT ; aT −1)π(aT ; sT )R(sT , aT )
(48) −
X sT ∈S X"
PROOF OF TECHNICAL RESULTS,0.6048728813559322,a′T −1∈A X
PROOF OF TECHNICAL RESULTS,0.6059322033898306,"a′T ∈A
π′(a′
T −1; sT −1)P(sT ; a′
T −1)π′(a′
T ; sT )R(sT , aT ) !# (49)"
PROOF OF TECHNICAL RESULTS,0.6069915254237288,"= EsT −1∼dθ
h
vπ,π2"
PROOF OF TECHNICAL RESULTS,0.6080508474576272,"i,T −1(sT −1) −vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.6091101694915254,"i,T −1(sT −1)
i
.
(50)"
PROOF OF TECHNICAL RESULTS,0.6101694915254238,"Hence, we have succeeded in proving that the expression (28) holds for T −k when k = 1."
PROOF OF TECHNICAL RESULTS,0.611228813559322,Our next goal is to prove that the expression holds for any 0 < k ≤T.
PROOF OF TECHNICAL RESULTS,0.6122881355932204,"Note that for any T ≥k > 0, we can write Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.6133474576271186,"T −k as Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.614406779661017,"T −k(I0, s0) = Eπ """
PROOF OF TECHNICAL RESULTS,0.6154661016949152,"R(sT −k, aT −k) +
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.6165254237288136,"j=T −j
c(Ij, Ij−1)δj
τk
(51) + γ
X"
PROOF OF TECHNICAL RESULTS,0.6175847457627118,"sk+1∈S
P(s′; sT −k, aT −k)Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.6186440677966102,"T −(k+1)(IT −(k+1), sT −(k+1)) # . (52)"
PROOF OF TECHNICAL RESULTS,0.6197033898305084,"Now we consider the case when we evaluate the expression (28) for any 0 < k ≤T. Our inductive
hypothesis is the the expression holds for some 0 < k ≤T, that is for any 0 < k ≤T we have that:
X"
PROOF OF TECHNICAL RESULTS,0.6207627118644068,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.621822033898305,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))P(sT −k; aT −(k+1))vπ,π2"
PROOF OF TECHNICAL RESULTS,0.6228813559322034,"i,T −k(IT −k, sT −k) −
X"
PROOF OF TECHNICAL RESULTS,0.6239406779661016,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.625,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))P(sT −k; a′
T −(k+1))vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.6260593220338984,"i,T −k(IT −k, sT −k)
(53) =
X"
PROOF OF TECHNICAL RESULTS,0.6271186440677966,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.628177966101695,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))P(sT −k; aT −(k+1))Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.6292372881355932,"T −k(IT −k, sT −k)
(54) −
X"
PROOF OF TECHNICAL RESULTS,0.6302966101694916,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6313559322033898,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))P(sT −k; a′
T −(k+1))Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.6324152542372882,"T −k (IT −k, sT −k).
(55)"
PROOF OF TECHNICAL RESULTS,0.6334745762711864,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.6345338983050848,"It remains to show that the expression holds for k + 1 time steps prior to the end of the horizon. The
result can be obtained using the dynamic programming principle and the base case (k = 1) result,
indeed we have that"
PROOF OF TECHNICAL RESULTS,0.635593220338983,"EsT −(k+1)∼dθ
h
Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.6366525423728814,"T −(k+1)(IT −(k+1), sT −(k+1)) −Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.6377118644067796,"T −(k+1)(IT −(k+1), sT −(k+1))
i"
PROOF OF TECHNICAL RESULTS,0.638771186440678,"= EsT −(k+1)∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.6398305084745762,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))φ(sT −(k+1), aT −(k+1)) −
X"
PROOF OF TECHNICAL RESULTS,0.6408898305084746,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))φ(sT −(k+1), a′
T −(k+1)) + γ
X"
PROOF OF TECHNICAL RESULTS,0.6419491525423728,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6430084745762712,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))P(sT −k; aT −(k+1))Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.6440677966101694,"T −k(IT −k, sT −k) −γ
X"
PROOF OF TECHNICAL RESULTS,0.6451271186440678,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6461864406779662,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))P(sT −k; a′
T −(k+1))Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.6472457627118644,"T −k (IT −k, sT −k) #"
PROOF OF TECHNICAL RESULTS,0.6483050847457628,"= EsT −(k+1)∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.649364406779661,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1)) "
PROOF OF TECHNICAL RESULTS,0.6504237288135594,"R(sT −(k+1), aT −(k+1)) +
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.6514830508474576,"j=T −1
c(Ij, Ij−1)δj
τk   + γ
X"
PROOF OF TECHNICAL RESULTS,0.652542372881356,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6536016949152542,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))P(sT −k; aT −(k+1))Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.6546610169491526,"T −k(IT −k, sT −k) − X"
PROOF OF TECHNICAL RESULTS,0.6557203389830508,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1)) "
PROOF OF TECHNICAL RESULTS,0.6567796610169492,"R(sT −(k+1), a′
T −(k+1)) +
X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.6578389830508474,"j=T −1
c(Ij, Ij−1)δj
τk   + γ
X"
PROOF OF TECHNICAL RESULTS,0.6588983050847458,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.659957627118644,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))P(sT −k; a′
T −(k+1))Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.6610169491525424,"T −k (IT −k, sT −k) #!"
PROOF OF TECHNICAL RESULTS,0.6620762711864406,"= EsT −(k+1)∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.663135593220339,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))R(sT −(k+1), aT −(k+1)) −
X"
PROOF OF TECHNICAL RESULTS,0.6641949152542372,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))R(sT −(k+1), a′
T −(k+1)) + γ ""
X"
PROOF OF TECHNICAL RESULTS,0.6652542372881356,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6663135593220338,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))P(sT −k; aT −(k+1))Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.6673728813559322,"T −k(IT −k, sT −k) −
X"
PROOF OF TECHNICAL RESULTS,0.6684322033898306,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6694915254237288,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))P(sT −k; a′
T −(k+1))Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.6705508474576272,"T −k (IT −k, sT −k) ## . (56)"
PROOF OF TECHNICAL RESULTS,0.6716101694915254,"= EsT −(k+1)∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.6726694915254238,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))R(sT −(k+1), aT −(k+1)) −
X"
PROOF OF TECHNICAL RESULTS,0.673728813559322,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))R(sT −(k+1), a′
T −(k+1)) + γ ""
X"
PROOF OF TECHNICAL RESULTS,0.6747881355932204,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6758474576271186,"aT −(k+1)∈A
π(aT −(k+1); sT −(k+1))P(sT −k; aT −(k+1))vπ,π2"
PROOF OF TECHNICAL RESULTS,0.676906779661017,"i,T −k(IT −k, sT −k) −
X"
PROOF OF TECHNICAL RESULTS,0.6779661016949152,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6790254237288136,"a′T −(k+1)∈A
π′(a′
T −(k+1); sT −(k+1))P(sT −k; a′
T −(k+1))vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.6800847457627118,"i,T −k(IT −k, sT −k) ## . (57)"
PROOF OF TECHNICAL RESULTS,0.6811440677966102,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.6822033898305084,"= EsT −(k+1)∼dθ
h
vπ,π2"
PROOF OF TECHNICAL RESULTS,0.6832627118644068,"i,T −(k+1)(IT −(k+1), sT −(k+1)) −vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.684322033898305,"T −(k+1)(IT −(k+1), si,T −(k+1))
i
,
(58)"
PROOF OF TECHNICAL RESULTS,0.6853813559322034,using the inductive hypothesis and where we have used the fact that
PROOF OF TECHNICAL RESULTS,0.6864406779661016,"EsT −(k+1)∼dθ ""
X"
PROOF OF TECHNICAL RESULTS,0.6875,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6885593220338984,aT −(k+1)∈A X
PROOF OF TECHNICAL RESULTS,0.6896186440677966,"aT −k∈A X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.690677966101695,"j=T
π(aT −(k+1); sT −(k+1))P(sT −k; aT −(k+1))π(aT −k; sT −k)c(Ij, Ij−1)δj
τk (59) −
X"
PROOF OF TECHNICAL RESULTS,0.6917372881355932,sT −k∈S X
PROOF OF TECHNICAL RESULTS,0.6927966101694916,a′T −(k+1)∈A X
PROOF OF TECHNICAL RESULTS,0.6938559322033898,"a′T −k∈A X k≥0 ∞
X"
PROOF OF TECHNICAL RESULTS,0.6949152542372882,"j=T
π′(a′
T −(k+1); sT −(k+1))P(sT −k; a′
T −(k+1))π′(a′
T −k; sT −k)c(Ij, Ij−1)δj
τk # (60) = K  
X"
PROOF OF TECHNICAL RESULTS,0.6959745762711864,"aT −k∈A
π(aT −k) −
X"
PROOF OF TECHNICAL RESULTS,0.6970338983050848,"a′T −k∈A
π′(a′
T −k) "
PROOF OF TECHNICAL RESULTS,0.698093220338983,"= 0,
(61)"
PROOF OF TECHNICAL RESULTS,0.6991525423728814,via similar reasoning as before and after which which we deduce the result in the ﬁnite case.
PROOF OF TECHNICAL RESULTS,0.7002118644067796,"For the inﬁnite horizon case, we must prove that there exists a measurable function B : Π × S →R
such that the following holds for any i ∈N and ∀π, π′
iπ′ ∈Π, ∀π−iπ′ ∈Π−i and ∀s ∈S:"
PROOF OF TECHNICAL RESULTS,0.701271186440678,"Es∼P
h
vπ,π2"
PROOF OF TECHNICAL RESULTS,0.7023305084745762,"i
−vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7033898305084746,"i

(z)
i
= Es∼P
h
Bπ,π2 −Bπ′,π2
(z)
i
.
(62)"
PROOF OF TECHNICAL RESULTS,0.7044491525423728,The result is proven by contradiction.
PROOF OF TECHNICAL RESULTS,0.7055084745762712,"To this end, let us ﬁrstly assume ∃c ̸= 0 such that"
PROOF OF TECHNICAL RESULTS,0.7065677966101694,"Es∼P
h
vπ,π2"
PROOF OF TECHNICAL RESULTS,0.7076271186440678,"i
−vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7086864406779662,"i

(z)
i
−Es∼P
h
Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.7097457627118644,"i
−Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7108050847457628,"i

(z)
i
= c."
PROOF OF TECHNICAL RESULTS,0.711864406779661,"Let us now deﬁne the following quantities for any s ∈S and for each ππ′ ∈Π and π−iπ′ ∈Π−i
and ∀i ∈N: vπ,π2"
PROOF OF TECHNICAL RESULTS,0.7129237288135594,"i,T ′ (z) := T ′
X"
PROOF OF TECHNICAL RESULTS,0.7139830508474576,"t=0
µ(s0)π(ai
0, s0)π−i(a−i
0 , s0) t−1
Y j=0 X"
PROOF OF TECHNICAL RESULTS,0.715042372881356,"sj+1∈S
γtP(sj+1; sj, aj)π(ai
j|sj)π−i(a−i
j |sj)Ri(zj, aj), and Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.7161016949152542,"T ′
(z) := T ′
X"
PROOF OF TECHNICAL RESULTS,0.7171610169491526,"t=0
µ(s0)π(ai
0, s0)π−i(a−i
0 , s0) t−1
Y j=0 X"
PROOF OF TECHNICAL RESULTS,0.7182203389830508,"sj+1∈S
P(sj+1; sj, aj) · π(ai
j|sj)π−i(a−i
j |sj)Θ(zj, aj),"
PROOF OF TECHNICAL RESULTS,0.7192796610169492,"so that the quantity vπ
i,T ′(s) measures the expected cumulative return until the point T ′ < ∞."
PROOF OF TECHNICAL RESULTS,0.7203389830508474,"Hence, we deduce that"
PROOF OF TECHNICAL RESULTS,0.7213983050847458,"vπ
i (z) ≡vπ
i,∞(z)"
PROOF OF TECHNICAL RESULTS,0.722457627118644,"= vπ
i,T ′(z) + γT ′µ(s0)π(ai
0, s0)π−i(a−i
0 , s0)"
PROOF OF TECHNICAL RESULTS,0.7235169491525424,"T ′−1
Y j=0 X"
PROOF OF TECHNICAL RESULTS,0.7245762711864406,"sj+1∈S
γtP(sj+1; sj, aj)π(ai
j|sj)π−i(a−i
j |sj)vπ
i (sT ′)."
PROOF OF TECHNICAL RESULTS,0.725635593220339,Next we observe that:
PROOF OF TECHNICAL RESULTS,0.7266949152542372,"c = Es∼P
h
vπ,π2"
PROOF OF TECHNICAL RESULTS,0.7277542372881356,"i
−vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7288135593220338,"i

(z)
i
−Es∼P
h
Bπ,π2 −Bπ′,π2
(z)
i"
PROOF OF TECHNICAL RESULTS,0.7298728813559322,"= Es∼P
h
vπ,π2"
PROOF OF TECHNICAL RESULTS,0.7309322033898306,"i,T ′ −vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7319915254237288,"i,T ′

(z)
i
−Es∼P
h
Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.7330508474576272,"T ′
−Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7341101694915254,"T ′

(s)
i"
PROOF OF TECHNICAL RESULTS,0.7351694915254238,"+γT ′EsT ′∼P """
PROOF OF TECHNICAL RESULTS,0.736228813559322,"µ(s0)π(ai
0, s0)π−i(a−i
0 , s0)"
PROOF OF TECHNICAL RESULTS,0.7372881355932204,"T ′−1
Y j=0 X"
PROOF OF TECHNICAL RESULTS,0.7383474576271186,"sj+1∈S
P(sj+1; sj, aj)π(ai
j|sj)π−i(a−i
j |sj)

vπ,π2"
PROOF OF TECHNICAL RESULTS,0.739406779661017,"i
(zT ′) −Bπ,π2(zT ′)
"
PROOF OF TECHNICAL RESULTS,0.7404661016949152,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.7415254237288136,"−µ(s0)π′
i(a′i
0 , s0)π−i(a−i
0 , s0)"
PROOF OF TECHNICAL RESULTS,0.7425847457627118,"T ′−1
Y j=0 X"
PROOF OF TECHNICAL RESULTS,0.7436440677966102,"sj+1∈S
P(sj+1; sj, a′
j)π′
i(a′i
j |sj)π−i(a−i
j |sj)

vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7447033898305084,"i
(zT ′) −Bπ′,π2(zT ′)
 # ."
PROOF OF TECHNICAL RESULTS,0.7457627118644068,"Considering the last expectation and its coefﬁcient and denoting the product by κ, using the fact
that by the Cauchy-Schwarz inequality we have ∥AX −BY ∥≤∥A∥∥X∥+ ∥B∥∥Y ∥, moreover
whenever A, B are non-expansive we have that ∥AX −BY ∥≤∥X∥+ ∥Y ∥, hence we observe
the following κ ≤∥κ∥≤2γT ′ (∥vi∥+ ∥B∥). Since we can choose T ′ freely and γ ∈]0, 1[, we can
choose T ′ to be sufﬁciently large so that γT ′ (∥vi∥+ ∥B∥) < 1"
PROOF OF TECHNICAL RESULTS,0.746822033898305,"4|c|. This then implies that
Es∼P"
PROOF OF TECHNICAL RESULTS,0.7478813559322034,""" 
vπ,π2"
PROOF OF TECHNICAL RESULTS,0.7489406779661016,"i,T ′ −vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.75,"i,T ′

(z) −

Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.7510593220338984,"T ′
−Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7521186440677966,"T ′

(z)"
PROOF OF TECHNICAL RESULTS,0.753177966101695,"# > 1 2c,"
PROOF OF TECHNICAL RESULTS,0.7542372881355932,which is a contradiction since we have proven that for any ﬁnite T ′ it is the case that Es∼P
PROOF OF TECHNICAL RESULTS,0.7552966101694916,""" 
vπ,π2"
PROOF OF TECHNICAL RESULTS,0.7563559322033898,"i,T ′ −vπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7574152542372882,"i,T ′

(z) −

Bπ,π2"
PROOF OF TECHNICAL RESULTS,0.7584745762711864,"T ′
−Bπ′,π2"
PROOF OF TECHNICAL RESULTS,0.7595338983050848,"T ′

(z) # = 0,"
PROOF OF TECHNICAL RESULTS,0.760593220338983,and hence we deduce the result in the inﬁnite horizon case.
PROOF OF TECHNICAL RESULTS,0.7616525423728814,PROOF OF PART III
PROOF OF TECHNICAL RESULTS,0.7627118644067796,"We begin by recalling that a Markov strategy is a policy πi : S × Ai →[0, 1] which requires as input
only the current system state (and not the game history or the other player’s action or strategy [22]).
With this, we give a formal description of the stable points of G in Markov strategies."
PROOF OF TECHNICAL RESULTS,0.763771186440678,"Deﬁnition 3 A policy proﬁle ˆπ = (ˆπ1, ˆπ2) ∈Π is a Markov perfect equilibrium (MPE) if the
following holds ∀i ̸= j ∈{1, 2}, ∀ˆπ′ ∈Πi: v(ˆπi,ˆπj)
i
(s0, I0) ≥v(ˆπ′,ˆπj)
i
(s0, I0), ∀(s0, I0) ∈
S × {0, 1}."
PROOF OF TECHNICAL RESULTS,0.7648305084745762,"The MPE describes a conﬁguration in policies in which no player can increase their payoff by
changing (unilaterally) their policy. Crucially, it deﬁnes the stable points to which independent
learners converge (if they converge at all)."
PROOF OF TECHNICAL RESULTS,0.7658898305084746,PROOF OF PROPOSITION 3
PROOF OF TECHNICAL RESULTS,0.7669491525423728,"Proof 7 We do the proof by contradiction. Let σ = (π, g) ∈arg sup
π′∈Π,g′ Bπ′,g′(s) for any s ∈S. Let us"
PROOF OF TECHNICAL RESULTS,0.7680084745762712,"now therefore assume that σ /∈NE{G}, hence there exists some other strategy proﬁle ˜σ = (g, ˜π)
for which Controller has a proﬁtable deviation where π′ ̸= π i.e. vπ′,π2
1
(s) > vπ,π2
1
(s) (using the
preservation of signs of integration). Prop. 4 however implies that Bπ′,π2(s) −Bπ,π2(s) > 0 which
is a contradiction since σ = (π, g) is a maximum of B. The proof can be straightforwardly adapted
to cover the case in which the deviating player is Shaper after which we deduce the desired result."
PROOF OF TECHNICAL RESULTS,0.7690677966101694,The last result completes the proof of Theorem 1.
PROOF OF TECHNICAL RESULTS,0.7701271186440678,PROOF OF PROPOSITION 2
PROOF OF TECHNICAL RESULTS,0.7711864406779662,"Proof 8 (Proof of Prop. 2) The proof is given by establishing a contradiction. Therefore suppose
that Mπ,π2ψ(sτk, I(τk)) ≤ψ(sτk, I(τk)) and suppose that the intervention time τ ′
1 > τ1 is an opti-
mal intervention time. Construct the Player 2 π′2 ∈Π2 and ˜π2 policy switching times by (τ ′
0, τ ′
1, . . . , )
and π′2 ∈Π2 policy by (τ ′
0, τ1, . . .) respectively. Deﬁne by l = inf{t > 0; Mπ,π2ψ(st, I0) =
ψ(st, I0)} and m = sup{t; t < τ ′
1}. By construction we have that"
PROOF OF TECHNICAL RESULTS,0.7722457627118644,"vπ1,π′2
2
(s, I0)"
PROOF OF TECHNICAL RESULTS,0.7733050847457628,"= E
h
R(s0, a0) + E
h
. . . + γl−1E
h
R(sτ1−1, aτ1−1) + . . . + γm−l−1E
h
R(sτ ′
1−1, aτ ′
1−1) + γMπ1,π′2vπ1,π′2
2
(s′, I(τ ′
1))
iiii"
PROOF OF TECHNICAL RESULTS,0.774364406779661,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.7754237288135594,"< E
h
R(s0, a0) + E
h
. . . + γl−1E
h
R(sτ1−1, aτ1−1) + γMπ1,˜π2vπ1,π′2
2
(sτ1, I(τ1))
iii"
PROOF OF TECHNICAL RESULTS,0.7764830508474576,"We now use the following observation E
h
R(sτ1−1, aτ1−1) + γMπ1,˜π2vπ1,π′2
2
(sτ1, I(τ1))
i"
PROOF OF TECHNICAL RESULTS,0.777542372881356,"≤max

Mπ1,˜π2vπ1,π′2
2
(sτ1, I(τ1)), max
aτ1∈A"
PROOF OF TECHNICAL RESULTS,0.7786016949152542,"h
R(sτk, aτk) + γ P"
PROOF OF TECHNICAL RESULTS,0.7796610169491526,"s′∈S P(s′; aτ1, sτ1)vπ1,π2
2
(s′, I(τ1))
i
."
PROOF OF TECHNICAL RESULTS,0.7807203389830508,Using this we deduce that
PROOF OF TECHNICAL RESULTS,0.7817796610169492,"vπ1,π′2
2
(s, I0) ≤E """
PROOF OF TECHNICAL RESULTS,0.7828389830508474,"R(s0, a0) + E "" . . ."
PROOF OF TECHNICAL RESULTS,0.7838983050847458,"+ γl−1E """
PROOF OF TECHNICAL RESULTS,0.784957627118644,"R(sτ1−1, aτ1−1) + γ max ("
PROOF OF TECHNICAL RESULTS,0.7860169491525424,"Mπ1,˜π2vπ1,π′2
2
(sτ1, I(τ1)), max
aτ1∈A """
PROOF OF TECHNICAL RESULTS,0.7870762711864406,"R(sτk, aτk) + γ
X"
PROOF OF TECHNICAL RESULTS,0.788135593220339,"s′∈S
P(s′; aτ1, sτ1)vπ1,π2
2
(s′, I(τ1"
PROOF OF TECHNICAL RESULTS,0.7891949152542372,"= E
h
R(s0, a0) + E
h
. . . + γl−1E
h
R(sτ1−1, aτ1−1) + γ
h
Tvπ1,˜π2
2
i
(sτ1, I(τ1))
iii
= vπ1,˜π2
2
(s, I0)),"
PROOF OF TECHNICAL RESULTS,0.7902542372881356,"where the ﬁrst inequality is true by assumption on M. This is a contradiction since π′2 is an optimal
policy for Player 2. Using analogous reasoning, we deduce the same result for τ ′
k < τk after which
deduce the result. Moreover, by invoking the same reasoning, we can conclude that it must be the
case that (τ0, τ1, . . . , τk−1, τk, τk+1, . . . , ) are the optimal switching times."
PROOF OF TECHNICAL RESULTS,0.7913135593220338,PROOF OF THEOREM 2
PROOF OF TECHNICAL RESULTS,0.7923728813559322,"To prove the theorem, we make use of the following result:"
PROOF OF TECHNICAL RESULTS,0.7934322033898306,"Theorem 3 (Theorem 1, pg 4 in [18]) Let Ξt(s) be a random process that takes values in Rn and
given by the following:"
PROOF OF TECHNICAL RESULTS,0.7944915254237288,"Ξt+1(s) = (1 −αt(s)) Ξt(s)αt(s)Lt(s),
(63)"
PROOF OF TECHNICAL RESULTS,0.7955508474576272,then Ξt(s) converges to 0 with probability 1 under the following conditions:
PROOF OF TECHNICAL RESULTS,0.7966101694915254,"i) 0 ≤αt ≤1, P"
PROOF OF TECHNICAL RESULTS,0.7976694915254238,t αt = ∞and P
PROOF OF TECHNICAL RESULTS,0.798728813559322,t αt < ∞
PROOF OF TECHNICAL RESULTS,0.7997881355932204,"ii) ∥E[Lt|Ft]∥≤γ∥Ξt∥, with γ < 1;"
PROOF OF TECHNICAL RESULTS,0.8008474576271186,iii) Var [Lt|Ft] ≤c(1 + ∥Ξt∥2) for some c > 0.
PROOF OF TECHNICAL RESULTS,0.801906779661017,"Proof 9 To prove the result, we show (i) - (iii) hold. Condition (i) holds by choice of learning rate.
It therefore remains to prove (ii) - (iii). We ﬁrst prove (ii). For this, we consider our variant of the
Q-learning update rule:"
PROOF OF TECHNICAL RESULTS,0.8029661016949152,"Qt+1(st, It, at) = Qt(st, It, at)"
PROOF OF TECHNICAL RESULTS,0.8040254237288136,"+ αt(st, It, at)

max

Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + γmax
a′∈A Q(s′, Iτk, a′)

−Qt(st, It, at)

."
PROOF OF TECHNICAL RESULTS,0.8050847457627118,"After subtracting Q⋆(st, It, at) from both sides and some manipulation we obtain that:"
PROOF OF TECHNICAL RESULTS,0.8061440677966102,"Ξt+1(st, It, at)
= (1 −αt(st, It, at))Ξt(st, It, at)"
PROOF OF TECHNICAL RESULTS,0.8072033898305084,"+ αt(st, It, at))

max

Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + γmax
a′∈A Q(s′, Iτk, a′)

−Q⋆(st, It, at)

,"
PROOF OF TECHNICAL RESULTS,0.8082627118644068,"where Ξt(st, It, at) := Qt(st, It, at) −Q⋆(st, It, at)."
PROOF OF TECHNICAL RESULTS,0.809322033898305,Let us now deﬁne by
PROOF OF TECHNICAL RESULTS,0.8103813559322034,"Lt(sτk, Iτk, a) := max

Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + γmax
a′∈A Q(s′, Iτk, a′)

−Q⋆(st, It, a)."
PROOF OF TECHNICAL RESULTS,0.8114406779661016,Under review as a conference paper at ICLR 2022 Then
PROOF OF TECHNICAL RESULTS,0.8125,"Ξt+1(st, It, at) = (1 −αt(st, It, at))Ξt(st, It, at) + αt(st, It, at)) [Lt(sτk, a)] .
(64)"
PROOF OF TECHNICAL RESULTS,0.8135593220338984,We now observe that
PROOF OF TECHNICAL RESULTS,0.8146186440677966,"E [Lt(sτk, Iτk, a)|Ft] =
X"
PROOF OF TECHNICAL RESULTS,0.815677966101695,"s′∈S
P(s′; a, sτk) max

Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + γmax
a′∈A Q(s′, Iτk, a′)

−Q⋆(sτk, a)"
PROOF OF TECHNICAL RESULTS,0.8167372881355932,"= TφQt(s, Iτk, a) −Q⋆(s, Iτk, a).
(65)"
PROOF OF TECHNICAL RESULTS,0.8177966101694916,"Now, using the ﬁxed point property that implies Q⋆= TφQ⋆, we ﬁnd that"
PROOF OF TECHNICAL RESULTS,0.8188559322033898,"E [Lt(sτk, Iτk, a)|Ft] = TφQt(s, Iτk, a) −TφQ⋆(s, Iτk, a)
≤∥TφQt −TφQ⋆∥
≤γ ∥Qt −Q⋆∥∞= γ ∥Ξt∥∞.
(66)"
PROOF OF TECHNICAL RESULTS,0.8199152542372882,using the contraction property of T established in Lemma 3. This proves (ii).
PROOF OF TECHNICAL RESULTS,0.8209745762711864,"We now prove iii), that is"
PROOF OF TECHNICAL RESULTS,0.8220338983050848,"Var [Lt|Ft] ≤c(1 + ∥Ξt∥2).
(67)"
PROOF OF TECHNICAL RESULTS,0.823093220338983,Now by (65) we have that
PROOF OF TECHNICAL RESULTS,0.8241525423728814,"Var [Lt|Ft] = Var

max

Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + γmax
a′∈A Q(s′, Iτk, a′)

−Q⋆(st, It, a)
 = E """
PROOF OF TECHNICAL RESULTS,0.8252118644067796,"max

Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + γmax
a′∈A Q(s′, Iτk, a′)
"
PROOF OF TECHNICAL RESULTS,0.826271186440678,"−Q⋆(st, It, a) −(TΦQt(s, Iτk, a) −Q⋆(s, Iτk, a)) !2# = E"
PROOF OF TECHNICAL RESULTS,0.8273305084745762,"""
max

Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + γmax
a′∈A Q(s′, Iτk, a′)

−TΦQt(s, Iτk, a)
2#"
PROOF OF TECHNICAL RESULTS,0.8283898305084746,"= Var

max

Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + γmax
a′∈A Q(s′, Iτk, a′)

−TΦQt(s, Iτk, a))2
"
PROOF OF TECHNICAL RESULTS,0.8294491525423728,"≤c(1 + ∥Ξt∥2),"
PROOF OF TECHNICAL RESULTS,0.8305084745762712,"for some c > 0 where the last line follows due to the boundedness of Q (which follows from
Assumptions 2 and 4). This concludes the proof of the Theorem."
PROOF OF TECHNICAL RESULTS,0.8315677966101694,PROOF OF CONVERGENCE WITH LINEAR FUNCTION APPROXIMATION
PROOF OF TECHNICAL RESULTS,0.8326271186440678,First let us recall the statement of the theorem:
PROOF OF TECHNICAL RESULTS,0.8336864406779662,Theorem 3 ROSA converges to a limit point r⋆which is the unique solution to the equation:
PROOF OF TECHNICAL RESULTS,0.8347457627118644,"ΠF(Φr⋆) = Φr⋆,
a.e.
(68)"
PROOF OF TECHNICAL RESULTS,0.8358050847457628,"where we recall that for any test function Λ ∈V, the operator F is deﬁned by FΛ := Θ +
γP max{MΛ, Λ}."
PROOF OF TECHNICAL RESULTS,0.836864406779661,"Moreover, r⋆satisﬁes the following:"
PROOF OF TECHNICAL RESULTS,0.8379237288135594,"∥Φr⋆−Q⋆∥≤c ∥ΠQ⋆−Q⋆∥.
(69)"
PROOF OF TECHNICAL RESULTS,0.8389830508474576,"The theorem is proven using a set of results that we now establish. To this end, we ﬁrst wish to prove
the following bound:"
PROOF OF TECHNICAL RESULTS,0.840042372881356,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.8411016949152542,Lemma 4 For any Q ∈V we have that
PROOF OF TECHNICAL RESULTS,0.8421610169491526,"∥FQ −Q′∥≤γ ∥Q −Q′∥,
(70)"
PROOF OF TECHNICAL RESULTS,0.8432203389830508,so that the operator F is a contraction.
PROOF OF TECHNICAL RESULTS,0.8442796610169492,"Proof 10 Recall, for any test function ψ , a projection operator Π acting Λ is deﬁned by the following"
PROOF OF TECHNICAL RESULTS,0.8453389830508474,"ΠΛ :=
arg min
¯Λ∈{Φr|r∈Rp}"
PROOF OF TECHNICAL RESULTS,0.8463983050847458,"¯Λ −Λ
 ."
PROOF OF TECHNICAL RESULTS,0.847457627118644,"Now, we ﬁrst note that in the proof of Lemma 3, we deduced that for any Λ ∈L2 we have that
MΛ −

ψ(·, a) + γmax
a∈A PaΛ′
 ≤γ ∥Λ −Λ′∥,"
PROOF OF TECHNICAL RESULTS,0.8485169491525424,(c.f. Lemma 3).
PROOF OF TECHNICAL RESULTS,0.8495762711864406,"Setting Λ = Q and ψ = Θ, it can be straightforwardly deduced that for any Q, ˆQ ∈L2:
MQ −ˆQ
 ≤γ
Q −ˆQ
. Hence, using the contraction property of M, we readily deduce
the following bound:"
PROOF OF TECHNICAL RESULTS,0.850635593220339,"max
nMQ −ˆQ
 ,
MQ −M ˆQ

o
≤γ
Q −ˆQ
 ,
(71)"
PROOF OF TECHNICAL RESULTS,0.8516949152542372,"We now observe that F is a contraction. Indeed, since for any Q, Q′ ∈L2 we have that:"
PROOF OF TECHNICAL RESULTS,0.8527542372881356,"∥FQ −FQ′∥= ∥Θ + γP max{MQ, Q} −(Θ + γP max{MQ′, Q′})∥"
PROOF OF TECHNICAL RESULTS,0.8538135593220338,"= γ ∥P max{MQ, Q} −P max{MQ′, Q′}∥"
PROOF OF TECHNICAL RESULTS,0.8548728813559322,"≤γ ∥max{MQ, Q} −max{MQ′, Q′}∥"
PROOF OF TECHNICAL RESULTS,0.8559322033898306,"≤γ ∥max{MQ −MQ′, Q −MQ′, MQ −Q′, Q −Q′}∥"
PROOF OF TECHNICAL RESULTS,0.8569915254237288,"≤γ max{∥MQ −MQ′∥, ∥Q −MQ′∥, ∥MQ −Q′∥, ∥Q −Q′∥}"
PROOF OF TECHNICAL RESULTS,0.8580508474576272,"= γ ∥Q −Q′∥,"
PROOF OF TECHNICAL RESULTS,0.8591101694915254,using (71) and again using the non-expansiveness of P.
PROOF OF TECHNICAL RESULTS,0.8601694915254238,We next show that the following two bounds hold:
PROOF OF TECHNICAL RESULTS,0.861228813559322,Lemma 5 For any Q ∈V we have that i)
PROOF OF TECHNICAL RESULTS,0.8622881355932204,"ΠFQ −ΠF ¯Q
 ≤γ
Q −¯Q
,"
PROOF OF TECHNICAL RESULTS,0.8633474576271186,"ii)
∥Φr⋆−Q⋆∥≤
1
√"
PROOF OF TECHNICAL RESULTS,0.864406779661017,1−γ2 ∥ΠQ⋆−Q⋆∥.
PROOF OF TECHNICAL RESULTS,0.8654661016949152,"Proof 11 The ﬁrst result is straightforward since as Π is a projection it is non-expansive and hence:
ΠFQ −ΠF ¯Q
 ≤
FQ −F ¯Q
 ≤γ
Q −¯Q
 ,"
PROOF OF TECHNICAL RESULTS,0.8665254237288136,"using the contraction property of F. This proves i). For ii), we note that by the orthogonality property
of projections we have that ⟨Φr⋆−ΠQ⋆, Φr⋆−ΠQ⋆⟩, hence we observe that:"
PROOF OF TECHNICAL RESULTS,0.8675847457627118,∥Φr⋆−Q⋆∥2 = ∥Φr⋆−ΠQ⋆∥2 + ∥Φr⋆−ΠQ⋆∥2
PROOF OF TECHNICAL RESULTS,0.8686440677966102,= ∥ΠFΦr⋆−ΠQ⋆∥2 + ∥Φr⋆−ΠQ⋆∥2
PROOF OF TECHNICAL RESULTS,0.8697033898305084,≤∥FΦr⋆−Q⋆∥2 + ∥Φr⋆−ΠQ⋆∥2
PROOF OF TECHNICAL RESULTS,0.8707627118644068,= ∥FΦr⋆−FQ⋆∥2 + ∥Φr⋆−ΠQ⋆∥2
PROOF OF TECHNICAL RESULTS,0.871822033898305,"≤γ2 ∥Φr⋆−Q⋆∥2 + ∥Φr⋆−ΠQ⋆∥2 ,"
PROOF OF TECHNICAL RESULTS,0.8728813559322034,after which we readily deduce the desired result.
PROOF OF TECHNICAL RESULTS,0.8739406779661016,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.875,"Lemma 6 Deﬁne the operator H by the following: HQ(z) =
MQ(z),
if MQ(z) > Φr⋆,
Q(z),
otherwise,
and ˜F by: ˜FQ := Θ + γPHQ."
PROOF OF TECHNICAL RESULTS,0.8760593220338984,"For any Q, ¯Q ∈L2 we have that
˜FQ −˜F ¯Q
 ≤γ
Q −¯Q

(72)"
PROOF OF TECHNICAL RESULTS,0.8771186440677966,and hence ˜F is a contraction mapping.
PROOF OF TECHNICAL RESULTS,0.878177966101695,"Proof 12 Using (71), we now observe that
˜FQ −˜F ¯Q
 =
Θ + γPHQ −
 
Θ + γPH ¯Q
"
PROOF OF TECHNICAL RESULTS,0.8792372881355932,"≤γ
HQ −H ¯Q"
PROOF OF TECHNICAL RESULTS,0.8802966101694916,"≤γ
max

MQ −M ¯Q, Q −¯Q, MQ −¯Q, M ¯Q −Q"
PROOF OF TECHNICAL RESULTS,0.8813559322033898,"≤γ max
MQ −M ¯Q
 ,
Q −¯Q
 ,
MQ −¯Q
 ,
M ¯Q −Q"
PROOF OF TECHNICAL RESULTS,0.8824152542372882,"≤γ max

γ
Q −¯Q
 ,
Q −¯Q
 ,
MQ −¯Q
 ,
M ¯Q −Q"
PROOF OF TECHNICAL RESULTS,0.8834745762711864,"= γ
Q −¯Q
 ,"
PROOF OF TECHNICAL RESULTS,0.8845338983050848,again using the non-expansive property of P.
PROOF OF TECHNICAL RESULTS,0.885593220338983,Lemma 7 Deﬁne by ˜Q := Θ + γPv˜π where
PROOF OF TECHNICAL RESULTS,0.8866525423728814,"v˜π(z) := Θ(sτk, a) + γmax
a∈A X"
PROOF OF TECHNICAL RESULTS,0.8877118644067796,"s′∈S
P(s′; a, sτk)Φr⋆(s′, I(τk)),
(73)"
PROOF OF TECHNICAL RESULTS,0.888771186440678,"then ˜Q is a ﬁxed point of ˜F ˜Q, that is ˜F ˜Q = ˜Q."
PROOF OF TECHNICAL RESULTS,0.8898305084745762,Proof 13 We begin by observing that
PROOF OF TECHNICAL RESULTS,0.8908898305084746,"H ˜Q(z) = H
 
Θ(z) + γPv˜π"
PROOF OF TECHNICAL RESULTS,0.8919491525423728,"=
MQ(z),
if MQ(z) > Φr⋆,
Q(z),
otherwise,"
PROOF OF TECHNICAL RESULTS,0.8930084745762712,"=
MQ(z),
if MQ(z) > Φr⋆,
Θ(z) + γPv˜π,
otherwise,"
PROOF OF TECHNICAL RESULTS,0.8940677966101694,= v˜π(z).
PROOF OF TECHNICAL RESULTS,0.8951271186440678,"Hence,
˜F ˜Q = Θ + γPH ˜Q = Θ + γPv˜π = ˜Q.
(74)"
PROOF OF TECHNICAL RESULTS,0.8961864406779662,which proves the result.
PROOF OF TECHNICAL RESULTS,0.8972457627118644,Lemma 8 The following bound holds:
PROOF OF TECHNICAL RESULTS,0.8983050847457628,"E

vˆπ(z0)

−E

v˜π(z0)

≤2
h
(1 −γ)
p"
PROOF OF TECHNICAL RESULTS,0.899364406779661,"(1 −γ2)
i−1
∥ΠQ⋆−Q⋆∥.
(75)"
PROOF OF TECHNICAL RESULTS,0.9004237288135594,"Proof 14 By deﬁnitions of vˆπ and v˜π (c.f (73)) and using Jensen’s inequality and the stationarity
property we have that,"
PROOF OF TECHNICAL RESULTS,0.9014830508474576,"E

vˆπ(z0)

−E

v˜π(z0)

= E

Pvˆπ(z0)

−E

Pv˜π(z0)
"
PROOF OF TECHNICAL RESULTS,0.902542372881356,"≤
E

Pvˆπ(z0)

−E

Pv˜π(z0)
"
PROOF OF TECHNICAL RESULTS,0.9036016949152542,"≤
Pvˆπ −Pv˜π .
(76)"
PROOF OF TECHNICAL RESULTS,0.9046610169491526,"Now recall that ˜Q := Θ + γPv˜π and Q⋆:= Θ + γPvπ⋆, using these expressions in (76) we ﬁnd
that"
PROOF OF TECHNICAL RESULTS,0.9057203389830508,"E

vˆπ(z0)

−E

v˜π(z0)

≤1 γ"
PROOF OF TECHNICAL RESULTS,0.9067796610169492,"˜Q −Q⋆ ."
PROOF OF TECHNICAL RESULTS,0.9078389830508474,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.9088983050847458,"Moreover, by the triangle inequality and using the fact that F(Φr⋆) = ˜F(Φr⋆) and that FQ⋆= Q⋆"
PROOF OF TECHNICAL RESULTS,0.909957627118644,"and F ˜Q = ˜Q (c.f. (75)) we have that
 ˜Q −Q⋆ ≤
 ˜Q −F(Φr⋆)
 +
Q⋆−˜F(Φr⋆)"
PROOF OF TECHNICAL RESULTS,0.9110169491525424,"≤γ
 ˜Q −Φr⋆ + γ ∥Q⋆−Φr⋆∥"
PROOF OF TECHNICAL RESULTS,0.9120762711864406,"≤2γ
 ˜Q −Φr⋆ + γ
Q⋆−˜Q
 ,"
PROOF OF TECHNICAL RESULTS,0.913135593220339,"which gives the following bound:
 ˜Q −Q⋆ ≤2 (1 −γ)−1  ˜Q −Φr⋆ ,"
PROOF OF TECHNICAL RESULTS,0.9141949152542372,"from which, using Lemma 5, we deduce that
 ˜Q −Q⋆ ≤2
h
(1 −γ)
p"
PROOF OF TECHNICAL RESULTS,0.9152542372881356,"(1 −γ2)
i−1  ˜Q −Φr⋆,
after which by (77), we ﬁnally obtain"
PROOF OF TECHNICAL RESULTS,0.9163135593220338,"E

vˆπ(z0)

−E

v˜π(z0)

≤2
h
(1 −γ)
p"
PROOF OF TECHNICAL RESULTS,0.9173728813559322,"(1 −γ2)
i−1  ˜Q −Φr⋆ ,"
PROOF OF TECHNICAL RESULTS,0.9184322033898306,as required.
PROOF OF TECHNICAL RESULTS,0.9194915254237288,Let us rewrite the update in the following way:
PROOF OF TECHNICAL RESULTS,0.9205508474576272,"rt+1 = rt + γtΞ(wt, rt),"
PROOF OF TECHNICAL RESULTS,0.9216101694915254,where the function Ξ : R2d × Rp →Rp is given by:
PROOF OF TECHNICAL RESULTS,0.9226694915254238,"Ξ(w, r) := φ(z) (Θ(z) + γ max {(Φr)(z′), M(Φr)(z′)} −(Φr)(z)) ,"
PROOF OF TECHNICAL RESULTS,0.923728813559322,"for any w ≡(z, z′) ∈(N × S)2 where z = (t, s) ∈N × S and z′ = (t, s′) ∈N × S and for any
r ∈Rp. Let us also deﬁne the function Ξ : Rp →Rp by the following:"
PROOF OF TECHNICAL RESULTS,0.9247881355932204,"Ξ(r) := Ew0∼(P,P) [Ξ(w0, r)] ; w0 := (z0, z1)."
PROOF OF TECHNICAL RESULTS,0.9258474576271186,"Lemma 9 The following statements hold for all z ∈{0, 1} × S:"
PROOF OF TECHNICAL RESULTS,0.926906779661017,"i) (r −r⋆)Ξk(r) < 0,
∀r ̸= r⋆,"
PROOF OF TECHNICAL RESULTS,0.9279661016949152,ii) Ξk(r⋆) = 0.
PROOF OF TECHNICAL RESULTS,0.9290254237288136,"Proof 15 To prove the statement, we ﬁrst note that each component of Ξk(r) admits a representation
as an inner product, indeed:"
PROOF OF TECHNICAL RESULTS,0.9300847457627118,"Ξk(r) = E [φk(z0)(Θ(z0) + γ max {Φr(z1), MΦ(z1)} −(Φr)(z0)]
= E [φk(z0)(Θ(z0) + γE [max {Φr(z1), MΦ(z1)} |z0] −(Φr)(z0)]
= E [φk(z0)(Θ(z0) + γP max {(Φr, MΦ)} (z0) −(Φr)(z0)]
= ⟨φk, FΦr −Φr⟩,"
PROOF OF TECHNICAL RESULTS,0.9311440677966102,using the iterated law of expectations and the deﬁnitions of P and F.
PROOF OF TECHNICAL RESULTS,0.9322033898305084,"We now are in position to prove i). Indeed, we now observe the following:"
PROOF OF TECHNICAL RESULTS,0.9332627118644068,"(r −r⋆) Ξk(r) =
X"
PROOF OF TECHNICAL RESULTS,0.934322033898305,"l=1
(r(l) −r⋆(l)) ⟨φl, FΦr −Φr⟩"
PROOF OF TECHNICAL RESULTS,0.9353813559322034,"= ⟨Φr −Φr⋆, FΦr −Φr⟩
= ⟨Φr −Φr⋆, (1 −Π)FΦr + ΠFΦr −Φr⟩
= ⟨Φr −Φr⋆, ΠFΦr −Φr⟩,"
PROOF OF TECHNICAL RESULTS,0.9364406779661016,"where in the last step we used the orthogonality of (1 −Π). We now recall that ΠFΦr⋆= Φr⋆
since Φr⋆is a ﬁxed point of ΠF. Additionally, using Lemma 5 we observe that ∥ΠFΦr −Φr⋆∥≤
γ∥Φr −Φr⋆∥. With this we now ﬁnd that"
PROOF OF TECHNICAL RESULTS,0.9375,"⟨Φr −Φr⋆, ΠFΦr −Φr⟩"
PROOF OF TECHNICAL RESULTS,0.9385593220338984,Under review as a conference paper at ICLR 2022
PROOF OF TECHNICAL RESULTS,0.9396186440677966,"= ⟨Φr −Φr⋆, (ΠFΦr −Φr⋆) + Φr⋆−Φr⟩"
PROOF OF TECHNICAL RESULTS,0.940677966101695,≤∥Φr −Φr⋆∥∥ΠFΦr −Φr⋆∥−∥Φr⋆−Φr∥2
PROOF OF TECHNICAL RESULTS,0.9417372881355932,"≤(γ −1) ∥Φr⋆−Φr∥2 ,"
PROOF OF TECHNICAL RESULTS,0.9427966101694916,which is negative since γ < 1 which completes the proof of part i).
PROOF OF TECHNICAL RESULTS,0.9438559322033898,The proof of part ii) is straightforward since we readily observe that
PROOF OF TECHNICAL RESULTS,0.9449152542372882,"Ξk(r⋆) = ⟨φl, FΦr⋆−Φr⟩= ⟨φl, ΠFΦr⋆−Φr⟩= 0,"
PROOF OF TECHNICAL RESULTS,0.9459745762711864,as required and from which we deduce the result.
PROOF OF TECHNICAL RESULTS,0.9470338983050848,"To prove the theorem, we make use of a special case of the following result:"
PROOF OF TECHNICAL RESULTS,0.948093220338983,"Theorem 4 (Th. 17, p. 239 in [3]) Consider a stochastic process rt : R × {∞} × Ω→Rk which
takes an initial value r0 and evolves according to the following:"
PROOF OF TECHNICAL RESULTS,0.9491525423728814,"rt+1 = rt + αΞ(st, rt),
(77)"
PROOF OF TECHNICAL RESULTS,0.9502118644067796,for some function s : R2d × Rk →Rk and where the following statements hold:
PROOF OF TECHNICAL RESULTS,0.951271186440678,"1. {st|t = 0, 1, . . .} is a stationary, ergodic Markov process taking values in R2d"
PROOF OF TECHNICAL RESULTS,0.9523305084745762,"2. For any positive scalar q, there exists a scalar µq such that E [1 + ∥st∥q|s ≡s0] ≤
µq (1 + ∥s∥q)"
PROOF OF TECHNICAL RESULTS,0.9533898305084746,"3. The step size sequence satisﬁes the Robbins-Monro conditions, that is P∞
t=0 αt = ∞and
P∞
t=0 α2
t < ∞"
PROOF OF TECHNICAL RESULTS,0.9544491525423728,"4. There exists scalars c and q such that ∥Ξ(w, r)∥≤c (1 + ∥w∥q) (1 + ∥r∥)"
PROOF OF TECHNICAL RESULTS,0.9555084745762712,"5. There exists scalars c and q such that P∞
t=0 ∥E [Ξ(wt, r)|z0 ≡z] −E [Ξ(w0, r)]∥≤
c (1 + ∥w∥q) (1 + ∥r∥)"
PROOF OF TECHNICAL RESULTS,0.9565677966101694,"6. There exists a scalar c > 0 such that ∥E[Ξ(w0, r)] −E[Ξ(w0, ¯r)]∥≤c∥r −¯r∥"
"THERE
EXISTS
SCALARS
C",0.9576271186440678,"7. There
exists
scalars
c
>
0
and
q
>
0
such
that
P∞
t=0 ∥E [Ξ(wt, r)|w0 ≡w] −E [Ξ(w0, ¯r)]∥≤c∥r −¯r∥(1 + ∥w∥q)"
"THERE
EXISTS
SCALARS
C",0.9586864406779662,8. There exists some r⋆∈Rk such that Ξ(r)(r −r⋆) < 0 for all r ̸= r⋆and ¯s(r⋆) = 0.
"THERE
EXISTS
SCALARS
C",0.9597457627118644,Then rt converges to r⋆almost surely.
"THERE
EXISTS
SCALARS
C",0.9608050847457628,"In order to apply the Theorem 4, we show that conditions 1 - 7 are satisﬁed."
"THERE
EXISTS
SCALARS
C",0.961864406779661,"Proof 16 Conditions 1-2 are true by assumption while condition 3 can be made true by choice of the
learning rates. Therefore it remains to verify conditions 4-7 are met."
"THERE
EXISTS
SCALARS
C",0.9629237288135594,"To prove 4, we observe that"
"THERE
EXISTS
SCALARS
C",0.9639830508474576,"∥Ξ(w, r)∥= ∥φ(z) (Θ(z) + γ max {(Φr)(z′), MΦ(z′)} −(Φr)(z))∥"
"THERE
EXISTS
SCALARS
C",0.965042372881356,≤∥φ(z)∥∥Θ(z) + γ (∥φ(z′)∥∥r∥+ MΦ(z′))∥+ ∥φ(z)∥∥r∥
"THERE
EXISTS
SCALARS
C",0.9661016949152542,≤∥φ(z)∥(∥Θ(z)∥+ γ∥MΦ(z′)∥) + ∥φ(z)∥(γ ∥φ(z′)∥+ ∥φ(z)∥) ∥r∥.
"THERE
EXISTS
SCALARS
C",0.9671610169491526,"Now using the deﬁnition of M, we readily observe that ∥MΦ(z′)∥≤∥Θ∥+ γ∥Pπ
s′stΦ∥≤∥Θ∥+
γ∥Φ∥using the non-expansiveness of P."
"THERE
EXISTS
SCALARS
C",0.9682203389830508,"Hence, we lastly deduce that"
"THERE
EXISTS
SCALARS
C",0.9692796610169492,"∥Ξ(w, r)∥≤∥φ(z)∥(∥Θ(z)∥+ γ∥MΦ(z′)∥) + ∥φ(z)∥(γ ∥φ(z′)∥+ ∥φ(z)∥) ∥r∥"
"THERE
EXISTS
SCALARS
C",0.9703389830508474,"≤∥φ(z)∥(∥Θ(z)∥+ γ∥Θ∥+ γ∥ψ∥) + ∥φ(z)∥(γ ∥φ(z′)∥+ ∥φ(z)∥) ∥r∥,"
"THERE
EXISTS
SCALARS
C",0.9713983050847458,"we then easily deduce the result using the boundedness of φ, Θ and ψ."
"THERE
EXISTS
SCALARS
C",0.972457627118644,Under review as a conference paper at ICLR 2022
"THERE
EXISTS
SCALARS
C",0.9735169491525424,Now we observe the following Lipschitz condition on Ξ:
"THERE
EXISTS
SCALARS
C",0.9745762711864406,"∥Ξ(w, r) −Ξ(w, ¯r)∥"
"THERE
EXISTS
SCALARS
C",0.975635593220339,"= ∥φ(z) (γ max {(Φr)(z′), MΦ(z′)} −γ max {(Φ¯r)(z′), MΦ(z′)}) −((Φr)(z) −Φ¯r(z))∥"
"THERE
EXISTS
SCALARS
C",0.9766949152542372,"≤γ ∥φ(z)∥∥max {φ′(z′)r, MΦ′(z′)} −max {(φ′(z′)¯r), MΦ′(z′)}∥+ ∥φ(z)∥∥φ′(z)r −φ(z)¯r∥"
"THERE
EXISTS
SCALARS
C",0.9777542372881356,≤γ ∥φ(z)∥∥φ′(z′)r −φ′(z′)¯r∥+ ∥φ(z)∥∥φ′(z)r −φ′(z)¯r∥
"THERE
EXISTS
SCALARS
C",0.9788135593220338,"≤∥φ(z)∥(∥φ(z)∥+ γ ∥φ(z)∥∥φ′(z′) −φ′(z′)∥) ∥r −¯r∥
≤c ∥r −¯r∥,"
"THERE
EXISTS
SCALARS
C",0.9798728813559322,"using
Cauchy-Schwarz
inequality
and
that
for
any
scalars
a, b, c
we
have
that
|max{a, b} −max{b, c}| ≤|a −c|."
"THERE
EXISTS
SCALARS
C",0.9809322033898306,"Using Assumptions 3 and 4, we therefore deduce that ∞
X"
"THERE
EXISTS
SCALARS
C",0.9819915254237288,"t=0
∥E [Ξ(w, r) −Ξ(w, ¯r)|w0 = w] −E [Ξ(w0, r) −Ξ(w0, ¯r)∥] ≤c ∥r −¯r∥(1 + ∥w∥l).
(78)"
"THERE
EXISTS
SCALARS
C",0.9830508474576272,"Part 2 is assured by Lemma 5 while Part 4 is assured by Lemma 8 and lastly Part 8 is assured by
Lemma 9."
"THERE
EXISTS
SCALARS
C",0.9841101694915254,Under review as a conference paper at ICLR 2022
"THERE
EXISTS
SCALARS
C",0.9851694915254238,"2
4
6
8
10
Steps (x 10000) 0.0 0.2 0.4 0.6 0.8 1.0"
"THERE
EXISTS
SCALARS
C",0.986228813559322,Return
"THERE
EXISTS
SCALARS
C",0.9872881355932204,"ROSA (L = RND)
ROSA (L = Count-based)"
"THERE
EXISTS
SCALARS
C",0.9883474576271186,"Figure 8: ROSA is robust to the component used to generate exploration bonus L. ROSA works
equally well when we RND or Count-based method forL."
"THERE
EXISTS
SCALARS
C",0.989406779661017,Rebuttal
"THERE
EXISTS
SCALARS
C",0.9904661016949152,"17
ADDITIONAL EXPERIMENT 1 - REPLACING RND FOR BONUS REWARD"
"THERE
EXISTS
SCALARS
C",0.9915254237288136,"In this experiment we sought to ascertain if ROSA is robust to the component used to generate
exploration bonus L. We ran two versions of ROSA, one where L is computed using RND and
one where L is computed using a simple count-based measure
1
Count(s) where s ∈S (the function
Count(s) simply tallies the number of times state s has been visited). Figure 8 shows performance of
these two versions of ROSA on the Maze environment shown in Figure 1. ROSA performs equally
well with both components, indicating that it is not dependent on a ﬁne-tuned exploration bonus.
The additional machinery of switching controls and choices of intrinsic rewards to add mean that
ROSA can work with basic exploration bonuses. Note that we generally use RND since it is simple
to implement and works equally well on discrete and continuous state spaces."
"THERE
EXISTS
SCALARS
C",0.9925847457627118,"18
ADDITIONAL EXPERIMENT 2 - ROBUSTNESS TO INITIALISATION OF φ"
"THERE
EXISTS
SCALARS
C",0.9936440677966102,"Some reviewers raised important questions about the requirement of a carefully initialised phi
function. Here, we show performance of individual runs of ROSA on the Maze shown in Figure 1. In
each run, φ is randomly initialised using Xavier initialisation, and as a consequence ROSA works
with a different φ function in each run. As can be seen in Figure 9, despite the randomness in φ the
performance of ROSA does not vary signiﬁcantly over the runs. This suggests that ROSA can adapt
to the φ function it is presented with, and still come up with good reward shaping."
"THERE
EXISTS
SCALARS
C",0.9947033898305084,Under review as a conference paper at ICLR 2022
"THERE
EXISTS
SCALARS
C",0.9957627118644068,"2
4
6
8
10
Steps (x 10000) 0.0 0.2 0.4 0.6 0.8 1.0"
"THERE
EXISTS
SCALARS
C",0.996822033898305,Return
"THERE
EXISTS
SCALARS
C",0.9978813559322034,"Run 0
Run 1
Run 2"
"THERE
EXISTS
SCALARS
C",0.9989406779661016,"Figure 9: ROSA is robust to the component used to generate exploration bonus L. ROSA works
equally well when we RND or Count-based method forL."
