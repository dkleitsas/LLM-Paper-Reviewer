Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0014792899408284023,"Training deep neural networks is a well-known highly non-convex problem. In re-
cent work (Pilanci & Ergen, 2020), it is shown that there is no duality gap for reg-
ularized two-layer neural networks with ReLU activation, which enables global
optimization via convex programs. For multi-layer linear networks with vector
outputs, we formulate convex dual problems and demonstrate that the duality gap
is non-zero for depth three and deeper networks. However, by modifying the deep
networks to more powerful parallel architectures, we show that the duality gap
is exactly zero. Therefore, strong convex duality holds, and hence there exist
equivalent convex programs that enable training deep networks to global optimal-
ity. We also demonstrate that the weight decay regularization in the parameters
explicitly encourages low-rank solutions via closed-form expressions. For three-
layer non-parallel ReLU networks, we show that strong duality holds for rank-1
data matrices, however, the duality gap is non-zero for whitened data matrices.
Similarly, by transforming the neural network architecture into a corresponding
parallel version, the duality gap vanishes."
INTRODUCTION,0.0029585798816568047,"1
INTRODUCTION"
INTRODUCTION,0.004437869822485207,"Deep neural networks demonstrate outstanding representation and generalization abilities in popular
learning problems ranging from computer vision, natural language processing to recommendation
system. Although the training problem of deep neural networks is a highly non-convex optimization
problem, simple algorithms, such as stochastic gradient descent, can ﬁnd a solution with good gener-
alization properties. The non-convex and non-linear nature of neural networks render the theoretical
understanding of neural networks extremely challenging."
INTRODUCTION,0.005917159763313609,"The Lagrangian dual problem (Boyd et al., 2004) plays an important role in the theory of convex
and non-convex optimization. For convex optimization problems, the convex duality is an important
tool to determine its optimal value and to characterize the optimal solutions. Even for a non-convex
primal problem, the dual problem is a convex optimization problem the can be solved efﬁciently. As
a result of weak duality, the optimal value of the dual problem serves as a non-trivial lower bound for
the optimal primal objective value. Although the duality gap is non-zero for non-convex problems,
the dual problem provides a convex relaxation of the non-convex primal problem. For example, the
semi-deﬁnite programming relaxation of the two-way partitioning problem can be derived from its
dual problem Boyd et al. (2004)."
INTRODUCTION,0.0073964497041420114,"The convex duality also has important applications in machine learning. In (Paternain et al., 2019),
the design problem of an all-encompassing reward can be formulated as a constrained reinforcement
learning problem, which is shown to have zero duality. This property gives a theoretical convergence
guarantee of the primal-dual algorithm for solving this problem. Meanwhile, the minimax generative
adversarial net (GAN) training problem can be tackled using duality (Farnia & Tse, 2018)."
INTRODUCTION,0.008875739644970414,"In lines of recent works, the convex duality can also be applied for analyzing the optimal layer
weights of two-layer neural networks with linear or ReLU activations (Pilanci & Ergen, 2020; Ergen
& Pilanci, 2020a). Based on the convex duality framework, the training problem of two-layer neural
networks with ReLU activation can be represented in terms of a single convex program in (Pilanci
& Ergen, 2020). Such convex optimization formulations are extended to two-layer and three-layer
convolutional neural network training problems in (Ergen & Pilanci, 2020c). Strong duality also
holds for deep linear neural networks with scalar output (Ergen & Pilanci, 2020b). The convex"
INTRODUCTION,0.010355029585798817,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.011834319526627219,"optimization formulation essentially gives a detailed characterization of the global optimum of the
training problem. This enables us to examine in numerical experiments whether popular optimizers
for neural networks, such as gradient descent or stochastic gradient descent, converge to the global
optimum of the training loss."
INTRODUCTION,0.013313609467455622,"Admittedly, the zero duality gap is hard to achieve for deep neural networks, especially for those
with vector outputs. This imposes more difﬁculty to train deep neural networks. Fortunately, neural
networks with parallel structures (also known as multi-branch architecture) appear to be easier to
train. Practically, the usage of parallel neural networks dates back to AlexNet (Krizhevsky et al.,
2012). Modern neural network architecture including Inception (Szegedy et al., 2017), Xception
(Chollet, 2017) and SqueezeNet (Iandola et al., 2016) utilize the parallel structure. As the “parallel”
version of ResNet (He et al., 2016a;b), ResNeXt (Xie et al., 2017) and Wide ResNet (Zagoruyko &
Komodakis, 2016) exhibit improved performance on many applications. Recently, it was shown that
neural networks with the parallel architecture have smaller duality gaps (Zhang et al., 2019) com-
pared to standard neural networks. On the other hand, it is known that overparameterized parallel
neural networks have benign training landscapes (Haeffele & Vidal, 2017). For training ℓ2 loss with
deep linear networks using Schatten norm regularization, Zhang et al. (2019) show that there is no
duality gap. From another perspective, the standard two-layer network is equivalent to the parallel
two-layer network. This may also explain why there is no duality gap for two-layer neural networks."
CONTRIBUTIONS,0.014792899408284023,"1.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.016272189349112426,"Following the convex duality framework introduced in (Ergen & Pilanci, 2020b;a), which showed
the duality gap is zero for two-layer networks, we go beyond two-layer and study the convex duality
for vector-output deep neural networks with linear activation and ReLU activation. Surprisingly, we
prove that three-layer networks may have duality gaps depending on their architecture, unlike
two-layer neural networks which always have zero duality gap. We summarize our contributions
as follows."
CONTRIBUTIONS,0.01775147928994083,"• For training standard vector-output deep linear networks using ℓ2 regularization, we pre-
cisely calculate the optimal value of the primal and dual problems and show that the duality
gap is non-zero, i.e., Lagrangian relaxation is inexact. We also demonstrate that the ℓ2-
regularization on the parameter explicitly forces a tendency toward a low-rank solution,
which is boosted with the depth. However, we show that the optimal solution is available
in closed-form.
• For parallel deep linear networks, with certain convex regularization, we show that the
duality gap is zero, i.e, Lagrangian relaxation is exact.
• For training vector-output three-layer ReLU networks with standard architecture using ℓ2
regularization, even when the data is whitened, we show that the duality gap is non-zero.
The gap can be closed by replacing the neural network to one with parallel architecture.
• For parallel deep ReLU networks of arbitrary depth, with certain convex regularization, we
prove strong duality, i.e., the duality gap is zero. Remarkably, this guarantees that there
is a convex program equivalent to the original deep ReLU neural network problem."
CONTRIBUTIONS,0.019230769230769232,We summarize the duality gaps for parallel/standard neural network in Table 1.
NOTATIONS,0.020710059171597635,"1.2
NOTATIONS"
NOTATIONS,0.022189349112426034,"We use bold capital letters to represent matrices and bold lowercase letters to represent vectors. De-
note [n] = {1, . . . , n}. For a matrix Wl ∈Rm1×m2, for i ∈[m1] and j ∈[m2], we denote wcol
l,i
as its i-th column and wrow
l,j
as its j-th row. Throughout the paper, X ∈RN×d is the data matrix
consisting of d dimensional N samples and Y ∈RN×K is the label matrix for a regression/classiﬁ-
cation task with K outputs."
MOTIVATIONS AND BACKGROUND,0.023668639053254437,"1.3
MOTIVATIONS AND BACKGROUND"
MOTIVATIONS AND BACKGROUND,0.02514792899408284,"Recently a series of papers (Pilanci & Ergen, 2020; Ergen & Pilanci, 2020b;a) studied two-layer
neural networks via convex duality and proved that strong duality holds for these architectures."
MOTIVATIONS AND BACKGROUND,0.026627218934911243,Under review as a conference paper at ICLR 2022
MOTIVATIONS AND BACKGROUND,0.028106508875739646,"Table 1: Duality gaps for L-layer standard and parallel architectures. we compare our duality gap
characterization with previous literature. Each check mark indicates whether a characterization of
the duality gap exists for the corresponding architecture and the number next to it indicates whether
the gap is zero or not."
MOTIVATIONS AND BACKGROUND,0.029585798816568046,"Linear
ReLU"
MOTIVATIONS AND BACKGROUND,0.03106508875739645,"L = 2
L = 3
L > 3
L = 2
L = 3
L > 3"
MOTIVATIONS AND BACKGROUND,0.03254437869822485,Standard
MOTIVATIONS AND BACKGROUND,0.034023668639053255,"Ergen & Pilanci (2020a; 2021a)



(0)


Bach (2017)
Pilanci & Ergen (2020)
(0)


(0)

"
MOTIVATIONS AND BACKGROUND,0.03550295857988166,"Ergen & Pilanci (2021b)



(0)


Ergen & Pilanci (2021c)
(0)





This paper
(0)
(̸= 0)
(̸= 0)
(0)
(̸= 0)
"
MOTIVATIONS AND BACKGROUND,0.03698224852071006,Parallel
MOTIVATIONS AND BACKGROUND,0.038461538461538464,"Ergen & Pilanci (2021b)



(0)
(0)

Zhang et al. (2019)
Ergen & Pilanci (2021c)
(0)
(0)
(0)


"
MOTIVATIONS AND BACKGROUND,0.03994082840236687,"This paper
(0)
(0)
(0)
(0)
(0)
(0)"
MOTIVATIONS AND BACKGROUND,0.04142011834319527,"Particularly, these prior works consider the following weight decay regularized training framework
for classiﬁcation/regression tasks. Given a data matrix X ∈RN×d consisting of d dimensional N
samples and the corresponding label matrix y ∈RN, the weight-decay regularized training problem
for a scalar-output neural network with m hidden neurons can be written as follows"
MOTIVATIONS AND BACKGROUND,0.042899408284023666,"P := min
W1,w2
1
2∥φ(XW1)w2 −y∥2
2 + β"
MOTIVATIONS AND BACKGROUND,0.04437869822485207,"2 (∥W1∥2
F + ∥w2∥2
2),
(1)"
MOTIVATIONS AND BACKGROUND,0.04585798816568047,"where W1 ∈Rd×m and w2 ∈Rm are the layer weights, β > 0 is a regularization parameter, and φ
is the activation function, which can be linear φ(z) = z or ReLU φ(z) = max{z, 0}. Then, one can
take the dual of (1) with respect to W1 and w2 obtain the following dual optimization problem"
MOTIVATIONS AND BACKGROUND,0.047337278106508875,"D := max
λ
−1"
MOTIVATIONS AND BACKGROUND,0.04881656804733728,"2∥λ −y∥2
2 + 1"
MOTIVATIONS AND BACKGROUND,0.05029585798816568,"2∥y∥2
2, s.t.
max
w1:∥w1∥2≤1 |λT φ(Xw1)| ≤β.
(2)"
MOTIVATIONS AND BACKGROUND,0.051775147928994084,"We ﬁrst note that since the training problem (1) is non-convex, strong duality may not hold, i.e.,
P ≥D. Surprisingly, as shown in Pilanci & Ergen (2020); Ergen & Pilanci (2020b;a), strong
duality in fact holds, i.e., P = D, for two-layer networks and therefore one can derive exact convex
representations for the non-convex training problem in (1). However, extensions of this approach to
deeper and state-of-the-art architectures are not available in the literature. Based on this observation,
the central question we address in this paper is:"
MOTIVATIONS AND BACKGROUND,0.05325443786982249,Does strong duality hold for deep neural networks?
MOTIVATIONS AND BACKGROUND,0.05473372781065089,"Depending on the answer to the question above, an immediate next questions we address is"
MOTIVATIONS AND BACKGROUND,0.05621301775147929,"Can we characterize the duality gap (P-D)? Is there an architecture for which strong duality holds
independent of the depth?"
MOTIVATIONS AND BACKGROUND,0.057692307692307696,"Consequently, throughout the paper, we provide a full characterization of convex duality for deeper
neural networks. Then, based on this characterization, we propose a modiﬁed architecture for which
strong duality holds regardless of depth."
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.05917159763313609,"1.4
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.060650887573964495,"We brieﬂy review the convex duality for two-layer neural networks introduced in (Ergen & Pilanci,
2020b;a). Consider the following weight-decay regularized training problem for a vector-output
neural network architecture with m hidden neurons"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.0621301775147929,"min
W1,W2
1
2∥φ(XW1)W2 −Y∥2
F + β"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.06360946745562131,"2 (∥W1∥2
F + ∥W2∥2
F ),
(3)"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.0650887573964497,Under review as a conference paper at ICLR 2022
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.06656804733727811,"where W1 ∈Rd×m and W2 ∈Rm×K are the variables, and β > 0 is a regularization parameter.
Here φ is the activation function, which can be linear φ(z) = z or ReLU φ(z) = max{z, 0}. As
long as the network is sufﬁciently overparameterized, there exists a feasible solution for such that
φ(XW1)W2 = Y. Then, a minimal norm variant1 of the training problem in (3) is given by"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.06804733727810651,"min
W1,W2
1
2(∥W1∥2
F + ∥W2∥2
F ) s.t. φ(XW1)W2 = Y.
(4)"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.0695266272189349,"As shown in (Pilanci & Ergen, 2020), after a suitable rescaling, this problem can be reformulated as"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.07100591715976332,"min
W1,W2 m
X"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.07248520710059171,"j=1
∥wrow
2,j ∥2 s.t. φ(XW1)W2 = Y, ∥wcol
1,j∥2 ≤1, j ∈[m].
(5)"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.07396449704142012,"where [m] = {1, . . . , m}. Here wrow
2,j represents the j-th row of W2 and wcol
1,j denotes the j-th
column of W1. The rescaling does not change the solution to (4). By taking the dual with respect
to W1 and W2, the dual problem of (5) with respect to variables is a convex optimization problem
given by"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.07544378698224852,"max
Λ tr(ΛT Y), s.t.
max
u:∥u∥2≤1 ∥ΛT φ(Xu)∥2 ≤1,
(6)"
CONVEX DUALITY FOR TWO-LAYER NEURAL NETWORKS,0.07692307692307693,"where Λ ∈RN×K is the dual variable. Provided that m ≥m∗, where m∗≤N + 1, the strong
duality holds, i.e., the optimal value of the primal problem (5) equals to the optimal value of the dual
problem (6)."
ORGANIZATION,0.07840236686390532,"1.5
ORGANIZATION"
ORGANIZATION,0.07988165680473373,"This paper is organized as follows. In Section 2, we review standard neural networks and intro-
duce parallel architectures. For deep linear networks, we derive primal and dual problems for both
standard and parallel architectures and provide calculations of optimal values of these problems in
Section 3. We derive primal and dual problems for three-layer ReLU networks with standard archi-
tecture and precisely calculate the optimal values for whitened data in Section 4. We also show that
deep ReLU networks with parallel structures have no duality gap."
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.08136094674556213,"2
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES"
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.08284023668639054,We start with the L-layer neural network with the standard architecture:
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.08431952662721894,"fθ(X) = AL−1WL,
Al = φ(Al−1Wl), ∀l ∈[L −1], A0 = X,"
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.08579881656804733,"where φ is the activation function, Wl ∈Rml−1×ml is the weight matrix in the l-th layer and θ =
(W1, . . . , WL) represents the parameter of the neural network. We also denote the input and output
dimensions as m0 = d and mL = K for simplicity and assume that ml ≥max{d, K}, ∀l ∈[L−2]."
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.08727810650887574,We then introduce the neural network with the parallel architecture:
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.08875739644970414,"f prl
θ (X) = AL−1WL,
Al,j = φ(Al−1,jWl,j), ∀j ∈[m], ∀l ∈[L −1],
A0,j = X, ∀j ∈[m]."
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.09023668639053255,"Here for l ∈[L −1], the l-th layer has m weight matrices Wl,j ∈Rml−1×ml where j ∈[m].
Speciﬁcally, we let mL−1 = 1. In short, we can view the output AL−1 from a parallel neural
network as a concatenation of m scalar-output standard neural work. In Figure 1 and 2, we provide
examples of neural networks with standard and parallel architectures. We shall emphasize that for
L = 2, the standard neural network is identical to the parallel neural network."
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.09171597633136094,"1This corresponds to weak regularization, i.e., β →0 in (3) as considered in Wei et al. (2018)."
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.09319526627218935,Under review as a conference paper at ICLR 2022
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.09467455621301775,"Layer 1
Input
Layer 2 Layer 3 Layer 4"
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.09615384615384616,Figure 1: Standard Architecture
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.09763313609467456,"Layer 1
Input
Layer 2 Layer 3 Layer 4"
STANDARD NEURAL NETWORKS VS PARALLEL ARCHITECTURES,0.09911242603550297,Figure 2: Parallel Architecture
DEEP LINEAR NETWORKS,0.10059171597633136,"3
DEEP LINEAR NETWORKS"
DEEP LINEAR NETWORKS,0.10207100591715976,"In this section, we discuss the convex duality of deep linear network with vector output."
STANDARD DEEP LINEAR NETWORKS,0.10355029585798817,"3.1
STANDARD DEEP LINEAR NETWORKS"
STANDARD DEEP LINEAR NETWORKS,0.10502958579881656,"We ﬁrst consider the neural network with standard architecture, i.e., fθ(X) = XW1 . . . WL. Con-
sider the following minimal norm optimization problem:"
STANDARD DEEP LINEAR NETWORKS,0.10650887573964497,"Plin =
min
{Wl}L
l=1 1
2 L
X"
STANDARD DEEP LINEAR NETWORKS,0.10798816568047337,"i=1
∥Wl∥2
F , s.t.
XW1, . . . , WL = Y,
(7)"
STANDARD DEEP LINEAR NETWORKS,0.10946745562130178,"where the variables are W1, . . . , WL. As shown in the Proposition 3.1 in (Ergen & Pilanci, 2020b),
by introducing a scale parameter t, the problem (7) can be reformulated as"
STANDARD DEEP LINEAR NETWORKS,0.11094674556213018,"Plin = min
t>0
L −2"
STANDARD DEEP LINEAR NETWORKS,0.11242603550295859,"2
t2 + Plin(t),"
STANDARD DEEP LINEAR NETWORKS,0.11390532544378698,where Plin(t) is deﬁned as
STANDARD DEEP LINEAR NETWORKS,0.11538461538461539,"Plin(t) =
min
{Wl}L
l=1 K
X"
STANDARD DEEP LINEAR NETWORKS,0.11686390532544379,"j=1
∥wrow
L,j ∥2,"
STANDARD DEEP LINEAR NETWORKS,0.11834319526627218,"s.t. ∥Wi∥F ≤t, i ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[mL−1],"
STANDARD DEEP LINEAR NETWORKS,0.11982248520710059,XW1 . . . WL = Y. (8)
STANDARD DEEP LINEAR NETWORKS,0.12130177514792899,"The following proposition characterize the dual problem of Plin(t) and its bi-dual, i.e., dual of the
dual problem."
STANDARD DEEP LINEAR NETWORKS,0.1227810650887574,Proposition 1 The dual problem of Plin(t) deﬁned in (8) is a convex optimization problem given by
STANDARD DEEP LINEAR NETWORKS,0.1242603550295858,"Dlin(t) = max
Λ tr(ΛT Y)"
STANDARD DEEP LINEAR NETWORKS,0.1257396449704142,"s.t. ∥ΛT XW1 . . . WL−2wL−1∥2 ≤1,
∥Wi∥F ≤t, ∀i ∈[L −2], ∥wL−1∥2 ≤1. (9)"
STANDARD DEEP LINEAR NETWORKS,0.12721893491124261,"There exists m∗≤KN + 1 such that the dual problem Dlin(t) can be reformulated as the bi-dual,
i.e.,"
STANDARD DEEP LINEAR NETWORKS,0.128698224852071,"Dlin(t) =
min
{Wl,j}L
l=1 m∗
X"
STANDARD DEEP LINEAR NETWORKS,0.1301775147928994,"j=1
∥wrow
L,j ∥2, s.t. m∗
X"
STANDARD DEEP LINEAR NETWORKS,0.13165680473372782,"j=1
XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = Y,"
STANDARD DEEP LINEAR NETWORKS,0.13313609467455623,"∥Wi,j∥F ≤t, i ∈[L −2], j ∈[m∗], ∥wcol
L−1,j∥2 ≤1, j ∈[m∗]. (10)"
STANDARD DEEP LINEAR NETWORKS,0.1346153846153846,Under review as a conference paper at ICLR 2022
STANDARD DEEP LINEAR NETWORKS,0.13609467455621302,"Detailed derivation of the dual and the bi-dual are provided in Appendix B.1. The reason why we
do not directly take the dual of Plin is that the objective function in Plin involves the weights of ﬁrst
L −1 layer, which prevents obtaining a meaningful dual problem. We note that bi-dual problem is
related to the minimal norm problem of a parallel neural network with aligned weights. Namely, the
Frobenius norm of the weight matrices in each branch has the same upper bound t."
STANDARD DEEP LINEAR NETWORKS,0.13757396449704143,"For a matrix A ∈Rm×n and p > 0, the Schatten-p quasi-norm of A is deﬁned as"
STANDARD DEEP LINEAR NETWORKS,0.1390532544378698,∥A∥Sp =  
STANDARD DEEP LINEAR NETWORKS,0.14053254437869822,"min{m,n}
X"
STANDARD DEEP LINEAR NETWORKS,0.14201183431952663,"i=1
σp
i (A)   1/p ,"
STANDARD DEEP LINEAR NETWORKS,0.14349112426035504,"where σi(A) is the i-th largest singular value of A. The optimal value of Plin(t) and Dlin(t) can be
precisely calculated."
STANDARD DEEP LINEAR NETWORKS,0.14497041420118342,"Theorem 1 For ﬁxed t > 0, the optimal value of Plin(t) and Dlin(t) are given by"
STANDARD DEEP LINEAR NETWORKS,0.14644970414201183,"Plin(t) = t−(L−2)∥X†Y∥S2/L,
(11)"
STANDARD DEEP LINEAR NETWORKS,0.14792899408284024,"and
Dlin(t) = t−(L−2)∥X†Y∥∗,
(12)"
STANDARD DEEP LINEAR NETWORKS,0.14940828402366865,where X† is the pseudo inverse of X and ∥· ∥∗represents the nuclear norm.
STANDARD DEEP LINEAR NETWORKS,0.15088757396449703,"As a result, the duality gap exists, i.e., P > D, for standard deep linear networks with L ≥3, if the
singular values of X†Y are not equal to the same value. We note that the optimal scale parameter
t for the primal problem Plin is given by t∗= ∥W∗∥1/L
S2/L. To the best of our knowledge, this
result wasn’t shown previously. We leave the proof in Appendix B.2 and provide conditions on
W1, . . . , WL to achieve the optimal value."
STANDARD DEEP LINEAR NETWORKS,0.15236686390532544,"In the calculation of Plin(t), we utilize the following proposition."
STANDARD DEEP LINEAR NETWORKS,0.15384615384615385,"Proposition 2 Suppose that W ∈Rd×K with rank r is given. Assume that mi ≥r for i = 0, . . . , L.
Consider the following optimization problem:"
STANDARD DEEP LINEAR NETWORKS,0.15532544378698224,"min
{Wl}L
l=1"
STANDARD DEEP LINEAR NETWORKS,0.15680473372781065,"1
2
 
∥W1∥2
F + · · · + ∥WL∥2
F

, s.t. W1W2 . . . WL = W.
(13)"
STANDARD DEEP LINEAR NETWORKS,0.15828402366863906,"Then, the optimal value of the problem (13) is given by L"
STANDARD DEEP LINEAR NETWORKS,0.15976331360946747,"2 ∥W∥2/L
S2/L."
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.16124260355029585,"3.2
PARALLEL DEEP LINEAR NEURAL NETWORKS"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.16272189349112426,"For the neural network with parallel structure, we consider the minimal norm optimization problem:"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.16420118343195267,"min
{Wl,j}L
l=1 1
2 "
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.16568047337278108,"
L−1
X l=1 m
X"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.16715976331360946,"j=1
∥Wl,j∥2
F + ∥WL∥2
F  , s.t. m
X"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.16863905325443787,"j=1
XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = Y. (14)"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.17011834319526628,"Due to a rescaling to achieve the lower bound of the inequality of arithmetic and geometric means,
we have the following result."
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.17159763313609466,Proposition 3 The problem (14) can be formulated as
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.17307692307692307,"min
{Wl,j}l∈[L],j∈[m] L 2 m
X"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.17455621301775148,"j=1
∥wrow
L,j ∥2/L
2
, s.t. m
X"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1760355029585799,"j=1
XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = Y,"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.17751479289940827,"∥Wl,j∥F ≤1, l ∈[L −2], j ∈[m], ∥wcol
L−1,j∥2 ≤1, j ∈[m]. (15)"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.17899408284023668,Under review as a conference paper at ICLR 2022
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1804733727810651,"We note that z2/L is a non-convex of z and we cannot take a meaningful dual. The bi-dual problem
of the standard neural network does NOT correspond to the primal problem of the parallel neural
network because the Frobenius norm of each parallel part of the parallel architecture can be different
in formulating (15)."
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1819526627218935,"We can consider another regularization, i.e.,"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1834319526627219,"P prl
lin =
min
{Wl,j}L
l=1 1
2 L−1
X l=1 m
X"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1849112426035503,"j=1
∥Wl,j∥L
F + ∥WL∥L
F , s.t. m
X"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1863905325443787,"j=1
XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = Y. (16)"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1878698224852071,Proposition 4 The problem (16) can be formulated as
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1893491124260355,"P prl
lin =
min
{Wl,j}l∈[L],j∈[m] L 2 m
X"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1908284023668639,"j=1
∥wrow
L,j ∥2, s.t. m
X"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.19230769230769232,"j=1
XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = Y,"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1937869822485207,"∥Wl,j∥F ≤1, l ∈[L −2], j ∈[m], ∥wcol
L−1,j∥2 ≤1, j ∈[m]. (17)"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1952662721893491,"The dual problem of P prl
lin is a convex problem"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.19674556213017752,"Dprl
lin = max
Λ tr(ΛT Y),"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.19822485207100593,"s.t. ∥ΛT XW1 . . . WL−2wL−1∥2 ≤L/2,
∀∥Wi∥F ≤1, i ∈[L −2], ∥wL−1∥2 ≤1. (18)"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.1997041420118343,"For the parallel linear network, the strong duality holds.
Theorem 2 There exists m∗≤KN + 1 such that as long as the number of branches m ≥m∗,
the strong duality holds for the problem (16). Namely, P prl
lin = Dprl
lin . The optimal values are both
L"
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.20118343195266272,2 ∥X†Y∥∗.
PARALLEL DEEP LINEAR NEURAL NETWORKS,0.20266272189349113,"This implies that there exist equivalent convex problems which achieve the global optimum of the
deep parallel linear network. Comparatively, optimizing deep parallel linear neural networks can be
much easier than optimizing deep standard linear networks."
GENERAL LOSS FUNCTIONS,0.20414201183431951,"3.3
GENERAL LOSS FUNCTIONS"
GENERAL LOSS FUNCTIONS,0.20562130177514792,"Now we consider general loss functions, i.e.,"
GENERAL LOSS FUNCTIONS,0.20710059171597633,"min
{Wl}L
l=1
ℓ(XW1 . . . WL, Y) + β 2 L
X"
GENERAL LOSS FUNCTIONS,0.20857988165680474,"i=1
∥Wi∥2
F ,"
GENERAL LOSS FUNCTIONS,0.21005917159763313,"where ℓ(Z, Y) is a general loss function and β > 0 is a regularization parameter. According to
Proposition 2, the above problem is equivalent to"
GENERAL LOSS FUNCTIONS,0.21153846153846154,"min
W ℓ(XW, Y) + βL"
GENERAL LOSS FUNCTIONS,0.21301775147928995,"2 ∥W∥2/L
S2/L.
(19)"
GENERAL LOSS FUNCTIONS,0.21449704142011836,"The ℓ2 regularization term becomes the Schatten-2/L quasi-norm on W to the power 2/L. Suppose
that there exists W such that l(XW, Y) = 0. With β →0, asymptotically, the optimal solution to
the problem (19) converges to the optimal solution of"
GENERAL LOSS FUNCTIONS,0.21597633136094674,"min
W ∥W∥2/L
S2/L, s.t. ℓ(XW, Y) = 0.
(20)"
GENERAL LOSS FUNCTIONS,0.21745562130177515,"In other words, the ℓ2 regularization explicitly regularizes the training problem to ﬁnd a low-rank
solution W."
GENERAL LOSS FUNCTIONS,0.21893491124260356,Under review as a conference paper at ICLR 2022
NEURAL NETWORKS WITH RELU ACTIVATION,0.22041420118343194,"4
NEURAL NETWORKS WITH RELU ACTIVATION"
NEURAL NETWORKS WITH RELU ACTIVATION,0.22189349112426035,"Now, we focus on the three-layer neural network with ReLU activation, i.e., φ(z) = max{z, 0}."
STANDARD THREE-LAYER RELU NETWORKS,0.22337278106508876,"4.1
STANDARD THREE-LAYER RELU NETWORKS"
STANDARD THREE-LAYER RELU NETWORKS,0.22485207100591717,"We ﬁrst focus on the three-layer ReLU network with standard architecture. Consider the minimal
norm problem"
STANDARD THREE-LAYER RELU NETWORKS,0.22633136094674555,"PReLU =
min
{Wi}3
i=1 1
2"
X,0.22781065088757396,"3
X"
X,0.22928994082840237,"i=1
∥Wi∥2
F , s.t. ((XW1)+W2)+W3 = Y.
(21)"
X,0.23076923076923078,"Here we denote (z)+ = max{z, 0}. Similarly, by introducing a scale parameter t, this problem can
be formulated as"
X,0.23224852071005916,"PReLU = min
t>0
1
2t2 + PReLU(t),"
X,0.23372781065088757,where PReLU(t) is deﬁned as
X,0.23520710059171598,"PReLU(t) =
min
{Wi}3
i=1 K
X"
X,0.23668639053254437,"j=1
∥wrow
3,j ∥2,"
X,0.23816568047337278,"s.t. ∥W1∥F ≤t, ∥wcol
2,j∥2 ≤1, j ∈[m2],"
X,0.23964497041420119,((XW1)+W2)+W3 = Y. (22)
X,0.2411242603550296,"The proof is analagous to the proof of Proposition 3.1 in (Ergen & Pilanci, 2020b). For W1 ∈
Rd×m, we deﬁne the set"
X,0.24260355029585798,"A(W1) = {((XW1)+w2)+|∥w2∥2 ≤1}.
(23)"
X,0.2440828402366864,Proposition 5 The dual problem of PReLU(t) deﬁned in (22) is a convex problem deﬁned as
X,0.2455621301775148,"DReLU(t) = max
Λ tr(ΛT Y), s.t. ∥ΛT v∥2 ≤1, v ∈A(W1), ∀∥W1∥F ≤t.
(24)"
X,0.2470414201183432,"There exists m∗≤KN + 1 such that the dual problem can be reformulated as the bi-dual problem,
i.e.,"
X,0.2485207100591716,"DReLU(t) =
min
{W1,j}m∗
j=1,W2∈Rm1×m∗,W3∈Rm∗×K K
X"
X,0.25,"j=1
∥wrow
3,j ∥2, s.t. m∗
X"
X,0.2514792899408284,"j=1
((XW1,j)+wcol
2,j)+wrow
3,j = Y, ∥W1,j∥F ≤t, ∥wcol
2,j∥2 ≤1. (25)"
X,0.2529585798816568,"For the case where the data matrix is with rank 1 and the neural network is with scalar output, there
is no duality gap."
X,0.25443786982248523,"Theorem 3 For a three-layer scalar-output ReLU network, let X be a rank-one data matrix. Then,
strong duality holds, i.e., PReLU(t) = DReLU(t). Suppose that λ∗is the optimal solution to the dual
problem DReLU(t), then the optimal weights for each layer can be formulated as"
X,0.2559171597633136,"W1 = tsign(|(λ∗)T (c)+| −|(λ∗)T (−c)+|)ρ0ρT
1 , w2 = ρ1."
X,0.257396449704142,"Here ρ0 = a0/∥a0∥2 and ρ1 ∈Rml
+ satisﬁes ∥ρ1∥= 1."
X,0.2588757396449704,"Now, we consider the case where the data matrix is whitened, i.e., XXT = In and Y has orthogonal
columns. We leave the proof of Theorem 4 and the characterization of optimal solutions in Appendix
C.3."
X,0.2603550295857988,Under review as a conference paper at ICLR 2022
X,0.2618343195266272,"Theorem 4 Let {X, Y} be a dataset such that XXT = In and Y has orthogonal columns. Then,
the optimal value of PReLU(t) and DReLU(t) are given by"
X,0.26331360946745563,"PReLU(t) = t−1  
K
X j=1"
X,0.26479289940828404,"
∥(yj)+∥2/3
2
+ ∥(−yj)+∥2/3
2

  3/2"
X,0.26627218934911245,",
(26) and"
X,0.2677514792899408,"DReLU(t) = t−1
n
X"
X,0.2692307692307692,"j=1
(∥(yj)+∥2 + ∥(−yj)+∥2) .
(27)"
X,0.27071005917159763,"Suppose that Y is the one-hot encoding of the label. Then, we note that yj ≥0 and ∥(yj)+∥2 is the
square root of the number of data points in the j-th class. Therefore, we recover the result about the
duality gap of standard three-layer ReLU networks."
PARALLEL DEEP RELU NETWORKS,0.27218934911242604,"4.2
PARALLEL DEEP RELU NETWORKS"
PARALLEL DEEP RELU NETWORKS,0.27366863905325445,"For the parallel architecture, we show that there is no duality gap for arbitrary deep ReLU network
with large enough number of branches. Consider the following minimal norm problem:"
PARALLEL DEEP RELU NETWORKS,0.27514792899408286,"P prl
ReLU = min 1 2 L−1
X l=1 m
X"
PARALLEL DEEP RELU NETWORKS,0.27662721893491127,"j=1
∥Wl,j∥L
F + ∥WL∥L
F , s.t. m
X"
PARALLEL DEEP RELU NETWORKS,0.2781065088757396,"j=1
(((XW1,j)+ . . . WL−2,j)+wcol
L−1,j)+wrow
L,j = Y. (28)"
PARALLEL DEEP RELU NETWORKS,0.27958579881656803,Proposition 6 The problem (28) can be reformulated as
PARALLEL DEEP RELU NETWORKS,0.28106508875739644,"P prl
ReLU = minL 2 m
X"
PARALLEL DEEP RELU NETWORKS,0.28254437869822485,"j=1
∥wrow
L,j ∥2, s.t. m
X"
PARALLEL DEEP RELU NETWORKS,0.28402366863905326,"j=1
(((XW1,j)+ . . . WL−2,j)+wcol
L−1,j)+wrow
L,j = Y,"
PARALLEL DEEP RELU NETWORKS,0.28550295857988167,"∥Wl,j∥F ≤1, l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[m]. (29)"
PARALLEL DEEP RELU NETWORKS,0.2869822485207101,The dual problem (29) is a convex problem deﬁned as
PARALLEL DEEP RELU NETWORKS,0.28846153846153844,"Dprl
ReLU = max tr(ΛT Y), s.t.
max
v=((XW1)+...WL−2)+wL−1)+,
∥Wl∥F ≤1,l∈[L−2],∥wL−1∥2≤1"
PARALLEL DEEP RELU NETWORKS,0.28994082840236685,"∥ΛT v∥2 ≤L/2.
(30)"
PARALLEL DEEP RELU NETWORKS,0.29142011834319526,"For deep ReLU network with parallel architecture, the strong duality holds."
PARALLEL DEEP RELU NETWORKS,0.29289940828402367,"Theorem 5 The strong duality holds for (29) in the sense that P prl
ReLU = Dprl
ReLU for m ≥m∗, where
m∗is upper bounded by KN + 1."
PARALLEL DEEP RELU NETWORKS,0.2943786982248521,"Similar to case of parallel deep linear networks, the parallel deep ReLU network also achieves zero-
duality gap. Therefore, to ﬁnd the global optimum for parallel deep ReLU network is equivalent to
solve a convex program."
CONCLUSION,0.2958579881656805,"5
CONCLUSION"
CONCLUSION,0.2973372781065089,"We present the convex duality framework for standard neural networks, considering both multi-layer
linear networks and three-layer ReLU networks with rank-1 or whitened data. In stark contrast to
the two-layer case, the duality gap can be non-zero for neural networks with depth three or more.
Meanwhile, for neural networks with parallel architecture, with the regularization of L-th power of
Frobenius norm in the parameters, we show that strong duality holds and the duality gap reduces to
zero. A limitation of our work is that we primarily focus on minimal norm interpolation problems.
We expect to generalize our results to general regularized training problems."
CONCLUSION,0.2988165680473373,Under review as a conference paper at ICLR 2022
ACKNOWLEDGEMENTS,0.30029585798816566,"6
ACKNOWLEDGEMENTS"
ACKNOWLEDGEMENTS,0.30177514792899407,"This work was partially supported by the National Science Foundation under grants ECCS-2037304,
DMS-2134248, the Army Research Ofﬁce."
REFERENCES,0.3032544378698225,REFERENCES
REFERENCES,0.3047337278106509,"Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629–681, 2017."
REFERENCES,0.3062130177514793,"Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004."
REFERENCES,0.3076923076923077,"Franc¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 1251–1258, 2017."
REFERENCES,0.3091715976331361,"Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In International Conference on Artiﬁcial Intelligence and Statistics, pp.
4024–4033. PMLR, 2020a."
REFERENCES,0.3106508875739645,"Tolga Ergen and Mert Pilanci.
Convex duality of deep neural networks.
arXiv preprint
arXiv:2002.09773, 2020b."
REFERENCES,0.3121301775147929,"Tolga Ergen and Mert Pilanci.
Implicit convex regularizers of cnn architectures: Convex opti-
mization of two-and three-layer networks in polynomial time. arXiv preprint arXiv:2006.14798,
2020c."
REFERENCES,0.3136094674556213,"Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
Journal of Machine Learning Research, 22(212):1–63, 2021a."
REFERENCES,0.3150887573964497,"Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks
via convex programs. In International Conference on Machine Learning, pp. 2993–3003. PMLR,
2021b."
REFERENCES,0.3165680473372781,"Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
In International Conference on Machine Learning, pp. 3004–3014. PMLR, 2021c."
REFERENCES,0.3180473372781065,"Farzan Farnia and David Tse.
A convex duality framework for gans.
In S. Ben-
gio,
H.
Wallach,
H.
Larochelle,
K.
Grauman,
N.
Cesa-Bianchi,
and
R.
Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
831caa1b600f852b7844499430ecac17-Paper.pdf."
REFERENCES,0.31952662721893493,"Benjamin D Haeffele and Ren´e Vidal. Global optimality in neural network training. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331–7339, 2017."
REFERENCES,0.3210059171597633,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016a."
REFERENCES,0.3224852071005917,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016b."
REFERENCES,0.3239644970414201,"Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016."
REFERENCES,0.3254437869822485,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.3269230769230769,"Santiago Paternain, Luiz FO Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained
reinforcement learning has zero duality gap. arXiv preprint arXiv:1910.13393, 2019."
REFERENCES,0.32840236686390534,Under review as a conference paper at ICLR 2022
REFERENCES,0.32988165680473375,"Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time con-
vex optimization formulations for two-layer networks. arXiv preprint arXiv:2002.10553, 2020."
REFERENCES,0.33136094674556216,"Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. L1 regularization in inﬁnite di-
mensional feature spaces. In International Conference on Computational Learning Theory, pp.
544–558. Springer, 2007."
REFERENCES,0.3328402366863905,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 31, 2017."
REFERENCES,0.3343195266272189,"Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. arXiv preprint arXiv:1810.05369, 2018."
REFERENCES,0.33579881656804733,"Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 1492–1500, 2017."
REFERENCES,0.33727810650887574,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.33875739644970415,"Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov. Deep neural networks with multi-branch
architectures are intrinsically less non-convex. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1099–1109. PMLR, 2019."
REFERENCES,0.34023668639053256,Under review as a conference paper at ICLR 2022
REFERENCES,0.34171597633136097,"A
STAIRS OF DUALITY GAP FOR STANDARD DEEP LINEAR NETWORKS"
REFERENCES,0.3431952662721893,"Now we consider partially dualizing the non-convex optimization problem by exchanging a subset
of the minimization problems with respect to the hidden layers. Consider the Lagrangian for the
primal problem of standard deep linear network"
REFERENCES,0.34467455621301774,"Plin(t) =
min
{Wl}L−1
l=1
max
Λ tr(ΛT Y) −I

∥ΛT XW1 . . . WL−2wL−1∥2 ≤1

,"
REFERENCES,0.34615384615384615,"s.t. ∥Wi∥F ≤t, i ∈[L −2], ∥wL−1∥2 ≤1.
(31)"
REFERENCES,0.34763313609467456,"By changing the order of L −2 mins and the max in (31), for l = 0, 1, . . . , L −2, we can deﬁne the
l-th partial “dual” problem"
REFERENCES,0.34911242603550297,"D(l)
lin(t) =
min
W1,...Wl max
Λ
min
Wl+1,...,WL−2 tr(ΛT Y) −I

∥ΛT XW1 . . . WL−2wL−1∥2 ≤1

,"
REFERENCES,0.3505917159763314,"s.t. ∥Wi∥F ≤t, i ∈[L −2], ∥wL−1∥2 ≤1.
(32)"
REFERENCES,0.3520710059171598,"For l = 0, D(l)
lin(t) corresponds the primal problem Plin(t), while for l = L −2, D(l)
lin(t) is the
dual problem Dlin(t). From the following proposition, we illustrate that the dual problem of D(l)
lin(t)
corresponds to a minimal norm problem of a neural network with parallel structure."
REFERENCES,0.35355029585798814,"Proposition 7 There exists m∗≤KN + 1 such that the problem D(l)
lin(t) is equivalent to the “bi-
dual” problem min m∗
X"
REFERENCES,0.35502958579881655,"j=1
∥wrow
L,j ∥2, s.t. m∗
X"
REFERENCES,0.35650887573964496,"j=1
XW1 . . . WlWl+1,j . . . WL−2,jwcol
L−1,jwrow
L,j = Y,"
REFERENCES,0.35798816568047337,"∥Wi∥F ≤t, i ∈[l], ∥Wi,j∥F ≤t, i = l + 1, . . . , L −2, j ∈[m∗],"
REFERENCES,0.3594674556213018,"∥wcol
L−1,j∥2 ≤1, j ∈[m∗], (33)"
REFERENCES,0.3609467455621302,"where the variables are Wi ∈Rmi−1×mi for i ∈[l], Wi,j ∈Rmi−1×mi for i = l + 1, . . . , L −2,
j ∈[m∗], WL−1 ∈RmL−2×m∗and WL ∈Rm∗×mL."
REFERENCES,0.3624260355029586,"We can interpret the problem (33) as the minimal norm problem of a linear network with parallel
structures in (l + 1)-th to (L −2)-th layers. This indicates that for l = 0, 1, . . . , L −2, the bi-dual
formulation of D(l)
lin(t) can be viewed as an interpolation from a network with standard structure to
a network with parallel structure. Now, we calculate the exact value of D(l)
lin(t)."
REFERENCES,0.363905325443787,"Proposition 8 The optimal value D(l)
lin(t) follows"
REFERENCES,0.36538461538461536,"D(l)
lin(t) = t−(L−2)∥X†Y∥S2/(l+2).
(34)"
REFERENCES,0.3668639053254438,"Suppose that the eigenvalues X†Y are not identical to each other. Then, we have"
REFERENCES,0.3683431952662722,"Plin(t) = D(L−2)
lin
(t) > D(L−3)
lin
(t) > · · · > D(0)
lin (t) = D(t).
(35)"
REFERENCES,0.3698224852071006,"In Figure 3, we plot D(l)
lin(t) for l = 0, . . . , 5 for an example."
REFERENCES,0.371301775147929,"B
PROOFS OF MAIN RESULTS FOR LINEAR NETWORKS"
REFERENCES,0.3727810650887574,"B.1
PROOF OF PROPOSITION 1"
REFERENCES,0.3742603550295858,Consider the Lagrangian function
REFERENCES,0.3757396449704142,"L(W1, . . . , WL, Λ) = K
X"
REFERENCES,0.3772189349112426,"j=1
∥wL,j∥2 + tr(ΛT (Y −XW1 . . . WL)).
(36)"
REFERENCES,0.378698224852071,Under review as a conference paper at ICLR 2022
REFERENCES,0.3801775147928994,"Figure 3: Example of D(l)
lin(t)."
REFERENCES,0.3816568047337278,Here Λ ∈RN×K is the dual variable. We note that
REFERENCES,0.3831360946745562,"P(t) =
min
W1,...,WL max
Λ L(W1, . . . , WL, Λ),"
REFERENCES,0.38461538461538464,"s.t. ∥Wi∥F ≤t, i ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[mL−1],"
REFERENCES,0.386094674556213,"=
min
W1,...,WL−1 max
Λ tr(ΛT Y) −"
REFERENCES,0.3875739644970414,"mL−1
X"
REFERENCES,0.3890532544378698,"j=1
I

∥ΛT XW1 . . . WL−2wL−1,j∥2 ≤1

,"
REFERENCES,0.3905325443786982,"s.t. ∥Wi∥F ≤t, i ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[mL−1],"
REFERENCES,0.39201183431952663,"=
min
W1,...,WL−2,WL−1 max
Λ tr(ΛT Y) −I

∥ΛT XW1 . . . WL−2wL−1∥2 ≤1

,"
REFERENCES,0.39349112426035504,"s.t. ∥Wi∥F ≤t, i ∈[L −2], ∥wL−1∥2 ≤1. (37)"
REFERENCES,0.39497041420118345,"Here I(A) is 0 if the statement A is true. Otherwise it is +∞. For ﬁxed W1, . . . , WL−1, the
constraint on WL is linear so we can exchange the order of maxΛ and minWL in the second line of
(37)."
REFERENCES,0.39644970414201186,"By exchanging the order of min and max, we obtain the dual problem"
REFERENCES,0.3979289940828402,"D(t) = max
Λ
min
W1,...,WL−2 tr(ΛT Y) −I

∥ΛT XW1 . . . WL−2wL−1∥2 ≤1

,"
REFERENCES,0.3994082840236686,"s.t. ∥Wi∥F ≤t, i ∈[L −2], ∥wL−1∥2 ≤1,"
REFERENCES,0.40088757396449703,"= max
Λ tr(ΛT Y)"
REFERENCES,0.40236686390532544,"s.t. ∥ΛT XW1 . . . WL−2wL−1∥2 ≤1
∀∥Wi∥F ≤t, i ∈[L −2], ∥wL−1∥2 ≤1. (38)"
REFERENCES,0.40384615384615385,Now we derive the bi-dual problem. The dual problem can be reformulated as
REFERENCES,0.40532544378698226,"max
Λ tr(ΛT Y),"
REFERENCES,0.4068047337278107,"s.t. ∥ΛT XW1 . . . WL−2wL−1∥2 ≤1,
∀(W1, . . . , WL−2, wL−1) ∈Θ. (39)"
REFERENCES,0.40828402366863903,Here the set Θ is deﬁned as
REFERENCES,0.40976331360946744,"Θ = {(W1, . . . , WL−2, wL−1)|∥Wi∥F ≤t, i ∈[L −2], ∥wL−1∥2 ≤1}.
(40)"
REFERENCES,0.41124260355029585,"By writing θ = (W1, . . . , WL−2, wL−1), the dual of the problem (39) is given by"
REFERENCES,0.41272189349112426,"min ∥µ∥TV,
s.t.
R"
REFERENCES,0.41420118343195267,"θ∈Θ XW1 . . . WL−2wL−1dµ (θ) = Y.
(41)"
REFERENCES,0.4156804733727811,"Here µ : Σ →RK is a signed vector measure and Σ is a σ-ﬁeld of subsets of Θ. The norm ∥µ∥TV
is the total variation of µ, which can be calculated by"
REFERENCES,0.4171597633136095,"∥µ∥T V =
sup
u:∥u(θ)∥2≤1 (Z"
REFERENCES,0.41863905325443784,"Θ
uT (θ)dµ(θ) =: K
X i=1 Z"
REFERENCES,0.42011834319526625,"Θ
ui(θ)dµi(θ) )"
REFERENCES,0.42159763313609466,",
(42)"
REFERENCES,0.4230769230769231,Under review as a conference paper at ICLR 2022
REFERENCES,0.4245562130177515,where we write µ =  
REFERENCES,0.4260355029585799,"µ1
...
µK "
REFERENCES,0.4275147928994083,. The formulation in (41) has inﬁnite width in each layer. According to
REFERENCES,0.4289940828402367,"Theorem 9 in Appendix E, the measure µ in the integral can be represented by ﬁnitely many Dirac
delta functions. Therefore, we can rewrite the problem (41) as min m∗
X"
REFERENCES,0.43047337278106507,"j=1
∥wrow
L,j ∥2, s.t. m∗
X"
REFERENCES,0.4319526627218935,"j=1
XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = Y,"
REFERENCES,0.4334319526627219,"∥Wi,j∥F ≤t, i ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[m∗]. (43)"
REFERENCES,0.4349112426035503,"Here the variables are Wi,j for i ∈[L −2] and j ∈[m∗], WL−1 and WL. As the strong duality
holds for the problem (43) and (39), we can reformulate the problem of Dlin(t) as the bi-dual
problem (43)."
REFERENCES,0.4363905325443787,"B.2
PROOF OF PROPOSITION 2"
REFERENCES,0.4378698224852071,"We restate Proposition 2 with details.
Proposition 9 Suppose that W ∈Rd×K with rank r is given. Consider the following optimization
problem: min 1"
REFERENCES,0.4393491124260355,"2
 
∥W1∥2
F + · · · + ∥WL∥2
F

, s.t. W1W2 . . . WL = W,
(44)"
REFERENCES,0.4408284023668639,"in variables Wi ∈Rmi−1×mi. Here m0 = d, mL = K and mi ≥r for i = 1, . . . , L −1. Then,
the optimal value of the problem (44) is given by L"
REFERENCES,0.4423076923076923,"2 ∥W∥2/L
S2/L.
(45)"
REFERENCES,0.4437869822485207,Suppose that W = UΣVT . The optimal value can be achieved when
REFERENCES,0.4452662721893491,"Wi = Ui−1Σ1/LUT
i ,
i = 1, . . . , N, U0 = U, UL = V.
(46)"
REFERENCES,0.4467455621301775,"Here Ui ∈Rr×mi satisﬁes that UT
i Ui = I."
REFERENCES,0.44822485207100593,"We start with two lemmas.
Lemma 1 Suppose that A ∈Sn×n is a positive semi-deﬁnite matrix. Then, for any 0 < p < 1, we
have
n
X"
REFERENCES,0.44970414201183434,"i=1
Ap
ii ≥ n
X"
REFERENCES,0.4511834319526627,"i=1
λi(A)p.
(47)"
REFERENCES,0.4526627218934911,Here λi is the i-th largest eigenvalue of A.
REFERENCES,0.4541420118343195,"Lemma 2 Suppose that P ∈Rd×d is a projection matrix. Then, for arbitrary W ∈Rd×K, we have"
REFERENCES,0.4556213017751479,"σi(PW) ≤σi(W),"
REFERENCES,0.45710059171597633,where σi(W) represents the i-th largest singular value of W.
REFERENCES,0.45857988165680474,"Now, we present the proof for Proposition 2. For L = 1, the statement apparently holds. Suppose
that for L = l this statement holds. For L = l + 1, by writing A = W2 . . . Wl+1, we have"
REFERENCES,0.46005917159763315,"min ∥W1∥2
F + · · · + ∥WL∥2
F , s.t. W1W2 . . . Wl+1 = W"
REFERENCES,0.46153846153846156,"= min ∥W1∥2
F + l∥A∥2/l
2/l, s.t. W1A = W,"
REFERENCES,0.4630177514792899,"= min t2 + l∥A∥2/l
2/l, s.t. W1A = W, ∥W1∥F ≤t. (48)"
REFERENCES,0.46449704142011833,Suppose that t is ﬁxed. It is sufﬁcient to consider the following problem:
REFERENCES,0.46597633136094674,"min ∥A∥2/l
2/l, s.t. W1A = W, ∥W1∥F ≤t.
(49)"
REFERENCES,0.46745562130177515,Under review as a conference paper at ICLR 2022
REFERENCES,0.46893491124260356,"Suppose that there exists W1 and A such that W = W1A.
Then, we have WA†A =
W1AA†A = W. As WA† = W1AA†, according to Lemma 2, ∥WA†∥F ≤∥W1∥F ≤t.
Therefore, (WA†, A) is also feasible for the problem (49). Hence, the problem (49) is equivalent
to
min ∥A∥2/l
2/l, s.t. WA†A = W, ∥WA†∥F ≤t.
(50)"
REFERENCES,0.47041420118343197,"Assume that W is with rank r. Suppose that A = UΣVT , where Σ ∈Rr0×r0. Here r0 ≥r. Then,
we have A† = VΣ−1UT . We note that"
REFERENCES,0.4718934911242604,"∥WA†∥2
F
= tr(WVΣ−2VT WT )"
REFERENCES,0.47337278106508873,= tr(VT WT WVΣ−2) (51)
REFERENCES,0.47485207100591714,"Denote G(V) = VT WT WV. This implies that r
X"
REFERENCES,0.47633136094674555,"i=1
σi(A)−2 (G(V))ii ≤t2."
REFERENCES,0.47781065088757396,"Therefore, we have
 r0
X"
REFERENCES,0.47928994082840237,"i=1
σi(A)−2 (G(V))ii"
REFERENCES,0.4807692307692308,"!  r0
X"
REFERENCES,0.4822485207100592,"i=1
σi(A)2/l
!l ≥ r0
X"
REFERENCES,0.48372781065088755,"i=1
(G(V))1/(l+1)
ii !l+1 ."
REFERENCES,0.48520710059171596,"As WVT V = W, the non-zero eigenvalues of G(V) are exactly the non-zero eigenvalues of
WVVT WT = WWT , i.e., the square of non-zero singular values of W. From Lemma 1, we
have
r0
X"
REFERENCES,0.48668639053254437,"i=1
(G(V))1/(l+1)
ii
≥ r0
X"
REFERENCES,0.4881656804733728,"i=1
λi(G(V))1/(l+1) ≥ r
X"
REFERENCES,0.4896449704142012,"i=1
σi(W)2/(l+1).
(52)"
REFERENCES,0.4911242603550296,"Therefore, we have"
REFERENCES,0.492603550295858,"∥A∥2/l
S2/l = r0
X"
REFERENCES,0.4940828402366864,"i=1
σi(A)2/l ≥t−2/l
 r
X"
REFERENCES,0.49556213017751477,"i=1
σi(W)2/(l+1)
!(l+1)/l (53)"
REFERENCES,0.4970414201183432,This also implies that
REFERENCES,0.4985207100591716,"min ∥A∥2/l
2/l, s.t. W1A = W, ∥W1∥F ≤t"
REFERENCES,0.5,"≥t−2/l
 r
X"
REFERENCES,0.5014792899408284,"i=1
σi(W)2/(l+1)
!(l+1)/l"
REFERENCES,0.5029585798816568,".
(54)"
REFERENCES,0.5044378698224852,"Suppose that W = Pr
i=1 uiσivT
i is the SVD of W. We can let A ="
REFERENCES,0.5059171597633136,"Pr
i=1 σ2/(l+1)
i
1/2 t r
X"
REFERENCES,0.507396449704142,"i=1
uiσl/(l+1)
i
ρT
i ,"
REFERENCES,0.5088757396449705,"W1 =
t
Pr
i=1 σ2/(l+1)
i
1/2 r
X"
REFERENCES,0.5103550295857988,"i=1
ρiσ1/(l+1)
i
vT
i . (55)"
REFERENCES,0.5118343195266272,"Here ∥ρi∥2 = 1 and ρT
i ρj = 0 for i ̸= j. Then, W1A = W and ∥W1∥F ≤t. We also have"
REFERENCES,0.5133136094674556,"∥A∥2/L
S2/L =t−2/l
 r
X"
REFERENCES,0.514792899408284,"i=1
σ2/(l+1)
i"
REFERENCES,0.5162721893491125,"!1/l
r
X"
REFERENCES,0.5177514792899408,"i=1
σ2/(l+1)
i"
REFERENCES,0.5192307692307693,"=t−2/l
 r
X"
REFERENCES,0.5207100591715976,"i=1
σi(W)2/(l+1)
!(l+1)/l ."
REFERENCES,0.522189349112426,Under review as a conference paper at ICLR 2022
REFERENCES,0.5236686390532544,"In summary, we have"
REFERENCES,0.5251479289940828,"min t2 + l∥A∥
S2/l
2/l , s.t. W1A = W, ∥W1∥F ≤t."
REFERENCES,0.5266272189349113,"= min
t>0 t2 + lt−2/l
 r
X"
REFERENCES,0.5281065088757396,"i=1
σi(W)2/(l+1)
!(l+1)/l"
REFERENCES,0.5295857988165681,"=(l + 1) r
X"
REFERENCES,0.5310650887573964,"i=1
σi(W)2/(l+1)
!(l+1)/2"
REFERENCES,0.5325443786982249,"=∥W∥2/(l+1)
S2/(l+1). (56)"
REFERENCES,0.5340236686390533,This completes the proof.
REFERENCES,0.5355029585798816,"B.3
PROOF OF THEOREM 1"
REFERENCES,0.5369822485207101,"We ﬁrst compute the optimal value P of the primal problem. From Proposition 2, the minimal norm
problem (7) is equivalent to"
REFERENCES,0.5384615384615384,"min L∥W∥2/L
S2/L, s.t. XW = Y,
(57)"
REFERENCES,0.5399408284023669,"in variable W ∈Rd×K. According to Lemma 2, for any feasible W satisfying XW = Y, because
X†XW = X†Y and X†X is a projection matrix, we have"
REFERENCES,0.5414201183431953,"L∥W∥2/L
S2/L ≥L∥X†Y∥2/L
S2/L.
(58)"
REFERENCES,0.5428994082840237,"We also note that XX†Y = XX†XW = XW = Y. Therefore, X†Y is also feasible for the
problem (57). This indicates that Plin = L"
REFERENCES,0.5443786982248521,"2 ∥X†Y∥2/L
S2/L"
REFERENCES,0.5458579881656804,"On
the
other
hand,
for
a
feasible
point
(W1, . . . , WL)
for
Plin(t),
we
note
that
(W1/t, . . . , WL−2/t, WL−1, tL−2WL) is feasible for Plin(1). This implies that tL−2Plin(t) =
Plin(1), or equivalently, Plin(t) = t−(L−2)Plin(1). Recall that"
REFERENCES,0.5473372781065089,"Plin = min
t>0
L −2"
REFERENCES,0.5488165680473372,"2
t2 + t−(L−2)Plin(1) =L"
REFERENCES,0.5502958579881657,"2 (Plin(1))2/L .
(59)"
REFERENCES,0.5517751479289941,This implies that Plin(1) = ∥X†Y∥S2/L and
REFERENCES,0.5532544378698225,"Plin(t) = t−(L−2)∥X†Y∥S2/L.
(60)"
REFERENCES,0.5547337278106509,"For the dual problem Dlin(t) deﬁned in (38), we note that"
REFERENCES,0.5562130177514792,"∥ΛT XW1 . . . WL−2wL−1∥2
≤∥ΛT XW1 . . . WL−2∥2∥wL−1∥2"
REFERENCES,0.5576923076923077,"≤∥ΛT X∥2 L−2
Y"
REFERENCES,0.5591715976331361,"l=1
∥Wl∥2∥wL−1∥2"
REFERENCES,0.5606508875739645,"≤∥ΛT X∥2 L−2
Y"
REFERENCES,0.5621301775147929,"l=1
∥Wl∥F ∥wL−1∥2 =
tL−2∥ΛT X∥2. (61)"
REFERENCES,0.5636094674556213,"The equality can be achieved when Wl = tuluT
l+1 for l ∈[L −2], where ∥ul∥2 = 1 for l =
1, . . . , L −1. Speciﬁcally, we set uL−1 = wL−1 and let u0 as right singular vector corresponds to
the largest singular value of ΛT X. Therefore, the constraints on Λ is equivalent to"
REFERENCES,0.5650887573964497,"∥ΛT X∥2 ≤t−(L−2).
(62)
Thus, according to the Von Neumann’s trace inequality, it follows"
REFERENCES,0.5665680473372781,"tr(ΛT Y) = tr(λT XX†Y) ≤∥λT X∥2∥X†Y∥∗≤t−(L−2)∥X†Y∥∗.
(63)"
REFERENCES,0.5680473372781065,"As a result, we have Dlin(t) = t−(L−2)∥X†Y∥∗< t−(L−2)∥X†Y∥S2/L = Plin(t). Namely, the
duality gap exists for the standard neural network."
REFERENCES,0.5695266272189349,Under review as a conference paper at ICLR 2022
REFERENCES,0.5710059171597633,"B.4
PROOF OF PROPOSITION 3"
REFERENCES,0.5724852071005917,"For simplicity, we write WL−1,j = wcol
L−1,j and WL,j = wrow
L,j for j ∈[m]. For the j-th branch of
the parallel network, let ˆ
Wl,j = αl,jWl,j for l ∈[L]. Here αl,j > 0 for l ∈[L] and they satisﬁes
that QL
l=1 αl,j = 1 for j ∈[m]. Therefore, we have"
REFERENCES,0.5739644970414202,"XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = X ˆ
W1,j . . . ˆ
WL−2,j ˆwcol
L−1,j ˆwrow
L,j .
(64)"
REFERENCES,0.5754437869822485,"This implies that { ˆ
Wl,j}l∈[L],j∈[m] is also feasible for the problem (14). According to the the
inequality of arithmetic and geometric means, the objective function in (14) is lower bounded by 1
2 m
X j=1 L
X"
REFERENCES,0.5769230769230769,"l=1
α2
l,j∥Wl,j∥2
F ≥ m
X j=1 L 2 L
Y l=1"
REFERENCES,0.5784023668639053,"
α2/L
l,j ∥Wl,j∥2/L
F
 =L 2 m
X j=1 L
Y"
REFERENCES,0.5798816568047337,"l=1
∥Wl,j∥2/L
F
. (65)"
REFERENCES,0.5813609467455622,"The equality is achieved when αl,j ="
REFERENCES,0.5828402366863905,"QL
l=1 ∥Wl,j∥1/L
F
∥Wl,j∥F
for l ∈[L] and j ∈[m]. As the scaling"
REFERENCES,0.584319526627219,"operation does not change QL
l=1 ∥Wl,j∥2/L
F
, we can simply let ∥Wl,j∥F = 1 and the lower bound
becomes L"
PM,0.5857988165680473,"2
Pm
i=1 ∥WL,j∥2/L
F
= L"
PM,0.5872781065088757,"2
Pm
i=1 ∥wrow
L,j ∥2/L
2
. This completes the proof."
PM,0.5887573964497042,"B.5
PROOF OF PROPOSITION 4"
PM,0.5902366863905325,"We ﬁrst show that the problem (16) is equivalent to (17). The proof is analogous to the proof of
Proposition 3. For simplicity, we write WL−1,j = wcol
L−1,j and WL,j = wrow
L,j for j ∈[m]. Let
αl,j > 0 for l ∈[L] and they satisﬁes that QL
l=1 αl,j = 1 for j ∈[m]. Consider another parallel
network { ˆ
Wl,j}l∈[L],j∈[m] whose j-th branch is deﬁned by ˆ
Wl,j = αl,jWl,j for l ∈[L]. As
QL
l=1 αl,j = 1, we have"
PM,0.591715976331361,"XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = X ˆ
W1,j . . . ˆ
WL−2,j ˆwcol
L−1,j ˆwrow
L,j .
(66)"
PM,0.5931952662721893,"This implies that { ˆ
Wl,j}l∈[L],j∈[m] is also feasible for the problem (16). According to the the
inequality of arithmetic and geometric means, the objective function in (14) is lower bounded by 1
2 m
X j=1 L
X"
PM,0.5946745562130178,"l=1
αL
l,j∥Wl,j∥L
F ≥ m
X j=1 L 2 L
Y"
PM,0.5961538461538461,"l=1
(αl,j∥Wl,j∥F ) =L 2 m
X j=1 L
Y"
PM,0.5976331360946746,"l=1
∥Wl,j∥F . (67)"
PM,0.599112426035503,"The equality is achieved when αl,j ="
PM,0.6005917159763313,"QL
l=1 ∥Wl,j∥1/L
F
∥Wl,j∥F
for l ∈[L] and j ∈[m]. As the scaling"
PM,0.6020710059171598,"operation does not change QL
l=1 ∥Wl,j∥F , we can simply let ∥Wl,j∥F = 1 and the lower bound
becomes L"
PM,0.6035502958579881,"2
Pm
i=1 ∥WL,j∥F = L"
PM,0.6050295857988166,"2
Pm
i=1 ∥wrow
L,j ∥2. Hence, the problem (16) is equivalent to (17)."
PM,0.606508875739645,"For the problem (17), we consider the Lagrangian function"
PM,0.6079881656804734,"L(W1, . . . , WL) = L 2 m
X"
PM,0.6094674556213018,"j=1
∥wrow
L,j ∥2 + tr "
PM,0.6109467455621301,"ΛT (Y − m
X"
PM,0.6124260355029586,"j=1
XW1,j . . . Wcol
L−1,jWrow
L,j ) "
PM,0.613905325443787,".
(68)"
PM,0.6153846153846154,Under review as a conference paper at ICLR 2022
PM,0.6168639053254438,The primal problem is equivalent to
PM,0.6183431952662722,"P prl
lin =
min
W1,...,WL max
Λ L(W1, . . . , WL, Λ),"
PM,0.6198224852071006,"s.t. ∥Wl,j∥F ≤t, j ∈[ml], l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[mL−1],"
PM,0.621301775147929,"=
min
W1,...,WL−1 max
Λ min
WL L(W1, . . . , WL, Λ),"
PM,0.6227810650887574,"s.t. ∥Wl,j∥F ≤1, l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[m],"
PM,0.6242603550295858,"=
min
W1,...,WL−1 max
Λ tr(ΛT Y) − mL
X"
PM,0.6257396449704142,"j=1
I

∥ΛT XW1,j . . . WL−2,jwcol
L−1,j∥2 ≤L/2

,"
PM,0.6272189349112426,"s.t. ∥Wl,j∥F ≤1, l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[m]. (69)"
PM,0.628698224852071,The dual problem follows
PM,0.6301775147928994,"Dprl
lin = max
Λ tr(ΛT Y),"
PM,0.6316568047337278,"s.t. ∥ΛT XW1,j . . . WL−2,j∥2 ≤L/2,"
PM,0.6331360946745562,"∀∥Wl,j∥F ≤1, l ∈[L −2], ∥Wcol
L−1,j∥2 ≤1, j ∈[m]"
PM,0.6346153846153846,"= max
Λ tr(ΛT Y),"
PM,0.636094674556213,"s.t. ∥ΛT XW1 . . . WL−2wL−1∥2 ≤L/2,
∀∥Wi∥F ≤1, i ∈[L −2], ∥wL−1∥2 ≤1. (70)"
PM,0.6375739644970414,"B.6
PROOF OF THEOREM 2"
PM,0.6390532544378699,We can rewrite the dual problem as
PM,0.6405325443786982,"Dprl
lin = max
Λ tr(ΛT Y),"
PM,0.6420118343195266,"s.t. ∥ΛT XW1 . . . WL−2wL−1∥2 ≤L/2,
∀(W1, . . . , WL−2, wL−1) ∈Θ, (71)"
PM,0.643491124260355,where the set Θ is deﬁned as
PM,0.6449704142011834,"Θ = {(W1, . . . , WL−2, wL−1)|∥Wl∥F ≤1, l ∈[L −2], ∥wL−1∥2 ≤1}.
(72)"
PM,0.6464497041420119,"By writing θ = (W1, . . . , WL−2, wL−1), the bi-dual problem, i.e., the dual problem of (71), is
given by
min ∥µ∥TV,
s.t.
R"
PM,0.6479289940828402,"θ∈Θ XW1 . . . WL−2wL−1dµ (θ) = Y.
(73)"
PM,0.6494082840236687,"Here µ : Σ →RK is a signed vector measure, where Σ is a σ-ﬁeld of subsets of Θ and ∥µ∥TV is its
total variation. The formulation in (73) has inﬁnite width in each layer. According to Theorem 9 in
Appendix E, the measure µ in the integral can be represented by ﬁnitely many Dirac delta functions.
Therefore, there exists m∗< KN + 1 such that we can rewrite the problem (73) as min m∗
X"
PM,0.650887573964497,"j=1
∥wrow
L,j ∥2, s.t. m∗
X"
PM,0.6523668639053254,"j=1
XW1,j . . . WL−2,jwcol
L−1,jwrow
L,j = Y,"
PM,0.6538461538461539,"∥Wi,j∥F ≤1, l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[m∗]. (74)"
PM,0.6553254437869822,"Here the variables are Wl,j for l ∈[L −2] and j ∈[m∗], WL−1 and WL. This is equivalent to
(17). As the strong duality holds for the problem (71) and (73), the primal problem (17) is equivalent
to the dual problem (71) as long as m ≥m∗."
PM,0.6568047337278107,Under review as a conference paper at ICLR 2022
PM,0.658284023668639,"Now, we compute the optimal value of Dprl
lin . Similar to the proof of Theorem 1, we can show that
the constraints in the dual problem (71) is equivalent to"
PM,0.6597633136094675,"∥ΛT X∥2 ≤L/2.
(75)"
PM,0.6612426035502958,"Therefore, we have"
PM,0.6627218934911243,tr(ΛT Y) ≤∥λT X∥2∥X†Y∥∗≤L
PM,0.6642011834319527,"2 ∥X†Y∥∗.
(76)"
PM,0.665680473372781,"This implies that P prl
lin = Dprl
lin = L"
PM,0.6671597633136095,2 ∥X†Y∥∗.
PM,0.6686390532544378,"B.7
PROOF OF PROPOSITION 8"
PM,0.6701183431952663,We note that
PM,0.6715976331360947,"max
Λ tr(ΛT Y),"
PM,0.6730769230769231,"s.t. ∥ΛT XW1 . . . WL−2∥2 ≤1, ∥Wi∥F ≤t, i = l + 1, . . . , L −2"
PM,0.6745562130177515,"= max
Λ
min
Wj+1,...,WL−2 tr(ΛT Y), s.t. ∥ΛT XW1 . . . Wl∥2 ≤t−(L−2−l)
(77)"
PM,0.6760355029585798,"Therefore, we can rewrite D(l)
lin(t) as"
PM,0.6775147928994083,"D(l)
lin(t) =
min
W1,...Wl max
Λ tr(ΛT Y),"
PM,0.6789940828402367,"s.t. ∥ΛT XW1 . . . Wl∥2 ≤t−(L−2−l), ∥Wi∥F ≤t, i ∈[l],"
PM,0.6804733727810651,"=
min
W1,...Wl max
Λ t−(L−2−l) tr(ΛT Y),"
PM,0.6819526627218935,"s.t. ∥ΛT XW1 . . . Wl∥2 ≤1, ∥Wi∥F ≤t, i ∈[l]. (78)"
PM,0.6834319526627219,"From the equation (11), we note that"
PM,0.6849112426035503,"min
W1,...Wj max
Λ tr(ΛT Y)"
PM,0.6863905325443787,"s.t. ∥ΛT XW1 . . . Wj∥2 ≤1, ∥Wi∥F ≤t, i ∈[j], = min K
X"
PM,0.6878698224852071,"j=1
∥wl+2,j∥2,"
PM,0.6893491124260355,"s.t. ∥Wi∥F ≤t, i ∈[L −2], ∥wL−1,j∥2 ≤1, j ∈[mL−1],
XW1 . . . Wl+2 = Y"
PM,0.6908284023668639,=t−l∥X†Y∥S2/(l+2). (79)
PM,0.6923076923076923,This completes the proof.
PM,0.6937869822485208,"C
PROOFS OF MAIN RESULTS FOR RELU NETWORKS"
PM,0.6952662721893491,"C.1
PROOF OF PROPOSITION 5"
PM,0.6967455621301775,"For the problem of P(t), introduce the Lagrangian function"
PM,0.6982248520710059,"L(W1, W2, W3, Λ) = K
X"
PM,0.6997041420118343,"j=1
∥wrow
3,j ∥2 −tr(ΛT (((XW1)+W2)+W3 −Y)).
(80)"
PM,0.7011834319526628,"According to the convex duality of two-layer ReLU network, we have"
PM,0.7026627218934911,"PReLU(t) =
min
∥W1∥F ≤t,∥w2∥≤1 max
Λ tr(ΛT Y) −I(∥ΛT ((XW1)+w2)+∥2 ≤1)"
PM,0.7041420118343196,"=
min
∥W1∥F ≤t max
Λ
min
∥w2∥≤1 tr(ΛT Y) −I(∥ΛT ((XW1)+w2)+∥2 ≤1)"
PM,0.7056213017751479,"=
min
∥W1∥F ≤t max
Λ tr(ΛT Y), s.t. ∥ΛT v∥2 ≤1, ∀v ∈A(W1). (81)"
PM,0.7071005917159763,Under review as a conference paper at ICLR 2022
PM,0.7085798816568047,"By changing the min and max, we obtain the dual problem.
DReLU(t) = max
Λ tr(ΛT Y), s.t. ∥ΛT v∥2 ≤1, v ∈A(W1), ∀∥W1∥F ≤t.
(82)"
PM,0.7100591715976331,"The dual of the dual problem writes
min ∥µ∥TV,
s.t.
R"
PM,0.7115384615384616,"∥W1∥F ≤t,∥w2∥2≤1 ((XW1)+w2)+ dµ (W1, w2) = Y.
(83)"
PM,0.7130177514792899,"Here µ is a signed vector measure and ∥µ∥TV is its total variation. Similar to the proof of Proposition
1, we can ﬁnd a ﬁnite representation for the optimal measure and transform this problem to"
PM,0.7144970414201184,"min
{W1,j}m∗
j=1,W2∈Rm1×m∗,W3∈Rm∗×K K
X"
PM,0.7159763313609467,"j=1
∥w3,j∥2, s.t. m∗
X"
PM,0.7174556213017751,"j=1
((XW1,j)+w2,j)+w3,j = Y, ∥W1,j∥F ≤t, ∥w2,j∥2 ≤1. (84)"
PM,0.7189349112426036,Here m∗≤KN + 1. This completes the proof.
PM,0.7204142011834319,"C.2
PROOF OF THEOREM 3"
PM,0.7218934911242604,"For rank-1 data matrix that X = caT
0 , suppose that A1 = (XW1)+. It is easy to observe that
A1 = (c)+aT
1,+ + (−c)+aT
1,−,
Here we let a1,+ = (W1a0)+ and a0,−= (−W1a0)+."
PM,0.7233727810650887,"For a three-layer network, i.e., L = 3, suppose that λ∗is the optimal solution to the dual problem
DReLU(t). We consider the extreme points
arg
max
∥W1∥F ≤t,∥w2∥2≤1 |(λ∗)∗((XW1)+w2)+|.
(85)"
PM,0.7248520710059172,"For ﬁxed W1, because aT
1,+a1,−= 0, suppose that
w2 = u1a1,+ + u2a1,−+ u3r,
where rT a1,+ = rT a1,−= 0 and ∥r∥2 = 1. The maximization problem on w2 becomes
arg max
(λ∗)T (c)+∥a1,+∥2
2(u1)+ + (λ∗)T (−c)+∥aL−2,−∥2
2(u2)+"
PM,0.7263313609467456,"s.t. u2
1∥a1,+∥2
2 + u2
2∥a1,+∥2
2 + u2
3 ≤1.
If (λ∗)T (c)+ and (λ∗)T (−c)+ have different signs, then the optimal value is
max{|(λ∗)T (c)+|∥aL−2,+∥2, |(λ∗)T (−c)+|∥aL−2,−∥2}.
And the corresponding optimal wL−1
is wL−1
=
aL−2,+/∥aL−2,+∥or wL−1
=
aL−2,−/∥aL−2,−∥. Then, the problem becomes
arg max max{|(λ∗)T (c)+|∥aL−2,+∥2, |(λ∗)T (−c)+|∥aL−2,−∥2}.
We note that
max{∥aL−2,+∥2, ∥aL−2,−∥2} ≤∥WT
1 a0∥2 ≤∥W1∥2∥a0∥2 ≤t∗∥a0∥2.
Thus the optimal W1 follows
W1 = t∗sign(|(λ∗)T (c)+| −|(λ∗)T (−c)+|)ρ0ρT
1
Here ρ0 = a0/∥a0∥2 and ρ1 ∈Rml
+ satisﬁes ∥ρ1∥= 1. This also gives the optimal w2 = ρ1."
PM,0.727810650887574,"On the other hand, if (λ∗)T (c)+ and (λ∗)T (−c)+ have same signs, then, the optimal w2 follows"
PM,0.7292899408284024,"w2 =
|(λ∗)T (c)+|aL−2,+ + |(λ∗)T (−c)+|aL−2,−
p"
PM,0.7307692307692307,"((λ∗)T (c)+)2∥aL−2,+∥2
2 + ((λ∗)T (−c)+)2∥aL−2,−∥2
2
."
PM,0.7322485207100592,"The maximization problem is equivalent to
arg max((λ∗)T (c)+)2∥aL−2,+∥2
2 + ((λ∗)T (c)−)2∥aL−2,−∥2
2
By noting that
∥aL−2,+∥2
2 + ∥aL−2,+∥2
2 = ∥WT
1 a0∥2
2 ≤∥W1∥2
2∥a0∥2
2 ≤(t∗)2∥a0∥2
2,
the optimal W1 follows
W1 = tsign(|(λ∗)T (c)+| −|(λ∗)T (−c)+|)ρ0ρT
1 .
Here ρ0 = a0/∥a0∥2 and ρ1 ∈Rm1
+ satisﬁes ∥ρ1∥= 1. This also gives the optimal w2 = ρ1."
PM,0.7337278106508875,Under review as a conference paper at ICLR 2022
PM,0.735207100591716,"C.3
PROOF OF THEOREM 4"
PM,0.7366863905325444,"We restate Theorem 4 with details as follows.
Theorem 6 Let {X, Y} be a dataset such that XXT = In and Y has orthogonal columns. Then,
the optimal value of PReLU(t) and DReLU(t) are given by"
PM,0.7381656804733728,"PReLU(t) = t−1  
K
X j=1"
PM,0.7396449704142012,"
∥(yj)+∥2/3
2
+ ∥(−yj)+∥2/3
2

  3/2"
PM,0.7411242603550295,",
(86) and"
PM,0.742603550295858,"DReLU(t) = t−1
n
X"
PM,0.7440828402366864,"j=1
(∥(yj)+∥2 + ∥(−yj)+∥2) .
(87)"
PM,0.7455621301775148,"For the bi-dual formulation of DReLU(t) deﬁned in (25), the optimal weight matrices for each layer
can be constructed as"
PM,0.7470414201183432,"W1,r =
φ0,r
∥φ0,r∥2
φT
1,r, w2,r =
φ1,r
∥φ1,r∥,"
PM,0.7485207100591716,"for r = 1, . . . , 2K. Here (φ0,2j−1, φ0,2j) =
 
XT (yj)+, XT (−yj)+

, ∥φ1,r∥= t, φ1,r ∈Rm1
+ ."
PM,0.75,"For PReLU(t), the optimal weight matrices for each layer write W1 ="
"K
X",0.7514792899408284,"2K
X"
"K
X",0.7529585798816568,"r=1
gr
φ0,r
∥φ0,r∥2
φT
1,r, W2 =
φ1,1, . . . , φ1,2K

. Here"
"K
X",0.7544378698224852,"g2j+1 =
∥(yj)+∥1/3
2
PK
j=1

∥(yj)+∥2/3
2
+ ∥(−yj)+∥2/3
2
3/2 ,"
"K
X",0.7559171597633136,"g2j+2 =
∥(−yj)+∥1/3
2
PK
j=1

∥(yj)+∥2/3
2
+ ∥(−yj)+∥2/3
2
3/2 ."
"K
X",0.757396449704142,"We require that ∥φ1,j∥2 = 1 for j ∈[2K] and φT
1,iφ1,j = 0 for i ̸= j."
"K
X",0.7588757396449705,"Since XXT = In, we can characterize the set as"
"K
X",0.7603550295857988,"∪∥W1∥F ≤tA(W1) = {(z)+|z ∈RN, ∥z∥2 ≤t}.
(88)"
"K
X",0.7618343195266272,"Here A(W1) is deﬁned in (23). For one thing, for z ∈RN, ∥z∥2 ≤t, we can let W1 = XT zvT and
w2 = v, where v ∈Rm1
+
with ∥v∥2 = 1. Then, ∥W1∥F ≤t, ∥w2∥2 ≤1 and (XW1)+w2)+ =
(z)+. For another, for any ∥W1∥F ≤t and ∥w2∥≤1, we note that"
"K
X",0.7633136094674556,∥(XW1)+w2∥2 ≤∥(XW1)+∥2 ≤∥(XW1)+∥F ≤∥XW1∥F ≤∥W1∥F ≤t.
"K
X",0.764792899408284,"Therefore, ∪∥W ∥F ≤tA(W) = {(z)+|z ∈RN, ∥z∥2 ≤t}."
"K
X",0.7662721893491125,"Thus, the constraint on Λ in the dual problem (24) is equivalent to say that t∥(Λ∗)T (z)+∥2 ≤1 for
all z ∈RN satisfying ∥z∥2 ≤1. As Y has orthogonal columns, we note that"
"K
X",0.7677514792899408,"tr((Λ∗)T Y) = K
X"
"K
X",0.7692307692307693,"j=1
(λ∗
j)T ((yj)+ −(−yj)+) ≤t−1
n
X"
"K
X",0.7707100591715976,"j=1
(∥(yj)+∥2 + ∥(−yj)+∥2) ."
"K
X",0.772189349112426,"Suppose that (yj)+ ̸= 0 and (−yj)+ ̸= 0 for all j = 1, . . . , K. Then, the dual problem is minimized
by"
"K
X",0.7736686390532544,"Λ∗= t−1

(y1)+
∥(y1)+∥2
−
(−y1)+
∥(−y1)+∥2
, . . . ,
(yK)+
∥(yK)+∥2
−
(−yK)+
∥(−yK)+∥2 
."
"K
X",0.7751479289940828,"We can also verify that
t∥(Λ∗)T (z)+∥2 ≤1, ∀∥z∥2 ≤1.
(89)"
"K
X",0.7766272189349113,Under review as a conference paper at ICLR 2022
"K
X",0.7781065088757396,"Suppose that (z)+ = PK
j=1 αj,1
(yj)+
∥(yj)+∥2 + αj,2
(−yj)+
∥(−yj)+∥2 + r, where r ∈RN
+ is orthogonal to
(yj)+ and (−yj)+. Here αj,1, αj,2 ≥0. As ∥(z)+∥≤∥z∥2 ≤1, we have K
X"
"K
X",0.7795857988165681,"j=1
(α2
j,1 + α2
j,2) ≤1 −∥r∥2
2 ≤1."
"K
X",0.7810650887573964,We note that
"K
X",0.7825443786982249,"∥(Λ∗)T t(z)+∥2
2 = K
X"
"K
X",0.7840236686390533,"j=1
(αj,1 −αj,2)2 ≤ K
X"
"K
X",0.7855029585798816,"j=1
(α2
j,1 + α2
j,2) ≤1.
(90)"
"K
X",0.7869822485207101,"Thus, Λ∗satisﬁes the constraint (89)."
"K
X",0.7884615384615384,We can characterize the optimal layer weight to the bi-dual problem as the extreme points that solves
"K
X",0.7899408284023669,"arg max
∥W1∥F ≤t,∥w2∥2≤1
∥(Λ∗)T ((XW1)+w2)+∥2.
(91)"
"K
X",0.7914201183431953,"These extreme points correspond to the constraints
(Λ∗)T
t(yj)+
∥(yj)+∥2"
"K
X",0.7928994082840237,"2
≤1,
(Λ∗)T
t(−yj)+
∥(−yj)+∥2 2
≤1."
"K
X",0.7943786982248521,"In other words, the dual problem is equivalent to"
"K
X",0.7958579881656804,"DReLU(t) = max
Λ tr(ΛT Y),"
"K
X",0.7973372781065089,"s.t.
ΛT t∗(yj)+"
"K
X",0.7988165680473372,∥(yj)+∥2
"K
X",0.8002958579881657,"2
≤1,
ΛT t∗(−yj)+"
"K
X",0.8017751479289941,∥(−yj)+∥2
"K
X",0.8032544378698225,"2
≤1, 1 ≤j ≤K.
(92)"
"K
X",0.8047337278106509,"Now, we consider an arbitrary matrix W1 satisfying ∥W1∥F ≤t. Denote"
"K
X",0.8062130177514792,"P(W1) = max
Λ tr(ΛT Y), s.t. ∥ΛT v∥2 ≤1, ∀v ∈A(W1).
(93)"
"K
X",0.8076923076923077,"Suppose that Λ∗is the optimal solution. Then, we have"
"K
X",0.8091715976331361,"tr((Λ∗)T Y) = K
X"
"K
X",0.8106508875739645,"j=1
(λ∗
j)T ((yj)+ −(−yj)+) = K
X"
"K
X",0.8121301775147929,"j=1
(λ∗
j)T
(yj)+
∥(yj)+∥2
∥(yj)+∥2 − K
X"
"K
X",0.8136094674556213,"j=1
(λ∗
j)T
(−yj)+
∥(−yj)+∥2
∥(−yj)+∥2."
"K
X",0.8150887573964497,"For a vector λ ∈RN, deﬁne"
"K
X",0.8165680473372781,"g(λ; W1) = max
∥w2∥≤1 |λT ((XW1)+w2)+|.
(94)"
"K
X",0.8180473372781065,"For a given u ∈RN
+, we want to estimate"
"K
X",0.8195266272189349,"max |uT λ| s.t. g(λ; W1) ≤1.
(95)"
"K
X",0.8210059171597633,"The following lemma gives an upper bound.
Lemma 3 Suppose that u ∈RN satisfying ∥u∥2 = 1. Then, for arbitrary λ satisfying g(λ; W1) ≤
1, we have |uT λ| ≤1/g(u; W1)."
"K
X",0.8224852071005917,"Denote z2j+1 =
(yj)+
∥(yj)+∥2 and z2j+2 =
(−yj)+
∥(−yj)+∥2 . For simplicity, we write g(z; W1) = g(z). We
note that λ∗
j satisﬁes that g(λ∗
j) ≤1. According to Lemma 2, we have"
"K
X",0.8239644970414202,"P ∗(W1) = tr((Λ)T Y) ≤ K
X j=1"
"K
X",0.8254437869822485,∥(yj)+∥2
"K
X",0.8269230769230769,g(z2j+1) + ∥(yj)+∥2
"K
X",0.8284023668639053,"g(z2j+2) 
."
"K
X",0.8298816568047337,Under review as a conference paper at ICLR 2022
"K
X",0.8313609467455622,"This serves as an upper bound for P ∗(W1). We note that for u ∈RN
+, we have"
"K
X",0.8328402366863905,"g(u)2 = max
∥w2∥≤1 |uT ((XW1)+w2)+|2"
"K
X",0.834319526627219,"≤max
∥w2∥≤1 |uT (XW1)+w2|2"
"K
X",0.8357988165680473,"=∥uT ((XW1)+∥2
2
≤∥uT XW1∥2
2
≤tr(uuT XW1WT
1 XT )."
"K
X",0.8372781065088757,"Then, we have"
"K
X",0.8387573964497042,"2K
X"
"K
X",0.8402366863905325,"j=1
g(zj)2 ≤tr   "
"K
X",0.841715976331361,"
2K
X"
"K
X",0.8431952662721893,"j=1
zjzT
j "
"K
X",0.8446745562130178,"XW1WT
1 X "
"K
X",0.8461538461538461,"≤tr(XW1WT
1 XT ) = t2."
"K
X",0.8476331360946746,"According to the Holder’s inequality, we have
 
K
X"
"K
X",0.849112426035503,"j=1
(∥(yj)+∥2/g(z2j+1) + ∥(yj)+∥2/g(z2j+2))   2/3 "
"K
X",0.8505917159763313,"
2K
X"
"K
X",0.8520710059171598,"j=1
g(zj)2   1/3 ≥ K
X j=1"
"K
X",0.8535502958579881,"
∥(yj)+∥2/3
2
+ ∥(−yj)+∥2/3
2

."
"K
X",0.8550295857988166,"Thus, it follows
K
X"
"K
X",0.856508875739645,"j=1
(∥(yj)+∥2/g(z2j+1) + ∥(yj)+∥2/g(z2j+2)) ≥t−1  
K
X j=1"
"K
X",0.8579881656804734,"
∥(yj)+∥2/3
2
+ ∥(−yj)+∥2/3
2

  3/2 ."
"K
X",0.8594674556213018,The optimal g(zj) follows
"K
X",0.8609467455621301,"g(z2j+1) =
t∥(yj)+∥1/3
2
PK
j=1

∥(yj)+∥2/3
2
+ ∥(−yj)+∥2/3
2
3/2 ,"
"K
X",0.8624260355029586,"g(z2j+2) =
t∥(−yj)+∥1/3
2
PK
j=1

∥(yj)+∥2/3
2
+ ∥(−yj)+∥2/3
2
3/2 ."
"K
X",0.863905325443787,This bound can be achieved for W1 =
"K
X",0.8653846153846154,"2K
X"
"K
X",0.8668639053254438,"j=1
g(zj)zjφT
1,j,"
"K
X",0.8683431952662722,"where ∥φ1,j∥2 = 1 for j ∈[2K] and φT
1,iφ1,j = 0 for i ̸= j."
"K
X",0.8698224852071006,"C.4
PROOF OF PROPOSITION 6"
"K
X",0.871301775147929,"Analogous to the proof of Proposition 4, we can reformulate (28) into (29). The rest of the proof is
analogous to the proof of Proposition 4. For the problem (29), we consider the Lagrangian function"
"K
X",0.8727810650887574,"L(W1, . . . , WL) =L 2 m
X"
"K
X",0.8742603550295858,"j=1
∥wrow
L,j ∥2 + tr "
"K
X",0.8757396449704142,"ΛT (Y − m
X"
"K
X",0.8772189349112426,"j=1
(((XW1,j)+ . . . . . . WL−2,j)+wcol
L−1,j)+wrow
L,j ) "
"K
X",0.878698224852071,".
(96)"
"K
X",0.8801775147928994,Under review as a conference paper at ICLR 2022
"K
X",0.8816568047337278,The primal problem is equivalent to
"K
X",0.8831360946745562,"P prl
ReLU
=
min
W1,...,WL max
Λ L(W1, . . . , WL, Λ),"
"K
X",0.8846153846153846,"s.t. ∥Wl,j∥F ≤t, j ∈[ml], l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[mL−1],"
"K
X",0.886094674556213,"=
min
W1,...,WL−1 max
Λ min
WL L(W1, . . . , WL, Λ),"
"K
X",0.8875739644970414,"s.t. ∥Wl,j∥F ≤1, l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[m],"
"K
X",0.8890532544378699,"=
min
W1,...,WL−1 max
Λ tr(ΛT Y) − m
X"
"K
X",0.8905325443786982,"j=1
I

∥ΛT (((XW1,j)+ . . . WL−2,j)+wcol
L−1,j)+∥2 ≤L/2

,"
"K
X",0.8920118343195266,"s.t. ∥Wl,j∥F ≤1, l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[m].
(97)
By exchanging the order of min and max, the dual problem follows"
"K
X",0.893491124260355,"Dprl
ReLU = max
Λ tr(ΛT Y),"
"K
X",0.8949704142011834,"s.t. ∥ΛT (((XW1,j)+ . . . WL−2,j)+wcol
L−1,j)+∥2 ≤L/2,"
"K
X",0.8964497041420119,"∀∥Wl,j∥F ≤1, l ∈[L −2], ∥Wcol
L−1,j∥2 ≤1, j ∈[m]"
"K
X",0.8979289940828402,"= max
Λ tr(ΛT Y),"
"K
X",0.8994082840236687,"s.t. ∥ΛT (((XW1)+ . . . WL−2)+wL−1)+∥2 ≤L/2,
∀∥Wi∥F ≤1, i ∈[L −2], ∥wL−1∥2 ≤1. (98)"
"K
X",0.900887573964497,"C.5
PROOF OF THEOREM 5"
"K
X",0.9023668639053254,The proof is analogous to the proof of Theorem 2. We can rewrite the dual problem as
"K
X",0.9038461538461539,"Dprl
ReLU = max
Λ tr(ΛT Y),"
"K
X",0.9053254437869822,"s.t. ∥ΛT (((XW1)+ . . . WL−2)+wL−1)+∥2 ≤L/2,
∀(W1, . . . , WL−2, wL−1) ∈Θ, (99)"
"K
X",0.9068047337278107,where the set Θ is deﬁned as
"K
X",0.908284023668639,"Θ = {(W1, . . . , WL−2, wL−1)|∥Wl∥F ≤1, l ∈[L −2], ∥wL−1∥2 ≤1}.
(100)"
"K
X",0.9097633136094675,"By writing θ = (W1, . . . , WL−2, wL−1), the bi-dual problem, i.e., the dual problem of (71), is
given by
min ∥µ∥TV,
s.t.
R"
"K
X",0.9112426035502958,"θ∈Θ(((XW1)+ . . . WL−2)+wL−1)+dµ (θ) = Y.
(101)"
"K
X",0.9127218934911243,"Here µ : Σ →RK is a signed vector measure, where Σ is a σ-ﬁeld of subsets of Θ and ∥µ∥TV is its
total variation. The formulation in (101) has inﬁnite width in each layer. According to Theorem 9 in
Appendix E, the measure µ in the integral can be represented by ﬁnitely many Dirac delta functions.
Therefore, there exists m∗≤KN + 1 such that we can rewrite the problem (101) as min m∗
X"
"K
X",0.9142011834319527,"j=1
∥wrow
L,j ∥2, s.t. m∗
X"
"K
X",0.915680473372781,"j=1
(((XW1,j)+ . . . WL−2,j)+wcol
L−1,j)+wrow
L,j = Y,"
"K
X",0.9171597633136095,"∥Wi,j∥F ≤1, l ∈[L −2], ∥wcol
L−1,j∥2 ≤1, j ∈[m∗]. (102)"
"K
X",0.9186390532544378,"Here the variables are Wl,j for l ∈[L−2] and j ∈[m∗], WL−1 and WL. This is equivalent to (29).
As the strong duality holds for the problem (99) and (101), the primal problem (29) is equivalent to
the dual problem (71) as long as m ≥m∗."
"K
X",0.9201183431952663,Under review as a conference paper at ICLR 2022
"K
X",0.9215976331360947,"D
PROOFS OF AUXILIARY RESULTS"
"K
X",0.9230769230769231,"D.1
PROOF OF LEMMA 1"
"K
X",0.9245562130177515,"Denote a ∈Rn such that ai = Aii and denote b ∈Rn such that bi = λi(A). We can show that a is
majorized by b, i.e., for k ∈[n −1], we have k
X"
"K
X",0.9260355029585798,"i=1
a(i) ≤ k
X"
"K
X",0.9275147928994083,"i=1
b(i),
(103)"
"K
X",0.9289940828402367,"and Pn
i=1 ai = Pn
i=1 bi. Here a(i) is the i-th largest entry in a. We ﬁrst note that n
X"
"K
X",0.9304733727810651,"i=1
Aii = tr(A) = n
X"
"K
X",0.9319526627218935,"i=1
λi(A)."
"K
X",0.9334319526627219,"On the other hand, for k ∈[n −1], we have k
X"
"K
X",0.9349112426035503,"i=1
a(i) =
max
v∈Rn,vi∈{0,1},1T v=k vT a"
"K
X",0.9363905325443787,"=
max
v∈Rn,vi∈{0,1},1T v=k tr(diag(v)Adiag(v))"
"K
X",0.9378698224852071,"≤
max
V ∈Rk×n,V V T =I tr(V AV T ) = k
X"
"K
X",0.9393491124260355,"i=1
λi(A) = k
X"
"K
X",0.9408284023668639,"i=1
b(i). (104)"
"K
X",0.9423076923076923,"Therefore, a is majorized by b. As f(x) = −xp is a convex function, according to the Karamata’s
inequality, we have
n
X"
"K
X",0.9437869822485208,"i=1
f(ai) ≤ n
X"
"K
X",0.9452662721893491,"i=1
f(bi)."
"K
X",0.9467455621301775,This completes the proof.
"K
X",0.9482248520710059,"D.2
PROOF OF LEMMA 2"
"K
X",0.9497041420118343,"According to the min-max principle for singular value, we have
σi(W) =
min
dim(S)=d−i+1
max
x∈S,∥x∥2=1 ∥Wx∥2."
"K
X",0.9511834319526628,"As P is a projection matrix, for arbitrary x ∈Rd, we have ∥PWx∥2 ≤∥Wx∥2. Therefore, we have
max
x∈S,∥x∥2=1 ∥PWx∥2 ≤
max
x∈S,∥x∥2=1 ∥Wx∥2."
"K
X",0.9526627218934911,This completes the proof.
"K
X",0.9541420118343196,"D.3
PROOF OF LEMMA 3"
"K
X",0.9556213017751479,Suppose that λ(u) is optimal solution to
"K
X",0.9571005917159763,"arg maxv|uT λ| s.t. g(λ; W1) ≤1.
(105)
Denote ne is the number of extreme points of"
"K
X",0.9585798816568047,"arg
max
v∈A(W1) |(λ(u))T v|."
"K
X",0.9600591715976331,"Denote v1, . . . , vne as extreme points of (105). We note that (105) is the dual problem for a scalar
output three-layer neural network. By applying the previous duality theorem, we can write u =
Pne
i=1 aivi, where vi ∈A(W1). We also have |uT λ(u)| = Pne
i=1 |ai|. Besides, we note that"
"K
X",0.9615384615384616,"1 = ∥u∥2
2 = ne
X"
"K
X",0.9630177514792899,"i=1
aiuT vi ≤ ne
X"
"K
X",0.9644970414201184,"i=1
|ai|g(u; W1) = |uT λ(u)|g(u; W1)."
"K
X",0.9659763313609467,This completes the proof.
"K
X",0.9674556213017751,Under review as a conference paper at ICLR 2022
"K
X",0.9689349112426036,"E
CARATHEODORY’S THEOREM AND FINITE REPRESENTATION"
"K
X",0.9704142011834319,"We ﬁrst review a generalized version of Caratheodory’s theorem introduced in (Rosset et al., 2007)."
"K
X",0.9718934911242604,"Theorem 7 Let µ be a positive measure supported on a bounded subset D ⊆RN. Then, there
exists a measure ν whose support is a ﬁnite subset of D, {z1, . . . , zk}, with k ≤N + 1 such that Z"
"K
X",0.9733727810650887,"D
zdµ(z) = k
X"
"K
X",0.9748520710059172,"i=1
zidν(zi),
(106)"
"K
X",0.9763313609467456,and ∥µ∥TV = ∥ν∥TV.
"K
X",0.977810650887574,We can generalize this theorem to signed vector measures.
"K
X",0.9792899408284024,"Theorem 8 Let µ : Σ →RK be a signed vector measure supported on a bounded subset D ⊆RN.
Here Σ is a σ-ﬁeld of subsets of D. Then, there exists a measure ν whose support is a ﬁnite subset
of D, {z1, . . . , zk}, with k ≤KN + 1 such that Z"
"K
X",0.9807692307692307,"D
zdµ(z) = k
X"
"K
X",0.9822485207100592,"i=1
zidν(zi),
(107)"
"K
X",0.9837278106508875,and ∥ν∥TV = ∥µ∥TV.
"K
X",0.985207100591716,"PROOF Let µ be a signed vector measure supported on a bounded subset D ⊆RN. Consider the
extended set ˜D = {zuT |z ∈D, u ∈RK, ∥u∥2 = 1}. Then, µ corresponds to a scalar-valued
measure ˜µ on the set ˜D and ∥µ∥TV = ∥˜µ∥TV. We note that ˜D is also bounded. Therefore, by
applying Theorem 7 to the set ˜D and the measure ˜µ, there exists a measure ˜ν whose support is a
ﬁnite subset of ˜D, {z1uT
1 , . . . , zkuT
k }, with k ≤KN + 1 such that Z"
"K
X",0.9866863905325444,"˜
D
Zd˜µ(Z) = k
X"
"K
X",0.9881656804733728,"i=1
ziuT
i d˜ν(ziuT
i ),
(108)"
"K
X",0.9896449704142012,"and ∥˜µ∥TV = ∥˜ν∥TV. We can deﬁne ν as the signed vector measure whose support is a ﬁnite subset
{z1, . . . , zk} and dν(zi) = uid˜(ziui). Then, ∥ν∥TV = ∥˜ν∥TV = ∥˜µ∥TV = ∥µ∥TV. This completes
the proof."
"K
X",0.9911242603550295,Now we are ready to present the theorem about the ﬁnite representation of a signed-vector measure.
"K
X",0.992603550295858,"Theorem 9 Suppose that θ is the parameter with a bounded domain Θ ⊆Rp and φ(X, θ) :
RN×d × Θ →RN is an embedding of the parameter into the feature space. Consider the following
optimization problem"
"K
X",0.9940828402366864,"min ∥µ∥TV, s.t.
Z"
"K
X",0.9955621301775148,"Θ
φ(X, θ)dµ(θ) = Y.
(109)"
"K
X",0.9970414201183432,"Assume that an optimal solution to (109) exists. Then, there exists an optimal solution ˆµ supported
on at most KN + 1 features in Θ."
"K
X",0.9985207100591716,"PROOF Let ˆµ be an optimal solution to (109). We can deﬁne a measure ˆP on RN as the push-
forward of ˆµ by ˆP(B) = ˆµ({θ|φ(X, θ) ∈B}). Denote D = {φ(X, θ)|θ ∈Θ}. We note that ˆP
is supported on D and D is bounded. By applying Theorem (8) to the set D and the measure ˆP,
we can ﬁnd a measure Q whose support is a ﬁnite subset of D, {z1, . . . , zk} with k ≤2K(n + 1).
For each zi ∈D, we can ﬁnd θi such that φ(X, θi) = zi. Then, ˜µ = Pk
i=1 δ(θ −θi)dQ(zi) is an
optimal solution to (109) with at most 2C(N + 1) features and ∥˜µ∥TV = ∥µ∥TV. Here δ(·) is the
Dirac delta measure."
