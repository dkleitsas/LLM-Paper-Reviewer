Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011918951132300357,"We propose a new general form of image-level supervision for semantic segmenta-
1"
ABSTRACT,0.0023837902264600714,"tion based on approximate targets for the relative size of segments. At each training
2"
ABSTRACT,0.003575685339690107,"image, such targets are represented by a categorical distribution for the “expected”
3"
ABSTRACT,0.004767580452920143,"average prediction over the image pixels. We motivate the zero-avoiding variant of
4"
ABSTRACT,0.0059594755661501785,"KL divergence as a general training loss for any segmentation architecture leading
5"
ABSTRACT,0.007151370679380214,"to quality on par with the full pixel-level supervision. However, our image-level
6"
ABSTRACT,0.00834326579261025,"supervision is significantly less expensive, it needs to know only an approximate
7"
ABSTRACT,0.009535160905840286,"fraction of an image occupied by each class. Such estimates are easy for a human
8"
ABSTRACT,0.010727056019070322,"annotator compared to pixel-accurate labeling. Our loss shows significant robust-
9"
ABSTRACT,0.011918951132300357,"ness to size target errors, which may even improve the generalization quality. The
10"
ABSTRACT,0.013110846245530394,"proposed size targets can be seen as an extension of the standard class tags, which
11"
ABSTRACT,0.014302741358760428,"correspond to non-zero size targets in each image. Using only a minimal amount
12"
ABSTRACT,0.015494636471990465,"of extra information, our supervision improves and simplifies the training. It works
13"
ABSTRACT,0.0166865315852205,"on standard segmentation architectures as is, unlike tag-based methods requiring
14"
ABSTRACT,0.017878426698450536,"complex specialized modifications and multi-stage training.
15"
INTRODUCTION,0.01907032181168057,"1
Introduction
16"
INTRODUCTION,0.02026221692491061,"Our image-level supervision approach applies to any semantic segmentation model and does not
17"
INTRODUCTION,0.021454112038140644,"require any modification. It can be technically described in one paragraph, as follows. Soft-max
18"
INTRODUCTION,0.02264600715137068,"prediction Sp = (S1
p, . . . , SK
p ) at any pixel p is a categorical distribution over K classes, including
19"
INTRODUCTION,0.023837902264600714,"background. At any image, the average prediction over all image pixels, denoted by set Ω, is
20"
INTRODUCTION,0.025029797377830752,¯S := 1 |Ω| X
INTRODUCTION,0.026221692491060787,"p∈Ω
Sp
(1)"
INTRODUCTION,0.027413587604290822,"where ¯S = ( ¯S1, . . . , ¯SK) is also a categorical distribution over K classes. It is an image-level
21"
INTRODUCTION,0.028605482717520857,"prediction of the relative or normalized sizes (volume, area, or cardinality) of the objects in the image.
22"
INTRODUCTION,0.029797377830750895,"We assume that training images have approximate size targets represented by categorical distributions
23"
INTRODUCTION,0.03098927294398093,"v = (vk)K
k=1, e.g. v = (0, .15, 0, . . . , 0, .75) for the middle image in Fig. 1 if “bird” is the second
24"
INTRODUCTION,0.03218116805721097,"class and “background” is the last. This representation also applies to multi-label images. For each
25"
INTRODUCTION,0.033373063170441,"training image, our size-target loss
26"
INTRODUCTION,0.03456495828367104,"Lsize
=
KL(v∥¯S)
=
X"
INTRODUCTION,0.03575685339690107,"k
vk ln vk"
INTRODUCTION,0.03694874851013111,"¯Sk
(2)"
INTRODUCTION,0.03814064362336114,"is based on Kullback–Leibler (KL) divergence. Figure 2(b) shows some results for a generic
27"
INTRODUCTION,0.03933253873659118,"segmentation network (ResNet101 [4] backbone) trained on PASCAL [5] using only image-level
28"
INTRODUCTION,0.04052443384982122,"supervision with approximate size targets (8% mean relative errors). Our total loss is very simple: it
29"
INTRODUCTION,0.041716328963051254,"combines size-target loss (2) and a common CRF loss (3) [6].
30"
INTRODUCTION,0.04290822407628129,"Tag
Bounding box
Full supervision"
SEC,0.04410011918951132,"20.0 sec 
47.1 sec 
239.7 sec"
SEC,0.04529201430274136,78.2% (R101) Point
SEC,0.04648390941597139,"23.3 sec 
66.6% (R101) 
62.7% (WR38) 
mIOU:
74.9% (R101)"
SEC,0.04767580452920143,Size target (ours)
SEC,0.04886769964243146,"36.2 sec 
72.6% (R101)"
SEC,0.050059594755661505,"bird:15%
bird
sec. per image:"
SEC,0.05125148986889154,"20 sec
32 sec
30 sec
151 sec
1045 sec"
SEC,0.052443384982121574,"Figure 1: Supervision types for segmentation: labeling speed and accuracy on PASCAL. The top-left
corner of each image shows its estimated labeling time based on observed instances. The table
shows per-image labeling times averaged over the data and mean Intersection-over-Union (mIoU) for
comparable end-to-end methods with similar ResNet backbones (ResNet101 or WideResNet38 [1]),
for fairness. We obtained mIoU scores, except for the “tag” and “box” scores from [2] and [3]. Our
supplemental materials detail evaluation of the labeling times and mIoU. For completeness, Tab.2
includes more complex architectures and multi-stage systems, e.g. for tags. This paper focuses on
standard segmentation architectures for size supervision."
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.05363528009535161,"1.1
Overview of weakly-supervised segmentation
31"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.054827175208581644,"By weakly-supervised semantic segmentation we refer to all methods that do not use full pixel-
32"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.05601907032181168,"precise ground truth (GT) masks for training. Such full supervision is overwhelmingly expensive for
33"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.057210965435041714,"segmentation and is unrealistic for many practical purposes, see the right image in Fig. 1. There are
34"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.058402860548271755,"many forms of weak supervision for semantic segmentation, e.g. based on partial pixel-level ground
35"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.05959475566150179,"truth defined by “seeds” [6, 7], boxes [3], or image-level class-tags [2, 8, 9], see Fig. 1. It is also
36"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.060786650774731825,"common to incorporate self-supervision based on various augmentation ideas and contrastive losses
37"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.06197854588796186,"[10–12].
38"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.0631704410011919,"Lack of supervision also motivates unsupervised loss functions such as standard old-school regulariza-
39"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.06436233611442194,"tion objectives for low-level segmentation or clustering. For example, many methods [13, 14, 12] use
40"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.06555423122765197,"variants of K-means objective (squared errors) enforcing the compactness of each class representation.
41"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.066746126340882,"It is also very common to use CRF-based pairwise loss functions [6, 7] that encourage segment shape
42"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.06793802145411204,"regularity and alignment to intensity contrast edges in each image [15]. The last point addresses the
43"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.06912991656734208,"well-known limitation of standard segmentation networks that often output low-resolution segments.
44"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07032181168057211,"Intensity contrast edges on the high-resolution input image is a good low-level cue of an object
45"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07151370679380215,"boundary and it can improve the details and localization of the semantic segments.
46"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07270560190703218,"Conditional or Markov random fields (CRF or MRF) are common basic examples of pairwise
47"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07389749702026222,"graphical models. The corresponding unsupervised loss functions can be formulated for continuous
48"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07508939213349225,"soft-max predictions Sp produced by segmentation networks, e.g. [6, 7, 9]. Thus, it is natural to use
49"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07628128724672228,"relaxations of the standard discrete CRF/MRF models, such as Potts [16] or its dense-CRF version
50"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07747318235995232,"[17]. We use a bilinear relaxation of the general Potts model
51"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07866507747318235,"Lcrf(S)
=
X"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.07985697258641239,"k
(1 −Sk)⊤WSk
(3)"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.08104886769964244,"where S := (Sp | p ∈Ω) is a field of all pixel-level soft-max predictions Sp in a given image, and
52"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.08224076281287247,"Sk := (Sk
p | p ∈Ω) is a vector of all pixel predictions specifically for class k. Matrix W = [wpq]
53"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.08343265792610251,"typically represents some given non-negative affinities wpq between pairs of pixels p, q ∈Ω. It is
54"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.08462455303933254,"easy to interpret loss (3) assuming, for simplicity, that all pixels have confident one-hot predictions
55"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.08581644815256258,"Sp so that each Sk is a binary indicator vector for segment k. The loss sums all weights wpq between
56"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.08700834326579261,"the pixels in different segments. Thus, the weights are interpreted as discontinuity penalties. The loss
57"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.08820023837902265,"minimizes the discontinuity costs [16].
58"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.08939213349225268,"In practice, affinity weights wpq are set close to 1 if two neighboring pixels p, q have similar intensities,
59"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.09058402860548272,"and weight wpq is set close to zero either when two pixels are far from each other on the pixel grid or
60"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.09177592371871275,"if they have largely different intensities [6, 16, 17]. The affinity matrix W could be arbitrarily dense
61"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.09296781883194279,"or sparse, e.g. many zeros when representing a 4-connected pixel grid. The non-zero discontinuity
62"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.09415971394517282,costs between neighboring pixels are often set by a Gaussian kernel wpq = exp −∥Ip−Iq∥2
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.09535160905840286,"2σ2
of given
63"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.09654350417163289,"bandwidth σ, which works as a soft threshold for detecting high-contrast intensity edges in the image.
64"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.09773539928486293,"Thus, loss (3) encourages both the alignment of the segmentation boundary to contrast edges in the
65"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.09892729439809297,"(high-resolution) input image and the shortness/regularity of this boundary.
66"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.10011918951132301,"(b)
(a)
(c)"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.10131108462455304,"Figure 2: Semantic segmentation with standard DeepLabV3+(R101) segmentation models [18]:
PASCAL validation results for training with (a) log-barrier (9) using class tags, (b) KL-divergence
(2) using our approximate size targets, (c) cross-entropy with full (ground truth mask) supervision."
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.10250297973778308,"Weakly supervised segmentation methods may also use partial pixel-level ground truth where only
67"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.10369487485101311,"some subset Seeds ⊂Ωof image pixels has class labels [6, 7, 9]. In this case it is common to use
68"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.10488676996424315,"partial cross-entropy loss
69"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.10607866507747318,"Lpce(S)
=
−
X"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.10727056019070322,"p∈Seeds
ln Syp
p
(4)"
OVERVIEW OF WEAKLY-SUPERVISED SEGMENTATION,0.10846245530393325,"where yp is the ground truth label at a seed pixel p.
70"
RELATED BALANCING LOSSES,0.10965435041716329,"1.2
Related balancing losses
71"
RELATED BALANCING LOSSES,0.11084624553039332,"Segmentation and classification methods often use “balancing” losses. In the context of classification,
72"
RELATED BALANCING LOSSES,0.11203814064362336,"image-level predictions can be balanced over the whole training data. For segmentation problems,
73"
RELATED BALANCING LOSSES,0.11323003575685339,"pixel-level predictions can be balanced within each training image. Our loss is an example of size
74"
RELATED BALANCING LOSSES,0.11442193087008343,"balancing. Below we review some examples of related balancing loss functions used in prior work.
75"
RELATED BALANCING LOSSES,0.11561382598331346,"Fully supervised classification. It is common to modify the standard cross-entropy loss to account
76"
RELATED BALANCING LOSSES,0.11680572109654351,"for unbalanced training data where some classes are represented more than others. One common
77"
RELATED BALANCING LOSSES,0.11799761620977355,"example is weighted cross-entropy, e.g. defined in [19] for image-level predictions Si as
78"
RELATED BALANCING LOSSES,0.11918951132300358,"Lwce(S)
=
−
X"
RELATED BALANCING LOSSES,0.12038140643623362,"i∈D
wyi ln Syi
i
(5)"
RELATED BALANCING LOSSES,0.12157330154946365,"where class weights wk ∝
1
1−βvk are motivated as a re-balancing factor based on the class distribution
79"
RELATED BALANCING LOSSES,0.12276519666269368,"v in the training dataset D and β is a hyper-parameter. In the fully supervised setting, the purpose
80"
RELATED BALANCING LOSSES,0.12395709177592372,"of re-weighting cross-entropy is not to make the predictions even closer to the known labels, but to
81"
RELATED BALANCING LOSSES,0.12514898688915377,"decrease over-fitting to over-represented classes, which improves the model’s generality.
82"
RELATED BALANCING LOSSES,0.1263408820023838,"Unsupervised classification. In the context of clustering with soft-max models [20, 21] it is common
83"
RELATED BALANCING LOSSES,0.12753277711561384,"to use fairness loss encouraging equal-size clusters. In this case, there is no ground truth and
84"
RELATED BALANCING LOSSES,0.12872467222884387,"fairness is one of the discriminative properties enforced by the total loss in order to improve the model
85"
RELATED BALANCING LOSSES,0.1299165673420739,"predictions on unlabeled training data. The fairness was motivated by information-theoretic arguments
86"
RELATED BALANCING LOSSES,0.13110846245530394,"in [20] deriving it as a negative entropy of the data-set-level average prediction ˆS :=
1
|D|
P"
RELATED BALANCING LOSSES,0.13230035756853398,"i∈D Si
87"
RELATED BALANCING LOSSES,0.133492252681764,"for dataset D
88"
RELATED BALANCING LOSSES,0.13468414779499405,"Lfair( ˆS)
=
−H( ˆS)
≡
X"
RELATED BALANCING LOSSES,0.13587604290822408,"k
ˆSk ln ˆSk c=
X"
RELATED BALANCING LOSSES,0.13706793802145412,"k
ˆSk ln
ˆSk"
RELATED BALANCING LOSSES,0.13825983313468415,"1/K ≡KL( ˆS∥u)
(6)"
RELATED BALANCING LOSSES,0.1394517282479142,where u = ( 1
RELATED BALANCING LOSSES,0.14064362336114422,"K , . . . , 1"
RELATED BALANCING LOSSES,0.14183551847437426,"K ) is a uniform categorical distribution, and symbol
c= indicates that the equality
89"
RELATED BALANCING LOSSES,0.1430274135876043,"is up to some additive constant independent of ˆS. Perona et al. [21] pointed out the equivalent KL-
90"
RELATED BALANCING LOSSES,0.14421930870083433,"divergence formulation of the fairness in (6) and generalized it to a balanced partitioning constraint
91"
RELATED BALANCING LOSSES,0.14541120381406436,"Lbal( ˆS)
=
KL( ˆS∥v)
(7)"
RELATED BALANCING LOSSES,0.1466030989272944,"with any given prior distribution v that could be different from uniform.
92"
RELATED BALANCING LOSSES,0.14779499404052443,"Semantic segmentation with image-level supervision. Most weakly-supervised semantic segmenta-
93"
RELATED BALANCING LOSSES,0.14898688915375446,"tion methods use losses based on segment sizes. This is particularly true for image-level supervision
94"
RELATED BALANCING LOSSES,0.1501787842669845,"techniques [2, 9, 22, 23]. Clearly, segments for tag classes should have positive sizes, and segments
95"
RELATED BALANCING LOSSES,0.15137067938021453,"for non-tag classes should have zero sizes.
96"
RELATED BALANCING LOSSES,0.15256257449344457,"Similarly to our paper, size-based constraints are often defined for the image-level average prediction
97"
RELATED BALANCING LOSSES,0.1537544696066746,"¯S, see (1), computed from pixel-level predictions Sp. Many generalized forms of pixel-prediction
98"
RELATED BALANCING LOSSES,0.15494636471990464,"averaging can be found in the literature, where they are often referred to as prediction pooling. Some
99"
RELATED BALANCING LOSSES,0.15613825983313467,"decay parameter often provides a wide spectrum of options from basic averaging to max-pooling.
100"
RELATED BALANCING LOSSES,0.1573301549463647,"While the specific form of pooling matters, for simplicity, we discuss the corresponding balancing
101"
RELATED BALANCING LOSSES,0.15852205005959474,"loss functions assuming basic average prediction ¯S in (1).
102"
RELATED BALANCING LOSSES,0.15971394517282478,"One of the earliest works on tag-supervised segmentation [9] uses log-barriers to “expand” tag
103"
RELATED BALANCING LOSSES,0.16090584028605484,"objects in each training image and to “suppress” the non-tag objects. Assuming image tags T, their
104"
RELATED BALANCING LOSSES,0.16209773539928488,"suppression loss is defined as
105"
RELATED BALANCING LOSSES,0.1632896305125149,"Lsuppress( ¯S)
∝
−
X"
RELATED BALANCING LOSSES,0.16448152562574495,"k̸∈T
ln(1 −¯Sk)
(8)"
RELATED BALANCING LOSSES,0.16567342073897498,"encouraging each non-tag class to have zero average prediction ¯Sk, which implies zero predictions
106"
RELATED BALANCING LOSSES,0.16686531585220502,"Sk
p at each pixel. Their expansion loss
107"
RELATED BALANCING LOSSES,0.16805721096543505,"Lexpand( ¯S)
∝
−
X"
RELATED BALANCING LOSSES,0.16924910607866508,"k∈T
ln ¯Sk.
(9)"
RELATED BALANCING LOSSES,0.17044100119189512,"encourages positive average predictions ¯Sk and non-trivial tag class segments.
108"
RELATED BALANCING LOSSES,0.17163289630512515,"We observe that the expansion loss (9) may have a bias to equal-size segments, as particularly evident
109"
RELATED BALANCING LOSSES,0.1728247914183552,"in the case of average predictions. Indeed, (9) implies
110"
RELATED BALANCING LOSSES,0.17401668653158522,"Lexpand( ¯S)
∝
KL(uT∥¯S)
(10)
which is a special case of our size loss (2) when the size target v = uT is a uniform distribution over
111"
RELATED BALANCING LOSSES,0.17520858164481526,"tag classes. The intention of the log barrier loss (9) is to push image-level size prediction ¯S from
112"
RELATED BALANCING LOSSES,0.1764004767580453,"the boundaries of the probability simplex ∆K corresponding to the zero-level for the tag classes
113"
RELATED BALANCING LOSSES,0.17759237187127533,"T. Figure 2(a) shows the results for training based on the total loss combining CRF loss (3) with
114"
RELATED BALANCING LOSSES,0.17878426698450536,"the log-barrier loss (9). Its unintended bias to equal-size segments (10) is obvious. Note that the
115"
RELATED BALANCING LOSSES,0.1799761620977354,"mentioned decay parameter used for generalized average predictions should reduce such bias.
116"
RELATED BALANCING LOSSES,0.18116805721096543,"Alternatively, it may be safer to use barriers for ¯S like
117"
RELATED BALANCING LOSSES,0.18235995232419547,"Lflat
=
−
X"
RELATED BALANCING LOSSES,0.1835518474374255,"k∈T
ln max{ ¯Sk, ϵ}
(11)"
RELATED BALANCING LOSSES,0.18474374255065554,"that have flat bottoms to avoid unintended bias to some specific size target inside the probability
118"
RELATED BALANCING LOSSES,0.18593563766388557,"simplex ∆K. Similar thresholded barriers are common [22].
119"
CONTRIBUTIONS,0.1871275327771156,"1.3
Contributions
120"
CONTRIBUTIONS,0.18831942789034564,"In general, it would be great to have effective image-level supervision for segmentation that only uses
121"
CONTRIBUTIONS,0.18951132300357568,"barriers like (9) or (11) since they do not require any specific size targets. This corresponds to tag-only
122"
CONTRIBUTIONS,0.1907032181168057,"supervision. However, our empirical results for semantic segmentation using such barriers were
123"
CONTRIBUTIONS,0.19189511323003575,"poor and comparable with those in [9]. A number of more recent semantic segmentation methods
124"
CONTRIBUTIONS,0.19308700834326578,"for tag-level supervision have considerably improved such results [12, 24–30], but they introduce
125"
CONTRIBUTIONS,0.19427890345649582,"significantly more complex multi-stage training procedures and various architectural modifications,
126"
CONTRIBUTIONS,0.19547079856972585,"which makes such methods hard to replicate, generalize, or to understand the results. We are focused
127"
CONTRIBUTIONS,0.1966626936829559,"on general easy-to-understand end-to-end training methods. Our main contributions are:
128"
CONTRIBUTIONS,0.19785458879618595,"• We propose and evaluate a new general form of weak supervision, size targets. The size-
129"
CONTRIBUTIONS,0.19904648390941598,"target supervision can be approximate and is relatively easy to get from human annotators.
130"
CONTRIBUTIONS,0.20023837902264602,"• We propose the zero-avoiding variant of KL divergence as a general training loss, allowing
131"
CONTRIBUTIONS,0.20143027413587605,"our end-to-end size-target approach to be integrated with any segmentation architecture.
132"
CONTRIBUTIONS,0.2026221692491061,"• Comprehensive experiments with our size-target method demonstrate state-of-the-art perfor-
133"
CONTRIBUTIONS,0.20381406436233612,"mance across multiple datasets using standard segmentation models typically employed for
134"
CONTRIBUTIONS,0.20500595947556616,"full supervision, without any architectural modifications.
135"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2061978545887962,"2
Size-target loss and its properties
136"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.20738974970202623,"Our proposed total loss is very simple
137"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.20858164481525626,"Ltotal := Lsize + Lcrf
(12)"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2097735399284863,"where the two terms are our size-target loss (2) and standard CRF loss (3). The core new com-
138"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.21096543504171633,"ponent is our size-target loss based on the forward KL-divergence. Our size-target loss (2) en-
139"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.21215733015494637,"courages specific target volumes for tag classes. Additionally, the size-target loss suppresses
140"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2133492252681764,"non-tag classes, encouraging zero volumes for classes not in the image. The CRF loss also con-
141"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.21454112038140644,"tributes to the suppression of redundant classes. Therefore, unlike most prior work on image-
142"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.21573301549463647,"level supervision for semantic segmentation, e.g. [9, 2, 12], we do not need separate suppres-
143"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2169249106078665,"sion loss terms like (8). We validated this claim experimentally, they did not change the results.
144"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.21811680572109654,"Figure 3: Forward vs reverse KL divergence. As-
suming binary classification K = 2, we can repre-
sent all possible probability distributions as points
on the interval [0,1]. The solid curves illustrate
our “strong” size constraint, i.e. the forward KL-
divergence KL(v∥¯S) for the average prediction
¯S. We show two examples of volumetric prior
v1 = (0.9, 0.1) (blue curve) and v2 = (0.5, 0.5)
(red curve). For comparison, the dashed curves
represent reverse KL divergence KL( ¯S∥v)."
SIZE-TARGET LOSS AND ITS PROPERTIES,0.21930870083432658,"The size-target loss can also be integrated into
145"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2205005959475566,"other weakly-supervised settings, e.g. partial
146"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.22169249106078665,"cross-entropy loss (4) commonly used for seeds.
147"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.22288438617401668,"We show that using approximate size targets
148"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.22407628128724671,"can significantly improve the seed-supervised
149"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.22526817640047675,"segmentation in [6] when the seed lengths are
150"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.22646007151370678,"short, see the right plot of Fig. 4.
151"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.22765196662693682,"L
′
total := Lsize + Lcrf + Lpce
(13)"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.22884386174016685,"As is well known, KL divergence is asymmetric.
152"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2300357568533969,"In our work on image-level supervised segmen-
153"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.23122765196662692,"tation, the order of the estimated and target dis-
154"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.232419547079857,"tributions is crucial. The forward KL divergence
155"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.23361144219308702,"possesses a zero-avoiding property, as illustrated
156"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.23480333730631706,"in Fig. 3. Specifically, forward KL divergence
157"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2359952324195471,"imposes an infinite penalty when any class with
158"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.23718712753277713,"a non-zero target is predicted as zero. In con-
159"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.23837902264600716,"trast, the penalty of the reverse KL divergence is
160"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2395709177592372,"finite and much weaker. When using reverse KL
161"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.24076281287246723,"divergence, segmentation models tend to gener-
162"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.24195470798569726,"ate trivial solutions, predicting all pixels as the
163"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2431466030989273,"background class. This issue likely arises due to
164"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.24433849821215733,"dataset imbalance, where the background class
165"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.24553039332538737,"is prevalent. The zero-avoiding property of forward KL divergence ensures that segmentation models
166"
SIZE-TARGET LOSS AND ITS PROPERTIES,0.2467222884386174,"do not produce trivial solutions and predict all classes in the image tag sets.
167"
EXPERIMENTS,0.24791418355184744,"3
Experiments
168"
EXPERIMENTAL SETTINGS,0.24910607866507747,"3.1
Experimental settings
169"
EXPERIMENTAL SETTINGS,0.25029797377830754,"Datasets. We evaluate our approach on three segmentation datasets: PASCAL VOC 2012 [5], MS
170"
EXPERIMENTAL SETTINGS,0.25148986889153757,"COCO 2014 [31], and 2017 ACDC Challenge1 [32]. The PASCAL dataset contains 21 classes. We
171"
EXPERIMENTAL SETTINGS,0.2526817640047676,"adopt the augmented training set with 10,582 images [33], following the common practice [34, 9].
172"
EXPERIMENTAL SETTINGS,0.25387365911799764,"Validation and testing contain 1449 and 1456 images. Seed supervision of the PASCAL dataset is
173"
EXPERIMENTAL SETTINGS,0.2550655542312277,"from [7]. COCO has 81 classes with 80K training and 40K validation images. ACDC Challenge is
174"
EXPERIMENTAL SETTINGS,0.2562574493444577,"to segment the left ventricular endocardium. The training and validation sets contain 1674 and 228
175"
EXPERIMENTAL SETTINGS,0.25744934445768775,"images. The exact size targets are extracted from the ground truth masks.
176"
EXPERIMENTAL SETTINGS,0.2586412395709178,"Approximate size targets. We train segmentation models using approximate size targets v =
177"
EXPERIMENTAL SETTINGS,0.2598331346841478,"(vk)K
k=1 generated for each image either by human annotators or by corrupting the exact size targets
178"
EXPERIMENTAL SETTINGS,0.26102502979737785,"ˆv = (ˆvk)K
k=1 with different levels of noise. In all cases, we report the segmentation accuracy on
179"
EXPERIMENTAL SETTINGS,0.2622169249106079,"validation data together with mean relative error (mRE) of the corresponding corrupted size targets.
180"
EXPERIMENTAL SETTINGS,0.2634088200238379,"For each training image containing class k, the relative error for the size target vk is defined as
181"
EXPERIMENTAL SETTINGS,0.26460071513706795,RE(vk) = |vk −ˆvk|
EXPERIMENTAL SETTINGS,0.265792610250298,"ˆvk
(14)"
EXPERIMENTAL SETTINGS,0.266984505363528,1https://www.creatis.insa-lyon.fr/Challenge/acdc/
EXPERIMENTAL SETTINGS,0.26817640047675806,"where ˆvk is the exact size. mRE averages RE over all images and all classes. For human annotated
182"
EXPERIMENTAL SETTINGS,0.2693682955899881,"size targets v = (vk)K
k=1, the relative size errors are computed directly from the definition (14).
183"
EXPERIMENTAL SETTINGS,0.27056019070321813,"When used, synthetic targets v = (vk)K
k=1 are generated by corrupting the exact targets ˆv = (ˆvk)K
k=1
184"
EXPERIMENTAL SETTINGS,0.27175208581644816,"vk ←−(1 + ϵ)ˆvk
for
ϵ ∼N(0, σ)
(15)
where ϵ is white noise with standard deviation σ controlling the level of corruption and operator ←−
185"
EXPERIMENTAL SETTINGS,0.2729439809296782,"represents re-normalization ensuring corrupted targets (vk)K
k=1 add up to one. Equation (15) defines
186"
EXPERIMENTAL SETTINGS,0.27413587604290823,"random variable vk as a function of ϵ. Thus, in this case, mRE can be analytically estimated from σ
187"
EXPERIMENTAL SETTINGS,0.27532777115613827,"mRE = E
|vk −ˆvk| ˆvk"
EXPERIMENTAL SETTINGS,0.2765196662693683,"
≈E(|ϵ|) = r"
EXPERIMENTAL SETTINGS,0.27771156138259834,"2
π σ
(16)"
EXPERIMENTAL SETTINGS,0.2789034564958284,"where E is the expectation operator. The approximation in the middle uses (15) as an equality
188"
EXPERIMENTAL SETTINGS,0.2800953516090584,"ignoring re-normalization of the corrupted sizes, and the last equality is a closed-form expression for
189"
EXPERIMENTAL SETTINGS,0.28128724672228844,"the mean absolute deviation (MAD) of the Normal distribution N(0, σ).
190"
EXPERIMENTAL SETTINGS,0.2824791418355185,"Evaluation metrics for segmentation. We employ mean Intersection-over-Union (mIoU) as the
191"
EXPERIMENTAL SETTINGS,0.2836710369487485,"evaluation criteria for PASCAL and COCO, and mean Dice similarity coefficient (DSC) for the
192"
EXPERIMENTAL SETTINGS,0.28486293206197855,"ACDC dataset. The quality on the PASCAL test set is assessed on the online evaluation server.
193"
EXPERIMENTAL SETTINGS,0.2860548271752086,"Implementation details. We evaluate our approach with two types of ResNet-based [4] and one vision
194"
EXPERIMENTAL SETTINGS,0.2872467222884386,"transformer (ViT) based [35] segmentation models on the PASCAL and COCO datasets. ResNet-
195"
EXPERIMENTAL SETTINGS,0.28843861740166865,"based models follow the implementation of DeepLabV3+ [18] using the backbone of ResNet101
196"
EXPERIMENTAL SETTINGS,0.2896305125148987,"(R101) or the backbone of WideResNet-38 (WR38) [1]. For brevity, we name them R101-based or
197"
EXPERIMENTAL SETTINGS,0.2908224076281287,"WR38-based DeepLabV3+ models. For the ViT-based network, We follow the implementation of
198"
EXPERIMENTAL SETTINGS,0.29201430274135876,"Segmenter [36], adopting its ViT-B/16 backbone and linear decoder. For experiments on the ACDC
199"
EXPERIMENTAL SETTINGS,0.2932061978545888,"datasets, we use MobileNetV2-based [37] DeepLabv3+ model. The R101, WR38, and MobileNetV2
200"
EXPERIMENTAL SETTINGS,0.2943980929678188,"backbones are ImageNet [38] pre-trained. ViT-B/16 backbone is pre-trained on ImageNet-21K [39]
201"
EXPERIMENTAL SETTINGS,0.29558998808104886,"and fine-tuned on ImageNet-1k [38]. We directly evaluate our size-target approach on top of the
202"
EXPERIMENTAL SETTINGS,0.2967818831942789,"standard architectures without any modification.
203"
EXPERIMENTAL SETTINGS,0.29797377830750893,"Images are resized to 512 × 512 for PASCAL and COCO, and 256 × 256 for ACDC. We employ
204"
EXPERIMENTAL SETTINGS,0.29916567342073896,"color jittering and horizontal flipping for data augmentation. Segmentation models are trained with
205"
EXPERIMENTAL SETTINGS,0.300357568533969,"stochastic gradient descent on one RTX A6000 GPU with 48 GB GDDR6: 60 epochs for PASCAL
206"
EXPERIMENTAL SETTINGS,0.30154946364719903,"and COCO, and 200 epochs for ACDC, with a polynomial learning rate scheduler (power of 0.9).
207"
EXPERIMENTAL SETTINGS,0.30274135876042907,"Batch sizes are set to 16 for ResNet and 20 for ViT models on PASCAL, 12 on ACDC, and 12
208"
EXPERIMENTAL SETTINGS,0.3039332538736591,"(ResNet) and 16 (ViT) for MS COCO. The initial learning rate is 0.005 for ACDC and PASCAL’s
209"
EXPERIMENTAL SETTINGS,0.30512514898688914,"ResNet models, and 0.0005 for PASCAL’s ViT models. The initial learning rate on COCO is 0.0005
210"
EXPERIMENTAL SETTINGS,0.3063170441001192,"for ResNet and 0.0001 for ViT models. Loss function (12) is employed for size-target supervision.
211"
EXPERIMENTAL SETTINGS,0.3075089392133492,"Loss (13) is only used for seed supervision in Sec. 3.3. The implementation of CRF loss (3) is the
212"
EXPERIMENTAL SETTINGS,0.30870083432657924,"same as [6]. We use 2e−9 as the weight of the CRF term following the strategy in [6]. Size-target
213"
EXPERIMENTAL SETTINGS,0.3098927294398093,"loss (2) and pCE (4) are used for medical images.
214"
ROBUSTNESS TO SIZE ERRORS,0.3110846245530393,"3.2
Robustness to Size Errors
215"
ROBUSTNESS TO SIZE ERRORS,0.31227651966626935,"We show the size targets can be approximate. The left plot in Fig. 4 illustrates the robustness of our
216"
ROBUSTNESS TO SIZE ERRORS,0.3134684147794994,"approach to size errors. Segmentation models are trained with synthetic size targets subjected to
217"
ROBUSTNESS TO SIZE ERRORS,0.3146603098927294,"varying levels of corruption, as defined in (15). The validation accuracy (solid red line) only drops
218"
ROBUSTNESS TO SIZE ERRORS,0.31585220500595945,"slightly when mRE (16) remains below 16%. The CRF loss (3) further enhances the robustness
219"
ROBUSTNESS TO SIZE ERRORS,0.3170441001191895,"(solid blue line). When the relative error (mRE) is 4%, there is a noticeable increase in validation
220"
ROBUSTNESS TO SIZE ERRORS,0.3182359952324195,"accuracy. The downward trend of the training accuracy (dashed blue line) suggests that the observed
221"
ROBUSTNESS TO SIZE ERRORS,0.31942789034564956,"increases in validation accuracy at mRE = 4% stem from improved neural network generalization.
222"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.3206197854588796,"3.3
Enhancing seed-based segmentation with size targets
223"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.3218116805721097,"Our size-target approach can be integrated with partial ground truth mask supervision (seeds). The
224"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.3230035756853397,"right plot in Fig. 4 demonstrates the results of seed-supervised semantic segmentation with and without
225"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.32419547079856975,"size-target supervision. Size targets significantly enhance performance, especially when the seed
226"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.3253873659117998,"lengths are short. Without size targets, segmentation performance degrades dramatically as the seed
227"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.3265792610250298,"length decreases. Notably, when only one pixel is labeled for each object (seed length ratio = 0.0),
228"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.32777115613825986,"size-target supervision boosts accuracy from 66.6% to 74%, approaching the performance of full
229"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.3289630512514899,"seed supervision (seed length ratio = 1.0).
230"
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.3301549463647199,"Figure 4: Segmentation results on the PASCAL dataset with R101-based DeeplabV3+ networks.
The green bar in both plots indicates the segmentation accuracy for full ground truth masks (i.e. full
supervision). The left plot shows the training and validation accuracy using approximate size targets.
The segmentation is trained using losses (2) (red curve) or (12) (blue curve), where size targets are
subject to various levels of corruption (15,16). The right plot shows validation accuracy for seed
supervision of varying lengths with (blue curve) and without (red curve) using size targets. The line
styles of the blue curves differentiate among various levels of corruption."
ENHANCING SEED-BASED SEGMENTATION WITH SIZE TARGETS,0.33134684147794996,"Figure 5: Left plot shows the quality of human annotations in terms of relative errors for the dog, cat,
and bird classes within the PASCAl dataset. The histograms are normalized by the number of images
in each class. The mean relative error for the three classes is 15.9%. For comparison, the dashed line
shows the relative error distribution of synthetic size targets as defined in (15) for σ = 20.0% which
aligns with the mRE of 15.9%, see (16). The right plot presents 4-way multi-class (cat, dog, bird,
and background) segmentation accuracy using human-annotated (red star at mRE = 15.9%) and
synthetic (blue curve) size targets, employing ResNet101-based DeeplabV3+ networks. Consistent
with experiments in Sec. 3.2, synthetic size targets are generated at various levels of corruption. The
green line indicates the segmentation accuracy of full supervision using ground truth masks."
HUMAN-ANNOTATED SIZE TARGETS,0.33253873659118,"3.4
Human-annotated size targets
231"
HUMAN-ANNOTATED SIZE TARGETS,0.33373063170441003,"Annotation tool. In this section, our approach is evaluated with size targets annotated by humans.
232"
HUMAN-ANNOTATED SIZE TARGETS,0.33492252681764006,"We annotated training images for a subset of PASCAL classes, including cat, dog, and bird. A
233"
HUMAN-ANNOTATED SIZE TARGETS,0.3361144219308701,"user interface with an assistance tool was developed to facilitate the annotation. The assistance tool
234"
HUMAN-ANNOTATED SIZE TARGETS,0.33730631704410013,"overlays grid lines partitioning the image into 5 × 4 small rectangles or 3 × 3 large rectangles. Users
235"
HUMAN-ANNOTATED SIZE TARGETS,0.33849821215733017,"can determine the size of a class in an image by counting rectangles (fractions allowed) or entering
236"
HUMAN-ANNOTATED SIZE TARGETS,0.3396901072705602,"the percentage relative to the image size. Annotators can choose finer or coarser partitioning for each
237"
HUMAN-ANNOTATED SIZE TARGETS,0.34088200238379024,"image depending on the object size. We evaluate relative errors with (14) for human annotations.
238"
HUMAN-ANNOTATED SIZE TARGETS,0.3420738974970203,"Empirical evidence shows that annotators are approximately two times more accurate with the
239"
HUMAN-ANNOTATED SIZE TARGETS,0.3432657926102503,"assistance tool, especially for small objects in the image. The last two columns of Table 1 report the
240"
HUMAN-ANNOTATED SIZE TARGETS,0.34445768772348034,"annotation speed per image and mean relative error (14) for each class. The left plot in Fig. 5 shows
241"
HUMAN-ANNOTATED SIZE TARGETS,0.3456495828367104,"the histograms of relative errors for human annotations. The histograms illustrate that annotated size
242"
HUMAN-ANNOTATED SIZE TARGETS,0.3468414779499404,"errors are mostly below 10%, but occasional large mistakes (heavy tails) raise the mean error.
243"
HUMAN-ANNOTATED SIZE TARGETS,0.34803337306317045,"Segmentation with human-annotated size. Segmentation models trained with human-annotated
244"
HUMAN-ANNOTATED SIZE TARGETS,0.3492252681764005,"size targets show robustness to human “heavy tail” errors. We compare the accuracy for human-
245"
HUMAN-ANNOTATED SIZE TARGETS,0.3504171632896305,"annotated and synthetic size targets in the right plot of Fig. 5. The accuracy for human-annotated
246"
HUMAN-ANNOTATED SIZE TARGETS,0.35160905840286055,"size (indicated by the red star in the plot) approaches 97.2% (89.6%/92.2%) of the full supervision
247"
HUMAN-ANNOTATED SIZE TARGETS,0.3528009535160906,"performance, demonstrating that size-target approach is significantly robust to human errors. Binary
248"
HUMAN-ANNOTATED SIZE TARGETS,0.3539928486293206,"segmentation accuracy for each class is reported in the shaded cells in Table 1. The performance of
249"
HUMAN-ANNOTATED SIZE TARGETS,0.35518474374255066,"supervision
gt mask
gt size
human-annotated size
mIoU
mIoU
mIoU
speed
mRE
cat
90.6%
88.8%
88.0%
12.6s
12.3%
dog
88.1%
84.3%
84.5%
9.1s
16.6%
bird
88.8%
86.2%
86.4%
15.2s
20.1%"
HUMAN-ANNOTATED SIZE TARGETS,0.3563766388557807,"Table 1: Human-annotated size targets. Two columns on the right show the average speed and relative
error for each class we annotated. The shaded cells compare the accuracy of binary segmentation
models trained with ground truth masks, ground truth size, and human-annotated size."
HUMAN-ANNOTATED SIZE TARGETS,0.3575685339690107,"binary segmentation models trained with human-annotated size targets is comparable to those trained
250"
HUMAN-ANNOTATED SIZE TARGETS,0.35876042908224076,"with precise size targets.
251"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3599523241954708,"3.5
Comparison with the state-of-the-art methods
252"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.36114421930870083,"Our general training losses are applied to three standard architectures (R101-DeepLabV3+, WR38-
253"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.36233611442193087,"DeepLabV3+, and ViT-Linear) for semantic segmentation as is, without any modification. Our results
254"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3635280095351609,"are highlighted in Table 2. The models are trained using synthetic size targets with an approximate
255"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.36471990464839094,"mean relative error (mRE) of 8%. We chose this corruption level because its performance is close
256"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.36591179976162097,"to human annotations, as shown in the right plot of Figure 5. Since our single-stage (end-to-end)
257"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.367103694874851,"approach is completely general, it is possible to use it in specialized architectures or complex
258"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.36829558998808104,"training procedures. Likely, this would further improve the results, but this is not the focus of
259"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3694874851013111,"our work. The rest of Table 2 shows the results for semantic segmentation methods (of different
260"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3706793802145411,"complexities) for weak and full supervision. Methods are divided into multi-stage and single-stage
261"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.37187127532777114,"methods, grouped by their backbones. Typical single-stage methods improve their results using
262"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3730631704410012,"complex architectural or training modifications such as additional training branches, extra refinement
263"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3742550655542312,"modules, or specialized training strategies. However, we achieve state-of-the-art using only standard
264"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.37544696066746125,"segmentation architectures, commonly used in full supervision. The R101-based DeepLabV3+ model
265"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3766388557806913,"trained with approximate size targets approaches 92% (71.9/78.2) of its full supervision performance
266"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3778307508939213,"on PASCAL. The WR38-based DeepLabV3+ model trained with approximate size-target supervision
267"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.37902264600715135,"surpasses other methods employing the same backbone by approximately 10%. Using the standard
268"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3802145411203814,"vision transformer architecture [36], the size-target approach achieves approximately 96% of the
269"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3814064362336114,"Backbone
Decoder
Architectural/training
Supervision
PASCAL
COCO
modification
Val
Test
Val
Multi-stage methods
R101
DeepLabV3+
MARS [40] arXiv’23
tags
77.7
77.2
49.4
R101
DeepLabV2
MatLabel [41] ICCV’23
tags
73.0
72.7
45.6
WR38
LargeFOV
MCT [42] CVPR’22
tags
71.9
71.6
42.0
WR38
LargeFOV
MCTOCR [43] CVPR’23
tags
72.7
72.0
42.5
SWIN
DeepLabV2
ReCAM [44] CVPR’22
tags
71.8
72.2
47.9
ViT-S
“Grad-clip”
WeakTr [26] arXiv’23
tags
78.4
79.0
50.3
Single-stage (end-to-end) methods
R101
DeeplabV3+
-
size (8%)
71.9
72.4
45.0
R101
DeeplabV3+
-
full
78.2
78.2
60.4
WR38
DeepLabV3+
SSSS [2] CVPR’20
tags
62.7
64.3
-
WR38
Conv
RRM [45] AAAI’20
tags
62.6
62.9
-
WR38
DeeplabV3+
-
size (8%)
72.7
72.6
-
ViT-B
LargeFOV
ToCo [28] CVPR’23
tags
71.1
72.2
42.3
ViT-B
Conv
SeCo [29] arXiv’24
tags
74.0
73.8
46.7
ViT-B
LargeFOV
CoSA [30] arXiv’24
tags
76.2
75.1
51.0
ViT-B
Linear
-
size (8%)
78.1
78.2
56.3
ViT-B
Linear
-
full
81.4
80.7
-"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.38259833134684146,"Table 2: Semantic segmentation results (mIoU%) on PASCAL and COCO. The supervision column
indicates a form of supervision: image-level class tags, size targets (our highlighted results), or full
supervision with pixel-accurate masks. The percentage after “size” is the accuracy (mRE) of our
corrupted size targets (15,16). Our approach does not require any complex architectural modification
or multi-stage training procedures needed for tag supervision, see “Modification” column. [22]"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3837902264600715,"Figure 6: Size-targets (2) vs. size-barriers (17) on the ACDC dataset. The left plot shows the accuracy
of the binary segmentation models (MobileNetV2-based DeeplabV3+) measured by DSC. The blue
curve shows size-target accuracy with various levels of corruption. The dashed green line shows the
accuracy of the size-barrier technique [22]. The dashed red line shows the accuracy using the mean
size target for all training images. The gray line indicates the result of full supervision. The right
image shows randomly selected qualitative results of size-barrier [22] and approximate size target
(mRE = 8%). Yellow shows true positive pixels, green is false positive, and red is false negatives."
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.38498212157330153,"full supervision performance on the Pascal dataset. Despite its simplicity, the size-target approach
270"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.38617401668653156,"outperforms other complex single-stage methods on both datasets.
271"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3873659117997616,"3.6
Medical data: size-target vs. size-barrier
272"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.38855780691299163,"Our method is also promising for medical image segmentation, benefiting from the consistency in
273"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.38974970202622167,"object sizes across similar medical images, which healthcare professionals can easily estimate. We
274"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3909415971394517,"compare our size-target approach with the thresholded size-barrier technique [22], proposed for the
275"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.39213349225268174,"weakly supervised medical image semantic segmentation. The size-barrier loss enforces inequality
276"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3933253873659118,"size constraints. Given the lower bound of each class, the thresholded size-barrier loss is
277"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.39451728247914186,"Lflat_sq(S)
=
X k"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.3957091775923719," 
max{ak −¯Sk, 0}
2 ,
(17)"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.39690107270560193,"where ak is a lower bound of class k. We train binary segmentation models with a combination
278"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.39809296781883197,"of partial cross-entropy loss (4) and size constraint loss: size-target (2) or size-barrier (17). Seeds
279"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.399284862932062,"used in the experiments are obtained using the same method provided in [22]. The object and
280"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.40047675804529204,"background barrier, aobj and abg are set based on [22]. In the size-barrier experiments, similarly to
281"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.40166865315852207,"[22], we suppress the non-tag classes, using the loss Lsup(S) = ( ¯Sobj)2. Conversely, size-target
282"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.4028605482717521,"loss automatically suppresses non-tag classes as discussed in Sec. 2. The left plot in Fig. 6 displays
283"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.40405244338498214,"the segmentation accuracy against different levels of size target corruption. Our size-target loss
284"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.4052443384982122,"consistently outperforms size-barrier loss, maintaining its superiority even when using highly noisy
285"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.4064362336114422,"size targets. The peak in the accuracy curve aligns with the experimental results in Sec. 3.2 and
286"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.40762812872467225,"Sec. 3.4. The accuracy of the model trained using size targets with relative errors of 8% surpasses
287"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.4088200238379023,"the full supervision performance. Additionally, using a fixed average size target across all training
288"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.4100119189511323,"images can yield performance comparable to the size-barrier method, see the dashed red line in the
289"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.41120381406436235,"left plot of Fig. 6. The right image in Fig. 6 shows qualitative examples of both methods.
290"
CONCLUSIONS,0.4123957091775924,"4
Conclusions
291"
CONCLUSIONS,0.4135876042908224,"We proposed a new image-level supervision for semantic segmentation: size targets. Such targets
292"
CONCLUSIONS,0.41477949940405245,"could be approximate. In fact, our results suggest that some errors can benefit generalization. The
293"
CONCLUSIONS,0.4159713945172825,"size annotation by humans requires little extra effort compared to the standard image-level tags and it
294"
CONCLUSIONS,0.4171632896305125,"is much cheaper than the full pixel-accurate ground truth masks. We proposed an effective size-target
295"
CONCLUSIONS,0.41835518474374256,"loss based on forward KL divergence between the soft size targets and the average prediction. In
296"
CONCLUSIONS,0.4195470798569726,"combination with the standard CRF-based regularization loss, our approximate size-target supervision
297"
CONCLUSIONS,0.42073897497020263,"on standard segmentation architectures (DeepLab and ViT) achieves state-of-the-art performance.
298"
CONCLUSIONS,0.42193087008343266,"Our general easy-to-understand approach outperforms significantly more complex weakly-supervised
299"
CONCLUSIONS,0.4231227651966627,"techniques based on model modifications and multi-stage training procedures.
300"
REFERENCES,0.42431466030989273,"References
301"
REFERENCES,0.42550655542312277,"[1] Zifeng Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet
302"
REFERENCES,0.4266984505363528,"model for visual recognition, 2016.
303"
REFERENCES,0.42789034564958284,"[2] Nikita Araslanov and Stefan Roth. Single-stage semantic segmentation from image labels. In
304"
REFERENCES,0.42908224076281287,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
305"
REFERENCES,0.4302741358760429,"4253–4262, 2020.
306"
REFERENCES,0.43146603098927294,"[3] V. Kulharia, S. Chandra, A. Agrawal, P. Torr, and A. Tyagi. Box2seg: Attention weighted loss
307"
REFERENCES,0.432657926102503,"and discriminative feature learning for weakly supervised segmentation. In ECCV’20, 2020.
308"
REFERENCES,0.433849821215733,"[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
309"
REFERENCES,0.43504171632896305,"recognition, 2015.
310"
REFERENCES,0.4362336114421931,"[5] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
311"
REFERENCES,0.4374255065554231,"The pascal visual object classes (voc) challenge. International journal of computer vision,
312"
REFERENCES,0.43861740166865315,"88:303–308, 2009.
313"
REFERENCES,0.4398092967818832,"[6] Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and
314"
REFERENCES,0.4410011918951132,"Yuri Boykov. On regularized losses for weakly-supervised cnn segmentation. In Proceedings of
315"
REFERENCES,0.44219308700834326,"the European Conference on Computer Vision (ECCV), pages 507–522, 2018.
316"
REFERENCES,0.4433849821215733,"[7] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised
317"
REFERENCES,0.4445768772348033,"convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on
318"
REFERENCES,0.44576877234803336,"computer vision and pattern recognition, pages 3159–3167, 2016.
319"
REFERENCES,0.4469606674612634,"[8] George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and
320"
REFERENCES,0.44815256257449343,"semi-supervised learning of a deep convolutional network for semantic image segmentation. In
321"
REFERENCES,0.44934445768772346,"Proceedings of the IEEE international conference on computer vision, pages 1742–1750, 2015.
322"
REFERENCES,0.4505363528009535,"[9] Alexander Kolesnikov and Christoph H Lampert. Seed, expand and constrain: Three principles
323"
REFERENCES,0.45172824791418353,"for weakly-supervised image segmentation. In Computer Vision–ECCV 2016: 14th European
324"
REFERENCES,0.45292014302741357,"Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages
325"
REFERENCES,0.4541120381406436,"695–711. Springer, 2016.
326"
REFERENCES,0.45530393325387364,"[10] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsuper-
327"
REFERENCES,0.4564958283671037,"vised image classification and segmentation. In Proceedings of the IEEE/CVF International
328"
REFERENCES,0.4576877234803337,"Conference on Computer Vision, pages 9865–9874, 2019.
329"
REFERENCES,0.45887961859356374,"[11] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. Picie: Unsupervised
330"
REFERENCES,0.4600715137067938,"semantic segmentation using invariance and equivariance in clustering. In Proceedings of the
331"
REFERENCES,0.4612634088200238,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16794–16804, 2021.
332"
REFERENCES,0.46245530393325385,"[12] Tianfei Zhou, Meijie Zhang, Fang Zhao, and Jianwu Li. Regional semantic contrast and
333"
REFERENCES,0.4636471990464839,"aggregation for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF
334"
REFERENCES,0.464839094159714,"Conference on Computer Vision and Pattern Recognition, pages 4299–4309, 2022.
335"
REFERENCES,0.466030989272944,"[13] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering
336"
REFERENCES,0.46722288438617404,"for unsupervised learning of visual features. In Proceedings of the European conference on
337"
REFERENCES,0.4684147794994041,"computer vision (ECCV), pages 132–149, 2018.
338"
REFERENCES,0.4696066746126341,"[14] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D Collins, Tien-Ju Yang, Xiao Zhang,
339"
REFERENCES,0.47079856972586415,"and Liang-Chieh Chen. Segsort: Segmentation by discriminative sorting of segments. In
340"
REFERENCES,0.4719904648390942,"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7334–7344,
341"
REFERENCES,0.4731823599523242,"2019.
342"
REFERENCES,0.47437425506555425,"[15] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for optimal boundary & region segmenta-
343"
REFERENCES,0.4755661501787843,"tion of objects in nd images. In Proceedings eighth IEEE international conference on computer
344"
REFERENCES,0.4767580452920143,"vision. ICCV 2001, volume 1, pages 105–112. IEEE, 2001.
345"
REFERENCES,0.47794994040524436,"[16] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph
346"
REFERENCES,0.4791418355184744,"cuts. IEEE Transactions on pattern analysis and machine intelligence, 23(11):1222–1239,
347"
REFERENCES,0.4803337306317044,"2001.
348"
REFERENCES,0.48152562574493446,"[17] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected CRFs with
349"
REFERENCES,0.4827175208581645,"Gaussian edge potentials. Advances in neural information processing systems, 24, 2011.
350"
REFERENCES,0.48390941597139453,"[18] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.
351"
REFERENCES,0.48510131108462456,"Encoder-decoder with atrous separable convolution for semantic image segmentation. In
352"
REFERENCES,0.4862932061978546,"Proceedings of the European conference on computer vision (ECCV), pages 801–818, 2018.
353"
REFERENCES,0.48748510131108463,"[19] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss
354"
REFERENCES,0.48867699642431467,"based on effective number of samples. In IEEE conference on Computer Vision and Pattern
355"
REFERENCES,0.4898688915375447,"Recognition (CVPR), pages 9268–9277, 2019.
356"
REFERENCES,0.49106078665077474,"[20] John Bridle, Anthony Heading, and David MacKay. Unsupervised classifiers, mutual informa-
357"
REFERENCES,0.4922526817640048,"tion and’phantom targets. Advances in neural information processing systems, 4, 1991.
358"
REFERENCES,0.4934445768772348,"[21] Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized
359"
REFERENCES,0.49463647199046484,"information maximization. Advances in neural information processing systems, 23, 2010.
360"
REFERENCES,0.4958283671036949,"[22] Hoel Kervadec, Jose Dolz, Meng Tang, Eric Granger, Yuri Boykov, and Ismail Ben Ayed.
361"
REFERENCES,0.4970202622169249,"Size-constraint loss for weakly supervised CNN segmentation. In Medical Imaging with Deep
362"
REFERENCES,0.49821215733015495,"Learning, 2018.
363"
REFERENCES,0.499404052443385,"[23] Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural
364"
REFERENCES,0.5005959475566151,"networks for weakly supervised segmentation. In Proceedings of the IEEE international
365"
REFERENCES,0.5017878426698451,"conference on computer vision, pages 1796–1804, 2015.
366"
REFERENCES,0.5029797377830751,"[24] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision
367"
REFERENCES,0.5041716328963052,"for weakly supervised semantic segmentation. In Proceedings of the IEEE conference on
368"
REFERENCES,0.5053635280095352,"computer vision and pattern recognition, pages 4981–4990, 2018.
369"
REFERENCES,0.5065554231227652,"[25] Zhaozheng Chen and Qianru Sun. Extracting class activation maps from non-discriminative
370"
REFERENCES,0.5077473182359953,"features as well. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
371"
REFERENCES,0.5089392133492253,"Recognition, pages 3135–3144, 2023.
372"
REFERENCES,0.5101311084624554,"[26] Lianghui Zhu, Yingyue Li, Jieming Fang, Yan Liu, Hao Xin, Wenyu Liu, and Xinggang Wang.
373"
REFERENCES,0.5113230035756854,"Weaktr: Exploring plain vision transformer for weakly-supervised semantic segmentation. arXiv
374"
REFERENCES,0.5125148986889154,"preprint arXiv:2304.01184, 2023.
375"
REFERENCES,0.5137067938021455,"[27] Xiaobo Yang and Xiaojin Gong. Foundation model assisted weakly supervised semantic
376"
REFERENCES,0.5148986889153755,"segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
377"
REFERENCES,0.5160905840286055,"Vision, pages 523–532, 2024.
378"
REFERENCES,0.5172824791418356,"[28] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised
379"
REFERENCES,0.5184743742550656,"semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
380"
REFERENCES,0.5196662693682956,"Pattern Recognition, pages 3093–3102, 2023.
381"
REFERENCES,0.5208581644815257,"[29] Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, and Zhijian Song. Separate
382"
REFERENCES,0.5220500595947557,"and conquer: Decoupling co-occurrence via decomposition and representation for weakly
383"
REFERENCES,0.5232419547079857,"supervised semantic segmentation. arXiv preprint arXiv:2402.18467, 2024.
384"
REFERENCES,0.5244338498212158,"[30] Xinyu Yang, Hossein Rahmani, Sue Black, and Bryan M Williams.
Weakly super-
385"
REFERENCES,0.5256257449344458,"vised co-training with swapping assignments for semantic segmentation.
arXiv preprint
386"
REFERENCES,0.5268176400476758,"arXiv:2402.17891, 2024.
387"
REFERENCES,0.5280095351609059,"[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
388"
REFERENCES,0.5292014302741359,"Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
389"
REFERENCES,0.5303933253873659,"Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
390"
REFERENCES,0.531585220500596,"Proceedings, Part V 13, pages 740–755. Springer, 2014.
391"
REFERENCES,0.532777115613826,"[32] Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann
392"
REFERENCES,0.533969010727056,"Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep
393"
REFERENCES,0.5351609058402861,"learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is
394"
REFERENCES,0.5363528009535161,"the problem solved? IEEE transactions on medical imaging, 37(11):2514–2525, 2018.
395"
REFERENCES,0.5375446960667462,"[33] Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik.
396"
REFERENCES,0.5387365911799762,"Semantic contours from inverse detectors. In 2011 international conference on computer vision,
397"
REFERENCES,0.5399284862932062,"pages 991–998. IEEE, 2011.
398"
REFERENCES,0.5411203814064363,"[34] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
399"
REFERENCES,0.5423122765196663,"Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv
400"
REFERENCES,0.5435041716328963,"preprint arXiv:1412.7062, 2014.
401"
REFERENCES,0.5446960667461264,"[35] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
402"
REFERENCES,0.5458879618593564,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
403"
REFERENCES,0.5470798569725864,"An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
404"
REFERENCES,0.5482717520858165,"arXiv:2010.11929, 2020.
405"
REFERENCES,0.5494636471990465,"[36] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for
406"
REFERENCES,0.5506555423122765,"semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer
407"
REFERENCES,0.5518474374255066,"vision, pages 7262–7272, 2021.
408"
REFERENCES,0.5530393325387366,"[37] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
409"
REFERENCES,0.5542312276519666,"Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference
410"
REFERENCES,0.5554231227651967,"on computer vision and pattern recognition, pages 4510–4520, 2018.
411"
REFERENCES,0.5566150178784267,"[38] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
412"
REFERENCES,0.5578069129916567,"Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
413"
REFERENCES,0.5589988081048868,"recognition challenge. International journal of computer vision, 115:211–252, 2015.
414"
REFERENCES,0.5601907032181168,"[39] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
415"
REFERENCES,0.5613825983313468,"scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
416"
REFERENCES,0.5625744934445769,"recognition, pages 248–255. Ieee, 2009.
417"
REFERENCES,0.5637663885578069,"[40] Sanghyun Jo, In-Jae Yu, and Kyungsu Kim. Mars: Model-agnostic biased object removal
418"
REFERENCES,0.564958283671037,"without additional supervision for weakly-supervised semantic segmentation. arXiv preprint
419"
REFERENCES,0.566150178784267,"arXiv:2304.09913, 2023.
420"
REFERENCES,0.567342073897497,"[41] Changwei Wang, Rongtao Xu, Shibiao Xu, Weiliang Meng, and Xiaopeng Zhang. Treating
421"
REFERENCES,0.5685339690107271,"pseudo-labels generation as image matting for weakly supervised semantic segmentation. In
422"
REFERENCES,0.5697258641239571,"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 755–765,
423"
REFERENCES,0.5709177592371871,"2023.
424"
REFERENCES,0.5721096543504172,"[42] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, and Dan Xu.
Multi-
425"
REFERENCES,0.5733015494636472,"class token transformer for weakly supervised semantic segmentation. In Proceedings of the
426"
REFERENCES,0.5744934445768772,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4310–4319, 2022.
427"
REFERENCES,0.5756853396901073,"[43] Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu Wei, Xiangyang Ji, Li Yuan,
428"
REFERENCES,0.5768772348033373,"Chang Liu, and Jie Chen. Out-of-candidate rectification for weakly supervised semantic
429"
REFERENCES,0.5780691299165673,"segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
430"
REFERENCES,0.5792610250297974,"Recognition, pages 23673–23684, 2023.
431"
REFERENCES,0.5804529201430274,"[44] Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng Hua, Hanwang Zhang, and Qianru
432"
REFERENCES,0.5816448152562574,"Sun. Class re-activation maps for weakly-supervised semantic segmentation. In Proceedings of
433"
REFERENCES,0.5828367103694875,"the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 969–978, 2022.
434"
REFERENCES,0.5840286054827175,"[45] Bingfeng Zhang, Jimin Xiao, Yunchao Wei, Mingjie Sun, and Kaizhu Huang. Reliability does
435"
REFERENCES,0.5852205005959475,"matter: An end-to-end weakly supervised semantic segmentation approach. In Proceedings of
436"
REFERENCES,0.5864123957091776,"the AAAI Conference on Artificial Intelligence, volume 34, pages 12765–12772, 2020.
437"
REFERENCES,0.5876042908224076,"[46] A. Bearman, O. Russakovsky, V. Ferrari, and F. Li. Semantic segmentation with point supervi-
438"
REFERENCES,0.5887961859356377,"sion. In ECCV, 2015.
439 Image"
REFERENCES,0.5899880810488677,GT mask
REFERENCES,0.5911799761620977,Prediction
REFERENCES,0.5923718712753278,"WR38
R101"
REFERENCES,0.5935637663885578,"ViT-B
ViT-B"
REFERENCES,0.5947556615017878,Prediction
REFERENCES,0.5959475566150179,"PASCAL
COCO"
REFERENCES,0.5971394517282479,"Figure 7: Segmentation examples using size-target supervision (mRE = 8%). Model backbones are
shown in the top-left corner of the predictions, see Table 2 for decoders."
REFERENCES,0.5983313468414779,"A
Appendix / supplemental material
440"
REFERENCES,0.599523241954708,"A.1
Labeling costs and accuracies reported in Figure 1
441"
REFERENCES,0.600715137067938,"Labelling costs. Figure 1 in the paper shows labeling speed and accuracy for different forms of
442"
REFERENCES,0.601907032181168,"supervision on PASCAL VOC. The table at the bottom of Figure 1 shows ballpark estimates of
443"
REFERENCES,0.6030989272943981,"average labeling time per image in the whole dataset. We use the data in [46], as well as Table 1 in
444"
REFERENCES,0.6042908224076281,"the paper, and aggregate all labeling speeds from “per class”, “per instance”, or “per point” to “per
445"
REFERENCES,0.6054827175208581,"image” using the average number of instances or classes in each image and the aggregation rules
446"
REFERENCES,0.6066746126340882,"formulated in [46], see their Section 4. The top-left corner in each picture shows the corresponding
447"
REFERENCES,0.6078665077473182,"estimated labeling times for the representative multi-instance image. All the labeling times are only
448"
REFERENCES,0.6090584028605482,"rough estimates, but they are intuitive. The relative costs for point supervision seem underestimated,
449"
REFERENCES,0.6102502979737783,"but they follow evaluation conventions detailed in [46].
450"
REFERENCES,0.6114421930870083,"Accuracies. The values of “point”, “size target” and “full supervision” accuracy (mIOU%) are based
451"
REFERENCES,0.6126340882002383,"on the experiments in the paper (Figure 4). We follow the learning rate scheme in DeepLabV3+ [18]
452"
REFERENCES,0.6138259833134684,"for the training with full supervision. For fairness, we compare these with end-to-end methods using
453"
REFERENCES,0.6150178784266984,"similar ResNet backbones in tag- [2] and box-supervision [3]. Typical SOTA methods for tag and
454"
REFERENCES,0.6162097735399285,"box supervision use special architectural modifications, unlike our generic size-target loss, cannot be
455"
REFERENCES,0.6174016686531585,"seamlessly plugged into any segmentation model.
456"
REFERENCES,0.6185935637663885,"A.2
Qualitative results
457"
REFERENCES,0.6197854588796186,"Figure 7 presents the qualitative examples of our method on PASCAL (left) and COCO (right)
458"
REFERENCES,0.6209773539928486,"validation sets. Despite size targets providing only image-level information, segmentation models
459"
REFERENCES,0.6221692491060786,"can precisely identify object locations, eliminating the need for localization methods like CAM.
460"
REFERENCES,0.6233611442193087,"NeurIPS Paper Checklist
461"
CLAIMS,0.6245530393325387,"1. Claims
462"
CLAIMS,0.6257449344457687,"Question: Do the main claims made in the abstract and introduction accurately reflect the
463"
CLAIMS,0.6269368295589988,"paper’s contributions and scope?
464"
CLAIMS,0.6281287246722288,"Answer: [Yes]
465"
CLAIMS,0.6293206197854588,"Justification: Contributions are included in the abstract and listed in Sec. 1.3 in the introduc-
466"
CLAIMS,0.6305125148986889,"tion.
467"
CLAIMS,0.6317044100119189,"Guidelines:
468"
CLAIMS,0.6328963051251489,"• The answer NA means that the abstract and introduction do not include the claims
469"
CLAIMS,0.634088200238379,"made in the paper.
470"
CLAIMS,0.635280095351609,"• The abstract and/or introduction should clearly state the claims made, including the
471"
CLAIMS,0.636471990464839,"contributions made in the paper and important assumptions and limitations. A No or
472"
CLAIMS,0.6376638855780691,"NA answer to this question will not be perceived well by the reviewers.
473"
CLAIMS,0.6388557806912991,"• The claims made should match theoretical and experimental results, and reflect how
474"
CLAIMS,0.6400476758045291,"much the results can be expected to generalize to other settings.
475"
CLAIMS,0.6412395709177592,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
476"
CLAIMS,0.6424314660309892,"are not attained by the paper.
477"
LIMITATIONS,0.6436233611442194,"2. Limitations
478"
LIMITATIONS,0.6448152562574494,"Question: Does the paper discuss the limitations of the work performed by the authors?
479"
LIMITATIONS,0.6460071513706794,"Answer: [No]
480"
LIMITATIONS,0.6471990464839095,"Justification: Although the limitations were not explicitly detailed in the paper, we mentioned
481"
LIMITATIONS,0.6483909415971395,"that only a subset of the PASCAL dataset was labeled due to resource constraints, see Sec. 3.4.
482"
LIMITATIONS,0.6495828367103695,"To address this, we generated approximate synthetic size targets by corrupting the exact size
483"
LIMITATIONS,0.6507747318235996,"targets. This allowed us to evaluate our method on the entire PASCAL dataset, as well as on
484"
LIMITATIONS,0.6519666269368296,"COCO and ACDC datasets.
485"
LIMITATIONS,0.6531585220500596,"Guidelines:
486"
LIMITATIONS,0.6543504171632897,"• The answer NA means that the paper has no limitation while the answer No means that
487"
LIMITATIONS,0.6555423122765197,"the paper has limitations, but those are not discussed in the paper.
488"
LIMITATIONS,0.6567342073897497,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
489"
LIMITATIONS,0.6579261025029798,"• The paper should point out any strong assumptions and how robust the results are to
490"
LIMITATIONS,0.6591179976162098,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
491"
LIMITATIONS,0.6603098927294399,"model well-specification, asymptotic approximations only holding locally). The authors
492"
LIMITATIONS,0.6615017878426699,"should reflect on how these assumptions might be violated in practice and what the
493"
LIMITATIONS,0.6626936829558999,"implications would be.
494"
LIMITATIONS,0.66388557806913,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
495"
LIMITATIONS,0.66507747318236,"only tested on a few datasets or with a few runs. In general, empirical results often
496"
LIMITATIONS,0.66626936829559,"depend on implicit assumptions, which should be articulated.
497"
LIMITATIONS,0.6674612634088201,"• The authors should reflect on the factors that influence the performance of the approach.
498"
LIMITATIONS,0.6686531585220501,"For example, a facial recognition algorithm may perform poorly when image resolution
499"
LIMITATIONS,0.6698450536352801,"is low or images are taken in low lighting. Or a speech-to-text system might not be
500"
LIMITATIONS,0.6710369487485102,"used reliably to provide closed captions for online lectures because it fails to handle
501"
LIMITATIONS,0.6722288438617402,"technical jargon.
502"
LIMITATIONS,0.6734207389749702,"• The authors should discuss the computational efficiency of the proposed algorithms
503"
LIMITATIONS,0.6746126340882003,"and how they scale with dataset size.
504"
LIMITATIONS,0.6758045292014303,"• If applicable, the authors should discuss possible limitations of their approach to
505"
LIMITATIONS,0.6769964243146603,"address problems of privacy and fairness.
506"
LIMITATIONS,0.6781883194278904,"• While the authors might fear that complete honesty about limitations might be used by
507"
LIMITATIONS,0.6793802145411204,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
508"
LIMITATIONS,0.6805721096543504,"limitations that aren’t acknowledged in the paper. The authors should use their best
509"
LIMITATIONS,0.6817640047675805,"judgment and recognize that individual actions in favor of transparency play an impor-
510"
LIMITATIONS,0.6829558998808105,"tant role in developing norms that preserve the integrity of the community. Reviewers
511"
LIMITATIONS,0.6841477949940405,"will be specifically instructed to not penalize honesty concerning limitations.
512"
THEORY ASSUMPTIONS AND PROOFS,0.6853396901072706,"3. Theory Assumptions and Proofs
513"
THEORY ASSUMPTIONS AND PROOFS,0.6865315852205006,"Question: For each theoretical result, does the paper provide the full set of assumptions and
514"
THEORY ASSUMPTIONS AND PROOFS,0.6877234803337307,"a complete (and correct) proof?
515"
THEORY ASSUMPTIONS AND PROOFS,0.6889153754469607,"Answer:[NA]
516"
THEORY ASSUMPTIONS AND PROOFS,0.6901072705601907,"Justification: The paper does not include theoretical results.
517"
THEORY ASSUMPTIONS AND PROOFS,0.6912991656734208,"Guidelines:
518"
THEORY ASSUMPTIONS AND PROOFS,0.6924910607866508,"• The answer NA means that the paper does not include theoretical results.
519"
THEORY ASSUMPTIONS AND PROOFS,0.6936829558998808,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
520"
THEORY ASSUMPTIONS AND PROOFS,0.6948748510131109,"referenced.
521"
THEORY ASSUMPTIONS AND PROOFS,0.6960667461263409,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
522"
THEORY ASSUMPTIONS AND PROOFS,0.6972586412395709,"• The proofs can either appear in the main paper or the supplemental material, but if
523"
THEORY ASSUMPTIONS AND PROOFS,0.698450536352801,"they appear in the supplemental material, the authors are encouraged to provide a short
524"
THEORY ASSUMPTIONS AND PROOFS,0.699642431466031,"proof sketch to provide intuition.
525"
THEORY ASSUMPTIONS AND PROOFS,0.700834326579261,"• Inversely, any informal proof provided in the core of the paper should be complemented
526"
THEORY ASSUMPTIONS AND PROOFS,0.7020262216924911,"by formal proofs provided in appendix or supplemental material.
527"
THEORY ASSUMPTIONS AND PROOFS,0.7032181168057211,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7044100119189511,"4. Experimental Result Reproducibility
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7056019070321812,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7067938021454112,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7079856972586412,"of the paper (regardless of whether the code and data are provided or not)?
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7091775923718713,"Answer: [Yes]
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7103694874851013,"Justification: Our size-target loss function is discussed in the 2. The experimental settings
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7115613825983313,"are discussed in the 3.1
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7127532777115614,"Guidelines:
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7139451728247914,"• The answer NA means that the paper does not include experiments.
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7151370679380215,"• If the paper includes experiments, a No answer to this question will not be perceived
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7163289630512515,"well by the reviewers: Making the paper reproducible is important, regardless of
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7175208581644815,"whether the code and data are provided or not.
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7187127532777116,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7199046483909416,"to make their results reproducible or verifiable.
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7210965435041716,"• Depending on the contribution, reproducibility can be accomplished in various ways.
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7222884386174017,"For example, if the contribution is a novel architecture, describing the architecture fully
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7234803337306317,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7246722288438617,"be necessary to either make it possible for others to replicate the model with the same
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7258641239570918,"dataset, or provide access to the model. In general. releasing code and data is often
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7270560190703218,"one good way to accomplish this, but reproducibility can also be provided via detailed
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7282479141835518,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7294398092967819,"of a large language model), releasing of a model checkpoint, or other means that are
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7306317044100119,"appropriate to the research performed.
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7318235995232419,"• While NeurIPS does not require releasing code, the conference does require all submis-
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.733015494636472,"sions to provide some reasonable avenue for reproducibility, which may depend on the
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.734207389749702,"nature of the contribution. For example
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.735399284862932,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7365911799761621,"to reproduce that algorithm.
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7377830750893921,"(b) If the contribution is primarily a new model architecture, the paper should describe
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7389749702026222,"the architecture clearly and fully.
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7401668653158522,"(c) If the contribution is a new model (e.g., a large language model), then there should
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7413587604290822,"either be a way to access this model for reproducing the results or a way to reproduce
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7425506555423123,"the model (e.g., with an open-source dataset or instructions for how to construct
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7437425506555423,"the dataset).
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7449344457687723,"(d) We recognize that reproducibility may be tricky in some cases, in which case
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7461263408820024,"authors are welcome to describe the particular way they provide for reproducibility.
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7473182359952324,"In the case of closed-source models, it may be that access to the model is limited in
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7485101311084624,"some way (e.g., to registered users), but it should be possible for other researchers
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497020262216925,"to have some path to reproducing or verifying the results.
567"
OPEN ACCESS TO DATA AND CODE,0.7508939213349225,"5. Open access to data and code
568"
OPEN ACCESS TO DATA AND CODE,0.7520858164481525,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
569"
OPEN ACCESS TO DATA AND CODE,0.7532777115613826,"tions to faithfully reproduce the main experimental results, as described in supplemental
570"
OPEN ACCESS TO DATA AND CODE,0.7544696066746126,"material?
571"
OPEN ACCESS TO DATA AND CODE,0.7556615017878426,"Answer: [No]
572"
OPEN ACCESS TO DATA AND CODE,0.7568533969010727,"Justification: To preserve anonymity, the code will be released in the final version.
573"
OPEN ACCESS TO DATA AND CODE,0.7580452920143027,"Guidelines:
574"
OPEN ACCESS TO DATA AND CODE,0.7592371871275327,"• The answer NA means that paper does not include experiments requiring code.
575"
OPEN ACCESS TO DATA AND CODE,0.7604290822407628,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
576"
OPEN ACCESS TO DATA AND CODE,0.7616209773539928,"public/guides/CodeSubmissionPolicy) for more details.
577"
OPEN ACCESS TO DATA AND CODE,0.7628128724672228,"• While we encourage the release of code and data, we understand that this might not be
578"
OPEN ACCESS TO DATA AND CODE,0.7640047675804529,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
579"
OPEN ACCESS TO DATA AND CODE,0.7651966626936829,"including code, unless this is central to the contribution (e.g., for a new open-source
580"
OPEN ACCESS TO DATA AND CODE,0.766388557806913,"benchmark).
581"
OPEN ACCESS TO DATA AND CODE,0.767580452920143,"• The instructions should contain the exact command and environment needed to run to
582"
OPEN ACCESS TO DATA AND CODE,0.768772348033373,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
583"
OPEN ACCESS TO DATA AND CODE,0.7699642431466031,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
584"
OPEN ACCESS TO DATA AND CODE,0.7711561382598331,"• The authors should provide instructions on data access and preparation, including how
585"
OPEN ACCESS TO DATA AND CODE,0.7723480333730631,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
586"
OPEN ACCESS TO DATA AND CODE,0.7735399284862932,"• The authors should provide scripts to reproduce all experimental results for the new
587"
OPEN ACCESS TO DATA AND CODE,0.7747318235995232,"proposed method and baselines. If only a subset of experiments are reproducible, they
588"
OPEN ACCESS TO DATA AND CODE,0.7759237187127532,"should state which ones are omitted from the script and why.
589"
OPEN ACCESS TO DATA AND CODE,0.7771156138259833,"• At submission time, to preserve anonymity, the authors should release anonymized
590"
OPEN ACCESS TO DATA AND CODE,0.7783075089392133,"versions (if applicable).
591"
OPEN ACCESS TO DATA AND CODE,0.7794994040524433,"• Providing as much information as possible in supplemental material (appended to the
592"
OPEN ACCESS TO DATA AND CODE,0.7806912991656734,"paper) is recommended, but including URLs to data and code is permitted.
593"
OPEN ACCESS TO DATA AND CODE,0.7818831942789034,"6. Experimental Setting/Details
594"
OPEN ACCESS TO DATA AND CODE,0.7830750893921334,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
595"
OPEN ACCESS TO DATA AND CODE,0.7842669845053635,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
596"
OPEN ACCESS TO DATA AND CODE,0.7854588796185935,"results?
597"
OPEN ACCESS TO DATA AND CODE,0.7866507747318237,"Answer: [Yes]
598"
OPEN ACCESS TO DATA AND CODE,0.7878426698450537,"Justification: The experimental setting is detailed in the Sec. 3.1
599"
OPEN ACCESS TO DATA AND CODE,0.7890345649582837,"Guidelines:
600"
OPEN ACCESS TO DATA AND CODE,0.7902264600715138,"• The answer NA means that the paper does not include experiments.
601"
OPEN ACCESS TO DATA AND CODE,0.7914183551847438,"• The experimental setting should be presented in the core of the paper to a level of detail
602"
OPEN ACCESS TO DATA AND CODE,0.7926102502979738,"that is necessary to appreciate the results and make sense of them.
603"
OPEN ACCESS TO DATA AND CODE,0.7938021454112039,"• The full details can be provided either with the code, in appendix, or as supplemental
604"
OPEN ACCESS TO DATA AND CODE,0.7949940405244339,"material.
605"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7961859356376639,"7. Experiment Statistical Significance
606"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.797377830750894,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.798569725864124,"information about the statistical significance of the experiments?
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.799761620977354,"Answer: [No]
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8009535160905841,"Justification: Error bars are not reported because it would be too computationally expensive.
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8021454112038141,"Our plots in Figure 4, 5, 6 are smooth enough to verify our method.
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8033373063170441,"Guidelines:
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8045292014302742,"• The answer NA means that the paper does not include experiments.
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8057210965435042,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8069129916567342,"dence intervals, or statistical significance tests, at least for the experiments that support
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8081048867699643,"the main claims of the paper.
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8092967818831943,"• The factors of variability that the error bars are capturing should be clearly stated (for
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8104886769964244,"example, train/test split, initialization, random drawing of some parameter, or overall
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8116805721096544,"run with given experimental conditions).
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8128724672228844,"• The method for calculating the error bars should be explained (closed form formula,
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8140643623361145,"call to a library function, bootstrap, etc.)
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8152562574493445,"• The assumptions made should be given (e.g., Normally distributed errors).
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8164481525625745,"• It should be clear whether the error bar is the standard deviation or the standard error
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8176400476758046,"of the mean.
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8188319427890346,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8200238379022646,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8212157330154947,"of Normality of errors is not verified.
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8224076281287247,"• For asymmetric distributions, the authors should be careful not to show in tables or
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8235995232419547,"figures symmetric error bars that would yield results that are out of range (e.g. negative
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8247914183551848,"error rates).
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8259833134684148,"• If error bars are reported in tables or plots, The authors should explain in the text how
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8271752085816448,"they were calculated and reference the corresponding figures or tables in the text.
632"
EXPERIMENTS COMPUTE RESOURCES,0.8283671036948749,"8. Experiments Compute Resources
633"
EXPERIMENTS COMPUTE RESOURCES,0.8295589988081049,"Question: For each experiment, does the paper provide sufficient information on the com-
634"
EXPERIMENTS COMPUTE RESOURCES,0.8307508939213349,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
635"
EXPERIMENTS COMPUTE RESOURCES,0.831942789034565,"the experiments?
636"
EXPERIMENTS COMPUTE RESOURCES,0.833134684147795,"Answer: [Yes]
637"
EXPERIMENTS COMPUTE RESOURCES,0.834326579261025,"Justification: The information on the computer resources is detailed in Sec. 3.1
638"
EXPERIMENTS COMPUTE RESOURCES,0.8355184743742551,"Guidelines:
639"
EXPERIMENTS COMPUTE RESOURCES,0.8367103694874851,"• The answer NA means that the paper does not include experiments.
640"
EXPERIMENTS COMPUTE RESOURCES,0.8379022646007152,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
641"
EXPERIMENTS COMPUTE RESOURCES,0.8390941597139452,"or cloud provider, including relevant memory and storage.
642"
EXPERIMENTS COMPUTE RESOURCES,0.8402860548271752,"• The paper should provide the amount of compute required for each of the individual
643"
EXPERIMENTS COMPUTE RESOURCES,0.8414779499404053,"experimental runs as well as estimate the total compute.
644"
EXPERIMENTS COMPUTE RESOURCES,0.8426698450536353,"• The paper should disclose whether the full research project required more compute
645"
EXPERIMENTS COMPUTE RESOURCES,0.8438617401668653,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
646"
EXPERIMENTS COMPUTE RESOURCES,0.8450536352800954,"didn’t make it into the paper).
647"
CODE OF ETHICS,0.8462455303933254,"9. Code Of Ethics
648"
CODE OF ETHICS,0.8474374255065554,"Question: Does the research conducted in the paper conform, in every respect, with the
649"
CODE OF ETHICS,0.8486293206197855,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
650"
CODE OF ETHICS,0.8498212157330155,"Answer: [Yes]
651"
CODE OF ETHICS,0.8510131108462455,"Justification: The research in the paper conforms with the code of ethics.
652"
CODE OF ETHICS,0.8522050059594756,"Guidelines:
653"
CODE OF ETHICS,0.8533969010727056,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
654"
CODE OF ETHICS,0.8545887961859356,"• If the authors answer No, they should explain the special circumstances that require a
655"
CODE OF ETHICS,0.8557806912991657,"deviation from the Code of Ethics.
656"
CODE OF ETHICS,0.8569725864123957,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
657"
CODE OF ETHICS,0.8581644815256257,"eration due to laws or regulations in their jurisdiction).
658"
BROADER IMPACTS,0.8593563766388558,"10. Broader Impacts
659"
BROADER IMPACTS,0.8605482717520858,"Question: Does the paper discuss both potential positive societal impacts and negative
660"
BROADER IMPACTS,0.8617401668653158,"societal impacts of the work performed?
661"
BROADER IMPACTS,0.8629320619785459,"Answer: [NA]
662"
BROADER IMPACTS,0.8641239570917759,"Justification: Our research on weakly-supervised semantic segmentation is a purely technical
663"
BROADER IMPACTS,0.865315852205006,"advancement to improve image segmentation, with no direct societal impacts or associated
664"
BROADER IMPACTS,0.866507747318236,"ethical concerns.
665"
BROADER IMPACTS,0.867699642431466,"Guidelines:
666"
BROADER IMPACTS,0.8688915375446961,"• The answer NA means that there is no societal impact of the work performed.
667"
BROADER IMPACTS,0.8700834326579261,"• If the authors answer NA or No, they should explain why their work has no societal
668"
BROADER IMPACTS,0.8712753277711561,"impact or why the paper does not address societal impact.
669"
BROADER IMPACTS,0.8724672228843862,"• Examples of negative societal impacts include potential malicious or unintended uses
670"
BROADER IMPACTS,0.8736591179976162,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
671"
BROADER IMPACTS,0.8748510131108462,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
672"
BROADER IMPACTS,0.8760429082240763,"groups), privacy considerations, and security considerations.
673"
BROADER IMPACTS,0.8772348033373063,"• The conference expects that many papers will be foundational research and not tied
674"
BROADER IMPACTS,0.8784266984505363,"to particular applications, let alone deployments. However, if there is a direct path to
675"
BROADER IMPACTS,0.8796185935637664,"any negative applications, the authors should point it out. For example, it is legitimate
676"
BROADER IMPACTS,0.8808104886769964,"to point out that an improvement in the quality of generative models could be used to
677"
BROADER IMPACTS,0.8820023837902264,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
678"
BROADER IMPACTS,0.8831942789034565,"that a generic algorithm for optimizing neural networks could enable people to train
679"
BROADER IMPACTS,0.8843861740166865,"models that generate Deepfakes faster.
680"
BROADER IMPACTS,0.8855780691299165,"• The authors should consider possible harms that could arise when the technology is
681"
BROADER IMPACTS,0.8867699642431466,"being used as intended and functioning correctly, harms that could arise when the
682"
BROADER IMPACTS,0.8879618593563766,"technology is being used as intended but gives incorrect results, and harms following
683"
BROADER IMPACTS,0.8891537544696066,"from (intentional or unintentional) misuse of the technology.
684"
BROADER IMPACTS,0.8903456495828367,"• If there are negative societal impacts, the authors could also discuss possible mitigation
685"
BROADER IMPACTS,0.8915375446960667,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
686"
BROADER IMPACTS,0.8927294398092968,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
687"
BROADER IMPACTS,0.8939213349225268,"feedback over time, improving the efficiency and accessibility of ML).
688"
SAFEGUARDS,0.8951132300357568,"11. Safeguards
689"
SAFEGUARDS,0.8963051251489869,"Question: Does the paper describe safeguards that have been put in place for responsible
690"
SAFEGUARDS,0.8974970202622169,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
691"
SAFEGUARDS,0.8986889153754469,"image generators, or scraped datasets)?
692"
SAFEGUARDS,0.899880810488677,"Answer: [NA]
693"
SAFEGUARDS,0.901072705601907,"Justification: This paper poses no such risks.
694"
SAFEGUARDS,0.902264600715137,"Guidelines:
695"
SAFEGUARDS,0.9034564958283671,"• The answer NA means that the paper poses no such risks.
696"
SAFEGUARDS,0.9046483909415971,"• Released models that have a high risk for misuse or dual-use should be released with
697"
SAFEGUARDS,0.9058402860548271,"necessary safeguards to allow for controlled use of the model, for example by requiring
698"
SAFEGUARDS,0.9070321811680572,"that users adhere to usage guidelines or restrictions to access the model or implementing
699"
SAFEGUARDS,0.9082240762812872,"safety filters.
700"
SAFEGUARDS,0.9094159713945172,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
701"
SAFEGUARDS,0.9106078665077473,"should describe how they avoided releasing unsafe images.
702"
SAFEGUARDS,0.9117997616209773,"• We recognize that providing effective safeguards is challenging, and many papers do
703"
SAFEGUARDS,0.9129916567342073,"not require this, but we encourage authors to take this into account and make a best
704"
SAFEGUARDS,0.9141835518474374,"faith effort.
705"
LICENSES FOR EXISTING ASSETS,0.9153754469606674,"12. Licenses for existing assets
706"
LICENSES FOR EXISTING ASSETS,0.9165673420738975,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
707"
LICENSES FOR EXISTING ASSETS,0.9177592371871275,"the paper, properly credited and are the license and terms of use explicitly mentioned and
708"
LICENSES FOR EXISTING ASSETS,0.9189511323003575,"properly respected?
709"
LICENSES FOR EXISTING ASSETS,0.9201430274135876,"Answer: [Yes]
710"
LICENSES FOR EXISTING ASSETS,0.9213349225268176,"Justification: The owners of assets used in this paper are credited and the license is mentioned
711"
LICENSES FOR EXISTING ASSETS,0.9225268176400476,"and respected.
712"
LICENSES FOR EXISTING ASSETS,0.9237187127532777,"Guidelines:
713"
LICENSES FOR EXISTING ASSETS,0.9249106078665077,"• The answer NA means that the paper does not use existing assets.
714"
LICENSES FOR EXISTING ASSETS,0.9261025029797377,"• The authors should cite the original paper that produced the code package or dataset.
715"
LICENSES FOR EXISTING ASSETS,0.9272943980929678,"• The authors should state which version of the asset is used and, if possible, include a
716"
LICENSES FOR EXISTING ASSETS,0.9284862932061978,"URL.
717"
LICENSES FOR EXISTING ASSETS,0.929678188319428,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
718"
LICENSES FOR EXISTING ASSETS,0.930870083432658,"• For scraped data from a particular source (e.g., website), the copyright and terms of
719"
LICENSES FOR EXISTING ASSETS,0.932061978545888,"service of that source should be provided.
720"
LICENSES FOR EXISTING ASSETS,0.933253873659118,"• If assets are released, the license, copyright information, and terms of use in the
721"
LICENSES FOR EXISTING ASSETS,0.9344457687723481,"package should be provided. For popular datasets, paperswithcode.com/datasets
722"
LICENSES FOR EXISTING ASSETS,0.9356376638855781,"has curated licenses for some datasets. Their licensing guide can help determine the
723"
LICENSES FOR EXISTING ASSETS,0.9368295589988082,"license of a dataset.
724"
LICENSES FOR EXISTING ASSETS,0.9380214541120382,"• For existing datasets that are re-packaged, both the original license and the license of
725"
LICENSES FOR EXISTING ASSETS,0.9392133492252682,"the derived asset (if it has changed) should be provided.
726"
LICENSES FOR EXISTING ASSETS,0.9404052443384983,"• If this information is not available online, the authors are encouraged to reach out to
727"
LICENSES FOR EXISTING ASSETS,0.9415971394517283,"the asset’s creators.
728"
NEW ASSETS,0.9427890345649583,"13. New Assets
729"
NEW ASSETS,0.9439809296781884,"Question: Are new assets introduced in the paper well documented and is the documentation
730"
NEW ASSETS,0.9451728247914184,"provided alongside the assets?
731"
NEW ASSETS,0.9463647199046484,"Answer: [NA]
732"
NEW ASSETS,0.9475566150178785,"Justification: The paper does not release new assets.
733"
NEW ASSETS,0.9487485101311085,"Guidelines:
734"
NEW ASSETS,0.9499404052443385,"• The answer NA means that the paper does not release new assets.
735"
NEW ASSETS,0.9511323003575686,"• Researchers should communicate the details of the dataset/code/model as part of their
736"
NEW ASSETS,0.9523241954707986,"submissions via structured templates. This includes details about training, license,
737"
NEW ASSETS,0.9535160905840286,"limitations, etc.
738"
NEW ASSETS,0.9547079856972587,"• The paper should discuss whether and how consent was obtained from people whose
739"
NEW ASSETS,0.9558998808104887,"asset is used.
740"
NEW ASSETS,0.9570917759237187,"• At submission time, remember to anonymize your assets (if applicable). You can either
741"
NEW ASSETS,0.9582836710369488,"create an anonymized URL or include an anonymized zip file.
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9594755661501788,"14. Crowdsourcing and Research with Human Subjects
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9606674612634089,"Question: For crowdsourcing experiments and research with human subjects, does the paper
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9618593563766389,"include the full text of instructions given to participants and screenshots, if applicable, as
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9630512514898689,"well as details about compensation (if any)?
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.964243146603099,"Answer: [NA]
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.965435041716329,"Justification: The paper does not involve crowdsourcing or research with human subjects.
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.966626936829559,"Guidelines:
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9678188319427891,"• The answer NA means that the paper does not involve crowdsourcing nor research with
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9690107270560191,"human subjects.
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702026221692491,"• Including this information in the supplemental material is fine, but if the main contribu-
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713945172824792,"tion of the paper involves human subjects, then as much detail as possible should be
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725864123957092,"included in the main paper.
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737783075089392,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749702026221693,"or other labor should be paid at least the minimum wage in the country of the data
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761620977353993,"collector.
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9773539928486293,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785458879618594,"Subjects
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797377830750894,"Question: Does the paper describe potential risks incurred by study participants, whether
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809296781883194,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821215733015495,"approvals (or an equivalent approval/review based on the requirements of your country or
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833134684147795,"institution) were obtained?
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845053635280095,"Answer: [NA]
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856972586412396,"Justification: The paper does not involve crowdsourcing or research with human subjects.
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868891537544696,"Guidelines:
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880810488676997,"• The answer NA means that the paper does not involve crowdsourcing nor research with
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892729439809297,"human subjects.
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904648390941597,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916567342073898,"may be required for any human subjects research. If you obtained IRB approval, you
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928486293206198,"should clearly state this in the paper.
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940405244338498,"• We recognize that the procedures for this may vary significantly between institutions
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952324195470799,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964243146603099,"guidelines for their institution.
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976162097735399,"• For initial submissions, do not include any information that would break anonymity (if
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99880810488677,"applicable), such as the institution conducting the review.
776"
