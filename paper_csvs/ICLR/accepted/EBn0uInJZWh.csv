Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002304147465437788,"Existing ofﬂine reinforcement learning (RL) methods face a few major challenges,
particularly the distributional shift between the learned policy and the behavior
policy. Ofﬂine Meta-RL is emerging as a promising approach to address these
challenges, aiming to learn an informative meta-policy from a collection of tasks.
Nevertheless, as shown in our empirical studies, ofﬂine Meta-RL could be outper-
formed by ofﬂine single-task RL methods on tasks with good quality of datasets,
indicating that a right balance has to be delicately calibrated between “exploring”
the out-of-distribution state-actions by following the meta-policy and “exploiting”
the ofﬂine dataset by staying close to the behavior policy. Motivated by such em-
pirical analysis, we propose model-based ofﬂine Meta-RL with regularized Policy
Optimization (MerPO), which learns a meta-model for efﬁcient task structure in-
ference and an informative meta-policy for safe exploration of out-of-distribution
state-actions. In particular, we devise a new meta-Regularized model-based Actor-
Critic (RAC) method for within-task policy optimization, as a key building block
of MerPO, using both conservative policy evaluation and regularized policy im-
provement; and the intrinsic tradeoff therein is achieved via striking the right bal-
ance between two regularizers, one based on the behavior policy and the other on
the meta-policy. We theoretically show that the learnt policy offers guaranteed
improvement over both the behavior policy and the meta-policy, thus ensuring the
performance improvement on new tasks via ofﬂine Meta-RL. Experiments corrob-
orate the superior performance of MerPO over existing ofﬂine Meta-RL methods."
INTRODUCTION,0.004608294930875576,"1
INTRODUCTION"
INTRODUCTION,0.0069124423963133645,"Ofﬂine reinforcement learning (a.k.a., batch RL) has recently attracted extensive attention by learn-
ing from ofﬂine datasets previously collected via some behavior policy (Kumar et al., 2020). How-
ever, the performance of existing ofﬂine RL methods could degrade signiﬁcantly due to the following
issues: 1) the possibly poor quality of ofﬂine datasets (Levine et al., 2020) and 2) the inability to
generalize to different environments (Li et al., 2020b). To tackle these challenges, ofﬂine Meta-RL
(Li et al., 2020a; Dorfman & Tamar, 2020; Mitchell et al., 2020; Li et al., 2020b) has emerged very
recently by leveraging the knowledge of similar ofﬂine RL tasks (Yu et al., 2021a). The main aim
of these studies is to enable quick policy adaptation for new ofﬂine tasks, by learning a meta-policy
with robust task structure inference that captures the structural properties across training tasks."
INTRODUCTION,0.009216589861751152,Figure 1: FOCAL vs. COMBO.
INTRODUCTION,0.01152073732718894,"Because tasks are trained on ofﬂine datasets, value
overestimation (Fujimoto et al., 2019) inevitably oc-
curs in dynamic programming based ofﬂine Meta-RL,
resulted from the distribution shift between the be-
havior policy and the learnt task-speciﬁc policy. To
guarantee the learning performance on new ofﬂine
tasks, a right balance has to be carefully calibrated be-
tween “exploring” the out-of-distribution state-actions
by following the meta-policy, and “exploiting” the of-
ﬂine dataset by staying close to the behavior policy.
However, such a unique “exploration-exploitation” tradeoff has not been considered in existing of-
ﬂine Meta-RL approaches, which would likely limit their ability to handle diverse ofﬂine datasets"
INTRODUCTION,0.013824884792626729,Published as a conference paper at ICLR 2022
INTRODUCTION,0.016129032258064516,"particularly towards those with good behavior policies. To illustrate this issue more concretely, we
compare the performance between a state-of-the-art ofﬂine Meta-RL algorithm FOCAL (Li et al.,
2020b) and an ofﬂine single-task RL method COMBO (Yu et al., 2021b) in two new ofﬂine tasks.
As illustrated in Figure 1, while FOCAL performs better than COMBO on the task with a bad-quality
dataset (left plot in Figure 1), it is outperformed by COMBO on the task with a good-quality dataset
(right plot in Figure 1). Clearly, existing ofﬂine Meta-RL fails in several standard environments (see
Figure 1 and Figure 11) to generalize universally well over datasets with varied quality. In order to
ﬁll such a substantial gap, we seek to answer the following key question in ofﬂine Meta-RL:"
INTRODUCTION,0.018433179723502304,"How to design an efﬁcient ofﬂine Meta-RL algorithm to strike the right balance between exploring
with the meta-policy and exploiting the ofﬂine dataset?"
INTRODUCTION,0.020737327188940093,"To this end, we propose MerPO, a model-based ofﬂine Meta-RL approach with regularized Policy
Optimization, which learns a meta-model for efﬁcient task structure inference and an informa-
tive meta-policy for safe exploration of out-of-distribution state-actions. Compared to existing ap-
proaches, MerPO achieves: (1) safe policy improvement: performance improvement can be guar-
anteed for ofﬂine tasks regardless of the quality of the dataset, by strike the right balance between
exploring with the meta-policy and exploiting the ofﬂine dataset; and (2) better generalization ca-
pability: through a conservative utilization of the learnt model to generate synthetic data, MerPO
aligns well with a recently emerging trend in supervised meta-learning to improve the generaliza-
tion ability by augmenting the tasks with “more data” (Rajendran et al., 2020; Yao et al., 2021). Our
main contributions can be summarized as follows:"
INTRODUCTION,0.02304147465437788,"(1) Learnt dynamics models not only serve as a natural remedy for task structure inference in of-
ﬂine Meta-RL, but also facilitate better exploration of out-of-distribution state-actions by generating
synthetic rollouts. With this insight, we develop a model-based approach, where an ofﬂine meta-
model is learnt to enable efﬁcient task model learning for each ofﬂine task. More importantly,
we propose a meta-regularized model-based actor-critic method (RAC) for within-task policy opti-
mization, where a novel regularized policy improvement module is devised to calibrate the unique
“exploration-exploitation” tradeoff by using an interpolation between two regularizers, one based
on the behavior policy and the other on the meta-policy. Intuitively, RAC generalizes COMBO to
the multi-task setting, with introduction of a novel regularized policy improvement module to strike
a right balance between the impacts of the meta-policy and the behavior policy."
INTRODUCTION,0.02534562211981567,"(2) We theoretically show that under mild conditions, the learnt task-speciﬁc policy based on MerPO
offers safe performance improvement over both the behavior policy and the meta-policy with high
probability. Our results also provide a guidance for the algorithm design in terms of how to ap-
propriately select the weights in the interpolation, such that the performance improvement can be
guaranteed for new ofﬂine RL tasks."
INTRODUCTION,0.027649769585253458,"(3) We conduct extensive experiments to evaluate the performance of MerPO. More speciﬁcally, the
experiments clearly show the safe policy improvement offered in MerPO, corroborating our theo-
retical results. Further, the superior performance of MerPO over existing ofﬂine Meta-RL methods
suggests that model-based approaches can be more beneﬁcial in ofﬂine Meta-RL."
RELATED WORK,0.029953917050691243,"2
RELATED WORK"
RELATED WORK,0.03225806451612903,"Ofﬂine single-task RL. Many existing model-free ofﬂine RL methods regularize the learnt policy
to be close to the behavior policy by, e.g., distributional matching (Fujimoto et al., 2019), support
matching (Kumar et al., 2019), importance sampling (Nachum et al., 2019; Liu et al., 2020), learning
lower bounds of true Q-values (Kumar et al., 2020). Along a different avenue, model-based algo-
rithms learn policies by leveraging a dynamics model obtained with the ofﬂine dataset. (Matsushima
et al., 2020) directly constrains the learnt policy to the behavior policy as in model-free algorithms.
To penalize the policy for visiting states where the learnt model is likely to be incorrect, MOPO
(Yu et al., 2020) and MoREL (Kidambi et al., 2020) modify the learnt dynamics such that the value
estimates are conservative when the model uncertainty is above a threshold. To remove the need
of uncertainty quantiﬁcation, COMBO (Yu et al., 2021b) is proposed by combining model-based
policy optimization (Janner et al., 2019) and conservative policy evaluation (Kumar et al., 2020)."
RELATED WORK,0.03456221198156682,"Ofﬂine Meta-RL. A few very recent studies have explored the ofﬂine Meta-RL. Particularly, (Li
et al., 2020a) considers a special scenario where the task identity is spuriously inferred due to biased"
RELATED WORK,0.03686635944700461,Published as a conference paper at ICLR 2022
RELATED WORK,0.03917050691244239,"datasets, and applies the triplet loss to robustify the task inference with reward relabelling. (Dorfman
& Tamar, 2020) extends an online Meta-RL method VariBAD (Zintgraf et al., 2019) to the ofﬂine
setup, and assumes known reward functions and shared dynamics across tasks. Based on MAML
(Finn et al., 2017), (Mitchell et al., 2020) proposes an ofﬂine Meta-RL algorithm with advantage
weighting loss, and learns initializations for both the value function and the policy, where they
consider the ofﬂine dataset in the format of full trajectories in order to evaluate the advantage. Based
on the off-policy Meta-RL method PEARL (Rakelly et al., 2019), (Li et al., 2020b) combines the idea
of deterministic context encoder and behavior regularization, under the assumption of deterministic
MDP. Different from the above works, we study a more general ofﬂine Meta-RL problem. More
importantly, MerPO strikes a right balance between exploring with the meta-policy and exploiting
the ofﬂine dataset, which guarantees safe performance improvement for new ofﬂine tasks."
PRELIMINARIES,0.041474654377880185,"3
PRELIMINARIES"
PRELIMINARIES,0.04377880184331797,"Consider a Markov decision process (MDP) M = (S, A, T, r, µ0, γ) with state space S, action
space A, the environment dynamics T(s′|s, a), reward function r(s, a), initial state distribution
µ0, and γ ∈(0, 1) is the discount factor. Without loss of generality, we assume that |r(s, a)| ≤
Rmax. Given a policy π, let dπ
M(s) := (1 −γ) P∞
t=0 γtPM(st = s|π) denote the discounted
marginal state distribution, where PM(st = s|π) denotes the probability of being in state s at
time t by rolling out π in M. Accordingly, let dπ
M(s, a) := dπ
M(s)π(a|s) denote the discounted
marginal state-action distribution, and J(M, π) :=
1
1−γ E(s,a)∼dπ
M(s,a)[r(s, a)] denote the expected
discounted return. The goal of RL is to ﬁnd the optimal policy that maximizes J(M, π). In ofﬂine
RL, no interactions with the environment are allowed, and we only have access to a ﬁxed dataset
D = {(s, a, r, s′)} collected by some unknown behavior policy πβ. Let dπβ
M(s) be the discounted
marginal state distribution of πβ. The dataset D is indeed sampled from dπβ
M(s, a) = dπβ
M(s)πβ(a|s).
Denote M as the empirical MDP induced by D and d(s, a) as a sample-based version of dπβ
M(s, a)."
PRELIMINARIES,0.04608294930875576,"In ofﬂine Meta-RL, consider a distribution of RL tasks p(M) as in standard Meta-RL (Finn et al.,
2017; Rakelly et al., 2019), where each task Mn is an MDP, i.e., Mn = (S, A, Tn, rn, µ0,n, γ), with
task-shared state and action spaces, and unknown task-speciﬁc dynamics and reward function.For
each task Mn, no interactions with the environment are allowed and we only have access to an
ofﬂine dataset Dn, collected by some unknown behavior policy πβ,n. The main objective is to learn
a meta-policy based on a set of ofﬂine training tasks {Mn}N
n=1."
PRELIMINARIES,0.04838709677419355,"Conservative Ofﬂine Model-Based Policy Optimization (COMBO). Recent model-based ofﬂine
RL algorithms, e.g., COMBO (Yu et al., 2021b), have demonstrated promising performance on
a single ofﬂine RL task by combining model-based policy optimization (Janner et al., 2019) and
conservative policy evaluation (CQL (Kumar et al., 2020)). Simply put, COMBO ﬁrst trains a
dynamics model bTθ(s′|s, a) parameterized by θ, via supervised learning on the ofﬂine dataset D.
The learnt MDP is constructed as c
M := (S, A, bT, r, µ0, γ). Then, the policy is learnt using D and
model-generated rollouts. Speciﬁcally, deﬁne the action-value function (Q-function) as Qπ(s, a) :=
E [P∞
t=0 γtr(st, at)|s0 = s, a0 = a], and the empirical Bellman operator as: bBπQ(s, a) = r(s, a)+
γE(s,a,s′)∼D[Q(s′, a′)], for a′ ∼π(·|s′). To penalize the Q functions in out-of-distribution state-
action tuples, COMBO employs conservative policy evaluation based on CQL:
bQk+1 ←arg min
Q(s,a) β(Es,a∼ρ[Q(s, a)] −Es,a∼D[Q(s, a)]) + 1"
PRELIMINARIES,0.05069124423963134,"2Es,a,s′∼df [(Q(s, a) −bBπ bQk(s, a))2] (1)"
PRELIMINARIES,0.052995391705069124,"where ρ(s, a) := dπ
c
M(s)π(a|s) is the discounted marginal distribution when rolling out π in c
M,"
PRELIMINARIES,0.055299539170506916,"and df(s, a) = fdπβ
M(s, a) + (1 −f)ρ(s, a) for f ∈[0, 1]. The Bellman backup bBπ over df can
be interpreted as an f-interpolation of the backup operators under the empirical MDP (denoted by
Bπ"
PRELIMINARIES,0.0576036866359447,"M) and the learnt MDP (denoted by Bπ
c
M). Given the Q-estimation bQπ, the policy can be learnt by:"
PRELIMINARIES,0.059907834101382486,"π′ ←arg max
π
Es∼ρ(s),a∼π(·|s)[ bQπ(s, a)].
(2)"
PRELIMINARIES,0.06221198156682028,"4
MERPO: MODEL-BASED OFFLINE META-RL WITH REGULARIZED
POLICY OPTIMIZATION"
PRELIMINARIES,0.06451612903225806,"Learnt dynamics models not only serves as a natural remedy for task structure inference in ofﬂine
Meta-RL, but also facilitates better exploration of out-of-distribution state-actions by generating"
PRELIMINARIES,0.06682027649769585,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.06912442396313365,Figure 2: Model-based ofﬂine Meta-RL with learning of ofﬂine meta-model and ofﬂine meta-policy.
PRELIMINARIES,0.07142857142857142,"synthetic rollouts (Yu et al., 2021b). Thus motivated, we propose a general framework of model-
based ofﬂine Meta-RL, as depicted in Figure 2. More speciﬁcally, the ofﬂine meta-model is ﬁrst
learnt by using supervised meta-learning, based on which the task-speciﬁc model can be quickly
adapted. Then, the main attention of this study is devoted to the learning of an informative meta-
policy via bi-level optimization, where 1) a model-based policy optimization approach is leveraged
in the inner loop for each task to learn a task-speciﬁc policy; and 2) the meta-policy is then updated
in the outer loop based on the learnt task-speciﬁc policies."
OFFLINE META-MODEL LEARNING,0.07373271889400922,"4.1
OFFLINE META-MODEL LEARNING"
OFFLINE META-MODEL LEARNING,0.07603686635944701,"Learning a meta-model based on the set of ofﬂine dataset {Dn}N
n=1 can be carried out via supervised
meta-learning. Many gradient-based meta-learning techniques can be applied here, e.g., MAML
(Finn et al., 2017) and Reptile (Nichol et al., 2018). In what follows, we outline the basic idea to
leverage the higher-order information of the meta-objective function. Speciﬁcally, we consider a
proximal meta-learning approach, following the same line as in (Zhou et al., 2019):"
OFFLINE META-MODEL LEARNING,0.07834101382488479,"min
φm
Lmodel(φm) = EMn"
OFFLINE META-MODEL LEARNING,0.08064516129032258,"
min
θn"
OFFLINE META-MODEL LEARNING,0.08294930875576037,"h
E(s,a,s′)∼Dn[log bTθn(s′|s, a)] + η∥θn −φm∥2
2
i
(3)"
OFFLINE META-MODEL LEARNING,0.08525345622119816,"where the learnt dynamics for each task Mn is parameterized by θn and the meta-model is parame-
terized by φm. Solving Eq. (3) leads to an ofﬂine meta-model."
OFFLINE META-MODEL LEARNING,0.08755760368663594,"Given the learnt meta-model Tφ∗m, the dynamics model for an individual ofﬂine task j can be found
by solving the following problem via gradient descent with initialization Tφ∗m using Dj, i.e.,
min
θj
E(s,a,s′)∼Dj[log bTθj(s′|s, a)] + η∥θj −φ∗
m∥2
2.
(4)"
OFFLINE META-MODEL LEARNING,0.08986175115207373,"Compared to learning the dynamics model from scratch, adapting from Tφ∗m can quickly generate
a dynamics model for task identity inference by leveraging the knowledge from similar tasks, and
hence improve the sample efﬁciency (Finn et al., 2017; Zhou et al., 2019)."
OFFLINE META-POLICY LEARNING,0.09216589861751152,"4.2
OFFLINE META-POLICY LEARNING"
OFFLINE META-POLICY LEARNING,0.0944700460829493,"In this section, we turn attention to tackle one main challenge in this study: How to learn an infor-
mative ofﬂine meta-policy in order to achieve the optimal tradeoff between “exploring” the out-of-
distribution state-actions by following the meta-policy and “exploiting” the ofﬂine dataset by staying
close to the behavior policy? Clearly, it is highly desirable for the meta-policy to safely ‘explore’
out-of-distribution state-action pairs, and for each task to utilize the meta-policy to mitigate the issue
of value overestimation."
OFFLINE META-POLICY LEARNING,0.0967741935483871,"4.2.1
HOW DO EXISTING PROXIMAL META-RL APPROACHES PERFORM?"
OFFLINE META-POLICY LEARNING,0.09907834101382489,"Proximal Meta-RL approaches have demonstrated remarkable performance in the online setting
(e.g., (Wang et al., 2020)), by explicitly regularizing the task-speciﬁc policy close to the meta-policy.
We ﬁrst consider the approach that applies the online Proximal Meta-RL method directly to devise
ofﬂine Meta-RL, which would lead to:"
OFFLINE META-POLICY LEARNING,0.10138248847926268,"max
πc
EMn"
OFFLINE META-POLICY LEARNING,0.10368663594470046,"
max
πn"
OFFLINE META-POLICY LEARNING,0.10599078341013825,"
E
s∼ρn,
a∼πn(·|s)"
OFFLINE META-POLICY LEARNING,0.10829493087557604,"h
ˆQπ
n(s, a)
i
−λD(πn, πc)

(5)"
OFFLINE META-POLICY LEARNING,0.11059907834101383,"where πc is the ofﬂine meta-policy, πn is the task-speciﬁc policy, ρn is the state marginal of ρn(s, a)
for task n and D(·, ·) is some distance measure between two probability distributions. To alleviate
value overestimation, conservative policy evaluation can be applied to learn ˆQπ
n by using Eq. (1)."
OFFLINE META-POLICY LEARNING,0.11290322580645161,Published as a conference paper at ICLR 2022
OFFLINE META-POLICY LEARNING,0.1152073732718894,"Intuitively, Eq. (5) corresponds to generalizing COMBO to the multi-task setting, where a meta
policy πc is learned to regularize the within-task policy optimization."
OFFLINE META-POLICY LEARNING,0.1175115207373272,"Figure 3:
Performance of
proximal Meta-RL Eq. (5)."
OFFLINE META-POLICY LEARNING,0.11981566820276497,"To get a sense of how the meta-policy learnt using Eq. (5) performs,
we evaluate its performance in an ofﬂine variant of standard Meta-RL
benchmark Walker-2D-Params with good-quality datasets, and evalu-
ate the testing performance of the task-speciﬁc policy after ﬁne-tuning
based on the learnt meta-policy, with respect to the meta-training
steps. As can be seen in Figure 3, the proximal Meta-RL algorithm
Eq. (5) performs surprisingly poorly and fails to learn an informative
meta-policy, despite conservative policy evaluation being applied in
within-task policy optimization to deal with the value overestimation.
In particular, the testing performance degrades along with the meta-
training process, implying that the quality of the learnt meta-policy is
in fact decreasing."
OFFLINE META-POLICY LEARNING,0.12211981566820276,"Why does the proximal Meta-RL method in Eq. (5) perform poorly in ofﬂine Meta-RL, even with
conservative policy evaluation? To answer this, it is worth to take a closer look at the within-task
policy optimization in Eq. (5), which is given as follows:
πn ←arg max
πn Es∼ρn,a∼πn(·|s)[ bQπ
n(s, a)] −λD(πn, πc).
(6)"
OFFLINE META-POLICY LEARNING,0.12442396313364056,"Clearly, the performance of Eq. (6) depends heavily on the quality of the meta-policy πc. A poor
meta-policy may have negative impact on the performance and result in a task-speciﬁc policy πn that
is even outperformed by the behaviour policy πβ,n. Without online exploration, the quality of πn
could not be improved, which in turn leads to a worse meta-policy πc through Eq. (5). The iterative
meta-training process would eventually result in the performance degradation in Figure 3."
OFFLINE META-POLICY LEARNING,0.12672811059907835,"In a nutshell, simply following the meta-policy may lead to worse performance of ofﬂine tasks when
πβ is a better policy than πc. Since it is infeasible to guarantee the superiority of the meta-policy a
priori, it is necessary to balance the tradeoff between exploring with the meta-policy and exploiting
the ofﬂine dataset, in order to guarantee the performance improvement of new ofﬂine tasks."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.12903225806451613,"4.2.2
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1313364055299539,"To tackle the above challenge, we next devise a novel regularized policy improvement for within-
task policy optimization of task n, through a weighted interpolation of two different regularizers
based on the behavior policy πβ,n and the meta-policy πc, given as follows:
πn ←arg max
πn Es∼ρn,a∼πn(·|s)[ bQπ
n(s, a)] −λαD(πn, πβ,n) −λ(1 −α)D(πn, πc),
(7)"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1336405529953917,"for some α ∈[0, 1]. Here, α controls the trade-off between staying close to the behavior policy and
following the meta-policy to “explore” out-of-distribution state-actions. Intuitively, as α is closer
to 0, the policy improvement is less conservative and tends to improve the task-speciﬁc policy πn
towards the actions in πc that have highest estimated Q-values. Compared to Eq. (6), the exploration
penalty induced by D(πn, πβ,n) serves as a safeguard and stops πn following πc over-optimistically."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1359447004608295,Algorithm 1 RAC
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1382488479262673,"1: Train dynamics model bTθn using Dn;
2: for k = 1, 2, ... do
3:
Perform model rollouts starting from
states in Dn and add into Dmodel,n;
4:
Policy evaluation by recursively solv-
ing Eq. (1) using Dn ∪Dmodel,n;
5:
Improve policy by solving Eq. (7);
6: end for"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.14055299539170507,"Safe Policy Improvement Guarantee. Based on con-
servative policy evaluation Eq. (1) and regularized pol-
icy improvement Eq. (7), we have the meta-regularized
model-based actor-critic method (RAC), as outlined in
Algorithm 1. Note that different distribution distance
measures can be used in Eq. (7). In this work, we theo-
retically show that the policy πn(a|s) learnt by RAC is
a safe improvement over both the behavior policy πβ,n
and the meta-policy πc on the underlying MDP Mn,
when using the maximum total-variation distance for
D(π1, π2), i.e., D(π1, π2) := maxs DT V (π1||π2)."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.14285714285714285,"For convenience, deﬁne νn(ρ, f) = Eρ [(ρ(s, a) −dn(s, a))/df,n(s, a)], and let δ ∈(0, 1/2). We
have the following important result on the safe policy improvement achieved by πn(a|s)."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.14516129032258066,"Theorem 1. (a) Let ϵ =
β[νn(ρπn,f)−νn(ρπβ,n,f)]"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.14746543778801843,"2λ(1−γ)D(πn,πβ,n)
. If νn(ρπn, f) −νn(ρπβ,n, f) > 0 and α ∈
 
max{ 1"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1497695852534562,"2 −ϵ, 0}, 1"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.15207373271889402,"2

, then J(Mn, πn) ≥max{J(Mn, πc) + ξ1, J(Mn, πβ,n) + ξ2} holds with
probability at least 1 −2δ, where both ξ1 and ξ2 are positive for large enough β and λ;"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1543778801843318,Published as a conference paper at ICLR 2022
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.15668202764976957,"(b) More generally, we have that J(Mn, πn) ≥max{J(Mn, πc) + ξ1, J(Mn, πβ,n) + ξ2} holds
with probability at least 1 −2δ, when α ∈(0, 1/2), where ξ1 is positive for large enough λ."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.15898617511520738,"Remark 1. The expressions of ξ1 and ξ2 are involved and can be found in Eq. (14) and Eq. (15) in
the appendix. In part (a) of Theorem 1, both ξ1 and ξ2 are positive for large enough β and λ, pointing
to guaranteed improvements over πc and πβ,n. Due to the fact that the dynamics T d
Mn learnt via
supervised learning is close to the true dynamics TMn on the states visited by the behavior policy
πβ,n, dπβ,n
d
Mn (s, a) is close to dπβ,n"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.16129032258064516,"Mn (s, a) and ρπβ,n is close to dn(s, a), indicating that the condition
νn(ρπn, f)−νn(ρπβ,n, f) > 0 is expected to hold in practical scenarios (Yu et al., 2021b). For more
general cases, a slightly weaker result can be obtained in part (b) of Theorem 1, where ξ1 is positive
for large enough λ and ξ2 can be negative."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.16359447004608296,"Remark 2. Intuitively, the selection of α balances the impact of πβ,n and πc, while delicately
leaning toward the meta-policy πc because πβ,n has played an important role in policy evaluation
to ﬁnd a lower bound of Q-value. As a result, Eq. (7) maximizes the true Q-value while implic-
itly regularized by a weighted combination, instead of α-interpolation, between D(πn, πβ,n) and
D(πn, πc), where the weights are carefully balanced through α. In particular, in the tabular setting,
the conservative policy evaluation in Eq. (1) corresponds to penalizing the Q estimation (Yu et al.,
2021b):
bQk+1
n
(s, a) = bBπ bQk
n(s, a) −β[ρ(s, a) −dn(s, a)]"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.16589861751152074,"df,n(s, a)
.
(8)"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.16820276497695852,"Clearly, ϵ increases with the value of the penalty term in Eq. (8). As a result, when the policy
evaluation Eq. (1) is overly conservative, the lower bound of α will be close to 0, and hence the
regularizer based on the meta-policy πc can play a bigger role so as to encourage the “exploration”
of out-of-distribution state-actions following the guidance of πc. On the other hand, when the policy
evaluation Eq. (1) is less conservative, the lower bound of α will be close to 1"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.17050691244239632,"2, and the regularizer
based on πβ,n will have more impact, leaning towards “exploiting” the ofﬂine dataset. In fact,
the introduction of 1) behavior policy-based regularizer and 2) the interpolation for modeling the
interaction between the behavior policy and the meta-policy, is the key to prove Theorem 1."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1728110599078341,"Practical Implementation. In practice, we can use the KL divergence to replace the total variation
distance between policies, based on Pinsker’s Inequality: ∥π1−π2∥≤
p"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.17511520737327188,"2DKL(π1||π2). Moreover,
since the behavior policy πβ,n is typically unknown, we can use the reverse KL-divergence between
πn and πβ,n to circumvent the estimation of πβ,n, following the same line as in (Fakoor et al., 2021):
DKL(πβ,n||πn) = Ea∼πβ,n[log πβ,n(a|s)] −Ea∼πβ,n[log πn(a|s)]"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.1774193548387097,"∝−Ea∼πβ,n[log πn(a|s)] ≈−E(s,a)∼Dn[log πn(a|s)]."
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.17972350230414746,"Then, the task-speciﬁc policy can be learnt by solving the following problem:"
SAFE POLICY IMPROVEMENT WITH META-REGULARIZATION,0.18202764976958524,"max
πn
Es∼ρn,a∼πn(·|s)
h
bQπ
n(s, a)
i
+ λαE(s,a)∼Dn[log πn(a|s)] −λ(1 −α)DKL(πn||πc).
(9)"
OFFLINE META-POLICY UPDATE,0.18433179723502305,"4.2.3
OFFLINE META-POLICY UPDATE"
OFFLINE META-POLICY UPDATE,0.18663594470046083,"Built on RAC, the ofﬂine meta-policy πc is updated by taking the following two steps, in an iterative
manner: 1) (inner loop) given the meta-policy πc, RAC is run for each training task to obtain the
task-speciﬁc policy πn; 2) (outer loop) based on {πn}n, πc is updated by solving:"
OFFLINE META-POLICY UPDATE,0.1889400921658986,"max
πc
EMn"
OFFLINE META-POLICY UPDATE,0.1912442396313364,"
E
s∼ρn,
a∼πn(·|s)"
OFFLINE META-POLICY UPDATE,0.1935483870967742,"h
ˆQπ
n(s, a)
i
+ λαE(s,a)∼Dn[log πn(a|s)] −λ(1 −α)DKL(πn||πc)

(10)"
OFFLINE META-POLICY UPDATE,0.195852534562212,"where both ρn and ˆQπ
n are from the last iteration of the inner loop for each training task. By
using RAC in the inner loop for within-task policy optimization, the learnt task-speciﬁc policy πn
and the meta-policy πc work in concert to regularize the policy search for each other, and improve
akin to ‘positive feedback’. Here the regularizer based on the behavior policy serves an important
initial force to boost the policy optimization against the ground: RAC in the inner loop aims to
improve the task-speciﬁc policy over the behavior policy at the outset and the improved task-speciﬁc
policy consequently regularizes the meta-policy search as in Eq. (10), leading to a better meta-policy
eventually. Noted that a meta-Q network is learnt using ﬁrst-order meta-learning to initialize task-
speciﬁc Q networks. It is worth noting that different tasks can have different values of α to capture
the heterogeneity of dataset qualities across tasks."
OFFLINE META-POLICY UPDATE,0.19815668202764977,Published as a conference paper at ICLR 2022
OFFLINE META-POLICY UPDATE,0.20046082949308755,"Figure 4: Performance comparison among COMBO, COMBO-3 and RAC, with good-quality meta-
policy (two ﬁgures on the left) and poor-quality meta-policy (two ﬁgures on the right)."
OFFLINE META-POLICY UPDATE,0.20276497695852536,"In a nutshell, the proposed model-based ofﬂine Meta-RL with regularized Policy Optimization
(MerPO) is built on two key steps: 1) learning the ofﬂine meta-model via Eq. (3) and 2) learning the
ofﬂine meta-policy via Eq. (10). The details are presented in Algorithm 2 in the appendix."
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.20506912442396313,"4.3
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK"
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.2073732718894009,"Let Tφ∗m and π∗
c be the ofﬂine meta-model and the ofﬂine meta-policy learnt by MerPO. For a new
ofﬂine RL task, the task model can be quickly adapted based on Eq. (4), and the task-speciﬁc policy
can be obtained based on π∗
c using the within-task policy optimization module RAC. Appealing to
Theorem 1, we have the following result on MerPO-based policy learning on a new task."
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.20967741935483872,"Proposition 1. Consider a new ofﬂine RL task with the true MDP M.
Suppose πo is the
MerPO-based task-speciﬁc policy, learnt by running RAC over the meta-policy π∗
c.
If ϵ =
β[ν(ρπo,f)−ν(ρπβ ,f)]"
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.2119815668202765,"2λ(1−γ)D(πo,πβ)
≥0 and α ∈
 
max{ 1"
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.21428571428571427,"2 −ϵ, 0}, 1"
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.21658986175115208,"2

, then πo achieves the safe performance
improvement over both π∗
c and πβ, i.e., J(M, πo) > max{J(M, π∗
c), J(M, πβ)} holds with prob-
ability at least 1 −2δ, for large enough β and λ."
MERPO-BASED POLICY OPTIMIZATION FOR NEW OFFLINE RL TASK,0.21889400921658986,"Proposition 1 indicates that MerPO-based policy optimization for learning task-speciﬁc policy guar-
antees a policy with higher rewards than both the behavior policy and the meta-policy. This is
particularly useful in the following two scenarios: 1) the ofﬂine dataset is collected by some poor
behavior policy, but the meta-policy is a good policy; and 2) the meta-policy is inferior to a good
behavior policy."
EXPERIMENTS,0.22119815668202766,"5
EXPERIMENTS"
EXPERIMENTS,0.22350230414746544,"In what follows, we ﬁrst evaluate the performance of RAC for within-task policy optimization on
an ofﬂine RL task to validate the safe policy improvement, and then examine how MerPO performs
when compared to state-of-art ofﬂine Meta-RL algorithms. Due to the space limit, we relegate
additional experiments to the appendix."
PERFORMANCE EVALUATION OF RAC,0.22580645161290322,"5.1
PERFORMANCE EVALUATION OF RAC"
PERFORMANCE EVALUATION OF RAC,0.22811059907834103,"Setup.
We evaluate RAC on several continuous control tasks in the D4RL benchmark (Fu et al.,
2020) from the Open AI Gym (Brockman et al., 2016), and compare its performance to 1) COMBO
(where no meta-policy is leveraged) and 2) COMBO with policy improvement Eq. (6) (namely,
COMBO-3), under different qualities of ofﬂine datasets and different qualities of meta-policy (good
and poor). For illustrative purpose, we use a random policy as a poor-quality meta-policy, and
choose the learnt policy after 200 episodes as a better-quality meta-policy. We evaluate the average
return over 4 random seeds after each episode with 1000 gradient steps."
PERFORMANCE EVALUATION OF RAC,0.2304147465437788,"Results. As shown in Figure 4, RAC can achieve comparable performance with COMBO-3 given
a good-quality meta-policy, and both clearly outperform COMBO. Besides, the training procedure
is also more stable and converges more quickly as expected when regularized with the meta-policy.
When regularized by a poor-quality meta-policy, that is signiﬁcantly worse than the behavior policy
in all environments, the performance of COMBO-3 degrades dramatically. However, RAC outper-
forms COMBO even when the meta-policy is a random policy. In a nutshell, RAC consistently"
PERFORMANCE EVALUATION OF RAC,0.23271889400921658,Published as a conference paper at ICLR 2022
PERFORMANCE EVALUATION OF RAC,0.2350230414746544,"achieves the best performance in various setups and demonstrates compelling robustness against the
quality of the meta-policy, for suitable parameter selections (α = 0.4 in Figure 4)."
PERFORMANCE EVALUATION OF RAC,0.23732718894009217,"Figure 5: Impact of α on the performance of RAC under
different qualities of ofﬂine datasets."
PERFORMANCE EVALUATION OF RAC,0.23963133640552994,"Impact of α.
As shown in Theorem
1, the selection of α is important to
guarantee the safe policy improvement
property of RAC. Therefore, we next
examine the impact of α on the per-
formance of RAC under different qual-
ities of datasets and meta-policy. More
speciﬁcally, we consider four choices
of α: α = 0, 0.4, 0.7, 1. Here, α =
0 corresponds to COMBO-3, i.e., reg-
ularized by the meta-policy only, and
the policy improvement step is regular-
ized by the behavior policy only when
α = 1. Figure 5 shows the average return of RAC over different qualities of meta-policies under
different qualities of the ofﬂine datasets. It is clear that RAC achieves the best performance when
α = 0.4 among the four selections of α, corroborating the result in Theorem 1. In general, the
performance of RAC is stable for α ∈[0.3, 0.5] in our experiments."
PERFORMANCE EVALUATION OF MERPO,0.24193548387096775,"5.2
PERFORMANCE EVALUATION OF MERPO"
PERFORMANCE EVALUATION OF MERPO,0.24423963133640553,"Setup. To evaluate the performance of MerPO, we follow the setups in the literature (Rakelly et al.,
2019; Li et al., 2020b) and consider continuous control meta-environments of robotic locomotion.
More speciﬁcally, tasks has different transition dynamics in Walker-2D-Params and Point-Robot-
Wind, and different reward functions in Half-Cheetah-Fwd-Back and Ant-Fwd-Back. We collect
the ofﬂine dataset for each task by following the same line as in (Li et al., 2020b). We consider the
following baselines: (1) FOCAL (Li et al., 2020b), a model-free ofﬂine Meta-RL approach based
on a deterministic context encoder that achieves the state-of-the-art performance; (2) MBML (Li
et al., 2020a), an ofﬂine multi-task RL approach with metric learning; (3) Batch PEARL, which
modiﬁes PEARL (Rakelly et al., 2019) to train and test from ofﬂine datasets without exploration;
(4) Contextual BCQ (CBCQ), which is a task-augmented variant of the ofﬂine RL algorithm BCQ
(Fujimoto et al., 2019) by integrating a task latent variable into the state information. We train on
a set of ofﬂine RL tasks, and evaluate the performance of the learnt meta-policy during the training
process on a set of unseen testing ofﬂine RL tasks."
PERFORMANCE EVALUATION OF MERPO,0.2465437788018433,"Fixed α vs Adaptive α. We consider two implementations of MerPO based on the selection of α.
1) MerPO: α is ﬁxed as 0.4 for all tasks; 2) MerPO-Adp: at each iteration k, given the task-policy
πk
n for task n and the meta-policy πk
c at iteration k, we update αk
n using one-step gradient descent to
minimize the following problem.
min
αkn
(1 −αk
n)(D(πk
n, πβ,n) −D(πk
n, πk
c )), s.t. αk
n ∈[0.1, 0.5].
(11)"
PERFORMANCE EVALUATION OF MERPO,0.2488479262672811,"The idea is to adapt αk
n in order to balance between D(πk
n, πβ,n) and D(πk
n, πk
c ), because Theorem
1 implies that the safe policy improvement can be achieved when the impacts of the meta-policy and
the behavior policy are well balanced. Speciﬁcally, at iteration k for each task n, αk
n is increased
when the task-policy πk
n is closer to the meta-policy πk
c , and is decreased when πk
n is closer to the
behavior policy. Note that αk
n is constrained in the range [0.1, 0.5] as suggested by Theorem 1."
PERFORMANCE EVALUATION OF MERPO,0.2511520737327189,"Results.
As illustrated in Figure 6, MerPO-Adp yields the best performance, and both MerPO-
Adp and MerPO achieve better or comparable performance in contrast to existing ofﬂine Meta-RL
approaches. Since the meta-policy changes during the learning process and the qualities of the
behavior policies vary across different tasks, MerPO-Adp adapts α across different iterations and
tasks so as to achieve a ‘local’ balance between the impacts of the meta-policy and the behavior
policy. As expected, MerPO-Adp can perform better than MerPO with a ﬁxed α. Here the best
testing performance for the baseline algorithms is selected over different qualities of ofﬂine datasets."
PERFORMANCE EVALUATION OF MERPO,0.2534562211981567,Ablation Study. We next provide ablation studies by answering the following questions.
PERFORMANCE EVALUATION OF MERPO,0.2557603686635945,"(1) Is RAC important for within-task policy optimization? To answer this question, we compare
MerPO with the approach Eq. (5) where the within-task policy optimization is only regularized by"
PERFORMANCE EVALUATION OF MERPO,0.25806451612903225,Published as a conference paper at ICLR 2022
PERFORMANCE EVALUATION OF MERPO,0.26036866359447003,"Figure 6: Performance comparison in terms of the average return in different environments. Clearly,
MerPO Adp and MerPO achieve better or comparable performance than the baselines."
PERFORMANCE EVALUATION OF MERPO,0.2626728110599078,"(a) Impact of RAC mod-
ule."
PERFORMANCE EVALUATION OF MERPO,0.26497695852534564,"(b) Impact of model uti-
lization."
PERFORMANCE EVALUATION OF MERPO,0.2672811059907834,"(c) Performance under dif-
ferent data qualities."
PERFORMANCE EVALUATION OF MERPO,0.2695852534562212,"(d) Testing
performance
for expert dataset."
PERFORMANCE EVALUATION OF MERPO,0.271889400921659,Figure 7: Ablation study of MerPO in Walker-2D-Params.
PERFORMANCE EVALUATION OF MERPO,0.27419354838709675,"the meta-policy. As shown in Figure 7(a), with the regularization based on the behavior policy in
RAC, MerPO performs signiﬁcantly better than Eq. (5), implying that the safe policy improvement
property of RAC enables MerPO to continuously improve the meta-policy."
PERFORMANCE EVALUATION OF MERPO,0.2764976958525346,"(2) Is learning the dynamics model important? Without the utilization of models, the within-task
policy optimization degenerates to CQL (Kumar et al., 2020) and the Meta-RL algorithm becomes a
model-free approach. Figure 7(b) shows the performance comparison between the cases whether the
dynamics model is utilized. It can be seen that the performance without model utilization is much
worse than that of MerPO. This indeed makes sense because the task identity inference (Dorfman
& Tamar, 2020; Li et al., 2020a;b) is a critical problem in Meta-RL. Such a result also aligns well
with a recently emerging trend in supervised meta-learning to improve the generalization ability by
augmenting the tasks with “more data” (Rajendran et al., 2020; Yao et al., 2021)."
PERFORMANCE EVALUATION OF MERPO,0.27880184331797236,"(3) How does MerPO perform in unseen ofﬂine tasks under different data qualities? We eval-
uate the average return in unseen ofﬂine tasks with different data qualities, and compare the per-
formance between (1) MerPO with α = 0.4 (“With meta”) and (2) Run a variant of COMBO with
behavior-regularized policy improvement, i.e., α = 1 (“With beha only”). For a fair comparison, we
initialize the policy network with the meta-policy in both cases. As shown in Figure 7(c), the aver-
age performance of “With meta” over different data qualities is much better than that of “With beha
only”. More importantly, for a new task with expert data, MerPO (“With meta”) clearly outperforms
COMBO as illustrated in Figure 7(d), whereas the performance of FOCAL is worse than COMBO."
CONCLUSION,0.28110599078341014,"6
CONCLUSION"
CONCLUSION,0.2834101382488479,"In this work, we study ofﬂine Meta-RL aiming to strike a right balance between “exploring” the
out-of-distribution state-actions by following the meta-policy and “exploiting” the ofﬂine dataset
by staying close to the behavior policy. To this end, we propose a model-based ofﬂine Meta-RL
approach, namely MerPO, which learns a meta-model to enable efﬁcient task model learning and a
meta-policy to facilitate safe exploration of out-of-distribution state-actions. Particularly, we devise
RAC, a meta-regularized model-based actor-critic method for within-task policy optimization, by
using a weighted interpolation between two regularizers, one based on the behavior policy and
the other on the meta-policy. We theoretically show that the learnt task-policy via MerPO offers
safe policy improvement over both the behavior policy and the meta-policy. Compared to existing
ofﬂine Meta-RL methods, MerPO demonstrates superior performance on several benchmarks, which
suggests a more prominent role of model-based approaches in ofﬂine Meta-RL."
CONCLUSION,0.2857142857142857,Published as a conference paper at ICLR 2022
CONCLUSION,0.2880184331797235,ACKNOWLEDGEMENT
CONCLUSION,0.2903225806451613,"This work is supported in part by NSF Grants CNS-2003081, CNS-2203239, CPS-1739344, and
CCSS-2121222."
REPRODUCIBILITY STATEMENT,0.2926267281105991,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.29493087557603687,"For the theoretical results presented in the main text, we state the full set of assumptions of all
theoretical results in Appendix B, and include the complete proofs of all theoretical results in Ap-
pendix C. For the experimental results presented in the main text, we include the code in the supple-
mental material, and specify all the training details in Appendix A. For the datasets used in the main
text, we also give a clear explanation in Appendix A."
REFERENCES,0.29723502304147464,REFERENCES
REFERENCES,0.2995391705069124,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.30184331797235026,"Ron Dorfman and Aviv Tamar.
Ofﬂine meta reinforcement learning.
arXiv preprint
arXiv:2008.02598, 2020."
REFERENCES,0.30414746543778803,"Rasool Fakoor, Jonas Mueller, Pratik Chaudhari, and Alexander J Smola. Continuous doubly con-
strained batch reinforcement learning. arXiv preprint arXiv:2102.09225, 2021."
REFERENCES,0.3064516129032258,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1126–1135. JMLR. org, 2017."
REFERENCES,0.3087557603686636,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.31105990783410137,"Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587–1596. PMLR, 2018."
REFERENCES,0.31336405529953915,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019."
REFERENCES,0.315668202764977,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861–1870. PMLR, 2018."
REFERENCES,0.31797235023041476,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. arXiv preprint arXiv:1906.08253, 2019."
REFERENCES,0.32027649769585254,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofﬂine reinforcement learning. arXiv preprint arXiv:2005.05951, 2020."
REFERENCES,0.3225806451612903,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019."
REFERENCES,0.3248847926267281,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.3271889400921659,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.3294930875576037,"Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Henrik Christensen, and Hao Su.
Multi-task batch reinforcement learning with metric learning. Advances in Neural Information
Processing Systems, 33, 2020a."
REFERENCES,0.3317972350230415,"Lanqing Li, Rui Yang, and Dijun Luo. Efﬁcient fully-ofﬂine meta-reinforcement learning via dis-
tance metric learning and behavior regularization. arXiv preprint arXiv:2010.01112, 2020b."
REFERENCES,0.33410138248847926,Published as a conference paper at ICLR 2022
REFERENCES,0.33640552995391704,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient
with stationary distribution correction. In Uncertainty in Artiﬁcial Intelligence, pp. 1180–1190.
PMLR, 2020."
REFERENCES,0.3387096774193548,"Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oﬁr Nachum, and Shixiang Gu. Deployment-
efﬁcient reinforcement learning via model-based ofﬂine optimization.
arXiv preprint
arXiv:2006.03647, 2020."
REFERENCES,0.34101382488479265,"Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Ofﬂine meta-
reinforcement learning with advantage weighting. arXiv preprint arXiv:2008.06043, 2020."
REFERENCES,0.3433179723502304,"Oﬁr Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019."
REFERENCES,0.3456221198156682,"Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018."
REFERENCES,0.347926267281106,"Janarthanan Rajendran, Alex Irpan, and Eric Jang. Meta-learning requires meta-augmentation. arXiv
preprint arXiv:2007.05549, 2020."
REFERENCES,0.35023041474654376,"Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efﬁcient off-policy
meta-reinforcement learning via probabilistic context variables. arXiv preprint arXiv:1903.08254,
2019."
REFERENCES,0.35253456221198154,"Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the global optimality of model-
agnostic meta-learning.
In International Conference on Machine Learning, pp. 9837–9846.
PMLR, 2020."
REFERENCES,0.3548387096774194,"Huaxiu Yao, Long-Kai Huang, Linjun Zhang, Ying Wei, Li Tian, James Zou, Junzhou Huang, et al.
Improving generalization in meta-learning via task augmentation. In International Conference on
Machine Learning, pp. 11887–11897. PMLR, 2021."
REFERENCES,0.35714285714285715,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma.
Mopo:
Model-based ofﬂine policy optimization.
arXiv preprint
arXiv:2005.13239, 2020."
REFERENCES,0.35944700460829493,"Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine, and Chelsea
Finn. Conservative data sharing for multi-task ofﬂine reinforcement learning. arXiv preprint
arXiv:2109.08128, 2021a."
REFERENCES,0.3617511520737327,"Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative ofﬂine model-based policy optimization. arXiv preprint arXiv:2102.08363,
2021b."
REFERENCES,0.3640552995391705,"Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efﬁcient meta learning via
minibatch proximal update. Advances in Neural Information Processing Systems, 32:1534–1544,
2019."
REFERENCES,0.3663594470046083,"Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson.
Varibad: A very good method for bayes-adaptive deep rl via meta-
learning. arXiv preprint arXiv:1910.08348, 2019."
REFERENCES,0.3686635944700461,Published as a conference paper at ICLR 2022
REFERENCES,0.3709677419354839,Table 1: Hyperparameters for RAC.
REFERENCES,0.37327188940092165,"Hyperparameters
Halfcheetah
Hopper
Walker2d
Discount factor
0.99
0.99
0.99
Sample batch size
256
256
256
Real data ratio
0.5
0.5
0.5
Model rollout length
5
5
1
Critic lr
3e-4
3e-4
1e-4
Actor lr
1e-4
1e-4
1e-5
Model lr
1e-3
1e-3
1e-3
Optimizer
Adam
Adam
Adam
β
1
1
10
Max entropy
True
True
True
λ
1
1
1"
REFERENCES,0.37557603686635943,"A
EXPERIMENTAL DETAILS"
REFERENCES,0.3778801843317972,"A.1
META ENVIRONMENT DESCRIPTION"
REFERENCES,0.38018433179723504,"• Walker-2D-Params: Train an agent to move forward. Different tasks correspond to different
randomized dynamcis parameters."
REFERENCES,0.3824884792626728,"• Half-Cheeta-Fwd-Back: Train a Cheetah robot to move forward or backward, and the re-
ward function depends on the moving direction. All tasks have the same dynamics model
but different reward functions."
REFERENCES,0.3847926267281106,"• Ant-Fwd-Back: Train an Ant robot to move forward or backward, and the reward function
depends on the moving direction. All tasks have the same dynamics model but different
reward functions."
REFERENCES,0.3870967741935484,"• Poing-Robot-Wind: Point-Robot-Wind is a variant of Sparse-Point-Robot (Li et al., 2020b),
a 2D navigation problem introduced in (Rakelly et al., 2019), where each task is to guide
a point robot to navigate to a speciﬁc goal location on the edge of a semi-circle from the
origin. In Point-Robot-Wind, each task is affected by a distinct “wind” uniformly sampled
from [−0.05, 0.05]2, and hence differs in the transition dynamics."
REFERENCES,0.38940092165898615,"A.2
IMPLEMENTATION DETAILS AND MORE EXPERIMENTS"
REFERENCES,0.391705069124424,"A.2.1
EVALUATION OF RAC"
REFERENCES,0.39400921658986177,"Model learning. Following the same line as in (Janner et al., 2019; Yu et al., 2020; 2021b), the
dynamics model for each task is represented as a probabilistic neural network that takes the current
state-action as input and outputs a Gaussian distribution over the next state and reward:
bTθ(st+1, r|s, a) = N(µθ(st, at), Σθ(st, at)).
An ensemble of 7 models is trained independently using maximum likelihood estimation, and the
best 5 models are picked based on the validation prediction error using a held-our set of the ofﬂine
dataset. Each model is represented by a 4-layer feedforward neural network with 256 hidden units.
And one model will be randomly selected from the best 5 models for model rollout."
REFERENCES,0.39631336405529954,"Policy optimization. We represent both Q-network and policy network as a 4-layer feedforward
neural network with 256 hidden units, and use clipped double Q-learning (Fujimoto et al., 2018) for
Q backup update. A max entropy term is also included to the value function for computing the target
Q value as in SAC (Haarnoja et al., 2018). The hyperparameters used for evaluating the performance
of RAC are described in Table 1."
REFERENCES,0.3986175115207373,"Additional experiments. We also evaluate the performance of RAC in Walker2d under different
qualities of the meta-policy. As shown in Figure 8(a) and 8(b), RAC achieves the best performance
in both scenarios, compared to COMBO and COMBO-3. Particularly, the performance of COMBO-
3 in Figure 8(a) degrades in the later stage of training because the meta-policy is not superior over
the behavior policy in this case. In stark contrast, the performance of RAC is consistently better, as
it provides a safe policy improvement guarantee over both the behavior policy and the meta-policy."
REFERENCES,0.4009216589861751,Published as a conference paper at ICLR 2022
REFERENCES,0.4032258064516129,"(a)
Performance
in
Walker2d
with
a
good
meta-policy."
REFERENCES,0.4055299539170507,"(b)
Performance
in
Walker2d with a random
meta-policy."
REFERENCES,0.4078341013824885,"(c)
Performance
in
HalfCheetah with expert
ofﬂine dataset."
REFERENCES,0.41013824884792627,"(d) Average return over
different
qualities
of
meta-policies
under
ex-
pert dataset for different
choices of α."
REFERENCES,0.41244239631336405,Figure 8: Performance evaluation of RAC.
REFERENCES,0.4147465437788018,"Beside, we also compare the performance of these three algorithms under an expert behavior policy
in Figure 8(c), where a meta-policy usually interferes the policy optimization and drags the learnt
policy away from the expert policy. As expected, RAC can still achieve comparable performance
with COMBO, as a result of safe policy improvement over the behavior policy for suitable parameter
selections."
REFERENCES,0.41705069124423966,"We examine the impact of α on the performance of RAC under different qualities of the meta-policy
for HalfCheetah with expert data. In this case, the meta-policy is a worse policy compared to the
behavior policy. As shown in Figure 8(d), the performance α = 0.4 is comparable to the case of
α = 1 where the policy improvement step is only regularized based on the behavior policy, and
clearly better than the other two cases."
REFERENCES,0.41935483870967744,"A.2.2
EVALUATION OF MERPO"
REFERENCES,0.4216589861751152,"Data collection. We collect the ofﬂine dataset for each task by training a stochastic policy network
using SAC (Haarnoja et al., 2018) for that task and rolling out the policies saved at each checkpoint
to collect trajectories. Different checkpoints correspond to different qualities of the ofﬂine datasets.
When training with MerPO, we break the trajectories into independent tuples {si, ai, ri, s′
i} and
store in a replay buffer. Therefore, the ofﬂine dataset for each task does not contain full trajectories
over entire episodes, but merely individual transitions collected by some behavior policy."
REFERENCES,0.423963133640553,"Setup. For each testing task, we obtain the task-speciﬁc policy through quick adaptation using the
within-task policy optimization method RAC, based on its own ofﬂine dataset and the learnt meta-
policy, and evaluate the average return of the adapted policy over 4 random seeds. As shown earlier,
we take α = 0.4 for all experiments about MerPO. In MerPO-Adp, we initialize α with 0.4 and
update with a learning rate of 1e −4."
REFERENCES,0.42626728110599077,"Meta-model learning. Similar as in section A.2.1, for each task we quickly adapt from the meta-
model to obtain an ensemble of 7 models and pick the best 5 models based on the validation error.
The neural network used for representing the dynamics model is same with that in section A.2.1."
REFERENCES,0.42857142857142855,"Meta-policy learning. As in RAC, we represent the task Q-network, the task policy network and
the meta-policy network as a 5-layer feedforward neural network with 300 hidden units, and use
clipped double Q-learning (Fujimoto et al., 2018) for within task Q backup update. For each task,
we also use dual gradient descent to automatically tune both the parameter β for conservative policy
evaluation and the parameter λ for regularized policy improvement:"
REFERENCES,0.4308755760368664,"• Tune β. Before optimizing the Q-network in policy evaluation, we ﬁrst optimize β by
solving the following problem:"
REFERENCES,0.43317972350230416,"min
Q max
β≥0 β(Es,a∼ρ[Q(s, a)] −Es,a∼D[Q(s, a)] −τ) + 1"
REFERENCES,0.43548387096774194,"2Es,a,s′∼df [(Q(s, a) −bBπ bQk(s, a))2]."
REFERENCES,0.4377880184331797,"Intuitively, the value of β will be increased to penalty the Q-values for out-of-distribution
state-actions if the difference Es,a∼ρ[Q(s, a)]−Es,a∼D[Q(s, a)] is larger than some thresh-
old value τ."
REFERENCES,0.4400921658986175,Published as a conference paper at ICLR 2022
REFERENCES,0.4423963133640553,Table 2: Hyperparameters for MerPO.
REFERENCES,0.4447004608294931,"Hyperparameters
Walker-2D-Params
Half-Cheetah-Fwd-Back
Ant-Fwd-Back
Point-Robot-Wind
Discount factor
0.99
0.99
0.99
0.9
Sample batch size
256
256
256
256
Task batch size
8
2
2
8
Real data ratio
0.5
0.5
0.5
0.5
Model rollout length
1
1
1
1
Inner critic lr
1e-3
1e-3
8e-4
1e-3
Inner actor lr
1e-3
5e-4
5e-4
1e-3
Inner steps
10
10
10
10
Outer critic lr
1e-3
1e-3
1e-3
1e-3
Outer actor lr
1e-3
1e-3
1e-3
1e-3
Meta-q lr
1e-3
1e-3
1e-3
1e-3
Task model lr
1e-4
1e-4
1e-4
1e-4
Meta-model lr
5e-2
5e-2
5e-2
5e-2
Model adaptation steps
25
25
25
25
Optimizer
Adam
Adam
Adam
Adam
Auto-tune λ
True
True
True
True
λ lr
1
1
1
1
λ initial
5
100
100
5
Target divergence
0.05
0.05
0.05
0.05
Auto-tune β
True
True
True
True
log β lr
1e-3
1e-3
1e-3
1e-3
log β initial
0
0
0
0
Q difference threshold
5
10
10
10
Max entropy
True
True
True
True
α
0.4
0.4
0.4
0.4
Testing adaptation steps
100
100
100
100
# training tasks
20
2
2
40
# testing tasks
5
2
2
10"
REFERENCES,0.4470046082949309,"• Tune λ. Similarly, we can optimize λ in policy improvement by solving the following
problem:
max
π
min
λ≥0 Es∼ρ(s),a∼π(·|s)[ bQπ(s, a)] −λα[D(π, πβ) −Dtarget] −λ(1 −α)[D(π, πc) −Dtarget]."
REFERENCES,0.44930875576036866,"Intuitively, the value of λ will be increased so as to have stronger regularizations if the
divergence is larger than some threshold Dtarget, and decreased if the divergence is smaller
than Dtarget."
REFERENCES,0.45161290322580644,"Besides, we also build a meta-Q network Qmeta over the training process as an initialization of
the task Q networks to facilitate the within task policy optimization. At the k-th meta-iteration for
meta-policy update, the meta-Q network is also updated using the average Q-values of current batch
B of training tasks with meta-q learning rate ξq, i.e.,"
REFERENCES,0.4539170506912442,"Qk+1
meta = Qk
meta −ξq[Qk
meta −1 |B| X"
REFERENCES,0.45622119815668205,"n∈B
Qn]."
REFERENCES,0.45852534562211983,"Therefore, we initialize the task Q networks and the task policy with the meta-Q network and the
meta-policy, respectively, for within task policy optimization during both meta-training and meta-
testing. The hyperparameters used in evaluation of MerPO are listed in Table 2."
REFERENCES,0.4608294930875576,"For evaluating the performance improvement in a single new ofﬂine task, we use a smaller learning
rate of 8e −5 for the Q network and the policy network update."
REFERENCES,0.4631336405529954,"A.2.3
MORE EXPERIMENTS."
REFERENCES,0.46543778801843316,"We also evaluate the impact of the utilization extent of the learnt model, by comparing the perfor-
mance of MerPO under different cases of real data ratio, i.e., the ratio of the data from the ofﬂine
dataset in the data batch for training. As shown in Figure 9(a), the performance of MerPO can be
further boosted with a more conservative utilization of the model."
REFERENCES,0.46774193548387094,"To understand how much beneﬁt MerPO can bring for policy learning in unseen ofﬂine RL tasks,
we compare the performance of the following cases with respect to the gradient steps taken for
learning in unseen ofﬂine RL tasks: (1) Initialize the task policy network with the meta-policy and
run RAC (“With meta”); (2) Run RAC using the meta-policy without network initialization (“With
meta (no init)”); (3) Run RAC with a single regularization based on behavior policy without network
initialization, i.e., α = 1 (“With beha only”); (4) Run COMBO (“No regul.”). As shown in Figure
9(b), “With meta” achieves the best performance and improves signiﬁcantly over “No regul.” and
“With beha only”, i.e., learning alone without any guidance of meta-policy, which implies that the"
REFERENCES,0.4700460829493088,Published as a conference paper at ICLR 2022
REFERENCES,0.47235023041474655,"(a) Impact of real data ratio.
(b) Performance comparison for
unseen tasks."
REFERENCES,0.47465437788018433,"(c) Training sample efﬁciency in
Point-Robot-Wind."
REFERENCES,0.4769585253456221,Figure 9: Ablation study of MerPO.
REFERENCES,0.4792626728110599,"(a) Training efﬁciency.
(b) Testing efﬁciency."
REFERENCES,0.4815668202764977,Figure 10: Sample efﬁciency.
REFERENCES,0.4838709677419355,"learnt meta-policy can efﬁciently guide the exploration of out-of-distribution state-actions. Without
network initialization, “With meta (no init)” and “With beha only” achieve similar performance
because good ofﬂine dataset is considered here. Such a result is also consistent with Figure 8(d)."
REFERENCES,0.4861751152073733,"We evaluate the testing performance of MerPO, by changing sample size of all tasks. Figure 10(a)
shows that the performance of MerPO is stable even if we decrease the number of trajectories for
each task to be around 200. In contrast, the number of trajectories collected in other baselines is
of the order 103. Figure 10(b) illustrates the testing sample efﬁciency of MerPO, by evaluating the
performance at new ofﬂine tasks under different sample sizes. Clearly, a good task-speciﬁc policy
can be quickly adapted at a new task even with 5 trajectories (1000 samples) of ofﬂine data. We also
evaluate the training sample efﬁciency of MerPO in Point-Robot-Wind. As shown in Figure 9(c) the
performance of MerPO is stable even if we decrease the number of trajectories for each task to be
around 200."
REFERENCES,0.48847926267281105,"A.2.4
MORE COMPARISON BETWEEN FOCAL AND COMBO"
REFERENCES,0.49078341013824883,"Following the setup as in Figure 1, we compare the performance between FOCAL and COMBO
in two more environments: Half-Cheetah-Fwd-Back and Ant-Fwd-Back. As shown in Figure 11,
although FOCAL performs better than COMBO on the task with a bad-quality dataset, it is outper-
formed by COMBO on the task with a good-quality dataset. This further conﬁrms the observation
made in Figure 1."
REFERENCES,0.4930875576036866,"A.3
ALGORITHMS"
REFERENCES,0.49539170506912444,We include the details of MerPO in Algorithm 2.
REFERENCES,0.4976958525345622,Published as a conference paper at ICLR 2022
REFERENCES,0.5,"(a) Performance comparison in Half-Cheetah-Fwd-
Back."
REFERENCES,0.5023041474654378,(b) Performance comparison in Ant-Fwd-Back.
REFERENCES,0.5046082949308756,Figure 11: FOCAL vs. COMBO.
REFERENCES,0.5069124423963134,Algorithm 2 Regularized policy optimization for model-based ofﬂine Meta-RL (MerPO)
REFERENCES,0.5092165898617511,"1: Initialize the dynamics, actor and critic for each task, and initialize the meta-model and the
meta-policy;
2: for k = 1, 2, ... do
3:
for each training task Mn do
4:
Solve the following problem with gradient descent for h steps to compute the dynamics
model bTθk
n based on the ofﬂine dataset Di:
min
θn
E(s,a,s′)∼Dn[log bTθn(s′|s, a)] + η∥θn −φm(k)∥2
2;"
REFERENCES,0.511520737327189,"5:
end for
6:
Update φm(k + 1) = φm(k) −ξ1[φm(k) −1"
REFERENCES,0.5138248847926268,"N
PN
n=1 θk
n];
7: end for
8: Quickly obtain the estimated dynamics model bTn for each training task by solving Eq. (4) with
t steps gradient descent;
9: for k = 1, 2, ... do
10:
for each training task Mn do
11:
for j = 1, ..., J do
12:
Perform model rollouts with bTn starting from states in Dn and add model rollouts to
Dn
model;
13:
Policy evaluation by recursively solving Eq. (1) using data from Dn ∪Dn
model;
14:
Given the meta-policy πk
c , improve policy πk
n by solving Eq. (7);
15:
end for
16:
end for
17:
Given the learnt policy πk
n for each task, update the meta-policy πk+1
c
by solving Eq. (10)
with one step gradient descent;
18: end for"
REFERENCES,0.5161290322580645,"B
PRELIMINARIES"
REFERENCES,0.5184331797235023,"For ease of exposition, let TM and rM denote the dynamics and reward function of the underlying
MDP M, TM and rM denote the dynamics and reward function of the empirical MDP M induced
by the dataset D, and T c
M and r c
M denote the dynamics and reward function of the learnt MDP c
M.
To prevent any trivial bound with ∞values, we assume that the cardinality of a state-action pair in
the dataset D, i.e., |D(s, a)|, in the denominator, is non-zero, by setting |D(s, a)| to be a small value
less than 1 when (s, a) /∈D."
REFERENCES,0.5207373271889401,"Following the same line as in (Kumar et al., 2020; Yu et al., 2021b), we make the following standard
assumption on the concentration properties of the reward and dynamics for the empirical MDP M
to characterize the sampling error."
REFERENCES,0.5230414746543779,"Assumption 1. For any (s, a) ∈M, the following inequalities hold with probability 1 −δ:"
REFERENCES,0.5253456221198156,"∥TM(s′|s, a) −TM(s′|s, a)∥1 ≤
CT,δ
p"
REFERENCES,0.5276497695852534,"|D(s, a)|
;
|rM(s, a) −rM| ≤
Cr,δ
p"
REFERENCES,0.5299539170506913,"|D(s, a)|
where CT,δ and Cr,δ are some constants depending on δ via a
p"
REFERENCES,0.532258064516129,log(1/δ) dependency.
REFERENCES,0.5345622119815668,Published as a conference paper at ICLR 2022
REFERENCES,0.5368663594470046,"Based on Assumption 1, we can bound the estimation error induced by the empirical Bellman backup
operator for any (s, a) ∈M:
Bπ"
REFERENCES,0.5391705069124424,"M bQk(s, a) −Bπ
M bQk(s, a) ="
REFERENCES,0.5414746543778802,"rM(s, a) −rM(s, a) + γ
X"
REFERENCES,0.543778801843318,"s′
(TM(s′|s, a) −TM(s′|s, a))Eπ(a′|s′)[ bQk(s′, a′)] "
REFERENCES,0.5460829493087558,"≤|rM(s, a) −rM(s, a)| + γ  X"
REFERENCES,0.5483870967741935,"s′
(TM(s′|s, a) −TM(s′|s, a))Eπ(a′|s′)[ bQk(s′, a′)] "
REFERENCES,0.5506912442396313,"≤
Cr,δ
p"
REFERENCES,0.5529953917050692,"|D(s, a)|
+ γ∥TM(s′|s, a) −TM(s′|s, a)∥1∥Eπ(a′|s′)[ bQk(s′, a′)]∥∞"
REFERENCES,0.5552995391705069,"≤Cr,δ + γCT,δRmax/(1 −γ)
p"
REFERENCES,0.5576036866359447,"|D(s, a)|"
REFERENCES,0.5599078341013825,"=((1 −γ)Cr,δ/Rmax + γCT,δ)Rmax"
REFERENCES,0.5622119815668203,"(1 −γ)
p"
REFERENCES,0.5645161290322581,"|D(s, a)|"
REFERENCES,0.5668202764976958,"≤(Cr,δ/Rmax + CT,δ)Rmax"
REFERENCES,0.5691244239631337,"(1 −γ)
p"
REFERENCES,0.5714285714285714,"|D(s, a)|
≜
Cr,T,δRmax
(1 −γ)
p"
REFERENCES,0.5737327188940092,"|D(s, a)|
."
REFERENCES,0.576036866359447,"Similarly, we can bound the difference between the Bellman backup induced by the learnt MDP c
M
and the underlying Bellman backup:
Bπ
c
M bQk(s, a) −Bπ
M bQk(s, a)"
REFERENCES,0.5783410138248848,"≤|r c
M(s, a) −rM(s, a)| + γRmax"
REFERENCES,0.5806451612903226,"1 −γ Dtv(T c
M, TM)"
REFERENCES,0.5829493087557603,"where Dtv(T c
M, TM) is the total-variation distance between T c
M and TM."
REFERENCES,0.5852534562211982,"For any two MDPs, M1 and M2, with the same state space, action space and discount factor γ,
and a given fraction f ∈(0, 1), deﬁne the f-interpolant MDP Mf as the MDP with dynamics:
TMf = fTM1 + (1 −f)TM2 and reward function: rMf = frM1 + (1 −f)rM2, which has the
same state space, action space and discount factor with M1 and M2. Let T π be the transition matrix
on state-action pairs induced by a stationary policy π, i.e.,
T π = T(s′|s, a)π(a′|s′)."
REFERENCES,0.5875576036866359,"To prove the main result, we ﬁrst restate the following lemma from (Yu et al., 2021b) to be used
later."
REFERENCES,0.5898617511520737,"Lemma 1. For any policy π, its returns in any MDP M, denoted by J(M, π), and in Mf, denoted
by J(M1, M2, f, π), satisfy the following:
J(M, π) −η ≤J(M1, M2, f, π) ≤J(M, π) + η
where"
REFERENCES,0.5921658986175116,η =2γ(1 −f)
REFERENCES,0.5944700460829493,"(1 −γ)2 RmaxDtv(TM2, TM) +
γf
1 −γ |Edπ
M[(T π
M −T π
M1)Qπ
M]|"
REFERENCES,0.5967741935483871,"+
f
1 −γ Es,a∼dπ
M[|rM1(s, a) −rM(s, a)|] + 1 −f"
REFERENCES,0.5990783410138248,"1 −γ Es,a∼dπ
M [|rM2(s, a) −rM(s, a)|]."
REFERENCES,0.6013824884792627,"Lemma 1 characterizes the relationship between policy returns in different MDPs in terms of the
corresponding reward difference and dynamics difference."
REFERENCES,0.6036866359447005,"C
PROOF OF THEOREM 1"
REFERENCES,0.6059907834101382,"Let d(s, a) := dπβ
M(s, a). In the setting without function approximation, by setting the derivation of
Equation Eq. (1) to 0, we have that
bQk+1(s, a) = bBπ bQk(s, a) −β ρ(s, a) −d(s, a)"
REFERENCES,0.6082949308755761,"df(s, a)
."
REFERENCES,0.6105990783410138,Published as a conference paper at ICLR 2022
REFERENCES,0.6129032258064516,"Denote ν(ρ, f) = Eρ
h
ρ(s,a)−d(s,a)"
REFERENCES,0.6152073732718893,"df (s,a)
i
as the expected penalty on the Q-value. It can be shown (Yu"
REFERENCES,0.6175115207373272,"et al., 2021b) that ν(ρ, f) ≥0 and increases with f, for any ρ and f ∈(0, 1). Then, RAC optimizes
the return of a policy in a f-interpolant MDP induced by the empirical MDP M and the learnt MDP
c
M, which is regularized by both the behavior policy πβ and the meta-policy πc:"
REFERENCES,0.619815668202765,"max
π
J(M, c
M, f, π) −β ν(ρπ, f)"
REFERENCES,0.6221198156682027,"1 −γ
−λαD(π, πβ) −λ(1 −α)D(π, πc).
(12)"
REFERENCES,0.6244239631336406,"Denote πo as the solution to the above optimization problem. Based on Lemma 1, we can ﬁrst
characterize the return of the learnt policy πo in the underlying MDP M in terms of its return in the
f-interpolant MDP:
J(M, πo) + η1 ≥J(M, c
M, f, πo)
(13)
where"
REFERENCES,0.6267281105990783,η1 =2γ(1 −f)
REFERENCES,0.6290322580645161,"(1 −γ)2 RmaxDtv(T c
M, TM) +
γf
1 −γ |Edπo
M[(T πo
M −T πo"
REFERENCES,0.631336405529954,"M)Qπo
M]|"
REFERENCES,0.6336405529953917,"+
f
1 −γ Es,a∼dπo
M[|rM(s, a) −rM(s, a)|] + 1 −f"
REFERENCES,0.6359447004608295,"1 −γ Es,a∼dπo
M[|r c
M(s, a) −rM(s, a)|]"
REFERENCES,0.6382488479262672,≤2γ(1 −f)
REFERENCES,0.6405529953917051,"(1 −γ)2 RmaxDtv(T c
M, TM) + γ2fCT,δRmax"
REFERENCES,0.6428571428571429,"(1 −γ)2
Es∼dπo
M(s) ""s"
REFERENCES,0.6451612903225806,"|A|
|D(s)| q"
REFERENCES,0.6474654377880185,"DCQL(πo, πβ)(s) + 1 #"
REFERENCES,0.6497695852534562,"+ Cr,δ"
REFERENCES,0.652073732718894,"1 −γ Es,a∼dπo
M ""
1
p"
REFERENCES,0.6543778801843319,"|D(s, a)| #"
REFERENCES,0.6566820276497696,"+
1
1 −γ Es,a∼dπo
M[|r c
M(s, a) −rM(s, a)|]"
REFERENCES,0.6589861751152074,"≜ηc
1.
Note that the inequality above holds because the following is true for the empirical MDP M (Kumar
et al., 2020):"
REFERENCES,0.6612903225806451,"|Edπ
M[(T π
M −T π"
REFERENCES,0.663594470046083,"M)Qπ
M]| ≤γCT,δRmax"
REFERENCES,0.6658986175115207,"1 −γ
Es∼dπ
M(s) ""s"
REFERENCES,0.6682027649769585,"|A|
|D(s)| q"
REFERENCES,0.6705069124423964,"DCQL(π, πβ)(s) + 1 #"
REFERENCES,0.6728110599078341,"for DCQL(π1, π2)(s) := P"
REFERENCES,0.6751152073732719,"a π1(a|s)

π1(a|s)
π2(a|s) −1

."
REFERENCES,0.6774193548387096,"C.1
SAFE IMPROVEMENT OVER πc"
REFERENCES,0.6797235023041475,"We ﬁrst show that the learnt policy offers safe improvement over the meta-policy πc. Following the
same line as in Eq. (13), we next bound the return of the meta-policy πc in the underlying MDP M
from above, in terms of its return in the f-interpolant MDP:
J(M, c
M, f, πc) ≥J(M, πc) −η2
where"
REFERENCES,0.6820276497695853,η2 ≤2γ(1 −f)
REFERENCES,0.684331797235023,"(1 −γ)2 RmaxDtv(T c
M, TM) + γ2fCT,δRmax"
REFERENCES,0.6866359447004609,"(1 −γ)2
Es∼dπc
M(s) ""s"
REFERENCES,0.6889400921658986,"|A|
|D(s)| q"
REFERENCES,0.6912442396313364,"DCQL(πc, πβ)(s) + 1 #"
REFERENCES,0.6935483870967742,"+ Cr,δ"
REFERENCES,0.695852534562212,"1 −γ Es,a∼dπc
M ""
1
p"
REFERENCES,0.6981566820276498,"|D(s, a)| #"
REFERENCES,0.7004608294930875,"+
1
1 −γ Es,a∼dπc
M[|r c
M(s, a) −rM(s, a)|]"
REFERENCES,0.7027649769585254,"≜ηc
2."
REFERENCES,0.7050691244239631,It follows that
REFERENCES,0.7073732718894009,"J(M, πo) + ηc
1 −β ν(ρπo, f)"
REFERENCES,0.7096774193548387,"1 −γ
−λαD(πo, πβ) −λ(1 −α)D(πo, πc)"
REFERENCES,0.7119815668202765,"≥J(M, c
M, f, πo) −β ν(ρπo, f)"
REFERENCES,0.7142857142857143,"1 −γ
−λαD(πo, πβ) −λ(1 −α)D(πo, πc)"
REFERENCES,0.716589861751152,"≥J(M, c
M, f, πc) −β ν(ρπc, f)"
REFERENCES,0.7188940092165899,"1 −γ
−λαD(πc, πβ)"
REFERENCES,0.7211981566820277,"≥J(M, πc) −ηc
2 −β ν(ρπc, f)"
REFERENCES,0.7235023041474654,"1 −γ
−λαD(πc, πβ),"
REFERENCES,0.7258064516129032,Published as a conference paper at ICLR 2022
REFERENCES,0.728110599078341,"where the second inequality is true because πo is the solution to Eq. (12). This gives us a lower
bound on J(M, πo) in terms of J(M, πc):"
REFERENCES,0.7304147465437788,"J(M, πo) ≥J(M, πc)−ηc
1 −ηc
2 +
β
1 −γ [ν(ρπo, f) −ν(ρπc, f)]"
REFERENCES,0.7327188940092166,"+ λαD(πo, πβ) + λ(1 −α)D(πo, πc) −λαD(πc, πβ)."
REFERENCES,0.7350230414746544,"It is clear that ηc
1 and ηc
2 are independent to β and λ. To show the performance improvement of πo
over the meta-policy πc, it sufﬁces to guarantee that for appropriate choices of β and λ,"
REFERENCES,0.7373271889400922,"∆c = λαD(πo, πβ) + λ(1 −α)D(πo, πc) −λαD(πc, πβ) +
β
1 −γ [ν(ρπo, f) −ν(ρπc, f)] > 0."
REFERENCES,0.7396313364055299,"To this end, the following lemma ﬁrst provides an upper bound on |ν(ρπo, f) −ν(ρπc, f)|:"
REFERENCES,0.7419354838709677,"Lemma 2. There exist some positive constants L1 and L2 such that
|ν(ρπo, f) −ν(ρπc, f)| ≤2(L1 + L2)Dtv(ρπo(s, a)||ρπc(s, a))."
REFERENCES,0.7442396313364056,"Proof. First, we have that
|ν(ρπo, f) −ν(ρπc, f)|"
REFERENCES,0.7465437788018433,"=
Eρπo

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a)"
REFERENCES,0.7488479262672811,"
−Eρπc

ρπc(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπc(s, a)  =  X (s,a)"
REFERENCES,0.7511520737327189,"
ρπo(s, a)
ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a) −ρπc(s, a)
ρπc(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπc(s, a)  ≤  X (s,a)"
REFERENCES,0.7534562211981567,"
ρπo(s, a)
ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a) −ρπc(s, a)
ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a)  +  X (s,a)"
REFERENCES,0.7557603686635944,"
ρπc(s, a)
ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a) −ρπc(s, a)
ρπc(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπc(s, a)  =  X"
REFERENCES,0.7580645161290323,"(s,a)
[ρπo(s, a) −ρπc(s, a)]
ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a)  +  X"
REFERENCES,0.7603686635944701,"(s,a)
ρπc(s, a)

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a) −
ρπc(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπc(s, a)  ≤
X"
REFERENCES,0.7626728110599078,"(s,a)
|ρπo(s, a) −ρπc(s, a)|

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a)  +
X"
REFERENCES,0.7649769585253456,"(s,a)
ρπc(s, a)

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a) −
ρπc(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπc(s, a) ."
REFERENCES,0.7672811059907834,"First, observe that for the term

ρπo(s,a)−d(s,a)
fd(s,a)+(1−f)ρπo(s,a)
,"
REFERENCES,0.7695852534562212,"• If ρπo(s, a) ≥d(s, a), then

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a) "
REFERENCES,0.771889400921659,"≤

ρπo(s, a)
fd(s, a) + (1 −f)ρπo(s, a)"
REFERENCES,0.7741935483870968,"≤

ρπo(s, a)
(1 −f)ρπo(s, a)"
REFERENCES,0.7764976958525346,"=
1
1 −f ."
REFERENCES,0.7788018433179723,"• If ρπo(s, a) < d(s, a), then

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a)"
REFERENCES,0.7811059907834101,"≤

d(s, a)
fd(s, a) + (1 −f)ρπo(s, a)"
REFERENCES,0.783410138248848,"≤

d(s, a)
fd(s, a) = 1 f ."
REFERENCES,0.7857142857142857,Published as a conference paper at ICLR 2022
REFERENCES,0.7880184331797235,"Therefore,

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a)"
REFERENCES,0.7903225806451613,"≤max
 1"
REFERENCES,0.7926267281105991,"f ,
1
1 −f"
REFERENCES,0.7949308755760369,"
≜L1."
REFERENCES,0.7972350230414746,"Next, for the term

ρπo(s,a)−d(s,a)
fd(s,a)+(1−f)ρπo(s,a) −
ρπc(s,a)−d(s,a)
fd(s,a)+(1−f)ρπc(s,a)
, consider the function g(x) ="
REFERENCES,0.7995391705069125,"x−d
fd+(1−f)x for x ∈[0, 1]. Clearly, when d(s, a) = 0,

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a) −
ρπc(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπc(s, a) = 0."
REFERENCES,0.8018433179723502,"For any (s, a) that d(s, a) > 0, it can be shown that g(x) is continuous and has bounded gradient,
i.e., |∇g(x)| ≤
1
f 2d ≜L2. Hence, it follows that

ρπo(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπo(s, a) −
ρπc(s, a) −d(s, a)
fd(s, a) + (1 −f)ρπc(s, a)"
REFERENCES,0.804147465437788,"≤L2|ρπo(s, a) −ρπc(s, a)|."
REFERENCES,0.8064516129032258,"Therefore, we can conclude that
|ν(ρπo, f) −ν(ρπc, f)| ≤L1
X"
REFERENCES,0.8087557603686636,"(s,a)
|ρπo(s, a) −ρπc(s, a)| + L2
X"
REFERENCES,0.8110599078341014,"(s,a)
ρπc(s, a)|ρπo(s, a) −ρπc(s, a)|"
REFERENCES,0.8133640552995391,"≤(L1 + L2)
X"
REFERENCES,0.815668202764977,"s,a
|ρπo(s, a) −ρπc(s, a)|"
REFERENCES,0.8179723502304147,"=2(L1 + L2)Dtv(ρπo(s, a)||ρπc(s, a))."
REFERENCES,0.8202764976958525,"Recall that
ρπo(s, a) = dπo
c
M(s)πo(a|s), ρπc(s, a) = dπc
c
M(s)πc(a|s),
which denote the marginal state-action distributions by rolling out πo and πc in the learnt model
c
M, respectively. Lemma 2 gives an upper bound on the difference between the expected penalties
induced under πo and πc, with regard to the difference between the marginal state-action distribu-
tions. Next, we need to characterize the relationship between the marginal state-action distribution
difference and the corresponding policy distance, which is captured in the following lemma."
REFERENCES,0.8225806451612904,"Lemma 3. Let D(π1||π2) = maxs Dtv(π1||π2) denote the maximum total-variation distance be-
tween two policies π1 and π2. Then, we can have that"
REFERENCES,0.8248847926267281,"Dtv(ρπo(s, a)||ρπc(s, a)) ≤
1
1 −γ max
s
Dtv(πo(a|s)||πc(a|s))."
REFERENCES,0.8271889400921659,Proof. Note that
REFERENCES,0.8294930875576036,"Dtv(ρπo(s, a)||ρπc(s, a)) ≤(1 −γ) ∞
X"
REFERENCES,0.8317972350230415,"t=0
γtDtv(ρπo
t (s, a)||ρπc
t (s, a))."
REFERENCES,0.8341013824884793,"It then sufﬁces to bound the state-action marginal difference at time t. Since both state-action
marginals here correspond to rolling out πo and πc in the same MDP c
M, based on Lemma B.1
and B.2 in (Janner et al., 2019), we can obtain that
Dtv(ρπo
t (s, a)||ρπc
t (s, a))
≤Dtv(ρπo
t (s)||ρπc
t (s)) + max
s
Dtv(πo(a|s)||πc(a|s))"
REFERENCES,0.836405529953917,"≤t max
s
Dtv(πo(a|s)||πc(a|s)) + max
s
Dtv(πo(a|s)||πc(a|s))"
REFERENCES,0.8387096774193549,"=(t + 1) max
s
Dtv(πo(a|s)||πc(a|s)),
which indicates that"
REFERENCES,0.8410138248847926,"Dtv(ρπo(s, a)||ρπc(s, a)) ≤(1 −γ) ∞
X"
REFERENCES,0.8433179723502304,"t=0
γt(t + 1) max
s
Dtv(πo(a|s)||πc(a|s))"
REFERENCES,0.8456221198156681,"=
1
1 −γ max
s
Dtv(πo(a|s)||πc(a|s))."
REFERENCES,0.847926267281106,Published as a conference paper at ICLR 2022
REFERENCES,0.8502304147465438,"Building on Lemma 2 and Lemma 3, we can show that"
REFERENCES,0.8525345622119815,"|ν(ρπo, f) −ν(ρπc, f)| ≤2(L1 + L2)"
REFERENCES,0.8548387096774194,"1 −γ
max
s
Dtv(πo(a|s)||πc(a|s))"
REFERENCES,0.8571428571428571,"≜C max
s
Dtv(πo(a|s)||πc(a|s))."
REFERENCES,0.8594470046082949,"Let D(·, ·) = maxs Dtv(·||·). It is clear that for λ ≥λ0 where λ0 >
Cβ
(1−γ)(1−2α) and α < 1 2,"
REFERENCES,0.8617511520737328,"∆c =λαD(πo, πβ) + λ(1 −α)D(πo, πc) −λαD(πc, πβ) +
β
1 −γ [ν(ρπo, f) −ν(ρπc, f)]"
REFERENCES,0.8640552995391705,"=λαD(πo, πβ) + λαD(πo, πc) −λαD(πc, πβ) + λ(1 −2α)D(πo, πc)"
REFERENCES,0.8663594470046083,"+
β
1 −γ [ν(ρπo, f) −ν(ρπc, f)]"
REFERENCES,0.868663594470046,"≥λ(1 −2α)D(πo, πc) +
β
1 −γ [ν(ρπo, f) −ν(ρπc, f)]"
REFERENCES,0.8709677419354839,"≥λ(1 −2α)D(πo, πc) −Cβ"
REFERENCES,0.8732718894009217,"1 −γ D(πo, πc)"
REFERENCES,0.8755760368663594,"=(λ −λ0)(1 −2α)D(πo, πc) +

λ0(1 −2α) −Cβ 1 −γ"
REFERENCES,0.8778801843317973,"
D(πo, πc) > 0."
REFERENCES,0.880184331797235,"In a nutshell, we can conclude that with probability 1 −δ"
REFERENCES,0.8824884792626728,"J(M, πo) ≥J(M, πc) −ηc
1 −ηc
2
|
{z
}
(a)"
REFERENCES,0.8847926267281107,"+(λ −λ0)(1 −2α)D(πo, πc)
|
{z
}
(b)"
REFERENCES,0.8870967741935484,"+

λ0(1 −2α) −Cβ 1 −γ"
REFERENCES,0.8894009216589862,"
D(πo, πc)
|
{z
}
(c) ,"
REFERENCES,0.8917050691244239,"where (a) depends on δ but is independent to λ, (b) is positive and increases with λ, and (c) is
positive. This implies that an appropriate choice of λ will make term (b) large enough to counteract
term (a) and lead to the performance improvement over the meta-policy πc:
J(M, πo) ≥J(M, πc) + ξ1
where ξ1 ≥0."
REFERENCES,0.8940092165898618,"C.2
SAFE IMPROVEMENT OVER πβ"
REFERENCES,0.8963133640552995,"Next, we show that the learnt policy πo achieves safe improvement over the behavior policy πβ.
Based on Lemma 1, we have
J(M1, M2, f, πβ) ≥J(M, πβ) −η3
where"
REFERENCES,0.8986175115207373,η3 =2γ(1 −f)
REFERENCES,0.9009216589861752,"(1 −γ)2 RmaxDtv(T c
M, TM) +
γf
1 −γ |Ed
πβ
M [(T πβ
M −T πβ"
REFERENCES,0.9032258064516129,"M )Qπβ
M]|"
REFERENCES,0.9055299539170507,"+
f
1 −γ Es,a∼d
πβ
M [|rM(s, a) −rM(s, a)|] + 1 −f"
REFERENCES,0.9078341013824884,"1 −γ Es,a∼d
πβ
M [|r c
M(s, a) −rM(s, a)|]"
REFERENCES,0.9101382488479263,≤2γ(1 −f)
REFERENCES,0.9124423963133641,"(1 −γ)2 RmaxDtv(T c
M, TM) + γ2fCT,δRmax"
REFERENCES,0.9147465437788018,"(1 −γ)2
Es∼d
πβ
M (s) ""s"
REFERENCES,0.9170506912442397,"|A|
|D(s)| #"
REFERENCES,0.9193548387096774,"+ Cr,δ"
REFERENCES,0.9216589861751152,"1 −γ Es,a∼d
πβ
M ""
1
p"
REFERENCES,0.923963133640553,"|D(s, a)| #"
REFERENCES,0.9262672811059908,"+
1
1 −γ Es,a∼d
πβ
M [|r c
M(s, a) −rM(s, a)|]"
REFERENCES,0.9285714285714286,"≜ηβ
3 ."
REFERENCES,0.9308755760368663,Published as a conference paper at ICLR 2022
REFERENCES,0.9331797235023042,"Therefore, it follows that"
REFERENCES,0.9354838709677419,"J(M, πo) + ηc
1 −β ν(ρπo, f)"
REFERENCES,0.9377880184331797,"1 −γ
−λαD(πo, πβ) −λ(1 −α)D(πo, πc)"
REFERENCES,0.9400921658986175,"≥J(M, c
M, f, πo) −β ν(ρπo, f)"
REFERENCES,0.9423963133640553,"1 −γ
−λαD(πo, πβ) −λ(1 −α)D(πo, πc)"
REFERENCES,0.9447004608294931,"≥J(M, c
M, f, πβ) −β ν(ρπβ, f)"
REFERENCES,0.9470046082949308,"1 −γ
−λ(1 −α)D(πβ, πc)"
REFERENCES,0.9493087557603687,"≥J(M, πβ) −ηβ
3 −β ν(ρπβ, f)"
REFERENCES,0.9516129032258065,"1 −γ
−λ(1 −α)D(πβ, πc),"
REFERENCES,0.9539170506912442,"which indicates that with probability 1 −δ
J(M, πc) ≥J(M, πβ) −ηc
1 −ηβ
3 +λαD(πo, πβ) + λ(1 −α)D(πo, πc) −λ(1 −α)D(πβ, πc)"
REFERENCES,0.956221198156682,"+
β
1 −γ [ν(ρπo, f) −ν(ρπβ, f)],"
REFERENCES,0.9585253456221198,"where ηβ
3 is some constant that depends on δ but is independent to β and λ."
REFERENCES,0.9608294930875576,"To conclude, we can have that with probability 1 −2δ
J(M, πo) ≥max{J(M, πc) + ξ1, J(M, πβ) + ξ2}
where"
REFERENCES,0.9631336405529954,"ξ1 = −ηc
1 −ηc
2 + (λ −λ0)(1 −2α)D(πo, πc) +

λ0(1 −2α) −Cβ 1 −γ"
REFERENCES,0.9654377880184332,"
D(πo, πc)
(14)"
REFERENCES,0.967741935483871,"and
ξ2 = −ηc
1 −ηβ
3 + λαD(πo, πβ)+λ(1 −α)D(πo, πc) −λ(1 −α)D(πβ, πc)
(15)"
REFERENCES,0.9700460829493087,"+
β
1 −γ [ν(ρπo, f) −ν(ρπβ, f)].
(16)"
REFERENCES,0.9723502304147466,"Moreover, as we noted earlier, ξ1 > 0 for a suitably selected λ and α < 1"
REFERENCES,0.9746543778801844,"2. For the term ν(ρπo, f) −"
REFERENCES,0.9769585253456221,"ν(ρπβ, f) in ξ2 where ν(ρπ, f) is deﬁned as Eρπ
h
ρπ(s,a)−d(s,a)"
REFERENCES,0.9792626728110599,"df (s,a)
i
, as noted in (Yu et al., 2021b),"
REFERENCES,0.9815668202764977,"ν(ρπβ, f) is expected to be smaller than ν(ρπo, f) in practical scenarios, due to the fact that the
dynamics T c
M learnt via supervised learning is close to the underlying dynamics TM on the states
visited by the behavior policy πβ. This directly indicates that dπβ
c
M(s, a) is close to dπβ"
REFERENCES,0.9838709677419355,"M(s, a) and ρπβ"
REFERENCES,0.9861751152073732,"is close to d(s, a). In this case, let"
REFERENCES,0.988479262672811,"ϵ = β[ν(ρπo, f) −ν(ρπβ, f)]"
REFERENCES,0.9907834101382489,"2λ(1 −γ)D(πo, πβ)
."
REFERENCES,0.9930875576036866,"We can show that for α > 1 2 −ϵ,"
REFERENCES,0.9953917050691244,"∆β =λαD(πo, πβ) + λ(1 −α)D(πo, πc) −λ(1 −α)D(πc, πβ) +
β
1 −γ [ν(ρπo, f) −ν(ρπβ, f)]"
REFERENCES,0.9976958525345622,"=λαD(πo, πβ) + λ(1 −α)D(πo, πc) −λ(1 −α)D(πc, πβ) + 2ϵλD(πo, πβ)
=λ [(2ϵ + α)D(πo, πβ) + (1 −α)D(πo, πc) −(1 −α)D(πc, πβ)]
>λ(1 −α)[D(πo, πβ) + D(πo, πc) −D(πc, πβ)]
>0,
and ∆β increases with λ, which implies that
J(M, πo) ≥J(M, πβ) + ξ2 = J(M, πβ) −ηc
1 −ηβ
3 + ∆β > J(M, πβ)
for an appropriate choice of λ."
