Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.011111111111111112,"Training action space selection for reinforcement learning (RL) is conﬂict-prone
due to complex state-action relationships. To address this challenge, this paper
proposes a Shapley-inspired methodology for training action space categorization
and ranking. To reduce exponential-time shapley computations, the methodology
includes a Monte Carlo simulation to avoid unnecessary explorations. The effec-
tiveness of the methodology is illustrated using a cloud infrastructure resource
tuning case study. It reduces the search space by 80% and categorizes the training
action sets into dispensable and indispensable groups. Additionally, it ranks dif-
ferent training actions to facilitate high-performance yet cost-efﬁcient RL model
design. The proposed data-driven methodology is extensible to different domains,
use cases, and reinforcement learning algorithms."
INTRODUCTION,0.022222222222222223,"1
INTRODUCTION"
INTRODUCTION,0.03333333333333333,"A reinforcement learning (RL) agent learns how to map states to actions in order to maximize a
long-term cumulative award signal in a given environment. Figure 1 shows the various artifacts
of an RL algorithm. An RL problem is deﬁned by a quartet of (S, A, Pa, Ra), where S is a set
of states or the state space; A is a set of actions or the action space available to inﬂuence S;
Pa(s, s′) = Pr(st+1 = s′|st = s, at = a) is the transition probability which is the probability that
action a in state s at time t will lead to state s′ at time t + 1; and ﬁnally, Ra(s, s′) is the immediate
reward signal received after transitioning from state s to state s′, due to action a. An RL agent
training involves recognizing an optimal policy function, π∗: a ←s, from a corpus of {(si, ai)} to
maximize the long-term cumulative reward, P Ra. The reward function, Ra, is deﬁned ab initio for
an efﬁcient goal accomplishment. The transition probability or state-action mapping is deﬁned by
the environment dynamics. In many real-life use cases, the agent cannot directly sense the effect of
its actions on the environment. This is particularly true when the state-action relationship cannot be
modeled by either closed-form analytical expressions [26, 29, 58] or explicit rules as in the popular
games such as Chess [28], Go [47, 46], and Atari [34]. This challenge is well documented in the
literature [51, 57]. To address this challenge for RL model training [52, 33], simulation-based action
models [50] play a pivotal role. The optimal choice of simulation parameters such as the training
action space is a non-trivial challenge because of the curse of dimensionality [17], non-linearity [11],
and non-uniform action set [25]."
INTRODUCTION,0.044444444444444446,"Training data valuation [22, 10] and associated artisanal software engineering efforts constitute a large
part of the machine learning (ML) life cycle (or MLOps). Yet, most research and development efforts
[36] focus on algorithms and infrastructure. Production-grade MLOps needs to handle data lifecycle
management (DLM) [40] challenges including: fairness and bias in labeled datasets [16], data quality
[9], limitations of benchmarks [42], and reproducibility concerns [39]. For RL, the DLM challenges
are further compounded due to complexities arising from non-linear state-action interactions, partially-
observable processes, non-isometric action spaces [32], and strong domain speciﬁcity [21] of action
models. With the recent emphasis on ML explainability, traditional supervised learning and deep
learning research communities are actively working on systematic data-driven frameworks [14, 53]
for training data valuation. We need equivalent frameworks for RL to streamline conﬂict-prone
training action selection [8, 37, 18]. Depending on the agent-environment interactions, different
training actions have different relative contributions to the RL agent performance. Some training
actions are indispensable because of their unique positions in the parametric space. Other dispensable
actions have different relative contributions to the reward function. Remarkably, a high-cardinality"
INTRODUCTION,0.05555555555555555,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.06666666666666667,Figure 1: The schematic diagram for a typical reinforcement learning algorithm
INTRODUCTION,0.07777777777777778,"training action space in many cases leads to lower cumulative reward than a well-designed training
action space. To the best of authors’ knowledge, there is hardly any data-driven tool for the evaluation
of training action space for RL. Such a tool leads to superior RL agent performance, lower model
training and maintenance cost, and strong multi-disciplinary collaboration [35]."
INTRODUCTION,0.08888888888888889,"This paper proposes a shapley-inspired [54] algorithm to categorize and rank training action sets.
It also assists in recognizing cut-off cardinality for the training action space to reduce unnecessary
exploration and ensure polynomial time complexity. While Section 2 describes the algorithms for
RL training action space evaluation, Section 3 illustrates the effectiveness of the algorithms in a
speciﬁc case study. Section 4 discusses the relevant work. Finally, Section 5 presents a summary
with possible future directions."
ALGORITHM,0.1,"2
ALGORITHM"
ALGORITHM,0.1111111111111111,"The proposed framework provides a shapley-inspired methodology [49] to efﬁciently ﬁlter out
unnecessary training actions and discover a near-optimal action set. Figure 2 shows the framework
consists of two algorithms and how these two algorithms interact with each other to design a near-
optimal RL model by training action space ﬁltering and evaluation. The ﬁrst algorithm computes cut-
off cardinality for the training action space. The cut-off cardinality is deﬁned as the minimium number
of action points needed in a training action set to enable high-ﬁdelity predictions within an acceptable
computational complexity and an error bound. The second algorithm takes action points only above
the cut-off cardinality and categorize action points into two classes: {dispensable, indispensable}
and ranking dispensable training action points based on the corresponding cumulative rewards.
Indispensable action points are absolute essential action points for the RL training to achieve the
desired goal. On the other hand, dispensable action points are non-essential action points that can be
discarded. However, the RL agent might perform sub-optimally without a disposable action point. By
ranking disposable training action points based on the corresponding cumulative rewards, the relative
contributions of different training action points are estimated."
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.12222222222222222,"2.1
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS"
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.13333333333333333,"RL deals with compilation of a quartet (S, A, Pa, Ra) and computation of an optimal policy, π∗
which maximizes the long-term cumulative reward. The search space for a shapley-inspired technique
needs to span across the power set for the training action space which has exponential complexity
with training action points. To circumvent this problem, the proposed algorithm, as discussed in
Algorithm 1, uses Monte Carlo method to compute the cut-off cardinality. The inputs to this algorithm
include an RL algorithm, the power set for the training action space, the corresponding state space,"
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.14444444444444443,Under review as a conference paper at ICLR 2022
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.15555555555555556,Figure 2: The proposed training action selection framework for reinforcement learning
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.16666666666666666,"the transition probability, an acceptable performance margin, and an acceptable number of iterations.
The output is the cut-off cardinality. The algorithm uses an iterative procedure to train the RL agent
with different numbers of training actions, starting from all available training actions to only one
training action. If for a certain cardinality, all the cumulative rewards do not improve by ϵ within n
iterations, then we set the status to failure, and assign the cut-off cardinality to that cardinality plus
one. The cut-off cardinality signiﬁes computational efﬁciency — a larger cut-off cadinality means
lower computational overhead and vice versa."
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.17777777777777778,"Algorithm 1 Monte Carlo method for cut-off cardinality computation for the training action space
Input: The RL algorithm, R, the power set for the training action space, P(A), the corresponding
state space, P(S|A), transition probability P , an acceptable performance margin, ϵ, and an
acceptable number of iterations, n.
Output: Cut-off cardinality, |A|cutoff"
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.18888888888888888,for k ←|A| to 1 do
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.2,repeat
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.2111111111111111,"π∗←R(S, A, Pa, Ra)
if the cumulative reward improves by ϵ with n iterations then"
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.2222222222222222,"status ←success
else"
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.23333333333333334,"status ←failure
end if
until all necessary training action sets for a given cardinality is exhausted.
if all statuses for a given cardinality, k, are failures then"
CUT-OFF CARDINALITY COMPUTATION FOR TRAINING ACTION POINTS,0.24444444444444444,"|A|cutoff ←k + 1
end if
end for"
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.25555555555555554,"2.2
CATEGORIZATION AND RANKING OF TRAINING ACTIONS"
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.26666666666666666,"Algorithm 2 offers a principled procedure for training action selection. As for inputs, it takes the
combinations of training state-action pairs above the cut-off cardinality, determined by Algorithm
1, the RL algorithm, the reward function, the threshold condition, and the acceptable number of
iterations. As for outputs, it predicts a categorization vector which classiﬁes training action points
into two classes: {dispensable, indispensable} and ranks dispensable training actions based on the
corresponding cumulative reward. At its core, Algorithm 2 determines whether the policy determined
from the given state-action pairs achieves the threshold condition within the acceptable number of
iterations. The algorithm uncovers the indispensability of some training action sets for a given RL
task. A training action set is deemed to be indispensable: iff upon removal of that training action"
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.2777777777777778,Under review as a conference paper at ICLR 2022
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.28888888888888886,"point, the RL agent fails to accomplish the goal within a ﬁnite number of iterations [24, 56, 23].
All other action points are dispensable. The dispensable action points are rank listed in L based on
the corresponding cumulative reward. Essentially, L informs the performance impact of different
disposable action points on the RL agent."
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.3,"Algorithm 2 Data-driven categorization and ranking for training action space evaluation
Input: All combinations of training state-action pairs upto cut-off cardinality: {(si, ai)}, the RL
algorithm, R, the reward function, the threshold condition, and the acceptable number of iterations.
Output: Categorization vector, V, which classiﬁes training action points into two classes: {dispens-
able, indispensable}. Rank order list of dispensable training actions based on the corresponding
cumulative reward.
repeat"
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.3111111111111111,"for < si, ai > do"
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.32222222222222224,"π∗←R(S, A, Pa, Ra)
if Threshold condition is never satisﬁed within the acceptable number of iterations then"
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.3333333333333333,"V(A −ai) ←indispensable
else"
CATEGORIZATION AND RANKING OF TRAINING ACTIONS,0.34444444444444444,"V(A −ai) ←dispensable
end if
end for
until all necessary training action sets are evaluated.
Rank List, L ←dispensable actions sorted in order of the corresponding cumulative rewards"
ACTION SHAPLEY,0.35555555555555557,"2.3
ACTION SHAPLEY"
ACTION SHAPLEY,0.36666666666666664,"The proposed framework adopts Action Shapley which introduces a formulation for the problem
of equitable training action valuation in reinforcement learning. Reinforcement learning has three
key building blocks. The ﬁrst building block is the training action space, A, the corresponding state
space S, and a pre-assigned reward function, Ra. The second building block is a reinforcement
learning algorithm, R, which is treated as a black box. It takes (S, A, Pa, Ra) and computes an
optimal policy function, π∗. The third building block is the performance measure or the cumulative
sum of reward, P Ra. The goal of the framework is to compute the valuation for each training action
point, φi(A, R, P Ra) as a function of three building blocks. A shapley technique is useful for
action valuation because it satisﬁes nullity, symmetry, and linearity [45]. From game theory, φ can be
expressed as shown in Equation 1:"
ACTION SHAPLEY,0.37777777777777777,"φi = C
X"
ACTION SHAPLEY,0.3888888888888889,S⊆A−{i}
ACTION SHAPLEY,0.4,"P Ra(S ∪{i}) −P Ra(S)
 n−1
|S|

(1)"
ACTION SHAPLEY,0.4111111111111111,"where, the sum is over all subsets of A not containing i and C is an arbitrary constant. We call
φi the Action Shapley value of action ai. Equation 1 suggests Action Shapley computation is
with exponential time complexity w.r.t. the training action points. This exponential computational
complexity warrants the need for the cut-off cardinality discussed in Algorithm 1."
CASE STUDY,0.4222222222222222,"3
CASE STUDY"
CASE STUDY,0.43333333333333335,"In this section, we design a resource tuning example (in the cloud) to illustrate the effectiveness of
the proposed algorithm for the training action space evaluation. We evaluate the performance of an
MDP [20]-based RL agent, as shown in Figure 1. The state-action mapping and transition probability
are modeled using time-series auto-regression [38] after a PCA-based dimensionality reduction [41]
with time complexity O(klogk), where k is the number of principal components. The choice of the
algorithms is purely driven by the nature of the training dataset. A more complex non-linear dataset
warrants more complex sequential models such as long short-term memory (LSTM) [44] or attention
based models [30]. For action updates, WLOG, an RL-based PID controller [1, 13] is used. Similar
to time-series modeling, action update can be conducted by other policy learning algorithms such as
SARSA [19]."
CASE STUDY,0.4444444444444444,Under review as a conference paper at ICLR 2022
CASE STUDY,0.45555555555555555,Table 1: Relevant case study for cloud resource tuning
CASE STUDY,0.4666666666666667,"RL Artifact
Description"
CASE STUDY,0.4777777777777778,"State
CPU utilization metrics
State Statistics
Median value of CPU utilization
Threshold
90% of CPU utilization
Action
Resource conﬁguration set points: (# of vCPUs, Memory Size (GB))
Reward
Negative of total time steps required to satisfy the threshold condition
Parametric Boundary
Polygon deﬁned by the parametric endpoints
Initial Action
(6, 14): Arbitrarily assigned WLOG
Error Margin
5%
Acceptable Steps
400
Computational Complexity
Polynomial time"
CASE STUDY,0.4888888888888889,Table 2: Five different AWS EC2 resource pairs used in training action space
CASE STUDY,0.5,"EC2 Type
of vCPUs
Memory Size (GB)"
CASE STUDY,0.5111111111111111,"small t3a
2
2
medium t3a
2
4
large t3a
2
8
xlarge t3a
4
16
2xlarge t3a
8
32"
CASE STUDY,0.5222222222222223,"As shown in Table 1, the objective of the RL agent for this case study is to quickly reduce high CPU
utilization below a pre-assigned threshold for a given workload. In most infrastructure/cloud resource
tuning technologies [55], CPU utilization represents a key metric. Therefore, the state space for this
RL case study consists of virtual machine CPU utilization (%) metrics and the action space is deﬁned
by the VM resource set points, i.e., (# of vCPUs, Memory Size (GB)). The RL agent uses AWS boto3
SDK [2] to manipulate actions and AWS CloudWatch [3] for state space monitoring. The reward is
deﬁned as the number of time steps required by the agent to accomplish the objective multiplied by
−1. The negative reward per time step was meant to push the agent to accomplish the task as fast as
possible."
CASE STUDY,0.5333333333333333,"The training data for this case study was generated internally [4] with an open source library, stress
(https://linux.die.net/man/1/stress). It uses a rectangular workload. The peak of the workload uses
the stress command: sudo stress –io 4 –vm 2 –vm-bytes 1024M –timeout 500s. Essentially, the peak
is running 4 I/O stressors and 2 VM workers spinning on malloc with 1024 MB per worker for 500
s. The simulated rectangular workload has a time period of 600 s: a high-stress phase of 500 s is
followed by an inactive phase of 100 s. The RL training action space is spanned by the power set
drawn from the ﬁve pairs of EC2 conﬁguration set points as shown in Table 2. Using Algorithm 1,
we notice that the power set below the cut-off cardinality of 4 produces trivial and unstable results.
Therefore, WLOG, the analysis in this paper has been focused on six training action sets as shown in
Table 4. That amounts to 81.25% reduction in the search space. For each EC2 instance in the training
action space, the corresponding state metrics, i.e., CPU utilization (%) are shown in Figure 3. The
training data was collected for a 24 hour period with 1 minute sampling interval. Using Algorithm
??, the optimal step-size for action update is identiﬁed to be 0.1 in both # of vCPUs and Memory
Size (GB) dimensions."
CASE STUDY,0.5444444444444444,"With this set up, different RL models are developed with different training action sets and the
corresponding rewards and ranks are noted in Table 4. Remarkably, a high cardinality training action
set does not guarantee the best reward: the agent with all training actions,<small t3a, medium t3a,
large t3a, xlarge t3a, 2xlarge t3a>, does not yield the highest reward. In fact, the action set with
<medium t3a, large t3a, xlarge t3a, 2xlarge t3a> yields the highest reward. This pattern could be
attributed to the state-action interaction in this particular case study. First, the parametric distances"
CASE STUDY,0.5555555555555556,Under review as a conference paper at ICLR 2022
CASE STUDY,0.5666666666666667,Table 3: Training action categorization based on the valuation vector
CASE STUDY,0.5777777777777777,"Training Action
Category"
CASE STUDY,0.5888888888888889,"small t3a
dispensable
medium t3a
dispensable
large t3a
indispensable
xlarge t3a
indispensable
2xlarge t3a
indispensable"
CASE STUDY,0.6,"Table 4: Rewards and ranks for different training actions. Some training action sets fail to satisfy the
objective. Therefore, the rewards and ranks for them are noted as none
."
CASE STUDY,0.6111111111111112,"Training Action Set
Reward
Rank"
CASE STUDY,0.6222222222222222,"<small t3a, medium t3a, large t3a, xlarge t3a, 2xlarge t3a>
−21
3
<medium t3a, large t3a, xlarge t3a, 2xlarge t3a>
−13
1
<small t3a, large t3a, xlarge t3a, 2xlarge t3a>
−16
2
<small t3a, medium t3a, xlarge t3a, 2xlarge t3a>
none
none
<small t3a, medium t3a, large t3a, 2xlarge t3a>
none
none
<small t3a, medium t3a, large t3a, xlarge t3a>
none
none"
CASE STUDY,0.6333333333333333,"between different EC2 instance pairs are not uniform. While the Euclidean distance between small
t3a and medium t3a is equal to 2, that between xlarge t3a and 2xlarge t3a is 16.5. The non-uniform
spacing for training action space is a considerable deterrent [31] for RL adoption. Second, in this
case study, the transient CPU utilization (%) patterns have undergone a material change from xlarge
t3a (max 100%) to 2xlarge t3a (max 73%). This indicates the strong inﬂuence of 2xlarge t3a for
the given RL task. Indeed, we noticed 2xlarge t3a to be an indispensable training action. As shown
in Table 3, a categorization of training actions can be inferred based on the valuation vector, V, as
described in Algorithm 2: two dispensable training actions are uncovered to be small t3a, medium
t3a and three indispensable training actions to be large t3a, xlarge t3a, 2xlarge t3a."
CASE STUDY,0.6444444444444445,"Figure 4 shows the RL loop action with three training actions: <small t3a, medium t3a, large t3a,
xlarge t3a, 2xlarge t3a>, <medium t3a, large t3a, xlarge t3a, 2xlarge t3a>, and <small t3a, large
t3a, xlarge t3a, 2xlarge t3a>."
CASE STUDY,0.6555555555555556,"• For the ﬁrst training action set of <small t3a, medium t3a, large t3a, xlarge t3a, 2xlarge
t3a>, the reward is −21. The left subplot in Figure 4(a) shows how the recommended action
is evolving with time from an arbitrary initial point of (6, 14). The recommended points
are superimposed on the training action parameter space to illustrate their relative position
with respect to the parametric boundary which is deﬁned by the trapezium with vertices:
{(2,2), (2,8), (8,32), (8,2)} in the (# of vCPUs, Memory Size (GB)) space. The right subplot
in Figure 4(a) shows how the median CPU utilization (%) comes below the 90% threshold
in 21 steps leading to −21 in reward. The blue dots represent the median CPU utilizations
for different training set points."
CASE STUDY,0.6666666666666666,"• For the second training action set of <medium t3a, large t3a, xlarge t3a, 2xlarge t3a>, the
reward is −13 as shown in Figure 4(b)."
CASE STUDY,0.6777777777777778,"• For the third training action set of <small t3a, large t3a, xlarge t3a, 2xlarge t3a>, the reward
is −16 as shown in Figure 4(c)."
CASE STUDY,0.6888888888888889,"As shown in Table 4, the reward scores can indeed be used for ranking different training action sets
leading to a data-driven approach for training action selection. As shown in Figure 5, large t3a is
an indispensable element in the training action space for the given RL agent. Without this training
action, the RL agent fails to accomplish the goal of bringing the CPU utilization below the critical
threshold. Similar observations can be made about xlarge t3a and 2xlarge t3a."
CASE STUDY,0.7,Under review as a conference paper at ICLR 2022
CASE STUDY,0.7111111111111111,"Figure 3: CPU utilization (%) responses on ﬁve different EC2 instances (Table 2) from 2PM-UTC
9/1/2021-2PM-UTC 9/2/2021 at 1 minute granularity. This training data was generated internally [4].
The median CPU utilization values (%) are noted to be equal to {95%, 95.5%, 99.5%, 100%, 72.5%}
."
RELATED WORK,0.7222222222222222,"4
RELATED WORK"
RELATED WORK,0.7333333333333333,"Shapley value was postulated in a classic paper in game theory [15] and inﬂuences the ﬁeld of
economics signiﬁcantly [43]. Various shapley-inspired techniques have been applied to model diverse
problems including voting [12], resource allocation [48] and bargaining [27]. Recent years saw
increased application of shapley-inspired methodologies in machine learning feature importance
evaluation and training data valuation [14, 10]. To the best of our knowledge, Shapley value has not
been used to quantify training action valuation in a reinforcement learning context. The application of
shapley-inspired technique for reinforcement learning is a challenging problem. First, the relationship
between state-action pairs and transition probabilities are expensive to model. Also, often the RL
training action points are not uniformly positioned. Finally, we have purposefully focused on a small
sample cloud infrastructure use case instead of cliched RL use cases [5]. That is because the problem
of optimal action selection has relatively more expensive in the cloud infrastructure space where the
state-action relationship is only partially observable."
CONCLUSION,0.7444444444444445,"5
CONCLUSION"
CONCLUSION,0.7555555555555555,"This paper proposes a data-driven methodology for training action space evaluation for RL. The
methodology offers a principled framework for training action space categorization and ranking
within a ﬁnite computational time. It unleashes a strategy for superior model performance and lower
modeling cost. Additionally, the proposed methodology is completely agnostic of use cases and
machine learning algorithms. Therefore, it is a general-purpose methodology extensible to different
domains including distributed computing, network trafﬁc control, healthcare, automatic locomotion,
building management system, and industrial controls, and different machine learning algorithms such
as PCA, Autoencoder, ARIMA, LSTM, Transformer, PID, SARSA, DQN, and many others. For the
next phase of the development for this data-agnostic methodology, we are planning to contribute a
general RL design library to relevant open source projects [6, 5, 7]."
CONCLUSION,0.7666666666666667,Under review as a conference paper at ICLR 2022 (a) (b) (c)
CONCLUSION,0.7777777777777778,"Figure 4: Examples of dispensable actions. (a) RL loop with all training actions, <small t3a, medium
t3a, large t3a, xlarge t3a, 2xlarge t3a>. The reward is noted to be −21. (b) RL loop with a training
action set of <medium t3a, large t3a, xlarge t3a, 2xlarge t3a>. The reward is noted to be −13. (c)
RL loop with a training action set of <small t3a, large t3a, xlarge t3a, 2xlarge t3a>. The reward is
noted to be −16. Both small t3a and medium t3a are noted to be dispensable actions."
CONCLUSION,0.7888888888888889,"Figure 5: RL loop with <small t3a, medium t3a, xlarge t3a, 2xlarge t3a> action space. The agent
could never accomplish the goal, therefore, large t3a is an indispensable element in the training
action space. Similarly, xlarge t3a and 2xlarge t3a are two indispensable elements in the training
action space."
CONCLUSION,0.8,Under review as a conference paper at ICLR 2022
REFERENCES,0.8111111111111111,REFERENCES
REFERENCES,0.8222222222222222,"[1]
https://www.ni.com/en- us/innovations/white- papers/06/pid-
theory-explained.html. [Online; accessed 30-September-2021].
[2]
https://instances.vantage.sh/. [Online; accessed 20-September-2021].
[3]
https://aws.amazon.com/cloudwatch/. [Online; accessed 30-September-2021].
[4]
https : / / github . com / rghosh8 / ec2 - rl - dataset. [Online; accessed 30-
September-2021].
[5]
https://gym.openai.com/. [Online; accessed 30-September-2021].
[6]
https://github.com/ray- project/ray/tree/master/python/ray.
[Online; accessed 30-September-2021].
[7]
https://github.com/kubeflow/katib. [Online; accessed 30-September-2021].
[8]
James F Cavanagh et al. “Conﬂict acts as an implicit cost in reinforcement learning”. In:
Nature communications 5.1 (2014), pp. 1–10.
[9]
Kate Crawford and Trevor Paglen. Excavating ai. 2019.
[10]
Daniel Fryer, Inga Strümke, and Hien Nguyen. “Shapley values for feature selection: the good,
the bad, and the axioms”. In: arXiv preprint arXiv:2102.10936 (2021).
[11]
Paul Garnier et al. “A review on deep reinforcement learning for ﬂuid mechanics”. In: Com-
puters & Fluids 225 (2021), p. 104973.
[12]
Andrew Gelman, Jonathan N Katz, and Joseph Bafumi. “Standard voting power indexes do not
work: an empirical analysis”. In: British Journal of Political Science 34.4 (2004), pp. 657–674.
[13]
Meysam Gheisarnejad and Mohammad Hassan Khooban. “An intelligent non-integer PID
controller-based deep reinforcement learning: Implementation and experimental results”. In:
IEEE Transactions on Industrial Electronics 68.4 (2020), pp. 3609–3618.
[14]
Amirata Ghorbani and James Zou. “Data shapley: Equitable valuation of data for machine
learning”. In: International Conference on Machine Learning. PMLR. 2019, pp. 2242–2251.
[15]
Donald Bruce Gillies. Some theorems on n-person games. Princeton University, 1953.
[16]
Naman Goel and Boi Faltings. “Deep bayesian trust: A dominant and fair incentive mechanism
for crowd”. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Vol. 33. 01.
2019, pp. 1996–2003.
[17]
Jiequn Han, Arnulf Jentzen, and E Weinan. “Overcoming the curse of dimensionality: Solv-
ing high-dimensional partial differential equations using deep learning”. In: arXiv preprint
arXiv:1707.02568 (2017), pp. 1–13.
[18]
Yousuf Hashmy et al. “Wide-area measurement system-based low frequency oscillation damp-
ing control through reinforcement learning”. In: IEEE Transactions on Smart Grid 11.6 (2020),
pp. 5072–5083.
[19]
J Fernando Hernandez-Garcia and Richard S Sutton. “Understanding multi-step deep reinforce-
ment learning: a systematic study of the DQN target”. In: arXiv preprint arXiv:1901.07510
(2019).
[20]
Mark N Howell and Matt C Best. “On-line PID tuning for engine idle-speed control using
continuous action reinforcement learning automata”. In: Control Engineering Practice 8.2
(2000), pp. 147–154.
[21]
Vladimir Ilievski et al. “Goal-oriented chatbot dialog management bootstrapping with transfer
learning”. In: arXiv preprint arXiv:1802.00500 (2018).
[22]
Ruoxi Jia et al. “Towards efﬁcient data valuation based on the shapley value”. In: The 22nd
International Conference on Artiﬁcial Intelligence and Statistics. PMLR. 2019, pp. 1167–1176.
[23]
Nan Jiang and Alekh Agarwal. “Open problem: The dependence of sample complexity lower
bounds on planning horizon”. In: Conference On Learning Theory. PMLR. 2018, pp. 3395–
3398.
[24]
Nan Jiang et al. “Contextual decision processes with low Bellman rank are PAC-learnable”. In:
International Conference on Machine Learning. PMLR. 2017, pp. 1704–1713.
[25]
Yujia Jin and Aaron Sidford. “Towards Tight Bounds on the Sample Complexity of Average-
reward MDPs”. In: arXiv preprint arXiv:2106.07046 (2021)."
REFERENCES,0.8333333333333334,Under review as a conference paper at ICLR 2022
REFERENCES,0.8444444444444444,"[26]
Nate Kohl and Peter Stone. “Policy gradient reinforcement learning for fast quadrupedal loco-
motion”. In: IEEE International Conference on Robotics and Automation, 2004. Proceedings.
ICRA’04. 2004. Vol. 3. IEEE. 2004, pp. 2619–2624.
[27]
David A Kravitz and John Iwaniszek. “Number of coalitions and resources as sources of power
in coalition bargaining.” In: Journal of Personality and Social Psychology 47.3 (1984), p. 534.
[28]
Matthew Lai. “Giraffe: Using deep reinforcement learning to play chess”. In: arXiv preprint
arXiv:1509.01549 (2015).
[29]
Jacky Liang et al. “GPU-accelerated robotic simulation for distributed reinforcement learning”.
In: Conference on Robot Learning. PMLR. 2018, pp. 270–282.
[30]
Yuxuan Liang et al. “Geoman: Multi-level attention networks for geo-sensory time series
prediction.” In: IJCAI. Vol. 2018. 2018, pp. 3428–3434.
[31]
Sandeep Manjanna, Herke van Hoof, and Gregory Dudek. “Reinforcement learning with non-
uniform state representations for adaptive search”. In: 2018 IEEE International Symposium on
Safety, Security, and Rescue Robotics (SSRR). IEEE. 2018, pp. 1–7.
[32]
Riccardo Marin et al. “Correspondence learning via linearly-invariant embedding”. In: (2020).
[33]
Volodymyr Mnih et al. “Human-level control through deep reinforcement learning”. In: nature
518.7540 (2015), pp. 529–533.
[34]
Volodymyr Mnih et al. “Playing atari with deep reinforcement learning”. In: arXiv preprint
arXiv:1312.5602 (2013).
[35]
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. “Agnostic federated learning”. In:
International Conference on Machine Learning. PMLR. 2019, pp. 4615–4625.
[36]
Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl. “Interpretable machine learning–a
brief history, state-of-the-art and challenges”. In: Joint European Conference on Machine
Learning and Knowledge Discovery in Databases. Springer. 2020, pp. 417–431.
[37]
Bła˙zej Osi´nski et al. “Simulation-based reinforcement learning for real-world autonomous
driving”. In: 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE.
2020, pp. 6411–6418.
[38]
Charles W Ostrom. Time series analysis: Regression techniques. 9. Sage, 1990.
[39]
Joelle Pineau et al. “Improving Reproducibility in Machine Learning Research”. In: Journal of
Machine Learning Research 22 (2021), pp. 1–20.
[40]
Neoklis Polyzotis et al. “Data lifecycle challenges in production machine learning: a survey”.
In: ACM SIGMOD Record 47.2 (2018), pp. 17–28.
[41]
Vikas Raunak, Vivek Gupta, and Florian Metze. “Effective dimensionality reduction for word
embeddings”. In: Proceedings of the 4th Workshop on Representation Learning for NLP
(RepL4NLP-2019). 2019, pp. 235–243.
[42]
Roberta Rocca and Tal Yarkoni. “Putting psychology to the test: Rethinking model evaluation
through benchmarking and prediction”. In: (2020).
[43]
Alvin E Roth. The Shapley value: essays in honor of Lloyd S. Shapley. Cambridge University
Press, 1988.
[44]
Alaa Sagheer and Mostafa Kotb. “Time series forecasting of petroleum production using deep
LSTM recurrent networks”. In: Neurocomputing 323 (2019), pp. 203–213.
[45]
Anthony F Shorrocks. Decomposition procedures for distributional analysis: a uniﬁed frame-
work based on the Shapley value. Tech. rep. mimeo, University of Essex, 1999.
[46]
David Silver et al. “Mastering the game of Go with deep neural networks and tree search”. In:
nature 529.7587 (2016), pp. 484–489.
[47]
Satinder Singh, Andy Okun, and Andrew Jackson. “Learning to play Go from scratch”. In:
Nature 550.7676 (2017), pp. 336–337.
[48]
Peibing Song et al. “Comprehensive Beneﬁt Allocation for Joint Operation of Multiple Reser-
voirs in the Ningbo Based on Shapely-Value Method and Beneﬁt-Incremental Method”. In:
MATEC Web of Conferences. Vol. 246. EDP Sciences. 2018, p. 01080.
[49]
Mukund Sundararajan and Amir Najmi. “The many Shapley values for model explanation”. In:
International Conference on Machine Learning. PMLR. 2020, pp. 9269–9278.
[50]
Richard S Sutton. “Dyna, an integrated architecture for learning, planning, and reacting”. In:
ACM Sigart Bulletin 2.4 (1991), pp. 160–163."
REFERENCES,0.8555555555555555,Under review as a conference paper at ICLR 2022
REFERENCES,0.8666666666666667,"[51]
Sebastian Thrun and Anton Schwartz. “Issues in using function approximation for reinforce-
ment learning”. In: Proceedings of the Fourth Connectionist Models Summer School. Hillsdale,
NJ. 1993, pp. 255–263.
[52]
Hado Van Hasselt. “Reinforcement learning in continuous state and action spaces”. In: Rein-
forcement learning. Springer, 2012, pp. 207–251.
[53]
Hilde JP Weerts, Werner van Ipenburg, and Mykola Pechenizkiy. “A human-grounded evalua-
tion of shap for alert processing”. In: arXiv preprint arXiv:1907.03324 (2019).
[54]
Eyal Winter. “The shapley value”. In: Handbook of game theory with economic applications 3
(2002), pp. 2025–2054.
[55]
Yu Zhang, Jianguo Yao, and Haibing Guan. “Intelligent cloud resource management with deep
reinforcement learning”. In: IEEE Cloud Computing 4.6 (2017), pp. 60–69.
[56]
Dongruo Zhou, Jiafan He, and Quanquan Gu. “Provably efﬁcient reinforcement learning for
discounted mdps with feature mapping”. In: International Conference on Machine Learning.
PMLR. 2021, pp. 12793–12802.
[57]
Hao Zhu et al. “Deep reinforcement learning for mobile edge caching: Review, new features,
and open issues”. In: IEEE Network 32.6 (2018), pp. 50–57.
[58]
Henry Zhu et al. “Dexterous manipulation with deep reinforcement learning: Efﬁcient, general,
and low-cost”. In: 2019 International Conference on Robotics and Automation (ICRA). IEEE.
2019, pp. 3651–3657."
NOTATION,0.8777777777777778,"6
NOTATION"
NOTATION,0.8888888888888888,"S
Space Space"
NOTATION,0.9,"A
Action Space"
NOTATION,0.9111111111111111,"Pa
Transition Probability"
NOTATION,0.9222222222222223,"Ra
Reward Signal"
NOTATION,0.9333333333333333,"P
Power Set"
NOTATION,0.9444444444444444,"π
Policy"
NOTATION,0.9555555555555556,"π∗
Optimal Policy"
NOTATION,0.9666666666666667,"π∗
Optimal Policy"
NOTATION,0.9777777777777777,"V
Categorization Vector"
NOTATION,0.9888888888888889,"L
Rank List of Dispensable Actions"
