Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002183406113537118,"In clustering algorithms, the choice of initial centers is crucial for the quality of
the learned clusters. We propose a new initialization scheme for the k-median
problem in the general metric space (e.g., discrete space induced by graphs), based
on the construction of metric embedding tree structure of the data. From the tree,
we propose a novel and efﬁcient search algorithm for good initial centers that
can be used subsequently for the local search algorithm. Our method, named the
HST initialization, can also be easily extended to the setting of differential privacy
(DP) to generate private initial centers. Theoretically, the initial centers from HST
initialization can achieve lower error than those from another popular initialization
method, k-median++, in the non-DP setting. Moreover, with privacy constraint,
we show that the error of applying DP local search followed by our private HST
initialization improves previous results, and approaches the known lower bound
within a small factor. Empirically, experiments are conducted to demonstrate the
effectiveness of our methods."
INTRODUCTION,0.004366812227074236,"1
INTRODUCTION"
INTRODUCTION,0.006550218340611353,"Clustering is an important problem in unsupervised learning that has been widely used in social
network analysis, sensor networks, etc. (Punj & Stewart, 1983; Dhillon & Modha, 2001; Banerjee
et al., 2005; Abbasi & Younis, 2007). The goal of clustering is to partition a set of data points into
clusters such that items in the same cluster are expected to be similar, while items in different clusters
should be different. This is concretely measured by the sum of distances (or squared distances)
between each point to its nearest cluster center. One conventional notion to evaluate a clustering
algorithms is: with high probability,
cost(C, D) ≤γOPTk(D) + ξ,
where C is the centers output by the algorithm and cost(C, D) is some cost function deﬁned for
C on the dataset D. OPTk(D) is the cost of optimal (oracle) k-median solution on D. When
everything is clear from context, we will use OPT for short. Here, γ is called multiplicative error
and ξ is called additive error. Alternatively, we may also use the notion of expected cost."
INTRODUCTION,0.008733624454148471,"Two popularly studied clustering problems are 1) the k-median problem, and 2) the k-means prob-
lem. The origin of k-median dates back to the 1970’s (e.g., Kaufman et al. (1977)), where one
tries to ﬁnd the best location of facilities that minimizes the cost measured by the distance between
clients and facilities. Formally, given a set of points D and a distance measure, the goal is to ﬁnd k
center points minimizing the sum of absolute distances of each sample point to its nearest center. In
k-means, the objective is to minimize the sum of squared distances instead. There are two popular
clustering algorithms. One heuristic is the Lloyd’s algorithm (Lloyd, 1982), which is built upon an
iterative distortion minimization approach. In most cases, this method can only be applied to nu-
merical data, typically in the Euclidean space. Clustering in general metric spaces (discrete spaces)
is also important and useful when dealing with, for example, the graph data, where Lloyd’s method
is no longer applicable. A more broadly applicable approach, the local search method (Kanungo
et al., 2002; Arya et al., 2004), has also been widely studied. It iteratively ﬁnds the optimal swap be-
tween the center set and non-center data points to keep lowering the cost. Local search can achieve
a constant approximation ratio (γ = O(1)) to the optimal solution for k-median (Arya et al., 2004)."
INTRODUCTION,0.010917030567685589,"Initialization of cluster centers. It is well-known that the performance of clustering can be highly
sensitive to initialization. If clustering starts with good initial centers (i.e., with small approxi-
mation error), the algorithm may use fewer iterations to ﬁnd a better solution. The k-median++"
INTRODUCTION,0.013100436681222707,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.015283842794759825,"algorithm (Arthur & Vassilvitskii, 2007) iteratively selects k data points as initial centers, favoring
distant points in a probabilistic way (see Appendix A for more details). Intuitively, the initial cen-
ters tend to be well spread over the data points (i.e., over different clusters). The produced initial
center is proved to have O(log k) multiplicative error. Followup works of k-means++ further im-
proved its efﬁciency and scalability, e.g., Bahmani et al. (2012); Bachem et al. (2016); Lattanzi &
Sohler (2019). In this work, we propose a new initialization framework based on metric embedding
methods, with comparable approximation error and running time as k-median++. Importantly, our
initialization scheme can be conveniently combined with the notion of differential privacy."
INTRODUCTION,0.017467248908296942,"Clustering with Differential Privacy. The concept of differential privacy (Dwork, 2006; McSherry
& Talwar, 2007) has been popular to rigorously deﬁne and resolve the problem of keeping useful
information for model learning, while protecting privacy for each individual. Private k-means prob-
lem has been widely studied, e.g., Feldman et al. (2009); Nock et al. (2016); Feldman et al. (2017),
mostly in the Euclidean space. The paper (Balcan et al., 2017) considered identifying a good candi-
date set (in a private manner) of centers before applying private local search, which yields O(log3 n)
multiplicative error and O((k2 + d) log5 n) additive error. Later on, the Euclidean k-means errors
are further improved to γ = O(1) and ξ = O(k1.01·d0.51+k1.5) by Stemmer & Kaplan (2018), with
more advanced candidate set selection. Huang & Liu (2018) gave an optimal algorithm in terms of
minimizing Wasserstein distance under some data separability condition."
INTRODUCTION,0.019650655021834062,"For private k-median clustering, Feldman et al. (2009) considered the problem in high dimensional
Euclidean space. However, it is rather difﬁcult to extend their analysis to more general metrics
in discrete spaces (e.g., on graphs). The strategy of Balcan et al. (2017) to form a candidate cen-
ter set could as well be adopted to k-median, which leads to O(log3/2 n) multiplicative error and
O((k2 +d) log3 n) additive error in high dimensional Euclidean space. Gupta et al. (2010) proposed
a private method for the classical local search heuristic in discrete space, which applies to both k-
medians and k-means. In order to cast privacy on each swapping step, the authors applied the
exponential mechanism of McSherry & Talwar (2007). Their method produced an ϵ-differentially
private solution with cost 6OPT + O(△k2 log2 n/ϵ), where △is the diameter of point set."
INTRODUCTION,0.021834061135371178,The main contributions of this work include :
INTRODUCTION,0.024017467248908297,"• We introduce Hierarchically Well-Separated Tree (HST), a metric embedding tree structure,
to the k-median clustering problem. Once the HST is constructed using the data samples,
we provide an efﬁcient sampling strategy to select the initial center set, with an approxima-
tion factor O(log min{k, △}) in the non-private setting, which is O(log min{k, d}) when
△= O(d) (e.g., bounded data). This improves the O(log k) error of k-means/median++
in e.g., the lower dimensional Euclidean space.
• The main strength of our HST initialization method is that it could be effectively adapted
to differential privacy (DP). The so-called DP-HST algorithm is ϵ-DP and outputs initial
centers with O(log n) multiplicative error and O(ϵ−1△k2 log2 n) additive error. Moreover,
running DP-local search starting from this initialization gives O(1) multiplicative error and
O(ϵ−1△k2(log log n) log n) additive error, which improves previous results towards the
well-known lower bound O(ϵ−1△k log(n/k)) on the additive error (Gupta et al., 2010)
within a small O(k log log n) factor. To our knowledge, this is the ﬁrst center initialization
method with differential privacy guarantee and improved error rate in general metrics.
• We conduct experiments on simulated and real-world datasets to demonstrate the effective-
ness of our methods. In both non-private and private settings, our proposed HST-based
approach achieves smaller cost at initialization than k-median++, which may also lead to
improvements in the ﬁnal clustering quality. Our private algorithm can also save computa-
tional costs by reaching a good solution with fewer iterations."
PRELIMINARIES,0.026200873362445413,"2
PRELIMINARIES"
PRELIMINARIES,0.028384279475982533,"Deﬁnition 2.1 (Differential Privacy (DP) (Dwork, 2006)). If for any two adjacent data sets D and
D′ with symmetric difference of size one, for any O ⊂Range(A), an algorithm A satisﬁes"
PRELIMINARIES,0.03056768558951965,"Pr[A(D) ∈O] ≤eϵPr[A(D′) ∈O],"
PRELIMINARIES,0.03275109170305677,then algorithm A is said to be ϵ-differentially private.
PRELIMINARIES,0.034934497816593885,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.03711790393013101,"Intuitively, differential privacy requires that after removing any observation, the output of D′ should
not be too different from that of the original dataset D. Smaller ϵ indicates stronger privacy, which,
however, usually sacriﬁces utility. Thus, one of the central topics in differential privacy literature is
to balance the utility-privacy trade-off."
PRELIMINARIES,0.039301310043668124,"To achieve DP, it is common to introduce noise to the data or the algorithm output. The Laplace
mechanism adds Laplace(η(f)/ϵ) noise to the output, which is known to achieve ϵ-DP. The expo-
nential mechanism is also a tool for many DP algorithms. Let O be the set of feasible outputs. The
utility function q : D × O →R is what we aim to maximize. The exponential mechanism outputs
an element o ∈O with probability P[A(D) = o] ∝exp( ϵq(D,o)"
PRELIMINARIES,0.04148471615720524,"2η(q) ), where D is the input dataset and
η(f) = sup|D−D′|=1 |f(D) −f(D′)| is the sensitivity of f. Both mechanisms will be used in the
design of our proposed DP approach."
PRELIMINARIES,0.043668122270742356,"k-Median Clustering.
Following the framework of Arya et al. (2004); Gupta et al. (2010), the
problem of k-median clustering (DP and non-DP) studied in our paper is stated as below."
PRELIMINARIES,0.04585152838427948,"Deﬁnition 2.2 (k-median). Given a universe point set U and a metric ρ : U × U →R, let D ⊆U
be a set of demand points. The goal of DP k-median is to pick F ⊆U with |F| = k to minimize"
PRELIMINARIES,0.048034934497816595,"Private k-median:
cost(F, D) =
X"
PRELIMINARIES,0.05021834061135371,"v∈D
min
f∈F ρ(v, f).
(1)"
PRELIMINARIES,0.05240174672489083,"At the same time, the output F is required to be ϵ-differentially private to D. For standard non-
private k-median, we assume U = D and minimize"
PRELIMINARIES,0.05458515283842795,"k-median:
cost(F, U) =
X"
PRELIMINARIES,0.056768558951965066,"v∈U
min
f∈F ρ(v, f).
(2)"
PRELIMINARIES,0.05895196506550218,We may drop “F” and use “cost(D)” or “cost(U)” if there is no risk of ambiguity.
PRELIMINARIES,0.0611353711790393,"To better understand the DP clustering problem, we provide an real-world example as follows."
PRELIMINARIES,0.06331877729257641,"Example 2.1. Consider U to be the universe of all users in a social network (e.g., Twitter). Each
user (account) is public, but also has some private information that can only be seen by the data
holder. Let D be users grouped by some information that might be set as private. Suppose a third
party plans to collaborate with the most inﬂuential users in D for e.g., commercial purposes, thus
requesting the cluster centers of D. In this case, we need a strategy to safely release the centers,
while protecting the individuals in D from being identiﬁed (since the membership of D is private)."
PRELIMINARIES,0.06550218340611354,"The local search procedure for k-median proposed by Arya et al. (2004) is summarized in Algo-
rithm 1. First we randomly pick k points in U as the initial centers. In each iteration, we search over
all x ∈F and y ∈U, and do the swap F ←F −{x} + {y} such that F −{x} + {y} improves the
cost of F the most (if more than factor (1 −α/k) where α > 0 is a hyper-parameter). We repeat the
procedure until no such swap exists. Arya et al. (2004) showed that the output centers F achieves 5
approximation error to the optimal solution, i.e., cost(F) ≤5OPT."
PRELIMINARIES,0.06768558951965066,"Algorithm 1: Local search for k-median clustering (Arya et al., 2004)"
PRELIMINARIES,0.06986899563318777,"Input: Data points U, parameter k, constant α
Initialization: Randomly select k points from U as initial center set F
while ∃x ∈F, y ∈U s.t. cost(F −{x} + {y}) ≤(1 −α/k)cost(F) do"
PRELIMINARIES,0.07205240174672489,"Select (x, y) ∈Fi × (D \ Fi) with arg minx,y{cost(F −{x} + {y})}
Swap operation: F ←F −{x} + {y}
Output: Center set F"
PRELIMINARIES,0.07423580786026202,"3
INITIALIZATION VIA HIERARCHICALLY WELL-SEPARATED TREE (HST)"
PRELIMINARIES,0.07641921397379912,"In this section, we propose our new initialization scheme for k-median clustering, and provide our
analysis in the non-private case solving (2). The idea is based on the metric embedding theory. We
will start with an introduction to the main tool used in our approach."
PRELIMINARIES,0.07860262008733625,"Under review as a conference paper at ICLR 2022 1 2 3 4 5
6 7
8 9 10"
PRELIMINARIES,0.08078602620087336,"4,2,…,10 10 10"
PRELIMINARIES,0.08296943231441048,"[1,2,3]
[6,4,5,7,8,9]"
PRELIMINARIES,0.0851528384279476,"1
[3,2]
[5,4,6]
[8,7]
9"
PRELIMINARIES,0.08733624454148471,Level 3
PRELIMINARIES,0.08951965065502183,Level 2
PRELIMINARIES,0.09170305676855896,Level 1
PRELIMINARIES,0.09388646288209607,"Figure 1: An illustrative example of a 3-level padded decomposition and corresponding 2-HST.
Left: The thickness of the ball represents the level. The color corresponds to the levels in the HST
in the right panel. “△”’s are the center nodes of partitions (balls), and “×”’s are non-center data
points. Right: The resulting 2-HST generated from the padded decomposition."
PRELIMINARIES,0.09606986899563319,"3.1
HIERARCHICALLY WELL-SEPARATED TREE (HST)"
PRELIMINARIES,0.0982532751091703,"In this paper, for an L-level tree, we will count levels in descending order down the tree. That is, the
root is level L, and its children form level L −1, and etc. We use hv to denote the level of v, and
ni be the number of nodes at level i. The Hierarchically Well-Separated Tree (HST) is based on the
padded decompositions of a general metric space in a hierarchical manner (Fakcharoenphol et al.,
2004). Let (U, ρ) be a metric space with |U| = n, and we will refer to this metric space without
speciﬁc clariﬁcation. A β–padded decomposition of U is a probabilistic distribution of partitions
of U such that the diameter of each cluster Ui ∈U is at most β, i.e., ρ(u, v) ≤β, ∀u, v ∈Ui,
i = 1, ..., k. Based on the this, we will adopt the standard 2-HST for our algorithm design."
PRELIMINARIES,0.10043668122270742,"Deﬁnition 3.1. Assume the smallest distance between u, v ∈U is 1, and △= maxu,v∈U ρ(u, v). A
2-Hierarchically Well-Separated Tree (2-HST) is an edge-weighted rooted tree T, such that an edge
between any pair of two nodes of level i −1 and level i has length at most △/2L−i."
PRELIMINARIES,0.10262008733624454,"We brieﬂy describe the construction of 2-HST, and place the detailed algorithm in Algorithm 7 in
the Appendix A. We ﬁrst ﬁnd a padded decomposition PL = {PL,1, ..., PL,nL} of U with parameter
β = △/2. The center of each partition in PL,j serves as a root node in level L. Then, we re-do a
padded decomposition for each partition PL,j, to ﬁnd sub-partitions with diameter β = △/4, and
set the corresponding centers as the nodes in level L −1, and so on. Each partition at level i is
obtained with β = △/2L−i. This process proceeds until a node has a single point, or a pre-speciﬁed
tree depth is reached. In Figure 1, we provide an example of L = 3-level 2-HST (left panel), along
with its underlying padded decompositions (right panel)."
PRELIMINARIES,0.10480349344978165,"HST induces a tree metric, which will be related to the general metric in our analysis. It is worth
mentioning that, there are polynomial time algorithms for computing an exact k-median solution in
the tree metric (Tamir (1996); Shah (2003)). However, the dynamic programming algorithms have
high complexity (O(kn2) and O(k2nh) where h is at least O(log n), respectively), making them
unsuitable to serve the purpose of fast initialization. Moreover, it is unknown how to apply them
effectively to the private case. Hence, next, we propose a novel tree search algorithm that is very
efﬁcient, produces a O(1) approximation error in the tree metric, and can be extended to DP easily."
HST INITIALIZATION ALGORITHM,0.10698689956331878,"3.2
HST INITIALIZATION ALGORITHM"
HST INITIALIZATION ALGORITHM,0.1091703056768559,"Let L = log ∆and suppose T is a level-L 2-HST in (U, ρ), where for simplicity we assume L is an
integer. For a node v at level i, we use T(v) to denote the subtree rooted at v. Let Nv = |T(v)| be the
number of data points in T(v). The sampling strategy for initial centers is presented in Algorithm 2,
which has two phases: subtree search and leaf search."
HST INITIALIZATION ALGORITHM,0.11135371179039301,"Subtree search. The ﬁrst step is to identify the subtrees that contain the k centers. To begin with, k
initial centers C1 are picked from T who have the largest score(v) = N(v) · 2hv. This is intuitive,
since to get a good clustering, we typically want the ball surrounding each center to include more
data points. Next, we do a screening over C1: if there is any ancestor-descendant pair of nodes, we
remove the ancestor from C1. If the current size of C1 is smaller than k, we repeat the process until
k centers are chosen (we do not re-select nodes in C1 and their ancestors). This way, C1 contains k"
HST INITIALIZATION ALGORITHM,0.11353711790393013,Under review as a conference paper at ICLR 2022
HST INITIALIZATION ALGORITHM,0.11572052401746726,Algorithm 2: HST initialization - NDP (Non-Differentially Private)
HST INITIALIZATION ALGORITHM,0.11790393013100436,"Input: U, △, k
Initialization: L = log △, C0 = ∅, C1 = ∅
Call Algorithm 7 to build a level-L 2-HST T based on input U
for each node v in T do"
HST INITIALIZATION ALGORITHM,0.12008733624454149,"Nv ←|U ∩T(v)|
score(v) ←Nv · 2hv
while |C1| < k do"
HST INITIALIZATION ALGORITHM,0.1222707423580786,"Add top (k −|C1|) nodes with highest score in T to C1
for each v ∈C1 do"
HST INITIALIZATION ALGORITHM,0.12445414847161572,"C1 = C1 \ {v}, if ∃v′ ∈C1 that v′ is a descendant of v
C0 = FIND-LEAF(T, C1)
Output: Initial center set C0 ⊆U"
HST INITIALIZATION ALGORITHM,0.12663755458515283,"Algorithm 3: FIND-LEAF (T, C1)"
HST INITIALIZATION ALGORITHM,0.12882096069868995,"Input: T, C1
Initialization: C0 = ∅
for each node v in C1 do"
HST INITIALIZATION ALGORITHM,0.13100436681222707,while v is not a leaf node do
HST INITIALIZATION ALGORITHM,0.1331877729257642,"v ←argw max{Nw, w ∈ch(v)}, where ch(v) denotes the children nodes of v
Add v to C0
Output: Initial center set C0 ⊆U"
HST INITIALIZATION ALGORITHM,0.13537117903930132,"root nodes of k disjoint subtrees. For any node w with T(w) ∩C1 = ∅and a node v ∈C1, if w is
not an ancestor of v, then score(w) ≤score(v)."
HST INITIALIZATION ALGORITHM,0.13755458515283842,"Leaf search. After we ﬁnd C1 the set of k subtrees, the next step is to ﬁnd the center in each subtree
using Algorithm 3 (“FIND-LEAF”). We employ a greedy search strategy, by ﬁnding the child node
with largest score level by level, until a leaf is found. This approach is intuitive since the diameter
of the partition ball exponentially decays with the level. Therefore, we are in a sense focusing more
and more on the region with higher density (i.e., with more data points)."
HST INITIALIZATION ALGORITHM,0.13973799126637554,"The time complexity is given as below, considering the Euclidean space as an example."
HST INITIALIZATION ALGORITHM,0.14192139737991266,Proposition 3.1 (Complexity). Algorithm 2 takes O(dn log n) time in the Euclidean space.
HST INITIALIZATION ALGORITHM,0.14410480349344978,"Remark 3.1. The complexity of HST initialization is in general comparable to the O(dnk) of k-
median++. Our algorithm would be faster if k, the number of centers, is larger. See Appendix B for
a numerical comparison of the running time."
APPROXIMATION ERROR OF HST INITIALIZATION,0.1462882096069869,"3.3
APPROXIMATION ERROR OF HST INITIALIZATION"
APPROXIMATION ERROR OF HST INITIALIZATION,0.14847161572052403,"Firstly, we show that the initial center set produced by Algorithm 2 is already a good approximation
to the optimal k-median solution. Let ρT (x, y) = dT (x, y) denote the “2-HST metric” between x
and y in the 2-HST T, where dT (x, y) is the tree distance between nodes x and y in T. In 2-HST,
by Deﬁnition 3.1 and △= 2L, in the analysis we assume equivalently that the edge weight of the
i-th level 2i−1. The crucial step of our analysis is to examine the approximation error in terms of
the 2-HST metric, after which the error can be easily adapted to the general metrics by Lemma 3.2."
APPROXIMATION ERROR OF HST INITIALIZATION,0.15065502183406113,"Lemma 3.2. (Bartal, 1996). In a metric space (U, ρ) with |U| = n and diameter △, it holds that
E[ρT (x, y)] = O(min{log n, log △})ρ(x, y). In Rd, E[ρT (x, y)] = O(d)ρ(x, y)."
APPROXIMATION ERROR OF HST INITIALIZATION,0.15283842794759825,"Recall C0, C1 from Algorithm 2. We deﬁne"
APPROXIMATION ERROR OF HST INITIALIZATION,0.15502183406113537,"costT
k (U) =
X"
APPROXIMATION ERROR OF HST INITIALIZATION,0.1572052401746725,"y∈U
min
x∈C0 ρT (x, y),
(3)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.15938864628820962,"costT
k
′(U, C1) =
min
|F ∩T (v)|=1,∀v∈C1 X"
APPROXIMATION ERROR OF HST INITIALIZATION,0.1615720524017467,"y∈U
min
x∈F ρT (x, y),
(4)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.16375545851528384,Under review as a conference paper at ICLR 2022
APPROXIMATION ERROR OF HST INITIALIZATION,0.16593886462882096,"OPT T
k (U) =
min
F ⊂U,|F |=k X"
APPROXIMATION ERROR OF HST INITIALIZATION,0.16812227074235808,"y∈U
min
x∈F ρT (x, y) ≡min
C′
1
costT
k
′(U, C′
1).
(5)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.1703056768558952,"For simplicity, we will use costT
k
′(U) to denote costT
k
′(U, C1). Here, OPT T
k (5) is the cost of
the global optimal solution with 2-HST metric. The last equality in (5) holds because the optimal
centers set can always located in k disjoint subtrees, as each leaf only contain one point. (3) is the
k-median cost with 2-HST metric of the output C0 of Algorithm 2. (4) is the oracle cost after the
subtrees are chosen. That is, it represents the optimal cost to pick one center from each subtree in
C1. Firstly, we bound the approximation error of the subtree search process."
APPROXIMATION ERROR OF HST INITIALIZATION,0.17248908296943233,"Lemma 3.3 (Subtree search). costT
k
′(U) ≤5OPT T
k (U)."
APPROXIMATION ERROR OF HST INITIALIZATION,0.17467248908296942,"Next, we show that the greedy leaf search (Algorithm 3) only has constant extra multiplicative error."
APPROXIMATION ERROR OF HST INITIALIZATION,0.17685589519650655,"Lemma 3.4 (Leaf search). costT
k (U) ≤2costT
k
′(U)."
APPROXIMATION ERROR OF HST INITIALIZATION,0.17903930131004367,"Combining Lemma 3.3 and Lemma 3.4, we have the next Theorem."
APPROXIMATION ERROR OF HST INITIALIZATION,0.1812227074235808,"Theorem 3.5 (2-HST error). Running Algorithm 2, we have costT
k (U) ≤10OPT T
k (U)."
APPROXIMATION ERROR OF HST INITIALIZATION,0.18340611353711792,"Thus, HST-initialization produces an O(1) approximation to OPT in the 2-HST metric. Deﬁne
costk(U) as (2) for our HST centers, and the optimal cost w.r.t. ρ as"
APPROXIMATION ERROR OF HST INITIALIZATION,0.185589519650655,"OPTk(U) = min
|F |=k X"
APPROXIMATION ERROR OF HST INITIALIZATION,0.18777292576419213,"y∈U
min
x∈F ρ(x, y).
(6)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.18995633187772926,We have the following result in the general metric space based on Lemma 3.2.
APPROXIMATION ERROR OF HST INITIALIZATION,0.19213973799126638,"Theorem 3.6. Running Algorithm 2 gives E[costk(U)] = O(min{log n, log △})OPTk(U)."
APPROXIMATION ERROR OF HST INITIALIZATION,0.1943231441048035,"Remark 3.2. In the Euclidean space, Makarychev et al. (2019) proved O(log k) random projections
sufﬁce for k-median to achieve O(1) error. Thus, if △= O(d) (e.g., bounded data), by Lemma 3.2,
there exists an algorithm with HST initialization that achieves O(log(min{d, k})) error, which is
better than O(log k) of k-median++ when d is small."
APPROXIMATION ERROR OF HST INITIALIZATION,0.1965065502183406,"NDP-HST Local Search. We can apply standard local search starting from the HST initialization
in Algorithm 1. We call it the NDP-HST (“NDP” stands for “Non-Differentially Private”) method,
with the following guarantee."
APPROXIMATION ERROR OF HST INITIALIZATION,0.19868995633187772,"Theorem 3.7. NDP-HST achieves O(1) approximation in expected O(k log min{log n, log △})
number of iterations for input in general metric space."
APPROXIMATION ERROR OF HST INITIALIZATION,0.20087336244541484,"Before ending this section, we remark that HST initialization and the analysis can be extended to
k-means clustering analogously (see Appendix D). In a general metric space, E[costkm(U)] =
O(min{log n, log △})2OPTkm(U) where costkm(U) is the optimal k-means cost."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.20305676855895197,"4
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2052401746724891,"In this section, we consider private HST initialization method. Recall in this setting, U is the universe
of data points, and D ⊂U is a demand set that needs to be clustered with privacy. Since U is public,
running initialization algorithms on U would preserve the privacy of D. Yet, this might be too
expensive, and in many cases one would probably want to incorporate some information about D
in the initialization, since D could be a very imbalanced subset of U. For example, D may only
contain data points from one cluster, out of tens of clusters in U. In this case, initialization on U
is likely to pick initial centers in multiple clusters, which would not be helpful for clustering on D.
Next, we show how our HST initialization can be easily combined with differential privacy that at
the same time contains information about the demand set D, leading to improved approximation
error (Theorem 4.3) and empirical clustering performance (Section 5)."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2074235807860262,"Again, suppose T is an L = log △-level 2-HST of universe U in a general metric space. Denote
Nv = |T(v) ∩D| for a node point v. Our private HST initialization is similar to the non-private
Algorithm 2. To gain privacy, we perturb Nv by adding i.i.d. Laplace noise:"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2096069868995633,"ˆ
Nv = Nv + Lap(2(L−hv)/ϵ),"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.21179039301310043,Under review as a conference paper at ICLR 2022
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.21397379912663755,Algorithm 4: HST initialization with differential privacy
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.21615720524017468,"Input: U, D, △, k, ϵ
Build a level-L 2-HST T based on input U
for each node v in T do"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2183406113537118,"Nv ←|D ∩T(v)|
ˆ
Nv ←Nv + Lap(2(L−hv)/ϵ)
score(v) ←ˆN(v) · 2hv"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2205240174672489,"Based on ˆNv, apply the same strategy as Algorithm 2: ﬁnd C1; C0 = FIND-LEAF(C1)
Output: Private initial center set C0 ⊆U"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.22270742358078602,"where Lap(2(L−hv)/ϵ) is a Laplace random number with rate 2(L−hv)/ϵ. We will use the perturbed
ˆNv for node sampling instead of the true value Nv, as described in Algorithm 4. The DP guarantee
of this initialization scheme is straightforward by the composition of the Laplace mechanisms."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.22489082969432314,Theorem 4.1. Algorithm 4 is ϵ-differentially private.
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.22707423580786026,"Proof. For each level i, the subtrees T(v, i) are disjoint to each other. The privacy used in i-th level
is ϵ/2(L−i), and the total privacy is P"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2292576419213974,i ϵ/2(L−i) < ϵ.
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2314410480349345,"We now consider the approximation error. As the structure of the analysis is similar to the non-DP
case, we present the main result here and defer the detailed proofs to Appendix C."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2336244541484716,Theorem 4.2. Algorithm 4 outputs an initial center set such that
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.23580786026200873,E[costk(D)] = O(log n)(OPTk(D) + kϵ−1△log n).
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.23799126637554585,"DP-HST Local Search. Similarly, we can use private HST initialization to improve the performance
of private k-median local search, which is presented in Algorithm 5. After initialization, the DP local
search procedure follows Gupta et al. (2010) using the exponential mechanism."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.24017467248908297,Algorithm 5: DP-HST local search
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2423580786026201,"Input: U, demand points D ⊆U, parameter k, ϵ, T
Initialization: F1 the private initial centers generated by Algorithm 4 with privacy ϵ/2
Set parameter ϵ′ =
ϵ
2△(T +1)
for i = 1 to T do"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2445414847161572,"Select (x, y) ∈Fi ×(V \Fi) with prob. proportional to exp(−ϵ′ ×(cost(Fi −{x}+{y}))
Let Fi+1 ←Fi −{x} + {y}
Select j from {1, 2, ..., T + 1} with probability proportional to exp(−ϵ′ × cost(Fj))
Output: F = Fj the private center set"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.24672489082969432,Theorem 4.3. Algorithm 5 achieves ϵ-differential privacy. The output centers admit
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.24890829694323144,costk(D) ≤6OPTk(D) + O(ϵ−1k2△(log log n) log n)
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.25109170305676853,"with probability (1 −1/poly(n)), with T = O(k log log n) iterations."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.25327510917030566,"The DP local search with random initialization (Gupta et al., 2010) has 6 multiplicative error and
O(ϵ−1△k2 log2 n) additive error. Our result improves the log n term to log log n in the additive er-
ror. Meanwhile, the number of iterations needed is improved from T = O(k log n) to O(k log log n)
(see Appendix B for an empirical justiﬁcation). Notably, it has been shown in Gupta et al. (2010)
that for k-median problem, the lower bounds on the multiplicative and additive error of any ϵ-DP al-
gorithm are O(1) and O(ϵ−1△k log(n/k)), respectively. Our result matches the lower bound on the
multiplicative error, and the additive error is only worse than the bound by a factor of O(k log log n)
which is typically small in many cases. Thus, our private HST initialization method pushes the ap-
proximation error of private local search closer to the lower bound. To our knowledge, Theorem 4.3
is the ﬁrst result in literature to improve the error of DP local search in general metric space."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2554585152838428,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2576419213973799,"5
EXPERIMENTS"
EXPERIMENTS,0.259825327510917,"We numerically test the proposed methods on two problems—clustering in an Euclidean space and
on a graph. Our results show that the proposed HST initialization can improve the performance of
using k-median++ initialization in both non-private and private clustering tasks."
DATASETS AND ALGORITHMS,0.26200873362445415,"5.1
DATASETS AND ALGORITHMS"
DATASETS AND ALGORITHMS,0.26419213973799127,"Discrete Euclidean space. Following Balcan et al. (2017), we test k-median clustering on the
MNIST hand-written digit dataset (LeCun et al., 1998) with 10 natural clusters (digit 0 to 9). We
set U as 10000 randomly chosen data points. We choose the demand set D using two strategies: 1)
“balance”, where we randomly choose 500 samples from U; 2) “imbalance”, where D contains 500
random samples from U only from digit “0” and “8” (two clusters). We note that, the imbalanced D
is a very practical setting in real-world scenarios, where data are typically not uniformly distributed.
On this dataset, we test clustering with both l1 and l2 distance as the underlying metric."
DATASETS AND ALGORITHMS,0.2663755458515284,"Metric space induced by graph. Random graphs have been widely considered in testing k-median
methods (Balcan et al., 2013; Todo et al., 2019). The construction of graphs follows a similar
approach as the synthetic pmedinfo graphs provided by the popular OR-Library (Beasley, 1990).
The metric ρ for this experiment is the shortest (weighted) path distance on graph. To generate a
size n graph, we ﬁrst randomly split the nodes into 10 clusters. Within each cluster, each pair of
nodes is connected with probability 0.2 and weight drawn from standard uniform distribution. For
each pair of clusters, we randomly connect some nodes from each cluster, with weights following
uniform [0.5, r]. A larger r makes the graph more separable, i.e., clusters are farther from each other
(see Appendix B for example graphs). We present two cases: r = 1 and r = 100. For this task, U
has 3000 nodes, and the private set D is chosen using similar “balanced” and “imbalanced” scheme
as described above. In the imbalanced case, we choose D randomly from only two clusters."
DATASETS AND ALGORITHMS,0.2685589519650655,"Algorithms.
We compare the following clustering algorithms in both non-DP and DP setting."
DATASETS AND ALGORITHMS,0.27074235807860264,• NDP-rand: Local search on D with random initialization (Algorithm 1).
DATASETS AND ALGORITHMS,0.27292576419213976,• NDP-kmedian++: Algorithm 1 with k-median++ initialization.
DATASETS AND ALGORITHMS,0.27510917030567683,"• NDP-HST: Algorithm 1 with HST initialization, as described in Section 3."
DATASETS AND ALGORITHMS,0.27729257641921395,"• DP-rand: Standard private local search algorithm (Gupta et al., 2010), which is Algo-
rithm 5 with initial centers randomly chosen from U."
DATASETS AND ALGORITHMS,0.2794759825327511,• DP-kmedian++: Algorithm 5 with k-median++ initialization run on U.
DATASETS AND ALGORITHMS,0.2816593886462882,"• DP-HST: Private local search with HST-initialization (Algorithm 5). For non-private tasks,
we set L = 6. For private clustering, we use L = 8."
DATASETS AND ALGORITHMS,0.2838427947598253,"For non-DP methods, we set α = 10−3 in Algorithm 1 and the maximum number of iterations as
20. To examine the quality of initialization as well as the ﬁnal centers, We report both the cost at
initialization and the cost of the ﬁnal output. For DP methods, we run the algorithms for T = 20
steps and report the results with ϵ = 1. We test k ∈{2, 5, 10, 15, 20}. The average cost over T
iterations is reported for more robustness. All results are averaged over 10 independent repetitions."
RESULTS,0.28602620087336245,"5.2
RESULTS"
RESULTS,0.28820960698689957,"The results on MNIST dataset are given in Figure 2 for l1 (left two columns) and l2 (right two
columns) metric. The comparisons are similar in both cases."
RESULTS,0.2903930131004367,"• We see that the initial centers generated by HST has lower k-median cost than k-median++
and random initialization, for both non-DP and DP setting, and for both balanced and
imbalanced demand set D. This conﬁrms that the proposed HST initialization is more
powerful than k-median++ in ﬁnding good initial centers."
RESULTS,0.2925764192139738,"• From the ﬁnal k-median cost plots, we also observe lower cost of HST approaches in DP
clustering. In the non-DP case, the curves overlap, which means despite that HST offers
better initial centers, local search can always ﬁnd a good solution eventually."
RESULTS,0.29475982532751094,Under review as a conference paper at ICLR 2022
RESULTS,0.29694323144104806,"2 
5 
10
15
20
k 4 5 6"
RESULTS,0.29912663755458513,initial cost
BALANCED D,0.30131004366812225,"104
Balanced D"
BALANCED D,0.3034934497816594,MNIST - l1
BALANCED D,0.3056768558951965,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
BALANCED D,0.3078602620087336,"2 
5 
10
15
20
k 3.5 4 4.5 5 5.5"
BALANCED D,0.31004366812227074,k-median cost
BALANCED D,0.31222707423580787,"104
Balanced D"
BALANCED D,0.314410480349345,MNIST - l1
BALANCED D,0.3165938864628821,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
BALANCED D,0.31877729257641924,"2 
5 
10
15
20
k 3500 4000 4500 5000"
BALANCED D,0.32096069868995636,initial cost
BALANCED D,0.3231441048034934,Balanced D
BALANCED D,0.32532751091703055,MNIST - l2
BALANCED D,0.32751091703056767,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
BALANCED D,0.3296943231441048,"2 
5 
10
15
20
k 3500 4000 4500"
BALANCED D,0.3318777292576419,k-median cost
BALANCED D,0.33406113537117904,Balanced D
BALANCED D,0.33624454148471616,MNIST - l2
BALANCED D,0.3384279475982533,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
BALANCED D,0.3406113537117904,"2 
5 
10
15
20
k 4 5 6 7"
BALANCED D,0.34279475982532753,initial cost
IMBALANCED D,0.34497816593886466,"104
Imbalanced D"
IMBALANCED D,0.3471615720524017,MNIST - l1
IMBALANCED D,0.34934497816593885,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.35152838427947597,"2 
5 
10
15
20
k 3.5 4 4.5 5 5.5 6"
IMBALANCED D,0.3537117903930131,k-median cost
IMBALANCED D,0.3558951965065502,"104
Imbalanced D"
IMBALANCED D,0.35807860262008734,MNIST - l1
IMBALANCED D,0.36026200873362446,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.3624454148471616,"2 
5 
10
15
20
k 3500 4000 4500 5000 5500"
IMBALANCED D,0.3646288209606987,initial cost
IMBALANCED D,0.36681222707423583,Imbalanced D
IMBALANCED D,0.36899563318777295,MNIST - l2
IMBALANCED D,0.37117903930131,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.37336244541484714,"2 
5 
10
15
20
k 3500 4000 4500"
IMBALANCED D,0.37554585152838427,k-median cost
IMBALANCED D,0.3777292576419214,Imbalanced D
IMBALANCED D,0.3799126637554585,MNIST - l2
IMBALANCED D,0.38209606986899564,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.38427947598253276,"Figure 2: k-median cost on MNIST dataset. 1st row: balanced D. 2nd row: imbalanced D.
Column 1 & 3: initial cost. Column 2 & 4: ﬁnal k-median cost."
IMBALANCED D,0.3864628820960699,"2 
5 
10
15
20
k 0 500 1000"
IMBALANCED D,0.388646288209607,initial cost
IMBALANCED D,0.39082969432314413,Balanced D
IMBALANCED D,0.3930131004366812,GRAPH r = 100
IMBALANCED D,0.3951965065502183,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.39737991266375544,"2 
5 
10
15
20
k 0 500 1000"
IMBALANCED D,0.39956331877729256,k-median cost
IMBALANCED D,0.4017467248908297,Balanced D
IMBALANCED D,0.4039301310043668,GRAPH r = 100
IMBALANCED D,0.40611353711790393,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.40829694323144106,"2 
5 
10
15
20
k 40 60 80 100"
IMBALANCED D,0.4104803493449782,initial cost
IMBALANCED D,0.4126637554585153,Balanced D
IMBALANCED D,0.4148471615720524,GRAPH r = 1
IMBALANCED D,0.4170305676855895,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.4192139737991266,"2 
5 
10
15
20
k 20 40 60 80 100"
IMBALANCED D,0.42139737991266374,k-median cost
IMBALANCED D,0.42358078602620086,Balanced D
IMBALANCED D,0.425764192139738,GRAPH r = 1
IMBALANCED D,0.4279475982532751,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.43013100436681223,"2 
5 
10
15
20
k 0 200 400 600 800"
IMBALANCED D,0.43231441048034935,initial cost
IMBALANCED D,0.4344978165938865,Imbalanced D
IMBALANCED D,0.4366812227074236,"GRAPH r = 100
NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.4388646288209607,"2 
5 
10
15
20
k 0 100 200 300"
IMBALANCED D,0.4410480349344978,k-median cost
IMBALANCED D,0.4432314410480349,Imbalanced D
IMBALANCED D,0.44541484716157204,GRAPH r = 100
IMBALANCED D,0.44759825327510916,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.4497816593886463,"2 
5 
10
15
20
k 20 40 60 80 100"
IMBALANCED D,0.4519650655021834,initial cost
IMBALANCED D,0.45414847161572053,Imbalanced D
IMBALANCED D,0.45633187772925765,GRAPH r = 1
IMBALANCED D,0.4585152838427948,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.4606986899563319,"2 
5 
10
15
20
k 20 40 60 80"
IMBALANCED D,0.462882096069869,k-median cost
IMBALANCED D,0.4650655021834061,Imbalanced D
IMBALANCED D,0.4672489082969432,GRAPH r = 1
IMBALANCED D,0.46943231441048033,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.47161572052401746,Figure 3: k-median costs on graph dataset. 1st row: balanced D. 2nd row: imbalanced D.
IMBALANCED D,0.4737991266375546,"• The advantage of HST in both initial and ﬁnal cost is more signiﬁcant when D is an imbal-
anced subset of U. As mentioned before, this is because our HST initialization approach
also privately incorporates the information of D."
IMBALANCED D,0.4759825327510917,"The set of results on the graph dataset is reported in Figure 3, which gives us similar conclusion. In
all cases, our proposed HST method ﬁnds better initial centers with smaller cost than k-median++. In
terms of the ﬁnal clustering output, HST again considerably outperforms k-median++ in the private
and imbalanced D setting, for both r = 100 (highly separable graph) and r = 1 (less separable
graph). The improvement of HST over k-median++ is especially signiﬁcant in the harder tasks
when r = 1, i.e., the clusters are nearly mixed up."
CONCLUSION,0.4781659388646288,"6
CONCLUSION"
CONCLUSION,0.48034934497816595,"In this paper, we propose a new initialization framework for the k-median problem in general dis-
crete metric space. Our approach is called HST, which leverages tools from metric embedding
theory. Our novel tree search approach has comparable efﬁciency and cost to k-median++ initializa-
tion. Moreover, we propose differentially private (DP) HST center initialization algorithm, which
adapts to the private demand point set, leading to better clustering performance. When combined
with subsequent DP local search heuristic, our algorithm is able to improve the addition error of
prior works, which is close to the theoretical lower bound within a small factor. Experiments with
Euclidean metrics and graph metrics verify the effectiveness of our methods."
CONCLUSION,0.48253275109170307,Under review as a conference paper at ICLR 2022
REFERENCES,0.4847161572052402,REFERENCES
REFERENCES,0.4868995633187773,"Ameer Ahmed Abbasi and Mohamed F. Younis. A survey on clustering algorithms for wireless
sensor networks. Comput. Commun., 30(14-15):2826–2841, 2007."
REFERENCES,0.4890829694323144,"David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Proceed-
ings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, New
Orleans, Louisiana, USA, January 7-9, 2007, pp. 1027–1035, 2007."
REFERENCES,0.4912663755458515,"Vijay Arya, Naveen Garg, Rohit Khandekar, Adam Meyerson, Kamesh Munagala, and Vinayaka
Pandit. Local search heuristics for k-median and facility location problems. SIAM J. Comput., 33
(3):544–562, 2004."
REFERENCES,0.49344978165938863,"Olivier Bachem, Mario Lucic, S. Hamed Hassani, and Andreas Krause. Approximate k-means++ in
sublinear time. In Dale Schuurmans and Michael P. Wellman (eds.), Proceedings of the Thirtieth
AAAI Conference on Artiﬁcial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pp.
1459–1467. AAAI Press, 2016."
REFERENCES,0.49563318777292575,"Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, and Sergei Vassilvitskii. Scal-
able k-means++. Proc. VLDB Endow., 5(7):622–633, 2012."
REFERENCES,0.4978165938864629,"Maria-Florina Balcan, Steven Ehrlich, and Yingyu Liang. Distributed k-means and k-median cluster-
ing on general communication topologies. In Advances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a
meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pp. 1995–2003, 2013."
REFERENCES,0.5,"Maria-Florina Balcan, Travis Dick, Yingyu Liang, Wenlong Mou, and Hongyang Zhang. Differen-
tially private clustering in high-dimensional euclidean spaces. In Proceedings of the 34th Interna-
tional Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,
pp. 322–331, 2017."
REFERENCES,0.5021834061135371,"Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with breg-
man divergences. J. Mach. Learn. Res., 6:1705–1749, 2005."
REFERENCES,0.5043668122270742,"Yair Bartal. Probabilistic approximations of metric spaces and its algorithmic applications. In 37th
Annual Symposium on Foundations of Computer Science, FOCS ’96, Burlington, Vermont, USA,
14-16 October, 1996, pp. 184–193. IEEE Computer Society, 1996."
REFERENCES,0.5065502183406113,"John E Beasley. Or-library: distributing test problems by electronic mail. Journal of the operational
research society, 41(11):1069–1072, 1990."
REFERENCES,0.5087336244541485,"Inderjit S. Dhillon and Dharmendra S. Modha. Concept decompositions for large sparse text data
using clustering. Mach. Learn., 42(1/2):143–175, 2001."
REFERENCES,0.5109170305676856,"Cynthia Dwork. Differential privacy. In Automata, Languages and Programming, 33rd International
Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II, pp. 1–12, 2006."
REFERENCES,0.5131004366812227,"Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary
metrics by tree metrics. J. Comput. Syst. Sci., 69(3):485–497, 2004."
REFERENCES,0.5152838427947598,"D. Feldman, C. Xiang, R. Zhu, and D. Rus. Coresets for differentially private k-means clustering
and applications to privacy in mobile sensor networks. In 2017 16th ACM/IEEE International
Conference on Information Processing in Sensor Networks (IPSN), pp. 3–16, 2017."
REFERENCES,0.517467248908297,"Dan Feldman, Amos Fiat, Haim Kaplan, and Kobbi Nissim. Private coresets. In Proceedings of the
41st Annual ACM Symposium on Theory of Computing, STOC 2009, Bethesda, MD, USA, May
31 - June 2, 2009, pp. 361–370, 2009."
REFERENCES,0.519650655021834,"Anupam Gupta, Katrina Ligett, Frank McSherry, Aaron Roth, and Kunal Talwar. Differentially
private combinatorial optimization. In Proceedings of the Twenty-First Annual ACM-SIAM Sym-
posium on Discrete Algorithms, SODA 2010, Austin, Texas, USA, January 17-19, 2010, pp. 1106–
1125, 2010."
REFERENCES,0.5218340611353712,Under review as a conference paper at ICLR 2022
REFERENCES,0.5240174672489083,"Zhiyi Huang and Jinyan Liu. Optimal differentially private algorithms for k-means clustering. In
Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database
Systems, Houston, TX, USA, June 10-15, 2018, pp. 395–408, 2018."
REFERENCES,0.5262008733624454,"Tapas Kanungo, David M. Mount, Nathan S. Netanyahu, Christine D. Piatko, Ruth Silverman, and
Angela Y. Wu. A local search approximation algorithm for k-means clustering. In Proceedings of
the 18th Annual Symposium on Computational Geometry, Barcelona, Spain, June 5-7, 2002, pp.
10–18, 2002."
REFERENCES,0.5283842794759825,"Leon Kaufman, Marc Vanden Eede, and Pierre Hansen. A plant and warehouse location problem.
Journal of the Operational Research Society, 28(3):547–554, 1977."
REFERENCES,0.5305676855895196,"Silvio Lattanzi and Christian Sohler. A better k-means++ algorithm via local search. In Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 3662–
3671. PMLR, 2019."
REFERENCES,0.5327510917030568,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.5349344978165939,"S. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28(2):
129–137, 1982."
REFERENCES,0.537117903930131,"Konstantin Makarychev, Yury Makarychev, and Ilya P. Razenshteyn.
Performance of johnson-
lindenstrauss transform for k-means and k-medians clustering. In Proceedings of the 51st Annual
ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26,
2019, pp. 1027–1038. ACM, 2019."
REFERENCES,0.5393013100436681,"Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual IEEE
Symposium on Foundations of Computer Science (FOCS 2007), October 20-23, 2007, Providence,
RI, USA, Proceedings, pp. 94–103, 2007."
REFERENCES,0.5414847161572053,"Richard Nock, Raphaël Canyasse, Roksana Boreli, and Frank Nielsen. k-variates++: more pluses
in the k-means++. In Proceedings of the 33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 145–154, 2016."
REFERENCES,0.5436681222707423,"Girish Punj and David W Stewart. Cluster analysis in marketing research: Review and suggestions
for application. Journal of marketing research, 20(2):134–148, 1983."
REFERENCES,0.5458515283842795,"Rahul Shah. Faster algorithms for k-median problem on trees with smaller heights. Technical report,
2003."
REFERENCES,0.5480349344978166,"Uri Stemmer and Haim Kaplan. Differentially private k-means with constant multiplicative error.
In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In-
formation Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pp.
5436–5446, 2018."
REFERENCES,0.5502183406113537,"Arie Tamir. An o(pn2) algorithm for the p-median and related problems on tree graphs. Oper. Res.
Lett., 19(2):59–64, 1996."
REFERENCES,0.5524017467248908,"Keisuke Todo, Atsuyoshi Nakamura, and Mineichi Kudo.
A fast approximate algorithm for k-
median problem on a graph. 15th International Workshop on Mining and Learning with Graphs,
2019."
REFERENCES,0.5545851528384279,Under review as a conference paper at ICLR 2022
REFERENCES,0.5567685589519651,"A
POSTPONED ALGORITHMS"
REFERENCES,0.5589519650655022,"A.1
k-MEDIAN++"
REFERENCES,0.5611353711790393,"In the paper, we compared our HST initialization mainly with another (perhaps most well-known)
initialization algorithm for clustering, the k-median++ (Arthur & Vassilvitskii, 2007). For reference,
we present the concrete procedures in Algorithm 6. Here, the function D(u, C) is the shortest
distance from a data point u to the closest (center) point in set C. Arthur & Vassilvitskii (2007)
showed that the output centers C by k-median++ achieves O(log k) approximation error, in O(dnk)
time."
REFERENCES,0.5633187772925764,"Algorithm 6: k-median++ (Arthur & Vassilvitskii, 2007)"
REFERENCES,0.5655021834061136,"Input: Data points U, number of centers k
Set C = [ ]
Randomly pick a point c1 in U and set C = C ∪{c1}
for i = 2 to k do"
REFERENCES,0.5676855895196506,"Select ci = u ∈U with probability
D(u,C)
P"
REFERENCES,0.5698689956331878,"u′∈U D(u′,C)
C = C ∪{ci}
Output: k-median++ initial center set C"
REFERENCES,0.5720524017467249,"A.2
CONSTRUCTING A 2-HST"
REFERENCES,0.574235807860262,"As presented in Algorithm 7, the construction starts by applying a permutation π on U, such that in
following steps the points are picked in a random sequence. We ﬁrst ﬁnd a padded decomposition
PL = {PL,1, ..., PL,nL} of U with parameter β = △/2. The center of each partition in PL,j serves
as a root node in level L. Then, we re-do a padded decomposition for each partition PL,j, to ﬁnd
sub-partitions with diameter β = △/4, and set the corresponding centers as the nodes in level L−1,
and so on. Each partition at level i is obtained with β = △/2L−i. This process proceeds until a
node has a single point, or a pre-speciﬁed tree depth is reached. In Figure 1, we provide an example
of L = 3-level 2-HST (left panel), along with its underlying padded decompositions (right panel)."
REFERENCES,0.5764192139737991,"Algorithm 7: Build 2-HST(U, L)"
REFERENCES,0.5786026200873362,"Input: Data points U with diameter △, L
Randomly pick a point in U as the root node of T
Let r = △/2
Apply a permutation π on U // so points will be chosen in a random sequence
for each v ∈U do"
REFERENCES,0.5807860262008734,"Set Cv = [v]
for each u ∈U do"
REFERENCES,0.5829694323144105,"Add u ∈U to Cv if d(v, u) ≤r and u /∈S"
REFERENCES,0.5851528384279476,"v′̸=v Cv′
Set the non-empty clusters Cv as the children nodes of T
for each non-empty cluster Cv do"
REFERENCES,0.5873362445414847,"Run 2-HST(Cv, L −1) to extend the tree T; stop until L levels or reaching a leaf node
Output: 2-HST T"
REFERENCES,0.5895196506550219,Under review as a conference paper at ICLR 2022
REFERENCES,0.5917030567685589,"B
MORE EXPERIMENTS"
REFERENCES,0.5938864628820961,"B.1
EXAMPLES OF GRAPH DATA"
REFERENCES,0.5960698689956332,"In Figure 4, we plot two example graphs (subgraphs of 50 nodes) with r = 100 and r = 1. When
r = 100, the graph is highly separable (i.e., clusters are far from each other). When r = 1, the
clusters are harder to be distinguished from each other. 1 2
3 4 5 6 7 8 9 10 11 12 13
14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
50"
REFERENCES,0.5982532751091703,"Figure 4: Example of synthetic graphs: subgraph of 50 nodes. Left: r = 1. Right: r = 100. Darker
and thicker edged have smaller distance. When r = 100, the graph is more separable."
REFERENCES,0.6004366812227074,"B.2
RUNNING TIME COMPARISON WITH k-MEDIAN++"
REFERENCES,0.6026200873362445,"In Proposition 3.1, we show that our HST initialization algorithm admits O(dn log n) complexity
when considering the Euclidean space. With a smart implementation of Algorithm 6 where each
data point tracks its distance to the current closest candidate center in C, k-median++ has O(dnk)
running time. Therefore, the running time of our algorithm is in general comparable to k-median++.
Our method would run faster if k = Ω(log n). In Figure 5, we plot the empirical running time
of HST initialization against k-median++, on MNIST dataset with l2 distance (similar comparison
holds for l1). From the left subﬁgure, we see that k-median++ becomes slower with increasing k,
and our method is more efﬁcient when k > 20. In the right panel, we observe that the running
time of both methods increases with larger sample size n. Our HST algorithm has a slightly faster
increasing rate, which is predicted by the complexity comparison (n log n v.s. n). However, this
difference in log n factor would not be too signiﬁcant unless the sample size is extremely large.
Overall, our numerical results suggest that in general, the proposed HST initialization would have
similar efﬁciency as k-median++ in common practical scenarios."
REFERENCES,0.6048034934497817,"2 
5 
10
20
30
k 0 0.2 0.4 0.6"
REFERENCES,0.6069868995633187,time (s)
REFERENCES,0.6091703056768559,"500 
1000
2000
3000
5000
n 0 0.4 0.8 1.2"
REFERENCES,0.611353711790393,time (s)
REFERENCES,0.6135371179039302,"Figure 5: Empirical time comparision of HST initialization v.s. k-median++, on MNIST dataset
with l2 distance. Left: The running time against k, on a subset of n = 2000 data points. Right: The
running time against n, with k = 20 centers."
REFERENCES,0.6157205240174672,Under review as a conference paper at ICLR 2022
REFERENCES,0.6179039301310044,"B.3
IMPROVED ITERATION COST OF DP-HST"
REFERENCES,0.6200873362445415,"In Theorem 4.3, we show that under differential privacy constraints, the proposed DP-HST (Algo-
rithm 5) improves both the approximation error and the number of iterations required to ﬁnd a good
solution of classical DP local search (Gupta et al., 2010). In this section, we provide some numerical
results to justify the theory."
REFERENCES,0.6222707423580786,"First, we need to properly measure the iteration cost of DP local search. This is because, unlike the
non-private clustering, the k-median cost after each iteration in DP local search is not decreasing
monotonically, due to the probabilistic exponential mechanism. To this end, for the cost sequence
with length T = 20, we compute its moving average sequence with window size 5. Attaining the
minimal value of the moving average indicates that the algorithm has found a “local optimum”, i.e.,
it has reached a “neighborhood” of solutions with small clustering cost. Thus, we use the number of
iterations to reach such local optimum as the measure of iteration cost. The results are provided in
Figure 6. We see that on all the tasks (MNIST with l1 and l2 distance, and graph dataset with r = 1
and r = 100), DP-HST has signiﬁcantly smaller iterations cost. In Figure 7, we further report the
k-median cost of the best solution in T iterations found by each DP algorithm. We see that DP-HST
again provide the smallest cost. This additional set of experiments again validates the claims of
Theorem 4.3, that DP-HST is able to found better initial centers in fewer iterations."
REFERENCES,0.6244541484716157,"2 
5 
10
15
20
k 6 8 10 12 14 16"
REFERENCES,0.6266375545851528,iterations to min cost
REFERENCES,0.62882096069869,MNIST - l1
REFERENCES,0.631004366812227,"DP-HST
DP-kmedian++
DP-rand"
REFERENCES,0.6331877729257642,"2 
5 
10
15
20
k 4 6 8 10 12 14"
REFERENCES,0.6353711790393013,iterations to min cost
REFERENCES,0.6375545851528385,MNIST - l2
REFERENCES,0.6397379912663755,"DP-HST
DP-kmedian++
DP-rand"
REFERENCES,0.6419213973799127,"2 
5 
10
15
20
k 6 8 10 12 14 16"
REFERENCES,0.6441048034934498,iterations to min cost
REFERENCES,0.6462882096069869,GRAPH r = 100
REFERENCES,0.648471615720524,"DP-HST
DP-kmedian++
DP-rand"
REFERENCES,0.6506550218340611,"2 
5 
10
15
20
k 6 8 10 12 14 16"
REFERENCES,0.6528384279475983,iterations to min cost
REFERENCES,0.6550218340611353,GRAPH r = 1
REFERENCES,0.6572052401746725,"DP-HST
DP-kmedian++
DP-rand"
REFERENCES,0.6593886462882096,"Figure 6: Iteration cost to reach a locally optimal solution, on MNIST and graph datasets with
different k. The demand set is an imbalanced subset of the universe."
REFERENCES,0.6615720524017468,"2 
5 
10
15
20
k 4 4.5 5"
REFERENCES,0.6637554585152838,min k-median cost
IMBALANCED D,0.665938864628821,"104
Imbalanced D"
IMBALANCED D,0.6681222707423581,MNIST - l1
IMBALANCED D,0.6703056768558951,"DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.6724890829694323,"2 
5 
10
15
20
k 3000 3500 4000"
IMBALANCED D,0.6746724890829694,min k-median cost
IMBALANCED D,0.6768558951965066,Imbalanced D
IMBALANCED D,0.6790393013100436,MNIST - l2
IMBALANCED D,0.6812227074235808,"DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.6834061135371179,"Figure 7: The k-median cost of the best solution found by each differentially private algorithm. The
demand set is an imbalanced subset of the universe. Same comparison holds on graph data."
IMBALANCED D,0.6855895196506551,Under review as a conference paper at ICLR 2022
IMBALANCED D,0.6877729257641921,"C
PROOFS"
IMBALANCED D,0.6899563318777293,The following composition result of differential privacy will be used in our proof.
IMBALANCED D,0.6921397379912664,"Theorem C.1 (Composition Theorem (Dwork, 2006)). If Algorithms A1, A2, ..., Am are
ϵ1, ϵ2, ..., ϵm differentially private respectively, then the union (A1(D), A2(D), ..., Am(D)) is
Pm
i=1 ϵi-DP."
IMBALANCED D,0.6943231441048034,"C.1
PROOF OF LEMMA 3.3"
IMBALANCED D,0.6965065502183406,"Proof. Consider the intermediate output of Algorithm 2, C1 = {v1, v2, ..., vk}, which is the set of
roots of the minimal subtrees each containing exactly one output center C0. Suppose one of the
optimal “root set” that minimizes (4) is C∗
1 = {v′
1, v′
2, ..., v′
k}. If C1 = C∗
1, the proof is done. Thus,
we prove the case for C1 ̸= C∗
1. Note that T(v), v ∈C1 are disjoint subtrees. We have the following
reasoning."
IMBALANCED D,0.6986899563318777,"• Case 1: for some i, j′, vi is a descendant node of v′
j. Since the optimal center point f ∗is a
leaf node by the deﬁnition of (4), we know that there must exist one child node of v′
j that
expands a subtree which contains f ∗. Therefore, we can always replace v′
j by one of its
child nodes. Hence, we can assume that vi is not a descendant of v′
j."
IMBALANCED D,0.7008733624454149,"Note that, we have score(v′
j) ≤score(vi) if v′
j /∈C∗
1 ∩C1. Algorithm 2 sorts all the nodes
based on cost value, and it would have more priority to pick v′
j than vi if score(v′
j) >
score(vi) and vi is not a child node of v′
j."
IMBALANCED D,0.7030567685589519,"• Case 2: for some i, j′, v′
j is a descendant of vi. In this case, optimal center point f ∗, which
is a leaf of T(vi), must also be a leaf node of T(v′
j). We can simply replace C1 with the
swap C1 \ {vi} + {v′
j} which does not change costT
k
′(U). Hence, we can assume that v′
j
is not a descendant of vi."
IMBALANCED D,0.7052401746724891,"• Case 3:
Otherwise.
By the construction of C1, we know that score(v′
j)
≤
min{score(vi), i = 1, ..., k} when v′
j ∈C∗
1 \ C1. Consider the swap between C1 and
C∗
1. By the deﬁnition of tree distance, we have OPT T
k (U) ≥P"
IMBALANCED D,0.7074235807860262,"vi∈C1\C∗
1 Nvi2hvi, since
{T(vi), vi ∈C1 \ C∗
1} does not contain any center of the optimal solution determined by
C∗
1 (which is also the optimal “root set” for OPT T
k (U))."
IMBALANCED D,0.7096069868995634,"Thus, we only need to consider Case 3. Let us consider the optimal clustering with center set be
C∗= {c∗
1, c∗
2, ..., c∗
k} (each center c∗
j is a leaf of subtree whose root be c′
j), and S′
j be the leaves
assigned to c∗
j. Let Sj denote the set of leaves in S′
j whose distance to c∗
j is strictly smaller than its
distance to any centers in C1. Let Pj denote the union of paths between leaves of Sj to its closest
center in C1. Let v′′
j be the nodes in Pj with highest level satisfying T(v′′
j ) ∩C1 = ∅. The score of"
IMBALANCED D,0.7117903930131004,"v′′
j is 2
hv′′
j N(v′′
j ). That means the swap with a center v′
j into C1 can only reduce 4 · 2
hv′′
j N(v′′
j ) to"
IMBALANCED D,0.7139737991266376,"costT
k
′(U) (the tree distance between any leaf in Sj and its closest center in C1 is at most 4 · 2
hv′′
j ).
We just use v′
j to represent v′′
j for later part of this proof for simplicity. By our reasoning, summing
all the swaps over C∗
1 \ C1 gives"
IMBALANCED D,0.7161572052401747,"costT
k
′(U) −OPT T
k (U) ≤4
X"
IMBALANCED D,0.7183406113537117,"v′
j∈C∗
1 \C1
Nv′
j2
hv′
j ,"
IMBALANCED D,0.7205240174672489,"OPT T
k (U) ≥
X"
IMBALANCED D,0.722707423580786,"vi∈C1\C∗
1
Nvi2hvi."
IMBALANCED D,0.7248908296943232,"Also, based on our discussion on Case 1, it holds that"
IMBALANCED D,0.7270742358078602,"Nv′
j2
hv′
j −Nvi2hvi ≤0."
IMBALANCED D,0.7292576419213974,"Summing them together, we have costT
k
′(U) ≤5OPT T
k (U)."
IMBALANCED D,0.7314410480349345,Under review as a conference paper at ICLR 2022
IMBALANCED D,0.7336244541484717,"C.2
PROOF OF LEMMA 3.4"
IMBALANCED D,0.7358078602620087,"Proof. Since the subtrees in C1 are disjoint, it sufﬁces to consider one subtree with root v. With a
little abuse of notation, let costT
1
′(v, U) denote the optimal k-median cost within the point set T(v)
with one center in 2-HST:"
IMBALANCED D,0.7379912663755459,"costT
1
′(v, U) = min
x∈T (v) X"
IMBALANCED D,0.740174672489083,"y∈T (v)
ρT (x, y),
(7)"
IMBALANCED D,0.74235807860262,"which is the optimal cost within the subtree. Suppose v has more than one children u, w, ..., other-
wise the optimal center is clear. Suppose the optimal solution of costT
1
′(v, U) chooses a leaf node
in T(u), and our HST initialization algorithm picks a leaf of T(w). If u = w, then HST chooses the
optimal one where the argument holds trivially. Thus, we consider u ̸= w. We have the following
two observations:"
IMBALANCED D,0.7445414847161572,"• Since one needs to pick a leaf of T(u) to minimize costT
1
′(v, U), we have costT
1
′(v, U) ≥
P"
IMBALANCED D,0.7467248908296943,"x∈ch(v),x̸=u Nx · 2hx where ch(u) denotes the children nodes of u."
IMBALANCED D,0.7489082969432315,"• By our greedy strategy, costT
1 (v, U) ≤P"
IMBALANCED D,0.7510917030567685,"x∈ch(u) Nx · 2hx ≤costT
1
′(v, U) + Nu · 2hu."
IMBALANCED D,0.7532751091703057,"Since hu = hw, we have
2hu · (Nu −Nw) ≤0,"
IMBALANCED D,0.7554585152838428,"since our algorithm picks subtree roots with highest scores.
Then we have costT
1 (v, U) ≤
costT
1
′(v, U) + Nw · 2hw ≤2costT
1
′(v, U). Since the subtrees in C1 are disjoint, the union of
centers for OPT T
1 (v, U), v ∈C1 forms the optimal centers with size k. Note that, for any data
point p ∈U \ C1, the tree distance ρT (p, f) for ∀f that is a leaf node of T(v), v ∈C1 is the same.
That is, the choice of leaf in T(v) as the center does not affect the k-median cost under 2-HST
metric. Therefore, union bound over k subtree costs completes the proof."
IMBALANCED D,0.75764192139738,"C.3
PROOF OF PROPOSITION 3.1"
IMBALANCED D,0.759825327510917,"Proof. It is known that the 2-HST can be constructed in O(dn log n) (Bartal, 1996). The subtree
search in Algorithm 2 involves at most sorting all the nodes in the HST based on the score, which
takes O(nlogn). We use a priority queue to store the nodes in C1. When we insert a new node v
into queue, its parent node (if existing in the queue) would be removed from the queue. The number
of nodes is O(n) and each operation (insertion, deletion) in a priority queue based on score has
O(log n) complexity. Lastly, the total time to obtain C0 is O(n), as the FIND-LEAF only requires
a top down scan in k disjoint subtrees of T. Summing parts together proves the claim."
IMBALANCED D,0.7620087336244541,"C.4
PROOF OF THEOREM 4.2"
IMBALANCED D,0.7641921397379913,"Similarly, we prove the error in general metric by ﬁrst analyzing the error in 2-HST metric. Then the
result follows from Lemma 3.2. Let costT
k (D), costT
k
′(D) and OPT T
k (D) be deﬁned analogously
to (3), (4) and (5), where “y ∈U” in the summation is changed into “y ∈D” since D is the demand
set. That is,"
IMBALANCED D,0.7663755458515283,"costT
k (D) =
X"
IMBALANCED D,0.7685589519650655,"y∈D
min
x∈C0 ρT (x, y),
(8)"
IMBALANCED D,0.7707423580786026,"costT
k
′(D, C1) =
min
|F ∩T (v)|=1,∀v∈C1 X"
IMBALANCED D,0.7729257641921398,"y∈D
min
x∈F ρT (x, y),
(9)"
IMBALANCED D,0.7751091703056768,"OPT T
k (D) =
min
F ⊂D,|F |=k X"
IMBALANCED D,0.777292576419214,"y∈D
min
x∈F ρT (x, y) ≡min
C′
1
costT
k
′(D, C′
1).
(10)"
IMBALANCED D,0.7794759825327511,We have the following.
IMBALANCED D,0.7816593886462883,"Lemma C.2. costT
k (D) ≤10OPT T
k (D) + 10ckϵ−1△log n with probability 1 −4k/nc."
IMBALANCED D,0.7838427947598253,Under review as a conference paper at ICLR 2022
IMBALANCED D,0.7860262008733624,"Proof. The result follows by combining the following Lemma C.4, Lemma C.5, and applying union
bound."
IMBALANCED D,0.7882096069868996,"Lemma C.3. For any node v in T, with probability 1 −1/nc, | ˆNv · 2hv −Nv · 2hv| ≤cϵ−1△log n."
IMBALANCED D,0.7903930131004366,"Proof. Since ˆ
Nv = Nv + Lap(2(L−hv)/2/ϵ), we have"
IMBALANCED D,0.7925764192139738,Pr[| ˆNv −Nv| ≥x/ϵ] = exp(−x/2(L−hv)).
IMBALANCED D,0.7947598253275109,"As L = log △, we have"
IMBALANCED D,0.7969432314410481,Pr[| ˆNv −Nv| ≥x△/(2hvϵ)] ≤exp(−x).
IMBALANCED D,0.7991266375545851,"Hence, for some constant c > 0,"
IMBALANCED D,0.8013100436681223,Pr[| ˆNv · 2hv −Nv · 2hv| ≤cϵ−1△log n] ≥1 −exp(−c log n) = 1 −1/nc.
IMBALANCED D,0.8034934497816594,"Lemma C.4 (DP Subtree Search). With probability 1 −2k/nc, costT
k
′(D) ≤5OPT T
k (D) +
4ckϵ−1△log n."
IMBALANCED D,0.8056768558951966,"Proof. The proof is similar to that of Lemma 3.3. Consider the intermediate output of Algorithm 2,
C1 = {v1, v2, ..., vk}, which is the set of roots of the minimal disjoint subtrees each containing
exactly one output center C0. Suppose one of the optimal “root set” that minimizes (4) is C∗
1 =
{v′
1, v′
2, ..., v′
k}. Assume C1 ̸= C∗
1. By the same argument as the proof of Lemma 3.3, we consider
for some i, j such that vi ̸= v′
j, where vi is not a descendent of v′
j and v′
j is either a descendent
of vi. By the construction of C1, we know that score(v′
j) ≤min{score(vi), i = 1, ..., k} when
v′
j ∈C∗
1 \ C1. Consider the swap between C1 and C∗
1. By the deﬁnition of tree distance, we have
OPT T
k (U) ≥P"
IMBALANCED D,0.8078602620087336,"vi∈C1\C∗
1 Nvi2hvi, since {T(vi), vi ∈C1 \ C∗
1} does not contain any center of the
optimal solution determined by C∗
1 (which is also the optimal “root set” for OPT T
k ). Let us consider
the optimal clustering with center set be C∗= {c∗
1, c∗
2, ..., c∗
k} (each center c∗
j is a leaf of subtree
whose root be c′
j), and S′
j be the leaves assigned to c∗
j. Let Sj denote the set of leaves in S′
j whose
distance to c∗
j is strictly smaller than its distance to any centers in C1. Let Pj denote the union of
paths between leaves of Sj to its closest center in C1. Let v′′
j be the nodes in Pj with highest level"
IMBALANCED D,0.8100436681222707,"satisfying T(v′′
j ) ∩C1 = ∅. The score of v′′
j is 2
hv′′
j N(v′′
j ). That means the swap with a center v′
j
into C1 can only reduce 4 · 2
hv′′
j N(v′′
j ) to costT
k
′(U) (the tree distance between any leaf in Sj and"
IMBALANCED D,0.8122270742358079,"its closest center in C1 is at most 4 · 2
hv′′
j ). We just use v′
j to represent v′′
j for later part of this proof
for simplicity. Summing all the swaps over C∗
1 \ C1, we obtain"
IMBALANCED D,0.8144104803493449,"costT
k
′(U) −OPT T
k (U) ≤4
X"
IMBALANCED D,0.8165938864628821,"v′
j∈C∗
1 \C1
Nv′
j2
hv′
j ,"
IMBALANCED D,0.8187772925764192,"OPT T
k (U) ≥
X"
IMBALANCED D,0.8209606986899564,"vi∈C1\C∗
1
Nvi2hvi."
IMBALANCED D,0.8231441048034934,"Applying union bound with Lemma C.3, with probability 1 −2/nc, we have"
IMBALANCED D,0.8253275109170306,"Nv′
j2
hv′
j −Nvi2hvi ≤2cϵ−1△log n."
IMBALANCED D,0.8275109170305677,"Consequently, we have with probability, 1 −2k/nc,"
IMBALANCED D,0.8296943231441049,"costT
k
′(D) ≤5OPT T
k (D) + 4c|C1 \ C∗
1|ϵ−1△log n"
IMBALANCED D,0.8318777292576419,"≤5OPT T
k (D) + 4ckϵ−1△log n."
IMBALANCED D,0.834061135371179,"Lemma C.5 (DP Leaf Search). With probability 1 −2k/nc, Algorithm 4 produces initial centers
with costT
k (D) ≤2costT
k
′(D) + 2ckϵ−1△log n."
IMBALANCED D,0.8362445414847162,Under review as a conference paper at ICLR 2022
IMBALANCED D,0.8384279475982532,"Proof. The proof strategy follows Lemma 3.4. We ﬁrst consider one subtree with root v. Let
costT
1
′(v, U) denote the optimal k-median cost within the point set T(v) with one center in 2-HST:"
IMBALANCED D,0.8406113537117904,"costT
1
′(v, D) = min
x∈T (v) X"
IMBALANCED D,0.8427947598253275,"y∈T (v)∩D
ρT (x, y).
(11)"
IMBALANCED D,0.8449781659388647,"Suppose v has more than one children u, w, ..., and the optimal solution of costT
1
′(v, U) chooses a
leaf node in T(u), and our HST initialization algorithm picks a leaf of T(w). If u = w, then HST
chooses the optimal one where the argument holds trivially. Thus, we consider u ̸= w. We have the
following two observations:"
IMBALANCED D,0.8471615720524017,"• Since one needs to pick a leaf of T(u) to minimize costT
1
′(v, U), we have costT
1
′(v, U) ≥
P"
IMBALANCED D,0.8493449781659389,"x∈ch(v),x̸=u Nx · 2hx where ch(u) denotes the children nodes of u."
IMBALANCED D,0.851528384279476,"• By our greedy strategy, costT
1 (v, U) ≤P"
IMBALANCED D,0.8537117903930131,"x∈ch(u) Nx · 2hx ≤costT
1
′(v, U) + Nu · 2hu."
IMBALANCED D,0.8558951965065502,"As hu = hw, leveraging Lemma C.3, with probability 1 −2/nc,"
IMBALANCED D,0.8580786026200873,2hu · (Nu −Nw) ≤2hu( ˆNu −ˆNw) + 2cϵ−1△log n
IMBALANCED D,0.8602620087336245,≤2cϵ−1△log n.
IMBALANCED D,0.8624454148471615,"since our algorithm picks subtree roots with highest scores.
Then we have costT
1 (v, D) ≤
costT
k
′(v, D) + Nw · 2hu + 2cϵ−1△log n ≤2costT
k
′(v, D) + 2cϵ−1△log n with high probabil-
ity. Lastly, applying union bound over the disjoint k subtrees gives the desired result."
IMBALANCED D,0.8646288209606987,"C.5
PROOF OF THEOREM 4.3"
IMBALANCED D,0.8668122270742358,"Proof. The privacy analysis is straightforward, by using the composition theorem (Theorem C.1).
Since the sensitivity of cost(·) is △, in each swap iteration the privacy budget is ϵ/2(T + 1). Also,
we spend another ϵ/2(T + 1) privacy for picking a output. Hence, the total privacy is ϵ/2 for local
search. Algorithm 4 takes ϵ/2 DP budget for initialization, so the total privacy is ϵ."
IMBALANCED D,0.868995633187773,"The analysis of the approximation error follows from Gupta et al. (2010), where the initial cost is
reduced by our private HST method. We need the following two lemmas."
IMBALANCED D,0.87117903930131,"Lemma C.6 (Gupta et al. (2010)). Assume the solution to the optimal utility is unique. For any
output o ∈O of 2△ϵ-DP exponential mechanism on dataset D, it holds for ∀t > 0 that"
IMBALANCED D,0.8733624454148472,"Pr[q(D, o) ≤max
o∈O q(D, o) −(ln |O| + t)/ϵ] ≤e−t,"
IMBALANCED D,0.8755458515283843,where |O| is the size of the output set.
IMBALANCED D,0.8777292576419214,"Lemma C.7 (Arya et al. (2004)). For any set F ⊆D with |F| = k, there exists some swap (x, y)
such that the local search method admits"
IMBALANCED D,0.8799126637554585,"costk(F, D) −costk(F −{x} + {y}, D) ≥costk(F, D) −5OPT(D) k
."
IMBALANCED D,0.8820960698689956,"From Lemma C.7, we know that when costk(Fi, D) > 6OPT(D), there exists a swap (x, y) s.t."
IMBALANCED D,0.8842794759825328,"costk(Fi −{x} + {y}, D) ≤(1 −1"
IMBALANCED D,0.8864628820960698,"6k )costk(Fi, D)."
IMBALANCED D,0.888646288209607,"At each iteration, there are at most n2 possible outputs (i.e., possible swaps), i.e., |O| = n2. Using
Lemma C.6 with t = 2 log n, for ∀i,"
IMBALANCED D,0.8908296943231441,"Pr[costk(Fi+1, D) ≥costk(F ∗
i+1, D) + 4log n"
IMBALANCED D,0.8930131004366813,"ϵ′
] ≥1 −1/n2,"
IMBALANCED D,0.8951965065502183,"where costk(F ∗
i+1, D) is the minimum cost among iteration 1, 2, ..., t + 1. Hence, we have that as
long as cost(Fi, D) > 6OPT(D) + 24k log n"
IMBALANCED D,0.8973799126637555,"ϵ′
, the improvement in cost is at least by a factor of"
IMBALANCED D,0.8995633187772926,Under review as a conference paper at ICLR 2022 (1−1
IMBALANCED D,0.9017467248908297,"6k). By Theorem 4.2, we have costk(F1, D) ≤C(log n)(6OPT(D)+6k△log n/ϵ) for some
constant C > 0. Let T = 6Ck log log n. We have that"
IMBALANCED D,0.9039301310043668,"E[cost(Fi, D)] ≤(6OPT(D) + 6kϵ−1△log n)C(log n)(1 −1/6k)6Ck log log n"
IMBALANCED D,0.9061135371179039,"≤6OPT(D) + 6kϵ−1△log n ≤6OPT(D) + 24k log n ϵ′
."
IMBALANCED D,0.9082969432314411,"Therefore, with probability at least (1 −T/n2), there exists an i <= T s.t.
cost(Fi, D) ≤
6OPT(D) + 24k log n"
IMBALANCED D,0.9104803493449781,"ϵ′
. Then by using the Lemma C.7, one will pick an Fj with additional additive
error 4 ln n/ϵ′ to the min{cost(Fj, D), j = 1, 2, ..., T} with probability 1 −1/n2. Consequently,
we know that the expected additive error is"
IMBALANCED D,0.9126637554585153,"24k△log n/ϵ′ + 4 log n/ϵ′ = O(ϵ−1k2△(log log n) log n),"
IMBALANCED D,0.9148471615720524,with probability 1 −1/poly(n).
IMBALANCED D,0.9170305676855895,"D
EXTEND HST INITIALIZATION TO k-MEANS"
IMBALANCED D,0.9192139737991266,"Naturally, our HST method can also be applied to k-means clustering problem. In this section, we
extend the HST to k-means and provide some brief analysis similar to k-median. We present the
analysis in the non-private case, which can then be easily adapted to the private case. Deﬁne the
following costs for k-means."
IMBALANCED D,0.9213973799126638,"costT
km(U) =
X"
IMBALANCED D,0.9235807860262009,"y∈U
min
x∈C0 ρT (x, y)2,
(12)"
IMBALANCED D,0.925764192139738,"costT
km
′(U, C1) =
min
|F ∩T (v)|=1,∀v∈C1 X"
IMBALANCED D,0.9279475982532751,"y∈U
min
x∈F ρT (x, y)2,
(13)"
IMBALANCED D,0.9301310043668122,"OPT T
km(U) =
min
F ⊂U,|F |=k X"
IMBALANCED D,0.9323144104803494,"y∈U
min
x∈F ρT (x, y)2 ≡min
C′
1
costT
km
′(U, C′
1).
(14)"
IMBALANCED D,0.9344978165938864,"For simplicity, we will use costT
km
′(U) to denote costT
km
′(U, C1) if everything is clear from context.
Here, OPT T
km (14) is the cost of the global optimal solution with 2-HST metric."
IMBALANCED D,0.9366812227074236,"Lemma D.1 (Subtree search). costT
km
′(U) ≤17OPT T
km(U)."
IMBALANCED D,0.9388646288209607,"Proof. The analysis is similar with the proof of Lemma 3.3. Thus, we mainly highlight the differ-
ence. Let us just use some notations the same as in Lemma 3.3 here. Let us consider the clustering
with center set be C∗= {c∗
1, c∗
2, ..., c∗
k} (each center c∗
j is a leaf of subtree whose root be c′
j), and
S′
j be the leaves assigned to c∗
j in optimal k-means clustering in tree metric. Let Sj denote the set
of leaves in S′
j whose distance to c∗
j is strictly smaller than its distance to any centers in C1. Let Pj
denote the union of paths between leaves of Sj to its closest center in C1. Let v′′
j be the nodes in"
IMBALANCED D,0.9410480349344978,"Pj with highest level satisfying T(v′′
j ) ∩C1 = ∅. The score of v′′
j is 2
hv′′
j N(v′′
j ). That means the"
IMBALANCED D,0.9432314410480349,"swap with a center v′
j into C1 can only reduce (4 · 2
hv′′
j )2N(v′′
j ) to costT
km
′(U). We just use v′
j to
represent v′′
j for later part of this proof for simplicity. By our reasoning, summing all the swaps over
C∗
1 \ C1 gives"
IMBALANCED D,0.9454148471615721,"costT
km
′(U) −OPT T
km(U) ≤
X"
IMBALANCED D,0.9475982532751092,"v′
j∈C∗
1 \C1
Nv′
j · (4 · 2
hv′
j )2,"
IMBALANCED D,0.9497816593886463,"OPT T
km(U) ≥
X"
IMBALANCED D,0.9519650655021834,"vi∈C1\C∗
1
Nvi(2hvi)2."
IMBALANCED D,0.9541484716157205,"Also, based on our discussion on Case 1, it holds that"
IMBALANCED D,0.9563318777292577,"Nv′
j2
hv′
j −Nvi2hvi ≤0."
IMBALANCED D,0.9585152838427947,"Summing them together, we have costT
km
′(U) ≤17OPT T
km(U)."
IMBALANCED D,0.9606986899563319,Under review as a conference paper at ICLR 2022
IMBALANCED D,0.962882096069869,"Next, we show that the greedy leaf search strategy (Algorithm 3) only leads to an extra multiplicative
error of 2."
IMBALANCED D,0.9650655021834061,"Lemma D.2 (Leaf search). costT
km(U) ≤2costT
km
′(U)."
IMBALANCED D,0.9672489082969432,"Proof. Since the subtrees in C1 are disjoint, it sufﬁces to consider one subtree with root v. With a
little abuse of notation, let costT
1
′(v, U) denote the optimal k-means cost within the point set T(v)
with one center in 2-HST:"
IMBALANCED D,0.9694323144104804,"costT
1
′(v, U) = min
x∈T (v) X"
IMBALANCED D,0.9716157205240175,"y∈T (v)
ρT (x, y)2,
(15)"
IMBALANCED D,0.9737991266375546,"which is the optimal cost within the subtree. Suppose v has more than one children u, w, ..., other-
wise the optimal center is clear. Suppose the optimal solution of costT
1
′(v, U) chooses a leaf node
in T(u), and our HST initialization algorithm picks a leaf of T(w). If u = w, then HST chooses the
optimal one where the argument holds trivially. Thus, we consider u ̸= w. We have the following
two observations:"
IMBALANCED D,0.9759825327510917,"• Since one needs to pick a leaf of T(u) to minimize costT
1
′(v, U), we have costT
1
′(v, U) ≥
P"
IMBALANCED D,0.9781659388646288,"x∈ch(v),x̸=u Nx · (2hx)2 where ch(u) denotes the children nodes of u."
IMBALANCED D,0.980349344978166,"• By our greedy strategy, costT
1 (v, U) ≤P"
IMBALANCED D,0.982532751091703,"x∈ch(u) Nx·(2hx)2 ≤costT
1
′(v, U)+Nu·(2hu)2."
IMBALANCED D,0.9847161572052402,"Since hu = hw, we have
2hu · (Nu −Nw) ≤0,"
IMBALANCED D,0.9868995633187773,"since our algorithm picks subtree roots with highest scores.
Then we have costT
1 (v, U) ≤
costT
1
′(v, U) + Nw · (2hw)2 ≤2costT
1
′(v, U). Since the subtrees in C1 are disjoint, the union
of centers for OPT T
1 (v, U), v ∈C1 forms the optimal centers with size k. Note that, for any data
point p ∈U \ C1, the tree distance ρT (p, f) for ∀f that is a leaf node of T(v), v ∈C1 is the same.
That is, the choice of leaf in T(v) as the center does not affect the k-median cost under 2-HST
metric. Therefore, union bound over k subtree costs completes the proof."
IMBALANCED D,0.9890829694323144,"We are ready to state the error bound for our proposed HST initialization (Algorithm 2), which is a
natural combination of Lemma D.1 and Lemma D.2."
IMBALANCED D,0.9912663755458515,"Theorem D.3 (HST initialization). costT
km(U) ≤34OPT T
km(U)."
IMBALANCED D,0.9934497816593887,We have the following result based on Lemma 3.2.
IMBALANCED D,0.9956331877729258,"Theorem D.4. In a general metric space,"
IMBALANCED D,0.9978165938864629,"E[costkm(U)] = O(min{log n, log △})2OPTkm(U)."
