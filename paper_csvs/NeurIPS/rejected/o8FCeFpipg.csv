Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019569471624266144,"Graph neural networks (GNNs) are currently highly regarded in graph repre-
1"
ABSTRACT,0.003913894324853229,"sentation learning tasks due to their significant performance. Although various
2"
ABSTRACT,0.005870841487279843,"propagation mechanisms and graph filters were proposed, few works have investi-
3"
ABSTRACT,0.007827788649706457,"gated their rationale from the perspective of learning. In this paper, we elucidate
4"
ABSTRACT,0.009784735812133072,"the criterion for the graph filter formed by power series, and further establish a
5"
ABSTRACT,0.011741682974559686,"scalable regularized learning framework that theoretically realizes very deep GNN.
6"
ABSTRACT,0.0136986301369863,"Following the framework, we introduce Adaptive Power GNN (APGNN), a deep
7"
ABSTRACT,0.015655577299412915,"GNN that employs exponentially decaying weights to aggregate graph information
8"
ABSTRACT,0.01761252446183953,"of varying orders, thus facilitating more effective mining of deeper neighbor infor-
9"
ABSTRACT,0.019569471624266144,"mation. Moreover, the multiple P-hop message passing strategy is proposed to
10"
ABSTRACT,0.021526418786692758,"efficiently perceive the higher-order neighborhoods. Different from other GNNs,
11"
ABSTRACT,0.023483365949119372,"the proposed APGNN can be seamlessly extended to an infinite-depth network. To
12"
ABSTRACT,0.025440313111545987,"clarify the learning guarantee, we theoretically analyze the generalization of the
13"
ABSTRACT,0.0273972602739726,"proposed learning framework via uniform convergence. Experimental results show
14"
ABSTRACT,0.029354207436399216,"that APGNN obtains superior performance compared to state-of-the-art GNNs,
15"
ABSTRACT,0.03131115459882583,"highlighting the effectiveness of our framework.
16"
INTRODUCTION,0.033268101761252444,"1
Introduction
17"
INTRODUCTION,0.03522504892367906,"Recently, Graph Neural Networks (GNNs) have shown commendable performance on numerous graph
18"
INTRODUCTION,0.03718199608610567,"representation learning tasks. In addition, GNNs have been introduced in a variety of application tasks,
19"
INTRODUCTION,0.03913894324853229,"such as recommendation systems [7, 11, 37], computer vision [4, 13, 23], and traffic forecasting
20"
INTRODUCTION,0.0410958904109589,"[8, 9]. The fundamental part of GNN is the design of the propagation mechanism or the graph
21"
INTRODUCTION,0.043052837573385516,"filter [5, 12, 27, 32, 34]. GNNs can be categorized into two groups based on the approach of
22"
INTRODUCTION,0.04500978473581213,"formulation. Spatial-based GNN formulates propagation mechanisms through the direct aggregation
23"
INTRODUCTION,0.046966731898238745,"of spatial features. As one of the most simple GNNs, Graph Convolutional Network (GCN) [15]
24"
INTRODUCTION,0.04892367906066536,"designs graph convolutional layer via aggregating one-hop information on the graph. Graph Attention
25"
INTRODUCTION,0.050880626223091974,"Network (GAT) [30] learns node relationships using an attention mechanism, enhancing the scalability
26"
INTRODUCTION,0.05283757338551859,"of the network. For extension of inductive learning, GraphSAGE [10] employs various pooling
27"
INTRODUCTION,0.0547945205479452,"operations as aggregation functions. Liu et. al proposed DAGNN, which integrates information from
28"
INTRODUCTION,0.05675146771037182,"multiple receptive fields for adaptive propagation [19]. Spectral-based GNN designs graph filters by
29"
INTRODUCTION,0.05870841487279843,"constructing filter functions in the graph Fourier domain, which aims to find a proper transformation
30"
INTRODUCTION,0.060665362035225046,"of the graph spectrum. Chebynet constructs the localized graph filter with Chebyshev polynomial [3].
31"
INTRODUCTION,0.06262230919765166,"From the view of the spectrum, GCN could be seen as a Chebyshev filter with first-order truncation
32"
INTRODUCTION,0.06457925636007827,"[15]. To construct deeper GNN, Personalize PageRank method is employed to design graph filter
33"
INTRODUCTION,0.06653620352250489,"[16]. GNN-LF/HF [36] concludes various designs of graph filters and constructs the graph filter
34"
INTRODUCTION,0.0684931506849315,"through a graph optimization framework.
35"
INTRODUCTION,0.07045009784735812,"Despite their success, few studies have explored the general rule for devising GNNs from the
36"
INTRODUCTION,0.07240704500978473,"perspective of learning. In this paper, we start from the graph filter formed by power series and
37 MLP"
INTRODUCTION,0.07436399217221135,"+
+
Convolution"
INTRODUCTION,0.07632093933463796,Classifier
INTRODUCTION,0.07827788649706457,"Observed samples
Learning
Principle ,"
INTRODUCTION,0.08023483365949119,is Lipschitz. APGNN with
INTRODUCTION,0.0821917808219178,"Figure 1: An illustration of the proposed APGNN that adheres to the learning principle. The model
incorporates the decay rate α to suppress the information from high-order neighbors while adaptively
learning bounded coefficients β. Furthermore, it aggregates information with P-hop to enlarge the
receptive field. This design enables the seamless extension of APGNN to an extremely deep network."
INTRODUCTION,0.08414872798434442,"discuss what makes a legitimate graph filter for the construction of deep GNN. A learning principle
38"
INTRODUCTION,0.08610567514677103,"is then proposed to summarize the rule of formulating a graph filter. Following this, we propose
39"
INTRODUCTION,0.08806262230919765,"Adaptive Power Graph Neural Network (APGNN), which adaptively learns the task-specific graph
40"
INTRODUCTION,0.09001956947162426,"filter for node representation learning. The main idea of APGNN is depicted in Figure 1. The
41"
INTRODUCTION,0.09197651663405088,"parameterized graph filter is designed with regularization of the exponential decay rate. A multiple
42"
INTRODUCTION,0.09393346379647749,"P-hop strategy is applied to enhance the capacity of perceiving the higher-order neighborhoods.
43"
INTRODUCTION,0.0958904109589041,"Furthermore, the generalization bound of APGNN is presented with the setting of the continuous
44"
INTRODUCTION,0.09784735812133072,"graph, which provides a learning guarantee for the proposed principle theoretically.
45"
INTRODUCTION,0.09980430528375733,"We conduct evaluations on five benchmark datasets on node classification tasks. The experimental
46"
INTRODUCTION,0.10176125244618395,"results suggest the superiority of the proposed method over the existing GNNs. The theoretical
47"
INTRODUCTION,0.10371819960861056,"analysis is also validated via the empirical study.
48"
PRELIMINARIES,0.10567514677103718,"2
Preliminaries
49"
PRELIMINARIES,0.10763209393346379,"Notations. Suppose we have an undirected graph G = (V, E, A) with node set V and |V| = n.
50"
PRELIMINARIES,0.1095890410958904,"A ∈Rn×n denotes the adjacency matrix indicating the edges in E. Assuming that the self-loops are
51"
PRELIMINARIES,0.11154598825831702,"contained in the graph, i.e., aii = 1. Let X = [x1, x2, · · · , xn]⊤∈Rn×d be the graph signals (or
52"
PRELIMINARIES,0.11350293542074363,"features) of the nodes. We use notation [n] ≜{1, 2, · · · , n} for n ∈N+. Assume that the label of xi
53"
PRELIMINARIES,0.11545988258317025,"is yi ∈Y for all i = [nl], where nl ≤n is the number of labeled samples.
54"
PRELIMINARIES,0.11741682974559686,"Graph Neural Networks. We introduce some essential concepts in GNNs. Let di = Pn
j=1 Aij
55"
PRELIMINARIES,0.11937377690802348,"be the degree of i-th node, so the degree matrix of A can be defined as D = diag(d1, d2, · · · , dn).
56"
PRELIMINARIES,0.12133072407045009,"The symmetrically normalized Laplacian is L = I −˜A, where ˜A ≜D−1/2AD−1/2 is normalized
57"
PRELIMINARIES,0.1232876712328767,"adjacency matrix. Consider the eigen-decomposition L = UΛU⊤, where Λ = diag(λ1, · · · , λn)
58"
PRELIMINARIES,0.12524461839530332,"is the diagonal matrix of eigenvalues, and U = [u1, · · · , un] represents the eigenvectors associated
59"
PRELIMINARIES,0.12720156555772993,"with the eigenvalues. Note that ˜A shares the same eigenvectors with L.
60"
PRELIMINARIES,0.12915851272015655,"Spectral convolution on graphs is defined as the following transformation [15, 28]:
61"
PRELIMINARIES,0.13111545988258316,"g ∗X = Ug(Λ)U⊤X,
(1)"
PRELIMINARIES,0.13307240704500978,"where g(·) : [0, 2] 7→R is called filter function and g(Λ) = diag(g(λ1), · · · , g(λn)). The com-
62"
PRELIMINARIES,0.1350293542074364,"mon approach in GNNs is to apply polynomial functions as the filters [3, 12, 15], which leads to
63"
PRELIMINARIES,0.136986301369863,"Ug(Λ)U⊤= g(L). Therefore, spectral convolution is usually written as g ∗X = g(L)X. The graph
64"
PRELIMINARIES,0.13894324853228962,"representation paradigm in GNN is generally expressed as follows:
65"
PRELIMINARIES,0.14090019569471623,"Z = g(L)f(X),
g(L) = K
X"
PRELIMINARIES,0.14285714285714285,"k=0
θk ˜Ak,
(2)"
PRELIMINARIES,0.14481409001956946,"where Z ∈Rn×c denotes the node representation, and f(·) represents a feature extractor such as
66"
PRELIMINARIES,0.14677103718199608,"multi-layer perceptions (MLPs).
67"
LEARNING PRINCIPLE FOR GNNS,0.1487279843444227,"3
Learning Principle for GNNs
68"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.1506849315068493,"3.1
The principle of devising graph filters
69"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.15264187866927592,"Current studies suggest a significant relationship between the performance of GNN and its graph filter
70"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.15459882583170254,"[16, 19]. Predominantly, the general graph filters are characterized by polynomials associated with
71"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.15655577299412915,"the adjacency matrix ˜A (or Laplacian matrix L), i.e., g(L) = PK
k=0 θk ˜Ak. However, the existing
72"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.15851272015655576,"methods still meet the issue that the depth of GNN is limited. The reason for this phenomenon
73"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.16046966731898238,"is that these GNNs are inconsistent with their ""infinite-depth"" version. That is, the corresponding
74"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.162426614481409,"models lose some essential properties as the depth K →∞. Consequently, the depth of the models
75"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.1643835616438356,"is restricted. To address this issue, it is necessary to study the properties of GNNs with infinite depth.
76"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.16634050880626222,"Therefore, we explore the graph filter reformulated as power series:
77"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.16829745596868884,"g( ˜A) = ∞
X"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.17025440313111545,"k=0
θk ˜Ak = ∞
X"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.17221135029354206,"k=0
θk(I −L)k.
(3)"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.17416829745596868,"First of all, a well-defined graph filter represented as equation 3 must be convergent. Consequently, it
78"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.1761252446183953,"becomes essential to investigate the properties that the coefficients θk should exhibit. The following
79"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.1780821917808219,"lemma provides appropriate constraints for the coefficients of the graph filter.
80"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.18003913894324852,"Lemma 1. Let {ak} and {γk} be the real number sequences, where γ ∈(−1, 1] and k ∈N. Then
81
P∞
k akγk converges uniformly and absolutely if and only if the series P∞
k ak converges absolutely.
82"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.18199608610567514,"As a direct corollary, the weights of the graph filter (i.e., θk) should satisfy the following theorem.
83"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.18395303326810175,"Theorem 1. Let ˜A = D−1/2AD−1/2 be the normalized adjacency matrix of a graph G with spectral
84"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.18590998043052837,"radius ρ( ˜A) ≤1. The matrix series P∞
k=0 θk ˜Ak converges uniformly and absolutely if and only if
85"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.18786692759295498,"the series P∞
k=0 θk converges absolutely.
86"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.1898238747553816,"The proofs are shown in Appendix. Theorem 1 offers a sufficient and necessary condition for the
87"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.1917808219178082,"convergence of graph filters formed by power series. Specifically, the condition requires the existence
88"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.19373776908023482,"of a finite real number M ≥0,
89"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.19569471624266144,"∥θ∥1 ≜ ∞
X"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.19765166340508805,"k=0
|θk| ≤M.
(4)"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.19960861056751467,"Therefore, an arbitrary graph filter formed by power series should satisfy the above convergence
90"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.20156555772994128,"condition, which gives the first requirement while designing GNN. Apart from convergence, we
91"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.2035225048923679,"expect the graph filter to possess good analytic properties such as smoothness. To this end, Lipschitz
92"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.2054794520547945,"continuity should be considered the second requirement of the graph filter. Let g(·) be an L-Lipschtiz
93"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.20743639921722112,"continuous function, meaning that
94"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.20939334637964774,"|g(λ) −g(λ′)| ≤L|λ −λ′|,
∀λ, λ′ ∈[0, 2).
(5)"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.21135029354207435,"This property indicates the stability or robustness of the model [6, 24]. If the graph is contaminated
95"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.21330724070450097,"and its eigenvalues are perturbed by at most ϵ, Lipschitz continuity ensures the perturbation of the
96"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.21526418786692758,"graph-filtered result is at most Lϵ. For instance, considering g(λ) = P∞
k=0(1 −λ)k/k2, which is
97"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.2172211350293542,"convergent, yet the Lipschitz condition does not satisfy for λ closed to zero. Therefore, this graph
98"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.2191780821917808,"filter might be sensitive to the input graph. Subsequently, we conclude the following criterion.
99"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.22113502935420742,"Z = gθ(L)f(X), with ∥θ∥1 ≤M, gθ(·) is a Lipschitz function.
(6)"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.22309197651663404,"To enhance the scalability of the model, we define θ as a learnable parameter (though its dimension
100"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.22504892367906065,"is infinite). In this way, (6) gives a regularized learning framework for GNN. Therefore, for a
101"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.22700587084148727,"K-order polynomial graph filter gK
θ (λ) = PK
k=0 θk(1 −λ)k, which is what we can implement in
102"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.22896281800391388,"practice, the condition (6) should be satisfied to keep the consistency with its infinitely deep version
103"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.2309197651663405,"g∞
θ (λ) = P∞
k=0 θk(1 −λ)k. We will present the applications of this criterion in this section, and
104"
THE PRINCIPLE OF DEVISING GRAPH FILTERS,0.2328767123287671,"further analyze the learning guarantee with generalization in section 4.
105"
RELATED WORKS,0.23483365949119372,"3.2
Related works
106"
RELATED WORKS,0.23679060665362034,"In this subsection, we investigate the relationship between our learning framework and several well-
107"
RELATED WORKS,0.23874755381604695,"known Graph Neural Networks (GNNs), focusing on the design of graph filters. Our findings indicate
108"
RELATED WORKS,0.24070450097847357,"that these GNNs are all special cases of our learning framework, which are summarized in Table 1.
109"
RELATED WORKS,0.24266144814090018,"GCN/SGC [15, 33]. Graph convolutional network (GCN) aims to learn a node representation by
110"
RELATED WORKS,0.2446183953033268,"stacking multiple graph convolutional layers. In each layer, GCN applies first-order Chebyshev
111"
RELATED WORKS,0.2465753424657534,"approximation as the graph filter followed by a fully connected layer. For simplicity, we analyze
112"
RELATED WORKS,0.24853228962818003,"one-layer GCN, which is formulated as Z = σ( ˜AXW), where W is a learnable weight matrix for
113"
RELATED WORKS,0.25048923679060664,"linear transformation. Therefore, the graph filter of one-layer GCN is gGCN(L) = I −L = ˜A, or a
114"
RELATED WORKS,0.25244618395303325,"trivial matrix power series:
115"
RELATED WORKS,0.25440313111545987,"gGCN(L) = ∞
X"
RELATED WORKS,0.2563600782778865,"k=0
θk ˜Ak,
where θk =
1,
if k = 1,
0,
otherwise.
(7)"
RELATED WORKS,0.2583170254403131,"It should be noted that this equation satisfies the condition described in (6).
116"
RELATED WORKS,0.2602739726027397,"SGC is a simplified version of GCN that eliminates the activation function and applies a single linear
117"
RELATED WORKS,0.2622309197651663,"projection to extract features. This simplification reduces the multiple-layer GCN into a more concise
118"
RELATED WORKS,0.26418786692759294,"model as Z = ˜AKXW. Similarly, the graph filter of SGC can be represented as:
119"
RELATED WORKS,0.26614481409001955,"gSGC(L) = ˜AK = ∞
X"
RELATED WORKS,0.26810176125244617,"k=0
θk ˜Ak,
where θk =
1,
if k = K,
0,
otherwise.
(8)"
RELATED WORKS,0.2700587084148728,"Both GCN and SGC use a monomial to construct the graph filter. Therefore, in the viewpoint of
120"
RELATED WORKS,0.2720156555772994,"spectral-GNN, their graph filters are too simple to capture the spectral characteristic. Besides, the
121"
RELATED WORKS,0.273972602739726,"small eigenvalue vanishes when K becomes very large, leaving only the largest eigenvalue, which
122"
RELATED WORKS,0.2759295499021526,"leads to the well-known over-smoothing problem [17].
123"
RELATED WORKS,0.27788649706457924,"PPNP [16]. PPNP uses Personalized PageRank as the graph filter, which balances local information
124"
RELATED WORKS,0.27984344422700586,"preservation and the utilization of high-order neighbor information. The model of PPNP is Z =
125"
RELATED WORKS,0.28180039138943247,"α(I −(1 −α) ˜A)−1H = (I + βL)−1H, where H = f(X) is a two-layer MLPs and β = 1/α −1.
126"
RELATED WORKS,0.2837573385518591,"Hence, the graph filter of PPNP is gPPNP(L) = (I + βL)−1. Considering its Taylor series, we have
127"
RELATED WORKS,0.2857142857142857,"gPPNP(L) = (I + βL)−1 =
1
1 + β ∞
X k=0"
RELATED WORKS,0.2876712328767123,"
β
1 + β"
RELATED WORKS,0.2896281800391389,"k
˜Ak = ∞
X"
RELATED WORKS,0.29158512720156554,"k=0
θk ˜A,
(9)"
RELATED WORKS,0.29354207436399216,"where θk = βk/(1 + β)k+1. It is straightforward to validate that P∞
k=0 θk = 1, and thus the
128"
RELATED WORKS,0.29549902152641877,"convergence requirement (4) holds. Moreover, the Lipschitz condition is easily verified. Thus
129"
RELATED WORKS,0.2974559686888454,"PPNP satisfies the criterion of (6). However, the performance of PPNP is heavily dependent on the
130"
RELATED WORKS,0.299412915851272,"hyperparameter β, which must be carefully tuned to achieve optimal performance.
131"
RELATED WORKS,0.3013698630136986,"DAGNN [19]. DAGNN adaptively adjusts the weight of information aggregation from different
132"
RELATED WORKS,0.30332681017612523,"neighbors to solve the over-smoothing problem. It designs a parameterized graph filter formulated as
133"
RELATED WORKS,0.30528375733855184,"a K-order polynomial:
134"
RELATED WORKS,0.30724070450097846,"gDAGNN(L) = K
X"
RELATED WORKS,0.30919765166340507,"k=0
θk ˜Ak,
s.t. 0 ≤θk ≤1,
(10)"
RELATED WORKS,0.3111545988258317,"where θk is the learnable parameter with bounded constraint. Due to this adaptive learning strategy,
135"
RELATED WORKS,0.3131115459882583,"DAGGN is able to learn a graph filter more suitable for node classification. The empirical studies
136"
RELATED WORKS,0.3150684931506849,"suggest DAGNN works well with a proper K. However, as K →∞, the constraint 0 ≤θk ≤1
137"
RELATED WORKS,0.31702544031311153,"cannot guarantee the convergence of the graph filter. It indicates that DAGNN is “inconsistent” with
138"
RELATED WORKS,0.31898238747553814,"its infinitely deep version. Therefore, it can not be naturally extended to significantly deep GNN.
139"
RELATED WORKS,0.32093933463796476,"3.3
Instantiation: Adaptive Power Graph Neural Network
140"
RELATED WORKS,0.32289628180039137,"We now introduce a novel GNN following the framework in section 3.1, called Adaptive Power GNN
141"
RELATED WORKS,0.324853228962818,"(APGNN). We first consider the following graph filter parameterized by β with the form:
142"
RELATED WORKS,0.3268101761252446,"g∞
β (λ) = ∞
X"
RELATED WORKS,0.3287671232876712,"k=0
βkαk(1 −λ)k,
where |βk| ≤1, 0 < α < 1,
(11)"
RELATED WORKS,0.33072407045009783,"where the coefficient of the power series θk = βkαk, with hyper-parameter α ∈(0, 1) ensuring the
143"
RELATED WORKS,0.33268101761252444,"convergence. Immediately, we check the condition of Lemma 1.
144"
RELATED WORKS,0.33463796477495106,"∥θ∥1 = ∞
X k=0"
RELATED WORKS,0.33659491193737767,"βkαk ≤ ∞
X"
RELATED WORKS,0.3385518590998043,"k=0
αk ≤
1
1 −α.
(12)"
RELATED WORKS,0.3405088062622309,Table 1: Graph filter for various GNNs
RELATED WORKS,0.3424657534246575,"Model
Filter function
Setting of θ
Learnable g(·)"
-LAYER GCN,0.34442270058708413,"1-layer GCN
g(L) = ∞
X"
-LAYER GCN,0.34637964774951074,"k=0
θk ˜Ak
θk =
1,
if k = 1
0,
otherwise
No"
-LAYER GCN,0.34833659491193736,"SGC
g(L) = ∞
X"
-LAYER GCN,0.350293542074364,"k=0
θk ˜Ak
θk =
1,
if k = K
0,
otherwise
No"
-LAYER GCN,0.3522504892367906,"PPNP
g(L) = ∞
X"
-LAYER GCN,0.3542074363992172,"k=0
θk ˜Ak
θk =
βk"
-LAYER GCN,0.3561643835616438,"(1 + β)k+1 , β > 0
No"
-LAYER GCN,0.35812133072407043,"DAGNN
g(L) = K
X"
-LAYER GCN,0.36007827788649704,"k=0
θk ˜Ak
0 ≤θk ≤1
Yes"
-LAYER GCN,0.36203522504892366,"Hence, the power series converges on [0, 2] absolutely and uniformly. Similarly, the associated matrix
145"
-LAYER GCN,0.3639921722113503,"series g∞
β (L) = P∞
k=0 βkαk ˜Ak also converges uniformly and absolutely by Theorem 1. Moreover,
146"
-LAYER GCN,0.3659491193737769,"g∞
β (·) is α(1 −α)−2-Lipschitz. To see this, for any |βk| ≤1 and 1 −λ ∈(−1, 1], we have
147"
-LAYER GCN,0.3679060665362035,"|∇g∞
β (λ)| =  ∞
X"
-LAYER GCN,0.3698630136986301,"k=1
(−1)kkβkαk(1 −λ)k−1
 ≤ ∞
X"
-LAYER GCN,0.37181996086105673,"k=1
kαk =
α
(1 −α)2 ,
(13)"
-LAYER GCN,0.37377690802348335,"which implies the Lipschitz continuous property. Thus, this graph filter fits the requirement of the
148"
-LAYER GCN,0.37573385518590996,"proposed criterion. However, the model with this graph filter is unavailable in practice as the number
149"
-LAYER GCN,0.3776908023483366,"of parameters to be learned is infinite. The K-order truncated polynomial is utilized for substitution,
150"
-LAYER GCN,0.3796477495107632,"i.e., gK
β (L) = PK
k=0 βkαk ˜Ak. We evaluate the approximation via the upper bound of K-order
151"
-LAYER GCN,0.3816046966731898,"truncation error:
152"
-LAYER GCN,0.3835616438356164,"|g∞
β (λ) −gK
β (λ)| ≤ ∞
X k=K+1"
-LAYER GCN,0.38551859099804303,"βkαk(1 −λ)k ≤ ∞
X"
-LAYER GCN,0.38747553816046965,"k=K+1
αk = αK+1"
-LAYER GCN,0.38943248532289626,"1 −α ,
(14)"
-LAYER GCN,0.3913894324853229,"which uniformly holds for ∀λ ∈[0, 2]. Likewise, the approximation error of matrix series is given by
153"
-LAYER GCN,0.3933463796477495,"g∞
β (L) −gK
β (L)

2 =
U
 
g∞
β (Λ) −gK
β (Λ)

U⊤
2 = sup
i∈[n]
|g∞
β (λi)−gK
β (λi)| ≤αK+1"
-LAYER GCN,0.3953033268101761,"1 −α , (15)"
-LAYER GCN,0.3972602739726027,"where λi denotes the i-th eigenvalue of L. This upper bound is independent of the given graph, which
154"
-LAYER GCN,0.39921722113502933,"can be controlled via tuning α and K. The higher K and smaller α yield a better approximation to
155"
-LAYER GCN,0.40117416829745595,"the exact graph filter g∞
β (·). Nevertheless, the small α tends to limit the capability of the graph filter.
156"
-LAYER GCN,0.40313111545988256,"Extremely, α →0 gives a trivial function gK
β (λ) = β0. This suggests that α should be elaborately
157"
-LAYER GCN,0.4050880626223092,"tuned to improve the performance.
158"
-LAYER GCN,0.4070450097847358,"Though the aforementioned graph filter is primarily motivated via spectral analysis, we can still
159"
-LAYER GCN,0.4090019569471624,"present the spatial perspective explanation for its design. Existing GNNs aggregate the neighbor
160"
-LAYER GCN,0.410958904109589,"information of different hops with certain weights, which could be either manually assigned or learned
161"
-LAYER GCN,0.41291585127201563,"adaptively. Typically, methods like GPR-GNN [2] and DAGNN [19] that learn the aggregation weight,
162"
-LAYER GCN,0.41487279843444225,"tend to treat the neighbor’s information of different hops equally. That is, the k-hop’s weight are
163"
-LAYER GCN,0.41682974559686886,"assigned with θk = O(1) for each k ∈[K]. However, it is shown in the previous research that the
164"
-LAYER GCN,0.4187866927592955,"propagation with the very high-order neighbor potentially leads to the over-smoothing issue [25, 33].
165"
-LAYER GCN,0.4207436399217221,"The current methods magnify this flaw of the high-order graph since they cannot distinguish the
166"
-LAYER GCN,0.4227005870841487,"significance of the information of different hops. This motivates the design of the decay rate in
167"
-LAYER GCN,0.4246575342465753,"APGNN, i.e., we employ weights with exponential decaying rate by assigning θk = O(αk) for some
168"
-LAYER GCN,0.42661448140900193,"0 < α < 1. This approach emphasizes the contribution of lower-order neighbors and restricts the
169"
-LAYER GCN,0.42857142857142855,"over-weighting of the information from high-order neighbors due to θk →0 with k →∞. Therefore,
170"
-LAYER GCN,0.43052837573385516,"it provides more effective aggregation and thus enhances the model’s scalability.
171"
-LAYER GCN,0.4324853228962818,"To take a further step in the construction of a deep GNN, we introduce a multiple P-hop strategy for
172"
-LAYER GCN,0.4344422700587084,"the graph filter of (11), which effectively extends the utmost neighborhood range that the graph filter
173"
-LAYER GCN,0.436399217221135,"can perceive by P times. Consider a different perspective regarding the construction of a filter with
174"
-LAYER GCN,0.4383561643835616,"the utmost order T = KP. The previous methods can be viewed as a one-hop graph filter by setting
175"
-LAYER GCN,0.44031311154598823,"P = 1. For P > 1, the graph filter is able to aggregate information from a larger neighborhood in
176"
-LAYER GCN,0.44227005870841485,"the same order. In addition, we will illustrate the advantages of this strategy from the perspective of
177"
-LAYER GCN,0.44422700587084146,"generalization in the following section.
178"
-LAYER GCN,0.4461839530332681,"Summarizing the above analysis, we present the following comprehensive architecture of APGNN:
179"
-LAYER GCN,0.4481409001956947,"Z = gβ(L)f(X),
f(X) = MLP(X),
gβ(L) = K
X"
-LAYER GCN,0.4500978473581213,"k=0
βkαk ˜AkP .
(16) 180"
-LAYER GCN,0.4520547945205479,"In short, APGNN incorporates the benefits from the decay rate α that exponentially suppresses the
181"
-LAYER GCN,0.45401174168297453,"information of extremely high-order neighbors and the multiple P-hop strategy to enlarge receptive
182"
-LAYER GCN,0.45596868884540115,"fields. These approaches make it possible to realize a sufficiently deep GNN.
183"
GENERALIZATION ANALYSIS,0.45792563600782776,"4
Generalization analysis
184"
GENERALIZATION ANALYSIS,0.4598825831702544,"The theoretical analysis of GNN’s generalization is widely studied. [31] provides the generalization
185"
GENERALIZATION ANALYSIS,0.461839530332681,"result of the algorithmic stability of GCN in the discrete graph setting. In contrast, [14] shows the
186"
GENERALIZATION ANALYSIS,0.4637964774951076,"convergence and stability guarantee over the random and continuous graph. In this section, we
187"
GENERALIZATION ANALYSIS,0.4657534246575342,"will present the uniform generalization bound of the proposed GNN learning framework under the
188"
GENERALIZATION ANALYSIS,0.46771037181996084,"continuous setup.
189"
GENERALIZATION ANALYSIS,0.46966731898238745,"We first introduce some notations for later discussion. Denote x ∈X as any samples from the
190"
GENERALIZATION ANALYSIS,0.47162426614481406,"input space X (we generally set X as a subset of Rd). Let ρ(·) be a probability measure defined
191"
GENERALIZATION ANALYSIS,0.4735812133072407,"over X. Assume xj is the j-th coordinate of x ∈X and E[x2
j] ≤c2
X for any j ∈[d]. To describe
192"
GENERALIZATION ANALYSIS,0.4755381604696673,"the graph relation between each pair (x, x′) over X × X, we define a continuous graph function
193"
GENERALIZATION ANALYSIS,0.4774951076320939,"A(·, ·) : X × X 7→R+, and its corresponding degree function is
194"
GENERALIZATION ANALYSIS,0.4794520547945205,"d(x′) =
Z"
GENERALIZATION ANALYSIS,0.48140900195694714,"X
A(x, x′)dρ(x′).
(17)"
GENERALIZATION ANALYSIS,0.48336594911937375,"Different from the setting of [18, 26], we assume 0 ≤A(x, x′) ≤cU, and 0 < cL ≤d(x) for any
195"
GENERALIZATION ANALYSIS,0.48532289628180036,"x, x′ ∈X. Therefore, we can define the symmetric normalized graph:
196"
GENERALIZATION ANALYSIS,0.487279843444227,"˜A(x, x′) =
A(x, x′)
p"
GENERALIZATION ANALYSIS,0.4892367906066536,"d(x)d(x′)
.
(18)"
GENERALIZATION ANALYSIS,0.4911937377690802,"Then the corresponding normalized Laplacian is L = I −˜A, where I indicates the identity operator
197"
GENERALIZATION ANALYSIS,0.4931506849315068,"over X. For a graph filter function gθ(λ) = PK
k=0 θk(1 −λ)k, graph convolution of the continuous
198"
GENERALIZATION ANALYSIS,0.49510763209393344,"graph is defined as the following integral operator:
199"
GENERALIZATION ANALYSIS,0.49706457925636005,"gθLf = K
X"
GENERALIZATION ANALYSIS,0.49902152641878667,"k=0
θk ˜Akf,
˜Af =
Z"
GENERALIZATION ANALYSIS,0.5009784735812133,"X
˜A(·, x)f(x)dρ(x),
(19)"
GENERALIZATION ANALYSIS,0.50293542074364,"where ˜Ak = ˜Ak−1 ◦˜A denotes k-order composition of integral operator with ˜A0 = I. Note we have
200
PK
k=0 θk∥˜A∥≤∥θ∥1 ≤M for any K ∈N, indicating P∞
k=0 θk ˜A is absolutely summable. This
201"
GENERALIZATION ANALYSIS,0.5048923679060665,"guarantees the existence of graph filter on the continuous graph when K →∞. For convenience in
202"
GENERALIZATION ANALYSIS,0.5068493150684932,"understanding, we provide the analysis on a simplified GNN, where we consider a semi-supervised
203"
GENERALIZATION ANALYSIS,0.5088062622309197,"learning task with two classes, i.e., yi ∈Y ≜{−1, 1}, and utilize linear feature extractor f(X) =
204"
GENERALIZATION ANALYSIS,0.5107632093933464,"w⊤X. Note that we can still extend our result for f(X) = MLP(X) and multi-class cases using the
205"
GENERALIZATION ANALYSIS,0.512720156555773,"techniques proposed in [1]. With the above setting, the hypothesis set over is described as
206"
GENERALIZATION ANALYSIS,0.5146771037181996,"HX = {h : h(x) = gθLf(x), f(x) = ⟨w, x⟩, ∥w∥2 ≤B, ∥θ∥1 ≤M}.
(20)
However, the integral in each hypothesis h ∈HX is intractable since the underlying graph function
207"
GENERALIZATION ANALYSIS,0.5166340508806262,"and the data distribution are unknown. Therefore, we should use the “empirical version” of the
208"
GENERALIZATION ANALYSIS,0.5185909980430529,"hypothesis to estimate h ∈HX . For this reason, we introduce the hypothesis set defined over the
209"
GENERALIZATION ANALYSIS,0.5205479452054794,"observed samples S and graph G:
210 HS = 
"
GENERALIZATION ANALYSIS,0.5225048923679061,"h : h(xi) = n
X"
GENERALIZATION ANALYSIS,0.5244618395303327,"j=1
gθ(L)ijx⊤
j w,
∥w∥2 ≤B, ∥θ∥1 ≤M 
"
GENERALIZATION ANALYSIS,0.5264187866927593,".
(21)"
GENERALIZATION ANALYSIS,0.5283757338551859,"Define the generalization error and the empirical error [21] as follows
211"
GENERALIZATION ANALYSIS,0.5303326810176126,"R(h) = E(x,y)[1yh(x)≤0],
ˆR(h) = 1 nl nl
X"
GENERALIZATION ANALYSIS,0.5322896281800391,"i=1
min(1, max(0, 1 −yih(xi))).
(22)"
GENERALIZATION ANALYSIS,0.5342465753424658,"We have the following theorem on the generalization of the proposed learning paradigm.
212"
GENERALIZATION ANALYSIS,0.5362035225048923,"Theorem 2. Suppose gθ(·) is LM-Lipschitz. Let hw,θ ∈HX and hw,θ ∈HS share the same
213"
GENERALIZATION ANALYSIS,0.538160469667319,"parameter (w, θ). Then there exists a constant C > 0 related to the graph function, with the
214"
GENERALIZATION ANALYSIS,0.5401174168297456,"probability at least 1 −δ, the following inequality holds.
215"
GENERALIZATION ANALYSIS,0.5420743639921722,"R(hw,θ) ≲ˆR(ˆhw,θ) + 2BMcX s"
GENERALIZATION ANALYSIS,0.5440313111545988,2d log(2K + 2)
GENERALIZATION ANALYSIS,0.5459882583170255,"nl
+ BCLMdcX r"
GENERALIZATION ANALYSIS,0.547945205479452,log(2/δ)
GENERALIZATION ANALYSIS,0.5499021526418787,"n
.
(23)"
GENERALIZATION ANALYSIS,0.5518590998043053,"The proof is given by excess risk decomposition, shown in Appendix. The notation ""≲"" denotes
216"
GENERALIZATION ANALYSIS,0.5538160469667319,"""less than or approximately equal to the right-hand side"" and guarantees an approximation error of
217"
GENERALIZATION ANALYSIS,0.5557729941291585,"at most O(
q"
GENERALIZATION ANALYSIS,0.5577299412915852,log(1/τ)
GENERALIZATION ANALYSIS,0.5596868884540117,"nl
) with a probability of at least 1 −O(τ). We remind readers the important
218"
GENERALIZATION ANALYSIS,0.5616438356164384,"difference between R(hw,θ) and ˆR(ˆhw,θ). The former term measures the population error over the
219"
GENERALIZATION ANALYSIS,0.5636007827788649,"whole input space with the continuous graph filter gθL. In contrast, ˆR(ˆhw,θ) is the empirical risk
220"
GENERALIZATION ANALYSIS,0.5655577299412916,"(i.e., training risk) on the sample set S with the discrete graph filter gθ(L). hw,θ shares the same
221"
GENERALIZATION ANALYSIS,0.5675146771037182,"learning parameter with ˆhw,θ. Therefore, the minimization of the right-hand-side of (23) w.r.t (w, θ)
222"
GENERALIZATION ANALYSIS,0.5694716242661448,"reduces the upper bound of the population error.
223"
GENERALIZATION ANALYSIS,0.5714285714285714,"We observe the first term of generalization bound is of order O((dn−1
l
log K)1/2), which outlines the
224"
GENERALIZATION ANALYSIS,0.5733855185909981,"model’s complexity. Although it becomes infinity when K →∞, the growth of this term is extremely
225"
GENERALIZATION ANALYSIS,0.5753424657534246,"slow as K increases. In practice, we generally set K < n since the neighbor information beyond
226"
GENERALIZATION ANALYSIS,0.5772994129158513,"n-hops is redundant, restricting the complexity away from infinity. Therefore, the generalization of
227"
GENERALIZATION ANALYSIS,0.5792563600782779,"the model is rigorously guaranteed for sufficiently large K, which allows us to construct significantly
228"
GENERALIZATION ANALYSIS,0.5812133072407045,"deep GNN in the proposed framework. We can obtain a more precise estimation for a certain model.
229"
GENERALIZATION ANALYSIS,0.5831702544031311,"In the following proposition, we unveil the generalization of APGNN as a direct application of
230"
GENERALIZATION ANALYSIS,0.5851272015655578,"Theorem 2.
231"
GENERALIZATION ANALYSIS,0.5870841487279843,"Proposition 1. Let β ∈RK and gK
β (λ) = PK
k=0 βkαk(1 −λ)k where 0 < α < 1 and ∥β∥∞≤1.
232"
GENERALIZATION ANALYSIS,0.589041095890411,"with the probability at least 1 −δ, the following inequality holds.
233"
GENERALIZATION ANALYSIS,0.5909980430528375,"R(hw,β) ≲ˆR(ˆhw,β) + 2BcX (1 −αK) 1 −α s"
GENERALIZATION ANALYSIS,0.5929549902152642,2d log(2K + 2)
GENERALIZATION ANALYSIS,0.5949119373776908,"nl
+ BCdcX α"
GENERALIZATION ANALYSIS,0.5968688845401174,(1 −α)2 r
GENERALIZATION ANALYSIS,0.598825831702544,log(2/δ)
GENERALIZATION ANALYSIS,0.6007827788649707,"n
.
(24)"
GENERALIZATION ANALYSIS,0.6027397260273972,"Proof. This is a direct result with M = (1−αK)/(1−α) and LM = α/(1−α)2 in Theorem 2.
234"
GENERALIZATION ANALYSIS,0.6046966731898239,"In (24), the complexity term becomes O(√log K(1 −αK)) with K = ⌊T/P⌋, which is relatively
235"
GENERALIZATION ANALYSIS,0.6066536203522505,"tighter than O(√log K). For this term, we promote further discussion with P-hop. Since it takes
236"
GENERALIZATION ANALYSIS,0.6086105675146771,"⌊T/P⌋steps to reach the T-order graph, the term becomes O(
p"
GENERALIZATION ANALYSIS,0.6105675146771037,"log ⌊T/P⌋(1 −α⌊T/P ⌋)). It is
237"
GENERALIZATION ANALYSIS,0.6125244618395304,"observed that the term decreases as P increases. Therefore, the appropriate P reduces the bound,
238"
GENERALIZATION ANALYSIS,0.6144814090019569,"explaining the mechanism of the P-hop method. On the other hand, larger α leads to a higher bound.
239"
GENERALIZATION ANALYSIS,0.6164383561643836,"From the point of spatial view, the information from high-order neighbors is underused, which limits
240"
GENERALIZATION ANALYSIS,0.6183953033268101,"the range of the graph filter. Thus α should be moderate to leverage the generalization and the
241"
GENERALIZATION ANALYSIS,0.6203522504892368,"capability of the model.
242"
EXPERIMENT,0.6223091976516634,"5
Experiment
243"
EXPERIMENT,0.62426614481409,"In this section, we conduct node classification experiments on various benchmark datasets to evaluate
244"
EXPERIMENT,0.6262230919765166,"the performance of APGNN. Specifically, we compare our method with state-of-the-art methods
245"
EXPERIMENT,0.6281800391389433,"and display the corresponding learned graph filter on different data sets. Moreover, to validate the
246"
EXPERIMENT,0.6301369863013698,"theoretical analysis, the influence of parameters K, α, and P is also investigated in experiments.
247"
EXPERIMENT,0.6320939334637965,"Table 2: The average accuracy (%) and standard deviation (%) on five benchmark datasets. The
highest accuracy in each column is shown in bold, while the second-best result is underlined."
EXPERIMENT,0.6340508806262231,"Dataset
Model
Cora
Citeseer
Pubmed
Wiki-CS
MS-Academic
MLP
57.79±0.11
61.20±0.08
73.23±0.05
65.66±0.20
87.79±0.42
ChebNet
79.92±0.18
70.90±0.37
76.98±0.16
63.24±1.43
90.76±0.73
GCN
82.03±0.27
71.05±0.33
79.26±0.18
72.05±0.45
92.07±0.13
SGC
81.89±0.26
72.18±0.24
78.58±0.15
72.76±0.35
89.01±0.40
GAT
82.82±0.36
71.96±0.39
79.15±0.34
74.36±0.58
91.86±0.27
GraphSage
82.14±0.25
71.80±0.36
79.20±0.27
73.17±0.41
91.53±0.15
PPNP
83.73±0.31
71.74±0.44
80.28±0.22
74.69±0.53
92.58±0.06
APPNP
83.73±0.21
71.70±0.21
80.07±0.21
74.91±0.61
92.81±0.12
GNN-LF(iter)
83.83±0.36
71.44±0.42
80.31±0.16
75.19±0.49
92.78±0.22
GNN-HF(iter)
83.68±0.31
71.58±0.36
79.99±0.22
74.71±0.55
92.72±0.31
DAGNN
82.70±0.17
71.90±0.06
80.06±0.30
75.63±0.48
93.24±0.21
Ours
84.15±0.23
72.44±0.56
80.74±0.24
76.03±0.51
93.69±0.20"
EXPERIMENT SETUP,0.6360078277886497,"5.1
Experiment Setup
248"
EXPERIMENT SETUP,0.6379647749510763,"Datasets. We perform experiments on five benchmark datasets commonly used in node classification
249"
EXPERIMENT SETUP,0.639921722113503,"tasks. 1). Cora, Citeseer, Pubmed[29, 35]: These are three standard citation networks where
250"
EXPERIMENT SETUP,0.6418786692759295,"each node is a paper and each edge is a citation link. 2).Wiki-CS[20]: This dataset defines the
251"
EXPERIMENT SETUP,0.6438356164383562,"computer science articles as nodes, while the hyperlinks are edges. 3). MS Acadamic[16]: The
252"
EXPERIMENT SETUP,0.6457925636007827,"nodes represent the author and the edges represent the co-authorships. A co-authorship Microsoft
253"
EXPERIMENT SETUP,0.6477495107632094,"Academic Graph, where the nodes are the bag-of-words representation of the papers’ abstract and
254"
EXPERIMENT SETUP,0.649706457925636,"edges are co-authorship. The data statistics and their partitions are presented in Appendix.
255"
EXPERIMENT SETUP,0.6516634050880626,"0.0
0.3
0.6
0.9
1.1
1.4
1.7
2.0 0.1 1.4 2.6 3.9 5.2 6.4 g( )"
EXPERIMENT SETUP,0.6536203522504892,"Cora
Citeseer
Pubmed
Wiki
MS (a)"
EXPERIMENT SETUP,0.6555772994129159,"0.0
0.3
0.6
0.9
1.1
1.4
1.7
2.0 0.9 1.6 2.4 3.1 3.8 4.6 g( )"
EXPERIMENT SETUP,0.6575342465753424,"Cora
Citeseer
Pubmed
Wiki
MS (b)"
EXPERIMENT SETUP,0.6594911937377691,"Figure 2: The graph filters learned on different data sets, with the parameter P being odd in subfigure
(a) and even in subfigure (b)."
EXPERIMENT SETUP,0.6614481409001957,"Baselines. To evaluate the effectiveness of APGNN, we compare it with the following baseline
256"
EXPERIMENT SETUP,0.6634050880626223,"models: 1) MLP [22], a traditional method that does not use graphs, 2) GAT [30] and GraphSAGE
257"
EXPERIMENT SETUP,0.6653620352250489,"[10], spatial methods that aggregate neighborhoods’ information, and 3) ChebNet [3], GCN [15],
258"
EXPERIMENT SETUP,0.6673189823874756,"SGC [33], PPNP, APPNP[16], GNN-LF (iteration form), GNN-HF (iteration form) [36], and DAGNN
259"
EXPERIMENT SETUP,0.6692759295499021,"[19], spectral methods analyzing GNNs with graph Fourier transform.
260"
EXPERIMENT SETUP,0.6712328767123288,"Settings. We conducted 10 runs for each method on each dataset, with a hidden dimension of 64. For
261"
EXPERIMENT SETUP,0.6731898238747553,"all compared methods, their parameter settings follow the previous practices [19, 36]: the dropout
262"
EXPERIMENT SETUP,0.675146771037182,"rate is 0.5 except for Cora, which had a rate of 0.8. Furthermore, the learning rate is 0.01 for Cora,
263"
EXPERIMENT SETUP,0.6771037181996086,"Citeseer, and Pubmed, but 0.03 for Wiki-CS and 0.02 for MS-Academic, while the weight decay is
264"
EXPERIMENT SETUP,0.6790606653620352,"0.005 for Cora and Pubmed, 0.02 for Citeseer, 0.0005 for Wiki-CS, and 0.00525 for MS-Academic.
265"
EXPERIMENT SETUP,0.6810176125244618,"We fix the polynomial order K to 10 in ChebNet, APPNP, GNN-LF, GNN-HF, DAGNN, and APPNP.
266"
EXPERIMENT SETUP,0.6829745596868885,"The best hyperparameters we choose for APGNN are presented in Appendix. To ensure a fair
267"
EXPERIMENT SETUP,0.684931506849315,"comparison with the compared methods, we also applied our optimal hyperparameters to them,
268"
EXPERIMENT SETUP,0.6868884540117417,"selecting the maximum value to display.
269"
EXPERIMENT SETUP,0.6888454011741683,"0
2
4
6
8
10
12
14
16
18
20
Parameter K 60 65 70 75 80 85"
EXPERIMENT SETUP,0.6908023483365949,Accuracy (%)
EXPERIMENT SETUP,0.6927592954990215,"Cora ACC
Citeseer ACC
Pubmed ACC (a)"
EXPERIMENT SETUP,0.6947162426614482,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9 0.99
Parameter 60 65 70 75 80 85"
EXPERIMENT SETUP,0.6966731898238747,Accuracy (%)
EXPERIMENT SETUP,0.6986301369863014,Max: 83.98%
EXPERIMENT SETUP,0.700587084148728,Max: 72.90%
EXPERIMENT SETUP,0.7025440313111546,Max: 80.40%
EXPERIMENT SETUP,0.7045009784735812,"Cora ACC
Citeseer ACC
Pubmed ACC (b)"
EXPERIMENT SETUP,0.7064579256360078,"1
2
3
4
5
6
Parameter P (fixing T) 70 72 74 76 78 80 82 84"
EXPERIMENT SETUP,0.7084148727984344,Accuracy (%)
EXPERIMENT SETUP,0.7103718199608611,Min: 82.75%
EXPERIMENT SETUP,0.7123287671232876,Min: 70.78%
EXPERIMENT SETUP,0.7142857142857143,"Min: 80.03%
Cora ACC
Citeseer ACC
Pubmed ACC (c)"
EXPERIMENT SETUP,0.7162426614481409,Figure 3: Accuracy with different (a) K. (b) α. (c) P (for fixing T).
ANALYSIS,0.7181996086105675,"5.2
Analysis
270"
ANALYSIS,0.7201565557729941,"Node Classification. As the metric for evaluation, the mean accuracy of 10 runs is used. We compare
271"
ANALYSIS,0.7221135029354208,"the performance of APGNN with other methods on five benchmark datasets. Experiment results
272"
ANALYSIS,0.7240704500978473,"are reported in Table 2. We can observe that APGNN achieves the highest accuracy across all five
273"
ANALYSIS,0.726027397260274,"datasets, demonstrating its superior performance.
274"
ANALYSIS,0.7279843444227005,"Learnable Graph Filters. Figure 2 shows the graph filters learned on various datasets via APGNN.
275"
ANALYSIS,0.7299412915851272,"When the parity of P varies, the graph filter has a distinctive shape. However, their shapes exhibit
276"
ANALYSIS,0.7318982387475538,"minimal impact on their accuracy regardless of the parity of P according to the experiment results.
277"
ANALYSIS,0.7338551859099804,"Moreover, the graph filters of each dataset are plotted in Appendix, more details are included in
278"
ANALYSIS,0.735812133072407,"Appendix. Our results show that the graph filters learned from different datasets vary in detail,
279"
ANALYSIS,0.7377690802348337,"even when their parameters have similar parity, demonstrating the efficacy of APGNN in learning
280"
ANALYSIS,0.7397260273972602,"task-specific graph filters.
281"
ANALYSIS,0.7416829745596869,"Polynomial Order K. To gain insight into the role of polynomial order K, we conduct the experiment
282"
ANALYSIS,0.7436399217221135,"tuning K in {1, 2, ..., 20} on Cora, Citeseer, and Pubmed dataset. Our theoretical analysis supports
283"
ANALYSIS,0.7455968688845401,"the observation that a small K can result in a large truncation error, leading to a low accuracy rate. It
284"
ANALYSIS,0.7475538160469667,"can be observed that the accuracy rate has little promotion when K is larger than 10, although at the
285"
ANALYSIS,0.7495107632093934,"cost of high computational resources.
286"
ANALYSIS,0.7514677103718199,"Decay Rate α. Figure 3 (b) depicts the accuracy curve corresponding to various α values ranging
287"
ANALYSIS,0.7534246575342466,"from 0.1 to 0.9 and 0.99 on Cora, Citeseer and Pubmed datasets. As α decreases, the classification
288"
ANALYSIS,0.7553816046966731,"accuracy initially increases and then declines sharply. This phenomenon verifies the theory that the
289"
ANALYSIS,0.7573385518590998,"truncation error decreases as α decreases, but it leads to a trivial function when α is extremely small.
290"
ANALYSIS,0.7592954990215264,"P -hop strategy. We investigate the accuracy associated with varying parameters P taken from
291"
ANALYSIS,0.761252446183953,"the set {1, 2, 3, 4, 5, 6} when fixing T = KP = 60. As we can see in Figure 3 (c), the accuracy
292"
ANALYSIS,0.7632093933463796,"increase when P > 1. This phenomenon can be attributed to the fact that the generalization bounding
293"
ANALYSIS,0.7651663405088063,"decreases when P increases, which suggests that the P-hop strategy can effectively explore deeper
294"
ANALYSIS,0.7671232876712328,"information with the same computational complexity.
295"
CONCLUSION,0.7690802348336595,"6
Conclusion
296"
CONCLUSION,0.7710371819960861,"This paper proposes a universal learning principle for a valid construction of GNN. An instantiation
297"
CONCLUSION,0.7729941291585127,"named APGNN is proposed to verify the effectiveness of our framework. APGNN employs a decay
298"
CONCLUSION,0.7749510763209393,"rate and a multiple P-hop strategy to learn the coefficients adaptively, which can efficiently aggregate
299"
CONCLUSION,0.776908023483366,"the information from high-order neighbors. We present a theoretical analysis of the generalization
300"
CONCLUSION,0.7788649706457925,"capabilities of both our framework and APGNN, which provides a learning guarantee. Comprehensive
301"
CONCLUSION,0.7808219178082192,"experiments show the superior performance of APGNN. In the future, it is worth exploring diverse
302"
CONCLUSION,0.7827788649706457,"graph filters based on the proposed principle. As shown in the generalization analysis, the upper
303"
CONCLUSION,0.7847358121330724,"bound of the model complexity relies on O(√log K). How to devise the GNN with complexity free
304"
CONCLUSION,0.786692759295499,"of the hyperparameter K is also a meaningful research direction.
305"
REFERENCES,0.7886497064579256,"References
306"
REFERENCES,0.7906066536203522,"[1] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds
307"
REFERENCES,0.7925636007827789,"for neural networks. In Proceedings of the Annual Conference on Neural Information Processing
308"
REFERENCES,0.7945205479452054,"Systems (NeurIPS), volume 30, page 6241–6250, 2017.
309"
REFERENCES,0.7964774951076321,"[2] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized
310"
REFERENCES,0.7984344422700587,"pagerank graph neural network. In Proceedings of the International Conference on Learning
311"
REFERENCES,0.8003913894324853,"Representations (ICLR), 2021.
312"
REFERENCES,0.8023483365949119,"[3] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
313"
REFERENCES,0.8043052837573386,"on graphs with fast localized spectral filtering. In Proceedings of the Annual Conference on
314"
REFERENCES,0.8062622309197651,"Neural Information Processing Systems (NeurIPS), volume 29, 2016.
315"
REFERENCES,0.8082191780821918,"[4] Leyan Deng, Defu Lian, Chenwang Wu, and Enhong Chen. Graph convolution network based
316"
REFERENCES,0.8101761252446184,"recommender systems: Learning guarantee and item mixture powered strategy. In Proceedings
317"
REFERENCES,0.812133072407045,"of the Annual Conference on Neural Information Processing Systems (NeurIPS), volume 35,
318"
REFERENCES,0.8140900195694716,"pages 3900–3912, 2022.
319"
REFERENCES,0.8160469667318982,"[5] Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. How powerful are k-hop
320"
REFERENCES,0.8180039138943248,"message passing graph neural networks. In Proceedings of the Annual Conference on Neural
321"
REFERENCES,0.8199608610567515,"Information Processing Systems (NeurIPS), volume 35, pages 4776–4790, 2022.
322"
REFERENCES,0.821917808219178,"[6] Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural
323"
REFERENCES,0.8238747553816047,"networks. IEEE Transactions on Signal Processing, 68:5680–5695, 2020.
324"
REFERENCES,0.8258317025440313,"[7] Francesco Giuliari, Geri Skenderi, Marco Cristani, Yiming Wang, and Alessio Del Bue. Spatial
325"
REFERENCES,0.8277886497064579,"commonsense graph for object localisation in partial scenes. In 2022 IEEE/CVF Conference on
326"
REFERENCES,0.8297455968688845,"Computer Vision and Pattern Recognition (CVPR), pages 19496–19505, 2022.
327"
REFERENCES,0.8317025440313112,"[8] Kan Guo, Yongli Hu, Yanfeng Sun, Sean Qian, Junbin Gao, and Baocai Yin. Hierarchical graph
328"
REFERENCES,0.8336594911937377,"convolution network for traffic forecasting. In Proceedings of the AAAI Conference on Artificial
329"
REFERENCES,0.8356164383561644,"Intelligence (AAAI), volume 35, pages 151–159, 2021.
330"
REFERENCES,0.837573385518591,"[9] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. Attention based spatial-
331"
REFERENCES,0.8395303326810176,"temporal graph convolutional networks for traffic flow forecasting. In Proceedings of the AAAI
332"
REFERENCES,0.8414872798434442,"Conference on Artificial Intelligence (AAAI), volume 33, pages 922–929, 2019.
333"
REFERENCES,0.8434442270058709,"[10] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
334"
REFERENCES,0.8454011741682974,"graphs. In Proceedings of the Annual Conference on Neural Information Processing Systems
335"
REFERENCES,0.8473581213307241,"(NeurIPS), volume 30, 2017.
336"
REFERENCES,0.8493150684931506,"[11] Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and Enhua Wu. Vision GNN: An image
337"
REFERENCES,0.8512720156555773,"is worth graph of nodes. In Proceedings of the Annual Conference on Neural Information
338"
REFERENCES,0.8532289628180039,"Processing Systems (NeurIPS), volume 35, pages 8291–8303, 2022.
339"
REFERENCES,0.8551859099804305,"[12] Mingguo He, Zhewei Wei, zengfeng Huang, and Hongteng Xu. Bernnet: Learning arbitrary
340"
REFERENCES,0.8571428571428571,"graph spectral filters via bernstein approximation. In Proceedings of the Annual Conference on
341"
REFERENCES,0.8590998043052838,"Neural Information Processing Systems (NeurIPS), volume 34, pages 14239–14251, 2021.
342"
REFERENCES,0.8610567514677103,"[13] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. LightGCN:
343"
REFERENCES,0.863013698630137,"Simplifying and powering graph convolution network for recommendation. In Proceedings
344"
REFERENCES,0.8649706457925636,"of the International ACM SIGIR Conference on Research and Development in Information
345"
REFERENCES,0.8669275929549902,"Retrieval (SIGIR), page 639–648, 2020.
346"
REFERENCES,0.8688845401174168,"[14] Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. Convergence and stability of graph convo-
347"
REFERENCES,0.8708414872798435,"lutional networks on large random graphs. In Proceedings of the Annual Conference on Neural
348"
REFERENCES,0.87279843444227,"Information Processing Systems (NeurIPS), volume 33, pages 21512–21523, 2020.
349"
REFERENCES,0.8747553816046967,"[15] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
350"
REFERENCES,0.8767123287671232,"networks. In Proceedings of the International Conference on Learning Representations (ICLR),
351"
REFERENCES,0.8786692759295499,"2017.
352"
REFERENCES,0.8806262230919765,"[16] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate:
353"
REFERENCES,0.8825831702544031,"Graph neural networks meet personalized pagerank. In Proceedings of the International
354"
REFERENCES,0.8845401174168297,"Conference on Learning Representations (ICLR), 2019.
355"
REFERENCES,0.8864970645792564,"[17] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
356"
REFERENCES,0.8884540117416829,"for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence
357"
REFERENCES,0.8904109589041096,"(AAAI), 2018.
358"
REFERENCES,0.8923679060665362,"[18] Shaojie Li, Sheng Ouyang, and Yong Liu. Understanding the generalization performance of
359"
REFERENCES,0.8943248532289628,"spectral clustering algorithms. arXiv preprint arXiv:2205.00281, 2022.
360"
REFERENCES,0.8962818003913894,"[19] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In
361"
REFERENCES,0.898238747553816,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data
362"
REFERENCES,0.9001956947162426,"Mining (SIGKDD), page 338–348, 2020.
363"
REFERENCES,0.9021526418786693,"[20] Péter Mernyei and C˘at˘alina Cangea. Wiki-CS: A wikipedia-based benchmark for graph neural
364"
REFERENCES,0.9041095890410958,"networks. arXiv preprint arXiv:2007.02901, 2020.
365"
REFERENCES,0.9060665362035225,"[21] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
366"
REFERENCES,0.9080234833659491,"MIT press, 2018.
367"
REFERENCES,0.9099804305283757,"[22] Sankar K Pal and Sushmita Mitra. Multilayer perceptron, fuzzy sets, and classification. IEEE
368"
REFERENCES,0.9119373776908023,"Transactions on neural networks, 3(5):683–697, 1992.
369"
REFERENCES,0.913894324853229,"[23] Yitong Pang, Lingfei Wu, Qi Shen, Yiming Zhang, Zhihua Wei, Fangli Xu, Ethan Chang,
370"
REFERENCES,0.9158512720156555,"Bo Long, and Jian Pei. Heterogeneous global graph neural networks for personalized session-
371"
REFERENCES,0.9178082191780822,"based recommendation. In Proceedings of the ACM International Conference on Web Search
372"
REFERENCES,0.9197651663405088,"and Data Mining (WSDM), page 775–783, 2022.
373"
REFERENCES,0.9217221135029354,"[24] Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgöwer. Training robust
374"
REFERENCES,0.923679060665362,"neural networks using lipschitz bounds. IEEE Control Systems Letters, 6:121–126, 2021.
375"
REFERENCES,0.9256360078277887,"[25] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
376"
REFERENCES,0.9275929549902152,"convolutional networks on node classification. In Proceedings of the International Conference
377"
REFERENCES,0.9295499021526419,"on Learning Representations (ICLR), 2020.
378"
REFERENCES,0.9315068493150684,"[26] Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators.
379"
REFERENCES,0.9334637964774951,"Journal of Machine Learning Research (JMLR), 11(2), 2010.
380"
REFERENCES,0.9354207436399217,"[27] Aliaksei Sandryhaila and José M. F. Moura. Discrete signal processing on graphs: Graph filters.
381"
REFERENCES,0.9373776908023483,"In Proceedings of the International Conference on Acoustics, Speech and Signal Processing
382"
REFERENCES,0.9393346379647749,"(ICASSP), pages 6163–6166, 2013.
383"
REFERENCES,0.9412915851272016,"[28] Aliaksei Sandryhaila and José M. F. Moura. Discrete signal processing on graphs: Graph fourier
384"
REFERENCES,0.9432485322896281,"transform. In Proceedings of the International Conference on Acoustics, Speech and Signal
385"
REFERENCES,0.9452054794520548,"Processing (ICASSP), pages 6167–6170, 2013.
386"
REFERENCES,0.9471624266144814,"[29] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-
387"
REFERENCES,0.949119373776908,"Rad. Collective classification in network data. AI Magazine, 29(3):93, 2008.
388"
REFERENCES,0.9510763209393346,"[30] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
389"
REFERENCES,0.9530332681017613,"Bengio. Graph attention networks. In Proceedings of the International Conference on Learning
390"
REFERENCES,0.9549902152641878,"Representations (ICLR), 2018.
391"
REFERENCES,0.9569471624266145,"[31] Saurabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolutional neural
392"
REFERENCES,0.958904109589041,"networks. In Proceedings of the ACM SIGKDD International Conference on Knowledge
393"
REFERENCES,0.9608610567514677,"Discovery and Data Mining (SIGKDD), pages 1539–1548, 2019.
394"
REFERENCES,0.9628180039138943,"[32] Xiyuan Wang and Muhan Zhang. How powerful are spectral graph neural networks. In
395"
REFERENCES,0.9647749510763209,"Proceedings of the International Conference on International Conference on Machine Learning
396"
REFERENCES,0.9667318982387475,"(ICML), volume 162, pages 23341–23362, 2022.
397"
REFERENCES,0.9686888454011742,"[33] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger.
398"
REFERENCES,0.9706457925636007,"Simplifying graph convolutional networks. In Proceedings of the International Conference on
399"
REFERENCES,0.9726027397260274,"International Conference on Machine Learning (ICML), 2019.
400"
REFERENCES,0.974559686888454,"[34] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
401"
REFERENCES,0.9765166340508806,"networks? In Proceedings of the International Conference on Learning Representations (ICLR),
402"
REFERENCES,0.9784735812133072,"2019.
403"
REFERENCES,0.9804305283757339,"[35] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
404"
REFERENCES,0.9823874755381604,"with graph embeddings. In Proceedings of the International Conference on International
405"
REFERENCES,0.9843444227005871,"Conference on Machine Learning (ICML), page 40–48, 2016.
406"
REFERENCES,0.9863013698630136,"[36] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph
407"
REFERENCES,0.9882583170254403,"neural networks with an optimization framework. In Proceedings of The International World
408"
REFERENCES,0.9902152641878669,"Wide Web Conference (WWW), page 1215–1226, 2021.
409"
REFERENCES,0.9921722113502935,"[37] Stefano Zorzi, Shabab Bazrafkan, Stefan Habenschuss, and Friedrich Fraundorfer. PolyWorld:
410"
REFERENCES,0.9941291585127201,"Polygonal building extraction with graph neural networks in satellite images. In Proceedings
411"
REFERENCES,0.9960861056751468,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
412"
REFERENCES,0.9980430528375733,"1848–1857, 2022.
413"
