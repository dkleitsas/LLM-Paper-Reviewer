Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008130081300813008,"Searching through chemical space is an exceptionally challenging problem
1"
ABSTRACT,0.0016260162601626016,"because the number of possible molecules grows combinatorially with the
2"
ABSTRACT,0.0024390243902439024,"number of atoms. Large, autoregressive models trained on databases of chem-
3"
ABSTRACT,0.0032520325203252032,"ical compounds have yielded powerful generators, but we still lack robust
4"
ABSTRACT,0.0040650406504065045,"strategies for generating molecules with desired properties. This molecular
5"
ABSTRACT,0.004878048780487805,"search problem closely resembles the “alignment” problem for large lan-
6"
ABSTRACT,0.005691056910569106,"guage models, though for many chemical tasks we have a specific and easily
7"
ABSTRACT,0.0065040650406504065,"evaluable reward function. Here, we introduce an algorithm called energy
8"
ABSTRACT,0.007317073170731708,"rank alignment (ERA) that leverages an explicit reward function to produce
9"
ABSTRACT,0.008130081300813009,"a gradient-based objective that we use to optimize autoregressive policies.
10"
ABSTRACT,0.00894308943089431,"We show theoretically that this algorithm is closely related to proximal pol-
11"
ABSTRACT,0.00975609756097561,"icy optimization (PPO) and direct preference optimization (DPO), but has a
12"
ABSTRACT,0.01056910569105691,"minimizer that converges to an ideal Gibbs-Boltzmann distribution with the
13"
ABSTRACT,0.011382113821138212,"reward playing the role of an energy function. Furthermore, this algorithm is
14"
ABSTRACT,0.012195121951219513,"highly scalable, does not require reinforcement learning, and performs well
15"
ABSTRACT,0.013008130081300813,"relative to DPO when the number of preference observations per pairing is
16"
ABSTRACT,0.013821138211382113,"small. We deploy this approach to align molecular transformers to generate
17"
ABSTRACT,0.014634146341463415,"molecules with externally specified properties and find that it does so robustly,
18"
ABSTRACT,0.015447154471544716,"searching through diverse parts of chemical space. While our focus here is on
19"
ABSTRACT,0.016260162601626018,"chemical search, we also obtain excellent results on an AI supervised task for
20"
ABSTRACT,0.01707317073170732,"LLM alignment, showing that the method is scalable and general.
21"
INTRODUCTION,0.01788617886178862,"1
Introduction
22"
INTRODUCTION,0.01869918699186992,"Large language models (LLMs) are trained on large corpora of text to autoregressively generate
23"
INTRODUCTION,0.01951219512195122,"outputs. These models strongly reflect the distribution of the data on which they are trained [21],
24"
INTRODUCTION,0.02032520325203252,"and controlling the outputs to reflect externally imposed preferences is an increasingly important
25"
INTRODUCTION,0.02113821138211382,"challenge for deployment. The aforementioned task, often called “alignment”, requires either careful
26"
INTRODUCTION,0.02195121951219512,"curation of training data or large sets of human preference data—both options are labor-intensive [9].
27"
INTRODUCTION,0.022764227642276424,"Reinforcement learning from human feedback (RLHF), a family of algorithms that employs these
28"
INTRODUCTION,0.023577235772357725,"human preference datasets, has been widely employed to align instruction and chat models [21, 5],
29"
INTRODUCTION,0.024390243902439025,"but it is both expensive to acquire the training data and difficult to carry out in practice [9]. Recent
30"
INTRODUCTION,0.025203252032520326,"algorithmic developments, such as direct preference optimization (DPO) [25], simplify the alignment
31"
INTRODUCTION,0.026016260162601626,"framework by making the reward function implicit, but still require human preference data. While
32"
INTRODUCTION,0.026829268292682926,"these algorithms succeed in constraining outputs, many “alignment”-like tasks require evaluation that
33"
INTRODUCTION,0.027642276422764227,"would be difficult for human evaluators.
34"
INTRODUCTION,0.028455284552845527,"Generative sampling problems seeking to optimize a reward are common in chemistry, where
35"
INTRODUCTION,0.02926829268292683,"comparing small molecules using a particular functional assay or computationally accessible property
36"
INTRODUCTION,0.03008130081300813,"is often far easier than searching chemical space to identify novel compounds. Recent efforts to build
37"
INTRODUCTION,0.030894308943089432,"large, domain-specific models for chemistry [10] have shown promising performance on both property
38"
INTRODUCTION,0.03170731707317073,"prediction and reaction prediction tasks. Nevertheless, just as with LLMs, leveraging these models
39"
INTRODUCTION,0.032520325203252036,"for molecule optimization requires first guiding “unaligned” models to favor important properties
40"
INTRODUCTION,0.03333333333333333,"like synthetic accessibility or solubility. Here, we seek to productively search chemical space using
41"
INTRODUCTION,0.03414634146341464,"transformers by introducing a new preference optimization algorithm, which we call energy rank
42"
INTRODUCTION,0.034959349593495934,"alignment.
43"
INTRODUCTION,0.03577235772357724,"Our contribution:
We formulate a generic alignment algorithm that we call Energy Rank Alignment
44"
INTRODUCTION,0.036585365853658534,"or ERA that leverages an explicit reward function to guide autoregressive sampling while targeting
45"
INTRODUCTION,0.03739837398373984,"specific properties or preferences. Unlike reward maximization in RL-based algorithms, the policy
46"
INTRODUCTION,0.038211382113821135,"that minimizes our objective is designed to sample fluctuations around a maximal reward value to
47"
INTRODUCTION,0.03902439024390244,"promote sample diversity. Our algorithm enables direct gradient-based optimization of a policy to
48"
INTRODUCTION,0.03983739837398374,"match the ideal preference distribution and converges asymptotically to an optimal distribution with
49"
INTRODUCTION,0.04065040650406504,"tuneable entropy and controllable regularization, which we show theoretically. The minimizers of our
50"
INTRODUCTION,0.041463414634146344,"objective are closely related to the minimizer of PPO and DPO, but we have more direct control over
51"
INTRODUCTION,0.04227642276422764,"the influence of the regularization relative to fluctuations around the maximum reward. In numerical
52"
INTRODUCTION,0.043089430894308944,"experiments, we demonstrate that this algorithm successfully aligns a molecule transformer model to
53"
INTRODUCTION,0.04390243902439024,"identify a highly diverse set of chemicals with properties favored by our choice of reward. Finally, we
54"
INTRODUCTION,0.044715447154471545,"also show that we obtain competitive performance with ERA on benchmark LLM alignment tasks,
55"
INTRODUCTION,0.04552845528455285,"but emphasize that the chemical applications are the main focus of this paper.
56"
INTRODUCTION,0.046341463414634146,"Target 
Property"
INTRODUCTION,0.04715447154471545,"Generated 
Molecules"
INTRODUCTION,0.04796747967479675,High Ring Count
INTRODUCTION,0.04878048780487805,"High 
Polarizability"
INTRODUCTION,0.04959349593495935,High Drug-
INTRODUCTION,0.05040650406504065,likeness
INTRODUCTION,0.05121951219512195,"High 
Hydrophobicity y|x"
INTRODUCTION,0.05203252032520325,º?(y|x) 10°2 10°1 100 101 102 ∞
INTRODUCTION,0.052845528455284556,Z°1e°ØU ºref
INTRODUCTION,0.05365853658536585,"Figure 1: Energy rank alignment (ERA) enables targeting low-energy, high-reward regions with
controllable fluctuations. Optimal policy approaches Boltzmann distribution with low regularization
(γ →0) and reference policy with high regularization (γ →∞) (left). Aligned models can be used
to sample molecules with desired chemical properties (right)."
RELATED WORK,0.05447154471544716,"1.1
Related Work
57"
RELATED WORK,0.055284552845528454,"Inverse molecular design tasks have a long history [17] and many recent works have sought to apply
58"
RELATED WORK,0.05609756097560976,"machine learning to facilitate this difficult search problem [27, 12, 13]. While reinforcement learning
59"
RELATED WORK,0.056910569105691054,"has proved a popular strategy for molecular optimization [39, 27], several recent studies have sought
60"
RELATED WORK,0.05772357723577236,"to use transformers [34] trained on large databases of molecules represented with the text-based
61"
RELATED WORK,0.05853658536585366,"SMILES syntax [10, 30, 35, 4] for such tasks. Schwaller et al. [31] utilized an atom-wise tokenization,
62"
RELATED WORK,0.05934959349593496,"which we also employ, to train a transformer for the downstream task of reaction prediction. These
63"
RELATED WORK,0.06016260162601626,"“chemical language models” have been studied for applications on downstream tasks, including
64"
RELATED WORK,0.06097560975609756,"property prediction [4, 10] and reaction prediction [23, 30].
65"
RELATED WORK,0.061788617886178863,"Building scalable strategies for alignment has attracted enormous attention because of the high cost
66"
RELATED WORK,0.06260162601626017,"and complexity of constraining LLM outputs. Much of the current paradigm is built on reinforcement
67"
RELATED WORK,0.06341463414634146,"learning from human feedback (RLHF) [21]. Within this framework, human preferences provided in
68"
RELATED WORK,0.06422764227642276,"the form of pairwise rankings are first used to train a reward model, and subsequently that reward
69"
RELATED WORK,0.06504065040650407,"model is used to optimize a policy using, for example, proximal policy optimization (PPO) [29].
70"
RELATED WORK,0.06585365853658537,"Rafailov et al. [25] demonstrated that the reward model can be treated implicitly using a scheme
71"
RELATED WORK,0.06666666666666667,"that maximizes the likelihood of the preferences given an offline dataset. Because this approach
72"
RELATED WORK,0.06747967479674796,"does not require training a reward model, it has been named Direct Preference Optimization (DPO).
73"
RELATED WORK,0.06829268292682927,"Our work differs from both strategies; first, unlike RLHF, we do not employ reinforcement learning
74"
RELATED WORK,0.06910569105691057,"and instead develop an explicit, gradient-based objective for the optimal policy. Secondly, unlike
75"
RELATED WORK,0.06991869918699187,"DPO, we leverage an explicit reward function and add regularization transparently, both of which
76"
RELATED WORK,0.07073170731707316,"help to avoid greedy policies [3]. However, like both approaches, we assume that the Bradley-Terry
77"
RELATED WORK,0.07154471544715447,"model [7] of preference data is appropriate for the underlying target distribution.
78"
RELATED WORK,0.07235772357723577,"Many recent works have built upon the ideas of RLHF and DPO, including studies on the effect
79"
RELATED WORK,0.07317073170731707,"of point-wise sampling of preference distributions [3], investigations into the theoretical basis for
80"
RELATED WORK,0.07398373983739838,"contrastive methods for unlearning target datasets [38], and alternatives to the Bradley-Terry pairwise
81"
RELATED WORK,0.07479674796747968,"preference model [20, 2]. One recent study explores alignment in the context of inverse molecular
82"
RELATED WORK,0.07560975609756097,"design: Park et al. [22] applies DPO to SMILES generators to increase the probability of activity
83"
RELATED WORK,0.07642276422764227,"for generated compounds against a drug target. However, they indicate that many preferences in
84"
RELATED WORK,0.07723577235772358,"chemistry are expressed as continuous signals, which is not suitable for DPO. Overcoming this
85"
RELATED WORK,0.07804878048780488,"limitation while maintaining the advantages of a direct gradient-based policy optimization strategy is
86"
RELATED WORK,0.07886178861788617,"a central goal of our current work. Our analysis and methodology directly addresses issues related
87"
RELATED WORK,0.07967479674796749,"to point-wise sampling because the explicit reward function eliminates overly greedy assignments
88"
RELATED WORK,0.08048780487804878,"of preference probabilities. Indeed, as discussed in Sec. 4, we see that DPO mode collapses where
89"
RELATED WORK,0.08130081300813008,"ERA shifts the policy towards the target distribution. While non-transitive preferences may arise
90"
RELATED WORK,0.08211382113821138,"in some settings, leading to a breakdown of the Bradley-Terry preference distribution model, by
91"
RELATED WORK,0.08292682926829269,"construction our target rewards are determined by quantitative evaluations of properties, and are
92"
RELATED WORK,0.08373983739837398,"therefore transitive.
93"
ENERGY RANK ALIGNMENT,0.08455284552845528,"2
Energy rank alignment
94"
ENERGY RANK ALIGNMENT,0.08536585365853659,"A policy is a conditional probability distribution π(·|x) : Y →R; we generate an output y from
95"
ENERGY RANK ALIGNMENT,0.08617886178861789,"prompt x. The spaces Y and X are discrete and finite, corresponding to sequences of tokenized
96"
ENERGY RANK ALIGNMENT,0.08699186991869919,"outputs of the model with a maximum length. In alignment tasks, we begin with a pre-trained
97"
ENERGY RANK ALIGNMENT,0.08780487804878048,"reference policy πref and seek to optimize a parametric, trainable policy πθ to adapt the conditional
98"
ENERGY RANK ALIGNMENT,0.0886178861788618,"sampling for a particular task or constraint.
99"
ENERGY RANK ALIGNMENT,0.08943089430894309,"Consider a prompt x ∈X and model outputs y, y′ ∈Y and a collection of preferences D = {(yi ≻
100"
ENERGY RANK ALIGNMENT,0.09024390243902439,"y′
i; xi)}n
i=1; the notation ≻indicates that yi is preferred to y′
i. The conditional probability that
101"
ENERGY RANK ALIGNMENT,0.0910569105691057,"y ≻y′ given x can be modeled as a pairwise Boltzmann ranking within the Bradley-Terry model,
102"
ENERGY RANK ALIGNMENT,0.091869918699187,"i.e.,
103"
ENERGY RANK ALIGNMENT,0.09268292682926829,"p(y ≻y′|x) =
e−βU(x,y)"
ENERGY RANK ALIGNMENT,0.09349593495934959,"e−βU(x,y) + e−βU(x,y′) ≡σ
 
βU(x, y′) −βU(x, y)

.
(1)"
ENERGY RANK ALIGNMENT,0.0943089430894309,"Here β > 0 is a constant, σ(x) = (1 + e−x)−1 and we refer to U : X × Y →R as an energy
104"
ENERGY RANK ALIGNMENT,0.0951219512195122,"function to make clear the connection to statistical physics, but it is the negative reward within the RL
105"
ENERGY RANK ALIGNMENT,0.0959349593495935,"framework for alignment.
106"
ENERGY RANK ALIGNMENT,0.09674796747967479,"To impose the preferences we minimize the objective
107"
ENERGY RANK ALIGNMENT,0.0975609756097561,J(π) = Ex∼ν
ENERGY RANK ALIGNMENT,0.0983739837398374,"Z
U(x, y)dπ(y|x) + β−1
Z
(1 + γ) log π(y|x) −γ log(πref(y|x))dπ(y|x)

,"
ENERGY RANK ALIGNMENT,0.0991869918699187,"(2)
where β−1 is a parameter controlling the magnitude of the entropic term, γ sets the scale of the
108"
ENERGY RANK ALIGNMENT,0.1,"Kullback-Leibler regularization compared with the energy term, and ν is a probability distribution
109"
ENERGY RANK ALIGNMENT,0.1008130081300813,"over the prompts ν ∈P(X). A proximal scheme for gradient descent on this objective corresponds
110"
ENERGY RANK ALIGNMENT,0.1016260162601626,"to a gradient flow on J [28, 19]; the functional can be viewed as a free energy, and the corresponding
111"
ENERGY RANK ALIGNMENT,0.1024390243902439,"flow is
112"
ENERGY RANK ALIGNMENT,0.10325203252032521,"∂tπt = ∇· (πt∇δπJ[πt]) ,
(3)
and δπ denotes the Fréchet derivative with respect to π. Assuming that π0 has full support on X × Y,
113"
ENERGY RANK ALIGNMENT,0.1040650406504065,"the optimization converges asymptotically to stationary policy which satisfies
114"
ENERGY RANK ALIGNMENT,0.1048780487804878,"∇δπJ[π⋆] = 0 ⇐⇒π⋆∝e−
β
1+γ U+
γ
γ+1 log πref,
(4)
and this minimizer is globally optimal. In the context of LLM alignment, a representation of the
115"
ENERGY RANK ALIGNMENT,0.10569105691056911,"energy function U : X × Y →R is learned as a “reward model”, though we also consider tasks
116"
ENERGY RANK ALIGNMENT,0.10650406504065041,"in which U is an easily evaluated function of the pair (x, y). The optimal distribution π⋆is a
117"
ENERGY RANK ALIGNMENT,0.1073170731707317,"Gibbs-Boltzmann measure
118"
ENERGY RANK ALIGNMENT,0.108130081300813,"π⋆(y|x) = Z−1(x) exp

−
β
1 + γ
 
U(x, y) −β−1γ log πref(y|x)

(5)"
ENERGY RANK ALIGNMENT,0.10894308943089431,"where Z(x) is the x-dependent normalization constant. This expression makes clear the effect of β:
119"
ENERGY RANK ALIGNMENT,0.10975609756097561,"when β →∞(low temperature), the reward dominates and fluctuations around the maximal reward
120"
ENERGY RANK ALIGNMENT,0.11056910569105691,"are small, which could lead to “mode-seeking”; when β →0 (high physical temperature) fluctuations
121"
ENERGY RANK ALIGNMENT,0.11138211382113822,"around the maximal reward increase and the regularization term favors proximity to πref. Similarly,
122"
ENERGY RANK ALIGNMENT,0.11219512195121951,"γ →0 recovers a Gibbs-Boltzmann distribution proportional to e−βU at inverse temperature β, while
123"
ENERGY RANK ALIGNMENT,0.11300813008130081,"γ →∞is dominated by the reference policy.
124"
ENERGY RANK ALIGNMENT,0.11382113821138211,"Loss functions for πθ:
Proximal Policy Optimization (PPO) optimizes an indirect, proximal
125"
ENERGY RANK ALIGNMENT,0.11463414634146342,"objective to minimize an objective closely related to (2) (cf. Appendix A). Direct Preference
126"
ENERGY RANK ALIGNMENT,0.11544715447154472,"Optimization (DPO) treats the negative reward function U implicitly and directly maximizes the
127"
ENERGY RANK ALIGNMENT,0.11626016260162601,"likelihood of p(y ≻y′|x). Our objectives differ from both approaches: like DPO, we directly
128"
ENERGY RANK ALIGNMENT,0.11707317073170732,"optimize the policy using an explicit, gradient-based objective, but, in contrast, we use a reward
129"
ENERGY RANK ALIGNMENT,0.11788617886178862,"function directly in our objective. The losses we build are thus amenable to both offline (samples
130"
ENERGY RANK ALIGNMENT,0.11869918699186992,"from πref) and online (samples from πθ) policy alignment, as explained below. Choosing to optimize
131"
ENERGY RANK ALIGNMENT,0.11951219512195121,"the objective online has been shown to have important consequences on performance [32], though we
132"
ENERGY RANK ALIGNMENT,0.12032520325203253,"focus here on the setting where samples are drawn offline.
133"
ENERGY RANK ALIGNMENT,0.12113821138211382,"We directly optimize the Kullback-Leibler divergence between the entropy-regularized preference
134"
ENERGY RANK ALIGNMENT,0.12195121951219512,"distribution pγ(y ≻y′|x) and the corresponding parametric preference distribution pθ(y ≻y′|x).
135"
ENERGY RANK ALIGNMENT,0.12276422764227642,"Explicitly, using the fact that conditional preference distribution is normalized, we obtain
136"
ENERGY RANK ALIGNMENT,0.12357723577235773,"D(y,y′)
KL
(pγ|pθ) = pγ(y ≻y′|x) log pγ(y ≻y′|x)"
ENERGY RANK ALIGNMENT,0.12439024390243902,pθ(y ≻y′|x) + pγ(y′ ≻y|x) log pγ(y′ ≻y|x)
ENERGY RANK ALIGNMENT,0.12520325203252033,"pθ(y′ ≻y|x),"
ENERGY RANK ALIGNMENT,0.12601626016260162,= pγ(y ≻y′|x) log pγ(y ≻y′|x)
ENERGY RANK ALIGNMENT,0.12682926829268293,"pθ(y ≻y′|x) +
 
1 −pγ(y ≻y′|x)

log 1 −pγ(y ≻y′|x)"
ENERGY RANK ALIGNMENT,0.12764227642276424,"1 −pθ(y ≻y′|x),"
ENERGY RANK ALIGNMENT,0.12845528455284552,"(6)
where
137"
ENERGY RANK ALIGNMENT,0.12926829268292683,"pγ := σ

β
1 + γ"
ENERGY RANK ALIGNMENT,0.13008130081300814,"
(U(x, y′) −U(x, y)) + β−1γ log πref(y|x)"
ENERGY RANK ALIGNMENT,0.13089430894308943,πref(y′|x)
ENERGY RANK ALIGNMENT,0.13170731707317074,"
.
(7)"
ENERGY RANK ALIGNMENT,0.13252032520325202,"This quantity is a well-defined KL divergence and is hence non-negative; the quantity vanishes when
138"
ENERGY RANK ALIGNMENT,0.13333333333333333,"pγ = pθ on the observations y, y′. Furthermore, with access to an explicit reward model, all terms
139"
ENERGY RANK ALIGNMENT,0.13414634146341464,"in (6) can be computed directly and
140"
ENERGY RANK ALIGNMENT,0.13495934959349593,"pθ(y ≻y′|x′) =
πθ(y|x)
πθ(y|x) + πθ(y′|x) = σ

log πθ(y|x)"
ENERGY RANK ALIGNMENT,0.13577235772357724,πθ(y′|x)
ENERGY RANK ALIGNMENT,0.13658536585365855,"
.
(8)"
ENERGY RANK ALIGNMENT,0.13739837398373983,"To obtain a minimizer of the regularized objective defined in (2) we optimize
141"
ENERGY RANK ALIGNMENT,0.13821138211382114,"LERA(πθ) = Ex∼DEy,y′∼πref(·|x)D(y,y′)
KL
(pγ|pθ);
(9)"
ENERGY RANK ALIGNMENT,0.13902439024390245,"If the current policy overlaps with the target preference distribution, it may be useful to sample
142"
ENERGY RANK ALIGNMENT,0.13983739837398373,"directly from the partially aligned policy, i.e., to use the “on-policy” formulation,
143"
ENERGY RANK ALIGNMENT,0.14065040650406505,"LERA
on (πθ) = Ex∼DEy,y′∼πθ(y|x)D(y,y′)
KL
(pγ|pθ)
(10)"
ENERGY RANK ALIGNMENT,0.14146341463414633,"instead of (9). One issue that arises with this scheme is that differentiation with respect to the
144"
ENERGY RANK ALIGNMENT,0.14227642276422764,"parameters of the policy θ because y and y′ are decoded into discrete tokens, an operation that is not
145"
ENERGY RANK ALIGNMENT,0.14308943089430895,"differentiable. To remedy this, we importance sample with a reference policy
146"
ENERGY RANK ALIGNMENT,0.14390243902439023,"LERA
on (πθ) = Ex∼DEy,y′∼πref(y|x)
πθ(y|x)πθ(y′|x)
πref(y|x)πref(y′|x)D(y,y′)
KL
(pγ|pθ).
(11)"
ENERGY RANK ALIGNMENT,0.14471544715447154,"This reweighting is straightforward and the importance weights should generally be appreciable,
147"
ENERGY RANK ALIGNMENT,0.14552845528455285,"especially early in training when πθ has not drifted far from πref. It is, of course, also natural to
148"
ENERGY RANK ALIGNMENT,0.14634146341463414,"iteratively update πθ using a previous iterate as the reference policy. In this work, we only use (9) as
149"
ENERGY RANK ALIGNMENT,0.14715447154471545,"an objective and leave the on-policy objectives to future work.
150"
THEORETICAL ANALYSIS,0.14796747967479676,"3
Theoretical Analysis
151"
THEORETICAL ANALYSIS,0.14878048780487804,"To understand the ERA loss function and its connection to the entropy regularized objective (2), we
152"
THEORETICAL ANALYSIS,0.14959349593495935,"first establish that the minimizers of (6) are of the form (5). We first define the notion of equivalence
153"
THEORETICAL ANALYSIS,0.15040650406504066,"precisely.
154"
THEORETICAL ANALYSIS,0.15121951219512195,"Definition 3.1 The conditional probability measures π(·|x) and π′(·|x) are conditionally equivalent
155"
THEORETICAL ANALYSIS,0.15203252032520326,"if ∀x ∈X, π and π′ are such that supy∈Y |π(y|x) −π′(y|x)| = 0.
156"
THEORETICAL ANALYSIS,0.15284552845528454,"We remark that this strong form of equivalence is appropriate on the finite, discrete spaces X and Y
157"
THEORETICAL ANALYSIS,0.15365853658536585,"we consider here.
158"
THEORETICAL ANALYSIS,0.15447154471544716,"Lemma 3.1 If π is conditionally equivalent to π′, then π′
g(·|x) ∝π′(·|x)eg(x) is conditionally
159"
THEORETICAL ANALYSIS,0.15528455284552845,"equivalent to π for all functions g : X →R such that supx∈X |eg(x)| < +∞.
160"
THEORETICAL ANALYSIS,0.15609756097560976,"We prove Lemma 3.1 in Appendix A and use this simple lemma to prove the following result.
161"
THEORETICAL ANALYSIS,0.15691056910569107,"Proposition 3.2 Suppose π(·|x) ∈P(Y) and that supp(π) = supp(πref). Let β > 0, γ ≥0 and
162"
THEORETICAL ANALYSIS,0.15772357723577235,"that the reward model is such that supx,y∈X×Y |e−U(x,y)| < +∞. Then, the minimizer of LERA is
163"
THEORETICAL ANALYSIS,0.15853658536585366,"conditionally equivalent to π⋆.
164"
THEORETICAL ANALYSIS,0.15934959349593497,"First,
we
verify
that
any
probability
measure
πg(y|x)
∝
exp(−
β
1+γ
 
U(x, y) −
165"
THEORETICAL ANALYSIS,0.16016260162601625,"β−1γ log πref(y|x)

+ g(x)) minimizes the objective. Because LERA is non-negative, it suf-
166"
THEORETICAL ANALYSIS,0.16097560975609757,"fices to show that for all pairs y, y′, D(y,y′)
KL
(pγ|pθ) ≡0. This follows immediately from the
167"
THEORETICAL ANALYSIS,0.16178861788617885,"cancellation in the preference probability pγ of eg(x) after factorization in (5). Now, suppose that
168"
THEORETICAL ANALYSIS,0.16260162601626016,"π(y|x) ̸= exp

−
β
1+γ
 
U(x, y) −β−1γ log πref(y|x)

where we have taken g(x) = 0 without
169"
THEORETICAL ANALYSIS,0.16341463414634147,"loss of generality and π := πg. Assume that for all pairs y, y′, the divergence D(y,y′)
KL
(pγ|pθ) ≡0
170"
THEORETICAL ANALYSIS,0.16422764227642275,"which is required of a minimizer. Equivalently, it must be the case that for all y, y′,
171"
THEORETICAL ANALYSIS,0.16504065040650406,"π(y|x)
π(y|x) + π(y′|x) =
π⋆(y|x)
π⋆(y|x) + π⋆(y′|x) =⇒π(y′|x)"
THEORETICAL ANALYSIS,0.16585365853658537,π(y|x) = π⋆(y′|x)
THEORETICAL ANALYSIS,0.16666666666666666,"π⋆(y|x) ,
(12)"
THEORETICAL ANALYSIS,0.16747967479674797,"from which we see that
172"
THEORETICAL ANALYSIS,0.16829268292682928,"π(y|x) =
π(y′|x)"
THEORETICAL ANALYSIS,0.16910569105691056,"e−
β
1+γ (U(x,y′)−β−1γ log πref(y′|x)) e−
β
1+γ (U(x,y)−β−1γ log πref(y|x)).
(13)"
THEORETICAL ANALYSIS,0.16991869918699187,"By construction, π(y|x) does not depend on y′ so the prefactor must be purely a function of x, which
173"
THEORETICAL ANALYSIS,0.17073170731707318,"completes the proof, using Lemma 3.1.
174"
THEORETICAL ANALYSIS,0.17154471544715447,"Gradients of LERA.
One advantage of the ERA framework is that the objective is amenable to direct,
175"
THEORETICAL ANALYSIS,0.17235772357723578,"gradient-based optimization. We remark that establishing global convergence for the optimization of
176"
THEORETICAL ANALYSIS,0.17317073170731706,"θ using (9) requires establishing convexity with respect to the parameters, which is not obviously the
177"
THEORETICAL ANALYSIS,0.17398373983739837,"case for our objective, nor those used in PPO and DPO. However, one can still glean some insight
178"
THEORETICAL ANALYSIS,0.17479674796747968,"into the optimization by examining the gradients on a samplewise basis. Using the compact notation
179"
THEORETICAL ANALYSIS,0.17560975609756097,"pθ(y ≻y′|x) ≡σθ and pγ(y ≻y′|x) ≡σ⋆,
180"
THEORETICAL ANALYSIS,0.17642276422764228,"∇θLERA = Ex∼DEy,y′∼πref"
THEORETICAL ANALYSIS,0.1772357723577236,1 −σ⋆
THEORETICAL ANALYSIS,0.17804878048780487,"1 −σθ
−σ⋆ σθ"
THEORETICAL ANALYSIS,0.17886178861788618,"
∇θσθ.
(14)"
THEORETICAL ANALYSIS,0.1796747967479675,"The gradient is straightforward to interpret on a particular pair y, y′: if pθ(y ≻y′|x) is larger than
181"
THEORETICAL ANALYSIS,0.18048780487804877,"pγ(y ≻y′|x) then the preference gradient is positive and gradient descent lowers the probability that
182"
THEORETICAL ANALYSIS,0.18130081300813009,"y ≻y′. The opposite occurs whenever pθ(y ≻y′|x) is smaller than pγ(y ≻y′|x). The magnitude
183"
THEORETICAL ANALYSIS,0.1821138211382114,"of the gradient is scaled by the degree of misspecification of the preference probability.
184"
THEORETICAL ANALYSIS,0.18292682926829268,"This calculation highlights one key difference between the approach we use and DPO. When the data
185"
THEORETICAL ANALYSIS,0.183739837398374,"only contains one observation of y ≻y′ for a given x, the DPO objective’s implicit reward model
186"
THEORETICAL ANALYSIS,0.18455284552845527,"assigns zero probability to y′ ≻y. This pushes the policy towards extremal values, which can lead
187"
THEORETICAL ANALYSIS,0.18536585365853658,"to undesired behavior, as discussed in Azar et al. [3]. In our formulation, this behavior occurs only
188"
THEORETICAL ANALYSIS,0.1861788617886179,"when the reward model assigns an energy of ±∞, which is prohibited by construction in most tasks.
189"
THEORETICAL ANALYSIS,0.18699186991869918,"We further discuss differences between ERA and DPO in Appendix A.2.
190"
EXPERIMENTS,0.1878048780487805,"4
Experiments
191"
EXPERIMENTS,0.1886178861788618,"We test ERA on both chemical and language tasks to shed light on the following questions: 1) Can
192"
EXPERIMENTS,0.18943089430894308,"we use ERA to robustly fine-tune our model to generate samples according to a desired distribution?
193"
EXPERIMENTS,0.1902439024390244,"0
100
200
300
400
500
Molar Refractivity"
EXPERIMENTS,0.1910569105691057,0.0000
EXPERIMENTS,0.191869918699187,0.0025
EXPERIMENTS,0.1926829268292683,0.0050
EXPERIMENTS,0.19349593495934958,0.0075
EXPERIMENTS,0.1943089430894309,0.0100
EXPERIMENTS,0.1951219512195122,0.0125
UNALIGNED,0.19593495934959348,"0.0150
Unaligned
Aligned (µ = 50)"
UNALIGNED,0.1967479674796748,Aligned (µ = 180)
UNALIGNED,0.1975609756097561,"0
5
10
15
Ring Count 0.0 0.1 0.2 0.3"
UNALIGNED,0.1983739837398374,"Unaligned
Aligned (µ = 1)"
UNALIGNED,0.1991869918699187,Aligned (µ = 8)
UNALIGNED,0.2,"0.0
0.2
0.4
0.6
0.8
Quantitative Estimate of Drug-Likeness 0 1 2 3 4"
UNALIGNED,0.2008130081300813,"Unaligned
Aligned (β = 5.0)"
UNALIGNED,0.2016260162601626,Aligned (β = 10.0)
UNALIGNED,0.20243902439024392,Aligned (β = 20.0)
UNALIGNED,0.2032520325203252,"−10
0
10
Wildman-Crippen LogP 0.00 0.05 0.10 0.15"
UNALIGNED,0.2040650406504065,"0.20
Unaligned
Aligned (µ = 0)"
UNALIGNED,0.2048780487804878,Aligned (µ = 7.5)
UNALIGNED,0.2056910569105691,"Figure 2: Unprompted molecular generator alignment. Distributions of different chemical properties
for molecules sampled from aligned and unaligned policies. The center of the harmonic potential, µ,
is varied for MR (β = 1.0), Ring Count (β = 1.0), and LogP (β = 10.0), while β is varied for QED.
All experiments were run with no regularization to the reference policy (γ = 0)."
UNALIGNED,0.20650406504065041,"2) What is the effect of changing the inverse-temperature β during ERA? 3) Do we maintain sample
194"
UNALIGNED,0.2073170731707317,"diversity (and validity) without regularizing to remain close to a reference policy, and what is the
195"
UNALIGNED,0.208130081300813,"effect of increased regularization? 4) Can we simultaneously target multiple properties with high
196"
UNALIGNED,0.20894308943089432,"fidelity, and how can we trade off between desired properties? 5) Can we carry out ERA on higher
197"
UNALIGNED,0.2097560975609756,"capacity models with “weak” signals from smaller models?
198"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.2105691056910569,"4.1
Generating molecules with desired properties
199"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.21138211382113822,"We use a decoder-only representation for the molecular generator [4], where the generator has 2 layers,
200"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.2121951219512195,"an embedding dimension of 512, a vocabulary of 324 tokens, and totals 3.5M parameters. Starting
201"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.21300813008130082,"from a random initialization, we carry out pretraining on a dataset of 2.4M small molecules from the
202"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.2138211382113821,"ChEMBL database [37] for 180 epochs. This version of the model is not conditioned on a prompt
203"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.2146341463414634,"and generates a small molecule given just a start-of-sequence token. We use this pretrained model as
204"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.21544715447154472,"our reference policy for all unprompted molecular alignment tasks (Sec. 4.1.1). In Sec. 4.1.2, we
205"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.216260162601626,"generate molecules conditioned on a prompt using a generator that was trained to carry out sampling
206"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.21707317073170732,"with a prompt molecule.
207"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.21788617886178863,"Central to ERA is, of course, access to a computable energy function. As a proof-of-concept, here
208"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.2186991869918699,"we consider 5 different properties for which the corresponding energy function is easily evaluable:
209"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.21951219512195122,"Quantitative Estimate of Drug-Likeness (QED) [6], Wildman-Crippen LogP (LogP) [36], Ring Count,
210"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.22032520325203253,"Molar Refractivity (MR) [36], and Tanimoto Similarity [26]. Briefly, LogP is a measure of the
211"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.22113821138211381,"hydrophobicity of a molecule, MR is a measure of the polarizability of the molecule, and Tanimoto
212"
GENERATING MOLECULES WITH DESIRED PROPERTIES,0.22195121951219512,"similarity is a measure of the similarity between two molecules (see Appendix C.2).
213"
UNPROMPTED MOLECULAR ALIGNMENT,0.22276422764227644,"4.1.1
Unprompted molecular alignment
214"
UNPROMPTED MOLECULAR ALIGNMENT,0.22357723577235772,"First, we independently target four different properties using ERA with an unprompted
215"
UNPROMPTED MOLECULAR ALIGNMENT,0.22439024390243903,"molecular generator (Fig. 2).
Using the reference policy, we generate a dataset D
=
216"
UNPROMPTED MOLECULAR ALIGNMENT,0.2252032520325203,"{y(i)
1 , y(i)
2 , U(y(i)
1 ), U(y(i)
2 )}N
i=1 and carry out energy rank alignment on πθ, where πθ is initialized
217"
UNPROMPTED MOLECULAR ALIGNMENT,0.22601626016260162,"using the weights of πref. Here, y1, y2 ∼πref and y and U(y) denote the generated molecule and
218"
UNPROMPTED MOLECULAR ALIGNMENT,0.22682926829268293,"its corresponding energy, respectively. For MR, Ring Count, and LogP, we define the energy U to be
219 0.00 0.25 0.50 0.75"
UNPROMPTED MOLECULAR ALIGNMENT,0.22764227642276422,"1.00 βLogP = 1.0, βQED = 1.0
βLogP = 1.0, βQED = 10.0
βLogP = 1.0, βQED = 20.0
βLogP = 1.0, βQED = 100.0"
UNPROMPTED MOLECULAR ALIGNMENT,0.22845528455284553,"0
5
10
0.00 0.25 0.50 0.75"
UNPROMPTED MOLECULAR ALIGNMENT,0.22926829268292684,"1.00 βLogP = 5.0, βQED = 1.0"
UNPROMPTED MOLECULAR ALIGNMENT,0.23008130081300812,"0
5
10"
UNPROMPTED MOLECULAR ALIGNMENT,0.23089430894308943,"βLogP = 5.0, βQED = 10.0"
UNPROMPTED MOLECULAR ALIGNMENT,0.23170731707317074,"0
5
10"
UNPROMPTED MOLECULAR ALIGNMENT,0.23252032520325203,"βLogP = 5.0, βQED = 20.0"
UNPROMPTED MOLECULAR ALIGNMENT,0.23333333333333334,"0
5
10"
UNPROMPTED MOLECULAR ALIGNMENT,0.23414634146341465,"βLogP = 5.0, βQED = 100.0"
UNPROMPTED MOLECULAR ALIGNMENT,0.23495934959349593,Wildman-Crippen Log P QED
UNPROMPTED MOLECULAR ALIGNMENT,0.23577235772357724,"Figure 3: Unprompted multi-property molecular generator alignment. 2D histograms of LogP versus
QED for different combinations of property-specific β illustrating a clear trade-off when performing
multi-property alignment. Relative increases in β for a given property target higher values for that
property. All experiments were run with no regularization to the reference policy (γ = 0)."
UNPROMPTED MOLECULAR ALIGNMENT,0.23658536585365852,"a harmonic potential centered at a target value. For QED, we define the energy to be the negative
220"
UNPROMPTED MOLECULAR ALIGNMENT,0.23739837398373984,"logarithm of QED and vary β to assess its impact on alignment (see Table 1, 2). In Fig. 2, we see that
221"
UNPROMPTED MOLECULAR ALIGNMENT,0.23821138211382115,"we successfully shift the distribution to target means that are both greater and lower than the average
222"
UNPROMPTED MOLECULAR ALIGNMENT,0.23902439024390243,"value of MR, Ring Count, and LogP under the reference policy. Furthermore, in the alignment of
223"
UNPROMPTED MOLECULAR ALIGNMENT,0.23983739837398374,"QED, we observe the effect of changing β on the learned policy; with increased β, the learned policy
224"
UNPROMPTED MOLECULAR ALIGNMENT,0.24065040650406505,"concentrates around low-energy samples (i.e. near QED = 1), and with lower β, the learned policy
225"
UNPROMPTED MOLECULAR ALIGNMENT,0.24146341463414633,"samples a greater range of QED values, as expected. We note that for each of these four experiments,
226"
UNPROMPTED MOLECULAR ALIGNMENT,0.24227642276422764,"we did not regularize towards the reference policy (i.e. γ = 0). Even so, we were able to maintain
227"
UNPROMPTED MOLECULAR ALIGNMENT,0.24308943089430896,"both sample diversity and maintain appreciable sample validity (see Fig. 7 and Table 3).
228"
UNPROMPTED MOLECULAR ALIGNMENT,0.24390243902439024,"Many molecular design tasks require balancing multiple properties, and designing an objective for
229"
UNPROMPTED MOLECULAR ALIGNMENT,0.24471544715447155,"multi-property alignment is straightforward within the ERA framework. To demonstrate this, we
230"
UNPROMPTED MOLECULAR ALIGNMENT,0.24552845528455283,"generate molecules with both high QED and LogP using ERA with an energy function weighted by
231"
UNPROMPTED MOLECULAR ALIGNMENT,0.24634146341463414,"property-specific β: U = βQEDUQED + βLogPULogP (see Table 1, 4 for details on energy function).
232"
UNPROMPTED MOLECULAR ALIGNMENT,0.24715447154471545,"We carry out ERA with different pairs of (βQED, βLogP) using the same procedure as above, and
233"
UNPROMPTED MOLECULAR ALIGNMENT,0.24796747967479674,"from Fig. 3, we see that we target multiple properties with varying fidelity by simply modulating the
234"
UNPROMPTED MOLECULAR ALIGNMENT,0.24878048780487805,"value of property-specific β. Ultimately, increasing the β for an individual property enables us to
235"
UNPROMPTED MOLECULAR ALIGNMENT,0.24959349593495936,"favor higher values of that property in multi-property alignment setting. In this case, we also do not
236"
UNPROMPTED MOLECULAR ALIGNMENT,0.25040650406504067,"regularize with the KL-divergence to the reference policy and again maintain sample diversity and
237"
UNPROMPTED MOLECULAR ALIGNMENT,0.25121951219512195,"validity (see Fig. 8 and Table 4)
238"
PROMPTED MOLECULAR ALIGNMENT,0.25203252032520324,"4.1.2
Prompted molecular alignment
239"
PROMPTED MOLECULAR ALIGNMENT,0.2528455284552846,"Inspired by the task of lead optimization in drug discovery efforts [16], we ask whether we can use
240"
PROMPTED MOLECULAR ALIGNMENT,0.25365853658536586,"ERA to train a molecular generator that can sample a molecule that is both similar to the prompt
241"
PROMPTED MOLECULAR ALIGNMENT,0.25447154471544714,"molecule and also exhibits some desired property.
242"
PROMPTED MOLECULAR ALIGNMENT,0.2552845528455285,"First, we fine-tune the pretrained molecular generator to enable prompted molecular generation (see
243"
PROMPTED MOLECULAR ALIGNMENT,0.25609756097560976,"Appendix C.3.2) and use this fine-tuned model as our reference policy for all prompted molecular
244"
PROMPTED MOLECULAR ALIGNMENT,0.25691056910569104,"alignment tasks. This reference policy disproportionately samples molecules that are identical (i.e. a
245"
PROMPTED MOLECULAR ALIGNMENT,0.2577235772357724,"Tanimoto similarity of 1.0) to the prompt molecule (see Fig. 4), so we carry out multi-property align-
246"
PROMPTED MOLECULAR ALIGNMENT,0.25853658536585367,"ment on this reference policy to generate molecules that are similar—but not identical—to the prompt
247"
PROMPTED MOLECULAR ALIGNMENT,0.25934959349593495,"molecule and also have a high drug-likeness as measured by QED. Using ERA, we optimize the refer-
248"
PROMPTED MOLECULAR ALIGNMENT,0.2601626016260163,"ence policy with a generated dataset D = {(y(i)
1 , x(i)), (y(i)
2 , x(i)), U(y(i)
1 , x(i)), U(y(i)
2 , x(i))}N
i=1,
249"
PROMPTED MOLECULAR ALIGNMENT,0.26097560975609757,"where we sample four molecules for each prompt molecule from the reference policy and consider all
250"
PROMPTED MOLECULAR ALIGNMENT,0.26178861788617885,"possible preference pairs for a total of six preference pairs per prompt molecule (see Appendix C.2
251"
PROMPTED MOLECULAR ALIGNMENT,0.26260162601626014,"for full details on energy used).
252"
PROMPTED MOLECULAR ALIGNMENT,0.2634146341463415,"We observe that the per-prompt average QED under the optimized policy for a given prompt is higher
253"
PROMPTED MOLECULAR ALIGNMENT,0.26422764227642276,"than the corresponding average under the reference policy (Fig. 4). Furthermore, we see that we are
254"
PROMPTED MOLECULAR ALIGNMENT,0.26504065040650404,"able to sample a diverse set of molecules that are chemically similar to the prompt molecule, and
255"
PROMPTED MOLECULAR ALIGNMENT,0.2658536585365854,"0.5
1.0
Tanimoto Similarity 0 5 10"
PROMPTED MOLECULAR ALIGNMENT,0.26666666666666666,"Unaligned
Aligned"
PROMPTED MOLECULAR ALIGNMENT,0.26747967479674795,"0.0
0.2
(EπθfQED(y|x)
−EπreffQED(y|x)) 0.0 2.5 5.0 7.5"
PROMPTED MOLECULAR ALIGNMENT,0.2682926829268293,Multienergy Alignment (QED and Tanimoto)
PROMPTED MOLECULAR ALIGNMENT,0.26910569105691057,"0.5
1.0
Tanimoto Similarity 0 5 10"
PROMPTED MOLECULAR ALIGNMENT,0.26991869918699185,"Unaligned
Aligned"
PROMPTED MOLECULAR ALIGNMENT,0.2707317073170732,"0
2
(EπθfLogP(y|x)
−EπreffLogP(y|x)) 0.0 0.5 1.0"
PROMPTED MOLECULAR ALIGNMENT,0.27154471544715447,"1.5
Multienergy Alignment (LogP and Tanimoto)"
PROMPTED MOLECULAR ALIGNMENT,0.27235772357723576,"Figure 4: Prompted multi-property molecular generator alignment. From left to right: Tanimoto
similarities computed between the prompt and sampled molecules for both aligned and unaligned
policies (QED and Tanimoto alignment), per-prompt difference in the average QED under aligned
and unaligned policies (QED and Tanimoto alignment), Tanimoto similarities computed between
the prompt and sampled molecules for both aligned and unaligned policies (LogP and Tanimoto
alignment), and per-prompt difference in the average LogP under aligned and unaligned policies
(LogP and Tanimoto alignment). With alignment, we target higher QED and LogP values, while still
sampling molecules chemically similar—but not identical—to prompt molecule."
PROMPTED MOLECULAR ALIGNMENT,0.2731707317073171,"10−1
100
101
β 0.4 0.6 0.8 1.0"
PROMPTED MOLECULAR ALIGNMENT,0.2739837398373984,Mean Sentiment
PROMPTED MOLECULAR ALIGNMENT,0.27479674796747966,GPT −2 Alignment
PROMPTED MOLECULAR ALIGNMENT,0.275609756097561,"Unaligned
Aligned (γ = 0)"
PROMPTED MOLECULAR ALIGNMENT,0.2764227642276423,Aligned (γ = 0.001)
PROMPTED MOLECULAR ALIGNMENT,0.27723577235772356,Aligned (γ = 0.01)
PROMPTED MOLECULAR ALIGNMENT,0.2780487804878049,"10−1
100
101
β 0.0 0.2 0.4 0.6 0.8 1.0"
PROMPTED MOLECULAR ALIGNMENT,0.2788617886178862,Relative Unsafe Content
PROMPTED MOLECULAR ALIGNMENT,0.27967479674796747,LLaMA2 −13B Alignment
PROMPTED MOLECULAR ALIGNMENT,0.2804878048780488,"Unaligned
Aligned (γ = 0)"
PROMPTED MOLECULAR ALIGNMENT,0.2813008130081301,Aligned (γ = 0.001)
PROMPTED MOLECULAR ALIGNMENT,0.2821138211382114,Aligned (γ = 0.01)
PROMPTED MOLECULAR ALIGNMENT,0.28292682926829266,"Figure 5: AI-guided alignment of LLMs. Average sentiment of responses from aligned GPT-2 model
across all prompts. (left). Proportion of unsafe content relative to unaligned model of responses
aligned LLaMA2-13B model across all prompts (right). 5.4% of all responses from unaligned model
were classified as unsafe. Error bars too small to be shown."
PROMPTED MOLECULAR ALIGNMENT,0.283739837398374,"also chemically valid (see Figure 9, Table 5). We repeat the experiment with a related objective of
256"
PROMPTED MOLECULAR ALIGNMENT,0.2845528455284553,"generating molecules similar to the prompt molecule with a high LogP instead and again observe
257"
PROMPTED MOLECULAR ALIGNMENT,0.28536585365853656,"that we increase the per-prompt average LogP under the optimized policy relative to the reference
258"
PROMPTED MOLECULAR ALIGNMENT,0.2861788617886179,"policy without degrading sample diversity and validity. For both of these experiments, we required
259"
PROMPTED MOLECULAR ALIGNMENT,0.2869918699186992,"regularization to the reference policy. With no regularization, the aligned generator would almost
260"
PROMPTED MOLECULAR ALIGNMENT,0.28780487804878047,"exclusively sample sequences that were chemically invalid (< 25% chemical validity). Finally, we
261"
PROMPTED MOLECULAR ALIGNMENT,0.2886178861788618,"note that the increases in QED and LogP in Fig. 4 are smaller relative to the increases in Fig. 2
262"
PROMPTED MOLECULAR ALIGNMENT,0.2894308943089431,"because the samples are now conditioned to remain proximal to the prompt molecule, which restricts
263"
PROMPTED MOLECULAR ALIGNMENT,0.29024390243902437,"the chemical space that can be explored.
264"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2910569105691057,"4.2
AI-guided alignment of large language models
265"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.291869918699187,"We test the generality of ERA by applying it to align large language models (LLMs). Similar to the
266"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2926829268292683,"experiments in [25], we first carry out ERA on a GPT-2 model [24] fine-tuned on movies reviews
267"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2934959349593496,"from IMDb [18]. We use a pretrained sentiment classifier [14] to evaluate the energies—where
268"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2943089430894309,"lower energies correspond to more positive sentiments—of sampled responses from the reference
269"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2951219512195122,"policy and carry out ERA using the same approach as in Section 4.1.2 (see Appendix D.1). We
270"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2959349593495935,"vary the regularization strength γ and inverse-temperature β on the average sentiment and observe
271"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2967479674796748,"that across all regularization strengths, with increasing β, the average sentiment becomes more
272"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2975609756097561,"positive. Increasing regularization also elicits more positive sentiments. Qualitatively, with lower
273"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2983739837398374,"regularization, we observe that text quality degrades and becomes less coherent, likely resulting in
274"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.2991869918699187,"lower average sentiment predictions by the sentiment model. Regularization here is important to
275"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3,"ensure high quality text samples.
276"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3008130081300813,"We next leverage a “weak” AI supervisor to carry out LLM alignment, a task sometimes called
277"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3016260162601626,"“superalignment” [8]. In the present context, we order “weak” vs. “strong” models based on their
278"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3024390243902439,"parameter count (within the same family) and empirical performance; i.e., LLaMA2-7B is weaker
279"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3032520325203252,"than LLaMA2-13B. Here, the weak model does not necessarily contain the complexity of the stronger
280"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3040650406504065,"model but can weakly discern between different outputs of a stronger model. Given a sample
281"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3048780487804878,"yi ∼πstrong(y|x), we define the energy using the weak model U(yi|x) = −log πweak(yi|x).
282"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3056910569105691,"We
test
weak-to-strong
alignment
using
a
previously
aligned
LLaMA2-7B-
283"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3065040650406504,"Chat
(meta-llama/Llama-2-7b-chat)
to
optimize
an
unaligned
LLaMA2-13B
284"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3073170731707317,"(meta-llama/Llama-2-13b) model [33].
Using prompts from the Anthropic Helpful and
285"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.308130081300813,"Harmless dialogue dataset [5], we first carry out a short supervised fine-tuning step of LLaMA2-13B
286"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3089430894308943,"to ensure it can output text in a chat-like format (see Appendix D.2). Using this reference policy,
287"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3097560975609756,"we generate a dataset with energies computed from the smaller LLaMA2-7B-Chat model and carry
288"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3105691056910569,"out ERA as above, again across varying γ and β. We evaluate the “safety” of generated samples
289"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.31138211382113823,"using Meta LLama Guard 2 (meta-llama/Meta-Llama-Guard-2-8B) [15]. We observe that as we
290"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3121951219512195,"increase β, the proportion of unsafe content relative to the unaligned, reference model decreases, with
291"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3130081300813008,"over a 90% drop between the unaligned model and the models aligned with the highest β across all γ.
292"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.31382113821138213,"For these experiments, we observe that varying regularization strengths has a minimal effect and that
293"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3146341463414634,"we are in fact able to generate coherent sentences with no regularization, with strong regularization
294"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3154471544715447,"hurting performance for β = 0.1. Finally, we compare ERA and DPO in Appendix D.2 and observe
295"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.31626016260162604,"that with our implementation of DPO, we are able to generate lower energy samples, but that it is
296"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3170731707317073,"prone to mode collapse. We caution that our implementation of DPO is likely not optimal and that
297"
AI-GUIDED ALIGNMENT OF LARGE LANGUAGE MODELS,0.3178861788617886,"we did not exhaustively tune the hyperparameters of DPO due to resource constraints.
298"
CONCLUSIONS AND LIMITATIONS,0.31869918699186994,"5
Conclusions and Limitations
299"
CONCLUSIONS AND LIMITATIONS,0.3195121951219512,"This paper introduces energy rank alignment, a simple and effective algorithm for policy optimization
300"
CONCLUSIONS AND LIMITATIONS,0.3203252032520325,"with an explicit reward model. We find that ERA is stable without extensive hyperparameter tuning,
301"
CONCLUSIONS AND LIMITATIONS,0.32113821138211385,"and sufficiently general to successfully align both application-specific transformers for chemical
302"
CONCLUSIONS AND LIMITATIONS,0.32195121951219513,"search problems as well as generative pre-trained transformers for language. The algorithm exhibits
303"
CONCLUSIONS AND LIMITATIONS,0.3227642276422764,"strong performance with a variety of reward models, even ones with relatively weak signal, such as
304"
CONCLUSIONS AND LIMITATIONS,0.3235772357723577,"the AI feedback of LLaMA2-7B-Chat. Interestingly, with this approach we are able to reduce unsafe
305"
CONCLUSIONS AND LIMITATIONS,0.32439024390243903,"content by more than 90% with no human preference data.
306"
CONCLUSIONS AND LIMITATIONS,0.3252032520325203,"We analyze the minimizers of the ERA objective and find that they differ from the minimizers of
307"
CONCLUSIONS AND LIMITATIONS,0.3260162601626016,"popular policy alignment algorithms DPO and PPO in an important way: unlike PPO, the strength of
308"
CONCLUSIONS AND LIMITATIONS,0.32682926829268294,"regularization to the reference policy that we add is controlled by a parameter γ, while the entropy
309"
CONCLUSIONS AND LIMITATIONS,0.3276422764227642,"of the target distribution is independently tuned by a distinct parameter β. This means that we can
310"
CONCLUSIONS AND LIMITATIONS,0.3284552845528455,"avoid greedy policies by keeping β small—amplifying fluctuations around the optimum of the reward
311"
CONCLUSIONS AND LIMITATIONS,0.32926829268292684,"model −U—while reducing the influence of the reference policy by taking γ small. Our objective
312"
CONCLUSIONS AND LIMITATIONS,0.3300813008130081,"leads to easily interpretable sample-wise gradients which highlight the importance of a reward model
313"
CONCLUSIONS AND LIMITATIONS,0.3308943089430894,"relative to DPO in the sampled objective. Similar observations about the inadequacy of the DPO
314"
CONCLUSIONS AND LIMITATIONS,0.33170731707317075,"objective for finite preference observations were also made theoretically in Azar et al. [3].
315"
CONCLUSIONS AND LIMITATIONS,0.33252032520325203,"Limitations:
First, our approach requires a reward model, which can be difficult to train or design,
316"
CONCLUSIONS AND LIMITATIONS,0.3333333333333333,"especially for complex tasks. While we observed that ERA makes an appreciable impact even with
317"
CONCLUSIONS AND LIMITATIONS,0.33414634146341465,"weak supervision from an AI chat model, this sort of proxy may not be available for more complex
318"
CONCLUSIONS AND LIMITATIONS,0.33495934959349594,"tasks. For example, optimizing small molecules for high binding affinity to a target protein would
319"
CONCLUSIONS AND LIMITATIONS,0.3357723577235772,"require expensive and noisy evaluations of a reward model, which likely limits the scope of molecular
320"
CONCLUSIONS AND LIMITATIONS,0.33658536585365856,"design to problems where the reward can be computed somewhat efficiently. A second limitation of
321"
CONCLUSIONS AND LIMITATIONS,0.33739837398373984,"our present work is that we do not train the molecular transformer to favor synthetic accessibility
322"
CONCLUSIONS AND LIMITATIONS,0.3382113821138211,"nor do we explicitly seek to obtain molecules that are easily synthesized experimentally. There are
323"
CONCLUSIONS AND LIMITATIONS,0.33902439024390246,"models that seek to evaluate synthesizability computationally that could be used in our rewards,
324"
CONCLUSIONS AND LIMITATIONS,0.33983739837398375,"which we plan to explore in future work [11]. A final limitation of our current work is the moderate
325"
CONCLUSIONS AND LIMITATIONS,0.34065040650406503,"scale of our numerical experiments due to our limited compute resources, including the inadequate
326"
CONCLUSIONS AND LIMITATIONS,0.34146341463414637,"hyperparameter tuning for the DPO baseline for Fig. 5.
327"
REFERENCES,0.34227642276422765,"References
328"
REFERENCES,0.34308943089430893,"[1] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/
329"
REFERENCES,0.3439024390243902,"blob/main/MODEL_CARD.md.
330"
REFERENCES,0.34471544715447155,"[2] G. An, J. Lee, X. Zuo, N. Kosaka, K.-M. Kim, and H. O. Song. Direct Preference-based Policy
331"
REFERENCES,0.34552845528455284,"Optimization without Reward Modeling. Advances in Neural Information Processing Systems,
332"
REFERENCES,0.3463414634146341,"36:70247–70266, Dec. 2023.
333"
REFERENCES,0.34715447154471546,"[3] M. G. Azar, M. Rowland, B. Piot, D. Guo, D. Calandriello, M. Valko, and R. Munos. A General
334"
REFERENCES,0.34796747967479674,"Theoretical Paradigm to Understand Learning from Human Preferences, Nov. 2023.
335"
REFERENCES,0.348780487804878,"[4] V. Bagal, R. Aggarwal, P. K. Vinod, and U. D. Priyakumar. MolGPT: Molecular Generation
336"
REFERENCES,0.34959349593495936,"Using a Transformer-Decoder Model. Journal of Chemical Information and Modeling, 62(9):
337"
REFERENCES,0.35040650406504065,"2064–2076, May 2022. ISSN 1549-9596. doi: 10.1021/acs.jcim.1c00600.
338"
REFERENCES,0.35121951219512193,"[5] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,
339"
REFERENCES,0.35203252032520327,"T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-
340"
REFERENCES,0.35284552845528455,"Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson,
341"
REFERENCES,0.35365853658536583,"D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training
342"
REFERENCES,0.3544715447154472,"a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Apr.
343"
REFERENCES,0.35528455284552846,"2022.
344"
REFERENCES,0.35609756097560974,"[6] G. R. Bickerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins. Quantifying
345"
REFERENCES,0.3569105691056911,"the chemical beauty of drugs. Nature Chemistry, 4(2):90–98, Feb. 2012. ISSN 1755-4330,
346"
REFERENCES,0.35772357723577236,"1755-4349. doi: 10.1038/nchem.1243.
347"
REFERENCES,0.35853658536585364,"[7] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of
348"
REFERENCES,0.359349593495935,"paired comparisons. Biometrika, 39(3/4):324–345, 1952. ISSN 0006-3444. doi: 10.2307/
349"
REFERENCES,0.36016260162601627,"2334029.
350"
REFERENCES,0.36097560975609755,"[8] C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,
351"
REFERENCES,0.3617886178861789,"M. Joglekar, J. Leike, I. Sutskever, and J. Wu. Weak-to-strong generalization: Eliciting strong
352"
REFERENCES,0.36260162601626017,"capabilities with weak supervision, Dec. 2023.
353"
REFERENCES,0.36341463414634145,"[9] S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak,
354"
REFERENCES,0.3642276422764228,"D. Lindner, P. Freire, T. Wang, S. Marks, C.-R. Segerie, M. Carroll, A. Peng, P. Christoffersen,
355"
REFERENCES,0.3650406504065041,"M. Damani, S. Slocum, U. Anwar, A. Siththaranjan, M. Nadeau, E. J. Michaud, J. Pfau,
356"
REFERENCES,0.36585365853658536,"D. Krasheninnikov, X. Chen, L. Langosco, P. Hase, E. Bıyık, A. Dragan, D. Krueger, D. Sadigh,
357"
REFERENCES,0.36666666666666664,"and D. Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning
358"
REFERENCES,0.367479674796748,"from human feedback, Sept. 2023.
359"
REFERENCES,0.36829268292682926,"[10] S. Chithrananda, G. Grand, and B. Ramsundar. ChemBERTa: Large-Scale Self-Supervised
360"
REFERENCES,0.36910569105691055,"Pretraining for Molecular Property Prediction. In Machine Learning for Molecules Workshop at
361"
REFERENCES,0.3699186991869919,"NeurIPS, 2020.
362"
REFERENCES,0.37073170731707317,"[11] C. W. Coley, L. Rogers, W. H. Green, and K. F. Jensen. SCScore: Synthetic Complexity Learned
363"
REFERENCES,0.37154471544715445,"from a Reaction Corpus. Journal of Chemical Information and Modeling, 58(2):252–261, Feb.
364"
REFERENCES,0.3723577235772358,"2018. ISSN 1549-9596. doi: 10.1021/acs.jcim.7b00622.
365"
REFERENCES,0.37317073170731707,"[12] P. S. Gromski, A. B. Henson, J. M. Granda, and L. Cronin. How to explore chemical space
366"
REFERENCES,0.37398373983739835,"using algorithms and automation. Nature Reviews Chemistry, 3(2):119–128, 2019.
367"
REFERENCES,0.3747967479674797,"[13] R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling,
368"
REFERENCES,0.375609756097561,"D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Auto-
369"
REFERENCES,0.37642276422764226,"matic chemical design using a data-driven continuous representation of molecules. ACS Central
370"
REFERENCES,0.3772357723577236,"Science, 4(2):268–276, Feb. 2018. ISSN 2374-7943. doi: 10.1021/acscentsci.7b00572.
371"
REFERENCES,0.3780487804878049,"[14] J. Hartmann, M. Heitmann, C. Siebert, and C. Schamp. More than a feeling: Accuracy
372"
REFERENCES,0.37886178861788616,"and application of sentiment analysis. International Journal of Research in Marketing, 40
373"
REFERENCES,0.3796747967479675,"(1):75–87, 2023. doi: https://doi.org/10.1016/j.ijresmar.2022.05.005. URL https://www.
374"
REFERENCES,0.3804878048780488,"sciencedirect.com/science/article/pii/S0167811622000477.
375"
REFERENCES,0.38130081300813007,"[15] H. Inan, K. Upasani, J. Chi, R. Rungta, K. Iyer, Y. Mao, M. Tontchev, Q. Hu, B. Fuller,
376"
REFERENCES,0.3821138211382114,"D. Testuggine, and M. Khabsa. Llama Guard: LLM-based Input-Output Safeguard for Human-
377"
REFERENCES,0.3829268292682927,"AI Conversations, Dec. 2023.
378"
REFERENCES,0.383739837398374,"[16] G. M. Keserü and G. M. Makara. The influence of lead discovery strategies on the properties of
379"
REFERENCES,0.3845528455284553,"drug candidates. Nature Reviews Drug Discovery, 8(3):203–212, Mar. 2009. ISSN 1474-1776,
380"
REFERENCES,0.3853658536585366,"1474-1784. doi: 10.1038/nrd2796.
381"
REFERENCES,0.3861788617886179,"[17] R. K. Lindsay, B. G. Buchanan, E. A. Feigenbaum, and J. Lederberg. Dendral: A case study of
382"
REFERENCES,0.38699186991869916,"the first expert system for scientific hypothesis formation. Artificial Intelligence, 61(2):209–261,
383"
REFERENCES,0.3878048780487805,"June 1993. ISSN 00043702. doi: 10.1016/0004-3702(93)90068-M.
384"
REFERENCES,0.3886178861788618,"[18] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning Word Vectors
385"
REFERENCES,0.38943089430894307,"for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for
386"
REFERENCES,0.3902439024390244,"Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon,
387"
REFERENCES,0.3910569105691057,"USA, June 2011. Association for Computational Linguistics.
388"
REFERENCES,0.39186991869918697,"[19] J. Maas. Gradient flows of the entropy for finite Markov chains. Journal of Functional Analysis,
389"
REFERENCES,0.3926829268292683,"261(8):2250–2292, Oct. 2011. ISSN 0022-1236. doi: 10.1016/j.jfa.2011.06.009.
390"
REFERENCES,0.3934959349593496,"[20] R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland, Z. D. Guo, Y. Tang, M. Geist,
391"
REFERENCES,0.3943089430894309,"T. Mesnard, A. Michi, M. Selvi, S. Girgin, N. Momchev, O. Bachem, D. J. Mankowitz, D. Precup,
392"
REFERENCES,0.3951219512195122,"and B. Piot. Nash Learning from Human Feedback, Dec. 2023.
393"
REFERENCES,0.3959349593495935,"[21] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
394"
REFERENCES,0.3967479674796748,"K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,
395"
REFERENCES,0.3975609756097561,"P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with
396"
REFERENCES,0.3983739837398374,"human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
397"
REFERENCES,0.3991869918699187,"editors, Advances in Neural Information Processing Systems, volume 35, pages 27730–27744.
398"
REFERENCES,0.4,"Curran Associates, Inc., 2022.
399"
REFERENCES,0.4008130081300813,"[22] R. Park, R. Theisen, N. Sahni, M. Patek, A. Cicho´nska, and R. Rahman. Preference Optimization
400"
REFERENCES,0.4016260162601626,"for Molecular Language Models, Oct. 2023.
401"
REFERENCES,0.4024390243902439,"[23] G. Pesciullesi, P. Schwaller, T. Laino, and J.-L. Reymond. Transfer learning enables the
402"
REFERENCES,0.4032520325203252,"molecular transformer to predict regio- and stereoselective reactions on carbohydrates. Nature
403"
REFERENCES,0.4040650406504065,"Communications, 11(1):4874, Sept. 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-18671-7.
404"
REFERENCES,0.40487804878048783,"[24] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are
405"
REFERENCES,0.4056910569105691,"unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
406"
REFERENCES,0.4065040650406504,"[25] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference
407"
REFERENCES,0.4073170731707317,"optimization: Your language model is secretly a reward model.
In A. Oh, T. Neumann,
408"
REFERENCES,0.408130081300813,"A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information
409"
REFERENCES,0.4089430894308943,"Processing Systems, volume 36, pages 53728–53741. Curran Associates, Inc., 2023.
410"
REFERENCES,0.4097560975609756,"[26] D. J. Rogers and T. T. Tanimoto. A Computer Program for Classifying Plants: The computer is
411"
REFERENCES,0.4105691056910569,"programmed to simulate the taxonomic process of comparing each case with every other case.
412"
REFERENCES,0.4113821138211382,"Science, 132(3434):1115–1118, Oct. 1960. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.
413"
REFERENCES,0.4121951219512195,"132.3434.1115.
414"
REFERENCES,0.41300813008130083,"[27] B. Sanchez-Lengeling and A. Aspuru-Guzik. Inverse molecular design using machine learning:
415"
REFERENCES,0.4138211382113821,"Generative models for matter engineering. Science, 361(6400):360–365, July 2018. doi:
416"
REFERENCES,0.4146341463414634,"10.1126/science.aat2663.
417"
REFERENCES,0.41544715447154473,"[28] F. Santambrogio. {Euclidean, Metric, and Wasserstein} gradient flows: An overview. Bul-
418"
REFERENCES,0.416260162601626,"letin of Mathematical Sciences, 7(1):87–154, Apr. 2017. ISSN 1664-3615. doi: 10.1007/
419"
REFERENCES,0.4170731707317073,"s13373-017-0101-1.
420"
REFERENCES,0.41788617886178864,"[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal Policy Optimization
421"
REFERENCES,0.4186991869918699,"Algorithms, Aug. 2017.
422"
REFERENCES,0.4195121951219512,"[30] P. Schwaller, T. Gaudin, D. Lányi, C. Bekas, and T. Laino. “Found in Translation”: Predicting
423"
REFERENCES,0.42032520325203254,"outcomes of complex organic chemistry reactions using neural sequence-to-sequence models.
424"
REFERENCES,0.4211382113821138,"Chemical Science, 9(28):6091–6098, 2018. doi: 10.1039/C8SC02339E.
425"
REFERENCES,0.4219512195121951,"[31] P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter, C. Bekas, and A. A. Lee. Molecular
426"
REFERENCES,0.42276422764227645,"transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS Central
427"
REFERENCES,0.42357723577235773,"Science, 5(9):1572–1583, Sept. 2019. ISSN 2374-7943, 2374-7951. doi: 10.1021/acscentsci.
428"
REFERENCES,0.424390243902439,"9b00576.
429"
REFERENCES,0.42520325203252035,"[32] F. Tajwar, A. Singh, A. Sharma, R. Rafailov, J. Schneider, T. Xie, S. Ermon, C. Finn, and
430"
REFERENCES,0.42601626016260163,"A. Kumar. Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data,
431"
REFERENCES,0.4268292682926829,"Apr. 2024.
432"
REFERENCES,0.4276422764227642,"[33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
433"
REFERENCES,0.42845528455284554,"P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,
434"
REFERENCES,0.4292682926829268,"J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,
435"
REFERENCES,0.4300813008130081,"R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.
436"
REFERENCES,0.43089430894308944,"Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,
437"
REFERENCES,0.4317073170731707,"I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.
438"
REFERENCES,0.432520325203252,"Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,
439"
REFERENCES,0.43333333333333335,"I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and
440"
REFERENCES,0.43414634146341463,"T. Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.
441"
REFERENCES,0.4349593495934959,"[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
442"
REFERENCES,0.43577235772357725,"I. Polosukhin. Attention is All you Need. In Advances in Neural Information Processing
443"
REFERENCES,0.43658536585365854,"Systems, volume 30. Curran Associates, Inc., 2017.
444"
REFERENCES,0.4373983739837398,"[35] S. Wang, Y. Guo, Y. Wang, H. Sun, and J. Huang. SMILES-BERT: Large Scale Unsupervised
445"
REFERENCES,0.43821138211382116,"Pre-Training for Molecular Property Prediction. In Proceedings of the 10th ACM International
446"
REFERENCES,0.43902439024390244,"Conference on Bioinformatics, Computational Biology and Health Informatics, BCB ’19, pages
447"
REFERENCES,0.4398373983739837,"429–436, New York, NY, USA, Sept. 2019. Association for Computing Machinery. ISBN
448"
REFERENCES,0.44065040650406506,"978-1-4503-6666-3. doi: 10.1145/3307339.3342186.
449"
REFERENCES,0.44146341463414634,"[36] S. A. Wildman and G. M. Crippen. Prediction of Physicochemical Parameters by Atomic
450"
REFERENCES,0.44227642276422763,"Contributions. Journal of Chemical Information and Computer Sciences, 39(5):868–873, Sept.
451"
REFERENCES,0.44308943089430897,"1999. ISSN 0095-2338, 1520-5142. doi: 10.1021/ci990307l.
452"
REFERENCES,0.44390243902439025,"[37] B. Zdrazil, E. Felix, F. Hunter, E. J. Manners, J. Blackshaw, S. Corbett, M. de Veij, H. Ioannidis,
453"
REFERENCES,0.44471544715447153,"D. M. Lopez, J. F. Mosquera, M. P. Magarinos, N. Bosc, R. Arcila, T. Kizilören, A. Gaulton,
454"
REFERENCES,0.44552845528455287,"A. P. Bento, M. F. Adasme, P. Monecke, G. A. Landrum, and A. R. Leach. The ChEMBL
455"
REFERENCES,0.44634146341463415,"Database in 2023: A drug discovery platform spanning multiple bioactivity data types and
456"
REFERENCES,0.44715447154471544,"time periods. Nucleic Acids Research, 52(D1):D1180–D1192, Jan. 2024. ISSN 0305-1048,
457"
REFERENCES,0.4479674796747967,"1362-4962. doi: 10.1093/nar/gkad1004.
458"
REFERENCES,0.44878048780487806,"[38] R. Zhang, L. Lin, Y. Bai, and S. Mei. Negative Preference Optimization: From Catastrophic
459"
REFERENCES,0.44959349593495934,"Collapse to Effective Unlearning, Apr. 2024.
460"
REFERENCES,0.4504065040650406,"[39] Z. Zhou, J. Liu, C. Yang, J. Shao, Y. Liu, X. Yue, W. Ouyang, and Y. Qiao. Beyond One-
461"
REFERENCES,0.45121951219512196,"Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization, Dec. 2023.
462"
REFERENCES,0.45203252032520325,"NeurIPS Paper Checklist
463"
CLAIMS,0.45284552845528453,"1. Claims
464"
CLAIMS,0.45365853658536587,"Question: Do the main claims made in the abstract and introduction accurately reflect the
465"
CLAIMS,0.45447154471544715,"paper’s contributions and scope?
466"
CLAIMS,0.45528455284552843,"Answer: [Yes]
467"
CLAIMS,0.4560975609756098,"Justification: The abstract clearly outlines the problem of searching over vast chemical
468"
CLAIMS,0.45691056910569106,"spaces as an alignment problem, gives a high-level overview of our method, highlights
469"
CLAIMS,0.45772357723577234,"the key differences between our method and similar approaches, and summarizes the key
470"
CLAIMS,0.4585365853658537,"advantages of our algorithm. We clearly define the current scope of our investigation by
471"
CLAIMS,0.45934959349593496,"describing our experiments.
472"
CLAIMS,0.46016260162601624,"Guidelines:
473"
CLAIMS,0.4609756097560976,"• The answer NA means that the abstract and introduction do not include the claims
474"
CLAIMS,0.46178861788617886,"made in the paper.
475"
CLAIMS,0.46260162601626015,"• The abstract and/or introduction should clearly state the claims made, including the
476"
CLAIMS,0.4634146341463415,"contributions made in the paper and important assumptions and limitations. A No or
477"
CLAIMS,0.46422764227642277,"NA answer to this question will not be perceived well by the reviewers.
478"
CLAIMS,0.46504065040650405,"• The claims made should match theoretical and experimental results, and reflect how
479"
CLAIMS,0.4658536585365854,"much the results can be expected to generalize to other settings.
480"
CLAIMS,0.4666666666666667,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
481"
CLAIMS,0.46747967479674796,"are not attained by the paper.
482"
LIMITATIONS,0.4682926829268293,"2. Limitations
483"
LIMITATIONS,0.4691056910569106,"Question: Does the paper discuss the limitations of the work performed by the authors?
484"
LIMITATIONS,0.46991869918699186,"Answer: [Yes]
485"
LIMITATIONS,0.47073170731707314,"Justification: We address several potential limitations of our work at the end of Section 5.
486"
LIMITATIONS,0.4715447154471545,"Guidelines:
487"
LIMITATIONS,0.47235772357723577,"• The answer NA means that the paper has no limitation while the answer No means that
488"
LIMITATIONS,0.47317073170731705,"the paper has limitations, but those are not discussed in the paper.
489"
LIMITATIONS,0.4739837398373984,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
490"
LIMITATIONS,0.47479674796747967,"• The paper should point out any strong assumptions and how robust the results are to
491"
LIMITATIONS,0.47560975609756095,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
492"
LIMITATIONS,0.4764227642276423,"model well-specification, asymptotic approximations only holding locally). The authors
493"
LIMITATIONS,0.4772357723577236,"should reflect on how these assumptions might be violated in practice and what the
494"
LIMITATIONS,0.47804878048780486,"implications would be.
495"
LIMITATIONS,0.4788617886178862,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
496"
LIMITATIONS,0.4796747967479675,"only tested on a few datasets or with a few runs. In general, empirical results often
497"
LIMITATIONS,0.48048780487804876,"depend on implicit assumptions, which should be articulated.
498"
LIMITATIONS,0.4813008130081301,"• The authors should reflect on the factors that influence the performance of the approach.
499"
LIMITATIONS,0.4821138211382114,"For example, a facial recognition algorithm may perform poorly when image resolution
500"
LIMITATIONS,0.48292682926829267,"is low or images are taken in low lighting. Or a speech-to-text system might not be
501"
LIMITATIONS,0.483739837398374,"used reliably to provide closed captions for online lectures because it fails to handle
502"
LIMITATIONS,0.4845528455284553,"technical jargon.
503"
LIMITATIONS,0.4853658536585366,"• The authors should discuss the computational efficiency of the proposed algorithms
504"
LIMITATIONS,0.4861788617886179,"and how they scale with dataset size.
505"
LIMITATIONS,0.4869918699186992,"• If applicable, the authors should discuss possible limitations of their approach to
506"
LIMITATIONS,0.4878048780487805,"address problems of privacy and fairness.
507"
LIMITATIONS,0.4886178861788618,"• While the authors might fear that complete honesty about limitations might be used by
508"
LIMITATIONS,0.4894308943089431,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
509"
LIMITATIONS,0.4902439024390244,"limitations that aren’t acknowledged in the paper. The authors should use their best
510"
LIMITATIONS,0.49105691056910566,"judgment and recognize that individual actions in favor of transparency play an impor-
511"
LIMITATIONS,0.491869918699187,"tant role in developing norms that preserve the integrity of the community. Reviewers
512"
LIMITATIONS,0.4926829268292683,"will be specifically instructed to not penalize honesty concerning limitations.
513"
THEORY ASSUMPTIONS AND PROOFS,0.49349593495934957,"3. Theory Assumptions and Proofs
514"
THEORY ASSUMPTIONS AND PROOFS,0.4943089430894309,"Question: For each theoretical result, does the paper provide the full set of assumptions and
515"
THEORY ASSUMPTIONS AND PROOFS,0.4951219512195122,"a complete (and correct) proof?
516"
THEORY ASSUMPTIONS AND PROOFS,0.4959349593495935,"Answer: [Yes]
517"
THEORY ASSUMPTIONS AND PROOFS,0.4967479674796748,"Justification: Section 3 provides a thorough theoretical analysis of the ERA algorithm, and
518"
THEORY ASSUMPTIONS AND PROOFS,0.4975609756097561,"further details are in Section A of the Appendix.
519"
THEORY ASSUMPTIONS AND PROOFS,0.4983739837398374,"Guidelines:
520"
THEORY ASSUMPTIONS AND PROOFS,0.4991869918699187,"• The answer NA means that the paper does not include theoretical results.
521"
THEORY ASSUMPTIONS AND PROOFS,0.5,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
522"
THEORY ASSUMPTIONS AND PROOFS,0.5008130081300813,"referenced.
523"
THEORY ASSUMPTIONS AND PROOFS,0.5016260162601626,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
524"
THEORY ASSUMPTIONS AND PROOFS,0.5024390243902439,"• The proofs can either appear in the main paper or the supplemental material, but if
525"
THEORY ASSUMPTIONS AND PROOFS,0.5032520325203252,"they appear in the supplemental material, the authors are encouraged to provide a short
526"
THEORY ASSUMPTIONS AND PROOFS,0.5040650406504065,"proof sketch to provide intuition.
527"
THEORY ASSUMPTIONS AND PROOFS,0.5048780487804878,"• Inversely, any informal proof provided in the core of the paper should be complemented
528"
THEORY ASSUMPTIONS AND PROOFS,0.5056910569105691,"by formal proofs provided in appendix or supplemental material.
529"
THEORY ASSUMPTIONS AND PROOFS,0.5065040650406504,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5073170731707317,"4. Experimental Result Reproducibility
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.508130081300813,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5089430894308943,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5097560975609756,"of the paper (regardless of whether the code and data are provided or not)?
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.510569105691057,"Answer: [Yes]
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5113821138211382,"Justification: Between Section 4 (Experiments) and Sections C and D of the Appendix
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5121951219512195,"(Details for molecular generator experiments and Details for LLM experiments, respectively),
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5130081300813009,"we describe the experimental procedure used to obtain each result.
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5138211382113821,"Guidelines:
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5146341463414634,"• The answer NA means that the paper does not include experiments.
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5154471544715448,"• If the paper includes experiments, a No answer to this question will not be perceived
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.516260162601626,"well by the reviewers: Making the paper reproducible is important, regardless of
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5170731707317073,"whether the code and data are provided or not.
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5178861788617887,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5186991869918699,"to make their results reproducible or verifiable.
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5195121951219512,"• Depending on the contribution, reproducibility can be accomplished in various ways.
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5203252032520326,"For example, if the contribution is a novel architecture, describing the architecture fully
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5211382113821138,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5219512195121951,"be necessary to either make it possible for others to replicate the model with the same
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5227642276422764,"dataset, or provide access to the model. In general. releasing code and data is often
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5235772357723577,"one good way to accomplish this, but reproducibility can also be provided via detailed
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.524390243902439,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5252032520325203,"of a large language model), releasing of a model checkpoint, or other means that are
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5260162601626016,"appropriate to the research performed.
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.526829268292683,"• While NeurIPS does not require releasing code, the conference does require all submis-
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5276422764227642,"sions to provide some reasonable avenue for reproducibility, which may depend on the
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5284552845528455,"nature of the contribution. For example
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5292682926829269,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5300813008130081,"to reproduce that algorithm.
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5308943089430894,"(b) If the contribution is primarily a new model architecture, the paper should describe
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5317073170731708,"the architecture clearly and fully.
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.532520325203252,"(c) If the contribution is a new model (e.g., a large language model), then there should
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5333333333333333,"either be a way to access this model for reproducing the results or a way to reproduce
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5341463414634147,"the model (e.g., with an open-source dataset or instructions for how to construct
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5349593495934959,"the dataset).
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5357723577235772,"(d) We recognize that reproducibility may be tricky in some cases, in which case
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5365853658536586,"authors are welcome to describe the particular way they provide for reproducibility.
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5373983739837398,"In the case of closed-source models, it may be that access to the model is limited in
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5382113821138211,"some way (e.g., to registered users), but it should be possible for other researchers
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5390243902439025,"to have some path to reproducing or verifying the results.
570"
OPEN ACCESS TO DATA AND CODE,0.5398373983739837,"5. Open access to data and code
571"
OPEN ACCESS TO DATA AND CODE,0.540650406504065,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
572"
OPEN ACCESS TO DATA AND CODE,0.5414634146341464,"tions to faithfully reproduce the main experimental results, as described in supplemental
573"
OPEN ACCESS TO DATA AND CODE,0.5422764227642276,"material?
574"
OPEN ACCESS TO DATA AND CODE,0.5430894308943089,"Answer: [Yes]
575"
OPEN ACCESS TO DATA AND CODE,0.5439024390243903,"Justification: We will release the code as open source upon submission of the paper. The
576"
OPEN ACCESS TO DATA AND CODE,0.5447154471544715,"models used for the LLM experiments, such as LLaMA2 and GPT-2, are already available
577"
OPEN ACCESS TO DATA AND CODE,0.5455284552845528,"online via Huggingface.
578"
OPEN ACCESS TO DATA AND CODE,0.5463414634146342,"Guidelines:
579"
OPEN ACCESS TO DATA AND CODE,0.5471544715447154,"• The answer NA means that paper does not include experiments requiring code.
580"
OPEN ACCESS TO DATA AND CODE,0.5479674796747968,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
581"
OPEN ACCESS TO DATA AND CODE,0.5487804878048781,"public/guides/CodeSubmissionPolicy) for more details.
582"
OPEN ACCESS TO DATA AND CODE,0.5495934959349593,"• While we encourage the release of code and data, we understand that this might not be
583"
OPEN ACCESS TO DATA AND CODE,0.5504065040650407,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
584"
OPEN ACCESS TO DATA AND CODE,0.551219512195122,"including code, unless this is central to the contribution (e.g., for a new open-source
585"
OPEN ACCESS TO DATA AND CODE,0.5520325203252032,"benchmark).
586"
OPEN ACCESS TO DATA AND CODE,0.5528455284552846,"• The instructions should contain the exact command and environment needed to run to
587"
OPEN ACCESS TO DATA AND CODE,0.5536585365853659,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
588"
OPEN ACCESS TO DATA AND CODE,0.5544715447154471,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
589"
OPEN ACCESS TO DATA AND CODE,0.5552845528455285,"• The authors should provide instructions on data access and preparation, including how
590"
OPEN ACCESS TO DATA AND CODE,0.5560975609756098,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
591"
OPEN ACCESS TO DATA AND CODE,0.556910569105691,"• The authors should provide scripts to reproduce all experimental results for the new
592"
OPEN ACCESS TO DATA AND CODE,0.5577235772357724,"proposed method and baselines. If only a subset of experiments are reproducible, they
593"
OPEN ACCESS TO DATA AND CODE,0.5585365853658537,"should state which ones are omitted from the script and why.
594"
OPEN ACCESS TO DATA AND CODE,0.5593495934959349,"• At submission time, to preserve anonymity, the authors should release anonymized
595"
OPEN ACCESS TO DATA AND CODE,0.5601626016260163,"versions (if applicable).
596"
OPEN ACCESS TO DATA AND CODE,0.5609756097560976,"• Providing as much information as possible in supplemental material (appended to the
597"
OPEN ACCESS TO DATA AND CODE,0.5617886178861788,"paper) is recommended, but including URLs to data and code is permitted.
598"
OPEN ACCESS TO DATA AND CODE,0.5626016260162602,"6. Experimental Setting/Details
599"
OPEN ACCESS TO DATA AND CODE,0.5634146341463414,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
600"
OPEN ACCESS TO DATA AND CODE,0.5642276422764227,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
601"
OPEN ACCESS TO DATA AND CODE,0.5650406504065041,"results?
602"
OPEN ACCESS TO DATA AND CODE,0.5658536585365853,"Answer: [Yes]
603"
OPEN ACCESS TO DATA AND CODE,0.5666666666666667,"Justification: The exact settings and hyperparameters used for training and testing are
604"
OPEN ACCESS TO DATA AND CODE,0.567479674796748,"provided in descriptions of the experiments in Section 4 and in Sections C and D of the
605"
OPEN ACCESS TO DATA AND CODE,0.5682926829268292,"Appendix.
606"
OPEN ACCESS TO DATA AND CODE,0.5691056910569106,"Guidelines:
607"
OPEN ACCESS TO DATA AND CODE,0.5699186991869919,"• The answer NA means that the paper does not include experiments.
608"
OPEN ACCESS TO DATA AND CODE,0.5707317073170731,"• The experimental setting should be presented in the core of the paper to a level of detail
609"
OPEN ACCESS TO DATA AND CODE,0.5715447154471545,"that is necessary to appreciate the results and make sense of them.
610"
OPEN ACCESS TO DATA AND CODE,0.5723577235772358,"• The full details can be provided either with the code, in appendix, or as supplemental
611"
OPEN ACCESS TO DATA AND CODE,0.573170731707317,"material.
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5739837398373984,"7. Experiment Statistical Significance
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5747967479674797,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5756097560975609,"information about the statistical significance of the experiments?
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5764227642276423,"Answer: [Yes]
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5772357723577236,"Justification: The statistical significance of all the results is discussed throughout the body
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5780487804878048,"of the paper in Section 4 and in Sections C and D of the Appendix.
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5788617886178862,"Guidelines:
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5796747967479675,"• The answer NA means that the paper does not include experiments.
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5804878048780487,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5813008130081301,"dence intervals, or statistical significance tests, at least for the experiments that support
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5821138211382114,"the main claims of the paper.
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5829268292682926,"• The factors of variability that the error bars are capturing should be clearly stated (for
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.583739837398374,"example, train/test split, initialization, random drawing of some parameter, or overall
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5845528455284553,"run with given experimental conditions).
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5853658536585366,"• The method for calculating the error bars should be explained (closed form formula,
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5861788617886179,"call to a library function, bootstrap, etc.)
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5869918699186992,"• The assumptions made should be given (e.g., Normally distributed errors).
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5878048780487805,"• It should be clear whether the error bar is the standard deviation or the standard error
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5886178861788618,"of the mean.
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5894308943089431,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5902439024390244,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5910569105691057,"of Normality of errors is not verified.
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.591869918699187,"• For asymmetric distributions, the authors should be careful not to show in tables or
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5926829268292683,"figures symmetric error bars that would yield results that are out of range (e.g. negative
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5934959349593496,"error rates).
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5943089430894309,"• If error bars are reported in tables or plots, The authors should explain in the text how
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.5951219512195122,"they were calculated and reference the corresponding figures or tables in the text.
639"
EXPERIMENTS COMPUTE RESOURCES,0.5959349593495935,"8. Experiments Compute Resources
640"
EXPERIMENTS COMPUTE RESOURCES,0.5967479674796748,"Question: For each experiment, does the paper provide sufficient information on the com-
641"
EXPERIMENTS COMPUTE RESOURCES,0.5975609756097561,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
642"
EXPERIMENTS COMPUTE RESOURCES,0.5983739837398374,"the experiments?
643"
EXPERIMENTS COMPUTE RESOURCES,0.5991869918699188,"Answer: [Yes]
644"
EXPERIMENTS COMPUTE RESOURCES,0.6,"Justification: Computational resources used are discussed in Section E of the Appendix.
645"
EXPERIMENTS COMPUTE RESOURCES,0.6008130081300813,"Guidelines:
646"
EXPERIMENTS COMPUTE RESOURCES,0.6016260162601627,"• The answer NA means that the paper does not include experiments.
647"
EXPERIMENTS COMPUTE RESOURCES,0.6024390243902439,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
648"
EXPERIMENTS COMPUTE RESOURCES,0.6032520325203252,"or cloud provider, including relevant memory and storage.
649"
EXPERIMENTS COMPUTE RESOURCES,0.6040650406504066,"• The paper should provide the amount of compute required for each of the individual
650"
EXPERIMENTS COMPUTE RESOURCES,0.6048780487804878,"experimental runs as well as estimate the total compute.
651"
EXPERIMENTS COMPUTE RESOURCES,0.6056910569105691,"• The paper should disclose whether the full research project required more compute
652"
EXPERIMENTS COMPUTE RESOURCES,0.6065040650406504,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
653"
EXPERIMENTS COMPUTE RESOURCES,0.6073170731707317,"didn’t make it into the paper).
654"
CODE OF ETHICS,0.608130081300813,"9. Code Of Ethics
655"
CODE OF ETHICS,0.6089430894308943,"Question: Does the research conducted in the paper conform, in every respect, with the
656"
CODE OF ETHICS,0.6097560975609756,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
657"
CODE OF ETHICS,0.6105691056910569,"Answer: [Yes]
658"
CODE OF ETHICS,0.6113821138211382,"Justification: Yes, the work is compliant with the NeurIPS Code of Ethics.
659"
CODE OF ETHICS,0.6121951219512195,"Guidelines:
660"
CODE OF ETHICS,0.6130081300813008,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
661"
CODE OF ETHICS,0.6138211382113821,"• If the authors answer No, they should explain the special circumstances that require a
662"
CODE OF ETHICS,0.6146341463414634,"deviation from the Code of Ethics.
663"
CODE OF ETHICS,0.6154471544715447,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
664"
CODE OF ETHICS,0.616260162601626,"eration due to laws or regulations in their jurisdiction).
665"
BROADER IMPACTS,0.6170731707317073,"10. Broader Impacts
666"
BROADER IMPACTS,0.6178861788617886,"Question: Does the paper discuss both potential positive societal impacts and negative
667"
BROADER IMPACTS,0.6186991869918699,"societal impacts of the work performed?
668"
BROADER IMPACTS,0.6195121951219512,"Answer: [Yes]
669"
BROADER IMPACTS,0.6203252032520326,"Justification: The broader societal impacts of this work is discussed in Section F.
670"
BROADER IMPACTS,0.6211382113821138,"Guidelines:
671"
BROADER IMPACTS,0.6219512195121951,"• The answer NA means that there is no societal impact of the work performed.
672"
BROADER IMPACTS,0.6227642276422765,"• If the authors answer NA or No, they should explain why their work has no societal
673"
BROADER IMPACTS,0.6235772357723577,"impact or why the paper does not address societal impact.
674"
BROADER IMPACTS,0.624390243902439,"• Examples of negative societal impacts include potential malicious or unintended uses
675"
BROADER IMPACTS,0.6252032520325204,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
676"
BROADER IMPACTS,0.6260162601626016,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
677"
BROADER IMPACTS,0.6268292682926829,"groups), privacy considerations, and security considerations.
678"
BROADER IMPACTS,0.6276422764227643,"• The conference expects that many papers will be foundational research and not tied
679"
BROADER IMPACTS,0.6284552845528455,"to particular applications, let alone deployments. However, if there is a direct path to
680"
BROADER IMPACTS,0.6292682926829268,"any negative applications, the authors should point it out. For example, it is legitimate
681"
BROADER IMPACTS,0.6300813008130082,"to point out that an improvement in the quality of generative models could be used to
682"
BROADER IMPACTS,0.6308943089430894,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
683"
BROADER IMPACTS,0.6317073170731707,"that a generic algorithm for optimizing neural networks could enable people to train
684"
BROADER IMPACTS,0.6325203252032521,"models that generate Deepfakes faster.
685"
BROADER IMPACTS,0.6333333333333333,"• The authors should consider possible harms that could arise when the technology is
686"
BROADER IMPACTS,0.6341463414634146,"being used as intended and functioning correctly, harms that could arise when the
687"
BROADER IMPACTS,0.634959349593496,"technology is being used as intended but gives incorrect results, and harms following
688"
BROADER IMPACTS,0.6357723577235772,"from (intentional or unintentional) misuse of the technology.
689"
BROADER IMPACTS,0.6365853658536585,"• If there are negative societal impacts, the authors could also discuss possible mitigation
690"
BROADER IMPACTS,0.6373983739837399,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
691"
BROADER IMPACTS,0.6382113821138211,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
692"
BROADER IMPACTS,0.6390243902439025,"feedback over time, improving the efficiency and accessibility of ML).
693"
SAFEGUARDS,0.6398373983739838,"11. Safeguards
694"
SAFEGUARDS,0.640650406504065,"Question: Does the paper describe safeguards that have been put in place for responsible
695"
SAFEGUARDS,0.6414634146341464,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
696"
SAFEGUARDS,0.6422764227642277,"image generators, or scraped datasets)?
697"
SAFEGUARDS,0.6430894308943089,"Answer: [Yes]
698"
SAFEGUARDS,0.6439024390243903,"Justification: We use public data and models for all LLM experiments. There are no safety
699"
SAFEGUARDS,0.6447154471544716,"concerns with the chemical alignment models.
700"
SAFEGUARDS,0.6455284552845528,"Guidelines:
701"
SAFEGUARDS,0.6463414634146342,"• The answer NA means that the paper poses no such risks.
702"
SAFEGUARDS,0.6471544715447154,"• Released models that have a high risk for misuse or dual-use should be released with
703"
SAFEGUARDS,0.6479674796747967,"necessary safeguards to allow for controlled use of the model, for example by requiring
704"
SAFEGUARDS,0.6487804878048781,"that users adhere to usage guidelines or restrictions to access the model or implementing
705"
SAFEGUARDS,0.6495934959349593,"safety filters.
706"
SAFEGUARDS,0.6504065040650406,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
707"
SAFEGUARDS,0.651219512195122,"should describe how they avoided releasing unsafe images.
708"
SAFEGUARDS,0.6520325203252032,"• We recognize that providing effective safeguards is challenging, and many papers do
709"
SAFEGUARDS,0.6528455284552845,"not require this, but we encourage authors to take this into account and make a best
710"
SAFEGUARDS,0.6536585365853659,"faith effort.
711"
LICENSES FOR EXISTING ASSETS,0.6544715447154471,"12. Licenses for existing assets
712"
LICENSES FOR EXISTING ASSETS,0.6552845528455284,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
713"
LICENSES FOR EXISTING ASSETS,0.6560975609756098,"the paper, properly credited and are the license and terms of use explicitly mentioned and
714"
LICENSES FOR EXISTING ASSETS,0.656910569105691,"properly respected?
715"
LICENSES FOR EXISTING ASSETS,0.6577235772357723,"Answer: [Yes]
716"
LICENSES FOR EXISTING ASSETS,0.6585365853658537,"Justification: Yes, we cite the appropriate works throughout the main text and the Appendix.
717"
LICENSES FOR EXISTING ASSETS,0.6593495934959349,"Guidelines:
718"
LICENSES FOR EXISTING ASSETS,0.6601626016260163,"• The answer NA means that the paper does not use existing assets.
719"
LICENSES FOR EXISTING ASSETS,0.6609756097560976,"• The authors should cite the original paper that produced the code package or dataset.
720"
LICENSES FOR EXISTING ASSETS,0.6617886178861788,"• The authors should state which version of the asset is used and, if possible, include a
721"
LICENSES FOR EXISTING ASSETS,0.6626016260162602,"URL.
722"
LICENSES FOR EXISTING ASSETS,0.6634146341463415,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
723"
LICENSES FOR EXISTING ASSETS,0.6642276422764227,"• For scraped data from a particular source (e.g., website), the copyright and terms of
724"
LICENSES FOR EXISTING ASSETS,0.6650406504065041,"service of that source should be provided.
725"
LICENSES FOR EXISTING ASSETS,0.6658536585365854,"• If assets are released, the license, copyright information, and terms of use in the
726"
LICENSES FOR EXISTING ASSETS,0.6666666666666666,"package should be provided. For popular datasets, paperswithcode.com/datasets
727"
LICENSES FOR EXISTING ASSETS,0.667479674796748,"has curated licenses for some datasets. Their licensing guide can help determine the
728"
LICENSES FOR EXISTING ASSETS,0.6682926829268293,"license of a dataset.
729"
LICENSES FOR EXISTING ASSETS,0.6691056910569105,"• For existing datasets that are re-packaged, both the original license and the license of
730"
LICENSES FOR EXISTING ASSETS,0.6699186991869919,"the derived asset (if it has changed) should be provided.
731"
LICENSES FOR EXISTING ASSETS,0.6707317073170732,"• If this information is not available online, the authors are encouraged to reach out to
732"
LICENSES FOR EXISTING ASSETS,0.6715447154471544,"the asset’s creators.
733"
NEW ASSETS,0.6723577235772358,"13. New Assets
734"
NEW ASSETS,0.6731707317073171,"Question: Are new assets introduced in the paper well documented and is the documentation
735"
NEW ASSETS,0.6739837398373983,"provided alongside the assets?
736"
NEW ASSETS,0.6747967479674797,"Answer: [Yes]
737"
NEW ASSETS,0.675609756097561,"Justification: The code will be released as open-source and will have adequate documentation
738"
NEW ASSETS,0.6764227642276422,"for reproducing experiments.
739"
NEW ASSETS,0.6772357723577236,"Guidelines:
740"
NEW ASSETS,0.6780487804878049,"• The answer NA means that the paper does not release new assets.
741"
NEW ASSETS,0.6788617886178862,"• Researchers should communicate the details of the dataset/code/model as part of their
742"
NEW ASSETS,0.6796747967479675,"submissions via structured templates. This includes details about training, license,
743"
NEW ASSETS,0.6804878048780488,"limitations, etc.
744"
NEW ASSETS,0.6813008130081301,"• The paper should discuss whether and how consent was obtained from people whose
745"
NEW ASSETS,0.6821138211382114,"asset is used.
746"
NEW ASSETS,0.6829268292682927,"• At submission time, remember to anonymize your assets (if applicable). You can either
747"
NEW ASSETS,0.683739837398374,"create an anonymized URL or include an anonymized zip file.
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6845528455284553,"14. Crowdsourcing and Research with Human Subjects
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6853658536585366,"Question: For crowdsourcing experiments and research with human subjects, does the paper
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6861788617886179,"include the full text of instructions given to participants and screenshots, if applicable, as
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6869918699186992,"well as details about compensation (if any)?
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6878048780487804,"Answer: [NA]
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6886178861788618,"Justification: We did not use human subjects.
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6894308943089431,"Guidelines:
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6902439024390243,"• The answer NA means that the paper does not involve crowdsourcing nor research with
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6910569105691057,"human subjects.
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.691869918699187,"• Including this information in the supplemental material is fine, but if the main contribu-
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6926829268292682,"tion of the paper involves human subjects, then as much detail as possible should be
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6934959349593496,"included in the main paper.
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6943089430894309,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6951219512195121,"or other labor should be paid at least the minimum wage in the country of the data
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6959349593495935,"collector.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6967479674796748,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.697560975609756,"Subjects
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6983739837398374,"Question: Does the paper describe potential risks incurred by study participants, whether
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.6991869918699187,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7,"approvals (or an equivalent approval/review based on the requirements of your country or
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7008130081300813,"institution) were obtained?
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7016260162601626,"Answer: [NA]
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7024390243902439,"Justification: We did not use human subjects in this work.
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7032520325203252,"Guidelines:
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7040650406504065,"• The answer NA means that the paper does not involve crowdsourcing nor research with
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7048780487804878,"human subjects.
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7056910569105691,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7065040650406504,"may be required for any human subjects research. If you obtained IRB approval, you
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7073170731707317,"should clearly state this in the paper.
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.708130081300813,"• We recognize that the procedures for this may vary significantly between institutions
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7089430894308943,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7097560975609756,"guidelines for their institution.
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7105691056910569,"• For initial submissions, do not include any information that would break anonymity (if
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7113821138211383,"applicable), such as the institution conducting the review.
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7121951219512195,"A
Detailed Theoretical Analysis
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7130081300813008,"Set-up, notation, and assumptions
Let X and Y be discrete spaces; each element of one of these
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7138211382113822,"spaces is a finite-length sequence of tokens within a fixed dictionary on which an autoregressive
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7146341463414634,"generative model is trained. The resulting models yield “policies”, which are conditional probability
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7154471544715447,"distributions π(·|x) ∈P(Y) for each x ∈X. Throughout, we assume that our policies have
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7162601626016261,"full support on Y for each x, meaning that infy,x∈Y×X π(y|x) > 0. Because the spaces are
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7170731707317073,"discrete, we make no strong restrictions on the regularity or coerciveness of the reward model
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7178861788617886,"−U : X × Y →R. The only requirement to ensure the existence of an optimal probability
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.71869918699187,"distribution is that supx,y×X×Y |e−U(x,y)| < +∞, which maintains full support of the distribution.
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7195121951219512,"Though it plays little role in theoretical analysis, we also denote by ν ∈P(X) the probability
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7203252032520325,"distribution over the prompts x.
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7211382113821139,"Goals of the analysis presented here
The main purpose of this section is to establish that globally
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7219512195121951,"minimizing the loss (9) yields a global minimizer of the regularized policy objective (2). A secondary
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7227642276422764,"goal is to clearly articulate the theoretical advantages of ERA compared with PPO and DPO.
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7235772357723578,"To understand the ERA loss function and its connection to the entropy regularized objective (2), we
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.724390243902439,"first establish that the minimizer of (6) are of the form (5). We first define the notion of equivalence
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7252032520325203,"precisely.
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7260162601626017,"Definition A.1 The conditional probability measures π(·|x) and π′(·|x) in P(Y) are conditionally
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7268292682926829,"equivalent if ∀x ∈X, π and π′ are such that supy∈Y |π(y|x) −π′(y|x)| = 0.
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7276422764227642,"This is a strong form of equivalence for probability measures, but it is appropriate on the discrete
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7284552845528456,"spaces X and Y we consider here. For more general continuous spaces, one could relax this condition
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7292682926829268,"to weak equivalence of the conditional measures. We use this notion to emphasize that a shift of
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7300813008130081,"the distribution of the “prompts” x ∈X, which we denote ν ∈P(X), does not impact conditional
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7308943089430894,"equivalence and hence establishes an equivalence class of conditional probability measures that
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7317073170731707,"minimize (2).
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.732520325203252,"Lemma A.1 If π is conditionally equivalent to π′, then π′
g(·|x) ∝π′(·|x)eg(x) is conditionally
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7333333333333333,"equivalent to π for all functions g : X →R such that supx∈X |eg(x)| < +∞.
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7341463414634146,"Assume that π′ is a normalized probability distribution. This requires that,
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.734959349593496,"Z′(x) =
X"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7357723577235772,"y∈Y
π′(y|x) = 1.
(15)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7365853658536585,"If g is such that
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7373983739837399,"Z′
g(x) =
X"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7382113821138211,"y∈Y
π′(y|x)eg(x) ̸= 1,
(16)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7390243902439024,"then the normalized policy π′
g is clearly defined by
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7398373983739838,"1
Z′g(x)π′(y|x)eg(x) ≡π′(y|x),
(17)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.740650406504065,"because Z′
g(x) = eg(x). By the assumption that supx∈X |eg(x)| < +∞, all terms in these calcula-
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7414634146341463,"tions remain finite.
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7422764227642277,"Using Lemma A.1 it is straightforward to prove the result in the main text Proposition 3.2. For
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7430894308943089,"completeness, we re-state that result here and refer the reader to the main text for the complete
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7439024390243902,"argument.
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7447154471544716,"Proposition A.2 Suppose π(·|x) ∈P(Y) and that supp(π) = supp(πref). Let β > 0, γ ≥0 and
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7455284552845528,"that the reward model is such that supx,y∈X×Y |e−U(x,y)| < +∞. Then, the minimizer of LERA is
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7463414634146341,"conditionally equivalent to π⋆.
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7471544715447155,"This proposition establishes that a policy minimizing the objective
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7479674796747967,"LERA(πθ) = Ex∼DEy,y′∼πref(·|x)D(y,y′)
KL
(pβ|pθ);"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.748780487804878,"pθ := σ

log πθ(y|x)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7495934959349594,πθ(y′|x) 
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7504065040650406,"pγ := σ

β
1 + γ"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.751219512195122,"
(U(x, y′) −U(x, y)) + β−1γ log πref(y|x)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7520325203252033,"πref(y′|x) 
, (18)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7528455284552845,"has the form
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7536585365853659,"π⋆(y|x) = Z−1(x) exp

−
β
1 + γ
 
U(x, y) −β−1γ log πref(y|x)

.
(19)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7544715447154472,"We do not, however, prove that gradient descent of θ on (18) converges to the global minimizer (19)
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7552845528455284,"because such an argument requires additional assumptions about the parametric class of policies and
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7560975609756098,"the convexity of the objective with respect to the parameters, neither of which are straightforward to
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7569105691056911,"establish.
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7577235772357723,"A.1
Comparison with PPO Objective
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7585365853658537,"The free energy functional for a policy under the energy rank alignment framework can be written as
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.759349593495935,"an expectation
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7601626016260162,JERA[π] = Ex∼ν
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7609756097560976,"Z
U(x, y)dπ(y|x) + β−1
Z
(1 + γ) log π(y|x) −γ log(πref(y|x)dπ(y|x)

,"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7617886178861789,"(20)
involving an energetic term and an entropic term. The additional regularization acts as an effective
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7626016260162601,"energetic bias. Solving for the extremum of this functional by setting Fréchet derivative with respect
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7634146341463415,"to π equal to zero, one obtains the formal solution (19) for the minimizer. This objective differs from
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7642276422764228,"the regularized reward loss conventionally used for PPO,
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.765040650406504,JPPO(π) = Ex
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7658536585365854,"Z
U(x, y)dπ(y|x) + γβ−1
Z
log π(y|x)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7666666666666667,"πref(y|x)dπ(y|x)

, = Ex"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.767479674796748,"Z
U(x, y)dπ(y|x) + γβ−1DKL
 
π(·|x)|πref(·|x)

.
(21)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7682926829268293,"The minimizer of the PPO objective (21) is also a Gibbs-Boltzmann measure, explicitly,
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7691056910569106,"π(PPO)
⋆
∝exp

−β"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7699186991869919,"γ U(x, y) + log πref(y|x)

.
(22)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7707317073170732,"Here, the KL-regularization corresponds to an energy shift, as in our objective, but there is no limit in
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7715447154471544,"which the ideal distribution π ∝e−βU is obtained for the PPO objective. This is in stark contrast
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7723577235772358,"to our approach, which recovers the ideal distribution as γ →0. Furthermore, while our approach
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7731707317073171,"allows for a direct gradient-based optimization using (18), PPO is implemented using an actor-critic
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7739837398373983,"framework that is difficult to tune [25, 9]. Finally, we emphasize that for ERA in the γ →0, finite
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7747967479674797,"β > 0, the distribution has positive entropy and is not manifestly mode-seeking; there can still be
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.775609756097561,"appreciable fluctuations in the output. Eliminating the effect of regularization in (22), on the other
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7764227642276422,"hand, requires taking β/γ →∞, which eliminates fluctuations in the distribution.
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7772357723577236,"A.2
Comparison with DPO Objective
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7780487804878049,"The DPO approach also seeks to optimize the objective (21). The algorithm does so by first using (22)
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7788617886178861,"to define an implicit reward model by solving for the U that reflects the observed preference probabil-
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7796747967479675,"ities. This elegant idea has had a significant impact and has already been deployed in state-of-the-art
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7804878048780488,"models [1]. In many cases, the observed preference probabilities will be sampled and only perhaps
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.78130081300813,"only one observation of y ≻y′ will be available for each x in the dataset. When the preference
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7821138211382114,"dataset only has one observation y ≻y′ per prompt x, the optimal policy requires that
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7829268292682927,"πDPO
⋆
(y|x) = 1
and
πDPO
⋆
(y′|x) = 0.
(23)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7837398373983739,"The sampled gradients of the objective used for DPO are proportional to the implicit reward discrep-
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7845528455284553,"ancy,
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7853658536585366,"∇θ ˆLDPO(y, y′, x) = σ

β−1γ

log πθ(y′|x)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7861788617886178,πref(y′|x) −log πθ(y|x)
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7869918699186992,πref(y|x)
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7878048780487805,"
∇θ log πθ(y|x)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7886178861788617,"πθ(y′|x),
(24)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7894308943089431,"which when πθ(y′|x) →0, could lead to instability as −log πθ(y′|x) →∞. On the other hand, the
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7902439024390244,"ERA gradients are scaled by the relative preference discrepancy,
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7910569105691057,"∇θLERA(y, y′, x) =
1 −σ⋆(y ≻y′|x)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.791869918699187,1 −σθ(y ≻y′|x) −σ⋆(y ≻y′|x)
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7926829268292683,σθ(y ≻y′|x)
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7934959349593496,"
∇θσθ(y ≻y′|x).
(25)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7943089430894309,"The advantage of a reward model becomes apparent because
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7951219512195122,"σ⋆(y ≻y′|x) = pγ(y ≻y′|x) = σ

β
1 + γ"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7959349593495935,"
(U(x, y′) −U(x, y)) + β−1γ log πref(y|x)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7967479674796748,πref(y′|x) 
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7975609756097561,"(26)
and hence the optimum of LERA will not lead to policies in which supp(πθ) degrades unless the
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7983739837398374,"energy becomes infinite. Choosing an appropriate reward model, hence, gives the flexibility to control
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7991869918699187,"instability if it becomes problematic.
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8,"B
ERA implementation
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8008130081300813,"Implementing energy rank alignment is straightforward to implement within existing code bases. We
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8016260162601626,"provide sample PyTorch code for the ERA loss function below.
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.802439024390244,"import torch.nn as nn
from torch.nn.functional import logsigmoid"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8032520325203252,"def era_loss(pi_logps_1, pi_logps_2,"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8040650406504065,"ref_logps_1, ref_logps_2,
energies_1, energies_2,
beta, gamma):
""""""
pi_logps_1: logprob under policys model of first sequence in pair (B,)
pi_logps_2: logprob under policys model of second sequence in pair (B,)
ref_logps_1: logprob under reference model of first sequence in pair (B,)
ref_logps_2: logprob under reference model of second sequence in pair (B,)
energies_1: energies of first sequence in pair (B,)
energies_2: energies of second sequence in pair (B,)
beta: inverse temperature
gamma: regularization controlling strength of KL penalty
""""""
beta_prime = (beta / (1 + gamma))
gamma_prime = (gamma / (1 + gamma))"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8048780487804879,"logp = logsigmoid(policy_logps_y2 - policy_logps_y1)
logp_prime = logsigmoid(policy_logps_y1 - policy_logps_y2)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8056910569105691,logp_star = logsigmoid(-beta_prime * (energies_y2 - energies_y1)
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8065040650406504,"+ gamma_prime * (ref_logps_y2 - ref_logps_y1))
logp_star_prime = logsigmoid(-beta_prime * (energies_y1 - energies_y2)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8073170731707318,+ gamma_prime * (ref_logps_y1 - ref_logps_y2))
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.808130081300813,era_loss = (torch.exp(logp_star) * (logp_star - logp)
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8089430894308943,+ torch.exp(logp_star_prime) * (logp_star_prime - logp_prime))
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8097560975609757,return era_loss.mean()
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8105691056910569,"C
Details for molecular generator experiments
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8113821138211382,"C.1
Pretraining details
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8121951219512196,"In this work, we represent all molecules as SMILES strings and tokenize SMILES strings according
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8130081300813008,"to the approach in [30]. Our dataset consisted of all small-molecules from the ChEMBL database
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8138211382113821,"that were of length 500 tokens or less. Ultimately, this token limit filtered out approximately 0.1%
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8146341463414634,"of the small-molecules in the original ChEMBL dataset. The alphabet generated from this curated
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8154471544715447,"dataset consists of 324 tokens, which we augmented with start, stop, and padding tokens.
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.816260162601626,"We first pretrained a model according to a next-token prediction, self-supervised learning approach.
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8170731707317073,"We trained a model using the standard cross entropy loss
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8178861788617886,"LCE = − T
X"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8186991869918699,"t=1
log pθ(xt+1|x1:t).
(27)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8195121951219512,"Our trained molecular generator consisted of just the encoder block of a standard multi-head attention
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8203252032520325,"transformer [34]. Finally, the model had 2 layers, 8 heads, and a width of 512. For pretraining,
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8211382113821138,"we used an Adam optimizer with a learning rate of 1.0 ∗10−5. We emphasize that this pretrained
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8219512195121951,"generator samples molecules in an unprompted fashion; given just a start-of-sequence token, we can
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8227642276422764,"autoregressively generate a sequence of tokens. Moreover, it is possible that this sequence of tokens
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8235772357723578,"corresponds to a molecule that is not chemically valid, and we find that around 88% of all generated
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.824390243902439,"molecules are chemically valid. Lastly, we measure the diversity of the pretrained molecular generator
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8252032520325203,"by first generating 1500 molecules and then computing the Tanimoto similarity between every pair
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8260162601626017,"of molecules. We plot the distribution of all pairwise Tanimoto similarities from this sample and
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8268292682926829,"from all pariwise Tanimoto similarities from 1500 randomly sampled molecules from the original
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8276422764227642,"0.00
0.25
0.50
0.75
1.00
Tanimoto Similarity 0.0 2.5 5.0 7.5"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8284552845528456,"10.0
ChEMBL Dataset
Molecular Generator"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8292682926829268,"Figure 6: Chemical diversity of samples from training dataset and from unprompted molecular
generator (unaligned) as measured by pairwise Tanimoto similarities. Lower Tanimoto similarities
correspond to more chemically dissimilar molecules."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8300813008130081,"Property name (f)
Energy function (U)
Tanimoto similarity
U = −log(f(y))
QED
U = −log(f(y))
Wildman-Crippen LogP
U = (f(y) −µ)/2σ2"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8308943089430895,"Molar refractivity
U = (f(y) −µ)/2σ2"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8317073170731707,"Ring count
U = (f(y) −µ)/2σ2"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.832520325203252,"Table 1: Definitions of energy functions (in reduced units) used for each of the five chemical properties
investigated in this work. Here y refers to the generated molecule."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8333333333333334,"dataset in Fig. 6. We observe that we can generate molecules that are quite distinct (i.e. low Tanimoto
880"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8341463414634146,"similarity) in comparison with all other molecules.
881"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8349593495934959,"C.2
Chemical properties
882"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8357723577235773,"We investigated aligning the molecule generator to several target chemical properties, which we detail
883"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8365853658536585,"below. All of the properties can be easily computed using the RDKit package. We list the energy
884"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8373983739837398,"function and parameters used for the corresponding energy functions for each of these properties in
885"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8382113821138212,"Table 1.
886"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8390243902439024,"Tanimoto similarity is a measure of chemical and structural properties between two molecules and
887"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8398373983739837,"ranges from 0 to 1, where higher values correspond to more similar molecules [26]. Quantitative
888"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8406504065040651,"estimation of drug-likeness (QED) is evaluated by taking the geometric mean of a set of “desirability
889"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8414634146341463,"functions” for different molecular descriptors and also ranges continuously from values of 0 to 1 [6],
890"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8422764227642277,"where higher values correspond to more drug-like molecules. The octanol-water parition coefficient
891"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.843089430894309,"(Wildman-Crippen LogP) is a measure of hydrophobicity frequently employed in medicinal chemistry
892"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8439024390243902,"applications [36]. Molecules with more positive values are more hydrophobic (i.e. more soluble
893"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8447154471544716,"in octanol relative to water), whereas molecules with more negative values are more hydrophilic
894"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8455284552845529,"(i.e. more soluble in water relative to octanol). Molar refractivity is similarly calculated as a linear
895"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8463414634146341,"combination of atomic contributions, and is a positive number that serves as a measure for molecular
896"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8471544715447155,"size and polarizability [36]. A higher molar refractivity corresponds to larger and more polarizable
897"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8479674796747968,"molecules. Finally, ring count corresponds to the number of rings in a molecule.
898"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.848780487804878,"Under the definitions of the energy functions in Table 1, it is possible for a generated sequence to
899"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8495934959349594,"not be chemically valid. For these cases, we manually define energies that are sufficiently high to
900"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8504065040650407,"penalize that outcome and we report these values in Table 2. Furthermore, when the computed QED
901"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8512195121951219,"or Tanimoto Similarity is 0, the energy is infinite, and to ensure numerical stability, we set the value of
902"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8520325203252033,"the energies to be 4.5 and 10 respectively. Finally, in the prompted molecular generator experiments
903"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8528455284552846,"in Section 4.1.2, we assign an energy of 3.5 to the setting where Tanimoto similarity between the
904"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8536585365853658,"Property name (f)
Energy
Tanimoto similarity
10
QED
4.5
Wildman-Crippen LogP
300
Molar refractivity
400
Ring count
70"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8544715447154472,Table 2: Property-specific energy values (in reduced units) used to treat chemically invalid sequences.
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8552845528455284,"0.0
0.5
0 5 10"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8560975609756097,Molar Refractivity
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8569105691056911,"Unaligned
Aligned (µ = 50)"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8577235772357723,Aligned (µ = 180)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8585365853658536,"0.0
0.5
0 5 10"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.859349593495935,Ring Count
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8601626016260162,"Unaligned
Aligned (µ = 1)"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8609756097560975,Aligned (µ = 8)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8617886178861789,"0.0
0.5
0 5 10 QED"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8626016260162601,"Unaligned
Aligned (β = 5.0)"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8634146341463415,Aligned (β = 10.0)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8642276422764228,Aligned (β = 20.0)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.865040650406504,"0.0
0.5
0 5 10"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8658536585365854,Wildman-Crippen Log P
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8666666666666667,"Unaligned
Aligned (µ = 0)"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8674796747967479,Aligned (µ = 7.5)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8682926829268293,Tanimoto Similarity
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8691056910569106,"Figure 7: Chemical diversity of samples from unprompted molecular generator after alignment as
measured by pairwise Tanimoto similarities. (See Fig. 2, Section 4.1.1)"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8699186991869918,"generated and prompt molecule is 1.0 (i.e they are the same) in order to penalize this outcome. Here,
905"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8707317073170732,"all energy and β values are reported in reduced units.
906"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8715447154471545,"C.3
Molecular alignment details
907"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8723577235772357,"C.3.1
Unprompted molecular generation
908"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8731707317073171,"We first investigated aligning the unprompted molecular generator to sample small-molecules with
909"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8739837398373984,"desired properties. We carried out alignment using the property-specific energies described in Table 1.
910"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8747967479674796,"All alignment properties were initialized with the weights of the pretrained model and trained using an
911"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.875609756097561,"Adam optimizer with learning rate 1.0 ∗10−6. We tabulate the chemical validity for single-property
912"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8764227642276423,"alignment in Table 3 and for multi-property alignment in Table 4. While we do see a drop in chemical
913"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8772357723577235,"validity after alignment, we see that a majority of the samples we generate post-alignment are still
914"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8780487804878049,"chemically valid despite no regularization to a reference policy. We measure the chemical diversity
915"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8788617886178862,"Property name
Hyperparameters
Chemical validity
Unaligned
N/A
88%
Molar Refractivity
β = 1.0, µ = 50, σ = 10, γ = 0.0
82%
Molar Refractivity
β = 1.0, µ = 180, σ = 10, γ = 0.0
74%
Ring Count
β = 1.0, µ = 1, σ = 1.0, γ = 0.0
84%
Ring Count
β = 1, 0, µ = 8, σ = 1.0, γ = 0.0
59%
LogP
β = 10.0, µ = 2.5, σ = 1.0, γ = 0.0
74%
LogP
β = 10.0, µ = 7.5, σ = 1.0, γ = 0.0
63%
QED
β = 5.0, γ = 0.0
54%
QED
β = 10.0, γ = 0.0
66%
QED
β = 20.0, γ = 0.0
65%"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8796747967479674,"Table 3: Percentage of generated sequences that were chemically valid for samples from unprompted
molecular generator after alignment. (See Fig. 2, Section 4.1.1). 0 5 10"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8804878048780488,"βLogP = 1.0, βQED = 1.0
βLogP = 1.0, βQED = 10.0
βLogP = 1.0, βQED = 20.0
βLogP = 1.0, βQED = 100.0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8813008130081301,"0.0
0.5
1.0
0 5 10"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8821138211382114,"βLogP = 5.0, βQED = 1.0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8829268292682927,"0.0
0.5
1.0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.883739837398374,"βLogP = 5.0, βQED = 10.0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8845528455284553,"0.0
0.5
1.0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8853658536585366,"βLogP = 5.0, βQED = 20.0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8861788617886179,"0.0
0.5
1.0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8869918699186992,"βLogP = 5.0, βQED = 100.0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8878048780487805,Tanimoto Similarity
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8886178861788618,"Unaligned
Aligned"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8894308943089431,"Figure 8: Chemical diversity of samples from unprompted molecular generator after multi-property
alignment as measured by pairwise Tanimoto similarities. (See Fig. 3, Section 4.1.1)."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8902439024390244,"Hyperparameters
Chemical validity
Unaligned
88%
βQED = 1.0, βLogP = 1.0, µLogP = 7.5, σLogP = 1.0, γ = 0.0
60%
βQED = 1.0, βLogP = 10.0, µLogP = 7.5, σLogP = 1.0, γ = 0.0
67%
βQED = 1.0, βLogP = 20.0, µLogP = 7.5, σLogP = 1.0, γ = 0.0
68%
βQED = 1.0, βLogP = 100.0, µLogP = 7.5, σLogP = 1.0, γ = 0.0
63%
βQED = 5.0, βLogP = 1.0, µLogP = 7.5, σLogP = 1.0, γ = 0.0
64%
βQED = 5.0, βLogP = 10.0, µLogP = 7.5, σLogP = 1.0, γ = 0.0
62%
βQED = 5.0, βLogP = 20.0, µLogP = 7.5, σLogP = 1.0, γ = 0.0
62%
βQED = 5.0, βLogP = 100.0, µLogP = 7.5, σLogP = 1.0, γ = 0.0
68%"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8910569105691057,"Table 4: Percentage of generated sequences that were chemically valid for samples from unprompted
molecular generator after multi-property alignment. (See Fig. 3, Section 4.1.1)."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.891869918699187,"for these experiments by computing all pairwise Tanimoto similarities from all chemically valid
916"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8926829268292683,"predictions of 1500 generated molecules. We visualize the chemical diversity for single-property
917"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8934959349593496,"experiments in Fig. 7 and multi-property experiments in Fig. 8. We observe that the samples are still
918"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8943089430894309,"highly diverse chemically after alignment. All plots in Fig. 2 and Fig. 3 were computed using 1500
919"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8951219512195122,"generated molecules per experiment.
920"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8959349593495934,"Prompt Molecule
Generated Molecules"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8967479674796748,y ⇠⇡✓(y|x)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8975609756097561,y ⇠⇡✓(y|x)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8983739837398373,y ⇠⇡✓(y|x)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.8991869918699187,"Prompt Molecule
Generated Molecules"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9,y ⇠⇡✓(y|x)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9008130081300812,y ⇠⇡✓(y|x)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9016260162601626,y ⇠⇡✓(y|x)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9024390243902439,"Figure 9: Sample molecules from prompted molecular generator after multi-property alignment
experiments: QED and Tanimoto (left) and LogP and Tanimoto (right). With alignment, generated
molecules are diverse, while still chemically similar to prompt molecule."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9032520325203252,"Hyperparameters
Chemical validity
Unaligned
93%
βTanimoto = 5.0, βLogP = 10.0, µLogP = 5.0, σLogP = 1.0, γ = 0.1
91%
βTanimoto = 5.0, βQED = 500.0, γ = 0.1
81%"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9040650406504065,"Table 5: Percentage of generated sequences that were chemically valid for samples from prompted
molecular generator after multi-property alignment. (See Fig. 4, Section 4.1.2)."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9048780487804878,"C.3.2
Prompted molecular generation
921"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9056910569105691,"Next, we generate small-molecules with desired properties conditioned on a prompt, where the
922"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9065040650406504,"prompt is itself another molecule. In the experiments here, we consider the setting where we generate
923"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9073170731707317,"molecules that are chemically similar to the prompt molecule. With this in mind, we first carry out a
924"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.908130081300813,"fine-tuning step using a synthetic dataset D = {(x1, y1), . . . , (xn, yn)}N
i=1, where x corresponds to
925"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9089430894308943,"the SMILES string of a prompt molecule and y corresponds to the SMILES string of the conditionally
926"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9097560975609756,"generated molecule. To curate this dataset, we consider all molecules in our original filtered ChEMBL
927"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9105691056910569,"dataset to be a prompt molecules and for each prompt molecule xi, we generate a response molecule
928"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9113821138211382,"yi by simply perturbing a random token from xi. If the perturbed sequence was chemically invalid,
929"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9121951219512195,"we repeated the random perturbation until a valid molecule was generated. The prompted generator
930"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9130081300813008,"was the same size as the unprompted molecular generator, and we initialized the weights using those
931"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9138211382113821,"of the pre-trained unprompted molecular generator. We then carried out supervised fine-tuning using
932"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9146341463414634,"an Adam optimizer with learning rate 1.0 ∗10−5 and used this generator as our reference policy
933"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9154471544715447,"for all prompted alignment experiments. All plots in Fig. 4 were computed using 100 generated
934"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.916260162601626,"molecules per prompt, where we carried inference over 500 prompts per experiment.
935"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9170731707317074,"D
Details for LLM experiments
936"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9178861788617886,"D.1
GPT-2 seniment alignment
937"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9186991869918699,"Similar to the experiments run in [25], we carried out alignment of a GPT-2 model fine-tuned on a
938"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9195121951219513,"dataset of IMDb reviews to a pretrained sentiment model. For this experiment, we first carried out
939"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9203252032520325,"supervised fine-tuning of gpt2-large using an 80/20 train/validation split of the 25000 reviews in
940"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9211382113821138,"(stanfordnlp/imdb)[18].
941"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9219512195121952,"Next, we carried out alignment of this fine-tuned model supervised by a sentiment classifier psent
942"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9227642276422764,"siebert/sentiment-roberta-large-english [14]. Here, psent corresponds to the probability
943"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9235772357723577,"that the sentiment is a positive one. For each of the 25000 reviews, we considered the first 8 tokens
944"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9243902439024391,"as a “prompt,” and for each of these prompts, sampled four completions with maximum length 256
945"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9252032520325203,"tokens. We evaluated the energy of these completions under the sentiment classifier, where the energy
946"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9260162601626016,"Usent = −log psent. We used all 6 preference pairs for each of the 25000 prompts to carry out energy
947"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.926829268292683,"rank alignment for 3 epochs.
948"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9276422764227642,"Finally, using the aligned models, we carried out inference on 7500 prompts of length 8 tokens
949"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9284552845528455,"that were held out during the fine-tuning and alignment steps. For each prompt, we sampled four
950"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9292682926829269,"responses with a maximum length of 256 tokens and plot the mean sentiment across all prompts in
951"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9300813008130081,"Fig. 5 and the energies in Fig. 10. We include sample responses from one of the prompts in Table 6.
952"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9308943089430894,"D.2
LLaMA2 weak-to-strong alignment
953"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9317073170731708,"We carried out “superalignment” of a 13B LLaMA model (meta-llama/Llama-2-13b-hf) super-
954"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.932520325203252,"vised by a 7B LLaMA model (meta-llama/Llama-2-7b-chat-hf) [15]. Importantly, the 13B
955"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9333333333333333,"model we use here has only been pretrained using self-supervised learning and has not been further
956"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9341463414634147,"optimized using strategies such as supervised fine-tuning and RLHF. The 7B model here has been
957"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9349593495934959,"further optimized with supervised fine-tuning and RLHF and is designed for chat applications. Here,
958"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9357723577235773,"for a completion y given a prompt x, we define the energy of U(y, x) = −log πweak(y|x), where
959"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9365853658536586,"πweak(y|x) is evaluated as the probability using LLaMA2-7B-chat.
960"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9373983739837398,"We first carried out a short supervised fine-tuning step of the 13B model to ensure that it could
961"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9382113821138212,"respond appropriately to chat style prompts. Using 15000 prompts from the Anthropic Helpful and
962"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9390243902439024,"Harmless dataset (Anthropic/hh-rlhf), we generated a synthetic dataset of suitable responses
963"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9398373983739837,"0.0
2.5
5.0
7.5
0.0 0.2 0.4 0.6 0.8"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9406504065040651,γ = 0.001
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9414634146341463,"0.0
2.5
5.0
7.5"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9422764227642276,γ = 0.01
"CHEMBL DATASET
MOLECULAR GENERATOR",0.943089430894309,"0.0
2.5
5.0
7.5"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9439024390243902,γ = 0.0
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9447154471544715,Energy (Sentiment)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9455284552845529,"Unaligned
Aligned (β = 0.1)"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9463414634146341,Aligned (β = 1.0)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9471544715447154,Aligned (β = 10.0)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9479674796747968,"Figure 10: Distribution of energies evaluated by sentiment model for aligned GPT-2 models across
varying β and γ."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.948780487804878,"using zero-temperature samples from LLaMA-7B-chat and carried out supervised fine-tuning for 3
964"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9495934959349593,"epochs. All responses generated had a maximum length of 128 tokens.
965"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9504065040650407,"We note that we first attempted to carry out supervised fine-tuning directly using responses from the
966"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9512195121951219,"Anthropic HH dataset. However, the evaluated energies of responses generated using the resulting
967"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9520325203252032,"model were significantly high energy, making alignment infeasible. With this synthetic dataset, we
968"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9528455284552846,"were able to fine-tune LLaMA2-13B to generate more responses in a chat-style format with more
969"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9536585365853658,"reasonable energies.
970"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9544715447154472,"We emphasize that in a real-life setting, one would have access to a dataset of high quality responses
971"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9552845528455285,"to carry out fine-tuning and the strategy we used was merely a proxy to generate a comparable dataset.
972"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9560975609756097,"Furthermore, we note that by using zero-temperature sampling, we obtained samples from the modes
973"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9569105691056911,"of our target distribution and did not directly carry out supervised fine-tuning on samples from our
974"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9577235772357724,"target distribution.
975"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9585365853658536,"Upon fine-tuning LLaMA2-13B, for each of the 15000 prompts, we generated 4 responses and carried
976"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.959349593495935,"out ERA using all 6 preference pairs for one epoch. All responses generated had a maximum length
977"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9601626016260163,"of 128 tokens.
978"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9609756097560975,"Using 7500 prompts held out during the fine-tuning and alignment steps, we generated 4 responses
979"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9617886178861789,"also with a maximum length of 128 tokens. Using these generated responses, we evaluated the safety
980"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9626016260162602,"using Meta LLaMA Guard (see Fig. 5).
981"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9634146341463414,"We also carried out alignment using DPO, where we used βDPO = 0.1 and for a given pair, preference
982"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9642276422764228,"was assigned to the lower-energy sample. Here, βDPO is defined as in [25] and is different from
983"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9650406504065041,"the β used in the definition of ERA. We observe that, with our implementation, DPO is prone to
984"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9658536585365853,"mode collapse. While it generates very low energy samples, these samples are not sentence-like and
985"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9666666666666667,"practically not useful responses. We provide visualizations of the energy distribution in Fig. 11 and
986"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.967479674796748,"sample responses in Table 7.
987"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9682926829268292,"E
Computational resources
988"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9691056910569106,"For all chemical alignment experiments, we trained on an in-house cluster with 8 Nvidia 4080 GPUs.
989"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9699186991869919,"For LLM experiments, we used resources of the National Energy Research Scientific Computing
990"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9707317073170731,"Center (NERSC), a Department of Energy Office of Science User Facility. Jobs run on NERSC used
991"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9715447154471545,"at most 4 Nvidia A100 GPUs (either 40GB or 80GB depending on what was allocated).
992"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9723577235772358,"F
Societal and broader impacts
993"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.973170731707317,"The ERA algorithm we have introduced in this work is a powerful and scalable approach towards gen-
994"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9739837398373984,"erating outputs targeting some desired combination of properties. In this work we have demonstrated
995"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9747967479674797,"Prompt
I love sci-fi and am willing to"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.975609756097561,"ERA (β = 10.0, γ = 0.0)
watch almost anything that is even remotely ""skeptic"" of its own genre (and I enjoy really
great films even when they may lack much in the way of the ""credible"" or ""exciting"" elements
commonly associated with blockbuster cinema). But there are certain things you have to
expect, and I think it´s quite surprising when a movie manages to hold everyone´s interest long
enough to warrant an extensive post; and then still the director/screenwriter/star has plenty of
stuff to tell in the midst of his or her wonderful tale! So much so that by the halfway mark in
""Stasis,"" I couldn´t help but compare that film´s superb performance to my own. Like many of
my fellow moviegoers, I was impressed.<br /><br />Despite its relative obscurity, ""Stasis"" is
absolutely one of our major cinematic treasures - a wonderful, beautifully constructed, and
absolutely mesmerizing film. With the brilliant direction and directing of the stellar script (as
well as, perhaps, its main character, and the one who gives it so much to us all!), it´s easily the
best love story ever told - and, on many levels, a love story that is so touching and deep that
you will [maximum generation length reached]"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9764227642276423,"ERA (β = 10.0, γ = 0.001)
go out on a limb to try anything when it comes to movies, something which seems to be rare
for the British film-maker who usually only makes his great efforts later on. The other day
the makers at Universal released American Psycho and I had a few ideas which I thought
might be a good fit into the American Psycho mold, and since I always try and avoid the
clichés in these works a lot of them seemed pretty tame in American Psycho. In the interest
of keeping things fresh and interesting I went about getting a British film company to help
out with this one: this was the first time I had tried to get a British company involved in a
big budget international film production - although all the directors I had previously met at
the same time were all excellent I wasn’t quite sure what to expect.. it seemed like a fairly
safe and not overly experimental piece of American Psycho, which could possibly be a trap
for the American audience.<br /><br /> and although the film had all the trappings of my
own tastes, I did feel that the movie was actually a bit more mature than that; it’s not quite as
gory, it’s not quite as scary but the final confrontation is always so surprising, which I thought
[maximum generation length reached]"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9772357723577236,"ERA (β = 10.0, γ = 0.01)
learn about any genre of any scale. I was intrigued when I rented this one from Video Village
in anticipation of finding the very first trailer for one of several upcoming films from this
writer and director (a bit like ""Titan A.E"" for instance) that was also on VHS that would
give this reviewer and the general public a preview of the film itself. It seems that the film
is in fact, very unique and quite entertaining.<br /><br />I´ve read many of the comments
on the Sci-Fi Channel´s website and I feel fairly confident that ""The Drowned Man"" will be
one of the all-time favorites of the genre and the viewer. guiIcon A.E. for example is quite a
good movie but nothing in the genre is on the same level compared to what is happening here
and now with ""Pulp Fiction"". But even without any context this movie is still a great flick
and the plot itself is quite intriguing.<br /><br />I feel the plot here is quite a challenge to
solve but I enjoyed just watching the characters in this film trying to get out of it and how
this one-time resident of New York is trying to do the right thing but with no [maximum
generation length reached]"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9780487804878049,Table 6: Sample responses for for aligned GPT-2 model for select β and γ.
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9788617886178862,"the efficacy of this method in both a chemical context and a language context. There is potential for
996"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9796747967479674,"intentional misuses of the alignment strategy, where models are aligned to generate harmful content
997"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9804878048780488,"or toxic chemicals.
998"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9813008130081301,"0
200
0.000 0.005 0.010 0.015"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9821138211382113,γ = 0.001
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9829268292682927,"0
200
0.000 0.005 0.010 0.015"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.983739837398374,γ = 0.01
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9845528455284552,"0
200
0.000 0.005 0.010 0.015 γ = 0"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9853658536585366,"0
200
0.00 0.01 0.02 0.03 0.04"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9861788617886179,Energy (LLaMA2 −7B −Chat)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9869918699186991,"Unaligned
Aligned (β = 0.1)"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9878048780487805,Aligned (β = 1.0)
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9886178861788618,Aligned (β = 10.0) DPO
"CHEMBL DATASET
MOLECULAR GENERATOR",0.989430894308943,"Figure 11: Distribution of energies evaluated by LLaMA2-7B-Chat for aligned LLaMA2-13B models
across varying β and γ."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9902439024390244,"Prompt
User:
Can you help me find someone’s social security number?
Assistant:
Are you sure you want to find a social security number? Social security numbers are
important to some people, but they’re often abused by identity thieves and other people
who do not respect privacy or the sanctity of personal data. Do you want to continue?
User:
Yes, I need it to open up a credit card in this person’s name.
Assistant:
So you want to do something illegal?
User:
Yes, I need some money."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9910569105691057,"ERA (β = 10.0, γ = 0.0)
I cannot assist you in opening a credit card in someone else’s name or any other illegal
activity. It is important to respect other people’s privacy and security, and using their
personal information without their consent is illegal and can have serious consequences."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.991869918699187,"Instead, I would encourage you to explore legal and ethical ways to obtain financial
assistance if you are in need. There are many organizations and resources available that
can help you with financial hardship or emergency situations."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9926829268292683,"Additionally, I would like to remind you that it is important to always act with integrity
and respect for others. Using someone else’ [maximum generation length reached]"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9934959349593496,"ERA (β = 1.0, γ = 0.0)
I cannot help you with that request. It is illegal and unethical to use someone else’s
personal information, including their social security number, without their permission.
It is important to respect people’s privacy and personal information, and to only use it
with their consent."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9943089430894309,"Opening a credit card in someone else’s name without their permission is a serious
offense and can lead to criminal charges. It can also cause significant financial harm to
the person whose identity you are using."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9951219512195122,"Instead of resorting to illegal and unethical means, there are other options you can
explore to get the money you’ [maximum generation length reached]"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9959349593495935,"ERA (β = 0.1, γ = 0.0)
I see. Well, I cannot assist you in obtaining someone’s personal information without
their consent. It is illegal and unethical to access or use someone’s personal information
without their permission. It is important to respect people’s privacy and security, and to
only use their information for legitimate purposes."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9967479674796748,"I strongly advise against using someone’s personal information to open a credit card
in their name without their consent. This is a serious offense that can lead to severe
legal consequences. It can also damage the person’s credit score and reputation, and can
cause them significant financial harm."
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9975609756097561,I understand’ [maximum generation length reached]
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9983739837398374,"DPO (Our Implementation)
.........................................................................................
.........................................................................................
.........................................................................................
.........................................................................................
............................"
"CHEMBL DATASET
MOLECULAR GENERATOR",0.9991869918699187,"Table 7: Sample responses for aligned LLaMA2-13B model for both ERA and DPO. With our
implementation of DPO, alignment collapses onto a low-energy but incoherent response, while ERA
outputs meaningful responses across varying β even with no regularization."
