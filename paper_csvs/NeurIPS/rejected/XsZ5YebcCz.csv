Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002183406113537118,"Offline reinforcement learning (RL) methodologies enforce constraints on the
1"
ABSTRACT,0.004366812227074236,"policy to adhere closely to the behavior policy, thereby stabilizing value learning
2"
ABSTRACT,0.006550218340611353,"and mitigating the selection of out-of-distribution (OOD) actions during test time.
3"
ABSTRACT,0.008733624454148471,"Conventional approaches apply identical constraints for both value learning and test
4"
ABSTRACT,0.010917030567685589,"time inference. However, our findings indicate that the constraints suitable for value
5"
ABSTRACT,0.013100436681222707,"estimation may in fact be excessively restrictive for action selection during test time.
6"
ABSTRACT,0.015283842794759825,"To address this issue, we propose a Mildly Constrained Evaluation Policy (MCEP)
7"
ABSTRACT,0.017467248908296942,"for test time inference with a more constrained target policy for value estimation.
8"
ABSTRACT,0.019650655021834062,"Since the target policy has been adopted in various prior approaches, MCEP can
9"
ABSTRACT,0.021834061135371178,"be seamlessly integrated with them as a plug-in. We instantiate MCEP based on
10"
ABSTRACT,0.024017467248908297,"TD3-BC [Fujimoto and Gu, 2021] and AWAC [Nair et al., 2020] algorithms. The
11"
ABSTRACT,0.026200873362445413,"empirical results on MuJoCo locomotion tasks show that the MCEP significantly
12"
ABSTRACT,0.028384279475982533,"outperforms the target policy and achieves competitive results to state-of-the-art
13"
ABSTRACT,0.03056768558951965,"offline RL methods. The codes are open-sourced at link.
14"
INTRODUCTION,0.03275109170305677,"1
Introduction
15"
INTRODUCTION,0.034934497816593885,"Offline reinforcement learning (RL) extracts a policy from data that is pre-collected by unknown
16"
INTRODUCTION,0.03711790393013101,"policies. This setting does not require interactions with the environment thus it is well-suited for tasks
17"
INTRODUCTION,0.039301310043668124,"where the interaction is costly or risky. Recently, it has been applied to Natural Language Process-
18"
INTRODUCTION,0.04148471615720524,"ing [Snell et al., 2022], e-commerce [Degirmenci and Jones] and real-world robotics [Kalashnikov
19"
INTRODUCTION,0.043668122270742356,"et al., 2021, Rafailov et al., 2021, Kumar et al., 2022, Shah et al., 2022] etc. Compared to the standard
20"
INTRODUCTION,0.04585152838427948,"online setting where the policy gets improved via trial and error, learning with a static offline dataset
21"
INTRODUCTION,0.048034934497816595,"raises novel challenges. One challenge is the distributional shift between the training data and the data
22"
INTRODUCTION,0.05021834061135371,"encountered during deployment. To attain stable evaluation performance under the distributional shift,
23"
INTRODUCTION,0.05240174672489083,"the policy is expected to stay close to the behavior policy. Another challenge is the ""extrapolation
24"
INTRODUCTION,0.05458515283842795,"error"" [Fujimoto et al., 2019, Kumar et al., 2019] that indicates value estimate error on unseen
25"
INTRODUCTION,0.056768558951965066,"state-action pairs or Out-Of-Distribution (OOD) actions. Worsely, this error can be amplified with
26"
INTRODUCTION,0.05895196506550218,"bootstrapping and cause instability of the training, which is also known as deadly-triad [Van Hasselt
27"
INTRODUCTION,0.0611353711790393,"et al., 2018]. Majorities of model-free approaches tackle these challenges by either constraining the
28"
INTRODUCTION,0.06331877729257641,"policy to adhere closely to the behavior policy [Wu et al., 2019, Kumar et al., 2019, Fujimoto and Gu,
29"
INTRODUCTION,0.06550218340611354,"2021] or regularising the Q to pessimistic estimation for OOD actions [Kumar et al., 2020, Lyu et al.,
30"
INTRODUCTION,0.06768558951965066,"2022]. In this work, we focus on policy constraints methods.
31"
INTRODUCTION,0.06986899563318777,"Policy constraints methods minimize the disparity between the policy distribution and the behavior
32"
INTRODUCTION,0.07205240174672489,"distribution. It is found that policy constraints introduce a tradeoff between stabilizing value estimates
33"
INTRODUCTION,0.07423580786026202,"and attaining better performance. While previous approaches focus on developing various constraints
34"
INTRODUCTION,0.07641921397379912,"for the learning policy to address this tradeoff, the tradeoff itself is not well understood. Current
35"
INTRODUCTION,0.07860262008733625,"solutions have confirmed that an excessively constrained policy enables stable values estimate
36"
INTRODUCTION,0.08078602620087336,"but degrades the evaluation performance [Kumar et al., 2019, Singh et al., 2022, Yu et al., 2023].
37"
INTRODUCTION,0.08296943231441048,"Nevertheless, it is not clear to what extent this constraint fails to stabilize value learning and to
38"
INTRODUCTION,0.0851528384279476,"what extent this constraint leads to a performant evaluation policy. It is essential to investigate these
39"
INTRODUCTION,0.08733624454148471,"questions as their answers indicate how well a solution can be found under the tradeoff. However,
40"
INTRODUCTION,0.08951965065502183,"the investigation into the latter question is impeded by the existing tradeoff, as it requires tuning the
41"
INTRODUCTION,0.09170305676855896,"constraint without influencing the value learning. We circumvent the tradeoff and seek solutions for
42"
INTRODUCTION,0.09388646288209607,"this investigation through the critic. For actor-critic methods, [Czarnecki et al., 2019] has shed light
43"
INTRODUCTION,0.09606986899563319,"on the potential of distilling a student policy that improves over the teacher using the teacher’s critic.
44"
INTRODUCTION,0.0982532751091703,"Inspired by this work, we propose to derive an extra evaluation policy from the critic to avoid solving
45"
INTRODUCTION,0.10043668122270742,"the above-mentioned tradeoff. The actor is now called target policy as it is used only to stabilize the
46"
INTRODUCTION,0.10262008733624454,"value estimation.
47"
INTRODUCTION,0.10480349344978165,"Based on the proposed framework, we empirically investigate the constraint strengths for 1) stabilizing
48"
INTRODUCTION,0.10698689956331878,"value learning and 2) better evaluation performance. The results find that a milder constraint improves
49"
INTRODUCTION,0.1091703056768559,"the evaluation performance but may fall beyond the constraint space of stable value estimation.
50"
INTRODUCTION,0.11135371179039301,"This finding indicates that the optimal evaluation performance may not be found under the tradeoff,
51"
INTRODUCTION,0.11353711790393013,"especially when stable value learning is the priority. Consequently, we propose a novel approach of
52"
INTRODUCTION,0.11572052401746726,"using a Mildly Constrained Evaluation Policy (MCEP) derived from the critic to avoid solving the
53"
INTRODUCTION,0.11790393013100436,"above-mentioned tradeoff and to achieve better evaluation performance.
54"
INTRODUCTION,0.12008733624454149,"As the target policy is commonly used in previous approaches, our MCEP can be integrated with
55"
INTRODUCTION,0.1222707423580786,"them seamlessly. In this paper, we first validate the finding of [Czarnecki et al., 2019] in the offline
56"
INTRODUCTION,0.12445414847161572,"setting by a toy maze experiment, where a constrained policy results in bad evaluation performance
57"
INTRODUCTION,0.12663755458515283,"but its off-policy Q estimation indicates an optimal policy. After that, our experiments on D4RL [Fu
58"
INTRODUCTION,0.12882096069868995,"et al., 2020] MoJoCo locomotion tasks showed that in most tasks milder constraint achieves better
59"
INTRODUCTION,0.13100436681222707,"evaluation performance while more restrictive constraint stabilizes the value estimate. Finally, we
60"
INTRODUCTION,0.1331877729257642,"instantiated MCEP on both TD3BC and AWAC algorithms. The empirical results find that the MCEP
61"
INTRODUCTION,0.13537117903930132,"significantly outperforms the target policy and achieves competitive results to state-of-the-art offline
62"
INTRODUCTION,0.13755458515283842,"RL methods.
63"
RELATED WORK,0.13973799126637554,"2
Related Work
64"
RELATED WORK,0.14192139737991266,"Policy constraints method (or behavior-regularized policy method) [Wu et al., 2019, Kumar et al.,
65"
RELATED WORK,0.14410480349344978,"2019, Siegel et al., 2020, Fujimoto and Gu, 2021] forces the policy distribution to stay close to the
66"
RELATED WORK,0.1462882096069869,"behavior distribution. Different discrepancy measurements such as KL divergence [Jaques et al., 2019,
67"
RELATED WORK,0.14847161572052403,"Wu et al., 2019], reverse KL divergence Cai et al. [2022] and Maximum Mean Discrepancy [Kumar
68"
RELATED WORK,0.15065502183406113,"et al., 2019] are applied in previous approaches. [Fujimoto and Gu, 2021] simply adds a behavior-
69"
RELATED WORK,0.15283842794759825,"cloning (BC) term to the online RL method Twin Delayed DDPG (TD3) [Fujimoto et al., 2018]
70"
RELATED WORK,0.15502183406113537,"and obtains competitive performances in the offline setting. While the above-mentioned methods
71"
RELATED WORK,0.1572052401746725,"calculate the divergence from the data, [Wu et al., 2022] estimates the density of the behavior
72"
RELATED WORK,0.15938864628820962,"distribution using VAE, and thus the divergence can be directly calculated. Except for explicit policy
73"
RELATED WORK,0.1615720524017467,"constraints, implicit constraints are achieved by different approaches. E.g. [Zhou et al., 2021] ensures
74"
RELATED WORK,0.16375545851528384,"the output actions stay in support of the data distribution by using a pre-trained conditional VAE
75"
RELATED WORK,0.16593886462882096,"(CVAE) decoder that maps latent actions to the behavior distribution. In all previous approaches, the
76"
RELATED WORK,0.16812227074235808,"constraints are applied to the learning policy that is queried during policy evaluation and is evaluated
77"
RELATED WORK,0.1703056768558952,"in the environment during deployment. Our approach does not count on this learning policy for the
78"
RELATED WORK,0.17248908296943233,"deployment, instead, it is used as a target policy only for the policy evaluation.
79"
RELATED WORK,0.17467248908296942,"While it is well-known that a policy constraint can be efficient to reduce extrapolation errors, its
80"
RELATED WORK,0.17685589519650655,"drawback is not well-studied yet. [Kumar et al., 2019] reveals a tradeoff between reducing errors in
81"
RELATED WORK,0.17903930131004367,"the Q estimate and reducing the suboptimality bias that degrades the evaluation policy. A constraint is
82"
RELATED WORK,0.1812227074235808,"designed to create a policy space that ensures the resulting policy is under the support of the behavior
83"
RELATED WORK,0.18340611353711792,"distribution for mitigating bootstrapping error. [Singh et al., 2022] discussed the inefficiency of policy
84"
RELATED WORK,0.185589519650655,"constraints on heteroskedastic dataset where the behavior varies across the state space in a highly
85"
RELATED WORK,0.18777292576419213,"non-uniform manner, as the constraint is state-agnostic. A reweighting method is proposed to achieve
86"
RELATED WORK,0.18995633187772926,"a state-aware distributional constraint to overcome this problem. Our work studies essential questions
87"
RELATED WORK,0.19213973799126638,"about the tradeoff [Kumar et al., 2019] and overcomes this drawback [Singh et al., 2022] by using an
88"
RELATED WORK,0.1943231441048035,"extra evaluation policy.
89"
RELATED WORK,0.1965065502183406,"There are methods that extract an evaluation policy from a learned Q estimate. One-step RL [Brand-
90"
RELATED WORK,0.19868995633187772,"fonbrener et al., 2021] first estimates the behavior policy and its Q estimate, which is later used
91"
RELATED WORK,0.20087336244541484,"for extracting the evaluation policy. Although its simplicity, one-step RL is found to perform badly
92"
RELATED WORK,0.20305676855895197,"in long-horizon problems due to a lack of iterative dynamic programming [Kostrikov et al., 2022].
93"
RELATED WORK,0.2052401746724891,"[Kostrikov et al., 2022] proposed Implicity Q learning (IQL) that avoids query of OOD actions
94"
RELATED WORK,0.2074235807860262,"by learning an upper expectile of the state value distribution. No explicit target policy is mod-
95"
RELATED WORK,0.2096069868995633,"eled during their Q learning. With the learned Q estimate, an evaluation policy is extracted using
96"
RELATED WORK,0.21179039301310043,"advantage-weighted regression [Wang et al., 2018, Peng et al., 2019]. Our approach has a similar
97"
RELATED WORK,0.21397379912663755,"form of extracting an evaluation from a learned Q estimate. However, one-step RL aims to avoid
98"
RELATED WORK,0.21615720524017468,"distribution shift and iterative error exploitation during iterative dynamic programming. IQL avoids
99"
RELATED WORK,0.2183406113537118,"error exploitation by eliminating OOD action queries and abandoning policy improvement (i.e. the
100"
RELATED WORK,0.2205240174672489,"policy is not trained against the Q estimate). Our work instead tries to address the error exploitation
101"
RELATED WORK,0.22270742358078602,"problem and evaluation performance by using policies of different constraint strengths.
102"
BACKGROUND,0.22489082969432314,"3
Background
103"
BACKGROUND,0.22707423580786026,"We model the environment as a Markov Decision Process (MDP) ⟨S, A, R, T, p0(s), γ, ⟩, where S is
104"
BACKGROUND,0.2292576419213974,"the state space, A is the action space, R is the reward function, T(s′|s, a) is the transition probability,
105"
BACKGROUND,0.2314410480349345,"p0(s) is initial state distribution and γ is a discount factor. In the offline setting, a static dataset
106"
BACKGROUND,0.2336244541484716,"Dβ = {(s, a, r, s′)} is pre-collected by a behavior policy πβ. The goal is to learn a policy πϕ(s) with
107"
BACKGROUND,0.23580786026200873,"the dataset D that maximizes the discounted cumulated rewards in the MDP:
108"
BACKGROUND,0.23799126637554585,"ϕ∗= arg max
ϕ
Es0∼p0(·),at∼πϕ(st),st+1∼T (·|st,at)[ ∞
X"
BACKGROUND,0.24017467248908297,"t=0
γtR(st, at)]
(1)"
BACKGROUND,0.2423580786026201,"Next, we introduce the general policy constraint method, where the policy πϕ and an off-policy Q
109"
BACKGROUND,0.2445414847161572,"estimate Qθ are updated by iteratively taking policy improvement steps and policy evaluation steps,
110"
BACKGROUND,0.24672489082969432,"respectively. The policy evaluation step minimizes the Bellman error:
111"
BACKGROUND,0.24890829694323144,"LQ(θ) = Est,at∼D,at+1∼πϕ(st+1)
 
Qθ(st, at) −(r + γQθ′(st, at+1))
2
.
(2)"
BACKGROUND,0.25109170305676853,"where the θ′ is the parameter for a delayed-updated target Q network. The Q value for the next state is
112"
BACKGROUND,0.25327510917030566,"calculated with actions at+1 from the learning policy that is updated through the policy improvement
113"
BACKGROUND,0.2554585152838428,"step:
114"
BACKGROUND,0.2576419213973799,"Lπ(ϕ) = Es∼D,a∼πϕ(s)[−Qθ(s, a) + wC(πβ, πϕ)],
(3)"
BACKGROUND,0.259825327510917,"where C is a constraint measuring the discrepancy between the policy distribution πϕ and the behavior
115"
BACKGROUND,0.26200873362445415,"distribution πβ. The w ∈(0, ∞] is a weighting factor. Different kinds of constraints were used such
116"
BACKGROUND,0.26419213973799127,"as Maximum Mean Discrepancy (MMD), KL divergence, and reverse KL divergence.
117"
METHOD,0.2663755458515284,"4
Method
118"
METHOD,0.2685589519650655,"In this section, we first introduce the generic algorithm that can be integrated with any policy
119"
METHOD,0.27074235807860264,"constraints method. Next, we introduce two examples based on popular offline RL methods TD3BC
120"
METHOD,0.27292576419213976,"and AWAC. With a mildly constrained evaluation policy, we name these two instances as TD3BC
121"
METHOD,0.27510917030567683,"with MCEP (TD3BC-MCEP) and AWAC with MCEP (AWAC-MCEP).
122"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.27729257641921395,"4.1
Offline RL with mildly constrained evaluation policy
123"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.2794759825327511,"The proposed method is designed for overcoming the tradeoff between a stable policy evaluation and
124"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.2816593886462882,"a performant evaluation policy. In previous constrained policy methods, a restrictive policy constraint
125"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.2838427947598253,"is applied to obtain stable policy evaluation. We retain this benefit but use this policy (actor) ˜π as
126"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.28602620087336245,"a target policy only to obtain stable policy evaluation. To achieve better evaluation performance,
127"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.28820960698689957,"we introduce an MCEP πe that is updated by taking policy improvement steps with the critic Q˜π.
128"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.2903930131004367,"Different from ˜π, πe does not participate in the policy evaluation procedure. Therefore, a mild policy
129"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.2925764192139738,"constraint can be applied, which helps πe go further away from the behavior distribution without
130"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.29475982532751094,"influencing the stability of policy evaluation. We demonstrate the policy spaces and policy trajectories
131"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.29694323144104806,"for ˜π and πe in the l.h.s. diagram of Figure 1, where πe is updated in the wider policy space using Q˜π.
132"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.29912663755458513,"Figure 1: Left: diagram depicts policy trajectories for target policy ˜π and MCEP πe. Right: policy
evaluation steps to update Q˜π and policy improvement steps to update ˜π and πe."
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.30131004366812225,Algorithm 1 MCEP Training
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3034934497816594,"1: Hyperparameters: LR α, EMA η, ˜w and we"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3056768558951965,"2: Initialize: θ, θ′, ψ, and ϕ
3: for i=1, 2, ..., N do
4:
θ ←θ −αLQ(θ) (Equation 2)
5:
θ′ ←(1 −η)θ′ + ηθ
6:
ψ ←ψ −αL˜π(ψ; ˜w) (Equation 3)
7:
ϕ ←ϕ −αLπe(ϕ; we) (Equation 3)"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3078602620087336,"The overall algorithm is shown as pseudo-codes
133"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.31004366812227074,"(Alg. 1). At each step, the Q˜π, ˜πψ and πe
ϕ are
134"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.31222707423580787,"updated iteratively. A policy evaluation step up-
135"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.314410480349345,"dates Q˜π by minimizing the TD error (line 7),
136"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3165938864628821,"i.e. the deviation between the approximate Q
137"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.31877729257641924,"and its target value. Next, a policy improve-
138"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.32096069868995636,"ment step updates ˜πψ (line 6. These two steps
139"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3231441048034934,"form the actor-critic algorithm. After that, πe
ϕ
140"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.32532751091703055,"is extracted from the Q˜π, by taking a policy im-
141"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.32751091703056767,"provement step with a policy constraint that is
142"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3296943231441048,"likely milder than the constraint for ˜πψ (line 7).
143"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3318777292576419,"Many approaches can be taken to obtain a milder
144"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.33406113537117904,"policy constraint. For example, tuning down the weight factor we for the policy constraint term or
145"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.33624454148471616,"replacing the constraint measurement with a less restrictive one. Note that the constraint for πe
ϕ is
146"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3384279475982533,"necessary (the constraint term should not be dropped) as the Q˜π has large approximate errors for
147"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3406113537117904,"state-action pairs that are far from the data distribution.
148"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.34279475982532753,"4.2
Two Examples: TD3BC-MCEP and AWAC-MCEP
149"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.34497816593886466,"TD3BC with MCEP TD3BC takes a minimalist modification on the online RL algorithm TD3. To
150"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3471615720524017,"keep the learned policy to stay close to the behavior distribution, a behavior-cloning term is added to
151"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.34934497816593885,"the policy improvement objective. TD3 learns a deterministic policy therefore the behavior cloning is
152"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.35152838427947597,"achieved by directly regressing the data actions. For TD3BC-MCEP, the target policy ˜πψ has the
153"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3537117903930131,"same policy improvement objective as TD3BC:
154"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3558951965065502,"L˜π(ψ) = E(s,a)∼D[−˜λQθ(s, ˜πψ(s)) +
 
a −˜πψ(s)
2],
(4)"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.35807860262008734,"where the ˜λ =
˜α
1
N
P"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.36026200873362446,"si,ai |Qθ(si,ai)| is a normalizer for Q values with a hyper-parameter ˜α: The Qθ
155"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3624454148471616,"is updated with the policy evaluation step similar to Eq. 2 using ˜πψ. The MCEP πe
ϕ is updated by
156"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3646288209606987,"policy improvement steps with the Q˜π taking part in. The policy improvement objective function for
157"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.36681222707423583,"πe
ϕ is similar to Eq. 4 but with a higher-value αe for the Q-value normalizer λe. The final objective
158"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.36899563318777295,"for πe
ϕ is
159"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.37117903930131,"Lπe(ϕ) = E(s,a)∼D[−λeQ(s, πe
ϕ(s)) +
 
a −πe
ϕ(s)
2].
(5)"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.37336244541484714,"AWAC with MCEP AWAC [Nair et al., 2020] is an advantage-weighted behavior cloning method.
160"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.37554585152838427,"As the target policy imitates the actions from the behavior distribution, it stays close to the behavior
161"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3777292576419214,"distribution during learning. In AWAC-MCEP, the policy evaluation follows the Eq. 2 with the target
162"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3799126637554585,"(a) Toy maze MDP
(b) V ∗, π∗
(c) V˜π, ˜π
(d) V˜π, arg max Q˜π"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.38209606986899564,"Figure 2: Evaluation of policy constraint method on a toy maze MDP 2a. In other figures, the color
of a grid represents the state value and arrows indicate the actions from the corresponding policy. 2b
shows the optimal value function and one optimal policy. 2c shows a constrained policy trained from
the above-mentioned offline data, with its value function calculated by Vπ = EaQ(s, π(a|s)). The
policy does not perform well in the low state-value area but its value function is close to the optimal
value function. 2d indicates that an optimal policy is recovered by deriving the greedy policy from
the off-policy Q estimate (the critic)."
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.38427947598253276,"policy ˜πψ that updates with the following objective:
163"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3864628820960699,"L˜π(ψ) = Es,a∼D"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.388646288209607,"
−exp
 1"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.39082969432314413,"˜λ
A(s, a)

log ˜πψ(a|s)

,
(6)"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3930131004366812,"where the advantage A(s, a) = Qθ(s, a)−Qθ(s, ˜πψ(s)). This objective function solves an advantage-
164"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.3951965065502183,"weighted maximum likelihood. Note that the gradient will not be passed through the advantage term.
165"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.39737991266375544,"As this objective has no policy improvement term, we use the original policy improvement with KL
166"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.39956331877729256,"divergence as the policy constraint and construct the following policy improvement objective:
167"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.4017467248908297,"Lπe(ϕ) = Es,a∼D,ˆa∼πe(·|s)[−Q(s, ˆa) + λeDKL
 
πβ(·|s)||πe
ϕ(·|s)

]
(7)"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.4039301310043668,"= Es,a∼D,ˆa∼πe(·|s)[−Q(s, ˆa) −λe log πe
ϕ(a|s)],
(8)
where the weighting factor λe is a hyper-parameter. Although the Eq. 6 is derived by solving Eq. 8
168"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.40611353711790393,"in a parametric-policy space, the original problem (Eq. 8) is less restrictive even with ˜λ = λe as it
169"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.40829694323144106,"includes a −Q(s, πe(s)) term. This difference means that even with a λe > ˜λ, the policy constraint
170"
OFFLINE RL WITH MILDLY CONSTRAINED EVALUATION POLICY,0.4104803493449782,"for πe could still be more relaxed than the policy constraint for ˜π.
171"
EXPERIMENTS,0.4126637554585153,"5
Experiments
172"
EXPERIMENTS,0.4148471615720524,"In this section, we set up 4 groups of experiments to illustrate: 1) the policy constraint might degrade
173"
EXPERIMENTS,0.4170305676855895,"the evaluation performance by forcing the policy to stay close to low-state-value transitions. 2) The
174"
EXPERIMENTS,0.4192139737991266,"suitable constraint for the final inference could be milder than the ones for safe Q estimates. 3) Our
175"
EXPERIMENTS,0.42139737991266374,"method shows significant performance improvement compared to the target policy and achieves
176"
EXPERIMENTS,0.42358078602620086,"competitive results to state-of-the-art offline RL methods on MuJoCo locomotion tasks. 4) the MCEP
177"
EXPERIMENTS,0.425764192139738,"generally gains a higher estimate Q compared to the target policy. Additionally, we adopt 2 groups of
178"
EXPERIMENTS,0.4279475982532751,"ablation studies to verify the benefit of an MCEP and to investigate the constraint strengths of MCEP.
179"
EXPERIMENTS,0.43013100436681223,"Environments D4RL [Fu et al., 2020] is an offline RL benchmark consisting of many task sets.
180"
EXPERIMENTS,0.43231441048034935,"Our experiments involve MuJoCo locomotion tasks (-v2) and two tasks from Adroit (-v0). For
181"
EXPERIMENTS,0.4344978165938865,"MuJoCo locomotion tasks, we select 4 versions of datasets: data collected by a uniformly-random
182"
EXPERIMENTS,0.4366812227074236,"agent (random), collected by a medium-performance policy (medium), a 50% −50% mixture of the
183"
EXPERIMENTS,0.4388646288209607,"medium data and the replay buffer during training a medium-performance policy (medium-replay), a
184"
EXPERIMENTS,0.4410480349344978,"50% −50% mixture of the medium data and expert demonstrations (medium-expert). For Adroit,
185"
EXPERIMENTS,0.4432314410480349,"we select pen-human and pen-cloned, where the pen-human includes a small number of human
186"
EXPERIMENTS,0.44541484716157204,"demonstrations, and pen-cloned is a 50% −50% mixture of demonstrations and data collected by
187"
EXPERIMENTS,0.44759825327510916,"rolling out an imitation policy on the demonstrations.
188"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4497816593886463,"5.1
Target policy that enables safe Q estimate might be overly constrained
189"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4519650655021834,"To investigate the policy constraint under a highly suboptimal dataset, we set up a toy maze MDP that
190"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.45414847161572053,"is similar to the one used in [Kostrikov et al., 2022]. The environment is depicted in Figure 2a, where
191"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.45633187772925765,"Figure 4: The training process of TD3BC and AWAC. Left: TD3BC
on hopper-medium-v2. Middle: TD3BC on walker2d-medium-replay-
v2. Right: AWAC on hopper-medium-replay-v2."
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4585152838427948,"Figure 5:
α values in
TD3BC for value estimate
and test time inference in
MuJoCo locomotion tasks."
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4606986899563319,"the lower left yellow grid is the starting point and the upper left green grid is the terminal state that
192"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.462882096069869,"gives a reward of 10. Other grids give no reward. Dark blue indicates un-walkable areas. The action
193"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4650655021834061,"space is defined as 4 direction movements (arrows) and staying where the agent is (filled circles).
194"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4672489082969432,"There is a 25% probability that a random action is taken instead of the action from the agent. For the
195"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.46943231441048033,"dataset, 99 trajectories are collected by a uniformly random agent and 1 trajectory is collected by an
196"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.47161572052401746,"expert policy. Fig. 2b shows the optimal value function (colors) and one of the optimal policies.
197"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4737991266375546,"We trained a constrained policy using Eq. 2 and Eq. 8 in an actor-critic manner, where the actor is
198"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4759825327510917,"constrained by a KL divergence with a weight factor of 1. Figure 2c shows the value function and the
199"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4781659388646288,"policy. We observe that the learned value function is close to the optimal one in Figure 2b. However,
200"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.48034934497816595,"the policy does not make optimal actions in the lower left areas where the state values are relatively
201"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.48253275109170307,"low. As the policy improvement objective shows a trade-off between the Q and the KL divergence,
202"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4847161572052402,"when the Q value is low, the KL divergence term will obtain higher priority. i.e. in low-Q-value
203"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4868995633187773,"areas, the KL divergence takes the majority for the learning objective, which makes the policy stays
204"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4890829694323144,"closer to the transitions in low-value areas. However, we find that the corresponding value function
205"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4912663755458515,"indicates an optimal policy. In Figure 2d, we recover a greedy policy underlying the learned critic
206"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.49344978165938863,"that shows an optimal policy. In conclusion, the constraint might degrade the evaluation performance
207"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.49563318777292575,"although the learned critic may indicate a better policy. Although such a trade-off between the Q
208"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.4978165938864629,"term and the KL divergence term can be alleviated in previous work [Fujimoto and Gu, 2021] by
209"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.5,"normalizing the Q values, in the next section, we will illustrate that the constraint required to obtain
210"
TARGET POLICY THAT ENABLES SAFE Q ESTIMATE MIGHT BE OVERLY CONSTRAINED,0.5021834061135371,"performant evaluation policy can still cause unstable value estimate.
211"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5043668122270742,"5.2
Test-time inference requires milder constraints
212"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5065502183406113,"The previous experiment shows that a restrictive constraint might harm the test-time inference, which
213"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5087336244541485,"motivates us to investigate what constraints make better evaluation performance. Firstly, we relax the
214"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5109170305676856,"policy constraint on TD3BC and AWAC by setting up different hyper-parameter values that control
215"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5131004366812227,"the strengths of the policy constraints. For TD3BC, we set α = {1, 4, 10} ([Fujimoto and Gu, 2021]
216"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5152838427947598,"recommends α = 2.5). For AWAC, we set λ = {1.0, 0.5, 0.3, 0.1} ([Nair et al., 2020] recommends
217"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.517467248908297,"λ = 1). Finally, We visualize the evaluation performance and the learned Q estimates.
218"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.519650655021834,"In Figure 4, the left two columns show the training of TD3BC in the hopper-medium-v2 and walker2d-
219"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5218340611353712,"medium-replay-v2. In both domains, we found that using a milder constraint by tuning the α from 1 to
220"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5240174672489083,"4 improves the evaluation performance, which motivates us to expect better performance with α = 10.
221"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5262008733624454,"As shown in the lower row, we do observe higher performances in some training steps. However,
222"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5283842794759825,"unstable training is caused by the divergence in value estimate, which indicates the tradeoff between
223"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5305676855895196,"the stable Q estimate and the evaluation performance. The rightmost column shows the training
224"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5327510917030568,"of AWAC in hopper-medium-replay-v2, we observe higher evaluation performance by relaxing the
225"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5349344978165939,"constraint (λ > 1). Although the Q estimate keeps stable during the training in all λ values, higher λ
226"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.537117903930131,"results in unstable policy performance and causes the performance crash with λ = 0.1.
227"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5393013100436681,"Concluding on all these examples, a milder constraint can potentially improve the performance
228"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5414847161572053,"but may cause unstable Q estimates or unstable policy performances. As we find that relaxing the
229"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5436681222707423,"constraint on current methods triggers unstable training, which hinders the investigation of constraints
230"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5458515283842795,"Task Name
BC
CQL
IQL
TD3BC
TD3BC-MCEP AWAC
AWAC-MCEP
(ours)
(ours)
halfcheetah-r
2.2±0.0
-
10±1.7
11.7±0.4
28.8±1.0
9.6±0.4
34.9±0.8
hopper-r
4.7±0.1
-
8.1±0.4
8.3±0.1
8.0±0.4
5.3±0.4
9.8±0.5
walker2d-r
1.6±0.0
-
5.6±0.1
1.2±0.0
-0.2±0.1
5.2±1.0
3.1±0.4
halfcheetah-m
42.4±0.1 44.0
47.4±0.1
48.7±0.2
55.5±0.4
45.1±0
46.6±0
hopper-m
54.1±1.1 58.5
65±3.6
56.1±1.2
91.8±0.9
58.9±1.9
91.1±1.5
walker2d-m
71±1.7
72.5
80.4±1.7
85.2±0.9
88.8±0.5
79.6±1.5
83.4±0.9
halfcheetah-m-r 37.8±1.1 45.5
43.2±0.8
44.8±0.3
50.6±0.2
43.3±0.1
44.9±0.1
hopper-m-r
22.5±3.0 95.0
74.2±5.3
55.2±10.8 100.9±0.4
64.8±6.2
101.4±0.2
walker2d-m-r
14.4±2.7 77.2
62.7±1.9
50.9±16.1 86.3±3.2
84.1±0.6
84.6±1.3
halfcheetah-m-e 62.3±1.5 91.6
91.2±1.0
87.1±1.4
71.5±3.7
77.6±2.6
76.2±5.5
hopper-m-e
52.5±1.4 105.4 110.2±0.3 91.7±10.5 80.1±12.7
52.4±8.7
92.5±8.3
walker2d-m-e
107±1.1
108.8 111.1±0.5 110.4±0.5 111.7±0.3
109.5±0.2 110.3±0.1
Average
39.3
-
59.0
54.2
64.5
52.9
64.9
pen-human
76.8±4.8 37.5
64.2±10.4 61.6±11
58.6±20.8
34.7±11.8 23.3 ±5.6
pen-cloned
28.5±6.7 39.2
32.1±7.5
49±9.5
43.4±20.3
20.8±7.3
19.0±7.5
Average
52.6
38.3
48.1
55.3
51.0
27.7
21.1"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5480349344978166,"Table 1: Normalized episode returns on D4RL benchmark. The results (except for CQL) are means
and standard errors from the last step of 5 runs using different random seeds. Performances that are
higher than corresponding baselines are underlined and task-wise best performances are bolded."
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5502183406113537,"for better evaluation performance. We instead systematically study the constraint strengths in TD3BC
231"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5524017467248908,"and TD3BC with evaluation policy (TD3BC-EP).
232"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5545851528384279,"We first tune the α for TD3BC to unveil the range for safe Q estimates. Then in TD3BC-EP, we
233"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5567685589519651,"tune the αe for the evaluation policy with a fixed ˜α = 2.5 to approximate the constraint range of
234"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5589519650655022,"better test inference performance (i.e. where the evaluation policy outperforms the target policy). The
235"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5611353711790393,"˜α = 2.5 is selected to ensure a stable Q estimate (also the paper-recommended value). The α (αe) is
236"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5633187772925764,"tuned within {2.5, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100}. For each α (αe), we observe the training
237"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5655021834061136,"of 5 runs with different random seeds. In Figure 5, we visualize these two ranges for each task from
238"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5676855895196506,"MuJoCo locomotion set. The blue area shows α values where the TD3BC Q estimate is stable for all
239"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5698689956331878,"seeds. The edge shows the lowest α value that causes Q value explosion. The orange area shows the
240"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5720524017467249,"range of αe where the learned evaluation policy outperforms the target policy. Its edge (the orange
241"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.574235807860262,"line) shows the lowest αe values where the evaluation policy performance is worse than the target
242"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5764192139737991,"policy. For each task, the orange area has a lower bound αe = 2.5 where the evaluation policy shows
243"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5786026200873362,"a similar performance to the target policy.
244"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5807860262008734,"Note that α weighs the Q term and thus a larger α indicates a less restrictive constraint. Comparing
245"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5829694323144105,"the blue area and the orange area, we observe that in 6 out of the 9 tasks, the α for better inference
246"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5851528384279476,"performance is higher than the α that enables safe Q estimates, indicating that test-time inference
247"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5873362445414847,"requires milder constraints. In the next section, we show that with an MCEP, we can achieve much
248"
TEST-TIME INFERENCE REQUIRES MILDER CONSTRAINTS,0.5895196506550219,"better inference performance without breaking the stable Q estimates.
249"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.5917030567685589,"5.3
Comparison on MuJoCo locomotion and Adroit
250"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.5938864628820961,"We compare the proposed method to state-of-the-art offline RL methods CQL and IQL, together with
251"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.5960698689956332,"our baselines TD3BC and AWAC. Similar hyper-parameters are used for all tasks from the same
252"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.5982532751091703,"domain. For our baseline methods (TD3BC and AWAC), we use the hyper-parameter recommended
253"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6004366812227074,"by their papers. TD3BC uses α = 2.5 for its Q value normalizer and AWAC uses 1.0 for the
254"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6026200873362445,"advantage value normalizer. In TD3BC-MCEP, the target policy uses ˜α = 2.5 and the MCEP uses
255"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6048034934497817,"αe = 10. In AWAC-MCEP, the target policy has ˜λ = 1.0 and the MCEP has λe = 0.6. The full list
256"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6069868995633187,"of hyper-parameters can be found in the Appendix.
257"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6091703056768559,"As is shown in Table 1, we observe that the evaluation policies with a mild constraint significantly
258"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.611353711790393,"outperform their corresponding target policy. TD3BC-MCEP gains progress on all medium and
259"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6135371179039302,"medium-replay datasets. Although the progress is superior, we observe a performance degradation on
260"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6157205240174672,"the medium-expert datasets which indicates an overly relaxed constraint for the evaluation policy. To
261"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6179039301310044,"overcome this imbalance problem, we designed a behavior-cloning normalizer. The results are shown
262"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6200873362445415,"in the Appendix. Nevertheless, the TD3BC-MCEP achieves much better general performance than the
263"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6222707423580786,"target policy. In the AWAC-MCEP, we observe a consistent performance improvement over the target
264"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6244541484716157,"policy on most tasks. Additionally, evaluation policies from both TD3BC-MCEP and AWAC-MCEP
265"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6266375545851528,"outperform the CQL and IQL while the target policies have relatively low performances. On Adroit
266"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.62882096069869,"tasks, the best results are obtained by behavioral cloning agent and TD3BC with a high BC weighting
267"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.631004366812227,"factor. Other agents fail to outperform the BC agent. We observe that MCEP does not benefit these
268"
COMPARISON ON MUJOCO LOCOMOTION AND ADROIT,0.6331877729257642,"tasks where behavior cloning is essential for the evaluation performance.
269"
ABLATION STUDY,0.6353711790393013,"5.4
Ablation Study
270"
ABLATION STUDY,0.6375545851528385,"In this section, we design 2 groups of ablation studies to investigate the effect of the extra evaluation
271"
ABLATION STUDY,0.6397379912663755,"policy and its constraint strengths. Reported results are averaged on 5 runs of different random seeds.
272"
ABLATION STUDY,0.6419213973799127,"Figure 6: Left: TD3BC with α = 2.5, α = 10 and TD3BC-
MCEP with ˜α = 2.5, αe = 10. Right: AWAC with λ = 1.0,
λ = 0.5 and AWAC-MCEP with ˜λ = 1.0 and λe = 0.5."
ABLATION STUDY,0.6441048034934498,"Performance of the extra evaluation
273"
ABLATION STUDY,0.6462882096069869,"policy. Now, we investigate the per-
274"
ABLATION STUDY,0.648471615720524,"formance of the introduced evalua-
275"
ABLATION STUDY,0.6506550218340611,"tion policy πe. For TD3BC, we set
276"
ABLATION STUDY,0.6528384279475983,"the parameter α = {2.5, 10.0}. A
277"
ABLATION STUDY,0.6550218340611353,"large α indicates a milder constraint.
278"
ABLATION STUDY,0.6572052401746725,"After that, we train TD3BC-MCEP
279"
ABLATION STUDY,0.6593886462882096,"with ˜α = 2.5 and αe = 10.0. For
280"
ABLATION STUDY,0.6615720524017468,"AWAC, we trained AWAC with the
281"
ABLATION STUDY,0.6637554585152838,"λ = {1.0, 0.5} and AWAC-MCEP
282"
ABLATION STUDY,0.665938864628821,"with ˜λ = 1.0 and λe = 0.5.
283"
ABLATION STUDY,0.6681222707423581,"The results are shown in Figure 6.
284"
ABLATION STUDY,0.6703056768558951,"By comparing TD3BC of different α
285"
ABLATION STUDY,0.6724890829694323,"values, we found a milder constraint
286"
ABLATION STUDY,0.6746724890829694,"(α = 10.0) brought performance im-
287"
ABLATION STUDY,0.6768558951965066,"provement in hopper tasks but de-
288"
ABLATION STUDY,0.6790393013100436,"grades the performance in walker2d tasks. The degradation is potentially caused by unstable value
289"
ABLATION STUDY,0.6812227074235808,"estimates (see experiment at section 5.2). Finally, the evaluation policy trained from the critic learned
290"
ABLATION STUDY,0.6834061135371179,"with a target policy with α = 2.5 achieves the best performance in all three tasks. In AWAC, a lower
291"
ABLATION STUDY,0.6855895196506551,"λ value brought policy improvement in hopper tasks but degrades performances in half-cheetah and
292"
ABLATION STUDY,0.6877729257641921,"walker2d tasks. Finally, an evaluation policy obtains the best performances in all tasks.
293"
ABLATION STUDY,0.6899563318777293,"In conclusion, we observe consistent performance improvement brought by an extra MCEP that
294"
ABLATION STUDY,0.6921397379912664,"circumvents the tradeoff brought by the constraint.
295"
ABLATION STUDY,0.6943231441048034,"Figure 7: Left: TD3BC-EP with α = 1.0, α = 2.5 and
α = 10.0. Right: AWAC-EP with λ = 1.4, λ = 1.0 and
λ = 0.6."
ABLATION STUDY,0.6965065502183406,"Constraint strengths of the evalua-
296"
ABLATION STUDY,0.6986899563318777,"tion policy. We set up two groups of
297"
ABLATION STUDY,0.7008733624454149,"ablation experiments to investigate the
298"
ABLATION STUDY,0.7030567685589519,"performance of evaluation policy un-
299"
ABLATION STUDY,0.7052401746724891,"der different constraint strengths. For
300"
ABLATION STUDY,0.7074235807860262,"TD3BC-MCEP, we tune the constraint
301"
ABLATION STUDY,0.7096069868995634,"strength by setting the Q normalizer
302"
ABLATION STUDY,0.7117903930131004,"hyper-parameter. The target policy
303"
ABLATION STUDY,0.7139737991266376,"hyper-parameter is fixed to α = 2.5.
304"
ABLATION STUDY,0.7161572052401747,"We pick three strengths for evaluation
305"
ABLATION STUDY,0.7183406113537117,"policy αe = {1.0, 2.5, 10.0} to create
306"
ABLATION STUDY,0.7205240174672489,"more restrictive, similar, and milder
307"
ABLATION STUDY,0.722707423580786,"constraints, respectively. For AWAC-
308"
ABLATION STUDY,0.7248908296943232,"MCEP, the target policy uses λ = 1.0.
309"
ABLATION STUDY,0.7270742358078602,"However, it is not straightforward to
310"
ABLATION STUDY,0.7292576419213974,"create a similar constraint for the eval-
311"
ABLATION STUDY,0.7314410480349345,"uation policy as it has a different policy improvement objective. We set λe = {0.6, 1.0, 1.4} to show
312"
ABLATION STUDY,0.7336244541484717,"how performance changes with different constraint strengths.
313"
ABLATION STUDY,0.7358078602620087,"The performance improvements over the target policy are shown in Fig. 7. The left column shows a
314"
ABLATION STUDY,0.7379912663755459,"significant performance drop when the evaluation policy has a more restrictive constraint (αe = 1.0)
315"
ABLATION STUDY,0.740174672489083,"than the target policy. A very close performance is shown when the target policy and the evaluation
316"
ABLATION STUDY,0.74235807860262,"policy have similar policy constraint strengths (αe = 2.5). Significant policy improvements are
317"
ABLATION STUDY,0.7445414847161572,"(a) medium-expert
(b) medium
(c) medium-replay
(d) random"
ABLATION STUDY,0.7467248908296943,"Figure 9: The distributions of Q(s, ˜π(s)) −Q(s, a) and Q(s, πe(s)) −
Q(s, a) on MuJoCo locomotion tasks. First row: policies of TD3BC-
MCEP learned in walker2d tasks. Second row: policies of AWAC-MCEP
learned in half cheetah tasks. See the Appendix for full results."
ABLATION STUDY,0.7489082969432315,"env
˜π (%)
πe (%)
TD3BC-MCEP
wa-me
69.8
87.2
wa-m
66.2
82.7
wa-mr
71.8
88.7
wa-r
89.6
99.0
AWAC-MCEP
ha-me
63.4
70.8
ha-m
64.7
68.3
ha-mr
68.6
73.1
ha-r
75.3
95.6"
ABLATION STUDY,0.7510917030567685,"Table 2: Proportion of
Q(s, π(s))
>
Q(s, a)
for target policies and
evalution policies in dif-
ferent tasks."
ABLATION STUDY,0.7532751091703057,"obtained with the target policy having a milder constraint (αe = 10). The right column presents the
318"
ABLATION STUDY,0.7554585152838428,"results of AWAC-MCEP. Generally, the performance in hopper tasks keeps increasing with milder
319"
ABLATION STUDY,0.75764192139738,"constraints while the half-cheetah and walker2d tasks show performances that increase from λ = 1.4
320"
ABLATION STUDY,0.759825327510917,"to λ = 1 and similar performances between λ = 1 and λ = 0.6. Compared to the target policy, the
321"
ABLATION STUDY,0.7620087336244541,"evaluation policy consistently outperforms in half-cheetah and hopper tasks. On the walker2d task, a
322"
ABLATION STUDY,0.7641921397379913,"strong constraint (λ = 1.4) causes a performance worse than the target policy but milder constraints
323"
ABLATION STUDY,0.7663755458515283,"(λ = {1, 0.6}) obtain similar performance to the target policy.
324"
ABLATION STUDY,0.7685589519650655,"In conclusion, for both algorithms, we observe that on evaluation policy, a milder constraint obtains
325"
ABLATION STUDY,0.7707423580786026,"higher performance than the target policy while a restrictive constraint may harm the performance.
326"
ESTIMATED Q VALUES FOR THE LEARNED EVALUATION POLICIES,0.7729257641921398,"5.5
Estimated Q values for the learned evaluation policies
327"
ESTIMATED Q VALUES FOR THE LEARNED EVALUATION POLICIES,0.7751091703056768,"To compare the performance of the policies learned in Section 5.3 on the learning objective (max-
328"
ESTIMATED Q VALUES FOR THE LEARNED EVALUATION POLICIES,0.777292576419214,"imizing the Q values), we counted Q differences between the policy action and the data action
329"
ESTIMATED Q VALUES FOR THE LEARNED EVALUATION POLICIES,0.7794759825327511,"Q(s, π(s)) −Q(s, a) in the training data (visualized in Figure 9). Proportions of data points that
330"
ESTIMATED Q VALUES FOR THE LEARNED EVALUATION POLICIES,0.7816593886462883,"show positive differences are listed in Table 2, where we find that on more than half of the data, both
331"
ESTIMATED Q VALUES FOR THE LEARNED EVALUATION POLICIES,0.7838427947598253,"the target policy and the MCEP have larger Q estimation than the behavior actions. Additionally,
332"
ESTIMATED Q VALUES FOR THE LEARNED EVALUATION POLICIES,0.7860262008733624,"the proportions for the MCEP are higher than the proportions for the target policy in all datasets,
333"
ESTIMATED Q VALUES FOR THE LEARNED EVALUATION POLICIES,0.7882096069868996,"indicating that the MCEP is able to move further toward large Q values.
334"
CONCLUSION,0.7903930131004366,"6
Conclusion
335"
CONCLUSION,0.7925764192139738,"This work focuses on the policy constraints methods where the constraint addresses the tradeoff
336"
CONCLUSION,0.7947598253275109,"between stable value estimate and evaluation performance. While to what extent the constraint
337"
CONCLUSION,0.7969432314410481,"achieves the best results for each end of this tradeoff remains unknown, we first investigate the
338"
CONCLUSION,0.7991266375545851,"constraint strength range for a stable value estimate and for evaluation performance. Our findings
339"
CONCLUSION,0.8013100436681223,"indicate that test time inference requires milder constraints that can go beyond the range of stable
340"
CONCLUSION,0.8034934497816594,"value estimates. We propose to use an auxiliary mildly constrained evaluation policy to circumvent
341"
CONCLUSION,0.8056768558951966,"the above-mentioned tradeoff and derive a performant evaluation policy. The empirical results show
342"
CONCLUSION,0.8078602620087336,"that MCEP obtains significant performance improvement compared to target policy and achieves
343"
CONCLUSION,0.8100436681222707,"competitive results to state-of-the-art offline RL methods. Our ablation studies show that an auxiliary
344"
CONCLUSION,0.8122270742358079,"evaluation policy and a milder policy constraint are essential for the proposed method. Additional
345"
CONCLUSION,0.8144104803493449,"empirical analysis demonstrates higher estimated Q values are obtained by the MCEP.
346"
CONCLUSION,0.8165938864628821,"Limitations. Although the MCEP is able to obtain a better performance, it depends on stable value
347"
CONCLUSION,0.8187772925764192,"estimation. Unstable value learning may crash both the target policy and the evaluation policy. While
348"
CONCLUSION,0.8209606986899564,"the target policy may recover its performance by iterative policy improvement and policy evaluation,
349"
CONCLUSION,0.8231441048034934,"we observe that the evaluation policy may fail to do so. Therefore, a restrictive constrained target
350"
CONCLUSION,0.8253275109170306,"policy that stabilizes the value learning is essential for the proposed method.
351"
REFERENCES,0.8275109170305677,"References
352"
REFERENCES,0.8296943231441049,"David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy
353"
REFERENCES,0.8318777292576419,"evaluation. Advances in neural information processing systems, 34:4933–4946, 2021.
354"
REFERENCES,0.834061135371179,"Y. Cai, C. Zhang, L. Zhao, W. Shen, X. Zhang, L. Song, J. Bian, T. Qin, and T. Liu. Td3 with
355"
REFERENCES,0.8362445414847162,"reverse kl regularizer for offline reinforcement learning from mixed datasets. In 2022 IEEE
356"
REFERENCES,0.8384279475982532,"International Conference on Data Mining (ICDM), pages 21–30, Los Alamitos, CA, USA, dec
357"
REFERENCES,0.8406113537117904,"2022. IEEE Computer Society. doi: 10.1109/ICDM54844.2022.00012. URL https://doi.
358"
REFERENCES,0.8427947598253275,"ieeecomputersociety.org/10.1109/ICDM54844.2022.00012.
359"
REFERENCES,0.8449781659388647,"Wojciech M Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz,
360"
REFERENCES,0.8471615720524017,"and Max Jaderberg. Distilling policy distillation. In The 22nd international conference on artificial
361"
REFERENCES,0.8493449781659389,"intelligence and statistics, pages 1331–1340. PMLR, 2019.
362"
REFERENCES,0.851528384279476,"Soysal Degirmenci and Chris Jones. Benchmarking offline reinforcement learning algorithms for
363"
REFERENCES,0.8537117903930131,"e-commerce order fraud evaluation. In 3rd Offline RL Workshop: Offline RL as a”Launchpad”.
364"
REFERENCES,0.8558951965065502,"Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
365"
REFERENCES,0.8580786026200873,"data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
366"
REFERENCES,0.8602620087336245,"Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
367"
REFERENCES,0.8624454148471615,"Advances in neural information processing systems, 34:20132–20145, 2021.
368"
REFERENCES,0.8646288209606987,"Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
369"
REFERENCES,0.8668122270742358,"critic methods. In International conference on machine learning, pages 1587–1596. PMLR,
370"
REFERENCES,0.868995633187773,"2018.
371"
REFERENCES,0.87117903930131,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
372"
REFERENCES,0.8733624454148472,"exploration. In International conference on machine learning, pages 2052–2062. PMLR, 2019.
373"
REFERENCES,0.8755458515283843,"Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
374"
REFERENCES,0.8777292576419214,"Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
375"
REFERENCES,0.8799126637554585,"implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
376"
REFERENCES,0.8820960698689956,"Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,
377"
REFERENCES,0.8842794759825328,"Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic rein-
378"
REFERENCES,0.8864628820960698,"forcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.
379"
REFERENCES,0.888646288209607,"Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit
380"
REFERENCES,0.8908296943231441,"q-learning. In International Conference on Learning Representations, 2022. URL https://
381"
REFERENCES,0.8930131004366813,"openreview.net/forum?id=68n2s9ZJWF8.
382"
REFERENCES,0.8951965065502183,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
383"
REFERENCES,0.8973799126637555,"q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems,
384"
REFERENCES,0.8995633187772926,"32, 2019.
385"
REFERENCES,0.9017467248908297,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
386"
REFERENCES,0.9039301310043668,"reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.
387"
REFERENCES,0.9061135371179039,"Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline
388"
REFERENCES,0.9082969432314411,"model-free robotic reinforcement learning. In Conference on Robot Learning, pages 417–428.
389"
REFERENCES,0.9104803493449781,"PMLR, 2022.
390"
REFERENCES,0.9126637554585153,"Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline
391"
REFERENCES,0.9148471615720524,"reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
392"
REFERENCES,0.9170305676855895,"editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.
393"
REFERENCES,0.9192139737991266,"net/forum?id=VYYf6S67pQc.
394"
REFERENCES,0.9213973799126638,"Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
395"
REFERENCES,0.9235807860262009,"reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
396"
REFERENCES,0.925764192139738,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
397"
REFERENCES,0.9279475982532751,"Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
398"
REFERENCES,0.9301310043668122,"Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning
399"
REFERENCES,0.9323144104803494,"from images with latent space models. In Learning for Dynamics and Control, pages 1154–1168.
400"
REFERENCES,0.9344978165938864,"PMLR, 2021.
401"
REFERENCES,0.9366812227074236,"Dhruv Shah, Arjun Bhorkar, Hrishit Leen, Ilya Kostrikov, Nicholas Rhinehart, and Sergey Levine.
402"
REFERENCES,0.9388646288209607,"Offline reinforcement learning for visual navigation. In 6th Annual Conference on Robot Learning,
403"
REFERENCES,0.9410480349344978,"2022. URL https://openreview.net/forum?id=uhIfIEIiWm_.
404"
REFERENCES,0.9432314410480349,"Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert,
405"
REFERENCES,0.9454148471615721,"Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked:
406"
REFERENCES,0.9475982532751092,"Behavior modelling priors for offline reinforcement learning. In International Conference on
407"
REFERENCES,0.9497816593886463,"Learning Representations, 2020. URL https://openreview.net/forum?id=rke7geHtwH.
408"
REFERENCES,0.9519650655021834,"Anikait Singh, Aviral Kumar, Quan Vuong, Yevgen Chebotar, and Sergey Levine. Offline rl with
409"
REFERENCES,0.9541484716157205,"realistic datasets: Heteroskedasticity and support constraints. arXiv preprint arXiv:2211.01052,
410"
REFERENCES,0.9563318777292577,"2022.
411"
REFERENCES,0.9585152838427947,"Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural
412"
REFERENCES,0.9606986899563319,"language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.
413"
REFERENCES,0.962882096069869,"Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph
414"
REFERENCES,0.9650655021834061,"Modayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648,
415"
REFERENCES,0.9672489082969432,"2018.
416"
REFERENCES,0.9694323144104804,"Qing Wang, Jiechao Xiong, Lei Han, Peng Sun, Han Liu, and Tong Zhang. Exponentially weighted
417"
REFERENCES,0.9716157205240175,"imitation learning for batched historical data. In Proceedings of the 32nd International Conference
418"
REFERENCES,0.9737991266375546,"on Neural Information Processing Systems, pages 6291–6300, 2018.
419"
REFERENCES,0.9759825327510917,"Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy opti-
420"
REFERENCES,0.9781659388646288,"mization for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
421"
REFERENCES,0.980349344978166,"and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL
422"
REFERENCES,0.982532751091703,"https://openreview.net/forum?id=KCXQ5HoM-fy.
423"
REFERENCES,0.9847161572052402,"Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
424"
REFERENCES,0.9868995633187773,"arXiv preprint arXiv:1911.11361, 2019.
425"
REFERENCES,0.9890829694323144,"Lantao Yu, Tianhe Yu, Jiaming Song, Willie Neiswanger, and Stefano Ermon.
Offline imita-
426"
REFERENCES,0.9912663755458515,"tion learning with suboptimal demonstrations via relaxed distribution matching. arXiv preprint
427"
REFERENCES,0.9934497816593887,"arXiv:2303.02569, 2023.
428"
REFERENCES,0.9956331877729258,"Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforce-
429"
REFERENCES,0.9978165938864629,"ment learning. In Conference on Robot Learning, pages 1719–1735. PMLR, 2021.
430"
