Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016339869281045752,"Recent work demonstrated the existence of Boolean functions for which Shapley
1"
ABSTRACT,0.0032679738562091504,"values provide misleading information about the relative importance of features in
2"
ABSTRACT,0.004901960784313725,"rule-based explanations. Such misleading information was broadly categorized into
3"
ABSTRACT,0.006535947712418301,"a number of possible issues. Each of those issues relates with features being relevant
4"
ABSTRACT,0.008169934640522876,"or irrelevant for a prediction, and all are significant regarding the inadequacy of
5"
ABSTRACT,0.00980392156862745,"Shapley values for rule-based explainability. This earlier work devised a brute-force
6"
ABSTRACT,0.011437908496732025,"approach to identify Boolean functions, defined on small numbers of features, and
7"
ABSTRACT,0.013071895424836602,"also associated instances, which displayed such inadequacy-revealing issues, and so
8"
ABSTRACT,0.014705882352941176,"served as evidence to the inadequacy of Shapley values for rule-based explainability.
9"
ABSTRACT,0.016339869281045753,"However, an outstanding question is how frequently such inadequacy-revealing
10"
ABSTRACT,0.017973856209150325,"issues can occur for Boolean functions with arbitrary large numbers of features.
11"
ABSTRACT,0.0196078431372549,"It is plain that a brute-force approach would be unlikely to provide insights on
12"
ABSTRACT,0.021241830065359478,"how to tackle this question. This paper answers the above question by proving
13"
ABSTRACT,0.02287581699346405,"that, for any number of features, there exist Boolean functions that exhibit one or
14"
ABSTRACT,0.024509803921568627,"more inadequacy-revealing issues, thereby contributing decisive arguments against
15"
ABSTRACT,0.026143790849673203,"the use of Shapley values as the theoretical underpinning of feature-attribution
16"
ABSTRACT,0.027777777777777776,"methods in explainability.
17"
INTRODUCTION,0.029411764705882353,"1
Introduction
18"
INTRODUCTION,0.03104575163398693,"Feature attribution is one of the most widely used approaches in machine learning (ML) explainability,
19"
INTRODUCTION,0.032679738562091505,"begin implemented with a variety of different methods [64, 56, 57]. Moreover, the use of Shapley
20"
INTRODUCTION,0.03431372549019608,"values [60] for feature attribution ranks among the most popular solutions [64, 65, 48, 17, 47],
21"
INTRODUCTION,0.03594771241830065,"offering a widely accepted theoretical justification on how to assign importance to features in machine
22"
INTRODUCTION,0.03758169934640523,"learning (ML) model predictions. Despite the success of using Shapley values for explainability,
23"
INTRODUCTION,0.0392156862745098,"it is also the case that their exact computation is in general intractable [8, 21, 22], with tractability
24"
INTRODUCTION,0.04084967320261438,"results for some families of boolean circuits [8]. As a result, a detailed assessment of the rigor of
25"
INTRODUCTION,0.042483660130718956,"feature attribution methods based on Shapley values, when compared with exactly computed Shapley
26"
INTRODUCTION,0.04411764705882353,"values has not been investigated. Furthermore, the definition Shapley values (as well as its use in
27"
INTRODUCTION,0.0457516339869281,"explainability) is purely axiomatic, i.e. there exists no formal proof that Shapley values capture any
28"
INTRODUCTION,0.04738562091503268,"specific properties related with explainability (even if defining such properties might prove elusive).
29"
INTRODUCTION,0.049019607843137254,"Feature selection represents a different alternative to feature attribution. The goal of feature selection
30"
INTRODUCTION,0.05065359477124183,"is to select a set of features as representing the reason for a prediction, i.e. if the selected features take
31"
INTRODUCTION,0.05228758169934641,"their assigned values, then the prediction cannot be changed. There are rigorous and non-rigorous
32"
INTRODUCTION,0.05392156862745098,"approaches for selecting the features that explain a prediction. This paper considers rigorous (or
33"
INTRODUCTION,0.05555555555555555,"model-precise) approaches for selecting such features. Furthermore, it should be plain that feature
34"
INTRODUCTION,0.05718954248366013,"selection must aim for irredundancy, since otherwise it would suffice to report all features as the
35"
INTRODUCTION,0.058823529411764705,"explanation. Given the universe of possible irreducible sets of feature selections that explain a
36"
INTRODUCTION,0.06045751633986928,"prediction, the features that do not occur in any such set are deemed irrelevant for a prediction;
37"
INTRODUCTION,0.06209150326797386,"otherwise features that occur in one or more feature selections are deemed relevant.
38"
INTRODUCTION,0.06372549019607843,"Since both feature attribution and feature selection measure contributions of features to explanations,
39"
INTRODUCTION,0.06535947712418301,"one would expect that the two approaches were related. However, this is not the case. Recent
40"
INTRODUCTION,0.06699346405228758,"work [35] observed that feature attribution based on Shapley values could produce misleading
41"
INTRODUCTION,0.06862745098039216,"information about features, in that irrelevant features (for feature selection) could be deemed more
42"
INTRODUCTION,0.07026143790849673,"important (in terms of feature attribution) than relevant features (also for feature selection). Clearly,
43"
INTRODUCTION,0.0718954248366013,"misleading information about the relative importance of features can easily induce human decision
44"
INTRODUCTION,0.07352941176470588,"makers in error, by suggesting the wrong features as those to analyze in greater detail. Furthermore,
45"
INTRODUCTION,0.07516339869281045,"situations where human decision makers can be misled are inadmissible in high-risk or safety-critical
46"
INTRODUCTION,0.07679738562091504,"uses of ML. Furthermore, a number of possible misleading issues of Shapley values for explainability
47"
INTRODUCTION,0.0784313725490196,"were identified [35], and empirically demonstrated to occur for some boolean functions. The existence
48"
INTRODUCTION,0.08006535947712418,"in practice of those misleading issues with Shapley values for explainability is evidently problematic
49"
INTRODUCTION,0.08169934640522876,"for their use as the theoretical underpinning of feature attribution methods.
50"
INTRODUCTION,0.08333333333333333,"However, earlier work [35] used a brute-force method to identify boolean functions, defined on a
51"
INTRODUCTION,0.08496732026143791,"very small number of variables, where the misleading issues could be observed. A limitation of
52"
INTRODUCTION,0.08660130718954248,"this earlier work [35] is that it offered no insights on how general the issues with Shapley values
53"
INTRODUCTION,0.08823529411764706,"for explainability are. For example, it could be the case that the identified misleading issues might
54"
INTRODUCTION,0.08986928104575163,"only occur for functions defined on a very small number of variables, or in a negligible number of
55"
INTRODUCTION,0.0915032679738562,"functions, among the universe of functions defined on a given number of variables. If that were to be
56"
INTRODUCTION,0.09313725490196079,"the case, then the issues with Shapley values for explainability might not be that problematic.
57"
INTRODUCTION,0.09477124183006536,"This paper proves that the identified misleading issues with Shapley values for explainability are
58"
INTRODUCTION,0.09640522875816994,"much more general that what was reported in earlier work [35]. Concretely, the paper proves that,
59"
INTRODUCTION,0.09803921568627451,"for any number of features larger than a small k (either 2 or 3), one can easily construct functions
60"
INTRODUCTION,0.09967320261437909,"which exhibit the identified misleading issues. The main implication of our results is clear: the use
61"
INTRODUCTION,0.10130718954248366,"of Shapley values for explainability can, for an arbitrary large number of boolean (classification)
62"
INTRODUCTION,0.10294117647058823,"functions, produce misleading information about the relative importance of features.
63"
INTRODUCTION,0.10457516339869281,"Organization. The paper is organized as follows. Section 2 introduces the notation and definitions
64"
INTRODUCTION,0.10620915032679738,"used throughout the paper. Section 3 revisits and extends the issues with Shapley values for ex-
65"
INTRODUCTION,0.10784313725490197,"plainability reported in earlier work [35], and illustrates the existence of those issues in a number
66"
INTRODUCTION,0.10947712418300654,"of motivating example boolean functions. Section 4 presents the paper’s main results, proving that
67"
INTRODUCTION,0.1111111111111111,"all the issues with Shapley values for explainability reported in earlier work [35] occur for boolean
68"
INTRODUCTION,0.11274509803921569,"functions with arbitrarily larger number of variables. (Due to lack of space, the detailed proofs are
69"
INTRODUCTION,0.11437908496732026,"all included in Appendix A, and the paper includes only brief insights into those proofs.) Also, the
70"
INTRODUCTION,0.11601307189542484,"proposed constructions offer ample confidence that the number of functions displaying one or more
71"
INTRODUCTION,0.11764705882352941,"of the issues is significant. Section 5 concludes the paper.
72"
PRELIMINARIES,0.119281045751634,"2
Preliminaries
73"
PRELIMINARIES,0.12091503267973856,"Boolean functions. Let B = {0, 1}. The results in the paper consider boolean functions, defined on
74"
PRELIMINARIES,0.12254901960784313,"m boolean variables, i.e. κ : Bm →B. (The fact that we consider only boolean functions does not
75"
PRELIMINARIES,0.12418300653594772,"restrict in the significance of the results.)
76"
PRELIMINARIES,0.12581699346405228,"In the rest of the paper, we will use the boolean functions shown in Figure 1, which are represented
77"
PRELIMINARIES,0.12745098039215685,"by truth tables. The highlighted rows will serve as concrete examples throughout.
78"
PRELIMINARIES,0.12908496732026145,"Classification in ML. A classification problem is defined on a set of features F = {1, . . . , m}, each
79"
PRELIMINARIES,0.13071895424836602,"with domain Di, and a set of classes K = {c1, c2, . . . , cK}. (As noted above, we will assume Di = B
80"
PRELIMINARIES,0.1323529411764706,"for 1 ≤i ≤m, but domains could be categorical or ordinal. Also, we will assume K = B.) Feature
81"
PRELIMINARIES,0.13398692810457516,"space F is defined as the cartesian product of the domains of the features, in order: F = D1×· · ·×Dm,
82"
PRELIMINARIES,0.13562091503267973,"which will be Bm throughout the paper. A classification function is a non-constant map from feature
83"
PRELIMINARIES,0.13725490196078433,"space into the set of classes, κ : F →K. (Clearly, a classifier would be useless if the classification
84"
PRELIMINARIES,0.1388888888888889,"function were constant.) Throughout the paper, we will not distinguish between classifiers and
85"
PRELIMINARIES,0.14052287581699346,"boolean functions. An instance is a pair (v, c) representing a point v = (v1, . . . , vm) in feature space,
86"
PRELIMINARIES,0.14215686274509803,"and the classifier’s prediction, i.e. κ(v) = c. Moreover, we let x = (x1, . . . , xm) denote an arbitrary
87"
PRELIMINARIES,0.1437908496732026,"point in the feature space. Abusing notation, we will also use xa..b to denote xa, . . . , xb, and va..b to
88"
PRELIMINARIES,0.1454248366013072,"denote va, . . . , vb. Finally, a classifier M is a tuple (F, F, K, κ). In addition, an explanation problem
89"
PRELIMINARIES,0.14705882352941177,"E is a tuple (M, (v, c)), where M = (F, F, K, κ) is a classifier.
90"
PRELIMINARIES,0.14869281045751634,"Shapley values for explainability. Shapley values were first introduced by L. Shapley [60] in the
91"
PRELIMINARIES,0.1503267973856209,"context of game theory. Shapley values have been extensively used for explaining the predictions
92"
PRELIMINARIES,0.15196078431372548,"of ML models, e.g. [64, 65, 20, 48, 15, 52, 62, 69], among a vast number of recent examples. The
93"
PRELIMINARIES,0.15359477124183007,"complexity of computing Shapley values (as proposed in SHAP [48]) has been studied in recent
94"
PRELIMINARIES,0.15522875816993464,"x1
x2
x3
κI1(x)
0
0
0
0
0
0
1
0
0
1
0
0
0
1
1
0
1
0
0
0
1
0
1
1
1
1
0
1
1
1
1
1"
PRELIMINARIES,0.1568627450980392,(a) Function κI1
PRELIMINARIES,0.15849673202614378,"x1
x2
x3
κI3(x)
0
0
0
0
0
0
1
0
0
1
0
0
0
1
1
1
1
0
0
1
1
0
1
0
1
1
0
1
1
1
1
1"
PRELIMINARIES,0.16013071895424835,(b) Function κI3
PRELIMINARIES,0.16176470588235295,"x1
x2
x3
x4
κI4(x)
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
1
1
0
0
1
0
0
0
0
1
0
1
0
0
1
1
0
0
0
1
1
1
1
1
0
0
0
0
1
0
0
1
0
1
0
1
0
1
1
0
1
1
0
1
1
0
0
1
1
1
0
1
1
1
1
1
0
1
1
1
1
1
1"
PRELIMINARIES,0.16339869281045752,(c) Function κI4
PRELIMINARIES,0.1650326797385621,"x1
x2
x3
x4
κI5(x)
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
1
1
0
0
1
0
0
0
0
1
0
1
0
0
1
1
0
0
0
1
1
1
1
1
0
0
0
0
1
0
0
1
0
1
0
1
0
0
1
0
1
1
1
1
1
0
0
0
1
1
0
1
1
1
1
1
0
0
1
1
1
1
0"
PRELIMINARIES,0.16666666666666666,(d) Function κI5
PRELIMINARIES,0.16830065359477125,"Figure 1: Example functions for issues I1, I3, I4, I5, respectively κI1, κI3, κI4, κI5"
PRELIMINARIES,0.16993464052287582,"years [8, 21, 7, 22]. This section provides a brief overview of Shapley values. Throughout the section,
95"
PRELIMINARIES,0.1715686274509804,"we adapt the notation used in recent work [8, 7], which builds on the work of [48].
96"
PRELIMINARIES,0.17320261437908496,"Let Υ : 2F →2F be defined by1,
97"
PRELIMINARIES,0.17483660130718953,"Υ(S; v) = {x ∈F | ∧i∈S xi = vi}
(1)"
PRELIMINARIES,0.17647058823529413,"i.e. for a given set S of features, and parameterized by the point v in feature space, Υ(S; v) denotes
98"
PRELIMINARIES,0.1781045751633987,"all the points in feature space that have in common with v the values of the features specified by S.
99"
PRELIMINARIES,0.17973856209150327,"Also, let ϕ : 2F →R be defined by,
100"
PRELIMINARIES,0.18137254901960784,"ϕ(S; M, v) =
1
2|F\S|
X"
PRELIMINARIES,0.1830065359477124,"x∈Υ(S;v)
κ(x)
(2)"
PRELIMINARIES,0.184640522875817,"For the purposes of this paper, we consider solely a uniform input distribution, and so the dependency
101"
PRELIMINARIES,0.18627450980392157,"on the input distribution is not accounted for. A more general formulation is considered in related
102"
PRELIMINARIES,0.18790849673202614,"work [8, 7]. However, assuming a uniform distribution suffices for the purposes of this paper. As a
103"
PRELIMINARIES,0.1895424836601307,"result, given a set S of features, ϕ(S; M, v) represents the average value of the classifier over the
104"
PRELIMINARIES,0.19117647058823528,"points of feature space represented by Υ(S; v).
105"
PRELIMINARIES,0.19281045751633988,"Finally, let Sv : F →R be defined by2,
106"
PRELIMINARIES,0.19444444444444445,"Sv(i; M, v) =
X"
PRELIMINARIES,0.19607843137254902,S⊆(F\{i})
PRELIMINARIES,0.1977124183006536,|S|!(|F| −|S| −1)!
PRELIMINARIES,0.19934640522875818,"|F|!
(ϕ(S ∪{i}; M, v) −ϕ(S; M, v))
(3)"
PRELIMINARIES,0.20098039215686275,"Given an instance (v, c), the Shapley value assigned to each feature measures the contribution of
107"
PRELIMINARIES,0.20261437908496732,"that feature with respect to the prediction. A positive/negative value indicates that the feature can
108"
PRELIMINARIES,0.2042483660130719,"contribute to changing the prediction, whereas a value of 0 indicates no contribution.
109"
PRELIMINARIES,0.20588235294117646,"Example 1. We consider the example boolean functions of Figure 1. If the functions are represented
110"
PRELIMINARIES,0.20751633986928106,"by a truth table, then the Shapley values can be computed in polynomial time on the size of the
111"
PRELIMINARIES,0.20915032679738563,"1When defining concepts, we will show the necessary parameterizations. However, in later uses, those
parameterizations will be omitted, for simplicity.
2We distinguish SHAP(·; ·, ·) from Sv(·; ·, ·). Whereas SHAP(·; ·, ·) represents the value computed by the
tool SHAP [48], Sv(·; ·, ·) represents the Shapley value in the context of (feature attribution based) explainability,
as studied in a number of works [64, 65, 48, 8, 21, 22]. Thus, SHAP(·; ·, ·) is a heuristic approximation of
Sv(·; ·, ·)."
PRELIMINARIES,0.2107843137254902,"truth table, since the number of subsets considered in (3) is also polynomial on the size of the truth
112"
PRELIMINARIES,0.21241830065359477,"table [35]. (Observe that for each subset used in (3), we can use the truth table for computing the
113"
PRELIMINARIES,0.21405228758169934,"average values in (2).) For example, for κI1 and for the point in feature space (0, 0, 1), one can
114"
PRELIMINARIES,0.21568627450980393,"compute the following Shapley values: Sv(1) = −0.417, Sv(2) = −0.042, and Sv(3) = 0.083.
115"
PRELIMINARIES,0.2173202614379085,"Logic-based explanations. There has been recent work on developing formal definitions of expla-
116"
PRELIMINARIES,0.21895424836601307,"nations. One type of explanations are abductive explanations [37] (AXp), which corresponds to
117"
PRELIMINARIES,0.22058823529411764,"a PI-explanations [61] in the case of boolean classifiers. AXp’s represent prime implicants of the
118"
PRELIMINARIES,0.2222222222222222,"discrete-valued classifier function (which computes the predicted class). AXp’s can also be viewed as
119"
PRELIMINARIES,0.2238562091503268,"an instantiation of logic-based abduction [24, 59, 13, 23]. Throughout this paper we will opt to use
120"
PRELIMINARIES,0.22549019607843138,"the acronym AXp to refer to abductive explanations.
121"
PRELIMINARIES,0.22712418300653595,"Let us consider a given classifier, computing a classification function κ on feature space F, a point
122"
PRELIMINARIES,0.22875816993464052,"v ∈F, with prediction c = κ(v), and let X denote a subset of the set of features F, X ⊆F. X is a
123"
PRELIMINARIES,0.23039215686274508,"weak AXp for the instance (v, c) if,
124"
PRELIMINARIES,0.23202614379084968,"WAXp(X; M, v)
:=
∀(x ∈F).
V"
PRELIMINARIES,0.23366013071895425,"i∈X (xi = vi)

→(κ(x) = c)
(4)"
PRELIMINARIES,0.23529411764705882,"where c = κ(v). Thus, given an instance (v, c), a (weak) AXp is a subset of features which, if fixed
125"
PRELIMINARIES,0.2369281045751634,"to the values dictated by v, then the prediction is guaranteed to be c, independently of the values
126"
PRELIMINARIES,0.238562091503268,"assigned to the other features.
127"
PRELIMINARIES,0.24019607843137256,"Moreover, X ⊆F is an AXp if, besides being a weak AXp, it is also subset-minimal, i.e.
128"
PRELIMINARIES,0.24183006535947713,"AXp(X; M, v)
:=
WAXp(X; M, v) ∧∀(X ′ ⊊X).¬WAXp(X ′; M, v)
(5)"
PRELIMINARIES,0.2434640522875817,"Observe that an AXp can be viewed as a possible irreducible answer to a “Why?” question, i.e.
129"
PRELIMINARIES,0.24509803921568626,"why is the classifier’s prediction c? It should be plain in this work, but also in earlier work, that the
130"
PRELIMINARIES,0.24673202614379086,"representation of AXp’s using subsets of features aims at simplicity. The sufficient condition for the
131"
PRELIMINARIES,0.24836601307189543,"prediction is evidently the conjunction of literals associated with the features contained in the AXp.
132"
PRELIMINARIES,0.25,"Example 2. Similar to the computation of Shapley values, given a truth table representation of a
133"
PRELIMINARIES,0.25163398692810457,"function, and for a given instance, there is a polynomial-time algorithm for computing the AXp’s [35].
134"
PRELIMINARIES,0.25326797385620914,"For example, for function κI4 (see Figure 1c), and for the instance ((0, 0, 1, 1), 0), it can be observed
135"
PRELIMINARIES,0.2549019607843137,"that, if features 3 and 4 are allowed to take other values, the prediction remains at 0. Hence, {1, 2}
136"
PRELIMINARIES,0.2565359477124183,"is an WAXp, which is easy to conclude that it is also an AXp. When interpreted as a rule, the AXp
137"
PRELIMINARIES,0.2581699346405229,"would yield the rule:
138"
PRELIMINARIES,0.25980392156862747,"IF
¬x1 ∧¬x2
THEN
κ(x) = 0
In a similar way, if features 1 and 3 are allowed to take other values, the prediction remains at 0.
139"
PRELIMINARIES,0.26143790849673204,"Hence, {2, 4} is another WAXp (which can easily be shown to be an AXp). Furthermore, considering
140"
PRELIMINARIES,0.2630718954248366,"all other possible subsets of fixed features, allows us to conclude that there are no more AXp’s.
141"
PRELIMINARIES,0.2647058823529412,"Similarly to the case of AXp’s, one can define (weak) contrastive explanations (CXp’s) [53, 36].
142"
PRELIMINARIES,0.26633986928104575,"Y ⊆F is a weak CXp for the instance (v, c) if,
143"
PRELIMINARIES,0.2679738562091503,"WCXp(Y; M, v)
:=
∃(x ∈F).
hV"
PRELIMINARIES,0.2696078431372549,"i̸∈Y(xi = vi)
i
∧(κ(x) ̸= c)
(6)"
PRELIMINARIES,0.27124183006535946,"(As before, for simplicity we keep the parameterization of WCXp on κ, v and c implicit.) Thus, given
144"
PRELIMINARIES,0.272875816993464,"an instance (v, c), a (weak) CXp is a subset of features which, if allowed to take any value from their
145"
PRELIMINARIES,0.27450980392156865,"domain, then there is an assignment to the features that changes the prediction to a class other than c,
146"
PRELIMINARIES,0.2761437908496732,"this while the features not in the explanation are kept to their values.
147"
PRELIMINARIES,0.2777777777777778,"Furthermore, a set Y ⊆F is a CXp if, besides being a weak CXp, it is also subset-minimal, i.e.
148"
PRELIMINARIES,0.27941176470588236,"CXp(Y; M, v)
:=
WCXp(Y; M, v) ∧∀(Y′ ⊊Y).¬WCXp(Y′; M, v)
(7)"
PRELIMINARIES,0.28104575163398693,"A CXp can be viewed as a possible irreducible answer to a “Why Not?” question, i.e. why isn’t the
149"
PRELIMINARIES,0.2826797385620915,"classifier’s prediction a class other than c?
150"
PRELIMINARIES,0.28431372549019607,"Example 3. For the example function κI4 (see Figure 1c), and instance ((0, 0, 1, 1), 0), if we fix
151"
PRELIMINARIES,0.28594771241830064,"features 1, 3 and 4, respectively to 0, 1 1, then by allowing feature 2 to change value, we see that the
152"
PRELIMINARIES,0.2875816993464052,"prediction changes, e.g. by considering the point (0, 1, 1, 1) with prediction 1. Thus, {2} is a CXp.
153"
PRELIMINARIES,0.28921568627450983,"In a similar way, by fixing the features 2 and 3, respectively to 0 and 1, then by allowing features 1
154"
PRELIMINARIES,0.2908496732026144,"and 4 to change value, we conclude that the prediction changes. Hence, {1, 4} is also a CXp.
155"
PRELIMINARIES,0.29248366013071897,"The sets of AXp’s and CXp’s are defined as follows:
156"
PRELIMINARIES,0.29411764705882354,"A(E) = {X ⊆F | AXp(X; M, v)}
C(E) = {Y ⊆F | CXp(Y; M, v)}
(8)"
PRELIMINARIES,0.2957516339869281,"(The parameterization on M and v is unnecessary, since the explanation problem E already accounts
157"
PRELIMINARIES,0.2973856209150327,"for those.) Moreover, let FA(E) = ∪X∈A(E)X and FC(E) = ∪Y∈C(E)Y. FA(E) aggregates the
158"
PRELIMINARIES,0.29901960784313725,"features occurring in any abductive explanation, whereas FC(E) aggregates the features occurring in
159"
PRELIMINARIES,0.3006535947712418,"any contrastive explanation. In addition, minimal hitting set duality between AXp’s and CXp’s [36]
160"
PRELIMINARIES,0.3022875816993464,"yields the following result3.
161"
PRELIMINARIES,0.30392156862745096,"Proposition 1. FA(E) = FC(E).
162"
PRELIMINARIES,0.3055555555555556,"Feature (ir)relevancy in explainability. Given the definitions above, we have the following charac-
163"
PRELIMINARIES,0.30718954248366015,"terization of features [33, 34, 32]:
164"
PRELIMINARIES,0.3088235294117647,"1. A feature i ∈F is necessary if ∀(X ∈A(E)).i ∈X.
165"
PRELIMINARIES,0.3104575163398693,"2. A feature i ∈F is relevant if ∃(X ∈A(E)).i ∈X.
166"
PRELIMINARIES,0.31209150326797386,"3. A feature is irrelevant if it is not relevant, i.e. ∀(X ∈A(E)).i ̸∈X.
167"
PRELIMINARIES,0.3137254901960784,"By Proposition 1, the definitions of necessary and relevant feature could instead use C(E). Throughout
168"
PRELIMINARIES,0.315359477124183,"the paper, we will use the predicate Irrelevant(i) which holds true if feature i is irrelevant, and
169"
PRELIMINARIES,0.31699346405228757,"predicate Relevant(i) which holds true if feature i is relevant. Furthermore, it should be noted that
170"
PRELIMINARIES,0.31862745098039214,"feature irrelevancy is a fairly demanding condition in that, a feature i is irrelevant if it is not included
171"
PRELIMINARIES,0.3202614379084967,"in any subset-minimal set of features that is sufficient for the prediction.
172"
PRELIMINARIES,0.32189542483660133,"Example 4. For the example function κI4 (see Figure 1c), and from Example 2, and instance
173"
PRELIMINARIES,0.3235294117647059,"((0, 0, 1, 1), 0), it becomes clear that feature 3 is irrelevant. Similarly, it is easy to conclude that
174"
PRELIMINARIES,0.32516339869281047,"features 1, 2 and 4 are relevant.
175"
PRELIMINARIES,0.32679738562091504,"How irrelevant are irrelevant features? The fact that a feature is declared irrelevant for an explana-
176"
PRELIMINARIES,0.3284313725490196,"tion problem E = (M, (v, c)) is significant. Given the minimal hitting set duality between abductive
177"
PRELIMINARIES,0.3300653594771242,"and contrastive explanations, then an irrelevant features does not occur neither in any abductive
178"
PRELIMINARIES,0.33169934640522875,"explanation, nor in any contrastive explanation. Furthermore, from the definition of AXp, each
179"
PRELIMINARIES,0.3333333333333333,"abductive explanation for E can be represented as a logic rule. Let R denote the set of all irreducible
180"
PRELIMINARIES,0.3349673202614379,"logic rules which can be used to predict c, given the literals dictated by v. Then, an irrelevant feature
181"
PRELIMINARIES,0.3366013071895425,"does not occur in any of those rules. Example 4 illustrates the irrelevancy of feature 3, in that feature
182"
PRELIMINARIES,0.3382352941176471,"3 would not occur in any irreducible rule for κI4 when predicting 0 using literals consistent with
183"
PRELIMINARIES,0.33986928104575165,"(0, 0, 1, 1).
184"
PRELIMINARIES,0.3415032679738562,"To further strengthen the above discussion, let us consider a (feature selection based) explanation
185"
PRELIMINARIES,0.3431372549019608,"X ⊆F such that WAXp(X) holds (i.e. (4) is true, and so X is sufficient for the prediction). Moreover,
186"
PRELIMINARIES,0.34477124183006536,"let i ∈F be an irrelevant feature, such that i ∈X. Then, by definition of irrelevant feature, there must
187"
PRELIMINARIES,0.3464052287581699,"exist some Z ⊆(X \ {i}), such that WAXp(Z) also holds (i.e. Z is also sufficient for the prediction).
188"
PRELIMINARIES,0.3480392156862745,"It is simple to understand why such set Z must exist. By definition of irrelevant feature, and because
189"
PRELIMINARIES,0.34967320261437906,"i ∈X, then X is not an AXp. However, there must exist an AXp W ⊊X which, by definition of
190"
PRELIMINARIES,0.35130718954248363,"irrelevant feature, must not include i. Furthermore, and invoking Occam’s razor4, there is no reason
191"
PRELIMINARIES,0.35294117647058826,"to select X over Z, and this remark applies to any set of features containing some irrelevant feature.
192"
PRELIMINARIES,0.3545751633986928,"Related work. Shapley values for explainability is one of the hallmarks of feature attribution methods
193"
PRELIMINARIES,0.3562091503267974,"in XAI [64, 65, 20, 48, 15, 47, 52, 17, 26, 16, 25, 62, 40, 58, 69, 5, 12, 30, 4, 67]. Motivated by
194"
PRELIMINARIES,0.35784313725490197,"the success of Shapley values for explainability, there exists a burgeoning body of work on using
195"
PRELIMINARIES,0.35947712418300654,"Shapley values for explainability (e.g. [39, 74, 71, 38, 54, 10, 6, 76, 44, 3, 63, 75, 49, 68, 45, 46,
196"
PRELIMINARIES,0.3611111111111111,"77, 28, 29, 31, 1]). Recent work studied the complexity of exactly computing Shapley values in the
197"
PRELIMINARIES,0.3627450980392157,"context of explainability [8, 21, 22]. Finally, there have been proposals for the exact computation of
198"
PRELIMINARIES,0.36437908496732024,"Shapley values in the case of circuit-based classifiers [8]. Although there exist some differences in
199"
PRELIMINARIES,0.3660130718954248,"the proposals for the use of Shapley values for explainability, the basic formulation is the same and
200"
PRELIMINARIES,0.36764705882352944,"can be expressed as in Section 2.
201"
PRELIMINARIES,0.369281045751634,"A number of authors have reported pitfalls with the use of SHAP and Shapley values as a measure of
202"
PRELIMINARIES,0.3709150326797386,"feature importance [73, 42, 66, 52, 27, 72, 55, 2, 70, 41, 14]. However, these earlier works do not
203"
PRELIMINARIES,0.37254901960784315,"identify fundamental flaws with the use of Shapley values in explainability. Attempts at addressing
204"
PRELIMINARIES,0.3741830065359477,"those pitfalls include proposals to integrate Shapley values with abductive explanations, as reported
205"
PRELIMINARIES,0.3758169934640523,"in recent work [43].
206"
PRELIMINARIES,0.37745098039215685,"Formal explainability is a fairly recent topic of research. Recent accounts include [51, 9, 50, 19].
207"
PRELIMINARIES,0.3790849673202614,"3All proofs are included in Appendix A.
4Here, we adopt a fairly standard definition of Occam’s razor [11]: given two explanations of the data, all
other things being equal, the simpler explanation is preferable."
PRELIMINARIES,0.380718954248366,"Recent work [35] argued for the inadequacy of Shapley values for explainability, by demonstrating
208"
PRELIMINARIES,0.38235294117647056,"experimentally that the information provided by Shapley values can be misleading for a human
209"
PRELIMINARIES,0.3839869281045752,"decision-maker. The approach proposed in [35] is based on exhaustive function enumeration, and so
210"
PRELIMINARIES,0.38562091503267976,"does not scale beyond a few features. However, this paper uses the truth-table algorithms outlined
211"
PRELIMINARIES,0.3872549019607843,"in [35], in all the examples, both for computing Shapley values, for computing explanations, and for
212"
PRELIMINARIES,0.3888888888888889,"deciding feature relevancy.
213"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.39052287581699346,"3
Relating Shapley Values with Feature Relevancy
214"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.39215686274509803,"Recent work [35] showed the existence of boolean functions (with up to four variables) that revealed a
215"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.3937908496732026,"number of issues with Shapley values for explainability. All those issues are related with taking feature
216"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.3954248366013072,"relevancy into consideration. (In [35], these functions were searched by exhaustive enumeration of
217"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.39705882352941174,"all the boolean functions up to a threshold on the number of variables.)
218"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.39869281045751637,"Issues with Shapley values for explainability. In this paper, we consider the following main issues
219"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.40032679738562094,"of Shapley values for explainability:
220"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4019607843137255,"I1. For a boolean classifier, with an instance (v, c), and feature i such that,
221"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4035947712418301,Irrelevant(i) ∧(Sv(i) ̸= 0)
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.40522875816993464,"Thus, an I1 issue is such that the feature is irrelevant, but its Shapley value is non-zero.
222"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4068627450980392,"I2. For a boolean classifier, with an instance (v, c) and features i1 and i2 such that,
223"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4084967320261438,Irrelevant(i1) ∧Relevant(i2) ∧(|Sv(i1)| > |Sv(i2)|)
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.41013071895424835,"Thus, an I2 issue is such that there is at least one irrelevant feature exhibiting a Shapley value
224"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4117647058823529,"larger (in absolute value) than the Shapley of a relevant feature.
225"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4133986928104575,"I3. For a boolean classifier, with instance (v, c), and feature i such that,
226"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4150326797385621,Relevant(i) ∧(Sv(i) = 0)
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4166666666666667,"Thus, an I3 issue is such that the feature is relevant, but its Shapley value is zero.
227"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.41830065359477125,"I4. For a boolean classifier, with instance (v, c), and features i1 and i2 such that,
228"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4199346405228758,[Irrelevant(i1) ∧(Sv(i1) ̸= 0)] ∧[Relevant(i2) ∧(Sv(i2) = 0)]
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4215686274509804,"Thus, an I4 issue is such that there is at least one irrelevant feature with a non-zero Shapley
229"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.42320261437908496,"value and a relevant feature with a Shapley value of 0.
230"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.42483660130718953,"I5. For a boolean classifier, with instance (v, c) and feature i such that,
231"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4264705882352941,"[Irrelevant(i) ∧∀1≤j≤m,j̸=i (|Sv(j)| < |Sv(i)|)]"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.42810457516339867,"Thus, an I5 issue is such that there is one irrelevant feature exhibiting the highest Shapley value
232"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4297385620915033,"(in absolute value). (I5 can be viewed as a special case of the other issues, and so it is not
233"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.43137254901960786,"analyzed separately in earlier work [35].)
234"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.43300653594771243,"The issues above are all related with Shapley values for explainability giving misleading information
235"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.434640522875817,"to a human decision maker, by assigning some importance to irrelevant features, by not assigning
236"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4362745098039216,"enough importance to relevant features, by assigning more importance to irrelevant features than to
237"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.43790849673202614,"relevant features and, finally, by assigning the most importance to irrelevant features.
238"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4395424836601307,"In the rest of the paper we consider mostly I1, I3, I4 and I5, given that I5 implies I2.
239"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4411764705882353,"Proposition 2. If a classifier and instance exhibits issue I5, then they also exhibit issue I2.
240"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.44281045751633985,"Examples. This section studies the example functions of Figure 1, which were derived from the
241"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4444444444444444,"main results of this paper (see Section 4). These example functions will then be used to motivate the
242"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.44607843137254904,"rationale for how those results are proved. In all cases, the reported Shapley values are computed
243"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4477124183006536,"using the truth-table algorithm outlined in earlier work [35]. Similarly, the relevancy/irrelevancy
244"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4493464052287582,"claims of features use the truth-table algorithms outlined in earlier work [35].
245"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.45098039215686275,"Example 5. Figure 1a illustrates a boolean function that exhibits issue I1. By inspection, we can
246"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4526143790849673,"conclude that the function shown corresponds to κI1(x1, x2, x3) = (x1 ∧x2 ∧¬x3) ∨(x1 ∧x3).
247"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4542483660130719,"Moreover, for the instance ((0, 0, 1), 0), Table 1 confirms that an issue I1 is identified.
248"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.45588235294117646,"Example 6. Figure 1b illustrates a boolean function that exhibits issue I3. By inspection, we can
249"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.45751633986928103,"conclude that the function shown corresponds to κI3(x1, x2, x3) = (x1∧¬x3)∨(x2∧x3). Moreover,
250"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4591503267973856,"for the instance ((1, 1, 1), 1), Table 1 confirms that an issue I3 is identified.
251"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.46078431372549017,Table 1: Examples of issues of Shapley values for functions in Figure 1
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4624183006535948,"Case
Instance
Relevant Irrelevant
Sv’s
Justification"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.46405228758169936,"I1
((0, 0, 1), 0)
1
2, 3
Sv(1) = −0.417
Sv(2) = −0.042
Sv(3) = 0.083"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.46568627450980393,Irrelevant(3) ∧Sv(3) ̸= 0
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4673202614379085,"I3
((1, 1, 1), 1)
1, 2, 3
–
Sv(1) = 0.125
Sv(2) = 0.375
Sv(3) = 0.000"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.46895424836601307,Relevant(3) ∧Sv(3) = 0
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.47058823529411764,"I4
((0, 0, 1, 1), 0)
1, 2, 4
3
Sv(1) = −0.125
Sv(2) = −0.333
Sv(3) = 0.083
Sv(4) = 0.000"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4722222222222222,"Irrelevant(3) ∧Sv(3) ̸= 0∧
Relevant(4) ∧Sv(4) = 0"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4738562091503268,"I5
((1, 1, 1, 1), 0)
1, 2, 3
4
Sv(1) = −0.12
Sv(2) = −0.12
Sv(3) = −0.12
Sv(4) = 0.17"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.47549019607843135,"Irrelevant(4)∧
∀(j ∈{1, 2, 3}).|Sv(j)| < Sv(4)|"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.477124183006536,"Example 7. Figure 1c illustrates a boolean function that exhibits issue I4. By inspection, we can
252"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.47875816993464054,"conclude that the function shown corresponds to κI4(x1, x2, x3, x4) = (x1 ∧x2 ∧¬x3) ∨(x1 ∧
253"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4803921568627451,"x3 ∧¬x4) ∨(x2 ∧x3 ∧x4). Moreover, for the instance ((0, 0, 1, 1), 0), Table 1 confirms that an
254"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4820261437908497,"issue I4 is identified.
255"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.48366013071895425,"Example 8. Figure 1d illustrates a boolean function that exhibits issue I5. By inspection, we can
256"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4852941176470588,"conclude that the function shown corresponds to κI5(x1, x2, x3, x4) = ((x1 ∧x2 ∧¬x3) ∨(x1 ∧
257"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4869281045751634,"x3 ∧¬x2) ∨(x2 ∧x3 ∧¬x1)) ∧x4. Moreover, for the instance ((1, 1, 1, 1), 0), Table 1 confirms that
258"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.48856209150326796,"an issue I5 is identified.
259"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.49019607843137253,"It should be underscored that Shapley values for explainability are not expected to give misleading
260"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4918300653594771,"information. Indeed, it is widely accepted that Shapley values measure the actual influence of a
261"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4934640522875817,"feature [64, 65, 48, 8, 21]. Concretely, [64] reads: “...if a feature has no influence on the prediction it
262"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.4950980392156863,"is assigned a contribution of 0.” But [64] also reads “According to the 2nd axiom, if two features
263"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.49673202614379086,"values have an identical influence on the prediction they are assigned contributions of equal size. The
264"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.49836601307189543,"3rd axiom says that if a feature has no influence on the prediction it is assigned a contribution of 0.”
265"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.5,"(In this last quote, the axioms refer to the axiomatic characterization of Shapley values.) Furthermore,
266"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.5016339869281046,"one might be tempted to look at the value of the prediction and relate that with the computed Shapley
267"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.5032679738562091,"value. For example, in the last row of Table 1, the prediction is 0, and the irrelevant feature 4 has a
268"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.5049019607843137,"positive Shapley value. As a result, one might be tempted to believe that the irrelevant feature 4 would
269"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.5065359477124183,"contribute to changing the value of the prediction. This is of course incorrect, since an irrelevant
270"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.5081699346405228,"feature does not occur in any CXp’s (besides not occurring in any AXp’s) and so it is never necessary
271"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.5098039215686274,"to changing the prediction. The key point here is that irrelevant features are never necessary, neither
272"
RELATING SHAPLEY VALUES WITH FEATURE RELEVANCY,0.511437908496732,"to keep nor to change the prediction.
273"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5130718954248366,"4
Refuting Shapley Values for Explainability
274"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5147058823529411,"The purpose of this section is to prove that for arbitrary large numbers of variables, there exist boolean
275"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5163398692810458,"functions and instances for which the Shapley values exhibit the issues reported in recent work [35],
276"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5179738562091504,"and detailed in Section 3. (Instead of detailed proofs, this section describes the key ideas of each
277"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5196078431372549,"proof. The detailed proofs are included in Appendix A.)
278"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5212418300653595,"Throughout this section, let m be the number of variables of the boolean functions we start from, and
279"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5228758169934641,"let n denote the number of variables of the functions we will be constructing. In this case, we set
280"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5245098039215687,"F = {1, . . . , n}. Furthermore, for the sake of simplicity, we opt to introduce the new features as the
281"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5261437908496732,"last features (e.g., feature n). This choice does not affect the proof’s argument in any way.
282"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5277777777777778,"Proposition 3. For any n ≥3, there exist boolean functions defined on n variables, and at least one
283"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5294117647058824,"instance, which exhibit an issue I1, i.e. there exists an irrelevant feature i ∈F, such that Sv(i) ̸= 0.
284"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5310457516339869,"Proof idea. The proof proposes to construct boolean functions, with an arbitrary number of variables
285"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5326797385620915,"(no smaller than 3), and the picking of an instance, such that a specific feature is irrelevant for the
286"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5343137254901961,"prediction, but its Shapley value is non-zero. To illustrate the construction, the example function
287"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5359477124183006,"from Figure 1a is used (see also Example 5).
288"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5375816993464052,"The construction works as follows.
We pick two non-constant functions κ1(x1, . . . , xm) and
289"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5392156862745098,"κ2(x1, . . . , xm), defined on m features, and such that: i) κ1 ⊨κ2 (which signifies that ∀(x ∈
290"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5408496732026143,"F).κ1(x) →κ2(x)), and ii) κ1 ̸= κ2. Observe that κ1 can be any boolean function defined on m
291"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5424836601307189,"variables, as long as κ2 can also be defined. We then construct a new function by adding a new
292"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5441176470588235,"feature n = m + 1, as follows:
293"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.545751633986928,"κ(x1, . . . , xm, xn) =
 κ1(x1, . . . , xm)
if xn = 0
κ2(x1, . . . , xm)
if xn = 1"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5473856209150327,"For the resulting function κ, we pick an instance (v, 0) such that: i) vn = 1 and ii) κ1(v1..m) =
294"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5490196078431373,"κ2(v1..m) = 0. The proof hinges on the fact that feature n is irrelevant, but Sv(n) ̸= 0.
295"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5506535947712419,"For the function Figure 1a, we set κ1(x1, x2) = x1 ∧x2 and κ1(x1, x2) = x1. Thus, as shown
296"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5522875816993464,"in Example 5, κI1(x1, x2, x3) = (x1 ∧x2 ∧¬x3) ∨(x1 ∧x3), which represents the function
297"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.553921568627451,"κ(x1, x2, x3). It is also clear that κ1 ⊨κ2. Moreover, and as Example 5 and Table 1 show, it is the
298"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5555555555555556,"case that feature 3 is irrelevant and Sv(3) ̸= 0.
299"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5571895424836601,"Proposition 4. For any odd n ≥3, there exist boolean functions defined on n variables, and at least
300"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5588235294117647,"one instance, which exhibits an I3 issue, i.e. for which there exists a relevant feature i ∈F, such that
301"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5604575163398693,"Sv(i) = 0.
302"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5620915032679739,"Proof idea. The proof proposes to construct boolean functions, with an arbitrary number of variables
303"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5637254901960784,"(no smaller than 3), and the picking of an instance, such that a specific feature is relevant for
304"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.565359477124183,"the prediction, but its Shapley value is zero. To illustrate the construction, the example function
305"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5669934640522876,"from Figure 1b is used (see also Example 6).
306"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5686274509803921,"The construction works as follows.
We pick two non-constant functions κ1(x1, . . . , xm) and
307"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5702614379084967,"κ2(xm+1, . . . , x2m), each defined on m features, where κ2 corresponds to κ1, but with a change of
308"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5718954248366013,"variables. Observe that κ1 can be any boolean function. We then construct a new function, defined in
309"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5735294117647058,"terms of κ1 and κ2, by adding a new feature n = 2m + 1, as follows:
310"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5751633986928104,"κ(x1, . . . , xm, xm+1, . . . , x2m, xn) =
 κ1(x1, . . . , xm)
if xn = 0
κ2(xm+1, . . . , x2m)
if xn = 1"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.576797385620915,"For the resulting function κ, we pick an instance (v, 1) such that: i) vn = 1, ii) vi = vm+i for any
311"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5784313725490197,"1 ≤i ≤m, and iii) κ1(v1..m) = κ2(vm+1..2m) = 1. The proof hinges on the fact that feature n is
312"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5800653594771242,"relevant, but Sv(n) = 0.
313"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5816993464052288,"For the function Figure 1b, we set κ1(x1) = x1 and κ1(x2) = x2. Thus, as shown in Example 6,
314"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5833333333333334,"κI3(x1, x2, x3) = (x1 ∧¬x3) ∨(¬x2 ∧x3), which represents the function κ(x1, x2, x3). Moreover,
315"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5849673202614379,"and as Example 6 and Table 1 show, it is the case that feature 3 is relevant and Sv(3) = 0.
316"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5866013071895425,"Proposition 5. For any even n ≥4, there exist boolean functions defined on n variables, and at least
317"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5882352941176471,"one instance, for which there exists an irrelevant feature i1 ∈F, such that Sv(i1) ̸= 0, and a relevant
318"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5898692810457516,"feature i2 ∈F \ {i1}, such that Sv(i2) = 0.
319"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5915032679738562,"Proof idea. The proof proposes to construct boolean functions, with an arbitrary number of variables
320"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5931372549019608,"(no smaller than 4), and the picking of an instance, such that two specific features are such that one is
321"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5947712418300654,"relevant but has a Shapley value of 0, and the other one is irrelevant but has a non-zero Shapley values.
322"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5964052287581699,"To illustrate the construction, the example function from Figure 1c is used (see also Example 7).
323"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5980392156862745,"The construction works as follows.
We pick two non-constant functions κ1(x1, . . . , xm) and
324"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.5996732026143791,"κ2(xm+1, . . . , x2m), each defined on m features, where κ2 corresponds to κ1, but with a change of
325"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6013071895424836,"variables. Also, observe that κ1 can be any boolean function. We then construct a new function,
326"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6029411764705882,"defined in terms of κ1 and κ2, by adding two new features. We let the new features be n −1 and n,
327"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6045751633986928,"and so n = 2m + 2. The function is organized as follows:
328"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6062091503267973,"κ(x1..m, xm+1..2m, xn−1, xn) = 
 "
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6078431372549019,"κ1(x1..m) ∧κ2(xm+1..2m)
if xn−1 = 0
κ1(x1..m)
if xn−1 = 1 ∧xn = 0
κ2(xm+1..2m)
if xn−1 = 1 ∧xn = 1"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6094771241830066,"For this function, we pick an instance (v, 0) such that: i) vn−1 = vn = 1, ii) vi = vm+i for any
329"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6111111111111112,"1 ≤i ≤m, and iii) κ1(v1..m) = κ2(vm+1..2m) = 0. The proof hinges on the fact that feature n −1
330"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6127450980392157,"is irrelevant, feature n is relevant, and Sv(n −1) ̸= 0 and Sv(n) = 0.
331"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6143790849673203,"For the function Figure 1c, we set κ1(x1) = x1 and κ1(x2) = x2, Thus, as shown in Example 7,
332"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6160130718954249,"κI4(x1, x2, x3, x4) = (x1 ∧x2 ∧¬x3) ∨(x1 ∧x3 ∧¬x4) ∨(x2 ∧x3 ∧x4), which represents the
333"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6176470588235294,"function κ(x1, x2, x3, x4). Moreover, and as Example 7 and Table 1 show, it is the case that feature 3
334"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.619281045751634,"is irrelevant, feature 4 is relevant, and also Sv(3) ̸= 0 and Sv(4) = 0.
335"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6209150326797386,"Proposition 6. For any n ≥4, there exists boolean functions defined on n variables, and at least
336"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6225490196078431,"one instance, for which there exists an irrelevant feature i ∈F = {1, . . . , n}, such that |Sv(i)| =
337"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6241830065359477,"max{|Sv(j)| | j ∈F}.
338"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6258169934640523,"Proof idea. The proof proposes to construct boolean functions, with an arbitrary number of variables
339"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6274509803921569,"(no smaller than 4), and the picking of an instance, such that one specific feature is irrelevant but it
340"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6290849673202614,"has the Shapley value with the largest absolute values. To illustrate the construction, the example
341"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.630718954248366,"function from Figure 1d is used (see also Example 8).
342"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6323529411764706,"The construction works as follows. We pick one non-constant function κ1(x1, . . . , xm), defined on
343"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6339869281045751,"m features, such that: i) κ1 predicts a specific point v1..m as 0, moreover, for any point x1..m such
344"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6356209150326797,"that dH(x1..m, v1..m) = 1, κ1(x1..m) = 1, where dH(·) denotes the Hamming distance. ii) and κ1
345"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6372549019607843,"predicts all the other points as 0. For example, let κ1(x1, . . . , xm) = 1 iff Pm
i=1 ¬x1 = 1. We then
346"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6388888888888888,"construct a new function, defined in terms of κ1, by adding one new feature. We let the new feature
347"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6405228758169934,"be n, and so n = m + 1. The new function is organized as follows:
348"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6421568627450981,"κ(x1, . . . , xm, xn) =
 0
if xn = 0
κ1(x1, . . . , xm)
if xn = 1"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6437908496732027,"For this function, we pick the instance (v, 0) such that: i) vn = 1, ii) v1..m is the only point within
349"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6454248366013072,"the Hamming ball and iii) κ1(v1..m) = 0. The proof hinges on the fact that feature n is irrelevant,
350"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6470588235294118,"but ∀(1 ≤j ≤m).|Sv(j)| < |Sv(n)|.
351"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6486928104575164,"For the function Figure 1d, we set κ1(x1, x2, x3) = (x1∧x2∧¬x3)∨(x1∧x3∧¬x2)∨(x2∧x3∧¬x1)
352"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6503267973856209,"(i.e. the function takes value 1 when exactly one feature is 0). Thus, as shown in Example 7,
353"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6519607843137255,"κI5(x1, x2, x3, x4) = ((x1 ∧x2 ∧¬x3)∨(x1 ∧x3 ∧¬x2)∨(x2 ∧x3 ∧¬x1))∧x4, which represents
354"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6535947712418301,"the function κ(x1, x2, x3, x4). Moreover, and as Example 8 and Table 1 show, it is the case that
355"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6552287581699346,"feature 4 is irrelevant and ∀(1 ≤j ≤3).|Sv(j)| < |Sv(4)|.
356"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6568627450980392,"For I2, we can restate the previous result, but such the functions constructed in the proof capture a
357"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6584967320261438,"more general family of functions.
358"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6601307189542484,"Proposition 7. For any n ≥4, there exist boolean functions defined on n variables, and at least one
359"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6617647058823529,"instance, for which there exists an irrelevant feature i1 ∈F, and a relevant feature i2 ∈F \ {i1},
360"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6633986928104575,"such that |Sv(i1)| > |Sv(i2)|.
361"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6650326797385621,"As noted above, for Propositions 3 to 5, the choice of the starting function is fairly flexible. In
362"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6666666666666666,"contrast, for Proposition 6, we pick one concrete function, which represents a trivial lower bound. As
363"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6683006535947712,"a result, and with the exception of I5, we can prove the following (fairly loose) lower bounds on the
364"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6699346405228758,"number of functions exhibiting the different issues.
365"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6715686274509803,"Proposition 8. For Propositions 3 to 5,and Proposition 7 the following are lower bounds on the
366"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.673202614379085,"numbers issues exhibiting the respective issues:
367"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6748366013071896,"1. For Proposition 3, a lower bound on the number of functions exhibiting I1 is 22(n−1) −n −3.
368"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6764705882352942,"2. For Proposition 4, a lower bound on the number of functions exhibiting I3 is 22(n −1)/2 −2.
369"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6781045751633987,"3. For Proposition 5, a lower bound on the number of functions exhibiting I4 is 22(n −2)/2 −2.
370"
REFUTING SHAPLEY VALUES FOR EXPLAINABILITY,0.6797385620915033,"4. For Proposition 7, a lower bound on the number of functions exhibiting I2 is 22n−2−(n−2)−1 −1.
371"
CONCLUSIONS,0.6813725490196079,"5
Conclusions
372"
CONCLUSIONS,0.6830065359477124,"This paper gives theoretical arguments to the fact that Shapley values for explainability can produce
373"
CONCLUSIONS,0.684640522875817,"misleading information about the relative importance of features. The paper distinguishes between the
374"
CONCLUSIONS,0.6862745098039216,"features that occur in one or more of the irreducible rule-based explanations, i.e. the relevant features,
375"
CONCLUSIONS,0.6879084967320261,"from those that do not occur in any irreducible rule-based explanation, i.e. the irrelevant features.
376"
CONCLUSIONS,0.6895424836601307,"The paper proves that, for boolean functions with arbitrary number of variables, irrelevant features
377"
CONCLUSIONS,0.6911764705882353,"can be deemed more important, given their Shapley value, than relevant features. Our results are also
378"
CONCLUSIONS,0.6928104575163399,"significant in practical deployment of explainability solutions. Indeed, misleading information about
379"
CONCLUSIONS,0.6944444444444444,"relative feature importance can induce human decision makers in error, by persuading them to look at
380"
CONCLUSIONS,0.696078431372549,"the wrong causes of predictions.
381"
CONCLUSIONS,0.6977124183006536,"One direction of research is to develop a better understanding of the distributions of functions
382"
CONCLUSIONS,0.6993464052287581,"exhibiting one or more of the issues of Shapley values.
383"
REFERENCES,0.7009803921568627,"References
384"
REFERENCES,0.7026143790849673,"[1] J. Adeoye, L.-W. Zheng, P. Thomson, S.-W. Choi, and Y.-X. Su. Explainable ensemble
385"
REFERENCES,0.704248366013072,"learning model improves identification of candidates for oral cancer screening. Oral Oncology,
386"
REFERENCES,0.7058823529411765,"136:106278, 2023.
387"
REFERENCES,0.7075163398692811,"[2] D. Afchar, V. Guigue, and R. Hennequin. Towards rigorous interpretations: a formalisation of
388"
REFERENCES,0.7091503267973857,"feature attribution. In ICML, pages 76–86, 2021.
389"
REFERENCES,0.7107843137254902,"[3] R. O. Alabi, A. Almangush, M. Elmusrati, I. Leivo, and A. A. Mäkitie. An interpretable machine
390"
REFERENCES,0.7124183006535948,"learning prognostic system for risk stratification in oropharyngeal cancer. International Journal
391"
REFERENCES,0.7140522875816994,"of Medical Informatics, 168:104896, 2022.
392"
REFERENCES,0.7156862745098039,"[4] M. S. Alam and Y. Xie. Appley: Approximate Shapley value for model explainability in linear
393"
REFERENCES,0.7173202614379085,"time. In Big Data, pages 95–100, 2022.
394"
REFERENCES,0.7189542483660131,"[5] E. Albini, J. Long, D. Dervovic, and D. Magazzeni. Counterfactual Shapley additive explana-
395"
REFERENCES,0.7205882352941176,"tions. In FACCT, pages 1054–1070, 2022.
396"
REFERENCES,0.7222222222222222,"[6] B. Alsinglawi, O. Alshari, M. Alorjani, O. Mubin, F. Alnajjar, M. Novoa, and O. Darwish.
397"
REFERENCES,0.7238562091503268,"An explainable machine learning framework for lung cancer hospital length of stay prediction.
398"
REFERENCES,0.7254901960784313,"Scientific reports, 12(1):1–10, 2022.
399"
REFERENCES,0.7271241830065359,"[7] M. Arenas, P. Barceló, L. E. Bertossi, and M. Monet. On the complexity of SHAP-score-based
400"
REFERENCES,0.7287581699346405,"explanations: Tractability via knowledge compilation and non-approximability results. CoRR,
401"
REFERENCES,0.7303921568627451,"abs/2104.08015, 2021.
402"
REFERENCES,0.7320261437908496,"[8] M. Arenas, P. Barceló, L. E. Bertossi, and M. Monet. The tractability of SHAP-score-based
403"
REFERENCES,0.7336601307189542,"explanations for classification over deterministic and decomposable boolean circuits. In AAAI,
404"
REFERENCES,0.7352941176470589,"pages 6670–6678, 2021.
405"
REFERENCES,0.7369281045751634,"[9] G. Audemard, S. Bellart, L. Bounia, F. Koriche, J. Lagniez, and P. Marquis. On the explanatory
406"
REFERENCES,0.738562091503268,"power of boolean decision trees. Data Knowl. Eng., 142:102088, 2022.
407"
REFERENCES,0.7401960784313726,"[10] M. L. Baptista, K. Goebel, and E. M. Henriques. Relation between prognostics predictor
408"
REFERENCES,0.7418300653594772,"evaluation metrics and local interpretability SHAP values. Artificial Intelligence, 306:103667,
409"
REFERENCES,0.7434640522875817,"2022.
410"
REFERENCES,0.7450980392156863,"[11] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Occam’s razor. Inf. Process.
411"
REFERENCES,0.7467320261437909,"Lett., 24(6):377–380, 1987.
412"
REFERENCES,0.7483660130718954,"[12] F. Bodria, S. Rinzivillo, D. Fadda, R. Guidotti, F. Giannotti, and D. Pedreschi. Explaining Black
413"
REFERENCES,0.75,"Box with Visual Exploration of Latent Space. In EuroVis – Short Papers, 2022.
414"
REFERENCES,0.7516339869281046,"[13] T. Bylander, D. Allemang, M. C. Tanner, and J. R. Josephson. The computational complexity of
415"
REFERENCES,0.7532679738562091,"abduction. Artif. Intell., 49(1-3):25–60, 1991.
416"
REFERENCES,0.7549019607843137,"[14] T. W. Campbell, H. Roder, R. W. Georgantas III, and J. Roder. Exact Shapley values for local
417"
REFERENCES,0.7565359477124183,"and model-true explanations of decision tree ensembles. Machine Learning with Applications,
418"
REFERENCES,0.7581699346405228,"9:100345, 2022.
419"
REFERENCES,0.7598039215686274,"[15] J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan. L-Shapley and C-Shapley: Efficient
420"
REFERENCES,0.761437908496732,"model interpretation for structured data. In ICLR, 2019.
421"
REFERENCES,0.7630718954248366,"[16] I. Covert and S. Lee. Improving KernelSHAP: Practical Shapley value estimation using linear
422"
REFERENCES,0.7647058823529411,"regression. In AISTATS, pages 3457–3465, 2021.
423"
REFERENCES,0.7663398692810458,"[17] I. Covert, S. M. Lundberg, and S. Lee. Understanding global feature contributions with additive
424"
REFERENCES,0.7679738562091504,"importance measures. In NeurIPS, 2020.
425"
REFERENCES,0.7696078431372549,"[18] Y. Crama and P. L. Hammer. Boolean functions: Theory, algorithms, and applications. Cam-
426"
REFERENCES,0.7712418300653595,"bridge University Press, 2011.
427"
REFERENCES,0.7728758169934641,"[19] A. Darwiche and A. Hirth. On the (complete) reasons behind decisions. J. Log. Lang. Inf.,
428"
REFERENCES,0.7745098039215687,"32(1):63–88, 2023.
429"
REFERENCES,0.7761437908496732,"[20] A. Datta, S. Sen, and Y. Zick. Algorithmic transparency via quantitative input influence: Theory
430"
REFERENCES,0.7777777777777778,"and experiments with learning systems. In IEEE S&P, pages 598–617, 2016.
431"
REFERENCES,0.7794117647058824,"[21] G. V. den Broeck, A. Lykov, M. Schleich, and D. Suciu. On the tractability of SHAP explanations.
432"
REFERENCES,0.7810457516339869,"In AAAI, pages 6505–6513, 2021.
433"
REFERENCES,0.7826797385620915,"[22] G. V. den Broeck, A. Lykov, M. Schleich, and D. Suciu. On the tractability of SHAP explanations.
434"
REFERENCES,0.7843137254901961,"J. Artif. Intell. Res., 74:851–886, 2022.
435"
REFERENCES,0.7859477124183006,"[23] T. Eiter and G. Gottlob. The complexity of logic-based abduction. J. ACM, 42(1):3–42, 1995.
436"
REFERENCES,0.7875816993464052,"[24] G. Friedrich, G. Gottlob, and W. Nejdl. Hypothesis classification, abductive diagnosis and
437"
REFERENCES,0.7892156862745098,"therapy. In ESE, pages 69–78, 1990.
438"
REFERENCES,0.7908496732026143,"[25] C. Frye, D. de Mijolla, T. Begley, L. Cowton, M. Stanley, and I. Feige. Shapley explainability
439"
REFERENCES,0.7924836601307189,"on the data manifold. In ICLR, 2021.
440"
REFERENCES,0.7941176470588235,"[26] C. Frye, C. Rowat, and I. Feige. Asymmetric Shapley values: incorporating causal knowledge
441"
REFERENCES,0.795751633986928,"into model-agnostic explainability. In NeurIPS, 2020.
442"
REFERENCES,0.7973856209150327,"[27] D. V. Fryer, I. Strümke, and H. D. Nguyen. Shapley values for feature selection: The good, the
443"
REFERENCES,0.7990196078431373,"bad, and the axioms. IEEE Access, 9:144352–144360, 2021.
444"
REFERENCES,0.8006535947712419,"[28] I. B. Galazzo, F. Cruciani, L. Brusini, A. M. A. Salih, P. Radeva, S. F. Storti, and G. Menegaz.
445"
REFERENCES,0.8022875816993464,"Explainable artificial intelligence for magnetic resonance imaging aging brainprints: Grounds
446"
REFERENCES,0.803921568627451,"and challenges. IEEE Signal Process. Mag., 39(2):99–116, 2022.
447"
REFERENCES,0.8055555555555556,"[29] M. Gandolfi, I. B. Galazzo, R. G. Pavan, F. Cruciani, N. Valè, A. Picelli, S. F. Storti, N. Smania,
448"
REFERENCES,0.8071895424836601,"and G. Menegaz. eXplainable AI allows predicting upper limb rehabilitation outcomes in
449"
REFERENCES,0.8088235294117647,"sub-acute stroke patients. IEEE J. Biomed. Health Informatics, 27(1):263–273, 2023.
450"
REFERENCES,0.8104575163398693,"[30] R. Guidotti, A. Monreale, S. Ruggieri, F. Naretto, F. Turini, D. Pedreschi, and F. Giannotti.
451"
REFERENCES,0.8120915032679739,"Stable and actionable explanations of black-box models through factual and counterfactual rules.
452"
REFERENCES,0.8137254901960784,"Data Mining and Knowledge Discovery, pages 1–38, 2022.
453"
REFERENCES,0.815359477124183,"[31] T. Huang, D. Le, L. Yuan, S. Xu, and X. Peng. Machine learning for prediction of in-hospital
454"
REFERENCES,0.8169934640522876,"mortality in lung cancer patients admitted to intensive care unit. Plos one, 18(1):e0280606,
455"
REFERENCES,0.8186274509803921,"2023.
456"
REFERENCES,0.8202614379084967,"[32] X. Huang, M. C. Cooper, A. Morgado, J. Planes, and J. Marques-Silva. Feature necessity &
457"
REFERENCES,0.8218954248366013,"relevancy in ML classifier explanations. In TACAS, pages 167–186. Springer, 2023.
458"
REFERENCES,0.8235294117647058,"[33] X. Huang, Y. Izza, A. Ignatiev, and J. Marques-Silva. On efficiently explaining graph-based
459"
REFERENCES,0.8251633986928104,"classifiers. In KR, pages 356–367, 2021.
460"
REFERENCES,0.826797385620915,"[34] X. Huang, Y. Izza, and J. Marques-Silva. Solving explainability queries with quantification:
461"
REFERENCES,0.8284313725490197,"The case of feature relevancy. In AAAI, 2 2023. In Press.
462"
REFERENCES,0.8300653594771242,"[35] X. Huang and J. Marques-Silva. The inadequacy of Shapley values for explainability. CoRR,
463"
REFERENCES,0.8316993464052288,"abs/2302.08160, 2023.
464"
REFERENCES,0.8333333333333334,"[36] A. Ignatiev, N. Narodytska, N. Asher, and J. Marques-Silva. From contrastive to abductive
465"
REFERENCES,0.8349673202614379,"explanations and back again. In AIxIA, pages 335–355, 2020.
466"
REFERENCES,0.8366013071895425,"[37] A. Ignatiev, N. Narodytska, and J. Marques-Silva. Abduction-based explanations for machine
467"
REFERENCES,0.8382352941176471,"learning models. In AAAI, pages 1511–1519, 2019.
468"
REFERENCES,0.8398692810457516,"[38] T. Inoguchi, Y. Nohara, C. Nojiri, and N. Nakashima. Association of serum bilirubin levels with
469"
REFERENCES,0.8415032679738562,"risk of cancer development and total death. Scientific reports, 11(1):1–12, 2021.
470"
REFERENCES,0.8431372549019608,"[39] T. Jansen, G. Geleijnse, M. Van Maaren, M. P. Hendriks, A. Ten Teije, and A. Moncada-Torres.
471"
REFERENCES,0.8447712418300654,"Machine learning explainability in breast cancer survival. In Digital Personalized Health and
472"
REFERENCES,0.8464052287581699,"Medicine, pages 307–311. IOS Press, 2020.
473"
REFERENCES,0.8480392156862745,"[40] N. Jethani, M. Sudarshan, I. C. Covert, S. Lee, and R. Ranganath. FastSHAP: Real-time Shapley
474"
REFERENCES,0.8496732026143791,"value estimation. In ICLR, 2022.
475"
REFERENCES,0.8513071895424836,"[41] I. Kumar, C. Scheidegger, S. Venkatasubramanian, and S. A. Friedler. Shapley residuals:
476"
REFERENCES,0.8529411764705882,"Quantifying the limits of the Shapley value for explanations. In NeurIPS, pages 26598–26608,
477"
REFERENCES,0.8545751633986928,"2021.
478"
REFERENCES,0.8562091503267973,"[42] I. E. Kumar, S. Venkatasubramanian, C. Scheidegger, and S. A. Friedler. Problems with
479"
REFERENCES,0.8578431372549019,"Shapley-value-based explanations as feature importance measures. In ICML, pages 5491–5500,
480"
REFERENCES,0.8594771241830066,"2020.
481"
REFERENCES,0.8611111111111112,"[43] C. Labreuche. Explanation of pseudo-boolean functions using cooperative game theory and
482"
REFERENCES,0.8627450980392157,"prime implicants. In SUM, pages 295–308, 2022.
483"
REFERENCES,0.8643790849673203,"[44] C. Ladbury, R. Li, J. Shiao, J. Liu, M. Cristea, E. Han, T. Dellinger, S. Lee, E. Wang, C. Fisher,
484"
REFERENCES,0.8660130718954249,"et al. Characterizing impact of positive lymph node number in endometrial cancer using
485"
REFERENCES,0.8676470588235294,"machine-learning: A better prognostic indicator than figo staging? Gynecologic Oncology,
486"
REFERENCES,0.869281045751634,"164(1):39–45, 2022.
487"
REFERENCES,0.8709150326797386,"[45] Y. Liu, Z. Liu, X. Luo, and H. Zhao. Diagnosis of parkinson’s disease based on SHAP value
488"
REFERENCES,0.8725490196078431,"feature selection. Biocybernetics and Biomedical Engineering, 42(3):856–869, 2022.
489"
REFERENCES,0.8741830065359477,"[46] H. W. Loh, C. P. Ooi, S. Seoni, P. D. Barua, F. Molinari, and U. R. Acharya. Application
490"
REFERENCES,0.8758169934640523,"of explainable artificial intelligence for healthcare: A systematic review of the last decade
491"
REFERENCES,0.8774509803921569,"(2011-2022). Comput. Methods Programs Biomed., 226:107161, 2022.
492"
REFERENCES,0.8790849673202614,"[47] S. M. Lundberg, G. G. Erion, H. Chen, A. J. DeGrave, J. M. Prutkin, B. Nair, R. Katz,
493"
REFERENCES,0.880718954248366,"J. Himmelfarb, N. Bansal, and S. Lee. From local explanations to global understanding with
494"
REFERENCES,0.8823529411764706,"explainable AI for trees. Nat. Mach. Intell., 2(1):56–67, 2020.
495"
REFERENCES,0.8839869281045751,"[48] S. M. Lundberg and S. Lee. A unified approach to interpreting model predictions. In NeurIPS,
496"
REFERENCES,0.8856209150326797,"pages 4765–4774, 2017.
497"
REFERENCES,0.8872549019607843,"[49] M. Ma, R. Liu, C. Wen, W. Xu, Z. Xu, S. Wang, J. Wu, D. Pan, B. Zheng, G. Qin, et al.
498"
REFERENCES,0.8888888888888888,"Predicting the molecular subtype of breast cancer and identifying interpretable imaging features
499"
REFERENCES,0.8905228758169934,"using machine learning algorithms. European Radiology, pages 1–11, 2022.
500"
REFERENCES,0.8921568627450981,"[50] J. Marques-Silva. Logic-based explainability in machine learning. CoRR, abs/2211.00541,
501"
REFERENCES,0.8937908496732027,"2022.
502"
REFERENCES,0.8954248366013072,"[51] J. Marques-Silva and A. Ignatiev. Delivering trustworthy AI through formal XAI. In AAAI,
503"
REFERENCES,0.8970588235294118,"pages 12342–12350, 2022.
504"
REFERENCES,0.8986928104575164,"[52] L. Merrick and A. Taly. The explanation game: Explaining machine learning models using
505"
REFERENCES,0.9003267973856209,"Shapley values. In CDMAKE, pages 17–38, 2020.
506"
REFERENCES,0.9019607843137255,"[53] T. Miller. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell.,
507"
REFERENCES,0.9035947712418301,"267:1–38, 2019.
508"
REFERENCES,0.9052287581699346,"[54] A. Moncada-Torres, M. C. van Maaren, M. P. Hendriks, S. Siesling, and G. Geleijnse. Explain-
509"
REFERENCES,0.9068627450980392,"able machine learning can outperform cox regression predictions and provide insights in breast
510"
REFERENCES,0.9084967320261438,"cancer survival. Scientific reports, 11(1):6968, 2021.
511"
REFERENCES,0.9101307189542484,"[55] R. K. Mothilal, D. Mahajan, C. Tan, and A. Sharma. Towards unifying feature attribution and
512"
REFERENCES,0.9117647058823529,"counterfactual explanations: Different means to the same end. In AIES, pages 652–663, 2021.
513"
REFERENCES,0.9133986928104575,"[56] M. T. Ribeiro, S. Singh, and C. Guestrin. ""why should I trust you?"": Explaining the predictions
514"
REFERENCES,0.9150326797385621,"of any classifier. In KDD, pages 1135–1144, 2016.
515"
REFERENCES,0.9166666666666666,"[57] W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen, and K. Müller, editors. Explainable AI:
516"
REFERENCES,0.9183006535947712,"Interpreting, Explaining and Visualizing Deep Learning. Springer, 2019.
517"
REFERENCES,0.9199346405228758,"[58] M. Sarvmaili, R. Guidotti, A. Monreale, A. Soares, Z. Sadeghi, F. Giannotti, D. Pedreschi, and
518"
REFERENCES,0.9215686274509803,"S. Matwin. A modularized framework for explaining black box classifiers for text data. In
519"
REFERENCES,0.923202614379085,"CCAI, 2022.
520"
REFERENCES,0.9248366013071896,"[59] B. Selman and H. J. Levesque. Abductive and default reasoning: A computational core. In
521"
REFERENCES,0.9264705882352942,"AAAI, pages 343–348, 1990.
522"
REFERENCES,0.9281045751633987,"[60] L. S. Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28):307–
523"
REFERENCES,0.9297385620915033,"317, 1953.
524"
REFERENCES,0.9313725490196079,"[61] A. Shih, A. Choi, and A. Darwiche. A symbolic approach to explaining bayesian network
525"
REFERENCES,0.9330065359477124,"classifiers. In IJCAI, pages 5103–5111, 2018.
526"
REFERENCES,0.934640522875817,"[62] D. Slack, A. Hilgard, S. Singh, and H. Lakkaraju. Reliable post hoc explanations: Modeling
527"
REFERENCES,0.9362745098039216,"uncertainty in explainability. In NeurIPS, pages 9391–9404, 2021.
528"
REFERENCES,0.9379084967320261,"[63] A. Sorayaie Azar, S. Babaei Rikan, A. Naemi, J. Bagherzadeh Mohasefi, H. Pirnejad,
529"
REFERENCES,0.9395424836601307,"M. Bagherzadeh Mohasefi, and U. K. Wiil. Application of machine learning techniques
530"
REFERENCES,0.9411764705882353,"for predicting survival in ovarian cancer. BMC Medical Informatics and Decision Making,
531"
REFERENCES,0.9428104575163399,"22(1):345, 2022.
532"
REFERENCES,0.9444444444444444,"[64] E. Strumbelj and I. Kononenko. An efficient explanation of individual classifications using
533"
REFERENCES,0.946078431372549,"game theory. J. Mach. Learn. Res., 11:1–18, 2010.
534"
REFERENCES,0.9477124183006536,"[65] E. Strumbelj and I. Kononenko. Explaining prediction models and individual predictions with
535"
REFERENCES,0.9493464052287581,"feature contributions. Knowl. Inf. Syst., 41(3):647–665, 2014.
536"
REFERENCES,0.9509803921568627,"[66] M. Sundararajan and A. Najmi. The many Shapley values for model explanation. In ICML,
537"
REFERENCES,0.9526143790849673,"pages 9269–9278, 2020.
538"
REFERENCES,0.954248366013072,"[67] V. Voukelatou, I. Miliou, F. Giannotti, and L. Pappalardo. Understanding peace through the
539"
REFERENCES,0.9558823529411765,"world news. EPJ Data Sci., 11(1):2, 2022.
540"
REFERENCES,0.9575163398692811,"[68] Y. Wang, J. Lang, J. Z. Zuo, Y. Dong, Z. Hu, X. Xu, Y. Zhang, Q. Wang, L. Yang, S. T. Wong,
541"
REFERENCES,0.9591503267973857,"et al. The radiomic-clinical model using the SHAP method for assessing the treatment response
542"
REFERENCES,0.9607843137254902,"of whole-brain radiotherapy: a multicentric study. European Radiology, pages 1–11, 2022.
543"
REFERENCES,0.9624183006535948,"[69] D. S. Watson. Rational Shapley values. In FAccT, pages 1083–1094, 2022.
544"
REFERENCES,0.9640522875816994,"[70] D. S. Watson, L. Gultchin, A. Taly, and L. Floridi. Local explanations via necessity and
545"
REFERENCES,0.9656862745098039,"sufficiency: unifying theory and practice. In UAI, volume 161, pages 1382–1392, 2021.
546"
REFERENCES,0.9673202614379085,"[71] E. Withnell, X. Zhang, K. Sun, and Y. Guo. Xomivae: an interpretable deep learning model
547"
REFERENCES,0.9689542483660131,"for cancer classification using high-dimensional omics data.
Briefings in Bioinformatics,
548"
REFERENCES,0.9705882352941176,"22(6):bbab315, 2021.
549"
REFERENCES,0.9722222222222222,"[72] T. Yan and A. D. Procaccia. If you like Shapley then you’ll love the core. In AAAI, pages
550"
REFERENCES,0.9738562091503268,"5751–5759, 2021.
551"
REFERENCES,0.9754901960784313,"[73] K. Young, G. Booth, B. Simpson, R. Dutton, and S. Shrapnel.
Deep neural network or
552"
REFERENCES,0.9771241830065359,"dermatologist? CoRR, abs/1908.06612, 2019.
553"
REFERENCES,0.9787581699346405,"[74] D. Yu, Z. Liu, C. Su, Y. Han, X. Duan, R. Zhang, X. Liu, Y. Yang, and S. Xu. Copy number
554"
REFERENCES,0.9803921568627451,"variation in plasma as a tool for lung cancer prediction using extreme gradient boosting (xgboost)
555"
REFERENCES,0.9820261437908496,"classifier. Thoracic cancer, 11(1):95–102, 2020.
556"
REFERENCES,0.9836601307189542,"[75] R. Zarinshenas, C. Ladbury, H. McGee, D. Raz, L. Erhunmwunsee, R. Pathak, S. Glaser,
557"
REFERENCES,0.9852941176470589,"R. Salgia, T. Williams, and A. Amini. Machine learning to refine prognostic and predictive
558"
REFERENCES,0.9869281045751634,"nodal burden thresholds for post-operative radiotherapy in completely resected stage iii-n2
559"
REFERENCES,0.988562091503268,"non-small cell lung cancer. Radiotherapy and Oncology, 173:10–18, 2022.
560"
REFERENCES,0.9901960784313726,"[76] G. Zhang, Y. Shi, P. Yin, F. Liu, Y. Fang, X. Li, Q. Zhang, and Z. Zhang. A machine learning
561"
REFERENCES,0.9918300653594772,"model based on ultrasound image features to assess the risk of sentinel lymph node metastasis
562"
REFERENCES,0.9934640522875817,"in breast cancer patients: Applications of scikit-learn and SHAP. Frontiers in Oncology, 12,
563"
REFERENCES,0.9950980392156863,"2022.
564"
REFERENCES,0.9967320261437909,"[77] Y. Zhang, Y. Weng, and J. Lund. Applications of explainable artificial intelligence in diagnosis
565"
REFERENCES,0.9983660130718954,"and surgery. Diagnostics, 12(2):237, 2022.
566"
