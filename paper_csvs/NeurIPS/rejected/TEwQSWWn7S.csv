Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009727626459143969,"Thanks to their simple architecture, Restricted Boltzmann Machines (RBMs) are
1"
ABSTRACT,0.0019455252918287938,"powerful tools for modeling complex systems and extracting interpretable insights
2"
ABSTRACT,0.0029182879377431907,"from data. However, training RBMs, as other energy-based models, on highly
3"
ABSTRACT,0.0038910505836575876,"structured data poses a major challenge, as effective training relies on mixing the
4"
ABSTRACT,0.0048638132295719845,"Markov chain Monte Carlo simulations used to estimate the gradient. This process
5"
ABSTRACT,0.005836575875486381,"is often hindered by multiple second-order phase transitions and the associated
6"
ABSTRACT,0.006809338521400778,"critical slowdown. In this paper, we present an innovative method in which the
7"
ABSTRACT,0.007782101167315175,"principal directions of the dataset are integrated into a low-rank RBM through a
8"
ABSTRACT,0.008754863813229572,"convex optimization procedure. This approach enables efficient sampling of the
9"
ABSTRACT,0.009727626459143969,"equilibrium measure via a static Monte Carlo process. By starting the standard
10"
ABSTRACT,0.010700389105058366,"training process with a model that already accurately represents the main modes of
11"
ABSTRACT,0.011673151750972763,"the data, we bypass the initial phase transitions. Our results show that this strategy
12"
ABSTRACT,0.01264591439688716,"successfully trains RBMs to capture the full diversity of data in datasets where
13"
ABSTRACT,0.013618677042801557,"previous methods fail. Furthermore, we use the training trajectories to propose a
14"
ABSTRACT,0.014591439688715954,"new sampling method, parallel trajectory tempering, which allows us to sample
15"
ABSTRACT,0.01556420233463035,"the equilibrium measure of the trained model much faster than previous optimized
16"
ABSTRACT,0.016536964980544747,"MCMC approaches and a better estimation of the log-likelihood. We illustrate the
17"
ABSTRACT,0.017509727626459144,"success of the training method on several highly structured datasets.
18"
INTRODUCTION,0.01848249027237354,"1
Introduction
19"
INTRODUCTION,0.019455252918287938,"Energy-based models (EBMs) are a classic approach to generative modeling that has been studied for
20"
INTRODUCTION,0.020428015564202335,"decades. They were introduced using the Restricted Boltzmann Machine formulation by Smolen-
21"
INTRODUCTION,0.021400778210116732,"sky [1] and later further developed by Sejnowski et al. [2]. They provide a straightforward method for
22"
INTRODUCTION,0.02237354085603113,"modeling effective interactions within complex data distributions and for sufficiently simple energy
23"
INTRODUCTION,0.023346303501945526,"functions, such as the Boltzmann machine (BM) [3], it is also possible to interpret and infer the
24"
INTRODUCTION,0.024319066147859923,"underlying constituent rules from the observed data. This inference strategy is often associated with
25"
INTRODUCTION,0.02529182879377432,"the inverse Ising problem and pairwise interaction models [4], and it has found a great variety of
26"
INTRODUCTION,0.026264591439688716,"applications in fields such as neuroscience [5] or computational biology [6]. A recent work has
27"
INTRODUCTION,0.027237354085603113,"proposed replacing the use of pairwise models with the Restricted Boltzmann Machine (RBM) [7],
28"
INTRODUCTION,0.02821011673151751,"as it allows the same direct interpretation of its energy function as an explicit many-body interaction
29"
INTRODUCTION,0.029182879377431907,"model while greatly extending the expressive power of the model. RBMs are also very useful for
30"
INTRODUCTION,0.030155642023346304,"grouping data into hierarchical families [8]. On the diametrically opposite side (on interpretability)
31"
INTRODUCTION,0.0311284046692607,"are generative ConvNets [9, 10], where the energy function is formulated as a deep neural network,
32"
INTRODUCTION,0.032101167315175094,"which are capable of synthesizing photorealistic images but are almost impossible to interpret as a
33"
INTRODUCTION,0.033073929961089495,"physical model.
34"
INTRODUCTION,0.03404669260700389,"The applications of simple EBMs in science are very diverse. For example, they are often used today
35"
INTRODUCTION,0.03501945525291829,"to encode the Hamiltonian of physical many-body systems, such as Quantum wave functions [11]
36"
INTRODUCTION,0.03599221789883268,"or the accurate determination of ground state wave functions of strongly interacting and entangled
37"
INTRODUCTION,0.03696498054474708,"quantum spins [12] or they have proven to be suitable for the representation of the AdS/CFT
38"
INTRODUCTION,0.037937743190661476,"correspondence in theories of quantum gravity [13, 14]. Simple EBMs are also very common to
39"
INTRODUCTION,0.038910505836575876,"encode the evolutionary constraints in protein families [6, 15], and to predict mutations [16], or to
40"
INTRODUCTION,0.03988326848249027,"generate realistic synthetic sequences, such as fake human genomes [17, 18]. These examples show
41"
INTRODUCTION,0.04085603112840467,"that, despite their somewhat old-fashioned architecture, shallow EBMs are increasingly seen as useful
42"
INTRODUCTION,0.04182879377431906,"tools for better understanding modern physics/biology, as they allow for a certain level of analytical
43"
INTRODUCTION,0.042801556420233464,"description.
44"
INTRODUCTION,0.04377431906614786,"Despite the appealing modeling properties of RBMs, they are notoriously difficult to train, a challenge
45"
INTRODUCTION,0.04474708171206226,"common to EBMs in general. The main difficulty arises from the computation of the log-likelihood
46"
INTRODUCTION,0.04571984435797665,"gradient, which requires an ergodic exploration of a dynamically evolving and potentially complex
47"
INTRODUCTION,0.04669260700389105,"free energy landscape using Markov Chain Monte Carlo (MCMC) processes. Recent studies have
48"
INTRODUCTION,0.047665369649805445,"shown that models trained with non-convergent MCMC processes suffer from out-of-equilibrium
49"
INTRODUCTION,0.048638132295719845,"dynamic memory effects [19, 20, 21]. This dynamical behavior can be explained analytically using
50"
INTRODUCTION,0.04961089494163424,"moment-matching arguments [19, 22]. While exploiting these effects can yield fast and accurate
51"
INTRODUCTION,0.05058365758754864,"generative models, even for highly structured data [23] or high-quality images with RBMs [24], this
52"
INTRODUCTION,0.05155642023346303,"approach results in a sharp separation between the model’s Gibbs-Boltzmann distribution and the
53"
INTRODUCTION,0.05252918287937743,"dataset distribution, thereby undermining the interpretability of the model parameters [22, 7]. Thus,
54"
INTRODUCTION,0.053501945525291826,"to extract meaningful information from datasets using RBMs, it is essential to ensure proper mixing
55"
INTRODUCTION,0.054474708171206226,"of the chains during training, in short, one needs equilibrium models.
56"
INTRODUCTION,0.05544747081712062,"Both the ability to train an RBM in equilibrium and to generate convincing new samples from its
57"
INTRODUCTION,0.05642023346303502,"equilibrium measure strongly depend on the dataset in question. For typical image datasets such
58"
INTRODUCTION,0.057392996108949414,"as MNIST or CIFAR-10, good RBMs can be obtained by increasing the number of MCMC steps.
59"
INTRODUCTION,0.058365758754863814,"However, this approach is no longer feasible for highly structured datasets [25]. Datasets from
60"
INTRODUCTION,0.05933852140077821,"which one seeks scientific insights are often highly structured, such as genomics/proteomics data
61"
INTRODUCTION,0.06031128404669261,"or low-temperature many-body physical systems. These datasets typically exhibit distinct clusters,
62"
INTRODUCTION,0.061284046692607,"identifiable via principal component analysis (PCA) which form distant groups of similar entries.
63"
INTRODUCTION,0.0622568093385214,"We show an example of the PCA of 4 clustered dataset we will be studying in this work in Fig. 1;
64"
INTRODUCTION,0.0632295719844358,"details about these datasets are given in the caption and in the Supplemental Information (SI). During
65"
INTRODUCTION,0.06420233463035019,"training, the model must evolve from an initial normal distribution to an increasingly multimodal
66"
INTRODUCTION,0.0651750972762646,"distribution. Sampling from multimodal distributions is particularly challenging because the mixing
67"
INTRODUCTION,0.06614785992217899,"times are determined by the transition times between modes. But this is not the only difficulty. These
68"
INTRODUCTION,0.06712062256809338,"distant modes are encoded by second-order phase transitions during training [26, 27, 28], leading to
69"
INTRODUCTION,0.06809338521400778,"diverging mixing times in these regions — a phenomenon known as critical slowdown —, which
70"
INTRODUCTION,0.06906614785992218,"means that mixing times are expected to grow with a power of their system size. This sampling
71"
INTRODUCTION,0.07003891050583658,"challenge not only hinders the training process, but also limits the model’s ability to generate new
72"
INTRODUCTION,0.07101167315175097,"samples. Obtaining new and independent configurations would require an impractically large number
73"
INTRODUCTION,0.07198443579766536,"of sampling steps.
74"
RELATED WORK,0.07295719844357977,"2
Related work
75"
RELATED WORK,0.07392996108949416,"Training EBMs by maximizing log-likelihood has long been a challenge in the community [29, 10].
76"
RELATED WORK,0.07490272373540856,"EBMs gained popularity with the introduction of the contrastive divergence algorithm [30], in which
77"
RELATED WORK,0.07587548638132295,"a set of parallel chains is initialized on independent examples in the minibatch and the MCMC
78"
RELATED WORK,0.07684824902723736,"process iterates for a few steps. Despite its widespread use, this algorithm yields models with poor
79"
RELATED WORK,0.07782101167315175,"equilibrium properties that are ineffective as generative models [31, 32, 21]. An improvement is the
80"
RELATED WORK,0.07879377431906615,"persistent contrastive divergence (PCD) algorithm [33], which maintains a permanent chain in which
81"
RELATED WORK,0.07976653696498054,"the last configurations used to estimate the previous gradient update are reused. PCD acts like a
82"
RELATED WORK,0.08073929961089495,"slow annealing process improving gradient estimation quality. However, it often fails on clustered
83"
RELATED WORK,0.08171206225680934,"data as the statistical properties of the permanent chain quickly move away from the equilibrium
84"
RELATED WORK,0.08268482490272373,"measure and degrade the model [25]. This problem, which is primarily related to phase coexistence,
85"
RELATED WORK,0.08365758754863813,"can be addressed with constrained MCMC methods if appropriate order parameters are identified.
86"
RELATED WORK,0.08463035019455253,"For RBMs, these order parameters are related to the singular value decomposition of the model
87"
RELATED WORK,0.08560311284046693,"coupling matrix, which enables efficient reconstruction of multimodal distributions [25]. Although
88"
RELATED WORK,0.08657587548638132,"this method is effective for evaluating model quality, it is too computationally intensive to be used
89"
RELATED WORK,0.08754863813229571,"in training, even if it leads to models with good equilibrium properties. Other optimized MCMC
90"
RELATED WORK,0.08852140077821012,"methods, such as the Parallel Tempering (PT) [34] algorithm, simulate multiple models at different
91"
RELATED WORK,0.08949416342412451,"temperatures, facilitating mixing through temperature exchange [35, 32]. However, PT is costly and
92"
RELATED WORK,0.09046692607003891,"−0.5
0.0
PC1 −0.4 −0.2 0.0 0.2 PC2"
RELATED WORK,0.0914396887159533,"0.2
0.4
PC1 0.2 0.3 0.4 0.5 PC2"
RELATED WORK,0.09241245136186771,"−0.25
0.00
0.25
PC1 −0.2 0.0 PC2"
RELATED WORK,0.0933852140077821,C – Human
RELATED WORK,0.0943579766536965,"Genome
B - 
Mickey"
RELATED WORK,0.09533073929961089,Dataset
RELATED WORK,0.0963035019455253,Low-rank RBM generation
RELATED WORK,0.09727626459143969,"0.2
0.4
PC1 0.2 0.4 0.6 PC2"
RELATED WORK,0.09824902723735408,"A - 
MNIST01 E
F
D"
RELATED WORK,0.09922178988326848,"Figure 1: Clustered datasets. In A-C we show the 4 different clustered data sets that we will consider
in this paper, projected onto their first two PCA components. In A we show the data of the MNIST
01 dataset (both projected and some instances), which contains only the 0-1 images of the complete
MNIST dataset. In B, we show the Mickey dataset, an artificial dataset whose PCA forms a “Mickey""”
face shape. In C, we show data from the Human Genome Dataset (HGD), which contains binary
vectors each corresponding to a human individual and whose sites correspond to selected genes. A
value of 1 at a particular position means that a mutation was observed there compared to an individual
reference sequence. Details of these data sets can be found in the SI. In D-F we show the samples we
generate with the low-rank RBMs that are used as initial point of a standard training."
RELATED WORK,0.10019455252918288,"often ineffective, especially because EBMs undergo first-order phase transitions at the temperature
93"
RELATED WORK,0.10116731517509728,"where PT typically fails because one needs too many temperatures to make the moves accepted. We
94"
RELATED WORK,0.10214007782101167,"will see below that a more appropriate approach exchanges the models at different training times,
95"
RELATED WORK,0.10311284046692606,"which only implies crossing second-order phase transitions.
96"
RELATED WORK,0.10408560311284047,"The population annealing algorithm, which reweights parallel chains during learning based on their
97"
RELATED WORK,0.10505836575875487,"relative weight changes during parameter updates, was proposed as an alternative [36]. Similarly,
98"
RELATED WORK,0.10603112840466926,"reweighting chains using non-equilibrium physics concepts such as the Jarzynski equality has been
99"
RELATED WORK,0.10700389105058365,"proposed [37]. Both approaches struggle with highly structured data sets. To prevent the different
100"
RELATED WORK,0.10797665369649806,"chains to get too correlated around the training phase transitions, one must either increase the
101"
RELATED WORK,0.10894941634241245,"number of sampling steps or decrease the learning rate, which in practice means very long training
102"
RELATED WORK,0.10992217898832685,"processes to ensure a proper equilibrium training. Another strategy is to use EBMs as corrections
103"
RELATED WORK,0.11089494163424124,"for straightforward-to-sample flow-based models [38]. This simplifies sampling and learning, but
104"
RELATED WORK,0.11186770428015565,"sacrifices the interpretability of the energy function, which was our goal. An evolving flow model
105"
RELATED WORK,0.11284046692607004,"can be used as a fast sampling moves proposer for the EBM [39] objective. This method requires the
106"
RELATED WORK,0.11381322957198443,"training of two different networks in parallel and may result in the drop of the move acceptancy as
107"
RELATED WORK,0.11478599221789883,"the EBM becomes specialized.
108"
RELATED WORK,0.11575875486381323,"For RBMs, a recent method called “stacked tempering"" [40] dramatically speeds up sampling by
109"
RELATED WORK,0.11673151750972763,"training smaller RBMs with latent variables from previous models, allowing fast updates to be
110"
RELATED WORK,0.11770428015564202,"proposed using a PT like algorithm. Authors also showed that this algorithm was much faster than
111"
RELATED WORK,0.11867704280155641,"the standard PT. While effective, it is too cumbersome for use in training. Also for RBMs, it has
112"
RELATED WORK,0.11964980544747082,"recently been shown that it is possible to train a low-rank RBM that accurately reproduces the
113"
RELATED WORK,0.12062256809338522,"statistics of the data projected along the d first data principal directions through a convex and very
114"
RELATED WORK,0.12159533073929961,"fast optimization process (see [41] and the discussion below). This low-rank model can be seen as a
115"
RELATED WORK,0.122568093385214,"good approximation to the correct RBM needed to describe the data, and has the nice property that it
116"
RELATED WORK,0.12354085603112841,"can be efficiently sampled via a static Monte Carlo process.
117"
RELATED WORK,0.1245136186770428,"In this paper, we will show how to drastically reduce training times by starting the RBM training
118"
RELATED WORK,0.1254863813229572,"process at this low-rank RBM, as this means that the first and strongest dynamic effects associated
119"
RELATED WORK,0.1264591439688716,"with them are directly bypassed. We also show that one can exploit the training trajectory to develop
120"
RELATED WORK,0.12743190661478598,"an effective sampling method, the parallel trajectory tempering (PTT) that outperforms the “stacked
121"
RELATED WORK,0.12840466926070038,"tempering"" [40] and only requires saving a reduced number of models during the training. This
122"
RELATED WORK,0.1293774319066148,"strategy also allows to obtain reliable estimations for the log-likelihood in well-trained models, much
123"
RELATED WORK,0.1303501945525292,"better than those obtained with the standard Annealing Important Sampling (AIS) techniques [42].
124"
RELATED WORK,0.13132295719844359,"Using both strategies, we show that we are able to train and evaluate methods that accurately represent
125"
RELATED WORK,0.13229571984435798,"the different modes in the dataset, where standard methods lead to mode collapse effects.
126"
THE RESTRICTED BOLTZMANN MACHINE,0.13326848249027237,"3
The Restricted Boltzmann Machine
127"
THE RESTRICTED BOLTZMANN MACHINE,0.13424124513618677,"The RBM is composed by Nv visible nodes and Nh hidden nodes. In our study, we primarily use
128"
THE RESTRICTED BOLTZMANN MACHINE,0.13521400778210116,"binary variables {0, 1} or ±1 for both layers. The two layers (visible and hidden) interact via a
129"
THE RESTRICTED BOLTZMANN MACHINE,0.13618677042801555,"weight matrix w, with no direct couplings within a given layer. Variables are also adjusted by visible
130"
THE RESTRICTED BOLTZMANN MACHINE,0.13715953307392997,"and hidden local biases, θ and η, respectively. The Gibbs-Boltzmann distribution for this model is
131"
THE RESTRICTED BOLTZMANN MACHINE,0.13813229571984437,"expressed as
132"
THE RESTRICTED BOLTZMANN MACHINE,0.13910505836575876,"p(v, h) = 1"
THE RESTRICTED BOLTZMANN MACHINE,0.14007782101167315,"Z exp [−H(v, h)] where H(v, h) = −
X"
THE RESTRICTED BOLTZMANN MACHINE,0.14105058365758755,"ia
viwiaha −
X"
THE RESTRICTED BOLTZMANN MACHINE,0.14202334630350194,"i
θivi −
X"
THE RESTRICTED BOLTZMANN MACHINE,0.14299610894941633,"a
ηaha,
(1)"
THE RESTRICTED BOLTZMANN MACHINE,0.14396887159533073,"where Z is the partition function of the system. As with other models containing hidden variables, the
133"
THE RESTRICTED BOLTZMANN MACHINE,0.14494163424124515,"training objective is to minimize the distance between the empirical distribution of the data, pD(v),
134"
THE RESTRICTED BOLTZMANN MACHINE,0.14591439688715954,"and the model’s marginal distribution over the visible variables, p(v) = P
h exp [−H(v, h)] /Z =
135"
THE RESTRICTED BOLTZMANN MACHINE,0.14688715953307394,"exp [−H(v)] /Z. Minimizing the Kullback-Leibler divergence is equivalent to maximizing the
136"
THE RESTRICTED BOLTZMANN MACHINE,0.14785992217898833,"likelihood of observing the dataset in the model. Thus, the log-likelihood L = ⟨−H(v)⟩D −
137"
THE RESTRICTED BOLTZMANN MACHINE,0.14883268482490272,"log Z can be maximized using the classical stochastic gradient ascent. For a training dataset D =
138"
THE RESTRICTED BOLTZMANN MACHINE,0.14980544747081712,"{v(m)}m=1,...,M, the log-likelihood gradient is given by
139"
THE RESTRICTED BOLTZMANN MACHINE,0.1507782101167315,"∂L
∂wia
= ⟨viha⟩D −⟨viha⟩RBM, ∂L"
THE RESTRICTED BOLTZMANN MACHINE,0.1517509727626459,"∂θi
= ⟨vi⟩D −⟨vi⟩RBM, ∂L"
THE RESTRICTED BOLTZMANN MACHINE,0.15272373540856032,"∂ηa
= ⟨ha⟩D −⟨ha⟩RBM,
(2)"
THE RESTRICTED BOLTZMANN MACHINE,0.15369649805447472,"where ⟨·⟩D denotes the average with respect to the entries in the dataset, and ⟨·⟩RBM with respect
140"
THE RESTRICTED BOLTZMANN MACHINE,0.1546692607003891,"to p(v, h). Since Z is intractable, the model averages in the gradient are typically estimated us-
141"
THE RESTRICTED BOLTZMANN MACHINE,0.1556420233463035,"ing Ns independent MCMC processes, and observable averages ⟨o(v, h)⟩RBM are replaced by
142
PR
r=1 o(v(r), h(r))/R, with (v(r), h(r)) being the last configurations reached with each of the
143"
THE RESTRICTED BOLTZMANN MACHINE,0.1566147859922179,"r = 1, . . . , R parallel chains. To obtain reliable estimates, it should be ensured that each of the
144"
THE RESTRICTED BOLTZMANN MACHINE,0.1575875486381323,"Markov chains mix well before each parameter update. However, ensuring equilibrium at each update
145"
THE RESTRICTED BOLTZMANN MACHINE,0.15856031128404668,"is impractical, slow and tedious. The common use of non-convergent MCMC processes is the cause
146"
THE RESTRICTED BOLTZMANN MACHINE,0.15953307392996108,"of most difficulties and weird dynamical behaviors encountered in training RBMs [21].
147"
THE RESTRICTED BOLTZMANN MACHINE,0.1605058365758755,"Typical MCMC mixing times in RBMs are very small at the beginning of the training and grow as
148"
THE RESTRICTED BOLTZMANN MACHINE,0.1614785992217899,"it progresses [21], suffering with sharp increases every-time the training trajectory crosses each of
149"
THE RESTRICTED BOLTZMANN MACHINE,0.16245136186770429,"the critical transitions that give birth to new modes [28]. In order to minimize out-of-equilibrium
150"
THE RESTRICTED BOLTZMANN MACHINE,0.16342412451361868,"effects, it is often useful to keep R permanent (or persistent) chains, which means that the last
151"
THE RESTRICTED BOLTZMANN MACHINE,0.16439688715953307,"configurations reached with the MCMC process used to estimate the gradient at a given parameter
152"
THE RESTRICTED BOLTZMANN MACHINE,0.16536964980544747,"update t, Pt ≡{(v(r)
t
, h(r)
t )}R
r=1, are used to initialize the chains of the subsequent update t + 1.
153"
THE RESTRICTED BOLTZMANN MACHINE,0.16634241245136186,"This algorithm is typically referred as PCD. In this scheme, the process of training can be mimicked
154"
THE RESTRICTED BOLTZMANN MACHINE,0.16731517509727625,"to a slow cooling process, only that instead of varying a single parameter, the temperature, a whole
155"
THE RESTRICTED BOLTZMANN MACHINE,0.16828793774319067,"set of parameters Θt = (wt, θt, ηt) are updated at every step to Θt+1 = Θt + γ∇Lt with ∇Lt
156"
THE RESTRICTED BOLTZMANN MACHINE,0.16926070038910507,"being the gradient in Eq. (2) estimated using the configurations in Pt, and γ being the learning rate.
157"
THE LOW-RANK RBM PRETRAINED,0.17023346303501946,"4
The low-rank RBM pretrained
158"
THE LOW-RANK RBM PRETRAINED,0.17120622568093385,"In Ref. [41], it was shown that it is possible to train exactly (i.e. by direct numerical integration
159"
THE LOW-RANK RBM PRETRAINED,0.17217898832684825,"instead of MCMC sampling) an RBM containing a reduced number of modes in the weight matrix
160"
THE LOW-RANK RBM PRETRAINED,0.17315175097276264,"W by exploiting a mapping between the RBM and a Restricted Coulomb Machine and solving a
161"
THE LOW-RANK RBM PRETRAINED,0.17412451361867703,"convex optimization problem, see the SI. In other words, it is possible to train a RBM with a coupling
162"
THE LOW-RANK RBM PRETRAINED,0.17509727626459143,"matrix of this simplified form
163 W = d
X"
THE LOW-RANK RBM PRETRAINED,0.17607003891050585,"α=1
wα ¯uαu⊤
α ,
with
(uα, ¯uα) ∈RNv × RNh,
(3)"
THE LOW-RANK RBM PRETRAINED,0.17704280155642024,"and where the right singular vectors {uα}d
α=1 correspond exactly to the first d principal directions
164"
THE LOW-RANK RBM PRETRAINED,0.17801556420233464,"of the data set. Under this assumption, it is possible to write p(v) only as a function of d order
165"
THE LOW-RANK RBM PRETRAINED,0.17898832684824903,"parameters given by the magnetizations along each of the uα components, mα(v) = uα · v/√Nv,
166"
THE LOW-RANK RBM PRETRAINED,0.17996108949416342,"and in particular,
167"
THE LOW-RANK RBM PRETRAINED,0.18093385214007782,"H(v) = −
X"
THE LOW-RANK RBM PRETRAINED,0.1819066147859922,"a
log cosh p Nv¯ua d
X"
THE LOW-RANK RBM PRETRAINED,0.1828793774319066,"α=1
wαmα + ηa !"
THE LOW-RANK RBM PRETRAINED,0.18385214007782102,"= H(m(v)),
(4)"
THE LOW-RANK RBM PRETRAINED,0.18482490272373542,"where m = (m1, . . . , mα). As proposed in [41], the optimal parameters of such a model can
168"
THE LOW-RANK RBM PRETRAINED,0.1857976653696498,"basically be determined by solving a regression problem. We describe this method in details in the
169"
THE LOW-RANK RBM PRETRAINED,0.1867704280155642,"SI. This means that once the model is trained, we obtain a probability p(m) defined on a much
170"
THE LOW-RANK RBM PRETRAINED,0.1877431906614786,"lower dimension than the original p(v). Such a probability can be straightforwardly sampled using
171"
THE LOW-RANK RBM PRETRAINED,0.188715953307393,"inverse transform sampling. Since this method requires a discretization of the m-space both for
172"
THE LOW-RANK RBM PRETRAINED,0.18968871595330739,"training and generation, we cannot consider intrinsic space dimension d > 4 dimensions in practice.
173"
THE LOW-RANK RBM PRETRAINED,0.19066147859922178,"These low-rank RBMs are then trained to reproduce the statistics of the dataset projected in its first d
174"
THE LOW-RANK RBM PRETRAINED,0.1916342412451362,"principal components. Despite their simplicity, the low-rank models are already able to generate an
175"
THE LOW-RANK RBM PRETRAINED,0.1926070038910506,"approximate version of the dataset, as shown in Fig. 1–D-F for the 4 datasets previously presented.
176"
THE LOW-RANK RBM PRETRAINED,0.193579766536965,"In the initial stage of the standard learning process, the model encodes the strongest PCA components
177"
THE LOW-RANK RBM PRETRAINED,0.19455252918287938,"of the data through multiple critical transitions [26, 27, 28]. Pre-training with the low-rank construc-
178"
THE LOW-RANK RBM PRETRAINED,0.19552529182879377,"tion allows us to bypass these transitions and avoid out-of-equilibrium effects caused by critical
179"
THE LOW-RANK RBM PRETRAINED,0.19649805447470817,"slowing down associated to these transitions. Once the main directions are incorporated, training can
180"
THE LOW-RANK RBM PRETRAINED,0.19747081712062256,"efficiently continue with standard algorithms like PCD, as the mixing times of pre-trained machines
181"
THE LOW-RANK RBM PRETRAINED,0.19844357976653695,"tend to be much shorter. In particular, in the PCD-100 training with MNIST01, relaxation times for
182"
THE LOW-RANK RBM PRETRAINED,0.19941634241245138,"the visible variables’ time correlation reach 5 · 105 MCMC steps at the first three transitions, coincid-
183"
THE LOW-RANK RBM PRETRAINED,0.20038910505836577,"ing with the growth of singular values in the model weight matrix W. In contrast, the pre-trained
184"
THE LOW-RANK RBM PRETRAINED,0.20136186770428016,"machine has a much shorter relaxation time of ∼103, allowing us to safely restart the PCD process
185"
THE LOW-RANK RBM PRETRAINED,0.20233463035019456,"from a set of equilibrium samples generated by static sampling of the low-rank RBM.
186"
THE LOW-RANK RBM PRETRAINED,0.20330739299610895,"Overcoming these transitions has dramatic implications for the quality of the models we can train
187"
THE LOW-RANK RBM PRETRAINED,0.20428015564202334,"and how accurately they reproduce the statistics of the data. In Fig. 2, we show for 3 datasets the
188"
THE LOW-RANK RBM PRETRAINED,0.20525291828793774,"equilibrium samples drawn from 3 RBMs trained with identical number of samples, minibatch size,
189"
THE LOW-RANK RBM PRETRAINED,0.20622568093385213,"k = 100 Gibbs steps, and learning rate γ = 0.01, but different training strategies. In particular,
190"
THE LOW-RANK RBM PRETRAINED,0.20719844357976655,"we consider 2 RBMs trained from scratch with the standard PCD [33] and the recently proposed
191"
THE LOW-RANK RBM PRETRAINED,0.20817120622568094,"Jarzynski reweighing method [23] (see SI for our specific implementation in the RBM), and a final
192"
THE LOW-RANK RBM PRETRAINED,0.20914396887159534,"machine trained with PCD and pre-trained with a low-rank RBM. In all cases, the quality of the
193"
THE LOW-RANK RBM PRETRAINED,0.21011673151750973,"generated samples is significantly better when pre-training is used. For the Mickey dataset, neither
194"
THE LOW-RANK RBM PRETRAINED,0.21108949416342412,"JarRBM nor normal PCD are able to generate convincing data. For the MNIST01 dataset, all 3
195"
THE LOW-RANK RBM PRETRAINED,0.21206225680933852,"methods are able to generate convincing data, but only Pretrain+PCD is able to correctly balance all
196"
THE LOW-RANK RBM PRETRAINED,0.2130350194552529,"modes, as can be seen in Fig. 3, where we compare the histograms of the generated data projected
197"
THE LOW-RANK RBM PRETRAINED,0.2140077821011673,"onto the first 3 PCA directions with those of the dataset and a random selection of the generated
198"
THE LOW-RANK RBM PRETRAINED,0.21498054474708173,"samples. We see that the pre-training+PCD training perfectly balances the different modes (here we
199"
THE LOW-RANK RBM PRETRAINED,0.21595330739299612,"show the first 3 directions, but it goes much further), unlike the other 2 methods, and also generates
200"
THE LOW-RANK RBM PRETRAINED,0.2169260700389105,"more diverse images. We can also compare the log-likelihood of all 3 models and find that the
201"
THE LOW-RANK RBM PRETRAINED,0.2178988326848249,"pre-trained RBM achieves higher values. At this point, it is important to emphasize that in order to
202"
THE LOW-RANK RBM PRETRAINED,0.2188715953307393,"properly quantify the increase in log-likelihood, we need to use the PTT algorithm (see section 5) to
203"
THE LOW-RANK RBM PRETRAINED,0.2198443579766537,"correctly thermalize in these well-trained machines. For comparison, we show our PTT measure in
204"
THE LOW-RANK RBM PRETRAINED,0.22081712062256809,"dark and solid lines, while the standard AIS [42] estimate is shown in light dashed lines.
205"
THE LOW-RANK RBM PRETRAINED,0.22178988326848248,"Already from the scatter plots we see that the pre-training has a dramatic effect in obtaining models
206"
THE LOW-RANK RBM PRETRAINED,0.2227626459143969,"where all modes are properly balanced, but also has important effects in the maximum test-likelihood
207"
THE LOW-RANK RBM PRETRAINED,0.2237354085603113,Mickey
THE LOW-RANK RBM PRETRAINED,0.2247081712062257,"Dataset
Jar-RBM
PCD
pre-train+PCD"
THE LOW-RANK RBM PRETRAINED,0.22568093385214008,"MNIST-01
HGD"
THE LOW-RANK RBM PRETRAINED,0.22665369649805447,"Figure 2: We compare the equilibrium samples generated by RBMs trained on the Mickey, MNIST01,
and HGD datasets using three different training schemes: Jarzynski (JarRBM), PCD, and PCD
initialized on low-rank RBMs (used to generate the samples in Fig. 1–D-F). To assess the fitting of
the modes, we show a density plot of the projections of the data in the first two principal directions
of each dataset. We compare these results with the density plot of the original datasets in the first
column."
THE LOW-RANK RBM PRETRAINED,0.22762645914396887,"we can achieve. In all cases, these equilibrium samples are drawn using the trajectory PT algorithm
208"
THE LOW-RANK RBM PRETRAINED,0.22859922178988326,"that will be explained in the next section, and the log-likelihood obtained using the equilibrium
209"
THE LOW-RANK RBM PRETRAINED,0.22957198443579765,"configurations obtained at different epochs as a result of the trajectory PT flow.
210"
THE LOW-RANK RBM PRETRAINED,0.23054474708171208,"5
Standard Gibbs sampling vs. Parallel Trajectory Tempering (PTT)
211"
THE LOW-RANK RBM PRETRAINED,0.23151750972762647,"One major challenge with structured datasets is quantifying the model’s quality, since sampling the
212"
THE LOW-RANK RBM PRETRAINED,0.23249027237354086,"equilibrium measure of a well-trained model is often too time-consuming. This affects the reliability of
213"
THE LOW-RANK RBM PRETRAINED,0.23346303501945526,"generated samples and indirect measures as log-likelihood’s estimation through Annealing Importance
214"
THE LOW-RANK RBM PRETRAINED,0.23443579766536965,"Sampling (AIS) [42], making them inaccurate and meaningless.
215"
THE LOW-RANK RBM PRETRAINED,0.23540856031128404,"To illustrate this problem, let us consider the MNIST01 and the HGD datasets. MNIST01 dataset is
216"
THE LOW-RANK RBM PRETRAINED,0.23638132295719844,"bimodal and the HGD highly multimodal as shown in their PCA in Figs. 1–A and C. Let us consider
217"
THE LOW-RANK RBM PRETRAINED,0.23735408560311283,"that we want to sample the equilibrium measure of the RBMs trained using low-rank RBM pretraining.
218"
THE LOW-RANK RBM PRETRAINED,0.23832684824902725,"In order to draw new samples from these models, one would typically run MCMC processes from
219"
THE LOW-RANK RBM PRETRAINED,0.23929961089494164,"random initialization and iterate them until convergence. The mixing time is controlled by the
220"
THE LOW-RANK RBM PRETRAINED,0.24027237354085604,"jumping time between clusters. To accurately estimate the relative weight between modes, the
221"
THE LOW-RANK RBM PRETRAINED,0.24124513618677043,"MCMC processes must be ergodic, requiring many back-and-forth jumps. However, as shown in
222"
THE LOW-RANK RBM PRETRAINED,0.24221789883268482,"Figs. 4–A and C for the MNIST01 and HGD datasets, Gibbs sampling dynamics are extremely slow,
223"
THE LOW-RANK RBM PRETRAINED,0.24319066147859922,"rarely producing jumps even after 104 MCMC steps. The yellow curves in Figs. 4–B and D show the
224"
THE LOW-RANK RBM PRETRAINED,0.2441634241245136,"mean number of jumps over 100 independent chains as a function of MCMC steps, indicating that a
225"
THE LOW-RANK RBM PRETRAINED,0.245136186770428,"proper equilibrium generation would require at least 106 −107 MCMC steps.
226"
THE LOW-RANK RBM PRETRAINED,0.24610894941634243,"One effective way to accelerate the dynamics is to exploit the training trajectory, where the model
227"
THE LOW-RANK RBM PRETRAINED,0.24708171206225682,"progressively specializes through second-order phase transitions. To achieve this, we save RBMs
228"
THE LOW-RANK RBM PRETRAINED,0.2480544747081712,"trained at various epochs and propose swaps between configurations of similarly trained models. We
229"
THE LOW-RANK RBM PRETRAINED,0.2490272373540856,pre-train+PCD
THE LOW-RANK RBM PRETRAINED,0.25,JarRBM PCD A B
THE LOW-RANK RBM PRETRAINED,0.2509727626459144,"−0.4
−0.2
0.0
PC1 0.0 2.5 5.0 7.5 10.0"
"DATASET
PCD
JAR-RBM",0.2519455252918288,"12.5
Dataset
PCD
Jar-RBM
pre-train+PCD"
"DATASET
PCD
JAR-RBM",0.2529182879377432,"−0.1
0.0
0.1
0.2
PC2 0 1 2 3 4 5"
"DATASET
PCD
JAR-RBM",0.2538910505836576,"−0.1
0.0
0.1
0.2
PC3 0 2 4 6"
"DATASET
PCD
JAR-RBM",0.25486381322957197,"101
103"
"DATASET
PCD
JAR-RBM",0.25583657587548636,Training time (epochs) −200 −150 −100 −50
"DATASET
PCD
JAR-RBM",0.25680933852140075,LL (nats)
"DATASET
PCD
JAR-RBM",0.25778210116731515,"PCD
JarRBM
RCM+PCD C"
"DATASET
PCD
JAR-RBM",0.2587548638132296,"Figure 3: We compare the samples generated by the 3 RBMs (JarRBM, PCD, pretrain+PCD) trained
with MNIST01 data. In A, we show the histograms of the generated data projected on the first, second
and third principal directions with those of the dataset. We see that only the pretrain+PCD correctly
balances the different modes. In B we show 10 images generated by each machine. In C, we compare
the log-likelihood of each model’s dataset as a function of training time. The dark and full curves
were obtained using the PTT algorithm discussed in section 5, and the lighter and dashed curves
using the AIS method [42]."
"DATASET
PCD
JAR-RBM",0.259727626459144,"call this the Parallel Trajectory Tempering (PTT) algorithm. Unlike the standard Parallel Tempering
230"
"DATASET
PCD
JAR-RBM",0.2607003891050584,"(PT) algorithm, which attempts swaps configurations between different temperatures, the PTT swaps
231"
"DATASET
PCD
JAR-RBM",0.2616731517509728,"between model parameters with different degrees of specialization. This approach is more natural
232"
"DATASET
PCD
JAR-RBM",0.26264591439688717,"for this problem because it involves crossing only second-order transitions, unlike the first-order
233"
"DATASET
PCD
JAR-RBM",0.26361867704280156,"transitions occurring in temperature annealing. And in fact, we show in Figs. 4–A and C, that this
234"
"DATASET
PCD
JAR-RBM",0.26459143968871596,"approach allows us to sharply accelerate the dynamics, as opposed to the standard PT algorithm
235"
"DATASET
PCD
JAR-RBM",0.26556420233463035,"(studied in detail for the MNIST dataset in [40]).
236"
"DATASET
PCD
JAR-RBM",0.26653696498054474,"In the PTT algorithm, the configurations x = (v, h) of neighboring machines indexed by t and t −1
237"
"DATASET
PCD
JAR-RBM",0.26750972762645914,"are interchanged with the probability
238"
"DATASET
PCD
JAR-RBM",0.26848249027237353,"pacc(xt ↔xt−1) = min
 
1, exp
 
∆Ht(xt) −∆Ht(xt−1)

."
"DATASET
PCD
JAR-RBM",0.2694552529182879,"This move satisfies detailed balance with our target equilibrium distribution p(x) = exp(−H(x))/Z,
239"
"DATASET
PCD
JAR-RBM",0.2704280155642023,"ensuring that the moves lead to the same equilibrium measure. As “nonspecialized"" models mix
240"
"DATASET
PCD
JAR-RBM",0.2714007782101167,"very quickly, either because the distribution is essentially Gaussian at the initialisation of a standard
241"
"DATASET
PCD
JAR-RBM",0.2723735408560311,"training, or because the low-rank RBM can be sampled with a static Monte Carlo process (yielding
242"
"DATASET
PCD
JAR-RBM",0.2733463035019455,"independent configurations each time), the trajectory flow significantly accelerates convergence
243"
"DATASET
PCD
JAR-RBM",0.27431906614785995,"to equilibrium. The time interval between successive machines is selected in such a way that the
244"
"DATASET
PCD
JAR-RBM",0.27529182879377434,"probability of accepting interchanges between neighboring machines remains around 0.3. Pre-trained
245"
"DATASET
PCD
JAR-RBM",0.27626459143968873,"machines require a significant fewer number of models to be effective, because most selected models
246"
"DATASET
PCD
JAR-RBM",0.2772373540856031,"are positioned at the most prominent phase transitions. We give the number of machines used for
247"
"DATASET
PCD
JAR-RBM",0.2782101167315175,"each sampling process in the SI. We also provide there a specific and detailed description of the
248"
"DATASET
PCD
JAR-RBM",0.2791828793774319,"algorithm used.
249"
"DATASET
PCD
JAR-RBM",0.2801556420233463,"In the red curves in Fig. (4)–B and D, we show the number of jumps between clusters as a function
250"
"DATASET
PCD
JAR-RBM",0.2811284046692607,"of the number of elementary MCMC steps, which in the PTT scheme refer to 1 Gibbs sampling step
251"
"DATASET
PCD
JAR-RBM",0.2821011673151751,"+ one swap proposal. For the DNA dataset, we have two measures corresponding to jumps along the
252"
"DATASET
PCD
JAR-RBM",0.2830739299610895,"two principal component directions. We observe at 104 MCMC steps an increase of the number of
253"
"DATASET
PCD
JAR-RBM",0.2840466926070039,"jumps by a factor of 80 for MNIST01 and by a factor of 1350 for the HGD in this machine, although
254 C)
D) A)
B)"
"DATASET
PCD
JAR-RBM",0.2850194552529183,"Figure 4: Comparison between PTT and classical Gibbs sampling for the MNIST01 dataset (A and
B, respectively) and the human genome dataset (C and D, respectively). In A and C, we show the
trajectory of two independent chains (red and orange) projected onto the PCA along the sampling
process of the pretraining+PCD model for 104 MCMC steps. The black contour represents the density
profile of the dataset and the position of the chains is plotted every 10 steps. In B and D we show the
average number of jumps from one cluster to another as a function of the MCMC steps performed.
The average is calculated over a population of 100 chains. In D, we show the average jump time
between clusters along the first (solid line) and second (dashed line) principal components of the data."
"DATASET
PCD
JAR-RBM",0.28599221789883267,"we achieve higher factors in other machines, as we show in the SI. The sampling of RBMs training
255"
"DATASET
PCD
JAR-RBM",0.28696498054474706,"on the MNIST01 dataset was the subject of the study of the “stacked tempering"" algorithm in [40].
256"
"DATASET
PCD
JAR-RBM",0.28793774319066145,"If we compare the numbers with their work, we see that we achieve a 3-4 times higher speedup factor,
257"
"DATASET
PCD
JAR-RBM",0.28891050583657585,"where our model has the advantage that it does not need additional training, but simply uses the stored
258"
"DATASET
PCD
JAR-RBM",0.2898832684824903,"machines correctly.
259"
"DATASET
PCD
JAR-RBM",0.2908560311284047,"Another desirable advantage of our PTT algorithm is that we can easily use it to compute an improved
260"
"DATASET
PCD
JAR-RBM",0.2918287937743191,"estimate of the AIS log-likelihood, except that in our case we consider the training trajectory instead
261"
"DATASET
PCD
JAR-RBM",0.2928015564202335,"of a cooling process and use the equilibrium samples obtained for each of the models to compute the
262"
"DATASET
PCD
JAR-RBM",0.29377431906614787,"model averages. In Figs. 3–C 5–A we compare the log-likelihood estimates obtained with our method
263"
"DATASET
PCD
JAR-RBM",0.29474708171206226,"(AIS-PTT) in full and dark lines and in light and dashed lines the AIS estimate (AIS). We see that
264"
"DATASET
PCD
JAR-RBM",0.29571984435797666,"both measures coincide for most parts of the training and that they split when the sampling becomes
265"
"DATASET
PCD
JAR-RBM",0.29669260700389105,"too long to thermalize along the temperature annealing curve in AIS. This effect is particularly evident
266"
"DATASET
PCD
JAR-RBM",0.29766536964980544,"for the JarRBM run in 5–A, where AIS takes a long time to recognize that the model suffers from a
267"
"DATASET
PCD
JAR-RBM",0.29863813229571984,"strong mode-collapse effect.
268"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.29961089494163423,"6
Overfitting and privacy loss as quality indicators
269"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3005836575875486,"In this section, we examine the quality of the samples generated, regarding overfitting and privacy
270"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.301556420233463,"criteria which have been defined for genomic data in particular. We look at this on the models trained
271"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3025291828793774,"with PCD with and without pre-training. We do not include the Jarzysnki method here, as this method
272"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3035019455252918,"fails to obtain a reliable model as clearly shown in the evolution of the Log-likelihood in Fig. 5. We
273"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3044747081712062,"focus on the human genome dataset, as shown in Fig. 1–C, to evaluate the ability of various state-
274"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.30544747081712065,"of-the-art generative models to generate realistic fake genomes while minimizing privacy concerns
275"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.30642023346303504,"(i.e., reducing overfitting). Recent studies [17, 18] have thoroughly investigated this for a variety of
276"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.30739299610894943,"generative models. Both studies concluded that the RBM was the most accurate method for generating
277"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.30836575875486383,"high-quality and private synthetic genomes. The comparison between models relies primarily on the
278"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3093385214007782,"Nearest Neighbor Adversarial Accuracy (AATS) and privacy loss indicators, introduced in Ref. [43],
279"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3103112840466926,"which quantify the similarity and the level of ""privacy"" of the data generated by a model w.r.t. the
280"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.311284046692607,training set. We have AATS = 1
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3122568093385214,"2
 
AATrue + AASynth

where AATrue [resp. AASynth] are two
281"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3132295719844358,"102
103
104"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3142023346303502,Training time (epochs) −500 −475 −450 −425 −400
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3151750972762646,LL (nats)
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.316147859922179,LL AIS on the HGD dataset
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.31712062256809337,"PCD
Jar-RBM
pre-train+PCD A"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.31809338521400776,"PCD
pre-train+PCD
Model 0.00 0.02 0.04 0.06 0.08"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.31906614785992216,Privacy Loss
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.32003891050583655,"PCD
pre-train+PCD
Model 0.00 0.25 0.50 0.75 1.00 Value"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.321011673151751,"AAtruth
AAsyn B
C"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3219844357976654,"Figure 5: We compare the quality of the RBMs trained with the human genome data (HGD). In A,
we show the log-likelihood as a function of the training epochs for the 3 training procedures. Solid
lines correspond to AIS-PTT and dashed lines to AIS. The JarRBM falls down because the training
breaks eventually. In B and C we compare privacy and overfitting based on the AATS indicator."
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3229571984435798,"quantities in [0, 1] obtained by merging two sets of real and synthetic data of equal size Ns and
282"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.3239299610894942,"measuring respectively the frequency that a real [rep. synthetic] has a synthetic [resp. real] as
283"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.32490272373540857,"nearest neighbor. If the generated samples are statistically indistinguishable from real samples, both
284"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.32587548638132297,"frequencies AATrue and AASynth should converge to 0.5 at large Ns. AATS can be evaluated both
285"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.32684824902723736,"with train or test samples and the privacy loss indicator is defined as Privacy loss = AAtest
TS −AAtrain
TS
286"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.32782101167315175,"and is expected to be strictly positive. Fig. 5 shows the comparison of AATS and privacy loss values
287"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.32879377431906615,"obtained with our two models, demonstrating that the pre-trained RBM clearly outperforms the
288"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.32976653696498054,"other model, and even achieves better results (AATS values much closer to 0.5) than those discussed
289"
OVERFITTING AND PRIVACY LOSS AS QUALITY INDICATORS,0.33073929961089493,"in [17, 18].
290"
CONCLUSIONS,0.3317120622568093,"7
Conclusions
291"
CONCLUSIONS,0.3326848249027237,"We have shown that the strategy of initiating the training on a pre-trained low-rank RBM is an
292"
CONCLUSIONS,0.3336575875486381,"extremely effective strategy to obtain high quality models for structured datasets that accurately
293"
CONCLUSIONS,0.3346303501945525,"represent all the modes in the datasets and with significantly higher log-likelihoods. We have also
294"
CONCLUSIONS,0.3356031128404669,"shown that the models obtained in that way are: (i) better generative models than those obtained
295"
CONCLUSIONS,0.33657587548638135,"with standard trainings, both, in the sense that they over-fit less at the same time they are more
296"
CONCLUSIONS,0.33754863813229574,"indistinguishable from the test samples, (ii) they display faster relaxational dynamics.
297"
CONCLUSIONS,0.33852140077821014,"We have also proposed a new fast sampling method that exploits the progressive learning of features
298"
CONCLUSIONS,0.33949416342412453,"in the training of RBMs to design an efficient trajectory PT strategy that allows accelerating the
299"
CONCLUSIONS,0.3404669260700389,"parallel Gibbs sampling dynamics by many orders of magnitude and overcome the performance
300"
CONCLUSIONS,0.3414396887159533,"of recent efficient sampling methods without adding any extra cost than saving models during the
301"
CONCLUSIONS,0.3424124513618677,"training.
302"
CONCLUSIONS,0.3433852140077821,"Both strategies for training and sampling are very general, and could be generalized to more complex
303"
CONCLUSIONS,0.3443579766536965,"EBMs. In this sense, the low-rank RBM model could be used as a more efficient pre-initialisation
304"
CONCLUSIONS,0.3453307392996109,"in deeper structures, and the trajectory PT algorithm is suitable to be directly used in any EBM no
305"
CONCLUSIONS,0.3463035019455253,"matter how complex it is.
306"
CODE AVAILABILITY,0.3472762645914397,"8
Code availability
307"
CODE AVAILABILITY,0.34824902723735407,"The code and datasets are available at https://github.com/nbereux/fast-RBM.
308"
REFERENCES,0.34922178988326846,"References
309"
REFERENCES,0.35019455252918286,"[1] Paul Smolensky. In Parallel Distributed Processing: Volume 1 by D. Rumelhart and J. McLel-
310"
REFERENCES,0.35116731517509725,"land, chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony
311"
REFERENCES,0.3521400778210117,"Theory. 194-281. MIT Press, 1986.
312"
REFERENCES,0.3531128404669261,"[2] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for
313"
REFERENCES,0.3540856031128405,"Boltzmann machines. Cognitive science, 9(1):147–169, 1985.
314"
REFERENCES,0.3550583657587549,"[3] Geoffrey E Hinton and Terrence J Sejnowski. Optimal perceptual inference. In Proceedings of
315"
REFERENCES,0.3560311284046693,"the IEEE conference on Computer Vision and Pattern Recognition, volume 448, pages 448–453.
316"
REFERENCES,0.35700389105058367,"Citeseer, 1983.
317"
REFERENCES,0.35797665369649806,"[4] H Chau Nguyen, Riccardo Zecchina, and Johannes Berg. Inverse statistical problems: from the
318"
REFERENCES,0.35894941634241245,"inverse ising problem to data science. Advances in Physics, 66(3):197–261, 2017.
319"
REFERENCES,0.35992217898832685,"[5] John Hertz, Yasser Roudi, and Joanna Tyrcha. Ising models for inferring network structure from
320"
REFERENCES,0.36089494163424124,"spike data. 2011.
321"
REFERENCES,0.36186770428015563,"[6] Simona Cocco, Christoph Feinauer, Matteo Figliuzzi, Rémi Monasson, and Martin Weigt.
322"
REFERENCES,0.36284046692607,"Inverse statistical physics of protein sequences: a key issues review. Reports on Progress in
323"
REFERENCES,0.3638132295719844,"Physics, 81(3):032601, 2018.
324"
REFERENCES,0.3647859922178988,"[7] Aurélien Decelle, Cyril Furtlehner, Alfonso de Jesús Navas Gómez, and Beatriz Seoane. In-
325"
REFERENCES,0.3657587548638132,"ferring effective couplings with restricted boltzmann machines. SciPost Physics, 16(4):095,
326"
REFERENCES,0.3667315175097276,"2024.
327"
REFERENCES,0.36770428015564205,"[8] Aurélien Decelle, Beatriz Seoane, and Lorenzo Rosset. Unsupervised hierarchical clustering
328"
REFERENCES,0.36867704280155644,"using the learning dynamics of restricted boltzmann machines. Phys. Rev. E, 108:014110, Jul
329"
REFERENCES,0.36964980544747084,"2023.
330"
REFERENCES,0.37062256809338523,"[9] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In
331"
REFERENCES,0.3715953307392996,"Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International
332"
REFERENCES,0.372568093385214,"Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research,
333"
REFERENCES,0.3735408560311284,"pages 2635–2644, New York, New York, USA, 20–22 Jun 2016. PMLR.
334"
REFERENCES,0.3745136186770428,"[10] Yang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint
335"
REFERENCES,0.3754863813229572,"arXiv:2101.03288, 2021.
336"
REFERENCES,0.3764591439688716,"[11] Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artificial
337"
REFERENCES,0.377431906614786,"neural networks. Science, 355(6325):602–606, 2017.
338"
REFERENCES,0.3784046692607004,"[12] Roger G Melko, Giuseppe Carleo, Juan Carrasquilla, and J Ignacio Cirac. Restricted boltzmann
339"
REFERENCES,0.37937743190661477,"machines in quantum physics. Nature Physics, 15(9):887–892, 2019.
340"
REFERENCES,0.38035019455252916,"[13] Koji Hashimoto, Sotaro Sugishita, Akinori Tanaka, and Akio Tomiya. Deep learning and the
341"
REFERENCES,0.38132295719844356,"ads/cft correspondence. Physical Review D, 98(4):046019, 2018.
342"
REFERENCES,0.38229571984435795,"[14] Koji Hashimoto. Ads/cft correspondence as a deep boltzmann machine. Physical Review D,
343"
REFERENCES,0.3832684824902724,"99(10):106017, 2019.
344"
REFERENCES,0.3842412451361868,"[15] Jérôme Tubiana, Simona Cocco, and Rémi Monasson. Learning protein constitutive motifs
345"
REFERENCES,0.3852140077821012,"from sequence data. Elife, 8:e39397, 2019.
346"
REFERENCES,0.3861867704280156,"[16] Juan Rodriguez-Rivas, Giancarlo Croce, Maureen Muscat, and Martin Weigt. Epistatic models
347"
REFERENCES,0.38715953307393,"predict mutable sites in sars-cov-2 proteins and epitopes. Proceedings of the National Academy
348"
REFERENCES,0.38813229571984437,"of Sciences, 119(4):e2113118119, 2022.
349"
REFERENCES,0.38910505836575876,"[17] Burak Yelmen, Aurélien Decelle, Linda Ongaro, Davide Marnetto, Corentin Tallec, Francesco
350"
REFERENCES,0.39007782101167315,"Montinaro, Cyril Furtlehner, Luca Pagani, and Flora Jay. Creating artificial human genomes
351"
REFERENCES,0.39105058365758755,"using generative neural networks. PLoS genetics, 17(2):e1009303, 2021.
352"
REFERENCES,0.39202334630350194,"[18] Burak Yelmen, Aurélien Decelle, Leila Lea Boulos, Antoine Szatkownik, Cyril Furtlehner,
353"
REFERENCES,0.39299610894941633,"Guillaume Charpiat, and Flora Jay. Deep convolutional and conditional neural networks for
354"
REFERENCES,0.3939688715953307,"large-scale genomic data generation. PLOS Computational Biology, 19(10):e1011584, 2023.
355"
REFERENCES,0.3949416342412451,"[19] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent
356"
REFERENCES,0.3959143968871595,"non-persistent short-run mcmc toward energy-based model. Advances in Neural Information
357"
REFERENCES,0.3968871595330739,"Processing Systems, 32, 2019.
358"
REFERENCES,0.3978599221789883,"[20] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy
359"
REFERENCES,0.39883268482490275,"of mcmc-based maximum likelihood learning of energy-based models. In Proceedings of the
360"
REFERENCES,0.39980544747081714,"AAAI Conference on Artificial Intelligence, volume 34, pages 5272–5280, 2020.
361"
REFERENCES,0.40077821011673154,"[21] Aurélien Decelle, Cyril Furtlehner, and Beatriz Seoane. Equilibrium and non-equilibrium
362"
REFERENCES,0.40175097276264593,"regimes in the learning of restricted boltzmann machines. Advances in Neural Information
363"
REFERENCES,0.4027237354085603,"Processing Systems, 34:5345–5359, 2021.
364"
REFERENCES,0.4036964980544747,"[22] Elisabeth Agoritsas, Giovanni Catania, Aurélien Decelle, and Beatriz Seoane. Explaining the
365"
REFERENCES,0.4046692607003891,"effects of non-convergent sampling in the training of energy-based models. arXiv preprint
366"
REFERENCES,0.4056420233463035,"arXiv:2301.09428, 2023.
367"
REFERENCES,0.4066147859922179,"[23] Alessandra Carbone, Aurélien Decelle, Lorenzo Rosset, and Beatriz Seoane.
Fast and
368"
REFERENCES,0.4075875486381323,"functional structured data generators rooted in out-of-equilibrium physics. arXiv preprint
369"
REFERENCES,0.4085603112840467,"arXiv:2307.06797, 2023.
370"
REFERENCES,0.4095330739299611,"[24] Renjie Liao, Simon Kornblith, Mengye Ren, David J Fleet, and Geoffrey Hinton. Gaussian-
371"
REFERENCES,0.41050583657587547,"bernoulli rbms without tears. arXiv preprint arXiv:2210.10318, 2022.
372"
REFERENCES,0.41147859922178986,"[25] Nicolas Béreux, Aurélien Decelle, Cyril Furtlehner, and Beatriz Seoane. Learning a restricted
373"
REFERENCES,0.41245136186770426,"boltzmann machine using biased monte carlo sampling. SciPost Physics, 14(3):032, 2023.
374"
REFERENCES,0.41342412451361865,"[26] A. Decelle, G. Fissore, and C. Furtlehner. Spectral dynamics of learning in restricted boltzmann
375"
REFERENCES,0.4143968871595331,"machines. Europhysics Letters, 119(6):60001, nov 2017.
376"
REFERENCES,0.4153696498054475,"[27] Aurélien Decelle, Giancarlo Fissore, and Cyril Furtlehner. Thermodynamics of restricted
377"
REFERENCES,0.4163424124513619,"boltzmann machines and related learning dynamics. Journal of Statistical Physics, 172:1576–
378"
REFERENCES,0.4173151750972763,"1608, 2018.
379"
REFERENCES,0.4182879377431907,"[28] A. Anonymous. Cascade of phase transitions in the training of energy-based models. arXiv:,
380"
REFERENCES,0.41926070038910507,"2024.
381"
REFERENCES,0.42023346303501946,"[29] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-
382"
REFERENCES,0.42120622568093385,"based learning. Predicting structured data, 1(0), 2006.
383"
REFERENCES,0.42217898832684825,"[30] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
384"
REFERENCES,0.42315175097276264,"computation, 14(8):1771–1800, 2002.
385"
REFERENCES,0.42412451361867703,"[31] Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
386"
REFERENCES,0.42509727626459143,"Proceedings of the 25th international conference on Machine learning, pages 872–879, 2008.
387"
REFERENCES,0.4260700389105058,"[32] Guillaume Desjardins, Aaron Courville, Yoshua Bengio, Pascal Vincent, and Olivier Delalleau.
388"
REFERENCES,0.4270428015564202,"Tempered markov chain monte carlo for training of restricted boltzmann machines. In Proceed-
389"
REFERENCES,0.4280155642023346,"ings of the thirteenth international conference on artificial intelligence and statistics, pages
390"
REFERENCES,0.428988326848249,"145–152. JMLR Workshop and Conference Proceedings, 2010.
391"
REFERENCES,0.42996108949416345,"[33] Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likeli-
392"
REFERENCES,0.43093385214007784,"hood gradient. In Proceedings of the 25th international conference on Machine learning, pages
393"
REFERENCES,0.43190661478599224,"1064–1071, 2008.
394"
REFERENCES,0.43287937743190663,"[34] Enzo Marinari and Giorgio Parisi. Simulated tempering: a new monte carlo scheme. Europhysics
395"
REFERENCES,0.433852140077821,"letters, 19(6):451, 1992.
396"
REFERENCES,0.4348249027237354,"[35] Russ R Salakhutdinov. Learning in markov random fields using tempered transitions. Advances
397"
REFERENCES,0.4357976653696498,"in neural information processing systems, 22, 2009.
398"
REFERENCES,0.4367704280155642,"[36] Oswin Krause, Asja Fischer, and Christian Igel. Population-contrastive-divergence: Does
399"
REFERENCES,0.4377431906614786,"consistency help with rbm training? Pattern Recognition Letters, 102:1–7, 2018.
400"
REFERENCES,0.438715953307393,"[37] Davide Carbone, Mengjian Hua, Simon Coste, and Eric Vanden-Eijnden. Efficient training of
401"
REFERENCES,0.4396887159533074,"energy-based models using jarzynski equality. Advances in Neural Information Processing
402"
REFERENCES,0.4406614785992218,"Systems, 36, 2024.
403"
REFERENCES,0.44163424124513617,"[38] E Nijkamp, R Gao, P Sountsov, S Vasudevan, B Pang, S-C Zhu, and YN Wu. Mcmc should
404"
REFERENCES,0.44260700389105057,"mix: learning energy-based model with neural transport latent space mcmc. In International
405"
REFERENCES,0.44357976653696496,"Conference on Learning Representations (ICLR 2022)., 2022.
406"
REFERENCES,0.44455252918287935,"[39] Louis Grenioux, Éric Moulines, and Marylou Gabrié. Balanced training of energy-based models
407"
REFERENCES,0.4455252918287938,"with adaptive flow sampling. In ICML 2023 Workshop on Structured Probabilistic Inference
408"
REFERENCES,0.4464980544747082,"and Generative Modeling, 2023.
409"
REFERENCES,0.4474708171206226,"[40] Clément Roussel, Jorge Fernandez-de Cossio-Diaz, Simona Cocco, and Remi Monasson.
410"
REFERENCES,0.448443579766537,"Accelerated sampling with stacked restricted boltzmann machines. In The Twelfth International
411"
REFERENCES,0.4494163424124514,"Conference on Learning Representations, 2023.
412"
REFERENCES,0.45038910505836577,"[41] Aurélien Decelle and Cyril Furtlehner. Exact training of restricted boltzmann machines on
413"
REFERENCES,0.45136186770428016,"intrinsically low dimensional data. Physical Review Letters, 127(15):158303, 2021.
414"
REFERENCES,0.45233463035019456,"[42] Oswin Krause, Asja Fischer, and Christian Igel. Algorithms for estimating the partition function
415"
REFERENCES,0.45330739299610895,"of restricted boltzmann machines. Artificial Intelligence, 278:103195, 2020.
416"
REFERENCES,0.45428015564202334,"[43] Andrew Yale, Saloni Dash, Ritik Dutta, Isabelle Guyon, Adrien Pavao, and Kristin P Ben-
417"
REFERENCES,0.45525291828793774,"nett. Generation and evaluation of privacy preserving synthetic health data. Neurocomputing,
418"
REFERENCES,0.45622568093385213,"416:244–255, 2020.
419"
REFERENCES,0.4571984435797665,"A
Details of the pre-training of a low rank RBM
420"
REFERENCES,0.4581712062256809,"A.1
The low-rank RBM and its sampling procedure
421"
REFERENCES,0.4591439688715953,"Our goal is to pre-train an RBM to directly encode the first d principal modes of the dataset in the
422"
REFERENCES,0.4601167315175097,"model’s coupling matrix. This approach avoids the standard procedure of progressively encoding
423"
REFERENCES,0.46108949416342415,"these modes through a series of second-order phase transitions, which negatively impact the quality
424"
REFERENCES,0.46206225680933855,"of gradient estimates during standard training. It also helps prevent critical relaxation slowdown of
425"
REFERENCES,0.46303501945525294,"MCMC dynamics in the presence of many separated clusters.
426"
REFERENCES,0.46400778210116733,"Given a dataset, we want to find a good set of model parameters (w, θ and η) for which the statistics
427"
REFERENCES,0.4649805447470817,"of the generated samples exactly match the statistics of the data projected onto the first d directions
428"
REFERENCES,0.4659533073929961,"of the PCA decomposition of the training set. Let us call each of these α = 1, . . . , d projections
429"
REFERENCES,0.4669260700389105,"mα = uα · v/√Nv the magnetizations along the mode α, where uα is the α-th mode of the PCA
430"
REFERENCES,0.4678988326848249,"decomposition of the dataset. A simple way to encode these d-modes is to parameterize the w-matrix
431"
REFERENCES,0.4688715953307393,"as:
432 w = d
X"
REFERENCES,0.4698443579766537,"α=1
wα ¯uαu⊤
α ,
with
(uα, ¯uα) ∈RNv × RNh,
(5)"
REFERENCES,0.4708171206225681,"where u and ˆu are respectively the right-hand and left-hand singular vectors of w, the former being
433"
REFERENCES,0.4717898832684825,"directly given by the PCA, while wα are the singular values of w. Using this decomposition, the
434"
REFERENCES,0.4727626459143969,"marginal energy on the visible variables, H(v) = log P"
REFERENCES,0.47373540856031127,"h exp H(v, h) can be rewritten in terms of
435"
REFERENCES,0.47470817120622566,"these magnetizations m ≡(m1, . . . , md)
436"
REFERENCES,0.47568093385214005,"H(v) = −
X"
REFERENCES,0.4766536964980545,"a
log cosh p Nv¯ua d
X"
REFERENCES,0.4776264591439689,"α=1
wαmα + ηa !"
REFERENCES,0.4785992217898833,"= H(m(v)).
(6)"
REFERENCES,0.4795719844357977,"Now, the goal of our pre-training is not to match the entire statistics of the data set, but only the
437"
REFERENCES,0.4805447470817121,"marginal probability of these magnetizations. In other words, we want to model the marginal
438"
REFERENCES,0.48151750972762647,"distribution
439"
REFERENCES,0.48249027237354086,"pemp(m) ≡
X"
REFERENCES,0.48346303501945526,"v
pemp(v) d
Y"
REFERENCES,0.48443579766536965,"α=1
δ

mα −
1
√Nv
uT
αv

,
(7)"
REFERENCES,0.48540856031128404,"where δ is the Dirac δ-distristribution. In this formulation, the distribution of the model over the
440"
REFERENCES,0.48638132295719844,"magnetization m can be easily characterized
441"
REFERENCES,0.48735408560311283,p(m) = 1 Z X
REFERENCES,0.4883268482490272,"v
e−H(v)
d
Y"
REFERENCES,0.4892996108949416,"α=1
δ

mα −
1
√Nv
uT
αv

(8) = 1"
REFERENCES,0.490272373540856,"Z N(m) exp
X"
REFERENCES,0.4912451361867704,"a
log cosh  ¯ua d
X"
REFERENCES,0.49221789883268485,"α=1
wαmα + ηa ! (9) = 1"
REFERENCES,0.49319066147859925,Z e−H(m)+Nvs(m) = 1
REFERENCES,0.49416342412451364,"Z e−Nvf(m)
(10)"
REFERENCES,0.49513618677042803,where N(m) = P
REFERENCES,0.4961089494163424,"v
Qd
α=1 δ

mα −
1
√Nv uT
αv

is the number of configurations with magnetiza-
442"
REFERENCES,0.4970817120622568,"tions m, and thus S(m) = log N(m)/Nv is the associated entropy. Now, for large Nv the entropic
443"
REFERENCES,0.4980544747081712,"term can be determined using large deviation theory, and in particular the Gärtner-Ellis theorem:
444"
REFERENCES,0.4990272373540856,pprior(m) = eNvs(m)
NV,0.5,"2Nv
≈exp (−NvI(m)) ,
(11)"
NV,0.5009727626459144,"with the rate function
445"
NV,0.5019455252918288,"I(m) = sup
µ"
NV,0.5029182879377432,"
mT µ −ϕ(µ)

= mT µ∗−ϕ(µ∗),
(12)"
NV,0.5038910505836576,"and
446"
NV,0.504863813229572,"ϕ(µ) =
lim
Nv→∞
1
Nv
log
D
eNvmT µE
=
lim
Nv→∞
1
Nv
log
1
2Nv
X"
NV,0.5058365758754864,"v
e
√Nv
Pd
α=1 µα
P"
NV,0.5068093385214008,"i uα,ivi
(13)"
NV,0.5077821011673151,"=
lim
Nv→∞
1
Nv Nv
X"
NV,0.5087548638132295,"i=1
log cosh p Nv d
X"
NV,0.5097276264591439,"α=1
µαuα,i !"
NV,0.5107003891050583,".
(14)"
NV,0.5116731517509727,"Then, given a magnetization m, we can compute the minimizer µ∗(m) of ϕ(µ) −mT µ which is
447"
NV,0.5126459143968871,"convex, using e.g. Newton method which converge really fast since we are in small dimension. Note
448"
NV,0.5136186770428015,"that in practice we will obviously use finite estimates of ϕ, assuming Nv is large enough. As a result
449"
NV,0.5145914396887159,"we get µ∗(m) satisfying implicit equations given by the constraints given at given Nv:
450"
NV,0.5155642023346303,"mα =
1
√Nv Nv
X"
NV,0.5165369649805448,"i=1
uα
i tanh  p Nv d
X"
NV,0.5175097276264592,"β=1
uβ
i µ∗
β "
NV,0.5184824902723736,".
(15)"
NV,0.519455252918288,"It is then straightforward to check that spins distributed as
451"
NV,0.5204280155642024,"pprior(v|m) ∝eNvµ∗T m(v)
(16)"
NV,0.5214007782101168,"fulfill well the requirement, as

uT
αv/√Nv"
NV,0.5223735408560312,"pprior = mα. In other words, we can generate samples
452"
NV,0.5233463035019456,"having mean magnetization mα just by choosing vi as
453"
NV,0.52431906614786,pprior(vi = 1|m) = sigmoid 
P,0.5252918287937743,"2
p Nv d
X"
P,0.5262645914396887,"α=1
uα,iµ∗
α(m) ! (17)"
P,0.5272373540856031,"The training can therefore be done directly in the subspace of dimension d. In Ref. [41], it has been
454"
P,0.5282101167315175,"shown that such RBM can be trained by mean of the Restricted Coulomb Machine, where the gradient
455"
P,0.5291828793774319,"is actually convex in the parameter’s space. It is then possible to do a mapping from the RCM to
456"
P,0.5301556420233463,"the RBM to recover the RBM’s parameters. In brief, the training of the low-dimensional RBM is
457"
P,0.5311284046692607,"performed by the RCM, and then the parameters are obtrained via a direct relation between the RCM
458"
P,0.5321011673151751,"and the RBM’s parameters. The detail of the definition and of the training of the RCM is detailed in
459"
P,0.5330739299610895,"the appendix A.2.
460"
P,0.5340466926070039,"A.2
The Restricted Coulomb Machine
461"
P,0.5350194552529183,"As introduced in [41], it is possible to exactly train a surrogate model for the RBM, called the
462"
P,0.5359922178988327,"Restricted Coulomb Machine (RCM), on a low dimensional dataset without explicitly sampling the
463"
P,0.5369649805447471,"machine allowing to learn even heavily clustered datasets. We will briefly outline the main steps to
464"
P,0.5379377431906615,"train the RCM. A more detailed explanation can be found in Appendix A.2.
465"
P,0.5389105058365758,"The RCM is an approximation of the marginal distribution of the RBM with {−1, 1} binary variables:
466"
P,0.5398832684824902,"H(v) = −
X"
P,0.5408560311284046,"i
viθi −
X"
P,0.541828793774319,"a
log cosh X"
P,0.5428015564202334,"i
wiavi + ηa !"
P,0.5437743190661478,".
(18)"
P,0.5447470817120622,"We then project both the parameters and variables of the RBM on the first d principal components of
467"
P,0.5457198443579766,"the dataset:
468"
P,0.546692607003891,"mα :=
1
√Nv Nv
X"
P,0.5476653696498055,"i=1
siuiα,
wαa := Nv
X"
P,0.5486381322957199,"i=1
wiauiα,
θα :=
1
√Nv Nv
X"
P,0.5496108949416343,"i=1
θiuiα
(19)"
P,0.5505836575875487,"with α ∈{1, . . . , d} and v the projection matrix of the PCA. The projected distribution of the model
469"
P,0.5515564202334631,"is then given by
470"
P,0.5525291828793775,"pRBM(m) =
exp

Nv
h
S(m) + Pd
α=1 θαmα +
1
Nv
PNh
a=1 log cosh
√Nv
Pd
α=1 mαwαa + ηa
i"
P,0.5535019455252919,"Z
(20)
where we ignore the fluctuations related to the transverse directions and S[m] accounts for the
471"
P,0.5544747081712063,"non-uniform prior on m due to the projection of the uniform prior on s for the way to compute it.
472"
P,0.5554474708171206,"The RCM is then built by approximating
473"
P,0.556420233463035,"log cosh(x) ≃|x| −log 2,
(21)"
P,0.5573929961089494,"which is valid for x large enough. The probability of the RCM is thus given by:
474"
P,0.5583657587548638,"pRCM(m) =
exp

Nv
h
S(m) + Pd
α=1 θαmα + PNh
a=1 qa
Pd
α=1 nαmα + za

i"
P,0.5593385214007782,"Z
(22)"
P,0.5603112840466926,"where
475 qa ="
P,0.561284046692607,"v
u
u
tNv d
X"
P,0.5622568093385214,"α=1
w2αa,
na =
wαa
qPd
α=1 w2αa
,
za =
ηa
q"
P,0.5632295719844358,"Nv
Pd
α=1 w2αa
.
(23)"
P,0.5642023346303502,"This can be easily inverted as
476"
P,0.5651750972762646,"wαa =
1
√Nv
qana
and
ηa = qaza,"
P,0.566147859922179,"in order to obtain the RBM from the RCM. The model is then trained through log-likelihood
477"
P,0.5671206225680934,"maximization over its parameters. However, this objective is non-convex if all the parameters are
478"
P,0.5680933852140078,"trained through gradient ascent. To relax the problem, since we’re in low dimension, we can define a
479"
P,0.5690661478599222,"family of hyperplanes (n, z) covering the space and let the model only learn the weights of each to
480"
P,0.5700389105058365,"the hyperplane. We can then discard the ones with a weight low enough for the approximation (21) to
481"
P,0.5710116731517509,"be bad.
482"
P,0.5719844357976653,"The gradients are given by
483 ∂J(Θ)"
P,0.5729571984435797,"∂qa
= Em∼pD(m)

|nT
a m + za|

−Em∼pRCM(m)

|nT
a m + za|

,
(24) 484 ∂J(Θ)"
P,0.5739299610894941,"∂θα
= Em∼pD(m) [mα] −Em∼pRCM(m) [mα] .
(25)"
P,0.5749027237354085,"The positive term is straightforward to compute. For the negative term, we rely on a discretization of
485"
P,0.5758754863813229,"the longitudinal space to estimate the probability density of the model and compute the averages.
486 ... ... ... ... ... ... ..."
P,0.5768482490272373,Initialization
P,0.5778210116731517,Sampling
MCMC STEP,0.5787937743190662,1 mcmc step Swap
MCMC STEP,0.5797665369649806,"RCM sampling
(only pre-train+PCD)"
MCMC STEP,0.580739299610895,"1 mcmc step
(only JarRBM and PCD)"
MCMC STEP,0.5817120622568094,"Figure 6: Scheme of PTT. We Initialize the chains of the models by starting from a configuration x(0)
0
and passing it through the machines along the training trajectory, each time performing ˜k mcmc steps.
For pre-train+PCD, x(0)
0
is a sampling from the RCM, otherwise it is a uniform random initialization.
The sampling consists of alternating one mcmc step for each model with a swap attempt between
adjacent machines. For pre-train+PCD, at each step we sample a new independent configuration for
RBM0 using the RCM."
MCMC STEP,0.5826848249027238,"Model
Dataset
# of machines
Alg. # of steps
acc. factor @ 104 steps"
MCMC STEP,0.5836575875486382,"pre-train+PCD
MNIST01
6 (+1)
10000
80
JarJar
MNIST01
28
10000
50
PCD
MNIST01
13
10000
30
pre-train+PCD
Human Genome
6 (+1)
10000
1350
PCD
Human Genome
13
10000
7100
Table 1: Performance comparison of different models on various datasets for the sampling using
PTT versus Gibbs sampling for 104 mcmc steps. The acceleration factor is defined as the ratio
of the average number of jumps obtained until 104 steps between PTT and Gibbs sampling. For
pre-train+PCD, the RCM machine has not to be counted among the list of models (hence the +1)
because it is very fast to sample from."
MCMC STEP,0.5846303501945526,"B
Sampling via Parallel Tempering using the learning trajectory
487"
MCMC STEP,0.585603112840467,"Assuming we have successfully trained a robust equilibrium model, there remains the challenge of
488"
MCMC STEP,0.5865758754863813,"efficiently generating equilibrium configurations from this model. Although models trained at equi-
489"
MCMC STEP,0.5875486381322957,"librium exhibit faster and more ergodic dynamics compared to poorly trained models, the sampling
490"
MCMC STEP,0.5885214007782101,"time can still be excessively long when navigating a highly rugged data landscape. Consequently,
491"
MCMC STEP,0.5894941634241245,"we devised a novel method for sampling equilibrium configurations that draws inspiration from
492"
MCMC STEP,0.5904669260700389,"the well-established parallel tempering approach. In this traditional method, multiple simulations
493"
MCMC STEP,0.5914396887159533,"are conducted in parallel at various temperatures, and configurations are exchanged among them
494"
MCMC STEP,0.5924124513618677,"using the Metropolis rule. Unlike this conventional technique, our method involves simultaneously
495"
MCMC STEP,0.5933852140077821,"simulating different models that are selected from various points along the training trajectory. This
496"
MCMC STEP,0.5943579766536965,"approach is motivated by the perspective that learning represents an annealing process for the model,
497"
MCMC STEP,0.5953307392996109,"encountering second-order type phase transitions during training. In contrast, annealing related to
498"
MCMC STEP,0.5963035019455253,"temperature changes involves first-order phase transitions, making traditional parallel tempering less
499"
MCMC STEP,0.5972762645914397,"effective for sampling from clustered multimodal distributions.
500"
MCMC STEP,0.5982490272373541,"A sketch of the Parallel Trajectory Tempering (PTT) is represented in fig. 6. Specifically, we save
501"
MCMC STEP,0.5992217898832685,"tf models at checkpoints t = 1, . . . , tf along the training trajectory. We denote the Hamiltonian of
502"
MCMC STEP,0.6001945525291829,"the model at checkpoint t as Ht, and refer to the Hamiltonian of the RCM model as H0. We define
503"
MCMC STEP,0.6011673151750972,"GibbsSampling(H, x, k) as the operation of performing k Gibbs sampling updates using the model
504"
MCMC STEP,0.6021400778210116,"H starting from the state x. In all our sampling simulations we used k = 1.
505"
MCMC STEP,0.603112840466926,"The first step is to initialize the models’ configurations efficiently. This involves sampling N
506"
MCMC STEP,0.6040856031128404,"chains from the RCM model, x(0)
0
∼RCMSampling(H0), and then passing the chains through
507"
MCMC STEP,0.6050583657587548,"all the models from t = 1 to t = tf, performing k Gibbs steps at each stage: x(0)
t
∼
508"
MCMC STEP,0.6060311284046692,"GibbsSampling(Ht, x(0)
t−1, k).
509"
MCMC STEP,0.6070038910505836,"The sampling process proceeds in steps where we update the configuration of each model except H0
510"
MCMC STEP,0.607976653696498,"with k Gibbs steps, and sample a completely new configuration for the RCM model H0. Following
511"
MCMC STEP,0.6089494163424124,"this update step, we propose swapping chains between adjacent models with an acceptance probability
512"
MCMC STEP,0.6099221789883269,"given by:
513"
MCMC STEP,0.6108949416342413,"pacc(xt ↔xt−1) = min (1, exp (∆Ht(xt) −∆Ht(xt−1))) ,
(26)"
MCMC STEP,0.6118677042801557,"where ∆Ht(x) = Ht(x) −Ht−1(x).
514"
MCMC STEP,0.6128404669260701,"We continue alternating between the update step and the swap step until a total of Nmcmc steps is
515"
MCMC STEP,0.6138132295719845,"reached. The sampling procedure is illustrated in the following pseudo-code:
516"
MCMC STEP,0.6147859922178989,"Input: Set of models {Ht}, t = 0, . . . , tf, Number of Gibbs steps k, Number of MCMC steps
517"
MCMC STEP,0.6157587548638133,"Nmcmc
518"
MCMC STEP,0.6167315175097277,"Output: Configurations xt for t = 1, . . . , tf
519"
MCMC STEP,0.617704280155642,"Initialize: Sample N chains from the RCM model x(0)
0
∼RCMSampling(H0)
520"
MCMC STEP,0.6186770428015564,"for t = 1 to tf do
521"
MCMC STEP,0.6196498054474708,"x(0)
t
∼GibbsSampling(Ht, x(0)
t−1, ˜k)
522"
MCMC STEP,0.6206225680933852,"end for
523"
MCMC STEP,0.6215953307392996,"for n = 1 to ⌊Nmcmc/k⌋do
524"
MCMC STEP,0.622568093385214,"for t = 1 to tf do
525"
MCMC STEP,0.6235408560311284,"x(n)
t
∼GibbsSampling(Ht, x(n−1)
t
, k)
526"
MCMC STEP,0.6245136186770428,"end for
527"
MCMC STEP,0.6254863813229572,"Resample x(n)
0
∼RCMSampling(H0)
528"
MCMC STEP,0.6264591439688716,"for t = 1 to tf do
529"
MCMC STEP,0.627431906614786,"Compute acceptance probability
530"
MCMC STEP,0.6284046692607004,"pacc(x(n)
t
↔x(n)
t−1) = min

1, exp

∆Ht(x(n)
t
) −∆Ht(x(n)
t−1)
"
MCMC STEP,0.6293774319066148,"Swap x(n)
t
and x(n)
t−1 with probability pacc(x(n)
t
↔x(n)
t−1)
531"
MCMC STEP,0.6303501945525292,"end for
532"
MCMC STEP,0.6313229571984436,"end for
533"
MCMC STEP,0.632295719844358,"A comparison of performances between PTT and standard Gibbs sampling is reported in Tab. 1.
534"
MCMC STEP,0.6332684824902723,"C
Training details
535"
MCMC STEP,0.6342412451361867,"We describe in Tables 2 and 3 the datasets and hyperparameters used during training. The test set
536"
MCMC STEP,0.6352140077821011,was used to evaluate the metrics. All experiments were run on a RTX 4090 with an AMD Ryzen 9
MCMC STEP,0.6361867704280155,Table 2: Details of the datasets used during training.
MCMC STEP,0.6371595330739299,"Name
#Samples
#Dimensions
Train size
Test size"
MCMC STEP,0.6381322957198443,"Human Genome Dataset (HGD)
4500
805
60%
40%
MNIST-01
10 610
784
60%
40%
Mickey
16 000
1000
60%
40% 537"
MCMC STEP,0.6391050583657587,"5950X.
538"
MCMC STEP,0.6400778210116731,"D
Training of the RBM using the Jarzynski equation
539"
MCMC STEP,0.6410505836575876,"In this section, we describe a procedure similar to the one introduced in [37] for training the RBM
540"
MCMC STEP,0.642023346303502,"by leveraging the Jarzynki equation. In one of its formulations, the Jarzynski equation states that
541"
MCMC STEP,0.6429961089494164,"we can relate the ensemble average of an observable O with the average obtained through many
542 A)
B) C)
D)"
MCMC STEP,0.6439688715953308,"PCD
JarRBM"
MCMC STEP,0.6449416342412452,"Figure 7: Comparison between PTT and standard Gibbs Sampling for RBMs trained using PCD (A
and B) and JarRBM (C and D) on the MNIST01 dataset. A and C show the sampling trajectory of
two chains recorded every 10 steps for a total of 104 mcmc steps. B and D show the average number
of jumps of a population of 100 chains as a function of the sampling time. PCD A)
B)"
MCMC STEP,0.6459143968871596,"Figure 8: Comparison between PTT and standard Gibbs Sampling for RBMs trained using PCD on
the Human Genome dataset. The sampling has been performed under the same conditions of fig. 7."
MCMC STEP,0.646887159533074,"repetitions of an out-of-equilibrium dynamical process. If we consider the training trajectory of an
543"
MCMC STEP,0.6478599221789884,"RBM, p0 →p1 →· · · →pt−1 →pt, we can write
544"
MCMC STEP,0.6488326848249028,⟨O⟩t = Oe−Wt
MCMC STEP,0.6498054474708171,"traj
⟨e−Wt⟩traj
,
(27)"
MCMC STEP,0.6507782101167315,"where the average on the lhs is done over the last model pt, the averages on the rhs are taken
545"
MCMC STEP,0.6517509727626459,"across many different trajectory realizations and Wt is a trajectory-dependent importance factor. By
546"
MCMC STEP,0.6527237354085603,"all practical means, under the assumption of having quasi-adiabatic parameters updates, namely
547"
MCMC STEP,0.6536964980544747,"p(Θt−1 →Θt) = p(Θt →Θt−1), this means that we can assign to each Markov chain of the
548"
MCMC STEP,0.6546692607003891,"simulation x(r), r = 1, . . . , R, an importance weight given by:
549"
MCMC STEP,0.6556420233463035,"W (r)
t
= t
X"
MCMC STEP,0.6566147859922179,"τ=1
[Hτ(x(r)
τ−1) −Hτ−1(x(r)
τ−1)]
(28)"
MCMC STEP,0.6575875486381323,"and then compute the gradient of the log-likelihood by means of a weighted average over the chains:
550"
MCMC STEP,0.6585603112840467,"⟨O⟩t ≃
PR
r=1 O(x(r)) e−W (r)
t
PR
r=1 e−W (r)
t
.
(29)"
MCMC STEP,0.6595330739299611,Table 3: Hyperparameters used for the training of RBMs.
MCMC STEP,0.6605058365758755,"Name
Batch size
#Chains
#Epochs
Learning rate
#MCMC steps
#Hidden nodes HGD"
MCMC STEP,0.6614785992217899,"PCD
2000
2000
10 000
0.01
100
185
Jar-RBM
2000
10 000
10 000
0.01
100
185
Pre-train+PCD
2000
2000
10 000
0.01
100
185"
MCMC STEP,0.6624513618677043,MNIST-01
MCMC STEP,0.6634241245136187,"PCD
2000
2000
10 000
0.01
100
200
Jar-RBM
2000
10 000
10 000
0.01
100
200
Pre-train+PCD
2000
2000
10 000
0.01
100
200"
MCMC STEP,0.664396887159533,Mickey
MCMC STEP,0.6653696498054474,"PCD
2000
2000
10 000
0.01
100
100
Jar-RBM
2000
10 000
10 000
0.01
100
100
Pre-train+PCD
2000
2000
10 000
0.01
100
100"
MCMC STEP,0.6663424124513618,"Notice that, since Eq. (27) is an exact result, the importance weights should, in principle, eliminate
551"
MCMC STEP,0.6673151750972762,"the bias brought by the non-convergent chains used for approximating the log-likelihood gradient in
552"
MCMC STEP,0.6682879377431906,"the classical PCD scheme. However, after many updates of the importance weights, one finds that
553"
MCMC STEP,0.669260700389105,"only a few chains carry almost all the importance mass. In other words, the vast majority of the chains
554"
MCMC STEP,0.6702334630350194,"we are simulating are statistically irrelevant, and we expect to get large fluctuations in the estimate of
555"
MCMC STEP,0.6712062256809338,"the gradient because of the small effective number of chains contributing to the statistical average. A
556"
MCMC STEP,0.6721789883268483,"good observable for monitoring this effect is the Effective Sample Size (ESS), defined as [37]
557 ESS ="
MCMC STEP,0.6731517509727627,"
R−1 PR
r=1 e−W (r)2"
MCMC STEP,0.6741245136186771,"R−1 PR
r=1 e−2W (r)
∈[0, 1],
(30)"
MCMC STEP,0.6750972762645915,"which measures the relative dispersion of the weights distribution. A way of circumventing the weight
558"
MCMC STEP,0.6760700389105059,"concentration on a few chains, then, is to resample the chain population according to the importance
559"
MCMC STEP,0.6770428015564203,"weights every time the ESS drops below a certain threshold, for instance 0.5. After this resampling,
560"
MCMC STEP,0.6780155642023347,"all the chain weights have to be set to 1 (W (r) = 0 ∀r = 1, . . . , R).
561"
MCMC STEP,0.6789883268482491,"NeurIPS Paper Checklist
562"
MCMC STEP,0.6799610894941635,"The checklist is designed to encourage best practices for responsible machine learning research,
563"
MCMC STEP,0.6809338521400778,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
564"
MCMC STEP,0.6819066147859922,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
565"
MCMC STEP,0.6828793774319066,"follow the references and precede the (optional) supplemental material. The checklist does NOT
566"
MCMC STEP,0.683852140077821,"count towards the page limit.
567"
MCMC STEP,0.6848249027237354,"Please read the checklist guidelines carefully for information on how to answer these questions. For
568"
MCMC STEP,0.6857976653696498,"each question in the checklist:
569"
MCMC STEP,0.6867704280155642,"• You should answer [Yes] , [No] , or [NA] .
570"
MCMC STEP,0.6877431906614786,"• [NA] means either that the question is Not Applicable for that particular paper or the
571"
MCMC STEP,0.688715953307393,"relevant information is Not Available.
572"
MCMC STEP,0.6896887159533074,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
573"
MCMC STEP,0.6906614785992218,"The checklist answers are an integral part of your paper submission. They are visible to the
574"
MCMC STEP,0.6916342412451362,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
575"
MCMC STEP,0.6926070038910506,"(after eventual revisions) with the final version of your paper, and its final version will be published
576"
MCMC STEP,0.693579766536965,"with the paper.
577"
MCMC STEP,0.6945525291828794,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
578"
MCMC STEP,0.6955252918287937,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
579"
MCMC STEP,0.6964980544747081,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
580"
MCMC STEP,0.6974708171206225,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
581"
MCMC STEP,0.6984435797665369,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
582"
MCMC STEP,0.6994163424124513,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
583"
MCMC STEP,0.7003891050583657,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
584"
MCMC STEP,0.7013618677042801,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
585"
MCMC STEP,0.7023346303501945,"please point to the section(s) where related material for the question can be found.
586"
CLAIMS,0.703307392996109,"1. Claims
587"
CLAIMS,0.7042801556420234,"Question: Do the main claims made in the abstract and introduction accurately reflect the
588"
CLAIMS,0.7052529182879378,"paper’s contributions and scope?
589"
CLAIMS,0.7062256809338522,"Answer: [Yes]
590"
CLAIMS,0.7071984435797666,"Justification:
591"
CLAIMS,0.708171206225681,"Guidelines:
592"
CLAIMS,0.7091439688715954,"• The answer NA means that the abstract and introduction do not include the claims
593"
CLAIMS,0.7101167315175098,"made in the paper.
594"
CLAIMS,0.7110894941634242,"• The abstract and/or introduction should clearly state the claims made, including the
595"
CLAIMS,0.7120622568093385,"contributions made in the paper and important assumptions and limitations. A No or
596"
CLAIMS,0.7130350194552529,"NA answer to this question will not be perceived well by the reviewers.
597"
CLAIMS,0.7140077821011673,"• The claims made should match theoretical and experimental results, and reflect how
598"
CLAIMS,0.7149805447470817,"much the results can be expected to generalize to other settings.
599"
CLAIMS,0.7159533073929961,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
600"
CLAIMS,0.7169260700389105,"are not attained by the paper.
601"
LIMITATIONS,0.7178988326848249,"2. Limitations
602"
LIMITATIONS,0.7188715953307393,"Question: Does the paper discuss the limitations of the work performed by the authors?
603"
LIMITATIONS,0.7198443579766537,"Answer: [Yes]
604"
LIMITATIONS,0.7208171206225681,"Justification:
605"
LIMITATIONS,0.7217898832684825,"Guidelines:
606"
LIMITATIONS,0.7227626459143969,"• The answer NA means that the paper has no limitation while the answer No means that
607"
LIMITATIONS,0.7237354085603113,"the paper has limitations, but those are not discussed in the paper.
608"
LIMITATIONS,0.7247081712062257,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
609"
LIMITATIONS,0.72568093385214,"• The paper should point out any strong assumptions and how robust the results are to
610"
LIMITATIONS,0.7266536964980544,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
611"
LIMITATIONS,0.7276264591439688,"model well-specification, asymptotic approximations only holding locally). The authors
612"
LIMITATIONS,0.7285992217898832,"should reflect on how these assumptions might be violated in practice and what the
613"
LIMITATIONS,0.7295719844357976,"implications would be.
614"
LIMITATIONS,0.730544747081712,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
615"
LIMITATIONS,0.7315175097276264,"only tested on a few datasets or with a few runs. In general, empirical results often
616"
LIMITATIONS,0.7324902723735408,"depend on implicit assumptions, which should be articulated.
617"
LIMITATIONS,0.7334630350194552,"• The authors should reflect on the factors that influence the performance of the approach.
618"
LIMITATIONS,0.7344357976653697,"For example, a facial recognition algorithm may perform poorly when image resolution
619"
LIMITATIONS,0.7354085603112841,"is low or images are taken in low lighting. Or a speech-to-text system might not be
620"
LIMITATIONS,0.7363813229571985,"used reliably to provide closed captions for online lectures because it fails to handle
621"
LIMITATIONS,0.7373540856031129,"technical jargon.
622"
LIMITATIONS,0.7383268482490273,"• The authors should discuss the computational efficiency of the proposed algorithms
623"
LIMITATIONS,0.7392996108949417,"and how they scale with dataset size.
624"
LIMITATIONS,0.7402723735408561,"• If applicable, the authors should discuss possible limitations of their approach to
625"
LIMITATIONS,0.7412451361867705,"address problems of privacy and fairness.
626"
LIMITATIONS,0.7422178988326849,"• While the authors might fear that complete honesty about limitations might be used by
627"
LIMITATIONS,0.7431906614785992,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
628"
LIMITATIONS,0.7441634241245136,"limitations that aren’t acknowledged in the paper. The authors should use their best
629"
LIMITATIONS,0.745136186770428,"judgment and recognize that individual actions in favor of transparency play an impor-
630"
LIMITATIONS,0.7461089494163424,"tant role in developing norms that preserve the integrity of the community. Reviewers
631"
LIMITATIONS,0.7470817120622568,"will be specifically instructed to not penalize honesty concerning limitations.
632"
THEORY ASSUMPTIONS AND PROOFS,0.7480544747081712,"3. Theory Assumptions and Proofs
633"
THEORY ASSUMPTIONS AND PROOFS,0.7490272373540856,"Question: For each theoretical result, does the paper provide the full set of assumptions and
634"
THEORY ASSUMPTIONS AND PROOFS,0.75,"a complete (and correct) proof?
635"
THEORY ASSUMPTIONS AND PROOFS,0.7509727626459144,"Answer: [NA]
636"
THEORY ASSUMPTIONS AND PROOFS,0.7519455252918288,"Justification:
637"
THEORY ASSUMPTIONS AND PROOFS,0.7529182879377432,"Guidelines:
638"
THEORY ASSUMPTIONS AND PROOFS,0.7538910505836576,"• The answer NA means that the paper does not include theoretical results.
639"
THEORY ASSUMPTIONS AND PROOFS,0.754863813229572,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
640"
THEORY ASSUMPTIONS AND PROOFS,0.7558365758754864,"referenced.
641"
THEORY ASSUMPTIONS AND PROOFS,0.7568093385214008,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
642"
THEORY ASSUMPTIONS AND PROOFS,0.7577821011673151,"• The proofs can either appear in the main paper or the supplemental material, but if
643"
THEORY ASSUMPTIONS AND PROOFS,0.7587548638132295,"they appear in the supplemental material, the authors are encouraged to provide a short
644"
THEORY ASSUMPTIONS AND PROOFS,0.7597276264591439,"proof sketch to provide intuition.
645"
THEORY ASSUMPTIONS AND PROOFS,0.7607003891050583,"• Inversely, any informal proof provided in the core of the paper should be complemented
646"
THEORY ASSUMPTIONS AND PROOFS,0.7616731517509727,"by formal proofs provided in appendix or supplemental material.
647"
THEORY ASSUMPTIONS AND PROOFS,0.7626459143968871,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7636186770428015,"4. Experimental Result Reproducibility
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7645914396887159,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7655642023346303,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7665369649805448,"of the paper (regardless of whether the code and data are provided or not)?
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7675097276264592,"Answer: [Yes]
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7684824902723736,"Justification:
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.769455252918288,"Guidelines:
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7704280155642024,"• The answer NA means that the paper does not include experiments.
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714007782101168,"• If the paper includes experiments, a No answer to this question will not be perceived
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723735408560312,"well by the reviewers: Making the paper reproducible is important, regardless of
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733463035019456,"whether the code and data are provided or not.
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.77431906614786,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7752918287937743,"to make their results reproducible or verifiable.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7762645914396887,"• Depending on the contribution, reproducibility can be accomplished in various ways.
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7772373540856031,"For example, if the contribution is a novel architecture, describing the architecture fully
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7782101167315175,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7791828793774319,"be necessary to either make it possible for others to replicate the model with the same
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7801556420233463,"dataset, or provide access to the model. In general. releasing code and data is often
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7811284046692607,"one good way to accomplish this, but reproducibility can also be provided via detailed
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7821011673151751,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7830739299610895,"of a large language model), releasing of a model checkpoint, or other means that are
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7840466926070039,"appropriate to the research performed.
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7850194552529183,"• While NeurIPS does not require releasing code, the conference does require all submis-
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7859922178988327,"sions to provide some reasonable avenue for reproducibility, which may depend on the
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7869649805447471,"nature of the contribution. For example
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7879377431906615,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7889105058365758,"to reproduce that algorithm.
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7898832684824902,"(b) If the contribution is primarily a new model architecture, the paper should describe
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7908560311284046,"the architecture clearly and fully.
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.791828793774319,"(c) If the contribution is a new model (e.g., a large language model), then there should
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7928015564202334,"either be a way to access this model for reproducing the results or a way to reproduce
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7937743190661478,"the model (e.g., with an open-source dataset or instructions for how to construct
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7947470817120622,"the dataset).
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7957198443579766,"(d) We recognize that reproducibility may be tricky in some cases, in which case
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.796692607003891,"authors are welcome to describe the particular way they provide for reproducibility.
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7976653696498055,"In the case of closed-source models, it may be that access to the model is limited in
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7986381322957199,"some way (e.g., to registered users), but it should be possible for other researchers
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7996108949416343,"to have some path to reproducing or verifying the results.
686"
OPEN ACCESS TO DATA AND CODE,0.8005836575875487,"5. Open access to data and code
687"
OPEN ACCESS TO DATA AND CODE,0.8015564202334631,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
688"
OPEN ACCESS TO DATA AND CODE,0.8025291828793775,"tions to faithfully reproduce the main experimental results, as described in supplemental
689"
OPEN ACCESS TO DATA AND CODE,0.8035019455252919,"material?
690"
OPEN ACCESS TO DATA AND CODE,0.8044747081712063,"Answer: [Yes]
691"
OPEN ACCESS TO DATA AND CODE,0.8054474708171206,"Justification:
692"
OPEN ACCESS TO DATA AND CODE,0.806420233463035,"Guidelines:
693"
OPEN ACCESS TO DATA AND CODE,0.8073929961089494,"• The answer NA means that paper does not include experiments requiring code.
694"
OPEN ACCESS TO DATA AND CODE,0.8083657587548638,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
695"
OPEN ACCESS TO DATA AND CODE,0.8093385214007782,"public/guides/CodeSubmissionPolicy) for more details.
696"
OPEN ACCESS TO DATA AND CODE,0.8103112840466926,"• While we encourage the release of code and data, we understand that this might not be
697"
OPEN ACCESS TO DATA AND CODE,0.811284046692607,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
698"
OPEN ACCESS TO DATA AND CODE,0.8122568093385214,"including code, unless this is central to the contribution (e.g., for a new open-source
699"
OPEN ACCESS TO DATA AND CODE,0.8132295719844358,"benchmark).
700"
OPEN ACCESS TO DATA AND CODE,0.8142023346303502,"• The instructions should contain the exact command and environment needed to run to
701"
OPEN ACCESS TO DATA AND CODE,0.8151750972762646,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
702"
OPEN ACCESS TO DATA AND CODE,0.816147859922179,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
703"
OPEN ACCESS TO DATA AND CODE,0.8171206225680934,"• The authors should provide instructions on data access and preparation, including how
704"
OPEN ACCESS TO DATA AND CODE,0.8180933852140078,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
705"
OPEN ACCESS TO DATA AND CODE,0.8190661478599222,"• The authors should provide scripts to reproduce all experimental results for the new
706"
OPEN ACCESS TO DATA AND CODE,0.8200389105058365,"proposed method and baselines. If only a subset of experiments are reproducible, they
707"
OPEN ACCESS TO DATA AND CODE,0.8210116731517509,"should state which ones are omitted from the script and why.
708"
OPEN ACCESS TO DATA AND CODE,0.8219844357976653,"• At submission time, to preserve anonymity, the authors should release anonymized
709"
OPEN ACCESS TO DATA AND CODE,0.8229571984435797,"versions (if applicable).
710"
OPEN ACCESS TO DATA AND CODE,0.8239299610894941,"• Providing as much information as possible in supplemental material (appended to the
711"
OPEN ACCESS TO DATA AND CODE,0.8249027237354085,"paper) is recommended, but including URLs to data and code is permitted.
712"
OPEN ACCESS TO DATA AND CODE,0.8258754863813229,"6. Experimental Setting/Details
713"
OPEN ACCESS TO DATA AND CODE,0.8268482490272373,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
714"
OPEN ACCESS TO DATA AND CODE,0.8278210116731517,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
715"
OPEN ACCESS TO DATA AND CODE,0.8287937743190662,"results?
716"
OPEN ACCESS TO DATA AND CODE,0.8297665369649806,"Answer: [Yes]
717"
OPEN ACCESS TO DATA AND CODE,0.830739299610895,"Justification:
718"
OPEN ACCESS TO DATA AND CODE,0.8317120622568094,"Guidelines:
719"
OPEN ACCESS TO DATA AND CODE,0.8326848249027238,"• The answer NA means that the paper does not include experiments.
720"
OPEN ACCESS TO DATA AND CODE,0.8336575875486382,"• The experimental setting should be presented in the core of the paper to a level of detail
721"
OPEN ACCESS TO DATA AND CODE,0.8346303501945526,"that is necessary to appreciate the results and make sense of them.
722"
OPEN ACCESS TO DATA AND CODE,0.835603112840467,"• The full details can be provided either with the code, in appendix, or as supplemental
723"
OPEN ACCESS TO DATA AND CODE,0.8365758754863813,"material.
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8375486381322957,"7. Experiment Statistical Significance
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8385214007782101,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8394941634241245,"information about the statistical significance of the experiments?
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8404669260700389,"Answer: [No]
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8414396887159533,"Justification:
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8424124513618677,"Guidelines:
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8433852140077821,"• The answer NA means that the paper does not include experiments.
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8443579766536965,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8453307392996109,"dence intervals, or statistical significance tests, at least for the experiments that support
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8463035019455253,"the main claims of the paper.
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8472762645914397,"• The factors of variability that the error bars are capturing should be clearly stated (for
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8482490272373541,"example, train/test split, initialization, random drawing of some parameter, or overall
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8492217898832685,"run with given experimental conditions).
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8501945525291829,"• The method for calculating the error bars should be explained (closed form formula,
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8511673151750972,"call to a library function, bootstrap, etc.)
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8521400778210116,"• The assumptions made should be given (e.g., Normally distributed errors).
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.853112840466926,"• It should be clear whether the error bar is the standard deviation or the standard error
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8540856031128404,"of the mean.
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8550583657587548,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8560311284046692,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8570038910505836,"of Normality of errors is not verified.
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.857976653696498,"• For asymmetric distributions, the authors should be careful not to show in tables or
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8589494163424124,"figures symmetric error bars that would yield results that are out of range (e.g. negative
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8599221789883269,"error rates).
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8608949416342413,"• If error bars are reported in tables or plots, The authors should explain in the text how
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8618677042801557,"they were calculated and reference the corresponding figures or tables in the text.
750"
EXPERIMENTS COMPUTE RESOURCES,0.8628404669260701,"8. Experiments Compute Resources
751"
EXPERIMENTS COMPUTE RESOURCES,0.8638132295719845,"Question: For each experiment, does the paper provide sufficient information on the com-
752"
EXPERIMENTS COMPUTE RESOURCES,0.8647859922178989,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
753"
EXPERIMENTS COMPUTE RESOURCES,0.8657587548638133,"the experiments?
754"
EXPERIMENTS COMPUTE RESOURCES,0.8667315175097277,"Answer: [Yes]
755"
EXPERIMENTS COMPUTE RESOURCES,0.867704280155642,"Justification:
756"
EXPERIMENTS COMPUTE RESOURCES,0.8686770428015564,"Guidelines:
757"
EXPERIMENTS COMPUTE RESOURCES,0.8696498054474708,"• The answer NA means that the paper does not include experiments.
758"
EXPERIMENTS COMPUTE RESOURCES,0.8706225680933852,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
759"
EXPERIMENTS COMPUTE RESOURCES,0.8715953307392996,"or cloud provider, including relevant memory and storage.
760"
EXPERIMENTS COMPUTE RESOURCES,0.872568093385214,"• The paper should provide the amount of compute required for each of the individual
761"
EXPERIMENTS COMPUTE RESOURCES,0.8735408560311284,"experimental runs as well as estimate the total compute.
762"
EXPERIMENTS COMPUTE RESOURCES,0.8745136186770428,"• The paper should disclose whether the full research project required more compute
763"
EXPERIMENTS COMPUTE RESOURCES,0.8754863813229572,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
764"
EXPERIMENTS COMPUTE RESOURCES,0.8764591439688716,"didn’t make it into the paper).
765"
CODE OF ETHICS,0.877431906614786,"9. Code Of Ethics
766"
CODE OF ETHICS,0.8784046692607004,"Question: Does the research conducted in the paper conform, in every respect, with the
767"
CODE OF ETHICS,0.8793774319066148,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
768"
CODE OF ETHICS,0.8803501945525292,"Answer: [Yes]
769"
CODE OF ETHICS,0.8813229571984436,"Justification:
770"
CODE OF ETHICS,0.882295719844358,"Guidelines:
771"
CODE OF ETHICS,0.8832684824902723,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
772"
CODE OF ETHICS,0.8842412451361867,"• If the authors answer No, they should explain the special circumstances that require a
773"
CODE OF ETHICS,0.8852140077821011,"deviation from the Code of Ethics.
774"
CODE OF ETHICS,0.8861867704280155,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
775"
CODE OF ETHICS,0.8871595330739299,"eration due to laws or regulations in their jurisdiction).
776"
BROADER IMPACTS,0.8881322957198443,"10. Broader Impacts
777"
BROADER IMPACTS,0.8891050583657587,"Question: Does the paper discuss both potential positive societal impacts and negative
778"
BROADER IMPACTS,0.8900778210116731,"societal impacts of the work performed?
779"
BROADER IMPACTS,0.8910505836575876,"Answer: [NA]
780"
BROADER IMPACTS,0.892023346303502,"Justification:
781"
BROADER IMPACTS,0.8929961089494164,"Guidelines:
782"
BROADER IMPACTS,0.8939688715953308,"• The answer NA means that there is no societal impact of the work performed.
783"
BROADER IMPACTS,0.8949416342412452,"• If the authors answer NA or No, they should explain why their work has no societal
784"
BROADER IMPACTS,0.8959143968871596,"impact or why the paper does not address societal impact.
785"
BROADER IMPACTS,0.896887159533074,"• Examples of negative societal impacts include potential malicious or unintended uses
786"
BROADER IMPACTS,0.8978599221789884,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
787"
BROADER IMPACTS,0.8988326848249028,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
788"
BROADER IMPACTS,0.8998054474708171,"groups), privacy considerations, and security considerations.
789"
BROADER IMPACTS,0.9007782101167315,"• The conference expects that many papers will be foundational research and not tied
790"
BROADER IMPACTS,0.9017509727626459,"to particular applications, let alone deployments. However, if there is a direct path to
791"
BROADER IMPACTS,0.9027237354085603,"any negative applications, the authors should point it out. For example, it is legitimate
792"
BROADER IMPACTS,0.9036964980544747,"to point out that an improvement in the quality of generative models could be used to
793"
BROADER IMPACTS,0.9046692607003891,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
794"
BROADER IMPACTS,0.9056420233463035,"that a generic algorithm for optimizing neural networks could enable people to train
795"
BROADER IMPACTS,0.9066147859922179,"models that generate Deepfakes faster.
796"
BROADER IMPACTS,0.9075875486381323,"• The authors should consider possible harms that could arise when the technology is
797"
BROADER IMPACTS,0.9085603112840467,"being used as intended and functioning correctly, harms that could arise when the
798"
BROADER IMPACTS,0.9095330739299611,"technology is being used as intended but gives incorrect results, and harms following
799"
BROADER IMPACTS,0.9105058365758755,"from (intentional or unintentional) misuse of the technology.
800"
BROADER IMPACTS,0.9114785992217899,"• If there are negative societal impacts, the authors could also discuss possible mitigation
801"
BROADER IMPACTS,0.9124513618677043,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
802"
BROADER IMPACTS,0.9134241245136187,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
803"
BROADER IMPACTS,0.914396887159533,"feedback over time, improving the efficiency and accessibility of ML).
804"
SAFEGUARDS,0.9153696498054474,"11. Safeguards
805"
SAFEGUARDS,0.9163424124513618,"Question: Does the paper describe safeguards that have been put in place for responsible
806"
SAFEGUARDS,0.9173151750972762,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
807"
SAFEGUARDS,0.9182879377431906,"image generators, or scraped datasets)?
808"
SAFEGUARDS,0.919260700389105,"Answer: [NA]
809"
SAFEGUARDS,0.9202334630350194,"Justification:
810"
SAFEGUARDS,0.9212062256809338,"Guidelines:
811"
SAFEGUARDS,0.9221789883268483,"• The answer NA means that the paper poses no such risks.
812"
SAFEGUARDS,0.9231517509727627,"• Released models that have a high risk for misuse or dual-use should be released with
813"
SAFEGUARDS,0.9241245136186771,"necessary safeguards to allow for controlled use of the model, for example by requiring
814"
SAFEGUARDS,0.9250972762645915,"that users adhere to usage guidelines or restrictions to access the model or implementing
815"
SAFEGUARDS,0.9260700389105059,"safety filters.
816"
SAFEGUARDS,0.9270428015564203,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
817"
SAFEGUARDS,0.9280155642023347,"should describe how they avoided releasing unsafe images.
818"
SAFEGUARDS,0.9289883268482491,"• We recognize that providing effective safeguards is challenging, and many papers do
819"
SAFEGUARDS,0.9299610894941635,"not require this, but we encourage authors to take this into account and make a best
820"
SAFEGUARDS,0.9309338521400778,"faith effort.
821"
LICENSES FOR EXISTING ASSETS,0.9319066147859922,"12. Licenses for existing assets
822"
LICENSES FOR EXISTING ASSETS,0.9328793774319066,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
823"
LICENSES FOR EXISTING ASSETS,0.933852140077821,"the paper, properly credited and are the license and terms of use explicitly mentioned and
824"
LICENSES FOR EXISTING ASSETS,0.9348249027237354,"properly respected?
825"
LICENSES FOR EXISTING ASSETS,0.9357976653696498,"Answer: [NA]
826"
LICENSES FOR EXISTING ASSETS,0.9367704280155642,"Justification:
827"
LICENSES FOR EXISTING ASSETS,0.9377431906614786,"Guidelines:
828"
LICENSES FOR EXISTING ASSETS,0.938715953307393,"• The answer NA means that the paper does not use existing assets.
829"
LICENSES FOR EXISTING ASSETS,0.9396887159533074,"• The authors should cite the original paper that produced the code package or dataset.
830"
LICENSES FOR EXISTING ASSETS,0.9406614785992218,"• The authors should state which version of the asset is used and, if possible, include a
831"
LICENSES FOR EXISTING ASSETS,0.9416342412451362,"URL.
832"
LICENSES FOR EXISTING ASSETS,0.9426070038910506,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
833"
LICENSES FOR EXISTING ASSETS,0.943579766536965,"• For scraped data from a particular source (e.g., website), the copyright and terms of
834"
LICENSES FOR EXISTING ASSETS,0.9445525291828794,"service of that source should be provided.
835"
LICENSES FOR EXISTING ASSETS,0.9455252918287937,"• If assets are released, the license, copyright information, and terms of use in the
836"
LICENSES FOR EXISTING ASSETS,0.9464980544747081,"package should be provided. For popular datasets, paperswithcode.com/datasets
837"
LICENSES FOR EXISTING ASSETS,0.9474708171206225,"has curated licenses for some datasets. Their licensing guide can help determine the
838"
LICENSES FOR EXISTING ASSETS,0.9484435797665369,"license of a dataset.
839"
LICENSES FOR EXISTING ASSETS,0.9494163424124513,"• For existing datasets that are re-packaged, both the original license and the license of
840"
LICENSES FOR EXISTING ASSETS,0.9503891050583657,"the derived asset (if it has changed) should be provided.
841"
LICENSES FOR EXISTING ASSETS,0.9513618677042801,"• If this information is not available online, the authors are encouraged to reach out to
842"
LICENSES FOR EXISTING ASSETS,0.9523346303501945,"the asset’s creators.
843"
NEW ASSETS,0.953307392996109,"13. New Assets
844"
NEW ASSETS,0.9542801556420234,"Question: Are new assets introduced in the paper well documented and is the documentation
845"
NEW ASSETS,0.9552529182879378,"provided alongside the assets?
846"
NEW ASSETS,0.9562256809338522,"Answer: [NA]
847"
NEW ASSETS,0.9571984435797666,"Justification:
848"
NEW ASSETS,0.958171206225681,"Guidelines:
849"
NEW ASSETS,0.9591439688715954,"• The answer NA means that the paper does not release new assets.
850"
NEW ASSETS,0.9601167315175098,"• Researchers should communicate the details of the dataset/code/model as part of their
851"
NEW ASSETS,0.9610894941634242,"submissions via structured templates. This includes details about training, license,
852"
NEW ASSETS,0.9620622568093385,"limitations, etc.
853"
NEW ASSETS,0.9630350194552529,"• The paper should discuss whether and how consent was obtained from people whose
854"
NEW ASSETS,0.9640077821011673,"asset is used.
855"
NEW ASSETS,0.9649805447470817,"• At submission time, remember to anonymize your assets (if applicable). You can either
856"
NEW ASSETS,0.9659533073929961,"create an anonymized URL or include an anonymized zip file.
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9669260700389105,"14. Crowdsourcing and Research with Human Subjects
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9678988326848249,"Question: For crowdsourcing experiments and research with human subjects, does the paper
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9688715953307393,"include the full text of instructions given to participants and screenshots, if applicable, as
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698443579766537,"well as details about compensation (if any)?
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708171206225681,"Answer: [No]
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717898832684825,"Justification:
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9727626459143969,"Guidelines:
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737354085603113,"• The answer NA means that the paper does not involve crowdsourcing nor research with
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747081712062257,"human subjects.
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97568093385214,"• Including this information in the supplemental material is fine, but if the main contribu-
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766536964980544,"tion of the paper involves human subjects, then as much detail as possible should be
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776264591439688,"included in the main paper.
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785992217898832,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795719844357976,"or other labor should be paid at least the minimum wage in the country of the data
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980544747081712,"collector.
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815175097276264,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824902723735408,"Subjects
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834630350194552,"Question: Does the paper describe potential risks incurred by study participants, whether
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844357976653697,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854085603112841,"approvals (or an equivalent approval/review based on the requirements of your country or
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863813229571985,"institution) were obtained?
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873540856031129,"Answer: [NA]
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883268482490273,"Justification:
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892996108949417,"Guidelines:
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902723735408561,"• The answer NA means that the paper does not involve crowdsourcing nor research with
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912451361867705,"human subjects.
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922178988326849,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931906614785992,"may be required for any human subjects research. If you obtained IRB approval, you
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941634241245136,"should clearly state this in the paper.
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995136186770428,"• We recognize that the procedures for this may vary significantly between institutions
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961089494163424,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970817120622568,"guidelines for their institution.
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980544747081712,"• For initial submissions, do not include any information that would break anonymity (if
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990272373540856,"applicable), such as the institution conducting the review.
891"
