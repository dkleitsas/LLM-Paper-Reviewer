Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001941747572815534,"Machine learning models that achieve high overall accuracy often make system-
atic errors on important subsets (or slices) of data. Identifying underperforming
slices is particularly challenging when working with high-dimensional inputs (e.g.
images, audio), where important slices are often unlabeled. In order to address this
issue, recent studies have proposed automated slice discovery methods (SDMs),
which leverage learned model representations to mine input data for slices on
which a model performs poorly. To be useful to a practitioner, these methods
must identify slices that are both underperforming and coherent (i.e. united by a
human-understandable concept). However, no quantitative evaluation framework
currently exists for rigorously assessing SDMs with respect to these criteria. Ad-
ditionally, prior qualitative evaluations have shown that SDMs often identify slices
that are incoherent. In this work, we address these challenges by ﬁrst designing a
principled evaluation framework that enables a quantitative comparison of SDMs
across 1,235 slice discovery settings in three input domains (natural images, med-
ical images, and time-series data). Then, motivated by the recent development
of powerful cross-modal representation learning approaches, we present Domino,
an SDM that leverages cross-modal embeddings and a novel error-aware mixture
model to discover and describe coherent slices. We ﬁnd that Domino accurately
identiﬁes 36% of the 1,235 slices in our framework – a 12 percentage point im-
provement over prior methods. Further, Domino is the ﬁrst SDM that can provide
natural language descriptions of identiﬁed slices, correctly generating the exact
name of the slice in 35% of settings."
INTRODUCTION,0.003883495145631068,"1
INTRODUCTION"
INTRODUCTION,0.005825242718446602,"Machine learning models often make systematic errors on important subsets (or slices) of data1.
For instance, models trained to detect collapsed lungs in chest X-rays have been shown to make
predictions based on the presence of chest drains, a device typically used during treatment (Oakden-
Rayner et al., 2019). As a result, these models frequently make prediction errors on cases without
chest drains, a critical data slice where false negative predictions could be life-threatening. Simi-
lar performance gaps across slices have been observed in radiograph classiﬁcation (Badgeley et al.,
2019; Zech et al., 2018; DeGrave et al., 2021), melanoma detection (Winkler et al., 2019), natu-
ral language processing (Orr et al., 2020; Goel et al., 2021), and object detection (de Vries et al.,
2019), among others. If underperforming slices can be accurately identiﬁed and labeled, we can
then improve model robustness by either updating the training dataset or using robust optimization
techniques (Zhang et al., 2018; Sagawa et al., 2020)."
INTRODUCTION,0.007766990291262136,"However, identifying underperforming slices is difﬁcult in practice. When working with high-
dimensional inputs (e.g. images, time-series data, video), slices are often “hidden”, meaning that
they cannot easily be extracted from the inputs and are not annotated in metadata (Oakden-Rayner
et al., 2019). For instance, in the collapsed lung example above, the absence of chest drains is
challenging to identify from raw image data and may not be explicitly labeled in metadata. In this
setting, we must perform slice discovery: the task of mining unstructured input data for semantically
meaningful subgroups on which the model performs poorly."
INTRODUCTION,0.009708737864077669,1We deﬁne a data slice as a group of data examples that share an attribute or characteristic.
INTRODUCTION,0.011650485436893204,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013592233009708738,"Figure 1: Proposed Approach. (Left) We design an evaluation framework to systematically com-
pare SDMs across diverse slice settings. Here, the example slice setting includes a dataset that
displays a strong correlation between the presence of birds and skies. (Right) A classiﬁer trained
to detect the presence of birds makes false positive predictions on skies without birds. We present
Domino, a novel SDM that uses cross-modal embeddings to identify and describe the error slice."
INTRODUCTION,0.015533980582524271,"In modern machine learning workﬂows, practitioners commonly attempt slice discovery with a com-
bination of feature-based interpretability methods (e.g. GradCAM, LIME) and manual inspection
(Selvaraju et al., 2017; Ribeiro et al., 2016). However, these approaches are time-consuming and
susceptible to conﬁrmation bias (Adebayo et al., 2018). As a result, recent works have proposed
automated slice discovery methods (SDMs), which use learned input representations to identify se-
mantically meaningful slices where the model makes prediction errors (d’Eon et al., 2021; Yeh et al.,
2020a; Sohoni et al., 2020; Kim et al., 2018; Singla et al., 2021). An ideal SDM should automat-
ically identify data slices that fulﬁll two desiderata: (a) slices should contain examples on which
the model underperforms, or has a high error rate and (b) slices should contain examples that are
coherent, or align closely with a human-understandable concept. An SDM that is able to reliably
satisfy these desiderata across a wide range of settings has yet to be demonstrated for two reasons:"
INTRODUCTION,0.017475728155339806,"Issue 1: No quantitative evaluation framework exists for measuring performance of SDMs with
respect to these desiderata. Existing SDM evaluations are either qualitative (d’Eon et al., 2021),
performed on purely synthetic data (Yeh et al., 2020a), or consider only a small selection of tasks
and slices (Sohoni et al., 2020). A comprehensive evaluation framework should be quantitative,
use realistic data, cover a broad range of contexts, and evaluate both underperformance and coher-
ence. Currently, no datasets or frameworks exist to support such an evaluation, making it difﬁcult to
evaluate the tradeoffs among prior SDMs."
INTRODUCTION,0.019417475728155338,"Issue 2:
Prior qualitative evaluations have demonstrated that existing SDMs often identify slices
that are incoherent. A practically useful SDM should discover coherent slices that are understand-
able by a domain expert. For example, in the chest X-ray setting described earlier, the slice “patients
without chest drains” is meaningful to a physician. Slice coherence has previously been evaluated
qualitatively by requiring users to manually inspect examples and identify common attributes (d’Eon
et al., 2021; Yeh et al., 2020a). Such evaluations have shown that discovered slices often do not align
with concepts understandable to a domain expert. Additionally, even if slices do align well with con-
cepts, it may be difﬁcult for humans to identify the shared attribute. Thus, an ideal SDM would not
only output coherent slices, but also identify the concept connecting examples in each slice."
INTRODUCTION,0.021359223300970873,"In this work, we address both of these issues by (1) developing a framework to quantitatively evaluate
the effectiveness of slice discovery methods at scale and (2) leveraging this framework to demon-
strate that a powerful class of recently-developed cross-modal embeddings can be used to create an
SDM that satisﬁes the above desiderata. Our approach – Domino – identiﬁes coherent slices and
generates automated slice descriptions."
INTRODUCTION,0.02330097087378641,"After formally describing the slice discovery problem in Section 2, we introduce an evaluation
framework for rigorously assessing SDM performance in Section 3. We curate a set of 1,235 slice
discovery settings, each consisting of a real-world dataset, a trained model, and one or more “ground
truth” slices corresponding to a concept in the domain. During evaluation, the SDM is provided with
the dataset and the model, and we measure if the labeled slices can be successfully identiﬁed. We
ﬁnd that existing methods identify “ground truth” slices in no more than 23% of these settings."
INTRODUCTION,0.02524271844660194,Published as a conference paper at ICLR 2022
INTRODUCTION,0.027184466019417475,"Motivated by the recent development of large cross-modal representation learning approaches (e.g.
CLIP) that embed inputs and text in the same latent representation space, in Section 4 we present
Domino, a novel SDM that uses cross-modal embeddings to identify coherent slices. Cross-modal
representations incorporate semantic meaning from text into input embeddings, which we demon-
strate can improve slice coherence and enable the generation of slice descriptions. Domino embeds
inputs alongside natural language with cross-modal representations, identiﬁes coherent slices with
an error-aware Gaussian mixture model, and generates natural language descriptions for discovered
slices. In Section 5, we use our evaluation framework to show that Domino identiﬁes 36% of the
“ground truth” coherent slices across three input domains (natural images, medical images, and
time-series) – a 12 percentage point improvement over existing methods."
RELATED WORK,0.02912621359223301,"2
RELATED WORK"
RELATED WORK,0.031067961165048542,"Slice performance gaps. Machine learning models are often limited by the presence of underper-
forming slices. In Section A.2, we provide a survey of underperforming slices reported by prior
studies across a range of application domains and slice types. Oakden-Rayner et al. (2019) referred
to this problem as “hidden stratiﬁcation” and motivated the need for slice discovery techniques."
RELATED WORK,0.03300970873786408,"Slice discovery. Prior work on slice discovery has predominantly focused on input datasets with rich
structure (e.g. tabular data) or metadata, where slicing can generally be performed with predicates
(e.g. nationality = Peruvian) (Chung et al., 2019; Sagadeeva & Boehm, 2021; Chen et al., 2021).
The slice discovery problem becomes particularly complex when input data lacks explicit structure
(e.g. images, audio, etc.) and metadata, and three recent studies present methods for performing
slice discovery in this unstructured setting (d’Eon et al., 2021; Sohoni et al., 2020; Kim et al., 2018;
Singla et al., 2021). The proposed SDMs follow two steps: (1) embed input data in a represen-
tation space and (2) identify underperforming slices using clustering or dimensionality reduction
techniques. These SDMs are typically evaluated by measuring performance over a limited number
of slice settings or by performing qualitative assessments. The trade-offs between these SDMs have
not been systematically compared, and as a result, the conditions under which these SDMs succeed
at identifying coherent slices remain unclear."
RELATED WORK,0.03495145631067961,"Benchmark datasets for machine learning robustness. Recently, several benchmark datasets have
been proposed for evaluating the performance of models on dataset shifts. These benchmarks are
valuable because they provide labels specifying important slices of data. However, these datasets do
not sufﬁce for systematic evaluations of SDMs because they either only annotate a small number of
slices (Koh et al., 2021) or do not provide pretrained models that are known to underperform on the
slices (He et al., 2021; Khosla et al., 2011; Hendrycks & Dietterich, 2019; Liang & Zou, 2021)."
RELATED WORK,0.036893203883495145,"Cross-modal embeddings. Cross-modal representation learning approaches, which embed input
data and text in the same representation space, yield powerful embeddings that have contributed to
large performance improvements across information retrieval and classiﬁcation tasks. Cross-modal
models generate semantically meaningful input representations that have been shown to be highly
effective on zero-shot classiﬁcation tasks (Radford et al., 2021). Cross-modal models that have
inspired our work include CLIP for natural images (Radford et al., 2021), ConVIRT for medical
images (Zhang et al., 2020), and WikiSatNet (Uzkent et al., 2019) for satellite imagery."
SLICE DISCOVERY PRELIMINARIES,0.038834951456310676,"3
SLICE DISCOVERY PRELIMINARIES"
SLICE DISCOVERY PRELIMINARIES,0.040776699029126215,"In this section, we formulate the slice discovery problem. Consider a standard classiﬁcation setting
with input X ∈X (e.g. an image, time-series, or graph) and label Y ∈Y over |Y| classes. Addition-
ally, assume there exists a set of k slices that partition the data into coherent (potentially overlapping)
subgroups, where each subgroup captures a distinct concept or attribute that would be familiar to a
domain expert. For each input, we represent slice membership as S = {S(j)}k
j=1 ∈{0, 1}k. As an
example, in the scenario presented in Section 1, X represents chest X-rays, Y is a binary label indi-
cating the presence of collapsed lungs, and S = {S(1), S(2)} represents two slices: one consisting
of normal X-rays with chest drains and the other consisting of collapsed lungs without chest drains."
SLICE DISCOVERY PRELIMINARIES,0.04271844660194175,"The inputs, labels, and slices vary jointly according to a probability distribution P(X, Y, S) over
X × Y × {0, 1}k. We assume that training, validation and test data are drawn independently and"
SLICE DISCOVERY PRELIMINARIES,0.04466019417475728,Published as a conference paper at ICLR 2022
SLICE DISCOVERY PRELIMINARIES,0.04660194174757282,"identically from this distribution. For some application-dependent value of ϵ, a model hθ : X →Y
exhibits degraded performance with respect to a slice S(j) and metric ℓ: Y × Y →R if
EX,Y |S(j)=1[ℓ(hθ(X), Y )] < EX,Y |S(j)=0[ℓ(hθ(X), Y )] −ϵ."
SLICE DISCOVERY PRELIMINARIES,0.04854368932038835,"Assuming that a trained classiﬁer hθ : X →Y exhibits degraded performance on each of the k
slices in S, we deﬁne the slice discovery problem as follows:"
SLICE DISCOVERY PRELIMINARIES,0.05048543689320388,"• Inputs: a trained classiﬁer hθ and labeled dataset D = {(xi, yi)}n
i=1 with n samples drawn
from P(X, Y )."
SLICE DISCOVERY PRELIMINARIES,0.05242718446601942,"• Output: a set of ˆk slicing functions Ψ = {ψ(j) : X × Y →{0, 1}}ˆk
j=1 that partition the
data into ˆk subgroups."
SLICE DISCOVERY PRELIMINARIES,0.05436893203883495,"We consider an output to be successful if, for each ground truth slice S(u), a slicing function ψ(v)"
SLICE DISCOVERY PRELIMINARIES,0.05631067961165048,predicts S(u) with precision above some threshold β:
SLICE DISCOVERY PRELIMINARIES,0.05825242718446602,"∀u ∈[k].
∃v ∈[ˆk].
P(S(u) = 1|ψ(v)(X, Y ) = 1) > β."
SLICE DISCOVERY PRELIMINARIES,0.06019417475728155,"A slice discovery method (SDM), M(D, hθ) →Ψ, aims to solve the slice discovery problem."
SLICE DISCOVERY EVALUATION FRAMEWORK,0.062135922330097085,"4
SLICE DISCOVERY EVALUATION FRAMEWORK"
SLICE DISCOVERY EVALUATION FRAMEWORK,0.06407766990291262,"It is challenging to measure how well an SDM satisﬁes the following desiderata outlined in Section
1: (a) the model hθ should exhibit degraded performance on slices predicted by the SDM and (b)
slices predicted by the SDM should be coherent. Most publicly available machine learning datasets
do not provide labels identifying coherent, underperforming slices. As a result, existing evaluations
are either qualitative (d’Eon et al., 2021), synthetic (Yeh et al., 2020b), or small-scale (Sohoni et al.,
2020). In this section, we propose an evaluation framework for estimating the success rate of an
SDM: how often it successfully identiﬁes the coherent slices on which the model underperforms."
SLICE DISCOVERY EVALUATION FRAMEWORK,0.06601941747572816,"We propose evaluating SDMs across large sets of slice discovery settings, each consisting of: (1)
a labeled dataset D = {(xi, yi)}n
i=1, (2) a model hθ trained on D, and (3) ground truth slice
annotations {si}n
i=1 for one or more coherent slices S on which the model hθ exhibits degraded
performance. As discussed in Section 3, (1) and (2) correspond to the inputs to the SDM, while (3)
corresponds to the expected output. Using these slice discovery settings, we can estimate how often
an SDM M(D, fθ) →Ψ successfully identiﬁes the slices S. Algorithm 1 details the procedure."
SLICE DISCOVERY EVALUATION FRAMEWORK,0.06796116504854369,Algorithm 1 SDM Evaluation Process
SLICE DISCOVERY EVALUATION FRAMEWORK,0.06990291262135923,"for (D, s, hθ) ∈settings do"
SLICE DISCOVERY EVALUATION FRAMEWORK,0.07184466019417475,"Ψ ←M(Dvalid, hθ) ▷Fit the SDM on the validation set, yielding a set of slicing functions Ψ
for j ∈[ˆk] do"
SLICE DISCOVERY EVALUATION FRAMEWORK,0.07378640776699029,"ˆs(j) ←ψ(j)(Dtest)
▷Apply the slicing functions to the test set, yielding ˆs ∈[0, 1]ntest.
end for
metrics ←{maxj∈[ˆk] L(s(i),ˆs(j))}k
i=1
▷Compute metric L comparing ˆs and s
end for"
SLICE DISCOVERY EVALUATION FRAMEWORK,0.07572815533980583,"In Section 4.1, we propose a process for generating representative slice discovery settings and in
Section 4.2, we describe the evaluation metrics L and datasets that we use."
GENERATING SLICE DISCOVERY SETTINGS,0.07766990291262135,"4.1
GENERATING SLICE DISCOVERY SETTINGS"
GENERATING SLICE DISCOVERY SETTINGS,0.07961165048543689,"To obtain accurate estimates of SDM performance, our slice discovery settings should be both rep-
resentative of real-world slice discovery settings and large in number. However, such slice discovery
settings aren’t readily available, since few datasets specify slices on which models underperform."
GENERATING SLICE DISCOVERY SETTINGS,0.08155339805825243,"Here, we propose a framework for programmatically generating a large number of realistic slice
discovery settings. We begin with a base dataset Dbase that has either a hierarchical label structure
(e.g. ImageNet) or rich metadata accompanying each example (e.g.. CelebA). We select a target"
GENERATING SLICE DISCOVERY SETTINGS,0.08349514563106795,Published as a conference paper at ICLR 2022
GENERATING SLICE DISCOVERY SETTINGS,0.0854368932038835,"variable Y and slice variable S, each deﬁned in terms of the class structure or metadata. This allows
us to derive target and slice labels {(yi, si)}n
i=1 directly from the dataset. In addition, because the
slice S is deﬁned in terms of meaningful annotations, we know that the slice is coherent. After
selecting a target variable and slice variable, we (1) generate a dataset D (Section 4.1.1) and (2)
generate a model hθ that exhibits degraded performance with respect to S (Section 4.1.2)."
DATASET GENERATION,0.08737864077669903,"4.1.1
DATASET GENERATION"
DATASET GENERATION,0.08932038834951456,"We categorize each slice discovery setting based on the underlying reason that the model hθ exhibits
degraded performance on the slices S. We survey the literature for examples of underperforming
slices in the wild, which we document in Section A.2. Based on our survey and prior work (Oakden-
Rayner et al., 2019), we identify three common slice types: rare slices, correlation slices, and noisy
label slices. We provide expanded descriptions in Section A.3."
DATASET GENERATION,0.0912621359223301,"Rare slice. Models often underperform on rare data slices, which consist of data subclasses that
occur infrequently in the training set (e.g. patients with a rare diseases, photos taken at night). To
generate settings with rare slices, we construct D such that for a given class label Y , elements in
subclass C occur with proportion α, where 0.01 < α < 0.1."
DATASET GENERATION,0.09320388349514563,"Correlation slice. If a target variable Y (e.g. pneumothorax) is correlated with another variable
C (e.g. chest tubes), the model may learn to rely on C to make predictions. To generate settings
with correlation slices, we construct D such that a linear correlation of strength α exists between the
target variable and other class labels, where 0.2 < α < 0.8. (Details in Section A.3.2)."
DATASET GENERATION,0.09514563106796116,"Noisy label slice. Particular slices of data may exhibit higher label error rates than the rest of the
training distribution. To generate settings with noisy labels, we construct D such that for each class
label Y , the elements in subclass C exhibit label noise with probability α, where 0.01 < α < 0.3."
MODEL GENERATION,0.0970873786407767,"4.1.2
MODEL GENERATION"
MODEL GENERATION,0.09902912621359224,"Each slice discovery setting includes a model hθ that exhibits degraded performance with respect
to a set of slices S. We propose generating two classes of models: (a) trained models, which are
realistic, and (b) synthetic models, which provide greater control over the evaluation."
MODEL GENERATION,0.10097087378640776,"Trained Models. We train a distinct model hθ across each of our generated datasets D. If the model
hθ exhibits a degradation in performance with respect to the slices S, then the model hθ, dataset D,
and slices S will comprise a valid slice discovery setting. (See Section 3 for the formal deﬁnition of
degraded performance; we use accuracy for our metric ℓand set ϵ = 10.)"
MODEL GENERATION,0.1029126213592233,"Synthetic Models. Because the trained model hθ may underperform on coherent slices other than
S, an SDM that fails to identify S may still recover other coherent, underperforming slices. This
complicates the interpretation of our results. To address this subtle issue, we also create settings
using synthetic models ¯h : [0, 1]k ×Y →[0, 1] that simulate predictions. This allows us to distribute
errors outside of S randomly, ensuring that there are likely no underperforming, coherent slices
outside of S. We simulate predictions by sampling from beta distributions (see Section A.3.4)."
EVALUATION APPROACH,0.10485436893203884,"4.2
EVALUATION APPROACH"
EVALUATION APPROACH,0.10679611650485436,"We instantiate our evaluation framework by generating 1,235 slice discovery settings across a num-
ber of different tasks, applications, and base datasets. Detailed statistics on our settings are provided
in Table 1. We generate slice discovery settings in the following three domains:"
EVALUATION APPROACH,0.1087378640776699,"Natural Images (CelebA and ImageNet): The CelebFaces Attributes Dataset (CelebA) includes
over 200k images with 40 labeled attributes (Liu et al., 2015). ImageNet includes 1.2 million images
across 1000 labeled classes organized in a hierarchical structure (Deng et al., 2009; Fellbaum, 1998)."
EVALUATION APPROACH,0.11067961165048544,"Medical Images (MIMIC-CXR): The MIMIC Chest X-Ray (MIMIC-CXR) dataset includes
377,110 chest x-rays collected from the Beth Israel Deaconess Medical Center. Annotations in-
dicate the presence or absence of fourteen conditions (Johnson et al., 2019; 2020)."
EVALUATION APPROACH,0.11262135922330097,"Medical Time-Series Data (EEG): In addition to our image modalities, we also explore time-series
data. We obtain a dataset of short 12 second electroencephalography (EEG) signals, which have been
used in prior work for predicting the onset of seizures (Saab et al., 2020)."
EVALUATION APPROACH,0.1145631067961165,Published as a conference paper at ICLR 2022
EVALUATION APPROACH,0.11650485436893204,"Figure 2: Evaluation Framework. We propose a framework for generating slice discovery settings
from any base dataset with class structure or metadata."
EVALUATION APPROACH,0.11844660194174757,"We evaluate SDM performance with precision-at-k, which measures the proportion of the top k
elements in the discovered slice that are in the ground truth slice. We use k = 10 in this work."
DOMINO,0.1203883495145631,"5
DOMINO"
DOMINO,0.12233009708737864,"In this section, we introduce Domino, an SDM that uses cross-modal embeddings to identify coher-
ent slices and generate natural language descriptions. Domino follows a three-step procedure:"
DOMINO,0.12427184466019417,"1. Embed (Section 5.1): We encode the inputs {xi}n
i=1 in a cross-modal embedding space via
a function ginput : X →Rd. We learn this embedding function ginput jointly with an embedding
function gtext : T →Rd that embeds text in the same space as the inputs."
DOMINO,0.1262135922330097,"2. Slice (Section 5.2): We identify underperforming regions in the cross-modal embedding space
using an error-aware mixture model ﬁt on the input embeddings zinput := {zi := ginput(xi)}n
i=1,
model predictions {ˆyi := hθ(xi)}n
i=1, and true class labels {yi}n
i=1. This yields ˆk slicing functions
of the form ψ(j)
slice : X × Y →{0, 1}."
DOMINO,0.12815533980582525,"3. Describe (Section 5.3): Finally, we use the text embedding function gtext learned in step (1) to
generate a set of ˆk natural language descriptions of the discovered slices."
EMBEDDING INPUTS WITH CROSS-MODAL REPRESENTATIONS,0.13009708737864079,"5.1
EMBEDDING INPUTS WITH CROSS-MODAL REPRESENTATIONS"
EMBEDDING INPUTS WITH CROSS-MODAL REPRESENTATIONS,0.13203883495145632,"Cross-modal representation learning algorithms embed input examples and paired text data in the
same latent representation space. Formally, given a dataset of paired inputs V ∈X and text descrip-
tions T ∈T , Dpaired = {(vi, ti)}npaired
i=1
, we learn two embedding functions ginput : X →Rd and
gtext : T →Rd such that the distances between pairs of embeddings dist(ginput(vi), gtext(tj)) reﬂect
the semantic similarity between vi and tj for all i, j ≤npaired."
EMBEDDING INPUTS WITH CROSS-MODAL REPRESENTATIONS,0.13398058252427184,"Ultimately, this joint training procedure enables the creation of semantically meaningful input em-
beddings that incorporate information from text. In this work, our key insight is that input represen-
tations generated from cross-modal learning techniques encode the semantic knowledge necessary
for identifying coherent slices. Our method relies on the assumption that we have access to either
(a) pretrained cross-modal embedding functions or (b) a dataset with paired input-text data that can
be used to learn cross-modal embedding functions. It is important to note that paired data is only
required if a practitioner wishes to generate custom cross-modal embedding functions."
EMBEDDING INPUTS WITH CROSS-MODAL REPRESENTATIONS,0.13592233009708737,"Domino uses four types of cross-modal embeddings to enable slice discovery across our input do-
mains: CLIP (Radford et al., 2021), ConVIRT (Zhang et al., 2020), MIMIC-CLIP, and EEG-CLIP.
We adapt CLIP and ConVIRT from prior work, and we train MIMIC-CLIP and EEG-CLIP on large
datasets with paired inputs and text (implementation details are provided in Section A.4.1)."
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL,0.1378640776699029,"5.2
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL"
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL,0.13980582524271845,"We then proceed to the second step in the Domino pipeline: slicing. Recall that our goal is to
ﬁnd a set of ˆk slicing functions that partition our data into coherent and underperforming slices.
Taking inspiration from the recently developed Spotlight algorithm (d’Eon et al., 2021), we propose"
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL,0.141747572815534,Published as a conference paper at ICLR 2022
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL,0.1436893203883495,"a mixture model that jointly models the input embeddings, class labels, and model predictions.
This encourages slices that are homogeneous with respect to error type (e.g. all false positives). The
model assumes that data is generated according to the following generative process: each example is
randomly assigned membership to a single slice according to a categorical distribution S ∼Cat(pS)
with parameter pS ∈{p ∈R¯k
+ : P¯k
i=1 pi = 1}. Given membership in slice j, the embeddings are
normally distributed Z|S(j) = 1 ∼N(µ(j), Σ(j)) with parameters mean µ(j) ∈Rd and covariance
Σ(j) ∈Sd
++ (the set of symmetric positive deﬁnite d × d matrices), the labels vary as a categorical
Y |S(j) = 1 ∼Cat(p(j)) with parameter p(j) ∈{p ∈Rc
+ : Pc
i=1 pi = 1}, and the model
predictions also vary as a categorical ˆY |S(j) = 1 ∼Cat(ˆp(j)) with parameter ˆp(j) ∈{ˆp ∈
Rc
+ : Pc
i=1 ˆpi = 1}. This assumes that the embedding, label, and prediction are all independent
conditioned on the slice."
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL,0.14563106796116504,"The mixture model is parameterized by φ = [pS, {µ(j), Σ(j), p(j), ˆp(j)}¯k
j=1]. The log-likelihood
over the validation dataset Dv is given as follows and maximized using expectation-maximization: ℓ(φ)= n
X"
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL,0.14757281553398058,"i=1
log ¯k
X"
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL,0.14951456310679612,"j=1
P(S(j) =1)P(Z =zi|S(j) =1)P(Y =yi|S(j) =1)γP( ˆY =hθ(xi)|S(j) =1)γ,"
CLUSTERING EMBEDDINGS WITH ERROR-AWARE MIXTURE MODEL,0.15145631067961166,"(1)
where γ is a hyperparameter that balances the trade-off between coherence and under-performance,
our two desiderata (see Section A.4.2)."
GENERATING NATURAL LANGUAGE DESCRIPTIONS OF DISCOVERED SLICES,0.1533980582524272,"5.3
GENERATING NATURAL LANGUAGE DESCRIPTIONS OF DISCOVERED SLICES"
GENERATING NATURAL LANGUAGE DESCRIPTIONS OF DISCOVERED SLICES,0.1553398058252427,"Domino can produce natural language statements that describe characteristics shared between ex-
amples in the discovered slices. To generate slice descriptions, we begin by sourcing a corpus of
candidate natural language phrases Dtext = {tj}ntext
j=1. Critically, this text data can be sourced inde-
pendently and does not need to be paired with examples in D. In Section A.4.3, we describe an
approach for generating a corpus of phrases relevant to the domain."
GENERATING NATURAL LANGUAGE DESCRIPTIONS OF DISCOVERED SLICES,0.15728155339805824,"We generate an embedding for each phrase in Dtext using the cross-modal embedding func-
tion gtext, yielding {ztext
j
:=
gtext(tj)}ntext
j=1.
Then, we compute a prototype embedding for
each of the discovered slices by taking the weighted average of the input embeddings in the
slice, {¯z(i)
slice := ψ(i)
slice(x, y)⊤zinput}ˆk
i=1. We also compute a prototype embedding for each class
{¯z(c)
class :=
1
nc
Pn
i=1 1[yi = c]zinput
i
}C
c=1. To distill the slice prototypes, we subtract out the proto-"
GENERATING NATURAL LANGUAGE DESCRIPTIONS OF DISCOVERED SLICES,0.15922330097087378,"type of the most common class in the slice ¯z(i)
slice −¯z(c)
class. Finally, to ﬁnd text that describes each
slice, we compute the dot product between the distilled slice prototypes and the text embeddings
and return the phrase with the highest value: argmaxj∈[ntext]ztext⊤
j
(¯z(i)
slice −¯z(c)
class)."
EXPERIMENTS,0.16116504854368932,"6
EXPERIMENTS"
EXPERIMENTS,0.16310679611650486,"We use the evaluation framework developed in Section 4 to systematically assess Domino, compar-
ing it to existing SDMs across 1,235 slice discovery settings. Our experiments validate the three
core design choices behind Domino: (1) the use of cross-modal embeddings, (2) the use of a novel
error-aware mixture model, and (3) the generation of natural language descriptions for slices. We
provide SDM implementation details in Section A.3.2 and extended evaluations in Section A.5."
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.1650485436893204,"6.1
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE"
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.1669902912621359,"In this section, we evaluate the effect of embedding type on performance. We hold our error-aware
slicing algorithm (Step 2) constant and vary our choice of embedding (Step 1)."
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.16893203883495145,"Natural Images. We compare four embeddings: ﬁnal-layer activations of a randomly-initialized
ResNet-50 (He et al., 2016), ﬁnal-layer activations of hθ, BiT (Kolesnikov et al., 2019), and CLIP
(Radford et al., 2021). CLIP embeddings are cross-modal. Results are shown in Figure 3."
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.170873786407767,Published as a conference paper at ICLR 2022
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.17281553398058253,"Figure 3: Cross-modal embeddings enable accurate slice discovery. Using our evaluation frame-
work, we demonstrate that the use of cross-modal embeddings leads to consistent improvements in
slice discovery across three datasets and two input modalities (1,235 settings)."
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.17475728155339806,"When evaluating with synthetic models, we ﬁnd that using CLIP embeddings results in a mean
precision-at-10 of 0.570 (95% CI: 0.554, 0.586), a 9 percentage point increase over BiT embeddings
and a 23 percentage point increase over random activations.2"
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.1766990291262136,"When evaluating with trained models, we ﬁnd no difference between using CLIP embeddings and
BiT embeddings. However, both outperform activations of the trained classiﬁer hθ by nearly 15
percentage points in mean precision-at-10. This ﬁnding is of particular interest given that classiﬁer
activations are a popular embedding choice in prior SDMs (d’Eon et al., 2021; Sohoni et al., 2020).
Notably, the gap between CLIP and hθ activations is much smaller in settings with correlation slices.
This makes sense because a model that relies on a correlate to make predictions will likely capture
information about the correlate in its activations (Sohoni et al., 2020)."
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.1786407766990291,"Medical Images. We compare ﬁve embeddings: the ﬁnal-layer activations of a ResNet-50 pre-
trained on ImageNet (He et al., 2016), the ﬁnal-layer activations of the trained classiﬁer hθ, BiT
(Kolesnikov et al., 2019), and domain-speciﬁc cross-modal embeddings that we trained using two
different methods: MIMIC-CLIP and ConVIRT. For synthetic models, cross-modal ConVIRT em-
beddings enable a mean precision-at-10 of 0.765 (95% CI: 0.747, 0.784), a 7 point improvement over
the best unimodal embeddings (BiT) with mean precision-at-10 of 0.695 (95% CI: 0.674, 0.716).
For trained models, we again ﬁnd that although hθ activations perform the worst on rare and noisy
label slices, they are competitive with cross-modal embeddings on correlation slices."
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.18058252427184465,"Medical Time Series. For our EEG dataset, we compare the ﬁnal-layer activations of a pretrained
seizure classiﬁer and a CLIP-style cross-modal embedding trained on EEG-report pairs. When
evaluating with synthetic models, we ﬁnd that cross-modal embeddings recover coherent slices with
a mean precision-at-10 of 0.697 (95% CI: 0.605, 0.784). This represents a 17 point gain over using
unimodal embeddings 0.532 (95% CI: 0.459, 0.608). Cross-modal embeddings also outperform
unimodal embeddings when evaluated with trained models. This demonstrates that cross-modal
embeddings can aid in recovering coherent slices even in input modalities other than images."
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.1825242718446602,"2Note that synthetic models do not have activations, so we cannot compare to trained activations here."
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.18446601941747573,Published as a conference paper at ICLR 2022
CROSS-MODAL EMBEDDINGS IMPROVE SDM PERFORMANCE,0.18640776699029127,"Figure 4: Error-aware mixture model enables accurate slice discovery.
When cross-modal
embeddings are provided as input, our error-aware mixture model often outperforms previously-
designed SDMs. Results on medical images and medical time-series data are in Section A.5."
ERROR-AWARE MIXTURE MODEL IMPROVES SDM PERFORMANCE,0.1883495145631068,"6.2
ERROR-AWARE MIXTURE MODEL IMPROVES SDM PERFORMANCE"
ERROR-AWARE MIXTURE MODEL IMPROVES SDM PERFORMANCE,0.19029126213592232,"In order to understand the effect of slicing algorithms on performance, we hold the cross-modal
embeddings (Step 1) constant and vary the slicing algorithm (Step 2). We compare the error-aware
mixture model to four prior SDMs: George (Sohoni et al., 2020), Multiaccuracy (Kim et al., 2018),
Spotlight (d’Eon et al., 2021), and a baseline we call ConfusionSDM, which outputs slicing func-
tions that partition data into the cells of the confusion matrix. We provide cross-modal embeddings
as input to all ﬁve SDMs. On noisy and rare slices in natural images, the error-aware mixture model
recovers ground truth slices with a mean precision-at-10 of 0.639 (95% CI: 0.617,0.660) – this
represents a 105% improvement over the next-best method, George. Interestingly, on correlation
slices, the naive ConfusionSDM baseline outperforms our error-aware mixture model. Extended
evaluations are provided in Section A.5."
ERROR-AWARE MIXTURE MODEL IMPROVES SDM PERFORMANCE,0.19223300970873786,"6.3
DOMINO GENERATES NATURAL LANGUAGE DESCRIPTIONS OF DISCOVERED SLICES."
ERROR-AWARE MIXTURE MODEL IMPROVES SDM PERFORMANCE,0.1941747572815534,"Domino is the ﬁrst SDM that can generate natural language descriptions for identiﬁed slices. For
natural images, we provide a quantitative analysis of these descriptions. Speciﬁcally, since Domino
returns a ranking over all phrases in the corpus Dtext, we can compute the percentage of settings in
which the name of the “ground truth” slice (or a WordNet synonym (Fellbaum, 1998)) appears in the
top-k words returned by Domino. In Figure 9, we plot this percentage for k = 1 to k = 10. We ﬁnd
that for 34.7% of rare slices, 41.0% of correlation slices, and 39.0% of noisy label slices, Domino
ranks the name of the slice (or a synonym) ﬁrst out of the thousands of phrases in our corpus. In
57.4%, 55.4%, and 48.7% of rare, correlation, and noisy label slices respectively, Domino ranks the
phrase in the top ten. In Section A.1, we show that Domino is successful at recovering accurate
explanations for natural image, medical image, and medical time series data."
CONCLUSION,0.19611650485436893,"7
CONCLUSION"
CONCLUSION,0.19805825242718447,"In this work, we analyze the slice discovery problem. First, we observe that existing approaches
for evaluating SDM performance do not allow for large-scale, quantitative evaluations. We address
this challenge by introducing a programmable framework to measure SDM performance across two
axes: underperformance and coherence. Second, we propose Domino, a novel SDM that combines
cross-modal representations with an error-aware mixture model. Using our evaluation framework,
we demonstrate that the embedding and slicing steps of Domino outperform those of existing SDMs.
We also show for the ﬁrst time that using cross-modal embeddings for slice discovery can enable
the generation of semantically meaningful slice descriptions. Notably, Domino requires only black-
box access to models and can thus be broadly useful in settings where users have API access to
models. Future directions include executing controlled user studies to evaluate when generated
explanations are actionable, developing strategies for computing input embeddings when access
to both pre-trained cross-modal embeddings and paired input-text data is limited, and exploring
strategies for improving slice discovery in settings where slice strength, α, is low. We hope that our
evaluation framework accelerates the development of slice discovery methods and that Domino will
help practitioners better evaluate their models."
CONCLUSION,0.2,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.20194174757281552,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.20388349514563106,"We provide an open-source implementation of our evaluation framework at https://github.
com/HazyResearch/domino. Users can run Domino on their own models and datasets by
installing our Python package via: pip install domino."
ETHICS STATEMENT,0.2058252427184466,"9
ETHICS STATEMENT"
ETHICS STATEMENT,0.20776699029126214,"Domino is a tool for identifying systematic model errors. No matter how effective it is at this task,
there may still be failure-modes Domino will not catch. There is a legitimate concern that model
debugging tools like Domino could give practitioners a false sense of security, when in fact their
models are failing on important slices not recovered by Domino. It is critical that practitioners still
run standard evaluations on accurately-labeled, representative test sets in addition to using Domino
for auditing models. Additionally, because Domino uses embeddings trained on image-text pairs
sourced from the web, it may reﬂect societal biases when identifying and describing slices. Future
work should explore the impacts of using biased embeddings to identify errors in models. What
kinds of error modes might we miss? Are certain underrepresented groups or concepts less likely to
be identiﬁed as an underperforming slice?"
CONTRIBUTIONS,0.20970873786407768,"10
CONTRIBUTIONS"
CONTRIBUTIONS,0.21165048543689322,"S.E., M.V., K.S., J.D., J.Z., and C.R. conceptualized the overall study. S.E., M.V., K.S., J.-B.D.,
J.D., J.Z., and C.R. contributed to the experimental design while S.E., M.V., K.S., and J.-B.D. wrote
computer code and performed experiments. S.E. trained all models for CelebA and ImageNet, M.V.
trained all models for MIMIC, K.S. trained all models for EEG, and J.-B.D generated ConVIRT
embeddings for MIMIC. C.L.-M. provided the labeled EEG data and clinical expertise. S.E., M.V.,
and K.S. prepared the manuscript. All authors contributed to manuscript review."
ACKNOWLEDGEMENTS,0.21359223300970873,"11
ACKNOWLEDGEMENTS"
ACKNOWLEDGEMENTS,0.21553398058252426,"We are thankful to Karan Goel, Laurel Orr, Michael Zhang, Sarah Hooper, Neel Guha, Megan
Leszczynski, Arjun Desai, Priya Mishra, Simran Arora, Jure Leskovec, Zach Izzo, Nimit Sohoni,
and Weixin Liang for helpful discussions and feedback. Sabri Eyuboglu is supported by the Na-
tional Science Foundation Graduate Research Fellowship. Maya Varma is supported by graduate
fellowship awards from the Department of Defense (NDSEG) and the Knight-Hennessy Scholars
program at Stanford University. Khaled Saab is supported by the Stanford Interdisciplinary Gradu-
ate Fellowship with the Wu Tsai Neurosciences Institute. James Zou is supported by NSF CAREER
1942926. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize),
NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301
(RTML); ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No.
N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Ap-
plying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx,
LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Eric-
sson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits
for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford
DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.
Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of
the authors and do not necessarily reﬂect the views, policies, or endorsements, either expressed or
implied, of NIH, ONR, or the U.S. Government."
REFERENCES,0.2174757281553398,REFERENCES
REFERENCES,0.21941747572815534,"Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity Checks for Saliency Maps. In S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-
Bianchi, and R Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9505–
9515. Curran Associates, Inc., 2018."
REFERENCES,0.22135922330097088,Published as a conference paper at ICLR 2022
REFERENCES,0.22330097087378642,"Marcus A Badgeley, John R Zech, Luke Oakden-Rayner, Benjamin S Glicksberg, Manway Liu,
William Gale, Michael V McConnell, Bethany Percha, Thomas M Snyder, and Joel T Dudley.
Deep learning predicts hip fracture using confounding patient and healthcare variables. npj Digital
Medicine, 2(1):1–10, April 2019."
REFERENCES,0.22524271844660193,"Atul Bansal, Ravinder Agarwal, and R K Sharma. Svm based gender classiﬁcation using iris images.
pp. 425–429, November 2012."
REFERENCES,0.22718446601941747,"Alceu Bissoto, Michel Fornaciali, Eduardo Valle, and Sandra Avila. (de) constructing bias on skin
lesion datasets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pp. 0–0, 2019."
REFERENCES,0.229126213592233,"Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commer-
cial gender classiﬁcation. In Conference on fairness, accountability and transparency, pp. 77–91.
PMLR, 2018."
REFERENCES,0.23106796116504855,"Mayee Chen, Karan Goel, Nimit S Sohoni, Fait Poms, Kayvon Fatahalian, and Christopher Re.
Mandoline: Model evaluation under distribution shift. In Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
1617–1629. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
chen21i.html."
REFERENCES,0.23300970873786409,"Yeounoh Chung, Tim Kraska, Neoklis Polyzotis, Ki Hyun Tae, and Steven Euijong Whang. Slice
ﬁnder: Automated data slicing for model validation. In 2019 IEEE 35th International Conference
on Data Engineering (ICDE), pp. 1550–1553. ieeexplore.ieee.org, April 2019."
REFERENCES,0.23495145631067962,"Terrance de Vries, Ishan Misra, Changhan Wang, and Laurens van der Maaten. Does object recog-
nition work for everyone? In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops, pp. 52–59, 2019."
REFERENCES,0.23689320388349513,"Alex J DeGrave, Jose Janizek, and Su-In Lee. AI for radiographic COVID-19 detection selects
shortcuts over signal. Nature Machine Intelligence, 3(7):610–619, May 2021."
REFERENCES,0.23883495145631067,"Jean-Benoit Delbrouck, Khaled Saab, Maya Varma, Sabri Eyuboglu, Jared A. Dunnmon, Pierre
Chambon, Juan Manuel Zambrano, Akshay Chaudhari, and Curtis P. Langlotz.
Vilmedic: a
framework for research at the intersection of vision and language in medical ai. In Proceedings of
the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstra-
tions. Association for Computational Linguistics, May 2022."
REFERENCES,0.2407766990291262,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255, June 2009."
REFERENCES,0.24271844660194175,"Greg d’Eon, Jason d’Eon, James R. Wright, and Kevin Leyton-Brown. The spotlight: A general
method for discovering systematic errors in deep learning models, 2021."
REFERENCES,0.2446601941747573,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding.
CoRR, abs/1810.04805, 2018.
URL
http://arxiv.org/abs/1810.04805."
REFERENCES,0.24660194174757283,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale, 2021."
REFERENCES,0.24854368932038834,"Christiane Fellbaum. WordNet: An Electronic Lexical Database. Bradford Books, 1998."
REFERENCES,0.2504854368932039,"Jason A Fries, Paroma Varma, Vincent S Chen, Ke Xiao, Heliodoro Tejeda, Priyanka Saha, Jared
Dunnmon, Henry Chubb, Shiraz Maskatia, Madalina Fiterau, Scott Delp, Euan Ashley, Christo-
pher R´e, and James R Priest. Weakly supervised classiﬁcation of aortic valve malformations using
unlabeled cardiac MRI sequences. Nat. Commun., 10(1), December 2019."
REFERENCES,0.2524271844660194,Published as a conference paper at ICLR 2022
REFERENCES,0.2543689320388349,"Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong,
Mohit Bansal, and Christopher R´e. Robustness Gym: Unifying the NLP Evaluation Landscape.
arXiv:2101. 04840 [cs], January 2021."
REFERENCES,0.2563106796116505,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. February 2015."
REFERENCES,0.258252427184466,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.26019417475728157,"Yue He, Zheyan Shen, and Peng Cui. Towards non-i.i.d. image classiﬁcation: A dataset and base-
lines. Pattern Recognit., 110:107383, February 2021."
REFERENCES,0.2621359223300971,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. March 2019."
REFERENCES,0.26407766990291265,"A Johnson, L Bulgarelli, T Pollard, S Horng, LA Celi, and R Mark. Mimic-iv (version 1.0), 2020."
REFERENCES,0.26601941747572816,"Alistair E W Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lun-
gren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identiﬁed publicly
available database of chest radiographs with free-text reports. Scientiﬁc data, 6(1):1–8, 2019."
REFERENCES,0.26796116504854367,"Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for ﬁne-
grained image categorization. In First Workshop on Fine-Grained Visual Categorization, IEEE
Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, June 2011."
REFERENCES,0.26990291262135924,"Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-Box Post-Processing for
Fairness in Classiﬁcation. arXiv:1805. 12317 [cs, stat], August 2018."
REFERENCES,0.27184466019417475,"Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.
6980 [cs], January 2017."
REFERENCES,0.2737864077669903,"Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor
Toups, John R Rickford, Dan Jurafsky, and Sharad Goel. Racial disparities in automated speech
recognition. Proceedings of the National Academy of Sciences, 117(14):7684–7689, 2020."
REFERENCES,0.2757281553398058,"Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsub-
ramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne
David, Ian Stavness, Wei Guo, Berton A Earnshaw, Imran S Haque, Sara Beery, Jure Leskovec,
Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A
Benchmark of in-the-Wild Distribution Shifts. arXiv:2012. 07421 [cs], March 2021."
REFERENCES,0.27766990291262134,"Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. December 2019."
REFERENCES,0.2796116504854369,"Andrey Kuehlkamp, Benedict Becker, and Kevin Bowyer.
Gender-from-iris or gender-from-
mascara? February 2017."
REFERENCES,0.2815533980582524,"Weixin Liang and James Zou. Metadataset: A dataset of datasets for evaluating distribution shifts
and training conﬂicts. In ICML2021 ML4data Workshop, 2021."
REFERENCES,0.283495145631068,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.2854368932038835,"Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher R´e. Hidden stratiﬁcation
causes clinically meaningful failures in machine learning for medical imaging. September 2019."
REFERENCES,0.287378640776699,"Laurel Orr, Megan Leszczynski, Simran Arora, Sen Wu, Neel Guha, Xiao Ling, and Christopher
Re. Bootleg: Chasing the tail with self-supervised named entity disambiguation. October 2020."
REFERENCES,0.28932038834951457,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. February
2021."
REFERENCES,0.2912621359223301,Published as a conference paper at ICLR 2022
REFERENCES,0.29320388349514565,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Explaining the
predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’16, pp. 1135–1144, New York, NY, USA,
2016. Association for Computing Machinery. ISBN 9781450342322. doi: 10.1145/2939672.
2939778. URL https://doi.org/10.1145/2939672.2939778."
REFERENCES,0.29514563106796116,"Subhrajit Roy, Isabell Kiral-Kornek, and Stefan Harrer. Chrononet: a deep recurrent neural network
for abnormal eeg identiﬁcation. In Conference on Artiﬁcial Intelligence in Medicine in Europe,
pp. 47–56. Springer, 2019."
REFERENCES,0.2970873786407767,"Khaled Saab, Jared Dunnmon, Christopher R´e, Daniel Rubin, and Christopher Lee-Messer. Weak
supervision as an efﬁcient approach for automated seizure detection in electroencephalog-
raphy.
npj Digital Medicine, 2020.
URL https://www.nature.com/articles/
s41746-020-0264-0#article-info."
REFERENCES,0.29902912621359223,"Svetlana Sagadeeva and Matthias Boehm. Sliceline: Fast, linear-algebra-based slice ﬁnding for ml
model debugging. In Proceedings of the 2021 International Conference on Management of Data,
pp. 2290–2299. Association for Computing Machinery, New York, NY, USA, June 2021."
REFERENCES,0.30097087378640774,"Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally Robust
Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Gener-
alization. arXiv:1911. 08731 [cs, stat], April 2020."
REFERENCES,0.3029126213592233,"Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct
2017."
REFERENCES,0.3048543689320388,"Ilya Semenov and Shamsul Areﬁn.
Wikipedia word frequency.
https://github.com/
IlyaSemenov/wikipedia-word-frequency, 2019."
REFERENCES,0.3067961165048544,"Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, and Eric Horvitz. Understanding failures
of deep networks via robust feature extraction. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 12853–12862, 2021."
REFERENCES,0.3087378640776699,"Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew Lungren.
Combining automatic labelers and expert annotations for accurate radiology report labeling us-
ing BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 1500–1519, Online, November 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.emnlp-main.117. URL https://aclanthology.org/
2020.emnlp-main.117."
REFERENCES,0.3106796116504854,"Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R´e.
No sub-
class left behind:
Fine-grained robustness in coarse-grained classiﬁcation problems.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 19339–19352. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
e0688d13958a19e087e123148555e4b4-Paper.pdf."
REFERENCES,0.312621359223301,"Juan E Tapia, Claudio A Perez, and Kevin W Bowyer. Gender classiﬁcation from the same iris code
used for recognition. IEEE Trans. Inf. Forensics Secur., 11(8):1760–1770, August 2016."
REFERENCES,0.3145631067961165,"Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang, Marshall Burke, David Lobell, and
Stefano Ermon. Learning to interpret satellite images using wikipedia. In Proceedings of the
Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, 2019."
REFERENCES,0.31650485436893205,"Maya Varma, Laurel Orr, Sen Wu, Megan Leszczynski, Xiao Ling, and Christopher R´e. Cross-
domain data integration for named entity disambiguation in biomedical text.
In Findings of
the Association for Computational Linguistics: EMNLP 2021, pp. 4566–4575, Punta Cana, Do-
minican Republic, November 2021. Association for Computational Linguistics. URL https:
//aclanthology.org/2021.findings-emnlp.388."
REFERENCES,0.31844660194174756,Published as a conference paper at ICLR 2022
REFERENCES,0.32038834951456313,"Julia K Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein, Rainer
Hofmann-Wellenhof, Luc Thomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz, and Holger A
Haenssle. Association Between Surgical Skin Markings in Dermoscopic Images and Diagnos-
tic Performance of a Deep Learning Convolutional Neural Network for Melanoma Recognition.
JAMA Dermatol., 155(10):1135, October 2019."
REFERENCES,0.32233009708737864,"Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pﬁster, and Pradeep Raviku-
mar.
On completeness-aware concept-based explanations in deep neural networks.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 20554–20565. Curran Asso-
ciates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/
ecb287ff763c169694f682af52c1f309-Paper.pdf."
REFERENCES,0.32427184466019415,"Chih-Kuan Yeh, Been Kim, Sercan O Arik, Chun-Liang Li, Tomas Pﬁster, and Pradeep Ravikumar.
On Completeness-aware Concept-Based Explanations in Deep Neural Networks. arXiv:1910.
07969 [cs, stat], June 2020b."
REFERENCES,0.3262135922330097,"John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl
Oermann. Variable generalization performance of a deep learning model to detect pneumonia in
chest radiographs: a cross-sectional study. PLoS medicine, 15(11):e1002683, 2018."
REFERENCES,0.32815533980582523,"Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating Unwanted Biases with Ad-
versarial Learning. Association for the Advancement of Artiﬁcial Intelligence (AAAI), January
2018."
REFERENCES,0.3300970873786408,"Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz.
Contrastive learning of medical visual representations from paired images and text.
CoRR,
abs/2010.00747, 2020. URL https://arxiv.org/abs/2010.00747."
REFERENCES,0.3320388349514563,Published as a conference paper at ICLR 2022
REFERENCES,0.3339805825242718,"A
APPENDIX"
REFERENCES,0.3359223300970874,CONTENTS
REFERENCES,0.3378640776699029,"A.1
Examples of Slice Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.33980582524271846,"A.2
Extended Related Work: Survey of Slices in the Wild . . . . . . . . . . . . . . . .
18"
REFERENCES,0.341747572815534,"A.3
Extended Description of Evaluation Framework . . . . . . . . . . . . . . . . . . .
19"
REFERENCES,0.34368932038834954,"A.3.1
Extended Description of Slice Categories . . . . . . . . . . . . . . . . . .
19"
REFERENCES,0.34563106796116505,"A.3.2
SDM Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.34757281553398056,"A.3.3
Extended Description of Trained Models
. . . . . . . . . . . . . . . . . .
21"
REFERENCES,0.34951456310679613,"A.3.4
Extended Description of Synthetic Models
. . . . . . . . . . . . . . . . .
21"
REFERENCES,0.35145631067961164,"A.4
Extended Description of Domino . . . . . . . . . . . . . . . . . . . . . . . . . . .
21"
REFERENCES,0.3533980582524272,"A.4.1
Extended Description of Cross-Modal Embeddings . . . . . . . . . . . . .
21"
REFERENCES,0.3553398058252427,"A.4.2
Extended Description of Error-Aware Mixture Model . . . . . . . . . . . .
23"
REFERENCES,0.3572815533980582,"A.4.3
Generating a Corpus of Natural Language Descriptions . . . . . . . . . . .
24"
REFERENCES,0.3592233009708738,"A.5
Extended Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25"
REFERENCES,0.3611650485436893,"A.5.1
Extended Analysis of Cross-Modal Embeddings
. . . . . . . . . . . . . .
25"
REFERENCES,0.36310679611650487,"A.5.2
Extended Analysis of Error-Aware Mixture Model
. . . . . . . . . . . . .
25"
REFERENCES,0.3650485436893204,"A.5.3
Extended Analysis of Natural Language Descriptions . . . . . . . . . . . .
26"
REFERENCES,0.36699029126213595,"A.5.4
Future Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26"
REFERENCES,0.36893203883495146,"A.6
Glossary of Notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27"
REFERENCES,0.37087378640776697,Published as a conference paper at ICLR 2022
REFERENCES,0.37281553398058254,"A.1
EXAMPLES OF SLICE DESCRIPTIONS"
REFERENCES,0.37475728155339805,"In this section, we provide examples of the natural language slice descriptions generated by Domino.
Figure 5 includes natural language descriptions and representative photos for discovered slices in the
natural image domain, Figure 6 includes natural language descriptions in the medical image domain,
and Figure 7 includes natural language descriptions in the medical time-series domain."
REFERENCES,0.3766990291262136,a photo of a woman with brown hair
REFERENCES,0.3786407766990291,a photo of a woman smiling warmly
REFERENCES,0.38058252427184464,a photo of a woman with a smile
REFERENCES,0.3825242718446602,a photo of a smiling woman
REFERENCES,0.3844660194174757,a photo of a girl with brownish hair
REFERENCES,0.3864077669902913,a photo of a woman with brown hair
REFERENCES,0.3883495145631068,a photo of a girl with brownish hair
REFERENCES,0.39029126213592236,a photo of a girl with brown hair
REFERENCES,0.39223300970873787,a photo of a girl with reddish hair
REFERENCES,0.3941747572815534,a photo of a woman with reddish hair
REFERENCES,0.39611650485436894,a photo of a man spectacled
REFERENCES,0.39805825242718446,a photo of a man with tinted glasses
REFERENCES,0.4,a photo of a man with refractive glasses
REFERENCES,0.40194174757281553,a photo of a guy with tinted glasses
REFERENCES,0.40388349514563104,a photo of a man in aviator sunglasses
REFERENCES,0.4058252427184466,"correlation
slice category slice"
REFERENCES,0.4077669902912621,"correlation
slice category
blonde hair
target
smiling
slice"
REFERENCES,0.4097087378640777,"correlation
slice category
wearing earrings 
target"
REFERENCES,0.4116504854368932,"brown hair
slice"
REFERENCES,0.41359223300970877,"black hair
target
black hair
target"
REFERENCES,0.4155339805825243,eyeglasses
REFERENCES,0.4174757281553398,a photo of submarine
REFERENCES,0.41941747572815535,a photo of underway
REFERENCES,0.42135922330097086,a photo of sub
REFERENCES,0.42330097087378643,a photo of warship
REFERENCES,0.42524271844660194,a photo of navy
REFERENCES,0.42718446601941745,"rare
slice category
vehicle
target"
REFERENCES,0.429126213592233,"submarine
slice"
REFERENCES,0.43106796116504853,a photo of mosque
REFERENCES,0.4330097087378641,a photo of imam
REFERENCES,0.4349514563106796,a photo of tripoli
REFERENCES,0.4368932038834951,a photo of caliphate
REFERENCES,0.4388349514563107,a photo of caliph
REFERENCES,0.4407766990291262,"rare
slice category
building
target"
REFERENCES,0.44271844660194176,"mosque
slice"
REFERENCES,0.4446601941747573,"rare
slice category
food
target"
REFERENCES,0.44660194174757284,"banana
slice"
REFERENCES,0.44854368932038835,a photo of banana
REFERENCES,0.45048543689320386,a photo of potassium
REFERENCES,0.4524271844660194,a photo of primate
REFERENCES,0.45436893203883494,a photo of eel
REFERENCES,0.4563106796116505,a photo of anarctica
REFERENCES,0.458252427184466,"noisy label
slice category
building
target"
REFERENCES,0.4601941747572815,"barn
slice"
REFERENCES,0.4621359223300971,"vehicle
target"
REFERENCES,0.4640776699029126,"locomotive
slice"
REFERENCES,0.46601941747572817,"food
target"
REFERENCES,0.4679611650485437,"beverage
slice"
REFERENCES,0.46990291262135925,"noisy label
slice category
noisy label
slice category"
REFERENCES,0.47184466019417476,run_id=19
REFERENCES,0.47378640776699027,a photo of barn
REFERENCES,0.47572815533980584,a photo of winter
REFERENCES,0.47766990291262135,a photo of snow
REFERENCES,0.4796116504854369,a photo of snowfall
REFERENCES,0.4815533980582524,a photo of farmhouse
REFERENCES,0.48349514563106794,a photo of locomotive
REFERENCES,0.4854368932038835,a photo of railway
REFERENCES,0.487378640776699,a photo of railroad
REFERENCES,0.4893203883495146,a photo of train
REFERENCES,0.4912621359223301,a photo of electrification
REFERENCES,0.49320388349514566,a photo of drink
REFERENCES,0.49514563106796117,a photo of coffee
REFERENCES,0.4970873786407767,a photo of beverage
REFERENCES,0.49902912621359224,a photo of milk
REFERENCES,0.5009708737864078,a photo of drinking
REFERENCES,0.5029126213592233,"Figure 5: Domino produces natural language descriptions of discovered slices. Natural language
descriptions for discovered slices in (top row) 3 settings randomly selected from the set of the 85
rare slice, natural image settings where Domino includes the exact name of the slice in its top 5
slice descriptions; (middle row) 3 settings randomly selected from the set of the 45 correlation
slice, natural image settings where Domino includes the exact name of the slice in its top 5 slice
descriptions and precision-at-25 exceeds 0.8; and (bottom row) 3 settings randomly selected from
the set of the 95 noisy label slice, natural image settings where Domino includes the exact name
of the slice in its top 5 slice descriptions and precision-at-25 exceeds 0.8. The length of the bars
beneath each description are proportional to the dot product score for the description (see Section
5.3). Also shown are the top 3-4 images that Domino associates with the discovered slice."
REFERENCES,0.5048543689320388,Published as a conference paper at ICLR 2022
REFERENCES,0.5067961165048543,"rare
slice category"
REFERENCES,0.5087378640776699,"noisy label
slice category"
REFERENCES,0.5106796116504855,"cardiac condition
target"
REFERENCES,0.512621359223301,"cardiomegaly
slice
...mild cardiomegaly..."
REFERENCES,0.5145631067961165,...heart is mildly enlarged...
REFERENCES,0.516504854368932,...the heart remains mildly enlarged...
REFERENCES,0.5184466019417475,...heart size remains moderately enlarged...
REFERENCES,0.5203883495145631,...the heart is mildly enlarged...
REFERENCES,0.5223300970873787,"correlation
slice category"
REFERENCES,0.5242718446601942,... dialysis catheter  terminates ...
REFERENCES,0.5262135922330097,"pleural effusion
target"
REFERENCES,0.5281553398058253,"support devices
slice"
REFERENCES,0.5300970873786408,...enteric tube is again noted...
REFERENCES,0.5320388349514563,...sternotomy wires are noted…
REFERENCES,0.5339805825242718,…a dual lead pacemaker...
REFERENCES,0.5359223300970873,…right-sided port-a-cath is again seen…
REFERENCES,0.537864077669903,"lung condition
target"
REFERENCES,0.5398058252427185,"atelectasis
slice...there is trace bibasilar atelectasis..."
REFERENCES,0.541747572815534,...with overlying atelectasis...
REFERENCES,0.5436893203883495,...felt  to most likely represent atelectasis...
REFERENCES,0.545631067961165,...mild right basilar atelectasis...
REFERENCES,0.5475728155339806,...with overlying atelectasis...
REFERENCES,0.5495145631067961,"rare
slice category"
REFERENCES,0.5514563106796116,"pleural condition
target"
REFERENCES,0.5533980582524272,"pneumothorax
slice
...there is no...pneumothorax…"
REFERENCES,0.5553398058252427,...there is no focal consolidation or pneumothorax...
REFERENCES,0.5572815533980583,"...there is no...effusion, or pneumothorax..."
REFERENCES,0.5592233009708738,...no evidence of pneumothorax...
REFERENCES,0.5611650485436893,...no pleural effusion or pneumothorax...
REFERENCES,0.5631067961165048,"correlation
slice category
lung opacity
target"
REFERENCES,0.5650485436893203,"pleural effusion
slice...improving pleural effusion on the right..."
REFERENCES,0.566990291262136,...basal pleural effusions are noted...
REFERENCES,0.5689320388349515,...small bilateral pleural effusions are  persistent...
REFERENCES,0.570873786407767,...and absence of pleural effusions...
REFERENCES,0.5728155339805825,...there is no significant interval change...
REFERENCES,0.574757281553398,"noisy label
slice category
lung condition
target"
REFERENCES,0.5766990291262136,"atelectasis
slice...mild bibasilar atelectasis slightly improved..."
REFERENCES,0.5786407766990291,...there is mild left basal atelectasis...
REFERENCES,0.5805825242718446,...probable bibasilar atelectasis...
REFERENCES,0.5825242718446602,…mild bibasilar atelectasis…
REFERENCES,0.5844660194174758,…likely reflect areas of atelectasis…
REFERENCES,0.5864077669902913,"Synthetic Model
Trained Model"
REFERENCES,0.5883495145631068,"Figure 6: Domino produces natural language descriptions for discovered slices in a medical
image dataset. Here, we provide the top ﬁve natural language descriptions for discovered slices in
(top row) two rare slice settings, (middle row) two correlation slice settings, and (bottom row) two
noisy label slice settings. Colored bars represent accurate slice descriptions, and gray bars represent
incorrect descriptions. Note that in our medical examples, the vocabulary consists of physician
reports in the training set; since, we are unable to provide the full-text reports due to patient privacy
concerns, the ﬁgure includes relevant fragments of reports. The length of the bars beneath each
description are proportional to the dot product score for the description (see Section 5.3)"
REFERENCES,0.5902912621359223,...25 year old healthy man...
REFERENCES,0.5922330097087378,...24 hour video-EEG recording...
REFERENCES,0.5941747572815534,...correlating seizures noted...
REFERENCES,0.596116504854369,...ex-term female with seizure...
REFERENCES,0.5980582524271845,...7 y/o ex-term male with...
REFERENCES,0.6,...15 month old baby...
REFERENCES,0.6019417475728155,"...intractable epilepsy...
...2 year old boy..."
REFERENCES,0.6038834951456311,...tonic clonic movements...
REFERENCES,0.6058252427184466,...awake/asleep EEG...
REFERENCES,0.6077669902912621,...15 month old baby...
REFERENCES,0.6097087378640776,"...intractable epilepsy...
...17 yo girl ex-35 weeker with IVH..."
REFERENCES,0.6116504854368932,...tonic clonic movements...
REFERENCES,0.6135922330097088,...a seizure lasting less than a minute...
REFERENCES,0.6155339805825243,"rare
slice category"
REFERENCES,0.6174757281553398,"seizure
target"
REFERENCES,0.6194174757281553,"age
slice"
REFERENCES,0.6213592233009708,"seizure
target"
REFERENCES,0.6233009708737864,"age
slice"
REFERENCES,0.625242718446602,"seizure
target"
REFERENCES,0.6271844660194175,"age
slice"
REFERENCES,0.629126213592233,"correlation
slice category
noisy label
slice category"
REFERENCES,0.6310679611650486,"Figure 7: Domino produces natural language explanations for discovered slices in a medical
time-series dataset. Natural language descriptions for discovered slices in 3 of our EEG slice
discovery settings. In all three settings, the model is trained to detect seizures and underperforms
on the slice of young patients. The three settings span all three slice categories and in each Domino
describes the slice with reports mentioning young age. Note that the description corpus consists
of physician reports in the training set; however, we are unable to provide the full-text reports due
to patient privacy concerns, so the table includes only relevant fragments. The bar lengths beneath
each description are proportional to the dot product score for the description (see Section 5.3)"
REFERENCES,0.6330097087378641,Published as a conference paper at ICLR 2022
REFERENCES,0.6349514563106796,"A.2
EXTENDED RELATED WORK: SURVEY OF SLICES IN THE WILD"
REFERENCES,0.6368932038834951,"Several recent studies have shown that machine learning models often make systematic errors on
critical data slices. In this section, we provide a survey of underperforming slices documented in the
literature."
REFERENCES,0.6388349514563106,"• Skin Lesion Classiﬁcation (Correlation Slice): Bissoto et al. (2019) reveal that models
trained to classify skin lesion images depend on clinically irrelevant information due to
biases in the training data. Speciﬁcally, a computer vision model trained to classify skin
lesions performs poorly on images of malignant lesions with color charts (i.e. colored
bandages), since color charts more commonly appear with benign lesions."
REFERENCES,0.6407766990291263,"• Melanoma Detection (Correlation Slice): A study by Winkler et al. (2019) showed that
melanoma detection models trained on dermascopic image datasets often rely on the pres-
ence of surgical skin markings when making predictions. Since dermatologists mark sus-
picious lesions during clinical practice with a gentian violet surgical skin marker, dermas-
copic images will often include skin markings, causing models to learn spurious correla-
tions between markings and the presence of melanoma. Models then underperform on the
slice of lesions without markings."
REFERENCES,0.6427184466019418,"• Pneumothorax Detection (Correlation Slice): Models trained to detect the presence of
pneumothorax (collapsed lungs) have been found to rely on the presence of chest drains, a
device used during treatment (Oakden-Rayner et al., 2019)."
REFERENCES,0.6446601941747573,"• Hip-Fracture Detection (Rare Slice): Due to the low prevalence of cervical fractures in a
pelvic X-ray dataset collected from the Royal Adelaide Hospital, computer vision models
trained to detect fractures underperform on this slice (Oakden-Rayner et al., 2019)."
REFERENCES,0.6466019417475728,"• Hip-Fracture Detection (Correlation Slice): Badgeley et al. (2019) show that the per-
formance of models trained to detect hip fractures from X-rays are sensitive to multiple
patient-speciﬁc and hospital-speciﬁc attributes. In particular, when the test distribution is
subsampled to remove correlations with patient attributes (age, gender, BMI, pain, fall) and
hospital attributes (scanner, department, radiation, radiologist name, order time), the model
performance drops to close to random."
REFERENCES,0.6485436893203883,"• Gender Classiﬁcation in Images (Rare Slice): Buolamwini & Gebru (2018) demon-
strated that facial analysis datasets are often composed primarily of lighter-skinned subjects
(based on Fitzpatrick Skin Types) and as a result, three commercial gender classiﬁcation
systems systematically underperform on rare subgroups (e.g. darker faces, female faces)."
REFERENCES,0.6504854368932039,"• COVID-19 Detection in Chest X-rays (Correlation Slice): DeGrave et al. (2021) re-
veal that some models trained to detect COVID-19 from radiographs do not generalize to
datasets from external hospitals, indicating that the models rely on source-speciﬁc attributes
instead of pathology markers."
REFERENCES,0.6524271844660194,"• Pneumonia Detection in Chest X-rays (Correlation Slice): Zech et al. (2018) evalu-
ated pneumonia screening CNNs on three external hospitals, and found that performance
on the external hospitals was signiﬁcantly lower than the original hospital dataset. Addi-
tionally, the CNNs were able to very accurately classify the hospital system and depart-
ment where the radiographs were acquired, indicating that the CNN features had learned
hospital-speciﬁc confounding variables."
REFERENCES,0.654368932038835,"• Weakly Supervised Aortic Valve Malformation Classiﬁcation (Noisy Label Slice):
Weak supervision is commonly used in medical machine learning practice to label clin-
ical datasets. Using a set of labeling functions, Fries et al. (2019) train a weakly-supervised
model to classify aortic valve malformations. They note that noise in the labels is induced
by labeling functions. These may systematically miss coherent slices of data, meaning that
a model trained on these labels may underperform on those slices."
REFERENCES,0.6563106796116505,"• Predicting Gender from Photos of the Iris (Correlation Slice): Several studies have
reported models capable of predicting a person’s gender from a photo of their iris Bansal
et al. (2012); Tapia et al. (2016). However, Kuehlkamp et al. (2017) show that these models
may be relying on the presence of mascara to make these predictions. This suggests that
performance will likely be degraded on the slice of females without mascara and males
with mascara."
REFERENCES,0.658252427184466,Published as a conference paper at ICLR 2022
REFERENCES,0.6601941747572816,"• Speech Recognition (Rare Slice): Koenecke et al. (2020) demonstrated that automated
speech recognition systems have large performance disparities between white and African
American speakers. The disparities were traced to the race gap in the corpus used to train
the model, indicating that African American speakers were a rare slice."
REFERENCES,0.6621359223300971,"• Object Recognition (Rare Slice): A study by de Vries et al. (2019) demonstrated that pub-
licly available object recognition algorithms often systematically underperform on house-
hold items that commonly occur in non-Western countries and low-income communities.
This is likely due to objects appearing in different environments as well as differences in
object appearance."
REFERENCES,0.6640776699029126,"• Named Entity Disambiguation (Rare Slice): Named entity disambiguation (NED) sys-
tems, which map textual mentions to structured entities, play a critical role in automated
text parsing pipelines. Several studies have demonstrated that NED systems underperform
on rare entities that occur infrequently in the training data (Orr et al., 2020; Varma et al.,
2021)."
REFERENCES,0.6660194174757281,"A.3
EXTENDED DESCRIPTION OF EVALUATION FRAMEWORK"
REFERENCES,0.6679611650485436,"Domain
Rare Settings
Correlation
Settings"
REFERENCES,0.6699029126213593,"Noisy Label
Settings"
REFERENCES,0.6718446601941748,Total settings
REFERENCES,0.6737864077669903,"Natural
Images"
REFERENCES,0.6757281553398058,"177 (trained)
197 (synthetic)"
REFERENCES,0.6776699029126214,"520 (trained)
520 (synthetic)"
REFERENCES,0.6796116504854369,"287 (trained)
394 (synthetic)"
REFERENCES,0.6815533980582524,"984 (trained)
1, 111 (synthetic)
MIMIC
15 (trained)
55 (synthetic)"
REFERENCES,0.683495145631068,"176 (trained)
352 (synthetic)"
REFERENCES,0.6854368932038835,"30 (trained)
55 (synthetic)"
REFERENCES,0.6873786407766991,"221 (trained)
462 (synthetic)
EEG
10 (trained)
10 (synthetic)"
REFERENCES,0.6893203883495146,"10 (trained)
10 (synthetic)"
REFERENCES,0.6912621359223301,"10 (trained)
10 (synthetic)"
REFERENCES,0.6932038834951456,"30 (trained)
30 (synthetic)"
REFERENCES,0.6951456310679611,"Table 1: Overview of evaluation framework. We evaluate SDMs across 1,235 (trained) slice
discovery settings across three domains, four slice categories, and ﬁve slice parameters (α)."
REFERENCES,0.6970873786407767,"A.3.1
EXTENDED DESCRIPTION OF SLICE CATEGORIES"
REFERENCES,0.6990291262135923,"In Section 4.1.1, we describe how we categorize each slice discovery setting based on the underlying
reason that the model hθ exhibits degraded performance on the slices S. We survey the literature
for examples of underperforming slices in the wild, which we document in Section A.2. Based on
our survey and prior work (Oakden-Rayner et al., 2019), we identify three popular slice types. We
provide expanded descriptions below."
REFERENCES,0.7009708737864078,"Rare slice. Consider a slice S ∈{0, 1} (e.g. patients with a rare disease, photos taken at night) that
occurs infrequently in the training set (i.e. P(S = 1) < α for some small α). Since a rare slice will
not signiﬁcantly affect model loss during training, the model may fail to learn to classify examples
within the slice. To generate settings with rare slices, we use base datasets with hierarchical label
schema (e.g. ImageNet (Deng et al., 2009)). We construct dataset D such that for a given class label
Y , the elements in subclass C occur with proportion α, where α ranges between 0.01 and 0.1."
REFERENCES,0.7029126213592233,"Correlation slice. If the target variable Y (e.g. pneumothorax) is correlated with another variable
C (e.g. chest tubes), the model may learn to rely on C to make predictions. This will induce
a slice S = 1[C ̸= Y ] (e.g. pneumothorax without chest tubes and normal with chest tubes)
with degraded performance. To generate settings with correlation slices, we use base datasets with
metadata annotations (e.g. CelebA (Liu et al., 2015)). We sub-sample the base dataset Dbase such
that the resulting dataset D exhibits a linear correlation of strength α between the target variable and
another metadata label C. Here, α ranges between 0.2 and 0.8."
REFERENCES,0.7048543689320388,"We now describe our procedure for subsampling the base dataset to achieve a desired correlation.
Assume we have two binary variables Y, C ∈{0, 1} (Bernoulli random variables) and a dataset of
Dbase = {(yi, ci)}nbase
i=1 . Given a target correlation α, we would like to subsample the dataset Dbase
such that the resulting dataset D = (yi, ci)n
i=1of size n exhibits a sample correlation between Y and
C of α."
REFERENCES,0.7067961165048544,Published as a conference paper at ICLR 2022
REFERENCES,0.7087378640776699,"The population correlation between Y and C is given by α(Y, C) =
cov(Y,C)"
REFERENCES,0.7106796116504854,"σY σC
and cov(Y, C) =
E[Y C] −E[Y ]E[C]. For a sample, the unbiased estimator of the covariance is:"
REFERENCES,0.7126213592233009,"cov(y, c) =
1
n −1 n
X"
REFERENCES,0.7145631067961165,"i=1
(yi −¯y)(ci −¯c) =
1
n −1 
n
X"
REFERENCES,0.7165048543689321,"i=1
yici −¯c¯y
"
REFERENCES,0.7184466019417476,"Since we know Y and C are Bernoulli random variables, we can express this in terms of variables
like ny=1,c=1 (i.e. the number of samples i where yi = 1 and ci = 1) and ny=1(i.e. the number of
samples i where yi=1)."
REFERENCES,0.7203883495145631,"cov(y, c) =
1
n −1"
REFERENCES,0.7223300970873786,"
ny=1,c=1 −ny=1nc=1 n "
REFERENCES,0.7242718446601941,"The correlation coefﬁcient can then be expressed in terms of this covariance and the sample standard
deviations sy and sc:"
REFERENCES,0.7262135922330097,"r = ny=1,c=1 −ny=1nc=1"
REFERENCES,0.7281553398058253,"n
(n −1)sysc"
REFERENCES,0.7300970873786408,"In addition to supplying a target correlation α, assume we also have target means µa and µb (these
could be the sample means in the original dataset D for example) and a target sample size n. Since
Y and C are Bernoulli random variables, we can compute sample standard deviations as sy =
µa(1 −µa) and sc = µb(1 −µb). We can then derive simple formulas for computing the desired
values needed to properly subsample the data:"
REFERENCES,0.7320388349514563,"ny=1 = µan
nc=1 = µbn"
REFERENCES,0.7339805825242719,"ny=1,c=1 = α(n −1)sysc + ny=1nc=1"
REFERENCES,0.7359223300970874,"n
ny=1,c=0 = ny=1 −ny=1,c=1
ny=0,c=1 = nc=1 −ny=1,c=1
ny=0,c=0 = n −(ny=1 + nc=1 −ny=1,c=1)"
REFERENCES,0.7378640776699029,"Noisy label slice. Errors in labels are not always distributed uniformly across the training distri-
bution. A slice of data S ∈{0, 1} may exhibit higher label error rates than the rest of the training
distribution. This could be due to a number of different factors: classes may be ambiguous (e.g. an-
notators labeling sandwiches may disagree whether to include hot dogs), labeling heuristics may fail
on important slices (e.g. medical imaging heuristics that only activate on images from one scanner
type), or human annotators may lack expertise on certain subsets (e.g. annotators from one country
labeling street signs in another). A model hθ trained on these labels will likely exhibit degraded per-
formance on S. To generate settings with noisy label slices, we can use a base dataset with metadata
annotations (e.g. CelebA (Liu et al., 2015)). We construct dataset D such that for each class label
Y , the elements in subclass C exhibit label noise with probability α, where α ranges between 0.01
and 0.3."
REFERENCES,0.7398058252427184,"A.3.2
SDM IMPLEMENTATIONS"
REFERENCES,0.7417475728155339,"Spotlight. d’Eon et al. (2021) search for an underperforming slice by maximizing the expected loss
with respect to a multivariate Gaussian with spherical covariance. We use the authors’ implemen-
tation provided at https://github.com/gregdeon/spotlight. We enforce a minimum
spotlight size equal to 2% of the validation data as recommended in the paper. We use a initial learn-
ing rate of 1 × 10−3 (the default in the implementation) and apply the same annealing and barriers
as the authors. We optimize for 1,000 steps per spotlight (the default in the implementation)."
REFERENCES,0.7436893203883496,"GEORGE. Sohoni et al. (2020) propose identifying underperforming slices by performing class
conditional clustering on a U-MAP reduction of the embeddings. We use the implementation pro-
vided at https://github.com/HazyResearch/hidden-stratification."
REFERENCES,0.7456310679611651,Published as a conference paper at ICLR 2022
REFERENCES,0.7475728155339806,"Multiaccuracy Boost. To identify slices where the model hθ is systematically making mistakes,
Kim et al. (2018) use ridge regression to learn a function f : Rd →R+ mapping from an example’s
embedding zi ∈Rd to the partial derivative of the cross entropy loss with respect to the prediction
∂ℓ(hθ(xi), yi)"
REFERENCES,0.7495145631067961,"∂hθ(xi)
=
1
1 −hθ(xi) −yi
.
(2)"
REFERENCES,0.7514563106796116,"Because this function grows as the absolute value of the residual |hθ(xi) −yi| grows, a good f
should correlate with the residual."
REFERENCES,0.7533980582524272,"In order to discover multiple distinct slices, the authors repeat this process ˆk times updating the
model predictions on each iteration according to"
REFERENCES,0.7553398058252427,"h(j+1)
θ
(xi) ∝e−ηf(zi)h(j)
θ (xi),
(3)
where η is a hyperparameter deﬁning the step size for the update."
REFERENCES,0.7572815533980582,"We use an implementation of Multiaccuracy Boost based on the authors’, which was released at
https://github.com/amiratag/MultiAccuracyBoost. We use η = 0.1 as in the
authors’ implementation. We ﬁt f on 70% of the validation data and use the remaining 30% for
evaluating the correlation with the residual. The authors use the same ratio in their implementation."
REFERENCES,0.7592233009708738,"Confusion SDM. A simple, embedding-agnostic way to identify underperforming subgroups is to
simply inspect the cells of the confusion matrix. We include this important baseline to determine
when more complicated slice discovery techniques are actually useful."
REFERENCES,0.7611650485436893,"A.3.3
EXTENDED DESCRIPTION OF TRAINED MODELS"
REFERENCES,0.7631067961165049,"In Section 4.1.2, we discuss training a distinct model hθ for each slice discovery setting. In this
section we provide additional details on model training."
REFERENCES,0.7650485436893204,"For our natural image settings and medical image settings, we used a ResNet-18 randomly ini-
tialized with He initialization (He et al., 2015; 2016). We applied an Adam optimizer with learning
rate 1 × 10−4 for 10 epochs and use early stopping using the validation dataset (Kingma & Ba,
2017). During training, we randomly crop each image, resize to 224 × 224, apply a random hori-
zontal ﬂip, and normalize using ImageNet mean and standard deviation (µ = [0.485, 0.456, 0.406],
σ = [0.229, 0.224, 0.225]). During inference, we resize to 256 × 256, apply a center crop of size
224 × 224 and normalize with the same mean and standard deviation as in training."
REFERENCES,0.7669902912621359,"For our medical time series settings, we use a densely connected inception convolution neural
network (Roy et al., 2019) randomly initialized with He initialization (He et al., 2015; 2016). Since
the EEG signals are sampled at 200 Hz, and the EEG clip length is 12 seconds, with 19 EEG
electrodes, the input EEG has shape 19 × 2400. The models are trained with a learning rate of 10−6
and a batch size of 16 for 15 epochs."
REFERENCES,0.7689320388349514,"A.3.4
EXTENDED DESCRIPTION OF SYNTHETIC MODELS"
REFERENCES,0.7708737864077669,"In Section 4.1.2, we discuss how synthesizing model predictions in order to provide greater control
over the evaluation. Here, we provide additional details describing this process."
REFERENCES,0.7728155339805826,"Assume a binary class label Y ∈{0, 1} and a slice variable S ∈{0, 1}. We sample the predicted
probability ˆY ∈[0, 1] from one of four beta distributions, conditional on Y and S. The parameters
of those four beta distributions are set so as to satisfy a desired speciﬁcity and sensitivity in the slice
(i.e. when S = 1) and out of the slice (i.e. when S = 0)."
REFERENCES,0.7747572815533981,"For natural image settings, we set both speciﬁcity and sensitivity to 0.4 in the slice and 0.75 out of
the slice. For medical image and time series settings, we set both speciﬁcity and sensitivity to 0.4 in
the slice and 0.8 out of the slice."
REFERENCES,0.7766990291262136,"A.4
EXTENDED DESCRIPTION OF DOMINO"
REFERENCES,0.7786407766990291,"A.4.1
EXTENDED DESCRIPTION OF CROSS-MODAL EMBEDDINGS"
REFERENCES,0.7805825242718447,"Here, we provide implementation details for the four cross-modal embeddings used in this work:
CLIP, ConVIRT, MIMIC-CLIP, and EEG-CLIP. Domino relies on the assumption that we have ac-"
REFERENCES,0.7825242718446602,Published as a conference paper at ICLR 2022
REFERENCES,0.7844660194174757,"cess to either (a) pretrained cross-modal embedding functions or (b) a dataset with paired input-text
data that can be used to learn embedding functions."
REFERENCES,0.7864077669902912,"Large-scale pretrained cross-modal embedding functions can be used to generate accurate represen-
tations of input examples. For instance, if our inference dataset consists of natural images, we can
use a pre-trained CLIP model as embedding functions ginput and gtext to obtain image embeddings
that lie in the same latent representation space as word embeddings."
REFERENCES,0.7883495145631068,"However, pre-trained cross-modal embeddings are only useful if the generated representations accu-
rately represent the inputs in the inference dataset. For example, if the inference dataset consists of
images from specialized domains (i.e. x-rays) or non-image inputs, CLIP is likely to generate poor
representations."
REFERENCES,0.7902912621359224,"If pre-trained cross-modal embeddings are not available or cannot effectively represent the inference
dataset, we require access to a separate dataset that can be used to learn the cross-modal embedding
functions ginput and gtext; we will refer to this dataset as the CM-training dataset for the remainder of
this work. The CM-training dataset must consist of paired input-text data (e.g. image-caption pairs
or radiograph-report pairs). Further, we assume that the text data provides a sufﬁcient description
of the input and includes information about potential correlates, such as object attributes or subject
demographics. Note that paired input-text data is only required for the CM-training dataset; we
make no such assumptions about the slice discovery dataset."
REFERENCES,0.7922330097087379,We use the following four cross-modal embeddings in our analysis:
REFERENCES,0.7941747572815534,"• CLIP (Natural Images): CLIP embeddings are cross-modal representations generated
from a large neural network trained on 400 million image-text pairs (Radford et al., 2021)."
REFERENCES,0.7961165048543689,"• ConVIRT (Medical Images): ConVIRT embeddings are generated from pairs of chest X-
rays and radiologist reports in the MIMIC-CXR dataset. We create a CM-training set with
70% of the subjects in MIMIC-CXR, ensuring that no examples in our training set occur
in validation or test data at slice discovery time. We then replicate the training procedure
detailed in Zhang et al. (2020), which uses contrastive learning to align embeddings. We
use the implementation provided in ViLMedic (Delbrouck et al., 2022)."
REFERENCES,0.7980582524271844,"• MIMIC (Medical Images): We generate a separate set of cross-modal embeddings for the
MIMIC-CXR dataset using a variant of the CLIP training procedure (Radford et al., 2021).
We use the same CM-training set as detailed above, with 89,651 image and report pairs. In
order to generate text representations, we extract the ﬁndings and impressions sections from
radiologist reports, which then serve as input to a BERT-based transformer initialized with
weights from CheXBert and then frozen (Smit et al., 2020). Chest x-rays are passed as input
to a visual transformer (ViT) pre-trained on ImageNet-21k and ImageNet 2012 at an image
resolution of 224 × 224 (Dosovitskiy et al., 2021). All images are resized to 224 × 224 and
normalized using mean and standard deviation values from ImageNet. Text representations
are extracted from the output of the [CLS] token and image representations are extracted
from the output of the ﬁnal model layer. Image representation and text representations are
separately passed through projection layers consisting of fully-connected layers and non-
linear activation functions as detailed in an open-source implementation of CLIP3. Finally,
we align text and image representations using the InfoNCE loss function with in-batch
negatives as detailed in Radford et al. (2021). We train our implementation for 30 epochs
with a learning rate of 10−4, a batch size of 64, and an embedding dimension of 256. The
training process comes to an early stop if the loss fails to decrease for ten epochs."
REFERENCES,0.8,"• EEG (Medical Time-Series Data): In order to generate cross-modal embeddings for EEG
readings and associated neurologist reports, we modify the CLIP training procedure to work
with time-series data (Radford et al., 2021; Saab et al., 2020). We create a CM-training
set with 6,254 EEG signals and associated neurologist reports. We use the same EEG
encoder described in Section A.3.3. In order to represent text representations, we extract
the ﬁndings and narrative from the reports, which are then served as input to a BERT-
based transformer initialized with weights from CheXBert (Smit et al., 2020). Text and
EEG representations are aligned using the InfoNCE loss function with in-batch negatives
as described in (Radford et al., 2021). We also add a binary cross-entropy classiﬁcation loss"
REFERENCES,0.8019417475728156,3https://github.com/moein-shariatnia/OpenAI-CLIP
REFERENCES,0.8038834951456311,Published as a conference paper at ICLR 2022
REFERENCES,0.8058252427184466,"for seizure classiﬁcation, where we weigh the InfoNCE loss by 0.9, and the cross-entropy
loss by 0.1. The cross-modal model is trained with a learning rate of 10−6, an embedding
dimension of 128, and a batch size of 32 for 200 epochs."
REFERENCES,0.8077669902912621,"A.4.2
EXTENDED DESCRIPTION OF ERROR-AWARE MIXTURE MODEL"
REFERENCES,0.8097087378640777,"In Section 5.2, we describe a mixture model that jointly models the input embeddings, class labels,
and model predictions. Here, we provide an expanded description of the model and additional
implementation details."
REFERENCES,0.8116504854368932,"Motivation and relation to prior work. Recall from Section 3 that our goal is to ﬁnd a set of ˆk
slicing functions that partition our data into subgroups. This task resembles a standard unsupervised
clustering problem but differs in an important way: we are speciﬁcally interested in ﬁnding clus-
ters where the model makes systematic prediction errors. It is not immediately obvious how this
constraint can be incorporated into out-of-the-box unsupervised clustering methods, such as princi-
pal component analysis or Gaussian mixture models. One potential approach would be to calculate
the model loss on each example xi in Dv, append each loss value to the corresponding embedding
ginput(xi), and cluster with standard methods (Sohoni et al., 2020). Empirically, we ﬁnd that this
approach often fails to identify underperforming slices, likely because the loss element is drowned
out by the other dimensions of the embedding."
REFERENCES,0.8135922330097087,"Recently, d’Eon et al. (2021) proposed Spotlight, an algorithm that searches the embedding space
for contiguous regions with high-loss. Our error-aware mixture model is inspired by Spotlight, but
differs in several important ways:"
REFERENCES,0.8155339805825242,"• Our model partitions the entire space, ﬁnding both high and low performing slices. In
contrast, Spotlight searches only for regions with high loss. Spotlight introduces the “spot-
light size” hyperparameter, which lower bounds the number of examples in the slice and
prevents Spotlight from identifying very small regions.
• Because we model both the class labels and predictions directly, our error-aware mixture
model tends to discover slices that are homogeneous with respect to error type. On the other
hand, Spotlight’s objective is based on the loss, a function of the labels and predictions that
makes false positives and false negatives indistinguishable (assuming cross-entropy).
• d’Eon et al. (2021) recommend using a spherical covariance matrix, Σ = aI with a ∈R,
when using Spotlight because it made their Adam optimization much faster and seemed
to produce good results. In contrast, we use a diagonal covariance matrix of the form
Σ = diag(a1, a2, ...aˆk). Fitting our mixture model with expectation maximization remains
tractable even with these more ﬂexible parameters."
REFERENCES,0.8174757281553398,"Additional implementation details. The mixture model’s objective encourages both slices with a
high concentration of mistakes as well as slices with a high concentration of correct predictions.
However, in the slice discovery problem described in Section 3, the goal is only to identify slices of
mistakes (i.e. slices that exhibit degraded performance with respect to some model hθ). To reconcile
the model’s objective with the goal of slice discovery, we model ¯k > ˆk slices and then select ˆk slices
with the highest concentrations of mistakes. Speciﬁcally, we model ¯k = 25 slices and return the top
ˆk clusters with the largest absolute difference between ˆp and p, Pc
i=1 |ˆpi −pi|)."
REFERENCES,0.8194174757281554,"In practice, when d is large (e.g. d > 256), we ﬁrst reduce the dimensionality of the embedding to
d = 128 using principal component analysis, which speeds up the optimization procedure signiﬁ-
cantly."
REFERENCES,0.8213592233009709,"We include an important hyperparameter γ ∈R+ that balances the importance of modeling the class
labels and predictions against the importance of modeling the embedding. The log-likelihood over
n examples is given as follows and maximized using expectation-maximization:"
REFERENCES,0.8233009708737864,"ℓ(φ) = n
X"
REFERENCES,0.8252427184466019,"i=1
log ¯k
X"
REFERENCES,0.8271844660194175,"j=1
P(S(j) =1)P(Z =zi|S(j) =1)P(Y =yi|S(j) =1)γP( ˆY =hθ(xi)|S(j) =1)γ"
REFERENCES,0.829126213592233,"(4)
In practice, this hyperparameter allows users to balance between the two desiderata highlighted in
Section 1. When γ is large (i.e. γ > 1), the mixture model is more likely to discover underper-"
REFERENCES,0.8310679611650486,Published as a conference paper at ICLR 2022
REFERENCES,0.8330097087378641,"forming slices, potentially at the expense of coherence. On the other hand, when γ is small (i.e.
0 ≤γ < 1), the mixture model is more likely to discover coherent slices, though they may not be
underperforming. In our experiments, we set γ = 10. We encourage users to tweak this parameter
as they explore model errors with Domino, decreasing it when the discovered slices seem incoherent
and increasing it when the discovered slices are not underperforming."
REFERENCES,0.8349514563106796,"We initialize our mixture model using a scheme based on the confusion matrix. Typically, the
parameters of a Gaussian mixture model are initialized with the centroids of a k-means ﬁt. However,
in our error-aware mixture model, we model not only embeddings (as Gaussians), but also labels and
predictions (as categoricals). It is unclear how best to combine these variables into a single k-means
ﬁt. We tried initializing by just applying k-means to the embeddings, but found that this led to
slices that were too heterogeneous with respect to error type (e.g. a mix of false positives and true
positives). Instead, we use initial clusters where almost all of the examples come from the same cell
in the confusion matrix. Formally, at initialization, each slice j is assigned a y(j) ∈Y and ˆy(j) ∈Y
(i.e. each slice is assigned a cell in the confusion matrix). This is typically done in a round-robin
fashion so that there are at least ⌊¯k/|Y|2⌋slices assigned to each cell in the confusion matrix. Then,
we ﬁll in the initial responsibility matrix Q ∈Rn×¯k, where each cell Qij corresponds to our model’s
initial estimate of P(S(j) = 1|Y = yi, ˆY = ˆyi). We do this according to"
REFERENCES,0.8368932038834952,"¯Qij ←
1 + ϵ
yi = y(j) ∧ˆyi = ˆy(j)"
REFERENCES,0.8388349514563107,"ϵ
otherwise
(5)"
REFERENCES,0.8407766990291262,"Qij ←
¯Qij
P¯k
l=1 ¯Qil
(6)"
REFERENCES,0.8427184466019417,"where ϵ is random noise which ensures that slices assigned to the same confusion matrix cell won’t
have the exact same initialization. We sample ϵ uniformly from the range [0, E] where E is a
hyperparameter set to 0.001."
REFERENCES,0.8446601941747572,"A.4.3
GENERATING A CORPUS OF NATURAL LANGUAGE DESCRIPTIONS"
REFERENCES,0.8466019417475729,"In Section 5.3, we describe an approach for generating natural language descriptions of discovered
slices. This approach uses cross-modal embeddings to retrieve descriptive phrases from a large cor-
pus of text Dtext. In this section, we describe how we curate domain-speciﬁc corpora of descriptions
to be used with Domino."
REFERENCES,0.8485436893203884,"First, we solicit a set of phrase templates from a domain expert. For example, since CelebA is a
dataset of celebrity portraits, we use templates like:"
REFERENCES,0.8504854368932039,"a photo of a person [MASK] [MASK]
a photo of a [MASK] woman
...
a [MASK] photo of a man"
REFERENCES,0.8524271844660194,"In our experiments with CelebA, we use a total of thirty templates similar to these (see GitHub for the
full list). In our experiments with ImageNet we use only one template: a photo of [MASK]."
REFERENCES,0.8543689320388349,"Next, we generate a large number of candidate phrases by ﬁlling in the [MASK]tokens using either
(1) a pretrained masked-language model (in our experiments with CelebA we use a BERT base
model (Devlin et al., 2018) and generate 100,000 phrases, keeping the ntext = 10,000) with the
lowest loss) or (2) a programmatic approach (in our experiments with ImageNet, we create one
phrase from each of the ntext = 10,000 most frequently used words on English Wikipedia (Semenov
& Areﬁn, 2019))."
REFERENCES,0.8563106796116505,"For our medical image and time-series datasets, we use corpora of physician reports sourced from
MIMIC-CXR (Johnson et al., 2019) and Saab et al. (2020) with ntext = 159,830 and ntext = 41,258,
respectively."
REFERENCES,0.858252427184466,Published as a conference paper at ICLR 2022
REFERENCES,0.8601941747572815,"A.5
EXTENDED RESULTS"
REFERENCES,0.8621359223300971,"A.5.1
EXTENDED ANALYSIS OF CROSS-MODAL EMBEDDINGS"
REFERENCES,0.8640776699029126,"In Section 6.1, we explore the effect of embedding type on slice discovery performance. Here, we
provide an extended evaluation of our results."
REFERENCES,0.8660194174757282,"We note that slice discovery performance is generally lower across rare slices when compared to
correlation and noisy label slices. This trend is visible for both model types (synthetic and trained)
and all embedding types (unimodal and cross-modal). This is likely due to the nature of the rare
slice setting; since a rare subclass occurs in the dataset with very low frequency, it is difﬁcult for
SDMs to identify the error slice."
REFERENCES,0.8679611650485437,"Slice discovery performance on synthetic models is often higher than performance on trained mod-
els. This is expected because the use of synthetic models allows us to explicitly control performance
on our labeled ground-truth slices, and as a result, Domino can more effectively recover the slice. On
the other hand, trained models are likely to include underperforming, coherent slices that are not la-
beled as “ground-truth”, which may limit the ability of Domino to recover the labeled slice. Trained
models are also likely to exhibit lower slice performance degradations than synthetic models. We
discuss these trade-offs in Section 4.1.2."
REFERENCES,0.8699029126213592,"A.5.2
EXTENDED ANALYSIS OF ERROR-AWARE MIXTURE MODEL"
REFERENCES,0.8718446601941747,"In Section 6.2, we explore how the choice of slicing algorithm affects slice discovery performance.
Here, we provide an extended evaluation of our results."
REFERENCES,0.8737864077669902,Additional experimental results with our error-aware mixture model are shown in Figure 8.
REFERENCES,0.8757281553398059,"Figure 8: Error-aware mixture model enables accurate slice discovery. We show that when
cross-modal embeddings are provided as input, our error-aware mixture model often outperforms
previously-designed SDMs."
REFERENCES,0.8776699029126214,"We note that the naive Confusion SDM demonstrates high performance on correlation slices across
all three datasets, even outperforming our error-aware mixture model in some cases. This ﬁnd-
ing suggests that when a strong correlation exists, simply inspecting the confusion matrix may be
sufﬁcient for effective slice discovery."
REFERENCES,0.8796116504854369,Published as a conference paper at ICLR 2022
REFERENCES,0.8815533980582524,"Our error-aware mixture model demonstrates signiﬁcantly higher performance on rare slices than
prior SDMs; this is especially visible in trained model results. This is likely because our error-aware
mixture model jointly models the input embeddings, class labels, and model predictions, allowing
for better identiﬁcation of rare slices when compared to existing methods."
REFERENCES,0.883495145631068,"A.5.3
EXTENDED ANALYSIS OF NATURAL LANGUAGE DESCRIPTIONS"
REFERENCES,0.8854368932038835,Please refer to Figure 9 for a quantitative evaluation of natural image descriptions.
REFERENCES,0.887378640776699,"correlation
slice category
rare
slice category
noisy label
slice category"
REFERENCES,0.8893203883495145,Slice mentioned in top-k predictions
REFERENCES,0.8912621359223301,% of settings
REFERENCES,0.8932038834951457,"k=1
k=2
k=3
k=4
k=5
k=10
k=9
k=8
k=7
k=6 70 40 60 50 30"
REFERENCES,0.8951456310679612,"Figure 9: Descriptions of discovered slices align with the names of the ground truth slices. Here,
we show the fraction of natural image settings where Domino includes the exact name of the ground
truth slice (or one of its WordNet synonyms (Fellbaum, 1998)) in the top-k slice descriptions."
REFERENCES,0.8970873786407767,"A.5.4
FUTURE WORK"
REFERENCES,0.8990291262135922,"Based on our analysis of model trends, we identify several directions for future work. First, we
observe that slice discovery is particularly difﬁcult when the strength of the slice (α) is low. In the
future, we aim to explore strategies for improving slice discovery and explanation generation in this
scenario. Additionally, we hope to explore strategies for generating informative input embeddings
when access to paired input-text data is limited. Finally, we intend to run controlled user studies
in order to understand when explanations generated by Domino are actionable for practitioners and
domain experts."
REFERENCES,0.9009708737864077,Published as a conference paper at ICLR 2022
REFERENCES,0.9029126213592233,"A.6
GLOSSARY OF NOTATION"
REFERENCES,0.9048543689320389,Classiﬁcation (Section 3)
REFERENCES,0.9067961165048544,"X
The set of values that the inputs can take on in a standard classiﬁcation setting. For exam-
ple, this could be the set of all possible 256 × 256 RGB images."
REFERENCES,0.9087378640776699,"Y
The set of values that the labels can take on in a standard classiﬁcation setting. In this
work, we deal primarily with binary classiﬁcation where Y = {0, 1}."
REFERENCES,0.9106796116504854,"X
A random variable representing the input in a standard classiﬁcation setting."
REFERENCES,0.912621359223301,"Y
A random variable representing the label in a standard classiﬁcation setting."
REFERENCES,0.9145631067961165,"P
A probability distribution. For example, the joint distribution over inputs and labels can
be expressed as P(X, Y )."
REFERENCES,0.916504854368932,"xi
The realization of the ith sample of X."
REFERENCES,0.9184466019417475,"yi
The realization of the ith sample of Y ."
REFERENCES,0.920388349514563,"n
The number of samples in a classiﬁcation dataset."
REFERENCES,0.9223300970873787,"x
The set of n samples of X, such that x = {xi}n
i=1 ∈X n."
REFERENCES,0.9242718446601942,"y
The set of n samples of Y , such that y = {yi}n
i=1 ∈Yn."
REFERENCES,0.9262135922330097,"D
A labeled dataset sampled from P(X, Y ), such that D = {(xi, yi)}n
i=1
hθ
A classiﬁer with parameters θ that predicts Y from X, where hθ : X →Y.
ˆY
A random variable representing the prediction of the model, such that ˆY = hθ(X)."
REFERENCES,0.9281553398058252,"ℓ
A performance metric for a standard classiﬁcation setting, where ℓ: Y × Y →R (e.g.
accuracy)."
REFERENCES,0.9300970873786408,Slice Discovery (Section 3)
REFERENCES,0.9320388349514563,"S(j)
A random variable representing the jth data slice. This is a binary variable S(j) ∈{0, 1}."
REFERENCES,0.9339805825242719,"s(j)
i
The realization of the ith sample of S(j)."
REFERENCES,0.9359223300970874,"k
The number of slices in the data."
REFERENCES,0.9378640776699029,"S
A random variable representing a set of k slices S = {S(j)}k
j=1 ∈{0, 1}k."
REFERENCES,0.9398058252427185,"si
The realization of the ith sample of S."
REFERENCES,0.941747572815534,"ψ(j)
A slicing function ψ(j) : X × Y →{0, 1}
ˆk
The number of slicing functions returned by a slice discovery method."
REFERENCES,0.9436893203883495,"Ψ
A set of ˆk slicing functions Ψ = {ψ(j)}ˆk
j=1."
REFERENCES,0.945631067961165,"M
A slice discovery method, M(D, hθ) →Ψ."
REFERENCES,0.9475728155339805,Evaluation Framework (Section 4)
REFERENCES,0.9495145631067962,"L
A slice discovery metric, L : 0, 1k × [0, 1] →R (e.g. precision-at-10)."
REFERENCES,0.9514563106796117,"C
A random variable representing a metadata attribute. We use C to generate datasets with
underperforming slices."
REFERENCES,0.9533980582524272,"α
The strength of a generated slice. For example, in a correlation slice, α is the Pearson
correlation coefﬁcient between the label Y and the correlate C."
REFERENCES,0.9553398058252427,"ϵ
The performance degradation in metric ℓrequired for a model to be considered underper-
forming.
¯hθ
A synthetic model, ¯h : [0, 1]k × Y →[0, 1], which samples predictions ˆY conditional on
Y and S."
REFERENCES,0.9572815533980582,Published as a conference paper at ICLR 2022
REFERENCES,0.9592233009708738,Domino (Section 5)
REFERENCES,0.9611650485436893,"T
The set of possible text strings."
REFERENCES,0.9631067961165048,"T
A random variable representing a text string in a paired data setting, T ∈T ."
REFERENCES,0.9650485436893204,"V
A random variable representing inputs in a paired data setting, V ∈X. Note that we do
not use X here in order to emphasize the difference between the data used to train the
classiﬁer and the data used to learn cross-modal embeddings."
REFERENCES,0.9669902912621359,"npaired
The number of examples in a paired dataset."
REFERENCES,0.9689320388349515,"Dpaired
A paired dataset D = {(vi, ti)}npaired
i=1 , where the text ti describes the input vi."
REFERENCES,0.970873786407767,"d
The dimensionality of the embeddings."
REFERENCES,0.9728155339805825,"ginput
An embedding function for inputs, ginput : X →Rd."
REFERENCES,0.974757281553398,"ginput
An embedding function for text, gtext : T →Rd."
REFERENCES,0.9766990291262136,"Z
A random variable representing the embedding of an input, such that Z = ginput(X)."
REFERENCES,0.9786407766990292,"zi
The value of the ith sample of Z, such that zi = ginput(xi)."
REFERENCES,0.9805825242718447,"z
The set of n samples of Z, such that z = {zi}n
i=1 ∈Rn×d."
REFERENCES,0.9825242718446602,"Ztext
A random variable representing the embedding of a text string, such that Z = gtext(T)."
REFERENCES,0.9844660194174757,"ztext
i
The realization of the ith sample of Ztext, such that ztext
i
= gtext(ti)."
REFERENCES,0.9864077669902913,"¯z(i)
slice
The average embedding of the ith slice."
REFERENCES,0.9883495145631068,"¯z(i)
class
The average embedding of the ith class."
REFERENCES,0.9902912621359223,"µ(i)
In the error-aware mixture model, the mean parameter of the Gaussian distribution used
to model Z for the ith slice."
REFERENCES,0.9922330097087378,"Σ(i)
In the error-aware mixture model, the covariance parameter of the Gaussian distribution
used to model Z for the ith slice."
REFERENCES,0.9941747572815534,"p(i)
In the error-aware mixture model, the parameter of the categorical distribution used to
model Y for the ith slice."
REFERENCES,0.996116504854369,"ˆp(i)
In the error-aware mixture model, the parameter of the categorical distribution used to
model ˆY for the ith slice."
REFERENCES,0.9980582524271845,"φ
The parameters of the error aware mixture model: φ = [pS, {µ(s), Σ(s), p(s), ˆp(s)}¯k
s=1]"
