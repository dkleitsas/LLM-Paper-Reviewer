Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006329113924050633,"The idea of using the recurrent neural network for visual attention has gained
popularity in computer vision community. Although the recurrent attention model
(RAM) leverages the glimpses with more large patch size to increasing its scope,
it may result in high variance and instability. For example, we need the Gaussian
policy with high variance to explore object of interests in a large image, which
may cause randomized search and unstable learning. In this paper, we propose to
unify the top-down and bottom-up attention together for recurrent visual attention.
Our model exploits the image pyramids and Q-learning to select regions of inter-
ests in the top-down attention mechanism, which in turn to guide the policy search
in the bottom-up approach. In addition, we add another two constraints over the
bottom-up recurrent neural networks for better exploration. We train our model in
an end-to-end reinforcement learning framework, and evaluate our method on vi-
sual classiﬁcation tasks. The experimental results outperform convolutional neural
networks (CNNs) baseline and the bottom-up recurrent attention models on visual
classiﬁcation tasks."
INTRODUCTION,0.012658227848101266,"1
INTRODUCTION"
INTRODUCTION,0.0189873417721519,"Recurrent visual attention model Mnih et al. (2014) (abbreviated as RAM) leverages reinforcement
learning Sutton & Barto (1998) and recurrent neural networks Schuster et al. (1997); LeCun et al.
(2015) to recognize the objects of interests in a sequential manner. Speciﬁcally, RAM models ob-
ject recognition as a sequential decision problem based on reinforcement learning. The agent can
adaptively select a series of regions based on its state and make decision to maximize the expected
return. The advantages of RAM are follows: ﬁrstly, its computation time scales linearly with the
complexity of the patch size and the glimpse length rather than the whole image, through a sequence
of glimpses and interactions locally. Secondly, this model is very ﬂexible and effective. For exam-
ple, it considers both local and global features by using more number of glimpses, scales and large
patch sizes to enhance its performance."
INTRODUCTION,0.02531645569620253,"Unfortunately, the idea of using glimpse in the local region limits its understanding to the holistic
view and restricts its power to capture the semantic regions in the image. For example, its initial
location and patch extraction is totally random, which will take more steps to localize the target
objects. Although we can increase the patch size to cover larger region, it will increase the compu-
tation time correspondently. In the same time, we need to set a high variance (if we use Gaussian
policy) to explore unseen regions. Thinking a situation that you were in desert and you were looking
for water, but you did not have GPS/Map over the whole scene. What would you do to ﬁnd water?
You might need to search each direction (east, south, north, and west), that is random searching and
time-consuming. In this paper, we want to provide a global view to localize where to search. For
example, the process of human perception is to scan the whole scene, and then may be attracted
by speciﬁc regions of interest. In a high level view, it can help us to ignore non-important parts
and focus information on where to look, and further to guide us to different ﬁxations for better
understanding the internal representation of the scene."
INTRODUCTION,0.03164556962025317,"To solve ‘where to look’ using reinforcement learning, the desired algorithm needs to satisfy the
following criterions: (1) it is stable and effective; (2) it is computationally efﬁcient; (3) it must be
more goal-oriented with global information; (4) instead of random searching, it attends regions of
interest even over large image; (5) it has better exploration strategy on ‘where to look’. For a large"
INTRODUCTION,0.0379746835443038,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04430379746835443,"image, the weakness of RAM is exposed throughly, because it relies on local ﬁxations to search the
next location, which increasing the policy uncertainty as well as the learning time to optimizing the
best in the interaction sequence. Inspired by the hierarchical image pyramids Adelson et al. (1984);
Lin et al. (2017) and reinforcement learning Watkins & Dayan (1992); Kulkarni et al. (2016), we
propose to unify the top-down and bottom-up mechanism to overcome the weakness of recurrent
visual attention model."
INTRODUCTION,0.05063291139240506,"In the high-level, we take a top-down mechanism to extract information at multiple scales and levels
of abstraction, and learn to where to attend regions of interests via reinforcement learning. While in
the low-level, we use the similar recurrent visual attention model to localize objects. In particular,
we add another two constraints over the bottom-up recurrent neural networks for better exploration.
Speciﬁcally, we add the entropy of image patch in the trajectory to enhance the policy search. In
addition, we constrain the region that attend to should be more related to target objects. By combin-
ing the sequential information together, we can understand the big picture and make better decision
while interacting with the environment."
INTRODUCTION,0.056962025316455694,"We train our model in an end-to-end reinforcement learning framework, and evaluate our method on
visual classiﬁcation tasks including MNIST, CIFAR 10 classes dataset and street view house num-
ber (SVHN) dataset. The experimental results outperform convolutional neural networks (CNNs)
baseline and the bottom-up recurrent attention models (RAM)."
RELATED WORK,0.06329113924050633,"2
RELATED WORK"
RELATED WORK,0.06962025316455696,"Since the recurrent models with visual attention (RAM) was proposed, many works have been in-
spired to either use RNNs or reinforcement learning to improve performance on computer vision
problems, such as object recognition, location and question answering problems. One direction is
the top-down attention mechanism. For example, Caicedo and Lazebnik extends the recurrent at-
tention model and designs an agent with actions to deform a bounding box to determine the most
speciﬁc location of target objects Caicedo & Lazebnik (2015). Xu et al. (2015) introduces an at-
tention based model to generate words over an given image. Basically, it partitions image into
regions, and then models the importance of the attention locations as latent variables, that can be
automatically learned to attend to the salient regions of image for corresponding words. Wang et al.
Wang et al. (2018) leverages the hierarchical reinforcement learning to capture multiple ﬁne-grained
actions in sub-goals and shows promising results on video captioning."
RELATED WORK,0.0759493670886076,"Another trend is to build the interpretation of the scene from bottom-up. Butko & Movellan (2009)
builds a Bayesian model to decide the location to attend while interacting with local patches in a
sequential process. Its idea is based on reinforcement learning, such as partially observed Markov
decision processes (POMDP) to integrate patches into high-level understanding of the whole im-
age. The design of RAM Mnih et al. (2014) takes a similar approach as Butko & Movellan (2008;
2009), where reinforcement learning is used to learn ‘where to attend’. Speciﬁcally, at each point,
the agent only senses locally with limited bandwidth, not observe the full environment. Compared
Butko & Movellan (2009), RAM leverages RNNs with hidden states to summarize observations
for better understanding the environment. Similarly, drl-RPN Pirinen & Sminchisescu (2018) pro-
poses a sequential attention model for object detection, which generates object proposal using deep
reinforcement learning, as well as automatically determines when to stop the search process."
RELATED WORK,0.08227848101265822,"Other related work combine local and global information to improve performance on computer vi-
sion problems. Attention to Context Convolution Neural Network (AC-CNN) exploits both global
and local contextual information and incorporates them effectively into the region-based CNN to im-
prove object detection performance Li et al. (2017). Anderson et al. Anderson et al. (2018) proposes
to combine bottom-up and top-down attention mechanism for image captioning and visual question
answering. They uses the recurrent neural networks (speciﬁcally, LSTM) to predict an attention dis-
tribution over image regions from bottom-up. However, no reinforcement learning technique used
to adaptively focus objects and other salient image regions of interests."
RELATED WORK,0.08860759493670886,"RAM learns to attend local regions while interacting with environment in a sequential decision
process with the purpose to maximum the cumulative return. However, if the scope of local glimpse
is small, then we lose the high level context information in the image when making decision in an
sequential manner. In contrast, if we increase the patch size to the scale as large as the original"
RELATED WORK,0.0949367088607595,Under review as a conference paper at ICLR 2022
RELATED WORK,0.10126582278481013,"image, then we lose the attention mechanism, as well as its efﬁciency of this model. Moreover,
when we increase the scale with the ﬁxed patch size, we will deprive distinguished features after
resizing back to original patch size. Also it is a challenge to balance the Gaussian policy variance
and patch size. Since the initial location is randomly generated, we need to set a large variance in
Gaussian policy to increase its policy exploration. However, it will result in high instability in the
learning process. In this paper, we propose to unify bottom-up and top-down mechanism to address
these issues mentioned above. In addition, we introduce entropy into policy gradient in order to
attend regions with high information content."
BACKGROUND,0.10759493670886076,"3
BACKGROUND"
REINFORCEMENT LEARNING,0.11392405063291139,"3.1
REINFORCEMENT LEARNING"
REINFORCEMENT LEARNING,0.12025316455696203,"The objective of reinforcement learning is to maximize a cumulative return with sequential interac-
tions between an agent and its environment Sutton & Barto (1998). At every time step t, the agent
selects an action at in the state st according its policy and receives a scalar reward rt(st, at), and
then transit to the next state st+1 with probability p(st+1|st, at). We model the agent’s behavior
with πθ(a|s), which is a parametric distribution from a neural network."
REINFORCEMENT LEARNING,0.12658227848101267,"Suppose we have the ﬁnite trajectory length while the agent interacting with the environment. The
return under the policy π for a trajectory τ = (st, at)T
t=0"
REINFORCEMENT LEARNING,0.13291139240506328,"J(θ) = Eτ∼πθ(τ)[ T
X"
REINFORCEMENT LEARNING,0.13924050632911392,"t=0
γtr(st, at)] = Eτ∼πθ(τ)[RT
0 ]
(1)"
REINFORCEMENT LEARNING,0.14556962025316456,"where γ is the return discount factor, which is necessary to decay the future rewards ensuring
bounded returns. πθ(τ) denotes the distribution of trajectories below"
REINFORCEMENT LEARNING,0.1518987341772152,"ρ(τ) = π(s0, a0, s1, ..., sT , aT )"
REINFORCEMENT LEARNING,0.15822784810126583,"= p(s0) T
Y"
REINFORCEMENT LEARNING,0.16455696202531644,"t=0
πθ(at|st)p(st+1|st, at)
(2)"
REINFORCEMENT LEARNING,0.17088607594936708,The goal of reinforcement learning is to learn a policy π which can maximize the expected returns.
REINFORCEMENT LEARNING,0.17721518987341772,"θ = arg max J(θ) = arg max Eτ∼πθ(τ)[RT
0 ]
(3)"
REINFORCEMENT LEARNING,0.18354430379746836,Policy gradient: Take the derivative w.r.t. θ
REINFORCEMENT LEARNING,0.189873417721519,"∇θJ(θ) = ∇θEτ∼πθ(τ)[RT
0 ] = ∇θ"
REINFORCEMENT LEARNING,0.1962025316455696,"Z
ρ(τ)RT
0 dτ"
REINFORCEMENT LEARNING,0.20253164556962025,"=
Z
∇θρ(τ)RT
0 dτ =
Z
ρ(τ)∇θρ(τ)"
REINFORCEMENT LEARNING,0.2088607594936709,"ρ(τ) RT
0 dτ"
REINFORCEMENT LEARNING,0.21518987341772153,"= Eτ∼πθ(τ)[∇θlogπθ(τ)RT
0 ]"
REINFORCEMENT LEARNING,0.22151898734177214,"= Eτ∼πθ(τ)[∇θlogπθ(τ)(RT
0 −b)]
(4)"
REINFORCEMENT LEARNING,0.22784810126582278,"Q-learning: The action-value function describes what the expected return of the agent is in state s
and action a under the policy π. The advantage of action value function is to make actions explicit,
so we can select actions even in the model-free environment. After taking an action at in state st
and thereafter following policy π, the action value function is formatted as:"
REINFORCEMENT LEARNING,0.23417721518987342,"Qπ(st, at) = Eτ∼πθ(τ)[Rt|st, at]"
REINFORCEMENT LEARNING,0.24050632911392406,"= Eτ∼πθ(τ)[ T
X"
REINFORCEMENT LEARNING,0.2468354430379747,"i=t
γ(i−t)r(si, ai)|st, at]
(5)"
REINFORCEMENT LEARNING,0.25316455696202533,"To get the optimal value function, we can use the maximum over actions, denoted as Q∗(st, at) =
maxπ Qπ(st, at), and the corresponding optimal policy π can be easily derived by π∗(s) ∈
arg maxat Q∗(st, at)."
REINFORCEMENT LEARNING,0.25949367088607594,Under review as a conference paper at ICLR 2022
REINFORCEMENT LEARNING,0.26582278481012656,"ac#on&
ac#on&"
REINFORCEMENT LEARNING,0.2721518987341772,Reward&
REINFORCEMENT LEARNING,0.27848101265822783,Bo-om/up&a-en#on&
REINFORCEMENT LEARNING,0.2848101265822785,model&
REINFORCEMENT LEARNING,0.2911392405063291,.&.&.&.&.&.&
REINFORCEMENT LEARNING,0.2974683544303797,Top/down&a-en#on&
REINFORCEMENT LEARNING,0.3037974683544304,model&
REINFORCEMENT LEARNING,0.310126582278481,Regions&of&
REINFORCEMENT LEARNING,0.31645569620253167,interests&
REINFORCEMENT LEARNING,0.3227848101265823,Loca#on&
REINFORCEMENT LEARNING,0.3291139240506329,New&loca#on&
REINFORCEMENT LEARNING,0.33544303797468356,Classiﬁca#on&
REINFORCEMENT LEARNING,0.34177215189873417,Rewards&
REINFORCEMENT LEARNING,0.34810126582278483,"(a) Top-down model
(b) Bottom-up model"
REINFORCEMENT LEARNING,0.35443037974683544,"Figure 1:
(a) The top-down attention model: it divides the region (marked as red) into 4 sub-
regions in the current resolution, and then concatenates features of each region (extracted from
neural networks) as in the input to action network to decide which sub-region it should choose to
attend; This process repeats to select sub-region in the next level of image pyramid (b) the bottom-
up attention model: based on the location output from the top-down attention model, it extracts
features at different scales at the current location and then combines location feature as input to
recurrent neural networks and action network to decide next location."
RECURRENT ATTENTION MODEL,0.36075949367088606,"3.2
RECURRENT ATTENTION MODEL"
RECURRENT ATTENTION MODEL,0.3670886075949367,"Instead of processing the whole image X, recurrent attention model (RAM) Mnih et al. (2014) uses
the recurrent neural networks (RNN) to guide the agent to make a sequential decision process while
interacting with a visual environment. At the time step t, the agent is at location Lt, observing
image patch xt with different patch size, and then it makes decision to next location Lt+1. Finally,
the agent executes action and receives a scalar reward (which will be used to correct its decision).
In classiﬁcation task, the agent will get positive reward if it predicts right. The goal of the agent is
to maximize the total sum of such rewards."
RECURRENT ATTENTION MODEL,0.37341772151898733,"Since RAM extracts one patch at each time step, the number of pixels it processes in the glimpse xt
is much smaller than that in the original image X. Compared to deep neural networks, it is much
efﬁcient, i.e. the computational cost of a single glimpse is independent of the size of the image. The
model structure of RAM is listed below with paraphrase:"
RECURRENT ATTENTION MODEL,0.379746835443038,"Glimpse network fg: it extracts features around position Lt at different scales via its sensor, and
then combines visual features and position features together via glimpse network fg to generate the
glimpse feature vector Gt."
RECURRENT ATTENTION MODEL,0.3860759493670886,"RNN fh: it takes the glimpse vector Gt and Ht−1 to output the next the hidden state Ht, which
summarizes its history knowledge over the environment."
RECURRENT ATTENTION MODEL,0.3924050632911392,"Action network fl: it uses the output from fh, and then decides the agent next location Lt based on
the internal hidden state Ht."
RECURRENT ATTENTION MODEL,0.3987341772151899,"Classiﬁcation network fy: it integrates all information to the current state in the sequence and de-
cides the class of the sequence or image (which further decides the reward the agent will receive)."
UNIFIED VISUAL ATTENTION MODEL,0.4050632911392405,"4
UNIFIED VISUAL ATTENTION MODEL"
UNIFIED VISUAL ATTENTION MODEL,0.41139240506329117,"In this section, we present an uniﬁed framework, which combines top-down and bottom-up mecha-
nism for visual attention."
TOP-DOWN ATTENTION MODEL,0.4177215189873418,"4.1
TOP-DOWN ATTENTION MODEL"
TOP-DOWN ATTENTION MODEL,0.4240506329113924,"The top-down attention mechanism is implemented to attend signiﬁcant regions by following coarse-
to-ﬁne image pyramid. In the coarse level, we divide the image into regions, and then select the
region based on its importance. Further, we map the region into the next level pyramid with higher
resolution and divide it into subregions and repeat the process until to the ﬁnal level (original image).
In Figure 1(a), we show an example image, which is divided into 2 × 2 regions in the coarsest level."
TOP-DOWN ATTENTION MODEL,0.43037974683544306,Under review as a conference paper at ICLR 2022
TOP-DOWN ATTENTION MODEL,0.43670886075949367,"In the next level, each region repeats this like quad tree image representation. How to select one
from four regions is based on the reward backups from the bottom-up attention model. Since we
divide the current level image into 2 × 2 regions, we need a strategy to select one region to look,
which tries to solve “where to look” in our top-down attention model."
TOP-DOWN ATTENTION MODEL,0.4430379746835443,"The action that learns which region to look is based on Q-learning, which is parametrized by 2-layer
networks in our work. As for the agent, the state is the image (representation) at the current level ℓ,
and its actions are to choose one of its 4 regions, which can map to subregions in the level ℓ+ 1.
The reward is from the bottom-up model, which will be introduced in the next part. For the object
recognition problem, if we classify the image correctly from bottom-up mechanism, then we get
positive reward, otherwise negative reward backward to top-down attention model."
TOP-DOWN ATTENTION MODEL,0.44936708860759494,"As for the network structure, we can use full connected neural networks (or CNN) to extract a set of
features from the image in each level in the hierarchy. For instance, if we partition the low resolution
image into A regions (where we use A to denote the action space), then we get the |A| vectors, each
of which represents as N dimensional vector corresponding to a part of the image in that level"
TOP-DOWN ATTENTION MODEL,0.45569620253164556,"ℓ= {v1, v2, ..., v|A|}, vi ∈RN
(6)
where each region is represented as N-dimensional vector vi. Then we concatenate all vectors
together to form the (sub)image representations at the current level"
TOP-DOWN ATTENTION MODEL,0.4620253164556962,"sℓ= [v1, ..., v|A|], sℓ∈R|A|N
(7)
To select one region from the total A regions in each level, we leverage Q-learning, which as an
off-policy RL algorithm and has been extensively studied since it was proposed Watkins & Dayan
(1992). The Q-value function q(s, a) should output |A|-dimensional values to indicate the impor-
tances of the A-regions. correspondently. Thus, we add another layer of full connected networks
which outputs |A|-dimensional values to decide the probability (softmax) to decide the next region
to look at in the next level. In our case, the agent needs to select regions according to its q-value."
TOP-DOWN ATTENTION MODEL,0.46835443037974683,"Suppose we use neural network parametrized by θq to approximate Q-value in the interactive envi-
ronment. To update Q-value function, we minimize the follow loss:
yℓ= r(sℓ, aℓ) + γ max
aℓ+1 Q(sℓ+1, aℓ+1; θq)
(8)"
TOP-DOWN ATTENTION MODEL,0.47468354430379744,"L(θq) = Esℓ∼pπ,aℓ∼π[(Q(sℓ, aℓ; θq) −yℓ)2]
(9)
where sℓ+1 is the state (or image region) in the level ℓ+ 1, aℓ+1 is the action to pick one region
from the total A regions. yℓis from Bellman equation, and we update it from its action at+1 ∈
{1, 2, .., |A|} which is taken from frozen policy network (actor) to stabilizing the learning. Since
we don not know which region to focus until the last layer reward from the bottom-up model, so we
design r(sℓ, aℓ) = 1 if ℓis the last pyramid layer (original image) as well as we recognize object
correctly from bottom-up model, otherwise r(sℓ, aℓ) = 0. Q(sℓ, aℓ; θq) is approximated with a
two-layer neural network (one convolution layer followed with fully connected layer)."
TOP-DOWN ATTENTION MODEL,0.4810126582278481,"In the current image level ℓ, the agent takes step to select one region to focus on according to q-
value, and then further repeat to select subregions based on q-value in the next level ℓ+ 1. How to
learn the q-value function will be dependent on the ﬁnal reward in the bottom-up model in the next
section. We can minimize Eq. 9 to learn parameter θq."
BOTTOM-UP ATTENTION MECHANISM,0.4873417721518987,"4.2
BOTTOM-UP ATTENTION MECHANISM"
BOTTOM-UP ATTENTION MECHANISM,0.4936708860759494,"Our bottom-up attention model takes the similar approach as RAM Mnih et al. (2014), to use the
recurrent neural networks (RNN) to guide the agent to make a sequential decision process while
interacting with a visual environment."
BOTTOM-UP ATTENTION MECHANISM,0.5,"At each location, the agent observes the low-level scene properties in a local region and build up
bottom-up processes over time to integrate information from visual environment in order to deter-
mine how to act and how to deploy its sensor most effectively.
Gt = fg(xt, Lt; θg)
(10a)
Ht = fh(Ht−1, Gt; θh)
(10b)
Lt = fl(Ht; θl)
(10c)
αt = fy(Ht; θy)
(10d)"
BOTTOM-UP ATTENTION MECHANISM,0.5063291139240507,Under review as a conference paper at ICLR 2022
BOTTOM-UP ATTENTION MECHANISM,0.5126582278481012,"where xt is the local patch or observation of the visual environment (image), with location spec-
iﬁed by Lt. fg combines visual feature Gimg(xt) and location feature Gloc(Lt) together, where
visual feature Gimg(xt) consists of three convolutional hidden layers with normalization and pool-
ing layers followed by a fully connected layer, and Gloc(Lt) is from full connected hidden layer.
And we use Gt = Gimg(xt) + Gloc(Lt) to get glimpse feature at Lt. Ht integrates the previous
hidden state and Gt to generate next hidden state, in order to capture the whole sequential infor-
mation. fy is the classiﬁer, which integrates all information to the current state in the sequence
and decides the class of the image (which further decides the reward the agent will receive). it is
formulated using a softmax output and for dynamic environments, its exact formulation depends on
the action set deﬁned for that particular environment. For classiﬁcation with K classes, we have
ˆy = arg maxk∈K αtk, αt ∈R1×K."
BOTTOM-UP ATTENTION MECHANISM,0.5189873417721519,"Compared to RAM, we add anther two constraints which balances the trade-off between exploration
and exploitation. On the one hand, we want to take action to select high entropy regions to better ex-
plore the whole environment. On the other hand, we hope the regions selected have high conﬁdence
on classiﬁcation task. To sum up, our contributions are followings:"
BOTTOM-UP ATTENTION MECHANISM,0.5253164556962026,"(1) Context constraint: the basic idea is that for the regions selected in policy search, we should
assign them more weights. αt is the softmax output at each step t, which in fact is the weight to
measure the importance of the current hidden state, or whether we should attend to current location.
Once the weight αt (which sums to one) are computed, then the global context vector C and its
softmax output ˆz is computed as C = T
X"
BOTTOM-UP ATTENTION MECHANISM,0.5316455696202531,"t=1
αtyHt"
BOTTOM-UP ATTENTION MECHANISM,0.5379746835443038,"ˆz = fc(C)
(11)"
BOTTOM-UP ATTENTION MECHANISM,0.5443037974683544,"where fc is the two-layer networks, with a fully connected layer and a softmax output layer. We
hope ˆz matches its groundtruth ˆz = y."
BOTTOM-UP ATTENTION MECHANISM,0.5506329113924051,"(2) Entropy constraint with better exploration: Entropy has been widely use to measure information,
where a higher entropy value indicates a relatively richer information content. As a bottom-up
algorithm, we want our model gives more weight on regions with higher entropy. Speciﬁcally, we
want to skip all black or white regions while searching objects. Thus, for each region we extracted,
we threshold it and binarialize it into 2 bins and then compute its entropy. In order to take action
towards high entropy regions, we modify the reward function as"
BOTTOM-UP ATTENTION MECHANISM,0.5569620253164557,"J(θ) = Eτ∼πθ(τ)logπθ(τ)(entropy(xt) + λ)(RT
0 −b)
(12)"
BOTTOM-UP ATTENTION MECHANISM,0.5632911392405063,"where entropy(xt) is the entropy over the patch located by Lt, λ is the constant to scale the im-
portance at the current time step, R0 is the reward over sequence and b is the baseline which can
be approximated by neural network. Since the action to localize the next region is backward from
reward, then we decide reward R0 as: if the model makes the right decision, then R0 = 1, otherwise
R0 = 0. The location network fl decides action at Lt, and the policy gradient can be formalized as"
BOTTOM-UP ATTENTION MECHANISM,0.569620253164557,"∇θJ(θ) = Eτ∼πθ(τ)[∇θlogπθ(τ)(entropy(xt) + λ)(RT
0 −b)]
(13)"
BOTTOM-UP ATTENTION MECHANISM,0.5759493670886076,"To learn action in Lt, we use Gaussian policy with variance σ which measures the exploration scale."
BOTTOM-UP ATTENTION MECHANISM,0.5822784810126582,The ﬁnal objective function in the bottom-up algorithm is hybrid supervised loss as follow:
BOTTOM-UP ATTENTION MECHANISM,0.5886075949367089,"loss = −ylogαT −β1ylogˆz + β2J(θ)
(14)"
BOTTOM-UP ATTENTION MECHANISM,0.5949367088607594,where β1 and β2 are the weights to balance the importance of each term.
ALGORITHM,0.6012658227848101,"4.3
ALGORITHM"
ALGORITHM,0.6075949367088608,"We outline the sketch of our training process in Algorithm 1. Note that we use another frozen
Q-learning network to stabilize the learning procedure."
ALGORITHM,0.6139240506329114,Under review as a conference paper at ICLR 2022
ALGORITHM,0.620253164556962,Algorithm 1 Uniﬁed visual attention model
ALGORITHM,0.6265822784810127,"1: Initialize model parameters in Eq. 9 and Eq. 10,
2: for each epoch do
3:
for each image, label(y) do
4:
Decide the action ai = maxaℓQ(sℓ, aℓ);
5:
Get the corresponding region vi via ai ∈A;
6:
Randomly sample location L0 from image region vi;
7:
Randomly generate initial hidden state H0;
8:
for each glimpse t = 0 to T do
9:
Run the RNN model to decide next location to focus via Eq. 10a, 10b and 10c;
10:
if condition is ok then
11:
Run classiﬁer via Eq. 10d;
12:
ˆyt = arg maxk∈K αtk;
13:
end if
14:
end for
15:
Evaluate the sequential reward R = [y == ˆy]
16:
if R is correct in the whole sequence then
17:
r(sℓ, aℓ) = 1;
18:
else
19:
r(sℓ, aℓ) = 0;
20:
end if
21:
Update Q-learning (top-down) model parameter θq by minimizing loss in Eq. 9;
22:
Update bottom-up model parameters {θg, θh, θl, θy} by minimizing loss in Eq. 14;
23:
end for
24: end for"
EXPERIMENTS,0.6329113924050633,"5
EXPERIMENTS"
EXPERIMENTS,0.6392405063291139,"We evaluated our approach on MNIST, CIFAR10 and SVHN datasets. As for parameter setting, we
use 2-level pyramid representation in the top-down q-learning: the coarse and the original image
level. Basically, the coarse level image is the half resolution (size) of the original image, and we
divide the coarse image into 2 by 2 subregions, then we can learn the location in the coarse level,
which in turn can be mapped back to the original image. In the bottom-up attention model, we
use Adam algorithm with learning rate 3e −4, batch size 128, λ = 0.5, β1 = 1, β2 = 0.01 and
σ = 0.15. Without other speciﬁcation, we use the same parameter setting above in all the following
experiments.
MNIST classiﬁcation: MNIST dataset is handwritten images (28×28) with digital numbers ranging
from 0 to 9. In this experiment, we tested our method by varying MNIST dataset including original
images and cluttered translated images with different size. The MNIST training set has total 60000
images, in which we use 54000 images for training and the rest 6000 for validation. Then we
evaluate the performance on test dataset with 10,000 examples. As for the experimental settings, we
use the similar RNN architecture (in Appendix) as RAM to make a fair comparison."
EXPERIMENTS,0.6455696202531646,"Patch feature encoding: at the location Lt, we extract m square patches with different scales. Then
we concatenate features at different scales together to get the ﬁnal patch representation xt centered
at location Lt. Note that Lt is a two dimensional vector normalized in the range [-1, 1], with the
image center (0, 0) and the right bottom (1, 1)."
EXPERIMENTS,0.6518987341772152,"Table 1: 28 × 28 MNIST classiﬁcation
Model
Error rate
FC, 2 layers (256 hiddens each)
1.69%
Convolutional networks, 2 layers
1.21%
RAM, 6 glimpses, 8 × 8, 1 scale
1.12%
RAM, 7 glimpses, 8 × 8, 1 scale
1.07%
Our method (Entropy), 6 glimpses, 8 × 8, 1 scale
1.05%
Our method (Entropy+Context), 6 glimpses, 8 × 8, 1 scale
1.01%"
EXPERIMENTS,0.6582278481012658,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.6645569620253164,"Since the digits in MNIST are centered, there is no need to use top-down mechanism “where to look”
to initialize the search. Thus, we only leverage the bottom-up strategy as RAM, but we introduce
new constrains in Eqs. 11 and 12 and want to test whether these new features get performance gain
or not. The evaluation result on the MNIST test dataset is shown in Table 1. Using 6 glimpses,
patch size 8 × 8 and 1 scale (where m = 1), our method with entropy beats RAM under the same
parameter setting. It demonstrates that the entropy constraint in Eq. 12 is helpful. In addition,
we tested whether the context constraint in Eq. 11 contributes or not, and it also shows that it can
improve the classiﬁcation task with more gain.
Cluttered Translated MNIST classiﬁcation: we use the same network architecture given in the
MNIST classiﬁcation experiment, but evaluate on the cluttered and translated digital images. In this
experiment, we use the same protocol as RAM to create the cluttered and translated MNIST data.
First, we place an MNIST digit in a random location of a larger blank image 100×100 and then add
8×8 subpatches which are sampled randomly from other random MNIST digits to random locations
of the image."
EXPERIMENTS,0.6708860759493671,"Table 2 shows the classiﬁcation results for the models we trained on 100 by 100 Cluttered Translated
MNIST with 10 pieces of clutter. The presence of clutter is kind of random noise, which makes the
task much more difﬁcult. Since our model uniﬁes top-down and bottom-up attention, it can ﬁnd
the meaningful regions and avoid the negative impact from noising subpatches. In Table 2, we
can see the performance of our attention model is affected less than the performance of the other
models. Using the same parameters as RAM with 6 glimpses, 12 × 12 and 4 scales, our approach
gains almost 3%, which demonstrates that our top-down and bottom-up attention uniﬁcation is more
powerful than RAM. By adding the entropy and global context constraints, our method gains another
3%. We can observe the similar results while using 8 glimpses."
EXPERIMENTS,0.6772151898734177,"Table 2: 100 × 100 Cluttered Translated MNIST classiﬁcation
Model
Error rate
FC, 2 layers (256 hiddens each)
57.3%
Convolutional, 5 layers
49.5%
RAM,1 glimpses, 8 × 8, 1 scale
88.6%
RAM,1 glimpses, 12 × 12, 1 scale
87.3%
RAM, 6 glimpses, 8 × 8, 1 scale
79.3%
RAM, 6 glimpses, 12 × 12, 4 scales
29.8%
RAM, 8 glimpses, 12 × 12, 4 scales
24.7%
Our method, 6 glimpses, 12 × 12, 4 scale
26.3%
Our method (with Entropy constraint), 6 glimpses, 12 × 12, 4 scale
23.6%
Our method (with Entropy+Context), 6 glimpses, 12 × 12, 4 scale
20.3%
Our method, 8 glimpses, 12 × 12, 4 scales
20.7%
Our method (with Entropy+Context), 8 glimpses, 12 × 12, 4 scales
17.3%"
EXPERIMENTS,0.6835443037974683,"Sequential multi-digit recognition: since the core network is based on recurrent neural networks,
we can easily extend it to handle multi-digit recognition Goodfellow et al. (2014); Ba et al. (2015).
Suppose that we have training images with the target {y1, y2, ..., yT }, where we can use y1 marks
the length of sequence or use yT to indicates the end of sequence. The task is to predict the digits as
it explores the local image glimpses."
EXPERIMENTS,0.689873417721519,Table 3: Whole sequence recognition error rates on 54 × 54 MNIST multi-digit SVHN dataset
EXPERIMENTS,0.6962025316455697,"Model
Error rate
11 layer CNN Goodfellow et al. (2014)
3.96%
10 layer CNN Ba et al. (2015)
4.11%
Single DRAM Ba et al. (2015), 18 glimpses, 12 × 12, 2 scales
5.1%
Our method (Entropy), 18 glimpses, 12 × 12, 2 scales
4.58%
Our method (Entropy+Context), 6 glimpses, 18 glimpses, 12 × 12, 2 scales
4.03%"
EXPERIMENTS,0.7025316455696202,"We take the same protocol as Goodfellow et al. (2014) on street view house number recognition
task. Speciﬁcally, we crop 64 x 64 images with multi-digits at the center and then randomly sample"
EXPERIMENTS,0.7088607594936709,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.7151898734177216,Table 4: The recognition accuracy on CIFAR10 dataset with PGD attack.
EXPERIMENTS,0.7215189873417721,"Model
Accuracy
VGG16 + PGD
47.76%
Our method + VGG16 + PGD
54.3%"
EXPERIMENTS,0.7278481012658228,"to create 54x54 jittered images with similar data augmentation. We have 212243 images for train-
ing (73257 digits from training dataset and the rest from 531131 additional image), 23511 digits
for validation and 26032 digits for testing. Same as DRAM Ba et al. (2015), our attention model
observes 3 glimpses for each digit before making a prediction. The recurrent model keeps running
until it predicts a terminal label or until the longest digit length in the dataset is reached. In the
SVHN dataset, up to 5 digits can appear in an image, so it means the recurrent model will run up to
18 glimpses per image, that is 5 x 3 plus 3 glimpses for a terminal label."
EXPERIMENTS,0.7341772151898734,"To conduct a fair comparison, we use the same architecture as DRAM (refer in Appendix). The
experiment results is shown in Table 3. Compared to DRAM, our model gains better result with
an error rate 4.03%. And our result is better than 10 layer CNN, and it is also comparable to the
11 layer CNN Goodfellow et al. (2014), which uses more complex network architecture and more
model parameters."
EXPERIMENTS,0.740506329113924,"Robust to PGD attack: in this experiment, we analyzed and answered whether our patch-based at-
tention model is robust to adversarial attack or not. Speciﬁcally, we will leverage projected gradient
descent (PGD) to generate adversarial examples because it has been widely used to attack deep neu-
ral networks Madry et al. (2018); Pintor et al. (2021). PGD is a very powerful adversary approach,
which has the following projected gradient descent on the negative loss function:"
EXPERIMENTS,0.7468354430379747,"xt+1 = Πx+S
 
xt + ϵsign(∇xℓ(x, y; θp))

(15)"
EXPERIMENTS,0.7531645569620253,"where ℓ(x, y; θp) is the loss function deﬁned in Eq. 14, S ⊆Rd formalizes the variance of ma-
nipulative space of the adversary attack, and θp is the set of model parameters. In order to test our
model’s robustness to PGD, we change equation equation 10d as follows:"
EXPERIMENTS,0.759493670886076,"feat = VGG(x)
(16)
αt = fy(cat(Ht, feat); θy)
(17)"
EXPERIMENTS,0.7658227848101266,"where VGG is the 16-layer neural networks deﬁned in Simonyan & Zisserman (2015), and cat
indicates feature concatenation. Our attention model use 6 glimpses with patch size 12×12 to make
prediction on each image. As for feature extraction, we concatenate VGG16’s output with Gt as
input in the RNN network. In the backpropagation stage, we can minimize loss in Eq. 14 and then
compute gradient w.r.t. feat in Eq. 16, so we can further calculate gradient w.r.t. x based on chain
rule. After we get ∇xℓ(x, y; θp), we can update x to generate adversarial examples according to Eq.
15. In CIFAR10 classiﬁcation task, we use step = 30 and ϵ = 0.3 in Eq. 15."
EXPERIMENTS,0.7721518987341772,"Our attention model takes a patch-based approach and then sequentially decides the next loca-
tion/patch to minimize the loss in Eq. 14. The result in Table 4 shows that our model is more
robust to PGD attack. The intuition behind this is that our model can select better patch in each
glimpse, and further it can learn some useful structure information to do better prediction."
CONCLUSION,0.7784810126582279,"6
CONCLUSION"
CONCLUSION,0.7848101265822784,"In this paper, we propose a visual attention approach, which uniﬁes the top-down attention mecha-
nism with the bottom-up recurrent attention model. Considering bottom-up RAM randomly initial-
izes the location to search target object, our top-down attention model is to answer ‘where to look’
the best initial location in an image. In addition, we introduce global context and entropy constraints,
in order to focus attention on edge, instead of pure black/while regions. To sum, our model leverages
reinforcement learning to combine top-down and bottom-up mechanism, both of which depends the
reward from classiﬁcation to decide next locations to attend in a sequential manner. We conduct a
bunch of experiments on MNIST, CIFAR10 and SVHN datasets, and our model shows better result
than RAM, DRAM and CNN (complex architecture). We also show our model is more robust than
other neural network models on recognizing noise images."
CONCLUSION,0.7911392405063291,Under review as a conference paper at ICLR 2022
REFERENCES,0.7974683544303798,REFERENCES
REFERENCES,0.8037974683544303,"E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. 1984, Pyramid methods
in image processing. RCA Engineer, 29(6):33–41, 1984."
REFERENCES,0.810126582278481,"Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and
Lei Zhang.
Bottom-up and top-down attention for image captioning and visual question an-
swering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
6077–6086. IEEE Computer Society, 2018."
REFERENCES,0.8164556962025317,"Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual
attention, 2015."
REFERENCES,0.8227848101265823,"Nicholas J. Butko and Javier R. Movellan. I-pomdp: An infomax model of eye movement. In IEEE
International Conference on Development and Learning (ICDL), 2008."
REFERENCES,0.8291139240506329,"Nicholas J. Butko and Javier R. Movellan. Optimal scanning for faster object detection. In IEEE
Conference on Computer Vision and Pattern Recognition, 2009."
REFERENCES,0.8354430379746836,"J. C. Caicedo and S. Lazebnik. Active object localization with deep reinforcement learning. In 2015
IEEE International Conference on Computer Vision (ICCV), pp. 2488–2496, 2015."
REFERENCES,0.8417721518987342,"Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit num-
ber recognition from street view imagery using deep convolutional neural networks. 2014."
REFERENCES,0.8481012658227848,"Tejas D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep
reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Daniel D.
Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Ad-
vances in Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 3675–3683, 2016."
REFERENCES,0.8544303797468354,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,
2015."
REFERENCES,0.8607594936708861,"Jianan Li, Yunchao Wei, Xiaodan Liang, Jian Dong, Tingfa Xu, Jiashi Feng, and Shuicheng Yan.
Attentive contexts for object detection. IEEE Trans. Multim., 19(5):944–954, 2017."
REFERENCES,0.8670886075949367,"Tsung-Yi Lin, Piotr Dollr, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie.
Feature pyramid networks for object detection. In CVPR, pp. 936–944. IEEE Computer Society,
2017."
REFERENCES,0.8734177215189873,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings. OpenReview.net, 2018."
REFERENCES,0.879746835443038,"Volodymyr Mnih, Nicolas Heess, Alex Graves, and koray kavukcuoglu.
Recurrent mod-
els of visual attention.
In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,
and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp.
2204–2212. Curran Associates, Inc., 2014.
URL http://papers.nips.cc/paper/
5542-recurrent-models-of-visual-attention.pdf."
REFERENCES,0.8860759493670886,"Maura Pintor, F. Roli, Wieland Brendel, and B. Biggio. Fast minimum-norm adversarial attacks
through adaptive norm constraints. ArXiv, abs/2102.12827, 2021."
REFERENCES,0.8924050632911392,"Aleksis Pirinen and Cristian Sminchisescu. Deep reinforcement learning of region proposal net-
works for object detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018."
REFERENCES,0.8987341772151899,"Mike Schuster, Kuldip K. Paliwal, and A. General. Bidirectional recurrent neural networks. IEEE
Transactions on Signal Processing, 1997."
REFERENCES,0.9050632911392406,Under review as a conference paper at ICLR 2022
REFERENCES,0.9113924050632911,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-
ings, 2015."
REFERENCES,0.9177215189873418,"Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive com-
putation and machine learning. MIT Press, 1998."
REFERENCES,0.9240506329113924,"X. Wang, W. Chen, J. Wu, Y. Wang, and W. Y. Wang. Video captioning via hierarchical reinforce-
ment learning. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
4213–4222, 2018."
REFERENCES,0.930379746835443,"Christopher J. C. H. Watkins and Peter Dayan. Q-learning. In Machine Learning, pp. 279–292,
1992."
REFERENCES,0.9367088607594937,"Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. In ICML, pp. 2048–2057, 2015. URL http://proceedings.mlr.
press/v37/xuc15.html."
REFERENCES,0.9430379746835443,"A
APPENDIX"
REFERENCES,0.9493670886075949,MNIST network architecture:
REFERENCES,0.9556962025316456,"Glimpse network Gt = fg(Xt, Lt; θg): fg is two-layer fully connected (FC) neural networks:"
REFERENCES,0.9620253164556962,"hg = Rect(Linear(Xt)), hl = Rect(Linear(Lt))
Gt = Rect(Linear(hg + ht))"
REFERENCES,0.9683544303797469,"where hg and hl both have dimensionality 128, while the dimension of Gt is 256 for all experiments."
REFERENCES,0.9746835443037974,"Recurrent network: Ht = fh(Ht−1, Gt; θh) to remember the sequence of glimpses in order to make
action to get best return. Ht = Rect(Linear(Ht−1) + Linear(Gt)))"
REFERENCES,0.9810126582278481,"Action network: Lt = fl(Ht; θl) is the policy network, where its mean is the linear network over
Ht and its variance σ is ﬁxed."
REFERENCES,0.9873417721518988,SVHN network architecture:
REFERENCES,0.9936708860759493,"the glimpse network is composed of three convolutional layers where the ﬁrst layer is with ﬁlter
size 5 × 5 and the later two layers with ﬁlter size 3 × 3. The number of ﬁlters in those layers was
{64, 64, 128} respectively. The recurrent model was 2-layer LSTM, with 512 hidden units in each
layer."
