Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0028089887640449437,"Continual learning requires the model to maintain the learned knowledge while
learning from a non-i.i.d data stream continually. Due to the single-pass training
setting, online continual learning is very challenging, but it is closer to the real-
world scenarios where quick adaptation to new data is appealing. In this paper, we
focus on online class-incremental learning setting in which new classes emerge
over time. Almost all existing methods are replay-based with a softmax classiﬁer.
However, the inherent logits bias problem in the softmax classiﬁer is a main cause
of catastrophic forgetting while existing solutions are not applicable for online
settings. To bypass this problem, we abandon the softmax classiﬁer and propose
a novel generative framework based on the feature space. In our framework, a
generative classiﬁer which utilizes replay memory is used for inference, and the
training objective is a pair-based metric learning loss which is proven theoretically
to optimize the feature space in a generative way. In order to improve the ability to
learn new data, we further propose a hybrid of generative and discriminative loss
to train the model. Extensive experiments on several benchmarks, including newly
introduced task-free datasets, show that our method beats a series of state-of-the-art
replay-based methods with discriminative classiﬁers, and reduces catastrophic
forgetting consistently with a remarkable margin."
INTRODUCTION,0.0056179775280898875,"1
INTRODUCTION"
INTRODUCTION,0.008426966292134831,"Humans excel at continually learning new skills and accumulating knowledge throughout their lifes-
pan. However, when learning a sequential of tasks emerging over time, neural networks notoriously
suffer from catastrophic forgetting (McCloskey & Cohen, 1989) on old knowledge. This problem
results from non-i.i.d distribution of data streams in such a scenario. To this end, continual learning
(CL) (Parisi et al., 2019; Lange et al., 2019) has been proposed to bridge the above gap between
intelligent agents and humans."
INTRODUCTION,0.011235955056179775,"In common CL settings, there are clear boundaries between distinct tasks which are known during
training. Within each task, a batch of data are accumulated and the model can be trained ofﬂine with
the i.i.d data. Recently, online CL (Aljundi et al., 2019c;a) setting has received growing attention
in which the model needs to learn from a non-i.i.d data stream in online settings. At each iteration,
new data are fed into the model only once and then discarded. In this manner, task boundary is not
informed, and thus online CL is compatible with task-free (Aljundi et al., 2019b; Lee et al., 2020)
scenario. In real-world scenarios, the distribution of data stream changes over time gradually instead
of switching between tasks suddenly. Moreover, the model is expected to quickly adapt to large
amount of new data, e.g. user-generated content. Online CL meets these requirements, so it is more
meaningful for practical applications. Many existing CL works deal with task-incremental learning
(TIL) setting (Kirkpatrick et al., 2017; Li & Hoiem, 2018), in which task identity is informed during
test and the model only needs to classify within a particular task. However, for online CL problem,
TIL is not realistic because of the dependence on task boundary as discussed above and reduces the
difﬁculty of online CL. In contrast, class-incremental learning (CIL) setting (Rebufﬁet al., 2017)
requires the model to learn new classes continually over time and classify samples over all seen
classes during test. Thus, online CIL setting is more suitable for online data streams in real-world CL
scenarios (Mai et al., 2021)."
INTRODUCTION,0.014044943820224719,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.016853932584269662,"1
2
3
4
5
Number of trained tasks 0 2 4 6 8"
INTRODUCTION,0.019662921348314606,Average logits
INTRODUCTION,0.02247191011235955,"Task1
Task2
Task3
Task4
Task5 (a)"
INTRODUCTION,0.025280898876404494,"Task1
Task2
Task3
Task4
Task5
0.0 0.2 0.4 0.6 0.8"
INTRODUCTION,0.028089887640449437,Accuracy/Forgetting
INTRODUCTION,0.03089887640449438,"Forgetting with Softmax Classifier
Forgetting with Generative Classifier
Accuracy with Softmax Classifier
Accuracy with Generative Classifier (b)"
INTRODUCTION,0.033707865168539325,"Figure 1: Logits bias phenomenon of softmax classiﬁer (left) and accuracy & forgetting on different tasks using
softmax vs. generative NCM classiﬁer (right). The results are obtained with ER and 1k replay memory on 5-task
Split CIFAR10."
INTRODUCTION,0.03651685393258427,"Most existing online CIL methods are based on experience replay (ER) (Robins, 1995; Riemer et al.,
2019) strategy which stores a subset of learned data in a replay memory and uses the data in memory
to retrain model thus alleviating forgetting. Recently, in CIL setting logits bias problem in the last
fully connected (FC) layer, i.e. softmax classiﬁer, is revealed (Wu et al., 2019), which is a main
cause of catastrophic forgetting. In Figure 1a, we show in online CIL, even if ER is used, logits bias
towards newly learned classes in the softamx classiﬁer is still serious and the forgetting on old tasks
is dramatic (See Figure 1b). Although some works (Wu et al., 2019; Belouadah & Popescu, 2019;
Zhao et al., 2020) propose different methods to reduce logits bias, they all depend on task boundaries
and extra ofﬂine phases during training so that not applicable for online CIL setting."
INTRODUCTION,0.03932584269662921,"In this paper, we propose to tackle the online CIL problem without the softmax classiﬁer to avoid
logits bias problem. Instead, we propose a new framework where training and inference are both in a
generative way. We are motivated by the insight that generative classiﬁer is more effective in low data
regime than discriminative classiﬁer which is demonstrated by Ng & Jordan (2001). Although the
conclusion is drawn on simple linear models (Ng & Jordan, 2001), similar results are also observed
on deep neural networks (DNNs) (Yogatama et al., 2017; Ding et al., 2020) recently. It should be
noticed that in online CIL setting the data is seen only once, not fully trained, so it is analogous to
the low data regime in which the generative classiﬁer is preferable. In contrast, the commonly used
softmax classiﬁer is a discriminative model."
INTRODUCTION,0.042134831460674156,"Concretely, we abandon the softmax FC layer and introduce nearest-class-mean (NCM) classi-
ﬁer (Mensink et al., 2013) for inference, which can be interpreted as classifying in a generative way.
The NCM classiﬁer is built on the feature space on the top of previous network layers. Thanks to
ER strategy, NCM classiﬁer can utilize the replay memory for inference. As for training, inspired
by a recent work (Boudiaf et al., 2020), which shows pair-based deep metric learning (DML) losses
can be interpreted as optimizing the feature space from a generative perspective, we introduce Multi-
Similarity (MS) loss (Wang et al., 2019) to obtain a good feature space for NCM classiﬁer. Meanwhile,
we prove theoretically that MS loss is an alternative to a training objective of the generative classiﬁer.
In this way, we can bypass logits bias."
INTRODUCTION,0.0449438202247191,"To strengthen the model’s capable of learning from new data in complex data streams, we further
introduce an auxiliary proxy-based DML loss (Movshovitz-Attias et al., 2017). Therefore, our whole
training objective is a hybrid of generative and discriminative losses. During inference, we ignore the
discriminative objective and classify with the generative NCM classiﬁer. By tuning weight of the
auxiliary loss, our method can work well in different data streams."
INTRODUCTION,0.047752808988764044,"In summary, our contributions are as follows:"
INTRODUCTION,0.05056179775280899,"1. We make the ﬁrst attempt to avoid logits bias problem in online CIL setting. In our generative
framework, a generative classiﬁer is introduced to replace softmax classiﬁer for inference
and for training, we introduce MS loss which is proven theoretically to optimize the model
in a generative way."
INTRODUCTION,0.05337078651685393,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.056179775280898875,"2. In order to improve the ability of MS loss to learn from new data, we further introduce an
auxiliary loss to achieve a good balance between retaining old knowledge and learning new
knowledge.
3. We conduct extensive experiments on four benchmarks in multiple online CIL settings,
including a new task-free setting we design for simulating more realistic scenarios. Empiri-
cal results demonstrate our method outperforms a variety of state-of-the-art replay-based
methods substantially, especially alleviating catastrophic forgetting signiﬁcantly."
RELATED WORK,0.05898876404494382,"2
RELATED WORK"
RELATED WORK,0.06179775280898876,"Current CL methods can be roughly divided into three categories: which are regularization, parameter
isolation and replay-based respectively (Lange et al., 2019). Regularization methods retain the
learned knowledge by imposing penalty constraints on model’s parameters (Kirkpatrick et al., 2017)
or outputs (Li & Hoiem, 2018) when learning new data. They work well in TIL setting but poor in
CIL setting (van de Ven & Tolias, 2018). Parameter isolation methods assign a speciﬁc subset of
model parameters, such as network weights (Mallya & Lazebnik, 2018) and sub-networks (Fernando
et al., 2017) to each task to avoid knowledge interference and thus the network may keep growing.
This type of method is mainly designed for TIL as task identity is usually necessary during test.
The mainstream of Replay-based methods is ER-like (Rebufﬁet al., 2017), which stores a subset of
old data and retrains it when learning new data to prevent forgetting of old knowledge. In addition,
generative replay method trains a generator to replay old data approximately (Shin et al., 2017)."
RELATED WORK,0.06460674157303371,"In the ﬁeld of online CL, most of methods are on the basis of ER. Chaudhry et al. (2019b) ﬁrst
explored ER in online CL settings with different memory update strategies. Authors suggested ER
method should be regarded as an important baseline as in this setting it is more effective than several
existing CL methods, such as A-GEM (Chaudhry et al., 2019a). GSS (Aljundi et al., 2019c) designs
a new memory update strategy by encouraging the divergence of gradients of samples in memory.
MIR (Aljundi et al., 2019a) is proposed to select the maximally interfered samples from memory for
replay. GMED (Jin et al., 2020) edits the replay samples with gradient information to obtain samples
likely to be forgotten, which can beneﬁt the replay in the future. Mai et al. (2021) focus on online CIL
setting and adopt the notion of Shapley Value to improve the replay memory update and sampling.
All of the above methods are replay-based with softmax classiﬁer. A contemporary work (Lange &
Tuytelaars, 2020) proposes CoPE, which is somewhat similar to our method. CoPE replaces softmax
classiﬁer with a prototype-based classiﬁer which is non-parametric and updated using features of
data samples. However, the loss function of CoPE is still discriminative and the way to classify is
analogous to softmax classiﬁer."
RELATED WORK,0.06741573033707865,"Apart from the above ER based methods, Zeno et al. (2018) propose an online regularization method,
however it performs very badly in online CL settings (Jin et al., 2020). Lee et al. (2020) propose a
parameter isolation method in which the network is dynamically expanded and a memory for storing
data is still required. Therefore, the memory usage is not ﬁxed and potentially unbounded."
RELATED WORK,0.0702247191011236,"A recent work (Yu et al., 2020) proposes SDC, a CIL method based on DML and NCM classiﬁer.
However, SDC requires an extra phase to correct semantic drift after training each task. This phase
depends on task boundaries and the accumulated data of a task, which is not applicable for online
CIL. In contrast, our method is based on ER and classiﬁes with replay memory and thus need not
correct the drift."
"ONLINE CLASS-INCREMENTAL LEARNING WITH A GENERATIVE
FRAMEWORK",0.07303370786516854,"3
ONLINE CLASS-INCREMENTAL LEARNING WITH A GENERATIVE
FRAMEWORK"
PRELIMINARIES AND MOTIVATIONS,0.07584269662921349,"3.1
PRELIMINARIES AND MOTIVATIONS"
ONLINE CLASS-INCREMENTAL LEARNING,0.07865168539325842,"3.1.1
ONLINE CLASS-INCREMENTAL LEARNING"
ONLINE CLASS-INCREMENTAL LEARNING,0.08146067415730338,"CIL setting has been widely used in online CL literature, e.g. (Aljundi et al., 2019a; Mai et al., 2021),
and a softmax classiﬁer is commonly used. A neural network f(·; θ) : X →Rd parameterized by θ
encodes data samples x ∈X into a d-dimension feature f(x) on which an FC layer g outputs logits
for classiﬁcation: o = Wf(x) + b. At each iteration, a minibatch of data Bn from a data stream S"
ONLINE CLASS-INCREMENTAL LEARNING,0.08426966292134831,Under review as a conference paper at ICLR 2022
ONLINE CLASS-INCREMENTAL LEARNING,0.08707865168539326,"arrives and the whole model (f, g) is trained on Bn only once. The training objective is cross-entropy
(CE) loss:"
ONLINE CLASS-INCREMENTAL LEARNING,0.0898876404494382,"LCE = − ˜
C
X"
ONLINE CLASS-INCREMENTAL LEARNING,0.09269662921348315,"c=1
t:c log ˆy:c,
ˆy:c =
eo:c
P ˜
C
c=1 eo:c
(1)"
ONLINE CLASS-INCREMENTAL LEARNING,0.09550561797752809,"where ˜C is the number of classes seen so far. t is one-hot label of x and the subscript :c denotes the
c-th component. The new classes from S emerge over time. The output space of g is the number of
seen classes and thus keeps growing. At test time, the model should classify over all C classes seen."
EXPERIENCE REPLAY FOR ONLINE CONTINUAL LEARNING,0.09831460674157304,"3.1.2
EXPERIENCE REPLAY FOR ONLINE CONTINUAL LEARNING"
EXPERIENCE REPLAY FOR ONLINE CONTINUAL LEARNING,0.10112359550561797,"ER makes two modiﬁcations during online training: (1) It maintains a replay memory M with limited
size which stores a subset of previously learned samples. (2) When a minibatch of new data Bn
is coming, it samples a minibatch Br from M and uses Bn ∪Br to optimize the model with one
SGD-like step. Then it updates M with Bn. Recent works, e.g. (Aljundi et al., 2019a; Mai et al.,
2021) regard ER-reservoir as a strong baseline, which combines ER with reservoir sampling (Vitter,
1985) for memory update and random sampling for Br. See Chaudhry et al. (2019b) for more details
about it."
LOGITS BIAS IN SOFTMAX CLASSIFIER,0.10393258426966293,"3.1.3
LOGITS BIAS IN SOFTMAX CLASSIFIER"
LOGITS BIAS IN SOFTMAX CLASSIFIER,0.10674157303370786,"Some recent works (Wu et al., 2019; Belouadah & Popescu, 2019; Zhao et al., 2020) show in CIL
scenarios, even with replay-based mechanism the logits outputted by model always have a strong
bias towards the newly learned classes, which leads to catastrophic forgetting actually. In preliminary
experiments, we also observe this phenomenon in online CIL setting. We run ER-reservoir baseline
on 5-task Split CIFAR10 (each task has two disjoint classes) online CIL benchmark. In Figure 1a,
we display the average logits of each already learned tasks over samples in test data after learning
each task. The model outputs much higher logits on the new classes (of the task just learned) than old
classes."
LOGITS BIAS IN SOFTMAX CLASSIFIER,0.10955056179775281,"Following (Ahn & Moon, 2020), we examine the CE loss in Eq (1), the gradient of LCE w.r.t logit
o:c of class c is ˆy:c −I[t:c = 1]. Thus, if c is the real label y, i.e. t:c = 1, the gradient is non-positive
and model is trained to increase o:c, otherwise the gradient is non-negative and model is trained
to decrease o:c. Therefore, logits bias problem is caused by the imbalance between the number
of samples of the new classes and that of the old classes with a limited size of M. As mentioned
in Section 1, existing solutions (Wu et al., 2019; Belouadah & Popescu, 2019; Zhao et al., 2020)
designed for conventional CIL need task boundaries to conduct extra ofﬂine training phases and even
depend on the accumulated data of one task. They are not applicable for online CIL setting where
task boundaries are not informed or even do not exist in task-free scenario."
INFERENCE WITH A GENERATIVE CLASSIFIER,0.11235955056179775,"3.2
INFERENCE WITH A GENERATIVE CLASSIFIER"
INFERENCE WITH A GENERATIVE CLASSIFIER,0.1151685393258427,"Proposed generative framework is based on ER strategy, and aims to avoid the intrinsic logits bias
problem by removing the softmax FC layer g and build a generative classiﬁer on the feature space
Z : z = f(x). If the feature z is well discriminative, we can conduct inference with samples in
M instead of a parametric classiﬁer which is prone to catastrophic forgetting (Rebufﬁet al., 2017).
We use NCM classiﬁer ﬁrstly suggested by Rebufﬁet al. (2017) for CL and show it is a generative
model. We use Mc to denote the subset of class c of M. The class mean µc is computed by
µM
c
=
1
|Mc|
P"
INFERENCE WITH A GENERATIVE CLASSIFIER,0.11797752808988764,"x∈Mc f(x). During inference, the prediction for x∗is made by:"
INFERENCE WITH A GENERATIVE CLASSIFIER,0.12078651685393259,"y∗= arg min
c
∥f(x∗) −µM
c ∥2
(2)"
INFERENCE WITH A GENERATIVE CLASSIFIER,0.12359550561797752,"In fact, the principle of prediction in Eq (2) is to ﬁnd a Gaussian distribution N(f(x∗)|µM
c , I)
with the maximal probability for x∗. Therefore, assuming the conditional distribution p(z|y =
c) = N(z|µM
c , I) and the prior distribution p(y) is uniform, NCM classiﬁer virtually deals with
p(y|z) by modeling p(z|y) in a generative way. The inference way is according to Bayes rule:
arg max p(y|x∗) = arg max p(f(x∗)|y)p(y). The assumption about p(y) simpliﬁes the analysis"
INFERENCE WITH A GENERATIVE CLASSIFIER,0.12640449438202248,Under review as a conference paper at ICLR 2022
INFERENCE WITH A GENERATIVE CLASSIFIER,0.12921348314606743,"and works well in practice. In contrast, softmax classiﬁer models p(y|x∗) in a typical discriminative
way."
INFERENCE WITH A GENERATIVE CLASSIFIER,0.13202247191011235,"As discussed above, online CIL is in a low data setting where generative classiﬁers are preferable
compared to discriminative classiﬁers (Ng & Jordan, 2001). Moreover, generative classiﬁers are
more robust to continual learning (Yogatama et al., 2017) and imbalanced data settings (Ding et al.,
2020). At each iteration, Bn ∪Br is also highly imbalanced. Considering these results, we hypothesis
generative classiﬁers are promising for online CIL problem. It should be noted our method only
models a simple generative classiﬁer p(z|y) on the feature space, instead of modeling p(x|y) on the
input space using DNNs (Yogatama et al., 2017), which is time-consuming and thus is not suitable
for online training."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.1348314606741573,"3.3
TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.13764044943820225,"To train the feature extractor f(θ) we resort to DML losses which aim to learn a feature space where
the distances represent semantic dissimilarities between data samples. From the perspective of mutual
information (MI), Boudiaf et al. (2020) theoretically show the equivalence between CE loss and
several pair-based DML losses, such as contrast loss (Hadsell et al., 2006) and Multi-Similarity (MS)
loss (Wang et al., 2019). The DML losses maximize MI between feature z and label y in a generative
way while CE loss in a discriminative way, which motivates us to train f(θ) with a pair-based DML
loss to obtain a good feature space Z for the generative classiﬁer."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.1404494382022472,"Especially, we choose the MS loss as a training objective. MS loss is one of the state-of-the-art
methods in the ﬁeld of DML. Wang et al. (2019) point out pair-based DML losses can be seen as
weighting each feature pair in the general pair weighting framework. As MS loss requires the feature
f(x) to be ℓ2-normalized ﬁrst, from now on, we use zi to denote the ℓ2-normalized feature of xi and
a feature pair is represented in the form of inner product Sij := zT
i zj. To weight feature pairs better,
MS loss is proposed to consider multiple types of similarity. MS loss on a dataset D is formulated as
follows:"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.14325842696629212,"LMS(D) =
1
|D| |D|
X i=1  1"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.14606741573033707,"α log[1 +
X"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.14887640449438203,"j∈Pi
e−α(Sij−λ)] + 1"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.15168539325842698,"β log[1 +
X"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.1544943820224719,"j∈Ni
eβ(Sij−λ)]
	
(3)"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.15730337078651685,"where α, β and λ are hyperparameters and Pi and Ni represent the index set of positive and
negative samples of xi1 respectively. MS loss also utilizes the hard mining strategy to ﬁlter out too
uninformative feature pairs, i.e. too similar positive pairs and too dissimilar negative pairs:"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.1601123595505618,"Pi = {j|Sij < max
yk̸=yi Sik + ϵ}
Ni = {j|Sij > min
yk=yi Sik −ϵ}
(4)"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.16292134831460675,"where ϵ is another hyperparameter in MS loss. At each iteration we use MS loss on the union of new
samples and replay samples LMS(Bn ∪Br) to train the model. The sampling of Br and update of
M are the same as ER-reservoir."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.16573033707865167,"To show the connection between LMS and the generative classiﬁer in Eq (2), we conduct some
theoretical analyses."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.16853932584269662,"Proposition 1. Assume dataset D = {(xi, yi)}n
i=1 is class-balanced and has C classes each of
which has n0 samples. For a generative model p(z, y), assume p(y) actually obeys the uniform
distribution and p(z|y = c) = N(z|µD
c , I) where µD
c =
1
n0
Pn
i=1 ziI[yi = c]. For MS loss assume
hard mining in Eq (4) is not employed. Then we have:"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.17134831460674158,"LMS
c
≥LGen−Bin
(5)"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.17415730337078653,"where
c
≥stands for upper than, up to an additive constant c and LGen−Bin is deﬁned in the following:"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.17696629213483145,"LGen−Bin = −1 n n
X"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.1797752808988764,"i=1
log p(zi|y = yi) + 1 nC n
X i=1 C
X"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.18258426966292135,"c=1
log p(zi|y = c)
(6)"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.1853932584269663,1The positive samples have the same labels as xi while the negative samples have different labels from xi.
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.18820224719101122,Under review as a conference paper at ICLR 2022
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.19101123595505617,"The proof of Proposition 1 is in Appendix. Proposition 1 shows LMS is an upper bound of LGen−Bin
and thus is an alternative to minimizing LGen−Bin. The ﬁrst term of LGen−Bin aims to minimize the
negative log-likelihood of the class-conditional generative classiﬁer, while the second term maximizes
the conditional entropy H(Z|Y ) of labels Y and features Z. It should be noticed LGen−Bin depends
on modeling p(z|y). With uniform p(y), classifying using p(z|y) equals to classifying using p(z, y),
and H(Z|Y ) is equivalent to H(Z, Y ), which can be regarded as a regularizer against features
collapsing. Thus, LGen−Bin actually optimizes the model in a generative way. The assumptions
in Proposition 1 are similar with those in Section 3.2 about NCM classiﬁer. The difference lies in
that NCM classiﬁer uses {µM
c } computed on replay memory M to approximate {µD
c }. Therefore,
Proposition 1 reveals that MS loss optimizes the feature space in a generative way and it models
p(z|y) for classiﬁcation which is consistent with the NCM classiﬁer."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.19382022471910113,"The real class means {µD
c } depend on all training data and change with the update of f(θ) so that
are intractable in online settings. MS loss can be efﬁciently computed as it does not depend on
{µD
c } thus the model can be trained efﬁciently. During inference, we use approximate class means
µM
c
to classify. In Figure 1b, on 5-task Split CIFAR10 benchmark, we empirically show compared
to softmax classiﬁer, on old tasks, our method achieves much higher accuracy and much lower
forgetting, which implies MS loss is an effective objective to train the model f and class mean µM
c
of replay memory is a good approximation of µD
c ."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.19662921348314608,"With discriminative loss like CE loss, the classiﬁer models a discriminative model p(y|z). Therefore,
if training with discriminative loss and inference with NCM classiﬁer based on the generative model
p(z|y), we can not expect to obtain good results. In the next section, experiments will verify this
conjecture. In contrast, the way to train and inference are coincided in proposed generative framework."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.199438202247191,"3.4
A HYBRID GENERATIVE/DISCRIMINATIVE LOSS"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.20224719101123595,"However, when addressing classiﬁcation tasks, generative classiﬁer has natural weakness, since
modeling joint distribution p(x, y) is much tougher than modeling conditional distribution p(y|x)
for NNs. Moreover, in preliminary experiments, we found if only trained with MS loss, the NCM
classiﬁer’s performance degenerates as the expected number of classes in Bn at each iteration
increases. This phenomenon is attributed to the inadequate ability to learn from new data, instead
of catastrophic forgetting. We speculate because the size of Bn is always ﬁxed to a small value (e.g.
10) in online CIL settings, the number of positive pairs in Bn decreases as the expected number of
classes increases."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.2050561797752809,"To remedy this problem, we take advantage of discriminative losses for fast adaptation in online
setting. To this end, we introduce Proxy-NCA (PNCA) (Movshovitz-Attias et al., 2017), a proxy-
based DML loss, as an auxiliary loss. For each class, PNCA loss maintains “proxies” as the real
feature to utilize the limited data in a minibatch better, which leads to convergence speed-up compared
to pair-based DML losses. Concretely, when a new class c emerges, we assign one trainable proxy
pc ∈Rd to it. PNCA loss is computed as:"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.20786516853932585,LP NCA(D) = −1 |D| X
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.21067415730337077,"(x,y)∈D
log
e−∥f(x)−py∥2
2
P ˜
C
c=1 e−∥f(x)−pc∥2
2
(7)"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.21348314606741572,"Movshovitz-Attias et al. (2017) suggest all proxies have the same norm NP and all features have the
norm NF . The latter satisﬁes as in MS loss the feature is ℓ2-normalized, i.e. NF = 1. We also set
NP = 1 by normalizing all proxies after each SGD-like update. In this way, LP NCA is equivalent to
a CE loss with ℓ2-normalized row vectors of W and without bias b, and thus we use PNCA instead
of CE loss to keep utilizing the normalized features of MS loss. Our full training objective is a hybrid
of generative and discriminative losses:
LHybrid = LMS + γLP NCA
(8)
where γ is a hyperparameter to control the weight of LP NCA. In general, generative classiﬁers have
a smaller variance but higher bias than discriminative classiﬁers, and using such a hybrid loss can
achieve a better bias-variance tradeoff (Bouchard & Triggs, 2004). Thus we think introducing the
discriminative loss LP NCA can reduce the bias of model so that boost the ability to learn from new
data."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.21629213483146068,"It should be noticed we train the model with LHybrid(Bn ∪Br), while we only use NCM classiﬁer in
Eq (2) for inference. In all experiments, we set α = 2, β = 50, ϵ = 0.1 following Wang et al. (2019)"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.21910112359550563,Under review as a conference paper at ICLR 2022
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.22191011235955055,"1000
2000
3000
4000
5000
Time step of samples 0.0 0.2 0.4 0.6 0.8 1.0"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.2247191011235955,Probability of each class
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.22752808988764045,Split CIFAR10
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.2303370786516854,"1000
2000
3000
4000
5000
Time step of samples 0.0 0.2 0.4 0.6 0.8 1.0"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.23314606741573032,Probability of each class
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.23595505617977527,Smooth CIFAR10
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.23876404494382023,"class 0
class 1
class 2
class 3
class 4
class 5
class 6
class 7
class 8
class 9"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.24157303370786518,"Figure 2: The probability distribution on class of the
sample at each time step on Split CIFAR10 (left) and
Smooth CIFAR10 (right)."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.2443820224719101,"1
2
3
4
5
6
7
8
9
10
Task Number 10 20 30 40 50"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.24719101123595505,Average accuracy
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.25,Split CIFAR-100 (10-task)
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.25280898876404495,"fine-tune
ER-reservoir
A-GEM
GSS-Greedy
MIR"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.2556179775280899,"GMED-ER
GMED-MIR
CoPE
Ours"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.25842696629213485,"1
2
3
4
5
6
7
8
9
10
Task Number 10 20 30 40 50"
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.2612359550561798,Split miniImageNet (10-task)
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.2640449438202247,"Figure 3: Average Accuracy on already learned
task during training."
"TRAINING WITH A PAIR-BASED METRIC LEARNING LOSS FROM A GENERATIVE
PERPECTIVE",0.26685393258426965,"and λ = 0.5 which always works well in online CIL settings. We only need to tune hyperparemeters
γ in Eq (8) for different experiment settings."
EXPERIMENTS,0.2696629213483146,"4
EXPERIMENTS"
EXPERIMENT SETUP,0.27247191011235955,"4.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.2752808988764045,"Datasets First, we conduct experiments on Split datasets which are commonly used in CIL and
online CIL literature. On Split MNIST and CIFAR10, the datasets are split into 5 tasks each of which
comprises 2 classes. On CIFAR100 and miniImageNet with 100 classes, we split them into 10 or 20
tasks. The number of classes in each task is 10 or 5 respectively. For MNIST we select 5k samples for
training following Aljundi et al. (2019a) and we use full training data for other datasets. To simulate
a task-free scenario, task boundaries are not informed during training (Jin et al., 2020)."
EXPERIMENT SETUP,0.27808988764044945,"To conduct a thorough evaluation in task-free scenarios, we design a new type of data streams. For a
data stream with C classes, we assume the length of stream is n and n0 = n/C. We denote pc(t) as
the occurrence probability of class c at time step t and assume pc(t) ∼N(t|(2c −1)n0/2, n0/2). At
each time step t, we calculate p(t) = (p1(t), . . . , pC(t)) and normalize p(t) as the parameters of a
Categorical distribution from which a class index ct is sampled. Then we sample one data of class
ct without replacement. In this setting, data distribution changes smoothly and there is no notion of
task. We call such data streams as Smooth datasets. To build Smooth datasets, we set n = 5k on
CIFAR10 and n = 40k on CIFAR100 and miniImageNet, using all classes in each dataset. For all
datasets, the size of minibatch Bn is 10. In Figure 2 we plot the probability distribution on class at
each time step in the data stream generation process for Split CIFAR10 and Smooth CIFAR10. For
better comparison, we set the length of data stream of two datasets both 5k. Split CIFAR10 has clear
task boundaries and within one task the distribution on class is unchanged and uniform. However, On
Smooth CIFAR10, the distribution on class keeps changing and there is no notion of task."
EXPERIMENT SETUP,0.2808988764044944,"Baselines We compare our method against a series of state-of-the-art online CIL methods, including:
ER-reservoir, A-GEM, GSS, MIR, GMED, CoPE and ASER. We have brieﬂy introduced them in
Section 2. Specially, we use GSS-greedy and ASERµ which are the best variants in the corresponding
paper. For GMED, we evaluate both GMED-ER and GMED-MIR. We also evaluate ﬁne-tune baseline
without any CL strategy. For all baselines and our method, the model is trained with 1 epoch, i.e.
online CL setting. In addition, the performances of i.i.d online and i.i.d ofﬂine are also provided, by
training the model 1 and 5 epochs respectively on i.i.d data streams. We reimplement all baselines
except ASERµ, whose results are from the original paper. Model Following Aljundi et al. (2019a),
the model f is a 2-layer MLP with 400 hidden units for MNIST and a reduced ResNet18 for other
datasets. For baselines with ER strategy, the size of replay minibatch Br is always 10. The budget of
memory M is 500 on MNIST and 1000 on others. We use a relatively small budget |M| to mimic a
practical setting. All models are optimized by SGD. The single-head evaluation is always used for
CIL. More details about datasets, hyperparameter selection and evaluation metrics are in Appendix."
MAIN RESULTS ON SPLIT DATASETS,0.28370786516853935,"4.2
MAIN RESULTS ON Split DATASETS"
MAIN RESULTS ON SPLIT DATASETS,0.28651685393258425,"On Split datasets, we use Average Accuracy and Average Forgetting after training all
tasks (Chaudhry et al., 2019b) for evaluation, which are reported in Table 1 and Table 2 respectively.
For each metric, we report the mean of 15 runs and the 95% conﬁdence interval."
MAIN RESULTS ON SPLIT DATASETS,0.2893258426966292,Under review as a conference paper at ICLR 2022
MAIN RESULTS ON SPLIT DATASETS,0.29213483146067415,"Methods
MNIST
(5-task)
CIFAR10
(5-task)
CIFAR100
(10-task)
CIFAR100
(20-task)
miniImageNet
(10-task)
miniImageNet
(20-task)"
MAIN RESULTS ON SPLIT DATASETS,0.2949438202247191,"ﬁne-tune
19.66±0.05
18.40±0.17
6.26±0.30
3.61±0.24
4.43±0.19
3.12±0.15
ER-reservoir
82.34±2.48
39.88±1.52
11.59±0.26
8.95±0.26
10.24±0.41
8.33±0.66
A-GEM
25.99±1.62
18.01±0.17
6.48±0.18
3.66±0.09
4.68±0.11
3.37±0.13
GSS-Greedy
83.88±0.72
39.07±2.02
10.78±0.28
7.94±0.47
9.20±0.61
7.76±0.35
MIR
86.81±0.95
42.10±1.27
11.52±0.37
8.61±0.34
9.99±0.49
7.93±0.70
GMED-ER
81.71±1.87
42.65±1.27
11.86±0.36
9.16±0.47
9.53±0.66
8.14±0.58
GMED-MIR
88.70±0.81
44.53±2.23
11.58±0.51
8.48±0.37
9.24±0.53
7.75±0.80
CoPE
87.58±0.65
47.36±0.96
10.79±0.36
9.11±0.44
11.03±0.68
9.92±0.61
ASERµ
∗
–
43.50±1.40
14.00±0.40
–
12.20±0.80
–
Ours
88.79±0.26
51.84±0.91
15.56±0.39
13.65±0.35
16.05±0.38
15.15±0.36
i.i.d. online
86.35±0.64
62.37±1.36
20.62±0.48
20.62±0.48
18.02±0.63
18.02±0.63
i.i.d. ofﬂine
92.44±0.61
79.90±0.51
45.59±0.29
45.59±0.29
38.63±0.59
38.63±0.59"
MAIN RESULTS ON SPLIT DATASETS,0.29775280898876405,"Table 1: Average Accuracy of 15 runs on Split datasets. Higher is better. ∗indicates the results are from the
original paper."
MAIN RESULTS ON SPLIT DATASETS,0.300561797752809,"Methods
MNIST
(5-task)
CIFAR10
(5-task)
CIFAR100
(10-task)
CIFAR100
(20-task)
miniImageNet
(10-task)
miniImageNet
(20-task)"
MAIN RESULTS ON SPLIT DATASETS,0.30337078651685395,"ﬁne-tune
99.24±0.09
85.45±0.63
51.60±0.77
65.51±0.78
41.12±0.82
52.99±0.89
ER-reservoir
18.33±1.77
52.72±1.90
45.94±0.55
57.31±0.71
36.05±0.78
47.70±0.90
A-GEM
89.90±2.02
82.80±0.73
54.15±0.42
67.61±0.53
43.31±0.52
54.47±0.78
GSS-Greedy
15.13±0.99
49.96±2.82
44.30±0.57
53.87±0.54
36.17±0.58
45.91±0.79
MIR
9.71±1.39
44.34±2.65
46.52±0.52
56.58±0.62
36.98±0.78
45.84±1.11
GMED-ER
16.21±2.70
44.93±1.68
46.35±0.50
57.76±0.94
35.22±1.16
45.08±1.28
GMED-MIR
12.52±1.05
39.88±2.23
46.56±0.65
58.14±0.55
34.79±1.01
45.50±1.49
CoPE
9.51±1.15
40.01±1.80
36.51±0.86
43.82±0.62
29.43±0.98
40.99±1.02
ASERµ
∗
–
47.90±1.60
45.00±0.70
–
28.00±1.30
–
Ours
9.36±0.37
35.37±1.35
21.79±0.69
27.10±1.10
21.26±0.59
24.98±0.87"
MAIN RESULTS ON SPLIT DATASETS,0.3061797752808989,"Table 2: Average Forgetting of 15 runs on Split datasets. Lower is better. ∗indicates the results are from the
original paper."
MAIN RESULTS ON SPLIT DATASETS,0.3089887640449438,"In Table 1, we can ﬁnd our method outperforms all baselines on all 6 settings. The improvement
of our method is signiﬁcant except on MNIST, where GMED-MIR is competitive with our method.
An interesting phenomenon is all existing methods do not have a substantial improvement over
ER-reservoir on CIFAR100 and miniImageNet, except ASER. We argue for online CIL problem,
we should pay more attention to complex settings. Nevertheless, our method is superior to ASER
obviously, especially on CIFAR10 and miniImageNet. Table 2 shows the forgetting of our method is
far lower than other methods based on the softmax classiﬁer, except on MNIST. Figure 3 shows our
method is almost consistently better than all baselines during the whole learning processes. More
results with various memory sizes can be found in Appendix."
MAIN RESULTS ON SPLIT DATASETS,0.31179775280898875,"Ablation Study We also conduct ablation study about training objective and classiﬁer in Table 4. The
ER-reservoir corresponds to the ﬁrst row and our method corresponds to the last row. Firstly, we ﬁnd
for ER-reservoir, replacing softmax classiﬁer with NCM classiﬁer makes a substantial improvement
on CIFAR10. However, it has no effect on more complex CIFAR100 (row 1&2). Secondly, only using
MS loss works very well on CIFAR10 while on CIFAR100 poor ability to learn from new data limits
its performance (row 3&6). Lastly, when hybrid loss is used, the NCM classiﬁer is much better than
proxy-based classiﬁer (row 5&6), and MS loss is critical for NCM classiﬁer (row 4&6). Note that
hybrid loss does not outperform MS loss much on Split-CIFAR10. This is because in Split-CIFAR10,
a minibatch of new data contains a maximum of two classes, and thus the positive pairs are enough
fo MS loss to learn new knowledge well. These results verify our statement in Section 3.2."
MAIN RESULTS ON SPLIT DATASETS,0.3146067415730337,"Comparison with Logits Bias Solutions for Conventional CIL Setting Although existing CIL
methods to alleviate logits bias are not applicable for online CIL settings as task boundaries are
necessary, after being modiﬁed in some ways they can be adapted to online CIL. To better reﬂect
the contribution of our method, we adapt iCaRL (Rebufﬁet al., 2017) and BiC (Wu et al., 2019)
to online CIL and compare modiﬁed iCaRL and modiﬁed BiC with our method in Table 3. iCaRL
replaces softmax classiﬁer with NCM classiﬁer and BiC uses a linear bias correction layer to reduce
logits bias. and iCaRL is modiﬁed in the following way: at each iteration, we minimize binary CE
loss used by iCaRL which encourages the model to mimic the outputs for all learned classes of the"
MAIN RESULTS ON SPLIT DATASETS,0.31741573033707865,Under review as a conference paper at ICLR 2022
MAIN RESULTS ON SPLIT DATASETS,0.3202247191011236,"Methods
MNIST
(5-task)
CIFAR10
(5-task)
CIFAR100
(20-task)
miniImageNet
(20-task)"
MAIN RESULTS ON SPLIT DATASETS,0.32303370786516855,"modiﬁed
iCaRL"
MAIN RESULTS ON SPLIT DATASETS,0.3258426966292135,"34.58
±1.18
29.77
±0.91
5.38
±0.26
7.16
±0.33
modiﬁed
BiC"
MAIN RESULTS ON SPLIT DATASETS,0.32865168539325845,"83.33
±1.35
43.65
±2.50
8.72
±0.30
7.91
±0.55"
MAIN RESULTS ON SPLIT DATASETS,0.33146067415730335,"Ours
88.79
±0.26
51.84
±0.91
13.65
±0.35
15.15
±0.36"
MAIN RESULTS ON SPLIT DATASETS,0.3342696629213483,"Table 3: The comparison between modiﬁed iCaRL,
modiﬁed BiC and our method (Ours) on Split
datasets."
MAIN RESULTS ON SPLIT DATASETS,0.33707865168539325,"CIFAR-100
miniImageNet
0 250 500 750 1000 1250 1500 1750 2000"
MAIN RESULTS ON SPLIT DATASETS,0.3398876404494382,Training time (seconds)
MAIN RESULTS ON SPLIT DATASETS,0.34269662921348315,"ER-reservoir
A-GEM"
MAIN RESULTS ON SPLIT DATASETS,0.3455056179775281,"GSS-Greedy
MIR"
MAIN RESULTS ON SPLIT DATASETS,0.34831460674157305,"GMED-ER
GMED-MIR"
MAIN RESULTS ON SPLIT DATASETS,0.351123595505618,"CoPE
Ours (a)"
MAIN RESULTS ON SPLIT DATASETS,0.3539325842696629,"CIFAR-100
miniImageNet
0 1 2 3 4 5 6 7"
MAIN RESULTS ON SPLIT DATASETS,0.35674157303370785,Test time (seconds)
MAIN RESULTS ON SPLIT DATASETS,0.3595505617977528,"Softmax Classifier
Generative Classifier (b)"
MAIN RESULTS ON SPLIT DATASETS,0.36235955056179775,"Figure 4: Comparison of training time (a) and test time
(b) on Split CIFAR100 and miniImageNet. The number
of tasks is 10."
MAIN RESULTS ON SPLIT DATASETS,0.3651685393258427,"Training
Inference
CIFAR10
CIFAR100
CE loss
Softmax (Dis)
39.88±1.52
8.95±0.26
CE loss
NCM (Gen)
44.46±0.95
8.96±0.38
MS loss
NCM (Gen)
51.72±1.02
9.99±0.32
PNCA loss
NCM (Gen)
41.91±1.78
9.31±0.58
Hybrid loss
Proxy (Dis)
48.16±1.21
7.02±0.75
Hybrid loss
NCM (Gen)
51.84±0.91
13.65±0.35"
MAIN RESULTS ON SPLIT DATASETS,0.36797752808988765,"Table 4: Ablation study on Split CIFAR10 and 20-
task Split CIFAR100. We show the performances of
different combinations of losses and inference ways.
Dis: Discriminative, Gen: Generative."
MAIN RESULTS ON SPLIT DATASETS,0.3707865168539326,"Method
CIFAR10
CIFAR100
miniImageNet"
MAIN RESULTS ON SPLIT DATASETS,0.37359550561797755,"ﬁne-tune
10.02±0.03
1.02±0.03
1.02±0.04
ER-reservoir
20.89±2.07
3.84±0.42
6.85±0.70
MIR
18.75±2.53
4.35±0.53
6.09±1.04
GMED-MIR
18.78±2.31
3.68±0.48
7.22±0.81
Ours
34.18±0.81
10.54±0.38
12.24±0.19
i.i.d online
31.23±2.11
18.08±0.62
17.23±0.42
i.i.d ofﬂine
48.37±1.23
42.68±0.37
39.82±0.46"
MAIN RESULTS ON SPLIT DATASETS,0.37640449438202245,Table 5: Final accuracy of 15 runs on Smooth datasets.
MAIN RESULTS ON SPLIT DATASETS,0.3792134831460674,"old model after the last iteration. We use reservoir sampling for memory update. NCM classiﬁer is
used for inference. For modiﬁed BiC, we use the linear bias correction layer of BiC to correct the
logits for all learned classes only before test as task boundaries are unavailable in online CIL setting.
As shown in Table 3, modiﬁed iCaRL performs very badly and the performances of modiﬁed BIC
are much worse than our method. These results imply adapting existing methods to online setting
can not alleviate logits bias effectively for online CIL. Therefore, our method makes a substantial
contribution for online CIL."
MAIN RESULTS ON SPLIT DATASETS,0.38202247191011235,"Time Comparison In Figure 4a, we report the training time of different methods. The training time
of our method is only a bit higher than ER-reservoir."
MAIN RESULTS ON SPLIT DATASETS,0.3848314606741573,"Most baselines, such as GSS, MIR, and GMED, improve ER-reservoir by designing new memory
update and sampling strategies which depend on extra gradient computations and thus are time-
consuming. The inference costs of softmax classiﬁer and our NCM classiﬁer are displayed in
Figure 4b. We can ﬁnd the extra time of NCM to compute the class means is (about 3%) slight, as the
size of memory is limited."
RESULTS ON TASK-FREE SMOOTH DATASETS,0.38764044943820225,"4.3
RESULTS ON TASK-FREE Smooth DATASETS"
RESULTS ON TASK-FREE SMOOTH DATASETS,0.3904494382022472,"In newly designed task-free Smooth datasets, the new classes emerge irregularly and the distribution
on class changes at each time step. In Table 5, we compare our method with several baselines on three
smooth datasets. The metric is ﬁnal accuracy after learning the whole data stream. We can ﬁnd these
datasets are indeed more complex as ﬁne-tune can only classify correctly on the last class, which is
due to the higher imbalance of data streams. For this reason, baselines such as ER-reservoir and MIR
degrade obviously compared with split datsets. However, our method performs best consistently."
CONCLUSION,0.39325842696629215,"5
CONCLUSION"
CONCLUSION,0.3960674157303371,"In this work, we tackle with online CIL problem from a generative perspective to bypass logits bias
problem in commonly used softmax classiﬁer. We ﬁrst propose to replace softmax classiﬁer with a
generative classiﬁer. Then we introduce MS loss for training and prove theoretically that it optimizes
the feature space in a generative way. We further propose a hybrid loss to boost the model’s ability
to learn from new data. Experimental results show the signiﬁcant and consistent superiority of our
method compared to existing state-of-the-art methods."
CONCLUSION,0.398876404494382,Under review as a conference paper at ICLR 2022
REFERENCES,0.40168539325842695,REFERENCES
REFERENCES,0.4044943820224719,"Hongjoon Ahn and Taesup Moon. A simple class decision balancing for incremental learning. CoRR,
abs/2003.13947, 2020. URL https://arxiv.org/abs/2003.13947."
REFERENCES,0.40730337078651685,"Rahaf Aljundi,
Eugene Belilovsky,
Tinne Tuytelaars,
Laurent Charlin,
Massimo Cac-
cia,
Min
Lin,
and
Lucas
Page-Caccia.
Online
continual
learning
with
maxi-
mal
interfered
retrieval.
In
NeurIPS,
2019a.
URL
http://papers.nips.cc/paper/
9357-online-continual-learning-with-maximal-interfered-retrieval."
REFERENCES,0.4101123595505618,"Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In CVPR,
2019b. doi: 10.1109/CVPR.2019.01151. URL http://openaccess.thecvf.com/content_CVPR_2019/
html/Aljundi_Task-Free_Continual_Learning_CVPR_2019_paper.html."
REFERENCES,0.41292134831460675,"Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio.
Gradient based sample se-
lection for online continual learning.
In NeurIPS, 2019c.
URL http://papers.nips.cc/paper/
9354-gradient-based-sample-selection-for-online-continual-learning."
REFERENCES,0.4157303370786517,"Eden Belouadah and Adrian Popescu. IL2M: class incremental learning with dual memory. In ICCV,
2019."
REFERENCES,0.41853932584269665,"Guillaume Bouchard and Bill Triggs. The tradeoff between generative and discriminative classiﬁers.
2004."
REFERENCES,0.42134831460674155,"Malik Boudiaf, Jérôme Rony, Imtiaz Masud Ziko, Eric Granger, Marco Pedersoli, Pablo Piantanida,
and Ismail Ben Ayed. A unifying mutual information view of metric learning: Cross-entropy vs.
pairwise losses. In ECCV, 2020. doi: 10.1007/978-3-030-58539-6\_33. URL https://doi.org/10.
1007/978-3-030-58539-6_33."
REFERENCES,0.4241573033707865,"Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient
lifelong learning with A-GEM. In ICLR, 2019a. URL https://openreview.net/forum?id=Hkf2_
sC5FX."
REFERENCES,0.42696629213483145,"Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual
learning. arXiv preprint arXiv:1902.10486, 2019b."
REFERENCES,0.4297752808988764,"Xiaoan Ding, Tianyu Liu, Baobao Chang, Zhifang Sui, and Kevin Gimpel.
Discriminatively-
tuned generative classiﬁers for robust natural language inference.
In EMNLP, 2020.
URL
https://www.aclweb.org/anthology/2020.emnlp-main.657/."
REFERENCES,0.43258426966292135,"Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural
networks. CoRR, abs/1701.08734, 2017."
REFERENCES,0.4353932584269663,"Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In CVPR, 2006."
REFERENCES,0.43820224719101125,"Elad Hoffer and Nir Ailon. Deep metric learning using triplet network. In ICLR Workshop, 2015.
URL http://arxiv.org/abs/1412.6622."
REFERENCES,0.4410112359550562,"Xisen Jin, Junyi Du, and Xiang Ren. Gradient based memory editing for task-free continual learning.
CoRR, abs/2006.15294, 2020. URL https://arxiv.org/abs/2006.15294."
REFERENCES,0.4438202247191011,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, et al.
Overcoming catastrophic forgetting in neural networks. PNAS, 2017."
REFERENCES,0.44662921348314605,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.449438202247191,"Matthias De Lange and Tinne Tuytelaars. Continual prototype evolution: Learning online from
non-stationary data streams. CoRR, abs/2009.00919, 2020. URL https://arxiv.org/abs/2009.00919."
REFERENCES,0.45224719101123595,"Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G.
Slabaugh, and Tinne Tuytelaars. Continual learning: A comparative study on how to defy forgetting
in classiﬁcation tasks. CoRR, abs/1909.08383, 2019. URL http://arxiv.org/abs/1909.08383."
REFERENCES,0.4550561797752809,Under review as a conference paper at ICLR 2022
REFERENCES,0.45786516853932585,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.4606741573033708,"Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture model
for task-free continual learning. In ICLR, 2020. URL https://openreview.net/forum?id=SJxSOJStPr."
REFERENCES,0.46348314606741575,"Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 2018."
REFERENCES,0.46629213483146065,"Zheda Mai, Dongsub Shim, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. Online
class-incremental continual learning with adversarial shapley value. AAAI, abs/2009.00093, 2021."
REFERENCES,0.4691011235955056,"Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning. In CVPR, 2018."
REFERENCES,0.47191011235955055,"Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation. Elsevier, 1989."
REFERENCES,0.4747191011235955,"Thomas Mensink, Jakob J. Verbeek, Florent Perronnin, and Gabriela Csurka.
Distance-based
image classiﬁcation: Generalizing to new classes at near-zero cost. TPAMI, 35, 2013. doi:
10.1109/TPAMI.2013.83. URL https://doi.org/10.1109/TPAMI.2013.83."
REFERENCES,0.47752808988764045,"Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and Saurabh Singh. No
fuss distance metric learning using proxies. In ICCV, 2017."
REFERENCES,0.4803370786516854,"Andrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classiﬁers: A compari-
son of logistic regression and naive bayes. In NIPS, 2001. URL http://papers.nips.cc/paper/
2020-on-discriminative-vs-generative-classiﬁers-a-comparison-of-logistic-regression-and-naive-bayes."
REFERENCES,0.48314606741573035,"German Ignacio Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter.
Continual lifelong learning with neural networks: A review. Neural Networks, 2019."
REFERENCES,0.4859550561797753,"Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classiﬁer and representation learning. In CVPR, 2017."
REFERENCES,0.4887640449438202,"Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro.
Learning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR,
2019. URL https://openreview.net/forum?id=B1gTShAct7."
REFERENCES,0.49157303370786515,"Anthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7(2), 1995.
doi: 10.1080/09540099550039318. URL https://doi.org/10.1080/09540099550039318."
REFERENCES,0.4943820224719101,"Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In NIPS, 2017."
REFERENCES,0.49719101123595505,"Gido M. van de Ven and Andreas S. Tolias. Three scenarios for continual learning. In NeurIPS
Continual Learning workshop, 2018."
REFERENCES,0.5,"Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning.
In Advances in Neural Information Processing Sys-
tems 29:
Annual Conference on Neural Information Processing Systems 2016, Decem-
ber 5-10, 2016, Barcelona, Spain, pp. 3630–3638, 2016.
URL http://papers.nips.cc/paper/
6385-matching-networks-for-one-shot-learning."
REFERENCES,0.5028089887640449,"Jeffrey S. Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software
(TOMS), 1985."
REFERENCES,0.5056179775280899,"Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss
with general pair weighting for deep metric learning. In CVPR, 2019. doi: 10.1109/CVPR.2019.
00516. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Multi-Similarity_
Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.html."
REFERENCES,0.5084269662921348,"Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large
scale incremental learning. In CVPR, 2019."
REFERENCES,0.5112359550561798,Under review as a conference paper at ICLR 2022
REFERENCES,0.5140449438202247,"Dani Yogatama, Chris Dyer, Wang Ling, and Phil Blunsom. Generative and discriminative text
classiﬁcation with recurrent neural networks. CoRR, abs/1703.01898, 2017. URL http://arxiv.org/
abs/1703.01898."
REFERENCES,0.5168539325842697,"Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui,
and Joost van de Weijer. Semantic drift compensation for class-incremental learning. In CVPR,
2020."
REFERENCES,0.5196629213483146,"Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning using
online variational bayes. arXiv preprint arXiv:1803.10123, 2018."
REFERENCES,0.5224719101123596,"Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and
fairness in class incremental learning. In CVPR, 2020."
REFERENCES,0.5252808988764045,"A
PROOFS"
REFERENCES,0.5280898876404494,"A.1
PROPOSITION 1"
REFERENCES,0.5308988764044944,"Proof. For simplicity, we ignore the coefﬁcient 1"
REFERENCES,0.5337078651685393,"n both in LMS and LGen−Bin in the proof. We
denote the two parts in the summation of LMS as L+
MS and L−
MS respectively, i.e.:"
REFERENCES,0.5365168539325843,"L+
MS = n
X i=1"
REFERENCES,0.5393258426966292,"1
α log[1 +
X"
REFERENCES,0.5421348314606742,"j∈Pi
e−α(Sij−λ)]"
REFERENCES,0.5449438202247191,"L−
MS = n
X i=1"
REFERENCES,0.547752808988764,"1
β log[1 +
X"
REFERENCES,0.550561797752809,"j∈Ni
eβ(Sij−λ)] (9)"
REFERENCES,0.5533707865168539,"Without hard mining, for L+
MS we have:"
REFERENCES,0.5561797752808989,"L+
MS = n
X i=1"
REFERENCES,0.5589887640449438,"1
α log[1 +
X"
REFERENCES,0.5617977528089888,"j:yj=yi
e−α(Sij−λ)] ≥ n
X i=1"
REFERENCES,0.5646067415730337,"1
α log[
X"
REFERENCES,0.5674157303370787,"j:yj=yi
e−α(Sij−λ)] c
≥ n
X i=1"
REFERENCES,0.5702247191011236,"1
α[ 1 n0 X"
REFERENCES,0.5730337078651685,"j:yj=yi
log e−α(Sij−λ)]"
REFERENCES,0.5758426966292135,"▷Jensen’s inequality = n
X i=1"
REFERENCES,0.5786516853932584,"1
n0
[
X"
REFERENCES,0.5814606741573034,"j:yj=yi
−(Sij −λ)] c= − n
X i=1 1
n0 X"
REFERENCES,0.5842696629213483,"yj=yi
Sij (10)"
REFERENCES,0.5870786516853933,Under review as a conference paper at ICLR 2022
REFERENCES,0.5898876404494382,"where c= stands for equal to, up to an additive constant. For L−
MS we can write:"
REFERENCES,0.5926966292134831,"L−
MS = n
X i=1"
REFERENCES,0.5955056179775281,"1
β log[1 +
X"
REFERENCES,0.598314606741573,"j:yj̸=yi
eβ(Sij−λ)] ≥ n
X i=1"
REFERENCES,0.601123595505618,"1
β log[
X"
REFERENCES,0.6039325842696629,"j:yj̸=yi
eβ(Sij−λ)] c
≥ n
X i=1"
REFERENCES,0.6067415730337079,"1
β [
1
n0(C −1) X"
REFERENCES,0.6095505617977528,"j:yj̸=yi
log eβ(Sij−λ)]"
REFERENCES,0.6123595505617978,"▷Jensen’s inequality = n
X i=1"
REFERENCES,0.6151685393258427,"1
n0(C −1) X"
REFERENCES,0.6179775280898876,"j:yj̸=yi
Sij (11)"
REFERENCES,0.6207865168539326,"According to Eq (10) and Eq (11), we have:"
REFERENCES,0.6235955056179775,"LMS
c
≥ n
X i=1 
−1 n0 X"
REFERENCES,0.6264044943820225,"j:yj=yi
Sij +
1
(C −1)n0 X"
REFERENCES,0.6292134831460674,"j:yj̸=yi
Sij
	
(12)"
REFERENCES,0.6320224719101124,"Now we consider LGen−Bin. Firstly, it can be written:"
REFERENCES,0.6348314606741573,"LGen−Bin c= n
X i=1"
REFERENCES,0.6376404494382022,"
−log p(zi|y = yi)"
REFERENCES,0.6404494382022472,"+
1
C −1 C
X"
REFERENCES,0.6432584269662921,"c=1,c̸=yi
log p(zi|y = c)
	
(13)"
REFERENCES,0.6460674157303371,"For convenience, we denote the features of data samples whose labels are c as {zci}n0
i=1. For the ﬁrst
part in the right hand of Eq (13), we have: n
X"
REFERENCES,0.648876404494382,"i=1
−log p(zi|y = yi) c=1 2 n
X"
REFERENCES,0.651685393258427,"i=1
∥zi −µD
c ∥2
2
▷p(z|y = c) = N(z|µD
c , I) =1 2 C
X c=1 n0
X"
REFERENCES,0.6544943820224719,"i=1
∥zci −µD
c ∥2
2 =1 2 C
X c=1 n0
X i=1"
REFERENCES,0.6573033707865169,"
∥zci∥2
2 −2zT
ciµD
c + ∥µD
c ∥2
2 c=1 2 C
X c=1"
REFERENCES,0.6601123595505618," n0
X"
REFERENCES,0.6629213483146067,"i=1
−2n0∥µD
c ∥2
2 + n0∥µD
c ∥2
2
	
▷∥z∥2
2 = 1 =1 2 C
X c=1 n0
X"
REFERENCES,0.6657303370786517,"i=1
−n0∥µD
c ∥2
2 =1 2 C
X c=1 n0
X i=1 n0
X"
REFERENCES,0.6685393258426966,"j=1
−1"
REFERENCES,0.6713483146067416,"n0
zT
cizcj = − n
X i=1 X"
REFERENCES,0.6741573033707865,j:yj=yi
REFERENCES,0.6769662921348315,"1
2n0
Sij (14)"
REFERENCES,0.6797752808988764,Under review as a conference paper at ICLR 2022
REFERENCES,0.6825842696629213,"Keep in mind that the mean of class c µD
c =
1
n0
Pn0
i=1 zci. For the second part in the right hand of
Eq (13), we have: n
X i=1 C
X"
REFERENCES,0.6853932584269663,"c=1,c̸=yi
log p(zi|y = c) =1 2 n
X i=1 C
X"
REFERENCES,0.6882022471910112,"c=1,c̸=yi
−∥zi −µD
c ∥2
2 =1 2 n
X i=1 C
X"
REFERENCES,0.6910112359550562,"c=1,c̸=yi"
REFERENCES,0.6938202247191011,"
−∥zi∥2
2 + 2zT
i µD
c −∥µD
c ∥2
2 c=1 2 n
X i=1 C
X"
REFERENCES,0.6966292134831461,"c=1,c̸=yi
2zT
i µD
c −(C −1)n0 C
X"
REFERENCES,0.699438202247191,"c=1
∥µD
c ∥2
2
▷∥z∥2
2 = 1 =1 2 n
X i=1 C
X"
REFERENCES,0.702247191011236,j:yj̸=yi
REFERENCES,0.7050561797752809,"2
n0
zT
i zj −C −1 n0 C
X c=1 n0
X i=1 n0
X"
REFERENCES,0.7078651685393258,"j=1
zT
cizcj = n
X i=1 
X"
REFERENCES,0.7106741573033708,j:yj̸=yi
REFERENCES,0.7134831460674157,"1
n0
Sij −
X"
REFERENCES,0.7162921348314607,j:yj=yi C −1
REFERENCES,0.7191011235955056,"2n0
Sij (15)"
REFERENCES,0.7219101123595506,"According to Eq (14) and Eq (15), we have:"
REFERENCES,0.7247191011235955,"LGen−Bin c= n
X i=1 
−1 n0 X"
REFERENCES,0.7275280898876404,"j:yj=yi
Sij +
1
(C −1)n0 X"
REFERENCES,0.7303370786516854,"j:yj̸=yi
Sij
	
(16)"
REFERENCES,0.7331460674157303,"According to Eq (12) and Eq (16), we can obtain:"
REFERENCES,0.7359550561797753,"LMS
c
≥LGen−Bin
(17)"
REFERENCES,0.7387640449438202,"B
DETAILS ABOUT EXPERIMENT SETUP"
REFERENCES,0.7415730337078652,"B.1
SPLIT DATASETS"
REFERENCES,0.7443820224719101,"In Table 6 we show the detailed statistics about Split datasets. On MNIST (LeCun et al., 1998), we ran-
domly select 500 samples of the original training data for each class as training data stream, following
previous works (Aljundi et al., 2019a;c; Jin et al., 2020). On CIFAR10 and CIFAR100 (Krizhevsky
et al., 2009), We use the full training data from which 5% samples are regarded as validation set. The
original miniImageNet dataset is used for meta learning (Vinyals et al., 2016) and 100 classes are
divided into 64 classes, 16 classes, 20 classes respectively for meta-training, meta-validation and
meta-test respectively. We merge all 100 classes to conduct class-incremental learning. There are
600 samples per class in miniImageNet. We divide 600 samples into 456 samples, 24 samples and
120 samples for training, validation and test respectively. We do not adopt any data augmentation
strategy."
REFERENCES,0.7471910112359551,"B.2
SMOOTH DATASETS"
REFERENCES,0.75,"For a data stream with C classes, we assume the length of stream is N and n0 = N"
REFERENCES,0.7528089887640449,"c . We denote pc(t)
as the occurrence probability of class c at time step t and assume pc(t) ∼N(t| (2c−1)n0"
REFERENCES,0.7556179775280899,"2
, n0"
REFERENCES,0.7584269662921348,"c ). At
each time step t, we calculate p(t) = (p1(t), ..., pC(t)) and normalize p(t) as the parameters of a
Categorical distribution from which a class index ct is sampled. Then we sample one data of class ct
without replacement. In this setting, data distribution changes smoothly and there is no notion of task.
We call such data streams as Smooth datasets."
REFERENCES,0.7612359550561798,"Here we show the characteristics of Smooth datasets in detail. In Table 7, we list the detailed statistics
of Smooth datasets. Due to the randomness in the data stream generation process, the number of"
REFERENCES,0.7640449438202247,Under review as a conference paper at ICLR 2022
REFERENCES,0.7668539325842697,"Dataset
MNIST
CIFAR10
CIFAR100
miniImageNet
Image Size
(1,32,32)
(3,32,32)
(3,32,32)
(3,84,84)
#Classes
10
10
100
100
#Train Samples
5000
47500
47500
45600
#Valid Samples
10000
2500
2500
2400
#Test Samples
10000
10000
10000
12000
#Task
5
5
10 or 20
10 or 20
#Classes per Task
2
2
10 or 5
10 or 5"
REFERENCES,0.7696629213483146,Table 6: Details about Split datasets.
REFERENCES,0.7724719101123596,"samples of each class is slightly imbalanced. On CIFAR100 and miniImageNet we set the mean
number of samples of each class n0 = 400 to avoid invalid sampling when exceeding the maximum
number of sample of one class in the original training set. The last rows of Table 7 show the range of
number of samples of each classes in our experiments. For example, in 15 repeated runs, 15 different
Smooth CIFAR10 data streams are established. The number of samples of each class is in the interval
[464, 536]. It should be emphasized that in all experiments we use the same random seed to obtain
the identical 15 data streams for fair comparison."
REFERENCES,0.7752808988764045,"Some existing works (Zeno et al., 2018; Lee et al., 2020) also experiment on task-free datasets where
a data stream is still divided into different tasks but the switch of task is gradual instead of abrupt.
In fact, in their settings, task switching only occurs in a part of time, so that in the left time the
distribution of data streams is still i.i.d. In contrast, in our Smooth datasets the distribution changes at
the class level which simulates task-free class-incremental setting. In addition, the distribution of
data streams is never i.i.d, which can reﬂect real-world CL scenarios better."
REFERENCES,0.7780898876404494,"In Figure 5, we plot the curves of training loss of ER-reservoir on Split CIFAR10 and Smooth
CIFAR10. When a new task emerges (each task consists of 100 iterations), the loss on Split CIFAR10
increases dramatically. Thus some methods can detect the change of loss to inference the task
boundaries although they are not informed and conduct additional ofﬂine training phase (Aljundi
et al., 2019b). However, such a trick is not applicable on Smooth datasets therefore only the real
online CL methods can work."
REFERENCES,0.7808988764044944,"Dataset
CIFAR10
CIFAR100
miniImageNet
#Classes (C)
10
100
100
#Train Samples (length of data stream) (n)
5000
40000
40000
#Valid Samples
2500
2500
2400
#Test Samples
10000
10000
12000
Mean number of samples of one class (n0)
500
400
400
Minimum number of samples of one class
464
357
354
Maximum number of samples of one class
536
450
446"
REFERENCES,0.7837078651685393,Table 7: Details about Smooth datasets.
REFERENCES,0.7865168539325843,"B.3
HYPERPARAMETER SELECTION"
REFERENCES,0.7893258426966292,"In our method, although there are several hyperparameters in LMS and LP NCA, we only need to tune
γ which is the weight of LP NCA in LHybrid. The value of other hyperparameters in LHybrid is ﬁxed
as stated in the main text. We select γ from [0, 0.1, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0]. Especially,
γ = 0 makes LHybrid become LMS."
REFERENCES,0.7921348314606742,"We follow previous works to use SGD optimizer in all experiments. However, we ﬁnd compared to CE
loss, a wider range of learning rate η should be searched in for DML losses. Previous works are based
on CE loss and always set η = 0.05 or η = 0.1 (Aljundi et al., 2019c;a; Jin et al., 2020; Mai et al.,
2021). We ﬁnd the optimal η for LHybrid is often larger than 0.1. However, when a larger η (e.g. 0.2
on MNIST and CIFAR10) is used, baselines based on CE loss will degrade obviously because of the
unstable results over multiple runs. Thus, for baselines we select η from [0.05, 0.1, 0.15, 0.2]. For our"
REFERENCES,0.7949438202247191,Under review as a conference paper at ICLR 2022
REFERENCES,0.797752808988764,"0
100
200
300
400
500
Iteration 0 2 4 6 8 10 12 14 Loss"
REFERENCES,0.800561797752809,Split CIFAR10
REFERENCES,0.8033707865168539,"0
100
200
300
400
500
Iteration 0 2 4 6 8 10 12 14 Loss"
REFERENCES,0.8061797752808989,Smooth CIFAR10
REFERENCES,0.8089887640449438,"Figure 5: The curves of training loss on Split CIFAR10 (left) and Smooth CIFAR10 (right). Split CIFAR10
comprises of 5 tasks, 100 iterations per task."
REFERENCES,0.8117977528089888,"method, we select η from [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]. The hyperparameters
of our method are displayed in Table 8."
REFERENCES,0.8146067415730337,"Split Datasets
Smooth Datasets
MNIST
CIFAR10
CIFAR100
miniImageNet
CIFAR10
CIFAR100
miniImageNet
γ
0.1
0.1
0.5
1
0.1
1.0
1.0
η
0.05
0.2
0.35
0.2
0.25
0.5
0.2"
REFERENCES,0.8174157303370787,Table 8: Hyperparamters of PNCA loss weight γ and learning rate η for our method on different datasets.
REFERENCES,0.8202247191011236,"B.4
EVALUATION METRICS"
REFERENCES,0.8230337078651685,"Following previous online CL works (Aljundi et al., 2019a; Mai et al., 2021), we use Average
Accuracy and Average Forgetting (Chaudhry et al., 2019b) on Split datasets after training all T tasks.
Average Accuracy evaluates the overall performance and Average Forgetting measures how much
learned knowledge has been forgetten. Let ak,j denote the accuracy on the held-out set of the j-th
task after training the model on the ﬁrst k tasks. For a dataset comprised of T tasks, the Average
Accuracy is deﬁned as follows:"
REFERENCES,0.8258426966292135,"AT = 1 T T
X"
REFERENCES,0.8286516853932584,"j=1
aT,j
(18)"
REFERENCES,0.8314606741573034,and Average Forgetting can be written as:
REFERENCES,0.8342696629213483,"f T
j =
max
l∈{1,...,T −1} al,j −aT,j, ∀j < T
(19)"
REFERENCES,0.8370786516853933,"FT =
1
T −1"
REFERENCES,0.8398876404494382,"T −1
X"
REFERENCES,0.8426966292134831,"j=1
f T
j
(20)"
REFERENCES,0.8455056179775281,"We report Average Accuracy and Average Forgetting in the form of percentage. It should be noted
that a higher Average Accuracy is better while a lower Average Forgetting is better."
REFERENCES,0.848314606741573,"On Smooth datasets, as there is no notion of task, we evaluate the performance with the accuracy on
the whole held-out set after ﬁnishing training the whole training set."
REFERENCES,0.851123595505618,"B.5
CODE DEPENDENCIES AND HARDWARE"
REFERENCES,0.8539325842696629,"The Python version is 3.7.6. We use the PyTorch deep learning library to implement our method
and baselines. The version of PyTorch is 1.7.0. Other dependent libraries include Numpy (1.17.3),
torchvision (0.4.1), matplotlib (3.1.2) and scikit-learn (0.22). The CUDA version is 10.2. We run all
experiments on 1 NVIDIA RTX 2080ti GPU. We will publish our codes once the paper is accepted."
REFERENCES,0.8567415730337079,Under review as a conference paper at ICLR 2022
REFERENCES,0.8595505617977528,"C
MORE EXPERIMENTAL RESULTS"
REFERENCES,0.8623595505617978,"C.1
EFFECT OF PROXY-NCA LOSS WEIGHT γ"
REFERENCES,0.8651685393258427,"In Table 6, we show the effect of Proxy-NCA loss weight γ on split datasets. We can ﬁnd on MNIST
and CIFAR10 where the number of classes is relatively small, although the best results are obtained
with γ = 0.1, we can have competitive results with γ = 0. In other words, only using MS loss is
effective enough. However, on CIFAR100 and miniImageNet, as discussed in main text, the expected
number of classes in each minibatch is larger which reduces the probability of occurrence of positive
pair so that limits the learning ability of pair-based MS loss. At this time, it is necessary to introduce
auxiliary PNCA loss. When γ is larger than 1, the performance begins to degrade, which implies
that excessively focusing on discriminative PNCA loss will affect the performance of the generative
NCM classiﬁer. 0.0 0.1 0.25 0.5 0.75 1.0 1.25 1.5 2.0 87.4 87.6 87.8 88.0 88.2"
REFERENCES,0.8679775280898876,Valid accuracy
REFERENCES,0.8707865168539326,Split MNIST 0.0 0.1 0.25 0.5 0.75 1.0 1.25 1.5 2.0 45.0 46.0 47.0 48.0 49.0 50.0 51.0 52.0
REFERENCES,0.8735955056179775,Valid accuracy
REFERENCES,0.8764044943820225,Split CIFAR10 0.0 0.1 0.25 0.5 0.75 1.0 1.25 1.5 2.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0
REFERENCES,0.8792134831460674,Valid accuracy
REFERENCES,0.8820224719101124,Split CIFAR100 (10-task) 0.0 0.1 0.25 0.5 0.75 1.0 1.25 1.5 2.0 10.0 10.5 11.0 11.5 12.0 12.5 13.0
REFERENCES,0.8848314606741573,Valid accuracy
REFERENCES,0.8876404494382022,Split CIFAR100 (20-task) 0.0 0.1 0.25 0.5 0.75 1.0 1.25 1.5 2.0 8.0 10.0 12.0 14.0 16.0
REFERENCES,0.8904494382022472,Valid accuracy
REFERENCES,0.8932584269662921,Split miniImageNet (10-task) 0.0 0.1 0.25 0.5 0.75 1.0 1.25 1.5 2.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0
REFERENCES,0.8960674157303371,Valid accuracy
REFERENCES,0.898876404494382,Split miniImageNet (20-task)
REFERENCES,0.901685393258427,"Figure 6: Average Accuracy on valid set of our method with different Proxy-NCA loss weight γ in the 6 Split
datasets as reported in Table 1 of main text. We report the mean of 15 runs."
REFERENCES,0.9044943820224719,"100
200
500
1000
Memory 70.0 72.5 75.0 77.5 80.0 82.5 85.0 87.5"
REFERENCES,0.9073033707865169,Valid accuracy
REFERENCES,0.9101123595505618,Split MNIST
REFERENCES,0.9129213483146067,"100
200
500
1000
Memory 20.0 25.0 30.0 35.0 40.0 45.0 50.0"
REFERENCES,0.9157303370786517,Valid accuracy
REFERENCES,0.9185393258426966,Split CIFAR10
REFERENCES,0.9213483146067416,"ER-reservoir
Ours"
REFERENCES,0.9241573033707865,"500
1000
2000
5000
Memory 10.0 12.0 14.0 16.0 18.0 20.0"
REFERENCES,0.9269662921348315,Valid accuracy
REFERENCES,0.9297752808988764,Split CIFAR100 (10-task)
REFERENCES,0.9325842696629213,"500
1000
2000
5000
Memory 6.0 8.0 10.0 12.0 14.0 16.0"
REFERENCES,0.9353932584269663,Valid accuracy
REFERENCES,0.9382022471910112,Split CIFAR100 (20-task)
REFERENCES,0.9410112359550562,"500
1000
2000
5000
Memory 8.0 10.0 12.0 14.0 16.0 18.0 20.0"
REFERENCES,0.9438202247191011,Valid accuracy
REFERENCES,0.9466292134831461,Split miniImageNet (10-task)
REFERENCES,0.949438202247191,"500
1000
2000
5000
Memory 6.0 8.0 10.0 12.0 14.0 16.0"
REFERENCES,0.952247191011236,Valid accuracy
REFERENCES,0.9550561797752809,Split miniImageNet (20-task)
REFERENCES,0.9578651685393258,Figure 7: Average Accuracy on valid set of ER-reservoir and our method with different size of M.
REFERENCES,0.9606741573033708,Under review as a conference paper at ICLR 2022
REFERENCES,0.9634831460674157,"C.2
RESULTS WITH DIFFERENT MEMORY SIZES"
REFERENCES,0.9662921348314607,"In Table 9 and Table 10, we report the performance of our method and all baselines compared in
main text on Split CIFAR100 and miniImageNet with 2k memory size. Our method performs best
in all settings and the improvements are obvious, which is similar with the results in 1k memory
size settings reported in Table 1-2 of main text. We also report Average Accuracy of ER-reservoir
and our method on valid set with a various of memory sizes. Our method outperforms ER-reservoir
consistently which shows broad applicability of our method. The improvements are relatively small
when memory size is 500 on CIFAR100 and miniImageNet, which is due to the fact that when size of
replay memory M is too small (5 samples per class on average), class mean µM
c
cannot approximate
the real class mean µD
c well and thus proposed NCM classiﬁer degrades."
REFERENCES,0.9691011235955056,"C.3
COMPARISON WITH TRIPLET LOSS"
REFERENCES,0.9719101123595506,"Yu et al. (2020) proposes SDC method for conventional CIL based on NCM classiﬁer and another
pair-based DML loss, triplet loss (Hoffer & Ailon, 2015). As discussed in Related Work of main
text, SDC is not applicable for online CIL. To further clarify our contribution given this work,
we evaluate the performance of NCM classiﬁer after training the model with the hybrid objective
LT riplet + γLP NCA, where LT riplet represents triplet loss. Please see Hoffer & Ailon (2015) and
(Yu et al., 2020) for details about LT riplet."
REFERENCES,0.9747191011235955,"We report the results of the hybrid objective involving triplet loss and compare it with our hybrid loss
(LMS + γLP NCA) in Table 11. We can ﬁnd our proposed loss is superior to LT riplet + γLP NCA
consistently. In addition to replace softmax classiﬁer with NCM classiﬁer for online CIL problem,
our contributions mainly reﬂect in introducing a hybrid of MS loss and PNCA loss in the view of
training the generative NCM classiﬁer. We believe the above results show MS loss is critical for
online CIL, and introducing MS loss is not a trivial contribution, even if given the SDC work (Yu
et al., 2020)."
REFERENCES,0.9775280898876404,"Methods
CIFAR100
(10-task)
CIFAR100
(20-task)
miniImageNet
(10-task)
miniImageNet
(20-task)
ﬁne-tune
6.26±0.30
3.61±0.24
4.43±0.19
3.12±0.15
ER-reservoir
15.15±0.37
12.76±0.69
13.88±0.68
11.76±0.88
A-GEM (Chaudhry et al., 2019a)
6.50±0.16
3.61±0.08
4.41±0.14
3.23±0.13
GSS-Greedy (Aljundi et al., 2019c)
12.66±0.67
11.62±0.61
13.95±0.40
11.70±0.55
MIR (Aljundi et al., 2019a)
15.11±0.63
12.45±0.54
14.22±0.93
12.35±1.08
GMED-ER (Jin et al., 2020)
14.93±0.39
11.97±0.60
12.10±1.29
9.90±0.95
GMED-MIR (Jin et al., 2020)
15.11±0.50
12.01±0.81
13.90±0.58
12.25±0.59
ASERµ (Mai et al., 2021)∗
17.20±0.50
–
14.80±1.10
–
Ours
18.96±0.42
16.69±0.76
19.17±0.38
17.10±0.58
i.i.d. online
20.62±0.48
20.62±0.48
18.02±0.63
18.02±0.63
i.i.d. ofﬂine
45.59±0.29
45.59±0.29
38.63±0.59
38.63±0.59"
REFERENCES,0.9803370786516854,"Table 9: Average Accuracy of 15 runs on Split datasets. Higher is better. ∗indicates the results are from the
original paper. The size of memory M is 2k."
REFERENCES,0.9831460674157303,Under review as a conference paper at ICLR 2022
REFERENCES,0.9859550561797753,"Methods
CIFAR100
(10-task)
CIFAR100
(20-task)
miniImageNet
(10-task)
miniImageNet
(20-task)
ﬁne-tune
51.60±0.77
65.51±0.78
41.12±0.82
52.99±0.89
ER-reservoir
40.19±0.71
52.45±0.85
32.91±0.79
44.93±0.81
A-GEM (Chaudhry et al., 2019a)
51.45±0.68
67.13±0.55
40.49±0.43
51.45±0.68
GSS-Greedy (Aljundi et al., 2019c)
40.02±0.81
45.90±2.09
30.50±0.83
39.09±2.34
MIR (Aljundi et al., 2019a)
39.82±0.79
48.54±0.60
29.27±1.32
39.74±1.50
GMED-ER (Jin et al., 2020)
42.99±0.75
55.45±0.68
31.81±1.73
43.68±1.23
GMED-MIR (Jin et al., 2020)
43.78±0.93
55.15±0.71
29.67±0.58
41.16±0.95
ASERµ (Mai et al., 2021)∗
38.60±0.60
–
22.20±1.60
–
Ours
26.59±0.58
29.53±0.94
17.82±0.36
24.10±0.71"
REFERENCES,0.9887640449438202,"Table 10: Average Forgetting of 15 runs on Split datasets. Lower is better. ∗indicates the results are from the
original paper. The size of memory M is 2k."
REFERENCES,0.9915730337078652,"Loss
MNIST
(5-task)
CIFAR10
(5-task)
CIFAR100
(10-task)
CIFAR100
(20-task)
miniImageNet
(10-task)
miniImageNet
(20-task)"
REFERENCES,0.9943820224719101,"LT riplet + γLP NCA
87.06±0.52
43.56±1.63
13.21±0.28
10.93±0.34
14.31±0.33
13.55±0.22
LMS + γLP NCA (Ours)
88.79±0.26
51.84±0.91
15.56±0.39
13.65±0.35
16.05±0.38
15.15±0.36"
REFERENCES,0.9971910112359551,"Table 11: Comparison between MS loss and Triplet Loss. We report Average Accuracy of 15 runs on Split
datasets. The size of memory M is 1k."
