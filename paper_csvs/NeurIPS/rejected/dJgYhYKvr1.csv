Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001996007984031936,"The grokking phenomenon as reported by Power et al. [13] refers to a regime where
1"
ABSTRACT,0.003992015968063872,"a long period of overfitting is followed by a seemingly sudden transition to perfect
2"
ABSTRACT,0.005988023952095809,"generalization. In this paper, we attempt to reveal the underpinnings of Grokking
3"
ABSTRACT,0.007984031936127744,"via a series of empirical studies. Specifically, we uncover an optimization anomaly
4"
ABSTRACT,0.00998003992015968,"plaguing adaptive optimizers at extremely late stages of training, referred to as
5"
ABSTRACT,0.011976047904191617,"the Slingshot Mechanism. A prominent artifact of the Slingshot Mechanism can
6"
ABSTRACT,0.013972055888223553,"be measured by the cyclic phase transitions between stable and unstable training
7"
ABSTRACT,0.015968063872255488,"regimes, and can be easily monitored by the cyclic behavior of the norm of the
8"
ABSTRACT,0.017964071856287425,"last layers weights. We empirically observe that without explicit regularization,
9"
ABSTRACT,0.01996007984031936,"Grokking as reported in [13] almost exclusively happens at the onset of Slingshots,
10"
ABSTRACT,0.021956087824351298,"and is absent without it. While common and easily reproduced in more general
11"
ABSTRACT,0.023952095808383235,"settings, the Slingshot Mechanism does not follow from any known optimization
12"
ABSTRACT,0.02594810379241517,"theories that we are aware of, and can be easily overlooked without an in depth
13"
ABSTRACT,0.027944111776447105,"examination. Our work points to a surprising and useful inductive bias of adaptive
14"
ABSTRACT,0.029940119760479042,"gradient optimizers at late stages of training, calling for a revised theoretical
15"
ABSTRACT,0.031936127744510975,"analysis of their origin.
16"
ABSTRACT,0.033932135728542916,"Norm
growth"
ABSTRACT,0.03592814371257485,"Loss
spike"
ABSTRACT,0.03792415169660679,"Norm
plateau"
ABSTRACT,0.03992015968063872,Slingshot
ABSTRACT,0.041916167664670656,"Figure 1: Slingshot Effects are observed with a fully-connected ReLU network (FCN). The FCN is trained with
200 randomly chosen CIFAR-10 samples with Adam. Multiple Slingshot Effects occur in a cyclic fashion as
indicated by the dotted red boxes. Each Slingshot Effect is characterized by a period of rapid growth of the last
layer weights, an ensuing training loss spike, and a norm plateau."
INTRODUCTION,0.043912175648702596,"1
Introduction
17"
INTRODUCTION,0.04590818363273453,"Recently, the grokking phenomenon was proposed by [13], in the context of studying the optimization
18"
INTRODUCTION,0.04790419161676647,"and generalization aspects in small, algorithmically generated datasets. Specifically, grokking refers
19"
INTRODUCTION,0.0499001996007984,"to a sudden transition from chance level validation accuracy to perfect generalization, long past the
20"
INTRODUCTION,0.05189620758483034,"point of perfect training accuracy, i.e., Terminal Phase of Training (TPT). This curious behavior
21"
INTRODUCTION,0.05389221556886228,"contradicts the common belief of early stopping in the overfitting regimes, and calls for further
22"
INTRODUCTION,0.05588822355289421,"understandings of the generalization behavior of deep neural networks.
23"
INTRODUCTION,0.05788423153692615,"In the literature, it has been suggested that in some scenarios, marginal improvements in validation
24"
INTRODUCTION,0.059880239520958084,"accuracy appears in TPT, which seem to directly support grokking. For example, it has been shown
25"
INTRODUCTION,0.06187624750499002,"in [14] that gradient descent on logistic regression problems converges to the maximum margin
26"
INTRODUCTION,0.06387225548902195,"solution, a result that has been since extended to cover a wider setting [11, 17]. A key finding in [14]
27"
INTRODUCTION,0.0658682634730539,"shows that when training on linearly separable data with gradient descent using logistic regression,
28"
INTRODUCTION,0.06786427145708583,"the classifier’s margin slowly improves at a rate of O(
1
log t), while the weight norm of the predictor
29"
INTRODUCTION,0.06986027944111776,"layer grows at a rate of O(t), where t is the number of training steps. While specified for gradient
30"
INTRODUCTION,0.0718562874251497,"descent, Wang et al. [17] showed that similar results also hold for adaptive optimizers (such as Adam
31"
INTRODUCTION,0.07385229540918163,"and RMSProp). Taking these results into consideration, one could reasonably hypothesise that deep
32"
INTRODUCTION,0.07584830339321358,"nonlinear networks could benefit from longer training time, even after achieving zero errors on the
33"
INTRODUCTION,0.07784431137724551,"training set.
34 35"
INTRODUCTION,0.07984031936127745,"In this paper, we provide in depth empirical analyses to the mechanism behind grokking. We find that
36"
INTRODUCTION,0.08183632734530938,"the phenomenology of grokking differs from those predicted by [14] in several key aspects. To be
37"
INTRODUCTION,0.08383233532934131,"concrete, we find that grokking occurs during the onset of another intriguing phenomenon directly
38"
INTRODUCTION,0.08582834331337326,"related to adaptive gradient methods (see Algorithm 1 for a generic description of adaptive gradient
39"
INTRODUCTION,0.08782435129740519,"methods). In particular, leveraging the basic setup in [13], we make the following observations:
40"
INTRODUCTION,0.08982035928143713,"1. During the TPT, training exhibits a cyclic behaviour between stable and unstable regimes. A
41"
INTRODUCTION,0.09181636726546906,"prominent artifact of this behaviour can be seen in the norm of a model’s last layer weights, which
42"
INTRODUCTION,0.09381237524950099,"exhibits a cyclical behavior with distinct, sharp phase transitions that alternate between rapid growth
43"
INTRODUCTION,0.09580838323353294,"and plateaus over the course of training.
44"
THE NORM GROWS RAPIDLY SOMETIME AFTER THE MODEL HAS PERFECT CLASSIFICATION ACCURACY ON TRAINING,0.09780439121756487,"2. The norm grows rapidly sometime after the model has perfect classification accuracy on training
45"
THE NORM GROWS RAPIDLY SOMETIME AFTER THE MODEL HAS PERFECT CLASSIFICATION ACCURACY ON TRAINING,0.0998003992015968,"data. A sharp phase transition then occurs when the model missclassifies training samples. This
46"
THE NORM GROWS RAPIDLY SOMETIME AFTER THE MODEL HAS PERFECT CLASSIFICATION ACCURACY ON TRAINING,0.10179640718562874,"phase change is accompanied by a sudden spike in training loss, and a plateau in the norm growth of
47"
THE NORM GROWS RAPIDLY SOMETIME AFTER THE MODEL HAS PERFECT CLASSIFICATION ACCURACY ON TRAINING,0.10379241516966067,"the final classification layer.
48"
THE NORM GROWS RAPIDLY SOMETIME AFTER THE MODEL HAS PERFECT CLASSIFICATION ACCURACY ON TRAINING,0.10578842315369262,"3. The features (pre-classification layer) show rapid evolution as the weight norm transitions from
49"
THE NORM GROWS RAPIDLY SOMETIME AFTER THE MODEL HAS PERFECT CLASSIFICATION ACCURACY ON TRAINING,0.10778443113772455,"rapid growth to a growth plateau, and change relatively little at the norm growth phase.
50"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.10978043912175649,"4. Phase transitions between norm growth and norm plateau phases are typically accompanied by a
51"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.11177644710578842,"sudden bump in generalization as measured by classification accuracy on a validation set, as observed
52"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.11377245508982035,"in a dramatic fashion in [13].
53"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.1157684630738523,"5. It is empirically observed that grokking as reported in [13] almost exclusively happens at the onset
54"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.11776447105788423,"of Slingshots, and is absent without it.
55"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.11976047904191617,"We denote the observations above as the Slingshot Effect, which is defined to be the full cycle starting
56"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.1217564870259481,"from the norm growth phase, and ending in the norm plateau phase. And empirically, a single training
57"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.12375249500998003,"run typically exhibits multiple Slingshot Effects. Moreover, while grokking as described in [13]
58"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.12574850299401197,"might be data dependent, we find that the Slingshot Mechanism is pervasive, and can be easily
59"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.1277445109780439,"reproduced in multiple scenarios, encompassing a variety of models (Transformers and MLPs) and
60"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.12974051896207583,"datasets (both vision, algorithmic and synthetic datasets). Since we only observe Slingshot Effects
61"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.1317365269461078,"when training classification models with adaptive optimizers, our work can be seen as empirically
62"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.13373253493013973,"characterizing an implicit bias of such optimizers. Finally, while our observations and conclusions
63"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.13572854291417166,"hold for most variants of adaptive gradient methods, we focus on Adam in the main paper, and
64"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.1377245508982036,"relegate all experiments with additional optimizers to the appendix.
65"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.13972055888223553,Algorithm 1 Generic Adaptive Gradient Method
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.14171656686626746,"Input: X1 ∈F, step size µ, sequence of functions {ϕt, ψt}T
t=1, ϵ ∈R+"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.1437125748502994,Output: Fitted α.
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.14570858283433133,"1 for t = 1..., T do"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.14770459081836326,"2
gt = ∇ft(xt)."
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.1497005988023952,"3
mt = ϕt(g1, ..., gt) and Vt = ψt(g1, ..., gt)."
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.15169660678642716,"4
xt+1 = xt −
µmt
√"
PHASE TRANSITIONS BETWEEN NORM GROWTH AND NORM PLATEAU PHASES ARE TYPICALLY ACCOMPANIED BY A,0.1536926147704591,"V 2
t +ϵ"
IMPLICATIONS OF OUR FINDINGS,0.15568862275449102,"1.1
Implications of Our Findings
66"
IMPLICATIONS OF OUR FINDINGS,0.15768463073852296,"The findings in this paper have both theoretical and practical implications that go beyond characteriz-
67"
IMPLICATIONS OF OUR FINDINGS,0.1596806387225549,"ing Grokking. A prominent feature of the Slingshot Mechanism is the repeating phase shifts between
68"
IMPLICATIONS OF OUR FINDINGS,0.16167664670658682,"stable and unstable training regimes, where the unstable phase is characterized by extremely large
69"
IMPLICATIONS OF OUR FINDINGS,0.16367265469061876,"gradients, and spiking training loss. Furthermore, we find that learning at late stages of training have
70"
IMPLICATIONS OF OUR FINDINGS,0.1656686626746507,"a cyclic property, where non trivial feature adaptation only takes place at the onset of a phase shift.
71"
IMPLICATIONS OF OUR FINDINGS,0.16766467065868262,"From a theoretical perspective, this is contradictory to common assumptions made in the literature of
72"
IMPLICATIONS OF OUR FINDINGS,0.16966067864271456,"convergence of adaptive optimizers, which typically require L smooth cost functions, and bounded
73"
IMPLICATIONS OF OUR FINDINGS,0.17165668662674652,"stochastic gradients, either in the L2 or L∞norm, decreasing step sizes and stable convergence
74"
IMPLICATIONS OF OUR FINDINGS,0.17365269461077845,"[18, 1, 2]. From the apparent generalization benefits of Slingshot Effects, we cast doubt on the ability
75"
IMPLICATIONS OF OUR FINDINGS,0.17564870259481039,"of current working theories to explain the Slingshot Mechanism.
76"
IMPLICATIONS OF OUR FINDINGS,0.17764471057884232,"Practically, our work presents additional evidence for the growing body of work indicating the
77"
IMPLICATIONS OF OUR FINDINGS,0.17964071856287425,"importance of the TPT stage of training for optimal performance [6, 13, 12].
78"
IMPLICATIONS OF OUR FINDINGS,0.18163672654690619,"In an era where the sheer size of models are quickly becoming out of reach for most practitioners,
79"
IMPLICATIONS OF OUR FINDINGS,0.18363273453093812,"our work suggest focusing on improved methods to prevent excessive norm growth either implicitly
80"
IMPLICATIONS OF OUR FINDINGS,0.18562874251497005,"through Slingshot Effects or through other forms of explicit regularization or normalization.
81"
RELATED WORK,0.18762475049900199,"2
Related Work
82"
RELATED WORK,0.18962075848303392,"The Slingshot Mechanism we uncover here is reminiscent of the catapult mechanism described in
83"
RELATED WORK,0.19161676646706588,"Lewkowycz et al. [9]. Lewkowycz et al. [9] show that loss of a model trained via gradient descent with
84"
RELATED WORK,0.1936127744510978,"an appropriately large learning rate shows a non-monotonic behavior —the loss initially increases
85"
RELATED WORK,0.19560878243512975,"and starts decreasing once the model ""catapults"" to a region of lower curvature —early in training.
86"
RELATED WORK,0.19760479041916168,"However, the catapult phenomenon differs from Slingshot Effects in several key aspects. The catapult
87"
RELATED WORK,0.1996007984031936,"mechanism is observed with vanilla or stochastic gradient descent unlike the Slingshot Mechanism
88"
RELATED WORK,0.20159680638722555,"that is seen with adaptive optimizers including Adam [7] and RMSProp [15]. Furthermore, the
89"
RELATED WORK,0.20359281437125748,"catapult phenomenon relates to a large initial learning rate, and does not exhibit a repeating cyclic
90"
RELATED WORK,0.2055888223552894,"behavior. More intriguingly, Slingshot Effects only emerge late in training, typically long after the
91"
RELATED WORK,0.20758483033932135,"model reaches perfect accuracy on the training data.
92"
RELATED WORK,0.20958083832335328,"Cohen et al. [3] describe a ""progressive sharpening"" phenomenon in which the maximum eigenvalue
93"
RELATED WORK,0.21157684630738524,"of the loss Hessian increases and reaches a value that is at equal to or slightly larger than 2/η where
94"
RELATED WORK,0.21357285429141717,"η is the learning rate. This ""progressive sharpening"" phenomenon leads to model to enter a regime
95"
RELATED WORK,0.2155688622754491,"Cohen et al. [3] call Edge of Stability where-in the model shows non-monotonic training loss behavior
96"
RELATED WORK,0.21756487025948104,"over short time spans. Edge of Stability is similar to the Slingshot Mechanism in that it is shown to
97"
RELATED WORK,0.21956087824351297,"occur later on in training. However, Edge of Stability is shown for full-batch gradient descent while
98"
RELATED WORK,0.2215568862275449,"we observe Slingshot Mechanism with adaptive optimizers, primarily Adam [7] or AdamW [10].
99"
RELATED WORK,0.22355289421157684,"As noted above, the Slingshot Mechanism emerges late in training, typically longer after the model
100"
RELATED WORK,0.22554890219560877,"reaches perfect accuracy and has low loss on training data. The benefits of continuing to training
101"
RELATED WORK,0.2275449101796407,"a model in this regime has been theoretically studied in several works including [14, 11]. Soudry
102"
RELATED WORK,0.22954091816367264,"et al. [14] show that training a linear model on separable data with gradient using the logistic
103"
RELATED WORK,0.2315369261477046,"loss function leads to a max-margin solution. Furthermore Soudry et al. [14] prove that the loss
104"
RELATED WORK,0.23353293413173654,decreases at a rate of O( 1
RELATED WORK,0.23552894211576847,"t ) while the margin increases much slower O(
1
log t), where t is the number
105"
RELATED WORK,0.2375249500998004,"of training steps. Soudry et al. [14] also note that the weight norm of the predictor layer increases
106"
RELATED WORK,0.23952095808383234,"at a logarithmic rate, i.e., O(log(t)). Lyu and Li [11] generalize the above results to homogeneous
107"
RELATED WORK,0.24151696606786427,"neural networks trained with exponential-type loss function and show that loss decreases at a rate of
108"
RELATED WORK,0.2435129740518962,"O(1/t(log(t))2−2/L). This is, where L is defined as the order of the homogenous neural network.
109"
RELATED WORK,0.24550898203592814,"Although these results indeed prove the benefits of training models, their analyses are limited
110"
RELATED WORK,0.24750499001996007,"to gradient descent. Moreover, the analyses developed by Soudry et al [14] do not predict any
111"
RELATED WORK,0.249500998003992,"phenomenon that resembles the Slingshot Mechanism. Wang et al. [17] show that homogenous neural
112"
RELATED WORK,0.25149700598802394,"networks trained with RMSProp [15] or Adam without momentum [17] do converge in direction to
113"
RELATED WORK,0.25349301397205587,"the max-margin solution. However, none of these papers can explain the Slingshot Mechanism and
114"
RELATED WORK,0.2554890219560878,"specifically the cyclical behavior of the norm of the last layer weights.
115"
RELATED WORK,0.25748502994011974,"101
102
103
104
105
106 step 10
10 10
8 10
6 10
4 10
2 100 loss"
RELATED WORK,0.25948103792415167,"train loss
last layer norm 0.0 0.5 1.0 1.5 2.0"
RELATED WORK,0.26147704590818366,last layer norm
RELATED WORK,0.2634730538922156,"101
102
103
104
105
106 step 10
2 10
1 100 101 loss"
RELATED WORK,0.2654690618762475,"validation loss
last layer norm 0.0 0.5 1.0 1.5 2.0"
RELATED WORK,0.26746506986027946,last layer norm
RELATED WORK,0.2694610778443114,"104
105
106 step 0.00 0.02 0.04 0.06 0.08"
RELATED WORK,0.2714570858283433,feature change
RELATED WORK,0.27345309381237526,"layer1 feature
last layer norm 0.5 1.0 1.5 2.0"
RELATED WORK,0.2754491017964072,last layer norm
RELATED WORK,0.2774451097804391,"(a)
(c)
(e)"
RELATED WORK,0.27944111776447106,"101
102
103
104
105
106 step 0 20 40 60 80 100"
RELATED WORK,0.281437125748503,accuracy
RELATED WORK,0.2834331337325349,"train accuracy
last layer norm 0.0 0.5 1.0 1.5 2.0"
RELATED WORK,0.28542914171656686,last layer norm
RELATED WORK,0.2874251497005988,"101
102
103
104
105
106 step 0 20 40 60 80 100"
RELATED WORK,0.2894211576846307,accuracy
RELATED WORK,0.29141716566866266,"validation accuracy
last layer norm 0.0 0.5 1.0 1.5 2.0"
RELATED WORK,0.2934131736526946,last layer norm
RELATED WORK,0.2954091816367265,"104
105
106 step 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035"
RELATED WORK,0.29740518962075846,feature change
RELATED WORK,0.2994011976047904,"layer2 feature
last layer norm 0.5 1.0 1.5 2.0"
RELATED WORK,0.3013972055888224,last layer norm
RELATED WORK,0.3033932135728543,"(b)
(d)
(f)"
RELATED WORK,0.30538922155688625,"Figure 2: Division dataset: Last layer weight norm growth versus a) loss on training data b) accuracy on training
data (c) loss on validation data d) accuracy on validation data e) normalized relative change in features of first
Transformer layer (f) normalized relative change in features of second Transformer layer. Note that the feature
change plots are shown starting at 10K step to emphasize the feature change behavior during norm growth and
plateau phases, revealing that the features stop changing during the norm growth phase and resume changing
during the plateaus."
THE SLINGSHOT MECHANISM,0.3073852295409182,"3
The Slingshot Mechanism
116"
EXPERIMENTAL SETUP,0.3093812375249501,"3.1
Experimental Setup
117"
EXPERIMENTAL SETUP,0.31137724550898205,"We use the training setup studied by Power et al. [13] in the main paper as a working example to
118"
EXPERIMENTAL SETUP,0.313373253493014,"illustrate the Slingshot Mechanism. In this setup, we train decoder-only Transformers [16] on a
119"
EXPERIMENTAL SETUP,0.3153692614770459,"modular division dataset [13] of the form a ÷ b = c, where a, b and c are discrete symbols and ÷
120"
EXPERIMENTAL SETUP,0.31736526946107785,"refers to division modulo p for some prime number p, split into training and validation sets. The
121"
EXPERIMENTAL SETUP,0.3193612774451098,"task consists of calculating c given a and b. The algorithmic operations and details of the datasets
122"
EXPERIMENTAL SETUP,0.3213572854291417,"considered in our experiments are described in Appendix B. The Transformer consists of 2 layers,
123"
EXPERIMENTAL SETUP,0.32335329341317365,"of width 128 and 4 attention heads with approximately 450K trainable parameters and is optimized
124"
EXPERIMENTAL SETUP,0.3253493013972056,"by Adam [7, 10]. For these experiments we set learning rate to 0.001, weight decay to 0, β1 = 0.9,
125"
EXPERIMENTAL SETUP,0.3273453093812375,"β2 = 0.98, ϵ = 10−8, linear learning rate warmup for the first 10 steps and minibatch size to 512
126"
EXPERIMENTAL SETUP,0.32934131736526945,"which are in line with the hyperparameters considered in [13].
127"
EXPERIMENTAL SETUP,0.3313373253493014,"Figure 2 shows the metrics of interest that we record on training and validation samples for modular
128"
EXPERIMENTAL SETUP,0.3333333333333333,"division dataset. Specifically, we measure 1) train loss; 2) train accuracy; 3) validation loss; 4)
129"
EXPERIMENTAL SETUP,0.33532934131736525,"validation accuracy; 5) last layer norm: denoting the norm of the classification layer’s weights and 6)
130"
EXPERIMENTAL SETUP,0.3373253493013972,"feature change: the relative change of features of the l-th layer (hl) after the t-th gradient update step
131"
EXPERIMENTAL SETUP,0.3393213572854291,"∥hl
t+1−hl
t∥
∥hl
t∥
. We observe from Figure 2b that the model is able to reach high training accuracy around
132"
EXPERIMENTAL SETUP,0.3413173652694611,"step 300 while validation accuracy starts improving after 105 steps as seen in Figure 2d. Power et
133"
EXPERIMENTAL SETUP,0.34331337325349304,"al. [13] originally showed this phenomenon and refer to it as grokking. We observe that while the
134"
EXPERIMENTAL SETUP,0.34530938123752497,"validation accuracy does not exhibit any change until much later in training, the validation loss shown
135"
EXPERIMENTAL SETUP,0.3473053892215569,"in Figure 2c exhibits a double descent behavior with an initial decrease, then a growth before rapidly
136"
EXPERIMENTAL SETUP,0.34930139720558884,"decreasing to zero.
137"
EXPERIMENTAL SETUP,0.35129740518962077,"Seemingly, some of these observations can be explained by the arguments in [14] and their extensions
138"
EXPERIMENTAL SETUP,0.3532934131736527,"to adaptive optimizers [17]. Namely, at the point of reaching perfect classification of the training set,
139"
EXPERIMENTAL SETUP,0.35528942115768464,"the cross-entropy (CE) loss by design pressures the classification layer to grow in norm at relatively
140"
EXPERIMENTAL SETUP,0.35728542914171657,"fast rate. Simultaneously, the implicit bias of the optimizer coupled with the CE loss, pushes the
141"
EXPERIMENTAL SETUP,0.3592814371257485,"direction of the classification layer to coincide with that of the maximum margin classifier, albeit at a
142"
EXPERIMENTAL SETUP,0.36127744510978044,"much slower rate.
143"
EXPERIMENTAL SETUP,0.36327345309381237,"These insights motivate us to measure the classifier’s last layer norm during training. We observe in
144"
EXPERIMENTAL SETUP,0.3652694610778443,"Figure 2a that once classification reaches perfect accuracy on the training set, the classification layer
145"
EXPERIMENTAL SETUP,0.36726546906187624,"norm exhibits a distinct cyclic behavior, alternating between rapid growth and plateau, with a sharp
146"
EXPERIMENTAL SETUP,0.36926147704590817,"phase transition between phases. Simultaneously, the training loss retains a low value in periods of
147"
EXPERIMENTAL SETUP,0.3712574850299401,"rapid norm growth, and then wildly fluctuating in periods of norm plateau. Figure 2e and Figure 2f
148"
EXPERIMENTAL SETUP,0.37325349301397204,"shows the evolution of the relative change in features output by each layer in the Transformer. We
149"
EXPERIMENTAL SETUP,0.37524950099800397,"observe that the feature maps are not updated much during the norm growth phase. However, at the
150"
EXPERIMENTAL SETUP,0.3772455089820359,"phase transition, we observe that the feature maps receive a rapid update, which suggests that the
151"
EXPERIMENTAL SETUP,0.37924151696606784,"internal representation of the model is updating.
152"
EXPERIMENTAL SETUP,0.3812375249500998,"Is Slingshot a general phenomenon?
In an attempt to ascertain the generality of Slingshot
153"
EXPERIMENTAL SETUP,0.38323353293413176,"Effects as an optimization artifact, we run similar experiments with additional architectures, datasets,
154"
EXPERIMENTAL SETUP,0.3852295409181637,"optimizers, and hyperparameters. We use all algorithmic datasets as proposed in [13], as well as
155"
EXPERIMENTAL SETUP,0.3872255489021956,"frequently used vision benchmarks such as CIFAR-10 [8], and even synthetic Gaussian dataset. For
156"
EXPERIMENTAL SETUP,0.38922155688622756,"architectures, we use Transformers, MLPs and deep linear models (see figure 1). We find abundant
157"
EXPERIMENTAL SETUP,0.3912175648702595,"evidence of Slingshot Effects in all of our experiments with Adam, AdamW and RMSProp. We
158"
EXPERIMENTAL SETUP,0.3932135728542914,"are unable to observe Slingshot Effects with Adagrad [5] and also with stochastic gradient descent
159"
EXPERIMENTAL SETUP,0.39520958083832336,"(SGD) or SGD with momentum, pointing to the generality of the mechanism across architectures and
160"
EXPERIMENTAL SETUP,0.3972055888223553,"datasets. We refer the reader to Appendix A for the full, detailed description of the experiments.
161"
EXPERIMENTAL SETUP,0.3992015968063872,"Why does Slingshot happen?
We hypothesize that the norm growth continues until the curvature
162"
EXPERIMENTAL SETUP,0.40119760479041916,"of the loss surface becomes large, effectively “flinging"" the weights to a different region in parameter
163"
EXPERIMENTAL SETUP,0.4031936127744511,"space as small gradient directions get amplified, reminiscent of the mechanics of a slingshot flinging a
164"
EXPERIMENTAL SETUP,0.405189620758483,"projectile. We attempt to quantify how far a model is flung by measuring the cosine distance between
165"
EXPERIMENTAL SETUP,0.40718562874251496,"a checkpoint during optimization and initial parameters. Specifically, we divide the model parameters
166"
EXPERIMENTAL SETUP,0.4091816367265469,"into representation (pre-classifier) parameters and classifier (last layer) parameters and calculate how
167"
EXPERIMENTAL SETUP,0.4111776447105788,"far these parameters have moved from initialization. We show that checkpoints collected after a
168"
EXPERIMENTAL SETUP,0.41317365269461076,"model experiences Slingshot have a larger representation cosine distance. We defer the reader to the
169"
EXPERIMENTAL SETUP,0.4151696606786427,"appendix for further details.
170"
EXPERIMENTAL SETUP,0.4171656686626746,"0
2000
4000
6000
8000
10000
epoch 0 10 20 30 40"
EXPERIMENTAL SETUP,0.41916167664670656,sharpness
EXPERIMENTAL SETUP,0.42115768463073855,"update sharpness
last layer norm 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
EXPERIMENTAL SETUP,0.4231536926147705,last layer norm
EXPERIMENTAL SETUP,0.4251497005988024,"0
2000
4000
6000
8000
10000
epoch 0 10 20 30 40"
EXPERIMENTAL SETUP,0.42714570858283435,sharpness
EXPERIMENTAL SETUP,0.4291417165668663,"update sharpness
last layer norm 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
EXPERIMENTAL SETUP,0.4311377245508982,last layer norm
EXPERIMENTAL SETUP,0.43313373253493015,"(a)
(c)"
EXPERIMENTAL SETUP,0.4351297405189621,"0
2000
4000
6000
8000
10000
epoch 0 10 20 30 40 50 60 70 80"
EXPERIMENTAL SETUP,0.437125748502994,sharpness
EXPERIMENTAL SETUP,0.43912175648702595,"update sharpness
last layer norm 0.0 0.5 1.0 1.5 2.0"
EXPERIMENTAL SETUP,0.4411177644710579,last layer norm
EXPERIMENTAL SETUP,0.4431137724550898,"0
2000
4000
6000
8000
10000
epoch 0 50 100 150 200 250"
EXPERIMENTAL SETUP,0.44510978043912175,sharpness
EXPERIMENTAL SETUP,0.4471057884231537,"update sharpness
last layer norm 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
EXPERIMENTAL SETUP,0.4491017964071856,last layer norm
EXPERIMENTAL SETUP,0.45109780439121755,"(b)
(d)"
EXPERIMENTAL SETUP,0.4530938123752495,"Figure 3: Curvature metric (denoted as ""update sharpness"") evolution vs norm growth on (a) addition, (b)
subtraction, (c) multiplication, and (d) division dataset. Note the spike in the sharpness metric near the phase
transitions between norm growth and plateau."
EXPERIMENTAL SETUP,0.4550898203592814,"By design, adaptive optimizers adapt the learning rate on a per parameter basis. In toy, convex
171"
EXPERIMENTAL SETUP,0.45708582834331335,"scenarios, the ϵ parameter provably determines whether the algorithm will converge stably. To
172"
EXPERIMENTAL SETUP,0.4590818363273453,"illustrate this, we take inspiration from [3], and consider a quadratic cost function L(A, B, C) =
173"
EXPERIMENTAL SETUP,0.46107784431137727,"1
2x⊤Ax + B⊤x + C, A ∈Rd×d, x, B ∈Rd, C ∈R, where we assume A is symmetric and positive
174"
EXPERIMENTAL SETUP,0.4630738522954092,"definite. Note that the global minimum of this cost is given by x⋆= −A−1B. The gradient of
175"
EXPERIMENTAL SETUP,0.46506986027944114,"this cost with respect to x is given by g = Ax + B. Consider optimizing the cost with adaptive
176"
EXPERIMENTAL SETUP,0.46706586826347307,"optimization steps of the simple form xt+1 = xt −µ
g
|g|+ϵ = xt −µ
Axt+B
|Axt+B|+ϵ where µ is a learning
177"
EXPERIMENTAL SETUP,0.469061876247505,"rate, and the division and absolute operations are taken element wise. Starting from some x0, the
178"
EXPERIMENTAL SETUP,0.47105788423153694,"error et = xt −x⋆evolves according to:
179"
EXPERIMENTAL SETUP,0.47305389221556887,"et+1 =
 
I −µdiag(
1
|Aet| + ϵ)A

et
def
= Mtet
(1)"
EXPERIMENTAL SETUP,0.4750499001996008,Note that the condition ∥A∥s < 2ϵ
EXPERIMENTAL SETUP,0.47704590818363274,"µ where ∥· ∥s denotes the spectral norm, implies that the mapping
180"
EXPERIMENTAL SETUP,0.47904191616766467,"Mt is a contraction for all values of t, and hence convergence to the global optimum is guaranteed
181"
EXPERIMENTAL SETUP,0.4810379241516966,"(This is in contrast to gradient descent, where the requirement is ∥A∥s < 2"
EXPERIMENTAL SETUP,0.48303393213572854,"µ). Note that the choice
182"
EXPERIMENTAL SETUP,0.48502994011976047,"of ϵ crucially controls the requirement on the curvature of the cost, represented by the the spectrum
183"
EXPERIMENTAL SETUP,0.4870259481037924,"of A in this case. In other words, the smaller ϵ, the more restrictive the requirements on the top
184"
EXPERIMENTAL SETUP,0.48902195608782434,"eigenvalue of A. In [3], it was observed that full batch gradient descent increases the spectral norm
185"
EXPERIMENTAL SETUP,0.49101796407185627,"of the Hessian to its maximum allowed value. We therefore hypothesize that for deep networks, a
186"
EXPERIMENTAL SETUP,0.4930139720558882,"small value for ϵ requires convergence to a low curvature local minimum, causing a Slingshot Effect
187"
EXPERIMENTAL SETUP,0.49500998003992014,"when this does not occur. Moreover, we may reasonably predict that increasing the value of ϵ would
188"
EXPERIMENTAL SETUP,0.49700598802395207,"lift the restriction on the curvature, and with it evidence of Slingshot Effects.
189"
EXPERIMENTAL SETUP,0.499001996007984,"Figure 3 shows evidence consistent with the hypothesis that Slingshot Effects occur in the vicinity of
190"
EXPERIMENTAL SETUP,0.500998003992016,"high loss curvature, by measuring the local loss surface curvature along the optimization trajectory.
191"
EXPERIMENTAL SETUP,0.5029940119760479,"Let Ht denote the local Hessian matrix of the loss, and ut the parameter update at time t given the
192"
EXPERIMENTAL SETUP,0.5049900199600799,"optimization algorithm of choice. We use the local curvature along the trajectory of the optimizer,
193"
EXPERIMENTAL SETUP,0.5069860279441117,"given by
1
∥ut∥2 u⊤
t Htut, as a curvature measure. Across the arithmetic datasets from [13], whenever
194"
EXPERIMENTAL SETUP,0.5089820359281437,"the last layer weight norm plateaus, the curvature measure momentarily peaks and settles back down.
195"
EXPERIMENTAL SETUP,0.5109780439121756,"Varying ϵ
We next observe from Figure 2a that the training loss value also spikes up around the
196"
EXPERIMENTAL SETUP,0.5129740518962076,"time step when the weight norm transitions from growth to plateau. A low training loss value suggests
197"
EXPERIMENTAL SETUP,0.5149700598802395,"that the gradients (and their moments) used as inputs to the optimizer are small, which in turn can
198"
EXPERIMENTAL SETUP,0.5169660678642715,"cause the ϵ hyperparameter value to play a role in calculating updates. Our hypothesis here is that the
199"
EXPERIMENTAL SETUP,0.5189620758483033,"Slingshot Effect should eventually disappear with a sufficiently large ϵ. To confirm this hypothesis,
200"
EXPERIMENTAL SETUP,0.5209580838323353,"we run an experiment where we vary ϵ while retaining the rest of the setup described in the previous
201"
EXPERIMENTAL SETUP,0.5229540918163673,"section.
202"
EXPERIMENTAL SETUP,0.5249500998003992,"Figure 4 shows the results for various values of ϵ considered in this experiment. We first observe that
203"
EXPERIMENTAL SETUP,0.5269461077844312,"the number of Slingshot Effect cycles is higher for smaller values of ϵ. Secondly, smaller values of ϵ
204"
EXPERIMENTAL SETUP,0.5289421157684631,"cause grokking to appear at an earlier time step when compared to larger values. More intriguingly,
205"
EXPERIMENTAL SETUP,0.530938123752495,"models that show signs of grokking also experience Slingshot Effects while models that do not
206"
EXPERIMENTAL SETUP,0.5329341317365269,"experience Slingshot Effects do not show any signs of grokking. Lastly, the model trained with the
207"
EXPERIMENTAL SETUP,0.5349301397205589,"largest ϵ = 10−5 shows no sign of generalization even after receiving 500K updates.
208"
EFFECTS ON GENERALIZATION,0.5369261477045908,"3.2
Effects on Generalization
209"
EFFECTS ON GENERALIZATION,0.5389221556886228,"In order to understand the relationship between Slingshot Effects and neural networks generalization,
210"
EFFECTS ON GENERALIZATION,0.5409181636726547,"we experiment with various models and datasets. We observe that models that exhibit Slingshot tend
211"
EFFECTS ON GENERALIZATION,0.5429141716566867,"to generalize better, which suggests the benefit of training models for a long time with Adam [7] and
212"
EFFECTS ON GENERALIZATION,0.5449101796407185,"AdamW [10]. More surprisingly, we observe that Slingshots and grokking tend to come in tandem.
213"
EFFECTS ON GENERALIZATION,0.5469061876247505,"Transformers with algorithmic datasets
We follow the setting in Power et al. [13] and generate
214"
EFFECTS ON GENERALIZATION,0.5489021956087824,"several datasets that represent algorithmic operations and consider several training and validation
215"
EFFECTS ON GENERALIZATION,0.5508982035928144,"splits. This dataset creation approach is consistent with the methodology used to demonstrate
216"
EFFECTS ON GENERALIZATION,0.5528942115768463,"grokking [13]. The Transformer is trained with AdamW [10] with a learning rate of 0.001, weight
217"
EFFECTS ON GENERALIZATION,0.5548902195608783,"decay set to 0, and with learning rate warmup for 500K steps. We consider ϵ of AdamW as a
218"
EFFECTS ON GENERALIZATION,0.5568862275449101,"hyperparameter in this experiment. Figure 5 summarizes the results for this experiment where the
219"
EFFECTS ON GENERALIZATION,0.5588822355289421,"x-axis indicates the algorithmic operation followed by the training data split size. As can be seen
220"
EFFECTS ON GENERALIZATION,0.5608782435129741,"in Figure 5, Slingshot Effects are seen with lower values of ϵ and disappear with higher values of ϵ
221"
EFFECTS ON GENERALIZATION,0.562874251497006,"ϵ = 10−8
ϵ = 10−7
ϵ = 10−5"
EFFECTS ON GENERALIZATION,0.564870259481038,"0
10000
20000
30000
40000
50000
epoch 10
10 10
8 10
6 10
4 10
2 100 loss"
EFFECTS ON GENERALIZATION,0.5668662674650699,"train loss
last layer norm 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
EFFECTS ON GENERALIZATION,0.5688622754491018,last layer norm
EFFECTS ON GENERALIZATION,0.5708582834331337,"0
10000
20000
30000
40000
50000
epoch 10
9 10
7 10
5 10
3 10
1 101 loss"
EFFECTS ON GENERALIZATION,0.5728542914171657,"train loss
last layer norm 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTS ON GENERALIZATION,0.5748502994011976,last layer norm
EFFECTS ON GENERALIZATION,0.5768463073852296,"0
10000
20000
30000
40000
50000
epoch 10
7 10
5 10
3 10
1 101 loss"
EFFECTS ON GENERALIZATION,0.5788423153692615,"train loss
last layer norm 0.06 0.08 0.10 0.12 0.14 0.16"
EFFECTS ON GENERALIZATION,0.5808383233532934,last layer norm
EFFECTS ON GENERALIZATION,0.5828343313373253,"(a)
(b)
(c)
training loss vs epochs"
EFFECTS ON GENERALIZATION,0.5848303393213573,"0
10000
20000
30000
40000
50000
epoch 10
2 10
1 100 101 loss"
EFFECTS ON GENERALIZATION,0.5868263473053892,"validation loss
last layer norm 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
EFFECTS ON GENERALIZATION,0.5888223552894212,last layer norm
EFFECTS ON GENERALIZATION,0.590818363273453,"0
10000
20000
30000
40000
50000
epoch 10
2 10
1 100 101 loss"
EFFECTS ON GENERALIZATION,0.592814371257485,"validation loss
last layer norm 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTS ON GENERALIZATION,0.5948103792415169,last layer norm
EFFECTS ON GENERALIZATION,0.5968063872255489,"0
10000
20000
30000
40000
50000
epoch"
EFFECTS ON GENERALIZATION,0.5988023952095808,3 × 100
EFFECTS ON GENERALIZATION,0.6007984031936128,4 × 100
EFFECTS ON GENERALIZATION,0.6027944111776448,6 × 100 loss
EFFECTS ON GENERALIZATION,0.6047904191616766,"validation loss
last layer norm 0.06 0.08 0.10 0.12 0.14 0.16"
EFFECTS ON GENERALIZATION,0.6067864271457086,last layer norm
EFFECTS ON GENERALIZATION,0.6087824351297405,"(d)
(e)
(f)
validation loss vs epochs"
EFFECTS ON GENERALIZATION,0.6107784431137725,"0
10000
20000
30000
40000
50000
epoch 0 20 40 60 80 100"
EFFECTS ON GENERALIZATION,0.6127744510978044,accuracy
EFFECTS ON GENERALIZATION,0.6147704590818364,"train accuracy
last layer norm 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
EFFECTS ON GENERALIZATION,0.6167664670658682,last layer norm
EFFECTS ON GENERALIZATION,0.6187624750499002,"0
10000
20000
30000
40000
50000
epoch 0 20 40 60 80 100"
EFFECTS ON GENERALIZATION,0.6207584830339321,accuracy
EFFECTS ON GENERALIZATION,0.6227544910179641,"train accuracy
last layer norm 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTS ON GENERALIZATION,0.624750499001996,last layer norm
EFFECTS ON GENERALIZATION,0.626746506986028,"0
10000
20000
30000
40000
50000
epoch 0 20 40 60 80 100"
EFFECTS ON GENERALIZATION,0.6287425149700598,accuracy
EFFECTS ON GENERALIZATION,0.6307385229540918,"train accuracy
last layer norm 0.06 0.08 0.10 0.12 0.14 0.16"
EFFECTS ON GENERALIZATION,0.6327345309381237,last layer norm
EFFECTS ON GENERALIZATION,0.6347305389221557,"(g)
(h)
(i)
training accuracy vs epochs"
EFFECTS ON GENERALIZATION,0.6367265469061876,"0
10000
20000
30000
40000
50000
epoch 0 20 40 60 80 100"
EFFECTS ON GENERALIZATION,0.6387225548902196,accuracy
EFFECTS ON GENERALIZATION,0.6407185628742516,"validation accuracy
last layer norm 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
EFFECTS ON GENERALIZATION,0.6427145708582834,last layer norm
EFFECTS ON GENERALIZATION,0.6447105788423154,"0
10000
20000
30000
40000
50000
epoch 0 20 40 60 80 100"
EFFECTS ON GENERALIZATION,0.6467065868263473,accuracy
EFFECTS ON GENERALIZATION,0.6487025948103793,"validation accuracy
last layer norm 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTS ON GENERALIZATION,0.6506986027944112,last layer norm
EFFECTS ON GENERALIZATION,0.6526946107784432,"0
10000
20000
30000
40000
50000
epoch 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
EFFECTS ON GENERALIZATION,0.654690618762475,accuracy
EFFECTS ON GENERALIZATION,0.656686626746507,"validation accuracy
last layer norm 0.06 0.08 0.10 0.12 0.14 0.16"
EFFECTS ON GENERALIZATION,0.6586826347305389,last layer norm
EFFECTS ON GENERALIZATION,0.6606786427145709,"(j)
(k)
(l)
validation accuracy vs epochs"
EFFECTS ON GENERALIZATION,0.6626746506986028,"Figure 4: Varying ϵ in Adam on the Division dataset. Observe that as ϵ increases, there is no Slingshot Effect or
grokking behavior. Figure (a) corresponds to default ϵ suggested in [7] where the model trained with smallest
value undergoes multiple Slingshot cycles."
EFFECTS ON GENERALIZATION,0.6646706586826348,"which confirms the observations made in Section 3 with modular division dataset. In addition, models
222"
EFFECTS ON GENERALIZATION,0.6666666666666666,"that exhibit Slingshot Effects and grokking (shown in green) tend to generalize better than models
223"
EFFECTS ON GENERALIZATION,0.6686626746506986,"that do not experience Slingshot Effects and grokking (shown in red).
224"
EFFECTS ON GENERALIZATION,0.6706586826347305,"ViT with CIFAR-10
For further validation of Slingshot Effects and generalization, we train a
225"
EFFECTS ON GENERALIZATION,0.6726546906187625,"Vision Transformer (ViT) [4] on CIFAR-10 [8]. The ViT consists of 12 layers, width 384 and
226"
EFFECTS ON GENERALIZATION,0.6746506986027944,"12 attention heads trained on fixed subsets of CIFAR-10 dataset [8]. The ViT model described
227"
EFFECTS ON GENERALIZATION,0.6766467065868264,"above is trained with 10K, 20K, 30K, 40K and 50K (full dataset) training samples. We train the
228"
EFFECTS ON GENERALIZATION,0.6786427145708582,"models with the following learning rates: 0.0001, 0.00031 and 0.001 and with a linear learning rate
229"
EFFECTS ON GENERALIZATION,0.6806387225548902,"warmup for the 1 epoch of optimization. We consider multiple learning rates to study the impact of
230"
EFFECTS ON GENERALIZATION,0.6826347305389222,"this hyperparameter on Slingshot taking inspiration from [13] where the authors report observing
231"
EFFECTS ON GENERALIZATION,0.6846307385229541,"No Slingshot Effects, no grokking
Slingshot Effects and grokking
Slingshot Effects, no grokking"
EFFECTS ON GENERALIZATION,0.6866267465069861,"Figure 5: Extended analysis on multiple grokking datasets. Points shown in green represent both Slingshot
Effects and grokking, points shown blue indicate Slingshot Effects but not grokking while points in red indicate
no Slingshot Effects and no grokking. ϵ in Adam is varied as shown in text. Observe that as ϵ increases, there
are no Slingshot Effects or grokking behavior."
EFFECTS ON GENERALIZATION,0.688622754491018,"grokking over a narrow range of learning rates . Figure 6 shows a plot of the highest test accuracy for
232"
EFFECTS ON GENERALIZATION,0.6906187624750499,"a set of hyperparameters (learning rate, number of training samples) as a function of the number of
233"
EFFECTS ON GENERALIZATION,0.6926147704590818,"training samples from which we make the following observations. The best test accuracy for a given
234"
EFFECTS ON GENERALIZATION,0.6946107784431138,"set of hyperparameters is typically achieved after Slingshot phase begins during optimization. The
235"
EFFECTS ON GENERALIZATION,0.6966067864271457,"checkpoints that achieve the highest test accuracy are labeled as ""post-slingshot"" and shown in green
236"
EFFECTS ON GENERALIZATION,0.6986027944111777,"in Figure 6. While post-Slingshot checkpoints seem to enjoy higher test accuracy, there are certain
237"
EFFECTS ON GENERALIZATION,0.7005988023952096,"combinations of hyperparameters that lead to models that show better test accuracy prior to the start
238"
EFFECTS ON GENERALIZATION,0.7025948103792415,"of the first Slingshot phase. We label these points as ""pre-slingshot"" (shown in blue) in Figure 6. The
239"
EFFECTS ON GENERALIZATION,0.7045908183632734,"above observations appear to be consistent with our finding that training long periods of time may
240"
EFFECTS ON GENERALIZATION,0.7065868263473054,"lead to better generalization seen with grokking datasets [13].
241"
EFFECTS ON GENERALIZATION,0.7085828343313373,"Non-Transformer Models
We conduct experiments with MLPs on synthetic data where the
242"
EFFECTS ON GENERALIZATION,0.7105788423153693,"synthetic data is a low dimensional embedding projected to higher dimensions via random projections.
243"
EFFECTS ON GENERALIZATION,0.7125748502994012,"This design choice is critical with showing the existence of the Slingshot Effect with synthetically
244"
EFFECTS ON GENERALIZATION,0.7145708582834331,"generated data. We find that using low dimensional data does not lead to any Slingshots. With this
245"
EFFECTS ON GENERALIZATION,0.716566866267465,"dataset, we show that generalization occurs late in training with Adam. Specifically, we tune ϵ in
246"
EFFECTS ON GENERALIZATION,0.718562874251497,"Adam and show that the optimizer is highly sensitive to this hyperparameter. These observations are
247"
EFFECTS ON GENERALIZATION,0.720558882235529,"consistent with the behavior reported above with Transformers and on algorithmic datasets as well
248"
EFFECTS ON GENERALIZATION,0.7225548902195609,"as standard vision benchmark such as CIFAR-10. We refer the reader to Appendix ?? for complete
249"
EFFECTS ON GENERALIZATION,0.7245508982035929,"description and details of these experiments.
250"
DRAWBACKS AND LIMITATIONS,0.7265469061876247,"3.3
Drawbacks and Limitations
251"
DRAWBACKS AND LIMITATIONS,0.7285429141716567,"While the Slingshot Mechanism exposes an interesting implicit bias of Adam that often promotes
252"
DRAWBACKS AND LIMITATIONS,0.7305389221556886,"generalization, due to its arresting of the norm growth and ensuing feature learning, it also leads to
253"
DRAWBACKS AND LIMITATIONS,0.7325349301397206,"some training instability and prolonged training time. In the Appendix we show that it is possible to
254"
DRAWBACKS AND LIMITATIONS,0.7345309381237525,"achieve similar levels of generalization with Adam on the modular division dataset [13] using the
255"
DRAWBACKS AND LIMITATIONS,0.7365269461077845,"same Transformer setup as above, while maintaining stable learning, in regimes that do not show
256"
DRAWBACKS AND LIMITATIONS,0.7385229540918163,"a clear Slingshot Effect. First we employ weight decay, which causes the training loss values to
257"
DRAWBACKS AND LIMITATIONS,0.7405189620758483,"converge to a higher value than the unregularized model. In this regime the model does not become
258"
DRAWBACKS AND LIMITATIONS,0.7425149700598802,"unstable, but instead regularization leads to comparable generalization, and much more quickly.
259"
DRAWBACKS AND LIMITATIONS,0.7445109780439122,"However, it is important to tune the regularization strength appropriately. Similarly, we find that it is
260"
DRAWBACKS AND LIMITATIONS,0.7465069860279441,"10
15
20
25
30
35
40
45
50
number of training samples (x1000) 55 60 65 70 75"
DRAWBACKS AND LIMITATIONS,0.7485029940119761,test accuracy 1e-04 3e-04 1e-03 1e-04 3e-04 1e-03
DRAWBACKS AND LIMITATIONS,0.7504990019960079,"1e-04
3e-04 1e-03"
DRAWBACKS AND LIMITATIONS,0.7524950099800399,"1e-04
3e-04 1e-03 1e-04 3e-04 1e-03"
DRAWBACKS AND LIMITATIONS,0.7544910179640718,"no slingshot
pre-slingshot
post-slingshot"
DRAWBACKS AND LIMITATIONS,0.7564870259481038,"Figure 6: Slingshot Effects on subsets of CIFAR-10 dataset. We train ViTs with multiple learning rates to verify
the impact this parameter has on Slingshot. Power et al [13] note that grokking occurs over a narrow range of
learning rates. Note that the points marked in: (i) green correspond to test accuracy for an experiment after the
Slingshot Effect begins, (ii) blue are for trials where best checkpoint is observed prior to start of a Slingshot
Effect and (iii) red are for trials with no Slingshot Effect."
DRAWBACKS AND LIMITATIONS,0.7584830339321357,"possible to normalize the features and weights using the following scheme to explicitly control norm
261"
DRAWBACKS AND LIMITATIONS,0.7604790419161677,"growth: w =
w
∥w∥, f(x) =
f(x)
∥f(x)∥, where w and f(x) are the weights and inputs to the classification
262"
DRAWBACKS AND LIMITATIONS,0.7624750499001997,"layer respectively, the norm used above is the L2 norm, and x is the input to the neural network. This
263"
DRAWBACKS AND LIMITATIONS,0.7644710578842315,"scheme also results in stable training and similar levels of generalization. In all cases the effects rely
264"
DRAWBACKS AND LIMITATIONS,0.7664670658682635,"on keeping the weight norms from growing uncontrollably, which may be the most important factor
265"
DRAWBACKS AND LIMITATIONS,0.7684630738522954,"for improving generalization. These results suggest that while the Slingshot Mechanism may be an
266"
DRAWBACKS AND LIMITATIONS,0.7704590818363274,"interesting self-correcting scheme for controlling norm growth, there are likely more efficient ways
267"
DRAWBACKS AND LIMITATIONS,0.7724550898203593,"to leverage adaptive optimizers to similar levels of generalization without requiring the instability
268"
DRAWBACKS AND LIMITATIONS,0.7744510978043913,"that is a hallmark of the Slingshot effect.
269"
DRAWBACKS AND LIMITATIONS,0.7764471057884231,"Finally, we lack a satisfactory theoretical explanation for the Slingshot Mechanism, and hence
270"
DRAWBACKS AND LIMITATIONS,0.7784431137724551,"removed all attempts at a more rigorous mathematical definition, which we feel would only serve as a
271"
DRAWBACKS AND LIMITATIONS,0.780439121756487,"distraction.
272"
CONCLUSION,0.782435129740519,"4
Conclusion
273"
CONCLUSION,0.7844311377245509,"We have empirically shown that optimizing deep networks with cross entropy loss and adaptive
274"
CONCLUSION,0.7864271457085829,"optimizers produces the Slingshot Mechanism, a curious optimization anomaly unlike anything
275"
CONCLUSION,0.7884231536926147,"described in the literature. We have provided ample evidence that Slingshot Effects can be observed
276"
CONCLUSION,0.7904191616766467,"with different neural architectures and datasets. Furthermore, we find that Grokking [13] almost
277"
CONCLUSION,0.7924151696606786,"always occurs in the presence of Slingshot Effects and associated regions of instability in the Terminal
278"
CONCLUSION,0.7944111776447106,"Phase of Training (TPT). These results in their pure form absent explicit regularization, reveal an
279"
CONCLUSION,0.7964071856287425,"intriguing inductive bias of adaptive gradient optimizers that becomes salient in the TPT, characterized
280"
CONCLUSION,0.7984031936127745,"by cyclic stepwise effects on the optimization trajectory. These effects often promote generalization
281"
CONCLUSION,0.8003992015968064,"in ways that differ from non-adaptive optimizers like SGD, and warrant further study to be able
282"
CONCLUSION,0.8023952095808383,"to harness efficiently. There are open question remaining to be answered, for instance 1) What’s
283"
CONCLUSION,0.8043912175648703,"the causal factor of the plateau of weight norm growth? 2) Are there better ways of promoting
284"
CONCLUSION,0.8063872255489022,"generalization without relying on this accidental training instability? Answering these questions w ill
285"
CONCLUSION,0.8083832335329342,"allow us to decouple optimization and regularization, and ultimately to control and improve them
286"
CONCLUSION,0.810379241516966,"independently.
287"
SOCIETAL IMPACT,0.812375249500998,"5
Societal Impact
288"
SOCIETAL IMPACT,0.8143712574850299,"This is a fundamental work in Deep Learning, it will impact the society via its effects on relevant
289"
SOCIETAL IMPACT,0.8163672654690619,"models and applications.
290"
REFERENCES,0.8183632734530938,"References
291"
REFERENCES,0.8203592814371258,"[1] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
292"
REFERENCES,0.8223552894211577,"over-parameterization. ArXiv, abs/1811.03962, 2019.
293"
REFERENCES,0.8243512974051896,"[2] Anas Barakat and Pascal Bianchi. Convergence and dynamical behavior of the adam algorithm
294"
REFERENCES,0.8263473053892215,"for nonconvex stochastic optimization. SIAM J. Optim., 31:244–274, 2021.
295"
REFERENCES,0.8283433133732535,"[3] Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient
296"
REFERENCES,0.8303393213572854,"descent on neural networks typically occurs at the edge of stability. arXiv preprint arXiv:
297"
REFERENCES,0.8323353293413174,"Arxiv-2103.00065, 2021.
298"
REFERENCES,0.8343313373253493,"[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
299"
REFERENCES,0.8363273453093812,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
300"
REFERENCES,0.8383233532934131,"Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
301"
REFERENCES,0.8403193612774451,"recognition at scale. arXiv preprint arXiv: Arxiv-2010.11929, 2020.
302"
REFERENCES,0.8423153692614771,"[5] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
303"
REFERENCES,0.844311377245509,"and stochastic optimization. Journal of Machine Learning Research, 12(61):2121–2159, 2011.
304"
REFERENCES,0.846307385229541,"[6] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the
305"
REFERENCES,0.8483033932135728,"generalization gap in large batch training of neural networks. ArXiv, abs/1705.08741, 2017.
306"
REFERENCES,0.8502994011976048,"[7] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
307"
REFERENCES,0.8522954091816367,"arXiv: Arxiv-1412.6980, 2014.
308"
REFERENCES,0.8542914171656687,"[8] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
309"
REFERENCES,0.8562874251497006,"[9] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari.
310"
REFERENCES,0.8582834331337326,"The large learning rate phase of deep learning: the catapult mechanism.
arXiv preprint
311"
REFERENCES,0.8602794411177644,"arXiv:2003.02218, 2020.
312"
REFERENCES,0.8622754491017964,"[10] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
313"
REFERENCES,0.8642714570858283,"arXiv:1711.05101, 2017.
314"
REFERENCES,0.8662674650698603,"[11] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural
315"
REFERENCES,0.8682634730538922,"networks. arXiv preprint arXiv:1906.05890, 2019.
316"
REFERENCES,0.8702594810379242,"[12] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the
317"
REFERENCES,0.872255489021956,"terminal phase of deep learning training. Proceedings of the National Academy of Sciences of
318"
REFERENCES,0.874251497005988,"the United States of America, 117:24652 – 24663, 2020.
319"
REFERENCES,0.8762475049900199,"[13] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking:
320"
REFERENCES,0.8782435129740519,"Generalization beyond overfitting on small algorithmic datasets. In ICLR MATH-AI Workshop,
321"
REFERENCES,0.8802395209580839,"2021.
322"
REFERENCES,0.8822355289421158,"[14] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
323"
REFERENCES,0.8842315369261478,"implicit bias of gradient descent on separable data. The Journal of Machine Learning Research,
324"
REFERENCES,0.8862275449101796,"19(1):2822–2878, 2018.
325"
REFERENCES,0.8882235528942116,"[15] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for
326"
REFERENCES,0.8902195608782435,"machine learning. University of Toronto, Technical Report, 6, 2012.
327"
REFERENCES,0.8922155688622755,"[16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
328"
REFERENCES,0.8942115768463074,"Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv: Arxiv-
329"
REFERENCES,0.8962075848303394,"1706.03762, 2017.
330"
REFERENCES,0.8982035928143712,"[17] Bohan Wang, Qi Meng, Wei Chen, and Tie-Yan Liu. The implicit bias for adaptive optimization
331"
REFERENCES,0.9001996007984032,"algorithms on homogeneous neural networks. In International Conference on Machine Learning,
332"
REFERENCES,0.9021956087824351,"pages 10849–10858. PMLR, 2021.
333"
REFERENCES,0.9041916167664671,"[18] J. Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
334"
REFERENCES,0.906187624750499,"training: A theoretical justification for adaptivity. arXiv: Optimization and Control, 2020.
335"
REFERENCES,0.908183632734531,"Checklist
336"
REFERENCES,0.9101796407185628,"The checklist follows the references. Please read the checklist guidelines carefully for information on
337"
REFERENCES,0.9121756487025948,"how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or
338"
REFERENCES,0.9141716566866267,"[N/A] . You are strongly encouraged to include a justification to your answer, either by referencing
339"
REFERENCES,0.9161676646706587,"the appropriate section of your paper or providing a brief inline description. For example:
340"
REFERENCES,0.9181636726546906,"• Did you include the license to the code and datasets? [Yes] See Section ??.
341"
REFERENCES,0.9201596806387226,"• Did you include the license to the code and datasets? [No] The code and the data are
342"
REFERENCES,0.9221556886227545,"proprietary.
343"
REFERENCES,0.9241516966067864,"• Did you include the license to the code and datasets? [N/A]
344"
REFERENCES,0.9261477045908184,"Please do not modify the questions and only use the provided macros for your answers. Note that the
345"
REFERENCES,0.9281437125748503,"Checklist section does not count towards the page limit. In your paper, please delete this instructions
346"
REFERENCES,0.9301397205588823,"block and only keep the Checklist section heading above along with the questions/answers below.
347"
REFERENCES,0.9321357285429142,"1. For all authors...
348"
REFERENCES,0.9341317365269461,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
349"
REFERENCES,0.936127744510978,"contributions and scope? [Yes]
350"
REFERENCES,0.93812375249501,"(b) Did you describe the limitations of your work? [Yes]
351"
REFERENCES,0.9401197604790419,"(c) Did you discuss any potential negative societal impacts of your work? [Yes]
352"
REFERENCES,0.9421157684630739,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
353"
REFERENCES,0.9441117764471058,"them? [Yes]
354"
REFERENCES,0.9461077844311377,"2. If you are including theoretical results...
355"
REFERENCES,0.9481037924151696,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
356"
REFERENCES,0.9500998003992016,"(b) Did you include complete proofs of all theoretical results? [N/A]
357"
REFERENCES,0.9520958083832335,"3. If you ran experiments...
358"
REFERENCES,0.9540918163672655,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
359"
REFERENCES,0.9560878243512974,"mental results (either in the supplemental material or as a URL)? [Yes]
360"
REFERENCES,0.9580838323353293,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
361"
REFERENCES,0.9600798403193613,"were chosen)? [Yes]
362"
REFERENCES,0.9620758483033932,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
363"
REFERENCES,0.9640718562874252,"ments multiple times)? [Yes]
364"
REFERENCES,0.9660678642714571,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
365"
REFERENCES,0.9680638722554891,"of GPUs, internal cluster, or cloud provider)? [Yes]
366"
REFERENCES,0.9700598802395209,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
367"
REFERENCES,0.9720558882235529,"(a) If your work uses existing assets, did you cite the creators? [Yes]
368"
REFERENCES,0.9740518962075848,"(b) Did you mention the license of the assets? [Yes]
369"
REFERENCES,0.9760479041916168,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
370"
REFERENCES,0.9780439121756487,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
371"
REFERENCES,0.9800399201596807,"using/curating? [Yes]
372"
REFERENCES,0.9820359281437125,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
373"
REFERENCES,0.9840319361277445,"information or offensive content? [Yes]
374"
REFERENCES,0.9860279441117764,"5. If you used crowdsourcing or conducted research with human subjects...
375"
REFERENCES,0.9880239520958084,"(a) Did you include the full text of instructions given to participants and screenshots, if
376"
REFERENCES,0.9900199600798403,"applicable? [Yes]
377"
REFERENCES,0.9920159680638723,"(b) Did you describe any potential participant risks, with links to Institutional Review
378"
REFERENCES,0.9940119760479041,"Board (IRB) approvals, if applicable? [Yes]
379"
REFERENCES,0.9960079840319361,"(c) Did you include the estimated hourly wage paid to participants and the total amount
380"
REFERENCES,0.998003992015968,"spent on participant compensation? [Yes]
381"
