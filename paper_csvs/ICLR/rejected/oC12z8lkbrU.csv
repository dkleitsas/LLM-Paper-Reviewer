Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002652519893899204,"Semi-Supervised Learning (SSL) has seen success in many application domains,
but this success often relies on the availability of task-speciﬁc unlabeled data.
Knowledge distillation (KD) has enabled compressing deep networks, achieving
the best results when distilling knowledge on fresh task-speciﬁc unlabeled data.
However, task-speciﬁc unlabeled data can be challenging to ﬁnd, especially for
NLP. We present a simple framework called “generate, annotate, and learn (GAL)”
that uses unconditional language models to synthesize in-domain unlabeled data,
helping advance SSL and KD on NLP and tabular tasks. To obtain strong task-
speciﬁc generative models, we either ﬁne-tune a large language model (LLM)
on inputs from speciﬁc tasks, or prompt a LLM with a few input examples to
generate more unlabeled examples. Then, we use existing classiﬁers to annotate
generated unlabeled examples with pseudo labels, which are used as additional
training data or as additional prompts. GAL improves prompt-based few-shot
learning on several NLP tasks. It also yields a new state-of-the-art for 6-layer
transformers on the GLUE leaderboard. Finally, self-training with GAL offers
large gains on four tabular tasks from the UCI repository."
INTRODUCTION,0.005305039787798408,"1
INTRODUCTION"
INTRODUCTION,0.007957559681697613,"Unlabeled data is abundant in the real world, but task-speciﬁc unlabeled data within the scope of
a given machine learning problem can be challenging to ﬁnd. For instance, one cannot easily ﬁnd
in-domain unlabeled data conforming to the input distribution of a speciﬁc Natural Language Pro-
cessing (NLP) task from the GLUE benchmark (Wang et al., 2019b). Some NLP tasks require an
input comprising a pair of sentences with a particular relationship between them. Moreover, classiﬁ-
cation datasets typically represent a tailored distribution of text and only include a limited number of
class labels. If task-speciﬁc unlabeled data were available, one could adopt self-training (Yarowsky,
1995) to automatically annotate unlabeled data with pseudo labels to improve accuracy and robust-
ness of classiﬁers (Xie et al., 2020; Carmon et al., 2019b). In addition, one can use knowledge
distillation (Hinton et al., 2015) on fresh task-speciﬁc unlabeled data to more effectively compress
deep neural networks and ensembles (Buciluˇa et al., 2006; Chen et al., 2020c)."
INTRODUCTION,0.010610079575596816,"When task-speciﬁc unlabeled examples do not exist, one can try to retrieve them from a large and
diverse open-domain dataset. For instance, Du et al. (2020) have used nearest neighbor retrieval
to harvest in-domain unlabeled text from the internet, leading to a successful application of self-
training and knowledge distillation to certain NLP tasks. While retrieval can indeed help to ﬁnd
in-domain data for problems with simple inputs, it is not practical for problems with complex input
schemes, e.g., sentence pairs with certain relations and tabular data. Accordingly, self-training and
retrieval-based methods have not been widely adopted for NLP tasks, e.g., on the GLUE benchmark."
INTRODUCTION,0.013262599469496022,"This paper presents a deceptively simple and general framework called “generate, annotate, and learn
(GAL)” to help advance semi-supervised learning and knowledge distillation on various applications
that do not come with unlabeled data. We advocate for the use of language models to synthesize
unlabeled tasks-speciﬁc data, in lieu of real unlabeled data. We build on recent advances in text
generation (Radford et al., 2019; Gao et al., 2021), and use powerful generative models to synthesize
unlabeled text and tables. Then, we use state-of-the-art classiﬁers to annotate generated unlabeled
data with pseudo labels. Finally, we combine labeled data with pseudo labeled data to train more
effective classiﬁers or for the purpose of knowledge distillation (KD)."
INTRODUCTION,0.015915119363395226,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01856763925729443,"We motivate GAL by making connections to empirical and vicinal risk minimization (Vapnik, 1992;
Chapelle et al., 2001), and demonstrate its utility by presenting empirical results on a wide range of
applications. Our key contributions include:"
INTRODUCTION,0.021220159151193633,"• We propose a simple way to advance SSL, KD, and few-shot learning on NLP by using language
models to synthesize large amounts of task-speciﬁc unlabeled data.
• We link GAL to empirical and vicinal risk minimization, helping explain why GAL works and
why synthetic samples from class-conditional language models are not as effective.
• We systematically dissect GAL and study the key components leading to its success.
• GAL establishes a new SoTA for a single 6-layer transformer on the GLUE test set.
• GAL improves prompt-based few-shot learning, providing an average improvement of 1.3% on
four 4-shot learning NLP tasks.
• GAL advance self-training for tabular tasks, outperforming XGBoost on 2 out of 4 tasks."
RELATED WORK,0.023872679045092837,"2
RELATED WORK"
RELATED WORK,0.026525198938992044,There has been a surge of interest in improving accuracy and label efﬁciency of classiﬁers via:
RELATED WORK,0.029177718832891247,"1. Self-Supervised pretraining on open-domain unlabeled data in a task-agnostic way (Peters et al.,
2018; Devlin et al., 2019; Chen et al., 2020b),
2. Self-Training using domain-speciﬁc unlabeled data in a task-speciﬁc way (Rosenberg et al.,
2005; McClosky et al., 2006; Xie et al., 2020).
While self-supervised learning can be applied to a broad distribution of unlabeled data, self-training
requires unlabeled data that at least can be annotated using the same set of class labels available for
the downstream task (Oliver et al., 2018). For instance, if one is interested in training a classiﬁer to
distinguish images of cats and dogs, self-training with images of aircraft is likely not helpful, but it
is conceivable that self-supervised learning with images of aircraft can still help. A growing body of
recent work suggests that perhaps self-supervised pretraining and self-training are compatible and
can be combined to achieve the best semi-supervised learning performance (Chen et al., 2020c; Du
et al., 2020). We corroborate the existing evidence by showing gains from generative self-training."
RELATED WORK,0.03183023872679045,"Semi-supervised learning (SSL) has a long and rich history in machine learning (Cooper & Free-
man, 1970; McLachlan & Ganesalingam, 1982; Riloff, 1996; Chapelle et al., 2009; Van Engelen &
Hoos, 2020). One of the oldest family of SSL algorithms is self-training, a.k.a. self-learning or self-
labeling (Scudder, 1965; Fralick, 1967; Agrawala, 1970; Yarowsky, 1995). Self-training encourages
knowledge transfer between a teacher and a student model in such a way that the student can outper-
form the teacher. Speciﬁcally, one leverages the teacher’s knowledge to annotate unlabeled data with
so-called pseudo labels, and the student learns from a mixture of pseudo- and human-labeled data.
Self-training has recently seen renewed interest across vision and NLP applications (Yalniz et al.,
2019; Xie et al., 2020; Zoph et al., 2020; Du et al., 2020). Recent work aims to combine self-training
and consistency regularization to develop powerful SSL algorithms. The key idea is to ensure that
the predictions of a classiﬁer on unlabeled examples are robust to strong augmentations (Berthelot
et al., 2019a; Sohn et al., 2020; Xie et al., 2019). We build on prior work and investigate the use of
synthetic data within the broad family of self-training methods."
RELATED WORK,0.034482758620689655,"Recent theoretical work analyzes self-training for linear models, often under the assumption that the
data distribution is (nearly) Gaussian (Carmon et al., 2019a; Raghunathan et al., 2020; Chen et al.,
2020d; Kumar et al., 2020a; Oymak & Gulcu, 2020). Wei et al. (2021) prove that, under “expansion”
and “class separation” assumptions, self-training can lead to more accurate neural network classi-
ﬁers. We present a theoretical framing of GAL in terms of empirical and vicinal risk minimization
(Vapnik, 1992; Chapelle et al., 2001)."
RELATED WORK,0.03713527851458886,"An important family of related work uses generative models for SSL by learning features that are
useful for both generation and discrimination (e.g., Chen et al., 2020a; Odena, 2016; Dai et al.,
2017). For instance, Kingma et al. (2014) approach SSL by viewing missing class labels as a set
of latent variables and use variational inference to impute missing labels as well as other factors
of variation. By contrast, our work does not learn features using generative models and keeps the
generative and discriminative processes separate. This offers more ﬂexibility and allows GAL to use
self-supervised pretraining methods that are not fully generative."
RELATED WORK,0.03978779840848806,"Our work is closely related to recent work on the uses of generative models for data augmenta-
tion (Norouzi et al., 2020; Yang et al., 2020). Unlike Norouzi et al. (2020), we do not use instance-"
RELATED WORK,0.042440318302387266,Under review as a conference paper at ICLR 2022
RELATED WORK,0.04509283819628647,"based generative models. Yang et al. (2020) propose a complex scheme, including data relabeling,
data ﬁltering, and two-stage training, to utilize synthetic data. By contrast, we show that a simple
mixture of the original data and synthetic data can provide sizable gains. Furthermore, we show a
more broad use of generative models on KD and few-shot learning, in addition to tabular tasks."
RELATED WORK,0.04774535809018567,"Knowledge Distillation (KD) (Buciluˇa et al., 2006; Hinton et al., 2015) uses a procedure similar
to self-training to distill knowledge of an expressive teacher model into a smaller student model.
In contrast, self-distillation (Furlanello et al., 2018; Zhang et al., 2019a; Mobahi et al., 2020) uses
teacher and student models of equal size, hoping to iteratively reﬁne class labels. Previous work
uses unlabeled data (Buciluˇa et al., 2006) and adversarial training (Wang et al., 2018) to improve
KD. We demonstrate that synthetic data generated by unconditional generative models can improve
KD on NLP, outperforming strong KD baselines, which often add more complexity and additional
hyper-parameters (e.g., Sun et al., 2019a; Jiao et al., 2019; Xu et al., 2020; Rashid et al., 2021)."
RELATED WORK,0.050397877984084884,"Advanced generative models are able to generate realistic images and text (Karras et al., 2017; Brock
et al., 2019; Karras et al., 2019; Radford et al., 2019; Brown et al., 2020). The quality of synthetic
samples has improved to the extent that deep fake detection has become an important research
topic itself (Zellers et al., 2019; Dolhansky et al., 2019). Recent work has aimed to utilize class-
conditional generative models to help improve supervised learning (Antoniou et al., 2017; Bowles
et al., 2018; Zhang et al., 2019b; Kumar et al., 2020b; Gao et al., 2020). However, Ravuri & Vinyals
(2019) have shown that images generated by state-of-the-art class-conditional generative models fall
short of improving ImageNet classiﬁcation accuracy, despite strong sample quality scores (Salimans
et al., 2016; Heusel et al., 2017). Similarly, Kumar et al. (2020b) ﬁnd that it is difﬁcult for sentences
generated by label-conditioned GPT-2 (Radford et al., 2019) to retain the semantics or pragmatics
of a speciﬁed category, which leads to poor performance on downstream tasks. We discuss why
class-conditional generative models are hardly effective for supervised learning, and instead, focus
on unconditional generative models."
BACKGROUND ON SELF-TRAINING,0.05305039787798409,"3
BACKGROUND ON SELF-TRAINING"
BACKGROUND ON SELF-TRAINING,0.05570291777188329,"Given a labeled dataset L = {(xi, yi)}N
i=1 and an unlabeled dataset U = {xj}M
j=1, we summarize
the general family of SSL algorithms known as self-training as:"
BACKGROUND ON SELF-TRAINING,0.058355437665782495,"1. First, an initial model denoted f1 is trained using supervised learning on the labeled dataset L.
2. Then, at iteration t, one adopts ft as the teacher model to annotate the unlabeled dataset U using
pseudo labels. Optionally, one uses a selection method to pick a subset St ⊆{(xj, ft(xj))}M
j=1
of pseudo labeled examples.
3. A student model ft+1 is trained to optimize a classiﬁcation loss on the combination of L and St:"
BACKGROUND ON SELF-TRAINING,0.0610079575596817,"ℓt+1 = E(x,y)∼(L ∪St)H(y, ft+1(x)) ,
(1)"
BACKGROUND ON SELF-TRAINING,0.0636604774535809,"where H(q, p) = q⊤log p is the softmax cross entropy loss, and y is assumed to be a one-hot
vector (original labels) or a vector of class probabilities (soft pseudo labels).
4. Self-training iterations are repeated T times or until performance plateaus."
BACKGROUND ON SELF-TRAINING,0.06631299734748011,"Many different variants of the basic self-training algorithm discussed above exist in the literature.
These variants differ in the type of pseudo labels used, the selection strategy to ﬁlter pseudo labeled
examples, the speed at which ft is replaced with ft+1, the choice of data augmentation strategy in
the teacher and student models, and the weighting of the two datasets in the objective (Berthelot
et al., 2019b;a; Xie et al., 2020; Sohn et al., 2020; Du et al., 2020)."
BACKGROUND ON SELF-TRAINING,0.06896551724137931,"An important design choice is the type of pseudo labels used. One can simply use soft class prob-
abilities predicted by a teacher ft (Du et al., 2020), sharpened class probabilities (Berthelot et al.,
2019b), or hard labels (a one-hot vector that is zero except at argmaxft(x)) (Lee et al., 2013). An-
other important consideration is the selection strategy to retain a subset of pseudo-labeled examples.
FixMatch (Sohn et al., 2020) uses a hyper-parameter τ to select examples on which the teacher
model has a certain level of conﬁdence, i.e.,
St = {(x, ft(x)) | x ∈U & max(ft(x)) ≥τ} .
(2)
NoisyStudent (Xie et al., 2020) also uses a form of conﬁdence ﬁltering but ensures that the class
labels in the selected subset are balanced. In principle, any method for out-of-distribution detec-
tion (Hendrycks & Gimpel, 2016) can be adopted for ﬁltering pseudo-labeled examples. We adopt
the simplest variant of self-training and limit hyper-parameter tuning to a bare minimum."
BACKGROUND ON SELF-TRAINING,0.07161803713527852,Under review as a conference paper at ICLR 2022
BACKGROUND ON SELF-TRAINING,0.07427055702917772,Open-domain
BACKGROUND ON SELF-TRAINING,0.07692307692307693,unlabeled data
BACKGROUND ON SELF-TRAINING,0.07957559681697612,Fine-tuning on in-domain
BACKGROUND ON SELF-TRAINING,0.08222811671087533,data without labels
BACKGROUND ON SELF-TRAINING,0.08488063660477453,Supervised
BACKGROUND ON SELF-TRAINING,0.08753315649867374,ﬁne-tuning
BACKGROUND ON SELF-TRAINING,0.09018567639257294,"Self-training 
or distillation"
BACKGROUND ON SELF-TRAINING,0.09283819628647215,Labeled data
BACKGROUND ON SELF-TRAINING,0.09549071618037135,BERT and
BACKGROUND ON SELF-TRAINING,0.09814323607427056,Friends
BACKGROUND ON SELF-TRAINING,0.10079575596816977,"Large LM
(e.g., GPT-2)"
BACKGROUND ON SELF-TRAINING,0.10344827586206896,"Self-supervised pretraining
(Masked language modelling)"
BACKGROUND ON SELF-TRAINING,0.10610079575596817,In-domain LM P(x)
BACKGROUND ON SELF-TRAINING,0.10875331564986737,Classiﬁer
BACKGROUND ON SELF-TRAINING,0.11140583554376658,P(y | x) MLE
BACKGROUND ON SELF-TRAINING,0.11405835543766578,Better or smaller
BACKGROUND ON SELF-TRAINING,0.11671087533156499,classiﬁer
BACKGROUND ON SELF-TRAINING,0.11936339522546419,"Synthetic
in-domain
unlabeled data"
BACKGROUND ON SELF-TRAINING,0.1220159151193634,"Figure 1: An illustration of GAL for NLP. We use open-domain data once for self-supervised pretraining
(e.g., BERT) and once for training a large LM (e.g., GPT-2). BERT is ﬁne-tuned on labeled data to yield a
classiﬁer for the task of interest. GPT-2 is ﬁne-tuned on the same data without labels to obtain an unconditional
task-speciﬁc LM, which is used to generate lots of synthetic in-domain unlabeled data for self-training and KD."
BACKGROUND ON SELF-TRAINING,0.1246684350132626,"4
GENERATE, ANNOTATE, AND LEARN (GAL)"
BACKGROUND ON SELF-TRAINING,0.1273209549071618,"Given a labeled dataset L = {(xi, yi)}N
i=1, we ﬁrst train an unconditional domain-speciﬁc gener-
ative model g(x) on Lx = {xi}N
i=1, and then use it to synthesize unlabeled data. Such synthetic
unlabeled data is used within self-training and KD even in the absence of in-domain unlabeled data.
We restrict our attention to basic KD and self-training methods, even though GAL can be combined
with more sophisticated semi-supervised techniques too."
BACKGROUND ON SELF-TRAINING,0.129973474801061,"The objective function of GAL for self-training during iteration t, provided a teacher model ft, is
expressed as:"
BACKGROUND ON SELF-TRAINING,0.13262599469496023,"ℓt+1
= λ E(x,y)∼LH(y, ft+1(x)) + (1 −λ) Eex∼g(x)H(ft(ex), ft+1(ex)) .
(3)"
BACKGROUND ON SELF-TRAINING,0.13527851458885942,"We use soft pseudo labels within self-training and KD. To improve computational efﬁciency of GAL,
we do not generate unlabeled data on the ﬂy, but generate as many unconditional samples as possible
and store them in a synthetic unlabeled dataset U. We simply set λ = 0.5, unless stated otherwise."
BACKGROUND ON SELF-TRAINING,0.13793103448275862,"Not surprisingly, the effectiveness of GAL depends on the ﬁdelity and diversity of synthetic exam-
ples. If we had access to the oracle generative process, we were able to obtain the best KD and SSL
results, as if we had access to real task-speciﬁc unlabeled data. Our preliminary experiments suggest
that large language models are particularly effective within the GAL framework. Hence, as shown in
Figure 1, to build the best domain-speciﬁc language model, we adopt a large large language model
pretrained on lots of open-domain text, and ﬁne-tune it on a given dataset’s inputs, i.e., Lx, ignoring
class labels. Both our theory and ablations conﬁrm that ignoring class labels is a good idea. Trans-
ferring the knowledge of large language models is particularly beneﬁcial a small input dataset Lx of
text is available (Hernandez et al., 2021). In what follows, we ﬁrst discuss practical considerations
around building domain-speciﬁc language models, and then link GAL to empirical and vicinal risk
minimization, to motivate the approach."
DOMAIN-SPECIFIC GENERATIVE MODELS OF TEXT AND TABLES,0.14058355437665782,"4.1
DOMAIN-SPECIFIC GENERATIVE MODELS OF TEXT AND TABLES"
DOMAIN-SPECIFIC GENERATIVE MODELS OF TEXT AND TABLES,0.14323607427055704,"Text. Many NLP tasks have a relatively small labeled dataset (Wang et al., 2019b;a). While self-
supervised pretraining, followed by supervised ﬁne-tuning (Devlin et al., 2019; Liu et al., 2019b;
Clark et al., 2020; Lewis et al., 2019) has become the prominent approach to NLP, previous work has
also investigated different data augmentation methods to increase the size of the training datasets. In
summary, existing approaches to data augmentation for NLP include lexicon replacement, sentence
retrieval, and round-trip machine translation (Wang & Yang, 2015; Yu et al., 2018; Kobayashi, 2018;
Wu et al., 2019; Lichtarge et al., 2019; Wei & Zou, 2019; Alberti et al., 2019; Du et al., 2020;
Shen et al., 2020). By contrast, we propose the use of unconditional autoregressive LMs for data
augmentation. This is simple, ﬂexible, and powerful."
DOMAIN-SPECIFIC GENERATIVE MODELS OF TEXT AND TABLES,0.14588859416445624,"We take a pretrained GPT-2 language model (Radford et al., 2019) and ﬁne-tune it separately on each
dataset of interest after removing class labels. We ﬁnd that training from scratch on these datasets"
DOMAIN-SPECIFIC GENERATIVE MODELS OF TEXT AND TABLES,0.14854111405835543,Under review as a conference paper at ICLR 2022
DOMAIN-SPECIFIC GENERATIVE MODELS OF TEXT AND TABLES,0.15119363395225463,"is hopeless, but the larger the pretrained GPT-2 variant, the better the validation perplexity scores
are. For tasks modeling a relationship between multiple sentences, we concatenate a separator token
“[SEP]” between consecutive sentences. Once a ﬁne-tuned GPT-2 model is obtained, we generate
task-speciﬁc synthetic data up to 40× larger than the original training sets. For some samples of
generated text for GLUE see Table C.1 to C.6. We believe using bigger LMs and larger synthetic
datasets will improve our results, but we are constrained by compute resources."
DOMAIN-SPECIFIC GENERATIVE MODELS OF TEXT AND TABLES,0.15384615384615385,"Tabular data. Features from tabular tasks are often in a well-structured format, i.e., each data point
comprises a ﬁxed number of attributes as in Table D.2. This property impedes the acquisition of
task-speciﬁc unlabeled data, i.e., most augmentation techniques such as round-trip translation and
retrieval are hardly applicable. To enable generative modeling on tabular data, we convert each
data point (i.e., row from the table) into a sentence by concatenating all of its attributes. This
reformatting enables the use of GPT-2 ﬁne-tuning similar to text, and surprisingly GPT-2 samples
are very effective."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.15649867374005305,"4.2
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.15915119363395225,"In supervised learning, one seeks to learn a mapping f that given an input x, predicts a reasonable
output y. To deﬁne the supervised learning problem formally, one assumes that input-output pairs
are drawn from a joint distribution P, i.e., (x, y) ∼P(x, y), and a loss function H(y, f(x)) is used
to assess the quality of a mapping f. This loss is used to deﬁne a notion of expected risk:"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.16180371352785147,"R(f) = EP (x,y)H(y, f(x)) .
(4)"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.16445623342175067,"In almost all practical applications P(x, y) is unknown. Hence, a labeled dataset of examples L =
{(xi, yi)}N
i=1 is used to approximate R(f) as"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.16710875331564987,bR(f) = 1 N XN
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.16976127320954906,"i=1 H(yi, f(xi)) .
(5)"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.1724137931034483,"This objective function is known as empirical risk, and learning f through minimizing bR(f) is
known as the empirical risk minimization principle (Vapnik, 1992). To compensate for the ﬁnite
sample size in (5), one typically combines bR(f) with a regularizer to improve generalization."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.17506631299734748,"Beyond empirical risk minimization. Empirical risk minimization (5) is motivated as a way to ap-
proximate P(x, y) through a set of Dirac delta functions on labeled examples: Pδ(x, y) = P"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.17771883289124668,"i δ(x=
xi, y=yi)/N. However, this approximation is far from perfect, hence one uses a heldout validation
set for early stopping and hyper parameter tuning."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.18037135278514588,"Vicinal risk minimization (Chapelle et al., 2001) approximates expected risk as EPν(x,y)H(y, f(x)),
using a vicinity distribution, e.g., ν(˜x, ˜y | x, y) = N(˜x −x, σ2)δ(˜y = y) to approximate P(x, y)
as
Pν(x, y) = 1 N XN"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.1830238726790451,"i=1 ν(˜x = x, ˜y = y | xi, yi) .
(6)"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.1856763925729443,"The goal is to increase the support of each labeled data point and improve the quality and robustness
of the risk function."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.1883289124668435,"Recent work on mixup regularization (Zhang et al., 2018) proposes an effective way to construct
another vicinity distribution by interpolating between two data points and their labels. Albeit its
simplicity, these smoothing techniques tend to improve matters."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.1909814323607427,"Generative models for risk minimization. One can factorize the joint distribution of input-output
pairs as P(x, y) = P(x)P(y | x). Accordingly, if one is able to learn a reasonable unconditional
generative model of x denoted g(x), then one can draw a pair (x, y) by ﬁrst drawing x ∼g(x) and
then using the current instance of ft to draw y ∼ft(x). Then, one can use ft and g to approximate
expected risk as
Rt(ft+1) = Ex∼g(x)Ey∼ft(x)H(y, ft+1(x)) .
(7)
The quality of this approximation highly depends on the quality of ft and g. If ft is far from an
optimal classiﬁer f ∗or g(x) is far from P(x), (7) yields a poor approximation."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.19363395225464192,"The expected risk in (7) smoothens the risk landscape in complex ways beyond simple Gaussian
smoothing and interpolation. This smoothing is applicable to any continuous, discrete, or structured
domain as long as expressive generative models of P(x) are available. That said, for almost all"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.1962864721485411,Under review as a conference paper at ICLR 2022
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.1989389920424403,"reasonable loss functions H (e.g., softmax cross entropy and squared error), (7) is minimized when
ft+1 = ft, which is not ideal, especially when ft is far from f ∗. On the other hand, empirical risk
(5) anchors the problem in real labeled examples that are provided as ground truth."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.20159151193633953,GAL aims to combine the beneﬁts of (5) and (7) via:
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.20424403183023873,"Rt(ft+1) =
λ
N XN"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.20689655172413793,"i=1 H(yi, ft+1(xi)) + (1 −λ)Ex∼g(x)Ey∼ft(x)H(y, ft+1(x))
(8)"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.20954907161803712,"In this formulation, if ft represents the minimizer of empirical risk (5), then ft+1 = ft is the
minimizer of (8) too. However, one does not seek the global minimizer of empirical risk, but rather
the best performance on heldout data. If ft is obtained by stochastic gradient descent on any risk
function, but early stopped according to empirical risk on a heldout set, then using such ft in (8)
to deﬁne Rt(ft+1) promotes the selection of a mapping ft+1 that minimizes empirical risk while
staying close to the best performing mapping so far (i.e., ft). This formulation motivates self-training
and GAL as regularizers in the functional space and explains why they can conceivably work."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.21220159151193635,"How about class-conditional generative models? One can also factorize the joint distribution
P(x, y) as P(y)P(x | y) and accordingly utilize a class-conditional generative model g(x | y) to
derive the following expected risk formulation:"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.21485411140583555,"R(f) = Ey∼P (y)Ex∼g(x|y)H(y, ft+1(x)) .
(9)"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.21750663129973474,"In this setting pseudo labeling is not needed as synthetic data is already labeled. One can show that
the optimal classiﬁer f ∗
g that minimizes (9) for cross entropy loss is given by,"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.22015915119363394,"f ∗
g (y | x) = g(x|y)P(y)
.X"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.22281167108753316,"y′ g(x|y′)P(y′) ,
(10)"
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.22546419098143236,"that is turning the class-conditional generative model into a classiﬁer by using the Bayes rule yields
the optimal solution."
AN EMPIRICAL RISK MINIMIZATION PERSPECTIVE,0.22811671087533156,"Provided that the accuracy of generative classiﬁers on natural image and text classiﬁcation is far
behind their discriminate counterparts (e.g., Ravuri & Vinyals, 2019), we think substituting (9)
into (8) is not a good idea. Essentially, by substituting (9) into the classiﬁcation objective, one
is regularizing f to remain close to f ∗
g , which is not an effective strategy if f ∗
g is not competi-
tive. This argument corroborates the evidence from our ablation studies and recent work showing
that using class-conditional generative models to augment supervised learning does not provide big
gains (Ravuri & Vinyals, 2019). That said, one can still use class-conditional generative models to
synthesize high-ﬁdelity samples. As long as these samples are treated as unlabeled examples and
annotated using a classiﬁer, e.g., ft, we believe this is a reasonable approach falling under GAL.
Our argument above only applies to the scenario that class-conditional generative models are used
to synthesize labeled examples."
EXPERIMENTS,0.23076923076923078,"5
EXPERIMENTS"
EXPERIMENTS,0.23342175066312998,"We asses the effectiveness of GAL on KD, self-training and few-shot learning for NLP. We also
present self-training results on tabular tasks. Appendix B shows the applicability of GAL to two
image classiﬁcation tasks as a proof of concept, but more advanced techniques such as Mixup (Zhang
et al., 2018) are needed to bridge the gap with the state-of-the-art."
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.23607427055702918,"5.1
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE"
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.23872679045092837,"We use the GLUE benchmark (Wang et al., 2019b) for our KD experiments; see Appendix D for
benchmark details and Appendix E for the details of synthetic text generation. Our synthetic unla-
beled dataset U includes 40× as many examples as the original dataset for each task in the GLUE
benchmark."
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.2413793103448276,"The goal of knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al., 2015) is to distill the
knowledge of a powerful teacher model into a compact student model with as little loss in perfor-
mance as possible. This can help with model compression (Jiao et al., 2019; Sun et al., 2019a) and
multi-task learning (Liu et al., 2019a; Clark et al., 2019). It is known that KD on fresh data, unseen
during training, performs better (Buciluˇa et al., 2006; Chen et al., 2020c) than KD on original train-
ing data. Accordingly, we investigate the effectiveness of knowledge distillation using generated
unlabeled data through GAL."
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.2440318302387268,Under review as a conference paper at ICLR 2022
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.246684350132626,"Table 1: GLUE test results for single 6-layer transformer models. GAL establishes a new state of the art
on KD for NLP. Baselines: BERT-Theseus (Xu et al., 2020), BERT-PKD (Sun et al., 2019a), tinyBERT (Jiao
et al., 2019) tinyRoBERTa (Rashid et al., 2021), DistilRoBERTa (Sanh et al., 2019), and DistilRoBERTa + KD
(standard KD) and DistilRoBERTa + RT (round-trip translation to generate unlabeled text). Accuracy scores
on MNLI-matched/MNLI-mismatched are reported for MNLI, Matthew‘s correlation is reported for CoLA,
F1/Accuracy scores are reported for QQP and MRPC, Pearson/Spearman correlations are reported for STS-B,
and Accuracy is reported for QNLI and RTE."
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.2493368700265252,"Model
MNLI
CoLA SST-2
MRPC
STS-B
QQP
QNLI RTE Avg"
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.2519893899204244,"Previous work:
BERT-Theseus
82.4/82.1
47.8
92.2
87.6/83.2 85.6/84.1 71.6/89.3
89.6
66.2 78.6
BERT-PKD
81.5/81.0
-
92.0
85.0/79.9
-
70.7/88.9
89.0
65.5
-
tinyBERT
84.6/83.2
51.1
93.1
87.3/82.6 85.0/83.7 71.6/89.1
90.4
70.0 79.8
tinyRoBERTa
86.2/85.6
58.6
95.1
91.2/88.1 88.5/88.4 73.0/89.7
92.4
76.6 83.5"
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.2546419098143236,"Our results:
DistilRoBERTa
83.8/83.4
55.9
93.2
87.4/83.1 87.5/87.5 71.7/89.1
90.6
73.3 81.2
DistilRoBERTa + KD
84.7/84.5
54.9
94.1
88.0/84.4 87.4/86.6 72.1/89.2
91.6
73.8 81.6
DistilRoBERTa + RT
86.1/86.1
53.0
94.6
91.0/87.8 89.2/88.8 73.1/89.9
92.4
76.9 82.7
DistilRoBERTa + GAL 87.4/86.5
60.0
95.3
91.9/89.2 90.0/89.6 73.3/90.0
92.7
81.8 84.8"
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.2572944297082228,"Table 2: RoBERTa base and GAL self-training results on GLUE dev sets, averaged across 5 independent runs."
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.259946949602122,"Model
MNLI CoLA SST-2 MRPC STS-B QQP QNLI RTE Avg"
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.2625994694960212,"RoBERTa base
87.7
63.6
94.8
90.1
90.8
91.5
92.6
78.8 86.2
+ GAL (iter 1)
87.9
65.1
95.3
91.7
91.4
91.8
93.1
81.4 87.2
+ GAL (iter 2)
88.0
65.2
95.3
92.2
91.5
91.7
93.2
82.4 87.4
+ GAL (iter 3)
87.9
65.5
95.3
92.2
91.7
91.7
93.2
82.0 87.4"
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.26525198938992045,"RoBERTa base + self-distillation
88.1
63.7
95.2
90.3
90.4
91.5
93.1
79.7 86.5"
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.26790450928381965,"We use the HuggingFace implementation (Wolf et al., 2020) for KD experiments and adopt a stan-
dard experimental setup consistent with previous work (Sun et al., 2019a; Xu et al., 2020). Following
Rashid et al. (2021), ﬁne-tuned RoBERTa-large (24-layer transformer) represents the teacher and a
DistilRoBERTa (6-layer transformer) (Sanh et al., 2019) is used as the student. We train the student
model on U and L, where U is annotated by an ensemble of 10 models, achieving an average score
of 87.9. We then mix U and L with a ratio of 1:4, which is equivalent to λ = 0.2. This ratio works
best on the dev set."
STATE-OF-THE-ART RESULTS ON KNOWLEDGE DISTILLATION ON GLUE,0.27055702917771884,"Table 1 shows the results of individual 6-layer transformers on the GLUE test set. All of the base-
lines use an identical student architecture. GAL achieves the best entry on the GLUE leaderboard,
marking a new state-of-the-art for KD on NLP. It outperforms strong KD baselines such as Dis-
tilRoBERTa (Sanh et al., 2019), BERT-PKD (Sun et al., 2019a), BERT-Theseus (Xu et al., 2020),
tinyBERT (Jiao et al., 2019) and tinyRoBERTa (Rashid et al., 2021). It also outperforms our own
DistilRoBERTa+KD baseline, which learns from soft labels produced by an identical RoBERTa-
large ensemble on the original labeled dataset. While the use of soft labels outperform the vanilla
ﬁne-tuned DistilRoBERTa model, it signiﬁcantly underperforms our KD+GAL baseline. We also
compare with round-trip translation (RT), a strong data-augmentation baseline (e.g., Yu et al., 2018;
Shleifer, 2019). We mirror the experimental setup of GAL and generate 40× unlabeled data using
German as the bridge language (English →German→English). The translations are generated via
the best model in WMT19 (Ng et al., 2019). Although DistilRoBERTa+RT is better than vanilla
DistilRoBERTa and KD variants, it still signiﬁcantly underperforms our approach."
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.27320954907161804,"5.2
NLP SELF-TRAINING EXPERIMENTS ON GLUE"
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.27586206896551724,"We ﬁne-tune pretrained RoBERTa models provided by fairseq (Ott et al., 2019) on each GLUE
task. Fine-tuned RoBERTa serves as the ﬁrst teacher model for self-training. Each student model
is initialized with the original pretrained RoBERTa and ﬁne-tuned with exactly the same hyper-
parameters as suggested by fairseq (Ott et al., 2019). We combine the labeled dataset L and the
synthetic dataset U with a ratio of 1:1, by oversampling labeled data. This corresponds to λ = 0.5
in Eq. (8)."
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.27851458885941643,"Table 2 shows that GAL provides an average improvement of +1.3% over RoBERTa-base. We see
consistent improvements with more GAL iterations, but performance saturates after three iterations."
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.28116710875331563,Under review as a conference paper at ICLR 2022
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.2838196286472148,"We further compare our approach with a self-distillation (Furlanello et al., 2018) baseline, in which
the teacher and student models use the same architecture and transfer knowledge via the original
labeled training set. Although self-distillation provides a slight improvement, the gains from GAL
are more signiﬁcant."
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.2864721485411141,"We delve deeper and combine GAL self-training with RoBERTa-large and report test results for
both single model and ensemble model in Table 3. We observe consistent gains coming from GAL
on RoBERTa-large. Our results underperform the latest and biggest LMs from the GLUE leader-
board, but we are optimistic that GAL can be effectively combined with enormous LMs to provide
additional gains."
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.2891246684350133,"Table 3: RoBERTa-large with GAL self-training and SoTA methods evaluated on GLUE test sets. The beneﬁt
of GAL on single models is larger than ensembles. It appears that self-training reduce the variance of models.
Baselines including much larger models: RoBERTa-large (Liu et al., 2019b), ELECTRA (Clark et al., 2020),
T5 (Raffel et al., 2020), ERNIE (Sun et al., 2019b), and DeBERTa (He et al., 2020)"
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.2917771883289125,"Model
MNLI
CoLA
SST-2
MRPC
STS-B
QQP
QNLI
RTE
Avg"
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.29442970822281167,"Individual Models (our implementation):
RoBERTa-large
90.1/89.7
63.8
96.1
91.2/88.3
90.9/90.7
72.5/89.6
94.5
85.9
86.5
RoBERTa-large + GAL
90.2/89.8
66.2
96.4
92.0/89.2
90.7/90.5
73.6/89.9
95.0
86.3
87.1"
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.29708222811671087,"Ensemble Models (our implementation):
RoBERTa-large
91.2/90.5
66.8
96.9
92.8/90.3
91.9/91.6
74.5/90.4
95.5
87.7
87.9
RoBERTa-large + GAL
91.0/90.7
67.9
97.1
93.1/90.8
91.6/91.4
74.5/90.4
95.8
88.2
88.2"
NLP SELF-TRAINING EXPERIMENTS ON GLUE,0.29973474801061006,"State-of-the-art:
RoBERTa-large
90.8/90.2
67.8
96.7
92.3/89.8
92.2/91.9
74.3/90.3
95.4
88.2
88.0
ELECTRA
91.3/90.8
71.7
97.1
93.1/90.7
92.9/92.5
75.6/90.8
95.8
89.8
89.2
T5
92.2/91.9
71.6
97.5
92.8/90.4
93.1/92.8
75.1/90.6
96.9
92.8
89.8
ERNIE
91.9/91.4
74.4
97.8
93.9/91.8
93.0/92.6
75.2/90.9
97.3
92.0
90.2
DeBERTa
91.9/91.6
71.5
97.5
94.0/92.0
92.9/92.6
76.2/90.8
99.2
93.2
90.3"
SELF-TRAINING ON TABULAR TASKS,0.30238726790450926,"5.3
SELF-TRAINING ON TABULAR TASKS"
SELF-TRAINING ON TABULAR TASKS,0.3050397877984085,"We assess the effectiveness of GAL self-training on four tabular tasks, namely connect-4 (Burton
& Kelly, 2006), Drug Review (Gr¨aßer et al., 2018), Drybean (Koklu & Ozkan, 2020) and Spam-
base (Dua & Graff, 2017). The details of these tasks can be found in Appendix D. We follow the
same protocol as GLUE tasks and generate 40× unlabeled data from a ﬁne-tuned GPT-2-large. Ta-
ble 4 shows that GAL achieves sizable gains on these tasks even though neither RoBERTa nor GPT-2
are optimized for tabular tasks. XGBoost (Chen & Guestrin, 2016), a strong supervised baseline for
tabular tasks underperforms RoBERTa+GAL on connect-4 and Drug Review. It is worth noting that
since inputs of Drug Review contain free form text, we convert them into numeric values through
the max-pooled representation of the last hidden states of RoBERTa base. XGBoost outperforms
RoBERTa on Drybean and Spambase, but it is important to note that these two datasets include
ﬂoating point attributes, which we simply tokenize using BPE (Sennrich et al., 2016) into subwords.
Nevertheless, GAL is capable of bridging the gap between transformers and XGBoost. We believe
GAL can be successfully combined with XGBoost, but we leave this to future work, since the XG-
Boost library does not easily accommodate soft labels."
SELF-TRAINING ON TABULAR TASKS,0.3076923076923077,"Table 4: RoBERTa-base and GAL results on four tabular datasets from the UCI repository. Accuracy is
reported for these datasets."
SELF-TRAINING ON TABULAR TASKS,0.3103448275862069,"Model
connect-4
Drug Review
Drybean
Spambase"
SELF-TRAINING ON TABULAR TASKS,0.3129973474801061,"XGBoost
86.0
80.1
92.1
96.7"
SELF-TRAINING ON TABULAR TASKS,0.3156498673740053,"RoBERTa base
85.0
84.6
85.0
87.7
+ GAL (iter 1)
87.0
85.7
85.8
89.0
+ GAL (iter 2)
87.5
85.8
86.0
88.8
+ GAL (iter 3)
87.3
85.6
85.9
89.3"
SELF-TRAINING ON TABULAR TASKS,0.3183023872679045,Under review as a conference paper at ICLR 2022
SELF-TRAINING ON TABULAR TASKS,0.3209549071618037,"Table 5: Few-shot learning results for GPT-J (6B) (Wang & Komatsuzaki, 2021) on four NLP datasets. Accu-
racy is reported for these datasets."
SELF-TRAINING ON TABULAR TASKS,0.32360742705570295,"Model
SST-2
PIQA
COPA
BoolQ
Avg"
-SHOT,0.32625994694960214,"4-shot
89.8
76.0
79.0
64.3
77.3
8-shot
91.3
76.2
79.0
66.2
78.2
16-shot
92.7
77.0
81.0
66.8
79.4"
-SHOT,0.32891246684350134,"4-shot + synthetic 12-shot (GAL)
91.5
76.7
80.0
65.9
78.5"
PROMPT-BASED FEW-SHOT EXPERIMENTS,0.33156498673740054,"5.4
PROMPT-BASED FEW-SHOT EXPERIMENTS"
PROMPT-BASED FEW-SHOT EXPERIMENTS,0.33421750663129973,"GPT3 (Brown et al., 2020) has introduced an optimization-free paradigm for few-shot learning for
NLP. Without updating the parameters, large LMs can correctly predict the labels of the inputs by
conditioning on a prompt, which consists of an instruction, a few labeled instances and a new unla-
beled input. We apply GAL to prompt-based few-shot learning. Speciﬁcally, we present k labeled
examples as a prompt to GPT-J (Wang & Komatsuzaki, 2021), an open-sourced re-implementation
of GPT-3-6B, and generate m synthetic examples, followed by the corresponding labels. Note that
to mitigate noisy outputs, the generation of each synthetic example only conditions on the original
k labeled examples. Finally, we concatenate the original k examples and m synthetic examples, and
conduct a (k + m)-shot learning experiment with GPT-J."
PROMPT-BASED FEW-SHOT EXPERIMENTS,0.33687002652519893,"Brown et al. (2020) studied a total of 51 few-shot learning tasks. Studying all of these tasks is
prohibitively expensive. Thus, we ﬁlter tasks by following these two steps. First, since generating
m synthetic examples for each test instance is computationally expensive, we exclude tasks that have
more than 5k test examples. Second, we ﬁlter tasks on which GPT-3-6B achieves a score lower than
65% (please refer to Table H.1 in Brown et al. (2020) for more details). After applying the ﬁltering
steps, we use four datasets: SST-2 (Wang et al., 2019b), PIQA (Bisk et al., 2020), COPA and
BoolQ (Wang et al., 2019a) as the testbed. We notice that in order to generate valid synthetic data,
GPT-J requires to see at least 4 labeled examples. In addition, at most 16 examples of BoolQ can be
fed into GPT-J without truncation. Thus, we set k and m to 4 and 12 respectively. As seen in Table 5,
GAL leads to an average improvement of 1.2% over 4-shot learning, and reduces the gap between
4-shot and 16-shot learning. We noticed that the quality of some generated examples is low. We
believe the performance of few-shot learning can be further improved with high-quality instances.
One solution is to generate many synthetic examples, and select a high-quality subset. Since each
test instance conditions on distinct labeled instances, one has to generate different synthetic instances
for each test example from GPT-J, which causes expensive computation. Due to such computational
constraints, we leave the investigation of data selection strategies to the future work."
ABLATING COMPONENTS OF GAL ON GLUE,0.3395225464190981,"5.5
ABLATING COMPONENTS OF GAL ON GLUE"
ABLATING COMPONENTS OF GAL ON GLUE,0.3421750663129973,"Table 6: GAL with various GPT-2
model sizes on GLUE dev sets. NA
indicates a vanilla RoBERTa base
model."
ABLATING COMPONENTS OF GAL ON GLUE,0.3448275862068966,"GPT-2
SST-2 RTE MRPC CoLA"
ABLATING COMPONENTS OF GAL ON GLUE,0.34748010610079577,"NA
94.8
78.8
90.1
63.6
small
95.5
81.3
90.9
63.9
medium
95.3
81.3
91.3
63.7
large
95.3
81.4
91.7
65.1"
ABLATING COMPONENTS OF GAL ON GLUE,0.35013262599469497,"We conduct an in-depth study of different components of GAL
on GLUE datasets. Unless stated otherwise, we use a RoBERTa-
base model with a combination of the original training data and
40× synthetic data for each experiment."
ABLATING COMPONENTS OF GAL ON GLUE,0.35278514588859416,"GPT-2 model size. Radford et al. (2019) present a few variants
of the GPT-2 model including GPT-2, GPT-2-medium, GPT-2-
large, and GPT-2-XL. Larger GPT-2 models yield better perplex-
ity scores and higher generation quality. We utilize these models
except GPT-2-XL within the GAL framework to study the impact
of the generative model’s quality on downstream task’s perfor-
mance. Table 6 shows that regardless of the GPT-2 model sizes, GAL consistently surpasses the
vanilla RoBERTa base. Moreover, SST-2 and RTE datasets are not sensitive to the capacity of the
GPT-2 model, but higher quality synthetic text improves the results on MRPC and CoLA datasets.
We leave investigation of GPT-2-XL and even larger LMs such as GPT-3 (Brown et al., 2020) to
future work."
ABLATING COMPONENTS OF GAL ON GLUE,0.35543766578249336,"Class-conditional synthetic data generation.
Previous work (Kumar et al., 2020b; Ravuri &
Vinyals, 2019) suggests that it is challenging to utilize synthetic data from class-conditional gener-"
ABLATING COMPONENTS OF GAL ON GLUE,0.35809018567639256,Under review as a conference paper at ICLR 2022
ABLATING COMPONENTS OF GAL ON GLUE,0.36074270557029176,"ative models to boost the accuracy of text and image classiﬁers. Our theory in Section 4.2 points to
the potential drawback of class-conditional synthetic data. We empirically study this phenomenon,
by ﬁne-tuning GPT-2 in a class-conditional manner. Table 7 shows that not only class-conditional
LMs underperform unconditional LMs in our GAL framework, but also they are much worse than
the baseline."
ABLATING COMPONENTS OF GAL ON GLUE,0.363395225464191,Table 7: Synthetic data from class-conditional LMs underperforms GAL and RoBERTa on GLUE dev sets.
ABLATING COMPONENTS OF GAL ON GLUE,0.3660477453580902,"Source of synthetic data
SST-2 RTE MRPC CoLA"
ABLATING COMPONENTS OF GAL ON GLUE,0.3687002652519894,"No synthetic data (baseline)
94.8
78.8
90.1
63.6
Class-conditional LM
92.9
74.4
86.0
58.4
Unconditional LM (GAL)
95.3
81.4
91.7
65.1"
CONCLUSION,0.3713527851458886,"6
CONCLUSION"
CONCLUSION,0.3740053050397878,"We present Generate, Annotate, and Learn (GAL): a framework for self-training and knowledge
distillation with generated unlabeled data. We motivate GAL from an expected risk minimization
perspective and demonstrate both theoretically and empirically that the use of unconditional genera-
tive models for synthetic data generation is more effective than class-conditional generative models,
previously used in the literature. GAL leverages advances in large pretrained language models to
help supervised learning and can have implications for learning from limited labeled data. GAL
works surprisingly well on NLP and tabular tasks, and helps improve knowledge distillation and
prompt-based few-shot learning. We hope that GAL will stimulate new research on the evaluation
and development of large language models."
REFERENCES,0.376657824933687,REFERENCES
REFERENCES,0.3793103448275862,"A. Agrawala. Learning with a probabilistic teacher. IEEE Transactions on Information Theory, 16
(4):373–379, 1970. doi: 10.1109/TIT.1970.1054472."
REFERENCES,0.3819628647214854,"Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. Synthetic qa cor-
pora generation with roundtrip consistency. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pp. 6168–6173, 2019."
REFERENCES,0.38461538461538464,"Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial
networks. arXiv:1711.04340, 2017."
REFERENCES,0.38726790450928383,"David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and
Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmenta-
tion anchoring. arXiv:1911.09785, 2019a."
REFERENCES,0.38992042440318303,"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Infor-
mation Processing Systems, pp. 5049–5059, 2019b."
REFERENCES,0.3925729442970822,"Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-
monsense in natural language. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 34, pp. 7432–7439, 2020."
REFERENCES,0.3952254641909814,"Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger Gunn, Alexander Ham-
mers, David Alexander Dickie, Maria Vald´es Hern´andez, Joanna Wardlaw, and Daniel Rueck-
ert.
Gan augmentation:
Augmenting training data using generative adversarial networks.
arXiv:1810.10863, 2018."
REFERENCES,0.3978779840848806,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity
natural image synthesis. In International Conference on Learning Representations, 2019."
REFERENCES,0.4005305039787798,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv:2005.14165, 2020."
REFERENCES,0.40318302387267907,Under review as a conference paper at ICLR 2022
REFERENCES,0.40583554376657827,"Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. Proceedings
of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 535–541, 2006."
REFERENCES,0.40848806366047746,"Ariel N Burton and Paul HJ Kelly. Performance prediction of paging workloads using lightweight
tracing. Future Generation Computer Systems, 22(7):784–793, 2006."
REFERENCES,0.41114058355437666,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems, volume 32, pp. 11192–11203. Curran Associates, Inc.,
2019a."
REFERENCES,0.41379310344827586,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data
improves adversarial robustness. arXiv:1905.13736, 2019b."
REFERENCES,0.41644562334217505,"Olivier Chapelle, Jason Weston, L´eon Bottou, and Vladimir Vapnik. Vicinal risk minimization.
Advances in neural information processing systems, 2001."
REFERENCES,0.41909814323607425,"Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-Supervised Learning. MIT, 2009."
REFERENCES,0.4217506631299735,"Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
Generative pretraining from pixels. ICML, 2020a."
REFERENCES,0.4244031830238727,"Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. Proceedings of the
22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785–794,
2016."
REFERENCES,0.4270557029177719,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. International conference on machine learning, pp.
1597–1607, 2020b."
REFERENCES,0.4297082228116711,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-
supervised models are strong semi-supervised learners. NeurIPS, 2020c."
REFERENCES,0.4323607427055703,"Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma.
Self-training avoids using spurious
features under domain shift. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, Decem-
ber 6-12, 2020, virtual, 2020d."
REFERENCES,0.4350132625994695,"Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc Le.
Bam! born-again multi-task networks for natural language understanding. Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 5931–5937, 2019."
REFERENCES,0.4376657824933687,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training
text encoders as discriminators rather than generators. International Conference on Learning
Representations, 2020."
REFERENCES,0.4403183023872679,"David B Cooper and John H Freeman. On the asymptotic improvement in the out-come of super-
vised learning provided by additional nonsupervised learning. IEEE Transactions on Computers,
1970."
REFERENCES,0.44297082228116713,"Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702–703, 2020."
REFERENCES,0.44562334217506633,"Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semi-
supervised learning that requires a bad gan. arXiv preprint arXiv:1705.09783, 2017."
REFERENCES,0.4482758620689655,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019."
REFERENCES,0.4509283819628647,Under review as a conference paper at ICLR 2022
REFERENCES,0.4535809018567639,"Brian Dolhansky, Russ Howes, Ben Pﬂaum, Nicole Baram, and Cristian Canton Ferrer. The deep-
fake detection challenge (dfdc) preview dataset. arXiv:1910.08854, 2019."
REFERENCES,0.4562334217506631,"Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Ves Stoy-
anov, and Alexis Conneau. Self-training improves pre-training for natural language understand-
ing. arXiv:2010.02194, 2020."
REFERENCES,0.4588859416445623,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.46153846153846156,"S Fralick. Learning to recognize patterns without a teacher. IEEE Transactions on Information
Theory, 1967."
REFERENCES,0.46419098143236076,"Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural networks. International Conference on Machine Learning, pp. 1607–1616,
2018."
REFERENCES,0.46684350132625996,"Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPoﬁ, Charles Foster, Laurence
Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric
Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot lan-
guage model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.
5371628."
REFERENCES,0.46949602122015915,"Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. arXiv:2012.15723, 2020."
REFERENCES,0.47214854111405835,"Felix Gr¨aßer, Surya Kallumadi, Hagen Malberg, and Sebastian Zaunseder. Aspect-based sentiment
analysis of drug reviews applying cross-domain and cross-data learning. Proceedings of the 2018
International Conference on Digital Health, pp. 121–125, 2018."
REFERENCES,0.47480106100795755,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.47745358090185674,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. arXiv:2006.03654, 2020."
REFERENCES,0.48010610079575594,"Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. ICLR, 2016."
REFERENCES,0.4827586206896552,"Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer,
2021."
REFERENCES,0.4854111405835544,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, pp. 6626–6637, 2017."
REFERENCES,0.4880636604774536,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distilling the knowledge in a neural network.
arXiv:1503.02531, 2015."
REFERENCES,0.4907161803713528,"Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding. arXiv:1909.10351, 2019."
REFERENCES,0.493368700265252,"Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. arXiv:1710.10196, 2017."
REFERENCES,0.4960212201591512,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adver-
sarial networks. Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 4401–4410, 2019."
REFERENCES,0.4986737400530504,"Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. Advances in neural information processing systems, pp.
3581–3589, 2014."
REFERENCES,0.5013262599469496,Under review as a conference paper at ICLR 2022
REFERENCES,0.5039787798408488,"Sosuke Kobayashi. Contextual augmentation: Data augmentation by words with paradigmatic rela-
tions. In Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp.
452–457, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:
10.18653/v1/N18-2072. URL https://www.aclweb.org/anthology/N18-2072."
REFERENCES,0.506631299734748,"Murat Koklu and Ilker Ali Ozkan. Multiclass classiﬁcation of dry beans using computer vision
and machine learning techniques. Computers and Electronics in Agriculture, 174:105507, 2020.
ISSN 0168-1699. doi: https://doi.org/10.1016/j.compag.2020.105507. URL https://www.
sciencedirect.com/science/article/pii/S0168169919311573."
REFERENCES,0.5092838196286472,Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.5119363395225465,"Ananya Kumar, Tengyu Ma, and Percy Liang.
Understanding self-training for gradual domain
adaptation. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Con-
ference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
5468–5479. PMLR, 13–18 Jul 2020a."
REFERENCES,0.5145888594164456,"Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data augmentation using pre-trained trans-
former models. arXiv:2003.02245, 2020b."
REFERENCES,0.5172413793103449,"Dong-Hyun Lee et al. Pseudo-label: The simple and efﬁcient semi-supervised learning method for
deep neural networks. Workshop on challenges in representation learning, ICML, 2013."
REFERENCES,0.519893899204244,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension. arXiv:1910.13461, 2019."
REFERENCES,0.5225464190981433,"Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam Shazeer, Niki Parmar, and Simon Tong. Cor-
pora generation for grammatical error correction. arXiv:1904.05780, 2019."
REFERENCES,0.5251989389920424,"Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neu-
ral networks via knowledge distillation for natural language understanding. arXiv:1904.09482,
2019a."
REFERENCES,0.5278514588859416,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv:1907.11692, 2019b."
REFERENCES,0.5305039787798409,"David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. Pro-
ceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp.
152–159, 2006."
REFERENCES,0.53315649867374,"Geoffrey J McLachlan and S Ganesalingam. Updating a discriminant function on the basis of un-
classiﬁed data. Communications in Statistics-Simulation and Computation, 1982."
REFERENCES,0.5358090185676393,"Hossein Mobahi, Mehrdad Farajtabar, and Peter L. Bartlett. Self-distillation ampliﬁes regularization
in hilbert space. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020."
REFERENCES,0.5384615384615384,"Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. Facebook fair’s
wmt19 news translation task submission. In Proceedings of the Fourth Conference on Machine
Translation (Volume 2: Shared Task Papers, Day 1), pp. 314–319, 2019."
REFERENCES,0.5411140583554377,"Sajad Norouzi, David J Fleet, and Mohammad Norouzi. Exemplar vaes for exemplar based genera-
tion and data augmentation. arXiv:2004.04795, 2020."
REFERENCES,0.5437665782493368,"Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint
arXiv:1606.01583, 2016."
REFERENCES,0.5464190981432361,"Avital Oliver, Augustus Odena, Colin Raffel, Ekin D Cubuk, and Ian J Goodfellow. Realistic evalu-
ation of deep semi-supervised learning algorithms. NeurIPS, 2018."
REFERENCES,0.5490716180371353,Under review as a conference paper at ICLR 2022
REFERENCES,0.5517241379310345,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics
(Demonstrations), pp. 48–53, 2019."
REFERENCES,0.5543766578249337,"Samet Oymak and Talha Cihad Gulcu.
Statistical and algorithmic insights for semi-supervised
learning with self-training. CoRR, abs/2006.11006, 2020. URL https://arxiv.org/abs/
2006.11006."
REFERENCES,0.5570291777188329,"Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, 2018."
REFERENCES,0.5596816976127321,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019."
REFERENCES,0.5623342175066313,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. Journal of Machine Learning Research, 21:1–67, 2020."
REFERENCES,0.5649867374005305,"Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. In Hal Daum´e III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 7909–7919. PMLR, 13–18 Jul 2020."
REFERENCES,0.5676392572944297,"Ahmad Rashid, Vasileios Lioutas, and Mehdi Rezagholizadeh. Mate-kd: Masked adversarial text, a
companion to knowledge distillation. arXiv preprint arXiv:2105.05912, 2021."
REFERENCES,0.5702917771883289,"Suman Ravuri and Oriol Vinyals. Classiﬁcation accuracy score for conditional generative models.
Advances in Neural Information Processing Systems, pp. 12268–12279, 2019."
REFERENCES,0.5729442970822282,"Ellen Riloff. Automatically generating extraction patterns from untagged text. Proceedings of the
national conference on artiﬁcial intelligence, pp. 1044–1049, 1996."
REFERENCES,0.5755968169761273,"Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object
detection models. Applications of Computer Vision and the IEEE Workshop on Motion and Video
Computing, IEEE Workshop on, 1:29–36, 2005."
REFERENCES,0.5782493368700266,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans.
Proceedings of the 30th International Conference on
Neural Information Processing Systems, pp. 2234–2242, 2016."
REFERENCES,0.5809018567639257,"Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108, 2019."
REFERENCES,0.583554376657825,"H Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions
on Information Theory, 1965."
REFERENCES,0.5862068965517241,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1715–1725, Berlin, Germany, August 2016. Association
for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://www.aclweb.
org/anthology/P16-1162."
REFERENCES,0.5888594164456233,"Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen. A simple but tough-
to-beat data augmentation approach for natural language understanding and generation. arXiv
preprint arXiv:2009.13818, 2020."
REFERENCES,0.5915119363395226,"Sam Shleifer.
Low resource text classiﬁcation with ulmﬁt and backtranslation.
arXiv preprint
arXiv:1903.09244, 2019."
REFERENCES,0.5941644562334217,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv:1409.1556, 2014."
REFERENCES,0.596816976127321,Under review as a conference paper at ICLR 2022
REFERENCES,0.5994694960212201,"Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning
with consistency and conﬁdence. arXiv:2001.07685, 2020."
REFERENCES,0.6021220159151194,"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Proceedings of the 33rd Annual Conference on Neural Information Processing Systems, 2019."
REFERENCES,0.6047745358090185,"Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu.
Patient knowledge distillation for bert model
compression. Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), pp. 4314–4323, 2019a."
REFERENCES,0.6074270557029178,"Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,
Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv
preprint arXiv:1904.09223, 2019b."
REFERENCES,0.610079575596817,"Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine Learn-
ing, 109(2):373–440, 2020."
REFERENCES,0.6127320954907162,"Vladimir Vapnik. Principles of risk minimization for learning theory. Advances in neural informa-
tion processing systems, 1992."
REFERENCES,0.6153846153846154,"Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R Bowman. SuperGLUE: A stickier benchmark for general-purpose language
understanding systems. arXiv:1905.00537, 2019a."
REFERENCES,0.6180371352785146,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. Inter-
national Conference on Learning Representations, 2019b."
REFERENCES,0.6206896551724138,"Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language
Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021."
REFERENCES,0.623342175066313,"William Yang Wang and Diyi Yang. That’s so annoying!!!: A lexical and frame-semantic embed-
ding based data augmentation approach to automatic categorization of annoying behaviors using#
petpeeve tweets. Proceedings of the 2015 conference on empirical methods in natural language
processing, pp. 2557–2563, 2015."
REFERENCES,0.6259946949602122,"Xiaojie Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. Kdgan: Knowledge distillation with genera-
tive adversarial networks. NeurIPS, 2018."
REFERENCES,0.6286472148541115,"Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training
with deep networks on unlabeled data. In International Conference on Learning Representations,
2021."
REFERENCES,0.6312997347480106,"Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text
classiﬁcation tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Process-
ing (EMNLP-IJCNLP), pp. 6383–6389, 2019."
REFERENCES,0.6339522546419099,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural
language processing.
Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations, pp. 38–45, October 2020. doi: 10.18653/v1/
2020.emnlp-demos.6."
REFERENCES,0.636604774535809,"Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and Songlin Hu. Conditional bert contextual
augmentation. International Conference on Computational Science, pp. 84–95, 2019."
REFERENCES,0.6392572944297082,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv:1708.07747, 2017."
REFERENCES,0.6419098143236074,Under review as a conference paper at ICLR 2022
REFERENCES,0.6445623342175066,"Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data
augmentation for consistency training. arXiv preprint arXiv:1904.12848, 2019."
REFERENCES,0.6472148541114059,"Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student
improves imagenet classiﬁcation. 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 10684–10695, 2020."
REFERENCES,0.649867374005305,"Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. Bert-of-theseus: Compressing
bert by progressive module replacing. Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 7859–7869, 2020."
REFERENCES,0.6525198938992043,"I Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-
supervised learning for image classiﬁcation. arXiv:1905.00546, 2019."
REFERENCES,0.6551724137931034,"Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping
Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. G-daug: Generative data augmen-
tation for commonsense reasoning. arXiv:2004.11546, 2020."
REFERENCES,0.6578249336870027,"David Yarowsky.
Unsupervised word sense disambiguation rivaling supervised methods.
33rd
annual meeting of the association for computational linguistics, pp. 189–196, 1995."
REFERENCES,0.6604774535809018,"Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,
and Quoc V Le. QANet: Combining local convolution with global self-attention for reading
comprehension. ICLR, 2018."
REFERENCES,0.6631299734748011,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock Richard
C. Wilson and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference
(BMVC), pp. 87.1–87.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.
30.87."
REFERENCES,0.6657824933687002,"Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. Defending against neural fake news. Advances in Neural Information Processing
Systems, 32:9054–9065, 2019."
REFERENCES,0.6684350132625995,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. ICLR, 2018."
REFERENCES,0.6710875331564987,"Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma.
Be
your own teacher: Improve the performance of convolutional neural networks via self distillation.
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3713–3722,
2019a."
REFERENCES,0.6737400530503979,"Xiaofeng Zhang, Zhangyang Wang, Dong Liu, and Qing Ling. Dada: Deep adversarial data aug-
mentation for extremely low data regime classiﬁcation. ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2807–2811, 2019b."
REFERENCES,0.6763925729442971,"Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le.
Rethinking pre-training and self-training. Advances in Neural Information Processing Systems,
33, 2020."
REFERENCES,0.6790450928381963,Under review as a conference paper at ICLR 2022
REFERENCES,0.6816976127320955,"A
PSEUDO-CODE OF ALGORITHMS"
REFERENCES,0.6843501326259946,"Algorithm 1 SelfTraining(L, U, f0, T)"
REFERENCES,0.6870026525198939,"Input: Labeled dataset L = {(xi, yi)}N
i=1
Unlabeled dataset U = {xj}M
j=1
Initial parameters of a classiﬁer f0
Output: A better classiﬁer fT +1 after T self-training steps"
REFERENCES,0.6896551724137931,"1: train a base model f1 by ﬁne-tuning f0 on L
2: for t = 1 to T do:
3:
apply ft to unlabeled instances of U
4:
select a subset St ⊆{(x, ft(x)) | x ∈U}
5:
train a new model ft+1 by either ﬁne-tuning f0 on L ∪St
or gradient descend on a minibatch from L ∪St
6: return fT +1"
REFERENCES,0.6923076923076923,"Algorithm 2 GAL(L, g0, f0, k, T)"
REFERENCES,0.6949602122015915,"Input: Labeled dataset L = {(xi, yi)}N
i=1
Initial parameters of a generative model g0
Initial parameters of a classiﬁer f0
Output: A better classiﬁer fT +1 after T GAL steps"
REFERENCES,0.6976127320954907,"1: train a generative model g by ﬁne-tuning g0 on Lx where Lx = {x | (x, y) ∈L}
2: generate U = {exj}kN
j=1 by drawing kN random samples i.i.d. from g(x), i.e., exj ∼g(x) for j = 1 to
kN.
3: return SelfTraining(L, U, f0, T)"
REFERENCES,0.7002652519893899,"B
GAL ON IMAGE CLASSIFICATION TASKS"
REFERENCES,0.7029177718832891,"As a proof of concept, in addition to NLP and tabular tasks, we assess the effectiveness of GAL
on CIFAR-10 (Krizhevsky & Hinton, 2009) and Fashion MNIST (Xiao et al., 2017) as well. We
adopt the NCSN model of Song & Ermon (2019) as the task-speciﬁc generative model. We use
the CIFAR-10 model provided by the authors and train a model on Fashion MNIST using the same
conﬁguration as CIFAR-10. We select the model checkpoint resulting in the best FID score vs.
training set (Heusel et al., 2017) based on 1000 samples. We then use the NCSN models to generate
up to 10× synthetic unlabeled data, i.e., 500K for CIFAR-10 and 600K for Fashion MNIST. See
Appendix C for representative samples."
REFERENCES,0.7055702917771883,"Table B.1: Classiﬁcation error rates on CIFAR-
10 test set with varying amounts of synthetic
data for three different model architectures. Re-
ported results are the average of 3 independent
runs."
REFERENCES,0.7082228116710876,"Model
VGG19 ResNet110 WRN28-10
# params
1.74M
20.11M
36.48M"
REFERENCES,0.7108753315649867,"Baseline
6.62
5.85
3.87"
REFERENCES,0.713527851458886,"GAL 1×
5.97
5.13
3.75
GAL 5×
5.80
5.11
3.25
GAL 10×
5.65
5.10
3.23"
REFERENCES,0.7161803713527851,"We adopt FixMatch (Sohn et al., 2020) to conduct
semi-supervised learning on vision tasks, since Fix-
Match has shown promising results on CIFAR-10.
Speciﬁcally, we train a classiﬁer on mini-batches of
intertwined labeled and unlabeled data (synthetic). In
each iteration, we obtain pseudo-labels for the unla-
beled data, but ﬁlter unlabeled examples based on clas-
siﬁer’s conﬁdence, i.e., examples are kept on which the
largest class probability exceeds τ. Weak augmenta-
tion is used to deﬁne pseudo labels, but strong augmen-
tations are used to obtain student model’s predictions.
We randomly sample from the strong augmentations
list deﬁned in RandAugment (Cubuk et al., 2020). We
only apply strong augmentations to the synthetic sam-
ples and not the original labeled data to ensure a fair comparison with the baseline."
REFERENCES,0.7188328912466844,"We conduct experiments on three different convolutional neural network architectures: VGG19 (Si-
monyan & Zisserman, 2014),
WideResnet28-10 (Zagoruyko & Komodakis, 2016),
and
ResNet110 (He et al., 2016). For the full list of hyperparameters and other implementation de-
tails, please refer to Appendix H. Each classiﬁer is trained for 200 epochs and 3 synthetic datasets
of size (1×, 5×, 10×) of the training dataset are used."
REFERENCES,0.7214854111405835,Under review as a conference paper at ICLR 2022
REFERENCES,0.7241379310344828,"Table B.1 shows that GAL achieves an average error reduction of 0.78% over the baseline on CIFAR-
10 across the 3 architectures tested. Further, it appears that the larger the synthetic dataset size, the
better the performance of GAL is. We note that the reported results are the average of 3 independent
runs. Similarly on Fashion MNIST, we witness consistent gains across all architectures. Fashion
MNIST results are included in Appendix I. Our image classiﬁcation experiments conﬁrm that even
when the generative model of GAL is not pretrained on open domain data and solely trained on the
dataset at hand, GAL can offer signiﬁcant improvements."
REFERENCES,0.726790450928382,"Table B.2 presents GAL results on Fashion MNIST dataset. Similar to CIFAR-10, we observe a
performance improvement across the three architectures."
REFERENCES,0.7294429708222812,"Table B.2: Classiﬁcation error rates on Fashion MNIST test set with varying amounts of synthetic data for
three different model architectures. Results reported are the average over 3 independent runs."
REFERENCES,0.7320954907161804,"Model
VGG19
WRN28-2
ResNet110
# params
1.74M
1.98M
20.11M"
REFERENCES,0.7347480106100795,"Baseline
5.41
4.92
5.21"
REFERENCES,0.7374005305039788,"GAL 1×
5.06
4.63
4.74
GAL 5×
5.14
4.85
4.85
GAL 10×
4.90
4.74
4.75"
REFERENCES,0.7400530503978779,Under review as a conference paper at ICLR 2022
REFERENCES,0.7427055702917772,"C
GENERATED UNLABELED EXAMPLES ANNOTATED WITH PSEUDO LABELS truck ship horse frog dog deer cat bird car"
REFERENCES,0.7453580901856764,airplane
REFERENCES,0.7480106100795756,"Figure C.1: CIFAR-10 synthetic samples generated by NCSN (Song & Ermon, 2019) and corresponding
pseudo-labels. Images are ﬁltered based on a conﬁdence threshold of τ = 0.95 and categorized based on
pseudo-labels. For each category, 16 random samples are shown."
REFERENCES,0.7506631299734748,Under review as a conference paper at ICLR 2022 boot bag
REFERENCES,0.753315649867374,sneaker shirt
REFERENCES,0.7559681697612732,sandal coat dress
REFERENCES,0.7586206896551724,pullover
REFERENCES,0.7612732095490716,trouser
REFERENCES,0.7639257294429708,t-shirt
REFERENCES,0.76657824933687,"Figure C.2: Fashion MNIST synthetic samples generated by NCSN (Song & Ermon, 2019) and pseudo-labels.
Images are ﬁltered based on a conﬁdence threshold of τ = 0.95 and categorized based on pseudo-labels. For
each category, 16 random samples are shown."
REFERENCES,0.7692307692307693,Under review as a conference paper at ICLR 2022
REFERENCES,0.7718832891246684,"Table C.1: QNLI: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)
from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examples
in parenthesis."
REFERENCES,0.7745358090185677,"When did the third Digimon series begin? [SEP] Unlike the two seasons before it and most
of the seasons that followed, Digimon Tamers takes a darker and more realistic approach
to its story featuring Digimon who do not reincarnate after their deaths and more complex
character development in the original Japanese. (not entailment)"
REFERENCES,0.7771883289124668,"KNN:
1: What is the name of the third season? [SEP] In addition to the ﬁrst two seasons, the third
season is the season that introduced new characters such as Captain Malice, a supervillain
who became the antagonist in season two; and the villains known as the Heartbreakers,
who introduced a group of crime ﬁghters. (not entailment)
2: When did the ”Walking Dead” series end? [SEP] In 2013, AMC announced that it
would develop a ”superhero series”, which would follow the storylines and characters
from the ”Walking Dead” series in order to bring the popular AMC original series to a
new and younger audience. (not entailment)
3: What is the main objective of the ﬁrst season of the X-Files? [SEP] The ﬁrst season was
notable in that the characters were introduced and developed within the space of a single
season, as was the format of the show itself. (not entailment)"
REFERENCES,0.7798408488063661,"What did Arsenal consider the yellow and blue colors to be after losing a FA Cup ﬁnal
wearing red and white? [SEP] Arsenal then competed in three consecutive FA Cup ﬁnals
between 1978 and 1980 wearing their ”lucky” yellow and blue strip, which remained the
club’s away strip until the release of a green and navy away kit in 1982–83. (entailment)"
REFERENCES,0.7824933687002652,"KNN:
1: Who was the most important player for Arsenal Football Club in the 1950s? [SEP]
Wenger continued to use Arsenal’s famous red shirts and red kits throughout the 1950s
and 1960s, and the red strip became the club’s most recognised and recognizable symbol.
(not entailment)
2: When were the ﬁrst two teams to play for the trophy in the Premier League? [SEP]
The trophy was awarded to Manchester United in 1990-91 and was named after Sir Bobby
Charlton, the club’s manager until 1990, and later Sir Stanley Matthews, the club’s most
successful manager. (not entailment)
3: What were the last four players to wear the yellow in the ﬁnal? [SEP] With Arsenal
having won all four major trophies in the period, they became the only club to have won
ﬁve in a row. (not entailment)"
REFERENCES,0.7851458885941645,"Table C.2: QQP: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)
from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examples
in parenthesis."
REFERENCES,0.7877984084880637,"How is the life of a math student? Could you describe your own experiences? [SEP] Which level of
prepration is enough for the exam jlpt5? (not duplicated)"
REFERENCES,0.7904509283819628,"KNN:
1: What are the best courses for a mechanical engineering student? [SEP] What is the best course
to do after completing a B.Tech in mechanical engineering? (not duplicated)
2: How much marks are needed to get through the GATE with electronics? [SEP] What is the
average score of the Gate EE exam? What are the cut-offs? (not duplicated)
3: What is the best time table for students to prepare for IAS? [SEP] How can one study for IAS in
a best time? (not duplicated)"
REFERENCES,0.7931034482758621,"How does an IQ test work and what is determined from an IQ test? [SEP] How does IQ test works?
(duplicated)"
REFERENCES,0.7957559681697612,"KNN:
1: What is the average IQ of the U.S. population? [SEP] How does an IQ test work? (not dupli-
cated)
2: Is the Iq test an effective way to measure intelligence? [SEP] How do IQ tests work? (duplicated)
3: How is an IQ test on a scale from 1 to 100 scored? [SEP] How do you get your IQ tested? (not
duplicated)"
REFERENCES,0.7984084880636605,Under review as a conference paper at ICLR 2022
REFERENCES,0.8010610079575596,"Table C.3: RTE: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)
from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examples
in parenthesis."
REFERENCES,0.8037135278514589,"Like the United States, U.N. ofﬁcials are also dismayed that Aristide killed a conference
called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the
feuding parties together. [SEP] Aristide had Prime Minister Robert Malval murdered in
Port-au-Prince. (not entailment)"
REFERENCES,0.8063660477453581,"KNN:
1: The government has been criticized for failing to prevent the mass protests that led to
the ouster of President Nicolas Sarkozy earlier this month, which led to his second election
defeat since assuming ofﬁce two years ago. [SEP] Prime Minister Jean-Marc Ayrault is a
former president of France. (not entailment)
2: The French president, Jacques Chirac, has been urged by both the Vatican and the U.N.
Security Council to step up efforts to prevent the return of former dictator Nicolas Sarkozy.
[SEP] Nicolas Sarkozy left France. (not entailment)
3: The French newspaper Le Monde says the French President Nicolas Sarkozy was ad-
vised by U.S. President George W. Bush about a possible trip to Iraq on Thursday. [SEP]
Nicolas Sarkozy is a member of the United States. (not entailment)"
REFERENCES,0.8090185676392573,"Only a week after it had no comment on upping the storage capacity of its Hotmail e-mail
service, Microsoft early Thursday announced it was boosting the allowance to 250MB
to follow similar moves by rivals such as Google, Yahoo, and Lycos. [SEP] Microsoft’s
Hotmail has raised its storage capacity to 250MB. (entailment)"
REFERENCES,0.8116710875331565,"KNN:
1: The company, known as Microsoft Ofﬁce, said it plans to sell all of the copies of its
popular Ofﬁce suite at a loss in the wake of the launch of Microsoft Windows 7, saying
it will also make $25 million in advertising costs, a move likely to hurt its long-standing
position among consumers and business leaders. [SEP] Microsoft Ofﬁce is a popular ofﬁce
suite. (entailment)
2: The company’s shares shot up more than 35% after the company said it has sold all
of its remaining inventory of the new Kindle e-readers at $70 each. The shares rose to
$65.20 on Wednesday, their highest since March 6, 2011. “The Kindle is our best selling
product,” said Jeff Bezos, founder and CEO of Amazon.com. [SEP] Amazon.com is based
in Seattle. (not entailment)
3: In response to concerns expressed by some investors, Microsoft last week said it would
reduce the amount of shares that will be available to the public by 10 percent in the ﬁrst
quarter, with a further reduction to 3 percent in the second quarter. The stock price has
plunged from $24 to $17, and Microsoft is currently offering $17 to $19 a share to its
most senior employees. Some investors had criticized Microsoft’s response to concerns
about the price of its stock and about the perception that the company is in trouble. [SEP]
Microsoft is struggling to sell its stock. (not entailment)"
REFERENCES,0.8143236074270557,Under review as a conference paper at ICLR 2022
REFERENCES,0.8169761273209549,"Table C.4: MRPC: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)
from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examples
in parenthesis."
REFERENCES,0.8196286472148541,"A BMI of 25 or above is considered overweight ; 30 or above is considered obese . [SEP]
A BMI between 18.5 and 24.9 is considered normal , over 25 is considered overweight
and 30 or greater is deﬁned as obese . (paraphrase)"
REFERENCES,0.8222811671087533,"KNN:
1: The report said that the average woman in her twenties who takes oral contraceptives
daily can expect a loss of around 40 per cent of her bone density between the ages of 20
and 45 . [SEP] The study said the average woman in her twenties who used the pill every
day , or every day for up to ﬁve years , can expect a loss of about 40 per cent of her bone
density between the ages of 20 and 45 . (paraphrase)
2: The report found that 17 percent of U.S. adults between ages 18 and 64 have a body
mass index at or above the ” normal ” 20 . [SEP] For people of that age , 17.1 percent of
adults have a body mass index at or above the ” normal ” 20 , while 12.6 percent have a
body mass index of 30 or above . (not paraphrase)
3: The survey shows the proportion of women between 20 and 44 who were obese was
6.3 percent , up from 5.7 percent in 2001 . [SEP] The proportion of women between 20
and 44 who were obese increased to 6.3 percent from 5.7 percent in 2001 . (paraphrase)"
REFERENCES,0.8249336870026526,"Shares of Genentech , a much larger company with several products on the market , rose
more than 2 percent . [SEP] Shares of Xoma fell 16 percent in early trade , while shares
of Genentech , a much larger company with several products on the market , were up 2
percent .(not paraphrase)"
REFERENCES,0.8275862068965517,"KNN:
1: Shares in Aventura fell as much as 5 percent , while shares in Medi-Cal climbed 2.5
percent . [SEP] Shares in Aventura were up 2.5 percent , while shares in Medi-Cal rose
2.5 percent . (paraphrase)
2: Shares of Amgen rose $ 2.29 , or 2.2 percent , to $ 41.10 in after-hours trading . [SEP]
Shares of Amgen , a division of Sanoﬁ-Aventis , rose $ 1.62 , or 1.6 percent , to $ 41.06
in after-hours trading .(paraphrase)
3: Shares of General Electric Co . GE.N rose more than 6 percent on the New York Stock
Exchange , while shares of PepsiCo Inc . PEP.N rose 4.7 percent . [SPE] General Electric
’s shares jumped almost 6 percent on the New York Stock Exchange , while PepsiCo ’s
climbed 4.7 percent . (paraphrase)"
REFERENCES,0.830238726790451,Under review as a conference paper at ICLR 2022
REFERENCES,0.8328912466843501,"Table C.5: MNLI: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)
from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examples
in parenthesis."
REFERENCES,0.8355437665782494,"One of our number will carry out your instructions minutely. [SEP] A member of my team
will execute your orders with immense precision. (entailment)"
REFERENCES,0.8381962864721485,"KNN:
1: We are at your disposal to help you with your investigation and provide a full range of
pro bono services. [SEP] We are the only ones who can help you with your investigation.
(neutral)
2: I will speak with the chief ofﬁcer of the contractor, who will be informed about the
results of this effort. [SEP] The contractor is being informed about the results of the
effort. (entailment)
3: We have an ofﬁce here to assist you. [SEP] An ofﬁce is where we will assist you, said
the manager. (neutral)"
REFERENCES,0.8408488063660478,"Conceptually cream skimming has two basic dimensions - product and geography. [SEP]
Product and geography are what make cream skimming work. (neutral)"
REFERENCES,0.843501326259947,"KNN:
1: There are two main types of analysis and they are the case study and the case report.
[SEP] The case study is the most popular method used to analyze a subject. (neutral)
2: A third approach to capturing and using this type of experience is to engage the program
management and ﬁnance systems of the organization. [SEP] There are two strategies to
capturing and using experience. (contradiction)
3: The ﬁrst is to see the basic elements of a business model in action. [SEP] Basic elements
of business models are the most important for the success of any company. (neutral)"
REFERENCES,0.8461538461538461,"I don’t mean to be glib about your concerns, but if I were you, I might be more concerned
about the near-term rate implications of this $1. [SEP] I am concerned more about your
issues than the near-term rate implications. (contradiction)"
REFERENCES,0.8488063660477454,"KNN:
1: I’m not here to tell you of my own experiences, but they are important to others who
might have similar concerns. [SEP] If you were to have similar concerns, I’d like to
encourage you to tell them to me. (neutral)
2: I don’t mean to sound judgmental, but as a person, I think that’s an issue you’re probably
pretty much on your own if you think about it. [SEP] You’re probably right if you think
about it. (neutral)
3: But I don’t mean to take your word for it. [SEP] I know you are correct, but I want to
make sure it’s clear that I do not agree. (contradiction)"
REFERENCES,0.8514588859416445,"Table C.6: SST-2: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)
from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examples
in parenthesis."
REFERENCES,0.8541114058355438,are more deeply thought through than in most ‘ right-thinking ’ ﬁlms (positive)
REFERENCES,0.8567639257294429,"KNN:
1: is far more sophisticated , insightful and thought-provoking than his previous ﬁlms .
(positive)
2:
is more sophisticated than its more obvious and less-than-dazzling counterparts
(positive)
3: is about as well-thought as the idea of a bad hair day , (negative)"
REFERENCES,0.8594164456233422,"contains no wit , only labored gags (negative)"
REFERENCES,0.8620689655172413,"KNN:
1: lacks insight , and lacks empathy (negative)
2: has little humor or intelligence (negative)
3: lacks all wit and humanity (negative)"
REFERENCES,0.8647214854111406,Under review as a conference paper at ICLR 2022
REFERENCES,0.8673740053050398,"D
DATASETS"
REFERENCES,0.870026525198939,"Table D.1: Summary of the three sets of tasks used for evaluation of GAL. STS-B is a regression task, so
#classes is not applicable."
REFERENCES,0.8726790450928382,"Dataset
task
domain
#train
#dev
#test
#classes"
REFERENCES,0.8753315649867374,"NLP - GLUE Benchmark:
SST-2
sentiment analysis
movie reviews
67k
872
1.8k
2
QQP
paraphrase
social QA questions
364k
40k
391k
2
QNLI
QA/natural language inference
Wikipedia
105k
5k
5.4k
2
RTE
natural language inference
news, Wikipedia
2.5k
277
3k
2
MNLI
natural language inference
misc.
393k
20k
20k
3
MRPC
paraphrase
news
3.7k
408
1.7k
2
CoLA
acceptability
misc.
8.5k
1043
1k
2
STS-B
sentence similarity
misc.
5.8k
15k
1.4k
−"
REFERENCES,0.8779840848806366,"Tabular Data - UCI:
connect-4
utility value
gaming
54k
6.8k
6.8k
3
Drug Review
sentiment analysis
medical
2.6k
0.5k
1k
3
Drybean
categorical classiﬁcation
grain
10.9k
1.4k
1.4k
7
Spambase
spam classiﬁcation
e-mail
3.7k
0.5k
0.5k
2"
REFERENCES,0.8806366047745358,"Computer Vision:
CIFAR-10
image classiﬁcation
real images
50K
N/A
10K
10
Fashion MNIST
image classiﬁcation
clothing - grey scale
60K
N/A
10K
10"
REFERENCES,0.883289124668435,Table D.2: 3 examples of input and labels for the Drybean tabular task.
REFERENCES,0.8859416445623343,"Attributes
Label"
REFERENCES,0.8885941644562334,"1
37316 , 718.059 , ... , 0.6738775377027459 , 0.9981482213213235
SIRA
2
50634 , 892.3919999999999 , ... , 0.48054883366111584 , 0.9942734696473365
HOROZ
3
33631 , 669.076 , ... , 0.8466656241160356 , 0.9981796305487345
SEKER"
REFERENCES,0.8912466843501327,"E
GENERATING SYNTHETIC TEXT FOR GLUE"
REFERENCES,0.8938992042440318,"To generate domain-speciﬁc synthetic data, we ﬁne-tune GPT-2-large on the training set of each
downstream task, excluding labels. For tasks with multiple input sentences, we concatenate input
sentences into a long sequences and separate sentences by special [SEP] tokens. We generate new
domain-speciﬁc data by using top-k random sampling similar to Radford et al. (2019). We do
not feed any prompt to the LM, but a special [BOS] token to initiate the generation chain. A
generation episode is terminated when a special [EOS] token is produced. We generate diverse
sentences by varying the random seed. After collecting enough synthetic data, we only retain unique
sentences. For tasks with α input sentences, we discard generated samples that violate this constraint
(approximately 10% of samples were rejected)."
REFERENCES,0.896551724137931,"Quality of synthetic dataset. An effective generative model of text should learn the word prefer-
ences and genre associated with a given corpus, but still produce novel sentences. In order to study
the characteristics of our synthetic datasets, Table E.1 reports the number of unique n-grams in the
training and synthetic datasets, as well as the number of unique n-grams shared between them. The
high degree of overlap on uni-grams suggests that the ﬁne-tuned GPT-2-large is somewhat domain-
speciﬁc. Meanwhile, the large number of unique n-grams in the synthetic dataset suggests that many
novel word combinations are generated, which is helpful for GAL."
REFERENCES,0.8992042440318302,"Table E.1: For each dataset we report the number of unique n-grams in (the original dataset, the synthetic
dataset, shared between the two)."
REFERENCES,0.9018567639257294,"SST-2
QNLI
RTE
MRPC
CoLA"
-GRAM,0.9045092838196287,"1-gram
(15k, 33k, 11k)
(89k, 231k, 55k)
(18k, 34k, 13k)
(15k, 27k, 10k)
(6k, 6k, 4k)
3-gram
(107k, 2M, 38k)
(2M, 10M, 513k)
(120k, 750k, 30k)
(105k, 562k, 27k)
(39k, 198k, 14k)
5-gram
(109k, 4M, 9k)
(2M, 25M, 146k)
(130k, 1M, 4k)
(120k, 1M, 7k)
(35k, 389k, 5k)"
-GRAM,0.9071618037135278,Under review as a conference paper at ICLR 2022
-GRAM,0.9098143236074271,"F
IMPORTANCE OF PSEUDO-LABELS"
-GRAM,0.9124668435013262,"We have argued and demonstrated that using class-conditional generative models to generate labeled
synthetic examples is less effective than GAL in section 4 and section 5. To further verify this
argument, we sample 100 instances from the synthetic RTE dataset generated by a class-conditional
LM. Then we annotate these examples using a human annotator and the RoBERTa classiﬁer. Finally,
we compute the Accuracy, F1, Precision and Recall scores between human labels and RoBERTa
labels, and between human labels and conditioning labels (i.e., labels that the class-conditional
LM conditions on.). Table F.1 shows that class-conditional LM has difﬁculty generating sentences
retaining the semantics or pragmatics of a speciﬁed category, which also corroborates our theoretical
analysis in section 4. On the other hand, RoBERTa is able to produce higher quality labels that
correlate better with human annotations."
-GRAM,0.9151193633952255,"Table F.1: Performance of RoBERTa annotation and conditioning labels on 100 random examples from the
synthetic RTE dataset generated by a class-conditional LM."
-GRAM,0.9177718832891246,"Label type
Accuracy
F1
Precision Recall"
-GRAM,0.9204244031830239,"RoBERTa
90.0
91.4
100.0
84.1
conditioning label
72.0
71.4
66.0
77.8"
-GRAM,0.9230769230769231,"G
GPT-2 FOR CLASSIFICATION"
-GRAM,0.9257294429708223,"We have conducted additional experiments, where we ﬁne-tune GPT-2 as a classiﬁer. We have
considered two variants of the GPT-2 model. The ﬁrst varant is the original GPT-2 model (GPT2-
original) pre-trained on open-domain text. The second variant is the GPT-2 model that was ﬁne-
tuned on the inputs of each task separately (GPT-2-ﬁnetuned). This model was used to generate
task-speciﬁc (synthetic) unlabeled data. Finally, we also consider self-training with GAL on top
of GPT2-original. Speciﬁcally, we use the GPT-2-ﬁnetuned model to synthesize 40x in-domain
unlabeled data. Then we apply self-training to GPT-2-original, where the data is a combination of
the original labeled data and pseudo-labeled synthetic data. Table G.1 suggests that the gains of
GAL come from the pseudo-labeled synthetic data, i.e., both synthetic unlabeled data and teacher’s
knowledge. Without the generation of synthetic unlabeled data, the domain-speciﬁc knowledge
embedded in GPT-2-ﬁnetuned model cannot be utilized. As such, GPT-2-ﬁnetuned model is inferior
to the GPT2-original model."
-GRAM,0.9283819628647215,Table G.1: GLUE test results of different GPT-2 models.
-GRAM,0.9310344827586207,"Model
MNLI
CoLA
SST-2
MRPC
STS-B
QQP
QNLI
RTE
Avg"
-GRAM,0.9336870026525199,"GPT-2-original
85.9/85.6
54.8
94.5
86.9/82.2
86.3/85.2
72.5/89.3
91.2
69.8
80.9
GPT-2-ﬁnetuned
85.8/85.5
40.9
94.5
87.0/81.0
85.6/84.3
71.4/88.5
91.5
69.0
78.8
GPT-2-original+GAL
86.2/85.8
55.7
94.7
87.9/83.4
86.9/85.9
72.6/89.4
91.9
70.6
81.5"
-GRAM,0.9363395225464191,"H
TRAINING DETAILS"
-GRAM,0.9389920424403183,"We use the fairseq codebase (Ott et al., 2019) for implementing both NLP and tabular experiments.
Training details are summarized in Table H.1 and Table H.2. We use the HuggingFace code-
base (Wolf et al., 2020) for KD experiments. All NLP models are trained for 5 epochs with a
learning rate of 2e-5 and a batch size of 32. All experiments are run on a single Nvidia V100 GPU."
-GRAM,0.9416445623342176,"For the CV tasks, we ﬁrst use the ofﬁcial implementation of NCSN (Song & Ermon, 2019) to
generate the synthetic images for CIFAR-10 and Fashion MNIST. We use the pretrained check-
points provided by the authors for the generation of synthetic CIFAR-10 images and we train
a new generative model for Fashion MNIST from scratch with the same hyperparameters of the
CIFAR-10 network. After generating the synthetic images, we apply GAL using a FixMatch-like
setup (Sohn et al., 2020), using the hyperparameters listed in Table H.4. We follow Cubuk et al."
-GRAM,0.9442970822281167,Under review as a conference paper at ICLR 2022
-GRAM,0.946949602122016,Table H.1: Training details for NLP tasks.
-GRAM,0.9496021220159151,"MNLI
CoLA
SST-2
MRPC
STS-B
QQP
QNLI
RTE"
-GRAM,0.9522546419098143,"lr
1e-5
1e-5
1e-5
1e-5
2e-5
1e-5
1e-5
2e-5
#sent.
32
16
32
16
16
32
32
16
warmup steps
7432
320
1256
137
214
28318
1986
122
validate steps
12386
535
2093
203
360
11307
3310
203
#epoch
2
2
2
2
2
2
2
2
Table H.2: Training details for tabular tasks."
-GRAM,0.9549071618037135,"connect4
Drug
Drybean
Spambase"
-GRAM,0.9575596816976127,"lr
1e-5
1e-5
1e-5
1e-5
#sent.
32
16
16
16
warmup steps
1013
116
408
138
validate steps
1686
212
681
231
#epoch
2
2
2
4"
-GRAM,0.9602122015915119,Table H.3: Training details for KD on GLUE benchmark.
-GRAM,0.9628647214854111,"MNLI
CoLA
SST-2
MRPC
STS-B
QQP
QNLI
RTE"
-GRAM,0.9655172413793104,"lr
2e-5
2e-5
2e-5
2e-5
2e-5
2e-5
2e-5
2e-5
#sent.
32
32
32
32
16
32
32
16
validate steps
24542
534
4208
208
358
22740
6546
154
#epoch
1
1
1
1
1
1
1
1"
-GRAM,0.9681697612732095,"(2020) for strong augmentations. Finally, the backbone of the classiﬁers is from this codebase:
https://github.com/bearpaw/pytorch-classification."
-GRAM,0.9708222811671088,Table H.4: Training details for CV experiments
-GRAM,0.9734748010610079,"Parameter
Description
Value
τ
Pseudo-labeling conﬁdence threshold
0.95
batch size
Number of labeled images per batch
64
µ
Ratio between number of unlabeled and labeled images in each batch
7
images per epoch
Number of labeled images per epoch
65536
#epoch
Number of epochs of training
200
lr
learning rate max value (10 epochs warmup then cosine decay)
0.03
weight decay
Weight decay regualrization coefﬁcient
5.00 × 10−4
momentum
Nesterov momentum for SGD optimizer
0.90"
-GRAM,0.9761273209549072,"I
ADDITIONAL DETAILS"
-GRAM,0.9787798408488063,"In Tables I.1, and I.2, we present some descriptive statistics of our CIFAR-10 synthetic image dataset
to complement the samples shown in Figure C.1 and to help shed some light on the nature of the
images generated by the NCSN network."
-GRAM,0.9814323607427056,Under review as a conference paper at ICLR 2022
-GRAM,0.9840848806366048,"Table I.1: Unﬁltered CIFAR-10 synthetic data statistics sorted by Count. The Class pseudo-label for each
synthetic image is ﬁrst obtained using a teacher model trained on the original CIFAR-10 data. Count denotes
the number of images per class in the entire synthetic dataset (500K images). Conﬁdence statistics shows the
mean and standard deviation of the teacher model conﬁdence score aggregated over each class."
-GRAM,0.986737400530504,"Conﬁdence
Class
Count
Mean
Std"
-GRAM,0.9893899204244032,"truck
64519
0.932
0.141
ship
32800
0.912
0.156
horse
39604
0.916
0.158
frog
76194
0.887
0.168
dog
38784
0.826
0.188
deer
38829
0.865
0.183
cat
65969
0.826
0.185
bird
37255
0.806
0.193
car
36264
0.936
0.140
airplane
69782
0.897
0.161"
-GRAM,0.9920424403183024,"Table I.2: Filtered CIFAR-10 synthetic data statistics sorted by Count. The Class pseudo-label is ﬁrst obtained
for each synthetic image using a teacher model trained on the original CIFAR-10 data. The dataset is then
ﬁltered based on the teacher conﬁdence score where only images with conﬁdence ≥τ = 0.95 are retained.
Count denotes the number of images per class in the ﬁltered synthetic dataset. Conﬁdence statistics shows the
mean and standard deviation of the teacher model conﬁdence score aggregated over each class."
-GRAM,0.9946949602122016,"Conﬁdence
Class
Count
Mean
Std"
-GRAM,0.9973474801061007,"truck
48796
0.996
0.009
ship
22741
0.995
0.010
horse
28498
0.996
0.010
frog
45923
0.993
0.012
dog
15984
0.989
0.014
deer
21413
0.993
0.012
cat
26311
0.988
0.014
bird
13440
0.988
0.014
car
28329
0.997
0.008
airplane
43745
0.992
0.012"
