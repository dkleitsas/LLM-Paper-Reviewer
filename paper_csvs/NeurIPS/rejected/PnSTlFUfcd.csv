Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000481000481000481,"To deploy reinforcement learning (RL) systems in real-world scenarios we need
1"
ABSTRACT,0.000962000962000962,"to consider requirements such as safety and constraint compliance, rather than
2"
ABSTRACT,0.001443001443001443,"blindly maximizing for reward. In this paper we study RL with regular safety
3"
ABSTRACT,0.001924001924001924,"properties. We present a constrained problem based on the satisfaction of regular
4"
ABSTRACT,0.002405002405002405,"safety properties with high probability and we compare our setup to the some
5"
ABSTRACT,0.002886002886002886,"common constrained Markov decision processes (CMDP) settings. We also present
6"
ABSTRACT,0.003367003367003367,"a meta-algorithm with provable safety-guarantees, that can be used to shield the
7"
ABSTRACT,0.003848003848003848,"agent from violating the regular safety property during training and deployment.
8"
ABSTRACT,0.004329004329004329,"We demonstrate the effectiveness and scalability of our framework by evaluating
9"
ABSTRACT,0.00481000481000481,"our meta-algorithm in both the tabular and deep RL setting.
10"
INTRODUCTION,0.005291005291005291,"1
Introduction
11"
INTRODUCTION,0.005772005772005772,"Figure 1: Diagrammatic repre-
sentation of runtime verifica-
tion and shielding."
INTRODUCTION,0.006253006253006253,"The field of safe reinforcement learning (RL) [6, 28] has gained in-
12"
INTRODUCTION,0.006734006734006734,"creasing interest, as practitioners begin to understand the challenges
13"
INTRODUCTION,0.007215007215007215,"of applying RL in the real world [26]. There exist several dis-
14"
INTRODUCTION,0.007696007696007696,"tinct paradigms in the literature, including constrained optimization
15"
INTRODUCTION,0.008177008177008177,"[2, 20, 49, 58, 62, 74], logical constraint satisfaction [17, 24, 36–
16"
INTRODUCTION,0.008658008658008658,"38, 66], safety-critical control [15, 19, 53], all of which are unified
17"
INTRODUCTION,0.00913900913900914,"by prioritizing safety- and risk-awareness during the decision making
18"
INTRODUCTION,0.00962000962000962,"process.
19"
INTRODUCTION,0.010101010101010102,"Constrained Markov decision processes (CMDP) [4] have emerged
20"
INTRODUCTION,0.010582010582010581,"as a popular framework for modelling safe RL, or RL with con-
21"
INTRODUCTION,0.011063011063011063,"straints. Typically, the goal is to obtain a policy that maximizes
22"
INTRODUCTION,0.011544011544011544,"reward while simultaneously ensuring that the expected cumulative cost remains below a pre-defined
23"
INTRODUCTION,0.012025012025012025,"threshold. A key limitation of this setting is that constraint violations are enforced in expectation
24"
INTRODUCTION,0.012506012506012507,"rather than with high probability, the constraint thresholds also have limited semantic meaning, can
25"
INTRODUCTION,0.012987012987012988,"be very challenging to tune and in some cases inappropriate for highly safety-critical scenarios
26"
INTRODUCTION,0.013468013468013467,"[66]. Furthermore, the cost function in the CMDP is typically Markovian and thus fails to capture a
27"
INTRODUCTION,0.013949013949013949,"significantly expressive class of safety properties and constraints.
28"
INTRODUCTION,0.01443001443001443,"Regular safety properties [9] are interesting because for all but the simplest properties the correspond-
29"
INTRODUCTION,0.014911014911014911,"ing cost function is non-Markovian. Our problem setup consists of the standard RL objective with
30"
INTRODUCTION,0.015392015392015393,"regular safety properties as constraints, we note that there has been a significant body of work that
31"
INTRODUCTION,0.015873015873015872,"combines temporal logic constraints with RL [17, 24, 36–38, 66], although many of these do not
32"
INTRODUCTION,0.016354016354016353,"explicitly separate reward and safety in the same way that we do.
33"
INTRODUCTION,0.016835016835016835,"Our approach relies on shielding [3], which is a safe exploration strategy that ensures the satisfaction
34"
INTRODUCTION,0.017316017316017316,"of temporal logic constraints by deploying the learned policy in conjunction with a reactive system
35"
INTRODUCTION,0.017797017797017797,"that overrides any unsafe actions. Most shielding approaches typically make highly restrictive
36"
INTRODUCTION,0.01827801827801828,"assumptions, such as full knowledge of the environment dynamics [3], or access to a simulator [29],
37"
INTRODUCTION,0.01875901875901876,"although there has been recent work to deal with these restrictions [30, 39, 73]. In this paper, we
38"
INTRODUCTION,0.01924001924001924,"opt for the most permissive setting, where the dynamics of the environment are unknown, runtime
39"
INTRODUCTION,0.019721019721019722,"verification of the agent is realized by finite horizon model checking with a learned approximation of
40"
INTRODUCTION,0.020202020202020204,"the environment dynamics. However, in principle our framework is flexible enough to accommodate
41"
INTRODUCTION,0.02068302068302068,"more standard model checking procedures as long as certain assumptions are met.
42"
INTRODUCTION,0.021164021164021163,"Our approach can be summarised as an online shielding approach (see Fig. 1), that dynamically
43"
INTRODUCTION,0.021645021645021644,"identifies unsafe actions during training and deployment, and deploys a safe ‘backup policy’ when
44"
INTRODUCTION,0.022126022126022125,"necessary. We summarise the main contributions of our paper as follows:
45"
INTRODUCTION,0.022607022607022607,"(1) We state a constrained RL problem based on the satisfaction of regular safety properties with high
46"
INTRODUCTION,0.023088023088023088,"probability, and we identify the conditions whereby our setup generalizes several CMDP settings,
47"
INTRODUCTION,0.02356902356902357,"including expected and probabilistic cumulative cost constraints.
48"
INTRODUCTION,0.02405002405002405,"(2) We present several model checking algorithms that can verify the finite-horizon satisfaction
49"
INTRODUCTION,0.024531024531024532,"probability of regular safety properties, this includes statistical model checking procedures that can
50"
INTRODUCTION,0.025012025012025013,"be used if either the transition probabilities are unavailable or if the state space is too large.
51"
INTRODUCTION,0.025493025493025494,"(3) We develop a set of sample complexity results for the statistical model checking procedures
52"
INTRODUCTION,0.025974025974025976,"introduced in point (2), which are then used to develop a shielding meta-algorithm with provable
53"
INTRODUCTION,0.026455026455026454,"safety guarantees, even in the most permissive setting (i.e., no access to the transition probabilities).
54"
INTRODUCTION,0.026936026936026935,"(4) We empirically demonstrate the effectiveness of our framework on a variety of regular safety
55"
INTRODUCTION,0.027417027417027416,"properties in both a tabular and deep RL settings.
56"
RELATED WORK,0.027898027898027897,"2
Related Work
57"
RELATED WORK,0.02837902837902838,"Safety Paradigms in Reinforcement Learning. There exist many safety paradigms in RL, the most
58"
RELATED WORK,0.02886002886002886,"popular being constrained MDPs. For CMDPs several constrained optimization algorithms have
59"
RELATED WORK,0.02934102934102934,"been developed, most are gradient-based methods built upon Lagrange relaxations of the constrained
60"
RELATED WORK,0.029822029822029823,"problem [20, 49, 58, 62] or projection-based local policy search [2, 74]. Model-based approaches to
61"
RELATED WORK,0.030303030303030304,"CMDP [7, 11, 41, 64] have also gathered recent interest as they enjoy better sample complexity than
62"
RELATED WORK,0.030784030784030785,"their model-free counterparts, which can be imperative for safe learning [44].
63"
RELATED WORK,0.031265031265031266,"Linear Temporal Logic (LTL) constraints [17, 24, 36–38, 66] for RL have been developed as an
64"
RELATED WORK,0.031746031746031744,"alternative to CMDPs to specify stricter and more expressive constraints. The LTL formula is typically
65"
RELATED WORK,0.03222703222703223,"treated as the entire task specification, although some works have aimed to separate LTL satisfaction
66"
RELATED WORK,0.03270803270803271,"and reward into two distinct objectives [66]. The typical procedure in this setting is to identify end
67"
RELATED WORK,0.03318903318903319,"components of the MDP that satisfy the LTL constraint and construct a corresponding reward function
68"
RELATED WORK,0.03367003367003367,"such that the optimal policy satisfies the LTL constraint with maximal probability. Formal PAC-style
69"
RELATED WORK,0.034151034151034154,"guarantees have been developed for this setting [27, 36, 66, 71] although they typically rely on
70"
RELATED WORK,0.03463203463203463,"non-trivial assumptions. We note that LTL constraints can capture regular safety properties, although
71"
RELATED WORK,0.03511303511303511,"we explicitly separate reward and safety, making the work in this paper distinct from previous work.
72"
RELATED WORK,0.035594035594035595,"More rigorous safety-guarantees can be obtained by using safety filters [3], control barrier functions
73"
RELATED WORK,0.03607503607503607,"(CBF) [5], and model predictive safety certification (MPSC) [67, 68]. To achieve zero-violation
74"
RELATED WORK,0.03655603655603656,"training these methods typically assume that the dynamics of the system are known and thus they
75"
RELATED WORK,0.037037037037037035,"are typically restricted to low-dimensional systems. While these methods come from safety-critical
76"
RELATED WORK,0.03751803751803752,"control, they are closely related to safe reinforcement learning [15].
77"
RELATED WORK,0.037999037999038,"Learning Over Regular Structures. RL and regular properties have been studied in conjunction
78"
RELATED WORK,0.03848003848003848,"before, perhaps most famously as ‘Reward Machines’ [42, 43] – a type of finite state automaton that
79"
RELATED WORK,0.03896103896103896,"specifies a different reward function at each automaton state. Reward machines do not explicitly
80"
RELATED WORK,0.039442039442039445,"deal with safety, rather non-Markovian reward functions that depend on histories distinguished by
81"
RELATED WORK,0.03992303992303992,"regular languages. Several methods have been developed to exploit the structure of these automata
82"
RELATED WORK,0.04040404040404041,"and dramatically speed up learning [42, 43, 55, 61], e.g., counter factual experiences.
83"
RELATED WORK,0.040885040885040885,"Regular decision processes (RDP) [13] are a specific class non-Markovian DPs [8] that have also
84"
RELATED WORK,0.04136604136604136,"been studied in several works [13, 22, 51, 59, 65]. Most of these works are theoretical and slightly
85"
RELATED WORK,0.04184704184704185,"out-of-scope for this paper, as the RDP setting does not explicitly handle safety and encompasses
86"
RELATED WORK,0.042328042328042326,"both non-Markovian rewards and transition probabilities.
87"
RELATED WORK,0.04280904280904281,"Shielding. From formal methods, shielding for safe RL [3] forces hard constraints on policies, using
88"
RELATED WORK,0.04329004329004329,"a reactive system that ‘shields’ the agent from taking unsafe actions. Synthesising a correct-by-
89"
RELATED WORK,0.04377104377104377,"construction reactive ‘shield’ typically requires access to the environment dynamics and can be
90"
RELATED WORK,0.04425204425204425,"computationally demanding when the state or action space is large. Several recent works have aimed
91"
RELATED WORK,0.044733044733044736,"to scale the concept of shielding to more general settings, relaxing the prerequisite assumptions for
92"
RELATED WORK,0.04521404521404521,"shielding, by either only assuming access to a ‘black box’ model for planning [29], or learning a world
93"
RELATED WORK,0.0456950456950457,"model from scratch [30, 39, 73]. Other notable works that can be viewed as shielding include, MASE
94"
RELATED WORK,0.046176046176046176,"[69] – a safe exploration algorithm with access to an ‘emergency reset button’, and Recovery-RL
95"
RELATED WORK,0.046657046657046654,"[63] – which has access to a ‘recovery policy’ that is activated when the probability of reaching an
96"
RELATED WORK,0.04713804713804714,"unsafe state is too high. A simple form of shielding with LTL specifications has also been considered
97"
RELATED WORK,0.047619047619047616,"[37, 54], but experimentally these methods have only been tested in quite simple settings.
98"
PRELIMINARIES,0.0481000481000481,"3
Preliminaries
99"
PRELIMINARIES,0.04858104858104858,"For a finite set S, let Pow(S) denote the power set of S. Also, let Dist(S) denote the set of
100"
PRELIMINARIES,0.049062049062049064,"distributions over S, where a distribution µ : S →[0, 1] is a function such that P"
PRELIMINARIES,0.04954304954304954,"s∈S µ(s) = 1. Let
101"
PRELIMINARIES,0.050024050024050026,"S∗and Sω denote the set of finite and infinite sequences over S respectively. The set of all finite and
102"
PRELIMINARIES,0.050505050505050504,"infinite sequences is denoted S∞= S∗∪Sω. We denote as |ρ| the length of a sequence ρ ∈S∞,
103"
PRELIMINARIES,0.05098605098605099,"where |ρ| = ∞if ρ ∈Sω. We also denote as ρ[i] the i + 1-th element of a sequence, when i < |ρ|,
104"
PRELIMINARIES,0.05146705146705147,"and we denote as ρ↓= ρ[|ρ| −1] the last element of a sequence, when ρ ∈S∗. A sequence ρ1 is a
105"
PRELIMINARIES,0.05194805194805195,"prefix of ρ2, denoted ρ1 ⪯ρ2, if |ρ1| ≤|ρ2| and ρ1[i] = ρ2[i] for all 0 ≤i ≤|ρ1|. A sequence ρ1 is
106"
PRELIMINARIES,0.05242905242905243,"a proper prefix of ρ2, denoted ρ1 ≺ρ2, if ρ1 ⪯ρ2 and ρ1 ̸= ρ2.
107"
PRELIMINARIES,0.05291005291005291,"Labelled MDPs and Markov Chains. An MDP is a tuple M = (S, A, P, P0, R, AP, L), where
108"
PRELIMINARIES,0.05339105339105339,"S and A are finite sets of states and actions resp.; P : S × A →Dist(S) is the transition
109"
PRELIMINARIES,0.05387205387205387,"function; P0 ∈Dist(S) is the initial state distribution; R : S × A →[0, 1] is the reward function;
110"
PRELIMINARIES,0.054353054353054354,"AP is a set of atomic propositions, where Σ = Pow(AP) is the alphabet over AP; and L :
111"
PRELIMINARIES,0.05483405483405483,"S →Σ is a labelling function, where L(s) denotes the set of atoms that hold in a given state
112"
PRELIMINARIES,0.05531505531505532,"s ∈S. A memory-less (stochastic) policy is a function π : S →Dist(A) and its value function,
113"
PRELIMINARIES,0.055796055796055795,"denoted Vπ : S →R is defined as the expected reward from a given state under policy π, i.e.,
114"
PRELIMINARIES,0.05627705627705628,"Vπ(s) = Eπ[PT
t=0 R(st, at)|s0 = s], where T is a fixed episode length. Furthermore, denote as
115"
PRELIMINARIES,0.05675805675805676,"Mπ = (S, Pπ, P0, AP, L) the Markov chain induced by a fixed policy π, where the transition
116"
PRELIMINARIES,0.05723905723905724,function is such that Pπ(s′|s) = P
PRELIMINARIES,0.05772005772005772,"a∈A P(s′|s, a)π(a|s). A path ρ ∈S∞through Mπ is a finite (or
117"
PRELIMINARIES,0.0582010582010582,"infinite) sequence of states. Using standard results from measure theory it can be shown that the set
118"
PRELIMINARIES,0.05868205868205868,"of all paths {ρ ∈Sω | ρpref ⪯ρ} with a common prefix ρpref is measurable [9].
119"
PRELIMINARIES,0.05916305916305916,"Probabilistic CTL. (PCTL) [9] is a branching-time temporal logic for specifying properties of
120"
PRELIMINARIES,0.059644059644059645,"stochastic systems. A well-formed PCTL property can be constructed with the following grammar,
121"
PRELIMINARIES,0.06012506012506012,Φ ::=true | a | ¬Φ | Φ ∧Φ | P▷◁p[φ]
PRELIMINARIES,0.06060606060606061,φ ::=XΦ | ΦUΦ | ΦU ≤nΦ
PRELIMINARIES,0.061087061087061086,"where a ∈AP, ▷◁∈{<, >, ≤, ≥} is a binary comparison operator, and p ∈[0, 1] is a probability.
122"
PRELIMINARIES,0.06156806156806157,"Negation ¬ and conjunction ∧are the familiar logical operators from propositional logic, and next X,
123"
PRELIMINARIES,0.06204906204906205,"until U and bounded until U ≤n are the temporal operators from CTL [9]. We make the distinction
124"
PRELIMINARIES,0.06253006253006253,"here between state formula Φ and path formula φ. The satisfaction relation for state formula Φ is
125"
PRELIMINARIES,0.06301106301106302,"defined in the standard way for Boolean connectives. For probabilistic quantification we say that
126"
PRELIMINARIES,0.06349206349206349,"s |= P▷◁p[φ] iff Pr(s |= φ) := Pr(ρ ∈Sω | ρ[0] = s, ρ |= φ) ▷◁p. Let PrM(s |= φ) be the
127"
PRELIMINARIES,0.06397306397306397,"probability w.r.t. the Markov chain M. For path formula φ the satisfaction relation is as follows,
128"
PRELIMINARIES,0.06445406445406446,"ρ |= XΦ
iff
ρ[1] |= Φ
ρ |= Φ1UΦ2
iff
∃j ≥0 s.t. (ρ[j] |= Φ2 ∧∀0 ≤i < j, ρ[i] |= Φ1)
ρ |= Φ1U ≤nΦ2
iff
∃0 ≤j ≤n s.t. (ρ[j] |= Φ2 ∧∀0 ≤i < j, ρ[i] |= Φ1)"
PRELIMINARIES,0.06493506493506493,"From the standard operators of propositional logic we may derive disjunction ∨, implication →and
129"
PRELIMINARIES,0.06541606541606541,"coimplication ↔. We also note that the common temporal operators ‘eventually’ ♢and ’always’ □,
130"
PRELIMINARIES,0.0658970658970659,"and their bounded counterparts ♢≤n and □≤n can be derived in a familiar way, i.e., ♢Φ ::= true UΦ,
131"
PRELIMINARIES,0.06637806637806638,"□Φ ::= ¬♢¬Φ, resp. ♢≤n Φ ::= true U ≤nΦ, □≤n Φ ::= ¬♢≤n¬Φ.
132"
PRELIMINARIES,0.06685906685906685,"Regular Safety Property. A linear time property Psafe ⊆Σω over the alphabet Σ is a safety property
133"
PRELIMINARIES,0.06734006734006734,"if for all words w ∈Σω \ Psafe, there exists a finite prefix wpref of w such that Psafe ∩{w′ ∈Σω |
134"
PRELIMINARIES,0.06782106782106782,"wpref ⪯w′} = ∅. Any such sequence wpref is called a bad prefix for Psafe, a bad prefix wpref
135"
PRELIMINARIES,0.06830206830206831,"is called minimal iff there does not exist w′′ ≺wpref such that w′′ is a bad prefix for Psafe. Let
136"
PRELIMINARIES,0.06878306878306878,"BadPref(Psafe) and MinBadPref(Psafe) denote the set of of bad and minimal bad prefixes resp.
137"
PRELIMINARIES,0.06926406926406926,"A safety property Psafe ∈Σω is regular if the set BadPref(Psafe) constitutes a regular language. That
138"
PRELIMINARIES,0.06974506974506975,"is, there exists some deterministic finite automata (DFA) that accepts the bad prefixes for Psafe [9],
139"
PRELIMINARIES,0.07022607022607022,"that is, a path ρ ∈Sω is ‘unsafe’ if the trace trace(ρ) = L(ρ[0]), L(ρ[1]), . . . ∈Σω is accepted by
140"
PRELIMINARIES,0.0707070707070707,"the corresponding DFA.
141"
PRELIMINARIES,0.07118807118807119,"Definition 3.1 (DFA). A deterministic finite automata is a tuple D = (Q, Σ, ∆, Q0, F), where Q
142"
PRELIMINARIES,0.07166907166907167,"is a finite set of states, Σ is a finite alphabet, ∆: Q × Σ →Q is the transition function, Q0 is the
143"
PRELIMINARIES,0.07215007215007214,"initial state, and F ⊆Q is the set of accepting states. The extended transition function ∆∗is the
144"
PRELIMINARIES,0.07263107263107263,"total function ∆∗: Q × Σ∗→Q defined recursively as ∆∗(q, w) = ∆(∆∗(q, w \ w↓), w↓). The
145"
PRELIMINARIES,0.07311207311207311,"language accepted by DFA D is denoted L(D) = {w ∈Σ∗| ∆∗(Q0, w) ∈F}.
146"
PRELIMINARIES,0.0735930735930736,"Furthermore, we denote as P H
safe ⊆Σω the corresponding finite-horizon safety property for H ∈Z+,
147"
PRELIMINARIES,0.07407407407407407,"where for all words w ∈Σω \ P H
safe there exists wpref ⪯w such that |wpref| ≤H and wpref ∈
148"
PRELIMINARIES,0.07455507455507455,"BadPref(Psafe). We model check regular safety properties by synchronizing the DFA and Markov
149"
PRELIMINARIES,0.07503607503607504,"chain in a standard way – by computing the product Markov chain.
150"
PRELIMINARIES,0.07551707551707551,"Definition 3.2 (Product Markov Chain). Let M = (S, P, P0, AP, L) be a Markov chain and D =
151"
PRELIMINARIES,0.075998075998076,"(Q, Σ, ∆, Q0, F) be a DFA. The product Markov chain is M ⊗D = (S × Q, P′, P′
0, {accept}, L′),
152"
PRELIMINARIES,0.07647907647907648,"where L′(⟨s, q⟩) = {accept} if q ∈F and L′(⟨s, q⟩) = ∅o/w, P′
0(⟨s, q⟩) = P0(s) if q =
153"
PRELIMINARIES,0.07696007696007696,"∆(Q0, L(s)) and 0 o/w, and P′(⟨s′, q′⟩|⟨s, q⟩) = P(s′|s) if q′ = ∆(q, L(s′)) and 0 o/w.
154"
PRELIMINARIES,0.07744107744107744,"To compute the satisfaction probability of Psafe for a given state s ∈S we consider the set of paths
155"
PRELIMINARIES,0.07792207792207792,"ρ ∈Sω from s and the corresponding trace in the DFA. We provide the following definition.
156"
PRELIMINARIES,0.0784030784030784,"Definition 3.3 (Satisfaction probability for Psafe). Let M = (S, P, P0, AP, L) be a Markov chain
157"
PRELIMINARIES,0.07888407888407889,"and let D = (Q, Σ, ∆, Q0, F) be the DFA such that L(D) = BadPref(Psafe). For a path ρ ∈Sω
158"
PRELIMINARIES,0.07936507936507936,"in the Markov chain, let trace(ρ) = L(ρ[0]), L(ρ[1]), . . . ∈Σω be the corresponding word over
159"
PRELIMINARIES,0.07984607984607985,"Σ = Pow(AP). From a given state s ∈S the satisfaction probability for Psafe is defined as follows,
160"
PRELIMINARIES,0.08032708032708033,"PrM(s |= Psafe) := PrM(ρ ∈Sω | ρ[0] = s, trace(ρ) ̸∈L(D))"
PRELIMINARIES,0.08080808080808081,"Perhaps more importantly, we note that this satisfaction probability can be written as the following
161"
PRELIMINARIES,0.08128908128908129,"reachability probability in the product Markov chain,
162"
PRELIMINARIES,0.08177008177008177,"PrM(s |= Psafe) = PrM⊗D(⟨s, qs⟩̸|= ♢accept)"
PRELIMINARIES,0.08225108225108226,"where qs = ∆(Q0, L(s)) and ♢accept is a PCTL path formula that reads, ‘eventually accept’ [9].
163"
PRELIMINARIES,0.08273208273208273,"For the corresponding finite-horizon safety property P H
safe we state the following result.
164"
PRELIMINARIES,0.08321308321308321,"Proposition 3.4 (Satisfaction probability for P H
safe). Let M and D be the MDP and DFA in Defn. 3.3.
165"
PRELIMINARIES,0.0836940836940837,"For a path ρ ∈Sω in the Markov chain, let traceH(ρ) = L(ρ[0]), L(ρ[1]) . . . , L(ρ[H]) be the
166"
PRELIMINARIES,0.08417508417508418,"corresponding finite word over Σ = Pow(AP). For a given state s ∈S the finite horizon satisfaction
167"
PRELIMINARIES,0.08465608465608465,"probability for Psafe is defined as follows,
168"
PRELIMINARIES,0.08513708513708514,"PrM(s |= P H
safe) := PrM(ρ ∈Sω | ρ[0] = s, traceH(ρ) ̸∈L(D))"
PRELIMINARIES,0.08561808561808562,"where H ∈Z+ is some fixed model checking horizon. Similar to before, we show that the finite
169"
PRELIMINARIES,0.0860990860990861,"horizon satisfaction probability can be written as the following bounded reachability probability,
170"
PRELIMINARIES,0.08658008658008658,"PrM(s |= P H
safe) = PrM⊗D(⟨s, qs⟩̸|= ♢≤Haccept)"
PRELIMINARIES,0.08706108706108706,"where qs = ∆(Q0, L(s)) is as before and ♢≤Haccept is the corresponding step-bounded PCTL path
171"
PRELIMINARIES,0.08754208754208755,"formula that reads, ‘eventually accept in H timesteps’.
172"
PRELIMINARIES,0.08802308802308802,"The unbounded reachability probability can be computed by solving a system of linear equations, the
173"
PRELIMINARIES,0.0885040885040885,"bounded reachability probability can be computed with O(H) matrix multiplications, in both cases
174"
PRELIMINARIES,0.08898508898508899,"the time complexity of the procedure is a polynomial in the size of the product Markov chain [9].
175"
PROBLEM SETUP,0.08946608946608947,"4
Problem Setup
176"
PROBLEM SETUP,0.08994708994708994,"In this paper, we are interested in the quantitative model checking of regular safety properties for
177"
PROBLEM SETUP,0.09042809042809043,"a fixed finite horizon H and in the context of episodic RL, i.e., where the length of the episode T
178"
PROBLEM SETUP,0.09090909090909091,"is fixed. In particular, at every timestep we constrain the (step-bounded) reachability probability
179"
PROBLEM SETUP,0.0913900913900914,"Pr(⟨s, q⟩̸|= ♢≤Haccept) in the product Markov chain Mπ ⊗D. We assume that H is chosen so as
180"
PROBLEM SETUP,0.09187109187109187,"to avoid any irrecoverable states [35, 64], i.e., those that lead to a violation of the safety property no
181"
PROBLEM SETUP,0.09235209235209235,"matter the sequence of actions taken, the precise details of this notion are presented in Section 6. We
182"
PROBLEM SETUP,0.09283309283309284,"specify the following constrained problem,
183"
PROBLEM SETUP,0.09331409331409331,"Problem 4.1 (Step-wise bounded regular safety property constraint). Let Psafe be a regular safety
184"
PROBLEM SETUP,0.09379509379509379,"property, D be the DFA such that L(D) = BadPref(Psafe) and M be the MDP;
185"
PROBLEM SETUP,0.09427609427609428,"max
π
Vπ
subject to
Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t ∈[0, T]"
PROBLEM SETUP,0.09475709475709476,"where all probability is taken under the product Markov Chain Mπ ⊗D, p1 ∈[0, 1] is a probability
186"
PROBLEM SETUP,0.09523809523809523,"threshold, H is the model checking horizon and T is the fixed episode length.
187"
PROBLEM SETUP,0.09571909571909572,"The hyperparameter p1 is be directly used to trade-off safety and exploration in a semantically
188"
PROBLEM SETUP,0.0962000962000962,"meaningful way; p1 prescribes the probability of satisfying the finite-horizon safety property P H
safe at
189"
PROBLEM SETUP,0.09668109668109669,"each timestep. In particular, if p1 is sufficiently small then we can guarantee (with high-probability)
190"
PROBLEM SETUP,0.09716209716209716,"that the regular safety property Psafe is satisfied for the entire episode length T.
191"
PROBLEM SETUP,0.09764309764309764,"Proposition 4.2. Let P T
safe denote the (episodic) regular safety property for a fixed episode length
192"
PROBLEM SETUP,0.09812409812409813,"T. Then satisfying Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1 for all t ∈[0, T] guarantees that Pr(s0 |=
193"
PROBLEM SETUP,0.0986050986050986,"P T
safe) ≥1 −p1 · ⌈T/H⌉, where s0 ∼P0 is the initial state.
194"
PROBLEM SETUP,0.09908609908609908,"Comparison to CMDP. In the remainder of this section, we compare our problem setup to various
195"
PROBLEM SETUP,0.09956709956709957,"CMDP settings [4], with the aim of unifying different perspectives from safe RL. The purpose of this
196"
PROBLEM SETUP,0.10004810004810005,"is to show that our proposed method for solving Problem 4.1 can also be used to satisfy other more
197"
PROBLEM SETUP,0.10052910052910052,"common CMDP constraints. First, we define the following cost function that prescribes a scalar cost
198"
PROBLEM SETUP,0.10101010101010101,"C > 0 when the regular safety property Psafe is violated and 0 otherwise.
199"
PROBLEM SETUP,0.10149110149110149,"Definition 4.3 (Cost function). Let Psafe be a regular safety property and let D be the DFA such
200"
PROBLEM SETUP,0.10197210197210198,"that L(D) = BadPref(Psafe), modified such that for all q ∈F, q →Q0. The cost function is then
201"
PROBLEM SETUP,0.10245310245310245,"defined as,
202"
PROBLEM SETUP,0.10293410293410293,"C(⟨s, q⟩) =
C
if accept ∈L′(⟨s, q⟩)
0
otherwise"
PROBLEM SETUP,0.10341510341510342,"where C > 0 is some generic scalar cost and L′ is the labelling function defined in Def. 3.2.
203"
PROBLEM SETUP,0.1038961038961039,"Resetting the DFA. Rather than reset the environment, the DFA is reset once it reaches an accepting
204"
PROBLEM SETUP,0.10437710437710437,"state, so as to measure the rate of constraint satisfaction over a fixed episode length T. This can easily
205"
PROBLEM SETUP,0.10485810485810486,"be realized by replacing any outgoing transitions from the accepting states with transitions back to
206"
PROBLEM SETUP,0.10533910533910534,"the initial state, i.e., for all q ∈F, q →Q0.
207"
PROBLEM SETUP,0.10582010582010581,"Non-Markovian costs. The cost function is Markov on the product states ⟨s, q⟩∈S × Q. However,
208"
PROBLEM SETUP,0.1063011063011063,"in most cases the cost function is non-Markovian in the original state space S, since the automaton
209"
PROBLEM SETUP,0.10678210678210678,"state q ∈Q could depend on some arbitrary history of states. Thus our problem setup generalizes the
210"
PROBLEM SETUP,0.10726310726310727,"standard CMDP framework with non-Markovian safety constraints.
211"
PROBLEM SETUP,0.10774410774410774,"Invariant properties. Invariant properties Pinv(Φ), also written □Φ (‘always Φ’), where Φ is a
212"
PROBLEM SETUP,0.10822510822510822,"propositional state formula, are the simplest type of safety properties where the cost function is still
213"
PROBLEM SETUP,0.10870610870610871,"Markov in the original state space. In this case we are operating in the standard CMDP framework,
214"
PROBLEM SETUP,0.1091871091871092,"we also note that checking invariant properties with a fixed model checking horizon has been studied
215"
PROBLEM SETUP,0.10966810966810966,"in previous works, as bounded safety [29, 30] and safety for a finite horizon [45].
216"
PROBLEM SETUP,0.11014911014911015,"The most common type of CMDP constraints are expected cumulative (cost) constraints, which
217"
PROBLEM SETUP,0.11063011063011063,"constrain the expected cost below a given threshold.
218"
PROBLEM SETUP,0.1111111111111111,"Problem 4.4 (Expected cumulative constraint [4, 58])."
PROBLEM SETUP,0.11159211159211159,"max
π
Vπ
subject to
E⟨st,qt⟩∼Mπ⊗D
hPT
t=0 C(⟨st, qt⟩)
i
≤d1"
PROBLEM SETUP,0.11207311207311207,"where d1 ∈R+ is the cost threshold and T is the fixed episode length.
219"
PROBLEM SETUP,0.11255411255411256,"Probabilistic cumulative (cost) constraints, are a stricter class of constraints that constrain the
220"
PROBLEM SETUP,0.11303511303511303,"cumulative cost with high probability, rather than in expectation.
221"
PROBLEM SETUP,0.11351611351611351,"Problem 4.5 (Probabilistic cumulative constraint [18, 56])."
PROBLEM SETUP,0.113997113997114,"max
π
Vπ
subject to
P⟨st,qt⟩∼Mπ⊗D
hPT
t=0 C(⟨st, qt⟩) ≤d2
i
≥1 −δ2"
PROBLEM SETUP,0.11447811447811448,"where d2 ∈R+ is the cost threshold, δ2 is a tolerance parameter, and T is the fixed episode length.
222"
PROBLEM SETUP,0.11495911495911496,"We also consider instantaneous constraints, which bound the cost ‘almost surely’ at each timestep
223"
PROBLEM SETUP,0.11544011544011544,"t ∈[0, T]. These are an even stricter type of constraint for highly safety-critical applications.
224"
PROBLEM SETUP,0.11592111592111592,"Problem 4.6 (Instantaneous constraint [23, 60, 69])."
PROBLEM SETUP,0.1164021164021164,"max
π
Vπ
subject to
P⟨st,qt⟩∼Mπ⊗D

C(⟨st, qt⟩) ≤d3

= 1
∀t ∈[0, T]"
PROBLEM SETUP,0.11688311688311688,"where d3 ∈R+ is the cost threshold and T is the fixed episode length.
225"
PROBLEM SETUP,0.11736411736411737,"In particular, these problems define a constrained set of feasible policies Π. We make the distinction
226"
PROBLEM SETUP,0.11784511784511785,"here between a feasible policy and a solution to the problem, the former being any policy satisfying
227"
PROBLEM SETUP,0.11832611832611832,"the constraints of the problem and the later being the optimal policy within the feasible set Π.
228"
PROBLEM SETUP,0.1188071188071188,"Theorem 4.7. A feasible policy for Problem 4.1 is also a feasible policy for Problems 4.4, 4.5 and
229"
PROBLEM SETUP,0.11928811928811929,"4.6 under specific parameter settings for p1, d1, d2 and δ2, and d3.
230"
PROBLEM SETUP,0.11976911976911978,"In Appendix G we provide a full set of statements that outline the relationships between the con-
231"
PROBLEM SETUP,0.12025012025012025,"strained problems presented in this section. The significance of these results is that they demonstrate
232"
PROBLEM SETUP,0.12073112073112073,"by solving Problem 4.1 with our proposed method we can obtain feasible policies for Problems 4.4,
233"
PROBLEM SETUP,0.12121212121212122,"4.5 and 4.6, although for most of these problems there is no direct relationship between our problem
234"
PROBLEM SETUP,0.12169312169312169,"setup, in particular we can say little about whether the optimal policy for one problem is necessarily
235"
PROBLEM SETUP,0.12217412217412217,"optimal for another. Nevertheless, we find it interesting to explore the relationships between our setup
236"
PROBLEM SETUP,0.12265512265512266,"and other perhaps more common constrained RL problems.
237"
MODEL CHECKING,0.12313612313612314,"5
Model checking
238"
MODEL CHECKING,0.12361712361712361,"In this section we outline several procedures for checking the finite-horizon satisfaction probability
239"
MODEL CHECKING,0.1240981240981241,"of regular safety properties and we summarise the settings in which they can be used.
240"
MODEL CHECKING,0.12457912457912458,"Assumption 5.1. We are given access to the ‘true’ transition probabilities P.
241"
MODEL CHECKING,0.12506012506012507,"Assumption 5.2. We are given access to a ‘black box’ model that perfectly simulates the ‘true’
242"
MODEL CHECKING,0.12554112554112554,"transition probabilities P.
243"
MODEL CHECKING,0.12602212602212604,"Assumption 5.3. We are given access to an approximate dynamic model bP ≈P, where the total
244"
MODEL CHECKING,0.1265031265031265,"variation (TV) distance DT V (Pπ(· | s), bPπ(· | s)) ≤ϵ/H, for all s ∈S.1
245"
MODEL CHECKING,0.12698412698412698,"Exact model checking. Under Assumption 5.1 we can precisely compute the (finite horizon)
246"
MODEL CHECKING,0.12746512746512748,"satisfaction probability of Psafe, in the Markov chain Mπ induced by the fixed policy π in time
247"
MODEL CHECKING,0.12794612794612795,"O(poly(size(Mπ ⊗D)) · H) [9], where D is the DFA such that L(D) = BadPref(Psafe) and H
248"
MODEL CHECKING,0.12842712842712842,"is the model checking horizon. H should not be too large and so the complexity of exact model
249"
MODEL CHECKING,0.12890812890812892,"checking ultimately depends on the size of the product Mπ ⊗D, and so if the size of either the MDP
250"
MODEL CHECKING,0.1293891293891294,"or DFA is too large then exact model checking may be infeasible.
251"
MODEL CHECKING,0.12987012987012986,"Monte-Carlo model checking. To address the limitations of exact model checking, we can drop
252"
MODEL CHECKING,0.13035113035113036,"Assumption 5.1. Rather, under Assumption 5.2, we can sample sufficiently many paths from a
253"
MODEL CHECKING,0.13083213083213083,"‘black box’ model of the environment dynamics and estimate the reachability probability Pr(⟨s, q⟩|=
254"
MODEL CHECKING,0.13131313131313133,"♢≤Haccept) in the product Markov chain Mπ ⊗D, by computing the proportion of accepting paths.
255"
MODEL CHECKING,0.1317941317941318,"Using statistical bounds, such as Hoeffding’s inequality [40] or Bernstein-type bounds [52], we can
256"
MODEL CHECKING,0.13227513227513227,"bound the error of this estimate, with high probability.
257"
MODEL CHECKING,0.13275613275613277,"Proposition 5.4. Let ϵ > 0, δ > 0, s ∈S and H ≥1 be given. Under Assumption 5.2, we can
258"
MODEL CHECKING,0.13323713323713324,"obtain an ϵ-approximate estimate for the probability Pr(⟨s, q⟩|= ♢≤Haccept) with probability at
259"
MODEL CHECKING,0.1337181337181337,"least 1 −δ, by sampling m ≥
1
2ϵ2 log
  2"
MODEL CHECKING,0.1341991341991342,"δ

paths from the ‘black box’ model.
260"
MODEL CHECKING,0.13468013468013468,"1For two discrete probability distributions µ1 and µ2 over the same space X the TV distance is defined as:
DT V (µ1(·), µ2(·)) = 1"
P,0.13516113516113515,"2
P"
P,0.13564213564213565,x∈X |µ1(x) −µ2(x)|
P,0.13612313612313612,"We note that the time complexity of these statistical methods does not depend in the size of the
261"
P,0.13660413660413662,"product MDP or DFA, since the product states ⟨s, q⟩∈S × Q can be computed on-the-fly, rather the
262"
P,0.1370851370851371,"time complexity depends on the horizon H, the desired level of accuracy ϵ, failure probability δ.
263"
P,0.13756613756613756,"Model checking with approximate models. In most realistic cases neither the ‘true’ transition
264"
P,0.13804713804713806,"probabilities nor a perfect ‘black box’ model is available to us before-hand. Under Assumption
265"
P,0.13852813852813853,"5.3 we can model check with an ‘approximate’ model of the MDP dynamics, which can either be
266"
P,0.139009139009139,"constructed ahead of time (offline) or learned from experience, with maximum likelihood (or similar).
267"
P,0.1394901394901395,"We can then either exact model check in with the ‘approximate’ probabilities, or if the MDP is too
268"
P,0.13997113997113997,"large, we can leverage statistical model checking by sampling paths from the ’approximate’ model.
269"
P,0.14045214045214044,"Proposition 5.5. Let ϵ > 0, δ > 0, s ∈S and H ≥1 be given. Under Assumption 5.3 we can make
270"
P,0.14093314093314094,"the following two statements:
271"
P,0.1414141414141414,"(1) We can obtain an ϵ-approximate estimate for Pr(⟨s, q⟩|= ♢≤Haccept) with probability 1 by
272"
P,0.1418951418951419,"exact model checking with the transition probabilities of bPπ in time O(poly(size(Mπ ⊗D)) · H).
273"
P,0.14237614237614238,"(2) We can obtain an ϵ-approximate estimate for Pr(⟨s, q⟩|= ♢≤Haccept) with probability at least
274"
P,0.14285714285714285,"1 −δ, by sampling m ≥
2
ϵ2 log
  2"
P,0.14333814333814335,"δ

paths from the ‘approximate’ dynamics model bPπ.
275"
SHIELDING THE POLICY,0.14381914381914382,"6
Shielding the policy
276"
SHIELDING THE POLICY,0.1443001443001443,"Algorithm 1 Shielding (with runtime verification
of regular safety properties)
Input: model checking parameters (ϵ, δ, p, H),
labelling function L, DFA D = (Q, Σ, ∆, Q0, F).
Optional: probabilities P, ‘backup policy’ πsafe.
Initialize: ‘task policy’ πtask, ‘backup policy’ πsafe
and (approximate) probabilities bP.
for each episode do"
SHIELDING THE POLICY,0.1447811447811448,"Observe s0, L(s0) and q0 ←∆(Q0, L(s0))
for t = 0, . . . , T do
▷Fixed episode length
Sample action a ∼πtask(· | st)
if Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1 then"
SHIELDING THE POLICY,0.14526214526214526,"// Use the proposed action
at ←a
else"
SHIELDING THE POLICY,0.14574314574314573,"// Override the action
at ∼πsafe(· | st, qt)
Play at and observe st+1, L(st+1), rt
qt+1 ←∆(qt, L(st+1)),
ct ←1[qt+1 ∈F]
Update πtask with (st, at, st+1, rt)
Update πsafe with (st, qt, at, st+1, qt+1, ct)
Update bP with (st, at, st+1)"
SHIELDING THE POLICY,0.14622414622414623,"At a high-level, the shielding meta-algorithm
277"
SHIELDING THE POLICY,0.1467051467051467,"works by switching between the ‘task policy’
278"
SHIELDING THE POLICY,0.1471861471861472,"trained with RL to maximize rewards and a
279"
SHIELDING THE POLICY,0.14766714766714767,"‘backup policy’, which typically constitutes a
280"
SHIELDING THE POLICY,0.14814814814814814,"low-reward, possibly rule-based policy that is
281"
SHIELDING THE POLICY,0.14862914862914864,"guaranteed to be safe.
In some cases this
282"
SHIELDING THE POLICY,0.1491101491101491,"‘backup policy’ may be available to us before
283"
SHIELDING THE POLICY,0.14959114959114958,"training, although in most realistic cases it will
284"
SHIELDING THE POLICY,0.15007215007215008,"need to be learned.
In our case we switch
285"
SHIELDING THE POLICY,0.15055315055315055,"from the ‘task policy’ to the ‘backup policy’
286"
SHIELDING THE POLICY,0.15103415103415102,"when the reachability probability Pr(⟨s, q⟩|=
287"
SHIELDING THE POLICY,0.15151515151515152,"♢≤Haccept) exceeds the probability threshold
288"
SHIELDING THE POLICY,0.151996151996152,"p1. To check this we can use any of the model
289"
SHIELDING THE POLICY,0.1524771524771525,"checking procedures presented earlier.
The
290"
SHIELDING THE POLICY,0.15295815295815296,"‘backup policy’ is used when the reachability
291"
SHIELDING THE POLICY,0.15343915343915343,"probability exceeds p1. Intuitively if the ‘backup
292"
SHIELDING THE POLICY,0.15392015392015393,"policy’ is guaranteed to be safe, then our system
293"
SHIELDING THE POLICY,0.1544011544011544,"should satisfy the constraints of Problem 4.1,
294"
SHIELDING THE POLICY,0.15488215488215487,"independent of the ‘task policy’.
295"
SHIELDING THE POLICY,0.15536315536315537,"Backup policy. In general we assume no knowl-
296"
SHIELDING THE POLICY,0.15584415584415584,"edge of the safety dynamics before training and
297"
SHIELDING THE POLICY,0.1563251563251563,"so the ‘backup policy’ needs to be learned. In
298"
SHIELDING THE POLICY,0.1568061568061568,"particular, we can use the cost function defined
299"
SHIELDING THE POLICY,0.15728715728715728,"in Defn. 4.3 and train the ‘backup policy’ with
300"
SHIELDING THE POLICY,0.15776815776815778,"RL to minimize the expected discounted cost
301"
SHIELDING THE POLICY,0.15824915824915825,"(Eπ[PT
t=0 γtC(st, qt)]). Importantly, we note that the cost function is defined on the product state
302"
SHIELDING THE POLICY,0.15873015873015872,"space S × Q and so the ‘backup policy’ must also operate on this state space, possibly leading
303"
SHIELDING THE POLICY,0.15921115921115922,"to slower convergence. However, we can eliminate this issue entirely by training the ‘backup pol-
304"
SHIELDING THE POLICY,0.1596921596921597,"icy’ with counterfactual experiences [42, 43] – a method originally used for reward machines that
305"
SHIELDING THE POLICY,0.16017316017316016,"generates additional synthetic data for the policy, by simulating experience from each automaton
306"
SHIELDING THE POLICY,0.16065416065416066,"state.
307"
SHIELDING THE POLICY,0.16113516113516113,"Meta Algorithm. We now present the structure of the shielding meta-algorithm (see Algorithm
308"
SHIELDING THE POLICY,0.16161616161616163,"1). The precise realization of this algorithm can vary depending on problem setting, tabular, deep
309"
SHIELDING THE POLICY,0.1620971620971621,"RL, etc., however the main structure of the algorithm remains the same. In particular, during
310"
SHIELDING THE POLICY,0.16257816257816257,"interaction with the environment we shield the agent by checking that the reachability probability
311"
SHIELDING THE POLICY,0.16305916305916307,"Pr(⟨s, q⟩|= ♢≤Haccept) does not exceed threshold p1. Then, with the new accumulated experience
312"
SHIELDING THE POLICY,0.16354016354016354,"we update the ‘task policy’ denoted πtask and the ‘backup policy’ denoted πsafe with RL, and if need be
313"
SHIELDING THE POLICY,0.164021164021164,"we update our (approximate) dynamics model accordingly. In principle, the underlying RL algorithm
314"
SHIELDING THE POLICY,0.1645021645021645,"used to train either ‘task policy’ or ‘backup policy’ can differ, and the dynamics model can be a
315"
SHIELDING THE POLICY,0.16498316498316498,"simple maximum likelihood estimate or something more complex, e.g., Gaussian Process model
316"
SHIELDING THE POLICY,0.16546416546416545,"[25, 70], ensemble of parametric neural networks [21, 44] or a world model [32, 33].
317"
SHIELDING THE POLICY,0.16594516594516595,"Global Safety Guarantees. In the tabular setting we can guarantee the safety of the system described
318"
SHIELDING THE POLICY,0.16642616642616642,"in Algorithm 1 under various assumptions, even when doing Monte-Carlo model checking on an
319"
SHIELDING THE POLICY,0.16690716690716692,"‘approximate’ model of the environment dynamics. First, we provide the following definitions.
320"
SHIELDING THE POLICY,0.1673881673881674,"Definition 6.1 (Non-critical state). A product state ⟨s, q⟩∈S × Q is said to be non-critical for a
321"
SHIELDING THE POLICY,0.16786916786916786,"given model checking horizon H if for all policies π we have Pr(⟨s, q⟩|= ♢≤Haccept) = 0.
322"
SHIELDING THE POLICY,0.16835016835016836,"Definition 6.2 (Irrecoverable). A critical state ⟨s, q⟩∈S × Q is said to be irrecoverable with
323"
SHIELDING THE POLICY,0.16883116883116883,"probability p1 if for all policies π we have Pr(⟨s, q⟩|= ♢accept) ≥p1. In other words, for any
324"
SHIELDING THE POLICY,0.1693121693121693,"sequence of actions a0, a1, . . . the minimum probability Prmin(⟨s, q⟩|= ♢accept) of reaching an
325"
SHIELDING THE POLICY,0.1697931697931698,"accepting state is p1, where Prmin(⟨s, q⟩|= ♢accept) = infπ PrMπ⊗D(⟨s, q⟩|= ♢accept)
326"
SHIELDING THE POLICY,0.17027417027417027,"The safety-guarantees for Algorithm 1 rely on the following assumptions.
327"
SHIELDING THE POLICY,0.17075517075517074,"Assumption 6.3. We assume H is sufficiently large so that it is not possible to transition from any
328"
SHIELDING THE POLICY,0.17123617123617124,"non-critical state to an irrecoverable state. Furthermore we assume that there exists some H∗< H
329"
SHIELDING THE POLICY,0.1717171717171717,"such that if Prmin(⟨s, q⟩|= ♢accept) = p1 then Prmin(⟨s, q⟩|= ♢≤H∗accept) = p1.
330"
SHIELDING THE POLICY,0.1721981721981722,"Assumption 6.4. The initial state ⟨s0, L(s0)⟩is non-critical and for any state ⟨s, q⟩∈S × Q that is
331"
SHIELDING THE POLICY,0.17267917267917268,"not irrecoverable, the ‘backup policy’ πsafe is satisfies PrMπC ⊗D(⟨s, q⟩|= ♢≤Haccept) ≤p1
332"
SHIELDING THE POLICY,0.17316017316017315,"Theorem 6.5. Under Assumption 6.3 and 6.4, and provided that every state action pair (s, a) ∈S×A
333"
SHIELDING THE POLICY,0.17364117364117365,"has been visited at least O

H2|S|2"
SHIELDING THE POLICY,0.17412217412217412,"ϵ2
log

|A||S|2"
SHIELDING THE POLICY,0.1746031746031746,"δ

times. Then with probability 1 −δ the system
334"
SHIELDING THE POLICY,0.1750841750841751,"satisfies the constraints of Problem 4.1, independent of the ‘task policy’.
335"
SHIELDING THE POLICY,0.17556517556517556,"The theory is quite conservative here due to the strong dependence on |S|, in practice we can replace
336"
SHIELDING THE POLICY,0.17604617604617603,"the outer |S|2 by the maximum number of successor states from any given state. With regards to our
337"
SHIELDING THE POLICY,0.17652717652717653,"assumptions, both are not overly restrictive. Assumption 6.3 essentially states that any irrecoverable
338"
SHIELDING THE POLICY,0.177008177008177,"states, will reach the accepting state with some probability > g within a fixed horizon H∗. Similar
339"
SHIELDING THE POLICY,0.1774891774891775,"statements have been considered in prior work [35, 64]. Assumption 6.4 states that the ‘backup
340"
SHIELDING THE POLICY,0.17797017797017797,"policy’ satisfies Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1 if possible, we would expect this to be the case when
341"
SHIELDING THE POLICY,0.17845117845117844,"training the ‘backup policy’ with RL to minimize cost. The analysis for Theorem 6.5 then follows
342"
SHIELDING THE POLICY,0.17893217893217894,"by showing that the system can be recovered to a non-critical state after entering a critical but not
343"
SHIELDING THE POLICY,0.1794131794131794,"irrecoverable state.
344"
EMPIRICAL EVALUATION,0.17989417989417988,"7
Empirical Evaluation
345"
EMPIRICAL EVALUATION,0.18037518037518038,"We implemented two separate realizations of Algorithm 1, the first adapted to tabular environments
346"
EMPIRICAL EVALUATION,0.18085618085618085,"which implements both exact or statistical model checking over the learned transition probabilities, the
347"
EMPIRICAL EVALUATION,0.18133718133718132,"second is adapted to (visual) deep RL, making use of world models [32, 33], specifically DreamerV3
348"
EMPIRICAL EVALUATION,0.18181818181818182,"[34], to learn a latent dynamics model for model checking and policy optimization.
349"
EMPIRICAL EVALUATION,0.1822991822991823,Table 1: Safety properties
EMPIRICAL EVALUATION,0.1827801827801828,property Psafe
EMPIRICAL EVALUATION,0.18326118326118326,"(1) □¬green
(2) □goal→♢≤10blue
(3) □goal→♢≤10□≤5purple"
EMPIRICAL EVALUATION,0.18374218374218373,"Tabular RL. We conduct experiments on a simple ‘colour’ grid-
350"
EMPIRICAL EVALUATION,0.18422318422318423,"world environment, with regular safety properties of increasing
351"
EMPIRICAL EVALUATION,0.1847041847041847,"difficulty. In short, the goal is to navigate from a starting state
352"
EMPIRICAL EVALUATION,0.18518518518518517,"to a goal position as frequently as possible, while respecting a
353"
EMPIRICAL EVALUATION,0.18566618566618567,"given regular safety property during training. The environment is
354"
EMPIRICAL EVALUATION,0.18614718614718614,"stochastic – with some probability p the agent’s action is ignored
355"
EMPIRICAL EVALUATION,0.18662818662818662,"and another action is chosen uniformly instead. For smaller p val-
356"
EMPIRICAL EVALUATION,0.18710918710918711,"ues the environment becomes more deterministic and the safety property typically becomes easier to
357"
EMPIRICAL EVALUATION,0.18759018759018758,"satisfy with higher probability, we refer the reader to Appendix D.1 for more details. Table 1 outlines
358"
EMPIRICAL EVALUATION,0.18807118807118808,"the three safety properties used for our environments. We use PCTL-like notation to describe the
359"
EMPIRICAL EVALUATION,0.18855218855218855,"safety properties, although strictly speaking (2) and (3) are actually PCTL∗path formula. Regardless
360"
EMPIRICAL EVALUATION,0.18903318903318903,"of this slight technical detail, properties (1)-(3) are valid regular safety properties, as we can come up
361"
EMPIRICAL EVALUATION,0.18951418951418952,"with a DFA that accepts the bad prefixes for them.
362"
EMPIRICAL EVALUATION,0.18999518999519,"We compare our approach to Q-learning (without any penalties), and Q-learning on the product
363"
EMPIRICAL EVALUATION,0.19047619047619047,"state space, with penalties provided by the cost function (Defn. 4.3) and trained with counterfactual
364"
EMPIRICAL EVALUATION,0.19095719095719096,"experiences [43]. In all cases, by separating reward and safety into two distinct policies, we are able
365"
EMPIRICAL EVALUATION,0.19143819143819144,"to effectively trade-off the two objectives. Q-learning simply finds the best policy ignoring the costs,
366"
EMPIRICAL EVALUATION,0.1919191919191919,"and Q-learning with penalties is able to find a safe policy, but struggles to meaningfully balance both
367"
EMPIRICAL EVALUATION,0.1924001924001924,"objectives (see Fig. 2). Hyperparameter settings for all experiments are detailed in Appendix E. In
368"
EMPIRICAL EVALUATION,0.19288119288119288,"addition, we provide an extensive series of ablation studies in Appendix F for these experiments. For
369"
EMPIRICAL EVALUATION,0.19336219336219337,"example, we show that we don’t loose much by using Monte Carlo model checking as opposed to
370"
EMPIRICAL EVALUATION,0.19384319384319385,"exact model checking with the ‘true’ probabilities. We also show that tuning the cost coefficient C
371"
EMPIRICAL EVALUATION,0.19432419432419432,"offers no meaningful way to trade-off reward and the probability of constraint satisfaction.
372"
EMPIRICAL EVALUATION,0.19480519480519481,"Figure 2: Episode reward and cost for tabular RL
‘colour’ gridworld environment."
EMPIRICAL EVALUATION,0.19528619528619529,"Figure 3: Episode reward and violation rate for
deep RL Atari Seaquest."
EMPIRICAL EVALUATION,0.19576719576719576,"Deep RL. We deploy our version of Algo-
373"
EMPIRICAL EVALUATION,0.19624819624819625,"rithm 1 built on DreamerV3 [34] on Atari
374"
EMPIRICAL EVALUATION,0.19672919672919673,"Seaquest, provided as part of the Arcade Learn-
375"
EMPIRICAL EVALUATION,0.1972101972101972,"ing Environment (ALE)[10, 50]. We experi-
376"
EMPIRICAL EVALUATION,0.1976911976911977,"ment with two different regular safety proper-
377"
EMPIRICAL EVALUATION,0.19817219817219817,"ties: (1) (□¬surface →□(surface →diver)) ∧
378"
EMPIRICAL EVALUATION,0.19865319865319866,"(□¬out-of-oxygen) ∧(□¬hit) and (2) □diver ∧
379"
EMPIRICAL EVALUATION,0.19913419913419914,"¬surface →♢≤30surface. We compare our ap-
380"
EMPIRICAL EVALUATION,0.1996151996151996,"proach to the base DreamerV3 algorithm and
381"
EMPIRICAL EVALUATION,0.2000962000962001,"a version of DreamerV3 that implements the
382"
EMPIRICAL EVALUATION,0.20057720057720058,"augmented Lagrangian penalty framework, sim-
383"
EMPIRICAL EVALUATION,0.20105820105820105,"ilarly to [7, 41], for additional details see Ap-
384"
EMPIRICAL EVALUATION,0.20153920153920155,"pendix B.1.
385"
EMPIRICAL EVALUATION,0.20202020202020202,"Again our approach is able to effectively trade-
386"
EMPIRICAL EVALUATION,0.2025012025012025,"off both objectives, while (base) DreamerV3 ig-
387"
EMPIRICAL EVALUATION,0.20298220298220299,"nores the cost, the Lagrangian approach appears
388"
EMPIRICAL EVALUATION,0.20346320346320346,"to learn a safe policy that is not always efficient
389"
EMPIRICAL EVALUATION,0.20394420394420396,"in terms of reward (see Fig. 3). We refer the
390"
EMPIRICAL EVALUATION,0.20442520442520443,"reader to Appendix D.2 for more details of the
391"
EMPIRICAL EVALUATION,0.2049062049062049,"environment and an extended discussion.
392"
EMPIRICAL EVALUATION,0.2053872053872054,"Separating Reward and Safety. The separa-
393"
EMPIRICAL EVALUATION,0.20586820586820587,"tion of reward and safety objectives into two dis-
394"
EMPIRICAL EVALUATION,0.20634920634920634,"tinct policies has been demonstrated as an effec-
395"
EMPIRICAL EVALUATION,0.20683020683020684,"tive strategy towards safety-aware decision mak-
396"
EMPIRICAL EVALUATION,0.2073112073112073,"ing [3, 30, 46, 63], in many cases the safety ob-
397"
EMPIRICAL EVALUATION,0.2077922077922078,"jective is simpler and can be more quickly learnt
398"
EMPIRICAL EVALUATION,0.20827320827320828,"[46]. In our experiments it is clear that when
399"
EMPIRICAL EVALUATION,0.20875420875420875,"the system enters a critical state, the ‘backup
400"
EMPIRICAL EVALUATION,0.20923520923520925,"policy’ is able to efficiently guide the system
401"
EMPIRICAL EVALUATION,0.20971620971620972,"back to a non-critical state where the task policy
402"
EMPIRICAL EVALUATION,0.2101972101972102,"can continue collecting reward. However, there
403"
EMPIRICAL EVALUATION,0.2106782106782107,"is evidence that the complete separation of poli-
404"
EMPIRICAL EVALUATION,0.21115921115921116,"cies is not always appropriate [31] and penalties
405"
EMPIRICAL EVALUATION,0.21164021164021163,"or a slight coupling of the policies is required
406"
EMPIRICAL EVALUATION,0.21212121212121213,"to stop the ‘task’ and ‘backup policy’ fighting
407"
EMPIRICAL EVALUATION,0.2126022126022126,"for control of the system. Furthermore, by separating reward and safety, we typically loose any
408"
EMPIRICAL EVALUATION,0.2130832130832131,"asymptotic convergence guarantees, similar to the situation faced for hierarchical RL [61], although
409"
EMPIRICAL EVALUATION,0.21356421356421357,"there has been recent work to develop convergence guarantees for shielding [75].
410"
CONCLUSION,0.21404521404521404,"8
Conclusion
411"
CONCLUSION,0.21452621452621454,"In this paper we propose a shielding meta-algorithm for the runtime verification of regular safety
412"
CONCLUSION,0.215007215007215,"properties, given as a probabilistic constraint on the system. We provide a thorough theoretical
413"
CONCLUSION,0.21548821548821548,"examination of the problem and develop probabilistic safety guarantees for the meta-algorithm,
414"
CONCLUSION,0.21596921596921598,"which hold under reasonable assumptions. Empirically, we demonstrate that shielding is able to
415"
CONCLUSION,0.21645021645021645,"effectively balance both reward and safety, in both the tabular and deep RL setting. A more thorough
416"
CONCLUSION,0.21693121693121692,"theoretical and empirical examinations of the conditions for when shielding is appropriate would be
417"
CONCLUSION,0.21741221741221742,"an interesting direction for future work.
418"
REFERENCES,0.2178932178932179,"References
419"
REFERENCES,0.2183742183742184,"[1] Pieter Abbeel and Andrew Y Ng. 2005. Exploration and apprenticeship learning in reinforcement
420"
REFERENCES,0.21885521885521886,"learning. In Proceedings of the 22nd international conference on Machine learning. 1–8.
421"
REFERENCES,0.21933621933621933,"[2] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. 2017. Constrained policy opti-
422"
REFERENCES,0.21981721981721983,"mization. In International conference on machine learning. PMLR, 22–31.
423"
REFERENCES,0.2202982202982203,"[3] Mohammed Alshiekh, Roderick Bloem, Rüdiger Ehlers, Bettina Könighofer, Scott Niekum,
424"
REFERENCES,0.22077922077922077,"and Ufuk Topcu. 2018. Safe reinforcement learning via shielding. In Proceedings of the AAAI
425"
REFERENCES,0.22126022126022127,"Conference on Artificial Intelligence, Vol. 32.
426"
REFERENCES,0.22174122174122174,"[4] Eitan Altman. 1999. Constrained Markov decision processes: stochastic modeling. Routledge.
427"
REFERENCES,0.2222222222222222,"[5] Aaron D Ames, Samuel Coogan, Magnus Egerstedt, Gennaro Notomista, Koushil Sreenath,
428"
REFERENCES,0.2227032227032227,"and Paulo Tabuada. 2019. Control barrier functions: Theory and applications. In 2019 18th
429"
REFERENCES,0.22318422318422318,"European control conference (ECC). IEEE, 3420–3431.
430"
REFERENCES,0.22366522366522368,"[6] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.
431"
REFERENCES,0.22414622414622415,"2016. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565 (2016).
432"
REFERENCES,0.22462722462722462,"[7] Yarden As, Ilnura Usmanova, Sebastian Curi, and Andreas Krause. 2022. Constrained policy
433"
REFERENCES,0.22510822510822512,"optimization via bayesian world models. arXiv preprint arXiv:2201.09802 (2022).
434"
REFERENCES,0.2255892255892256,"[8] Fahiem Bacchus, Craig Boutilier, and Adam Grove. 1996. Rewarding behaviors. In Proceedings
435"
REFERENCES,0.22607022607022606,"of the National Conference on Artificial Intelligence. 1160–1167.
436"
REFERENCES,0.22655122655122656,"[9] Christel Baier and Joost-Pieter Katoen. 2008. Principles of model checking. MIT press.
437"
REFERENCES,0.22703222703222703,"[10] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. 2013. The Arcade Learning Environ-
438"
REFERENCES,0.2275132275132275,"ment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research
439"
REFERENCES,0.227994227994228,"47 (jun 2013), 253–279.
440"
REFERENCES,0.22847522847522847,"[11] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. 2017. Safe
441"
REFERENCES,0.22895622895622897,"model-based reinforcement learning with stability guarantees. Advances in neural information
442"
REFERENCES,0.22943722943722944,"processing systems 30 (2017).
443"
REFERENCES,0.2299182299182299,"[12] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
444"
REFERENCES,0.2303992303992304,"Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
445"
REFERENCES,0.23088023088023088,"Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. http:
446"
REFERENCES,0.23136123136123135,"//github.com/google/jax
447"
REFERENCES,0.23184223184223185,"[13] Ronen I Brafman, Giuseppe De Giacomo, et al. 2019. Regular Decision Processes: A Model
448"
REFERENCES,0.23232323232323232,"for Non-Markovian Domains.. In IJCAI. 5516–5522.
449"
REFERENCES,0.2328042328042328,"[14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
450"
REFERENCES,0.2332852332852333,"and Wojciech Zaremba. 2016. Openai gym. arXiv preprint arXiv:1606.01540 (2016).
451"
REFERENCES,0.23376623376623376,"[15] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati,
452"
REFERENCES,0.23424723424723426,"and Angela P Schoellig. 2022. Safe learning in robotics: From learning-based control to safe
453"
REFERENCES,0.23472823472823473,"reinforcement learning. Annual Review of Control, Robotics, and Autonomous Systems 5 (2022),
454"
REFERENCES,0.2352092352092352,"411–444.
455"
REFERENCES,0.2356902356902357,"[16] Emma Brunskill, Bethany R Leffler, Lihong Li, Michael L Littman, and Nicholas Roy. 2009.
456"
REFERENCES,0.23617123617123617,"Provably efficient learning with typed parametric models. (2009).
457"
REFERENCES,0.23665223665223664,"[17] Mingyu Cai, Shaoping Xiao, Zhijun Li, and Zhen Kan. 2021. Optimal probabilistic motion
458"
REFERENCES,0.23713323713323714,"planning with potential infeasible LTL constraints. IEEE transactions on automatic control 68,
459"
REFERENCES,0.2376142376142376,"1 (2021), 301–316.
460"
REFERENCES,0.23809523809523808,"[18] Weiqin Chen, Dharmashankar Subramanian, and Santiago Paternain. 2024. Probabilistic
461"
REFERENCES,0.23857623857623858,"constraint for safety-critical reinforcement learning. IEEE Trans. Automat. Control (2024).
462"
REFERENCES,0.23905723905723905,"[19] Richard Cheng, Gábor Orosz, Richard M Murray, and Joel W Burdick. 2019. End-to-end safe
463"
REFERENCES,0.23953823953823955,"reinforcement learning through barrier functions for safety-critical continuous control tasks. In
464"
REFERENCES,0.24001924001924002,"Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 3387–3395.
465"
REFERENCES,0.2405002405002405,"[20] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. 2018. Risk-
466"
REFERENCES,0.240981240981241,"constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning
467"
REFERENCES,0.24146224146224146,"Research 18, 167 (2018), 1–51.
468"
REFERENCES,0.24194324194324193,"[21] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. 2018. Deep rein-
469"
REFERENCES,0.24242424242424243,"forcement learning in a handful of trials using probabilistic dynamics models. Advances in
470"
REFERENCES,0.2429052429052429,"neural information processing systems 31 (2018).
471"
REFERENCES,0.24338624338624337,"[22] Roberto Cipollone, Anders Jonsson, Alessandro Ronca, and Mohammad Sadegh Talebi. 2024.
472"
REFERENCES,0.24386724386724387,"Provably Efficient Offline Reinforcement Learning in Regular Decision Processes. Advances in
473"
REFERENCES,0.24434824434824434,"Neural Information Processing Systems 36 (2024).
474"
REFERENCES,0.24482924482924484,"[23] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
475"
REFERENCES,0.2453102453102453,"Tassa. 2018. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757
476"
REFERENCES,0.24579124579124578,"(2018).
477"
REFERENCES,0.24627224627224628,"[24] Giuseppe De Giacomo, Luca Iocchi, Marco Favorito, and Fabio Patrizi. 2020. Restraining
478"
REFERENCES,0.24675324675324675,"bolts for reinforcement learning agents. In Proceedings of the AAAI Conference on Artificial
479"
REFERENCES,0.24723424723424722,"Intelligence, Vol. 34. 13659–13662.
480"
REFERENCES,0.24771524771524772,"[25] Marc Deisenroth and Carl E Rasmussen. 2011. PILCO: A model-based and data-efficient
481"
REFERENCES,0.2481962481962482,"approach to policy search. In Proceedings of the 28th International Conference on machine
482"
REFERENCES,0.24867724867724866,"learning (ICML-11). 465–472.
483"
REFERENCES,0.24915824915824916,"[26] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. 2019. Challenges of real-world
484"
REFERENCES,0.24963924963924963,"reinforcement learning. arXiv preprint arXiv:1904.12901 (2019).
485"
REFERENCES,0.25012025012025013,"[27] Jie Fu and Ufuk Topcu. 2014. Probably approximately correct MDP learning and control with
486"
REFERENCES,0.2506012506012506,"temporal logic constraints. arXiv preprint arXiv:1404.7073 (2014).
487"
REFERENCES,0.2510822510822511,"[28] Javier Garcıa and Fernando Fernández. 2015. A comprehensive survey on safe reinforcement
488"
REFERENCES,0.25156325156325154,"learning. Journal of Machine Learning Research 16, 1 (2015), 1437–1480.
489"
REFERENCES,0.25204425204425207,"[29] M Giacobbe, Mohammadhosein Hasanbeig, Daniel Kroening, and Hjalmar Wijk. 2021. Shield-
490"
REFERENCES,0.25252525252525254,"ing atari games with bounded prescience. In Proceedings of the International Joint Conference
491"
REFERENCES,0.253006253006253,"on Autonomous Agents and Multiagent Systems, AAMAS.
492"
REFERENCES,0.2534872534872535,"[30] Alexander W Goodall and Francesco Belardinelli. 2023. Approximate Model-Based Shielding
493"
REFERENCES,0.25396825396825395,"for Safe Reinforcement Learning. arXiv preprint arXiv:2308.00707 (2023).
494"
REFERENCES,0.2544492544492544,"[31] Alexander W Goodall and Francesco Belardinelli. 2024. Leveraging Approximate Model-based
495"
REFERENCES,0.25493025493025495,"Shielding for Probabilistic Safety Guarantees in Continuous Environments. arXiv preprint
496"
REFERENCES,0.2554112554112554,"arXiv:2402.00816 (2024).
497"
REFERENCES,0.2558922558922559,"[32] David Ha and Jürgen Schmidhuber. 2018. Recurrent World Models Facilitate Policy Evo-
498"
REFERENCES,0.25637325637325636,"lution. In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach,
499"
REFERENCES,0.25685425685425683,"H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran As-
500"
REFERENCES,0.25733525733525736,"sociates, Inc.
https://proceedings.neurips.cc/paper_files/paper/2018/file/
501"
REFERENCES,0.25781625781625783,"2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf
502"
REFERENCES,0.2582972582972583,"[33] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and
503"
REFERENCES,0.2587782587782588,"James Davidson. 2019. Learning latent dynamics for planning from pixels. In International
504"
REFERENCES,0.25925925925925924,"conference on machine learning. PMLR, 2555–2565.
505"
REFERENCES,0.2597402597402597,"[34] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2023. Mastering diverse
506"
REFERENCES,0.26022126022126024,"domains through world models. arXiv preprint arXiv:2301.04104 (2023).
507"
REFERENCES,0.2607022607022607,"[35] Alexander Hans, Daniel Schneegass, Anton Schäfer, and Steffen Udluft. 2008. Safe Exploration
508"
REFERENCES,0.2611832611832612,"for Reinforcement Learning. 143–148.
509"
REFERENCES,0.26166426166426165,"[36] Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2018. Logically-
510"
REFERENCES,0.2621452621452621,"constrained reinforcement learning. arXiv preprint arXiv:1801.08099 (2018).
511"
REFERENCES,0.26262626262626265,"[37] Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2020. Cautious rein-
512"
REFERENCES,0.2631072631072631,"forcement learning with logical constraints. arXiv preprint arXiv:2002.12156 (2020).
513"
REFERENCES,0.2635882635882636,"[38] Mohammadhosein Hasanbeig, Daniel Kroening, and Alessandro Abate. 2020. Deep reinforce-
514"
REFERENCES,0.26406926406926406,"ment learning with temporal logics. In Formal Modeling and Analysis of Timed Systems: 18th
515"
REFERENCES,0.26455026455026454,"International Conference, FORMATS 2020, Vienna, Austria, September 1–3, 2020, Proceedings
516"
REFERENCES,0.265031265031265,"18. Springer, 1–22.
517"
REFERENCES,0.26551226551226553,"[39] Chloe He, Borja G León, and Francesco Belardinelli. [n.d.]. Do androids dream of electric
518"
REFERENCES,0.265993265993266,"fences? Safety-aware reinforcement learning with latent shielding. CEUR Workshop Proceed-
519"
REFERENCES,0.2664742664742665,"ings. https://ceur-ws.org/Vol-3087/paper_50.pdf
520"
REFERENCES,0.26695526695526695,"[40] Wassily Hoeffding. 1963. Probability Inequalities for Sums of Bounded Random Variables. J.
521"
REFERENCES,0.2674362674362674,"Amer. Statist. Assoc. 58, 301 (1963), 13–30.
522"
REFERENCES,0.26791726791726794,"[41] Weidong Huang, Jiaming Ji, Borong Zhang, Chunhe Xia, and Yaodong Yang. 2023. Safe Dream-
523"
REFERENCES,0.2683982683982684,"erV3: Safe Reinforcement Learning with World Models. arXiv preprint arXiv:2307.07176
524"
REFERENCES,0.2688792688792689,"(2023).
525"
REFERENCES,0.26936026936026936,"[42] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. 2018. Using
526"
REFERENCES,0.2698412698412698,"reward machines for high-level task specification and decomposition in reinforcement learning.
527"
REFERENCES,0.2703222703222703,"In International Conference on Machine Learning. PMLR, 2107–2116.
528"
REFERENCES,0.2708032708032708,"[43] Rodrigo Toro Icarte, Toryn Q Klassen, Richard Valenzano, and Sheila A McIlraith. 2022.
529"
REFERENCES,0.2712842712842713,"Reward machines: Exploiting reward function structure in reinforcement learning. Journal of
530"
REFERENCES,0.27176527176527177,"Artificial Intelligence Research 73 (2022), 173–208.
531"
REFERENCES,0.27224627224627224,"[44] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. When to trust your model:
532"
REFERENCES,0.2727272727272727,"Model-based policy optimization. Advances in neural information processing systems 32
533"
REFERENCES,0.27320827320827323,"(2019).
534"
REFERENCES,0.2736892736892737,"[45] Nils Jansen, Bettina Könighofer, Sebastian Junges, Alex Serban, and Roderick Bloem. 2020.
535"
REFERENCES,0.2741702741702742,"Safe reinforcement learning using probabilistic shields. In 31st International Conference on
536"
REFERENCES,0.27465127465127465,"Concurrency Theory (CONCUR 2020). Schloss-Dagstuhl-Leibniz Zentrum für Informatik.
537"
REFERENCES,0.2751322751322751,"[46] Nils Jansen, Bettina Könighofer, Sebastian Junges, Alexandru C Serban, and Roderick Bloem.
538"
REFERENCES,0.2756132756132756,"2018. Safe reinforcement learning via probabilistic shields. arXiv preprint arXiv:1807.06096
539"
REFERENCES,0.2760942760942761,"(2018).
540"
REFERENCES,0.2765752765752766,"[47] Sham Kakade, Michael J Kearns, and John Langford. 2003. Exploration in metric state spaces. In
541"
REFERENCES,0.27705627705627706,"Proceedings of the 20th International Conference on Machine Learning (ICML-03). 306–312.
542"
REFERENCES,0.2775372775372775,"[48] Michael Kearns and Satinder Singh. 2002. Near-optimal reinforcement learning in polynomial
543"
REFERENCES,0.278018278018278,"time. Machine learning 49 (2002), 209–232.
544"
REFERENCES,0.2784992784992785,"[49] Qingkai Liang, Fanyu Que, and Eytan Modiano. 2018. Accelerated primal-dual policy opti-
545"
REFERENCES,0.278980278980279,"mization for safe reinforcement learning. arXiv preprint arXiv:1802.06480 (2018).
546"
REFERENCES,0.27946127946127947,"[50] Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and
547"
REFERENCES,0.27994227994227994,"Michael Bowling. 2018. Revisiting the Arcade Learning Environment: Evaluation Protocols
548"
REFERENCES,0.2804232804232804,"and Open Problems for General Agents. Journal of Artificial Intelligence Research 61 (2018),
549"
REFERENCES,0.2809042809042809,"523–562.
550"
REFERENCES,0.2813852813852814,"[51] Sultan Javed Majeed, Marcus Hutter, et al. 2018. On Q-learning Convergence for Non-Markov
551"
REFERENCES,0.2818662818662819,"Decision Processes.. In IJCAI, Vol. 18. 2546–2552.
552"
REFERENCES,0.28234728234728235,"[52] Andreas Maurer and Massimiliano Pontil. 2009. Empirical bernstein bounds and sample
553"
REFERENCES,0.2828282828282828,"variance penalization. arXiv preprint arXiv:0907.3740 (2009).
554"
REFERENCES,0.2833092833092833,"[53] Stephen McIlvanna, Nhat Nguyen Minh, Yuzhu Sun, Mien Van, and Wasif Naeem. 2022.
555"
REFERENCES,0.2837902837902838,"Reinforcement learning-enhanced control barrier functions for robot manipulators. arXiv
556"
REFERENCES,0.2842712842712843,"preprint arXiv:2211.11391 (2022).
557"
REFERENCES,0.28475228475228476,"[54] Rohan Mitta, Hosein Hasanbeig, Jun Wang, Daniel Kroening, Yiannis Kantaros, and Alessandro
558"
REFERENCES,0.2852332852332852,"Abate. 2024. Safeguarded Progress in Reinforcement Learning: Safe Bayesian Exploration
559"
REFERENCES,0.2857142857142857,"for Control Policy Synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence,
560"
REFERENCES,0.28619528619528617,"Vol. 38. 21412–21419.
561"
REFERENCES,0.2866762866762867,"[55] Andrew Y Ng, Daishi Harada, and Stuart Russell. 1999. Policy invariance under reward
562"
REFERENCES,0.28715728715728717,"transformations: Theory and application to reward shaping. In Icml, Vol. 99. 278–287.
563"
REFERENCES,0.28763828763828764,"[56] Santiago Paternain, Miguel Calvo-Fullana, Luiz FO Chamon, and Alejandro Ribeiro. 2022. Safe
564"
REFERENCES,0.2881192881192881,"policies for reinforcement learning via primal-dual methods. IEEE Trans. Automat. Control 68,
565"
REFERENCES,0.2886002886002886,"3 (2022), 1321–1336.
566"
REFERENCES,0.2890812890812891,"[57] Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. 2020. A game theoretic framework for
567"
REFERENCES,0.2895622895622896,"model based reinforcement learning. In International conference on machine learning. PMLR,
568"
REFERENCES,0.29004329004329005,"7953–7963.
569"
REFERENCES,0.2905242905242905,"[58] Alex Ray, Joshua Achiam, and Dario Amodei. 2019. Benchmarking safe exploration in deep
570"
REFERENCES,0.291005291005291,"reinforcement learning. arXiv preprint arXiv:1910.01708 7, 1 (2019), 2.
571"
REFERENCES,0.29148629148629146,"[59] Alessandro Ronca and Giuseppe De Giacomo. 2021. Efficient PAC reinforcement learning in
572"
REFERENCES,0.291967291967292,"regular decision processes. arXiv preprint arXiv:2105.06784 (2021).
573"
REFERENCES,0.29244829244829246,"[60] Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. 2015. Safe exploration for
574"
REFERENCES,0.29292929292929293,"optimization with Gaussian processes. In International conference on machine learning. PMLR,
575"
REFERENCES,0.2934102934102934,"997–1005.
576"
REFERENCES,0.29389129389129387,"[61] Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between MDPs and semi-MDPs: A
577"
REFERENCES,0.2943722943722944,"framework for temporal abstraction in reinforcement learning. Artificial intelligence 112, 1-2
578"
REFERENCES,0.29485329485329487,"(1999), 181–211.
579"
REFERENCES,0.29533429533429534,"[62] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. 2018. Reward constrained policy opti-
580"
REFERENCES,0.2958152958152958,"mization. arXiv preprint arXiv:1805.11074 (2018).
581"
REFERENCES,0.2962962962962963,"[63] Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho
582"
REFERENCES,0.29677729677729675,"Hwang, Joseph E Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg. 2021. Recovery
583"
REFERENCES,0.2972582972582973,"rl: Safe reinforcement learning with learned recovery zones. IEEE Robotics and Automation
584"
REFERENCES,0.29773929773929775,"Letters 6, 3 (2021), 4915–4922.
585"
REFERENCES,0.2982202982202982,"[64] Garrett Thomas, Yuping Luo, and Tengyu Ma. 2021. Safe reinforcement learning by imagining
586"
REFERENCES,0.2987012987012987,"the near future. Advances in Neural Information Processing Systems 34 (2021), 13859–13869.
587"
REFERENCES,0.29918229918229916,"[65] Rodrigo Toro Icarte, Ethan Waldie, Toryn Klassen, Rick Valenzano, Margarita Castro, and
588"
REFERENCES,0.2996632996632997,"Sheila McIlraith. 2019. Learning reward machines for partially observable reinforcement
589"
REFERENCES,0.30014430014430016,"learning. Advances in neural information processing systems 32 (2019).
590"
REFERENCES,0.30062530062530063,"[66] Cameron Voloshin, Hoang Le, Swarat Chaudhuri, and Yisong Yue. 2022. Policy optimization
591"
REFERENCES,0.3011063011063011,"with linear temporal logic constraints. Advances in Neural Information Processing Systems 35
592"
REFERENCES,0.30158730158730157,"(2022), 17690–17702.
593"
REFERENCES,0.30206830206830204,"[67] Kim P Wabersich and Melanie N Zeilinger. 2018. Linear model predictive safety certification
594"
REFERENCES,0.30254930254930257,"for learning-based control. In 2018 IEEE Conference on Decision and Control (CDC). IEEE,
595"
REFERENCES,0.30303030303030304,"7130–7135.
596"
REFERENCES,0.3035113035113035,"[68] Kim Peter Wabersich and Melanie N Zeilinger. 2021. A predictive safety filter for learning-based
597"
REFERENCES,0.303992303992304,"control of constrained nonlinear dynamical systems. Automatica 129 (2021), 109597.
598"
REFERENCES,0.30447330447330445,"[69] Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. 2018. Safe exploration and opti-
599"
REFERENCES,0.304954304954305,"mization of constrained mdps using gaussian processes. In Proceedings of the AAAI Conference
600"
REFERENCES,0.30543530543530545,"on Artificial Intelligence, Vol. 32.
601"
REFERENCES,0.3059163059163059,"[70] Christopher KI Williams and Carl Edward Rasmussen. 2006. Gaussian processes for machine
602"
REFERENCES,0.3063973063973064,"learning. Vol. 2. MIT press Cambridge, MA.
603"
REFERENCES,0.30687830687830686,"[71] Eric M Wolff, Ufuk Topcu, and Richard M Murray. 2012. Robust control of uncertain Markov
604"
REFERENCES,0.30735930735930733,"decision processes with temporal logic specifications. In 2012 IEEE 51st IEEE Conference on
605"
REFERENCES,0.30784030784030786,"decision and control (CDC). IEEE, 3372–3379.
606"
REFERENCES,0.30832130832130833,"[72] Jorge Nocedal Stephen J Wright. 2006. Numerical optimization.
607"
REFERENCES,0.3088023088023088,"[73] Wenli Xiao, Yiwei Lyu, and John Dolan. 2023. Model-based Dynamic Shielding for Safe
608"
REFERENCES,0.30928330928330927,"and Efficient Multi-agent Reinforcement Learning. In Proceedings of the 2023 International
609"
REFERENCES,0.30976430976430974,"Conference on Autonomous Agents and Multiagent Systems. 1587–1596.
610"
REFERENCES,0.31024531024531027,"[74] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. 2020. Projection-
611"
REFERENCES,0.31072631072631074,"based constrained policy optimization. arXiv preprint arXiv:2010.03152 (2020).
612"
REFERENCES,0.3112073112073112,"[75] Wen-Chi Yang, Giuseppe Marra, Gavin Rens, and Luc De Raedt. 2023. Safe reinforcement
613"
REFERENCES,0.3116883116883117,"learning via probabilistic logic shields. arXiv preprint arXiv:2303.03226 (2023).
614"
REFERENCES,0.31216931216931215,"A
Algorithms
615"
REFERENCES,0.3126503126503126,"Algorithm 2 Exact Model Checking [9]
Input: model checking parameters (p, H), current state ⟨s, q⟩, current action a, product MC
Mπ ⊗D = (S × Q, P′, P′
0, {accept}, L′)
Output: true if Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1
Initialize zero vector x(0) ←0 with size |S| × |Q|
Initialize probability matrix A ←(P′(s, t))s,t̸∈accept (ignoring accepting states)
Initialize probability vector b ←(P′(s, accept))s̸∈accept (going to accepting states)
// Iterate over the model checking horizon
for i = 1, . . . , H do"
REFERENCES,0.31313131313131315,"Compute x(i) = Ax(i−1) + b
// Get the corresponding probability
Let X ←x⟨s,q⟩
If X < p return true else return false"
REFERENCES,0.3136123136123136,"Algorithm 3 Monte-Carlo Model Checking
Input: model checking parameters (ϵ, δ, p, H), current state ⟨s, q⟩, current action a, policy π,
labelling function L, DFA D = (Q, Σ, ∆, Q0, F) and (approximate) transition probabilities P
Output: true if Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1
Choose m ≥2/(ϵ2) log(2/δ)
for i = 1, . . . , m do"
REFERENCES,0.3140933140933141,"Set s0 ←s, q0 ←q and a0 ←a
// Sample a path through the model
for j = 1, . . . , H do"
REFERENCES,0.31457431457431456,"Sample next state sj ∼P(· | sj−1, aj−1),
Compute qj ←∆(qj−1, L(sj)),
Sample action aj ∼π(· | sj)
// Check if the path is accepting
Let Xi ←1 [qH ∈F]
// Construct probability estimate
Let e
X ←1"
REFERENCES,0.31505531505531503,"m
Pm
i=1 Xi
If e
X < p −ϵ return true else return false"
REFERENCES,0.31553631553631556,"Algorithm 4 Tabular Q-learning (Regular Safety Property) with Counter Factual Experiences [65]
Input: MDP M = (S, A, P, P0, R, AP, L), DFA D = (Q, Σ, ∆, Q0, F), discount factor γ ∈
(0, 1], learning rate α ∈(0, 1], temperature τ > 0, cost coefficient C and fixed episode length T
Initialize: (Q-table) ˆQ(s, q, a) ←0 ∀s ∈S, q ∈Q, a ∈A
for each episode do"
REFERENCES,0.31601731601731603,"Observe s0, L(s0) and q0 ←∆(Q0, L(s0))
for t = 0, . . . , T do"
REFERENCES,0.3164983164983165,"Sample action at from ⟨st, qt⟩using the Boltzmann policy derived from ˆQ with temp. τ
Play action at and observe st+1, L(st+1) and rt (reward is optional).
// Generate synthetic data by simulating all automaton transitions
for ¯q ∈Q do"
REFERENCES,0.31697931697931697,"Compute ¯q′ ←∆(q′, L(st+1))
Compute cost ¯c′ ←C · 1[¯q′ ∈F]
Compute done ←1[¯q′ ∈F]
// Q-learning step"
REFERENCES,0.31746031746031744,"ˆQ(st, ¯q, at) ←(1 −α) · ˆQ(st, ¯q, at) + α · (rt + ¯c′ + γ · done · maxa′∈A ˆQ(st+1, ¯q′, a′)
Compute qt+1 ←∆(qt, L(st+1)) and continue"
REFERENCES,0.3179413179413179,"Algorithm 5 DreamerV3 [34] with Shielding (Regular Safety Property)
Initialize: replay buffer D with S random episodes, world model parameters θ, ‘task policy’ πtask
and ‘backup policy’ πsafe randomly.
for each episode do"
REFERENCES,0.31842231842231844,"Observe o0, L(s0) and q0 ←∆(Q0, L(s0))
for t = 1, ..., T do"
REFERENCES,0.3189033189033189,"Sample action a ∼πtask from the task policy
// Estimate the reachability probability using the world model pθ
if Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1 then"
REFERENCES,0.3193843193843194,"Use proposed action
at ←a
else"
REFERENCES,0.31986531986531985,"// Override action
at ∼πsafe
Play action at and observe ot+1, L(st+1) and rt
Compute qt+1 ←∆(qt, L(st+1)),
Compute cost ct ←1[qt+1 ∈F]
Append (ot, at, rt, ct, ot+1) to the replay buffer D
if update then"
REFERENCES,0.3203463203463203,"// World model learning
Sample a batch B of transition sequences {(ot′, at′, rt′, ct′, ot′+1)} ∼D.
Update the world model parameters θ with maximum likelihood.
// Task policy optimization
‘Imagine’ sequences {ˆot′:t′+H, ˆrt′:t′+H, ˆct′:t′+H} with the ‘task policy’ πtask
Update the ‘task policy’ πtask with RL (to maximize reward).
Update the corresponding value critics with maximum likelihood
// Backup policy optimization
‘Imagine’ sequences {ˆot′:t′+H, ˆrt′:t′+H, ˆct′:t′+H} with the ‘backup policy’ πsafe
Update the ‘backup policy’ πsafe with RL (to minimize cost)
Update the corresponding value critics with maximum likelihood"
REFERENCES,0.32082732082732085,"B
Technical Details
616"
REFERENCES,0.3213083213083213,"B.1
Augmented Lagrangian
617"
REFERENCES,0.3217893217893218,"We first define the following objective functions,
618"
REFERENCES,0.32227032227032226,"JR(π)
=
Eπ "" T
X"
REFERENCES,0.32275132275132273,"t=0
R(st, at) # (1)"
REFERENCES,0.32323232323232326,"JC(π)
=
Eπ "" T
X"
REFERENCES,0.32371332371332373,"t=0
C(st, at) # (2) (3)"
REFERENCES,0.3241943241943242,"The augmented Lagrangian [72] is an adaptive penalty-based technique for the following constrained
619"
REFERENCES,0.3246753246753247,"optimization problem,
620"
REFERENCES,0.32515632515632514,"max
π
JR(π)
subject to
JC(π) ≤d
(4)"
REFERENCES,0.3256373256373256,"where d is some cost threshold. The corresponding Lagrangian is given by,
621"
REFERENCES,0.32611832611832614,"max
π
min
λ≥0

JR(π) −λ (JC(π) −d)

= max
π"
REFERENCES,0.3265993265993266,"JR(π)
if JC(π) < d
−∞
otherwise
(5)"
REFERENCES,0.3270803270803271,"The LHS is an equivalent form for the constrained optimization problem (RHS), since if π is feasible,
622"
REFERENCES,0.32756132756132755,"i.e. JC(π) < d then the maximum value for λ is λ = 0. If π is not feasible then λ can be arbitrarily
623"
REFERENCES,0.328042328042328,"large to solve this equation. Unfortunately this form of the objective function is non-smooth when
624"
REFERENCES,0.32852332852332855,"moving from feasible to infeasible policies, thus we introduce a proximal relaxation of the augmented
625"
REFERENCES,0.329004329004329,"Lagrangian [72],
626"
REFERENCES,0.3294853294853295,"max
π
min
λ≥0"
REFERENCES,0.32996632996632996,"
JR(π) −λ (JC(π) −d) + 1"
REFERENCES,0.33044733044733043,"µk
(λ −λk)2

(6)"
REFERENCES,0.3309283309283309,"where µk is a non-decreasing penalty multiplier dependent on the gradient step k. The new term
627"
REFERENCES,0.33140933140933143,"that has been introduced here encourages the λ to stay close to the previous value λk, resulting in a
628"
REFERENCES,0.3318903318903319,"smooth and differentiable function. The derivative w.r.t λ gives us the following gradient update step,
629"
REFERENCES,0.3323713323713324,"λk+1 =
λk + µk(JC(π) −d)
if λk + µk(JC(π) −d) ≥0
0
otherwise
(7)"
REFERENCES,0.33285233285233284,"At each gradient step, the penalty multiplier µk is updated in a non-decreasing way by using some
630"
REFERENCES,0.3333333333333333,"small fixed (power) parameter σ,
631"
REFERENCES,0.33381433381433384,"µk+1 = max{(µk)1+σ, 1}
(8)"
REFERENCES,0.3342953342953343,"The policy π is then updated by taking gradient steps of the following unconstrained objective,
632"
REFERENCES,0.3347763347763348,"˜J(π, λk, µk) = JR(π) −ΨC(π, λk, µk)"
REFERENCES,0.33525733525733525,"where,
633"
REFERENCES,0.3357383357383357,"ΨC(π, λk, µk) ="
REFERENCES,0.3362193362193362,"(
λk(JC(π) −d) + µk"
REFERENCES,0.3367003367003367,"2 (JC(π) −d)2
if λk + µk(JC(π) −d) ≥0"
REFERENCES,0.3371813371813372,−(λk)2
REFERENCES,0.33766233766233766,"2µk
otherwise"
REFERENCES,0.33814333814333813,"C
Technical Proofs
634"
REFERENCES,0.3386243386243386,"C.1
Proof of Proposition 3.4
635"
REFERENCES,0.33910533910533913,"Proposition 3.4 (restated) (Satisfaction probability for P H
safe). Let M and D be the MDP and
636"
REFERENCES,0.3395863395863396,"DFA from before (Defn. 3.3).
For a path ρ ∈Sω in the Markov chain, let traceH(ρ) =
637"
REFERENCES,0.3400673400673401,"L(ρ[0]), L(ρ[1]) . . . , L(ρ[H]) be the corresponding finite word over Σ = Pow(AP). For a given
638"
REFERENCES,0.34054834054834054,"state s ∈S the finite horizon satisfaction probability for Psafe is defined as follows,
639"
REFERENCES,0.341029341029341,"PrM(s |= P H
safe) := PrM(ρ ∈Sω | ρ[0] = s, traceH(ρ) ̸∈L(D))"
REFERENCES,0.3415103415103415,"where H ∈Z+ is some fixed model checking horizon. Similar to before, we show that the finite
640"
REFERENCES,0.341991341991342,"horizon satisfaction probability can be written as the following bounded reachability probability,
641"
REFERENCES,0.3424723424723425,"PrM(s |= P H
safe) = PrM⊗D(⟨s, qs⟩̸|= ♢≤Haccept)"
REFERENCES,0.34295334295334295,"where qs = ∆(Q0, L(s)) is as before and ♢≤Haccept is the corresponding step-bounded PCTL path
642"
REFERENCES,0.3434343434343434,"formula that reads, ‘eventually accept in H timesteps’.
643"
REFERENCES,0.3439153439153439,"Proof. Let Psafe be a regular safety property and let D = (Q, Σ, ∆, Q0, F) be the DFA such that
644"
REFERENCES,0.3443963443963444,"L(D) = BadPref(Psafe). We provide a formal definition for Psafe and the corresponding finite
645"
REFERENCES,0.3448773448773449,"horizon property P H
safe, respectively:
646"
REFERENCES,0.34535834535834536,"Psafe = {w ∈Σω | ∀wpref ∈Σωs.t. wpref ⪯w, wpref ̸∈L(D)}
(9)"
REFERENCES,0.34583934583934584,"P H
safe = {w ∈Σω | ∀wpref ∈Σωs.t. wpref ⪯w ∧|wpref| ≤H + 1, wpref ̸∈L(D)}
(10)"
REFERENCES,0.3463203463203463,"Let M = (S, P, P0, AP, L) be a Markov chain and consider the product Markov chain M ⊗D
647"
REFERENCES,0.3468013468013468,"from Defn. 3.2. For any path ρ = s0, s1, s2, . . ., there exists a unique run q0, q1, q2, . . . for the trace
648"
REFERENCES,0.3472823472823473,"trace(ρ) = L(s0), L(s1), L(s2) . . ., and denote,
649"
REFERENCES,0.3477633477633478,"ρ+ = ⟨s0, q0⟩, ⟨s1, q1⟩, ⟨s2, q2⟩. . .
(11)"
REFERENCES,0.34824434824434825,"where start state is ⟨s0, ∆(Q0, L(s0))⟩. Before we deal with probabilities let’s just consider a
650"
REFERENCES,0.3487253487253487,"fixed path ρ ∈Sω, the finite trace traceH(ρ) = L(ρ[0]), L(ρ[1]) . . . , L(ρ[H]), the unique run
651"
REFERENCES,0.3492063492063492,"q0, q1, q2, . . . , qH and the path ρ+ ∈Σω × Qω in the product Markov chain. We prove the following
652"
REFERENCES,0.3496873496873497,"statement,
653"
REFERENCES,0.3501683501683502,"ρ ̸|= P H
safe
if and only if
ρ+ |= ♢accept≤H
(12)"
REFERENCES,0.35064935064935066,"We start with the (→) direction, in particular, ρ ̸|= P H
safe if and only if traceH(ρ) ∈L(D). Recall
654"
REFERENCES,0.3511303511303511,"that by definition L(D) = {w ∈Σ∗| ∆∗(Q0, w) ∈F}, and so traceH(ρ) ∈L(D) implies that
655"
REFERENCES,0.3516113516113516,"qH = ∆∗(Q0, traceH(ρ)) ∈F, which by construction implies that ρ+ |= ♢accept≤H.
656"
REFERENCES,0.35209235209235207,"The opposite direction (←) is a little more involved, in particular, ρ+ |= ♢accept≤H implies that
657"
REFERENCES,0.3525733525733526,"for the unique run q0, q1, q2, . . . , qH there exists t ≤H such that qt ∈F. We notice that since
658"
REFERENCES,0.35305435305435307,"L(D) = BadPref(Psafe) then once the DFA reaches an accepting state it will remain in an accepting
659"
REFERENCES,0.35353535353535354,"state for the rest of the run. Therefore, qt ∈F for t ≤H implies that qH ∈F. Then by definition
660"
REFERENCES,0.354016354016354,"the trace traceH(ρ) that determined the unique run q0, q1, q2, . . . , qH must be in the language L(D),
661"
REFERENCES,0.3544973544973545,"which again by definition implies that ρ ̸|= P H
safe.
662"
REFERENCES,0.354978354978355,"We now deal with the probabilities. First we note that the DFA D does not affect the probabilities of
663"
REFERENCES,0.3554593554593555,"the product Markov chain – it can be shown that for every measurable set P of paths in M,
664"
REFERENCES,0.35594035594035595,"PrM(P) = PrM⊗A(ρ+ | ρ ∈P)
(13)"
REFERENCES,0.3564213564213564,"see [9]. It now remains to construct this set P in the proper way. In particular, if P is the set of paths
665"
REFERENCES,0.3569023569023569,"starting in some state s ∈S and that refute Psafe in the next H timesteps, i.e.,
666"
REFERENCES,0.35738335738335736,"P = {ρ ∈Sω | ρ[0] = s, {w′ ∈Σ∗| wpref ⪯trace(ρ) ∧|wpref| ≤H + 1} ∩L(D) ̸= ∅} (14)"
REFERENCES,0.3578643578643579,"and P + is defined as the set of paths starting from the corresponding state ⟨s, qs⟩(where qs =
667"
REFERENCES,0.35834535834535836,"∆(Q0, L(s))) in M ⊗D that eventually reach an accepting state of D in the next H steps, i.e.
668"
REFERENCES,0.3588263588263588,"P + = {ρ+ ∈(S × Q)ω | ρ+[0] = ⟨s, qs⟩∧ρ+ |= ♢≤Haccept}
(15)"
REFERENCES,0.3593073593073593,"Then by construction we have,
669"
REFERENCES,0.35978835978835977,"PrM(P) = PrM⊗D(ρ+ | ρ[0] = s, ρ ∈P) = PrM⊗D(P +)
(16)"
REFERENCES,0.3602693602693603,"Finally the probability PrM(P) and PrM(s |= P H
safe) are related as follows,
670"
REFERENCES,0.36075036075036077,"PrM(s |= P H
safe) = 1 −PrM(P)
(17)"
REFERENCES,0.36123136123136124,"= 1 −PrM⊗D(P +)
(18)"
REFERENCES,0.3617123617123617,"= 1 −PrM⊗D(⟨s, qs⟩|= ♢≤Haccept)
(19)"
REFERENCES,0.3621933621933622,"= PrM⊗D(⟨s, qs⟩̸|= ♢≤Haccept)
(20) 671"
REFERENCES,0.36267436267436265,"C.2
Proof of Proposition 4.2
672"
REFERENCES,0.3631553631553632,"Proposition 4.2 (restated). Let P T
safe denote the (episodic) regular safety property for a fixed episode
673"
REFERENCES,0.36363636363636365,"length T. Then satisfying Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1 for all t ∈[0, T] guarantees that
674"
REFERENCES,0.3641173641173641,"Pr(s0 |= P T
safe) ≥1 −p1 · ⌈T/H⌉, where s0 ∼P0 is the initial state.
675"
REFERENCES,0.3645983645983646,"Proof. Consider splitting up the episode in to ⌈T/H⌉chunks with length at most H.
Let
676"
REFERENCES,0.36507936507936506,"X0, X1, . . . X⌈T/H⌉−1 be the indicator random variables defined as follows,
677"
REFERENCES,0.3655603655603656,"Xi =
1
if ⟨si·H, qi·H⟩|= ♢≤Haccept
0
otherwise
(21)"
REFERENCES,0.36604136604136606,"Since Pr(⟨st, qt⟩|= ♢≤Haccept) ≤p1 for all t ∈[0, T] then the probability Pr(Xi = 1) ≤p1. By
678"
REFERENCES,0.3665223665223665,"construction we have,
679 if"
REFERENCES,0.367003367003367,"⌈T/H⌉−1
\"
REFERENCES,0.36748436748436747,"i=0
Xi = 0
then
s0 |= P T
safe
(22)"
REFERENCES,0.36796536796536794,"Intuitively we satisfy Psafe for the entire episode length if we never enter an accepting state in each of
680"
REFERENCES,0.36844636844636847,"the ⌈T/H⌉chunks. The final result is then obtained by taking a union bound as follows,
681"
REFERENCES,0.36892736892736894,"Pr(s0 |= P T
safe) ≥Pr  "
REFERENCES,0.3694083694083694,"⌈T/H⌉−1
\"
REFERENCES,0.3698893698893699,"i=0
Xi = 0 "
REFERENCES,0.37037037037037035,"
(23)"
REFERENCES,0.3708513708513709,= 1 −Pr  
REFERENCES,0.37133237133237135,"⌈T/H⌉−1
["
REFERENCES,0.3718133718133718,"i=0
Xi = 1 "
REFERENCES,0.3722943722943723,"
(24) ≥1 −"
REFERENCES,0.37277537277537276,"⌈T/H⌉−1
X"
REFERENCES,0.37325637325637323,"i=0
Pr(Xi = 1)
(25)"
REFERENCES,0.37373737373737376,"≥1 −p1 · ⌈T/H⌉
(26)
(27) 682"
REFERENCES,0.37421837421837423,"C.3
Proof of Proposition 5.4
683"
REFERENCES,0.3746993746993747,"Proposition 5.4 (restated). Let ϵ > 0, δ > 0, s ∈S be given. Under Assumption 5.2, we can obtain
684"
REFERENCES,0.37518037518037517,"an ϵ-approximate estimate for Pr(⟨s, q⟩|= ♢≤Haccept) with probability at least 1 −δ, by sampling
685"
REFERENCES,0.37566137566137564,"m ≥
1
2ϵ2 log
  2"
REFERENCES,0.37614237614237617,"δ

paths from the ‘black box’ model.
686"
REFERENCES,0.37662337662337664,"Proof. In words, we estimate Pr(⟨s, q⟩|= ♢≤Haccept) by sampling m paths from a ‘black box’
687"
REFERENCES,0.3771043771043771,"model of the environment dynamics. We label each path as satisfying or not and return the proportion
688"
REFERENCES,0.3775853775853776,"of satisfying traces as an estimate for Pr(⟨s, q⟩|= ♢≤Haccept). We proceed as follows, let ρ1, . . . ρm
689"
REFERENCES,0.37806637806637805,"be a sequence of paths sampled from the ‘black box’ model and let trace(ρ1), . . . trace(ρm) be the
690"
REFERENCES,0.3785473785473785,"corresponding traces. Furthermore, let X1, . . . , Xm be indicator r.v.s such that,
691"
REFERENCES,0.37902837902837905,"Xi =
1
if trace(ρ1) |= ♢≤Haccept,
0
otherwise
(28)"
REFERENCES,0.3795093795093795,"Recall that trace(ρ1) |= ♢≤Haccept can be checked in time O(poly(H)). Now let,
692 X = 1 m m
X"
REFERENCES,0.37999037999038,"i=1
Xi where E[X] = Pr(⟨s, q⟩|= ♢≤Haccept)
(29)"
REFERENCES,0.38047138047138046,"then by Hoeffding’s inequality [40],
693"
REFERENCES,0.38095238095238093,"P

|X −E[X]| ≥ϵ

≤2 exp
 
−2mϵ2
(30)"
REFERENCES,0.38143338143338146,"Bounding the RHS from above by δ and rearranging gives the desired result.
694"
REFERENCES,0.38191438191438193,"C.4
Proof of Proposition 5.5
695"
REFERENCES,0.3823953823953824,"We start by introducing the following lemma.
696"
REFERENCES,0.38287638287638287,"Lemma C.1 (Error amplification for trace distributions). Let bP ≈P be such that,
697"
REFERENCES,0.38335738335738334,"DT V

P(· | s), bP(· | s)

≤α ∀s ∈S
(31)"
REFERENCES,0.3838383838383838,"Let the start state s0 ∈S be given, and let Pt(·) and bPt(·) denote the path distribution (at time t) for
698"
REFERENCES,0.38431938431938434,"the two transition probabilities P and bP respectively. Then the total variation distance between the
699"
REFERENCES,0.3848003848003848,"two path distributions (at time t) are bounded as follows,
700"
REFERENCES,0.3852813852813853,"DT V

Pt(·), bPt(·)

≤αt ∀t
(32)"
REFERENCES,0.38576238576238575,"Proof. We will prove this fact by doing an induction on t. We recall that Pt(·) and bPt(·) denote the
701"
REFERENCES,0.3862433862433862,"path distribution (at time t) for the two transition probabilities P and bP respectively. Formally we
702"
REFERENCES,0.38672438672438675,"define them as follows,
703"
REFERENCES,0.3872053872053872,"Pt(ρ) = Pr(s0, . . . , st ⪯ρ | s0 = s, P)
(33)
bPt(ρ) = Pr(s0, . . . , st ⪯ρ | s0 = s, bP)
(34)"
REFERENCES,0.3876863876863877,"These probabilities read as follows, ‘the probability of the sequence s0, . . . , st ⪯ρ at time t’, or
704"
REFERENCES,0.38816738816738816,"similarly ‘the probability that the sequence s0, . . . , st is a prefix of ρ at time t’ Since the start state
705"
REFERENCES,0.38864838864838863,"s0 ∈S is given we note that,
706"
REFERENCES,0.3891293891293891,"P0(·) = bP0(·)
(35)"
REFERENCES,0.38961038961038963,"Before we continue with the induction on t we make the following observation, for any path ρ ∈Sω
707"
REFERENCES,0.3900913900913901,"we have by the triangle inequality,
708"
REFERENCES,0.39057239057239057,"Pt(ρ) −bPt(ρ)
 =
P(st | st−1)Pt−1(ρ) −bP(st | st−1) bPt−1(ρ)

(36)"
REFERENCES,0.39105339105339104,"≤Pt−1(ρ)
P(st | st−1) −bP(st | st−1)
 + bP(st | st−1)
Pt−1(ρ) −bPt−1(ρ)

(37)"
REFERENCES,0.3915343915343915,"Now we continue with the induction on t,
709"
REFERENCES,0.39201539201539204,"2DT V (Pt(·), bPt(·)) =
X ρ∈Sω"
REFERENCES,0.3924963924963925,"Pt(ρ) −bPt(ρ)

(38) ≤
X"
REFERENCES,0.392977392977393,"ρ∈Sω
Pt−1(ρ)
P(st | st−1) −bP(st | st−1) +
X"
REFERENCES,0.39345839345839345,"ρ∈Sω
bP(st | st−1)
Pt−1(ρ) −bPt−1(ρ)

(39) ≤
X"
REFERENCES,0.3939393939393939,"ρ∈Sω
Pt−1(ρ) · (2α) +
X ρ∈Sω"
REFERENCES,0.3944203944203944,"Pt−1(ρ) −bPt−1(ρ)

(40)"
REFERENCES,0.3949013949013949,"= 2α + 2DT V (Pt−1(·), bPt−1(·))
(41)
≤2αt
(42)"
REFERENCES,0.3953823953823954,"The final result is obtained by an induction on t where the base case comes from P0(·) = bP0(·).
710"
REFERENCES,0.39586339586339586,"Proposition 5.5 (restated). Let ϵ > 0, δ > 0, s ∈S and horizon H ≥1 be given. Under Assumption
711"
REFERENCES,0.39634439634439633,"5.3 we can make the following two statements:
712"
REFERENCES,0.3968253968253968,"(1) We can obtain an ϵ-approximate estimate for Pr(⟨s, q⟩|= ♢≤Haccept) with probability 1 by
713"
REFERENCES,0.39730639730639733,"exact model checking with the transition probabilities of bPπ in time O(poly(size(Mπ ⊗D)) · H).
714"
REFERENCES,0.3977873977873978,"(2) We can obtain an ϵ-approximate estimate for Pr(⟨s, q⟩|= ♢≤Haccept) with probability at least
715"
REFERENCES,0.39826839826839827,"1 −δ, by sampling m ≥
2
ϵ2 log
  2"
REFERENCES,0.39874939874939874,"δ

paths from the ‘approximate’ dynamics model bPπ.
716"
REFERENCES,0.3992303992303992,"Proof. We start by proving statement (1) and then statement (2) will follow quickly. First let
717"
REFERENCES,0.3997113997113997,"Pr(⟨s, q⟩|= ♢≤Haccept) and c
Pr(⟨s, q⟩|= ♢≤Haccept) denote the acceptance probabilities for the
718"
REFERENCES,0.4001924001924002,"two transition probabilities P and bP respectively. We also let g(·) and bg(·) denote the average trace
719"
REFERENCES,0.4006734006734007,"distribution (over the next H timesteps) for the two transition probabilities P and bP respectively,
720"
REFERENCES,0.40115440115440115,"where,
721"
REFERENCES,0.4016354016354016,"g(ρ) = 1 H H
X"
REFERENCES,0.4021164021164021,"t=1
Pt(ρ)
(43)"
REFERENCES,0.4025974025974026,"bg(ρ) = 1 H H
X"
REFERENCES,0.4030784030784031,"t=1
bPt(ρ)
(44)"
REFERENCES,0.40355940355940356,"Before we continue with the proof of (1) we make the following observations,
722"
REFERENCES,0.40404040404040403,"• max
⟨s,q⟩"
REFERENCES,0.4045214045214045,"Pr(⟨s, q⟩|= ♢≤Haccept) −c
Pr(⟨s, q⟩|= ♢≤Haccept)
 ≤1
723"
REFERENCES,0.405002405002405,"• Let f(x) : x ∈X →[0, 1] be a real-valued function. Let P1(·) and P2(·) be probability
724"
REFERENCES,0.4054834054834055,"distributions over the space X, then.
725
Ex∼P1(·)[f(x)] −Ex∼P2(·)[f(x)]
 ≤DT V (P1(·), P2(·))"
REFERENCES,0.40596440596440597,"We continue by showing the following,
726"
REFERENCES,0.40644540644540644,"Pr(⟨s, q⟩|=♢≤Haccept) −c
Pr(⟨s, q⟩|= ♢≤Haccept)

(45)"
REFERENCES,0.4069264069264069,"=
Eρ∼g

1

⟨s, q⟩|= ♢≤Haccept

−Eρ∼bg

1

⟨s, q⟩|= ♢≤Haccept
 
(46)"
REFERENCES,0.4074074074074074,"≤DT V (g(·), bg(·))
(47) = 1 2 X"
REFERENCES,0.4078884078884079,"ρ∈Sω
|g(ρ) −bg(ρ)|
(48)"
REFERENCES,0.4083694083694084,"=
1
2H X ρ∈Sω  H
X"
REFERENCES,0.40885040885040885,"t=1
Pt(ρ) −bPt(ρ) (49)"
REFERENCES,0.4093314093314093,"≤
1
2H H
X t=1  X"
REFERENCES,0.4098124098124098,"ρ∈Sω
Pt(ρ) −bPt(ρ) (50)"
REFERENCES,0.4102934102934103,"≤
1
2H H
X"
REFERENCES,0.4107744107744108,"t=1
H(ϵ/H)
(51)"
REFERENCES,0.41125541125541126,"= ϵ/2
(52)
(53)"
REFERENCES,0.41173641173641173,"The first inequality (Eq. 47) comes from our earlier observations. The second inequality (Eq. 50) is
727"
REFERENCES,0.4122174122174122,"straightforward and the final inequality (Eq. 51) is obtained by applying Lemma C.1 and Assumption
728"
REFERENCES,0.4126984126984127,"5.3. We note that this result is similar to the simulation lemma [48], which has been proved many
729"
REFERENCES,0.4131794131794132,"times for several different settings [1, 16, 47, 57].
730"
REFERENCES,0.4136604136604137,"This concludes the proof of statement (1), since we have shown that c
Pr(⟨s, q⟩|= ♢≤Haccept) is an
731"
REFERENCES,0.41414141414141414,"ϵ/2-approximate estimate of Pr(⟨s, q⟩|= ♢≤Haccept), under the Assumption 5.3.
732"
REFERENCES,0.4146224146224146,"The proof of statement (2) follows quickly. We have established that,
733"
REFERENCES,0.4151034151034151,"Pr(⟨s, q⟩|= ♢≤Haccept) −c
Pr(⟨s, q⟩|= ♢≤Haccept)
 ≤ϵ/2
(54)"
REFERENCES,0.4155844155844156,"It remains to obtain an ϵ/2-approximate estimate of c
Pr(⟨s, q⟩|= ♢≤Haccept). By using the
734"
REFERENCES,0.4160654160654161,"same reasoning as in the proof of Proposition 5.4. We can obtain an ϵ/2-approximate estimate
735"
REFERENCES,0.41654641654641655,"of c
Pr(⟨s, q⟩|= ♢≤Haccept) by sampling m paths, ρ1, . . . ρm, from the approximate dynamics model
736"
REFERENCES,0.417027417027417,"bP. Then provided,
737 m ≥2"
REFERENCES,0.4175084175084175,"ϵ2 log
2 δ"
REFERENCES,0.41798941798941797,"
(55)"
REFERENCES,0.4184704184704185,"with probability 1 −δ we can obtain ϵ/2-approximate estimate of c
Pr(⟨s, q⟩|= ♢≤Haccept) and by
738"
REFERENCES,0.41895141895141896,"extension an ϵ-approximate estimate of Pr(⟨s, q⟩|= ♢≤Haccept). This concludes the proof.
739"
REFERENCES,0.41943241943241943,"C.5
Proof of Theorem 6.5
740"
REFERENCES,0.4199134199134199,"Theorem 6.5 (restated). Under Assumption 6.3 and 6.4, and provided that every state action pair
741"
REFERENCES,0.4203944203944204,"(s, a) ∈S × A has been visited at least O

H2|S|2"
REFERENCES,0.4208754208754209,"ϵ2
log

|A||S|2"
REFERENCES,0.4213564213564214,"δ

times. Then with probability 1 −δ
742"
REFERENCES,0.42183742183742184,"the system satisfies the constraints of Problem 4.1, independent of the ‘task policy’.
743"
REFERENCES,0.4223184223184223,"Proof. We split the proof up in to three parts, (1), (2) and (3). In part (1) we show that the given
744"
REFERENCES,0.4227994227994228,"sample complexity bound gives us an approximate model of the environment dynamics with high
745"
REFERENCES,0.42328042328042326,"probability. In part (2) we use our assumptions to reason about the probabilistic recoverability
746"
REFERENCES,0.4237614237614238,"of the system when it enters a critical state. In part (3) we put everything together and deal with
747"
REFERENCES,0.42424242424242425,"approximation error ϵ the remaining failure probability that are both unavoidable for the statistical
748"
REFERENCES,0.4247234247234247,"model checking procedures used to shield the system.
749"
REFERENCES,0.4252044252044252,"(1)
We show that the following holds with probability 1 −δ/2,
750"
REFERENCES,0.42568542568542567,"DT V

Pπ(· | s), bPπ(· | s)

≤ϵ/H ∀s ∈S
(56)"
REFERENCES,0.4261664261664262,"when every state action pair (s, a) ∈S × A has been visited at least,
751"
REFERENCES,0.42664742664742666,"O
H2|S|2"
REFERENCES,0.42712842712842713,"ϵ2
log
|A||S|2 δ "
REFERENCES,0.4276094276094276,"times. First we let #(s, a) denote the total number of times that (s, a) has been observed, similarly
752"
REFERENCES,0.4280904280904281,"we let #(s′, s, a) denote the total number of times that (s′, s, a) has been observed. The maximum
753"
REFERENCES,0.42857142857142855,"likelihood estimate for the unknown probability P(s′ |, s, a) is bP(s′ | s, a) = #(s′, s, a)/#(s, a).
754"
REFERENCES,0.4290524290524291,"Let us fix some (s, a) ∈S × A, and s′ ∈S, we let ps′ = P(s′ | s, a) denote the true probability of
755"
REFERENCES,0.42953342953342954,"transitioning to s′ from (s, a) and we let ˆps′ = #(s′, s, a)/#(s, a) denote our estimate. We note that
756"
REFERENCES,0.43001443001443,"E[ˆps′] = ps′, i.e. ˆps′ is an unbiased estimator for ps′. Let m = #(s, a) also be the number of times
757"
REFERENCES,0.4304954304954305,"that (s, a) has been observed, then by Hoeffding’s inequality [40] we have,
758"
REFERENCES,0.43097643097643096,"P

|ps′ −ˆps′| ≥
ϵ
H|S|"
REFERENCES,0.4314574314574315,"
≤2 exp

−2m
ϵ2"
REFERENCES,0.43193843193843195,H2|S|2
REFERENCES,0.4324194324194324,"
(57)"
REFERENCES,0.4329004329004329,"Bounding the LHS from above by 1 −δ/2(|A||S|2) and rearranging gives the following lower bound
759"
REFERENCES,0.43338143338143337,"for m,
760"
REFERENCES,0.43386243386243384,m ≥H2|S|2
REFERENCES,0.43434343434343436,"2ϵ2
log
4|A||S|2 δ"
REFERENCES,0.43482443482443484,"
(58)"
REFERENCES,0.4353054353054353,"Taking a union bound over all (s′, s, a) ∈S × S × A, then for all state action pairs (s, a) ∈S × A
761"
REFERENCES,0.4357864357864358,"we have the following with probability at least 1 −δ.
762"
DT V,0.43626743626743625,"2DT V

P(· | s, a), bP(· |, s, a)

=
X"
DT V,0.4367484367484368,"s′∈S
|ps′ −ˆps′| ≤
X s′∈S"
DT V,0.43722943722943725,"ϵ
H|S| ≤ϵ/H
(59)"
DT V,0.4377104377104377,"Now fix some s ∈S and we observe the following,
763"
DT V,0.4381914381914382,"2DT V

Pπ(· | s), bPπ(· | s)

=
X"
DT V,0.43867243867243866,"s′∈S
|Pπ(s′ | s) −bPπ(s′ | s)|
(60) =
X s′∈S X"
DT V,0.43915343915343913,"a∈A
|P(s′ | s, a)π(a | s) −bP(s′ | s, a)π(a | s)|
(61) =
X"
DT V,0.43963443963443966,"a∈A
π(a | s)
X"
DT V,0.4401154401154401,"s′∈S
|P(s′ | s, a) −bP(s′ | s, a)|
(62) =
X"
DT V,0.4405964405964406,"a∈A
π(a | s)2DT V

P(· | s, a), bP(· |, s, a)

(63)"
DT V,0.44107744107744107,"≤ϵ/H
(64)"
DT V,0.44155844155844154,"Thus with probability at least 1 −δ/2 we have for all s ∈S that,
764"
DT V,0.44203944203944207,"DT V

Pπ(· | s), bPπ(· | s)

≤ϵ/H
(65)"
DT V,0.44252044252044254,"(2)
Using Assumption 6.3 and 6.4 we can argue about the safety of the system. Suppose firstly,
765"
DT V,0.443001443001443,"that we can check the condition Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1, precisely and without any failure
766"
DT V,0.4434824434824435,"probability (we will deal with statistical model checking in part (3)). From any non-critical state we
767"
DT V,0.44396344396344395,"can transition arbitrarily to a critical state, although under Assumption 6.3 this critical state is not
768"
DT V,0.4444444444444444,"irrecoverable with probability ≥p1. We now consider the following two cases:
769"
DT V,0.44492544492544495,"(i) Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1 under the ‘task’ policy.
770"
DT V,0.4454064454064454,"(ii) Pr(⟨s, q⟩|= ♢≤Haccept) > p1 under the ‘task’ policy.
771"
DT V,0.4458874458874459,"For case (i) we can safely use the ‘task’ policy and return to a non-critical state within H timesteps
772"
DT V,0.44636844636844636,"with probability at least 1 −p1. For case (ii) we deploy the ‘safe’ policy and under Assumption 6.4
773"
DT V,0.44684944684944683,"we can return to a non-critical state within H timesteps with probability at least 1 −p1. We have now
774"
DT V,0.44733044733044736,"established an invariant, since from every non-critical state we can return to a non-critical state with
775"
DT V,0.4478114478114478,"probability 1 −p1 and thus satisfy Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1 at every timestep t ∈[0, T].
776"
DT V,0.4482924482924483,"(3)
We now make a similar argument but for the statistical model checking procedure where we
777"
DT V,0.44877344877344877,"can only obtain an ϵ-approximate estimate for the probability Pr(⟨s, q⟩|= ♢≤Haccept) with high
778"
DT V,0.44925444925444924,"probability. Let us denote our ϵ-approximate estimate c
Pr(⟨s, q⟩|= ♢≤Haccept), rather than check
779"
DT V,0.4497354497354497,"the condition Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1, we can check condition c
Pr(⟨s, q⟩|= ♢≤Haccept) ≤
780"
DT V,0.45021645021645024,"p1 −ϵ, and if c
Pr(⟨s, q⟩|= ♢≤Haccept) is indeed an ϵ-approximate estimate then this guarantees
781"
DT V,0.4506974506974507,"Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1. Consider the following two cases:
782"
DT V,0.4511784511784512,"(i) Our estimate c
Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1 −ϵ
783"
DT V,0.45165945165945165,"(ii) Our estimate c
Pr(⟨s, q⟩|= ♢≤Haccept) > p1 −ϵ
784"
DT V,0.4521404521404521,"For case (i) we can safely use the ‘task’ policy and return to a non-critical state within H timesteps
785"
DT V,0.45262145262145265,"with probability at least 1 −p1. For case (ii) we deploy the ‘safe’ policy and under Assumption 6.4
786"
DT V,0.4531024531024531,"we can return to a non-critical state within H timesteps with probability at least 1 −p1. Again we
787"
DT V,0.4535834535834536,"have established an invariant, since from every non-critical state we can return to a non-critical state
788"
DT V,0.45406445406445406,"with probability 1 −p1 and thus satisfy Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1 at every timestep t ∈[0, T].
789"
DT V,0.45454545454545453,"We still need to deal with the failure probability of the statistical model checking procedure at
790"
DT V,0.455026455026455,"each timestep, by choosing failure probability 1 −δ/2T we can guarantee (by a union bound) an
791"
DT V,0.4555074555074555,"ϵ-approximate estimate for each timestep with probability 1 −δ/2. Finally, taking a union bound
792"
DT V,0.455988455988456,"over part (1) and (2) gives the desired total failure probability 1 −δ.
793 794"
DT V,0.45646945646945647,"D
Environment Details
795"
DT V,0.45695045695045694,"D.1
Colour Gridworld
796"
DT V,0.4574314574314574,"Figure 4: Colour gridworld en-
vironment. Top left hand cor-
ner (agent) is the start posi-
tion. The agent must navigate
to the goal position in the bot-
tom right hand corner of grid-
world. The coloured states la-
belled blue, green and purple
correspondingly."
DT V,0.45791245791245794,"The colour gridworld environment is a simple 9 × 9 grid, with
797"
DT V,0.4583934583934584,"state space |S| = 81 and action space |A| = 5, where each action
798"
DT V,0.4588744588744589,"corresponds to the following movements: Left,Right, Up, Down, Stay.
799"
DT V,0.45935545935545935,"The objective is to navigate from the start state in one corner of the
800"
DT V,0.4598364598364598,"grid, to the goal state in the other corner, after reaching the goal state
801"
DT V,0.4603174603174603,"the agent is then sent back to the start state. The agent must navigate
802"
DT V,0.4607984607984608,"to the goal state as many times as possible in a fixed episode length
803"
DT V,0.4612794612794613,"of T = 1000. The reward function is a sparse reward that gives the
804"
DT V,0.46176046176046176,"agent +1 reward for reaching the goal and 0 otherwise. When the
805"
DT V,0.46224146224146223,"environment is fully deterministic the maximum achievable reward
806"
DT V,0.4627224627224627,"is 58.
807"
DT V,0.46320346320346323,"In addition to the goal state, there are three other distinct states,
808"
DT V,0.4636844636844637,"green, blue and purple, each labelled with their corresponding
809"
DT V,0.46416546416546417,"colours, see Fig. 4. The set of atomic propositions is thus AP =
810"
DT V,0.46464646464646464,"{green, blue, purple, goal}, the safety properties are specified over
811"
DT V,0.4651274651274651,"the set AP, in particular we conduct experiments with 3 different
812"
DT V,0.4656084656084656,"safety properties of increasing complexity:
813"
DT V,0.4660894660894661,"• (1) □¬green
814"
DT V,0.4665704665704666,"• (2) □goal→♢≤10blue
815"
DT V,0.46705146705146705,"• (3) □goal→♢≤10□≤5purple
816"
DT V,0.4675324675324675,"Property (1) is a simple invariant property Pinv(¬green) that states the green state must always be
817"
DT V,0.468013468013468,"avoided. Property (2) and (3) are more complex safety properties that interfere with the goal state. In
818"
DT V,0.4684944684944685,"particular, property (2) states that once the goal state is reached then the blue state must be reached
819"
DT V,0.468975468975469,"within 10 steps, this actually has no direct consequences on the maximum reward achievable but may
820"
DT V,0.46945646945646946,"interfere with convergence as the goal state seemingly leads to a high penalty if the blue state is not
821"
DT V,0.46993746993746993,"reached.
822"
DT V,0.4704184704184704,"Property (3) states that once the goal state is reached then the purple state must be reached within 10
823"
DT V,0.4708994708994709,"steps and then purple must hold for the next 5 timesteps. In safety property both interferes with the
824"
DT V,0.4713804713804714,"goal and has direct consequences on the maximum achievable reward as staying in purple for 5 steps
825"
DT V,0.47186147186147187,"does not lead to progress towards the goal state. In terms of the size of the DFA |Q|, property (1) is
826"
DT V,0.47234247234247234,"an invariant so the cost function is Markov and the size of the DFA is 2, for property (2) and (3) the
827"
DT V,0.4728234728234728,"size of the DFA is 12 and 62 respectively.
828"
DT V,0.4733044733044733,Table 2: Safety properties and p value
DT V,0.4737854737854738,"property
rand. act. p"
DT V,0.4742664742664743,"(1) □¬green
0.25
(2) □goal→♢≤10blue
0.25
(3) □goal→♢≤10□≤5purple
0.1"
DT V,0.47474747474747475,"Each of the safety properties are tested with the cor-
829"
DT V,0.4752284752284752,"responding p value for the environment, detailed in
830"
DT V,0.4757094757094757,"Table 1, which is repeated here for reference. The p
831"
DT V,0.47619047619047616,"value corresponds to the level of stochasticity in the
832"
DT V,0.4766714766714767,"environment. In particular, if p = 0.25 then there is
833"
DT V,0.47715247715247716,"a 25% chance of the agents action being overridden
834"
DT V,0.47763347763347763,"with another random action chosen uniformly. Given
835"
DT V,0.4781144781144781,"the environment is stochastic then it is difficult to satisfy the safety properties with probability 1.
836"
DT V,0.4785954785954786,"Through preliminary statistical analysis we computed the maximum satisfaction probabilities for
837"
DT V,0.4790764790764791,"each property, to help inform an appropriate p value to test with. With p = 0.25, property (1) can be
838"
DT V,0.47955747955747957,"satisfies with very high probability close to 1, while still achieving maximum reward. With p = 0.25
839"
DT V,0.48003848003848004,"property (2) can be satisfied with probability ≈0.93 while still achieving maximum reward. With
840"
DT V,0.4805194805194805,"p = 0.1 property (3) can be satisfied with probability ≈0.75 while still achieving good reward.
841"
DT V,0.481000481000481,"Hyperparameter settings.
We discuss some of the hyperparameter settings for our shielding
842"
DT V,0.48148148148148145,"approach that are not detailed in Table 5.
843"
DT V,0.481962481962482,"Property (1): we use a model checking horizon of H = 3, and probability threshold p1 = 1.0, with
844"
DT V,0.48244348244348245,"the number of samples m = 4096, we can obtain a roughly ϵ = 0.05 approximate estimate of the
845"
DT V,0.4829244829244829,"finite horizon satisfaction probability with failure probability δ = 0.01.
846"
DT V,0.4834054834054834,"Property (2): we use a model checking horizon of H = 10, and probability threshold p1 = 0.9, with
847"
DT V,0.48388648388648386,"the number of samples m = 8192, we can obtain a roughly ϵ = 0.05 approximate estimate of the
848"
DT V,0.4843674843674844,"finite horizon satisfaction probability with a smaller failure probability δ = 0.001.
849"
DT V,0.48484848484848486,"Property (3): again we use a model checking horizon of H = 10, and probability threshold p1 = 0.6,
850"
DT V,0.48532948532948533,"with the number of samples m = 1024, we can obtain roughly a ϵ = 0.1 approximate estimate of the
851"
DT V,0.4858104858104858,"finite horizon satisfaction probability with failure probability δ = 0.01.
852"
DT V,0.4862914862914863,"Extended discussion of results. First we provide slightly larger figures that than provided in the
853"
DT V,0.48677248677248675,"main paper, see Figure 5.
854"
DT V,0.48725348725348727,"In general we observe that our shielding method is able to effectively trade-off reward and safety, in
855"
DT V,0.48773448773448774,"all cases converging to a system that obtains superior or comparable performance with the baseline.
856"
DT V,0.4882154882154882,"For property (1) we might expect our method to be able to recover the optimal policy that avoids the
857"
DT V,0.4886964886964887,"green state, it is clear in this case that the shielding procedure has harmed convergence and perhaps
858"
DT V,0.48917748917748916,"further investigation and hyperparameter tuning will encourage improvements. For property (2) and
859"
DT V,0.4896584896584897,"(3) the results are what we expect – we can recover the best policy that satisfies the step-wise bounded
860"
DT V,0.49013949013949015,"safety property with the desired probability p1.
861"
DT V,0.4906204906204906,"The intuitive reason for why simply penalising Q-learning doesn’t work, is that tuning the cost
862"
DT V,0.4911014911014911,"coefficient C is challenging for stochastic environments, where safety cannot be enforced ‘almost
863"
DT V,0.49158249158249157,"surely’ (with probability 1), and the precise value of C offers little to no semantic meaning. For
864"
DT V,0.49206349206349204,"different levels of stochasticity p values it is hard to know what desired level of safety we can achieve
865"
DT V,0.49254449254449256,"while still converging to a high reward policy, making tuning C even harder without knowing more
866"
DT V,0.49302549302549303,"about the structure of the environment. In Appendix F we study more closely the effect of C and
867"
DT V,0.4935064935064935,"p. Furthermore, we note te sensitivity of our method to the chosen model checking horizon H. In
868"
DT V,0.493987493987494,"particular, if H is too large we might expect the system to be overly conservative, we also address
869"
DT V,0.49446849446849445,"this in more detail in Appendix F.
870"
DT V,0.494949494949495,Figure 5: Episode reward and cost for tabular RL ‘colour’ gridworld environment.
DT V,0.49543049543049544,"D.2
Atari Seaquest
871"
DT V,0.4959114959114959,"Figure 6: Atari Seaquest environment
[10, 50]. The goal is to rescue divers
(small blue people), while shooting en-
emy sharks and submarines."
DT V,0.4963924963924964,"Our DreamerV3 [34] based shielding procedure is tested
872"
DT V,0.49687349687349686,"on Atari Seaquest, provided as part of the Arcade Learn-
873"
DT V,0.4973544973544973,"ing Environment (ALE)[10, 50]. Seaquest is a partially
874"
DT V,0.49783549783549785,"observable environment meaning we do not have direct
875"
DT V,0.4983164983164983,"access to the underlying state space S, we are however
876"
DT V,0.4987974987974988,"provided with observations o ∈O as pixel images which
877"
DT V,0.49927849927849927,"correspond to 64 × 64 × 3 tensors. Fortunately Dream-
878"
DT V,0.49975949975949974,"erV3 is specifically designed to operate in visual settings
879"
DT V,0.5002405002405003,"and is able to effectively learn a predictive world model
880"
DT V,0.5007215007215007,"that closely approximate the environment dynamics. The
881"
DT V,0.5012025012025012,"action space of Seaquest is finite, specifically |A| = 18,
882"
DT V,0.5016835016835017,"where each action corresponds to a joystick movement and fire button interaction. Rewards are
883"
DT V,0.5021645021645021,"obtained by ‘shooting’ an enemy shark or submarine, or by rescuing divers and returning them to the
884"
DT V,0.5026455026455027,"surface. In addition, the agent must manage its oxygen resources and avoid being hit by sharks and
885"
DT V,0.5031265031265031,"the enemy submarines which fire back, see Fig. 6. The environment is also made stochastic by using
886"
DT V,0.5036075036075036,"‘sticky actions’ [50], where the agents previous action is repeated with probability p = 0.25.
887"
DT V,0.5040885040885041,"In terms of safety properties we experiment with the following two properties,
888"
DT V,0.5045695045695046,"• (1) (□¬surface→□(surface→diver)) ∧(□¬out-of-oxygen) ∧(□¬hit)
889"
DT V,0.5050505050505051,"• (2) □diver ∧¬surface→♢≤30surface
890"
DT V,0.5055315055315055,"Property (1) states that after diving (i.e. not surface), the agent must only surface with a diver on
891"
DT V,0.506012506012506,"board, and never run out-of-oxygen and never get hit by an enemy. The size of the DFA for this
892"
DT V,0.5064935064935064,"property is |D| = 4. Property (2) states that once a diver is on board the agent must surface within 30
893"
DT V,0.506974506974507,"timesteps (i.e. rescue the diver).
894"
DT V,0.5074555074555075,"Hyperparameter settings. For our shielding approach almost all the hyperparameters are specified
895"
DT V,0.5079365079365079,"in Appendix E. The only hyperparameter that varies is the model checking horizon H. For property
896"
DT V,0.5084175084175084,"(1) we use H = 30, empirically this seems adequate enough to avoid running out-of-oxygen and
897"
DT V,0.5088985088985089,"begin surfacing in enough time. For property (2) we use H = 50, this is to avoid picking up a diver
898"
DT V,0.5093795093795094,"at the bottom of the ocean where it may not be possible to return to the surface in 30 timesteps.
899"
DT V,0.5098605098605099,"Extended discussion of results. First we provide slightly larger figures that than provided in the
900"
DT V,0.5103415103415103,"main paper, see Figure 7
901"
DT V,0.5108225108225108,Figure 7: Episode reward and violation rate for deep RL Atari Seaquest.
DT V,0.5113035113035113,"For both safety properties DreamerV3 with shielding obtains comparative performance in terms of
902"
DT V,0.5117845117845118,"reward with the unmodified DreamerV3 baseline. Of course this baseline entirely ignores the safety
903"
DT V,0.5122655122655123,"properties and simply maximizes reward. We remark on the differences between the safety properties
904"
DT V,0.5127465127465127,"themselves, property (1) in particular specifies the natural safety properties of the environment, since
905"
DT V,0.5132275132275133,"violating property (1) results in a death, the agent only start with 4 lives (and can gain one more ever
906"
DT V,0.5137085137085137,"10000 points) and so satisfying property (1) is beneficial for long term reward, short the behaviour
907"
DT V,0.5141895141895142,"satisfying property (1) is correlated with higher reward and we might expect the globally optimal
908"
DT V,0.5146705146705147,"policy in the environment to never violated property (1). Property (2) specifies that once a diver is
909"
DT V,0.5151515151515151,"recovered the submarine must return to the surface in 30 timesteps, we would not expect that the
910"
DT V,0.5156325156325157,"globally optimal policy satisfies this property (2) rather we would expect to converge to a locally
911"
DT V,0.5161135161135161,"optimal policy satisfying property (2) while still obtaining good reward.
912"
DT V,0.5165945165945166,"With respect to the baseline DreamerV3 (LAG) which has access to the cost function, we see that in
913"
DT V,0.517075517075517,"both cases it fails to reliable learn a safe policy that simultaneously maximizes reward. For property
914"
DT V,0.5175565175565175,"(2) DreamerV3 (LAG) appear to do slightly better in terms of safety, however when qualitatively
915"
DT V,0.5180375180375181,"inspecting the runs for property (2) we see the DreamerV3 (LAG) agent intentionally get hit by
916"
DT V,0.5185185185185185,"enemy submarines/sharks to re-spawn on the surface without actually having to navigate there. This
917"
DT V,0.518999518999519,"may be a more effective way to satisfy the safety property with high probability but it clearly leads to
918"
DT V,0.5194805194805194,"worse long term reward.
919"
DT V,0.51996151996152,"E
Hyperparameters & Implementation Details
920"
DT V,0.5204425204425205,"E.1
Access to Code
921"
DT V,0.5209235209235209,"To maintain a high standard of anonymity we provide code for the experiments run on ‘colour’
922"
DT V,0.5214045214045214,"gridworld as supplementary material, rather than through GitHub. The colour gridworld environment
923"
DT V,0.5218855218855218,"is implemented with the Gym [14] interface. Tabular Q-learning is implemented with numpy in Python,
924"
DT V,0.5223665223665224,"the model checking procedures (both exact and Monte Carlo) are implemented with JAX [12] which
925"
DT V,0.5228475228475229,"supports vectorized computation on GPU and CPU. The code for the Atari Seaquest experiments
926"
DT V,0.5233285233285233,"are not currently available, although our code base was heavily derived from the code base for
927"
DT V,0.5238095238095238,"Approximate Model-based Shielding (AMBS) [30], see https://github.com/sacktock/AMBS
928"
DT V,0.5242905242905243,"(MIT License).
929"
DT V,0.5247715247715248,"Training details.
For collecting both sets of experiments we has access to 2 Nvidia Tesla A30
930"
DT V,0.5252525252525253,"(24GB RAM) GPU and a 24-core/48 thread Intel Xeon CPU each with 32GB RAM. For the ‘colour’
931"
DT V,0.5257335257335257,"gridworld experiments each run can take several minutes up to a day depending on which property is
932"
DT V,0.5262145262145262,"being tested, for example one run for property (3) can take roughly 1.5 days as the product state space
933"
DT V,0.5266955266955267,"is fairly large. For the Atari Seaquest experiments each run can take 8 hours to 1 day depending on
934"
DT V,0.5271765271765272,"the precise configuration of DreamerV3, in general we see a slow down of ×2 when using shielding
935"
DT V,0.5276575276575276,"compared to the unmodified DreamerV3 baseline. Memory requirements may differ depending on
936"
DT V,0.5281385281385281,"the DreamerV3 configuration used, for the xlarge DreamerV3 configuration 32GB of GPU memory
937"
DT V,0.5286195286195287,"should suffice.
938"
DT V,0.5291005291005291,"Statistical significance. Error bars are provided for each of our experiments. In particular, we report
939"
DT V,0.5295815295815296,"5 random initializations (seeds) for each experiment, the error bars are non-parametric (bootstrap) 95%
940"
DT V,0.53006253006253,"confidence intervals, provided by seaborn.lineplot with default parameters: errorbar=(‘ci’,
941"
DT V,0.5305435305435305,"95), n_boot=1000. The error bars capture the randomness in the initialization of the DreamerV3
942"
DT V,0.5310245310245311,"world model and policy parameters, the randomness of the environment and any randomness in the
943"
DT V,0.5315055315055315,"batch sampling.
944"
DT V,0.531986531986532,"E.2
Colour Gridworld
945"
DT V,0.5324675324675324,"Table 3: Q-learning
Name
Symbol
value"
DT V,0.532948532948533,"Learning rate
α
0.1
Discount factor
γ
0.95
Exploration type
-
Boltzmann
Temperature
τ
0.05"
DT V,0.5334295334295335,"Table 4: Q-learning with counter factual experiences [43]
Name
Symbol
value"
DT V,0.5339105339105339,"Learning rate
α
0.1
Discount factor
γ
0.95
Exploration type
-
Boltzmann
Temperature
τ
0.05
Cost coefficient
C
10.0"
DT V,0.5343915343915344,"Table 5: Q-learning with shielding (Algorithm 1)
Name
Symbol
value"
DT V,0.5348725348725348,"Model checking type
-
Monte-Carlo
Approximate model
-
True
Shielding
-
Task
Number of samples
m
varies
Approximation error
ϵ
varies
Failure probability
δ
varies
Model checking horizon
H
varies
Satisfaction prob.
p
varies
Prior
-
uninformative"
DT V,0.5353535353535354,"‘Task policy’ πtask
See Q-learning (Table 3)
..."
DT V,0.5358345358345359,"‘Backup policy’ πsafe
See Q-learning with counter factual experiences (Table 4)
..."
DT V,0.5363155363155363,"E.3
Atari Seaquest
946"
DT V,0.5367965367965368,"Table 6: DreamerV3 [34]
Name
Symbol
value"
DT V,0.5372775372775372,General
DT V,0.5377585377585378,"Replay capacity
|D|
106
Batch size
|B|
16
Batch length
-
64
Number of envs
-
8
Train ratio
-
64
Number of MLP layers
-
5
Number of MLP units
-
1024
Activation
-
LayerNorm + SiLU"
DT V,0.5382395382395382,World Model
DT V,0.5387205387205387,"Configuration size
-
medium
Number of latents
-
32
Classes per latent
-
32
Number of layers
-
3
Number of hidden units
-
640
Number of recurrent units
-
1024
CNN depth
-
48
RSSM loss scales
βpred, βdyn, βrep
1.0, 0.5, 0.1
Predictor loss scales
βo, βr, βc, βγ
1.0, 1.0, 1.0, 1.0
Learning rate
-
10−4"
DT V,0.5392015392015392,"Adam epsilon
ϵadam
10−8
Gradient clipping
-
1000"
DT V,0.5396825396825397,Actor Critic
DT V,0.5401635401635402,"Imagination horizon
H
15
Discount factor
γ
0.997
TD lambda
λ
0.95
Critic EMA decay
-
0.98
Critic EMA regularizer
-
1
Return norm. scale
Sreward
Per(R, 95) −Per(R, 5)
Return norm. limit
Lreward
1
Return norm. decay
-
0.99
Actor entropy scale
ηactor
3 · 10−4"
DT V,0.5406445406445406,"Learning rate
-
3 · 10−5"
DT V,0.5411255411255411,"Adam epsilon
ϵadam
10−5
Gradient clipping
-
100"
DT V,0.5416065416065416,"Table 7: Augmented Lagrangian [7, 41, 72]
Name
Symbol
value"
DT V,0.5420875420875421,Augmented Lagrangian
DT V,0.5425685425685426,"Penalty multiplier
µk
5 · 10−9"
DT V,0.543049543049543,"Initial Lagrange multiplier
λk
0.01
Penalty power
σ
10−6
Cost coefficient
C
1.0
Cost threshold
d
1.0"
DT V,0.5435305435305435,Penalty Critic
DT V,0.5440115440115441,"See ‘Actor Critic’ in Table 6
..."
DT V,0.5444925444925445,"Table 8: DreamerV3 with Shielding (Algorithm 5)
Name
Symbol
value"
DT V,0.544973544973545,Shielding
DT V,0.5454545454545454,"Approximation error
ϵ
0.09
Number of samples
m
512
Failure probability
δ
0.01
Look-ahead/shielding horizon
H
varies
Satisfaction prob.
p
0.9
Cost coefficient
C
10"
DT V,0.5459355459355459,‘Task policy’
DT V,0.5464165464165465,"See ‘Actor Critic’ in Table 6
..."
DT V,0.5468975468975469,‘Backup policy’
DT V,0.5473785473785474,"See ‘Actor Critic’ in Table 6
..."
DT V,0.5478595478595478,"F
Ablation Studies
947"
DT V,0.5483405483405484,"In this section we provide several ablation studies for the ‘colour’ gridworld environment. We test the
948"
DT V,0.5488215488215489,"most significant hyperparameters and algorithmic components of our method including the baseline
949"
DT V,0.5493025493025493,"(Q-learning with penalties). In particular we demonstrate the counter factual experiences is crucial
950"
DT V,0.5497835497835498,"for learning the safety properties of the environment when the size of the corresponding DFA is non
951"
DT V,0.5502645502645502,"trivial. We also experiment with using exact model checking – demonstrating that we don’t loose
952"
DT V,0.5507455507455508,"much by using statistical model checking procedures. Furthermore, we experiment with the cost
953"
DT V,0.5512265512265512,"coefficient C, the model checking horizon H and the level of stochasticity p.
954"
DT V,0.5517075517075517,"F.1
Counter factual experiences
955"
DT V,0.5521885521885522,"We run our method and the baseline (Q-learning with penalties) without counterfactual experiences
956"
DT V,0.5526695526695526,"to train the ‘backup policy’ or penalized task policy (baseline).
957"
DT V,0.5531505531505532,"Figure 8: Episode reward and cost for Q-learning
(Shield) and Q-learning (COST-CF) with and with-
out counterfactual experiences (CF)."
DT V,0.5536315536315536,"Figure 9: Episode reward and cost for Shield
(Exact-True) – exact model checking with the ‘true’
probabilities, Shield (Exact-Approx) - exact model
checking with the learning transition probabilities,
and Shield (MC-Approx) – from the main paper."
DT V,0.5541125541125541,"For property (2) and (3) we see a significant
958"
DT V,0.5545935545935546,"drop in safety performance, since learning to
959"
DT V,0.555074555074555,"respect the safety property over the much larger
960"
DT V,0.5555555555555556,"product state space will require much more ex-
961"
DT V,0.556036556036556,"perience and without exploiting the structure of
962"
DT V,0.5565175565175565,"the DFA (using counter factual experiences) to
963"
DT V,0.556998556998557,"generate synthetic data the task behaviour will
964"
DT V,0.5574795574795575,"be much more quickly learnt. For property (1),
965"
DT V,0.557960557960558,"the invariant property, we observe identical per-
966"
DT V,0.5584415584415584,"formance as the DFA is trivial (only 2 states),
967"
DT V,0.5589225589225589,"and so counter factual experiences is essentially
968"
DT V,0.5594035594035595,"redundant in this case.
969"
DT V,0.5598845598845599,"F.2
Exact model checking
970"
DT V,0.5603655603655604,"We run our method (Shielding) with two differ-
971"
DT V,0.5608465608465608,"ent configurations: exact model checking with
972"
DT V,0.5613275613275613,"the ‘approximate’ transition probabilities (learn-
973"
DT V,0.5618085618085618,"ing from experience) and exact model check-
974"
DT V,0.5622895622895623,"ing with the ‘true’ transition probabilities. We
975"
DT V,0.5627705627705628,"compare these two methods to the configuration
976"
DT V,0.5632515632515632,"used in the main paper: Monte Carlo (statisti-
977"
DT V,0.5637325637325638,"cal) model checking with the learned transition
978"
DT V,0.5642135642135642,"probabilities.
979"
DT V,0.5646945646945647,"In all cases we see that Shield (MC-Approx)
980"
DT V,0.5651755651755652,"obtains almost identical performance to Shield
981"
DT V,0.5656565656565656,"(Exact-True), which demonstrates that we don’t
982"
DT V,0.5661375661375662,"loose much by statistical model checking with
983"
DT V,0.5666185666185666,"the learned probabilities, when for example we
984"
DT V,0.5670995670995671,"don’t have access to the transition probabilities
985"
DT V,0.5675805675805676,"ahead of time, or the MDP is too large to ex-
986"
DT V,0.568061568061568,"act model check. We see some variance with
987"
DT V,0.5685425685425686,"Shield (Exact-Approx), which can be explained
988"
DT V,0.569023569023569,"by sub-optimal convergence in terms of reward,
989"
DT V,0.5695045695045695,"although note that the safety performance is con-
990"
DT V,0.56998556998557,"sistent with the other configurations. Perhaps ex-
991"
DT V,0.5704665704665705,"act model checking with an inaccurate model of
992"
DT V,0.570947570947571,"the transition probabilities restricts exploration
993"
DT V,0.5714285714285714,"to areas of the state space that are actually safe.
994"
DT V,0.5719095719095719,"F.3
Cost coefficient C
995"
DT V,0.5723905723905723,"We experiment with different values for the cost coefficient C used for our baseline (Q-learning with
996"
DT V,0.5728715728715729,"penalties). In particular, we use C ∈{0.1, 1.0, 10.0, 100.0}, we expect that a larger cost coefficient
997"
DT V,0.5733525733525734,"will penalize unsafe behaviour more harshly and result in ‘safer’ behaviour (i.e., fewer safety-property
998"
DT V,0.5738335738335738,"violations).
999"
DT V,0.5743145743145743,"Figure 10: Episode reward and cost for Q-learning
(COST-CF) – baseline from the main paper, with
different cost coefficients C."
DT V,0.5747955747955747,"Unsurprisingly, across the board, by increasing
1000"
DT V,0.5752765752765753,"the cost coefficient C we obtain a policy that has
1001"
DT V,0.5757575757575758,"fewer safety-property violations. The improved
1002"
DT V,0.5762385762385762,"‘safety performance’ is of course at the expense
1003"
DT V,0.5767195767195767,"of reward or task performance, this is a trade-off
1004"
DT V,0.5772005772005772,"we would expect. In particular for C = 100.0
1005"
DT V,0.5776815776815777,"we see that the learned policy essentially avoids
1006"
DT V,0.5781625781625782,"the goal state (achieving zero reward) all but
1007"
DT V,0.5786435786435786,"guaranteeing safety (no safety-violations). The
1008"
DT V,0.5791245791245792,"purpose of this ablation study is to demonstrate
1009"
DT V,0.5796055796055796,"that while we can achieve any desired level of
1010"
DT V,0.5800865800865801,"safety by tuning the cost coefficient C, the actual
1011"
DT V,0.5805675805675806,"value of C offers little to no semantic meaning
1012"
DT V,0.581048581048581,"for the probability of violating the safety prop-
1013"
DT V,0.5815295815295816,"erty.
1014"
DT V,0.582010582010582,"F.4
Model checking horizon H
1015"
DT V,0.5824915824915825,"As was alluded to in the main paper, our method
1016"
DT V,0.5829725829725829,"can be very sensitive to the model checking hori-
1017"
DT V,0.5834535834535834,"zon (hyperparameter) H. In particular, if H is
1018"
DT V,0.583934583934584,"too large then we might expect the system to
1019"
DT V,0.5844155844155844,"exhibit overly conservative behaviour. As a rule of thumb we suggest that H should be set to roughly
1020"
DT V,0.5848965848965849,"the shortest path in the DFA from the initial state to an accepting state – this can easily be computed
1021"
DT V,0.5853775853775853,"by using Dijkstra’s (shortest-path) algorithm. In this ablation we experiment with much larger H
1022"
DT V,0.5858585858585859,"than recommended. This significantly impacts the performance of our proposed approach. However,
1023"
DT V,0.5863395863395864,"we do propose a solution, Q-learning (Shield-Rec) which in short, checks that the action proposed by
1024"
DT V,0.5868205868205868,"the ‘task policy’ is recoverable with the ‘backup policy’, or in other words by playing with the action
1025"
DT V,0.5873015873015873,"a ∼πtask proposed by the ‘task policy’ We can still satisfy Pr(⟨s, q⟩|= ♢≤Haccept) ≤p1 by using
1026"
DT V,0.5877825877825877,"the ‘backup policy’ after playing a.
1027"
DT V,0.5882635882635883,"Figure 11: Episode reward and cost for Q-learning
(Shield) - from the main paper, Q-learning (Shield)
with bigger H and Q-learning (Shield-Rec) with
bigger H."
DT V,0.5887445887445888,"In general we observe that when H is too large
1028"
DT V,0.5892255892255892,"our original method (Shield) is overly conser-
1029"
DT V,0.5897065897065897,"vative, sacrificing reward or task performance
1030"
DT V,0.5901875901875901,"for safety guarantees. Our proposed solution
1031"
DT V,0.5906685906685907,"(Shield-Rec) is alleviates this issue partly, pro-
1032"
DT V,0.5911495911495912,"viding reasonable safety performance and com-
1033"
DT V,0.5916305916305916,"parable task performance. We note that this
1034"
DT V,0.5921115921115921,"solution is clearly not perfect as is it appears
1035"
DT V,0.5925925925925926,"to be slightly more permissive allowing more
1036"
DT V,0.5930735930735931,"safety-violations than necessary. More investi-
1037"
DT V,0.5935545935545935,"gation into this framework would be interesting
1038"
DT V,0.594035594035594,"future work, and perhaps more hyperparameter
1039"
DT V,0.5945165945165946,"tuning, specifically by tuning p1, could improve
1040"
DT V,0.594997594997595,"this method. The goal would be to obtain an al-
1041"
DT V,0.5954785954785955,"gorithm that is not overly sensitive to H, and as
1042"
DT V,0.5959595959595959,"long as H is sufficiently big to guarantee safety
1043"
DT V,0.5964405964405964,"we don’t see much performance degradation by
1044"
DT V,0.596921596921597,"further increasing H.
1045"
DT V,0.5974025974025974,"F.5
Level of stochasticity p
1046"
DT V,0.5978835978835979,"Finally we investigate the effect of the level of stochasticity of the environment. Specifically, the value
1047"
DT V,0.5983645983645983,"p corresponding the the probability that the agent’s action is ignored and another action is chosen
1048"
DT V,0.5988455988455988,"(uniformly at random) from the action space and played instead. For example, of p = 0.25 and the
1049"
DT V,0.5993265993265994,"agent chooses the action Right, there is a 75% chance that the agent goes right and a 25% chance
1050"
DT V,0.5998075998075998,"the agent goes a different direction. If p = 0.0 (deterministic environment) then achieving complete
1051"
DT V,0.6002886002886003,"safety (zero-violations) becomes easier as the agent has complete control of the environment through
1052"
DT V,0.6007696007696007,"their actions.
1053"
DT V,0.6012506012506013,"Figure 12: Episode reward and cost for Q-learning,
Q-learning (COST-CF) and Q-learning (Shield) –
all from the main paper. With smaller levels of
stochasticity p"
DT V,0.6017316017316018,"We experiment with the following p values: p =
1054"
DT V,0.6022126022126022,"0.1 for property (1), p = 0.1 for property (2)
1055"
DT V,0.6026936026936027,"and p = 0.05 for property (3). For these smaller
1056"
DT V,0.6031746031746031,"p values we would expect it to be easier for
1057"
DT V,0.6036556036556037,"our methods including the baseline to achieve
1058"
DT V,0.6041366041366041,"a higher-rate of safety and possibly complete
1059"
DT V,0.6046176046176046,"safety in some cases.
1060"
DT V,0.6050986050986051,"We see a similar situation as in the main paper,
1061"
DT V,0.6055796055796056,"Q-learning (without penalties) simply finds the
1062"
DT V,0.6060606060606061,"best policy ignoring costs. However, Q-learning
1063"
DT V,0.6065416065416065,"(with penalties) is able to obtain the same perfor-
1064"
DT V,0.607022607022607,"mance now as our method Q-learning (Shield),
1065"
DT V,0.6075036075036075,"both in terms of reward and cost. With a smaller
1066"
DT V,0.607984607984608,"p value the safety-property can be satisfied with
1067"
DT V,0.6084656084656085,"higher probability while still visiting the goal
1068"
DT V,0.6089466089466089,"state frequently and obtaining high reward. In
1069"
DT V,0.6094276094276094,"particular, these p values are chosen such that
1070"
DT V,0.60990860990861,"each of the safety properties can be satisfies with
1071"
DT V,0.6103896103896104,"probability at least 0.9 from the goal state, thus
1072"
DT V,0.6108706108706109,"penalizing safety-violations with C = 10.0 ap-
1073"
DT V,0.6113516113516113,"pears to be enough to guarantee safety above
1074"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6118326118326118,"0.9 at each timestep while still achieving high
1075"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6123136123136124,"reward. For different values of C we might expect the baseline to have a different performance
1076"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6127946127946128,"profile.
1077"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6132756132756133,"G
Comparison to CMDP
1078"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6137566137566137,"In this additional section we analyze the relationships between our problem setup and other common
1079"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6142376142376142,"CMDP settings, for both the finite horizon and corresponding (discounted) infinite horizon problems.
1080"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6147186147186147,"G.1
Finite Horizon
1081"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6151996151996152,"For reference we restate Problem 4.1 here.
1082"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6156806156806157,"Problem 4.1 (restated) (Step-wise bounded regular safety property constraint). Let Psafe be a regular
1083"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6161616161616161,"safety property, D be the DFA such that L(D) = BadPref(Psafe) and M be the MDP;
1084"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6166426166426167,"max
π
Vπ
subject to
Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t ∈[0, T]"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6171236171236171,"where all probability is taken under the product Markov Chain Mπ ⊗D, p1 ∈[0, 1] is a probability
1085"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6176046176046176,"threshold, H is the model checking horizon and T is the fixed episode length.
1086"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6180856180856181,"G.1.1
Expected Cumulative Constraint
1087"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6185666185666185,"First we restate Problem 4.4.
1088"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6190476190476191,"Problem 4.4 (restated) (Expected cumulative constraint [4, 58])."
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6195286195286195,"max
π
Vπ
subject to
E⟨st,qt⟩∼Mπ⊗D
hPT
t=0 C(⟨st, qt⟩)
i
≤d1"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.62000962000962,"where d1 ∈R+ is the cost threshold and T is the fixed episode length.
1089"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6204906204906205,"Proposition G.1. A feasible policy π for Problem 4.1 with parameters p1 ∈[0, 1] is also a feasible
1090"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.620971620971621,"policy for Problem 4.4 with parameter d1 ∈R+, provided that d1 ≥(T + 1) · p1.
1091"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6214526214526215,"Proof. For t ∈[0, T] we define, the following random variables, X0, . . . , XT , where
1092"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6219336219336219,"Xt = C(⟨st, qt⟩) = 1 [accept ∈L′(⟨st, qt⟩)]
(66)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6224146224146224,"where,
1093"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.622895622895623,"E [Xt] = E [1 [accept ∈L′(⟨st, qt⟩)]]
(67)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6233766233766234,"= Pr (accept ∈L′(⟨st, qt⟩))
(68)
≤p1
(69)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6238576238576239,"The argument is straightforward if at every timestep t ∈[0, T] we have Pr(⟨st, qt⟩|= ♢≤Haccept) ≤
1094"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6243386243386243,"p1 then with probability ≤p1 we have accept ∈L(⟨st, qt⟩). Then, under mild assumptions
1095"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6248196248196248,"(i.e. C(⟨st, qt⟩) < ∞) we consider the following decomposition of the expected cumulative cost,
1096 Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6253006253006252,"t=0
C(⟨st, qt⟩) # = Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6257816257816258,"t=0
Xt # (70)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6262626262626263,"= Es0∼P0(·) [X0] + Es1∼P1(·) [X1] + . . . + EsT ∼PT (·) [XT ]
(71)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6267436267436267,"= Eπ [X0] + Eπ [X1] + . . . + Eπ [XT ]
(72)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6272246272246272,"We replace the subscript ‘⟨st, qt⟩∼Mπ ⊗D’ here for brevity. Clearly by linearity of expectations
1097"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6277056277056277,"this statement holds. Although it is worth noting that each expectation is taken under a different
1098"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6281866281866282,"marginal state distribution (i.e. Pt(·)), which depends on π (apart from the initial state distribution
1099"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6286676286676287,"P0(·)). From now on we will write this is implicitly (i.e. Eq. 72), rather than writing the marginal
1100"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6291486291486291,"state distribution (at time t) for each expectation. Using our earlier observations we can now bound
1101"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6296296296296297,"the expected cumulative cost from above as follows,
1102 Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6301106301106301,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6305916305916306,"= Eπ [X0] + Eπ [X1] + . . . + Eπ [XT −1] + Eπ [XT ]
(73)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6310726310726311,"≤(T + 1) · p1
(74) 1103"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6315536315536315,"Proposition G.2. The converse is not strictly true, since there may be a feasible policy π for Problem
1104"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6320346320346321,"4.4 with threshold d1 ≤(T + 1) · p1 which does not satisfy the constraints of Problem 4.1.
1105"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6325156325156325,"Proof. We want to prove the following statement, a policy π satisfying,
1106 Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.632996632996633,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6334776334776335,"≤(T + 1) · p1
(75)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6339586339586339,"does not imply that,
1107"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6344396344396345,"Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t ∈[0, T]
(76)
To prove this we will show that there may be some policy π that satisfies Eq. 75, but does not satisfy
1108"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6349206349206349,"Eq. 76 at some timestep t. For simplicity we consider the first timestep (i.e. t = 0). First we assume
1109"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6354016354016354,"π is such that Eq. 75 holds, assuming H ≤T then clearly we have,
1110 Eπ "" H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6358826358826358,"t=0
C(⟨st, qt⟩) # ≤Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6363636363636364,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6368446368446369,"≤(T + 1) · p1
(77)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6373256373256373,"Let Pr(⟨s0, q0⟩|= ♢≤Haccept) denote the proportion of accepting paths from the initial state
1111"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6378066378066378,"s0 ∼P0(·) and automaton state q0 = ∆(Q0, L(s0)). Suppose π is such that Pr(⟨s0, q0⟩|=
1112"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6382876382876382,"♢≤Haccept) > p1. We note that for each path ρ ∈Sω and corresponding trace(ρ) ∈Σω such that
1113"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6387686387686388,"trace(ρ) |= ♢≤Haccept the sum PH
t=0 C(⟨st, qt⟩) ≥1, and now we have,
1114"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6392496392496393,"(T + 1) · p1 ≥Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6397306397306397,"t=0
C(⟨st, qt⟩) # ≥Eπ "" H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6402116402116402,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6406926406926406,"> p1
(78)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6411736411736412,"Now clearly for all p1 ∈[0, 1] and T ∈Z+ the following holds,
1115"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6416546416546417,"p1 < (T + 1) · p1
(79)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6421356421356421,"This implies that there may exist some π satisfying Eq. 75 and such that Pr(⟨s0, q0⟩
|=
1116"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6426166426166426,"♢≤Haccept) > p1, i.e. does not satisfy Eq. 76 at timestep t = 0.
1117"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6430976430976431,"Proposition G.3. A feasible policy π for Problem 4.4 with threshold d1 ≤p1, satisfies Pr(⟨st, qt⟩|=
1118"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6435786435786436,"♢≤Haccept) ≤p1 for all t ∈[0, T]. This bound is tight.
1119"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6440596440596441,"Proof. Firstly, a feasible policy π for Problem 4.4 with threshold d1 ≤p1 clearly satisfies,
1120 Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6445406445406445,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.645021645021645,"≤p1
(80)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6455026455026455,"Assuming H ≤T, then this implies that for all t′ ∈[0, T −H] we have,
1121 Eπ "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.645983645983646,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6464646464646465,"t=t′
C(⟨st, qt⟩)  ≤Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6469456469456469,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6474266474266475,"≤p1
(81)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6479076479076479,"Let Pr(⟨st′, qt′⟩|= ♢≤Haccept) denote the proportion of accepting paths at timestep t′, where st′ ∼
1122"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6483886483886484,"Pt′(·). Here Pt′(·) denotes the marginal state distribution at time t′. Recall that for each path ρ ∈Sω
1123"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6488696488696488,"and corresponding trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept the sum Pt′+H
t=t′ C(⟨st, qt⟩) ≥1.
1124"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6493506493506493,"Without loss of generality fix some t′ ∈[0, T −H] and suppose that Pr(⟨st′, qt′⟩|= ♢≤Haccept) >
1125"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6498316498316499,"p1. This implies that,
1126 Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6503126503126503,"t=0
C(⟨st, qt⟩) # ≥Eπ "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6507936507936508,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6512746512746512,"t=t′
C(⟨st, qt⟩) "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6517556517556518,"> p1
(82)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6522366522366523,"Which is a contradiction. Therefore, it must be the case that when Eq. 80 is satisfied then so is
1127"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6527176527176527,"Pr(⟨st, qt⟩|= ♢≤Haccept]) ≤p1 for all t ∈[0, T −H]. For the remaining t′ ∈[T −H, T] a similar
1128"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6531986531986532,"argument can be made, the only detail is to ensure the sum in Eq. 81 is up to T rather than t′ + H.
1129"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6536796536796536,"To prove that this bound is tight we can again show the possible existence of a counter example. In
1130"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6541606541606542,"particular, we want to prove the following statement, a policy π satisfying,
1131 Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6546416546416547,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6551226551226551,"≤p1 + c
(83)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6556036556036556,"for some constant c > 0, does not imply that,
1132"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.656084656084656,"Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t ∈[0, T]
(84)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6565656565656566,"We will show that there may exist some policy π that satisfies Eq. 83 but does not satisfy Eq. 84 at
1133"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6570466570466571,"some timestep t. Firstly, we assume π is such that Eq. 83 holds, this implies that for all t′ ∈[0, T −H]
1134"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6575276575276575,"we have,
1135 Eπ "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.658008658008658,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6584896584896585,"t=t′
C(⟨st, qt⟩)  ≤Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.658970658970659,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6594516594516594,"≤p1 + c
(85)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6599326599326599,"Fix some t′ ∈[0, T −H] and once again let Pr(⟨st′, qt′⟩|= ♢≤Haccept) denote the proportion of
1136"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6604136604136605,"accepting paths at timestep t′. Suppose π is such that Pr(⟨st′, qt′⟩|= ♢≤Haccept) > p1. Again recall
1137"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6608946608946609,"that for each path ρ ∈Sω and corresponding trace trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept
1138"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6613756613756614,"the sum Pt′+H
t=t′ C(⟨st, qt⟩) ≥1, and so,
1139"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6618566618566618,"p1 + c ≥Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6623376623376623,"t=0
C(⟨st, qt⟩) # ≥Eπ "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6628186628186629,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6632996632996633,"t=t′
C(⟨st, qt⟩) "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6637806637806638,"> p1
(86)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6642616642616642,"Now clearly for all p1 ∈[0, 1] and c > 0, the following holds,
1140"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6647426647426647,"p1 < p1 + c
(87)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6652236652236653,"This implies that there may exist some π satisfying Eq. 83 and such that Pr(⟨st′, qt′⟩|=
1141"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6657046657046657,"♢≤Haccept) > p1, i.e. does not satisfy Eq. 84 at timestep t = t′.
1142"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6661856661856662,"G.1.2
Probabilistic Cumulative Constraint
1143"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6666666666666666,"First we restate Problem 4.5.
1144"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6671476671476672,"Problem 4.5 (restated) (Probabilistic cumulative constraint [18, 56])."
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6676286676286677,"max
π
Vπ
subject to
P⟨st,qt⟩∼Mπ⊗D
hPT
t=0 C(⟨st, qt⟩) ≤d2
i
≥1 −δ2"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6681096681096681,"where d2 ∈R+ is the cost threshold, δ2 is a tolerance parameter and T is the fixed episode length.
1145"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6685906685906686,"Proposition G.4. A feasible policy π for Problem 4.1 with parameters p1 ∈[0, 1] is also a
1146"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.669071669071669,"feasible policy for Problem 4.5 with parameters d2 ∈R+ and δ2 ∈(0, 1], provided that,
1147"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6695526695526696,"d2 ≥
p"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.67003367003367,"(T + 1)/2 · log(1/δ2) + (T + 1) · p1.
1148"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6705146705146705,"Proof. For t ∈[0, T] we define the following random variables, X0, . . . , XT , where,
1149"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.670995670995671,"Xt = C(⟨st, qt⟩) = 1 [accept ∈L′(⟨st, qt⟩)]
(88)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6714766714766714,"and we make the same following observation,
1150"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.671957671957672,"E [Xt] = E [1 [accept ∈L′(⟨st, qt⟩)]]
(89)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6724386724386724,"= Pr (accept ∈L′(⟨st, qt⟩))
(90)
≤p1 · δ
(91)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6729196729196729,"See the proof of Prop. G.1 for details, the argument is identical. Once again, under mild assumptions
1151"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6734006734006734,"(i.e. C(⟨st, qt⟩) < ∞) we consider the following decomposition of the expected cumulative cost,
1152 Eπ "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6738816738816739,"t=0
C(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6743626743626744,"= Eπ [X0] + Eπ [X1] + . . . + Eπ [XT ]
(92)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6748436748436748,"≤(T + 1) · p1
(93)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6753246753246753,"Again we replace the subscript ‘⟨st, qt⟩∼Mπ⊗D’ here for brevity, see the proof of Prop. G.1 for the
1153"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6758056758056759,"full details. Before we proceed we must first deal with the dependence between the random variables
1154"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6762866762866763,"X0, . . . , XT . Strictly speaking it is not the case that Pr(Xt = 1 | Xt−1, . . . , X0) = Pr(Xt = 1).
1155"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6767676767676768,"However, we have already established that Pr(Xt = 1) ≤p1, as such we can simulate X0, . . . , XT
1156"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6772486772486772,"as a sequence of independent coin flips Y0, . . . , YT with probability p1, it is then the case that
1157"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6777296777296777,"P[PT
t=0 Xt > d2] ≤P[PT
t=0 Yt > d2]. We can now continue by bounding the probability we care
1158"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6782106782106783,"about,
1159 1 −P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6786916786916787,"t=0
C(⟨st, qt⟩) ≤d2 # = P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6791726791726792,"t=0
C(⟨st, qt⟩) > d2 # (94) = P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6796536796536796,"t=0
Xt > d2 # (95) ≤P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6801346801346801,"t=0
Yt > d2 # (96) = P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6806156806156806,"t=0
Yt > (T + 1) · p1 + d2 −(T + 1) · p1 # (97) = P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6810966810966811,"t=0
Yt > E "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6815776815776816,"t=0
Yt #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.682058682058682,+ d2 −(T + 1) · p1 # (98) ≤exp 
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6825396825396826,"−
2 · (d2 −(T + 1) · p1)2
PT
t=0(max{Yi} −min{Yi})2 ! (99)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.683020683020683,"= exp

−2 · (d2 −(T + 1) · p1)2"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6835016835016835,(T + 1)
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.683982683982684,"
(100)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6844636844636844,"The first inequality (Eq. 96) comes from our earlier construction and the second (Eq. 99) is obtained
1160"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.684944684944685,"from Hoeffding’s inequality [40] for bounded random variables. Finally, bounding the final expression
1161"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6854256854256854,"from above by δ2 and rearranging gives the desired result.
1162"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6859066859066859,"Proposition G.5. A feasible policy π for Problem 4.5 with parameters δ2 ≤p1 and d2 < 1, satisfies
1163"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6863876863876864,"Pr(⟨st, qt⟩|= ♢≤Haccept) ≤p1 for all t ∈[0, T]. This bound is tight.
1164"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6868686868686869,"Proof. A feasible policy π for Problem 4.5 with parameters δ2 ≤p1 and d2 < 1 clearly implies that,
1165 P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6873496873496874,"t=0
C(⟨st, qt⟩) < 1 #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6878306878306878,"≥1 −p1
(101)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6883116883116883,"Assuming H ≤T, then this implies that for all t′ ∈[0, T −H] we have,
1166 P "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6887926887926888,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6892736892736893,"t=t′
C(⟨st, qt⟩) < 1  ≥P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6897546897546898,"t=0
C(⟨st, qt⟩) < 1 #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6902356902356902,"≥1 −p1
(102)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6907166907166907,"Let Pr(⟨st′, qt′⟩|= ♢≤Haccept) denote the proportion of accepting paths at timestep t′, where st′ ∼
1167"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6911976911976911,"Pt′(·). Again Pt′(·) denotes the marginal state distribution at time t′. Recall that for each path ρ ∈Sω
1168"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6916786916786917,"and corresponding trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept the sum Pt′+H
t=t′ C(⟨st, qt⟩) ≥1.
1169"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6921596921596922,"Without loss of generality fix some t′ ∈[0, T −H] and suppose that Pr(⟨st′, qt′⟩|= ♢≤Haccept) >
1170"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6926406926406926,"p1. This implies that,
1171 P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6931216931216931,"t=0
C(⟨st, qt⟩) ≥1 # ≥P "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6936026936026936,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6940836940836941,"t=t′
C(⟨st, qt⟩) ≥1 "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6945646945646946,"> p1
(103)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.695045695045695,"Which is a contradiction. Therefore, it must be the case that when Eq. 101 is satisfied then so is
1172"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6955266955266955,"Pr(⟨st, qt⟩|= ♢≤Haccept]) ≤p1 for all t ∈[0, T −H]. For the remaining t′ ∈[T −H, T] a similar
1173"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.696007696007696,"argument can be made, the only detail is to ensure the sum in Eq. 102 is up to T rather than t′ +H. To
1174"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6964886964886965,"prove that this bound is tight we can show the possible existence of a counter example. In particular,
1175"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.696969696969697,"we want to prove the following statement, a policy π satisfying,
1176 P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6974506974506974,"t=0
C(⟨st, qt⟩) < 1 #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.697931697931698,"≥1 −(p1 + c)
(104)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6984126984126984,"for some constant c > 0 does not imply that,
1177"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6988936988936989,"Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t ∈[0, T]
(105)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6993746993746994,"We will show that there may exist some policy π that satisfies Eq. 104 but does not satisfy Eq. 105
1178"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.6998556998556998,"at some timestep t. Firstly, we assume π is such that Eq. 104 holds, this implies that for all
1179"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7003367003367004,"t′ ∈[0, T −H] we have,
1180 P "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7008177008177008,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7012987012987013,"t=t′
C(⟨st, qt⟩) < 1  ≥P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7017797017797017,"t=0
C(⟨st, qt⟩) < 1 #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7022607022607023,"≥1 −(p1 + c)
(106)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7027417027417028,"Fix some t′ ∈[0, T −H] and let Pr(⟨st′, qt′⟩|= ♢≤Haccept) denote the proportion of accepting
1181"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7032227032227032,"paths at timestep t′. Suppose that π is such that Pr(⟨st′, qt′⟩|= ♢≤Haccept) > p1. Again recall that
1182"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7037037037037037,"for each path ρ ∈Sω and corresponding trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept the sum
1183
Pt′+H
t=t′ C(⟨st, qt⟩) ≥1, and so,
1184"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7041847041847041,"p1 + c ≥P "" T
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7046657046657047,"t=0
C(⟨st, qt⟩) ≥1 # ≥P "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7051467051467052,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7056277056277056,"t=t′
C(⟨st, qt⟩) ≥1 "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7061087061087061,"> p1
(107)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7065897065897065,"Now clearly for all p1 ∈[0, 1] and c > 0, the following holds,
1185"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7070707070707071,"p1 < p1 + c
(108)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7075517075517076,"This implies that there may exist some π satisfying Eq. 104 and such that Pr(⟨st′, qt′⟩|=
1186"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.708032708032708,"♢≤Haccept) > p1, i.e. does not satisfy Eq. 105 at timestep t = t′.
1187"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7085137085137085,"G.1.3
Instantaneous constraint
1188"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.708994708994709,"First we restate Problem 4.6.
1189"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7094757094757095,"Problem 4.6 (restated) (Instantaneous constraint [23, 60, 69])."
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.70995670995671,"max
π
Vπ
subject to
P⟨st,qt⟩∼Mπ⊗D

C(⟨st, qt⟩) ≤d3

= 1
∀t ∈[0, T]"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7104377104377104,"Proposition G.6. A feasible policy π for Problem 4.6 with threshold d3 < 1 (otherwise the problem
1190"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.710918710918711,"is trivial) is a feasible policy for Problem 4.1 if and only if p1 = 0.
1191"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7113997113997114,"Proof. We start by proving the 4.6 ⇒4.1 direction. A feasible policy π for Problem 4.6 with d3 < 1
1192"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7118807118807119,"satisfies,
1193"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7123617123617123,"Pr (C(⟨st, qt⟩) < 1) = 1
∀t ∈[0, T]
(109)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7128427128427128,"which implies that,
1194"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7133237133237134,"Pr (C(⟨st, qt⟩) = 0) = 1
∀t ∈[0, T]
(110)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7138047138047138,"and by Defn. 4.3,
1195"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7142857142857143,"Pr (accept ̸∈L′(⟨st, qt⟩)) = 1
∀t ∈[0, T]
(111)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7147667147667147,"Then if for all t ∈[0, T], accept ̸∈L′(⟨st, qt⟩) then we have Pr(⟨s0, q0⟩̸|= ♢accept) = 1, where
1196"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7152477152477152,"q0 = ∆(Q0, L(s0)) and by extension we have Pr(⟨st, qt⟩̸|= ♢accept≤H) = 1 for all t ∈[0, T].
1197"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7157287157287158,"This completes the proof of this direction.
1198"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7162097162097162,"Now we prove the 4.1 ⇒4.6 direction. A policy π satisfying Pr(⟨st, qt⟩|= ♢accept≤H)) = 0 for all
1199"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7166907166907167,"t ∈[0, T] implies that Pr(⟨st, qt⟩̸|= ♢accept≤H) = 1 for all t ∈[0, T] which implies the following,
1200"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7171717171717171,"Pr (accept ̸∈L′(⟨st, qt⟩)) = 1
∀t ∈[0, T]
(112)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7176527176527177,"and by Defn. 4.3,
1201"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7181337181337182,"Pr [C(⟨st, qt⟩) = 0] = 1
∀t ∈[0, T]
(113)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7186147186147186,"which implies that,
1202"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7190957190957191,"Pr [C(⟨st, qt⟩) < 1] = 1
∀t ∈[0, T]
(114)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7195767195767195,"which concludes the proof.
1203"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7200577200577201,"G.2
Infinite Horizon
1204"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7205387205387206,"While in this paper we only consider finite horizon problems with a fixed episode length T, we note
1205"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.721019721019721,"that we can also make a set of similar statements for the infinite horizon (discounted) setting. In this
1206"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7215007215007215,"section we provide the corresponding statements and proofs for the infinite horizon setting. Firstly,
1207"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.721981721981722,"we consider the following infinite horizon problem.
1208"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7224627224627225,"Problem G.7 (Step-wise bounded regular safety property constraint). Let Psafe be a regular safety
1209"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7229437229437229,"property, D be the DFA such that L(D) = BadPref(Psafe) and M be the MDP;
1210"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7234247234247234,"max
π
Vπ
subject to
Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t = 0, 1, 2, . . ."
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7239057239057239,"where all probability is taken under the product Markov chain Mπ ⊗D, p1 ∈[0, 1] is a probability
1211"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7243867243867244,"threshold H is the model checking horizon .
1212"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7248677248677249,"G.2.1
Expected Cumulative Constraint
1213"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7253487253487253,Problem G.8 (Expected cumulative constraint).
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7258297258297258,"max
π
Vπ
subject to
E⟨st,qt⟩∼Mπ⊗D
h P∞
t=0 γtC(⟨st, qt⟩)
i
≤d1"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7263107263107264,"where d1 ∈R+ is the cost threshold and γ ∈[0, 1) is the discount factor.
1214"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7267917267917268,"Proposition G.9. A feasible policy π for Problem G.7 with parameters p1 ∈[0, 1], is also a feasible
1215"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7272727272727273,"policy for Problem G.8 with parameter d1 ∈R+, provided that d1 ≥T · p1, where T = 1/(1 −γ) is
1216"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7277537277537277,"the effective horizon.
1217"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7282347282347282,"Proof. For t = 0, 1, 2, . . . we define, the following random variables, X0, X1, X2, . . ., where,
1218"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7287157287157288,"Xt = C(⟨st, qt⟩) = 1 [accept ∈L′(⟨st, qt⟩)]
(115)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7291967291967292,"where,
1219"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7296777296777297,"E [Xt] = E [1 [accept ∈L′(⟨st, qt⟩)]]
(116)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7301587301587301,"= Pr (accept ∈L′(⟨st, qt⟩))
(117)
≤p1
(118)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7306397306397306,"The argument for this is straightforward. If at every timestep t = 0, 1, 2, . . . we have Pr(⟨st, qt⟩|=
1220"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7311207311207312,"♢≤Haccept) ≤p1 then with probability ≤p1 we have accept ∈L(⟨st, qt⟩). Let T = 1/(1 −γ)
1221"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7316017316017316,"be the effective horizon, then under mild assumptions (i.e. C(⟨st, qt⟩) < ∞) we can consider the
1222"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7320827320827321,"following decomposition of the expected cumulative cost,
1223 Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7325637325637325,"t=0
γtC(⟨st, qt⟩) # = Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.733044733044733,"t=0
γtXt # (119)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7335257335257336,= Es0∼P0(·) [X0] + γ · Es1∼P1(·) [X1] + . . .
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.734006734006734,"+ γT · EsT ∼PT (·) [XT ] + . . .
(120)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7344877344877345,"= Eπ [X0] + γ · Eπ [X1] + . . . + γT · Eπ [XT ] + . . .
(121)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7349687349687349,"We replace the subscript ‘⟨st, qt⟩∼Mπ ⊗D’ here for brevity. Clearly by linearty of expectations
1224"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7354497354497355,"this statement holds. Although it is worth noting that each expectation is taken under a different
1225"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7359307359307359,"marginal state distribution (i.e. Pt(·)), which depends on π (apart from the initial state distribution
1226"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7364117364117364,"P0(·)). From now on we will write this is implicitly (i.e. Eq. 121), rather than writing the marginal
1227"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7368927368927369,"state distribution (at time t) for each expectation. Using our earlier observations we can now bound
1228"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7373737373737373,"the expected cumulative cost from above as follows,
1229 Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7378547378547379,"t=0
γtC(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7383357383357383,"= Eπ [X0] + γ · Eπ [X1] + . . . + γT · Eπ [XT ] + . . .
(122)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7388167388167388,"≤p1 + γ · p1 + . . .
+ γT −1 · p1 + γT · p1 + . . .
(123)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7392977392977393,"= p1 · ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7397787397787398,"t=0
γt = p1 · (1/(1 −γ)) = T · p1
(124) 1230"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7402597402597403,"Proposition G.10. The converse is not strictly true, since there may be a feasible policy π for
1231"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7407407407407407,"Problem G.8 with threshold d1 ≤T · p1 which does not satisfy the constraints of Problem G.7
1232"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7412217412217412,"We want to prove the following statement, a policy π satisfying,
1233 Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7417027417027418,"t=0
γtC(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7421837421837422,"≤T · p1
(125)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7426647426647427,"does not imply that,
1234"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7431457431457431,"Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t = 0, 1, 2, . . .
(126)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7436267436267436,"Proof. To prove this we will show that there may be some policy π that satisfies Eq. 125, but does
1235"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7441077441077442,"not satisfy Eq. 126 at some timestep t. For simplicity we consider the first timestep (i.e. t = 0). First
1236"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7445887445887446,"we assume π is such that Eq. 125 holds, then clearly we have,
1237 Eπ "" H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7450697450697451,"t=0
γtC(⟨st, qt⟩) # ≤Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7455507455507455,"t=0
γtC(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.746031746031746,"≤T · p1
(127)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7465127465127465,"Let Pr(⟨s0, q0⟩|= ♢≤Haccept) denote the proportion of accepting paths from the initial state s0 ∼
1238"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.746993746993747,"P0(·). Suppose π is such that Pr(⟨s0, q0⟩|= ♢≤Haccept) > p1. We note that for each path ρ ∈Sω
1239"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7474747474747475,"and corresponding trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept the sum PH
t=0 γtC(⟨st, qt⟩) ≥
1240"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7479557479557479,"γH, and so,
1241"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7484367484367485,"T · p1 ≥Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7489177489177489,"t=0
γtC(⟨st, qt⟩) # ≥Eπ "" H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7493987493987494,"t=0
γtC(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7498797498797499,"> p1 · γH
(128)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7503607503607503,"Now clearly for all p1 ∈[0, 1], γ ∈[0, 1), H ∈Z+ and T = 1/(1 −γ) the following holds,
1242"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7508417508417509,"p1 · γH < T · p1
(129)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7513227513227513,"This implies that there may exist some π satisfying Eq. 125 and such that Pr(⟨s0, q0⟩|=
1243"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7518037518037518,"♢≤Haccept) > p1, i.e. does not satisfy Eq. 126 at timestep t = 0.
1244"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7522847522847523,"Proposition G.11. A feasible policy π for Problem 4.4 with threshold d1 ≤p1 · γT +H satisfies
1245"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7527657527657527,"Pr(⟨st, qt⟩|= ♢≤Haccept) ≤p1 up to the effective horizon T = 1/(1 −γ). This bound is tight.
1246"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7532467532467533,"Proof. Let T = 1/(1−γ) be the effective horizon. A feasible policy π for Problem 4.4 with threshold
1247"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7537277537277537,"d1 ≤p1 · γT +H clearly satisfies,
1248 Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7542087542087542,"t=0
γtC(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7546897546897547,"≤p1 · γT +H
(130)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7551707551707552,"which implies that for all t′ ∈[0, T] we have,
1249"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7556517556517557,"p1 · γT +H ≥Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7561327561327561,"t=0
γtC(⟨st, qt⟩) # ≥Eπ "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7566137566137566,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.757094757094757,"t=t′
γtC(⟨st, qt⟩) "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7575757575757576,"
(131) = Eπ "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7580567580567581,"γt′ t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7585377585377585,"t=t′
γt−t′C(⟨st, qt⟩) "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.759018759018759,"
(132)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7594997594997595,= γt′ · Eπ 
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.75998075998076,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7604617604617605,"t=t′
γt−t′C(⟨st, qt⟩) "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7609427609427609,"
(133)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7614237614237614,"Let Pr(⟨st′, qt′⟩|= ♢≤Haccept) denote the proportion of accepting paths at timestep t′, where
1250"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7619047619047619,"st′ ∼Pt′(·).
Here Pt′(·) denotes the marginal state distribution at time t′.
Recall that for
1251"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7623857623857624,"each path ρ ∈Sω and corresponding trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept the sum
1252
Pt′+H
t=t′ γt−t′C(⟨st, qt⟩) ≥γH. Without loss of generality fix some t′ ∈[0, T] and suppose that
1253"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7628667628667629,"Pr(⟨st′, qt′⟩|= ♢≤Haccept) > p1. This implies that,
1254 Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7633477633477633,"t=0
γtC(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7638287638287639,≥γt′ · Eπ 
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7643097643097643,"
t′+H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7647907647907648,"t=t′
γt−t′C(⟨st, qt⟩) "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7652717652717653,"
(134)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7657527657527657,"> p1 · γH · γt′ ≥p1 · γT +H
(135)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7662337662337663,"Which is a contradiction. Therefore, it must be the case that when Eq. 130 is satisfied then so is
1255"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7667147667147667,"Pr(⟨st, qt⟩|= ♢≤Haccept]) ≤p1 for all t ∈[0, T]. To prove that this bound is tight we can again
1256"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7671957671957672,"show the possible existence of a counter example. In particular, we want to prove the following
1257"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7676767676767676,"statement, a policy π satisfying,
1258 Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7681577681577682,"t=0
γtC(⟨st, qt⟩) #"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7686387686387687,"≤p1 · γT +H + c
(136)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7691197691197691,"for some constant c > 0, does not imply that,
1259"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7696007696007696,"Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t ∈[0, T]
(137)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.77008177008177,"We will show that there may exist some policy π that satisfies Eq. 136 but does not satisfy Eq. 137 at
1260"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7705627705627706,"some timestep t. For simplicity we consider timestep t = T, although we note that with a little extra
1261"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7710437710437711,"work we could come up with a proof for any t ∈[0, T]. Firstly, we assume π is such that Eq. 136
1262"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7715247715247715,"holds, then we have,
1263"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.772005772005772,"p1 · γT +H + c ≥Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7724867724867724,"t=0
γtC(⟨st, qt⟩) # ≥Eπ"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.772967772967773,"""T +H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7734487734487735,"t=T
γtC(⟨st, qt⟩) # (138)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7739297739297739,"Let Pr(⟨sT , qT ⟩|= ♢≤Haccept) denote the proportion of accepting paths at timestep T. Suppose π
1264"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7744107744107744,"is such that Pr(⟨sT , qT ⟩|= ♢≤Haccept) > p1. We note that for each path ρ ∈Sω and corresponding
1265"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7748917748917749,"trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept the sum PT +H
t=T γtC(⟨st, qt⟩) ≥γT +H, and so,
1266"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7753727753727754,"p1 · γT +H + c ≥Eπ "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7758537758537759,"t=0
γtC(⟨st, qt⟩) # (139) ≥Eπ"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7763347763347763,"""T +H
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7768157768157768,"t=T
γtC(⟨st, qt⟩) # (140)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7772967772967773,"> p1 · γT +H
(141)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7777777777777778,"Now clearly for all p1 ∈[0, 1], γ ∈[0, 1), c > 0, H ∈Z+ and T = 1/(1 −γ), the following holds,
1267"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7782587782587782,"p1 · γT +H < p1 · γT +H + c
(142)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7787397787397787,"This implies that there may exist some π satisfying Eq. 136 and such that Pr(⟨sT , qT ⟩|=
1268"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7792207792207793,"♢≤Haccept) > p1, i.e. does not satisfy Eq. 137 at timestep t = T.
1269"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7797017797017797,"G.3
Probabilistic Cumulative Constraint
1270"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7801827801827802,Problem G.12 (Probabilistic cumulative constraint).
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7806637806637806,"max
π
Vπ
subject to
P⟨st,qt⟩∼Mπ⊗D
h P∞
t=0 γtC(⟨st, qt⟩) ≤d2
i
≥1 −δ2"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7811447811447811,"where d2 ∈R+ is the cost threshold, δ2 is a tolerance parameter and γ ∈[0, 1) is the discount factor.
1271 1272"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7816257816257817,"Proposition G.13. A feasible policy π for Problem G.7 with parameters p1 ∈[0, 1], is also a
1273"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7821067821067821,"feasible policy for Problem G.12 with parameters d2 ∈R+ and δ2 ∈(0, 1], provided that, d2 ≥
1274
p"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7825877825877826,"(⌈log(T)⌉· T)/2 · log(1/δ2)+⌈log(T)⌉·T ·p1 +1, where T = 1/(1−γ) is the effective horizon.
1275 1276"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.783068783068783,"Proof. Again t = 0, 1, 2, . . . we define the following random variables, X0, X1, X2, . . ., where,
1277"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7835497835497836,"Xt = C(⟨st, qt⟩) = 1 [accept ∈L′(⟨st, qt⟩)]
(143)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7840307840307841,"and we make the following observation,
1278"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7845117845117845,"E [Xt] = E [1 [accept ∈L′(⟨st, qt⟩)]]
(144)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.784992784992785,"= Pr (accept ∈L′(⟨st, qt⟩))
(145)
≤p1
(146)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7854737854737854,"See the proof of Prop. G.9, the argument is identical. Under mild assumptions (i.e. C(⟨st, qt⟩) < ∞)
1279"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.785954785954786,"we consider the following decomposition of the (undiscounted) expected cumulative cost up to
1280"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7864357864357865,"timestep ⌈log(T)⌉· T −1,
1281 Eπ  "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7869167869167869,"⌈log(T )⌉·T −1
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7873977873977874,"t=0
C(⟨st, qt⟩) "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7878787878787878,"= Eπ [X0] + Eπ [X1] + . . . + Eπ

X⌈log(T )⌉·T −1

(147)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7883597883597884,"≤⌈log(T)⌉· T · p1
(148)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7888407888407888,"Again we replace the subscript ‘⟨st, qt⟩∼Mπ ⊗D’ here for brevity, see the proof of Prop. G.9 for
1282"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7893217893217893,"more details. Before we proceed we must first deal with the dependence between the random variables
1283"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7898027898027898,"X0, . . . X⌈log(T )⌉·T −1. Strictly speaking it is not the case that Pr(Xt = 1 | Xt−1, . . . , X0) =
1284"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7902837902837903,"Pr(Xt = 1). However, we have already established that Pr(Xt = 1) ≤p1, as such we can simulate
1285"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7907647907647908,"X0, . . . , X⌈log(T )⌉·T −1 as a sequence of independent coin flips Y0, . . . , Y⌈log(T )⌉·T −1 with probability
1286"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7912457912457912,"p1, it is then the case that P[P⌈log(T )⌉·T −1
t=0
Xt > d2] ≤P[P⌈log(T )⌉·T −1
t=0
Yt > d2]. Now we can
1287"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7917267917267917,"bound the probability that we care about,
1288 1 −P "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7922077922077922,"t=0
γtC(⟨st, qt⟩) ≤d2 # = P "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7926887926887927,"t=0
γtC(⟨st, qt⟩) > d2 # (149) = P "" ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7931697931697932,"t=0
γtXt > d2 # (150) = P  "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7936507936507936,"⌈log(T )⌉·T −1
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7941317941317941,"t=0
γtXt + ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7946127946127947,"t=⌈log(T )⌉·T
γtXt > d2 "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7950937950937951,"
(151) ≤P  "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7955747955747956,"⌈log(T )⌉·T −1
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.796055796055796,"t=0
Xt + 1 > d2 "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7965367965367965,"
(152) ≤P  "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7970177970177971,"⌈log(T )⌉·T −1
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7974987974987975,"t=0
Yt + 1 > d2 "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.797979797979798,"
(153) = P  "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7984607984607984,"⌈log(T )⌉·T −1
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.798941798941799,"t=0
Yt > ⌈log(T)⌉· T · p1 + d2 −⌈log(T)⌉· T · p1 −1   (154) = P  "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7994227994227994,"⌈log(T )⌉·T −1
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.7999037999037999,"t=0
Yt > E  "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8003848003848004,"⌈log(T )⌉·T −1
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8008658008658008,"t=0
Yt "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8013468013468014,+ d2 −⌈log(T)⌉· T · p1 −1   (155) ≤exp 
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8018278018278018,"−
2 · (d2 −⌈log(T)⌉· T · p1 −1)2
P⌈log(T )⌉·T −1
t=0
(max{Yi} −min{Yi})2 ! (156)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8023088023088023,"= exp

−2 · (d2 −⌈log(T)⌉· T · p1 −1)2"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8027898027898028,⌈log(T)⌉· T
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8032708032708032,"
(157)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8037518037518038,"Here
the
first
inequality
(Eq.
152)
comes
from
the
following
two
facts,
certainly
1289
P⌈log(T )⌉·T −1
t=0
γtXt ≤P⌈log(T )⌉·T −1
t=0
Xt and we have that P∞
t=⌈log(T )⌉·T γtXt ≤1. The sec-
1290"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8042328042328042,"ond fact is a little harder to see, first we note that limγ→1 γT = 1/e, where T = 1/(1 −γ) is the
1291"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8047138047138047,"effective horizon. Then we can rewrite,
1292 ∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8051948051948052,"t=⌈log(T )⌉·T
γtXt =

γ⌈log(T )⌉·T 
·  
∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8056758056758057,"t=⌈log(T )⌉·T
γt−⌈log(T )⌉·T Xt "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8061568061568062,"
(158)"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8066378066378066,"=

(γT )⌈log(T )⌉
·  
∞
X"
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8071188071188071,"t=⌈log(T )⌉·T
γt−⌈log(T )⌉·T Xt "
AT EACH TIMESTEP WHILE STILL ACHIEVING HIGH,0.8075998075998077,"
(159) ≤"
E,0.8080808080808081,"1
e"
E,0.8085618085618086,⌈log(T )⌉!
E,0.809042809042809,"·

1
1 −γ 
≤"
E,0.8095238095238095,"1
e"
E,0.81000481000481,log(T )!
E,0.8104858104858105,· T = 1
E,0.810966810966811,"T · T = 1
(160)"
E,0.8114478114478114,"The second inequality (Eq. 153) comes from our earlier construction. The final inequality (Eq. 156)
1293"
E,0.8119288119288119,"is obtained from Hoeffding’s inequality [40] for bounded random variables. Finally, by bounding the
1294"
E,0.8124098124098124,"final expression (Eq. 157) from above by δ2 and rearranging gives the desired result.
1295"
E,0.8128908128908129,"Proposition G.14. A feasible policy π for Problem G.12 with parameters δ2 ≤p1 and d2 < γT +H,
1296"
E,0.8133718133718134,"satisfies Pr(⟨st, qt⟩|= ♢≤Haccept) ≤p1 up to the effective horizon T = 1/(1 −γ). This bound is
1297"
E,0.8138528138528138,"tight.
1298"
E,0.8143338143338144,"Proof. A feasible policy π for Problem G.12 with parameters δ2 ≤p1 and d2 < γT +H clearly
1299"
E,0.8148148148148148,"implies that,
1300 P "" ∞
X"
E,0.8152958152958153,"t=0
γtC(⟨st, qt⟩) < γT +H
#"
E,0.8157768157768158,"≥1 −p1
(161)"
E,0.8162578162578162,"and certainly for all t′ ∈[0, T] we have that,
1301"
E,0.8167388167388168,"1 −p1 ≤P "" ∞
X"
E,0.8172198172198172,"t=0
γtC(⟨st, qt⟩) < γT +H
# (162) ≤P "
E,0.8177008177008177,"
t′+H
X"
E,0.8181818181818182,"t=t′
γtC(⟨st, qt⟩) < γT +H "
E,0.8186628186628186,"
(163) = P "
E,0.8191438191438192,"γt′ t′+H
X"
E,0.8196248196248196,"t=t′
γt−t′C(⟨st, qt⟩) < γT +H "
E,0.8201058201058201,"
(164) = P "
E,0.8205868205868206,"
t′+H
X"
E,0.8210678210678211,"t=t′
γt−t′C(⟨st, qt⟩) < (γT +H/γt′) "
E,0.8215488215488216,"
(165) ≤P "
E,0.822029822029822,"
t′+H
X"
E,0.8225108225108225,"t=t′
γt−t′C(⟨st, qt⟩) < γH "
E,0.8229918229918229,"
(166)"
E,0.8234728234728235,"Let Pr(⟨st′, qt′⟩|= ♢≤Haccept) denote the proportion of accepting paths at timestep t′, where
1302"
E,0.823953823953824,"st′ ∼Pt′(·).
Here Pt′(·) denotes the marginal state distribution at time t′.
Recall that for
1303"
E,0.8244348244348244,"each path ρ ∈Sω and corresponding trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept the sum
1304
Pt′+H
t=t′ γt−t′C(⟨st, qt⟩) ≥γH. Without loss of generality fix some t′ ∈[0, T] and suppose that
1305"
E,0.8249158249158249,"Pr(⟨st′, qt′⟩|= ♢≤Haccept) > p1. This implies that,
1306 P "" ∞
X"
E,0.8253968253968254,"t=0
γtC(⟨st, qt⟩) ≥γT +H
# ≥P "
E,0.8258778258778259,"
t′+H
X"
E,0.8263588263588264,"t=t′
γt−t′C(⟨st, qt⟩) ≥γH "
E,0.8268398268398268,"> p1
(167)"
E,0.8273208273208273,"Which is a contradiction. Therefore, it must be the case that when Eq. 161 is satisfied then so is
1307"
E,0.8278018278018278,"Pr(⟨st, qt⟩|= ♢≤Haccept]) ≤p1 for all t ∈[0, T]. To prove that this bound is tight we can show
1308"
E,0.8282828282828283,"the possible existence of a counter example. In particular, we want to prove the following statement,
1309"
E,0.8287638287638288,"a policy π satisfying,
1310 P "" ∞
X"
E,0.8292448292448292,"t=0
γtC(⟨st, qt⟩) < γT +H
#"
E,0.8297258297258298,"≥1 −(p1 + c)
(168)"
E,0.8302068302068302,"for some constant c > 0 does not imply that,
1311"
E,0.8306878306878307,"Pr
 
⟨st, qt⟩|= ♢≤Haccept

≤p1
∀t ∈[0, T]
(169)"
E,0.8311688311688312,"We will show that there may exist some policy π that satisfies Eq. 168 but does not satisfy Eq. 169 at
1312"
E,0.8316498316498316,"some timestep t. Firstly, we assume π is such that Eq. 168 holds, this implies that for all t′ ∈[0, T]
1313"
E,0.8321308321308322,"we have,
1314"
E,0.8326118326118326,"1 −(p1 + c) ≤P "" ∞
X"
E,0.8330928330928331,"t=0
γtC(⟨st, qt⟩) < γT +H
# (170) ≤P "
E,0.8335738335738335,"
t′+H
X"
E,0.834054834054834,"t=t′
γtC(⟨st, qt⟩) < γT +H "
E,0.8345358345358346,"
(171) ≤P "
E,0.835016835016835,"
t′+H
X"
E,0.8354978354978355,"t=t′
γt−t′C(⟨st, qt⟩) < γH "
E,0.8359788359788359,"
(172)"
E,0.8364598364598365,"Fix some t′ ∈[0, T] and let Pr(⟨st′, qt′⟩|= ♢≤Haccept) denote the proportion of accepting paths
1315"
E,0.836940836940837,"at timestep t′. Suppose that π is such that Pr(⟨st′, qt′⟩|= ♢≤Haccept) > p1. Again recall that
1316"
E,0.8374218374218374,"for each path ρ ∈Sω and corresponding trace(ρ) ∈Σω such that trace(ρ) |= ♢≤Haccept the sum
1317
Pt′+H
t=t′ γt−t′C(⟨st, qt⟩) ≥γH, and so,
1318"
E,0.8379028379028379,"p1 + c ≥P "" ∞
X"
E,0.8383838383838383,"t=0
γtC(⟨st, qt⟩) ≥γT +H
# (173) ≥P "
E,0.8388648388648389,"
t′+b
X"
E,0.8393458393458394,"t=t′
γt−t′C(⟨st, qt⟩) ≥γH "
E,0.8398268398268398,"> p1
(174)"
E,0.8403078403078403,"Now clearly for all p1 ∈[0, 1], and c > 0, the following holds,
1319"
E,0.8407888407888408,"p1 < p1 + c
(175)"
E,0.8412698412698413,"This implies that there may exist some π satisfying Eq. 168 such that Pr(⟨st′, qt′⟩|= ♢≤Haccept) >
1320"
E,0.8417508417508418,"p1, i.e. does not satisfy Eq. 169 at timestep t = t′.
1321"
E,0.8422318422318422,"NeurIPS Paper Checklist
1322"
CLAIMS,0.8427128427128427,"1. Claims
1323"
CLAIMS,0.8431938431938432,"Question: Do the main claims made in the abstract and introduction accurately reflect the
1324"
CLAIMS,0.8436748436748437,"paper’s contributions and scope?
1325"
CLAIMS,0.8441558441558441,"Answer: [Yes]
1326"
CLAIMS,0.8446368446368446,"Justification: The proof of safety guarantees (Theorem 6.5) is provided in Appendix C.5.
1327"
CLAIMS,0.8451178451178452,"Furthermore, we provide experimental results demonstrating the scalability of our approach
1328"
CLAIMS,0.8455988455988456,"in Section 7.
1329"
CLAIMS,0.8460798460798461,"Guidelines:
1330"
CLAIMS,0.8465608465608465,"• The answer NA means that the abstract and introduction do not include the claims
1331"
CLAIMS,0.847041847041847,"made in the paper.
1332"
CLAIMS,0.8475228475228476,"• The abstract and/or introduction should clearly state the claims made, including the
1333"
CLAIMS,0.848003848003848,"contributions made in the paper and important assumptions and limitations. A No or
1334"
CLAIMS,0.8484848484848485,"NA answer to this question will not be perceived well by the reviewers.
1335"
CLAIMS,0.8489658489658489,"• The claims made should match theoretical and experimental results, and reflect how
1336"
CLAIMS,0.8494468494468495,"much the results can be expected to generalize to other settings.
1337"
CLAIMS,0.84992784992785,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
1338"
CLAIMS,0.8504088504088504,"are not attained by the paper.
1339"
LIMITATIONS,0.8508898508898509,"2. Limitations
1340"
LIMITATIONS,0.8513708513708513,"Question: Does the paper discuss the limitations of the work performed by the authors?
1341"
LIMITATIONS,0.8518518518518519,"Answer: [Yes]
1342"
LIMITATIONS,0.8523328523328524,"Justification: The limitations of our framework are discussed under in Section 7 in the
1343"
LIMITATIONS,0.8528138528138528,"paragraph Separating Reward and Safety, furthermore the limitations of our proposed
1344"
LIMITATIONS,0.8532948532948533,"method are also discussed and explored in Appendix F.
1345"
LIMITATIONS,0.8537758537758537,"Guidelines:
1346"
LIMITATIONS,0.8542568542568543,"• The answer NA means that the paper has no limitation while the answer No means that
1347"
LIMITATIONS,0.8547378547378547,"the paper has limitations, but those are not discussed in the paper.
1348"
LIMITATIONS,0.8552188552188552,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
1349"
LIMITATIONS,0.8556998556998557,"• The paper should point out any strong assumptions and how robust the results are to
1350"
LIMITATIONS,0.8561808561808562,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
1351"
LIMITATIONS,0.8566618566618567,"model well-specification, asymptotic approximations only holding locally). The authors
1352"
LIMITATIONS,0.8571428571428571,"should reflect on how these assumptions might be violated in practice and what the
1353"
LIMITATIONS,0.8576238576238576,"implications would be.
1354"
LIMITATIONS,0.8581048581048581,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
1355"
LIMITATIONS,0.8585858585858586,"only tested on a few datasets or with a few runs. In general, empirical results often
1356"
LIMITATIONS,0.8590668590668591,"depend on implicit assumptions, which should be articulated.
1357"
LIMITATIONS,0.8595478595478595,"• The authors should reflect on the factors that influence the performance of the approach.
1358"
LIMITATIONS,0.86002886002886,"For example, a facial recognition algorithm may perform poorly when image resolution
1359"
LIMITATIONS,0.8605098605098606,"is low or images are taken in low lighting. Or a speech-to-text system might not be
1360"
LIMITATIONS,0.860990860990861,"used reliably to provide closed captions for online lectures because it fails to handle
1361"
LIMITATIONS,0.8614718614718615,"technical jargon.
1362"
LIMITATIONS,0.8619528619528619,"• The authors should discuss the computational efficiency of the proposed algorithms
1363"
LIMITATIONS,0.8624338624338624,"and how they scale with dataset size.
1364"
LIMITATIONS,0.862914862914863,"• If applicable, the authors should discuss possible limitations of their approach to
1365"
LIMITATIONS,0.8633958633958634,"address problems of privacy and fairness.
1366"
LIMITATIONS,0.8638768638768639,"• While the authors might fear that complete honesty about limitations might be used by
1367"
LIMITATIONS,0.8643578643578643,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
1368"
LIMITATIONS,0.8648388648388649,"limitations that aren’t acknowledged in the paper. The authors should use their best
1369"
LIMITATIONS,0.8653198653198653,"judgment and recognize that individual actions in favor of transparency play an impor-
1370"
LIMITATIONS,0.8658008658008658,"tant role in developing norms that preserve the integrity of the community. Reviewers
1371"
LIMITATIONS,0.8662818662818663,"will be specifically instructed to not penalize honesty concerning limitations.
1372"
THEORY ASSUMPTIONS AND PROOFS,0.8667628667628667,"3. Theory Assumptions and Proofs
1373"
THEORY ASSUMPTIONS AND PROOFS,0.8672438672438673,"Question: For each theoretical result, does the paper provide the full set of assumptions and
1374"
THEORY ASSUMPTIONS AND PROOFS,0.8677248677248677,"a complete (and correct) proof?
1375"
THEORY ASSUMPTIONS AND PROOFS,0.8682058682058682,"Answer: [Yes]
1376"
THEORY ASSUMPTIONS AND PROOFS,0.8686868686868687,"Justification: For the key proofs in the main paper we explicitly provide the assumptions
1377"
THEORY ASSUMPTIONS AND PROOFS,0.8691678691678691,"used, the proofs of each theoretical result from the main paper can also be found in Appendix
1378"
THEORY ASSUMPTIONS AND PROOFS,0.8696488696488697,"C.
1379"
THEORY ASSUMPTIONS AND PROOFS,0.8701298701298701,"Guidelines:
1380"
THEORY ASSUMPTIONS AND PROOFS,0.8706108706108706,"• The answer NA means that the paper does not include theoretical results.
1381"
THEORY ASSUMPTIONS AND PROOFS,0.8710918710918711,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
1382"
THEORY ASSUMPTIONS AND PROOFS,0.8715728715728716,"referenced.
1383"
THEORY ASSUMPTIONS AND PROOFS,0.8720538720538721,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
1384"
THEORY ASSUMPTIONS AND PROOFS,0.8725348725348725,"• The proofs can either appear in the main paper or the supplemental material, but if
1385"
THEORY ASSUMPTIONS AND PROOFS,0.873015873015873,"they appear in the supplemental material, the authors are encouraged to provide a short
1386"
THEORY ASSUMPTIONS AND PROOFS,0.8734968734968735,"proof sketch to provide intuition.
1387"
THEORY ASSUMPTIONS AND PROOFS,0.873977873977874,"• Inversely, any informal proof provided in the core of the paper should be complemented
1388"
THEORY ASSUMPTIONS AND PROOFS,0.8744588744588745,"by formal proofs provided in appendix or supplemental material.
1389"
THEORY ASSUMPTIONS AND PROOFS,0.8749398749398749,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
1390"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8754208754208754,"4. Experimental Result Reproducibility
1391"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8759018759018758,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
1392"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8763828763828764,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
1393"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8768638768638769,"of the paper (regardless of whether the code and data are provided or not)?
1394"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8773448773448773,"Answer: [Yes]
1395"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8778258778258778,"Justification: other than access to code, we provide pseudo-code for the algorithms used in
1396"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8783068783068783,"our experiments (see Appendix A)
1397"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8787878787878788,"Guidelines:
1398"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8792688792688793,"• The answer NA means that the paper does not include experiments.
1399"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8797498797498797,"• If the paper includes experiments, a No answer to this question will not be perceived
1400"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8802308802308803,"well by the reviewers: Making the paper reproducible is important, regardless of
1401"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8807118807118807,"whether the code and data are provided or not.
1402"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8811928811928812,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
1403"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8816738816738817,"to make their results reproducible or verifiable.
1404"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8821548821548821,"• Depending on the contribution, reproducibility can be accomplished in various ways.
1405"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8826358826358827,"For example, if the contribution is a novel architecture, describing the architecture fully
1406"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8831168831168831,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
1407"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8835978835978836,"be necessary to either make it possible for others to replicate the model with the same
1408"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8840788840788841,"dataset, or provide access to the model. In general. releasing code and data is often
1409"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8845598845598845,"one good way to accomplish this, but reproducibility can also be provided via detailed
1410"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8850408850408851,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
1411"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8855218855218855,"of a large language model), releasing of a model checkpoint, or other means that are
1412"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.886002886002886,"appropriate to the research performed.
1413"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8864838864838864,"• While NeurIPS does not require releasing code, the conference does require all submis-
1414"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.886964886964887,"sions to provide some reasonable avenue for reproducibility, which may depend on the
1415"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8874458874458875,"nature of the contribution. For example
1416"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8879268879268879,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
1417"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8884078884078884,"to reproduce that algorithm.
1418"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8888888888888888,"(b) If the contribution is primarily a new model architecture, the paper should describe
1419"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8893698893698894,"the architecture clearly and fully.
1420"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8898508898508899,"(c) If the contribution is a new model (e.g., a large language model), then there should
1421"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8903318903318903,"either be a way to access this model for reproducing the results or a way to reproduce
1422"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8908128908128908,"the model (e.g., with an open-source dataset or instructions for how to construct
1423"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8912938912938912,"the dataset).
1424"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8917748917748918,"(d) We recognize that reproducibility may be tricky in some cases, in which case
1425"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8922558922558923,"authors are welcome to describe the particular way they provide for reproducibility.
1426"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8927368927368927,"In the case of closed-source models, it may be that access to the model is limited in
1427"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8932178932178932,"some way (e.g., to registered users), but it should be possible for other researchers
1428"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8936988936988937,"to have some path to reproducing or verifying the results.
1429"
OPEN ACCESS TO DATA AND CODE,0.8941798941798942,"5. Open access to data and code
1430"
OPEN ACCESS TO DATA AND CODE,0.8946608946608947,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
1431"
OPEN ACCESS TO DATA AND CODE,0.8951418951418951,"tions to faithfully reproduce the main experimental results, as described in supplemental
1432"
OPEN ACCESS TO DATA AND CODE,0.8956228956228957,"material?
1433"
OPEN ACCESS TO DATA AND CODE,0.8961038961038961,"Answer: [Yes]
1434"
OPEN ACCESS TO DATA AND CODE,0.8965848965848966,"Justification: We provide access to the code for our first set of experiments in the supple-
1435"
OPEN ACCESS TO DATA AND CODE,0.897065897065897,"mentary material, with a corresponding script to reproduce the results in the main paper. For
1436"
OPEN ACCESS TO DATA AND CODE,0.8975468975468975,"the second set of experiments we provide directions to the code base that we adapted and
1437"
OPEN ACCESS TO DATA AND CODE,0.8980278980278981,"throughout the paper and appendices we provide sufficient details to reproduce these results
1438"
OPEN ACCESS TO DATA AND CODE,0.8985088985088985,"without too much difficulty.
1439"
OPEN ACCESS TO DATA AND CODE,0.898989898989899,"Guidelines:
1440"
OPEN ACCESS TO DATA AND CODE,0.8994708994708994,"• The answer NA means that paper does not include experiments requiring code.
1441"
OPEN ACCESS TO DATA AND CODE,0.8999518999519,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
1442"
OPEN ACCESS TO DATA AND CODE,0.9004329004329005,"public/guides/CodeSubmissionPolicy) for more details.
1443"
OPEN ACCESS TO DATA AND CODE,0.9009139009139009,"• While we encourage the release of code and data, we understand that this might not be
1444"
OPEN ACCESS TO DATA AND CODE,0.9013949013949014,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1445"
OPEN ACCESS TO DATA AND CODE,0.9018759018759018,"including code, unless this is central to the contribution (e.g., for a new open-source
1446"
OPEN ACCESS TO DATA AND CODE,0.9023569023569024,"benchmark).
1447"
OPEN ACCESS TO DATA AND CODE,0.9028379028379029,"• The instructions should contain the exact command and environment needed to run to
1448"
OPEN ACCESS TO DATA AND CODE,0.9033189033189033,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1449"
OPEN ACCESS TO DATA AND CODE,0.9037999037999038,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1450"
OPEN ACCESS TO DATA AND CODE,0.9042809042809042,"• The authors should provide instructions on data access and preparation, including how
1451"
OPEN ACCESS TO DATA AND CODE,0.9047619047619048,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1452"
OPEN ACCESS TO DATA AND CODE,0.9052429052429053,"• The authors should provide scripts to reproduce all experimental results for the new
1453"
OPEN ACCESS TO DATA AND CODE,0.9057239057239057,"proposed method and baselines. If only a subset of experiments are reproducible, they
1454"
OPEN ACCESS TO DATA AND CODE,0.9062049062049062,"should state which ones are omitted from the script and why.
1455"
OPEN ACCESS TO DATA AND CODE,0.9066859066859067,"• At submission time, to preserve anonymity, the authors should release anonymized
1456"
OPEN ACCESS TO DATA AND CODE,0.9071669071669072,"versions (if applicable).
1457"
OPEN ACCESS TO DATA AND CODE,0.9076479076479076,"• Providing as much information as possible in supplemental material (appended to the
1458"
OPEN ACCESS TO DATA AND CODE,0.9081289081289081,"paper) is recommended, but including URLs to data and code is permitted.
1459"
OPEN ACCESS TO DATA AND CODE,0.9086099086099086,"6. Experimental Setting/Details
1460"
OPEN ACCESS TO DATA AND CODE,0.9090909090909091,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1461"
OPEN ACCESS TO DATA AND CODE,0.9095719095719096,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1462"
OPEN ACCESS TO DATA AND CODE,0.91005291005291,"results?
1463"
OPEN ACCESS TO DATA AND CODE,0.9105339105339105,"Answer: [Yes]
1464"
OPEN ACCESS TO DATA AND CODE,0.911014911014911,"Justification: We provide a thorough description of the environmental settings in Appendix
1465"
OPEN ACCESS TO DATA AND CODE,0.9114959114959115,"D.1 and D.2, furthermore, hyperparameters and details with regards to access to the code
1466"
OPEN ACCESS TO DATA AND CODE,0.911976911976912,"are provided in Appendix E.
1467"
OPEN ACCESS TO DATA AND CODE,0.9124579124579124,"Guidelines:
1468"
OPEN ACCESS TO DATA AND CODE,0.9129389129389129,"• The answer NA means that the paper does not include experiments.
1469"
OPEN ACCESS TO DATA AND CODE,0.9134199134199135,"• The experimental setting should be presented in the core of the paper to a level of detail
1470"
OPEN ACCESS TO DATA AND CODE,0.9139009139009139,"that is necessary to appreciate the results and make sense of them.
1471"
OPEN ACCESS TO DATA AND CODE,0.9143819143819144,"• The full details can be provided either with the code, in appendix, or as supplemental
1472"
OPEN ACCESS TO DATA AND CODE,0.9148629148629148,"material.
1473"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9153439153439153,"7. Experiment Statistical Significance
1474"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9158249158249159,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1475"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9163059163059163,"information about the statistical significance of the experiments?
1476"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9167869167869168,"Answer: [Yes] ,
1477"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9172679172679172,"Justification: We provide error bars for all of our experiments, over 5 random initializations
1478"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9177489177489178,"(seeds), provided by seaborn.lineplot, see Appendix E for details.
1479"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9182299182299183,"Guidelines:
1480"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9187109187109187,"• The answer NA means that the paper does not include experiments.
1481"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9191919191919192,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1482"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9196729196729196,"dence intervals, or statistical significance tests, at least for the experiments that support
1483"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9201539201539202,"the main claims of the paper.
1484"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9206349206349206,"• The factors of variability that the error bars are capturing should be clearly stated (for
1485"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9211159211159211,"example, train/test split, initialization, random drawing of some parameter, or overall
1486"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9215969215969216,"run with given experimental conditions).
1487"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.922077922077922,"• The method for calculating the error bars should be explained (closed form formula,
1488"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9225589225589226,"call to a library function, bootstrap, etc.)
1489"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.923039923039923,"• The assumptions made should be given (e.g., Normally distributed errors).
1490"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9235209235209235,"• It should be clear whether the error bar is the standard deviation or the standard error
1491"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.924001924001924,"of the mean.
1492"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9244829244829245,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1493"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.924963924963925,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1494"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9254449254449254,"of Normality of errors is not verified.
1495"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9259259259259259,"• For asymmetric distributions, the authors should be careful not to show in tables or
1496"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9264069264069265,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1497"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9268879268879269,"error rates).
1498"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9273689273689274,"• If error bars are reported in tables or plots, The authors should explain in the text how
1499"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9278499278499278,"they were calculated and reference the corresponding figures or tables in the text.
1500"
EXPERIMENTS COMPUTE RESOURCES,0.9283309283309283,"8. Experiments Compute Resources
1501"
EXPERIMENTS COMPUTE RESOURCES,0.9288119288119289,"Question: For each experiment, does the paper provide sufficient information on the com-
1502"
EXPERIMENTS COMPUTE RESOURCES,0.9292929292929293,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1503"
EXPERIMENTS COMPUTE RESOURCES,0.9297739297739298,"the experiments?
1504"
EXPERIMENTS COMPUTE RESOURCES,0.9302549302549302,"Answer: [Yes]
1505"
EXPERIMENTS COMPUTE RESOURCES,0.9307359307359307,"Justification: We provide details the compute resources used for our experiments in Appendix
1506"
EXPERIMENTS COMPUTE RESOURCES,0.9312169312169312,"E and the expected time and memory requirements for an individual run.
1507"
EXPERIMENTS COMPUTE RESOURCES,0.9316979316979317,"Guidelines:
1508"
EXPERIMENTS COMPUTE RESOURCES,0.9321789321789322,"• The answer NA means that the paper does not include experiments.
1509"
EXPERIMENTS COMPUTE RESOURCES,0.9326599326599326,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1510"
EXPERIMENTS COMPUTE RESOURCES,0.9331409331409332,"or cloud provider, including relevant memory and storage.
1511"
EXPERIMENTS COMPUTE RESOURCES,0.9336219336219336,"• The paper should provide the amount of compute required for each of the individual
1512"
EXPERIMENTS COMPUTE RESOURCES,0.9341029341029341,"experimental runs as well as estimate the total compute.
1513"
EXPERIMENTS COMPUTE RESOURCES,0.9345839345839346,"• The paper should disclose whether the full research project required more compute
1514"
EXPERIMENTS COMPUTE RESOURCES,0.935064935064935,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1515"
EXPERIMENTS COMPUTE RESOURCES,0.9355459355459356,"didn’t make it into the paper).
1516"
CODE OF ETHICS,0.936026936026936,"9. Code Of Ethics
1517"
CODE OF ETHICS,0.9365079365079365,"Question: Does the research conducted in the paper conform, in every respect, with the
1518"
CODE OF ETHICS,0.936988936988937,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1519"
CODE OF ETHICS,0.9374699374699375,"Answer: [Yes]
1520"
CODE OF ETHICS,0.937950937950938,"Justification: We have reviewed the NeurIPS Code of Ethics and we do not think the research
1521"
CODE OF ETHICS,0.9384319384319384,"in this paper has any particular ethical concerns, to the best of our ability we have tried to
1522"
CODE OF ETHICS,0.9389129389129389,"maintain anonymity.
1523"
CODE OF ETHICS,0.9393939393939394,"Guidelines:
1524"
CODE OF ETHICS,0.9398749398749399,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1525"
CODE OF ETHICS,0.9403559403559404,"• If the authors answer No, they should explain the special circumstances that require a
1526"
CODE OF ETHICS,0.9408369408369408,"deviation from the Code of Ethics.
1527"
CODE OF ETHICS,0.9413179413179413,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1528"
CODE OF ETHICS,0.9417989417989417,"eration due to laws or regulations in their jurisdiction).
1529"
BROADER IMPACTS,0.9422799422799423,"10. Broader Impacts
1530"
BROADER IMPACTS,0.9427609427609428,"Question: Does the paper discuss both potential positive societal impacts and negative
1531"
BROADER IMPACTS,0.9432419432419432,"societal impacts of the work performed?
1532"
BROADER IMPACTS,0.9437229437229437,"Answer: [NA] .
1533"
BROADER IMPACTS,0.9442039442039442,"Justification: As our paper is mostly foundational we do not foresee any immediate positive
1534"
BROADER IMPACTS,0.9446849446849447,"or negative societal impact of this research.
1535"
BROADER IMPACTS,0.9451659451659452,"Guidelines:
1536"
BROADER IMPACTS,0.9456469456469456,"• The answer NA means that there is no societal impact of the work performed.
1537"
BROADER IMPACTS,0.9461279461279462,"• If the authors answer NA or No, they should explain why their work has no societal
1538"
BROADER IMPACTS,0.9466089466089466,"impact or why the paper does not address societal impact.
1539"
BROADER IMPACTS,0.9470899470899471,"• Examples of negative societal impacts include potential malicious or unintended uses
1540"
BROADER IMPACTS,0.9475709475709476,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1541"
BROADER IMPACTS,0.948051948051948,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1542"
BROADER IMPACTS,0.9485329485329486,"groups), privacy considerations, and security considerations.
1543"
BROADER IMPACTS,0.949013949013949,"• The conference expects that many papers will be foundational research and not tied
1544"
BROADER IMPACTS,0.9494949494949495,"to particular applications, let alone deployments. However, if there is a direct path to
1545"
BROADER IMPACTS,0.94997594997595,"any negative applications, the authors should point it out. For example, it is legitimate
1546"
BROADER IMPACTS,0.9504569504569504,"to point out that an improvement in the quality of generative models could be used to
1547"
BROADER IMPACTS,0.950937950937951,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1548"
BROADER IMPACTS,0.9514189514189514,"that a generic algorithm for optimizing neural networks could enable people to train
1549"
BROADER IMPACTS,0.9518999518999519,"models that generate Deepfakes faster.
1550"
BROADER IMPACTS,0.9523809523809523,"• The authors should consider possible harms that could arise when the technology is
1551"
BROADER IMPACTS,0.9528619528619529,"being used as intended and functioning correctly, harms that could arise when the
1552"
BROADER IMPACTS,0.9533429533429534,"technology is being used as intended but gives incorrect results, and harms following
1553"
BROADER IMPACTS,0.9538239538239538,"from (intentional or unintentional) misuse of the technology.
1554"
BROADER IMPACTS,0.9543049543049543,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1555"
BROADER IMPACTS,0.9547859547859547,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1556"
BROADER IMPACTS,0.9552669552669553,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1557"
BROADER IMPACTS,0.9557479557479558,"feedback over time, improving the efficiency and accessibility of ML).
1558"
SAFEGUARDS,0.9562289562289562,"11. Safeguards
1559"
SAFEGUARDS,0.9567099567099567,"Question: Does the paper describe safeguards that have been put in place for responsible
1560"
SAFEGUARDS,0.9571909571909571,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1561"
SAFEGUARDS,0.9576719576719577,"image generators, or scraped datasets)?
1562"
SAFEGUARDS,0.9581529581529582,"Answer: [NA] .
1563"
SAFEGUARDS,0.9586339586339586,"Justification: We do not believe that the new assets provided in the paper pose any such
1564"
SAFEGUARDS,0.9591149591149591,"risks.
1565"
SAFEGUARDS,0.9595959595959596,"Guidelines:
1566"
SAFEGUARDS,0.9600769600769601,"• The answer NA means that the paper poses no such risks.
1567"
SAFEGUARDS,0.9605579605579606,"• Released models that have a high risk for misuse or dual-use should be released with
1568"
SAFEGUARDS,0.961038961038961,"necessary safeguards to allow for controlled use of the model, for example by requiring
1569"
SAFEGUARDS,0.9615199615199616,"that users adhere to usage guidelines or restrictions to access the model or implementing
1570"
SAFEGUARDS,0.962000962000962,"safety filters.
1571"
SAFEGUARDS,0.9624819624819625,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1572"
SAFEGUARDS,0.9629629629629629,"should describe how they avoided releasing unsafe images.
1573"
SAFEGUARDS,0.9634439634439634,"• We recognize that providing effective safeguards is challenging, and many papers do
1574"
SAFEGUARDS,0.963924963924964,"not require this, but we encourage authors to take this into account and make a best
1575"
SAFEGUARDS,0.9644059644059644,"faith effort.
1576"
LICENSES FOR EXISTING ASSETS,0.9648869648869649,"12. Licenses for existing assets
1577"
LICENSES FOR EXISTING ASSETS,0.9653679653679653,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1578"
LICENSES FOR EXISTING ASSETS,0.9658489658489658,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1579"
LICENSES FOR EXISTING ASSETS,0.9663299663299664,"properly respected?
1580"
LICENSES FOR EXISTING ASSETS,0.9668109668109668,"Answer: [Yes]
1581"
LICENSES FOR EXISTING ASSETS,0.9672919672919673,"Justification: We are the original creators/owners of the code used for the first set of
1582"
LICENSES FOR EXISTING ASSETS,0.9677729677729677,"experiments, for the second set of experiments we explicitly cite the paper and provide the
1583"
LICENSES FOR EXISTING ASSETS,0.9682539682539683,"URL for the code that we have adapted in this paper, which is available under the MIT
1584"
LICENSES FOR EXISTING ASSETS,0.9687349687349688,"License as stated in Appendix E.
1585"
LICENSES FOR EXISTING ASSETS,0.9692159692159692,"Guidelines:
1586"
LICENSES FOR EXISTING ASSETS,0.9696969696969697,"• The answer NA means that the paper does not use existing assets.
1587"
LICENSES FOR EXISTING ASSETS,0.9701779701779701,"• The authors should cite the original paper that produced the code package or dataset.
1588"
LICENSES FOR EXISTING ASSETS,0.9706589706589707,"• The authors should state which version of the asset is used and, if possible, include a
1589"
LICENSES FOR EXISTING ASSETS,0.9711399711399712,"URL.
1590"
LICENSES FOR EXISTING ASSETS,0.9716209716209716,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1591"
LICENSES FOR EXISTING ASSETS,0.9721019721019721,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1592"
LICENSES FOR EXISTING ASSETS,0.9725829725829725,"service of that source should be provided.
1593"
LICENSES FOR EXISTING ASSETS,0.9730639730639731,"• If assets are released, the license, copyright information, and terms of use in the
1594"
LICENSES FOR EXISTING ASSETS,0.9735449735449735,"package should be provided. For popular datasets, paperswithcode.com/datasets
1595"
LICENSES FOR EXISTING ASSETS,0.974025974025974,"has curated licenses for some datasets. Their licensing guide can help determine the
1596"
LICENSES FOR EXISTING ASSETS,0.9745069745069745,"license of a dataset.
1597"
LICENSES FOR EXISTING ASSETS,0.974987974987975,"• For existing datasets that are re-packaged, both the original license and the license of
1598"
LICENSES FOR EXISTING ASSETS,0.9754689754689755,"the derived asset (if it has changed) should be provided.
1599"
LICENSES FOR EXISTING ASSETS,0.9759499759499759,"• If this information is not available online, the authors are encouraged to reach out to
1600"
LICENSES FOR EXISTING ASSETS,0.9764309764309764,"the asset’s creators.
1601"
NEW ASSETS,0.976911976911977,"13. New Assets
1602"
NEW ASSETS,0.9773929773929774,"Question: Are new assets introduced in the paper well documented and is the documentation
1603"
NEW ASSETS,0.9778739778739779,"provided alongside the assets?
1604"
NEW ASSETS,0.9783549783549783,"Answer: [Yes]
1605"
NEW ASSETS,0.9788359788359788,"Justification: We provide
1606"
NEW ASSETS,0.9793169793169794,"Guidelines:
1607"
NEW ASSETS,0.9797979797979798,"• The answer NA means that the paper does not release new assets.
1608"
NEW ASSETS,0.9802789802789803,"• Researchers should communicate the details of the dataset/code/model as part of their
1609"
NEW ASSETS,0.9807599807599807,"submissions via structured templates. This includes details about training, license,
1610"
NEW ASSETS,0.9812409812409812,"limitations, etc.
1611"
NEW ASSETS,0.9817219817219818,"• The paper should discuss whether and how consent was obtained from people whose
1612"
NEW ASSETS,0.9822029822029822,"asset is used.
1613"
NEW ASSETS,0.9826839826839827,"• At submission time, remember to anonymize your assets (if applicable). You can either
1614"
NEW ASSETS,0.9831649831649831,"create an anonymized URL or include an anonymized zip file.
1615"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836459836459837,"14. Crowdsourcing and Research with Human Subjects
1616"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841269841269841,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1617"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846079846079846,"include the full text of instructions given to participants and screenshots, if applicable, as
1618"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850889850889851,"well as details about compensation (if any)?
1619"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9855699855699855,"Answer: [NA] .
1620"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860509860509861,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1621"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865319865319865,"Guidelines:
1622"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987012987012987,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1623"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874939874939875,"human subjects.
1624"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987974987974988,"• Including this information in the supplemental material is fine, but if the main contribu-
1625"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884559884559885,"tion of the paper involves human subjects, then as much detail as possible should be
1626"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889369889369889,"included in the main paper.
1627"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894179894179894,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1628"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98989898989899,"or other labor should be paid at least the minimum wage in the country of the data
1629"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903799903799904,"collector.
1630"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908609908609909,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1631"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913419913419913,"Subjects
1632"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918229918229918,"Question: Does the paper describe potential risks incurred by study participants, whether
1633"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923039923039924,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1634"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927849927849928,"approvals (or an equivalent approval/review based on the requirements of your country or
1635"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932659932659933,"institution) were obtained?
1636"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937469937469937,"Answer: [NA] .
1637"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942279942279942,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1638"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947089947089947,"Guidelines:
1639"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951899951899952,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1640"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956709956709957,"human subjects.
1641"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961519961519961,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1642"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966329966329966,"may be required for any human subjects research. If you obtained IRB approval, you
1643"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9971139971139971,"should clearly state this in the paper.
1644"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9975949975949976,"• We recognize that the procedures for this may vary significantly between institutions
1645"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980759980759981,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1646"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985569985569985,"guidelines for their institution.
1647"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990379990379991,"• For initial submissions, do not include any information that would break anonymity (if
1648"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9995189995189995,"applicable), such as the institution conducting the review.
1649"
