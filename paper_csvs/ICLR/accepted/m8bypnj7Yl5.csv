Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0026109660574412533,"Synthesizing optimal controllers for dynamical systems often involves solving op-
timization problems with hard real‚Äìtime constraints. These constraints determine
the class of numerical methods that can be applied: computationally expensive
but accurate numerical routines are replaced by fast and inaccurate methods, trad-
ing inference time for solution accuracy. This paper provides techniques to im-
prove the quality of optimized control policies given a Ô¨Åxed computational bud-
get. We achieve the above via a hypersolvers (Poli et al., 2020a) approach, which
hybridizes a differential equation solver and a neural network. The performance
is evaluated in direct and receding‚Äìhorizon optimal control tasks in both low and
high dimensions, where the proposed approach shows consistent Pareto improve-
ments in solution accuracy and control performance."
INTRODUCTION,0.005221932114882507,"1
INTRODUCTION"
INTRODUCTION,0.007832898172323759,"Optimal control of complex, high‚Äìdimensional systems requires computationally expensive numer-
ical methods for differential equations (Pytlak, 2006; Rao, 2009). Here, real‚Äìtime and hardware
constraints preclude the use of accurate and expensive methods, forcing instead the application of
cheaper and less accurate algorithms. While the paradigm of optimal control has successfully been
applied in various domains (Vadali et al., 1999; Lewis et al., 2012; Zhang et al., 2016), improving
accuracy while satisfying computational budget constraints is still a great challenge (Ross & Fahroo,
2006; Baoti¬¥c et al., 2008). To alleviate computational overheads, we detail a procedure for ofÔ¨Çine
optimization and subsequent online application of hypersolvers (Poli et al., 2020a) to optimal con-
trol problems. These hybrid solvers achieve the accuracy of higher‚Äìorder methods by augmenting
numerical results of a base solver with a learning component trained to approximate local truncation
residuals. When the cost of a single forward‚Äìpass of the learning component is kept sufÔ¨Åciently
small, hypersolvers improve the computation‚Äìaccuracy Pareto front of low‚Äìorder explicit solvers
(Butcher, 1997). However, direct application of hybrid solvers to controlled dynamical system in-
volves learning truncation residuals on the higher‚Äìdimensional spaces of state and control inputs. To
extend the range of applicability of hypersolvers to controlled dynamical systems, we propose two
pretraining strategies designed to improve, in the set of admissible control inputs, on the average or
worst‚Äìcase hypersolver solution. With the proposed methodology, we empirically show that Pareto
front improvements of hypersolvers hold even for optimal control tasks. In particular, we then carry
out performance and generalization evaluations in direct and model predictive control tasks. Here,
we conÔ¨Årm Pareto front improvements in terms of solution accuracy and subsequent control per-
formance, leading to higher quality control policies and lower control losses. In high‚Äìdimensional
regimes, we obtain the same control policy as the one obtained by accurate high‚Äìorder solvers with
more than 3√ó speedup."
INTRODUCTION,0.010443864229765013,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013054830287206266,"{ùë•ùë•0, ùë¢ùë¢0}"
INTRODUCTION,0.015665796344647518,Hypersolver Training
INTRODUCTION,0.018276762402088774,"Distribution ùúâùúâ(ùë•ùë•, ùë¢ùë¢) of possible states and controller values"
INTRODUCTION,0.020887728459530026,"Hypersolver ùíàùíàùíòùíò(ùë•ùë•, ùë¢ùë¢) with base solver ùúìùúì"
INTRODUCTION,0.02349869451697128,"Calculate ‚Ñì(ùëÖùëÖ, ùíàùíàùíòùíò(ùë•ùë•, ùë¢ùë¢))"
INTRODUCTION,0.02610966057441253,and backpropagate
INTRODUCTION,0.028720626631853787,"ùëÖùëÖ‚Üêùõ∑ùõ∑ùë•ùë•, ùë¢ùë¢‚àíùúìùúìùë•ùë•, ùë¢ùë¢‚àíùë•ùë•
ùúñùúñùëùùëù+1"
INTRODUCTION,0.031331592689295036,Pre-trained ùíàùíàùíòùíò
INTRODUCTION,0.033942558746736295,"Sample batches from ùúâùúâ(ùë•ùë•, ùë¢ùë¢)"
INTRODUCTION,0.03655352480417755,"Calculate residuals
ùíàùíà
‚Ñì
ùíôùíô ùíñùíñ"
INTRODUCTION,0.0391644908616188,"{ùë•ùë•1, ùë¢ùë¢1}
{ùë•ùë•2, ùë¢ùë¢2}
{ùë•ùë•ùëõùëõ, ùë¢ùë¢ùëõùëõ}"
INTRODUCTION,0.04177545691906005,Optimal Control
INTRODUCTION,0.044386422976501305,"Cost function ùêΩùêΩ(ùë•ùë•, ùë¢ùë¢)"
INTRODUCTION,0.04699738903394256,Initial condition ùë•ùë•0
INTRODUCTION,0.04960835509138381,Solve optimization problem
INTRODUCTION,0.05221932114882506,where ùë•ùë•ùëòùëò+1 = ùë•ùë•ùëòùëò+ ùúñùúñùúñùúñ+ ùúñùúñùëùùëù+1 ùíàùíàùíòùíò
INTRODUCTION,0.05483028720626632,Apply control inputs ùë¢ùë¢0 ùë¢ùë¢1 ùë¢ùë¢2 ùë¢ùë¢3
INTRODUCTION,0.057441253263707574,Solve optimization problem
INTRODUCTION,0.06005221932114883,"ùíàùíà
ùíàùíàùíòùíò
ùíôùíôùíåùíå ùíñùíñùíåùíå"
INTRODUCTION,0.06266318537859007,"min
ùë¢ùë¢
‡∑ç"
INTRODUCTION,0.06527415143603134,"ùëòùëò
ùêΩùêΩ(ùë•ùë•ùëòùëò, ùë¢ùë¢ùëòùëò) , ùëòùëò‚àà(0, ùëáùëá‚àí1)"
INTRODUCTION,0.06788511749347259,Feed-forward hypersolver
INTRODUCTION,0.07049608355091384,Shift the receding
INTRODUCTION,0.0731070496083551,horizon forward ùúñùúñ ùë•ùë•0 ùë•ùë•0
INTRODUCTION,0.07571801566579635,"Figure 1: Overview of the proposed method. [Left] The hypersolver is trained to approximate residuals given
a distribution of control inputs and states. [Right] The pre‚Äìtrained hypersolver model is then used to accelerate
and improve the accuracy of numerical solutions used during optimization of control policies, leading to higher‚Äì
quality controllers."
NUMERICAL OPTIMAL CONTROL,0.0783289817232376,"2
NUMERICAL OPTIMAL CONTROL"
NUMERICAL OPTIMAL CONTROL,0.08093994778067885,We consider control of general nonlinear systems of the form
NUMERICAL OPTIMAL CONTROL,0.0835509138381201,"Àôx(t) = f(t, x(t), uŒ∏(t))
x(0) = x0
(1)"
NUMERICAL OPTIMAL CONTROL,0.08616187989556136,"with state x ‚ààX ‚äÇRnx, input uŒ∏ ‚ààU ‚äÇRnu deÔ¨Åned on a compact time domain T := [t0, T]
where Œ∏ is a Ô¨Ånite set of free parameters of the controller. Solutions of (1) are denoted with x(t) =
Œ¶(x(s), s, t) for all s, t ‚ààT . Given some objective function J : X √óU ‚ÜíR; x0, uŒ∏ 7‚ÜíJ(x0, uŒ∏(t))
and a distribution œÅ0(x0) of initial conditions with support in X, we consider the following nonlinear
program, constrained to the system dynamics:"
NUMERICAL OPTIMAL CONTROL,0.08877284595300261,"min
uŒ∏(t)
Ex0‚àºœÅ0(x0) [J(x0, uŒ∏(t))]"
NUMERICAL OPTIMAL CONTROL,0.09138381201044386,"subject to
Àôx(t) = f(t, x(t), uŒ∏(t))
x(0) = x0
t ‚ààT (2)"
NUMERICAL OPTIMAL CONTROL,0.09399477806788512,"where the controller parameters Œ∏ are optimized. We will henceforth omit the subscript Œ∏ and write
u(t) = uŒ∏(t). Since analytic solutions of (2) exist only for limited classes of systems and objectives,
numerical solvers are often applied to iteratively Ô¨Ånd a solution. For these reasons, problem 2 is often
referred to as numerical optimal control."
NUMERICAL OPTIMAL CONTROL,0.09660574412532637,"Direct optimal control
If the problem (2) is solved ofÔ¨Çine by directly optimizing over complete
trajectories, we call it direct optimal control. The inÔ¨Ånite‚Äìdimensional optimal control problem is
time‚Äìdiscretized and solved numerically: the obtained control policy is then applied to the real target
system without further optimization."
NUMERICAL OPTIMAL CONTROL,0.09921671018276762,"Model predictive control
Also known in the literature as receding horizon control, Model Pre-
dictive Control (MPC) is a class of Ô¨Çexible control algorithms capable of taking into consideration
constraints and nonlinearities (Mayne & Michalska, 1988; Garcia et al., 1989). MPC considers Ô¨Å-
nite time windows which are then shifted forward in a receding manner. The control problem is
then solved for each window by iteratively forward‚Äìpropagating trajectories with numerical solvers
i.e. predicting the set of future trajectories with a candidate controller u(t) and then adjusting it it-
eratively to optimize the cost function J (further details on the MPC formulation in Appendix B.2).
The optimization is reiterated online until the end of the control time horizon."
NUMERICAL OPTIMAL CONTROL,0.10182767624020887,Published as a conference paper at ICLR 2022
SOLVER RESIDUALS,0.10443864229765012,"2.1
SOLVER RESIDUALS"
SOLVER RESIDUALS,0.10704960835509138,"Given nominal solutions Œ¶ of (1) we can deÔ¨Åne the residual of a numerical ODE solver as the
normalized error accumulated in a single step size of the method, i.e."
SOLVER RESIDUALS,0.10966057441253264,"Rk = R(tk, x(tk), u(tk)) =
1
œµp+1"
SOLVER RESIDUALS,0.1122715404699739,"h
Œ¶(x(tk), tk, tk+1) ‚àíx(tk) ‚àíœµœàœµ(tk, x(tk), u(tk))
i
(3)"
SOLVER RESIDUALS,0.11488250652741515,"where œµ is the step size and p is the order of the numerical solver corresponding to œàœµ. From the
deÔ¨Ånition of residual in (3), we can deÔ¨Åne the local truncation error ek :=
œµp+1Rk

2 which is the
error accumulated in a single step; while the global truncation error Ek = ‚à•x(tk) ‚àíxk‚à•2 represents
the error accumulated in the Ô¨Årst k steps of the numerical solution. Given a p‚Äìth order explicit
solver, we have ek = O(œµp+1) and Ek = O(œµp) (Butcher, 1997)."
HYPERSOLVERS FOR OPTIMAL CONTROL,0.1174934725848564,"3
HYPERSOLVERS FOR OPTIMAL CONTROL"
HYPERSOLVERS FOR OPTIMAL CONTROL,0.12010443864229765,"We extend the range of applicability of hypersolvers (Poli et al., 2020a) to controlled dynamical sys-
tems. In this Section we discuss the proposed hypersolver architectures and pre‚Äìtraining strategies
of the proposed hypersolver methodology for numerical optimal control of controlled dynamical
systems."
HYPERSOLVERS,0.1227154046997389,"3.1
HYPERSOLVERS"
HYPERSOLVERS,0.12532637075718014,"Given a p‚Äìorder base solver update map œàœµ, the corresponding hypersolver is the discrete iteration"
HYPERSOLVERS,0.1279373368146214,"xk+1 = xk + œµœàœµ (tk, xk, uk)
|
{z
}
base solver step"
HYPERSOLVERS,0.13054830287206268,"+œµp+1 gœâ (tk, xk, uk)
|
{z
}
approximator
(4)"
HYPERSOLVERS,0.13315926892950392,"where gœâ (tk, xk, uk) is some o(1) parametric function with free parameters œâ. The core idea is to
select gœâ as some function with universal approximation properties and Ô¨Åt the higher-order terms of
the base solver by explicitly minimizing the residuals over a set of state and control input samples.
This procedure leads to a reduction of the overall local truncation error ek, i.e. we can improve the
base solver accuracy with the only computational overhead of evaluating the function gœâ. It is also
proven that, if gœâ is a Œ¥‚Äìapproximator of R, i.e. ‚àÄk ‚ààN‚â§K
‚à•R (tk, x(tk), u(tk)) ‚àígœâ (tk, x(tk), u(tk))‚à•2 ‚â§Œ¥
(5)"
HYPERSOLVERS,0.13577023498694518,"then ek ‚â§o(Œ¥œµp+1), where Œ¥ > 0 depends on the hypersolver training results (Poli et al., 2020a,
Theorem 1). This result practically guarantees that if gœâ is a good approximator for R, i.e. Œ¥ ‚â™1,
then the overall local truncation error of the hypersolved ODE is signiÔ¨Åcantly reduced with guaran-
teed upper bounds."
NUMERICAL OPTIMAL CONTROL WITH HYPERSOLVERS,0.13838120104438642,"3.2
NUMERICAL OPTIMAL CONTROL WITH HYPERSOLVERS"
NUMERICAL OPTIMAL CONTROL WITH HYPERSOLVERS,0.1409921671018277,"Our approach relies on the pre‚Äìtrained hypersolver model for obtaining solutions to the trajectories
of the optimal control problem (2). After the initial training stage, control policies are numerically
optimized to minimize the cost function J (see Appendix B.3 for further details). Figure 1 shows an
overview of the proposed approach consisting in pre‚Äìtraining and system control."
NUMERICAL OPTIMAL CONTROL WITH HYPERSOLVERS,0.14360313315926893,"4
HYPERSOLVER PRE‚ÄìTRAINING AND ARCHITECTURES"
NUMERICAL OPTIMAL CONTROL WITH HYPERSOLVERS,0.1462140992167102,"We introduce in Section 4.1 loss functions which are used in the proposed pre‚Äìtraining methods
of Section 4.2 and Section 4.3. We also check the generalization properties of hypersolvers with
different architectures in Section 4.4. In Section 4.5 we introduce multi‚Äìstage hypersolvers in which
an additional Ô¨Årst‚Äìorder learned term is employed for correcting errors in the vector Ô¨Åeld."
LOSS FUNCTIONS,0.14882506527415143,"4.1
LOSS FUNCTIONS"
LOSS FUNCTIONS,0.1514360313315927,"Residual Ô¨Åtting
Training the hypersolver on a single nominal trajectory {x(tk)}k results in a
supervised learning problem where we minimize point‚Äìwise the Euclidean distance between the"
LOSS FUNCTIONS,0.15404699738903394,Published as a conference paper at ICLR 2022
LOSS FUNCTIONS,0.1566579634464752,"‚àí20
0
20
u [N] 10‚àí8 10‚àí5 10‚àí2 101"
LOSS FUNCTIONS,0.15926892950391644,Mean Residual R
LOSS FUNCTIONS,0.1618798955613577,œµ = 0.01 [s]
LOSS FUNCTIONS,0.16449086161879894,"‚àí20
0
20
u [N]"
LOSS FUNCTIONS,0.1671018276762402,œµ = 0.03 [s]
LOSS FUNCTIONS,0.16971279373368145,"‚àí20
0
20
u [N]"
LOSS FUNCTIONS,0.17232375979112272,œµ = 0.1 [s]
LOSS FUNCTIONS,0.17493472584856398,"‚àí20
0
20
u [N]"
LOSS FUNCTIONS,0.17754569190600522,œµ = 0.3 [s]
LOSS FUNCTIONS,0.1801566579634465,"‚àí20
0
20
u [N]"
LOSS FUNCTIONS,0.18276762402088773,œµ = 1.0 [s]
LOSS FUNCTIONS,0.185378590078329,HyperEuler Euler
LOSS FUNCTIONS,0.18798955613577023,Midpoint RK4
LOSS FUNCTIONS,0.1906005221932115,"Figure 2: Mean local residuals of the spring‚Äìmass system of (17) as a function of control inputs at different step
sizes œµ. HyperEuler (see Appendix A.1 for its explicit formulation) improves on the local residuals compared
to the baseline Euler and even compared to higher-order ODE solvers at larger step sizes."
LOSS FUNCTIONS,0.19321148825065274,"residual (3) and the output of gœâ, resulting in an optimization problem minimizing a loss function ‚Ñì
of the form"
LOSS FUNCTIONS,0.195822454308094,"‚Ñì(t, x, u) = 1 K K‚àí1
X"
LOSS FUNCTIONS,0.19843342036553524,"k=0
‚à•R (tk, x(tk), u(tk)) ‚àígœâ (tk, x(tk), u(tk))‚à•2
(6)"
LOSS FUNCTIONS,0.2010443864229765,which is also called residual Ô¨Åtting since the target of gw is the residual R.
LOSS FUNCTIONS,0.20365535248041775,"Trajectory Ô¨Åtting
The optimization can also be carried out via trajectory Ô¨Åtting as following"
LOSS FUNCTIONS,0.206266318537859,"‚Ñì(t, x, u) = 1 K K‚àí1
X"
LOSS FUNCTIONS,0.20887728459530025,"k=0
‚à•x(tk+1), xk+1‚à•2
(7)"
LOSS FUNCTIONS,0.21148825065274152,"where x(tk+1) corresponds to the exact one‚Äìstep trajectory and xk+1 is its approximation, derived
via (4) for standard hypersolvers or via (11) for their multi‚Äìstage counterparts. This method can also
be used to contain the global truncation error in the T domain. We will refer to ‚Ñìas a loss function
of either residual or trajectory Ô¨Åtting types; we note that these loss functions may also be combined
depending on the application. The goal is to train the hypersolver network to explore the state‚Äì
control spaces so that it can effectively minimize the truncation error. We propose two methods
with different purposes: stochastic exploration aiming at minimizing the average truncation error
and active error minimization whose goal is to reduce the maximum error i.e., due to control inputs
yielding high losses."
STOCHASTIC EXPLORATION,0.21409921671018275,"4.2
STOCHASTIC EXPLORATION"
STOCHASTIC EXPLORATION,0.21671018276762402,"Stochastic exploration aims to minimize the average error of the visited state‚Äìcontroller space i.e.,
to produce optimal hypersolver parameters œâ‚àóas the solution of a nonlinear program"
STOCHASTIC EXPLORATION,0.2193211488250653,"œâ‚àó= arg min
œâ
EŒæ(x,u)[‚Ñì(t, x, u)]
(8)"
STOCHASTIC EXPLORATION,0.22193211488250653,"where Œæ(x, u) is a distribution with support in X √ó U of the state and controller spaces and ‚Ñìis
the training loss function. In order to guarantee sufÔ¨Åcient exploration of the state‚Äìcontroller space,
we use Monte Carlo sampling (Robert & Casella, 2013) from the given distribution. In particular,
batches of initial conditions {xi
0}, {ui
0} are sampled from Œæ and the loss function ‚Ñìis calculated with
the given system and step size œµ. We then perform backpropagation for updating the parameters of
the hypersolver using a stochastic gradient descent (SGD) algorithm e.g., Adam (Kingma & Ba,
2017) and repeat the procedure for every training epoch. Figure 2 shows pre‚Äìtraining results with
stochastic exploration for different step sizes (see Appendix C.2). We notice how higher residual
values generally correspond to higher absolute values of control inputs. Many systems in practice
are subject to controls that are constrained in magnitude either due to physical limitations of the
actuators or safety restraints of the workspace. This property allows us to design an exploration
strategy that focuses on worst-case scenarios i.e. largest control inputs."
STOCHASTIC EXPLORATION,0.2245430809399478,Published as a conference paper at ICLR 2022
STOCHASTIC EXPLORATION,0.22715404699738903,"‚àí4œÄ‚àí2œÄ 0
2œÄ 4œÄ
p ‚àí10 0 10 q"
STOCHASTIC EXPLORATION,0.2297650130548303,Without Hypersolver
STOCHASTIC EXPLORATION,0.23237597911227154,Euler Residuals
STOCHASTIC EXPLORATION,0.2349869451697128,"‚àí4œÄ‚àí2œÄ 0
2œÄ 4œÄ
p Tanh"
STOCHASTIC EXPLORATION,0.23759791122715404,"x ‚Üíex‚àíe‚àíx
ex+e‚àíx"
STOCHASTIC EXPLORATION,0.2402088772845953,"‚àí4œÄ‚àí2œÄ 0
2œÄ 4œÄ
p"
STOCHASTIC EXPLORATION,0.24281984334203655,"ReLU
x ‚Üímax(0, x)"
STOCHASTIC EXPLORATION,0.2454308093994778,"‚àí4œÄ‚àí2œÄ 0
2œÄ 4œÄ
p"
STOCHASTIC EXPLORATION,0.24804177545691905,"SIREN
x ‚Üísin(Wix + bi)"
STOCHASTIC EXPLORATION,0.2506527415143603,"‚àí4œÄ‚àí2œÄ 0
2œÄ 4œÄ
p"
STOCHASTIC EXPLORATION,0.25326370757180156,"Snake
x ‚Üíx + 1
a sin2(ax) 15 30 45 60 75"
STOCHASTIC EXPLORATION,0.2558746736292428,Mean Residual
STOCHASTIC EXPLORATION,0.2584856396866841,"Figure 4: Generalization outside of the training region (red rectangle) in the state space of an inverted pendu-
lum model with different hypersolver activation functions. Architectures containing activation functions with
periodic components achieve better extrapolation properties compared to the others."
ACTIVE ERROR MINIMIZATION,0.26109660574412535,"4.3
ACTIVE ERROR MINIMIZATION 0.0 2.5 5.0 7.5 MAE √ó10‚àí4"
ACTIVE ERROR MINIMIZATION,0.26370757180156656,"0.0
0.1
0.2
0.3
Time [s] 0.0 2.5 5.0 7.5 MAE √ó10‚àí4"
ACTIVE ERROR MINIMIZATION,0.26631853785900783,Stochastic Exploration
ACTIVE ERROR MINIMIZATION,0.2689295039164491,Active Error Minimization
ACTIVE ERROR MINIMIZATION,0.27154046997389036,Midpoint
ACTIVE ERROR MINIMIZATION,0.2741514360313316,"Figure 3:
Mean Absolute Error (MAE) along tra-
jectories with different pre‚Äìtraining techniques on the
spring‚Äìmass system of (17). [Top] Stochastic explo-
ration performs better on average i.e. u ‚àà[‚àí100, 100].
[Bottom] Active error minimization achieves better re-
sults in limit situations as in the case of a bang‚Äìbang
controller i.e. u ‚àà{‚àí100, 100}, in which controllers
yielding the highest residuals have been minimized."
ACTIVE ERROR MINIMIZATION,0.27676240208877284,"The goal of active error minimization is to ac-
tively reduce the highest losses in terms of the
control inputs, i.e., to obtain w‚àóas the solution
to a minmax problem:"
ACTIVE ERROR MINIMIZATION,0.2793733681462141,"w‚àó= arg min
œâ
max
u‚ààU ‚Ñì(t, x, u)
(9)"
ACTIVE ERROR MINIMIZATION,0.2819843342036554,"Similarly to stochastic exploration, we create
distribution Œæ(x, u) with support in X √ó U and
perform Monte Carlo sampling of n batches
{(xi, ui)}, from Œæ. Then, losses are computed
pair‚Äìwisely for each state xj, j = 0, . . . , n ‚àí1
with each control input uk, k = 0, . . . , n ‚àí1.
We then take the Ô¨Årst n controllers {ui‚Ä≤
0 } yield-
ing the maximum loss for each state.
The
loss is recalculated using these controller values
with their respective states and SGD updates
to hypersolver parameters are performed. Fig-
ure 3 shows a comparison of the pre‚Äìtraining
techniques (further experimental details in Ap-
pendix C.2). The propagated error on trajecto-
ries for the hypersolver pre‚Äìtrained via stochas-
tic exploration is lower on average with ran-
dom control inputs compared with the one pre‚Äì
trained with active error minimization.
The
latter accumulates lower error for controllers
yielding high residuals."
ACTIVE ERROR MINIMIZATION,0.2845953002610966,Different exploration strategies may be used depending on the down‚Äìstream control task.
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.28720626631853785,"4.4
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.2898172323759791,"We have assumed the state and controller spaces to be bounded and that training be performed
by sampling for their known distributions. While this is sufÔ¨Åcient for optimal control problems
given a priori known bounds, we also investigate how the system generalizes to unseen states and
control input values. In particular, we found that activation functions have an impact on the end
result of generalization beyond training boundaries. We take into consideration two commonly
used activation functions, Tanh : x ‚Üíex‚àíe‚àíx"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.2924281984334204,"ex+e‚àíx and ReLU : x ‚Üímax(0, x), along with network
architectures which employ activation functions containing periodic components: SIREN : x ‚Üí
sin (Wx + b) (Sitzmann et al., 2020) and Snake : x ‚Üíx + 1"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.2950391644908616,"a sin2(ax) (Ziyin et al., 2020). We
train hypersolver models with the different activation functions for the inverted pendulum model
of (18) with common experimental settings (see Appendix C.3). Figure 4 shows generalization"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.29765013054830286,Published as a conference paper at ICLR 2022
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.3002610966057441,"outside the training states (see Figure 9 in Appendix C.3 for generalization of controllers and step
sizes). We notice that while Tanh and ReLU perform well on the training set of interest, performance
degrades rapidly outside of it. On the other one hand, SIREN and Snake manage to extrapolate the
periodicity of the residual distribution even outside of the training region, thus providing further
empirical evidence of the universal extrapolation theorem (Ziyin et al., 2020, Theorem 3)."
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.3028720626631854,Activation function choice plays an important role in Hypersolver performance and generalization.
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.30548302872062666,"4.5
MULTI‚ÄìSTAGE HYPERSOLVERS"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.30809399477806787,"We have so far considered the case in which the vector Ô¨Åeld (1) fully characterizes the system dy-
namics. However, if the model does not completely describe the actual system dynamics, Ô¨Årst‚Äìorder
errors are introduced. We propose Multi-Stage Hypersolvers to correct these errors: an
additional term is introduced in order to correct the inaccurate dynamics f. The resulting procedure
is a modiÔ¨Åed version of (4) in which the base solver œàœµ (tk, xk, uk) does not iterate over the modeled
vector Ô¨Åeld f but over its corrected version f ‚ãÜ:"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.31070496083550914,"f ‚ãÜ(tk, xk, uk) = f (tk, xk, uk)
|
{z
}
partial dynamics"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.3133159268929504,"+ hw (tk, xk, uk)
|
{z
}
inner stage (10)"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.31592689295039167,"where hw is a function with universal approximation properties. While the inner stage hw is a Ô¨Årst‚Äì
order error approximator, the outer stage gœâ further reduces errors approximating the p‚Äìth order
residuals:"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.3185378590078329,xk+1 = xk + œµœàœµ Ô£´
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.32114882506527415,"Ô£¨
Ô£≠tk, xk, uk, f ‚ãÜ(tk, xk, uk)
|
{z
}
corrected dynamics Ô£∂"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.3237597911227154,"Ô£∑
Ô£∏+
œµp+1 gœâ (tk, xk, uk)
|
{z
}
outer stage
(11)"
GENERALIZATION PROPERTIES OF DIFFERENT ARCHITECTURES,0.3263707571801567,"We note that f ‚ãÜis continuously adjusted due to the optimization of hw. For this reason, it is not
possible to derive the analytical expression of the residuals to train the stages with the residual Ô¨Åtting
loss function (6). Instead, both stages can be optimized at the same time via backpropagation calcu-
lated on one‚Äìstep trajectory Ô¨Åtting loss (7) which does not require explicit residuals calculation."
EXPERIMENTS,0.3289817232375979,"5
EXPERIMENTS"
EXPERIMENTS,0.33159268929503916,"We introduce the experimental results divided for each system into hypersolver pre‚Äìtraining and sub-
sequent optimal control. We use as accurate adaptive step‚Äìsize solvers the Dormand/Prince method
dopri5 (Dormand & Prince, 1980) and an improved version of it by Tsitouras tsit5 (Tsitouras,
2011) for training the hypersolvers and to test the control performance at runtime."
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.3342036553524804,"5.1
DIRECT OPTIMAL CONTROL OF A PENDULUM"
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.3368146214099217,"Hypersolver pre‚Äìtraining
We consider the inverted pendulum model with a torsional spring
described in (18).
We select Œæ(x, u) as a uniform distribution with support in X √ó U where
X = [‚àí2œÄ, 2œÄ] √ó [‚àí2œÄ, 2œÄ] and U = [‚àí5, 5] to guarantee sufÔ¨Åcient exploration of the state-
controller space. Nominal solutions are calculated using tsit5 with absolute and relative tolerances
set to 10‚àí5. We train the hypersolver on local residuals via stochastic exploration using the Adam
optimizer with learning rate of 3 √ó 10‚àí4 for 3 √ó 105 epochs."
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.3394255874673629,"Direct optimal control
The goal is to stabilize the inverted pendulum in the vertical position
x‚ãÜ= [0, 0]. We choose t ‚àà[0, 3] and a step size œµ = 0.2 s for the experiment. The control
input is assumed continuously time‚Äìvarying. The neural controller is optimized via SGD with Adam
with learning rate of 3 √ó 10‚àí3 for 1000 epochs. Figure 5 shows nominal controlled trajectories
of HyperEuler and other baseline Ô¨Åxed‚Äìstep size solvers. Trajectories obtained with the controller
optimized with HyperEuler reach Ô¨Ånal positions q = (1.6 ¬± 17.6) √ó 10‚àí2 while Midpoint and RK4
ones q = (‚àí0.6 ¬± 12.7) √ó 10‚àí2 and q = (1.1 ¬± 12.8) √ó 10‚àí2 respectively. On the other hand,
the controller optimized with the Euler solver fails to control some trajectories obtaining a Ô¨Ånal"
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.34203655352480417,Published as a conference paper at ICLR 2022
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.34464751958224543,"q = (6.6 ¬± 19.4) √ó 10‚àí1. HyperEuler considerably improved on the Euler baseline while requiring
only 1.2% more Floating Point Operations (FLOPs) and 49.5% less compared to Midpoint. Further
details are available in Appendix C.3."
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.3472584856396867,"‚àí3
3
q
‚àí5 0 5 p"
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.34986945169712796,HyperEuler
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.3524804177545692,"‚àí3
3
q Euler"
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.35509138381201044,"‚àí3
3
q"
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.3577023498694517,Midpoint
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.360313315926893,"‚àí3
3
q RK4"
DIRECT OPTIMAL CONTROL OF A PENDULUM,0.3629242819843342,"Figure 5: Direct optimal control of the inverted pendulum in phase space. While the controller optimized with
the Euler solver fails to control the system for some trajectories, the one obtained with HyperEuler improves
the performance while introducing a minimal overhead with results comparable to higher‚Äìorder solvers."
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.36553524804177545,"5.2
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM"
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.3681462140992167,"Hypersolver pre‚Äìtraining
We consider the partial dynamics of the cart‚Äìpole system of (19) with
wrong parameters for the frictions between cart and track as well as the one between cart and pole.
We employ the multi‚Äìstage hypersolver approach to correct the Ô¨Årst‚Äìorder error in the vector Ô¨Åeld
as well as base solver residual. We select Œæ(x, u) as a uniform distribution with support in X √ó U
where X = [‚àí2œÄ, 2œÄ] √ó [‚àí2œÄ, 2œÄ] √ó [‚àí2œÄ, 2œÄ] √ó [‚àí2œÄ, 2œÄ] and U = [‚àí10, 10]. Nominal solutions
are calculated on the accurate system using RungeKutta 4 instead of adaptive‚Äìstep solvers due
faster training times. We train our multi‚Äìstage Hypersolver (i.e. a multi‚Äìstage hypersolver with
the second‚Äìorder Midpoint as base solver with the partial dynamics) on nominal trajectories of the
accurate system via stochastic exploration using the Adam optimizer for 5 √ó 104 epochs, where we
set the learning rate to 10‚àí2 for the Ô¨Årst 3 √ó104 epochs, then decrease it to 10‚àí3 for 104 epochs and
to 10‚àí4 for the last 104."
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.370757180156658,"Model predictive control
The goal is to stabilize the cart‚Äìpole system in the vertical position
around the origin, e.g. x‚ãÜ= [0, 0, 0, 0]. We choose t ‚àà[0, 3] and a step size œµ = 0.05 s for
the experiment. The control input is assumed piece-wise constant during MPC sampling times.
The receding horizon is chosen as 1 s. The neural controller is optimized via SGD with Adam
with learning rate of 3 √ó 10‚àí3 for a maximum of 200 iterations at each sampling time. Figure 6
shows nominal controlled trajectories of multi‚Äìstage Hypersolver and other baseline solvers. The
Midpoint solver on the inaccurate model fails to stabilize the system at the origin position x =
(39.7 ¬± 97.7) cm, while multi‚Äìstage Hypersolver manages to stabilize the cart‚Äìpole system and
improve on Ô¨Ånal positions x = (7.8 ¬± 3.0) cm. Further details are available in Appendix C.4."
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.3733681462140992,"0
1
2
3
Time [s] ‚àí1 0 1 2 x [m]"
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.37597911227154046,"0
1
2
3
Time [s] 0 œÄ
2 œÄ"
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.3785900783289817,Œ∏ [rad]
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.381201044386423,"0
1
2
3
Time [s] ‚àí30 0 30"
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.3838120104438642,Control input [N]
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.38642297650130547,"0
1
2
3
Time [s] 0 50 100"
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.38903394255874674,Abs. energy input [J]
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.391644908616188,"Multistage Hypersolver (inaccurate model)
Midpoint (inaccurate model)
Euler (nominal model)
Midpoint (nominal model)
Figure 6: Model Predictive Control with constrained inputs on the cart‚Äìpole model. MPC with the Midpoint
solver iterating on the partial dynamic model successfully swings up the pole but fails to reach the target
position. Multi‚Äìstage Hypersolver with the Midpoint base solver has knowledge restricted to the inaccurate
system, yet it manages to obtain a similar control performance compared to controllers with access to the
nominal dynamics while also needing less control effort and absolute energy inÔ¨Çow compared to its base solver."
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.39425587467362927,Multi‚Äìstage Hypersolvers can correct Ô¨Årst‚Äìorder errors on dynamic models and base solver residuals.
MODEL PREDICTIVE CONTROL OF A CART-POLE SYSTEM,0.3968668407310705,Published as a conference paper at ICLR 2022
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.39947780678851175,"5.3
MODEL PREDICTIVE CONTROL OF A QUADCOPTER"
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.402088772845953,"Hypersolver pre‚Äìtraining
We consider the quadcopter model of (20). We select Œæ(x, u) as a
uniform distribution with support in X √ó U where X is chosen as a distribution of possible visited
states and each of the four motors i ‚àà[0, 3] has control inputs ui ‚àà[0, 2.17] √ó 105 rpm. Nominal
solutions are calculated on the accurate system using dopri5 with relative and absolute tolerances
set to 10‚àí7 and 10‚àí9 respectively. We train HyperEuler on local residuals via stochastic exploration
using the Adam optimizer with learning rate of 10‚àí3 for 105 epochs."
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4046997389033943,"Model predictive control
The control goal is to reach a Ô¨Ånal positions [x, y, z]‚ãÜ= [8, 8, 8] m.
We choose t ‚àà[0, 3] and a step size œµ = 0.02 s for the experiment. The control input is assumed
piece‚Äìwise constant during MPC sampling times. The receding horizon is chosen as 0.5 s. The
neural controller is optimized via SGD with Adam with learning rate of 10‚àí2 for 20 iterations at each
sampling time. Figure 7 shows local residual distribution and control performance on the quadcopter
over 30 experiments starting at random initial conditions which are kept common for the different
ODE solvers. HyperEuler requires a single function evaluation per step as for the Euler solver
compared to two function evaluations per step for Midpoint and four for RK4. Controlled trajectories
optimized with Euler, Midpoint and RK4 collect an error on Ô¨Ånal positions of (1.09 ¬± 0.37) m,
(0.71 ¬± 0.17) m, (0.70 ¬± 0.19) m respectively while HyperEuler achieves the lowest terminal error
value of (0.66 ¬± 0.24) m. Additional experimental details are available in Appendix C.5."
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4073107049608355,"HyperEuler
Euler
Midpoint
RK4 10‚àí5 10‚àí4 10‚àí3 10‚àí2 10‚àí1 100 101 102"
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.40992167101827676,Mean Residual R x [m]
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.412532637075718,"0
2
4
6
8
10 y [m] 0 2 4 6 8 10 z [m]"
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4151436031331593,"0
2
4
6
8 10"
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4177545691906005,HyperEuler Euler
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.42036553524804177,Midpoint RK4
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.42297650130548303,Initial Position
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4255874673629243,"Target
HyperEuler
Euler
Midpoint
RK4 100"
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4281984334203655,3 √ó 10‚àí1
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4308093994778068,4 √ó 10‚àí1
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.43342036553524804,6 √ó 10‚àí1
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4360313315926893,2 √ó 100
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4386422976501306,Final Position Error
MODEL PREDICTIVE CONTROL OF A QUADCOPTER,0.4412532637075718,"Figure 7: [Left] Local residual distribution for the quadcopter model for œµ = 0.02 s. [Center] Trajectories
of controlled quadcopters with MPC whose receding horizon controller is optimized by solving the ODE with
different methods. [Right] Final positions error distribution. The proposed approach with HyperEuler achieves
lower average error compared to other baseline solvers while requiring a low overhead compared to higher‚Äì
order solvers due to a smaller number of dynamics function evaluations."
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.44386422976501305,"5.4
BOUNDARY CONTROL OF A TIMOSHENKO BEAM"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4464751958224543,"Hypersolver pre‚Äìtraining
We consider the Ô¨Ånite element discretization of the Timoshenko beam
of (22). We create Œæ(x, u) as a distribution with support in X √ó U which is generated at training
time via random walks from known boundary conditions in order to guarantee both physical feasi-
bility and sufÔ¨Åcient exploration of the state-controller space (see Appendix C.6 for further details).
Nominal solutions are calculated using tsit5 with absolute and relative tolerances set to 10‚àí5. We
train the hypersolver on local residuals via stochastic exploration using the Adam optimizer for 105
epochs, where we set the learning rate to 10‚àí3 for the Ô¨Årst 8 √ó 104 epochs, then decrease it to 10‚àí4
for 104 epochs and to 10‚àí5 for the last 104."
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4490861618798956,"Boundary direct optimal control
The task is to stabilize the beam in the straight position, i.e.
each of its elements i have velocities vi
t, vi
r and displacements œÉi
t, œÉi
r equal to 0. We choose t ‚àà[0, 3]
and step size œµ = 5 ms for the experiment. The control input is assumed continuously time‚Äì
varying. The neural controller is optimized via SGD with Adam with learning rate of 10‚àí3 for 1000
epochs. Figure 8 shows nominal controlled trajectories for HyperEuler and other baseline Ô¨Åxed‚Äì
step size solvers. Control policies trained with Euler and Midpoint obtain averaged Ô¨Ånal states of
(‚àí2.8¬±4.2)√ó10‚àí1 and (‚àí0.04¬±4.6)√ó10‚àí1 thus failing to stabilize the beam, while HyperEuler
and RK4 obtain (‚àí0.6¬±4.9)√ó10‚àí3 and (‚àí0.5¬±3.3)√ó10‚àí3 respectively. HyperEuler considerably
improves on both the Euler and Midpoint baselines obtaining a very similar performance to RK4,
while requiring 72.9% less FLOPs; the mean runtime per training iteration was cut from 8.24 s for
RK4 to just 2.53 s for HyperEuler. Further details on this experiment are available in Appendix C.6."
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4516971279373368,Published as a conference paper at ICLR 2022
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.45430809399477806,"t
0
1
2
3 x
0.0 0.5 1.0"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.45691906005221933,"œÉt(x, t) ‚àí1 0 1"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4595300261096606,HyperEuler
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4621409921671018,"t
0
1
2
3 x
0.0 0.5 1.0"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.46475195822454307,"œÉt(x, t) ‚àí1 0 1 Euler"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.46736292428198434,"t
0
1
2
3 x
0.0 0.5 1.0"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4699738903394256,"œÉt(x, t) ‚àí1 0 1"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4725848563968668,Midpoint
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4751958224543081,"t
0
1
2
3 x
0.0 0.5 1.0"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.47780678851174935,"œÉt(x, t) ‚àí1 0 1 RK4"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4804177545691906,"t
0
1
2
3 x
0.0 0.5 1.0"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4830287206266319,"œÉr(x, t) ‚àí1 0 1"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4856396866840731,"t
0
1
2
3 x
0.0 0.5 1.0"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.48825065274151436,"œÉr(x, t) ‚àí1 0 1"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4908616187989556,"t
0
1
2
3 x
0.0 0.5 1.0"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4934725848563969,"œÉr(x, t) ‚àí1 0
1"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.4960835509138381,"t
0
1
2
3 x
0.0 0.5 1.0"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.49869451697127937,"œÉr(x, t) ‚àí1 0 1"
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.5013054830287206,"Figure 8: Displacement variables œÉt and œÉr of the discretized Timoshenko beam as a function of position x of
the Ô¨Ånite elements and time t. The controller optimized with HyperEuler manages to stabilize the beam while
the baseline solvers Euler and Midpoint fail, yet requiring less than a third in terms of runtime compared to
RK4."
BOUNDARY CONTROL OF A TIMOSHENKO BEAM,0.5039164490861618,Hypersolvers are even more impactful in complex high‚Äìdimensional controlled systems.
RELATED WORK,0.5065274151436031,"6
RELATED WORK"
RELATED WORK,0.5091383812010444,"This work is rooted in the broader literature on surrogate methods for speeding up simulations and
solutions of dynamical systems (Grzeszczuk et al., 1998; James & Fatahalian, 2003; Gorissen et al.,
2010). Differently from these approaches, we investigate a methodology to enable faster solution
during a downstream, online optimization problem involving a potential mismatch compared to data
seen during pre‚Äìtraining. We achieve this through the application of the hypersolver (Poli et al.,
2020a) paradigm. Modeling mismatches between approximate and nominal models is explored in
(Saveriano et al., 2017) where residual dynamics are learned efÔ¨Åciently along with the control policy
while (Fisac et al., 2018; Taylor et al., 2019) model systems uncertainties in the context of safety‚Äì
critical control. In contrast to previous work, we model uncertainties with the proposed multi‚Äìstage
hypersolver approach by closely interacting with the underlying ODE base solvers and their residu-
als to improve solution accuracy. The synergy between machine learning and optimal control con-
tinues a long line of research on introducing neural networks in optimal control (Hunt et al., 1992),
applied to modeling (Lin & Cunningham, 1995), identiÔ¨Åcation (Chu et al., 1990) or parametrization
of the controller itself (Lin et al., 1991). Existing surrogate methods for systems (Grzeszczuk et al.,
1998; James & Fatahalian, 2003) pay a computational cost upfront to accelerate downstream sim-
ulation. However, ensuring transfer from ofÔ¨Çine optimization to the online setting is still an open
problem. In our approach, we investigate several strategies for an accurate ofÔ¨Çine‚Äìonline transfer
of a given hypersolver, depending on desiderata on its performance in terms of average residuals
and error propagation on the online application. Beyond hypersolvers, our approach further lever-
ages the latest advances in hardware and machine learning software (Paszke et al., 2019) by solving
thousands of ODEs in parallel on graphics processing units (GPUs)."
CONCLUSION,0.5117493472584856,"7
CONCLUSION"
CONCLUSION,0.5143603133159269,"We presented a novel method for obtaining fast and accurate control policies. Hypersolver mod-
els were Ô¨Årstly pre‚Äìtrained on distributions of states and controllers to approximate higher‚Äìorder
residuals of base Ô¨Åxed‚Äìstep ODE solvers. The obtained models were then employed to improve the
accuracy of trajectory solutions over which control policies were optimized. We veriÔ¨Åed that our
method shows consistent improvements in the accuracy of ODE solutions and thus on the quality
of control policies optimized through numerical solutions of the system. We envision the proposed
approach to beneÔ¨Åt the control Ô¨Åeld and robotics in both simulated and potentially real‚Äìworld envi-
ronments by efÔ¨Åciently solving high‚Äìdimensional space‚Äìcontinuous problems."
CONCLUSION,0.5169712793733682,Published as a conference paper at ICLR 2022
CONCLUSION,0.5195822454308094,CODE OF ETHICS
CONCLUSION,0.5221932114882507,"We acknowledge that all the authors of this work have read and commit to adhering to the ICLR
Code of Ethics."
REPRODUCIBILITY STATEMENT,0.5248041775456919,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.5274151436031331,"We share the code used in this paper and make it publicly available on Github1. The following ap-
pendix also supplements the main text by providing additional clariÔ¨Åcations. In particular, Appendix
A provides further details on the considered hypersolver models. We provide additional information
on optimal control policy in Appendix B while in Appendix C we provide details on on the system
dynamics, architectures and other experimental details. Additional explanations are also provided
as comments in the shared code implementation."
REFERENCES,0.5300261096605744,REFERENCES
REFERENCES,0.5326370757180157,"Martin Aln√¶s, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris
Richardson, Johannes Ring, Marie E Rognes, and Garth N Wells. The fenics project version 1.5.
Archive of Numerical Software, 3(100), 2015."
REFERENCES,0.5352480417754569,"Mato Baoti¬¥c, Francesco Borrelli, Alberto Bemporad, and Manfred Morari. EfÔ¨Åcient on-line compu-
tation of constrained optimal control. SIAM Journal on Control and Optimization, 47(5):2470‚Äì
2489, 2008."
REFERENCES,0.5378590078328982,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016."
REFERENCES,0.5404699738903395,"John
Butcher.
Numerical
methods
for
differential
equations
and
applications.
http://www.math.auckland.ac.nz/Research/Reports/view.php?id=370, 22, 12 1997."
REFERENCES,0.5430809399477807,"Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary dif-
ferential equations, 2019."
REFERENCES,0.5456919060052219,"S Reynold Chu, Rahmat Shoureshi, and Manoel Tenorio. Neural networks for system identiÔ¨Åcation.
IEEE Control systems magazine, 10(3):31‚Äì35, 1990."
REFERENCES,0.5483028720626631,"J. R. Dormand and P. J. Prince. A family of embedded runge-kutta formulae. Journal of Computa-
tional and Applied Mathematics, 6:19‚Äì26, 1980."
REFERENCES,0.5509138381201044,"Jaime F. Fisac, Anayo K. Akametalu, Melanie N. Zeilinger, Shahab Kaynama, Jeremy Gillula, and
Claire J. Tomlin. A general safety framework for learning-based control in uncertain robotic
systems, 2018."
REFERENCES,0.5535248041775457,RÀòazvan Florian. Correct equations for the dynamics of the cart-pole system. 08 2005.
REFERENCES,0.556135770234987,"C. E. Garcia, D. M. Prett, and M. Morari. Model predictive control: Theory and practice - a survey.
Autom., 25:335‚Äì348, 1989."
REFERENCES,0.5587467362924282,"Dirk Gorissen, Ivo Couckuyt, Piet Demeester, Tom Dhaene, and Karel Crombecq. A surrogate
modeling and adaptive sampling toolbox for computer based design. The Journal of Machine
Learning Research, 11:2051‚Äì2055, 2010."
REFERENCES,0.5613577023498695,"Radek Grzeszczuk, Demetri Terzopoulos, and Geoffrey Hinton. Neuroanimator: Fast neural net-
work emulation and control of physics-based models. In Proceedings of the 25th annual confer-
ence on Computer graphics and interactive techniques, pp. 9‚Äì20, 1998."
REFERENCES,0.5639686684073107,"K.J. Hunt, D. Sbarbaro, R. ÀôZbikowski, and P.J. Gawthrop.
Neural networks for control sys-
tems‚Äîa survey. Automatica, 28(6):1083‚Äì1112, 1992. ISSN 0005-1098. doi: https://doi.org/
10.1016/0005-1098(92)90053-I. URL https://www.sciencedirect.com/science/
article/pii/000510989290053I."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.566579634464752,"1Supporting reproducibility code is at
https : //github.com/DiffEqML/diffeqml ‚àíresearch/tree/master/hypersolvers ‚àícontrol"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5691906005221932,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5718015665796344,"Doug L James and Kayvon Fatahalian. Precomputing interactive dynamic deformable scenes. ACM
Transactions on Graphics (TOG), 22(3):879‚Äì887, 2003."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5744125326370757,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.577023498694517,"Frank L Lewis, Draguna Vrabie, and Vassilis L Syrmos. Optimal control. John Wiley & Sons, 2012."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5796344647519582,"Chin-Teng Lin, C. S. George Lee, et al. Neural-network-based fuzzy logic control and decision
system. IEEE Transactions on computers, 40(12):1320‚Äì1336, 1991."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5822454308093995,"Yinghua Lin and George A Cunningham. A new approach to fuzzy-neural system modeling. IEEE
Transactions on Fuzzy systems, 3(2):190‚Äì198, 1995."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5848563968668408,"Alessandro Macchelli and Claudio Melchiorri. Modeling and control of the timoshenko beam. the
distributed port hamiltonian approach. SIAM J. Control. Optim., 43:743‚Äì767, 2004."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.587467362924282,"Stefano Massaroli, Michael Poli, Sho Sonoda, Taji Suzuki, Jinkyoo Park, Atsushi Yamashita, and
Hajime Asama. Differentiable multiple shooting layers. CoRR, abs/2106.03885, 2021. URL
https://arxiv.org/abs/2106.03885."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5900783289817232,"David Q Mayne and Hannah Michalska. Receding horizon control of nonlinear systems. In Pro-
ceedings of the 27th IEEE Conference on Decision and Control, pp. 464‚Äì465. IEEE, 1988."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5926892950391645,"Jacopo Panerati, Hehui Zheng, Siqi Zhou, James Xu, Amanda Prorok, and Angela P. Schoellig.
Learning to Ô¨Çy - a gym environment with pybullet physics for reinforcement learning of multi-
agent quadcopter control. CoRR, abs/2103.02142, 2021. URL https://arxiv.org/abs/
2103.02142."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.5953002610966057,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. arXiv preprint arXiv:1912.01703, 2019."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.597911227154047,"Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, and Jinkyoo Park. Hyper-
solvers: Toward fast continuous-depth models. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
21105‚Äì21117. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.
cc/paper/2020/file/f1686b4badcf28d33ed632036c7ab0b8-Paper.pdf."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6005221932114883,"Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, and Jinkyoo Park. Torchdyn:
A neural differential equations library, 2020b."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6031331592689295,"Radoslaw Pytlak. Numerical methods for optimal control problems with state constraints. Springer,
2006."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6057441253263708,"Anil V Rao. A survey of numerical methods for optimal control. Advances in the Astronautical
Sciences, 135(1):497‚Äì528, 2009."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.608355091383812,"Christian Robert and George Casella. Monte Carlo statistical methods. Springer Science & Business
Media, 2013."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6109660574412533,"I Michael Ross and Fariba Fahroo. Issues in the real-time computation of optimal control. Mathe-
matical and computer modelling, 43(9-10):1172‚Äì1188, 2006."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6135770234986945,"Matteo Saveriano, Yuchao Yin, Pietro Falco, and Dongheui Lee. Data-efÔ¨Åcient control policy search
using residual dynamics learning. In 2017 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pp. 4709‚Äì4715, 2017. doi: 10.1109/IROS.2017.8206343."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6161879895561357,"Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wet-
zstein. Implicit neural representations with periodic activation functions, 2020."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.618798955613577,"Andrew Taylor, Andrew Singletary, Yisong Yue, and Aaron Ames.
Learning for safety-critical
control with control barrier functions, 2019."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6214099216710183,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6240208877284595,"Ch. Tsitouras. Runge‚Äìkutta pairs of order 5(4) satisfying only the Ô¨Årst column simplifying assump-
tion. Computers Mathematics with Applications, 62(2):770‚Äì775, 2011. ISSN 0898-1221. doi:
https://doi.org/10.1016/j.camwa.2011.06.002. URL https://www.sciencedirect.com/
science/article/pii/S0898122111004706."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6266318537859008,"S Vadali, Hanspeter Schaub, and K Alfriend. Initial conditions and fuel-optimal control for forma-
tion Ô¨Çying of satellites. In Guidance, Navigation, and Control Conference and Exhibit, pp. 4265,
1999."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6292428198433421,"Yue J Zhang, Andreas A Malikopoulos, and Christos G Cassandras. Optimal control and coordina-
tion of connected and automated vehicles at urban trafÔ¨Åc intersections. In 2016 American Control
Conference (ACC), pp. 6227‚Äì6232. IEEE, 2016."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6318537859007833,"Liu Ziyin, Tilman Hartwig, and Masahito Ueda. Neural networks fail to learn periodic functions
and how to Ô¨Åx it, 2020."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6344647519582245,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6370757180156658,"A
ADDITIONAL HYPERSOLVER MATERIAL"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.639686684073107,"A.1
EXPLICIT HYPEREULER FORMULATION"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6422976501305483,"Our analysis in the experiments takes into consideration the hypersolved version of the Euler
scheme, namely HyperEuler.
Since Euler is a Ô¨Årst‚Äìorder method, it requires the least number
of function evaluations (NFE) of the vector Ô¨Åeld f in (1) and yields a second order local trun-
cation error ek :=
œµ2Rk

2.
This error is larger than other Ô¨Åxed‚Äìstep solvers and thus has
the most room for potential improvements. The base solver scheme œàœµ of (4) can be written as
œàœµ (tk, xk, uk) = f (tk, xk, uk), which is approximating the next state by adding an evaluation of
the vector Ô¨Åeld multiplied by the step size œµ. We can write the HyperEuler update explicitly as
xk+1 = xk + œµf(tk, xk, uk) + œµ2gw (tk, xk, uk)
(12)
while we write its residual as"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6449086161879896,"R (x(tk), u(tk))) = 1"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6475195822454308,"œµ2 (Œ¶(x(tk), tk, tk+1) ‚àíx(tk) ‚àíœµf(tk, xk, uk))
(13)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6501305483028721,"A.2
HYPERSOLVERS FOR TIME‚ÄìINVARIANT SYSTEMS"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6527415143603134,"A time‚Äìinvariant system with time‚Äìinvariant controller can be described as following
Àôx(t) = f(x(t), u(x(t)))
x(0) = x0
(14)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6553524804177546,"in which f and u do not explicitly depend on time. The models considered in the experiments satisfy
this property."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6579634464751958,"B
CONTROL POLICY DETAILS"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.660574412532637,"B.1
OPTIMAL CONTROL COST FUNCTION"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6631853785900783,The general form of the integral cost functional can be written as follows
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6657963446475196,"J(x(t), u(t)) = [x‚ä§(tf) ‚àíx‚ãÜ]P[x(tf) ‚àíx‚ãÜ] +
Z tf t0"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6684073107049608," 
[x‚ä§(t)x‚ãÜ]Q[x(t) ‚àíx‚ãÜ] + u‚ä§(t)Ru(t)

dt"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6710182767624021,"(15)
where matrix P is a penalty on deviations from the target x‚ãÜof the last states, Q penalizes all
deviations from the target of intermediate states while R is a regulator for the control inputs. Eval-
uation of (15) usually requires numerical solvers such as the proposed hypersolvers of this work.
Discretizations of the cost functional are also called cost function in the literature."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6736292428198434,"B.2
MODEL PREDICTIVE CONTROL FORMULATION"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6762402088772846,The following problem is solved online and iteratively until the end of the time span
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6788511749347258,"min
uk"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6814621409921671,"T ‚àí1
X"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6840731070496083,"k=0
J (xk, uk)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6866840731070496,"subject to
Àôx(t) = f(t, x(t), u(t))
x(0) = x0
t ‚ààT (16)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6892950391644909,"where J is a cost function and T ‚ààT is the receding horizon over which predicted future trajectories
are optimized."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6919060052219321,"B.3
NEURAL OPTIMAL CONTROL"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6945169712793734,"We parametrize the control policy of problem (2) as uŒ∏ : t, x 7‚ÜíuŒ∏(t, x) where Œ∏ is a Ô¨Ånite set of
free parameters. SpeciÔ¨Åcally, we consider the case of neural optimal control in which controller uŒ∏
is a multi‚Äìlayer perceptron. The optimal control task is to minimize the cost function J described in
(15) and we do so by optimizing the parameters Œ∏ via SGD; in particular, we use the Adam (Kingma
& Ba, 2017) optimizer for all the experiments."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6971279373368147,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.6997389033942559,"C
EXPERIMENTAL DETAILS"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7023498694516971,"In this section we include additional modeling and experimental details divided into the different
dynamical systems."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7049608355091384,"C.1
HYPERSOLVER NETWORK ARCHITECTURE"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7075718015665796,"We design the hypersolver networks gw as feed‚Äìforward neural networks. Table 1 summarizes the
parameters used for the considered controlled systems, where Activation denotes the activation func-
tions, i.e. SoftPlus: x 7‚Üílog(1 + ex), Tanh: x 7‚Üíex‚àíe‚àíx"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7101827676240209,ex+e‚àíx and Snake : x ‚Üíx + 1
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7127937336814621,"a sin2(ax)
(Ziyin et al., 2020). We also use the vector Ô¨Åeld f as an input of the hypersolver, which does not"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7154046997389034,Table 1: Hyper‚Äìparameters for the hypersolver networks in the experiments.
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7180156657963447,"Spring‚ÄìMass
Inverted
Pendulum 2
Cart‚ÄìPole3
Quadcopter
Timoshenko
Beam
Input Layer
5
5
9
28
322
Hidden Layer 1
32
32
32
64
256
Activation 1
Softplus
Softplus
Snake
Softplus
Snake
Hidden Layer 2
32
32
32
64
256
Activation 2
Tanh
Tanh
Snake
Softplus
Snake
Output Layer
2
2
4
12
160"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.720626631853786,"require a further evaluation since it is pre‚Äìevaluated at runtime by the base solver œà. We empha-
size that the size of the network should depend on the application: a too‚Äìlarge neural network may
require more computations than just increasing the numerical solver‚Äôs order: Pareto optimality of hy-
persolvers also depends on their complexity. Keeping their neural network small enough guarantees
that evaluating the hypersolvers is cheaper than resorting to more complex numerical routines."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7232375979112271,"C.2
SPRING-MASS SYSTEM"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7258485639686684,"System Dynamics
The spring-mass system considered is described in the Hamiltonian formula-
tion by

Àôq
Àôp"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7284595300261096,"
=

0
1/m
‚àík
0"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7310704960835509," 
q
p"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7336814621409922,"
+

0
1"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7362924281984334,"
u
(17)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7389033942558747,where m = 1 [Kg] and k = 0.5 [N/m].
SUPPORTING REPRODUCIBILITY CODE IS AT,0.741514360313316,"Pre-training methods comparison
We select Œæ(x, u) as a uniform distribution with support in
X √ó U where X = [‚àí20, 20] √ó [‚àí20, 20] while U = [‚àí100, 100]. Nominal solutions are calculated
on the accurate system using dopri5 with relative and absolute tolerances set to 10‚àí7 and 10‚àí9
respectively. We train two separate HyperEuler models with different training methods on local
residual for step size œµ = 0.03 s: stochastic exploration and active error minimization. The optimizer
used is Adam with learning rate of 10‚àí3 for 104 epochs."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7441253263707572,"Hypersolvers with different step sizes
We select Œæ(x, u) as a uniform distribution with support
in X √ó U where X = [‚àí5, 5] √ó [‚àí5, 5] while U = [‚àí20, 20]. Nominal solutions are calculated
on the accurate system using dopri5 with relative and absolute tolerances set to 10‚àí7 and 10‚àí9
respectively. We train separate HyperEuler models with stochastic exploration with different step
sizes œµ. The optimizer used is Adam with learning rate of 10‚àí3 for 104 epochs."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7467362924281984,"2This architecture refers to the optimal control experiment. Details on hypersolver models for the general-
ization experiment on the inverted pendulum are available in Appendix C.3.
3In the multi‚Äìstage hypersolver experiment we consider both the inner stage hw and the outer stage gœâ with
the same architecture and jointly trained (more information and ablation study in Appendix C.4)."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7493472584856397,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7519582245430809,"‚àí20
0
20
u [N] 10‚àí2 10‚àí1 100 101"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7545691906005222,Mean Residual R Euler
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7571801566579635,Midpoint RK4
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7597911227154047,Tanh [HE]
SUPPORTING REPRODUCIBILITY CODE IS AT,0.762402088772846,ReLU [HE]
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7650130548302873,SIREN [HE]
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7676240208877284,Snake [HE]
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7702349869451697,Training Region
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7728459530026109,"10‚àí4
10‚àí3
10‚àí2
10‚àí1
100 œµ [s] 10‚àí3 10‚àí2 10‚àí1 100 101"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7754569190600522,Mean Residual R Euler
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7780678851174935,Midpoint RK4
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7806788511749347,Tanh [HE]
SUPPORTING REPRODUCIBILITY CODE IS AT,0.783289817232376,ReLU [HE]
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7859007832898173,SIREN [HE]
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7885117493472585,Snake [HE]
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7911227154046997,Training Step
SUPPORTING REPRODUCIBILITY CODE IS AT,0.793733681462141,"Figure 9: Generalization with different hypersolver activation functions (HyperEuler models are marked with
‚ÄùHE‚Äù) on the inverted pendulum. [Left] Generalization for the controller space outside of the training region
(red area). The architecture with Snake can to generalize better compared to other hypersolvers. [Right]
Generalization for different time steps outside of the training step œµ = 0.1 s (red line). HyperEuler is able to
improve the baseline Euler solver performance even for unseen œµ."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7963446475195822,"C.3
INVERTED PENDULUM"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.7989556135770235,"System Dynamics
We model the inverted pendulum with elastic joint with Hamiltonian dynamics
via the following:

Àôq
Àôp"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8015665796344648,"
=

0
1/m
‚àík
‚àíŒ≤/m"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.804177545691906," 
q
p"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8067885117493473,"
‚àí

0
mgl sin q"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8093994778067886,"
+

0
1"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8120104438642297,"
u
(18)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.814621409921671,"where m = 1 [Kg], k = 0.5 [N/ rad], r = 1 [m], Œ≤ = 0.01 [Ns/ rad], g = 9.81 [m/s2]."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8172323759791122,"Pre‚Äìtraining for the generalization study
We perform sampling via stochastic exploration from
the uniform distribution Œæ(x, u) with support in X √ó U with X = [‚àí2œÄ, 2œÄ] √ó [‚àí2œÄ, 2œÄ] and
U = [‚àí10, 10] for the different architectures. We choose as a common time step œµ = 0.1 s; the
networks are trained for 100000 epochs with the Adam optimizer and learning rate of 10‚àí3. The
network architectures share the same parameters as the inverted pendulum ones in 1, while the
activation functions are substituted by the ones in Figure 4. The SIREN architecture is chosen with
2 hidden layers of size 64. Figure 9 provides an additional empirical results on generalization
properties across controller values and step sizes: we notice how Snake can generalize to unseen
control values better compared to other hypersolvers."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8198433420365535,"Additional visualization
Figure 10 provides an additional visualization of the inverted pendulum
controlled trajectories from Figure 5 with positions q and momenta p over time. ‚àí4 0 4 q(t)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8224543080939948,"HyperEuler
Euler
Midpoint
RK4"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.825065274151436,"0
1
2
3
4
5
t ‚àí5 0 5 p(t)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8276762402088773,"0
1
2
3
4
5
t"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8302872062663186,"0
1
2
3
4
5
t"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8328981723237598,"0
1
2
3
4
5
t
Figure 10: Controlled trajectories of the inverted pendulum with controllers optimized via different solvers."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.835509138381201,"C.4
CART-POLE"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8381201044386423,"System Dynamics
We consider a continuous version of a cart‚Äìpole system additionally taking into
account the full dynamic model in Florian (2005). This formulation considers the friction coefÔ¨Åcient"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8407310704960835,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8433420365535248,"between the track and the cart ¬µc inducing a force opposing the linear motion as well as the friction
generated between the cart and the pole ¬µp, whose generated torque opposes the angular motion.
The full cart‚Äìpole model is described by the four variables x, Àôx, Œ∏, ÀôŒ∏ and the accelerations update is
as following"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8459530026109661,"Nc = (mc + mp) g ‚àímpl

¬®Œ∏ sin Œ∏ + ÀôŒ∏2 cos Œ∏
"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8485639686684073,"¬®Œ∏ =
g sin Œ∏ + cos Œ∏
h
‚àíu‚àímpl ÀôŒ∏2(sin Œ∏+¬µc sgn(Nc Àôx) cos Œ∏)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8511749347258486,"mc+mp
+ ¬µcg sgn(Nc Àôx)
i
‚àí¬µp ÀôŒ∏ mpl"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8537859007832899,"l
h
4
3 ‚àímp cos Œ∏"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.856396866840731,"mc+mp (cos Œ∏ ‚àí¬µc sgn(Nc Àôx))
i"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8590078328981723,"¬®x =
u + mpl

ÀôŒ∏2 sin Œ∏ ‚àí¬®Œ∏ cos Œ∏

‚àí¬µcNc"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8616187989556136,mc + mp (19)
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8642297650130548,"where mc = 1 [Kg], mp = 0.1 [Kg], l = 0.5 [m] and g = 9.81 [m/s2]. Nc represents the
normal force acting on the cart. For simulation purposes, we consider its sign to be always positive
when evaluating the sign (sgn) function as the cart should normally not jump off the track. Setting
¬µc, ¬µp to 0 results in the same dynamic model used in the OpenAI Gym (Brockman et al., 2016)
implementation."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8668407310704961,"MS joint training
MS separated training
MS no inner stage
MS no outer stage
Residual dynamics
Euler (nominal)
Midpoint (nominal) Midpoint (inaccurate) 10‚àí3 10‚àí2 10‚àí1"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8694516971279374,One Step MAE
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8720626631853786,"Figure 11: One step Mean Absolute Error (MAE) for multi‚Äìstage hypersolvers and different solvers as well
as correction schemes in the ablation study. Multi‚Äìstage Hypersolver (MS) with joint training and Midpoint
base solver iterating on an inaccurate vector Ô¨Åeld agnostic of friction forces outperforms the Midpoint solver
with full knowledge of the vector Ô¨Åeld."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8746736292428199,"Multistage training strategies
We study two different training strategies for the inner and outer
networks hw and gœâ in (11). We Ô¨Årst consider a joint training strategy in which both stages are
trained at the same time via stochastic exploration. Secondly, we do a separated training in which
only the inner stage network hw is trained Ô¨Årst and then the outer stage network gœâ is added and
only its parameters are trained in a Ô¨Ånetuning process. We Ô¨Ånd that, as shown in Figure 11, joint
training yields slightly better results. A further advantage of jointly training both stages is that only
a single training procedure is required."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8772845953002611,"Ablation study
We consider the same model as our Multi‚Äìstage Hypersolver with base Midpoint
solver but no Ô¨Årst‚Äìstage hw, which corresponds to learning the residual dynamics only, and we train
this model with stochastic exploration. We show in Figure 11 that while the residual dynamics model
can improve the one‚Äìstep error compared to the base solver on the inaccurate dynamics, it performs
worse the Multi‚Äìstage Hypersolver scheme. We additionally study the contribution of each stage in
the prediction error improvements by separately zeroing out the contributions of the inner and outer
stage. While iterating over the inner stage only improves on the base‚Äìsolver error, including the
outer stage further contributes in improving the error. We notice how the excluding the inner‚Äìstage
yields higher errors: this may be due to the fact that the inner‚Äìstage specializes in correcting the
Ô¨Årst‚Äìorder vector Ô¨Åeld inaccuracies while the outer‚Äìstage corrects the one step base solver residual."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8798955613577023,"Additional experimental details
For the Multi‚Äìstage Hypersolver control experiment, we pre‚Äì
train both inner and outer stage networks hw and gœâ in (11) at the same time using stochastic
exploration. The base solver is chosen as the second‚Äìorder Midpoint iterating on the partial dy-
namics (19) with ¬µc, ¬µp set to 0. The nominal dynamics considers non‚Äìnull friction forces: we set
the cart friction coefÔ¨Åcient to ¬µc = 0.1 and the one of the pole to ¬µp = 0.03. We note how the
friction coefÔ¨Åcients make the vector Ô¨Åeld (19) non‚Äìsmooth: simulation through adaptive‚Äìstep size"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8825065274151436,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8851174934725848,"solvers as tsit5 results experimentally time‚Äìconsuming, hence we resort to RK4 for training the
hypersolver networks. Nonetheless, as shown in the error propagation of Figure 12, this does not
degrade the performance of the trained multi‚Äìstage hypersolver scheme. All neural networks in the
experiements, including the ablation study, are trained with the Adam optimizer for 5 √ó 104 epochs,
where we set the learning rate to 10‚àí2 for the Ô¨Årst 3 √ó 104 epochs, then decrease it to 10‚àí3 for 104
epochs and to 10‚àí4 for the last 104."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8877284595300261,"C.5
QUADCOPTER"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8903394255874674,"System Dynamics
The quadcopter model is a suitably modiÔ¨Åed version of the explicit dynamics
update in (Panerati et al., 2021) for batched training in PyTorch. The following accelerations update
describes the dynamic model"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8929503916449086,"¬®x =

R ¬∑ [0, 0, kF
P3
i=0œâ2
i ] ‚àí[0, 0, mg]

m‚àí1"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8955613577023499,"¬®
œà = J‚àí1 
œÑ(l, kF , kT , [œâ2
0, œâ2
1, œâ2
2, œâ2
3]) ‚àíÀôœà √ó

J Àôœà

(20)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.8981723237597912,"mwhere x = [x, y, z] corresponds to the drone positions and œà = [œÜ, Œ∏, œà] to its angular positions;
R and J are its rotation and inertial matrices respectively, œÑ(¬∑) is a function calculating the torques
induced by the motor speeds œâi, while arm length l, mass m, gravity acceleration constant g along
with kF and kT are scalar variables describing the quadcopter‚Äôs physical properties."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9007832898172323,"0
1
Time [s] 10‚àí4 10‚àí3 10‚àí2 10‚àí1 SMAPE"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9033942558746736,Position
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9060052219321149,"0
1
Time [s] 10‚àí2 10‚àí1 SMAPE"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9086161879895561,Velocity
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9112271540469974,"0
1
Time [s] 10‚àí3 10‚àí2 SMAPE"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9138381201044387,Angular Position
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9164490861618799,"0
1
Time [s] 10‚àí2 10‚àí1 SMAPE"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9190600522193212,Angular Velocity
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9216710182767625,"Multistage Hypersolver (inaccurate model)
Midpoint (inaccurate model)
Euler (nominal model)
Midpoint (nominal model)
Figure 12: Symmetric Mean Absolute Percentage Error (SMAPE) propagation along controlled trajectories
of the cart‚Äìpole system. The Multi‚Äìstage Hypersolver with knowledge limited to the inaccurate model manages
to outperform the Euler solver iterating on the accurate dynamics in terms of positions and angular positions."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9242819843342036,"C.6
TIMOSHENKO BEAM"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9268929503916449,"System Dynamics
We consider as a system from the theory of continuum dynamics the Tim-
oshenko beam with no dissipation described in (Macchelli & Melchiorri, 2004; Massaroli et al.,
2021). The system can be described in the coenergy formulation by the following partial differen-
tial equation (PDE)
Ô£Æ Ô£ØÔ£∞"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9295039164490861,"œÅA
0
0
0
0
IœÅ
0
0
0
0
Cb
0
0
0
0
Cs Ô£π Ô£∫Ô£ª‚àÇ ‚àÇt Ô£´ Ô£¨
Ô£≠"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9321148825065274,"vt
vr
œÉr
œÉt Ô£∂ Ô£∑
Ô£∏= Ô£Æ Ô£ØÔ£∞"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9347258485639687,"0
0
0
‚àÇx
0
0
‚àÇx
1
0
‚àÇx
0
0
‚àÇx
‚àí1
0
0 Ô£π Ô£∫Ô£ª Ô£´ Ô£¨
Ô£≠"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9373368146214099,"vt
vr
œÉr
œÉt Ô£∂"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9399477806788512,"Ô£∑
Ô£∏
(21)"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9425587467362925,"where œÅ is the mass density, A is the cross section area, IœÅ is the rotational inertia, Cs and Cb are the
shear and bending compliance; the discretized state variables vt, vr represent the translational and
rotational velocities respectively while œÉt, œÉr denote the translational and rotational displacements.
cantilever beam. In order to discretize the problem, we implement a software routine based on the
fenics (Aln√¶s et al., 2015) open‚Äìsource software suite to obtain the Ô¨Ånite‚Äìelements discretization
of the Timoshenko PDE of (21) given the number of elements, physical parameters of the model and
initial conditions of the beam. We choose a 40 elements discretization of the PDE for a total of 160
dimensions of the discretized state z = [vt, vr, œÉt, œÉr]‚ä§and we initialize the beam at time t = 0
as z(x, 0) = [sin(œÄx), sin(3œÄx), 0, 0]. The system can thus be reduced to the following controlled"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9451697127937336,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9477806788511749,linear system Ô£Æ Ô£ØÔ£∞
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9503916449086162,"Àôvt
Àôvr
ÀôœÉt
ÀôœÉr Ô£π Ô£∫Ô£ª= Ô£Æ Ô£ØÔ£ØÔ£∞"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9530026109660574,"√ó
√ó
√ó
‚àíM ‚àí1
œÅA D‚ä§
1
√ó
√ó
‚àíM ‚àí1
IœÅ D‚ä§
2
‚àíM ‚àí1
IœÅ D‚ä§
0
√ó
M ‚àí1
Cb D2
√ó
√ó
M ‚àí1
Cs D1
M ‚àí1
Cs D0
√ó
√ó Ô£π Ô£∫Ô£∫Ô£ª Ô£Æ Ô£ØÔ£∞"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9556135770234987,"vt
vr
œÉt
œÉr Ô£π Ô£∫Ô£ª+ Ô£Æ Ô£ØÔ£ØÔ£∞"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.95822454308094,"√ó
M ‚àí1
œÅA BF
M ‚àí1
IœÅ BT
√ó
√ó
√ó
√ó
√ó Ô£π Ô£∫Ô£∫Ô£ª"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9608355091383812,"
u1
‚àÇ
u2
‚àÇ "
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9634464751958225,"(22)
where the mass matrices MœÅA, MIœÅ, MCb, MCs, matrices D0, D1, D2, vectors BF , BT are
computed through the fenics routine and boundary controllers u1
‚àÇand u2
‚àÇare the control torque
and the control force applied at the free end of the beam."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9660574412532638,"Stochastic exploration strategy via random walks
We pre‚Äìtrain the hypersolver model via
stochastic exploration of the state-controller space X √ó U. We restrict the boundary control input
values u = [u1
‚àÇ, u2
‚àÇ]‚ä§in [‚àí1, 1] √ó [‚àí1, 1]. As for the state space, naively generating a probability
distribution with box boundaries on each of the 160 dimensions of X would require an inefÔ¨Åcient
search over this high-dimensional space: in fact, not every combination is physically feasible due to
the Timoshenko beam‚Äôs structure. We solve this problem by propagating batched trajectories with
RK4 from the initial boundary condition z(x, 0) = [sin(œÄx), sin(3œÄx), 0, 0] with random control
actions sampled from a uniform distribution with support in [‚àí1, 1] √ó [‚àí1, 1] applied for a time
t1 ‚àºU[0.002, 1] s. We save the states {z(x, t1)i} and forward propagate from these states again
by sampling from the controller and time distributions. We repeat the process K times and obtain a
sequence [{z(x, t1)i}, . . . , {z(x, tK)i}] of batched initial conditions characterized by physical fea-
sibility. Finally, we train the hypersolver with stochastic exploration by sampling from the generated
distribution Œæ(x, u) on local one‚Äìstep residuals as described in Section 5.4. This initial state gener-
ation strategy is repeated every 100 epochs for guaranteeing an extensive exploration of all possible
boundary conditions. Figure 13 shows the error propagation over controlled trajectories: the trained
HyperEuler achieves the lowest error among baseline Ô¨Åxed‚Äìstep solvers."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9686684073107049,"0
1
2
3
t [s] 10‚àí4 10‚àí1 102 105 108 MAE vt"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9712793733681462,"Euler
HyperEuler
Midpoint
RK4"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9738903394255874,"0
1
2
3
t [s] vr"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9765013054830287,"0
1
2
3
t [s] œÉt"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.97911227154047,"0
1
2
3
t [s] œÉr"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9817232375979112,"Figure 13: Mean Absolute Error (MAE) propagation on velocities vt, vr and displacements œÉt, œÉr for the
Ô¨Ånite elements of the discretized Timoshenko beam along controlled trajectories. While solutions from Euler
and Midpoint quickly diverge due to the system‚Äôs stiffness, HyperEuler manages not only to contain errors but
even outperform the fourth-order RK4 whilst requiring a fraction of the number of vector Ô¨Åeld evaluations."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9843342036553525,"Additional details on the results
We report additional details regarding the runtime of the exper-
iments on the Timoshenko Beam. As for the training time, training the hypersolver for 105 epochs
takes around 80 minutes, where the time for each training epoch slightly varies depending on the
length of the sequence of initial condition batches obtained via random walks. As for the averaged
runtime per training epoch during the control policy optimization, HyperEuler takes (2.53 ¬± 0.09) s
per training iteration, Euler (2.01 ¬± 0.04) s, Midpoint (4.02 ¬± 0.08) s and RK4 (8.24 ¬± 0.14) s.
Experiments were run on the CPU of the machine described in Section C.7."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9869451697127938,"C.7
HARDWARE AND SOFTWARE"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9895561357702349,"Experiments were carried out on a machine equipped with an AMD RYZEN THREADRIPPER
3960X CPU with 48 threads and two NVIDIA RTX 3090 graphic cards. Software‚Äìwise, we
used PyTorch (Paszke et al., 2019) for deep learning and the torchdyn (Poli et al., 2020b) and"
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9921671018276762,Published as a conference paper at ICLR 2022
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9947780678851175,"torchdiffeq (Chen et al., 2019) libraries for ODE solvers. We additionally share the code used in
this paper and make it publicly available on Github4."
SUPPORTING REPRODUCIBILITY CODE IS AT,0.9973890339425587,"4Supporting reproducibility code is at
https : //github.com/DiffEqML/diffeqml ‚àíresearch/tree/master/hypersolvers ‚àícontrol"
