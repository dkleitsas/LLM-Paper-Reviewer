Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033333333333333335,"Graph neural networks have recently attracted a lot of attention and have been ap-
plied with great success to several important graph problems. The Random Walk
Graph Neural Network model was recently proposed as a more intuitive alterna-
tive to the well-studied family of message passing neural networks. This model
compares each input graph against a set of latent “hidden graphs” using a kernel
that counts common random walks up to some length. In this paper, we propose
a new architecture, called Geometric Random Walk Graph Neural Network (GR-
WNN), that generalizes the above model such that it can count common walks of
inﬁnite length in two graphs. The proposed model retains the transparency of Ran-
dom Walk Graph Neural Networks since its ﬁrst layer also consists of a number of
trainable “hidden graphs” which are compared against the input graphs using the
geometric random walk kernel. To compute the kernel, we employ a ﬁxed-point
iteration approach involving implicitly deﬁned operations. Then, we capitalize on
implicit differentiation to derive an efﬁcient training scheme which requires only
constant memory, regardless of the number of ﬁxed-point iterations. The em-
ployed random walk kernel is differentiable, and therefore, the proposed model
is end-to-end trainable. Experiments on standard graph classiﬁcation datasets
demonstrate the effectiveness of the proposed approach in comparison with state-
of-the-art methods."
INTRODUCTION,0.006666666666666667,"1
INTRODUCTION"
INTRODUCTION,0.01,"Recent years have witnessed an enormous growth in the amount of data represented as graphs. In-
deed, graphs emerge naturally in several domains, including social networks, bioinformatics, and
neuroscience, just to name a few. Besides the increase in the amount of graph-structured data,
there is also a growing interest in applying machine learning techniques to data modeled as graphs.
Among others, the graph classiﬁcation and graph regression tasks have attracted a great deal of
attention in the past years. These tasks have served as the fundamental building block within ap-
plications that deal with problems ranging from drug design Kearnes et al. (2016) to session-based
recommendation Wu et al. (2019)."
INTRODUCTION,0.013333333333333334,"Graph Neural Networks (GNNs) provide a powerful tool for machine learning on graphs, So far,
the ﬁeld of GNNs has been largely dominated by message passing architectures. Indeed, most of
them share the same basic idea, and can be reformulated into a single common framework, so-
called message passing neural networks (MPNNs) Gilmer et al. (2017). These models employ
a message passing procedure to aggregate local information of vertices. For graph-related tasks,
MPNNs usually apply some permutation invariant readout function to the vertex representations to
produce a representation for the entire graph. The family of MPNNs has been heavily studied in the
past few years, and there are now available very expressive models which have achieved state-of-
the-art results in several tasks Xu et al. (2019); Morris et al. (2019). Although the family of MPNNs
is perhaps the most successful story in the ﬁeld of graph representation learning, there exist models
that follow different design paradigms and do not fall into this family. An example of such a model
is the recently proposed Random Walk Graph Neural Network (RWNN) Nikolentzos & Vazirgiannis
(2020). This model contains a number of trainable “hidden graphs”, and it compares the input graphs
against these graphs using a random walk kernel which counts the number of common walks in two
graphs. The emerging kernel values are fed into a fully-connected neural network which acts as the
classiﬁer or regressor. The employed random walk kernel is differentiable, and thus RWNN is end-"
INTRODUCTION,0.016666666666666666,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02,"to-end trainable. However, this kernel considers only random walks of a small length. Such local
patterns may fail to capture the overall large-scale shape of the graphs, while several interesting
properties of graphs depend on the graph’s global structure. Furthermore, increasing the length of
the walks has a direct impact on the model’s computational complexity."
INTRODUCTION,0.023333333333333334,"In this paper, we propose a novel approach to tackle these challenges. Speciﬁcally, we propose a
new architecture, called Geometric Random Walk Graph Neural Network (GRWNN), that general-
izes the RWNN model such that it can count common walks of inﬁnite length in two graphs. The
model contains a number of trainable “hidden graphs”, and it compares the input graphs against
these graphs using the geometric random walk kernel. Thus, instead of walks of small length, the
proposed model considers walks of inﬁnite length. To compute the kernel, GRWNN uses a ﬁxed-
point iteration approach. The kernel values are then passed on to a fully-connected neural network
which produces the output. The proposed neural network is end-to-end trainable since we can di-
rectly differentiate through the ﬁxed-point equations via implicit differentation, which leads to a very
efﬁcient implementation in terms of memory requirements. Hence, we can still update the “hidden
graphs” during training with backpropagation. We compare the performance of the proposed model
to state-of-the-art graph kernels and recently-proposed neural architectures on several graph clas-
siﬁcation datasets. Results show that in most cases, the GRWNN model matches or outperforms
competing methods. Our main contributions are summarized as follows:"
INTRODUCTION,0.02666666666666667,"• We propose a novel neural network model, Geometric Random Walk Graph Neural Net-
work, which employs the geometric random walk kernel to produce graph representations.
The model counts common walks of inﬁnite length in the input graph and a set of randomly
initialized “hidden graphs”.
• We employ an efﬁcient scheme to compute the random walk graph kernel using ﬁxed-point
iterations. We show that we can directly differentiate through the ﬁxed-point equations via
implicit differentation, which leads to an efﬁcient implementation.
• We evaluate the model’s performance on several standard graph classiﬁcation datasets and
show that it achieves results similar and in some cases superior to those obtained by recent
GNNs and graph kernels."
RELATED WORK,0.03,"2
RELATED WORK"
RELATED WORK,0.03333333333333333,"Graph kernels have a long history in the ﬁeld of graph representation learning Kriege et al. (2020).
A graph kernel is a kernel function between graphs, i. e., a symmetric positive semideﬁnite function
deﬁned on the space of graphs. These methods generate implicitly (or explicitly) graph representa-
tions and enable the application of kernel methods such as the SVM classiﬁer to graphs. Most graph
kernels are instances of the R-convolution framework Haussler (1999), and they compare substruc-
tures extracted from the graphs to each other. Such substructures include shortest paths Borgwardt
& Kriegel (2005), random walks G¨artner et al. (2003); Kashima et al. (2003), small subgraphs Sher-
vashidze et al. (2009), and others. Our work is related to random walk kernels, i. e., kernels that
compare random walks to each other. The ﬁrst such kernels were proposed by G¨artner et al. G¨artner
et al. (2003) and by Kashima et al. Kashima et al. (2003). The work of Kashima et al. was later
reﬁned by Mah´e et al. Mah´e et al. (2004). Vishwanathan et al. Vishwanathan et al. (2010) and Kang
et al. Kang et al. (2012) proposed new algorithms for efﬁciently computing random walk kernels.
These algorithms improve the time complexity of kernel computation. Sugiyama and Borgwardt
studied the problem of halting (i. e., longer walks are downweighted so much that the kernel value
is completely dominated by the comparison of walks of length 1) that occurs in random walk ker-
nels, and showed that its extent depends on properties of the graphs being compared Sugiyama &
Borgwardt (2015). Zhang et al. deﬁned a different kernel which does not compare random walks
to each other, but instead, compares the return probabilities of random walks Zhang et al. (2018b).
Finally, Kalofolias et al. proposed a variant of the random walk kernel where structurally dissimilar
vertices are not just down-weighed, but are not allowed to be visited during the simultaneous walk
Kalofolias et al. (2021)."
RELATED WORK,0.03666666666666667,"Although the ﬁrst GNNs were proposed several years ago Sperduti & Starita (1997); Scarselli et al.
(2009); Micheli (2009), until recently, these models had attracted limited attention. In recent years,
with the rise of deep learning, a lot of models started to emerge Bruna et al. (2014); Li et al. (2015);
Duvenaud et al. (2015); Atwood & Towsley (2016); Defferrard et al. (2016); Lei et al. (2017). Most"
RELATED WORK,0.04,Under review as a conference paper at ICLR 2022
RELATED WORK,0.043333333333333335,"models update the representation of each vertex by aggregating the feature vectors of its neighbors.
This update procedure can be viewed as a form of message passing algorithm and thus, these models
are known as message passing neural networks (MPNNs) Gilmer et al. (2017). To compute a feature
vector for the entire graph, MPNNs apply some permutation invariant readout function to all the ver-
tices of the graph. The family of MPNNs has been heavily studied in the past few years, and there
are now available several sophisticated models which can produce expressive graph representations
Xu et al. (2019); Morris et al. (2019); Dehmamy et al. (2019); Morris et al. (2020). Despite the gen-
eral recent focus on MPNNs, some works have proposed architectures that are not variants of this
family of models Niepert et al. (2016); Maron et al. (2019b;a); Nikolentzos & Vazirgiannis (2020).
The work closest to ours is the one reported in Nikolentzos & Vazirgiannis (2020) which presents
the Random Walk Graph Neural Network (RWNN) model. In fact, in this paper, we generalize the
RWNN model to compare random walks of inﬁnite length in two graphs. Recently, another method
that uses random walks to extract features which are then processed by a standard convolutional neu-
ral network was proposed Toenshoff et al. (2021). However, the proposed approach decouples data
representation from learning since random walks are sampled in a preprocessing stage. Our work
is also related to implicit models which have been applied successfully to many problems de Avila
Belbute-Peres et al. (2018); Chen et al. (2018); Amos et al. (2018); Bai et al. (2019). The outputs
of these models are determined implicitly by a solution of some underlying sub-problem. Implicit
models have also been deﬁned in the context of graph representation learning. For instance, Gu et
al. proposed IGNN, a model that seeks the ﬁxed-point of some equation which is equivalent to run-
ning an inﬁnite number of message passing iterations Gu et al. (2020). Thus, the ﬁnal representation
potentially contains information from all neighbors in the graph capturing long-range dependencies.
Gallicchio and Micheli proposed a similar model which generates graph representations based on
the ﬁxed point of a recursive/dynamical system, but is actually only partially trained Gallicchio &
Micheli (2020). In contrast to these approaches whose objective is to apply a large (or inﬁnite)
number of message passing layers implicitly, in our setting, we employ a ﬁxed-point iteration ap-
proach to compute the random walk kernel and then we directly differentiate through the ﬁxed point
equations via implicit differentation."
PRELIMINARIES,0.04666666666666667,"3
PRELIMINARIES"
NOTATION,0.05,"3.1
NOTATION"
NOTATION,0.05333333333333334,"Let [n] = {1, . . . , n} ⊂N for n ≥1. Let G = (V, E) be an undirected graph, where V is the
vertex set and E is the edge set. We will denote by n the number of vertices and by m the number
of edges. The adjacency matrix A ∈Rn×n of a graph G is a symmetric (typically sparse) matrix
used to encode edge information in the graph. The element of the ith row and jth column is equal
to the weight of the edge between vertices vi and vj if such an edge exists, and 0 otherwise. The
degree d(v) of a vertex v is equal to the sum of the weights of the edges that are adjacent to the
vertex. For vertex-attributed graphs, every vertex in the graph is associated with a feature vector.
We use X ∈Rn×d to denote the vertex features where d is the feature dimensionality. The feature
of a given vertex vi corresponds to the ith row of X."
NOTATION,0.056666666666666664,"The direct (tensor) product G× = (V×, E×) of two graphs G = (V, E) and G′ = (V ′, E′) is deﬁned
as follows:
V× = {(v, v′) ∈V × V ′}"
NOTATION,0.06,"E× =
 
(v, v′), (u, u′)

∈V× × V× | (v, u) ∈E, and (v′, u′) ∈E′"
NOTATION,0.06333333333333334,"We denote by A× the adjacency matrix of G×, and denote by ∆× and ¯d× the maximum and average
of the vertex degrees of G×, respectively. Thus, ¯d× = 1/n P"
NOTATION,0.06666666666666667,"v∈V× d(v). A walk in a graph is a
sequence of vertices such that consecutive vertices are linked by an edge. Performing a random
walk on the direct product G× of two graphs G and G′ is equivalent to performing a simultaneous
random walk on the two graphs G and G′."
NOTATION,0.07,"We use ⊗to represent the Kronecker product, and use ⊙to represent elementwise multiplication
between two matrices or vectors of the same dimension. For a p × q matrix V, vec(V) ∈Rpq
represents the vectorized form of V, obtained by stacking its columns. Let also vec−1 denote
the inverse vectorization operator which transforms a vector into a matrix, i. e., for a pq vector v,
V = vec−1(v) where V ∈Rp×q (see the appendix for the exact deﬁnition of the vec and vec−1
operators)."
NOTATION,0.07333333333333333,Under review as a conference paper at ICLR 2022
RANDOM WALK KERNEL,0.07666666666666666,"3.2
RANDOM WALK KERNEL"
RANDOM WALK KERNEL,0.08,"Given two graphs G and G′, the random walk kernel counts all pairs of matching walks on G and
G′ G¨artner et al. (2003). There are different variants of the kernel. For instance, the p-step random
walk kernel (where p ∈N) counts all pairs of matching walks up to length p on two graphs. The
number of matching walks can be obtained through the adjacency matrix A× of the product graph
G× Vishwanathan et al. (2010) since a random walk on G× is equivalent to a simultaneous random
walk on the two graphs. Assuming a uniform distribution for the starting and stopping probabilities
over the vertices of two graphs, the p-step random walk kernel is deﬁned as:"
RANDOM WALK KERNEL,0.08333333333333333,"κp(G, G′) ="
RANDOM WALK KERNEL,0.08666666666666667,"|V×|
X i=1"
RANDOM WALK KERNEL,0.09,"|V×|
X i=j "" p
X"
RANDOM WALK KERNEL,0.09333333333333334,"l=0
λlAl
× # ij"
RANDOM WALK KERNEL,0.09666666666666666,"where λ0, λ1, λ2, . . . , λp are positive, real-valued weights, and A0
× is the identity matrix, i. e., A0
× =
I. For p →∞, we obtain κ∞(G, G′) which is known as the random walk kernel."
RANDOM WALK KERNEL,0.1,"It turns out that if the sequence of weights λ0, λ1, λ2, . . . coresponds to the geometric sequence
deﬁned as λl = λl, then the limit κ∞(G, G′) can be computed analytically as follows:"
RANDOM WALK KERNEL,0.10333333333333333,"k∞(G, G′) ="
RANDOM WALK KERNEL,0.10666666666666667,"|V×|
X i=1"
RANDOM WALK KERNEL,0.11,"|V×|
X i=j "" ∞
X"
RANDOM WALK KERNEL,0.11333333333333333,"l=0
λlAl
× # ij
="
RANDOM WALK KERNEL,0.11666666666666667,"|V×|
X i=1"
RANDOM WALK KERNEL,0.12,"|V×|
X i=j"
RANDOM WALK KERNEL,0.12333333333333334,"
(I −λA×)−1"
RANDOM WALK KERNEL,0.12666666666666668,"ij = 1⊤(I −λA×)−11
(1)"
RANDOM WALK KERNEL,0.13,"It is well-known that the geometric series of matrices I+λA× +(λA×)2 +. . . converges only if the
the largest-magnitude eigenvalue of A× (which is also the maximum eigenvalue if G× is a graph
with non-negative edge weights), denoted by µmax
×
, is strictly smaller than 1/λ. Therefore, the
geometric random walk kernel k∞is well-deﬁned only if λ < 1/µmax
×
. Interestingly, the maximum
eigenvalue of A× is sandwiched between the average and the maximum of the vertex degrees of G×
Brouwer & Haemers (2011). We thus have that ¯d× ≤µmax
×
≤∆×, and by setting λ < 1/∆×, the
geometric series of matrices is guaranteed to converge."
RANDOM WALK KERNEL,0.13333333333333333,"By deﬁning initial and stopping probability distributions over the vertices of G and G′, we can
obtain a probabilistic variant of the geometric random walk kernel. Let p and p′ be two vectors that
represent the initial probability distributions over the vertices of G and G′. Likewise, let q and q′
denote stopping probability distributions over the vertices of G and G′. For uniform distributions for
the initial and stopping probabilities over the vertices of the two graphs, we have pi = qi = 1/|V |
and p′
i = q′
i = 1/|V ′|. Then, p× = p p′⊤and q× = q q′⊤, and the variant of the geometric random
walk kernel can be computed as k∞(G, G′) = vec(q×)⊤(I −λA×)−1vec(p×)."
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.13666666666666666,"4
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.14,"The proposed GRWNN model maps input graphs to vectors by comparing them against a number of
“hidden graphs”, i. e., graphs whose adjacency and attribute matrices are trainable. The function that
we employ to compare the input graphs against the “hidden graphs” is the geometric random walk
graph kernel, one of the most well-studied kernels between graphs G¨artner et al. (2003); Mah´e et al.
(2004); Vishwanathan et al. (2010). The proposed GRWNN model contains N “hidden graphs” in
total. The graphs may differ from each other in terms of size (i. e., number of vertices). Further-
more, the vertices and/or edges of those graphs can be annotated with continuous multi-dimensional
features. As mentioned above, both the structure and the vertex attributes (if any) of these “hidden
graphs” are trainable. Thus, the adjacency matrix of a “hidden graph” Gi of size n is described by a
trainable matrix Wi ∈Rn×n, while the vertex attributes are contained in the rows of another train-
able matrix Qi ∈Rn×d. Note that the “hidden graphs” correspond to weighted graphs, which can
be directed or undirected graphs with or without self-loops. In our implementation, we constraint
them to be undirected graphs without self-loops (n(n−1)/2 trainable parameters in total)."
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.14333333333333334,"To compare an input graph G against a “hidden graph” Gi, the model uses the geometric random
walk kernel that was introduced in the previous section:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.14666666666666667,"k∞(G, Gi) ="
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.15,"|V×|
X i=1"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.15333333333333332,"|V×|
X i=j "" ∞
X"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.15666666666666668,"l=0
λlAl
× # ij
="
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.16,"|V×|
X i=1"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.16333333333333333,"|V×|
X i=j"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.16666666666666666,"
(I −λA×)−1"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.17,"ij = 1⊤(I −λA×)−11
(2)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.17333333333333334,Under review as a conference paper at ICLR 2022
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.17666666666666667,"where A× = A⊗Ai and Ai is the adjacency matrix of “hidden graph” Gi obtained as Ai = f(W).
Here, f(·) is a function whose output is non-negative and potentially bounded, i. e., f(Wi) =
ReLU(Wi) or f(Wi) = σ(Wi) where σ(·) denotes the sigmoid activation function. Then, given
the set Gh = {G1, G2, . . . , GN} where G1, G2, . . . , GN denote the N “hidden graphs”, we can
compute N kernel values in total. These kernel values can be thought of as features of the input
graph, and can be concatenated to form a vector representation of the input graph. This vector can
then be fed into a fully-connected neural network to produce the output."
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.18,"Following Vishwanathan et al. (2010), to compute the geometric random walk graph kernel shown
in Equation equation 2 above, we employ a two-step approach . We ﬁrst need to solve the following
linear system for z:
(I −λA×) z = 1"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.18333333333333332,"Then, given z, we can compute the kernel value as k∞(G, Gi) = 1⊤z. To solve the above linear
system, we capitalize on ﬁxed point methods. We ﬁrst rewrite the above system as:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.18666666666666668,"z = 1 + λA× z
(3)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.19,"Now, solving for z is equivalent to ﬁnding a ﬁxed point of Equation equation 3 Nocedal & Wright
(2006). Such a ﬁxed point can be obtained by simply iterating the ﬁrst part of the forward pass.
Letting z(t) denote the value of z at iteration t, we set z(0) = 1, and then compute the following:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.19333333333333333,z(t+1) = 1 + λA× z(t)
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.19666666666666666,"repeatedly until ||z(t+1) −z(t)|| < ϵ, where || · || denotes the Euclidean norm and ϵ some predeﬁned
tolerance or until a speciﬁc number of iterations has been reached. As mentioned in the previous
section, the above problem is guaranteed to converge if the maximum eigenvalue of A× is strictly
smaller than 1/λ, thus if all the eigenvalues of λA× lie inside the unit disk. If the values of the
elements of Ai are bounded, we can compute an upper bound on the maximum degree of G× and
set the parameter λ to some value smaller than the inverse of the upper bound."
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.2,"Efﬁcient implementation.
If the input graph G consists of n vertices and a “hidden graph” Gi
consists of c vertices, then A× is an nc × nc matrix. Thus, multiplying A× by some vector inside
the ﬁxed-point algorithm requires O(n2c2) operations in total. Fortunately, to compute the kernel,
it is not necessary to explicitly compute matrix A×. Speciﬁcally, the Kronecker product and vec
operator are linked by the well-known property Bernstein (2009):"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.20333333333333334,"vec(A B C) = (C⊤⊗A)vec(B)
(4)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.20666666666666667,"Then, let Z ∈Rn×c be a matrix such that Z = vec−1(z). Recall also that A× = A ⊗Ai. Based on
the above and on Equation equation 4, we can write:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.21,"A× z = (A ⊗Ai)vec(Z) = vec(Ai ZA⊤) = vec(Ai vec−1(z)A⊤)
(5)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.21333333333333335,"The above matrix-vector product can be computed in O(n2c) time in case n > c. If A is sparse,
then it can be computed yet more efﬁciently. Furthermore, we do not need to compute and store
matrix A× which might not be feasible due to high memory requirements. Then, instead of solving
the system of Equation equation 3, we solve the following equivalent system:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.21666666666666667,"z = 1 + λ vec(Ai vec−1(z)A⊤)
(6)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.22,"Node attributes.
In many real-world problems, vertices of the input graphs are annotated with
real-valued multi-dimensional vertex attributes. We next generalize the proposed model to graphs
that contain such vertex attributes. Let X ∈Rn×d denote the matrix that contains the vertex at-
tributes of the input graph G. As already mentioned, we also associate a trainable matrix Qi ∈Rc×d
to each “hidden graph” Gi, where c is the number of vertices of Gi. Then, let S = σ(X Q⊤
i ) ∈Rc×n"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.22333333333333333,"where σ(·) denotes the sigmoid function. The (j, k)th element of matrix S is equal to the inner prod-
uct (followed by a sigmoid) between the attributes of the jth vertex of the input graph G and the
kth vertex of the “hidden graph” Gi. Roughly speaking, this matrix encodes the similarity between
the attributes of the vertices of the two graphs. Note that instead of directly using matrix X, we can
ﬁrst transform it into matrix ˜X using a single- or a multi-layer perceptron. Let s = vec(S) where
s ∈Rnc. Each element of s corresponds to a vertex of G× and quantiﬁes the similarity between"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.22666666666666666,Under review as a conference paper at ICLR 2022
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.23,"the attributes of the pair of vertices (i. e., one from G and one from Gi) it represents. Then, we can
compute the geometric random walk kernel as follows:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.23333333333333334,"k∞(G, G′) ="
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.23666666666666666,"|V×|
X i=1"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.24,"|V×|
X i=j "" ∞
X"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.24333333333333335,"l=0
λl 
(s s⊤) ⊙A×
l
# ij ="
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.24666666666666667,"|V×|
X i=1"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.25,"|V×|
X i=j"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.25333333333333335,"h 
I −λ(s s⊤) ⊙A×
−1i"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.25666666666666665,"ij = 1⊤ 
I −λ(s s⊤) ⊙A×
−11 (7)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.26,"Note that since the elements of s take values between 0 and 1, the same applies to the elements of
the output of the outer product s s⊤. Therefore, the maximum degree of the vertices of the graph
derived from the matrix s s⊤⊙A× is not greater than that of the graph derived from matrix A×,
and we do not thus need to set λ to a new value. Then, to compute the kernel, we ﬁrst need to solve
the following system:
z = 1 + λ(s s⊤⊙A×)z
(8)
Again, naively computing the right part of the above Equation is expensive and requires O(n2c2)
operations in total. The following result shows that in fact we can compute the above in a more time
and memory efﬁcient manner.
Proposition 1. Let A1 ∈Rn×n and A2 ∈Rm×m be two real matrices. Let also s, y ∈Rnm be
two real vectors. Then, we have that:
 
s s⊤⊙(A1 ⊗A2)

y = s ⊙vec
 
A2 vec−1(y ⊙s)A⊤
1
"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.2633333333333333,"Based on the above result (the proof is left to the appendix), the system that needs to be solved is:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.26666666666666666,"z = 1 + λ

s ⊙vec
 
Ai vec−1(z ⊙s)A⊤"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.27,"Since we store matrix A as a sparse matrix, if there are O(m) non-zero entries in A, then com-
puting one iteration of the above equation for all N “hidden graphs” takes O
 
Nc(n(d + c) + m)
"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.2733333333333333,computational time where d is the size of the vertex attributes.
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.27666666666666667,"Implicit differentiation.
Clearly, iteratively computing Equation equation 3 or Equation equa-
tion 8 to ﬁnd the ﬁxed point corresponds to a differentiable module. However, to train the model,
we need to backpropagate the error through the ﬁxed point solver in the backward pass. That would
require storing all the intermediate terms, which could be prohibitive in practice. Fortunately, thanks
to recent advances in implicit layers and equilibrium models Bai et al. (2019), this can be performed
in a simple and efﬁcient manner which requires constant memory, and assumes no knowledge of the
ﬁxed point solver. Speciﬁcally, based on ideas from Bai et al. (2019), we derive the form of implicit
backpropagation speciﬁc to the employed ﬁxed point iteration layer.
Theorem 1. Let fθ be the system of Equation equation 3 or Equation equation 8, and z⋆∈Rnc be a
solution to that linear system. Let also gθ(z⋆; A, X) = fθ(z⋆; A, X) −z⋆. Since z⋆is a ﬁxed point,
we have that gθ(z⋆; A, X) →0 and z⋆is thus the root of gθ. Let y ∈R denote the ground-truth
target of the input sample, h : R →R be any differentiable function and let L : R × R →R be a
loss function that computes:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.28,"ℓ= L
 
h(1⊤z⋆), y

= L
 
h
 
1⊤FindRoot(gθ; A, X)

, y

(9)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.2833333333333333,"Then, the gradient of the loss w.r.t. (·) (e. g., θ, A or X) is:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.2866666666666667,"∂ℓ
∂(·) = −∂ℓ"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.29,"∂z⋆
 
J−1
gθ

z⋆
∂fθ(z⋆; A, X)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.29333333333333333,"∂(·)
= −∂ℓ"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.2966666666666667,"∂h
∂h
∂z⋆
 
J−1
gθ

z⋆
∂fθ(z⋆; A, X)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.3,"∂(·)
(10)"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.30333333333333334,"where J−1
gθ

z⋆is the inverse Jacobian of gθ evaluated at z⋆."
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.30666666666666664,"The above formula gives a form for the necessary Jacobian without needing to backpropagate
through the method used to obtain the ﬁxed point. Thus, as mentioned above, we only need to
ﬁnd the ﬁxed point, and we can compute the necessary Jacobians at this speciﬁc point using the
above analytical form. No intermediate terms of the iterative method used to compute the ﬁxed
point need to be stored in memory, while there is also no need to unroll the forward computations"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.31,Under review as a conference paper at ICLR 2022
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.31333333333333335,"within an automatic differentiation layer. Still, to compute the analytical backward gradient at the
solution of the ﬁxed point equation, it is necessary to ﬁrst compute the exact inverse Jacobian J−1
gθ
which has a cubic cost. As shown in Bai et al. (2019), we can instead compute the −∂ℓ"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.31666666666666665,"∂z⋆
 
J−1
gθ

z⋆
"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.32,term by solving the following linear system:
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.3233333333333333,"x =
∂fθ(z⋆; A, X) ∂z⋆"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.32666666666666666,"⊤
x +
 ∂ℓ ∂z⋆ ⊤"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.33,"which in fact is also a ﬁxed point equation and can be solved via some iterative procedure. Note that
the ﬁrst term of the above Equation is a vector-Jacobian product which can be efﬁciently computed
via autograd packages (e. g., PyTorch Paszke et al. (2017)) for any x, without explicitly writing out
the Jacobian matrix. Finally, we can compute
∂ℓ
∂(·) as follows:"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.3333333333333333,"∂ℓ
∂(·) =
∂fθ(z⋆; A, X) ∂(·) ⊤
x"
GEOMETRIC RANDOM WALK GRAPH NEURAL NETWORKS,0.33666666666666667,"where again this product is itself a vector-Jacobian product, computable via normal automatic dif-
ferentiation packages."
EXPERIMENTAL EVALUATION,0.34,"5
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.3433333333333333,We next evaluate the proposed GRWNN model on standard graph classiﬁcation datasets.
EXPERIMENTAL EVALUATION,0.3466666666666667,"Datasets. We evaluate the proposed model on 10 publicly available graph classiﬁcation datasets
including 5 bio/chemo-informatics datasets: MUTAG, D&D, NCI1, PROTEINS, ENZYMES, and 5
social interaction datasets: IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI-
5K, COLLAB Kersting et al. (2016). To show that the proposed model also scales to larger datasets,
we additionally use two Open Graph Benchmark (OGB) datasets Hu et al. (2020). Speciﬁcally, we
use a molecular property prediction dataset: ogbg-molhiv, and a code summarization dataset:
ogbg-code2. More details about the datasets are given in the appendix."
EXPERIMENTAL EVALUATION,0.35,"Experimental Setup. In the case of the 10 standard benchmark datasets, we compare the proposed
model against the following three graph kernels: (1) graphlet kernel (GR) Shervashidze et al. (2009),
(2) shortest path kernel (SP) Borgwardt & Kriegel (2005), and (3) Weisfeiler-Lehman subtree kernel
(WL) Shervashidze et al. (2011), and against the following six neural network models: (1) DGCNN
Zhang et al. (2018a), (2) DiffPool Ying et al. (2018), (3) ECC Simonovsky & Komodakis (2017),
(4) GIN Xu et al. (2019), (5) GraphSAGE Hamilton et al. (2017), and (6) RWNN Nikolentzos &
Vazirgiannis (2020). We also compare the proposed model against GRWNN-ﬁxed, a variant of the
model whose “hidden graphs” are randomly initialized and kept ﬁxed during training. To evaluate
the proposed model, we employ the experimental protocol proposed in Errica et al. (2020). There-
fore, we perform 10-fold cross-validation to obtain an estimate of the generalization performance of
each method, while within each fold a model is selected based on a 90%/10% split of the training
set. We use exactly the same splits as in Errica et al. (2020) and in Nikolentzos & Vazirgiannis
(2020), hence, for the different datasets, we use the results reported in these two papers."
EXPERIMENTAL EVALUATION,0.35333333333333333,"For all datasets, we set the batch size to 64 and the number of epochs to 300. We use the Adam opti-
mizer with initial learning rate 0.001 and applied an adaptive learning rate decay based on validation
results. We use a 1-layer perceptron to transform the vertex attributes. We apply layer normalization
Ba et al. (2016) on the generated graph representations (i. e., vector consisting of kernel values). The
hyper-parameters we tune for each dataset are: (1) the number of “hidden graphs” ∈{32, 64}, (2)
the number of vertices of the “hidden graphs” ∈{5, 10}, (3) the hidden-dimension size of the vertex
features ∈{32, 64} for the bio/chemo-informatics datasets and ∈{8, 16} for the social interaction
datasets, and (4) the dropout ratio ∈{0, 0.1}."
EXPERIMENTAL EVALUATION,0.3566666666666667,"For both OGB datasets, we used the available predeﬁned splits. We compare the proposed model
against the following neural network models: GCN Kipf & Welling (2017), GIN Xu et al. (2019),
GCN-FLAG Kong et al. (2020), GIN-FLAG Kong et al. (2020), PNA Corso et al. (2020), GSN
Bouritsas et al. (2020), HIMP Fey et al. (2020), and DGN Beaini et al. (2020). For all models, we
use the results that are reported in the respective papers. For ogbg-code2, we did not add the
inverse edges to the graphs. All reported results are averaged over 10 runs."
EXPERIMENTAL EVALUATION,0.36,Under review as a conference paper at ICLR 2022
EXPERIMENTAL EVALUATION,0.36333333333333334,"Table 1: Classiﬁcation accuracy (± standard deviation) of the proposed model and the baselines on
the 5 chemo/bio-informatics and on the 5 social interaction benchmark datasets. OOR means Out
of Resources, either time (> 72 hours for a single training) or GPU memory. Best performance per
dataset in bold, among the neural network architectures underlined."
EXPERIMENTAL EVALUATION,0.36666666666666664,"MUTAG
D&D
NCI1
PROTEINS
ENZYMES
SP
80.2 (± 6.5)
78.1 (± 4.1)
72.7 (± 1.4)
75.3 (± 3.8)
38.3 (± 8.0)
GR
80.8 (± 6.4)
75.4 (± 3.4)
61.8 (± 1.7)
71.6 (± 3.1)
25.1 (± 4.4)
WL
84.6 (± 8.3)
78.1 (± 2.4)
84.8 (± 2.5)
73.8 (± 4.4)
50.3 (± 5.7)
DGCNN
84.0 (± 6.7)
76.6 (± 4.3)
76.4 (± 1.7)
72.9 (± 3.5)
38.9 (± 5.7)
DiffPool
79.8 (± 7.1)
75.0 (± 3.5)
76.9 (± 1.9)
73.7 (± 3.5)
59.5 (± 5.6)
ECC
75.4 (± 6.2)
72.6 (± 4.1)
76.2 (± 1.4)
72.3 (± 3.4)
29.5 (± 8.2)
GIN
84.7 (± 6.7)
75.3 (± 2.9)
80.0 (± 1.4)
73.3 (± 4.0)
59.6 (± 4.5)
GraphSAGE
83.6 (± 9.6)
72.9 (± 2.0)
76.0 (± 1.8)
73.0 (± 4.5)
58.2 (± 6.0)
1-step RWNN
89.2 (± 4.3)
77.6 (± 4.7)
71.4 (± 1.8)
74.7 (± 3.3)
56.7 (± 5.2)
2-step RWNN
88.1 (± 4.8)
76.9 (± 4.6)
73.0 (± 2.0)
74.1 (± 2.8)
57.4 (± 4.9)
GRWNN-ﬁxed
81.9 (± 6.4)
73.2 (± 3.5)
66.9 (± 2.4)
74.6 (± 4.0)
56.8 (± 3.7)
GRWNN
83.4 (± 5.6)
75.6 (± 4.6)
67.7 (± 2.2)
74.9 (± 3.5)
62.7 (± 5.2)"
EXPERIMENTAL EVALUATION,0.37,"IMDB
IMDB
REDDIT
REDDIT
COLLAB
BINARY
MULTI
BINARY
MULTI-5K
SP
57.7 (± 4.1)
39.8 (± 3.7)
89.0 (± 1.0)
51.1 (± 2.2)
79.9 (± 2.7)
GR
63.3 (± 2.7)
39.6 (± 3.0)
76.6 (± 3.3)
38.1 (± 2.3)
71.1 (± 1.4)
WL
72.8 (± 4.5)
51.2 (± 6.5)
74.9 (± 1.8)
49.6 (± 2.0)
78.0 (± 2.0)
DGCNN
69.2 (± 3.0)
45.6 (± 3.4)
87.8 (± 2.5)
49.2 (± 1.2)
71.2 (± 1.9)
DiffPool
68.4 (± 3.3)
45.6 (± 3.4)
89.1 (± 1.6)
53.8 (± 1.4)
68.9 (± 2.0)
ECC
67.7 (± 2.8)
43.5 (± 3.1)
OOR
OOR
OOR
GIN
71.2 (± 3.9)
48.5 (± 3.3)
89.9 (± 1.9)
56.1 (± 1.7)
75.6 (± 2.3)
GraphSAGE
68.8 (± 4.5)
47.6 (± 3.5)
84.3 (± 1.9)
50.0 (± 1.3)
73.9 (± 1.7)
1-step RWNN
70.8 (± 4.8)
47.8 (± 3.8)
90.4 (± 1.9)
51.7 (± 1.5)
71.7 (± 2.1)
2-step RWNN
70.6 (± 4.4)
48.8 (± 2.9)
90.3 (± 1.8)
51.7 (± 1.4)
71.3 (± 2.1)
GRWNN-ﬁxed
72.1 (± 4.1)
48.1 (± 3.6)
82.2 (± 2.4)
53.1 (± 1.8)
71.3 (± 1.9)
GRWNN
72.8 (± 4.2)
49.0 (± 2.9)
90.0 (± 1.8)
54.4 (± 1.7)
72.1 (± 1.9)"
EXPERIMENTAL EVALUATION,0.37333333333333335,"For both OGB datasets, we set the batch size to 128. For the ogb-molhiv dataset, we set the
number of epochs to 300, the number of “hidden graphs” to 200, the number of vertices of the
“hidden graphs” to 5, the hidden-dimension size of the vertex features to 128 and the dropout ratio
to 0.1. Furthermore, we employ the probabilistic variant of the geometric random walk kernel
and use uniform distributions for the initial and stopping probabilities over the vertices of the two
compared graphs. For the ogb-code2 dataset, we set the number of epochs to 100, the number of
“hidden graphs” to 200, the number of vertices of the “hidden graphs” to 5, the hidden-dimension
size of the vertex features to 128 and the dropout ratio to 0.2. For both datasets, we apply layer
normalization Ba et al. (2016) on the generated graph representations."
EXPERIMENTAL EVALUATION,0.37666666666666665,"Implementation Details. To set the value of parameter λ, we assume a transductive setting, where
we are given a collection of graphs beforehand. Therefore, we can ﬁnd the vertex of highest degree
across all graphs and set the value of λ accordingly. In the inductive learning setting, since we do
not know a priori target graphs that the model may receive in the future, λ should be small enough
so that λ < 1/µmax
×
for any pair of an unseen graph and a “hidden graph”. This is a limitation of
the proposed model since in case the model receives at test time a graph whose largest eigenvalue is
higher than expected, we need to set λ to a smaller value and retrain the model."
EXPERIMENTAL EVALUATION,0.38,"To compute the ﬁxed point of Equation equation 3 or equation 8, we followed the naive approach
where we simply performed multiple times the forward iteration. In practice, there are more efﬁcient
ﬁxed point iteration methods, such as Anderson Acceleration Walker & Ni (2011), that converge
faster than the naive forward iteration at the cost of some additional memory complexity. However,
as shown next, we found that in our setting, the naive forward iteration converges in a small number
of steps, while the additional cost introduced by more efﬁcient methods associated with the gener-
ation and manipulation of new tensors made them overall slower than the naive forward iteration
even though they required fewer iterations to converge."
EXPERIMENTAL EVALUATION,0.38333333333333336,"The model was implemented with PyTorch Paszke et al. (2019), and all experiments were run on a
single machine equipped with an NVidia Titan Xp GPU card."
EXPERIMENTAL EVALUATION,0.38666666666666666,"Results. Table 1 illustrates average prediction accuracies and standard deviations for the 10 standard
graph classiﬁcation datasets. We observe that the proposed GRWNN model is the best-performing
method on 2 out of the 10 datasets, while it provides the second best and third best accuracy on 3"
EXPERIMENTAL EVALUATION,0.39,Under review as a conference paper at ICLR 2022
EXPERIMENTAL EVALUATION,0.3933333333333333,"Method
Dataset"
EXPERIMENTAL EVALUATION,0.39666666666666667,"ogbg-molhiv
ogbg-code2"
EXPERIMENTAL EVALUATION,0.4,"GCN
76.06 ± 0.97
15.07 ± 0.18
GIN
75.58 ± 1.40
14.95 ± 0.23
GCN+
76.83 ± 1.02
–
FLAG
GIN+
76.54 ± 1.14
–
FLAG
GSN
77.99 ± 1.00
–
HIMP
78.80 ± 0.82
–
PNA
79.05 ± 1.32
15.70 ± 0.32
DGN
79.70 ± 0.97
–
GRWNN
78.38 ± 0.99
15.03 ± 0.21"
EXPERIMENTAL EVALUATION,0.4033333333333333,"(a) Performance of the proposed model
and the baselines on the OGB datasets.
Reported values correspond to ROC-
AUC scores for ogbg-molhiv and
F1-scores for ogbg-code2."
EXPERIMENTAL EVALUATION,0.4066666666666667,"0
100
200
300 101 102"
EXPERIMENTAL EVALUATION,0.41,Avg. iterations
EXPERIMENTAL EVALUATION,0.41333333333333333,Forward Pass
EXPERIMENTAL EVALUATION,0.4166666666666667,"0
100
200
300 101"
BACKWARD PASS,0.42,"102
Backward Pass"
BACKWARD PASS,0.42333333333333334,"0
100
200
300
Epoch 0.25 0.50 0.75 1.00"
BACKWARD PASS,0.4266666666666667,Accuracy
BACKWARD PASS,0.43,Train Accuracy
BACKWARD PASS,0.43333333333333335,"0
100
200
300
Epoch 0.2 0.4 0.6"
VALIDATION ACCURACY,0.43666666666666665,"0.8
Validation Accuracy"
VALIDATION ACCURACY,0.44,"λ=1/5
λ=1/10
λ=1/20
λ=1/30
λ=1/40
λ=1/50
(b) Number of ﬁxed point iterations during the forward and back-
ward pass (top), and training and validation accuracy (bottom) on the
ENZYMES dataset for different values of λ."
VALIDATION ACCURACY,0.44333333333333336,"Figure 1: Performance on the OGB datasets and impact of the value of parameter λ on running time
and performance of the model."
VALIDATION ACCURACY,0.44666666666666666,"and 1 out of the remaining 8 datasets, respectively. The most successful method is the WL kernel
which performs best on 4 of the 10 datasets, while it outperforms the other approaches with quite
wide margins in most cases. Among the neural network models, the proposed GRWNN model
outperforms the baseline models on 4 out of the 10 datasets. On the remaining 6 datasets, GIN
is the best-performing model on half of them, and RWNN on the other half. On the ENZYMES
and IMDB-BINARY datasets, our model offers respective absolute improvements of 3.1%, and
1.6% in accuracy over GIN. Overall, the model exhibits highly competitive performance on the
graph classiﬁcation datasets, while the achieved accuracies follow different patterns from all the
baseline methods. Furthermore, the proposed model outperforms GRWNN-ﬁxed on all datasets,
demonstrating that the set of trainable “hidden graphs” is an indispensable component of the model."
VALIDATION ACCURACY,0.45,"The Table shown in Figure 1a illustrates the performance on the two OGB datasets. Note that the
proposed model does not utilize the edge features that are provided for the different datasets. Still,
we can see that it managed to outperform several of the baselines on the ogbg-molhiv dataset,
where it achieved the fourth best ROC-AUC. On the ogbg-code2 dataset, GRWNN outperformed
GIN, while it achieved an F1-score similar to that of GCN. However, all these three models achieved
a much smaller F1-score than the one achieved by PNA which is the best-performing model."
VALIDATION ACCURACY,0.4533333333333333,"As already discussed, the running time of the model depends on the number of ﬁxed point iterations
that need to be performed until convergence. Figure 1b (top) illustrates the average number of
iterations (across all batches) for the forward and backward pass for different values of λ and for
each epoch. The model was trained on a single split of the ENZYMES dataset. The maximum
eigenvalue of all graphs of the dataset is equal to 5.47, while the highest degree is equal to 9. The
number of nodes of the “hidden graphs” was set to 5. If the elements of the adjacency matrices of
the “hidden graphs” take values no greater than one, then no vertex of G× can have a degree greater
than 9∗4 = 36. Thus, setting λ < 1/36 guarantees convergence. In practice, as shown in the Figure,
we found that even if λ takes larger values, we only need a small number of iterations. For λ = 1/5,
we can see that the ﬁxed point equation fails to converge since the average number of iterations is
close to 100 (which is the upper limit we have set). For λ = 1/10 and for smaller values of λ, the
system converges in a small number of iterations. In terms of performance, as shown in Figure 1b
(bottom), the model achieves the highest levels of validation accuracy for λ = 1/20 and λ = 1/30,
while for λ = 1/5, the model yields much worse performance compared to the other values of λ.
Similar behavior was observed on the other datasets.
6
CONCLUSION
In this paper, we introduced the GRWNN model, a new architecture which generates graph repre-
sentations by comparing the input graphs against “hidden graphs” using the geometric random walk
kernel. To compute the kernel, the proposed model uses a ﬁxed point iteration algorithm, and to
update the “hidden graphs” during backpropagation, the model capitalizes on implicit differenta-
tion.The model was evaluated on several graph classiﬁcation datasets where it achieved competitive
performance."
VALIDATION ACCURACY,0.45666666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.46,REFERENCES
REFERENCES,0.4633333333333333,"Brandon Amos, Ivan Dario Jimenez Rodriguez, Jacob Sacks, Byron Boots, and J Zico Kolter. Dif-
ferentiable MPC for End-to-end Planning and Control. In Advances in Neural Information Pro-
cessing Systems, volume 31, pp. 8299–8310, 2018."
REFERENCES,0.4666666666666667,"James Atwood and Don Towsley. Diffusion-Convolutional Neural Networks . In Advances in Neural
Information Processing Systems, volume 29, pp. 1993–2001, 2016."
REFERENCES,0.47,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.47333333333333333,"Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep Equilibrium Models. In Advances in Neural
Information Processing Systems, volume 32, pp. 690–701, 2019."
REFERENCES,0.4766666666666667,"Dominique Beaini, Saro Passaro, Vincent L´etourneau, William L Hamilton, Gabriele Corso, and
Pietro Li`o. Directional Graph Networks. arXiv preprint arXiv:2010.02863, 2020."
REFERENCES,0.48,"Dennis S Bernstein. Matrix mathematics: theory, facts, and formulas. Princeton University Press,
2009."
REFERENCES,0.48333333333333334,"K. Borgwardt, C. Ong, S. Sch¨onauer, S. Vishwanathan, A. Smola, and H. Kriegel. Protein function
prediction via graph kernels. Bioinformatics, 21(Suppl. 1):i47–i56, 2005."
REFERENCES,0.4866666666666667,"K. M. Borgwardt and H. Kriegel. Shortest-path kernels on graphs. In Proceedings of the 5th Inter-
national Conference on Data Mining, pp. 74–81, 2005."
REFERENCES,0.49,"Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein.
Improv-
ing graph neural network expressivity via subgraph isomorphism counting.
arXiv preprint
arXiv:2006.09252, 2020."
REFERENCES,0.49333333333333335,"Andries E Brouwer and Willem H Haemers. Spectra of graphs. Springer Science & Business Media,
2011."
REFERENCES,0.49666666666666665,"Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally
connected networks on Graphs. In 2nd International Conference on Learning Representations,
2014."
REFERENCES,0.5,"Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary Dif-
ferential Equations.
In Advances in Neural Information Processing Systems, volume 31, pp.
6572–6583, 2018."
REFERENCES,0.5033333333333333,"Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li`o, and Petar Veliˇckovi´c. Principal
Neighbourhood Aggregation for Graph Nets.
In Advances in Neural Information Processing
Systems, volume 33, 2020."
REFERENCES,0.5066666666666667,"Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J Zico Kolter.
End-to-End Differentiable Physics for Learning and Control. Advances in neural information
processing systems, 31:7178–7189, 2018."
REFERENCES,0.51,"A. Debnath, R. Lopez de Compadre, G. Debnath, A. Shusterman, and C. Hansch. Structure-Activity
Relationship of Mutagenic Aromatic and Heteroaromatic Nitro Compounds. Correlation with
Molecular Orbital Energies and Hydrophobicity. Journal of Medicinal Chemistry, 34(2):786–
797, 1991."
REFERENCES,0.5133333333333333,"Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional Neural Networks on
Graphs with Fast Localized Spectral Filtering. In Advances in Neural Information Processing
Systems, volume 29, pp. 3837–3845, 2016."
REFERENCES,0.5166666666666667,"Nima Dehmamy, Albert-Laszlo Barabasi, and Rose Yu. Understanding the Representation Power
of Graph Neural Networks in Learning Graph Topology. In Advances in Neural Information
Processing Systems, volume 32, 2019."
REFERENCES,0.52,"P. Dobson and A. Doig. Distinguishing Enzyme Structures from Non-enzymes Without Alignments.
Journal of Molecular Biology, 330(4):771–783, 2003."
REFERENCES,0.5233333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.5266666666666666,"David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional Networks on Graphs for Learning Molecular
Fingerprints. In Advances in Neural Information Processing Systems, volume 28, 2015."
REFERENCES,0.53,"Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A Fair Comparison of Graph
Neural Networks for Graph Classiﬁcation. In Proceedings of the International Conference on
Learning Representations, 2020."
REFERENCES,0.5333333333333333,"Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical Inter-Message Passing for Learning
on Molecular Graphs. arXiv preprint arXiv:2006.12179, 2020."
REFERENCES,0.5366666666666666,"Claudio Gallicchio and Alessio Micheli. Fast and Deep Graph Neural Networks. In Proceedings of
the 34th AAAI Conference on Artiﬁcial Intelligence, pp. 3898–3905, 2020."
REFERENCES,0.54,"Thomas G¨artner, Peter Flach, and Stefan Wrobel. On Graph Kernels: Hardness Results and Efﬁcient
Alternatives. In Learning Theory and Kernel Machines, pp. 129–143. 2003."
REFERENCES,0.5433333333333333,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
Message Passing for Quantum Chemistry. In Proceedings of the 34th International Conference
on Machine Learning, pp. 1263–1272, 2017."
REFERENCES,0.5466666666666666,"Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui. Implicit Graph
Neural Networks. In Advances in Neural Information Processing Systems, volume 33, pp. 11984–
11995, 2020."
REFERENCES,0.55,"Will Hamilton, Zhitao Ying, and Jure Leskovec.
Inductive Representation Learning on Large
Graphs. In Advances in Neural Information Processing Systems, pp. 1024–1034, 2017."
REFERENCES,0.5533333333333333,"David Haussler. Convolution kernels on discrete structures. Technical report, Technical report,
Department of Computer Science, University of California at Santa Cruz, 1999."
REFERENCES,0.5566666666666666,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. arXiv
preprint arXiv:2005.00687, 2020."
REFERENCES,0.56,"Janis Kalofolias, Pascal Welke, and Jilles Vreeken. SUSAN: The Structural Similarity Random
Walk Kernel. In Proceedings of the 2021 SIAM International Conference on Data Mining, 2021."
REFERENCES,0.5633333333333334,"U Kang, Hanghang Tong, and Jimeng Sun. Fast Random Walk Graph Kernel. In Proceedings of the
2012 SIAM International Conference on Data Mining, pp. 828–838, 2012."
REFERENCES,0.5666666666666667,"Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.
Marginalized Kernels Between Labeled
Graphs. In Proceedings of the 20th International Conference on Machine Learning, pp. 321–
328, 2003."
REFERENCES,0.57,"Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond ﬁngerprints. Journal of Computer-Aided Molecular Design, 30(8):
595–608, 2016."
REFERENCES,0.5733333333333334,"Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Bench-
mark data sets for graph kernels, 2016. http://graphkernels.cs.tu-dortmund.de."
REFERENCES,0.5766666666666667,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. In In 5th International Conference on Learning Representations, 2017."
REFERENCES,0.58,"Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor,
and Tom Goldstein. Flag: Adversarial Data Augmentation for Graph Neural Networks. arXiv
preprint arXiv:2010.09891, 2020."
REFERENCES,0.5833333333333334,"Nils M Kriege, Fredrik D Johansson, and Christopher Morris. A survey on graph kernels. Applied
Network Science, 5(1):1–42, 2020."
REFERENCES,0.5866666666666667,"Tao Lei, Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Deriving Neural Architectures from
Sequence and Graph Kernels. In Proceedings of the 34th International Conference on Machine
Learning, pp. 2024–2033, 2017."
REFERENCES,0.59,Under review as a conference paper at ICLR 2022
REFERENCES,0.5933333333333334,"Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated Graph Sequence Neural
Networks. In 3rd International Conference on Learning Representations, 2015."
REFERENCES,0.5966666666666667,"Pierre Mah´e, Nobuhisa Ueda, Tatsuya Akutsu, Jean-Luc Perret, and Jean-Philippe Vert. Extensions
of Marginalized Graph Kernels. In Proceedings of the 21st International Conference on Machine
Learning, 2004."
REFERENCES,0.6,"Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably Powerful Graph
Networks. In Advances in Neural Information Processing Systems, 2019a."
REFERENCES,0.6033333333333334,"Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and Equivariant Graph
Networks. In 7th International Conference on Learning Representations, 2019b."
REFERENCES,0.6066666666666667,"Alessio Micheli. Neural Network for Graphs: A Contextual Constructive Approachs. IEEE Trans-
actions on Neural Networks, 20(3):498–511, 2009."
REFERENCES,0.61,"Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and Leman Go Neural: Higher-order Graph Neural Net-
works. In Proceedings of the 33rd AAAI Conference on Artiﬁcial Intelligence, pp. 4602–4609,
2019."
REFERENCES,0.6133333333333333,"Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and Leman go sparse: Towards
scalable higher-order graph embeddings. In Advances in Neural Information Processing Systems,
volume 33, pp. 21824–21840, 2020."
REFERENCES,0.6166666666666667,"Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning Convolutional Neural Net-
works for Graphs. In Proceedings of the 33rd International Conference on Machine Learning,
pp. 2014–2023, 2016."
REFERENCES,0.62,"Giannis Nikolentzos and Michalis Vazirgiannis. Random Walk Graph Neural Networks. Advances
in Neural Information Processing Systems, 33:16211–16222, 2020."
REFERENCES,0.6233333333333333,"Jorge Nocedal and Stephen Wright. Numerical Optimization. Springer Science & Business Media,
2006."
REFERENCES,0.6266666666666667,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in
PyTorch. 2017."
REFERENCES,0.63,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An Imperative Style,
High-Performance Deep Learning Library. In Advances in Neural Information Processing Sys-
tems, volume 32, pp. 8026–8037, 2019."
REFERENCES,0.6333333333333333,"Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The Graph Neural Network Model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009."
REFERENCES,0.6366666666666667,"N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-
Lehman Graph Kernels. The Journal of Machine Learning Research, 12:2539–2561, 2011."
REFERENCES,0.64,"Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten M Borgwardt.
Efﬁcient graphlet kernels for large graph comparison. In The 12th International Conference on
Artiﬁcial Intelligence and Statistics, pp. 488–495, 2009."
REFERENCES,0.6433333333333333,"Martin Simonovsky and Nikos Komodakis. Dynamic Edge-Conditioned Filters in Convolutional
Neural Networks on Graphs. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3693–3702, 2017."
REFERENCES,0.6466666666666666,"Alessandro Sperduti and Antonina Starita. Supervised Neural Networks for the Classiﬁcation of
Structures. IEEE Transactions on Neural Networks, 8(3):714–735, 1997."
REFERENCES,0.65,"Mahito Sugiyama and Karsten Borgwardt. Halting in Random Walk Kernels. Advances in Neural
Information Processing Systems, 28:1639–1647, 2015."
REFERENCES,0.6533333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.6566666666666666,"Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph learning with 1d convolu-
tions on random walks. arXiv preprint arXiv:2102.08786, 2021."
REFERENCES,0.66,"S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. Graph
Kernels. Journal of Machine Learning Research, 11:1201–1242, 2010."
REFERENCES,0.6633333333333333,"N. Wale, I. Watson, and G. Karypis.
Comparison of descriptor spaces for chemical compound
retrieval and classiﬁcation. Knowledge and Information Systems, 14(3):347–375, 2008."
REFERENCES,0.6666666666666666,"Homer F Walker and Peng Ni. Anderson acceleration for ﬁxed-point iterations. SIAM Journal on
Numerical Analysis, 49(4):1715–1735, 2011."
REFERENCES,0.67,"Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-Based Rec-
ommendation with Graph Neural Networks. In Proceedings of the 33rd AAAI Conference on
Artiﬁcial Intelligence, pp. 346–353, 2019."
REFERENCES,0.6733333333333333,"Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S
Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learn-
ing. Chemical Science, 9(2):513–530, 2018."
REFERENCES,0.6766666666666666,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural
Networks? In 7th International Conference on Learning Representations, 2019."
REFERENCES,0.68,"P. Yanardag and S. Vishwanathan. Deep Graph Kernels. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 1365–1374, 2015."
REFERENCES,0.6833333333333333,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical Graph Representation Learning with Differentiable Pooling. In Advances in Neural
Information Processing Systems, pp. 4801–4811, 2018."
REFERENCES,0.6866666666666666,"Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An End-to-End Deep Learning
Architecture for Graph Classiﬁcation. In Proceedings of the 32nd AAAI Conference on Artiﬁcial
Intelligence, pp. 4438–4445, 2018a."
REFERENCES,0.69,"Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, and Arye Nehorai. RetGK: Graph Kernels
based on Return Probabilities of Random Walks. Advances in Neural Information Processing
Systems, 31:3964–3974, 2018b."
REFERENCES,0.6933333333333334,"A
APPENDIX"
REFERENCES,0.6966666666666667,"The appendix is organized as follows. In section B, we deﬁne some basic concepts from linear
algebra. In section C, we prove the Proposition 1. In section D, we give more details about the
direct differentiation through the ﬁxed point, while section E provides a detailed description of the
graph classiﬁcation datasets. Section F gives details about the parameter λ, while section G about
the running time of the model. Finally, in section H, we perform a sensitivity analysis."
REFERENCES,0.7,"B
LINEAR ALGEBRA CONCEPTS"
REFERENCES,0.7033333333333334,"In this Section, we provide deﬁnitions for concepts of linear algebra, namely the vectorization oper-
ator, the inverse vectorization operator and the Kronecker product, which we use heavily in the main
paper.
Deﬁnition 1. Given a real matrix A ∈Rm×n, the vectorization operator vec : Rm×n →Rmn is
deﬁned as:"
REFERENCES,0.7066666666666667,vec(A) =  
REFERENCES,0.71,"A:1
A:2
...
A:n  "
REFERENCES,0.7133333333333334,where A:i is the ith column of A.
REFERENCES,0.7166666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.72,"Deﬁnition 2. Given a real vector b ∈Rmn, the inverse vectorization operator vec−1 : Rnm →
Rn×m is deﬁned as:"
REFERENCES,0.7233333333333334,vec−1(b) =  
REFERENCES,0.7266666666666667,"b1
bn+1
. . .
bn(m−1)+1
b2
bn+2
. . .
bn(m−1)+2
...
...
...
...
bn
b2n
. . .
bnm  "
REFERENCES,0.73,"Deﬁnition 3. Given real matrices A ∈Rn×m and B ∈Rp×q, the Kronecker product A ⊗B ∈
Rnp×mq deﬁned as:"
REFERENCES,0.7333333333333333,A ⊗B =  
REFERENCES,0.7366666666666667,"A11 B
A12 B
. . .
A1m B
A21 B
A22 B
. . .
A2m B
...
...
...
...
An1 B
An2 B
. . .
Anm B  "
REFERENCES,0.74,"C
PROOF OF PROPOSITION 1"
REFERENCES,0.7433333333333333,"For convenience, we restate the Proposition below.
Proposition 2. Let A1 ∈Rn×n and A2 ∈Rm×m be two real matrices. Let also s, y ∈Rnm be
two real vectors. Then, we have that:
 
s s⊤⊙(A1 ⊗A2)

y = s ⊙vec
 
A2 vec−1(s ⊙y)A⊤
1
"
REFERENCES,0.7466666666666667,"Proof. Let Ds denote a diagonal matrix with the vector s as its main diagonal. Then, we have:
 
s s⊤⊙(A1 ⊗A2)

y =
 
Ds (A1 ⊗A2) Ds

y"
REFERENCES,0.75,"The Hadamard product of two vectors s and y is the same as matrix multiplication of vector y by
the corresponding diagonal matrix Ds of vector s, i. e., Ds y = s ⊙y. Thus, it follows that:
 
Ds (A1 ⊗A2) Ds

y = Ds (A1 ⊗A2) (s ⊙y)"
REFERENCES,0.7533333333333333,"Note that the Kronecker product and vec operator are linked by the well-known property Bernstein
(2009)(Proposition 7.1.9):
vec(A B C) = (C⊤⊗A)vec(B)
Therefore, we have that:
 
Ds (A1 ⊗A2) (s ⊙y) = Ds vec
 
A2 vec−1(s ⊙y) A⊤
1

= s ⊙vec(A2 vec−1(s ⊙y) A⊤
2 )"
REFERENCES,0.7566666666666667,which concludes the proof.
REFERENCES,0.76,"D
IMPLICIT DIFFERENTIATION"
REFERENCES,0.7633333333333333,"Clearly, iteratively computing Equation (3) or Equation (8) (main paper) to ﬁnd the ﬁxed point corre-
sponds to a differentiable module. However, to train the model, we need to backpropagate the error
through the ﬁxed point solver in the backward pass. That would require storing all the intermediate
terms, which could be prohibitive in practice. Fortunately, thanks to recent advances in implicit lay-
ers and equilibrium models Bai et al. (2019), this can be performed in a simple and efﬁcient manner
which requires constant memory, and assumes no knowledge of the ﬁxed point solver. Speciﬁcally,
based on ideas from Bai et al. (2019), we derive the form of implicit backpropagation speciﬁc to the
employed ﬁxed point iteration layer.
Theorem 2. Let fθ be the system of Equation (3) or Equation (8) (main paper), and z⋆∈Rnc be a
solution to that linear system. Let also gθ(z⋆; A, X) = fθ(z⋆; A, X) −z⋆. Since z⋆is a ﬁxed point,
we have that gθ(z⋆; A, X) →0 and z⋆is thus the root of gθ. Let y ∈R denote the ground-truth
target of the input sample, h : R →R be any differentiable function and let L : R × R →R be a
loss function that computes:"
REFERENCES,0.7666666666666667,"ℓ= L
 
h(1⊤z⋆), y

= L
 
h
 
1⊤FindRoot(gθ; A, X)

, y

(11)"
REFERENCES,0.77,Under review as a conference paper at ICLR 2022
REFERENCES,0.7733333333333333,"Then, the gradient of the loss w.r.t. (·) (e. g., θ, A or X) is:"
REFERENCES,0.7766666666666666,"∂ℓ
∂(·) = −∂ℓ"
REFERENCES,0.78,"∂z⋆
 
J−1
gθ

z⋆
∂fθ(z⋆; A, X)"
REFERENCES,0.7833333333333333,"∂(·)
= −∂ℓ"
REFERENCES,0.7866666666666666,"∂h
∂h
∂z⋆
 
J−1
gθ

z⋆
∂fθ(z⋆; A, X)"
REFERENCES,0.79,"∂(·)
(12)"
REFERENCES,0.7933333333333333,"where J−1
gθ

z⋆is the inverse Jacobian of gθ evaluated at z⋆."
REFERENCES,0.7966666666666666,"The above Theorem gives a form for the necessary Jacobian without needing to backpropagate
through the method used to obtain the ﬁxed point. We can thus treat the ﬁxed point algorithm as a
black box, and we do not need to store intermediate terms associated with the ﬁxed point algorithm
into memory. We only need to apply some algorithm that will produce a solution to the system (i. e.,
it will compute the ﬁxed point)."
REFERENCES,0.8,"Following Bai et al. (2019), we differentiate the two sides of the ﬁxed point equation z⋆=
fθ(z⋆; A, X) wih respect to (·): dz⋆"
REFERENCES,0.8033333333333333,"d(·) = dfθ(z⋆; A, X)"
REFERENCES,0.8066666666666666,"d(·)
= ∂fθ(z⋆; A, X)"
REFERENCES,0.81,"∂(·)
+ ∂fθ(z⋆; A, X)"
REFERENCES,0.8133333333333334,"∂z⋆
dz⋆ d(·)"
REFERENCES,0.8166666666666667,"=⇒

I −∂fθ(z⋆; A, X) ∂z⋆  dz⋆"
REFERENCES,0.82,"d(·) = ∂fθ(z⋆; A, X) ∂(·)"
REFERENCES,0.8233333333333334,"Since gθ(z⋆) = fθ(z⋆; A, X) −z⋆, we have:"
REFERENCES,0.8266666666666667,"Jgθ

z⋆= −

I −∂fθ(z⋆; A, X) ∂z⋆ "
REFERENCES,0.83,which implies the following:
REFERENCES,0.8333333333333334,"∂ℓ
∂(·) = ∂ℓ"
REFERENCES,0.8366666666666667,"∂z⋆
dz⋆"
REFERENCES,0.84,d(·) = −∂ℓ
REFERENCES,0.8433333333333334,"∂z⋆
 
J−1
gθ

z⋆
∂fθ(z⋆; A, X)"
REFERENCES,0.8466666666666667,"∂(·)
."
REFERENCES,0.85,"Unfortunately, computing the exact inverse Jacobian J−1
gθ has a cubic cost. As shown in Bai et al.
(2019), we can instead compute the −∂ℓ"
REFERENCES,0.8533333333333334,"∂z⋆
 
J−1
gθ

z⋆

term of the gradient (which contains the Jaco-
bian) by solving the following linear system:"
REFERENCES,0.8566666666666667,x⊤= −∂ℓ
REFERENCES,0.86,"∂z⋆
 
J−1
gθ

z⋆

= ∂ℓ ∂z⋆"
REFERENCES,0.8633333333333333,"
I −∂fθ(z⋆; A, X) ∂z⋆ −1 =⇒x ="
REFERENCES,0.8666666666666667,"
I −∂fθ(z⋆; A, X) ∂z⋆"
REFERENCES,0.87,⊤!−1 ∂ℓ ∂z⋆ ⊤
REFERENCES,0.8733333333333333,"=⇒

I −∂fθ(z⋆; A, X) ∂z⋆"
REFERENCES,0.8766666666666667,"⊤
x =
 ∂ℓ ∂z⋆ ⊤"
REFERENCES,0.88,"=⇒x =
∂fθ(z⋆; A, X) ∂z⋆"
REFERENCES,0.8833333333333333,"⊤
x +
 ∂ℓ ∂z⋆ ⊤"
REFERENCES,0.8866666666666667,"which in fact is also a ﬁxed point equation and can be solved via some iterative procedure. Note that
the ﬁrst term of the above Equation is a vector-Jacobian product which can be efﬁciently computed
via autograd packages (e. g., PyTorch Paszke et al. (2017)) for any x, without explicitly writing out
the Jacobian matrix. Finally, we can compute
∂ℓ
∂(·) as follows:"
REFERENCES,0.89,"∂ℓ
∂(·) =
∂fθ(z⋆; A, X) ∂(·) ⊤
x"
REFERENCES,0.8933333333333333,"where again this product is itself a vector-Jacobian product, computable via normal automatic dif-
ferentiation packages."
REFERENCES,0.8966666666666666,"E
DATASETS"
REFERENCES,0.9,"We evaluated the proposed model on 10 publicly available graph classiﬁcation datasets including 5
bio/chemo-informatics datasets: MUTAG, D&D, NCI1, PROTEINS and ENZYMES, as well as 5"
REFERENCES,0.9033333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.9066666666666666,Table 2: Summary of the 10 datasets that were used in our experiments.
REFERENCES,0.91,"Dataset
MUTAG
D&D
NCI1
PROTEINS
ENZYMES
IMDB
IMDB
REDDIT
REDDIT
COLLAB
BINARY
MULTI
BINARY
MULTI-5K"
REFERENCES,0.9133333333333333,"Max # vertices
28
5,748
111
620
126
136
89
3,782
3,648
492
Min # vertices
10
30
3
4
2
12
7
6
22
32
Average # vertices
17.93
284.32
29.87
39.05
32.63
19.77
13.00
429.61
508.50
74.49
Max # edges
33
14,267
119
1,049
149
1,249
1,467
4,071
4,783
40,119
Min # edges
10
63
2
5
1
26
12
4
21
60
Average # edges
19.79
715.66
32.30
72.81
62.14
96.53
65.93
497.75
594.87
2,457.34
# labels
7
82
37
3
–
–
–
–
–
–
# attributes
–
–
–
–
18
–
–
–
–
–
# graphs
188
1,178
4,110
1,113
600
1,000
1,500
2,000
4,999
5,000
# classes
2
2
2
2
6
2
3
2
5
3"
REFERENCES,0.9166666666666666,"social interaction datasets: IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI-
5K and COLLAB Kersting et al. (2016). A summary of the 10 datasets is given in Table 2. MU-
TAG consists of 188 mutagenic aromatic and heteroaromatic nitro compounds. The task is to pre-
dict whether or not each chemical compound has mutagenic effect on the Gram-negative bacterium
Salmonella typhimurium Debnath et al. (1991). ENZYMES contains 600 protein tertiary structures
represented as graphs obtained from the BRENDA enzyme database. Each enzyme is a member of
one of the Enzyme Commission top level enzyme classes (EC classes) and the task is to correctly
assign the enzymes to their classes Borgwardt et al. (2005). NCI1 contains more than four thousand
chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer
cell lines Wale et al. (2008). PROTEINS contains proteins represented as graphs where vertices are
secondary structure elements and there is an edge between two vertices if they are neighbors in the
amino-acid sequence or in 3D space. The task is to classify proteins into enzymes and non-enzymes
Borgwardt et al. (2005). D&D contains over a thousand protein structures. Each protein is a graph
whose nodes correspond to amino acids and a pair of amino acids are connected by an edge if they
are less than 6 ˚Angstroms apart. The task is to predict if a protein is an enzyme or not Dobson &
Doig (2003). IMDB-BINARY and IMDB-MULTI were created from IMDb, an online database of
information related to movies and television programs. The graphs contained in the two datasets cor-
respond to movie collaborations. The vertices of each graph represent actors/actresses and two ver-
tices are connected by an edge if the corresponding actors/actresses appear in the same movie. Each
graph is the ego-network of an actor/actress, and the task is to predict which genre an ego-network
belongs to Yanardag & Vishwanathan (2015). REDDIT-BINARY and REDDIT-MULTI-5K contain
graphs that model the social interactions between users of Reddit. Each graph represents an online
discussion thread. Speciﬁcally, each vertex corresponds to a user, and two users are connected by an
edge if one of them responded to at least one of the other’s comments. The task is to classify graphs
into either communities or subreddits Yanardag & Vishwanathan (2015). COLLAB is a scientiﬁc
collaboration dataset that consists of the ego-networks of several researchers from three subﬁelds
of Physics (High Energy Physics, Condensed Matter Physics and Astro Physics). The task is to
determine the subﬁeld of Physics to which the ego-network of each researcher belongs Yanardag &
Vishwanathan (2015)."
REFERENCES,0.92,"We also evaluated the proposed model on two datasets from the Open Graph Benchmark (OGB)
Hu et al. (2020), a collection of large-scale and diverse benchmark datasets for machine learning
on graphs. A summary of the two datasets is given in Table 3. The ogbg-molhiv dataset is
a molecular property prediction dataset that is adopted from the MoleculeNet Wu et al. (2018).
The dataset consists of 41, 127 molecules and corresponds to a binary classiﬁcation dataset where
the task is to predict whether a molecule inhibits HIV virus replication or not. The molecules in
the training, validation and test sets are divided using a scaffold splitting procedure that splits the
molecules based on their two-dimensional structural frameworks. The scaffold splitting attempts to
separate structurally different molecules into different subsets. The ogbg-code2 dataset contains
a large number of Abstract Syntax Trees (ASTs) that are extracted from approximately 450, 000
Python method deﬁnitions. For each method, the AST edges, the AST nodes, and the tokenized
method name are retrieved. Given the body of a method represented by the AST and its node
features, the task (which is known as code summarization) is to predict the sub-tokens forming the
name of the method. The ASTs for the training set are obtained from GitHub projects that do not
appear in the validation and test sets. We refer the reader to Hu et al. (2020) for more details about
the OGB datasets."
REFERENCES,0.9233333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.9266666666666666,Table 3: Statistics of the 2 OGB datasets that we used in our experiments.
REFERENCES,0.93,"Dataset
ogbg-molhiv
ogbg-code2"
REFERENCES,0.9333333333333333,"Average # vertices
25.5
125.2
Average # edges
27.5
124.2
Node features
✓
✓
Edge features
✓
✓
Directed
–
✓
# graphs
41,127
452,741
# tasks
1
1
Split scheme
Scaffold
Project
Split ratio
80/10/10
90/5/5
Task type
Binary class.
Sub-token prediction
Metric
ROC-AUC
F1-score"
REFERENCES,0.9366666666666666,Table 4: Values of λ that we used in our experiments.
REFERENCES,0.94,"MUTAG
D&D
NCI1
PROTEINS
ENZYMES
IMDB
IMDB
REDDIT
REDDIT
COLLAB
BINARY
MULTI
BINARY
MULTI-5K"
REFERENCES,0.9433333333333334,"λ
1/5
1/20
1/20
1/30
1/20
1/200
1/300
1/500
1/400
1/2000"
REFERENCES,0.9466666666666667,"F
PARAMETER λ"
REFERENCES,0.95,"Given an input graph G and a “hidden graph” Gi, since the “hidden graph” is trainable, the maximum
vertex degree of the product graph G× is not known beforehand. However, in case the weights of
the edges of the “hidden graph” are bounded, we can compute an upper bound to that. Let ∆denote
the maximum vertex degree of G, c denote the number of vertices of the “hidden graph” Gi, and b
the maximum edge weight of the “hidden graph”. Then, we have that ∆× ≤∆cb, and therefore,
to guarantee convergence, we need to set λ ≤1/∆cb. In practice, we empirically found that even if
λ takes higher values, the geometric series converges within a small number of iterations. Table 4
shows the value of λ that we used for each dataset."
REFERENCES,0.9533333333333334,"G
RUNTIME ANALYSIS"
REFERENCES,0.9566666666666667,"The proposed model is indeed computationally more expensive than the RWNN model due to the
ﬁxed point iteration which is not parallelizable. However, as already discussed, we empirically
observed that the forward iteration converges in a small number of steps, thus incurring a relatively
small overhead in the model’s running time. We have computed the average running time per epoch
of the proposed model, and 3 of the baselines (2-RWNN, 3-RWNN and GIN) on the 10 graph
classiﬁcation datasets. We use the same values for the common hyperparameters (e. g., number and
size of hidden graphs for GRWNN and RWNN, and hidden dimension size, batch size, etc for all 3
models). The results are shown in Table 5 (in seconds). As we can see, the proposed model is not
much more expensive than the baselines. In fact, in most cases, its average running time per epoch
is 1 −3 times higher than that of the baselines, which is by no means prohibitive for real-world
scenarios."
REFERENCES,0.96,"Table 5: Average running time per epoch (in seconds) of the proposed model and 3 baselines on the
10 graph classiﬁcation datasets."
REFERENCES,0.9633333333333334,"MUTAG
D&D
NCI1
PROTEINS
ENZYMES
IMDB
IMDB
REDDIT
REDDIT
COLLAB
BINARY
MULTI
BINARY
MULTI-5K"
REFERENCES,0.9666666666666667,"GIN
0.03
0.34
0.50
0.14
0.07
0.13
0.19
0.81
2.43
0.98
2-RWNN
0.03
0.19
0.57
0.16
0.08
0.14
0.20
0.43
1.13
0.89
3-RWNN
0.04
0.23
0.76
0.21
0.11
0.18
0.28
0.55
1.42
1.04"
REFERENCES,0.97,"GRWNN
0.07
0.77
0.94
0.32
0.17
0.24
0.34
2.93
6.19
2.69"
REFERENCES,0.9733333333333334,Under review as a conference paper at ICLR 2022
REFERENCES,0.9766666666666667,"20
40
60
80
100
# hidden graphs 45 50 55 60 65 70"
REFERENCES,0.98,accuracy
REFERENCES,0.9833333333333333,"5.0
7.5
10.0
12.5
15.0
17.5
20.0
22.5
25.0
# nodes of hidden graphs 56 58 60 62 64 66 68 70"
REFERENCES,0.9866666666666667,accuracy
REFERENCES,0.99,"Figure 2: Performance on the ENZYMES dataset as a function of the number of “hidden graphs”
(left) and the number of vertices of the “hidden graphs” (right)."
REFERENCES,0.9933333333333333,"H
SENSITIVITY ANALYSIS"
REFERENCES,0.9966666666666667,"The proposed GRWNN model involves two main parameters: (1) the number of “hidden graphs”,
and (2) the number of vertices of “hidden graphs”. We next investigate how these two parameters
inﬂuence the performance of the GRWNN model. Speciﬁcally, in Figure 2, we examine how the
different values of these parameters affect the performance of GRWNN on the ENZYMES dataset.
We observe that the accuracy on the test set increases as the number of “hidden graphs” increases.
The number of “hidden graphs” seems to have a signiﬁcant impact on the performance of the model.
When the number of graphs is set equal to 5, the model achieves an accuracy smaller than 50%,
while when the number of graphs is set equal to 100, it yields an accuracy greater than 65%. On the
other hand, the number of vertices of the “hidden graphs” does not affect that much the performance
of the model."
