Section,Section Appearance Order,Paragraph
HHMI JANELIA RESEARCH CAMPUS,0.0,"1 HHMI Janelia Research Campus
2 Harvard University
3 MIT
{xur,prakhyas,turagas}@janelia.hhmi.org
{albertlin,samuel}@g.harvard.edu
{lumi,shanir}@mit.edu"
ABSTRACT,0.0029069767441860465,ABSTRACT
ABSTRACT,0.005813953488372093,"The availability of both anatomical connectivity and brain-wide neural activity mea-
surements in C. elegans make the worm a promising system for learning detailed,
mechanistic models of an entire nervous system in a data-driven way. However,
one faces several challenges when constructing such a model. We often do not have
direct experimental access to important modeling details such as single-neuron
dynamics and the signs and strengths of the synaptic connectivity. Further, neural
activity can only be measured in a subset of neurons, often indirectly via calcium
imaging, and signiﬁcant trial-to-trial variability has been observed. To address
these challenges, we introduce a connectome-constrained latent variable model
(CC-LVM) of the unobserved voltage dynamics of the entire C. elegans nervous
system and the observed calcium signals. We used the framework of variational
autoencoders to ﬁt parameters of the mechanistic simulation constituting the genera-
tive model of the LVM to calcium imaging observations. A variational approximate
posterior distribution over latent voltage traces for all neurons is efﬁciently inferred
using an inference network, and constrained by a prior distribution given by the
biophysical simulation of neural dynamics. We applied this model to an experimen-
tal whole-brain dataset, and found that connectomic constraints enable our LVM
to predict the activity of neurons whose activity were withheld signiﬁcantly better
than models unconstrained by a connectome. We explored models with different
degrees of biophysical detail, and found that models with realistic conductance-
based synapses provide markedly better predictions than current-based synapses
for this system."
INTRODUCTION,0.00872093023255814,"1
INTRODUCTION"
INTRODUCTION,0.011627906976744186,"The anatomical connectivity of the entire C. elegans nervous system, including both chemical and
electrical synapses, has been known for several decades [30; 27; 31]. However, well-calibrated
and predictive connectome-constrained mechanistic models of this nervous system have yet to be
demonstrated [13; 28; 10; 7]. This is partially because whole-brain recordings are insufﬁcient to com-
pletely constrain computational models. First, the single-neuron and synapse dynamics are generally
unknown. Second, the connectome does not directly inform the signs and strengths of individual
synapses. Third, the response properties of sensory neurons are incompletely known. Further, it is
unclear what level of biophysical detail is necessary to reproduce the essential computational function
of the C. elegans nervous system."
INTRODUCTION,0.014534883720930232,"Here, we used whole-brain calcium imaging data to constrain the missing parameters in a connectome-
constrained, biophysically detailed model of the C. elegans nervous system. We started with a
simpliﬁed non-spiking passive point-neuron model of the voltage dynamics of individual neurons
in the circuit. We modeled inputs to the neurons from both electrical and chemical synapses."
INTRODUCTION,0.01744186046511628,†equal contribution
INTRODUCTION,0.020348837209302327,Published as a conference paper at ICLR 2022
INTRODUCTION,0.023255813953488372,"The chemical synapses are modeled nonlinearly, with either current-based or conductance-based
biophysics. The challenge of ﬁtting such a model to data is two-fold. First, the voltage dynamics
of the neurons are not directly observed, but rather indirectly measured via their slow calcium
dynamics. Second, there is signiﬁcant trial-to-trial variability in neural activity, suggesting strong
initial state dependence in the neural responses to the same sensory stimulus [9]. These issues can
both be addressed by treating the collective voltage signals of all neurons in the nervous system as an
unobserved latent variable whose dynamics are determined by the simpliﬁed connectome-constrained
biophysical model with unknown neuronal and synaptic parameters."
INTRODUCTION,0.02616279069767442,"Our connectome-constrained latent variable model (CC-LVM) of voltage dynamics of the C. elegans
nervous system is a large-scale latent variable model with a very high-dimensional latent space
consisting of voltage dynamics of 300 neurons over 5 minutes of time at the simulation frequency
of 160 Hz. The generative model for these latent variables is described by stochastic differential
equations modeling the nonlinear dynamics of the network activity. We developed a variational
autoencoder-based framework for inferring the unobserved voltage dynamics from the observed
calcium dynamics of only a subset of the neurons in the nervous system."
INTRODUCTION,0.029069767441860465,"The C. elegans nervous system consists of 300 neurons, divided into 118 distinct classes (often
bilaterally symmetric pairs) [26; 14; 25]. These neurons can be categorized into sensory neurons,
interneurons, and motor neurons. The majority of these neurons, about 200, are concentrated in
the head of the animal. The synaptic connectivity of C. elegans has been mapped with electron
microscopy, providing researchers with a complete connectome [30; 31]."
INTRODUCTION,0.03197674418604651,"We applied the CC-LVM to whole-brain calcium imaging data which captured 170 of the 300 neurons
in multiple worms as they responded to chemosensory stimuli [32]. In principle, an accurate model
of the nervous system constrained by incomplete activity measurements but a complete description of
the animal’s anatomical connectivity can enable accurate predictions of activity in neurons which
were not recorded. We tested this hypothesis by using the CC-LVM to predict the activity of neurons
which were measured but whose activity was withheld during model training. We also used the
model to predict the activity of entire worms, by holding out single trials during training. The
CC-LVM predicted activity of withheld neurons and withheld worms signiﬁcantly better than models
unconstrained by the connectome, demonstrating the utility of the connectome even when little is
known about the signs and strengths of individual connections. Further, we found that models with
conductance-based synapses provide superior predictions to models with current-based synapses.
This is surprising because while conductance-based synapses are more mechanistically accurate, they
do not necessarily support efﬁcient inference [8]. CC-LVMs thus provide a new tool for connectome
and activity constrained modeling of neural circuits and for discovering the appropriate level of detail
of biophysical model for a given system. This work provides a framework not only for modeling the
C. elegans nervous system in particular, but also the neuronal networks of other biological systems."
PRIOR WORK,0.03488372093023256,"1.1
PRIOR WORK"
PRIOR WORK,0.0377906976744186,"Previous work in modeling the C. elegans nervous system has focused on creating network simulations
based on anatomical connectome data with unknown single neuron synaptic biophysics [18; 7; 28; 19;
13]. These network models provide a holistic view of the entire nervous system and were validated
by comparing simulated locomotion with movements of live animals [11]. However, they were
not able ﬁt against prerecorded calcium ﬂuorescence data, so their simulated neuronal activities
were not conﬁrmed. Work has also been done on modeling unlabeled neuron populations but these
models cannot predict activity at a single neuron resolution because the activity data is not mapped
to speciﬁc cells [4]. Recent advances in machine learning have enabled increasingly sophisticated
latent variable models that uncover structure from data generated by advanced neural interfacing
technologies [21]. This framework has been used in other contexts to infer neuronal voltage dynamics
from high-dimensional calcium ﬂuorescence recordings [20; 1; 24]. Despite their utility, these models
do not account for connectomic data. Our work employs both LVMs and connectomic constraints to
create a model that is informed both by neural population dynamics and anatomical network structure."
PRIOR WORK,0.040697674418604654,"Recently, Bayesian models have been widely applied in neuronal datasets to infer variables such as
spiking activity, neural dynamics, and connectivity [28; 1; 24]. In particular, Linderman et al. [17]
created a hierarchical state space model of C. elegans neural activity. However, because such a model
is not mechanistic, it cannot predict the activity of neurons that were never measured. Warrington"
PRIOR WORK,0.0436046511627907,Published as a conference paper at ICLR 2022
PRIOR WORK,0.046511627906976744,(e) Trial-to-trial variability in measured calcium fluorescence trace
PRIOR WORK,0.04941860465116279,(d) Labeled whole-brain calcium imaging
PRIOR WORK,0.05232558139534884,"J. White, et al. 1986"
PRIOR WORK,0.055232558139534885,"~170 neurons recorded
5 min recording time
4 Hz recording rate
21 experiment trials"
PRIOR WORK,0.05813953488372093,"sensory neuron
interneuron
motor neuron
recorded neuron
unrecorded neuron
chemical synapse
electrical synapse"
PRIOR WORK,0.061046511627906974,"E. Yemini, et al. 2021"
-BUTANONE,0.06395348837209303,"2-butanone
2,3-pentanedione
NaCl ΔF/F V"
-BUTANONE,0.06686046511627906,"1
2
3
4
T (min)"
-BUTANONE,0.06976744186046512,"1
2
3
4
T (min)"
-BUTANONE,0.07267441860465117,(f) Recorded calcium dynamics
-BUTANONE,0.0755813953488372,(g) Inferred latent voltage dynamics } }
-BUTANONE,0.07848837209302326,"unrecorded
   neurons"
-BUTANONE,0.08139534883720931,"unrecorded
   neurons"
-BUTANONE,0.08430232558139535,(c) C. elegans connectome ...
-BUTANONE,0.0872093023255814,(b) Inference network
-BUTANONE,0.09011627906976744,1D convolution layers + upsampling layers
-BUTANONE,0.09302325581395349,"...
v(0)"
-BUTANONE,0.09593023255813954,(a) Generative model ...
-BUTANONE,0.09883720930232558,"...
chemosensory
     stimulus"
-BUTANONE,0.10174418604651163,"voltage 
dynamics"
-BUTANONE,0.10465116279069768,"fluorescence 
      trace C"
-BUTANONE,0.10755813953488372,"connectome
 strength f(∆t) v(∆t) o(∆t)"
-BUTANONE,0.11046511627906977,f(2∆t)
-BUTANONE,0.11337209302325581,v(2∆t)
-BUTANONE,0.11627906976744186,o(2∆t)
-BUTANONE,0.11918604651162791,f(r∆t)
-BUTANONE,0.12209302325581395,v(r∆t)
-BUTANONE,0.125,o(r∆t)
-BUTANONE,0.12790697674418605,"f(t)
f(t+r∆t)"
-BUTANONE,0.1308139534883721,"(         ,          )
μ
Σ
(t+r∆t)
(t+r∆t)
(    ,     ) 
μ Σ
(t)
(t)
(        ,         )
μ
Σ
(t+∆t)
(t+∆t)
..."
-BUTANONE,0.13372093023255813,"o(t)
o(t+r∆t)"
-BUTANONE,0.13662790697674418,"v(t+r∆t)
^
v(t+∆t)
^
v(t)
^"
-BUTANONE,0.13953488372093023,"Pθ(f,v|o)"
-BUTANONE,0.14244186046511628,"Qφ(v|f,o) ... ..."
-BUTANONE,0.14534883720930233,sample
-BUTANONE,0.14825581395348839,"BAG: sensory neuron
AVA: interneuron
DB02: motor neuron"
-BUTANONE,0.1511627906976744,"Figure 1: Anatomical and functional whole-brain datasets available in C. elegans. a. The connectome-
constrained generative model produces a prior distribution of voltage dynamics and ﬂuorescence traces given
chemosensory stimulus. Observed data is highlighted in color and r is the upsampling factor from the acquisition
rate to the simulation rate. b. The inference network generates a posterior distribution of voltage dynamics
given observed ﬂuorescence traces and chemosensory stimuli. c. The connectome of C.elegans nervous system,
with chemical connections in blue and electrical connections in orange [30]. Neurons recorded in the functional
imaging dataset [32] are colored green. d. Labeled whole-brain calcium imaging of 170 neurons in the head of
the worm while a panel of chemosensory stimuli were presented [32]. Deterministic multicolor labels allow for
the identiﬁcation of all neurons. e. Sample ﬂuorescence activity traces of three measured neurons, with stimulus
delivery marked by the colored bars. Note the signiﬁcant trial-to-trial variability in neural activity."
-BUTANONE,0.15406976744186046,"et al. [28] used sequential Monte Carlo (SMC) to impute the intracellular voltage potentials of C.
elegans neurons from 49 recorded calcium traces [12]. Their work combined neuronal, body and
calcium observation simulators in order to model locomotion [3]. Their simulator produced a series
of exemplar neuronal voltage traces, but they did not test their model’s ability to predict the activity
of unrecorded neurons, so the accuracy of their inferences remains unknown."
-BUTANONE,0.1569767441860465,"Our model uses a variational auto-encoder (VAE) to perform inference instead of SMC which is
more computationally efﬁcient because it allows direct sampling from the approximate posterior. The
CC-LVM was trained on activity recordings of 170 neurons [32], a large fraction of the 300 total
neurons in the C. elegans nervous system [29; 31]. We validated our model by evaluating its ability
to predict the activity of neurons held out from the training data. Finally, we searched a space of
generative models and optimized voltage predictions by comparing them to calcium activity data."
CONNECTOME-CONSTRAINED LATENT VARIABLE MODEL,0.15988372093023256,"2
CONNECTOME-CONSTRAINED LATENT VARIABLE MODEL"
CONNECTOME-CONSTRAINED LATENT VARIABLE MODEL,0.16279069767441862,"We constructed a connectome-constrained latent variable model (CC-LVM) of the C. elegans nervous
system, where each node in our network represents a speciﬁc neuron in the animal. The activity
of each neuron was modeled with a latent variable analogous to voltage. Since the neurons in C.
elegans do not spike [2], the dynamics of each neuron in the network is represented by a stochastic
non-spiking passive leaky integrator equation with learned time constants and resting membrane
potentials [5]. The neurons were coupled by both chemical and electrical synapses with learned
weights [5]. We allowed these weights to be non-zero only where the connectome indicates the
existence of synapses. Given a set of learned parameters (weights, time constants, and resting
membrane potentials), the dynamics of the network deﬁne a prior distribution over neural activity
trajectories. Importantly, the stochastic nature of the neuron voltages causes deviations from perfectly
deterministic dynamics, allowing us to model the observed variability in single-neuron dynamics.
This variability has several potential sources, including the unmeasured initial states of the neurons
and our incomplete knowledge of the sensory inputs driving the nervous system."
CONNECTOME-CONSTRAINED LATENT VARIABLE MODEL,0.16569767441860464,"A latent variable model of this scale with a nonlinear generative model deﬁned by the stochastic
dynamics is difﬁcult to ﬁt because of the high number of parameters. To address this challenge,
we used the probabilistic inference framework of variational autoencoders (VAE) [22] to train a
black-box voltage inference network to predict a posterior distribution over neural activity trajectories.
We then used this inference network to train the parameters of the CC-LVM. The resulting LVM has
a biologically realistic generative model of the nonlinear neural dynamics of the C. elegans nervous
system, and a black-box temporal convolutional inference network which, given sensory stimulus"
CONNECTOME-CONSTRAINED LATENT VARIABLE MODEL,0.1686046511627907,Published as a conference paper at ICLR 2022
CONNECTOME-CONSTRAINED LATENT VARIABLE MODEL,0.17151162790697674,"and calcium imaging measurements, predicts a factorized Gaussian distribution over the voltages
of all the neurons in the network. Several variants of the LVM were developed: we tested models
in which the synaptic connections were modeled as either current-based or conductance-based [5],
and evaluated different types of connectome constraint. The LVM was optimized with the ELBO
(evidence lower bound) objective."
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.1744186046511628,"2.1
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.17732558139534885,"Neurons in the C. elegans nervous system are largely non-spiking [2], so we model the voltage
dynamics for these neurons as a passive point neurons with a single electrical compartment. Let
v ∈RN denote the voltages of the N neurons. The voltage vi(t) for each post-synaptic neuron i at
the time t was calculated using a ﬁrst-order leaky integrator equation given by"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.18023255813953487,"τi ˙vi(t) + vi(t) = sc
i(t) + se
i(t) + vrest
i
+ oi(t),
(1)"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.18313953488372092,"where τi is the voltage time constant, oi is the chemosensory input provided to only the sensory
neurons, sc
i is the chemical synaptic input, se
i is the electrical synaptic input, vrest
i
is the resting
neuron voltage."
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.18604651162790697,"We studied two variations of the model, the current-based model and the conductance-based model,
which differ in their formulations of chemical synaptic input sc
i. Since neurons in the C. elegans
nervous system are largely non-spiking, we model the chemical synapses as having graded release of
neurotransmitter, rather than the all-or-none quantal release seen in spiking neurons. In both models,
we model the amount of neurotransmitter released, W c
jig(vj(t)), in proportion to the pre-synaptic
voltage vj followed by a softplus activation g(·) which sets a minimum voltage below which there
is no synaptic release. We use g(·), to denote a softplus function for the rest of the paper. In our
current-based model, synaptic input sc
i to a post-synaptic neuron i is directly proportional pre-synaptic
neurotransmitter concentration:"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.18895348837209303,"sc
i(t) = N
X"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.19186046511627908,"j
W c
jig(vj(t)),
(2)"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.19476744186046513,"where W c
ji represents the chemical synaptic weight between pre-synaptic neuron j and post-synaptic
neuron i. W c
ji can be positive or negative depending on if the synaptic connection is excitatory or
inhibitory. If neurons j and i are not connected, W c
ji is set to zero. In the conductance-based model,
we model the synaptic current entering the post-synaptic neuron with more biophysical detail as"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.19767441860465115,"sc
i(t) = N
X"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.2005813953488372,"j
(Eji −vi(t))W c
jig(vj(t)).
(3)"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.20348837209302326,"Here, the pre-synaptic neurotransmitter concentration W c
jig(vj(t)) is more accurately modeled as
proportional to the conductance at the post-synaptic terminal. The post-synaptic current is then given
by the product of the synaptic conductance and the difference between the post-synaptic voltage vi(t)
and the synaptic reversal potential Eji."
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.2063953488372093,"In contrast to the current-based synapse, whose input is independent of the post-synaptic voltage vi(t),
the more biophysically accurate conductance-based synapse model also has a dependence on the
post-synaptic voltage. Additionally, this model decouples the sign of the synapse from the strength.
The reversal potential of a synapse Eji dictates whether a synapse is excitatory or inhibitory. A large
and positive Eji corresponds to an excitatory synapse causing depolarization of the postsynaptic
neuron. Conversely, an inhibitory synapse will have a negative Eji causing hyperpolarization. In this
model, we can now independently train the sign of a synapse and its non-negative strength W c
ji, which
is not easily possible with current-based synapses. In both the current-based and conductance-based
models, the following equation was used to represent electrical synaptic inputs:"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.20930232558139536,"se
i(t) = N
X"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.21220930232558138,"j
W e
ji(vj(t) −vi(t)),
(4)"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.21511627906976744,"where W e
ji is restricted to be non-negative and vj −vi is the potential difference between presynaptic
and postsynaptic neurons. We also restrict W e
ji = W e
ij because the potential differences between
electrical synapses are symmetric. To directly compare the outputs of the LVM to neural activity"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.2180232558139535,Published as a conference paper at ICLR 2022
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.22093023255813954,"measurements, our model must generate calcium signals from the voltage traces. We model the
calcium concentration [Ca]i of each neuron i as a ﬁrst-order leaky integrator, driven by voltage-gated
calcium channels with the same nonlinear current-voltage (I-V) function g(vi):"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.2238372093023256,"τ[Ca][ ˙Ca]i(t) + [Ca]i(t) = g(vi(t)),
(5)"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.22674418604651161,"where g(·) represents softplus activation, τ[Ca] is a time constant shared across all neurons. We
then map calcium concentration [Ca] into the measured calcium ﬂuorescence signals f via an afﬁne
transform with scalar αf and bias βf,"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.22965116279069767,"fi(t) = αf[Ca]i(t) + βf + σfϵf
i (t),
(6)"
NETWORK MODEL WITH PASSIVE POINT-NEURON VOLTAGE DYNAMICS,0.23255813953488372,"with measurement noise represented by a noise amplitude σf and a noise term ϵf
i (t) ∼N(0, 1).These
equations are simulated in discrete time using Euler integration."
VARIATIONAL INFERENCE,0.23546511627906977,"2.2
VARIATIONAL INFERENCE"
VARIATIONAL INFERENCE,0.23837209302325582,"Note that voltage dynamics in equation 1 is a deterministic equation. We provide further ﬂexibility to
the model in order to model the signiﬁcant trial-to-trial variability we observed in Figure 1c. Such
variability could be caused by different initial state of neurons, or by unknown, unmeasured inputs
from environment. In order to address this, we allow for the voltage dynamics to deviate from the
deterministic dynamics, and introduce a Gaussian prior over these deviations. This results in the
following stochastic version of the voltage dynamics,"
VARIATIONAL INFERENCE,0.24127906976744187,"τi ˙vi(t) + vi(t) = sc
i(t) + se
i(t) + vrest
i
+ oi(t) + σv
i ϵv
i (t),
(7)"
VARIATIONAL INFERENCE,0.2441860465116279,"Where σv
i is the noise amplitude, and the standard normal noise term is ϵv
i ∼N(0, 1). With this
approach, we re-formulated the neuronal voltage as a latent variable v with the generative model
given by 7 and must perform Bayesian inference to estimate their value. Exact Bayesian inference
is intractable for this high-dimensional nonlinear stochastic dynamical system. For this reason, we
developed a computationally efﬁcient strategy for inferring an approximate posterior distribution
P(v|f) over the latent variable v, given the measured ﬂuorescence data f using variational Bayes in
the framework of the variational autoencoder [15], see derivation in Appendix A.1."
INFERENCE NETWORK,0.24709302325581395,"2.3
INFERENCE NETWORK"
INFERENCE NETWORK,0.25,"The inference network Qφ(v|f, o) generates the approximate posterior distribution of the latent
variable v (voltage for all neurons at all time points) given the measured ﬂuorescence data f and the
sensory inputs o. It consists of multiple 1D convolutional layers that transform the input data into
a Gaussian distributions vi(t) ∼N(µi(t), σi(t)) for each neuron i = 1, ..., N at every time point
t = 0, ..., D. The approximate posterior distribution Qφ(v|f, o) is factorized as"
INFERENCE NETWORK,0.25290697674418605,"Qφ(v|f, o) =
Y"
INFERENCE NETWORK,0.2558139534883721,"t=0,...,D
Qφ (v(t)|f(0), .., f(D), o(0), .., o(D)) ,
(8)"
INFERENCE NETWORK,0.25872093023255816,"D is the length of the time window of data fed into the inference network, and φ is its parameters."
GENERATIVE MODEL,0.2616279069767442,"2.4
GENERATIVE MODEL"
GENERATIVE MODEL,0.26453488372093026,"The generative model outputs the calcium ﬂuorescence trace f and neuronal voltage v given sensory
inputs o. It can be formulated as"
GENERATIVE MODEL,0.26744186046511625,"Pθ(f, v|o) = Pθ(f|v, o)Pθ(v|o),
(9)"
GENERATIVE MODEL,0.2703488372093023,"where θ contains parameters of the generative model. Pθ(v|o) is a biophysically realistic connectome-
constrained network with passive point-neuron dynamics. It outputs a prior distribution over voltage
v given sensory input o. We use a nonlinear sensory mapping H to transform our chemosensory
input to neuronal stimulus to the sensory neurons. The stimuli is represented as a set of binary vectors
hk(t), which are 1 given the presence of stimulus k at time t and 0 otherwise. Pθ(f|v, o) is another
realistic model that maps the voltage v into reconstructed ﬂuorescence f, given sensory inputs o.
The voltages v are sampled from the approximate posterior distributions generated by the inference
network. Details of both distributions are described in Appendix A.2."
GENERATIVE MODEL,0.27325581395348836,Published as a conference paper at ICLR 2022
GENERATIVE MODEL,0.2761627906976744,"model
current-based
conductance-based"
GENERATIVE MODEL,0.27906976744186046,"weightparameter learned
dense
learned
sparse
conne.
sparsity"
GENERATIVE MODEL,0.2819767441860465,"learned
dense
learned
sparse
learned
total count
conne.
sparsity
conne.
count1
conne.
count2"
GENERATIVE MODEL,0.28488372093023256,"W c
ji"
GENERATIVE MODEL,0.2877906976744186,"range
(−∞, ∞)(−∞, ∞)(−∞, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
T c
ji
1
1
1
1[Cc
ji > 0]
1
1
1[Cc
ji > 0]1[Cc
ji > 0]1[Cc
ji > 0]
M c
ji
✓
✓
✓
✓
✓
✓
✓
Cc
ji
Cc
ji
αc
0.01
0.01
0.01
0.01
0.01
0.01
0.002
✓
✓"
GENERATIVE MODEL,0.29069767441860467,"W e
ji"
GENERATIVE MODEL,0.2936046511627907,"range
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
[0, ∞)
T e
ji
1
1
1
1[Ce
ji > 0]
1
1
1[Ce
ji > 0]1[Ce
ji > 0]1[Ce
ji > 0]
M e
ji
✓
✓
✓
✓
✓
✓
✓
Ce
ji
Ce
ji
αe
0.01
0.01
0.01
0.01
0.01
0.01
0.065
✓
✓
Table 1: Deﬁnitions and constraints for the LVM variants. We evaluated a total of 9 model variants. Across
these variants, we explored two methods of modeling chemical synapses: current-based and conductance-based
(Sec. 2.1). We also tested several different levels of connectome constraint. (Sec. 2.6). For both the chemical
synapse W c
ji and electrical synapses W e
ji, the weight matrix Wji = αTjiMji, where Tji is the sparsity matrix,
Mji is the matrix of connection magnitudes, and α is a global scalar. Trainable parameters are indicated by a ✓.
We compared the connectome-constrained versions to two unconstrained learned models, a learned dense model
in which all neurons are connected, a learned sparse model using a L1 regularization ∥M∥1, and a learned total
count model with a L1 regularization with connectome synapse count C using ∥∥M∥1 −∥C∥1∥2
2."
OBJECTIVE FUNCTION FOR GENERATIVE MODEL AND INFERENCE NETWORK,0.29651162790697677,"2.5
OBJECTIVE FUNCTION FOR GENERATIVE MODEL AND INFERENCE NETWORK"
OBJECTIVE FUNCTION FOR GENERATIVE MODEL AND INFERENCE NETWORK,0.29941860465116277,"The variational objective of our latent variable model uses the ELBO.The reconstructed calcium
ﬂuorescence f is then ﬁt to the measured ﬂuorescence data by maximizing the ELBO for each
measured time point. The objective function is a combination of reconstruction loss between the
reconstructed and measured ﬂuorescence traces and the KL divergence between voltage prior and
posterior. (details in Appendix A.4, A.5):"
OBJECTIVE FUNCTION FOR GENERATIVE MODEL AND INFERENCE NETWORK,0.3023255813953488,"L = −Ev∼Q(v|f,o) [log (Pθ (f|v, o))] + DKL (Qφ(v|f, o)∥Pθ(v|o)) ,
(10)"
OBJECTIVE FUNCTION FOR GENERATIVE MODEL AND INFERENCE NETWORK,0.30523255813953487,"Optimization of the KL divergence with the reconstruction loss allows our inference network to learn
the time dynamics of our neuronal voltages because the difference between the posterior and prior
distributions is made small."
CONNECTOME CONSTRAINT FOR MODEL WEIGHTS,0.3081395348837209,"2.6
CONNECTOME CONSTRAINT FOR MODEL WEIGHTS"
CONNECTOME CONSTRAINT FOR MODEL WEIGHTS,0.311046511627907,"To build the model using chemical and electrical synapse count, Cc and Ce, as a connectome
constraint, we factorized the chemical synaptic weight W c
ji as W c
ji = αcT c
jiM c
ji, where αc represents
a global scaling factor which scales synapse counts to per-synapse currents or conductances depending
on the model class. T c
ji is a binary value which indicates the connectivity, and M c
ji reﬂects the
magnitude of connection strength. The electrical synaptic weights matrix W e
ji is similarly deﬁned
as W e
ji = αeT e
jiM e
ji. We compared the performance of the connectome constrained models to
unconstrained ones (Table 1, Appendix A.6). The anatomical connectome can be represented
mathematically as two N × N matrices: one containing the chemical synapse counts (Cc), and one
containing the sizes of electrical synapses (Ce). We designed the connectome sparsity constraint
by ﬁxing the sparsity of our model to that of the connectome. For chemical connections, the model
sparsity T c
ji = 1[Cc
ji > 0], and likewise for electrical connections, T e
ji = 1[Ce
ji > 0]. For both
chemical and electrical connections, the magnitudes M c
ji and M e
ji are trainable."
CONNECTOME CONSTRAINT FOR MODEL WEIGHTS,0.313953488372093,"We applied the connectome sparsity constraint to both current and conductance-based versions of the
LVM. In the conductance-based model, we were able1 to apply the even stronger connectome count
constraint: directly assigning the synapse counts to the chemical connection magnitudes (M c
ji = Cc
ji)
and the synapse sizes to the electrical connection magnitudes (M e
ji = Ce
ji). Under this constraint,
both the sparsity and magnitude are ﬁxed, and only the global scaling factors αc and αe are trainable.
Additionally, we evaluated another variant of this constraint, connectome count2. This constraint uses
parameters of connectome count1 to initialize M c
ji. M c
ji is then allowed to deviate from Cc
ji during
re-training. We compared our connectome-constrained models to two unconstrained models. The
learned dense model used fully connected sparsity matrices T c and T e, with the magnitudes M c and
M e as trainable parameters. The learned sparse model uses L1 regularization on both magnitude
matrices: ∥M e∥1 and ∥M c∥1. The learned total count model preserves the total synapse count for"
CONNECTOME CONSTRAINT FOR MODEL WEIGHTS,0.3168604651162791,"1Since in the current-based model, the range of W c
ji is (−∞, ∞), and because the signs of the synaptic
connections are unknown, we could not directly apply the connectome count constraint to it."
CONNECTOME CONSTRAINT FOR MODEL WEIGHTS,0.31976744186046513,Published as a conference paper at ICLR 2022
CONNECTOME CONSTRAINT FOR MODEL WEIGHTS,0.3226744186046512,"both magnitude matrices: ∥∥M c∥1 −∥Cc∥1∥2
2 and ∥∥M e∥1 −∥Ce∥1∥2
2. αc and αe are 0.002 and
0.065 respectively, which is the same in the conne.count1 model. Thus, it regularizes the sum of
elements in M c and M e equal to that of Cc and Ce."
EXPERIMENTS,0.32558139534883723,"3
EXPERIMENTS"
DATASET,0.32848837209302323,"3.1
DATASET"
DATASET,0.3313953488372093,"We applied the CC-LVM to a calcium imaging dataset in which immobilized, pan-neuronally labeled
C.elegans were presented with a panel of chemosensory stimuli (2-butanone, 2,3-pentanedione, and
NaCl) [32]. The activity of 170 neurons in the head of the animal was measured across 21 individuals.
Each recording lasts for 5 min, with an acquisition rate of 4 Hz. In this dataset, every neuron in the
worm brain was identiﬁed in vivo using a deterministic multicolor landmark, allowing whole brain
dynamics to be mapped onto the anatomical connectome with single-cell resolution for the ﬁrst time.
The connectome constraints we applied utilized the anatomical connectivity data from [30]. The
complete hermaphrodite C.elegans was reconstructed using electron microscopy, and the connections
between its 300 neurons were mapped. The connectome contains 3464 chemical synapses and 1031
electrical synapses. More details can be found in appendix A.6."
NEURON HOLDOUT EVALUATION,0.33430232558139533,"3.2
NEURON HOLDOUT EVALUATION"
NEURON HOLDOUT EVALUATION,0.3372093023255814,"We hypothesized that the CC-LVM which incorporates the complete connectome but has access only
to the measured activity of a subset of the neurons may nevertheless be able to accurately predict the
activity of unmeasured neurons. This would constitute experimentally testable predictions from our
mechanistic model. We tested this hypothesis by performing neuron holdout evaluations, withholding
a single bilateral pair of measured neurons from the model during both training and testing. Note that
our latent variable model always infers the voltages of all neurons regardless of how many neurons
are observed in any experiment, since the latent states are the voltages of the entire nervous system.
The goal of this evaluation is to test the model’s ability to predict the neural activity of neurons which
were never observed at any time. We ﬁt a version of the model with a pair of neurons withheld,
and then compare the model predictions for the same pair of neurons with the ground truth calcium
ﬂuorescence measurements. We repeated this evaluation for all 107 measured neuron pairs. Because
many of the neurons in C. elegans are bilaterally symmetric, the symmetric pairs tend to have high
correlations with each other. In order to prevent the model from predicting one neuron solely based
on its sibling, we removed the symmetric neurons in groups of two. For every neuron pair, we
performed the neuron holdout evaluation for both current and conductance models with different
levels of connectome constraints. For these experiments, we calculated the correlation coefﬁcient
(details in Appendix A.4) between the predicted and measured calcium ﬂuorescence magnitudes to
quantify the predictive performance (Figure 2a and Table 2, more predicted traces in Appendix A.9).
For each evaluation, we trained 4 models with random initialization, and reported both the mean
value and standard error (SE) for each model. We found connectome sparsity and connectome count
constrained models were better at predicting traces of held-out neurons than unconstrained models."
WORM HOLDOUT EVALUATION,0.34011627906976744,"3.3
WORM HOLDOUT EVALUATION"
WORM HOLDOUT EVALUATION,0.3430232558139535,"Another evaluation method we performed was to hold out the data from a handful of individual
worms, train the model on the remaining worms, and predict the activity of the neurons of the held-out
individuals. For each of the 9 model variants, we trained on 15 worms, and tested the model on 6
withheld worms. In each case, we trained 4 models with different random initialization. For the worm
holdout traces, we calculated the correlation coefﬁcient between the predicted and measured calcium
ﬂuorescence. We also reported ELBO, reconstruction loss and KL divergence (Appendix A.4) on
tested worms to represent the ﬁtting performance. We reported the mean and standard error (SE) from
models trained with 4 different random initializations. We found that models using the connectome
count constraints performed better at predicting calcium traces from held out worms in Table 2."
WORM HOLDOUT EVALUATION,0.34593023255813954,Published as a conference paper at ICLR 2022
WORM HOLDOUT EVALUATION,0.3488372093023256,"model
current-based
conductance-based"
WORM HOLDOUT EVALUATION,0.35174418604651164,"holdout
metric
learned
dense
learned
sparse
conne.
sparsity"
WORM HOLDOUT EVALUATION,0.3546511627906977,"learned
dense
learned
sparse
learned
total count
conne.
sparsity
conne.
count1 conne.
count2"
WORM HOLDOUT EVALUATION,0.35755813953488375,"neuron
corr ↑mean
-0.104
0.001
0.087
0.081
0.008
-0.033
0.262
0.318
0.319
SE
±0.013 ±0.010 ±0.021 ±0.012 ±0.011
±0.011 ±0.016 ±0.016 ±0.016
mean
0.465
0.439
0.440
0.457
0.445
0.470
0.463
0.474
0.468
corr ↑
SE
±0.006 ±0.007 ±0.006 ±0.006 ±0.006
±0.006 ±0.006 ±0.006 ±0.006"
WORM HOLDOUT EVALUATION,0.36046511627906974,"ELBO↑mean
-0.772
-0.826
-0.803
-0.849
-0.862
-0.931
-0.898
-0.854
-0.697
worm
SE
±0.037 ±0.036 ±0.036 ±0.033 ±0.035
±0.033 ±0.032 ±0.036 ±0.066"
WORM HOLDOUT EVALUATION,0.3633720930232558,"recon ↓mean
0.745
0.765
0.754
0.771
0.780
0.805
0.082
0.744
0.636
SE
±0.037 ±0.037 ±0.035 ±0.033 ±0.035
±0.032 ±0.031 ±0.035 ±0.065"
WORM HOLDOUT EVALUATION,0.36627906976744184,"KLD ↓mean
0.028
0.061
0.049
0.078
0.082
0.126
0.077
0.110
0.061
SE
±0.001 ±0.003 ±0.002 ±0.003 ±0.002
±0.005 ±0.003 ±0.006 ±0.002
Table 2: Holdout evaluation results. We performed neuron holdout evaluations on each of the 107 measured
neuron pairs in the dataset, and report the average correlation coefﬁcient between the predicted and measured
ﬂuorescence traces (Sec. 3.2). We also performed worm holdout evaluations, witholding 6 of the 21 individuals
from the training (Sec. 3.3). For the worm holdouts, we report several metrics: correlation coefﬁcient, ELBO,
reconstruction loss, and KL divergence. Overall, the conductance-based model under connectome count
constraint produced the best predictions among the 9 models."
COMPARING THE DIFFERENT MODEL VARIANTS,0.3691860465116279,"3.4
COMPARING THE DIFFERENT MODEL VARIANTS"
COMPARING THE DIFFERENT MODEL VARIANTS,0.37209302325581395,"The gold standard for comparing generative models in machine learning is to compare the model
evidence P(f) on a test dataset. However, this is intractable for many generative models, and so the
evidence lower bound (ELBO) is used for model ﬁtting and sometimes also for model comparison.
However, the ELBO is not a tight lower bound, and so is not guaranteed to provide the same ordering
over models as the model evidence P(f). In this paper, exploiting the interpretable nature of our
mechanistic generative model, we developed a stricter criterion for comparing models based on
the neuron holdout evaluation. Neuron holdout predictive performance is biologically meaningful,
as it evaluates the ability of a model to make testable predictions about neurons which were not
directly measured. We found that overall, the conductance-based CC-LVMs under connectome count
constraint make the best neuron holdout predictions as quantiﬁed by the correlation coefﬁcient. We
found that model accuracies at neuron holdout prediction are not very tightly correlated with their
test ELBO. While connectome count2 still achieves the best performance, the learned dense model
achieves the second best performance, despite performing worst in the neuron holdout evaluation.
This suggests that the learned dense model has potentially discovered an equally accurate but
mechanistically incorrect way to model the calcium imaging data."
COMPARING THE DIFFERENT MODEL VARIANTS,0.375,"Stronger connectome constraint improves the neuron holdout predictions. We found that our
CC-LVM with the connectome count constraint produced signiﬁcantly better predictions than CC-
LVM with the connectome sparsity constraint, and both connectome constrained models outperformed
unconstrained LVMs with the same dynamics (Table 2). This suggests that the neuron activity pre-
dictions are improved by a stronger connectome constraint. Note that connectome count2 achieves
slightly better performance than connectome count1, as its parameters get initialized from connec-
tome count1, then are allowed to deviate during re-training. We also explored whether CC-LVMs
achieve better performance due solely to the smaller number of trainable parameters. Comparing
the connectome-constrained models to the learned sparse LVM with sparsity regularization, we
found that the learned sparse model performed worse. This indicates that the connectome constraint
improves predictability due to its topology, not due to the number of trainable parameters."
COMPARING THE DIFFERENT MODEL VARIANTS,0.37790697674418605,"Perhaps relatedly, we found superior predictions for interneurons and motor neurons, compared to
sensory neurons. While the inputs to motor neurons and especially interneurons are largely contained
in the connectome, the inputs to sensory neurons are not as well constrained by the connectome since
they also respond to external stimuli. Instead, we trained a black-box model to predict the tunings
of sensory neurons to experimentally presented odors, but this likely does not fully characterize all
the sensory inputs to the worm. This result additionally suggests the utility of the connectome, for
neurons where the connectome provides information about the dominant sources of input."
COMPARING THE DIFFERENT MODEL VARIANTS,0.3808139534883721,"Conductance-based models outperform current-based models. We found that conductance-based
models achieve better performance in neuron holdout evaluations, despite needing to estimate the
reversal potential E in addition to the non-negative strength of a chemical synapse W c. This could be
for several reasons. The biophysical model of a conductance-based synapse might be a more accurate
description of reality. There are also two technical advantages to this model. First, conductance-based
synapses allow us to make the fullest use of the connectomic data by using the non-negative synapse
counts. Second, separating the optimization of the sign of a synapse from its magnitude might allow"
COMPARING THE DIFFERENT MODEL VARIANTS,0.38372093023255816,Published as a conference paper at ICLR 2022
COMPARING THE DIFFERENT MODEL VARIANTS,0.3866279069767442,"Figure 2: LVM predictive performance across neuron types. a. Violin plots comparing the distribution of
correlation coefﬁcients between predicted and measured neuron activity for the 9 LVM variants. Neurons are
divided into sensory, inter, and motor neurons. b. Measured traces and predictions made by the 9 models for
three selected neurons: sensory neuron BAG, interneuron AVA, and motor neuron DB02."
COMPARING THE DIFFERENT MODEL VARIANTS,0.38953488372093026,"for easier optimization of both. In contrast, the sign and magnitude of a current-based synapse are
coupled in the same parameter, and we have observed strong local minima preventing the efﬁcient
optimization of the sign and the magnitude. Accordingly, we found that conductance-based models
converge to a connectome closer to the counts given by the original data with less variance due to
the removal of the neuron sign parameter (Appendix A.7). We found that a synapse initialized as
excitatory is unlikely to change its sign after optimization. For the same reasons, we are unable to
effectively use the non-negative synapse counts contained in the connectomic data to inform the
magnitude of the current-based synapse while leaving the sign unconstrained."
DISCUSSION,0.39244186046511625,"4
DISCUSSION"
DISCUSSION,0.3953488372093023,"In this work, we proposed a connectome-constrained latent variable model (CC-LVM) of the C.elegans
nervous system. Our latent variable framework enables the estimation of the initial state of the network
dynamics and compensates for modeling imperfections. This ﬂexibility allows us to ﬁt models to
calcium imaging data with signiﬁcant trial-to-trial variability. In contrast to black-box generative
models used in LVMs in neural data modeling [21], we developed a biophysics-based, connectome-
constrained, mechanistically detailed generative model for the neural dynamics. We evaluated
current-based and conductance-based models for synapses. We used connectomic data to constrain
the model to different degrees. Since CC-LVMs enable the prediction of neurons which were not used
to ﬁt the model, we were able to simulate and validate the process of making biologically relevant
and experimentally testable predictions of the activity of unmeasured neurons. We found that the
conductance-based model produces much more accurate predictions in neuron holdout evaluation
and worm holdout evaluation. Knowledge of the connectome strongly constrains in silico predictions
of individual neurons whose activity were not measured. We hope that this mechanistic model can
be used to predict the effects of experimental perturbations, leading to a causal understanding of
neural computation in neural circuits. The research in this paper was enabled by recent advances in
whole-brain calcium imaging in identiﬁed neurons [32]."
DISCUSSION,0.39825581395348836,"While we have only analyzed the ﬁrst datasets to be collected in this manner, we hope that a rich
database of such recordings across a diversity of conditions will lead to achieving the dream of
standard models of the C. elegans nervous system [11] which generalize across different sensory
modalities and behaviors. Rapidly developing whole-brain imaging technologies and advances in
high-throughput connectomics are now making high-dimensional neural activity and anatomical
connectivity data available in other organisms [23; 33; 6; 16]. We hope that this approach will
eventually lead to accurate mechanistic models of these larger, more complex biological networks."
DISCUSSION,0.4011627906976744,Published as a conference paper at ICLR 2022
REPRODUCIBILITY,0.40406976744186046,"5
REPRODUCIBILITY"
REPRODUCIBILITY,0.4069767441860465,"We trained each of our LVMs on 1 Quadro RTX 8000. Training procedures are described in
Appendix A.3, A.4, A.5. We released our software and datasets (https://github.com/
TuragaLab/wormvae) for reproducibility."
ACKNOWLEDGEMENT,0.40988372093023256,"6
ACKNOWLEDGEMENT"
ACKNOWLEDGEMENT,0.4127906976744186,"This work was funded by the Howard Hughes Medical Institute. Lu Mi is supported by Intel and
MathWorks. Albert Lin and Aravinthan Samuel were funded by NSF Physics of Living Systems
(NSF 1806818); NSF Ideas (NSF IOS-1555914); and the NIH (1 U01 NS111697-01). We thank
William Bishop, Roman Vaxenburg, Core Francisco Park, Helena Casademunt for comments on the
manuscript."
REFERENCES,0.41569767441860467,REFERENCES
REFERENCES,0.4186046511627907,"[1] Aitchison, L., Russell, L., Packer, A. M., Yan, J., Castonguay, P., Hausser, M. and Turaga, S. C.
[2017], Model-based bayesian inference of neural activity and connectivity from all-optical
interrogation of a neural circuit, in ‘Advances in Neural Information Processing Systems’,
Vol. 30, Curran Associates, Inc."
REFERENCES,0.42151162790697677,"[2] Bargmann, C. I. [1998], ‘Neurobiology of the Caenorhabditis elegans genome’, Science ."
REFERENCES,0.42441860465116277,"[3] Boyle, J. H., Berri, S. and Cohen, N. [2012], ‘Gait modulation in C. elegans: an integrated
neuromechanical model’, Front. Comput. Neurosci ."
REFERENCES,0.4273255813953488,"[4] Chen, X., Randi, F., Leifer, A. M. and Bialek, W. [2019], ‘Searching for collective behavior in a
small brain’, Phys. Rev. E ."
REFERENCES,0.43023255813953487,"[5] Dayan, P., Abbott, L. F. et al. [2003], ‘Theoretical neuroscience: computational and mathemati-
cal modeling of neural systems’, Journal of Cognitive Neuroscience 15(1), 154–155."
REFERENCES,0.4331395348837209,"[6] Dorkenwald, S., Mckellar, C., Macrina, T., Kemnitz, N., Lee, K., Wu, J., Popovych, S., Bae, A.,
Mitchell, E., Nehoran, B., Jia, Z., Zung, J., Brittain, D., Collman, F., Jordan, C., Silversmith,
W., Baker, C., Deutsch, D., Kumar, S., Burke, A., Gager, J., Hebditch, J., Moore, M., Morejohn,
S., Silverman, B., Willie, K., Willie, R., Murthy, M. and Seung, H. S. [2020], ‘FlyWire: Online
community for whole-brain connectomics’, bioRxiv pp. 1–34."
REFERENCES,0.436046511627907,"[7] Gleeson, P., Lung, D., Grosu, R., Hasani, R. and Larson, S. D. [2018], ‘c302: a multiscale
framework for modelling the nervous system of Caenorhabditis elegans’, Philos Trans R Soc
Lond B Biol Sci ."
REFERENCES,0.438953488372093,"[8] Gonçalves, P. J., Lueckmann, J.-M., Deistler, M., Nonnenmacher, M., Öcal, K., Bassetto, G.,
Chintaluri, C., Podlaski, W. F., Haddad, S. A., Vogels, T. P., Greenberg, D. S. and Macke,
J. H. [2020], ‘Training deep neural density estimators to identify mechanistic models of neural
dynamics’, eLife ."
REFERENCES,0.4418604651162791,"[9] Gordus, A., Pokala, N., Levy, S., Flavell, S. W. and Bargmann, C. I. [2015], ‘Feedback from
network states generates variability in a probabilistic olfactory circuit’, Cell ."
REFERENCES,0.44476744186046513,"[10] Izquierdo, E. J. and Beer, R. D. [2013], ‘Connecting a connectome to behavior: An ensemble of
neuroanatomical models of c. elegans klinotaxis’, PLOS Computational Biology 9(2), 1–20."
REFERENCES,0.4476744186046512,"[11] Izquierdo, E. J. and Beer, R. D. [2016], ‘The whole worm: brain-body-environment models of
C. elegans’, Curr Opin Neurobiol ."
REFERENCES,0.45058139534883723,"[12] Kato, S., Kaplan, H. S., Schrödel, T., Skora, S., Lindsay, T. H., Yemini, E., Lockery, S.
and Manuel, Z. [2015], ‘Global brain dynamics embed the motor command sequence of
Caenorhabditis elegans’, Cell ."
REFERENCES,0.45348837209302323,Published as a conference paper at ICLR 2022
REFERENCES,0.4563953488372093,"[13] Kim, J., Santos, J. A., Alkema, M. J. and Shlizerman, E. [2019], ‘Whole integration of neu-
ral connectomics, dynamics and bio-mechanics for identiﬁcation of behavioral sensorimotor
pathways in Caenorhabditis elegans’, biorXiv preprint biorXiv:1711.01846 ."
REFERENCES,0.45930232558139533,"[14] Kimble, J. and Hirsh, D. [1979], ‘The postembryonic cell lineages of the hermaphrodite and
male gonads in Caenorhabditis elegans’, Dev. Biol. 70(2), 396–417."
REFERENCES,0.4622093023255814,"[15] Kingma, D. P. and Welling, M. [2014], ‘Auto-encoding variational bayes’, arXiv preprint
arXiv:1312.6114 ."
REFERENCES,0.46511627906976744,"[16] Lin, A., Witvliet, D., Hernandez-Nunez, L., Linderman, S. W., Samuel, A. D. T. and Venkat-
achalam, V. [2022], ‘Imaging whole-brain activity to understand behaviour’, Nat Rev Phys
."
REFERENCES,0.4680232558139535,"[17] Linderman, S., Nichols, A., Blei, D., Zimmer, M. and Paninski, L. [2019], ‘Hierarchical
recurrent state space models reveal discrete and continuous dynamics of neural activity in C.
elegans’, biorXiv ."
REFERENCES,0.47093023255813954,"[18] Liu, H., Kim, J. and Shlizerman, E. [2018], ‘Functional connectomics from neural dynamics:
probabilistic graphical models for neuronal network of Caenorhabditis elegans’, Philos Trans R
Soc Lond B Biol Sci ."
REFERENCES,0.4738372093023256,"[19] Olivares, E., Izquierdo, E. J. and Beer, R. D. [2021], ‘A neuromechanical model of multiple
network rhythmic pattern generators for forward locomotion in C. elegans’, Front. Comput.
Neurosci ."
REFERENCES,0.47674418604651164,"[20] Pandarinath, C., O’Shea, D. J., Collins, J., Jozefowicz, R., Kao, J. C., Trautmann, E. M.,
Kaufman, M. T., Ryu, S. I., Hochberg, L. R., Henderson, J. M., Shenoy, K. V., Abbott, L.
and Sussillo, D. [2018], ‘Inferring single-trial neural population dynamics using sequential
auto-encoders’, Nature Methods ."
REFERENCES,0.4796511627906977,"[21] Pei, F., Ye, J., Zoltowski, D., Wu, A., Chowdhury, R. H., Sohn, H., O’Doherty, J. E., Shenoy,
K. V., Kaufman, M. T., Churchland, M., Jazayeri, M., Miller, L. E., Pillow, J., Park, I. M., Dyer,
L. E. and Pandrinath, C. [2021], ‘Neural latents benchmark ’21: Evaluating latent variable
models of neural population activity’, arXiv preprint arXiv:2109.04463 ."
REFERENCES,0.48255813953488375,"[22] Rezende, D. J., Mohamed, S. and Wierstra, D. [2014], ‘Stochastic backpropagation and approx-
imate inference in deep generative models’, arXiv preprint arXiv:1401.4082 ."
REFERENCES,0.48546511627906974,"[23] Scheffer, L. K., Xu, C. S., Januszewski, M., Lu, Z., Takemura, S. Y., Hayworth, K. J., Huang,
G. B., Shinomiya, K., Maitin-Shepard, J., Berg, S., Clements, J., Hubbard, P. M., Katz, W. T.,
Umayam, L., Zhao, T., Ackerman, D., Blakely, T., Bogovic, J., Dolaﬁ, T., Kainmueller, D.,
Kawase, T., Khairy, K. A., Leavitt, L., Li, P. H., Lindsey, L., Neubarth, N., Olbris, D. J., Otsuna,
H., Trautman, E. T., Ito, M., Bates, A. S., Goldammer, J., Wolff, T., Svirskas, R., Schlegel, P.,
Neace, E. R., Knecht, C. J., Alvarado, C. X., Bailey, D. A., Ballinger, S., Borycz, J. A., Canino,
B. S., Cheatham, N., Cook, M., Dreher, M., Duclos, O., Eubanks, B., Fairbanks, K., Finley,
S., Forknall, N., Francis, A., Hopkins, G. P., Joyce, E. M., Kim, S., Kirk, N. A., Kovalyak,
J., Lauchie, S. A., Lohff, A., Maldonado, C., Manley, E. A., McLin, S., Mooney, C., Ndama,
M., Ogundeyi, O., Okeoma, N., Ordish, C., Padilla, N., Patrick, C., Paterson, T., Phillips,
E. E., Phillips, E. M., Rampally, N., Ribeiro, C., Robertson, M. K., Rymer, J. T., Ryan, S. M.,
Sammons, M., Scott, A. K., Scott, A. L., Shinomiya, A., Smith, C., Smith, K., Smith, N. L.,
Sobeski, M. A., Suleiman, A., Swift, J., Takemura, S., Talebi, I., Tarnogorska, D., Tenshaw,
E., Tokhi, T., Walsh, J. J., Yang, T., Horne, J. A., Li, F., Parekh, R., Rivlin, P. K., Jayaraman,
V., Costa, M., Jefferis, G. S., Ito, K., Saalfeld, S., George, R., Meinertzhagen, I. A., Rubin,
G. M., Hess, H. F., Jain, V. and Plaza, S. M. [2020], ‘A connectome and analysis of the adult
drosophila central brain’, eLife 9, 1–74."
REFERENCES,0.4883720930232558,"[24] Speiser, A., Yan, J., Archer, E., Buesing, L., Turaga, S. C. and Macke, J. H. [2017], ‘Fast
amortized inference of neural activity from calcium imaging data with variational autoencoders’,
arXiv preprint arXiv:1711.01846 ."
REFERENCES,0.49127906976744184,"[25] Sulston, J. E. [1983], ‘Neuronal cell lineages in the nematode Caenorhabditis elegans.’, Cold
Spring Harbor Symposia on Quantitative Biology 48 Pt 2, 443–452."
REFERENCES,0.4941860465116279,Published as a conference paper at ICLR 2022
REFERENCES,0.49709302325581395,"[26] Sulston, J. E. and Horvitz, H. R. [1977], ‘Post-embryonic cell lineages of the nematode,
Caenorhabditis elegans’, Dev. Biol. 56(1), 110–156."
REFERENCES,0.5,"[27] Varshney, L. R., Chen, B. L., Paniagua, E., Hall, D. H. and Chklovskii, D. B. [2011], ‘Structural
properties of the Caenorhabditis elegans neuronal network’, PLoS Comput. Biol. 7(2)."
REFERENCES,0.502906976744186,"[28] Warrington, A., Spencer, A. and Wood, F. [2019], ‘The virtual patch clamp: Imputing c. elegans
membrane potentials from calcium imaging’, arXiv preprint arXiv:1907.11075 ."
REFERENCES,0.5058139534883721,"[29] White, J. G., Southgate, E., Thomson, J. N., Brenner, S. et al. [1986], ‘The structure of the
nervous system of the nematode caenorhabditis elegans’, Philos Trans R Soc Lond B Biol Sci
314(1165), 1–340."
REFERENCES,0.5087209302325582,"[30] White, J., Southgate, E., Thomson, J. and Brenner, S. [1986], ‘The structure of the nervous sys-
tem of the nematode Caenorhabditis elegans’, Phils Trans R Soc of Lond B, Biol Sci 314(1165), 1
LP – 340."
REFERENCES,0.5116279069767442,"[31] Witvliet, D., Mulcahy, B., Mitchell, J. K., Meirovitch, Y., Berger, D. K., Wu, Y., Liu, Y., Koh,
W. X., Parvathala, R., Holmyard, D. et al. [2020], ‘Connectomes across development reveal
principles of brain maturation in C. elegans’, bioRxiv ."
REFERENCES,0.5145348837209303,"[32] Yemini, E., Lin, A., Nejatbakhsh, A., Varol, E., Sun, R., Mena, G. E., Samuel, A. D., Paninski,
L., Venkatachalam, V. and Hobert, O. [2019], ‘NeuroPAL: A Neuronal Polychromatic Atlas of
Landmarks for Whole-Brain Imaging in C. elegans’, bioRxiv p. 676312."
REFERENCES,0.5174418604651163,"[33] Zheng, Z., Lauritzen, J. S., Perlman, E., Robinson, C. G., Nichols, M., Milkie, D., Torrens,
O., Price, J., Fisher, C. B., Shariﬁ, N., Calle-Schuler, S. A., Kmecova, L., Ali, I. J., Karsh, B.,
Trautman, E. T., A, B. J., Hanslovsky, P., Jefferis, G. S. X. E., Kazhdan, M., Khairy, K., Saalfeld,
S., Fetter, R. D. and Bock, D. [2018], ‘A Complete Electron Microscopy Volume of the Brain
of Adult Drosophila melanogaster’, Cell 174(3), 730–743.e22."
REFERENCES,0.5203488372093024,Published as a conference paper at ICLR 2022
REFERENCES,0.5232558139534884,"A
APPENDIX"
REFERENCES,0.5261627906976745,"A.1
VARIATIONAL INFERENCE"
REFERENCES,0.5290697674418605,"We used Bayesian inference (VAE) to compute the posterior probability over the latent variables v
(neuron voltages) given the data f (measured calcium ﬂuorescence),"
REFERENCES,0.5319767441860465,P(v|f) = P(f|v)P(v)
REFERENCES,0.5348837209302325,"P(f)
(11)"
REFERENCES,0.5377906976744186,"To solve this problem, we needed to compute the model evidence,"
REFERENCES,0.5406976744186046,"P(f) =
Z
P(f|v)P(v)dv.
(12)"
REFERENCES,0.5436046511627907,"However, computing P(f) is a mathematically intractable problem, so we instead used variational
inference to approximate the value of P(v|f) [15]. Denoting the approximate posterior Q(v|f), we
can formulate evidence lower bound (ELBO) as"
REFERENCES,0.5465116279069767,"log P(f) ≥L = EQ(v|f)[log P(f, v) −log Q(f|v)],
(13)"
REFERENCES,0.5494186046511628,"which tightens the lower bound on log P(f) as L is maximized. This allowed us to improve the
inference network and ﬁnd the posterior such that Q(v|f) ≈P(v|f), since the Kullback-Leibler
(KL) divergence, which measures the distance between the distributions Q and P, is minimized by
maximizing ELBO."
REFERENCES,0.5523255813953488,"A.2
PRIOR AND POSTERIOR DISTRIBUTION AT A DISCRETE TIME STEP"
REFERENCES,0.5552325581395349,"Using Euler discretization with a time step of ∆t, the ﬁrst-order leaky integrator equation in Equation 1
in the network model is"
REFERENCES,0.5581395348837209,"τi(vi(t + ∆t) −vi(t))/∆t + vi(t) = sc
i(t) + se
i(t) + vrest
i
+ oi(t + ∆t),
(14)"
REFERENCES,0.561046511627907,Then the voltage vi(t + ∆t) of neuron i at time step t + ∆t is formulated as
REFERENCES,0.563953488372093,"vi(t + ∆t) = ∆t/τi(sc
i(t) + se
i(t) + vrest
i
+ oi(t + ∆t) −vi(t)) + vi(t),
(15)"
REFERENCES,0.5668604651162791,"This equation indicates that the voltage state vi(t) at the current time step t is determined by a
function over multiple components from the last time step t −∆t, including the past voltage state,
the chemical and electric synaptic inputs, and the sensory input:"
REFERENCES,0.5697674418604651,"vi(t + ∆t) = F(vi(t), sc
i(t), se
i(t), oi(t + ∆t)),
(16)"
REFERENCES,0.5726744186046512,"For the posterior distribution Qφ(v(t)|f(t), o(t)), the inference network outputs µi(t) and σi(t) for
every neuron i at each time step t. The posterior distribution for the voltage generated by the inference
network at each time step is vi(t) ∼N(µi(t), σi(t)), given a sequence of measured ﬂuorescence
data f(0), ..., f(D∆t) and sensory input o(0), ..., o(D∆t) with total number of steps D + 1. We
then used the reparameterization trick to sample the voltage vi(t) from this posterior distribution to
generate a sequence of voltage sample from vi(0), ..., vi(D∆t)."
REFERENCES,0.5755813953488372,"For the prior distribution, we used the voltage samples vi(t) from the posterior distribution at the
last step as the past voltage state in F to calculate the prior distribution Pθ(vi(t + ∆t)|oi(t + ∆t)) at
time step t + ∆t. The prior distribution is deﬁned as N(F(vi(t), sc
i(t), se
i(t), oi(t + ∆t)), σv
i ). σv
i
is the constant standard deviation of the prior distribution that is trainable during optimization. At the
initial time step 0, we deﬁned another trainable parameter µi(0) to represent the mean of the voltage
prior distribution."
REFERENCES,0.5784883720930233,"Similarly, we used Euler integration to discretize the ﬁrst-order leaky integrator equation (Equation
5). The calcium concentration [Ca]i(t + ∆t) of neuron i at time step t + ∆t is given by"
REFERENCES,0.5813953488372093,Published as a conference paper at ICLR 2022
REFERENCES,0.5843023255813954,"Figure 3: Schematic of the connectome-constrained latent variable model (CC-LVM). Sensory inputs are
fed through a biophyiscally realistic connectome-constrained network to generate the voltage prior Pθ(v|o).
To train the model, we ﬁrst used an inference network to infer from the data the approximate voltage posterior
Qφ(v|f). Fluorescence traces are then reconstructed via a generative model Pθ(f|v, o). We then compared the
reconstructed ﬂuorescence traces to the measured traces, and optimized the model with evidence lower bound
(ELBO), with a combination of reconstruction loss and KL divergence."
REFERENCES,0.5872093023255814,"[Ca]i(t + ∆t) = ∆t/τ[Ca](g(vi(t)) −[Ca]i(t)) + [Ca]i(t),
(17)"
REFERENCES,0.5901162790697675,"We used a sequence of voltage samples vi(t), t = 0, ..., D∆t as the inputs to this formula to get a
sequence of calcium concentration [Ca]i(1), ..., [Ca]i(D∆t). The initial concentration is estimated
from the measured ﬂuorescence trace at time step 0 using [Ca]i(0) = (Fi(0) −βf)/αf. Equation 6
in the generative model Pθ generates the output distributions Pθ(f(t)|v(t), o) of ﬂuorescence trace at
time step t as N(αf[Ca]i(t) + βf, σf). The values αf, βf, σf are trainable ﬂuorescence parameters."
REFERENCES,0.5930232558139535,"A.3
MODEL ARCHITECTURE"
REFERENCES,0.5959302325581395,"The calcium ﬂuorescence data is given in a matrix of dimensions (N, T) where N is the total number
of neurons in the connectome. The chemosensory stimuli input is modeled as a matrix of dimensions
(3, T) which consists of 3 one-hot encoding vectors that are 1 when the stimuli is present and 0
otherwise. Each of the 3 vectors represents one of the three chemical stimuli presented: 2-butanone
and 2,3-pentanedione, NaCl."
REFERENCES,0.5988372093023255,"The calcium ﬂuorescence matrix is passed to the input layer of our inference network Q(v|f, o)
which is a 1D convolutional layer. The output of the input layer then gets passed through an
upsampling layer, a ReLU nonlinearity, another 1D convolutional layer, and then another upsampling
layer. The ﬁrst convolutional layer has 2N input channels, 2N output channels and a kernel size
of 11; the second has 2N input channels, 2N output channels and a kernel size of 21. The ﬁrst
upsampling layer upsamples by a factor of 4 and the second upsamples by a factor of 10. These
factors account for the fact that we simulate our network r times faster with upsampling factor r = 40
than the recording rate. The sensory input is passed through a 1D convolutional layer with N input
channels, 2N output channels and a kernel size of 21. This is then passed through a ReLU and then
concatenated to the output of the 2nd convolution on our calcium data. The merged matrix is then
1D convolved again with 2N inputs, 2N outputs and a kernel size of 41. This is one of the ﬁnal
outputs of our inference network and gives a (N, T) matrix that represents the means of our posterior
distribution. To generate the standard deviation of our factorized posterior distribution, we pass our
merged data through another convolutional layer with the same dimensions but different parameters.
We pass that output through a nonlinear afﬁne transform (using a softplus nonlinearity) to rescale the
output to an appropriate level of noise. The parameters of this inference network are all the weights
and biases of the convolutions and upsampling layers described above, which we denote by φ in the
paper."
REFERENCES,0.6017441860465116,"The input to the generative model is the same sensory input that we pass to our inference network.
The ﬁrst layer of our generative model is a single layer nonlinear transform that converts our 3-
dimensional input to an N dimensional input that is 0 to any non-sensory neurons. The next layer is
our recurrent neural dynamics, given by Equation 1, which passes information to itself in the next
time step based on the connectivity given by the connectome. Although some sources say that C.
elegans has 302 neurons, we only simulate 300 because two cells originally classiﬁed as neurons
are not connected to the rest of the connectome Witvliet et al. [31]. The simulated voltage for each
neuron in the connectome is passed to our calcium simulation layer (Equation 5). The ﬁnal layer of"
REFERENCES,0.6046511627906976,Published as a conference paper at ICLR 2022
REFERENCES,0.6075581395348837,"our model is calcium to ﬂuorescence afﬁne transformation (Equation 6). The output of this ﬁnal layer
is then compared to the ground truth ﬂuorescence data to calculate the reconstruction loss. Below, we
have included some pseudocode describing the architecture of our network."
REFERENCES,0.6104651162790697,"Inference_Network ( calcium_fluor ,
missing_data_mask ,
s e n s o r y _ i n p u t ) :
calcium_input = c o n c a t e n a t e ( calcium_fluor ,
missing_data_mask )
conv1_out = r e l u ( conv1 ( calcium_input ) )
up1_out = upsample ( conv1_out )
conv2_out = r e l u ( conv2 ( up1_out ) )
up2_out = upsample ( conv2_out )"
REFERENCES,0.6133720930232558,"sensory_conv_out = r e l u ( conv3 ( s e n s o r y _ i n p u t ) )
merged_calcium_sensory = c o n c a t e n a t e ( up2_out ,
sensory_conv_out )"
REFERENCES,0.6162790697674418,mean_latent_neuron_voltage = conv4 ( merged_calc_sensory )
REFERENCES,0.6191860465116279,"s t d _ l a t e n t _ n e u r o n _ v o l t a g e =
s o f t p l u s ( conv5 ( merged_calcim_sensory ) )"
REFERENCES,0.622093023255814,"s a m p l e _ l a t e n t _ n e u r o n _ v o l t a g e = mean_latent_neuron_voltage
+ rand_norm *
s t d _ l a t e n t _ n e u r o n _ v o l t a g e"
REFERENCES,0.625,"r e t u r n
s a m p l e _ l a t e n t _ n e u r o n _ v o l t a g e"
REFERENCES,0.627906976744186,"Generative_Model ( s a m pl e _ l at e n t _ n e u r o n _ v o l t a g e ,
s e n s o r y _ i n p u t ) :"
REFERENCES,0.6308139534883721,"# Equation 1
neuron_voltage_dynamics = leaky_integrator_connectome_dynamics (
s a mp l e _ la t e n t_ n e u r on _ v o l t a g e ,
s e n s o r y _ i n p u t )"
REFERENCES,0.6337209302325582,"# Equation 5
calcium_concentration_dynamics = l e a k y _ i n t e g r a t o r _ c a l c i u m _ m o d e l (
neuron_voltage_dynamics )"
REFERENCES,0.6366279069767442,"# Equation 6
f l u o r e s c e n c e _ t r a c e = n o n l i n e a r _ a f f i n e _ t r a n s f o r m (
calcium_concentration_dynamcis )"
REFERENCES,0.6395348837209303,"r e t u r n
f l u o r e s c e n c e _ t r a c e"
REFERENCES,0.6424418604651163,"A.4
FORMULATIONS FOR OBJECTIVE FUNCTIONS AND METRICS"
REFERENCES,0.6453488372093024,"Here we provide the mathematical formulations for the objective functions and metrics we used in
this work."
REFERENCES,0.6482558139534884,"The evidence lower bound (ELBO) is a combination of reconstruction log likelihood and the negative
KL divergence. The reconstruction log-likelihood, given the predicted ﬂuorescence trace ˆfi(t) of
neuron i at time step t as output distribution N( ˆfi(t), σf). And ˆfi is formulated as"
REFERENCES,0.6511627906976745,"ˆfi(t) = αf[Ca](t) + βf.
(18)"
REFERENCES,0.6540697674418605,"Given the measured ﬂuorescence trace fi(t), number of neurons N, and the total number of time
steps D + 1, we calculated the reconstruction negative log likelihood as the reconstruction loss"
REFERENCES,0.6569767441860465,"Lrecon = − D∆t
X"
REFERENCES,0.6598837209302325,"t=0
(2π(σf)2)−N/2 exp "
REFERENCES,0.6627906976744186,"−
1
2(σf)2 N
X"
REFERENCES,0.6656976744186046,"i=1
(fi(t) −ˆfi(t))2
! (19)"
REFERENCES,0.6686046511627907,"The KL divergence describes the distance between voltage prior and posterior distribution. We
represent the posterior distribution generated from the inference network as N(µi(t), σi(t)) for"
REFERENCES,0.6715116279069767,Published as a conference paper at ICLR 2022
REFERENCES,0.6744186046511628,"neuron i at each time step t. We then sample vi(t) from the posterior distribution. According to
Equation 16, we represent the mean ˆvi(t) of prior distribution N( ˆvi(t), σv
i ) as"
REFERENCES,0.6773255813953488,"ˆvi(t) = F(vi(t), sc
i(t), se
i(t), oi(t + ∆t)),
(20)"
REFERENCES,0.6802325581395349,"where σv
i is a trainable parameter."
REFERENCES,0.6831395348837209,"We then use a closed-form of KL divergence deﬁned between two univariate Gaussian distributions DKL = D∆t
X t=0 N
X i=1"
REFERENCES,0.686046511627907,"
log
 σv
i
σi(t)"
REFERENCES,0.688953488372093,"
+ (σi(t))2 + (µi(t) −ˆvi(t))2"
REFERENCES,0.6918604651162791,"2(σv
i )2
−1 2"
REFERENCES,0.6947674418604651,"
(21)"
REFERENCES,0.6976744186046512,We minimize the objective function L = −ELBO during optimization given by
REFERENCES,0.7005813953488372,"L = Lrecon + DKL
(22)"
REFERENCES,0.7034883720930233,"In the worm holdout evaluation, for each neuron pair, we calculated the ELBO, reconstruction loss
and KL divergence metrics on each holdout worm, for a total 6 worms. We also trained each model
setting 4 times under the same constraints and different initializations. We calculated the mean and
standard error of ELBO, reconstruction loss and KL divergence based on 4 × 6 = 24 samples for
each model."
REFERENCES,0.7063953488372093,"For both neuron and worm holdout evaluations, we calculated the correlation coefﬁcients between
the measured fi(t) and reconstructed ﬂuorescence traces ˆfi(t). The correlation coefﬁcient for each
neuron is simply ri = D∆t
X t=0"
REFERENCES,0.7093023255813954,"
ˆfi(t) −¯ˆfi(t)

fi(t) −¯fi(t)
"
REFERENCES,0.7122093023255814,"v
u
u
t D∆t
X t=0"
REFERENCES,0.7151162790697675,"
ˆfi(t) −¯ˆfi(t)
2 D∆t
X t=0"
REFERENCES,0.7180232558139535,"
fi(t) −¯fi(t)
2
.
(23)"
REFERENCES,0.7209302325581395,"In the neuron holdout evaluation, for each neuron pair, we trained a model in which the ground truth of
said neuron pair was removed from the training data. There were 107 pairs and 170 measured neurons
in total. We trained each model 4 times with different initializations under each of 8 connectome
constraints, for a total of 8 × 107 × 4 = 3424 different models. We calculated the mean and standard
error of ri based on 4 × 170 = 680 samples for each model."
REFERENCES,0.7238372093023255,"In the worm holdout evaluation, we calculated these metrics on each holdout worm, for a total of 6
worms. Each worm has about 170 measured neurons. We trained each model 4 times under the same
connectomic constraints and with different initializations. We calculated the mean and standard error
of ri based on 6 × 170 × 4 = 4080 samples for each model in our evaluation."
REFERENCES,0.7267441860465116,"A.5
GRADIENT-BASED OPTIMIZATION"
REFERENCES,0.7296511627906976,"We used PyTorch to minimize the ELBO using gradient descent with respect to the parameters of
both the generative and inference models,"
REFERENCES,0.7325581395348837,"For LVMs which used connectome count constraint with αc and αe trainable. The objective function
is
L = L(τ, τ[Ca], αc, αe, αf, βf, σv, σf, E, MLP, φ)
(24)"
REFERENCES,0.7354651162790697,"For LVMs which used other constraints including connectome count and learned dense, learned
sparse, where M c and M e trainable. The objective function is"
REFERENCES,0.7383720930232558,"L = L(τ, τ[Ca], M c, M e, αf, βf, σv, σf, E, MLP, φ)
(25)"
REFERENCES,0.7412790697674418,Published as a conference paper at ICLR 2022
REFERENCES,0.7441860465116279,"where φ is all the parameters of the inference network. We used a clamp in PyTorch to constrain
the weight magnitude M c and M e to be non-negative in several models. We restrict the M e to be
symmetric by deﬁning it as the sum of an upper triangular matrix and its transpose. Our simulation
time step is 6.25ms. Each window size D lasts for 7.5s with 30 imaging steps. We used weights of 1
for both the reconstruction loss and KL divergence. We used Adam with a learning rate scheduler to
perform the optimization. We used an initial learning rate of 3e−4, with a learning rate schedular
with a step size of 50, and a gamma of 0.5. We also set a gradient clip value of 1. Each model was
trained for 300 epochs in which each epoch is one full pass through all the training data."
REFERENCES,0.747093023255814,"In the paper, we use the variable θ to denote the trainable parameters of our generative model. The
parameters in θ are given by"
REFERENCES,0.75,"θ = {τ, τ[Ca], M c, M e, αf, βf, σv, σf, E, H}
(26)"
REFERENCES,0.752906976744186,"where τ and τ[Ca] are the time scales of the neuron and calcium dynamics. M c and M e are the
magnitudes of neuronal connections for chemical and electrical synapses respectively. αf and βf
are the scaling and bias parameters of the ﬂuorescence afﬁne transform. σv and σf are the noise
magnitudes in the neural dynamics equation (Equation 1) and the ﬂuorescence equation (Equation 6).
The variable H represents the parameters of the sensory mapping that transforms chemosensory data
into neural stimulus."
REFERENCES,0.7558139534883721,"The variable φ denotes all the trainable parameters of the inference network. These are just all the
weights and biases of the 5 different convolutional layers described in section A.3."
REFERENCES,0.7587209302325582,"A.6
CONNECTOME CONSTRAINTS FOR MODEL WEIGHTS"
REFERENCES,0.7616279069767442,"We constructed CC-LVMs on both current-based and conductance-based synapses with different levels
of connectomic constraints. Given anatomical connectome data with two N × N matrices: chemical
synapse counts (Cc), and sizes of electrical synapses (Ce). The chemical synaptic weight W c
ji as
W c
ji = αcT c
jiM c
ji, where αc represents a global scaling factor. T c
ji is a binary value which indicates
the connectivity, and M c
ji reﬂects the magnitude of connection strength. The electrical synaptic
weights matrix W e
ji is similarly deﬁned as W e
ji = αeT e
jiM e
ji. We provide more implementation
details of each constraint below."
REFERENCES,0.7645348837209303,"conductance-based + connectome count1 We directly assign the synapse counts to the chemical
connection magnitudes (M c
ji = Cc
ji) and the synapse sizes to the eletrical connection magnitudes
(M e
ji = Ce
ji). Under this constraint, both the sparsity T c
ji and T e
ji are ﬁxed to the connectome sparsity,
magnitude M c
ji and M e
ji are ﬁxed to the connectome synapse count, and only the global scaling
factors αc and αe are trainable. M c
ji and M e
ji are restricted to be non-negative."
REFERENCES,0.7674418604651163,"conductance-based + connectome count2 We initialize all model parameters with the parameters
from the trained conductance-based + connectome count1 model. And then the sparsity T c
ji and
T e
ji are ﬁxed to the connectome sparsity, global scaling factors αc and αe are ﬁxed to 0.01. And
now magnitude M c
ji and M e
ji are trainable during the re-training. M c
ji and M e
ji are restricted to be
non-negative."
REFERENCES,0.7703488372093024,"conductance-based + connectome sparsity For chemical and electrical connections, the model
sparsities are T c
ji = 1[Cc
ji > 0] and T e
ji = 1[Ce
ji > 0] respectively. For both chemical and electrical
connections, the magnitudes M c
ji and M e
ji are trainable. The sparsity T c
ji and T e
ji are ﬁxed to the
connectome sparsity and global scaling factors, αc and αe, are ﬁxed to 0.01. M c
ji and M e
ji are
restricted to be non-negative."
REFERENCES,0.7732558139534884,"conductance-based + learned total count For chemical and electrical connections, the model is
fully connected with the sparsity T c
ji = 1 and T e
ji = 1 respectively. For both chemical and electrical
connections, the magnitudes M c
ji and M e
ji are trainable. The model is constrained to preserve the
same total synapse count as connectome count ∥Cc∥for both magnitude matrices adding ∥∥M c∥1 −
∥Cc∥1∥2
2 and ∥∥M e∥1 −∥Ce∥1∥2
2 in the objective function. αc and αe are deﬁned as 0.002 and
0.065, which are the same as the trained conne.count1 model, so that regularizes the sum of elements
in M c equal to that of Cc, and likewise for M e and Ce. The sparsity T c
ji and T e
ji are ﬁxed to 1 and
M c
ji and M e
ji are restricted to be non-negative."
REFERENCES,0.7761627906976745,Published as a conference paper at ICLR 2022
REFERENCES,0.7790697674418605,"model
current-based
conductance-based"
REFERENCES,0.7819767441860465,"consistency weight learned
dense
learned
sparse
conne.
sparsity"
REFERENCES,0.7848837209302325,"learned
dense
learned
sparse
learned
total count
conne.
sparsity
conne.
count1 conne.
count2"
REFERENCES,0.7877906976744186,"inter
W c
0.142
0.084
0.497
0.082
0.084
0.092
1.000
1.000
1.000
W e
0.024
0.000
0.748
0.097
0.000
0.010
0.997
1.000
0.997"
REFERENCES,0.7906976744186046,"connectome
W c
-0.002
-0.001
0.469
-0.001
0.001
0.003
1.000
1.000
1.000
W e
-0.002
0.000
0.654
0.006
0.000
0.007
0.998
1.000
0.996"
REFERENCES,0.7936046511627907,"Table 3: Inter-consistency and connectome-consistency. Correlation between multiple model
weights with different initializations, and correlation coefﬁcient between model weights and connec-
tome counts. Evaluations are performed for models ﬁtted on a single worm and across current-based
and conductance-based models with different levels of connectome constraints. The CC-LVM is
more consistent among different random initializations and more highly correlated to the connectome
counts."
REFERENCES,0.7965116279069767,"conductance-based + learned sparse For chemical and electrical connections, the model is fully
connected with the sparsity T c
ji = 1 and T e
ji = 1 respectively. For both chemical and electrical
connections, the magnitudes M c
ji and M e
ji are trainable. Meanwhile, it uses an additional L1
regularization on both magnitude matrices: ∥M c∥1 and ∥M e∥1, so that the fraction of nonzero
elements in M c is small, and likewise for M e. The sparsity T c
ji and T e
ji are ﬁxed to 1 and global
scaling factors αc and αe are ﬁxed to 0.01. M c
ji and M e
ji are restricted to be non-negative."
REFERENCES,0.7994186046511628,"conductance-based + learned dense For chemical connections and electrical connections, the model
is a fully connected with sparsity T c
ji = 1 and T e
ji = 1 respectively. For both chemical and electrical
connections, the magnitudes M c
ji and M e
ji are trainable. The sparsity matrices, T c
ji and T e
ji are ﬁxed.
Global scaling factors αc and αe are also ﬁxed to 0.01. M c
ji and M e
ji are restricted to be non-negative."
REFERENCES,0.8023255813953488,"current-based + connectome sparsity For chemical and electrical connections, the model sparsity is
T c
ji = 1[Cc
ji > 0] and T e
ji = 1[Ce
ji > 0] respectively. For both chemical and electrical connections,
the magnitudes M c
ji and M e
ji are trainable. The sparsity T c
ji and T e
ji are ﬁxed to the connectome
sparsity and global scaling factors, αc and αe, are ﬁxed to 0.01. M e
ji are restricted to be non-negative,
while the sign of M c
ji is not restricted."
REFERENCES,0.8052325581395349,"current-based + learned sparse For chemical and electrical connections, the model is fully connected
with sparsity T c
ji = 1 and T e
ji = 1. For both chemical and electrical connections, the magnitudes
M c
ji and M e
ji are trainable. For both chemical and electrical connections, the magnitudes M c
ji and
M e
ji are trainable. Meanwhile, it uses an additional L1 regularization on both magnitude matrices:
||M c||1 and ||M e||1, so that the fraction of nonzero elements in M c is small, and likewise for M e.
The sparsity T c
ji and T e
ji are ﬁxed to 1, global scaling factors, αc and αe, are ﬁxed to 0.01. M e
ji are
restricted to be non-negative, while the sign of M c
ji is not restricted."
REFERENCES,0.8081395348837209,"current-based + learned dense For chemical and electrical connections, the model is fully connected
with T c
ji = 1, and T e
ji = 1. For both chemical and electrical connections, the magnitudes M c
ji and
M e
ji are trainable. The sparsity T c
ji and T e
ji are ﬁxed to 1 and global scaling factors αc and αe are
ﬁxed to 0.01. M e
ji are restricted to be non-negative, while the sign of M c
ji is not restricted."
REFERENCES,0.811046511627907,"A.7
MODEL WEIGHT CONSISTENCY ANALYSIS"
REFERENCES,0.813953488372093,"We used the correlation coefﬁcient to quantify the inter-consistency of models trained with different
random initializations, as well as the connectome-consistency, which is the correlation coefﬁcient
between model weights and connectome counts."
REFERENCES,0.8168604651162791,"In particular, for the connectome consistency, we compare the consistency of our model weights and
connectome synapse count. We calculate the correlation coefﬁcient ri between the absolute value of
model weights |Wij| and connectome synapse count Cij along each row i. Then it is averaged across
N rows and 4 models with identical constraints but different random initializations to get ¯ri as our
metric for connectome-consistency,"
REFERENCES,0.8197674418604651,Published as a conference paper at ICLR 2022
REFERENCES,0.8226744186046512,"Figure 4: Worm holdout evaluation: Violin plots comparing the distribution of correlation coefﬁ-
cients between predicted and measured neuron activity for the 9 LVM variants. Neurons are divided
into sensory, inter, and motor neurons. ri = N
X j=1"
REFERENCES,0.8255813953488372,"
|Wij| −
¯
|Wij|

Cij −¯
Cij
"
REFERENCES,0.8284883720930233,"v
u
u
t N
X j=1"
REFERENCES,0.8313953488372093,"
|Wij| −
¯
|Wij|
2
N
X j=1"
REFERENCES,0.8343023255813954,"
Cij −¯
Cij
2
.
(27)"
REFERENCES,0.8372093023255814,"For inter-consistency, we evaluate how model weights are affected by random initialization after
training. We calculate the correlation coefﬁcient ri between the weights W 1
ij of model 1 and weights
W 2
ij of model 2 along each row i. Model 1 and model 2 have the same constraints but different
random initializations. The correlation coefﬁcient between them is ri = N
X j=1"
REFERENCES,0.8401162790697675,"
W 1
ij −¯
W 1
ij

W 2
ij −¯
W 2
ij
"
REFERENCES,0.8430232558139535,"v
u
u
t N
X j=1"
REFERENCES,0.8459302325581395,"
W 1
ij −¯
W 1
ij
2
N
X j=1"
REFERENCES,0.8488372093023255,"
W 2
ij −¯
W 2
ij
2
.
(28)"
REFERENCES,0.8517441860465116,"This coefﬁcient, ri, is then averaged across N rows and each combination of 2 models within the 4
models with random initialization to get ¯ri as our metric for inter-consistency."
REFERENCES,0.8546511627906976,"As shown in Table 3, the conductance-based model parameters are more consistent under different
random initializations. Optimization of the conductance-based model is easier because the chemical
weights W c of the conductance-based synapses are always positive and we do not need a trainable
sign variable. Meanwhile, the model parameters are better constrained with the connectome sparsity
or counts, which enables it to be consistent across different initialization."
REFERENCES,0.8575581395348837,"A.8
PREDICTIVE PERFORMANCE FOR WORM HOLDOUT V.S. NEURON HOLDOUT"
REFERENCES,0.8604651162790697,"We demonstrate the violin plot for the model performance in worm holdout evaluation in Figure 4. We
found that the results from worm holdout are different from those in the neuron holdout experiment
in Figure 2. Here, all 9 LVM variants perform similarly across different categories. Our connectome-
constrained models do not outperform other unconstrained baselines."
REFERENCES,0.8633720930232558,"We also studied whether the predictive performance of worm holdouts and neuron holdouts were
correlated. In Figure 5, we show the scatter plot of predictive performance for worm holdouts
v.s. neuron holdouts. Each point represents one neuron from the same worm. We also report the
correlation coefﬁcient between the predictive performance for worm holdout and neuron holdout for
each LVM variant. We found that our connectome-constrained model achieves the highest correlation,"
REFERENCES,0.8662790697674418,Published as a conference paper at ICLR 2022
REFERENCES,0.8691860465116279,"Figure 5: Scatter plot for predictive performance of worm holdout v.s. neuron holdout. Each
point represents one neuron. Our best model connectome count2 achieves highest correlation, which
indicates highest consistency for neurons which gets better performance in both evaluations."
REFERENCES,0.872093023255814,"and also indicates the highest consistency for neuron sets which gets better performance in both
evaluations."
REFERENCES,0.875,"A.9
PREDICTED HOLDOUT TRACES WITH VARIOUS CONSTRAINTS"
REFERENCES,0.877906976744186,"For each neuron, we performed both neuron holdout evaluation and worm holdout evaluation for
current-based and conductance-based models with different levels of connectomic constraint."
REFERENCES,0.8808139534883721,"For neuron holdout evaluation, we plot additional predicted traces from 15 representative neurons,
including 12 with good performance (top), and 3 with worse performance (bottom) in Figure 6,
models with connectome count constraint outperforms other baselines."
REFERENCES,0.8837209302325582,"For worm holdout evaluation, we show the results on the same 15 neurons from 9 LVMs in Figure 7,
all models achieve comparable results."
REFERENCES,0.8866279069767442,"A.10
PREDICTED TRACES AND VOLTAGE LATENT SPACE FOR NEURON HOLDOUT EVALUATION"
REFERENCES,0.8895348837209303,"For neuron holdout evaluation, we also show the predicted traces and voltage latent space for all
measured neurons. We show 189 neurons in Figure 8, Figure 9 and Figure 10. In each subplot, we
reported the measured ﬂuorescence trace ground truth, predicted ﬂuorescence trace ground truth,
and the mean and standard deviation (std) of voltage latent space (posterior distribution) from the
inference network."
REFERENCES,0.8924418604651163,"A.11
PREDICTED TRACES AND VOLTAGE LATENT SPACE FOR WORM HOLDOUT EVALUATION"
REFERENCES,0.8953488372093024,"For worm holdout evaluation, similarly, we also show the predicted traces and voltage latent space
for all neurons. We show 300 neurons from a train worm in Figure 11–Figure 15. Meanwhile, we
also show 300 neurons from a test worm in Figure 16–Figure 20. In each subplot, we reported the
measured ﬂuorescence trace ground truth (for recorded neurons), predicted ﬂuorescence trace ground"
REFERENCES,0.8982558139534884,Published as a conference paper at ICLR 2022
REFERENCES,0.9011627906976745,"Figure 6: LVM predictive performance across neuron types for neuron holdout evaluation.
Measured traces and predictions made by the 9 LVMs for 15 neurons including sensory neurons,
interneurons and motor neurons."
REFERENCES,0.9040697674418605,"truth, and the mean and standard deviation (std) of voltage latent space (posterior distribution) from
the inference network."
REFERENCES,0.9069767441860465,Published as a conference paper at ICLR 2022
REFERENCES,0.9098837209302325,"Figure 7: LVM predictive performance across neuron types for worm holdout evaluation. Mea-
sured traces and predictions made by the 9 LVMs for 15 neurons including sensory neurons, interneu-
rons and motor neurons."
REFERENCES,0.9127906976744186,"For the train worm, we ﬁnd most of the neurons have good predictive performance compared to the
measured ground truth. That indicates optimization is saturated on the train set. However, the test
worm has a relatively worse predictive performance, which indicates the generalization gaps between"
REFERENCES,0.9156976744186046,Published as a conference paper at ICLR 2022
REFERENCES,0.9186046511627907,"each individual trial (animal). While some neurons still achieve good results, that indicates our model
is still generalized to unseen individuals to some degree."
REFERENCES,0.9215116279069767,"Interestingly, for both train and test worms in the worm holdout evaluation, we ﬁnd those unmeasured
neurons still have reasonable patterns. That indicates our model successfully avoids the degeneracy
of solutions for those unmeasured neurons."
REFERENCES,0.9244186046511628,Published as a conference paper at ICLR 2022
REFERENCES,0.9273255813953488,"Figure 8: Neuron holdout evaluated on connectome count2: CC-LVM predicted traces and the
corresponding voltage latent space across all recorded neurons (part 1)."
REFERENCES,0.9302325581395349,Published as a conference paper at ICLR 2022
REFERENCES,0.9331395348837209,"Figure 9: Neuron holdout evaluated on connectome count2: CC-LVM predicted traces and the
corresponding voltage latent space across all recorded neurons (part 2)."
REFERENCES,0.936046511627907,Published as a conference paper at ICLR 2022
REFERENCES,0.938953488372093,"Figure 10: Neuron holdout evaluated on connectome count2: CC-LVM predicted traces and the
corresponding voltage latent space across all recorded neurons (part 3)."
REFERENCES,0.9418604651162791,Published as a conference paper at ICLR 2022
REFERENCES,0.9447674418604651,"Figure 11: A train worm evaluated on worm holdout using connectome count2 constraint:
CC-LVM predicted traces and the corresponding voltage latent space across all neurons including
unmeasured neurons (part 1)."
REFERENCES,0.9476744186046512,Published as a conference paper at ICLR 2022
REFERENCES,0.9505813953488372,"Figure 12: A train worm evaluated on worm holdout using connectome count2 constraint:
CC-LVM predicted traces and the corresponding voltage latent space across all neurons including
unmeasured neurons (part 2)."
REFERENCES,0.9534883720930233,Published as a conference paper at ICLR 2022
REFERENCES,0.9563953488372093,"Figure 13: A train worm evaluated on worm holdout using connectome count2 constraint:
CC-LVM predicted traces and the corresponding voltage latent space across all neurons including
unmeasured neurons (part 3)."
REFERENCES,0.9593023255813954,Published as a conference paper at ICLR 2022
REFERENCES,0.9622093023255814,"Figure 14: A train worm evaluated on worm holdout using connectome count2 constraint:
CC-LVM predicted traces and the corresponding voltage latent space across all neurons including
unmeasured neurons (part 4)."
REFERENCES,0.9651162790697675,Published as a conference paper at ICLR 2022
REFERENCES,0.9680232558139535,"Figure 15: A train worm evaluated on worm holdout using connectome count2 constraint:
CC-LVM predicted traces and the corresponding voltage latent space across all neurons including
unmeasured neurons (part 5)."
REFERENCES,0.9709302325581395,Published as a conference paper at ICLR 2022
REFERENCES,0.9738372093023255,"Figure 16: A hold-out (test) worm evaluated on worm holdout using connectome count2 con-
straint: CC-LVM predicted traces and the corresponding voltage latent space across all neurons
including unmeasured neurons (part 1)."
REFERENCES,0.9767441860465116,Published as a conference paper at ICLR 2022
REFERENCES,0.9796511627906976,"Figure 17: A hold-out (test) worm evaluated on worm holdout using connectome count2 con-
straint: CC-LVM predicted traces and the corresponding voltage latent space across all neurons
including unmeasured neurons (part 2)."
REFERENCES,0.9825581395348837,Published as a conference paper at ICLR 2022
REFERENCES,0.9854651162790697,"Figure 18: A hold-out (test) worm evaluated on worm holdout using connectome count2 con-
straint: CC-LVM predicted traces and the corresponding voltage latent space across all neurons
including unmeasured neurons (part 3)."
REFERENCES,0.9883720930232558,Published as a conference paper at ICLR 2022
REFERENCES,0.9912790697674418,"Figure 19: A hold-out (test) worm evaluated on worm holdout using connectome count2 con-
straint: CC-LVM predicted traces and the corresponding voltage latent space across all neurons
including unmeasured neurons (part 4)."
REFERENCES,0.9941860465116279,Published as a conference paper at ICLR 2022
REFERENCES,0.997093023255814,"Figure 20: A hold-out (test) worm evaluated on worm holdout using connectome count2 con-
straint: CC-LVM predicted traces and the corresponding voltage latent space across all neurons
including unmeasured neurons (part 5)."
