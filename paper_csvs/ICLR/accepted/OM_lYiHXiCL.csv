Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018083182640144665,"Deep neural networks (DNNs) are proved to be vulnerable against backdoor at-
tacks. A backdoor is often embedded in the target DNNs through injecting a back-
door trigger into training examples, which can cause the target DNNs misclassify
an input attached with the backdoor trigger. Existing backdoor detection meth-
ods often require the access to the original poisoned training data, the parameters
of the target DNNs, or the predictive conﬁdence for each given input, which are
impractical in many real-world applications, e.g., on-device deployed DNNs. We
address the black-box hard-label backdoor detection problem where the DNN is
fully black-box and only its ﬁnal output label is accessible. We approach this prob-
lem from the optimization perspective and show that the objective of backdoor de-
tection is bounded by an adversarial objective. Further theoretical and empirical
studies reveal that this adversarial objective leads to a solution with highly skewed
distribution; a singularity is often observed in the adversarial map of a backdoor-
infected example, which we call the adversarial singularity phenomenon. Based
on this observation, we propose the adversarial extreme value analysis (AEVA)
to detect backdoors in black-box neural networks. AEVA is based on an extreme
value analysis of the adversarial map, computed from the monte-carlo gradient es-
timation. Evidenced by extensive experiments across multiple popular tasks and
backdoor attacks, our approach is shown effective in detecting backdoor attacks
under the black-box hard-label scenarios."
INTRODUCTION,0.003616636528028933,"1
INTRODUCTION"
INTRODUCTION,0.0054249547920434,"Deep Neural Networks (DNNs) have pervasively been used in a wide range of applications such
as facial recognition (Masi et al., 2018), object detection (Szegedy et al., 2013; Li et al., 2022), au-
tonomous driving (Okuyama et al., 2018), and home assistants (Singh et al., 2020; Zhai et al., 2021).
In the meanwhile, DNNs become increasingly complex. Training state-of-the-art models requires
enormous data and expensive computation. To address this problem, vendors and developers start to
provide pre-trained DNN models. Similar to softwares shared on GitHub, pre-trained DNN models
are being published and shared on online venues like the BigML model market, ONNX zoo and
Caffe model zoo."
INTRODUCTION,0.007233273056057866,"Since the dawn of software distribution, there has been an ongoing war between publishers sneaking
malicious code and backdoors in their software and security personnel detecting them. Recent stud-
ies show that DNN models can contain similar backdoors, which are induced due to contaminated
training data. Sometimes models containing backdoors can perform better than the regular models
under untampered test inputs. However, under inputs tampered with a speciﬁc pattern (called trojan
trigger) models containing backdoors can suffer from signiﬁcant accuracy loss."
INTRODUCTION,0.009041591320072333,"There has been a signiﬁcant amount of recent work on detecting the backdoor triggers. However,
those solutions require access to the original poisoned training data (Chen et al., 2019a; Tran et al.,
2018; Huang et al., 2022), the parameters of the trained model (Chen et al., 2019b; Guo et al., 2020;
Liu et al., 2019; Wang et al., 2019; 2020; Dong et al., 2021; Kolouri et al., 2020), or the predicted
conﬁdence score of each class (Dong et al., 2021). Unfortunately, it is costly and often impractical
for the defender to access the original poisoned training dataset. In situations when DNNs are"
INTRODUCTION,0.0108499095840868,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012658227848101266,"Cloud Server
Smart Phone
Edge Device"
INTRODUCTION,0.014466546112115732,"Input 
W/ Trigger"
INTRODUCTION,0.0162748643761302,"Input 
W/O Trigger"
INTRODUCTION,0.018083182640144666,Label Cat
INTRODUCTION,0.019891500904159132,Label Dog
INTRODUCTION,0.0216998191681736,"Blackbox to the model User/Defender
Model User/Defender Label"
INTRODUCTION,0.023508137432188065,Deploy Query
INTRODUCTION,0.02531645569620253,Third Party AI Device
INTRODUCTION,0.027124773960216998,Figure 1: An illustration of the black-box hard-label backdoors.
INTRODUCTION,0.028933092224231464,"deployed in safety-critical platforms (Fowers et al., 2018; Okuyama et al., 2018; Li et al., 2021b) or
in cloud services (Fowers et al., 2018; Chen et al., 2019c), it is also impractical to access either their
parameters or the predicted conﬁdence score of each class (Chen et al., 2019c; 2020b)."
INTRODUCTION,0.03074141048824593,"We present the black-box hard-label backdoor detection problem where the DNN is a fully black-
box and only its ﬁnal output label is accessible (Fig. 1). Detecting backdoor-infected DNNs in such
black-box setting becomes critical due to the emerging model deployment in embedded devices and
remote cloud servers. In this setting, the typical optimization objective of backdoor detection (Chen
et al., 2019b; Liu et al., 2019) becomes impossible to solve due to the limited information. However,
according to a theoretical analysis, we show that the backdoor objective is bounded by an adversarial
objective, which can be optimized using Monte Carlo gradient estimation in our black-box hard-
label setup. Further theoretical and empirical studies reveal that this adversarial objective leads to
a solution with highly skewed distribution; a singularity is likely to be observed in the adversarial
map of a backdoor-infected example, which we call the adversarial singularity phenomenon."
INTRODUCTION,0.0325497287522604,"Based on these ﬁndings, we propose the adversarial extreme value analysis (AEVA) algorithm
to detect backdoors in black-box neural networks. The AEVA algorithm is based on an extreme
value analysis (Leadbetter, 1991) on the adversarial map. We detect the adversarial singularity
phenomenon by looking at the adversarial peak, i.e., maximum value of the computed adversarial
perturbation map. We perform a statistical study which reveals that, there is around 60% chance that
the adversarial peak of a random sample for a backdoor-infected DNN is larger than the adversarial
peak of any examples for an uninfected DNN. Inspired by the Univariate theory, we propose a global
adversarial peak (GAP) value by sampling multiple examples and choosing the maximum over their
adversarial peaks, to ensure a high success rate. Following previous works (Dong et al., 2021; Wang
et al., 2019), the Median Absolute Deviation (MAD) algorithm is implemented on top of the GAP
values to test whether a DNN is backdoor-infected."
INTRODUCTION,0.034358047016274866,"Through extensive experiments across three popular tasks and state-of-the-art backdoor techniques,
AEVA is proved to be effective in detecting backdoor attacks under the black-box hard-label scenar-
ios. The results show that AEVA can efﬁciently detect backdoor-infected DNNs, yielding an overall
detection accuracy ≥86.7% across various tasks, DNN models and triggers. Rather interestingly,
when comparing to two state-of-art white-box backdoor detection methods, AEVA yields compara-
ble performance, even though AEVA is a black-box method with limited access to information."
INTRODUCTION,0.03616636528028933,We summarize our contributions as below 1:
INTRODUCTION,0.0379746835443038,"1. To the best of our knowledge, we are the ﬁrst to present the black-box hard-label backdoor
detection problem and provide an effective solution to this problem."
WE PROVIDE A THEORETICAL ANALYSIS WHICH SHOWS BACKDOOR DETECTION OPTIMIZATION IS BOUNDED,0.039783001808318265,"2. We provide a theoretical analysis which shows backdoor detection optimization is bounded
by an adversarial objective. And we further reveal the adversarial singularity phenomenon
where the adversarial perturbation computed from a backdoor-infected neural network is
likely to suffer from a highly skewed distribution."
WE PROVIDE A THEORETICAL ANALYSIS WHICH SHOWS BACKDOOR DETECTION OPTIMIZATION IS BOUNDED,0.04159132007233273,"3. We propose a generic backdoor detection framework AEVA, optimizing the adversarial
objective and performing extreme value analysis on the optimized adversarial map. AEVA
is applicable to the black-box hard-label setting with a Monte Carlo gradient estimation."
WE PROVIDE A THEORETICAL ANALYSIS WHICH SHOWS BACKDOOR DETECTION OPTIMIZATION IS BOUNDED,0.0433996383363472,1Code :https://github.com/JunfengGo/AEVA-Blackbox-Backdoor-Detection-main
WE PROVIDE A THEORETICAL ANALYSIS WHICH SHOWS BACKDOOR DETECTION OPTIMIZATION IS BOUNDED,0.045207956600361664,Published as a conference paper at ICLR 2022
WE EVALUATE AEVA ON THREE WIDELY-ADOPTED TASKS WITH DIFFERENT BACKDOOR TRIGGER IMPLE-,0.04701627486437613,"4. We evaluate AEVA on three widely-adopted tasks with different backdoor trigger imple-
mentations and complex black-box attack variants. All results suggest that AEVA is effec-
tive in black-box hard-label backdoor detection."
RELATED WORK,0.048824593128390596,"1.1
RELATED WORK"
RELATED WORK,0.05063291139240506,"Backdoor attacks. BadNets (Gu et al., 2019) is probably the ﬁrst work on backdoor attacks against
DNNs, which causes target misclassiﬁcation to DNNs through injecting a small trigger into some
training samples and mis-labeling these samples with a speciﬁc target class. Trojaning attack (Liu
et al., 2018) generates a trigger which can cause the maximum activation value of certain selected
neurons with limited training data. Chen et al. (Chen et al., 2017) further propose to perform back-
door attacks under a rather weak threat model where the attacker cannot access the target model and
training dataset. Most recently, a set of backdoor attacks (Ji et al., 2018; Liu et al., 2020; Saha et al.,
2020; Turner et al., 2019; Yao et al., 2019) have been proposed, which are built upon existing work
but focused on various speciﬁc scenarios, e.g., physical-world, transfer learning, etc."
RELATED WORK,0.05244122965641953,"Backdoor detection (white-box). A few works (Chen et al., 2019a; Tran et al., 2018) are proposed
to detect the backdoor samples within the training dataset. Chen et al. (2019a) propose a neuron
activation clustering approach to identify backdoor samples through clustering the training data
based on the neuron activation of the target DNN. Tran et al. (2018) distinguish backdoor samples
from clean samples based on the spectrum of the feature covariance of the target DNN. Few other
works focus on detecting backdoor-infected DNNs (Liu et al., 2019; Wang et al., 2019). ABS (Liu
et al., 2019) identiﬁes the infected DNNs by seeking compromised neurons representing the features
of backdoor triggers. Neural Cleanse (Wang et al., 2019) conducts the reverse engineering to restore
the trigger through solving an optimization problem. Recent works (Chen et al., 2019b; Guo et al.,
2020; Wang et al., 2020) improve Neural Cleanse with better objective functions. However, these
methods are all white-box based, requiring access to model parameters or internal neuron values."
RELATED WORK,0.054249547920433995,"Backdoor detection (black-box). A recent work (Chen et al., 2019b) claims to detect backdoor at-
tacks in the “black-box” settings. However, their method still need the DNN’s parameters to train a
separate generator (Goodfellow et al., 2014). So, strictly speaking, their method is not “black-box”,
which is also revealed by (Dong et al., 2021). To the best of our knowledge, Dong et al. (2021) is the
only existing work on detecting backdoor-infected DNNs in the black-box settings. However, their
method requires the predictive conﬁdence score for each input to perform the NES algorithm (Wier-
stra et al., 2014), which weakens its practicability. Our work differs from all previous methods in
that we address a purely black-box setup where only the hard output label is accessible. Both model
parameters and training examples are inaccessible in the black-box hard-label setting."
RELATED WORK,0.05605786618444846,"2
PRELIMINARIES: BLACK-BOX BACKDOOR ATTACK AND DEFENSE"
THREAT MODEL,0.05786618444846293,"2.1
THREAT MODEL"
THREAT MODEL,0.059674502712477394,"Our considered threat model contains two parts: the adversary and the defender. The threat model
of the adversary follows previous works (Chen et al., 2017; Gu et al., 2019; Liu et al., 2018). In this
model, the attacker can inject an arbitrary amount of backdoor samples into the training dataset and
cause target misclassiﬁcation to a speciﬁc label without affecting the model’s accuracy on normal
examples. From the perspective of the defender, we consider the threat model with the weakest
assumption, in which the poisoned training dataset and the parameters of the target DNNs f(·) are
inaccessible. Moreover, the defender can only obtain the ﬁnal predictive label for each input from
the target DNNs and a validation set (40 images for each class). Therefore, the defender can only
query the target DNNs to obtain its ﬁnal decisions. The defender’s goal is to identify whether the
target DNNs is infected and which label is infected."
PROBLEM DEFINITION,0.06148282097649186,"2.2
PROBLEM DEFINITION"
PROBLEM DEFINITION,0.06329113924050633,"Consistent with prior studies (Dong et al., 2021; Kolouri et al., 2020; Wang et al., 2019; Huang et al.,
2022), we deem a DNN is backdoor infected if one can make an arbitrary input misclassiﬁed as the
target label, with minor modiﬁcation to the input. Without loss of generability, given the original"
PROBLEM DEFINITION,0.0650994575045208,Published as a conference paper at ICLR 2022
PROBLEM DEFINITION,0.06690777576853527,"input x ∈Rn, the modiﬁed input containing the backdoor trigger can be formulated as:"
PROBLEM DEFINITION,0.06871609403254973,"ˆx = b(x) = (1 −m) ⊙x + m ⊙∆,
(1)"
PROBLEM DEFINITION,0.0705244122965642,"where ∆∈Rn represents the backdoor trigger and ⊙represents the element-wise product. m ∈
{0, 1}n is a binary mask that ensures the position and magnitude of the backdoor trigger. Typically,
||m||1 ought to be very small to ensure the visually-indistinguishability between ˆx and x."
PROBLEM DEFINITION,0.07233273056057866,"We deﬁne f(·; θ) as the target DNN with parameters θ, which is a function that maps the input x
to a label y. φ(x, y; θ) ∈[0, 1] is deﬁned as the probability that x is classiﬁed as y by f(·; θ). The
argument θ is sometimes omitted in favor of simplicity. For f to mislabel ˆx as the target label yt,
there are two possible reasons: the backdoored training data either contains the same trigger but
mislabeled as yt or contains similar features as the normal inputs (not belonging to yt, but visually
similar to yt) and labeled as yt."
PROBLEM DEFINITION,0.07414104882459313,"ˆθ = arg min
θ No
X"
PROBLEM DEFINITION,0.0759493670886076,"i=1
ℓ(φ(xi, yi; θ), yi) + Nb
X"
PROBLEM DEFINITION,0.07775768535262206,"j=1
ℓ(φ(ˆxj, yt; θ), yt),
(2)"
PROBLEM DEFINITION,0.07956600361663653,"where there are No original training examples and Nb backdoor examples. ℓ(·) represents the cross-
entropy loss function; xi and yi represent the training sample and its corresponding ground-truth
label, respectively. In the inference phase, the target DNN will predict each ˆx as the target label yt,
which can be formulated as: f(ˆx) = yt. Prior works (Dong et al., 2021; Wang et al., 2019; Chen
et al., 2019b) seek to reveal the existence of a backdoor through investigating whether there exists a
backdoor trigger with minimal ||m||1 that can cause misclassiﬁcation to a certain label."
PROBLEM DEFINITION,0.081374321880651,"We mainly study the sparse mask since it is widely used in previous works (Wang et al., 2020; Dong
et al., 2021; Chen et al., 2019b; Guo et al., 2020; Wang et al., 2019; Gu et al., 2019). In cases when
the mask becomes dense, the mask m has to contain low magnitude continuous values to ensure
the backdoor examples visually indistinguishable. However, when the value of m is small, every
normal example xi becomes near the decision boundary of function φ. It is revealed by Taylor the-
orem, φ(ˆxi) = φ((1 −m)xi + m∆) = φ(xi + m(∆−xi)) ≈φ(xi) + m(∆−xi)⊺∇xφ. And the
function output label changes dramatically from φ(xi) to φ(ˆxi), so the gradient ∇xφ has to have a
large magnitude, which results in a non-robust model sensitive to subtle input change. Such mod-
els become impractical because a small random perturbation on the input can lead to signiﬁcantly
different model output. More details with empirical study can be found in the Appendix A."
BLACK-BOX BACKDOOR DETECTION AND ITS CHALLENGES,0.08318264014466546,"2.3
BLACK-BOX BACKDOOR DETECTION AND ITS CHALLENGES"
BLACK-BOX BACKDOOR DETECTION AND ITS CHALLENGES,0.08499095840867993,"Most previous works (Wang et al., 2019; Dong et al., 2021; Chen et al., 2019b) focus on reverse-
engineering the backdoor trigger for each target label yt using the following optimization:"
BLACK-BOX BACKDOOR DETECTION AND ITS CHALLENGES,0.0867992766726944,"arg min
m,∆ X"
BLACK-BOX BACKDOOR DETECTION AND ITS CHALLENGES,0.08860759493670886,"i
ℓ(φ((1 −m) ⊙xi + m ⊙∆, yt), yt) + β||m||1,
(3)"
BLACK-BOX BACKDOOR DETECTION AND ITS CHALLENGES,0.09041591320072333,"where β is the balancing parameter. Unfortunately, solving Eq. 3 is notoriously hard in the black-
box hard-label setting since θ is unknown. Additional difﬁculty comes with the fact that an effective
zero-th order gradient estimation requires each example xi superimposed with the trigger to be close
to the decision boundary of yt (Chen et al., 2020a; Cheng et al., 2020; Li et al., 2020). However,
such condition is hard to achieve in the hard-label blackbox settings."
ADVERSARIAL EXTREME VALUE ANALYSIS FOR BACKDOOR DETECTION,0.0922242314647378,"3
ADVERSARIAL EXTREME VALUE ANALYSIS FOR BACKDOOR DETECTION"
ADVERSARIAL EXTREME VALUE ANALYSIS FOR BACKDOOR DETECTION,0.09403254972875226,"Our AEVA framework is introduced in this section. We ﬁrst derive an upper bound to the backdoor
detection objective in Eq. 3 and ﬁnd its connection to adversarial attack. The adversarial singularity
phenomenon is further revealed in both theoretical results and empirical studies. Based on our
ﬁndings, we propose the Global Adversarial Peak (GAP) measure, computed by extreme value
analysis on the adversarial perturbation. The GAP score will be ﬁnally used with Median Absolute
Deviation (MAD) to detect backdoors in neural networks. Monte Carlo gradient estimation is further
introduced in order to solve the adversarial objective in the black-box setting."
ADVERSARIAL EXTREME VALUE ANALYSIS FOR BACKDOOR DETECTION,0.09584086799276673,Published as a conference paper at ICLR 2022
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.09764918625678119,"3.1
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.09945750452079566,"We present in this section the connection between the backdoor objective in Eq. 3 with the objective
in adversarial attacks. To begin with, we ﬁrst transform the backdoor masking equation (1 −m) ⊙
xi + m ⊙∆= xi + µ −m ⊙xi, where µ = m ⊙∆. Following this, the optimization in Eq. 3
converts to minimizing the following term: F = 1 N N
X"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.10126582278481013,"i=1
ℓ(φ(xi + µ −m ⊙xi, yt; θ), yt) .
(4)"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.10307414104882459,We further introduce an important result from the Multivariate Taylor Theorem below.
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.10488245931283906,"Lemma 1. Given a continuous differentiable function f(·) deﬁned on Rn and vectors x, h ∈Rn,
for any M ≥|∂f/∂hi| , ∀1 ≤i ≤n, we have |f(x + h) −f(x)| ≤M∥h∥1."
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.10669077757685352,"The lemma can be proved using the Lagrange Remainder. Please refer to the Appendix B.1 for a
detailed proof. According to the lemma, we have
F −1 N N
X"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.10849909584086799,"i=1
ℓ(φ(xi + µ, yt; θ), yt) ≤1 N N
X"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.11030741410488246,"i=1
C∥m ⊙xi∥1 ≤C∥m∥1 = C∥µ∥0 ,
(5)"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.11211573236889692,"where the latter inequality holds because each example xi ≤1 is bounded and the last equality
∥m∥1 = ∥m⊙∆∥0 = ∥µ∥0 holds because m is a binary mask. Then, we can see that the adversarial
objective is an upper bound of the objective in backdoor detection, i.e., F ≤1 N
P"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.11392405063291139,"i ℓ(φ(xi+µ), yt)+
C∥µ∥0. Instead of optimizing the original Eq. 3, we here propose to minimize the ℓ0-regularized
objective. While ℓ0 enforces the sparsity of the solution, optimizing it is NP-hard. In practice, it is
replaced with an ℓ1-norm as an envelope of the objective (Ramirez et al., 2013; Donoho, 2006):"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.11573236889692586,"ˆµ = arg min
µ X"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.11754068716094032,"i
ℓ(φ(xi + µ, yt; θ), yt) + λ∥µ∥1 .
(6)"
ADVERSARIAL OBJECTIVE AS AN UPPER BOUND OF THE BACKDOOR OBJECTIVE,0.11934900542495479,"This adversarial optimization can be solved using Monte Carlo gradient estimation in our black-box
setting. More details are described in Section 3.4. In the following, we will ﬁrst discuss how the
solution to this adversarial objective can be used to detect backdoors in neural networks."
THE ADVERSARIAL SINGULARITY PHENOMENON,0.12115732368896925,"3.2
THE ADVERSARIAL SINGULARITY PHENOMENON"
THE ADVERSARIAL SINGULARITY PHENOMENON,0.12296564195298372,"A natural and important question here is what the adversarial perturbation ˆµ would look like when
the input is infected with backdoor attacks. A direct analysis on deep neural networks is hard. So,
we start by analyzing a linear model to shed light on the intuition of our approach."
THE ADVERSARIAL SINGULARITY PHENOMENON,0.12477396021699819,"Lemma 2. Given a linear model parameterized by θ optimized on No original training examples
and Nb backdoored training examples with an objective in Eq. 2 and a mean-squared-error loss, the
adversarial solution ˆµ optimized using Eq. 6 will be dominated by input dimensions corresponding
to backdoor mask m when Nb is large, i.e., limNb→∞∥(1 −m) ⊙ˆµ∥1/∥ˆµ∥1 = 0."
THE ADVERSARIAL SINGULARITY PHENOMENON,0.12658227848101267,"The lemma can be proved by least square solutions and exact gradient computation. Detailed proof
can be found in the Appendix B.2. The lemma reveals that the majority of the mass in the adversarial
perturbation will be occupied in the mask area. Since the mask m is usually sparse, it is reasonable
to expect a highly skewed distribution in the adversarial map ˆµ."
THE ADVERSARIAL SINGULARITY PHENOMENON,0.12839059674502712,"We suspect that such skewed distributions might also occur in the adversarial map of a deep neural
network. While a thorough analysis on DNN is hard, recent studies in Neural Tangent Kernel (NTK)
(Jacot et al., 2018) have shown that a deep neural network with inﬁnite width can be treated as kernel
least square. So, we further extended our analysis to a k-way kernel least square classiﬁer as the
model and a cross-entropy loss used in the adversarial analysis."
THE ADVERSARIAL SINGULARITY PHENOMENON,0.1301989150090416,"Lemma 3. Suppose the training dataset consists of No original examples and Nb backdoor exam-
ples, i.i.d. sampled from uniform distribution and belonging to k classes. Let φ be a multivariate
kernel regression with the objective in Eq. 2, an RBF kernel. Then the adversarial solution ˆµ to Eq.
6 under cross-entropy loss should satisfy that limNb→∞E[(1 −m) ⊙ˆµ] = 0."
THE ADVERSARIAL SINGULARITY PHENOMENON,0.13200723327305605,Published as a conference paper at ICLR 2022
THE ADVERSARIAL SINGULARITY PHENOMENON,0.13381555153707053,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
THE ADVERSARIAL SINGULARITY PHENOMENON,0.13562386980108498,(a) Backdoor map ∆ x
THE ADVERSARIAL SINGULARITY PHENOMENON,0.13743218806509946,"0
5
10
15
20
25
30 y 0 5 10 15 20 25 30 z 0.005 0.010 0.015 0.020 0.025"
THE ADVERSARIAL SINGULARITY PHENOMENON,0.13924050632911392,(b) ˆµ for the infected label x
THE ADVERSARIAL SINGULARITY PHENOMENON,0.1410488245931284,"0
5
10
15
20
25
30 y 0 5 10 15 20 25 30 z 0.000 0.005 0.010 0.015 0.020 0.025"
THE ADVERSARIAL SINGULARITY PHENOMENON,0.14285714285714285,(c) ˆµ for the uninfected label
THE ADVERSARIAL SINGULARITY PHENOMENON,0.14466546112115733,"Figure 2: The distribution of values in the normalized adversarial perturbation µ for the infected (b)
and uninfected (c) labels, using a backdoor map in (a). x, y axes represent the image space; z axis
represents the absolute value of the normalized µ corresponding to each pixel location."
THE ADVERSARIAL SINGULARITY PHENOMENON,0.14647377938517178,"The proof can be found in Appendix B.3. The lemma reveals a similar result as the linear case
that the majority mass of the adversarial perturbation (in expectation) stays in the mask area when
sufﬁcient backdoor examples are used. Although the lemma does not directly address a deep neural
network, we hope this result could help understand the problem better in the NTK regime."
THE ADVERSARIAL SINGULARITY PHENOMENON,0.14828209764918626,"We perform an empirical study on DNNs to validate our intuition. Following previous works (Dong
et al., 2021; Wang et al., 2019; 2020), we implement BadNets (Gu et al., 2019) as the backdoor
attack method. We randomly select a label as the infected label and train a ResNet-44 on CIFAR-10
embedded with the backdoor attack, resulting in an attack success rate of 99.87%. The backdoor
trigger is a 4x4 square shown in Fig. 2(a), which occupies at most 1.6% of the whole image. The
adversarial perturbations ˆµ (by optimizing Eq. 6) are generated for both infected and uninfected
labels. The perturbations are normalized to a heat map as |ˆµ|/||ˆµ||1 and shown in Fig. 2. A high
concentration of mass is clearly observed from the infected case in Figure 2(b). We call it adversarial
singularity phenomenon. This motivates our adversarial extreme value analysis (AEVA) method."
ADVERSARIAL EXTREME VALUE ANALYSIS,0.15009041591320071,"3.3
ADVERSARIAL EXTREME VALUE ANALYSIS 0.02 max 0.00 0.05 0.10 0.15 0.20 0.25"
ADVERSARIAL EXTREME VALUE ANALYSIS,0.1518987341772152,Proportion
ADVERSARIAL EXTREME VALUE ANALYSIS,0.15370705244122965,"Uninfected
Infected"
ADVERSARIAL EXTREME VALUE ANALYSIS,0.15551537070524413,"Figure 3: The distributions of ad-
versarial peak µmax for infected
labels and uninfected labels."
ADVERSARIAL EXTREME VALUE ANALYSIS,0.15732368896925858,"A natural way to identify singularity is to look at the peak value of
the distribution. We deﬁne the adversarial peak µmax = max µij
for an adversarial perturbation µ. To gain a statistical understand-
ing of the relationship between adversarial peak and backdoor at-
tacks, We randomly sample 1000 normalized µ for the uninfected
and infected labels and plot the distribution of µmax in Fig. 3. The
experimental setup is consistent with the empirical study in Sec.
3.2. We notice from the ﬁgure that µmax of the infected labels
reaches a much larger range compared to that of uninfected la-
bels. However, they still overlap; around 47% infected µmax stays
within the uninfected range (the area with both blue and orange
bars). This result suggests that"
ADVERSARIAL EXTREME VALUE ANALYSIS,0.15913200723327306,"the adversarial singularity phenomenon does not always occur in backdoor-infected DNNs. It is rea-
sonable because some samples are inherently vulnerable to adversarial attacks and their adversarial
perturbations have similar properties as backdoors, which reduces the chance of singularity."
ADVERSARIAL EXTREME VALUE ANALYSIS,0.1609403254972875,"Now, we know if we deﬁne a threshold T as the maximum value of the uninfected labels, the
probability of a backdoor-infected µmax being smaller than T is 0.47, i.e., P(µmax < T) = 0.47.
Taking one example is impossible to know whether a label is infected. However, according to the
Univariate Theory (Gomes & Guillou, 2015), if we sample k examples of µmax for the same label,"
ADVERSARIAL EXTREME VALUE ANALYSIS,0.162748643761302,"P(max{µ1
max, µ2
max, . . . , µk
max} < T) = (P(µmax < T))k .
(7)
Here we name the maximum value over all the k adversarial peaks as the Global Adversarial Peak
(GAP). We vary the choice of k in Figure 4 and we can see that the chance of GAP value being
lower than T is diminishing. For example, if we take k = 6, then P(GAP < T) = 0.476 = 0.01
and the success rate of identifying a backdoor-infected label is 99%. Please be advised that, given"
ADVERSARIAL EXTREME VALUE ANALYSIS,0.16455696202531644,Published as a conference paper at ICLR 2022
ADVERSARIAL EXTREME VALUE ANALYSIS,0.16636528028933092,"the long-tail distribution of the infected µmax, this threshold T is not sensitive and can be made even
larger to be safe to include all uninfected labels. However, for larger T values, we need a larger k to
ensure a desired success rate."
BLACK-BOX OPTIMIZATION VIA GRADIENT ESTIMATION,0.16817359855334538,"3.4
BLACK-BOX OPTIMIZATION VIA GRADIENT ESTIMATION"
BLACK-BOX OPTIMIZATION VIA GRADIENT ESTIMATION,0.16998191681735986,"100
101
102"
BLACK-BOX OPTIMIZATION VIA GRADIENT ESTIMATION,0.1717902350813743,Sample size (k) for thresholding 0.5 0.6 0.7 0.8 0.9 1.0
BLACK-BOX OPTIMIZATION VIA GRADIENT ESTIMATION,0.1735985533453888,Success Rate
BLACK-BOX OPTIMIZATION VIA GRADIENT ESTIMATION,0.17540687160940324,"Figure 4: The proportion of in-
fected labels whose max{µi
max}
larger than that of uninfected ones."
BLACK-BOX OPTIMIZATION VIA GRADIENT ESTIMATION,0.17721518987341772,"After investigating the unique properties of backdoor-infected
DNNs in the white-box settings, we here illustrate how to ex-
ploit such properties under a rather restricted scenario as we con-
sider. In the white-box settings, the free access to θ can enable
us to compute the ∇xφ(x, yt) accurately, resulting in µ with min-
imum ||µ||1 through optimizing Eq. 6. Regarding the black-box
settings, we propose to leverage the zero-order gradient estima-
tion technique (Fu & Hu, 2012) to address the challenge for calcu-
lating ∇xφ(x, yt). We choose Monte Carlo based method (Fu &
Hu, 2012) for obtaining the estimated gradient f
∇φx(x, yt), which
sends several inputs and utilizes the corresponding outputs to esti-
mate the gradient. More details about gradient estimation can be
found in Appendix C."
FINAL ALGORITHM,0.17902350813743217,"3.5
FINAL ALGORITHM Input ..."
FINAL ALGORITHM,0.18083182640144665,Adversarial Perturbation ...
FINAL ALGORITHM,0.18264014466546113,Aggregated GAP
FINAL ALGORITHM,0.1844484629294756,0.2701
FINAL ALGORITHM,0.18625678119349007,MAD Detector
FINAL ALGORITHM,0.18806509945750452,Anomaly Index
FINAL ALGORITHM,0.189873417721519,"Query
Label"
FINAL ALGORITHM,0.19168173598553345,Black-box Backdoored
FINAL ALGORITHM,0.19349005424954793,DNNs (Trigger:   )
FINAL ALGORITHM,0.19529837251356238,Gradient Estimation
FINAL ALGORITHM,0.19710669077757687,0.1494 ...
FINAL ALGORITHM,0.19891500904159132,"7.6219
(Malicious)"
FINAL ALGORITHM,0.2007233273056058,1.0133 ... Dog
FINAL ALGORITHM,0.20253164556962025,Figure 5: Overview of AEVA – Adversarial Extreme Value Analysis.
FINAL ALGORITHM,0.20433996383363473,"Putting all the above ideas together, we present the AEVA framework (illustrated in Fig. 5). The
algorithm for computing the GAP value of a given label is described in Algorithm 1. Our approach
requires a batch of legitimate samples Xi for each class i. For each label yt to be analyzed, the
algorithm will tell whether yt is backdoor-infected. We treat yt as the targeted label. Then we
collect a batch of legitimate samples belonging to yt, denoted as Xt with the same batch size as
that of Xi. Then, we leverage Xt to make each sample within every {Xi}n
i=0 to approach the
decision boundary of yt. This is computed by optimizing ˆµ from Eq. 6 using Monte Carlo gradient
estimation. The adversarial peak µmax is then computed for each example. The global adversarial
peak (GAP) is aggregated over all labels. After calculating the GAP value R(yi) for each class i,
following previous work (Wang et al., 2019; Dong et al., 2021), we implement Median Absolute
Deviation (MAD) to detect the outliers among {R(yi)}n
i=1. Speciﬁcally, we use MAD to calculate
the anomalous index for each R(yi) by assuming {R(yi)}n
i=1 ﬁts Gaussian distribution. The outlier
is then detected by thresholding the anomaly index."
EXPERIMENTS,0.20614828209764918,"4
EXPERIMENTS"
SETTINGS,0.20795660036166366,"4.1
SETTINGS"
SETTINGS,0.20976491862567812,"Datasets. We evaluate our approach on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. For
each task, We build 240 infected and 240 benign models with different architectures: ResNets (He
et al., 2016) and DenseNets (Huang et al., 2017). We randomly select one label for each backdoor-
infected model and inject sufﬁcient poisoned samples to ensure the attack success rate ≥98%. More
details can be found in the Appendix."
SETTINGS,0.2115732368896926,Published as a conference paper at ICLR 2022
SETTINGS,0.21338155515370705,"BadNets
Watermark"
SETTINGS,0.21518987341772153,"Label Consistent
Invisible Attack"
SETTINGS,0.21699819168173598,Uninfected
SETTINGS,0.21880650994575046,"Figure 6: The visualization of adversarial perturbation ˆµ for infected labels with different backdoor attacks on
TinyImageNet. For each attack, we show the trigger (left), ˆµ map (middle) and ˆµ distribution in 3D (right)."
SETTINGS,0.2206148282097649,"Task
Detection Accuracy (ACC)
BadNets
Watermark
Label Consistent
Invisible Attack
Total(Infected)
Benign"
SETTINGS,0.2224231464737794,"CIFAR-10
86.7%
93.3%
93.3%
93.3%
91.7%
95.4%
CIFAR-100
93.3%
100%
96.7%
96.7%
96.7%
97.5%
TinyImageNet
96.7%
100%
93.3%
96.7%
96.7%
99.2%"
SETTINGS,0.22423146473779385,Table 1: Overall performance of our approach on three tasks.
SETTINGS,0.22603978300180833,"Attack methods. Four different attack methods are implemented: BadNets (Gu et al., 2019), Wa-
termark attack (Chen et al., 2017), Label-Consistent Attack (Turner et al., 2019) and Invisible At-
tack (Li et al., 2021a). For BadNets and Watermark, the triggers are 4×4 squares. The transparency
ratio for Watermark attack is 0.1. For label-consistent and invisible attacks, we use the default less-
visible triggers. Around 10% poison data is injected in training. Each model is trained for 200
epochs with data augmentation. More details can be found in the Appendix G."
SETTINGS,0.22784810126582278,"Baseline methods. Since there is no existing black-box hard-label method, we compare our ap-
proach with two state-of-art white box backdoor detection methods: Neural Cleanse (NC) (Wang
et al., 2019) and DL-TND (Wang et al., 2020)2. For each task, we use 200 samples for the gradient
estimation, and the batch size for each {Xi}n
i=1 is set to 40 in Algorithm 1."
SETTINGS,0.22965641952983726,"Outlier detection. To accurately identify the anomalous R(yi) among {R(yi)}n
i=1, we assume
the scores ﬁt to a normal distribution and apply a constant estimator (1.4826) to normalize the
computed anomaly index similar to (Wang et al., 2019). We set the threshold value τ = 4 for our
MAD detector, which means we identify the class whose corresponding anomaly index larger than
4 as infected. This value is chosen using a hold-out validation set of infected and benign models."
SETTINGS,0.2314647377938517,"Evaluation metrics. We follow previous work (Kolouri et al., 2020; Wang et al., 2020; Dong et al.,
2021; Guo et al., 2020) on using two common metrics: (a) The Area under Receiver Operating
Curve (AUROC) – The Receiver Operating Curve (ROC) shows the trade-off between detection
success rate for infected models and detection error rate for benign models across different decision
thresholds τ for anomaly index; (b) Detection Accuracy (ACC) – The proportion of models are cor-
rectly identiﬁed. Regarding infected models, they are correctly identiﬁed if and only if the infected
labels are identiﬁed without mistagging other uninfected labels."
RESULTS,0.2332730560578662,"4.2
RESULTS"
RESULTS,0.23508137432188064,"We ﬁrst investigate whether AEVA can reveal the singularity phenomenon of labels infected with
different attack approaches. We randomly select an infected model infected by each attack approach
and an uninfected model for TinyImageNet task. Notably, all these selected models are correctly
identiﬁed by our approach. We plot the corresponding normalized adversarial perturbation µ in
Fig. 6, which demonstrates that AEVA can accurately and distinguish the uninfected and infected
labels and reveal the singularity phenomena of backdoor triggers."
RESULTS,0.23688969258589512,"Table 1 presents the overall results.
AEVA can accurately detect the infected label with ACC
≥86.7% across all three tasks and various trigger settings. We compare our approach with ex-
isting white-box detection approaches including Neural Cleanse (NC) (Wang et al., 2019) and DL-
TND (Wang et al., 2020). The comparison results over all infected and benign models are shown in"
RESULTS,0.23869801084990958,"2We implement NC (Wang et al., 2019) and DL-TND (Wang et al., 2020) following https://github.
com/bolunwang/backdoor and https://github.com/wangren09/TrojanNetDetector"
RESULTS,0.24050632911392406,Published as a conference paper at ICLR 2022
RESULTS,0.2423146473779385,"0.0
0.1
0.2
0.3
0.4
0.5
Detection error rate for"
RESULTS,0.244122965641953,benign models 0.70 0.75 0.80 0.85 0.90 0.95 1.00
RESULTS,0.24593128390596744,Detection success rate
RESULTS,0.24773960216998192,for infected models
RESULTS,0.24954792043399637,"NC(White-box)
DL-TND(White-box)
AEVA"
RESULTS,0.2513562386980108,(a) CIFAR-10
RESULTS,0.25316455696202533,"0.0
0.1
0.2
0.3
0.4
0.5
Detection error rate for"
RESULTS,0.2549728752260398,benign models 0.70 0.75 0.80 0.85 0.90 0.95 1.00
RESULTS,0.25678119349005424,Detection success rate
RESULTS,0.2585895117540687,for infected models
RESULTS,0.2603978300180832,"NC(White-box)
DL-TND(White-box)
AEVA"
RESULTS,0.26220614828209765,(b) CIFAR-100
RESULTS,0.2640144665461121,"0.0
0.1
0.2
0.3
0.4
0.5
Detection error rate for"
RESULTS,0.26582278481012656,benign models 0.70 0.75 0.80 0.85 0.90 0.95 1.00
RESULTS,0.26763110307414106,Detection success rate
RESULTS,0.2694394213381555,for infected models
RESULTS,0.27124773960216997,"NC(White-box)
DL-TND(White-box)
AEVA"
RESULTS,0.2730560578661845,(c) TinyImageNet
RESULTS,0.27486437613019893,"Figure 7: The Receiver Operating Cure(ROC) for NC, DL-TND and AEVA on CIFAR-10, CIFAR-
100 and TinyImageNet tasks. AEVA is black-box while the other two methods are white-box."
RESULTS,0.2766726943942134,"Fig. 7. More results can be found in Appendix J. The results suggest that AEVA achieves compara-
ble performance with existing white-box detection approaches across different settings. Such close
performance also indicates the efﬁcacy of AEVA on black-box hard-label backdoor detection."
ABLATION STUDY,0.27848101265822783,"4.3
ABLATION STUDY"
ABLATION STUDY,0.28028933092224234,"2x2
6x6
10x10
14x14
18x18
Trigger Size 0.0 0.2 0.4 0.6 0.8 1.0 ACC"
ABLATION STUDY,0.2820976491862568,BadNets
ABLATION STUDY,0.28390596745027125,Watermark
ABLATION STUDY,0.2857142857142857,"Figure 8: The impact of trigger
size on detection accuracy."
ABLATION STUDY,0.2875226039783002,"The impact of trigger size. We test AEVA on TinyImageNet with
different trigger sizes for BadNets and Watermark attacks. For
each trigger size and attack method, we build 60 infected models
with various architectures following the conﬁgurations in Sec. 4.2.
Fig. 8 shows that AEVA remains effective when trigger size is less
than 14×14 with ACC ≥71.7%. For large triggers, AEVA cannot
identify the infected label since the singularity property is allevi-
ated. This is consistent with our theoretical analysis. Even though
large trigger attacks can bypass AEVA, they are either visually-
distinguishable or leading to a non-robust model sensitive to input
change, making it less stealthy and impractical."
ABLATION STUDY,0.28933092224231466,"1
2
3
4
5
6
7
The number of infected labels 0.0 0.2 0.4 0.6 0.8 1.0 ACC"
ABLATION STUDY,0.2911392405063291,BadNets
ABLATION STUDY,0.29294755877034356,Watermark
ABLATION STUDY,0.29475587703435807,"Figure 9: The impact of infected
label size on detection accuracy."
ABLATION STUDY,0.2965641952983725,"The impact of infected label numbers. We further investigate
the impact of the number of infected labels. We build 60 infected
models for CIFAR-10 to evaluate AEVA. We randomly select in-
fected labels, and we inject a 2 × 2 trigger for each infected label.
Fig. 9 shows that AEVA performs effectively when the number of
infected labels is less than 3 (i.e., 30% of the entire labels) with
ACC ≥78.3%. It is because too many infected labels fail the
MAD outlier detection. Interestingly, existing white-box detection
methods (Wang et al., 2019; Chen et al., 2019b; Guo et al., 2020;
Liu et al., 2019; Wang et al., 2020) cannot perform effectively in
such scenarios either. However, too many infected labels would
reduce the stealth of the attacker as well."
ABLATION STUDY,0.298372513562387,"Besides these studies, we further evaluate our approach under more scenarios, including the impact
of the number of available labels, multiple triggers for a single infected label, different backdoor
trigger shapes and potential adaptive backdoor attacks, which are included in the Appendix."
CONCLUSION,0.30018083182640143,"5
CONCLUSION"
CONCLUSION,0.30198915009041594,"This paper takes a ﬁrst step addressing the black-box hard-label backdoor detection problem. We
propose the adversarial extreme value analysis (AEVA) algorithm, which is based on an extreme
value analysis on the adversarial map, computed from the monte-carlo gradient estimation due to the
black-box hard-label constraint. Extensive experiments demonstrate the efﬁcacy of AEVA across a
set of popular tasks and state-of-the-art backdoor attacks."
CONCLUSION,0.3037974683544304,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.30560578661844484,ETHICS STATEMENT
ETHICS STATEMENT,0.3074141048824593,"Our work aims at providing neural network practitioners additional protection against backdoor
attacks. We believe our work could contribute positively to the human society and avoid potential
harm since it addresses a critical safety problem. We are unaware of any direct negative impact out
of this work. Our method has certain limitations such as the sensitivity to large backdoor trigger
and multiple infected labels. However, as we earlier in the paper, these scenarios make the attack
become less stealthy and not practical in real applications."
REPRODUCIBILITY STATEMENT,0.3092224231464738,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.31103074141048825,"Our work is built upon comprehensive theoretical results and clear motivations. We believe the pro-
posed method can be reproduced according to the content in the paper, e.g., Algorithm 1. In addi-
tion, we have released the implementation of AEVA in https://github.com/JunfengGo/
AEVA-Blackbox-Backdoor-Detection-main."
REFERENCES,0.3128390596745027,REFERENCES
REFERENCES,0.31464737793851716,"Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. In SafeAI@AAAI, 2019a. URL http://ceur-ws.org/Vol-2301/
paper_18.pdf."
REFERENCES,0.31645569620253167,"Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan
detection and mitigation framework for deep neural networks. In IJCAI, pp. 4658–4664, 2019b."
REFERENCES,0.3182640144665461,"Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efﬁcient
decision-based attack. In 2020 ieee symposium on security and privacy (sp), pp. 1277–1294.
IEEE, 2020a."
REFERENCES,0.32007233273056057,"Steven Chen, Nicholas Carlini, and David Wagner. Stateful detection of black-box adversarial at-
tacks. In Proceedings of the 1st ACM Workshop on Security and Privacy on Artiﬁcial Intelligence,
pp. 30–39, 2020b."
REFERENCES,0.321880650994575,"Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. CoRR, abs/1712.05526, 2017. URL http://arxiv.
org/abs/1712.05526."
REFERENCES,0.32368896925858953,"Yao Chen, Jiong He, Xiaofan Zhang, Cong Hao, and Deming Chen. Cloud-dnn: An open framework
for mapping dnn models to cloud fpgas. In Proceedings of the 2019 ACM/SIGDA international
symposium on ﬁeld-programmable gate arrays, pp. 73–82, 2019c."
REFERENCES,0.325497287522604,"Minhao Cheng, Simranjit Singh, Patrick H. Chen, Pin-Yu Chen, Sijia Liu, and Cho-Jui Hsieh. Sign-
opt: A query-efﬁcient hard-label adversarial attack. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=SklTQCNtvS."
REFERENCES,0.32730560578661844,"Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. Black-
box detection of backdoor attacks with limited information and data.
In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV), pp. 16482–16491, October
2021."
REFERENCES,0.3291139240506329,"D.L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306,
2006. doi: 10.1109/TIT.2006.871582."
REFERENCES,0.3309222423146474,"Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo,
Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, et al. A conﬁgurable cloud-
scale dnn processor for real-time ai. In 2018 ACM/IEEE 45th Annual International Symposium
on Computer Architecture (ISCA), pp. 1–14. IEEE, 2018."
REFERENCES,0.33273056057866185,"Michael C Fu and Jian-Qiang Hu. Conditional Monte Carlo: Gradient estimation and optimization
applications, volume 392. Springer Science & Business Media, 2012."
REFERENCES,0.3345388788426763,Published as a conference paper at ICLR 2022
REFERENCES,0.33634719710669075,"M Ivette Gomes and Armelle Guillou. Extreme value theory and statistics of univariate extremes: a
review. International statistical review, 83(2):263–292, 2015."
REFERENCES,0.33815551537070526,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sher-
jil Ozair,
Aaron Courville,
and Yoshua Bengio.
Generative adversarial nets.
In
Z. Ghahramani, M. Welling,
C. Cortes,
N. Lawrence,
and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems,
volume 27. Curran Associates,
Inc.,
2014.
URL
https://proceedings.neurips.cc/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf."
REFERENCES,0.3399638336347197,"Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring
attacks on deep neural networks. IEEE Access, pp. 47230–47244, 2019. doi: 10.1109/ACCESS.
2019.2909068."
REFERENCES,0.34177215189873417,"Wenbo Guo, Lun Wang, Yan Xu, Xinyu Xing, Min Du, and Dawn Song. Towards inspecting and
eliminating trojan backdoors in deep neural networks. In 2020 IEEE International Conference on
Data Mining (ICDM), pp. 162–171, 2020. doi: 10.1109/ICDM50108.2020.00025."
REFERENCES,0.3435804701627486,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.3453887884267631,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700–4708, 2017."
REFERENCES,0.3471971066907776,"Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling
the training process. In ICLR, 2022."
REFERENCES,0.34900542495479203,"Arthur Jacot, Franck Gabriel, and Clement Hongler.
Neural tangent kernel: Convergence and
generalization in neural networks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf."
REFERENCES,0.3508137432188065,"Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. Model-reuse attacks on deep
learning systems. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Com-
munications Security, pp. 349–363, 2018."
REFERENCES,0.352622061482821,"Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns:
Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 301–310, 2020."
REFERENCES,0.35443037974683544,"M.R.
Leadbetter.
On
a
basis
for
peaks
over
threshold
modeling.
Statistics
and
Probability Letters, 12(4):357–362, 1991.
ISSN 0167-7152.
doi:
https://doi.org/10.
1016/0167-7152(91)90107-3.
URL https://www.sciencedirect.com/science/
article/pii/0167715291901073."
REFERENCES,0.3562386980108499,"Huichen Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, and Bo Li. Qeba: Query-efﬁcient boundary-
based blackbox attack. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2020."
REFERENCES,0.35804701627486435,"Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible
backdoor attacks on deep neural networks via steganography and regularization. IEEE Transac-
tions on Dependable and Secure Computing, 18(5):2088–2105, 2021a. doi: 10.1109/TDSC.2020.
3021407."
REFERENCES,0.35985533453887886,"Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, and Shu-Tao Xia. Few-shot backdoor attacks
on visual object tracking. In ICLR, 2022."
REFERENCES,0.3616636528028933,"Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor
attack with sample-speciﬁc triggers. In ICCV, 2021b."
REFERENCES,0.36347197106690776,Published as a conference paper at ICLR 2022
REFERENCES,0.36528028933092227,"Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu
Zhang. Trojaning attack on neural networks. In 25nd Annual Network and Distributed System Se-
curity Symposium, NDSS 2018, San Diego, California, USA, February 18-221, 2018. The Internet
Society, 2018."
REFERENCES,0.3670886075949367,"Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs:
Scanning neural networks for back-doors by artiﬁcial brain stimulation. In Proceedings of the
2019 ACM SIGSAC Conference on Computer and Communications Security, pp. 1265–1282,
2019."
REFERENCES,0.3688969258589512,"Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reﬂection backdoor: A natural backdoor
attack on deep neural networks. In European Conference on Computer Vision, pp. 182–199.
Springer, 2020."
REFERENCES,0.3707052441229656,"Iacopo Masi, Yue Wu, Tal Hassner, and Prem Natarajan. Deep face recognition: A survey. In 2018
31st SIBGRAPI conference on graphics, patterns and images (SIBGRAPI), pp. 471–478. IEEE,
2018."
REFERENCES,0.37251356238698013,"Takafumi Okuyama, Tad Gonsalves, and Jaychand Upadhay. Autonomous driving system based on
deep q learnig. In 2018 International Conference on Intelligent Autonomous Systems (ICoIAS),
pp. 201–205. IEEE, 2018."
REFERENCES,0.3743218806509946,"Carlos Ramirez, Vladik Kreinovich, and Miguel Argaez. Why l1 is a good approximation to l0: A
geometric explanation. 7:203–207, 01 2013."
REFERENCES,0.37613019891500904,"Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor at-
tacks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 11957–
11965, 2020."
REFERENCES,0.3779385171790235,"Shrutika Singh, Harshita Arya, and P Arun Kumar.
Voice assistant for ubuntu implementation
using deep neural network. In Advanced Computing Technologies and Applications, pp. 11–20.
Springer, 2020."
REFERENCES,0.379746835443038,"Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks for object detec-
tion. 2013."
REFERENCES,0.38155515370705245,"Brandon Tran, Jerry Li, and Aleksander Madry.
Spectral signatures in backdoor attacks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
280cf18baf4311c92aa5a042336587d3-Paper.pdf."
REFERENCES,0.3833634719710669,"Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
Label-consistent backdoor attacks.
arXiv preprint arXiv:1912.02771, 2019."
REFERENCES,0.38517179023508136,"Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019
IEEE Symposium on Security and Privacy (SP), pp. 707–723. IEEE, 2019."
REFERENCES,0.38698010849909587,"Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang. Practical
detection of trojan neural networks: Data-limited and data-free cases.
In Proceedings of the
European Conference on Computer Vision (ECCV), 2020."
REFERENCES,0.3887884267631103,"Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J¨urgen Schmidhuber.
Natural evolution strategies. The Journal of Machine Learning Research, 15(1):949–980, 2014."
REFERENCES,0.39059674502712477,"Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao.
Latent backdoor attacks on deep
neural networks.
In Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security, CCS ’19, pp. 2041–2055, New York, NY, USA, 2019. Association
for Computing Machinery.
ISBN 9781450367479.
doi: 10.1145/3319535.3354209.
URL
https://doi.org/10.1145/3319535.3354209."
REFERENCES,0.3924050632911392,"Tongqing Zhai, Yiming Li, Ziqi Zhang, Baoyuan Wu, Yong Jiang, and Shu-Tao Xia. Backdoor
attack against speaker veriﬁcation. In ICASSP, 2021."
REFERENCES,0.39421338155515373,Published as a conference paper at ICLR 2022
REFERENCES,0.3960216998191682,APPENDIX
REFERENCES,0.39783001808318263,Table of Contents
REFERENCES,0.3996383363471971,"A Other Backdoor Attacks
14
A.1
Dense Watermark Backdoor Attacks. . . . . . . . . . . . . . . . . . . . . . . .
14
A.2
Feature Space Backdoor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14"
REFERENCES,0.4014466546112116,"B
Proofs
14
B.1
The proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
B.2
The proof of Lemma 2 – a theory on the linear case . . . . . . . . . . . . . . . .
15
B.3
Proof of Lemma 3 – An analysis for Kernel Regression . . . . . . . . . . . . . .
16"
REFERENCES,0.40325497287522605,"C Monte Carlos gradient estimation
18"
REFERENCES,0.4050632911392405,"D Algorithm for Computing Aggregated Global Adversarial Peak (GAP)
19"
REFERENCES,0.40687160940325495,"E
The detailed conﬁgurations for experimental datasets and triggers
19"
REFERENCES,0.40867992766726946,"F
The accuracy and attack success rate(ASR) for evaluated models
19"
REFERENCES,0.4104882459312839,"G The amounts of evaluated models for various task and attack approaches
20"
REFERENCES,0.41229656419529837,"H Details of visualization process
20"
REFERENCES,0.4141048824593128,"I
Detailed ROC for varous tasks and attack approaches
20"
REFERENCES,0.4159132007233273,"J
The comparison results between AEVA and NC, DL-TND
20"
REFERENCES,0.4177215189873418,"K Experiments for different triggers
21
K.1
Evaluation on dynamic and sparse but not compact triggers . . . . . . . . . . . .
21"
REFERENCES,0.41952983725135623,"L
Results for impact of the number of images per labels.
22"
REFERENCES,0.4213381555153707,"M The impact of the number of the labels for available images.
23"
REFERENCES,0.4231464737793852,"N Multiple triggers within the single infected label scenarios
23"
REFERENCES,0.42495479204339964,"O Potential adaptive backdoor attacks
24
O.1
Attacks with multiple target labels . . . . . . . . . . . . . . . . . . . . . . . . .
24
O.2
Attack with large triggers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
REFERENCES,0.4267631103074141,Published as a conference paper at ICLR 2022
REFERENCES,0.42857142857142855,"A
OTHER BACKDOOR ATTACKS"
REFERENCES,0.43037974683544306,"A.1
DENSE WATERMARK BACKDOOR ATTACKS."
REFERENCES,0.4321880650994575,"In short, we argue that low-magnitude dense backdoor attack(watermark) is not a practical attack
method and is less concerned compared to the threat model in our paper."
REFERENCES,0.43399638336347196,"First, the low-magnitude attack is a special case of our formulation where the mask m is a continuous
value (here we use a to represent transparency). The low-magnitude problem can be formulated as:"
REFERENCES,0.4358047016274864,"mina,∆
P"
REFERENCES,0.4376130198915009,"i ℓ(φ((1 −a)xi, a∆), yt) + λ|a|"
REFERENCES,0.4394213381555154,"This formulation does not have discrete variables so its black-box optimization is an easier task
compared to our problem (where m is discrete)."
REFERENCES,0.4412296564195298,"Second, we will show theoretically that, in this scenario, every normal example is near the decision
boundary of the network, because due to Taylor expansion and the fact that a is a small value,"
REFERENCES,0.4430379746835443,φ((1 −a)xi + a∆) = φ(xi + a(∆−xi)) ≈φ(xi) + a(∆−xi)⊺∇xφ
REFERENCES,0.4448462929475588,"We know from xi to (1 −a)xi + a∆, the function output label is changed. So every example xi is
at the decision boundary of the function φ."
REFERENCES,0.44665461121157324,"So the infected model is actually very sensitive to random noise and becomes ineffective in attacking
purposes."
REFERENCES,0.4484629294755877,"To empirically support this claim, we conduct experiments on CIFAR-10 using ResNet-44. The
results are reported over average among ten different target labels. We select watermark triggers
randomly which occupy the entire image with 0.1 transparency rate (a). We here add noise which
ﬁts uniform distribution with a low magnitude as 0.025 to each normal sample. The proportion of
normal samples misclassiﬁed by the infected label is shown below. We ﬁnd that such dense low-
magnitude watermark triggers would make normal samples to be misclassiﬁed as the target label yt
via adding small random noise. Such observations can easily identify models infected with a dense
and low-magnitude watermark trigger."
REFERENCES,0.45027124773960214,"• Using a global watermark trigger, 67.8% of the normal samples are misclassiﬁed to other
labels(Infected label)
• Using a 16x16 watermark trigger, 6.42% of the normal samples are misclassiﬁed to other
labels(Infected label)
• Using a 4x4 watermark trigger leads to 1.41% of the normal samples misclassiﬁed
• Using a 2x2 watermark trigger leads to 0.24% of the normal samples misclassiﬁed
• Training with normal examples results in 0.2% of the normal samples misclassiﬁed."
REFERENCES,0.45207956600361665,"A.2
FEATURE SPACE BACKDOOR"
REFERENCES,0.4538878842676311,"Feature space backdoor attack which employs a generative model to transfer the style of a normal
image into a trigger. However, the style transfer leads to signiﬁcant visual changes which becomes
less stealthy. For physical attacks like surveillance cameras, this approach is hard to implement
because it requires changing the background as well. While our work is mostly focused on the input
space trigger, we believe the latent space problem is beyond our threat model as well as focused
problem and could be an interesting future work (for example, consider using an off-the-shelf style
transfer model to evaluate the model’s sensitivity towards different style effects."
REFERENCES,0.45569620253164556,"B
PROOFS"
REFERENCES,0.45750452079566006,"B.1
THE PROOF OF LEMMA 1"
REFERENCES,0.4593128390596745,"Lemma 1.
Given a continuous differential function f deﬁned on Rn and vectors x, h ∈Rn, there
exists a real value M ≥|∂f/∂hi| ∀1 ≤i ≤n such that"
REFERENCES,0.46112115732368897,"|f(x + h) −f(x)| ≤M∥h∥1 .
(8)"
REFERENCES,0.4629294755877034,Published as a conference paper at ICLR 2022
REFERENCES,0.46473779385171793,"Proof of Lemma 1:
The ﬁrst order Taylor expansion gives"
REFERENCES,0.4665461121157324,"f(x + h) = f(x) + R(h)
(9)"
REFERENCES,0.46835443037974683,"where R(h) is the Lagrange remainder, i.e.,"
REFERENCES,0.4701627486437613,"R(h) = n
X i=1"
REFERENCES,0.4719710669077758,∂f(x + ch)
REFERENCES,0.47377938517179025,"∂xi
· hi,
for some c ∈(0, 1).
(10)"
REFERENCES,0.4755877034358047,"Let M ≥|∂f(x)/∂xi| ∀x, we have"
REFERENCES,0.47739602169981915,"|R(h)| ≤M  n
X"
HI,0.47920433996383366,"1
hi"
HI,0.4810126582278481,"= M∥h∥1 .
(11)"
HI,0.48282097649186256,"So, we have
|f(x + h) −f(x)| ≤M∥h∥1 .
(12) □"
HI,0.484629294755877,"B.2
THE PROOF OF LEMMA 2 – A THEORY ON THE LINEAR CASE"
HI,0.4864376130198915,"We perform a theoretical study of linear models to shed light about our empirical ﬁnding. Here, we
restate Lemma 3 below for convenience."
HI,0.488245931283906,"Lemma 2.
Given a linear model parameterized by θ optimized on No original training examples
and Nb backdoored training examples with an objective in Eq. 2 and a mean-squared-error loss, the
adversarial solution ˆµ optimized using Eq. 6 will be dominated by input dimensions corresponding
to backdoor mask m when Nb is large, i.e.,"
HI,0.49005424954792043,"lim
Nb→∞
∥(1 −m) ⊙ˆµ∥1"
HI,0.4918625678119349,"∥ˆµ∥1
= 0 .
(13)"
HI,0.4936708860759494,"Proof of Lemma 2:
To ease the reading of notations, we use (·)a×b to represent matrix with a
rows and b columns, respectively. Assuming φ(x; θ) = θ⊺x is a linear model whose output is a
ﬂoating number, and the loss function is ℓ2, i.e., ℓ(x, y) = (x −y)2. Then Eq. 2 becomes a linear
regression problem and can be represented in the matrix form of least square, i.e.,"
HI,0.49547920433996384,"F = ∥Xθ −Y ∥2 + ∥ˆXθ −ˆY ∥2
(14)"
HI,0.4972875226039783,"where ˆxi = (1−m)⊙x′
i +m⊙∆is an backdoor example converted from a normal example x′
i and
ˆyi = yt. X is the matrix composed of row vectors xi and Y is the matrix composed of row vectors
yi. The solution of this least square can be found by setting gradient to zero, such that"
HI,0.49909584086799275,"ˆθ = (X⊺X + ˆX⊺ˆX)−1(X⊺Y + ˆX⊺ˆY ) .
(15)"
HI,0.5009041591320073,"It is known that a normal training without backdoors will lead to the pseudo-inverse solution θ =
(X⊺X)−1X⊺Y . Without loss of generality, we assume the backdoor trigger occupies the last q
dimensions. And the ﬁrst p dimensions are normal values, i.e., q = d −p + 1 where d is the
total number of dimensions. So, we can rewrite the input X = [X1:p, Xp:d] and the mask m =
[01×p, 11×(d−p+1)]. Let δ be the sub-vector of ∆such that δ = ∆p+1:d, a q dimensional row
vector. Then we have ˆX = [X1:p, 1Nb×1δ] where Nb is the total number of backdoor examples.
Then,"
HI,0.5027124773960217,"ˆX⊺ˆX =

X1:p
⊺X1:p
0
0
Nbδ⊺δ"
HI,0.5045207956600362,"
.
(16)"
HI,0.5063291139240507,"Similarly, we ﬁnd ˆY = 1yt and assume zero mean of the input, i.e., P xi = 0, then we have"
HI,0.5081374321880651,"ˆX⊺ˆY =

0p×1
Nbytδ⊺"
HI,0.5099457504520796,"
.
(17)"
HI,0.5117540687160941,Published as a conference paper at ICLR 2022
HI,0.5135623869801085,"While Nb becomes larger, both ˆX⊺ˆX and ˆX⊺ˆY increase signiﬁcantly. When Nb →∞, Eq. 15
becomes
ˆθ →( ˆX⊺ˆX)−1( ˆX⊺ˆY ) =

0p×1
yt(δ⊺δ)−1δ⊺"
HI,0.515370705244123,"
.
(18)"
HI,0.5171790235081374,"In another word, we know the normalized values in the non-masked area of θ approach 0 such that"
HI,0.5189873417721519,"lim
Nb→∞
∥(1 −m) ⊙ˆθ∥1"
HI,0.5207956600361664,"∥ˆθ∥1
= 0 .
(19)"
HI,0.5226039783001808,"Now, let us look at the adversarial objective in Eq. 8, which is equivalent to"
HI,0.5244122965641953,"G = ∥θ⊺(x + µ) −yt∥2 ,
(20)"
HI,0.5262206148282098,the gradient of which is
HI,0.5280289330922242,"∇µG = 2(θ⊺(x + µ) −yt)θ .
(21)"
HI,0.5298372513562387,"Since 2(θ⊺(x+µ)−yt) is a scalar and each of the gradient updates is conducted by µ = µ+λ∇µG,
we know no matter how many gradient update steps are applied, µ, initialized at 0, always moves
in the exact same direction of θ, i.e., ˆµ ∥θ. In this case, ˆµ will also be dominant by the last q
dimensions, or the dimensions corresponded to the mask m. In another word, we have"
HI,0.5316455696202531,"lim
Nb→∞
∥(1 −m) ⊙ˆµ∥1"
HI,0.5334538878842676,"∥ˆµ∥1
= 0 .
(22) □"
HI,0.5352622061482821,"B.3
PROOF OF LEMMA 3 – AN ANALYSIS FOR KERNEL REGRESSION"
HI,0.5370705244122965,"Lemma 3.
Suppose the training dataset consists of No original examples and Nb backdoor ex-
amples, i.i.d. sampled from uniform distribution and belonging to k classes. Each class contains
equally No/k normal examples. Let φ be a multivariate kernel regression with the objective in
Eq. 2, an RBF kernel K(·, ·) Then the adversarial solution ˆµ to Eq. 6 under cross-entropy loss
ℓ(ˆy, y) = −P"
HI,0.538878842676311,"i yi log ˆyi (y ∈{0, 1}k is the one-hot label vector) should satisfy that"
HI,0.5406871609403255,"lim
Nb→∞E[(1 −m) ⊙ˆµ] = 0 .
(23)"
HI,0.5424954792043399,"Proof of Lemma 3:
The output of φ is a k dimensional vector. Let us assume φt(·) ∈R be the
output corresponding to the target class t. We know the kernel regression solution is"
HI,0.5443037974683544,"φt(·) =
PNo
i=1 K(·, xi)yi,t + PNb
i=1 K(·, ˆxi)ˆyi,t
PNo
i=1 K(·, xi) + PNb
i=1 K(·, ˆxi)
(24)"
HI,0.546112115732369,"where ˆxi = (1 −m) ⊙xi + m ⊙∆is the backdoored example and ˆyi is the corresponding one-
hot label (so we always have ˆyi,t = 1). Because the examples are evenly distributed, there are
N/k examples labeled positive for class t. Without loss of generality, we assume yj,t = 1 when
j ∈[1, No/k] and yj,t = 0 otherwise. Then the regression solution becomes"
HI,0.5479204339963833,"φt(·) =
PNo/k
i=1
K(·, xi) + PNb
i=1 K(·, ˆxi)
PNo
i=1 K(·, xi) + PNb
i=1 K(·, ˆxi)
(25)"
HI,0.5497287522603979,"Please note the regression above is derived with a mean squared loss. In the adversarial analysis Eq.
6, we can use any alternative loss function. In this Lemma, we assume the loss in the adversarial
analysis is cross-entropy, which is a common choice. So, we have"
HI,0.5515370705244123,"ℓ(φ(x), yt) = −log φt(x)
(26)"
HI,0.5533453887884268,"= −log(St + ˆS) + log(S + ˆS)
(27)"
HI,0.5551537070524413,where Si(·) = P
HI,0.5569620253164557,"yj=i K(·, xj) and S = S1 + S2 + . . . + Sk and ˆS = Pnb
i=1 K(·, ˆxi)."
HI,0.5587703435804702,Published as a conference paper at ICLR 2022
HI,0.5605786618444847,The derivative of loss w.r.t. x becomes
HI,0.5623869801084991,∂ℓ(φ(x))
HI,0.5641952983725136,"∂x
= −
1"
HI,0.566003616636528,St + ˆS
HI,0.5678119349005425,∂(St + ˆS)
HI,0.569620253164557,"∂x
+
1"
HI,0.5714285714285714,S + ˆS
HI,0.5732368896925859,∂(S + ˆS)
HI,0.5750452079566004,"∂x
(28)"
HI,0.5768535262206148,"By using gradient descent, ˆµ moves along the negative direction of the loss gradient, i.e.,"
HI,0.5786618444846293,"∆µ =
1"
HI,0.5804701627486437,St + ˆS
HI,0.5822784810126582,∂(St + ˆS)
HI,0.5840867992766727,"∂x
−
1"
HI,0.5858951175406871,S + ˆS
HI,0.5877034358047016,∂(S + ˆS)
HI,0.5895117540687161,"∂x
(29) = ∂ˆS ∂x 
1"
HI,0.5913200723327305,"St + ˆS
−
1"
HI,0.593128390596745,S + ˆS
HI,0.5949367088607594,"
+ ∂St ∂x 
1"
HI,0.596745027124774,"St + ˆS
−
1"
HI,0.5985533453887885,"S + ˆS 
−
1"
HI,0.6003616636528029,S + ˆS
HI,0.6021699819168174,∂(S −St)
HI,0.6039783001808319,"∂x
(30)"
HI,0.6057866184448463,= a∂ˆS
HI,0.6075949367088608,∂x + a∂St
HI,0.6094032549728752,∂x −b∂(S −St)
HI,0.6112115732368897,"∂x
(31)"
HI,0.6130198915009042,"where a, b are both positive scalar values. When Nb becomes large, the ﬁrst term dominates the
gradient direction, i.e., ∆µ ∝∂ˆS ∂x ."
HI,0.6148282097649186,"Since we assume the kernel K(x, x′) = exp(−γ∥x −x′∥2) is RBF, then we have its derivative"
HI,0.6166365280289331,"∂K(x, x′)"
HI,0.6184448462929476,"∂x
= −2γK(x, x′)(x −x′)
(32)"
HI,0.620253164556962,Then we have
HI,0.6220614828209765,"∂ˆS
∂x = Nb
X i=1"
HI,0.6238698010849909,"∂K(x, ˆxi)"
HI,0.6256781193490054,"∂x
(33) = Nb
X"
HI,0.6274864376130199,"i=1
−2γK(x, ˆxi)(x −ˆxi)
(34)"
HI,0.6292947558770343,"Without loss of generality, we assume the mask m = [01×p, 11×(d−p+1)] (where ﬁrst p dimensions
are 0s and the remaining are 1s). Then the ﬁrst p dimensions of ˆxi are the same as those of xi, while
the remaining dimensions equivalent to those of ∆."
HI,0.6311030741410488,"Case 1: p + 1 ≤j ≤d.
The j-th dimension of ∂ˆS"
HI,0.6329113924050633,"∂x can be written as
 
∂ˆS
∂µ ! j
=  −2γ Nb
X"
HI,0.6347197106690777,"i=1
K(z + µ, ˆxi) !"
HI,0.6365280289330922,"(zj + µj −δj)
(35)"
HI,0.6383363471971067,"So at convergence, µj = δj −zj for the last d −p + 1 dimensions. The expectation is E[µj] =
δj −E[zj]."
HI,0.6401446654611211,"Case 2: 1 ≤j ≤p.
For the ﬁrst p dimensions, we have
 
∂ˆS
∂µ !"
HI,0.6419529837251357,"j
= −2γ Nb
X"
HI,0.64376130198915,"i=1
K(z + µ, ˆxi)(zj + µj −xij)
(36) = −2γ Nb
X"
HI,0.6455696202531646,"i=1
e−γ∥z+µ−ˆxi∥2(zj + µj −xij) .
(37)"
HI,0.6473779385171791,"Since µs + zs = δs = ˆxis = xis for p + 1 ≤s ≤d, then we have
 
∂ˆS
∂µ !"
HI,0.6491862567811935,"j
= −2γ Nb
X"
HI,0.650994575045208,"i=1
e−γ P"
HI,0.6528028933092225,"1≤v≤p(zv+µv−xiv)2(zj + µj −xij)
(38)"
HI,0.6546112115732369,"Since Nb is large and ˆxi is i.i.d., then the summation can be approximated with integration, i.e.,
 
∂ˆS
∂µ !"
HI,0.6564195298372514,"j
= −2γ
Z"
HI,0.6582278481012658,"x
e−γ P"
HI,0.6600361663652803,"1≤v≤p(zv+µv−xiv)2(zj + µj −xj)dx .
(39)"
HI,0.6618444846292948,Published as a conference paper at ICLR 2022
HI,0.6636528028933092,"When γ is large, the integration is the expectation of the offset to the center of a Gaussian distribution
(centered at zj + µj). Due to symmetry, the derivative becomes 0 for most of the µ locations. This
would be the common cases because a smaller γ means examples with longer distance can more
signiﬁcantly inﬂuence the function output (which is not desired in a classiﬁer with high accuracy)."
HI,0.6654611211573237,"Nonetheless, in rare cases when the γ becomes small, we can still ﬁnd the stationary point of the
derivative. Given the fact that e−γ P"
HI,0.6672694394213382,"1≤v≤p(zv+µv−xiv)2 is always positive, we know

∂ˆS
∂µ
 j ="
HI,0.6690777576853526,"0 =⇒zj + µj −xj = 0. But it is impossible to reach. Again, utilizing symmetry of the normal
distribution and the fact that x is i.i.d. uniform, we ﬁnd when zj + µj −E[xj] < 0, the derivative
is positive; and when zj + µj −E[xj] > 0, the derivative is negative. Since we use gradient
descent to ﬁnd the optimal ˆµ. We can conclude that, at the convergence, zj + µj = E[xj]. Then
E[µj] = E[xj] −E[zj] = 0 because z is sampled from the same distribution of x."
HI,0.6708860759493671,"To summarize,"
HI,0.6726943942133815,"E[ˆµj] =
0,
1 ≤j ≤p
δj −E[zj],
p < j ≤d
(40)"
HI,0.674502712477396,"Then we know, as Nb →∞,
E[(1 −m) ⊙µ] →0
(41)
□"
HI,0.6763110307414105,"C
MONTE CARLOS GRADIENT ESTIMATION"
HI,0.6781193490054249,"Leveraging the Monte Carlo based estimation to craft target adversarial perturbations for the targeted
label yt typically requires x and xt, where xt is a legitimate sample satisﬁes f(xt) = yt. Firstly, x
is forced to approach the decision boundary of f(·; θ) for the target label yt(φ(x, yt) →0.5), which
stimulates the efﬁciency and accuracy of gradient estimation. We can make φ(x, yt) →0.5 through
projection process:
x ←(1 −α)x + αxt,
(42)
α ∈(0, 1) is a parameters for projection. During each gradient estimation procedure, α is set
through binary search (Chen et al., 2020a)."
HI,0.6799276672694394,"After that, we leverage Monte Carlo sampling to estimate f
∇φx(x, yt), its procedure can be expressed
as:"
HI,0.6817359855334539,"g
∇xφ(x, yt) = 1 N N
X"
HI,0.6835443037974683,"i=0
S(x + δµ′
i, yt)µ′
i,
(43)"
HI,0.6853526220614828,"where {µ′}N
i=0 are perturbations i.i.d sampled from uniform distribution and δ is a small positive
value representing the magnitude of perturbation. S is an indicator function such that"
HI,0.6871609403254972,"S(x, y) =

1,
f(x) = y,
−1,
f(x) ̸= y.
(44)"
HI,0.6889692585895117,"To further eliminate the variance induced by Monte Carlo sampling, we can improve f
∇φx(x, yt)
via:"
HI,0.6907775768535263,"f
∇φx(x, yt) = 1 N N
X"
HI,0.6925858951175407,"i=0
{S(x + δµ′
i, yt) −1 N N
X"
HI,0.6943942133815552,"i=0
S(x + δµ′
i, yt)}µ′
i
(45)"
HI,0.6962025316455697,"Using cosine angle to measure the similarity between g
∇xφ(x, yt) and ∇xφ(x, yt), previous
work (Chen et al., 2020a) have proved that:"
HI,0.6980108499095841,"lim
δ→0 cos∠(E[ g
∇xφ(x, yt)], ∇xφ(x, yt)) = 1 ,
(46)"
HI,0.6998191681735986,thus a smaller δ (0.01) is selected for ensuring the efﬁcacy of gradient estimation.
HI,0.701627486437613,Published as a conference paper at ICLR 2022
HI,0.7034358047016275,"Regarding the constraints on ||µ||1, we further processed the estimated gradient via:"
HI,0.705244122965642,"g
∇xφ(x, yt) ←
g
∇xφ(x, yt)"
HI,0.7070524412296564,"|| g
∇xφ(x, yt)||1
.
(47)"
HI,0.7088607594936709,"We refer readers to (Chen et al., 2020a) for the complete algorithm for gradient estimation."
HI,0.7106690777576854,"D
ALGORITHM FOR COMPUTING AGGREGATED GLOBAL ADVERSARIAL
PEAK (GAP)"
HI,0.7124773960216998,Algorithm 1 Aggregated Global Adversarial Peak (GAP)
HI,0.7142857142857143,"1: Input: Targeted DNN f(·; θ); label to be analyzed yt; legitimate input batches {Xi}n
i=1 evenly sampled
across classes; the targeted input batch Xt;
2: Output: Aggregated GAP value R(yt);"
HI,0.7160940325497287,"3: Initialize R(yt) = 0
4: for i = 1, . . . , t −1, t + 1, . . . , n do
5:
for j = 1, . . . , len(Xi) do
6:
Solve µi
j = arg minµ ℓ(φ(xij + µ, yt; θ), yt) + λ∥µ∥1 using MC gradient estimation;
7:
end for
8:
Find the GAP value for the current label: µi
max = maxj,u,v µi
j,u,v;
9:
Aggregate GAP values over all labels: R(yt) = µi
max + R(yt);
10: end for
11: Return: R(yt)"
HI,0.7179023508137432,"E
THE DETAILED CONFIGURATIONS FOR EXPERIMENTAL DATASETS AND
TRIGGERS"
HI,0.7197106690777577,"Task
# labels
Input size
# training images"
HI,0.7215189873417721,"CIFAR-10
10
32x32
50000
CIFAR-100
100
32x32
50000
TinyImageNet
200
64x64
1000000"
HI,0.7233273056057866,Table 2: Detailed information about dataset for each task.
HI,0.7251356238698011,"The detailed information for each task is included in the Table. E. The data augmentation uti-
lized for building each model is following https://keras.io/zh/examples/cifar10_
resnet/."
HI,0.7269439421338155,"F
THE ACCURACY AND ATTACK SUCCESS RATE(ASR) FOR EVALUATED
MODELS"
HI,0.72875226039783,The accuracy and ASR for the evaluated models for each task in included in Table. 3.
HI,0.7305605786618445,"Task
Infected Model
Normal Model Accuracy
Accuracy
ASR
CIFAR-10
≥90.04%
≥97.7%
≥92.31%
CIFAR-100
≥69.17%
≥96.27%
≥71.41%
TinyImageNet
≥58.98%
≥97.22%
≥60.11%"
HI,0.7323688969258589,Table 3: Accuracy and ASR for the evaluated models for each task
HI,0.7341772151898734,Published as a conference paper at ICLR 2022
HI,0.7359855334538878,"G
THE AMOUNTS OF EVALUATED MODELS FOR VARIOUS TASK AND ATTACK
APPROACHES"
HI,0.7377938517179023,"For each attack and task, we build 60 infected models and 60 uninfected models, respectively. No-
tably, the uninfected models for different attack approaches are randomly selected and different.
Each uninfected and infected model sets are built evenly upon ResNet-18, ResNet-44, ResNet-56,
DenseNet-33, DenseNet-58 these ﬁve models."
HI,0.7396021699819169,"H
DETAILS OF VISUALIZATION PROCESS"
HI,0.7414104882459313,Algorithm 2 Visualize µ
HI,0.7432188065099458,"1: Input: Targeted DNN f(·; θ); label to be analyzed yt; legitimate input batches {Xi}n
i=1 evenly sampled
across classes; the targeted input batch Xt;
2: Initialize µ
3: Output: Normalized µ;"
HI,0.7450271247739603,"4: for i = 1, . . . , t −1, t + 1, . . . , n do
5:
for j = 1, . . . , len(Xi) do
6:
Solve µi
j = arg minµ ℓ(φ(xij + µ, yt; θ), yt) + λ∥µ∥1 using MC gradient estimation;
7:
end for
8:
Find µi owns the GAP value for the current label: µi = arg maxµi
j µi
j,u,v;"
HI,0.7468354430379747,"9:
Aggregate µi over all labels: µ = µi + µ;
10: end for
11: Return: µ/||µ||1"
HI,0.7486437613019892,"I
DETAILED ROC FOR VAROUS TASKS AND ATTACK APPROACHES"
HI,0.7504520795660036,"0.0
0.1
0.2
0.3
0.4
0.5"
HI,0.7522603978300181,Detection error rate for benign models 0.70 0.75 0.80 0.85 0.90 0.95 1.00
HI,0.7540687160940326,Detection success rate for infected models
HI,0.755877034358047,BadNets
HI,0.7576853526220615,Watermark
HI,0.759493670886076,Label Consistent
HI,0.7613019891500904,Invisible Backdoor
HI,0.7631103074141049,(a) CIFAR-10
HI,0.7649186256781193,"0.0
0.1
0.2
0.3
0.4
0.5"
HI,0.7667269439421338,Detection error rate for benign models 0.70 0.75 0.80 0.85 0.90 0.95 1.00
HI,0.7685352622061483,Detection success rate for infected models
HI,0.7703435804701627,BadNets
HI,0.7721518987341772,Watermark
HI,0.7739602169981917,Label Consistent
HI,0.7757685352622061,Invisible Backdoor
HI,0.7775768535262206,(b) CIFAR-100
HI,0.779385171790235,"0.0
0.1
0.2
0.3
0.4
0.5"
HI,0.7811934900542495,Detection error rate for benign models 0.70 0.75 0.80 0.85 0.90 0.95 1.00
HI,0.783001808318264,Detection success rate for infected models
HI,0.7848101265822784,BadNets
HI,0.786618444846293,Watermark
HI,0.7884267631103075,Label Consistent
HI,0.7902350813743219,Invisible Backdoor
HI,0.7920433996383364,(c) TinyImageNet
HI,0.7938517179023508,"Figure 10: The Receiver Operating Cure(ROC) for AEVA on CIFAR-10, CIFAR-100 and TinyIma-
geNet tasks."
HI,0.7956600361663653,"J
THE COMPARISON RESULTS BETWEEN AEVA AND NC, DL-TND"
HI,0.7974683544303798,"Attack
Method
Detection Results
AUROC
ACC"
HI,0.7992766726943942,BadNets
HI,0.8010849909584087,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8028933092224232,"0.976
0.999
0.930"
HI,0.8047016274864376,"95.0%
99.2%
91.5%"
HI,0.8065099457504521,Watermark
HI,0.8083182640144665,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.810126582278481,"0.983
1
0.968"
HI,0.8119349005424955,"95.8%
100%
94.4%"
HI,0.8137432188065099,Label Consistent
HI,0.8155515370705244,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8173598553345389,"0.961
0.992
0.968"
HI,0.8191681735985533,"94.2%
98.3%
94.4%"
HI,0.8209764918625678,Invisible Attack
HI,0.8227848101265823,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8245931283905967,"0.980
0.999
0.970"
HI,0.8264014466546112,"95.8%
99.1%
94.4%"
HI,0.8282097649186256,"Table 4: The two metrics for backdoor detection on the CIFAR-10 task using three backdoor detec-
tion methods: NC, DL-TND, and AEVA. Higher values in AUROC and ACC are better."
HI,0.8300180831826401,Published as a conference paper at ICLR 2022
HI,0.8318264014466547,"Attack
Method
Detection Results
AUROC
ACC"
HI,0.833634719710669,BadNets
HI,0.8354430379746836,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8372513562386981,"0.978
0.999
0.973"
HI,0.8390596745027125,"95.8%
99.1%
95.4%"
HI,0.840867992766727,Watermark
HI,0.8426763110307414,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8444846292947559,"0.961
1
0.992"
HI,0.8462929475587704,"94.2%
100%
98.8%"
HI,0.8481012658227848,Label Consistent
HI,0.8499095840867993,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8517179023508138,"0.964
0.999
0.981"
HI,0.8535262206148282,"94.2%
99.1%
96.8%"
HI,0.8553345388788427,Invisible Attack
HI,0.8571428571428571,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8589511754068716,"0.989
1
0.984"
HI,0.8607594936708861,"97.5%
100%
97.1%"
HI,0.8625678119349005,"Table 5: The two metrics for backdoor detection on the CIFAR-100 task using three backdoor
detection methods: NC, DL-TND, and AEVA. Higher values in AUROC and ACC are better."
HI,0.864376130198915,"Attack
Method
Detection Results
AUROC
ACC"
HI,0.8661844484629295,BadNets
HI,0.8679927667269439,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8698010849909584,"0.956
0.999
0.988"
HI,0.8716094032549728,"91.6%
99.1%
97.9%"
HI,0.8734177215189873,Watermark
HI,0.8752260397830018,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8770343580470162,"0.972
1
0.999"
HI,0.8788426763110307,"94.2%
100%
99.6%"
HI,0.8806509945750453,Label Consistent
HI,0.8824593128390597,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.8842676311030742,"0.983
0.999
0.983"
HI,0.8860759493670886,"96.7%
99.1%
96.7%"
HI,0.8878842676311031,Invisible Attack
HI,0.8896925858951176,"NC (Wang et al., 2019)
DL-TND (Wang et al., 2020)
AEVA (Ours)"
HI,0.891500904159132,"0.982
0.999
0.986"
HI,0.8933092224231465,"96.7%
99.1%
97.9%"
HI,0.895117540687161,"Table 6: The two metrics for backdoor detection on the Tiny Imagenet task using three backdoor
detection methods: NC, DL-TND, and AEVA. Higher values in AUROC and ACC are better. Each
approach are evaluated using 60 infected and 60 benign models."
HI,0.8969258589511754,"K
EXPERIMENTS FOR DIFFERENT TRIGGERS"
HI,0.8987341772151899,"K.1
EVALUATION ON DYNAMIC AND SPARSE BUT NOT COMPACT TRIGGERS"
HI,0.9005424954792043,"We evaluate AEVA for CIFAR-10 task under the dynamic and non-compact triggers scenario. We
randomly generate three non-compact triggers(shown in Figure. 11) and perform backdoor attacks
against a randomly selected label using these dynamic triggers. We test AEVA using 10 different
DNN models, which are evenly built upon ResNet-18, ResNet-44, ResNet-56, DenseNet-33 and
DenseNet-58."
HI,0.9023508137432188,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
HI,0.9041591320072333,(a) Dynamic trigger I
HI,0.9059674502712477,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
HI,0.9077757685352622,(b) Dynamic trigger II
HI,0.9095840867992767,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
HI,0.9113924050632911,(c) Dynamic trigger III
HI,0.9132007233273056,Figure 11: Dynamic Triggers
HI,0.9150090415913201,Published as a conference paper at ICLR 2022
HI,0.9168173598553345,"AEVA successfully identify all models embedded with dynamic backdoors and the corresponding
infected labels. The detailed Aggregated Global Adversarial Peak(AGAP) values and Anomaly
Index given by the MAD outlier detection for infected and uninfected labels are shown in Fig. 12.
We ﬁnd AEVA predicts all backdoored models as infected with Anomaly Index ≥4. Such results
demonstrate that AEVA is resilient to dynamic and sparse but not compact triggers."
HI,0.918625678119349,"Infected
Uninfected
Labels 0.00 0.05 0.10 0.15 0.20 0.25 0.30 AGAP"
HI,0.9204339963833634,(a) AGAP
HI,0.9222423146473779,"Infected
Uninfected
Labels 2 0 2 4 6 8 10 12 14"
HI,0.9240506329113924,Anomaly Index
HI,0.9258589511754068,(b) Anamoly Index
HI,0.9276672694394213,Figure 12: AGAP and Anomaly Index for infected and uninfected labels
HI,0.9294755877034359,"We also evaluate AEVA on three different triggers with different shapes, which are shown in Fig. 13.
For each trigger, we build 30 models on TinyImageNet. Each set of models are evenly built upon
ResNet-18, ResNet-44, ResNet-56, DenseNet-33, DenseNet-58 these architectures. The results are
shown in Table. 7."
HI,0.9312839059674503,"Attack Approach
ACC"
HI,0.9330922242314648,"Trigger I
93.3%
Trigger II
93.3%
Trigger III
90.0%"
HI,0.9349005424954792,Table 7: Results for other different triggers
HI,0.9367088607594937,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
HI,0.9385171790235082,(a) Trigger I
HI,0.9403254972875226,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
HI,0.9421338155515371,(b) Trigger II
HI,0.9439421338155516,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
HI,0.945750452079566,(c) Trigger III
HI,0.9475587703435805,Figure 13: Triggers with different shapes
HI,0.9493670886075949,"L
RESULTS FOR IMPACT OF THE NUMBER OF IMAGES PER LABELS."
HI,0.9511754068716094,"We further explore how many samples for each label are required by AEVA to be effective. We test
AEVA on TinyImageNet and the experimental conﬁgurations are consistent with Sec.4.1(i.e., 240"
HI,0.9529837251356239,Published as a conference paper at ICLR 2022
HI,0.9547920433996383,"infected and 240 benign models). Our results are illustrated in the Fig. 14, which plots the metrics
against the number of samples per class. We ﬁnd that our approach requires more samples (32) for
each class to achieve optimal performance."
HI,0.9566003616636528,"0
10
20
30
40
# of samples per class 0.0 0.2 0.4 0.6 0.8 1.0 ACC"
HI,0.9584086799276673,Figure 14: The impact for the number of samples for each class
HI,0.9602169981916817,"M
THE IMPACT OF THE NUMBER OF THE LABELS FOR AVAILABLE IMAGES."
HI,0.9620253164556962,"Since our approach aims to detect backdoors in the black-box setting, a realistic scenario is that while
determining if a speciﬁc label is infected, the clean data from all other labels may be unavailable
to the defender. So, in this section, we investigate the impact of the number of labels for which
clean data is available for our approach. We test our approach on TinyImageNet using 60 models
infected with BadNets (4 × 4 squares) following the conﬁgurations of Sec. 4.2. We vary the number
of uninfected labels for available samples between [1, 200], randomly sampled from the 200 labels
available in the TingImageNet dataset. Figure. 15 illustrates our results by plotting the metrics
against the number of available labels. We observe that even with eight available labels, our approach
can still detect backdoors in most cases, i.e., with merely 4% of the labels being available, our
approach is still viable. This study exhibits our approach’s practicality, which can work in the black-
box setting and on relatively small available data. Notably, with fewer labels AEVA will always
correctly tag the uninfected models. This is because few labels will cause the anomaly index low
thus resilient to uninfected models."
HI,0.9638336347197106,"100
101
102
103"
HI,0.9656419529837251,Available Labels 0.0 0.2 0.4 0.6 0.8 1.0 ACC
HI,0.9674502712477396,Figure 15: The impact for the number of samples for each class
HI,0.969258589511754,"N
MULTIPLE TRIGGERS WITHIN THE SINGLE INFECTED LABEL SCENARIOS"
HI,0.9710669077757685,"We also investigate the impact caused by multiple triggers within the single infected label. We
here choose different 4 × 4 squares located at different places as the triggers implemented following"
HI,0.972875226039783,Published as a conference paper at ICLR 2022
HI,0.9746835443037974,"BadNets. We randomly select a label as the infected label. We built 60 infected models with different
architectures(i.e., ResNet-18, ResNet-44, ResNet-56, DenseNet-33, DenseNet-58). The results for
TinyImageNet are shown in Table. 8. Notably, for TinyImageNet, injecting too many triggers (≥4)
in the single label would cause the target model’s accuracy drop (i.e., ≥3.7%), which is inconsistent
with the threat model for backdoor attacks (Gu et al., 2019; Chen et al., 2017; Liu et al., 2018)."
HI,0.976491862567812,"As seen in Table. 8, our approach can still perform effective when injecting two 4 × 4 triggers in
the single infected label. However, when the number of injected triggers becomes more than 2, our
approach becomes less effective. This should be caused by that multiple-triggers would reduce the
singularity properties for the adversarial perturbations. To address this issue, we select the sum of
the largest ﬁve points as the µmax instead. The results are shown in Table. 9. By selecting more
points to calculate the µmax, AEVA can still perform effective under the multiple triggers within the
single label scenarios and have no impact on the detection accuracy for uninfected models."
HI,0.9783001808318263,"Attack Approach
ACC"
HI,0.9801084990958409,"Two infected triggers
81.7%
Three infected triggers
48.3%"
HI,0.9819168173598554,Table 8: Results for multiple triggers within the single infected label
HI,0.9837251356238698,"Attack Approach
ACC"
HI,0.9855334538878843,"Two infected triggers
93.3%
Three infected triggers
88.3%"
HI,0.9873417721518988,Table 9: Results II for multiple triggers within the single infected label
HI,0.9891500904159132,"O
POTENTIAL ADAPTIVE BACKDOOR ATTACKS"
HI,0.9909584086799277,We here consider two potential backdoor attacks which can bypass AEVA.
HI,0.9927667269439421,"O.1
ATTACKS WITH MULTIPLE TARGET LABELS"
HI,0.9945750452079566,"Since AEVA is sensitive to the number of infected labels, the attacker can infect multiple labels with
different backdoor triggers. Indeed, making multiple labels (i.e.≥30%;) infected will make the
anomaly indexes produced by the MAD detector signiﬁcantly drop. However, such attack can only
be successfully implemented for some small datasets which own a few labels(e.g., CIFAR-10, etc).
Reported by (Wang et al., 2019), the state-of-the-art model (DeepID) for Youtube Face dataset can
not maintain the average attack success rate and model accuracy at the same time when more than
15.6% labels are infected. In another word, when multiple labels are infected, the infected model’s
accuracy is likely to get worse if the attacker wants to keep attack success rate. So too many infected
labels will reduce the stealth of the attack. We conducted experiments on TinyImageNet, which
reveals that when over 14 labels are infected with different triggers, the model accuracy (ResNet-44
and DenseNet-58) decreases to ≤57.61% (around 3% lower than a normal model) when preserving
the attack effectiveness (ASR ≥97.12%) for each infected label."
HI,0.9963833634719711,"O.2
ATTACK WITH LARGE TRIGGERS"
HI,0.9981916817359855,"Another potential adaptive backdoor attack is that the attacker would implement a dense backdoor
trigger which would alleviate the singularity phenomenon. As we claim in Section 4.3, a dense back-
door trigger would appear visually-distinguishable to the human beings which perform less stealthy,
that is also reported by Neural Cleanse (Wang et al., 2019). Indeed, there exists an interesting excep-
tional watermark attacks. However, we empirically prove that such dense watermark attack (Chen
et al., 2017) would make infected DNNs non-robust which would make the inputs sensitive to the
random noise. The details are included in Appendix A."
