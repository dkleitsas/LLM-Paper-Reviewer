Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0019083969465648854,"Building models of human decision-making from observed behaviour is critical to
better understand, diagnose and support real-world policies such as clinical care.
As established policy learning approaches remain focused on imitation perfor-
mance, they fall short of explaining the demonstrated decision-making process.
Policy Extraction through decision Trees (POETREE) is a novel framework for in-
terpretable policy learning, compatible with fully-ofﬂine and partially-observable
clinical decision environments – and builds probabilistic tree policies determin-
ing physician actions based on patients’ observations and medical history. Fully-
differentiable tree architectures are grown incrementally during optimization to
adapt their complexity to the modelling task, and learn a representation of pa-
tient history through recurrence, resulting in decision tree policies that adapt over
time with patient information. This policy learning method outperforms the state-
of-the-art on real and synthetic medical datasets, both in terms of understanding,
quantifying and evaluating observed behaviour as well as in accurately replicating
it – with potential to improve future decision support systems."
INTRODUCTION,0.003816793893129771,"1
INTRODUCTION"
INTRODUCTION,0.0057251908396946565,"Different approaches to integrating, analysing and acting upon clinical information leads to wide,
unwarranted variation in medical practice across regions and institutions (McKinlay et al., 2007;
Westert et al., 2018). Algorithms designed to support the clinical decision-making process can
help overcome this issue, with successful examples in oncology prognosis (Beck et al., 2011; Es-
teva et al., 2017), retinopathy detection (Gulshan et al., 2016) and radiotherapy planning (Valdes
et al., 2017). Still, these support systems focus on effectively replacing physicians with autonomous
agents, trained to optimise patient outcomes or diagnosis accuracy. With limited design concern for
end-users, these algorithms lack interpretability, leading to mistrust from the medical community
(La¨ı et al., 2020; The Royal Society, 2017)."
INTRODUCTION,0.007633587786259542,"Instead, our work aims to better describe and understand the decision-making process by observing
physician behaviour, in a form of epistemology. These transparent models of behaviour can clearly
synthesise domain expertise from current practice and could thus form the building blocks of future
decision support systems (Li et al., 2015), designed in partnership with clinicians for more reliable
validation, better trust, and widespread acceptance. In addition, such policy models will enable
quantitative comparisons of different strategies of care, and may in turn lead to novel guidelines and
educational materials to combat variability of practice."
INTRODUCTION,0.009541984732824428,"The associated machine learning challenge, therefore, is to explain how physicians choose clinical
actions, given new patient observations and their prior medical history – and thus describe an inter-
pretable decision-making policy. Modelling clinical diagnostic and treatment policies is particularly
challenging, with observational data involving confounding factors and unknown evolution dynam-"
INTRODUCTION,0.011450381679389313,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013358778625954198,Observations
INTRODUCTION,0.015267175572519083,Actions
INTRODUCTION,0.01717557251908397,Histories
INTRODUCTION,0.019083969465648856,"Infection
Antibiotics"
INTRODUCTION,0.02099236641221374,T > 37.5°C yes no
INTRODUCTION,0.022900763358778626,T > 38.5°C yes no
INTRODUCTION,0.02480916030534351,Antibiotics yes no
INTRODUCTION,0.026717557251908396,"Figure 1: Schematic representation of behavioural policies as trees, adapting over time."
INTRODUCTION,0.02862595419847328,"ics, which we wish to then translate into a transparent representation of decision-making. State-of-
the-art solutions for uncovering demonstrated policies, such as inverse reinforcement learning (Ng
& Russell, 2000) or imitation learning (Bain & Sammut, 1996; Piot et al., 2014; Ho & Ermon, 2016),
fall short of interpretability – mainly proposing black-box models as behaviour representations."
INTRODUCTION,0.030534351145038167,"In contrast, transparent policy parametrisations based on visual decision boundaries (H¨uy¨uk et al.,
2021), high-level programming syntax (Verma et al., 2018) or outcome preferences (Yau et al., 2020)
can explain learned behaviour, at the cost of restrictive modelling constraints and obscure dynamics.
Among possible interpretable representations, decision trees offer the ﬂexibility to capture high-
dimensional environments and, as we demonstrate, can model time-series through recurrence. In
addition, decision trees are highly familiar to the medical community, with many clinical guidelines
having this form (Chou et al., 2007; McCreery & Truelove, 1991; Burch et al., 2012). Cognitive re-
search suggests human decision-making does follow a hierarchical, branching process (Zylberberg
et al., 2017), making these models straightforward to understand. As a result, our algorithm Pol-
icy Extraction through decision Trees (POETREE) represents clinical policies as decision trees,
evolving over time with patient information – as illustrated in Figure 1."
INTRODUCTION,0.03244274809160305,"Contributions
The main contributions of this work are as follows: (i) A novel framework for
policy learning in the form of incrementally grown, probabilistic decision trees, which adapt their
complexity to the task at hand. (ii) An interpretable model for representation learning over time-
series called recurrent decision trees. (iii) Illustrative analyses of our algorithm’s expressive power
in disambiguating policies otherwise unidentiﬁable with related benchmarks; integration of domain
knowledge through inductive bias; and formalisation and quantiﬁcation of abstract behavioural con-
cepts (e.g. uncertain, anomalous or low-value actions) – thus demonstrating its effectiveness in both
understanding and replicating behaviour."
PROBLEM FORMALISM,0.03435114503816794,"2
PROBLEM FORMALISM"
PROBLEM FORMALISM,0.03625954198473282,"Our goal is to learn an interpretable representation of a demonstrated decision-making process to
understand agent behaviour. The clinical context requires ofﬂine learning using observational data
only, as experimenting with policies would be both unethical and impractical. Finally, as previ-
ous observations and actions affect treatment choice, we are concerned with a partially-observable
decision-making environment."
PROBLEM FORMALISM,0.03816793893129771,"We assume access to a dataset of m demonstrated patient trajectories in discrete time D =
{(zi
1, ai
1, . . . , zi
τi, ai
τi)}m
i=1, where τi is the length of trajectory i. We drop index i to denote a generic
draw from the population. At timestep t, the physician-agent observes patient features, denoted by
random variable Zt ∈Z, and chooses treatment or diagnostic action At ∈A, where A is a ﬁnite
set of actions. Let zt and at denote realisations of these random variables. In contrast to traditional
Markovian environments, the agent’s reward function R, state space S and transition dynamics are
all inaccessible."
PROBLEM FORMALISM,0.04007633587786259,"Let f denote a representation function, mapping the history of patient observations and actions to
a representation space H, and ht = f(z1, a1, ...zt−1, at−1) = f(z1:t−1, a1:t−1) ∈H represent
the patient history prior to observing a new set of features zt. Following the partially-observable
Markov Decision Process (POMDP) formalism, information in {ht, zt} can be combined to form a
belief over the inaccessible patient state st. In contrast to recent policy learning work on POMDPs,
which uncover mappings from belief to action space (Sun et al., 2017; Choi & Kim, 2011; Makino
& Takeuchi, 2012; H¨uy¨uk et al., 2021), however, our aim is to highlight how newly-acquired obser-
vations zt conditions action at, to ensure decisions are based on interpretable, meaningful variables.
As a result, agent behaviour must be represented as a mapping from observation to action space."
PROBLEM FORMALISM,0.04198473282442748,"An adaptive decision-making policy is a stationary mapping π : Z × H × A →[0, 1], where
π(at|zt, ht) encodes the probability of choosing action at given patient history ht and latest obser-"
PROBLEM FORMALISM,0.04389312977099236,Published as a conference paper at ICLR 2022
PROBLEM FORMALISM,0.04580152671755725,"vation zt – thus ∀t, P"
PROBLEM FORMALISM,0.04770992366412214,"at∈A π(at|zt, ht) = 1. We assume trajectories in D are generated by an agent
following a policy πE, such that at ∼πE(·|zt, ht). The goal of our work is to ﬁnd an interpretable
policy representation π which matches and explains the behaviour of πE."
INTERPRETABLE POLICY LEARNING WITH DECISION TREES,0.04961832061068702,"3
INTERPRETABLE POLICY LEARNING WITH DECISION TREES"
SOFT DECISION TREES,0.05152671755725191,"3.1
SOFT DECISION TREES"
SOFT DECISION TREES,0.05343511450381679,"Motivated by the interpretability and pervasiveness of trees in the medical literature (Chou et al.,
2007), Policy Extraction through decision Trees (POETREE) represents observed decision-making"
SOFT DECISION TREES,0.05534351145038168,leaf node
SOFT DECISION TREES,0.05725190839694656,inner node
SOFT DECISION TREES,0.05916030534351145,"(a) DT policy.
time delay"
SOFT DECISION TREES,0.061068702290076333,"(b) RDT policy.
Figure 2: POETREE policy structures."
SOFT DECISION TREES,0.06297709923664122,"policies as trees. Our model architecture is inspired by soft
decision trees – structures with binary nodes, probabilistic
decision boundaries and ﬁxed leaf outputs (Irsoy et al., 2012)
– for their transparency and modelling performance (Frosst
& Hinton, 2017). This supervised learning framework can
be adapted for policy learning by setting input variables as
x = zt or as {ht, zt} (for fully- or partially-observable envi-
ronments), and targets as y = at (Bain & Sammut, 1996)."
SOFT DECISION TREES,0.0648854961832061,"Inner nodes
Illustrated in Figure 2a, the path followed by
normalised input x ∈RD is determined by gating functions
encoding the probability of taking the rightmost branch at
each non-leaf node n: pn
gate(x) = σ
 
xT wn + bn
, where
{wn ∈RD;
bn ∈R}n∈Inner are trainable parameters
and σ is the sigmoid function. This design choice is moti-
vated by its demonstrated ﬂexibility and expressivity (Silva
et al., 2020), and can be made interpretable by retaining the
largest component of wn to form an unidimensional axis-
aligned threshold at each node, as illustrated in Figure 1 and
detailed in Appendix C. Model simpliﬁcation can be made
more robust through L1 regularisation, encouraging sparsity
in {wn}n∈Inner. The path probability of each node, P n(x),
is therefore given by the product of gating functions leading
to it from the tree root."
SOFT DECISION TREES,0.06679389312977099,"Leaf nodes
In our tree architecture, leaves deﬁne a ﬁxed
probability distribution over K categorical output classes.
Leaf l encodes relative activation values ˆal
t = softmax(θl
a),
where θl
a ∈RK is a learnable parameter. For ease of com-
putation and interpretation, the output of the maximum-probability leaf ˆalmax
t
, where lmax =
arg maxl P l(x), is taken as overall classiﬁcation decision, following Frosst & Hinton (2017)."
RECURRENT DECISION TREES,0.06870229007633588,"3.2
RECURRENT DECISION TREES"
RECURRENT DECISION TREES,0.07061068702290077,"To account for previously acquired information, our algorithm must jointly extract and depend on
a representation of patient history ht. As in Figure 2b, RDTs take history ht and new observation
zt as inputs, and output both predicted action at and subsequent history embeddings ht+1 from
decision leaves. These are computed as hl
t+1 = tanh(θl
h), where θl
h ∈RM is an additional leaf
parameter – alternative leaf models are discussed in Appendix B. Finally, as third leaf output with
parameters θl
z ∈RD, our model also predicts patient evolution, or observations at the next timestep
˜zt+1, effectively capturing the expected effects of a treatment on the patient trajectory as a policy
explanation (Yau et al., 2020). A single tree structure is used at all timesteps: unfolded over time,
our recurrent tree can be viewed as a sequence of cascaded trees (Ding et al., 2021) with shared
parameters Θ = {{wn, bn}n∈Inner; {θl
a, θl
hθl
z}l∈Leaf} and topology T."
RECURRENT DECISION TREES,0.07251908396946564,Published as a conference paper at ICLR 2022
TREE GROWTH AND OPTIMISATION,0.07442748091603053,"3.3
TREE GROWTH AND OPTIMISATION"
TREE GROWTH AND OPTIMISATION,0.07633587786259542,"Optimisation objective
The policy learning objective is to recover the tree topology T and param-
eters Θ which best describe the observation-to-action mapping demonstrated by physician agents.
Our loss function combines the cross-entropy between one-hot encoded target at ∈{0, 1}K and
each leaf’s action output, weighted by its respective path probability under input {ht, zt}:"
TREE GROWTH AND OPTIMISATION,0.07824427480916031,"L({ht, zt, at}; T, Θ) = −
X"
TREE GROWTH AND OPTIMISATION,0.08015267175572519,"l∈Leaf
P l(ht, zt)
X"
TREE GROWTH AND OPTIMISATION,0.08206106870229007,"k
at,k log

ˆal
t,k

(1)"
TREE GROWTH AND OPTIMISATION,0.08396946564885496,"where ˆal
t,k is the output probability for action class k in leaf l. Additional objectives are necessary
for the multiple outputs to match their intended target: regularisation term L˜z is designed to learn
patient evolution ˜zt+1 as expected by the acting physician. It minimises prediction error on zt+1 and
ensures the policy is consistent between timesteps by constraining predicted observations to lead to
similar action choices as true ones, under the new history ht+1:
L˜z({ht, zt, zt+1}; T, Θ) = δ1 ∥zt+1 −˜zt+1∥2 + δ2 DKL (π(·|ht+1, zt+1∥π(·|ht+1, ˜zt+1)
(2)
where {δ1, δ2} are tunable hyperparameters, weighting ﬁdelity to true evolution and to demonstrated
behaviour. For a set of training trajectories D, and an overall loss L = L + L˜z, our policy learning
optimisation objective becomes:
arg min
T,Θ
L(D)
(3)"
TREE GROWTH AND OPTIMISATION,0.08587786259541985,"The probabilistic nature of our model choice makes it fully-differentiable, allowing optimisation of
parameters Θ through stochastic gradient descent and backpropagation through the structure for a
ﬁxed topology T. Optimising with respect to T, however, requires a more involved algorithm."
TREE GROWTH AND OPTIMISATION,0.08778625954198473,Algorithm 1: Tree growth optimisation.
"GLOBAL OPTIMISATION
AND PATH PRUNING
INNER NODE",0.08969465648854962,"3. Global optimisation
and path pruning
inner node"
"GLOBAL OPTIMISATION
AND PATH PRUNING
INNER NODE",0.0916030534351145,suboptimal leaf
INITIALISATION,0.09351145038167939,"1. Initialisation ✗
✗ ✔
✔
✔ ✔
✔"
INITIALISATION,0.09541984732824428,Local optimisation
INITIALISATION,0.09732824427480916,"2. For each suboptimal leaf:
a. Split node
b. Label as optimal
or ✗
✗ ✗"
INITIALISATION,0.09923664122137404,"optimal leaf ✗
✔"
INITIALISATION,0.10114503816793893,"Keep best-performing
in validation"
INITIALISATION,0.10305343511450382,"1. Initialise T to inner node with two suboptimal leaves;
Optimise Θ via gradient descent of loss L;
while ∃suboptimal leaves in T do"
INITIALISATION,0.1049618320610687,"Split suboptimal leaf into inner node with two leaves;
2a. Locally optimise Θ via gradient descent of L;
if validation performance improves then retain split;
else 2b. Retain leaf as optimal;"
INITIALISATION,0.10687022900763359,"3. Globally optimise Θ via gradient descent of L;
Prune branches in T with low path probability P l(Dval)."
INITIALISATION,0.10877862595419847,"Tree growth
Following the work of Irsoy et al. (2012) and Tanno et al. (2019), our decision tree
architecture is incrementally grown and optimised during the training process, allowing the model
to independently adapt its complexity to the task at hand. Algorithm 1 summarises the growth
and optimisation procedure. Starting from an optimised shallow structure, tree growth is carried
out by sequentially splitting each leaf node and locally optimising new parameters – the split is
accepted if validation performance is improved. After a ﬁnal global optimisation, branches with a
path probability below a certain threshold on the validation set are pruned. Further details on the
tree growth procedure are given in Appendix B."
INITIALISATION,0.11068702290076336,"Complexity
Policy training can be computationally complex due to the multiple local tree optimi-
sations involved and signiﬁcant number of parameters in comparison to other interpretable models
(H¨uy¨uk et al., 2021). Still, at test time, computing tree path probabilities for a given input is inex-
pensive, which is advantageous to allow fast decision support. A detailed comparison of runtimes
involved with our method and related work is proposed in Appendix D."
RELATED WORK,0.11259541984732824,"4
RELATED WORK"
RELATED WORK,0.11450381679389313,"In this section, we compare related work on interpretable policy learning, and contrast them in terms
of key desiderata outlined in Section 2. Table 1 summarises our ﬁndings."
RELATED WORK,0.11641221374045801,"Policy learning
While sequential decision-making problems are traditionally addressed through
reinforcement learning, this framework is inapplicable to our policy learning goal due to the unavail-
ability of a reward signal R. Instead, we focus on the inverse task of replicating the demonstrated
behaviour of an agent, known as imitation learning (IL), which assumes no access to R. Com-
mon approaches include behavioural cloning (BC) where this task is reduced to supervised learning,"
RELATED WORK,0.1183206106870229,Published as a conference paper at ICLR 2022
RELATED WORK,0.12022900763358779,Table 1: Related work. Comparison to policy learning methods in terms of interpretability.
RELATED WORK,0.12213740458015267,"Related work
Ofﬂine
Partial
Interpretable
Modelling
Learning
Observability
Policy
Assumptions"
RELATED WORK,0.12404580152671756,"BC-IL (Bain & Sammut, 1996)



–
PO-BC-IL (Sun et al., 2017)



–
Interpretable BC-IL (H¨uy¨uk et al., 2021)



S known; low-dim. env.
DM-IL (Ho & Ermon, 2016)



–
MB-IL (Englert et al., 2013)



Simple dynamics"
RELATED WORK,0.12595419847328243,"IRL (Ng & Russell, 2000)



πE optimal wrt. R
PO-IRL (Choi & Kim, 2011)



πE optimal wrt. R
Ofﬂine PO-IRL (Makino & Takeuchi, 2012)



πE optimal wrt. R
Interpretable RL (Silva et al., 2020)



R known"
RELATED WORK,0.12786259541984732,"POETREE (Ours)



–"
RELATED WORK,0.1297709923664122,"mapping states to actions (Bain & Sammut, 1996; Piot et al., 2014); and distribution-matching (DM)
methods, where state-action distributions between demonstrator and learned policies are matched
through adversarial learning (Ho & Ermon, 2016; Jeon et al., 2018; Kostrikov et al., 2020). Ap-
prenticeship learning (AL) aims to reach or surpass expert performance on a given task – a closely
related but distinct problem. The state-of-the-art solution is inverse reinforcement learning (IRL),
which recovers R from demonstration trajectories (Ng & Russell, 2000; Abbeel & Ng, 2004; Ziebart
et al., 2008); followed by a forward reinforcement learning algorithm to obtain the policy."
RELATED WORK,0.1316793893129771,"Towards interpretability for understanding decisions
Imitation and apprenticeship learning so-
lutions have been developed to tackle the challenges of fully-ofﬂine learning in partially-observable
decision environments, as required by the clinical context: in IL, Sun et al. (2017) learn belief states
through RNNs, and IRL was adapted for partial-observability (Choi & Kim, 2011) and ofﬂine learn-
ing (Makino & Takeuchi, 2012). However, while black-box neural networks typically provide best
results, a transparent description of behaviour requires parametrising the learned policy to be in-
formative and understandable. This often involves sacriﬁcing action-matching performance (Lage
et al., 2018), and introduces assumptions which limit the applicability of the proposed method, such
as Markovianity (Silva et al., 2020) or low-dimensional state spaces (H¨uy¨uk et al., 2021). The latter
work, for instance, parametrises action choice through decision boundaries within the agent’s belief
space. In addition to leaving the observations-to-action mapping obscure, this imposes constraints
on modelled states and dynamics – which limit predictive performance, scalability and policy iden-
tiﬁability (Biasutti et al., 2012)."
RELATED WORK,0.13358778625954199,"One approach to achieve interpretability in decision models is to implement interpretable reward
functions for IRL, providing insight into demonstrating agents’ inner objectives. Behaviour has thus
been explained as preferences over counterfactual outcomes (Bica et al., 2021), or over information
value in a time-pressured context (Jarrett & van der Schaar, 2020). Still, the need for black-box RL
algorithms to extract a policy obscures how observations affect action choice. Instead, the agent’s
policy function can be directly parametrised as an interpretable structure. RL policies have been
represented through a high-level programming syntax (Verma et al., 2018), or through explanations
in terms of intended outcomes (Yau et al., 2020). Related work on building interpretable models for
time-series in general is also discussed in Appendix B."
RELATED WORK,0.13549618320610687,"Adaptive decision trees for time-series
Related soft and grown tree architectures have been pro-
posed for image-based tasks (Frosst & Hinton, 2017; Tanno et al., 2019) – little research has been
reported on probabilistic decision trees for the dynamic, heterogeneous and sparse nature of time-
series data. Tree structures have been proposed for representation learning, relying for instance on
projecting inputs to more informative features in between decision nodes (Tanno et al., 2019) or on
cascaded trees, sequentially using their outputs as input to subsequent ones (Ding et al., 2021). In
all cases, however, models lose interpretability, as decisions are based on unintuitive variables: in
contrast, we build thresholds over native observation variables, meaningful to human experts. Our
work is further contrasted with established tree models in Appendix B."
ILLUSTRATIVE EXAMPLES,0.13740458015267176,"5
ILLUSTRATIVE EXAMPLES"
EXPERIMENTAL SETUP,0.13931297709923665,"5.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.14122137404580154,"Decision-Making Environments
Three medical datasets were studied to evaluate our work. First,
as ground-truth policies πE are inaccessible in real data, a synthetic dataset of 1000 patient trajec-"
EXPERIMENTAL SETUP,0.1431297709923664,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.1450381679389313,"tories over 9 timesteps, denoted SYNTH, was simulated. In a POMDP with binary disease state
space, observations include one-dimensional diagnostic tests {z+, z−} as well as 10 noisy dimen-
sions disregarded by the agent (e.g. irrelevant measurements); and actions {a+, a−} correspond
to treatment and lack thereof. Assuming treatments require exposure beyond the end of symp-
toms (Entsuah et al., 1996), the expert policy treats patients who tested positive over the last three
timesteps: at = πE(ht, zt) = a+ if z+ ∈{zt−2, zt−1, zt}; a−else. Next, a real medical dataset was
explored, following 1,625 patients from the Alzheimer’s Disease Neuroimaging Initiative (ADNI),
as in H¨uy¨uk et al. (2021) for benchmarking. We consider the task of predicting, at each visit,
whether a Magnetic Resonance Imaging (MRI) scan is ordered for cognitive disorder diagnosis
(NICE, 2018). Patient observations consist of the Clinical Dementia Rating (CDR-SB) on a severity
scale (normal; questionable impairment; severe dementia) following O’Bryant et al. (2008); and the
MRI outcome of the previous visit, categorised into four possibilities (no MRI scan; and below-
average, average and above-average hippocampal volume Vh). Finally, we also consider a dataset of
4,222 ICU patients over up to 6 timesteps extracted from the third Medical Information Mart for In-
tensive Care (MIMIC-III) database (Johnson et al., 2016) and predict antibiotic prescription based
on 8 observations – temperature, white blood cell count, heart rate, hematocrit, hemoglobin, blood
pressure, creatinine and potassium; as in Bica et al. (2021). By dataset design or by the nature of
real-world clinical practice, observation history must be considered by the acting policies – making
our decision-making environments partially-observable."
EXPERIMENTAL SETUP,0.14694656488549618,"Success Metrics
Reﬂecting our unconventional priority for policy interpretability over imitation
performance, we provide illustrative examples evidencing greater insight into observed behaviour
through decision tree representations. For a more quantitative assessment of interpretability, we also
surveyed ﬁve practising UK clinicians of different seniority level, asking them to score models out
of ten on how well they could understand the decision-making process – with details in Appendix
G. Action-matching performance was also evaluated through the areas under the receiver-operating-
characteristic curve (AUROC) and precision-recall curve (AUPRC), and through Brier calibration."
EXPERIMENTAL SETUP,0.14885496183206107,"Benchmark Algorithms
For benchmarking, we implemented different behavioural cloning mod-
els, mapping observations or belief states to actions: (i) decision tree (Tree BC-IL) and logistic
regression models with no patient history; (ii) a partially-observable BC algorithm (PO-BC-IL)
extracting history embeddings through an RNN (Sun et al., 2017), on which a feature importance
analysis is carried out (Lundberg et al., 2017); (iii) and an interpretable PO-BC-IL model, INTER-
POLE (H¨uy¨uk et al., 2021), described in Section 4. Benchmarks also include (iv) model-based
imitation learning PO-MB-IL (Englert et al., 2013), adapted for partial-observability with a learned
environment model; (v) Bayesian Inverse Reinforcement Learning for POMDPs (PO-IRL), based
on Jarrett & van der Schaar (2020); and (vi) a Ofﬂine PO-IRL version of this algorithm as in
Makino & Takeuchi (2012). Implementation details are provided in Appendix E."
INTERPRETABILITY,0.15076335877862596,"5.2
INTERPRETABILITY"
INTERPRETABILITY,0.15267175572519084,"In light of our priority on recovering an interpretable policy, we ﬁrst propose several clinical exam-
ples and insights from the ADNI dataset to highlight the usefulness of our tree representation."
INTERPRETABILITY,0.15458015267175573,"Explaining patient trajectories
Our ﬁrst example studies decision-making within typical ADNI
trajectories. Let Patient A be healthy with normal CDR score and average Vh on initial scan. Let
Patient B initially have mild cognitive impairment (MCI), progressing towards dementia: their CDR
degrades from questionable to severe at the last timestep and a below-average Vh is measured at each
visit. Patient C is diagnosed with dementia at the ﬁrst visit, with severe CDR and low Vh: no further
scans are ordered as they would be uninformative (NICE, 2018). Figure 3a illustrates our learned
tree policy, both varying with and determining patient history at each timestep (see Appendix F
for the distinct policy of each patient). Patient A is not ordered any further scan, as they never
show low Vh or abnormal CDR, but may be ordered one if they develop cognitive impairment. For
Patient B with low Vh but a CDR score not yet severe, another MRI is ordered to monitor disease
progression. At the following visit, patient history conditions the physician to be more cautious of
scan results: even if Vh is found to be acceptable, another scan may be ordered if the CDR changes.
Finally, Patient C’s observations already give a certain diagnosis of dementia: no scan is necessary.
At following timesteps, unless the patient unexpectedly recovers, their history suggests to not carry
out any further scans. We must highlight the similar strategy between our decision tree policy and
published guidelines for Alzheimer’s diagnosis in Biasutti et al. (2012), reproduced in Appendix F:
our policy correctly learns that investigations are only needed until diagnosis is conﬁrmed."
INTERPRETABILITY,0.15648854961832062,Published as a conference paper at ICLR 2022
INTERPRETABILITY,0.15839694656488548,Vh low
INTERPRETABILITY,0.16030534351145037,"CDR-SB normal
CDR-SB severe"
INTERPRETABILITY,0.16221374045801526,"MRI
MRI"
INTERPRETABILITY,0.16412213740458015,Healthy
INTERPRETABILITY,0.16603053435114504,Vh low
INTERPRETABILITY,0.16793893129770993,"CDR-SB questionable
CDR-SB severe"
INTERPRETABILITY,0.16984732824427481,"MRI
MRI"
INTERPRETABILITY,0.1717557251908397,"MCI
Dementia"
INTERPRETABILITY,0.1736641221374046,CDR-SB severe MRI yes no yes no yes no
INTERPRETABILITY,0.17557251908396945,"Patient A
Patient B
Patient C"
INTERPRETABILITY,0.17748091603053434,(a) Adaptive decision tree policy
INTERPRETABILITY,0.17938931297709923,"Healthy
Dementia MCI MRI"
INTERPRETABILITY,0.18129770992366412,"(b) Decision
boundary over belief"
INTERPRETABILITY,0.183206106870229,Vh low
INTERPRETABILITY,0.1851145038167939,"CDR-SB normal
MRI yes no"
INTERPRETABILITY,0.18702290076335878,No previous MRI MRI
INTERPRETABILITY,0.18893129770992367,"(c) Static decision
tree policy"
INTERPRETABILITY,0.19083969465648856,"0.0
0.2
0.4
0.6
0.8
1.0 .50 .55 .60 .65 .70 .75 .80 .85"
INTERPRETABILITY,0.19274809160305342,"(d) Logistic
regression"
INTERPRETABILITY,0.1946564885496183,".00
.05
.10
.15
.20
.25"
INTERPRETABILITY,0.1965648854961832,"(e) Feature
importance"
INTERPRETABILITY,0.1984732824427481,"Figure 3: Comparison of interpretable policy learning methods on ADNI. Crosses stand for no action. Fig.
(b) follows a method from H¨uy¨uk et al. (2021)."
INTERPRETABILITY,0.20038167938931298,"64%
29%"
INTERPRETABILITY,0.20229007633587787,Vh low
INTERPRETABILITY,0.20419847328244276,"CDR-SB normal
CDR-SB severe"
INTERPRETABILITY,0.20610687022900764,"MRI
MRI
63%
76% yes no"
INTERPRETABILITY,0.20801526717557253,"83%
67%"
INTERPRETABILITY,0.2099236641221374,"81%
11%"
INTERPRETABILITY,0.21183206106870228,Vh low
INTERPRETABILITY,0.21374045801526717,"CDR-SB questionable
CDR-SB severe"
INTERPRETABILITY,0.21564885496183206,"MRI
MRI yes no"
INTERPRETABILITY,0.21755725190839695,"Figure 4: Policy conﬁdence on ADNI. Maximum probability paths are highlighted in blue, with action proba-
bilities in green/red."
INTERPRETABILITY,0.21946564885496184,"In contrast, the black-box history extraction procedure required by most benchmarks for partial-
observability (PO-BC-IL, PO-IRL, Ofﬂine PO-IRL, PO-MB-IL) is difﬁcult to interpret, with no
clear decision-making process. The policy learned by INTERPOLE (H¨uy¨uk et al., 2021) in Figure
3b, deﬁned by decision boundaries over a subjective disease belief space, provides more insight:
this model suggests respectively few and frequent scans for healthy Patient A and MCI Patient B, at
different vertices of the belief space, but cannot account for scan reduction in diagnosed Patient C as
this requires discontinuous action regions. This policy representation only relies on disease beliefs
and thus cannot identify the effect of different observations on treatment strategies. Finally, com-
mon interpretable models proposed in Figures 3c–3e fail to convey an understanding of behavioural
strategy and evolution."
INTERPRETABILITY,0.22137404580152673,"Decision-making uncertainty
Our behaviour model also inherently captures the useful notion of
decision conﬁdence thanks to its probabilistic nature:"
INTERPRETABILITY,0.22328244274809161,"π(at = k|ht, zt) =
X"
INTERPRETABILITY,0.22519083969465647,"l∈Leaf
P l(ht, zt) · al
t,k
(4)"
INTERPRETABILITY,0.22709923664122136,"where, for each leaf node l, P l(ht, zt) and al
t,k are the path and output probability for action k.
Figure 4 highlights how policy conﬁdence varies over time for each typical patient. Uncertainty for
Patient A can be attributed to variability in healthy patient care in the demonstrations dataset, as
scans may provide information about their state, but must be balanced with cost and time considera-
tions (NICE, 2018). For Patient C with a dementia diagnosis, further investigations are not required
(Biasutti et al., 2012), reﬂected in conﬁdence increase. As a result, for Patient B with degrading
symptoms, scans are initially ordered with low conﬁdence to monitor progression, but as symptoms
worsen and the patient is diagnosed, decision conﬁdence increases. Figure 4 also evidences both
greater inter-path and intra-leaf uncertainty1 for MRI prediction: our policy model identiﬁes clear
conditions not warranting a scan after diagnosis, whereas conditions warranting one are more am-
biguous. This example illustrates the value of uncertainty estimation afforded by our framework,
modulating probability values within tree paths and leaf outputs."
INTERPRETABILITY,0.22900763358778625,1Inter-path uncertainty can be measured as the entropy within leaf path probabilities −P
INTERPRETABILITY,0.23091603053435114,"l P l(x) log P l(x),
while intra-leaf uncertainty can be measured as the average leaf output entropy: −1/l P l
P"
INTERPRETABILITY,0.23282442748091603,"k ˆal
t,k log ˆal
t,k."
INTERPRETABILITY,0.23473282442748092,Published as a conference paper at ICLR 2022
INTERPRETABILITY,0.2366412213740458,"4%
95%"
INTERPRETABILITY,0.2385496183206107,"92%
60%"
INTERPRETABILITY,0.24045801526717558,Vh low
INTERPRETABILITY,0.24236641221374045,"CDR-SB normal
CDR-SB severe"
INTERPRETABILITY,0.24427480916030533,"MRI
MRI yes no"
INTERPRETABILITY,0.24618320610687022,(a) Anomalous behaviour per cohort.
INTERPRETABILITY,0.2480916030534351,A (Healthy)
INTERPRETABILITY,0.25,B (Degrading)
INTERPRETABILITY,0.25190839694656486,C (Dementia) 0 1 2 3 4
INTERPRETABILITY,0.2538167938931298,Uninformative MRIs (%)
INTERPRETABILITY,0.25572519083969464,(b) Uninformative actions.
INTERPRETABILITY,0.25763358778625955,"Figure 5: Anomalous and low-value actions in various patient cohorts: patients over 75 years old, patients
carrying the ϵ4 allele of the apolipoprotein E gene (ApoE4), patients with signs of MCI or dementia at their
ﬁrst visit, female and male patients."
INTERPRETABILITY,0.2595419847328244,"Anomalous behaviour detection
Learned models of behaviour are also valuable to ﬂag actions
incompatible with observed policies. Visits where an MRI is predicted with 90% certainty, yet the
physician agent does not order it – as in Figure 5a – make up 8.4% of ADNI. This may highlight
demonstration ﬂaws, as the learned policy is conﬁdent and has well-calibrated probability estimates
(low Brier score in Table 2). Anomalous behaviour followed by an MRI ‘correcting’ the off-policy
action can also be detected, suggesting 6.7% of patients are thus investigated late – comparable to
belated diagnoses found in 6.5% of patients by H¨uy¨uk et al. (2021). As shown in Figure 5a, high-
risk cohorts due to age, female gender or ApoE4 allele (Launer et al., 1999; Lindsay et al., 2002)
are more often subject to late actions. Most anomalous actions on patients with ApoE4 or female
patients end up corrected, in contrast to older patients or ones with early symptoms – imaging is less
paramount for the latter, as they may be diagnosed clinically (NICE, 2018)."
INTERPRETABILITY,0.26145038167938933,"Action value quantiﬁcation through counterfactual evolution
Our joint expected evolution and
policy learning model allows action value to be quantiﬁed (Yau et al., 2020; Bica et al., 2021). Low-
value actions correspond to an ordered scan showing similar observations to the previous timestep,
when this could already be foreseen from expected evolution ˜z (less than average variation minus one
standard deviation). As shown in Figure 5b, ill patients are more often monitored with uninformative
MRIs than healthy ones. This analysis explains the high rate of uninformative actions for patients
with early signs of disease, found in previous work (H¨uy¨uk et al., 2021): the policy recommends
investigation for diagnosis, yet their expected evolution is typically unambiguous. Overall, our
decision tree architecture enables straightforward assessment of counterfactual patient evolution
under different observation values."
INTERPRETABILITY,0.2633587786259542,"Overall, our decision tree policy parametrisation explains demonstrated decision-making behaviour
in terms of how actions are chosen based on observations and prior history, and, in the above
illustrative analyses, allows a formalisation of abstract notions – such as uncertain, anomalous or
low-value choices – for quantitative policy evaluation. Next, we show that these insights are not
gained at the cost of imitation performance."
POLICY FIDELITY,0.2652671755725191,"5.3
POLICY FIDELITY"
POLICY FIDELITY,0.26717557251908397,"Action-matching performance
As shown in Table 2, our approach outperforms or comes second-
best on all success metrics compared to benchmark algorithms. This conﬁrms the expressive power
of our recurrent tree policies over decision boundaries (H¨uy¨uk et al., 2021) and even over neural-
network-based methods (Sun et al., 2017), speciﬁcally optimised for action-matching. The poor
action-matching performance of static decision trees (Tree BC-IL) highlights the importance of his-
tory representation learning in overcoming the partial-observability of the decision-making environ-
ment. Mean relative evolution error between zt and ˜zt was measured as 13 ± 4% and 26 ± 9% on
ADNI and MIMIC respectively, giving reasonable and insightful values. Our model was scored as
the second most understandable in our user survey, following the overly-simplistic static decision
tree (Fig. 3c). Collected feedback is provided in Appendix G."
POLICY FIDELITY,0.26908396946564883,"Policy identiﬁability
Figure 6 illustrates policies learned from our high-dimensional SYNTH
dataset with POETREE and the closest related work (H¨uy¨uk et al., 2021). Despite the apparent
simplicity of the decision-making problem, INTERPOLE fails to identify the outlined treatment strat-
egy, as evidenced by its inferior performance in Table 2. This illustrates greater ﬂexibility of our"
POLICY FIDELITY,0.27099236641221375,Published as a conference paper at ICLR 2022
POLICY FIDELITY,0.2729007633587786,"Table 2: Comparison of the performance of policy learning algorithms2on medical datasets. Interpretabil-
ity scores out of ten were obtained through our clinician survey. Lower is better for Brier calibration. Standard
errors for MIMIC and SYNTH were ≤0.04."
POLICY FIDELITY,0.2748091603053435,"Task
ADNI MRI scans
MIMIC antibiotics
SYNTH"
POLICY FIDELITY,0.2767175572519084,"Algorithm
Interpretability
AUROC
AUPRC
Brier
AUROC
Brier
AUROC
Brier"
POLICY FIDELITY,0.2786259541984733,"Tree BC-IL
9.3 ± 0.3
0.53 ± 0.01
0.72 ± 0.01
0.25 ± 0.01
0.50
0.23
–
–
PO-BC-IL [47]
0.3 ± 0.3
0.59 ± 0.04
0.80 ± 0.08
0.18 ± 0.05
0.67
0.16
0.98
0.02
INTERPOLE [23]
7.3 ± 0.5
0.44 ± 0.04
0.75 ± 0.09
0.19 ± 0.07
0.65
0.21
0.84
0.12
PO-MB-IL [13]
–
0.54 ± 0.08
0.81 ± 0.07
0.19 ± 0.03
–
–
–
–
PO-IRL [25]
–
0.50 ± 0.08
0.82 ± 0.04
0.23 ± 0.01
–
–
–
–
Ofﬂine PO-IRL [37]
–
0.54 ± 0.06
0.83 ± 0.04
0.23 ± 0.01
–
–
–
–"
POLICY FIDELITY,0.28053435114503816,"POETREE
8.3 ± 0.3
0.62 ± 0.01
0.82 ± 0.01
0.18 ± 0.01
0.68
0.19
0.99
0.01"
POLICY FIDELITY,0.2824427480916031,"Healthy
Diseased"
POLICY FIDELITY,0.28435114503816794,Treatment
POLICY FIDELITY,0.2862595419847328,"(a) Decision-boundary policy
(H¨uy¨uk et al., 2021). yes no"
POLICY FIDELITY,0.2881679389312977,Positive test
POLICY FIDELITY,0.2900763358778626,Treatment
POLICY FIDELITY,0.2919847328244275,Positive test
POLICY FIDELITY,0.29389312977099236,"Treatment
Treatment"
POLICY FIDELITY,0.2958015267175573,Positive test
POLICY FIDELITY,0.29770992366412213,"Treatment
Treatment"
POLICY FIDELITY,0.29961832061068705,(b) Adaptive decision tree policy.
POLICY FIDELITY,0.3015267175572519,"Figure 6: Recovered policies for
SYNTH."
POLICY FIDELITY,0.30343511450381677,"tree representation in disambiguating decision-making factors
which simply cannot be captured by previous work; and, due to not
relying on HMM-like dynamics, can be credited to fewer modelling
assumptions and scalability to high-dimensional environments."
POLICY FIDELITY,0.3053435114503817,"Inductive bias
Having demonstrated that our policy learning
framework allows us to recover domain expertise, we investigated
the effect of incorporating prior policy knowledge on learning per-
formance. In practice, our model’s action prediction structure can
be initialised based on a published guideline (Biasutti et al., 2012)
as in Appendix F: training time was reduced by a factor of 1.4
and action-matching AUPRC was improved to 0.84 ± 0.01. This
‘warm-start’ is a promising strategy to learn from physician obser-
vation while building on established clinical practice standards."
POLICY FIDELITY,0.30725190839694655,"Accuracy-interpretability trade-off: complexity analysis
In-
crementally grown trees outperform ﬁxed structures by eliminating
unnecessary partitions, with ∼40% fewer parameters for the same
depth, greater readability (Lage et al., 2018) and reasonable training
time. Grown structures adapt their depth and complexity to avoid
overﬁtting while capturing the modelling task, in a form of Occam’s
razor – jointly optimising policy accuracy and interpretability. In
particular, sample complexity modulates learned policies in train-
ing. Detailed experimental results are provided in Appendix D."
CONCLUSION,0.30916030534351147,"6
CONCLUSION"
CONCLUSION,0.3110687022900763,"Established paradigms to replicate demonstrated behaviour fall short of explaining learned decision
policies, whereas the state-of-the-art in interpretable policy learning was criticised by end-users for
restrictive modelling assumptions (H¨uy¨uk et al., 2021) – thus not fully capturing diagnostic strate-
gies. Policy Extraction with Decision Trees (POETREE) overcomes these challenges, outputting
most likely actions and uncertainty estimates, given patients’ observations and medical history. Our
algorithm carries out joint policy learning, history representation learning and evolution prediction
tasks through a recurrent, multi-output tree model – justifying agent decisions by comparing coun-
terfactual patient evolutions. Optimised structures outperform the state-of-the-art in interpretability,
policy ﬁdelity and action-matching, thus providing an understanding of demonstrated behaviour
without compromising accuracy. In particular, we can formalise and quantify concepts of practice
variation with time; uncertain, anomalous, or low-value behaviour detection; and domain knowledge
integration through inductive bias. Finally, we validate clinical insights through a survey of intended
users."
CONCLUSION,0.31297709923664124,"Beyond policy learning, our novel interpretable framework for time-series modelling shows promise
for alternative tasks relevant to clinical decision support, such as patient trajectory prediction, out-
come estimation or human-in-the-loop learning for treatment recommendation – a promising avenue
of research for further work."
CONCLUSION,0.3148854961832061,"2References: [47] Sun et al. (2017); [23] H¨uy¨uk et al. (2021); [13] Englert et al. (2013); [25] Jarrett &
van der Schaar (2020); [37] Makino & Takeuchi (2012)."
CONCLUSION,0.31679389312977096,Published as a conference paper at ICLR 2022
CONCLUSION,0.3187022900763359,ACKNOWLEDGMENTS
CONCLUSION,0.32061068702290074,"We thank all NHS clinicians who participated in our survey and the Alzheimer’s Disease Neuroimag-
ing Initiative and Medical Information Mart for Intensive Care for access to datasets. This publica-
tion was partially made possible by an ETH AI Center doctoral fellowship to AP. AJC would like
to acknowledge and thank Microsoft Research for its support through its PhD Scholarship Program
with the EPSRC. Many thanks to group members of the van der Schaar Lab and to our reviewers for
their valuable feedback."
REFERENCES,0.32251908396946566,REFERENCES
REFERENCES,0.3244274809160305,"Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In-
ternational Conference on Machine Learning (ICML), 2004."
REFERENCES,0.32633587786259544,"Michael Bain and Claude Sammut. A framework for behavioural cloning. MACHINE INTELLI-
GENCE 15, pp. 103–129, 1996. URL http://citeseerx.ist.psu.edu/viewdoc/
summary?doi=10.1.1.25.1759."
REFERENCES,0.3282442748091603,"AH Beck, AR Sangoi, S Leung, RJ Marinelli, TO Nielsen, MJ van de Vijver, RB West, M van de
Rijn, and D Koller.
Systematic analysis of breast cancer morphology uncovers stromal fea-
tures associated with survival. Science translational medicine, 3, 11 2011. ISSN 1946-6242.
doi: 10.1126/SCITRANSLMED.3002564. URL https://pubmed.ncbi.nlm.nih.gov/
22072638/."
REFERENCES,0.3301526717557252,"Yoshua Bengio and Paolo Frasconi. An input output hmm architecture. Adv. Neural Inf. Process.
Syst., 7:427–434, 1995."
REFERENCES,0.3320610687022901,"Maria Biasutti, Natacha Dufour, Clotilde Ferroud, William Dab, and Laura Temime.
Cost-
effectiveness of magnetic resonance imaging with a new contrast agent for the early diagnosis of
alzheimer’s disease. PLOS ONE, 7:e35559, 4 2012. ISSN 1932-6203. doi: 10.1371/JOURNAL.
PONE.0035559.
URL https://journals.plos.org/plosone/article?id=10.
1371/journal.pone.0035559."
REFERENCES,0.33396946564885494,"Ioana Bica, Daniel Jarrett, Alihan H¨uy¨uk, and Mihaela van der Schaar. Learning ”what-if” explana-
tions for sequential decision-making. Proc. 9th International Conference on Learning Represen-
tations (ICLR 2021), 2021. URL http://arxiv.org/abs/2007.13531."
REFERENCES,0.33587786259541985,"Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and regression trees.
Wadsworth & Brooks/Cole, 1984."
REFERENCES,0.3377862595419847,"J Burch, S Hinde, S Palmer, F Beyer, J Minton, A Marson, U Wieshmann, N Woolacott, and
M Soares.
The clinical effectiveness and cost-effectiveness of technologies used to visualise
the seizure focus in people with refractory epilepsy being considered for surgery: a systematic
review and decision-analytical model. Health Technology Assessment, 16:1–163, 9 2012. ISSN
ISSN: 2046-4924. doi: 10.3310/HTA16340. URL www.hta.ac.uk."
REFERENCES,0.33969465648854963,"Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz, Walter F. Stewart, and
Jimeng Sun. Retain: An interpretable predictive model for healthcare using reverse time attention
mechanism. Avances in Neural Information Processing Systems (NIPS), 29, 8 2016. URL http:
//arxiv.org/abs/1608.05745."
REFERENCES,0.3416030534351145,"Jaedeug Choi and Kee-Eung Kim. Inverse reinforcement learning in partially observable environ-
ments. Journal of Machine Learning Research, 12:691–730, 2011. ISSN 1533-7928. URL
http://jmlr.org/papers/v12/choi11a.html."
REFERENCES,0.3435114503816794,"Roger Chou, Amir Qaseem, Vincenza Snow, Donald Casey, Thomas J. Cross, Paul Shekelle, and
Douglas K. Owens. Diagnosis and treatment of low back pain: A joint clinical practice guide-
line from the american college of physicians and the american pain society. Annals of Internal
Medicine, 147:478–491, 10 2007. doi: 10.7326/0003-4819-147-7-200710020-00006."
REFERENCES,0.34541984732824427,"Zihan Ding, Pablo Hernandez-Leal, Gavin Weiguang Ding, Changjian Li, and Ruitong Huang.
Cdt:
Cascading decision trees for explainable reinforcement learning.
arXiv preprint
arXiv:2011.07553, 2021.
URL https://github.com/openai/gym/blob/master/
gym/envs/box2d/lunar."
REFERENCES,0.3473282442748092,Published as a conference paper at ICLR 2022
REFERENCES,0.34923664122137404,"Peter Englert, Alexandros Paraschos, Marc Peter Deisenroth, and Jan Peters. Probabilistic model-
based imitation learning. Adaptive Behavior, 21:388–403, 10 2013. ISSN 10597123. doi: 10.
1177/1059712313491614.
URL https://journals.sagepub.com/doi/full/10.
1177/1059712313491614."
REFERENCES,0.3511450381679389,"A. R. Entsuah, R. L. Rudolph, D. Hackett, and S. Miska. Efﬁcacy of venlafaxine and placebo
during long-term treatment of depression: A pooled analysis of relapse rates. International Clin-
ical Psychopharmacology, 11:137–145, 1996. doi: 10.1097/00004850-199611020-00008. URL
/record/1996-05005-007."
REFERENCES,0.3530534351145038,"Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and
Sebastian Thrun. Dermatologist-level classiﬁcation of skin cancer with deep neural networks.
Nature 2017 542:7639, 542:115–118, 1 2017. ISSN 1476-4687. doi: 10.1038/nature21056. URL
https://www.nature.com/articles/nature21056."
REFERENCES,0.3549618320610687,"Wael Farag. Safe-driving cloning by deep learning for autonomous cars. International Journal of
Advanced Mechatronic Systems, 7:390–397, 2017. doi: 10.1504/IJAMECHS.2017.099318."
REFERENCES,0.3568702290076336,"Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. CEX
workshop at AI*IA 2017 conference, 11 2017.
URL http://arxiv.org/abs/1711.
09784."
REFERENCES,0.35877862595419846,"Matthew Golub, Steven Chase, and Byron Yu. Learning an internal dynamics model from control
demonstration. pp. 606–614, 2 2013. ISSN 1938-7228. URL http://proceedings.mlr.
press/v28/golub13.html."
REFERENCES,0.3606870229007634,"V Gulshan, L Peng, M Coram, MC Stumpe, D Wu, A Narayanaswamy, S Venugopalan, K Widner,
T Madams, J Cuadros, R Kim, R Raman, PC Nelson, JL Mega, and DR Webster. Development
and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus
photographs. JAMA, 316:2402–2410, 12 2016. ISSN 1538-3598. doi: 10.1001/JAMA.2016.
17216. URL https://pubmed.ncbi.nlm.nih.gov/27898976/."
REFERENCES,0.36259541984732824,"Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. 30th Conference on
Neural Information Processing Systems, 2016."
REFERENCES,0.36450381679389315,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735–
1780, 11 1997. ISSN 08997667. doi: 10.1162/neco.1997.9.8.1735. URL http://direct.
mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf."
REFERENCES,0.366412213740458,"Stephanie L. Hyland, Martin Faltys, Matthias H¨user, Xinrui Lyu, Thomas Gumbsch, Crist´obal Es-
teban, Christian Bock, Max Horn, Michael Moor, Bastian Rieck, Marc Zimmermann, Dean Bo-
denham, Karsten Borgwardt, Gunnar R¨atsch, and Tobias M. Merz. Early prediction of circu-
latory failure in the intensive care unit using machine learning. Nature Medicine 2020 26:3,
26:364–373, 3 2020.
ISSN 1546-170X.
doi: 10.1038/s41591-020-0789-4.
URL https:
//www.nature.com/articles/s41591-020-0789-4."
REFERENCES,0.3683206106870229,"Alihan H¨uy¨uk, Daniel Jarrett, Cem Tekin, and Mihaela Van Der Schaar. Explaining by imitating:
Understanding decisions by interpretable policy learning. ICLR, 2021."
REFERENCES,0.3702290076335878,"Ozan Irsoy, Olcay Taner Yıldız, and Ethem Alpaydın. Soft decision trees. 21st International Con-
ference on Pattern Recognition (ICPR), 2012."
REFERENCES,0.37213740458015265,"Daniel Jarrett and Mihaela van der Schaar. Inverse active sensing: Modeling and understanding
timely decision-making. Proceedings of the 37 th International Conference on Machine Learning,
2020."
REFERENCES,0.37404580152671757,"Wonseok Jeon, Seokin Seo, and Kee-Eung Kim. A bayesian approach to generative adversarial
imitation learning. Advances in Neural Information Processing Systems, 31, 2018."
REFERENCES,0.37595419847328243,"Alistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li Wei H. Lehman, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G. Mark. Mimic-iii,
a freely accessible critical care database. Scientiﬁc Data, 3:1–9, 5 2016. ISSN 20524463. doi:
10.1038/sdata.2016.35. URL www.nature.com/sdata/."
REFERENCES,0.37786259541984735,Published as a conference paper at ICLR 2022
REFERENCES,0.3797709923664122,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 3rd International
Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, 12 2015.
URL https://arxiv.org/abs/1412.6980v9."
REFERENCES,0.3816793893129771,"Ilya Kostrikov, Oﬁr Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution
matching. ICLR, 12 2020. URL http://arxiv.org/abs/1912.05032."
REFERENCES,0.383587786259542,"Isaac Lage, Andrew Slavin Ross, Been Kim Google Brain, Samuel J Gershman, and Finale Doshi-
Velez. Human-in-the-loop interpretability prior. 32nd Conference on Neural Information Pro-
cessing Systems (NIPS 2018), 2018."
REFERENCES,0.38549618320610685,"LJ Launer, K Andersen, ME Dewey, L Letenneur, A Ott, LA Amaducci, C Brayne, JR Copeland,
JF Dartigues, P Kragh-Sorensen, Lobo A, Martinez-Lage JM, T Stijnen, and A Hofman. Rates
and risk factors for dementia and alzheimer’s disease: results from eurodem pooled analyses.
eurodem incidence research group and work groups. european studies of dementia. Neurology,
52:78–84, 1 1999. ISSN 0028-3878. doi: 10.1212/WNL.52.1.78. URL https://pubmed.
ncbi.nlm.nih.gov/9921852/."
REFERENCES,0.38740458015267176,"M.-C La¨ı, M Brian, and M.-F Mamzer. Perceptions of artiﬁcial intelligence in healthcare: ﬁndings
from a qualitative survey study among actors in france. J Transl Med, 18:14, 2020. doi: 10.1186/
s12967-019-02204-y. URL https://doi.org/10.1186/s12967-019-02204-y."
REFERENCES,0.3893129770992366,"A Li, S Jin, L Zhang, and Y Jia. A sequential decision-theoretic model for medical diagnostic
system. Technology and health care : ofﬁcial journal of the European Society for Engineering
and Medicine, 23 Suppl 1:S37–S42, 5 2015. ISSN 1878-7401. doi: 10.3233/THC-150926. URL
https://pubmed.ncbi.nlm.nih.gov/26410326/."
REFERENCES,0.39122137404580154,"Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from vi-
sual demonstrations. Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 3815–3825, 2017. URL https://github.com/ermongroup/
InfoGAIL."
REFERENCES,0.3931297709923664,"J Lindsay, D Laurin, R Verreault, R H´ebert, B Helliwell, GB Hill, and I McDowell. Risk factors for
alzheimer’s disease: a prospective analysis from the canadian study of health and aging. American
journal of epidemiology, 156:445–453, 9 2002. ISSN 0002-9262. doi: 10.1093/AJE/KWF074.
URL https://pubmed.ncbi.nlm.nih.gov/12196314/."
REFERENCES,0.3950381679389313,"Scott M Lundberg, Paul G Allen, and Su-In Lee. A uniﬁed approach to interpreting model pre-
dictions. 31st Conference on Neural Information Processing Systems (NIPS 2017), 2017. URL
https://github.com/slundberg/shap."
REFERENCES,0.3969465648854962,"Takaki Makino and Johane Takeuchi. Apprenticeship learning for model parameters of partially ob-
servable environments. Proceedings of the 29 th International Conference on Machine Learning,
pp. 891–898, 2012."
REFERENCES,0.3988549618320611,"R. G. Masterton, A. Galloway, G. French, M. Street, J. Armstrong, E. Brown, J. Cleverley, P. Dil-
worth, C. Fry, A. D. Gascoigne, Alan Knox, Dilip Nathwani, Robert Spencer, and Mark Wilcox.
Guidelines for the management of hospital-acquired pneumonia in the uk: Report of the work-
ing party on hospital-acquired pneumonia of the british society for antimicrobial chemotherapy.
Journal of Antimicrobial Chemotherapy, 62:5–34, 7 2008. ISSN 0305-7453. doi: 10.1093/JAC/
DKN162. URL https://academic.oup.com/jac/article/62/1/5/844812."
REFERENCES,0.40076335877862596,"Ann M. McCreery and Edmond Truelove. Decision making in dentistry. part i: A historical and
methodological overview. The Journal of Prosthetic Dentistry, 65:447–451, 3 1991. ISSN 0022-
3913. doi: 10.1016/0022-3913(91)90241-N."
REFERENCES,0.4026717557251908,"J. B. McKinlay, C. L. Link, K. M. Freund, L. D. Marceau, A. B. O’Donnell, and K. L.
Lutfey.
Sources of variation in physician adherence with clinical guidelines:
Results
from a factorial experiment.
Journal of General Internal Medicine, 22:289, 3 2007.
doi:
10.1007/S11606-006-0075-2.
URL /pmc/articles/PMC1824760//pmc/articles/
PMC1824760/?report=abstracthttps://www.ncbi.nlm.nih.gov/pmc/
articles/PMC1824760/."
REFERENCES,0.40458015267175573,Published as a conference paper at ICLR 2022
REFERENCES,0.4064885496183206,"Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. Proc. 17th
International Conf. on Machine Learning, pp. 663–670, 2000."
REFERENCES,0.4083969465648855,"NICE. Dementia: Assessment, management and support for people living with dementia and their
carers. National Institute for Health and Care Excellence (NICE) Guideline No. 97, 2018. URL
http://www.nice.org.uk/guidance/ng97."
REFERENCES,0.41030534351145037,"SE O’Bryant, SC Waring, CM Cullum, J Hall, L Lacritz, PJ Massman, PJ Lupo, JS Reisch, and
R Doody. Staging dementia using clinical dementia rating scale sum of boxes scores: a texas
alzheimer’s research consortium study. Archives of neurology, 65:1091–1095, 8 2008. ISSN
1538-3687.
doi: 10.1001/ARCHNEUR.65.8.1091.
URL https://pubmed.ncbi.nlm.
nih.gov/18695059/."
REFERENCES,0.4122137404580153,"Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, and Jan Peters.
An algorithmic perspective on imitation learning. Foundations and Trends ® in Robotics, 7:
1–179, 2018. doi: 10.1561/2300000053."
REFERENCES,0.41412213740458015,"Bilal Piot, Matthieu Geist, and Olivier Pietquin.
Boosted and reward-regularized classiﬁcation
for apprenticeship learning. Proceedings of the 13th Inter-national Conference on Autonomous
Agents and MultiagentSystems (AAMAS 2014), 2014. URL www.ifaamas.org."
REFERENCES,0.41603053435114506,"Andrew Silva, Taylor Killian, Ivan Rodriguez Jimenez, Sung-Hyun Son, and Matthew Gombolay.
Optimization methods for interpretable differentiable decision trees in reinforcement learning.
Proceedings of the 23rdInternational Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), 2020."
REFERENCES,0.4179389312977099,"Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply
aggrevated: Differentiable imitation learning for sequential prediction. Proceedings of the 34th
International Conference on Machine Learning, pp. 3309–3318, 7 2017. ISSN 2640-3498. URL
http://proceedings.mlr.press/v70/sun17d.html."
REFERENCES,0.4198473282442748,"Ryutaro Tanno, Kai Arulkumaran, Daniel C Alexander, Antonio Criminisi, and Aditya Nori. Adap-
tive neural trees. Proceedings of the 36th International Conference on Machine Learning, pp.
6166–6175, 5 2019. ISSN 2640-3498. URL http://proceedings.mlr.press/v97/
tanno19a.html."
REFERENCES,0.4217557251908397,"The Royal Society. Machine learning: The power and promise of computers that learn by example.
The Royal Society, 2017."
REFERENCES,0.42366412213740456,"Gilmer Valdes, Charles B. Simone, Josephine Chen, Alexander Lin, Sue S. Yom, Adam J. Pat-
tison, Colin M. Carpenter, and Timothy D. Solberg. Clinical decision support of radiotherapy
treatment planning: A data-driven machine learning strategy for patient-speciﬁc dosimetric de-
cision making. Radiotherapy and Oncology, 125:392–397, 12 2017. ISSN 0167-8140. doi:
10.1016/J.RADONC.2017.10.014."
REFERENCES,0.4255725190839695,"Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri.
Programmatically interpretable reinforcement learning. 35th International Conference on Ma-
chine Learning, ICML 2018, 11:8024–8033, 4 2018.
URL https://arxiv.org/abs/
1804.02477v3."
REFERENCES,0.42748091603053434,"Gert P. Westert,
Stef Groenewoud,
John E. Wennberg,
Catherine Gerard,
Phil Dasilva,
Femke Atsma,
and David C. Goodman.
Medical practice variation:
Public report-
ing a ﬁrst necessary step to spark change.
International Journal for Quality in Health
Care, 30:731–735, 11 2018.
ISSN 14643677.
doi:
10.1093/intqhc/mzy092.
URL
/pmc/articles/PMC6307331//pmc/articles/PMC6307331/?report=
abstracthttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6307331/."
REFERENCES,0.42938931297709926,"Herman Yau, Chris Russell, and Simon Hadﬁeld. What did you think would happen? explaining
agent behaviour through intended outcomes. Advances in Neural Information Processing Systems,
33:18375–18386, 2020. URL https://github.com/hmhyau/rl-intention."
REFERENCES,0.4312977099236641,"Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. Proceedings of the Twenty-Third AAAI Conference on Artiﬁcial Intelli-
gence, 2008. URL www.aaai.org."
REFERENCES,0.43320610687022904,Published as a conference paper at ICLR 2022
REFERENCES,0.4351145038167939,"Ariel Zylberberg, Jeannette AM Lorteije, Brian G Ouellette, Chris I De Zeeuw, Mariano
Sigman,
and Pieter Roelfsema.
Serial,
parallel and hierarchical decision making in
primates.
eLife, 6, 6 2017.
doi:
10.7554/ELIFE.17331.
URL /pmc/articles/
PMC5484613//pmc/articles/PMC5484613/?report=abstracthttps:
//www.ncbi.nlm.nih.gov/pmc/articles/PMC5484613/."
REFERENCES,0.43702290076335876,"A
ADDITIONAL RELATED WORK"
REFERENCES,0.4389312977099237,"A.1
IMITATION LEARNING"
REFERENCES,0.44083969465648853,"A.1.1
BEHAVIOURAL CLONING"
REFERENCES,0.44274809160305345,"Behavioural cloning (BC) is the simplest form imitation learning, as it reduces policy learning
to supervised learning of a direct mapping from states to actions, assuming a fully-observable
setting where S = Z (Bain & Sammut, 1996; Piot et al., 2014).
For a trajectories dataset
D = {(z1:T , a1:T )}, the objective is to recover the deterministic policy mapping π : Z →A,
such that π(zt) is the action following observation zt:"
REFERENCES,0.4446564885496183,"π = arg min
π
Ezt,at∼D [L (π(zt), at)]
(5)"
REFERENCES,0.44656488549618323,"where L(·, y) is a supervised learning loss function for target value y."
REFERENCES,0.4484732824427481,"This approach is typically compatible with ofﬂine learning, as it only requires access to demonstrated
trajectories of system states and actions (zt, at) at each timestep t. Its excellent learning efﬁciency
has motivated its use and deployment, including in safety-critical applications such as autonomous
driving (Farag, 2017)."
REFERENCES,0.45038167938931295,"With no explicit handling of system dynamics, however, this approach is highly susceptible to dis-
tributional shift (Ho & Ermon, 2016): at test time, the agent might enter states unseen in training
trajectories and thus drift away from its training domain and perform poorly. Likewise, BC is not
designed for non-Markovian or partially-observable environments, where the latest observation zt
is insufﬁcient to determine action choice. Representation learning of POMDP belief states through
recurrent neural networks (Sun et al., 2017) can help tackle these issues at the cost of interpretability,
as it introduces black-box elements in the model."
REFERENCES,0.45229007633587787,"A.1.2
STATE-ACTION DISTRIBUTION MATCHING"
REFERENCES,0.4541984732824427,"More recently, an alternative approach to dealing with covariate shift was proposed, based on match-
ing state-action distributions between the demonstrator and learned policies. Deﬁning occupancy
measures µπ : S × A →R for each policy π as µπ(s, a) = π(a|s) P∞
t=0 γtP(st = s|π), this can
be achieved by minimising the Jensen-Shannon divergence in occupancy measures for each policy,
while maximising the entropy of the learned one H(π) through generative adversarial training (Ho
& Ermon, 2016; Jeon et al., 2018; Kostrikov et al., 2020):"
REFERENCES,0.45610687022900764,"arg min
π
DJS (µπ∥µπE)
and
arg max
π
H(π)
(6)"
REFERENCES,0.4580152671755725,"In contrast to our descriptive goal, this approach therefore focuses on matching feature expectations
rather than explicitely recovering the demonstrated policy mapping from observations or belief states
to actions, based on the assumption that this behaviour is optimal – and thus aims to maximise
performance on a given task. In addition, it suffers from the same issue as BC-IL in providing
interpretable representations of belief states in partially-observable settings, despite Li et al. (2017)
improving explainability by clustering similar demonstration trajectories."
REFERENCES,0.4599236641221374,"In all cases, online roll-outs of the learned policy are required to train a dynamics model of the
environment, as P(st = s|π) is needed to estimate occupancy, making this work incompatible
with purely observational clinical data. To overcome this, model-based imitation learning (MB-
IL) proposes to train autoregressive exogenous models of environment dynamics (Englert et al.,
2013) for ofﬂine feature expectation. Encouraging results were obtained for fully-observable robotic
applications, but these dynamics remain excessively simple for the healthcare context."
REFERENCES,0.4618320610687023,A comprehensive review of imitation learning methods can be found in Osa et al. (2018).
REFERENCES,0.4637404580152672,Published as a conference paper at ICLR 2022
REFERENCES,0.46564885496183206,"A.2
APPRENTICESHIP LEARNING"
REFERENCES,0.4675572519083969,"Apprenticeship learning is another approach to modelling expert behaviour, concerned with match-
ing the performance of the demonstrating expert in terms of some unknown utility measure R. The
state-of-the-art solution to this task is inverse reinforcement learning (IRL), which involves recov-
ering this reward function from demonstration trajectories (Ng & Russell, 2000) – identifying the
expert’s hidden objectives. A forward reinforcement learning (RL) algorithm is subsequently ap-
plied to obtain the policy."
REFERENCES,0.46946564885496184,"The inverse task of uncovering the reward function is fundamentally ill-posed, as the observed be-
haviour can be optimal under a multitude of reward signals. Additional constraints must thus be
speciﬁed to obtain a unique solution, for instance by maximising policy entropy (Ziebart et al.,
2008; Choi & Kim, 2011) as described above for distribution-matching imitation learning, or by
maximising the margin between induced policies in terms of feature expectations (Ng & Russell,
2000; Abbeel & Ng, 2004):
arg min
π
Es∼µπE [V π(s) −V πE(s)]
(7)"
REFERENCES,0.4713740458015267,"where V π(s) is a parametrisation of the value function for policy π, the expected sum of total dis-
counted rewards following this policy, assuming a start in state s: V π(s) = Eπ[P∞
t=0 γtR(st)|s0 =
s]."
REFERENCES,0.4732824427480916,"IRL can deal with partially-observable environments (Choi & Kim, 2011) and even learn dynam-
ics models fully ofﬂine (Makino & Takeuchi, 2012). Still, in terms of providing insight into the
decision-making process, the indirect nature of the IRL ◦RL framework results in uninterpretable
policies – despite attempts to parametrise reward functions for explainability (Bica et al., 2021)."
REFERENCES,0.4751908396946565,"Table 3: Comparison of related work in light of our key policy learning goals.
DM-IL stands for
distribution-matching imitation learning. f stands for a representation learning step; bt is the agent’s belief
over the true state st."
REFERENCES,0.4770992366412214,"Related work
Optimisation
Ofﬂine
Partial
Interpretable
Objective
Learning
Observability
Policy"
REFERENCES,0.47900763358778625,"BC-IL (Bain & Sammut, 1996)
L(arg maxa π(a|zt), at)



PO-BC-IL (Sun et al., 2017)

L(arg maxa π(a|bt), at); bt = f(z1:t)



Interpretable BC-IL (H¨uy¨uk et al., 2021)


"
REFERENCES,0.48091603053435117,"DM-IL (Ho & Ermon, 2016)
)"
REFERENCES,0.48282442748091603,DKL (π(at|zt)∥πE(at|zt))
REFERENCES,0.4847328244274809,"


Interpretable DM-IL (Li et al., 2017)



MB-IL (Englert et al., 2013)


"
REFERENCES,0.4866412213740458,"IRL (Ng & Russell, 2000)
π = arg minπ Ezt∼µπE [V π(zt) −V πE(zt)]



PO-IRL (Choi & Kim, 2011)
)
π = arg minπ Ebt∼µπE [V π(bt) −V πE(bt)];
bt = f(z1:t)"
REFERENCES,0.48854961832061067,"


Ofﬂine PO-IRL (Makino & Takeuchi, 2012)



Subjective PO-IRL (Golub et al., 2013)


"
REFERENCES,0.4904580152671756,"Table 3 compares how related research addresses the key goals outlined for our work. Our policy
learning algorithm must be operable ofﬂine on observational data; must accommodate a partially-
observable environment to allow adaptive behaviour over time; and must result in an interpretable
representation of the policy. In comparison to H¨uy¨uk et al. (2021) which addresses all three goals,
we propose to implement a policy learning algorithm with fewer assumptions and restrictions on
the belief state and dynamics model structure, as well as a more interpretable and expressive policy
parametrisation."
REFERENCES,0.49236641221374045,"B
SOFT DECISION TREE OPTIMISATION"
REFERENCES,0.49427480916030536,"In this section, additional details are provided on our decision tree structure and optimisation proce-
dure."
REFERENCES,0.4961832061068702,"To improve optimisation, Frosst & Hinton (2017) encourage soft decision tree models to split data
evenly across tree inner nodes through an additional regularisation term Lsplit:"
REFERENCES,0.49809160305343514,"Lsplit = −λ
X"
REFERENCES,0.5,"n∈Inner
2−dn [0.5 log αn + 0.5 log(1 −αn)]
(8) where αn = P"
REFERENCES,0.5019083969465649,"x P n(x) pn
gate(x)
P"
REFERENCES,0.5038167938931297,"x P n(x)
,
(9)"
REFERENCES,0.5057251908396947,"and λ is a splitting penalty weight. This additional loss is essentially a cross-entropy between the
discrete binary distribution (0.5; 0.5) and the inner node’s data split (αn; 1 −αn); and decays with"
REFERENCES,0.5076335877862596,Published as a conference paper at ICLR 2022
REFERENCES,0.5095419847328244,"depth dn to allow uneven splits as the tree learns more complex hierarchies. This regularisation was
not found to signiﬁcantly affect optimisation but was nevertheless included in our objective in light
of Frosst & Hinton (2017)’s favourable results."
REFERENCES,0.5114503816793893,"The path probability P n(x) of node n, conditioned on tree input x, is formally deﬁned as follows:"
REFERENCES,0.5133587786259542,"P n(x) =
Y"
REFERENCES,0.5152671755725191,"m∈Spath(n)
pm
gate(x)1n∈Sright(m) ·
 
1 −pm
gate(x)
1−1n∈Sright(m)
(10)"
REFERENCES,0.517175572519084,"where Spath(n) are the set of nodes on the path from the tree root to node n; 1x is the indicator
function (returns 1 if x is true, 0 else); and Sright(m) are the set of nodes descending from m’s right
branch."
REFERENCES,0.5190839694656488,"Non-categorical action types can also be modelled by passing leaf parameters θl
a through a hy-
perbolic tangent function, and replacing the cross-entropy in equation 1 with the mean-squared
error. Validation performance in Algorithm 1 can be determined with traditional supervised learn-
ing metrics, such as target prediction accuracy, as well as with alternative success measures such as
interpretability ratings or domain-expert approval."
REFERENCES,0.5209923664122137,"A particular implementation challenge is to minimise the loss of information from previous opti-
misations during growth of the history tree, as new leaves are created (step 2a in Figure 9b). To
overcome this, split leaves are initialised to the previous value of the parent node, with added Gaus-
sian noise of variance σ2 = 1/d where d is the leaf depth. This facilitates local optimisation as the
tree grows, by ensuring new leaves still capture similar distributions to optimised parent nodes."
REFERENCES,0.5229007633587787,"Table 4 contrasts mappings learned by our recurrent decision trees (RDTs) and RNNs for different
time-series tasks. Note that for action or evolution estimation beyond the input timestep, i.e. for
τ > 0 in this table, state-action distribution matching methods become important in training to
overcome compounding errors at test time. Our differentiable recurrent tree structures can also be
optimised by backpropagation through time as for RNNs."
REFERENCES,0.5248091603053435,"B.1
INTERPRETABLE REPRESENTATION LEARNING FOR TIME-SERIES"
REFERENCES,0.5267175572519084,"Related work.
The state-of-the-art in interpretable time-series modelling or representation learn-
ing is to highlight the relative importance of features in contributing to changes in model predictions.
This can be achieved with attention mechanisms as in Choi et al. (2016), or with Shapley explana-
tions (Lundberg et al., 2017). Overall, these methods remain tools to gain approximate insight into
complex models, rather than a transparent description of their inner workings. In addition, patient
observations and medical actions are all treated as independent variables as illustrated in Table 4 –
a signiﬁcant assumption for policy learning, making models susceptible to confounding in obser-
vational data. In contrast, our work aims to distill decision-making pathways from observations to
actions in a clear format, comparable to the human thought process."
REFERENCES,0.5286259541984732,"Recurrent decision trees
This section demonstrates the gains achieved by our choice of history
representation learning model to tackle the challenges of partially-observable environments. In par-
ticular, Recurrent Decision Tree (RDT) models address our requirement for interpretable policies.
While the history embedding ht may not be interpretable, we experimentally ﬁnd that the tree-based
model is more understandable than an RNN. Different structures shown in Figures 7b and 7c were"
REFERENCES,0.5305343511450382,Table 4: Comparison of time-series methods with actions.
REFERENCES,0.5324427480916031,"Framework
Recurrent Neural Network (RNN)
Recurrent Decision Tree (RDT)"
REFERENCES,0.5343511450381679,"Tasks & targets
• Policy learning (explored in this work): at+τ|z1:t, a1:t−1
(τ ∈N)
• Evolution prediction (decision support): zt+τ|z1:t−1, a1:t−1
• Outcome prediction (decision support): yt+τ|z1:t−1, a1:t−1
Mapping
{ht; xt := {zt, at}} →ht+1
{ht; zt} →{at; ht+1}"
REFERENCES,0.5362595419847328,"Typical model
ht+1 = σ(Wrht + Wxxt + bh)
n
at = almax
t
; ht+1 = hlmax
t+1
o
; lmax = arg maxl P l(ht, zt)"
REFERENCES,0.5381679389312977,"Merits
• Modelling performance & ﬂexibility
• Interpretable model; ht modulates mapping from zt to target
• Disentanglement of ht and at via multiple outputs
• Joint recurrence and target prediction"
REFERENCES,0.5400763358778626,Published as a conference paper at ICLR 2022
REFERENCES,0.5419847328244275,"also studied, in which ht is ﬁrst extracted from z1:t−1 through an RNN or RDT, and is concatenated
to latest observation zt as input to a distinct action-decision tree. Results in Table 5 highlight that our
RDT architecture performs as well as the RNN history extractor in action-matching performance,
and, as expected, is preferred by surveyed physicians for its interpretability. Performance remained
acceptable as history dimensionality was decreased to 1 (< 5% loss in accuracy), which is valuable
for visualisation purposes."
REFERENCES,0.5438931297709924,(a) DT policy.
REFERENCES,0.5458015267175572,time delay RNN
REFERENCES,0.5477099236641222,(b) RNN+DT policy.
REFERENCES,0.549618320610687,time delay
REFERENCES,0.5515267175572519,(c) RDT+DT policy.
REFERENCES,0.5534351145038168,time delay
REFERENCES,0.5553435114503816,(d) RDT policy.
REFERENCES,0.5572519083969466,Figure 7: Decision tree policies with different history extraction models.
REFERENCES,0.5591603053435115,"Table 5: Performance of different history extraction models for decision tree policies on ADNI. Unless
shown, standard errors were all ≤0.02. History-extraction and action-prediction trees have respective depths
dH and dA."
REFERENCES,0.5610687022900763,"History extraction model
dH
dA
# Parameters
Interpretability
AUPRC
Brier"
REFERENCES,0.5629770992366412,"None (DT)
–
3.7 ± 0.4
36 ± 4
9.3 ± 0.3
0.72
0.25"
REFERENCES,0.5648854961832062,"RNN+DT
–
4.8 ± 0.2
253 ± 6
5.0 ± 0.5
0.83
0.17
RDT+DT
3.3 ± 0.4
2.5 ± 0.3
167 ± 12
6.3 ± 0.3
0.80
0.21
RDT
3.3 ± 0.7
122 ± 18
8.3 ± 0.3
0.82
0.18"
REFERENCES,0.566793893129771,"The unexpected result that our decision trees perform history representation nearly as well as neural
networks can be attributed to the sparse and heterogeneous nature of our time-series data, often
better captured through models based on decision trees (Frosst & Hinton, 2017). It is however not
expected that RDTs would be applicable to complex, structured history embeddings as in speech or
language modelling."
REFERENCES,0.5687022900763359,Published as a conference paper at ICLR 2022
REFERENCES,0.5706106870229007,"Leaf recurrence models
An important step in model design consists of choosing leaf parameters
for history prediction. In increasing order of complexity in terms of numbers of parameters per
leaf, Table 6 highlight a number of possibilities, where {θh ∈RM, θr ∈RM, θr ∈RM×M, θf ∈
RM×D} are trainable parameters for each leaf (for history and observation dimensionality M and D
respectively) and ⊙denotes element-wise multiplication. As for action prediction, the simplest solu-
tion is for leaves to be independent of inputs, outputting a ﬁxed history vector – either a distribution
over categorical values with equation 11, or a vector of continuous values through equation 12. De-
pendence on previous history ht−1 and on the newest observation zt−1 can be incorporated through
element-wise or matrix multiplications (equations 13–17). In particular, equation 17 corresponds to
a simple RNN unit."
REFERENCES,0.5725190839694656,"Table 6: Performance of different leaf models for history recurrence on ADNI. Standard error on tree depth
d is omitted for brevity."
REFERENCES,0.5744274809160306,"Recurrence equation
d
# Parameters
Accuracy
Warm Start Acc."
REFERENCES,0.5763358778625954,"ht+1 = softmax(θh)
(11)
2.7
114 ± 11
0.72 ± 0.02
–
ht+1 = tanh(θh)
(12)
3.0
130 ± 23
0.76 ± 0.03
–
ht+1 = tanh(θh + θr ⊙ht)
(13)
2.7
154 ± 26
0.68 ± 0.01
–
ht+1 = tanh(θh + θfzt)
(14)
3.0
335 ± 46
0.76 ± 0.01
0.73 ± 0.01
ht+1 = tanh(θh + θr ⊙ht + θfzt)
(15)
2.3
306 ± 19
0.75 ± 0.02
0.73 ± 0.01
ht+1 = tanh(θh + θrht)
(16)
3.3
522 ± 110
0.73 ± 0.02
0.77 ± 0.02
ht+1 = tanh(θh + θrht + θfzt)
(17)
3.7
847 ± 152
0.76 ± 0.02
0.77 ± 0.01"
REFERENCES,0.5782442748091603,"More complex leaf models tend perform best on ADNI, at the cost of tree readability and computa-
tion cost. Still, using ﬁxed leaf history parameters through a hyperbolic tangent activation (equation
12) result in second-best performance with a minimal number of parameters, and intuitive readabil-
ity – justifying the use of this parsimonious model for all other investigations. Initialising parameters
with values previously optimised for the RNN+DT recurrence model, in a warm start, was found to
achieve similar performance, but did not systematically improve results. A caveat should be added,
as some decision-making environments require more complex recurrence models to capture sub-
tleties in patient history. For instance, policy optimisation for our proof-of-concept SYNTH dataset
was challenging with history-independent models, and recurrence equation 16 was used instead.
Overall, this architecture choice depends on the complexity of the modelling task and on the nature
of demonstrated trajectories."
REFERENCES,0.5801526717557252,"B.2
IMPROVEMENT ON ESTABLISHED DECISION TREE MODELS"
REFERENCES,0.5820610687022901,"We conclude this section with a comparison of our method with the traditional Classiﬁcation And
Regression Tree (CART) and soft decision tree (SDT) algorithms in Table 7, to highlight our con-
tributions. In particular, our work combines soft and grown tree architectures (Frosst & Hinton,
2017; Tanno et al., 2019) and pioneers their application to time-series data instead of images. We
formalise and study the conversion of multidimensional gating functions to axis-aligned ones for in-
terpretability in Section C. Our ﬁnal contribution is an entirely novel approach to model time-series
through a multi-output recurrent tree structure, extending the representation learning model of Ding
et al. (2021) to the sequential setting and improving its interpretability by marginalising out obscure
latent variables."
REFERENCES,0.583969465648855,"Table 7: Comparison of our proposed architecture with traditional, soft and cascaded decision trees
(SDT) as in Breiman et al. (1984), Frosst & Hinton (2017) and Ding et al. (2021)."
REFERENCES,0.5858778625954199,"CART
SDT
CDT
POETREE"
REFERENCES,0.5877862595419847,"MODELLING TASKS
Discrete, categorical, continuous outputs




Multiple outputs (e.g., {at, ht, ˜zt})




Multidimensional decision boundaries




Interpretable decision boundaries




Time-dependence handling via recurrence



"
REFERENCES,0.5896946564885496,"OPTIMISATION
Optimisation objective
Gini impurity
Prediction error
Prediction error
Prediction error
Probabilistic decision boundaries




Gradient-descent optimisation




Tree depth growth & path pruning



"
REFERENCES,0.5916030534351145,Published as a conference paper at ICLR 2022
REFERENCES,0.5935114503816794,"While gradient-boosted decision tree structures have been reported to obtain good performance on
time-series forecasting tasks (Hyland et al., 2020), such ensemble methods result in multiple tree
structures to navigate which negatively affects interpretability (Lage et al., 2018). In addition, man-
ual pre-processing of the time-series data is required to include history information, which our model
avoids by independently learning history embeddings and marginalising them at interpretation time."
REFERENCES,0.5954198473282443,"C
AXIS-ALIGNED DECISION TREE MODELS"
REFERENCES,0.5973282442748091,"For easier human interpretation, our decision tree policies should be represented with deterministic,
axis-aligned thresholds at each inner node. We propose a post-processing method to achieve this
from multidimensional trees, as well as a variant of our proposed architecture to directly learn axis-
aligned decision boundaries during training, and optimise a policy closer to the structure used at
inference time."
REFERENCES,0.5992366412213741,"C.1
MULTIDIMENSIONAL TREE POST-PROCESSING"
REFERENCES,0.601145038167939,"A ﬁnal technical consideration required by the model architecture proposed in Section 3.1 is policy
post-processing for human interpretation. After optimisation, our decision trees consist of multidi-
mensional gating functions over the history and observations variables which are difﬁcult to under-
stand – as illustrated in Figure 8a, and must be converted to axis-aligned, unidimensional thresholds."
REFERENCES,0.6030534351145038,"Each inner node thus consists of learned parameters {b, w} and computes the probability of taking
the right-most branch as follows:"
REFERENCES,0.6049618320610687,"pgate(ht, zt) = σ  b + M
X"
REFERENCES,0.6068702290076335,"i=1
wi · ht,i + D
X"
REFERENCES,0.6087786259541985,"i=1
wm+i · zt,i ! (18)"
REFERENCES,0.6106870229007634,"where M and D are history and observation space dimensionalities. Superscripts denoting node
indices are omitted for clarity."
REFERENCES,0.6125954198473282,"The history vector is ﬁrst marginalised out into the bias term, to condition the resulting observation-
to-action policy on the history as desired. This results in a speciﬁc tree model for each timepoint,
which only depends on observation zt for easier human interpretation. Gating functions are re-
expressed as follows:"
REFERENCES,0.6145038167938931,"pgate(zt) = σ  b′ + D
X"
REFERENCES,0.6164122137404581,"i=1
wm+i · zt,i ! (19)"
REFERENCES,0.6183206106870229,"where b′ = b + PM
i=1 wi · ht,i is now a history-dependent bias term."
REFERENCES,0.6202290076335878,"As in Silva et al. (2020), these time-dependent multidimensional policies are then converted into
axis-aligned unidimensional trees by taking the largest component of the remaining gating parame-
ters at each node, wmax:"
REFERENCES,0.6221374045801527,"pgate(zt) > 0.5
∼∼∼
b′ + wmax · zt,max > 0,
where wmax = max{wm+i}D
i=1
(20)"
REFERENCES,0.6240458015267175,"=⇒zt,max > −b′/wmax
(21)"
REFERENCES,0.6259541984732825,"(or
zt,max < −b′/wmax
if wmax < 0)
(22)"
REFERENCES,0.6278625954198473,"Figures 8b and 8c are examples of axis-aligned policies at different timesteps for typical patient B
(with degrading MCI symptoms), much easier to follow than the multidimensional tree in Figure 8a."
REFERENCES,0.6297709923664122,"Table 8: Performance of multidimensional and different axis-aligned decision tree policy structures. Stan-
dard errors all ≤1%."
REFERENCES,0.6316793893129771,"Adaptive decision tree policy
ADNI Accuracy (%)
MIMIC Accuracy (%)"
REFERENCES,0.6335877862595419,"Multidimensional
77.6
79.9
Axis-aligned; bias b′
74.7
75.9
Axis-aligned; bias b′ + P"
REFERENCES,0.6354961832061069,"i̸=max wi · zt,i
76.9
77.8
Axis-aligned; bias b′ + P"
REFERENCES,0.6374045801526718,"i̸=max wi · ˜zt,i
75.4
76.0"
REFERENCES,0.6393129770992366,"Axis-aligned thresholds were also improved by exploiting our framework to predict expected obser-
vations at the subsequent timestep ˜zt+1. Thresholds were adjusted by incorporating the predictions"
REFERENCES,0.6412213740458015,Published as a conference paper at ICLR 2022
REFERENCES,0.6431297709923665,(a) Multidimensional policy.
REFERENCES,0.6450381679389313,Vh low
REFERENCES,0.6469465648854962,"MRI
MRI"
REFERENCES,0.648854961832061,Vh low
REFERENCES,0.6507633587786259,Vh not low
REFERENCES,0.6526717557251909,CDR-SB normal MRI
REFERENCES,0.6545801526717557,No previous MRI MRI
REFERENCES,0.6564885496183206,Go right
REFERENCES,0.6583969465648855,CDR-SB normal
REFERENCES,0.6603053435114504,"MRI
MRI"
REFERENCES,0.6622137404580153,CDR-SB severe yes no
REFERENCES,0.6641221374045801,"(b) Axis-aligned policy, t = 1."
REFERENCES,0.666030534351145,"MRI
MRI"
REFERENCES,0.6679389312977099,"Vh low
CDR-SB normal MRI"
REFERENCES,0.6698473282442748,No previous MRI MRI
REFERENCES,0.6717557251908397,Go right
REFERENCES,0.6736641221374046,CDR-SB normal
REFERENCES,0.6755725190839694,"MRI
MRI"
REFERENCES,0.6774809160305344,CDR-SB severe
REFERENCES,0.6793893129770993,"Go right
CDR-SB questionable yes no"
REFERENCES,0.6812977099236641,"(c) Axis-aligned policy, t = 4."
REFERENCES,0.683206106870229,Vh low
REFERENCES,0.6851145038167938,"CDR-SB normal
No previous MRI"
REFERENCES,0.6870229007633588,"MRI
MRI"
REFERENCES,0.6889312977099237,"(d) Axis-aligned policy after pruning, t = 1."
REFERENCES,0.6908396946564885,CDR-SB severe MRI
REFERENCES,0.6927480916030534,"(e) Axis-aligned policy after pruning, t = 4."
REFERENCES,0.6946564885496184,"Figure 8: Transformation of multidimensional tree policy into axis-aligned and pruned structure. History
and expected evolution outputs are not represented for clarity."
REFERENCES,0.6965648854961832,Published as a conference paper at ICLR 2022
REFERENCES,0.6984732824427481,"for all components except ˜zt,max in the bias term, transforming equation 21 into:"
REFERENCES,0.700381679389313,"zt,max > −
b′ + P"
REFERENCES,0.7022900763358778,"i̸=max wi · ˜zt,i"
REFERENCES,0.7041984732824428,"wmax
(23)"
REFERENCES,0.7061068702290076,"As shown in Table 8, incorporating true and expected observation values in the thresholds improved
the axis-aligned model performance. Naturally, true test observations for the corresponding timestep
are unavailable, and can therefore not be used to adjust the threshold; only expected observations
can be used for this. This investigation highlights another beneﬁt of the expected patient evolution
model, in addition to explanatory insights."
REFERENCES,0.7080152671755725,"Further post-processing of axis-aligned policies in Figures 8b and 8c is required in the form of node
pruning, setting a minimum path probability threshold on the validation set, to address the following
concerns:"
REFERENCES,0.7099236641221374,"• Axis-aligned thresholds are often sensible and typically in the [0, 1] range for categorical
variables, converting straightforwardly to meaningful decisions. Negative or out-of-scope
thresholds, however, exclude all data from a child path (e.g., left branch of the ”Go right”
node in Figure 8b), which can be pruned from the tree.
• Learning interdependencies between variables within the hierarchical structure is also chal-
lenging. For example, in Figure 8b, the ﬁrst right branch (low Vh on previous scan) is
incompatible with the following right branch (not low Vh). In practice, we obtained this
structure due to (1) soft partitions, with examples shared across incompatible branches;
and as (2) multi-dimensional gating affects thresholds in training. Again, the solution is to
eliminate non-sensical children nodes in post-processing."
REFERENCES,0.7118320610687023,"Accuracy loss was found to be virtually null (< 2%) with a pruning probability threshold of 0.05 on
the validation set Dval. Formally, average path probability for node l was estimated as:"
REFERENCES,0.7137404580152672,"P l(Dval) =
1
|Dval| X"
REFERENCES,0.7156488549618321,"{ht,zt} ∈Dval
P l(ht, zt)
(24)"
REFERENCES,0.7175572519083969,"and nodes with P l(Dval) < 0.05 were eliminated from the structure. Indistinguishable performance
of the multidimensional and axis-aligned tree mode was even obtained on ADNI by training the
multidimensional structure with an increasing L1 regularisation weight, to allow greater ﬂexibility
at initial stages and induce a more axis-aligned structure at the end of training. With an L1 weight
from 10−5 to 10−1, this resulted in an action-matching accuracy of 0.75 ± 0.01 and AUROC of 0.52
± 0.02 with both multidimensional and post-processed models."
REFERENCES,0.7194656488549618,"Overall, axis-aligned trees therefore offered excellent performance, comparable to multidimensional
ones. As the observation space dimensionality d increases, however, approximations become less
accurate. To overcome this, trees can be trained closer to their axis-aligned version during opti-
misation, for instance with L1 regularisation to induce sparsity in the gating functions. On a 15-
dimensional observation space with the SYNTH simulated policy, an L1-regularisation weight of
0.01 on gating parameters decreased multidimensional action-matching accuracy by 0.7% overall,
but improved axis-aligned performance by 5%. This suggests a promising avenue to ensure the
scalability of our model."
REFERENCES,0.7213740458015268,"C.2
AXIS-ALIGNED TREE TRAINING"
REFERENCES,0.7232824427480916,"In this section, we propose a variant to our optimisation algorithm in order to directly train a structure
that is closer to the one used in inference time."
REFERENCES,0.7251908396946565,"The following gating function was designed to allow for axis-aligned training while keeping the tree
structure differentiable:"
REFERENCES,0.7270992366412213,"pgate(x) = d
Y"
REFERENCES,0.7290076335877863,"i=1
σ (xiwi + bi)
(25)"
REFERENCES,0.7309160305343512,"for an input x ∈Rd. This function deﬁnes a soft AND gate over d axis-aligned thresholds, where
a threshold value −bi/wi is learned for each input dimension i. At inference time, each inner node
consists of d axis-aligned thresholds, one for each input dimension, which must all be satisﬁed to
proceed to the right child branch."
REFERENCES,0.732824427480916,Published as a conference paper at ICLR 2022
REFERENCES,0.7347328244274809,"In a recurrent architecture, multidimensional gating functions on the history embedding ht can be
kept for greater model ﬂexibility, since these are marginalised at interpretation time. Axis-aligned
gating functions can be applied to the observation values zt only, such that:"
REFERENCES,0.7366412213740458,"pgate(ht, zt) = σ
 
hT
t w′ + b′
× d
Y"
REFERENCES,0.7385496183206107,"i=1
σ (zt,iwi + bi)
(26)"
REFERENCES,0.7404580152671756,"where σ is the sigmoid function and {w′, b′, {wi, bi}d
i=1} are leaf parameters."
REFERENCES,0.7423664122137404,"Action-matching performance. Overall, as shown in Table 9, slightly superior performance was
still obtained from training and post-processing multidimensional soft architectures in comparison to
axis-aligned ones. This greater performance may be explained by greater ﬂexibility during training
time, particularly as tree structures are grown from low-depth trees with poor discrimination if only
considering unidimensional partitions."
REFERENCES,0.7442748091603053,"Table 9: Performance of multidimensional and axis-aligned decision tree policy structures on ADNI. The
static setting corresponds to learning a mapping zt to at."
REFERENCES,0.7461832061068703,"Static setting
Recurrent setting"
REFERENCES,0.7480916030534351,"Tree structure
Accuracy
AUROC
Accuracy
AUROC"
REFERENCES,0.75,"Multidimensional
0.75 ± 0.01
0.53 ± 0.01
0.78 ± 0.02
0.62 ± 0.01
Axis-aligned (post-processed from multi.)
0.73 ± 0.01
0.52 ± 0.02
0.76 ± 0.02
0.59 ± 0.01
Trained axis-aligned (SDT)
0.72 ± 0.02
0.52 ± 0.02
0.75 ± 0.02
0.56 ± 0.02
Trained axis-aligned (CART)
0.69 ± 0.01
0.50 ± 0.04
–
–"
REFERENCES,0.7519083969465649,"Better performance over a deterministic tree structure (CART) was also obtained. On our relatively
small datasets of a few thousand patients, sharing information from each patient across every node
through probabilistic gating functions ensures no part of the tree is trained on excessively few exam-
ples – this may help combat overﬁtting, which decision trees are often subject to (Frosst & Hinton,
2017). An additional source of difference in performance at test time is that our differentiable struc-
ture is trained by gradient descent, while traditional trees are built by Gini impurity splitting."
REFERENCES,0.7538167938931297,"D
ALGORITHM IMPLEMENTATION DETAILS"
REFERENCES,0.7557251908396947,"Our POETREE algorithm was implemented as follows for our experimental investigations3. Models
were built using the open-source automatic differentiation framework JAX4."
REFERENCES,0.7576335877862596,"All observation inputs were normalised prior to modelling. Hyperbolic tangent outputs for ˜zt+1
and axis-aligned thresholds were mapped back to the observation space after training for human-
readability and interpretation. History representation learning was integrated in the decision, through
the recurrent decision tree structure described in Section 3.2. Leaf models for history prediction were
independent of previous history: ht+1 = tanh(b), where b ∈Rm is a trainable parameter for each
leaf and m is the history dimension."
REFERENCES,0.7595419847328244,"Following Algorithm 1, tree structures were initialised from depth 2, to avoid excessively simple
local optima. For tree growth, limited to depth 5, the AUROC score was used as a measure of
validation performance, to determine whether to split leaves into new inner nodes. All global and
local optimisations were carried out until convergence – no improvements on the validation set for
50 update iterations. We used the Adam optimiser (Kingma & Ba, 2015) with a step size of 0.001
and a batch size of 32. Model hyperparameters in Table 10 were optimised through grid search on
validation datasets – random subsets of 10% of the training data."
REFERENCES,0.7614503816793893,"All experiments were run with 5 different model initialisations and data splits and the average results
and standard errors were reported. Optimisation of tree structures was found to be temperamental,
with some initialisations not improving in performance with training. A restarting procedure was
therefore implemented if loss did not decrease by more than 5% over the ﬁrst ﬁve epochs; and was
also applied to benchmark algorithms if necessary. Experiments were performed on a Microsoft
Azure virtual machine with 6 cores and powered by a Tesla K80 GPU."
REFERENCES,0.7633587786259542,"3Code is made available at https://github.com/alizeepace/poetree and https://
github.com/vanderschaarlab/mlforhealthlabpub.
4https://github.com/google/jax"
REFERENCES,0.7652671755725191,Published as a conference paper at ICLR 2022
REFERENCES,0.767175572519084,"Table 10: Hyperparameter grid for POETREE optimisation, with values optimised for the ADNI dataset."
REFERENCES,0.7690839694656488,"Hyperparameter
Search range
Optimal values"
REFERENCES,0.7709923664122137,"History dimension m
1 – 20
8
Splitting penalty λ
10−4 – 101
10−1"
REFERENCES,0.7729007633587787,"Evolution prediction δ1
10−3 – 10−1
10−2"
REFERENCES,0.7748091603053435,"Evolution prediction δ2
10−4 – 10−1
10−3"
REFERENCES,0.7767175572519084,"Ablation Study
The results of a brief study of the impact of different loss terms is provided in
Table 11. The choice for the loss of the tree structure itself L was ﬁrst studied, comparing the
cross-entropy between targets and each leaf’s output distribution ˆal, weighted by their respective
path probability P l(z), the cross-entropy with the weighted average of all leaf outputs (L′), and
the cross-entropy with the output of the maximum-probability leaf lmax (L′′). The ﬁrst objective
function, proposed by Frosst & Hinton (2017), returned marginally better results, as it may better
capture the different contribution from each element of the structure."
REFERENCES,0.7786259541984732,"Table 11: Performance of decision tree policies optimised with different objective functions on ADNI.
CE(·, a) is to the categorical cross-entropy loss with respect to target a."
REFERENCES,0.7805343511450382,"Objective function
Action-matching accuracy
Relative MSE on ˜zt+1 (%) L = P"
REFERENCES,0.7824427480916031,"l P l(z) · CE(ˆal, a)
0.78 ± 0.01
60 ± 10
L′ = CE(P"
REFERENCES,0.7843511450381679,"l P l(z) · ˆal, a)
0.76 ± 0.01
–
L′′ = CE(ˆalmax, a) where lmax = maxl P l(z)
0.77 ± 0.01
–"
REFERENCES,0.7862595419847328,"L + MSE(zt+1)
0.75 ± 0.02
7 ± 4
L + L˜z
0.77 ± 0.02
16 ± 6
L + L˜z + Lsplit
0.77 ± 0.01
13 ± 4"
REFERENCES,0.7881679389312977,"The performance of our model was also evaluated under the addition of the expected evolu-
tion regulariser L˜z, itself composed of a mean-squared error term on the predicted observation,
MSE(zt+1) = δ1 ∥zt+1 −˜zt+1∥2, and a term penalising inconsistent action choices based on this
prediction, δ2 DKL (π(·|ht+1, zt+1∥π(·|ht+1, ˜zt+1)). Results suggest that the additional evolution
loss term L˜z allows to balance consistency to the policy (by restoring the action-matching perfor-
mance degraded with only the MSE term) and ﬁdelity to the observation evolution. Finally, the
splitting regularisation term Lsplit improved consistency between runs but did not largely improve
performance."
REFERENCES,0.7900763358778626,"Complexity Analysis
This section highlights the beneﬁt of our tree growth procedure for both
interpretability, complexity and performance optimisation. Figure 9a compares results obtained
from trees of ﬁxed depth and trees grown during training. Incrementally grown trees outperformed
all ﬁxed structures with a minimal number of parameters (with, on average, 40% fewer parameters
than a ﬁxed tree of the same depth) and reasonable training time (faster than ﬁxed trees of depth
4 or 5, despite multiple local optimisations). Trees grown during the training process thus learn to
capture general hierarchies in the data while eliminating unnecessary partitions, which otherwise
make the tree more difﬁcult to read as depth increases (Lage et al., 2018)."
REFERENCES,0.7919847328244275,"A comparison of performance against tree depth after optimisation is also proposed in Figure 9b,
demonstrating that our incrementally grown structure adapts its complexity to the the task at hand.
Indeed, while performance on the training set continuously increases with tree depth, test accuracy
is optimal at intermediate model complexities which avoid overﬁtting while capturing subtleties in
the data. With an average tree depth of 3.3 ± 0.7, our optimisation procedure therefore success-
fully implements this Occam’s razor. Tree depth was also found to adapt to sample complexity, as
optimised depth decreased to 2.4 ± 0.3 when input dataset size was halved."
REFERENCES,0.7938931297709924,"Table 12: Complexity analysis of different policy learning algorithms on ADNI. Runtime is measured as
computation time for the prediction of all test actions (10% of demonstrations trajectories)."
REFERENCES,0.7958015267175572,"Model
Optimisation time (s)
Runtime (s)"
REFERENCES,0.7977099236641222,"POETREE
205 ± 32
< 1
POETREE with inductive bias (Figure 12)
141 ± 24
< 1
INTERPOLE (H¨uy¨uk et al., 2021)
752 ± 63
26 ± 4"
REFERENCES,0.799618320610687,Published as a conference paper at ICLR 2022
REFERENCES,0.8015267175572519,(a) Fixed trees
REFERENCES,0.8034351145038168,"1
2
3
4
5
6
Grown tree depth 64 66 68 70 72 74 76 78 80"
REFERENCES,0.8053435114503816,Accuracy (%)
REFERENCES,0.8072519083969466,Average optimized depth
REFERENCES,0.8091603053435115,"Test
Training"
REFERENCES,0.8110687022900763,(b) Grown trees
REFERENCES,0.8129770992366412,"Figure 9: Performance on ADNI as a function of tree depth, for ﬁxed and grown trees. Our growth
optimisation procedure adapts model complexity to the task at hand to (a) facilitate interpretation and (b) avoid
overﬁtting."
REFERENCES,0.8148854961832062,"Finally, Table 12 highlights the superior efﬁciency of our algorithm in training and runtime over its
closest related work, an interpretable PO-BC-IL model. This can be understood from the expensive
computation of Input-Output Hidden Markov Model (IOHMM) parameters to represent the POMDP
environment in the latter (Bengio & Frasconi, 1995). Model optimisation was accelerated when
initialising the tree structure with a sensible clinical policy as detailed in Section F."
REFERENCES,0.816793893129771,"E
BENCHMARKS IMPLEMENTATION DETAILS"
REFERENCES,0.8187022900763359,"All learned POMDP environment models are implemented as an Input-Output Hidden Markov
Model (IOHMM) (Bengio & Frasconi, 1995) and initialised uniformly at random. Benchmark im-
plementation is similar to H¨uy¨uk et al. (2021) as this work shares our goal of interpretable policy
learning. Online-learning algorithms such as explainable DM-IL (Li et al., 2017) could not be used
for benchmarking, as our medical datasets do not allow interaction with the decision-making envi-
ronment."
REFERENCES,0.8206106870229007,"PO-BC-IL
The recurrent neural network is implemented as an LSTM unit (Hochreiter & Schmid-
huber, 1997) and a fully-connected layer, both of size 64; and trained through optimisation of the
categorical cross-entropy over actions using the Adam optimiser (Kingma & Ba, 2015) with learning
rate 0.001 and batch size 32 until convergence."
REFERENCES,0.8225190839694656,"PO-IRL
IOHMM parameters are ﬁrst trained (Bengio & Frasconi, 1995) and freezed. The reward
function is initialised as R(s, a) ∼N(0, 0.0012) and is estimated as the average of 50 Markov Chain
Monte Carlo (MCMC) samples, retaining every tenth drawn value after burning-in 500 samples. In
Ofﬂine PO-IRL, IOHMM parameters and reward signal are jointly learned by MCMC sampling.
Q-values are then recovered with an off-the-shelf POMDP solver5."
REFERENCES,0.8244274809160306,"PO-MB-IL
After training and freezing IOHMM parameters, policies are parametrised in terms of
decision boundaries as in H¨uy¨uk et al. (2021), and initialised randomly. Training is carried out by
maximising the likelihood of actions in the demonstrations dataset by Expectation-Maximisation.
Maximisation is carried out with the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.001
until convergence."
REFERENCES,0.8263358778625954,"Interpretability benchmarks
Our fully-observable behavioural cloning benchmarks include lo-
gistic regression6 and a static version of POETREE illustrated in Figure 2a. Feature importance
analysis extracts each input dimension’s contribution to the predictions of the PO-BC-IL model
through Shapley values (Lundberg et al., 2017)."
REFERENCES,0.8282442748091603,"5https://www.pomdp.org/code/index.html.
6https://scikit-learn.org/"
REFERENCES,0.8301526717557252,Published as a conference paper at ICLR 2022
REFERENCES,0.8320610687022901,"F
DATASET DETAILS"
REFERENCES,0.833969465648855,"F.1
PROOF-OF-CONCEPT POLICY"
REFERENCES,0.8358778625954199,"For the SYNTH dataset, treatment efﬁcacy on diseased patients was set to 40%, with no spontaneous
state changes and an initial 80% diseased population. Diagnostic tests were given a 99% precision
and 5% false alarms."
REFERENCES,0.8377862595419847,"F.2
THE ALZHEIMER’S DISEASE NEUROIMAGING INITIATIVE (ADNI)"
REFERENCES,0.8396946564885496,"Rare visits without a CDR-SB measurement were discarded, as well as visits separated by more
than 6 months from the previous one – leaving 1,626 patients with at least two visits and a median
of three visits. Average MRI hippocampal volumes were taken as within half a standard deviation
from the population mean."
REFERENCES,0.8416030534351145,"Published guidelines for Alzheimer’s disease investigation are reproduced in Figure 11. With the
help of one of the surveyed physicians, this was distilled as a static tree policy in Figure 12 to
initialise POETREE and assess the effect of inductive bias on optimisation performance. Gaussian
noise of variance σ2 = 0.1 was added to initialisation parameters. Table 12 evidences the complexity
gains afforded by this incorporation of prior knowledge, with action-matching performance reaching
0.62 ± 0.01 AUROC, 0.84 ± 0.01 AUPRC, and 0.18 ± 0.01 Brier score."
REFERENCES,0.8435114503816794,"F.3
MEDICAL INFORMATION MART FOR INTENSIVE CARE (MIMIC-III)"
REFERENCES,0.8454198473282443,"Patients trajectories from the MIMIC dataset with missing values in one of the observation fea-
tures, or with non-consecutive measurements, were eliminated. In total, 4,222 ICU patients were
considered over up to 6 timesteps."
REFERENCES,0.8473282442748091,"Policies recovered for the MIMIC antibiotic prediction task by our algorithm and the decision-
boundary framework (H¨uy¨uk et al., 2021) are given in Figure 13. The latter again falls short of
highlighting how patients are mapped onto the belief state, giving our decision tree greater expres-
sive power. Our results are consistent with policy learning work on the same task and dataset (Bica
et al., 2021), which identiﬁes similar discriminative features: fever and abnormally high or low
white blood cell counts are known clinical criteria for infection and antibiotic treatment, if a bacte-
rial source is suspected (Masterton et al., 2008). Continuous variable thresholds evolve over time
through their dependence on history: the temperature criterion, for instance, adjusts to whether the
patient has a prior for infection."
REFERENCES,0.8492366412213741,"G
DISCUSSION WITH CLINICIANS"
REFERENCES,0.851145038167939,"To evaluate the interpretability of our decision tree policies, we consulted ﬁve practising physicians
for their feedback on our work in comparison to the state-of-the-art. Each physician was given
a short presentation introducing (1) our goal of representing the decision-making behaviour in an
intelligible way, (2) the ADNI dataset and the studied clinical environment, (3) how policies can
be represented in terms of decision boundaries over belief simplices (H¨uy¨uk et al., 2021), black-
box mappings from beliefs to actions (Sun et al., 2017), and direct mappings from observations to
actions (as in our work). All participants were unaware of what method we proposed, to minimise
any implicit bias in results."
ETHICS STATEMENT,0.8530534351145038,"Ethics Statement
Our survey does not require formal ethics oversight as the identity of our clin-
icians cannot be obtained from published information (see U.S. Department of Health and Human
Services 45 CFR 46.104(d)(2)(i)), and as no sensitive or private data was shared with nor obtained
from participants. We followed the same procedure as in H¨uy¨uk et al. (2021) to ensure a fair com-
parison to this related work."
ETHICS STATEMENT,0.8549618320610687,"All ﬁve clinicians expressed the following preference in terms of interpretability of the decision-
making process:
RNN < Decision Boundaries < Decision Trees"
ETHICS STATEMENT,0.8568702290076335,"where the RNN policies, learned by PO-BC-IL (Sun et al., 2017), correspond to black-box mappings
from observation to history, and from history to actions. This was illustrated in our survey through
Table 13. Decision-boundary policies correspond to the INTERPOLE benchmark in Figure 3b (H¨uy¨uk
et al., 2021). Finally, decision tree policies learned through our POETREE framework were illustrated
by Figure 3a."
ETHICS STATEMENT,0.8587786259541985,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.8606870229007634,Vh low
ETHICS STATEMENT,0.8625954198473282,"CDR-SB normal
CDR-SB severe"
ETHICS STATEMENT,0.8645038167938931,"MRI
MRI"
ETHICS STATEMENT,0.8664122137404581,Healthy yes no
ETHICS STATEMENT,0.8683206106870229,Vh low
ETHICS STATEMENT,0.8702290076335878,"CDR-SB questionable
CDR-SB severe"
ETHICS STATEMENT,0.8721374045801527,"MRI
MRI"
ETHICS STATEMENT,0.8740458015267175,"MCI
Dementia"
ETHICS STATEMENT,0.8759541984732825,CDR-SB severe MRI yes no yes no
ETHICS STATEMENT,0.8778625954198473,(a) Patient A: Healthy
ETHICS STATEMENT,0.8797709923664122,Vh low
ETHICS STATEMENT,0.8816793893129771,"CDR-SB questionable
CDR-SB severe"
ETHICS STATEMENT,0.8835877862595419,"MRI
MRI MCI"
ETHICS STATEMENT,0.8854961832061069,Vh low
ETHICS STATEMENT,0.8874045801526718,"CDR-SB normal
CDR-SB severe"
ETHICS STATEMENT,0.8893129770992366,"MRI
MRI"
ETHICS STATEMENT,0.8912213740458015,Healthy yes no
ETHICS STATEMENT,0.8931297709923665,Dementia
ETHICS STATEMENT,0.8950381679389313,CDR-SB severe MRI yes no yes no
ETHICS STATEMENT,0.8969465648854962,(b) Patient B: Degrading
ETHICS STATEMENT,0.898854961832061,Dementia
ETHICS STATEMENT,0.9007633587786259,CDR-SB severe MRI
ETHICS STATEMENT,0.9026717557251909,Vh low
ETHICS STATEMENT,0.9045801526717557,"CDR-SB normal
CDR-SB severe"
ETHICS STATEMENT,0.9064885496183206,"MRI
MRI"
ETHICS STATEMENT,0.9083969465648855,Healthy
ETHICS STATEMENT,0.9103053435114504,Vh low
ETHICS STATEMENT,0.9122137404580153,"CDR-SB questionable
CDR-SB severe"
ETHICS STATEMENT,0.9141221374045801,"MRI
MRI MCI yes no yes no yes no"
ETHICS STATEMENT,0.916030534351145,(c) Patient C: Dementia
ETHICS STATEMENT,0.9179389312977099,"Figure 10: Adaptive decision tree policy for three typical ADNI patients, with each path highlighted in blue."
ETHICS STATEMENT,0.9198473282442748,"Figure 11: Published clinical diagnostic policy for patient with MCI symptoms. Figure reproduced from
Biasutti et al. (2012)."
ETHICS STATEMENT,0.9217557251908397,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.9236641221374046,Vh low
ETHICS STATEMENT,0.9255725190839694,CDR-SB severe
ETHICS STATEMENT,0.9274809160305344,CDR-SB questionable
ETHICS STATEMENT,0.9293893129770993,"MRI
Vh above avg MRI"
ETHICS STATEMENT,0.9312977099236641,"MRI
Vh low MRI yes no"
ETHICS STATEMENT,0.933206106870229,No previous MRI
ETHICS STATEMENT,0.9351145038167938,"Figure 12: Policy initialisation for inductive bias analysis, based on Figure 11."
ETHICS STATEMENT,0.9370229007633588,"Healthy
Infected"
ETHICS STATEMENT,0.9389312977099237,Antibiotics
ETHICS STATEMENT,0.9408396946564885,"(a) Decision-boundary policy
(H¨uy¨uk et al., 2021)."
ETHICS STATEMENT,0.9427480916030534,T > 38°C
ETHICS STATEMENT,0.9446564885496184,"Hematocrit < 36
WBC > 12"
ETHICS STATEMENT,0.9465648854961832,"WBC < 3
Antibiotics"
ETHICS STATEMENT,0.9484732824427481,Antibiotics
ETHICS STATEMENT,0.950381679389313,Antibiotics
ETHICS STATEMENT,0.9522900763358778,"Healthy
Infected yes no"
ETHICS STATEMENT,0.9541984732824428,Antibiotics
ETHICS STATEMENT,0.9561068702290076,T > 37.8°C yes no
ETHICS STATEMENT,0.9580152671755725,WBC > 11
ETHICS STATEMENT,0.9599236641221374,Antibiotics yes no
ETHICS STATEMENT,0.9618320610687023,T > 39°C
ETHICS STATEMENT,0.9637404580152672,"(b) Adaptive decision tree policy
(only 2 timesteps visualised)."
ETHICS STATEMENT,0.9656488549618321,Figure 13: Recovered policies for MIMIC. Units for white blood cell count (WBC) are 1000/mm3.
ETHICS STATEMENT,0.9675572519083969,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.9694656488549618,"Although more participants would be required for greater reliability, this survey was useful to con-
ﬁrm end-user’s preference for our representation of behaviour through trees, and to validate the pri-
orities of our work. Additional comments on the belief space and decision-boundary policy (H¨uy¨uk
et al., 2021) are summarised below:"
ETHICS STATEMENT,0.9713740458015268,"• Classifying patients within discrete disease states aligns with the cognitive process followed
in clinical practice (O’Bryant et al., 2008), but is challenging to achieve in practice, as
patients often fall on a continuum of symptoms and illness severity."
ETHICS STATEMENT,0.9732824427480916,"• Visual representations of belief states can be misleading. In particular, the triangular rep-
resentation in H¨uy¨uk et al. (2021) does not evidence the linear progression from normal to
MCI to dementia, considered the traditional patient evolution (NICE, 2018). In addition,
higher-dimensional, hierarchical or overlapping belief states cannot be readily visualised."
ETHICS STATEMENT,0.9751908396946565,"• The mapping from patient observations Z to belief space S, and the evolution of patient
trajectories in this space as new information is acquired, is unclear."
ETHICS STATEMENT,0.9770992366412213,"Feedback on our decision tree policies was also collected, highlighting important advantages and
limitations:"
ETHICS STATEMENT,0.9790076335877863,"• The ﬂow chart captured by the decision tree was described by clinicians as most similar
to their own approach to care, hierarchically eliminating possible diagnoses, treatments or
investigations – in agreement with cognitive research (Zylberberg et al., 2017) and estab-
lished guidelines illustrated in Figure 11."
ETHICS STATEMENT,0.9809160305343512,"• The concept of action value (for diagnostic tests, value of information; for treatments,
patient improvement) is captured in an easily understandable way in terms of expected
patient evolution. In contrast, they noted, reward functions and belief updates proposed by
related work (Makino & Takeuchi, 2012; H¨uy¨uk et al., 2021) do not represent the medical
thought-process as faithfully."
ETHICS STATEMENT,0.982824427480916,"• Although the recurrent decision tree mapping from leaves to a subsequent tree policy was
appreciated, there remain limitations in terms of the interpretability of the history embed-
ding, particularly with complex leaf recurrence models. In further work, we could visu-
alise history embeddings over a belief simplex following H¨uy¨uk et al. (2021), conditioning
vertices to encode distinct beliefs about the patient condition (e.g., diagnoses). Different
regions of belief space would thus be associated with different tree policies, rather than a
single most-likely action."
ETHICS STATEMENT,0.9847328244274809,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.9866412213740458,"Table 13: Patient trajectories under a policy learned by PO-BC-IL. History embeddings are obtained
through a recurrent neural network."
ETHICS STATEMENT,0.9885496183206107,"Timestep
Observation zt
History ht
Action at
CDR-SB
Previous MRI"
ETHICS STATEMENT,0.9904580152671756,"t = 1
Normal
Average Vh
[+0.5, −0.5, +0.5, +1.0, +0.2, −0.1, +0.9, −0.9]

t = 2
Normal

[−0.4, +1.0, −0.7, +0.9, +0.8, +1.0, −0.9, +0.6]

t = 3
Normal

[−0.2, +0.1, −0.1, +0.6, +0.9, +0.8, +0.6, −0.4]

t = 4
Normal

[−0.2, +0.8, −0.6, +0.3, +0.8, +1.0, +0.0, +0.4]

(a) Patient A (Healthy)"
ETHICS STATEMENT,0.9923664122137404,"Timestep
Observation zt
History ht
Action at
CDR-SB
Previous MRI"
ETHICS STATEMENT,0.9942748091603053,"t = 1
Questionable
Low Vh
[+0.8, +0.3, +0.3, +0.2, +0.1, +0.7, −0.5, −0.7]
MRI
t = 2
Questionable
Low Vh
[+0.9, −0.1, −0.2, +0.6, −0.2, +0.1, −0.5, −0.8]
MRI
t = 3
Questionable
Low Vh
[+0.9, +0.6, −0.4, +0.9, −0.1, +0.6, −0.7, −0.7]
MRI
t = 4
Severe
Low Vh
[+0.8, +0.2, −0.9, +0.9, −0.5, −0.6, −0.8, −0.8]

(b) Patient B (Degrading)"
ETHICS STATEMENT,0.9961832061068703,"Timestep
Observation zt
History ht
Action at
CDR-SB
Previous MRI"
ETHICS STATEMENT,0.9980916030534351,"t = 1
Severe
Low Vh
[−0.5, +0.9, +0.2, +0.5, +0.3, +0.5, −0.9, −0.9]

t = 2
Severe

[−0.7, −0.5, +0.8, −0.4, +0.3, −0.1, −0.5, +0.7]

t = 3
Severe

[−0.8, +0.7, +0.9, −0.8, +0.6, −0.1, −0.9, +0.9]

t = 4
Severe

[−0.8, +0.4, +1.0, −0.9, +0.7, −0.4, −0.9, +0.9]

(c) Patient C (Dementia)"
