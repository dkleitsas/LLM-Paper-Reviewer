Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018867924528301887,"Fairness in machine learning is of growing concern as more instances of biased
1"
ABSTRACT,0.0037735849056603774,"model behavior are documented while their adoption continues to rise. The majority
2"
ABSTRACT,0.005660377358490566,"of studies have focused on binary classiﬁcation settings, despite the fact that many
3"
ABSTRACT,0.007547169811320755,"real-world problems are inherently multi-class. This paper considers fairness
4"
ABSTRACT,0.009433962264150943,"in multi-class classiﬁcation under the notion of parity of true positive rates—an
5"
ABSTRACT,0.011320754716981131,"extension of binary class equalized odds [23]—which ensures equal opportunity
6"
ABSTRACT,0.013207547169811321,"to qualiﬁed individuals regardless of their demographics. We focus on algorithm
7"
ABSTRACT,0.01509433962264151,"design and provide a post-processing method that derives fair classiﬁers from pre-
8"
ABSTRACT,0.016981132075471698,"trained score functions. The method is developed by analyzing the representation
9"
ABSTRACT,0.018867924528301886,"of the optimal fair classiﬁer, and is efﬁcient in both sample and time complexity,
10"
ABSTRACT,0.020754716981132074,"as it is implemented by linear programs on ﬁnite samples. We demonstrate its
11"
ABSTRACT,0.022641509433962263,"effectiveness at reducing disparity on benchmark datasets, particularly under large
12"
ABSTRACT,0.024528301886792454,"numbers of classes, where existing methods fall short.
13"
INTRODUCTION,0.026415094339622643,"1
Introduction
14"
INTRODUCTION,0.02830188679245283,"Algorithmic fairness has emerged as a topic of signiﬁcant concern in the ﬁeld of machine learning,
15"
INTRODUCTION,0.03018867924528302,"due to the potential for models to exhibit discriminatory behavior towards historically disadvantaged
16"
INTRODUCTION,0.03207547169811321,"demographics [9, 4, 6], all while their adoption continues to rise in domains including high-stakes
17"
INTRODUCTION,0.033962264150943396,"areas such as criminal justice, healthcare, and ﬁnance [3, 7]. To address the concern, a variety of
18"
INTRODUCTION,0.035849056603773584,"fairness criteria have been proposed (e.g., demographic parity, equalized odds) along with mitigation
19"
INTRODUCTION,0.03773584905660377,"methods [10, 19, 23, 26]. On classiﬁcation problems, the majority of work focuses on the binary class
20"
INTRODUCTION,0.03962264150943396,"setting [2, Table 1], where one class is typically considered to be more favorable (e.g., the approval
21"
INTRODUCTION,0.04150943396226415,"vs. rejection of a credit card application).
22"
INTRODUCTION,0.04339622641509434,"Yet, many real-world problems are multi-class in nature. In the case of credit card applications,
23"
INTRODUCTION,0.045283018867924525,"issuers may opt to assigning higher-tier interest rates to high-risk applicants rather than outright
24"
INTRODUCTION,0.04716981132075472,"rejecting them, which creates opportunities to applicants who would otherwise be denied credit
25"
INTRODUCTION,0.04905660377358491,"and also generates returns for the banks. Similarly, in online advertising, recruiting platforms can
26"
INTRODUCTION,0.0509433962264151,"employ machine learning models to match users to relevant job postings across multiple occupation
27"
INTRODUCTION,0.052830188679245285,"categories. There are evidences, however, for such systems to exhibit gender bias [8, 13, 44]; for
28"
INTRODUCTION,0.05471698113207547,"instance, models that are trained to identify occupation from biography tend to show higher accuracy
29"
INTRODUCTION,0.05660377358490566,"(recall) on male biographies than on their female counterparts in occupations that are historically
30"
INTRODUCTION,0.05849056603773585,"male-dominated [14].
31"
INTRODUCTION,0.06037735849056604,"In the example above, unfairness is manifested in a disparity of true positive rates (TPRs) across
32"
INTRODUCTION,0.062264150943396226,"demographic groups A (generalizing the true positive and negative rates in binary classiﬁcation),
33"
INTRODUCTION,0.06415094339622641,"TPRa(bY )y := P(bY = y | Y = y, A = a),
∀y ∈[k], a ∈[m].
A classiﬁer satisfying parity of TPRs, i.e., TPRa = TPRa′ for all a, a′, ensures that individuals with
34"
INTRODUCTION,0.0660377358490566,"the same qualiﬁcation (Y ) will have equal opportunity of receiving their favorable outcome (bY = Y )
35"
INTRODUCTION,0.06792452830188679,"Boundary of feasible 
region (group 1)"
INTRODUCTION,0.06981132075471698,"Boundary of feasible 
region (group 2)"
INTRODUCTION,0.07169811320754717,"TPR parity feasible 
region"
INTRODUCTION,0.07358490566037736,TPR parity optimum
INTRODUCTION,0.07547169811320754,TPR curves (2 classes)
INTRODUCTION,0.07735849056603773,TPR of class 1
INTRODUCTION,0.07924528301886792,TPR of class 2
INTRODUCTION,0.08113207547169811,TPR of class 3 0 1
INTRODUCTION,0.0830188679245283,"TPR surfaces (upper half; 3 classes) 1
0 0 1"
INTRODUCTION,0.08490566037735849,TPR of class 2 0 1
INTRODUCTION,0.08679245283018867,"TPR of class 1
1
0"
INTRODUCTION,0.08867924528301886,"Figure 1: Feasible region of TPRs on a binary class (left) and a three-class problem (right). The
black (resp. colored) arrow indicates the utility-maximizing direction (of each group)."
INTRODUCTION,0.09056603773584905,"regardless of demographics [20], e.g., being shown job postings on recruiting platforms for which the
36"
INTRODUCTION,0.09245283018867924,"user is qualiﬁed. When the classes are binary, this fairness notion recovers equalized odds [23].
37"
INTRODUCTION,0.09433962264150944,"In this paper, we focus on the design of algorithm for mitigating TPR disparity and provide an efﬁcient
38"
INTRODUCTION,0.09622641509433963,"post-processing method that derives attribute-aware fair classiﬁers from (pre-trained) scoring models.
39"
INTRODUCTION,0.09811320754716982,"Our method works on multi-class and multi-group classiﬁcation problems, guarantees fairness by a
40"
INTRODUCTION,0.1,"sample complexity bound, can be implemented by linear programs, and achieves higher reductions in
41"
INTRODUCTION,0.1018867924528302,"disparity compared to existing algorithms that are applicable to multi-class—a recently proposed post-
42"
INTRODUCTION,0.10377358490566038,"processing method based on model projection [2], and adversarial debiasing [41], an in-processing
43"
INTRODUCTION,0.10566037735849057,"method—especially when the number of classes is large.
44"
INTRODUCTION,0.10754716981132076,"Organization.
We introduce the problem setup and objectives in Section 2, then describe our
45"
INTRODUCTION,0.10943396226415095,"post-processing method for TPR parity in Section 3, along with suboptimality analyses; in particular,
46"
INTRODUCTION,0.11132075471698114,"our method yields the optimal fair classiﬁer when applied to the Bayes optimal score function.
47"
INTRODUCTION,0.11320754716981132,"Our method is instantiated for ﬁnite sample estimation in Section 4, and we also provide sample
48"
INTRODUCTION,0.11509433962264151,"complexity bounds to complete the analysis. Finally, in Section 5, we compare our algorithm with
49"
INTRODUCTION,0.1169811320754717,"existing methods for disparity reduction on benchmark datasets.1 A high-level summary of our results
50"
INTRODUCTION,0.11886792452830189,"is provided in Section 1.1.
51"
SUMMARY OF RESULTS,0.12075471698113208,"1.1
Summary of Results
52"
SUMMARY OF RESULTS,0.12264150943396226,"One way to interpret and understand TPR parity is through visualizing the feasible regions of TPRs.
53"
SUMMARY OF RESULTS,0.12452830188679245,"In Fig. 1, we plot the feasible regions (achievable by probabilistic classiﬁers) of two groups on a
54"
SUMMARY OF RESULTS,0.12641509433962264,"(hypothetical) binary classiﬁcation problem on the left, and those on a three-class problem on the
55"
SUMMARY OF RESULTS,0.12830188679245283,"right, where each axis represents the TPR of a class. Achieving optimal TPR parity amounts to
56"
SUMMARY OF RESULTS,0.13018867924528302,"ﬁrst ﬁnding the TPR that maximizes the overall utility (e.g., accuracy) in the intersection of feasible
57"
SUMMARY OF RESULTS,0.1320754716981132,"regions, and subsequently an (attribute-aware) classiﬁer attaining that target TPR on all groups. Note
58"
SUMMARY OF RESULTS,0.1339622641509434,"that the left ﬁgure is equivalent to the ROC curve (with a ﬂip of the horizontal axis, because the TPR
59"
SUMMARY OF RESULTS,0.13584905660377358,"of class 1 equals one minus the false negative rate by treating class-1 as the negative class), which
60"
SUMMARY OF RESULTS,0.13773584905660377,"was used by Hardt et al. [23] for studying equalized odds. And thus, the TPR (hyper)surface plots in
61"
SUMMARY OF RESULTS,0.13962264150943396,"higher dimensions are a natural generalization of the ROC curve to multi-class settings.
62"
SUMMARY OF RESULTS,0.14150943396226415,"Step one of ﬁnding the optimal fair TPR can be formulated as a linear program when estimating from
63"
SUMMARY OF RESULTS,0.14339622641509434,"ﬁnite samples. For the second step, our method derives a classiﬁer attaining the target TPR from the
64"
SUMMARY OF RESULTS,0.14528301886792452,"score function; in particular, it yields the optimal fair classiﬁer when the score is Bayes optimal:
65"
SUMMARY OF RESULTS,0.1471698113207547,"Theorem 1.1. Let f ∗
1 , · · · , f ∗
m : X →∆k denote the Bayes score function on each group, f ∗
a(x) :=
E[Y | X = x, A = a], and q1, · · · , qm ∈∆k be arbitrary. Then under a continuity assumption (2.3),
∃β1, · · · , βm ∈[0, 1] and λ1, · · · , λm ∈Rk s.t. the probabilistic attribute-aware classiﬁer"
SUMMARY OF RESULTS,0.1490566037735849,"(x, a) 7→
 arg maxy′(λa)y′ · f ∗
a(x)y′
w.p. 1 −βa
(1a)"
SUMMARY OF RESULTS,0.1509433962264151,"y
w.p. βa · (qa)y, ∀y ∈[k]
(1b)"
SUMMARY OF RESULTS,0.15283018867924528,"achieves the maximum utility subject to TPR parity.
66"
SUMMARY OF RESULTS,0.15471698113207547,1Our code is provided in the supplemental material.
SUMMARY OF RESULTS,0.15660377358490565,"The post-processed classiﬁer returned by our method is a mixture of two models (weighted by
67"
SUMMARY OF RESULTS,0.15849056603773584,"β). Eq. (1a) returns the class with the highest likelihood after a class-wise rescaling, called a
68"
SUMMARY OF RESULTS,0.16037735849056603,"tilting [2], which generalizes the concept of thresholding in binary classiﬁers. Eq. (1b) makes random
69"
SUMMARY OF RESULTS,0.16226415094339622,"assignments sampled from a Multinoulli(q) distribution, which handles situations where the fair TPR
70"
SUMMARY OF RESULTS,0.1641509433962264,"lies in the interior of the feasible region (see Fig. 1, where the optimum is located within the interior
71"
SUMMARY OF RESULTS,0.1660377358490566,"of group 2 feasible region). To alleviate potential ethical concerns regarding this randomization, we
72"
SUMMARY OF RESULTS,0.16792452830188678,"point out that the parameter qa’s used in class sampling can be speciﬁed per-group by the practitioner
73"
SUMMARY OF RESULTS,0.16981132075471697,"responsibly, e.g., uniform 1/k, or ey′ with y′ being an advantaged outcome.
74"
SUMMARY OF RESULTS,0.17169811320754716,"Among the possibly inﬁnitely many fair classiﬁers derived from the score function f, our method
75"
SUMMARY OF RESULTS,0.17358490566037735,"speciﬁcally seeks the simplistic representation in Eq. (1) because it can be estimated via linear
76"
SUMMARY OF RESULTS,0.17547169811320754,"programs from ﬁnite samples. More importantly, it immediately extrapolates to unseen examples,
77"
SUMMARY OF RESULTS,0.17735849056603772,"and provides good generalization performance at the rate of eO(
p"
SUMMARY OF RESULTS,0.1792452830188679,"k/n) thanks to its low function
78"
SUMMARY OF RESULTS,0.1811320754716981,"complexity (Theorem 4.2).
79"
SUMMARY OF RESULTS,0.1830188679245283,"When the score function being post-processed is not Bayes optimal, our method is still applicable, but
80"
SUMMARY OF RESULTS,0.18490566037735848,"the resulting classiﬁer may not be optimal nor exactly achieve TPR parity without access to labeled
81"
SUMMARY OF RESULTS,0.18679245283018867,"data (the method itself only needs unlabeled data with the sensitive attribute) or additional knowledge
82"
SUMMARY OF RESULTS,0.18867924528301888,"of the model. But these suboptimalities are minimized if the model is calibrated (Theorem 3.5);
83"
SUMMARY OF RESULTS,0.19056603773584907,"this answers the question raised in [2] about the effects of base model inaccuracies on downstream
84"
SUMMARY OF RESULTS,0.19245283018867926,"post-processing.
85"
RELATED WORK,0.19433962264150945,"1.2
Related Work
86"
RELATED WORK,0.19622641509433963,"Fairness Critetia.
The notion of TPR parity has appeared in the literature as conditional procedure
87"
RELATED WORK,0.19811320754716982,"accuracy equality [7], avoiding disparate mistreatment [39], and (multi-class) equal opportunity [14,
88"
RELATED WORK,0.2,"29, 31] (to be distinguished from the fairness criterion with the same name in [23]). Other group
89"
RELATED WORK,0.2018867924528302,"fairness notions that extend to multi-class include (but not limited to) equalized odds [23] (of which
90"
RELATED WORK,0.2037735849056604,"TPR parity is a necessary condition), and demographic parity (DP) [10] (where Xian et al. [35]
91"
RELATED WORK,0.20566037735849058,"recently proposed an optimal post-processing method). However, DP may be less desirable than
92"
RELATED WORK,0.20754716981132076,"TPR parity in some use cases because the perfect classiﬁer is not permitted under DP when the base
93"
RELATED WORK,0.20943396226415095,"rates differ [42]. It is worth noting that TPR parity implies accuracy parity [9]. In addition to group
94"
RELATED WORK,0.21132075471698114,"fairness, there are notions deﬁned on the individual level [19].
95"
RELATED WORK,0.21320754716981133,"Mitigation Methods.
Our method is based on post-processing [25, 23]. There are also in-processing
96"
RELATED WORK,0.21509433962264152,"methods via fair representation learning [40, 41, 43, 30] or solving zero-sum games [1, 36], and
97"
RELATED WORK,0.2169811320754717,"pre-processing methods that debias the data prior to model training [11, 44]; see [4, 12] for a survey.
98"
RELATED WORK,0.2188679245283019,"For multi-class TPR parity, the only applicable post-processing method to date, to our knowledge,
99"
RELATED WORK,0.22075471698113208,"is due to Alghamdi et al. [2] (which is the primary baseline for our method in our experiments).
100"
RELATED WORK,0.22264150943396227,"It is a general-purpose method that transforms the scores to satisfy fairness while minimizing the
101"
RELATED WORK,0.22452830188679246,"distributional divergence (e.g., KL) between the transformed scores and the original. However, the
102"
RELATED WORK,0.22641509433962265,"tradeoff between model performance and fairness is unclear as they did not relate the divergence to
103"
RELATED WORK,0.22830188679245284,"utility. Furthermore, while the authors provided a sample complexity bound for their optimization
104"
RELATED WORK,0.23018867924528302,"objective, it is not explicitly related to the violation of the fairness criteria.
105"
PRELIMINARIES,0.2320754716981132,"2
Preliminaries
106"
PRELIMINARIES,0.2339622641509434,"A k-class classiﬁcation problem is deﬁned by a joint distribution µ of input X ∈X, demographic
107"
PRELIMINARIES,0.2358490566037736,"group membership A ∈[m] := {1, · · · , m} (a.k.a. the sensitive attribute), and class label Y ∈[k].
108"
PRELIMINARIES,0.23773584905660378,"We denote the joint distribution of (X, A) by µX,A, and, the (k −1)-dimensional probability simplex
109"
PRELIMINARIES,0.23962264150943396,"by ∆k := {z ∈Rk
≥0 : ∥z∥1 = 1}.
110"
PRELIMINARIES,0.24150943396226415,"Let f : X ×A →∆k be an attribute-aware (pre-trained) score function, whose outputs are probability
111"
PRELIMINARIES,0.24339622641509434,"vectors that estimate the class probabilities as in f(x, a)y ≈Pµ(Y = y | X = x, A = a). We will
112"
PRELIMINARIES,0.24528301886792453,"write fa : X →∆k to denote the component of f associated with group a, i.e., fa(x) ≡f(x, a). Our
113"
PRELIMINARIES,0.24716981132075472,"goal is to ﬁnd fair (probabilistic) post-processing maps g1, · · · , gm : ∆k →Y to derive a classiﬁer
114"
PRELIMINARIES,0.2490566037735849,"(x, a) 7→ga ◦fa(x) that satisﬁes TPR parity while maximizing utility (e.g., classiﬁcation accuracy).
115"
PRELIMINARIES,0.2509433962264151,"We allow for controllable tradeoffs between utility and fairness through the following relaxation of
116"
PRELIMINARIES,0.2528301886792453,"TPR parity, and call a classiﬁer α-fair if it satisﬁes α-TPR parity:
117"
PRELIMINARIES,0.25471698113207547,"Deﬁnition 2.1 (Approximate TPR Parity). Let α ∈[0, 1]. A predictor bY is said to satisfy α-TPR
118"
PRELIMINARIES,0.25660377358490566,"parity if ∆TPR(bY ) ≤α, where
119"
PRELIMINARIES,0.25849056603773585,"∆TPR(bY ) := max
a,a′∈A"
PRELIMINARIES,0.26037735849056604,"TPRa(bY ) −TPRa′(bY )

∞,
(2)"
PRELIMINARIES,0.2622641509433962,"and TPRa(bY ) := P(bY | Y = y, A = a) ∈[0, 1]k; P includes the randomness of the predictor.
120"
PRELIMINARIES,0.2641509433962264,"Beyond classiﬁcation accuracy, we also allow for any utility functions that depend only on the TPRs:2
121"
PRELIMINARIES,0.2660377358490566,"Deﬁnition 2.2 (Utility). The utility function u : [k] × [k] →R is deﬁned for some υ ∈Rk by
122"
PRELIMINARIES,0.2679245283018868,"u(ˆy, y) :=
X"
PRELIMINARIES,0.269811320754717,"y′∈[k]
υy′ 1[y = y′, ˆy = y′]."
PRELIMINARIES,0.27169811320754716,"E.g., accuracy, 1[y = ˆy], is obtained by setting υ = 1k. The term υ will appear in our analyses,
123"
PRELIMINARIES,0.27358490566037735,"and the signiﬁcance of considering utilities of this form is that we could evaluate a classiﬁer by a
124"
PRELIMINARIES,0.27547169811320754,"weighted sum of its TPRs. Deﬁne pay := Pµ(A = a, Y = y), then
125"
PRELIMINARIES,0.27735849056603773,"U(bY ) = E u(bY , Y ) =
X"
PRELIMINARIES,0.2792452830188679,"a∈[m],y∈[k]
υypay TPRa(bY )y ≡U(TPR1(bY ), · · · , TPRm(bY )).
(3)"
PRELIMINARIES,0.2811320754716981,"Finally, we make the following continuity assumption on the distributions of score to avoid technical
126"
PRELIMINARIES,0.2830188679245283,"complexities related to tie-breaking (on the atoms). This assumption has also appeared in prior work
127"
PRELIMINARIES,0.2849056603773585,"on fair post-processing [16, 21, 35]; it holds when the input distributions are continuous and the score
128"
PRELIMINARIES,0.28679245283018867,"function is injective, or can be satisﬁed by adding small random perturbations to the scores.
129"
PRELIMINARIES,0.28867924528301886,"Assumption 2.3. The conditional distribution of score, P(fa(X) | A = a), is (Lebesgue absolutely)
130"
PRELIMINARIES,0.29056603773584905,"continuous, ∀a ∈[m].
131"
TPR PARITY VIA POST-PROCESSING,0.29245283018867924,"3
TPR Parity via Post-Processing
132"
TPR PARITY VIA POST-PROCESSING,0.2943396226415094,"Given a score function f : X × A →∆k, and access to the (unlabeled) joint distribution µX,A (i.e.,
133"
TPR PARITY VIA POST-PROCESSING,0.2962264150943396,"no estimation error), we describe a method for deriving an attribute-aware α-fair classiﬁer while
134"
TPR PARITY VIA POST-PROCESSING,0.2981132075471698,"maximizing utility, in the form of (x, a) 7→ga ◦fa(x), where the ga’s are (probabilistic) fair
135"
TPR PARITY VIA POST-PROCESSING,0.3,"post-processing maps for each group. That is, we want to solve
136"
TPR PARITY VIA POST-PROCESSING,0.3018867924528302,"max
g1,··· ,gm U(bY )
s.t.
∆TPR(bY ) ≤α
where
bY = gA ◦fA(X)."
TPR PARITY VIA POST-PROCESSING,0.30377358490566037,"Although the method only returns classiﬁers derived from f as opposed to searching over the space of
137"
TPR PARITY VIA POST-PROCESSING,0.30566037735849055,"all classiﬁers h : X × A →Y, it would yield the optimal fair classiﬁer provided that the information
138"
TPR PARITY VIA POST-PROCESSING,0.30754716981132074,"of (A, Y ) is preserved in the output of f; this is the case when the score function is Bayes optimal.
139"
TPR PARITY VIA POST-PROCESSING,0.30943396226415093,"3.1
Deriving Optimal Fair Classiﬁer From Bayes Score Function
140"
TPR PARITY VIA POST-PROCESSING,0.3113207547169811,"In this section, we explain how to obtain an optimal fair classiﬁer by deriving from the Bayes score
141"
TPR PARITY VIA POST-PROCESSING,0.3132075471698113,"function f ∗, thereby providing a proof of Theorem 1.1 (omitted proof are deferred to the appendix).
142"
TPR PARITY VIA POST-PROCESSING,0.3150943396226415,"Step 1 (Finding Utility-Maximizing Fair TPRs).
Let Da ⊆[0, 1]k denote the set of feasible TPRs
143"
TPR PARITY VIA POST-PROCESSING,0.3169811320754717,"on group a achieved by probabilistic classiﬁers. The ﬁrst step is to ﬁnd utility-maximizing fair TPRs
144"
TPR PARITY VIA POST-PROCESSING,0.31886792452830187,"contained in an ℓ∞-ball of diameter α per Deﬁnition 2.1 of α-TPR parity (left ﬁgure of Fig. 2):
145"
TPR PARITY VIA POST-PROCESSING,0.32075471698113206,"max
t1∈D1,··· ,tm∈Dm U(t1, · · · , tm)
s.t.
∥ta −ta′∥∞≤α, ∀a, a′ ∈[m].
(4)"
TPR PARITY VIA POST-PROCESSING,0.32264150943396225,"When α = 0, this reduces to ﬁnding a single t ∈T
a Da, and because each Da is convex (since
146"
TPR PARITY VIA POST-PROCESSING,0.32452830188679244,"probabilistic classiﬁers are allowed), it can be found with ternary search as suggested in [23]. If
147"
TPR PARITY VIA POST-PROCESSING,0.3264150943396226,"instead the ta’s are to be estimated from ﬁnite samples, then the empirical bDa’s are described by
148"
TPR PARITY VIA POST-PROCESSING,0.3283018867924528,"polytopes and the problem can be formulated as a linear program (Section 4).
149"
TPR PARITY VIA POST-PROCESSING,0.330188679245283,"2This includes all possible utility/loss functions in binary classiﬁcation, since TPR(bY )1 (true negative rate)
and TPR(bY )2 (true positive rate) fully determine the 2 × 2 confusion matrix."
TPR PARITY VIA POST-PROCESSING,0.3320754716981132,"TPR of class 1
1
0"
TPR PARITY VIA POST-PROCESSING,0.3339622641509434,TPR of class 2 0 1
TPR PARITY VIA POST-PROCESSING,0.33584905660377357,"β
1 – β"
TPR PARITY VIA POST-PROCESSING,0.33773584905660375,"Step 1
Finding utility-maximizing fair TPRs"
TPR PARITY VIA POST-PROCESSING,0.33962264150943394,"Step 2
Obtaining fair classiﬁer of desired form q ∆2 α"
TPR PARITY VIA POST-PROCESSING,0.34150943396226413,TPR of class 2 0 1
TPR PARITY VIA POST-PROCESSING,0.3433962264150943,TPR of class 2 0 1
TPR PARITY VIA POST-PROCESSING,0.3452830188679245,"TPR of class 1
1
0
TPR of class 1
1
0"
TPR PARITY VIA POST-PROCESSING,0.3471698113207547,"Figure 2: Achieving α-TPR parity on a binary class problem. First, the utility-maximizing TPRs
residing in an ℓ∞-ball of diameter α are found (left). Then, classiﬁers achieving the fair TPRs are
obtained: a tilting of the scores when the TPR lies on the boundary (middle), otherwise, a mixture of
tilting and randomization (right). The simplex ∆k is always inscribed in the feasible region."
TPR PARITY VIA POST-PROCESSING,0.3490566037735849,"The feasible regions of TPR generally differ across groups, due to uncertainties that are inherent to
150"
TPR PARITY VIA POST-PROCESSING,0.35094339622641507,"each group in the task of interest, or to inadequate and biased collection or sourcing of data. The more
151"
TPR PARITY VIA POST-PROCESSING,0.35283018867924526,"the Da’s differ, the greater the tradeoff between fairness and utility; hence TPR parity incentivizes
152"
TPR PARITY VIA POST-PROCESSING,0.35471698113207545,"the practitioner to improve data collection and aspects of modeling that induces a balanced predictive
153"
TPR PARITY VIA POST-PROCESSING,0.35660377358490564,"capability on all groups [23].
154"
TPR PARITY VIA POST-PROCESSING,0.3584905660377358,"Because f ∗(X, A) is sufﬁcient statistic for Y , the fair TPRs we found above are always achievable
155"
TPR PARITY VIA POST-PROCESSING,0.360377358490566,"by classiﬁers derived from f ∗. Or more concretely,
156"
TPR PARITY VIA POST-PROCESSING,0.3622641509433962,"Proposition 3.1. Let f ∗: X →∆k denote the Bayes score function, then D := {TPR(h) ∈[0, 1]k |
157"
TPR PARITY VIA POST-PROCESSING,0.3641509433962264,"h : X →Y (probabilistic)} = {TPR(g ◦f ∗) ∈[0, 1]k | g : ∆k →Y (probabilistic)}.
158"
TPR PARITY VIA POST-PROCESSING,0.3660377358490566,"Step 2 (Obtaining Fair Classiﬁer of Desired Form).
Having found the utility-maximizing fair TPR
159"
TPR PARITY VIA POST-PROCESSING,0.36792452830188677,"ta’s, the next step is to derive a classiﬁer that attains ta on each group. This is provided by the
160"
TPR PARITY VIA POST-PROCESSING,0.36981132075471695,"following theorem:
161"
TPR PARITY VIA POST-PROCESSING,0.37169811320754714,"Theorem 3.2. Let f ∗: X →∆k denote the Bayes score function, and q ∈∆k be arbitrary. Then
162"
TPR PARITY VIA POST-PROCESSING,0.37358490566037733,"under Assumption 2.3, ∀t ∈D, there exists β ∈[0, 1] and λ ∈Rk s.t. TPR(h) = t, where
163"
TPR PARITY VIA POST-PROCESSING,0.3754716981132076,"h(x) =
 arg maxy′ λy′f ∗(x)y′
w.p. 1 −β
y
w.p. βqy, ∀y ∈[k]."
TPR PARITY VIA POST-PROCESSING,0.37735849056603776,"The construction uses the observation that the boundary of D, denoted by ∂D, is given by the set of
164"
TPR PARITY VIA POST-PROCESSING,0.37924528301886795,"TPRs attained by tiltings of the Bayes score:
165"
TPR PARITY VIA POST-PROCESSING,0.38113207547169814,"Proposition 3.3. Let f ∗: X →∆k denote the Bayes score function, then h : X →Y (probabilistic)
166"
TPR PARITY VIA POST-PROCESSING,0.38301886792452833,"satisﬁes TPR(h) ∈∂D if and only if ∃λ ∈Rk, λ ̸= 0 s.t. h(x) ∈arg maxy λyf ∗(x)y.
167"
TPR PARITY VIA POST-PROCESSING,0.3849056603773585,"Proof of Theorem 3.2. If the target TPR lies on the boundary of D, then by Proposition 3.3, it is
168"
TPR PARITY VIA POST-PROCESSING,0.3867924528301887,"achieved by a tilting of the Bayes score without any randomization (i.e., β = 0; center ﬁgure of
169"
TPR PARITY VIA POST-PROCESSING,0.3886792452830189,"Fig. 2). This holds due to Assumption 2.3, because we may break ties arbitrarily without affecting
170"
TPR PARITY VIA POST-PROCESSING,0.3905660377358491,"TPR, since the set of tied scores (ﬁnite union of (k −2)-d subspaces) has (Lebesgue) measure zero.
171"
TPR PARITY VIA POST-PROCESSING,0.39245283018867927,"Otherwise, and generally, there must exists t′ ∈∂D and β ∈[0, 1] s.t. t can be written as a linear
172"
TPR PARITY VIA POST-PROCESSING,0.39433962264150946,"combination of t = βq + (1 −β)t′. This is simply because q ∈∆k ⊆D, and the line connecting q
173"
TPR PARITY VIA POST-PROCESSING,0.39622641509433965,"and t must intersect ∂D at some point t′ (right ﬁgure of Fig. 2). Since the TPR of the input-agnostic
174"
TPR PARITY VIA POST-PROCESSING,0.39811320754716983,"randomization according to Multinoulli(q) equals q, and t′ is achieved by a tilting of the score per
175"
TPR PARITY VIA POST-PROCESSING,0.4,"Proposition 3.3, their β-mixture achieves the target TPR t by linearity.
176"
DERIVING FROM ANY SCORE FUNCTION,0.4018867924528302,"3.2
Deriving From Any Score Function
177"
DERIVING FROM ANY SCORE FUNCTION,0.4037735849056604,"The post-processing method described in the previous section, which only requires unlabeled data
178"
DERIVING FROM ANY SCORE FUNCTION,0.4056603773584906,"(X, A), yields the optimal α-fair classiﬁer when applied to Bayes scores f ∗. Yet, in practice, there
179"
DERIVING FROM ANY SCORE FUNCTION,0.4075471698113208,Algorithm 1 Post-Process Score Function for α-TPR parity
DERIVING FROM ANY SCORE FUNCTION,0.40943396226415096,"1: Input: α ∈[0, 1], q1, · · · , qm ∈∆k, score function f : X × A →∆k, distribution µX,A"
DERIVING FROM ANY SCORE FUNCTION,0.41132075471698115,"2:
eDa := {]
TPRa(h) | h : X →Y (probabilistic)}
▷Eq. (5), induced TPR feasible region
3: ˜t1, · · · , ˜tm ←arg max˜t1∈e
D1,··· ,˜tm∈e
Dm U(˜t1, · · · , ˜tm) s.t. ∥˜ta −˜ta′∥∞≤α, ∀a, a′ ∈[m]"
DERIVING FROM ANY SCORE FUNCTION,0.41320754716981134,"▷utility-maximizing fair TPRs
4: for a = 1 to m do
5:
Find ha, βa ∈[0, 1] s.t. ]
TPRa(ha) ∈∂eDa and ˜ta = (1 −βa)]
TPRa(ha) + βaqa
6:
Find λa ∈Rk s.t. ha(x) ∈arg maxy′(λa)y′ · fa(x)y′, ∀x ∈supp(µX
a )
7: end for
8: Return: (x, a) 7→arg maxy′(λa)y′ · fa(x) w.p. 1 −βa, and y w.p. βa · (qa)y for each y ∈[k]"
DERIVING FROM ANY SCORE FUNCTION,0.41509433962264153,"is the concern that Bayes score functions could be arbitrarily complex and are often not exactly
180"
DERIVING FROM ANY SCORE FUNCTION,0.4169811320754717,"learnable due to limited data or computational constraints [34].
181"
DERIVING FROM ANY SCORE FUNCTION,0.4188679245283019,"Nonetheless, our method is still applicable to arbitrary (approximations to the Bayes) score functions
182"
DERIVING FROM ANY SCORE FUNCTION,0.4207547169811321,"f : X × A →∆k for deriving classiﬁers that are approximately fair and optimal, by treating them
183"
DERIVING FROM ANY SCORE FUNCTION,0.4226415094339623,"as if they were Bayes optimal (Algorithm 1). Where, the only tweak we made is replacing the
184"
DERIVING FROM ANY SCORE FUNCTION,0.42452830188679247,"ground-truth TPRs and feasible regions (which are unknown without access to the Bayes score) by
185"
DERIVING FROM ANY SCORE FUNCTION,0.42641509433962266,"approximations induced by f, i.e.,
186"
DERIVING FROM ANY SCORE FUNCTION,0.42830188679245285,"eDa :=
n
]
TPRa(h) ∈[0, 1]k  h : X →Y (probabilistic)
o
,
(5)"
DERIVING FROM ANY SCORE FUNCTION,0.43018867924528303,"where
187"
DERIVING FROM ANY SCORE FUNCTION,0.4320754716981132,"]
TPRa(h)y :=
1
˜pay Z"
DERIVING FROM ANY SCORE FUNCTION,0.4339622641509434,"x∈X
fa(x)y P(h(x) = y) dµX,A(x, a),
˜pay :=
Z"
DERIVING FROM ANY SCORE FUNCTION,0.4358490566037736,"x∈X
fa(x)y dµX,A(x, a)."
DERIVING FROM ANY SCORE FUNCTION,0.4377358490566038,"(6)
It is not hard to show that they are equal to their ground-truth counterparts when f = f ∗.
188"
DERIVING FROM ANY SCORE FUNCTION,0.439622641509434,"We may control and minimize the suboptimalities of the classiﬁer returned from Algorithm 1 by
189"
DERIVING FROM ANY SCORE FUNCTION,0.44150943396226416,"performing group-wise distribution calibration to the score function f (using labeled data (X, A, Y )):
190"
DERIVING FROM ANY SCORE FUNCTION,0.44339622641509435,"Deﬁnition 3.4 (Distribution Calibration). A score R is said to be (group-wise) distribution calibrated
191"
DERIVING FROM ANY SCORE FUNCTION,0.44528301886792454,"if P(Y = y | R = s) = sy, ∀s ∈∆k, y ∈[k] (resp. P(Y = y | R = s, A = a) = sy, ∀a ∈[m]).
192"
DERIVING FROM ANY SCORE FUNCTION,0.44716981132075473,"Distribution calibration is a multi-class generalization of the original deﬁnition of calibration for
193"
DERIVING FROM ANY SCORE FUNCTION,0.4490566037735849,"binary predictors [15, 32], requiring the predicted score to match the underlying class distribution
194"
DERIVING FROM ANY SCORE FUNCTION,0.4509433962264151,"conditioned on the score across all classes, not just the most conﬁdent one [22]. Although this
195"
DERIVING FROM ANY SCORE FUNCTION,0.4528301886792453,"deﬁnition is convenient to work with mathematically, it could be difﬁcult to achieve in practice. In the
196"
DERIVING FROM ANY SCORE FUNCTION,0.4547169811320755,"proof of Theorem 3.5, we relax it to a recently proposed notion of decision calibration [45] (w.r.t. the
197"
DERIVING FROM ANY SCORE FUNCTION,0.45660377358490567,"set of all tiltings; derived from multicalibration [24]), which could be achieved in polynomial time.
198"
DERIVING FROM ANY SCORE FUNCTION,0.45849056603773586,"Theorem 3.5. Let f : X × A →∆k be a score function, and h : X × A →Y the (probabilistic)
199"
DERIVING FROM ANY SCORE FUNCTION,0.46037735849056605,"classiﬁer derived from f using Algorithm 1. Then under Assumption 2.3, for any group-wise calibrated
200"
DERIVING FROM ANY SCORE FUNCTION,0.46226415094339623,"reference score function ¯f : X × A →∆k,
201"
DERIVING FROM ANY SCORE FUNCTION,0.4641509433962264,"U −U(h)
 ≤
X"
DERIVING FROM ANY SCORE FUNCTION,0.4660377358490566,"a∈[m],y∈[k]
3υyϵay,
∆TPR(h) ≤α +
max
a∈[m],y∈[k]
4ϵay pay
,"
DERIVING FROM ANY SCORE FUNCTION,0.4679245283018868,"where pay := Pµ(A = a, Y = y), υ is from the utility function in Deﬁnition 2.2, U denotes the utility
202"
DERIVING FROM ANY SCORE FUNCTION,0.469811320754717,"achieved by the optimal α-fair classiﬁer derived from the calibrated reference ¯f, and
203"
DERIVING FROM ANY SCORE FUNCTION,0.4716981132075472,"ϵay := E
 ¯fa(X)y −fa(X)y
 · 1[A = a]
"
DERIVING FROM ANY SCORE FUNCTION,0.47358490566037736,"is the L1(µ) difference between f and the calibrated reference ¯f on group a and class y.
204"
DERIVING FROM ANY SCORE FUNCTION,0.47547169811320755,"We draw two conclusions from this result. First, by using the Bayes score function f ∗as the reference,
205"
DERIVING FROM ANY SCORE FUNCTION,0.47735849056603774,"it states that the suboptimality of the derived classiﬁer when f ̸= f ∗is upper bounded by the
206"
DERIVING FROM ANY SCORE FUNCTION,0.47924528301886793,"difference between the approximate scores and the ground-truth; this answers the question raised
207"
DERIVING FROM ANY SCORE FUNCTION,0.4811320754716981,"in [2] regarding the impact of base model inaccuracies. Second, if f satisﬁes calibration, then by
208"
DERIVING FROM ANY SCORE FUNCTION,0.4830188679245283,"using itself as the reference, the result guarantees that the classiﬁer derived using Algorithm 1 exactly
209"
DERIVING FROM ANY SCORE FUNCTION,0.4849056603773585,"achieves the desired level of fairness, and is optimal among all fair classiﬁers derived from f (which
210"
DERIVING FROM ANY SCORE FUNCTION,0.4867924528301887,"cannot be improved without labeled data).
211"
FINITE-SAMPLE ALGORITHM AND GUARANTEES,0.48867924528301887,"4
Finite-Sample Algorithm and Guarantees
212"
FINITE-SAMPLE ALGORITHM AND GUARANTEES,0.49056603773584906,"We instantiate the post-processing method above for TPR parity to the case where we do not have
213"
FINITE-SAMPLE ALGORITHM AND GUARANTEES,0.49245283018867925,"access to the distribution µX,A but only samples drawn from it (i.e., to perform estimation), and
214"
FINITE-SAMPLE ALGORITHM AND GUARANTEES,0.49433962264150944,"analyze the sample complexity.
215"
FINITE-SAMPLE ALGORITHM AND GUARANTEES,0.4962264150943396,"Assumption 4.1. We have n i.i.d. (unlabeled) samples of (X, A), which are independent of the score
216"
FINITE-SAMPLE ALGORITHM AND GUARANTEES,0.4981132075471698,"function f being post-processed.
217"
FINITE-SAMPLE ALGORITHM AND GUARANTEES,0.5,"Denote the number of samples from group a by na, and the samples themselves by (xa,i)i∈[na].
218"
ALGORITHM,0.5018867924528302,"4.1
Algorithm
219"
ALGORITHM,0.5037735849056604,"We adapt Algorithm 1 to handle ﬁnite samples by replacing eDa and U with their empirical counterparts
220"
ALGORITHM,0.5056603773584906,"(essentially calling it with the empirical distribution ˆµX,A formed by the samples as the argument),
221"
ALGORITHM,0.5075471698113208,"and implement the optimization problems on Lines 3, 5 and 6 using linear programs.
222"
ALGORITHM,0.5094339622641509,"Step 1 (Finding Utility-Maximizing Fair TPRs).
The empirical induced feasible region of TPRs, bDa,
223"
ALGORITHM,0.5113207547169811,"can be computed via evaluating the TPRs of all (probabilistic) classiﬁers acting on the samples—by
224"
ALGORITHM,0.5132075471698113,"representing them using na × k lookup tables (each row gives the probabilities of the random class
225"
ALGORITHM,0.5150943396226415,"assignment on the corresponding sample):
226"
ALGORITHM,0.5169811320754717,"bDa :=
n
[
TPRa(γa)
 γa ∈Rna×k
≥0
, P"
ALGORITHM,0.5188679245283019,"y∈[k](γa)i,y = 1, ∀i ∈[na]
o
,"
ALGORITHM,0.5207547169811321,"where
227"
ALGORITHM,0.5226415094339623,"[
TPRa(γ)y :=
1
nˆpay X"
ALGORITHM,0.5245283018867924,"i∈[na]
fa(xa,i)y · (γa)i,y,
ˆpay := 1 n X"
ALGORITHM,0.5264150943396226,"i∈[na]
fa(xa,i)y"
ALGORITHM,0.5283018867924528,"(cf. Line 2 and Eqs. (5) and (6)). Note that bDa is a polygon, as it is speciﬁed by linear constraints.
228"
ALGORITHM,0.530188679245283,"To obtain the utility-maximizing fair TPR ˆta’s, we take the empirical maximizer subject to the α-TPR
229"
ALGORITHM,0.5320754716981132,"constraint via solving a linear program (cf. Line 3 and Eqs. (3) and (4)):
230"
ALGORITHM,0.5339622641509434,"LP1(α) :
max
ˆt1∈b
D1,··· ,ˆtm∈b
Dm
bU(ˆt1, · · · , ˆtm)
s.t.
∥ˆta −ˆta′∥∞≤α, ∀a, a′ ∈[m],"
ALGORITHM,0.5358490566037736,"where bU(ˆt1, · · · , ˆtm) := P"
ALGORITHM,0.5377358490566038,"a,y υy ˆpay(ˆta)y is the empirical utility.
231"
ALGORITHM,0.539622641509434,"Step 2 (Obtaining Fair Classiﬁer of Desired Form).
The next step is ﬁnding a classiﬁer that achieves
232"
ALGORITHM,0.5415094339622641,"˜ta’s on the empirical distribution, i.e., Lines 5 and 6. To implement Line 5, note that another way of
233"
ALGORITHM,0.5433962264150943,"approaching this problem is to realize that among all eligible (βa, ha)-pairs, the ha associated with
234"
ALGORITHM,0.5452830188679245,"the maximum βa value must satisfy ]
TPRa(ha) ∈∂eDa (otherwise, a contradiction can be reached
235"
ALGORITHM,0.5471698113207547,"using the fact that eDa ⊆[0, 1]k is compact; also see the right ﬁgure of Fig. 2). Combined with the
236"
ALGORITHM,0.5490566037735849,"strategy above of representing classiﬁers using lookup tables, we get the following linear program:
237"
ALGORITHM,0.5509433962264151,"LP2(t, q) :
max
γ,β β
s.t.
t = (1 −β)[
TPR(γ) + βq
and
γ ∈Rn×k
≥0 ,
X"
ALGORITHM,0.5528301886792453,"y∈[k]
γi,y = 1, ∀i ∈[n]."
ALGORITHM,0.5547169811320755,"Finally, on Line 6, we ﬁnd a tilting λa s.t. after coordinate-wise multiplied by the scores, the argmax
238"
ALGORITHM,0.5566037735849056,"class assignment has nonzero probability according to the classiﬁer γa found in the preceding step:
239"
ALGORITHM,0.5584905660377358,"LP3(γ) :
min
λ 0
s.t.
λyf(xi)y ≥λy′f(xi)y′
∀i ∈[n], y, y′ ∈[k], γi,y > 0."
ALGORITHM,0.560377358490566,"The feasible set of this problem is nonempty by Proposition 3.1, because we are treating f as if it
240"
ALGORITHM,0.5622641509433962,"were the Bayes score function, and the empirical distribution ˆµX,A as the population.
241"
ALGORITHM,0.5641509433962264,"All combined, our algorithm involves solving (2m+1) linear programs, where LP1 is the dominating
242"
ALGORITHM,0.5660377358490566,"one with O(nk) variables and constraints; solving which (to near-optimality) takes, e.g., eO(poly(nk))
243"
ALGORITHM,0.5679245283018868,"time using interior point methods [33].
244"
SAMPLE COMPLEXITY,0.569811320754717,"4.2
Sample Complexity
245"
SAMPLE COMPLEXITY,0.5716981132075472,"Thanks to the low function complexity of post-processing maps used in our algorithm to derive
246"
SAMPLE COMPLEXITY,0.5735849056603773,"classiﬁers (Eq. (1)), it enjoys the following efﬁcient sample complexity:
247"
SAMPLE COMPLEXITY,0.5754716981132075,"Theorem 4.2. Let f : X × A →∆k be a score function, and h : X × A →Y the (probabilistic)
248"
SAMPLE COMPLEXITY,0.5773584905660377,"classiﬁer derived from f using Algorithm 1 with the empirical distribution formed by samples
249"
SAMPLE COMPLEXITY,0.5792452830188679,"from Assumption 4.1 as the argument. Then under Assumption 2.3, for any group-wise calibrated
250"
SAMPLE COMPLEXITY,0.5811320754716981,"(Deﬁnition 3.4) reference score function ¯f : X × A →∆k, and n ≥Ω(maxa,y ln(mk/δ)/pay),
251"
SAMPLE COMPLEXITY,0.5830188679245283,"U −U(h)
 ≤O  
X"
SAMPLE COMPLEXITY,0.5849056603773585,"a∈[m],y∈[k]
υy r kpay"
SAMPLE COMPLEXITY,0.5867924528301887,"n
ln mk δ
+ k"
SAMPLE COMPLEXITY,0.5886792452830188,"n + ϵay ! ,"
SAMPLE COMPLEXITY,0.590566037735849,∆TPR(h) ≤α + O 
SAMPLE COMPLEXITY,0.5924528301886792,"max
a∈[m],y∈[k] s"
SAMPLE COMPLEXITY,0.5943396226415094,"k
npay
ln mk"
SAMPLE COMPLEXITY,0.5962264150943396,"δ
+
k
npay
+ ϵay pay !! ,"
SAMPLE COMPLEXITY,0.5981132075471698,"where U denotes the utility achieved by the optimal α-fair classiﬁer derived from the calibrated
252"
SAMPLE COMPLEXITY,0.6,"reference ¯f, and ϵay := E[| ¯fa(X)y −fa(X)y| · 1[A = a]].
253"
SAMPLE COMPLEXITY,0.6018867924528302,"The bound consists of a calibration error ϵay as discussed in the remarks of Theorem 3.5, an estimation
254"
SAMPLE COMPLEXITY,0.6037735849056604,"error from applying uniform convergence (the Natarajan dimension of the set of tiltings is O(k)), and
255"
SAMPLE COMPLEXITY,0.6056603773584905,"a k/n term that comes from the disagreement over class assignments on the samples between the
256"
SAMPLE COMPLEXITY,0.6075471698113207,"(deterministic) tilting found on Line 6 and the (probabilistic) classiﬁer on Line 5 due to tie-breaking.
257"
EXPERIMENTS,0.6094339622641509,"5
Experiments
258"
EXPERIMENTS,0.6113207547169811,"We evaluate Algorithm 1 for reducing TPR disparity on benchmark datasets, and demonstrate its
259"
EXPERIMENTS,0.6132075471698113,"effectiveness compared to existing post-processing as well as in-processing bias mitigation methods.
260"
EXPERIMENTS,0.6150943396226415,"Datasets.
The ﬁrst task is income prediction, for which, we use the ACSIncome dataset [18]—an
261"
EXPERIMENTS,0.6169811320754717,"extension of the UCI Adult dataset [27] with much more examples (1.6 million vs. 30,162), allowing
262"
EXPERIMENTS,0.6188679245283019,"us to compare methods conﬁdently. We consider a binary setting where the sensitive attribute is
263"
EXPERIMENTS,0.620754716981132,"gender and the target is whether the income is over $50k, as well as a multi-group multi-class setting
264"
EXPERIMENTS,0.6226415094339622,"with ﬁve race categories and ﬁve income buckets. The second is text classiﬁcation, of identifying
265"
EXPERIMENTS,0.6245283018867924,"occupations (28 in total) from biographies in the BiasBios dataset [14]; sensitive attribute is gender.
266"
EXPERIMENTS,0.6264150943396226,"Baselines and Setup.
The main baseline is FairProjection [2]—the only post-processing algo-
267"
EXPERIMENTS,0.6283018867924528,"rithm applicable for multi-class TPR parity to our knowledge.3 In the binary setting, we also compare
268"
EXPERIMENTS,0.630188679245283,"to RejectOption [25]. To demonstrate the deﬁciencies of existing methods at reducing TPR dispar-
269"
EXPERIMENTS,0.6320754716981132,"ity, we additionally include in-processing results using Reductions [1] and Adversarial [41].45
270"
EXPERIMENTS,0.6339622641509434,"On each task, we ﬁrst create a pre-training split from the dataset and train a linear logistic regression
271"
EXPERIMENTS,0.6358490566037736,"scoring model (with isotonic calibration and ﬁve-fold cross-validation as implemented in scikit-
272"
EXPERIMENTS,0.6377358490566037,"learn [37, 38, 28]), then randomly split the remaining data for post-processing and testing with
273"
EXPERIMENTS,0.6396226415094339,"10 different seeds and aggregate the results (the pre-trained model remains the same). For in-
274"
EXPERIMENTS,0.6415094339622641,"processing, we use the same splits but merge the pre-training and post-processing data for training.
275"
EXPERIMENTS,0.6433962264150943,"On BiasBios, linear logistic regression is performed on the embeddings of the biographies computed
276"
EXPERIMENTS,0.6452830188679245,"by a previously ﬁne-tuned BERT model [17] (in other words, head-tuning). Additional details
277"
EXPERIMENTS,0.6471698113207547,"including hyperparameters are included in the appendix.
278"
EXPERIMENTS,0.6490566037735849,"Results.
In Fig. 3, we plot the tradeoff curves from varying the fairness tolerance (α for our
279"
EXPERIMENTS,0.6509433962264151,"method). Our method is consistently the most effective at minimizing TPR disparity, particularly
280"
EXPERIMENTS,0.6528301886792452,"under multi-class settings, where existing algorithms only manage to partially reduce ∆TPR (and
281"
EXPERIMENTS,0.6547169811320754,"at a greater cost to accuracy when using FairProject and RejectOption). It also outperforms
282"
EXPERIMENTS,0.6566037735849056,"3We use the authors’ code, where TPR parity is equivalent to the meo constraint. The results from using the
KL divergence variant is included, which are better than the cross-entropy variant in our experiments.
4Although Reductions is extended to multi-class by Yang et al. [36], an implementation was not provided.
5The implementation (with minor modiﬁcations) in the AIF360 library is used for the latter methods [5]. 0.772 0.774 0.776 0.778"
EXPERIMENTS,0.6584905660377358,ACSIncome
EXPERIMENTS,0.660377358490566,"0.00
0.05
0.10
0.15"
EXPERIMENTS,0.6622641509433962,"0.725
0.750 0.472 0.478"
EXPERIMENTS,0.6641509433962264,ACSIncome
EXPERIMENTS,0.6660377358490566,"0.2
0.4
0.6 0.42 0.43 0.44"
EXPERIMENTS,0.6679245283018868,"0.855
0.856"
EXPERIMENTS,0.6698113207547169,BiasBios
EXPERIMENTS,0.6716981132075471,"0.2
0.4 0.82 0.83"
EXPERIMENTS,0.6735849056603773,"(2 genders, 2 classes)
(5 races, 5 classes)
(2 genders, 28 occupations)"
EXPERIMENTS,0.6754716981132075,"Violation of TPR parity, 
TPR"
EXPERIMENTS,0.6773584905660377,Classification accuracy
EXPERIMENTS,0.6792452830188679,"Ours
FairProjection-KL
RejectOption
Reductions
Adversarial (MLP)"
EXPERIMENTS,0.6811320754716981,"Figure 3: Tradeoff curves between accuracy and ∆TPR (Eq. (2)). The base model is logistic regression
(except for Adversarial, which uses a feedforward network). Error bars indicate the standard deviation
over 10 runs with different random dataset splits. Running time is reported in the appendix."
EXPERIMENTS,0.6830188679245283,"the in-processing Reductions on binary ACSIncome, and Adversarial in terms of ∆TPR, which,
283"
EXPERIMENTS,0.6849056603773584,"although enjoys higher accuracies because of the use of the more expressive feedforward networks
284"
EXPERIMENTS,0.6867924528301886,"as the prediction model, fails to reduce TPR parity. Sharper drops in accuracies are observed when
285"
EXPERIMENTS,0.6886792452830188,"applying our method with small α settings, e.g., 0.001 to 0.0001. We saw this happen when the
286"
EXPERIMENTS,0.690566037735849,"randomized component in Eq. (1b) is activated (i.e., β > 0), meaning that Line 3 ﬁnds a fair TPR
287"
EXPERIMENTS,0.6924528301886792,"that lies in the interior of the feasible region of the better-performing group in order to match the
288"
EXPERIMENTS,0.6943396226415094,"feasible TPR on the worse-performing one(s). Hence the drop is expected because utility is being
289"
EXPERIMENTS,0.6962264150943396,"sacriﬁced to achieve TPR parity.
290"
EXPERIMENTS,0.6981132075471698,"Although our method greatly reduces TPR disparity, there remains a gap to reaching ∆TPR = 0,
291"
EXPERIMENTS,0.7,"especially on tasks with more classes (i.e., BiasBios, where a higher variance is also observed).
292"
EXPERIMENTS,0.7018867924528301,"While this could be due to miscalibration, or potentially a violation of Assumption 2.3, the main
293"
EXPERIMENTS,0.7037735849056603,"reason is suspected to be insufﬁcient sample size. Recall from Theorem 4.2 that the sample complexity
294"
EXPERIMENTS,0.7056603773584905,"for ∆TPR scales as eO(
p"
EXPERIMENTS,0.7075471698113207,"k/npay) in the worse-case (a, y), which is itself at least eO(
p"
EXPERIMENTS,0.7094339622641509,"mk2/n).
295"
EXPERIMENTS,0.7113207547169811,"Thus, learning generalizable classiﬁers that satisfy TPR parity under more groups and classes is much
296"
EXPERIMENTS,0.7132075471698113,"harder in terms of data requirement (and by extension, computing resource).
297"
EXPERIMENTS,0.7150943396226415,"Lastly, we emphasize the necessity of group-wise calibration for achieving low ∆TPR, as the
298"
EXPERIMENTS,0.7169811320754716,"deﬁnition of the criterion involves conditioning on the true label (it is also reﬂected by the calibration
299"
EXPERIMENTS,0.7188679245283018,"error term ϵay in Theorem 4.2). In an ablation study in the appendix, a larger (minimum achievable)
300"
EXPERIMENTS,0.720754716981132,"∆TPR is observed when no efforts are made to calibrate the scoring model. It is therefore necessary
301"
EXPERIMENTS,0.7226415094339622,"for model vendors to provide accurate uncertainty quantiﬁcations, and for practitioners building fair
302"
EXPERIMENTS,0.7245283018867924,"classiﬁers to verify and improve calibration.
303"
CONCLUSIONS AND LIMITATIONS,0.7264150943396226,"6
Conclusions and Limitations
304"
CONCLUSIONS AND LIMITATIONS,0.7283018867924528,"We described a post-processing method for reducing TPR disparity for equal opportunity in multi-class
305"
CONCLUSIONS AND LIMITATIONS,0.730188679245283,"classiﬁcation, and demonstrated its performance in comparison to existing algorithms on benchmarks
306"
CONCLUSIONS AND LIMITATIONS,0.7320754716981132,"datasets, especially when the number of classes is large. We analyzed the sample complexity of our
307"
CONCLUSIONS AND LIMITATIONS,0.7339622641509433,"method, and established its optimality under model calibration.
308"
CONCLUSIONS AND LIMITATIONS,0.7358490566037735,"The effectiveness of our method at reducing TPR disparity is largely contributed to the tailored
309"
CONCLUSIONS AND LIMITATIONS,0.7377358490566037,"analysis, although it limits our method to this fairness notion only. Some use cases may demand
310"
CONCLUSIONS AND LIMITATIONS,0.7396226415094339,"equalized odds (bY ⊥A | Y ) beyond TPR parity (1[bY = Y ] ⊥A | Y ), which is a more stringent
311"
CONCLUSIONS AND LIMITATIONS,0.7415094339622641,"criterion: TPR parity only needs to match the main diagonal of the (conditional) confusion matrix
312"
CONCLUSIONS AND LIMITATIONS,0.7433962264150943,"across groups, whereas equalized odds requires matching all k2 entries. The design of efﬁcient
313"
CONCLUSIONS AND LIMITATIONS,0.7452830188679245,"algorithms for achieving equalized odds remains an open problem.6
314"
CONCLUSIONS AND LIMITATIONS,0.7471698113207547,"6We note that most (general-purpose) fairness algorithms, e.g., [2], are only evaluated for TPR parity but not
equalized odds."
REFERENCES,0.7490566037735849,"References
315"
REFERENCES,0.7509433962264151,"[1] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. A
316"
REFERENCES,0.7528301886792453,"Reductions Approach to Fair Classiﬁcation. In Proceedings of the 35th International Conference
317"
REFERENCES,0.7547169811320755,"on Machine Learning, pages 60–69, 2018.
318"
REFERENCES,0.7566037735849057,"[2] Wael Alghamdi, Hsiang Hsu, Haewon Jeong, Hao Wang, P. Winston Michalak, Shahab Asoodeh,
319"
REFERENCES,0.7584905660377359,"and Flavio P. Calmon. Beyond Adult and COMPAS: Fair Multi-Class Prediction via Information
320"
REFERENCES,0.7603773584905661,"Projection. In Advances in Neural Information Processing Systems, 2022.
321"
REFERENCES,0.7622641509433963,"[3] Solon Barocas and Andrew D. Selbst. Big Data’s Disparate Impact. California Law Review,
322"
REFERENCES,0.7641509433962265,"104(3):671–732, 2016.
323"
REFERENCES,0.7660377358490567,"[4] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning: Limita-
324"
REFERENCES,0.7679245283018868,"tions and Opportunities. MIT Press, 2023.
325"
REFERENCES,0.769811320754717,"[5] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde,
326"
REFERENCES,0.7716981132075472,"Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic,
327"
REFERENCES,0.7735849056603774,"Seema Nagar, Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna
328"
REFERENCES,0.7754716981132076,"Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. AI Fairness 360: An
329"
REFERENCES,0.7773584905660378,"Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias,
330"
REFERENCES,0.779245283018868,"2018. arxiv:1810.01943 [cs.AI].
331"
REFERENCES,0.7811320754716982,"[6] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On
332"
REFERENCES,0.7830188679245284,"the Dangers of Stochastic Parrots: Can Language Models Be Too Big?
. In Proceedings
333"
REFERENCES,0.7849056603773585,"of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610–623,
334"
REFERENCES,0.7867924528301887,"2021.
335"
REFERENCES,0.7886792452830189,"[7] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in
336"
REFERENCES,0.7905660377358491,"Criminal Justice Risk Assessments: The State of the Art. Sociological Methods & Research, 50
337"
REFERENCES,0.7924528301886793,"(1):3–44, 2021.
338"
REFERENCES,0.7943396226415095,"[8] Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. Man
339"
REFERENCES,0.7962264150943397,"is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. In
340"
REFERENCES,0.7981132075471699,"Advances in Neural Information Processing Systems, volume 29, 2016.
341"
REFERENCES,0.8,"[9] Joy Buolamwini and Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities
342"
REFERENCES,0.8018867924528302,"in Commercial Gender Classiﬁcation. In Proceedings of the 2018 Conference on Fairness,
343"
REFERENCES,0.8037735849056604,"Accountability, and Transparency, pages 77–91, 2018.
344"
REFERENCES,0.8056603773584906,"[10] Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building Classiﬁers with Independency
345"
REFERENCES,0.8075471698113208,"Constraints. In 2009 IEEE International Conference on Data Mining Workshops, pages 13–18,
346"
REFERENCES,0.809433962264151,"2009.
347"
REFERENCES,0.8113207547169812,"[11] Flavio P. Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and
348"
REFERENCES,0.8132075471698114,"Kush R. Varshney. Optimized Pre-Processing for Discrimination Prevention. In Advances in
349"
REFERENCES,0.8150943396226416,"Neural Information Processing Systems, volume 30, 2017.
350"
REFERENCES,0.8169811320754717,"[12] Simon Caton and Christian Haas.
Fairness in Machine Learning:
A Survey, 2020.
351"
REFERENCES,0.8188679245283019,"arxiv:2010.04053 [cs.LG].
352"
REFERENCES,0.8207547169811321,"[13] Jeffrey Dastin. Amazon scraps secret AI recruiting tool that showed bias against women.
353"
REFERENCES,0.8226415094339623,"Reuters, oct 2018.
URL https://www.reuters.com/article/us-amazon-com-jobs-
354"
REFERENCES,0.8245283018867925,"automation-insight-idUSKCN1MK08G.
355"
REFERENCES,0.8264150943396227,"[14] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexan-
356"
REFERENCES,0.8283018867924529,"dra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in
357"
REFERENCES,0.8301886792452831,"Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting. In Proceedings
358"
REFERENCES,0.8320754716981132,"of the 2019 ACM Conference on Fairness, Accountability, and Transparency, pages 120–128,
359"
REFERENCES,0.8339622641509434,"2019.
360"
REFERENCES,0.8358490566037736,"[15] Morris H. DeGroot and Stephen E. Fienberg. The Comparison and Evaluation of Forecasters.
361"
REFERENCES,0.8377358490566038,"Journal of the Royal Statistical Society. Series D (The Statistician), 32(1/2):12–22, 1983.
362"
REFERENCES,0.839622641509434,"[16] Christophe Denis, Romuald Elie, Mohamed Hebiri, and François Hu. Fairness guarantee in
363"
REFERENCES,0.8415094339622642,"multi-class classiﬁcation, 2023. arxiv:2109.13642 [math.ST].
364"
REFERENCES,0.8433962264150944,"[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
365"
REFERENCES,0.8452830188679246,"Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019
366"
REFERENCES,0.8471698113207548,"Conference of the North American Chapter of the Association for Computational Linguistics:
367"
REFERENCES,0.8490566037735849,"Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.
368"
REFERENCES,0.8509433962264151,"[18] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring Adult: New Datasets
369"
REFERENCES,0.8528301886792453,"for Fair Machine Learning. In Advances in Neural Information Processing Systems, 2021.
370"
REFERENCES,0.8547169811320755,"[19] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
371"
REFERENCES,0.8566037735849057,"Through Awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science
372"
REFERENCES,0.8584905660377359,"Conference, pages 214–226, 2012.
373"
REFERENCES,0.8603773584905661,"[20] Executive Ofﬁce of the President. Big Data: A Report on Algorithmic Systems, Opportunity,
374"
REFERENCES,0.8622641509433963,"and Civil Rights. The White House, 2016. URL https://purl.fdlp.gov/GPO/gpo90618.
375"
REFERENCES,0.8641509433962264,"[21] Solenne Gaucher, Nicolas Schreuder, and Evgenii Chzhen. Fair learning with Wasserstein
376"
REFERENCES,0.8660377358490566,"barycenters for non-decomposable performance measures. In Proceedings of The 26th Interna-
377"
REFERENCES,0.8679245283018868,"tional Conference on Artiﬁcial Intelligence and Statistics, pages 2436–2459, 2023.
378"
REFERENCES,0.869811320754717,"[22] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On Calibration of Modern Neural
379"
REFERENCES,0.8716981132075472,"Networks. In Proceedings of the 34th International Conference on Machine Learning, pages
380"
REFERENCES,0.8735849056603774,"1321–1330, 2017.
381"
REFERENCES,0.8754716981132076,"[23] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of Opportunity in Supervised Learning.
382"
REFERENCES,0.8773584905660378,"In Advances in Neural Information Processing Systems, volume 29, 2016.
383"
REFERENCES,0.879245283018868,"[24] Úrsula Hébert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Multicalibra-
384"
REFERENCES,0.8811320754716981,"tion: Calibration for the (Computationally-Identiﬁable) Masses. In Proceedings of the 35th
385"
REFERENCES,0.8830188679245283,"International Conference on Machine Learning, pages 1939–1948, 2018.
386"
REFERENCES,0.8849056603773585,"[25] Faisal Kamiran, Asim Karim, and Xiangliang Zhang. Decision Theory for Discrimination-
387"
REFERENCES,0.8867924528301887,"Aware Classiﬁcation. In 2012 IEEE 12th International Conference on Data Mining, pages
388"
REFERENCES,0.8886792452830189,"924–929, 2012.
389"
REFERENCES,0.8905660377358491,"[26] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing Fairness Gerryman-
390"
REFERENCES,0.8924528301886793,"dering: Auditing and Learning for Subgroup Fairness. In Proceedings of the 35th International
391"
REFERENCES,0.8943396226415095,"Conference on Machine Learning, pages 2564–2572, 2018.
392"
REFERENCES,0.8962264150943396,"[27] Ron Kohavi. Scaling Up the Accuracy of Naive-Bayes Classiﬁers: A Decision-Tree Hybrid. In
393"
REFERENCES,0.8981132075471698,"Proceedings of the Second International Conference on Knowledge Discovery and Data Mining,
394"
REFERENCES,0.9,"pages 202–207, 1996.
395"
REFERENCES,0.9018867924528302,"[28] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
396"
REFERENCES,0.9037735849056604,"Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vander-
397"
REFERENCES,0.9056603773584906,"plas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard
398"
REFERENCES,0.9075471698113208,"Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research,
399"
REFERENCES,0.909433962264151,"12(85):2825–2830, 2011.
400"
REFERENCES,0.9113207547169812,"[29] Preston Putzel and Scott Lee. Blackbox Post-Processing for Multiclass Fairness. In Proceedings
401"
REFERENCES,0.9132075471698113,"of the Workshop on Artiﬁcial Intelligence Safety 2022, volume 3087, 2022.
402"
REFERENCES,0.9150943396226415,"[30] Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null It Out:
403"
REFERENCES,0.9169811320754717,"Guarding Protected Attributes by Iterative Nullspace Projection. In Proceedings of the 58th
404"
REFERENCES,0.9188679245283019,"Annual Meeting of the Association for Computational Linguistics, pages 7237–7256, 2020.
405"
REFERENCES,0.9207547169811321,"[31] Julien Rouzot, Julien Ferry, and Marie-José Huguet. Learning Optimal Fair Scoring Systems for
406"
REFERENCES,0.9226415094339623,"Multi-Class Classiﬁcation. In 2022 IEEE 34th International Conference on Tools with Artiﬁcial
407"
REFERENCES,0.9245283018867925,"Intelligence, 2022.
408"
REFERENCES,0.9264150943396227,"[32] Hao Song, Tom Diethe, Meelis Kull, and Peter Flach. Distribution Calibration for Regression.
409"
REFERENCES,0.9283018867924528,"In Proceedings of the 36th International Conference on Machine Learning, pages 5897–5906,
410"
REFERENCES,0.930188679245283,"2019.
411"
REFERENCES,0.9320754716981132,"[33] Pravin M. Vaidya. Speeding-up linear programming using fast matrix multiplication. In 30th
412"
REFERENCES,0.9339622641509434,"Annual Symposium on Foundations of Computer Science, pages 332–337, 1989.
413"
REFERENCES,0.9358490566037736,"[34] Blake Woodworth, Suriya Gunasekar, Mesrob I. Ohannessian, and Nathan Srebro. Learning
414"
REFERENCES,0.9377358490566038,"Non-Discriminatory Predictors. In Proceedings of the 2017 Conference on Learning Theory,
415"
REFERENCES,0.939622641509434,"pages 1920–1953, 2017.
416"
REFERENCES,0.9415094339622642,"[35] Ruicheng Xian, Lang Yin, and Han Zhao. Fair and Optimal Classiﬁcation via Post-Processing
417"
REFERENCES,0.9433962264150944,"Predictors. In Proceedings of the 40th International Conference on Machine Learning, 2023.
418"
REFERENCES,0.9452830188679245,"[36] Forest Yang, Mouhamadou Cisse, and Sanmi Koyejo. Fairness with Overlapping Groups. In
419"
REFERENCES,0.9471698113207547,"Advances in Neural Information Processing Systems, volume 33, pages 4067–4078, 2020.
420"
REFERENCES,0.9490566037735849,"[37] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision
421"
REFERENCES,0.9509433962264151,"trees and naive Bayesian classiﬁers. In Proceedings of the Eighteenth International Conference
422"
REFERENCES,0.9528301886792453,"on Machine Learning, pages 609–616, 2001.
423"
REFERENCES,0.9547169811320755,"[38] Bianca Zadrozny and Charles Elkan. Transforming Classiﬁer Scores into Accurate Multiclass
424"
REFERENCES,0.9566037735849057,"Probability Estimates. In Proceedings of the Eighth ACM SIGKDD International Conference
425"
REFERENCES,0.9584905660377359,"on Knowledge Discovery and Data Mining, pages 694–699, 2002.
426"
REFERENCES,0.960377358490566,"[39] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi.
427"
REFERENCES,0.9622641509433962,"Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classiﬁcation without
428"
REFERENCES,0.9641509433962264,"Disparate Mistreatment. In Proceedings of the 26th International Conference on World Wide
429"
REFERENCES,0.9660377358490566,"Web, pages 1171–1180, 2017.
430"
REFERENCES,0.9679245283018868,"[40] Richard Zemel, Yu Ledell Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning Fair
431"
REFERENCES,0.969811320754717,"Representations. In Proceedings of the 30th International Conference on Machine Learning,
432"
REFERENCES,0.9716981132075472,"pages 325–333, 2013.
433"
REFERENCES,0.9735849056603774,"[41] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating Unwanted Biases with
434"
REFERENCES,0.9754716981132076,"Adversarial Learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and
435"
REFERENCES,0.9773584905660377,"Society, pages 335–340, 2018.
436"
REFERENCES,0.9792452830188679,"[42] Han Zhao and Geoffrey J. Gordon. Inherent Tradeoffs in Learning Fair Representations. Journal
437"
REFERENCES,0.9811320754716981,"of Machine Learning Research, 23(57):1–26, 2022.
438"
REFERENCES,0.9830188679245283,"[43] Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J. Gordon. Conditional Learning of
439"
REFERENCES,0.9849056603773585,"Fair Representations. In International Conference on Learning Representations, 2020.
440"
REFERENCES,0.9867924528301887,"[44] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender Bias
441"
REFERENCES,0.9886792452830189,"in Coreference Resolution: Evaluation and Debiasing Methods. In Proceedings of the 2018
442"
REFERENCES,0.9905660377358491,"Conference of the North American Chapter of the Association for Computational Linguistics:
443"
REFERENCES,0.9924528301886792,"Human Language Technologies, Volume 2 (Short Papers), pages 15–20, 2018.
444"
REFERENCES,0.9943396226415094,"[45] Shengjia Zhao, Michael P. Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating
445"
REFERENCES,0.9962264150943396,"Predictions to Decisions: A Novel Approach to Multi-Class Calibration. In Advances in Neural
446"
REFERENCES,0.9981132075471698,"Information Processing Systems, 2021.
447"
