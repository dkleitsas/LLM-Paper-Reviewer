Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001763668430335097,"We present a new monotonic improvement guarantee for optimizing decentralized
policies in cooperative Multi-Agent Reinforcement Learning (MARL), which
holds even when the transition dynamics are non-stationary. This new analysis
provides a theoretical understanding of the strong performance of two recent
actor-critic methods for MARL, i.e., Independent Proximal Policy Optimization
(IPPO) (Schröder de Witt et al., 2020) and Multi-Agent PPO (MAPPO) (Yu et al.,
2021), which both rely on independent ratios, i.e., computing probability ratios
separately for each agent’s policy. We show that, despite the non-stationarity that
independent ratios cause, a monotonic improvement guarantee still arises as a
result of enforcing the trust region constraint over joint policies. We also show
this trust region constraint can be effectively enforced in a principled way by
bounding independent ratios based on the number of agents in training, providing
a theoretical foundation for proximal ratio clipping. Moreover, we show that the
surrogate objectives optimized in IPPO and MAPPO are essentially equivalent
when their critics converge to a ﬁxed point. Finally, our empirical results support
the hypothesis that the strong performance of IPPO and MAPPO is a direct result
of enforcing such a trust region constraint via clipping in centralized training, and
the good values of the hyperparameters for this enforcement are highly sensitive to
the number of agents, as predicted by our theoretical analysis."
INTRODUCTION,0.003527336860670194,"1
INTRODUCTION"
INTRODUCTION,0.005291005291005291,"In cooperative multi-agent reinforcement learning (MARL), a team of agents must coordinate their
behavior to maximize a single cumulative return (Panait & Luke, 2005). In such a setting, partial
observability and/or communication constraints necessitate the learning of decentralized policies that
condition only on the local action-observation history of each agent. In a simulated or laboratory
setting, decentralized policies can often be learned in a centralized fashion, i.e., Centralized Training
with Decentralized Execution (CTDE)(Oliehoek & Amato, 2016), which allows agents to access
each other’s observations and unobservable extra state information during training."
INTRODUCTION,0.007054673721340388,"Actor-critic algorithms (Konda & Tsitsiklis, 2000) are a natural approach to CTDE because critics
can exploit centralized training by conditioning on extra information not available to the decentralized
policies (Lowe et al., 2017; Foerster et al., 2017). Unfortunately, such actor-critic methods have long
been outperformed by value-based methods such as QMIX (Rashid et al., 2018) on MARL benchmark
tasks such as Starcraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019). However, two
recent actor-critic algorithms (Schröder de Witt et al., 2020; Yu et al., 2021) have upended this
ranking by outperforming previously dominant MARL methods, such as MADDPG (Lowe et al.,
2017) and value-decomposed Q-learning (Sunehag et al., 2017; Rashid et al., 2018). Both algorithms
are multi-agent extensions of Proximal Policy Optimization (PPO) (Schulman et al., 2017) but one
uses decentralized critics, i.e., independent PPO (IPPO) (Schröder de Witt et al., 2020), and the other
uses centralized critics, i.e., multi-agent PPO (MAPPO) (Yu et al., 2021)."
INTRODUCTION,0.008818342151675485,"One key feature of PPO-based methods is the use of ratios (between the policy probabilities before
and after updating) in the objective. Both IPPO and MAPPO extend this feature of PPO to the
multi-agent setting by computing ratios separately for each agent’s policy during training, which we
call independent ratios. Unfortunately, until now there has been no theoretical justiﬁcation for the
use of such independent ratios. In this paper we show that the analysis that underpins the monotonic"
INTRODUCTION,0.010582010582010581,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.012345679012345678,"policy improvement guarantee for PPO (Schulman et al., 2015) does not carry over to the use of
independent ratios in IPPO and MAPPO. Instead, a direct application of this analysis leads to a joint
policy optimization and suggests the use of joint ratios, i.e., computing ratios between joint policies."
INTRODUCTION,0.014109347442680775,"The difference is crucial because, based on the existing trust region analysis for PPO, only a joint
ratios approach enjoys a monotonic policy improvement guarantee. Moreover, as independent ratios
consider only the change in one agent’s policy and ignore the fact that the other agents’ policies also
change, the transition dynamics underlying these independent ratios are non-stationary (Papoudakis
et al., 2019), breaking the assumptions in the monotonic improvement analysis (Schulman et al.,
2015). While some studies attempt to extend the monotonic improvement analysis to MARL (Wen
et al., 2021; Li & He, 2020), they primarily consider optimizing policies with joint ratios, rather than
independent ratios as in our paper, and are thus not applicable to IPPO or MAPPO."
INTRODUCTION,0.015873015873015872,"To address this gap, we provide a new monotonic improvement analysis that holds even when the
transition dynamics are non-stationary. We show that, despite this non-stationarity, a monotonic
improvement guarantee still arises as a result of enforcing the trust region constraint over joint
policies, i.e., a centralized trust region constraint. In other words, constraining the update of joint
policies in centralized training addresses the non-stationarity of learning decentralized policies. This
new analysis implies that independent ratios can also enjoy the same performance guarantee as joint
ratios if the centralized trust region constraint is properly enforced by bounding independent ratios.
In this way both IPPO and MAPPO can guarantee monotonic policy improvement. We also provide
a theoretical foundation for proximal ratio clipping by showing that centralized trust region can
be enforced in a principled way by bounding independent ratios based on the number of agents in
training. Furthermore, we show that the surrogate objectives optimized in IPPO and MAPPO are
essentially equivalent when their critics converge to a ﬁxed point."
INTRODUCTION,0.01763668430335097,"Finally, we provide empirical results that support the hypothesis that the strong performance of IPPO
and MAPPO is a direct result of enforcing such a trust region constraint via clipping in centralized
training. Particularly, we show that good values of the hyperparameters for the clipping range are
highly sensitive to the number of agents, as these hyperparameters, together with the number of
agents, effectively determine the size of the centralized trust region. Moreover, we show that IPPO
and MAPPO have comparable performance on SMAC maps with varied difﬁculty and numbers of
agents. This comparable performance also implies that the way of training critics could be less crucial
in practice than enforcing a trust region constraint."
RELATED WORK,0.019400352733686066,"2
RELATED WORK"
RELATED WORK,0.021164021164021163,"The use of trust region optimization in MARL traces back to parameter-sharing TRPO (PS-
TRPO) (Gupta et al., 2017), which combines parameter sharing with TRPO for cooperative multi-
agent continuous control but provides no theoretical support. Our analysis showing that a trust region
constraint is pivotal to guarantee performance improvement in MARL applies to PS-TRPO, among
other algorithms. Multi-agent trust region learning (MATRL) (Wen et al., 2021) uses a trust region
for independent learning with a game-theoretical analysis in the policy space. MATRL considers in-
dependent learning and proposes to enforce a trust region constraint by approximating the stable ﬁxed
point via a meta-game. Despite the improvement guarantee for joint policies, solving a meta-game
itself can be challenging because its complexity increases exponentially in the number of agents.
We instead consider centralized learning and enforce the trust region constraint in a centralized and
scalable way. Multi-Agent TRPO (MATRPO) directly extends TRPO to the multi-agent case (Li &
He, 2020) and divides the trust region by the number of agents. However, the analysis assumes a
private reward for each agent, which yields different theoretical results from ours. Non-stationarity
has been discussed in multi-agent mirror descent with trust region decomposition (Li et al., 2021),
which ﬁrst decomposes the trust region for each decentralized policy and then approximates the
KL divergence through additional training. However, this method needs to learn a fully centralized
action-value function and thus becomes becomes impractical when there are many agents."
BACKGROUND,0.02292768959435626,"3
BACKGROUND"
BACKGROUND,0.024691358024691357,"Dec-MDPs.
We consider a fully cooperative multi-agent task in which a team of cooperative agents
choose sequential actions in a stochastic environment. It can be modeled as a decentralized Markov"
BACKGROUND,0.026455026455026454,Under review as a conference paper at ICLR 2022
BACKGROUND,0.02821869488536155,"decision process (Dec-MDP), deﬁned by a tuple {N, S, A, P, r, ρ0, γ}, where N ≜{1, . . . , N}
denotes the set of N agents and s ∈S ≜S1×S2×...×SN describes the joint state of the environment.
The initial state s0 ∼ρ0 is drawn from distribution ρ0, and at each time step t, all agents k ∈N
choose simultaneously one action ak
t ∈Ak, yielding a joint action at ≜a1
t × a2
t × ... × aN
t ∈A ≜
A1 × A2 × ... × AN. After executing the joint action at in state st, the next state st+1 ∼P(st, at)
is drawn from transition kernel P and a collaborative reward rt = r(st, at) is returned. In a Dec-
MDP, each agent k ∈N has a local state sk
t ∈Sk, and chooses its actions with a decentralized
policy ak
t ∼πk(·|sk
t ) based only on its local state. The collaborating team of agents aims to
learn a joint policy, π(at|st) ≜QN
k=1 πk(ak
t |sk
t ), that maximizes their expected discounted return,
η(π) ≜E(st,at)[P∞
t=0 γtrt], where γ ∈[0, 1) is a discount factor."
BACKGROUND,0.029982363315696647,"Policy optimization methods.
For single-agent RL that is modeled as an inﬁnite-horizon dis-
counted Markov decision process (MDP) {S, A, P, r, ρ0, γ}, the performance for a policy π(a|s) is
deﬁned as: η(π) = E(st,at)
 P∞
t=0 γtr(st, at)

. The action-value function Qπ and value function
Vπ are deﬁned as:"
BACKGROUND,0.031746031746031744,"Qπ(st, at) = Est+1∼p(·|st,at),
at+1∼π(·|st+1) h ∞
X"
BACKGROUND,0.03350970017636684,"l=0
γlr(st+l, at+l)
i
,
Vπ(st) = Eat∼π(·|st)
h
Qπ(st, at)
i
."
BACKGROUND,0.03527336860670194,"Let the advantage function be Aπ(s, a) = Qπ(s, a) −Vπ(s); the following useful identity expresses
the expected return of another policy ˜π in terms of the advantage over π (Kakade & Langford,
2002): η(˜π) = η(π) + P"
BACKGROUND,0.037037037037037035,s ρ˜π(s) P
BACKGROUND,0.03880070546737213,"a ˜π(a|s)Aπ(s, a), where ρ˜π(s) is the state distribution induced
by ˜π. The complex dependency of ρ˜π(s) on ˜π makes the righthand side difﬁcult to optimize directly.
Schulman et al. (2015) proposed to consider the following surrogate objective"
BACKGROUND,0.04056437389770723,"Lπ(˜π) = η(π) +
X"
BACKGROUND,0.042328042328042326,"s
ρπ(s)
X"
BACKGROUND,0.04409171075837742,"a
˜π(a|s)Aπ(s, a) = η(π) + E(s,a)∼ρπ
h ˜π(a|s)"
BACKGROUND,0.04585537918871252,"π(a|s)Aπ(s, a)
i
,"
BACKGROUND,0.047619047619047616,"where the ρ˜π is replaced with ρπ. Deﬁne Dmax
TV (π, ˜π) ≜maxs DTV
 
π(·|s), ˜π(·|s)

, where DTV is
the total variation (TV) divergence.
Theorem 1. (Theorem 1 in Schulman et al. (2015)) Let α = Dmax
TV (π, ˜π). Then the following bound
holds
η(˜π) ≥Lπ(˜π) −
4ϵγ
(1 −γ)2 α2,"
BACKGROUND,0.04938271604938271,"where ϵ = maxs,a|Aπ(s, a)|."
BACKGROUND,0.05114638447971781,"This theorem forms the foundation of policy optimization methods, including Trust Region Policy
Optimization (TRPO) (Schulman et al., 2015) and Proximal Policy Optimization (PPO) (Schulman
et al., 2017). TRPO suggests a robust way to take large update steps by using a constraint, rather than
a penalty, on the TV divergence, and considers the following practical optimization problem,"
BACKGROUND,0.05291005291005291,"TRPO:
max
˜π
E(s,a)∼ρπ
h ˜π(a|s)"
BACKGROUND,0.054673721340388004,"π(a|s)Aπ(s, a)
i
,
s.t.
Dmax
TV (π(·|s), ˜π(·|s)) ≤α.
(1)"
BACKGROUND,0.0564373897707231,"This constrained optimization is complicated as it requires using conjugate gradient algorithms with
a quadratic approximation to the constraint. PPO simpliﬁes the above optimization by clipping
probability ratios λ˜π = ˜π(a|s)"
BACKGROUND,0.0582010582010582,π(a|s) to form a lower bound of Lπ(˜π):
BACKGROUND,0.059964726631393295,"PPO:
max
˜π
E(s,a)∼ρπ

min
 
λ˜πAπ(s, a), clip(λ˜π, 1 −ϵ, 1 + ϵ)Aπ(s, a)

,
(2)"
BACKGROUND,0.06172839506172839,where ϵ is a hyperparameter to specify the clipping range.
BACKGROUND,0.06349206349206349,"Independent PPO and Multi-Agent PPO.
Both IPPO (Schröder de Witt et al., 2020) and
MAPPO (Yu et al., 2021) optimize decentralized policies with independent ratios. In particular, the
main objective IPPO and MAPPO optimize is"
BACKGROUND,0.06525573192239859,"max
˜πk E(sk,ak)∼ρπk

min
 
λ˜πkAk(sk, ak), clip(λ˜πk, 1 −ϵ, 1 + ϵ)Ak(sk, ak)

∀k ∈{1, 2, ..., N}, (3)"
BACKGROUND,0.06701940035273368,Under review as a conference paper at ICLR 2022
BACKGROUND,0.06878306878306878,where λ˜πk = ˜πk(ak|sk)
BACKGROUND,0.07054673721340388,"πk(ak|sk) denotes the ratio between the decentralized policy probabilities of agent k
before and after updating. The difference between IPPO and MAPPO lies in how they estimate the
advantage function: IPPO learns a decentralized advantage function Ak(sk, ak) ≜P∞
t=0[r(sk
t , ak
t )]−
V (sk) based on the local information (sk, ak) for each agent, while MAPPO uses a centralized critic
that conditions on centralized state information s: Ak(sk, ak) ≜Es−k
 P∞
t=0[r(sk
t , ak
t )] −V (s)

,
where −k refers the set of all agents except agent k. Both methods use parameter sharing between
agents. Consequently, as all agents share the same actor and critic networks, one can ignore the agent
speciﬁer k in the objective and use experience from all agents to update the actor and critic networks:"
BACKGROUND,0.07231040564373897,"max
˜πθ X"
BACKGROUND,0.07407407407407407,"k
E(sk,ak)∼ρπθ

min
 
λθAφ(sk, ak), clip(λθ, 1 −ϵ, 1 + ϵ)Aφ(sk, ak)

,
(4)"
BACKGROUND,0.07583774250440917,where λθ = ˜πθ(ak|sk)
BACKGROUND,0.07760141093474426,"πθ(ak|sk), and θ, φ are shared parameters for policy and advantage networks. The use
of independent ratios together with parameter sharing has shown strong empirical results in various
MARL benchmark tasks (Schröder de Witt et al., 2020; Yu et al., 2021)."
TRUST REGION ANALYSIS FOR MARL,0.07936507936507936,"4
TRUST REGION ANALYSIS FOR MARL"
TRUST REGION ANALYSIS FOR MARL,0.08112874779541446,"In this section, we ﬁrst directly apply TRPO’s trust region analysis to cooperative MARL, which
yields joint ratios rather than the independent ratios adopted in IPPO and MAPPO. We then show that
optimization with independent ratios induces non-stationarity in MARL, which breaks the stationarity
assumption in the trust region analysis. Finally, we provide a new analysis that shows how monotonic
policy improvement can still arise from non-stationary transition dynamics with independent ratios."
OPTIMIZATION WITH JOINT RATIOS,0.08289241622574955,"4.1
OPTIMIZATION WITH JOINT RATIOS"
OPTIMIZATION WITH JOINT RATIOS,0.08465608465608465,"Consider the joint policy π(a|s) and the centralized advantage function Aπ(s, a) = Qπ(s, a) −
Vπ(s). Then, the trust region analysis for single-agent RL carries over directly to MARL with the
surrogate objective as Lπ(˜π) = η(π) + P"
OPTIMIZATION WITH JOINT RATIOS,0.08641975308641975,s ρπ(s) P
OPTIMIZATION WITH JOINT RATIOS,0.08818342151675485,"a ˜π(a|s)Aπ(s, a). One can consider the same
optimization problem for TRPO shown in equation 1,"
OPTIMIZATION WITH JOINT RATIOS,0.08994708994708994,"max
˜π
E(s,a)∼ρπ
h ˜π(a|s)"
OPTIMIZATION WITH JOINT RATIOS,0.09171075837742504,"π(a|s)Aπ(s, a)
i
,
s.t.
Dmax
TV (π(·|s), ˜π(·|s)) ≤α.
(5)"
OPTIMIZATION WITH JOINT RATIOS,0.09347442680776014,"The trust region constraint is enforced over joint policies, which we refer as a centralized trust region
constraint. With joint ratios deﬁned as λ˜π =
˜π(a|s)
π(a|s) = QN
k=1
 ˜πk(ak|sk)"
OPTIMIZATION WITH JOINT RATIOS,0.09523809523809523,"πk(ak|sk)

, one can simplify the
above optimization as PPO to have the following objective,"
OPTIMIZATION WITH JOINT RATIOS,0.09700176366843033,"JR-PPO:
max
˜π
E(s,a)∼ρπ

min
 
λ˜πAπ(s, a), clip(λ˜π, 1 −ϵ, 1 + ϵ)Aπ(s, a)

.
(6)"
OPTIMIZATION WITH JOINT RATIOS,0.09876543209876543,We call the resulting algorithm Joint Ratio PPO (JR-PPO) (see Algorithm 1 in the appendix).
OPTIMIZATION WITH JOINT RATIOS,0.10052910052910052,"Unlike IPPO and MAPPO, JR-PPO consider joint ratios over the joint policies, rather than independent
ones. This difference is crucial, as joint ratios naturally enjoy the monotonic improvement guarantee
carried over from the single-agent trust region analysis:, i.e., Theorem 1. Furthermore, the objective
used in IPPO and MAPPO is not equivalent to the above objective as they are lower bounds of
different objectives. Thus, Theorem 1 does not imply any guarantees for IPPO and MAPPO."
OPTIMIZATION WITH INDEPENDENT RATIOS,0.10229276895943562,"4.2
OPTIMIZATION WITH INDEPENDENT RATIOS"
OPTIMIZATION WITH INDEPENDENT RATIOS,0.10405643738977072,"Optimization with independent ratios, however, induces non-stationarity in MARL. When optimizing
decentralized policies, the environment is non-stationary from the perspective of a single agent
since the other agents also change their policies during training. To analyze the non-stationarity in
decentralized policy optimization, we ﬁrst consider the Markov chain for the local state sk induced by
the underlying MDP for agent k. When all agents’ policies are updated from π1, ..., πN to ˜π1, ..., ˜πN,
the state transition distribution of this Markov chain also shifts. We denote such transition shift from
sk to ˜sk for agent k as"
OPTIMIZATION WITH INDEPENDENT RATIOS,0.10582010582010581,"∆˜π1,...,˜πN
π1,...,πN (˜sk|sk) ≜
X ak"
OPTIMIZATION WITH INDEPENDENT RATIOS,0.10758377425044091,"
p˜π1,...,˜πN (˜sk|sk, ak)˜πk(ak|sk) −pπ1,...,πN (˜sk|sk, ak)πk(ak|sk)

,"
OPTIMIZATION WITH INDEPENDENT RATIOS,0.10934744268077601,Under review as a conference paper at ICLR 2022
OPTIMIZATION WITH INDEPENDENT RATIOS,0.1111111111111111,"where pπ1,...,πN and p˜π1,...,˜πN refer to the transition dynamics before and update π is updated. The
state transition shift consists of two parts: an exogenous part, which is caused by the update of other
agents’ policies (i.e., the change of transition dynamics from pπk to p˜πk), and an endogenous part,
which is contributed by the update of the agent’s own policy (i.e., the change of agent k’s policy from
πk to ˜πk). See the appendix A.2 for detailed analysis. The exogenous shift breaks the assumption in
the monotonic improvement guarantee (Schulman et al., 2015) that the MDP is stationary, i.e., the
state transition shift arises only endogenously from the agent’s policy changes. As a result, Theorem 1
no longer holds if one optimizes with independent ratios as in IPPO and MAPPO."
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.1128747795414462,"4.3
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.1146384479717813,"We now provide a new analysis for optimization with independent ratios. As the above analysis
suggests that the exogenous transition shift breaks the trust region analysis in TRPO, we instead
consider how to handle this exogenous shift in training. Speciﬁcally, since the exogenous shift is
caused by the changes of other agents’ policies, it can be controlled by constraining the update of
other agents’ policies in centralized training.
Proposition 1. In a Dec-MDP, the transition shift ∆˜π1,...,˜πN
π1,...,πN (˜sk|sk) decomposes as follows:"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.1164021164021164,"∆˜π1,...,˜πN
π1,...,πN (˜sk|sk) = ∆˜π1,π2,...,πN
π1,π2,...,πN (˜sk|sk) + ∆˜π1,˜π2,π3,...,πN
˜π1,π2,π3,...,πN (˜sk|sk) + ... + ∆˜π1,...,˜πN−1,˜πN
˜π1,...,˜πN−1,πN (˜sk|sk)."
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.11816578483245149,"The proof is given in the appendix A.3.1. This proposition implies that the state transition shift at
local observation sk is caused by the shifts arising from all decentralized policies. This decompo-
sition inspires the derivation of a new monotonic improvement guarantee for decentralized policy
optimization by enforcing the trust region over joint policies."
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.11992945326278659,Deﬁne the surrogate objective for decentralized policy πk as
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.12169312169312169,"L(k)
π1,π2,...,πN (˜πk′) =
X"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.12345679012345678,"sk
ρπ1,π2,...,πN (sk)
X"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.12522045855379188,"ak
˜πk′(ak|sk)Aπk′(sk, ak),"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.12698412698412698,and the expected return of decentralized policy πk as
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.12874779541446207,"ηπ1,...,πN (πk) = E(sk,ak)∼ρπ1,...,πN (sk,ak)

rk(sk, ak)

."
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.13051146384479717,"In practice, rk(sk, ak) is usually unknown. As the state transition shift decomposes into the sum of
transition shifts caused by each decentralized policy, we can bound this state transition shift with a
centralized trust region constraint as in equation 5.
Theorem 2. Let α = Dmax
TV (π, ˜π). Then the following bound holds :"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.13227513227513227,"η˜π1,...,˜πN (˜πk) ≥ηπ1,...,πN (πk) + N
X"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.13403880070546736,"k′=1
L(k)
π1,...,πN (˜πk′) −
4ϵγ
(1 −γ)2 α2
∀k,"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.13580246913580246,"where ϵ = maxk∈N maxsk,ak|Aπk(sk, ak)|."
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.13756613756613756,"The proof is given in the appendix A.3.2. This theorem implies that, for sufﬁciently small α, the
performance increase of a decentralized policy πk is lower bounded by the sum of surrogate objectives
for each decentralized policy with respect to the samples generated by πk. In other words, if the
trust region is enforced, the sum of surrogate objectives yields an approximate lower bound for
η˜π1,˜π2,...,˜πN (˜πk), which holds for any decentralized policy ˜πk."
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.13932980599647266,"Theorem 2 differs from Theorem 1 in three respects. First, the lower bound for one decentral-
ized policy effectively relies on surrogate objectives for all agents, since the update of one agent’s
policy affects all other agents’ transition probability. Therefore, to improve the performance for
policy πk, we can simultaneously maximize L(k)
π1,...,πN (˜π1) + L(k)
π1,...,πN (˜π2) + ... + L(k)
π1,...,πN (˜πN).
Second, unlike the surrogate objective in Theorem 1, the new surrogate objective implicitly con-
tains an independent ratio λ˜πk ≜˜πk(ak|sk)"
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.14109347442680775,"πk(ak|sk) as it can be rewritten as follows: L(k)
π1,π2,...,πN (˜πk′) ="
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.14285714285714285,"E(sk,ak)
h
˜πk′(ak|sk)
πk′(ak|sk)Aπk(sk, ak)
i
. Third, the additional term
4ϵγ
(1−γ)2 α2 requires computing the total
variation between joint policies π(a|s) and ˜π(a|s), rather than the policies that are directly opti-
mized. We show in the next section that, in centralized training, this requirement is easily satisﬁed as
the magnitude of the constraint on the update is proportional to the number of agents."
MONOTONIC POLICY IMPROVEMENT FOR INDEPENDENT RATIOS,0.14462081128747795,Under review as a conference paper at ICLR 2022
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.14638447971781304,"5
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.14814814814814814,"Theorem 2 indicates that the centralized trust region is crucial to guarantee monotonic improvement.
In this section, we show that bounding independent ratios is an effective way to enforce such a
centralized trust region constraint, and this enforcement requires taking into account the number of
agents. To achieve this, we ﬁrst present two lemmas about DTV divergence."
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.14991181657848324,"Proposition 2. In a Dec-MDP, Dmax
TV (π, ˜π) ≤PN
k=1 Dmax
TV (πk, ˜πk)."
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.15167548500881833,"This proposition is a direct result of the fact that the joint policy in a Dec-MDP factors as a product
of decentralized polices, i.e., π = QN
k=1 πk ."
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.15343915343915343,"Proposition 3. Dmax
TV (πk, ˜πk) = maxs∈S
P"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.15520282186948853,"˜πk(ak|sk)≥πk(ak|sk)

˜πk(ak|sk) −πk(ak|sk)

."
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.15696649029982362,"This useful identity follows from a property of DTV: DTV(µ(x), ν(x)) = P
µ(x)>ν(x)[µ(x) −ν(x)]
where µ and ν are two distributions. This proposition indicates that a decentralized trust region is
also deﬁned by the sum of probability differences over a special subset. We use this to upper bound
the trust region in the following analysis.
Assumption 1. Assume the advantage function Aπk(sk, ak) converges to a ﬁxed point for ∀k."
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.15873015873015872,Theorem 3. If independent ratios λk ≜˜πk(ak|sk)
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.16049382716049382,"πk(ak|sk) are within the range [1 −ϵk, 1 + ϵk] for ∀k ∈N,"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.16225749559082892,"then the following bound holds: Dmax
TV (πk(·|s), ˜πk(·|s)) < ϵk; Dmax
TV (π(·|s), ˜π(·|s)) < PN
k=1 ϵk."
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.164021164021164,"This theorem comes from that fact that optimizing ˜π(s, a) with respect to a converged A(s, a) leads
to ˜π(a|s) > π(a|s) if A(s, a) > 0, ∀s, a in actor-critic algorithms (Konda & Tsitsiklis, 2000). Thus,
based on Proposition 3, we have the following"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.1657848324514991,"Dmax
TV (πk, ˜πk) = max
sk
X ak∈Ak"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.1675485008818342,"Ak(sk,ak)>0"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.1693121693121693,"[˜πk(ak|sk) −πk(ak|sk)] ≤max
sk
X a∈Ak"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.1710758377425044,"Ak(sk,ak)>0"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.1728395061728395,[ϵkπk(ak|sk)] < ϵk.
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.1746031746031746,"The equation is from Proposition 3 by considering A(s, a) > 0 such that ˜π(a|s) > π(a|s). The
ﬁrst inequality is a result of bounded ratios and the second is from P"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.1763668430335097,"a∈{a:A(s,a)>0}[π(a|s)] < 1
(considering the lower bound yields the same analysis.) Furthermore, the trust region constraint over
joint policies is a direct result of Proposition 2. The detailed proof is given in the appendix A.3.3. As
DTV is a bounded divergence between [0, 1], the ratio guarantee makes sense when ϵk ≤1.0."
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.1781305114638448,Theorem 3 implies that bounding independent ratios ˜πk(ak|sk)
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.17989417989417988,"πk(ak|sk) with [1 −ϵk, 1 + ϵk] amounts to
enforcing a trust region constraint with size ϵ over decentralized policies. Thus, to enforce the
centralized trust region constraint over joint policies, i.e., Dmax
TV (π, ˜π) ≤α as in Theorem 2, one"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.18165784832451498,"can consider bounding independent ratios according to the number of agents, e.g., λk ≜˜πk(ak|sk)"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.18342151675485008,"πk(ak|sk) ∈
[1 −α"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.18518518518518517,"N , 1 + α"
REALIZING TRUST REGIONS VIA BOUNDING INDEPENDENT RATIOS,0.18694885361552027,"N ]. In the next section, we present practical implementations of these ratio constraints."
INSTANTIATING RATIO CONSTRAINTS,0.18871252204585537,"6
INSTANTIATING RATIO CONSTRAINTS"
INSTANTIATING RATIO CONSTRAINTS,0.19047619047619047,"In this section, we show that IPPO and MAPPO effectively satisfy the conditions of Theorem 2 and
Theorem 3. Speciﬁcally, the independent clipping and parameter sharing used by IPPO and MAPPO
are useful ways to approximate the ratio constraints in Theorem 3 and to optimize the surrogate
objective in Theorem 2. Furthermore, we show that the surrogate objectives optimized in IPPO and
MAPPO are essentially equivalent when their critics converge to a ﬁxed point."
OPTIMIZING SURROGATE OBJECTIVES,0.19223985890652556,"6.1
OPTIMIZING SURROGATE OBJECTIVES"
OPTIMIZING SURROGATE OBJECTIVES,0.19400352733686066,"The objective is to update all agents’ policies simultaneously with the experience from all agents,
which can be further simpliﬁed with parameter sharing (Gupta et al., 2017): max
θ X k X"
OPTIMIZING SURROGATE OBJECTIVES,0.19576719576719576,"sk
ρπ1,π2,...,πN (sk)
X"
OPTIMIZING SURROGATE OBJECTIVES,0.19753086419753085,"uk
˜πθ(uk|sk)Aφ(sk, uk),
(7)"
OPTIMIZING SURROGATE OBJECTIVES,0.19929453262786595,"s.t.
Dmax
T V (π(·|s), ˜π(·|s)) ≤α,
(8)"
OPTIMIZING SURROGATE OBJECTIVES,0.20105820105820105,Under review as a conference paper at ICLR 2022
OPTIMIZING SURROGATE OBJECTIVES,0.20282186948853614,"where θ and φ are the shared parameters for decentralized policies and critics. Furthermore, to
effectively optimize the surrogate objective, we can clip the probability ratios of each decentralized
policies to form a lower bound of the objective in equation 7, similar to PPO (Schulman et al., 2017).
Namely, with independent ratios λk = ˜πθ(uk|sk)"
OPTIMIZING SURROGATE OBJECTIVES,0.20458553791887124,"πθ(uk|sk)
∀k ∈N, we can optimize the following objective: max
θ X"
OPTIMIZING SURROGATE OBJECTIVES,0.20634920634920634,"k
E(sk,uk)∼ρ(sk,uk)

min
 
λkAk, clip(λk, 1 −ϵ, 1 + ϵ)Ak
,"
OPTIMIZING SURROGATE OBJECTIVES,0.20811287477954143,which is exactly the objective used by IPPO and MAPPO.
ENFORCING THE TRUST REGION CONSTRAINT,0.20987654320987653,"6.2
ENFORCING THE TRUST REGION CONSTRAINT"
ENFORCING THE TRUST REGION CONSTRAINT,0.21164021164021163,"Proposition 2 implies that, in centralized training, one way to enforce trust region constraint is to
delegate the centralized trust region constraint to each agent, such that the update of each decentralized
policy πk(ak|sk) is bounded. Therefore, to enforce the centralized trust region constraint, one can
impose a sufﬁcient condition as follows,"
ENFORCING THE TRUST REGION CONSTRAINT,0.21340388007054673,"Dmax
TV
 
πθ(·|sk), ˜πθ(·|sk)

≤α"
ENFORCING THE TRUST REGION CONSTRAINT,0.21516754850088182,"N
∀k ∈N,
(9)"
ENFORCING THE TRUST REGION CONSTRAINT,0.21693121693121692,"which suggests that enforcement of the centralized trust region constraint translates to a decentralized
trust region constraint if the trust region is speciﬁed properly according to the number of agents.
Furthermore, based on Theorem 3, bounding independent ratios is an effective way to enforce the
trust region constraint. One can thus impose a sufﬁcient condition to constrain independent ratios
λk such that λk ∈[1 −α"
ENFORCING THE TRUST REGION CONSTRAINT,0.21869488536155202,"N , 1 + α"
ENFORCING THE TRUST REGION CONSTRAINT,0.2204585537918871,"N ], where N is the number of agents in training. Clipping is one of
many ways to achieve this sufﬁcient condition but itself is a heuristic approximation so often fails to
bound ratios exactly within the ranges. In practice, one would need to tune the the clipping range
and the number of epochs so the ratios can be properly bounded. We show in the experiment section
that good values of the hyperparameters for the clipping range are highly sensitive to the number of
agents, as these hyperparameters, together with the number of agents, effectively determine the size
of the centralized trust region."
LEARNING ADVANTAGE FUNCTIONS,0.2222222222222222,"6.3
LEARNING ADVANTAGE FUNCTIONS"
LEARNING ADVANTAGE FUNCTIONS,0.2239858906525573,"We now look at the training of the advantage function, where IPPO and MAPPO differ. IPPO trains a
decentralized advantage function, while MAPPO trains a centralized one that incorporates centralized
state information. Assume a stationary distribution of (sk, ak) exists. From Lyu et al. (2021), we
have the following:"
LEARNING ADVANTAGE FUNCTIONS,0.2257495590828924,"Proposition 4. (Lemma 1 & 2 in Lyu et al. (2021)) Training of centralized critic and k-th decen-
tralized critic admits unique ﬁxed points Qπ(sk, s−k, ak, a−k) and Es−k,a−k[Qπ(sk, s−k, ak, a−k)]
respectively, where Qπ is the true expected return under the joint policy π."
LEARNING ADVANTAGE FUNCTIONS,0.2275132275132275,"Accordingly,
based
on
the
deﬁnition,
the
centralized
value
function
is
V (s)
=
V (sk, s−k)
=
Eak,a−k[Qπ(sk, s−k, ak, a−k)] and the decentralized one is V (sk)
=
Es−k,ak,a−k[Qπ(sk, s−k, uk, a−k)]
=
Es−k[V (sk, s−k)]
=
Es−k[V (s)].
Thus, we have
AIPPO(sk, ak) = AMAPPO(sk, ak) (and so IPPO and MAPPO objectives are equivalent) given that the
underlying critics converge to a ﬁxed point."
EXPERIMENTS,0.2292768959435626,"7
EXPERIMENTS"
EXPERIMENTS,0.2310405643738977,"We consider the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) for our empirical
analysis as it provides a wide range of multi-agent tasks with varied difﬁculty and numbers of agents.
We ﬁrst show that clipping is an effective way to constraint ratios when the number of optimization
epochs and the learning rate are properly speciﬁed. Furthermore, we show that clipping also requires
taking into account the number of agents such that the centralized trust region can be properly
enforced. We then empirically demonstrate that bounding independent ratios in effect enforces the
trust region over joint policies. Finally, we present results showing that IPPO and MAPPO perform
equivalently on SMAC maps with varied difﬁculty and numbers of agents."
EXPERIMENTS,0.2328042328042328,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2345679012345679,"0
10
20
30
40
50
Number of optimization epochs 0 2 4 6 8"
EXPERIMENTS,0.23633156966490299,Range of ratios
EXPERIMENTS,0.23809523809523808,"No clipping
Clipping 0.5
Clipping 0.4
Clipping 0.3
Clipping 0.2
Clipping 0.1 (a)"
EXPERIMENTS,0.23985890652557318,"0.0
0.2
0.4
0.6
0.8
1.0
Dmax
TV (
k,
k) 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.24162257495590828,Cumulative percentage
EXPERIMENTS,0.24338624338624337,Cliping range
EXPERIMENTS,0.24514991181657847,"No clipping
0.5
0.4
0.3
0.2
0.1 (b)"
EXPERIMENTS,0.24691358024691357,"0.0
0.2
0.4
0.6
0.8
1.0
Dmax
TV ( ,
) 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.24867724867724866,Cumulative percentage
EXPERIMENTS,0.25044091710758376,Number of agents
EXPERIMENTS,0.25220458553791886,"10
9
8
6
5
3
2 (c)"
EXPERIMENTS,0.25396825396825395,"0.0
0.2
0.4
0.6
0.8
1.0
Dmax
TV ( ,
) 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.25573192239858905,Cumulative percentage
EXPERIMENTS,0.25749559082892415,Number of epochs
EXPERIMENTS,0.25925925925925924,"40
30
25
20
15
10 (d)"
EXPERIMENTS,0.26102292768959434,"Figure 1: Trust region with respect to the clipping range, the number of agents and the number
of optimization epochs: (a) Ratio ranges for 5 agents with the number of optimization epochs; (b)
Cumulative percentage of decentralized trust region as the clipping value varies; (c) Cumulative
percentage of centralized trust region as the number of agents varies (clipping at 0.1); and (d)
Cumulative percentage of centralized trust region with optimization epochs (clipping at 0.1)."
EXPERIMENTS,0.26278659611992944,"Clipping and ratio ranges.
Theorem 3 indicates that bounding the independent ratios amounts to
enforcing a trust region constraint over joint policies. We empirically show that independent ratio
clipping approximately bounds the independent ratios in the training if some hyperparameters are
properly set. We train decentralized policies on one map, 2s3z, and clip the independent ratios when
optimizing the surrogate objective. Figure 1a shows how the the max and min of the ratios changes
according to the number of optimization epochs with different clipping values. Independent ratio
clipping can effectively constrain the range of ratios only when the number of optimization epochs and
the clipping range are properly speciﬁed. In particular, the range of independent ratios grows as the
number of optimization epochs increases. This growth is slower when the clipping range is smaller,
e.g., ϵ = 0.1. Furthermore, the clipping range may not strictly bound ratios between [1 −ϵ, 1 + ϵ]:
when the clipping range is 0.1, the independent ratios can exceed 1.2; and the independent ratios can
even grow up to 1.6 when the clipping range is 0.3."
EXPERIMENTS,0.26455026455026454,"Ratio clipping and trust region constraint.
Next, we show that the trust region deﬁned by the
total variation is empirically bounded by independent ratio clipping, and this bound is also propor-
tional to the number of agents. Speciﬁcally, we compute the maximum total variation divergence
Dmax
TV over empirical samples collected by the behavior policy during the ﬁrst round of actor update,
which contains 100 optimization epochs, and report the distribution of Dmax
TV . Figure 1b shows the
distribution of Dmax
TV over decentralized policies when clipping range varies. For clipping at 0.1,
all Dmax
TV values are smaller than 0.2, meaning that the trust region is effectively enforced to be
small. As the clipping range increases, more Dmax
TV values exceed 0.3. For the case without clipping,
Dmax
TV almost uniformly distributes among [0.0, 0.8], implying trust region is no longer enforced.
Figure 1c presents the distribution of Dmax
TV over joint polices for clipping at 0.1, on maps with
different number of agents. See appendix Table 1 for more details. The Dmax
TV (π, ˜π) is estimated by
summing up the empirical total variation distances Dmax
TV (πk, ˜πk) over all agents. The Dmax
TV (π, ˜π)
grows almost proportionally with the number of agents, indicating that enforcing the centralized trust
region with independent ratios clipping also requires considering the number of agents. Figure 1d
presents the distribution of Dmax
TV over joint polices with different numbers of epochs for clipping at
0.1. Compared to the number of agents, the number of epochs has less impact on the trust region."
EXPERIMENTS,0.26631393298059963,"Independent ratios clipping on SMAC
Figure 2 shows the empirical returns and trust region
estimates with different ratio clipping values across different maps in SMAC. 1 Notably, when the
clipping value is small, e.g., ϵ = 0.1, the joint total variation distance, i.e., the centralized trust region,
can be effectively bounded, as in the second row in Figure 2. The empirical returns corresponding to
ϵ = 0.1 are thus improved monotonically, outperforming all other returns consistently in four maps.
Moreover, as the number of agents increases, the trust region enforced by clipping value ϵ = 0.1 in
the initial training phase also grows from less than 0.3 to more than 0.5. In contrast, for clipping at"
EXPERIMENTS,0.26807760141093473,"1Trained via decentralized advantage, i.e., IPPO. Results with centralized advantage are similar, as presented
in the appendix A.4. Unlike Yu et al. (2021), the value function is not clipped in our experiments."
EXPERIMENTS,0.2698412698412698,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2716049382716049,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
EXPERIMENTS,0.27336860670194,Test Return Mean 2s3z
EXPERIMENTS,0.2751322751322751,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
EXPERIMENTS,0.2768959435626102,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 4 6 8 10 12 14 16 18"
EXPERIMENTS,0.2786596119929453,Test Return Mean 3s5z
EXPERIMENTS,0.2804232804232804,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
EXPERIMENTS,0.2821869488536155,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
EXPERIMENTS,0.2839506172839506,Test Return Mean
EXPERIMENTS,0.2857142857142857,10m_vs_11m
EXPERIMENTS,0.2874779541446208,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
EXPERIMENTS,0.2892416225749559,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
EXPERIMENTS,0.291005291005291,Test Return Mean
EXPERIMENTS,0.2927689594356261,27m_vs_30m
EXPERIMENTS,0.2945326278659612,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
EXPERIMENTS,0.2962962962962963,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.2980599647266314,Joint Approx TV 2s3z
EXPERIMENTS,0.2998236331569665,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
EXPERIMENTS,0.30158730158730157,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
EXPERIMENTS,0.30335097001763667,Joint Approx TV 3s5z
EXPERIMENTS,0.30511463844797176,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
EXPERIMENTS,0.30687830687830686,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.30864197530864196,Joint Approx TV
EXPERIMENTS,0.31040564373897706,10m_vs_11m
EXPERIMENTS,0.31216931216931215,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
EXPERIMENTS,0.31393298059964725,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 1.0 1.5 2.0 2.5 3.0"
EXPERIMENTS,0.31569664902998235,Joint Approx TV
EXPERIMENTS,0.31746031746031744,27m_vs_30m
EXPERIMENTS,0.31922398589065254,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
EXPERIMENTS,0.32098765432098764,Figure 2: Empirical returns and trust region estimates for independent ratios clipping.
EXPERIMENTS,0.32275132275132273,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
EXPERIMENTS,0.32451499118165783,Test Battle Win Mean 3s5z
EXPERIMENTS,0.3262786596119929,"IPPO (4)
MAPPO (4)"
EXPERIMENTS,0.328042328042328,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.3298059964726631,Test Battle Win Mean
EXPERIMENTS,0.3315696649029982,1c3s5z
EXPERIMENTS,0.3333333333333333,"IPPO (4)
MAPPO (4)"
EXPERIMENTS,0.3350970017636684,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.2 0.4 0.6 0.8"
EXPERIMENTS,0.3368606701940035,Test Battle Win Mean
EXPERIMENTS,0.3386243386243386,10m_vs_11m
EXPERIMENTS,0.3403880070546737,"IPPO (4)
MAPPO (4)"
EXPERIMENTS,0.3421516754850088,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
EXPERIMENTS,0.3439153439153439,Test Battle Win Mean
EXPERIMENTS,0.345679012345679,bane_vs_bane
EXPERIMENTS,0.3474426807760141,"IPPO (4)
MAPPO (4)"
EXPERIMENTS,0.3492063492063492,Figure 3: Contrasting IPPO and MAPPO across different maps.
EXPERIMENTS,0.3509700176366843,"0.5 and 1.0, the learning quickly plateaus at local optima, especially on maps with many agents, e.g.,
10m_vs_11m and 27m_vs_30m, which shows that the policy performance η(πk) is closely related
to the enforcement of trust region. We also apply the same clipping values to joint clipping and
independent clipping, see Appendix A.6 for more analysis."
EXPERIMENTS,0.3527336860670194,"IPPO and MAPPO
We show that the empirical performance of IPPO and MAPPO are very similar
despite the fact that the advantage functions are learned differently. We evaluate IPPO and MAPPO
on maps of varied difﬁculty and numbers of agents. We heuristically set the clipping range based
on the number of agents. Speciﬁcally, we set the clipping range ϵ for 3s5z, 1c3s5z, 10m_vs_11m,
and bane_vs_bane, as 0.1, 0.1, 0.1, and 0.05, respectively. The results are presented in Figure 3. On
the four maps considered, IPPO and MAPPO perform similarly. This phenomenon can be observed
in Yu et al. (2021), which provides more empirical comparisons between IPPO and MAPPO. Such
comparable performance also implies that, for actor-critic methods in MARL, the way of training
critics could be less crucial than enforcing the trust region constraint."
CONCLUSION,0.3544973544973545,"8
CONCLUSION"
CONCLUSION,0.3562610229276896,"In this paper, we present a new monotonic improvement guarantee for optimizing decentralized
policies in cooperative MARL. We show that, despite the non-stationarity in IPPO and MAPPO, a
monotonic improvement guarantee still arises from enforcing the trust region constraint over joint
policies. This guarantee provides a theoretical understanding of the strong performance of IPPO and
MAPPO. Furthermore, we provide a theoretical foundation for proximal ratio clipping by showing
that a trust region constraint can be effectively enforced in a principled way by bounding independent
ratios based on the number of agents in training. Finally, our empirical results support the hypothesis"
CONCLUSION,0.35802469135802467,Under review as a conference paper at ICLR 2022
CONCLUSION,0.35978835978835977,"that the strong performance of IPPO and MAPPO is a direct result of enforcing such a trust region
constraint via clipping in centralized training."
REFERENCES,0.36155202821869487,REFERENCES
REFERENCES,0.36331569664902996,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual Multi-Agent Policy Gradients. arXiv:1705.08926 [cs], December 2017. URL
http://arxiv.org/abs/1705.08926. arXiv: 1705.08926."
REFERENCES,0.36507936507936506,"Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pp. 66–83. Springer, 2017."
REFERENCES,0.36684303350970016,"Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In In
Proc. 19th International Conference on Machine Learning. Citeseer, 2002."
REFERENCES,0.36860670194003525,"Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008–1014, 2000."
REFERENCES,0.37037037037037035,"Hepeng Li and Haibo He.
Multi-agent trust region policy optimization.
arXiv preprint
arXiv:2010.07916, 2020."
REFERENCES,0.37213403880070545,"Wenhao Li, Xiangfeng Wang, Bo Jin, Junjie Sheng, and Hongyuan Zha.
Dealing with non-
stationarity in multi-agent reinforcement learning via trust region decomposition. arXiv preprint
arXiv:2102.10616, 2021."
REFERENCES,0.37389770723104054,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017."
REFERENCES,0.37566137566137564,"Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato. Contrasting centralized and
decentralized critics in multi-agent reinforcement learning. arXiv preprint arXiv:2102.04402,
2021."
REFERENCES,0.37742504409171074,"Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.
SpringerBriefs in Intelligent Systems. Springer International Publishing, 2016. ISBN 978-3-
319-28927-4. doi: 10.1007/978-3-319-28929-8. URL https://www.springer.com/gp/
book/9783319289274."
REFERENCES,0.37918871252204583,"Liviu Panait and Sean Luke. Cooperative Multi-Agent Learning: The State of the Art. Autonomous
Agents and Multi-Agent Systems, 11:387–434, November 2005. doi: 10.1007/s10458-005-2631-2."
REFERENCES,0.38095238095238093,"Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, and Stefano V Albrecht. Dealing with
non-stationarity in multi-agent deep reinforcement learning. arXiv preprint arXiv:1906.04737,
2019."
REFERENCES,0.38271604938271603,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018."
REFERENCES,0.3844797178130511,"Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019."
REFERENCES,0.3862433862433862,"Christian Schröder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H. S.
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge?
CoRR, abs/2011.09533, 2020. URL https://arxiv.org/abs/
2011.09533."
REFERENCES,0.3880070546737213,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897. PMLR,
2015."
REFERENCES,0.3897707231040564,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.3915343915343915,Under review as a conference paper at ICLR 2022
REFERENCES,0.3932980599647266,"Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel.
Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv:1706.05296 [cs],
June 2017. URL http://arxiv.org/abs/1706.05296. arXiv: 1706.05296."
REFERENCES,0.3950617283950617,"Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu Chen, and Jun Wang. A game-
theoretic approach to multi-agent trust region optimization. arXiv preprint arXiv:2106.06828,
2021."
REFERENCES,0.3968253968253968,"Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021."
REFERENCES,0.3985890652557319,Under review as a conference paper at ICLR 2022
REFERENCES,0.400352733686067,"A
APPENDIX"
REFERENCES,0.4021164021164021,"A.1
JOINT RATIO PPO"
REFERENCES,0.4038800705467372,Algorithm 1 Joint Ratio PPO (JR-PPO)
REFERENCES,0.4056437389770723,"for iteration i = 0, 1, 2, . . . do"
REFERENCES,0.4074074074074074,"Roll out decentralized policies [π1, π2, ..., πN] in environment;
Compute centralized advantage estimates A(s, a);
Compute joint ratios λ˜π = ˜π(a|s)"
REFERENCES,0.4091710758377425,"π(a|s) = QN
k=1
 ˜πk(ak|sk)"
REFERENCES,0.4109347442680776,"πk(ak|sk)

;
Optimize the surrogate objective max˜π E

min
 
λ˜πA(s, a), clip(λ˜π, 1 −ϵ, 1 + ϵ)A(s, a)

.
end for"
REFERENCES,0.4126984126984127,"A.2
STATIONARITY ASSUMPTION IN TRPO"
REFERENCES,0.4144620811287478,The single-agent TRPO relies on the following analysis:
REFERENCES,0.41622574955908287,"Lπ(˜π) −Lπ(π) =
X"
REFERENCES,0.41798941798941797,"s
ρ(s)
X a"
REFERENCES,0.41975308641975306," 
˜π(a|s) −π(a|s)

Aπ(s, a) =
X"
REFERENCES,0.42151675485008816,"s
ρ(s)
X a"
REFERENCES,0.42328042328042326," 
˜π(a|s) −π(a|s)

r(s) +
X"
REFERENCES,0.42504409171075835,"s′
p(s′|s, a)γv(s′) −v(s)
 =
X"
REFERENCES,0.42680776014109345,"s
ρ(s)
X s′ X a"
REFERENCES,0.42857142857142855," 
˜π(a|s) −π(a|s)

p(s′|s, a)γv(s′) =
X"
REFERENCES,0.43033509700176364,"s
ρ(s)
X s′ X a"
REFERENCES,0.43209876543209874," 
˜π(a|s)p(s′|s, a) −π(a|s)p(s′|s, a)

γv(s′) =
X"
REFERENCES,0.43386243386243384,"s
ρ(s)
X s′"
REFERENCES,0.43562610229276894," 
p˜π(s′|s) −pπ(s′|s)

γv(s′)."
REFERENCES,0.43738977072310403,"This analysis is based on the assumption that p(s′|s, a) remains the same before and after π is
updated, such that transition shift p˜π(s′|s) −pπ(s′|s) is only caused by the agent’s policy update,
i.e., endogenously. Such analysis no longer holds when the transition dynamics p(s′|s, a) are
non-stationary: p˜π(s′|s, a) ̸= pπ(s′|s, a)."
REFERENCES,0.43915343915343913,"A.3
PROOFS"
REFERENCES,0.4409171075837742,"A.3.1
PROOF OF PROPOSITION 1"
REFERENCES,0.4426807760141093,"Proof. Assume agent k’s policy πk is executed independently of other agents policies π−k, we have"
REFERENCES,0.4444444444444444,"∆˜π1,...,˜πN"
REFERENCES,0.4462081128747795,"π1,...,πN (˜sk|sk) =
X"
REFERENCES,0.4479717813051146,"˜s−k,s−k"
REFERENCES,0.4497354497354497,"ak,a−k"
REFERENCES,0.4514991181657848,"p(˜sk, ˜s−k|sk, s−k, ak, a−k)

˜πk(ak|sk)˜π−k(a−k|s−k) −πk(ak|sk)π−k(a−k|s−k)
 =
X"
REFERENCES,0.4532627865961199,"˜s−k,s−k"
REFERENCES,0.455026455026455,"ak,a−k"
REFERENCES,0.4567901234567901,"p(˜sk, ˜s−k|sk, s−k, ak, a−k) ·

˜πk(ak|sk)˜π−k(a−k|s−k) −˜πk(ak|sk)π−k(a−k|s−k)
|
{z
}
exogenous"
REFERENCES,0.4585537918871252,"+ ˜πk(ak|zk)π−k(a−k|s−k) −πk(ak|sk)π−k(a−k|s−k)
|
{z
}
endogenous 
."
REFERENCES,0.4603174603174603,"The above decomposition can be repeated such that the exogenous part can be translated into
endogenous parts that are speciﬁc to each agent. Speciﬁcally, repeat the decomposition for the"
REFERENCES,0.4620811287477954,Under review as a conference paper at ICLR 2022
REFERENCES,0.4638447971781305,exogenous part by considering agent k′ (k′ ̸= k):
REFERENCES,0.4656084656084656,˜πk(ak|sk)˜π−k(a−k|s−k) −˜πk(ak|sk)π−k(a−k|s−k)
REFERENCES,0.4673721340388007,"=˜πk(ak|sk)

˜πk′(ak′|sk′)˜π−{k,k′}(a−{k,k′}|s−{k,k′}) −πk′(ak′|sk′)π−{k,k′}(a−{k,k′}|s−{k,k′})
"
REFERENCES,0.4691358024691358,"=˜πk(ak|sk)

˜πk′(ak′|sk′)˜π−{k,k′}(a−{k,k′}|s−{k,k′}) −˜πk′(ak′|sk′)π−{k,k′}(a−{k,k′}|s−{k,k′})
|
{z
}
πk-exogenous"
REFERENCES,0.4708994708994709,"+ ˜πk′(ak′|sk′)π−{k,k′}(a−{k,k′}|s−{k,k′}) −πk′(ak′|sk′)π−{k,k′}(a−{k,k′}|s−{k,k′})
|
{z
}
πk-endogenous ."
REFERENCES,0.47266313932980597,"So on and so forth, one can decompose ∆˜π1,...,˜πN"
REFERENCES,0.47442680776014107,"π1,...,πN (˜sk|sk) as follows:"
REFERENCES,0.47619047619047616,"∆˜π1,...,˜πN"
REFERENCES,0.47795414462081126,"π1,...,πN (˜sk|sk) = ∆˜π1,π2,...,πN"
REFERENCES,0.47971781305114636,"π1,π2,...,πN (˜sk|sk) + ∆˜π1,˜π2,π3,...,πN"
REFERENCES,0.48148148148148145,"˜π1,π2,π3,...,πN (˜sk|sk) + ... + ∆˜π1,...,˜πN−1,˜πN"
REFERENCES,0.48324514991181655,"˜π1,...,˜πN−1,πN (˜sk|sk),"
REFERENCES,0.48500881834215165,"which implies that the state transition shift at local observation sk is caused by the shifts arising from
all decentralized policies."
REFERENCES,0.48677248677248675,"A.3.2
PROOF OF THEOREM 2"
REFERENCES,0.48853615520282184,"Proof. This proof is based on the perturbation theory.
Let Gsi
= (1 + γP si
π1,π2,...,πN +
(γP si
π1,π2,...,πN )2 + ... = (1 −γP si
π1,π2,...,πN )−1 and ˜Gsi = (1 + γP si
˜π1,˜π2,...,˜πN + (γP si
˜π1,˜π2,...,˜πN )2 +
... = (1−γP si
˜π1,˜π2,...,˜πN )−1 denote the distribution of state si under π1, π2, ..., πN and ˜π1, ˜π2, ..., ˜πN.
We will use the convention that ρ (a density on state space) is a vector and r (a reward function
on state space) is a dual vector (i.e., linear functional on vectors), thus rρ is a scalar meaning the
expected reward under density ρ. Note that η(π) = rGρ0, and η(˜π) = r ˜Gρ0. Denote the state shift
of si as ∆(si) = P si
˜π1,˜π2,...,˜πN −P si
π1,π2,...,πN . Using the perturbation theory, we have the following"
REFERENCES,0.49029982363315694,"[Gsi]−1 −[ ˜Gsi]−1 = γP si
˜π1,˜π2,...,˜πN −γP si
π1,π2,...,πN = γ∆(si)."
REFERENCES,0.49206349206349204,Left multiply by Gsi and right multiply by ˜Gsi:
REFERENCES,0.49382716049382713,˜Gsi = Gsi + γGsi∆(si) ˜Gsi.
REFERENCES,0.49559082892416223,Substituting the right-hand side into ˜Gsi gives
REFERENCES,0.4973544973544973,˜Gsi −Gsi = γGsi∆(si)Gsi + γ2Gsi∆(si)Gsi∆(si) ˜Gsi).
REFERENCES,0.4991181657848324,Consider decentralized policy πi:
REFERENCES,0.5008818342151675,"η˜π1,˜π2,...,˜πN (˜πi) −ηπ1,π2,...,πN (πi)"
REFERENCES,0.5026455026455027,=r ˜Gsiρ0 −rGsiρ0 = r( ˜Gsi −Gsi)ρ0
REFERENCES,0.5044091710758377,=γrGsi∆(si)Gsiρ0 + γ2rGsi∆(si)Gsi∆(si) ˜Gsiρ0.
REFERENCES,0.5061728395061729,"Let us ﬁrst consider the leading term γrGsi∆(si)Gsiρ0,"
REFERENCES,0.5079365079365079,"γrGsi∆(si)Gsiρ0 =
X"
REFERENCES,0.5097001763668431,"si
ρ(si)
X"
REFERENCES,0.5114638447971781,"s′
i
(p˜π(s′
i|si) −pπ(s′
i|si))γv(s′
i) =
X"
REFERENCES,0.5132275132275133,"si
ρ(si)
X"
REFERENCES,0.5149911816578483,"s′
i
∆˜π1,...,˜πN
π1,...,πN (s′
i|si)γv(s′
i) =
X"
REFERENCES,0.5167548500881834,"si
ρ(si)
X s′
i"
REFERENCES,0.5185185185185185,"h
∆˜π1,π2,...,πN
π1,π2,...,πN (s′
i|si) + ∆˜π1,˜π2,π3,...,πN
˜π1,π2,π3,...,πN (s′
i|si) + ... + ∆˜π1,...,˜πN−1,˜πN
˜π1,...,˜πN−1,πN (s′
i|si)
i
γv(s′
i)."
REFERENCES,0.5202821869488536,Under review as a conference paper at ICLR 2022
REFERENCES,0.5220458553791887,"For one of these summation terms, we have the following 2:
X"
REFERENCES,0.5238095238095238,"si
ρ(si)
X s′
i"
REFERENCES,0.5255731922398589,"h
∆˜π1,...,˜πj−1,˜πj,...,πN
˜π1,...,˜πj−1,πj,...,πN (s′
i|si)
i
γv(s′
i)
(10) =
X"
REFERENCES,0.527336860670194,"si
ρ(si)
X s′
i X ai"
REFERENCES,0.5291005291005291,"
p˜π1,...,˜πj−1,πj,...,πN (s′
i|si, ai)˜πj(ai|si) −p˜π1,...,˜πj−1,πj,...,πN (s′
i|si, ai)πj(ai|si)

γv(s′
i) (11) =
X"
REFERENCES,0.5308641975308642,"si
ρ(si)
X ai"
REFERENCES,0.5326278659611993," 
˜πj(ai|si) −πj(ai|si)
 X"
REFERENCES,0.5343915343915344,"s′
i
p˜π1,...,˜πj−1,πj,...,πN (s′
i|si, ai)γv(s′
i)
(12) =
X"
REFERENCES,0.5361552028218695,"si
ρ(si)
X ai"
REFERENCES,0.5379188712522046," 
˜πj(ai|si) −πj(ai|si)

r(si) +
X"
REFERENCES,0.5396825396825397,"s′
i
p˜π1,...,˜πj−1,πj,...,πN (s′
i|si, ai)γv(s′
i) −v˜π1,...,˜πj−1,
πj,...,πN
(si)
 (13) =
X"
REFERENCES,0.5414462081128748,"si
ρ(si)
X ai"
REFERENCES,0.5432098765432098," 
˜πj(ai|si) −πj(ai|si)

Aπj(si, ai)
(14)"
REFERENCES,0.544973544973545,"=L(i)
π1,π2,...,πN (˜πj) −L(i)
π1,π2,...,πN (πj).
(15)
The derivation from line 13 to 14 is based on the following deﬁnition:"
REFERENCES,0.54673721340388,"Aπj(si, ai) ≜r(si) +
X"
REFERENCES,0.5485008818342152,"s′
i
p˜π1,...,˜πj−1,πj,...,πN (s′
i|si, ai)γv(s′
i) −v˜π1,...,˜πj−1,
πj,...,πN
(si),"
REFERENCES,0.5502645502645502,"where v˜π1,...,˜πj−1,
πj,...,πN
(si) ≜r(si) + γ P"
REFERENCES,0.5520282186948854,"s′
i p˜π1,...,˜πj−1,πj,...,πN (s′
i|si, ai) P"
REFERENCES,0.5537918871252204,"ai πj(ai|si)v(s′
i). which"
REFERENCES,0.5555555555555556,"will result in L(i)
π1,π2,...,πN (πj) = 0 for ∀j. In fact, for line 13, r(si) and v˜π1,...,˜πj−1,
πj,...,πN
(si) can be"
REFERENCES,0.5573192239858906,"interpreted as functions over si, which will be zero if integrated with P"
REFERENCES,0.5590828924162258,"ai
 
˜πj(ai|si) −πj(ai|si)

.
Note that the advantage function also conditions on the πj."
REFERENCES,0.5608465608465608,"Note that the advantage of πj with respect to si, ai in multi-agent RL is deﬁned differently from the
advantage function in the single agent case."
REFERENCES,0.562610229276896,We can thus repeat the above decomposition iteratively and have the following:
REFERENCES,0.564373897707231,"γrGsi∆(si)Gsiρ0 = L(i)
π1,π2,...,πN (˜π1) −L(i)
π1,π2,...,πN (π1)"
REFERENCES,0.5661375661375662,"+ L(i)
π1,π2,...,πN (˜π2) −L(i)
π1,π2,...,πN (π2)"
REFERENCES,0.5679012345679012,"+ ... + L(i)
π1,π2,...,πN (˜πN) −L(i)
π1,π2,...,πN (πN)."
REFERENCES,0.5696649029982364,"Note that L(i)
π1,π2,...,πN (πj) = 0 for ∀j. Thus,"
REFERENCES,0.5714285714285714,"γrGsi∆(si)Gsiρ0 = L(i)
π1,π2,...,πN (˜π1) + L(i)
π1,π2,...,πN (˜π2) + ... + L(i)
π1,π2,...,πN (˜πN)."
REFERENCES,0.5731922398589065,"Next we bound the second term γ2rGsi∆(si)Gsi∆(si) ˜Gsiρ0.
First we consider the product
γrGsi∆(si) = γv∆(si). Consider the component of this dual vector:"
REFERENCES,0.5749559082892416,"|(γrGsi∆(si)s| = |
X"
REFERENCES,0.5767195767195767,"s′
i
(p˜πi(s′
i|si) −pπi(s′
i|si))γvi(s′
i)| =|
X"
REFERENCES,0.5784832451499118,"s′
1,...,s′
N X"
REFERENCES,0.5802469135802469,"s1,...,si−1,si+1,sN X"
REFERENCES,0.582010582010582,"a1,...,aN
p(s′
1, ..., s′
N|s1, ..., sN, a1, ..., aN)vi(s′
i)
 N
Y"
REFERENCES,0.5837742504409171,"j=1
˜πj(ai|si) − N
Y"
REFERENCES,0.5855379188712522,"j=1
πj(ai|si)

| =|
X"
REFERENCES,0.5873015873015873,"s1,...,si−1,si+1,sN X"
REFERENCES,0.5890652557319224,"a1,...,aN
Ai(si, ai)
 N
Y"
REFERENCES,0.5908289241622575,"j=1
˜πj(ai|si) − N
Y"
REFERENCES,0.5925925925925926,"j=1
πj(ai|si)

|."
REFERENCES,0.5943562610229277,"Thus, we have"
REFERENCES,0.5961199294532628,"∥(γrGsi∆(si)s∥≤
X"
REFERENCES,0.5978835978835979,"s1,...,sN X"
REFERENCES,0.599647266313933,"a1,...,aN
|Ai(si, ai)|| N
Y"
REFERENCES,0.6014109347442681,"j=1
˜πj(ai|si) − N
Y"
REFERENCES,0.6031746031746031,"j=1
πj(ai|si)| ≤2αϵ."
REFERENCES,0.6049382716049383,"2 Note that Aπj(si, ai) in the analysis is deﬁned with the transition dynamics p˜π1,...,˜πj−1,πj,...,πN (s′
i|si, ai)."
REFERENCES,0.6067019400352733,Under review as a conference paper at ICLR 2022
REFERENCES,0.6084656084656085,We bound the other portion Gsi∆(si) ˜Gsiρ0 using the l1 operator norm:
REFERENCES,0.6102292768959435,∥Gsi∆(si) ˜Gsiρ0∥1 ≤∥∆(si)∥1
REFERENCES,0.6119929453262787,(1 −γ)2 .
REFERENCES,0.6137566137566137,The ∥∆(si)∥1 can be bound as follows:
REFERENCES,0.6155202821869489,"∥∆(si)∥1 =
X"
REFERENCES,0.6172839506172839,"s′
i,si
|p˜πi(s′
i|si) −pπi(s′
i|si)| =
X"
REFERENCES,0.6190476190476191,"s′
1,...,s′
N X"
REFERENCES,0.6208112874779541,"s1,...,sN X"
REFERENCES,0.6225749559082893,"a1,...,aN
p(s′
1, ..., s′
N|s1, ..., sN, a1, ..., aN)| N
Y"
REFERENCES,0.6243386243386243,"j=1
˜πj(ai|si) − N
Y"
REFERENCES,0.6261022927689595,"j=1
πj(ai|si)| =
X"
REFERENCES,0.6278659611992945,"s1,...,sN X"
REFERENCES,0.6296296296296297,"a1,...,aN
| N
Y"
REFERENCES,0.6313932980599647,"j=1
˜πj(ai|si) − N
Y"
REFERENCES,0.6331569664902998,"j=1
πj(ai|si)| ≤2α."
REFERENCES,0.6349206349206349,So we have that
REFERENCES,0.63668430335097,"γ2|rGsi∆(si)Gsi∆(si) ˜Gsiρ0| ≤γ∥rGsi∆(si)∥∞∥Gsi∆(si) ˜Gsiρ0∥1 ≤
4ϵγ
(1 −γ)2 α2."
REFERENCES,0.6384479717813051,"A.3.3
PROOF OF THEOREM 3"
REFERENCES,0.6402116402116402,"Proof. Given the assumption that the advantage function A(s, a) converges to a ﬁxed point. we have
the fact that optimizing ˜π(s, a) with respect to A(s, a) leads to ˜π(a|s) > π(a|s) if A(s, a) > 0, ∀s, a.
Consider independent ratios λk ≜˜πk(ak|sk)"
REFERENCES,0.6419753086419753,"πk(ak|sk) which are within the range [1 −ϵk, 1 + ϵk] for ∀k ∈N,
Thus, based on Proposition 3, we have the following"
REFERENCES,0.6437389770723104,"Dmax
TV (πk, ˜πk) = max
sk
X"
REFERENCES,0.6455026455026455,"ak∈Ak
˜π(a|s)>π(a|s)"
REFERENCES,0.6472663139329806,[˜πk(ak|sk) −πk(ak|sk)]
REFERENCES,0.6490299823633157,"≤max
sk
X ak∈Ak"
REFERENCES,0.6507936507936508,"Ak(sk,ak)>0"
REFERENCES,0.6525573192239859,[(1 + ϵk)πk(ak|sk) −πk(ak|sk)]
REFERENCES,0.654320987654321,"≤max
sk
X a∈Ak"
REFERENCES,0.656084656084656,"Ak(sk,ak)>0"
REFERENCES,0.6578483245149912,[ϵkπk(ak|sk)] < ϵk.
REFERENCES,0.6596119929453262,The ﬁrst inequality is a result of bounded ratios and the second is from P
REFERENCES,0.6613756613756614,"a∈{a:A(s,a)>0}[π(a|s)] < 1.
Furthermore, the trust region constraint over joint policies is a direct result of Proposition 2. One can
also consider A(s, a) < 0, which leads to ˜π(a|s) < π(a|s) in optimization. Given that independent
ratios are also lower bounded by 1 −ϵ, the same conclusion can be reached for Dmax
TV ."
REFERENCES,0.6631393298059964,"A.4
EXPERIMENT DETAILS AND MORE RESULTS"
REFERENCES,0.6649029982363316,The number of agents in each is given in Table 1.
REFERENCES,0.6666666666666666,"Test battle win mean of IPPO on maps with varied difﬁculty and numbers of agents is presented in
Figure 4."
REFERENCES,0.6684303350970018,"Empirical test battle win mean, test returns and trust region estimates of MAPPO on maps with varied
difﬁcult and numbers of agents are presented in Figure 5."
REFERENCES,0.6701940035273368,"A.5
ABLATIONS ON SMALL CLIPPING VALUES"
REFERENCES,0.671957671957672,"We also present the ablation results for small clipping values, i.e., < 0.1, in Figure 6. It is true that a
small clipping value results in a small trust region, and thus small clipping values, e.g., 0.08, 0.05
and 0.03, would be preferred for maps with a large number of agents, e.g., maps 10m_vs_11m (10"
REFERENCES,0.673721340388007,Under review as a conference paper at ICLR 2022
REFERENCES,0.6754850088183422,Table 1: Number of agents on maps.
REFERENCES,0.6772486772486772,"SMAC Map
Number of agents"
REFERENCES,0.6790123456790124,"2s_vs_1sc
2"
REFERENCES,0.6807760141093474,"3s_vs_5z
3"
REFERENCES,0.6825396825396826,"2s3z
5"
REFERENCES,0.6843033509700176,"6h_vs_8z
6"
REFERENCES,0.6860670194003528,"1c3s5z
9"
REFERENCES,0.6878306878306878,"10m_vs_11m
10"
REFERENCES,0.689594356261023,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.691358024691358,Test Battle Win Mean 2s3z
REFERENCES,0.6931216931216931,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
REFERENCES,0.6948853615520282,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.6966490299823633,Test Battle Win Mean 3s5z
REFERENCES,0.6984126984126984,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
REFERENCES,0.7001763668430335,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.7019400352733686,Test Battle Win Mean
REFERENCES,0.7037037037037037,10m_vs_11m
REFERENCES,0.7054673721340388,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
REFERENCES,0.7072310405643739,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.708994708994709,Test Battle Win Mean
REFERENCES,0.7107583774250441,27m_vs_30m
REFERENCES,0.7125220458553791,"0.1 (5)
0.3 (5)
0.5 (5)
1.0 (5)
No clipping (5)"
REFERENCES,0.7142857142857143,Figure 4: Test battle win mean of IPPO on maps with varied difﬁculty and numbers of agents
REFERENCES,0.7160493827160493,"agents) and 27m_vs_30m (27 agents). However, when the clip value is too small, e.g., ϵ = 0.01 in
maps with 5 and 8 agents, the resultant trust region is also small and the update step in each iteration
can thus be too small to improve the policy. Thus, one would need to trade off between the trust
region constraint, to ensure monotonic improvement, and the policy update step, to ensure a sufﬁcient
parameter update at each iteration."
REFERENCES,0.7178130511463845,"A.6
COMPARISON BETWEEN JOINT RATIO CLIPPING AND INDEPENDENT RATIO CLIPPING"
REFERENCES,0.7195767195767195,"We apply the same clipping values to these two types of clipping, and use maps with many agents,
i.e., 10m_vs_11m and 27m_vs_30m, to make the difference more salient (based on the theoretical
results in the paper). The results are presented in Figure 7 and 8."
REFERENCES,0.7213403880070547,"Compared to joint ratio clipping, the independent ratio clipping is more sensitive to the number of
agents. In particular, for a small clipping value, e.g., ϵ = 0.1, joint ratio clipping consistently produces
better performance than independent ratio clipping, even when the number of agents changes from
10 to 27. As the clipping value increases to 0.5, the performance gap between these two types of
clipping becomes larger, which is also aligned with our theoretical analysis."
REFERENCES,0.7231040564373897,Under review as a conference paper at ICLR 2022
REFERENCES,0.7248677248677249,"0
250000 500000 750000 10000001250000150000017500002000000 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.7266313932980599,test_battle_won_mean 3s5z
REFERENCES,0.7283950617283951,"0.1 (4)
0.3 (4)
0.5 (4)
1.0 (4)
No clipping (4)"
REFERENCES,0.7301587301587301,"0
250000 500000 750000 10000001250000150000017500002000000 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.7319223985890653,test_battle_won_mean
REFERENCES,0.7336860670194003,10m_vs_11m
REFERENCES,0.7354497354497355,"0.1 (4)
0.3 (4)
0.5 (4)
1.0 (4)
No clipping (4)"
REFERENCES,0.7372134038800705,"0
250000 500000 750000 10000001250000150000017500002000000 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.7389770723104057,test_battle_won_mean
REFERENCES,0.7407407407407407,27m_vs_30m
REFERENCES,0.7425044091710759,"0.1 (3)
0.3 (3)
0.5 (3)
1.0 (3)
No clipping (3)"
REFERENCES,0.7442680776014109,"0
250000 500000 750000 10000001250000150000017500002000000 4 6 8 10 12 14 16 18"
REFERENCES,0.746031746031746,test_return_mean 3s5z
REFERENCES,0.7477954144620811,"0.1 (4)
0.3 (4)
0.5 (4)
1.0 (4)
No clipping (4)"
REFERENCES,0.7495590828924162,"0
250000 500000 750000 10000001250000150000017500002000000
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
REFERENCES,0.7513227513227513,test_return_mean
REFERENCES,0.7530864197530864,10m_vs_11m
REFERENCES,0.7548500881834215,"0.1 (4)
0.3 (4)
0.5 (4)
1.0 (4)
No clipping (4)"
REFERENCES,0.7566137566137566,"0
250000 500000 750000 10000001250000150000017500002000000 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
REFERENCES,0.7583774250440917,test_return_mean
REFERENCES,0.7601410934744268,27m_vs_30m
REFERENCES,0.7619047619047619,"0.1 (3)
0.3 (3)
0.5 (3)
1.0 (3)
No clipping (3)"
REFERENCES,0.763668430335097,"0
250000 500000 750000 10000001250000150000017500002000000 0.4 0.6 0.8 1.0 1.2 1.4"
REFERENCES,0.7654320987654321,joint_approx_TV 3s5z
REFERENCES,0.7671957671957672,"0.1 (4)
0.3 (4)
0.5 (4)
1.0 (4)
No clipping (4)"
REFERENCES,0.7689594356261023,"0
250000 500000 750000 10000001250000150000017500002000000 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.7707231040564374,joint_approx_TV
REFERENCES,0.7724867724867724,10m_vs_11m
REFERENCES,0.7742504409171076,"0.1 (4)
0.3 (4)
0.5 (4)
1.0 (4)
No clipping (4)"
REFERENCES,0.7760141093474426,"0
250000 500000 750000 10000001250000150000017500002000000 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75"
REFERENCES,0.7777777777777778,joint_approx_TV
REFERENCES,0.7795414462081128,27m_vs_30m
REFERENCES,0.781305114638448,"0.1 (3)
0.3 (3)
0.5 (3)
1.0 (3)
No clipping (3)"
REFERENCES,0.783068783068783,"Figure 5: Empirical test battle win mean (ﬁrst row), test returns (second row) and trust region
estimates (third row) of MAPPO on maps with varied difﬁcult and numbers of agents"
REFERENCES,0.7848324514991182,Under review as a conference paper at ICLR 2022
REFERENCES,0.7865961199294532,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
REFERENCES,0.7883597883597884,Test Return Mean 2s3z
REFERENCES,0.7901234567901234,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.7918871252204586,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
REFERENCES,0.7936507936507936,Test Return Mean 3s5z
REFERENCES,0.7954144620811288,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.7971781305114638,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
REFERENCES,0.798941798941799,Test Return Mean
REFERENCES,0.800705467372134,10m_vs_11m
REFERENCES,0.8024691358024691,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8042328042328042,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
REFERENCES,0.8059964726631393,Test Return Mean
REFERENCES,0.8077601410934744,27m_vs_30m
REFERENCES,0.8095238095238095,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8112874779541446,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.1 0.2 0.3 0.4"
REFERENCES,0.8130511463844797,Joint Approx TV 2s3z
REFERENCES,0.8148148148148148,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8165784832451499,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40"
REFERENCES,0.818342151675485,Joint Approx TV 3s5z
REFERENCES,0.8201058201058201,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8218694885361552,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45"
REFERENCES,0.8236331569664903,Joint Approx TV
REFERENCES,0.8253968253968254,10m_vs_11m
REFERENCES,0.8271604938271605,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8289241622574955,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.8306878306878307,Joint Approx TV
REFERENCES,0.8324514991181657,27m_vs_30m
REFERENCES,0.8342151675485009,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8359788359788359,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8377425044091711,Test Battle Win Mean 2s3z
REFERENCES,0.8395061728395061,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8412698412698413,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.8430335097001763,Test Battle Win Mean 3s5z
REFERENCES,0.8447971781305115,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8465608465608465,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.8483245149911817,Test Battle Win Mean
REFERENCES,0.8500881834215167,10m_vs_11m
REFERENCES,0.8518518518518519,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8536155202821869,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.855379188712522,Test Battle Win Mean
REFERENCES,0.8571428571428571,27m_vs_30m
REFERENCES,0.8589065255731922,"0.01 (5)
0.03 (5)
0.05 (5)
0.08 (5)"
REFERENCES,0.8606701940035273,"Figure 6: Empirical returns, trust region estimates and test battle win rate for small values of
independent ratios clipping."
REFERENCES,0.8624338624338624,Under review as a conference paper at ICLR 2022
REFERENCES,0.8641975308641975,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8659611992945326,Joint Approx TV
REFERENCES,0.8677248677248677,10m_vs_11m
REFERENCES,0.8694885361552028,"independent (4)
joint (4)"
REFERENCES,0.8712522045855379,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.873015873015873,Joint Approx TV
REFERENCES,0.8747795414462081,27m_vs_30m
REFERENCES,0.8765432098765432,"independent (4)
joint (4)"
REFERENCES,0.8783068783068783,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
REFERENCES,0.8800705467372134,Test Return Mean
REFERENCES,0.8818342151675485,10m_vs_11m
REFERENCES,0.8835978835978836,"independent (4)
joint (4)"
REFERENCES,0.8853615520282186,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
REFERENCES,0.8871252204585538,Test Return Mean
REFERENCES,0.8888888888888888,27m_vs_30m
REFERENCES,0.890652557319224,"independent (4)
joint (4)"
REFERENCES,0.892416225749559,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
REFERENCES,0.8941798941798942,Joint Approx TV
REFERENCES,0.8959435626102292,10m_vs_11m
REFERENCES,0.8977072310405644,"independent (4)
joint (4)"
REFERENCES,0.8994708994708994,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
REFERENCES,0.9012345679012346,Joint Approx TV
REFERENCES,0.9029982363315696,27m_vs_30m
REFERENCES,0.9047619047619048,"independent (4)
joint (4)"
REFERENCES,0.9065255731922398,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2 4 6 8 10 12 14 16 18"
REFERENCES,0.908289241622575,Test Return Mean
REFERENCES,0.91005291005291,10m_vs_11m
REFERENCES,0.9118165784832452,"independent (4)
joint (4)"
REFERENCES,0.9135802469135802,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2 4 6 8 10 12 14 16 18"
REFERENCES,0.9153439153439153,Test Return Mean
REFERENCES,0.9171075837742504,27m_vs_30m
REFERENCES,0.9188712522045855,"independent (4)
joint (4)"
REFERENCES,0.9206349206349206,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.9223985890652557,Joint Approx TV
REFERENCES,0.9241622574955908,10m_vs_11m
REFERENCES,0.9259259259259259,"independent (4)
joint (4)"
REFERENCES,0.927689594356261,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.9294532627865961,Joint Approx TV
REFERENCES,0.9312169312169312,27m_vs_30m
REFERENCES,0.9329805996472663,"independent (3)
joint (3)"
REFERENCES,0.9347442680776014,(a) Joint TV divergence
REFERENCES,0.9365079365079365,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2 4 6 8 10 12 14 16"
REFERENCES,0.9382716049382716,Test Return Mean
REFERENCES,0.9400352733686067,10m_vs_11m
REFERENCES,0.9417989417989417,"independent (4)
joint (4)"
REFERENCES,0.9435626102292769,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 2 4 6 8 10 12 14 16 18"
REFERENCES,0.9453262786596119,Test Return Mean
REFERENCES,0.9470899470899471,27m_vs_30m
REFERENCES,0.9488536155202821,"independent (3)
joint (3)"
REFERENCES,0.9506172839506173,(b) Empirical returns
REFERENCES,0.9523809523809523,"Figure 7: Joint divergence estimates and empirical returns for two types of ratio clipping at different
clipping values: 0.1 (ﬁrst row), 0.3 (ﬁrst row) and 0.5 (ﬁrst row)."
REFERENCES,0.9541446208112875,Under review as a conference paper at ICLR 2022
REFERENCES,0.9559082892416225,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.9576719576719577,Test Battle Win Mean
REFERENCES,0.9594356261022927,10m_vs_11m
REFERENCES,0.9611992945326279,"independent (4)
joint (4)"
REFERENCES,0.9629629629629629,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
REFERENCES,0.9647266313932981,Test Battle Win Mean
REFERENCES,0.9664902998236331,27m_vs_30m
REFERENCES,0.9682539682539683,"independent (4)
joint (4)"
REFERENCES,0.9700176366843033,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.9717813051146384,Test Battle Win Mean
REFERENCES,0.9735449735449735,10m_vs_11m
REFERENCES,0.9753086419753086,"independent (4)
joint (4)"
REFERENCES,0.9770723104056437,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.9788359788359788,Test Battle Win Mean
REFERENCES,0.9805996472663139,27m_vs_30m
REFERENCES,0.982363315696649,"independent (4)
joint (4)"
REFERENCES,0.9841269841269841,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.9858906525573192,Test Battle Win Mean
REFERENCES,0.9876543209876543,10m_vs_11m
REFERENCES,0.9894179894179894,"independent (4)
joint (4)"
REFERENCES,0.9911816578483245,"0
250
500
750
1000
1250
1500
1750
2000
Number of timesteps (K) 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.9929453262786596,Test Battle Win Mean
REFERENCES,0.9947089947089947,27m_vs_30m
REFERENCES,0.9964726631393298,"independent (3)
joint (3)"
REFERENCES,0.9982363315696648,"Figure 8: Test battle win rate for two types of ratio clipping at different clipping values: 0.1 (ﬁrst
row), 0.3 (ﬁrst row) and 0.5 (ﬁrst row)."
