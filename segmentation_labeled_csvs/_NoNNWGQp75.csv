Section,Section Appearance Order,Paragraph
ABSTRACT,0.019230769230769232,"Abstract—Continuous learning has become a famous method
by retraining light-weighted experts with sampled video frames
from each video stream to cope with the data drift in real
time video analysis area. However, retraining models consumes a
considerable amount of computational resources, affecting video
inference and leading to a decline in inference accuracy and
throughput. Given that video streams often display temporal or
spatial consistency (for instance, vehicles on the same route will
pass through the same video scenes; similar video scenes will be
brought about by the same lighting and weather in different
places), reusing previous expert models on different cameras
holds potential.To validate the potential of reusing models, I
carried out a comparative experiment between reusing and
retraining models on the Cityscapes dataset. It was observed that
as the number of cameras increased progressively, model reuse
brought about an improvement of around 10 mAP compared to
model retraining.
Index Terms—video analysis, edge compute, continuous learn-
ing, model reuse"
INTRODUCTION,0.038461538461538464,I. INTRODUCTION
INTRODUCTION,0.057692307692307696,"Deep neural network (DNN)-based image analysis technolo-
gy has broad application prospects in corporate security, retail,
trafﬁc management, transportation, and other ﬁelds. In these
real applications, the system needs to be deployed in edge
devices to carry out analysis work directly, such as using local
edge servers, to provide real-time results [1]. However, edge
computing resources cannot support the continuous growth of
video analysis workload, DNN models, and video streams [2],
[3]. For applications running in resource-rich environments,
such as shared clouds, despite recent progress in DNN re-
source utilization efﬁciency, the cost of running video analysis
is still high. For example, the high-end NVIDIA V100 GPU
can only support the YOLOv5-L [4] at a speed of 30 FPS
while processing two-channel video streams, and the cost on
shared clouds is as high as $1100/month/stream [5].
The resources provided by edge computing are limited. The
inconsistency between the growth rate of model computation
requirements and processor computation cycles further exacer-
bates this boundary. Therefore, edge deployment is dependent
on model compression. The compression of DNNs was ini-
tially trained for representative data in each video stream, but
when used in the ﬁeld, DNNs are affected by data drift, that
is, there is a signiﬁcant difference between on-site video data
and the data used in training. Over time, cameras installed
on streets and intelligent cars will see a variety of scenes"
INTRODUCTION,0.07692307692307693,"with different lighting, different crowd densities, and different
object combinations. Even small changes in the scene can
affect the accuracy of inference, but it is difﬁcult to reﬂect all
changes during training. Therefore, data drift greatly reduces
the accuracy of edge DNNs. In fact, compressed DNNs have
fewer weight values and shallow structures, which are not
suitable for providing highly accurate predictions when data
changes greatly. Such expert DNNs require continuous retrain-
ing to maintain high inference accuracy under the limitation
of the number of object appearances and scenes that can be
learned in their compressed structure. Recent research in the
ﬁeld of computer vision and systems [6]–[8] shows that this
method is effective in edge video analysis, providing high
resource efﬁciency and result accuracy.
Model training is much more expensive than model infer-
ence, and these systems [7], [8] need to spend a total of 70%-
90% of computational resources to retrain their expert models,
which makes adaptive services become a key bottleneck for
the resource efﬁciency and accuracy consistency of the video
analysis system. In addition, knowledge distillation requires
large-teacher models to annotate sample frames, which also
occupy computational resources. In order to solve data drift
in a resource-limited environment, it is necessary to reduce
the system’s need for retraining.
Research indicates that video streams often exhibit cyclical
patterns in time and space [9], [10], that is, video scenes with
similar environmental factors such as lighting (morning or
evening), weather (clear or rainy), and location (drones revisit
the same street) recur on the same camera. Importantly, video
scenes from one camera may also occur on other cameras,
especially those in geographically adjacent locations, such as
an autonomous car passing through places where other cars
on the same route have passed. These temporal and spatial
correlations mean that some expert models that were trained
on video scenes in the past can perform reasonably on current
video scenes. Therefore, these historical expert models can be
utilized to minimize the need for retraining."
LIT REVIEW,0.09615384615384616,II. RELATED WORKS
LIT REVIEW,0.11538461538461539,A. Continuous Learning for video analysis
LIT REVIEW,0.1346153846153846,"general-purpose DNNs are often too expensive to be con-
tinuously used for video analysis. A common approach is
to deploy specialized and compressed DNNs (or ”expert”
models) that are trained using knowledge from general and
more costly DNNs (or ”teacher” models). The idea is to utilize
knowledge distillation [12] to transfer knowledge from a large
teacher model to a smaller expert model tailored for speciﬁc
video clips or streams. On matching video clips, expert models
can save orders of magnitude of computational resources while
achieving model precision similar to the large teacher models.
Given that expert models are only capable of identifying a
conﬁned assortment of object looks and video setups, their per-
formance isn’t optimal on dynamic live videos, which undergo
unavoidable changes in objects and scenes over time such
as various positioning, light conditions, and object categories
[8]. An encouraging technique for utilizing expert models in
dynamic real-time videos would involve recurrently retraining
the expert model utilizing the most recent video frames. The
most recent research [6]–[8], [13] has made it clear that
the perpetual retraining and application of more compact,
specialized models can yield both high precision and resource
sustainability when dealing with constantly changing video
content. It’s also worth noting that this method of constant
retraining has proven to be more effective than using a larger,
instructor-based model employed on only a select amount of
frames and then extrapolating the labels (such as via optical
ﬂow tracking techniques) [7].
Fig. 1 serves as an illustration of the high-level components
of a video analysis system capable of continuous retraining and
deployment of expert models. The major components include:
(1) camera service: It periodically sends new sample video
frames to the adaptation service. (2) adaptation service: It
ﬁne-tunes the camera’s expert model (a copy) using recent
sample frames, emulating the larger teacher model in the
current scene, then sends the updated expert model to the infer-
ence service. And (3) inference service: It uses the received
lightweight expert model to perform real-time inference on
video frames from the camera service.
Retraining expert models demands signiﬁcant computational
resources and time, resulting in the adaptation service being
imperative for resource efﬁciency and consistency in accuracy,
thus serving as a key bottleneck. These systems [7], [8] require
a total of 70%-90% of their computational resources to re-
train their expert models, due to the much higher cost of
model training compared to model inference. Additionally,
knowledge distillation requires the operation of a large-scale
teacher model to generate data labels on sample frames.
Therefore, as a response to this fundamental challenge, an
efﬁcient method is needed to minimize the necessity for
extensive expert retraining."
LIT REVIEW,0.15384615384615385,B. Optimization of Video Analysis Systems
LIT REVIEW,0.17307692307692307,"Optimization of video analysis systems is a dynamic ﬁeld
that aims to enhance effectiveness and efﬁciency in the pro-
cessing, interpretation, and use of data derived from video
sources. Ibrahimet et al. [14] focuses on how to improve"
LIT REVIEW,0.19230769230769232,Fig. 1. Architecture of a continuous learning video analysis system.
LIT REVIEW,0.21153846153846154,"the end-to-end performance of video analysis systems by
optimizing video analytics systems’ performance, including
neural network-based methods. Hua et al. [15] revolves around
the application of video analysis for the optimization of sports
techniques, in this case, running. Arun et al. [16] addresses
the reduction of transmission time and the increase in video
resolution in IP-based video analytical systems. The focus of
Hua et al. [17] is on automating home video editing using a
set of video and music analysis algorithms to produce near-
professional results.
In order to maintain high inference accuracy in environ-
ments demanding low resource utilization and speedy respons-
es, video analysis systems have explored various methods,
including model distillation [12], model architecture pruning,
adaptive conﬁguration, and frame selection.Model distillation
entails creating a lightweight model that is small, fast, and
adequately accurate for speciﬁc scenes. The challenge with
this approach is that changes in the scene can lead to drops in
the model’s accuracy; therefore, the system has to create new
expert models to adapt to new scenes. Existing solutions rely
on the following two methods: Model retraining techniques:
retraining on the latest video frames [6]–[8] or the most
relevant images in the training set [13]; Model selection
techniques: selecting models from a collection of historical
models [10] or cascading models with increased capacity [13]."
LIT REVIEW,0.23076923076923078,C. Model Selection under Data Drift
LIT REVIEW,0.25,"The Mixture of Experts (MoE) is a form of ensemble
learning method that models complex input-output relationship
by strategically combining the decisions of multiple ”experts”
or simpler models, with each being highly proﬁcient at a
certain subset of the data space. The model dynamically
determines which expert to rely upon by computing a weighted
combination, where the weights are typically found using
a ”gating” function that quantiﬁes the expert’s reliability in
regard to a speciﬁc input. An indicative study on MoE is
[19], where the concept was applied to solve classiﬁcation
problems. The model’s broad range applicability and efﬁcacy
can be appreciated in diverse studies such as [20] wherein the
researchers embedded MoE into a deep learning framework to
scale network capacity.
After Shazeer et al. [20] found that MoE can signiﬁcantly
reduce the computational cost of DNN, MoE has attracted
widespread attention. Recent works have achieved precision
equivalent to advanced models requiring extensive computa-
tional resources with few computational resources [21].
To manage MoEs gradually comprising new models, the
video analysis system must retrain the gating network or model
selector. To avoid retraining the model selector (which would
consume a signiﬁcant amount of computational resources and
time), recent work [10] uses autoencoders to project input
video frames to latent space and map new models to a region
in the latent space. Models are selected by looking for the
model region to which the new video frames belong in the
latent space, so no retraining of the selector is needed when
adding new models. However, the training of the autoencoder
is to learn the distribution of only the input data, not to simply
learn which frames can share a good expert model. The former
task is too general, making it difﬁcult to learn an efﬁcient
encoder for deployment in practice.
Another approach to model selection [10], [22] is to map
video content into an embedding space (via an autoencoder),
partition the embedding space, and map each partition to a
speciﬁc expert model. This approach does poorly in practice
[18]. The intuitive reason is that training an autoencoder is
to learn the distribution only of input data (e.g., which video
frames look similar) rather than simply learning which frames
can share a good expert model. The former task is too generic,
thus, learning an efﬁcient autoencoder and deploying it in
practice is very challenging."
LIT REVIEW,0.2692307692307692,D. Resource Allocation for DNNs
LIT REVIEW,0.28846153846153844,"Resource Allocation for DNNs centers around efﬁciently
managing computational resources to optimize the perfor-
mance and efﬁcacy of DNNs. Managing these resources
involves allocating and distributing elements such as power,
networking capacity, processing ability, and memory storage
across different parts of the network. This has implications
not only in improving the speed of computations, but also in
ensuring the reliability and robustness of DNNs.
Taking reference from multiple scholarly works, we see dif-
ferent angles approached for resource allocation in DNNs:One
study by Sun et al., 2017 showcases the application of DNNs"
LIT REVIEW,0.3076923076923077,"in wireless resource management, aimed at optimizing the
non-linear mapping between input and output of resource
allocation. Zhou et al., 2018 propose a resource allocation
strategy based on DNNs speciﬁcally for cognitive radio
networks, showcasing a speciﬁc implementation context of
resource allocation. Yang et al., 2019 explored the role of
DNNs for resource management in Non-Orthogonal Multiple
Access (NOMA) networks, illustrating the deep relationships
between resource allocation and network architecture. Gao
et al., 2019 investigated power allocation considerations for
DNNs, highlighting the role of resource allocation in calibrat-
ing performance.
Resource sharing for DNN-related tasks has been widely
studied in the systems literature. This includes sharing of
GPU and network resources among multiple concurrent DNN
training tasks, inference tasks for video analysis, as well as
resource sharing between inference and training tasks [8]. The
common challenge these settings face is predicting how much
each task’s precision can be improved given the same amount
of computational/network resources. Prediction methods can
be categorized as ofﬂine, periodic, or through reusing compu-
tational data."
IMPLEMENTATION/METHODS,0.3269230769230769,III. IMPLEMENTATION
IMPLEMENTATION/METHODS,0.34615384615384615,A. Datasets
IMPLEMENTATION/METHODS,0.36538461538461536,"I evaluate these two methods on object detection using
Cityscapes dataset [1]. The Cityscapes dataset is a com-
prehensive library focusing on the semantic understanding
of urban street scenes. It contains high-quality, pixel-level
annotations of 5,000 images from 50 different European cities,
with more than 30 categories being marked and segmented per
scene. The dataset supports advanced machine learning tasks
including, but not limited to, semantic segmentation, instance
segmentation, and object detection. It serves as a pivotal asset
in the development of autonomous driving technology, as it
facilitates the understanding and identiﬁcation of diverse urban
elements like pedestrians, vehicles, and road markings."
IMPLEMENTATION/METHODS,0.38461538461538464,B. Models
IMPLEMENTATION/METHODS,0.40384615384615385,"In object detection, I used YoloX-Nano and YoloX-x [1] as
the expert and teacher models respectively. Both models were
pre-trained on the COCO [1] dataset. More details about the
models can be found in Table I."
IMPLEMENTATION/METHODS,0.4230769230769231,C. Metrics
IMPLEMENTATION/METHODS,0.4423076923076923,"TABLE I
MODEL SPECIFICATIONS"
IMPLEMENTATION/METHODS,0.46153846153846156,"Model
Speed V100 (ms)a
Params (M)
FLOPs (G)
YoloX-Nano
3.2
0.91
1.08
YoloX-x
17.3
99.1
281.9
atime required to process a single frame using NVIDIA V100."
IMPLEMENTATION/METHODS,0.4807692307692308,D. Setup
IMPLEMENTATION/METHODS,0.5,"In my experiment, since the Cityscapes dataset only pro-
vides sampled images, I divided every 10 images into a group,
deﬁned as a retraining window. Each retraining window has a
retraining time of 30 seconds. And also, model selection and
retraining happen in a adaption center (e.g., in the cloud or
an edge compute cluster, etc.), and edge devices only perform
inference tasks. Instead of simulating a real system, I saved
each expert used and conducted the tests all at once at the
end. Since the chosen expert (YoloX-Nano) can perform real-
time inference on edge devices with inferior computational
capabilities (like NVIDIA Jetson Nano) with an inference
throughput of up to 30FPS, this method can be used to
simulate actual results."
IMPLEMENTATION/METHODS,0.5192307692307693,E. Baselines
IMPLEMENTATION/METHODS,0.5384615384615384,I compared the performance of the following four methods:
IMPLEMENTATION/METHODS,0.5576923076923077,"• No Adaptation: train a single expert model based on all
training data, and deploy this expert on the test data."
IMPLEMENTATION/METHODS,0.5769230769230769,"• Continuous Retraining: periodically retrain an expert
model for each camera using the most recent video
segments. This can serve as a method for recent model
retraining systems, such as AMS [7] and Ekya [8]. The
retraining algorithm is shown in Algorithm 1. The early
stop time µ is a hyperparameter and here I set it to 1s."
IMPLEMENTATION/METHODS,0.5961538461538461,"• Ideal Model Reuse: Deploy the best expert model from
a collection of experts each trained using data from
different cities in the training set (ignoring the overhead
of model selection). This can be regarded as a strictly
better version of ODIN [10]."
IMPLEMENTATION/METHODS,0.6153846153846154,"• Ideal Reuse with Retraining: Combining 2 and 3 (retrain-
ing the expert model selected in 3). This shows how much
improvement an ideal model reuse scheme can bring in
a continuous retraining framework."
RESULTS/EXPERIMENTS,0.6346153846153846,IV. RESULTS
RESULTS/EXPERIMENTS,0.6538461538461539,A. Beneﬁts in Resource Efﬁciency
RESULTS/EXPERIMENTS,0.6730769230769231,"Fig. 2 shows the mean Average Precision (mAP) score
on the test data while varying the number of cameras. The
observations are two-fold.
To begin with, in the quest to minimize retraining, model
reuse emerges as a propitious approach. The merits of this
technique become notably apparent when computational re-
sources are insufﬁcient for the retraining of expert models
on more expansive camera networks (comprising 4, 6, and
8 cameras). Happily, even when computing resources are suf-
ﬁcient for retraining (as in the case with 2 cameras), the Ideal
Model Reuse approach can still match the mAP of Continuous"
RESULTS/EXPERIMENTS,0.6923076923076923,"Algorithm 1 Retraining Scheduler Algorithm
Input: retraining tasks set R, early stop time µ seconds,
retrain window T seconds"
RESULTS/EXPERIMENTS,0.7115384615384616,"1: for r in R do
2:
gain[r] ←+∞"
RESULTS/EXPERIMENTS,0.7307692307692307,"3: end for
4: while T > µ do"
RESULTS/EXPERIMENTS,0.75,"5:
r ←argmaxgain
6:
acc ←r.eval()"
RESULTS/EXPERIMENTS,0.7692307692307693,"7:
train request r for µ seconds
8:
T ←T −µ"
RESULTS/EXPERIMENTS,0.7884615384615384,"9:
gain[r] ←r.eval() −acc
10: end while"
RESULTS/EXPERIMENTS,0.8076923076923077,"Fig. 2.
The object detection accuracy of different designs under varying
numbers of cameras. (mAP)"
RESULTS/EXPERIMENTS,0.8269230769230769,"Retraining. This ﬁnding is heartening as it suggests that the use
of historical models circumvents the need for resources (not
demonstrated here) to train new expert models. Moreover, the
most effective historical model can achieve a level of accuracy
that is on a par with expert models trained on the latest video
data.
In addition, there exists an auspicious synergy between
model reuse and continuous retraining, as evidenced by the
Ideal Reuse with Retraining technique consistently achieving
the highest mAP. The rationale behind this success is that the
reused model offers a robust launchpad for retraining, thus
decreasing the computational resources required by the retrain-
ing process (due to quicker convergence). Simultaneously, it
heightens the precision of the evolved expert models’ inference
capabilities."
RESULTS/EXPERIMENTS,0.8461538461538461,B. Beneﬁts in accuracy consistency
RESULTS/EXPERIMENTS,0.8653846153846154,"Fig. 3.
The object detection accuracy of different designs under varying
numbers of cameras. (mAP)"
RESULTS/EXPERIMENTS,0.8846153846153846,"have the ability to rapidly select and alter the expert model,
sidestepping the delay associated with training a new expert
(as detailed in Fig. 3). For example, at the 1st percentile,
Ideal Model Reuse sustains a 24% mAP, whereas Continuous
Retraining dips to an unsatisfactory 7% mAP. Fig. 3 provides
a tangible illustration of this scenario. As the vehicle enters
the tunnel (id = 2), Ideal Model Reuse transitions to a suitable
expert much more swiftly (id = 5) compared to Continuous
Retraining (id = 7), resulting in a considerably lesser decline
in model accuracy."
CONCLUSION/DISCUSSION,0.9038461538461539,V. DISCUSSION
CONCLUSION/DISCUSSION,0.9230769230769231,"To maximize the beneﬁts of model reuse, several technical
obstacles must be overcome. The ideal scenario for Model
Reuse is to always select the optimal expert model without
any computation cost or delay when browsing the entire model
zoo, which is however, not feasible. Recent solutions for
model reuse in the database sector, such as ODIN [10], have
not yet addressed these issues, as they lack the design to
efﬁciently share computation resources among model selection
and retraining functions for numerous edge devices. In order
to realize the potential of model reuse in a practical sense, a
mechanism must be in place to swiftly and precisely ﬁnd the
best expert model. It’s also essential to control the cost and
latency of model selection to prevent them from escalating
indeﬁnitely with the proliferation of videos or cameras.
To conclude, leveraging historical expert models serves
as a beneﬁcial supplement to model retraining, and when
utilized together, it promotes better resource efﬁciency and
a more consistent, accurate model adaptation. Nevertheless,
to transform model reuse into a practical strategy, numerous
technical difﬁculties linger, which we will address in the
subsequent section."
CONCLUSION/DISCUSSION,0.9423076923076923,ACKNOWLEDGMENT
CONCLUSION/DISCUSSION,0.9615384615384616,"I am grateful to Ms. Nagasaki Soyo for bringing a touch of
joy to my dreary college life."
REFERENCES,0.9807692307692307,REFERENCES
