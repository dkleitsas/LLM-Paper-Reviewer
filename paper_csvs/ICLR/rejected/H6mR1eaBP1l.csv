Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.009615384615384616,"Sequence labeling task (part-of-speech tagging, named entity recognition) is one
of the most common in NLP. At different times, the following architectures were
used to solve it: CRF, BiLSTM, BERT (in chronological order). The combined
model BiLSTM / BERT + CRF, where the last one is the topmost layer, however,
performs better than just BiLSTM / BERT. It is common when there is a small
amount of labeled data available for the task. Hence it is difﬁcult to train a model
with good generalizing capability, so one has to resort to semi-supervised learning
approaches. One of them is called pseudo-labeling, the gist of what is increasing
the training samples with unlabeled data, but it cannot be used alongside with
the CRF layer, as this layer simulates the probability distribution of the entire
sequence, not of individual tokens. In this paper, we propose an alternative to the
CRF layer — the Prior Knowledge Layer, that allows one to obtain probability
distributions of each token and also takes into account prior knowledge concerned
the structure of label sequences."
INTRODUCTION,0.019230769230769232,"1
INTRODUCTION"
INTRODUCTION,0.028846153846153848,"Sequence labeling, along with the text classiﬁcation, is one of the most common in natural language
processing (NLP). In general, the task is to match each token — word or sub-word in a sentence
— with a corresponding label. Particular cases of this problem are part-of-speech tagging (POS
tagging) and named entity recognition (NER)."
INTRODUCTION,0.038461538461538464,"Before the rise in popularity of deep learning methods, the sequence labeling problem was solved
using graph probabilistic models, among which the Conditional Random Fields (CRF) (Lafferty
et al., 2001) architecture showed the best quality. Now, to tackle this problem, both Recurrent
Neural Networks (RNN) (Rumelhart et al., 1986) (Karpathy, 2015), for example Bidirectional Long
Short-Term Memory (BiLSTM) (Hochreiter & Schmidhuber, 1997) (Schuster & Paliwal, 1997), and
neural networks based on the Transformer (Vaswani et al., 2017) architecture, such as Bidirectional
Encoder Representations from Transformers (BERT) (Devlin et al., 2018) were used."
INTRODUCTION,0.04807692307692308,"The combined model BiLSTM / BERT + CRF (Huang et al., 2015) (Lample et al., 2016), where the
last one is the topmost layer, performs better than just BiLSTM / BERT, demonstrating quality close
to state-of-the-art. This fact could be explained that CRF layer models a joint probability distribution
over the entire sequence, which allows one to take into account the structure of the label sequences,
for example, the presence of forbidden subsequences of labels."
INTRODUCTION,0.057692307692307696,"It is common when there is a small amount of labeled data, and therefore it becomes challenging to
train a neural network with good generalizing capability and one has to resort to semi-supervised
learning approaches. One of them is called pseudo-labeling (Lee, 2013), the gist of what is increas-
ing the training samples with unlabeled data. To implement this approach, it is required that the
model returns a probability distribution over each token in the sequence, but when using CRF, this
cannot be done, since this layer models the probability distribution over the entire sequence."
INTRODUCTION,0.0673076923076923,"To achieve the desired result, BiLSTM / BERT model without the CRF layer could be used, but the
structure of the label sequences will not be taken into account, which leads to worse quality of the
model."
INTRODUCTION,0.07692307692307693,"In this paper, we propose an alternative to the CRF layer — the Prior Knowledge Layer (PKL),
that allows one to obtain probability distributions of each token through a baseline model such as"
INTRODUCTION,0.08653846153846154,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.09615384615384616,"BiLSTM / BERT, and also takes into account prior knowledge of the structure of the label sequences,
which makes it possible to use this architecture together with pseudo-labeling approach."
BACKGROUND,0.10576923076923077,"2
BACKGROUND"
SEQUENCE LABELING MODELS,0.11538461538461539,"2.1
SEQUENCE LABELING MODELS"
SEQUENCE LABELING MODELS,0.125,"The problem of sequence labeling is widely known and there are many ways to approach it. One
of the ﬁrst models used to solve this is the Hidden Markov Model (HMM), which has its origins in
Leonard Baum’s 1966 work (Baum & Petrie, 1966) (Rabiner & Juang, 1986). This model has found
its application not only in sequence labeling, but also in speech recognition and analysis of biological
sequences, in particular DNA. This model was replaced with the Maximum-Entropy Markov Model
(MEMM, 2000) (McCallum et al., 2000), which combines the features of the HMM and Maximum
Entropy models. A breakthrough was marked with the appearance of the Conditional Random
Fields (CRF, 2001) model, which at that time demonstrated state-of-the-art quality. There are also
modiﬁcations and generalizations of CRF, such as CRF with partial training (Mann & McCallum,
2007), Hidden-state CRF (HCRF) (Quattoni et al., 2007), Dynamic CRF (DCRF) (Sutton et al.,
2007) and Continuous CRF (CCRF) (Qin et al., 2008)."
SEQUENCE LABELING MODELS,0.1346153846153846,"The disadvantage of the aforementioned model is the need for manual feature generation and domain
knowledge to train a model with good generalizing capability. This problem was solved by deep
neural networks, which do not require manual generation of features, and also show better results,
provided that there is a sufﬁcient amount of labeled data."
SEQUENCE LABELING MODELS,0.14423076923076922,"Based on the sequence labeling problem structure, which implies a sequence at the input and a
sequence of the same length at the output, the architecture of the Recurrent Neural Network is
well-suited for its solution. RNNs are known to have a vanishing gradient problem, so the other
architectures such as Long Short-Term Memory (LSTM, 1997) (Hochreiter & Schmidhuber, 1997)
and Gated Recurrent Unit (GRU, 2004) (Cho et al., 2014) (Chung et al., 2014) have been invented
to solve this problem."
SEQUENCE LABELING MODELS,0.15384615384615385,"Sequence labeling task allows reading the entire input sequence to make a decision, that makes it
possible to use bidirectional architectures, particularly Bidirectional RNN (BiRNN) (Schuster &
Paliwal, 1997), to process the input sequence in two directions from left to right and from right to
left for better performance. But there is an important point, that such architectures are not ”true”
bidirectional, but rather two unidirectional ones."
SEQUENCE LABELING MODELS,0.16346153846153846,"Thus, we come to one of the most commonly used models for solving the problem of sequence
labeling, namely Bidirectional Long Short-Term Memory (BiLSTM) (Hochreiter & Schmidhuber,
1997) (Schuster & Paliwal, 1997)."
SEQUENCE LABELING MODELS,0.17307692307692307,"In 2017, as the article ”Attention Is All You Need” (Vaswani et al., 2017) was published, in which
the authors presented a completely new architecture called Transformer. This architecture surpassed
all previous state-of-the-art models in the task of machine translation. The authors of the article
continued the ideas of the seq2seq (Cho et al., 2014) (Sutskever et al., 2014) models and the attention
mechanism (Bahdanau et al., 2014) introducing two new operations: Scaled Dot-Product Attention
and Multi-Head Attention. These operations allow to consider information about all tokens in the
sentence when processing a speciﬁc token, which makes them ”truly” bidirectional."
SEQUENCE LABELING MODELS,0.18269230769230768,"Transformer is an encoder-decoder model, and therefore cannot be used for sequence labeling. But
it was just a matter of time before the bidirectional attention mechanism and Transformer architec-
ture was adapted in other NLP tasks, and in 2018 the Bidirectional Encoder Representations from
Transformers (BERT) (Devlin et al., 2018) model appeared. Architecturally, BERT is a Transformer
encoder, and this model can be used to solve a wide range of NLP tasks, including sequence labeling."
SEQUENCE LABELING MODELS,0.19230769230769232,"The next architectural improvement was to combine the baseline model like BiLSTM / BERT with
CRF (Huang et al., 2015) (Lample et al., 2016), where the last acts as the topmost layer and models
the joint probability distribution over the entire sequence. In this approach, the need for manual
generation of features becomes irrelevant, since the logits of the baseline model, which are used as
features, enter the CRF. BERT + CRF model shows quality close to state-of-the-art."
SEQUENCE LABELING MODELS,0.20192307692307693,Under review as a conference paper at ICLR 2022
PSEUDO-LABELING,0.21153846153846154,"2.2
PSEUDO-LABELING"
PSEUDO-LABELING,0.22115384615384615,"Pseudo-labeling is a simple and effective semi-supervised method, that was originally invented for
deep neural networks. The core of it is that the model is jointly trained using both labeled and
unlabeled data. First, model trained on labeled data, and then used for marking unlabeled data —
taking label with the maximum predicted probability given by the model for each data sample. Then
these labels used as true labels for ﬁtting the model on all data (Lee, 2013)."
NAMED ENTITY RECOGNITION TAGGING,0.23076923076923078,"2.3
NAMED ENTITY RECOGNITION TAGGING"
NAMED ENTITY RECOGNITION TAGGING,0.2403846153846154,"Named Entity Recognition task consists in determining and extracting subsequences of words, called
spans, related to one named entity. In order to classify entire spans as named entities, special
markups (Carpenter, 2009) are used. The most popular markups are BIO (Ramshaw & Marcus,
1995) and BILUO (BIOES, BMEWO) (Borthwick, 1999)."
NAMED ENTITY RECOGNITION TAGGING,0.25,"Name BIO stands for Beginning-Inside–Outside. Within this markup, there are tags such as named
entity names and O (Outside), followed by one of the two preﬁxes B- (Beginning), I- (Inside). The B-
preﬁx corresponds to the beginning of the named entity, the I- preﬁx corresponds to the continuation
of the named entity, in case if the named entity consists of more than one token."
NAMED ENTITY RECOGNITION TAGGING,0.25961538461538464,"BILUO markup extends BIO with two additional preﬁxes L- (Last), U- (Unit), which denote the
end of a named entity, in case the named entity consists of more than one token, and a single named
entity, consisting of one token, respectively."
NAMED ENTITY RECOGNITION TAGGING,0.2692307692307692,"Note that after applying one of these markups, the number of labels becomes greater than the number
of named entities. Let the number of named entities be N, then:"
NAMED ENTITY RECOGNITION TAGGING,0.27884615384615385,"• the number of labels in the BIO markup is 2N + 1
• the number of labels in the BILUO markup is 4N + 1"
NAMED ENTITY RECOGNITION TAGGING,0.28846153846153844,"Despite the increase in the number of labels, not every label can follow arbitrary other label. Let’s
denote the named entity by X.
BIO markup:"
NAMED ENTITY RECOGNITION TAGGING,0.2980769230769231,• the label I-X can only follow the label B-X
NAMED ENTITY RECOGNITION TAGGING,0.3076923076923077,BILUO markup:
NAMED ENTITY RECOGNITION TAGGING,0.3173076923076923,"• the labels I-X and L-X can only follow the label B-X
• label O can only follow label O, L-X or U-X"
NAMED ENTITY RECOGNITION TAGGING,0.3269230769230769,"In other words, there are pairs of consecutive labels that are not correct and contradict the rules
described above."
NAMED ENTITY RECOGNITION TAGGING,0.33653846153846156,"It is important to note that the structure of label sequences can be in any sequence labeling task, but
this structure is most indicative in NER, namely BIO / BILUO markups."
METHOD,0.34615384615384615,"3
METHOD"
PROBLEM STATEMENT,0.3557692307692308,"3.1
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.36538461538461536,"Let D be the set of labeled sequences (x, y), where:"
PROBLEM STATEMENT,0.375,"• x = (x1, ..., xl) - sequence of tokens from X
• y = (y1, ..., yl) is a sequence of labels from Y"
PROBLEM STATEMENT,0.38461538461538464,"In the sequence (x, y) ∈D, the token xi corresponds to the label yi."
PROBLEM STATEMENT,0.3942307692307692,"Consider an arbitrary probabilistic model with parameters θ: pθ(y|x)
The Maximum Likelihood principle for solving the sequence labeling problem looks as follows:
X"
PROBLEM STATEMENT,0.40384615384615385,"(x,y)∈D
ln pθ(y|x) →max
θ"
PROBLEM STATEMENT,0.41346153846153844,Under review as a conference paper at ICLR 2022
PROBLEM STATEMENT,0.4230769230769231,Optimal sequence of labels with known parameters θ:
PROBLEM STATEMENT,0.4326923076923077,"ˆy = arg max
y∈Yl
pθ(y|x)"
PRIOR KNOWLEDGE MATRIX,0.4423076923076923,"3.2
PRIOR KNOWLEDGE MATRIX"
PRIOR KNOWLEDGE MATRIX,0.4519230769230769,"For a convenient representation of correct / incorrect pairs of consecutive labels, it is convenient
to use a matrix, which we call the Prior Knowledge Matrix (PKM). The rows of the matrix will
correspond to the labels at the i position, and the columns will correspond to the labels at the i + 1
position. Let 1 be an incorrect pair of labels, and 0 a correct one."
PRIOR KNOWLEDGE MATRIX,0.46153846153846156,"This approach is generalized to correct / incorrect sequences of labels of length more than two
by transition from matrices to multidimensional arrays (tensors) of dimension, corresponding to
the length of the largest forbidden sequence. The number of incorrect label sequences increases
exponentially with the length of the label sequence, while the number of correct sequences increases
much more slowly. This leads to the fact that the resulting multidimensional array is very sparse."
PRIOR KNOWLEDGE MATRIX FOR NER,0.47115384615384615,"3.3
PRIOR KNOWLEDGE MATRIX FOR NER"
PRIOR KNOWLEDGE MATRIX FOR NER,0.4807692307692308,Consider the NER task with two named entities:
PRIOR KNOWLEDGE MATRIX FOR NER,0.49038461538461536,"• person (PER)
• location (LOC)"
PRIOR KNOWLEDGE MATRIX FOR NER,0.5,Let’s build a PKM for both BIO and BILUO markups.
PRIOR KNOWLEDGE MATRIX FOR NER,0.5096153846153846,PKM row/column names for:
PRIOR KNOWLEDGE MATRIX FOR NER,0.5192307692307693,"• BIO: O, B-PER, B-LOC, I-PER, I-LOC
• BILUO: O, B-PER, B-LOC, I-PER, I-LOC, L-PER, L-LOC, U-PER, U-LOC "
PRIOR KNOWLEDGE MATRIX FOR NER,0.5288461538461539,"


"
PRIOR KNOWLEDGE MATRIX FOR NER,0.5384615384615384,"0
0
0
1
1
0
0
0
0
1
0
0
0
1
0
0
0
0
0
1
0
0
0
1
0 "
PRIOR KNOWLEDGE MATRIX FOR NER,0.5480769230769231,"


"
PRIOR KNOWLEDGE MATRIX FOR NER,0.5576923076923077,Figure 1: Prior Knowledge Matrix for BIO tagging 
PRIOR KNOWLEDGE MATRIX FOR NER,0.5673076923076923,"









"
PRIOR KNOWLEDGE MATRIX FOR NER,0.5769230769230769,"0
0
0
1
1
1
1
0
0
1
0
0
0
1
0
1
1
1
1
0
0
1
0
1
0
1
1
1
1
1
0
1
0
1
1
1
1
1
1
1
0
1
0
1
1
0
0
0
1
1
1
1
0
0
0
0
0
1
1
1
1
0
0
0
0
0
1
1
1
1
0
0
0
0
0
1
1
1
1
0
0 "
PRIOR KNOWLEDGE MATRIX FOR NER,0.5865384615384616,"









"
PRIOR KNOWLEDGE MATRIX FOR NER,0.5961538461538461,Figure 2: Prior Knowledge Matrix for BILUO tagging
PRIOR KNOWLEDGE LAYER,0.6057692307692307,"3.4
PRIOR KNOWLEDGE LAYER"
PRIOR KNOWLEDGE LAYER,0.6153846153846154,"Let’s take a look at how the Prior Knowledge Layer (PKL) works. Since this layer takes into account
knowledge about incorrect label subsequences, this layer initializes with Prior Knowledge Matrix."
PRIOR KNOWLEDGE LAYER,0.625,Under review as a conference paper at ICLR 2022
PRIOR KNOWLEDGE LAYER,0.6346153846153846,"PKL has no trainable parameters. It receives as input the logits of each token obtained by the
baseline BiLSTM / BERT model and returns a scalar loss α — PKL loss — that acts as an weighted
additive term in the minimized loss function. Let’s convert the Maximum Likelihood principle to
minimization optimization problem multiplying by -1 and add scalar additive term: −
X"
PRIOR KNOWLEDGE LAYER,0.6442307692307693,"(x,y)∈D
ln pθ(y|x) + λα →min
θ"
PRIOR KNOWLEDGE LAYER,0.6538461538461539,λ acts as a regularization parameter and could be tuned as hyperparameter.
PRIOR KNOWLEDGE LAYER,0.6634615384615384,"PKL loss is equal to the sum of all scalar penalties, calculated on all consecutive pairs (in the general
case, sequences of arbitrary length) tokens, taking into account an information of incorrect labels
subsequences - Prior Knowledge Matrix. To calculate one scalar penalties, we need to deﬁne an
operation on two probability distributions (over two consecutive tokens) and one matrix (PKM),
which will penalize model for high probability values corresponding to incorrect pairs of labels."
PRIOR KNOWLEDGE LAYER,0.6730769230769231,"Let’s denote the probability distribution matrices, corresponding to each token in consecutive pair
as P1 and P2, correspondingly and Prior Knowledge Matrix as K."
PRIOR KNOWLEDGE LAYER,0.6826923076923077,"One way to specify such an operation is the average over diagonal elements of the product of the
following three matrices P1, P2 and K:"
PRIOR KNOWLEDGE LAYER,0.6923076923076923,mean(diag(P1KP2))
PRIOR KNOWLEDGE LAYER,0.7019230769230769,"Since the loss function additive term, provided by Prior Knowledge Layer, takes into account infor-
mation about incorrect label subsequences, this leads to:"
PRIOR KNOWLEDGE LAYER,0.7115384615384616,"• better generalization capability, especially with a small amount of training data
• faster convergence, comparing with baseline model"
PRIOR KNOWLEDGE LAYER,0.7211538461538461,"Prior Knowledge Layer uses only during training phase. During inference the model behaves the
same as the baseline BiLSTM / BERT model without CRF layer."
PRIOR KNOWLEDGE LAYER,0.7307692307692307,"(a) CRF Layer 1
(b) PKL Layer 2"
PRIOR KNOWLEDGE LAYER,0.7403846153846154,Figure 3: Baseline models with different topmost layers
REFERENCES,0.75,REFERENCES
REFERENCES,0.7596153846153846,Dzmitry Bahdanau et al. Neural machine translation by jointly learning to align and translate. 2014.
REFERENCES,0.7692307692307693,"L.E. Baum and T. Petrie.
Statistical inference for probabilistic functions of ﬁnite state markov
chains. 1966."
REFERENCES,0.7788461538461539,Andrew Borthwick. A maximum entropy approach to named entity recognition. 1999.
REFERENCES,0.7884615384615384,"Bob Carpenter. Coding chunkers as taggers: Io, bio, bmewo, and bmewo+. 2009."
REFERENCES,0.7980769230769231,Under review as a conference paper at ICLR 2022
REFERENCES,0.8076923076923077,"Kyunghyun Cho et al. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. 2014."
REFERENCES,0.8173076923076923,"Junyoung Chung et al. Empirical evaluation of gated recurrent neural networks on sequence model-
ing. 2014."
REFERENCES,0.8269230769230769,"Jacob Devlin et al. Bert: Pre-training of deep bidirectional transformers for language understanding.
2018."
REFERENCES,0.8365384615384616,"Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997."
REFERENCES,0.8461538461538461,Zhiheng Huang et al. Bidirectional lstm-crf models for sequence tagging. 2015.
REFERENCES,0.8557692307692307,Andrej Karpathy. The unreasonable effectiveness of recurrent neural networks. 2015.
REFERENCES,0.8653846153846154,"John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. Conditional random ﬁelds: Proba-
bilistic models for segmenting and labeling sequence data. 2001."
REFERENCES,0.875,Guillaume Lample et al. Neural architectures for named entity recognition. 2016.
REFERENCES,0.8846153846153846,"Dong-Hyun Lee. Pseudo-label : The simple and efﬁcient semi-supervised learning method for deep
neural networks. 2013."
REFERENCES,0.8942307692307693,"Gideon Mann and Andrew McCallum.
Efﬁcient computation of entropy gradient for semi-
supervised conditional random ﬁelds. 2007."
REFERENCES,0.9038461538461539,"Andrew McCallum, Dayne Freitag, and Fernando Pereira. Maximum entropy markov models for
information extraction and segmentation. 2000."
REFERENCES,0.9134615384615384,Tao Qin et al. Global ranking using continuous conditional random ﬁelds. 2008.
REFERENCES,0.9230769230769231,Ariadna Quattoni et al. Hidden conditional random ﬁelds. 2007.
REFERENCES,0.9326923076923077,L.R. Rabiner and B. H. Juang. An introduction to hidden markov models. 1986.
REFERENCES,0.9423076923076923,"Lance A. Ramshaw and Mitchell P. Marcus. Text chunking using transformation-based learning.
1995."
REFERENCES,0.9519230769230769,"David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by
back-propagating errors. Nature, 1986."
REFERENCES,0.9615384615384616,M. Schuster and K.K. Paliwal. Bidirectional recurrent neural networks. 1997.
REFERENCES,0.9711538461538461,Ilya Sutskever et al. Sequence to sequence learning with neural networks. 2014.
REFERENCES,0.9807692307692307,"Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. Dynamic conditional random
ﬁelds: Factorized probabilistic models for labeling and segmenting sequence data. 2007."
REFERENCES,0.9903846153846154,Ashish Vaswani et al. Attention is all you need. 2017.
