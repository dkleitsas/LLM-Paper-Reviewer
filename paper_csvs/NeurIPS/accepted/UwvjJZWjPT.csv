Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017271157167530224,"Neural networks are often trained on multiple tasks, either simultaneously (multi-
task learning, MTL) or sequentially (pretraining and subsequent finetuning,
PT+FT). In particular, it is common practice to pretrain neural networks on a
large auxiliary task before finetuning on a downstream task with fewer samples.
Despite the prevalence of this approach, the inductive biases that arise from learn-
ing multiple tasks are poorly characterized. In this work, we address this gap. We
describe novel implicit regularization penalties associated with MTL and PT+FT in
diagonal linear networks and single-hidden-layer ReLU networks. These penalties
indicate that MTL and PT+FT induce the network to reuse features in different
ways. 1) Both MTL and PT+FT exhibit biases towards feature reuse between
tasks, and towards sparsity in the set of learned features. We show a “conservation
law” that implies a direct tradeoff between these two biases. 2) PT+FT exhibits a
novel “nested feature selection” regime, not described by either the “lazy” or “rich”
regimes identified in prior work, which biases it to rely on a sparse subset of the
features learned during pretraining. This regime is much narrower for MTL. 3)
PT+FT (but not MTL) in ReLU networks benefits from features that are correlated
between the auxiliary and main task. We confirm these findings empirically with
teacher-student models, and introduce a technique – weight rescaling following
pretraining – that can elicit the nested feature selection regime. Finally, we validate
our theory in deep neural networks trained on image classification. We find that
weight rescaling improves performance when it causes models to display signatures
of nested feature selection. Our results suggest that nested feature selection may be
an important inductive bias for finetuning neural networks."
INTRODUCTION,0.0034542314335060447,"1
Introduction"
INTRODUCTION,0.0051813471502590676,"Neural networks are often trained on multiple tasks, either simultaneously (“multi-task learning,”
henceforth MTL, see [1, 2]) or sequentially (“pretraining” and subsequent “finetuning,” henceforth
PT+FT, see [3, 4]). Empirically, models can transfer knowledge from auxiliary tasks to improve
performance on tasks of interest. However, theoretical understanding of how auxiliary tasks influence
learning and generalization is limited."
INTRODUCTION,0.0069084628670120895,"Auxiliary tasks are especially useful when there is less data available for the target task. Modern
“foundation models,” trained on data-rich general-purpose auxiliary tasks (like next-word prediction
or image generation) before adaptation to downstream tasks, are a timely example of this use case [5]."
INTRODUCTION,0.008635578583765112,"*Equal contributions.
**Work primarily conducted while at the Center for Theoretical Neuroscience, Columbia University."
INTRODUCTION,0.010362694300518135,"Auxiliary tasks are also commonly used in reinforcement learning, where performance feedback can
be scarce [6]. Intuitively, auxiliary task learning biases the target task solution to use representations
shaped by the auxiliary task. When the tasks share common structure, this influence may enable
generalization from relatively few training samples for the task of interest. However, it can also have
downsides, causing a model to inherit undesirable biases from auxiliary task learning [7, 8]."
INTRODUCTION,0.012089810017271158,"An influential strategy in the literature on the theory of single-task learning has been to characterize
the implicit regularization conferred by the combination of network architecture and optimization
algorithm [9–13]. Alternatively, some others characterize the effects of explicit parameter regular-
ization (e.g. an ℓ2-penalty on weights) on the inductive bias of networks towards learning certain
functions [14–16]. Compared to the single-task case, the inductive bias of MTL and PT+FT, whether
obtained via explicit regularization or implicit regularization induced by optimization dynamics, is
less well understood. Here we make progress on this question by studying inductive biases of MTL
and PT+FT in two network architectures that have been extensively theoretically studied: diagonal
linear networks, and densely connected networks with one hidden layer and a ReLU nonlinearity. We
then demonstrate that our insights transfer to practically relevant scenarios by studying deep neural
networks trained on image classification tasks."
INTRODUCTION,0.013816925734024179,Our specific contributions are as follows:
INTRODUCTION,0.015544041450777202,"• We characterize regularization penalties associated with MTL or PT+FT in both diagonal
linear networks and single hidden-layer ReLU networks (Section 3).
• We find that both MTL and PT+FT are biased towards solutions that reuse features between
tasks, and that rely on a sparse set of features (Section 4.2).
• We then find that under suitable scalings, PT+FT exhibits a “nested feature selection” regime,
distinct from previously characterized “rich” and “lazy” regimes, which biases finetuning to
extract sparse subsets of the features learned during pretraining (Sections 4.3 and 4.4).
• We find that PT+FT in ReLU networks can benefit from correlated (not just identical)
features between the main and auxiliary task, but only when coefficients of features in the
main task weights are of comparable magnitude (Section 4.5).
• Finally, we study deep neural networks trained on natural image data (CIFAR-100 and
ImageNet) (Section 5). Remarkably, we find that rescaling weights before finetuning
improves accuracy in ResNets. Our analysis of the network representations suggests that
this weight rescaling also results in the network relying on a low-dimensional subspace of
its pretrained representation (i.e. exhibiting nested feature selection behavior). Intriguingly,
Vision Transformers already exhibit signatures of nested feature selection without weight
rescaling, and do not benefit from weight rescaling, suggesting that the nested feature
selection regime is beneficial for finetuning performance."
RELATED WORK,0.017271157167530225,"2
Related work"
RELATED WORK,0.018998272884283247,"A variety of studies have characterized implicit regularization effects in deep learning. These include
biases toward low-frequency functions [17], stable minima in the loss landscape [18], low-rank
solutions [19], and lower-order moments of the data distribution [20]. Chizat & Bach [13] show that
when using cross-entropy loss, shallow (single hidden layer) networks are biased to minimize the
F1 norm, an infinite-dimensional analogue of the ℓ1 norm over the space of possible hidden-layer
features [see also 12, 14]. Other work has shown that implicit regularization for mean squared error
loss in nonlinear networks cannot be exactly characterized as norm minimization [21], though F1
norm minimization is a precise description under certain assumptions on the inputs [22]."
RELATED WORK,0.02072538860103627,"Compared to the body of work on inductive biases of single-task learning, theoretical treatments
of MTL and PT+FT are more scarce. Some prior studies have characterized benefits of multi-task
learning with a shared representational layer in terms of bounds on sample efficiency [23–25].
Others have characterized the learning dynamics of deep linear networks trained from nonrandom
initializations, which can be applied to understand finetuning dynamics [26, 27]. Similarly, insights on
the implicit regularization of gradient descent in linear models has been applied to better understand
forgetting and generalization in a continual learning setup [28–31]. However, while these works
demonstrate an effect of pretrained initializations on learned solutions, the linear models they study
do not capture the notion of feature learning we are interested in. Finally, teacher-student setups"
RELATED WORK,0.022452504317789293,"have been used to study the impact of task similarity on continual learning in deep neural networks
[32, 33]. This methodology could also be applied to our setup (i.e. to investigate generalization on a
finetuning task), and, similarly, our tools could be applied to continual learning setups."
RELATED WORK,0.024179620034542316,"A few empirical studies have compared the performance of MTL vs. PT+FT in language tasks, with
mixed results depending on the task studied [34, 35]. Others have observed that PT+FT outperforms
PT + “linear probing” (training only the readout layer and keeping the previous layers frozen after
pretraining), implying that finetuning benefits from the ability to learn task-specific features [36, 37]."
RELATED WORK,0.025906735751295335,"Inductive biases of diagonal linear networks. The theoretical component of our study relies heavily
on a line of work [38–42] that studies the inductive bias of a simplified “diagonal linear network”
model. Diagonal linear networks parameterize linear maps f : RD →R as"
RELATED WORK,0.027633851468048358,"f⃗w(⃗x) = ⃗β(⃗w) · ⃗x,
βd(⃗w) := w(2)
+,dw(1)
+,d −w(2)
−,dw(1)
−,d
(1)"
RELATED WORK,0.02936096718480138,"where ⃗β(⃗w) ∈RD. These correspond to two-layer linear networks in which the first layer consists of
one-to-one connections, with duplicate + and −pathways to avoid saddle point dynamics around
⃗w = 0. Woodworth et al. [38] showed that overparameterized diagonal linear networks trained with
gradient descent on mean squared error loss find the zero-training-error solution that minimizes
∥⃗β∥2, when trained from large initial weight magnitude (the “lazy” regime, equivalent to ridge
regression). When trained from small initial weight magnitude, networks instead minimize ∥⃗β∥1 (the
“rich” regime). This bias is a linear analogue of feature learning/feature selection, as a model with an
ℓ1 penalty tends to learn solutions that depend on a sparse set of input dimensions."
RELATED WORK,0.031088082901554404,"Implicit vs. explicit regularization. Theoretical work on the inductive biases conferred by different
architectures has studied both the implicit regularization induced by gradient descent and explicit ℓ2-
regularization. Notably, in homogeneous networks trained with crossentropy loss, implicit and explicit
regularization yield identical inductive biases in the limit of infinite training time and infinitesimal
regularization [12, 43], but this does not hold in general [38]. While [38, 40] are able to characterize
the implicit regularization of gradient descent for diagonal linear networks, it is technically much
more challenging to derive a similar result for multi-output diagonal linear networks, or for ReLU
networks of any kind. In contrast, the impact of explicit weight regularization has been characterized
for both multi-output diagonal linear networks [16] and multi-output ReLU networks [44–46]."
RELATED WORK,0.03281519861830743,"Our main technical contributions to this theoretical landscape are (1) spelling out the implications
of existing results on implicit regularization in diagonal linear networks and (2) providing a novel
characterization of the inductive bias induced by applying explicit parameter regularization to the
finetuning of ReLU networks from arbitrary initialization. Our choice to study explicit parameter
regularization for ReLU networks is made primarily for theoretical tractability; we view these results
as a proxy for understanding the more theoretically complex problem of implicit regularization."
RELATED WORK,0.03454231433506045,"3
Implicit and explicit regularization penalties for MTL and PT+FT"
THEORETICAL SETUP,0.03626943005181347,"3.1
Theoretical setup"
THEORETICAL SETUP,0.037996545768566495,"Architectures. First, we consider diagonal linear networks with hidden weights ⃗w+, ⃗w−∈RD
and O ∈{1, 2} output weights v+, v−∈RO×D. (O is 1 or 2 depending on the training paradigm,
see below.) The resulting network function is defined as"
THEORETICAL SETUP,0.039723661485319514,"fw,v(⃗x) = β(w, v)⃗x,
⃗βo(w, v) := ⃗v+,o ◦⃗w+ −⃗v−,o ◦⃗w−,
β(w, v) ∈RO×D.
(2)"
THEORETICAL SETUP,0.04145077720207254,"Second, we consider ReLU networks with H hidden neurons, hidden weights w ∈RH×D, and
O ∈{1, 2} readout weights v ∈RO×H. The network function is defined as"
THEORETICAL SETUP,0.04317789291882556,"fw,v(⃗x) = PH
h=1 ⃗vh(⟨⃗wh, ⃗x⟩)+,
(3)
where (·)+ is the ReLU nonlinearity. Importantly, the ReLU nonlinearity is homogeneous and, as a
result, the network function is invariant to rescaling the hidden weights by α > 0 and the readout
weights by 1"
THEORETICAL SETUP,0.044905008635578586,α. It will therefore be useful to consider a re-parameterization in terms of the
THEORETICAL SETUP,0.046632124352331605,"magnitude mh := vh∥⃗wh∥2 and direction ⃗θh := ⃗wh/∥⃗wh∥2.
(4)
Under this re-parameterization, an equivalent definition of the network function is given by"
THEORETICAL SETUP,0.04835924006908463,"fm,θ(⃗x) = PH
h=1 mh(⟨⃗θh, ⃗x⟩)+ = fw,v(⃗x).
(5) 10−5 10"
THEORETICAL SETUP,0.05008635578583765,"0.1
10
Magnitude Norm"
THEORETICAL SETUP,0.05181347150259067,"0.001
1
1000 a 10−5 10"
THEORETICAL SETUP,0.0535405872193437,"0.1
10
Magnitude Norm b"
THEORETICAL SETUP,0.055267702936096716,"Corr.: 1
Corr.: 0.99
Corr.: 0.9
Corr.: 0"
THEORETICAL SETUP,0.05699481865284974,"0.1
10
0.1
10
0.1
10
0.1
10 10−5 10"
THEORETICAL SETUP,0.05872193436960276,Magnitude Norm
THEORETICAL SETUP,0.06044905008635579,"c
Auxiliary
task feature
coefficient"
THEORETICAL SETUP,0.06217616580310881,"Figure 1: Theoretically derived regularization penalties. a, Explicit regularization penalty associated
with multi-task learning. b, Implicit regularization penalty associated with finetuning in diagonal
linear networks. c, Explicit regularization penalty associated with finetuning in ReLU networks. This
penalty also depends on the changes in feature direction over finetuning (measured by the correlation
between the unit-normalized feature directions pre vs. post finetuning)."
THEORETICAL SETUP,0.06390328151986183,"Training paradigms.
We consider two datasets: an auxiliary task Xaux ∈Rnaux×D, ⃗yaux ∈
Rnaux and a main task Xmain ∈Rnmain×D, ⃗ymain ∈Rnmain."
THEORETICAL SETUP,0.06563039723661486,"First, we consider multi-task learning (MTL): simultaneous training on both the auxiliary and main
task. In this case, we consider networks with O = 2 outputs. The first output corresponds to the
auxiliary task and the second output to the main task. Accordingly, we denote βaux := β1 and
βmain := β2 in the diagonal linear network and ⃗vaux := ⃗v1 and ⃗vmain := ⃗v2 in the ReLU network."
THEORETICAL SETUP,0.06735751295336788,"Second, we consider pretraining on the auxiliary task and subsequent finetuning on the main task
(PT+FT). In this case, we consider networks with a single output (O = 1), but re-initialize the readout
weights before finetuning. Accordingly, we denote the parameters learned after pretraining by waux
and vaux, and the parameters learned after finetuning by wmain and vmain. We define βmain, βaux
(for the diagonal network) and mmain, θmain, maux, θaux (for the ReLU network) analogously."
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.0690846286701209,"3.2
Explicit regularization penalties in multi-task learning"
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.07081174438687392,"To theoretically understand the inductive bias of diagonal linear and ReLU networks trained with MTL,
we consider the effect of minimizing the ℓ2 parameter norm as an approximation of the implicit bias
of training with gradient descent from small initialization. We argue that this is a reasonable heuristic.
First, the analogous result holds in the single-output case for infinitesimally small initialization and
two layers (though not for deeper networks, see [38]). Second, for cross-entropy loss it has been
shown that gradient flow on all positively homogeneous networks (including diagonal linear networks
and ReLU networks) converges to a KKT point of a max-margin/min-parameter-norm objective [12].
Finally, explicit ℓ2 parameter norm regularization (“weight decay”) is commonly used in practice,
making its inductive bias important to understand in its own right as well."
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.07253886010362694,We now derive the norms minimized by explicit regularization in MTL:
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.07426597582037997,"Corollary 1. For the multi-output diagonal linear network defined in Eq. 2, a solution β∗with
minimal parameter norm ∥w∥2
2 + ∥v∥2
2 subject to the constraint that it fits the training data
(Xmain⃗βmain = ⃗ymain, Xaux⃗βaux = ⃗yaux) also minimizes the following:"
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.07599309153713299,"β∗= arg min
β"
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.07772020725388601,"
2 PD
d=1
p"
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.07944732297063903,"(βaux
d
)2 + (βmain
d
)2

s.t.
Xmain⃗βmain = ⃗ymain, Xaux⃗βaux = ⃗yaux."
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.08117443868739206,"This norm is known as the group lasso [47] and denoted as ∥· ∥1,2. For ReLU networks, by
an argument analogous to the one above, parameter norm minimization translates to minimizing
PH
h=1
p"
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.08290155440414508,"(maux
h
)2 + (mmain
h
)2 (see Appendix A.1 and [44, 46])."
EXPLICIT REGULARIZATION PENALTIES IN MULTI-TASK LEARNING,0.0846286701208981,"The norm ∥· ∥1,2 is plotted in Fig. 1a. We analyze its impact in Section 4."
REGULARIZATION PENALTIES IN FINETUNING,0.08635578583765112,"3.3
Regularization penalties in finetuning"
REGULARIZATION PENALTIES IN FINETUNING,0.08808290155440414,"Diagonal linear networks. We now consider the behavior of PT+FT in overparameterized diagonal
linear networks trained to minimize mean-squared error using gradient flow. We assume that the"
REGULARIZATION PENALTIES IN FINETUNING,0.08981001727115717,"network is initialized prior to pre-training with infinitesimal weights, and that during pretraining,
network weights are optimized to convergence on the training dataset (Xaux, ⃗yaux) from the auxiliary
task. After pretraining, the second-layer weights (v+ and v−) are reinitialized with constant magnitude
γ. Further, to ensure that the network output pre-finetuning is zero (as in [38]), we set the values of
corresponding positive and negative pathway weights to be equal to their sum following pretraining1.
The network weights are further optimized to convergence on the main task dataset (Xmain, ⃗ymain).
The dynamics of the pretraining and finetuning steps can be derived as a corollary of [38, 40]:"
REGULARIZATION PENALTIES IN FINETUNING,0.09153713298791019,"Corollary 2. If the gradient flow solution ⃗βaux for the diagonal linear model in Eq. 1 during
pretraining fits the auxiliary task training data with zero error (i.e. Xaux⃗βaux = ⃗yaux), and
following reinitialization of the second-layer weights and finetuning, the gradient flow solution ⃗β∗"
REGULARIZATION PENALTIES IN FINETUNING,0.09326424870466321,"after finetuning fits the main task data with zero training error (i.e. Xmain⃗βmain = ⃗ymain), then
⃗β∗= arg min
⃗βmain
∥⃗βmain∥Q
s.t.
X⃗β = ⃗y,"
REGULARIZATION PENALTIES IN FINETUNING,0.09499136442141623,"∥⃗βmain∥Q := D
X d=1"
REGULARIZATION PENALTIES IN FINETUNING,0.09671848013816926," 
|βaux
d
| + γ2
q

2βmain
d
|βaux
d
| + γ2"
REGULARIZATION PENALTIES IN FINETUNING,0.09844559585492228,"
,
q(z) = 2 −
p"
REGULARIZATION PENALTIES IN FINETUNING,0.1001727115716753,4 + z2 + z · arcsinh(z/2).
REGULARIZATION PENALTIES IN FINETUNING,0.10189982728842832,This corollary is proven in Appendix A.2. The norm is plotted in Fig. 1b.
REGULARIZATION PENALTIES IN FINETUNING,0.10362694300518134,"ReLU networks. We assume that after pretraining, the readout layer is re-initialized with arbitrary
new weights ⃗γ ∈RH. We then characterize the solution to the finetuning task that minimizes the
ℓ2 norm of weight changes from this initialization. This is similar to the explicit weight regulariza-
tion considered in the previous section, except we now penalize weight changes from a particular
initialization rather than the origin. We chose to consider this regularization penalty for two reasons.
First, it is sometimes studied in the continual learning setting [30, 48]. Second, infinitesimal explicit
regularization is equivalent to the implicit regularization induced by gradient descent in the case of
shallow linear models [10]. While this is not true more generally, it is a useful heuristic to motivate
our theoretical analysis (which we then validate using our experiments in Section 4)."
REGULARIZATION PENALTIES IN FINETUNING,0.10535405872193437,"We show that this finetuning solution implicitly minimizes the following penalty:
Proposition 3. Consider a single-output ReLU network (see Eq. 3) with first-layer weights ⃗waux
h
∈
RD after pretraining, and second-layer weights re-initialized to ⃗γ ∈RH. The solution to the
finetuning task that minimizes the ℓ2 norm of changes in the weights, i.e. minimizes PH
h=1 ∥⃗wh −
⃗waux
h
∥2
2 + (vh −γh)2, is equivalent to the solution that minimizes"
REGULARIZATION PENALTIES IN FINETUNING,0.1070811744386874,"R(θmain, mmain|θaux, maux) := H
X"
REGULARIZATION PENALTIES IN FINETUNING,0.10880829015544041,"h=1
r(⃗θmain
h
, mmain
h
|⃗θaux
h
, maux
h
),"
REGULARIZATION PENALTIES IN FINETUNING,0.11053540587219343,"r(⃗θmain
h
, mmain
h
|⃗θaux
h
, maux
h
) := (mmain
h
/u∗−γh)2 + (u∗)2 + maux
h
−2u∗p"
REGULARIZATION PENALTIES IN FINETUNING,0.11226252158894647,"maux
h
⟨⃗θmain
h
, ⃗θaux
h
⟩,
where u∗is the unique positive real root of"
REGULARIZATION PENALTIES IN FINETUNING,0.11398963730569948,"−(mmain
h
)2 + γhmmain
h
u −maux
h
⟨⃗θmain
h
, ⃗θaux
h
⟩u3 + u4 = 0.
(6)"
REGULARIZATION PENALTIES IN FINETUNING,0.1157167530224525,"We prove the proposition in Appendix A.3. It implies that the regularization penalty associated with
finetuning in the ReLU network only depends on the correlation ρh := ⟨⃗θmain
h
, ⃗θaux
h
⟩between the
first-layer feature weights before and after finetuning, and the magnitudes of the weights of these
features. We plot this penalty in Fig. 1c."
REGULARIZATION PENALTIES IN FINETUNING,0.11744386873920552,"4
Implications of the theory: multiple regimes of feature reuse"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.11917098445595854,"4.1
Sample efficiency in teacher-student models"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.12089810017271158,"To validate these theoretical characterizations and illustrate their consequences, we now perform
experiments in a teacher-student setup. In the diagonal linear network case, we consider a linear"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.1226252158894646,"1Note that the need for this procedure is an idiosyncrasy of the diagonal linear network setup, and further is
unnecessary if γ = 0. Following pretraining, for each input dimension, either the positive or negative pathway
weights will be zero, so setting both pathway parameters to equal the sum across pathways has the effect of
copying the nonzero value over to the zeroed-out pathway"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.12435233160621761,"STL
MTL
PT+FT"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.12607944732297063,"32
256
32
256
32
256 1 10−3 10−6"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.12780656303972365,# Samples
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.12953367875647667,Gen. loss 5 40
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.13126079447322972,"# Non-zero
dims a"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.13298791018998274,"32
256
32
256
32
256 0.1 10−4"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.13471502590673576,Full overlap
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.13644214162348878,# Samples
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.1381692573402418,Gen. loss 1
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.13989637305699482,"6
# Units b 1 10−3 10−6"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.14162348877374784,"32
256
# Samples"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.14335060449050085,Gen. loss
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.14507772020725387,"MTL
PT+FT
STL
PT+FT (LP)
STL (LP) c 0.1 10−4"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.14680483592400692,"32
256
# Samples"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.14853195164075994,Gen. loss
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.15025906735751296,"MTL
PT+FT
STL
PT+FT (LP)
PT+FT (NTK)
STL (LP)
STL (NTK) d"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.15198618307426598,"20/40
30/40"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.153713298791019,"32
256
32
256 1 10−3 10−6"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.15544041450777202,# Samples
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.15716753022452504,Gen. loss e
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.15889464594127806,"STL
MTL
Full overlap
PT+FT
3/6
5/6"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.16062176165803108,"32
256
32
256 0.1 10−4"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.16234887737478412,# Samples
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.16407599309153714,Gen. loss f
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.16580310880829016,"Figure 2: PT+FT and MTL benefit from feature sparsity and reuse. a,b, Generalization loss for a)
diagonal linear networks and b) ReLU networks trained on a) a linear model with distinct active
dimensions and b) a teacher network with distinct units between auxiliary and main task (STL: single-
task learning). MTL and PT+FT benefit from a sparser teacher on the main task. c,d, Generalization
loss for c) diagonal linear networks and d) ReLU networks trained on a teacher model sharing all
features between the auxiliary and main task. PT+FT and MTL both generalize better than STL. e,f,
Generalization loss for e) diagonal linear networks and f) ReLU networks trained on a teacher model
with overlapping features. Networks benefit from feature sharing and can learn new features."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.16753022452504318,"regression task defined by ⃗w ∈R1000 with a sparse set of k non-zero entries. We sample two such
vectors, corresponding to “auxiliary” and “main” tasks, varying the number of non-zero entries kaux
and kmain, and the number of shared features (overlapping non-zero entries). We then uniformly
sample input vectors ⃗x ∈R1000 from the unit sphere, using the ground-truth weights to generate the
target. We train on 1024 auxiliary samples and vary the number of main task samples."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.1692573402417962,"In the ReLU network case, we consider a “teacher” ReLU network with a sparse number of units (i.e.
a low-dimensional hidden layer) and different kinds of overlaps (e.g. shared, correlated, or orthogonal
features) between the auxiliary and main task. We randomly sample input data ⃗x ∈R15 from the
unit sphere and use the teacher network to generate the target. We train on 1024 auxiliary samples
and vary the number of main task samples. During finetuning, we randomly re-initialize the readout
weights using a normal distribution with a variance of 10−3p 2/H."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.17098445595854922,"4.2
PT+FT and MTL benefit from sparse and shared features"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.17271157167530224,"Feature sparsity. To understand the impact of the derived penalties in the case of features either
used or not used during pretraining, we consider their limit behavior for very large or very small
pretrained features. First, we consider the limit
|βd|
|βaux
d
|+γ2 →∞(capturing the case of a feature not
used during pretraining). In this limit, the MTL penalty converges to 2|βd|. Similarly, for finetuning
in diagonal linear networks, the penalty converges to c|βd| where c ∼O
 
log
 
1/(|βaux
d
| + γ2)
"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.17443868739205526,"(per an analysis in [38]). For new features, both networks therefore have an ℓ1 norm minimization
bias, suggesting that they tend to learn a sparse set of new features (just like in the single-task case)."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.17616580310880828,"To test this insight, we consider a teacher-student setup without any shared features between the
auxiliary and main task, and vary the number of main task features. Indeed, we found that both MTL
and PT+FT have a more rapidly decreasing generalization loss for fewer features (just like single-task
learning (STL)) (Fig. 2a). We further confirmed that they learned a sparse set of weights (Fig. 7)."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.17789291882556132,"The same phenomenon holds true for the ReLU network penalties as well. We considered a teacher-
student network with six auxiliary task features and one to six uncorrelated main task features. Again,
both MTL and PT+FT have a lower generalization loss for fewer features (Fig. 2b)."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.17962003454231434,"Feature sharing. Next, we consider the opposite limit, i.e. large pretrained features:
|βd|
|βaux
d
|+γ2 →0."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.18134715025906736,"In this limit, both the MTL and the PT+FT penalty for diagonal linear networks converges to
β2
d
|βaux
d
|,"
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.18307426597582038,"a weighted ℓ2 bias. This implies that the networks preferentially use large pretrained features. To
test this, we consider a teacher-student setup with fully overlapping dimensions. Indeed, both MTL
and PT+FT outperform STL in this case (Fig. 2c). Notably, they perform similarly to a network
where we only finetune the second layer (PT+FT (Linear Probing, LP)), which exactly implements
the weighted ℓ2 bias."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.1848013816925734,"In the case of ReLU networks, we considered a teacher network with the same six features for the
auxiliary and main task. Again, we found that MTL and PT+FT outperformed STL, though training a
linear readout from a model with fixed features (either by using the hidden layer (PT+FT (LP)) or the
neural tangent kernel (PT+FT (NTK))) generalized even better (Fig. 2d)."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.18652849740932642,"Simultaneous sparsity and feature sharing. Finally, our analysis above considered the limit
behavior of each feature separately. This suggests that models should be able to 1) preferentially rely
on pretrained features and 2) when necessary, learn a sparse set of new features. To test this insight,
we consider partially overlapping teacher models. In diagonal linear networks, we consider teacher
models with forty auxiliary and main task features, with twenty or thirty of those features overlapping
(Fig. 2e). On the one hand, both PT+FT and MTL outperformed STL, indicating that they were able
to benefit from the pretrained features. On the other hand, they also performed better than the PT+FT
(LP) model which only finetuned the second layer (and therefore did not have a sparse inductive bias),
indicating that they tended to learn a sparse set of new features. In ReLU networks, we consider
teacher models with six auxiliary and main task features, varying their overlap. Again, we find that
MTL and PT+FT outperformed both STL and PT+FT (LP), indicating that they benefitted from
feature learning by implementing an inductive bias towards sparse and shared features."
SAMPLE EFFICIENCY IN TEACHER-STUDENT MODELS,0.18825561312607944,"Differences between the MTL and PT+FT norms. So far, we have highlighted several similarities
between the MTL and PT+FT norms in diagonal linear networks: they tend towards the ℓ1 norm for
small auxiliary features and a weighted ℓ2 norm for large auxiliary features. In the next section, we
will highlight an important difference that arises in the intermediate regime. Here we briefly highlight
a difference in the limit behavior for diagonal linear networks: the relative weights of the ℓ1- and
weighted ℓ2-penalty are different between MTL and PT+FT. In particular, in the ℓ1 penalty limit,
there is an extra factor of order O
 
log
 
1/(|βaux
d
| + γ2)

in the PT+FT penalty. Assuming small
initializations, this factor tends to be larger than 2, the corresponding coefficient in the MTL penalty.
Thus, PT+FT is more strongly biased toward reusing features from the auxiliary task compared to
MTL. We are careful to note, however, that in the case of ReLU networks this effect is complicated
by a qualitatively different phenomenon with effects in the reverse direction (see Section 4.5)."
A CONSERVATION LAW,0.18998272884283246,"4.3
A conservation law"
A CONSERVATION LAW,0.19170984455958548,"We now turn our attention to the intermediate regime where the coefficients of a feature are of
similar magnitude in the auxiliary and main tasks. We define two functions for a given penalty
P(βmain
d
, βaux
d
) (where βmain
d
is the main task feature coefficient and βaux
d
is the auxiliary task
feature coefficient): 1) the “ℓ-order,”
∂log P
∂log βmain
d
, which measures, locally, how strongly P changes"
A CONSERVATION LAW,0.19343696027633853,"with increasing βmain
d
and 2) the “feature dependence” (FD),
∂log P
∂log βaux
d
, which measures, locally,
how much P decreases for a larger auxiliary feature. In the previous section, we found that for
βaux
d
→0, ℓ-order →1 and FD →0, i.e. the penalty becomes ℓ1-like and does not depend on the
magnitude of the auxiliary feature. In contrast, for βaux
d
→1, ℓ-order →2 and FD →−1, i.e. the
penalty becomes ℓ2-like and depends inversely on the auxiliary task feature coefficient magnitude."
A CONSERVATION LAW,0.19516407599309155,"How are ℓ-order and FD related for intermediate values of βaux
d
? Remarkably, we find an exact
analytical relationship between these measures that holds for all penalties considered here:"
A CONSERVATION LAW,0.19689119170984457,"Proposition 4. For the MTL and PT+FT penalties derived in both the diagonal linear and ReLU
cases, ℓ-order + FD = 1."
A CONSERVATION LAW,0.19861830742659758,See Appendix A.4 for proof.
A CONSERVATION LAW,0.2003454231433506,"Thus, there is an exact tradeoff between the sparsity and feature dependence even for intermediate
auxiliary task feature coefficient values: an increase in sparsity (i.e. a smaller ℓ-order) yields a
corresponding decrease in feature dependence (i.e. FD becomes closer to zero). 1 2 0 −1"
A CONSERVATION LAW,0.20207253886010362,"0.001
10"
A CONSERVATION LAW,0.20379965457685664,Order of penalty FD
A CONSERVATION LAW,0.20552677029360966,"MTL
PT+FT a"
A CONSERVATION LAW,0.20725388601036268,"32
256
32
256 1 10−3 10−6"
A CONSERVATION LAW,0.20898100172711573,"MTL
PT+FT"
A CONSERVATION LAW,0.21070811744386875,# Samples
A CONSERVATION LAW,0.21243523316062177,Gen. loss 5 40
A CONSERVATION LAW,0.2141623488773748,"# Non-zero
entries b"
A CONSERVATION LAW,0.2158894645941278,"32
256
32
256 1 10−3 10−6"
A CONSERVATION LAW,0.21761658031088082,# Samples
A CONSERVATION LAW,0.21934369602763384,Gen. loss c 0 1 2 1 0 −1
A CONSERVATION LAW,0.22107081174438686,"0.001
10"
A CONSERVATION LAW,0.22279792746113988,Order of penalty 0
A CONSERVATION LAW,0.22452504317789293,"1
Corr."
A CONSERVATION LAW,0.22625215889464595,"d
MTL
PT+FT"
A CONSERVATION LAW,0.22797927461139897,"32
256
32
256 0.1 10−4"
A CONSERVATION LAW,0.229706390328152,# Samples
A CONSERVATION LAW,0.231433506044905,Gen. loss 1
A CONSERVATION LAW,0.23316062176165803,"6
# Units"
A CONSERVATION LAW,0.23488773747841105,"e
PT+FT (0.1)
PT+FT (10)"
A CONSERVATION LAW,0.23661485319516407,PT+FT (0.01)
A CONSERVATION LAW,0.23834196891191708,"32
256
32
256 0.1 10−4"
A CONSERVATION LAW,0.24006908462867013,# Samples
A CONSERVATION LAW,0.24179620034542315,Gen. loss f
A CONSERVATION LAW,0.24352331606217617,Auxiliary feat.
A CONSERVATION LAW,0.2452504317789292,Auxiliary feat. FD
A CONSERVATION LAW,0.2469775474956822,PT+FT (0.1)
A CONSERVATION LAW,0.24870466321243523,"Figure 3: PT+FT (much moreso than MTL) exhibits a nested feature selection regime. a-c, Diagonal
linear networks. a, ℓ-order/feature dependence plotted for βmain
d
= 1 and varying the auxiliary task
feature coefficient. b, Generalization loss for models trained on a teacher with 40 active units during
the auxiliary task and a subset of those units active during the main task. c, Generalization loss for
PT+FT models whose weights are rescaled by the factor in the parentheses before finetuning. d-f,
ReLU networks. d, ℓ-order/feature dependence plotted for the explicit finetuning and MTL penalties,
for m = 1 and varying the auxiliary task feature coefficient. e, Generalization loss for models trained
on a teacher network with six active units on the auxiliary task and a subset of those units on the main
task. f, Generalization loss for PT+FT models whose weights are rescaled before finetuning."
THE NESTED FEATURE SELECTION REGIME,0.2504317789291883,"4.4
The nested feature selection regime"
THE NESTED FEATURE SELECTION REGIME,0.25215889464594127,"Proposition 4 predicts the existence of a novel “nested feature selection” regime: for intermediate
magnitudes of the auxiliary features, the penalties should encourage both sparsity and feature
dependence. To test this prediction in diagonal linear networks, we use a teacher-student setting in
which all of the main task features are a subset of the auxiliary task features, i.e. kmain ≤kaux, and
the number of overlapping units is equal to kmain. Solving this task most efficiently involves ""nested
feature selection,"" a bias towards feature reuse and towards sparsity among the reused features. We
plot ℓ-order and FD for βmain
d
= 1 and varying the auxiliary task feature coefficient (Fig. 3a) and
find that for βaux
d
≈1, both norms exhibit “lazy regime”-like behavior (ℓ-order of around 2, and
FD near 0). This predicts that neither MTL nor PT+FT networks should be able to benefit from
nested sparsity task structure in this regime, which we confirm empirically: as kmain decreases, the
networks’ sample efficiency does not become substantially better (Fig. 3b)."
THE NESTED FEATURE SELECTION REGIME,0.2538860103626943,"However, for features with auxiliary task coefficients that are moderately smaller than their main task
coefficients, Fig. 3a suggests a broad regime where the ℓ-order is closer to 1 (incentivizing sparsity),
but feature dependence remains high. We can produce this behavior in these tasks by rescaling the
weights of the network following pretraining by a factor less than 1. In line with the prediction of the
theory, performing this manipulation enables PT+FT to leverage sparse structure within auxiliary task
features (Fig. 3c), even while retaining their ability to privilege features learned during pretraining
above others (see Fig. 9). By contrast, this regime is much narrower for MTL (Fig. 3a)."
THE NESTED FEATURE SELECTION REGIME,0.2556131260794473,"For ReLU networks, the MTL penalty is the same as in diagonal linear networks. We plot the
regularization penalty derived for PT+FT, conditioned on various correlations ⟨θmain
h
, θaux
h
⟩between
the post-pretraining and post-finetuning feature weights (Fig. 3d). For features that are fully aligned
before and after pretraining, this penalty is again ℓ2-like for maux ≈1. However, in most cases, these
features change direction at least a little bit, and we find that in that case the penalty is more ℓ1-like
while still remaining sufficiently feature-dependent. This suggests that a nested feature selection
regime may arise in ReLU networks even when auxiliary task feature coefficients have comparable
magnitude to main task feature coefficients. To test this insight, we considered a set of tasks in which
the main task solution relies exclusively on a subset of auxiliary task features. We found that PT+FT
(even without any weight rescaling) was able to benefit from the nested sparsity structure, but MTL
was not (Fig. 3e).2 Performing weight rescaling in either direction following pretraining uncovers the"
THE NESTED FEATURE SELECTION REGIME,0.25734024179620035,"2We note that we had observed this behavior before deriving the PT+FT regularization penalty for ReLU
networks. Unlike all other described experiments (for which we derived the described predictions from inspecting
the norms before running any simulations), this is therefore a postdiction rather than a prediction."
THE NESTED FEATURE SELECTION REGIME,0.25906735751295334,"initialization-insensitive (FD near 0), sparsity-biased (ℓ-order near 1) rich / feature-learning regime
and the initialization-biased (FD near −1), no-sparsity-bias (ℓ-order near 2) lazy learning regime
(Fig. 3f). This suggests that for different architectures and different tasks, different rescaling values
may be required to enter the nested feature selection regime."
THE NESTED FEATURE SELECTION REGIME,0.2607944732297064,"4.5
PT+FT, but not MTL, in ReLU networks benefits from correlated features 0.1 10−4"
THE NESTED FEATURE SELECTION REGIME,0.26252158894645944,"32
256
# Samples"
THE NESTED FEATURE SELECTION REGIME,0.26424870466321243,"MTL
PT+FT
STL a"
THE NESTED FEATURE SELECTION REGIME,0.2659758203799655,"Mag.: 1
Corr: 0.9
Mag.: 0.1Mag.: 0.01"
THE NESTED FEATURE SELECTION REGIME,0.26770293609671847,"32 256
32 256
32 256 0.1 10−4"
THE NESTED FEATURE SELECTION REGIME,0.2694300518134715,# Samples
THE NESTED FEATURE SELECTION REGIME,0.2711571675302245,Gen. loss
THE NESTED FEATURE SELECTION REGIME,0.27288428324697755,"STL
PT+FT (Corr.: 1)
PT+FT (Corr.: 0.9)
PT+FT (Corr.: 0.8) b"
THE NESTED FEATURE SELECTION REGIME,0.27461139896373055,Gen. loss
THE NESTED FEATURE SELECTION REGIME,0.2763385146804836,"Figure 4: PT+FT, but not MTL, in ReLU networks
benefits from correlated features. a, Generalization
loss for main task features that are correlated (0.9
cosine similarity) with the auxiliary task features.
PT+FT outperforms both MTL and STL. b, Gener-
alization loss for main task features with varying
correlation and magnitude (mag.). PT+FT only
outperforms STL if the features are either identical
in direction or identical in magnitude."
THE NESTED FEATURE SELECTION REGIME,0.27806563039723664,"The regularization associated with PT+FT yields
benefits even when main/auxiliary task direc-
tions are correlated but not identical (Fig. 1c).
In contrast, MTL cannot softly share features as
it encodes correlated features with entirely dis-
tinct units. To test this hypothesis, we conduct
experiments in which the ground-truth auxiliary
and main tasks rely on correlated but distinct
features. Indeed, we find PT+FT outperforms
STL in this case, whereas MTL only does so in
the low-sample setting (Fig. 4a). Notably, if fea-
tures are identical, MTL outperforms PT+FT in
the low-sample setting (Fig. 2d). Thus, PT+FT
(compared to MTL) trades off the flexibility
to “softly” share features for reduced sample-
efficiency when such flexibility is not needed."
THE NESTED FEATURE SELECTION REGIME,0.27979274611398963,"The analysis in Fig. 3d would predict that ReLU
networks can no longer benefit from correlated
features if the magnitude of auxiliary task fea-
tures is much higher than that of main task fea-
tures. To test this, we varied the magnitude of
the main task features and their correlation with
auxiliary task features (Fig. 4b). We found that for lower magnitudes, PT+FT still improved perfor-
mance for the case of identical but not correlated feature directions, confirming our prediction."
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.2815198618307427,"5
Weight rescaling in deep networks gives rise to nested feature selection"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.28324697754749567,"Our analysis has focused on shallow networks trained on synthetic tasks. To test the applicability of
our insights, we conduct experiments with convolutional networks (ResNet-18, [49]) on a vision task
(CIFAR-100, [50]), using classification of two image categories (randomly sampled for each training
run) as the primary task and classification of the other 98 as the auxiliary task. As in our experiments
above, MTL and PT+FT improve sample efficiency compared to single-task learning (Fig. 5a)."
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.2849740932642487,"Our findings in Section 4.4 indicate that the nested feature selection bias of PT+FT can be exposed or
masked by rescaling the network weights following pretraining. Such a bias may be beneficial when
the main task depends on a small subset of features learned during pretraining, as may often be the
case in practice. We experiment with rescaling in our CIFAR setup. We find that rescaling values
less than 1 improve finetuning performance (Fig. 5b). These results suggest that rescaling network
weights before finetuning may be practically useful. We corroborate this hypothesis with additional
experiments using networks pre-trained on ImageNet [51] (see Fig. 10)."
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.2867012089810017,"To facilitate comparison of the phenomenology in deep networks with our teacher-student experi-
ments above, we propose a signature of nested feature selection that can be characterized without
knowledge of the underlying feature space (since the correct feature basis to analyze is less clear
in multi-hidden-layer entworks). Specifically, we propose to measure (1) the dimensionality of the
network representation pre- and post-finetuning, and (2) the extent to which the representational
structure post-finetuning is shared with / inherited from that of the network following pretraining
prior to finetuning. We employ the commonly used participation ratio (PR, [52]) as a measure
of dimensionality, and the effective number of shared dimensions (ENSD, [53]) as a soft measure
of the number of aligned principal components between two representations. Intuitively, the PR
and ENSD of network representations pre- and post-finetuning capture the key phenomena of the
nested feature selection regime: we expect the dimensionality of network after finetuning to be 0.1 0.5"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.28842832469775476,"20
500
# Samples"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.29015544041450775,Accuracy
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.2918825561312608,"MTL
PT+FT
STL a 0.1 0.5"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.29360967184801384,"20
500
# Samples"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.29533678756476683,Accuracy
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.2970639032815199,Scaling
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.2987910189982729,"0.125
0.25
0.5
1
2 b 10 20 10"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.3005181347150259,"20
Scaling: 0.125 Scaling: 1
Scaling: 2"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.3022452504317789,"1
7
1
7
1
7
0 10 20 Layer"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.30397236614853196,"PR Pre
PR Post
ENSD c 0.05 0.20"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.30569948186528495,"20
500
# Samples"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.307426597582038,Accuracy
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.30915371329879104,"MTL
PT+FT
STL d 0.05 0.20"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.31088082901554404,"20
500
# Samples"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.3126079447322971,Accuracy
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.3143350604490501,Scaling
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.3160621761658031,"0.125
0.25
0.5
1
2 e 0 40 80 10
20"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.3177892918825561,"Scaling: 0.125 Scaling: 1
Scaling: 2"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.31951640759930916,"1
8
1
8
1
8
0.0 2.5 5.0 Layer"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.32124352331606215,"PR Pre
PR Post
ENSD f"
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.3229706390328152,"Figure 5: Experiments in deep neural networks trained on CIFAR-100: a-c, ResNet-18, d-f, ViT.
a,d, Accuracy for MTL, PT+FT, and STL in a) ResNet-18 and d) ViT. b,e Accuracy for PT+FT with
weight rescaling in b) ResNet-18 and e) ViT. c,f The participation ration of c) ResNet-18’s and f)
ViT’s layers before and after finetuning (PR Pre and PR Post) as well as their ENSD."
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.32469775474956825,"lower than after pretraining (PR(XF T ) < PR(XP T )), and for nearly all of the representational
dimensions expressed by the network post-finetuning to be inherited from the network state after
pretraining (ENSD(XP T , XF T ) ≈PR(XF T )). We validate that this description holds in our
nonlinear teacher-student experiments with networks in the nested feature selection regime (Fig. 11)."
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.32642487046632124,"Remarkably, we find that the ResNet-18 exhibits the same phenomenology, but only for weights
rescaled by a small value (Fig. 5c). This supports the hypothesis that the observed benefits of rescaling
indeed arise from pushing the network into the nested feature selection regime."
WEIGHT RESCALING IN DEEP NETWORKS GIVES RISE TO NESTED FEATURE SELECTION,0.3281519861830743,"Finally, we conduct the same experiment for Vision Transformers (ViT) [54]. We confirm that PT+FT
improves performance over single-task learning, though MTL offers no similar benefit in this case
(Fig. 5d). We further find that rescaling before finetuning (both by larger and smaller values) decreases
generalization performance (Fig. 5e). Notably, our representational analysis reveals that rescaling
by a larger value yields a higher-dimensional subspace both before and after finetuning, whereas
rescaling by a smaller value yields a lower-dimensional subspace, but pushes the effective number of
shared dimensions down substantially (Fig. 5f). This indicates that a rescaling value of 1 may already
give rise to the nested feature selection regime and rescaling by a smaller value pushes the ViT
towards the pure feature learning regime. Taken together, our results suggest finetuning performance
is best when networks operate in the nested feature selection regime, and weight rescaling can push
networks into this regime when it does not arise naturally."
CONCLUSION,0.3298791018998273,"6
Conclusion"
CONCLUSION,0.3316062176165803,"In this work we have provided a detailed characterization of the inductive biases associated with
two common training strategies, MTL and PT+FT, in diagonal linear and ReLU networks. These
biases incentivize both feature sharing and sparse task-specific feature learning. In the case of PT+FT,
we characterized a novel nested feature selection learning regime which encourages sparsity within
features inherited from pretraining. This insight motivates a simple technique for improving PT+FT
performance by pushing networks into this regime, which shows promising empirical results."
CONCLUSION,0.3333333333333333,"There are several avenues for extending our theoretical work: for example, connecting our derived
penalties for ReLU networks (which assumed explicit parameterization) to the implicit regularization
induced by dynamics of gradient descent, and extending our theory to the case of cross-entropy loss.
In addition, more work is needed to extend our theory to more complex tasks and larger models. For
instance, we are interested in investigating how the weight magnitudes required to enter the nested
feature regime depend on architecture and properties of the tasks — we observed a difference between
the rescaling values required for ResNets and Vision Transformers, which our shallow network theory
is unable to speak to. Better understanding the conditions needed for nested feature selection could
also inspire more sophisticated interventions than our weight rescaling trick. Finally, we considered a
particularly simple multi-task setup, with identical formats between auxiliary and main tasks — our
theory could be extended to cases where the different tasks use different objectives. Nevertheless, our
work already provides new and practical insights into multi-task learning and finetuning."
CONCLUSION,0.33506044905008636,Acknowledgments
CONCLUSION,0.33678756476683935,"We are grateful to the members of the Center for Theoretical Neuroscience for helpful comments and
discussions. The work was supported by NSF 1707398 (Neuronex), Gatsby Charitable Foundation
GAT3708, and the NSF AI Institute for Artificial and Natural Intelligence (ARNI)."
REFERENCES,0.3385146804835924,References
REFERENCES,0.34024179620034545,"1.
Vafaeikia, P., Namdar, K. & Khalvati, F. A Brief Review of Deep Multi-task Learning and
Auxiliary Task Learning arXiv:2007.01126 [cs, stat]. July 2020. http://arxiv.org/abs/
2007.01126 (2023).
2.
Zhang, Y. & Yang, Q. A Survey on Multi-Task Learning. IEEE Transactions on Knowledge
and Data Engineering 34. Conference Name: IEEE Transactions on Knowledge and Data
Engineering, 5586–5609. ISSN: 1558-2191 (Dec. 2022).
3.
Du, Y., Liu, Z., Li, J. & Zhao, W. X. A Survey of Vision-Language Pre-Trained Models
arXiv:2202.10936 [cs]. July 2022. http://arxiv.org/abs/2202.10936 (2023).
4.
Zhou, C. et al. A Comprehensive Survey on Pretrained Foundation Models: A History from
BERT to ChatGPT arXiv:2302.09419 [cs]. May 2023. http://arxiv.org/abs/2302.09419
(2023).
5.
Bommasani, R. et al. On the Opportunities and Risks of Foundation Models arXiv:2108.07258
[cs]. July 2022. http://arxiv.org/abs/2108.07258 (2023).
6.
Jaderberg,
M.
et
al.
Reinforcement
Learning
with
Unsupervised
Auxiliary
Tasks
arXiv:1611.05397 [cs]. Nov. 2016. http://arxiv.org/abs/1611.05397 (2023).
7.
Wang, A. & Russakovsky, O. Overwriting Pretrained Bias with Finetuning Data
arXiv:2303.06167 [cs]. Aug. 2023. http://arxiv.org/abs/2303.06167 (2023).
8.
Steed, R., Panda, S., Kobren, A. & Wick, M. Upstream Mitigation Is Not All You Need:
Testing the Bias Transfer Hypothesis in Pre-Trained Language Models in Proceedings of
the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) (Association for Computational Linguistics, Dublin, Ireland, May 2022), 3524–3542.
https://aclanthology.org/2022.acl-long.247 (2023).
9.
Neyshabur, B. Implicit Regularization in Deep Learning arXiv:1709.01953 [cs]. Sept. 2017.
http://arxiv.org/abs/1709.01953 (2023).
10.
Gunasekar, S., Lee, J. D., Soudry, D. & Srebro, N. Implicit Bias of Gradient Descent on
Linear Convolutional Networks in Advances in Neural Information Processing Systems 31
(Curran Associates, Inc., 2018). https://proceedings.neurips.cc/paper/2018/hash/
0e98aeeb54acf612b9eb4e48a269814c-Abstract.html (2023).
11.
Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. & Srebro, N. The implicit bias of gradient
descent on separable data. The Journal of Machine Learning Research 19, 2822–2878. ISSN:
1532-4435 (Jan. 2018).
12.
Lyu, K. & Li, J. Gradient Descent Maximizes the Margin of Homogeneous Neural Networks
arXiv:1906.05890 [cs, stat]. Dec. 2020. http://arxiv.org/abs/1906.05890 (2023).
13.
Chizat, L. & Bach, F. Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks
Trained with the Logistic Loss en. in Proceedings of Thirty Third Conference on Learning
Theory ISSN: 2640-3498 (PMLR, July 2020), 1305–1338. https://proceedings.mlr.
press/v125/chizat20a.html (2023).
14.
Savarese, P., Evron, I., Soudry, D. & Srebro, N. How do infinite width bounded norm networks
look in function space?: 32nd Conference on Learning Theory, COLT 2019. Proceedings of
Machine Learning Research 99, 2667–2690. http://www.scopus.com/inward/record.
url?scp=85132757852&partnerID=8YFLogxK (2023) (2019).
15.
Ongie, G., Willett, R., Soudry, D. & Srebro, N. A function space view of bounded norm infinite
width relu nets: The multivariate case. arXiv preprint arXiv:1910.01635 (2019).
16.
Dai, Z., Karzand, M. & Srebro, N. Representation Costs of Linear Neural Networks: Analysis
and Design in Advances in Neural Information Processing Systems 34 (Curran Associates, Inc.,
2021), 26884–26896. https://proceedings.neurips.cc/paper_files/paper/2021/
hash/e22cb9d6bbb4c290a94e4fff4d68a831-Abstract.html (2023)."
REFERENCES,0.34196891191709844,"17.
Rahaman, N. et al. On the Spectral Bias of Neural Networks. en. https://openreview.net/
forum?id=r1gR2sC9FX (2023) (Sept. 2018).
18.
Mulayoff, R., Michaeli, T. & Soudry, D. The Implicit Bias of Minima Stability: A View from
Function Space in Advances in Neural Information Processing Systems 34 (Curran Associates,
Inc., 2021), 17749–17761. https://proceedings.neurips.cc/paper/2021/hash/
944a5ae3483ed5c1e10bbccb7942a279-Abstract.html (2023).
19.
Huh, M. et al. The Low-Rank Simplicity Bias in Deep Networks arXiv:2103.10427 [cs]. Mar.
2023. http://arxiv.org/abs/2103.10427 (2023).
20.
Refinetti, M., Ingrosso, A. & Goldt, S. Neural networks trained with SGD learn distributions of
increasing complexity arXiv:2211.11567 [cond-mat, stat]. May 2023. http://arxiv.org/
abs/2211.11567 (2023).
21.
Razin, N. & Cohen, N. Implicit Regularization in Deep Learning May Not Be Explainable
by Norms in Advances in Neural Information Processing Systems 33 (Curran Associates,
Inc., 2020), 21174–21187. https://proceedings.neurips.cc/paper/2020/hash/
f21e255f89e0f258accbe4e984eef486-Abstract.html (2023).
22.
Boursier, E., Pillaud-Vivien, L. & Flammarion, N. Gradient flow dynamics of shallow ReLU
networks for square loss and orthogonal inputs en. in (May 2022). https://openreview.
net/forum?id=L74c-iUxQ1I (2023).
23.
Maurer, A., Pontil, M. & Romera-Paredes, B. The Benefit of Multitask Representation Learning.
Journal of Machine Learning Research 17, 1–32. ISSN: 1533-7928. http://jmlr.org/
papers/v17/15-242.html (2023) (2016).
24.
Wu, S., Zhang, H. R. & Ré, C. Understanding and Improving Information Transfer in Multi-Task
Learning arXiv:2005.00944 [cs]. May 2020. http://arxiv.org/abs/2005.00944 (2023).
25.
Collins, L., Hassani, H., Soltanolkotabi, M., Mokhtari, A. & Shakkottai, S. Provable Multi-Task
Representation Learning by Two-Layer ReLU Neural Networks en. in (June 2024). https:
//openreview.net/forum?id=M8UbECx485 (2024).
26.
Braun, L., Dominé, C., Fitzgerald, J. & Saxe, A. Exact learning dynamics of deep linear
networks with prior knowledge. en. Advances in Neural Information Processing Systems
35, 6615–6629. https://proceedings.neurips.cc/paper_files/paper/2022/
hash / 2b3bb2c95195130977a51b3bb251c40a - Abstract - Conference . html (2023)
(Dec. 2022).
27.
Shachaf, G., Brutzkus, A. & Globerson, A. A Theoretical Analysis of Fine-tuning with Lin-
ear Teachers in Advances in Neural Information Processing Systems 34 (Curran Associates,
Inc., 2021), 15382–15394. https://proceedings.neurips.cc/paper/2021/hash/
82039d16dce0aab3913b6a7ac73deff7-Abstract.html (2023).
28.
Evron, I., Moroshko, E., Ward, R., Srebro, N. & Soudry, D. How catastrophic can catastrophic
forgetting be in linear regression? in Conference on Learning Theory (PMLR, 2022), 4028–
4079.
29.
Lin, S., Ju, P., Liang, Y. & Shroff, N. Theory on Forgetting and Generalization of Continual
Learning en. in Proceedings of the 40th International Conference on Machine Learning ISSN:
2640-3498 (PMLR, July 2023), 21078–21100. https://proceedings.mlr.press/v202/
lin23f.html (2024).
30.
Evron, I. et al. Continual learning in linear classification on separable data in International
Conference on Machine Learning (PMLR, 2023), 9440–9484.
31.
Goldfarb, D., Evron, I., Weinberger, N., Soudry, D. & HAnd, P. The Joint Effect of Task
Similarity and Overparameterization on Catastrophic Forgetting—An Analytical Model in The
Twelfth International Conference on Learning Representations (2024).
32.
Lee, S., Goldt, S. & Saxe, A. Continual Learning in the Teacher-Student Setup: Impact of Task
Similarity en. in Proceedings of the 38th International Conference on Machine Learning ISSN:
2640-3498 (PMLR, July 2021), 6109–6119. https://proceedings.mlr.press/v139/
lee21e.html (2024).
33.
Lee, S., Mannelli, S. S., Clopath, C., Goldt, S. & Saxe, A. Maslow’s Hammer in Catastrophic
Forgetting: Node Re-Use vs. Node Activation en. in Proceedings of the 39th International
Conference on Machine Learning ISSN: 2640-3498 (PMLR, June 2022), 12455–12477. https:
//proceedings.mlr.press/v162/lee22g.html (2024)."
REFERENCES,0.3436960276338515,"34.
Dery, L. M., Michel, P., Talwalkar, A. & Neubig, G. Should We Be Pre-training? An Argument
for End-task Aware Training as an Alternative en. in (Oct. 2021). https://openreview.
net/forum?id=2bO2x8NAIMB (2023).
35.
Weller, O., Seppi, K. & Gardner, M. When to Use Multi-Task Learning vs Intermediate Fine-
Tuning for Pre-Trained Encoder Transfer Learning in Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 2: Short Papers) (Association for
Computational Linguistics, Dublin, Ireland, May 2022), 272–282. https://aclanthology.
org/2022.acl-short.30 (2023).
36.
Kumar, A., Raghunathan, A., Jones, R., Ma, T. & Liang, P. Fine-Tuning can Distort Pretrained
Features and Underperform Out-of-Distribution arXiv:2202.10054 [cs]. Feb. 2022. http:
//arxiv.org/abs/2202.10054 (2023).
37.
Kornblith, S., Shlens, J. & Le, Q. V. Do better ImageNet models transfer better? in (2019).
https://arxiv.org/pdf/1805.08974.pdf (2023).
38.
Woodworth, B. et al. Kernel and Rich Regimes in Overparametrized Models en. in Proceedings
of Thirty Third Conference on Learning Theory ISSN: 2640-3498 (PMLR, July 2020), 3635–
3673. https://proceedings.mlr.press/v125/woodworth20a.html (2023).
39.
Pesme, S., Pillaud-Vivien, L. & Flammarion, N. Implicit Bias of SGD for Diagonal Linear
Networks: a Provable Benefit of Stochasticity en. in (Nov. 2021). https://openreview.net/
forum?id=vvi7KqHQiA (2023).
40.
Azulay, S. et al. On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror
Descent en. in Proceedings of the 38th International Conference on Machine Learning (PMLR,
July 2021), 468–477. https://proceedings.mlr.press/v139/azulay21a.html (2023).
41.
HaoChen, J. Z., Wei, C., Lee, J. & Ma, T. Shape Matters: Understanding the Implicit Bias of the
Noise Covariance en. in Proceedings of Thirty Fourth Conference on Learning Theory (PMLR,
July 2021), 2315–2357. https://proceedings.mlr.press/v134/haochen21a.html
(2023).
42.
Moroshko, E. et al. Implicit Bias in Deep Linear Classification: Initialization Scale vs Train-
ing Accuracy in Advances in Neural Information Processing Systems 33 (Curran Associates,
Inc., 2020), 22182–22193. https://proceedings.neurips.cc/paper/2020/hash/
fc2022c89b61c76bbef978f1370660bf-Abstract.html (2023).
43.
Ji, Z., Dudík, M., Schapire, R. E. & Telgarsky, M. Gradient descent follows the regularization
path for general losses in Conference on Learning Theory (PMLR, 2020), 2109–2136.
44.
Yang, L. et al. A Better Way to Decay: Proximal Gradient Training Algorithms for Neural Nets
en. in (Nov. 2022). https://openreview.net/forum?id=4y1xh8jClhC (2024).
45.
Parhi, R. & Nowak, R. D. Near-Minimax Optimal Estimation With Shallow ReLU Neural
Networks. IEEE Transactions on Information Theory 69. Conference Name: IEEE Transactions
on Information Theory, 1125–1140. ISSN: 1557-9654. https://ieeexplore.ieee.org/
abstract/document/9899453 (2024) (Feb. 2023).
46.
Shenouda, J., Parhi, R., Lee, K. & Nowak, R. D. Variation Spaces for Multi-Output Neural
Networks: Insights on Multi-Task Learning and Network Compression. Journal of Machine
Learning Research 25, 1–40. ISSN: 1533-7928. http://jmlr.org/papers/v25/23-
0677.html (2024) (2024).
47.
Yuan, M. & Lin, Y. Model selection and estimation in regression with grouped variables.
en. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00532.x, 49–67. ISSN: 1467-
9868. https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.
00532.x (2023) (2006).
48.
Lubana, E. S., Trivedi, P., Koutra, D. & Dick, R. How do quadratic regularizers prevent
catastrophic forgetting: The role of interpolation in Conference on Lifelong Learning Agents
(PMLR, 2022), 819–837.
49.
He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition in (2016),
770–778. https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_
Residual_Learning_CVPR_2016_paper.html (2024).
50.
Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images.
Publisher: Toronto, ON, Canada (2009).
51.
Deng, J. et al. ImageNet: A large-scale hierarchical image database in 2009 IEEE Conference
on Computer Vision and Pattern Recognition (2009), 248–255."
REFERENCES,0.3454231433506045,"52.
Gao, P. et al. A theory of multineuronal dimensionality, dynamics and measurement en. Nov.
2017. https://www.biorxiv.org/content/10.1101/214262v2 (2023).
53.
Giaffar, H., Buxó, C. R. & Aoi, M. The effective number of shared dimensions: A simple method
for revealing shared structure between datasets en. July 2023. https://www.biorxiv.org/
content/10.1101/2023.07.27.550815v1 (2023).
54.
Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929 (2020).
55.
Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems 32 (2019).
56.
Kornblith, S., Norouzi, M., Lee, H. & Hinton, G. Similarity of Neural Network Representations
Revisited en. in Proceedings of the 36th International Conference on Machine Learning (PMLR,
May 2019), 3519–3529. https://proceedings.mlr.press/v97/kornblith19a.html
(2023)."
REFERENCES,0.3471502590673575,"A
Derivation of regularization penalties"
REFERENCES,0.3488773747841105,"A.1
Multi-task learning in ReLU networks"
REFERENCES,0.35060449050086356,Multi-task ReLU networks with a shared feature layer and O outputs can be written as
REFERENCES,0.35233160621761656,"fw(x) = H
X"
REFERENCES,0.3540587219343696,"h=1
⃗vh(⟨
⃗
wh, ⃗x⟩)+ = H
X"
REFERENCES,0.35578583765112265,"h=1
⃗mh(⟨⃗θh, ⃗x⟩)+,
(7)"
REFERENCES,0.35751295336787564,"⃗
mh = ⃗vh∥⃗wh∥2, ⃗θh = ⃗wh/∥⃗wh∥2,
(8)"
REFERENCES,0.3592400690846287,"where ⃗wh, mh ∈RO. We are interested in how minimizing the parameter norm PH
h=1 ∥⃗wh∥2
2+∥⃗vh∥2
2
maps to minimizing a norm over the solution weights ⃗m. For a given ⃗mh ∈RO, the norm of the
input weight ∥⃗wh∥2 is a free parameter zh > 0, as we can set ⃗vh := ⃗mh/z. As any value for zh leads
to the same function, we choose zh so as to minimize"
REFERENCES,0.3609671848013817,"∥⃗wh∥2
2 + ∥⃗vh∥2
2 = z2
h + ∥⃗mh∥2
2/z2
h.
(9)"
REFERENCES,0.3626943005181347,"Setting the derivative to zero implies
z2
h = ∥⃗
mh∥2.
(10)
As a result,
H
X"
REFERENCES,0.3644214162348877,"h=1
∥⃗wh∥2
2 + ∥⃗vh∥2
2 = 2 H
X"
REFERENCES,0.36614853195164077,"h=1
∥⃗mh∥2,
(11)"
REFERENCES,0.36787564766839376,"where the right-hand side of the equation is the ℓ1,2 norm."
REFERENCES,0.3696027633851468,"A.2
Finetuning in diagonal linear networks"
REFERENCES,0.37132987910189985,"Corollary 2. If the gradient flow solution ⃗βaux for the diagonal linear model in Eq. 1 during
pretraining fits the auxiliary task training data with zero error (i.e. Xaux⃗βaux = ⃗yaux), and
following reinitialization of the second-layer weights and finetuning, the gradient flow solution ⃗β∗"
REFERENCES,0.37305699481865284,"after finetuning fits the main task data with zero training error (i.e. Xmain⃗βmain = ⃗ymain), then
⃗β∗= arg min
⃗βmain
∥⃗βmain∥Q
s.t.
X⃗β = ⃗y,"
REFERENCES,0.3747841105354059,"∥⃗βmain∥Q := D
X d=1"
REFERENCES,0.3765112262521589," 
|βaux
d
| + γ2
q

2βmain
d
|βaux
d
| + γ2"
REFERENCES,0.37823834196891193,"
,
q(z) = 2 −
p"
REFERENCES,0.3799654576856649,4 + z2 + z · arcsinh(z/2).
REFERENCES,0.38169257340241797,"Proof. We derive this result in two steps: first, we note that after pretraining (because of the
infinitesimal initial weight magnitudes), one of the pathways remains at zero and one pathway
encodes the effective linear predictor βaux. Thus, the first hidden layer has the weights √βaux,
which yields the new weight magnitude along the positive and negative pathway (due to the summing
operation we perform following finetuning, which ensures that the weights of both pathways are set
to √βaux). Having set the readout initialization to γ, we then apply Theorem 4.1 in [40]."
REFERENCES,0.38341968911917096,"A.3
Finetuning in ReLU networks"
REFERENCES,0.385146804835924,"Proposition 3. Consider a single-output ReLU network (see Eq. 3) with first-layer weights ⃗waux
h
∈
RD after pretraining, and second-layer weights re-initialized to ⃗γ ∈RH. The solution to the
finetuning task that minimizes the ℓ2 norm of changes in the weights, i.e. minimizes PH
h=1 ∥⃗wh −
⃗waux
h
∥2
2 + (vh −γh)2, is equivalent to the solution that minimizes"
REFERENCES,0.38687392055267705,"R(θmain, mmain|θaux, maux) := H
X"
REFERENCES,0.38860103626943004,"h=1
r(⃗θmain
h
, mmain
h
|⃗θaux
h
, maux
h
),"
REFERENCES,0.3903281519861831,"r(⃗θmain
h
, mmain
h
|⃗θaux
h
, maux
h
) := (mmain
h
/u∗−γh)2 + (u∗)2 + maux
h
−2u∗p"
REFERENCES,0.3920552677029361,"maux
h
⟨⃗θmain
h
, ⃗θaux
h
⟩,"
REFERENCES,0.39378238341968913,where u∗is the unique positive real root of
REFERENCES,0.3955094991364421,"−(mmain
h
)2 + γhmmain
h
u −maux
h
⟨⃗θmain
h
, ⃗θaux
h
⟩u3 + u4 = 0.
(6)"
REFERENCES,0.39723661485319517,Proof. We define the norm of the deviation from the parameters at initialization as
REFERENCES,0.39896373056994816,"R(w, v|waux, γ) := H
X"
REFERENCES,0.4006908462867012,"h=1
(vh −γ)2 + ∥⃗wh −waux
h
∥2
2.
(12)"
REFERENCES,0.40241796200345425,"Note that w and v are parameterized in a redundant manner, as we can multiply vh by a constant a > 0
and obtain the same function as long as we divide ⃗wh by the same constant. To parameterize a function
in a unique fashion, we describe it in terms of the normalized hidden weight ⃗θh := ⃗wh/∥⃗wh∥2 and
the (signed) magnitude mh := ∥⃗w(1)
h ∥2vh. We wish to compute the norm of the deviation from the
initialization in terms of this parameterization. Specifically, we compute"
REFERENCES,0.40414507772020725,"˜R(⃗θ, m|waux, γ) := min
w,v R(w, v|waux, γ) s.t. ∀h⃗θh = ⃗wh/∥⃗wh∥2, mh = ∥⃗wh∥2vh.
(13)"
REFERENCES,0.4058721934369603,"Note that Woodworth et al. [38] solve a version of this optimization problem for diagonal linear
networks (though only for the case in which corresponding first and second layer weights are
initialized with equal magnitude)."
REFERENCES,0.4075993091537133,"As this optimization problem decomposes over hidden units, we can solve it for each hidden unit
individually:"
REFERENCES,0.40932642487046633,"˜R(⃗θ, m|waux, γ) = H
X"
REFERENCES,0.4110535405872193,"h=1
˜r(⃗θh, mh|⃗waux
h
, γ),
(14)"
REFERENCES,0.41278065630397237,"˜r(⃗θh, mh|⃗waux
h
, γ) := min
⃗wh,vh(vh −γ)2 + ∥⃗wh −⃗waux
h
∥2
2
(15)"
REFERENCES,0.41450777202072536,"s.t. ⃗θh = ⃗wh/∥⃗wh∥2, mh = ∥⃗wh∥2vh.
(16)"
REFERENCES,0.4162348877374784,We can express this as
REFERENCES,0.41796200345423146,"˜r(⃗θh, mh|waux
h
, γ) = min
u>0 p(u),
p(u) := (mh/u −γ)2 +
u⃗θh −⃗waux
h

2"
REFERENCES,0.41968911917098445,"2 .
(17)"
REFERENCES,0.4214162348877375,"Note that this function is smooth and diverges to ∞both for u →∞and u →0. As long as we have
a unique positive stationary point u∗, this stationary point must therefore be a minimum. We simplify"
REFERENCES,0.4231433506044905,"p(u) = (mh/u −γ)2 + u2 + ∥⃗waux
h
∥2
2 −2u⟨⃗θh, ⃗waux
h
⟩,
(18)"
REFERENCES,0.42487046632124353,"and, defining maux
h
:= ∥⃗waux
h
∥2, and the cosine similarity between corresponding pretraining and"
REFERENCES,0.4265975820379965,"finetuning task weights ρh :=
⟨⃗w(1)
h ,⃗waux
h
⟩"
REFERENCES,0.4283246977547496,"∥⃗w(1)
h ∥2∥⃗waux
h
∥2 , write this as"
REFERENCES,0.43005181347150256,"p(u) = (mh/u −γ)2 + u2 + (maux
h
)2 −2umaux
h
ρh.
(19)"
REFERENCES,0.4317789291882556,We then determine the stationary points by computing the derivative
REFERENCES,0.43350604490500866,"p′(u) = −2(mh/u −γ)mh/u2 + 2u −2maux
h
ρh,
(20)"
REFERENCES,0.43523316062176165,and setting it to zero. Simplifying this equation (and multiplying by u3/2) yields
REFERENCES,0.4369602763385147,"−m2
h + γmhu −maux
h
ρhu3 + u4 = 0.
(21)"
REFERENCES,0.4386873920552677,"We can compute the solution to this quartic equation, e.g. using np.roots and select the (unique, in
all empirical cases we explored) positive real root, u∗. We can then compute the original norm by
plugging in u∗:"
REFERENCES,0.44041450777202074,"˜r(⃗θh, mh|⃗waux
h
, γ) = (mh/u∗−γ)2 + u∗2 + (maux
h
)2 −2u∗maux
h
ρh.
(22)"
REFERENCES,0.4421416234887737,"A.4
Proof of Proposition 4"
REFERENCES,0.4438687392055268,"Proposition 4. For the MTL and PT+FT penalties derived in both the diagonal linear and ReLU
cases, ℓ-order + FD = 1."
REFERENCES,0.44559585492227977,"Proof. Below, for simplicity’s sake, we always denote βmain = β, mmain = m, and θmain = θ."
REFERENCES,0.4473229706390328,"MTL penalty. We first consider the MTL penalty, which is identical for diagonal linear networks
and ReLU networks. We assume (without loss of generalization) maux > 0. In that case,"
REFERENCES,0.44905008635578586,"ℓ-order =
∂log
p"
REFERENCES,0.45077720207253885,(maux)2 + m2 −maux
REFERENCES,0.4525043177892919,"∂log m
= m
p"
REFERENCES,0.4542314335060449,"(maux)2 + m2 −maux
m
p"
REFERENCES,0.45595854922279794,"(maux)2 + m2 =
m2"
REFERENCES,0.45768566493955093,(maux)2 + m2 −mauxp
REFERENCES,0.459412780656304,"(maux)2 + m2 , (23) and"
REFERENCES,0.46113989637305697,"FD =
∂log
p"
REFERENCES,0.46286701208981,(maux)2 + m2 −maux
REFERENCES,0.46459412780656306,"∂log maux
="
REFERENCES,0.46632124352331605,"maux
p"
REFERENCES,0.4680483592400691,(maux)2 + m2 −maux
REFERENCES,0.4697754749568221,"maux
p"
REFERENCES,0.47150259067357514,(maux)2 + m2 −1 !
REFERENCES,0.47322970639032813,"=
(maux)2 −mauxp"
REFERENCES,0.4749568221070812,(maux)2 + m2
REFERENCES,0.47668393782383417,(maux)2 + m2 −mauxp
REFERENCES,0.4784110535405872,(maux)2 + m2 . (24)
REFERENCES,0.48013816925734026,"Thus, ℓ-order + FD = 1."
REFERENCES,0.48186528497409326,"Finetuning in diagonal network. We assume (without loss of generalization) βaux
d
> 0 and derive"
REFERENCES,0.4835924006908463,"ℓ-order =
∂log

(|βaux
d
|) q

2βd
βaux
d "
REFERENCES,0.4853195164075993,"∂log βd
=
2βd"
REFERENCES,0.48704663212435234,"(βaux
d
)2q

2βd
|βaux
d
|
q′ 
2βd
|βaux
d
|

.
(25) and"
REFERENCES,0.48877374784110533,"FD =
∂log

|βaux
d
|q

2βd
|βaux
d
|
"
REFERENCES,0.4905008635578584,"∂log |βaux
d
|
1"
REFERENCES,0.49222797927461137,"q

2βd
|βaux
d
|


q

2βd
|βaux
d
|

−q′ 
2βd
|βaux
d
|

2βd
(βaux
d
)2

= 1 −
2βd"
REFERENCES,0.4939550949913644,"(βaux
d
)2q

2βd
|βaux
d
|
q′ 
2βd
|βaux
d
|

.
(26)"
REFERENCES,0.49568221070811747,"Thus, ℓ-order + FD = 1."
REFERENCES,0.49740932642487046,Finetuning in ReLU networks. We define
REFERENCES,0.4991364421416235,"˜r(⃗θh, mh, u|⃗θaux
h
, maux
h
) := (mh/u −γ)2 + u2 + maux
h
−2u
p"
REFERENCES,0.5008635578583766,"maux
h
⟨⃗θh, ⃗θaux
h
⟩,
(27)"
REFERENCES,0.5025906735751295,"where, without loss of generalization, we assume that maux
h
> 0 and note that"
REFERENCES,0.5043177892918825,"r(⃗θh, mh|⃗θaux
h
, maux
h
) = ˜r(⃗θh, mh, u∗|⃗θaux
h
, maux
h
),
(28)"
REFERENCES,0.5060449050086355,"where u∗is the critical point, depending on ⃗θh, mh, ⃗θaux
h
, maux
h
. By the chain rule,"
REFERENCES,0.5077720207253886,"∂r(⃗θh, mh|⃗θaux
h
, maux
h
)
∂mh
= ∂˜r(⃗θh, mh, u∗|⃗θaux
h
, maux
h
)
∂mh
+ ∂˜r(⃗θh, mh, u∗|⃗θaux
h
, maux
h
)
∂u∗
∂u∗"
REFERENCES,0.5094991364421416,"mh
, (29)"
REFERENCES,0.5112262521588946,"and the analogous statement is true for maux
h
."
REFERENCES,0.5129533678756477,"However, by definition,
∂˜r(⃗θh, mh, u∗|⃗θaux
h
, maux
h
)
∂u∗
= 0.
(30) Thus,"
REFERENCES,0.5146804835924007,ℓ-order = mh
REFERENCES,0.5164075993091537,"r
∂˜r
∂mh
=
2m2
h
r(u∗)2 ,"
REFERENCES,0.5181347150259067,"FD = maux
h
r
∂˜r
∂maux
h
= maux
h
r "
REFERENCES,0.5198618307426598,"1 −
ρhu∗
pmaux
h ! = 1"
REFERENCES,0.5215889464594128,"r
 
maux
h
−ρh
p"
REFERENCES,0.5233160621761658,"maux
h
u∗
.
(31)"
REFERENCES,0.5250431778929189,"To prove the statement, we therefore must prove that"
REFERENCES,0.5267702936096719,"2m2
h
(u∗)2 + maux
h
−ρ
p"
REFERENCES,0.5284974093264249,"maux
h
u∗= r =
m2
h
(u∗)2 + (u∗)2 + (maux
h
) −2u∗maux
h
ρh.
(32)"
REFERENCES,0.5302245250431779,"To do so, we leverage that (30) implies"
REFERENCES,0.531951640759931,"−m2
h
(u∗)2 + (u∗)2 −u∗p"
REFERENCES,0.533678756476684,"maux
h
ρh = 0,
(33)"
REFERENCES,0.5354058721934369,"and therefore
m2
h
(u∗)2 = (u∗)2 −u∗p"
REFERENCES,0.5371329879101899,"maux
h
ρh = 0.
(34)"
REFERENCES,0.538860103626943,Plugging this into (32) yields
REFERENCES,0.540587219343696,"2m2
h
(u∗)2 + maux
h
−ρ
p"
REFERENCES,0.542314335060449,"maux
h
u∗=
m2
h
(u∗)2 + (u∗)2 + maux
h
−2ρ
p"
REFERENCES,0.5440414507772021,"maux
h
u∗= r
(35)"
REFERENCES,0.5457685664939551,and completes the proof.
REFERENCES,0.5474956822107081,"B
Detailed methods"
REFERENCES,0.5492227979274611,We trained all networks with PyTorch [55].
REFERENCES,0.5509499136442142,"B.1
Diagonal linear networks"
REFERENCES,0.5526770293609672,"We train the diagonal linear networks until they have reached a mean squared error below 10−10,
for pretraining, finetuning, multi-task learning and single-task learning. As a rough heuristic for
determining the learning rate, we begin at a learning rate of 106 and divide by ten whenever the loss
exceeds 100 (indicating divergence)."
REFERENCES,0.5544041450777202,"B.2
ReLU networks"
REFERENCES,0.5561312607944733,"We train ReLU networks with 1000 hidden units using the same learning rate heuristic and train until
the mean squared error has reached a threshold of 10−8."
REFERENCES,0.5578583765112263,"B.3
ResNets"
REFERENCES,0.5595854922279793,"We consider a ResNet-18 in the standard PyTorch implementation and train it with gradient descent
with a learning rate of 10−3 and momentum of 0.9."
REFERENCES,0.5613126079447323,"B.4
Vision Transformers"
REFERENCES,0.5630397236614854,"We consider a Vision Transformer with seven layers, eight heads, 384 hidden dimensions and an MLP
with 1,536 hidden dimensions. We train the transformer using the Adam optimizer with a learning
rate of 10−3 and weight decay 0.00005."
REFERENCES,0.5647668393782384,"B.5
Reproducibility"
REFERENCES,0.5664939550949913,"We provide a code database that enables reproducing all data used to create the main figures. We
used one CPU for each diagonal network (total number of experiments: 5,544) and ReLU network
experiment (total number of experiments: 6,740) and all of them took up to four hours. We used on
GPU for the CIFAR and ViT experiments and all of them took up to twelve hours (total number of
experiments: 5,000). As a rough (upper-bound) estimate, the experiments therefore required 50,000
hours of CPU time and 60,000 hours of GPU time. 1 10−3"
REFERENCES,0.5682210708117443,"64
4096
# Samples Loss"
REFERENCES,0.5699481865284974,# Teacher units
REFERENCES,0.5716753022452504,"40
1000"
REFERENCES,0.5734024179620034,Scale of init.
REFERENCES,0.5751295336787565,"1
0.001 a"
REFERENCES,0.5768566493955095,"0/40
20/40
35/40
40/40"
REFERENCES,0.5785837651122625,"32
4096 32
4096 32
4096 32
4096 0.1 10−4"
REFERENCES,0.5803108808290155,# Samples Loss
REFERENCES,0.5820379965457686,"MTL
PT+FT
STL
PT+FT (LP)
PT+FT (NTK)
STL (LP)
STL (NTK) b"
REFERENCES,0.5837651122625216,"Figure 6: Larger-scale teacher-student experiments. a, Generalization loss of shallow ReLU networks
trained on data from a ReLU teacher network. b, Generalization loss for different numbers of
overlapping features (out of 40 total) between main and auxiliary tasks. NTK indicates the (lazy)
tangent kernel solution. This is comparable to Fig. 2d, except with more teacher units and more data."
REFERENCES,0.5854922279792746,"C
Robustness of main results to choice of number of auxiliary task samples
and input dimension"
REFERENCES,0.5872193436960277,"To increase confidence that our main results are robust to the number of data samples used (1024
auxiliary task samples and up to 1024 main task samples in most of our experiments), and the number
of ground-truth units in the teacher network (6), we repeated the experiments of Fig. 2d with 8192
auxiliary task samples and 40 ground-truth features. Indeed, in this setting the rich regime also helps
with generalization if and only if the teacher units are sparse (Fig. 6a). Further, MTL and PT+FT tend
to outperform STL if the features are overlapping and MTL tends to outperform PT+FT (Fig. 6b). In
particular, the finetuned networks still benefit from feature learning, especially if some features are
novel."
REFERENCES,0.5889464594127807,"D
Analysis of learned solutions in linear and nonlinear networks"
REFERENCES,0.5906735751295337,"Our theory predicts inductive biases towards solutions that minimize norms, often either ℓ1-like
(incentivizing sparsity) or ℓ2-like. Our experiments in the main text corroborate these description by
analyzing how sample complexity depends on the feature sparsity of the ground-truth task solution,
and how the sparse feature structures of the main and auxliary tasks relate. However, this evidence
for sparsity biases (or lack thereof) is indirect; here we present more direct analyses of the learned
solutions in linear and nonlinear networks that support the account we provide in the main text."
REFERENCES,0.5924006908462867,"D.1
Diagonal linear networks"
REFERENCES,0.5941278065630398,"To check whether the implicit regularization theory is a good explanation for these performance
results, we directly measured the ℓ1,2 and Q norms of the solutions learned by networks, compared
to the corresponding penalties of the ground truth weights. In Fig. 7a we see that as the amount
of training data increases, the norms all converge to that of the ground truth solution, but in the
low-sample regime, MTL and PT+FT find solutions with lower values of their corresponding norm
than the ground-truth function, consistent with the implicit regularization picture (by contrast, STL
does not consistently find solutions with lower values of these norms than the ground truth)."
REFERENCES,0.5958549222797928,"Our theory predicts that weight rescaling by a factor less than 1.0 following pretraining reduces the
propensity of the network to share features between auxiliary and main tasks during finetuning. We
confirm that this is the case in Fig. 7b by analyzing the overlap between the auxiliary task features
and the learned linear predictor for the main task."
REFERENCES,0.5975820379965457,"In Fig. 7c we show that our measure of effective sparsity of learned solutions in diagonal linear
networks effectively distinguishes between networks trained in the feature selection regime and
networks trained with linear probing (only training second-layer weights). Moreover, in Fig. 7d we
show that the L1 norm of the solution increases with the training sample size, consistent with a bias
towards L1 minimization. There is an interesting discrepancy between the behavior of the sparsity of
the solutions (nonmonotonic, see Fig. 7c) and the L1 norm (largely monotonic, see Fig. 7d). This
is attributable to the discrepancy between the L1 norm (which diagonal linear networks in the rich
regime are biased to minimize) and sparsity (for which L1 norm is only a proxy). 0.8 0.9 1.0"
REFERENCES,0.5993091537132987,"Q
l_12"
REFERENCES,0.6010362694300518,"32
256
32
256
0.3 1.0 3.0 10.0"
REFERENCES,0.6027633851468048,# Samples
REFERENCES,0.6044905008635578,Rel. penalty
REFERENCES,0.6062176165803109,"MTL
PT+FT
STL a 0.0 0.5 1.0"
REFERENCES,0.6079447322970639,"32
256
# Samples"
REFERENCES,0.6096718480138169,Overlap b/w tasks 0.01
SCALE,0.6113989637305699,"1.00
Scale"
SCALE,0.613126079447323,overlap
SCALE,0.614853195164076,"no
yes b"
SCALE,0.616580310880829,"STL
STL (LP)"
SCALE,0.6183074265975821,"0
200 0
200
0.0 0.5 1.0"
SCALE,0.6200345423143351,# Weights
SCALE,0.6217616580310881,Unexplained
SCALE,0.6234887737478411,Variance 32 256
SCALE,0.6252158894645942,# Samples c
SCALE,0.6269430051813472,"2
3
4
5
6"
SCALE,0.6286701208981001,"32
256
# Samples"
SCALE,0.6303972366148531,L1 norm d
SCALE,0.6321243523316062,"0/40
20/40
30/40
40/40"
SCALE,0.6338514680483592,"PT+FT
MTL"
SCALE,0.6355785837651122,"0
200 0
200 0
200 0
200 0.0 0.5 1.0 0.0 0.5 1.0"
SCALE,0.6373056994818653,# Weights
SCALE,0.6390328151986183,Unexplained
SCALE,0.6407599309153713,Variance 32 256
SCALE,0.6424870466321243,# Samples e
SCALE,0.6442141623488774,"0/40
20/40
30/40
40/40"
SCALE,0.6459412780656304,"32
256
32
256
32
256
32
256
0.0 0.5 1.0"
SCALE,0.6476683937823834,# Samples
SCALE,0.6493955094991365,Overlap b/w tasks
SCALE,0.6511226252158895,"STL
STL (LP)
MTL
PT+FT f"
SCALE,0.6528497409326425,"Figure 7: a, ℓ1,2 norm and Q penalty for MTL, STL, and PT+FT networks from Fig. 2b (40/40
overlapping features case). b, Proportion of the weight norm in the 40 dimensions relevant for the
auxiliary task, for the networks in Fig. 3b,c. Weight rescaling decreases this overlap. c, Proportion of
variance concentrated in the top k weights, as a function of k, for training on a single-task. When both
layers are trained from small initialization (STL), this variance decreases much more rapidly than for
pure linear readout training (STL (LP)), demonstrating the sparsity of the learned solution. d, L1 norm
for STL as a function of the number of samples. e, Proportion of variance across different overlaps
and for different learning setups (see also Fig. 2b). The rapid decrease in variance demonstrates the
sparsity of the learned solutions both for PT+FT and MTL. f, Proportion of weight norm in the 40
dimensions relevant for the auxiliary task (see also Fig. 2b)."
SCALE,0.6545768566493955,"In Fig. 7e we show the sparsity of networks trained on teacher with different values of the number of
overlapping features between main and auxiliary tasks (each of which uses 40 features). We find that
learned solutions across a range of overlaps are as sparse as using single-task learning (see Fig. 7c)"
SCALE,0.6563039723661486,"when task features do not overlap (0/40 case) and more sparse otherwise (on account of the bias
toward reuse of the sparse features learned during the auxiliary task, see next paragraph)."
SCALE,0.6580310880829016,"In Fig. 7f we show the overlap between features active on either task, finding that learned main task
solutions are biased to share auxiliary task features when few samples are available."
SCALE,0.6597582037996546,"D.2
ReLU networks"
SCALE,0.6614853195164075,"We adopt a clustering-based approach to analyzing the effective sparse structure of learned task solu-
tions. Specifically, for a given trained network, we perform k-means clustering with a predetermined
value of K clusters on the normalized input weights to each hidden-layer neuron in the network3.
We measure the extent to which K cluster centers are able to explain the variance in input weights
across hidden units; the fraction of variance left unexplained is commonly referred to as the “inertia.”
For values of K at which the inertia is close to zero, we can say that (to a good approximation) the
network effectively makes use of at most K nonlinear features."
SCALE,0.6632124352331606,"D.2.1
Single-task learning: rich inductive bias yields clusters of similarly tuned neurons that
approximate sparse ground-truth features"
SCALE,0.6649395509499136,"In the single-task learning case, we measure the inertia of trained networks as a function of K. We
find that for networks in the rich regime (small initialization scale), for tasks with sparse ground-truth
(six units in the ReLU teacher network), the networks do indeed learn solutions that make use of
approximately six nonlinear features (Fig. 8a). For tasks with many (1000) units in the teacher
network, the network finds solutions that use a small number of feature clusters when main task
samples are limited, but gradually uses more clusters as the number of samples is increased (Fig. 8a),
at which point the network matches the teacher function very well. This bias towards sparser-than-
ground-truth solutions given insufficient data corroborates our claim of an inductive bias towards
sparse solutions. By contrast, networks in the lazy learning regime (large initialization scale) display
no such bias, corroborating our claim that the sparse ℓ1-like inductive bias is a property of the rich
regime but not the lazy regime. Interestingly, in the sparse ground-truth case learned solutions are
relatively less sparse for an intermediate number of training examples. This may arise because an
ℓ1-like inductive bias is not exactly the same as a bias toward sparse solutions over nonlinear features,
particularly when training data is limited. We leave an in-depth investigation of this phenomenon to
future work."
SCALE,0.6666666666666666,"Our clustering analysis allows us to measure the extent to which the effective features employed
by the network (cluster centers) are aligned with the ground-truth task features. Specifically, for
each teacher unit, we compute an “alignment score” between teacher and student networks by taking
each teacher unit, measuring its cosine similarity with all the cluster centers, choosing the maximum
value, and averaging this quantity across all teacher units. We find that the learned feature clusters
are indeed highly aligned with the ground-truth teacher features in the sparse ground-truth case, and
moreso as the number of main task samples (and consequently task performance) increases (Fig. 8b)."
SCALE,0.6683937823834197,"D.2.2
Pretraining+finetuning finds sparse solutions and improves alignment of feature
clusters learned during pretraining"
SCALE,0.6701208981001727,"We find that pretraining+finetuning improves performance over single-task learning when main and
auxiliary task features are shared (or correlated), and maintains an apparent bias toward sparsity
in new task-specific features. To corroborate these claims, we performed our clustering analysis
on the solutions learned through PT+FT. We find that the solutions learned are indeed quite sparse
(comparable to the sparsity of solutions learned by single-task learning), even when the auxiliary task
and main task features are disjoint (Fig. 8c). Moreover, we find that MTL also learns sparse solutions
(Fig. 8d). In particular, as expected, the effective features on tasks with overlapping features is equal
to the number of total unique features. Moreover, we observe that when main task and auxiliary task
features are shared, PT+FT and MTL networks exhibit higher alignment between learned features
and ground-truth features than single-task-trained networks, especially when main task samples are"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6718480138169257,"3weighting the importance of each unit to the k-means objective by the weight of its contribution to the
network’s input-output function, specifically the magnitude of the product of its associated input and output
weights"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6735751295336787,"Units: 6
Units: 1000"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6753022452504318,"0
6
20 0
6
20
0.0
0.5
1.0"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6770293609671848,# Clusters
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6787564766839378,Inertia a
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6804835924006909,"Units: 6
Units: 1000"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6822107081174439,"64
512
64
512 0.5 1.0"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6839378238341969,# Samples
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6856649395509499,Alignment b 64
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.687392055267703,"512
# Samples"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.689119170984456,Scale of init.
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.690846286701209,"0.001
1"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6925734024179621,"6/6
Corr.
Mixture"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.694300518134715,"0/6
3/6
5/6"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.696027633851468,"0 6
20 0 6
20 0 6
20 0.0 0.5 0.0 0.5"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.697754749568221,# Clusters
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.6994818652849741,Inertia c
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7012089810017271,"0/6
3/6
5/6
6/6
Corr.
Mixture"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7029360967184801,"0
6
20 0
6
20 0
6
20 0
6
20 0
6
20 0
6
20
0.0 0.5"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7046632124352331,# Clusters
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7063903281519862,Inertia d
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7081174438687392,"0/6
3/6
5/6
6/6
Corr.
Mixture"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7098445595854922,"32
256
32
256
32
256
32
256
32
256
32
256 0.5 1.0"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7115716753022453,# Samples
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7132987910189983,Alignment
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7150259067357513,"MTL
PT+FT
STL e"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7167530224525043,"MTL
PT+FT"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7184801381692574,"32 256
32 256
0.0 0.5"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7202072538860104,# Samples
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7219343696027634,Inertia
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7236614853195165,"0.1
1.0
10.0
Weight scaling f"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7253886010362695,"MTL
PT+FT 0.1 PT+FT 1 PT+FT 10"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7271157167530224,"32
256
32
256
32
256
32
256 0.5 1.0"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7288428324697754,# Samples
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7305699481865285,Alignment
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7322970639032815,Same Units
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7340241796200345,"no
yes g"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7357512953367875,"Figure 8: Analysis of effective sparsity of learned ReLU network solutions. a Inertia (k-means
reconstruction error for clustering of hidden-unit input weights) as a function of the number of
clusters used for k-means, for different numbers of main task samples and ground-truth teacher
network units, in single-task learning. b Alignment score – average alignment (across teacher units)
of the best-aligned student network cluster uncovered via k-means. c, Inertia for networks trained
using PT+FT for the tasks of Fig. 2d,e and Fig. 4a. d, Same as panel c but for networks trained with
MTL. e, Alignment score for networks trained with MTL, PT+FT, and STL on the same tasks as in
panels c and d. f Inertia (using k = 1 clusters) for networks trained on an auxiliary task that relies on
only one ground-truth feature, which is one of the six ground-truth features used in the auxiliary task
(as in Fig. 3e,f), using MTL or PT+FT with various rescaling factors applied to the weights prior to
finetuning. g Alignment score for the networks and task in panel f."
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7374784110535406,"limited (Fig. 8e). This provides a mechanistic underpinning for the relationship between the inductive
bias of PT+FT that we describe in the main text and its performance benefits."
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7392055267702936,"D.2.3
Nested feature selection regime allows network to prioritize a sparse subset of feature
clusters learned during pretraining"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7409326424870466,"In the main text we describe the “nested feature selection” regime, which occurs at intermediate
values of the ratio between ground-truth main task feature coefficients and pretrained network weight
scale. In this regime, networks can more efficiently learn main tasks that make use of a subset of the
features used in the auxiliary task. Importantly, they still maintain a bias towards reusing features
from the auxiliary task, as we show for diagonal linear networks and ReLU networks in Fig. 9. Here"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7426597582037997,"MTL
PT+FT 1 PT+FT 0.1 PT+FT 0.01"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7443868739205527,"32
256
32
256
32
256
32
256"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7461139896373057,"1
10−3
10−6"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7478411053540587,# Samples Loss
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7495682210708118,overlap
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7512953367875648,"no
yes a"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7530224525043178,"MTL
PT+FT 10 PT+FT 1 PT+FT 0.1"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7547495682210709,"32
256
32
256
32
256
32
256"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7564766839378239,"0.1
10−4"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7582037996545768,# Samples Loss
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7599309153713298,"Same
Units"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7616580310880829,"no
yes b"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7633851468048359,"Figure 9: Comparison between tasks with sparse main task features that are either subsets of the
auxiliary task features or new features. Networks are trained with MTL or with PT+FT, potentially
with rescaling (as indicated by the number). a, Diagonal linear networks trained on five main task
features. b, ReLU networks trained on a teacher network with one feature. We see that MTL (to
some extent) and PT+FT can benefit from such an overlap, but for small rescaling values, this benefit
becomes smaller."
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7651122625215889,"50
500
5000
Main task samples 0 50 100"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7668393782383419,Accuracy (%)
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.768566493955095,"ResNet18, STL10"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.770293609671848,"PT+FT
PT+rescaling+FT"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.772020725388601,"500
5000
50000
Main task samples 0 50 100"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7737478411053541,"VGG11, CIFAR100"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7754749568221071,"500
5000
50000
Main task samples 0 50 100"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7772020725388601,"ResNet18, CIFAR100"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7789291882556131,"no rescaling
rescaling
0 2"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7806563039723662,Error (%)
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7823834196891192,"ResNet18, STL10 (binary)"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7841105354058722,"no rescaling
rescaling
0 2"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7858376511226253,Error (%)
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7875647668393783,"VGG11, CIFAR100 (binary)"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7892918825561313,"no rescaling
rescaling
0 5"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7910189982728842,Error (%)
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7927461139896373,"ResNet18, CIFAR100 (binary)"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7944732297063903,"Figure 10: Results for finetuning deep convolutional networks trained on ImageNet, with/without
weight rescaling (factor of 0.5) prior to finetuning."
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7962003454231433,"we show that networks in this regime (obtained most clearly in the shallow ReLU network case
when networks are rescaled by a value of 1.0 after pretraining) indeed learn very sparse (effectively
1-feature) solutions when the ground-truth main task consists of a single auxiliary task features
(Fig. 8e, right). By contrast, networks with weights rescaled by a factor of 10.0 following pretraining
exhibit no such nested sparsity bias (consistent with lazy-regime behavior). Similarly, multi-task
networks cannot exhibit such a bias in their internal representation as they still need to maintain the
features needed for the main task (Fig. 8e, left). Additionally, supporting the idea that the nested
feature selection regime allows networks to benefit from feature reuse, we find that networks in
this regime exhibit a higher alignment score with the ground-truth teacher network when the main
task features are a subset of the auxiliary task features compared to when they are disjoint from the
auxiliary task features (Fig. 8g). This alignment benefit is mostly lost when networks are rescaled by
a factor of 0.1 following pretrainning (a signature of rich-regime-like behavior)."
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7979274611398963,"E
Further evaluations of the rescaling method for finetuning"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.7996545768566494,"To evaluate the robustness / general-purpose utility of our suggested approach of rescaling network
weights following pretraining, we experimented with finetuning convolutional networks pretrained
on ImageNet on downstream classification tasks: pretrained ResNet-18 finetuned on CIFAR100,
pretrained VGG11 finetuned on CIFAR100, and pretrained ResNet-18 finetuned on STL-10. We
experimented both with finetuning on the full multi-way classification task, and also on binary
classification tasks obtained by subsampling pairs of classes from the main task dataset (which we
found exposes performance differences more strongly). Due to computational constraints, we did not
sweep over the choice of the rescaling factor, but simply used a factor of 0.5 in all cases. We find that
rescaling improves finetuning performance, to varying degrees, in all of our experiments (Fig. 10)."
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8013816925734024,"F
Analysis of representations learned in the nested feature selection regime:
bridging the gap from shallow to deep networks"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8031088082901554,"We were interested in testing whether our theoretical understanding of shallow networks is truly
responsible for the behavior of deeper networks (with more direct evidence than performance results
/ sample complexity). Specifically, we sought to understand whether the observed benefit of rescaling
network weights following pretraining (Fig. 5b, Appendix E) relates to the nested feature selection
regime we characterized in shallow networks. Doing so is challenging, as the space of “features”
learnable by a deep network is difficult to characterize explicitly (making the feature clustering
analysis employed in Appendix D inapplicable). To circumvent this issue, we propose a signature of
nested feature selection that can be characterized without knowledge of the underlying feature space.
Specifically, we propose to measure (1) the dimensionality of the network representation pre- and
post-finetuning, and (2) the extent to which the representational structure post-finetuning is shared
with / inherited from that of the network following pretraining prior to finetuning."
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8048359240069085,"We employ the commonly used participation ratio [PR; 52] as a measure of dimensionality. For an
n × p matrix X representing n mean-centered samples of p-dimensional network responses, with a
p × p covariance matrix CX = 1"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8065630397236615,"nX⊤X, the participation ratio is defined as"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8082901554404145,"PR(X) = (Pp
i=1 λi)2
Pp
i=1 λ2
i
= trace (CX)2"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8100172711571675,"trace (C2
X)"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8117443868739206,"=
trace
 
X⊤X
2"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8134715025906736,"trace (X⊤XX⊤X)
(36)"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8151986183074266,"where λi are the eigenvalues of the covariance matrix CX. The PR scales from 1 to p and measures the
extent to which the covariance structure of responses X is dominated by a few principal components
or is spread across many. We argue that low-dimensional representations are a signature of networks
that use a sparse set of features. We confirm that this is the case in our teacher-student setting:
networks in the rich regime, which are biased towards sparse solutions, learn representations with
lower PR than networks in the lazy regime, which are not biased toward sparse solutions (Fig. 11a)."
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8169257340241797,"Our measure of shared dimensionality between two representations is the effective number of shared
dimensions (ENSD) introduced by Giaffar et al. [53]. The ENSD for an n × p matrix of responses X
from one network and an n×p matrix of responses Y from another network, denoted ENSD(X, Y ),
is given by"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8186528497409327,"trace
 
Y⊤XX⊤Y

· trace
 
X⊤X

· trace
 
Y⊤Y
"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8203799654576857,"trace (X⊤XX⊤X) · trace (Y⊤YY⊤Y)
(37)"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8221070811744386,"This measure is equal to the centered kernel alignment (CKA), a measure of similarity of two
network representations [56], multiplied by the geometric mean of the participation ratios of the two
representations. It measures an intuitive notion of “shared dimensions” — for example, if X consists
of 10 uncorrelated units, if Y is taken from a subset of five of those units, the ENSD(X, Y) will be 5. 4 6 8"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8238341968911918,"64
512
# Samples"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8255613126079447,"Participation
Ratio"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8272884283246977,Scale of init.
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8290155440414507,"1
0.001 a"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8307426597582038,"0.1
1
10"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8324697754749568,"32
256
32
256
32
256
1 4"
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8341968911917098,# Samples
WEIGHTING THE IMPORTANCE OF EACH UNIT TO THE K-MEANS OBJECTIVE BY THE WEIGHT OF ITS CONTRIBUTION TO THE,0.8359240069084629,"Participation
Ratio 1"
UNITS,0.8376511226252159,"6
Units b"
UNITS,0.8393782383419689,"PT PR
FT PR
PT/FT ENSD"
UNITS,0.8411053540587219,"32
256
32
256
32
256
1 4"
UNITS,0.842832469775475,# Samples
UNITS,0.844559585492228,"0.1
1.0
10.0
Weight scaling c"
UNITS,0.846286701208981,"PT PR
FT PR
PT/FT ENSD"
UNITS,0.8480138169257341,"0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
2
3
4
5
6 Layer 0.125"
WEIGHT SCALING,0.8497409326424871,"1
Weight scaling d"
WEIGHT SCALING,0.8514680483592401,"Figure 11: Dimensionality of the network representations before and after finetuning. a, Participation
ratio of the ReLU networks’ internal representation after training on a task with six teacher units.
b, Participation ratio of the network representation after finetuning on the nested sparsity task with
different weight rescalings. c, Participation ratio before (left panel) and after finetuning (middle panel)
and the effective number of shared dimensions between the two representations (right panel). Small
weight scaling decreases the participation ratio after training. d, The same quantities for ResNet18
before and after finetuning (see also Fig. 5c)."
WEIGHT SCALING,0.853195164075993,"If Y is taken to be five uncorrelated units that are themselves uncorrelated with all those in X, the
ENSD(X, Y) will be zero."
WEIGHT SCALING,0.8549222797927462,"Intuitively, the PR and ENSD of network representations pre- and post-finetuning capture the key phe-
nomena of the nested feature selection regime. In a case in which the main task uses a subset of the fea-
tures of the auxiliary task, if the network truly extracts this sparse subset of features, we expect the di-
mensionality of network after finetuning to be lower than after pretraining (PR(XF T ) < PR(XP T )),
and for nearly all of the representational dimensions expressed by the network post-finetuning to be
inherited from the network state after pretraining (ENSD(XP T , XF T ) ≈PR(XF T )). By contrast,
networks not in the nested feature selection regime should exhibit an ℓ2-like rather than ℓ1-like bias
with respect to features inherited from pretraining and thus not exhibit a substantial decrease in
dimensionality during finetuning."
WEIGHT SCALING,0.8566493955094991,"We show that this description holds in our nonlinear teacher-student experiments. Networks that we
identified as being in the “nested feature selection” regime (weights rescaled by 1.0 following pre-
training), and also networks in the rich regime, exhibit decreased PR following finetuning (Fig. 11b).
By contrast, lazy networks (weights rescaled by 10.0 following pretraining) exhibit no dimensionality
decrease during finetuning. Additionally (see Fig. 11c), the ENSD between pretrained (PT) and
finetuned (FT) networks is almost identical to the dimensionality of the finetuned representation (PR
FT)."
WEIGHT SCALING,0.8583765112262521,"Strikingly, we observe very similar behavior in our ResNet-18 model pretrained on 98 CIFAR-100
classes and finetuned on the 2 remaining classes (Fig. 11d), when we apply our method of rescaling
weights post-finetuning. Analyzing the PR and ENSD of the outputs of different stages of the network
following pretraining and following finetuning, we see that dimensionality decreases with finetuning,
and ENSD between the pretrained and finetuned networks is very close to the PR of the finetuned
network. Moreover, this phenomenology is only observed when we apply the weight rescaling
method; finetuning the raw pretrained network yields no dimensionality decrease. These results
suggest that the success of our rescaling method may indeed be attributable to pushing the network
into the nested feature selection regime."
WEIGHT SCALING,0.8601036269430051,NeurIPS Paper Checklist
CLAIMS,0.8618307426597582,1. Claims
CLAIMS,0.8635578583765112,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8652849740932642,Answer: [Yes]
CLAIMS,0.8670120898100173,"Justification: We characterize the inductive bias associated with MTL and PT+FT by
characterizing existing penalties and deriving a new penalty for PT+FT in ReLU networks,
which we do in Section 3. We then test these inductive biases to understand how they
manifest in practice in Section 4 and find that they are relevant for large-scale neural network
architectures in Section 5."
CLAIMS,0.8687392055267703,Guidelines:
CLAIMS,0.8704663212435233,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8721934369602763,2. Limitations
LIMITATIONS,0.8739205526770294,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8756476683937824,Answer: [Yes]
LIMITATIONS,0.8773747841105354,Justification: We discuss limitations in the latter half of Section 6.
LIMITATIONS,0.8791018998272885,Guidelines:
LIMITATIONS,0.8808290155440415,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8825561312607945,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8842832469775475,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8860103626943006,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8877374784110535,"Justification: We derive 1) the penalty associated with MTL in ReLU networks in Ap-
pendix A.1, 2) the penalty associated with PT+FT in ReLU networks in Appendix A.3, and
3) the conservation law (Prop. 4) in Appendix A.4."
THEORY ASSUMPTIONS AND PROOFS,0.8894645941278065,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8911917098445595,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8929188255613126,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8946459412780656,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8963730569948186,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8981001727115717,"Justification: We describe the way in which we generate the teacher-student experiments
in Section 3.3 and further discuss our methods in detail in Appendix B. Finally we are
providing a codebase to reproduce all figure in the main text."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8998272884283247,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9015544041450777,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9032815198618307,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9050086355785838,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9067357512953368,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9084628670120898,"Answer: [Yes]
Justification: We have attached a codebase that enables reproduction of all figures in the
main text."
OPEN ACCESS TO DATA AND CODE,0.9101899827288429,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9119170984455959,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9136442141623489,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9153713298791019,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.917098445595855,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.918825561312608,Justification: We provide these details in Appendix B
OPEN ACCESS TO DATA AND CODE,0.9205526770293609,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9222797927461139,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.924006908462867,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.92573402417962,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.927461139896373,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9291882556131261,"Justification: For all experiments, we report the mean ± two standard deviations."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9309153713298791,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9326424870466321,• The answer NA means that the paper does not include experiments.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9343696027633851,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9360967184801382,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We describe these resources in Appendix B.5.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9378238341968912,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9395509499136442,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: This manuscript presents foundational research without direct path to negative
applications. Further, the manuscript does not present any experiments involving human
subjects.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9412780656303973,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9430051813471503,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This manuscript presents foundational research without direct societal impact."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9447322970639033,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9464594127806563,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9481865284974094,11. Safeguards
SAFEGUARDS,0.9499136442141624,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9516407599309153,Answer: [NA]
SAFEGUARDS,0.9533678756476683,"Justification: This manuscript presents foundational research and there is no clear potential
risk of misuse."
SAFEGUARDS,0.9550949913644214,Guidelines:
SAFEGUARDS,0.9568221070811744,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9585492227979274,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9602763385146805,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9620034542314335,"Answer: [Yes]
Justification: We use two standard vision datasets: CIFAR-100 and Imagenet. For CIFAR-
100, we were unable to find an associated license. For Imagenet, the data is available for
free to researchers for non-commercial use. Both websites hosting these datasets request a
citation of the relevant paper which we provided."
LICENSES FOR EXISTING ASSETS,0.9637305699481865,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9654576856649395,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset."
LICENSES FOR EXISTING ASSETS,0.9671848013816926,"• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9689119170984456,13. New Assets
NEW ASSETS,0.9706390328151986,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9723661485319517,Answer: [NA]
NEW ASSETS,0.9740932642487047,Justification: We do not release new assets.
NEW ASSETS,0.9758203799654577,Guidelines:
NEW ASSETS,0.9775474956822107,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792746113989638,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810017271157168,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827288428324698,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844559585492227,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861830742659758,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879101899827288,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896373056994818,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913644214162349,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930915371329879,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948186528497409,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965457685664939,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998272884283247,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
