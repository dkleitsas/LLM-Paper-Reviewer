Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00196078431372549,"We give the first efficient algorithm for learning halfspaces in the testable learning
1"
ABSTRACT,0.00392156862745098,"model recently defined by Rubinfeld and Vasilyan [RV23]. In this model, a learner
2"
ABSTRACT,0.0058823529411764705,"certifies that the accuracy of its output hypothesis is near optimal whenever the
3"
ABSTRACT,0.00784313725490196,"training set passes an associated test, and training sets drawn from some target
4"
ABSTRACT,0.00980392156862745,"distribution must pass the test. This model is more challenging than distribution-
5"
ABSTRACT,0.011764705882352941,"specific agnostic or Massart noise models where the learner is allowed to fail
6"
ABSTRACT,0.013725490196078431,"arbitrarily if the distributional assumption does not hold. We consider the setting
7"
ABSTRACT,0.01568627450980392,"where the target distribution is the standard Gaussian in d dimensions and the
8"
ABSTRACT,0.01764705882352941,"label noise is either Massart or adversarial (agnostic). For Massart noise, our
9"
ABSTRACT,0.0196078431372549,"tester-learner runs in polynomial time and outputs a hypothesis with (information-
10"
ABSTRACT,0.021568627450980392,"theoretically optimal) error opt + ϵ (and extends to any fixed strongly log-concave
11"
ABSTRACT,0.023529411764705882,"target distribution). For adversarial noise, our tester-learner obtains error O(opt)+ϵ
12"
ABSTRACT,0.025490196078431372,"in polynomial time. Prior work on testable learning ignores the labels in the
13"
ABSTRACT,0.027450980392156862,"training set and checks that the empirical moments of the covariates are close to
14"
ABSTRACT,0.029411764705882353,"the moments of the base distribution. Here we develop new tests of independent
15"
ABSTRACT,0.03137254901960784,"interest that make critical use of the labels and combine them with the moment-
16"
ABSTRACT,0.03333333333333333,"matching approach of [GKK23]. This enables us to implement a testable variant
17"
ABSTRACT,0.03529411764705882,"of the algorithm of [DKTZ20a, DKTZ20b] for learning noisy halfspaces using
18"
ABSTRACT,0.03725490196078431,"nonconvex SGD.
19"
INTRODUCTION,0.0392156862745098,"1
Introduction
20"
INTRODUCTION,0.041176470588235294,"Learning halfspaces in the presence of noise is one of the most basic and well-studied problems in
21"
INTRODUCTION,0.043137254901960784,"computational learning theory. A large body of work has obtained results for this problem under a
22"
INTRODUCTION,0.045098039215686274,"variety of different noise models and distributional assumptions (see e.g. [BH21] for a survey). A
23"
INTRODUCTION,0.047058823529411764,"major issue with common distributional assumptions such as Gaussianity, however, is that they can
24"
INTRODUCTION,0.049019607843137254,"be hard or impossible to verify in the absence of any prior information.
25"
INTRODUCTION,0.050980392156862744,"The recently defined model of testable learning [RV23] addresses this issue by replacing such
26"
INTRODUCTION,0.052941176470588235,"assumptions with efficiently testable ones. In this model, the learner is required to work with an
27"
INTRODUCTION,0.054901960784313725,"arbitrary input distribution DXY and verify any assumptions it needs to succeed. It may choose to
28"
INTRODUCTION,0.056862745098039215,"reject a given training set, but if it accepts, it is required to output a hypothesis with error close to
29"
INTRODUCTION,0.058823529411764705,"opt(C, DXY), the optimal error achievable over DXY by any function in a concept class C. Further,
30"
INTRODUCTION,0.060784313725490195,"whenever the training set is drawn from a distribution DXY whose marginal is truly a well-behaved
31"
INTRODUCTION,0.06274509803921569,"target distribution D∗(such as the standard Gaussian), the algorithm is required to accept with high
32"
INTRODUCTION,0.06470588235294118,"probability. Such an algorithm, or tester-learner, is then said to testably learn C with respect to target
33"
INTRODUCTION,0.06666666666666667,"marginal D∗. (See Definition 2.1.) Note that unlike ordinary distribution-specific agnostic learners, a
34"
INTRODUCTION,0.06862745098039216,"tester-learner must take some nontrivial action regardless of the input distribution.
35"
INTRODUCTION,0.07058823529411765,"The work of [RV23, GKK23] established foundational algorithmic and statistical results for this
36"
INTRODUCTION,0.07254901960784314,"model and showed that testable learning is in general provably harder than ordinary distribution-
37"
INTRODUCTION,0.07450980392156863,"specific agnostic learning. As one of their main algorithmic results, they showed tester-learners for
38"
INTRODUCTION,0.07647058823529412,"the class of halfspaces over Rd that succeed whenever the target marginal is Gaussian (or one of a
39"
INTRODUCTION,0.0784313725490196,"more general class of distributions), achieving error opt + ϵ in time and sample complexity d e
O(1/ϵ2).
40"
INTRODUCTION,0.0803921568627451,"This matches the running time of ordinary distribution-specific agnostic learning of halfspaces over
41"
INTRODUCTION,0.08235294117647059,"the Gaussian using the standard approach of [KKMS08]. Their testers are simple and label-oblivious,
42"
INTRODUCTION,0.08431372549019608,"and are based on checking whether the low-degree empirical moments of the unknown marginal
43"
INTRODUCTION,0.08627450980392157,"match those of the target D∗.
44"
INTRODUCTION,0.08823529411764706,"These works essentially resolve the question of designing tester-learners achieving error opt + ϵ
45"
INTRODUCTION,0.09019607843137255,"for halfspaces, matching known hardness results for (ordinary) agnostic learning [GGK20, DKZ20,
46"
INTRODUCTION,0.09215686274509804,"DKPZ21]. Their running time, however, necessarily scales exponentially in 1/ϵ.
47"
INTRODUCTION,0.09411764705882353,"A long line of research has sought to obtain more efficient algorithms at the cost of relaxing the
48"
INTRODUCTION,0.09607843137254903,"optimality guarantee [ABL17, DKS18, DKTZ20a, DKTZ20b]. These works give polynomial-time
49"
INTRODUCTION,0.09803921568627451,"algorithms achieving bounds of the form opt + ϵ and O(opt) + ϵ for the Massart and agnostic setting
50"
INTRODUCTION,0.1,"respectively under structured distributions (see Section 1.1 for more discussion). The main question
51"
INTRODUCTION,0.10196078431372549,"we consider here is whether such guarantees can be obtained in the testable learning framework.
52"
INTRODUCTION,0.10392156862745099,"Our contributions. In this work we design the first tester-learners for halfspaces that run in fully
53"
INTRODUCTION,0.10588235294117647,"polynomial time in all parameters. We match the optimality guarantees of fully polynomial-time
54"
INTRODUCTION,0.10784313725490197,"learning algorithms under Gaussian marginals for the Massart noise model (where the labels arise
55"
INTRODUCTION,0.10980392156862745,"from a halfspace but are flipped by an adversary with probability at most η) as well as for the agnostic
56"
INTRODUCTION,0.11176470588235295,"model (where the labels can be completely arbitrary). In fact, for the Massart setting our guarantee
57"
INTRODUCTION,0.11372549019607843,"holds with respect to any chosen target marginal D∗that is isotropic and strongly log-concave, and
58"
INTRODUCTION,0.11568627450980393,"the same is true of the agnostic setting albeit with a slightly weaker guarantee.
59"
INTRODUCTION,0.11764705882352941,"Theorem 1.1 (Formally stated as Theorem 4.1). Let C be the class of origin-centered halfspaces over
60"
INTRODUCTION,0.11960784313725491,"Rd, and let D∗be any isotropic strongly log-concave distribution. In the setting where the labels are
61"
INTRODUCTION,0.12156862745098039,corrupted with Massart noise at rate at most η < 1
INTRODUCTION,0.12352941176470589,"2, C can be testably learned w.r.t. D∗up to error
62"
INTRODUCTION,0.12549019607843137,"opt + ϵ using poly(d, 1"
INTRODUCTION,0.12745098039215685,"ϵ ,
1
1−2η) time and sample complexity.
63"
INTRODUCTION,0.12941176470588237,"Theorem 1.2 (Formally stated as Theorem 5.1). Let C be as above. In the adversarial noise or
64"
INTRODUCTION,0.13137254901960785,"agnostic setting where the labels are completely arbitrary, C can be testably learned w.r.t. N(0, Id)
65"
INTRODUCTION,0.13333333333333333,"up to error O(opt) + ϵ using poly(d, 1"
INTRODUCTION,0.13529411764705881,"ϵ ) time and sample complexity.
66"
INTRODUCTION,0.13725490196078433,"Our techniques. The tester-learners we develop are significantly more involved than prior work on
67"
INTRODUCTION,0.1392156862745098,"testable learning. We build on the nonconvex optimization approach to learning noisy halfspaces
68"
INTRODUCTION,0.1411764705882353,"due to [DKTZ20a, DKTZ20b] as well as the structural results on fooling functions of halfspaces
69"
INTRODUCTION,0.14313725490196078,"using moment matching due to [GKK23]. Unlike the label-oblivious, global moment tests of
70"
INTRODUCTION,0.1450980392156863,"[RV23, GKK23], our tests make crucial use of the labels and check local properties of the distribution
71"
INTRODUCTION,0.14705882352941177,"in regions described by certain candidate vectors. These candidates are approximate stationary points
72"
INTRODUCTION,0.14901960784313725,"of a natural nonconvex surrogate of the 0-1 loss, obtained by running gradient descent. When the
73"
INTRODUCTION,0.15098039215686274,"distribution is known to be well-behaved, [DKTZ20a, DKTZ20b] showed that any such stationary
74"
INTRODUCTION,0.15294117647058825,"point is in fact a good solution (for technical reasons we must use a slightly different surrogate
75"
INTRODUCTION,0.15490196078431373,"loss). Their proof relies crucially on structural geometric properties that hold for these well-behaved
76"
INTRODUCTION,0.1568627450980392,"distributions, an important one being that the probability mass of any region close to the origin is
77"
INTRODUCTION,0.1588235294117647,"proportional to its geometric measure.
78"
INTRODUCTION,0.1607843137254902,"In the testable learning setting, we must efficiently check this property for candidate solutions. Since
79"
INTRODUCTION,0.1627450980392157,"these regions may be described as intersections of halfspaces, we may hope to apply the moment-
80"
INTRODUCTION,0.16470588235294117,"matching framework of [GKK23]. Naïvely, however, they only allow us to check in polynomial time
81"
INTRODUCTION,0.16666666666666666,"that the probability masses of such regions are within an additive constant of what they should be
82"
INTRODUCTION,0.16862745098039217,"under the target marginal. But we can view these regions as sub-regions of a known band described
83"
INTRODUCTION,0.17058823529411765,"by our candidate vector. By running moment tests on the distribution conditioned on this band and
84"
INTRODUCTION,0.17254901960784313,"exploiting the full strength of the moment-matching framework, we are able to effectively convert our
85"
INTRODUCTION,0.17450980392156862,"weak additive approximations to good multiplicative ones. This allows us to argue that our stationary
86"
INTRODUCTION,0.17647058823529413,"points are indeed good solutions.
87"
INTRODUCTION,0.1784313725490196,"Limitations and Future Work. In this paper we provide the first efficient tester-learners for
88"
INTRODUCTION,0.1803921568627451,"halfspaces when the noise is either adversarial or Massart. An interesting direction for future work
89"
INTRODUCTION,0.18235294117647058,"would be to design tester-learners for the agnostic setting whose target marginal distributions may
90"
INTRODUCTION,0.1843137254901961,"lie within a large family (e.g., strongly log-concave distributions) but still achieve error of O(opt).
91"
INTRODUCTION,0.18627450980392157,"Another interesting direction is providing tester-learners that are not tailored to a single target
92"
INTRODUCTION,0.18823529411764706,"distribution, but are guaranteed to accept any member of a large family of distributions.
93"
RELATED WORK,0.19019607843137254,"1.1
Related work
94"
RELATED WORK,0.19215686274509805,"We provide a partial summary of some of the most relevant prior and related work on efficient
95"
RELATED WORK,0.19411764705882353,"algorithms for learning halfspaces in the presence of adversarial label or Massart noise, and refer the
96"
RELATED WORK,0.19607843137254902,"reader to [BH21] for a survey.
97"
RELATED WORK,0.1980392156862745,"In the distribution-specific agnostic setting where the marginal is assumed to be isotropic and log-
98"
RELATED WORK,0.2,"concave, [KLS09] showed an algorithm achieving error O(opt1/3)+ϵ for the class of origin-centered
99"
RELATED WORK,0.2019607843137255,"halfspaces. [ABL17] later obtained O(opt) + ϵ using an approach that introduced the principle of
100"
RELATED WORK,0.20392156862745098,"iterative localization, where the learner focuses attention on a band around a candidate halfspace in
101"
RELATED WORK,0.20588235294117646,"order to produce an improved candidate. [Dan15] used this principle to obtain a PTAS for agnostically
102"
RELATED WORK,0.20784313725490197,"learning halfspaces under the uniform distribution on the sphere, and [BZ17] extended it to more
103"
RELATED WORK,0.20980392156862746,"general s-concave distributions. Further works in this line include [YZ17, Zha18, ZSA20, ZL21].
104"
RELATED WORK,0.21176470588235294,"[DKTZ20b] introduced the simplest approach yet, based entirely on nonconvex SGD, and showed
105"
RELATED WORK,0.21372549019607842,"that it achieves O(opt) + ϵ for origin-centered halfspaces over a wide class of structured distributions.
106"
RELATED WORK,0.21568627450980393,"Other related works include [DKS18, DKTZ22].
107"
RELATED WORK,0.21764705882352942,"In the Massart noise setting with noise rate bounded by η, work of [DGT19] gave the first efficient
108"
RELATED WORK,0.2196078431372549,"distribution-free algorithm achieving error η + ϵ; further improvements and followups include
109"
RELATED WORK,0.22156862745098038,"[DKT21, DTK22]. However, the optimal error opt achievable by a halfspace may be much smaller
110"
RELATED WORK,0.2235294117647059,"than η, and it has been shown that there are distributions where achieving error competitive with opt
111"
RELATED WORK,0.22549019607843138,"as opposed to η is computationally hard [DK22, DKMR22]. As a result, the distribution-specific
112"
RELATED WORK,0.22745098039215686,"setting remains well-motivated for Massart noise. Early distribution-specific algorithms were given
113"
RELATED WORK,0.22941176470588234,"by [ABHU15, ABHZ16], but a key breakthrough was the nonconvex SGD approach introduced by
114"
RELATED WORK,0.23137254901960785,"[DKTZ20a], which achieved error opt+ϵ for origin-centered halfspaces efficiently over a wide range
115"
RELATED WORK,0.23333333333333334,"of distributions. This was later generalized by [DKK+22].
116"
TECHNICAL OVERVIEW,0.23529411764705882,"1.2
Technical overview
117"
TECHNICAL OVERVIEW,0.2372549019607843,"Our starting point is the nonconvex optimization approach to learning noisy halfspaces due to
118"
TECHNICAL OVERVIEW,0.23921568627450981,"[DKTZ20a, DKTZ20b]. The algorithms in these works consist of running SGD on a natural non-
119"
TECHNICAL OVERVIEW,0.2411764705882353,"convex surrogate Lσ for the 0-1 loss, namely a smooth version of the ramp loss. The key structural
120"
TECHNICAL OVERVIEW,0.24313725490196078,"property shown is that if the marginal distribution is structured (e.g. log-concave) and the slope of
121"
TECHNICAL OVERVIEW,0.24509803921568626,"the ramp is picked appropriately, then any w that has large angle with an optimal w∗cannot be an
122"
TECHNICAL OVERVIEW,0.24705882352941178,"approximate stationary point of the surrogate loss Lσ, i.e. that ∥∇Lσ(w)∥must be large. This is
123"
TECHNICAL OVERVIEW,0.24901960784313726,"proven by carefully analyzing the contributions to the gradient norm from certain critical regions of
124"
TECHNICAL OVERVIEW,0.25098039215686274,"span(w, w∗), and crucially using the distributional assumption that the probability masses of these
125"
TECHNICAL OVERVIEW,0.2529411764705882,"regions are proportional to their geometric measures. (See Fig. 3.) In the testable learning setting,
126"
TECHNICAL OVERVIEW,0.2549019607843137,"the main challenge we face in adapting this approach is checking such a property for the unknown
127"
TECHNICAL OVERVIEW,0.2568627450980392,"distribution we have access to.
128"
TECHNICAL OVERVIEW,0.25882352941176473,"A preliminary observation is that the critical regions of span(w, w∗) that we need to analyze are
129"
TECHNICAL OVERVIEW,0.2607843137254902,"rectangles, and are hence functions of a small number of halfspaces. Encouragingly, one of the key
130"
TECHNICAL OVERVIEW,0.2627450980392157,"structural results of the prior work of [GKK23] pertains to “fooling” such functions. Concretely, they
131"
TECHNICAL OVERVIEW,0.2647058823529412,"show that whenever the true marginal DX matches moments of degree at most eO(1/τ 2) with a target
132"
TECHNICAL OVERVIEW,0.26666666666666666,"D∗that satisfies suitable concentration and anticoncentration properties, then | EDX [f]−ED∗[f]| ≤τ
133"
TECHNICAL OVERVIEW,0.26862745098039215,"for any f that is a function of a small number of halfspaces. If we could run such a test and ensure
134"
TECHNICAL OVERVIEW,0.27058823529411763,"that the probabilities of the critical regions over our empirical marginal are also related to their areas,
135"
TECHNICAL OVERVIEW,0.2725490196078431,"then we would have a similar stationary point property.
136"
TECHNICAL OVERVIEW,0.27450980392156865,"However, the difficulty is that since we wish to run in fully polynomial time, we can only hope to fool
137"
TECHNICAL OVERVIEW,0.27647058823529413,"such functions up to τ that is a constant. Unfortunately, this is not sufficient to analyze the probability
138"
TECHNICAL OVERVIEW,0.2784313725490196,"masses of the critical regions we care about as they may be very small.
139"
TECHNICAL OVERVIEW,0.2803921568627451,"The chief insight that lets us get around this issue is that each critical region R is in fact of a very spe-
140"
TECHNICAL OVERVIEW,0.2823529411764706,"cific form, namely a rectangle that is axis-aligned with w: R = {x : ⟨w, x⟩∈[−σ, σ] and ⟨v, x⟩∈
141"
TECHNICAL OVERVIEW,0.28431372549019607,"[α, β]} for some values α, β, σ and some v orthogonal to w. Moreover, we know w, meaning
142"
TECHNICAL OVERVIEW,0.28627450980392155,"we can efficiently estimate the probability PDX [⟨w, x⟩∈[−σ, σ]] up to constant multiplicative
143"
TECHNICAL OVERVIEW,0.28823529411764703,"factors without needing moment tests. Denoting the band {x : ⟨w, x⟩∈[−σ, σ]} by T and
144"
TECHNICAL OVERVIEW,0.2901960784313726,"writing PDX [R] = PDX [⟨v, x⟩∈[α, β] | x ∈T] PDX [T], it turns out that we should expect
145"
TECHNICAL OVERVIEW,0.29215686274509806,"PDX [⟨v, x⟩∈[α, β] | x ∈T] = Θ(1), as this is what would occur under the structured target distri-
146"
TECHNICAL OVERVIEW,0.29411764705882354,"bution D∗. (Such a “localization” property is also at the heart of the algorithms for approximately
147"
TECHNICAL OVERVIEW,0.296078431372549,"learning halfspaces of, e.g., [ABL17, Dan15].) To check this, it suffices to run tests that ensure that
148"
TECHNICAL OVERVIEW,0.2980392156862745,"PDX [⟨v, x⟩∈[α, β] | x ∈T] is within an additive constant of this probability under D∗.
149"
TECHNICAL OVERVIEW,0.3,"We can now describe the core of our algorithm (omitting some details such as the selection of the
150"
TECHNICAL OVERVIEW,0.30196078431372547,"slope of the ramp). First, we run SGD on the surrogate loss L to arrive at an approximate stationary
151"
TECHNICAL OVERVIEW,0.30392156862745096,"point and candidate vector w (technically a list of such candidates). Then, we define the band T
152"
TECHNICAL OVERVIEW,0.3058823529411765,"based on w, and run tests on the empirical distribution conditioned on T. Specifically, we check
153"
TECHNICAL OVERVIEW,0.307843137254902,"that the low-degree empirical moments conditioned on T match those of D∗conditioned on T,
154"
TECHNICAL OVERVIEW,0.30980392156862746,"and then apply the structural result of [GKK23] to ensure conditional probabilities of the form
155"
TECHNICAL OVERVIEW,0.31176470588235294,"PDX [⟨v, x⟩∈[α, β] | x ∈T] match PD∗[⟨v, x⟩∈[α, β] | x ∈T] up to a suitable additive constant.
156"
TECHNICAL OVERVIEW,0.3137254901960784,"This suffices to ensure that even over our empirical marginal, the particular stationary point w we
157"
TECHNICAL OVERVIEW,0.3156862745098039,"have is indeed close in angular distance to an optimal w∗.
158"
TECHNICAL OVERVIEW,0.3176470588235294,"A final hurdle that remains, often taken for granted under structured distributions, is that closeness
159"
TECHNICAL OVERVIEW,0.3196078431372549,"in angular distance ∡(w, w∗) does not immediately translate to closeness in terms of agreement,
160"
TECHNICAL OVERVIEW,0.3215686274509804,"P[sign(⟨w, x⟩) ̸= sign(⟨w∗, x⟩)], over our unknown marginal. Nevertheless, we show that when
161"
TECHNICAL OVERVIEW,0.3235294117647059,"the target distribution is Gaussian, we can run polynomial-time tests that ensure that an angle of
162"
TECHNICAL OVERVIEW,0.3254901960784314,"θ = ∡(w, w∗) translates to disagreement of at most O(θ). When the target distribution is a general
163"
TECHNICAL OVERVIEW,0.32745098039215687,"strongly log-concave distribution, we show a slightly weaker relationship: for any k ∈N, we can
164"
TECHNICAL OVERVIEW,0.32941176470588235,"run tests requiring time d e
O(k) that ensure that an angle of θ translates to disagreement of at most
165 O(
√"
TECHNICAL OVERVIEW,0.33137254901960783,"k · θ1−1/k). In the Massart noise setting, we can make ∡(w, w∗) arbitrarily small, and so obtain
166"
TECHNICAL OVERVIEW,0.3333333333333333,"our opt + ϵ guarantee for any target strongly log-concave distribution in polynomial time. In the
167"
TECHNICAL OVERVIEW,0.3352941176470588,"adversarial noise setting, we face a more delicate tradeoff and can only make ∡(w, w∗) as small
168"
TECHNICAL OVERVIEW,0.33725490196078434,"as Θ(opt). When the target distribution is Gaussian, this is enough to obtain final error O(opt) + ϵ
169"
TECHNICAL OVERVIEW,0.3392156862745098,"in polynomial time. When the target distribution is a general strongly log-concave distribution, we
170"
TECHNICAL OVERVIEW,0.3411764705882353,"instead obtain eO(opt) + ϵ in quasipolynomial time.
171"
PRELIMINARIES,0.3431372549019608,"2
Preliminaries
172"
PRELIMINARIES,0.34509803921568627,"Notation and setup
Throughout, the domain will be X = Rd, and labels will lie in Y = {±1}.
173"
PRELIMINARIES,0.34705882352941175,"The unknown joint distribution over X × Y that we have access to will be denoted by DXY, and its
174"
PRELIMINARIES,0.34901960784313724,"marginal on X will be denoted by DX . The target marginal on X will be denoted by D∗. We use
175"
PRELIMINARIES,0.3509803921568627,"the following convention for monomials: for a multi-index α = (α1, . . . , αd) ∈Zd
≥0, xα denotes
176
Q"
PRELIMINARIES,0.35294117647058826,"i xαi
i , and |α| = P"
PRELIMINARIES,0.35490196078431374,"i αi denotes its total degree. We use C to denote a concept class mapping
177"
PRELIMINARIES,0.3568627450980392,"Rd to {±1}, which throughout this paper will be the class of halfspaces or functions of halfspaces
178"
PRELIMINARIES,0.3588235294117647,"over Rd. We use opt(C, DXY) to denote the optimal error inff∈C P(x,y)∼DXY[f(x) ̸= y], or just opt
179"
PRELIMINARIES,0.3607843137254902,"when C and DXY are clear from context. We recall the definitions of the noise models we consider.
180"
PRELIMINARIES,0.3627450980392157,"In the Massart noise model, the labels satisfy Py∼DXY|x[y ̸= sign(⟨w∗, x⟩) | x] = η(x), where
181"
PRELIMINARIES,0.36470588235294116,η(x) ≤η < 1
PRELIMINARIES,0.36666666666666664,"2 for all x. In the adversarial label noise or agnostic model, the labels may be completely
182"
PRELIMINARIES,0.3686274509803922,"arbitrary. In both cases, the learner’s goal is to produce a hypothesis with error competitive with opt.
183"
PRELIMINARIES,0.37058823529411766,"We now formally define testable learning. The following definition is an equivalent reframing
184"
PRELIMINARIES,0.37254901960784315,"of the original definition [RV23, Def 4], folding the (label-aware) tester and learner into a single
185"
PRELIMINARIES,0.37450980392156863,"tester-learner.
186"
PRELIMINARIES,0.3764705882352941,"Definition 2.1 (Testable learning, [RV23]). Let C be a concept class mapping Rd to {±1}. Let D∗
187"
PRELIMINARIES,0.3784313725490196,"be a certain target marginal on Rd. Let ϵ, δ > 0 be parameters, and let ψ : [0, 1] →[0, 1] be some
188"
PRELIMINARIES,0.3803921568627451,"function. We say C can be testably learned w.r.t. D∗up to error ψ(opt) + ϵ with failure probability
189"
PRELIMINARIES,0.38235294117647056,"δ if there exists a tester-learner A meeting the following specification. For any distribution DXY
190"
PRELIMINARIES,0.3843137254901961,"on Rd × {±1}, A takes in a large sample S drawn from DXY, and either rejects S or accepts and
191"
PRELIMINARIES,0.3862745098039216,"produces a hypothesis h : Rd →{±1}. Further, the following conditions must be met:
192"
PRELIMINARIES,0.38823529411764707,"(a) (Soundness.) Whenever A accepts and produces a hypothesis h, with probability at least
193"
PRELIMINARIES,0.39019607843137255,"1 −δ (over the randomness of S and A), h must satisfy P(x,y)∼DXY[h(x) ̸= y] ≤
194"
PRELIMINARIES,0.39215686274509803,"ψ(opt(C, DXY)) + ϵ.
195"
PRELIMINARIES,0.3941176470588235,"(b) (Completeness.) Whenever DXY truly has marginal D∗, A must accept with probability at
196"
PRELIMINARIES,0.396078431372549,"least 1 −δ (over the randomness of S and A).
197"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.3980392156862745,"3
Testing properties of strongly log-concave distributions
198"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4,"In this section we define the testers that we will need for our algorithm. All the proofs from this
199"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4019607843137255,"section can be found in Appendix B. We begin with a structural lemma that strengthens the key
200"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.403921568627451,"structural result of [GKK23], stated here as Proposition A.3. It states that even when we restrict an
201"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.40588235294117647,"isotropic strongly log-concave D∗to a band around the origin, moment matching suffices to fool
202"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.40784313725490196,"functions of halfspaces whose weights are orthogonal to the normal of the band.
203"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.40980392156862744,"Proposition 3.1. Let D∗be an isotropic strongly log-concave distribution. Let w ∈Sd−1 be any
204"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4117647058823529,"fixed direction. Let p be a constant. Let f : Rd →R be a function of p halfspaces of the form in
205"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4137254901960784,"Eq. (A.2), with the additional restriction that its weights vi ∈Sd−1 satisfy ⟨vi, w⟩= 0 for all i. For
206"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.41568627450980394,"some σ ∈[0, 1], let T denote the band {x : |⟨w, x⟩| ≤σ}. Let D be any distribution such that D|T
207"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4176470588235294,"matches moments of degree at most k = eO(1/τ 2) with D∗
|T up to an additive slack of d−e
O(k). Then
208"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4196078431372549,"|ED∗[f | T] −ED[f | T]| ≤τ.
209"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4215686274509804,"We now describe some of the testers that we use. First, we need a tester that ensures that the
210"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4235294117647059,"distribution is concentrated in every single direction. More formally, the tester checks that the
211"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.42549019607843136,"moments of the distribution along any direction are small.
212"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.42745098039215684,"Proposition 3.2. For any isotropic strongly log-concave D∗, there exists some constants C1 and a
213"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4294117647058823,"tester T1 that takes a set S ⊆Rd × {±1}, an even k ∈N, a parameter δ ∈(0, 1) and runs and in
214"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.43137254901960786,"time poly
 
dk, |S|, log 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.43333333333333335,"δ

. Let D denote the uniform distribution over S. If T1 accepts, then for any
215"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.43529411764705883,"v ∈Sd−1
216"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4372549019607843,"E
(x,y)∼D[(⟨v, x⟩)k] ≤(C1k)k/2.
(3.1)"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4392156862745098,"Moreover, if S is obtained by taking at least

dk,
 
log 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4411764705882353,"δ
kC1 i.i.d. samples from a distribution
217"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.44313725490196076,"whose Rd-marginal is D∗, the test T1 passes with probability at least 1 −δ.
218"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.44509803921568625,"Secondly, we will use a tester that makes sure the distribution is not concentrated too close to a specific
219"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4470588235294118,"hyperplane. This is one of the properties we will need to use in order to employ the localization
220"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.44901960784313727,"technique of [ABL17].
221"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.45098039215686275,"Proposition 3.3. For any isotropic strongly log-concave D∗, there exist some constants C2, C3 and
222"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.45294117647058824,"a tester T2 that takes a set S ⊆Rd × {±1} a vector w ∈Sd−1, parameters σ, δ ∈(0, 1) and runs in
223"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4549019607843137,"time poly
 
d, |S|, log 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4568627450980392,"δ

. Let D denote the uniform distribution over S. If T2 accepts, then
224"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4588235294117647,"P
(x,y)∼D[|⟨w, x⟩| ≤σ] ∈(C2σ, C3σ).
(3.2)"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.46078431372549017,"Moreover, if S is obtained by taking at least
100
K1σ2 log
  1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4627450980392157,"δ

i.i.d. samples from a distribution whose
225"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4647058823529412,"Rd-marginal is D∗, the test T2 passes with probability at least 1 −δ.
226"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4666666666666667,"Finally, in order to use the localization idea of [ABL17] in a manner similar to [DKTZ20b], we need
227"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.46862745098039216,"to make sure that the distribution is well-behaved also within a band around to a certain hyperplane.
228"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.47058823529411764,"The main property of the distribution that we establish is that functions of constantly many halfspaces
229"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4725490196078431,"have expectations very close to what they would be under our distributional assumption. As we show
230"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4745098039215686,"later in this work, having the aforementioned property allows us to derive many other properties
231"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4764705882352941,"that strongly log-concave distributions have, including many of the key properties that make the
232"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.47843137254901963,"localization technique successful.
233"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4803921568627451,"Proposition 3.4. For any isotropic strongly log-concave D∗and a constant C4, there exists a constant
234"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4823529411764706,"C5 and a tester T3 that takes a set S ⊆Rd × {±1} a vector w ∈Sd−1, parameters σ, τ δ ∈(0, 1)
235"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4843137254901961,"and runs in time poly

d
˜
O( 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.48627450980392156,"τ2 ), 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.48823529411764705,"σ, |S|, log 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.49019607843137253,"δ

. Let D denote the uniform distribution over S, let
236"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.492156862745098,"T denote the band {x : |⟨w, x⟩| ≤σ} and let Fw denote the set {±1}-valued functions of C4
237"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.49411764705882355,"halfspaces whose weight vectors are orthogonal to w. If T3 accepts, then
238"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.49607843137254903,"max
f∈Fw"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.4980392156862745,"E
x∼D∗[f(x) | x ∈T] −
E
(x,y)∼D[f(x) | x ∈T]
 ≤τ,
(3.3) 239"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.5,"max
v∈Sd−1: ⟨v,w⟩=0"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.5019607843137255,"E
x∼D∗[(⟨v, x⟩)2 | x ∈T] −
E
(x,y)∼D[(⟨v, x⟩)2 | x ∈T]
 ≤τ.
(3.4)"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.503921568627451,"Moreover, if S is obtained by taking at least

1
τ · 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.5058823529411764,"σ · d
1
τ2 logC5( 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.5078431372549019,"τ ) ·
 
log 1 δ
 1"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.5098039215686274,τ2 logC5( 1
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.5117647058823529,"τ )
C5
i.i.d.
240"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.5137254901960784,"samples from a distribution whose Rd-marginal is D∗, the test T3 passes with probability at least
241"
TESTING PROPERTIES OF STRONGLY LOG-CONCAVE DISTRIBUTIONS,0.515686274509804,"1 −δ.
242"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5176470588235295,"4
Testably learning halfspaces with Massart noise
243"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5196078431372549,"In this section we prove that we can testably learn halfspaces with Massart noise with respect to
244"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5215686274509804,"isotropic strongly log-concave distributions (see Definition A.1).
245"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5235294117647059,"Theorem 4.1 (Tester-Learner for Halfspaces with Massart Noise). Let DXY be a distribution over
246"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5254901960784314,"Rd × {±1} and let D∗be an isotropic strongly log-concave distribution over Rd. Let C be the class
247"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5274509803921569,"of origin centered halfspaces in Rd. Then, for any η < 1/2, ϵ > 0 and δ ∈(0, 1), there exists an
248"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5294117647058824,"algorithm (Algorithm 1) that testably learns C w.r.t. D∗up to excess error ϵ and error probability
249"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5313725490196078,"at most δ in the Massart noise model with rate at most η, using time and a number of samples from
250"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5333333333333333,"DXY that are polynomial in d, 1/ϵ,
1
1−2η and log(1/δ).
251"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5352941176470588,"Algorithm 1: Tester-learner for halfspaces
Input: Training sets S1, S2, parameters σ, δ, α
Output: A near-optimal weight vector w, or rejection
Run PSGD on the empirical loss Lσ over S1 to get a list L of candidate vectors.
Test whether L contains an α-approximate stationary point w of the empirical loss Lσ over S2.
Reject if no such w exists.
for each candidate w′ in {w, −w} do"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5372549019607843,"Let B′
w(σ) denote the band {x : |⟨w′, x⟩| ≤σ}. Let F′
w denote the class of functions of at
most two halfspaces with weights orthogonal to w′.
Let δ′ = Θ(δ).
Run T1(S2, k = 2, δ) to verify that the empirical marginal is approximately isotropic. Reject
if T1 rejects.
Run T2(S2, w′, σ, δ′) to verify that PS[B′
w] = Θ(σ). Reject if T2 rejects.
Run T3(S2, w′, σ = σ/6, τ, δ′) and T3(S, w′, σ = σ/2, τ, δ′) for a suitable constant τ to
verify that the empirical distribution conditioned on B′
w(σ/6) and B′
w(σ/2) fools F′
w up to
τ. Reject if T3 rejects.
Estimate the empirical error of w′ on S.
If all tests have accepted, output w′ ∈{w, −w} with the best empirical error."
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5392156862745098,"To show our result, we revisit the approach of [DKTZ20a] for learning halfspaces with Massart
252"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5411764705882353,"noise under well-behaved distributions. Their result is based on the idea of minimizing a surrogate
253"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5431372549019607,"loss that is non convex, but whose stationary points correspond to halfspaces with low error. They
254"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5450980392156862,"also require that their surrogate loss is sufficiently smooth, so that one can find a stationary point
255"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5470588235294118,"efficiently. While the distributional assumptions that are used to demonstrate that stationary points of
256"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5490196078431373,"the surrogate loss can be discovered efficiently are mild, the main technical lemma, which demostrates
257"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5509803921568628,"that any stationary point suffices, requires assumptions that are not necessarily testable. We establish
258"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5529411764705883,"a label-dependent approach for testing, making use of tests that are applied during the course of our
259"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5549019607843138,"algorithm.
260"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5568627450980392,"We consider a slightly different surrogate loss than the one used in [DKTZ20a]. In particular, for
261"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5588235294117647,"σ > 0, we let
262"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5607843137254902,"Lσ(w) =
E
(x,y)∼DXY 
ℓσ"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5627450980392157,"
−y ⟨w, x⟩ ∥w∥2"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5647058823529412,"
,
(4.1)"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5666666666666667,"where ℓσ : R →[0, 1] is a smooth approximation to the ramp function with the properties described
263"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5686274509803921,"in Proposition C.1 (see Appendix C), obtained using a piecewise polynomial of degree 3. Unlike
264"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5705882352941176,"the standard logistic function, our loss function has derivative exactly 0 away from the origin (for
265"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5725490196078431,"|t| > σ/2). This makes the analysis of the gradient of Lσ easier, since the contribution from points
266"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5745098039215686,"lying outside a certain band is exactly 0.
267"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5764705882352941,"The smoothness allows us to run PSGD to obtain stationary points efficiently, and we now state the
268"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5784313725490197,"convergence lemma we need.
269"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5803921568627451,"Proposition 4.2 (PSGD Convergence, Lemmas 4.2 and B.2 in [DKTZ20a]). Let Lσ be as in Equation
270"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5823529411764706,"(4.1) with σ ∈(0, 1], ℓσ as described in Proposition C.1 and DXY such that the marginal DX on Rd
271"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5843137254901961,"satisfies Property (3.1) for k = 2. Then, for any ϵ > 0 and δ ∈(0, 1), there is an algorithm whose
272"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5862745098039216,time and sample complexity is O( d
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5882352941176471,σ4 + log(1/δ)
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5901960784313726,"ϵ4σ4 ), which, having access to samples from DXY, outputs
273"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.592156862745098,a list L of vectors w ∈Sd−1 with |L| = O( d
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5941176470588235,σ4 + log(1/δ)
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.596078431372549,"ϵ4σ4 ) so that there exists w ∈L with
274"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.5980392156862745,"∥∇wLσ(w)∥2 ≤ϵ , with probability at least 1 −δ ."
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6,"In particular, the algorithm performs Stochastic Gradient Descent on Lσ Projected on Sd−1 (PSGD).
275"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6019607843137255,"It now suffices to show that, upon performing PSGD on Lσ, for some appropriate choice of σ, we
276"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6039215686274509,"acquire a list of vectors that testably contain a vector which is approximately optimal. We first prove
277"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6058823529411764,"the following lemma, whose distributional assumptions are relaxed compared to the corresponding
278"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6078431372549019,"structural Lemma 3.2 of [DKTZ20a]. In particular, instead of requiring the marginal distribution to be
279"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6098039215686275,"“well-behaved"", we assume that the quantities of interest (for the purposes of our proof) have expected
280"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.611764705882353,"values under the true marginal distribution that are close, up to multiplicative factors, to their expected
281"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6137254901960785,"values under some “well-behaved"" (in fact, strongly log-concave) distribution. While some of the
282"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.615686274509804,"quantities of interest have values that are miniscule and estimating them up to multiplicative factors
283"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6176470588235294,"could be too costly, it turns out that the source of their vanishing scaling can be completely attributed
284"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6196078431372549,"to factors of the form P[|⟨w, x⟩| ≤σ] (where σ is small), which, due to standard concentration
285"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6215686274509804,"arguments, can be approximated up to multiplicative factors, given w ∈Sd−1 and σ > 0 (see
286"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6235294117647059,"Proposition 3.3). As a result, we may estimate the remaining factors up to sufficiently small additive
287"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6254901960784314,"constants (see Proposition 3.4) to get multiplicative overall closeness to the “well behaved"" baseline.
288"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6274509803921569,"We defer the proof of the following Lemma to Appendix C.1.
289"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6294117647058823,"Lemma 4.3. Let Lσ be as in Equation (4.1) with σ ∈(0, 1], ℓσ as described in Proposition C.1, let
290"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6313725490196078,"w ∈Sd−1 and consider DXY such that the marginal DX on Rd satisfies Properties (3.2) and (3.3)
291"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6333333333333333,"for C4 = 2 and accuracy τ. Let w∗∈Sd−1 define an optimum halfspace and let η < 1/2 be an
292"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6352941176470588,"upper bound on the rate of the Massart noise. Then, there are constants c1, c2, c3 > 0 such that if
293"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6372549019607843,"∥∇wLσ(w)∥2 < c1(1 −2η) and τ ≤c2, then
294"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6392156862745098,"∡(w, w∗) ≤
c3
1 −2η · σ
or
∡(−w, w∗) ≤
c3
1 −2η · σ"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6411764705882353,"Combining Proposition 4.2 and Lemma 4.3, we get that for any choice of the parameter σ ∈(0, 1], by
295"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6431372549019608,"running PSGD on Lσ, we can construct a list of vectors of polynomial size (in all relevant parameters)
296"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6450980392156863,"that testably contains a vector that is close to the optimum weight vector. In order to link the zero-one
297"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6470588235294118,"loss to the angular similarity between a weight vector and the optimum vector, we use the following
298"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6490196078431373,"Proposition (for the proof, see Appendix C.2).
299"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6509803921568628,"Proposition 4.4. Let DXY be a distribution over Rd × {±1}, w∗∈arg minw∈Sd−1 PDXY[y ̸=
300"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6529411764705882,"sign(⟨w, x⟩)] and w ∈Sd−1. Then, for any θ ≥∡(w, w∗), θ ∈[0, π/4], if the marginal DX
301"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6549019607843137,"on Rd satisfies Property (3.1) for C1 > 0 and some even k ∈N and Property (3.2) with σ set to
302 (C1k)"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6568627450980392,"k
2(k+1) · (tan θ)
k
k+1 , then, there exists a constant c > 0 such that the following is true.
303"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6588235294117647,"P
DXY[y ̸= sign(⟨w, x⟩)] ≤opt + c · k1/2 · θ1−
1
k+1 ."
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6607843137254902,"We are now ready to prove Theorem 4.1.
304"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6627450980392157,"Proof of Theorem 4.1. Throughout the proof we consider δ′ to be a sufficiently small polynomial
305"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6647058823529411,"in all the relevant parameters. Each of the failure events will have probability at least δ′ and their
306"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6666666666666666,"number will be polynomial in all the relevant parameters, so by the union bound, we may pick δ′ so
307"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6686274509803921,"that the probability of failure is at most δ.
308"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6705882352941176,"The algorithm we run is Algorithm 1, with appropriate selection of parameters and given samples
309"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6725490196078432,"S1, S2, each of which are sufficiently large sets of independent samples from the true unknown
310"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6745098039215687,"distribution DXY. For some σ ∈(0, 1] to be defined later, we run PSGD on the empirical loss Lσ
311"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6764705882352942,"over S1 as described in Proposition 4.2 with ϵ = c1(1−2η)σ/4, where c1 is given by Lemma 4.3. By
312"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6784313725490196,"Proposition 4.2, we get a list L of vectors w ∈Sd−1 with |L| = poly(d, 1/σ) such that there exists
313"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6803921568627451,w ∈L with ∥∇wLσ(w)∥2 < 1
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6823529411764706,"2c1(1 −2η) under the true distribution, if the marginal is isotropic.
314"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6843137254901961,"Figure 1: Critical regions in the proofs of main structural lemmas (Lemmas 4.3, 5.2). We analyze the
contributions of the regions labeled A1, A2 to the quantities A1, A2 in the proofs. Specifically, the
regions A1 (which have height σ/3 so that the value of ℓ′
σ(xw) for any x in these regions is exactly
1/σ, by Proposition C.1) form a subset of the region G, and their probability mass under DX is (up to
a multiplicative factor) a lower bound on the quantity A1 (see Eq (C.3)). Similarly, the region A2 is a
subset of the intersection of Gc with the band of height σ, and has probability mass that is (up to a
multiplicative factor) an upper bound on the quantity A2 (see Eq (C.4))."
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6862745098039216,"Having acquired the list L using sample S1, we use the independent samples in S2 to test whether
315"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6882352941176471,"L contains an approximately stationary point of the empirical loss on S2. If this is not the case,
316"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6901960784313725,"then we may safely reject: for large enough |S1|, if the distribution is indeed isotropic strongly
317"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.692156862745098,"logconcave, there is an approximate stationary of the population loss in L and if |S2| is large enough,
318"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6941176470588235,"the gradient of the empirical loss on S2 will be close to the gradient of the population loss on each of
319"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.696078431372549,"the elements of L, due to appropriate concentration bounds for log-concave distributions as well as
320"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.6980392156862745,"the fact that the elements of L are independent from S2. For the following, let w be a point such that
321"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7,"∥∇wLσ(w)∥2 < c1(1 −2η) under theempirical distribution over S2
322"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7019607843137254,"In Lemma 4.3 and Proposition 4.4 we have identified certain properties of the marginal distribution
323"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.703921568627451,"that are sufficient for our purposes, given that L contains an approximately stationary point of the
324"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7058823529411765,"empirical (surrogate) loss on S2. Our testers T1, T2, T3 verify that these properties hold for the
325"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.707843137254902,"empirical marginal over our sample S2, and it will be convenient to analyze the optimality of our
326"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7098039215686275,"algorithm purely over S2. In particular, we will need to require that |S2| is sufficiently large, so
327"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.711764705882353,"that when the true marginal is indeed the target D∗, our testers succeed with high probability (for
328"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7137254901960784,"the corresponding sample complexity, see Propositions 3.2, 3.3 and 3.4). Moreover, by standard
329"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7156862745098039,"generalization theory, since the VC dimension of halfspaces is only O(d) and for us |S2| is a large
330"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7176470588235294,"poly(d, 1/ϵ), both the error of our final output and the optimal error over S2 will be close to that over
331"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7196078431372549,"DXY. So in what follows, we will abuse notation and refer to the uniform distribution over S2 as
332"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7215686274509804,"DXY and the optimal error over S2 simply as opt.
333"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7235294117647059,"We proceed with some basic tests. Throughout the rest of the algorithm, whenever a tester fails,
334"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7254901960784313,"we reject, otherwise we proceed. First, we run testers T2 with inputs (w, σ/2, δ′) and (w, σ/6, δ′)
335"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7274509803921568,"(Proposition 3.3) and T3 with inputs (w, σ/2, c2, δ′) and with (w, σ/6, c2, δ′) (Proposition 3.4, c2
336"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7294117647058823,"as defined in Lemma 4.3). This ensures that for the approximate stationary point w of the Lσ, the
337"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7313725490196078,"probability within the band Bw(σ/2) = {x : |⟨w, x⟩| ≤σ/2} is Θ(σ) (and similarly for Bw(σ/6))
338"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7333333333333333,"and moreover that our marginal conditioned on each of the bands fools (up to an additive constant)
339"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7352941176470589,"functions of halfspaces with weights orthogonal to w. As a result, we may apply Lemma 4.3 to
340"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7372549019607844,"w and form a list of 2 vectors {w, −w} which contains some w′ with ∡(w′, w∗) ≤c2σ/(1 −2η)
341"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7392156862745098,"(where c3 is as defined in Lemma 4.3).
342"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7411764705882353,"We run T1 (Proposition 3.2) with k = 2 to verify that the marginals are approximately isotropic and
343"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7431372549019608,"we use T2 once again, with appropriate parameters for each w and its negation, to apply Proposition
344"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7450980392156863,"4.4 and get that {w, −w} contains a vector w′ with
345"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7470588235294118,"P
DXY[y ̸= sign(⟨w′, x⟩)] ≤opt + c · θ2/3,"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7490196078431373,"where ∡(w′, w∗) ≤θ := c2σ/√1 −2η. By picking σ = Θ(ϵ3/2(1 −2η)), we get
346"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7509803921568627,"P
DXY[y ̸= sign(⟨w′, x⟩)] ≤opt + ϵ ."
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7529411764705882,"However, we do not know which of the weight vectors in {w, −w} is the one guaranteed to achieve
347"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7549019607843137,"small error. In order to discover this vector, we estimate the probability of error of each of the
348"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7568627450980392,"corresponding halfspaces (which can be done efficiently, due to Hoeffding’s bound) and pick the one
349"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7588235294117647,"with the smallest error. This final step does not require any distributional assumptions and we do not
350"
TESTABLY LEARNING HALFSPACES WITH MASSART NOISE,0.7607843137254902,"need to perform any further tests.
351"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7627450980392156,"5
Testably learning halfspaces in the agnostic setting
352"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7647058823529411,"In this section, we provide our result on efficiently and testably learning halfspaces in the agnostic
353"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7666666666666667,"setting with respect to isotropic strongly log-concave target marginals. We defer the proofs to
354"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7686274509803922,"Appendix D. The algorithm we use is once more Algorithm 1, but we call it multiple times for
355"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7705882352941177,"different choices of the parameter σ, reject if any call rejects and output the vector that achieved
356"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7725490196078432,"the minimum empirical error overall, otherwise. Also, the tester T1 is called for a general k (not
357"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7745098039215687,"necessarily k = 2).
358"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7764705882352941,"Theorem 5.1 (Efficient Tester-Learner for Halfspaces in the Agnostic Setting). Let DXY be a
359"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7784313725490196,"distribution over Rd × {±1} and let D∗be a strongly log-concave distribution over Rd (Definition
360"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7803921568627451,"A.1). Let C be the class of origin centered halfspaces in Rd. Then, for any even k ∈N, any ϵ > 0
361"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7823529411764706,"and δ ∈(0, 1), there exists an algorithm that agnostically testably learns C w.r.t. D∗up to error
362"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7843137254901961,"O(k1/2 · opt1−
1
k+1 ) + ϵ, where opt = minw∈Sd−1 PDXY[y ̸= sign(⟨w, x⟩)], and error probability
363"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7862745098039216,"at most δ, using time and a number of samples from DXY that are polynomial in d ˜
O(k), (1/ϵ) ˜
O(k)
364"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.788235294117647,"and (log(1/δ))O(k).
365"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7901960784313725,"In particular, by picking some appropriate k ≤log2 d, we obtain error ˜O(opt)+ϵ in quasipolynomial
366"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.792156862745098,"time and sample complexity, i.e. poly(2polylog d, ( 1"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7941176470588235,"ϵ )polylog d).
367"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.796078431372549,"To prove Theorem 5.1, we may follow a similar approach as the one we used for the case of Massart
368"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.7980392156862746,"noise. However, in this case, the main structural lemma regarding the quality of the stationary points
369"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8,"involves an additional requirement about the parameter σ. In particular, σ cannot be arbitrarily small
370"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8019607843137255,"with respect to the error of the optimum halfspace, because, in this case, there is no upper bound
371"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.803921568627451,"on the amount of noise that any specific point x might be associated with. As a result, picking σ
372"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8058823529411765,"to be arbitrarily small would imply that our algorithm only considers points that lie within a region
373"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.807843137254902,"that has arbitrarily small probability and can hence be completely corrupted with the adversarial
374"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8098039215686275,"opt budget. On the other hand, the polynomial slackness that the testability requirement introduces
375"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8117647058823529,"(through Proposition 4.4) between the error we achieve and the angular distance guarantee we can get
376"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8137254901960784,"via finding a stationary point of Lσ (which is now coupled with opt), appears to the exponent of the
377"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8156862745098039,"guarantee we achieve in Theorem 5.1.
378"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8176470588235294,"Lemma 5.2. Let Lσ be as in Equation (4.1) with σ ∈(0, 1], ℓσ as described in Proposition C.1, let
379"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8196078431372549,"w ∈Sd−1 and consider DXY such that the marginal DX on Rd satisfies Properties (3.2), (3.3) and
380"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8215686274509804,"(3.4) for w with C4 = 2 and accuracy parameter τ. Let opt be the minimum error achieved by some
381"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8235294117647058,"origin centered halfspace and let w∗∈Sd−1 be a corresponding vector. Then, there are constants
382"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8254901960784313,"c1, c2, c3, c4 > 0 such that if opt ≤c1σ, ∥∇wLσ(w)∥2 < c2, and τ ≤c3 then
383"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8274509803921568,"∡(w, w∗) ≤c4σ
or
∡(−w, w∗) ≤c4σ."
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8294117647058824,"We obtain our main result for Gaussian target marginals by refining Proposition 4.4 for the specific
384"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8313725490196079,"case when the target marginal distribution D∗is the standard multivariate Gaussian distribution. The
385"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8333333333333334,"algorithm for the Gaussian case is similar to the one of Theorem 5.1, but it runs different tests for the
386"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8352941176470589,"improved version (see Proposition D.1) of Proposition 4.4.
387"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8372549019607843,"Theorem 5.3. In Theorem 5.1, if D∗is the standard Gaussian in d dimensions, we obtain error
388"
TESTABLY LEARNING HALFSPACES IN THE AGNOSTIC SETTING,0.8392156862745098,"O(opt) + ϵ in polynomial time and sample complexity, i.e. poly(d, 1/ϵ, log(1/δ)).
389"
REFERENCES,0.8411764705882353,"References
390"
REFERENCES,0.8431372549019608,"[ABHU15] Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient
391"
REFERENCES,0.8450980392156863,"learning of linear separators under bounded noise. In Conference on Learning Theory,
392"
REFERENCES,0.8470588235294118,"pages 167–190. PMLR, 2015. 1.1
393"
REFERENCES,0.8490196078431372,"[ABHZ16] Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Hongyang Zhang. Learning
394"
REFERENCES,0.8509803921568627,"and 1-bit compressed sensing under asymmetric noise. In Conference on Learning
395"
REFERENCES,0.8529411764705882,"Theory, pages 152–192. PMLR, 2016. 1.1
396"
REFERENCES,0.8549019607843137,"[ABL17] Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization
397"
REFERENCES,0.8568627450980392,"for efficiently learning linear separators with noise. Journal of the ACM (JACM),
398"
REFERENCES,0.8588235294117647,"63(6):1–27, 2017. 1, 1.1, 1.2, 3, 3
399"
REFERENCES,0.8607843137254902,"[BH21] Maria-Florina Balcan and Nika Haghtalab. Noise in classification. Beyond the Worst-
400"
REFERENCES,0.8627450980392157,"Case Analysis of Algorithms, page 361, 2021. 1, 1.1
401"
REFERENCES,0.8647058823529412,"[BZ17] Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient
402"
REFERENCES,0.8666666666666667,"learning algorithms under s-concave distributions. Advances in Neural Information
403"
REFERENCES,0.8686274509803922,"Processing Systems, 30, 2017. 1.1
404"
REFERENCES,0.8705882352941177,"[Dan15] Amit Daniely. A ptas for agnostically learning halfspaces. In Conference on Learning
405"
REFERENCES,0.8725490196078431,"Theory, pages 484–502. PMLR, 2015. 1.1, 1.2
406"
REFERENCES,0.8745098039215686,"[DGT19] Ilias Diakonikolas, Themis Gouleakis, and Christos Tzamos. Distribution-independent
407"
REFERENCES,0.8764705882352941,"pac learning of halfspaces with massart noise. Advances in Neural Information Process-
408"
REFERENCES,0.8784313725490196,"ing Systems, 32, 2019. 1.1
409"
REFERENCES,0.8803921568627451,"[DK22] Ilias Diakonikolas and Daniel Kane. Near-optimal statistical query hardness of learning
410"
REFERENCES,0.8823529411764706,"halfspaces with massart noise. In Conference on Learning Theory, pages 4258–4282.
411"
REFERENCES,0.884313725490196,"PMLR, 2022. 1.1
412"
REFERENCES,0.8862745098039215,"[DKK+22] Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos
413"
REFERENCES,0.888235294117647,"Zarifis. Learning general halfspaces with general massart noise under the gaussian
414"
REFERENCES,0.8901960784313725,"distribution. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of
415"
REFERENCES,0.8921568627450981,"Computing, pages 874–885, 2022. 1.1
416"
REFERENCES,0.8941176470588236,"[DKMR22] Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, and Lisheng Ren. Cryptographic
417"
REFERENCES,0.8960784313725491,"hardness of learning halfspaces with massart noise. In Advances in Neural Information
418"
REFERENCES,0.8980392156862745,"Processing Systems, 2022. 1.1
419"
REFERENCES,0.9,"[DKPZ21] Ilias Diakonikolas, Daniel M Kane, Thanasis Pittas, and Nikos Zarifis. The optimality
420"
REFERENCES,0.9019607843137255,"of polynomial regression for agnostic learning under gaussian marginals in the sq model.
421"
REFERENCES,0.903921568627451,"In Conference on Learning Theory, pages 1552–1584. PMLR, 2021. 1
422"
REFERENCES,0.9058823529411765,"[DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts
423"
REFERENCES,0.907843137254902,"with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on
424"
REFERENCES,0.9098039215686274,"Theory of Computing, pages 1061–1073, 2018. 1, 1.1
425"
REFERENCES,0.9117647058823529,"[DKT21] Ilias Diakonikolas, Daniel Kane, and Christos Tzamos. Forster decomposition and
426"
REFERENCES,0.9137254901960784,"learning halfspaces with noise. Advances in Neural Information Processing Systems,
427"
REFERENCES,0.9156862745098039,"34:7732–7744, 2021. 1.1
428"
REFERENCES,0.9176470588235294,"[DKTZ20a] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning
429"
REFERENCES,0.9196078431372549,"halfspaces with massart noise under structured distributions. In Conference on Learning
430"
REFERENCES,0.9215686274509803,"Theory, pages 1486–1513. PMLR, 2020. (document), 1, 1, 1.1, 1.2, 4, 4.2, 4, C.1
431"
REFERENCES,0.9235294117647059,"[DKTZ20b] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex
432"
REFERENCES,0.9254901960784314,"sgd learns halfspaces with adversarial label noise. Advances in Neural Information
433"
REFERENCES,0.9274509803921569,"Processing Systems, 33:18540–18549, 2020. (document), 1, 1, 1.1, 1.2, 3, D.1
434"
REFERENCES,0.9294117647058824,"[DKTZ22] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning gen-
435"
REFERENCES,0.9313725490196079,"eral halfspaces with adversarial label noise via online gradient descent. In International
436"
REFERENCES,0.9333333333333333,"Conference on Machine Learning, pages 5118–5141. PMLR, 2022. 1.1
437"
REFERENCES,0.9352941176470588,"[DKZ20] Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis. Near-optimal sq lower bounds
438"
REFERENCES,0.9372549019607843,"for agnostically learning halfspaces and relus under gaussian marginals. Advances in
439"
REFERENCES,0.9392156862745098,"Neural Information Processing Systems, 33:13586–13596, 2020. 1
440"
REFERENCES,0.9411764705882353,"[DTK22] Ilias Diakonikolas, Christos Tzamos, and Daniel M Kane. A strongly polynomial
441"
REFERENCES,0.9431372549019608,"algorithm for approximate forster transforms and its application to halfspace learning.
442"
REFERENCES,0.9450980392156862,"arXiv preprint arXiv:2212.03008, 2022. 1.1
443"
REFERENCES,0.9470588235294117,"[GGK20] Surbhi Goel, Aravind Gollakota, and Adam Klivans. Statistical-query lower bounds via
444"
REFERENCES,0.9490196078431372,"functional gradients. Advances in Neural Information Processing Systems, 33:2147–
445"
REFERENCES,0.9509803921568627,"2158, 2020. 1
446"
REFERENCES,0.9529411764705882,"[GKK23] Aravind Gollakota, Adam R Klivans, and Pravesh K Kothari. A moment-matching
447"
REFERENCES,0.9549019607843138,"approach to testable learning and a new characterization of rademacher complexity.
448"
REFERENCES,0.9568627450980393,"Proceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023.
449"
REFERENCES,0.9588235294117647,"To appear. (document), 1, 1, 1.2, 3, A.3, A
450"
REFERENCES,0.9607843137254902,"[KKMS08] Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio.
451"
REFERENCES,0.9627450980392157,"Agnostically learning halfspaces. SIAM Journal on Computing, 37(6):1777–1805, 2008.
452 1
453"
REFERENCES,0.9647058823529412,"[KLS09] Adam R Klivans, Philip M Long, and Rocco A Servedio. Learning halfspaces with
454"
REFERENCES,0.9666666666666667,"malicious noise. Journal of Machine Learning Research, 10(12), 2009. 1.1
455"
REFERENCES,0.9686274509803922,"[RV23] Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learning al-
456"
REFERENCES,0.9705882352941176,"gorithms. Proceedings of the fifty-fifth annual ACM Symposium on Theory of Computing,
457"
REFERENCES,0.9725490196078431,"2023. To appear. (document), 1, 1, 2, 2.1
458"
REFERENCES,0.9745098039215686,"[SW14] Adrien Saumard and Jon A Wellner. Log-concavity and strong log-concavity: a review.
459"
REFERENCES,0.9764705882352941,"Statistics surveys, 8:45, 2014. A.1, A.2, A
460"
REFERENCES,0.9784313725490196,"[YZ17] Songbai Yan and Chicheng Zhang. Revisiting perceptron: Efficient and label-optimal
461"
REFERENCES,0.9803921568627451,"learning of halfspaces. Advances in Neural Information Processing Systems, 30, 2017.
462"
REFERENCES,0.9823529411764705,"1.1
463"
REFERENCES,0.984313725490196,"[Zha18] Chicheng Zhang. Efficient active learning of sparse halfspaces. In Conference on
464"
REFERENCES,0.9862745098039216,"Learning Theory, pages 1856–1880. PMLR, 2018. 1.1
465"
REFERENCES,0.9882352941176471,"[ZL21] Chicheng Zhang and Yinan Li. Improved algorithms for efficient active learning
466"
REFERENCES,0.9901960784313726,"halfspaces with massart and tsybakov noise. In Conference on Learning Theory, pages
467"
REFERENCES,0.9921568627450981,"4526–4527. PMLR, 2021. 1.1
468"
REFERENCES,0.9941176470588236,"[ZSA20] Chicheng Zhang, Jie Shen, and Pranjal Awasthi. Efficient active learning of sparse
469"
REFERENCES,0.996078431372549,"halfspaces with arbitrary bounded noise. Advances in Neural Information Processing
470"
REFERENCES,0.9980392156862745,"Systems, 33:7184–7197, 2020. 1.1
471"
