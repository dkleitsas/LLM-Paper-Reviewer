Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000855431993156544,"Unsupervised Anomaly Detection (UAD) plays a crucial role in identifying abnor-
1"
ABSTRACT,0.001710863986313088,"mal patterns within data without labeled examples, holding significant practical
2"
ABSTRACT,0.0025662959794696323,"implications across various domains. Although the individual contributions of
3"
ABSTRACT,0.003421727972626176,"representation learning and clustering to anomaly detection are well-established,
4"
ABSTRACT,0.00427715996578272,"their interdependencies remain under-explored due to the absence of a unified
5"
ABSTRACT,0.0051325919589392645,"theoretical framework. Consequently, their collective potential to enhance anomaly
6"
ABSTRACT,0.005988023952095809,"detection performance remains largely untapped. To bridge this gap, in this paper,
7"
ABSTRACT,0.006843455945252352,"we propose a novel probabilistic mixture model for anomaly detection to establish
8"
ABSTRACT,0.007698887938408896,"a theoretical connection among representation learning, clustering, and anomaly
9"
ABSTRACT,0.00855431993156544,"detection. By maximizing a novel anomaly-aware data likelihood, representation
10"
ABSTRACT,0.009409751924721984,"learning and clustering can effectively reduce the adverse impact of anomalous
11"
ABSTRACT,0.010265183917878529,"data and collaboratively benefit anomaly detection. Meanwhile, a theoretically sub-
12"
ABSTRACT,0.011120615911035072,"stantiated anomaly score is naturally derived from this framework. Lastly, drawing
13"
ABSTRACT,0.011976047904191617,"inspiration from gravitational analysis in physics, we have devised an improved
14"
ABSTRACT,0.01283147989734816,"anomaly score that more effectively harnesses the combined power of representa-
15"
ABSTRACT,0.013686911890504704,"tion learning and clustering. Extensive experiments, involving 17 baseline methods
16"
ABSTRACT,0.01454234388366125,"across 30 diverse datasets, validate the effectiveness and generalization capability
17"
ABSTRACT,0.015397775876817793,"of the proposed method, surpassing state-of-the-art methods.
18"
INTRODUCTION,0.016253207869974338,"1
Introduction
19"
INTRODUCTION,0.01710863986313088,"Unsupervised Anomaly Detection (UAD) refers to the task dedicated to identifying abnormal patterns
20"
INTRODUCTION,0.017964071856287425,"or instances within data in the absence of labeled examples [8]. It has long received extensive
21"
INTRODUCTION,0.018819503849443968,"attention in the past decades for its wide-ranging applications in numerous practical scenarios,
22"
INTRODUCTION,0.019674935842600515,"including financial auditing [3], healthcare monitoring [44] and e-commerce sector [23]. Due to the
23"
INTRODUCTION,0.020530367835757058,"lack of explicit label guidance, the key to UAD is to uncover the dominant patterns that widely exist
24"
INTRODUCTION,0.0213857998289136,"in the dataset so that samples do not conform to these patterns can be recognized as anomalies. To
25"
INTRODUCTION,0.022241231822070145,"achieve this, early works [7] have heavily relied on powerful unsupervised representation learning
26"
INTRODUCTION,0.023096663815226688,"methods to extract the normal patterns from high-dimensional and complex data such as images, text,
27"
INTRODUCTION,0.023952095808383235,"and graphs. More recent works [45, 2] have utilized clustering, a widely observed natural pattern in
28"
INTRODUCTION,0.02480752780153978,"real-world data, to provide critical global information for anomaly detection and achieved tremendous
29"
INTRODUCTION,0.02566295979469632,"success.
30"
INTRODUCTION,0.026518391787852865,"While the individual contributions of representation learning and clustering to anomaly detection
31"
INTRODUCTION,0.02737382378100941,"are well-established, their interrelationships remain largely unexplored. Intuitively, discriminative
32"
INTRODUCTION,0.028229255774165955,"representation learning can leverage accurate clustering results to differentiate samples from distinct
33"
INTRODUCTION,0.0290846877673225,"clusters in the embedding space (i.e., ➀). Similarly, it can utilize accurate anomaly detection to
34"
INTRODUCTION,0.029940119760479042,"avoid preserving abnormal patterns (i.e., ➁). For accurate clustering, it can gain advantages from
35"
INTRODUCTION,0.030795551753635585,"representation learning by operating in the discriminative embedding space (i.e., ➂). Meanwhile, it
36"
INTRODUCTION,0.03165098374679213,"Figure 1: Interdependent relationships among representation learning, clustering, and anomaly
detection."
INTRODUCTION,0.032506415739948676,"can potentially benefit from accurate anomaly detection by excluding anomalies when formulating
37"
INTRODUCTION,0.033361847733105215,"clusters (i.e., ➃). Anomaly detection can greatly benefit from both discriminative representation
38"
INTRODUCTION,0.03421727972626176,"learning and accurate clustering (i.e., ➄& ➅). However, these benefits hinge on the successful
39"
INTRODUCTION,0.03507271171941831,"identification of anomalies and the reduction of their detrimental impact on the aforementioned
40"
INTRODUCTION,0.03592814371257485,"tasks. As depicted in Figure 1, the integration of these three elements exhibits a significant reciprocal
41"
INTRODUCTION,0.036783575705731396,"nature. In summary, representation learning, clustering, and anomaly detection are interdependent and
42"
INTRODUCTION,0.037639007698887936,"intricately intertwined. Therefore, it is crucial for anomaly detection to fully leverage and mutually
43"
INTRODUCTION,0.03849443969204448,"enhance the relationships among these three components.
44"
INTRODUCTION,0.03934987168520103,"Despite the intuitive significance of the interactions among representation learning, clustering, and
45"
INTRODUCTION,0.04020530367835757,"anomaly detection, existing methods have only made limited attempts to exploit them and fall short
46"
INTRODUCTION,0.041060735671514116,"of expectations. On one hand, some methods [58] have acknowledged the interplay among these
47"
INTRODUCTION,0.041916167664670656,"three factors, but their focus remains primarily on the interactions between two factors at a time,
48"
INTRODUCTION,0.0427715996578272,"making only targeted improvements. For instance, some strategies include explicitly removing outlier
49"
INTRODUCTION,0.04362703165098375,"samples during the clustering process [9] or designing robust representation learning methods [10] to
50"
INTRODUCTION,0.04448246364414029,"mitigate the influence of anomalies. On the other hand, recent methods [45] have begun to explore
51"
INTRODUCTION,0.045337895637296836,"the simultaneous optimization of these three factors within a single framework. However, these
52"
INTRODUCTION,0.046193327630453376,"attempts are still in the stage of merely superimposing the objectives of the three factors without a
53"
INTRODUCTION,0.04704875962360992,"unified theoretical framework. This lack of a guiding framework prevents the adequate modeling of
54"
INTRODUCTION,0.04790419161676647,"the interdependencies among these factors, thereby limiting their collective contribution to a unified
55"
INTRODUCTION,0.04875962360992301,"anomaly detection objective. Consequently, we aim to address the following question: Is it possible
56"
INTRODUCTION,0.04961505560307956,"to employ a unified theoretical framework to jointly model these three interdependent objectives,
57"
INTRODUCTION,0.0504704875962361,"thereby leveraging their respective strengths to enhance anomaly detection?
58"
INTRODUCTION,0.05132591958939264,"In this paper, we try to answer this question and propose a novel model named UniCAD for anomaly
59"
INTRODUCTION,0.05218135158254919,"detection. The proposed UniCAD integrates representation learning, clustering, and anomaly de-
60"
INTRODUCTION,0.05303678357570573,"tection into a unified framework, achieved through the theoretical guidance of maximizing the
61"
INTRODUCTION,0.05389221556886228,"anomaly-aware data likelihood. Specifically, we explicitly model the relationships between samples
62"
INTRODUCTION,0.05474764756201882,"and multiple clusters in the representation space using the probabilistic mixture models for the
63"
INTRODUCTION,0.055603079555175364,"likelihood estimation. Moreover, we creatively introduce a learnable indicator function into the
64"
INTRODUCTION,0.05645851154833191,"objective of maximum likelihood to explicitly attenuate the influence of anomalies on representation
65"
INTRODUCTION,0.05731394354148845,"learning and clustering. Under this framework, we can theoretically derive an anomaly score that
66"
INTRODUCTION,0.058169375534645,"indicates the abnormality of samples, rather than heuristically designing it based on clustering results
67"
INTRODUCTION,0.05902480752780154,"as existing works do. Furthermore, building upon this theoretically supported anomaly score and
68"
INTRODUCTION,0.059880239520958084,"inspired by the theory of universal gravitation, we propose a more comprehensive anomaly metric that
69"
INTRODUCTION,0.06073567151411463,"considers the complex relationships between samples and multiple clusters. This allows us to better
70"
INTRODUCTION,0.06159110350727117,"utilize the learned representations and clustering results from this framework for anomaly detection.
71"
INTRODUCTION,0.06244653550042772,"To sum up, we underline our contributions as follows:
72"
INTRODUCTION,0.06330196749358426,"• We propose a unified theoretical framework to jointly optimize representation learning, clustering,
73"
INTRODUCTION,0.0641573994867408,"and anomaly detection, allowing their mutual enhancement and aid in anomaly detection.
74"
INTRODUCTION,0.06501283147989735,"• Based on the proposed framework, we derive a theoretically grounded anomaly score and further
75"
INTRODUCTION,0.0658682634730539,"introduce a more comprehensive score with the vector summation, which fully releases the power
76"
INTRODUCTION,0.06672369546621043,"of the framework for effective anomaly detection.
77"
INTRODUCTION,0.06757912745936698,"• Extensive experiments have been conducted on 30 datasets to validate the superior unsupervised
78"
INTRODUCTION,0.06843455945252352,"anomaly detection performance of our approach, which surpassed the state-of-the-art through
79"
INTRODUCTION,0.06928999144568007,"comparative evaluations with 17 baseline methods.
80"
RELATED WORK,0.07014542343883662,"2
Related Work
81"
RELATED WORK,0.07100085543199315,"Typical unsupervised anomaly detection (UAD) methods calculate a continuous score for each sample
82"
RELATED WORK,0.0718562874251497,"to measure its anomaly degree. Various UAD methods have been proposed based on different
83"
RELATED WORK,0.07271171941830624,"assumptions, making them suitable for detecting various types of anomaly patterns, including
84"
RELATED WORK,0.07356715141146279,"subspace-based models [24], statistical models [16], linear models [49, 32], density-based models [6,
85"
RELATED WORK,0.07442258340461934,"38], ensemble-based models [39, 29], probability-based models [40, 58, 28, 27], neural network-
86"
RELATED WORK,0.07527801539777587,"based models [42, 51], and cluster-based models [18, 9]. Considering the field of anomaly detection
87"
RELATED WORK,0.07613344739093242,"has progressed by integrating clustering information to enhance detection accuracy [26, 56], we
88"
RELATED WORK,0.07698887938408897,"primarily focus on and analyze anomaly patterns related to clustering, incorporating a global clustering
89"
RELATED WORK,0.07784431137724551,"perspective to assess the degree of anomaly. Notable methods in this context include CBLOF [18],
90"
RELATED WORK,0.07869974337040206,"which evaluates anomalies based on the size of the nearest cluster and the distance to the nearest large
91"
RELATED WORK,0.07955517536355859,"cluster. Similarly, DCFOD [45] introduces innovation by applying the self-training architecture of
92"
RELATED WORK,0.08041060735671514,"the deep clustering [50] to outlier detection. Meanwhile, DAGMM [58] combines deep autoencoders
93"
RELATED WORK,0.08126603934987169,"with Gaussian mixture models, utilizing sample energy as a metric to quantify the anomaly degree.
94"
RELATED WORK,0.08212147134302823,"In contrast, our approach introduces a unified theoretical framework that integrates representation
95"
RELATED WORK,0.08297690333618478,"learning, clustering, and anomaly detection, overcoming the limitations of heuristic designs and the
96"
RELATED WORK,0.08383233532934131,"overlooked anomaly influence in existing methods.
97"
METHODOLOGY,0.08468776732249786,"3
Methodology
98"
METHODOLOGY,0.0855431993156544,"In this section, we first define the problem we studied and the notations used in this paper. Then we
99"
METHODOLOGY,0.08639863130881095,"elaborate on the proposed method UniCAD. More specifically, we first introduce a novel learning
100"
METHODOLOGY,0.0872540633019675,"objective that optimizes representation learning, clustering, and anomaly detection within a unified
101"
METHODOLOGY,0.08810949529512403,"theoretical framework by maximizing the data likelihood. A novel anomaly score with theoretical
102"
METHODOLOGY,0.08896492728828058,"support is also naturally derived from this framework. Then, inspired by the concept of universal
103"
METHODOLOGY,0.08982035928143713,"gravitation, we further propose an enhanced anomaly scoring approach that leverages the intricate
104"
METHODOLOGY,0.09067579127459367,"relationship between samples and clustering to detect anomalies effectively. Finally, we present an
105"
METHODOLOGY,0.09153122326775022,"efficient iterative optimization strategy to optimize this model and provide a complexity analysis for
106"
METHODOLOGY,0.09238665526090675,"the proposed model.
107"
METHODOLOGY,0.0932420872540633,"Definition 1 (Unsupervised Anomaly Detection). Given a dataset X ∈RN×D comprising N
108"
METHODOLOGY,0.09409751924721985,"instances with D-dimensional features, unsupervised anomaly detection aims to learn an anomaly
109"
METHODOLOGY,0.0949529512403764,"score oi for each instance xi in an unsupervised manner so that the abnormal ones have higher
110"
METHODOLOGY,0.09580838323353294,"scores than the normal ones.
111"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.09666381522668947,"3.1
Maximizing Anomaly-aware Likelihood
112"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.09751924721984602,"Previous research has demonstrated the importance of discriminative representation and accurate
113"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.09837467921300257,"clustering in anomaly detection [45]. However, the presence of anomalous samples can significantly
114"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.09923011120615911,"disrupt the effectiveness of both representation learning and clustering [12]. While some existing
115"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10008554319931566,"studies have attempted to integrate these three separate learning objectives, the lack of a unified
116"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1009409751924722,"theoretical framework has hindered their mutual enhancement, leading to suboptimal results.
117"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10179640718562874,"To tackle this issue, in this paper, we propose a unified and coherent approach that considers
118"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10265183917878529,"representation learning, clustering, and anomaly detection by maximizing the likelihood of the
119"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10350727117194183,"observed data. Specifically, we denote the parameters of representation learning as Θ, the clustering
120"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10436270316509838,"parameter as Φ, and the dynamic indicator function for anomaly detection as δ(·). These parameters
121"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10521813515825491,"are optimized simultaneously by maximizing the likelihood of the observed data X:
122"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10607356715141146,"max log p(X|Θ, Φ) = max N
X"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10692899914456801,"i=1
δ(xi) log p(xi|Θ, Φ) = max N
X"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10778443113772455,"i=1
δ(xi) log K
X"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1086398631308811,"k=1
p(xi, ci = k|Θ, Φ),"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.10949529512403763,"(1)
where ci represents the latent cluster variable associated with xi, and ci = k denotes the probabilistic
123"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11035072711719418,"event that xi belongs to the k-th cluster. The δ(xi) is an indicator function that determines whether a
124"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11120615911035073,"sample xi is an anomaly of value 0 or a normal sample of value 1.
125"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11206159110350727,"3.1.1
Joint Representation Learning and Clustering with p(xi|Θ, Φ)
126"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11291702309666382,"Based on the aforementioned advantages of MMs, we estimate the likelihood p(xi|Θ, Φ) with mixture
127"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11377245508982035,"models defined as:
128"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1146278870829769,"p(xi|Θ, Φ) = K
X"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11548331907613345,"k=1
p(xi, ci = k|Θ, Φ) = K
X"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11633875106929,"k=1
p(ci = k) · p(xi|ci = k, Θ, µk, Σk) = K
X"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11719418306244654,"k=1
ωk · p(xi|ci = k, Θ, µk, Σk), (2)"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11804961505560307,"where Φ = {ωk, µk, Σk}. The mixture model is parameterized by the prototypes µk, covariance
129"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11890504704875962,"matrices Σk, and mixture weights ωk from all clusters. PK
k=1 ωk = 1, and k = 1, 2, · · · , K.
130"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.11976047904191617,"In practice, the samples are usually attributed to high-dimensional features and it is challenging to
131"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12061591103507271,"detect anomalies from the raw feature space [41]. Therefore, modern anomaly detection methods [42,
132"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12147134302822926,"58] often map raw data samples X = {xi} ∈RN×D into a low-dimensional representation space
133"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1223267750213858,"Z = {zi} ∈RN×d with a representation learning function zi = fΘ(xi) and detect anomalies within
134"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12318220701454234,"this latent representation space.
135"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12403763900769889,"Following this widely adopted practice, we model the distribution of samples in the latent represen-
136"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12489307100085544,"tation space with a multivariate Student’s-t distribution giving its cluster ci = k. The Student’s-t
137"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12574850299401197,"distribution is robust against outliers due to its heavy tails. Bayesian robustness theory leverages
138"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12660393498716851,"such distributions to dismiss outlier data, favoring reliable sources, making the Student’s-t process
139"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12745936698032506,"preferable over Gaussian processes for data with atypical information [1]. Thus the probability
140"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1283147989734816,"distribution of generating xi with latent representation zi given its cluster ci = k can be expressed as:
141"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.12917023096663816,"p(xi|ci = k, Θ, µk, Σk) = Γ( ν+1"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1300256629597947,2 )|Σk|−1/2 Γ( ν 2)√νπ
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.13088109495295125,"
1 + 1"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1317365269461078,"ν DM(zi, µk)2
−ν+1"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.13259195893926431,"2
,
(3)"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.13344739093242086,"where zi = fΘ(xi) denotes the representation obtained from the data mapped through the neural
142"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1343028229255774,"network parameterized by Θ. Γ denotes the gamma function while ν is the degree of freedom.
143"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.13515825491873396,"Σk is the scale parameter. DM(zi, µk) =
q"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1360136869118905,"(zi −µk)T Σ−1
k (zi −µk) represents the Mahalanobis
144"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.13686911890504705,"distance [33]. In the unsupervised setting, as cross-validating ν on a validation set or learning it is
145"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1377245508982036,"unnecessary, ν is set as 1 for all experiments [50, 48]. The overall marginal likelihood of the observed
146"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.13857998289136014,"data xi can be simplified as:
147"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1394354148845167,"p(xi|Θ, Φ) = K
X"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.14029084687767324,"k=1
ωk ·
π−1 · |Σk|−1/2"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.14114627887082976,"1 + DM(zi, µk)2 .
(4)"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1420017108639863,"3.1.2
Anomaly Indicator δ(xi) and Score oi
148"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.14285714285714285,"As we have discussed, the indicator function δ(xi) not only benefits both representation and clustering
149"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1437125748502994,"but also directly serves as the output of anomaly detection. Ideally, with the percentage of outliers
150"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.14456800684345594,"denoted as l, an optimal solution for δ(xi) that maximizes the objective function J(Θ, Φ) entails
151"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1454234388366125,"setting all δ(xi) = 0 for xi among the l percent of outliers with lowest generation possibility
152"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.14627887082976904,"p(xi|Θ, Φ), and otherwise δ(xi) = 1 is set for the remaining normal samples. Therefore, the
153"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.14713430282292558,"indicator function is determined as:
154"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.14798973481608213,"δ(xi) =
0,
if p(xi|Θ, Φ) is among the l lowest,
1,
otherwise.
(5)"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.14884516680923868,"As this method involves sorting the samples based on the generation probability as being anomalous,
155"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1497005988023952,"the values of p(xi|Θ, Φ) can serve as a form of anomaly score, a classic approach within the mixture
156"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.15055603079555174,"model framework [40, 58]. This suggests that the likelihood of a sample being anomalous is inversely
157"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.1514114627887083,"related to its generative probability since a lower generative probability indicates a higher chance of
158"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.15226689478186484,"the sample being an outlier. Thus the anomaly score of sample xi can be defined as:
159"
MAXIMIZING ANOMALY-AWARE LIKELIHOOD,0.15312232677502138,"oi =
1
p(xi|Θ, Φ) =
1
PK
k=1 ωk ·
π−1·|Σk|−1/2
1+DM(zi,µk)2
.
(6)"
GRAVITY-INSPIRED ANOMALY SCORING,0.15397775876817793,"3.2
Gravity-inspired Anomaly Scoring
160"
GRAVITY-INSPIRED ANOMALY SCORING,0.15483319076133448,"In practical applications, it is proved that anomaly scores derived from generation probabilities often
161"
GRAVITY-INSPIRED ANOMALY SCORING,0.15568862275449102,"yield suboptimal performance [17]. This observation prompts a reconsideration of how to fully
162"
GRAVITY-INSPIRED ANOMALY SCORING,0.15654405474764757,"leverage the complex relationships among samples or even across multiple clusters for anomaly
163"
GRAVITY-INSPIRED ANOMALY SCORING,0.15739948674080412,"detection. In this section, we first provide a brief introduction to the concept of Newton’s Law of
164"
GRAVITY-INSPIRED ANOMALY SCORING,0.15825491873396064,"Universal Gravitation [35] and then demonstrate how the anomaly score is intriguingly similar to this
165"
GRAVITY-INSPIRED ANOMALY SCORING,0.15911035072711718,"cross-field principle. Finally, we discuss the advantages of introducing the vector sum operation into
166"
GRAVITY-INSPIRED ANOMALY SCORING,0.15996578272027373,"the anomaly score inspired by the analogy.
167"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16082121471343028,"3.2.1
Analog Anomaly Scoring and Force Analysis
168"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16167664670658682,"To begin with, Newton’s Law of Universal Gravitation [35] stands as a fundamental framework for
169"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16253207869974337,"describing the interactions among entities in the physical world. According to this law, every object
170"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16338751069289992,"in the universe experiences an attractive force from another object. In classical mechanics, force
171"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16424294268605646,"analysis involves calculating the vector sum of all forces acting on an object, known as the resultant
172"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.165098374679213,"force, which is crucial in determining an object’s acceleration or change in motion:
173"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16595380667236956,"⃗Fi,total = K
X"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16680923866552608,"k=1
⃗Fik, with ⃗Fik = G · mimk"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16766467065868262,"r2
ik
·⃗rik,
(7)"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16852010265183917,"where ⃗Fik represents the k-th force acting on the object i. This force is proportional to the product of
174"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.16937553464499572,"their masses, (mi and mk), and inversely proportional to the square of the distance rik between them.
175"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.17023096663815226,"G represents the gravitational constant, and ⃗rij is the unit direction vector.
176"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.1710863986313088,"Similarly, if denoting: eFik = p(xi, ci = k|Θ, Φ) = ωk ·
π−1·|Σk|−1/2"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.17194183062446536,"1+DM(zi,µk)2 , the score of Equation (6)
177"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.1727972626176219,"bears analogies to the summation of the magnitudes of forces as:
178"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.17365269461077845,"oi =
1
PK
k=1 eFik
, with eFik =
eG · emi emk"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.174508126603935,"er2
ik
,
(8)"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.17536355859709152,"where eG = π−1, emk = ωk|Σk|−1/2, emi = 1, and erik =
p"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.17621899059024806,"1 + DM(zi, µk)2. Here, erik is taken as
179"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.1770744225834046,"the measure of distance within the representation space, modified slightly by an additional term for
180"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.17792985457656116,"smoothness. The constant eG serves a role akin to the gravitational constant in this analogy, whereas
181"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.1787852865697177,"emk resembles the concept of mass for the cluster. The notation emi suggests a standardization where
182"
ANALOG ANOMALY SCORING AND FORCE ANALYSIS,0.17964071856287425,"the mass of each data point is considered uniform and not differentiated.
183"
ANOMALY SCORING WITH VECTOR SUM,0.1804961505560308,"3.2.2
Anomaly Scoring with Vector Sum
184"
ANOMALY SCORING WITH VECTOR SUM,0.18135158254918735,"Comparing Equation (7) with Equation (8), what still differs is that, unlike a simple sum of the
185"
ANOMALY SCORING WITH VECTOR SUM,0.1822070145423439,"scalar value, the resultant force ⃗Fi,total employs the vector sum and incorporates both the magnitude
186"
ANOMALY SCORING WITH VECTOR SUM,0.18306244653550044,"and direction brik of each force. This distinction is crucial because forces in different directions
187"
ANOMALY SCORING WITH VECTOR SUM,0.18391787852865696,"can neutralize each other with a large angle between them or enhance each other’s effects with a
188"
ANOMALY SCORING WITH VECTOR SUM,0.1847733105218135,"small angle. Inspired by this difference, we consider modeling the relationship between samples and
189"
ANOMALY SCORING WITH VECTOR SUM,0.18562874251497005,"clusters as a vector, and aggregating them through vector summation. The vector-formed anomaly
190"
ANOMALY SCORING WITH VECTOR SUM,0.1864841745081266,"score oV
i is defined as:
191"
ANOMALY SCORING WITH VECTOR SUM,0.18733960650128315,"oV
i =
1"
ANOMALY SCORING WITH VECTOR SUM,0.1881950384944397,"∥PK
k=1 eFik ·⃗rik∥
,
(9)"
ANOMALY SCORING WITH VECTOR SUM,0.18905047048759624,"where ⃗rik represents the unit direction vector in the representation space from the sample zi to the
192"
ANOMALY SCORING WITH VECTOR SUM,0.1899059024807528,"cluster prototype µk , and ∥· ∥represents the L2 norm.
193"
ITERATIVE OPTIMIZATION,0.19076133447390933,"3.3
Iterative Optimization
194"
ITERATIVE OPTIMIZATION,0.19161676646706588,"Given the challenge posed by the interdependence of the parameters of the network Θ and those of the
195"
ITERATIVE OPTIMIZATION,0.1924721984602224,"mixture model {ωk, µk, Σk} in joint optimization, we propose an iterative optimization procedure.
196"
ITERATIVE OPTIMIZATION,0.19332763045337895,"The pseudocode for training the model is presented in Algorithm 1 in the appendix.
197"
ITERATIVE OPTIMIZATION,0.1941830624465355,"3.3.1
Update Φ
198"
ITERATIVE OPTIMIZATION,0.19503849443969204,"To update the parameters of the mixture model Φ = {ωk, µk, Σk}, we use the Expectation-
199"
ITERATIVE OPTIMIZATION,0.1958939264328486,"Maximization (EM) algorithm to maximize equation (1) [36]. The detailed derivation is included in
200"
ITERATIVE OPTIMIZATION,0.19674935842600513,"Appendix B.
201"
ITERATIVE OPTIMIZATION,0.19760479041916168,"E-step. During the E-step of iteration (t + 1), our goal is to compute the posterior probabilities of
202"
ITERATIVE OPTIMIZATION,0.19846022241231823,"each data point belonging to the k-th cluster within the mixture model. Given the observed sample
203"
ITERATIVE OPTIMIZATION,0.19931565440547477,"xi and the current estimates of the parameters Θ(t) and Φ(t), the expected value of the likelihood
204"
ITERATIVE OPTIMIZATION,0.20017108639863132,"function of latent variable ck, or the posterior possibilities, can be expressed as:
205"
ITERATIVE OPTIMIZATION,0.20102651839178784,"τ (t+1)
ik
= p(ci = k|xi, Θ, Φ(t)) =
p(xi, ci = k|Θ, Φ(t))
PK
j=1 p(xi, ci = j|Θ, Φ(t))
=
eF(t)
ik
PK
j=1 eF(t)
ij
.
(10)"
ITERATIVE OPTIMIZATION,0.2018819503849444,"The scale factor[36] serving as an intermediate result for subsequent updates in the M-step is :
206"
ITERATIVE OPTIMIZATION,0.20273738237810093,"u(t+1)
ik
=
2"
ITERATIVE OPTIMIZATION,0.20359281437125748,"1 + DM(z(t)
i , µ(t)
k )
.
(11)"
ITERATIVE OPTIMIZATION,0.20444824636441403,"M-step. In the M-step of iteration (t + 1), given the gradients ∂J(Θ,Φ)"
ITERATIVE OPTIMIZATION,0.20530367835757057,"∂ωk
= 0, ∂J(Θ,Φ)"
ITERATIVE OPTIMIZATION,0.20615911035072712,"∂µk
= 0, and
207"
ITERATIVE OPTIMIZATION,0.20701454234388367,"∂J(Θ,Φ)"
ITERATIVE OPTIMIZATION,0.20786997433704021,"∂Σk
= 0, we derive the analytical solutions for the mixture model parameters ωk, µk, and Σk.
208"
ITERATIVE OPTIMIZATION,0.20872540633019676,"Assume the anomalous ratio is l ∈[0, 1], the number of the normal samples is n = int(l ∗N). The
209"
ITERATIVE OPTIMIZATION,0.20958083832335328,"updating process for {ω(t+1)
k
, µ(t+1)
k
, Σ(t+1)
k
} is as follows:
210"
ITERATIVE OPTIMIZATION,0.21043627031650983,"• The mixture weights ωk are updated by averaging the posterior probabilities over all data points
211"
ITERATIVE OPTIMIZATION,0.21129170230966637,"with the number of samples , reflecting the relative presence of each component in the mixture:
212"
ITERATIVE OPTIMIZATION,0.21214713430282292,"ω(t+1)
k
= n
X"
ITERATIVE OPTIMIZATION,0.21300256629597947,"i=1
τ (t+1)
ik
/n.
(12)"
ITERATIVE OPTIMIZATION,0.21385799828913601,"• The prototypes µk are updated to be the weighted average of the data points, where weights are the
213"
ITERATIVE OPTIMIZATION,0.21471343028229256,"posterior probabilities:
214"
ITERATIVE OPTIMIZATION,0.2155688622754491,"µ(t+1)
k
= n
X i=1"
ITERATIVE OPTIMIZATION,0.21642429426860565,"
τ (t+1)
ik
u(t+1)
ik
zi

/ n
X i=1"
ITERATIVE OPTIMIZATION,0.2172797262617622,"
τ (t+1)
ik
u(t+1)
ik

.
(13)"
ITERATIVE OPTIMIZATION,0.21813515825491872,"• The covariance matrices Σk are updated by considering the dispersion of the data around the newly
215"
ITERATIVE OPTIMIZATION,0.21899059024807527,"computed prototypes:
216"
ITERATIVE OPTIMIZATION,0.21984602224123181,"Σ(t+1)
k
=
Pn
i=1 τ (t+1)
ik
u(t+1)
ik
(zi −µ(t+1)
k
)(zi −µ(t+1)
k
)⊺
PK
j=1 τ (t+1)
ij
.
(14)"
ITERATIVE OPTIMIZATION,0.22070145423438836,"3.3.2
Update Θ
217"
ITERATIVE OPTIMIZATION,0.2215568862275449,"We focus on anomaly-aware representation learning and use stochastic gradient descent to optimize
218"
ITERATIVE OPTIMIZATION,0.22241231822070145,"the network parameters Θ, by minimizing the following joint loss:
219"
ITERATIVE OPTIMIZATION,0.223267750213858,"L = −J(Θ, Φ) + g(Θ),
(15)"
ITERATIVE OPTIMIZATION,0.22412318220701455,"where J(Θ, Φ) = log p(X|Θ, Φ). An additional constraint term g(Θ) is introduced to prevent short-
220"
ITERATIVE OPTIMIZATION,0.2249786142001711,"cut solution [15]. In practice, an autoencoder architecture is implemented, utilizing a reconstruction
221"
ITERATIVE OPTIMIZATION,0.22583404619332764,"loss g(Θ) = ∥x −ˆx∥2 as the constraint.
222"
ITERATIVE OPTIMIZATION,0.22668947818648416,"These updates are iteratively performed until convergence, resulting in optimized model parameters
223"
ITERATIVE OPTIMIZATION,0.2275449101796407,"that best fit the given data according to the mixture model framework.
224"
EXPERIMENTS,0.22840034217279725,"4
Experiments
225"
DATASETS & BASELINES,0.2292557741659538,"4.1
Datasets & Baselines
226"
DATASETS & BASELINES,0.23011120615911035,"We evaluated UniCAD on an extensive collection of datasets, comprising 30 tabular datasets that
227"
DATASETS & BASELINES,0.2309666381522669,"span 16 diverse fields. We specifically focused on naturally occurring anomaly patterns, rather
228"
DATASETS & BASELINES,0.23182207014542344,"than synthetically generated or injected anomalies, as this aligns more closely with real-world
229"
DATASETS & BASELINES,0.23267750213858,"scenarios. The detailed descriptions are provided in Table 4 of Appendix D.1. Following the setup
230"
DATASETS & BASELINES,0.23353293413173654,"in ADBench [17], we adopt an inductive setting to predict newly emerging data, a highly beneficial
231"
DATASETS & BASELINES,0.23438836612489308,"approach for practical applications.
232"
DATASETS & BASELINES,0.2352437981180496,"To assess the effectiveness of UniCAD, we compared it with 17 advanced unsupervised anomaly
233"
DATASETS & BASELINES,0.23609923011120615,"detection methods, including: (1) traditional methods: SOD [24] and HBOS [16]; (2) linear methods:
234"
DATASETS & BASELINES,0.2369546621043627,"PCA [49] and OCSVM [32]; (3) density-based methods: LOF [6] and KNN [38]; (4) ensemble-based
235"
DATASETS & BASELINES,0.23781009409751924,"methods: LODA [39] and IForest [29]; (5) probability-based methods: DAGMM [58], ECOD [28],
236"
DATASETS & BASELINES,0.2386655260906758,"and COPOD [27]; (6) cluster-based methods: DBSCAN [13], CBLOF [18], DCOD [45] and KMeans-
237"
DATASETS & BASELINES,0.23952095808383234,"- [9]; and (7) neural network-based methods: DeepSVDD [42] and DIF [51]. These baselines
238"
DATASETS & BASELINES,0.24037639007698888,"encompass the majority of the latest methods, providing a comprehensive overview of the state-of-
239"
DATASETS & BASELINES,0.24123182207014543,"the-art. For a detailed description, please refer to Appendix D.2.
240"
EXPERIMENT SETTINGS,0.24208725406330198,"4.2
Experiment Settings
241"
EXPERIMENT SETTINGS,0.24294268605645852,"In the unsupervised setting, we employ the default hyperparameters from the original papers for all
242"
EXPERIMENT SETTINGS,0.24379811804961504,"comparison methods. Similarly, the UniCAD also utilizes a fixed set of parameters to ensure a fair
243"
EXPERIMENT SETTINGS,0.2446535500427716,"comparison. For all datasets, we employ a two-layer MLP with a hidden dimension of d = 128 and
244"
EXPERIMENT SETTINGS,0.24550898203592814,"ReLU activation function as both encoder and decoder. We utilize the Adam optimizer [21] with a
245"
EXPERIMENT SETTINGS,0.24636441402908468,"learning rate of 1e−4 for 100 epochs. For the EM process, we set the maximum iteration number
246"
EXPERIMENT SETTINGS,0.24721984602224123,"to 100 and a tolerance of 1e−3 for stopping training when the objectives converge. The number of
247"
EXPERIMENT SETTINGS,0.24807527801539778,"components in the mixture model is set as k = 10, and the proportion of the outlier is set as l = 1%.
248"
EXPERIMENT SETTINGS,0.24893071000855432,"We evaluate the methods using Area Under the Receiver Operating Characteristic (AUC-ROC) and
249"
EXPERIMENT SETTINGS,0.24978614200171087,"Area Under the Precision-Recall Curve (AUC-PR) metrics [17], reporting the average ranking (Avg.
250"
EXPERIMENT SETTINGS,0.2506415739948674,"Rank) across all datasets. All experiments are run 3 times with different seeds, and the mean results
251"
EXPERIMENT SETTINGS,0.25149700598802394,"are reported.
252"
PERFORMANCE AND ANALYSIS,0.2523524379811805,"4.3
Performance and Analysis
253"
PERFORMANCE AND ANALYSIS,0.25320786997433703,"Performance Comparison. Table 1 presents a comparison of UniCAD with 10 unsupervised
254"
PERFORMANCE AND ANALYSIS,0.2540633019674936,"baseline methods across 30 tabular datasets using the AUC-ROC metric. The experimental results,
255"
PERFORMANCE AND ANALYSIS,0.2549187339606501,"which encompass 17 baselines, are included in Tables 5 and 6 of Appendix D.3, with additional
256"
PERFORMANCE AND ANALYSIS,0.2557741659538067,"experiments on other data domains presented in Appendix E. Our proposed UniCAD achieves the
257"
PERFORMANCE AND ANALYSIS,0.2566295979469632,"top average ranking, exhibiting the best or near-best performance on a larger number of datasets
258"
PERFORMANCE AND ANALYSIS,0.25748502994011974,"and confirming advanced capabilities. It is noteworthy that there is no one-size-fits-all unsupervised
259"
PERFORMANCE AND ANALYSIS,0.2583404619332763,"anomaly detection method suitable for every type of dataset, as demonstrated by the observation that
260"
PERFORMANCE AND ANALYSIS,0.25919589392643283,"other methods have also achieved some of the best results on certain datasets. However, our model
261"
PERFORMANCE AND ANALYSIS,0.2600513259195894,"showcased a remarkable ability to generalize across most datasets featuring natural anomalies, as
262"
PERFORMANCE AND ANALYSIS,0.2609067579127459,"evidenced by statistical average ranking. As for clustering-based methods such as KMeans--, DCOD,
263"
PERFORMANCE AND ANALYSIS,0.2617621899059025,"and CBLOF, they mostly rank in the top tier among all baseline methods, supporting the advantage of
264"
PERFORMANCE AND ANALYSIS,0.262617621899059,"combining deep clustering with anomaly detection. However, our method significantly outperformed
265"
PERFORMANCE AND ANALYSIS,0.2634730538922156,"these methods by mitigating their limitations and further providing a unified framework for joint
266"
PERFORMANCE AND ANALYSIS,0.2643284858853721,"representation learning, clustering, and anomaly detection.
267"
PERFORMANCE AND ANALYSIS,0.26518391787852863,"Effectiveness of Vector Sum in Anomaly Scoring. As demonstrated in Table 1, we compare the
268"
PERFORMANCE AND ANALYSIS,0.2660393498716852,"anomaly score oi derived directly from the generation possibility with its vector summation form oV
i .
269"
PERFORMANCE AND ANALYSIS,0.2668947818648417,"According to our statistical findings, we observe that vector scores oV
i consistently outperform scalar
270"
PERFORMANCE AND ANALYSIS,0.2677502138579983,"scores oi. This indicates that the introduction of the vector summation, analogous to the concept
271"
PERFORMANCE AND ANALYSIS,0.2686056458511548,"of resultant force, makes a substantial difference in anomaly detection scenarios involving multiple
272"
PERFORMANCE AND ANALYSIS,0.2694610778443114,"clusters. The performance gains of the vector sum scores strongly demonstrate the effectiveness
273"
PERFORMANCE AND ANALYSIS,0.2703165098374679,"of the UniCAD in capturing the subtle differences in the distinctions among multiple clusters and
274"
PERFORMANCE AND ANALYSIS,0.2711719418306245,"underscore the utility of this factor in the context of anomaly detection based on clustering.
275"
PERFORMANCE AND ANALYSIS,0.272027373823781,"Table 1: AUCROC of 10 unsupervised algorithms on 30 tabular benchmark datasets. In each dataset,
the algorithm with the highest AUCROC is marked in red, the second highest in blue, and the third
highest in green."
PERFORMANCE AND ANALYSIS,0.2728828058169376,"Dataset
OC
SVM
LOF
IForest
DA
GMM
ECOD
DB
SCAN
CBLOF
DCOD
KMeans--
DIF
UniCAD
(Scalar)
UniCAD
(Vector)"
PERFORMANCE AND ANALYSIS,0.2737382378100941,"annthyroid
57.23
70.20
82.01
56.53
78.66
50.08
62.28
55.01
64.99
66.76
75.27
72.72
backdoor
85.04
85.79
72.15
55.98
86.08
76.55
81.91
79.57
89.11
92.87
87.28
89.24
breastw
80.30
40.61
98.32
N/A
99.17
85.20
96.86
99.02
97.05
77.45
98.15
98.56
campaign
65.70
59.04
71.71
56.03
76.10
50.60
64.34
63.16
63.51
67.53
73.52
73.64
celeba
70.70
38.95
70.41
44.74
76.48
50.36
73.99
91.41
56.76
65.29
81.38
82.00
census
54.90
47.46
59.52
59.65
67.63
58.50
60.17
72.84
63.33
59.66
67.90
67.84
glass
35.36
69.20
77.13
76.09
65.83
54.55
78.30
78.07
77.30
84.57
79.52
82.17
Hepatitis
67.75
38.06
69.75
54.80
75.22
68.12
73.05
48.38
64.64
74.24
75.53
80.62
http
99.59
27.46
99.96
N/A
98.10
49.97
99.60
99.53
99.55
99.49
99.53
99.52
Ionosphere
75.92
90.59
84.50
73.41
73.15
81.12
90.79
57.78
91.36
89.74
92.04
90.37
landsat
36.15
53.90
47.64
43.92
36.10
50.17
63.69
33.40
55.31
54.84
49.60
57.37
Lymphography
99.54
89.86
99.81
72.11
99.52
74.16
99.81
81.19
100.00
83.67
99.29
99.73
mnist
82.95
67.13
80.98
67.23
74.61
50.00
79.96
65.23
82.45
88.16
86.00
86.64
musk
80.58
41.18
99.99
76.85
95.40
50.00
100.00
42.19
72.16
98.22
99.92
100.00
pendigits
93.75
47.99
94.76
64.22
93.01
55.33
96.93
94.33
94.37
93.79
95.12
95.52
Pima
66.92
65.71
72.87
55.93
63.05
51.39
71.49
72.16
70.44
67.28
75.16
74.87
satellite
59.02
55.88
70.43
62.33
58.09
55.52
71.32
55.97
67.71
74.52
72.46
77.65
satimage-2
97.35
47.36
99.16
96.29
96.28
75.74
99.84
86.01
99.88
99.63
99.87
99.88
shuttle
97.40
57.11
99.56
97.92
99.13
50.40
93.07
97.20
69.97
97.00
99.15
98.75
skin
49.45
46.47
68.21
N/A
49.08
50.00
68.03
64.34
65.47
66.36
72.26
69.69
Stamps
83.86
51.26
91.21
88.89
87.87
52.08
69.89
93.41
79.78
87.95
91.37
94.18
thyroid
87.92
86.86
98.30
79.75
97.94
53.57
94.74
78.55
92.26
96.26
97.66
97.48
vertebral
37.99
49.29
36.66
53.20
40.66
49.74
41.01
38.13
38.14
47.20
33.11
47.37
vowels
61.59
93.12
73.94
60.58
62.24
57.50
92.12
51.56
93.45
81.02
88.38
92.09
Waveform
56.29
73.32
71.47
49.35
62.36
66.41
71.27
63.47
74.35
75.33
71.81
74.29
WBC
99.03
54.17
99.01
N/A
99.11
87.43
96.88
94.92
97.45
81.27
97.68
98.93
Wilt
31.28
50.65
41.94
37.29
36.30
49.96
34.50
44.71
34.91
39.46
48.95
52.56
wine
73.07
37.74
80.37
61.70
77.22
40.33
27.14
82.18
27.36
41.69
82.72
95.25
WPBC
45.35
41.41
46.63
47.80
46.65
52.22
45.32
49.67
45.01
44.69
48.02
49.90"
PERFORMANCE AND ANALYSIS,0.2745936698032506,"Avg. Rank
7.8
8.9
5.1
8.7
6.4
9.3
5.7
7.4
6.0
5.8
3.7
2.6"
PERFORMANCE AND ANALYSIS,0.2754491017964072,"(a) Optimization Analysis
(b) AUC-ROC Surface
(c) AUC-PR Surface"
PERFORMANCE AND ANALYSIS,0.2763045337895637,"Figure 2: (a) demonstrates the performance variations during the optimization process on the satimage-
2 dataset. (b) & (c) Analysis of cluster count k, anomaly ratio l."
PERFORMANCE AND ANALYSIS,0.2771599657827203,"Analysis of EM Iterative Optimization. To comprehend the iterative training within our model,
276"
PERFORMANCE AND ANALYSIS,0.2780153977758768,"we have illustrated the performance variations accompanying the increase in iteration counts in
277"
PERFORMANCE AND ANALYSIS,0.2788708297690334,"Figure 2a. Specifically, we monitored the iteration number t for the satimage-2 dataset, ranging
278"
PERFORMANCE AND ANALYSIS,0.2797262617621899,"from 0 to 10, while maintaining other default parameters constant. Both AUC-ROC and AUC-PR
279"
PERFORMANCE AND ANALYSIS,0.2805816937553465,"performance curves displayed consistent trends, with minor fluctuations only during the initial phase.
280"
PERFORMANCE AND ANALYSIS,0.281437125748503,"The performance remained relatively stable throughout the last steps, illustrating the effectiveness
281"
PERFORMANCE AND ANALYSIS,0.2822925577416595,"and convergence of iterative EM optimization.
282"
PERFORMANCE AND ANALYSIS,0.2831479897348161,"Runtime Comparison. We present a analysis of the runtime performance of various methods,
283"
PERFORMANCE AND ANALYSIS,0.2840034217279726,"including our proposed approach, as detailed in Table 2. Our experiments, conducted on the backdoor
284"
PERFORMANCE AND ANALYSIS,0.2848588537211292,"dataset, reveal that while non-deep learning methods exhibit lower runtime, they often simplify the
285"
PERFORMANCE AND ANALYSIS,0.2857142857142857,"problem space excessively, failing to capture the complex non-linear relationships present in the
286"
PERFORMANCE AND ANALYSIS,0.2865697177074423,"data. In contrast, our method, when compared to existing deep learning techniques, demonstrates
287"
PERFORMANCE AND ANALYSIS,0.2874251497005988,"a significant reduction in computational time. This indicates that our approach not only manages
288"
PERFORMANCE AND ANALYSIS,0.28828058169375537,Table 2: Runtime Comparison. The runtime is reported in seconds (s).
PERFORMANCE AND ANALYSIS,0.2891360136869119,"Phase
IForest
KMeans--
DAGMM
DCOD
UniCAD"
PERFORMANCE AND ANALYSIS,0.28999144568006846,"Fit
0.256
103.697
795.004
4548.634
246.113
Infer
0.0186
0.059
4.190
16.190
0.079"
PERFORMANCE AND ANALYSIS,0.290846877673225,"Table 3: Ablation study on AUC-ROC scores, calculated across 30 datasets."
PERFORMANCE AND ANALYSIS,0.2917023096663815,"Metric
w/ Gauss.
w/o J(Θ, Φ)
w/o δ(xi)
Full Model"
PERFORMANCE AND ANALYSIS,0.2925577416595381,"Avg. Rank (w/ baselines & variants)
6.2
6.6
5.0
4.2"
PERFORMANCE AND ANALYSIS,0.2934131736526946,"to efficiently model complex patterns but also achieves an optimal balance between computational
289"
PERFORMANCE AND ANALYSIS,0.29426860564585117,"efficiency and modeling capability.
290"
ABLATION STUDIES,0.2951240376390077,"4.4
Ablation Studies
291"
ABLATION STUDIES,0.29597946963216426,"In this section, we examine the contributions of different components in UniCAD. Tables 3 reports the
292"
ABLATION STUDIES,0.2968349016253208,"results. We make three major observations. Firstly, the anomaly detection performance experiences a
293"
ABLATION STUDIES,0.29769033361847735,"significant drop when replacing the Student’s t distribution with a Gaussian distribution for the Mixture
294"
ABLATION STUDIES,0.2985457656116339,"Model, highlighting the robustness of the Student’s t distribution in unsupervised anomaly detection.
295"
ABLATION STUDIES,0.2994011976047904,"Secondly, omitting the likelihood maximization loss (w/o J(Θ, Φ)) also results in a considerable
296"
ABLATION STUDIES,0.30025662959794697,"decrease in overall performance. This observation underscores the importance of deriving both
297"
ABLATION STUDIES,0.3011120615911035,"the optimization objectives and anomaly scores from the likelihood generation probability through
298"
ABLATION STUDIES,0.30196749358426006,"a theoretical framework, which allows for unified joint optimization of anomaly detection and
299"
ABLATION STUDIES,0.3028229255774166,"clustering in the representation space. Furthermore, the indicator function δ(xi) also contributes to a
300"
ABLATION STUDIES,0.30367835757057315,"performance increase. These results further confirm the effectiveness of our UniCAD in mitigating the
301"
ABLATION STUDIES,0.3045337895637297,"negative influence of anomalies in the clustering process, as the existence of outliers may significantly
302"
ABLATION STUDIES,0.30538922155688625,"degrade the performance of clustering. In summary, all these ablation studies clearly demonstrate
303"
ABLATION STUDIES,0.30624465355004277,"the effectiveness of our theoretical framework in simultaneously considering representation learning,
304"
ABLATION STUDIES,0.30710008554319934,"clustering, and anomaly detection.
305"
SENSITIVITY OF HYPERPARAMETERS,0.30795551753635586,"4.5
Sensitivity of Hyperparameters
306"
SENSITIVITY OF HYPERPARAMETERS,0.3088109495295124,"In this section, we conducted a sensitivity analysis on key hyperparameters of the model applied
307"
SENSITIVITY OF HYPERPARAMETERS,0.30966638152266895,"to the donors dataset, focusing on the number of clusters k and the proportion of the outlier set l.
308"
SENSITIVITY OF HYPERPARAMETERS,0.3105218135158255,"The results of this analysis are illustrated in Figure 2. Notably, the optimal range for l tends to be
309"
SENSITIVITY OF HYPERPARAMETERS,0.31137724550898205,"lower than the actual proportion of anomalies in the dataset. Furthermore, a pattern was observed
310"
SENSITIVITY OF HYPERPARAMETERS,0.31223267750213857,"with the number of clusters k, where the model performance initially improved with an increase in k,
311"
SENSITIVITY OF HYPERPARAMETERS,0.31308810949529514,"followed by a subsequent decline. This suggests the existence of an optimal range for the number of
312"
SENSITIVITY OF HYPERPARAMETERS,0.31394354148845166,"clusters, which should be carefully selected based on the specific application context.
313"
CONCLUSION,0.31479897348160824,"5
Conclusion
314"
CONCLUSION,0.31565440547476475,"This paper presents UniCAD, a novel model for Unsupervised Anomaly Detection (UAD) that
315"
CONCLUSION,0.3165098374679213,"seamlessly integrates representation learning, clustering, and anomaly detection within a unified
316"
CONCLUSION,0.31736526946107785,"theoretical framework. Specifically, UniCAD introduces an anomaly-aware data likelihood based on
317"
CONCLUSION,0.31822070145423437,"the mixture model with the Student-t distribution to guide the joint optimization process, effectively
318"
CONCLUSION,0.31907613344739094,"mitigating the impact of anomalies on representation learning and clustering. This framework
319"
CONCLUSION,0.31993156544054746,"enables a theoretically grounded anomaly score inspired by universal gravitation, which considers
320"
CONCLUSION,0.32078699743370404,"complex relationships between samples and multiple clusters. Extensive experiments on 30 datasets
321"
CONCLUSION,0.32164242942686055,"across various domains demonstrate the effectiveness and generalization capability of UniCAD,
322"
CONCLUSION,0.32249786142001713,"surpassing 15 baseline methods and establishing it as a state-of-the-art solution in unsupervised
323"
CONCLUSION,0.32335329341317365,"anomaly detection. Despite its potential, the proposed method’s applicability to broader fields like
324"
CONCLUSION,0.3242087254063302,"time series and multimodal anomaly detection requires further exploration and validation, highlighting
325"
CONCLUSION,0.32506415739948674,"a significant area for future work.
326"
REFERENCES,0.32591958939264326,"References
327"
REFERENCES,0.32677502138579984,"[1] J Ailton A Andrade. On the robustness to outliers of the student-t process. Scandinavian
328"
REFERENCES,0.32763045337895635,"Journal of Statistics, 50(2):725–749, 2023.
329"
REFERENCES,0.32848588537211293,"[2] Caglar Aytekin, Xingyang Ni, Francesco Cricri, and Emre Aksu. Clustering and unsupervised
330"
REFERENCES,0.32934131736526945,"anomaly detection with l 2 normalized deep auto-encoder representations. In 2018 International
331"
REFERENCES,0.330196749358426,"Joint Conference on Neural Networks (IJCNN), pages 1–6. IEEE, 2018.
332"
REFERENCES,0.33105218135158254,"[3] Alexander Bakumenko and Ahmed Elragal. Detecting anomalies in financial data using machine
333"
REFERENCES,0.3319076133447391,"learning algorithms. Systems, 10(5):130, 2022.
334"
REFERENCES,0.33276304533789564,"[4] Sambaran Bandyopadhyay, Saley Vishal Vivek, and MN Murty. Outlier resistant unsupervised
335"
REFERENCES,0.33361847733105215,"deep architectures for attributed network embedding. In Proceedings of the 13th international
336"
REFERENCES,0.33447390932420873,"conference on web search and data mining, pages 25–33, 2020.
337"
REFERENCES,0.33532934131736525,"[5] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
338"
REFERENCES,0.3361847733105218,"Translating embeddings for modeling multi-relational data. Advances in neural information
339"
REFERENCES,0.33704020530367834,"processing systems, 26, 2013.
340"
REFERENCES,0.3378956372968349,"[6] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. Lof: identifying
341"
REFERENCES,0.33875106928999144,"density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference
342"
REFERENCES,0.339606501283148,"on Management of data, pages 93–104, 2000.
343"
REFERENCES,0.34046193327630453,"[7] Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey.
344"
REFERENCES,0.3413173652694611,"arXiv preprint arXiv:1901.03407, 2019.
345"
REFERENCES,0.3421727972626176,"[8] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM
346"
REFERENCES,0.34302822925577414,"computing surveys (CSUR), 41(3):1–58, 2009.
347"
REFERENCES,0.3438836612489307,"[9] Sanjay Chawla and Aristides Gionis. k-means–: A unified approach to clustering and outlier
348"
REFERENCES,0.34473909324208724,"detection. In Proceedings of the 2013 SIAM international conference on data mining, pages
349"
REFERENCES,0.3455945252352438,"189–197. SIAM, 2013.
350"
REFERENCES,0.34644995722840033,"[10] Hyunsoo Cho, Jinseok Seol, and Sang-goo Lee. Masked contrastive learning for anomaly
351"
REFERENCES,0.3473053892215569,"detection. arXiv preprint arXiv:2105.08793, 2021.
352"
REFERENCES,0.3481608212147134,"[11] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed
353"
REFERENCES,0.34901625320787,"networks. In Proceedings of the 2019 SIAM International Conference on Data Mining, pages
354"
REFERENCES,0.3498716852010265,"594–602. SIAM, 2019.
355"
REFERENCES,0.35072711719418304,"[12] Lian Duan, Lida Xu, Ying Liu, and Jun Lee. Cluster-based outlier detection. Annals of
356"
REFERENCES,0.3515825491873396,"Operations Research, 168:151–168, 2009.
357"
REFERENCES,0.35243798118049613,"[13] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm
358"
REFERENCES,0.3532934131736527,"for discovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226–231,
359"
REFERENCES,0.3541488451668092,"1996.
360"
REFERENCES,0.3550042771599658,"[14] Haoyi Fan, Fengbin Zhang, and Zuoyong Li. Anomalydae: Dual autoencoder for anomaly
361"
REFERENCES,0.3558597091531223,"detection on attributed networks. In ICASSP 2020-2020 IEEE International Conference on
362"
REFERENCES,0.3567151411462789,"Acoustics, Speech and Signal Processing (ICASSP), pages 5685–5689. IEEE, 2020.
363"
REFERENCES,0.3575705731394354,"[15] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,
364"
REFERENCES,0.358426005132592,"Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature
365"
REFERENCES,0.3592814371257485,"Machine Intelligence, 2(11):665–673, 2020.
366"
REFERENCES,0.360136869118905,"[16] Markus Goldstein and Andreas Dengel. Histogram-based outlier score (hbos): A fast unsuper-
367"
REFERENCES,0.3609923011120616,"vised anomaly detection algorithm. KI-2012: poster and demo track, 1:59–63, 2012.
368"
REFERENCES,0.3618477331052181,"[17] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly
369"
REFERENCES,0.3627031650983747,"detection benchmark. Advances in Neural Information Processing Systems, 35:32142–32159,
370"
REFERENCES,0.3635585970915312,"2022.
371"
REFERENCES,0.3644140290846878,"[18] Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers.
372"
REFERENCES,0.3652694610778443,"Pattern recognition letters, 24(9-10):1641–1650, 2003.
373"
REFERENCES,0.3661248930710009,"[19] Meng Jiang. Catching social media advertisers with strategy analysis. In Proceedings of the
374"
REFERENCES,0.3669803250641574,"First International Workshop on Computational Methods for CyberSafety, pages 5–10, 2016.
375"
REFERENCES,0.3678357570573139,"[20] Ming Jin, Yixin Liu, Yu Zheng, Lianhua Chi, Yuan-Fang Li, and Shirui Pan. Anemone: Graph
376"
REFERENCES,0.3686911890504705,"anomaly detection with multi-scale contrastive learning. In Proceedings of the 30th ACM
377"
REFERENCES,0.369546621043627,"International Conference on Information & Knowledge Management, pages 3122–3126, 2021.
378"
REFERENCES,0.3704020530367836,"[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
379"
REFERENCES,0.3712574850299401,"arXiv:1412.6980, 2014.
380"
REFERENCES,0.3721129170230967,"[22] Thomas N Kipf and Max Welling.
Variational graph auto-encoders.
arXiv preprint
381"
REFERENCES,0.3729683490162532,"arXiv:1611.07308, 2016.
382"
REFERENCES,0.3738237810094098,"[23] Yufeng Kou, Chang-Tien Lu, Sirirat Sirwongwattana, and Yo-Ping Huang. Survey of fraud
383"
REFERENCES,0.3746792130025663,"detection techniques. In IEEE International Conference on Networking, Sensing and Control,
384"
REFERENCES,0.37553464499572287,"2004, volume 2, pages 749–754. IEEE, 2004.
385"
REFERENCES,0.3763900769888794,"[24] Hans-Peter Kriegel, Peer Kröger, Erich Schubert, and Arthur Zimek. Outlier detection in
386"
REFERENCES,0.3772455089820359,"axis-parallel subspaces of high dimensional data. In Advances in Knowledge Discovery and
387"
REFERENCES,0.3781009409751925,"Data Mining: 13th Pacific-Asia Conference, PAKDD 2009 Bangkok, Thailand, April 27-30,
388"
REFERENCES,0.378956372968349,"2009 Proceedings 13, pages 831–838. Springer, 2009.
389"
REFERENCES,0.3798118049615056,"[25] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory
390"
REFERENCES,0.3806672369546621,"in temporal interaction networks. In Proceedings of the 25th ACM SIGKDD international
391"
REFERENCES,0.38152266894781867,"conference on knowledge discovery & data mining, pages 1269–1278, 2019.
392"
REFERENCES,0.3823781009409752,"[26] Jinbo Li, Hesam Izakian, Witold Pedrycz, and Iqbal Jamal. Clustering-based anomaly detection
393"
REFERENCES,0.38323353293413176,"in multivariate time series data. Applied Soft Computing, 100:106919, 2021.
394"
REFERENCES,0.3840889649272883,"[27] Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. Copod: copula-based outlier
395"
REFERENCES,0.3849443969204448,"detection. In 2020 IEEE international conference on data mining (ICDM), pages 1118–1123.
396"
REFERENCES,0.3857998289136014,"IEEE, 2020.
397"
REFERENCES,0.3866552609067579,"[28] Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and George Chen. Ecod: Unsu-
398"
REFERENCES,0.38751069289991447,"pervised outlier detection using empirical cumulative distribution functions. IEEE Transactions
399"
REFERENCES,0.388366124893071,"on Knowledge and Data Engineering, 2022.
400"
REFERENCES,0.38922155688622756,"[29] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee
401"
REFERENCES,0.3900769888793841,"international conference on data mining, pages 413–422. IEEE, 2008.
402"
REFERENCES,0.39093242087254065,"[30] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly
403"
REFERENCES,0.3917878528656972,"detection on attributed networks via contrastive self-supervised learning. IEEE transactions on
404"
REFERENCES,0.39264328485885375,"neural networks and learning systems, 33(6):2378–2392, 2021.
405"
REFERENCES,0.39349871685201027,"[31] Xuexiong Luo, Jia Wu, Amin Beheshti, Jian Yang, Xiankun Zhang, Yuan Wang, and Shan Xue.
406"
REFERENCES,0.3943541488451668,"Comga: Community-aware attributed graph anomaly detection. In Proceedings of the Fifteenth
407"
REFERENCES,0.39520958083832336,"ACM International Conference on Web Search and Data Mining, pages 657–665, 2022.
408"
REFERENCES,0.3960650128314799,"[32] Larry M Manevitz and Malik Yousef. One-class svms for document classification. Journal of
409"
REFERENCES,0.39692044482463645,"machine Learning research, 2(Dec):139–154, 2001.
410"
REFERENCES,0.397775876817793,"[33] Goeffrey J McLachlan. Mahalanobis distance. Resonance, 4(6):20–26, 1999.
411"
REFERENCES,0.39863130881094955,"[34] Emmanuel Müller, Patricia Iglesias Sánchez, Yvonne Mülle, and Klemens Böhm. Ranking
412"
REFERENCES,0.39948674080410607,"outlier nodes in subspaces of attributed graphs. In 2013 IEEE 29th international conference on
413"
REFERENCES,0.40034217279726264,"data engineering workshops (ICDEW), pages 216–222. IEEE, 2013.
414"
REFERENCES,0.40119760479041916,"[35] Isaac Newton. Philosophiae naturalis principia mathematica, volume 1. G. Brookman, 1833.
415"
REFERENCES,0.4020530367835757,"[36] David Peel and Geoffrey J McLachlan. Robust mixture modelling using the t distribution.
416"
REFERENCES,0.40290846877673225,"Statistics and computing, 10:339–348, 2000.
417"
REFERENCES,0.4037639007698888,"[37] Zhen Peng, Minnan Luo, Jundong Li, Luguo Xue, and Qinghua Zheng. A deep multi-view
418"
REFERENCES,0.40461933276304535,"framework for anomaly detection on attributed networks. IEEE Transactions on Knowledge
419"
REFERENCES,0.40547476475620187,"and Data Engineering, 34(6):2539–2552, 2020.
420"
REFERENCES,0.40633019674935844,"[38] Leif E Peterson. K-nearest neighbor. Scholarpedia, 4(2):1883, 2009.
421"
REFERENCES,0.40718562874251496,"[39] Tomáš Pevn`y. Loda: Lightweight on-line detector of anomalies. Machine Learning, 102:275–
422"
REFERENCES,0.40804106073567153,"304, 2016.
423"
REFERENCES,0.40889649272882805,"[40] Douglas A Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663),
424"
REFERENCES,0.40975192472198463,"2009.
425"
REFERENCES,0.41060735671514115,"[41] Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grégoire Montavon, Wojciech
426"
REFERENCES,0.41146278870829767,"Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert Müller. A unifying review of
427"
REFERENCES,0.41231822070145424,"deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756–795, 2021.
428"
REFERENCES,0.41317365269461076,"[42] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui,
429"
REFERENCES,0.41402908468776733,"Alexander Binder, Emmanuel Müller, and Marius Kloft. Deep one-class classification. In
430"
REFERENCES,0.41488451668092385,"International conference on machine learning, pages 4393–4402. PMLR, 2018.
431"
REFERENCES,0.41573994867408043,"[43] Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear
432"
REFERENCES,0.41659538066723695,"dimensionality reduction. In Proceedings of the MLSDA 2014 2nd workshop on machine
433"
REFERENCES,0.4174508126603935,"learning for sensory data analysis, pages 4–11, 2014.
434"
REFERENCES,0.41830624465355004,"[44] Osman Salem, Yaning Liu, Ahmed Mehaoua, and Raouf Boutaba. Online anomaly detection in
435"
REFERENCES,0.41916167664670656,"wireless body area networks for reliable healthcare monitoring. IEEE journal of biomedical
436"
REFERENCES,0.42001710863986313,"and health informatics, 18(5):1541–1551, 2014.
437"
REFERENCES,0.42087254063301965,"[45] Hanyu Song, Peizhao Li, and Hongfu Liu. Deep clustering based fair outlier detection. In
438"
REFERENCES,0.42172797262617623,"Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,
439"
REFERENCES,0.42258340461933275,"pages 1481–1489, 2021.
440"
REFERENCES,0.4234388366124893,"[46] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly
441"
REFERENCES,0.42429426860564584,"detection. In International Conference on Machine Learning, pages 21076–21089. PMLR,
442"
REFERENCES,0.4251497005988024,"2022.
443"
REFERENCES,0.42600513259195893,"[47] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer,
444"
REFERENCES,0.4268605645851155,"Remi Munos, Petar Veliˇckovi´c, and Michal Valko. Large-scale representation learning on graphs
445"
REFERENCES,0.42771599657827203,"via bootstrapping. arXiv preprint arXiv:2102.06514, 2021.
446"
REFERENCES,0.42857142857142855,"[48] Laurens Van Der Maaten. Learning a parametric embedding by preserving local structure. In
447"
REFERENCES,0.4294268605645851,"Artificial intelligence and statistics, pages 384–391. PMLR, 2009.
448"
REFERENCES,0.43028229255774164,"[49] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics
449"
REFERENCES,0.4311377245508982,"and intelligent laboratory systems, 2(1-3):37–52, 1987.
450"
REFERENCES,0.43199315654405473,"[50] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering
451"
REFERENCES,0.4328485885372113,"analysis. In International conference on machine learning, pages 478–487. PMLR, 2016.
452"
REFERENCES,0.43370402053036783,"[51] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for
453"
REFERENCES,0.4345594525235244,"anomaly detection. IEEE Transactions on Knowledge and Data Engineering, 2023.
454"
REFERENCES,0.4354148845166809,"[52] Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, and Thomas AJ Schweiger. Scan: a structural
455"
REFERENCES,0.43627031650983744,"clustering algorithm for networks. In Proceedings of the 13th ACM SIGKDD international
456"
REFERENCES,0.437125748502994,"conference on Knowledge discovery and data mining, pages 824–833, 2007.
457"
REFERENCES,0.43798118049615054,"[53] Zhiming Xu, Xiao Huang, Yue Zhao, Yushun Dong, and Jundong Li. Contrastive attributed
458"
REFERENCES,0.4388366124893071,"network anomaly detection with data augmentation. In Advances in Knowledge Discovery and
459"
REFERENCES,0.43969204448246363,"Data Mining: 26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May 16–19, 2022,
460"
REFERENCES,0.4405474764756202,"Proceedings, Part II, pages 444–457. Springer, 2022.
461"
REFERENCES,0.4414029084687767,"[54] Xu Yuan, Na Zhou, Shuo Yu, Huafei Huang, Zhikui Chen, and Feng Xia. Higher-order structure
462"
REFERENCES,0.4422583404619333,"based anomaly detection on attributed networks. In 2021 IEEE International Conference on
463"
REFERENCES,0.4431137724550898,"Big Data (Big Data), pages 2691–2700. IEEE, 2021.
464"
REFERENCES,0.4439692044482464,"[55] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-Ping Phoebe Chen. Genera-
465"
REFERENCES,0.4448246364414029,"tive and contrastive self-supervised learning for graph anomaly detection. IEEE Transactions
466"
REFERENCES,0.44568006843455943,"on Knowledge and Data Engineering, 2021.
467"
REFERENCES,0.446535500427716,"[56] Shuang Zhou, Xiao Huang, Ninghao Liu, Qiaoyu Tan, and Fu-Lai Chung. Unseen anomaly
468"
REFERENCES,0.4473909324208725,"detection on networks via multi-hypersphere learning. In Proceedings of the 2022 SIAM
469"
REFERENCES,0.4482463644140291,"International Conference on Data Mining (SDM), pages 262–270. SIAM, 2022.
470"
REFERENCES,0.4491017964071856,"[57] Shuang Zhou, Qiaoyu Tan, Zhiming Xu, Xiao Huang, and Fu-lai Chung. Subtractive aggregation
471"
REFERENCES,0.4499572284003422,"for attributed network anomaly detection. In Proceedings of the 30th ACM International
472"
REFERENCES,0.4508126603934987,"Conference on Information & Knowledge Management, pages 3672–3676, 2021.
473"
REFERENCES,0.4516680923866553,"[58] Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and
474"
REFERENCES,0.4525235243798118,"Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection.
475"
REFERENCES,0.4533789563729683,"In International conference on learning representations, 2018.
476"
REFERENCES,0.4542343883661249,Algorithm 1 Model training for UniCAD
REFERENCES,0.4550898203592814,"Input: data points X, cluster number K, outlier ratio l, tolerance λ, iterations t
Output: network parameters Θ, mixture parameters {ωk, µk, Σk}"
REFERENCES,0.455945252352438,"1: Initialize Θ and {µk, ωk, Σk};
2: for i = 1 to t do
3:
if i = 1 then
4:
Xi ←X;
5:
else
6:
Re-order the point in X such that o1 ≥· · · ≥on;
7:
Li ←{x1, . . . , x⌊N∗l⌋};
8:
Xi ←X \ Li;
9:
end if
10:
Update Θ with Equation (15);
11:
while |J(Θ, Φ) −Jold(Θ, Φ)| > λ do
12:
Jold(Θ, Φ) = J(Θ, Φ);
13:
Calculate τ with Equation (10);
14:
Update {ωk, µk, Σk} with Equation (12), (13) and (14);
15:
end while
16:
Calculate oi with Equation (9);
17: end for
18: return Θ and {ωk, µk, Σk}"
REFERENCES,0.4568006843455945,"A
Iterative Training Algorithm
477"
REFERENCES,0.4576561163387511,"The pseudocode for training the model is presented in Algorithm 1. Initially, all parameters undergo
478"
REFERENCES,0.4585115483319076,"random initialization. In subsequent iterations, following the initial round, the outlier set L undergoes
479"
REFERENCES,0.4593669803250642,"updates based on the anomaly score oi. This is succeeded by the adjustment of the network parameters
480"
REFERENCES,0.4602224123182207,"Θ based on xi, further optimizing the performance of Θ through the utilization of the estimated
481"
REFERENCES,0.46107784431137727,"parameters µk, ωk, Σk. The essence of the algorithm is embedded in its alternating optimization
482"
REFERENCES,0.4619332763045338,"strategy, iteratively refining the accuracy of representation learning and mixed model parameter
483"
REFERENCES,0.4627887082976903,"estimation, thereby augmenting the overall training effectiveness of the model.
484"
REFERENCES,0.4636441402908469,"B
Derivation of EM Algorithm
485"
REFERENCES,0.4644995722840034,"This appendix provides the detailed derivation of the Expectation-Maximization (EM) algorithm
486"
REFERENCES,0.46535500427716,"for optimizing the parameters of a mixture model based on Student’s t-distribution. The focus is
487"
REFERENCES,0.4662104362703165,"on deriving analytical solutions for the maximization of the parameters Φ = {µk, Σk, ωk} of the
488"
REFERENCES,0.46706586826347307,"mixture components. The EM algorithm alternates between two steps:
489"
REFERENCES,0.4679213002566296,"In the E-step, we calculate the posterior probabilities τik, representing the probability of data point
490"
REFERENCES,0.46877673224978617,"i belonging to cluster k, given the current parameters. The posterior probabilities for a Student’s
491"
REFERENCES,0.4696321642429427,"t-distribution mixture model are formulated as:
492"
REFERENCES,0.4704875962360992,"τik =
ωk · p(zi|µk, Σk)
PK
j=1 ωj · p(zi|µj, Σj)
,
(16)"
REFERENCES,0.4713430282292558,"where τ(zi|µk, Σk) denotes the Student’s t-distribution for data point i with respect to cluster k, and
493"
REFERENCES,0.4721984602224123,"K is the number of mixture components.
494"
REFERENCES,0.47305389221556887,"The Student’s t-distribution is depicted as a hierarchical conditional probability, resembling a Gaussian
495"
REFERENCES,0.4739093242087254,"distribution with an accuracy scale factor u, where its latent variable follows a gamma distribution.
496"
REFERENCES,0.47476475620188197,"Adopting a degree of freedom ν = 1, the value of uik is given by:
497"
REFERENCES,0.4756201881950385,"uik =
ν + 1
ν + DM(zi, µk) =
2
1 + DM(zi, µk)
(17)"
REFERENCES,0.47647562018819506,"In the M-step, we update the parameters Φ = {ωk, µk, and Σk} using the derivatives obtained in
498"
REFERENCES,0.4773310521813516,"the previous steps. In our model, the likelihood function for a Student’s-t Distribution Mixture Model
499"
REFERENCES,0.47818648417450815,Figure 3: Score comparison with other methods.
REFERENCES,0.47904191616766467,"(SMM) is represented as:
500"
REFERENCES,0.4798973481608212,"L(ω, µ, Σ) = N
X i=1 K
X"
REFERENCES,0.48075278015397777,"k=1
ωk ·
π−1 · |Σk|−1 2"
REFERENCES,0.4816082121471343,"1 + (zi −µk)T Σ−1
k (zi −µk),
(18)"
REFERENCES,0.48246364414029086,"where ωk are the mixture weights, Σk the covariance matrices, µk the means, and zi the data points.
501"
REFERENCES,0.4833190761334474,"The derivative with respect to ωk must consider the constraint that the sum of the mixture weights
502"
REFERENCES,0.48417450812660395,"equals 1, i.e., P
k ωk = 1. Hence, we introduce a Lagrange multiplier λ to address this constraint
503"
REFERENCES,0.48502994011976047,"and construct the Lagrangian L′:
504"
REFERENCES,0.48588537211291705,"L′(ω, µ, Σ, λ) = L(ω, µ, Σ) + λ  1 − K
X"
REFERENCES,0.48674080410607357,"k=1
ωk !"
REFERENCES,0.4875962360992301,",
(19)"
REFERENCES,0.48845166809238666,"The derivative with respect to ωk is:
505 ∂L′"
REFERENCES,0.4893071000855432,"∂ωk
= ∂L"
REFERENCES,0.49016253207869975,"∂ωk
−λ,
(20)"
REFERENCES,0.49101796407185627,"Substituting the definition of L(ω, µ, Σ), we obtain:
506"
REFERENCES,0.49187339606501285,"∂L
∂ωk
=
X i"
REFERENCES,0.49272882805816937,"p(zi|µk, Σk)
PK
j=1 ωj · p(zi|µj, Σj)
=
X i τik"
REFERENCES,0.49358426005132594,"ωk
,
(21)"
REFERENCES,0.49443969204448246,"To solve for ωk, we first multiply both sides of the equation by ωk and apply the constraint condition:
507 X k
ωk X i τik ωk
−λ !"
REFERENCES,0.49529512403763903,"= 0,
(22)"
REFERENCES,0.49615055603079555,"Upon further organization, we find that the Lagrange multiplier λ actually equals the total number of
508"
REFERENCES,0.49700598802395207,data points N (since P
REFERENCES,0.49786142001710865,"i τik = Nk, where Nk is the expected total number of data points belonging
509"
REFERENCES,0.49871685201026517,"to the kth component, and the sum of all Nk equals the total number of data points N).
510"
REFERENCES,0.49957228400342174,"Finally, we can solve for ωk:
511"
REFERENCES,0.5004277159965783,"ωk =
P
i τik
N
,
(23)"
REFERENCES,0.5012831479897348,"This result indicates that the weight ωk of each mixture component equals the proportion of the
512"
REFERENCES,0.5021385799828914,"posterior probabilities of the data points it contains relative to all data points.
513"
REFERENCES,0.5029940119760479,"To update µk and Σk, we consider the conditional expectation of the data log-likelihood function:
514"
REFERENCES,0.5038494439692045,"Q(µk, Σk) = N
X"
REFERENCES,0.504704875962361,"i=1
τik"
REFERENCES,0.5055603079555175,"
−log(π) −1"
REFERENCES,0.5064157399486741,2 log |σk| + 1
LOG UIK,0.5072711719418306,2 log uik −1
LOG UIK,0.5081266039349872,"2uik(zi −µk)T Σ−1
k (zi −µk)

(24)"
LOG UIK,0.5089820359281437,"(a) Scalar Sum
(b) Vector Sum"
LOG UIK,0.5098374679213002,Figure 4: Analysis of gravitational force.
LOG UIK,0.5106928999144568,"Maximizing Q(µk, Σk) with respect to µk leads to:
515"
LOG UIK,0.5115483319076134,"∂Q
∂µk
= 1 2 N
X"
LOG UIK,0.5124037639007699,"i=1
τikuik(2Σ−1
k µk −2Σ−1
k zik)
(25)"
LOG UIK,0.5132591958939264,Setting ∂Q
LOG UIK,0.514114627887083,"∂µk = 0 results in the updated mean µ(t+1)
k
:
516"
LOG UIK,0.5149700598802395,"µ(t+1)
k
= n
X i=1"
LOG UIK,0.5158254918733961,"
τ (t+1)
ik
u(t+1)
ik
zi

/ n
X i=1"
LOG UIK,0.5166809238665526,"
τ (t+1)
ik
u(t+1)
ik

.
(26)"
LOG UIK,0.5175363558597091,"Considering the derivative of Q(µk, Σk) with respect to Σ−1
k :
517"
LOG UIK,0.5183917878528657,"∂Q
∂Σ−1
k
= 1 2 N
X"
LOG UIK,0.5192472198460223,"i=1
τik
 
Σk −uik(zi −µk) × (zi −µk)T 
.
(27)"
LOG UIK,0.5201026518391788,Setting ∂Q
LOG UIK,0.5209580838323353,"∂µk = 0 yields the updated covariance matrix Σ(t+1)
k
:
518"
LOG UIK,0.5218135158254918,"Σ(t+1)
k
=
Pn
i=1 τ (t+1)
ik
u(t+1)
ik
(zi −µ(t+1)
k
)(zi −µ(t+1)
k
)T
PK
j=1 τ (t+1)
ij
.
(28)"
LOG UIK,0.5226689478186484,"C
Anomaly Score with Vector Sum
519"
LOG UIK,0.523524379811805,"C.1
Advantages
520"
LOG UIK,0.5243798118049615,"Here we discuss the advantages of employing vector sum in anomaly score with a toy example.
521"
LOG UIK,0.525235243798118,"The application of the vector sum principle extends beyond physical mechanics and finds relevance
522"
LOG UIK,0.5260906757912746,"in various domains. In relational embedding [5], for example, relationships can be represented as
523"
LOG UIK,0.5269461077844312,"vectors. Aggregating these vectors allows for capturing complexities like transitivity, symmetry, and
524"
LOG UIK,0.5278015397775877,"antisymmetry.
525"
LOG UIK,0.5286569717707442,"Similarly, in our context, the vector sum can help capture more complex relationships along clus-
526"
LOG UIK,0.5295124037639007,"ters. Consider Figure 4 as an example, where a sample v is attracted by two groups of cluster
527"
LOG UIK,0.5303678357570573,"prototypes ({µ1, µ2}, {µ3, µ4}) with the same mass and sample-prototype distances ( em1 = em2 =
528"
LOG UIK,0.5312232677502139,"em3 = em4, erv1 = erv2 = erv3 = erv4). Without considering the direction of the forces, the two groups
529"
LOG UIK,0.5320786997433704,"of prototypes would attract the sample with equal forces. However, we argue that the two groups of
530"
LOG UIK,0.5329341317365269,"prototypes should exert different influences. A sample close to two clusters with a large difference
531"
LOG UIK,0.5337895637296834,"({µ1, µ2}) is more likely to be an anomaly compared to a sample that is close to two clusters with
532"
LOG UIK,0.5346449957228401,"a smaller difference ({µ3, µ4}). For example, in a social network, a user who equally likes two
533"
LOG UIK,0.5355004277159966,"extremely different communities, like money-saving tips and luxury items, is more anomalous than
534"
LOG UIK,0.5363558597091531,"a user who equally likes two similar communities, like private jets and luxury items. Applying
535"
LOG UIK,0.5372112917023096,"the vector sum, the total force of {µ1, µ2} is much smaller than that of {µ3, µ4}. As the anomaly
536"
LOG UIK,0.5380667236954663,"score is inversely related to the total force, it is more anomalous when equally attracted by {µ1, µ2}
537"
LOG UIK,0.5389221556886228,"with large difference. This indicates that the vector sum successfully captures subtle differences
538"
LOG UIK,0.5397775876817793,"in the distinctions among multiple clusters, thereby assisting in the identification of more accurate
539"
LOG UIK,0.5406330196749358,"anomalies.
540"
LOG UIK,0.5414884516680923,"C.2
Toy Example
541"
LOG UIK,0.542343883661249,"In the appendix, as illustrated in Figure 3, we investigated a toy example. We discussed a specific
542"
LOG UIK,0.5431993156544055,"pattern of anomalies termed group anomalies, where a small number of anomalous samples cluster
543"
LOG UIK,0.544054747647562,"together. It is crucial to note that we do not claim this anomaly pattern is common in real-world data;
544"
LOG UIK,0.5449101796407185,"our goal is merely to point out a specific anomaly pattern that is challenging for traditional cluster-
545"
LOG UIK,0.5457656116338752,"based anomaly detection methods to detect. Specifically, we utilize three Gaussian distributions with
546"
LOG UIK,0.5466210436270317,"high variance (each generating 300 data samples) and one with lower variance (generating 30 data
547"
LOG UIK,0.5474764756201882,"samples). Because the samples from the smaller Gaussian follow a different generative mechanism
548"
LOG UIK,0.5483319076133447,"and represent a minority in the dataset, we consider them anomalies.
549"
LOG UIK,0.5491873396065012,"We set the cluster number for KMeans-- and GMM at four, indicating that the Gaussian distribution
550"
LOG UIK,0.5500427715996579,"comprising anomalous samples was also recognized as a cluster. KMeans-- employs a cluster-based
551"
LOG UIK,0.5508982035928144,"approach, using the distance to the nearest cluster center as the anomaly score, while GMM uses
552"
LOG UIK,0.5517536355859709,"a probability-based approach, considering the samples’ likelihood in the mixture model as the
553"
LOG UIK,0.5526090675791274,"anomaly score. However, both approaches are ineffective in this scenario. Rather than identifying the
554"
LOG UIK,0.553464499572284,"small cluster as anomalous, they tend to misidentify samples on the peripheries of larger clusters as
555"
LOG UIK,0.5543199315654406,"anomalies.
556"
LOG UIK,0.5551753635585971,"By contrast, our scoring method views the entire small cluster as more likely anomalous, followed by
557"
LOG UIK,0.5560307955517536,"outlier samples on the margins of the larger clusters. This visualization provides a perspective that
558"
LOG UIK,0.5568862275449101,"distinguishes our method from previous efforts.
559"
LOG UIK,0.5577416595380668,"D
Experimental Supplementary
560"
LOG UIK,0.5585970915312233,"D.1
Benchmark Datasets Details
561"
LOG UIK,0.5594525235243798,"Due to space constraints in the main text, we utilized 30 public datasets from ADBench [17], covering
562"
LOG UIK,0.5603079555175363,"all different types of data. The details of the 30 datasets are presented in Table 4.
563"
LOG UIK,0.561163387510693,"D.2
Baselines Details
564"
LOG UIK,0.5620188195038495,"A comprehensive overview of the unsupervised anomaly detection methods is presented below.
565"
LOG UIK,0.562874251497006,"D.2.1
Traditional Models
566"
LOG UIK,0.5637296834901625,"• Subspace Outlier Detection (SOD) [24]: Identifies outliers in varying subspaces of a high-
567"
LOG UIK,0.564585115483319,"dimensional feature space, targeting anomalies that emerge in lower-dimensional projections.
568"
LOG UIK,0.5654405474764757,"• Histogram-based Outlier Detection (HBOS) [16]: Assumes feature independence and calculates
569"
LOG UIK,0.5662959794696322,"outlyingness via histograms, offering scalability and efficiency.
570"
LOG UIK,0.5671514114627887,"D.2.2
Linear Models
571"
LOG UIK,0.5680068434559452,"• Principal Component Analysis (PCA) [49]: Utilizes singular value decomposition for dimension-
572"
LOG UIK,0.5688622754491018,"ality reduction, with anomalies indicated by reconstruction errors.
573"
LOG UIK,0.5697177074422584,"• One-class SVM (OCSVM) [32]: Defines a decision boundary to separate normal samples from
574"
LOG UIK,0.5705731394354149,"outliers, maximizing the margin from the data origin.
575"
LOG UIK,0.5714285714285714,"D.2.3
Density-based Models
576"
LOG UIK,0.572284003421728,"• Local Outlier Factor (LOF) [6] : Measures local density deviation, marking samples as outliers if
577"
LOG UIK,0.5731394354148845,"they lie in less dense regions compared to their neighbors.
578"
LOG UIK,0.5739948674080411,"• K-Nearest Neighbors (KNN) [38]: Anomaly scores are assigned based on the distance to the k-th
579"
LOG UIK,0.5748502994011976,"nearest neighbor, embodying a simple yet effective approach.
580"
LOG UIK,0.5757057313943541,"D.2.4
Ensemble-based Models
581"
LOG UIK,0.5765611633875107,"• Lightweight On-line Detector of Anomalies (LODA) [39] : An ensemble method suitable for
582"
LOG UIK,0.5774165953806673,"real-time processing and adaptable to concept drift through random projections and histograms.
583"
LOG UIK,0.5782720273738238,"• Isolation Forest (IForest) [29]: Isolates anomalies by randomly selecting features and split values,
584"
LOG UIK,0.5791274593669803,"leveraging the ease of isolating anomalies to identify them efficiently.
585"
LOG UIK,0.5799828913601369,Table 4: Statistics of tabular benchmark datasets.
LOG UIK,0.5808383233532934,"Data
# Samples
# Features
# Anomaly
% Anomaly
Category"
LOG UIK,0.58169375534645,"annthyroid
7200
6
534
7.42
Healthcare
backdoor
95329
196
2329
2.44
Network
breastw
683
9
239
34.99
Healthcare
campaign
41188
62
4640
11.27
Finance
celeba
202599
39
4547
2.24
Image
census
299285
500
18568
6.20
Sociology
glass
214
7
9
4.21
Forensic
Hepaitis
80
19
13
16.25
Healthcare
http
567498
3
2211
0.39
Web
Ionosphere
351
33
126
35.90
Oryctognosy
landsat
6435
36
1333
20.71
Astronautics
Lymphography
148
18
6
4.05
Healthcare
magic.gamma
19020
10
6688
35.16
Physical
mnist
7603
100
700
9.21
Image
musk
3062
166
97
3.17
Chemistry
pendigits
6870
16
156
2.27
Image
Pima
768
8
268
34.90
Healthcare
satellite
6435
36
2036
31.64
Astronautics
satimage-2
5803
36
71
1.22
Astronautics
shuttle
49097
9
3511
7.15
Astronautics
skin
245057
3
50859
20.75
Image
Stamps
340
9
31
9.12
Document
thyroid
3772
6
93
2.47
Healthcare
vertebral
240
6
30
12.50
Biology
vowels
1456
12
50
3.43
Linguistics
Waveform
3443
21
100
2.90
Physics
WBC
223
9
10
4. 48
Healthcare
Wilt
4819
5
257
5.33
Botany
wine
129
13
10
7.75
Chemistry
WPBC
198
33
47
23.74
Healthcare"
LOG UIK,0.5825491873396065,"D.2.5
Probability-based Models
586"
LOG UIK,0.583404619332763,"• Deep Autoencoding Gaussian Mixture Model (DAGMM) [58]: Combines a deep autoencoder
587"
LOG UIK,0.5842600513259196,"with a GMM for anomaly scoring, utilizing both low-dimensional representation and reconstruction
588"
LOG UIK,0.5851154833190761,"error.
589"
LOG UIK,0.5859709153122327,"• Empirical-Cumulative-distribution-based Outlier Detection (ECOD) [28]: Uses ECDFs to
590"
LOG UIK,0.5868263473053892,"estimate feature densities independently, targeting outliers in distribution tails.
591"
LOG UIK,0.5876817792985458,"• Copula Based Outlier Detector (COPOD) [27]: A hyperparameter-free method leveraging
592"
LOG UIK,0.5885372112917023,"empirical copula models for interpretable and efficient outlier detection.
593"
LOG UIK,0.5893926432848589,"D.2.6
Cluster-based Models
594"
LOG UIK,0.5902480752780154,"• DBSCAN [13]: A density-based clustering algorithm that identifies clusters based on the density
595"
LOG UIK,0.5911035072711719,"of data points, effectively separating high-density clusters from low-density noise, and is widely
596"
LOG UIK,0.5919589392643285,"used for anomaly detection in spatial data.
597"
LOG UIK,0.592814371257485,"• Clustering Based Local Outlier Factor (CBLOF) [18]: Calculates anomaly scores based on
598"
LOG UIK,0.5936698032506416,"cluster distances, using global data distribution.
599"
LOG UIK,0.5945252352437981,"• KMeans-- [45]: Extends k-means to include outlier detection in the clustering process, offering an
600"
LOG UIK,0.5953806672369547,"integrated approach to anomaly detection.
601"
LOG UIK,0.5962360992301112,"• Deep Clustering-based Fair Outlier Detection (DCFOD) [9]: Enhances outlier detection with a
602"
LOG UIK,0.5970915312232677,"focus on fairness, combining deep clustering and adversarial training for representation learning.
603"
LOG UIK,0.5979469632164243,"Table 5: AUCROC of 17 unsupervised algorithms on 30 tabular benchmark datasets. In each dataset,
the algorithm with the highest AUCROC is marked in red, the second highest in blue, and the third
highest in green."
LOG UIK,0.5988023952095808,"Dataset
SOD
HBOS
PCA
OC
SVM
LOF
KNN
LODA
IForest
DA
GMM
ECOD
COPOD
DB
SCAN
CBLOF
DCOD
KMeans--
Deep
SVDD
DIF
UniCAD
(Scalar)
UniCAD
(Vector)"
LOG UIK,0.5996578272027374,"annthyroid
77.38
60.15
66.24
57.23
70.20
71.69
41.02
82.01
56.53
78.66
76.80
50.08
62.28
55.01
64.99
76.09
66.76
75.27
72.72
backdoor
68.77
71.56
80.16
85.04
85.79
80.58
66.38
72.15
55.98
86.08
80.97
76.55
81.91
79.57
89.11
78.83
92.87
87.28
89.24
breastw
93.97
98.94
95.13
80.30
40.61
97.01
98.49
98.32
N/A
99.17
99.68
85.20
96.86
99.02
97.05
63.36
77.45
98.15
98.56
campaign
69.16
78.55
72.78
65.70
59.04
72.27
51.67
71.71
56.03
76.10
77.69
50.60
64.34
63.16
63.51
54.42
67.53
73.52
73.64
celeba
48.44
76.18
79.38
70.70
38.95
59.63
60.17
70.41
44.74
76.48
75.68
50.36
73.99
91.41
56.76
45.17
65.29
81.38
82.00
census
62.12
64.89
68.74
54.90
47.46
66.88
37.14
59.52
59.65
67.63
69.07
58.50
60.17
72.84
63.33
54.16
59.66
67.90
67.84
glass
73.36
77.23
66.29
35.36
69.20
82.29
73.13
77.13
76.09
65.83
72.43
54.55
78.30
78.07
77.30
55.71
84.57
79.52
82.17
Hepatitis
67.83
79.85
75.95
67.75
38.06
52.76
64.87
69.75
54.80
75.22
82.05
68.12
73.05
48.38
64.64
57.45
74.24
75.53
80.62
http
78.04
99.53
99.72
99.59
27.46
3.37
12.48
99.96
N/A
98.10
99.29
49.97
99.60
99.53
99.55
60.38
99.49
99.53
99.52
Ionosphere
86.37
62.49
79.19
75.92
90.59
88.26
78.42
84.50
73.41
73.15
79.34
81.12
90.79
57.78
91.36
53.94
89.74
92.04
90.37
landsat
59.54
55.14
35.76
36.15
53.90
57.95
38.17
47.64
43.92
36.10
41.55
50.17
63.69
33.40
55.31
62.48
54.84
49.60
57.37
Lymphography
71.22
99.49
99.82
99.54
89.86
55.91
85.55
99.81
72.11
99.52
99.48
74.16
99.81
81.19
100.00
71.91
83.67
99.29
99.73
mnist
60.10
60.42
85.29
82.95
67.13
80.58
72.27
80.98
67.23
74.61
77.74
50.00
79.96
65.23
82.45
50.98
88.16
86.00
86.64
musk
74.09
100.00
100.00
80.58
41.18
69.89
95.11
99.99
76.85
95.40
94.20
50.00
100.00
42.19
72.16
66.02
98.22
99.92
100.00
pendigits
66.29
93.04
93.73
93.75
47.99
72.95
89.10
94.76
64.22
93.01
90.68
55.33
96.93
94.33
94.37
27.32
93.79
95.12
95.52
Pima
61.25
71.07
70.77
66.92
65.71
73.43
65.93
72.87
55.93
63.05
69.10
51.39
71.49
72.16
70.44
49.49
67.28
75.16
74.87
satellite
63.96
74.80
59.62
59.02
55.88
65.18
61.98
70.43
62.33
58.09
63.20
55.52
71.32
55.97
67.71
57.40
74.52
72.46
77.65
satimage-2
83.08
97.65
97.62
97.35
47.36
92.60
97.56
99.16
96.29
96.28
97.21
75.74
99.84
86.01
99.88
55.68
99.63
99.87
99.88
shuttle
69.51
98.63
98.62
97.40
57.11
69.64
60.95
99.56
97.92
99.13
99.35
50.40
93.07
97.20
69.97
51.81
97.00
99.15
98.75
skin
60.35
60.15
45.26
49.45
46.47
71.46
45.75
68.21
N/A
49.08
47.55
50.00
68.03
64.34
65.47
45.69
66.36
72.26
69.69
Stamps
73.26
90.73
91.47
83.86
51.26
68.61
87.18
91.21
88.89
87.87
93.40
52.08
69.89
93.41
79.78
59.48
87.95
91.37
94.18
thyroid
92.81
95.62
96.34
87.92
86.86
95.93
74.30
98.30
79.75
97.94
94.30
53.57
94.74
78.55
92.26
52.14
96.26
97.66
97.48
vertebral
40.32
28.56
37.06
37.99
49.29
33.79
30.57
36.66
53.20
40.66
25.64
49.74
41.01
38.13
38.14
37.81
47.20
33.11
47.37
vowels
92.65
72.21
65.29
61.59
93.12
97.26
70.36
73.94
60.58
62.24
53.15
57.50
92.12
51.56
93.45
49.87
81.02
88.38
92.09
Waveform
68.57
68.77
65.48
56.29
73.32
73.78
60.13
71.47
49.35
62.36
75.03
66.41
71.27
63.47
74.35
53.94
75.33
71.81
74.29
WBC
94.60
98.72
98.20
99.03
54.17
90.56
96.91
99.01
N/A
99.11
99.11
87.43
96.88
94.92
97.45
62.46
81.27
97.68
98.93
Wilt
53.25
32.49
20.39
31.28
50.65
48.42
26.42
41.94
37.29
36.30
33.40
49.96
34.50
44.71
34.91
45.90
39.46
48.95
52.56
wine
46.11
91.36
84.37
73.07
37.74
44.98
90.12
80.37
61.70
77.22
88.65
40.33
27.14
82.18
27.36
64.26
41.69
82.72
95.25
WPBC
51.28
51.24
46.01
45.35
41.41
46.59
49.31
46.63
47.80
46.65
49.34
52.22
45.32
49.67
45.01
44.01
44.69
48.02
49.90"
LOG UIK,0.6005132591958939,"Avg. Rank
11.00
8.26
8.98
11.59
13.59
10.00
13.24
7.09
13.24
9.19
8.29
14.21
8.07
10.90
8.71
15.48
8.38
5.41
3.59"
LOG UIK,0.6013686911890505,"(a) AUC-ROC
(b) AUC-PR"
LOG UIK,0.602224123182207,Figure 5: Critical difference diagrams for AUC-ROC and AUC-PR.
LOG UIK,0.6030795551753636,"D.2.7
Neural Network-based Models
604"
LOG UIK,0.6039349871685201,"• Deep Support Vector Data Description (DeepSVDD) [42]: Minimizes the volume of a hyper-
605"
LOG UIK,0.6047904191616766,"sphere enclosing network data representations, isolating anomalies outside this sphere.
606"
LOG UIK,0.6056458511548332,"• Deep Isolation Forest for Anomaly Detection (DIF) [51]: Utilizes deep learning to enhance
607"
LOG UIK,0.6065012831479898,"traditional isolation forest techniques, offering improved anomaly detection in complex datasets
608"
LOG UIK,0.6073567151411463,"with minimal parameter tuning.
609"
LOG UIK,0.6082121471343028,"Each method’s unique mechanism and application context provide a rich landscape of techniques
610"
LOG UIK,0.6090675791274593,"for unsupervised anomaly detection, illustrating the field’s diverse methodologies and the breadth of
611"
LOG UIK,0.6099230111206159,"approaches to tackling anomaly detection challenges.
612"
LOG UIK,0.6107784431137725,"D.3
Supplementary Experimental Results
613"
LOG UIK,0.611633875106929,"In the appendix, we detail the statistical analysis conducted to compare the performance of various
614"
LOG UIK,0.6124893071000855,"anomaly detectors. We obtained this diagram by conducting a Friedman test (p-value: 4.657e-19),
615"
LOG UIK,0.613344739093242,"indicating significant differences among different detectors. We utilized average ranks and the
616"
LOG UIK,0.6142001710863987,"Nemenyi test to generate the critical difference diagram, as shown in Figure 5. It is noteworthy that
617"
LOG UIK,0.6150556030795552,"the vector version exhibits significantly superior performance compared to the scalar version across
618"
LOG UIK,0.6159110350727117,"more methods. The detailed outcomes for the AUCROC and AUCPR metrics, spanning 30 datasets
619"
LOG UIK,0.6167664670658682,"and against 17 baseline approaches, are showcased in Table 5 and Table 6.
620"
LOG UIK,0.6176218990590248,"D.4
Complexity Analysis
621"
LOG UIK,0.6184773310521814,"The complexity of each iteration in UniCAD involves three parts: constructing the outlier set,
622"
LOG UIK,0.6193327630453379,"updating the network parameters Θ, and optimizing the mixture model using the EM algorithm.
623"
LOG UIK,0.6201881950384944,"Table 6: AUCPR of 17 unsupervised algorithms on 30 tabular benchmark datasets. In each dataset,
the algorithm with the highest AUCPR is marked in red, the second highest in blue, and the third
highest in green."
LOG UIK,0.621043627031651,"Dataset
SOD
HBOS
PCA
OC
SVM
LOF
KNN
LODA
IForest
DA
GMM
ECOD
COPOD
DB
SCAN
CBLOF
DCOD
KMeans--
Deep
SVDD
DIF
UniCAD
(Scalar)
UniCAD
(Vector)"
LOG UIK,0.6218990590248076,"annthyroid
18.84
16.99
16.12
10.37
15.71
16.74
7.06
30.47
9.64
25.35
16.58
7.60
13.74
10.01
15.41
21.75
18.93
26.37
25.03
backdoor
37.07
4.96
31.29
8.79
26.14
44.37
13.84
4.75
5.47
10.72
7.69
21.04
7.03
6.77
15.47
55.70
41.46
37.77
36.36
breastw
84.88
97.71
95.11
82.70
28.55
92.19
97.04
96.04
N/A
98.54
99.40
78.42
91.94
96.83
92.25
48.60
50.65
94.47
95.90
campaign
19.14
38.01
27.90
29.25
14.59
27.18
14.11
32.26
14.54
36.65
38.58
11.43
20.88
19.61
18.86
16.75
26.52
27.66
27.12
celeba
2.36
13.82
15.89
10.73
1.73
3.14
4.04
8.96
1.95
13.96
13.69
2.32
11.22
17.48
3.19
2.73
5.44
15.12
14.66
census
8.54
8.68
10.02
6.82
5.48
9.04
5.03
7.78
9.03
9.46
9.92
7.52
7.52
10.92
8.13
8.42
7.42
9.70
9.75
glass
18.73
11.82
10.05
8.02
20.11
20.26
13.37
10.99
24.58
15.35
9.78
6.88
11.57
9.66
14.66
8.46
18.86
13.29
15.33
Hepatitis
24.73
37.73
36.65
29.44
13.67
21.95
30.90
26.25
22.93
32.80
41.50
22.31
36.54
19.53
25.14
30.04
34.93
36.08
43.37
http
8.32
44.79
56.43
46.86
3.82
0.70
0.67
90.83
N/A
16.61
35.19
0.37
47.53
44.03
45.09
13.39
41.72
43.53
43.52
Ionosphere
85.88
41.78
73.92
74.54
88.07
90.41
73.04
80.41
64.97
64.69
69.89
63.04
89.77
47.63
91.36
43.24
87.45
89.55
87.61
landsat
26.38
22.03
16.18
16.21
24.69
24.65
18.86
19.81
24.48
16.24
17.48
20.80
31.05
15.57
22.40
36.92
24.35
20.84
23.27
Lymphography
22.00
91.83
97.02
93.59
23.08
38.69
44.54
97.31
19.52
90.87
88.68
7.66
97.31
12.34
100.00
34.58
32.84
91.69
96.66
mnist
19.15
12.51
39.93
33.20
20.90
35.53
25.86
27.71
23.75
17.45
21.35
9.21
30.60
23.59
37.12
20.18
44.55
41.19
41.94
musk
7.59
100.00
99.89
10.61
2.82
9.65
47.60
99.61
32.76
50.13
34.79
3.16
100.00
2.87
37.55
8.78
70.70
97.65
99.96
pendigits
4.46
29.27
23.65
23.52
3.78
6.50
18.71
26.05
4.67
30.65
21.22
2.94
32.87
22.21
32.67
1.53
23.75
24.86
21.68
Pima
48.24
56.61
54.03
50.00
47.18
55.14
44.09
55.82
41.55
50.45
55.19
36.65
52.99
50.24
53.50
35.02
46.34
54.66
54.23
satellite
47.23
67.25
59.64
57.61
37.68
50.01
61.94
65.92
58.33
52.22
56.58
37.56
61.43
43.31
54.68
41.77
68.92
71.68
75.13
satimage-2
26.11
78.04
85.69
82.71
4.30
39.14
80.52
93.45
22.07
64.49
76.55
12.08
97.09
8.12
97.13
2.58
72.90
97.33
97.31
shuttle
20.27
96.40
92.35
85.29
13.76
20.38
48.75
97.62
93.20
90.45
96.56
7.68
79.89
81.82
32.66
12.41
67.23
92.05
92.36
skin
24.61
23.70
17.40
19.03
18.25
28.72
18.44
26.08
N/A
18.37
17.99
20.89
28.34
26.29
25.58
19.06
25.36
28.87
28.72
Stamps
20.28
35.24
41.09
31.39
21.29
23.53
34.60
39.49
43.73
33.21
43.10
11.03
24.46
47.36
35.63
12.07
34.68
42.39
50.94
thyroid
23.56
50.98
44.34
21.23
20.81
34.98
14.68
63.11
16.06
51.06
19.64
9.44
29.88
10.56
31.69
2.70
50.36
60.99
60.06
vertebral
11.79
9.23
10.49
10.94
14.24
10.57
9.68
10.46
15.24
11.84
8.89
13.11
11.43
11.58
10.54
10.62
14.31
9.78
12.96
vowels
38.88
13.41
8.92
8.24
34.42
63.41
13.82
15.12
12.22
10.56
4.14
13.27
35.14
3.58
49.10
4.58
14.97
26.52
32.42
Waveform
9.66
5.86
5.79
4.37
11.33
13.04
4.71
6.24
3.11
4.76
6.90
5.33
17.93
4.26
19.74
4.41
11.28
6.49
7.83
WBC
54.00
73.56
82.29
89.87
5.57
66.55
78.67
90.49
N/A
86.19
86.19
30.25
67.31
33.43
71.88
8.99
13.32
68.69
83.14
Wilt
5.53
3.84
3.13
3.62
5.05
4.73
3.36
4.23
4.00
3.93
3.69
5.33
3.74
4.62
3.76
4.65
4.05
4.80
5.19
wine
7.95
43.08
30.87
21.56
7.77
8.43
48.82
25.96
17.51
23.54
45.71
8.11
5.98
24.44
6.27
18.78
8.38
21.40
49.59
WPBC
25.62
23.04
23.01
22.93
20.29
21.49
25.39
22.42
22.49
21.24
22.81
23.86
21.08
22.86
20.58
25.00
20.73
22.71
24.90"
LOG UIK,0.6227544910179641,"Avg. Rank
10.83
8.19
8.31
11.14
13.24
9.36
11.79
7.29
11.96
9.36
9.53
14.91
8.53
11.97
9.03
13.41
9.10
6.31
4.74"
LOG UIK,0.6236099230111206,"Constructing the outlier set requires a sorting operation, for which we use Numpy’s built-in quantile
624"
LOG UIK,0.6244653550042771,"calculation with a time complexity of O(N log N). Considering the number of network parameters
625"
LOG UIK,0.6253207869974337,"along with the computation of the loss function, the computational complexity for optimizing Θ is
626"
LOG UIK,0.6261762189905903,"approximately O(TNDd + TNKd). The EM algorithm for the Student’s t mixture model includes
627"
LOG UIK,0.6270316509837468,"two main steps: the E-step, where the complexity for computing the probability (or responsibility)
628"
LOG UIK,0.6278870829769033,"of each data point belonging to each component is approximately O(NKd), and the M-step, where
629"
LOG UIK,0.6287425149700598,"the full computational complexity of updating the parameters (mean, covariance matrix) of each
630"
LOG UIK,0.6295979469632165,"component is O(NKd2). In practice, we use diagonal covariance matrices, which reduces the
631"
LOG UIK,0.630453378956373,"update complexity to roughly O(NKd). If the EM algorithm requires T round to converge, its
632"
LOG UIK,0.6313088109495295,"time complexity is approximately O(TNKd). Therefore, the time complexity for t-iterations is
633"
LOG UIK,0.632164242942686,"O(tN(log N + Td(D + K))).
634"
LOG UIK,0.6330196749358425,"E
Additional Experiments on Graph
635"
LOG UIK,0.6338751069289992,"E.1
Baselines
636"
LOG UIK,0.6347305389221557,"Our proposed method was compared with 16 graph domain baseline methods grouped into three
637"
LOG UIK,0.6355859709153122,"categories as follows:
638"
LOG UIK,0.6364414029084687,"• Contrastive Learning-based Methods:
This group includes CoLA [30], SLGAD [55],
639"
LOG UIK,0.6372968349016254,"CONAD [53], and ANEMONE [20]. These methods primarily assume that the contrastive loss
640"
LOG UIK,0.6381522668947819,"between anomalous nodes and their neighborhoods is more significant.
641"
LOG UIK,0.6390076988879384,"• Autoencoder-based Methods: This category consists of MLPAE [43], GCNAE [22], DOMI-
642"
LOG UIK,0.6398631308810949,"NANT [11], GUIDE [54], ComGA [31], AnomalyDAE [14], ALARM [37], DONE/AdONE [4]
643"
LOG UIK,0.6407185628742516,"and AAGNN [57]. These methods focus on the reconstruction errors of anomalous nodes during
644"
LOG UIK,0.6415739948674081,"the process of reconstructing the graph structure or features.
645"
LOG UIK,0.6424294268605646,"• Clustering-based Methods: This category of methods encompasses SCAN [52], CBLOF [18],
646"
LOG UIK,0.6432848588537211,"and DCFOD [45]. These methods generally identify anomalies by detecting if a sample deviates
647"
LOG UIK,0.6441402908468776,"from the clustering.
648"
LOG UIK,0.6449957228400343,"E.2
Datasets
649"
LOG UIK,0.6458511548331908,"We assess the performance of our model using four graph benchmark datasets containing organic
650"
LOG UIK,0.6467065868263473,"anomalies. Table 7 presents the statistical summary for each dataset. These datasets contain naturally
651"
LOG UIK,0.6475620188195038,"occurring real-world anomalies and are valuable for assessing the performance of anomaly detection
652"
LOG UIK,0.6484174508126604,"algorithms in real-world scenarios. The sources and compositions of these datasets are as follows:
653"
LOG UIK,0.649272882805817,Table 7: Statistics of graph benchmark datasets.
LOG UIK,0.6501283147989735,"Dataset
# Nodes
# Edges
# Features
# Anomaly
Category"
LOG UIK,0.65098374679213,"Disney
124
670
28
6
co-purchase network
Weibo
8,405
407,963
400
868
social media network
Reddit
10,984
168,016
64
366
user-subreddit network
T-Finance
39,357
42,445,086
10
1,803
trading network"
LOG UIK,0.6518391787852865,"• Weibo[19] is a labeled graph comprising user posts extracted from the social media platform
654"
LOG UIK,0.6526946107784432,"Tencent Weibo. The user-user graph establishes connections between users who exhibit similar
655"
LOG UIK,0.6535500427715997,"topic labels. A user is considered anomalous if they have engaged in a minimum of five suspicious
656"
LOG UIK,0.6544054747647562,"events, whereas normal nodes represent users who have not.
657"
LOG UIK,0.6552609067579127,"• Reddit[25] consists of a user-subreddit graph extracted from the popular social media platform
658"
LOG UIK,0.6561163387510693,"Reddit. This publicly accessible dataset encompasses user posts within various subreddits over
659"
LOG UIK,0.6569717707442259,"a month. Each user is assigned a binary label indicating whether they have been banned on the
660"
LOG UIK,0.6578272027373824,"platform. Our assumption is that banned users exhibit anomalous behavior compared to regular
661"
LOG UIK,0.6586826347305389,"Reddit users.
662"
LOG UIK,0.6595380667236954,"• Disney[34] is a co-purchase network of movies that includes attributes such as price, rating, and the
663"
LOG UIK,0.660393498716852,"number of reviews. The ground truth labels, indicating whether a movie is considered anomalous
664"
LOG UIK,0.6612489307100086,"or not, were assigned by high school students through majority voting.
665"
LOG UIK,0.6621043627031651,"• T-Finance[46] aims to identify anomalous accounts within a trading network. The nodes in
666"
LOG UIK,0.6629597946963216,"this network represent unique anonymous accounts, each characterized by ten features related to
667"
LOG UIK,0.6638152266894782,"registration duration, recorded activity, and interaction frequency. Graph edges denote transaction
668"
LOG UIK,0.6646706586826348,"records between accounts. If a node is associated with activities such as fraud, money laundering,
669"
LOG UIK,0.6655260906757913,"or online gambling, human experts will designate it as an anomaly.
670"
LOG UIK,0.6663815226689478,"E.3
Experiment Settings
671"
LOG UIK,0.6672369546621043,Table 8: AUC-ROC and AUC-PR of 16 unsupervised algorithms on 4 graph benchmark datasets.
LOG UIK,0.6680923866552609,"Group
Method
Weibo
Reddit
Disney
T-Finance
AUC-ROC
AUC-PR
AUC-ROC
AUC-PR
AUC-ROC
AUC-PR
AUC-ROC
AUC-PR"
LOG UIK,0.6689478186484175,CL-Based
LOG UIK,0.669803250641574,"CoLA
0.382
0.087
0.527
0.036
0.455
0.060
0.243
0.031
SL-GAD
0.421
0.109
0.594
0.040
0.494
0.061
0.442
0.041
ANEMONE
0.320
0.082
0.536
0.036
0.454
0.068
0.226
0.030
CONAD
0.806
0.432
0.551
0.037
0.600
0.138
N/A
N/A"
LOG UIK,0.6706586826347305,AE-Based
LOG UIK,0.6715141146278871,"MLPAE
0.880
0.629
0.501
0.035
0.563
0.064
0.299
0.030
GCNAE
0.847
0.567
0.526
0.033
0.517
0.059
0.295
0.030
GUIDE
0.897
0.692
0.566
0.040
0.521
0.060
N/A
N/A
DOMINANT
0.927
0.797
0.561
0.037
0.590
0.077
N/A
N/A
ComGA
0.925
0.809
0.568
0.037
0.494
0.058
N/A
N/A
AnomalyDAE
0.892
0.694
0.560
0.037
0.520
0.070
N/A
N/A
ALARM
0.952
0.843
0.559
0.037
0.595
0.123
N/A
N/A
DONE
0.856
0.579
0.551
0.037
0.517
0.061
0.550
0.046
AAGNN
0.804
0.530
0.564
0.045
0.479
0.059
N/A
N/A"
LOG UIK,0.6723695466210436,Cluster-Based
LOG UIK,0.6732249786142002,"SCAN
0.701
0.186
0.496
0.033
0.548
0.053
N/A
N/A
CBLOF*
0.972
0.875
0.503
0.035
0.574
0.146
0.524
0.046
DCFOD*
0.684
0.196
0.552
0.038
0.675
0.119
0.521
0.066
UniCAD *
0.985
0.927
0.560
0.040
0.701
0.130
0.876
0.422"
LOG UIK,0.6740804106073567,"In this experiment, we compared graph-based methods on relational data. For methods originally
672"
LOG UIK,0.6749358426005133,"designed around feature vectors, including CBLOF, DCFOD, and our approach, we uniformly
673"
LOG UIK,0.6757912745936698,"employed the same graph representation learning technique as described in BGRL [47]. Specifically,
674"
LOG UIK,0.6766467065868264,"we used a two-layer Graph Convolutional Network (GCN) for encoding, which produced output
675"
LOG UIK,0.6775021385799829,"embeddings with a dimensionality of 128. The training epochs were set to 3000, including a warm-up
676"
LOG UIK,0.6783575705731394,"period of 300 epochs. The hidden size of the predictor was set to 512, and the momentum was fixed
677"
LOG UIK,0.679213002566296,"at 0.99.
678"
LOG UIK,0.6800684345594525,"E.4
Performance Analysis
679"
LOG UIK,0.6809238665526091,"The performance of UniCAD compared to 16 baseline methods on the four datasets are summarized
680"
LOG UIK,0.6817792985457656,"in Table 8. From the results, we have the following observations: Our model consistently outperforms
681"
LOG UIK,0.6826347305389222,"the baseline methods on most datasets, underlining its effectiveness in anomaly detection even within
682"
LOG UIK,0.6834901625320787,"graph data contexts. This highlights the superiority of UniCAD in detecting anomalies in real-world
683"
LOG UIK,0.6843455945252352,"graph data.
684"
LOG UIK,0.6852010265183918,"When comparing UniCAD with the four contrastive learning-based methods, it exhibits a distinct
685"
LOG UIK,0.6860564585115483,"advantage, outperforming them by a substantial margin across all metrics. Unlike contrastive learning
686"
LOG UIK,0.6869118905047049,"methods that rely on the local neighborhood for anomaly detection, UniCAD leverages the global
687"
LOG UIK,0.6877673224978614,"clustering distribution. This key difference contributes to its consistently superior performance.
688"
LOG UIK,0.688622754491018,"Although CONAD incorporates human prior knowledge about anomalies, enabling it to outperform
689"
LOG UIK,0.6894781864841745,"other similar methods on the Weibo and Disney datasets, it still falls short compared to our proposed
690"
LOG UIK,0.6903336184773311,"UniCAD.
691"
LOG UIK,0.6911890504704876,"Compared to the autoencoder-based methods, UniCAD offers the advantage of lower memory
692"
LOG UIK,0.6920444824636441,"requirements along with better performance. Graph autoencoders typically reconstruct the entire
693"
LOG UIK,0.6928999144568007,"adjacency matrix during full graph training, resulting in memory usage of at least O(N 2). In contrast,
694"
LOG UIK,0.6937553464499572,"UniCAD, as a clustering-based method, only requires O(N × K). Among the autoencoder-based
695"
LOG UIK,0.6946107784431138,"methods, GCNAE, DONE, and AdONE can be extended to the T-Finance dataset as they only
696"
LOG UIK,0.6954662104362703,"reconstruct the sampled subgraphs rather than the entire adjacency matrix. However, UniCAD still
697"
LOG UIK,0.6963216424294268,"showcases superior performance while being more memory-efficient.
698"
LOG UIK,0.6971770744225834,"UniCAD also demonstrates superior performance compared to various other clustering-based methods,
699"
LOG UIK,0.69803250641574,"including traditional structural clustering (SCAN) methods that treat the embedding from BGRL as
700"
LOG UIK,0.6988879384088965,"tabular data (CBLOF, DCFOD).
701"
LOG UIK,0.699743370402053,"NeurIPS Paper Checklist
702"
LOG UIK,0.7005988023952096,"The checklist is designed to encourage best practices for responsible machine learning research,
703"
LOG UIK,0.7014542343883661,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
704"
LOG UIK,0.7023096663815227,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
705"
LOG UIK,0.7031650983746792,"follow the references and precede the (optional) supplemental material. The checklist does NOT
706"
LOG UIK,0.7040205303678357,"count towards the page limit.
707"
LOG UIK,0.7048759623609923,"Please read the checklist guidelines carefully for information on how to answer these questions. For
708"
LOG UIK,0.7057313943541489,"each question in the checklist:
709"
LOG UIK,0.7065868263473054,"• You should answer [Yes] , [No] , or [NA] .
710"
LOG UIK,0.7074422583404619,"• [NA] means either that the question is Not Applicable for that particular paper or the relevant
711"
LOG UIK,0.7082976903336184,"information is Not Available.
712"
LOG UIK,0.7091531223267751,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
713"
LOG UIK,0.7100085543199316,"The checklist answers are an integral part of your paper submission. They are visible to the
714"
LOG UIK,0.7108639863130881,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
715"
LOG UIK,0.7117194183062446,"(after eventual revisions) with the final version of your paper, and its final version will be published
716"
LOG UIK,0.7125748502994012,"with the paper.
717"
LOG UIK,0.7134302822925578,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
718"
LOG UIK,0.7142857142857143,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
719"
LOG UIK,0.7151411462788708,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
720"
LOG UIK,0.7159965782720273,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
721"
LOG UIK,0.716852010265184,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
722"
LOG UIK,0.7177074422583405,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
723"
LOG UIK,0.718562874251497,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
724"
LOG UIK,0.7194183062446535,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
725"
LOG UIK,0.72027373823781,"please point to the section(s) where related material for the question can be found.
726"
LOG UIK,0.7211291702309667,"IMPORTANT, please:
727"
LOG UIK,0.7219846022241232,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
728"
LOG UIK,0.7228400342172797,"• Keep the checklist subsection headings, questions/answers and guidelines below.
729"
LOG UIK,0.7236954662104362,"• Do not modify the questions and only use the provided macros for your answers.
730"
CLAIMS,0.7245508982035929,"1. Claims
731"
CLAIMS,0.7254063301967494,"Question: Do the main claims made in the abstract and introduction accurately reflect the
732"
CLAIMS,0.7262617621899059,"paper’s contributions and scope?
733"
CLAIMS,0.7271171941830624,"Answer: [Yes]
734"
CLAIMS,0.7279726261762189,"Justification: The main claims presented in the abstract and introduction are consistent with
735"
CLAIMS,0.7288280581693756,"the paper’s contributions and accurately outline the scope.
736"
CLAIMS,0.7296834901625321,"Guidelines:
737"
CLAIMS,0.7305389221556886,"• The answer NA means that the abstract and introduction do not include the claims made
738"
CLAIMS,0.7313943541488451,"in the paper.
739"
CLAIMS,0.7322497861420018,"• The abstract and/or introduction should clearly state the claims made, including the
740"
CLAIMS,0.7331052181351583,"contributions made in the paper and important assumptions and limitations. A No or NA
741"
CLAIMS,0.7339606501283148,"answer to this question will not be perceived well by the reviewers.
742"
CLAIMS,0.7348160821214713,"• The claims made should match theoretical and experimental results, and reflect how much
743"
CLAIMS,0.7356715141146278,"the results can be expected to generalize to other settings.
744"
CLAIMS,0.7365269461077845,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
745"
CLAIMS,0.737382378100941,"are not attained by the paper.
746"
LIMITATIONS,0.7382378100940975,"2. Limitations
747"
LIMITATIONS,0.739093242087254,"Question: Does the paper discuss the limitations of the work performed by the authors?
748"
LIMITATIONS,0.7399486740804107,"Answer: [Yes]
749"
LIMITATIONS,0.7408041060735672,"Justification: The limitations of the method’s application scope are discussed in Section 5 of
750"
LIMITATIONS,0.7416595380667237,"the paper, along with considerations for future work.
751"
LIMITATIONS,0.7425149700598802,"Guidelines:
752"
LIMITATIONS,0.7433704020530368,"• The answer NA means that the paper has no limitation while the answer No means that
753"
LIMITATIONS,0.7442258340461934,"the paper has limitations, but those are not discussed in the paper.
754"
LIMITATIONS,0.7450812660393499,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
755"
LIMITATIONS,0.7459366980325064,"• The paper should point out any strong assumptions and how robust the results are to
756"
LIMITATIONS,0.7467921300256629,"violations of these assumptions (e.g., independence assumptions, noiseless settings, model
757"
LIMITATIONS,0.7476475620188195,"well-specification, asymptotic approximations only holding locally). The authors should
758"
LIMITATIONS,0.7485029940119761,"reflect on how these assumptions might be violated in practice and what the implications
759"
LIMITATIONS,0.7493584260051326,"would be.
760"
LIMITATIONS,0.7502138579982891,"• The authors should reflect on the scope of the claims made, e.g., if the approach was only
761"
LIMITATIONS,0.7510692899914457,"tested on a few datasets or with a few runs. In general, empirical results often depend on
762"
LIMITATIONS,0.7519247219846023,"implicit assumptions, which should be articulated.
763"
LIMITATIONS,0.7527801539777588,"• The authors should reflect on the factors that influence the performance of the approach.
764"
LIMITATIONS,0.7536355859709153,"For example, a facial recognition algorithm may perform poorly when image resolution is
765"
LIMITATIONS,0.7544910179640718,"low or images are taken in low lighting. Or a speech-to-text system might not be used
766"
LIMITATIONS,0.7553464499572284,"reliably to provide closed captions for online lectures because it fails to handle technical
767"
LIMITATIONS,0.756201881950385,"jargon.
768"
LIMITATIONS,0.7570573139435415,"• The authors should discuss the computational efficiency of the proposed algorithms and
769"
LIMITATIONS,0.757912745936698,"how they scale with dataset size.
770"
LIMITATIONS,0.7587681779298546,"• If applicable, the authors should discuss possible limitations of their approach to address
771"
LIMITATIONS,0.7596236099230111,"problems of privacy and fairness.
772"
LIMITATIONS,0.7604790419161677,"• While the authors might fear that complete honesty about limitations might be used by
773"
LIMITATIONS,0.7613344739093242,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
774"
LIMITATIONS,0.7621899059024807,"limitations that aren’t acknowledged in the paper. The authors should use their best
775"
LIMITATIONS,0.7630453378956373,"judgment and recognize that individual actions in favor of transparency play an important
776"
LIMITATIONS,0.7639007698887939,"role in developing norms that preserve the integrity of the community. Reviewers will be
777"
LIMITATIONS,0.7647562018819504,"specifically instructed to not penalize honesty concerning limitations.
778"
THEORY ASSUMPTIONS AND PROOFS,0.7656116338751069,"3. Theory Assumptions and Proofs
779"
THEORY ASSUMPTIONS AND PROOFS,0.7664670658682635,"Question: For each theoretical result, does the paper provide the full set of assumptions and
780"
THEORY ASSUMPTIONS AND PROOFS,0.76732249786142,"a complete (and correct) proof?
781"
THEORY ASSUMPTIONS AND PROOFS,0.7681779298545766,"Answer: [Yes]
782"
THEORY ASSUMPTIONS AND PROOFS,0.7690333618477331,"Justification: The full set of assumptions and complete proofs for each theoretical result
783"
THEORY ASSUMPTIONS AND PROOFS,0.7698887938408896,"are provided and can be found in the appendix, specifically in Section B, ensuring that the
784"
THEORY ASSUMPTIONS AND PROOFS,0.7707442258340462,"theoretical framework is transparent and verifiable.
785"
THEORY ASSUMPTIONS AND PROOFS,0.7715996578272027,"Guidelines:
786"
THEORY ASSUMPTIONS AND PROOFS,0.7724550898203593,"• The answer NA means that the paper does not include theoretical results.
787"
THEORY ASSUMPTIONS AND PROOFS,0.7733105218135158,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
788"
THEORY ASSUMPTIONS AND PROOFS,0.7741659538066724,"referenced.
789"
THEORY ASSUMPTIONS AND PROOFS,0.7750213857998289,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
790"
THEORY ASSUMPTIONS AND PROOFS,0.7758768177929855,"• The proofs can either appear in the main paper or the supplemental material, but if they
791"
THEORY ASSUMPTIONS AND PROOFS,0.776732249786142,"appear in the supplemental material, the authors are encouraged to provide a short proof
792"
THEORY ASSUMPTIONS AND PROOFS,0.7775876817792986,"sketch to provide intuition.
793"
THEORY ASSUMPTIONS AND PROOFS,0.7784431137724551,"• Inversely, any informal proof provided in the core of the paper should be complemented
794"
THEORY ASSUMPTIONS AND PROOFS,0.7792985457656116,"by formal proofs provided in appendix or supplemental material.
795"
THEORY ASSUMPTIONS AND PROOFS,0.7801539777587682,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
796"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7810094097519247,"4. Experimental Result Reproducibility
797"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7818648417450813,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
798"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7827202737382378,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
799"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7835757057313943,"of the paper (regardless of whether the code and data are provided or not)?
800"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7844311377245509,"Answer: [Yes]
801"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852865697177075,"Justification: All necessary information for reproducing the main experimental results,
802"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.786142001710864,"including dataset links, baseline comparisons, and the methodologies of our proposed
803"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7869974337040205,"approach, are comprehensively included within the submission files, facilitating transparency
804"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.787852865697177,"and reproducibility of the research findings.
805"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7887082976903336,"Guidelines:
806"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7895637296834902,"• The answer NA means that the paper does not include experiments.
807"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7904191616766467,"• If the paper includes experiments, a No answer to this question will not be perceived well
808"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7912745936698032,"by the reviewers: Making the paper reproducible is important, regardless of whether the
809"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7921300256629598,"code and data are provided or not.
810"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7929854576561164,"• If the contribution is a dataset and/or model, the authors should describe the steps taken to
811"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7938408896492729,"make their results reproducible or verifiable.
812"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7946963216424294,"• Depending on the contribution, reproducibility can be accomplished in various ways.
813"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.795551753635586,"For example, if the contribution is a novel architecture, describing the architecture fully
814"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7964071856287425,"might suffice, or if the contribution is a specific model and empirical evaluation, it may be
815"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7972626176218991,"necessary to either make it possible for others to replicate the model with the same dataset,
816"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7981180496150556,"or provide access to the model. In general. releasing code and data is often one good
817"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7989734816082121,"way to accomplish this, but reproducibility can also be provided via detailed instructions
818"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7998289136013687,"for how to replicate the results, access to a hosted model (e.g., in the case of a large
819"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8006843455945253,"language model), releasing of a model checkpoint, or other means that are appropriate to
820"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8015397775876818,"the research performed.
821"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8023952095808383,"• While NeurIPS does not require releasing code, the conference does require all submis-
822"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8032506415739948,"sions to provide some reasonable avenue for reproducibility, which may depend on the
823"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8041060735671514,"nature of the contribution. For example
824"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.804961505560308,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how to
825"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8058169375534645,"reproduce that algorithm.
826"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.806672369546621,"(b) If the contribution is primarily a new model architecture, the paper should describe
827"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8075278015397775,"the architecture clearly and fully.
828"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8083832335329342,"(c) If the contribution is a new model (e.g., a large language model), then there should
829"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8092386655260907,"either be a way to access this model for reproducing the results or a way to reproduce
830"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8100940975192472,"the model (e.g., with an open-source dataset or instructions for how to construct the
831"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8109495295124037,"dataset).
832"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8118049615055604,"(d) We recognize that reproducibility may be tricky in some cases, in which case authors
833"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8126603934987169,"are welcome to describe the particular way they provide for reproducibility. In the
834"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8135158254918734,"case of closed-source models, it may be that access to the model is limited in some
835"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8143712574850299,"way (e.g., to registered users), but it should be possible for other researchers to have
836"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8152266894781864,"some path to reproducing or verifying the results.
837"
OPEN ACCESS TO DATA AND CODE,0.8160821214713431,"5. Open access to data and code
838"
OPEN ACCESS TO DATA AND CODE,0.8169375534644996,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
839"
OPEN ACCESS TO DATA AND CODE,0.8177929854576561,"tions to faithfully reproduce the main experimental results, as described in supplemental
840"
OPEN ACCESS TO DATA AND CODE,0.8186484174508126,"material?
841"
OPEN ACCESS TO DATA AND CODE,0.8195038494439693,"Answer: [Yes]
842"
OPEN ACCESS TO DATA AND CODE,0.8203592814371258,"Justification: The paper ensures open access to both the data and code necessary for
843"
OPEN ACCESS TO DATA AND CODE,0.8212147134302823,"reproducing the main experimental results, complemented by detailed instructions in the
844"
OPEN ACCESS TO DATA AND CODE,0.8220701454234388,"supplemental material.
845"
OPEN ACCESS TO DATA AND CODE,0.8229255774165953,"Guidelines:
846"
OPEN ACCESS TO DATA AND CODE,0.823781009409752,"• The answer NA means that paper does not include experiments requiring code.
847"
OPEN ACCESS TO DATA AND CODE,0.8246364414029085,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
848"
OPEN ACCESS TO DATA AND CODE,0.825491873396065,"public/guides/CodeSubmissionPolicy) for more details.
849"
OPEN ACCESS TO DATA AND CODE,0.8263473053892215,"• While we encourage the release of code and data, we understand that this might not be
850"
OPEN ACCESS TO DATA AND CODE,0.8272027373823782,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
851"
OPEN ACCESS TO DATA AND CODE,0.8280581693755347,"including code, unless this is central to the contribution (e.g., for a new open-source
852"
OPEN ACCESS TO DATA AND CODE,0.8289136013686912,"benchmark).
853"
OPEN ACCESS TO DATA AND CODE,0.8297690333618477,"• The instructions should contain the exact command and environment needed to run to
854"
OPEN ACCESS TO DATA AND CODE,0.8306244653550042,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
855"
OPEN ACCESS TO DATA AND CODE,0.8314798973481609,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
856"
OPEN ACCESS TO DATA AND CODE,0.8323353293413174,"• The authors should provide instructions on data access and preparation, including how to
857"
OPEN ACCESS TO DATA AND CODE,0.8331907613344739,"access the raw data, preprocessed data, intermediate data, and generated data, etc.
858"
OPEN ACCESS TO DATA AND CODE,0.8340461933276304,"• The authors should provide scripts to reproduce all experimental results for the new
859"
OPEN ACCESS TO DATA AND CODE,0.834901625320787,"proposed method and baselines. If only a subset of experiments are reproducible, they
860"
OPEN ACCESS TO DATA AND CODE,0.8357570573139436,"should state which ones are omitted from the script and why.
861"
OPEN ACCESS TO DATA AND CODE,0.8366124893071001,"• At submission time, to preserve anonymity, the authors should release anonymized ver-
862"
OPEN ACCESS TO DATA AND CODE,0.8374679213002566,"sions (if applicable).
863"
OPEN ACCESS TO DATA AND CODE,0.8383233532934131,"• Providing as much information as possible in supplemental material (appended to the
864"
OPEN ACCESS TO DATA AND CODE,0.8391787852865698,"paper) is recommended, but including URLs to data and code is permitted.
865"
OPEN ACCESS TO DATA AND CODE,0.8400342172797263,"6. Experimental Setting/Details
866"
OPEN ACCESS TO DATA AND CODE,0.8408896492728828,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
867"
OPEN ACCESS TO DATA AND CODE,0.8417450812660393,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
868"
OPEN ACCESS TO DATA AND CODE,0.8426005132591959,"results?
869"
OPEN ACCESS TO DATA AND CODE,0.8434559452523525,"Answer: [Yes]
870"
OPEN ACCESS TO DATA AND CODE,0.844311377245509,"Justification: Detailed information regarding the training and test setups, including data
871"
OPEN ACCESS TO DATA AND CODE,0.8451668092386655,"splits, hyperparameters and their selection process, the type of optimizer used, and other
872"
OPEN ACCESS TO DATA AND CODE,0.8460222412318221,"relevant details, are thoroughly documented in Section 4.2 of the paper.
873"
OPEN ACCESS TO DATA AND CODE,0.8468776732249786,"Guidelines:
874"
OPEN ACCESS TO DATA AND CODE,0.8477331052181352,"• The answer NA means that the paper does not include experiments.
875"
OPEN ACCESS TO DATA AND CODE,0.8485885372112917,"• The experimental setting should be presented in the core of the paper to a level of detail
876"
OPEN ACCESS TO DATA AND CODE,0.8494439692044482,"that is necessary to appreciate the results and make sense of them.
877"
OPEN ACCESS TO DATA AND CODE,0.8502994011976048,"• The full details can be provided either with the code, in appendix, or as supplemental
878"
OPEN ACCESS TO DATA AND CODE,0.8511548331907614,"material.
879"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8520102651839179,"7. Experiment Statistical Significance
880"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8528656971770744,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
881"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.853721129170231,"information about the statistical significance of the experiments?
882"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8545765611633875,"Answer: [Yes]
883"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8554319931565441,"Justification: The paper reports the statistical significance of the experiments by detailing
884"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8562874251497006,"the results of the Friedman test and the Nemenyi test in Appendix D.3.
885"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"Guidelines:
886"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8579982891360137,"• The answer NA means that the paper does not include experiments.
887"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8588537211291702,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confidence
888"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8597091531223268,"intervals, or statistical significance tests, at least for the experiments that support the main
889"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8605645851154833,"claims of the paper.
890"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8614200171086399,"• The factors of variability that the error bars are capturing should be clearly stated (for
891"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8622754491017964,"example, train/test split, initialization, random drawing of some parameter, or overall run
892"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.863130881094953,"with given experimental conditions).
893"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8639863130881095,"• The method for calculating the error bars should be explained (closed form formula, call
894"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.864841745081266,"to a library function, bootstrap, etc.)
895"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8656971770744226,"• The assumptions made should be given (e.g., Normally distributed errors).
896"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8665526090675791,"• It should be clear whether the error bar is the standard deviation or the standard error of
897"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8674080410607357,"the mean.
898"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8682634730538922,"• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably
899"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8691189050470488,"report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality
900"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8699743370402053,"of errors is not verified.
901"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8708297690333618,"• For asymmetric distributions, the authors should be careful not to show in tables or figures
902"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8716852010265184,"symmetric error bars that would yield results that are out of range (e.g. negative error
903"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8725406330196749,"rates).
904"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8733960650128315,"• If error bars are reported in tables or plots, The authors should explain in the text how they
905"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.874251497005988,"were calculated and reference the corresponding figures or tables in the text.
906"
EXPERIMENTS COMPUTE RESOURCES,0.8751069289991446,"8. Experiments Compute Resources
907"
EXPERIMENTS COMPUTE RESOURCES,0.8759623609923011,"Question: For each experiment, does the paper provide sufficient information on the com-
908"
EXPERIMENTS COMPUTE RESOURCES,0.8768177929854577,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
909"
EXPERIMENTS COMPUTE RESOURCES,0.8776732249786142,"the experiments?
910"
EXPERIMENTS COMPUTE RESOURCES,0.8785286569717707,"Answer: [Yes]
911"
EXPERIMENTS COMPUTE RESOURCES,0.8793840889649273,"Justification: Detailed information on the compute resources, including the type of compute
912"
EXPERIMENTS COMPUTE RESOURCES,0.8802395209580839,"workers (CPU/GPU), memory, and execution time for each experiment, is provided in the
913"
EXPERIMENTS COMPUTE RESOURCES,0.8810949529512404,"supplementary materials, enabling accurate reproduction of the experiments.
914"
EXPERIMENTS COMPUTE RESOURCES,0.8819503849443969,"Guidelines:
915"
EXPERIMENTS COMPUTE RESOURCES,0.8828058169375534,"• The answer NA means that the paper does not include experiments.
916"
EXPERIMENTS COMPUTE RESOURCES,0.88366124893071,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or
917"
EXPERIMENTS COMPUTE RESOURCES,0.8845166809238666,"cloud provider, including relevant memory and storage.
918"
EXPERIMENTS COMPUTE RESOURCES,0.8853721129170231,"• The paper should provide the amount of compute required for each of the individual
919"
EXPERIMENTS COMPUTE RESOURCES,0.8862275449101796,"experimental runs as well as estimate the total compute.
920"
EXPERIMENTS COMPUTE RESOURCES,0.8870829769033362,"• The paper should disclose whether the full research project required more compute than
921"
EXPERIMENTS COMPUTE RESOURCES,0.8879384088964928,"the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t
922"
EXPERIMENTS COMPUTE RESOURCES,0.8887938408896493,"make it into the paper).
923"
CODE OF ETHICS,0.8896492728828058,"9. Code Of Ethics
924"
CODE OF ETHICS,0.8905047048759623,"Question: Does the research conducted in the paper conform, in every respect, with the
925"
CODE OF ETHICS,0.8913601368691189,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
926"
CODE OF ETHICS,0.8922155688622755,"Answer: [Yes]
927"
CODE OF ETHICS,0.893071000855432,"Justification: The research adheres to the NeurIPS Code of Ethics, including considerations
928"
CODE OF ETHICS,0.8939264328485885,"for anonymity, fairness, and transparency, with no deviations reported or necessary under
929"
CODE OF ETHICS,0.894781864841745,"current laws or regulations.
930"
CODE OF ETHICS,0.8956372968349017,"Guidelines:
931"
CODE OF ETHICS,0.8964927288280582,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
932"
CODE OF ETHICS,0.8973481608212147,"• If the authors answer No, they should explain the special circumstances that require a
933"
CODE OF ETHICS,0.8982035928143712,"deviation from the Code of Ethics.
934"
CODE OF ETHICS,0.8990590248075278,"• The authors should make sure to preserve anonymity (e.g., if there is a special consideration
935"
CODE OF ETHICS,0.8999144568006844,"due to laws or regulations in their jurisdiction).
936"
BROADER IMPACTS,0.9007698887938409,"10. Broader Impacts
937"
BROADER IMPACTS,0.9016253207869974,"Question: Does the paper discuss both potential positive societal impacts and negative
938"
BROADER IMPACTS,0.9024807527801539,"societal impacts of the work performed?
939"
BROADER IMPACTS,0.9033361847733106,"Answer: [Yes]
940"
BROADER IMPACTS,0.9041916167664671,"Justification: The paper thoroughly discusses both the potential positive impacts, such as
941"
BROADER IMPACTS,0.9050470487596236,"enhancements in anomaly detection for critical applications.
942"
BROADER IMPACTS,0.9059024807527801,"Guidelines:
943"
BROADER IMPACTS,0.9067579127459366,"• The answer NA means that there is no societal impact of the work performed.
944"
BROADER IMPACTS,0.9076133447390933,"• If the authors answer NA or No, they should explain why their work has no societal impact
945"
BROADER IMPACTS,0.9084687767322498,"or why the paper does not address societal impact.
946"
BROADER IMPACTS,0.9093242087254063,"• Examples of negative societal impacts include potential malicious or unintended uses
947"
BROADER IMPACTS,0.9101796407185628,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g.,
948"
BROADER IMPACTS,0.9110350727117195,"deployment of technologies that could make decisions that unfairly impact specific groups),
949"
BROADER IMPACTS,0.911890504704876,"privacy considerations, and security considerations.
950"
BROADER IMPACTS,0.9127459366980325,"• The conference expects that many papers will be foundational research and not tied to
951"
BROADER IMPACTS,0.913601368691189,"particular applications, let alone deployments. However, if there is a direct path to any
952"
BROADER IMPACTS,0.9144568006843456,"negative applications, the authors should point it out. For example, it is legitimate to point
953"
BROADER IMPACTS,0.9153122326775022,"out that an improvement in the quality of generative models could be used to generate
954"
BROADER IMPACTS,0.9161676646706587,"deepfakes for disinformation. On the other hand, it is not needed to point out that a
955"
BROADER IMPACTS,0.9170230966638152,"generic algorithm for optimizing neural networks could enable people to train models that
956"
BROADER IMPACTS,0.9178785286569717,"generate Deepfakes faster.
957"
BROADER IMPACTS,0.9187339606501284,"• The authors should consider possible harms that could arise when the technology is being
958"
BROADER IMPACTS,0.9195893926432849,"used as intended and functioning correctly, harms that could arise when the technology is
959"
BROADER IMPACTS,0.9204448246364414,"being used as intended but gives incorrect results, and harms following from (intentional
960"
BROADER IMPACTS,0.9213002566295979,"or unintentional) misuse of the technology.
961"
BROADER IMPACTS,0.9221556886227545,"• If there are negative societal impacts, the authors could also discuss possible mitigation
962"
BROADER IMPACTS,0.9230111206159111,"strategies (e.g., gated release of models, providing defenses in addition to attacks, mecha-
963"
BROADER IMPACTS,0.9238665526090676,"nisms for monitoring misuse, mechanisms to monitor how a system learns from feedback
964"
BROADER IMPACTS,0.9247219846022241,"over time, improving the efficiency and accessibility of ML).
965"
SAFEGUARDS,0.9255774165953806,"11. Safeguards
966"
SAFEGUARDS,0.9264328485885372,"Question: Does the paper describe safeguards that have been put in place for responsible
967"
SAFEGUARDS,0.9272882805816938,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
968"
SAFEGUARDS,0.9281437125748503,"image generators, or scraped datasets)?
969"
SAFEGUARDS,0.9289991445680068,"Answer: [NA]
970"
SAFEGUARDS,0.9298545765611634,"Justification: The paper poses no such risks.
971"
SAFEGUARDS,0.93071000855432,"Guidelines:
972"
SAFEGUARDS,0.9315654405474765,"• The answer NA means that the paper poses no such risks.
973"
SAFEGUARDS,0.932420872540633,"• Released models that have a high risk for misuse or dual-use should be released with
974"
SAFEGUARDS,0.9332763045337895,"necessary safeguards to allow for controlled use of the model, for example by requiring
975"
SAFEGUARDS,0.9341317365269461,"that users adhere to usage guidelines or restrictions to access the model or implementing
976"
SAFEGUARDS,0.9349871685201027,"safety filters.
977"
SAFEGUARDS,0.9358426005132592,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
978"
SAFEGUARDS,0.9366980325064157,"should describe how they avoided releasing unsafe images.
979"
SAFEGUARDS,0.9375534644995723,"• We recognize that providing effective safeguards is challenging, and many papers do not
980"
SAFEGUARDS,0.9384088964927289,"require this, but we encourage authors to take this into account and make a best faith
981"
SAFEGUARDS,0.9392643284858854,"effort.
982"
LICENSES FOR EXISTING ASSETS,0.9401197604790419,"12. Licenses for existing assets
983"
LICENSES FOR EXISTING ASSETS,0.9409751924721984,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
984"
LICENSES FOR EXISTING ASSETS,0.941830624465355,"the paper, properly credited and are the license and terms of use explicitly mentioned and
985"
LICENSES FOR EXISTING ASSETS,0.9426860564585116,"properly respected?
986"
LICENSES FOR EXISTING ASSETS,0.9435414884516681,"Answer: [Yes]
987"
LICENSES FOR EXISTING ASSETS,0.9443969204448246,"Justification: The paper appropriately credits the creators of the utilized assets, including
988"
LICENSES FOR EXISTING ASSETS,0.9452523524379812,"code, data, and models, and explicitly mentions the licenses and terms of use, ensuring
989"
LICENSES FOR EXISTING ASSETS,0.9461077844311377,"compliance with the original terms set by the asset owners.
990"
LICENSES FOR EXISTING ASSETS,0.9469632164242943,"Guidelines:
991"
LICENSES FOR EXISTING ASSETS,0.9478186484174508,"• The answer NA means that the paper does not use existing assets.
992"
LICENSES FOR EXISTING ASSETS,0.9486740804106074,"• The authors should cite the original paper that produced the code package or dataset.
993"
LICENSES FOR EXISTING ASSETS,0.9495295124037639,"• The authors should state which version of the asset is used and, if possible, include a URL.
994"
LICENSES FOR EXISTING ASSETS,0.9503849443969205,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
995"
LICENSES FOR EXISTING ASSETS,0.951240376390077,"• For scraped data from a particular source (e.g., website), the copyright and terms of service
996"
LICENSES FOR EXISTING ASSETS,0.9520958083832335,"of that source should be provided.
997"
LICENSES FOR EXISTING ASSETS,0.9529512403763901,"• If assets are released, the license, copyright information, and terms of use in the package
998"
LICENSES FOR EXISTING ASSETS,0.9538066723695466,"should be provided. For popular datasets, paperswithcode.com/datasets has curated
999"
LICENSES FOR EXISTING ASSETS,0.9546621043627032,"licenses for some datasets. Their licensing guide can help determine the license of a
1000"
LICENSES FOR EXISTING ASSETS,0.9555175363558597,"dataset.
1001"
LICENSES FOR EXISTING ASSETS,0.9563729683490163,"• For existing datasets that are re-packaged, both the original license and the license of the
1002"
LICENSES FOR EXISTING ASSETS,0.9572284003421728,"derived asset (if it has changed) should be provided.
1003"
LICENSES FOR EXISTING ASSETS,0.9580838323353293,"• If this information is not available online, the authors are encouraged to reach out to the
1004"
LICENSES FOR EXISTING ASSETS,0.9589392643284859,"asset’s creators.
1005"
NEW ASSETS,0.9597946963216424,"13. New Assets
1006"
NEW ASSETS,0.960650128314799,"Question: Are new assets introduced in the paper well documented and is the documentation
1007"
NEW ASSETS,0.9615055603079555,"provided alongside the assets?
1008"
NEW ASSETS,0.962360992301112,"Answer: [NA]
1009"
NEW ASSETS,0.9632164242942686,"Justification: The paper does not release new assets.
1010"
NEW ASSETS,0.9640718562874252,"Guidelines:
1011"
NEW ASSETS,0.9649272882805817,"• The answer NA means that the paper does not release new assets.
1012"
NEW ASSETS,0.9657827202737382,"• Researchers should communicate the details of the dataset/code/model as part of their sub-
1013"
NEW ASSETS,0.9666381522668948,"missions via structured templates. This includes details about training, license, limitations,
1014"
NEW ASSETS,0.9674935842600513,"etc.
1015"
NEW ASSETS,0.9683490162532079,"• The paper should discuss whether and how consent was obtained from people whose asset
1016"
NEW ASSETS,0.9692044482463644,"is used.
1017"
NEW ASSETS,0.9700598802395209,"• At submission time, remember to anonymize your assets (if applicable). You can either
1018"
NEW ASSETS,0.9709153122326775,"create an anonymized URL or include an anonymized zip file.
1019"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717707442258341,"14. Crowdsourcing and Research with Human Subjects
1020"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9726261762189906,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1021"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9734816082121471,"include the full text of instructions given to participants and screenshots, if applicable, as
1022"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9743370402053037,"well as details about compensation (if any)?
1023"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751924721984602,"Answer: [NA]
1024"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760479041916168,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1025"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769033361847733,"Guidelines:
1026"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777587681779298,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1027"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786142001710864,"human subjects.
1028"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979469632164243,"• Including this information in the supplemental material is fine, but if the main contribution
1029"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9803250641573995,"of the paper involves human subjects, then as much detail as possible should be included
1030"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981180496150556,"in the main paper.
1031"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820359281437125,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or
1032"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828913601368692,"other labor should be paid at least the minimum wage in the country of the data collector.
1033"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837467921300257,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1034"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846022241231822,"Subjects
1035"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854576561163387,"Question: Does the paper describe potential risks incurred by study participants, whether
1036"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863130881094953,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1037"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871685201026519,"approvals (or an equivalent approval/review based on the requirements of your country or
1038"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880239520958084,"institution) were obtained?
1039"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888793840889649,"Answer: [NA]
1040"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897348160821214,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1041"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905902480752781,"Guidelines:
1042"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914456800684346,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1043"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923011120615911,"human subjects.
1044"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931565440547476,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1045"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940119760479041,"may be required for any human subjects research. If you obtained IRB approval, you
1046"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948674080410608,"should clearly state this in the paper.
1047"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957228400342173,"• We recognize that the procedures for this may vary significantly between institutions
1048"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965782720273738,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1049"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974337040205303,"guidelines for their institution.
1050"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998289136013687,"• For initial submissions, do not include any information that would break anonymity (if
1051"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991445680068435,"applicable), such as the institution conducting the review.
1052"
