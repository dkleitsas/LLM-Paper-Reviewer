Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0015923566878980893,"In this paper, we propose a novel neural exploration strategy in contextual bandits,
EE-Net, distinct from the standard UCB-based and TS-based approaches. Contex-
tual multi-armed bandits have been studied for decades with various applications.
To solve the exploitation-exploration tradeoff in bandits, there are three main tech-
niques: epsilon-greedy, Thompson Sampling (TS), and Upper Conﬁdence Bound
(UCB). In recent literature, linear contextual bandits have adopted ridge regression
to estimate the reward function and combine it with TS or UCB strategies for
exploration. However, this line of works explicitly assumes the reward is based on
a linear function of arm vectors, which may not be true in real-world datasets. To
overcome this challenge, a series of neural bandit algorithms have been proposed,
where a neural network is used to learn the underlying reward function and TS or
UCB are adapted for exploration. Instead of calculating a large-deviation based
statistical bound for exploration like previous methods, we propose ""EE-Net"", a
novel neural-based exploration strategy. In addition to using a neural network
(Exploitation network) to learn the reward function, EE-Net uses another neural
network (Exploration network) to adaptively learn potential gains compared to the
currently estimated reward for exploration. Then, a decision-maker is constructed
to combine the outputs from the Exploitation and Exploration networks. We prove
that EE-Net can achieve O(√T log T) regret and show that EE-Net outperforms
existing linear and neural contextual bandit baselines on real-world datasets."
INTRODUCTION,0.0031847133757961785,"1
INTRODUCTION"
INTRODUCTION,0.004777070063694267,"The stochastic contextual multi-armed bandit (MAB) (Lattimore and Szepesvári, 2020) has been
studied for decades in machine learning community to solve sequential decision making, with
applications in online advertising (Li et al., 2010), personal recommendation (Wu et al., 2016; Ban
and He, 2021b), etc. In the standard contextual bandit setting, a set of n arms are presented to a
learner in each round, where each arm is represented by a context vector. Then by certain strategy,
the learner selects and plays one arm, receiving a reward. The goal of this problem is to maximize
the cumulative rewards of T rounds."
INTRODUCTION,0.006369426751592357,"MAB algorithms have principled approaches to address the trade-off between Exploitation and
Exploration (EE), as the collected data from past rounds should be exploited to get good rewards but
also under-explored arms need to be explored with the hope of getting even better rewards. The most
widely-used approaches for EE trade-off can be classiﬁed into three main techniques: Epsilon-greedy
(Langford and Zhang, 2008), Thompson Sampling (TS) (Thompson, 1933), and Upper Conﬁdence
Bound (UCB) (Auer, 2002; Ban and He, 2020)."
INTRODUCTION,0.007961783439490446,"Linear bandits (Li et al., 2010; Dani et al., 2008; Abbasi-Yadkori et al., 2011), where the reward is
assumed to be a linear function with respect to arm vectors, have been well studied and succeeded
both empirically and theoretically. Given an arm, ridge regression is usually adapted to estimate its
reward based on collected data from past rounds. UCB-based algorithms (Li et al., 2010; Chu et al.,
2011; Wu et al., 2016; Ban and He, 2021b) calculate an upper bound for the conﬁdence ellipsoid
of estimated reward and determine the arm according to the sum of estimated reward and UCB.
TS-based algorithms (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017) formulate each arm as
a posterior distribution where mean is the estimated reward and choose the one with the maximal"
INTRODUCTION,0.009554140127388535,Published as a conference paper at ICLR 2022 !! ! Arm
INTRODUCTION,0.011146496815286623,Predict r
INTRODUCTION,0.012738853503184714,"Reward !"""
INTRODUCTION,0.014331210191082803,"∇!""""($; &"")"
INTRODUCTION,0.01592356687898089,Gradient
INTRODUCTION,0.01751592356687898,Predict
INTRODUCTION,0.01910828025477707,r −!!(%; '!)
INTRODUCTION,0.020700636942675158,Potential Gain
INTRODUCTION,0.022292993630573247,"Exploitation Network !!
Exploration Network !"" ℎ(!)"
INTRODUCTION,0.02388535031847134,"&!(!; (!) (!
("" ℎ(!)"
INTRODUCTION,0.025477707006369428,&!(!; (!)
INTRODUCTION,0.027070063694267517,"!""(∇!!; '"")"
INTRODUCTION,0.028662420382165606,"!""(∇!!; '"")"
INTRODUCTION,0.030254777070063694,"Case 1: Upward Exploration
Case 2: Downward Exploration"
INTRODUCTION,0.03184713375796178,"Adaptive Exploitation-Exploration: !! + !"""
INTRODUCTION,0.03343949044585987,Expected Reward
INTRODUCTION,0.03503184713375796,"Expected Reward
Estimated Reward"
INTRODUCTION,0.03662420382165605,Estimated Reward
INTRODUCTION,0.03821656050955414,"Figure 1: Left ﬁgure: Structure of EE-Net. In the right ﬁgure, Case 1: ""Upward"" exploration should
be made when the learner underestimates the reward; Case 2: ""Downward"" exploration should
be chosen when the learner overestimates the reward. EE-Net has the ability to adaptively make
exploration according to different cases. In contrast, UCB-based strategy will always make upward
exploration, and TS-based strategy will randomly choose upward or downward exploration."
INTRODUCTION,0.03980891719745223,"sampled reward. However, the linear assumption regarding the reward may not be true in real-world
applications (Valko et al., 2013)."
INTRODUCTION,0.041401273885350316,"To learn non-linear reward functions, recent works have utilized deep neural networks to learn the
underlying reward functions, thanks to its powerful representation ability. Considering the past
selected arms and received rewards as training samples, a neural network f1 is built for exploitation.
Zhou et al. (2020) computes a gradient-based upper conﬁdence bound with respect to f1 and uses
UCB strategy to select arms. Zhang et al. (2021) formulates each arm as a normal distribution where
the mean is f1 and deviation is calculated based on gradient of f1, and then uses the TS strategy to
choose arms. Both Zhou et al. (2020) and Zhang et al. (2021) achieve the near-optimal regret bound
of O(
√"
INTRODUCTION,0.042993630573248405,T log T).
INTRODUCTION,0.044585987261146494,"In this paper, we propose a novel neural exploration strategy, named ""EE-Net"". Similar to other
neural bandits, EE-Net has another exploitation network f1 to estimate rewards for each arm. The
crucial difference from existing works is that EE-Net has an exploration network f2 to predict the
potential gain for each arm compared to current reward estimate. The input to the exploration network
is the gradient of f1 and the ground-truth is residual difference between the true received reward
and the estimated reward from f1. The strategy is inspired by recent advances in the neural UCB
strategies (Zhou et al., 2020; Ban et al., 2021). Finally, a decision-maker f3 is constructed to select
arms. f3 has two modes: linear or nonlinear. In linear mode, f3 is a linear combination of f1 and f2,
inspired by the UCB strategy. In the nonlinear mode, f3 is formulated as a neural network with input
(f1, f2) and the goal is to learn the probability of being an optimal arm for each arm. Figure 1 depicts
the workﬂow of EE-Net and its advantages for exploration compared to UCB or TS-based methods
(see more details in Appendix D). To sum up, the contributions of this paper can be summarized as
follows:"
INTRODUCTION,0.04617834394904458,"1. We propose a novel neural exploration strategy, EE-Net, where another neural network is
assigned to learn the potential gain compared to the current reward estimate."
INTRODUCTION,0.04777070063694268,"2. Under standard assumptions of over-parameterized neural networks, we prove that EE-Net
can achieve the regret upper bound of O(√T log T), which improves a multiplicative factor
of √log T and is independent of either input or effective dimension, compared to existing
state-of-the-art neural bandit algorithms."
INTRODUCTION,0.04936305732484077,"3. We conduct extensive experiments on four real-world datasets, showing that EE-Net outper-
forms baselines including linear and neural versions of ϵ-greedy, TS, and UCB."
INTRODUCTION,0.050955414012738856,"Next, we discuss the problem deﬁnition in Sec.3, elaborate on the proposed EE-Net in Sec.4, and
present our theoretical analysis in Sec.5. In the end, we provide the empirical evaluation (Sec.6) and
conclusion."
INTRODUCTION,0.052547770700636945,Published as a conference paper at ICLR 2022
RELATED WORK,0.054140127388535034,"2
RELATED WORK"
RELATED WORK,0.05573248407643312,"Constrained Contextual bandits. The common constrain placed on the reward function is the linear
assumption, usually calculated by ridge regression (Li et al., 2010; Abbasi-Yadkori et al., 2011; Valko
et al., 2013; Dani et al., 2008). The linear UCB-based bandit algorithms (Abbasi-Yadkori et al.,
2011; Li et al., 2016) and the linear Thompson Sampling (Agrawal and Goyal, 2013; Abeille and
Lazaric, 2017) can achieve successful performance and the near-optimal regret bound of ˜O(
√"
RELATED WORK,0.05732484076433121,"T). To
break the linear assumption, Filippi et al. (2010) generalizes the reward function to a composition of
linear and non-linear functions and then adopt a UCB-based algorithm to deal with it; Bubeck et al.
(2011) imposes the Lipschitz property on reward metric space and constructs a hierarchical optimistic
optimization to make selections; Valko et al. (2013) embeds the reward function into Reproducing
Kernel Hilbert Space and proposes the kernelized TS/UCB bandit algorithms."
RELATED WORK,0.0589171974522293,"Neural Bandits. To learn non-linear reward functions, deep neural networks have been adapted to
bandits with various variants. Riquelme et al. (2018); Lu and Van Roy (2017) build L-layer DNN to
learn the arm embeddings and apply Thompson Sampling on the last layer for exploration. Zhou et al.
(2020) ﬁrst introduces a provable neural-based contextual bandit algorithm with a UCB exploration
strategy and then Zhang et al. (2021) extends the neural network to Thompson sampling framework.
Their regret analysis is built on recent advances on the convergence theory in over-parameterized
neural networks(Du et al., 2019; Allen-Zhu et al., 2019) and utilizes Neural Tangent Kernel (Jacot
et al., 2018; Arora et al., 2019) to construct connections with linear contextual bandits (Abbasi-
Yadkori et al., 2011). Ban and He (2021a) further adopts convolutional neural networks with UCB
exploration aiming for visual-aware applications. Xu et al. (2020) performs UCB-based exploration
on the last layer of neural networks to reduce the computational cost brought by gradient-based UCB.
Different from the above existing works, EE-Net keeps the powerful representation ability of neural
networks to learn reward function and ﬁrst assigns another neural network to determine exploration."
PROBLEM DEFINITION,0.06050955414012739,"3
PROBLEM DEFINITION"
PROBLEM DEFINITION,0.06210191082802548,"We consider the standard contextual multi-armed bandit with the known number of rounds T (Zhou
et al., 2020; Zhang et al., 2021). In each round t ∈[T], where the sequence [T] = [1, 2, . . . , T],
the learner is presented with n arms, Xt = {xt,1, . . . , xt,n}, in which each arm is represented by a
feature vector xt,i ∈Rd for each i ∈[n]. After playing one arm xt,i, its reward rt,i is assumed to be
generated by the function:
rt,i = h(xt,i) + ηt,i,
(3.1)
where the unknown reward function h(xt,i) can be either linear or non-linear and the noise ηt,i is
drawn from certain distribution with expectation E[ηt,i] = 0. Following many existing works (Zhou
et al., 2020; Ban et al., 2021; Zhang et al., 2021), we consider bounded rewards, rt,i ∈[a, b]. For the
brevity, we denote the selected arm in round t by xt and the reward received in t by rt. The pseudo
regret of T rounds is deﬁned as:"
PROBLEM DEFINITION,0.06369426751592357,"RT = E "" T
X"
PROBLEM DEFINITION,0.06528662420382166,"t=1
(r∗
t −rt) #"
PROBLEM DEFINITION,0.06687898089171974,",
(3.2)"
PROBLEM DEFINITION,0.06847133757961783,"where E[r∗
t | Xt] = maxi∈[n] h(xt,i) is the maximal expected reward in the round t. The goal of this
problem is to minimize RT by certain selection strategy."
PROBLEM DEFINITION,0.07006369426751592,"Notation. We denote by {xi}t
i=1 the sequence (x1, . . . , xt). We use ∥v∥2 to denote the Euclidean
norm for a vector v, and ∥W∥2 and ∥W∥F to denote the spectral and Frobenius norm for a matrix
W. We use ⟨·, ·⟩to denote the standard inner product between two vectors or two matrices. We may
use ▽θ1
t f1(xt,i) or ▽θ1
t f1 to represent the gradient ▽θ1
t f1(xt,i; θ1
t) for brevity. We use {xτ, rτ}t
τ=1
to represent the collected data up to round t."
PROBLEM DEFINITION,0.07165605095541401,"4
PROPOSED METHOD: EE-NET"
PROBLEM DEFINITION,0.0732484076433121,"EE-Net is composed of three components. The ﬁrst component is the exploitation network, f1(·; θ1),
which focuses on learning the unknown reward function h based on the data collected in past rounds."
PROBLEM DEFINITION,0.07484076433121019,Published as a conference paper at ICLR 2022
PROBLEM DEFINITION,0.07643312101910828,Table 1: Structure of EE-Net (Round t).
PROBLEM DEFINITION,0.07802547770700637,"Input
Network
Label"
PROBLEM DEFINITION,0.07961783439490445,"{xτ}t
τ=1
f1(·; θ1) (Exploitation)
{rτ}t
τ=1"
PROBLEM DEFINITION,0.08121019108280254,"{▽θ1
τ−1f1(xτ; θ1
τ−1)}t
τ=1
f2(·; θ2) (Exploration)"
PROBLEM DEFINITION,0.08280254777070063," 
rτ −f1(xτ; θ1
τ−1)
	t τ=1"
PROBLEM DEFINITION,0.08439490445859872,"{(f1(xτ; θ1
τ−1), f2(▽θ1
τ−1f1; θ2
τ−1))}t
τ=1
f3(·; θ3)
(Decision-
maker with non-linear
function)"
PROBLEM DEFINITION,0.08598726114649681,"{pτ}t
τ=1"
PROBLEM DEFINITION,0.0875796178343949,"The second component is the exploration network, f2(·; θ2), which focuses on characterizing the level
of exploration needed for each arm in the present round. The third component is the decision-maker,
f3, which focuses on suitably combining the outputs of the exploitation and exploration networks
leading to the arm selection."
PROBLEM DEFINITION,0.08917197452229299,"1) Exploitation Net. The exploitation net f1 is a neural network which learns the mapping from
arms to rewards. In round t, denote the network by f1(·; θ1
t−1), where the superscript of θ1
t−1 is the
index of network and the subscript represents the round where the parameters of f1 ﬁnished the last
update. Given an arm xt,i, i ∈[n], f1(xt,i; θ1
t−1) is considered the ""exploitation score"" for xt,i. By
some criterion, after playing arm xt, we receive a reward rt. Therefore, we can conduct gradient
descent to update θ1 based on the collected training samples {xτ, rτ}t
τ=1 and denote the updated
parameters by θ1
t."
PROBLEM DEFINITION,0.09076433121019108,"2) Exploration Net. Our exploration strategy is inspired by existing UCB-based neural bandits
(Zhou et al., 2020; Ban et al., 2021). Based on the Lemma 5.2 in (Ban et al., 2021), given an arm xt,i,
with probability at least 1 −δ, we have the following UCB form:"
PROBLEM DEFINITION,0.09235668789808917,"|h(xt,i) −f1(xt,i; θ1
t−1)| ≤Ψ(▽θ1
t−1f1(xt,i; θ1
t−1)),
(4.1)"
PROBLEM DEFINITION,0.09394904458598727,"where h is deﬁned in Eq. (3.1) and Ψ is an upper conﬁdence bound represented by a function with
respect to the gradient ▽θ1
t−1f1 (see more details and discussions in Appendix D). Then we have the
following deﬁnition."
PROBLEM DEFINITION,0.09554140127388536,"Deﬁnition 4.1. In round t, given an arm xt,i, we deﬁne h(xt,i) −f1(xt,i; θ1
t−1) as the ""expected
potential gain"" for xt,i and rt,i −f1(xt,i; θ1
t−1) as the ""potential gain"" for xt,i."
PROBLEM DEFINITION,0.09713375796178345,"Let yt,i = rt,i −f1(xt,i; θ1
t−1). When yt,i > 0, the arm xt,i has positive potential gain compared to
the estimated reward f1(xt,i; θ1
t−1). A large positive yt,i makes the arm more suitable for exploration,
whereas a small (or negative) yt,i makes the arm unsuitable for exploration. Recall that traditional
approaches such as UCB intend to estimate such potential gain yt,i using standard tools, e.g., Markov
inequality, Hoeffding bounds, etc., from large deviation bounds."
PROBLEM DEFINITION,0.09872611464968153,"Instead of calculating a large-deviation based statistical bound for yt,i, we use a neural network
f2(·; θ2) to represent Ψ, where the input is ▽θ1
t−1f1(xt,i) and the ground truth is rt,i −f1(xt,i; θ1
t−1).
Adopting gradient ▽θ1
t−1f1(xt,i) as the input also is due to the fact that it incorporates two aspects of
information: the feature of the arm and the discriminative information of f1."
PROBLEM DEFINITION,0.10031847133757962,"Moreover, in the upper bound of NeuralUCB or the variance of NeuralTS, there is a recursive term
At−1 = I + Pt−1
τ=1 ∇θ1
τ−1f1(xτ)∇θ1
τ−1f1(xτ)⊤which is a function of past gradients up to (t −1)
and incorporates relevant historical information. On the contrary, in EE-Net, the recursive term
which depends on past gradients is θ2
t−1 in the exploration network f2 because we have conducted
gradient descent for θ2
t−1 based on {∇θ1
τ−1f1(xτ)}t−1
τ=1. Therefore, this form θ2
t−1 is similar to At−1
in neuralUCB/TS, but EE-net does not (need to) make a speciﬁc assumption about the functional
form of past gradients, and is also more memory-efﬁcient."
PROBLEM DEFINITION,0.10191082802547771,"To sum up, in round t, we consider f2(▽θ1
t−1f1(xt,i); θ2
t−1) as the ""exploration score"" of xt,i,"
PROBLEM DEFINITION,0.1035031847133758,"because it indicates the potential gain of xt,i compared to our current exploitation score f1(xt,i; θ1
t−1).
Therefore, after receiving the reward rt, we can use gradient descent to update θ2 based on collected
training samples {▽θ1
τ−1f1(xτ), rτ −f1(xτ; θ1
τ−1}t
τ=1. We also provide other two heuristic forms"
PROBLEM DEFINITION,0.10509554140127389,"for f2’s ground-truth label: |rt,i −f1(xt,i; θ1
t−1)| and ReLU(rt,i −f1(xt,i; θ1
t−1)). We compare
them in an ablation study in Appendix B."
PROBLEM DEFINITION,0.10668789808917198,Published as a conference paper at ICLR 2022
PROBLEM DEFINITION,0.10828025477707007,"Algorithm 1 EE-Net
Input: f1, f2, f3, T (number of rounds), η1 (learning rate for f1), η2 (learning rate for f2), η3
(learning rate for f3), K1 (number of iterations for f1), K2 (number of iterations for f2) , K3
(number of iterations for f3), φ (normalization operator)"
PROBLEM DEFINITION,0.10987261146496816,"1: Initialize θ1
0, θ2
0, θ3
0; bθ
1
0 = θ1
0, bθ
2
0 = θ2
0, bθ
3
0 = θ3
0
2: for t = 1, 2, . . . , T do
3:
Observe n arms {xt,1, . . . , xt,n}
4:
for each i ∈[n] do
5:
Compute f1(xt,i; θ1
t−1), f2(φ(▽θ1
t−1f1(xt,i)); θ2
t−1), f3((f1, f2); θ3
t−1)
6:
end for
7:
xt = arg maxxt,i,i∈[n] f3

f1(xt,i; θ1
t−1), f2(φ(▽θ1
t−1f1(xt,i)); θ2
t−1); θ3
t−1
"
PROBLEM DEFINITION,0.11146496815286625,"8:
Play xt and observe reward rt
9:
θ1
t, θ2
t, θ3
t = GRADIENTDESCENT(θ0, {xτ}t
τ=1, {rτ}t
τ=1)
10: end for
11:
12: procedure GRADIENTDESCENT(θ0, {xτ}t
τ=1, {rτ}t
τ=1)"
PROBLEM DEFINITION,0.11305732484076433,"13:
L1 = 1"
PT,0.11464968152866242,"2
Pt
τ=1
 
f1(xτ; θ1) −rτ
2"
PT,0.11624203821656051,"14:
θ1,(0) = θ1
0
15:
for k ∈{1, . . . , K1} do
16:
θ1,(k) = θ1,(k−1) −η1▽θ1,(k−1)L1
17:
end for
18:
bθ
1
t = θ1,(K1)"
PT,0.1178343949044586,"19:
L2 = 1"
PT,0.11942675159235669,"2
Pt
τ=1

f2(φ(▽θ1
τ−1f1(xτ)); θ2) −(rτ −f1(xτ; θ1
τ−1))
2"
PT,0.12101910828025478,"20:
θ2,(0) = θ2
0
21:
for k ∈{1, . . . , K2} do
22:
θ2,(k) = θ2,(k−1) −η2▽θ2,(k−1)L2
23:
end for
24:
bθ
2
t = θ2,(K2)"
PT,0.12261146496815287,"25:
Determine label pt
26:
L3 = −1"
PT,0.12420382165605096,"t
Pt
i=1

pt log f3((f1, f2); θ3) + (1 −pt) log(1 −f3((f1, f2); θ3))
"
PT,0.12579617834394904,"27:
θ3,(0) = θ3
0
28:
for k ∈{1, . . . , K3} do
29:
θ3,(k) = θ3,(k−1) −η3▽θ3,(k−1)L3
30:
end for
31:
bθ
3
t = θ3,(K3)"
PT,0.12738853503184713,"32:
Randomly choose (θ1
t, θ2
t) uniformly from {(bθ
1
0, bθ
2
0), (bθ
1
1, bθ
2
1), . . . , (bθ
1
t, bθ
2
t)}"
PT,0.12898089171974522,"33:
Randomly choose θ3
t uniformly from {bθ
3
0, bθ
3
1, . . . , bθ
3
t}
34:
Return θ1
t, θ2
t, θ3
t
35: end procedure"
PT,0.1305732484076433,"3) Decision-maker. In round t, given an arm xt,i, i ∈[n], with the computed exploitation score
f1(xt,i; θ1
t−1) and exploration score f2(▽θ1
t−1f1; θ2
t−1), we use a function f3
 
f1, f2; θ3
to trade
off between exploitation and exploration and compute the ﬁnal score for xt,i. The selection criterion
is deﬁned as"
PT,0.1321656050955414,"xt = arg
max
xt,i,i∈[n] f3

f1(xt,i; θ1
t−1), f2

▽θ1
t−1f1(xt,i); θ2
t−1

; θ3
t−1

."
PT,0.1337579617834395,Note that f3 can be either linear or non-linear functions. We provide the following two forms.
PT,0.13535031847133758,(1) Linear function. f3 can be formulated as a linear function with respect to f1 and f2 :
PT,0.13694267515923567,"f3(f1, f2; θ3) = w1f1(xt,i; θ1) + w2f2(▽θ1f1; θ2)"
PT,0.13853503184713375,Published as a conference paper at ICLR 2022
PT,0.14012738853503184,"where w1, w2 are two weights preset by the learner. When w1 = w2 = 1, f3 can be thought of as
UCB-type policy, where the estimated reward f1 and potential gain f2 are simply added together. In
experiments, we report its empirical performance in ablation study (Appendix B)."
PT,0.14171974522292993,"(2) Non-linear function. f3 also can be formulated as a neural network to learn the mapping from
(f1, f2) to the optimal arm. We transform the bandit problem into a binary classiﬁcation problem.
Given an arm xt,i, we deﬁne pt,i as the probability of being the optimal arm for xt,i in round t. For
brevity, we denote by pt the probability of being the optimal arm for the selected arm xt in round t.
According to different reward distributions, we have different approaches to determine pt."
PT,0.14331210191082802,"1. Binary reward. ∀t ∈[T], suppose rt is a binary variable over a, b(a < b), it is straightfor-
ward to set: pt = 1.0 if rt = b; pt = 0.0, otherwise."
PT,0.1449044585987261,"2. Continuous reward. ∀t ∈[T], suppose rt is a continuous variable over the range [a, b], we
provide two ways to determine pt. (1) pt can be directly set as rt−a"
PT,0.1464968152866242,"b−a . (2) The learner can set
a threshold θ, (a < θ < b). Then pt = 1.0 if rt > θ; pt = 0.0, otherwise."
PT,0.1480891719745223,"Therefore, with the collected training samples
n
f1(xτ; θ1
τ−1), f2(▽θ1
τ−1f1; θ2
τ−1)

, pτ
ot"
PT,0.14968152866242038,τ=1 in
PT,0.15127388535031847,"round t, we can conduct gradient descent to update parameters of f3(·; θ3)."
PT,0.15286624203821655,"Table 1 details the working structure of EE-Net. Algorithm 1 depicts the workﬂow of EE-Net, where
the input of f2 is normalized, i.e., φ(▽θ1
t−1f1(xt,i)). Algorithm 1 provides a version of gradient"
PT,0.15445859872611464,"descent (GD) to update EE-Net, where drawing (θ1
t, θ2
t) uniformly from their stored historical
parameters is for the sake of analysis. One can easily extend EE-Net to stochastic GD to update the
parameters incrementally."
PT,0.15605095541401273,"Remark 4.1 (Network structure). The networks f1, f2, f3 can be different structures according
to different applications. For example, in the vision tasks, f1 can be set up as convolutional layers
(LeCun et al., 1995). For the exploration network f2, the input ▽θ1f1 may have exploding dimensions
when the exploitation network f1 becomes wide and deep, which may cause huge computation cost
for f2. To address this challenge, we can apply dimensionality reduction techniques to obtain low-
dimensional vectors of ▽θ1f1. In the experiments, we use Roweis and Saul (2000) to acquire a
10-dimensional vector for ▽θ1f1 and achieve the best performance among all baselines."
PT,0.15764331210191082,"Remark 4.2 (Exploration direction). EE-Net has the ability to determine exploration direction.
Given an arm xt,i, when the estimation f1(xt,i) is lower than the expected reward h(xt,i), the learner
should make the ""upward"" exploration, i.e., increase the chance of xt,i being explored; When f1(xt,i)
is higher than h(xt,i), the learner should do the ""downward"" exploration, i.e., decrease the chance
of xt,i being explored. EE-Net uses the neural network f2 to learn h(xt,i) −f1(xt,i) (which has
positive and negative scores) and has the ability to determine the exploration direction. In contrast,
NeuralUCB will always make ""upward"" exploration and NeuralTS will randomly choose between
""upward"" exploration and ""downward"" exploration (see selection criteria in Table 2 and more details
in Appendix D)."
PT,0.1592356687898089,"Remark 4.3 (Space complexity). NeuralUCB and NeuralTS have to maintain the gradient outer
product matrix (e.g., At = Pt
τ=1 ▽θ1f1(xτ; θ1
τ)▽θ1f1(xτ; θ1
τ)⊤∈Rp×p) and, for θ1 ∈Rp, have
a space complexity of O(p2) to store the outer product. On the contrary, EE-Net does not have this
matrix and only regards ▽θ1f1 as the input of f2. Thus, EE-Net reduces the space complexity from
O(p2) to O(p)."
REGRET ANALYSIS,0.160828025477707,"5
REGRET ANALYSIS"
REGRET ANALYSIS,0.1624203821656051,"In this section, we provide the regret analysis of EE-Net when f3 is set as the linear function
f3 = f1+f2, which can be thought of as the UCB-type trade-off between exploitation and exploration.
For the sake of simplicity, we conduct the regret analysis on some unknown but ﬁxed data distribution
D. In each round t, n samples {(xt,1, rt,1), (xt,2, rt,2), . . . , (xt,n, rt,n)} are drawn i.i.d. from D.
This is standard distribution assumption in over-parameterized neural networks (Cao and Gu, 2019).
Then, for the analysis, we have the following assumption, which is a standard input assumption in
neural bandits and over-parameterized neural networks(Zhou et al., 2020; Allen-Zhu et al., 2019)."
REGRET ANALYSIS,0.16401273885350318,Published as a conference paper at ICLR 2022
REGRET ANALYSIS,0.16560509554140126,"Assumption 5.1 (ρ-Separability). For any t ∈[T], i ∈[n], ∥xt,i∥2 = 1, and rt,i ∈[0, 1]. Then, for
every pair xt,i, xt′,i′, t′ ∈[T], i′ ∈[k], and (t, i) ̸= (t′, i′), ∥xt,i −xt′,i′∥2 > ρ, and suppose there
exists an operator such that ∥φ(·)∥2 = 1 and ∥φ(▽θ1f1(xt,i)) −φ(▽θ1f1(xt′,i′))∥2 ≥ρ"
REGRET ANALYSIS,0.16719745222929935,"For example, the operator can be designed as φ(▽θ1f1(xt,i)) = (
▽θ1f1(xt,i)
√"
REGRET ANALYSIS,0.16878980891719744,"2∥▽θ1f1(xt,i)∥2 , xt,i
√"
REGRET ANALYSIS,0.17038216560509553,"2 ). The
analysis will focus on over-parameterized neural networks (Jacot et al., 2018; Du et al., 2019; Allen-
Zhu et al., 2019). Given an input x ∈Rd, without loss of generality, we deﬁne the fully-connected
network f with depth L ≥2 and width m:"
REGRET ANALYSIS,0.17197452229299362,"f(x; θ) = WLσ(WL−1σ(WL−2 . . . σ(W1x)))
(5.1)"
REGRET ANALYSIS,0.1735668789808917,"where σ is the ReLU activation function, W1 ∈Rm×d, Wl ∈Rm×m, for 2 ≤l ≤L −1,
WL ∈R1×m, and θ = [vec(W1)⊺, vec(W2)⊺, . . . , vec(WL)⊺]⊺."
REGRET ANALYSIS,0.1751592356687898,"Initialization. For any l ∈[L −1], each entry of Wl is drawn from the normal distribution N(0, 2"
REGRET ANALYSIS,0.1767515923566879,"m)
and WL is drawn from the normal distribution N(0, 1"
REGRET ANALYSIS,0.17834394904458598,"m). Note that EE-Net at most has three networks
f1, f2, f3. We deﬁne them following the deﬁnition of f for brevity, although they may have different
depth or width. Then, we have the following theorem for EE-Net. Recall that η1, η2 are the learning
rates for f1, f2; K1 is the number of iterations of gradient descent for f1 in each round; and K2 is the
number of iterations for f2.
Theorem 1. Let f1, f2 follow the setting of f (Eq. (5.1) ) with the same width m and depth L. Let
L1, L2 be loss functions deﬁned in Algorithm 1. Set f3 as f3 = f1 + f2. For any δ ∈(0, 1), ϵ ∈
(0, O( 1"
REGRET ANALYSIS,0.17993630573248406,"T )], ρ ∈(0, O( 1"
REGRET ANALYSIS,0.18152866242038215,"L)], suppose"
REGRET ANALYSIS,0.18312101910828024,"m ≥eΩ

poly(T, n, L, ρ−1) · log(1/δ) · e
√"
REGRET ANALYSIS,0.18471337579617833,"log(T n/δ))

,"
REGRET ANALYSIS,0.18630573248407642,"η1 = η2 = min

Θ

T 5
√ 2δ2m"
REGRET ANALYSIS,0.18789808917197454,"
, Θ

ρ
poly(T, n, L) · m 
,"
REGRET ANALYSIS,0.18949044585987262,"K1 = K2 = Θ
poly(T, n, L)"
REGRET ANALYSIS,0.1910828025477707,"ρδ2
· log
 
ϵ−1
. (5.2)"
REGRET ANALYSIS,0.1926751592356688,"Then, with probability at least 1 −δ over the initialization, the pseudo regret of EE-Net in T rounds
satisﬁes"
REGRET ANALYSIS,0.1942675159235669,"RT ≤O(1) + (2
√"
REGRET ANALYSIS,0.19585987261146498,"T −1)3
√"
REGRET ANALYSIS,0.19745222929936307,"2O(L) + O  (2
√ T −1) r"
REGRET ANALYSIS,0.19904458598726116,2 log O(Tn) δ !
REGRET ANALYSIS,0.20063694267515925,".
(5.3)"
REGRET ANALYSIS,0.20222929936305734,"Comparison with existing works. Under the similar assumptions in over-parameterized neural
networks, the regret bounds complexity of NeuralUCB (Zhou et al., 2020) and NeuralTS (Zhang
et al., 2021) both are"
REGRET ANALYSIS,0.20382165605095542,"RT ≤O
q"
REGRET ANALYSIS,0.2054140127388535,"˜dT log T

· O
q"
REGRET ANALYSIS,0.2070063694267516,"˜d log T

, and ˜d = log det(I + H/λ)"
REGRET ANALYSIS,0.2085987261146497,log(1 + Tn/λ)
REGRET ANALYSIS,0.21019108280254778,"where H is the neural tangent kernel matrix (NTK) (Jacot et al., 2018; Arora et al., 2019) and λ is a
regularization parameter. Similarly, in linear contextual bandits, Abbasi-Yadkori et al. (2011) achieve
O(d
√"
REGRET ANALYSIS,0.21178343949044587,"T log T) and Li et al. (2017) achieve O(
√"
REGRET ANALYSIS,0.21337579617834396,"dT log T).
Remark 5.1. Compared to NeuralUCB/TS, EE-Net roughly improves by a multiplicative factor
of √log T, because our proof of EE-Net is directly built on recent advances in convergence theory
(Allen-Zhu et al., 2019) and generalization bound (Cao and Gu, 2019) of over-parameterized neural
networks. Instead, the analysis for NeuralUCB/TS contains three parts of approximation error by
calculating the distances between the expected reward and ridge regression, ridge regression and
NTK, and NTK and network function.
Remark 5.2. The regret bound of EE-Net does not have the effective dimension ˜d or input dimension
d. ˜d or d may cause signiﬁcant error, when the determinant of H is extremely large or d > T."
REGRET ANALYSIS,0.21496815286624205,"The proof of Theorem 1 is in Appendix C and mainly based on the following generalization bound.
The bound results from an online-to-batch conversion while using convergence guarantees of deep
learning optimization."
REGRET ANALYSIS,0.21656050955414013,Published as a conference paper at ICLR 2022
REGRET ANALYSIS,0.21815286624203822,"Lemma 5.1. For any δ ∈(0, 1), ϵ ∈(0, 1), ρ ∈(0, O( 1"
REGRET ANALYSIS,0.2197452229299363,"L)), suppose m, η1, η2, K1, K2 satisfy the
conditions in Eq. (5.2) and (xτ,i, rτ,i) ∼D, ∀τ ∈[t], i ∈[n]. Let"
REGRET ANALYSIS,0.2213375796178344,"xt = arg
max
xt,i,i∈[n]"
REGRET ANALYSIS,0.2229299363057325,"h
f2

φ(▽θ1
t−1f1(xt,i; θ1
t−1)); θ2
t−1

+ f1(xt,i; θ1
t−1)
i
,"
REGRET ANALYSIS,0.22452229299363058,"and rt is the corresponding reward, given (xt,i, rt,i), i ∈[n]. Then, with probability at least (1 −δ)
over the random of the initialization, it holds that"
REGRET ANALYSIS,0.22611464968152867,"E
(xt,i,rt,i),i∈[n]"
REGRET ANALYSIS,0.22770700636942676,"hf2

φ(▽θ1
t−1f1(xt,i; θ1
t−1)); θ2
t−1

−
 
rt −f1(xt; θ1
t−1)
 | {xτ, rτ}t−1
τ=1
i ≤ r 2ϵ"
REGRET ANALYSIS,0.22929936305732485,"t + O
 3L
√"
T,0.23089171974522293,2t
T,0.23248407643312102,"
+ (1 + 2ξ) r"
T,0.2340764331210191,2 log(O(tn/δ))
T,0.2356687898089172,"t
,
(5.4)"
T,0.2372611464968153,"where the expectation is also taken over (θ1
t−1, θ2
t−1) that are uniformly drawn from (bθ
1
τ, bθ
2
τ), τ ∈
[t −1]."
T,0.23885350318471338,"Remark 5.3. Lemma 5.1 provides a ﬁxed ˜O( 1
√"
T,0.24044585987261147,"t)-rate generalization bound for exploitation-
exploration networks f1, f2 in contrast with the relative bound w.r.t. the Neural Tangent Random
Feature (NTRF) benchmark (Cao and Gu, 2019). We achieve this by working in the regression rather
than classiﬁcation setting and utilizing the convergence guarantees for square loss (Allen-Zhu et al.,
2019). Note that the bound in Lemma 5.1 holds in the setting of bounded (possibly random) rewards
r ∈[0, 1] instead of a ﬁxed function in the conventional classiﬁcation setting."
EXPERIMENTS,0.24203821656050956,"6
EXPERIMENTS"
EXPERIMENTS,0.24363057324840764,"In this section, we evaluate EE-Net on four real-world datasets comparing with strong state-of-the-art
baselines. We ﬁrst present the setup of experiments, then show regret comparison and report ablation
study. Codes are available at 1."
EXPERIMENTS,0.24522292993630573,"We use four real-world datasets: Mnist, Yelp, Movielens, and Disin, the details and settings of
which are attached in Appendix A."
EXPERIMENTS,0.24681528662420382,"Figure 2: Regret comparison on Movielens and Yelp (mean of 10 runs with standard deviation
(shadow)). With the same exploitation network f1, EE-Net outperforms all baselines.
Baselines. To comprehensively evaluate EE-Net, we choose 3 neural-based bandit algorithms, one
linear and one kernelized bandit algorithms."
EXPERIMENTS,0.2484076433121019,"1. LinUCB (Li et al., 2010) explicitly assumes the reward is a linear function of arm vector
and unknown user parameter and then applies the ridge regression and un upper conﬁdence
bound to determine selected arm."
EXPERIMENTS,0.25,"2. KernelUCB (Valko et al., 2013) adopts a predeﬁned kernel matrix on the reward space
combined with a UCB-based exploration strategy."
EXPERIMENTS,0.2515923566878981,"3. Neural-Epsilon adapts the epsilon-greedy exploration strategy on exploitation network f1.
I.e., with probability 1 −ϵ, the arm is selected by xt = arg maxi∈[n] f1(xt,i; θ1) and with
probability ϵ, the arm is chosen randomly."
EXPERIMENTS,0.2531847133757962,1https://github.com/banyikun/EE-Net-ICLR-2022
EXPERIMENTS,0.25477707006369427,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.25636942675159236,"Figure 3: Regret comparison on Mnist and Disin (mean of 10 runs with standard deviation (shadow)).
With the same exploitation network f1, EE-Net outperforms all baselines."
EXPERIMENTS,0.25796178343949044,"4. NeuralUCB (Zhou et al., 2020) uses the exploitation network f1 to learn the reward function
coming with an UCB-based exploration strategy."
EXPERIMENTS,0.25955414012738853,"5. NeuralTS (Zhang et al., 2021) adopts the exploitation network f1 to learn the reward function
coming with an Thompson Sampling exploration strategy."
EXPERIMENTS,0.2611464968152866,"Note that we do not report results of LinTS and KernelTS in experiments, because of the limited
space in ﬁgures, but LinTS and KernelTS have been signiﬁcantly outperformed by NeuralTS (Zhang
et al., 2021)."
EXPERIMENTS,0.2627388535031847,"Setup for EE-Net. To compare fairly, for all the neural-based methods including EE-Net, the
exploitation network f1 is built by a 2-layer fully-connected network with 100 width. For the
exploration network f2, we use a 2-layer fully-connected network with 100 width as well. For the
decision maker f3, by comprehensively evaluate both linear and nonlinear functions, we found that
the most effective approach is combining them together, which we call "" hybrid decision maker"". In
detail, for rounds t ≤500, f3 is set as f3 = f2 + f1, and for t > 500, f3 is set as a neural network
with two 20-width fully-connected layers. Setting f3 in this way is because the linear decision maker
can maintain stable performance in each running (robustness) and the non-linear decision maker can
further improve the performance (see details in Appendix B). The hybrid decision maker can combine
these two advantages together. The conﬁgurations of all methods are attached in Appendix A."
EXPERIMENTS,0.2643312101910828,"Results. Figure 2 and Figure 3 show the regret comparison on these four datasets. EE-Net consistently
outperforms all baselines across all datasets. For LinUCB and KernelUCN, the simple linear reward
function or predeﬁned kernel cannot properly formulate ground-truth reward function existed in
real-world datasets. In particular, on Mnist and Disin datasets, the correlations between rewards
and arm feature vectors are not linear or some simple mappings. Thus, LinUCB and KernelUCB
barely exploit the past collected data samples and fail to select correct arms. For neural-based
bandit algorithms, the exploration probability of Neural-Epsilon is ﬁxed and difﬁcult to be adjustable.
Thus it is usually hard to make effective exploration. To make exploration, NeuralUCB statistically
calculates a gradient-based upper conﬁdence bound and NeuralTS draws each arm’s predicted reward
from a normal distribution where the standard deviation is computed by gradient. However, the
conﬁdence bound or standard deviation they calculated only consider the worst cases and thus
may not be able represent the actual potential of each arm, and they cannot make ""upward"" and
""downward"" exploration properly. Instead, EE-Net uses a neural network f2 to learn each arm’s
potential by neural network’s powerful representation ability. Therefore, EE-Net can outperform
these two state-of-the-art bandit algorithms. Note that NeuralUCB/TS does need two parameters to
tune UCB/TS according to different scenarios while EE-Net only needs to set up a neural network
and automatically learns it."
EXPERIMENTS,0.2659235668789809,"Ablation Study. In Appendix B, we conduct ablation study regarding the label function y of f2 and
the different setting of f3.
7
CONCLUSION
In this paper, we propose a novel exploration strategy, EE-Net. In addition to a neural network that
exploits collected data in past rounds , EE-Net has another neural network to learn the potential gain
compared to current estimation for exploration. Then, a decision maker is built to make selections
to further trade off between exploitation and exploration. We demonstrate that EE-Net outperforms
NeuralUCB and NeuralTS both theoretically and empirically, becoming the new state-of-the-art
exploration policy."
EXPERIMENTS,0.267515923566879,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.26910828025477707,"Acknowledgements: We are grateful to Shiliang Zuo and Yunzhe Qi for the valuable discussions
in the revisions of EE-Net. This research work is supported by National Science Foundation
under Awards No. IIS-1947203, IIS-2002540, IIS-2137468, IIS-1908104, OAC-1934634, and DBI-
2021898, and a grant from C3.ai. The views and conclusions are those of the authors and should not
be interpreted as representing the ofﬁcial policies of the funding agencies or the government."
REFERENCES,0.27070063694267515,REFERENCES
REFERENCES,0.27229299363057324,"Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic bandits. In
Advances in Neural Information Processing Systems, pages 2312–2320, 2011."
REFERENCES,0.27388535031847133,"M. Abeille and A. Lazaric. Linear thompson sampling revisited. In Artiﬁcial Intelligence and
Statistics, pages 176–184. PMLR, 2017."
REFERENCES,0.2754777070063694,"S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In
International Conference on Machine Learning, pages 127–135. PMLR, 2013."
REFERENCES,0.2770700636942675,"H. Ahmed, I. Traore, and S. Saad. Detecting opinion spams and fake news using text classiﬁcation.
Security and Privacy, 1(1):e9, 2018."
REFERENCES,0.2786624203821656,"Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In International Conference on Machine Learning, pages 242–252. PMLR, 2019."
REFERENCES,0.2802547770700637,"S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. On exact computation with
an inﬁnitely wide neural net. In Advances in Neural Information Processing Systems, pages
8141–8150, 2019."
REFERENCES,0.2818471337579618,"P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397–422, 2002."
REFERENCES,0.28343949044585987,"Y. Ban and J. He. Generic outlier detection in multi-armed bandit. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 913–923,
2020."
REFERENCES,0.28503184713375795,"Y. Ban and J. He. Convolutional neural bandit: Provable algorithm for visual-aware advertising.
arXiv preprint arXiv:2107.07438, 2021a."
REFERENCES,0.28662420382165604,"Y. Ban and J. He. Local clustering in contextual multi-armed bandits. In Proceedings of the Web
Conference 2021, pages 2335–2346, 2021b."
REFERENCES,0.28821656050955413,"Y. Ban, J. He, and C. B. Cook. Multi-facet contextual bandits: A neural network perspective. In
The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,
Singapore, August 14-18, 2021, pages 35–45, 2021."
REFERENCES,0.2898089171974522,"S. Bubeck, R. Munos, G. Stoltz, and C. Szepesvári. X-armed bandits. Journal of Machine Learning
Research, 12(5), 2011."
REFERENCES,0.2914012738853503,"Y. Cao and Q. Gu. Generalization bounds of stochastic gradient descent for wide and deep neural
networks. Advances in Neural Information Processing Systems, 32:10836–10846, 2019."
REFERENCES,0.2929936305732484,"N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning
algorithms. Advances in neural information processing systems, 14, 2001."
REFERENCES,0.2945859872611465,"E. Chlebus. An approximate formula for a partial sum of the divergent p-series. Applied Mathematics
Letters, 22(5):732–737, 2009."
REFERENCES,0.2961783439490446,"W. Chu, L. Li, L. Reyzin, and R. Schapire. Contextual bandits with linear payoff functions. In
Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics,
pages 208–214, 2011."
REFERENCES,0.29777070063694266,"V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. 2008."
REFERENCES,0.29936305732484075,"S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent ﬁnds global minima of deep neural
networks. In International Conference on Machine Learning, pages 1675–1685. PMLR, 2019."
REFERENCES,0.30095541401273884,Published as a conference paper at ICLR 2022
REFERENCES,0.30254777070063693,"S. Filippi, O. Cappe, A. Garivier, and C. Szepesvári. Parametric bandits: The generalized linear case.
In Advances in Neural Information Processing Systems, pages 586–594, 2010."
REFERENCES,0.304140127388535,"D. Fu and J. He. SDG: A simpliﬁed and dynamic graph neural network. In SIGIR ’21: The 44th
International ACM SIGIR Conference on Research and Development in Information Retrieval,
Virtual Event, Canada, July 11-15, 2021, pages 2273–2277. ACM, 2021."
REFERENCES,0.3057324840764331,"F. M. Harper and J. A. Konstan. The movielens datasets: History and context. Acm transactions on
interactive intelligent systems (tiis), 5(4):1–19, 2015."
REFERENCES,0.3073248407643312,"A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in neural information processing systems, pages 8571–8580, 2018."
REFERENCES,0.3089171974522293,"J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information.
In Advances in neural information processing systems, pages 817–824, 2008."
REFERENCES,0.3105095541401274,"T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.31210191082802546,"Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The handbook
of brain theory and neural networks, 3361(10):1995, 1995."
REFERENCES,0.31369426751592355,"Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.31528662420382164,"L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news
article recommendation. In Proceedings of the 19th international conference on World wide web,
pages 661–670, 2010."
REFERENCES,0.31687898089171973,"L. Li, Y. Lu, and D. Zhou. Provably optimal algorithms for generalized linear contextual bandits. In
International Conference on Machine Learning, pages 2071–2080. PMLR, 2017."
REFERENCES,0.3184713375796178,"S. Li, A. Karatzoglou, and C. Gentile. Collaborative ﬁltering bandits. In Proceedings of the 39th
International ACM SIGIR conference on Research and Development in Information Retrieval,
pages 539–548, 2016."
REFERENCES,0.3200636942675159,"X. Lu and B. Van Roy. Ensemble sampling. arXiv preprint arXiv:1705.07347, 2017."
REFERENCES,0.321656050955414,"C. Riquelme, G. Tucker, and J. Snoek. Deep bayesian bandits showdown: An empirical comparison
of bayesian deep networks for thompson sampling. arXiv preprint arXiv:1802.09127, 2018."
REFERENCES,0.3232484076433121,"S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. science,
290(5500):2323–2326, 2000."
REFERENCES,0.3248407643312102,"W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3/4):285–294, 1933."
REFERENCES,0.32643312101910826,"M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis of kernelised
contextual bandits. arXiv preprint arXiv:1309.6869, 2013."
REFERENCES,0.32802547770700635,"Q. Wu, H. Wang, Q. Gu, and H. Wang. Contextual bandits in a collaborative environment. In
Proceedings of the 39th International ACM SIGIR conference on Research and Development in
Information Retrieval, pages 529–538, 2016."
REFERENCES,0.32961783439490444,"P. Xu, Z. Wen, H. Zhao, and Q. Gu. Neural contextual bandits with deep representation and shallow
exploration. arXiv preprint arXiv:2012.01780, 2020."
REFERENCES,0.33121019108280253,"W. Zhang, D. Zhou, L. Li, and Q. Gu. Neural thompson sampling. In International Conference on
Learning Representations, 2021."
REFERENCES,0.3328025477707006,"D. Zhou, L. Li, and Q. Gu. Neural contextual bandits with ucb-based exploration. In International
Conference on Machine Learning, pages 11492–11502. PMLR, 2020."
REFERENCES,0.3343949044585987,Published as a conference paper at ICLR 2022
REFERENCES,0.3359872611464968,"A
DATASETS AND SETUP"
REFERENCES,0.3375796178343949,"MNIST dataset. MNIST is a well-known image dataset (LeCun et al., 1998) for the 10-class
classiﬁcation problem. Following the evaluation setting of existing works (Valko et al., 2013; Zhou
et al., 2020; Zhang et al., 2021), we transform this classiﬁcation problem into bandit problem.
Consider an image x ∈Rd, we aim to classify it from 10 classes. First, in each round, the image
x is transformed into 10 arms and presented to the learner, represented by 10 vectors in sequence
x1 = (x, 0, . . . , 0), x2 = (0, x, . . . , 0), . . . , x10 = (0, 0, . . . , x) ∈R10d. The reward is deﬁned as 1
if the index of selected arm matches the index of x’s ground-truth class; Otherwise, the reward is 0."
REFERENCES,0.339171974522293,"Yelp2 and Movielens (Harper and Konstan, 2015) datasets. Yelp is a dataset released in the Yelp
dataset challenge, which consists of 4.7 million rating entries for 1.57 × 105 restaurants by 1.18
million users. MovieLens is a dataset consisting of 25 million ratings between 1.6 × 105 users
and 6 × 104 movies. We build the rating matrix by choosing the top 2000 users and top 10000
restaurants(movies) and use singular-value decomposition (SVD) to extract a 10-dimension feature
vector for each user and restaurant(movie). In these two datasets, the bandit algorithm is to choose the
restaurants(movies) with bad ratings. We generate the reward by using the restaurant(movie)’s gained
stars scored by the users. In each rating record, if the user scores a restaurant(movie) less than 2 stars
(5 stars totally), its reward is 1; Otherwise, its reward is 0. In each round, we set 10 arms as follows:
we randomly choose one with reward 1 and randomly pick the other 9 restaurants(movies) with 0
rewards; then, the representation of each arm is the concatenation of corresponding user feature
vector and restaurant(movie) feature vector."
REFERENCES,0.34076433121019106,"Disin (Ahmed et al., 2018) dataset. Disin is a fake news dataset on kaggle3 including 12600 fake
news articles and 12600 truthful news articles, where each article is represented by the text. To
transform the text into vectors, we use the approach (Fu and He, 2021) to represent each article by a
300-dimension vector. Similarly, we form a 10-arm pool in each round, where 9 real news and 1 fake
news are randomly selected. If the fake news is selected, the reward is 1; Otherwise, the reward is 0."
REFERENCES,0.34235668789808915,"Conﬁgurations. For LinUCB, following (Li et al., 2010), we do a grid search for the exploration
constant α over (0.01, 0.1, 1) which is to tune the scale of UCB. For KernelUCB (Valko et al., 2013),
we use the radial basis function kernel and stop adding contexts after 1000 rounds, following (Valko
et al., 2013; Zhou et al., 2020). For the regularization parameter λ and exploration parameter ν in
KernelUCB, we do the grid search for λ over (0.1, 1, 10) and for ν over (0.01, 0.1, 1). For NeuralUCB
and NeuralTS, following setting of (Zhou et al., 2020; Zhang et al., 2021), we use the exploiation
network f1 and conduct the grid search for the exploration parameter ν over (0.001, 0.01, 0.1, 1) and
for the regularization parameter λ over (0.01, 0.1, 1). For NeuralEpsilon, we use the same neural
network f1 and do the grid search for the exploration probability ϵ over (0.01, 0.1, 0.2). For the
neural bandits NeuralUCB/TS, following their setting, as they have expensive computation cost to
store and compute the whole gradient matrix, we use a diagonal matrix to make approximation. For
all neural networks, we conduct the grid search for learning rate over (0.01, 0.001, 0.0005, 0.0001).
For all grid-searched parameters, we choose the best of them for the comparison and report the
averaged results of 10 runs for all methods."
REFERENCES,0.34394904458598724,"Create exploration samples for f2. When the selected arm is not optimal in a round, the optimal
arm must exist among the remaining arms, and thus the exploration consideration should be added
to the remaining arms. Based on this fact, we create additional samples for the exploration network
f2 in practice. For example, in setting of binary reward, e.g., 0 or 1 reward, if the received reward
rt = 0 while select xt, we add new train samples for f2, (xt,i, cr) for each i ∈[i] ∩xt,i ̸= xt, where
cr ∈(0, 1) usually is a small constant. This measure can further improve the performance of EE-Net
in our experiments."
REFERENCES,0.34554140127388533,"B
ABLATION STUDY"
REFERENCES,0.3471337579617834,"In this section, we conduct ablation study regarding the label function y for exploration network f2
and seting of decision maker f3 on two representative datasets Movielens and Mnist."
REFERENCES,0.3487261146496815,"2https://www.yelp.com/dataset
3https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset"
REFERENCES,0.3503184713375796,Published as a conference paper at ICLR 2022
REFERENCES,0.3519108280254777,"Figure 4:
Ablation study on label function y for f2. EE-Net denotes y1 = r −f1, EE-Net-abs
denotes y2 = |r −f1|, and EE-Net-ReLU denotes y3 = ReLU(r −f1). EE-Net shows the best
performance on these two datasets."
REFERENCES,0.3535031847133758,"Figure 5: Ablation study on decision maker f3. EE-Net-Lin denotes f3 = f1 + f2, EE-Net-NoLin
denote the nonlinear one where f3 is a neural network (2 layer, width 20), EE-Net denotes the hybrid
one where f3 = f1 + f2 if t ≤500 and f3 is the neural network if t > 500. EE-Net has the most
stable and best performance."
REFERENCES,0.35509554140127386,"Label function y. In this paper, we use y1 = r −f1 to measure the potential gain of an arm, as the
label of f2. Moreover, we provide other two intuitive form y2 = |r −f1| and y3 = ReLU(r −f1).
Figure 4 shows the regret with different y, where ""EE-Net"" denotes our method with default y1,
""EE-Net-abs"" represents the one with y2 and ""EE-Net-ReLU"" is with y3. On Movielens and Mnist
datasets, EE-Net slightly outperforms EE-Net-abs and EE-Net-ReLU. In fact, y1 can effectively
represent the positive potential gain and negative potential gain, such that f2 intends to score the
arm with positive gain higher and score the arm with negative gain lower. However, y2 treats the
positive/negative potential gain evenly, weakening the discriminative ability. y3 can recognize the
positive gain while neglecting the difference of negative gain. Therefore, y1 usually is the most
effective one for empirical performance."
REFERENCES,0.35668789808917195,"Setting of f3. f3 can be set as an either linear function or non-linear function. In the experiment, we
test the simple linear function f3 = f1 + f2, denoted by ""EE-Net-Lin"", and a non-linear function
represented by a 2-layer 20-width fully-connected neural network, denoted by ""EE-Net-NoLin"". For
the default hybrid setting, denoted by ""EE-Net"", when rounds t ≤500, f3 = f1 + f2; Otherwise, f3
is the neural network. Figure 5 reports the regret with these three different modes. EE-Net achieves
the best performance with small standard deviation. In contrast, EE-Net-NoLin obtains the worst
performance and largest standard deviation. However, notice that EE-Net-NoLin can achieve the
best performance in certain running (the green shallow) but it is erratic. Because in the begin phase,
without enough training samples, EE-Net-NoLin strongly relies on the quality of collected samples.
With appropriate training samples, gradient descent can lead f3 to global optimum. On the other hand,
with misleading training samples, gradient descent can deviate f3 from global optimum. Therefore,
EE-Net-NoLin shows very unstable performance. In contrast, EE-Net-Lin is inspired by the UCB
strategy, i.e., the exploitation plus the exploration, exhibiting stable performance. To combine their"
REFERENCES,0.35828025477707004,Published as a conference paper at ICLR 2022
REFERENCES,0.35987261146496813,"advantages together, we propose the hybrid approach, EE-Net, achieving the best performance with
strong stability."
REFERENCES,0.3614649681528662,"C
PROOF OF THEOREM 1"
REFERENCES,0.3630573248407643,"In this section, we provide the proof of Theorem 1 and related lemmas."
REFERENCES,0.3646496815286624,"Proof. For brevity, for the selected arm xt in round t, let h(xt) be its expected reward
and x∗
t
=
arg maxxt,i,i∈[n] h(xt,i) be the optimal arm in round t.
Let f3(x; θt−1)
="
REFERENCES,0.3662420382165605,"f2

φ(▽θ1
t−1f1(x; θ1
t−1)); θ2
t−1

+ f1(x; θ1
t−1)."
REFERENCES,0.3678343949044586,"Note that (xt,i, rt,i) ∼D, for each i ∈[n]. Then, the expected regret of round t is given by
Rt = Ext,i,i∈[n][h(x∗
t ) −h(xt)]"
REFERENCES,0.36942675159235666,"= Ext,i,i∈[n][h(x∗
t ) −f3(xt) + f3(xt) −h(xt)]"
REFERENCES,0.37101910828025475,"≤Ext,i,i∈[n][h(x∗
t ) −f3(x∗
t ) + f3(xt) −f3(xt)
|
{z
}
I1"
REFERENCES,0.37261146496815284,+f3(xt) −h(xt)]
REFERENCES,0.37420382165605093,"= Ext,i,i∈[n][h(x∗
t ) −f3(x∗
t ) + f3(xt) −h(xt)]"
REFERENCES,0.37579617834394907,"= Ext,i,i∈[n][h(x∗
t ) −f3(x∗
t ; θt−1) + f3(xt; θt−1) −h(xt)]"
REFERENCES,0.37738853503184716,"(a)
= Ext,i,i∈[n][h(x∗
t ) −f3(x∗
t ; θ∗
t−1) + f3(x∗
t ; θ∗
t−1) −f3(x∗
t ; θt−1) + f3(xt; θt−1) −h(xt)]"
REFERENCES,0.37898089171974525,"= Ext,i,i∈[n][h(x∗
t ) −f3(x∗
t ; θ∗
t−1)|] + Ext,i,i∈[n][f3(x∗
t ; θ∗
t−1) −f3(x∗
t ; θt−1)]"
REFERENCES,0.38057324840764334,"+ Ext,i,i∈[n][f3(xt; θt−1) −h(xt)]"
REFERENCES,0.3821656050955414,"≤
E
(xt,i,rt,i),i∈[n]"
REFERENCES,0.3837579617834395,"hf2

φ(▽θ1,∗
t−1f1(x∗
t ; θ1,∗
t−1)); θ2,∗
t−1

−

r∗
t −f1(x∗
t ; θ1,∗
t−1)

i"
REFERENCES,0.3853503184713376,"|
{z
}
I2"
REFERENCES,0.3869426751592357,"+ Ext,i,i∈[n]
hf2

φ(▽θ1,∗
t−1f1(x∗
t ; θ1,∗
t−1)); θ2,∗
t−1

−f2

φ(▽θ1
t−1f1(x∗
t ; θ1
t−1)); θ2
t−1

i"
REFERENCES,0.3885350318471338,"|
{z
}
I3"
REFERENCES,0.39012738853503187,"+ Ext,i,i∈[n]
hf1(x∗
t ; θ1,∗
t−1) −f1(x∗
t ; θ1
t−1)

i"
REFERENCES,0.39171974522292996,"|
{z
}
I4"
REFERENCES,0.39331210191082805,"+
E
(xt,i,rt,i),i∈[n]"
REFERENCES,0.39490445859872614,"hf2

φ(▽θ1
t−1f1(xt; θ1
t−1)); θ2
t−1

−
 
rt −f1(xt; θ1
t−1)

i"
REFERENCES,0.3964968152866242,"|
{z
}
I5
(C.1)
where I1 is because f3(xt) = maxi∈[n] f3(xt,i) and f3(xt) −f3(x∗
t ) ≥0 and (a) introduces the
additional parameters θ∗
t−1 = (θ1,∗
t−1, θ2,∗
t−1) which will be suitably chosen."
REFERENCES,0.3980891719745223,"Because, for each i ∈[n], (xt,i, rt,i) ∼D, applying Lemma C.1 and Corollary C.1, with probability
at least (1 −δ) over the randomness of initialization, for I2, I5, we have"
REFERENCES,0.3996815286624204,"I2, I5 ≤ r 2ϵ"
REFERENCES,0.4012738853503185,"t + O
 3L
√"
T,0.4028662420382166,2t
T,0.40445859872611467,"
+ (1 + 2ξ) r"
T,0.40605095541401276,2 log(O(tn)/δ)
T,0.40764331210191085,"t
,
(C.2) where"
T,0.40923566878980894,"ξ = O(1) + O
t3nL log m ρ√m 
+ O"
T,0.410828025477707,t4nL2 log11/6 m
T,0.4124203821656051,ρ4/3m1/6 ! (C.3)
T,0.4140127388535032,"and we apply the union bound over round τ, ∀τ ∈[t] to make Lemma C.1 and Corollary C.1 hold for
each round τ, τ ∈[t]."
T,0.4156050955414013,"For I3, I4, based on Lemma C.2, with probability at least 1 −δ, we have"
T,0.4171974522292994,"I3, I4 ≤  1 + O"
T,0.41878980891719747,tL3 log5/6 m
T,0.42038216560509556,ρ1/3m1/6 !!
T,0.42197452229299365,"O
 Lt3"
T,0.42356687898089174,"ρ√m log m

+ O"
T,0.4251592356687898,t4L2 log11/6 m
T,0.4267515923566879,ρ4/3m1/6 !
T,0.428343949044586,":= ξ1.
(C.4)"
T,0.4299363057324841,Published as a conference paper at ICLR 2022
T,0.4315286624203822,"To sum up, with probability at least 1 −δ, we have Rt ≤2 r 2ϵ"
T,0.43312101910828027,"t + O
 3L
√"
T,0.43471337579617836,2t
T,0.43630573248407645,"
+ (1 + 2ξ) r"
T,0.43789808917197454,2 log(O(tn)/δ)
T,0.4394904458598726,"t
+ ξ1 !"
T,0.4410828025477707,".
(C.5)"
T,0.4426751592356688,"Then expected regret of T rounds is computed by RT = T
X"
T,0.4442675159235669,"t=1
Rt ≤2 T
X t=1 r 2ϵ"
T,0.445859872611465,"t + O
 3L
√"
T,0.44745222929936307,2t
T,0.44904458598726116,"
+ (1 + 2ξ) r"
T,0.45063694267515925,2 log(O(tn)/δ)
T,0.45222929936305734,"t
+ ξ1 ! ≤(2
√"
T,0.4538216560509554,"T −1)2
√"
T,0.4554140127388535,"2ϵ + (2
√"
T,0.4570063694267516,"T −1)3
√"
T,0.4585987261146497,"2O(L) + 2(1 + 2ξ)(2
√"
T,0.4601910828025478,"T −1)
p"
T,0.46178343949044587,"2 log(O(Tn)/δ)
|
{z
}
I2 +O(1)"
T,0.46337579617834396,"= (2
√"
T,0.46496815286624205,"T −1)(2
√"
T,0.46656050955414013,"2ϵ + 3
√"
T,0.4681528662420382,"2O(L)) + 2(1 + 2ξ)(2
√"
T,0.4697452229299363,"T −1)
p"
T,0.4713375796178344,"2 log(O(Tn)/δ) + O(1)
(C.6)
where I2 is because PT
t=1
1
√"
T,0.4729299363057325,"t ≤
R T
1
1
√"
T,0.4745222929936306,"t dx + 1 = 2
√"
T,0.47611464968152867,"T −1 (Chlebus, 2009) and the bound of ξ1 is"
T,0.47770700636942676,"due to the choice of m, i.e., since ξ1 = e
O(1/m1/6) and m ≥eΩ(poly(T)), m can be chosen so that
Tξ1 = e
O(T/m1/6) ≤O(1)."
T,0.47929936305732485,"Then, when ϵ ≤1/T, we have"
T,0.48089171974522293,"RT ≤O(1) + (2
√"
T,0.482484076433121,"T −1)3
√"
T,0.4840764331210191,"2O(L) + 2(1 + 2ξ)(2
√"
T,0.4856687898089172,"T −1)
p"
T,0.4872611464968153,"2 log(O(Tn)/δ).
(C.7)"
T,0.4888535031847134,"As the choice of m, we have ξ ≤O(1). Therefore, we have"
T,0.49044585987261147,"RT ≤O(1) + (2
√"
T,0.49203821656050956,"T −1)3
√"
T,0.49363057324840764,"2O(L) + O

(2
√"
T,0.49522292993630573,"T −1)
p"
T,0.4968152866242038,"2 log(O(Tn)/δ)

.
(C.8)"
T,0.4984076433121019,The proof is completed.
T,0.5,"Lemma C.1. [Lemma 5.1 restated] For any δ, ϵ ∈(0, 1), ρ ∈(0, O( 1"
T,0.5015923566878981,"L)), suppose m, η1, η2, K1, K2
satisfy the conditions in Eq. (5.2) and (xτ,i, rτ,i) ∼D, ∀τ ∈[t], i ∈[n]. Let"
T,0.5031847133757962,"xt = arg
max
xt,i,i∈[n]"
T,0.5047770700636943,"h
f2

φ(▽θ1
t−1f1(xt,i; θ1
t−1)); θ2
t−1

+ f1(xt,i; θ1
t−1)
i
,"
T,0.5063694267515924,"and rt is the corresponding reward, given (xt,i, rt,i), i ∈[n]. Then, with probability at least (1 −δ)
over the random of the initialization, it holds that"
T,0.5079617834394905,"E
(xt,i,rt,i),i∈[n]"
T,0.5095541401273885,"hf2

φ(▽θ1
t−1f1(xt; θ1
t−1)); θ2
t−1

−
 
rt −f1(xt; θ1
t−1)
 | {xτ, rτ}t−1
τ=1
i ≤ r 2ϵ"
T,0.5111464968152867,"t + O
 3L
√"
T,0.5127388535031847,2t
T,0.5143312101910829,"
+ (1 + 2ξ) r"
T,0.5159235668789809,2 log(O(tn/δ))
T,0.517515923566879,"t
,
(C.9)"
T,0.5191082802547771,"where the expectation is also taken over (θ1
t−1, θ2
t−1) that are uniformly drawn from (bθ
1
τ, bθ
2
τ), τ ∈
[t −1]."
T,0.5207006369426752,"Proof. In this proof, we consider the collected data of up to round t −1, {xτ, rτ}t−1
τ=1, as the training
dataset and then obtain a generalization bound for it, inspired by Cao and Gu (2019)."
T,0.5222929936305732,"For convenience, we use x := xt,i, r := rt,i, noting that the same analysis holds for each i ∈[n].
Consider the exploration network f2, applying Lemma C.3. With probability at least 1 −δ, for any
τ ∈[t], we have"
T,0.5238853503184714,"f2

φ(▽bθ
1
τ f1(x; bθ
1
τ)); bθ
2
τ
 ≤ξ.
(C.10)"
T,0.5254777070063694,"Similarly, applying Lemma C.3 again, with probability at least 1 −δ, for any t ∈[T], we have"
T,0.5270700636942676,"|f1(x; bθ
1
τ)| ≤ξ
(C.11)"
T,0.5286624203821656,Published as a conference paper at ICLR 2022
T,0.5302547770700637,"Because for any r ∼Dr, |r| ≤1, with Eq. (C.10) and (C.11), applying union bound, with probability
at least (1 −2δ) over the random initialization, we have"
T,0.5318471337579618,"f2

φ(▽bθ
1
τ f1(x; bθ
1
τ)); bθ
2
τ

−(r −f1(x; bθ
1
τ))
 ≤1 + 2ξ.
(C.12)"
T,0.5334394904458599,"Noting that (C.12) is for x = xτ,i for a speciﬁc τ ∈[t], i ∈[n].
By union bound, (C.12)"
T,0.535031847133758,"holds ∀τ ∈[t], i ∈[n] with probability at least (1 −ntδ). For brevity, let f2(x; bθ
2
τ) represent"
T,0.5366242038216561,"f2

φ(▽bθ
1
τ f1(x; bθ
1
τ)); bθ
2
τ

."
T,0.5382165605095541,"Recall that, for each τ ∈[t −1], bθ
1
τ and bθ
2
τ are the parameters training on {xτ ′, rτ ′}τ
τ ′=1 according
to Algorithm 1. In round τ ∈[t], let xτ = arg maxxτ,i,i∈[n][f1(xτ,i; θ1
τ−1) + f2(xτ,i; θ2
τ−1)], given
(xτ,i, rτ,i) ∼D, i ∈[n]. Let rτ be the corresponding reward. Let (x′
τ,i, r′
τ,i) ∼D, i ∈[n] be shadow
samples from the same distribution and let x′
τ = arg maxx′
τ,i,i∈[n][f1(x′
τ,i; θ1
τ−1) + f2(x′
τ,i; θ2
τ−1)],
with r′
τ being the corresponding reward. Then, we deﬁne"
T,0.5398089171974523,"Vτ :=
E
(x′
τ,i,r′
τ,i),i∈[n]"
T,0.5414012738853503,"hf2(x′
τ; bθ
2
τ−1) −

r′
τ −f1(x′
τ; bθ
1
τ−1)

i"
T,0.5429936305732485,"−
f2(xτ; bθ
2
τ−1) −

rτ −f1(xτ; bθ
1
τ−1)
 .
(C.13)"
T,0.5445859872611465,"Then, as (xτ,i, rτ,i) ∼D, i ∈[n], based on the deﬁnition of (xτ, rτ), we have"
T,0.5461783439490446,"E[Vτ|Fτ−1] =
E
(x′
τ,i,r′
τ,i),i∈[n]"
T,0.5477707006369427,"hf2

x′
τ; bθ
2
τ−1

−

r′
τ −f1(x′
τ; bθ
1
τ−1)
 | Fτ−1
i"
T,0.5493630573248408,"−
E
(xτ,i,rτ,i),i∈[n]"
T,0.5509554140127388,"hf2

xτ; bθ
2
τ−1

−

rτ −f1(xτ; bθ
1
τ−1)
 |Fτ−1
i = 0 ,"
T,0.552547770700637,(C.14)
T,0.554140127388535,"where Fτ−1 denotes the σ-algebra generated by the history Hτ−1 = {xτ ′, rτ ′}τ−1
τ ′=1."
T,0.5557324840764332,"Moreover, we have 1 t t
X"
T,0.5573248407643312,"τ=1
Vτ = 1 t t
X"
T,0.5589171974522293,"τ=1
E
(x′
τ,i,r′
τ,i),i∈[n]"
T,0.5605095541401274,"hf2(x′
τ; bθ
2
τ−1) −

r′
τ −f1(x′
τ; bθ
1
τ−1)

i −1 t t
X τ=1"
T,0.5621019108280255,"f2

xτ; bθ
2
τ−1

−

rτ −f1(xτ; bθ
1
τ−1)
"
T,0.5636942675159236,"Since {Vτ}t
τ=1 is a martingale difference sequence, inspired by Lemma 1 in (Cesa-Bianchi et al.,
2001), applying the Hoeffding-Azuma inequality, with probability at least 1 −3δ, we have P   1 t t
X"
T,0.5652866242038217,"τ=1
Vτ −1 t t
X"
T,0.5668789808917197,"τ=1
E[Vτ|Fτ]"
T,0.5684713375796179,"|
{z
}
I1"
T,0.5700636942675159,"> (1 + 2ξ)
|
{z
}
I2 r"
T,0.571656050955414,2 log(1/δ) t 
T,0.5732484076433121,"
≤δ ⇒
P ""
1 t t
X"
T,0.5748407643312102,"τ=1
Vτ > (1 + 2ξ) r"
T,0.5764331210191083,"2 log(1/δ) t # ≤δ ,"
T,0.5780254777070064,(C.15)
T,0.5796178343949044,where I1 = 0 according to (C.14) and I2 is because of (C.12).
T,0.5812101910828026,Published as a conference paper at ICLR 2022
T,0.5828025477707006,"According to Algorithm 1, (θ1
t−1, θ2
t−1) is uniformly drawn from {(bθ
1
τ, bθ
2
τ)}t−1
τ=0. Thus, with proba-
bility 1 −3δ, we have"
T,0.5843949044585988,"E
(x′
t,i,r′
t,i),i∈[n]
E
(θ1
t−1,θ2
t−1)"
T,0.5859872611464968,"f2
 
x′
t; θ2
t−1

−
 
r′
t −f1(x′
t; θ1
t−1)
 = 1 t t
X"
T,0.5875796178343949,"τ=1
E(x′
τ,i,r′
τ,i),i∈[n]
hf2

x′
τ; bθ
2
τ−1

−

r′
τ −f1(x′
τ; bθ
1
τ−1)

i ≤1 t t
X τ=1"
T,0.589171974522293,"f2

xτ; bθ
2
τ−1

−

rτ −f1(xτ; bθ
1
τ−1)
"
T,0.5907643312101911,"|
{z
}
I3"
T,0.5923566878980892,+(1 + 2ξ) r
T,0.5939490445859873,"2 log(1/δ) t
."
T,0.5955414012738853,(C.16)
T,0.5971337579617835,"For I3, according to Lemma C.6, for any eθ
2 satisfying ∥eθ
2−θ2
0∥2 ≤O(
t3"
T,0.5987261146496815,"ρ√m log m) , with probability
1 −δ, we have 1 t t
X τ=1"
T,0.6003184713375797,"f2(xτ; bθ
2
τ−1) −

rτ −f1(xτ; bθ
1
τ−1)
 ≤1 t t
X τ=1"
T,0.6019108280254777,"f2(xτ; eθ
2) −

rτ −f1(xτ; bθ
1
τ−1)
"
T,0.6035031847133758,"|
{z
}
I4"
T,0.6050955414012739,"+O
 3L
√"
T,0.606687898089172,2t
T,0.60828025477707,"
.
(C.17)"
T,0.6098726114649682,"For I4, according to Lemma C.4 (1), these exists eθ
2 satisfying ∥eθ
2 −θ2
0∥2 ≤O(
t3"
T,0.6114649681528662,"ρ√m log m), with
probability 1 −δ, such that 1 t t
X τ=1"
T,0.6130573248407644,"f2(xτ; eθ
2) −

rτ −f1(xτ; bθ
1
τ−1)
 ≤1 t √ t"
T,0.6146496815286624,"v
u
u
u
u
u
t t
X τ=1"
T,0.6162420382165605,"
f2(xτ; eθ
2) −

rτ −f1(xτ; bθ
1
τ−1)
2"
T,0.6178343949044586,"|
{z
}
I5 ≤1
√ t"
T,0.6194267515923567,"r 2ϵ
|{z}
I5
,"
T,0.6210191082802548,(C.18)
T,0.6226114649681529,"where I5 follows by a direct application of Lemma C.4 (1) by deﬁning the loss L(eθ
2) ="
T,0.6242038216560509,"1
2
Pt
τ=1

f2(xτ; eθ
2) −

rτ −f1(xτ; bθ
1
τ−1)
2
≤ϵ."
T,0.6257961783439491,"Combining Eq.(C.16), Eq.(C.17) and Eq.(C.18), with probability (1 −5δ) we have"
T,0.6273885350318471,"E
(xt,i,rt,i),i∈[n]"
T,0.6289808917197452,"f2
 
xt; θ2
t−1

−
 
rt −f1(xt; θ1
t−1)
 |{xτ, rτ}t−1
τ=1
 ≤ r 2ϵ"
T,0.6305732484076433,"t + O
 3L
√"
T,0.6321656050955414,2t
T,0.6337579617834395,"
+ (1 + 2ξ) r"
T,0.6353503184713376,2 log(1/δ)
T,0.6369426751592356,"t
.
(C.19)"
T,0.6385350318471338,"where the expectation over (θ1
t−1, θ2
t−1) that is uniformly drawn from {(bθ
1
τ, bθ
2
τ)}t−1
τ=0."
T,0.6401273885350318,"Then, applying union bound to t, n and rescaling the δ complete the proof."
T,0.64171974522293,"Corollary C.1. For any δ, ϵ ∈(0, 1), ρ ∈(0, O( 1"
T,0.643312101910828,"L)), suppose m, η1, η2, K1, K2 satisfy the condi-
tions in Eq. (5.2) and (xτ,i, rτ,i) ∼D, ∀τ ∈[t], i ∈[n]. For any τ ∈[t], let"
T,0.6449044585987261,"x∗
τ = arg
max
xτ,i,i∈[n] [h(xτ,i)] ,"
T,0.6464968152866242,Published as a conference paper at ICLR 2022
T,0.6480891719745223,"and r∗
τ is the corresponding reward, given (xτ,i, rτ,i), i ∈[n]. Then, with probability at least (1 −δ)
over the random of the initialization, there exist θ1,∗
t−1, θ2,∗
t−1, s.t., ∥θ1,∗
t−1 −θ1
0∥2 ≤O(
t3"
T,0.6496815286624203,ρ√m log m)
T,0.6512738853503185,"and ∥θ2,∗
t−1 −θ2
0∥2 ≤O(
t3"
T,0.6528662420382165,"ρ√m log m) , such that"
T,0.6544585987261147,"E
(xt,i,rt,i),i∈[n]"
T,0.6560509554140127,"hf2

φ(▽θ1,∗
t−1f1(x∗
t ; θ1,∗
t−1)); θ2,∗
t−1

−

r∗
t −f1(x∗
t ; θ1,∗
t−1)
 | {x∗
τ, r∗
τ}t−1
τ=1
i ≤ r 2ϵ"
T,0.6576433121019108,"t + O
 3L
√"
T,0.6592356687898089,2t
T,0.660828025477707,"
+ (1 + 2ξ) r"
T,0.6624203821656051,"2 log(O(tn/δ)) t
,"
T,0.6640127388535032,"(C.20)
where the expectation is also taken over (θ1,∗
t−1, θ2,∗
t−1) that are uniformly drawn from (bθ
1,∗
τ , bθ
2,∗
τ ), τ ∈
[t −1]."
T,0.6656050955414012,"Proof. This a direct corollary of Lemma C.1, given the optimal historical pairs {x∗
τ, r∗
τ}t−1
τ=1. For"
T,0.6671974522292994,"brevity, let f2(x; bθ
2,∗
τ ) represent f2

φ(▽bθ
1,∗
τ f1(x; bθ
1,∗
τ )); bθ
2,∗
τ

."
T,0.6687898089171974,"Suppose that, for each τ ∈[t −1], bθ
1,∗
τ
and bθ
2,∗
τ
are the parameters training on {x∗
τ ′, r∗
τ ′}τ
τ ′=1
according to Algorithm 1. Note that these pairs {x∗
τ ′, r∗
τ ′}τ
τ ′=1 are unknown to the algorithm we run,"
T,0.6703821656050956,"and the parameters (bθ
1,∗
τ , bθ
2,∗
τ ) are not estimated. However, for the analysis, it is sufﬁcient to show
that there exist such parameters so that the conditional expectation of the error can be bounded."
T,0.6719745222929936,"In round τ ∈[t], let x∗
τ = arg maxxτ,ii∈[n][h(xτ,i)], given (xτ,i, rτ,i) ∼D, i ∈[n]. Let r∗
τ be the
corresponding reward. Let (x′
τ,i, r′
τ,i) ∼D, i ∈[n] be shadow samples from the same distribution
and let x′∗
τ = arg maxx′
τ,i,i∈[n] h(x′
τ,i), with r′∗
τ being the corresponding reward. Then, we deﬁne"
T,0.6735668789808917,"Vτ :=
E
(x′
t,i,r′
t,i),i∈[n]"
T,0.6751592356687898,"hf2(x′∗
τ ; bθ
2,∗
τ−1) −

r′∗
τ −f1(x′∗
τ ; bθ
1,∗
τ−1)

i"
T,0.6767515923566879,"−
f2(x∗
τ; bθ
2,∗
τ−1) −

r∗
τ −f1(x∗
τ; bθ
1,∗
τ−1)
 .
(C.21)"
T,0.678343949044586,"Then, as (xτ,i, rτ,i) ∼D, i ∈[n], we have"
T,0.6799363057324841,"E[Vτ|Fτ−1] =
E
(x′
τ,i,r′
τ,i),i∈[n]"
T,0.6815286624203821,"hf2

x′∗
τ ; bθ
2,∗
τ−1

−

r′∗
τ −f1(x′∗
τ ; bθ
1,∗
τ−1)
 | Fτ−1
i"
T,0.6831210191082803,"−
E
(xτ,i,rτ,i),i∈[n]"
T,0.6847133757961783,"hf2

x∗
τ; bθ
2,∗
τ−1

−

r∗
τ −f1(x∗
τ; bθ
1,∗
τ−1)
 | Fτ−1
i = 0 ,"
T,0.6863057324840764,(C.22)
T,0.6878980891719745,"where Fτ−1 denotes the σ-algebra generated by the history {x∗
τ ′, r∗
τ ′}τ−1
τ ′=1."
T,0.6894904458598726,"Therefore, {Vτ}t
τ=1 is a martingale difference sequence. Similarly, applying the Hoeffding-Azuma
inequality to Vτ, with probability 1 −3δ, we have"
T,0.6910828025477707,"E
(x′
t,i,r′
t,i),i∈[n]
E
(θ1,∗
t−1,θ2,∗
t−1)"
T,0.6926751592356688,"hf2

x′∗
t ; θ2,∗
t−1

−

r′∗
t −f1(x′∗
t ; θ1,∗
t−1)

i = 1 t t
X"
T,0.6942675159235668,"τ=1
E(x′
τ,i,r′
τ,i),i∈[n]
hf2

x′∗
τ ; bθ
2,∗
τ−1

−

r′∗
τ −f1(x′∗
τ ; bθ
1,∗
τ−1)

i ≤1 t t
X τ=1"
T,0.695859872611465,"f2

x∗
τ; bθ
2,∗
τ−1

−

r∗
τ −f1(x∗
τ; bθ
1,∗
τ−1)
"
T,0.697452229299363,"|
{z
}
I3"
T,0.6990445859872612,+(1 + 2ξ) r
T,0.7006369426751592,"2 log(1/δ) t
."
T,0.7022292993630573,(C.23)
T,0.7038216560509554,Published as a conference paper at ICLR 2022
T,0.7054140127388535,"For I3, according to Lemma C.6, for any eθ
2,∗satisfying ∥eθ
2,∗−θ2
0∥2 ≤O(
t3"
T,0.7070063694267515,"ρ√m log m) , with
probability 1 −δ, we have 1 t t
X τ=1"
T,0.7085987261146497,"f2(x∗
τ; bθ
2,∗
τ−1) −

r∗
τ −f1(x∗
τ; bθ
1,∗
τ−1)
 ≤1 t t
X τ=1"
T,0.7101910828025477,"f2(x∗
τ; eθ
2,∗) −

r∗
τ −f1(x∗
τ; bθ
1,∗
τ−1)
"
T,0.7117834394904459,"|
{z
}
I4"
T,0.7133757961783439,"+O
 3L
√"
T,0.714968152866242,2t
T,0.7165605095541401,"
.
(C.24)"
T,0.7181528662420382,"For I4, according to Lemma C.4 (1), these exists eθ
2,∗satisfying ∥eθ
2,∗−θ2
0∥2 ≤O(
t3"
T,0.7197452229299363,"ρ√m log m),
with probability 1 −δ, such that 1 t t
X τ=1"
T,0.7213375796178344,"f2(x∗
τ; eθ
2,∗) −

r∗
τ −f1(x∗
τ; bθ
1,∗
τ−1)
 ≤1 t √ t"
T,0.7229299363057324,"v
u
u
u
u
u
t t
X τ=1"
T,0.7245222929936306,"
f2(x∗
τ; eθ
2,∗) −

r∗
τ −f1(x∗
τ; bθ
1,∗
τ−1)
2"
T,0.7261146496815286,"|
{z
}
I5 ≤1
√ t"
T,0.7277070063694268,"r 2ϵ
|{z}
I5
,"
T,0.7292993630573248,(C.25)
T,0.7308917197452229,"where I5 follows by a direct application of Lemma C.4 (1) by deﬁning the loss L(eθ
2,∗) ="
T,0.732484076433121,"1
2
Pt
τ=1

f2(x∗
τ; eθ
2,∗) −

r∗
τ −f1(x∗
τ; bθ
1,∗
τ−1)
2
≤ϵ. Combining above inequalities, with proba-"
T,0.7340764331210191,bility (1 −5δ) we have
T,0.7356687898089171,"E
(xt,i,rt,i),i∈[n]"
T,0.7372611464968153,"hf2

x∗
t ; θ2,∗
t−1

−

r∗
t −f1(x∗
t ; θ1,∗
t−1)
 |{x∗
τ, r∗
τ}t−1
τ=1
i ≤ r 2ϵ"
T,0.7388535031847133,"t + O
 3L
√"
T,0.7404458598726115,2t
T,0.7420382165605095,"
+ (1 + 2ξ) r"
T,0.7436305732484076,2 log(1/δ)
T,0.7452229299363057,"t
.
(C.26)"
T,0.7468152866242038,"Then, applying union bound to t, n and rescaling the δ complete the proof."
T,0.7484076433121019,"Lemma C.2. Given δ, ϵ ∈(0, 1), ρ ∈(0, O( 1"
T,0.75,"L)), suppose m, η1, η2, K1, K2 satisfy the conditions
in Eq. (5.2). Then, with probability at least 1 −δ, in each round t ∈[T], for any ∥x∥2 = 1, we have"
T,0.7515923566878981,"(1)
|f1(x; θ1,∗
t−1) −f1(x; θ1
t−1)| ≤  1 + O"
T,0.7531847133757962,tL3 log5/6 m
T,0.7547770700636943,ρ1/3m1/6 !!
T,0.7563694267515924,"O
 Lt3"
T,0.7579617834394905,"ρ√m log m

+ O"
T,0.7595541401273885,t4L2 log11/6 m
T,0.7611464968152867,ρ4/3m1/6 !
T,0.7627388535031847,";
(C.27)"
T,0.7643312101910829,"(2)
f2

φ(▽θ1,∗
t−1f1(x; θ1,∗
t−1)); θ2,∗
t−1

−f2

φ(▽θ1
t−1f1(x; θ1
t−1)); θ2
t−1
 ≤  1 + O"
T,0.7659235668789809,tL3 log5/6 m
T,0.767515923566879,ρ1/3m1/6 !!
T,0.7691082802547771,"O
 Lt3"
T,0.7707006369426752,"ρ√m log m

+ O"
T,0.7722929936305732,t4L2 log11/6 m
T,0.7738853503184714,ρ4/3m1/6 !
T,0.7754777070063694,";
(C.28)"
T,0.7770700636942676,"(3)
∥▽θ1
t−1f1(x; θ1
t−1)∥2, ∥▽θ2
t−1f2

φ(▽θ1
t−1f1(x; θ1
t−1)); θ2
t−1

∥2 ≤  1 + O"
T,0.7786624203821656,tL3 log5/6 m
T,0.7802547770700637,ρ1/3m1/6 !!
T,0.7818471337579618,"O(L) .
(C.29)"
T,0.7834394904458599,Published as a conference paper at ICLR 2022
T,0.785031847133758,"Proof. According to Lemma C.4 (2), ∥bθ
1
τ−1 −θ1
0∥2 ≤O(
t3"
T,0.7866242038216561,"ρ√m log m), ∀τ ∈[t]. Thus, we have"
T,0.7882165605095541,"∥θ1
t−1 −θ1
0∥2 ≤O(
t3"
T,0.7898089171974523,ρ√m log m).
T,0.7914012738853503,"First, based on Triangle inequality, for any ∥x∥2 = 1, we have"
T,0.7929936305732485,"∥▽θ1
t−1f1(x; θ1
t−1)∥2 ≤∥▽θ1
0f1(x; θ1
0)∥2 + ∥▽θ1
t−1f1(x; θ1
t−1) −▽θ1
0f1(xi; θ1
0)∥2 ≤  1 + O"
T,0.7945859872611465,tL3 log5/6 m
T,0.7961783439490446,ρ1/3m1/6 !!
T,0.7977707006369427,"O(L)
(C.30)"
T,0.7993630573248408,where the last inequality is because of Lemma C.4 (3) and Lemma C.7.
T,0.8009554140127388,"Applying Lemma C.5 (1), for any x ∼D, ∥x∥2 = 1 and ∥θ1,∗
t−1 −θ1
t−1∥≤O(
t3"
T,0.802547770700637,"ρ√m log m) = w, we
have"
T,0.804140127388535,"|f1(x; θ1,∗
t−1) −f1(x; θ1
t−1)|"
T,0.8057324840764332,"≤|⟨▽θ1
t−1f1(xi; θ1
t−1), θ1,∗
t−1 −θ1
t−1⟩| + O(L2p"
T,0.8073248407643312,"m log(m))∥θ1,∗
t−1 −θ1
t−1∥2w1/3"
T,0.8089171974522293,"≤∥▽θ1
t−1f1(xi; θ1
t−1)∥2∥θ1,∗
t−1 −θ1
t−1∥2 + O(L2p"
T,0.8105095541401274,"m log(m))∥θ1,∗
t−1 −θ1
t−1∥2w1/3 ≤  1 + O"
T,0.8121019108280255,tL3 log5/6 m
T,0.8136942675159236,ρ1/3m1/6 !!
T,0.8152866242038217,O( Lt3
T,0.8168789808917197,ρ√m log m) + O
T,0.8184713375796179,t4L2 log11/6 m
T,0.8200636942675159,ρ4/3m1/6
T,0.821656050955414,"!
(C.31)"
T,0.8232484076433121,"Similarly, we can use the same way to prove the lemmas for f2."
T,0.8248407643312102,"Lemma C.3. Let f(·; bθt) follow the stochastic gradient descent of f1 or f2 in Algorithm 1. Suppose
m, η1, η2 satisfy the conditions in Eq. (5.2). With probability at least 1 −δ, for any x with ∥x∥2 = 1
and t ∈[T], it holds that"
T,0.8264331210191083,"|f(x; bθt)| ≤O(1) + O
t3nL log m ρ√m 
+ O"
T,0.8280254777070064,t4nL2 log11/6 m
T,0.8296178343949044,ρ4/3m1/6 ! .
T,0.8312101910828026,"Proof. Considering an inequality |a −b| ≤c, we have |a| ≤|b| + c. Let θ0 be randomly initialized.
Then applying Lemma C.5 (1), for any x ∼D, ∥x∥2 = 1 and ∥bθt −θ0∥≤w, we have"
T,0.8328025477707006,"|f(x; bθt)| ≤|f(x; θ0)| + |⟨▽θ0f(xi; θ0), bθt −θ0⟩| + O(L2p"
T,0.8343949044585988,m log(m))∥bθt −θ0∥2w1/3
T,0.8359872611464968,"≤O(1)
| {z }
I0"
T,0.8375796178343949,"+ ∥▽θ0f(xi; θ0)∥2∥bθt −θ0∥2
|
{z
}
I1"
T,0.839171974522293,+O(L2p
T,0.8407643312101911,m log(m))∥bθt −θ0∥2w1/3
T,0.8423566878980892,"≤O(1) + O(L) · O

t3"
T,0.8439490445859873,"ρ√m log m
"
T,0.8455414012738853,"|
{z
}
I2"
T,0.8471337579617835,"+ O

L2p"
T,0.8487261146496815,"m log(m)

· O

t3"
T,0.8503184713375797,"ρ√m log m
4/3"
T,0.8519108280254777,"|
{z
}
I3"
T,0.8535031847133758,"= O(1) + O
t3L log m ρ√m 
+ O"
T,0.8550955414012739,t4L2 log11/6 m
T,0.856687898089172,ρ4/3m1/6 !
T,0.85828025477707,"(C.32)
where: I0 is based on the Lemma C.4 (3); I1 is an application of Cauchy–Schwarz inequality; I2 is
according to Lemma C.4 (2) and (3) in which bθt can be considered as one step gradient descent; I3 is
due to Lemma C.4 (2)."
T,0.8598726114649682,"Then, the proof is completed."
T,0.8614649681528662,"Lemma C.4. Given a constant 0 < ϵ < 1, suppose m satisﬁes the conditions in Eq. (5.2), the
learning rate η = Ω(
ρ
poly(t,n,L)m), the number of iterations K = Ω( poly(t,n,L)"
T,0.8630573248407644,"ρ2
· log ϵ−1). Then, with
probability at least 1 −δ, starting from random initialization θ0,"
T,0.8646496815286624,"(1) (Theorem 1 in (Allen-Zhu et al., 2019)) In round t ∈[T], given the collected data
{xτ, rτ}t
i=τ, the loss function is deﬁned as: L(θ) =
1
2
Pt
τ=1 (f(xτ; θ) −rτ)2. Then,"
T,0.8662420382165605,Published as a conference paper at ICLR 2022
T,0.8678343949044586,"there exists eθ satisfying ∥eθ −θ0∥2
≤O

t3"
T,0.8694267515923567,"ρ√m log m

, such that L(eθ) ≤ϵ in"
T,0.8710191082802548,"K = Ω( poly(t,n,L)"
T,0.8726114649681529,"ρ2
· log ϵ−1) iterations;"
T,0.8742038216560509,"(2) (Theorem 1 in (Allen-Zhu et al., 2019)) For any k ∈[K], it holds uniformly that ∥θ(k)
t
−"
T,0.8757961783439491,"θ0∥2 ≤O

t3"
T,0.8773885350318471,"ρ√m log m

;"
T,0.8789808917197452,"(3) Following the initialization, given ∥x∥2 = 1, it holds that"
T,0.8805732484076433,"∥▽θ0f(x; θ0)∥2 ≤O(L),
|f(x; θ0)| ≤O(1)"
T,0.8821656050955414,"where θ(k)
t
represents the parameters of f after k ∈[K] iterations of gradient descent in round t."
T,0.8837579617834395,"Proof. Note that the output dimension d in (Allen-Zhu et al., 2019) is removed because the output
of network function in this paper always is a scalar. For (1) and (2), the only different setting from
(Allen-Zhu et al., 2019) is that the initialization of last layer WL ∼N(0, 2"
T,0.8853503184713376,"m) in this paper while
WL ∼N(0, 1"
T,0.8869426751592356,"d) in (Allen-Zhu et al., 2019). Because d = 1 and m > d here, the upper bound in
(Allen-Zhu et al., 2019) still holds for WL: with probability at least 1−exp (−Ω(m/L)), ∥WL∥F ≤
p"
T,0.8885350318471338,"m/d. Therefore, (1) and (2) still hold for the initialization of this paper."
T,0.8901273885350318,"For (3), based on Lemma 7.1 in Allen-Zhu et al. (2019), we have |f(x; θ0)| ≤O(1). Denote by D
the ReLU function. For any l ∈[L],"
T,0.89171974522293,"∥▽Wlf(x; θ0)∥F ≤∥WLDWL−1 · · · DWl+1∥F · ∥DWl+1 · · · x∥F ≤O(
√ L)"
T,0.893312101910828,"where the inequality is according to Lemma 7.2 in Allen-Zhu et al. (2019). Therefore, we have
∥▽θ0f(x; θ0)∥2 ≤O(L)."
T,0.8949044585987261,"Lemma C.5 (Lemma 4.1, (Cao and Gu, 2019)). For any δ ∈(0, 1), if w satisﬁes"
T,0.8964968152866242,"O(m−3/2L−3/2[log(tnL2/δ)]3/2) ≤w ≤O(L−6[log m]−3/2),"
T,0.8980891719745223,"then, with probability at least 1 −δ over randomness of θ0, for any t ∈[T], ∥x∥2 = 1, and θ, θ′"
T,0.8996815286624203,"satisfying ∥θ −θ0∥2 ≤w and ∥θ′ −θ0∥2 ≤w , it holds uniformly that"
T,0.9012738853503185,"|f(xi; θ) −f(xi; θ′) −⟨▽θ′f(xi; θ′), θ −θ′⟩| ≤O(w1/3L2p"
T,0.9028662420382165,"m log(m))∥θ −θ′∥2.
(C.33)"
T,0.9044585987261147,"Lemma C.6. For any δ > 0, suppose"
T,0.9060509554140127,"m > ˜O

poly(T, n, ρ−1, L, log(1/δ) · e
√"
T,0.9076433121019108,"log 1/δ)

."
T,0.9092356687898089,"Then, with probability at least 1 −δ, setting η2 = Θ(
t5 δ2√"
T,0.910828025477707,"2m) for algorithm 1, for any eθ
2 satisfying"
T,0.9124203821656051,"∥eθ
2 −θ2
0∥2 ≤O(
t3"
T,0.9140127388535032,"ρ√m log m), it holds that t
X τ=1"
T,0.9156050955414012,"f2

φ(▽bθ
1
τ−1f1(xτ; bθ
1
τ−1)); bθ
2
τ−1

−

rτ −f1(xτ; bθ
1
τ−1)
 ≤ t
X τ=1"
T,0.9171974522292994,"f2

φ(▽bθ
1
τ−1f1(xτ; bθ
1
τ−1)); eθ
2
−

rτ −f1(xτ; bθ
1
τ−1)
 + O
3L
√ t
√ 2 "
T,0.9187898089171974,"Proof. This is a direct application of Lemma 4.3 in (Cao and Gu, 2019) by setting R = t3"
T,0.9203821656050956,"ρ log m, ϵ = LR
√"
T,0.9219745222929936,"2νt, and ν = ν′R2, where ν′ is some small enough absolute constant. We set Lτ(bθ
2
τ−1) =
f2(▽bθ
1
τ−1f1; bθ
2
τ−1) −

rτ −f1(xτ; bθ
1
τ−1)
. Based on Lemma C.4 (2), for any τ ∈[t], we have"
T,0.9235668789808917,"∥bθ
2
τ −bθ
2
τ−1∥2 ≤∥bθ
2
τ −θ0∥2 + ∥θ0 −bθ
2
τ−1∥2 ≤O(
t3"
T,0.9251592356687898,ρ√m log m).
T,0.9267515923566879,Published as a conference paper at ICLR 2022
T,0.928343949044586,"Table 2: Selection Criterion Comparison (xt: selected arm in round t).
Methods
Selection Criterion"
T,0.9299363057324841,"Neural Epsilon-greedy
With probability 1 −ϵ, xt = arg maxxt,i,i∈[n] f1(xt,i; θ1);
Otherwise, select xt randomly."
T,0.9315286624203821,"NeuralTS (Zhang et al., 2021)
For xt,i, ∀i ∈[n], draw ˆrt,i from N(f1(xt,i; θ1), σt,i2). Then,
select xt,ˆi, ˆi = arg maxi∈[n] ˆrt,i."
T,0.9331210191082803,"NeuralUCB (Zhou et al., 2020)
xt = arg maxxt,i,i∈[n]
 
f1(xt,i; θ1) + UCBt,i

."
T,0.9347133757961783,"EE-Net (Our approach)
∀i ∈[n], compute f1(xt,i; θ1), f2
 
▽θ1f1(xt,i; θ1); θ2
(Ex-
ploration Net). Then xt = arg maxxt,ii∈[n] f3(f1, f2; θ3)."
T,0.9363057324840764,"Then, according to Lemma 4.3 in (Cao and Gu, 2019), then, for any eθ
2 satisfying ∥eθ
2 −θ2
0∥2 ≤
O(
t3"
T,0.9378980891719745,"ρ√m log m), there exist a small enough absolute constant ν′, such that t
X"
T,0.9394904458598726,"τ=1
Lτ(bθ
2
τ−1) ≤ t
X"
T,0.9410828025477707,"τ=1
Lτ(eθ
2) + 3tϵ.
(C.34)"
T,0.9426751592356688,"Then, replacing ϵ completes the proof."
T,0.9442675159235668,"Lemma C.7 (Theorem 5, Allen-Zhu et al. (2019)). For any δ ∈(0, 1), if w satisﬁes that"
T,0.945859872611465,"O(m−3/2L−3/2 max{log−3/2 m, log3/2(Tn/δ)}) ≤w ≤O(L−9/2 log−3 m),
(C.35)"
T,0.947452229299363,"then, with probability at least 1 −δ, for all ∥θ −θ0∥2 ≤w, we have"
T,0.9490445859872612,"∥▽θf(x; θ) −▽θ0f(x; θ0)∥2 ≤O(
p"
T,0.9506369426751592,"log mw1/3L3)∥▽θ0f(x; θ0)∥2.
(C.36)"
T,0.9522292993630573,"D
MOTIVATION OF EXPLORATION NETWORK"
T,0.9538216560509554,"In this section, we list one gradient-based UCB from existing works (Ban et al., 2021; Zhou et al.,
2020), which motivates our design of exploration network f2. Let g(xt; θt) = ▽θtf(xt; θt)."
T,0.9554140127388535,"Lemma D.1. (Lemma 5.2 in (Ban et al., 2021)). Given a set of context vectors {xt}T
t=1 and the
corresponding rewards {rt}T
t=1 , E(rt) = h(xt) for any xt ∈{xt}T
t=1. Let f(xt; θ) be the L-layers
fully-connected neural network where the width is m, the learning rate is η, the number of iterations
of gradient descent is K. Then, there exist positive constants C1, C2, S, such that if"
T,0.9570063694267515,"m ≥poly(T, n, L, log(1/δ) · d · e
√"
T,0.9585987261146497,"log 1/δ), η = O(TmL + mλ)−1, K ≥e
O(TL/λ),"
T,0.9601910828025477,"then, with probability at least 1 −δ, for any xt ∈{xt}T
t=1, we have the following upper conﬁdence
bound:
|h(xt) −f(xt; θt)| ≤γ1∥g(xt; θt)/√m∥A−1
t
+ γ2 + γ1γ3 + γ4,
(D.1) where"
T,0.9617834394904459,"γ1(m, L) = (λ + tO(L)) · ((1 −ηmλ)J/2p"
T,0.9633757961783439,t/λ) + 1
T,0.964968152866242,"γ2(m, L, δ) = ∥g(xt; θ0)/√m∥A
′−1
t
· s"
T,0.9665605095541401,"log
det(A′
t)
det(λI)"
T,0.9681528662420382,"
−2 log δ + λ1/2S !"
T,0.9697452229299363,"γ3(m, L) = C2m−1/6p"
T,0.9713375796178344,"log mt1/6λ−7/6L7/2, γ4(m, L) = C1m−1/6p"
T,0.9729299363057324,log mt2/3λ−2/3L3
T,0.9745222929936306,"At = λI + t
X"
T,0.9761146496815286,"i=1
g(xt; θt)g(xt; θt)⊺/m, A′
t = λI + t
X"
T,0.9777070063694268,"i=1
g(xt; θ0)g(xt; θ0)⊺/m."
T,0.9792993630573248,"Note that g(xt; θ0) is the gradient at initialization, which can be initialized as constants. Therefore,
the above UCB can be represented as the following form for exploitation network f1: |h(xt,i) −
f1(xt,i; θ1
t)| ≤Ψ(g(xt; θt))."
T,0.9808917197452229,Published as a conference paper at ICLR 2022
T,0.982484076433121,"Table 3: Exploration Direction Comparison.
Methods
""Upward"" Exploration
""Downward"" Exploration
NeuralUCB
√
×"
T,0.9840764331210191,"NeuralTS
Randomly
Randomly"
T,0.9856687898089171,"EE-Net
√
√"
T,0.9872611464968153,"EE-Net has smaller approximation error. Given an arm x, let f1(x) be the estimated reward and
h(x) be the expected reward. The exploration network f2 in EE-Net is to learn h(x) −f1(x), i.e.,
the residual between expected reward and estimated reward, which is the ultimate goal of making
exploration. There are advantages of using a network f2 to learn h(x) −f1(x) in EE-Net, compared
to giving a statistical upper bound for it such as NeuralUCB, (Ban et al., 2021), and NeuralTS (in
NeuralTS, the variance ν can be thought of as the upper bound). For EE-Net, the approximation error
for h(x) −f1(x) is caused by the genenalization error of the neural network (Lemma B.1. in the
manuscript). In contrast, for NeuralUCB, (Ban et al., 2021), and NeuralTS, the approximation error
for h(x) −f1(x) includes three parts. The ﬁrst part is caused by ridge regression. The second part
of the approximation error is caused by the distance between ridge regression and Neural Tangent
Kernel (NTK). The third part of the approximation error is caused by the distance between NTK and
the network function. Because they use the upper bound to make selections, the errors inherently
exist in their algorithms. By reducing the three parts of the approximation errors to only the neural
network convergence error, EE-Net achieves tighter regret bound compared to them (improving by
roughly √log T)."
T,0.9888535031847133,"𝑓!(𝑥"",$ ; 𝜃!)"
T,0.9904458598726115,"ℎ(𝑥"",$ ) Gap"
T,0.9920382165605095,"𝑓!(𝑥"",$ ; 𝜃!)"
T,0.9936305732484076,"ℎ(𝑥"",$ ) Gap"
T,0.9952229299363057,"Case 1: Upward Exploration
Case 2: Downward Exploration"
T,0.9968152866242038,"Figure 6:
Two types of exploration: Upward exploration and Downward exploration. f1 is the
exploitation network (estimated reward) and h is the expected reward."
T,0.9984076433121019,"EE-Net has the ability to determine exploration direction. The two types of exploration are
described by Figure 6. When the estimated reward is larger than the expected reward, i.e., h(x) −
f1(x) < 0, we need to do the ‘downward exploration’, i.e., lowering the exploration score of x to
reduce its chance of being explored; when h(x) −f1(x) > 0, we should do the ‘upward exploration’,
i.e., raising the exploration score of x to increase its chance of being explored. For EE-Net, f2 is
to learn h(x) −f1(x). When h(x) −f1(x) > 0, f2(x) will also be positive to make the upward
exploration. When h(x) −f1(x) < 0, f2(x) will be negative to make the downward exploration. In
contrast, NeuralUCB will always choose upward exploration, i.e., f1(x) + UCB(x) where UCB(x)
is always positive. In particular, when h(x)−f1(x) < 0, NeuralUCB will further amplify the mistake.
NeuralTS will randomly choose upward or downward exploration for all cases, because it draws a
sampled reward from a normal distribution where the mean is f1(x) and the variance ν is the upper
bound."
