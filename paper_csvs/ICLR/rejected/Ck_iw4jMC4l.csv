Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021691973969631237,"The choice of activation functions and their motivation is a long-standing issue
within the neural network community. Neuronal representations within artiﬁcial
neural networks are commonly understood as logits, representing the log-odds
score of presence (versus absence) of features within the stimulus. In this work,
we investigate the implications on activation functions of considering features to
be logits. We derive operations equivalent to AND, OR, and XNOR for log-odds
ratio representations of independent probabilities. Since these functions involve
taking multiple exponents and logarithms, they are not well suited to be directly
used within neural networks. Consequently, we construct efﬁcient approximations
named ANDAIL (the AND operator Approximate for Independent Logits), ORAIL,
and XNORAIL, which utilize only comparison and addition operations and can
be deployed as activation functions in neural networks. Like MaxOut, ANDAIL
and ORAIL are generalizations of ReLU to two-dimensions. We deploy these new
activation functions, both in isolation and in conjunction, and demonstrate their
effectiveness on a variety of tasks including image classiﬁcation, transfer learning,
abstract reasoning, and compositional zero-shot learning."
INTRODUCTION,0.004338394793926247,"1
INTRODUCTION"
INTRODUCTION,0.006507592190889371,"An activation function is a non-linearity which is interlaced between linear layers within an artiﬁcial
neural network. The non-linearity is essential in order for higher-order representations to form, since
otherwise the network would be degeneratively equivalent to a single linear layer."
INTRODUCTION,0.008676789587852495,"Early artiﬁcial neural networks were inspired by biological neural networks, with the activation
function analogous to a neuron’s need to exceed a potentiation threshold in order to ﬁre an action
potential. Biological neurons are have long been known to be more complex than this simple
abstraction, including features such as non-linearities in dendritic integration. Recent work has
demonstrated that a single biological neuron can compute the XOR of its inputs (Gidon et al., 2020),
a property long known to be lacking in artiﬁcial neurons (Minsky & Papert, 1969). This suggests that
gains in artiﬁcial neurons can be made by using activation functions which operate on more than one
input to the neuron at once."
INTRODUCTION,0.010845986984815618,"The earliest artiﬁcial neural networks featured either logistic-sigmoid or tanh as their activation
function. These activation functions were motivated by the idea that each layer of the network is
building another layer of abstraction of the stimulus space from the last layer. Each neuron in a layer
identiﬁes whether certain properties or features are present within the stimulus, and the pre-activation
(potentiation) value of the neuron indicates a score or logit for the presence of that feature. The
sigmoid function, σ(x) = 1/(1+e−x), was hence a natural choice of activation function, since as with
logistic regression, this will convert the logits of features into probabilities."
INTRODUCTION,0.013015184381778741,"There is some evidence that this interpretation has merit. Previous work has been done to identify
which features neurons are tuned to. Examples include LSTM neurons tracking quotation marks,
line length, and brackets (Karpathy et al., 2015); LSTM neurons tracking sentiment (Radford et al.,
2017); methods for projecting features back to the input space to view them (Olah et al., 2017); and
interpretable combinations of neural activities (Olah et al., 2020). Analogously, within biological
neural networks, neurons are tuned to respond more strongly to certain stimuli, and less strongly to"
INTRODUCTION,0.015184381778741865,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01735357917570499,"others. At high representation levels, concept cells respond predominantly to certain concepts, such
one’s grandmother, or Jennifer Aniston (Gross, 2002; Quian Quiroga et al., 2005)."
INTRODUCTION,0.019522776572668113,"Sigmoidal activation functions are no longer commonly used within machine learning between
layers of representations, though sigmoid is still widely used for gating operations which scale the
magnitude of other features in an attention-like manner. The primary disadvantage of the sigmoid
activation function is its vanishing gradient — as the potentiation rises, activity converges to a plateau,
and hence the gradient goes to zero. This prevents feedback information propagating back through
the network from the loss function to the early layers of the network, which consequently prevents it
from learning to complete the task."
INTRODUCTION,0.021691973969631236,"The Rectiﬁed Linear Unit activation function (Fukushima, 1980; Jarrett et al., 2009; Nair & Hinton,
2010), ReLU(x) = max(0, x), does not have this problem, since in its non-zero regime it has a
gradient of 1. Another advantage of ReLU is it has very low computational demands. Since it is both
effective and efﬁcient, it has proven to be a highly choice of popular activation function. The chief
drawback to ReLU is it has no sensitivity to changes across half of its input domain, and on average
passes no gradient back to its inputs. This can lead to neurons dying1 if their weights make them
never reach their activation threshold. If sigmoid is equivalent to converting our feature logits into
probabilities, then the ReLU activation function is equivalent to truncating our logits and denying any
evidence for the absence of a feature. We hypothesise that omitting negative evidence in this way is
undesirable and reduces the generalisation ability of ReLU-based networks."
INTRODUCTION,0.02386117136659436,"Variants of ReLU have emerged, aiming to smooth out its transition between domains and provide
a gradient in its inactive regime. These include ELU (Clevert et al., 2016), CELU (Barron, 2017),
SELU (Klambauer et al., 2017), GELU (Hendrycks & Gimpel, 2020), SiLU (Elfwing et al., 2017;
Ramachandran et al., 2017), and Mish (Misra, 2019). However, all these activation functions still
bear the general shape of ReLU and truncate negative logits."
INTRODUCTION,0.026030368763557483,"Fuzzy logic operators are generalizations of boolean logic operations to continuous variables, using
rules similar to applying logical operators in probability space. Previous work has explored networks
of fuzzy logic operations, including neural networks which use an activation functions that constitutes
a learnable interpolation between fuzzy logic operators (Godfrey & Gashler, 2017). In this work, we
introduce activation functions which are similar to fuzzy logic operators, but derived for working in
logit space instead of in probability space."
INTRODUCTION,0.028199566160520606,"In this work we set out to develop activation functions based on the principle that neurons encode
logits — scores that represent the presence of features in the log-odds space. In Section 2 we derive
and deﬁne these functions in detail for different logical operators, and then consider their performance
on numerous task types including parity (Section 3.1), image classiﬁcation (Sections 3.4 and 3.5),
transfer learning (Section 3.6), abstract reasoning (Appendix A.14), soft-rule guided classiﬁcation
as exempliﬁed by the Bach chorale dataset (Section 3.3), and compositional zero-shot learning (Ap-
pendix A.15). These tasks were selected to (1) survey the performance of the new activations on
existing benchmark tasks, and (2) evaluate their performance on tasks which we suspect in particular
may require logical reasoning and hence beneﬁt from activation functions which apply these logical
operations to logits."
DERIVATION,0.03036876355748373,"2
DERIVATION"
DERIVATION,0.03253796095444685,"Manipulation of probabilities in logit-space is known to be more efﬁcient for many calculations.
For instance, the log-odds form of Bayes’ Rule (Equation 9) states that the posterior logit equals
the prior logit plus the log of the likelihood ratio for the new evidence (the log of the Bayes
factor).Thus, working in logit-space allows us to perform Bayesian updates on many sources of
evidence simultaneously, merely by summing together the log-likelihood ratios for the evidence.
A weighted sum may be used if the amount of credence given to the sources differs — and this is
precisely the operation performed by a linear layer in a neural network."
DERIVATION,0.03470715835140998,"When considering sets of probabilities, a natural choice of operation to perform is measuring the joint
probability of two events both occurring — the AND operation for probabilities. Suppose our input
space is x ∈[0, 1]2, and the goal is to output y > 0 if xi = 1 ∀i, and y < 0 otherwise, using model"
DERIVATION,0.0368763557483731,1Though this problem is very rare when using BatchNorm to stabilise feature distributions.
DERIVATION,0.039045553145336226,Under review as a conference paper at ICLR 2022
DERIVATION,0.04121475054229935,"with a weight vector w and bias term b, such that y = wT x + b. This can be trivially solved with the
weight matrix w = [1, 1] and bias term b = −1.5. However, since this is only a linear separator, the
solution can not generalise to the case y > 0 iff xi > 0 ∀i."
DERIVATION,0.04338394793926247,"Similarly, let us consider how the OR function solved with a linear layer. Our goal is to output y > 0
if ∃xi = 1, and y < 0 otherwise. The binary case can be trivially solved with the weight matrix
w = [1, 1] and bias term b = −0.5. The difference between this and the solution for AND is only an
offset to our bias term. In each case, if the input space is expanded beyond binary to R2, the output
can be violated by changing only one of the arguments."
AND,0.0455531453362256,"2.1
AND"
AND,0.04772234273318872,"Suppose we are given x and y as the logits for the presence (vs absence) of two events, X and Y .
These logits have equivalent probability values, which can be obtained using the sigmoid function,
σ(u) = (1 + e−u)−1. Let us assume that the events X and Y are independent of each other. In this
case, the probability of both events occurring (the joint probability) is P(X, Y ) = P(X ∧Y ) =
P(X) P(Y ) = σ(x) σ(y)."
AND,0.049891540130151846,"However, we wish to remain in logit-space, and must determine the logit of the joint probability,
logit(P(X, Y )). This is given by"
AND,0.052060737527114966,"ANDIL := logit(P(X ∧Y )x⊥⊥y) = log

p
1 −p"
AND,0.05422993492407809,"
, where p = σ(x) σ(y),"
AND,0.05639913232104121,"= log

σ(x) σ(y)
1 −σ(x) σ(y)"
AND,0.05856832971800434,"
,
(1)"
AND,0.06073752711496746,"which we coin as ANDIL, the AND operator for independent logits (IL). This 2d function is illustrated
as a contour plot (Figure 1, left). Across the plane, the order of magnitude of the output is the same
as at least one of the two inputs, scaling approximately linearly."
AND,0.06290672451193059,"The approximately linear behaviour of the function is suitable for use as an activation function
(no vanishing gradient), however taking exponents and logs scales poorly from a computational
perspective. Hence, we developed a computationally efﬁcient approximation as follows. Observe
that we can loosely approximate ANDIL with the minimum function (Figure 1, right panel). This is
equivalent to assuming the probability of both X and Y being true equals the probability of the least
likely of X and Y being true — a na¨ıve approximation which holds well in three quadrants of the
plane, but overestimates the probability when both X and Y are unlikely. In this quadrant, when both
X and Y are both unlikely, a better approximation for ANDIL is the sum of their logits."
AND,0.0650759219088937,"We thus propose ANDAIL, a linear-approximate AND function for independent logits (AIL, i.e.
approximate IL)."
AND,0.06724511930585683,"ANDAIL(x, y) :=
x + y,
x < 0, y < 0
min(x, y),
otherwise
(2)"
AND,0.06941431670281996,"As shown in Figure 1 (left, middle), we observe that their output values and shape are very similar."
AND,0.07158351409978309,"−4
−2
0
2
4 −4 −2 0 2 4 ANDIL"
AND,0.0737527114967462,"−4
−2
0
2
4"
AND,0.07592190889370933,ANDAIL
AND,0.07809110629067245,"−4
−2
0
2
4 min −10 −8 −6 −4 −2 +0 +2 +4 +6 +8 +10"
AND,0.08026030368763558,"Figure 1: Heatmap comparing the outputs for the exact logit-space probabilistic-and function for
independent logits, ANDIL(x, y); our constructed approximation, ANDAIL(x, y); and max(x, y)."
AND,0.0824295010845987,Under review as a conference paper at ICLR 2022
OR,0.08459869848156182,"2.2
OR"
OR,0.08676789587852494,"Similarly, we can construct the logit-space OR function, for independent logits. For a pair of logits x
and y, the probability that either of the corresponding events is true is given by p = 1−σ(−x) σ(−y).
This can be converted into a logit as"
OR,0.08893709327548807,"ORIL(x, y) := logit(P(X ∨Y )x⊥⊥y) = log

p
1 −p"
OR,0.0911062906724512,"
, where p = 1 −σ(−x) σ(−y)
(3)"
OR,0.09327548806941431,"which can be roughly approximated by the max function. This is equivalent to setting the probability
of either of event X or Y occurring to be equal to the probability of the most likely event. This
underestimates the upper-right quadrant (below), which we can approximate better as the sum of the
input logits, yielding"
OR,0.09544468546637744,"ORAIL(x, y) :=
x + y,
x > 0, y > 0
max(x, y),
otherwise
(4)"
OR,0.09761388286334056,"−4
−2
0
2
4 −4 −2 0 2 4 ORIL"
OR,0.09978308026030369,"−4
−2
0
2
4 ORAIL"
OR,0.1019522776572668,"−4
−2
0
2
4 max −10 −8 −6 −4 −2 +0 +2 +4 +6 +8 +10"
OR,0.10412147505422993,"Figure 2: Comparison of the exact logit-space probabilistic-or function for independent logits,
ORIL(x, y); our constructed approximation, ORAIL(x, y); and max(x, y)."
XNOR,0.10629067245119306,"2.3
XNOR"
XNOR,0.10845986984815618,"We also consider the construction of a logit-space XNOR operator. This is the probability that X and
Y occur either together, or not at all, given by"
XNOR,0.11062906724511931,"XNORIL(x, y) := logit(P(X ¯⊕Y )x⊥⊥y) = log

p
1 −p"
XNOR,0.11279826464208242,"
,
(5)"
XNOR,0.11496746203904555,where p = σ(x) σ(y) + σ(−x) σ(−y). We can approximate this with
XNOR,0.11713665943600868,"XNORAIL(x, y) := sgn(xy) min(|x|, |y|),
(6)"
XNOR,0.1193058568329718,which focuses on the logit of the feature most likely to ﬂip the expected parity (Figure 3).
XNOR,0.12147505422993492,"We could use other approximations, such as the sign-preserving geometric mean,"
XNOR,0.12364425162689804,"signed geomean(x, y) := sgn(xy)
p"
XNOR,0.12581344902386118,"|xy|,
(7)"
XNOR,0.1279826464208243,"but note that the gradient of this is divergent, both along x = 0 and along y = 0."
DISCUSSION,0.1301518438177874,"2.4
DISCUSSION"
DISCUSSION,0.13232104121475055,"By working via probabilities, and assuming inputs encode independent events, we have derived
logit-space equivalents of the boolean logic functions, AND, OR, and XNOR. Since these are
computationally demanding to compute repeatedly within a neural network, we have constructed
approximations of them: ANDAIL, ORAIL, and XNORAIL. Like ReLU, these involve only com-
parison, addition, and multiplication operations which are cheap to perform. In fact, ANDAIL and
ORAIL are a generalization of ReLU to an extra dimension, since ORAIL(x, y = 0) = max(x, 0)."
DISCUSSION,0.13449023861171366,"The majority of activation functions are one-dimensional, f : R →R. In contrast to this, our proposed
activation functions are all two-dimensional, f : R2 →R. They must be applied to pairs of features"
DISCUSSION,0.13665943600867678,Under review as a conference paper at ICLR 2022
DISCUSSION,0.13882863340563992,"−4
−2
0
2
4 −4 −2 0 2 4"
DISCUSSION,0.14099783080260303,XNORIL
DISCUSSION,0.14316702819956617,"−4
−2
0
2
4"
DISCUSSION,0.14533622559652928,XNORAIL
DISCUSSION,0.1475054229934924,"−4
−2
0
2
4"
DISCUSSION,0.14967462039045554,signed_geomean −10 −8 −6 −4 −2 +0 +2 +4 +6 +8 +10
DISCUSSION,0.15184381778741865,"Figure 3: Comparison of the exact logit-space probabilistic-xnor function for independent logits,
XNORIL(x, y); our constructed approximation, XNORAIL(x, y); and signed geomean(x, y)."
DISCUSSION,0.1540130151843818,"from the embedding space, and will reduce the dimensionality of the input space by a factor of 2. This
behaviour is the same as seen in MaxOut networks (Goodfellow et al., 2013) which use max as their
activation function, MaxOut(x, y; k) := max(x, y). Similar to MaxOut, our activation functions
could be generalised to higher dimensional inputs, f : Rk →R, by considering the behaviour of the
logit-space AND, OR, XNOR operations with regard to more inputs. For simplicity, we restrict this
work to consider only k=2, but note these activation functions also generalize to higher dimensions."
ENSEMBLING,0.1561822125813449,"2.5
ENSEMBLING"
ENSEMBLING,0.15835140997830802,"By using multiple logit-boolean activation functions simultaneously alongside each other, we permit
the network multiple options of how to relate features together. When combining the activation
functions, we considered two strategies."
ENSEMBLING,0.16052060737527116,"In the partition (p) strategy, we split the nc dimensional pre-activation embedding equally into m
partitions, apply different activation functions on each partition, and concatenate the results together.
Using AIL activation functions under this strategy, the output dimension will always be half that of
the input, as it is for each AIL activation function individually."
ENSEMBLING,0.16268980477223427,"In the duplication (d) strategy, we apply m different activation functions in parallel to the same nc
elements. The output is consequently larger, with dimension m nc. If desired, we can counteract the
2→1 reduction of AIL activation functions by using two of them together under this strategy."
ENSEMBLING,0.1648590021691974,"Utilising ANDAIL, ORAIL and XNORAIL simultaneously allows our networks to access logit-space
equivalent of 12 of the 16 boolean logical operations with only a single sign inversion (in either one
of the inputs or the output). Including the bias term and skip connections, the network has easy access
to logit-space equivalents of all 16 boolean logical operations."
EXPERIMENTS,0.16702819956616052,"3
EXPERIMENTS"
EXPERIMENTS,0.16919739696312364,"We evaluated the performance of our AIL activation functions, both individually and together in an
ensemble, on a range of benchmarking tasks. Since ANDAIL and ORAIL are equivalent when the
sign of operands and outputs can be freely chosen, we evaluate only on ORAIL and not both."
EXPERIMENTS,0.17136659436008678,"We compared the AIL activation functions against three primary baselines:
(1) ReLU, (2)
max(x, y) = MaxOut([x, y]; k=2), and (3) the concatenation of max(x, y) and min(x, y), denoted
{Max, Min (d)}. The {Max, Min (d)} ensemble is equivalent to GroupSort with a group size of 2
(Anil et al., 2019; Chernodub & Nowicki, 2017), sometimes referred to as the MaxMin operator, and
is comparable to the concatenation of ORAIL and ANDAIL under our duplication strategy."
PARITY,0.1735357917570499,"3.1
PARITY"
PARITY,0.175704989154013,"In a simple initial experiment, we constructed a synthetic dataset whose labels could be derived
directly using the logical operation XNOR. Each sample in this dataset consisted of four input logit
values, with a label that was derived by converting each logit to probability space, rounding to the"
PARITY,0.17787418655097614,Under review as a conference paper at ICLR 2022
PARITY,0.18004338394793926,"nearest integer, and taking the parity over this set of binary digits (i.e. true when we have an even
number of activated bits, false otherwise)."
PARITY,0.1822125813449024,"Figure 4: Visualisation of weight matrices learnt
by two-layer MLPs on a binary classiﬁcation task,
where the target output is the parity of the inputs.
Widths of lines indicate weight magnitudes (or-
ange: +ve values, blue: -ve). Network with ReLU:
60% accuracy. Network with XNORAIL: 100%
accuracy."
PARITY,0.1843817787418655,"A very small MLP model with two hidden lay-
ers (the ﬁrst with four neurons, the second
with two neurons) should be capable of per-
fect classiﬁcation accuracy on this dataset with
a sparse weight matrix by learning to extract
pairwise binary relationships between inputs us-
ing XNORAIL. We trained such an MLP, for
100 epochs using Adam, one-cycle learning rate
schedule, max LR 0.01, weight decay 1 × 10−4."
PARITY,0.18655097613882862,"The two-layer MLP using the XNORAIL acti-
vation learned a sparse weight matrix able to
perfectly classify any input combination, shown
in Figure 4. In comparison, an identical network
setup using ReLU was only able to produce 60%
classiﬁcation accuracy. Though this accuracy
could be improved by increasing the MLP width
or depth, the weight matrix was still not sparse.
This experiment provides an example situation
where XNORAIL is utilized by a network to
directly extract information about the relation-
ships between network inputs. For additional
results, see Appendix A.7."
MLP ON COVERTYPE,0.18872017353579176,"3.2
MLP ON COVERTYPE"
MLP ON COVERTYPE,0.19088937093275488,"We trained small one, two, and three-layer MLP
models on the Covertype dataset (UCI Machine
Learning Repository, 1998; Blackard & Dean,
1998) from the UCI Machine Learning Repos-
itory. The Covertype dataset is a classiﬁcation
task consisting of 581 012 samples of forest cov-
erage across 7 classes. Each sample has 54 at-
tributes. We used a ﬁxed random 80:20 train:test
split; for training details and accuracy vs model
size, see Appendix A.8."
MLP ON COVERTYPE,0.19305856832971802,"For each activation function, we varied the number of hidden units per layer to investigate how
the activation functions affected the performance of the networks as its capacity changed. We did
not use weight decay or data augmentation for this experiment, and so the network exhibits clear
overﬁtting with larger architectures. As shown in Figure 16, Appendix A.8, XNORAIL performs
signiﬁcantly best on Covertype (p<10−5) for MLP with 2 or 3 layers, followed by networks which
include XNORAIL alongside with other activations, and signed geomean. The duplication strategy
outperforms partition."
MLP ON BACH CHORALES AND LOGIT INDEPENDENCE,0.19522776572668113,"3.3
MLP ON BACH CHORALES AND LOGIT INDEPENDENCE"
MLP ON BACH CHORALES AND LOGIT INDEPENDENCE,0.19739696312364424,"The Bach Chorale dataset (Boulanger-Lewandowski et al., 2012) consists of 382 chorales composed
by JS Bach, each ∼12 measures long, totalling approximately 83,000 notes. Represented as discrete
sequences of tokens, it has served as a benchmark for music processing for decades, from heuristic
methods to HMMs, RNNs, and CNNs (Mozer, 1990; Hild et al., 1992; Allan & Williams, 2005;
Liang, 2016; Hadjeres et al., 2017; Huang et al., 2019). The chorales are comprised of 4 voices
(melodic lines) whose behaviour is guided by soft musical rules that depend on the prior movement of
that voice as well as the movement of the other voices. An example of one such rule is that two voices
a ﬁfth apart ought not to move in parallel with one another. The task we choose here is to determine
whether a given short four-part musical excerpt is taken from a Bach chorale or not. During training,
we stochastically corrupt chorale excerpts to provide negative examples (see Appendix A.9). We"
MLP ON BACH CHORALES AND LOGIT INDEPENDENCE,0.19956616052060738,Under review as a conference paper at ICLR 2022
MLP ON BACH CHORALES AND LOGIT INDEPENDENCE,0.2017353579175705,"trained 2-layer MLP discriminators with a single sigmoid output and exchanged all other activation
functions (summarized in Figure 17). We found that {OR, AND, XNORAIL (d)} performed best,
but that overall the results were comparable (p<0.1 between best and worst, Student’s t-test between
10 random initialisations)."
MLP ON BACH CHORALES AND LOGIT INDEPENDENCE,0.2039045553145336,"Additionally, we investigated the independence between logits in the trained pre-activation embed-
dings. We would expect that an MLP which is optimally engaging its neurons would maintain
independence between features in order to maximize information. To capture the existence of correla-
tions, we took the cosine similarity between rows of the weight matrix. Since the inputs to all features
in a given layer are the same, this is equivalent to measuring the similarity between corresponding
pair of pre-activation features. We performed two experiments. In the ﬁrst, we measured correlations
between all pairwise combinations, and in the second we took correlations between only adjacent
pre-activations that would be paired for the logical activation functions. For these experiments we
used 2 hidden layers and a width that showed maximal performance for each activation function. The
results are shown in Appendix A.10."
CNN AND MLP ON MNIST,0.20607375271149675,"3.4
CNN AND MLP ON MNIST 10 5 10 6 10 7"
CNN AND MLP ON MNIST,0.20824295010845986,Number of parameters 98.0 98.1 98.2 98.3 98.4 98.5 98.6 98.7 98.8
CNN AND MLP ON MNIST,0.210412147505423,Test accuracy (%)
CNN AND MLP ON MNIST,0.21258134490238612,"MNIST
, MLP (2 hidden layers) ReLU SiLU Max"
CNN AND MLP ON MNIST,0.21475054229934923,"Max, Min (d)"
CNN AND MLP ON MNIST,0.21691973969631237,signed_geomean XNOR AIL OR AIL
CNN AND MLP ON MNIST,0.21908893709327548,"OR, AND AIL (d)"
CNN AND MLP ON MNIST,0.22125813449023862,"OR, XNOR AIL (p)"
CNN AND MLP ON MNIST,0.22342733188720174,"OR, XNOR AIL (d)"
CNN AND MLP ON MNIST,0.22559652928416485,"OR, AND, XNOR AIL (p)"
CNN AND MLP ON MNIST,0.227765726681128,"OR, AND, XNOR AIL (d) 10 4 10 5 10 6 10 7"
CNN AND MLP ON MNIST,0.2299349240780911,Number of parameters 99.2 99.3 99.4 99.5 99.6
CNN AND MLP ON MNIST,0.23210412147505424,Test accuracy (%)
CNN AND MLP ON MNIST,0.23427331887201736,"MNIST
, CNN (6 layer)"
CNN AND MLP ON MNIST,0.23644251626898047,"Figure 5: We trained CNN on MNIST, MLP on ﬂattened-MNIST, using Adam (1-cycle, 10 ep),
hyperparams determined by random search. Mean (bars: std dev) of n=40 weight inits."
CNN AND MLP ON MNIST,0.2386117136659436,"We trained 2-layer MLP and 6-layer CNN models on MNIST with Adam (Kingma & Ba, 2017),
1-cycle schedule (Smith & Topin, 2017; Smith, 2018), and using hyperparameters tuned through a
random search against a validation set comprised of the last 10k images of the training partition."
CNN AND MLP ON MNIST,0.24078091106290672,"The MLP used two hidden layers, the widths of which were varied together to evaluate the perfor-
mance for a range of model sizes. The CNN used six layers of 3x3 convolution layers, with 2x2 max
pooling (stride 2) after every other conv layer. The output was ﬂattened and followed by three MLP
layers. The widths of the layers were scaled up approximately linearly to explore a range of model
sizes (see Appendix A.11 for more details)."
CNN AND MLP ON MNIST,0.24295010845986983,"For the MLP, XNORAIL performed best along with signed geomean (p < 0.1), ahead of all
other activations (p < 0.01; Figure 5 left panel). With the CNN, ﬁve activation conﬁgurations
({OR, AND, XNORAIL (p)}, {OR, XNORAIL (d/p)}, Max, and SiLU) performed best (p<0.05;
Figure 5, right panel). Additionally, we note that CNNs which used ORAIL or Max (alone or in an
ensemble) maintained high performance with an order of magnitude fewer parameters (3 × 104) than
networks which did not (3 × 105 params)."
CNN AND MLP ON MNIST,0.24511930585683298,"3.5
RESNET50 ON CIFAR-10/100"
CNN AND MLP ON MNIST,0.2472885032537961,"We explored the impact of our AIL activation functions on the performance of deep networks by
deploying them within a pre-activation ResNet50 model (He et al., 2015; 2016). We exchanged all
ReLU activation functions in the network to a candidate activation function while maintaining the
size of pass-through embedding. We experimented with changing the width of the network, scaling
up the embedding space and all hidden layers by a common factor. The network was trained on"
CNN AND MLP ON MNIST,0.24945770065075923,Under review as a conference paper at ICLR 2022
CNN AND MLP ON MNIST,0.25162689804772237,"CIFAR-10/-100 for 100 epochs using Adam (Kingma & Ba, 2017), 1-cycle (Smith, 2018; Smith
& Topin, 2017). Hyperparameters were optimized through a random search against a validation
partition of CIFAR-100 for a ﬁxed width factor w = 2 only. The same hyperparameters were reused
for experiments of changing width, and for CIFAR-10. We used width factors of 0.5, 1, 2, and 4 for
1→1 activation functions (ReLU, etc), widths of 0.75, 1.5, 3, and 6 for 2→1 activation functions
(Max, etc), and widths of 0.4, 0.75, 1.5, and 3 for {ORAIL, ANDAIL, XNORAIL (d)}."
CNN AND MLP ON MNIST,0.25379609544468545,"107
108"
CNN AND MLP ON MNIST,0.2559652928416486,Number of parameters 93.0 93.5 94.0 94.5 95.0 95.5 96.0
CNN AND MLP ON MNIST,0.25813449023861174,Test accuracy (%)
CNN AND MLP ON MNIST,0.2603036876355748,"CIFAR-10, ResNet50"
CNN AND MLP ON MNIST,0.26247288503253796,"ReLU
PReLU
SiLU
Max
Max, Min (d)
ORAIL
OR, AND (d)
OR, XNOR (d)
OR, AND, XNOR (d)
OR, XNOR (p)
OR, AND, XNOR (p)"
CNN AND MLP ON MNIST,0.2646420824295011,"107
108"
CNN AND MLP ON MNIST,0.2668112798264642,Number of parameters 72 73 74 75 76 77 78 79
CNN AND MLP ON MNIST,0.26898047722342733,Test accuracy (%)
CNN AND MLP ON MNIST,0.27114967462039047,"CIFAR-100, ResNet50"
CNN AND MLP ON MNIST,0.27331887201735355,"Figure 6: ResNet50 on CIFAR-10/100, varying the activation function. The same activation function
(or ensemble) was used through the network. The width was varied to explore a range of network
sizes (see text). Trained for 100 ep. with Adam, using hyperparams determined by random search on
CIFAR-100, w = 2, only."
CNN AND MLP ON MNIST,0.2754880694143167,"For both CIFAR-10 and 100, we ﬁnd that SiLU, ORAIL, and Max outperform ReLU across a
wide range of width values, as shown in Figure 6. These three activation functions hold up their
performance best as the number of parameters is reduced. Note that SiLU was previously discovered
as an optimal activation function for deep residual image models (Ramachandran et al., 2017), and so
it is expected to perform highly in this setting. Meanwhile, other AIL activation functions perform
similarly to ReLU when the width is thin, and slightly worse than ReLU when the width is wide.
When used on its own and not part of an ensemble, the XNORAIL activation function performed
very poorly (off the bottom of the chart), indicating it is not well suited for this task."
TRANSFER LEARNING,0.27765726681127983,"3.6
TRANSFER LEARNING"
TRANSFER LEARNING,0.279826464208243,"We considered the task of transfer learning on several image classiﬁcation datasets. We used
a ResNet18 model (He et al., 2015) pretrained on ImageNet-1k. The weights were frozen (not
ﬁne-tuned) and used to generate embeddings of samples from other image datasets. We trained a two-
layer MLP to classify the images from these embeddings, using various activation functions. For a
comprehensive set of baselines, we compared against every activation function built into PyTorch 1.10.
To make the number of parameters similar, we used a width of 512 for activation functions with
1→1 mapping (e.g. ReLU), a width of 650 for activation functions with a 2→1 mapping (e.g. Max,
ORAIL), and a width of 438 for {OR, AND, XNORAIL (d)}. See Appendix A.13 for more details."
TRANSFER LEARNING,0.28199566160520606,"Our results are shown in Table 1. We found that all our activation functions outperformed ReLU
on every transfer task. In particular, our three activation ensembles using the duplication strategy
performed very highly across all 7 transfer learning tasks — overall, {OR, AND, XNORAIL (d)}
appears to perform best (top for 3 datasets, second on 3), followed by {OR, ANDAIL (d)}. Our
activation functions were beaten on Caltech101, Oxford Flowers, and Stanford Cars, and only
by PReLU. For these three datasets, a linear readout outperformed an MLP with ReLU, and on
Caltech101 the linear layer performed best. This suggests that PReLU excels only on particularly
simple tasks. For further discussion, see Appendix A.13."
ADDITIONAL RESULTS,0.2841648590021692,"3.7
ADDITIONAL RESULTS"
ADDITIONAL RESULTS,0.28633405639913234,"For results on abstract reasoning and compositional zero-shot learning, please see Appendix A.14
and Appendix A.15, respectively."
ADDITIONAL RESULTS,0.2885032537960954,Under review as a conference paper at ICLR 2022
ADDITIONAL RESULTS,0.29067245119305857,"Table 1: Transfer learning from a frozen ResNet-18 architecture pretrained on ImageNet-1k to other
computer vision datasets. An MLP head with two layers of pre-activation width of either 438, 512, or
650 (depending on activation function, to keep the number of params approximately constant) was
trained, without re-training the pretrained base model. Trained with SGD, cosine annealed LR 0.01,
for 25 epochs. Mean (standard error) of n=5 random initializations of the MLP (same pretrained
network). Bold: best. Underlined: second best. Italic: no signiﬁcant difference from best (two-sided
Student’s t-test, p>0.05). Background: linear color scale from ReLU baseline (white) to best (black)."
ADDITIONAL RESULTS,0.2928416485900217,Test Accuracy (%)
ADDITIONAL RESULTS,0.2950108459869848,"Activation function
Caltech101
CIFAR10
CIFAR100
Flowers
Cars
STL-10
SVHN"
ADDITIONAL RESULTS,0.29718004338394793,"Linear layer only
88.35±0.15
78.56±0.09
57.39±0.09
92.32±0.20
33.51±0.06
94.68±0.02
45.42±0.06"
ADDITIONAL RESULTS,0.2993492407809111,"ReLU
86.58±0.17
81.63±0.05
58.04±0.11
90.71±0.26
30.97±0.26
94.62±0.06
51.39±0.06
LeakyReLU
86.60±0.13
81.67±0.11
58.01±0.09
90.73±0.32
31.09±0.24
94.61±0.05
51.40±0.05
PReLU
87.83±0.21
81.03±0.13
58.90±0.18
93.17±0.19
39.84±0.18
94.54±0.05
51.42±0.09
Softplus
86.16±0.18
79.13±0.08
56.58±0.07
89.39±0.29
21.23±0.13
94.63±0.03
47.44±0.06"
ADDITIONAL RESULTS,0.30151843817787416,"ELU
87.18±0.09
80.44±0.08
58.08±0.10
91.71±0.14
34.70±0.06
94.55±0.05
50.07±0.07
CELU
87.18±0.09
80.44±0.08
58.08±0.10
91.71±0.14
34.70±0.06
94.55±0.05
50.07±0.07
SELU
87.74±0.09
79.93±0.13
58.24±0.06
92.27±0.13
37.51±0.17
94.53±0.07
49.38±0.06
GELU
87.10±0.15
81.39±0.09
58.51±0.13
91.51±0.15
33.43±0.15
94.62±0.06
51.56±0.08
SiLU
86.91±0.11
80.53±0.11
58.14±0.12
91.37±0.18
32.15±0.17
94.59±0.05
50.69±0.09
Hardswish
87.12±0.12
80.10±0.10
58.25±0.10
91.56±0.25
33.17±0.23
94.62±0.05
50.09±0.09
Mish
87.11±0.12
81.09±0.11
58.37±0.10
91.61±0.15
33.75±0.14
94.61±0.05
51.29±0.08"
ADDITIONAL RESULTS,0.3036876355748373,"Softsign
81.47±0.18
80.03±0.09
54.84±0.09
82.34±0.22
17.33±0.10
94.70±0.03
49.48±0.07
Tanh
87.48±0.06
80.56±0.07
57.35±0.08
90.32±0.20
29.51±0.12
94.63±0.07
50.15±0.08"
ADDITIONAL RESULTS,0.30585683297180044,"GLU
86.71±0.31
79.19±0.07
57.64±0.10
90.34±0.19
27.04±0.12
94.57±0.03
48.28±0.17"
ADDITIONAL RESULTS,0.3080260303687636,"Max
86.96±0.20
81.76±0.14
58.60±0.12
90.98±0.18
33.37±0.15
94.70±0.06
51.36±0.12
Max, Min (d)
87.23±0.13
82.31±0.10
59.05±0.10
91.68±0.18
34.91±0.12
94.64±0.04
51.72±0.04"
ADDITIONAL RESULTS,0.31019522776572667,"XNORAIL
86.97±0.18
81.83±0.06
58.46±0.10
90.93±0.15
32.56±0.10
94.71±0.06
51.54±0.05
ORAIL
87.45±0.14
81.88±0.07
59.10±0.09
92.00±0.15
36.01±0.12
94.69±0.04
51.52±0.07
OR, ANDAIL (d)
87.43±0.11
82.38±0.06
59.90±0.08
92.07±0.18
37.16±0.15
94.55±0.05
52.11±0.09
OR, XNORAIL (p)
87.42±0.12
81.92±0.07
59.09±0.10
91.93±0.12
35.99±0.17
94.68±0.03
51.43±0.08
OR, XNORAIL (d)
87.09±0.21
82.20±0.04
59.44±0.07
91.90±0.10
36.88±0.10
94.69±0.06
52.02±0.16
OR, AND, XNORAIL (p)
87.43±0.14
81.78±0.06
59.27±0.13
91.98±0.29
35.90±0.09
94.66±0.03
51.47±0.13
OR, AND, XNORAIL (d)
87.49±0.11
82.50±0.08
59.83±0.12
92.37±0.08
37.60±0.20
94.72±0.02
52.23±0.14"
DISCUSSION,0.3123644251626898,"4
DISCUSSION"
DISCUSSION,0.31453362255965295,"In this work we motivated and introduced novel activation functions analogous to boolean operators
in logit-space. We designed the AIL functions, fast approximates to the true logit-space functions
equivalent to manipulating the corresponding probabilities, and demonstrated their effectiveness on a
wide range of tasks."
DISCUSSION,0.31670281995661603,"Although our activation functions assume independence (which is generally approximately true for the
pre-activation features learnt with 1d activation functions), we found the network learnt to induce anti-
correlations between features which were paired together by our activation functions (Appendix A.10).
This suggests that the assumption of independence is not essential to the performance of our proposed
activation functions."
DISCUSSION,0.3188720173535792,"We found that the XNORAIL activation function was highly effective in the setting of shallow net-
works. Meanwhile, the ORAIL activation function was highly effective for representation learning in
the setting of a deep ResNet architecture trained on images. In scenarios which involve manipulating
high-level features extracted by an embedding network, we ﬁnd that using an ensemble of AIL
activation functions together works best, and that the duplication ensembling strategy outperforms
partitioning. In this work we have restricted ourselves to only considering using a single activation
function (or ensemble) throughout the network, however our results together indicate that stronger re-
sults may be found by using ORAIL for feature extraction and an ensemble of {OR, AND, XNORAIL
(d)} for later higher-order reasoning layers within the network."
DISCUSSION,0.3210412147505423,"The idea we propose is nascent and there is a great deal of scope for exploring other forms of
activation functions that combine multiple pre-activation features by utilizing “higher-order activation
functions”."
DISCUSSION,0.3232104121475054,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.32537960954446854,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3275488069414317,"We have shared code to run all experiments considered in this paper with the reviewers via an anony-
mous download URL. The code base contains detailed instructions on how to setup each experiment,
including downloading the datasets and installing environments with pinned dependencies. It should
be possible to reproduce all our experimental results with this code. For the ﬁnal version of the paper,
we will make the code publicly available on an online repository and share a link to it within the
paper."
REFERENCES,0.3297180043383948,REFERENCES
REFERENCES,0.3318872017353579,"Moray Allan and Christopher Williams. Harmonising chorales by probabilistic inference. In L. Saul,
Y. Weiss, and L. Bottou (eds.), Advances in Neural Information Processing Systems, volume 17.
MIT Press, 2005."
REFERENCES,0.33405639913232105,"Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation, 2019."
REFERENCES,0.3362255965292842,"David G. T. Barrett, Felix Hill, Adam Santoro, Ari S. Morcos, and Timothy Lillicrap. Measuring
abstract reasoning in neural networks, 2018."
REFERENCES,0.3383947939262473,"Jonathan T. Barron. Continuously differentiable exponential linear units, 2017."
REFERENCES,0.3405639913232104,"Yaniv Benny, Niv Pekar, and Lior Wolf. Scale-localized abstract reasoning, 2021."
REFERENCES,0.34273318872017355,"Jock A. Blackard. Comparison of Neural Networks and Discriminant Analysis in Predicting Forest
Cover Types. PhD thesis, Colorado State University, USA, 1998. AAI9921979."
REFERENCES,0.34490238611713664,"Jock A Blackard and Denis J Dean. Comparative accuracies of neural networks and discriminant
analysis in predicting forest cover types from cartographic variables. In Second Southern Forestry
GIS Conference, pp. 189–199, 1998."
REFERENCES,0.3470715835140998,"Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependen-
cies in high-dimensional sequences: Application to polyphonic music generation and transcription,
2012."
REFERENCES,0.3492407809110629,"Artem Chernodub and Dimitri Nowicki. Norm-preserving orthogonal permutation linear unit activa-
tion functions (oplu), 2017."
REFERENCES,0.351409978308026,"Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus), 2016."
REFERENCES,0.35357917570498915,"Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence
and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011."
REFERENCES,0.3557483731019523,"Ekin Dogus Cubuk, Barret Zoph, Dandelion Man´e, Vijay Vasudevan, and Quoc V. Le. Autoaugment:
Learning augmentation policies from data. CoRR, abs/1805.09501, 2018. URL http://arxiv.
org/abs/1805.09501."
REFERENCES,0.3579175704989154,"Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning, 2017."
REFERENCES,0.3600867678958785,"Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions
on pattern analysis and machine intelligence, 28(4):594–611, 2006."
REFERENCES,0.36225596529284165,"Franc¸ois Fleuret, Ting Li, Charles Dubout, Emma K. Wampler, Steven Yantis, and Donald Ge-
man.
Comparing machines and humans on a visual categorization test.
Proceedings of
the National Academy of Sciences, 108(43):17621–17625, 2011.
ISSN 0027-8424.
doi:
10.1073/pnas.1109168108. URL https://www.pnas.org/content/108/43/17621."
REFERENCES,0.3644251626898048,"Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193–202, Apr
1980. ISSN 1432-0770. doi: 10.1007/BF00344251. URL https://doi.org/10.1007/
BF00344251."
REFERENCES,0.3665943600867679,Under review as a conference paper at ICLR 2022
REFERENCES,0.368763557483731,"Albert Gidon, Timothy Adam Zolnik, Pawel Fidzinski, Felix Bolduan, Athanasia Papoutsi, Panayiota
Poirazi, Martin Holtkamp, Imre Vida, and Matthew Evan Larkum.
Dendritic action poten-
tials and computation in human layer 2/3 cortical neurons. Science, 367(6473):83–87, 2020.
doi: 10.1126/science.aax6239. URL https://www.science.org/doi/abs/10.1126/
science.aax6239."
REFERENCES,0.37093275488069416,"Luke B. Godfrey and Michael S. Gashler. A parameterized activation function for learning fuzzy
logic operations in deep neural networks. CoRR, abs/1708.08557, 2017. URL http://arxiv.
org/abs/1708.08557."
REFERENCES,0.37310195227765725,"Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International
Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp.
1319–1327, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL https://proceedings.
mlr.press/v28/goodfellow13.html."
REFERENCES,0.3752711496746204,"Charles G. Gross. Genealogy of the “grandmother cell”. The Neuroscientist, 8(5):512–518, 2002.
doi: 10.1177/107385802237175. URL https://doi.org/10.1177/107385802237175.
PMID: 12374433."
REFERENCES,0.3774403470715835,"Ga¨etan Hadjeres, Franc¸ois Pachet, and Frank Nielsen. Deepbach: a steerable model for bach chorales
generation, 2017."
REFERENCES,0.3796095444685466,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385."
REFERENCES,0.38177874186550975,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks, 2016."
REFERENCES,0.3839479392624729,"Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs), 2020."
REFERENCES,0.38611713665943603,"Hermann Hild, Johannes Feulner, and Wolfram Menzel. Harmonet: A neural net for harmonizing
chorales in the style of JS Bach. In Advances in neural information processing systems, pp.
267–274, 1992."
REFERENCES,0.3882863340563991,"Sheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, and Shihao Bai. Stratiﬁed rule-aware network for
abstract visual reasoning, 2020."
REFERENCES,0.39045553145336226,"Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, and Douglas Eck. Coun-
terpoint by convolution, 2019."
REFERENCES,0.3926247288503254,"Phillip Isola, Joseph J Lim, and Edward H Adelson. Discovering states and transformations in image
collections. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 1383–1391, 2015."
REFERENCES,0.3947939262472885,"Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is the best
multi-stage architecture for object recognition? In 2009 IEEE 12th International Conference on
Computer Vision, pp. 2146–2153, 2009. doi: 10.1109/ICCV.2009.5459469."
REFERENCES,0.3969631236442516,"Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017."
REFERENCES,0.39913232104121477,"H M Dipu Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali, Abbas Khosravi, Amir F Atiya,
Saeid Nahavandi, and Dipti Srinivasan. Spinalnet: Deep neural network with gradual input, 2020."
REFERENCES,0.40130151843817785,"Daniel Kahneman. Thinking, fast and slow. Thinking, fast and slow. Farrar, Straus and Giroux, New
York, NY, US, 2011. ISBN 0-374-27563-7 (Hardcover); 1-4299-6935-0 (PDF); 978-0-374-27563-1
(Hardcover); 978-1-4299-6935-2 (PDF)."
REFERENCES,0.403470715835141,"Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks,
2015."
REFERENCES,0.40563991323210413,Under review as a conference paper at ICLR 2022
REFERENCES,0.4078091106290672,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."
REFERENCES,0.40997830802603036,"G¨unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural
networks. In Proceedings of the 31st international conference on neural information processing
systems, pp. 972–981, 2017."
REFERENCES,0.4121475054229935,"Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013."
REFERENCES,0.41431670281995664,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009."
REFERENCES,0.4164859002169197,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.41865509761388287,"Feynman Liang. Bachbot: Automatic composition in the style of Bach chorales. University of
Cambridge, 8:19–48, 2016."
REFERENCES,0.420824295010846,"Marvin Minsky and Seymour Papert. Perceptrons. Perceptrons. M.I.T. Press, Oxford, England, 1969."
REFERENCES,0.4229934924078091,"Diganta Misra. Mish: A self regularized non-monotonic neural activation function, 2019."
REFERENCES,0.42516268980477223,"Michael C Mozer. Connectionist music composition based on melodic, stylistic, and psychophysical
constraints. University of Colorado, Boulder, Department of Computer Science, 1990."
REFERENCES,0.42733188720173537,"Vinod Nair and Geoffrey Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
volume 27, pp. 807–814, 06 2010."
REFERENCES,0.42950108459869846,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011."
REFERENCES,0.4316702819956616,"Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number
of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
pp. 722–729. IEEE, 2008."
REFERENCES,0.43383947939262474,"Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017. doi:
10.23915/distill.00007. https://distill.pub/2017/feature-visualization."
REFERENCES,0.4360086767895879,"Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.
Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001."
REFERENCES,0.43817787418655096,"Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, and Marc’Aurelio Ranzato. Task-driven
modular networks for zero-shot compositional learning, 2019."
REFERENCES,0.4403470715835141,"R Quian Quiroga, L Reddy, G Kreiman, C Koch, and I Fried. Invariant visual representation by single
neurons in the human brain. Nature, 435:1102–1107, 2005. doi: 10.1038/nature03687."
REFERENCES,0.44251626898047725,"Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering
sentiment, 2017."
REFERENCES,0.44468546637744033,"Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017."
REFERENCES,0.44685466377440347,"John C Raven and JH Court. Raven’s progressive matrices. Western Psychological Services Los
Angeles, CA, 1938."
REFERENCES,0.4490238611713666,"Jason Rennie, Lawrence Shih, Jaime Teevan, and David Karger. Tackling the poor assumptions of
naive bayes text classiﬁers. Proceedings of the Twentieth International Conference on Machine
Learning, 41, 07 2003."
REFERENCES,0.4511930585683297,"Leslie N. Smith. A disciplined approach to neural network hyper-parameters: Part 1 - learning
rate, batch size, momentum, and weight decay. CoRR, abs/1803.09820, 2018. URL http:
//arxiv.org/abs/1803.09820."
REFERENCES,0.45336225596529284,Under review as a conference paper at ICLR 2022
REFERENCES,0.455531453362256,"Leslie N. Smith and Nicholay Topin. Super-convergence: Very fast training of residual networks
using large learning rates. CoRR, abs/1708.07120, 2017. URL http://arxiv.org/abs/
1708.07120."
REFERENCES,0.45770065075921906,"UCI Machine Learning Repository. Covertype data set, Aug 1998. URL https://archive.
ics.uci.edu/ml/datasets/Covertype."
REFERENCES,0.4598698481561822,"Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. RAVEN: A dataset for
relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019."
REFERENCES,0.46203904555314534,Under review as a conference paper at ICLR 2022
REFERENCES,0.4642082429501085,"A
APPENDIX"
REFERENCES,0.46637744034707157,"A.1
LINEAR RELU EXAMPLES"
REFERENCES,0.4685466377440347,"In Figure 7, we show a representation of what a 2-d linear layer followed by the ReLU activation
function looks like. The output is the same up to rotation."
REFERENCES,0.47071583514099785,"Figure 7: ReLU unit, and ReLU units followed by a linear layer to try to approximate ORIL, leaving
a dead space where the negative logits should be."
REFERENCES,0.47288503253796094,"If we apply a second linear layer on top of the outputs of the ﬁrst two units, we can try to approximate
the logit AND or OR function. However, the solution using ReLU leaves a quadrant of the output
space hollowed out as zero due to its behaviour at truncating away information."
REFERENCES,0.4750542299349241,"A.2
SOLVING XOR"
REFERENCES,0.4772234273318872,"A long-standing criticism of artiﬁcial neural networks is their inability to solve XOR with a single
layer (Minsky & Papert, 1969). Of course, adding a single hidden layer allows a network using
ReLU to solve XOR. However, the way that it solves the problem is to join two of the disconnected
regions together in a stripe (see Figure 8). Meanwhile, our XNORAIL is trivial able to solve the XOR
problem without any hidden layers. For comparison here, we include one hidden layer with 2 units for
each network. Including a layer before the activation function makes the task harder for XNORAIL,
which must learn how to project the input space in order to compute the desired separation. Also,
including the linear layer allows the network to generalise to rotations and offset versions of the task."
REFERENCES,0.4793926247288503,Under review as a conference paper at ICLR 2022
REFERENCES,0.48156182212581344,"(a) ReLU
(b) XNORAIL"
REFERENCES,0.4837310195227766,"Figure 8: Solving XOR with a single hidden layer of 2 units, using either ReLU or XNORAIL
activation. Circles indicate negative (blue) and positive (red) training samples. The heatmaps indicate
the output probabilities of the two networks."
REFERENCES,0.48590021691973967,"A.3
LINEAR LAYERS AND BAYES’ RULE IN LOG-ODDS FORM"
REFERENCES,0.4880694143167028,Bayes’ Theorem or Bayes’ Rule is given by
REFERENCES,0.49023861171366595,P(H|X) = P(X|H) P(H)
REFERENCES,0.4924078091106291,"P(X)
.
(8)"
REFERENCES,0.4945770065075922,"In this case, we update the probability of our hypothesis, H, based on the event of the observation of
a new piece of evidence, X. Our prior belief for the hypothesis is P(H), and posterior is P(H|X).
To update our belief from the prior and yield the posterior, we multiply by the Bayes factor for the
evidence which is given by P(X|H)/P(X)."
REFERENCES,0.4967462039045553,"Converting the probabilities into log-odds ratios (logits) yields the following representation of Bayes’
Rule."
REFERENCES,0.49891540130151846,"log
 P(H|X)"
REFERENCES,0.5010845986984815,P(HC|X)
REFERENCES,0.5032537960954447,"
= log
 P(H) P(HC)"
REFERENCES,0.5054229934924078,"
+ log
 P(X|H)"
REFERENCES,0.5075921908893709,"P(X|HC) 
(9)"
REFERENCES,0.5097613882863341,"Here, HC is the complement to H (the event that the hypothesis is false), and P(HC) = 1 −
P(H). Our prior log-odds ratio is log ((P(H)/P(HC)), and our posterior after updating based on
the observation of new evidence X is log (P(H|X)/P(HC|X)). To update our belief from the prior
and yield the posterior, we add the log-odds Bayes factor for the evidence which is given by
log (P(X|H)/P(X|HC))."
REFERENCES,0.5119305856832972,"In log-odds space, a series of updates with multiple pieces of independent evidence can be performed
at once with a summation operation."
REFERENCES,0.5140997830802603,"log
 P(H|x)"
REFERENCES,0.5162689804772235,P(HC|x)
REFERENCES,0.5184381778741866,"
= log
 P(H) P(HC) 
+
X"
REFERENCES,0.5206073752711496,"i
log
 P(Xi|H)"
REFERENCES,0.5227765726681128,P(Xi|HC)
REFERENCES,0.5249457700650759,"
.
(10)"
REFERENCES,0.527114967462039,"This is operation can be represented by the linear layer in an artiﬁcial neural network, zk = bk +wT
k x.
Here, the bias term bk = log ((P(H)/P(HC)) is the prior for hypothesis (the presence of the feature
represented by the k-th neuron), and the series of weighted inputs from the previous layer, wki xi
provide evidential updates. This is also equivalent to the operation of a multinomial na¨ıve Bayes
classiﬁer, expressed in log-space, if we choose wki = log pki (Rennie et al., 2003)."
REFERENCES,0.5292841648590022,"A.4
DIFFERENCE BETWEEN AIL AND IL FUNCTIONS"
REFERENCES,0.5314533622559653,"Here, we measure and show the difference between the true logit-space operations and our AIL
approximations, shown in Figure 9, Figure 10, and Figure 11."
REFERENCES,0.5336225596529284,"In each case, we observe that the magnitude of the difference is never more than 1, which occurs
along the boundary lines in AIL. Since the magnitude of the three functions increase as we move
away from the origin, the relative difference decreases in magnitude as the size of x and y increase."
REFERENCES,0.5357917570498916,Under review as a conference paper at ICLR 2022
REFERENCES,0.5379609544468547,"Figure 9: Heatmaps showing ANDIL, ANDAIL, their difference, and their relative difference."
REFERENCES,0.5401301518438177,"Figure 10: Heatmaps showing ORIL, ORAIL, their difference, and their relative difference."
REFERENCES,0.5422993492407809,Under review as a conference paper at ICLR 2022
REFERENCES,0.544468546637744,"Figure 11: Heatmaps showing XNORIL, XNORAIL, their difference, and their relative difference."
REFERENCES,0.5466377440347071,Under review as a conference paper at ICLR 2022
REFERENCES,0.5488069414316703,"A.5
GRADIENT OF AIL AND IL FUNCTIONS"
REFERENCES,0.5509761388286334,"We show the gradient of each of the logit-space boolean operators and their AIL approximates in
Figure 12, Figure 13, and Figure 14. By the symmetry of each of the functions, the derivative with
respect to y is a reﬂected copy of the gradient with respect to x."
REFERENCES,0.5531453362255966,"We ﬁnd that the gradient of each AIL function closely matches that of the exact form. Whilst there
are “dead” regions where the gradient is zero, this only occurs for one of the derivatives at a time
(there is always a gradient with respect to at least one of x and y)."
REFERENCES,0.5553145336225597,Figure 12: Heatmaps showing the gradient with respect to x and y of ANDIL and ANDAIL.
REFERENCES,0.5574837310195228,Under review as a conference paper at ICLR 2022
REFERENCES,0.559652928416486,Figure 13: Heatmaps showing the gradient with respect to x and y of ORIL or ORAIL.
REFERENCES,0.561822125813449,Figure 14: Heatmaps showing the gradient with respect to x and y of XNORIL xnor XNORAIL.
REFERENCES,0.5639913232104121,"A.6
DATASET SUMMARY"
REFERENCES,0.5661605206073753,The datasets used in this work are summarised in Table 2.
REFERENCES,0.5683297180043384,We used the same splits for Caltech101 as used in Kabir et al. (2020).
REFERENCES,0.5704989154013015,Under review as a conference paper at ICLR 2022
REFERENCES,0.5726681127982647,Table 2: Dataset summaries.
REFERENCES,0.5748373101952278,№Samples
REFERENCES,0.5770065075921909,"Dataset
Train
Test
Classes
Reference"
REFERENCES,0.579175704989154,"Bach Chorales
229
77
2
Boulanger-Lewandowski et al. (2012)
Caltech101
6162
1695
101
Fei-Fei et al. (2006)
CIFAR-10
50 000
10 000
10
Krizhevsky (2009)
CIFAR-100
50 000
10 000
100
Krizhevsky (2009)
Covertype
464 810
116 202
7
Blackard (1998); Blackard & Dean (1998)
I-RAVEN
6000
2000
—
Hu et al. (2020)
MIT-States
30 338
12 995
245 obj, 115 attr
Isola et al. (2015)
MNIST
60 000
10 000
10
LeCun et al. (1998)
Oxford Flowers
6552
818
102
Nilsback & Zisserman (2008)
Stanford Cars
8144
8041
196
Krause et al. (2013)
STL-10
5000
8000
10
Coates et al. (2011)
SVHN
73 257
26 032
10
Netzer et al. (2011)"
REFERENCES,0.5813449023861171,"The Covertype dataset was chosen as a dataset that contains only simple features (and not pixels of an
image) on which we could study a simple MLP architecture, and was selected based on its popularity
on the UCI ML repository."
REFERENCES,0.5835140997830802,"The Bach Chorales dataset was chosen because — in addition to being in continued use by ML
researchers for decades — it presents an interesting opportunity to consider a task where logical
activation functions are intuitively applicable, as it is a relatively small dataset that has also been
approached with rule-based frameworks, e.g. the expert system by Ebcioglu (1988)."
REFERENCES,0.5856832971800434,"MNIST, CIFAR-10, CIFAR-100 are standard image datasets, commonly used. We used small MLP
and CNN architectures for the experiments on MNIST so we could investigate the performance
of the network for many conﬁgurations (varying the size of the network). We used ResNet-50, a
very common deep convolutional architecture within the computer vision ﬁeld, on CIFAR-10/100 to
evaluate the performance in the context of a deep network."
REFERENCES,0.5878524945770065,"The datasets used for the transfer learning task are all common and popular natural image datasets,
with some containing coarse-grained classiﬁcation (CIFAR-10), others ﬁne-grained (Stanford Cars),
and with a varying dataset size (5000—75000 training samples). We chose to do an experiment
involving transfer learning because it is a common practical situation where one must train only a
small network that handles high-level features, and is the sort of situation which involves manipulating
high-level features, relying on the pretrained network to do the feature extraction."
REFERENCES,0.5900216919739696,"We considered other domains where logical reasoning is involved as a component of the task, and
isolated abstract reasoning and compositional zero-shot learning as suitable tasks."
REFERENCES,0.5921908893709328,"For abstract reasoning, we wanted to use an IQ style test, and determined that I-RAVEN was a
state-of-the-art dataset within this domain (having ﬁxed some problems with the pre-existing RAVEN
dataset). We determined that the SRAN architecture from the paper which introduced I-RAVEN was
still state-of-the-art on this task, and so used this."
REFERENCES,0.5943600867678959,"Another problem domain in which we thought it would be interesting to study our activation functions
was compositional zero-shot learning (CZSL). This is because the task inherently involves combining
an attribute with an object (i.e. the AND operation). For CZSL, we looked at SOTA methods on
https://paperswithcode.com. The best performance was from SymNet, but this was only implemented
in TensorFlow and our code was set up in PyTorch so we built our experiments on the back of the
second-best instead, which is the TMN architecture. In the TMN paper, they used two datasets:
MIT-States and UT-Zappos-1. In our preliminary experiments, we found that the network started
overﬁtting on MIT-States after around 6 epochs, but on UT-Zappos-1 it was overﬁtting after the ﬁrst
or second epoch (one can not tell beyond the fact the val performance is best for the ﬁrst epoch). In
the context of zero-shot learning, an epoch uses every image once, but there are also only a ﬁnite
number of tasks in the dataset. Because there are multiple samples for each noun/adjective pair, and
each noun only appears with a handful of adjectives and vice versa, there is in a way fewer tasks in
one epoch than there are images. Hence it is possible for a zero-shot learning model to overﬁt to the
training tasks in less than one epoch (recall that the network includes a pretrained ResNet model"
REFERENCES,0.596529284164859,Under review as a conference paper at ICLR 2022
REFERENCES,0.5986984815618221,"for extracting features from the images). For simplicity, we dropped UT-Zappos-1 and focused on
MIT-States."
REFERENCES,0.6008676789587852,"A.7
PARITY EXPERIMENTS"
REFERENCES,0.6030368763557483,"Following on from the parity experiment described in the main text (Section 3.1), we also introduced
a second synthetic dataset with a labelling function that, while slightly more complex than the ﬁrst,
was still solvable by applying the logical XNOR operation to the network inputs. In this dataset
increased our number of inputs to 8, and derived our labels by applying a set of nested XNORIL
operations:"
REFERENCES,0.6052060737527115,"XNORIL( XNORIL(XNORIL(x2, x5), XNORIL(x3, x4)),
XNORIL(XNORIL(x6, x7), XNORIL(x0, x1))."
REFERENCES,0.6073752711496746,"For this more difﬁcult task we also reformulated our initial experiment into a regression problem,
as the continuous targets produced by this labelling function are more informative than the rounded
binary targets used in the ﬁrst experiment. We also adjusted our network setup to have an equal number
of neurons at each hidden layer as we found that this signiﬁcantly improved model performance2. We
again trained using the same model hyper-parameters for 100 epochs."
REFERENCES,0.6095444685466378,"While this time the model was not able to learn a sparse weight matrix that exactly reﬂected our
labelling function (see Figure 15), the model was again able to leverage the XNORAIL activation
function to signiﬁcantly outperform an identical model utilizing the ReLU activation function."
REFERENCES,0.6117136659436009,"(a) XNORAIL learned weight matrix
(b) ReLU learned weight matrix"
REFERENCES,0.613882863340564,"Figure 15: Training results, regression experiment on second synthetic dataset"
REFERENCES,0.6160520607375272,"We found that a simple model with three hidden layers, each with eight neurons, utilizing XNORAIL
was able to go from a validation RMSE of 0.287 at the beginning of training to a validation RMSE of
0.016 after 100 epochs. Comparatively, an identical model utilizing the ReLU activation function was
only able to achieve a validation RMSE of 0.271 after 100 epochs. In order for our ReLU network
to match the validation RMSE of our 8-neuron-per-layer XNORAIL model, we had to increase the
model size by 32 times to 256 neurons at each hidden layer."
REFERENCES,0.6182212581344902,"2We hypothesize that, because our XNORAIL activation function reduces the number of hidden layer neurons
by a factor of k, having a reduced number of neurons at each layer creates a bottleneck in the later layers of the
network which restricts the amount of information that made its way through to the ﬁnal layer"
REFERENCES,0.6203904555314533,Under review as a conference paper at ICLR 2022 10 4 10 5
REFERENCES,0.6225596529284165,Number of parameters 86 88 90 92 94
REFERENCES,0.6247288503253796,Test accuracy (%)
REFERENCES,0.6268980477223427,"Covertype, MLP (1 hidden layer) 10 4 10 5 10 6 10 7"
REFERENCES,0.6290672451193059,Number of parameters 92 93 94 95 96 97
REFERENCES,0.631236442516269,Test accuracy (%)
REFERENCES,0.6334056399132321,"Covertype, MLP (2 hidden layers) 10 4 10 5 10 6 10 7"
REFERENCES,0.6355748373101953,Number of parameters 92 93 94 95 96 97
REFERENCES,0.6377440347071583,Test accuracy (%)
REFERENCES,0.6399132321041214,"Covertype, MLP (3 hidden layers) ReLU SiLU Max"
REFERENCES,0.6420824295010846,"Max, Min (d)"
REFERENCES,0.6442516268980477,signed_geomean XNOR AIL OR AIL
REFERENCES,0.6464208242950108,"OR, AND (d)"
REFERENCES,0.648590021691974,"OR, XNOR (p)"
REFERENCES,0.6507592190889371,"OR, XNOR (d)"
REFERENCES,0.6529284164859002,"OR, AND, XNOR (p)"
REFERENCES,0.6550976138828634,"OR, AND, XNOR (d)"
REFERENCES,0.6572668112798264,"Figure 16: We trained MLPs on the Covertype dataset, with a ﬁxed 80:20 random split. Trained with
Adam, 50 ep., 1-cycle, using LRs determined automatically with LR-ﬁnder. Mean (bars: std dev) of
n=10 weight inits."
REFERENCES,0.6594360086767896,"A.8
COVERTYPE"
REFERENCES,0.6616052060737527,"Our one, two, and three-layer MLPs were trained using 1-cycle (Smith & Topin, 2017; Smith, 2018)
for 50 epochs, with a batch size of 1024, without weight decay. We used a subset of 15% of the
training data for validation. The learning rate was selected using an automated learning rate ﬁnder
approach, veriﬁed against the validation partition. For our ﬁnal experiments, the validation partition
was included in the training data. No data augmentation was performed."
REFERENCES,0.6637744034707158,"A.9
BACH CHORALE TRAINING DETAILS"
REFERENCES,0.665943600867679,"For each of the 4 voices, we restricted the available pitches to 3 octaves, resulting in 37 one-hot
tokens (including silence). Since we only planned on feeding small time-windows of the chorale into
our model, for pre-processing, we converted the data into a shape of (seq len, 4, 37), where we set
seq len to 4."
REFERENCES,0.6681127982646421,"To generate training examples for the discriminator, we transposed by {−5, −4, . . . , 5, 6} semitones,
chosen uniformly at random, and there was a 0.5 probability that the sample was corrupted by the
following method:"
REFERENCES,0.6702819956616052,"• Choose 2-3 notes in the (seq len × 4) window to be corrupted
• For each note, corrupt by the following mixture distribution:"
REFERENCES,0.6724511930585684,"– (p = 0.6) Sample a pitch from a Gaussian centered on the existing note, with σ =
3 semitones, forcing the new pitch to be distinct
– (p = 0.2) Copy a pitch from the current voice, forcing the new pitch to be distinct
– (p = 0.2) Extend the previous note in time
– (p = 0.1) Sample uniformly from all 37 possible tokens"
REFERENCES,0.6746203904555315,"A.10
CORRELATIONS BETWEEN PRE-ACTIVATIONS"
REFERENCES,0.6767895878524945,"Results on correlations between weights in the JSB Chorale models are shown in Figure 18. We
found that when taking all pre-activations into account, every activation function generally showed
independence between features. Interestingly, the cosine similarities between inputs that were paired
together for the bivariate activation functions showed anticorrelation in almost all cases where Max
or ORAIL were used, and other cases generally showed more correlation than ReLU."
REFERENCES,0.6789587852494577,"We found that randomly selected pairs of preactivation features within the same layer have correlations
that are given by a Gaussian-like distribution centered around zero. This was the case for all of the
activation functions we tested. The behaviour of randomly selected pairs of features is thus reasonably
consistent with the assumption of independence which we have made. We also investigated the
correlation between the pairs of preactivation features which were passing into our two-dimensional
activation functions. Here, we found the correlation structure is different, and the correlation depends"
REFERENCES,0.6811279826464208,Under review as a conference paper at ICLR 2022
REFERENCES,0.6832971800433839,"104
105
106
107
Number of parameters 84 86 88 90 92 94"
REFERENCES,0.6854663774403471,Test accuracy (%)
REFERENCES,0.6876355748373102,"JSB Chorales, MLP (2 hidden layers)"
REFERENCES,0.6898047722342733,"Figure 17: We trained 2-layer MLPs discriminators on JSB Chorales using Adam (constant LR
1 × 10−3, 150 ep.), Mean (bars: std dev) of n=10 weight inits."
REFERENCES,0.6919739696312365," 
 


      "
REFERENCES,0.6941431670281996," 
 


      )"
REFERENCES,0.6963123644251626," 
 


     "
REFERENCES,0.6984815618221258,)!
REFERENCES,0.7006507592190889," 
 


     "
REFERENCES,0.702819956616052,"%!"" !"
REFERENCES,0.7049891540130152," 
 


       AIL"
REFERENCES,0.7071583514099783," 
 


       AIL"
REFERENCES,0.7093275488069414," 
 


       AIL  AIL "
REFERENCES,0.7114967462039046," 
 


       AIL"
REFERENCES,0.7136659436008677, AIL #
REFERENCES,0.7158351409978309," 
 


       AIL"
REFERENCES,0.7180043383947939, AIL 
REFERENCES,0.720173535791757," 
 


       AIL  AIL"
REFERENCES,0.7223427331887202, AIL #
REFERENCES,0.7245119305856833," 
 


       AIL  AIL"
REFERENCES,0.7266811279826464, AIL 
REFERENCES,0.7288503253796096,"""%!% $&%&(!#$&'&""!%%&*$"
REFERENCES,0.7310195227765727," 
 


      "
REFERENCES,0.7331887201735358," 
 


      )"
REFERENCES,0.735357917570499," 
 


     "
REFERENCES,0.737527114967462,)!
REFERENCES,0.7396963123644251," 
 


     "
REFERENCES,0.7418655097613883,"%!"" !"
REFERENCES,0.7440347071583514," 
 


       AIL"
REFERENCES,0.7462039045553145," 
 


       AIL"
REFERENCES,0.7483731019522777," 
 


       AIL  AIL "
REFERENCES,0.7505422993492408," 
 


       AIL"
REFERENCES,0.7527114967462039, AIL #
REFERENCES,0.754880694143167," 
 


       AIL"
REFERENCES,0.7570498915401301, AIL 
REFERENCES,0.7592190889370932," 
 


       AIL  AIL"
REFERENCES,0.7613882863340564, AIL #
REFERENCES,0.7635574837310195," 
 


       AIL  AIL"
REFERENCES,0.7657266811279827, AIL 
REFERENCES,0.7678958785249458,"""%!% $&%&(!#$#$&'&""!%%&*$"
REFERENCES,0.7700650759219089,"Figure 18: Cosine similarities between pre-activation weights of two activation functions in the ﬁrst
layer of an MLP trained on JSB Chorales."
REFERENCES,0.7722342733188721,"on the activation function being used. With Max and ORAIL activations, the network learns to make
the columns of the weight matrix (and hence the preactivation scores for the pair of features) be
inversely correlated. With XNORAIL, the network learns features which are either positively or
negatively correlated (a wider distribution of correlations than seen with random pairs of features).
We observe that (in all cases) the network learns to make the features passed to the AIL activation
functions be correlated instead of independent, despite our assumption of independence. So it appears
clear that the assumption of independence is violated, but also that it doesn’t really matter because
the network is choosing to break the assumption and induce these correlations between the features
to get better performance."
REFERENCES,0.7744034707158352,Under review as a conference paper at ICLR 2022
REFERENCES,0.7765726681127982,"A.11
CNN AND MLP ON MNIST"
REFERENCES,0.7787418655097614,"In this experiment we trained MLP and CNN models for 10 epochs on the MNIST dataset using
Adam optimizer, one-cycle learning rate schedule, and cross entropy loss, with batch size of 256. We
augmented our training samples using a random afﬁne transformation from: rotation of ±10 degrees,
scale of factor 0.8 to 1.2, translation with max absolute fraction for horizontal and vertical directions
of 0.08, and shear parallel to the x-axes of ±0.3."
REFERENCES,0.7809110629067245,"The hyper-parameters for the optimizer and scheduler were selected through a random search of the
hyper-parameter space. We chose to do random search instead of grid search because it typically
yields better results for the same number of test cases."
REFERENCES,0.7830802603036876,"For the hyper-parameter search, we trained on the ﬁrst 50 000 samples of the training partition and
used the ﬁnal 10 000 samples as a validation set. We ran the search for four iterations, with each
iteration sampling 120 different hyper-parameter settings. The initial bounds for our hyper-parameter
samples were set as described in Table 3. These bounds were chosen to be suitably wide such that the
optimal conﬁguration should be contained within them for all activation functions considered."
REFERENCES,0.7852494577006508,Table 3: Hyper-parameter random search parameters.
REFERENCES,0.7874186550976139,"Hyper-parameter
Variable
Sampling (initial bounds)"
REFERENCES,0.789587852494577,"Adam beta1
1 −10x
x ∼Uniform(−3, −0.5)
Adam beta2
1 −10x
x ∼Uniform(−5, −1)
Adam epsilon
10x
x ∼Uniform(−10, −6)
Weight decay
10x
x ∼Uniform(−7, −3)
One-cycle max LR
10x
x ∼Uniform(−4, 0)
One-cycle peak
x
x ∼Uniform(0.1, 0.5)"
REFERENCES,0.7917570498915402,"The bounds on our uniform random variable x were tightened at each iteration by selecting the top-5
performing hyper-parameter settings, taking the mean ¯x and weighted standard deviation σ across
those settings, and re-setting the bounds for the next iteration to be equal to ¯x±1.5 σ. After the fourth
iteration, we ran a ﬁnal iteration where we selected the top-10 performing hyper-parameter settings
across the four previous iterations, and then re-ran these for another 120 seeds (i.e. random weight
initializations). We then selected the hyper-parameter settings which had the highest performance
across all 120 seeds."
REFERENCES,0.7939262472885033,"We found that the XNOR activation function required quite different hyper-parameters than the
other activation functions, but OR and Max activation functions used similar hyper-parameters to
ReLU. However, we have no reason to believe that the proposed AIL activation functions are more
susceptible than others to the choice of hyper-parameters or the way in which hyper-parameters are
selected."
REFERENCES,0.7960954446854663,"Our MLP model consisted of two hidden layers of equal size with batch norm applied to each. The
number of neurons in each layer was set, taking into consideration the current activation function
being tested, to ensure the number of trainable parameters in the network remained static across
experiments"
REFERENCES,0.7982646420824295,"Our CNN model consisted of six layers, where each layer was comprised of a set of 2D convolution
ﬁlters (kernel size 3, stride 1, padding 1), batch normalization, and a non-linear activation. The
network also applied a pooling layer (kernel size 2, stride 2) at the end of every second layer. After
the 6 CNN layers the network ﬂattens the output and applies three linear layers. The number of output
channels (ie. pre-activations) for the six 2D convolutions were [c, 2c, 4c, 4c, 8c, 8c], and the number
of pre-activation neurons produced by the subsequent linear layers were [32c, 16c]. Similar to the
MLP model, c was chosen to ensure the number of trainable parameters in the network remained
ﬁxed across experiments. When varying the number of network parameters in our experiments, if the
number of parameters dropped below 1 × 106 in the CNN model, we changed the structure of the
convolution layers and linear layers to [c, c, c, c, 1.5c, 1.5c] and [2c, 1.5c], respectively. This ensured
that each layer had at least two channels for our activation functions to aggregate."
REFERENCES,0.8004338394793926,"Once our optimal hyper-parameters were selected for both our MLP and CNN models, we then
trained each model several times with varying number of trainable parameters. The reason we vary"
REFERENCES,0.8026030368763557,Under review as a conference paper at ICLR 2022
REFERENCES,0.8047722342733189,"the number of parameters in the network instead of the network size/structure is because a network
using our logical activation functions can have a signiﬁcantly different number of parameters than an
identical network using ReLU activation, because our logical activations aggregate across neurons at
each layer."
REFERENCES,0.806941431670282,"A.12
RESNET50 ON CIFAR-10/100"
REFERENCES,0.8091106290672451,"For this experiment we trained a ResNet50 model for 100 epochs on CIFAR-10 and CIFAR-100 using
Adam optimizer, one-cycle learning rate schedule, cross entropy loss, and augmentations derived for
CIFAR-10 by AutoAugment (Cubuk et al., 2018), with batch size of 128. The hyper-parameters for
the optimizer and scheduler were determined through a random search of the hyper parameter space
on the CIFAR-100 dataset3. The hyper-parameter search was tuned against a partition of 10% of the
CIFAR-100 training samples. Our hyper-parameter search follows a similar approach to the one used
for our MLP and CNN models on MNIST (Appendix A.11), but due to compute constraints with our
larger models here we only trained 100 seeds each iteration, and only re-examined the top 10 seeds in
the ﬁnal round."
REFERENCES,0.8112798264642083,"For PReLU, we did not perform a hyper-parameter search and merely re-used the same hyper-
parameters as discovered in the search with ReLU."
REFERENCES,0.8134490238611713,"Once our optimal hyper-parameters were selected we trained a ResNet50 model with automatic
mixed precision using four different width parameters: 0.5, 1, 2, and 4. Because activation func-
tions such as XNORAIL reduce the number of post-activation neurons by a factor of 2, and the
{OR, AND, XNORAIL (d)} activation function increases the number of post-activation neurons by
a factor of 3/2, we adjust the width parameter for these activation function to result in all networks
having approximately the same number of parameters. For the 2 →1 activation functions (Max,
ORAIL, XNORAIL, {OR, XNORAIL (p)}, and {OR, AND, XNORAIL (p)}) we use width values
of 0.75, 1.5, 3, and 6. For the 2→3 activation function {OR, AND, XNORAIL (d)}, we use width
values of 0.4, 0.75, 1.5, and 3."
REFERENCES,0.8156182212581344,"A.13
TRANSFER LEARNING"
REFERENCES,0.8177874186550976,"We used a pretrained ResNet50 model taken from the pytorch hub. The 2-layer MLP head was trained
for 25 epochs on each dataset. Since we are interested in transfer learning in a data-limited regime
for these experiments, we used only a small amount of data augmentation. We applied horizontal ﬂip
(p=0.5), scaling (0.7 to 1), aspect ratio stretching (3/4 to 4/3), and colour jitter (intensity 0.4) only.
Pixel intensity normalization was done against the ImageNet mean and std."
REFERENCES,0.8199566160520607,"The MLP head was optimized using SGD, momentum 0.9. We used a batch size of 128, maximum
learning rate 0.01, and weight decay 1 × 10−4. Before training began, we passed one epoch worth of
inputs through the network without updating the weights in order to refresh the batch normalization
statistics to the new dataset. We also performed one epoch of training with a warmup learning rate of
1 × 10−5 before commencing training at the maximum learning rate of 0.01. The learning rate was
decayed with a cosine annealing schedule over 24 epochs. We report the performance of the ﬁnal
model at the end of the 25 epochs."
REFERENCES,0.8221258134490239,"We used a pre-activation width of 512 neurons for ReLU and other and activation functions which
map 1 →1. To approximately match up the number of parameters, we used a width of 650 for
activation functions which map 2→1, and 438 for {OR, AND, XNORAIL (d)} which maps features
3→2. These values control the total number of parameters in the head to be the same for datasets
with 100 classes (the median number of classes in the datasets we considered). Our results with
widths 438/512/650 and ∼600k parameters are shown in Table 1. The precise widths and number of
trainable parameters used for these experiments are shown in Table 4."
REFERENCES,0.824295010845987,"Since the transfer learning experiments were performed without retraining the base network, we
found (Table 1) the performance is limited by the features which the base network produces, whose
relevance depends on the other lap between the domain of the pretraining task (ImageNet) and the
new task. This information bottleneck, and variation in its utility, means the variation between"
REFERENCES,0.8264642082429501,"3We used the same set of hyper-parameters from this search for both the CIFAR-10 and CIFAR-100
experiments."
REFERENCES,0.8286334056399133,Under review as a conference paper at ICLR 2022
REFERENCES,0.8308026030368764,"performance on different datasets is much larger than the variation for different activation functions
within the same dataset."
REFERENCES,0.8329718004338394,"For transfer tasks which involve coarse-grained discrimination on images which are similar to
ImageNet, the embedding generated by the pretrained model is already sufﬁcient to separate the
classes in the new dataset. Examples of this are Caltech101, where linear layer beats out using
additional layers with non-linearities; and STL-10, where our best model is on-par with the linear
model. The results on these two transfer learning tasks are too simple to be of interest to study.
The performance is limited by the information retained by the pretrained embedding, but it appears
that what task-relevant information is there is readily available with a linear layer without needing
additional logic to interpret it."
REFERENCES,0.8351409978308026,"Other transfer learning tasks we attempted are less trivial and show a larger difference in performance
across the activation functions, and a gap from the linear layer readout model. The Stanford
Cars dataset involves ﬁne-grained discrimination between different car models, for which features
generated by a model pretrained on ImageNet are not effective. The SVHN dataset, which contains
images of house numbers, is coarse-grained but uses images which are outside the domain of
ImageNet, which makes the transfer-learning task more difﬁcult. Additionally, our use of random
horizontal reﬂections during training will have hindered performance on this dataset by notably
increasing the task complexity, since numerals are not invariant under reﬂection."
REFERENCES,0.8373101952277657,"Table 4: Hidden pre-activation widths and number of trainable parameters used in transfer learning
experiments (corresponding to results shown in Table 1)."
REFERENCES,0.8394793926247288,№Trainable Parameters
REFERENCES,0.841648590021692,"Activation function
Map
Width
Caltech101
CIFAR10
CIFAR100
Flowers
Cars
STL-10
SVHN"
REFERENCES,0.8438177874186551,"Linear layer only
52k
5k
51k
52k
101k
5k
5k"
REFERENCES,0.8459869848156182,"ReLU
1→1
512
577k
530k
577k
578k
626k
530k
530k
LeakyReLU
1→1
512
577k
530k
577k
578k
626k
530k
530k
PReLU
1→1
512
577k
530k
577k
578k
626k
530k
530k
Softplus
1→1
512
577k
530k
577k
578k
626k
530k
530k"
REFERENCES,0.8481561822125814,"ELU
1→1
512
577k
530k
577k
578k
626k
530k
530k
CELU
1→1
512
577k
530k
577k
578k
626k
530k
530k
SELU
1→1
512
577k
530k
577k
578k
626k
530k
530k
GELU
1→1
512
577k
530k
577k
578k
626k
530k
530k
SiLU
1→1
512
577k
530k
577k
578k
626k
530k
530k
Hardswish
1→1
512
577k
530k
577k
578k
626k
530k
530k
Mish
1→1
512
577k
530k
577k
578k
626k
530k
530k"
REFERENCES,0.8503253796095445,"Softsign
1→1
512
577k
530k
577k
578k
626k
530k
530k
Tanh
1→1
512
577k
530k
577k
578k
626k
530k
530k"
REFERENCES,0.8524945770065075,"GLU
2→1
650
578k
549k
578k
579k
609k
549k
549k"
REFERENCES,0.8546637744034707,"Max
2→1
650
578k
549k
578k
579k
609k
549k
549k
Max, Min (d)
1→1
512
577k
530k
577k
578k
626k
530k
530k"
REFERENCES,0.8568329718004338,"XNORAIL
2→1
650
578k
549k
578k
579k
609k
549k
549k
ORAIL
2→1
650
578k
549k
578k
579k
609k
549k
549k
OR, ANDAIL (d)
1→1
512
577k
530k
577k
578k
626k
530k
530k
OR, XNORAIL (p)
2→1
650
578k
549k
578k
579k
609k
549k
549k
OR, XNORAIL (d)
1→1
512
577k
530k
577k
578k
626k
530k
530k
OR, AND, XNORAIL (p)
2→1
650
578k
549k
578k
579k
609k
549k
549k
OR, AND, XNORAIL (d)
2→3
438
579k
519k
579k
580k
642k
519k
519k"
REFERENCES,0.8590021691973969,"Additionally, we ran the experiment again with a width of w = 512 for the 2→1 activation functions.
The results with constant pre-activation width w = 512 are shown in Table 5. By using a constant
pre-activation width, the number of parameters used is not consistent across activation functions. The
number of trainable parameters in each experiment is shown in tab:transfer-nparam-512."
REFERENCES,0.8611713665943601,"We found that reducing the width from 650 to 512 (and hence reducing total number of trainable param-
eters) reduced the performance of 2→1 activation functions XNORAIL, ORAIL, {OR, XNORAIL
(p)}, and {OR, AND, XNORAIL (p)}. Increasing the width from 438 to 512 increased the perfor-
mance of {OR, AND, XNORAIL (d)}."
REFERENCES,0.8633405639913232,Under review as a conference paper at ICLR 2022
REFERENCES,0.8655097613882863,"Table 5: Transfer learning with from a frozen ResNet-18 architecture pretrained on ImageNet-1k to
other computer vision datasets. As with Table 1, but in this case we show results where the MLP has
a pre-activation width of w=512. Note that although the pre-activation width is constant, the number
of parameters in the network is not consistent between experiments. The number of parameters is
shown in Table 6. Mean (standard error) of n=5 inits of the MLP (same pretrained network)."
REFERENCES,0.8676789587852495,Test Accuracy (%)
REFERENCES,0.8698481561822126,"Activation function
Caltech101
CIFAR10
CIFAR100
Flowers
Cars
STL-10
SVHN"
REFERENCES,0.8720173535791758,"Linear layer only
88.35±0.15
78.56±0.09
57.39±0.09
92.32±0.20
33.51±0.06
94.68±0.02
45.42±0.06"
REFERENCES,0.8741865509761388,"ReLU
86.58±0.17
81.63±0.05
58.04±0.11
90.71±0.26
30.97±0.26
94.62±0.06
51.39±0.06
LeakyReLU
86.60±0.13
81.67±0.11
58.01±0.09
90.73±0.32
31.09±0.24
94.61±0.05
51.40±0.05
PReLU
87.83±0.21
81.03±0.13
58.90±0.18
93.17±0.19
39.84±0.18
94.54±0.05
51.42±0.09
Softplus
86.16±0.18
79.13±0.08
56.58±0.07
89.39±0.29
21.23±0.13
94.63±0.03
47.44±0.06"
REFERENCES,0.8763557483731019,"ELU
87.18±0.09
80.44±0.08
58.08±0.10
91.71±0.14
34.70±0.06
94.55±0.05
50.07±0.07
CELU
87.18±0.09
80.44±0.08
58.08±0.10
91.71±0.14
34.70±0.06
94.55±0.05
50.07±0.07
SELU
87.74±0.09
79.93±0.13
58.24±0.06
92.27±0.13
37.51±0.17
94.53±0.07
49.38±0.06
GELU
87.10±0.15
81.39±0.09
58.51±0.13
91.51±0.15
33.43±0.15
94.62±0.06
51.56±0.08
SiLU
86.91±0.11
80.53±0.11
58.14±0.12
91.37±0.18
32.15±0.17
94.59±0.05
50.69±0.09
Hardswish
87.12±0.12
80.10±0.10
58.25±0.10
91.56±0.25
33.17±0.23
94.62±0.05
50.09±0.09
Mish
87.11±0.12
81.09±0.11
58.37±0.10
91.61±0.15
33.75±0.14
94.61±0.05
51.29±0.08"
REFERENCES,0.8785249457700651,"Softsign
81.47±0.18
80.03±0.09
54.84±0.09
82.34±0.22
17.33±0.10
94.70±0.03
49.48±0.07
Tanh
87.48±0.06
80.56±0.07
57.35±0.08
90.32±0.20
29.51±0.12
94.63±0.07
50.15±0.08"
REFERENCES,0.8806941431670282,"GLU
86.34±0.16
79.35±0.05
57.22±0.12
90.10±0.20
26.72±0.13
94.63±0.05
48.51±0.09"
REFERENCES,0.8828633405639913,"Max
86.86±0.11
81.56±0.06
58.12±0.10
90.59±0.25
32.80±0.04
94.65±0.05
51.15±0.08
Max, Min (d)
87.23±0.13
82.31±0.10
59.05±0.10
91.68±0.18
34.91±0.12
94.64±0.04
51.72±0.04"
REFERENCES,0.8850325379609545,"XNORAIL
86.42±0.13
81.74±0.05
57.88±0.09
90.50±0.17
31.55±0.11
94.78±0.04
51.23±0.10
ORAIL
87.28±0.09
81.81±0.02
58.68±0.05
91.78±0.23
35.28±0.09
94.67±0.07
51.27±0.12
OR, ANDAIL (d)
87.43±0.11
82.38±0.06
59.90±0.08
92.07±0.18
37.16±0.15
94.55±0.05
52.11±0.09
OR, XNORAIL (p)
87.26±0.06
81.85±0.06
58.72±0.09
91.80±0.24
35.27±0.09
94.64±0.07
51.25±0.14
OR, XNORAIL (d)
87.09±0.21
82.20±0.04
59.44±0.07
91.90±0.10
36.88±0.10
94.69±0.06
52.02±0.16
OR, AND, XNORAIL (p)
87.03±0.19
81.75±0.07
58.72±0.08
91.85±0.14
35.46±0.14
94.69±0.04
51.32±0.09
OR, AND, XNORAIL (d)
87.26±0.15
82.45±0.08
60.20±0.11
92.41±0.17
37.89±0.14
94.65±0.04
52.19±0.09"
REFERENCES,0.8872017353579176,"Table 6: Number of trainable parameters used in transfer learning experiments (corresponding to
results shown in Table 5)."
REFERENCES,0.8893709327548807,№Trainable Parameters
REFERENCES,0.8915401301518439,"Activation function
Map
Width
Caltech101
CIFAR10
CIFAR100
Flowers
Cars
STL-10
SVHN"
REFERENCES,0.8937093275488069,"Linear layer only
52k
5k
51k
52k
101k
5k
5k"
REFERENCES,0.89587852494577,"ReLU
1→1
512
577k
530k
577k
578k
626k
530k
530k
LeakyReLU
1→1
512
577k
530k
577k
578k
626k
530k
530k
PReLU
1→1
512
577k
530k
577k
578k
626k
530k
530k
Softplus
1→1
512
577k
530k
577k
578k
626k
530k
530k"
REFERENCES,0.8980477223427332,"ELU
1→1
512
577k
530k
577k
578k
626k
530k
530k
CELU
1→1
512
577k
530k
577k
578k
626k
530k
530k
SELU
1→1
512
577k
530k
577k
578k
626k
530k
530k
GELU
1→1
512
577k
530k
577k
578k
626k
530k
530k
SiLU
1→1
512
577k
530k
577k
578k
626k
530k
530k
Hardswish
1→1
512
577k
530k
577k
578k
626k
530k
530k
Mish
1→1
512
577k
530k
577k
578k
626k
530k
530k"
REFERENCES,0.9002169197396963,"Softsign
1→1
512
577k
530k
577k
578k
626k
530k
530k
Tanh
1→1
512
577k
530k
577k
578k
626k
530k
530k"
REFERENCES,0.9023861171366594,"GLU
2→1
512
420k
397k
420k
420k
445k
397k
397k"
REFERENCES,0.9045553145336226,"Max
2→1
512
420k
397k
420k
420k
445k
397k
397k
Max, Min (d)
1→1
512
577k
530k
577k
578k
626k
530k
530k"
REFERENCES,0.9067245119305857,"XNORAIL
2→1
512
420k
397k
420k
420k
445k
397k
397k
ORAIL
2→1
512
420k
397k
420k
420k
445k
397k
397k
OR, ANDAIL (d)
1→1
512
577k
530k
577k
578k
626k
530k
530k
OR, XNORAIL (p)
2→1
512
420k
397k
420k
420k
445k
397k
397k
OR, XNORAIL (d)
1→1
512
577k
530k
577k
578k
626k
530k
530k
OR, AND, XNORAIL (p)
2→1
512
420k
397k
420k
420k
445k
397k
397k
OR, AND, XNORAIL (d)
2→3
512
734k
664k
733k
735k
807k
664k
664k"
REFERENCES,0.9088937093275488,Under review as a conference paper at ICLR 2022
REFERENCES,0.911062906724512,"A.14
ABSTRACT REASONING"
ABSTRACT,0.913232104121475,"Abstract reasoning is challenging for neural networks to learn because their structure and objective
function are more effective for tasks which come instinctively to humans (“System 1” of Kahneman,
2011), such as object recognition, as opposed to demanding (“System 2”) logic tasks."
ABSTRACT,0.9154013015184381,"In recent years, several challenges have been proposed to evaluate the ability of neural networks
to perform abstract reasoning. The Raven’s Progressive Matrices (Raven & Court, 1938) are a
long-standing IQ test, which have been emulated by Barrett et al. (2018) with Procedurally Generated
Matrices (PGM), and then the RAVEN task by (Zhang et al., 2019). Recently Hu et al. (2020) and
Benny et al. (2021) have improved on that task with I-RAVEN and RAVEN-FAIR, respectively, both
aiming to make the task more balanced. Other abstract reasoning tasks have also been proposed
(Fleuret et al., 2011; Johnson et al., 2017; Barrett et al., 2018)."
ABSTRACT,0.9175704989154013,"We considered the application of AIL activation functions in the context of the I-RAVEN task,
by adapting the Stratiﬁed Rule-Aware Network (SRAN) of Hu et al. (2020) to include our AIL
activation functions. We ﬁrst added LayerNorm to the network, and then swapped out the seven
ReLU activations in the gating module. The architecture for the three ResNet-18 (He et al., 2015)
base models were unchanged. Where necessary, the number of units per layer was modiﬁed to
facilitate the change in dimensionality caused by our activation function. The networks were trained
using the same procedure as described by Hu et al. (2020)."
ABSTRACT,0.9197396963123644,"Table 7: Performance of SRAN-based models on the I-RAVEN dataset (Hu et al., 2020). Bold: best.
Underlined: second best. Background: color scale from worst in to best, linear with accuracy value."
ABSTRACT,0.9219088937093276,I-RAVEN Test Acc (%)
ABSTRACT,0.9240780911062907,"Activation function
Params
Acc
Center
2×2G
3×3G
O-IC
O-IG
L-R
U-D"
ABSTRACT,0.9262472885032538,"ReLU, Base SRAN (Hu et al., 2020)
44.0M
60.8
78.2
50.1
42.4
68.2
46.3
70.1
70.3
ReLU, SRAN+LayerNorm
45.6M
63.0
84.0
50.0
42.5
69.9
48.3
73.5
72.3
PReLU
45.6M
54.5
69.2
45.4
40.5
64.4
47.6
57.5
57.3"
ABSTRACT,0.928416485900217,"CELU
45.6M
56.6
73.5
46.5
41.0
66.6
46.1
62.8
59.7
SELU
45.6M
53.5
67.2
43.8
40.1
62.8
44.5
58.0
58.1
GELU
45.6M
61.4
80.8
49.2
42.5
69.2
48.3
70.3
69.2
SiLU
45.6M
59.4
78.0
47.2
41.4
66.3
47.8
69.1
66.2"
ABSTRACT,0.93058568329718,"Max
44.6M
57.8
76.3
45.2
39.7
65.3
48.7
64.6
64.7
Max, Min (d)
45.6M
60.2
80.2
49.5
41.9
66.5
47.0
68.5
67.7"
ABSTRACT,0.9327548806941431,"XNORAIL
44.6M
57.7
74.7
46.0
40.0
66.8
47.7
65.6
63.2
ORAIL
44.6M
64.3
84.4
49.5
44.0
71.5
47.1
76.5
77.0
OR, ANDAIL (d)
45.6M
57.5
74.7
45.6
41.3
68.3
44.9
64.5
63.0
OR, XNORAIL (p)
44.6M
59.8
80.5
45.8
41.4
67.2
48.2
67.5
68.1
OR, XNORAIL (d)
45.6M
62.8
83.7
49.1
43.3
68.1
49.5
73.8
72.2
OR, AND, XNORAIL (p)
44.6M
55.0
68.2
47.2
41.1
65.0
45.0
61.0
57.5"
ABSTRACT,0.9349240780911063,"We found the network using ORAIL activation function performed best overall, and across most of
the subtasks. The second best performing activation functions were {ORAIL, XNORAIL (d)} and
ReLU."
ABSTRACT,0.9370932754880694,"A.15
COMPOSITIONAL ZERO-SHOT LEARNING"
ABSTRACT,0.9392624728850325,"Zero-shot learning encompasses all problems which involve completing novel tasks which the subject
has never seen before. The subject must infer both the task and its solution based on their previous
experiences (a meta-learning task). Compositional zero-shot learning is a subset of zero-shot learning
which involves combining knowledge about multiple stimulus properties together in novel pairings.
For instance, if the network has been trained on “sliced bread,” “sliced pear,” and “caramelized pear,”
is it able to classify images of “caramelized bread” despite having never seen an example of this
before?"
ABSTRACT,0.9414316702819957,"We based our experiments on the Task-driven Modular Networks (TMN) proposed by Purushwalkam
et al. (2019). We used the code shared by the authors, but were unable to replicate the results they
reported in the paper when using a different random seed (see Table 8). We adapted this network
by changing out all the ReLU activation functions in the gate and module networks with a different"
ABSTRACT,0.9436008676789588,Under review as a conference paper at ICLR 2022
ABSTRACT,0.9457700650759219,"activation function. Because the modules each terminate with an activation function, we needed to
double the size of the hidden layer for some of our networks in order to maintain the dimensionality
of the output. Consequently, some experiments had around 50% more parameters in total than others."
ABSTRACT,0.9479392624728851,"Experiments were performed on the MIT-States dataset (Isola et al., 2015).We trained and tested on
the corresponding partitions of the dataset as introduced by Purushwalkam et al. (2019). We used
the same paradigm as Purushwalkam et al. (2019): Adam (Kingma & Ba, 2017) with a learning rate
of 0.001 for the module network and 0.01 for the gating network, momentum 0.9, batch size 256,
weight decay 5 × 10−5. We evaluated the network using the AUC between seen and unseen samples
(Purushwalkam et al., 2019). The network was trained until the validation AUC had plateaued,
determined by not increasing for 5 epochs. We selected the model from the epoch with highest
validation AUC to apply to the test set. The best performance was typically attained after around 5
epochs."
ABSTRACT,0.9501084598698482,"As shown in Table 8, we ﬁnd that the trio of activation functions applied in parallel,
{OR, AND, XNORAIL (p)}, performs best."
ABSTRACT,0.9522776572668112,"Table 8: Performance of TMN-based networks at compositional zero-shot learning (CZSL) on the
MIT-States dataset. Mean (standard error) of n=5 random initializations."
ABSTRACT,0.9544468546637744,MIT-States Test AUC (%)
ABSTRACT,0.9566160520607375,"Activation function
Mapping
Params
Top-1
Top-2
Top-3"
ABSTRACT,0.9587852494577006,"TMN (Purushwalkam et al., 2019)
1→1
2.9
7.1
11.5
TMN repro. (ReLU)
1→1
438 k
2.47 ± 0.07
6.42 ± 0.09
10.27 ± 0.11"
ABSTRACT,0.9609544468546638,"Max
2→1
650 k
2.42 ± 0.07
6.37 ± 0.08
10.28 ± 0.06
Max, Min (d)
1→1
438 k
2.53 ± 0.06
6.69 ± 0.11
10.61 ± 0.14"
ABSTRACT,0.9631236442516269,"XNORAIL
2→1
650 k
1.22 ± 0.05
3.47 ± 0.12
5.82 ± 0.19
ORAIL
2→1
650 k
2.65 ± 0.05
6.80 ± 0.08
10.78 ± 0.13
OR, ANDAIL (d)
1→1
438 k
2.61 ± 0.05
6.73 ± 0.05
10.77 ± 0.12
OR, XNORAIL (p)
2→1
650 k
2.65 ± 0.05
6.80 ± 0.08
10.78 ± 0.13
OR, XNORAIL (d)
1→1
438 k
1.89 ± 0.13
5.12 ± 0.21
8.35 ± 0.24
OR, AND, XNORAIL (p)
2→1
650 k
2.67 ± 0.10
6.95 ± 0.14
10.96 ± 0.16"
ABSTRACT,0.96529284164859,"Since we could not ﬂexibly scale the total number of parameters in the network with the original
architecture, we modiﬁed the TMN architecture by adding an additional linear layer to the end of
each module which projects from the activation function to the embedding space. The modules
would otherwise terminate with an activation function, which makes it difﬁcult to handle activation
functions which map 2→1. We dub the modiﬁed version of the network “TMNx.”"
ABSTRACT,0.9674620390455532,"Comparing the TMN results in Table 8 to TMNx in Table 9, we can see that adding the extra linear
layer improved performance of the network in of itself. Intuitively, this makes sense since the output
of the TMN modules are weighted with the output of the gating network and then summed, and this
weighting and summing of evidence is best performed with on logits instead of the truncated output
of ReLU units. But also, performance may have improved just because the model became larger."
ABSTRACT,0.9696312364425163,"We performed a hyperparameter search across the training parameters for the new network against the
validation set using the ReLU activation function only. We adopted the hyperparameters discovered
for ReLU for all other activation functions. The batch size was reduced to 128 due to the increase
in size of the model. The discovered hyperparameters were a learning rate of 3 × 10−3 for both the
module and gating network, and a weight decay of 1 × 10−5. Other training hyperparameters, such
as the ratio of negative samples to present, were left unchanged."
ABSTRACT,0.9718004338394793,"In order to match the number of parameters in the network, we used the original TMN hidden width of
64 for the module and gater networks with activation functions which map 1→1, and increased this
to 70 for activation function which map 2→1, to maintain the total number of trainable parameters.
To investigate a comprehensive set of baselines, we compared against every activation function
implemented in PyTorch v1.10. The results are shown in Table 9."
ABSTRACT,0.9739696312364425,"We found that the ELU family of activations (ELU, CELU, SELU) performed best across
all three top-k values.
Our best activation functions were {OR, ANDAIL (d)}, followed by
{OR, AND, XNORAIL (p)}, which outperformed ReLU. The other AIL activation functions we"
ABSTRACT,0.9761388286334056,Under review as a conference paper at ICLR 2022
ABSTRACT,0.9783080260303688,"Table 9: Performance of TMNx networks at compositional zero-shot learning (CZSL) on the MIT-
States dataset. Pre-activation width of 64 or 70 (depending on activation function, to keep the number
of parameters approximately constant). Mean (standard error) of n=5 random initializations. Bold:
best. Underlined: second best. Italic: no signiﬁcant difference from best (two-sided Student’s t-test,
p>0.05). Background: color scale from second-worst in column to best, linear with accuracy value."
ABSTRACT,0.9804772234273319,MIT-States Test Accuracy (%)
ABSTRACT,0.982646420824295,"Activation function
Mapping
Params
Top-1
Top-2
Top-3"
ABSTRACT,0.9848156182212582,"ReLU
1→1
1.15M
2.76 ± 0.08
7.11 ± 0.08
11.26 ± 0.09
LeakyReLU
1→1
1.15M
2.72 ± 0.09
6.99 ± 0.17
10.95 ± 0.20
PReLU
1→1
1.15M
2.83 ± 0.06
7.25 ± 0.14
11.44 ± 0.17
Softplus
1→1
1.15M
2.92 ± 0.11
7.46 ± 0.19
11.75 ± 0.18"
ABSTRACT,0.9869848156182213,"ELU
1→1
1.15M
3.13 ± 0.05
7.78 ± 0.14
12.04 ± 0.20
CELU
1→1
1.15M
3.17 ± 0.06
7.81 ± 0.15
12.00 ± 0.19
SELU
1→1
1.15M
3.05 ± 0.04
7.77 ± 0.10
12.37 ± 0.16
GELU
1→1
1.15M
2.81 ± 0.06
7.19 ± 0.11
11.34 ± 0.15
SiLU
1→1
1.15M
2.87 ± 0.08
7.34 ± 0.14
11.60 ± 0.18
Hardswish
1→1
1.15M
2.84 ± 0.08
7.23 ± 0.11
11.48 ± 0.11
Mish
1→1
1.15M
2.90 ± 0.08
7.37 ± 0.17
11.59 ± 0.13"
ABSTRACT,0.9891540130151844,"Softsign
1→1
1.15M
2.90 ± 0.04
7.28 ± 0.09
11.53 ± 0.18
Tanh
1→1
1.15M
2.93 ± 0.09
7.47 ± 0.16
11.64 ± 0.20"
ABSTRACT,0.9913232104121475,"GLU
2→1
1.16M
2.59 ± 0.06
6.86 ± 0.15
10.91 ± 0.24"
ABSTRACT,0.9934924078091106,"Max
2→1
1.16M
2.45 ± 0.08
6.52 ± 0.18
10.53 ± 0.29
Max, Min (d)
1→1
1.15M
2.53 ± 0.10
6.61 ± 0.15
10.65 ± 0.15"
ABSTRACT,0.9956616052060737,"XNORAIL
2→1
1.16M
1.88 ± 0.06
4.94 ± 0.15
7.99 ± 0.19
ORAIL
2→1
1.16M
2.61 ± 0.08
6.83 ± 0.12
10.97 ± 0.20
OR, ANDAIL (d)
1→1
1.15M
2.81 ± 0.06
7.12 ± 0.10
11.27 ± 0.14
OR, XNORAIL (p)
2→1
1.16M
2.61 ± 0.08
6.83 ± 0.12
10.97 ± 0.20
OR, XNORAIL (d)
1→1
1.15M
2.46 ± 0.04
6.51 ± 0.11
10.45 ± 0.17
OR, AND, XNORAIL (p)
2→1
1.16M
2.72 ± 0.11
7.15 ± 0.19
11.32 ± 0.22
OR, AND, XNORAIL (d)
2→3
1.22M
2.73 ± 0.10
6.95 ± 0.16
11.03 ± 0.09"
ABSTRACT,0.9978308026030369,"considered peformed less well, and XNOR performed particularly poorly on this task. This suggests
this is a domain where logical activation functions are less well suited."
