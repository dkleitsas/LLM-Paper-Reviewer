Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005376344086021506,"In this paper we develop a novel regularization method for deep neural networks
by penalizing the trace of Hessian. This regularizer is motivated by a recent guar-
antee bound of the generalization error. Hutchinson method is a classical unbiased
estimator for the trace of a matrix, but it is very time-consuming on deep learn-
ing models. Hence a dropout scheme is proposed to efﬁciently implements the
Hutchinson method. Then we discuss a connection to linear stability of a nonlin-
ear dynamical system. Experiments demonstrate that our method outperforms ex-
isting regularizers such as Jacobian, conﬁdence penalty, and label smoothing. Our
regularization method is also orthogonal to data augmentation methods, achieving
the best performance when our method is combined with data augmentation."
INTRODUCTION,0.010752688172043012,"1
INTRODUCTION"
INTRODUCTION,0.016129032258064516,"Deep neural networks (DNNs) are developing rapidly and are widely used in many ﬁelds such as
image classiﬁcation, machine translation, language modeling and speech recognition. As more and
more models are proposed in the literature, deep neural networks have shown remarkable improve-
ments in performance. However, among various learning problems, over-ﬁtting on training data is
a great problem that affects the test accruacy. So a certain regularization method is often needed in
the training process."
INTRODUCTION,0.021505376344086023,"In linear models, Ridge Regression (Hoerl & Kennard, 1970) and Lasso (Tibshirani, 1996) are usu-
ally used to avoid over-ﬁtting. They are also called L2 and L1 regularization. L2 regularization
has the effect of shrinkage while L1 regularization can be conductive to both shrinkage and spar-
sity. From the Bayesian perspective, L2 and L1 regularization can be interpreted with normal prior
distribution and laplace prior distribution respectively."
INTRODUCTION,0.026881720430107527,"Apart from L2 and L1 regularization, there are many other forms of regularizers in DNNs. The
most widely used one is Weight-Decay (Krogh & Hertz, 1992). Loshchilov & Hutter (2019) also
showed that L2 regularization and Weight-Decay are not identical. Dropout (Srivastava et al., 2014)
is another method to avoid over-ﬁtting by reducing co-adapting between units in neural networks.
Dropout has inspired a large body of work studying its effects (Wager et al. (2013); Helmbold
& Long (2015); Wei et al. (2020)). After dropout, various regularization schemes can be applied
additionally."
INTRODUCTION,0.03225806451612903,"In this paper, we propose a new regularization by penalizing the trace of second derivative of loss
function. We refer to our regularization method as Stochastic Estimators of Hessian Trace (SEHT).
On one hand, our hessian regularization is valuable to guarantee good generalization. On the other
hand, from the perspective of dynamical system, it inﬂuences the stability of the system, in which
parameters move in the parameter space on the basis of training data. In our experiments, Hessian
regularization shows competing test performance and low time consumption with our stochastic
algorithm."
RELATED WORK,0.03763440860215054,"2
RELATED WORK"
RELATED WORK,0.043010752688172046,"There are many regularization methods in previous work. Label Smoothing (Szegedy et al., 2016)
estimates the marginalized effect of label-dropout and reduces over-ﬁtting by preventing a network
from assigning full probability to each training example. Conﬁdence Penalty (Pereyra et al., 2017)"
RELATED WORK,0.04838709677419355,Under review as a conference paper at ICLR 2022
RELATED WORK,0.053763440860215055,"prevents peaked distributions, leading to better generalization. A network appears to be overconﬁ-
dent when it places all probability on a single class in the training set, which is often a symptom
of over-ﬁtting. DropBlock (Ghiasi et al., 2018) is a structured form of dropout, it drops contiguous
regions from a feature map of a layer instead of dropping out independent random units."
RELATED WORK,0.05913978494623656,"Data augmentation methods are also used in practice to improve model’s accuracy and robustness
when training neural networks. Cutout (DeVries & Taylor, 2017) is a data augmentation method
where parts of the input examples are zeroed out, in order to encourage the network to focus more
on less prominent features, then generalize to situations like occlusion. Mixup (Zhang et al., 2017)
extends the training distribution by incorporating the prior knowledge that linear interpolations of
feature vectors should lead to linear interpolations of the associated targets."
RELATED WORK,0.06451612903225806,"Sokoli´c et al. (2017) ﬁrst proposed Jacobian regularization, a method focusing on the norm of Ja-
cobian matrix with respect to input data. It was proved that generalization error can be bounded
by the norm of Jacobian matrix. Besides that, Jacobian matrix shows improved stability of the
model predictions against input perturbations according to Taylor expansion. Hoffman et al. (2019)
showed that Jacobian regularization enlarges the size of decision cells and is practically effective in
improving the generalization error and robustness of the models. To simplify calculation, stochastic
algorithm of Jacobian regularization was also proposed."
RELATED WORK,0.06989247311827956,"Motivated by Jacobian regularization, we consider the generalization error and stability of the model
respect to Hessian matrix. Then we combine Linear Stability Analysis and propose Hessian regu-
larization with corresponding stochastic algorithms. We compare our Hessian regularization with
other methods and demonstrate promising performance in experiments. The main idea to estimate
the trace of Hessian matrix is Hutchinson Method(Avron & Toledo, 2011) and the algorithm was
also discussed by Yao et al. (2020a). We make an improvement by designing a new probability
distribution to dropout parameters which decrease time consumption obviously without losing gen-
eralization."
RELATED WORK,0.07526881720430108,"Hessian information is powerful tool used on analyzing the property of neural networks. Yao et al.
(2020b) designed AdaHessian, a second order stochastic optimization algorithm. Yu et al. (2021)
used Hessian trace to measure sensitivity, developing a Hessian Aware Pruning method to ﬁnd in-
sensitive parameters in a neural network model and a Neural Implant technique to alleviate accuracy
degradation. However, their methods are static in essence, we focus on dynamical motion of param-
eters in parameter space. Sankar et al. (2021) also proposed a Hessian regularization. They forcused
on the layerwise loss landscape via the eigenspectrum of the Hessian at each layer. We start from
different perspectives, generalization error and dynamical system of parameters. Our experiments
also shows better results than Sankar et al. (2021)’s method."
HESSIAN REGULARIZATION,0.08064516129032258,"3
HESSIAN REGULARIZATION"
HESSIAN REGULARIZATION,0.08602150537634409,"Here we introduce a Hessian regularization method based on generalization error and corresponding
stochastic algorithms in details. Then we discuss linear stability analysis to explain why Hessian
regularization can prevent neural networks from over-ﬁtting."
TRACE OF HESSIAN MATRIX,0.0913978494623656,"3.1
TRACE OF HESSIAN MATRIX"
TRACE OF HESSIAN MATRIX,0.0967741935483871,"In this study, we consider a multi-class classiﬁcation problem. Input x is a N-dimensional vector,
where x ∈X ⊆RN, with X is the input space. Y = {1, 2, ..., M} is the label space, which
means that we have M classes. Each input x has a label y ∈Y. Sample space is deﬁned as
S = X × Y. An element of S is denoted by s = (x, y). We assume that samples s from S are
drawn according to a probability distribution P. A training set of n samples drawn from P is denoted
by Sn = {si}n
i=1 = {(xi, yi)}n
i=1."
TRACE OF HESSIAN MATRIX,0.10215053763440861,"Our goal is to ﬁnd a classiﬁcation function f, which takes x ∈RN as input and outputs z = f(x).
z is a M-dimensional score vector, where each element zi is the score x belonging to category
i ∈Y. The highest score indicates the most probable label. So the estimated label is given as
ˆy = g(x)= arg maxi zi = arg maxi∈Y f(x)i."
TRACE OF HESSIAN MATRIX,0.10752688172043011,Under review as a conference paper at ICLR 2022
TRACE OF HESSIAN MATRIX,0.11290322580645161,"A loss function is used to measure the discrepancy between the true label y and the estimated label
g(x). In this paper, we use the cross-entropy loss,"
TRACE OF HESSIAN MATRIX,0.11827956989247312,"ℓ(f(x), y) = −log
exp(f(x)y)
P"
TRACE OF HESSIAN MATRIX,0.12365591397849462,"y′ exp(f(x)y′).
(1)"
TRACE OF HESSIAN MATRIX,0.12903225806451613,The empirical loss of the classiﬁer f(x) associated with the training set is deﬁned as
TRACE OF HESSIAN MATRIX,0.13440860215053763,"ℓemp(f) = ˆE[ℓ(f(x), y)] = 1 n X"
TRACE OF HESSIAN MATRIX,0.13978494623655913,"si∈Sn
ℓ(f(xi), yi),
(2)"
TRACE OF HESSIAN MATRIX,0.14516129032258066,and the expected loss of the classiﬁer f(x) is deﬁned as
TRACE OF HESSIAN MATRIX,0.15053763440860216,"ℓexp(f) = E[ℓ(f(x), y)] = Es∼P [ℓ(f(x), y)],
(3)"
TRACE OF HESSIAN MATRIX,0.15591397849462366,then the difference between ℓemp(f) and ℓexp(f) is called generalization error:
TRACE OF HESSIAN MATRIX,0.16129032258064516,"GE(f) = ||ℓexp(f) −ℓemp(f)||.
(4)"
TRACE OF HESSIAN MATRIX,0.16666666666666666,"Wei et al. (2020) showed generalization bound of linear models with cross-entropy loss of M classes.
Let W is the weight matrix, µ(W ) = ˆE[∥J∥2] and v(W ) = ˆE[tr(H)]. For linear models, the"
TRACE OF HESSIAN MATRIX,0.17204301075268819,Jacobian matrix J is a vector deﬁned as ∂ℓ
TRACE OF HESSIAN MATRIX,0.1774193548387097,"∂z and the Hessian matrix H is deﬁned as {
∂2ℓ
∂zi∂zj }M×M.
With probability 1 −δ over the training examples, for all weight matrices W satisfying the norm
bound ||W T ||2,1 ≤A, the following bound holds:"
TRACE OF HESSIAN MATRIX,0.1827956989247312,"E[¯ℓ] −1.01ˆE[¯ℓ] ≲(Aµ(W ))
2
3 (θB)
1
3"
TRACE OF HESSIAN MATRIX,0.1881720430107527,"n
1
3
+ A
p"
TRACE OF HESSIAN MATRIX,0.1935483870967742,"Bv(W )θ
√n
+
BA2θ
n(log2( BA2θ"
TRACE OF HESSIAN MATRIX,0.1989247311827957,"v(W )n) + 1)
+ ζ.
(5)"
TRACE OF HESSIAN MATRIX,0.20430107526881722,"Here with some ﬁxed bound B > 0,
¯ℓ= min{ℓ, B},"
TRACE OF HESSIAN MATRIX,0.20967741935483872,"∥W ∥2,1 =
X j sX"
TRACE OF HESSIAN MATRIX,0.21505376344086022,"i
(W 2
ij),"
TRACE OF HESSIAN MATRIX,0.22043010752688172,"θ = log3(nM) max
i
∥xi∥2
2 ,"
TRACE OF HESSIAN MATRIX,0.22580645161290322,"ζ = B(log(1/δ) + log log n) n
. (6)"
TRACE OF HESSIAN MATRIX,0.23118279569892472,"So one can guarantee good generalization when the trace of Hessian matrix and norm of Jacobian
matrix are small. On one hand, when learning with gradient descent, we want to ﬁnd a local or global
minimum of loss function. Naturally, at minimum the gradient is zero and the norm of Jacobian
matrix is small near minimum. So gradient descent helps us to ensure the norm of Jacobian Matrix
to be small."
TRACE OF HESSIAN MATRIX,0.23655913978494625,"On the other hand, the trace of Hessian Matrix is hard to be constrained by gradient descent. From
this aspect, we proposed Hessian regularization for linear models as:
1
ntr(Hℓ,z).
(7)"
TRACE OF HESSIAN MATRIX,0.24193548387096775,"It’s the trace of second derivative of loss ℓwith respect to output of linear model z, which is also
the end nodes of a linear model. A DNN is consist of many layers, with each layer being viewed
as a linear model (except the nonlinear activation functions). Thus. we generalize the Hessian
regularization to every node in a DNN and deﬁne it as
1
ntr(Hℓ,ω).
(8)"
TRACE OF HESSIAN MATRIX,0.24731182795698925,It’s the trace of second derivative of loss ℓwith respect to parameters ω.
TRACE OF HESSIAN MATRIX,0.25268817204301075,Here we deﬁne a new loss with our Hessian regularization as
TRACE OF HESSIAN MATRIX,0.25806451612903225,Loss = ℓemp(f) + λ · 1
TRACE OF HESSIAN MATRIX,0.26344086021505375,"ntr(Hℓ,ω),
(9)"
TRACE OF HESSIAN MATRIX,0.26881720430107525,where λ controls the strength of our Hessian regularization.
TRACE OF HESSIAN MATRIX,0.27419354838709675,Under review as a conference paper at ICLR 2022
HUTCHINSON METHOD,0.27956989247311825,"3.2
HUTCHINSON METHOD"
HUTCHINSON METHOD,0.2849462365591398,"In a typical DNN, there are more than millions of parameters. So the calculation of Hessian matrix
is difﬁcult. Hutchinson Method (Avron & Toledo, 2011) is an unbiased estimator for the trace of a
matrix. Let A be an n × n symmetric matrix with tr(A) ̸= 0. Let σ be a random vector whose
entries are i.i.d Rademacher random variables (Pr(σi = ±1) =
1
2), then σT Aσ is an unbiased
estimator of tr(A), based on the following equation:"
HUTCHINSON METHOD,0.2903225806451613,"tr(A) = tr(AI) = tr(AE[σσT ]) = E[tr(AσσT )] = E[σT Aσ].
(10)"
HUTCHINSON METHOD,0.2956989247311828,"In this paper, we consider the trace of Hessian matrix H, which is the second derivative matrix.
Since the Rademacher random vector is irrelevant to network parameters, we expand the expression
of Hutchinson estimator as follow:"
HUTCHINSON METHOD,0.3010752688172043,σT Hσ = σT d dl
HUTCHINSON METHOD,0.3064516129032258,"dω
dω σ = σT (d dl"
HUTCHINSON METHOD,0.3118279569892473,"dω
dω · σ + dl"
HUTCHINSON METHOD,0.3172043010752688,dω · dσ
HUTCHINSON METHOD,0.3225806451612903,dω ) = σT (d( dl
HUTCHINSON METHOD,0.3279569892473118,dω · σ)
HUTCHINSON METHOD,0.3333333333333333,"dω
).
(11)"
HUTCHINSON METHOD,0.3387096774193548,Based on Equation 11 and dσ
HUTCHINSON METHOD,0.34408602150537637,"dω = 0, we only have to calculate the gradient g = dl"
HUTCHINSON METHOD,0.34946236559139787,"dω and the derivative
of dl"
HUTCHINSON METHOD,0.3548387096774194,"dω ·σ, instead of the whole Hessian matrix. The whole calculation process only include two inner
products and two derivations. We refer to the Hutchinson stochastic estimator of Hessian trace as
SEHT-H."
HUTCHINSON METHOD,0.3602150537634409,"Algorithm 1: SEHT-H
Input: n-dimensional gradient g
Output: Estimation of tr(H)"
HUTCHINSON METHOD,0.3655913978494624,1 trace = 0 ;
HUTCHINSON METHOD,0.3709677419354839,2 for i = 1 to maxIter do
HUTCHINSON METHOD,0.3763440860215054,"3
σ ∼Rademacher(n) ;
/* n-dim vector with each element sampled
from Rademacher distribution.
*/"
HUTCHINSON METHOD,0.3817204301075269,"4
v = g · σ ;
/* inner product */"
HUTCHINSON METHOD,0.3870967741935484,"5
h = dv"
HUTCHINSON METHOD,0.3924731182795699,"dω ;
/* derivative of v */"
HUTCHINSON METHOD,0.3978494623655914,"6
t = σT · h ;
/* inner product */"
HUTCHINSON METHOD,0.4032258064516129,"7
trace += t ;"
END,0.40860215053763443,8 end
"RETURN
TRACE
MAXITER",0.41397849462365593,"9 return
trace
maxIter"
"RETURN
TRACE
MAXITER",0.41935483870967744,"Even though it’s a stochastic algorithm, it cost much time because of a great number of parameters.
So we propose another efﬁcient algorithm below based on the basic idea of Hutchinson Method."
DROPOUT METHOD,0.42473118279569894,"3.3
DROPOUT METHOD"
DROPOUT METHOD,0.43010752688172044,"This stochastic algorithm is inspired by Dropout (Srivastava et al., 2014). Every node in the neu-
ral network has a probability p to be igonred in the training process to reduce co-adaptations. In
our Hessian regularization, we want to lower the trace tr(H) = P"
DROPOUT METHOD,0.43548387096774194,"i
∂2ℓ
∂ωi∂ωi , the sum of diagonal
elements of Hessian matrix. Based on the idea of Dropout, we can ignore some parameters in con-
straining the tr(H), since reducing the partial sum of diagonal elements can have a large chance
to reduce the total sum. The partial sum of diagonal element is denoted as ˜tr(H). Considering
the layer structures of neural networks, the process of randomized parameter selection can be di-
vided into two steps: randomly select layers in neural network with probability p1 and randomly
select parameters in the selected layers with probability p2. In other words, when carrying out Hes-
sian regularization, we ignore other layers with probability 1 −p1, and ignore other parameters in
selected layers with probability 1 −p2. In our experiment, we simply set p1 = p2."
DROPOUT METHOD,0.44086021505376344,"In addition, Hutchinson method shows a technique, which avoids to calculate the whole Hessian
matrix. Here we deﬁne a new probability distribution Q(p) (if x ∼Q(p), then Pr(x = ±1) = p
and Pr(x = 0) = 1 −2p). Then, let σ be a random vector whose entries are i.i.d Q random
variables,
E[σσT |fix the position of 0 in σ] = ˜I.
(12)"
DROPOUT METHOD,0.44623655913978494,Under review as a conference paper at ICLR 2022
DROPOUT METHOD,0.45161290322580644,"Here ˜I = diag(0, 1), a diagonal matrix with diagonal elements equal to 0 or 1."
DROPOUT METHOD,0.45698924731182794,"Similar to Equation 10, if we ﬁx the position of 0 in σ, we have unbiased estimator of the partial
sum of diagonal elements:
˜tr(A) = tr(A ˜I) = tr(AE[σσT ]) = E[tr(AσσT )] = E[σT Aσ].
(13)
We can expand the expression same as Equation 11 and transform the calculation process into two
inner product and two derivation. We name this method as SEHT-D."
DROPOUT METHOD,0.46236559139784944,"Algorithm 2: SEHT-D
Input: probability p, parameter ω in selected layers, and corresponding n-dim gradient g
Output: estimation of ˜tr(H)"
DROPOUT METHOD,0.46774193548387094,1 trace = 0 ;
DROPOUT METHOD,0.4731182795698925,2 for i = 1 to maxIter do
DROPOUT METHOD,0.478494623655914,"3
σ ∼Q(p) ;
/* n-dim vector with each element sampled from Q(p)
distribution */"
DROPOUT METHOD,0.4838709677419355,"4
v = g · σ ;
/* inner product */"
DROPOUT METHOD,0.489247311827957,"5
h = dv"
DROPOUT METHOD,0.4946236559139785,"dω ;
/* derivative of v */"
DROPOUT METHOD,0.5,"6
t = σT · h ;
/* inner product */"
DROPOUT METHOD,0.5053763440860215,"7
trace += t ;"
END,0.510752688172043,8 end
"RETURN
TRACE
MAXITER",0.5161290322580645,"9 return
trace
maxIter"
"RETURN
TRACE
MAXITER",0.521505376344086,"In our experiments, we mainly test the performance of this method. Compared with other regu-
larization methods, our Hessian regularization shows improved test performance with fast training
speed."
LINEAR STABILITY ANALYSIS,0.5268817204301075,"3.4
LINEAR STABILITY ANALYSIS"
LINEAR STABILITY ANALYSIS,0.532258064516129,"Training process can be regarded as a motion in the parameter space, from the initial parameter to a
local or global minimum. Current parameters is a point in the parameter space and gradient descent
is the move of the parameter point each time. Then we can see gradient descent from another
perspective. Original gradient descent is deﬁned as a series of discrete updates:
ωt+1 = ωt −ηgt.
(14)
Here ωt is the parameters in step t, η is learning rate and gt is gradient."
LINEAR STABILITY ANALYSIS,0.5376344086021505,"If we consider learning rate as discrete time interval to move ω in parameter space, ∆t = η, then
∆ω"
LINEAR STABILITY ANALYSIS,0.543010752688172,"∆t = −g(ω, x).
(15)"
LINEAR STABILITY ANALYSIS,0.5483870967741935,"We assume time interval or learning rate is small enough, approximately we get a contiunous form:
dω"
LINEAR STABILITY ANALYSIS,0.553763440860215,"dt = −g(ω, x).
(16)"
LINEAR STABILITY ANALYSIS,0.5591397849462365,"Thereafter, with an initial condition, we have the complete trajectory of parameter point based on
ordinary differential equation (ODE) theory. The process of gradient descent is transformed to a
Nonlinear Dynamical System. So we introduce Linear Stability Analysis of Nonlinear Dynamical
Systems below."
LINEAR STABILITY ANALYSIS,0.5645161290322581,"Nonlinear Dynamical System (Thomas Witelski & Mark Bowen, 2015) is deﬁned as a differential
function dx"
LINEAR STABILITY ANALYSIS,0.5698924731182796,"dt = f(x, t) with an initial condition x(0) = x0. x(t) = (x1(t), x2(t), . . . , xn(t)) ∈Rn
is the vector of state variables, describes n properties of interest in the system, evolving for times
t ≥0 and starting from a given initial state x(0) = x0. The rate functions for the rates of change
of each xi, dxi"
LINEAR STABILITY ANALYSIS,0.5752688172043011,"dt = fi, have similarly been collected in a vector f = (f1, f2, . . . , fn), where each
fi can potentially depend on all of the state variables. Since fi doesn’t have to be a linear function,
the system is called Nonlinear Dynamical System. A classic example of a dynamical system from
mechanics is the system for motion of a particle. A system is called non-autonomous if the rate
function have an explicit dependence on time. In this paper, we only focus on autonomous systems,
dx"
LINEAR STABILITY ANALYSIS,0.5806451612903226,"dt = f(x), x(0) = x0.
(17)"
LINEAR STABILITY ANALYSIS,0.5860215053763441,Under review as a conference paper at ICLR 2022
LINEAR STABILITY ANALYSIS,0.5913978494623656,"The equilibrium points x∗are deﬁned by positions where rate functions vanish, f(x∗) = 0. If any
solution starting near an equilibrium point leaves the neighbourhood of x∗as t →∞, then x∗is
called asymptotically unstable, while if all solutions starting within the neighbourhood approach x∗
as t →∞then the equilibrium is called asymptotically stable. Lyapunov (1992) gave more rigorous
deﬁnition and discussion, known as Lyapunov Stability Theory. The idea of Lyapunov Stability
can be extended to inﬁnite-dimensional manifolds, where it is known as Structural Stability (Pugh
& Peixoto, 2008), which concerns the behavior of different but ”nearby” solutions to differential
equations."
LINEAR STABILITY ANALYSIS,0.5967741935483871,"For a basic Nonlinear Dynamical System, to ensure equilibrium point x∗is stable, we need to
construct Jacobian matrix J = ∂f"
LINEAR STABILITY ANALYSIS,0.6021505376344086,"∂x. Given the equilibrium point x∗, J(x∗) is a constant matrix.
Using the conclusion from Linear State Space Model, if all eigenvalues of J(x∗) have real parts
less than zero, then x∗is stable. If at least one of the eigenvalues has a real part greater than zero,
then x∗is unstable."
LINEAR STABILITY ANALYSIS,0.6075268817204301,"Back to gradient descent, it can be regarded as a Nonlinear Dynamical System, according to Equa-
tion 16. The local or global minimum, which is the goal of gradient descent, is the equilibrium
point in such system, since that minimum point satisﬁes the condition dω"
LINEAR STABILITY ANALYSIS,0.6129032258064516,"dt = 0. The Jacobian ma-
trix in this dynamical system is the negative of the Hessian matrix in our Hessian regularization,
J(ω∗) = −∂g"
LINEAR STABILITY ANALYSIS,0.6182795698924731,∂ω = −∂∂ℓ
LINEAR STABILITY ANALYSIS,0.6236559139784946,"∂ω
∂ω
= −Hℓ. And the trace and the eigenvalues of J(ω∗) are also negative of
the trace and the eigenvalues of Hℓ."
LINEAR STABILITY ANALYSIS,0.6290322580645161,"It’s easy to see that in our Hessian regularization, we lower the trace of Hessian Matrix Hℓ, thus
increasing the trace of the Jacobian matrix J(ω∗). However, in real matrix, complex eigenvalues are
always conjugate and the trace are always real number. When we increase the trace, we increase the
real parts of eigenvalues of J(ω∗) to some extent. In other word, the goal is to preclude the stability
of equilibrium point by our Hessian regularization."
LINEAR STABILITY ANALYSIS,0.6344086021505376,"Why we want instability in this dynamical system of gradient descent? The stability of local or
global minimum shows the stability toward training data. The whole dynamical system of gradient
descent is a motion based on training set since the motion of parameters is decided by Equation 17.
Equation 17 is consist of two parts: one is the initial condition, which is randomized in a DNN, while
the other part is the differential function Equation 16, which is directly determined by training data.
In other words, with different training data, the parameters have different trajectory in parameter
space. It only use information about training data and the equilibrium point depends on training
data. Therefore, the stability is relevant to training set and reducing the stability to some extent can
avoid over-ﬁtting to training data."
EXPERIMENTS,0.6397849462365591,"4
EXPERIMENTS"
EXPERIMENTS,0.6451612903225806,"We evaluate our Hessian regularization with other regularization methods on a variety of datasets.
We also combine data augmentation methods with our Hessian regularization to test its efﬁcacy."
IMAGE CLASSIFICATION,0.6505376344086021,"4.1
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.6559139784946236,"4.1.1
CIFAR-10"
IMAGE CLASSIFICATION,0.6612903225806451,"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per
class. There are 50000 training images and 10000 test images."
IMAGE CLASSIFICATION,0.6666666666666666,"For our experiment, we use ResNet-18 (He et al., 2016) as the backbone neural network. For all
models, we use Weight Decay of 5 × 10−4. We set learning rate 0.01, batch size 32, momentum 0.9
and all models were trained 200 epochs with Cosine Annealing (Loshchilov & Hutter, 2016). For
Jacobian regularization, we set number of projections nproj = 1 and weight values λJR = 0.01. For
DropBlock, block size = 7 and keep prob = 0.9. For Conﬁdence Penalty and Label Smoothing,
we set weight value 0.1, which is found to work best on CIFAR-10 by Pereyra et al. (2017) in their
experiments. For our Hessian regularization SEHT-D, we also set weight value 0.1, testing with
probability value 0.01 and 0.05. In addition, we always add output layer in the selected layer, which
corresponds to Equation 7. Cutout size of 16 × 16 pixels is used in our experiment, based on the"
IMAGE CLASSIFICATION,0.6720430107526881,Under review as a conference paper at ICLR 2022
IMAGE CLASSIFICATION,0.6774193548387096,"validation results mentioned by DeVries & Taylor (2017). For mixup, α = 0.1. Averages and 95%
conﬁdence intervals are estimated over 5 distinct runs."
IMAGE CLASSIFICATION,0.6827956989247311,"We also observe the computational cost of the training with SEHT-D (maxIter=1, prob=0.01) to
be only 1.2 times that of the baseline. Although increasing the probability to select parameters
can improve test accuracy, the time consumption will increase a lot. In our experiment, SEHT-D
(maxIter=1, prob=0.05) costs 1.5 times of the baseline."
IMAGE CLASSIFICATION,0.6881720430107527,"In our experiments, we ﬁnd that Jacobian regularization and Dropblock have worse performance
than the baseline with Weight-Decay.
Conﬁdence Penalty has slight improvement and Label
Smoothing has obvious improvement. However, our SEHT-D shows better results, compared with
all other regularization methods which are tested. In addition, our SEHT is suitable for combination
with other methods. When we combin SEHT-D with mixup, we get the best test accuracy in our
experiment, 1.45 more than baseline."
IMAGE CLASSIFICATION,0.6935483870967742,"In Sankar et al. (2021)’s experiment, they got test accuracy 88.13 on CIFAR-10 with ResNet-18,
which is much worse than our result: 94.35 with SEHT-D (maxIter=1, prob=0.01) and 94.37 with
SEHT-D (maxIter=1, prob=0.05). Moreover, their improvement based on their methods is only 0.02
for full-network method and 0.10 for middle-network method. Our Hessian regularization method
improves the model 0.35 on test accuracy with SEHT-D (maxIter=1, prob=0.01) and improves 0.37
on test accuracy with SEHT-D (maxIter=1, prob=0.05), which is over 3 times of their improvement."
IMAGE CLASSIFICATION,0.6989247311827957,Table 1: ResNet-18 on CIFAR-10
IMAGE CLASSIFICATION,0.7043010752688172,"Model
Test Accuracy
Baseline with Weight-Decay
94.00 ± 0.47
Jacobian
89.23 ± 1.02
DropBlock
89.23 ± 0.44
Sanker’s Method for Full Network
88.05 ± 0.22
Sanker’s Method for Middle Network
88.13 ± 0.12
Conﬁdence Penalty
94.01 ± 0.40
Label Smoothing
94.26 ± 0.26
SEHT-D (maxIter=1, prob=0.01)
94.35 ± 0.18
SEHT-D (maxIter=1, prob=0.05)
94.37 ± 0.27
SEHT-D (maxIter=1, prob=0.01) + Label Smoothing
94.38 ± 0.24
cutout
94.02 ± 0.22
mixup
95.39 ± 0.13
SEHT-D (maxIter=1, prob=0.01) + mixup
95.45 ± 0.06"
IMAGE CLASSIFICATION,0.7096774193548387,"4.1.2
CIFAR-100"
IMAGE CLASSIFICATION,0.7150537634408602,"CIFAR-100 dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each.
There are 500 training images and 100 testing images per class."
IMAGE CLASSIFICATION,0.7204301075268817,"We use Wide Residual Networks (WRN) (Zagoruyko & Komodakis, 2016) as the backbone neural
network. We use WRN-28-10 speciﬁcally, with depth 28 and ﬁxed widening factor of 10. For all
models, we use Weight Decay of 5 × 10−4. We set batch size 32, momentum 0.9 and all models
were trained 200 epochs. The learning rate is initially set to 0.1 and is scheduled to decrease by a
factor of 5 after each of the 60th, 120th, and 160th epochs. We test Dropout with a drop probability
of p = 0.3, determined by Zagoruyko & Komodakis (2016)’s cross-validation. For Conﬁdence
Penalty, Label Smoothing and our Hessian regularization, we set weight value 0.1. Cutout size of 8
× 8 pixels is used according to DeVries & Taylor (2017)’s validation results. For mixup, we still set
α = 0.1. Averages and 95% conﬁdence intervals of top-1 accuracy and top-5 accuracy are estimated
over 5 distinct runs."
IMAGE CLASSIFICATION,0.7258064516129032,"In this experiments, Conﬁdence Penalty has better top-5 accuracy, worse top-1 accuracy and Label
Smoothing has better top-1 accuracy, worse top-5 accuracy, compared with baseline method. How-
ever, our SEHT-D method shows better results on both top-1 accuracy and top-5 accuracy, improving
1.13 and 0.65 respectively. Our Hessian regularization method also perform better when combined"
IMAGE CLASSIFICATION,0.7311827956989247,Under review as a conference paper at ICLR 2022
IMAGE CLASSIFICATION,0.7365591397849462,"with other method on this dataset. When testing together with Dropout, our SEHT-D has best accu-
racy, compared with Label Smoothing and Conﬁdence Penalty. Our SEHT-D also improves cutout
for 0.39 on top-1 accuracy, 0.26 on top-5 accuracy, and improves mixup 0.21 on top-1 accuracy,
0.09 on top-5 accuracy."
IMAGE CLASSIFICATION,0.7419354838709677,"Table 2: WRN-28-10 on CIFAR-100
Model
Top-1 Accuracy
Top-5 Accuracy
Baseline with Weight-Decay
73.79 ± 2.68
92.01 ± 1.32
Conﬁdence Penalty
73.46 ± 1.21
92.16 ± 0.58
Label Smoothing
74.15 ± 0.92
90.40 ± 0.73
SEHT-D(maxIter=1, prob=0.01)
74.92 ± 0.77
92.66 ± 0.54
Conﬁdence Penalty + Dropout
74.80 ± 0.91
93.09 ± 0.51
Label Smoothing + Dropout
72.89 ± 1.57
90.43 ± 0.97
SEHT-D(maxIter=1, prob=0.01) + Dropout
77.75 ± 0.37
94.38 ± 0.09
cutout
76.70 ± 0.79
93.72 ± 0.40
SEHT-D(maxIter=1, prob=0.01) + cutout
77.09 ± 0.37
93.98 ± 0.21
mixup
78.38 ± 0.31
94.37 ± 0.31
SEHT-D(maxIter=1, prob=0.01) + mixup
78.59 ± 0.46
94.46 ± 0.30"
IMAGE CLASSIFICATION,0.7473118279569892,"Our experiments on Image Classiﬁcation shows that our Hessian regularization method outperforms
other regularization methods and can be efﬁciently combined with data augmentation methods."
LANGUAGE MODELING,0.7526881720430108,"4.2
LANGUAGE MODELING"
LANGUAGE MODELING,0.7580645161290323,"4.2.1
WIKI-TEXT2"
LANGUAGE MODELING,0.7634408602150538,"The Wiki-Text language modeling dataset is a collection of over 100 million tokens extracted from
the set of veriﬁed Good and Featured articles on Wikipedia."
LANGUAGE MODELING,0.7688172043010753,"We use a 2-layer LSTM (Hochreiter & Schmidhuber, 1997). The size of word embeddings is 512
and the number of hidden units per layer is 512. We run every algorithm for 40 epochs, with batch
size 20, gradient clipping 0.25, and Dropout ratio 0.5. We perform a grid search over Dropout
ratios {0, 0.1, 0.2, 0.3, 0.4, 0.5} and ﬁnd 0.5 to work best. We tune the initial learning rate from
{0.001, 0.01, 0.1, 0.5, 1, 10, 20, 40} and decrease the learning rate by factor of 4 when the valida-
tion error saturates. We ﬁnd initial learning rate 20 works best. Parameters are initialized from a
uniform distribution [−0.1, 0.1]. For label smoothing, we perform a grid search over weight values
{0.001, 0.005, 0.01, 0.05, 0.1} and ﬁnd 0.01 to work best. For the conﬁdence penalty, we perform
a grid search over weight values {0.001, 0.005, 0.01, 0.05, 0.1} and ﬁnd 0.01 to work best. For our
Hessian regularization, we perform a grid search over weight values {0.001, 0.005, 0.01, 0.05, 0.1},
probability values {0.01, 0.05}. Weight 0.01 and probability 0.05 work best. Averages and 95%
conﬁdence intervals are estimated over 5 distinct runs."
LANGUAGE MODELING,0.7741935483870968,"In this experiments with LSTM, our SEHT-D has the best test perplexity and Label Smoothing
shows best validation perplexity. SEHT-D improves the model 2.61 on test perplexity. Conﬁdence
Penalty performs only sightly better than the baseline method."
LANGUAGE MODELING,0.7795698924731183,"Table 3: LSTM on Wiki-Text2
Model
Validation Perplexity
Test Perplexity
Baseline with Dropout
101.82 ± 0.32
95.65 ± 0.19
Conﬁdence Penalty
101.39 ± 0.32
95.57 ± 0.11
Label Smoothing
99.58 ± 0.11
95.03 ± 0.58
SEHT-D
100.69 ± 0.53
94.86 ± 0.50"
LANGUAGE MODELING,0.7849462365591398,Under review as a conference paper at ICLR 2022
LANGUAGE MODELING,0.7903225806451613,"We also tested with a 2-layer GRU (Cho et al., 2014). The size of word embeddings is 512 and
the number of hidden units per layer is 512. We run every algorithm for 40 epochs, with batch
size 20, gradient clipping 0.25 and Dropout ratio 0.3. We perform a grid search over Dropout
ratios {0, 0.1, 0.2, 0.3, 0.4, 0.5} and ﬁnd 0.3 to work best. We tune the initial learning rate from
{0.001, 0.01, 0.1, 0.5, 1, 10, 20, 40} and decrease the learning rate by factor of 4 when the vali-
dation error saturates. We ﬁnd initial learning rate 20 works best, same as LSTM. Parameters
are initialized from a uniform distribution [−0.1, 0.1]. For label smoothing, we perform a grid
search over weight values {0.001, 0.005, 0.01, 0.05, 0.1} and ﬁnd 0.05 to work best. For the conﬁ-
dence penalty, we perform a grid search over weight values {0.001, 0.005, 0.01, 0.05, 0.1} and ﬁnd
0.005 to work best. For our Hessian regularization, we perform a grid search over weight values
{0.001, 0.005, 0.01, 0.05, 0.1}. Weight 0.001 works best. We set probability values 0.01. Averages
and 95% conﬁdence intervals are estimated over 5 distinct runs."
LANGUAGE MODELING,0.7956989247311828,"Our Hessian regularization method has both the best validation perplexity and the best test per-
plexity, improving 2.83 and 2.61 respectively compared with baseline method. Conﬁdence Penalty
surpasses Label Smoothing with GRU model, compared with LSTM. Label Smoothing also show
better results than baseline."
LANGUAGE MODELING,0.8010752688172043,"Table 4: GRU on Wiki-Text2
Model
Validation Perplexity
Test Perplexity
Baseline with Dropout
119.04 ± 4.67
111.64 ± 3.67
Conﬁdence Penalty
116.40 ± 0.17
109.27 ± 0.05
Label Smoothing
117.47 ± 0.48
110.46 ± 0.87
SEHT-D
116.21 ± 0.60
109.03 ± 0.30"
LANGUAGE MODELING,0.8064516129032258,"Our experiments on Language Modelling demonstrate that all these three regularization methods
can improve models, while our SEHT-D is the best."
CONCLUSION,0.8118279569892473,"5
CONCLUSION"
CONCLUSION,0.8172043010752689,"We propose a new regularization method named as Stochastic Estimators of Hessian Trace (SEHT).
Our method is motivated by a guarantee bound that a lower trace of the Hessian can result in a
lower generalization error. We also explained our method with dynamical system theory. Our ex-
periment shows that SEHT-D yields promising test performance with fast training speed. SEHT-D
also achieves better results when combined with data augmentation methods."
REPRODUCIBILITY STATEMENT,0.8225806451612904,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.8279569892473119,Our codes are available at here .
REFERENCES,0.8333333333333334,REFERENCES
REFERENCES,0.8387096774193549,"Haim Avron and Sivan Toledo. Randomized algorithms for estimating the trace of an implicit sym-
metric positive semi-deﬁnite matrix. Journal of the ACM (JACM), 58(2):1–34, 2011."
REFERENCES,0.8440860215053764,"Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pp. 1724–1734. Association for Computational
Linguistics, October 2014."
REFERENCES,0.8494623655913979,"Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with Cutout. arXiv preprint arXiv:1708.04552, 2017."
REFERENCES,0.8548387096774194,"Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock: A regularization method for convolu-
tional networks. Advances in Neural Information Processing Systems, 31:10727–10737, 2018."
REFERENCES,0.8602150537634409,Under review as a conference paper at ICLR 2022
REFERENCES,0.8655913978494624,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.8709677419354839,"David P Helmbold and Philip M Long. On the inductive bias of dropout. The Journal of Machine
Learning Research, 16(1):3403–3454, 2015."
REFERENCES,0.8763440860215054,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.8817204301075269,"Arthur E Hoerl and Robert W Kennard. Ridge regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67, 1970."
REFERENCES,0.8870967741935484,"Judy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with jacobian regularization.
Conference on the Mathematical Theory of Deep Learning (DeepMath), 2019."
REFERENCES,0.8924731182795699,"Anders Krogh and John Hertz. A simple weight decay can improve generalization. Advances in
Neural Information Processing Systems, 4, 1992."
REFERENCES,0.8978494623655914,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.9032258064516129,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. International Conference
on Learning Representations, 2019."
REFERENCES,0.9086021505376344,"Aleksandr Mikhailovich Lyapunov. The general problem of the stability of motion. International
journal of control, 55(3):531–534, 1992."
REFERENCES,0.9139784946236559,"Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548,
2017."
REFERENCES,0.9193548387096774,"Charles Pugh and Maur´ıcio Matos Peixoto. Structural stability. Scholarpedia, 3(9):4008, 2008."
REFERENCES,0.9247311827956989,"Adepu Ravi Sankar, Yash Khasbage, Rahul Vigneswaran, and Vineeth N Balasubramanian. A deeper
look at the hessian eigenspectrum of deep neural networks and its applications to regularization.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 9481–9488,
2021."
REFERENCES,0.9301075268817204,"Jure Sokoli´c, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep
neural networks. IEEE Transactions on Signal Processing, 65(16):4265–4280, 2017."
REFERENCES,0.9354838709677419,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine
learning research, 15(1):1929–1958, 2014."
REFERENCES,0.9408602150537635,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.946236559139785,"Thomas Witelski Thomas Witelski and Mark Bowen Mark Bowen. Methods of mathematical mod-
elling: Continuous systems and differential equations, 2015."
REFERENCES,0.9516129032258065,"Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267–288, 1996."
REFERENCES,0.956989247311828,"Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. Advances
in neural information processing systems, 26:351–359, 2013."
REFERENCES,0.9623655913978495,"Colin Wei, Sham Kakade, and Tengyu Ma.
The implicit and explicit regularization effects of
dropout. In International Conference on Machine Learning, pp. 10181–10192. PMLR, 2020."
REFERENCES,0.967741935483871,"Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks
through the lens of the hessian. In 2020 IEEE International Conference on Big Data (Big Data),
pp. 581–590. IEEE, 2020a."
REFERENCES,0.9731182795698925,Under review as a conference paper at ICLR 2022
REFERENCES,0.978494623655914,"Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W Ma-
honey. Adahessian: An adaptive second order optimizer for machine learning. arXiv preprint
arXiv:2006.00719, 2020b."
REFERENCES,0.9838709677419355,"Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong, Michael W Mahoney, and Kurt Keutzer.
Hessian-aware pruning and optimal neural implant. arXiv preprint arXiv:2101.08940, 2021."
REFERENCES,0.989247311827957,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.9946236559139785,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017."
