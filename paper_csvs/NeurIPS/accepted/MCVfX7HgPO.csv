Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001976284584980237,"Given the intractably large size of the space of proofs, any model that is capable of
general deductive reasoning must generalize to proofs of greater complexity. Re-
cent studies have shown that large language models (LLMs) possess some abstract
deductive reasoning ability given chain-of-thought prompts. However, they have
primarily been tested on proofs using modus ponens or of a specific size, and from
the same distribution as the in-context examples. To measure the general deductive
reasoning ability of LLMs, we test on a broad set of deduction rules and measure
their ability to generalize to more complex proofs from simpler demonstrations
from multiple angles: depth-, width-, and compositional generalization. To fa-
cilitate systematic exploration, we construct a new synthetic and programmable
reasoning dataset that enables control over deduction rules and proof complexity.
Our experiments on four LLMs of various sizes and training objectives show that
they are able to generalize to compositional proofs. However, they have difficulty
generalizing to longer proofs, and they require explicit demonstrations to produce
hypothetical subproofs, specifically in proof by cases and proof by contradiction."
INTRODUCTION,0.003952569169960474,"1
Introduction"
INTRODUCTION,0.005928853754940711,"In many tasks that require deductive reasoning, such as theorem proving or medical diagnosis,
the complexity of proofs can grow without bound via the use of multiple deduction rules and the
composition of subproofs. Given the large space of proofs, it is infeasible to find data to cover
proofs of all sizes. Therefore, a general reasoning model must extrapolate to complex proofs from
simpler ones. Recent work has shown that LLMs, combined with in-context learning (ICL) and
chain-of-thought (CoT) prompting, are capable of deductive reasoning to an extent [Huang and
Chang, 2022, Han et al., 2022, Wei et al., 2022b, Kojima et al., 2022, Lewkowycz et al., 2022, Nye
et al., 2021, Gontier et al., 2020]. However, much of the prior work focused on a limited set of
deduction rules such as modus ponens [Zhang et al., 2022a, Saparov and He, 2023, Tafjord et al.,
2021]. In addition, the evaluation is in-demonstration, where the test example comes from the same
distribution as the in-context demonstrations. In this work, we evaluate whether LLMs are capable of
general deductive reasoning by measuring how well they generalize to proofs that are more complex"
INTRODUCTION,0.007905138339920948,"∗Equal contribution.
†,∆GPT and PaLM experiments in this paper were conducted independently. Authors affiliated with NYU†"
INTRODUCTION,0.009881422924901186,"were responsible for the GPT experiments, and authors affiliated with Google∆were responsible for the PaLM
experiments."
INTRODUCTION,0.011857707509881422,Train on:
INTRODUCTION,0.01383399209486166,"“Alex is a dog. All dogs are
mammals. Alex is a mammal.”"
INTRODUCTION,0.015810276679841896,Test on unseen deduction rules:
INTRODUCTION,0.017786561264822136,"“Alex is not a mammal. All dogs are mammals. Suppose Alex is a dog. Alex is a
mammal. This contradicts with Alex is not a mammal. Alex is not a dog.”"
INTRODUCTION,0.019762845849802372,Train on:
INTRODUCTION,0.021739130434782608,"“Alex is a dog. All dogs are
mammals. Alex is a mammal.”"
INTRODUCTION,0.023715415019762844,Test on deeper proofs:
INTRODUCTION,0.025691699604743084,“Alex is a dog. All dogs are mammals. Alex is a mammal.
INTRODUCTION,0.02766798418972332,All mammals are vertebrates. Alex is a vertebrate.”
INTRODUCTION,0.029644268774703556,Train on:
INTRODUCTION,0.03162055335968379,"“Alex is a dog. Alex is soft.
Alex is a dog and soft.”"
INTRODUCTION,0.03359683794466403,Test on wider proofs:
INTRODUCTION,0.03557312252964427,"“Alex is a dog. Alex is soft. Alex is kind. Alex is a dog
and soft and kind.”"
INTRODUCTION,0.037549407114624504,Train on:
INTRODUCTION,0.039525691699604744,"“Alex is a dog. All dogs are
mammals. Alex is a mammal.”"
INTRODUCTION,0.041501976284584984,"“Fae is a cat. Fae is soft. Fae
is soft and a cat.”"
INTRODUCTION,0.043478260869565216,Test on compositional proofs:
INTRODUCTION,0.045454545454545456,“Alex is a dog. All dogs are mammals. Alex is a mammal. Alex is not mean.
INTRODUCTION,0.04743083003952569,Alex is a mammal and not mean.”
INTRODUCTION,0.04940711462450593,"FIGURE 1: An overview of the kinds of OOD generalization that we test in our experiments. Each training
example is a sample CoT demonstration provided to the LLM in the few-shot prompt, whereas each test example
is a sample proof that the model is expected to output."
INTRODUCTION,0.05138339920948617,than their demonstrations.1
INTRODUCTION,0.0533596837944664,"We characterize the complexity of proofs from three angles: the deduction rules involved, the depth
of the proof (i.e. length of a sequential chain of proof steps), and the width of the proof (i.e. the
number of premises of each proof step). Each of the three dimensions contributes to the overall size
of the proof. To measure the general deductive reasoning ability of LLMs, we extend prior studies in
two key ways. First, we determine whether LLMs have learned a complete set of deduction rules,
beyond modus ponens. Second, we evaluate whether they can reason over longer proofs than those
given as in-context examples (depth- and width- generalization); and whether they are able to use
multiple different deduction rules in a single proof (compositional generalization). Figure 1 shows an
overview of our study."
INTRODUCTION,0.05533596837944664,"Our findings suggest that in-context learning is best applied to reasoning tasks by including examples
that cover a diverse set of deduction rules, and keeping the examples simple. The in-context examples
should especially contain examples of deduction rules that are less familiar to the model (i.e. proof
by cases and proof by contradiction), and distractors should be provided for such examples as the
model is more prone to overfitting."
INTRODUCTION,0.05731225296442688,"We test four different LLMs of different scales and training objectives: GPT-3.5 175B [Ouyang et al.,
2022], PaLM 540B [Chowdhery et al., 2022], LLaMA 65B [Touvron et al., 2023], and FLAN-T5
11B [Chung et al., 2022], and we find:"
INTRODUCTION,0.05928853754940711,"1. CoT is able to elicit out-of-demonstration (OOD) reasoning in LLMs generalizing to composi-
tional proofs. This is somewhat surprising given the amount of previous work that claim that
LLMs are not able to generalize compositionally [Hosseini et al., 2022, An et al., 2023]. See
Section 4.2.2 and Figure 6.
2. ICL generalizes differently compared to supervised learning (i.e. gradient descent on in-context
examples). We find numerous examples where it is strictly worse to provide in-context examples
from the same distribution as the test example. For instance, in some cases, we observe better
generalization to compositional proofs when the in-context examples each contain individual
deduction rules. See Sections 4.2.2 and 4.3, and Figures 6 and 8.
3. However, the LLMs cannot generalize to some deduction rules without explicit demonstrations,
specifically, proof by cases and proof by contradiction, suggesting that pretraining is not sufficient
to teach the model to generate hypothetical subproofs. See Section 4.2.1 and Figure 4.
4. Model size does not strongly correlate with performance. Smaller (but not the smallest) models
with instruction tuning and longer pretraining perform comparably to larger models."
INTRODUCTION,0.06126482213438735,"1All analysis code, data, data generation scripts, and model outputs are publicly available at
github.com/asaparov/prontoqa"
INTRODUCTION,0.06324110671936758,Dataset
INTRODUCTION,0.06521739130434782,"Automated
evaluation of
proofs"
INTRODUCTION,0.06719367588932806,"Contains proofs
with multiple
deduction rules"
INTRODUCTION,0.0691699604743083,"Tests proof
depth
generalization"
INTRODUCTION,0.07114624505928854,"Tests proof
width
generalization"
INTRODUCTION,0.07312252964426877,"Tests
compositional
generalization"
INTRODUCTION,0.07509881422924901,"Data
generation
code available"
INTRODUCTION,0.07707509881422925,"CLUTRR
Sinha et al. [2019]
✗
∼
✗
✓
∼
✓"
INTRODUCTION,0.07905138339920949,"LogiQA
Liu et al. [2020]
✗
✓
∼
∼
∼
human-
annotated
ProofWriter
Tafjord et al. [2021]
✓
∼
✓
∼
∼
✗"
INTRODUCTION,0.08102766798418973,"FOLIO
Han et al. [2022]
✗
✓
∼
∼
∼
human-
annotated"
INTRODUCTION,0.08300395256916997,"PRONTOQA
Saparov and He [2023]
✓
✗
✓
✗
✗
✓"
INTRODUCTION,0.08498023715415019,"PRONTOQA-OOD
(this dataset)
✓
✓
✓
✓
✓
✓"
INTRODUCTION,0.08695652173913043,"TABLE 1: Comparison of existing datasets to evaluate reasoning ability. Datasets marked with ∼contain
examples of varying width, depth, and compositionality, but these are not programmable (i.e. we cannot
generate new examples controlling for these variables), and splitting the existing examples would produce highly
imbalanced splits."
RELATED WORK,0.08893280632411067,"2
Related work"
RELATED WORK,0.09090909090909091,"OOD generalization of LLMs.
Previous work has measured the generalization ability of LLMs
on tasks such as bit parity and Boolean variable assignment [Anil et al., 2022], semantic parsing
[Hosseini et al., 2022, Qiu et al., 2022], deductive reasoning [Zhang et al., 2022a, Sanyal et al., 2022,
Kazemi et al., 2023], and arithmetic reasoning [Kudo et al., 2023], where the length/complexity
of the test example is greater than that of the in-context examples. On the bit parity and variable
assignment tasks, LLMs are able to generalize to longer inputs with scratchpad prompting [Nye
et al., 2021], but this generalization is imperfect, and accuracy still degrades with increasing input
length. Generally, larger models tend to be better at generalization than smaller ones. The studies
on reasoning were limited to reasoning using modus ponens. Wu et al. [2021] tests the OOD
generalization of transformers and graph neural networks on symbolic mathematical reasoning. Our
study more systematically examines OOD generalization of LLMs to larger proofs as well as to other
deduction rules."
RELATED WORK,0.09288537549407115,"Evaluating reasoning abilities of LLMs.
A number of recent studies measured the reasoning
ability of LLMs [Huang and Chang, 2022, Han et al., 2022]. Table 1 provides a comparison of our
proposed dataset to datasets from these studies. Many of these datasets are not amenable to automated
evaluation of proofs, relying instead on measuring label accuracy. The datasets also do not test for
proof width generalization and compositional generalization. Some datasets focus on a limited set of
deduction rules, namely modus ponens. Our work is closest to PRONTOQA [Saparov and He, 2023]
but extends it to a complete set of deduction rules and to compositional proofs."
RELATED WORK,0.09486166007905138,"Understanding in-context learning.
Recent work has shed some light on ICL, and the mechanism
by which the model learns from in-context examples. Akyürek et al. [2023], Dai et al. [2023], von
Oswald et al. [2022] showed that transformers can learn in-context by performing gradient descent
on in-context examples internally. Xie et al. [2022], Wang et al. [2023] show that LLMs exhibit
behavior similar to that of topic models where their output is dependent on a latent topic, and the
in-context examples help to specify the topic. Ye et al. [2022] demonstrated that ICL is more effective
when the in-context examples are both diverse and relevant to the test example. An et al. [2023] and
Levy et al. [2022] explored the effect of in-context demonstrations on compositional generalization,
showing that it benefits from diverse and individually simple demonstrations. Our results contribute
to this growing literature by showing that the generalization behavior in ICL is different from that
of supervised learning, and so algorithms such as gradient descent are not the only mechanisms
underlying ICL."
APPROACH,0.09683794466403162,"3
Approach"
APPROACH,0.09881422924901186,"A programmable dataset.
Our main evaluation approach is to prompt the LLM with simpler
proofs and test it on proofs with greater depth and width, or with those using additional deduction
rules. Therefore, we require a programmable approach to data generation, where the deduction rules"
APPROACH,0.1007905138339921,"Deduction rule
Formal definition
Natural language example"
APPROACH,0.10276679841897234,"Implication elimination
(i.e. modus ponens)"
APPROACH,0.10474308300395258,"f(a)
∀x(f(x) →g(x)) g(a)"
APPROACH,0.1067193675889328,"“Alex is a cat. All cats are carnivores. Alex is a
carnivore.”"
APPROACH,0.10869565217391304,"Conjunction
introduction A
B A ∧B"
APPROACH,0.11067193675889328,"“Alex is a cat. Alex is orange. Alex is a cat and
orange.”"
APPROACH,0.11264822134387352,"Conjunction
elimination A ∧B"
APPROACH,0.11462450592885376,"A
“Alex is a cat and orange. Alex is orange.”"
APPROACH,0.116600790513834,"Disjunction
introduction A"
APPROACH,0.11857707509881422,"A ∨B
“Alex is a cat. Alex is a cat or orange.”"
APPROACH,0.12055335968379446,"Disjunction elimination
(i.e. proof by cases)"
APPROACH,0.1225296442687747,"A ∨B
A ⊢C
B ⊢C C"
APPROACH,0.12450592885375494,"“Alex is a cat or a dog. Suppose Alex is a cat . . . then
Alex is warm-blooded. Suppose Alex is a dog . . . then
Alex is warm-blooded. Alex is warm-blooded.”"
APPROACH,0.12648221343873517,"Proof by contradiction
A ⊢B
¬B ¬A"
APPROACH,0.12845849802371542,"“Alex is cold-blooded. If Alex is a mammal, Alex is
not cold-blooded. Suppose Alex is a mammal. Alex is
not cold-blooded. This contradicts with Alex is cold-
blooded. Alex is not a mammal.”"
APPROACH,0.13043478260869565,"TABLE 2: An overview of the deduction rules in PRONTOQA-OOD. The notation A ⊢B denotes entailment:
that B is provable from A."
APPROACH,0.1324110671936759,"“Alex is a dog.
All dogs are mammals.
Alex is a mammal.
Alex is
blue.
All mean things are not blue.
Suppose Alex is mean.
Alex is
not blue.
This contradicts with Alex is blue.
Therefore, Alex is
not mean.
Alex is a mammal and not mean.”"
APPROACH,0.13438735177865613,"dog(alex)
∀x(dog(x) →mammal(x))"
APPROACH,0.13636363636363635,mammal(alex)
APPROACH,0.1383399209486166,blue(alex)
APPROACH,0.14031620553359683,"mean(alex)
∀x(mean(x) →¬blue(x))"
APPROACH,0.1422924901185771,¬blue(alex)
APPROACH,0.1442687747035573,¬mean(alex)
APPROACH,0.14624505928853754,mammal(alex) ∧¬mean(alex)
APPROACH,0.1482213438735178,"FIGURE 2: An example of a compositional proof containing modus ponens, proof by contradiction, and
conjunction introduction, shown in both natural language and a formal tree representation."
APPROACH,0.15019762845849802,"used, as well as the depth and width of each proof, are controllable parameters. To this end, we
propose PRONTOQA-OOD, a generative process for synthetic reasoning questions. Each example in
PRONTOQA-OOD contains a handful of premises, a query (the target fact to be proved/disproved),
and a gold CoT containing the proof of the query. See Figure 10 for an example from this dataset.
Specifically, we extend the PRONTOQA dataset [Saparov and He, 2023] that contains proofs gen-
erated from synthetic world models using modus ponens. (1) To evaluate reasoning using different
deduction rules, we generate proofs with deduction rules for all connectives in propositional logic:
conjunction ∧, disjunction ∨, implication →, and negation ¬. (2) To study width/depth generalization,
the proof depth and width are controllable parameters in proof generation, where they control the
number of generated deduction rules, and the number of premises in each deduction rule, respectively.
(3) To study compositional generalization, we generate compositional proofs using a simple recursive
procedure, where each proof contains multiple subproofs with distinct deduction rules."
APPROACH,0.15217391304347827,"Generating proofs with a complete set of deduction rules.
We follow the deduction rules of
natural deduction [Gentzen, 1935, Pfenning, 2004], a well-studied proof calculus with desirable
completeness properties.2 Examples of each deduction rule are shown in Table 2. For each type of
deduction rule, we randomly generate a proof that applies that rule (see details in section A.4. An
example of a compositional proof is shown in Figure 2. To prevent LLMs from exploiting knowledge
from pretraining to solve the problems without reasoning, we use fictional names for all concepts
(e.g. “wumpus” instead of “cat” etc.)."
APPROACH,0.1541501976284585,"2With minor differences: Negation introduction/elimination, truth introduction, and falsity elimination are
impossible to express in natural text, and they are omitted. We use proof by contradiction to reason over negation."
APPROACH,0.15612648221343872,"Varying proof width and depth.
To characterize the size or complexity of each proof, we represent
each proof as a tree (Figure 2), where each proof step corresponds to a node, and its premises
correspond to the parent nodes. Then the size of the proof can be naturally described by the width
and depth of this tree. When generating proofs, we control the depth by continuing to append proof
steps until a proof of the desired depth is generated. The number of premises of deduction rules is set
to the desired proof width."
APPROACH,0.15810276679841898,"Generating compositional proofs.
To generate proofs combining many different types of deduction
rules, we use a simple recursive procedure: (1) select a deduction rule uniformly at random, (2) select
the premises for the selected rule, (3) recursively generate a subproof for each premise. See section
A.5 for details and pseudocode."
APPROACH,0.1600790513833992,"Adding distractors.
One key challenge to OOD generalization is shortcut solutions. For example,
given the facts “Alex is a cat,” “All cats are feline,” since there is no other fact of the form “All
cats are...,” the model can deterministically follow the only valid deduction and conclude “Alex is
feline.” To make the heuristics uninformative, we add distractor sentences. In the above case, a
distractor sentence would be “All cats are graceful.” Then the model is forced to choose the correct
premise from two options for the next deduction step. See Section A.7 for details on distractors for
all deduction rules."
APPROACH,0.16205533596837945,"Formal evaluation of chain-of-thought.
Unlike previous datasets that evaluate on a binary
true/false answer, PRONTOQA-OOD requires LLMs to generate full proofs.3 Therefore, we need a
way to evaluate the correctness of the output proofs directly. The sentences in PRONTOQA-OOD
are syntactically simple and amenable to semantic parsing, which allows us to formally analyze each
step in the CoT. To determine whether a predicted CoT is correct, we: (1) semantically parse each
CoT sentence into first-order logic, (2) determine whether each logical form follows from previous
logical forms via a rule of deduction, and (3) compute whether there exists a path of correct steps
from the premises to the goal. An example of this process is shown below:"
APPROACH,0.16403162055335968,"“Alex is a dog.
All dogs are mammals.
Alex is a mammal.”"
APPROACH,0.16600790513833993,"dog(alex),
∀x(dog(x) →mammal(x)),
mammal(alex)"
APPROACH,0.16798418972332016,"∀x(dog(x) →mammal(x))
dog(alex)"
APPROACH,0.16996047430830039,mammal(alex)
APPROACH,0.17193675889328064,"Each proof step is considered correct if it is valid and if it immediately follows one of its premises.4
For further details see section A.6."
RESULTS,0.17391304347826086,"4
Results"
RESULTS,0.17588932806324112,Assets
RESULTS,0.17786561264822134,"FLAN-T5
LLaMA
GPT-3.5"
B,0.17984189723320157,"11B
65B
175B* PaLM"
"B
MODEL SIZE",0.18181818181818182,"540B
Model Size"
"B
MODEL SIZE",0.18379446640316205,Instruction Tuned RLHF
"B
MODEL SIZE",0.1857707509881423,"Open
Limited
Limited
Limited
Access"
"B
MODEL SIZE",0.18774703557312253,"FIGURE 3: An overview and properties of the LLMs in our experi-
ments. We place an asterisk* for GPT-3.5 since we were not able
to verify its size."
"B
MODEL SIZE",0.18972332015810275,"In this section, we test existing LLMs
on PRONTOQA-OOD and analyze
whether they can produce longer and
compositional proofs given simpler
demonstrations. We experiment with
a variety of models, with different
sizes and training objectives, as shown
in Figure 3. In all experiments, we use
8-shot chain-of-thought prompting.5
We compare performance in two settings: (1) an in-demonstration (ID) setting where the 8 in-context
demonstrations come from the same distribution as the test example, and (2) an out-of-demonstration
(OOD) setting where the in-context demonstrations come from a distribution that is different from
that of the test example. 95% confidence intervals are provided for all results."
"B
MODEL SIZE",0.191699604743083,"3True/false queries introduce an extra implicit negation step of the final conclusion. The negated conclusion
may be tricky to represent in natural language given the extensive space of potential conclusions of the proofs
that we generate.
4Note that if we only check for validity, the following proof would be considered correct: “Fae is a cat. All
cats are carnivores. Fae is a carnivore. All carnivores are mammals. Fae is a mammal. All mammals are not
herbivorous. Fae is not herbivorous.” Here, the goal is to prove “Fae is not herbivorous.” Every step of the proof
is correct except “All mammals are not herbivorous.” To avoid this, we require each step to immediately follow
one of its premises.
5Following PRONTOQA, since adding further in-context examples did not improve performance."
"B
MODEL SIZE",0.19367588932806323,"implication
elimination"
"B
MODEL SIZE",0.1956521739130435,"conjunction
introduction"
"B
MODEL SIZE",0.1976284584980237,"conjunction
elimination"
"B
MODEL SIZE",0.19960474308300397,"disjunction
introduction"
"B
MODEL SIZE",0.2015810276679842,"disjunction
elimination"
"B
MODEL SIZE",0.20355731225296442,"proof by
contradiction 0.00 0.25 0.50 0.75 1.00"
"B
MODEL SIZE",0.20553359683794467,proof accuracy
"B
MODEL SIZE",0.2075098814229249,ID: Rule generalization
"B
MODEL SIZE",0.20948616600790515,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
"B
MODEL SIZE",0.21146245059288538,"implication
elimination"
"B
MODEL SIZE",0.2134387351778656,"conjunction
introduction"
"B
MODEL SIZE",0.21541501976284586,"conjunction
elimination"
"B
MODEL SIZE",0.21739130434782608,"disjunction
introduction"
"B
MODEL SIZE",0.21936758893280633,"disjunction
elimination"
"B
MODEL SIZE",0.22134387351778656,"proof by
contradiction −1.0 −0.5 0.0 0.5"
"B
MODEL SIZE",0.22332015810276679,∆proof accuracy
"B
MODEL SIZE",0.22529644268774704,OOD: Rule generalization
"B
MODEL SIZE",0.22727272727272727,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
"B
MODEL SIZE",0.22924901185770752,"FIGURE 4: (top) Proof accuracy across examples with different deduction rules. The in-context examples and
test examples come from the same distribution. (bottom) Change in proof accuracy, where the test example
is out-of-demonstration with respect to the in-context examples. That is, the test example has the specified
deduction rule, but the in-context examples are uniformly distributed over all other deduction rules. See Figure 11
in the Appendix for the equivalent plot with absolute proof accuracy on the y-axis. See Figure 5 for an incorrect
example. Implication elimination examples have proof width of 1 and depth of 2. Conjunction introduction,
conjunction elimination, and disjunction introduction examples have proof width 3 and depth 2. Disjunction
elimination examples have proof width 3 and depth 1. Proof by contradiction examples have proof width 2 and
depth 1."
"B
MODEL SIZE",0.23122529644268774,"Prove:
Max is a gorpus."
"B
MODEL SIZE",0.233201581027668,"Predicted answer:
Max is a tumpus or a rompus or a lempus.
Max is a tumpus.
Tumpuses are wumpuses.
Max is a wumpus.
Rompuses are gorpuses.
Max is a gorpus.
Max is a gorpus."
"B
MODEL SIZE",0.23517786561264822,"Expected answer:
Assume Max is a tumpus.
Tumpuses are gorpuses.
Max is a gorpus.
Assume Max is a rompus.
Rompuses are gorpuses.
Max is a gorpus.
Assume Max is a lempus.
Lempuses are gorpuses.
Max is a gorpus.
Since Max is a tumpus or a rompus or a lempus, Max is a gorpus."
"B
MODEL SIZE",0.23715415019762845,"FIGURE 5: Example of an incorrect proof generated by GPT-3.5 on an out-of-demonstration disjunction
elimination example. The premises (axioms) are given in blue, and invalid steps are given in red. For the full
example, see Figure 14 in the Appendix."
"B
MODEL SIZE",0.2391304347826087,"4.1
Can LLMs use deduction rules other than modus ponens?"
"B
MODEL SIZE",0.24110671936758893,"We first evaluate whether LLMs “know” all deduction rules (Table 2) when provided with correspond-
ing CoT prompts. For each deduction rule, we independently and identically generate 8 in-context
examples and one test example, and prompt the model to answer the test example. We run each
experiment for 100 trials and measure the accuracy of the output proofs. The accuracies are shown in
the top chart of Figure 4. We emphasize that the ∆proof accuracies in the bottom row of the figure
should be interpreted in comparison with the accuracies in the top row (e.g. for some rules, the zero
∆accuracy for FLAN-T5 is due to zero absolute accuracy). For clarity, we provide the same plots
using absolute accuracy rather than ∆accuracy in Section A.2."
"B
MODEL SIZE",0.24308300395256918,"In general, most models are able to use each deduction rule reasonably well, with GPT-3.5 performing
the best. Similar to prior studies [Liang et al., 2022], we do not observe a strong correlation between
model size and performance. LLaMA performs comparably to PaLM, despite being smaller. FLAN-
T5 is smaller and performs reasonably on implication and conjunction elimination, but is not able to
learn the other deduction rules."
"B
MODEL SIZE",0.2450592885375494,min depth: 2
"B
MODEL SIZE",0.24703557312252963,rule types: 2
"B
MODEL SIZE",0.2490118577075099,min depth: 2
"B
MODEL SIZE",0.2509881422924901,rule types: 3
"B
MODEL SIZE",0.25296442687747034,min depth: 2
"B
MODEL SIZE",0.2549407114624506,rule types: 4
"B
MODEL SIZE",0.25691699604743085,min depth: 4
"B
MODEL SIZE",0.25889328063241107,rule types: 3 0.00 0.25 0.50 0.75 1.00
"B
MODEL SIZE",0.2608695652173913,proof accuracy
"B
MODEL SIZE",0.2628458498023715,ID: Compositional examples
"B
MODEL SIZE",0.2648221343873518,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
"B
MODEL SIZE",0.26679841897233203,min depth: 2
"B
MODEL SIZE",0.26877470355731226,rule types: 2
"B
MODEL SIZE",0.2707509881422925,min depth: 2
"B
MODEL SIZE",0.2727272727272727,rule types: 3
"B
MODEL SIZE",0.274703557312253,min depth: 2
"B
MODEL SIZE",0.2766798418972332,rule types: 4
"B
MODEL SIZE",0.27865612648221344,min depth: 4
"B
MODEL SIZE",0.28063241106719367,rule types: 3 −0.50 −0.25 0.00 0.25 0.50
"B
MODEL SIZE",0.2826086956521739,∆proof accuracy
"B
MODEL SIZE",0.2845849802371542,OOD: Compositional examples
"B
MODEL SIZE",0.2865612648221344,"Prove:
Polly is not a lempus."
"B
MODEL SIZE",0.2885375494071146,"Predicted answer:
Polly is a
wumpus, a jompus, and a tumpus.
Everything that is a wumpus, a
jompus, and a tumpus is not a
lempus.
Polly is not a lempus."
"B
MODEL SIZE",0.29051383399209485,"Expected answer:
Polly is a
tumpus.
Polly is a jompus.
Polly
is a wumpus.
Polly is a wumpus and
a jompus and a tumpus.
Everything
that is a wumpus, a jompus, and a
tumpus is not a lorpus.
Polly is
not a lorpus.
Assume Polly is a lempus.
Each
lempus is an impus and a lorpus and
a rompus.
Polly is an impus and
a lorpus and a rompus.
Polly is
a lorpus.
This contradicts with
Polly is not a lorpus.
Polly is
not a lempus."
"B
MODEL SIZE",0.2924901185770751,"FIGURE 6: (top-left) Proof accuracy on compositional examples where the in-context examples are also
compositional examples with the same min depth and number of rule types. (bottom-left) Change in proof
accuracy where the test examples are compositional but the in-context examples are those of individual deduction
rules. See Figure 12 in the Appendix for the equivalent plot with absolute proof accuracy on the y-axis. (right)
Example of an incorrect proof generated by GPT-3.5 on an out-of-demonstration example with min depth 2 and
4 rule types. The premises (axioms) are given in blue, and invalid steps are given in red. For the full example,
see Figure 15 in the Appendix."
OUT-OF-DEMONSTRATION GENERALIZATION,0.29446640316205536,"4.2
Out-of-demonstration generalization"
OUT-OF-DEMONSTRATION GENERALIZATION,0.2964426877470356,"4.2.1
Can LLMs generalize to unseen deduction rules?"
OUT-OF-DEMONSTRATION GENERALIZATION,0.2984189723320158,"While the above results show that LLMs are able to reason with a variety of deduction rules, it is
unclear whether the ability is learned from in-context examples or elicited from pretraining. We
test the LLM with examples where the test proof requires a deduction rule that does not appear in
the in-context examples (i.e. for each in-context example, we sample a deduction rule uniformly
at random from the set of deduction rules excluding that of the test example). Our intuition was
that LLMs would not be able to use deduction rules unless given explicit demonstrations thereof
(aside from those like modus ponens which are well-represented in pretraining). The change in proof
accuracy relative to the ID setting is shown in the bottom chart of Figure 4. Evidently, the models
are able to use four deduction rules despite not being shown an in-context example with those rules:
both conjunction rules, disjunction introduction, and (somewhat) implication elimination. GPT-3.5
was additionally able to use proof by contradiction by relying on an alternate deduction rule called
modus tollens (i.e. given ¬f(c) and ∀x(g(c) →f(c)), conclude ¬g(c)). This is in contrast with
McKenzie et al. [2022] which showed that reasoning with modus tollens exhibited inverse scaling
behavior, and yet GPT-3.5 is able to use it correctly without any demonstrations. However, Wei et al.
[2022a] has shown that when trained with additional compute, models are able to perform modus
tollens. GPT-3.5 performed worse on disjunction elimination possibly due to the fact that there is no
equivalent alternate rule (an example of an error is given in figure 5). However, no model is able to
use disjunction elimination and proof by contradiction without demonstrations."
OUT-OF-DEMONSTRATION GENERALIZATION,0.30039525691699603,"4.2.2
Can LLMs generalize to compositional proofs?"
OUT-OF-DEMONSTRATION GENERALIZATION,0.30237154150197626,"Next, we test whether the model is able to generalize to compositional proofs that contain multiple
different deduction rules. In the ID setting, the in-context examples and test examples are both
generated from the same distribution of compositional proofs. In the OOD setting, the in-context
demonstrations contain non-compositional examples of each rule that appears in the test example.
In Figure 6, in all but three experiments, we observe that the gap in proof accuracy between the ID
and OOD settings is close to zero, indicating that the models are able to generalize compositionally"
OUT-OF-DEMONSTRATION GENERALIZATION,0.30434782608695654,"2
3
4
5
proof depth of test example 0.00 0.25 0.50 0.75 1.00"
OUT-OF-DEMONSTRATION GENERALIZATION,0.30632411067193677,proof accuracy
OUT-OF-DEMONSTRATION GENERALIZATION,0.308300395256917,Depth generalization: Implication elimination
OUT-OF-DEMONSTRATION GENERALIZATION,0.3102766798418972,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
OUT-OF-DEMONSTRATION GENERALIZATION,0.31225296442687744,"2
3
4
5
proof depth of test example 0.00 0.25 0.50 0.75 1.00"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3142292490118577,proof accuracy
OUT-OF-DEMONSTRATION GENERALIZATION,0.31620553359683795,Depth generalization: Conjunction introduction
OUT-OF-DEMONSTRATION GENERALIZATION,0.3181818181818182,"2
3
4
5
proof width of test example 0.00 0.25 0.50 0.75 1.00"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3201581027667984,proof accuracy
OUT-OF-DEMONSTRATION GENERALIZATION,0.3221343873517787,Width generalization: Conjunction elimination
OUT-OF-DEMONSTRATION GENERALIZATION,0.3241106719367589,"2
3
4
5
proof width of test example 0.00 0.25 0.50 0.75 1.00"
OUT-OF-DEMONSTRATION GENERALIZATION,0.32608695652173914,proof accuracy
OUT-OF-DEMONSTRATION GENERALIZATION,0.32806324110671936,Width generalization: Conjunction introduction
OUT-OF-DEMONSTRATION GENERALIZATION,0.3300395256916996,"FIGURE 7: (top row) Proof accuracy vs proof depth of test examples, where in-context examples have fixed
proof depth 2. (bottom row) Proof accuracy vs proof width of test examples, where in-context examples have
fixed proof width 2. Dashed lines indicate in-distribution accuracy, where the depth and width of the in-context
examples are the same as that of the text-examples. In these experiments, there are 4 rather than 8 in-context
examples."
OUT-OF-DEMONSTRATION GENERALIZATION,0.33201581027667987,"to an extent. This is surprising since past studies show that LLMs struggle with compositional
generalization, but this could be due to the fact that much of the previous work focused on semantic
parsing rather than on reasoning. But there is prior work showing that models can generalize
compositionally in some settings, such as in Hosseini et al. [2022] (see Figure 4) and in Press et al.
[2022] (see Figure 6). In addition, our study is limited by the token limit of the LLMs, as we are
not able to further increase the complexity of the proofs without reducing the number of in-context
examples, which would render the results difficult to compare. GPT-3.5 and PALM have difficulty
when the number of rule types is 4, with an example of an incorrect output given in the right side
of Figure 6. Interestingly, PALM, LLAMA, and FLAN-T5 sometimes perform better in the OOD
setting than in the ID setting, showing that, in ICL, it is not always best to provide demonstrations
from the same distribution as the test example."
OUT-OF-DEMONSTRATION GENERALIZATION,0.3339920948616601,"4.2.3
Can LLMs generalize to bigger proofs?"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3359683794466403,"To test whether LLMs can generalize to bigger proofs, we test the models on examples where the
proof width or depth is larger than those of the in-context examples. As is evident from Figure 7, when
shown demonstrations of proofs of depth 2, the models’ performance decreases with increasing depth.
But this is due to the increase in the inherent difficulty of the task, as both ID and OOD accuracies
decrease with increasing depth. Though the notable exception is GPT-3.5 on conjunction elimination,
where ID accuracy remains high as OOD accuracy decreases. Models are able to generalize better
with increasing proof width on conjunction elimination, possibly because there are ample examples
of long conjunctions in natural language, but only GPT-3.5 is able to generalize to greater proof
widths on conjunction introduction."
OUT-OF-DEMONSTRATION GENERALIZATION,0.33794466403162055,"4.3
Do distractors help OOD generalization?"
OUT-OF-DEMONSTRATION GENERALIZATION,0.33992094861660077,"In supervised learning, one challenge to OOD generalization is spurious correlations [Zhang et al.,
2022b]. Intuitively, if ICL were to behave like supervised learning on in-context examples [Akyürek
et al., 2023, Dai et al., 2023, von Oswald et al., 2022], we would expect that without distractors,
the models would overfit to the in-context examples and perform poorly on OOD examples. An
example where distractors hurt generalization is shown in Figure 9 where GPT-3.5 copies many of
the distractor sentences into the output, likely due to the fact that it has learned to apply a copying
heuristic from the in-context demonstrations. It seems only GPT-3.5 acquires these heuristics in
implication and disjunction elimination. Surprisingly, this is not the case for all deduction rules, as is"
OUT-OF-DEMONSTRATION GENERALIZATION,0.34189723320158105,"implication
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3438735177865613,"conjunction
introduction"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3458498023715415,"conjunction
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.34782608695652173,"disjunction
introduction"
OUT-OF-DEMONSTRATION GENERALIZATION,0.34980237154150196,"disjunction
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.35177865612648224,"proof by
contradiction 0.00 0.25 0.50 0.75 1.00"
OUT-OF-DEMONSTRATION GENERALIZATION,0.35375494071146246,proof accuracy
OUT-OF-DEMONSTRATION GENERALIZATION,0.3557312252964427,ID: In-context examples with distractors
OUT-OF-DEMONSTRATION GENERALIZATION,0.3577075098814229,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
OUT-OF-DEMONSTRATION GENERALIZATION,0.35968379446640314,"implication
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3616600790513834,"conjunction
introduction"
OUT-OF-DEMONSTRATION GENERALIZATION,0.36363636363636365,"conjunction
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.36561264822134387,"disjunction
introduction"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3675889328063241,"disjunction
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3695652173913043,"proof by
contradiction −1.0 −0.5 0.0 0.5"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3715415019762846,∆proof accuracy
OUT-OF-DEMONSTRATION GENERALIZATION,0.37351778656126483,OOD: In-context examples without distractors
OUT-OF-DEMONSTRATION GENERALIZATION,0.37549407114624506,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3774703557312253,"implication
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3794466403162055,"conjunction
introduction"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3814229249011858,"conjunction
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.383399209486166,"disjunction
introduction"
OUT-OF-DEMONSTRATION GENERALIZATION,0.38537549407114624,"disjunction
elimination"
OUT-OF-DEMONSTRATION GENERALIZATION,0.38735177865612647,"proof by
contradiction −1.0 −0.5 0.0 0.5"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3893280632411067,∆proof accuracy
OUT-OF-DEMONSTRATION GENERALIZATION,0.391304347826087,OOD: In-context examples with irrelevant sentences
OUT-OF-DEMONSTRATION GENERALIZATION,0.3932806324110672,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
OUT-OF-DEMONSTRATION GENERALIZATION,0.3952569169960474,"FIGURE 8: (top) Proof accuracy on examples where both the in-context examples and test examples have
distractors. Sentences in all questions are ordered randomly. (middle) Proof accuracy when distractors are
removed from the in-context examples, but not from the test examples. The sentences of the in-context questions
have a fixed order (corresponding to a postorder traversal of the ontology tree), whereas the sentences in the
test question have random order. See Figure 13 in the Appendix for the equivalent plot with absolute proof
accuracy on the y-axis. (bottom) The same setting as (middle) except the distractors in the in-context examples
are instead replaced with irrelevant sentences. Implication elimination examples have proof width of 1 and
depth of 2. Conjunction introduction, conjunction elimination, and disjunction introduction examples have proof
width 2 and depth 2. Disjunction elimination examples have proof width 3 and depth 1. Proof by contradiction
examples have proof width 2 and depth 1."
OUT-OF-DEMONSTRATION GENERALIZATION,0.39723320158102765,"Prove:
Sally is dull."
OUT-OF-DEMONSTRATION GENERALIZATION,0.39920948616600793,"Predicted answer:
Sally is an impus.
Impuses are sterpuses.
Every grimpus is a
zumpus.
Every brimpus is a grimpus.
Lorpuses are dumpuses.
Brimpuses are vumpuses.
Every lorpus is a brimpus.
Sally is a brimpus.
Each vumpus is opaque.
Each dumpus
is not brown.
Every grimpus is dull.
Sally is a grimpus.
Sally is dull."
OUT-OF-DEMONSTRATION GENERALIZATION,0.40118577075098816,"Expected answer:
Sally is a brimpus.
Every brimpus is a grimpus.
Sally is a
grimpus.
Each grimpus is dull.
Sally is dull."
OUT-OF-DEMONSTRATION GENERALIZATION,0.4031620553359684,"FIGURE 9: Example of an incorrect proof generated by GPT-3.5 on an OOD implication elimination example
where the in-context demonstrations have no distractors, but the test example does. The premises (axioms) are
given in blue, and invalid steps are given in red. For the full example, see Figure 16 in the Appendix."
OUT-OF-DEMONSTRATION GENERALIZATION,0.4051383399209486,"visible in Figure 8. The models’ performance is largely unaffected, with the exception of a few rules
for specific models. This is in stark contrast to supervised learning, where it is always best to train on
examples from the same distribution as the test example."
CONCLUSION AND FUTURE WORK,0.40711462450592883,"5
Conclusion and future work"
CONCLUSION AND FUTURE WORK,0.4090909090909091,"In this study, we provide a systematic test of the general deductive reasoning capabilities of LLMs,
specifically measuring their rule-, depth-, width-, and compositional generalization abilities. We
found that LLMs exhibit mixed generalization to unseen deduction rules, but they exhibit more robust
generalization to compositional proofs than previously suggested."
CONCLUSION AND FUTURE WORK,0.41106719367588934,"One important future direction is to better understand the mechanism of ICL and CoT prompting.
We found that in many cases, for a given test example, the best in-context examples were drawn
from a distribution distinct from that of the test example. This is not explained by existing theories
of Bayesian inference [Xie et al., 2022] or gradient descent [Akyürek et al., 2023, Dai et al., 2023,
von Oswald et al., 2022]. Are simpler examples better even if the test example is fairly complex?
Should we include examples with a diverse set of deduction rules [Levy et al., 2022]? Or should the
in-context examples focus on rules for which the model’s OOD generalization is poor? Further study
is needed to better characterize generalization from in-context examples."
REPRODUCIBILITY STATEMENT,0.41304347826086957,Reproducibility statement
REPRODUCIBILITY STATEMENT,0.4150197628458498,"For the sake of reproducibility of the analysis, model outputs (except those of PaLM), code for
data generation, and analysis code are freely available with a permissive open-source license at
github.com/asaparov/prontoqa. The generated data for all experiments in this paper is avail-
able in the file generated_ood_data.zip. The command python make_plots.py produces all
figures used in this paper. GPT-3.5 experiments were run using the OpenAI API with the model
text-davinci-003 on April 20th–23rd, 2023. Experiments for Figure 7 and the bottom row of
Figure 8 were run on August 3rd–7th, 2023."
REPRODUCIBILITY STATEMENT,0.41699604743083,Acknowledgements
REPRODUCIBILITY STATEMENT,0.4189723320158103,"We thank Tania Bedrax-Weiss, Xin Xu, and Deepak Ramachandran for their valuable feedback. This
work was supported by Open Philanthropy, AWS AI, Samsung Advanced Institute of Technology
(under the project Next Generation Deep Learning: From Pattern Recognition to AI), and in part
through the NYU IT High Performance Computing resources, services, and staff expertise. NJ is
supported by an NSF Graduate Research Fellowship under grant number 1839302."
REFERENCES,0.4209486166007905,References
REFERENCES,0.42292490118577075,"Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning
algorithm is in-context learning? Investigations with linear models. In The Eleventh International
Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=
0g0X4H8yN4I."
REFERENCES,0.424901185770751,"Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, and Dongmei Zhang.
How do in-context examples affect compositional generalization? CoRR, abs/2305.04835, 2023.
URL https://arxiv.org/abs/2305.04835."
REFERENCES,0.4268774703557312,"Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay V. Ramasesh,
Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization
in large language models. CoRR, abs/2207.04901, 2022. doi: 10.48550/arXiv.2207.04901. URL
https://doi.org/10.48550/arXiv.2207.04901."
REFERENCES,0.4288537549407115,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Er-
ica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
Palm: Scaling language model-
ing with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL
https://doi.org/10.48550/arXiv.2204.02311."
REFERENCES,0.4308300395256917,"Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff
Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-
finetuned language models. CoRR, abs/2210.11416, 2022. doi: 10.48550/arXiv.2210.11416. URL
https://doi.org/10.48550/arXiv.2210.11416."
REFERENCES,0.43280632411067194,"Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-
context? language models secretly perform gradient descent as meta optimizers. In Findings of the
Association for Computational Linguistics: ACL 2023, Toronto, Canada, 2023. Association for
Computational Linguistics."
REFERENCES,0.43478260869565216,"G. Gentzen. Untersuchungen über das logische schließen i. Mathematische Zeitschrift, 39:176–210,
1935."
REFERENCES,0.4367588932806324,"Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Christopher Pal. Measuring systematic general-
ization in neural proof generation with transformers. In Hugo Larochelle, Marc’Aurelio Ranzato,
Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.
cc/paper/2020/hash/fc84ad56f9f547eb89c72b9bac209312-Abstract.html."
REFERENCES,0.43873517786561267,"Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy
Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian
Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R.
Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir
Radev. FOLIO: natural language reasoning with first-order logic. CoRR, abs/2209.00840, 2022.
doi: 10.48550/arXiv.2209.00840. URL https://doi.org/10.48550/arXiv.2209.00840."
REFERENCES,0.4407114624505929,"Arian Hosseini, Ankit Vani, Dzmitry Bahdanau, Alessandro Sordoni, and Aaron C. Courville.
On the compositional generalization gap of in-context learning. In Jasmijn Bastings, Yonatan
Belinkov, Yanai Elazar, Dieuwke Hupkes, Naomi Saphra, and Sarah Wiegreffe, editors, Pro-
ceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks
for NLP, BlackboxNLP@EMNLP 2022, Abu Dhabi, United Arab Emirates (Hybrid), Decem-
ber 8, 2022, pages 272–280. Association for Computational Linguistics, 2022. URL https:
//aclanthology.org/2022.blackboxnlp-1.22."
REFERENCES,0.4426877470355731,"Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.
CoRR, abs/2212.10403, 2022. doi: 10.48550/arXiv.2212.10403. URL https://doi.org/10.
48550/arXiv.2212.10403."
REFERENCES,0.44466403162055335,"Seyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. LAM-
BADA: backward chaining for automated reasoning in natural language. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics, Toronto, Canada, July 2023.
Association for Computational Linguistics."
REFERENCES,0.44664031620553357,"Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
Large language models are zero-shot reasoners.
In Advances in Neural Information Pro-
cessing Systems, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html."
REFERENCES,0.44861660079051385,"Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Ana Brassard, Masashi Yoshikawa, Keisuke Sak-
aguchi, and Kentaro Inui. Do deep neural networks capture compositionality in arithmetic reason-
ing? In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of
the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik,
Croatia, May 2-6, 2023, pages 1343–1354. Association for Computational Linguistics, 2023. URL
https://aclanthology.org/2023.eacl-main.98."
REFERENCES,0.4505928853754941,"Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional
generalization. CoRR, abs/2212.06800, 2022. doi: 10.48550/arXiv.2212.06800. URL https:
//doi.org/10.48550/arXiv.2212.06800."
REFERENCES,0.4525691699604743,"Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai
Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems
with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.
net/forum?id=IFXTZERXdM7."
REFERENCES,0.45454545454545453,"Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby
Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas,
Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu
Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun,
Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang,
Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas
Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang,
and Yuta Koreeda. Holistic evaluation of language models. CoRR, abs/2211.09110, 2022. doi:
10.48550/arXiv.2211.09110. URL https://doi.org/10.48550/arXiv.2211.09110."
REFERENCES,0.45652173913043476,"Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A
challenge dataset for machine reading comprehension with logical reasoning. In Christian Bessiere,
editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,
IJCAI 2020, pages 3622–3628. ijcai.org, 2020. doi: 10.24963/ijcai.2020/501. URL https:
//doi.org/10.24963/ijcai.2020/501."
REFERENCES,0.45849802371541504,"Ian McKenzie, Alexander Lyzhov, Alicia Parrish, Ameya Prabhu, Aaron Mueller, Najoung Kim,
Sam Bowman, and Ethan Perez. Inverse scaling prize: Second round winners, 2022. URL
https://irmckenzie.co.uk/round2."
REFERENCES,0.46047430830039526,"Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and
Augustus Odena. Show your work: Scratchpads for intermediate computation with language
models. CoRR, abs/2112.00114, 2021. URL https://arxiv.org/abs/2112.00114."
REFERENCES,0.4624505928853755,"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and
Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh,
Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information
Processing Systems, 2022. URL https://openreview.net/forum?id=TG8KACxEON."
REFERENCES,0.4644268774703557,"Frank Pfenning. Natural deduction. Lecture notes in 15-815 Automated Theorem Proving, 2004.
URL https://www.cs.cmu.edu/~fp/courses/atp/handouts/ch2-natded.pdf."
REFERENCES,0.466403162055336,"Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring
and narrowing the compositionality gap in language models. CoRR, abs/2210.03350, 2022. doi:
10.48550/arXiv.2210.03350. URL https://doi.org/10.48550/arXiv.2210.03350."
REFERENCES,0.4683794466403162,"Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and
Kristina Toutanova. Evaluating the impact of model scale for compositional generalization in
semantic parsing. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,
United Arab Emirates, December 7-11, 2022, pages 9157–9179. Association for Computational
Linguistics, 2022. URL https://aclanthology.org/2022.emnlp-main.624."
REFERENCES,0.47035573122529645,"Soumya Sanyal, Zeyi Liao, and Xiang Ren. Robustlr: A diagnostic benchmark for evaluating logical
robustness of deductive reasoners. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 9614–9631.
Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.emnlp-main.653. URL
https://doi.org/10.18653/v1/2022.emnlp-main.653."
REFERENCES,0.4723320158102767,"Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis
of chain-of-thought. In The Eleventh International Conference on Learning Representations, 2023.
URL https://openreview.net/forum?id=qFVVBzXxR2V."
REFERENCES,0.4743083003952569,"Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. CLUTRR:
A diagnostic benchmark for inductive reasoning from text. In Kentaro Inui, Jing Jiang, Vincent
Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4505–4514.
Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1458. URL https:
//doi.org/10.18653/v1/D19-1458."
REFERENCES,0.4762845849802372,"Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and
abductive statements over natural language. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto
Navigli, editors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021,
Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 3621–3634.
Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.317. URL
https://doi.org/10.18653/v1/2021.findings-acl.317."
REFERENCES,0.4782608695652174,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand
Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language
models. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.
org/10.48550/arXiv.2302.13971."
REFERENCES,0.48023715415019763,"Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev,
Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent.
CoRR, abs/2212.07677, 2022. doi: 10.48550/arXiv.2212.07677. URL https://doi.org/10.
48550/arXiv.2212.07677."
REFERENCES,0.48221343873517786,"Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic mod-
els: Explaining and finding good demonstrations for in-context learning. CoRR, abs/2301.11916,
2023.
doi: 10.48550/arXiv.2301.11916.
URL https://doi.org/10.48550/arXiv.2301.
11916."
REFERENCES,0.4841897233201581,"Jason Wei, Yi Tay, and Quoc V. Le. Inverse scaling can become u-shaped. CoRR, abs/2211.02011,
2022a. doi: 10.48550/arXiv.2211.02011. URL https://doi.org/10.48550/arXiv.2211.
02011."
REFERENCES,0.48616600790513836,"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V
Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in
Neural Information Processing Systems, 2022b. URL https://openreview.net/forum?id=
_VjQlMeSB_J."
REFERENCES,0.4881422924901186,"Yuhuai Wu, Albert Q. Jiang, Jimmy Ba, and Roger Baker Grosse. INT: an inequality benchmark
for evaluating generalization in theorem proving. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL
https://openreview.net/forum?id=O6LPudowNQm."
REFERENCES,0.4901185770750988,"Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context
learning as implicit bayesian inference. In The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL
https://openreview.net/forum?id=RdJVFCHjUMI."
REFERENCES,0.49209486166007904,"Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru.
Complementary explanations for effective in-context learning. CoRR, abs/2211.13892, 2022. doi:
10.48550/arXiv.2211.13892. URL https://doi.org/10.48550/arXiv.2211.13892."
REFERENCES,0.49407114624505927,"Hanlin Zhang, Yi-Fan Zhang, Li Erran Li, and Eric P. Xing. The impact of symbolic representations
on in-context learning for few-shot reasoning. CoRR, abs/2212.08686, 2022a. doi: 10.48550/arXiv.
2212.08686. URL https://doi.org/10.48550/arXiv.2212.08686."
REFERENCES,0.49604743083003955,"Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the
paradox of learning to reason from data. CoRR, abs/2205.11502, 2022b. doi: 10.48550/arXiv.
2205.11502. URL https://doi.org/10.48550/arXiv.2205.11502."
REFERENCES,0.4980237154150198,"A
Appendix"
REFERENCES,0.5,"A.1
Example from PRONTOQA-OOD"
REFERENCES,0.5019762845849802,"Q: Everything that is a lorpus, a brimpus, or a jompus is a shumpus.
Every
wumpus is a vumpus and a sterpus and a brimpus.
Everything that is a
vumpus, a grimpus, or a brimpus is a lempus.
Everything that is a lempus
or a jompus or a lorpus is a dumpus.
Vumpuses are rompuses.
Every sterpus
is a gorpus.
Everything that is a vumpus, a grimpus, or a brimpus is a
dumpus.
Wumpuses are shumpuses.
Polly is a rompus.
Polly is a wumpus.
Prove:
Polly is a lempus or an impus or a yumpus."
REFERENCES,0.5039525691699605,"A: Polly is a wumpus.
Every wumpus is a vumpus and a sterpus and a brimpus.
Polly is a vumpus and a sterpus and a brimpus.
Polly is a brimpus.
Polly
is a vumpus, a grimpus, or a brimpus.
Everything that is a vumpus, a
grimpus, or a brimpus is a lempus.
Polly is a lempus.
Polly is a lempus,
an impus, or a yumpus."
REFERENCES,0.5059288537549407,"FIGURE 10: An example from PRONTOQA-OOD. This is a compositional example with a min depth of 4 and
3 rule types. The given answer is the expected chain-of-thought. The question is shown in blue, the query in
red, and the chain-of-thought/answer in green."
REFERENCES,0.5079051383399209,"A.2
Results with absolute accuracy"
REFERENCES,0.5098814229249012,"implication
elimination"
REFERENCES,0.5118577075098815,"conjunction
introduction"
REFERENCES,0.5138339920948617,"conjunction
elimination"
REFERENCES,0.5158102766798419,"disjunction
introduction"
REFERENCES,0.5177865612648221,"disjunction
elimination"
REFERENCES,0.5197628458498024,"proof by
contradiction 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.5217391304347826,proof accuracy
REFERENCES,0.5237154150197628,ID: Rule generalization
REFERENCES,0.525691699604743,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
REFERENCES,0.5276679841897233,"implication
elimination"
REFERENCES,0.5296442687747036,"conjunction
introduction"
REFERENCES,0.5316205533596838,"conjunction
elimination"
REFERENCES,0.5335968379446641,"disjunction
introduction"
REFERENCES,0.5355731225296443,"disjunction
elimination"
REFERENCES,0.5375494071146245,"proof by
contradiction 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.5395256916996047,proof accuracy
REFERENCES,0.541501976284585,OOD: Rule generalization
REFERENCES,0.5434782608695652,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
REFERENCES,0.5454545454545454,"FIGURE 11: (top) Proof accuracy across examples with different deduction rules. The in-context examples
and test examples come from the same distribution. (bottom) Proof accuracy where the test example is out-of-
demonstration with respect to the in-context examples (for comparison, the in-demonstration proof accuracy is
shown as the dotted black bars). That is, the test example has the specified deduction rule, but the in-context
examples are uniformly distributed over all other deduction rules. See Figure 5 for an incorrect example.
Implication elimination examples have proof width of 1 and depth of 2. Conjunction introduction, conjunction
elimination, and disjunction introduction examples have proof width 3 and depth 2. Disjunction elimination
examples have proof width 3 and depth 1. Proof by contradiction examples have proof width 2 and depth 1."
REFERENCES,0.5474308300395256,min depth: 2
REFERENCES,0.549407114624506,rule types: 2
REFERENCES,0.5513833992094862,min depth: 2
REFERENCES,0.5533596837944664,rule types: 3
REFERENCES,0.5553359683794467,min depth: 2
REFERENCES,0.5573122529644269,rule types: 4
REFERENCES,0.5592885375494071,min depth: 4
REFERENCES,0.5612648221343873,rule types: 3 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.5632411067193676,proof accuracy
REFERENCES,0.5652173913043478,ID: Compositional examples
REFERENCES,0.567193675889328,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
REFERENCES,0.5691699604743083,min depth: 2
REFERENCES,0.5711462450592886,rule types: 2
REFERENCES,0.5731225296442688,min depth: 2
REFERENCES,0.575098814229249,rule types: 3
REFERENCES,0.5770750988142292,min depth: 2
REFERENCES,0.5790513833992095,rule types: 4
REFERENCES,0.5810276679841897,min depth: 4
REFERENCES,0.5830039525691699,rule types: 3 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.5849802371541502,proof accuracy
REFERENCES,0.5869565217391305,OOD: Compositional examples
REFERENCES,0.5889328063241107,"FIGURE 12: (top) Proof accuracy on compositional examples where the in-context examples are also composi-
tional examples with the same min depth and number of rule types. (bottom) Proof accuracy where the test
examples are compositional but the in-context examples are those of individual deduction rules (for comparison,
the in-demonstration proof accuracy is shown as the dotted black bars)."
REFERENCES,0.5909090909090909,"implication
elimination"
REFERENCES,0.5928853754940712,"conjunction
introduction"
REFERENCES,0.5948616600790514,"conjunction
elimination"
REFERENCES,0.5968379446640316,"disjunction
introduction"
REFERENCES,0.5988142292490118,"disjunction
elimination"
REFERENCES,0.6007905138339921,"proof by
contradiction 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.6027667984189723,proof accuracy
REFERENCES,0.6047430830039525,ID: In-context examples with distractors
REFERENCES,0.6067193675889329,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
REFERENCES,0.6086956521739131,"implication
elimination"
REFERENCES,0.6106719367588933,"conjunction
introduction"
REFERENCES,0.6126482213438735,"conjunction
elimination"
REFERENCES,0.6146245059288538,"disjunction
introduction"
REFERENCES,0.616600790513834,"disjunction
elimination"
REFERENCES,0.6185770750988142,"proof by
contradiction 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.6205533596837944,proof accuracy
REFERENCES,0.6225296442687747,OOD: In-context examples without distractors
REFERENCES,0.6245059288537549,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
REFERENCES,0.6264822134387352,"implication
elimination"
REFERENCES,0.6284584980237155,"conjunction
introduction"
REFERENCES,0.6304347826086957,"conjunction
elimination"
REFERENCES,0.6324110671936759,"disjunction
introduction"
REFERENCES,0.6343873517786561,"disjunction
elimination"
REFERENCES,0.6363636363636364,"proof by
contradiction 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.6383399209486166,proof accuracy
REFERENCES,0.6403162055335968,OOD: In-context examples with irrelevant sentences
REFERENCES,0.642292490118577,"GPT-3.5
PaLM
LLaMA
FLAN-T5"
REFERENCES,0.6442687747035574,"FIGURE 13: (top) Proof accuracy on examples where both the in-context examples and test examples have
distractors. Sentences in all questions are ordered randomly. (middle) Proof accuracy when distractors are
removed from the in-context examples, but not from the test examples (for comparison, the in-demonstration
proof accuracy is shown as the dotted black bars). The sentences of the in-context questions have a fixed order
(corresponding to a postorder traversal of the ontology tree), whereas the sentences in the test question have
random order. (bottom) The same setting as (middle) except the distractors in the in-context examples are
instead replaced with irrelevant sentences. Implication elimination examples have proof width of 1 and depth of
2. Conjunction introduction, conjunction elimination, and disjunction introduction examples have proof width 2
and depth 2. Disjunction elimination examples have proof width 3 and depth 1. Proof by contradiction examples
have proof width 2 and depth 1."
REFERENCES,0.6462450592885376,"A.3
Full examples of incorrect predicted proofs"
REFERENCES,0.6482213438735178,"Question:
Lempuses are gorpuses.
Rompuses are wumpuses.
Tumpuses are wumpuses.
Dumpuses are gorpuses.
Every zumpus is a wumpus.
Every brimpus is a gorpus.
Yumpuses are gorpuses.
Lempuses are wumpuses.
Rompuses are gorpuses.
Tumpuses
are gorpuses.
Max is a tumpus or a rompus or a lempus.
Max is a zumpus, a tumpus, or
a rompus."
REFERENCES,0.650197628458498,"Prove:
Max is a gorpus."
REFERENCES,0.6521739130434783,"Predicted answer:
Max is a tumpus or a rompus or a lempus.
Max is a tumpus.
Tumpuses are wumpuses.
Max is a wumpus.
Rompuses are gorpuses.
Max is a gorpus.
Max is a gorpus."
REFERENCES,0.6541501976284585,"Expected answer:
Assume Max is a tumpus.
Tumpuses are gorpuses.
Max is a gorpus.
Assume Max is a rompus.
Rompuses are gorpuses.
Max is a gorpus.
Assume Max is a lempus.
Lempuses are gorpuses.
Max is a gorpus.
Since Max is a tumpus or a rompus or a lempus, Max is a gorpus."
REFERENCES,0.6561264822134387,"FIGURE 14: Example of an incorrect proof generated by GPT-3.5 on an out-of-demonstration disjunction
elimination example. The premises (axioms) are given in blue, and invalid steps are given in red."
REFERENCES,0.658102766798419,"Question:
Polly is a wumpus.
Every rompus is a dumpus.
Lempuses are vumpuses.
Everything that is a wumpus, a jompus, and a tumpus is a dumpus.
Polly is a gorpus.
Every impus is a brimpus.
Everything that is a wumpus, a gorpus, and a shumpus is
a vumpus.
Each lempus is an impus and a lorpus and a rompus.
Everything that is
a wumpus, a jompus, and a tumpus is not a lorpus.
Zumpuses are lorpuses.
Polly is
a shumpus.
Polly is a zumpus.
Polly is a zumpus.
Polly is a wumpus.
Polly is a
jompus.
Polly is a tumpus."
REFERENCES,0.6600790513833992,"Prove:
Polly is not a lempus."
REFERENCES,0.6620553359683794,"Predicted answer:
Polly is a wumpus, a jompus, and a tumpus.
Everything that is a
wumpus, a jompus, and a tumpus is not a lempus.
Polly is not a lempus."
REFERENCES,0.6640316205533597,"Expected answer:
Polly is a tumpus.
Polly is a jompus.
Polly is a wumpus.
Polly
is a wumpus and a jompus and a tumpus.
Everything that is a wumpus, a jompus, and a
tumpus is not a lorpus.
Polly is not a lorpus.
Assume Polly is a lempus.
Each lempus is an impus and a lorpus and a rompus.
Polly
is an impus and a lorpus and a rompus.
Polly is a lorpus.
This contradicts with
Polly is not a lorpus.
Polly is not a lempus."
REFERENCES,0.66600790513834,"FIGURE 15: Example of an incorrect proof generated by GPT-3.5 on an out-of-demonstration example with
min depth 2 and 4 rule types. The premises (axioms) are given in blue, and invalid steps are given in red."
REFERENCES,0.6679841897233202,"Question:
Every grimpus is a zumpus.
Each vumpus is opaque.
Each dumpus is not
brown.
Each grimpus is dull.
Lorpuses are moderate.
Every brimpus is rainy.
Impuses are temperate.
Lorpuses are dumpuses.
Brimpuses are vumpuses.
Every
lorpus is a brimpus.
Zumpuses are angry.
Grimpuses are yumpuses.
Every brimpus
is a grimpus.
Impuses are sterpuses.
Sally is an impus.
Sally is a brimpus."
REFERENCES,0.6699604743083004,"Prove:
Sally is dull."
REFERENCES,0.6719367588932806,"Predicted answer:
Sally is an impus.
Impuses are sterpuses.
Every grimpus is a
zumpus.
Every brimpus is a grimpus.
Lorpuses are dumpuses.
Brimpuses are vumpuses.
Every lorpus is a brimpus.
Sally is a brimpus.
Each vumpus is opaque.
Each dumpus
is not brown.
Every grimpus is dull.
Sally is a grimpus.
Sally is dull."
REFERENCES,0.6739130434782609,"Expected answer:
Sally is a brimpus.
Every brimpus is a grimpus.
Sally is a
grimpus.
Each grimpus is dull.
Sally is dull."
REFERENCES,0.6758893280632411,"FIGURE 16: Example of an incorrect proof generated by GPT-3.5 on an OOD implication elimination example
where the in-context demonstrations have no distractors, but the test example does. The premises (axioms) are
given in blue, and invalid steps are given in red."
REFERENCES,0.6778656126482213,"A.4
Generative process details"
REFERENCES,0.6798418972332015,"In this section, we describe the process to generate examples of each deduction rule."
REFERENCES,0.6818181818181818,"Implication elimination (i.e. modus ponens)
Given f(c) and ∀x(f(x) →g(x)), prove g(c). These
are the examples in the original PRONTOQA. We follow the same process here:"
REFERENCES,0.6837944664031621,"1. Generate an ontology. For simplicity, we generate linear ontologies, consisting of a collection
of concepts, as well as subtype-supertype relations between those concepts (i.e. concept f is a
subtype of the supertype g if every instance of f is an instance of g). For simplicity, we limit
each type to have at most one supertype.
2. Perform a random walk of length k from a randomly selected start vertex, where k is the desired
proof depth.
3. Traverse the edges of the ontology and convert each into a sentence of the question.
4. Convert each step of the random walk into a sentence of the gold chain-of-thought.
Note that this process allows us to generate proofs of any depth, but the width is fixed to 1."
REFERENCES,0.6857707509881423,"Conjunction introduction
Given A and B, prove A ∧B. The generative process is a modified
version of that for implication elimination. Instead of generating rules of the form ∀x(f(x) →g(x)),
we generate rules of the form ∀x(f1(x) ∧. . . ∧fn(x) →g(x)), where n is the proof width. Given,
f1(c), . . ., and fn(c), the model must first prove f1(c) ∧. . . ∧fn(c) before applying implication
elimination to prove g(c). To increase the depth of the proof, g(c) itself can be part of a conjunct in
the antecedent of another rule."
REFERENCES,0.6877470355731226,"Conjunction elimination
Given A ∧B, prove A. These examples are identical to those in conjunc-
tion introduction, except the conjunction appears in the consequent of each rule, rather than in the
antecedent: ∀x(f(x) →g1(x) ∧. . . ∧gn(x)) where n is the proof width."
REFERENCES,0.6897233201581028,"Disjunction introduction
Given A, prove A ∨B. These examples are identical to those in conjunc-
tion introduction, except the conjunction is replaced with disjunction: ∀x(f1(x)∨. . .∨fn(x) →g(x))
where n is the proof width. But note that grounded axioms are not necessary for every disjunct: To
apply the rule ∀x(f1(x) ∨. . . ∨fn(x) →g(x)), knowing fn(c) is sufficient, and we do not need to
generate grounded axioms for the other disjuncts fi(c) for i < n."
REFERENCES,0.691699604743083,"Disjunction elimination (i.e. proof by cases)
Given A1 ∨. . . ∨An, and Ai ⊢C for all i, prove
C. Here, n is the proof width. While it is possible to construct proofs containing multiple nested
applications of disjunction elimination, such proofs are quite complex, even for humans to understand,
and so we fix the depth of these examples to 1. To generate an example, we first generate the
disjunction: f1(c) ∨. . . ∨fn(c). Next, generate the rules for each case: ∀x(fi(x) →g(x)) for all i.
The goal is to prove g(c)."
REFERENCES,0.6936758893280632,"Proof by contradiction
Given A ⊢B and ¬B, prove ¬A. Note that this is a rule composed
of two natural deduction rules: negation elimination and introduction. But since those individual
rules do not lend themselves to a natural text representation, we choose to study their composition.
Similar to disjunction elimination, it is possible to construct proofs containing multiple nested
applications of proof by contradiction, but such proofs are unnaturally complex. So we fix the
depth to 1. To generate an example, we first generate an axiom ¬g(c). Next, for each subproof, we
generate a rule ∀x(f1(x) ∨. . . ∨fn(x) →g(x)), where n is the proof width. The goal is to prove
¬f1(c) ∧. . . ∧¬fn(c). Note that in addition to proof by contradiction, this proof requires disjunction
introduction, implication elimination, and conjunction introduction."
REFERENCES,0.6956521739130435,"Note that the above list constitutes a complete set of deduction rules from propositional natural
deduction, save for one rule: implication introduction. However, it is unclear how to construct an
example with this deduction rule where its difficulty can be controlled by increasing the width or
depth of the proof (e.g. how can a statement of the form A1 →A2 →. . . →An be expressed in
natural language?)."
REFERENCES,0.6976284584980237,"Algorithm 1: Pseudocode for generating examples of compositional proofs in PRONTOQA-OOD. In
this algorithm, Ωdenotes the set of all logical forms. The function generate_compositional_proof is
initially called with parameters Ω, ∅, d, e, and false, where d is the requested depth and e is a randomly
selected entity name (e.g. alex, fae, etc). sample is a helper function that, given an input set of logical
forms S and an entity e, returns sample_uniform({set of logical forms in S with minimal depth where all
atoms are of the form t(e) where t is a predicate})."
REFERENCES,0.6996047430830039,"1 function generate_compositional_proof(set of possible conclusions (logical forms) C,
disallowed deduction rules R,
requested depth d,
ground entity e,
is proof hypothetical h)"
INITIALIZE A AS THE SET OF ALL DEDUCTION RULES EXCLUDING THOSE IN R,0.7015810276679841,"2
initialize A as the set of all deduction rules excluding those in R
/* filter deduction rules such that:
(1) an element of C can be a conclusion
of the rule, (2) for which we have sufficient depth, and (3) we don’t create
overly complex logical forms */"
IF C DOES NOT CONTAIN A CONJUNCTION,0.7035573122529645,"3
if C does not contain a conjunction"
IF C DOES NOT CONTAIN A CONJUNCTION,0.7055335968379447,"4
set A = A \ {conjunction_introduction}"
IF C DOES NOT CONTAIN A DISJUNCTION,0.7075098814229249,"5
if C does not contain a disjunction"
IF C DOES NOT CONTAIN A DISJUNCTION,0.7094861660079052,"6
set A = A \ {disjunction_introduction}"
IF C DOES NOT CONTAIN A DISJUNCTION,0.7114624505928854,"7
if h = true or d = 1 or C does not contain a negation"
IF C DOES NOT CONTAIN A DISJUNCTION,0.7134387351778656,"8
set A = A \ {proof_by_contradiction}"
IF C DOES NOT CONTAIN A DISJUNCTION,0.7154150197628458,"9
if h = true or d = 1 or C contains only conjunctions or only disjunctions"
IF C DOES NOT CONTAIN A DISJUNCTION,0.717391304347826,"10
set A = A \ {disjunction_elimination}"
IF C CONTAINS ONLY CONJUNCTIONS OR ONLY DISJUNCTIONS,0.7193675889328063,"11
if C contains only conjunctions or only disjunctions"
IF C CONTAINS ONLY CONJUNCTIONS OR ONLY DISJUNCTIONS,0.7213438735177866,"12
set A = A \ {conjunction_elimination}"
IF C CONTAINS ONLY CONJUNCTIONS OR ONLY DISJUNCTIONS AND ANY OPERAND IS NEGATED,0.7233201581027668,"13
if C contains only conjunctions or only disjunctions and any operand is negated"
IF C CONTAINS ONLY CONJUNCTIONS OR ONLY DISJUNCTIONS AND ANY OPERAND IS NEGATED,0.7252964426877471,"14
set A = A \ {implication_elimination}"
IF C CONTAINS ONLY CONJUNCTIONS OR ONLY DISJUNCTIONS AND ANY OPERAND IS NEGATED,0.7272727272727273,"15
if d = 0 or C contains a singleton logical form or A = ∅"
IF C CONTAINS ONLY CONJUNCTIONS OR ONLY DISJUNCTIONS AND ANY OPERAND IS NEGATED,0.7292490118577075,"16
return axiom step with conclusion given by sample(C, e)"
IF C CONTAINS ONLY CONJUNCTIONS OR ONLY DISJUNCTIONS AND ANY OPERAND IS NEGATED,0.7312252964426877,"17
r = sample_uniform(A)"
IF C CONTAINS ONLY CONJUNCTIONS OR ONLY DISJUNCTIONS AND ANY OPERAND IS NEGATED,0.733201581027668,"18
if r = implication_elimination"
DO,0.7351778656126482,"19
do"
DO,0.7371541501976284,"20
for any c ∈C, a and c share any operands or negations of operands"
DO,0.7391304347826086,"21
while a = generate_compositional_proof(Ω, ∅, d −1, e, h)"
DO,0.741106719367589,"22
do"
A AND S DO NOT SHARE ANY OPERANDS OR NEGATIONS OF OPERANDS,0.7430830039525692,"23
a and s do not share any operands or negations of operands"
A AND S DO NOT SHARE ANY OPERANDS OR NEGATIONS OF OPERANDS,0.7450592885375494,"24
while s = sample(C, e)"
A AND S DO NOT SHARE ANY OPERANDS OR NEGATIONS OF OPERANDS,0.7470355731225297,"25
return implication_elimination with premises a and ∀x(a[e →x] →s[e →x])"
A AND S DO NOT SHARE ANY OPERANDS OR NEGATIONS OF OPERANDS,0.7490118577075099,"26
else if r = conjunction_introduction"
A AND S DO NOT SHARE ANY OPERANDS OR NEGATIONS OF OPERANDS,0.7509881422924901,"27
initialize P as an empty list, and i = 0"
A AND S DO NOT SHARE ANY OPERANDS OR NEGATIONS OF OPERANDS,0.7529644268774703,"28
L = |C| if C contains only conjunctions, else L = 3"
DO,0.7549407114624506,"29
do"
DO,0.7569169960474308,"30
let Ci = ith operand of C if C contains only conjunctions, else Ci = Ω"
DO,0.758893280632411,"31
a = generate_compositional_proof(Ci, {conjunction_elimination}, d −1, e, h)"
IF A IS ATOMIC AND A IS NOT ANY OTHER OPERAND OF C,0.7608695652173914,"32
if a is atomic and a is not any other operand of C"
APPEND A TO P,0.7628458498023716,"33
append a to P"
APPEND A TO P,0.7648221343873518,"34
i = i + 1"
APPEND A TO P,0.766798418972332,"35
while i < L"
APPEND A TO P,0.7687747035573123,"36
return conjunction_introduction with premises P"
APPEND A TO P,0.7707509881422925,"37
else if r = conjunction_elimination"
APPEND A TO P,0.7727272727272727,"38
let C′ be the set of conjunctions of length 3, i = sample_uniform({1, 2, 3})"
APPEND A TO P,0.7747035573122529,"39
C′ = {c ∈C′ : the ith operand of c′ is in C}"
DO,0.7766798418972332,"40
do"
DO,0.7786561264822134,"41
a = generate_compositional_proof(C′, {conjunction_introduction}, d −1, e, h)"
DO,0.7806324110671937,"42
while a has no duplicate operands, and each operand of a is not itself a conjunction or disjunction"
DO,0.782608695652174,"43
return conjunction_elimination with premise a and conclusion given by the ith operand of a"
DO,0.7845849802371542,Algorithm 1: (continued from previous page)
DO,0.7865612648221344,"44
else if r = disjunction_introduction"
DO,0.7885375494071146,"45
if C = Ωlet C be the set of disjunctions of length 3"
DO,0.7905138339920948,"46
i = sample_uniform( number of disjuncts in C)"
DO,0.7924901185770751,"47
do"
DO,0.7944664031620553,"48
let Ci = ith operand of C"
DO,0.7964426877470355,"49
a = generate_compositional_proof(Ci, {disjunction_elimination}, d −1, e, h)"
WHILE A IS ATOMIC AND A IS NOT ANY OTHER OPERAND OF C,0.7984189723320159,"50
while a is atomic and a is not any other operand of C"
REPLACE ITH OPERAND OF C WITH A,0.8003952569169961,"51
replace ith operand of C with a"
DO,0.8023715415019763,"52
do"
DO,0.8043478260869565,"53
x = sample(C, e)"
WHILE ITH DISJUNCT OF X IS DISTINCT FROM ALL OTHER DISJUNCTS,0.8063241106719368,"54
while ith disjunct of x is distinct from all other disjuncts"
WHILE ITH DISJUNCT OF X IS DISTINCT FROM ALL OTHER DISJUNCTS,0.808300395256917,"55
return disjunction_introduction with premise given by the ith operand of x and conclusion x"
WHILE ITH DISJUNCT OF X IS DISTINCT FROM ALL OTHER DISJUNCTS,0.8102766798418972,"56
else if r = disjunction_elimination"
INITIALIZE P AS AN EMPTY LIST,0.8122529644268774,"57
initialize P as an empty list"
INITIALIZE P AS AN EMPTY LIST,0.8142292490118577,"58
while |P| < 2 do"
INITIALIZE P AS AN EMPTY LIST,0.8162055335968379,"59
p = generate_compositional_proof(C, {disjunction_introduction}, d −1, e, true)"
INITIALIZE P AS AN EMPTY LIST,0.8181818181818182,"60
if p is not a conjunction or disjunction and p has an axiom that is not an axiom of any q ∈P"
APPEND P TO P,0.8201581027667985,"61
append p to P"
APPEND P TO P,0.8221343873517787,"62
let Ai be the set of axioms of Pi that are not axioms of Pj for i ̸= j"
APPEND P TO P,0.8241106719367589,"63
let ai = sample_uniform(Ai) for all i"
APPEND P TO P,0.8260869565217391,"64
let a′ be a disjunction with disjuncts ai"
APPEND P TO P,0.8280632411067194,"65
a = generate_compositional_proof({a′}, {disjunction_introduction}, d −1, e, h)"
APPEND P TO P,0.8300395256916996,"66
return disjunction_introduction with premises a and Pi"
APPEND P TO P,0.8320158102766798,"67
else if r = proof_by_contradiction"
LET N BE THE SET OF ALL NEGATED LOGICAL FORMS,0.83399209486166,"68
let N be the set of all negated logical forms"
LET N BE THE SET OF ALL NEGATED LOGICAL FORMS,0.8359683794466403,"69
a = generate_compositional_proof(N, {proof_by_contradiction}, d −1, h)"
DO,0.8379446640316206,"70
do"
DO,0.8399209486166008,"71
let a = ¬s"
DO,0.841897233201581,"72
b = generate_compositional_proof({s}, {proof_by_contradiction}, d −1, e, true)"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8438735177865613,"73
while b has an atomic non-negated axiom that is not an axiom of a"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8458498023715415,"74
s′ = sample_uniform({atomic non-negated axioms of b that are not axioms of a})"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8478260869565217,"75
return proof_by_contradiction with premises a and b and conclusion ¬s′"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.849802371541502,"A.5
Generating compositional proofs"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8517786561264822,"We use a simple recursive procedure to generate compositional proofs: (1) select a deduction
rule uniformly at random, (2) select the premises for the selected rule, (3) recursively generate a
subproof for each premise. A consistency checking step is required to make sure we avoid generating
contradictory axioms.6 In addition, we avoid generating an elimination rule directly following an
introduction rule (or vice versa).7 See Algorithm 1 in the appendix for pseudocode of this procedure.
To test compositional proofs of various sizes, we implement a parameter that controls the minimum
depth of the proof tree, and another parameter that controls the number of distinct rule types in each
proof."
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8537549407114624,"A.6
Further details on evaluation of CoT"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8557312252964426,"We aim to test whether LLMs are able to use deduction rules OOD, where the rules do not appear
in the in-context examples, and we take care not to be overly strict. For example, we wish to avoid
penalizing the model for formatting differences, so long as the reasoning is correct. To this end, in
determining whether a logical form follows from previous logical forms, we consider any deduction
rule listed in Table 2. We also allow for two additional rules: (1) given ∀x(f(x) →g(x)) and"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.857707509881423,"6An example is: Suppose we select conjunction introduction as the first rule; next, we recursively generate
the proof of each conjunct; suppose for each of these, we choose to generate the axioms cat(alex) and
¬cat(alex).
7In the following example, a conjunction introduction step immediately follows a conjunction elimination
step: “Jay is a cat and orange. Jay is a cat. Jay is orange. Jay is a cat and orange.”"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8596837944664032,"∀x(g(x) →h(x)) conclude ∀x(f(x) →h(x)), and (2) given ∀x(f(x) →g(x)) and ¬g(c) conclude
¬f(c) (i.e. modus tollens).8 Additionally, we are flexible with respect to the ordering of conjuncts
and disjuncts. For example, given the previous steps f(a)∧g(a) and ∀x(g(x)∧f(x) →u(x)∨v(x)),
we consider v(a) ∨u(a) to be valid."
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8616600790513834,"A.7
Generating distractors"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8636363636363636,"Implication elimination For any rule ∀x(f(x) →g(x)) in the gold proof, we generate a distractor
rule ∀x(f(x) →h(x)) where the concept h is a distractor and is not helpful in completing the
proof. In addition, for any ground logical form in the gold proof f(c), we generate a distractor
logical form h(c) as well as a rule ∀x(h(x) →h′(x)). Note that the original PRONTOQA only
adds a single distractor, whereas we add multiple, one for each hop in the proof.
Conjunction introduction Similar to those in implication elimination. For any rule ∀x(f1(x)∧. . .∧
fn(x) →g(x)), we generate a rule of the form ∀x(h1(x)∧. . .∧hn−1(x)∧fn(x) →g(x)) where
hi are distractor concepts. Grounded distractor conjuncts are also generated as axioms hi(c), so
that, given fn(c), both the gold rule and distractor rule are valid proof steps.
Conjunction elimination Distractors are generated similarly to the conjunction introduction case.
Disjunction introduction Distractors are generated similarly to the conjunction introduction case.
Disjunction elimination Since this deduction step has many premises, multiple distractors are neces-
sary to ensure the model doesn’t resort to heuristics. For every rule of the form ∀x(fi(x) →g(x)),
two distractor rules are generated: ∀x(fi(x) →h′(x)) and ∀x(hi(x) →g(x)). A distractor
disjunction is also generated: h′′(c) ∨h1(c) ∨. . . ∨hn−1(c).
Proof by contradiction As with disjunction elimination, multiple distractors are necessary here. We
generate two distractor rules ∀x(f1(x)∨. . .∨fn(x) →h(x)) and ∀x(h1(x)∨. . .∨hn(x) →g(x)).
We also generate the distractor axiom ¬h′(c) so that the model is forced to choose between two
axioms for the first step of the proof."
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8656126482213439,"To avoid creating inconsistencies when generating a distractor rule, we avoid using existing predicates
in the consequent of each rule."
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8675889328063241,8Analogous to the broadly-valid steps in PRONTOQA.
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8695652173913043,"Algorithm 2: Pseudocode for evaluating the output chain-of-thought. Here, the comparison operations
between logical forms ignore the order of conjuncts if both operands are conjunctions; and similarly for
disjunctions. In addition, when iterating over previous steps in the proof, we consider them in reverse order,
so that more recent steps are prioritized. The helper function negate is defined, in order of precedence:
negate(¬A) = A, negate(A ∨B) = negate(A) ∧negate(B), negate(A ∧B) = negate(A) ∨
negate(B), or negate(A) = ¬A."
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8715415019762845,"1 function evaluate_cot(context sentences Q1, . . . , Qm,
predicted chain-of-thought sentences C1, . . . , Cn,
goal sentence g)"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8735177865612648,"2
Lg = semantic_parse(g)
/* parse the goal */"
WHILE B HAS AN ATOMIC NON-NEGATED AXIOM THAT IS NOT AN AXIOM OF A,0.8754940711462451,"3
for i ∈1, . . . , m do
/* parse the context */"
LQ,0.8774703557312253,"4
LQ
i = semantic_parse(Qi)"
LQ,0.8794466403162056,"5
for i ∈1, . . . , m do
/* parse the predicted chain-of-thought */"
LC,0.8814229249011858,"6
LC
i = semantic_parse(Ci)"
LC,0.883399209486166,"7
initialize S as an empty set, and H as an empty map"
LC,0.8853754940711462,"8
for i ∈1, . . . , n do
/* reconstruct the proof from the chain-of-thought */"
IF LC,0.8873517786561265,"9
if LC
i indicates ‘this is a contradiction’"
IF LC,0.8893280632411067,"10
if negate(LC
i+1) ∈H(LC
i−1)"
IF LC,0.8913043478260869,"11
(P, D, k) = ({LC
i−1, negate(LC
i+1)}, {negate(LC
i+1)}, 1)"
ELSE CONTINUE,0.8932806324110671,"12
else continue"
ELSE,0.8952569169960475,"13
else"
ELSE,0.8972332015810277,"14
(P, D, k) = is_provable(LC
i , {LQ
1 , . . . , LQ
m}, S, H)"
ELSE,0.8992094861660079,"15
set H(LC
i ) = S"
ELSE,0.9011857707509882,p∈P H(p) \ D
ELSE,0.9031620553359684,"16
if k ≥0"
"ADD LC
I TO S",0.9051383399209486,"17
add LC
i to S"
"ADD LC
I TO S",0.9071146245059288,"18
return Lg ∈S
/* the proof is correct if the final conclusion is provable */"
"ADD LC
I TO S",0.9090909090909091,"19 function is_provable(logical form φ,
set of axioms A,
previous conclusions S,
hypothesis map H)"
"ADD LC
I TO S",0.9110671936758893,"20
if φ ∈A"
"ADD LC
I TO S",0.9130434782608695,"21
return ({φ}, 1)
/* this is an axiom */"
"ADD LC
I TO S",0.9150197628458498,"22
else if φ is a conjunction or disjunction"
"ADD LC
I TO S",0.9169960474308301,"23
initialize P ′ as an empty list, and k′ = 0"
"ADD LC
I TO S",0.9189723320158103,"24
for φi operand in φ do"
"ADD LC
I TO S",0.9209486166007905,"25
(P, k) = is_provable(φi, A, S, H)"
"ADD LC
I TO S",0.9229249011857708,"26
if φ is a conjunction"
"ADD LC
I TO S",0.924901185770751,"27
if k ≥0 and the step immediately preceding φ in the proof is in P"
"ADD LC
I TO S",0.9268774703557312,"28
append P to P ′"
"ADD LC
I TO S",0.9288537549407114,"29
set k′ = k′ + k"
ELSE BREAK,0.9308300395256917,"30
else break"
ELSE BREAK,0.932806324110672,"31
else if k > 0 and φ is a disjunction"
ELSE BREAK,0.9347826086956522,"32
return (P, ∅, k + 1)
/* provable by disjunction introduction */"
ELSE BREAK,0.9367588932806324,"33
if P ′ has the same size as φ has operands"
ELSE BREAK,0.9387351778656127,"34
return (S P ′, ∅, k′)
/* provable by conjunction introduction */"
ELSE BREAK,0.9407114624505929,"35
for a ∈S ∪A do"
ELSE BREAK,0.9426877470355731,"36
if a is a conjunction and φ = ai for some i"
ELSE BREAK,0.9446640316205533,"37
return ({a}, ∅, 1 + 1{a ∈A})
/* provable by conjunction elimination */"
ELSE BREAK,0.9466403162055336,"38
else if a has form ∀x(ψ →γ) where γ[x 7→c] = φ"
ELSE BREAK,0.9486166007905138,"39
(P, k) = is_provable(ψ[x 7→c], A, S, H)"
ELSE BREAK,0.950592885375494,"40
if k ≥0 and the step immediately preceding φ in the proof is in P ∪{a}"
ELSE BREAK,0.9525691699604744,"41
return (P ∪{a}, ∅, k + 1{a ∈A})
/* provable by conjunction elimination */"
ELSE BREAK,0.9545454545454546,"42
for s ∈S where s is a disjunction do"
ELSE BREAK,0.9565217391304348,"43
if for all disjuncts si, there is a sj ∈S such that sj = φ and si ∈H(sj)"
ELSE BREAK,0.958498023715415,"44
return ({sj}, {si}, 1)
/* provable by disjunction elimination */"
ELSE BREAK,0.9604743083003953,Algorithm 2: (continued from previous page)
ELSE BREAK,0.9624505928853755,"45
for a ∈S ∪A do"
ELSE BREAK,0.9644268774703557,"46
if a has form ∀x(ψ →γ) where γ[x 7→c] = φ"
ELSE BREAK,0.9664031620553359,"47
(P, k) = is_provable(ψ[x 7→c], A, S, H)"
ELSE BREAK,0.9683794466403162,"48
if k ≥0 and the step immediately preceding φ in the proof is in P ∪{a}"
ELSE BREAK,0.9703557312252964,"49
return (P ∪{a}, ∅, k + 1{a ∈A})
/* provable by implication elimination */"
ELSE BREAK,0.9723320158102767,"50
else if a has form ∀x(ψ →γ) where negate(ψ[x 7→c]) = φ"
ELSE BREAK,0.974308300395257,"51
(P, k) = is_provable(negate(γ[x 7→c]), A, S, H)"
ELSE BREAK,0.9762845849802372,"52
if k ≥0 and the step immediately preceding φ in the proof is in P ∪{a}"
ELSE BREAK,0.9782608695652174,/* provable with additional deduction rules (modus tollens) */
ELSE BREAK,0.9802371541501976,"53
return (P ∪{a}, ∅, k + 1{a ∈A})"
ELSE BREAK,0.9822134387351779,"54
if φ ∈S"
ELSE BREAK,0.9841897233201581,"55
return ({φ}, ∅, 0)
/* proved by previous step */"
ELSE BREAK,0.9861660079051383,"56
else if φ has form ∀x(ψ →γ)"
ELSE BREAK,0.9881422924901185,"/* note:
we precompute this graph */"
ELSE BREAK,0.9901185770750988,"57
let G be the graph where for any axiom in A with form ∀x(α →β), α and β are vertices and there is a
directed edge from α to β"
ELSE BREAK,0.9920948616600791,"58
if there is a path in G from ψ to γ"
ELSE BREAK,0.9940711462450593,/* provable with additional deduction rules */
ELSE BREAK,0.9960474308300395,"59
return ( axioms corresponding to path edges , ∅, length of path )"
ELSE BREAK,0.9980237154150198,"60
return (∅, ∅, −1)
/* this step is not provable (i.e., invalid) */"
