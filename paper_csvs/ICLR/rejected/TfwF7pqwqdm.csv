Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005681818181818182,"Adversarial attacks have been developed as intentionally designed perturbations
added to the inputs in order to fool deep neural network classiﬁers. Adversarial
training has been shown to be an effective approach to improve the robustness of
image classiﬁers against such attacks especially in the white-box setting. In this
work, we demonstrate that some geometric consequences of adversarial training
on the decision boundary of deep networks give an edge to certain types of black-
box attacks. In particular, we introduce a highly parallelizable black-box attack
against classiﬁers equipped with an ℓ2 norm similarity detector, which exploits
the low mean curvature of the decision boundary. We use this black-box attack to
demonstrate that adversarially-trained networks might be easier to fool in certain
scenarios. Moreover, we deﬁne a metric called robustness gain to show that while
adversarial training is an effective method to improve the robustness in the white-
box attack setting, it may not provide such a good robustness gain against the more
realistic decision-based black-box attacks."
INTRODUCTION,0.011363636363636364,"1
INTRODUCTION"
INTRODUCTION,0.017045454545454544,"It is known in the literature that adversarial training can make deep neural networks more robust
Madry et al. (2018); Shafahi et al. (2019); Wong et al. (2019) against adversarial attacks Goodfellow
et al. (2014); Carlini & Wagner (2017); Moosavi-Dezfooli et al. (2016); Szegedy et al. (2013).
Arguably, adversarial training can be assumed as one of the most effective techniques for robustness
improvement Athalye et al. (2018). Moreover, it is empirically shown in Moosavi-Dezfooli et al.
(2019) that adversarial training causes the boundary of the image classiﬁers to become ﬂatter (less
curved) compared to normally-trained ones."
INTRODUCTION,0.022727272727272728,"Adversarial attacks can be executed in the white-box setting Carlini & Wagner (2017); Goodfellow
et al. (2014); Moosavi-Dezfooli et al. (2016), score-based black-box setting Chen et al. (2017); Ilyas
et al. (2018); Narodytska & Kasiviswanathan (2016) or decision-based black-box setting Brendel
et al. (2018); Chen et al.; Cheng et al. (2019); Liu et al. (2019); Rahmati et al. (2020). The attacker’s
level of information about the classiﬁer plays a key role on the quality of the generated adversarial
examples. In order to craft an adversarial perturbation in a decision-based black-box setting, the
critical information is mostly the normal vector to the decision boundary. In this setting, the esti-
mation of the normal vector is conducted with carefully designed ﬁne-tuned queries at a boundary
point of the image classiﬁer, usually based on the linearization of the boundary. Chen et al.; Cheng
et al. (2019); Liu et al. (2019); Rahmati et al. (2020). The objective of such black-box attacks is
typically to reduce the number of queries as much as possible with an efﬁcient estimate of the nor-
mal vector. However, to make sure that this linearization approximation is valid at a boundary point,
the ℓ2 distance of these queries with the boundary point should be small enough. As a result, such
similar queries can be detected using a k-nearest neighbours (KNN) similarity detector as in Chen
et al. (2020). Moreover, the efﬁcient estimation of the normal to the decision boundary heavily relies
on the assumption that the boundary of the image classiﬁer has a low mean curvature in the vicinity
of input samples Fawzi et al. (2016); Moosavi-Dezfooli et al. (2019). Therefore, such estimators
are expected to work better if the decision boundary is less curved. Interestingly, it is empirically
shown that adversarial training leads to neural networks with ﬂatter decision boundaries, compared
to the boundaries learned through regular training methods Qin et al. (2019); Moosavi-Dezfooli et al.
(2019). We will show that this characteristic of the adversarially-trained networks indeed gives an
edge to black-box attacks."
INTRODUCTION,0.028409090909090908,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03409090909090909,"The goal of this paper is to show some evidence that although the adversarial training improves
the robustness of deep image classiﬁers effectively against the minimal-norm perturbation white-
box attacks, it becomes less effective in more practical attack settings. In particular, we propose
a parallelizable attack against classiﬁers equipped with an ℓ2 norm similarity detector to demon-
strate that adversarially-trained networks might even be fooled with smaller ℓ2 norm for a given
query budget compared to regularly-trained networks due to their excessive linear behavior. That is,
decision-based black-box attacks can exploit the excessive ﬂatness caused by adversarial training.
In addition, we deﬁne a metric called robustness gain as the ratio of ℓ2 norm of adversarial pertur-
bation required to fool the adversarially-trained network to that required for the regular network.
We observe that the level of information available to the attacker about the classiﬁer impacts the
robustness gain; in particular, the robustness gain increases when the information available to the
attacker increases (e.g., from black-box to white-box, or by increasing the number of queries in the
black-box setting). We summarize the contributions of this paper as follows:"
INTRODUCTION,0.03977272727272727,"• We empirically show that there is an interesting trade-off between adversarial training and
the attack’s effectiveness to fool the classiﬁer. Moreover, we demonstrate that this trade-off
is even more critical in certain black-box attacks which rely on the estimation of the normal
vector at the boundary."
INTRODUCTION,0.045454545454545456,"• We introduce a highly parallelizable attack which is effective against a classiﬁer equipped
with a query similarity detector based on ℓ2 norm. Using normal vectors estimated at
multiple points on the boundary, we develop an attack which is, interestingly, more effective
against an adversarially-trained network as compared to a regular network."
INTRODUCTION,0.05113636363636364,"• We deﬁne a metric called robustness gain as the ratio of ℓ2 norm of adversarial perturba-
tions required for the robust network to that for the regular network. We show that while
adversarial training is an effective approach against minimum perturbation white-box at-
tacks, it may not provide a good robustness gain against black-box attacks."
INTRODUCTION,0.056818181818181816,"The rest of the paper is organized as follows. In Section 3, the problem setting, the similarity de-
tector, and the multi-point normal vector estimator are introduced. In Section 4, the performance
of our proposed multi-point attack and the state of the art black-box attacks is analyzed for the
adversarially-trained network compared to the regular one. In Section 5, we evaluate the effective-
ness of adversarial training against a minimum perturbation white-box attack and ﬁnally Section 6
concludes the paper."
RELATED WORK,0.0625,"2
RELATED WORK"
RELATED WORK,0.06818181818181818,"Adversarial Training
The basic idea of adversarial training is to create and then incorporate ad-
versarial examples into the training process Szegedy et al. (2013); Goodfellow et al. (2014). In
Madry et al. (2018), authors show an effective version of an adversarially-trained network to im-
prove robustness against white-box attacks. In Shafahi et al. (2019), the authors proposed a so-called
“free” version of adversarial training with a cost nearly as equal as natural (regular) training. Their
key idea is to update both the model parameters and image perturbations using one simultaneous
backward-pass. Recently, in Wong et al. (2019), the authors discovered that adversarial training can
be conducted in a cheaper manner using the fast gradient sign method (FGSM) Goodfellow et al.
(2014) added with random initialization. This approach can be useful to adversarially train large
datasets such as ImageNet much faster.
Adversarial attacks
Adversarial attacks can be executed in different categories depending on the
attacker’s level of information including white-box setting Carlini & Wagner (2017); Goodfellow
et al. (2014); Moosavi-Dezfooli et al. (2016); Szegedy et al. (2013), score-based black-box set-
ting Chen et al. (2017); Ilyas et al. (2018); Narodytska & Kasiviswanathan (2016) or decision-based
black-box scenarios Brendel et al. (2018); Chen et al.; Cheng et al. (2019); Liu et al. (2019); Rahmati
et al. (2020). In order to craft an adversarial example, the critical information is the normal vector
to the decision boundary of the classiﬁer. The most successful black-box attacks directly estimate
the normal vector with linearization of boundary. For example, the HSJA Chen et al. deploys the
gradient direction estimation information. In Liu et al. (2019); Rahmati et al. (2020), authors lo-
cally approximated the decision boundary with a hyper-plane, and searched the closest point on the
hyper-plane to the benign input as the perturbation."
RELATED WORK,0.07386363636363637,Under review as a conference paper at ICLR 2022
RELATED WORK,0.07954545454545454,"Black-box defenses
The generation of an adversarial example requires the black-box attacker to
submit multiple similar queries to the target model. Thus, the main idea of most of black-box
defences is to detect such a similarity across multiple queries. In Chen et al. (2020), authors propose
to equip an existing classiﬁer with a detection component, which stores the similarity vectors for all
incoming queries, computed by a pre-trained similarity encoder. For each new query, it computes
the k-nearest-neighbor distance between it and all other vectors in the memory. Blacklight Li et al.
(2020) computes a compact set of one-way hash values for each query image that form a probabilistic
ﬁngerprint. The variants of an image make almost identical ﬁngerprints, which makes it robust
against manipulation. In Byun et al. (2021), the authors introduce Small Noise Defense (SND) in
which even a small additive input noise can neutralize most query-based attacks."
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.08522727272727272,"3
PROBLEM STATEMENT AND MULTI-POINT ATTACK"
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.09090909090909091,"Motivation and background
One of the most challenging settings to perform adversarial attack
to image classiﬁers is when the attacker only has access to the top-1 label of the classiﬁer, where
the attacker’s level of information from the image classiﬁer is the least. A query is a request that
results in the top-1 label of an image classiﬁer for a given input. The state-of-the-art attacks try
to obtain the smallest possible ℓp norm of the perturbations with efﬁcient use of queries to the
image classiﬁer Brendel et al. (2018); Chen et al.; Cheng et al. (2019); Liu et al. (2019); Rahmati
et al. (2020). An implicit assumption here is that the image classiﬁer is naive enough to respond
to multiple consecutive similar queries with no complain. This is a strong assumption which is in
contrast with the common sense in terms of security. In practice, the defender can take advantage
of such characteristic of the queries to detect the suspicious set of queries. In addition, all these
attacks perform in an iterative manner which can be time consuming even for a powerful attacker
with lots of processing power. Having a parallelizable attack can expedite the running time of the
attack which can be critical in certain scenarios that attacker should act as quick as possible. Our
goal is to propose a highly parallelizable attack to fool the classiﬁer equipped with an ℓ2 similarity
detector by taking advantage of the low mean curvature of the decision boundary of state-of-the-art
deep classiﬁers."
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.09659090909090909,"Similarity detector, simple yet effective defense
As in Chen et al.; Cheng et al. (2019); Liu et al.
(2019); Rahmati et al. (2020), most of the state-of-the-art black-box attacks generate queries with
additive Gaussian or Uniform noises to a boundary point to estimate the normal to the decision
boundary. Inherently, these types of estimators need similar, i.e. very close queries in terms of ℓ2
distance to make sure the linearization assumption is valid. However, such similar queries can be
simply detected by a k nearest neighbour (KNN) similarity detector with k ≪N, where N is the
query budget for the attacker. Thus, the k nearest neighbours of the given query i are obtained among
the queries stored in a buffer. In particular, for a given query i, the classiﬁer computes the average
ℓ2 distance between the query i and its k nearest neighbors di deﬁned as in Chen et al. (2020):"
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.10227272727272728,"di = 1 k k
X"
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.10795454545454546,"t=1
di,t,
(1)"
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.11363636363636363,"where di,t is the ℓ2 distance of the given query i with its nearest neighbours t where 1 ≤t ≤k. Then,
by introducing the detection threshold δ, if di < δ, an attack is detected and the user is blocked. The
value for δ should not be too small so that none of the queries get captured and also not too large
to detect some clean queries as a false attack. Although, the computational complexity of of the
KNN detector may be high, it is quite effective and simple in detecting the queries generated by
the techniques deploying normal vector estimation as in HopSkipJump Chen et al., GeoDA Rahmati
et al. (2020), Sign-OPT Cheng et al. (2019), and qFool Liu et al. (2019). We stress here that the
goal of the paper is not to outperform the state-of-the-art black-box attacks, but rather to provide
certain practical scenarios in which the ﬂatter boundary of the adversarially-trained networks are
exploitable by the black-box attacker. Therefore, the question is:"
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.11931818181818182,"Is there a way to design a parallelizable attack to generate a query-efﬁcient ℓ2-norm-minimized
perturbation with total N queries against a classiﬁer equipped with a similarity detector of k nearest
neighbours with threshold δ?"
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.125,"We show that the answer to this question is afﬁrmative and introduce a multi-point normal estimator
which evades the similarity detector."
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.13068181818181818,Under review as a conference paper at ICLR 2022
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.13636363636363635,Boundary
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.14204545454545456,Figure 1: Multi-point normal vector estimation.
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.14772727272727273,"Problem statement
We consider a trained L-class classiﬁer with parameters θ represented as f :
Rd →RL, where x ∈Rd is the input image and ˆk(x) = argmaxk fk(x) is the top-1 classiﬁcation
label where fk(x) is the k-th component of f(x) corresponding to the k-th class. The attacker’s
goal is to minimize the ℓ2 norm as much as possible with a limited budget of N queries while it
cannot get detected with the similarity detector. We deﬁne the optimization problem as:"
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.1534090909090909,"min
v
∥v∥2
(2)"
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.1590909090909091,"s.t.
ˆk(x + v) ̸= ˆk(x),
di > δ, ∀i ≤N."
PROBLEM STATEMENT AND MULTI-POINT ATTACK,0.16477272727272727,"The last constraint ensures that the queries are not detected by the ℓ2 similarity detector. Without the
similarity detector constraint, problem equation 2 is already solved in the literature with different
approaches Brendel et al. (2018); Chen et al.; Cheng et al. (2019); Liu et al. (2019); Rahmati et al.
(2020). The main idea is to obtain a point on the boundary of the image classiﬁer and estimate the
normal to the decision boundary at this point. However, with the presence of the similarity detector,
the normal vector estimation becomes more challenging for the attacker as it can not generate large
number of queries with small ℓ2 norms to the neural network."
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.17045454545454544,"3.1
MULTI-POINT NORMAL VECTOR ESTIMATOR"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.17613636363636365,"We employ the fact that the decision boundaries of the state-of-the-art deep networks have a low
mean curvature in the vicinity of inputs Fawzi et al. (2016; 2018). Therefore, to simplify equa-
tion 2, we locally approximate the boundary of image classiﬁer at multiple boundary points with a
hyperplane as:"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.18181818181818182,"min
v
∥v∥2
(3)"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.1875,"s.t.
wT (x + v) −wT xB,j = 0, ∀j ≤M
di > δ, ∀i ≤N,"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.19318181818181818,"where xB,j is the j-th boundary point and M is the number of boundary points obtained using a
binary search. Having a single boundary point M = 1 and spend all N queries at xB,1 to estimate
the normal to the decision boundary can make the attack parallelizable. However, such an attack
can be simply detected by the similarity detector. Also, iterative attack is not desirable since at each
iteration the boundary points distances decreases. Thus, we need to design an estimator in which the
queries cannot be detected by the similarity detector."
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.19886363636363635,"Multi-point normal estimator
To alleviate the aforementioned problem, we propose the multi-
point normal vector estimator in which the queries are generated on multiple boundary points (M ≫
1). The key idea is to distribute N queries to estimate the normal vector to the boundary using
multiple boundary points, rather than spending all N queries on just a single point. Apparently, a
considerable portion of queries are allocated to obtain the boundary points along with binary search.
This is the cost imposed by query detector to the attackers. This estimator is both parallelizable and,
as we will see, successful against a classiﬁer equipped with an ℓ2 similarity detector."
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.20454545454545456,"As seen in Fig. 1, starting from the original image x0, one can ﬁnd M points on the boundary
denoted by xB,1, xB,2, . . . , xB,M. Similar to the method proposed Chen et al.; Liu et al. (2019);
Rahmati et al. (2020), the boundary points can be obtained using binary search along several random"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.21022727272727273,Under review as a conference paper at ICLR 2022
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.2159090909090909,Algorithm 1: Multi-point attack
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.2215909090909091,"1 Inputs: Original image x0, query budget N."
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.22727272727272727,2 Output: Adversarial example xadv.
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.23295454545454544,"3 Obtain the optimal number of boundary points M ∗by M ∗=
N−β
k−b−γ∗."
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.23863636363636365,"4 Obtain M ∗starting point on the boundary xB,1, xB,2, . . . , xB,M ∗."
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.24431818181818182,5 Estimate normal ˆw with equation 5.
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.25,6 Push the original image x0 towards the boundary in the direction of ˆw.
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.2556818181818182,"7 ˆr ←min{r′ > 0 : ˆk(x0 + r′ ˆ
w) ̸= ˆk(x)}"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.26136363636363635,8 xadv ←x0 + ˆr ˆw
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.26704545454545453,"directions, with b queries on average per boundary point Normally, these b queries are quite close
to each other and can be assumed as bad queries which we have to minimize in our design as much
as possible (from the attacker perspective). In general, the close queries are not desirable since they
can reduce the KNN mean di for a given query i and increase the chance of getting detected by the
similarity detector. On the other hand, the most informative queries are the ones with small distances
with one another which creates an interesting trade-off. We assume that the attacker knows that the
query detector deploys k nearest neighbours to compute di for each query i. Here, without loss of
generality, we assume that the closest boundary point to xi is xB,j. The similarity detector observes
three types of queries when computing its k nearest neighbours for query i. The ﬁrst type are the
queries used to obtain the boundary point with added Gaussian noise along with binary search. The
second type are the ones deployed to estimate the normal to the boundary at xB,j. Finally, the type
three distances for query xi are obtained between the xi and its second closest boundary point to xi.
At each boundary point xB,j, 1 ≤j ≤M, the attacker can allocate n = k−b−γ queries to estimate
the normal to the boundary at point xB,j, where b is the average number of type II queries, and γ is
the number of type III queries. Thus, the number of required boundary points can be obtained by:"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.2727272727272727,"M =
N −β
k −b −γ
(4)"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.2784090909090909,"where β is the number of queries required to push the x0 towards the direction of the estimated
normal vector towards the boundary (step 6 of the Algorithm 1). At each boundary point j, the
boundary is locally approximated with a hyperplane wT
j (x −xB,j) = 0. In order to estimate the
normal vector wj, the key idea is to generate n samples ζi, i ∈{1, . . . , n} from a multivariate
normal distribution ζi ∼N(0, Σ) which results in queries with the form of xB,j + ζi, ∀i ∈N
Rahmati et al. (2020). The estimator ˆwj of wj with n queries is ˆwj = 1"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.2840909090909091,"n
Pn
i=1 ziζi, where zi = 1
if ˆk(xB,j + ζi) ̸= ˆk(xB,j) and otherwise zi = −1. Eventually, the average normalized direction of
estimated normal vector over all the boundary points can be given as:"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.2897727272727273,"ˆw =
ΣM
j=1 ˆwj
∥ΣM
j=1 ˆwj∥.
(5)"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.29545454545454547,"After estimating the normal to the boundary ˆw, we push the original image x0 towards the boundary
in the direction of ˆw with amplifying the magnitude of the vector. The ﬁnal multi-point attack is
summarized in Algorithm 1.
3.2
NUMBER OF BOUNDARY POINTS"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.30113636363636365,"In this section, our goal is to have an estimate for the optimal number of boundary points. The
number of boundary points M should not be too small resulting in close queries that will get detected
by the similarity detector, and must not be too large that wastes the number of queries (as obtaining
each boundary point costs b queries on average). We assume that the attacker has the information
about k and δ. Thus, by computing the KNN distance in the design step of the attack, the attacker can
evade the similarity detector. As mentioned previously, for a new query i, the k nearest neighbour
queries and their distances to xi can be categorized into three types. We aim to have an estimate for
the optimal number of boundary points. For more details about mean distance of queries (µi), refer
to Appendix A.1."
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.3068181818181818,"Having the mean distances (µi) of queries in hand, the attacker can have an estimate on the number
of type II queries at each boundary point to get an estimation of the normal vector of the boundary"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.3125,Under review as a conference paper at ICLR 2022
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.3181818181818182,"without getting detected by the similarity detector. The number of queries at each boundary point
should not be too large so that the similarity detector can detect the queries due to the small di.
Moreover, it should not be too small to incur excessive overhead in ﬁnding boundary points. Thus:"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.32386363636363635,"di = 1 k k
X"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.32954545454545453,"t=1
di,t = 1"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.3352272727272727,"k (bµ1 + nµ2 + γµ3) = δ + λ,
(6)"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.3409090909090909,"where λ is a margin to the threshold to make sure that the average distance can not be detected by
the similarity detector. We may ignore the nµ2 term as it is typically small compared to µ1 and µ3.
Having this approximation, the optimal value for γ can be obtained as γ∗= k(δ+λ)−bµ1"
MULTI-POINT NORMAL VECTOR ESTIMATOR,0.3465909090909091,"µ3
: It simply
can be seen that if µ3 is large, then the number of type II queries n per boundary point increases
which results in more efﬁcient deployment of queries. The optimal number of boundary points can
be obtained by plugging γ∗into equation 4 as M ∗=
N−β
k−b−γ∗. Based on this, by increasing the
number of type III distances which results in increasing the KNN mean, one can see that the number
of required boundary points to satisfy di = δ + λ decreases as well."
"EFFECTIVENESS OF ADVERSARIALLY-TRAINED NETWORKS AGAINST
BLACK-BOX ATTACKS",0.3522727272727273,"4
EFFECTIVENESS OF ADVERSARIALLY-TRAINED NETWORKS AGAINST
BLACK-BOX ATTACKS"
"EFFECTIVENESS OF ADVERSARIALLY-TRAINED NETWORKS AGAINST
BLACK-BOX ATTACKS",0.35795454545454547,"In this section, our goal is to evaluate the effectiveness of adversarial training against decision-based
black-box attacks in which the attacker has only access to the output label of the image classiﬁer
for a given input. We evaluate our experiments on a pre-trained ResNet-50 He et al. (2016) called
regular network and the adversarially-trained ResNet-50 Madry et al. (2018) called robust network
throughout this section. We consider 300 correctly classiﬁed images by both networks which are
randomly selected from the ILSVRC2012’s validation set Deng et al. (2009)."
MULTI-POINT ATTACK AGAINST SIMILARITY DETECTOR,0.36363636363636365,"4.1
MULTI-POINT ATTACK AGAINST SIMILARITY DETECTOR"
MULTI-POINT ATTACK AGAINST SIMILARITY DETECTOR,0.3693181818181818,"Here, we evaluate the performance of our novel multi-point attack and an iterative attack1 (GeoDA
Rahmati et al. (2020)) on an adversarially-trained network equipped with an ℓ2 similarity detector
in Fig 2a. For the similarity detector, we assume k = 100, δ = 5, ζ = 5. We also set nrbst = 45
and nreg = 30. We deploy uniform GeoDA with 400 queries per iteration. It can be observed
that the iterative attack can be detected after a few number of queries as similarly detector’s KNN
mean drops quickly below δ + ζ. However, the multi-point attack by-pass the similarity detector by
keeping the KNN mean di, ∀i greater than the detection threshold by distributing the queries over
the distant boundary points."
MULTI-POINT ATTACK AGAINST SIMILARITY DETECTOR,0.375,"In Fig. 2b, we compare the ℓ2 norm of single-point attack (M = 1, which is highly paralleliz-
able, but not successful against the similarity detector) and our proposed multi-point attack on both
regular and adversarially-trained networks with respect to number queries. For the single iteration
attack, the performance of attack on regular network is better compared to the robust network as ex-
pected. However, for the extension to multi-point attack which can successfully evade the similarity
detector, we interestingly see that the performance of the multi-point attack on the robust network is
almost the same as the performance of the single point attack. However, for the regular network, the
performance of the multi-point attack is much worse compared to the one for the single point attack.
Thus, interestingly, adversarially-trained network can be fooled with smaller ℓ2 norm with the same
amount of queries. Also, we observe that the trend of the convergence for the regular network is
more noisy which is due to non-smoothness of the boundary."
MULTI-POINT ATTACK AGAINST SIMILARITY DETECTOR,0.3806818181818182,"The reasons behind the above observations are twofold. First, it is quite intuitive that the multi-point
normal estimator can preform better on smoother boundaries. In particular, the ﬂatter the boundary
the more aligned the directions of the normal vectors at different boundary points are. In the extreme
case that the boundary is a hyper-plane, the normal vectors to the boundary on all over the hyper-
plane are in the same direction. Thus, this results in a better estimation of the normal vector to the
boundary for the classiﬁer in black-box setting when the network is more is adversarially-trained
(boundary is ﬂatter)."
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.38636363636363635,"1Note that most of the state of the art attacks in which they estimate the normal to the decision boundary are
iterative and follow the similar procedure. Thus, comparing with one of them is sufﬁcient for this experiment."
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.39204545454545453,Under review as a conference paper at ICLR 2022
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.3977272727272727,"0
200
400
600
800
1000
1200
1400
1600
Number of queries 0 50 100 150 200 250 300"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4034090909090909,Detector ℓ2 distance for k = 100
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4090909090909091,"Multi-point attack
Iterative attack
Detector threshold (a)"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4147727272727273,"0
2000
4000
6000
8000
10000
12000
Number of queries 20 25 30 35 40 45 50 55"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.42045454545454547,Median of ℓ2 norm
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.42613636363636365,"Multi-point - Regular
Multi-point - Robust
Single-point - Regular
Single-point - Robust"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4318181818181818,"(b)
Figure 2: (a) KNN query similarity detector ℓ2 distance for iterative and multi-point estimator. (b)
Performance of single-point and multi-point attacks on both regular and robust ResNet50."
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4375,"Second, as shown in equation 9, the larger the type III mean distance µ3 is, the less number of
type III queries γ∗is needed to satisfy equation 8. In equation 4, the larger γ∗follows with fewer
number of boundary points M ∗. This leads to a more efﬁcient deployment of the queries in the
robust network as obtaining each boundary point impose the cost of approximately b queries per
boundary point to the attacker. We empirically show in the Table 1 that the mean distance of the
points on the boundary µ3 is larger for the adversarially-trained network compared to the one for
regular network. As discussed above, this can reduce the number of required boundary points to
satisfy di = δ + λ in equation 8 which beneﬁts the attacker. Thus, we have µrbst
3
> µreg
3 which leads
to γrbst < γreg in equation 9 and also M rbst < M reg. As a result in equation 4, we have nrbst > nreg
which results in larger number of queries at each boundary point. The smaller number of boundary
points is beneﬁcial for the attacker as it can save on average b queries per boundary point. Moreover,
larger number of queries at each boundary point nrbst > nreg can increase the accuracy of the normal
estimation at each boundary point."
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4431818181818182,"Black-box attacks performance evaluation
We compare the performance of black-box attacks
HSJA Chen et al., GeoDA Rahmati et al. (2020), boundary attack (BA) Brendel et al. (2018) on both
regular and robust ResNet-50 networks in Fig. 3a. For a given query budget, the ℓ2 norm of pertur-
bations for the attacks against the robust network is larger compared to that of the regular network
as expected. However, an interesting observation is that while GeoDA has almost the same ℓ2 norm
as HSJA for the regular network, it provides smaller ℓ2 norm for perturbations against the robust
network compared to HSJA for a ﬁxed amount of queries. The reason for this phenomenon is that
GeoDA is explicitly built based on the assumption that the boundary of the classiﬁer has a low mean
curvature. On the other hand, adversarially trained-networks has ﬂatter decision boundaries which
actually gives an edge to GeoDA. Thus, to attack robust networks more efﬁciently, it is beneﬁcial
for the attackers to deploy attacks exploiting the ﬂatness of the decision boundary.
Robustness gain
Here, our goal is to evaluate how much adversarially-trained networks can im-
prove the robustness under various kind of attacks. We plot the robustness gain for different attacks
in Fig. 3b. The larger the η for a given attack is, the better the adversarial training can improve the
robustness compared to the case of the regular network. In Fig. 3b, it is observed that η is equal
to around 17 (see Table 2) for the white-box attack DeepFool (DF) Moosavi-Dezfooli et al. (2016)
which is a quite good improvement. Having a black-box setting, we evaluate the η for HSJA Chen
et al., GeoDA Rahmati et al. (2020), boundary attack Brendel et al. (2018), single iteration and multi-
point attacks as well. First, it can be seen that, in general, the robustness gain is lower than that of
the DeepFool. Second, by extracting more information from the image classiﬁer (more queries), the
robustness gain increases. For the single iteration and multi-point attack, η is much lower compared
to the that of other black box attacks with a small increasing slope along with increasing the number
of queries. It may imply that the less information you know about the image classiﬁer, i.e., the more
practical the attack scenario is, the less the adversarially-trained network can improve the robust-
ness. That being said, adversarial training for the deep image classiﬁers is much more effective for
white-box scenarios."
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.44886363636363635,"Mean distance of boundary points
The mean distance of boundary points µ3 is a critical design
parameter for the multi-point attack as it determines the number for boundary points. Here, we have
done experiments to measure the µ3 for both networks with different number of boundary points M.
We start with a boundary point xB,1 and obtain the ℓ2 norm to this point from all other boundary"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.45454545454545453,Under review as a conference paper at ICLR 2022
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4602272727272727,"0
2000
4000
6000
8000
10000
12000
Number of queries 0 10 20 30 40 50 60 70 80"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4659090909090909,Median of ℓ2 norm
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4715909090909091,"GeoDA - Robust
GeoDA - Regular
HSJA - Robust
HSJA - Regular
BA - Robust
BA - Regular (a)"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4772727272727273,"0
2000
4000
6000
8000
10000
12000
Number of queries 100 101"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.48295454545454547,Robustness gain
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.48863636363636365,"DeepFool
GeoDA
HSJA
BA
Single-point
Multi-point"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.4943181818181818,"(b)
Figure 3: (a) Performance comparison of different black-box attacks for both regular and robust
ResNet50. (b) The robustness gain for ℓ2 norm under different attack scenarios."
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.5,"points. The average of such a distance over 300 correctly classiﬁer images by both networks is
reported in Table 1. It is observed that the µ3 is large for the robust network compared to that of
regular network. Moreover, we can see that the value of µ3 even for M = 2 is very close to the
true mean. Thus, the estimate of the µ3 can be cheaply obtained with only M = 2 since in the
high-dimensional regime, the ℓ2 distances are close enough."
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.5056818181818182,"M = 2
M = 10
M = 100"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.5113636363636364,"Regular He et al. (2016)
67.01
67.38
69.34"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.5170454545454546,"Adv. trained Madry et al. (2018)
90.11
88.47
89.71"
NOTE THAT MOST OF THE STATE OF THE ART ATTACKS IN WHICH THEY ESTIMATE THE NORMAL TO THE DECISION BOUNDARY ARE,0.5227272727272727,"Table 1: Mean distance of boundary points µ3 on both robust and adversarially-trained networks for
different number of boundary points averaged over 200 samples."
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5284090909090909,"5
ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5340909090909091,"We already discussed the effectiveness of the adversarially-trained network with respect to num-
ber of required queries against decision-based black-box attacks in query-limited regime. In the
white-box scenario, we evaluate the effectiveness through the number of required iterations for the
convergence of a minimal ℓ2 norm perturbation white-box attack. To this end, we choose a minimal
ℓ2 norm white-box attack DeepFool Moosavi-Dezfooli et al. (2016) and compare its performance
on an adversarially-trained Madry et al. (2018) and a regular ResNet-50 in Table 2."
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5397727272727273,"DeepFool performance
To this end, we choose a minimal ℓ2 norm white-box attack Deep-
Fool Moosavi-Dezfooli et al. (2016) and compare its performance on an adversarially-trained Madry
et al. (2018) and a regular ResNet-50 in Table 2. The main reason we choose DeepFool is its depen-
dence on linearizing the output function of the classiﬁer. The algorithm starts with locally linearizing
the output function of the classiﬁer and repeats such an approximation iteratively to compensate for
the effect of the non-linearity of the output function. The more linear the output function of the
image classiﬁer is, the fewer iterations required for DeepFool to converge. Interestingly, despite
that the adversarially-trained network has perturbations with larger ℓ2 norm, due to the more linear
behavior of its output function, DeepFool converges faster on this network. In this sense, one can
conclude that it is easier to attack adversarially-trained networks even in the white-box setting."
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5454545454545454,"Med Iters
Max Iter
ℓ2 norm"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5511363636363636,"Regular He et al. (2016)
4
15
0.209"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5568181818181818,"Adv. trained Madry et al. (2018)
2
4
3.618"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5625,"Table 2: DeepFool Moosavi-Dezfooli et al. (2016) performance on robust and regular ResNet-50
networks.
DeepFool iterations behaviour
In this experiment, the goal is to qualitatively study the behaviour
of output function along the trajectory of the iterations of DF for a single data point. In this case,"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5681818181818182,Under review as a conference paper at ICLR 2022
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5738636363636364,"0.0
0.2
0.4
0.6
0.8
1.0
Line parameter 0 1 2 3 4 5 6 7"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5795454545454546,Difference of the output
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5852272727272727,DeepFool performance on ResNet-50
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5909090909090909,Straight line
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.5965909090909091,DF segment 1
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6022727272727273,DF segment 2
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6079545454545454,DF segment 3
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6136363636363636,DF segment 4
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6193181818181818,DF segment 5 (a)
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.625,"0.0
0.2
0.4
0.6
0.8
1.0
Line parameter 0.0 0.5 1.0 1.5 2.0"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6306818181818182,Difference of the output
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6363636363636364,DeepFool performance on robust ResNet-50
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6420454545454546,Straight line
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6477272727272727,DF segment 1
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6534090909090909,DF segment 2
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6590909090909091,DF segment 3 (b)
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6647727272727273,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Line parameter −2 0 2 4 6"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6704545454545454,Difference of the output
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6761363636363636,Regular
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6818181818181818,Adv. Trained (c)
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6875,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Line parameter 2 0 2 4 6 8 10 12"
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6931818181818182,Difference of the output
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.6988636363636364,Regular
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.7045454545454546,Adv. Trained (d)
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.7102272727272727,"Figure 4: Performance evaluation of DeepFool over different iterations on (a) Regular ResNet 50
network, (b) Adversarially-trained ResNet 50 network. Differences of classiﬁer’s output for clean
and adversarial labels when adversarial sample obtained by (c) DeepFool (close to the boundary),
(d) Randomly using Gaussian perturbation (far from the boundary)."
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.7159090909090909,"DeepFool requires 3 and 5 iterations to converge for robust and regular ResNet-50, respectively. We
consider the difference of the logits corresponding to the original and the adversarial labels for our
evaluation. We track this difference along two paths: 1) the straight path between the original image
and the DF adversarial example (i.e., green line in Figs. 4a and 4b), and 2) the path taken by DF in
each iteration (i.e. black and red line segments). We generate images on the line from the original
image to the minimal perturbation adversarial example obtained by DeepFool. By varying the line
parameter t, we consider the images along the line x = x0+t(xadv −x0), where t = 0 corresponds
to the original image and t = 1 gives the adversarial image which falls on the boundary. When the
image is on the clean label side, the output value of the clean label is larger than the adversarial
label. Approaching the boundary, this difference decreases where on the boundary the difference
is equal to zero and the transition occurs. Assuming xi as the output of DF in iteration i, each
line segment i (i.e. black and red segments in Figs. 4a and 4b) is corresponding to the images on
the line x = x(i−1) + t(xi −x(i−1)) for t ∈[0, 1], where xi = xadv if i is the last iteration.
First, it can be seen that the straight path (green line) is much closer to the path constructed with
DF iterations’ segments for the adversarially-trained network compared to that of regular network.
Second, it is shown that even in each line segment corresponding to each iteration traversed by DF
algorithm, there is more non-linearity in regular networks as they are curved. As a result, although
the adversarial training improved the robustness (increases the minimal ℓ2 norm), it provide an
opportunity for the attacker to attack easier (with less number of iterations to converge) due to more
linear behaviour of adversarially-trained networks."
"ADVERSARIAL TRAINING AGAINST MINIMAL-NORM PERTURBATION
WHITE-BOX ATTACK",0.7215909090909091,"Non-linearity of the output
The goal is to see how the output of the classiﬁer behaves when
we push the image towards the boundary. In Fig. 4c the adversarial example is obtained using
DeepFool, while in Fig. 4d the adversarial point is obtained with adding Gaussian noise along with
binary search. In general, the difference of output in the adversarially trained network is more
smooth and linear. This inherently shows that the no-linearity of the regular network is much higher
than robust networks. Moreover, it can be seen that if the adversarial image is chosen randomly
which is far from the original image (e.g. in Fig. 4d), this non-linearity is more sever. Thus, we
can see that in the case of black-box attack, the attacker faces more non-linearity compared to the
case of white-box setting. Since most of the black-box attacks try to obtain the normal vector to
the boundary or estimate the gradient around a random boundary point, the more linear behaviour
of output function and lower curvature of the boundary can help the adversary to better estimate the
normal vector."
CONCLUSION,0.7272727272727273,"6
CONCLUSION"
CONCLUSION,0.7329545454545454,"We showed that although the adversarial training is quiet effective against white-box attacks, in
query-limited decision-based black-box attacks, it may not perform as efﬁciently as in the case for
the white-box attacks. We demonstrated that since the adversarial training leads to a signiﬁcantly
ﬂatter boundary and a more linear behavior of the image classiﬁer, it can give an edge to certain types
of black-box attackers whose goal is to estimate the normal vector to the boundary. This feature of
the adversarially-trained networks can also provide a chance for minimal norm perturbation whit-
box attacks to produce adversarial examples with less number of iterations. We introduced a highly
parallelizable attack which can be successful against a similarity detector that can fool the robust
network even easier compared to the regular classiﬁer."
CONCLUSION,0.7386363636363636,Under review as a conference paper at ICLR 2022
REFERENCES,0.7443181818181818,REFERENCES
REFERENCES,0.75,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, pp. 274–283. PMLR, 2018."
REFERENCES,0.7556818181818182,"Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. In International Conference on Learning
Representations, 2018."
REFERENCES,0.7613636363636364,"Junyoung Byun, Hyojun Go, and Changick Kim. Small input noise is enough to defend against
query-based black-box attacks. arXiv preprint arXiv:2101.04829, 2021."
REFERENCES,0.7670454545454546,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39–57, 2017."
REFERENCES,0.7727272727272727,"J Chen, MI Jordan, and MJ Wainwright.
Hopskipjumpattack: A query-efﬁcient decision-based
attack. In 2020 IEEE Symposium on Security and Privacy (SP), pp. 668–685."
REFERENCES,0.7784090909090909,"Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security, pp. 15–26. ACM,
2017."
REFERENCES,0.7840909090909091,"Steven Chen, Nicholas Carlini, and David Wagner. Stateful detection of black-box adversarial at-
tacks. In Proceedings of the 1st ACM Workshop on Security and Privacy on Artiﬁcial Intelligence,
pp. 30–39, 2020."
REFERENCES,0.7897727272727273,"Minhao Cheng, Simranjit Singh, Patrick H Chen, Pin-Yu Chen, Sijia Liu, and Cho-Jui Hsieh. Sign-
opt: A query-efﬁcient hard-label adversarial attack. In International Conference on Learning
Representations, 2019."
REFERENCES,0.7954545454545454,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255, 2009."
REFERENCES,0.8011363636363636,"Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classiﬁers:
from adversarial to random noise. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, pp. 1632–1640, 2016."
REFERENCES,0.8068181818181818,"Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano Soatto. Empirical
study of the topology and geometry of deep networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3762–3770, 2018."
REFERENCES,0.8125,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.8181818181818182,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.8238636363636364,"Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. arXiv preprint arXiv:1804.08598, 2018."
REFERENCES,0.8295454545454546,"Huiying Li, Shawn Shan, Emily Wenger, Jiayun Zhang, Haitao Zheng, and Ben Y Zhao. Black-
light:
Defending black-box adversarial attacks on deep neural networks.
arXiv preprint
arXiv:2006.14042, 2020."
REFERENCES,0.8352272727272727,"Yujia Liu, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. A geometry-inspired decision-
based attack. In Proceedings of the IEEE International Conference on Computer Vision, pp.
4890–4898, 2019."
REFERENCES,0.8409090909090909,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.8465909090909091,Under review as a conference paper at ICLR 2022
REFERENCES,0.8522727272727273,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pp. 2574–2582, 2016."
REFERENCES,0.8579545454545454,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Ro-
bustness via curvature regularization, and vice versa. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 9078–9086, 2019."
REFERENCES,0.8636363636363636,"Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations
for deep networks. arXiv preprint arXiv:1612.06299, 2016."
REFERENCES,0.8693181818181818,"Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein
Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. Adversarial robustness through local
linearization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates,
Inc., 2019."
REFERENCES,0.875,"Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Huaiyu Dai. GeoDA: a geo-
metric framework for black-box adversarial attacks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 8446–8455, 2020."
REFERENCES,0.8806818181818182,"Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein.
Adversarial training for free!
In
Advances in Neural Information Processing Systems, pp. 3358–3369, 2019."
REFERENCES,0.8863636363636364,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013."
REFERENCES,0.8920454545454546,"Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training.
In International Conference on Learning Representations, 2019."
REFERENCES,0.8977272727272727,"A
APPENDIX"
REFERENCES,0.9034090909090909,"A.1
MEAN DISTANCE OF THE QUERIES"
REFERENCES,0.9090909090909091,"In this section, our goal is to have an estimate for the optimal number of boundary points. The
number of boundary points M should not be too small resulting in close queries that will get detected
by the similarity detector, and must not be too large that wastes the number of queries (as obtaining
each boundary point costs b queries on average)."
REFERENCES,0.9147727272727273,"We assume that the attacker has the information about k and δ. Thus, by computing the KNN
distance in the design step of the attack, the attacker can evade the similarity detector. As mentioned
previously, for a new query i, the k nearest neighbour queries and their distances to xi can be
categorized into three types. In this section, we compute the average distance of each type of queries
to xi."
REFERENCES,0.9204545454545454,"• Type I: These queries are the ones required to obtain the boundary points. The boundary
points can be obtained by starting from an adversarial perturbation with large ℓ2 norm
and pushing it towards the boundary with binary search. The average distance of such
queries with given query xi can be computed as µ1 =
1
b
Pb
t=1 ∥xi −xt∥. Obtaining
the exact µ1 can be complicated and not even necessary. Instead, we approximate it with
µ1 = ∥xi −xB,j∥where xB,j is the nearest boundary point to xi. Please note that b is
number of queries required for binary search. It could be determined after obtaining the
ﬁrst boundary point.
• Type II: The type II queries are the ones generated on the obtained boundary points to
estimate the normal vector to the boundary. These queries are the most valuable queries in
terms of information one can get from the image classiﬁer. However, since their pairwise
distances are small, they are the ones easily detected if not employed carefully. The means
that we provide here are for a given type II query xi ∼N(xB,j, σ2I). The nearest queries
of the type II xt has the same multivariate Gaussian distribution y = zT z in which z ="
REFERENCES,0.9261363636363636,Under review as a conference paper at ICLR 2022
REFERENCES,0.9318181818181818,"xi −xt follows a Gamma distribution Y ∼Γ(α, β) with α = N/2, β = 4σ2. Thus,
di,t = √y has a G ∼Nakagami(m, Ω) distribution where m = N/2 and Ω= 2Nσ2.
Therefor the mean distance of the Type II queries is given by:"
REFERENCES,0.9375,"µ2 = Γ
 
m + 1 2
 Γ(m)  Ω m"
REFERENCES,0.9431818181818182,"1/2
.
(7)"
REFERENCES,0.9488636363636364,"• Type III: Finally, type III includes the distance of queries between each group of queries
on boundary points. These queries have large distance with each other which can increase
the mean of the KNN similarity detector. We approximate it with the mean distance of the
boundary points from one another. Thus, µ3 =
1
M
PM
t=1 ∥xB,j −xB,t∥, 1 ≤t ≤M."
REFERENCES,0.9545454545454546,"Having the above information in hand, the attacker can have an estimate on the number of type II
queries at each boundary point to get an estimation of the normal vector of the boundary without
getting detected by the similarity detector. The number of queries at each boundary point should
not be too large so that the similarity detector can simply detect the queries due to the small di.
Moreover, it should not be too small to incur excessive overhead in ﬁnding boundary points. Thus,
we have:"
REFERENCES,0.9602272727272727,"di = 1 k k
X"
REFERENCES,0.9659090909090909,"t=1
di,t = 1"
REFERENCES,0.9715909090909091,"k (bµ1 + nµ2 + γµ3) = δ + λ,
(8)"
REFERENCES,0.9772727272727273,"where λ is a margin to the threshold to make sure that the average distance can not be detected by
the similarity detector. We may ignore the nµ2 term as it is typically small compared to µ1 and µ3.
Having this approximation, the optimal value for γ can be obtained as:"
REFERENCES,0.9829545454545454,γ∗= k(δ + λ) −bµ1
REFERENCES,0.9886363636363636,"µ3
.
(9)"
REFERENCES,0.9943181818181818,"It simply can be seen that if µ3 is large, then the number of type II queries n per boundary point
increases which results in more efﬁcient deployment of queries. The optimal number of boundary
points can be obtained by plugging γ∗into equation 4 as M ∗=
N−β
k−b−γ∗. Based on this, by increas-
ing the number of type III distances which results in increasing the KNN mean, one can see that the
number of required boundary points to satisfy di = δ + λ decreases as well."
