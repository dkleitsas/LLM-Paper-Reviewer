Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002028397565922921,"We study a generalization of boosting to the multiclass setting. We introduce
a weak learning condition for multiclass classification that captures the original
notion of weak learnability as being “slightly better than random guessing”. We
give a simple and efficient boosting algorithm, that does not require realizability
assumptions and its sample and oracle complexity bounds are independent of the
number of classes.
In addition, we utilize our new boosting technique in several theoretical applications
within the context of List PAC Learning. First, we establish an equivalence to weak
PAC learning. Furthermore, we present a new result on boosting for list learners,
as well as provide a novel proof for the characterization of multiclass PAC learning
and List PAC learning. Notably, our technique gives rise to a simplified analysis,
and also implies an improved error bound for large list sizes, compared to previous
results."
INTRODUCTION,0.004056795131845842,"1
Introduction"
INTRODUCTION,0.006085192697768763,"Boosting is a powerful algorithmic approach used to boost the accuracy of weak learning models,
transforming them into strong learners. Boosting was first studied in the context of binary classifica-
tion in a line of seminal works which include the celebrated Adaboost algorithm, as well an many
other algorithms with various applications (see e.g. [14, 22, 11, 12])."
INTRODUCTION,0.008113590263691683,"The fundamental assumption underlying boosting is that a method already exists for finding poor,
yet not entirely trivial classifiers. Concretely, binary boosting assumes there exists a learning
algorithm that, when presented with training examples, can find a classifier h : X 7→{0, 1} that
has classification error less than 1/2. That is, it performs slightly better than random guessing. The
intuition is that this is the most minimal assumption one can make about a learning algorithm, without
it being impractical. This assumption is called the weak learning assumption, and it is central to the
study of boosting."
INTRODUCTION,0.010141987829614604,"While binary boosting theory has been extensively studied, extending it to the multiclass setting
has proven to be challenging. In particular, it turns out that the original notion of weak learnability
as being ""slightly better than a random guess"", does not easily extend to the multiclass case. For"
INTRODUCTION,0.012170385395537525,"example, perhaps the most natural extension is to assume that the learner has accuracy that is slightly
better than 1/k, where Y = {1, ..., k}. However, this naive extension is in fact known to be too weak
for boosting (see Section 2 below for a detailed discussion). Instead, previous works [19, 7, 23]
have formulated various complex weak learning assumptions with respect to carefully tailored loss
functions, and rely on restrictive realizability assumptions, making them less useful in practice."
INTRODUCTION,0.014198782961460446,"A weak learning assumption.
In this work, we generalize the classic formulation of boosting to
the multiclass setting. We introduce a weak learning condition that captures the original intuition of
weak learnability as “slightly better-than-random guessing”. The key idea that renders this condition
useful compared to previous attempts, is based on a ""hint"" given to the weak learner. The hint takes
the form of a list of k labels per example, where k is possibly smaller than |Y|. Then, the assumption
is that there exists a learner capable of producing not entirely trivial classifiers, if it was provided
with a ""good hint"". In other words, if the list provided to the learner happens to contain the correct
label, we expect the learner to perform slightly better than randomly guessing a label from the list.
Specifically, the assumption is that for any k ≥2, if the given lists of size k contain the true labels,
the learner will output a classifier h : X 7→Y with error slightly better than random guessing among
the k labels. Notice that this encompasses both the binary case when k = 2, as well as the naive
extension mentioned above, when k = |Y|. We call this new condition the ""better-than-random
guess"", or BRG condition."
INTRODUCTION,0.016227180527383367,"The BRG condition also generalizes the classic binary case condition in a practical sense. Previous
methods on multiclass boosting are framed within the PAC (Probably Approximately Correct) setting,
correspond to a known hypothesis class H ⊆YX , and assume that weak learning hold for every
distribution D over the entire domain X. Practically, these requirements can be very difficult to check
or guarantee. In contrast, as in binary boosting, the BRG condition can be relaxed to a more benign
empirical weak learning assumption, that can be verified immediately in an actual learning setting."
INTRODUCTION,0.018255578093306288,"Recursive boosting.
Our main contribution is a new boosting algorithm that is founded on the
BRG condition. Our boosting methodology yields a simple and efficient algorithm. It is based on the
key observation that even a naive weak learner can produce a useful hint. Recall that when given no
hint at all, the naive weak learner can still find a hypothesis with a slight edge (γ > 0), over random
guessing among |Y| labels. Although this may result in a poor predictor, we prove that it effectively
reduces the label space per example to approximately 1/γ labels. This initial hint serves as the
starting point for subsequent iterations of the boosting algorithm. The process continues recursively
until the label list per example is reduced to size 2, at which point any classic binary boosting can
yield a strong classifier. Unlike previous methods, our boosting algorithm and guarantees do not
rely on realizability assumptions nor do they scale with |Y|. In fact, we show that the sample and
oracle-call complexity of our algorithm are entirely independent of |Y|, which implies our approach
is effective even in cases where the label space Y is possibly infinite. Moreover, the overall running
time of our algorithm is polynomial in the size of its input."
INTRODUCTION,0.02028397565922921,"An important insight that underlies our approach is the link between the naive weak learning condition,
which we term weak-BRG learning, to that of list learning. In list learning [5, 8, 18], rather than
predicting a single outcome for a given unseen input, the goal is to provide a short list of predictions.
Here we use this technique as an intermediate goal of the algorithm, by which effectively reducing
the size of the label space in each round. The generalization analysis relies on sample compression
arguments which result in efficient bounds on the sample and oracle complexities."
INTRODUCTION,0.02231237322515213,"Perhaps surprisingly, the connection between weak learnability and list learnability is even more
fundamental. We prove that there is an equivalence between these two notions. Specifically, we
establish that a γ-weak learner is equivalent to an (1/γ)-list learner."
INTRODUCTION,0.02434077079107505,"Lastly, we demonstrate the strength of our boosting framework. First, we give a generalization of our
boosting technique to hold for list PAC learning algorithms. Then, we showcase the effectiveness
of the weak learning criteria in capturing learnability in two fundamental learning settings: PAC
learning, and List PAC learning. Recently, [5] proved a characterization of multiclass PAC learning
using the Daniely-Shwartz (DS) dimension. In a subsequent study, [8] gave a characterization of
list learnability using a natural extension of the DS dimension. Here we show that in both cases,
assuming the appropriate dimension is bounded, one can devise a simple weak learning algorithm.
Thus, it is also amenable to a boosting method similarly to our approach, leading to a novel and
alternative proof of the characterization of learnability. We note that for cases where the dimension"
INTRODUCTION,0.02636916835699797,"is much smaller than the list size, we have an improved result over previous bound. Moreover, our
approach offers a simpler algorithm and analysis technique, potentially benefiting future applications
as well."
MAIN RESULT,0.028397565922920892,"1.1
Main result"
MAIN RESULT,0.030425963488843813,The main contributions in this work are as follows.
MAIN RESULT,0.032454361054766734,"1. Multiclass boosting framework. Our main result is a boosting framework for the multiclass
setting, which is a natural generalization of binary boosting theory. We give a simple weak
learning assumption that retains the notion of weak learnability as ""slightly-better-than-
random-guess"" from the binary case. Furthermore, we give an efficient multiclass boosting
algorithm, as formally stated in Theorem 1 below. Our boosting algorithm is given in Section
3 (Algorithm 3)."
MAIN RESULT,0.034482758620689655,"2. Applications: List PAC learning. First, we establish an equivalence between List PAC
learning and Weak PAC learning, demonstrating the strong ties between List PAC learning
and multiclass boosting theory. Furthermore, we present a new result on boosting for list
learners. Lastly, we give a novel and alternative proof for characterization of PAC learning
and List PAC learning. In particular, the results imply a simplified algorithmic approach
compared to previous works, and improved error bound for cases where the list size is larger
than the appropriate dimension [5, 8]."
MAIN RESULT,0.036511156186612576,"We will now introduce the main weak learning assumption, which we call the ""better-than-random
guess"", or BRG condition, and state our main result in Theorem 1 below."
MAIN RESULT,0.038539553752535496,"In its original form, the boosting question begins by assuming that a given hypothesis class H ⊆
{0, 1}X is weakly-PAC learnable. Similarly, here we present the BRG condition framed as weak
(multiclass) PAC setting, followed by a relaxation to an empirical variant of the assumption."
MAIN RESULT,0.04056795131845842,"Definition 1 (BRG condition). We say that an hypothesis h : X →Y satisfies the γ-BRG condition
with respect a list function µ : X →Yk on a distribution D over examples if"
MAIN RESULT,0.04259634888438134,"Pr
(x,y)∼D[h(x) = y] ≥
1"
MAIN RESULT,0.04462474645030426,"k + γ

Pr
(x,y)∼D[y ∈µ(x)].
(1)"
MAIN RESULT,0.04665314401622718,"We say that a learning rule W satisfies the γ-BRG condition for a hypothesis class H if for every
H-realizable distribution D, for every k ≥2, for every list function µ : X →Yk, the output
hypothesis h outputted by W satisfies Equation (1) with probability 1 −δ, when given m0(δ) i.i.d.
examples from D, and given µ."
MAIN RESULT,0.0486815415821501,"In words, the condition determines that if y belongs to the set µ(x), then h has a higher probability of
correctly classifying x by an additional factor of γ, compared to a random guess from the list µ(x)."
MAIN RESULT,0.05070993914807302,"However, requiring that the labels be deterministic according to a target function from a known class
H, and that weak learning hold for every distribution D over the entire domain X are impractical, as
they can be very difficult to check or guarantee."
MAIN RESULT,0.05273833671399594,"Instead, as in the binary boosting setting, our condition can be relaxed to a more benign empirical
weak learning assumption, as given next."
MAIN RESULT,0.05476673427991886,"Definition 2 (Empirical BRG condition). Let S ∈(X ×Y)m. We say that a learning rule W satisfies
the empirical γ-BRG condition for S if there is an integer m0 such that for every distribution p over
[m], for every k ≥2, for every list function1 µ : X|S →Yk, when given m0 examples from S drawn
i.i.d. according to p, and given µ, it outputs a hypothesis h such that, m
X"
MAIN RESULT,0.056795131845841784,"i=1
pi · 1[h(xi) = yi] ≥
1"
MAIN RESULT,0.058823529411764705,"k + γ
 m
X"
MAIN RESULT,0.060851926977687626,"i=1
pi · 1[yi ∈µ(xi)].
(2)"
MAIN RESULT,0.06288032454361055,"Next, we give our main result of an efficient boosting algorithm, as stated in Theorem 1 below."
MAIN RESULT,0.06490872210953347,"1We denote X|S = {x ∈X : ∃y ∈Y s.t. (x, y) ∈S}."
MAIN RESULT,0.06693711967545639,"Theorem 1 (Boosting (Informal)). There exists a multiclass boosting algorithm B such that for any
ϵ, δ > 0, and any distribution D over X × Y, when given a training set S ∼Dm and oracle access
to a learning rule W where2 m = ˜O

m0
γ3ϵ

and applying B with a total of ˜O
 
1/γ3
oracle calls"
MAIN RESULT,0.06896551724137931,"to W, it outputs a predictor ¯H : X 7→Y such that with probability at least 1 −δ, we get that if W
satisfies the empirical γ-BRG condition for S then,"
MAIN RESULT,0.07099391480730223,"Pr
(x,y)∼D"
MAIN RESULT,0.07302231237322515,"h
¯H(x) ̸= y
i
≤ϵ."
RELATED WORK,0.07505070993914807,"1.2
Related work"
RELATED WORK,0.07707910750507099,"Boosting theory has been extensively studied, originally designed for binary classification (e.g.,
AdaBoost and similar variants) [23]. There are various extension of boosting to the multiclass setting."
RELATED WORK,0.07910750507099391,"The early extensions include AdaBoost.MH, AdaBoost.MR, and approaches based on Error-
Correcting Output Codes (ECOC) [24, 1]. These works often reduce the k-class task into a single
binary task. The binary reduction can have various problems, including increased complexity, and
lack of guarantees of an optimal joint predictor."
RELATED WORK,0.08113590263691683,"Other works on multiclass boosting focus on practical considerations and demonstrate empirical
performance improvements across various applications [29, 16, 15, 3, 6, 4, 21]. However, they lack a
comprehensive theoretical framework for the multiclass boosting problem and often rely on earlier
formulations such as one-versus-all reductions to the binary setting or multi-dimensional predictors
and codewords."
RELATED WORK,0.08316430020283976,"Notably, a work by [19] established a theoretical framework for multiclass boosting, which generalizes
previous learning conditions. However, this requires the assumption that the weak learner minimizes
a complicated loss function, that is significantly different from simple classification error. Moreover,
it is based on a restrictive realizability assumption with respect to a known hypothesis class. In
contrast, we do not require realizability, and only consider the standard classification loss."
RELATED WORK,0.08519269776876268,"More recently, [7] followed a formulation for multiclass boosting similar to that of [19]. They proved
a hardness result showing that a broad, yet restricted, class of boosting algorithms must incur a cost
which scales polynomially with |Y|. Our approach does not fall in this class of algorithms. Moreover,
our algorithm has sample and oracle complexity bounds that are entirely independent of |Y|."
RELATED WORK,0.0872210953346856,"2
Warmup: too-weak weak learning"
RELATED WORK,0.08924949290060852,"When there are only 2 labels, the weak learner must find a hypothesis that predicts the correct label
a bit better than a random guess. That is, with a success probability that is slightly more than 1/2.
When the number of labels k is more than 2, perhaps the most natural extension requires that the
weak learner outputs hypotheses that predict the correct label a bit better than a random guess among
k labels. That is, with a success probability that is slightly more than 1/k."
RELATED WORK,0.09127789046653144,"However, this is in fact known to be too weak for boosting (see e.g., [23], Chapter 10). Here we first
give a simple example that demonstrates that fact. However, we also show that all is not yet lost for
the ""better-than-random-guess"" intuition. Specifically, we describe how this condition can still allow
us to extract valuable knowledge about which labels are incorrect. This observation will serve as a
foundation for our main results, which we will elaborate on in the next section."
RELATED WORK,0.09330628803245436,"We start by defining the notion of better-than-random weak learner that we term weak-BRG learning.
Definition 3 (weak-BRG learning). A learning algorithm W is a weak-BRG learner for a hypothesis
class H ⊆[k]X if there is γ > 0 and m0 : (0, 1) 7→N such that for any δ0 > 0, and any H-realizable
distribution D over X × [k], when given m0 ≥m0(δ0) samples from D, it returns h : X →Y such
that with probability 1 −δ0,"
RELATED WORK,0.09533468559837728,"Pr
(x,y)∼D[h(x) = y] ≥1"
RELATED WORK,0.0973630831643002,"k + γ.
(3)"
RELATED WORK,0.09939148073022312,"To get an intuition for why this definition is indeed too weak for boosting, consider the following
simple example. Suppose that X = {a, b, c}, Y = {1, 2, 3}, and that the training set consists of the"
RELATED WORK,0.10141987829614604,"2The ˜O notation conceals polylog(m, 1/δ) factors."
RELATED WORK,0.10344827586206896,"three labeled examples (a, 1), (b, 2), and (c, 3). Further, we suppose that we are using a weak learner
which chooses weak classifiers that never distinguish between a and b. In particular, the weak learner
always chooses one of two weak classifiers: h1 and h2, defined as follows. For x ∈{a, b} then h1
always returns 1 and h2 always returns 2. For x = c they both return 3."
RELATED WORK,0.10547667342799188,"Then, notice that for any distribution over the training set, either h1 or h2 must achieve an accuracy
of at least 1/2, which is significantly higher than the accuracy of 1/k = 1/3. However, regardless
of how weak classifiers are aggregated, any final classifier H that relies solely on the predictions of
the weak hypotheses will unavoidably misclassify either a or b. As a result, the training accuracy
of H on the three examples can never exceed 2/3, making it impossible to achieve perfect accuracy
through any boosting method."
RELATED WORK,0.1075050709939148,"Furthermore, we note that this simple example can also be extended to a case where the data is
realizable by a hypothesis class which is not learnable by any learning algorithm (let alone boosting).
For example, consider the hypothesis class H = {1, 2, 3}X for X = N. Then, H is not PAC learnable
(e.g., via No-Free-Lunch ([26], Theorem 5.1)). However, similarly as above, one can construct a
learning rule that returns a hypothesis with accuracy 1/2 > 1/k over an H-realizable distribution."
RELATED WORK,0.10953346855983773,"Next, we will examine a useful observation that will form the basic building block of our algorithmic
methodology. We demonstrate that the natural weak learner given in Definition 3, while weak, is
nonetheless useful. This can be shown by examining the guarantees obtained through its application in
boosting. Specifically, we consider the following classic variant of boosting via the Hedge algorithm."
RELATED WORK,0.11156186612576065,"Algorithm 1 Boosting via Hedge
Given: Training data S ∈(X × [k])m, parameter η > 0.
Output: A predictor H : X × Y 7→R."
RELATED WORK,0.11359026369168357,"1: Initialize: w1(i) = 1 for all i = 1, ..., m.
2: for t = 1, . . . , T do
3:
Denote by Dt the distribution over [m] obtained by normalizing wt.
4:
Draw m0 examples from Dt and pass to the weak learner.
5:
Get weak hypothesis ht : X 7→Y, and update for i = 1, ..., m:"
RELATED WORK,0.11561866125760649,wt+1(i) = wt(i)e−η·1[ht(xi)=yi].
RELATED WORK,0.11764705882352941,"6: end for
7: Output H such that for all (x, y) ∈X × [k],"
RELATED WORK,0.11967545638945233,"H(x, y) = T
X"
RELATED WORK,0.12170385395537525,"t=1
1[ht(x) = y]."
RELATED WORK,0.12373225152129817,"Notice that the output of Algorithm 1 returns a predictor that is not a classifier, but a scoring function
with the aim of predicting the likelihood of a given label candidate y ∈[k] for some x ∈X. Typically,
boosting algorithms combine the weak hypothesis into such a scoring function yet their final output
applies an argmax over it, to yield a valid classifier. However, since the weak learning assumption is
too weak as we have shown above, taking the argmax is useless in this setting."
RELATED WORK,0.1257606490872211,"Instead, the following lemma shows that by boosting the ""too-weak"" learner, we can guarantee to
eliminate one label for each example in the data. Towards that end, we consider a relaxed variant
of the weak-BRG learner, to be defined over a data set S, which we term the empirical weak-BRG
learner. Specifically, we say that a learner satisfies the empirical weak-BRG condition if there is an
integer m0 such that for any distribution over the training examples, when given m0 examples drawn
i.i.d from it, the learner outputs a hypothesis that satisfies Equation (3)."
RELATED WORK,0.12778904665314403,Proofs are deferred to the appendix.
RELATED WORK,0.12981744421906694,"Lemma 1 (Remove one label). Let S ∈(X × [k])m. Let W be an empirical weak-BRG learner for
S with respect to some γ and sample size m0. Then, the output H : (X × [k]) 7→[0, T] obtained"
RELATED WORK,0.13184584178498987,by running Algorithm 1 with T ≥8 log(m)
RELATED WORK,0.13387423935091278,"γ2
and η =
q ln(m)"
RELATED WORK,0.1359026369168357,"2T , guarantees that for all (x, y) ∈S,"
RELATED WORK,0.13793103448275862,"H(x,y) T
≥1 k + γ"
RELATED WORK,0.13995943204868155,"2 . Moreover, for all (x, y) ∈S the minimally scored label ˆℓ= arg minℓ∈[k] H(x, ℓ)
must be incorrect. That is ˆℓ̸= y."
RELATED WORK,0.14198782961460446,"Notice that if we were to take the argmax of H as is typically done in boosting, the guarantees given
in Lemma 1 do not suggest this will result in the correct prediction. In fact, this approach might yield
a rather bad classifier even for the set S on which it was trained. In other words, for any (x, y) ∈S it
may be that there is some incorrect label y′ ̸= y with H(x, y′) > H(x, y)."
RELATED WORK,0.1440162271805274,"However, notice that the lemma does suggest a good classifier of incorrect labels. That is, the lowest
scored label will always be an incorrect one, over the training data. This property can be shown to
generalize via compression arguments, as discussed in Section 5. This allows us to effectively reduce
the size of the label space by one, and is used as the basic building of our algorithm, as detailed in the
next section."
MULTICLASS BOOSTING RESULTS,0.1460446247464503,"3
Multiclass boosting results"
MULTICLASS BOOSTING RESULTS,0.14807302231237324,"We start by introducing the notion of weak learnability that is assumed by our boosting algorithm.
We note that it is a relaxation of the empirical BRG condition introduced in Definition 2 in the sense
that it does not make any guarantees for the case that the given hint list does not contain the correct
label. This may seem like significantly weakening the assumption, yet it turns out to be sufficient for
our boosting approach to hold."
MULTICLASS BOOSTING RESULTS,0.15010141987829614,"In the resulting fully relaxed framework, no assumptions at all are made about the data. Although
the BRG condition is not explicitly assumed to hold, when this is the case, our final bound given in
Theorem 2 implies a high generalization accuracy."
MULTICLASS BOOSTING RESULTS,0.15212981744421908,"Definition 4 (Relaxed Empirical γ-BRG learning). Let S ∈(X × Y)m, γ > 0, and integer m0.
Let M be a set of list functions of the form 3 µ : X 7→Yk for any integer k, such that for each
µ ∈M and i ∈[m], then yi ∈µ(xi). A learning algorithm satisfies this condition with respect to
(S, γ, m0, M), if for any distribution p over S and any µ : X 7→Yk such that µ ∈M, when given a
sample S′ ∼pm0 and access to µ, it returns h : X 7→Y such that,"
MULTICLASS BOOSTING RESULTS,0.15415821501014199,"Pr
(x,y)∼p[h(x) = y] ≥1"
MULTICLASS BOOSTING RESULTS,0.15618661257606492,"k + γ.
(4)"
MULTICLASS BOOSTING RESULTS,0.15821501014198783,"Notice that when the list µ returns the set of all possible labels Y and it is of size k, this condition
is essentially equivalent to the empirical weak-BRG condition, which as shown above is too weak
for boosting. Requiring that the condition will hold for any list size k ≤|Y| is sufficient to facilitate
boosting, as shown in Theorem 2."
MULTICLASS BOOSTING RESULTS,0.16024340770791076,"The starting point of our overall boosting algorithm (given in Algorirhm 3), is a simple learning
procedure specified in Algorithm 4 that is used to effectively reduce the size of the label space. In
particular, it is used to produce the initial ""hint"" function that is used by the boosting method."
MULTICLASS BOOSTING RESULTS,0.16227180527383367,"Algorithm 2 Initial hint
Given: S ∈(X × Y)m, parameters m0, p > 0.
Output: A function µ : X 7→Yp."
MULTICLASS BOOSTING RESULTS,0.1643002028397566,"1: Set S1 := S.
2: for j = 1, ..., p do
3:
Let Uj denote the uniform distribution over Sj.
4:
Draw m0 examples from Uj and pass to the weak learner, with µ0 ≡Y.
5:
Get weak hypothesis hj : X 7→Y.
6:
Set Si+1 to be all the points in Si which hj predicts incorrectly.
7: end for
8: Output µ defined by:
µ(x) =

h1(x), . . . , hp(x)
	
."
MULTICLASS BOOSTING RESULTS,0.1663286004056795,"3We also allow lists that return an infinite subset of labels. In that case we simply have that the weak
hypothesis satisfies Pr(x,y)∼p[h(x) = y] ≥γ."
MULTICLASS BOOSTING RESULTS,0.16835699797160245,"We can now present our main boosting method in Algorithm 3, and state its guarantees in Theorem 2."
MULTICLASS BOOSTING RESULTS,0.17038539553752535,"Algorithm 3 Recursive Boosting
Given: Training data S ∈(X × Y)m, edge γ > 0, parameters T, η, p > 0.
Output: A predictor ¯H : X 7→Y."
MULTICLASS BOOSTING RESULTS,0.1724137931034483,"1: Initialize: get µ1 by applying Algorithm 2 over S.
2: for j = 1, . . . , p −1 do
3:
Call Hedge (Algorithm 1) with S and µj, and parameters η, T to get Hj : X × Y 7→R."
MULTICLASS BOOSTING RESULTS,0.1744421906693712,"\\ Modify Algorithm 1 to receive µj as input,"
MULTICLASS BOOSTING RESULTS,0.17647058823529413,and in line 4 pass it to the weak learner.
MULTICLASS BOOSTING RESULTS,0.17849898580121704,"4:
Construct µj+1 : X|S 7→Yp−j such that for all x,"
MULTICLASS BOOSTING RESULTS,0.18052738336713997,"µj+1(x) =

y : y ∈µj(x) ∧Hj(x, y) >
T
p −j + 1 
,"
MULTICLASS BOOSTING RESULTS,0.18255578093306288,"5: end for
6: Output the final hypothesis ¯H := µp."
MULTICLASS BOOSTING RESULTS,0.1845841784989858,"The following theorem is formally stating the main result given in Theorem 1.
Theorem 2 (Boosting). Let W denote a learning rule that when given any set of labeled examples
and a list function, returns some hypothesis h : X 7→Y. Let ϵ, δ, γ, m0 > 0, and let D a distribution"
MULTICLASS BOOSTING RESULTS,0.18661257606490872,"over X × Y. Then, when given a sample S ∼Dm for m ≥
102 m0 (ln2(m) ln( m"
MULTICLASS BOOSTING RESULTS,0.18864097363083165,"δ ))
γ3 ϵ
, oracle access to"
MULTICLASS BOOSTING RESULTS,0.19066937119675456,W and T ≥8 ln(m)
MULTICLASS BOOSTING RESULTS,0.1926977687626775,"γ2
, p ≥2 ln(m)"
MULTICLASS BOOSTING RESULTS,0.1947261663286004,"γ
, and η =
q ln(m)"
MULTICLASS BOOSTING RESULTS,0.19675456389452334,"2T , Algorithm 3 outputs a predictor ¯H such that
the following holds. Denote by M the sets of list functions on which W was trained throughout
Algorithm 3. Then, with probability at least 1 −δ, we get that if W satisfies the γ-BRG condition (as
given in Definition 4) with respect to (S, γ, m0, M) then,"
MULTICLASS BOOSTING RESULTS,0.19878296146044624,"Pr
(x,y)∼D"
MULTICLASS BOOSTING RESULTS,0.20081135902636918,"h
¯H(x) ̸= y
i
≤ϵ."
MULTICLASS BOOSTING RESULTS,0.2028397565922921,"Observe that Theorem 2 implicitly assumes that the sample complexity of the weak learner m0 is not
strongly dependent on the overall sample size m and scales at most poly-logarithmically with m. In
other words, although the statement holds for any m0, the result becomes vacuous otherwise."
MULTICLASS BOOSTING RESULTS,0.20486815415821502,"In addition, notice that Theorem 2 is quite agnostic in the sense that we have made no prior assump-
tions about the data distribution. Concretely, Theorem 2 tells us that the generalization error will
be small if the given oracle learner W happens to satisfy the γ-BRG condition with respect to the
particular inputs it receives throughout our boosting procedure."
MULTICLASS BOOSTING RESULTS,0.20689655172413793,"Adaptive boosting
Boosting algorithms typically do not assume knowing the value of γ and are
adapted to it on the fly, as in the well-known Adaboost algorithm [23]. However, the boosting
algorithm given in Algorithm 1, as well as our boosting method as a whole, requires feeding the
algorithm with a value estimating γ. If the estimation of γ provided to the algorithm is too large, the
algorithm may fail. This can be resolved by a simple preliminary binary-search-type procedure, in
which we guess gamma, and possible halve it based on the observed outcome. This procedure only
increases the overall runtime by a logarithmic factor of O(ln(1/γ)), and has no affect on the sample
complexity bounds."
APPLICATIONS TO LIST PAC LEARNING,0.20892494929006086,"4
Applications to List PAC learning"
APPLICATIONS TO LIST PAC LEARNING,0.21095334685598377,"The applications given in this section are based on the framework of List PAC learning [5, 8], and
demonstrate that it is in fact closely related to the multiclass boosting theory. First, we establish
an equivalence between list learnability and weak learnability in the context of the PAC model.
Furthermore, we present a new result on boosting for list PAC learners. Lastly, we give a novel and
alternative proof for characterization of PAC learnability and List PAC learnability. In particular,
these imply a simplified algorithmic approach compared to previous works [5, 8]."
APPLICATIONS TO LIST PAC LEARNING,0.2129817444219067,"We start with introducing list learning in Definition 5, followed by the definition of weak PAC
learning, similarly to the weak-BRG learning definition we give in this work.
Definition 5 (k-List PAC Learning). We say that a hypothesis class H ⊆YX is k-list PAC learnable,
if there is an algorithm such that for every H-realizable distribution D, and every ϵ, δ > 0, when
given S ∼Dm for m ≥m(ϵ, δ), it returns µS : X →Yk such that with probability 1 −δ,"
APPLICATIONS TO LIST PAC LEARNING,0.2150101419878296,"Pr
(x,y)∼D"
APPLICATIONS TO LIST PAC LEARNING,0.21703853955375255,"
y ∈µS(x)

≥1 −ϵ."
APPLICATIONS TO LIST PAC LEARNING,0.21906693711967545,"Definition 6 (γ-weak PAC Learning). We say that a hypothesis class H ⊆YX is γ-weak PAC
learnable, if there is an algorithm such that for every H-realizable distribution D, and every δ > 0,
when given S ∼Dm for m ≥m(δ), it returns hS : X →Y such that with probability 1 −δ,"
APPLICATIONS TO LIST PAC LEARNING,0.2210953346855984,"Pr
(x,y)∼D"
APPLICATIONS TO LIST PAC LEARNING,0.2231237322515213,"
y = hS(x)

≥γ."
APPLICATIONS TO LIST PAC LEARNING,0.22515212981744423,"Next, in the following lemmas we show the strong connection between these two notions. Specifically,
we give an explicit construction of a list learner given oracle access to a weak learner, and vice versa.
Lemma 2 (Weak ⇒List Learning). Let H ⊆YX be a hypothesis class. Assume W is a γ-weak
PAC learner for H with sample complexity mw : (0, 1) 7→N. Let k be the smallest integer such that
1
k < γ, and denote σ = γ −1"
APPLICATIONS TO LIST PAC LEARNING,0.22718052738336714,"k. Then, there is an (k −1)-List PAC learner with sample complexity"
APPLICATIONS TO LIST PAC LEARNING,0.22920892494929007,"m(ϵ, δ) = ˜O

mw(δ/T )"
APPLICATIONS TO LIST PAC LEARNING,0.23123732251521298,"σ2ϵ

where T = ˜O( 1"
APPLICATIONS TO LIST PAC LEARNING,0.2332657200811359,σ2 ) is the number of its oracle calls to W.
APPLICATIONS TO LIST PAC LEARNING,0.23529411764705882,"Lemma 3 (List ⇒Weak Learning). Let H ⊆YX be a hypothesis class. Assume L is a k-List PAC
learner for H with sample complexity mℓ: (0, 1) 7→N. Then, for any ϵ > 0 there is an γ-Weak
PAC learner where γ = 1−2ϵ"
APPLICATIONS TO LIST PAC LEARNING,0.23732251521298176,"k , with sample complexity m(δ) = ˜O
 
mℓ(ϵ, 1/2) · k + (k/ϵ)2
where
q = 2k log(2/δ) is the number of its oracle calls to L."
APPLICATIONS TO LIST PAC LEARNING,0.23935091277890466,"Lastly, Theorem 3 concludes this section demonstrating the strong ties between weak and list
learnability. Concretely, it combines the results of both Lemma 2 and Lemma 3 above to show
that when the appropriate parameters γ and k are optimal, then γ-PAC learnability and k-list PAC
learnability are in fact equivalent.
Theorem 3 (Optimal accuracy ⇐⇒Optimal list size). Let H ⊆YX . Denote by k(H) the smallest
integer k for which H is k-list PAC learnable, assuming that k(H) < ∞. Denote by γ(H) the
supremum over γ ∈[0, 1] for which H is γ-weak PAC learnable. Then, it holds that k(H)·γ(H) = 1."
LIST BOOSTING AND CONFORMAL LEARNING,0.2413793103448276,"4.1
List boosting and conformal learning"
LIST BOOSTING AND CONFORMAL LEARNING,0.2434077079107505,"List prediction rules naturally arise in the setting of conformal learning. In this model, algorithms
make their predictions while also offering some indication of the level of reliable confidence in those
predictions. For example in multiclass classification, given an unlabeled test point x, the conformal
learner might output a list of all possible classes along with scores which reflect the probability that x
belongs to each class. This list can then be truncated to a shorter one which contains only the classes
with the highest score. See the book by [28] and surveys by [25, 2] for more details."
LIST BOOSTING AND CONFORMAL LEARNING,0.24543610547667344,"We now consider a closely related notion of List PAC learnability, that similarly to conformal learning
allows the list size to depend on the desired confidence. This was also defined in [8], termed weak
List PAC Learning, due to the dependence of the list size on the input parameter."
LIST BOOSTING AND CONFORMAL LEARNING,0.24746450304259635,"Indeed, it is natural to expect that the list size will increase when we require a more refined accuracy,
and perhaps that this is a weaker notion of learnability than that of List PAC learning, which
corresponds to a fixed list size."
LIST BOOSTING AND CONFORMAL LEARNING,0.24949290060851928,"Interestingly, it turns out that weak List PAC Learning is in fact equivalent to strong List PAC
Learning. In other words, a list learner with a list size that varies with the desired accuracy parameter
can be boosted to a list learner with a fixed list size, and arbitrarily good accuracy. The proof is by
way of a generalization of our boosting technique to lists, as stated in Theorem 4.
Theorem 4 (List boosting). Let H ⊆YX . Let ϵ0, δ0 > 0, and assume that there exists an algorithm
such that for every H-realizable distribution D, and for some integer k0 := k0(ϵ0), when given
S ∼Dm for m ≥m(ϵ0, δ0), it returns µS : X →Yk0 such that with probability 1 −δ0,"
LIST BOOSTING AND CONFORMAL LEARNING,0.2515212981744422,"Pr
(x,y)∼D"
LIST BOOSTING AND CONFORMAL LEARNING,0.2535496957403651,"
y ∈µS(x)

≥1 −ϵ0."
LIST BOOSTING AND CONFORMAL LEARNING,0.25557809330628806,"Then, there is a k-List PAC learning algorithm for H for a fixed list size k =
j
k0
1−2ϵ0 k
."
LIST BOOSTING AND CONFORMAL LEARNING,0.25760649087221094,"Observe that Theorem 4 indeed generalizes classic boosting. Specifically, consider the binary setting
and notice that when k0 = 1, and ϵ0 is slightly smaller than 1/2, Theorem 4 implies that weak
learning with edge ≈
1
2 −ϵ0, is equivalent to strong learning with arbitrarily small error. The
following corollary shows that weak List PAC Learning implies strong List PAC Learning.
Corollary 1. If a class H ⊆YX is weakly-List PAC learnable then it is also List PAC learnable."
CHARACTERIZATION OF LIST PAC LEARNABILITY,0.25963488843813387,"4.2
Characterization of List PAC learnability"
CHARACTERIZATION OF LIST PAC LEARNABILITY,0.2616632860040568,"We now focus on the characterization of List PAC learnability, which also implies the characteri-
zation of PAC learnability. Towards that end, we define the Daniely-Shwartz (DS) dimension [9].
Specifically, we give the natural generalization of it to k-sized lists, called the k-DS dimension.
Definition 7 (k-DS dimension [8]). Let H ⊆YX be a hypothesis class and let S ∈X d be a sequence.
We say that H k-DS shatters S if there exists F ⊆H, |F| < ∞such that ∀f ∈F|S, ∀i ∈[d], f has
at least k i-neighbors. The k-DS dimension of H, denoted as dk
DS = dk
DS(H), is the largest integer
d such that H k-DS shatters some sequence S ∈X d."
CHARACTERIZATION OF LIST PAC LEARNABILITY,0.26369168356997974,"We note that when k = 1, this captures the standard DS dimension [9]. We show that when the
k-DS dimension is bounded, one can construct a simple weak learner which satisfies our BRG
condition. Thus, it is also amenable to our boosting method, leading to a qualitatively similar results
for characterization of learnability as in [5, 8]. The result is given in the next theorem.
Theorem 5 (PAC and List-PAC learnability). Let H ⊆YX be an hypothesis class with k-DS
dimension d < ∞. Then, H is List PAC learnable. Furthermore, there is a learning algorithm A
for H with the following guarantees. For every H-realizable distribution D, every δ > 0 and every
integer m, given an input sample S ∼Dm, the algorithm A outputs µ = A(S) such that4"
CHARACTERIZATION OF LIST PAC LEARNABILITY,0.2657200811359026,"Pr
(x,y)∼D[µ(x) ̸∋y] ≤˜O"
CHARACTERIZATION OF LIST PAC LEARNABILITY,0.26774847870182555,"d5k4 + log(1/δ) m ! ,"
CHARACTERIZATION OF LIST PAC LEARNABILITY,0.2697768762677485,"with probability at least 1 −δ over S. In particular, if k = 1, then H is PAC learnable."
CHARACTERIZATION OF LIST PAC LEARNABILITY,0.2718052738336714,"We remark that for cases where d ≪k we have an improved result over the bound given by [8]. For
comparison, the error bound given by [8], Theorem 2 is ˜O

d1.5k6+log(1/δ) m

."
CHARACTERIZATION OF LIST PAC LEARNABILITY,0.2738336713995943,"Thus, Theorem 5 demonstrates that our boosting-based approach gives rise to an alternative proof for
the characterization of PAC learnability and List PAC learnability. Moreover, our approach offers a
simpler algorithm and analysis technique, that can perhaps be of use in future applications as well."
GENERALIZATION VIA COMPRESSION,0.27586206896551724,"5
Generalization via compression"
GENERALIZATION VIA COMPRESSION,0.2778904665314402,"This section is concerned with the analysis of our boosting method given in Algorithm 3, and the
proof of our main result given in Theorem 1 (and formally in Theorem 2)."
GENERALIZATION VIA COMPRESSION,0.2799188640973631,"The boosting algorithm given in this work is best thought of as a sample compression scheme [17]. A
sample compression scheme (Definition 8) is an abstraction of a common property to many learning
algorithms. It can be viewed as a two-party protocol between a compresser and a reconstructor. The
compresser gets as input a sample S. The compresser picks a small subsample S′ of S and sends it to
the reconstructor. The reconstructor outputs an hypothesis h. The correctness criteria is that h needs
to correctly classify all examples in the input sample S. We formally define it next.
Definition 8 (Sample Compression Scheme [17]). Let r ≤m be integers. An m →r sample
compression scheme consists of a reconstruction function"
GENERALIZATION VIA COMPRESSION,0.281947261663286,ρ : (X × Y)r →YX
GENERALIZATION VIA COMPRESSION,0.2839756592292089,"such that for every S ∈(X × Y)m, there exists S′ ⊆S of size r, such that for all (x, y) ∈S it holds
that h(x) = y, where h = ρ(S′)."
GENERALIZATION VIA COMPRESSION,0.28600405679513186,"4The ˜O notation conceals polylog(m, 1/γ) factors."
GENERALIZATION VIA COMPRESSION,0.2880324543610548,"We are now ready to prove the main result, given in Theorem 2. The next paragraph highlights the
assumptions that were made, followed by the proof of the theorem."
GENERALIZATION VIA COMPRESSION,0.29006085192697767,"Specifically, we assume for simplicity that the learning algorithm does not employ internal random-
ization. Thus, it can be regarded as a fixed, deterministic mapping from a sequence of m0 unweighted
examples, and a function µ : X 7→Yk, to a hypothesis h : X 7→Y. We note that our results remain
valid for a randomized learner as well, yet we assume the above for ease of exposition."
GENERALIZATION VIA COMPRESSION,0.2920892494929006,"Proof of Theorem 2. The proof is given via a sample compression scheme, demonstrating that if
weak learnability holds, then the final predictor ¯H can be represented using a small number of training
examples, and that it is consistent with the entire training set."
GENERALIZATION VIA COMPRESSION,0.29411764705882354,"First, we fix the sample S and assume that the γ-BRG condition holds for S as in the theorem
statement. We will then show for each j = 1...p that µj+1 satisfies the following 3 properties: (a) for
each x ∈X it returns at most p −j labels, (b) for all (x, y) ∈S, it holds that y ∈µj+1(x), and (c) it
can be represented using only a small number of training examples."
GENERALIZATION VIA COMPRESSION,0.2961460446247465,"First, note that µ1 is indeed a mapping to at most p labels by its construction in Algorithm 2. Moreover,
recall that by the above assumption, the weak learner is a deterministic mapping from its input to
a hypothesis. Therefore, any hypothesis produced by the weak learner within Algorithm 2 can be
represented simply by the sequence of m0 examples on which it was trained. Lemma 4 implies that
there is a subset S′ ⊊S of size at most m0 · p, where p = ⌈log(m)/γ⌉such that the following holds:
There are p hypotheses h′
i : X 7→Y, that comprise the list µ1, where each h′
i can be represented by
m0 examples in S′. It is also guaranteed by Lemma 4 that for all (x, y) ∈S, it holds that y ∈µ1(x)."
GENERALIZATION VIA COMPRESSION,0.29817444219066935,"Next, we will show that the 3 properties (a)-(c) above holds for µ2 (and similarly for all j ≥2).
Consider the first T weak hypotheses h(1)
1 , ..., h(1)
T
generated by Algorithm 3, within its first call to
Algorithm 1. Notice that each h(1)
t
can now be represented by the sequence of m0 examples on which
it was trained, as well as the same m0 · p examples from above that correspond to µ1. Therefore, we
can represent the mapping µ2 by a total of T · m0 + m0 · p examples. Next, we will show that (a)
and (b) hold, by applying Lemma 1. Specifically, we use it to prove that for each (x, y) ∈S, µ2(x)
returns p −1 labels, and also that y ∈µ2(x)."
GENERALIZATION VIA COMPRESSION,0.3002028397565923,"We first show that the conditions of Lemma 1 are met, by considering a simple conversion of all
the labels according to µ1. Specifically, since both Lemma 1 and Algorithm 1 assume the labels are
in [k], yet both Algorithm 3 and our weak learner assume the labels in S are in Y, we can think of
mapping each y ∈Y to [p + 1] according to µ1, getting its corresponding label ℓ∈[p + 1] in the
mapped space, and then remapping back to the Y space when returning to Algorithm 3."
GENERALIZATION VIA COMPRESSION,0.3022312373225152,"Concretely, for each pair (x, y) ∈S, convert it to (x, ℓ) ∈X × [p], such that the ℓ-th entry of µ1(x)
is y, denoted µ1(x)ℓ= y. By Definition 4 we obtain a hypothesis h : X 7→Y. For its internal use in
Algorithm 1, we convert it into a hypothesis h′ : X 7→[p + 1] such that if h(x) ∈µ(x), set h′(x) = ℓ
for ℓthat satisfies µ1(x)ℓ= h(x), or h′(x) = p + 1 if there is no such ℓ∈[p]. Finally, we set the
output H1 of Algorithm 1 to be defined with respect to the original, remapped, weak hypotheses h."
GENERALIZATION VIA COMPRESSION,0.30425963488843816,"Now applying Lemma 1 with k := p, we get that for all (x, y) ∈S, we have H1(x, y) > T/p. There-
fore, it must hold that y ∈µ2(x). Moreover, since P"
GENERALIZATION VIA COMPRESSION,0.30628803245436104,"y′∈µ1(x) H1(x, y′) ≤P"
GENERALIZATION VIA COMPRESSION,0.30831643002028397,"y′∈Y H1(x, y′) ≤T,"
GENERALIZATION VIA COMPRESSION,0.3103448275862069,"Lemma 1 implies that there must be a label y′ ̸= y such that y′ ∈µ1(x) for which H1(x, y′) < T/p.
Therefore, by construction of µ2 we get that y′ /∈µ2(x), and |µ2(x)| ≤|µ1(x) \ {y′}| = p −1."
GENERALIZATION VIA COMPRESSION,0.31237322515212984,"Next, we continue in a similar fashion for all rounds j = 3, ..., p −1. Namely, the same arguments
as above show that by applying Lemma 1 with k := p −j + 2, we get that µj+1 satisfies the above
conditions over S. Moreover, to represent each weak hypotheses h(j)
t
generated by Algorithm 3
within its j-th call to Algorithm 1, we use the sequence of m0 examples on which it was trained, as
well as the same (j −1) · T · m0 + m0 · p examples from above that correspond to µj."
GENERALIZATION VIA COMPRESSION,0.3144016227180527,"Overall, we have shown that if W satisfies the γ-BRG condition (as given in Definition 4) with
respect to (S, γ, m0, M) then the final predictor ¯H := µp is both consistent with the sample S, and
can be represented using only r examples, where,"
GENERALIZATION VIA COMPRESSION,0.31643002028397565,"r = (p −1) · T · m0 + m0 · p = O
p · m0 ln(m) γ2"
GENERALIZATION VIA COMPRESSION,0.3184584178498986,"
= O
m0 ln2(m) γ3"
GENERALIZATION VIA COMPRESSION,0.3204868154158215,"
.
(5)"
GENERALIZATION VIA COMPRESSION,0.3225152129817444,"We can now apply a sample compression scheme bound to obtain the final result. Specifically, we
apply Theorem 6 (for k = 1), for a m →r sample compression scheme algorithm A equipped with
a reconstruction function ρ (see Definition 8). We denote errD( ¯H) = Pr(x,y)∼D[ ¯H(x) ̸= y]. Then,
by Theorem 6 we get that,"
GENERALIZATION VIA COMPRESSION,0.32454361054766734,"Pr
S∼Dm,A"
GENERALIZATION VIA COMPRESSION,0.3265720081135903,"
¯H consistent with S ⇒errD( ¯H) > r ln(m) + ln(1/δ) m −r 
≤δ,"
GENERALIZATION VIA COMPRESSION,0.3286004056795132,"where the overall randomness of our algorithm is denoted by A. Plugging in r from Equation (5),
and m given in the theorem statement, yields the desired bound."
GENERALIZATION VIA COMPRESSION,0.3306288032454361,Acknowledgments and Disclosure of Funding
GENERALIZATION VIA COMPRESSION,0.332657200811359,"AD received funding from the European Research Council (ERC) under the European Union’s
Horizon 2022 research and innovation program (grant agreement No. 101041711), and the Israel
Science Foundation (grant number 2258/19). This research was done as part of the NSF-Simons
Sponsored Collaboration on the Theoretical Foundations of Deep Learning."
GENERALIZATION VIA COMPRESSION,0.33468559837728196,"YM received funding from the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation program (grant agreement No. 882396), by the Israel Science
Foundation (grant number 993/17), the Yandex Initiative for Machine Learning at Tel Aviv University
and a grant from the Tel Aviv University Center for AI and Data Science (TAD)."
GENERALIZATION VIA COMPRESSION,0.3367139959432049,"SM is a Robert J. Shillman Fellow; he acknowledges support by ISF grant 1225/20, by BSF grant
2018385, by an Azrieli Faculty Fellowship, by Israel PBC-VATAT, by the Technion Center for
Machine Learning and Intelligent Systems (MLIS), and by the the European Union (ERC, GENER-
ALIZATION, 101039692). Views and opinions expressed are however those of the author(s) only and
do not necessarily reflect those of the European Union or the European Research Council Executive
Agency. Neither the European Union nor the granting authority can be held responsible for them."
REFERENCES,0.33874239350912777,References
REFERENCES,0.3407707910750507,"[1] Erin L Allwein, Robert E Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying
approach for margin classifiers. Journal of machine learning research, 1(Dec):113–141, 2000."
REFERENCES,0.34279918864097364,"[2] Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction
and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021."
REFERENCES,0.3448275862068966,"[3] Ron Appel and Pietro Perona. A simple multi-class boosting framework with theoretical
guarantees and empirical proficiency. In International Conference on Machine Learning, pages
186–194. PMLR, 2017."
REFERENCES,0.34685598377281945,"[4] Oscar Beijbom, Mohammad Saberian, David Kriegman, and Nuno Vasconcelos. Guess-averse
loss functions for cost-sensitive multiclass boosting. In International Conference on Machine
Learning, pages 586–594. PMLR, 2014."
REFERENCES,0.3488843813387424,"[5] Nataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, and Amir Yehudayoff. A characteri-
zation of multiclass learnability. arXiv preprint arXiv:2203.01550, 2022."
REFERENCES,0.3509127789046653,"[6] Nataly Brukhim and Elad Hazan. Online boosting with bandit feedback. In Algorithmic
Learning Theory, pages 397–420. PMLR, 2021."
REFERENCES,0.35294117647058826,"[7] Nataly Brukhim, Elad Hazan, Shay Moran, and Robert E. Schapire. Multiclass boosting and
the cost of weak learning. In NIPS, 2021."
REFERENCES,0.35496957403651114,"[8] Moses Charikar and Chirag Pabbaraju. A characterization of list learnability. arXiv preprint
arXiv:2211.04956, 2022."
REFERENCES,0.35699797160243407,"[9] Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems. In COLT,
pages 287–316, 2014."
REFERENCES,0.359026369168357,"[10] Sally Floyd and Manfred Warmuth.
Sample compression, learnability, and the vapnik-
chervonenkis dimension. Machine learning, 21(3):269–304, 1995."
REFERENCES,0.36105476673427994,"[11] Yoav Freund. Boosting a weak learning algorithm by majority. In Mark A. Fulk and John Case,
editors, Proceedings of the Third Annual Workshop on Computational Learning Theory, COLT
1990, University of Rochester, Rochester, NY, USA, August 6-8, 1990, pages 202–216. Morgan
Kaufmann, 1990."
REFERENCES,0.3630831643002028,"[12] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. J. Comput. Syst. Sci., 55(1):119–139, 1997."
REFERENCES,0.36511156186612576,"[13] David Haussler, Nick Littlestone, and Manfred K Warmuth. Predicting {0, 1}-functions on
randomly drawn points. Information and Computation, 115(2):248–292, 1994."
REFERENCES,0.3671399594320487,"[14] M. Kearns. Thoughts on hypothesis boosting. Unpublished, December 1988."
REFERENCES,0.3691683569979716,"[15] Balázs Kégl. The return of adaboost. mh: Multi-class hamming trees. ICLR, 2014."
REFERENCES,0.3711967545638945,"[16] Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Multi-class deep boosting. In Advances in
Neural Information Processing Systems, pages 2501–2509, 2014."
REFERENCES,0.37322515212981744,"[17] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Unpub-
lished manuscript, 1986."
REFERENCES,0.3752535496957404,"[18] Shay Moran, Ohad Sharon, and Iska Tsubari.
List online classification.
arXiv preprint
arXiv:2303.15383, 2023."
REFERENCES,0.3772819472616633,"[19] Indraneel Mukherjee and Robert E Schapire. A theory of multiclass boosting. Journal of
Machine Learning Research, 14:437–497, 2011."
REFERENCES,0.3793103448275862,"[20] Benjamin Rubinstein, Peter Bartlett, and J Hyam Rubinstein. Shifting, one-inclusion mistake
bounds and tight multiclass expected risk bounds. In NIPS, pages 1193–1200, 2006."
REFERENCES,0.3813387423935091,"[21] Mohammad J Saberian and Nuno Vasconcelos. Multiclass boosting: Theory and algorithms. In
Advances in Neural Information Processing Systems, pages 2124–2132, 2011."
REFERENCES,0.38336713995943206,"[22] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990."
REFERENCES,0.385395537525355,"[23] Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. Cambridge
university press, 2012."
REFERENCES,0.38742393509127787,"[24] Robert E Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated
predictions. Machine learning, 37(3):297–336, 1999."
REFERENCES,0.3894523326572008,"[25] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine
Learning Research, 9(3), 2008."
REFERENCES,0.39148073022312374,"[26] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge University Press, 2014."
REFERENCES,0.3935091277890467,"[27] John von Neumann. Zur theorie der gesellschaftsspiele. Mathematische Annalen, 100(1):295–
320, 1928."
REFERENCES,0.39553752535496955,"[28] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random
world, volume 29. Springer, 2005."
REFERENCES,0.3975659229208925,"[29] Shaodan Zhai, Tian Xia, and Shaojun Wang. A multi-class boosting method with direct
optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 273–282, 2014."
REFERENCES,0.3995943204868154,"A
Missing proofs of Section 2"
REFERENCES,0.40162271805273836,"Proof of Lemma 1. First, standard analysis of the Hedge algorithm [12] implies that, T
X"
REFERENCES,0.40365111561866124,"t=1
Pr
i∼Dt[ht(xi) = yi]) ≤ln(m)"
REFERENCES,0.4056795131845842,"η
+ ηT + H(xi, yi).
(6)"
REFERENCES,0.4077079107505071,"For completeness we re-prove it here. Denote ϕt = Pm
i=1 wt(i) and αt = Pri∼Dt[ht(xi) = yi]),
and observe that,"
REFERENCES,0.40973630831643004,"ϕt+1 = m
X"
REFERENCES,0.4117647058823529,"i=1
wt+1(i) = m
X"
REFERENCES,0.41379310344827586,"i=1
wt(i)e−η1[ht(xi)=yi] = ϕt m
X"
REFERENCES,0.4158215010141988,"i=1
Dt(i)e−η1[ht(xi)=yi] ≤ϕt m
X"
REFERENCES,0.4178498985801217,"i=1
Dt(i)(1 −η1[ht(xi) = yi] + η2) = ϕt(1 −ηαt + η2) ≤ϕte−ηαt+η2,"
REFERENCES,0.4198782961460446,"where the two inequalities follows from e−x ≤1 −x + x2 for x ≥0, and 1 + x ≤ex, respectively."
REFERENCES,0.42190669371196754,"Therefore, after T rounds we get ϕT ≤me−η PT
t=1 αt+T η2. Furthermore, for every i ∈[m] we have,"
REFERENCES,0.4239350912778905,"e−ηH(xi,yi) = wT (i) ≤ϕT ≤me−η PT
t=1 αt+T η2."
REFERENCES,0.4259634888438134,"Taking logarithms and re-arranging we get Equation (6), as needed."
REFERENCES,0.4279918864097363,"Next, we lower bound the left-hand side with 1"
REFERENCES,0.4300202839756592,k + γ by the weak learning guarantee. By also
REFERENCES,0.43204868154158216,"substituting η =
q ln(m)"
REFERENCES,0.4340770791075051,"2T , we get,"
REFERENCES,0.43610547667342797,"1
k + γ − r"
REFERENCES,0.4381338742393509,2 ln(m)
REFERENCES,0.44016227180527384,"T
≤H(xi, yi) T
."
REFERENCES,0.4421906693711968,Plugging in T yields the desired bound.
REFERENCES,0.44421906693711966,"To prove the second part of the lemma, observe that by its first statement it holds that for each
(x, y) ∈S there must be a label ℓ̸= y for which H(x, ℓ) < T/k."
REFERENCES,0.4462474645030426,"The following lemma proves that Algorithm 2 constructs a good initial hint to the boosting algorithm
(Algorithm 3).
Lemma 4 (Initial hint). Let m0, γ > 0, and let S ∈(X × Y)m. Given oracle access to an empirical
γ-BRG learner for S (Definition 4), when run with p = ⌈ln(m)/γ⌉, Algorithm 2 outputs µ of size p,
such that for all (x, y) ∈S it holds that y ∈µ(x)."
REFERENCES,0.4482758620689655,"Proof. Observe that for all j ∈[p] the output of the weak learner satisfies,"
REFERENCES,0.45030425963488846,"Pr
(x,y)∼Uj[hj(x) = y] ≥γ,
(7)"
REFERENCES,0.45233265720081134,where this follows from Definition 4 . This implies that |Sp| ≤(1 −γ)pm < e−γpm < 1.
REFERENCES,0.4543610547667343,"B
Missing proofs of Section 4"
REFERENCES,0.4563894523326572,"We say that a list function µ : X →Yk is consistent with a sample S ∈(X × Y)m if for every
(x, y) ∈S, it holds that y ∈µ(x)."
REFERENCES,0.45841784989858014,"We now define a compression scheme for lists as follows. Specifically, we say that an algorithm
A is based on an m →r compression scheme for k-lists if there exists a reconstruction function
ρ : (X ×Y)r →(Yk)X , such that when A is provided with a set S ∈(X ×Y)m of training examples,
it somehow chooses (possibly in a randomized fashion) a subset S′ ⊊S of size r, and outputs a k-list
function µ = ρ(S′). Denote its error with respect to a distribution D over X × Y as,"
REFERENCES,0.460446247464503,"errD(µ) =
Pr
(x,y)∼D"
REFERENCES,0.46247464503042596,"h
y /∈µ(x)
i
."
REFERENCES,0.4645030425963489,"Next, we prove a generalization bound for a compression scheme for k-lists. The proof follows
similarly to [17, 10], extending these classical generalization bounds to the notion of lists."
REFERENCES,0.4665314401622718,"Theorem 6. Let r ≤m be integers and k < |Y|. Let A denote a (possibly randomized) algorithm
based on an m →r compression scheme for k-lists. For any S ∈(X × Y)m denote by fS the output
of A over S. Let δ > 0 and set ϵ = r ln(m)+ln(1/δ)"
REFERENCES,0.4685598377281947,"m−r
. Then, for any distribution D over X × Y, with
probability at least 1 −δ it holds that if fS is consistent with S then, errD(fS) ≤ϵ."
REFERENCES,0.47058823529411764,"Proof. We want to show that,"
REFERENCES,0.4726166328600406,"Pr
S∼Dm,A"
REFERENCES,0.4746450304259635,"h
fS is consistent with S
⇒errD(fS) ≤ϵ
i
≥1 −δ.
(8)"
REFERENCES,0.4766734279918864,"In other words, we want to show that,"
REFERENCES,0.4787018255578093,"Pr
S∼Dm,A"
REFERENCES,0.48073022312373226,"h
fS is consistent with S ∧errD(fS) > ϵ
i
≤δ.
(9)"
REFERENCES,0.4827586206896552,"The equivalence of Equations (8) and (9) is because if A and B are events, then A ⇒B is exactly
equivalent to (¬A) ∨B whose negation is A ∧(¬B)."
REFERENCES,0.4847870182555781,"We overload notation and for any S′ ∈(X × Y)r we denote fS′ := ρ(S′), where ρ is the
reconstruction function associated with A."
REFERENCES,0.486815415821501,"First, we fix indices i1, ..., ir ∈[m]. Given the random training set S. Observe that for any list
function f : X →Yk that is ϵ-bad, then the probability of also having f be consistent with S \ S′ is
at most (1 −ϵ)m−r. This holds for any such f, and in particular fS′ if it happens to be ϵ-bad. Since
this also holds for any other fixed choice of indices i1, ..., ir ∈[m], we can bound the above for all
possible sequences of indices. Therefore, by the union bound, since there are mr choices for these
indices, we have that for any fS chosen by A it holds that,"
REFERENCES,0.48884381338742394,"Pr
S∼Dm,A"
REFERENCES,0.4908722109533469,"h
fS consistent with S ∧fS is ϵ-bad
i
(10)"
REFERENCES,0.49290060851926976,"≤
Pr
S∼Dm"
REFERENCES,0.4949290060851927,"h
∃S′ ∈Sr : fS′ consistent with S \ S′ ∧fS′ is ϵ-bad
i
(11) ≤
X"
REFERENCES,0.4969574036511156,"i1...ir∈[m]
Pr
S∼Dm"
REFERENCES,0.49898580121703856,"h
fS′ consistent with S \ S′ ∧fS′ is ϵ-bad
i
(12)"
REFERENCES,0.5010141987829615,"= mr ·
Pr
S′′∼Dm−r
S′∼Dr"
REFERENCES,0.5030425963488844,"h
fS′ consistent with S′′ ∧fS′ is ϵ-bad
i
(13)"
REFERENCES,0.5050709939148073,"= mr ·
Pr
S′′∼Dm−r
S′∼Dr"
REFERENCES,0.5070993914807302,"h
fS′ consistent with S′′  fS′ is ϵ-bad
i
·
Pr
S′∼Dr"
REFERENCES,0.5091277890466531,"h
fS′ is ϵ-bad
i (14)"
REFERENCES,0.5111561866125761,"≤mr ·
Pr
S′′∼Dm−r
S′∼Dr"
REFERENCES,0.513184584178499,"h
fS′ consistent with S′′  fS′ is ϵ-bad
i
(15)"
REFERENCES,0.5152129817444219,"= mr · ES′∼Dr

Pr
S′′∼Dm−r"
REFERENCES,0.5172413793103449,"h
fS′ is consistent with S′′ i  fS′ is ϵ-bad

(16)"
REFERENCES,0.5192697768762677,"≤mr(1 −ϵ)m−r,
(17)"
REFERENCES,0.5212981744421906,"where in Equation (12) we use the notation S′ = {(xi1, yi1), ..., (xir, yir)}, and where the last
inequality holds since for any particular selection of the examples S′ ∈(X × Y)r for which fS′"
REFERENCES,0.5233265720081136,"is ϵ-bad, we have that PrS′′∼Dm−r
h
fS′ is consistent with S′′ i
≤(1 −ϵ)m−r. This means that it
also holds true if these examples are selected at random. Then, we get the desired bound since
mr(1 −ϵ)m−r ≤mre−ϵ(m−r) = δ, which follows by solving for δ via our definition of ϵ above."
REFERENCES,0.5253549695740365,"Theorem 7. Let r ≤m be integers and k < |Y|. Let A denote a (possibly randomized) algorithm
based on an m →r compression scheme for k-lists. For any S ∈(X × Y)m denote by fS the output
of A over S. Let δ > 0 and set ϵ = r ln(m)+ln(1/δ)"
REFERENCES,0.5273833671399595,"m−r
. Then, for any distribution D over X × Y,"
REFERENCES,0.5294117647058824,"Pr
S∼Dm,A"
REFERENCES,0.5314401622718052,"h
errD(fS) > ϵ
i
≤
Pr
S∼Dm,A"
REFERENCES,0.5334685598377282,"h
fS is not consistent with S
i
+ δ.
(18)"
REFERENCES,0.5354969574036511,"Proof. Let CA(S) = 1

fS is consistent with S

. First, using the simple fact that for any two events
A and B it holds that Pr[A] = Pr[A ∧B] + Pr[A ∧¬B] ≤Pr[A ∧B] + Pr[¬B], we get,"
REFERENCES,0.537525354969574,"Pr
S∼Dm,A [fS is ϵ-bad] ≤
Pr
S∼Dm,A [CA(S) ∧fS is ϵ-bad] +
Pr
S∼Dm,A [¬CA(S)] ."
REFERENCES,0.539553752535497,"Thus, it remains to prove that,"
REFERENCES,0.5415821501014199,"Pr
S∼Dm,A [CA(S) ∧fS is ϵ-bad] ≤δ,
(19)"
REFERENCES,0.5436105476673428,which holds true by Theorem 6 above.
REFERENCES,0.5456389452332657,"Algorithm 4 Boosting Weak-to-List Learning
Given: training data S ∈(X × Y)m, edge γ > 0, parameters T, η > 0.
Output: A list µ : X 7→Yk−1."
REFERENCES,0.5476673427991886,1: Let k be the smallest integer such that 1
REFERENCES,0.5496957403651116,"k < γ.
2: Call Hedge (Algorithm 1) with S and parameters η, T to get H : X × Y 7→[0, T].
3: Construct µ : X|S 7→Yk−1 such that for all x,"
REFERENCES,0.5517241379310345,"µ(x) =

ℓ∈Y : H(x, ℓ) > T k 
."
REFERENCES,0.5537525354969574,"4: Output µ : X →Yk−1.
\\ For x /∈S, truncate µ(x) to first k −1 labels, if contains more."
REFERENCES,0.5557809330628803,"The following lemma describes the list-learning guarantees that can be obtained when given access
to a γ-weak learner (Definition 6). It is a re-phrasing of Lemma 2 from the main paper, using exact
constants. The next paragraph highlights the assumptions that were made, followed by the proof of
the lemma."
REFERENCES,0.5578093306288032,"Specifically, we assume for simplicity that the learning algorithm does not employ internal random-
ization. Thus, it can be regarded as a fixed, deterministic mapping from a sequence of m0 unweighted
examples, to a hypothesis h : X →Y. We note that our results remain valid for a randomized learner
as well, yet we assume the above for ease of exposition.
Lemma 5 (Weak-to-List Boosting). Let H ⊆YX be a hypothesis class. Let γ, ϵ, δ > 0, let k be
the smallest integer such that 1"
REFERENCES,0.5598377281947262,"k < γ, and denote σ = γ −1"
REFERENCES,0.5618661257606491,k. Let D be an H-realizable distribution
REFERENCES,0.563894523326572,"over X × Y. Then, when given a sample S ∼Dm for m ≥16 m0 ln2(m)"
REFERENCES,0.565922920892495,"σ2ϵ
+ ln(2/δ)"
REFERENCES,0.5679513184584178,"ϵ
, oracle access"
REFERENCES,0.5699797160243407,to a γ-weak learner (Definition 6) for H with m0 ≥m0( δ
REFERENCES,0.5720081135902637,2T ) and T ≥8 ln(m)
REFERENCES,0.5740365111561866,"σ2
, and η =
q ln(m)"
REFERENCES,0.5760649087221096,"2T ,
Algorithm 4 outputs a list function µ : X →Yk−1 such that with probability at least 1 −δ,"
REFERENCES,0.5780933062880325,"Pr
(x,y)∼D"
REFERENCES,0.5801217038539553,"h
y /∈µ(x)
i
≤ϵ."
REFERENCES,0.5821501014198783,"Proof. By the learning guarantee in Definition 6, we know that for every distribution D over the
examples in S, when given m0 examples from D, with probability at least 1 −δ/(2T) the learner
returns h : X →Y such that m
X"
REFERENCES,0.5841784989858012,"i=1
D(i)1[h(xi) = yi] ≥γ = 1"
REFERENCES,0.5862068965517241,k + σ.
REFERENCES,0.5882352941176471,"In line 2 of Algorithm 4, we call Algorithm 1 with S and the weak learner, with T, η and m0 as
defined above. Then, we follow the same proof as in Lemma 1, applied to S such that the guarantees
of the weak learner holds with probability at least 1−δ/(2T) as above. Then, taking the union bound
over all T calls to the weak learner, by the same argument of Lemma 1 we get that with probability at
least 1 −δ/2 it holds that for all (x, y) ∈S,"
T,0.59026369168357,"1
T T
X"
T,0.592292089249493,"t=1
1[ht(x) = y] ≥1 k + σ"
T,0.5943204868154158,"2 ,
(20)"
T,0.5963488843813387,"where h1, ..., hT are the weak hypotheses obtained via Algorithm 1 and that comprise of H, the
output of Algorithm 1. Moreover, notice that since the weak learning algorithm can be regarded
as a fixed, deterministic mapping from a sequence of m0 unweighted examples to a hypothesis,
hypothesis ht can be expressed via these very m0 examples. Notice that H can be represented by
T · m0 examples. Thus, the list function µ defined in Line 4 of Algorithm 4, can also be represented
by T · m0 examples."
T,0.5983772819472617,"Furthermore, we show that for every (x, y) ∈S the list µ(x) is indeed of size at most k −1. This is
true since for every x ∈X by the definition of H it holds that P"
T,0.6004056795131846,"ℓ∈Y H(x, ℓ) ≤T, and so we have
that for all (x, y) ∈S it holds that at most k −1 labels ℓ∈Y can satisfy H(x, ℓ) > T/k, and thus
|µ(x)| ≤k −1."
T,0.6024340770791075,"Lastly, we observe that µ is consistent with S. That is, by Equation (20) and the definition of µ we
know that for every (x, y) ∈S, we have y ∈µ(x)."
T,0.6044624746450304,"Overall, we get that with probability at least 1 −T · δ/(2T) = 1 −δ/2 over the random choice of
Tm0 examples, it holds that the list function µ is consistent with the sample S. Moreover, it can be
represented using only r examples, where r = Tm0."
T,0.6064908722109533,"We can now apply a sample compression scheme bound to obtain the final result. Specifically, we
consider the bound given in Theorem 7 for a m →r sample compression scheme for k-lists. Then, if
µ is consistent with S then with probability of at least 1 −δ/2 over the choice of S it also holds that,"
T,0.6085192697768763,"Pr
(x,y)∼D"
T,0.6105476673427992,"h
y /∈µ(x)
i
≤r ln(m) + ln(2/δ)"
T,0.6125760649087221,"m −r
."
T,0.6146044624746451,"Plugging in r = Tm0 from above, and setting"
T,0.6166328600405679,m = 8 m0 ln2(m)
T,0.6186612576064908,"σ2ϵ
+ ln(2/δ)"
T,0.6206896551724138,"ϵ
+ 8 ln(m)m0 σ2
,"
T,0.6227180527383367,"yields the desired bound. That is, with probability at least 1 −δ, we have PrD[y ̸∈µ(x)] ≤ϵ."
T,0.6247464503042597,"B.1
Proof of Lemma 3"
T,0.6267748478701826,"Proof of Lemma 3. First, recall that the given oracle is such that for every H-realizable distribution
D, when given S ∼Dmℓfor mℓ≥mℓ(ϵ, δ), it returns µS : X →Yk such that with probability
1 −δ,
Pr
(x,y)∼D"
T,0.6288032454361054,"
y ∈µS(x)

≥1 −ϵ.
(21)"
T,0.6308316430020284,"Then, we show that there is a γ-weak learning algorithm W for H where γ := 1−2ϵ"
T,0.6328600405679513,"k , for any ϵ > 0.
The construction of the algorithm W is as follows. First, let mℓ:= mℓ(ϵ, δ0) for δ0 = 1/2. The
γ-weak learner W has a sample size m to be determined later such that m ≥mℓ, and is given a
training set S ∼Dm. Then, the learner W first calls the k-list learner with parameters ϵ, δ0, mℓover
S and obtains the list function µS that satisfies Equation (21) with probability at least 1 −δ0 = 1/2."
T,0.6348884381338742,"Henceforth, we only consider the probability-half event in which we have that Equation (21) holds.
Furthermore, let Z denote the set of all elements (x, y) ∼D for which y ∈µS(x). Then, notice that
by randomly picking j ≤k and denoting the hypothesis hj
S : X 7→Y defined by the j-the entry of
µS, i.e., hj
S(x) := µS(x)j, we get for any (x, y) ∈Z,"
T,0.6369168356997972,"Pr
j∼Unif(k)"
T,0.6389452332657201,"
y = hj
S(x)

≥1 k ."
T,0.640973630831643,"Then, by von Neumann’s minimax theorem [27] it holds that for any distribution D′ over Z, there
exists some particular j0 ≤k for which"
T,0.6430020283975659,"Pr
(x,y)∼D′

y = hj0
S (x)

≥1 k ."
T,0.6450304259634888,"Since this is true for the distribution D conditioned on Z as well, the algorithm can pick j ≤k
uniformly at random and so it will pick j0 with probability 1"
T,0.6470588235294118,k. Overall we get that when given S the
T,0.6490872210953347,"algorithm can call the list learner to obtain µS and sample j uniformly to obtain hj
S : X 7→Y such
that,"
T,0.6511156186612576,"Pr
S∼Dmℓ"
T,0.6531440162271805,"
Pr
j∼Unif(k)"
T,0.6551724137931034,"
Pr
(x,y)∼D"
T,0.6572008113590264,"
y = hj
S(x)

≥1 −ϵ k 
≥1 k"
T,0.6592292089249493,"
≥1 −δ0."
T,0.6612576064908722,"Since we set δ0 = 1/2, we get that,"
T,0.6632860040567952,"Pr
S∼Dmℓ,j∼Unif(k)"
T,0.665314401622718,"
Pr
(x,y)∼D"
T,0.6673427991886409,"
y = hj
S(x)

≥1 −ϵ k 
≥1"
T,0.6693711967545639,"2k .
(22)"
T,0.6713995943204868,"Lastly, note that the confidence parameter of
1
2k can easily be made arbitrarily large using standard
confidence-boosting techniques. That is, for any δ > 0, we can set q = 2k log(2/δ) and draw q
sample sets S1, ..., Sq from Dmℓand call the algorithm we described so far to generate hypotheses
h1, ..., hq which satisfy Equation (22) with respect to the different samples S, and different random
draws of j ∼Unif(k). By the choice of q, we have that the probability that no h has accuracy at
least 1−ϵ"
T,0.6734279918864098,"k , is at most (1 −
1
2k)q ≤e−q"
T,0.6754563894523327,"2k = δ/2. Thus, we get that with probability at least 1 −δ/2
at least one of these hypothesis is good, in that it has accuracy of at least 1−ϵ"
T,0.6774847870182555,"k . We then pick the
best hypothesis as tested over another independent set Sq+1, which we sample i.i.d. from Dr where
r = 10 log(2q/δ)"
T,0.6795131845841785,"ϵ′2
and ϵ′ = ϵ/k. Then, by Chernoff’s inequality we have for each of the h1, ..., hq
that with probability 1 −δ/(2q) that the gap between its accuracy over Sq+1 and over D is at most
ϵ′/2. By taking the union bound, we get that this holds with probability 1 −δ/2 for all h1, ..., hq
simultaneously."
T,0.6815415821501014,"Thus when we choose the h with the most empirical accuracy over Sq+1, then with probability 1 −δ
it will have an empirical accuracy of at least 1−ϵ"
T,0.6835699797160243,"k
−ϵ′/2, and true accuracy of at least 1−ϵ"
T,0.6855983772819473,"k
−ϵ′.
Therefore, with probability at least 1 −δ we get that the hypothesis chosen by our final algorithm has
accuracy of at least γ = 1−2ϵ"
T,0.6876267748478702,"k
over D."
T,0.6896551724137931,"Overall, we get that W is a γ-weak PAC learner for H, where γ = 1−2ϵ"
T,0.691683569979716,"k
, and with sample complexity"
T,0.6937119675456389,"m = O

k · log(1/δ) · mℓ(ϵ, 1/2) + k2 · log(k/δ)"
T,0.6957403651115619,"ϵ2

."
T,0.6977687626774848,"B.2
Proof of Theorem 3"
T,0.6997971602434077,"Proof of Theorem 3. We start with showing that γ(H) ≥1/k(H). By assumption, we have that for
any fixed ϵ1, δ1 > 0, when given m1 := m1(ϵ1, δ1) examples S drawn i.i.d. from a H-realizable
distribution D, the list learner returns a function µS : X →Yk(H) such that,"
T,0.7018255578093306,"Pr
S∼Dm1"
T,0.7038539553752535,"h
Pr
(x,y)∼D"
T,0.7058823529411765,"
y ∈µS(x)

≥1 −ϵ1
i
≥1 −δ1.
(23)"
T,0.7079107505070994,"By Lemma 3 we get that for any ϵ > 0, there is a γ-weak learner for H, where γ := 1−2ϵ k(H)."
T,0.7099391480730223,"Moreover, by definition of γ(H) as the supremum over all such γ values, we have γ(H) ≥(1−ϵ)"
T,0.7119675456389453,"k(H) .
Lastly, since this holds for any choice of ϵ, we can take ϵ →0 and get γ(H) ≥
1
k(H)."
T,0.7139959432048681,We now show that γ(H) ≤1/k(H). Let k denote the smallest integer for which 1
T,0.716024340770791,"k < γ(H), and let
σ0 := γ(H) −1/k > 0. We will now show how to construct a list learner via the given weak learner,
for a list size of k −1. It suffices to show that, since by the choice of k we have γ(H) ≤1/(k −1),
and so it holds that k(H) ≤k −1 ≤1/γ(H) and we get the desired bound of γ(H) ≤1/k(H).
Thus, all that is left is to show the existence of the (k −1)-list learner."
T,0.718052738336714,"Let ϵ1, δ1 > 0, and let m(ϵ1, δ1) be determined as m in lemma 5 for ϵ1, δ1. Set T = 8 log(m)"
T,0.7200811359026369,"σ2
0
. Let"
T,0.7221095334685599,m0 := m0(δ0 := δ1/(2T)) denote the sample size of a γ-weak PAC learner with γ := σ0 2 + 1
T,0.7241379310344828,"k.
Denote this weak learner by W. Then, by applying Lemma 5 with the γ-weak learner W and with
σ := σ0, we obtain the (k −1)-list learner, which concludes the proof."
T,0.7261663286004056,"B.3
Proof of Theorem 4"
T,0.7281947261663286,"Proof of Theorem 4. The proof follows by constructing a weak learner from the given oracle, and
then applying a weak-to-list boosting procedure."
T,0.7302231237322515,"First, by Lemma 3 we get that there is a γ-weak learning algorithm W for H where γ := 1−2ϵ0 k0
.5"
T,0.7322515212981744,"Then, by applying lemma 5 with this γ-weak learner W, we obtain a (t −1)-List PAC learner, where"
T,0.7342799188640974,"1
t < γ ≤
1
t−1. Then, we also get that 1−2ϵ0"
T,0.7363083164300203,"k0
≤
1
t−1 and so (t −1) ≤
j
k0
1−2ϵ0 k
."
T,0.7383367139959433,"Therefore, there is a k-List PAC learning algorithm for H with a fixed list size k =
j
k0
1−2ϵ0 k
."
T,0.7403651115618661,"C
Proof of Theorem 5"
T,0.742393509127789,"At a high level, the proof describes a construction of a simple weak learner which is shown to satisfy
our BRG condition (Definition 4). This implies it is also amenable to our boosting method, which
yields the final result given in Theorem 5."
T,0.744421906693712,"The construction of the weak learner is based on an object called the One-inclusion Graph (OIG) [13,
20] of a hypothesis class, which is often useful in devising learning algorithms. Typically, one is
interested in orienting the edges of this graph in a way that results in accurate learning algorithms. As
in the analysis used to characterize PAC, and List PAC learning [5, 8], as well as in most applications
of the OIG [9, 13, 20], a good learner corresponds to an orientation with a small maximal out-degree.
However, here we show that a simpler task of minimizing the in-degree is sufficient to obtain a
reasonable weak learner, and therefore a strong learner as well."
T,0.7464503042596349,We start with introducing relevant definitions.
T,0.7484787018255578,"The DS dimension was originally introduced by [9]. Here we follow the formulation given in [5], and
so we first introduce the notion of pseudo-cubes.
Definition 9 (Pseudo-cube). A class H ⊆Yd is called a pseudo-cube of dimension d if it is non-empty,
finite and for every h ∈H and i ∈[d], there is an i-neighbor g ∈H of h (i.e., g(i) ̸= h(i) and
g(j) = h(j) for all j ̸= i)."
T,0.7505070993914807,"When Y = {0, 1}, the two notions “Boolean cube” and “pseudo-cube” coincide: The Boolean cube
{0, 1}d is of course a pseudo-cube. Conversely, every pseudo-cube H ⊆{0, 1}d is the entire Boolean
cube H = {0, 1}d. When |Y| > 2, the two notions do not longer coincide. Every copy of the Boolean
cube is a pseudo-cube, but there are pseudo-cubes that are not Boolean cubes (see [5] for further
details). We are now ready to define the DS dimension.
Definition 10 (DS dimension). A set S ∈X n is DS-shattered by H ⊆YX if H|S contains an
n-dimensional pseudo-cube. The DS dimension dDS(H) is the maximum size of a DS-shattered
sequence."
T,0.7525354969574036,"A natural analogue of this dimension in the context of predicting lists of size k is as follows:
Definition 11 (k-DS dimension [8]). Let H ⊆YX be a hypothesis class and let S ∈X d be a
sequence. We say that H k-DS shatters S if there exists F ⊆H, |F| < ∞such that ∀f ∈F|S, ∀i ∈
[d], f has at least k i-neighbors. The k-DS dimension of H, denoted as dk
DS = dk
DS(H), is the
largest integer d such that H k-DS shatters some sequence S ∈X d."
T,0.7545638945233266,"Observe that the two definitions above differ only in the number of i-neighbors they require each
hypothesis to have, and that dDS = dk
DS for k = 1. Moreover, there are simple hypothesis classes
with infinite DS dimension, yet finite k-DS dimension for k ≥2, see Example 3 in [8]."
T,0.7565922920892495,"Next, we define the object called the One-inclusion Graph (OIG) [13, 20] of a hypothesis class, and
related definitions which will be used for the construction of the weak learner.
Definition 12 (One-inclusion Graph [13, 20]). The one-inclusion graph of H ⊆Yn is a hypergraph
G(H) = (V, E) that is defined as follows.6 The vertex-set is V = H. For each i ∈[n] and"
T,0.7586206896551724,"5We note that although Lemma 3 assumes access to a k-List PAC learner, the same argument holds for the
weaker version of a list learner with list size k0 which depends on ϵ0, and the proofs of both cases is identical.
6We use the term “one-inclusion graph” although it is actually a hypergraph."
T,0.7606490872210954,"f : [n] \ {i} →Y, let ei,f be the set of all h ∈H that agree with f on [n] \ {i}. The edge-set is"
T,0.7626774847870182,"E =

ei,f : i ∈[n], f : [n] \ {i} →Y, ei,f ̸= ∅
	
.
(24)"
T,0.7647058823529411,"We say that the edge ei,f ∈E is in the direction i, and is adjacent to/contains the vertex v if v ∈ei,f.
Every vertex h ∈V is adjacent to exactly m edges. The size of the edge ei,f is the size of the set
|ei,f|."
T,0.7667342799188641,"We remark that similarly to [5, 8], edges could be of size one, and each vertex v is contained in
exactly n edges. This is not the standard structure of edges in hypergraphs, but we use this notation
because it provides a better model for learning problems."
T,0.768762677484787,"With respect to the one-inclusion graph, one can think about the degrees of its vertices, all of which
originally introduced in [8], and are natural generalizations of the degrees defined in [9, 5]"
T,0.77079107505071,"Definition 13 (k-degree). Let G(H) = (V, E) be the one-inclusion graph of H ⊆Ym. The k-degree
of a vertex v ∈V is"
T,0.7728194726166329,"degk(v) = |{e ∈E : v ∈e, |e| > k}|."
T,0.7748478701825557,"We now define the notion of orienting edges of a one-inclusion graph to lists of vertices they are
adjacent to. As alluded to earlier, an orientation corresponds to the behavior of a (deterministic)
learning algorithm while making predictions on an unlabeled test point, given a set of labeled points
as input.
Definition 14 (List orientation). A list orientation σk of the one-inclusion graph G(H) = (V, E)
having list size k is a mapping σk : E →{V ′ ⊆V : |V ′| ≤k} such that for each edge e ∈E,
σk(e) ⊆e."
T,0.7768762677484787,"The k-out-degree of a list orientation σk is defined as:
Definition 15 (k-out-degree of a list orientation). Let G(H) = (V, E) be the one-inclusion graph of
a hypothesis class H, and let σk be a k-list orientation of it. The k-out-degree of v ∈V in σk is"
T,0.7789046653144016,"outdegk(v; σk) = |{e : v ∈e, v /∈σk(e)}|.
(25)"
T,0.7809330628803245,The maximum k-out-degree of σk is
T,0.7829614604462475,"outdegk(σk) = sup
v∈V
outdegk(v; σk).
(26)"
T,0.7849898580121704,"The following lemmas demonstrates how a bound on the dimensions helps one greedily construct a
list orientation of small maximum k-out-degree.
Lemma 6 (Lemma 3.1, [8]). If H ⊆Yd+1 has k-DS dimension at most d, then there exists a k-list
orientation σk of G(H) with outdegk(σk) ≤d."
T,0.7870182555780934,"We now describe a list version of the one-inclusion algorithm below, originally introduced by [8]."
T,0.7890466531440162,Algorithm 5 The one-inclusion list algorithm for H ⊆YX
T,0.7910750507099391,"Input: An H-realizable sample U =
 
(x1, y1), . . . , (xn, yn)

.
Output: A k-list hypothesis µk
U : X →{Y ⊆Y : |Y | ≤k}."
T,0.7931034482758621,"For each x ∈X, the k-list µk
S(x) is computed as follows:"
T,0.795131845841785,"1: Consider the class of all patterns over the unlabeled data H|(x1,...,xn,x) ⊆Yn+1.
2: Find a k-list orientation σk of G(H|(x1,...,xn,x)) that minimizes the maximum k-out-degree.
3: Consider the following edge defined by revealing all labels in U:"
T,0.7971602434077079,"e = {h ∈H|(x1,...,xn,x) : ∀i ∈[n] h(i) = yi}."
T,0.7991886409736308,"4: Set µk
U(x) = {h(x) : h ∈σk(e)}."
T,0.8012170385395537,"Lemma 7. Let H ⊆YX be a hypothesis class, let D be an H-realizable distribution over X ×Y, and
let k, n > 0 be integers. Let M be an upper bound on the maximum k-out-degree of all orientations
σk chosen by Algorithm 5, and let µk
U denote its output for an input sample U ∼Dn. Then,"
T,0.8032454361054767,"Pr
(U,(x,y))∼Dn+1

µk
U(x) ̸∋y

≤
M
n + 1."
T,0.8052738336713996,"Proof. By the leave-one-out symmetrization argument (Fact 14, [7]) we have,"
T,0.8073022312373225,"Pr
(U,(x,y))∼Dn+1

µk
U(x) ̸∋y

=
Pr
(U ′,i)∼Dn+1×Unif(n+1)"
T,0.8093306288032455,"h
µk
U ′
−i(x′
i) ̸∋y′
i
i
."
T,0.8113590263691683,"It therefore suffices to show that for every sample U ′ that is realizable by H,"
T,0.8133874239350912,"Pr
i∼Unif(n+1)"
T,0.8154158215010142,"h
µk
U ′
−i(x′
i) ̸∋y′
i
i
≤
M
n + 1.
(27)"
T,0.8174442190669371,"Fix U ′ that is realizable by H for the rest of the proof. Denote by σk the orientation of G(H′) that
the algorithm chooses."
T,0.8194726166328601,"Let y′ denote the vertex in G(H′) defined by y′ = (y′
1, . . . , y′
n+1), and let ei denote the edge in the
ith direction adjacent to y′. Then, we have that"
T,0.821501014198783,"Pr
i∼Unif(n+1)"
T,0.8235294117647058,"h
µk
U ′
−i(x′
i) ̸∋y′
i
i
=
1
n + 1 n+1
X"
T,0.8255578093306288,"i=1
1
h
µk
U ′
−i(x′
i) ̸∋y′
i
i"
T,0.8275862068965517,"=
1
n + 1 n+1
X"
T,0.8296146044624746,"i=1
1

σk(ei) ̸∋y′"
T,0.8316430020283976,= outdegk(y′; σk) n + 1
T,0.8336713995943205,"≤
M
n + 1."
T,0.8356997971602435,"C.1
Initial phase: pre-processing"
T,0.8377281947261663,"Lemma 8. Let H ⊆YX be a hypothesis class with k-DS dimension d < ∞. Then, for every
H-realizable set S ∈(X × Y)m, any D distribution over S, for a sample U ∼Dn for n := d, when
given U ∈(X × Y)n, Algorithm 5 returns hU : X 7→Y such that,"
T,0.8397565922920892,"EU∼Dd

Pr
(x,y)∼D[y ∈µk
U(x)]

≥
1
d + 1.
(28)"
T,0.8417849898580122,Proof. This follows directly by combining Lemma 6 and Lemma 7
T,0.8438133874239351,"Lemma 9. Let H ⊆YX with k-DS dimension d < ∞, and let S ∈(X × Y)m be an H-realizable
set. Denote by L a k-list learning algorithm as given in Algorithm 5. Denote p = k · q and
q = (d + 1) ln(2m), and let r = qd = O(d2 ln(m)). Then, there exists an algorithm A based on an
m →r compression scheme for p-lists as described next. When provided with S, and given oracle
access to L, the algorithm yields a list function µ : X →Yp such that µ is consistent with S."
T,0.845841784989858,"Proof. We describe an algorithm that sequentially finds q subsets of examples S′
1, ..., S′
q ⊆S each of
size d, and returns a list µ : X 7→Yk, using r examples. First, let U denote the uniform distribution
over the m examples in S and let α =
1
d+1. By Lemma 8 applied to the distribution U, for a random
sample S′
1 ∼Ud, in expectation at least α of the examples (x, y) ∈S satisfy that y ∈µ1(x), where
µ1 is the output of L over S′
1. In particular, there exists S′
1 for which the corresponding µ1 covers
at least αm examples. Algorithmically, it can be found via a brute-force search over all possible
md subsets S′
1 ⊆S. Next, remove from S all examples (x, y) for which y ∈µ1(x) and repeat the
same reasoning on the remaining sample. This way at each step j we find a sample S′
j and a list"
T,0.847870182555781,"µj = L(S′
j) that covers at least an α-fraction of the remaining examples. After q steps, all examples
in S are covered since |Sq| ≤(1 −
1
d+1)qm < e−
q
d+1 m < 1."
T,0.8498985801217038,The final list returned by our method is µ defined by the concatenation:
T,0.8519269776876268,µ(x) = q[
T,0.8539553752535497,"j=1
µj(x),"
T,0.8559837728194726,"and is based only on the examples in S′
q, ..., S′
q which is a total of r examples as claimed."
T,0.8580121703853956,"C.2
Constructing a weak learner for wrong labels"
T,0.8600405679513184,"The following lemmas demonstrates how a bound on the dimensions helps one greedily construct a
list orientation of small maximum (p −1)-out-degree, for a class with p labels."
T,0.8620689655172413,"Lemma 10. Let p, n ∈N and assume H ⊆[p]n is a hypothesis class with a (p −1)-DS dimension
at most d, where d < n. Then, there exists an orientation σp−1 of G(H) with outdegp−1(σp−1) ≤
4(p −1)2d."
T,0.8640973630831643,"Proof. Notice that H is finite, and so the number of vertices in G(H) is finite. We consider another
notion of dimension called the k-Exponential dimension. We say that S ⊆X is k-exponential
shattered by H ⊆YX if |H|S| ≥(k + 1)|S|. The k-exponential dimension dk
E(H) is the maximum
size of an k-exponential shattered sequence."
T,0.8661257606490872,"Notice that for the case that Y = [p] and k = p −1, we have that a set S is (p −1)-exponential
shattered if |H|S| ≥p|S|. Observe that this definition coincides with that of the (p−1)-DS dimension
shattering. That is, a set S is (p −1)-DS shattered if for every h ∈H|S and every i ∈S it holds that
h has exactly p −1 neighbors that agree with it on [n] \ {i}. This definition implies that |H|S| ≥p|S|."
T,0.8681541582150102,"Therefore, for H ⊆[p]n we have that"
T,0.8701825557809331,"dp−1
DS (H) = dp−1
E
(H)."
T,0.8722109533468559,"Then, by Corollary 6.5 of [8] we get that there is an there is a (p −1)-list orientation of G(H) with
maximum (p −1)-out-degree at most 4(p −1)2dp−1
E
(H). Replacing dp−1
E
(H) by dp−1
DS (H) yields
the desired bound."
T,0.8742393509127789,The following lemma is analogous to Lemma 8.
T,0.8762677484787018,"Lemma 11. Let p ∈N and assume H ⊆[p]X is a hypothesis class with a (p −1)-DS dimension
at most d. For every H-realizable set S ∈(X × [p])m, any D distribution over S, denote by
µp−1
U
: X 7→[p]p−1 the p −1-list that is the output of Algorithm 5 when given a sample U ∼D4pd.
Then,"
T,0.8782961460446247,"EU∼D4pd

Pr
(x,y)∼D[y ∈µp−1
U
(x)]

≥1 −1"
T,0.8803245436105477,"4p.
(29)"
T,0.8823529411764706,"Proof. For any U = {(x1, y1), ..., (xn, yn)} where n = 4pd, and any text point (x, y) ∼D, let
H ⊆[p]n+1 be the class of all patterns over the unlabeled data H|(x1,...,xn,x), as in Algorithm 5.
Notice that by Lemma 10, we have that for any such class H, there is an orientation with a maximal
(p −1)-out-degree of at most d. Therefore, by Lemma 7 we get that,"
T,0.8843813387423936,"EU∼D4pd

Pr
(x,y)∼D[y ∈µp−1
U
(x)]

=
Pr
(U,(x,y))∼D4pd+1"
T,0.8864097363083164,"h
y ∈µp−1
U
(x)
i"
T,0.8884381338742393,"≥1 −
d
4pd + 1 = d(4p −1) + 1"
T,0.8904665314401623,"4pd + 1
> (4p −1)/4p."
T,0.8924949290060852,"Theorem 8. Let p ∈N and assume H ⊆[p]X is a hypothesis class with a (p −1)-DS dimension
at most d, and let S ∈(X × [p])m be an H-realizable set of examples. Denote by L a (p −1)-list
learning algorithm as given in Algorithm 5, and let r = 4pd · ⌈8p2 ln(2m)⌉."
T,0.8945233265720081,"Then, there exists an algorithm A based on an m →r compression scheme for (p −1)-lists such
that, when provided with S and given oracle access to L, the algorithm A yields a list function
µ : X →[p]p−1 that is consistent with S."
T,0.896551724137931,"Proof. Let D denote any distribution over the m examples in S . For a random sample U ∼U4pd
denote µU = L(U). By Lemma 11 we have that in expectation over the random choice of U, it
holds that Pr(x,y)∼D[y ∈µU(x)] ≥1 −1"
T,0.8985801217038539,"4p. Therefore, there exists a particular set U of size 4pd for
which this holds as well. Algorithmically, this particular set can be found via a brute-force search
over all possible m4pd subsets U ⊆S of size 4pd. We now have a (p −1)-list function µU = L(U)
such that with probability 1 satisfies Pr(x,y)∼D[y ∈µU(x)] ≥1 −1 4p."
T,0.9006085192697769,"Notice that while µU covers a large fraction of the examples, it is not entirely consistent with S. The
remainder of the proof will be concerned with boosting the list learner we have described above, into
a new list µ : X →Yp−1 that is indeed consistent with the entire sample."
T,0.9026369168356998,"Towards that end, we first use µU to define a classifier fU : X 7→[p] that predicts the incorrect labels.
Specifically, we set fU(x) = ˆy for ˆy ∈[p] \ µU(x). Thus, we have that with probability 1,"
T,0.9046653144016227,"Pr
(x,y)∼D[fU(x) = y] < 1"
T,0.9066937119675457,"4p.
(30)"
T,0.9087221095334685,"So far we have shown that for any distribution D over S there is a set U ∈S4pd and a corresponding
function fU : X 7→[p] (constructed from µU = L(U) as above), that satisfies Equation (30)."
T,0.9107505070993914,"Next, by von Neumann’s minimax theorem [27] there exists a distribution Q over all possible subsets
U ⊆S of size at most 4pd such that for all examples (x, y) ∈S it holds that:"
T,0.9127789046653144,"Pr
U∼Q[fU(x) = y] < 1 4p."
T,0.9148073022312373,"Let U1, ..., Uℓbe random samples from Q where ℓ= ⌈8p2 ln(2m)⌉. Denote ¯Uℓ= (U1, ..., Uℓ), and
define F ¯Uℓto be a corresponding averaged-vote, such that for any x ∈X and y ∈[p]:"
T,0.9168356997971603,"F ¯Uℓ(x, y) = 1 ℓ ℓ
X"
T,0.9188640973630832,"j=1
1
h
fUj(x) = y
i
."
T,0.920892494929006,"Observe that for a fixed example pair (x, y) ∈S then by a Chernoff bound we have,"
T,0.922920892494929,"Pr
¯Uℓ∼Qℓ"
T,0.9249492900608519,"
F ¯Uℓ(x, y) ≥1"
P,0.9269776876267748,2p
P,0.9290060851926978,"
≤
Pr
¯Uℓ∼Qℓ"
P,0.9310344827586207,"
F ¯Uℓ(x, y) ≥E[F ¯Uℓ(x, y)] + 1"
P,0.9330628803245437,4p
P,0.9350912778904665,"
(31)"
P,0.9371196754563894,"≤e−2(ℓ/4p)2/ℓ= e−ℓ/(8p2) ≤
1
2m.
(32)"
P,0.9391480730223124,"Then, by a union bound over all m examples in S we have that with positive probability over the
random choice of ¯Uℓit holds that F ¯Uℓ(x, y) <
1
2p over all (x, y) ∈S simultaneously. This implies
that there exist a particular choice of ¯Uℓfor which we have with probability 1 for all (x, y) ∈S that:"
P,0.9411764705882353,"y /∈arg max
ˆy∈[p] F ¯Uℓ(x, ˆy)."
P,0.9432048681541582,"In words, this means that taking the plurality-vote as induced by ¯Uℓ, we will yield an incorrect label
with probability 1 over all examples in S simultaneously."
P,0.9452332657200812,"Lastly, we are ready to define the final list function µ : X →[p]p−1 by:"
P,0.947261663286004,"µ(x) = [p] \ arg max
ˆy∈[p] F ¯Uℓ(x, ˆy)."
P,0.949290060851927,"To conclude, we have constructed an algorithm based on m →4pd · ℓcompression scheme for
(p −1)-lists, that is consistent with the training sample S, as claimed."
P,0.9513184584178499,"Algorithm 6 k-List PAC Learning for a class H ⊆YX with dk
DS = d < ∞
Given: Training data S ∈(X × Y)m.
Output: A list µ : X 7→Yk."
P,0.9533468559837728,"1: Set p = k · (d + 1) · ln(2m) (as chosen in Lemma9).
2: Initialize: applying pre-processing as given in Lemma 9 to get µ1 : X 7→Yp.
3: for j = 1, ..., p −k do
4:
Let Sj and Hj denote S and H with all labels converted from Y to [p −j + 1] via µj.
5:
Call the list learner given in Theorem 8 over Sj and Hj, to obtain ˜µj : X →[p −j + 1]p−j.
6:
Define µj+1 : X →Yp−j by :"
P,0.9553752535496958,"µj+1(x) =
n
y : ∃ℓ, µj(x)ℓ= y ∧ℓ∈˜µj(x)
o
."
P,0.9574036511156186,"7: end for
8: Output the final list µ := µp−k+1."
P,0.9594320486815415,"Proof of Theorem 5. The proof is given via a sample compression scheme argument, demonstrating
that for a class with a finite k-DS dimension d, Algorithm 6 is a k-list PAC learner. That is, we prove
that it returns a list µ : X 7→Yk which can be represented using a small number of training examples,
and that it is consistent with the entire training set."
P,0.9614604462474645,"We will then show for each j = 1...p −k that µj+1 satisfies the following 2 properties: (a) the list
µj+1 is consistent with the entire sample S, and (b) it can be represented using only a small number
rj of training examples."
P,0.9634888438133874,"First, notice that by Lemma 9 it is guaranteed µ1 is consistent with S, and that it can be represented
by r1 = d(d + 1) ln(2m) examples, as it is constructed by an m →r1 compression scheme."
P,0.9655172413793104,"Next, we will show that the 2 properties (a) and (b) above hold for all j ≥2. Notice that in each
round j of Algorithm 6, the class Hj has only p −j + 1 labels, and its k-DS dimension remains d as
in H. Furthermore, by the properties of the k-DS dimension it holds that for any k′ > k we have
dk′
DS ≤dk
DS. Thus, for all j ≤p −k,"
P,0.9675456389452333,"dp−j
DS (Hj) ≤dk
DS(Hj) = d."
P,0.9695740365111561,"Therefore, we can apply Theorem 8 and get that both properties (a) and (b) hold with rj = 4(p −j +
1)d · ⌈8(p −j + 1)2 ln(2m)⌉, for all j = 2, ..., p −k."
P,0.9716024340770791,"Overall, we have shown that the final list µ := µp−k+1 is both consistent with the sample S, and can
be represented using only r examples, where,"
P,0.973630831643002,r = r1 +
P,0.9756592292089249,"p−k+1
X"
P,0.9776876267748479,"j=2
rj
(33)"
P,0.9797160243407708,= d(d + 1) ln(2m) +
P,0.9817444219066938,"p−k+1
X j=2"
P,0.9837728194726166,"
4(p −j + 1)d · ⌈8(p −j + 1)2 ln(2m)⌉

(34)"
P,0.9858012170385395,"≤d(d + 1) ln(2m) + 32d ln(2m)p4 + p
(35)"
P,0.9878296146044625,"= O

d5 · k4 · ln5(m)

,
(36)"
P,0.9898580121703854,where the bound follows by plugging in the value of p from Algorithm 6 (and Lemma 9).
P,0.9918864097363083,"We can now apply a sample compression scheme bound to obtain the final result. Specifically,
we apply Theorem 7, for a m →r sample compression scheme algorithm A equipped with a
reconstruction function ρ (see Definition 8). We denote errD(µ) = Pr(x,y)∼D[µ(x) ̸∋y]. Then, by
Theorem 7 we get that for any δ > 0,"
P,0.9939148073022313,"Pr
S∼Dm"
P,0.9959432048681541,"
errD(µ) > r ln(m) + ln(1/δ) m −r 
≤δ,"
P,0.9979716024340771,Plugging in r from Equation (36) yields the desired bound.
