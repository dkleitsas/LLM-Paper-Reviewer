Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.005813953488372093,"In many real-world inverse problems, only incomplete measurement data are avail-
able for training which can pose a problem for learning a reconstruction function.
Indeed, unsupervised learning using a fixed incomplete measurement process is im-
possible in general, as there is no information in the nullspace of the measurement
operator. This limitation can be overcome by using measurements from multiple
operators. While this idea has been successfully applied in various applications, a
precise characterization of the conditions for learning is still lacking. In this paper,
we fill this gap by presenting necessary and sufficient conditions for learning the
underlying signal model needed for reconstruction which indicate the interplay
between the number of distinct measurement operators, the number of measure-
ments per operator, the dimension of the model and the dimension of the signals.
Furthermore, we propose a novel and conceptually simple unsupervised learning
loss which only requires access to incomplete measurement data and achieves
a performance on par with supervised learning when the sufficient condition is
verified. We validate our theoretical bounds and demonstrate the advantages of
the proposed unsupervised loss compared to previous methods via a series of
experiments on various imaging inverse problems, such as accelerated magnetic
resonance imaging, compressed sensing and image inpainting."
INTRODUCTION,0.011627906976744186,"1
Introduction"
INTRODUCTION,0.01744186046511628,"In multiple sensing applications, we observe measurements y ∈Rm associated with a signal
x ∈X ⊂Rn, through the forward process"
INTRODUCTION,0.023255813953488372,"y = Ax + ϵ
(1)"
INTRODUCTION,0.029069767441860465,"where A ∈Rn×n is a linear measurement operator and ϵ denotes the noise affecting the measurements.
This is the case of computed tomography [1], depth ranging [2] and non-line-of-sight imaging [3]
to name a few. Estimating x from y is generally an ill-posed inverse problem due to the incomplete
operator A with m < n and the presence of noise. Knowledge of the signal model is required to
make this problem well-posed."
INTRODUCTION,0.03488372093023256,"In many cases, obtaining ground-truth reconstructions x to learn the reconstruction function y 7→x
might be very expensive or even impossible. For example, in medical imaging, it is not always
possible to obtain fully sampled images of patients as they require long acquisition times. In
astronomical imaging, it is impossible to obtain ground-truth references due to physical limitations.
In electron-microscopy imaging [4], we can only measure 2D projections of a molecule. In these
settings, we can only access measurements y for learning. Moreover, if the measurement process A
is incomplete, it is fundamentally impossible to learn the model with only measurements y, as there"
INTRODUCTION,0.040697674418604654,"is no information about the model in the nullspace of A. Thus, we end up with a chicken-and-egg
problem: in order to reconstruct x we need the reconstruction function, but to learn this function we
require some reconstructed samples x."
INTRODUCTION,0.046511627906976744,"This fundamental limitation can be overcome by using information from multiple incomplete sensing
operators A1, . . . , AG, the general principle being that each operator can provide additional infor-
mation about the signal model if it has a different nullspace. For example, in the image inpainting
problem, Studer and Baraniuk [5] used the fact that the set of missing pixels may vary between
observed images to learn a sparse dictionary model and reconstruct the images. Yang et al. [6]
used multiple operators to learn a Gaussian mixture model in the context of hyperspectral imaging
and high-speed video. Bora et al. [7] exploited this idea for learning a generative model in various
imaging problems such as deblurring and compressed sensing. Matrix completion methods [8] exploit
a similar principle, as the missing entries of each column (i.e., signal) are generally different. Ideally
we would like to learn the reconstruction function and signal model from only a small number of
different measurement operators. We are thus motivated to determine typically how many such
operators are required."
INTRODUCTION,0.05232558139534884,"The problem can be formalized as follows. We first focus on the noiseless case to study the intrinsic
identifiability problems associated to having only incomplete measurement data. The effect of noise
will be discussed in Section 4. We assume that we observe a set of N training samples yi, where the
ith signal is observed via Agi ∈Rm×n, one of G linear operators , i.e.,"
INTRODUCTION,0.05813953488372093,"yi = Agixi
(2)"
INTRODUCTION,0.06395348837209303,"where gi ∈{1, . . . , G} and i = 1, . . . , N. While we assume that the measurement operator Agi is
known for all observed signals, it is important to note that we do not know a priori if two observations
(yi, Agi) and (yi′, Agi′) are related to the same signal as in Noise2Noise [9]. There are two natural
questions regarding this learning problem:"
INTRODUCTION,0.06976744186046512,"Q1. Signal Recovery Is there a unique signal x ∈X which verifies the measurements y = Agx? In
other words, is the reconstruction function f : (y, Ag) 7→x one-to-one?
Q2. Model Identification Can we uniquely identify the signal model from measurement data alone
obtained via the incomplete operators A1, . . . , AG?"
INTRODUCTION,0.0755813953488372,"In general, there can be a unique solution for neither problem, just one or both. There might be a
unique solution for signal recovery if the model is known, but it might be impossible to learn the
model in the first place (e.g., blind compressed sensing [10]). The converse is also possible, that is,
uniquely identifying a model without having enough measurements per sample to uniquely identify
the associated signal (e.g., subspace learning from rank-1 projections [11])."
INTRODUCTION,0.08139534883720931,"The answer to Q1 is well-known from generalized compressed sensing theory, see for example [12].
Unique signal recovery is possible if the signal model is low-dimensional, i.e., if Ag has m > 2k
measurements, where k is the model dimension. On the other hand, Q2 has been mostly studied in
the context of matrix completion, where the set of signals is assumed to lie in a low-dimensional
subspace of Rn. Bora et al. [7] presented some results in the general setting, but only for the case
where G = ∞which is quite restrictive. In this paper, we provide sharp necessary and sufficient
conditions which hold for any low-dimensional distribution (beyond linear subspaces) and only
require a finite number of operators G."
INTRODUCTION,0.0872093023255814,"If the conditions for signal recovery and model identification are satisfied, we can expect to learn the
reconstruction function from measurement data alone. We introduce a new unsupervised learning
objective which can be used to learn the reconstruction function f : (y, Ag) 7→x, and provides
performances on par with supervised learning when the sufficient conditions are met. The main
contributions of this paper are as follows:"
INTRODUCTION,0.09302325581395349,"• We show that unsupervised learning from a finite number of incomplete measurement
operators is only possible if the model is low-dimensional. More precisely, we show that
m ≥n/G measurements per operator are necessary for learning, and that for almost every
set of G operators, m > k + n/G measurements per operator are sufficient.
• We propose a new unsupervised loss for learning the reconstruction function that only
requires incomplete measurement data, which empirically obtains a performance on par
with fully supervised methods when the sufficient condition m > k + n/G is met."
INTRODUCTION,0.09883720930232558,"• A series of experiments demonstrate that our bounds accurately characterize the perfor-
mance of unsupervised approaches on synthetic and real datasets, and that the proposed
unsupervised approach outperforms previous methods in various inverse problems."
RELATED WORK,0.10465116279069768,"1.1
Related Work"
RELATED WORK,0.11046511627906977,"Blind Compressed Sensing
The fundamental limitation of failing to learn a signal model from
incomplete (compressed) measurements data goes back to blind compressed sensing [10] for the
specific case of models exploiting sparsity on an orthogonal dictionary. In order to learn the dictionary
from incomplete data, [10] imposed additional constraints on the dictionary, while some subsequent
papers [13, 14] removed these assumptions by proposing to use multiple operators Ag as studied here.
This paper can be seen as a generalization of such results to more general signal models."
RELATED WORK,0.11627906976744186,"Matrix Completion
Matrix completion consists of inferring missing entries of a data matrix Y =
[y1, . . . , yN], whose columns are generally inpainted samples from a low-dimensional distribution,
i.e., yi = Agixi where the operators Agi randomly select a subset of m entries of the signal xi. This
problem can be viewed as the combination of model identification, i.e., identifying the low-rank
subspace that the columns of X = [x1, . . . , xN] belong to, and signal recovery, i.e., reconstructing the
individual columns. Assuming that the samples belong to a k-dimensional subspace can be imposed
by recovering a rank-k signal matrix X from Y . If the columns are sampled via G sufficiently
different patterns Agi with the same number of entries m, a sufficient condition [15] for uniquely
recovering almost every subspace model is1 m ≥(1 −1/G)k + n/G."
RELATED WORK,0.12209302325581395,"A similar condition was shown in [16] for the case of high-rank matrix completion [17], which arises
when the samples xi belong to a union of k-dimensional subspaces. We show that model identification
is possible for almost every set of G operators with m > k + n/G measurements, however the theory
presented here goes beyond linear subspaces, being also valid for general low-dimensional models."
RELATED WORK,0.12790697674418605,"Deep Nets for Inverse Problems
Despite providing very competitive results, most deep learning
based solvers require measurements and signal pairs (xi, yi) (or at least clean signals xi) in order to
learn the reconstruction function y 7→x from incomplete measurements. A first step to overcome
this limitation is due to Noise2Noise [9], where the authors show that it is possible to learn from only
noisy samples. However, their ideas only apply to denoising settings where there is a trivial nullspace,
as the operator A is the identity matrix. This approach was extended in [18] to the case where two
measurements are observed per signal, each associated with a different operator. Yaman et al. [19]
and Artifact2Artifact [20] empirically showed that it is possible to exploit different measurement
operators to learn the reconstruction function in the context of magnetic resonance imaging (MRI).
AmbientGAN [7] proposed to learn a signal distribution from only incomplete measurements using
multiple forward operators, however they only provide reconstruction guarantees for the case where
an infinite number of operators Ag is available2, a condition that is not met in practice."
RELATED WORK,0.13372093023255813,"Another line of work focuses on learning using measurements from a single incomplete operator. The
works in [21, 22] use the large system limit properties of random compressed sensing operators to
learn from measurements alone. The equivariant imaging approach [23, 24] leverages invariance of
the signal set to a group of transformations to learn from general incomplete operators."
SIGNAL RECOVERY PRELIMINARIES,0.13953488372093023,"2
Signal Recovery Preliminaries"
SIGNAL RECOVERY PRELIMINARIES,0.14534883720930233,"We denote the nullspace of A as NA. Its complement, the range space of the pseudo-inverse A†, is
denoted as RA, where RA ⊕NA = Rn and ⊕denotes the direct sum. Throughout the paper, we
assume that the signals are sampled from a measure µ supported on the signal set X ⊂Rn. Signal
recovery has a unique solution if and only if the forward operator x 7→y is one-to-one, i.e., if for
every pair of signals x1, x2 ∈X where x1 ̸= x2 we have that"
SIGNAL RECOVERY PRELIMINARIES,0.1511627906976744,"Ax1 ̸= Ax2
(3)
A(x1 −x2) ̸= 0
(4)"
SIGNAL RECOVERY PRELIMINARIES,0.1569767441860465,"1A larger number of measurements m = O(k log n) is required to guarantee a stable recovery when the
number of patterns G is large [8].
2Their result relies on the Cramér-Wold theorem, which is discussed in Section 3."
SIGNAL RECOVERY PRELIMINARIES,0.16279069767441862,"In other words, there is no vector x1 −x2 ̸= 0 in the nullspace of A. It is well-known that this is
only possible if the signal set X is low-dimensional. There are multiple ways to define the notion of
dimensionality of a set in Rn. In this paper, we focus on the upper box-counting dimension which is
defined for a compact subset S ⊂Rn as"
SIGNAL RECOVERY PRELIMINARIES,0.1686046511627907,"boxdim(S) = lim sup
ϵ→0
log N(S, ϵ)"
SIGNAL RECOVERY PRELIMINARIES,0.1744186046511628,"−log ϵ
(5)"
SIGNAL RECOVERY PRELIMINARIES,0.18023255813953487,"where N(S, ϵ) is the minimum number of closed balls of radius ϵ with respect to the norm ∥· ∥
that are required to cover S. This definition of dimension covers both well-behaved models such as
compact manifolds and more general low-dimensional sets. The mapping x 7→y is one-to-one for
almost every forward operator A ∈Rm×n if [25]
m > boxdim(∆X)
(6)
where ∆X denotes the normalized secant set which is defined as"
SIGNAL RECOVERY PRELIMINARIES,0.18604651162790697,"∆X = {∆x ∈Rn| ∆x =
x2 −x1
∥x2 −x1∥, x1, x2 ∈X, x2 ̸= x1}.
(7)"
SIGNAL RECOVERY PRELIMINARIES,0.19186046511627908,"The term almost every means that the complement has Lebesgue measure 0 in the space of linear
measurement operators Rm×n. The normalized secant set of models of dimension k generally has
dimension 2k, requiring m > 2k measurements to ensure signal recovery. For example, the union of
k-dimensional subspaces requires at least 2k measurements3 to guarantee one-to-oneness [26]. This
includes well-known models such as k-sparse models (e.g., convolutional sparse coding [27]) and
co-sparse models (e.g., total variation [28]). In the regime k < m ≤2k, the subset of signals where
one-to-oneness fails is at most (2k −m)-dimensional [25]."
SIGNAL RECOVERY PRELIMINARIES,0.19767441860465115,"3
Uniqueness of Any Model?"
SIGNAL RECOVERY PRELIMINARIES,0.20348837209302326,"A natural first question when considering uniqueness of the model is: can we recover any probability
measure µ observed via forward operators A1, . . . , AG, even in the case where its support X is the
full Rn? We show that, in general, the answer is no."
SIGNAL RECOVERY PRELIMINARIES,0.20930232558139536,"Uniqueness can be analysed from the point of view of the characteristic function of µ, defined as
φ(w) = E{eiw⊤x} where the expectation is taken with respect to µ and i = √−1 is the imaginary
unit. If two distributions have the same characteristic function, then they are necessarily the same
almost everywhere. Each forward operator provides information about a subspace of the characteristic
function as
E{eiw⊤A†
gy} = E{eiw⊤A†
gAgx}
(8)"
SIGNAL RECOVERY PRELIMINARIES,0.21511627906976744,"= E{ei(A†
gAgw)⊤x}
(9)"
SIGNAL RECOVERY PRELIMINARIES,0.22093023255813954,"= φ(A†
gAgw)
(10)"
SIGNAL RECOVERY PRELIMINARIES,0.22674418604651161,"where A†
gAg is a linear projection onto the subspace RAg. Given that m < n, the characteristic
function is only observed in the subspaces RAg for all g ∈{1, . . . , G}. For any finite number of
operators, the union of these subspaces does not cover the whole Rn, and hence there is loss of
information, i.e., the signal model cannot be uniquely identified."
SIGNAL RECOVERY PRELIMINARIES,0.23255813953488372,"In the case of an infinite number of operators G = ∞, the Cramér-Wold theorem guarantees unique-
ness of the signal distribution if all possible one dimensional projections (m = 1) are available [29, 7].
However, in most practical settings we can only access a finite number of operators and many
distributions will be non-identifiable."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.23837209302325582,"4
Uniqueness of Low-Dimensional Models"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.2441860465116279,"Most models appearing in signal processing and machine learning are assumed to be approximately
low-dimensional, with a dimension k which is much lower than the ambient dimension n. As
discussed in Section 2, the low-dimensional property is the key to obtain stable reconstructions, e.g.,
in compressed sensing. In the rest of the paper, we impose the following assumptions on the model:"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.25,"3While the bound in (6) guarantees unique signal recovery, more measurements (e.g., an additional factor of
O(log n) measurements) are typically necessary in order to have a stable inverse f : y 7→x, i.e., possessing a
certain Lipschitz constant. A detailed discussion can be found for example in [12]."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.2558139534883721,A1 The signal set X is either
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.2616279069767442,"(a) A bounded set with box-counting dimension k.
(b) An unbounded conic set whose intersection with the unit sphere has box-counting
dimension k −1."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.26744186046511625,"This assumption has been widely adopted in the inverse problems literature, as it is a necessary
assumption to guarantee signal recovery. Our definition of dimension covers most models used in
practice, such as simple subspace models, union of subspaces (convolutional sparse coding models,
k-sparse models), low-rank matrices and compact manifolds. It is worth noting that dimension is a
property of the dataset and thus independent of the specific algorithm used for learning."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.27325581395348836,"In the rest of the paper, we focus on conditions for the identification of the support X instead of
the signal distribution µ, due to the following observation: if there is a one-to-one reconstruction
function (which happens for almost every A with m > 2k as explained in Section 2), uniqueness of
the support implies uniqueness of µ. If X is known and there is a measurable one-to-one mapping
from each observed measurement y to X, then it is possible to obtain µ as the push-forward of the
measurement distribution."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.27906976744186046,"Before delving into the main theorem, we present a simple example which provides intuition of how
a low-dimensional model can be learned via multiple projections Ag:"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.28488372093023256,"Learning a one-dimensional subspace
Consider a toy signal model with support X ⊂R3 which
consists of a one-dimensional linear subspace spanned by ϕ = [1, 1, 1]⊤, and G = 3 measurement
operators A1, A2, A3 ∈R2×3 which project the signals into the x(3) = 0, x(2) = 0 and x(1) = 0
planes respectively, where x(i) denotes the ith entry of the vector x. The example is illustrated
in Figure 1. The first operator A1 imposes a constraint on X, that is, every x ∈X should verify
x(1) −x(2) = 0. Without more operators providing additional information about X, this constraint
yields a plane containing X, and there are infinitely many one-dimensional models that would fit the
training data perfectly. However, the additional operator A2 adds the constraint x(2) −x(3) = 0,
which is sufficient to uniquely identify X as
ˆ
X = X = {v ∈R3| v(1) −v(2) = v(2) −v(3) = 0}"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.29069767441860467,"is the desired 1-dimensional subspace. Finally, note that in this case the operator A3 does not restrict
the signal set further, as the constraint x(1) −x(3) = 0 is verified by the other two constraints."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.29651162790697677,"Figure 1: Toy example of a 1-dimensional subspace embedded in R3. If we only observe the
projection of the signal set into the plane x(3) = 0, then there are infinite possible lines that are
consistent with the measurements (red plane). Adding the projection into the x(1) = 0 plane, allows
us to uniquely identify the signal model."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.3023255813953488,"The ideas from the one-dimensional subspace example can be generalized and formalized as follows:
for each projection Ag, we can constrain the model support X by considering the set
ˆ
Xg = {v ∈Rn| v = ˆxg + u, ˆxg ∈X, u ∈NAg}
(11)"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.3081395348837209,"which has dimension at most n −(m −k). Note that the true signal model is a subset of ˆ
Xg. The
inferred signal set belongs to the intersection of these sets"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.313953488372093,"ˆ
X =
\"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.31976744186046513,"g∈G
ˆ
Xg
(12)"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.32558139534883723,"which can be expressed concisely as
ˆ
X = {v ∈Rn| Ag(xg −v) = 0, g = 1, . . . , G, x1, . . . , xG ∈X}
(13)"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.3313953488372093,"Even though we have derived the set ˆ
X from a purely geometrical argument, the constraints in (13)
also offer a simple algebraic intuition: the inferred signal set consists of the points v ∈Rn which
verify the following system of equations
 "
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.3372093023255814,"A1
...
AG  v =  "
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.3430232558139535,"A1x1
...
AGxG "
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.3488372093023256,".
(14)"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.3546511627906977,"for all possible choices of G points x1, . . . , xG in X. In other words, given a dataset of N incomplete
measurements {Agixi}N
i=1, it is possible to build ˆ
X by trying all the possible combinations of G
samples4 and keeping only the points v which are the solutions of (14)."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.36046511627906974,"It is trivial to see that X ⊆ˆ
X, but when can we guarantee X = ˆ
X? As in the previous toy example, if
there are not enough constraints, e.g., if we have a single A and no additional measurement operators,
the inferred set will have a dimension larger than k, containing undesired aliases. In particular, we
have the following lower bound on the minimum number of measurements:"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.36627906976744184,"Proposition 4.1 (Theorem 1 in [23]). A necessary condition for model uniqueness from the measure-
ment sets {AgX}G
g=1 is that rank(  "
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.37209302325581395,"A1
...
AG "
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.37790697674418605,") = n
(15)"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.38372093023255816,and thus m ≥n/G.
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.38953488372093026,"Proof. In order to have model uniqueness, the system in (14) should only admit a solution if
v = x1 = · · · = xG. If the rank condition in (15) is not satisfied, there is more than one solution for
any choice of x1, . . . , xG ∈X."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.3953488372093023,"Note that this necessary condition does not take into account the dimension of the model. As discussed
in Section 3, a sufficient condition for model uniqueness must depend on the dimension of the signal
set k. Our main theorem shows that k additional measurements per operator are sufficient for model
identification:"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.4011627906976744,"Theorem 4.2. For almost every set of G mappings A1, . . . , AG ∈Rm×n, under assumption A1 the
signal model X can be uniquely recovered from the measurement sets {AgX}G
g=1 if the number of
measurements per operator verifies m > k + n/G."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.4069767441860465,"The proof is included in the supplementary material. If we have a large number of independent
operators G ≥n, Theorem 4.2 states that only m > k + 1 measurements are sufficient for model
identification, which is slightly smaller (if the model is not trivial, i.e., k > 1) than the number of
measurements typically needed for signal recovery m > 2k. In this case, it is possible to uniquely
identify the model, without necessarily having a unique reconstruction of each observed signal.
However, as discussed in Section 2, for k < m ≤2k, the subset of signals which cannot be uniquely
recovered is at most (2k −m)-dimensional."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.4127906976744186,"Operators with Different Number of Measurements
The results of the previous subsections
can be easily extended to the setting where each measurement operator has a different number of
measurements, i.e. A1 ∈Rm1×n, . . . , AG ∈RmG×n. In this case, the necessary condition in
Proposition 4.1 is PG
g=1 mg ≥n, and the sufficient condition in Theorem 4.2 is 1"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.4186046511627907,"G
PG
g=1 mg >
k + n/G. As the proofs mirror the ones of Proposition 4.1 and Theorem 4.2, we leave the details to
the reader."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.42441860465116277,"Noisy measurement data
Surprisingly, the results of this section are also theoretically valid if the
measurements are corrupted by independent additive noise ϵ, i.e., y = Agx + ϵ, as long as the noise
distribution is known and has a nowhere zero characteristic function (e.g., Gaussian noise):"
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.43023255813953487,"4Despite providing a good intuition, this procedure for estimating X is far from being practical as it would
require an infinite number of observed samples if the dimension of the signal set is not trivial k > 0."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.436046511627907,"Proposition 4.3. For a fixed noise distribution, if its characteristic function is nowhere zero, then
there is a one-to-one mapping between the space of clean measurement distributions and noise
measurement distributions."
UNIQUENESS OF LOW-DIMENSIONAL MODELS,0.4418604651162791,"The proof is included in the supplementary material. If the clean measurement distribution can be
uniquely identified from the noisy distribution, the results in Theorem 4.2 and Proposition 4.1 also
carry over to the noisy setting. Note that this only guarantees model identifiability and makes no
claims on the sample complexity of any learning process."
ALGORITHMS,0.4476744186046512,"5
Algorithms"
ALGORITHMS,0.45348837209302323,"Unsupervised algorithms mainly come in two flavours: we can first learn a model ˆ
X to then reconstruct
by projecting measurements into this set, or we can attempt to directly learn the reconstruction
function parameterized by a deep network."
LEARN THE MODEL AND RECONSTRUCT,0.45930232558139533,"5.1
Learn the Model and Reconstruct"
LEARN THE MODEL AND RECONSTRUCT,0.46511627906976744,"Dictionary and Subspace Learning
If X is (approximately) a union of subspaces [10] or a single
subspace [8], we can learn a model by"
LEARN THE MODEL AND RECONSTRUCT,0.47093023255813954,"arg min
z,D
E(y,g)∥y −AgDz∥2 + ρ1(D) + ρ2(z)
(16)"
LEARN THE MODEL AND RECONSTRUCT,0.47674418604651164,"where ρ1(D) and ρ2(z) are regularisation terms that promote low-dimensional solutions, e.g., sparse
codes z if X is a union of subspaces. At test time, the dictionary is fixed and the optimization is
performed over the codes only."
LEARN THE MODEL AND RECONSTRUCT,0.48255813953488375,"AmbientGAN
Complex datasets are often better modelled by a generative network f : Rk 7→Rn
whose input is a low-dimensional latent code z ∈Rk. The generative model can be learned using an
adversarial strategy, i.e.,"
LEARN THE MODEL AND RECONSTRUCT,0.4883720930232558,"arg min
f
max
d
E(y,g)q{d(A†
gy)} + EzEgq

1 −d
 
A†
gAgf(z)
	
(17)"
LEARN THE MODEL AND RECONSTRUCT,0.4941860465116279,"where d : Rn 7→Rn is the discriminator network which compares measurements in the image
domain, z is usually sampled from a Gaussian distribution, and q(t) = log(t) for standard GANs and
q(t) = t for Wasserstein GANs. At test time, the reconstruction can be obtained by finding the latent
code that best fits the measurements ˆx = f(arg minz ∥y −Agf(z)∥2), as in [30]."
LEARN TO RECONSTRUCT,0.5,"5.2
Learn to Reconstruct"
LEARN TO RECONSTRUCT,0.5058139534883721,"Another approach consists in learning directly the reconstruction function f : Rm × Rm×n 7→Rn
whose inputs are the measurement y and the associated operator Ag, and the output is the reconstructed
signal x. The reconstruction function can have either a denoiser form, f(y, Ag) = ˜f(A†
gy) where ˜f
is independent of Ag [1], or a more complex unrolled structure with many denoising and gradient
steps [31]."
LEARN TO RECONSTRUCT,0.5116279069767442,"Measurement Splitting
Inspired by the Noise2Noise approaches [9, 18], some self-supervised
methods [19, 20] split each measurement into two parts, y⊤= [y⊤
1 , y⊤
2 ], such that the input is y1 and
the target is y2. These methods can be summarised as minimising the following loss"
LEARN TO RECONSTRUCT,0.5174418604651163,"arg min
f
E(y,g)∥y2 −Ag,2f(y1, Ag,1)∥2
(18)"
LEARN TO RECONSTRUCT,0.5232558139534884,"where y1 = Ag,1x + ϵ1 and y2 = Ag,2x + ϵ2. This approach suffers from the fact that f does not use
all the available information in a given measurement, as it attempts to solve a harder reconstruction
problem associated with y1 = Ag,1x + ϵ1. As reconstruction networks often fail to generalise to
operators with more measurements [32], this method can suffer from suboptimal reconstructions at
test time."
LEARN TO RECONSTRUCT,0.5290697674418605,"Proposed Method
The analysis in Section 4 shows that model identifiability necessarily requires
that reconstructed signals are consistent with all operators A1, . . . , AG. Thus, we propose an
unsupervised loss that ensures consistency across all projections Ag, that is"
LEARN TO RECONSTRUCT,0.5348837209302325,"arg min
f
E(y,g)

∥y −Agf(y, Ag)∥2 + Es∥ˆx −f(Asˆx, As)∥2	
(19)"
LEARN TO RECONSTRUCT,0.5406976744186046,"where ˆx = f(y, Ag). The first term ensures measurement consistency y = Agf(y, Ag), whereas
the second term enforces consistency across operators, i.e., f(y, Ag) = f(Asf(y, Ag), As) for all
g ̸= s. Crucially, the second term prevents the network from learning the trivial pseudo-inverse
f(y, Ag) = A†
gy. In practice, we choose an operator As uniformly at random per minibatch. Thus,
compared the supervised case, we only require an additional evaluation of f and As per minibatch.
We coin this approach multi-operator imaging (MOI). This loss overcomes the main disadvantages of
previous approaches: it doesn’t require training a discriminator network, and it learns to reconstruct
using all the available information in each measurement y."
EXPERIMENTS,0.5465116279069767,"6
Experiments"
EXPERIMENTS,0.5523255813953488,"In this section, we present a series of experiments where the goal is to learn the reconstruction
function with deep networks using real datasets. Experiments on low-dimensional subspace learning
using synthetic datasets are presented in the supplementary material. All our experiments were
performed using an internal cluster of 4 NVIDIA RTX 3090 GPUs with a total compute time of
approximately 48 hs."
EXPERIMENTS,0.5581395348837209,"Compressed Sensing and Inpainting with MNIST
We evaluate the theoretical bounds on the
MNIST dataset, based on the well known approximation of its box-counting dimension k ≈12 [33].
The dataset contains N = 60000 training samples, and these are partitioned such that N/G different
samples are observed via each operator. The forward operators are compressed sensing (CS) matrices
with entries sampled from a Gaussian distribution with zero mean and variance m−1. The test set
consists of 10000 samples, which are also randomly divided into G parts, one per operator. To evaluate
the theoretical bounds, we attempt to minimize the impact of the inductive bias of the networks’
architecture [34, 35] by using a network with 5 fully connected layers and relu non-linearities."
EXPERIMENTS,0.563953488372093,"Figure 2a shows the average test peak-signal-to-noise ratio (PSNR) achieved by the model trained
using the proposed MOI loss for G = 1, 10, 20, 30, 40 and m = 50, 100, 200, 300, 400. The results
follow the bound presented in Section 4 (indicated by the red dashed line), as the network is only
able to learn the reconstruction mapping when the sufficient condition m > k + n/G is verified. In
sensing regimes below this condition, the performance is similar to the pseudo-inverse A†
g."
EXPERIMENTS,0.5697674418604651,"We also evaluate the reconstruction for a different number G of random inpainting masks and
different rates m. The inpainting operators have a diagonal structure which has zero measure in
Rm×n, however our sufficient condition still provides a reasonable lower bound on predicting the
performance, as shown in Figure 2b. It is likely that due to the coherence between measurement
operators and images (both operators and MNIST images are sparse), more measurements are required
to obtain good reconstructions than in the CS case."
EXPERIMENTS,0.5755813953488372,"(a) Compressed Sensing
(b) Inpainting"
EXPERIMENTS,0.5813953488372093,"Figure 2: Average test PSNR improvement in dB over the pseudo-inverse for the MNIST dataset using
the proposed training loss, for different number of CS operators or inpainting masks and measurements
per operator. The curve in red shows the necessary condition of Theorem 4.2, m > k + n/G."
EXPERIMENTS,0.5872093023255814,"Inpainting/CelebA
A†y
AmbientGAN
MOI (ours)
Supervised"
EXPERIMENTS,0.5930232558139535,"9.05±1.65
29.57±1.24
34.05±3.77
36.21±3.76"
EXPERIMENTS,0.5988372093023255,"Acc. MRI/FastMRI
A†y
Meas. Splitting
MOI (ours)
Supervised"
EXPERIMENTS,0.6046511627906976,"Denoiser f
25.77±2.71
28.72±1.64
29.51±1.85
31.45±1.98"
EXPERIMENTS,0.6104651162790697,"Unrolled f
25.77±2.71
29.47±2.02
31.39±2.17
32.42±2.44"
EXPERIMENTS,0.6162790697674418,"Table 1: Comparison of supervised and unsupervised learning methods for inpainting and accelerated
MRI. Reported values correspond to average PSNR in dB on the testing set."
EXPERIMENTS,0.622093023255814,"Inpainting with CelebA
We evaluate the unsupervised methods in Section 5 on the CelebA
dataset [36], which is split into 32556 images for training and 32556 images for testing. We use the
same U-Net (see the supplementary material for more details) for MOI and supervised learning, and
the DCGAN architecture [37] for AmbientGAN as in [7]. We observed that using the U-Net for
AmbientGAN’s generator achieves worse results than the DCGAN architecture. Reconstructed test
images are shown in Figure 3 and average test PSNR is presented in Table 1. The proposed method
obtains an improvement of more than 4 dB with respect to AmbientGAN and falls only 2.1 dB behind
supervised learning."
EXPERIMENTS,0.627906976744186,"Accelerated MRI with FastMRI
Finally, we consider the FastMRI dataset [38], where the set
of forward operators Ag consist of different sets of single-coil k-space measurements, with 4×
acceleration, i.e., m/n = 0.25. We used 900 images for training and 74 for testing, which we
split across G = 40 operators. We compare measurement splitting, MOI and supervised learning,
all using the same denoiser or unrolled architecture (see the supplementary material for details).
For measurement splitting, we follow the strategy in [19], and choose to assign a random subset
representing 60% of the measurements in Ag to Ag,1 and the remaining to Ag,2. We observed that
a model trained with measurement splitting obtained less test error using reduced measurements
associated with Ag,1 instead of the full measurements Ag, so we report the best5 results using Ag,1.
As observed in [32], the network fails to generalize to operators with more measurements. Average
test PSNR is presented in Table 1. Reconstructed test images with the unrolled architecture are shown
in Figure 4. The proposed method performs better than measurement splitting while obtaining results
close to the supervised setting. All training approaches obtain a better performance with the unrolled
architecture than using the denoising network due to the architectural improvements."
EXPERIMENTS,0.6337209302325582,"7.65
29.06
33.89
35.49"
EXPERIMENTS,0.6395348837209303,"Figure 3: Reconstructed test images for the inpainting task using the CelebA dataset. From left
to right: pseudo-inverse A†y, AmbientGAN, MOI, supervised and ground-truth. PSNR values are
reported in yellow."
LIMITATIONS,0.6453488372093024,"7
Limitations"
LIMITATIONS,0.6511627906976745,"Theorem 4.2 does not cover cases where the operators Ag present some problem specific constraints
(e.g., they are inpainting matrices) as well as cases where the signal model is only approximately low
dimensional. Note however that Proposition 4.1 applies to constrained operators. We leave the study
of sufficient conditions for these particular cases for future work. The proposed loss might not be
effective in problems where learning the reconstruction function is impossible, e.g., due to very high
noise affecting the measurements [4]. In this particular case, it might be possible to learn a generative
model as in AmbientGAN [7]."
LIMITATIONS,0.6569767441860465,"5Using the full measurements with the denoiser architecture obtains an average test PSNR of 27.3 dB, i.e., a
decrease of 1.4 dB with respect to the performance using Ag,1 presented in Table 1."
LIMITATIONS,0.6627906976744186,"24.13
31.34
33.00
34.47"
LIMITATIONS,0.6686046511627907,"Figure 4: Examples of reconstructed test images for the accelerated MRI task using an unrolled
network architecture (PGD-3). From left to right: pseudo-inverse A†y, measurement splitting, MOI,
supervised and ground-truth. PSNR values are reported in yellow."
CONCLUSIONS,0.6744186046511628,"8
Conclusions"
CONCLUSIONS,0.6802325581395349,"We presented sensing theorems for the unsupervised learning of signal models from incomplete
measurements using multiple measurement operators. Our bounds characterize the interplay between
the fundamental properties of the problem: the ambient dimension, the data dimension and the
number of measurement operators. The bounds are agnostic of the learning algorithms and provide
useful necessary and sufficient conditions for designing principled sensing strategies."
CONCLUSIONS,0.686046511627907,"Furthermore, we presented a new practical unsupervised learning loss which learns to reconstruct
incomplete measurement data from multiple operators, outperforming previously proposed unsuper-
vised methods. The proposed strategy avoids the adversarial training in AmbientGAN, which can
suffer from mode collapse [39], and, contrary to measurement splitting, is trained using full operators
Ag. Our results shed light into the setting where access to ground truth data cannot be guaranteed
which is of extreme importance in various applications."
CONCLUSIONS,0.6918604651162791,Acknowledgements
CONCLUSIONS,0.6976744186046512,This work is supported by the ERC C-SENSE project (ERCADG-2015-694888).
REFERENCES,0.7034883720930233,References
REFERENCES,0.7093023255813954,"[1] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser. Deep convolu-
tional neural network for inverse problems in imaging. IEEE Transactions on Image Processing,
26(9):4509–4522, 2017."
REFERENCES,0.7151162790697675,"[2] Joshua Rapp, Julian Tachella, Yoann Altmann, Stephen McLaughlin, and Vivek K Goyal.
Advances in single-photon lidar for autonomous vehicles: Working principles, challenges, and
recent advances. IEEE Signal Processing Magazine, 37(4):62–71, 2020."
REFERENCES,0.7209302325581395,"[3] Matthew O’Toole, David B Lindell, and Gordon Wetzstein. Confocal non-line-of-sight imaging
based on the light-cone transform. Nature, 555(7696):338–341, 2018."
REFERENCES,0.7267441860465116,"[4] Harshit Gupta, Thong H Phan, Jaejun Yoo, and Michael Unser. Multi-CryoGAN: Reconstruction
of continuous conformations in cryo-em using generative adversarial networks. In European
Conference on Computer Vision, pages 429–444. Springer, 2020."
REFERENCES,0.7325581395348837,"[5] Christoph Studer and Richard G Baraniuk. Dictionary learning from sparsely corrupted or
compressed signals. In 2012 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 3341–3344. IEEE, 2012."
REFERENCES,0.7383720930232558,"[6] Jianbo Yang, Xuejun Liao, Xin Yuan, Patrick Llull, David J. Brady, Guillermo Sapiro, and
Lawrence Carin. Compressive sensing by learning a gaussian mixture model from measurements.
IEEE Transactions on Image Processing, 24(1):106–119, 2015."
REFERENCES,0.7441860465116279,"[7] Ashish Bora, Eric Price, and Alexandros G. Dimakis. AmbientGAN: Generative models from
lossy measurements. In International Conference on Learning Representations, 2018."
REFERENCES,0.75,"[8] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization.
Foundations of Computational mathematics, 9(6):717, 2009."
REFERENCES,0.7558139534883721,"[9] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala,
Timo Aila, et al. Noise2noise. In International Conference on Machine Learning. PMLR, 2018."
REFERENCES,0.7616279069767442,"[10] Sivan Gleichman and Yonina C Eldar. Blind compressed sensing. IEEE Transactions on
Information Theory, 57(10):6958–6975, 2011."
REFERENCES,0.7674418604651163,"[11] Y. Chen, Y. Chi, and A. J. Goldsmith. Exact and stable covariance estimation from quadratic
sampling via convex programming. IEEE Transactions on Information Theory, 61(7):4034–
4059, 2015."
REFERENCES,0.7732558139534884,"[12] Anthony Bourrier, Mike E Davies, Tomer Peleg, Patrick Pérez, and Rémi Gribonval. Fundamen-
tal performance limits for ideal decoders in high-dimensional linear inverse problems. IEEE
Transactions on Information Theory, 60(12):7928–7946, 2014."
REFERENCES,0.7790697674418605,"[13] Jorge Silva, Minhua Chen, Yonina C Eldar, Guillermo Sapiro, and Lawrence Carin. Blind
compressed sensing over a structured union of subspaces. arXiv preprint arXiv:1103.2469,
2011."
REFERENCES,0.7848837209302325,"[14] Mohammad Aghagolzadeh and Hayder Radha. New guarantees for blind compressed sensing. In
2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton),
pages 1227–1234. IEEE, 2015."
REFERENCES,0.7906976744186046,"[15] Daniel L Pimentel-Alarcón, Nigel Boston, and Robert D Nowak. A characterization of deter-
ministic sampling patterns for low-rank matrix completion. IEEE Journal of Selected Topics in
Signal Processing, 10(4):623–636, 2016."
REFERENCES,0.7965116279069767,"[16] Daniel Pimentel-Alarcon and Robert Nowak. The information-theoretic requirements of sub-
space clustering with missing data. In International Conference on Machine Learning, pages
802–810. PMLR, 2016."
REFERENCES,0.8023255813953488,"[17] Brian Eriksson, Laura Balzano, and Robert Nowak. High-rank matrix completion. In Artificial
Intelligence and Statistics, pages 373–381. PMLR, 2012."
REFERENCES,0.8081395348837209,"[18] Zhihao Xia and Ayan Chakrabarti. Training image estimators without image ground truth. In
Advances in Neural Information Processing Systems, volume 32, 2019."
REFERENCES,0.813953488372093,"[19] Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta Ellermann, Kâmil
U˘gurbil, and Mehmet Akçakaya. Self-supervised learning of physics-guided reconstruction
neural networks without fully sampled reference data. Magnetic Resonance in Medicine,
84(6):3172–3191, 2020."
REFERENCES,0.8197674418604651,"[20] Jiaming Liu, Yu Sun, Cihat Eldeniz, Weijie Gan, Hongyu An, and Ulugbek S Kamilov. Rare:
Image reconstruction using deep priors learned without groundtruth. IEEE Journal of Selected
Topics in Signal Processing, 14(6):1088–1099, 2020."
REFERENCES,0.8255813953488372,"[21] Christopher A Metzler, Ali Mousavi, Reinhard Heckel, and Richard G Baraniuk. Unsupervised
learning with stein’s unbiased risk estimator. arXiv preprint arXiv:1805.10531, 2018."
REFERENCES,0.8313953488372093,"[22] Magauiya Zhussip, Shakarim Soltanayev, and Se Young Chun. Training deep learning based
image denoisers from undersampled measurements without ground truth and without image
prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 10255–10264, 2019."
REFERENCES,0.8372093023255814,"[23] Dongdong Chen, Julián Tachella, and Mike E Davies. Equivariant imaging: Learning beyond
the range space. In Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV), pages 4379–4388, October 2021."
REFERENCES,0.8430232558139535,"[24] Dongdong Chen, Julián Tachella, and Mike E Davies. Robust equivariant imaging: a fully
unsupervised framework for learning to image from noisy and partial measurements.
In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
5647–5656, 2022."
REFERENCES,0.8488372093023255,"[25] Tim Sauer, James A Yorke, and Martin Casdagli. Embedology. Journal of statistical Physics,
65(3):579–616, 1991."
REFERENCES,0.8546511627906976,"[26] T. Blumensath and M. E. Davies. Sampling theorems for signals from the union of finite-
dimensional linear subspaces. IEEE Transactions on Information Theory, 55(4):1872–1882,
2009."
REFERENCES,0.8604651162790697,"[27] Hilton Bristow, Anders Eriksson, and Simon Lucey. Fast convolutional sparse coding. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
391–398, 2013."
REFERENCES,0.8662790697674418,"[28] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal
algorithms. Physica D: nonlinear phenomena, 60(1-4):259–268, 1992."
REFERENCES,0.872093023255814,"[29] Harald Cramér and Herman Wold. Some theorems on distribution functions. Journal of the
London Mathematical Society, 1(4):290–294, 1936."
REFERENCES,0.877906976744186,"[30] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using
generative models. In International Conference on Machine Learning, pages 537–546. PMLR,
2017."
REFERENCES,0.8837209302325582,"[31] Vishal Monga, Yuelong Li, and Yonina C. Eldar. Algorithm unrolling: Interpretable, efficient
deep learning for signal and image processing. IEEE Signal Processing Magazine, 38(2):18–44,
2021."
REFERENCES,0.8895348837209303,"[32] Vegard Antun, Francesco Renna, Clarice Poon, Ben Adcock, and Anders C. Hansen. On
instabilities of deep learning in image reconstruction and the potential costs of ai. Proceedings
of the National Academy of Sciences, 117(48):30088–30095, 2020."
REFERENCES,0.8953488372093024,"[33] Matthias Hein and Jean-Yves Audibert. Intrinsic dimensionality estimation of submanifolds in
rd. In Proceedings of the 22nd international conference on Machine learning, pages 289–296,
2005."
REFERENCES,0.9011627906976745,"[34] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pages 9446–9454, 2018."
REFERENCES,0.9069767441860465,"[35] Julian Tachella, Junqi Tang, and Mike Davies. The neural tangent link between CNN denoisers
and non-local filters. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 8618–8627, June 2021."
REFERENCES,0.9127906976744186,"[36] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.9186046511627907,"[37] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015."
REFERENCES,0.9244186046511628,"[38] Florian Knoll, Jure Zbontar, Anuroop Sriram, Matthew J. Muckley, Mary Bruno, Aaron Defazio,
Marc Parente, Krzysztof J. Geras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal
Drozdzalv, Adriana Romero, Michael Rabbat, Pascal Vincent, James Pinkerton, Duo Wang,
Nafissa Yakubova, Erich Owens, C. Lawrence Zitnick, Michael P. Recht, Daniel K. Sodickson,
and Yvonne W. Lui. fastMRI: A publicly available raw k-space and DICOM dataset of knee
images for accelerated mr image reconstruction using machine learning. Radiology: Artificial
Intelligence, 2(1):e190007, 2020. PMID: 32076662."
REFERENCES,0.9302325581395349,"[39] Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory
and empirics. In International Conference on Learning Representations, 2018."
REFERENCES,0.936046511627907,Checklist
REFERENCES,0.9418604651162791,1. For all authors...
REFERENCES,0.9476744186046512,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes]"
REFERENCES,0.9534883720930233,(c) Did you discuss any potential negative societal impacts of your work? [Yes]
REFERENCES,0.9593023255813954,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results..."
REFERENCES,0.9651162790697675,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments..."
REFERENCES,0.9709302325581395,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes]
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes]
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9767441860465116,"(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [Yes]"
REFERENCES,0.9825581395348837,(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
REFERENCES,0.9883720930232558,"We used existing models and datasets.
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [Yes] All datasets considered in the paper are publicly available.
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [Yes]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9941860465116279,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A] We did not use any crowdsourcing or conducted research with
human subjects.
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
