Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0022222222222222222,"We propose GRAph Neural Diffusion with a source term (GRAND++) for graph
deep learning with a limited number of labeled nodes, i.e., low-labeling rate.
GRAND++ is a class of continuous-depth graph deep learning architectures whose
theoretical underpinning is the diffusion process on graphs with a source term.
The source term guarantees two interesting theoretical properties of GRAND++:
(i) the representation of graph nodes, under the dynamics of GRAND++, will not
converge to a constant vector over all nodes even as the time goes to inﬁnity,
which mitigates the over-smoothing issue of graph neural networks and enables
graph learning in very deep architectures. (ii) GRAND++ can provide accurate
classiﬁcation even when the model is trained with a very limited number of la-
beled training data. We experimentally verify the above two advantages on vari-
ous graph deep learning benchmark tasks, showing a signiﬁcant improvement over
many existing graph neural networks."
INTRODUCTION,0.0044444444444444444,"1
INTRODUCTION"
INTRODUCTION,0.006666666666666667,"Graph neural networks (GNNs) are the backbone for deep learning on graphs. Recent GNN ar-
chitectures include graph convolutional networks (GCNs) [30], ChebyNet [16], GraphSAGE [29],
neural graph ﬁngerprints [20], message passing neural networks [28], and graph attention networks
(GATs) [54]. These graph deep networks have achieved success in many applications, including
computational physics and computational chemistry [20, 28, 3], recommender systems [41, 62], and
social networks [63, 47]. Hyperbolic GNNs have also been proposed to enable certain kinds of
data embedding with much smaller distortion [11, 37]. See [6] for some recent advances of GNN
algorithm development and applications."
INTRODUCTION,0.008888888888888889,"A well-known problem of GNNs is that increasing the depth of GNNs often results in a signiﬁcant
drop in performance on various graph learning tasks. This performance degradation has been widely
interpreted as the over-smoothing issue of GNNs [35, 44, 12]. Intuitively, GNN layers update the
node representation by taking a weighted average of its neighbors’ features, making representations
for neighboring nodes to be similar. As the GNN architecture gets deeper, all nodes’ representa-
tion will become indistinguishable resulting in over-smoothing. In Sec. 2, we brieﬂy show that
certain GNNs have a diffusive nature which makes over-smoothing inevitable. Another interesting
interpretation of the GNN performance degradation is via a bottleneck [1], since a GNN tends to
represent exponentially growing information from neighbors with ﬁxed-size vectors. Several algo-
rithms have been proposed to mitigate the over-smoothing of GNNs, including skip connection and
dilated convolution [33], Jumping Knowledge [60], DropEdge [49], PairNorm [64], graph neural
diffusion (GRAND) [10], and wave equation motivated GNNs [21]. Nevertheless, developing deep
GNN architectures is still in its infancy compared to the development of other deep networks."
INTRODUCTION,0.011111111111111112,"Besides suffering from over-smoothing, we notice that the accuracy of existing GNNs drops severely
when they are trained with a limited labeled data. As illustrated in Fig. 1, the test accuracy of several"
INTRODUCTION,0.013333333333333334,∗Correspond to wangbaonj@gmail.com or matthew.thorpe-2@manchester.ac.uk
INTRODUCTION,0.015555555555555555,Published as a conference paper at ICLR 2022
INTRODUCTION,0.017777777777777778,"celebrated GNN architectures, including GCN, GAT, and GraphSage, drops rapidly when they are
trained with fewer labeled data. Moreover, the variance of classiﬁcation accuracy grows signiﬁcantly
as number of labeled nodes drops. Indeed, semi-supervised graph learning with very low-labeling
rates has been studied in the Laplace learning and graph deep learning settings, see, e.g., [36, 9, 23];
one question is can we develop new GNN architectures to improve the performance of graph deep
learning in low-labeling rate regimes?"
INTRODUCTION,0.02,"0
5
10
15
20"
INTRODUCTION,0.022222222222222223,#Labeled Nodes Per Class 30 40 50 60 70 80
INTRODUCTION,0.024444444444444446,Classification Acc (%)
INTRODUCTION,0.02666666666666667,"GCN
GAT
GraphSage"
INTRODUCTION,0.028888888888888888,"0
5
10
15
20"
INTRODUCTION,0.03111111111111111,#Labeled Nodes Per Class 30 40 50 60 70 80
INTRODUCTION,0.03333333333333333,Classification Acc (%)
INTRODUCTION,0.035555555555555556,"GCN
GAT
GraphSage"
INTRODUCTION,0.03777777777777778,"0
5
10
15
20"
INTRODUCTION,0.04,#Labeled Nodes Per Class 40 45 50 55 60 65 70 75 80
INTRODUCTION,0.042222222222222223,Classification Acc (%)
INTRODUCTION,0.044444444444444446,"GCN
GAT
GraphSage"
INTRODUCTION,0.04666666666666667,"CORA
CiteSeer
PubMed
Figure 1: Test accuracy of GCN, GAT, and GraphSage vs. the number of labeled nodes per class. All networks
have 2 layers, and each experiment is run with 100 splits and 20 random seeds following [10]. The accuracy
drops rapidly with fewer labeled data for training. CORA, CiteSeer, and PubMed have 2485, 2120, and 19717
nodes in total respectively. Results on more benchmark GNN architectures are in Appendix D.4."
OUR CONTRIBUTION,0.04888888888888889,"1.1
OUR CONTRIBUTION"
OUR CONTRIBUTION,0.051111111111111114,"With the above GNN problems in mind, we focus on developing new continuous-depth GNNs to
overcome over-smoothing and boost the accuracy of GNNs with a limited number of labeled data.
We ﬁrst present a random walk interpretation of the GRAND model [10], revealing a potentially in-
evitable over-smoothing phenomenon when GRAND is implicitly very deep. Based on the random
walk viewpoint of GRAND, we then propose graph neural diffusion with a source term (GRAND++)
that corrects the bias arising from the diffusion process, see Sec. 5 for details. GRAND++ theoret-
ically guarantees that: (i) under GRAND++ dynamics, the graph node features do not converge to
a constant vector over all nodes even as the time goes to inﬁnity, and (ii) GRAND++ can provide
accurate prediction even when it is trained with a limited number of labeled nodes. Moreover, these
theoretical results resonate with the practical advantages of GRAND++. We summarize the major
practical advantages of GRAND++ below.
• GRAND++ can effectively overcome the over-smoothing issue; it is remarkably more accurate
than existing GNNs when the architecture is very deep.
• GRAND++ is suitable for graph deep learning when only a few nodes are labeled as training data.
Moreover, in the low-labeling rates, GRAND++ can be more accurate when the network is deeper.
• GRAND++ inherits the continuous-depth merit from GRAND, which deﬁnes the network depth
implicitly and enables memory-efﬁcient training by using the adjoint method."
RELATED WORK,0.05333333333333334,"1.2
RELATED WORK
Diffusion on graphs and continuous-depth graph neural networks.
Diffusion has been deﬁned
on graphs, see, e.g., [25, 24], and used in various applications, including data clustering and di-
mension reduction [15, 4], image processing [27, 22, 17, 38], and semi-supervised graph nodes
classiﬁcation [67, 65]. From the numerical viewpoint, fast algorithms have been proposed for using
diffusion on graphs to solve penalized graph cut problems [26]. The connection between GNNs
and diffusion on graphs has been studied substantially. For instance, GNN has been interpreted as a
diffusion process on graphs, which performs low-pass ﬁltering on the input features [43]. Moreover,
insights from the diffusion process on graphs have been used to improve the performance of GNNs,
see, e.g., [2, 36, 31, 57]."
RELATED WORK,0.05555555555555555,"Leveraging neural ordinary differential equations (ODEs) [13], continuous-depth GNNs have been
proposed, see, e.g., [45, 58, 68]. One recent work is GRAND [10], which parameterizes the diffusion
equation on graphs with a neural network. See Sec. 3 for a brief review of GRAND.
Neural ODEs.
Neural ODEs [13] are a class of continuous-depth neural networks whose depth is
deﬁned implicitly. Training neural ODEs using the adjoint method [46] is more memory efﬁcient
than training other neural networks using backpropagation. We provide a brief review of neural
ODEs and the adjoint method in Appendix C. GRANDs [10] are a class of neural partial differen-
tial equations (PDEs) on graphs that can also be considered as a coupled system of neural ODEs.
Furthermore, GRANDs are also trained by using the adjoint method."
RELATED WORK,0.057777777777777775,Published as a conference paper at ICLR 2022
RELATED WORK,0.06,"Laplace learning and Poisson learning.
Laplace learning has been used for semi-supervised data
classiﬁcation [67, 65, 56], image processing [7, 27], etc. Direct application of Laplace learning
with Gaussian weights [5] or locally linear embedding weights [50] for the above tasks may cause
inference inconsistency when only a limited number of graph nodes are labeled, resulting in poor
performance. Several algorithms address the inference inconsistency at low labeling rate. They
include up-weighting the weights of the labeled data [52] and the p-Laplacian [8, 48, 66]. In [9], the
authors have proposed Poisson learning for improving Laplace learning at extremely low-labeling
rate regimes. Poisson learning augments Laplace learning with a Green’s function at each labeled
data, enabling accurate node classiﬁcation when only a few labeled data are available. Compared to
Laplace learning, Poisson learning adds Green’s function to the label of each labeled node and then
performs label propagation to predict the label for unlabeled graph nodes. GRAND and GRAND++
both learn graph node representations and perform prediction by activating the node representations,
which are fundamentally different from Laplace and Poisson learning."
"NOTATION
WE DENOTE SCALARS BY LOWER- OR UPPER-CASE LETTERS AND VECTORS AND MATRICES BY LOWER- AND UPPER-CASE",0.06222222222222222,"1.3
NOTATION
We denote scalars by lower- or upper-case letters and vectors and matrices by lower- and upper-case
boldface letters, respectively. For a matrix A, we denote its transpose as A⊤and its Hadamard
product with another matrix B as A ⊙B, i.e., the entrywise multiplication of A and B. We write
the set {1, 2, · · · , n} as [n]. We denote the probability and expectation of a given random variable
x as P(x) and E[x], respectively. The meaning of other notations can be inferred from the context."
ORGANIZATION,0.06444444444444444,"1.4
ORGANIZATION
The paper is organized as follows: In Sec. 2, we review diffusion equation on graphs and its con-
nection to GNNs. In Secs. 3 and 4, we brieﬂy review GRAND and present a random walk inter-
pretation of GRAND, respectively. Leveraging the random walk viewpoint of GRAND, we propose
GRAND++ for deep graph learning with theoretical guarantees in Sec. 5. We verify the efﬁcacy of
GRAND++ in Sec. 6. Technical proofs and more validations are provided in the appendix."
BACKGROUND,0.06666666666666667,"2
BACKGROUND
Diffusion equation on graphs.
Let G = (X, W ) represent an undirected graph with n nodes,
where X =
 
[x(1)]⊤, · · · , [x(n)]⊤⊤∈Rn×d with each row x(i) ∈Rd a feature vector and
W :=
 
Wij

a n × n matrix with Wij representing the similarity (edge weight) between the ith"
BACKGROUND,0.06888888888888889,"and jth feature vectors, and we assume Wij = Wji. Consider the following diffusion process that
evolves the feature matrix X on the graph (see Appendix A for a brief review of calculus on graphs): ∂X(t)"
BACKGROUND,0.07111111111111111,"∂t
= div
 
G(X(t), t) ⊙∇X(t)

,
(1)"
BACKGROUND,0.07333333333333333,"where X(t) =
 
[x(1)(t)]⊤, · · · , [x(n)(t)]⊤⊤∈Rn×d with x(i)(0) = x(i), ∇and div are the
gradient and divergence operators, respectively. The matrix G(X(t), t) is chosen such that W ⊙G
is right-stochastic, i.e., each row of W ⊙G summing to 1. In the machine learning setting, we can
parameterize G with learnable parameters θ which we denote by G(X(t), t, θ). The initial features
are evolved under the diffusion dynamics (1) from t = 0 to T to learn the ﬁnal representation X(T)
for further machine learning tasks."
BACKGROUND,0.07555555555555556,"In the simplest case when G(X(t), t) is only dependent on the initial node features X, i.e., G
is time-independent, right-stochasticity implies P"
BACKGROUND,0.07777777777777778,"j WijGij = 1 for all i, and so we focus on the
particular case when Gij = 1/di with di = Pn
j=1 Wij. In this case the right-hand side of (1)
reduces to the negative of the random-walk Laplacian applied to X(t) and (1) becomes ∂X(t)"
BACKGROUND,0.08,"∂t
= div
 
G(X(t), t) ⊙∇X(t)

= −LX(t),
(2)"
BACKGROUND,0.08222222222222222,"where L = I −D−1W := I −A (A := A(X)) is the random walk Laplacian and D is diagonal
with Dii = di. See [14, 25, 24] for more about random walk Laplacian and diffusion on graphs."
BACKGROUND,0.08444444444444445,"Graph neural networks. Applying forward Euler discretization, with step size δt < 1, of (2) gives"
BACKGROUND,0.08666666666666667,"X(kδt) = X((k −1)δt) −δtLX((k −1)δt) := ˜LX((k −1)δt), for k = 1, 2, · · · , K,
(3)"
BACKGROUND,0.08888888888888889,Published as a conference paper at ICLR 2022
BACKGROUND,0.09111111111111111,"T = Kδt, and X(0) = X. Note that the matrix ˜L is the discretization of the diffusion operator,
which is a special low-pass ﬁlter. Equation (3) is a prototype for motivating GNNs: by introducing
weights W (k) ∈Rd×d and a nonlinearity σ, e.g., ReLU, into (3), we have"
BACKGROUND,0.09333333333333334,"X((k + 1)δt) = σ
  ˜LX(kδt)W (k)
.
(4)"
BACKGROUND,0.09555555555555556,"The model in (4) is similar to the well-established GCN architecture proposed in [30]. The diffusive
nature of the GNN architecture in (4) further explains the over-smoothing issue of training deep
GNNs; the deeper the network architecture is, the more the node features diffuse. Eventually, all
nodes share similar features and become indistinguishable. See Sec. 5 for a detailed analysis."
A BRIEF REVIEW OF GRAND,0.09777777777777778,"3
A BRIEF REVIEW OF GRAND
GRAND is a new continuous-depth GNN proposed in [10]. It integrates a learnable encoder function
φ and a learnable decoder function ψ with the neural network parameterized graph diffusion process,
resulting in the prediction Y = ψ(X(T)), where X(T) is computed as"
A BRIEF REVIEW OF GRAND,0.1,"X(T) = X(0) +
Z T 0 ∂X(t)"
A BRIEF REVIEW OF GRAND,0.10222222222222223,"∂t
dt, with X(0) = φ(X),
(5)"
A BRIEF REVIEW OF GRAND,0.10444444444444445,"where ∂X(t)/∂t is given by the graph diffusion equation (2). From the neural ODE perspective, we
can perform forward propagation of GRAND, i.e., we solve (5), using numerical ODE solvers."
A BRIEF REVIEW OF GRAND,0.10666666666666667,"In the simplest case, when G is only dependent on the initial node features, we can rewrite (1) as ∂X(t)"
A BRIEF REVIEW OF GRAND,0.10888888888888888,"∂t
=
 
A(X) −I

X(t),
(6)"
A BRIEF REVIEW OF GRAND,0.1111111111111111,"GRAND models the diffusivity A(X) in (6) by the multi-head self-attention mechanism; potential
choices of the attention function include the ones proposed in [53, 54]. More precisely, in GRAND
A(X) =
1
h
Ph
l=1 Al(X) with h being the number of heads and the attention matrix Al(X) =
(al(xi, xj)), for l = 1, · · · , h, is computed as follows:"
A BRIEF REVIEW OF GRAND,0.11333333333333333,"al(xi, xj) =
exp
 
LeakyReLU(al⊤[W lxi∥W lxj])
 P"
A BRIEF REVIEW OF GRAND,0.11555555555555555,"k∈Ni exp
 
LeakyReLU(al⊤[W lxi∥W lxk])
,
(7)"
A BRIEF REVIEW OF GRAND,0.11777777777777777,"where W l and al are learned, ∥is the concatenation operator, and Ni is the index set of the nodes
that are connected to the ith node in the graph. GRAND with the attention in (7) is called GRAND-l,
that is, GRAND-l is a special case of GRAND when the diffusivity is dependent only on the initial
graph node features. Time-dependent attention and graph rewiring can be integrated into GRAND,
resulting in GRAND-nl and GRAND-nl-rw, respectively [10]. From the ODE viewpoint, GRAND
and its variants are a class of coupled neural ODEs deﬁned on an unweighted graph. Their merits
include continuous-depth and memory-efﬁcient training using the adjoint method [46, 13]."
RANDOM WALK VIEWPOINT OF GRAND,0.12,"4
RANDOM WALK VIEWPOINT OF GRAND"
RANDOM WALK VIEWPOINT OF GRAND,0.12222222222222222,"In this section, we present a random walk interpretation of GRAND. The connection between graph
random walks and the diffusion equation has been extensively studied, but we recap the key idea
here to motivate the new GRAND with a source term architecture. Let {B(i)(k)}k∈N be the random
walk on {x(j)(0)}n
j=1 deﬁned by, for δt ∈[0, 1],"
RANDOM WALK VIEWPOINT OF GRAND,0.12444444444444444,B(i)(0) = x(i)(0)
RANDOM WALK VIEWPOINT OF GRAND,0.12666666666666668,"P
 
B(i)(k + 1) = x(ℓ)(0)|B(i)(k) = x(j)(0)

=
 1 −δt
if ℓ= j
δtWjℓ"
RANDOM WALK VIEWPOINT OF GRAND,0.1288888888888889,"dj
if ℓ̸= j
(8)"
RANDOM WALK VIEWPOINT OF GRAND,0.13111111111111112,"where dj = Pn
ℓ=1 Wjℓ(assume Wℓℓ= 0 for all ℓ). Proposition 1 below is well-known, see [67].
We provide the proof of Proposition 1 and all the subsequent theoretical results in Appendix B."
RANDOM WALK VIEWPOINT OF GRAND,0.13333333333333333,"Proposition 1 Let X solve (3) and B(i) be the random walk determined by (8) where δt ∈[0, 1].
Then
x(i)(δtk) = E

B(i)(k)

."
RANDOM WALK VIEWPOINT OF GRAND,0.13555555555555557,Published as a conference paper at ICLR 2022
RANDOM WALK VIEWPOINT OF GRAND,0.13777777777777778,Proposition 2 below gives the stationary distribution of the random walk {B(i)(k)}k∈N.
RANDOM WALK VIEWPOINT OF GRAND,0.14,"Proposition 2 Assume the graph G = (X, W ) is connected. Then, the stationary distribution of
{B(i)(k)}k∈N is"
RANDOM WALK VIEWPOINT OF GRAND,0.14222222222222222,"π =

d1
Pn
j=1 dj
, . . . ,
dn
Pn
j=1 dj"
RANDOM WALK VIEWPOINT OF GRAND,0.14444444444444443,"
,
(9)"
RANDOM WALK VIEWPOINT OF GRAND,0.14666666666666667,which is independent of the starting position x(i).
RANDOM WALK VIEWPOINT OF GRAND,0.14888888888888888,"Furthermore, we have the following theoretical result on the asymptotic behavior of graph node
features under the GRAND dynamics given by (3)."
RANDOM WALK VIEWPOINT OF GRAND,0.1511111111111111,"Proposition 3 Assume the graph G = (X, W ) is connected. Then for all i = 1, · · · , n, we have"
RANDOM WALK VIEWPOINT OF GRAND,0.15333333333333332,"x(i)(kδt) →ex := n
X"
RANDOM WALK VIEWPOINT OF GRAND,0.15555555555555556,"j=1
x(j)(0)πj,
as k →∞."
RANDOM WALK VIEWPOINT OF GRAND,0.15777777777777777,"Hence, for the case of (3), i.e., GRAND-l, we expect the output to be approximately independent of
the input, due to over-smoothing. Of course, once we reintroduce the X(t) dependence back into G
in (1) and (2) or into the operator A in (6) then the above arguments no longer hold. Nevertheless,
the GRAND architectures are built on a principle that is ill-suited to deep networks. In the next
section we introduce a source term and perform a similar random walk analysis that illustrates how
the new architecture can be better suited for deep GNN architectures."
RANDOM WALK VIEWPOINT OF GRAND,0.16,"5
GRAND++: GRAPH NEURAL DIFFUSION WITH A SOURCE TERM"
ALGORITHM AND FORMULATION,0.1622222222222222,"5.1
ALGORITHM AND FORMULATION"
ALGORITHM AND FORMULATION,0.16444444444444445,"At the core of GRAND++ is the introduction of a source term into GRAND, leveraging the random
walk viewpoint of the diffusion process. We take a small subset of feature vectors, indexed by
I ⊆[n], believed to be “trustworthy” for use as a source term. In particular, we use the features of
labeled data. The GRAND++ dynamics are deﬁned by a diffusion equation with a source term (we
use the variable z for GRAND++-related dynamics and x for GRAND dynamics)"
ALGORITHM AND FORMULATION,0.16666666666666666,∂z(i)(t)
ALGORITHM AND FORMULATION,0.1688888888888889,"∂t
= div [G(Z(t), t) ⊙∇Z(t)](i) +
X"
ALGORITHM AND FORMULATION,0.1711111111111111,"j∈I
δijCj
(10)"
ALGORITHM AND FORMULATION,0.17333333333333334,where Cj is the source at feature vector of node j. Below we motivate a particular choice of Cj.
ALGORITHM AND FORMULATION,0.17555555555555555,"The key idea is to ﬁrst characterise the bias that arises from the diffusion and use that to propose
a correction via the choice of source terms Cj. Following the simpliﬁcations in (2), our diffusion
equation (without the source term) follows the approximate dynamics when t ≫1"
ALGORITHM AND FORMULATION,0.17777777777777778,∂x(i)(t)
ALGORITHM AND FORMULATION,0.18,"∂t
= −[LX(t)](i) = −x(i)(t)
| {z }
≈ex + 1 di n
X"
ALGORITHM AND FORMULATION,0.18222222222222223,"j=1
Wij x(j)(t)
| {z }
≈ex ≈0."
ALGORITHM AND FORMULATION,0.18444444444444444,"For i ∈I, it transpires that choosing Ci = x(i) −ˆx (where ˆx is deﬁned below) gives rise to
a random walk interpretation that allows us to prove that the oversmoothing seen in the GRAND
model is avoided."
ALGORITHM AND FORMULATION,0.18666666666666668,"One can in fact choose ex with a certain degree of freedom. If we initialise X(0) = X then we obtain
ex = Pn
j=1 x(j)πj (as is usual in the GRAND model). However, as the similarities are encoded in
the graph weights, and the diffusion dynamics will drive it towards a non-trivial state, we can choose
a different initialization than X(0) = X. Through connections with random walks we, in the next
subsection, motivate an alternative initialisation n
X"
ALGORITHM AND FORMULATION,0.18888888888888888,"i=1
z(i)(0) =
X i∈I"
ALGORITHM AND FORMULATION,0.19111111111111112,x(i) −ˆx
ALGORITHM AND FORMULATION,0.19333333333333333,"di
, where ˆx = 1 |I| X"
ALGORITHM AND FORMULATION,0.19555555555555557,"j∈I
x(j)
(11)"
ALGORITHM AND FORMULATION,0.19777777777777777,Published as a conference paper at ICLR 2022
ALGORITHM AND FORMULATION,0.2,with the dynamics
ALGORITHM AND FORMULATION,0.20222222222222222,∂z(i)(t)
ALGORITHM AND FORMULATION,0.20444444444444446,"∂t
= div [G(Z(t), t) ⊙∇Z(t)](i) +
X"
ALGORITHM AND FORMULATION,0.20666666666666667,"j∈I
δij

x(i) −ˆx

.
(12)"
ALGORITHM AND FORMULATION,0.2088888888888889,"For example, we could choose"
ALGORITHM AND FORMULATION,0.2111111111111111,"z(i)(0) =

1
di
 
x(i) −ˆx

if i ∈I
0
otherwise,
or z(i)(0) = x(i) −c,"
ALGORITHM AND FORMULATION,0.21333333333333335,where c = 1
ALGORITHM AND FORMULATION,0.21555555555555556,"n
 Pn
i=1 x(i)−P"
ALGORITHM AND FORMULATION,0.21777777777777776,"j∈I
x(j)−ˆx dj"
ALGORITHM AND FORMULATION,0.22,"
is chosen such that (11) holds. We do not believe that the
constant c (that shifts by a constant) is particularly important but it is included to provide a random
walk interpretation which helps to understand the deep architecture (when T is big) behaviour of
the model GRAND++. The justiﬁcation for this choice will be made in Sec. 5.2. To summarize, the
GRAND++ model in (12), with initial condition satisfying (11), simply adds a source term to the
original GRAND model and uses a different initial condition. Therefore, the nonlinear diffusivity
and graph rewiring tricks used by GRAND can be easily integrated into GRAND++. In terms of
implementation, since GRAND++ merely changes the right-hand side of GRAND, which again can
be regarded as a system of coupled ﬁrst-order neural ODEs; we can leverage neural ODE training,
testing, and inference for GRAND++ similar to GRAND."
ALGORITHM AND FORMULATION,0.2222222222222222,"In the next subsection we explore the random walk connection of the above model, suggesting that
building a graph neural network based on the diffusion with source model does not suffer from the
same degeneracy as we observed in Sec. 4 and is therefore better suited to build deep GNNs. In
particular, we can write the diffusion with source model as the short time expected behaviour of a
random walk and therefore we do not have the issue of reaching the stationary state (in other words
passing the mixing time). Our experiments in Sec. 6 suggest the formal motivation holds and we are
able to design deep GNNs."
ALGORITHM AND FORMULATION,0.22444444444444445,"5.2
THE RANDOM WALK PERSPECTIVE OF GRAND++"
ALGORITHM AND FORMULATION,0.22666666666666666,"Let us continue to consider the simpliﬁed model in the previous subsection, i.e., assume the dynam-
ics are governed by
∂z(i)(t)"
ALGORITHM AND FORMULATION,0.2288888888888889,"∂t
= −[LZ(t)](i) +
X"
ALGORITHM AND FORMULATION,0.2311111111111111,"j∈I
δij

x(i) −ˆx

(13)"
ALGORITHM AND FORMULATION,0.23333333333333334,"where the initial condition satisﬁes (11). Using the forward Euler discretisation of the above dynam-
ics we have"
ALGORITHM AND FORMULATION,0.23555555555555555,"z(i)(δtk) = z(i)(δt(k −1)) −δt [LZ(δt(k −1))](i) + δt
X"
ALGORITHM AND FORMULATION,0.23777777777777778,"j∈I
δij

x(i) −ˆx

,
(14)"
ALGORITHM AND FORMULATION,0.24,"for k = 1, 2, . . . , K where again T = Kδt."
ALGORITHM AND FORMULATION,0.24222222222222223,"We use the same random walk as that introduced in Sec. 4, i.e. the random walk deﬁned by (8), but
we will now only consider random walks that are initialised on the nodes indexed by I."
ALGORITHM AND FORMULATION,0.24444444444444444,"Proposition 4 Let Z solve (14) with the initial condition satisfying (11), and let B(i) be the random
walk determined by (8). Then,
z(i)(kδt) −E

k
X s=0"
DI,0.24666666666666667,"1
di X j∈I"
DI,0.24888888888888888,"
x(j) −ˆx

1B(j)(s)=x(i)
 →0 as k →∞."
DI,0.2511111111111111,Remark 1 In the limit k →∞the term
DI,0.25333333333333335,"E

k
X s=0"
DI,0.25555555555555554,"1
di X"
DI,0.2577777777777778,"j∈I
x(j)1B(j)(s)=x(i)
"
DI,0.26,"is formally a function of the random walk at all times. Whilst if k is very large (i.e. in comparison to
the mixing time) we still have that E
 1 di X"
DI,0.26222222222222225,"j∈I
x(j)1B(j)(k)=x(i)

= 1 di X"
DI,0.2644444444444444,"j∈I
x(j) P

B(j)(k) = x(j)"
DI,0.26666666666666666,"|
{z
}
≈πi ≈πi di X"
DI,0.2688888888888889,"j∈I
x(j)
(15)"
DI,0.27111111111111114,Published as a conference paper at ICLR 2022
DI,0.2733333333333333,"and on the other hand E
 1 di X"
DI,0.27555555555555555,"j∈I
ˆx1B(j)(k)=x(i)

= ˆx di X"
DI,0.2777777777777778,"j∈I
P

B(j)(k) = x(j)"
DI,0.28,"|
{z
}
≈πi"
DI,0.2822222222222222,≈πi|I|ˆx
DI,0.28444444444444444,"di
.
(16)"
DI,0.2866666666666667,"From the deﬁnition of ˆx we see that (15) and (16) are approximately equal. And therefore we can
understand E[ 1 di
P"
DI,0.28888888888888886,"j∈I ˆx1B(j)(k)=x(i)] as the long time behaviour of E[ 1 di
P"
DI,0.2911111111111111,"j∈I x(j)1B(j)(k)=x(i)]
Very formally we can see that subtracting the long-time behaviour from the all-time behaviour leaves
us with the short time behaviour. This provides one explanation as to why we do not expect the deep
layers to be determined by the stationary state of the random walk (at which point there is little
dependence on the initial layers, causing the deep layers to be approximately constant)."
DI,0.29333333333333333,Remark 2 The random walk interpretation
DI,0.29555555555555557,"E

k
X s=0"
DI,0.29777777777777775,"1
di X j∈I"
DI,0.3,"
x(j) −ˆx

1B(j)=x(i)

(17)"
DI,0.3022222222222222,"can be considered to be dual to the random walk interpretation in Sec. 4: in Sec. 4 we released
the random walker from the node of interest, whilst now we release the random walkers from nodes
indexed by I and see how many of them hit the node of interest. We note also that we do not require
a lower bound on the size of the set I. Indeed, if |I| is ﬁxed whilst one takes the number of feature
vectors n →∞we still expect many properties of GRAND++, in particular Proposition 5 below, to
hold. This is due to the asymptotic well-posedness of the dual random walk in low labeling rates [9]."
DI,0.30444444444444446,"Proposition 3 reveals that in the simple setting of (2), GRAND converges to a constant when its
depth goes to inﬁnity. However, this is not true for GRAND++ since the graph node features will
not converge to a constant vector driven by the GRAND++, as shown in Proposition 5 below."
DI,0.30666666666666664,"Proposition 5 Assume the graph G = (X, W ) is connected. Then z(i)(kδt) that was deﬁned in
(14) does not converge to a constant vector as a function of i as k →∞. That is, the node features
will not become the same across graph nodes under the GRAND++ dynamics."
DI,0.3088888888888889,"Remark 3 Proposition 5 guarantees GRAND++ is less likely to suffer from over-smoothing than
GRAND, and in particular it shows that we have a non-constant deep layer limit, i.e., as t →
∞. Analysing the limit is beyond the scope of the paper but we have seen one characterisation in
Proposition 4. By construction we have ∂z(i)(t)/∂t ≈0 for i ∈I so one should expect that the
deep layer limit is (close to) a smooth interpolation of the feature vectors labeled by I."
DI,0.3111111111111111,"The continuous time model (10) is, in the special case of (13), the mean ﬁeld limit of the probabilistic
formulation (17). Our proposed algorithm is formulated from the mean-ﬁeld limit."
EXPERIMENTS,0.31333333333333335,"6
EXPERIMENTS"
EXPERIMENTS,0.31555555555555553,"In this section, we compare the performance of GRAND++ with GRAND and several other popu-
lar GNNs on various graph node classiﬁcation tasks. We aim to show the practical advantages of
GRAND++ in learning with limited labeled data and using deep architectures. Without mentioning
clearly, we use the same hyperparameters that that used for GRAND in [10] for GRAND++. We
provide detailed descriptions of experimental settings and datasets that are omitted in the main text
in Appendix D.1. For all experiments, we run 100 splits for each dataset with 20 random seeds for
each split, which are conducted on a server with four NVIDIA RTX 3090 graphics cards."
EXPERIMENTS,0.31777777777777777,"We compare the performance of GRAND++ and its nonlinear and graph rewiring variants with sev-
eral popular GNNs on various graph node classiﬁcation benchmarks. Except for the integration time,
which measures the implicit depth of GRAND and GRAND++, we adopt the experimental settings
of GRAND in [10] for GRAND++ include numerical differential equation solvers. Following [10],
we study seven graph node classiﬁcation datasets, namely CORA, CiteSeer, PubMed, CoauthorCS,
Computer, Photo, and ogbn-arxiv; we describe these datasets in Appendix D.1."
EXPERIMENTS,0.32,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.32222222222222224,"6.1
GRAND++ IS MORE RESILIENT TO DEEP ARCHITECTURES"
EXPERIMENTS,0.3244444444444444,"We ﬁrst show that our introduced source term in (12) can improve the accuracy of GRAND-l when
the architecture is deep, i.e., the integration time T in (5) is big. We denote GRAND-l with the source
term as GRAND++-l. For each node classiﬁcation task, we train all models using the same number
of labeled nodes as in [10]. Figure 2 contrasts the performance of GRAND-l and GRAND++-l
with different depths, or T, on CORA, CiteSeer, Computer, and Photo datasets. We provide the
detailed results on PubMed and CoauthorCS, together with more comparisons of GRAND++-l with
GRAND-l and several other celebrated GNNs include GCN, GAT, and GraphSage in Table 5 in
Appendix D.2. The results in Fig. 2 and Appendix D.2 conﬁrm that GRAND-l suffers less from
over-smoothing compared to GCN, GAT, and GraphSage. Moreover, GRAND++-l performs on
par with GRAND-l when the depth (T) of the network is small, but GRAND++-l signiﬁcantly
outperforms GRAND-l when T is large. As T increases, the margin becomes wider, indicating
that GRAND++-l can overcome over-smoothing much more effectively than GRAND-l. Note that
we did not use uniform depth for GRAND-l and GRAND++-l on all datasets because the adaptive
step-size ODE solver fails when T is large for some tasks."
EXPERIMENTS,0.32666666666666666,"16
64
128
256
Depth (T) 60 65 70 75 80 85"
EXPERIMENTS,0.3288888888888889,Accuracy (%)
EXPERIMENTS,0.33111111111111113,"GRAND++-l
GRAND-l"
EXPERIMENTS,0.3333333333333333,"4
16
64
Depth (T) 66 68 70 72 74 76 78"
EXPERIMENTS,0.33555555555555555,Accuracy (%)
EXPERIMENTS,0.3377777777777778,"GRAND++-l
GRAND-l"
EXPERIMENTS,0.34,"1
4
16
32
Depth (T) 65 70 75 80 85"
EXPERIMENTS,0.3422222222222222,Accuracy (%)
EXPERIMENTS,0.34444444444444444,"GRAND++
GRAND"
EXPERIMENTS,0.3466666666666667,"1
4
16
32
Depth (T) 88 89 90 91 92 93 94"
EXPERIMENTS,0.3488888888888889,Accuracy (%)
EXPERIMENTS,0.3511111111111111,"GRAND++
GRAND"
EXPERIMENTS,0.35333333333333333,"CORA
CiteSeer
Computer
Photo
Figure 2: Test accuracy vs. the “depth” (T in (5)) of GRAND-l and GRAND++-l on the four graph node
classiﬁcation tasks. We see that GRAND++-l is much more resilient to deep architectures than GRAND-l.
These results show that GRAND++ is better suited for learning with a very deep architecture than GRAND."
EXPERIMENTS,0.35555555555555557,"Next, we compare GRAND-l and GRAND++-l on the ogbn-arxiv node classiﬁcation task, which
is a large-scale benchmark.
We train two models using labeling rates of 3.0% and 5.0%, re-
spectively; the corresponding test accuracy for GRAND-l/GRAND++-l are 65.26%/66.64% and
67.42%/67.77%, respectively. GRAND++-l outperforms GRAND-l in both labeling rates.We fur-
ther compare GRAND and GRAND++ with different depth on the ogbn-arxiv task in Appendix D.6."
EXPERIMENTS,0.35777777777777775,"6.2
GRAND++ IS MORE ACCURATE WITH LIMITED LABELED TRAINING DATA"
EXPERIMENTS,0.36,"4 16
64
128
Depth (T) 25 30 35 40 45 50 55 60 65"
EXPERIMENTS,0.3622222222222222,Accuracy (%)
EXPERIMENTS,0.36444444444444446,"GRAND++-l
GRAND-l"
EXPERIMENTS,0.36666666666666664,"1 4
16
64
Depth (T) 35 40 45 50 55 60 65"
EXPERIMENTS,0.3688888888888889,Accuracy (%)
EXPERIMENTS,0.3711111111111111,"GRAND++-l
GRAND-l"
EXPERIMENTS,0.37333333333333335,"CORA
CiteSeer
Figure 3: Accuracy of GRAND++-l and GRAND-l for
CORA and CiteSeer, where both models, with different
depth (T), are train with 1 labeled node per class. These
results show that GRAND++ is more effective in learn-
ing with low-labeling rates than GRAND."
EXPERIMENTS,0.37555555555555553,"Besides helping to overcome over-smoothing,
our theory shows that the source term can
boost the accuracy of GRAND-l with low-
labeling rates. Table 1 compares the accuracy
of GRAND++-l with GRAND-l, GCN, GAT,
GraphSage, and MoNet, trained with different
numbers of labeled data. Here, we slightly tune
T for GRAND++ based on the optimal value
for GRAND, see Table 4 in the Appendix for
their values. We see that with few labeled data,
in most tasks GRAND++-l is signiﬁcantly more
accurate than the other GNNs include GRAND-
l, conﬁrming our theoretical insight. For Coau-
thorCS task, both GRAND-l and GRAND++-l are worse than GCN and GraphSage. Moreover,
increasing the depth of GRAND++-l can improve the classiﬁcation accuracy with limited training
data, but this is not the case for GRAND-l, see Fig. 3. We perform the t-test in Appendix D.5 to
conﬁrm the statistical signiﬁcance of the accuracy gain of GRAND++ over GRAND in Table 1."
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.37777777777777777,"6.3
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING
The previous experimental results show that GRAND++-l enhances the accuracy of GRAND-l in the
cases when the labeled training data is limited and when the network is deep. Here, we explore the
same strategy for GRAND-nl and GRAND-nl-rw; we name the corresponding models with the new
source term GRAND++-nl and GRAND++-nl-rw, respectively. Table 2 compares GRAND-nl and"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.38,Published as a conference paper at ICLR 2022
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.38222222222222224,"Model
#per class
CORA
CiteSeer
PubMed
CoauthorCS
Computer
Photo"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.3844444444444444,"1
54.94 ± 16.09
58.95 ± 9.59
65.94 ± 4.87
60.30 ± 1.50
67.65 ± 0.37
83.12 ± 0.78
2
66.92 ± 10.04
64.98 ± 8.31
69.31 ± 4.87
76.53 ± 1.85
76.47 ± 1.48
83.71 ± 0.90
GRAND++-l
5
77.80 ± 4.46
70.03 ± 3.63
71.99 ± 1.91
84.83 ± 0.84
82.64 ± 0.56
88.33 ± 1.21
(ours)
10
80.86 ± 2.99
72.34 ± 2.42
75.13 ± 3.88
86.94 ± 0.46
82.99 ± 0.81
90.65 ± 1.19
20
82.95 ± 1.37
73.53 ± 3.31
79.16 ± 1.37
90.80 ± 0.34
85.73 ± 0.50
93.55 ± 0.38"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.38666666666666666,"1
52.53 ± 16.40
50.06 ± 17.98
62.11 ± 10.58
59.15 ± 5.73
48.67 ± 1.66
81.25 ± 2.50
2
64.82 ± 11.16
59.55 ± 10.89
69.00 ± 7.55
73.83 ± 5.58
74.77 ± 1.85
82.13 ± 3.27
GRAND-l
5
76.07 ± 5.08
68.37 ± 5.00
73.98 ± 5.08
85.29 ± 2.19
80.72 ± 1.09
88.27 ± 1.94
[10]
10
80.25 ± 3.40
71.90 ± 7.66
76.33 ± 3.41
87.81 ± 1.36
82.42 ± 1.10
90.98 ± 0.93
20
82.86 ± 2.39
73.02 ± 5.89
78.76 ± 1.69
91.03 ± 0.47
84.54 ± 0.90
93.53 ± 0.47"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.3888888888888889,"1
47.72 ± 15.33
48.94 ± 10.24
58.61 ± 12.83
65.22 ± 2.25
49.46 ± 1.65
82.94 ± 2.17
2
60.85 ± 14.01
58.06 ± 9.76
60.45 ± 16.20
83.61 ± 1.49
76.90 ± 1.49
83.61 ± 0.71
GCN [30]
5
73.86 ± 7.97
67.24 ± 4.19
68.69 ± 7.93
86.66 ± 0.43
82.47 ± 0.97
88.86 ± 1.56
10
78.82 ± 5.38
72.18 ± 3.47
72.59 ± 3.19
88.60 ± 0.50
82.53 ± 0.74
90.41 ± 0.35
20
82.07 ± 2.03
74.21 ± 2.90
76.89 ± 3.27
91.09 ± 0.35
82.94 ± 1.54
91.95 ± 0.11"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.39111111111111113,"1
47.86 ± 15.38
50.31 ± 14.27
58.84 ± 12.81
51.13 ± 5.24
37.14 ± 7.81
73.58 ± 8.15
2
58.30 ± 13.55
55.55 ± 9.19
60.24 ± 14.44
63.12 ± 6.09
65.07 ± 8.86
76.89 ± 4.89
GAT [54]
5
71.04 ± 5.74
67.37 ± 5.08
68.54 ± 5.75
71.65 ± 4.53
71.43 ± 7.34
83.01 ± 3.64
10
76.31 ± 4.87
71.35 ± 4.92
72.44 ± 3.50
74.71 ± 3.35
76.04 ± 0.35
87.42 ± 2.38
20
79.92 ± 2.28
73.22 ± 2.90
75.55 ± 4.11
79.95 ± 2.88
80.05 ± 1.81
89.38 ± 2.48"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.3933333333333333,"1
43.04 ± 14.01
48.81 ± 11.45
55.53 ± 12.71
61.35 ± 1.35
27.65 ± 2.39
45.36 ± 7.13
2
53.96 ± 12.18
54.39 ± 11.37
58.97 ± 12.65
76.51 ± 1.31
42.63 ± 4.29
51.93 ± 4.21
GraphSage
5
68.14 ± 6.95
64.79 ± 5.16
66.07 ± 6.16
89.06 ± 0.69
64.83 ± 1.62
78.26 ± 1.93
[29]
10
75.04 ± 5.03
68.90 ± 5.08
70.74 ± 3.11
89.68 ± 0.39
74.66 ± 1.29
84.38 ± 1.75
20
80.04 ± 2.54
72.02 ± 2.82
74.55 ± 3.09
91.33 ± 0.36
79.98 ± 0.96
91.29 ± 0.67"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.39555555555555555,"1
47.72 ± 15.53
39.13 ± 11.37
56.47 ± 4.67
58.99 ± 5.17
23.78 ± 7.57
34.72 ± 8.18
MoNet
2
60.85 ± 14.01
48.52 ± 9.52
61.03 ± 6.93
76.57 ± 4.06
38.19 ± 3.72
43.03 ± 8.22
[40]
5
73.86 ± 7.97
61.66 ± 6.61
67.92 ± 2.50
87.02 ± 1.67
59.38 ± 4.73
71.80 ± 5.02
10
78.82 ± 5.38
68.08 ± 6.29
71.24 ± 1.54
88.76 ± 0.49
68.66 ± 3.30
78.66 ± 3.17
20
82.07 ± 2.03
71.52 ± 4.11
76.49 ± 1.75
90.31 ± 0.41
73.66 ± 2.87
88.61 ± 1.18"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.3977777777777778,"Table 1: Classiﬁcation accuracy of different GNNs trained with different number of labeled data per class
(#per class) on six benchmark graph node classiﬁcation tasks. The highest accuracy is highlighted in bold for
each number of labeled data per class. These results show that GRAND++ is more effective in learning with
low-labeling rates than GRAND. (Unit: %)"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.4,"GRAND-nl-rw with the corresponding model with a source term. We see that overall GRAND++-nl
(GRAND++-nl-rw) outperforms GRAND-nl (GRAND-nl-rw) when the network is deep, i.e., T is
big. We further study the low-labeling rate regimes in Appendix D.3."
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.4022222222222222,"Model
Depth (T )
GRAND-nl [10]
GRAND-nl-rw [10]
GRAND++-nl (ours)
GRAND++-nl-rw (ours)"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.40444444444444444,"CORA
1
79.70 ± 1.88
79.07 ± 3.05
79.24 ± 1.48
79.24 ± 1.48
4
82.31 ± 0.91
82.47 ± 1.32
82.64 ± 0.89
82.23 ± 1.14
16
82.11 ± 1.42
82.05 ± 1.31
83.24 ± 0.20
81.48 ± 1.07
32
79.42 ± 0.64
81.01 ± 0.81
81.21 ± 0.37
82.20 ± 1.15"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.4066666666666667,"CiteSeer
1
71.84 ± 2.98
71.84 ± 2.66
70.45 ± 2.12
71.74 ± 1.37
16
72.65 ± 2.42
73.06 ± 2.98
72.48 ± 1.10
73.29 ± 1.37
64
70.29 ± 2.58
69.65 ± 2.50
72.64 ± 0.93
73.38 ± 0.95
128
65.19 ± 6.77
65.45 ± 7.18
74.24 ± 0.70
74.23 ± 0.70"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.4088888888888889,"PubMed
1
77.93 ± 1.27
77.93 ± 1.26
78.01 ± 0.68
78.01 ± 0.68
4
77.95 ± 1.28
78.02 ± 1.14
78.41 ± 0.88
78.17 ± 0.93
16
76.51 ± 2.73
76.88 ± 2.57
78.43 ± 0.78
78.12 ± 0.87"
TIME-DEPENDENT ATTENTION AND GRAPH REWIRING,0.4111111111111111,"Table 2: Classiﬁcation accuracy of GRAND and GRAND++ variants of different depth trained 20 labeled data
per class. The highest accuracy is highlighted in bold for each of the depths T = 1, 4, 16, 32, 64, and 128. We
test T only up to 16 for PubMed and up to 32 for 32 since the neural ODE solver failed for larger T. (Unit: %)"
CONCLUDING REMARKS,0.41333333333333333,"7
CONCLUDING REMARKS"
CONCLUDING REMARKS,0.41555555555555557,"We propose GRAND++, which augments graph neural diffusion with a source term. We present
some theory that connects the model to a random walk formulation on graphs. GRAND++ out-
performs many existing GNNs for graph deep learning with very deep architectures and when the
number of labeled data is limited. GRAND++ can be regarded as coupled ODE system in which
each ODE has an external force term. As such, it is natural to consider if advanced techniques in
accelerating training, test, and inference of neural ODEs can be leveraged to improve the efﬁciency
and accuracy of GRAND++, in particular high-order neural ODEs [19, 61, 42, 59] and noise injec-
tion [55]. It is interesting to note that the second-order neural ODE can be connected to the wave
equation in the graph setting, which can automatically bypass over-smoothing. We leave studying
the second-order neural ODE on graphs as future work."
CONCLUDING REMARKS,0.4177777777777778,Published as a conference paper at ICLR 2022
ACKNOWLEDGEMENT,0.42,"8
ACKNOWLEDGEMENT"
ACKNOWLEDGEMENT,0.4222222222222222,"This material is based on research sponsored by NSF grants DMS-1924935, DMS-1952339, DMS-
2027248 and NSF CCF-1934568, DOE grant DE-SC0021142, and ONR grant N00014-18-1-2527
and the MURI grant N00014-20-1-2787. MT would like to thank the Isaac Newton Institute for
Mathematical Sciences for support and hospitality during the programme Mathematics of Deep
Learning when work on this paper was undertaken (EPSRC grant number EP/R014604/1) and ac-
knowledge support from the European Union Horizon 2020 research and innovation programmes
under the Marie Skłodowska-Curie grant agreement No. 777826 (NoMADS). MT also holds a Tur-
ing Fellowship at the Alan Turing Institute."
REFERENCES,0.42444444444444446,REFERENCES
REFERENCES,0.4266666666666667,"[1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical impli-
cations. In International Conference on Learning Representations, 2021."
REFERENCES,0.4288888888888889,"[2] James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in
neural information processing systems, pages 1993–2001, 2016."
REFERENCES,0.4311111111111111,"[3] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction
networks for learning about objects, relations and physics. In Advances in Neural Information
Processing Systems, pages 4502–4510, 2016."
REFERENCES,0.43333333333333335,"[4] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data
representation. Neural Computation, 15(6):1373–1396, 2003."
REFERENCES,0.43555555555555553,"[5] Mikhail Belkin and Partha Niyogi. Semi-supervised learning on Riemannian manifolds. Ma-
chine learning, 56(1-3):209–239, 2004."
REFERENCES,0.43777777777777777,"[6] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learn-
ing: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021."
REFERENCES,0.44,"[7] Antoni Buades, Bartomeu Coll, and Jean M. Morel. Neighborhood ﬁlters and PDE’s. Nu-
merische Mathematik, 105(1):1–34, 2006."
REFERENCES,0.44222222222222224,"[8] Jeff Calder. The game theoretic p-Laplacian and semi-supervised learning with few labels.
Nonlinearity, 32(1), 2018."
REFERENCES,0.4444444444444444,"[9] Jeff Calder, Brendan Cook, Matthew Thorpe, and Dejan Slepcev. Poisson learning: Graph
based semi-supervised learning at very low label rates. In Hal Daum´e III and Aarti Singh,
editors, Proceedings of the 37th International Conference on Machine Learning, volume 119
of Proceedings of Machine Learning Research, pages 1306–1316. PMLR, 13–18 Jul 2020."
REFERENCES,0.44666666666666666,"[10] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb,
and Emanuele Rossi. GRAND: Graph neural diffusion. In Marina Meila and Tong Zhang,
editors, Proceedings of the 38th International Conference on Machine Learning, volume 139
of Proceedings of Machine Learning Research, pages 1407–1418. PMLR, 18–24 Jul 2021."
REFERENCES,0.4488888888888889,"[11] Ines Chami, Zhitao Ying, Christopher R´e, and Jure Leskovec. Hyperbolic graph convolutional
neural networks. Advances in neural information processing systems, 32:4868–4879, 2019."
REFERENCES,0.45111111111111113,"[12] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the
over-smoothing problem for graph neural networks from the topological view. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3438–3445, 2020."
REFERENCES,0.4533333333333333,"[13] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Cur-
ran Associates, Inc., 2018."
REFERENCES,0.45555555555555555,"[14] Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Math-
ematical Soc., 1997."
REFERENCES,0.4577777777777778,Published as a conference paper at ICLR 2022
REFERENCES,0.46,"[15] Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Frederick
Warner, and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis and struc-
ture deﬁnition of data: Diffusion maps. Proceedings of the National Academy of Sciences,
102(21):7426–7431, 2005."
REFERENCES,0.4622222222222222,"[16] Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral ﬁltering. Advances in neural information processing
systems, 29:3844–3852, 2016."
REFERENCES,0.46444444444444444,"[17] Xavier Desquesnes, Abderrahim Elmoataz, and L´ezoray, Olivier. Eikonal equation adaptation
on weighted graphs: fast geometric diffusion process for local and non-local image and data
processing. Journal of Mathematical Imaging and Vision, 46(2):238–257, 2013."
REFERENCES,0.4666666666666667,"[18] John R Dormand and Peter J Prince. A family of embedded Runge-Kutta formulae. Journal
of Computational and Applied Mathematics, 6(1):19–26, 1980."
REFERENCES,0.4688888888888889,"[19] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.4711111111111111,"[20] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel,
Alan Aspuru-Guzik, and Ryan P Adams.
Convolutional networks on graphs for learning
molecular ﬁngerprints. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates,
Inc., 2015."
REFERENCES,0.47333333333333333,"[21] Moshe Eliasof, Eldad Haber, and Eran Treister. PDE-GCN: Novel architectures for graph
neural networks motivated by partial differential equations. arXiv preprint arXiv:2108.01938,
2021."
REFERENCES,0.47555555555555556,"[22] Abderrahim Elmoataz, Olivier Lezoray, and S´ebastien Bougleux. Nonlocal discrete regulariza-
tion on weighted graphs: A framework for image and manifold processing. IEEE Transactions
on Image Processing, 17(7):1047–1060, 2008."
REFERENCES,0.4777777777777778,"[23] Francesco Farina and Emma Slade. Data efﬁciency in graph networks through equivariance.
arXiv preprint arXiv:2106.13786, 2021."
REFERENCES,0.48,"[24] Mark Freidlin and Shuenn-Jyi Sheu. Diffusion processes on graphs: stochastic differential
equations, large deviation principle. Probability theory and related ﬁelds, 116(2):181–220,
2000."
REFERENCES,0.4822222222222222,"[25] Mark I. Freidlin and Alexander D. Wentzell. Diffusion Processes on Graphs and the Averaging
Principle. The Annals of Probability, 21(4):2215 – 2245, 1993."
REFERENCES,0.48444444444444446,"[26] Cristina Garcia-Cardona, Ekaterina Merkurjev, Andrea L. Bertozzi, Arjuna Flenner, and Al-
lon G. Percus. Multiclass data segmentation using diffuse interface methods on graphs. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 36(8):1600–1613, 2014."
REFERENCES,0.4866666666666667,"[27] Guy Gilboa and Stanley Osher. Nonlocal operators with applications to image processing.
Multiscale Modeling & Simulation, 7(3):1005–1028, 2008."
REFERENCES,0.4888888888888889,"[28] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.
Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh, edi-
tors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pages 1263–1272. PMLR, 06–11 Aug 2017."
REFERENCES,0.4911111111111111,"[29] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017."
REFERENCES,0.49333333333333335,"[30] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional
networks. In Proceedings of the 5th International Conference on Learning Representations,
ICLR ’17, 2017."
REFERENCES,0.4955555555555556,Published as a conference paper at ICLR 2022
REFERENCES,0.49777777777777776,"[31] Johannes Klicpera, Stefan Weißenberger, and Stephan G¨unnemann. Diffusion improves graph
learning. Advances in Neural Information Processing Systems, 32:13354–13366, 2019."
REFERENCES,0.5,"[32] Guohao Li, Matthias M¨uller, Bernard Ghanem, and Vladlen Koltun. Training graph neural
networks with 1000 layers. arXiv preprint arXiv:2106.07476, 2021."
REFERENCES,0.5022222222222222,"[33] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as
deep as cnns? In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 9267–9276, 2019."
REFERENCES,0.5044444444444445,"[34] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to
train deeper gcns. arXiv preprint arXiv:2006.07739, 2020."
REFERENCES,0.5066666666666667,"[35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional net-
works for semi-supervised learning. In Thirty-Second AAAI conference on artiﬁcial intelli-
gence, 2018."
REFERENCES,0.5088888888888888,"[36] Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. Lanczosnet: Multi-scale
deep graph convolutional networks. In International Conference on Learning Representations,
2019."
REFERENCES,0.5111111111111111,"[37] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. Advances
in Neural Information Processing Systems, 32:8230–8241, 2019."
REFERENCES,0.5133333333333333,"[38] Franc¸ois Lozes, Abderrahim Elmoataz, and Olivier L´ezoray. Partial difference operators on
weighted graphs for image processing on surfaces and point clouds. IEEE Transactions on
Image Processing, 23(9):3896–3909, 2014."
REFERENCES,0.5155555555555555,"[39] Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dis-
secting neural odes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems, volume 33, pages 3952–3963.
Curran Associates, Inc., 2020."
REFERENCES,0.5177777777777778,"[40] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and
Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model
cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
5425–5434, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society."
REFERENCES,0.52,"[41] Federico Monti, Michael M Bronstein, and Xavier Bresson. Geometric matrix completion with
recurrent multi-graph neural networks. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 3700–3710, 2017."
REFERENCES,0.5222222222222223,"[42] Alexander Norcliffe, Cristian Bodnar, Ben Day, Nikola Simidjievski, and Pietro Li´o. On sec-
ond order behaviour in augmented neural odes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
pages 5911–5921. Curran Associates, Inc., 2020."
REFERENCES,0.5244444444444445,"[43] Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass
ﬁlters. arXiv preprint arXiv:1905.09550, 2019."
REFERENCES,0.5266666666666666,"[44] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
node classiﬁcation. In International Conference on Learning Representations, 2020."
REFERENCES,0.5288888888888889,"[45] Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, and
Jinkyoo Park. Graph neural ordinary differential equations. arXiv preprint arXiv:1911.07532,
2019."
REFERENCES,0.5311111111111111,"[46] Lev Semenovich Pontryagin. Mathematical theory of optimal processes. CRC press, 1987."
REFERENCES,0.5333333333333333,"[47] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. Deepinf:
Social inﬂuence prediction with deep learning. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pages 2110–2119, 2018."
REFERENCES,0.5355555555555556,Published as a conference paper at ICLR 2022
REFERENCES,0.5377777777777778,"[48] Mauricio Flores Rios, Jeff Calder, and Gilad Lerman. Algorithms for lp-based semi-supervised
learning on graphs. arXiv preprint arXiv:1901.05031, 2019."
REFERENCES,0.54,"[49] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang.
Dropedge: Towards deep
graph convolutional networks on node classiﬁcation. In International Conference on Learning
Representations, 2020."
REFERENCES,0.5422222222222223,"[50] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear
embedding. science, 290(5500):2323–2326, 2000."
REFERENCES,0.5444444444444444,"[51] Yulia Rubanova, Ricky T. Q. Chen, and David K Duvenaud.
Latent ordinary differential
equations for irregularly-sampled time series. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.5466666666666666,"[52] Zuoqiang Shi, Stanley J. Osher, and Wei. Zhu. Weighted nonlocal Laplacian on interpolation
from sparse data. Journal of Scientiﬁc Computing, 73(2-3):1164–1177, 2017."
REFERENCES,0.5488888888888889,"[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.5511111111111111,"[54] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and
Yoshua Bengio. Graph attention networks. In International Conference on Learning Rep-
resentations, 2018."
REFERENCES,0.5533333333333333,"[55] Bao Wang, Binjie Yuan, Zuoqiang Shi, and Stanley Osher. Resnets ensemble via the Feynman-
Kac formalism to improve natural and robust accuracies. In Advances in Neural Information
Processing Systems, pages 1655–1665, 2019."
REFERENCES,0.5555555555555556,"[56] Fei Wang, Changshui Zhang, Helen C Shen, and Jingdong Wang. Semi-supervised classiﬁca-
tion using linear neighborhood propagation. In 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’06), volume 1, pages 160–167. IEEE, 2006."
REFERENCES,0.5577777777777778,"[57] Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process
in linear graph convolutional networks. arXiv preprint arXiv:2102.10739, 2021."
REFERENCES,0.56,"[58] Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In
Hal Daum´e III and Aarti Singh, editors, Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10432–
10441. PMLR, 13–18 Jul 2020."
REFERENCES,0.5622222222222222,"[59] Hedi Xia, Vai Suliafu, Hangjie Ji, Tan Minh Nguyen, Andrea Bertozzi, Stanley Osher, and
Bao Wang. Heavy ball neural ordinary differential equations. In A. Beygelzimer, Y. Dauphin,
P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Sys-
tems, 2021."
REFERENCES,0.5644444444444444,"[60] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
International Conference on Machine Learning, pages 5453–5462. PMLR, 2018."
REFERENCES,0.5666666666666667,"[61] Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki. ODE2VAE: deep generative second
order odes with Bayesian neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.5688888888888889,"[62] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Pro-
ceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pages 974–983, 2018."
REFERENCES,0.5711111111111111,Published as a conference paper at ICLR 2022
REFERENCES,0.5733333333333334,"[63] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Proceed-
ings of the 32nd International Conference on Neural Information Processing Systems, pages
5171–5181, 2018."
REFERENCES,0.5755555555555556,"[64] Lingxiao Zhao and Leman Akoglu. PairNorm: Tackling oversmoothing in GNNs. In Interna-
tional Conference on Learning Representations, 2020."
REFERENCES,0.5777777777777777,"[65] Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch¨olkopf.
Learning with local and global consistency. In Advances in neural information processing
systems, pages 321–328, 2004."
REFERENCES,0.58,"[66] Dengyong Zhou and Bernhard Sch¨olkopf. Regularization on discrete spaces. In 27th DAGM
Conference on Pattern Recognition, pages 361–368, 2005."
REFERENCES,0.5822222222222222,"[67] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using Gaus-
sian ﬁelds and harmonic functions. In Proceedings of the 20th International Conference on
Machine learning, pages 912–919, 2003."
REFERENCES,0.5844444444444444,"[68] Juntang Zhuang, Nicha Dvornek, Xiaoxiao Li, and James S. Duncan. Ordinary differential
equations on graph networks. https://openreview.net/forum?id=SJg9z6VFDr, 2020."
REFERENCES,0.5866666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.5888888888888889,"A
BACKGROUND ON GRAPH DIFFERENTIAL OPERATORS"
REFERENCES,0.5911111111111111,"Let (X, W ) represent a graph where X = ([x(1)]⊤, . . . , [x(n)]⊤)⊤∈Rn×d is the matrix where
each row x(i) ∈Rd is a feature vector and W = (Wij)n
i,j=1 is a n×n matrix with Wij representing
the similarity (edge weight) between the ith and jth feature vector. We assume that we are dealing
with an undirected graph, i.e., Wij = Wji. A Rk1-valued function on the nodes of the graph can
be represented as a matrix U ∈Rn×k1 by U = ([u(1)]⊤, . . . , [u(n)]⊤)⊤and we deﬁne the inner
product"
REFERENCES,0.5933333333333334,"⟨U, V ⟩= n
X"
REFERENCES,0.5955555555555555,"i=1
u(i) · v(i)."
REFERENCES,0.5977777777777777,"Similarly, a Rk2-valued function on the edges can be represented as a third-order tensor U ∈
Rn×n×k2 which we write as U =  
"
REFERENCES,0.6,"U(1,1)
· · ·
U(1,n)
...
...
...
U(n,1)
· · ·
U(n,n)  
"
REFERENCES,0.6022222222222222,"and U(i,j) ∈Rk2. On edge functions we use the inner product"
REFERENCES,0.6044444444444445,"⟨U, V⟩= 1 2 n
X"
REFERENCES,0.6066666666666667,"i,j=1
WijU(i,j) · V(i,j)."
REFERENCES,0.6088888888888889,"Multiplication between a matrix and an edge function is usually deﬁned pointwise and we use the
notation [A ⊙U](i,j) = AijU(i,j) ∈Rk2, for a matrix A ∈Rn×n and an edge function U ∈
Rn×n×k2, to make this clear. Similarly, pointwise multiplication between two matrices A, B ∈
Rn×n is deﬁned by [A ⊙B]ij = AijBij ∈R. When a matrix is acting as a linear operator on a
node function we use the usual matrix-vector notation and write [AU](i) = Pn
j=1 Aiju(j) ∈Rk1"
REFERENCES,0.6111111111111112,"for a matrix A ∈Rn×n and node function U = ([u(1)]⊤, . . . , [u(n)]⊤)⊤∈Rn×k1. In the sequel
we will have k1 = k2 = d."
REFERENCES,0.6133333333333333,"The gradient of a node-function U = ([u(1)]⊤, . . . , [u(n)]⊤)⊤∈Rn×d is deﬁned as the edge-
function ∇U
∈Rn×n×d with [∇U](i,j) = u(j) −u(i) ∈Rd.
The divergence divV =
([[divV](1)]⊤, . . . , [[divV](n)]⊤)⊤∈Rn×d of an edge-function V ∈Rn×n×d is deﬁned as"
REFERENCES,0.6155555555555555,"[divV](i) = n
X"
REFERENCES,0.6177777777777778,"j=1
WijV(i,j)"
REFERENCES,0.62,"for all i = 1, . . . , n. For anti-symmetric edge functions, i.e. V(i,j) = −V(j,i) for all i, j, we have
that the divergence is the negative adjoint to the gradient, i.e."
REFERENCES,0.6222222222222222,"⟨divV, U⟩= −⟨V, ∇U⟩."
REFERENCES,0.6244444444444445,"B
TECHNICAL PROOFS"
REFERENCES,0.6266666666666667,"Proof:
[Proof of Proposition 1] For notational convenience let us assume that x(i)(0) = x(i).
Clearly"
REFERENCES,0.6288888888888889,"E
h
B(i)(0)
i
= x(i) = x(i)(0)"
REFERENCES,0.6311111111111111,"for all i = 1, . . . , n. Assume that"
REFERENCES,0.6333333333333333,"E
h
B(i)(k)
i
= x(i)(δtk)"
REFERENCES,0.6355555555555555,Published as a conference paper at ICLR 2022
REFERENCES,0.6377777777777778,"for all i = 1, . . . , n. Then,"
REFERENCES,0.64,"E
h
B(i)(k + 1)
i = n
X"
REFERENCES,0.6422222222222222,"j=1
x(j)P

B(i)(k + 1) = x(j) = n
X j=1 n
X"
REFERENCES,0.6444444444444445,"ℓ=1
x(j)P

B(ℓ)(k) = x(j)|B(i)(1) = x(ℓ)
P

B(i)(1) = x(ℓ) = n
X j=1 n
X"
REFERENCES,0.6466666666666666,"ℓ=1
x(j)

(1 −δt) 1i=ℓ+ δtWiℓ di"
REFERENCES,0.6488888888888888,"
P

B(i)(1) = x(ℓ)"
REFERENCES,0.6511111111111111,"= (1 −δt) n
X"
REFERENCES,0.6533333333333333,"j=1
x(j)P

B(i)(1) = x(ℓ)
+ δt di n
X"
REFERENCES,0.6555555555555556,"ℓ=1
Wiℓ n
X"
REFERENCES,0.6577777777777778,"j=1
x(j)P

B(ℓ)(k) = x(j)"
REFERENCES,0.66,"= (1 −δt)E
h
B(i)(k)
i
+ δt di n
X"
REFERENCES,0.6622222222222223,"ℓ=1
WiℓE
h
B(ℓ)(k)
i"
REFERENCES,0.6644444444444444,"= (1 −δt)x(i)(δtk) + δt di n
X"
REFERENCES,0.6666666666666666,"ℓ=1
Wiℓx(ℓ)(δtk)"
REFERENCES,0.6688888888888889,"= x(i)(δtk) + δt di n
X"
REFERENCES,0.6711111111111111,"ℓ=1
WiℓWiℓ

x(ℓ)(δtk) −x(i)(δtk)
"
REFERENCES,0.6733333333333333,= x(i)(δtk) −δt [LX(δtk)](i)
REFERENCES,0.6755555555555556,"= x(i)(δt(k + 1)),
as required.
□"
REFERENCES,0.6777777777777778,"Proof:
[Proof of Proposition 2] Let P =
 
Pij

∈Rn×n be the probability transition kernel, so"
REFERENCES,0.68,"Pij =
 1 −δt
if i = j,
δtWij"
REFERENCES,0.6822222222222222,"di
if i ̸= j."
REFERENCES,0.6844444444444444,"We have
n
X"
REFERENCES,0.6866666666666666,"i=1
πiPij = n
X i=1"
REFERENCES,0.6888888888888889,"di
Pn
k=1 dk"
REFERENCES,0.6911111111111111,"
(1 −δt1i=j + δtWij di "
REFERENCES,0.6933333333333334,"= dj(1 −δt)
Pn
k=1 dk
+ δt
Pn
i=1 Wij
Pn
k=1 dk"
REFERENCES,0.6955555555555556,"=
dj
Pn
k=1 dk
= πj,
as required.
□"
REFERENCES,0.6977777777777778,"Proof:
[Proof of Proposition 3] The proof follows from a simple application of Propositions 1
and 2. Namely, for any i ∈{1, . . . , n}"
REFERENCES,0.7,"x(i)(kδt) = E

B(i)(k)

= n
X"
REFERENCES,0.7022222222222222,"j=1
x(j)(0)P
 
B(i)(k) = x(j)
→ n
X"
REFERENCES,0.7044444444444444,"j=1
x(j)(0)πj = ex"
REFERENCES,0.7066666666666667,"as k →∞.
□"
REFERENCES,0.7088888888888889,"Proof:
[Proof of Proposition 4] Let"
REFERENCES,0.7111111111111111,"y(i)(k) = E  
k
X s=0"
DI,0.7133333333333334,"1
di X j∈I"
DI,0.7155555555555555,"
x(j) −ˆx

1B(j)(s)=x(i)  ."
DI,0.7177777777777777,Published as a conference paper at ICLR 2022
DI,0.72,"Notice that E "" k
X"
DI,0.7222222222222222,"s=0
1B(j)(s)=x(i) # = k
X"
DI,0.7244444444444444,"s=0
P

B(j)(s) = x(i)"
DI,0.7266666666666667,"= P

B(j)(0) = x(i)"
DI,0.7288888888888889,"|
{z
}
δij + k
X"
DI,0.7311111111111112,"s=1
P

B(j)(s) = x(i)"
DI,0.7333333333333333,"= δij + k
X s=1 n
X"
DI,0.7355555555555555,"ℓ=1
P

B(j)(s) = x(i)|B(j)(s −1) = x(ℓ)
P

B(j)(s −1) = x(ℓ)"
DI,0.7377777777777778,"= δij + k
X s=1 n
X ℓ=1"
DI,0.74,"
(1 −δt)δℓi + δtWℓi dℓ"
DI,0.7422222222222222,"
P

B(j)(s −1) = x(ℓ)"
DI,0.7444444444444445,"= δij + (1 −δt) k
X"
DI,0.7466666666666667,"s=1
P

B(j)(s −1) = x(i)
+ δt n
X ℓ=1 Wℓi dℓ k
X"
DI,0.7488888888888889,"s=1
P

B(j)(s −1) = x(ℓ)"
DI,0.7511111111111111,"= δij + (1 −δt) k−1
X"
DI,0.7533333333333333,"s=0
P

B(j)(s) = x(i)
+ δt n
X ℓ=1 Wℓi dℓ k−1
X"
DI,0.7555555555555555,"s=0
P

B(j)(s) = x(ℓ)"
DI,0.7577777777777778,= δij + (1 −δt)E
DI,0.76,"""k−1
X"
DI,0.7622222222222222,"s=0
1B(j)(s)=x(i) # + δt n
X ℓ=1 Wℓi dℓ
E"
DI,0.7644444444444445,"""k−1
X"
DI,0.7666666666666667,"s=0
1B(j)(s)=x(ℓ) # ."
DI,0.7688888888888888,From the deﬁnition of Y and the above recursive relationship we have
DI,0.7711111111111111,y(i)(k) = 1 di X j∈I
DI,0.7733333333333333,"
x(j) −ˆx

E "" k
X"
DI,0.7755555555555556,"s=0
1B(j)(s)=x(i) # = 1 di X j∈I"
DI,0.7777777777777778,"
x(j) −ˆx

δij + (1 −δt) 1 di X j∈I"
DI,0.78,"
x(j) −ˆx

E"
DI,0.7822222222222223,"""k−1
X"
DI,0.7844444444444445,"s=0
1B(j)(s)=x(ℓ) # + δt di n
X ℓ=1 Wℓi dℓ X j∈I"
DI,0.7866666666666666,"
x(j) −ˆx

E"
DI,0.7888888888888889,"""k−1
X"
DI,0.7911111111111111,"s=0
1B(j)(s)=x(ℓ) # = 1 di X j∈I"
DI,0.7933333333333333,"
x(j) −ˆx

δij + (1 −δt)y(i)(k −1) + δt di n
X"
DI,0.7955555555555556,"ℓ=1
Wℓiy(ℓ)(k −1)"
DI,0.7977777777777778,= y(i)(k −1) + 1 di X j∈I
DI,0.8,"
x(j) −ˆx

δij −δt [LY (k −1)](i) ."
DI,0.8022222222222222,Now we let
DI,0.8044444444444444,"w(i)(k) = di

z(i)(kδt) −y(i)(k)
"
DI,0.8066666666666666,so that W satisﬁes
DI,0.8088888888888889,w(i)(k) = w(i)(k −1) −δt [LW (k −1)](i) = [P W (k −1)](i) .
DI,0.8111111111111111,"Hence, W (k) = P kW (0). Since the stationary distribution of the random walk with transition
kernel is π, we have limk→∞P k = 1π⊤. Hence, as k →∞, w(i)(k) →Pn
j=1 πjw(j)(0) = 0
since w(j)(0) = 0 by the choice in the initialisation of Z.
□"
DI,0.8133333333333334,Published as a conference paper at ICLR 2022
DI,0.8155555555555556,"Proof:
[Proof of Proposition 5] We prove the result by contradiction. Assume that there exists z
such that z(i)(kδt) →z for all i = 1, . . . , n as k →∞. Then LZ(kδt) →0. Since we can write"
DI,0.8177777777777778,z(i)(kδt) −z(j)(kδt) = z(i)((k −1)δt) −z(j)((k −1)δt) −δt 
DI,0.82,"[LZ((k −1)δt)](i) −[LZ((k −1)δt)](j)
!"
DI,0.8222222222222222,"+ δt
X ℓ∈I"
DI,0.8244444444444444,"
δiℓ(x(i) −ˆx) −δjℓ(x(j) −ˆx)

,"
DI,0.8266666666666667,"then taking the limit k →∞implies
X"
DI,0.8288888888888889,"ℓ∈I
δiℓ(x(i) −ˆx) =
X"
DI,0.8311111111111111,"ℓ∈I
δjℓ(x(j) −ˆx)"
DI,0.8333333333333334,"for all i, j which is clearly not true.
□"
DI,0.8355555555555556,"C
NEURAL ODES AND TRAINING NEURAL ODES WITH ADJOINT METHOD"
DI,0.8377777777777777,"Neural ODEs [13] are a class of continuous-depth (-time) neural networks that are particu-
larly suitable for learning complex dynamics from irregularly sampled sequential data, see, e.g.,
[13, 51, 19, 39, 42]. Mathematically, a neural ODE is the ﬁrst-order ODE: dh(t)"
DI,0.84,"dt
= f(h(t), t, θ),
(18)"
DI,0.8422222222222222,"where f(h(t), t, θ) ∈Rd is speciﬁed by a neural network parameterised by θ, e.g., a two-layer feed-
forward neural network. Starting from the input h(0), neural ODEs learn the representation and
perform prediction by solving (18) from t = 0 to T using a numerical integrator with a given error
tolerance, often with an adaptive step size solver (or adaptive solver for short) [18]. Solving (18)
from t = 0 to T in a single pass with an adaptive solver requires evaluating f(h(t), t, θ) at various
timesteps, with computational complexity counted by the number of forward function evaluations
(forward NFEs) [13]."
DI,0.8444444444444444,"The adjoint sensitivity method (or adjoint method) [46], is a memory-efﬁcient method for training
neural ODEs. We regard the output h(T) as the prediction and denote the loss between h(T) and
the ground truth as L. Let a(t) := ∂L/∂h(t) be the adjoint state, then we have (see [13, 46] for
details)
dL"
DI,0.8466666666666667,"dθ =
Z T"
DI,0.8488888888888889,"0
a(t)⊤∂f(h(t), t, θ)"
DI,0.8511111111111112,"∂θ
dt,
(19)"
DI,0.8533333333333334,with a(t) satisfying the following adjoint ODE da(t)
DI,0.8555555555555555,"dt
= −a(t)⊤∂"
DI,0.8577777777777778,"∂hf(h(t), t, θ),
(20)"
DI,0.86,"which is solved numerically from t = T to 0 and also requires the evaluation of the right-hand side
of (20) at various timestamps, and the backward NFEs measure the computational complexity."
DI,0.8622222222222222,"D
EXPERIMENTAL DETAILS AND MORE EXPERIMENTAL RESULTS"
DI,0.8644444444444445,"D.1
DATASETS AND EXPERIMENTAL SETTINGS"
DI,0.8666666666666667,"Graph node classiﬁcation dataset.
Following [10], we consider the largest connected component
of seven graph node classiﬁcation datasets, including CORA, CiteSeer, PubMed, coauthor graph
CoauthorCS, and Amazon co-purchasing graphs Computer and Photo, and a large scale ogbn-arxiv
dataset. For completeness, we list the number of classes, the number of features, and the number of
nodes and edges of each dataset in Table 3. More detailed information can be found in [10]."
DI,0.8688888888888889,"Depth of GRAND and GRAND++ for the results in Table 1.
Table 4 lists the ﬁne-tuned T for
the results in Table 1. Due to the limited time, we only search around the value of optimal T for
GRAND with grid spacing 0.1."
DI,0.8711111111111111,Published as a conference paper at ICLR 2022
DI,0.8733333333333333,"Dataset
Classes
Features
#Nodes
#Edges"
DI,0.8755555555555555,"CORA
7
1433
2485
5069
CiteSeer
6
3703
2120
3679
PubMed
3
500
19717
44324
CoauthorCS
15
6805
18333
81894
Computer
10
767
13381
245778
Photo
8
745
7487
119043
ogbn-arxiv
40
128
169343
1166243"
DI,0.8777777777777778,Table 3: Summary of the graph node classiﬁcation datasets.
DI,0.88,"Model
CORA
CiteSeer
PubMed
CoauthorCS
Computer
Photo"
DI,0.8822222222222222,"GRAND++-l
18.3
8.0
13.0
4.0
3.2
3.6"
DI,0.8844444444444445,"GRAND-l
18.2948
7.8741
12.9423
3.2490
3.5824
3.6760"
DI,0.8866666666666667,"Table 4: The value of the ﬁne-tuned T, i.e. depth of the continuous-depth GNNs, for GRAND and GRAND++
in learning with different labeling results, and the corresponding accuracy are reported in Table 1. The values
of T for GRAND++ are adopted from the paper [10]."
DI,0.8888888888888888,"D.2
CLASSIFICATION ACCURACY OF GNNS WITH DIFFERENT DEPTHS"
DI,0.8911111111111111,"In this subsection, we provide detailed numbers that correspond to Fig. 1. We further compare
GRAND++-l with several other GNN architectures, including GCN, GAT, and GraphSage, with
different depths on a few benchmark datasets. Table 5 lists the classiﬁcation accuracy of GRAND++,
GRAND, and three benchmark GNNs with different depths on six graph node classiﬁcation tasks.
Again, we see that GRAND++ is better than the other GNN models when the networks are deep."
DI,0.8933333333333333,"Model
depth
CORA
CiteSeer
PubMed
CoauthorCS
Computer
Photo"
DI,0.8955555555555555,"1
77.48 ± 1.43
71.23 ± 3.47
78.11 ± 1.47
90.42 ± 0.76
84.11 ± 0.51
92.93 ± 0.84
4
81.98 ± 1.42
72.58 ± 3.79
79.20 ± 0.74
90.89 ± 0.36
84.19 ± 0.93
93.54 ± 0.38
16
82.49 ± 1.37
73.84 ± 2.66
79.49 ± 0.84
90.24 ± 0.30
78.97 ± 2.33
92.69 ± 0.61
GRAND++-l
32
82.48 ± 0.71
73.29 ± 1.29
79.81 ± 1.61
NA
76.01 ± 1.33
92.94 ± 0.90
(ours)
64
80.99 ± 1.76
72.81 ± 2.18
NA
NA
NA
NA
128
80.29 ± 1.98
NA
NA
NA
NA
NA
256
79.04 ± 2.94
NA
NA
NA
NA
NA"
DI,0.8977777777777778,"1
78.59 ± 1.17
71.96 ± 2.74
77.93 ± 1.26
90.79 ± 0.93
83.41 ± 0.69
92.66 ± 0.42
4
82.80 ± 1.62
73.87 ± 2.12
78.71 ± 1.19
90.94 ± 0.21
84.23 ± 1.05
92.47 ± 0.53
16
82.75 ± 1.17
72.61 ± 2.42
78.79 ± 0.93
87.66 ± 1.70
77.67 ± 1.94
92.37 ± 0.27
GRAND-l
32
82.19 ± 1.73
72.65 ± 3.15
78.70 ± 1.08
NA
69.56 ± 2.20
89.61 ± 1.33
[10]
64
80.87 ± 2.28
69.84 ± 2.66
NA
NA
NA
NA
128
77.22 ± 2.88
NA
NA
NA
NA
NA
256
67.79 ± 3.10
NA
NA
NA
NA
NA"
DI,0.9,"1
76.92 ± 0.56
72.80 ± 1.69
72.78 ± 1.80
91.53 ± 0.45
81.44 ± 0.24
91.31 ± 0.19
GCN
4
81.35 ± 1.27
70.54 ± 6.61
77.15 ± 3.00
87.84 ± 0.96
75.73 ± 1.02
90.11 ± 0.66
[30]
16
19.70 ± 7.06
24.78 ± 1.45
41.36 ± 1.77
14.49 ± 0.91
12.86 ± 2.39
23.11 ± 1.76
32
21.86 ± 6.09
24.23 ± 1.65
40.66 ± 1.86
12.14 ± 1.64
21.15 ± 13.10
24.30 ± 0.73"
DI,0.9022222222222223,"1
72.49 ± 2.03
71.83 ± 1.53
77.24 ± 0.72
79.22 ± 0.60
73.97 ± 1.20
87.08 ± 0.37
GAT
4
80.95 ± 2.28
72.31 ± 2.82
77.37 ± 1.32
78.05 ± 1.10
76.67 ± 2.79
87.95 ± 1.76
[54]
16
29.14 ± 1.02
24.84 ± 1.45
39.21 ± 0.43
24.20 ± 2.22
37.07 ± 2.99
29.97 ± 3.68
32
29.75 ± 1.57
24.83 ± 1.45
39.02 ± 0.12
22.73 ± 2.08
32.53 ± 3.09
25.57 ± 4.03"
DI,0.9044444444444445,"1
73.47 ± 1.98
71.94 ± 1.45
72.42 ± 0.61
91.74 ± 0.26
75.95 ± 0.70
88.10 ± 0.87
GraphSage
4
79.83 ± 2.43
50.00 ± 14.27
76.01 ± 2.35
87.94 ± 0.23
75.62 ± 2.85
90.68 ± 2.11
[29]
16
25.52 ± 6.45
24.84 ± 1.45
37.55 ± 3.92
10.12 ± 2.21
22.79 ± 10.77
25.57 ± 3.31
32
29.14 ± 1.02
28.38 ± 2.54
39.21 ± 4.39
7.91 ± 3.15
37.07 ± 13.22
20.09 ± 5.67"
DI,0.9066666666666666,"Table 5: Classiﬁcation accuracy of different GNN models with different depths on six benchmark graph node
classiﬁcation tasks. NA: neural ODE solver failed. These results show that GRAND++ is better suited for
learning with a very deep architecture than GRAND. (Unit: %)"
DI,0.9088888888888889,"D.3
MORE RESULTS ON TIME-DEPENDENT ATTENTION AND GRAPH REWIRING"
DI,0.9111111111111111,"We further explore the effects of the source term for GRAND-nl and GRAND-nl-rw in the low-
labeling rate regimes. Table 6 compares GRAND-nl and GRAND-nl-rw with the corresponding
model with a source term. We see that GRAND-nl and GRAND-nl-rw are almost always worse than
the vanilla GRAND-l, consistent with the results reported in [10]. GRAND++-nl and GRAND++-
nl-rw cannot help learning at low labeling rates anymore. However, when the labeling rates are
not low, GRAND++-nl or GRAND++-nl-rw can outperform GRAND-nl and GRAND-nl-rw, even
outperform GRAND++."
DI,0.9133333333333333,Published as a conference paper at ICLR 2022
DI,0.9155555555555556,"Model
#per class
GRAND-nl [10]
GRAND-nl-rw [10]
GRAND++-nl (ours)
GRAND++-nl-rw (ours)"
DI,0.9177777777777778,"1
50.55 ± 15.68
50.63 ± 17.71
48.89 ± 11.51
47.94 ± 11.06
2
65.06 ± 9.35
61.24 ± 16.19
59.96 ± 7.90
58.25 ± 11.97
CORA
5
76.93 ± 3.10
76.50 ± 3.91
74.01 ± 1.73
74.25 ± 1.99
10
79.60 ± 2.69
79.38 ± 3.25
80.14 ± 0.69
80.18 ± 0.40
20
82.22 ± 1.93
82.14 ± 2.49
83.24 ± 0.20
81.48 ± 1.07"
DI,0.92,"1
50.25 ± 17.66
50.20 ± 17.90
49.65 ± 5.45
53.10 ± 5.51
2
59.87 ± 10.89
59.95 ± 10.48
59.16 ± 8.13
60.26 ± 5.10
CiteSeer
5
68.21 ± 5.08
68.05 ± 5.48
66.13 ± 2.09
67.81 ± 1.97
10
71.88 ± 6.94
71.92 ± 7.34
68.84 ± 2.84
71.45 ± 1.64
20
72.84 ± 6.61
72.72 ± 6.85
72.52 ± 1.24
73.87 ± 1.35"
DI,0.9222222222222223,"1
66.97 ± 10.07
67.69 ± 7.89
63.85 ± 4.86
67.45 ± 3.88
2
69.17 ± 2.46
69.42 ± 2.13
66.98 ± 5.30
69.11 ± 1.80
PubMed
5
72.56 ± 3.36
72.68 ± 2.52
71.49 ± 1.53
72.05 ± 3.67
10
76.03 ± 3.73
75.32 ± 3.45
74.94 ± 2.15
75.09 ± 2.88
20
78.55 ± 1.59
78.30 ± 1.43
78.41 ± 0.99
79.44 ± 0.56"
DI,0.9244444444444444,"Table 6: Classiﬁcation accuracy of the variants of GRAND and GRAND++ models trained with different
numbers of labeled data per class (#per class) on graph node classiﬁcation tasks. (Unit: %)"
DI,0.9266666666666666,"D.4
CLASSIFICATION ACCURACY OF GNNS WITH FEWER NUMBER OF TRAINING DATA"
DI,0.9288888888888889,"Besides the results shown in Fig. 1, we further test the classiﬁcation accuracy of more benchmark
GNN architectures trained with fewer numbers of labeled data per class. Tables 7-9 list the classi-
ﬁcation accuracy, on the test set, of different benchmark GNN models when they are trained with
different numbers of labeled nodes per class."
DI,0.9311111111111111,"#labeled nodes per class
1
2
5
10
20"
DI,0.9333333333333333,"GCN [30]
47.72 ± 15.53
60.85 ± 14.01
73.86 ± 7.97
78.82 ± 5.38
82.07 ± 2.03
GAT [54]
47.86 ± 15.38
58.30 ± 13.55
71.04 ± 5.74
76.31 ± 4.87
79.92 ± 2.28
GraphSage [29]
43.04 ± 14.01
53.96 ± 12.18
68.14 ± 6.95
75.04 ± 5.03
80.04 ± 2.54
MoNet [40]
47.72 ± 15.53
60.85 ± 14.01
73.86 ± 7.97
78.82 ± 5.38
82.07 ± 2.03
Lanczos [36]
47.41 ± 11.82
60.94 ± 4.00
74.28 ± 3.07
76.12 ± 0.93
79.85 ± 1.82
AdaLanczos [36]
48.23 ± 11.82
61.46 ± 4.96
74.24 ± 3.25
77.61 ± 1.36
81.03 ± 1.56
GCNN [58]
43.31 ± 11.95
60.28 ± 12.89
72.75 ± 4.21
78.92 ± 1.32
81.89 ± 1.12
GRAND-l [10]
52.53 ± 16.40
64.82 ± 11.16
76.07 ± 5.08
80.25 ± 3.40
82.86 ± 2.39
GRAND-nl [10]
40.97 ± 14.87
50.59 ± 13.25
65.13 ± 9.14
72.55 ± 6.65
77.76 ± 4.21
GRAND-nl-rw (gdc) [10]
52.68 ± 12.48
65.54 ± 10.01
74.94 ± 7.04
80.64 ± 6.19
82.47 ± 1.93
GRAND-nl-rw (two-hop) [10]
53.79 ± 17.72
64.50 ± 11.88
74.33 ± 6.28
79.61 ± 4.47
82.37 ± 1.98"
DI,0.9355555555555556,"Table 7: Classiﬁcation accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: CORA."
DI,0.9377777777777778,"#labeled nodes per class
1
2
5
10
20"
DI,0.94,"GCN [30]
48.94 ± 10.24
58.06 ± 9.76
67.24 ± 4.19
72.18 ± 3.47
74.21 ± 2.90
GAT [54]
50.31 ± 14.27
55.55 ± 9.19
67.37 ± 5.08
71.35 ± 4.92
73.22 ± 2.90
GraphSage [29]
48.81 ± 11.45
54.39 ± 11.37
64.79 ± 5.16
68.90 ± 5.08
72.02 ± 2.82
MoNet [40]
39.13 ± 11.37
48.52 ± 9.52
61.66 ± 6.61
68.08 ± 6.29
71.52 ± 4.11
Lanczos [36]
49.16 ± 3.63
57.65 ± 7.60
66.72 ± 9.38
71.01 ± 4.90
72.14 ± 2.00
AdaLanczos [36]
50.32 ± 7.42
58.35 ± 7.97
67.39 ± 8.20
72.15 ± 4.85
74.33 ± 2.83
GCNN [58]
40.58 ± 15.32
51.71 ± 13.87
63.16 ± 12.26
67.06 ± 5.65
69.84 ± 1.77
GRAND-l [10]
50.06 ± 17.98
59.55 ± 10.89
68.37 ± 5.00
71.90 ± 7.66
73.02 ± 5.89
GRAND-nl [10]
49.96 ± 18.62
59.57 ± 11.03
68.21 ± 7.08
71.88 ± 6.94
72.84 ± 6.61
GRAND-nl-rw (gdc) [10]
50.35 ± 17.74
59.98 ± 10.32
68.39 ± 5.81
71.83 ± 7.26
72.81 ± 6.94
GRAND-nl-rw (two-hop) [10]
50.20 ± 17.90
59.95 ± 10.48
68.05 ± 5.49
71.92 ± 7.34
72.72 ± 6.85"
DI,0.9422222222222222,"Table 8: Classiﬁcation accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: CiteSeer."
DI,0.9444444444444444,"D.5
T-TEST OF THE ACCURACY IMPROVEMENT OF GRAND++ OVER GRAND"
DI,0.9466666666666667,"To conﬁrm the statistical signiﬁcance of the accuracy improvement of GRAND++ over GRAND in
Table 1, in this subsection, we conduct t-test experiments at 0.95 conﬁdence to compare GRAND and
GRAND++ on six different benchmark graph node classiﬁcation tasks. We ﬁrst perform unpaired
t-tests to show the improvement of GRAND++ over GRAND on low labeled datasets using the
following t-score"
DI,0.9488888888888889,"t-score = µGRAND++ −µGRAND
q"
DI,0.9511111111111111,"σ2
GRAND++"
DI,0.9533333333333334,"n
+ σ2
GRAND++ n"
DI,0.9555555555555556,",
(21)"
DI,0.9577777777777777,Published as a conference paper at ICLR 2022
DI,0.96,"#labeled nodes per class
1
2
5
10
20"
DI,0.9622222222222222,"GCN [30]
58.61 ± 12.83
60.45 ± 16.20
68.69 ± 7.93
72.59 ± 3.19
76.89 ± 3.27
GAT [54]
58.84 ± 12.81
60.24 ± 14.44
68.54 ± 5.75
72.44 ± 3.50
75.55 ± 4.11
GraphSage [29]
55.53 ± 12.71
58.97 ± 12.65
66.07 ± 6.16
70.74 ± 3.11
74.55 ± 3.09
MoNet [40]
56.47 ± 4.67
61.03 ± 6.93
67.92 ± 2.50
71.24 ± 1.54
76.49 ± 1.75
Lanczos [36]
60.12 ± 6.37
63.65 ± 6.97
70.61 ± 4.50
73.01 ± 3.27
78.35 ± 1.84
AdaLanczos [36]
61.07 ± 5.16
64.11 ± 6.88
69.05 ± 3.00
72.79 ± 2.74
78.10 ± 1.91
GCNN [58]
60.78 ± 20.64
65.14 ± 19.45
72.72 ± 10.82
76.47 ± 6.03
79.24 ± 3.45
GRAND-l [10]
62.11 ± 10.58
69.00 ± 7.55
73.98 ± 5.08
76.33 ± 3.41
78.76 ± 1.69
GRAND-nl [10]
61.75 ± 11.12
69.16 ± 8.46
72.35 ± 5.35
76.03 ± 3.72
78.55 ± 1.59
GRAND-nl-rw (gdc) [10]
61.70 ± 10.74
69.42 ± 8.21
72.39 ± 5.25
75.32 ± 3.45
78.30 ± 1.43
GRAND-nl-rw (two-hop) [10]
61.65 ± 12.09
68.49 ± 8.99
72.68 ± 5.92
75.72 ± 3.50
78.77 ± 1.88"
DI,0.9644444444444444,"Table 9: Classiﬁcation accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: PubMed."
DI,0.9666666666666667,"where µ and σ2 are the mean and variance of the performances of each model, and n is the number
of runs for each model. The t-test score are shown in Table 10."
DI,0.9688888888888889,"#per class
CORA
CiteSeer
PubMed
CoauthorCS
Computer
Photo"
DI,0.9711111111111111,"1
1.05
4.36
3.28
1.95
111.60
45.33
2
1.39
3.96
0.34
4.59
7.18
4.65
5
2.55
2.68
-3.67
-1.96
15.67
0.26"
DI,0.9733333333333334,"Table 10: Unpaired t-test scores of GRAND++ v.s. GRAND on six different benchmark graph node classiﬁca-
tion tasks. With n = 100, over 0.95 conﬁdence is equivalent to exceed roughly 1.66 t-test scores. Highlighted
are the ones passing the test."
DI,0.9755555555555555,"For some entries in Table 10 that are not signiﬁcant enough, we further conduct paired t-test between
GRAND++ and GRAND on these speciﬁc datasets as shown in Table 11. Since a large portion of
variance comes from splitting of the datasets, we pair up tests of GRAND and GRAND++ with the
same splitting in this experiment. In this case, a sample of difference of size n is computed, and
t-test score can be computed using the equation"
DI,0.9777777777777777,t-score = µdiff −0
DI,0.98,"σdiff/√n.
(22)"
DI,0.9822222222222222,"Dataset
#per class
Accuracy Difference
# splits
t-score
p-score"
DI,0.9844444444444445,"CORA
1
1.06 ± 6.24
100
1.80
0.044
CORA
2
1.45 ± 5.23
100
2.78
0.003"
DI,0.9866666666666667,"Table 11: Paired t-test scores of GRAND++ v.s. GRAND on datasets where unpaired t-test scores are not
signiﬁcant enough."
DI,0.9888888888888889,"D.6
TASKS FOR FURTHER EVALUATING DEEP GRAND AND GRAND++"
DI,0.9911111111111112,"Open graph benchmark with paper citation network (ogbn-arxiv).
Ogbn-arxiv consists of 169,
343 nodes and 1, 166, 243 directed edges. Each node is an arxiv paper represented by a 128-
dimensional features and each directed edge indicates the citation direction. This dataset is used
for node property prediction and has been a popular benchmark to test the advantage of deep graph
neural networks over shallow graph neural networks [34, 32]. Compared to the GRAND model used
in [10], we reduce the hidden dimension from 162 to 81 to ﬁt the model into the GPU in our lab."
DI,0.9933333333333333,"Model
depth (T )
GRAND-l [10]
GRAND++-l (ours)
Improvement from GRAND++-l (ours)"
DI,0.9955555555555555,"1
68.50 ± 0.76
68.79 ± 0.35
0.29
4
69.53 ± 0.21
69.68 ± 0.38
0.15
6
69.46 ± 0.43
69.71 ± 0.24
0.25
8
69.44 ± 0.30
69.61 ± 0.28
0.17
OGBN-arXiv
32
67.44 ± 0.59
69.41 ± 0.53
1.97
64
63.47 ± 0.28
68.05 ± 0.73
4.58
96
55.95 ± 1.24
67.26 ± 0.61
11.31"
DI,0.9977777777777778,"Table 12: Classiﬁcation accuracy of the linear GRAND and GRAND++ models trained with different depth
on the OGBN-arXiv graph node classiﬁcation task. Compared to the GRAND model used in [10], we reduce
the hidden dimension from 162 to 81 to ﬁt the model into the GPU in our lab. (Unit: %)"
