Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001841620626151013,"Federated learning (FL) is a privacy-preserving paradigm where multiple partic-
ipants jointly solve a machine learning problem without sharing raw data. Un-
like traditional distributed learning, a unique characteristic of FL is statistical het-
erogeneity, namely, data distributions across participants are different from each
other. Meanwhile, recent advances in the interpretation of neural networks have
seen a wide use of neural tangent kernel (NTK) for convergence and generaliza-
tion analyses. In this paper, we propose a novel FL paradigm empowered by the
NTK framework. The proposed paradigm addresses the challenge of statistical
heterogeneity by transmitting update data that are more expressive than those of
the traditional FL paradigms. Speciﬁcally, sample-wise Jacobian matrices, rather
than model weights/gradients, are uploaded by participants. The server then con-
structs an empirical kernel matrix to update a global model without explicitly per-
forming gradient descent. We further develop a variant with improved communi-
cation efﬁciency and enhanced privacy. Numerical results show that the proposed
paradigm can achieve the same accuracy while reducing the number of communi-
cation rounds by an order of magnitude compared to federated averaging."
INTRODUCTION,0.003683241252302026,"1
INTRODUCTION"
INTRODUCTION,0.0055248618784530384,"Federated learning (FL) has emerged as a popular paradigm involving a large number of workers col-
laboratively solving a machine learning problem (Kairouz et al., 2021). In a typical FL framework,
a server broadcasts a global model to selected workers and collects model updates without needing
to access the raw data. One popular algorithm is known as federated averaging (FedAvg) (McMa-
han et al., 2017), in which workers perform stochastic gradient descent (SGD) to update the local
models and upload the weight vectors to the server. A new global model is constructed on the server
by averaging the received weight vectors."
INTRODUCTION,0.007366482504604052,"Li et al. (2020) characterized some unique challenges of FL. First, client data are generated lo-
cally and remain decentralized, which implies that they may not be independent and identically
distributed (IID). Prior works have shown that statistical heterogeneity can negatively inﬂuence the
convergence of FedAvg (Zhao et al., 2018). This phenomenon may be explained that local updating
under data heterogeneity will cause cost-function inconsistency (Wang et al., 2020). More chal-
lengingly, the learning procedure is susceptible to system heterogeneity, including the diversity of
hardware, battery power, and network connectivity. Local updating schemes often exacerbate the
straggler issue caused by heterogeneous system characteristics."
INTRODUCTION,0.009208103130755065,"Recent studies have proposed various strategies to alleviate the statistical heterogeneity. One possi-
ble solution is to share a globally available dataset with participants to reduce the distance between
client-data distributions and the population distribution (Zhao et al., 2018). In practice, though, such
a dataset may be unavailable or too small to meaningfully compensate for the heterogeneity. Some
researchers replaced the coordinate-wise weight averaging strategy in FedAvg with nonlinear ag-
gregation schemes (Wang et al., 2020; Chen & Chao, 2021). The nonlinear aggregation relies on
a separate optimization routine, which can be elusive, especially when the algorithm does not con-
verge well. Another direction is to modify the local objectives or local update schemes to cancel the
effects of client drift (Li et al., 2020; Karimireddy et al., 2020). However, some studies reported that
these methods are not consistently effective, and may perform worse than FedAvg when evaluated
in various settings (Reddi et al., 2021; Haddadpour et al., 2021; Chen & Chao, 2021)."
INTRODUCTION,0.011049723756906077,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01289134438305709,"In this work, we present a neural tangent kernel empowered federated learning (NTK-FL) paradigm.
Given a ﬁxed number of communication rounds, NTK-FL outperforms state-of-the-art methods in
terms of test accuracy. We summarize our contributions as follows."
INTRODUCTION,0.014732965009208104,"• We propose a novel FL paradigm without requiring workers to perform gradient descent. To the
best of our knowledge, this is the ﬁrst work using the NTK method to replace gradient descent in
FL algorithms."
INTRODUCTION,0.016574585635359115,"• Our scheme inherently solves the non-IID data problem of FL. Compared with FedAvg, it is robust
to different degrees of data heterogeneity and has a consistently fast convergence speed. We verify
the effectiveness of the paradigm theoretically and experimentally."
INTRODUCTION,0.01841620626151013,"• We add communication-efﬁcient and privacy-preserving features to the paradigm and develop
CP-NTK-FL by combining strategies such as random projection and data subsampling. We show
that some strategies can also be applied to traditional FL methods. Although such methods cause
performance degradation when applied to FedAvg, they only slightly worsen the model accuracy
when applied to the proposed CP-NTK-FL."
RELATED WORK,0.020257826887661142,"2
RELATED WORK"
RELATED WORK,0.022099447513812154,"Neural Tangent Kernel.
Jacot et al. (2018) showed that training an inﬁnitely wide neural net-
work with gradient descent in the parameter space is equivalent to kernel regression in the function
space. Lee et al. (2019) used a ﬁrst-order Taylor expansion to approximate the neural network output
and derived the training dynamics in a closed form. Chen et al. (2020) established the generalization
bounds for a two-layer over-parameterized neural network with the NTK framework. The NTK com-
putation has been extended to convolutional neural networks (CNNs) (Arora et al., 2019), recurrent
neural networks (RNNs) (Alemohammad et al., 2021), and even to neural networks with arbitrary
architectures (Yang & Littwin, 2021). Empirical studies have also provided a good understanding
of the wide neural networks training (Lee et al., 2020)."
RELATED WORK,0.02394106813996317,"Federated Learning.
FL aims to train a model with distributed workers without transmitting local
data (McMahan et al., 2017; Kairouz et al., 2021). FedAvg has been proposed as a generic solution
with many theoretical analyses and implementation variants. Recent studies have shown a growing
interest in improving its communication efﬁciency, privacy guarantees, and robustness to hetero-
geneity. To reduce communication cost, gradient quantization and sparsiﬁcation were incorporated
into FedAvg (Reisizadeh et al., 2020; Sattler et al., 2019). From the security perspective, Zhu et al.
(2019) showed that sharing gradients may cause privacy leakage. To address this challenge, differen-
tially private federated optimization and decentralized aggregation methods were developed (Girgis
et al., 2021; Cheng et al., 2021). Other works put the focus on the statistical heterogeneity issue and
designed various methods such as adding regularization terms to the objective function (Li et al.,
2020; Smith et al., 2017). In this work, we focus on a novel FL paradigm where the global model
is derived based on the NTK evolution. We show that the proposed NTK-FL is robust to statistical
heterogeneity by design, and extend it to a variant with improved communication efﬁciency and
enhanced privacy."
RELATED WORK,0.02578268876611418,"Kernel Methods in Federated Learning.
The NTK framework has been mostly used for con-
vergence analyses in FL. Seo et al. (2020) studied two knowledge distillation methods in FL and
compared their convergence properties based on the neural network function evolution in the NTK
regime. Li et al. (2021) incorporated batch normalization layers to local models, and provided the-
oretical justiﬁcation for its faster convergence by studying the minimum nonnegative eigenvalue of
the tangent kernel matrix. Huang et al. (2021) directly used the NTK framework to analyze the con-
vergence rate and generalization bound of two-layer ReLU neural networks trained with FedAvg. Su
et al. (2021) studied the convergence behavior of a set of FL algorithms in the kernel regression set-
ting. In comparison, our work does not focus on pure convergence analyses of existing algorithms.
We propose a novel FL framework by replacing the gradient descent with the NTK evolution."
RELATED WORK,0.027624309392265192,Under review as a conference paper at ICLR 2022
BACKGROUND AND PRELIMINARIES,0.029465930018416207,"3
BACKGROUND AND PRELIMINARIES"
BACKGROUND AND PRELIMINARIES,0.03130755064456722,"We use lowercase nonitalic boldface, nonitalic boldface capital, and italic boldface capital letters to
denote vectors, matrices, and tensors, respectively. For example, for column vectors aj ∈RM, j ∈
{1, . . . , N}, A = [a1, . . . , aN] is an M × N matrix. A third-order tensor A ∈RK×M×N can be
viewed as a concatenation of such matrices. We use a slice to denote a matrix in a third-order tensor
by varying two indices (Kolda & Bader, 2009). Take tensor A, for instance: Ai:: is a matrix of the
ith horizontal slice, and A:j: is its jth lateral slice (Kolda & Bader, 2009). Finally, the indicator
function of an event is denoted by 1 (·)."
FEDERATED LEARNING MODEL,0.03314917127071823,"3.1
FEDERATED LEARNING MODEL"
FEDERATED LEARNING MODEL,0.034990791896869246,"Consider an FL architecture where a server trains a global model by indirectly using datasets
distributed among M workers.
The local dataset of the mth worker is denoted by Dm =
{(xm,i, ym,i)}Nm
i=1, where (xm,i, ym,i) is an input-output pair, drawn from a distribution Pm. The
local objective can be formulated as an empirical risk minimization over Nm training examples:
Fm(w) =
1
Nm
PNm
i=1 R(w; xm,j, ym,i), where R is a sample-wise risk function quantifying the er-
ror of model with a weight vector w ∈Rd estimating the label ym,i for an input xm,i. The global
objective function is denoted by F(w), and the optimization problem may be formulated as:"
FEDERATED LEARNING MODEL,0.03683241252302026,"min
w∈Rd F(w) = 1 M M
X"
FEDERATED LEARNING MODEL,0.03867403314917127,"m=1
Fm(w).
(1)"
LINEARIZED NEURAL NETWORK MODEL,0.040515653775322284,"3.2
LINEARIZED NEURAL NETWORK MODEL"
LINEARIZED NEURAL NETWORK MODEL,0.0423572744014733,"Let (xi, yi) denote a training pair, with xi ∈Rd1 and yi ∈Rd2, where d1 is the input dimension and
d2 is the output dimension. X ≜[x1, . . . , xN]⊤represents the input matrix and Y ≜[y1, . . . , yN]⊤
represents the label matrix. Consider a neural network function f : Rd1 →Rd2 parameterized by a
vector w ∈Rd, which is the vectorization of all weights for the multilayer network. Given an input
xi, the network outputs a prediction ˆyi = f(w; xi). Let ℓ(ˆyi, yi) be the loss function measuring the
dissimilarity between the predicted result ˆyi and the true label yi. We are interested in ﬁnding an
optimal weight vector w⋆that minimizes the empirical loss over N training examples:"
LINEARIZED NEURAL NETWORK MODEL,0.04419889502762431,"w⋆= argmin
w
L(w; X, Y) ≜1 N N
X"
LINEARIZED NEURAL NETWORK MODEL,0.04604051565377532,"i=1
ℓ(ˆyi, yi).
(2)"
LINEARIZED NEURAL NETWORK MODEL,0.04788213627992634,"One common optimization method is the gradient descent training. Given the learning rate η, gra-
dient descent updates the weights at each time step as: w(t+1) = w(t) −η∇wL. To simplify the
notation, let f (t)(x) be the output at time step t with an input x, i.e., f (t)(x) ≜f(w(t); x). Following
Lee et al. (2019), we use the ﬁrst-order Taylor expansion around the initial weight vector w(0) to
approximate the neural network output given an input x, i.e.,"
LINEARIZED NEURAL NETWORK MODEL,0.049723756906077346,"f (t)(x) ≈f (0)(x) + J(0)(x)(w(t) −w(0)),
(3)"
LINEARIZED NEURAL NETWORK MODEL,0.05156537753222836,"where J(0)(x) = [∇f (0)
1 (x), . . . , ∇f (0)
d2 (x)]⊤, with ∇f (t)
j (x) ≜[∂ˆy(t)
j /∂w(t)
1 , . . . , ∂ˆy(t)
j /∂w(t)
d ]⊤"
LINEARIZED NEURAL NETWORK MODEL,0.053406998158379376,"being the gradient of the jth component of the neural network output with respect to w(t). Consider
the halved mean-squared error (MSE) loss ℓ, namely, ℓ=
1
d2
Pd2
j=1
1
2(ˆyj −yj)2. Based on the
continuous-time limit, one can show that the dynamics of the gradient ﬂow are governed by the
following differential equation:
df
dt = −η H(0) 
f (t)(X) −Y

,
(4)"
LINEARIZED NEURAL NETWORK MODEL,0.055248618784530384,"where f (t)(X) ∈RN×d2 is a matrix of concatenated output for all training examples, and H(0) is
the neural tangent kernel at time step 0, with each entry (H(0))ij equal to the scaled Frobenius inner
product of the Jacobian matrices: (H(0))ij =
1
d2

J(0)(xi), J(0)(xj)"
LINEARIZED NEURAL NETWORK MODEL,0.0570902394106814,"F . The differential equation
(4) has the closed-form solution:"
LINEARIZED NEURAL NETWORK MODEL,0.058931860036832415,"f (t)(X) =

I −e−ηt"
LINEARIZED NEURAL NETWORK MODEL,0.06077348066298342,"N H(0)
Y + e−ηt"
LINEARIZED NEURAL NETWORK MODEL,0.06261510128913444,"N H(0)f (0)(X).
(5)"
LINEARIZED NEURAL NETWORK MODEL,0.06445672191528545,Under review as a conference paper at ICLR 2022
LINEARIZED NEURAL NETWORK MODEL,0.06629834254143646,"The neural network state f (t)(X) can thus be directly obtained from (5) without running the gradient
descent algorithm."
PROPOSED FL PARADIGM VIA THE NTK FRAMEWORK,0.06813996316758748,"4
PROPOSED FL PARADIGM VIA THE NTK FRAMEWORK"
PROPOSED FL PARADIGM VIA THE NTK FRAMEWORK,0.06998158379373849,"In this section, we present the NTK-FL paradigm (Figure 1) and then extend it to the variant CP-
NTK-FL (Figure 2) with improved communication efﬁciency and enhanced privacy. The detailed
algorithm descriptions are presented as follows."
NTK-FL PARADIGM,0.0718232044198895,"4.1
NTK-FL PARADIGM"
NTK-FL PARADIGM,0.07366482504604052,the mth worker
NTK-FL PARADIGM,0.07550644567219153,aggregation server
NTK-FL PARADIGM,0.07734806629834254,x server updates
NTK-FL PARADIGM,0.07918968692449356,"& broadcats
worker receives
weight w(k)"
NTK-FL PARADIGM,0.08103130755064457,"w(k)
Dm"
NTK-FL PARADIGM,0.08287292817679558,"y
worker sends
J(k)
m , f (k)(Xm),
and Ym"
NTK-FL PARADIGM,0.0847145488029466,z server builds
NTK-FL PARADIGM,0.0865561694290976,"kernel H(k) &
peforms weight
evolution"
NTK-FL PARADIGM,0.08839779005524862,"w(k,t1)"
NTK-FL PARADIGM,0.09023941068139964,"w(k,t2)"
NTK-FL PARADIGM,0.09208103130755065,"w(k,tΨ) t1 t2 tΨ {"
NTK-FL PARADIGM,0.09392265193370165,"server
evaluates
w(k,tj) &
selects the
best one"
NTK-FL PARADIGM,0.09576427255985268,"w(k,t(k))"
NTK-FL PARADIGM,0.09760589318600368,"Figure 1: Schematic of NTK-FL. Each worker
ﬁrst receives the weight w(k), and then uploads
the Jacobian tensor J(k)
m , label Ym, and initial
condition f (k)(Xm). The server builds a global
kernel H(k) and performs the weight evolution
with {t1, . . . , tΨ}. We use (9) to ﬁnd the best tj
and update the weight accordingly."
NTK-FL PARADIGM,0.09944751381215469,"NTK-FL follows four steps to update the global
model in each round.
First, the server will
select a subset Ck of workers and broad-
cast to them a model weight vector w(k)
from the kth round.
Here, the superscript
k is the communication round index, and it
should be distinguished from the gradient de-
scent time step t in Section 3.2.
Second,
each worker will use its local training data
Dm to compute a Jacobian tensor J(k)
m
∈
RNm×d2×d, ∀m ∈Ck, which is a concate-
nation of Nm sample-wise Jacobian matrices
(J(k)
m )i:: = [∇f (k)
1
(xm,i), . . . , ∇f (k)
d2 (xm,i)]⊤,
i ∈{1, . . . , Nm}. The worker will then up-
load the Jacobian tensor J(k)
m , labels Ym, and
initial condition f (k)(Xm) to the server. The
transmitted information corresponds to the vari-
ables in the state evolution of f (t) in (5). Third,
the server will construct a global Jacobian ten-
sor J(k) ∈RN×d2×d based on received J(k)
m ’s,
with each worker contributing Nm horizontal
slices to J(k)."
NTK-FL PARADIGM,0.10128913443830571,"We use a toy example to explain the process as follows. Suppose the server selects worker 1 and
worker 2 in a certain round. Workers 1 and 2 will compute the Jacobian tensors J(k)
1
and J(k)
2 ,
respectively. The global Jacobian tensor is constructed as:"
NTK-FL PARADIGM,0.10313075506445672,"J(k)
i:: ="
NTK-FL PARADIGM,0.10497237569060773,"(
J(k)
1,i:: , if i ∈{1, . . . , N1},"
NTK-FL PARADIGM,0.10681399631675875,"J(k)
2,j:: , j = i −N1, if i ∈{N1 + 1, . . . , N1 + N2}.
(6)"
NTK-FL PARADIGM,0.10865561694290976,"After obtaining the global Jacobian tensor J(k), the (i, j)th entry of the global kernel H(k) is cal-
culated as the scaled Frobenius inner product of two horizontal slices of J(k), i.e., (H(k))ij =
1
d2 ⟨J(k)
i:: , J(k)
j:: ⟩F. For simplicity, Fourth, the server will perform the NTK evolution to obtain the
updated neural network function f (k+1) and weight vector w(k+1). With a slight abuse of notation,
let f (k,t) denote the neural network output at gradient descent step t in communication round k. The
neural network function evolution dynamics and weight evolution dynamics are given by:"
NTK-FL PARADIGM,0.11049723756906077,"f (k,t) =

I −e−ηt"
NTK-FL PARADIGM,0.11233885819521179,"N H(k)
Y(k) + e−ηt"
NTK-FL PARADIGM,0.1141804788213628,"N H(k)f (k),
(7a)"
NTK-FL PARADIGM,0.11602209944751381,"w(k,t) = d2
X"
NTK-FL PARADIGM,0.11786372007366483,"j=1
(J(k)
:j: )⊤R(k,t)
:j
+ w(k),
(7b)"
NTK-FL PARADIGM,0.11970534069981584,Under review as a conference paper at ICLR 2022
NTK-FL PARADIGM,0.12154696132596685,"where J(k)
:j: is the jth lateral slice of J(k), and R(k,t)
:j
is the jth column of the residual matrix R(k,t)
deﬁned as follows:"
NTK-FL PARADIGM,0.12338858195211787,"R(k,t) ≜
η
Nd2 t−1
X u=0"
NTK-FL PARADIGM,0.1252302025782689,"
Y(k) −f (k,u)(X(k))

.
(8)"
NTK-FL PARADIGM,0.1270718232044199,"Note that X(k) = [X⊤
1 , . . . , X⊤
Ck]⊤denotes a concatenation of worker training examples, and
Y(k) = [Y⊤
1 , . . . , Y⊤
Ck]⊤denotes a concatenation of worker labels.
The weight evolution in (7b)
is derived by unfolding the gradient descent steps. To update the global weight, the server performs
the evolution with various integer steps {t1, . . . , tΨ} and selects the best one with the smallest loss
value. Our goal is to minimize the training loss with a small generalization gap (Nakkiran et al.,
2020). The updated weight is decided by the following procedure:"
NTK-FL PARADIGM,0.1289134438305709,"w(k+1) ≜w(k),
t(k) = argmin
tj
L(f(w(k,tj); X(k), Y(k))).
(9)"
NTK-FL PARADIGM,0.13075506445672191,"Alternatively, if the server has an available validation dataset, the optimal number of update steps
can be selected based on the model validation performance. In practice, such a validation dataset
can be obtained from held-out workers (Wang et al., 2021). Based on the closed-form solution in
(7b), the search of t(k) over the grid {t1, . . . , tΨ} can be completed in O(Ψ) time."
NTK-FL PARADIGM,0.13259668508287292,"Comparison of NTK-FL and Huang et al. (2021).
Huang et al. (2021) presented the details
of FedAvg by letting clients use local updates and upload gradients to train a two-layer neural net-
work. In contrast, NTK-FL let each client transmit Jacobian matrices without performing local SGD
steps. The model weight is updated via NTK evolution in (7b). The main differences include: (1)
clients transmit more expressive Jacobian matrices to improve model performance in the non-IID
FL setting; (2) the computation is shifted to the server."
NTK-FL PARADIGM,0.13443830570902393,"Robustness Against Statistical Heterogeneity.
In essence, statistical heterogeneity comes from
the decentralized data of heterogeneous distributions owned by individual workers. If privacy is not
an issue, the non-IID challenge can be readily resolved by mixing all workers’ datasets and training
a centralized model. In NTK-FL, instead of building a centralized dataset, we use Jacobian matrices
to construct a global kernel H(k), which is a concise representation of gathered data points from
all selected workers. This representation is yet more expressive/less compact than that of a tradi-
tional FL algorithm. More precisely, the update being sent for NTK-FL regarding the ith training
example of the mth worker for NTK-FL is Jm = [∇f1(xm,i), . . . , ∇fd2(xm,i)]⊤, whereas the gra-
dient update being sent for FedAvg is ∇L(w; xm,i, ym,i) =
1
d2
Pd2
j=1 (ˆym,i,j −ym,i,j) ∇fj(xm,i),
a weighted sum of coordinates of Jm. By sending Jacobian matrices Jm and jointly processing them
on the server, NTK-FL delays the more aggressive data aggregation step after the communication
stage and therefore better approximates the centralized learning setting than FedAvg does."
CP-NTK-FL VARIANT,0.13627992633517497,"4.2
CP-NTK-FL VARIANT"
CP-NTK-FL VARIANT,0.13812154696132597,"Compared to FedAvg, NTK-FL does not incur additional client computational overhead since cal-
culating the Jacobian tensor enjoys the same communication efﬁciency with computing aggregated
gradients. Without locally updating weight vectors, NTK-FL is faster than FedAvg on the client
side. In this section, we focus on the perspectives of the communication efﬁciency and security in
terms of data conﬁdentiality and membership privacy."
CP-NTK-FL VARIANT,0.13996316758747698,"For communication, we follow the widely adopted analysis framework in wireless communication
to examine only the client uplink overhead, assuming that the downlink bandwidth is much larger
and the server will have enough transmission power (Tran et al., 2019). In NTK-FL, the client up-
link communication cost and space complexity are dominated by a third-order tensor J(k)
m , i.e., an
O (Nmd2d) complexity compared to O (d) in FedAvg. For security, we investigate a threat model
where a curious server may perform membership inference attacks (Nasr et al., 2018) or data re-
construction attacks (Zhu et al., 2019). Compared to the averaged gradient, sample-wise Jacobian
matrices are more expressive, which may facilitate such attacks from the aggregation server. We
extend NTK-FL by combining various tools to solve the aforementioned problems without jeopar-
dizing the performance severely. Although it is possible to incorporate these tools into FedAvg, we
will show that overall it will lead to more severe accuracy drop."
CP-NTK-FL VARIANT,0.141804788213628,Under review as a conference paper at ICLR 2022
CP-NTK-FL VARIANT,0.143646408839779,shufﬂing server
CP-NTK-FL VARIANT,0.14548802946593,aggregation server
CP-NTK-FL VARIANT,0.14732965009208104,trusted key server
CP-NTK-FL VARIANT,0.14917127071823205,the mth worker
CP-NTK-FL VARIANT,0.15101289134438306,"x
key server
encrypts ρ
& transmits"
CP-NTK-FL VARIANT,0.15285451197053407,"E(k+
m, ρ)"
CP-NTK-FL VARIANT,0.15469613259668508,y worker receives
CP-NTK-FL VARIANT,0.15653775322283608,"weight w(k) &
decrypts to get
the seed ρ"
CP-NTK-FL VARIANT,0.15837937384898712,"w(k),ρ
Bm ⊂Dm
z
worker gets Z′
m
sends C(J′(k)
m ),
f (k)
m (Z′
m), Ym"
CP-NTK-FL VARIANT,0.16022099447513813,"{
shufﬂing server
performs
permutation"
CP-NTK-FL VARIANT,0.16206261510128914,| aggregagtion
CP-NTK-FL VARIANT,0.16390423572744015,"server builds
kernel H(k) &
obtains ∆w(k)"
CP-NTK-FL VARIANT,0.16574585635359115,"Figure 2: Schematic of CP-NTK-FL. A trusted
key server (orange) sends an encrypted seed
E(k+
m, ρ) with the public key k+
m for random pro-
jection. The client transmits the required message
to the shufﬂing server (blue) for permutation."
CP-NTK-FL VARIANT,0.16758747697974216,"3
6
9
12
15
18
Communication Round 0.5 0.6 0.7 0.8 0.9"
CP-NTK-FL VARIANT,0.1694290976058932,Test Accuracy
CP-NTK-FL VARIANT,0.1712707182320442,"NTK-FL
NTK-FL'
FedAvg
FedAvg'"
CP-NTK-FL VARIANT,0.1731123388581952,"Figure 3: Training results of 300 workers via
NTK-FL and FedAvg, along with variants with
the local dataset subsampling and random pro-
jection, denoted as NTK-FL′ and FedAvg′, re-
spectively. We train a two-layer multilayer per-
ceptron on the Fashion-MNIST dataset.
The
joint effect causes more accuracy degradation in
FedAvg (red) than in NTK-FL (black)."
CP-NTK-FL VARIANT,0.17495395948434622,"Jacobian Dimension Reduction.
First, we let the mth worker sample a subset Bm from its dataset
Dm uniformly for the training. Let β ∈(0, 1) denote the sampling rate, Bm contains N ′
m = βNm
data points, with the training pairs denoted by (X′
m, Y′
m). Next, we consider using a random pro-
jection to preprocess the input data via a seed shared by a trusted key server. Formally, the sampled
training examples are projected into Z′
m, i.e., Z′
m = X′
mP, where P ∈Rd1×d′
1 is a projection
matrix generated based on a seed ρ with IID standard Gaussian entries. In general, we have d′
1 < d1
and an non-invertible projection operation. The concept of trusted key server follows the trusted
third party in cryptography (Van Oorschot, 2020), and we assume it will not be compromised."
CP-NTK-FL VARIANT,0.17679558011049723,"These two steps can already reduce the uplink communication overhead and enhance privacy. We
ﬁrst examine the current Jacobian tensor J′(k)
m
∈RN′
m×d2×d′. Compared with its original version
J(k)
m , it has reduced dimensionality at the cost of certain information loss. Meanwhile, the random
projection will defend against the data reconstruction attack, as the Jacobian tensor is now evaluated
at the projected data Z′
m. We empirically verify their impact on the test accuracy in Figure 3. We
set d′
1 = 100 and sampling rate β = 0.4, and train a multilayer perceptron with 100 hidden nodes
on the Fashion-MNIST dataset (Xiao et al., 2017). The joint effect of these strategies is a slight
accuracy drop in NTK-FL and a nonnegligible performance degradation in FedAvg."
CP-NTK-FL VARIANT,0.17863720073664824,"Jacobian Compression and Shufﬂing.
We use a compression scheme to reduce the size of the Ja-
cobian tensor by zeroing out the coordinates with small magnitude (Alistarh et al., 2018). In addition
to the communication efﬁciency, this compression scheme is empirically effective against the data
reconstruction attack (Zhu et al., 2019). To further ensure the conﬁdentiality and membership pri-
vacy, we introduce a shufﬂing server, inspired by some recent frameworks (Girgis et al., 2021; Cheng
et al., 2021), to permute Jacobian tensors J(k)
m ’s, neural network states f (k)
m ’s, and labels Ym’s.
Based on (7b), we denote the model update by ∆w(k) ≜w(k+1) −w(k) = Pd2
j=1(J(k)
:j: )⊤R(k,t)
:j
,
which is a sum of matrix products. If rows and columns are permuted in synchronization, the weight
update ∆w(k) will remain unchanged. Considering the high dimensionality of the neural network
weight, the reconstruction attack becomes computationally infeasible. As provable differential pri-
vacy guarantee does not explicitly protect against the reconstruction attack (Zhang et al., 2020), we
leave a thorough privacy study for future work."
ANALYSIS OF ALGORITHM,0.18047882136279927,"5
ANALYSIS OF ALGORITHM"
ANALYSIS OF ALGORITHM,0.18232044198895028,"In this section, we analyze the loss decay rate between successive communication rounds in NTK-
FL and make comparisons with FedAvg. Similar to Du et al. (2019) and Dukler et al. (2020), we"
ANALYSIS OF ALGORITHM,0.1841620626151013,Under review as a conference paper at ICLR 2022
ANALYSIS OF ALGORITHM,0.1860036832412523,consider a two-layer neural network f : Rd →R of the following form to facilitate our analysis:
ANALYSIS OF ALGORITHM,0.1878453038674033,"f(x; V, c) =
1
√n n
X"
ANALYSIS OF ALGORITHM,0.18968692449355432,"r=1
crσ(v⊤
r x),
(10)"
ANALYSIS OF ALGORITHM,0.19152854511970535,"where x ∈Rd1 is an input, vr ∈Rd1 is the weight vector in the ﬁrst layer, V = [v1, · · · , vn],
cr ∈R is the weight in the second layer, and σ(·) is the rectiﬁed linear unit (ReLU) function,
namely σ(z) = max(z, 0), applied coordinatewise. We state two assumptions as prerequisites.
Assumption 1 The ﬁrst layer vr’s are sampled from N(0, α2I). The second layer cr’s are sampled
from {−1, 1} with equal probability and are kept ﬁxed during training."
ANALYSIS OF ALGORITHM,0.19337016574585636,"Assumption 1 gives the initial distribution of the neural network parameters. Similar assumptions
can be found in Dukler et al. (2020). We add restrictions to the input data in the next assumption.
Assumption 2 (Normalized input). The input data are normalized, i.e., ∥xi∥2 ⩽1, ∀i."
ANALYSIS OF ALGORITHM,0.19521178637200737,"For this neural network model, the (i, j)th entry of the empirical kernel matrix H(k) given in (3.2)
can be calculated as: (H(k))ij = 1"
ANALYSIS OF ALGORITHM,0.19705340699815838,"nx⊤
i xj
Pn
r=1 1(k)
ir 1(k)
jr , where 1(k)
ir ≜1{⟨v(k)
r , xi⟩⩾0}, and the
term c2
r is omitted according to Assumption 1. Deﬁne H∞, whose (i, j)th entry is given by:"
ANALYSIS OF ALGORITHM,0.19889502762430938,"(H∞)ij ≜Ev∼N(0,α2I)

x⊤
i xj 1(v⊤xi ⩾0) 1(v⊤xj ⩾0)

.
(11)
Let λ0 denote the minimum eigenvalue of H∞, which is restricted in the next assumption.
Assumption 3 The kernel matrix H∞is positive deﬁnite, namely, λ0 > 0."
ANALYSIS OF ALGORITHM,0.2007366482504604,"In fact, the positive-deﬁnite property of H∞can be shown under certain conditions (Dukler et al.,
2020). For simplicity, we omit the proof details and directly assume the positive deﬁniteness of H∞"
ANALYSIS OF ALGORITHM,0.20257826887661143,"in Assumption 3. Next, we study the residual term ∥f (k)(X)−y∥2
2 in communication round k, where
X = [X⊤
1 , . . . , X⊤
M]⊤∈RN×d1 denote a concatenation of client inputs and y = [y⊤
1 , . . . , y⊤
M]⊤∈
RN denote a concatenation of client labels. We give the convergence result by analyzing how the
residual term decays between successive rounds.
Theorem 1 For the NTK-FL scheme under Assumptions 1 to 3, let the learning rate η = O
  λ0 N
"
ANALYSIS OF ALGORITHM,0.20441988950276244,"and the neural network width n = Ω

N2"
ANALYSIS OF ALGORITHM,0.20626151012891344,"λ2
0 ln N2"
ANALYSIS OF ALGORITHM,0.20810313075506445,"δ

, then with probability at least 1−δ, the one-round
loss decay of NTK-FL is"
ANALYSIS OF ALGORITHM,0.20994475138121546,"f (k+1)(X) −y
2
2 ⩽

1 −ηλ0"
N,0.21178637200736647,2N
N,0.2136279926335175,"t(k) f (k)(X) −y
2
2,
(12)"
N,0.2154696132596685,where t(k) is the number of NTK update steps deﬁned in (9).
N,0.21731123388581952,"The proof of Theorem 1 can be found in Appendix B. By studying the asymmetric kernel matrix
caused by local update (Huang et al., 2021), we have the following theorem for FedAvg, where the
proof can be found in Appendix C."
N,0.21915285451197053,"Theorem 2 For FedAvg under Assumptions 1 to 3, let the learning rate η = O

λ0
τN|Ck|

and the"
N,0.22099447513812154,"neural network width n = Ω

N2"
N,0.22283609576427257,"λ2
0 ln N2"
N,0.22467771639042358,"δ

, then with probability at least 1 −δ, the one-round loss
decay of FedAvg is
f (k+1)(X) −y
2
2 ⩽

1 −ητλ0"
N,0.2265193370165746,2N|Ck|
N,0.2283609576427256," f (k)(X) −y
2
2,
(13)"
N,0.2302025782688766,"where τ is the number of local iterations, and |Ck| is the cardinality of the worker set in round k.
Remark 1 (Fast Convergence of NTK-FL). The convergence rate of NTK-FL is faster than FedAvg.
To see this, we compare the Binomial approximation of the decay coefﬁcient in Theorem 1 with the
decay coefﬁcient in Theorem 2, i.e., 1 −η1t(k)λ0"
N,0.23204419889502761,"2N
+ O
 
η2
1

< 1 −η2τλ0"
N,0.23388581952117865,"2N|Ck|, where η ≪1 for a large"
N,0.23572744014732966,"N 1. The number of NTK update steps t(k) is chosen dynamically in (9), which is on the order of
102 to 103, whereas τ is often on the order of magnitude of 10 in literature (Reisizadeh et al., 2020;
Haddadpour et al., 2021). One can verify that η1t(k)λ0 is larger than η2τλ0/|Ck| and draw the
conclusion in (1)."
N,0.23756906077348067,"1For example, if we have 100 clients, each of which has more than 100 data points, then N is on the order
of 104. Considering the choice of the learning rate η1, the Binomial approximation holds in (1)."
N,0.23941068139963168,Under review as a conference paper at ICLR 2022
N,0.24125230202578268,"0
10
20
30
40
50
Communication Round 0.75 0.80 0.85 0.90 0.95"
N,0.2430939226519337,Test Accuracy
N,0.24493554327808473,"Centralized
NTK-FL
DataShare
FedNova
FedAvg (a)"
N,0.24677716390423574,"0
10
20
30
40
50
Communication Round 0.84 0.88 0.92 0.96"
N,0.24861878453038674,Test Accuracy
N,0.2504604051565378,"Centralized
NTK-FL
DataShare
FedNova
FedAvg (b)"
N,0.2523020257826888,"0
10
20
30
40
50
Communication Round 0.72 0.76 0.80 0.84 0.88"
N,0.2541436464088398,Test Accuracy
N,0.2559852670349908,"Centralized
NTK-FL
DataShare
FedNova
FedAvg (c)"
N,0.2578268876611418,"Figure 4: Test accuracy versus communication round of different methods evaluated on: (a) EM-
NIST dataset, where the heterogeneity comes from feature skewness. (b) non-IID MNIST dataset
with label skewness, where the Dirichlet distribution parameter α = 0.5. (c) non-IID Fashion-
MNIST dataset with label skewness, where the Dirichlet distribution parameter α = 0.5. NTK-FL
outperforms all baseline FL algorithms in different scenarios, and achieves similar test performance
compared with the ideal centralized training case."
EXPERIMENTAL RESULTS,0.2596685082872928,"6
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.26151012891344383,"Federated Settings.
We use three datasets, namely, MNIST (LeCun et al., 1998), Fashion-
MNIST (Xiao et al., 2017), and EMNIST (Cohen et al., 2017) digits. All of them contain C = 10
categories. For MNIST and Fashion-MNIST, we follow Hsu et al. (2019) to simulate non-IID data
with the symmetric Dirichlet distribution (Good, 1976). Speciﬁcally, for the mth worker, we draw
a random vector qm ∼Dir(α), where qm = [qm,1, . . . , qm,C]⊤belongs to the (C −1)-standard
simplex. Images with category k are assigned to the mth worker in proportional to (100 · qm,k)%.
The heterogeneity in this setting mainly comes from label skewness. For the EMNIST dataset, it
has a federated version that splits the dataset into shards indexed by the original writer of the digits
(Kairouz et al., 2021). The heterogeneity mainly comes from feature skewness. A multilayer per-
ceptron model with 100 hidden nodes is chosen as the target neural network model. We consider a
total of 300 workers and select 20 of them with equal probability in each round."
EXPERIMENTAL RESULTS,0.26335174953959484,"Convergence.
We empirically verify the convergence rate of the proposed method. For FedAvg,
we use the number of local iterations from {10, 20, . . . , 50} and report the best results. For NTK-FL,
we choose t(k) over the set {100, 200, . . . , 2000}. We use the following methods that are robust to
the non-IID setting as the baselines: (i) Data sharing scheme suggested by Zhao et al. (2018), where
a global dataset is broadcasted to workers for local training; the size of the global dataset is set to be
10% of the total number of local data points. (ii) Federated normalized averaging (FedNova) (Wang
et al., 2020), where the workers transmit normalized gradient vectors to the server. (iii) Central-
ized training simulation, where the server collects the data points from subset Ck of workers and
performs gradient descent to directly train the global model. Clearly, scheme (iii) achieves the per-
formance that can be considered as an upper bound of all other algorithms. The training curves over
three repetitions are shown in Figure 4. More implementation details and the results on CIFAR-10
(Krizhevsky, 2009) can be found in Appendix A. Our proposed NTK-FL method shows consistent
advantages over other methods in different non-IID scenarios."
EXPERIMENTAL RESULTS,0.26519337016574585,"Degree of Heterogeneity.
In this experiment, we select the Dirichlet distribution parameter
α from {0.1, 0.2, 0.3, 0.4, 0.5} and simulate different degrees of heterogeneity on Fashion-MNIST
dataset. A smaller α will increase the degree of heterogeneity in the data distribution. We evaluate
NTK-FL, DataShare, FedNova, and FedAvg model test accuracy after training for 50 rounds. The
mean values over three repetitions are shown in Figure 5, where each point is obtained over ﬁve
repetitions with standard deviation less than 1%. It can be observed that NTK-FL achieves stable
test accuracy in different heterogeneous settings. In comparison, FedAvg and FedNova show a
performance drop in the small α region. NTK-FL has more advantages over baselines methods
when the degree of heterogeneity is larger."
EXPERIMENTAL RESULTS,0.26703499079189685,"Effect of Hyperparameters.
We study the effect of the tunable parameters in CP-NTK-FL. We
change the local data sampling rate β and dimension d′
1, and evaluate the model test accuracy on the
non-IID Fashion-MNIST dataset (α = 0.1) after 10 communication rounds. The results are shown
in Figure 6. A larger data sampling rate β or a larger dimension d′
1 will cause less information loss,"
EXPERIMENTAL RESULTS,0.26887661141804786,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.27071823204419887,"0.1
0.2
0.3
0.4
0.5 0.81 0.82 0.83 0.84 0.85 0.86"
EXPERIMENTAL RESULTS,0.27255985267034993,Test Accuracy
EXPERIMENTAL RESULTS,0.27440147329650094,"NTK-FL
DataShare
FedNova
FedAvg"
EXPERIMENTAL RESULTS,0.27624309392265195,"Figure 5: Test accuracy versus the Dirichlet distri-
bution parameter α for different methods evaluated
on the non-IID Fashion-MNIST dataset. Reducing
the value of α will increase the degree of hetero-
geneity in the data distribution. NTK-FL is robust
to different heterogeneous data distributions, and
shows more advantages over FedAvg and FedNova
when the degree of heterogeneity is larger."
EXPERIMENTAL RESULTS,0.27808471454880296,"100
200
300
400
500
600
d′1"
EXPERIMENTAL RESULTS,0.27992633517495397,"0.7
0.6
0.5
0.4
0.3
0.2"
EXPERIMENTAL RESULTS,0.281767955801105,"82.9
84.1
84.3
84.7
84.8
84.6"
EXPERIMENTAL RESULTS,0.283609576427256,"83.1
83.8
84.8
84.4
84.5
85.0"
EXPERIMENTAL RESULTS,0.285451197053407,"82.8
83.9
84.4
84.6
84.4
84.9"
EXPERIMENTAL RESULTS,0.287292817679558,"82.4
83.8
84.3
84.4
84.4
84.5"
EXPERIMENTAL RESULTS,0.289134438305709,"82.4
83.5
84.2
84.4
84.1
84.2"
EXPERIMENTAL RESULTS,0.29097605893186,"81.9
83.2
83.7
84.2
84.5
84.6 82 83 84 85"
EXPERIMENTAL RESULTS,0.292817679558011,Accuracy (%)
EXPERIMENTAL RESULTS,0.2946593001841621,"Figure 6: CP-NTK-FL test accuracy for dif-
ferent hyperparams. A larger data sampling
rate β and a larger dimension d′
1 are expected
to give a higher test accuracy. In general, the
scheme is robust to different combinations of
hyperparameters."
EXPERIMENTAL RESULTS,0.2965009208103131,"and are expected to achieve a higher test accuracy. The results also show that the scheme is robust
to different combinations of hyperparameters."
EXPERIMENTAL RESULTS,0.2983425414364641,"Table 1: Uplink communication cost to reach 85%
on non-IID Fashion-MNIST dataset (α = 0.1).
CP-NTK-FL can achieve the target goal within
the fewest communication rounds without incur-
ring communication cost signiﬁcantly."
EXPERIMENTAL RESULTS,0.3001841620626151,"optimization
algorithms
comm.
rounds
comm.
cost (MB)"
EXPERIMENTAL RESULTS,0.3020257826887661,"CP-NTK-FL
26
386"
EXPERIMENTAL RESULTS,0.30386740331491713,"FedCOM
250
379
QSGD (4 bit)
614
465
FedAvg
284
1720"
EXPERIMENTAL RESULTS,0.30570902394106814,"Uplink Communication.
We evaluate the
uplink communication efﬁciency of CP-NTK-
FL (d′
1 = 200, β = 0.3) by measuring the num-
ber of rounds and cumulative uplink commu-
nication cost to reach a test accuracy of 85%
on non-IID Fashion-MNIST dataset (α = 0.1).
The results over three repetitions are shown
in Table 1.
Compared with federated learn-
ing with compression (FedCOM) (Haddadpour
et al., 2021), quantized SGD (QSGD) (Alis-
tarh et al., 2017), and FedAvg, CP-NTK-FL
achieves the goal within an order of magnitude
fewer iterations, which is particularly advanta-
geous for applications with nonnegligible en-
coding/decoding delays or network latency."
CONCLUSION AND FUTURE WORK,0.30755064456721914,"7
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.30939226519337015,"In this paper, we have proposed an NTK empowered FL paradigm. It inherently solves the statistical
heterogeneity challenge. By constructing a global kernel based on the local sample-wise Jacobian
matrices, the global model weights can be updated via NTK evolution in the parameter space. Com-
pared with traditional algorithms such as FedAvg, NTK-FL has a more centralized training ﬂavor by
transmitting more expressive updates. The effectiveness of the proposed paradigm has been veriﬁed
theoretically and experimentally."
CONCLUSION AND FUTURE WORK,0.31123388581952116,"In future work, it will be interesting to extend the paradigm for other neural network architectures,
such as CNNs, residual networks (ResNets) (He et al., 2016), and RNNs. It is also worthwhile to
further improve the efﬁciency of NTK-FL and explore its savings in wall-clock time. We believe the
proposed paradigm will provide a new perspective to solve federated learning challenges."
REFERENCES,0.31307550644567217,REFERENCES
REFERENCES,0.3149171270718232,"Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk. The recurrent neural
tangent kernel. In International Conference on Learning Representations, 2021."
REFERENCES,0.31675874769797424,Under review as a conference paper at ICLR 2022
REFERENCES,0.31860036832412525,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
QSGD:
Communication-efﬁcient SGD via gradient quantization and encoding. Advances in Neural In-
formation Processing Systems, 2017."
REFERENCES,0.32044198895027626,"Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cedric
Renggli. The convergence of sparsiﬁed gradient methods. In Advances in Neural Information
Processing Systems, 2018."
REFERENCES,0.32228360957642727,"Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. In Advances in Neural Information
Processing Systems, 2019."
REFERENCES,0.3241252302025783,"Hong-You Chen and Wei-Lun Chao. Fedbe: Making Bayesian model ensemble applicable to feder-
ated learning. In International Conference on Learning Representations, 2021."
REFERENCES,0.3259668508287293,"Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. A generalized neural tangent kernel
analysis for two-layer neural networks. In Advances in Neural Information Processing Systems,
2020."
REFERENCES,0.3278084714548803,"Pau-Chen Cheng, Kevin Eykholt, Zhongshu Gu, Hani Jamjoom, KR Jayaram, Enriquillo Valdez,
and Ashish Verma. Separation of powers in federated learning. arXiv preprint arXiv:2105.09400,
2021."
REFERENCES,0.3296500920810313,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik.
EMNIST: Extending
MNIST to handwritten letters. In International Joint Conference on Neural Networks, 2017."
REFERENCES,0.3314917127071823,"Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019."
REFERENCES,0.3333333333333333,"Yonatan Dukler, Guido Montufar, and Quanquan Gu. Optimization theory for ReLu neural networks
trained with normalization layers. In International Conference on Machine Learning, 2020."
REFERENCES,0.3351749539594843,"Antonious M. Girgis, Deepesh Data, Suhas N. Diggavi, Peter Kairouz, and Ananda Theertha Suresh.
Shufﬂed model of differential privacy in federated learning. In International Conference on Arti-
ﬁcial Intelligence and Statistics, 2021."
REFERENCES,0.3370165745856354,"Irving J Good. On the application of symmetric dirichlet distributions and their mixtures to contin-
gency tables. The Annals of Statistics, 4(6):1159–1189, 1976."
REFERENCES,0.3388581952117864,"Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Feder-
ated learning with compression: Uniﬁed analysis and sharp guarantees. In International Confer-
ence on Artiﬁcial Intelligence and Statistics, 2021."
REFERENCES,0.3406998158379374,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In International Conference on Computer Vision and Pattern Recognition, 2016."
REFERENCES,0.3425414364640884,"Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classiﬁcation. arXiv preprint arXiv:1909.06335, 2019."
REFERENCES,0.3443830570902394,"Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. FL-NTK: A neural tangent kernel-based
framework for federated learning convergence analysis. arXiv preprint arXiv:2105.05001, 2021."
REFERENCES,0.3462246777163904,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.34806629834254144,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, et al. Ad-
vances and open problems in federated learning. Foundations and Trends in Machine Learning,
2021."
REFERENCES,0.34990791896869244,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, 2020."
REFERENCES,0.35174953959484345,Under review as a conference paper at ICLR 2022
REFERENCES,0.35359116022099446,"Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455–500, 2009."
REFERENCES,0.35543278084714547,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Master thesis, Dept. of
Comput. Sci., Univ. of Toronto, Toronto, Canada, 2009."
REFERENCES,0.3572744014732965,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.35911602209944754,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.36095764272559855,"Jaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus inﬁnite neural networks: an empirical study. In Advances
in Neural Information Processing Systems, 2020."
REFERENCES,0.36279926335174956,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020."
REFERENCES,0.36464088397790057,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Sys-
tems, 2020."
REFERENCES,0.3664825046040516,"Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learn-
ing on non-iid features via local batch normalization. In International Conference on Learning
Representations, 2021."
REFERENCES,0.3683241252302026,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelli-
gence and Statistics, 2017."
REFERENCES,0.3701657458563536,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In ICLR 2020 : Eighth International
Conference on Learning Representations, 2020."
REFERENCES,0.3720073664825046,"Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine learning with membership privacy us-
ing adversarial regularization. In ACM SIGSAC Conference on Computer and Communications
Security, pp. 634–646, 2018."
REFERENCES,0.3738489871086556,"Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn´y,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
Conference on Learning Representations, 2021."
REFERENCES,0.3756906077348066,"Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efﬁcient federated learning method with periodic averaging and quan-
tization. In International Conference on Artiﬁcial Intelligence and Statistics, 2020."
REFERENCES,0.3775322283609576,"Felix Sattler, Simon Wiedemann, Klaus-Robert M¨uller, and Wojciech Samek.
Robust and
communication-efﬁcient federated learning from non-iid data. IEEE Transactions on Neural Net-
works and Learning Systems, 31(9):3400–3413, 2019."
REFERENCES,0.37937384898710863,"Hyowoon Seo, Jihong Park, Seungeun Oh, Mehdi Bennis, and Seong-Lyun Kim. Federated knowl-
edge distillation. arXiv preprint arXiv:2011.02367, 2020."
REFERENCES,0.3812154696132597,"Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar.
Federated multi-task
learning. In Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.3830570902394107,"Lili Su, Jiaming Xu, and Pengkun Yang. Achieving statistical optimality of federated learning:
Beyond stationary points. arXiv preprint arXiv:2106.15216, 2021."
REFERENCES,0.3848987108655617,"Nguyen H Tran, Wei Bao, Albert Zomaya, Minh NH Nguyen, and Choong Seon Hong. Federated
learning over wireless networks: Optimization model design and analysis. In IEEE INFOCOM
2019-IEEE Conference on Computer Communications, pp. 1387–1395. IEEE, 2019."
REFERENCES,0.3867403314917127,Under review as a conference paper at ICLR 2022
REFERENCES,0.3885819521178637,"Paul C Van Oorschot. Computer Security and the Internet: Tools and Jewels. Springer Nature, 2020."
REFERENCES,0.39042357274401474,"Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. In International Conference on Learning Represen-
tations, 2020."
REFERENCES,0.39226519337016574,"Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling the objective in-
consistency problem in heterogeneous federated optimization. In Advances in Neural Information
Processing Systems, 2020."
REFERENCES,0.39410681399631675,"Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to feder-
ated optimization. arXiv preprint arXiv:2107.06917, 2021."
REFERENCES,0.39594843462246776,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.39779005524861877,"Greg Yang and Etai Littwin. Tensor programs iib: Architectural universality of neural tangent kernel
training dynamics. In International Conference on Machine Learning, 2021."
REFERENCES,0.3996316758747698,"Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret re-
vealer: Generative model-inversion attacks against deep neural networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 253–261, 2020."
REFERENCES,0.4014732965009208,"Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018."
REFERENCES,0.40331491712707185,"Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
Information Processing Systems, 2019."
REFERENCES,0.40515653775322286,Under review as a conference paper at ICLR 2022
REFERENCES,0.40699815837937386,"A
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.4088397790055249,"0
10
20
30
40
50
Communication Round 0.25 0.30 0.35 0.40 0.45"
REFERENCES,0.4106813996316759,Test Accuracy
REFERENCES,0.4125230202578269,"Centralized
NTK-FL
DataShare
FedNova
FedAvg"
REFERENCES,0.4143646408839779,"Figure 7: Test accuracy versus communication
round of different methods evaluated on the non-
IID CIFAR-10 dataset, where the Dirichlet distri-
bution parameter α = 0.1."
REFERENCES,0.4162062615101289,"We give the detailed setting of the learning rate
and batch size.
For the learning rate η, we
search over the set {10−3, 3 × 10−3, 10−2, 3 ×
10−2, 10−1}.
The learning rate is ﬁxed dur-
ing the training.
For the client batch size,
we set it to 200 for all datasets.
We eval-
uate different methods, including the central-
ized training simulation, data sharing method
(Zhao et al., 2018), FedNova (Wang et al.,
2020), FedAvg (McMahan et al., 2017), and the
proposed NTK-FL on the non-IID CIFAR-10
dataset (Krizhevsky, 2009) and present the re-
sults in Figure 7. NTK-FL outperforms other
FL algorithms and shows test accuracy close to
the centralized simulation. The observation is
consistent with the results in Figure 4."
REFERENCES,0.4180478821362799,"B
PROOF OF THEOREM 1"
REFERENCES,0.4198895027624309,"Let Im denote a set of indices such that for i ∈Im, (xi, yi) ∈Dm. We ﬁrst present some lemmas
to facilitate the convergence analysis. In communication round k, deﬁne S(k)
i
as the set of indices
corresponding to neurons whose activation pattern is similar to its initial state for an input xi:"
REFERENCES,0.42173112338858193,"S(k)
i
≜

r ∈{1, . . . , n}
 ∃v, ∥v −v(0)
r ∥2 ⩽R, 1(0)
ir ̸= 1
 
v⊤xi ⩾0
 	
.
(14)"
REFERENCES,0.42357274401473294,"We upper bound the cardinality of Si in Lemma 1.
Lemma 1 Under Assumption 1 to 2, with probability at least 1 −δ, we have"
REFERENCES,0.425414364640884,|Si| ⩽ r
REFERENCES,0.427255985267035,"2
π
nR"
REFERENCES,0.429097605893186,"δα ,
∀i ∈{1, . . . , N}.
(15)"
REFERENCES,0.430939226519337,"Proof. To bound |Si| =
nP"
REFERENCES,0.43278084714548803,"r=1
1 (r ∈Si), consider an event Air deﬁned as follows:"
REFERENCES,0.43462246777163904,"Air ≜{∃v, ∥v −v(0)
r ∥2 ⩽R, 1(0)
ir ̸= 1
 
v⊤xi ⩾0

}.
(16)"
REFERENCES,0.43646408839779005,"Clearly, 1 (r ∈Si) = 1 (Air). According to Assumption 2, ∥xi∥⩽1, it can be shown that the event
Air happens if and only if |(v(0)
r )⊤xi| ⩽R based on a geometric argument. Based on Assumption 1,
we have (v(0)
r )⊤xi ∼N(0, α2). The probability of event Air is"
REFERENCES,0.43830570902394106,"P[Air] = P
h
|(v(0)
r )⊤xi| ⩽R
i
(17a)"
REFERENCES,0.44014732965009207,"= erf
 R
√ 2α 
⩽ r 2
π
R"
REFERENCES,0.4419889502762431,"α .
(17b)"
REFERENCES,0.4438305709023941,"By Markov’s inequality, we have with probability at least 1 −δ, n
X"
REFERENCES,0.44567219152854515,"r=1
1 (r ∈Si) ⩽ r"
REFERENCES,0.44751381215469616,"2
π
nR"
REFERENCES,0.44935543278084716,"δα .
(18)"
REFERENCES,0.45119705340699817,"The proof is complete.
We bound the perturbation of the kernel matrix H(k,t) in Lemma 2."
REFERENCES,0.4530386740331492,"Lemma 2 Under Assumption 1 to 2, if ∀r ∈{1, . . . , n}, ∥v(k,t)
r
−v(0)
r ∥2 ⩽R, then"
REFERENCES,0.4548802946593002,"∥H(k,t) −H(0)∥2 ⩽2
√"
NR,0.4567219152854512,"2NR
√πδα .
(19)"
NR,0.4585635359116022,Under review as a conference paper at ICLR 2022
NR,0.4604051565377532,Proof. We have
NR,0.4622467771639042,"∥H(k,t) −H(0)∥2
2 ⩽∥H(k,t) −H(0)∥2
F
(20a) = N
X i=1 N
X j=1"
NR,0.46408839779005523,"h
(H(k,t))ij −(H(0))ij
i2
(20b) = 1 n2 N
X i=1 N
X"
NR,0.46593001841620624,"j=1
(x⊤
i xj)2
 n
X"
NR,0.4677716390423573,"r=1
1(k,t)
ir
1(k,t)
jr
−1(0)
ir 1(0)
jr !2"
NR,0.4696132596685083,".
(20c)"
NR,0.4714548802946593,"Consider the event Air deﬁned in (16). Let φ(k,t)
ijr
≜1(k,t)
ir
1(k,t)
jr
−1(0)
ir 1(0)
jr . If ¬Air and ¬Ajr
happen, clearly we have |φ(k,t)
ijr | = 0. Therefore, the expectation of |φ(k,t)
ijr | can be bounded as"
NR,0.4732965009208103,"E
hφ(k,t)
ijr

i
⩽P(Air ∪Ajr) · 1
(21a)"
NR,0.47513812154696133,"⩽P(Air) + P(Ajr)
(21b) x
⩽2 r 2
π
R"
NR,0.47697974217311234,"α ,
(21c)"
NR,0.47882136279926335,"where x comes from (17b). By Markov’s inequality, we have with probability at least 1 −δ,"
NR,0.48066298342541436,"|φ(k,t)
ijr | ⩽2 r"
NR,0.48250460405156537,"2
π
R
δα.
(22)"
NR,0.4843462246777164,Plugging (22) into (20c) yields
NR,0.4861878453038674,"∥H(k,t) −H(0)∥2
2 ⩽N 2"
NR,0.4880294659300184,"n2
8n2R2"
NR,0.48987108655616945,πδ2α2 = 8N 2R2
NR,0.49171270718232046,"πδ2α2 .
(23)"
NR,0.49355432780847147,Taking the square root on both sides completes the proof.
NR,0.4953959484346225,"Lemma 3 With probability at least 1 −δ,"
NR,0.4972375690607735,∥H(0) −H∞∥2 ⩽N r
NR,0.4990791896869245,ln (2N 2/δ)
N,0.5009208103130756,"2n
.
(24)"
N,0.5027624309392266,Proof. We have
N,0.5046040515653776,"∥H(0) −H∞∥2
2 ⩽∥H(0) −H∞∥2
F = N
X i=1 N
X j=1"
N,0.5064456721915286,"h
(H(0))ij −(H∞)ij
i2
.
(25)"
N,0.5082872928176796,Note that (H(0))ij = 1
N,0.5101289134438306,"nx⊤
i xj nP"
N,0.5119705340699816,"r=1
1(0)
ir 1(0)
jr , (H(0))ij ∈[−1, 1]. By Hoeffding’s inequality, we have"
N,0.5138121546961326,"with probability at least 1 −δ/n2,
(H(0))ij −(H∞)ij
 ⩽ r"
N,0.5156537753222836,ln (2N 2/δ)
N,0.5174953959484346,"2n
.
(26)"
N,0.5193370165745856,"Applying the union bound over i, j ∈[N] yields"
N,0.5211786372007366,∥H(0) −H∞∥2 ⩽N r
N,0.5230202578268877,ln (2N 2/δ)
N,0.5248618784530387,"2n
.
(27)"
N,0.5267034990791897,The proof is complete.
N,0.5285451197053407,Now we are going to prove Theorem 1.
N,0.5303867403314917,"Theorem 1 For the NTK-FL scheme under Assumptions 1 to 3, let the learning rate η = O
  λ0"
N,0.5322283609576427,"N

and"
N,0.5340699815837937,"the neural network width n = Ω

N2"
N,0.5359116022099447,"λ2
0 ln 2N2"
N,0.5377532228360957,"δ

, then with probability at least 1 −δ, the one-round
loss decay of NTK-FL is"
N,0.5395948434622467,"∥f (k+1)(X) −y∥2
2 ≤

1 −ηλ0"
N,0.5414364640883977,2N t(k)
N,0.5432780847145487,"∥f (k)(X) −y∥2
2.
(28)"
N,0.5451197053406999,Under review as a conference paper at ICLR 2022
N,0.5469613259668509,Proof. Taking the difference between successive terms yields
N,0.5488029465930019,"f (k,t+1)(xi) −f (k,t)(xi) =
1
√n n
X r=1"
N,0.5506445672191529,"h
crσ

(v(k,t+1)
r
)⊤xi

−crσ

(v(k,t)
r
)⊤xi
i
.
(29)"
N,0.5524861878453039,"We decompose the difference term to the sum of dI
i and dII
i , based on the set Si:"
N,0.5543278084714549,"dI
i ≜
1
√n X"
N,0.5561694290976059,r /∈Si
N,0.5580110497237569,"h
crσ

(v(k,t+1)
r
)⊤xi

−crσ

(v(k,t)
r
)⊤xi
i
,
(30a)"
N,0.5598526703499079,"dII
i ≜
1
√n X r∈Si"
N,0.5616942909760589,"h
crσ

(v(k,t+1)
r
)⊤xi

−crσ

(v(k,t)
r
)⊤xi
i
.
(30b)"
N,0.56353591160221,"Consider the residual term
f (k,t+1)(X) −y
2
2
(31a)"
N,0.565377532228361,"=
f (k,t+1)(X) −f (k,t)(X) + f (k,t)(X) −y
2
2
(31b)"
N,0.567219152854512,"=
f (k,t)(X) −y
2
2 + 2
D
dI + dII, f (k,t)(X) −y
E
+
f (k,t+1)(X) −f (k,t)(X)
2
2.
(31c)"
N,0.569060773480663,"We will give upper bounds for the inner product terms

dI, f (k,t)(X) −y

,

dII, f (k,t)(X) −y

,"
N,0.570902394106814,"and the difference term
f (k,t+1)(X) −f (k,t)(X)
2
2, separately. Based on the property of the set
Si, we have"
N,0.572744014732965,"dI
i = −η
√n X"
N,0.574585635359116,"r /∈Si
cr ⟨∇vrL, xi⟩1(k,t)
ir
(32a) = −η nN N
X j=1"
N,0.576427255985267,"
f (k,t)(xj) −yj

x⊤
j xi
X"
N,0.578268876611418,"r /∈Si
c2
r 1(k,t)
ir
1(k,t)
jr
(32b) = −η N N
X j=1"
N,0.580110497237569,"
f (k,t)(xj) −yj
 
(H(k,t))ij −(H⊥(k,t))ij

,
(32c)"
N,0.58195211786372,"where (H⊥(k,t))ij is deﬁned as"
N,0.583793738489871,"(H⊥(k,t))ij ≜1"
N,0.585635359116022,"nx⊤
i xj n
X"
N,0.5874769797421732,"r∈Si
1(k,t)
ir
1(k,t)
jr
.
(33)"
N,0.5893186003683242,"For the inner product term

dI, f (k,t)(X) −y

, we have
D
dI, f (k,t)(X) −y
E
= −η"
N,0.5911602209944752,"N (f (k,t)(X) −y)⊤(H(k,t) −H⊥(k,t))(f (k,t)(X) −y).
(34)"
N,0.5930018416206262,Let T1 and T2 denote the following terms
N,0.5948434622467772,"T1 ≜−(f (k,t)(X) −y)⊤H(k,t)(f (k,t)(X) −y),
(35a)"
N,0.5966850828729282,"T2 ≜(f (k,t)(X) −y)⊤H⊥(k,t)(f (k,t)(X) −y).
(35b)"
N,0.5985267034990792,"With probability at least 1 −δ, T1 can be bounded as:"
N,0.6003683241252302,"T1 = −(f (k,t)(X) −y)⊤(H(k,t) −H(0) + H(0) −H∞+ H∞)(f (k,t)(X) −y)
(36a)"
N,0.6022099447513812,"⩽−(f (k,t)(X) −y)⊤(H(k,t) −H(0))(f (k,t)(X) −y)"
N,0.6040515653775322,"−(f (k,t)(X) −y)⊤(H(0) −H∞)(f (k,t)(X) −y) −λ0
f (k,t)(X) −y
2
2
(36b) x
⩽ 2
√"
NR,0.6058931860036832,"2NR
√πδα
+ N r"
NR,0.6077348066298343,ln (2N 2/δ)
N,0.6095764272559853,"2n
−λ0"
N,0.6114180478821363,"!
f (k,t)(X) −y
2
2,
(36c)"
N,0.6132596685082873,Under review as a conference paper at ICLR 2022
N,0.6151012891344383,"where x comes from Lemma 2 and Lemma 3. To bound the term T2, consider the ℓ2 norm of the
matrix H⊥(k,t). With probability at least 1 −δ, we have:"
N,0.6169429097605893,"∥H⊥(k,t)∥2 ⩽∥H⊥(k,t)∥F
(37a) =  
N
X i=1 N
X j=1"
N,0.6187845303867403,"1
n X"
N,0.6206261510128913,"r∈Si
x⊤
i xj1(k,t)
ir
1(k,t)
jr !2  1
2 (37b) ⩽N"
N,0.6224677716390423,"n |Si|
x
⩽ r"
N,0.6243093922651933,"2
π
NR"
N,0.6261510128913443,"δα ,
(37c)"
N,0.6279926335174953,"where x comes from Lemma 1. Therefore, with probability at least 1 −δ, we have T2 ⩽ r"
N,0.6298342541436464,"2
π
NR δα"
N,0.6316758747697975,"f (k,t)(X) −y
2
2.
(38)"
N,0.6335174953959485,"Combine the results of (36c) and (38):
D
dI, f (k,t)(X) −y
E
⩽η 3
√"
R,0.6353591160220995,"2R
√πδα + r"
R,0.6372007366482505,ln (2N 2/δ)
N,0.6390423572744015,"2n
−λ0 N"
N,0.6408839779005525,"!
f (k,t)(X) −y
2
2.
(39)"
N,0.6427255985267035,"For the inner product term

dII, f (k,t)(X) −y

, we ﬁrst bound ∥dII∥2
2 as follows:"
N,0.6445672191528545,"∥dII∥2
2 = N
X i=1 1
√n X r∈Si"
N,0.6464088397790055,"h
crσ

(v(k,t+1)
r
)⊤xi

−crσ

(v(k,t)
r
)⊤xi
i!2 (40a) x
⩽η2 n N
X"
N,0.6482504604051565,"i=1
|Si|
X"
N,0.6500920810313076,"r∈Si
(cr⟨∇vrL, xi⟩)2
(40b) y
⩽η2 n N
X"
N,0.6519337016574586,"i=1
|Si|
X"
N,0.6537753222836096,"r∈Si
∥∇vrL∥2
2 ∥xi∥2
2
(40c) ⩽η2N"
N,0.6556169429097606,"n |Si|2 max
r∈[n]"
N,0.6574585635359116,"∇vrL
2
2
(40d)"
N,0.6593001841620626,⩽η2|Si|2
N,0.6611418047882136,"n2
f (k,t)(X) −y
2
2,
(40e)"
N,0.6629834254143646,"where x comes from the Lipschitz continuity of the ReLU function σ(·), y holds due to Cauchy–
Schwartz inequality. Plug (18) into (40e), we have with probability at least 1 −δ:"
N,0.6648250460405156,"∥dII∥2
2 ⩽2η2R2"
N,0.6666666666666666,"πδ2α2
f (k,t)(X) −y
2
2.
(41)"
N,0.6685082872928176,"The inner product term

dII, f (k,t)(X) −y

can be bounded as
D
dII, f (k,t)(X) −y
E
⩽ √"
N,0.6703499079189686,"2ηR
√πδα"
N,0.6721915285451197,"f (k,t)(X) −y
2
2.
(42)"
N,0.6740331491712708,"Finally, the bound for the difference term is derived as"
N,0.6758747697974218,"f (k,t+1)(X) −f (k,t)(X)
2
2 ⩽ N
X i=1 η
√n n
X"
N,0.6777163904235728,"r=1
cr⟨∇vrL, xi⟩ !2"
N,0.6795580110497238,"⩽η2f (k,t)(X) −y
2
2.
(43)"
N,0.6813996316758748,"Combine the results of (39), (42) and (43):"
N,0.6832412523020258,"f (k,t+1)(X)−y
2
2 ⩽ """
N,0.6850828729281768,"1 + 8
√"
N,0.6869244935543278,"2ηR
√πδα + 2η r"
N,0.6887661141804788,ln (2N 2/δ)
N,0.6906077348066298,"2n
−2ηλ0"
N,0.6924493554327809,"N
+ η2
#
f (k,t)(X)−y
2
2. (44)"
N,0.6942909760589319,"Let R = O
  δαλ0"
N,0.6961325966850829,"N

, n = Ω

N2"
N,0.6979742173112339,"λ2
0 ln N2"
N,0.6998158379373849,"δ

, and η = O( λ0"
N,0.7016574585635359,"N ), we have"
N,0.7034990791896869,"f (k,t+1)(X) −y
2
2 ⩽

1 −ηλ0"
N,0.7053406998158379,2N
N,0.7071823204419889," f (k,t)(X) −y
2
2.
(45)"
N,0.7090239410681399,Summing up over the selected number t(k) iterations completes the proof.
N,0.7108655616942909,Under review as a conference paper at ICLR 2022
N,0.712707182320442,"C
PROOF OF THEOREM 2"
N,0.714548802946593,"Theorem 2 For FedAvg under Assumptions 1 to 3, let the learning rate η = O

λ0
τN|Ck|

and the"
N,0.716390423572744,"neural network width n = Ω

N2"
N,0.7182320441988951,"λ2
0 ln 2N2"
N,0.7200736648250461,"δ

, then with probability at least 1 −δ, the one-round loss
decay of FedAvg is
f (k+1)(X) −y
2
2 ⩽

1 −ητλ0"
N,0.7219152854511971,2N|Ck|
N,0.7237569060773481," f (k)(X) −y
2
2.
(46)"
N,0.7255985267034991,"Proof. We ﬁrst construct a different set of kernel matrices {Λ(k), Λ(k,τ)
m
} similar to Huang et al.
(2021). Let 1(k,u)
imr ≜1{⟨v(k,u)
m,r , xi⟩⩾0}, the (i, j)th entry of Λ(k,u)
m
and Λ(k,u) is deﬁned as"
N,0.7274401473296501,"(Λ(k,u)
m
)ij ≜1"
N,0.7292817679558011,"nx⊤
i xj n
X"
N,0.7311233885819521,"r=1
1(k,0)
imr 1(k,u)
jmr ,
(47a)"
N,0.7329650092081031,"(Λ(k,u))ij ≜(Λ(k,u)
m
)ij,
if (xj, yj) ∈Dm.
(47b)
Taking the difference between successive terms yields"
N,0.7348066298342542,"f (k+1)(xi) −f (k)(xi) =
1
√n n
X r=1"
N,0.7366482504604052,"h
crσ

(v(k+1)
r
)⊤xi

−crσ

(v(k)
r )⊤xi
i
.
(48)"
N,0.7384898710865562,"We decompose the difference term to the sum of dI
i and dII
i , based on the set Si and its complement:"
N,0.7403314917127072,"dI
i ≜
1
√n X"
N,0.7421731123388582,r /∈Si
N,0.7440147329650092,"h
crσ

(v(k+1)
r
)⊤xi

−crσ

(v(k)
r )⊤xi
i
,
(49a)"
N,0.7458563535911602,"dII
i ≜
1
√n X r∈Si"
N,0.7476979742173112,"h
crσ

(v(k+1)
r
)⊤xi

−crσ

(v(k)
r )⊤xi
i
.
(49b)"
N,0.7495395948434622,"Consider the residual term
f (k+1)(X) −y
2
2
(50a)"
N,0.7513812154696132,"=
f (k+1)(X) −f (k)(X) + f (k)(X) −y
2
2
(50b)"
N,0.7532228360957642,"=
f (k)(X) −y
2
2 + 2
D
dI + dII, f (k)(X) −y
E
+
f (k+1)(X) −f (k)(X)
2
2.
(50c)"
N,0.7550644567219152,"We will give upper bounds for the inner product terms

dI, f (k)(X) −y

,

dII, f (k)(X) −y

, and"
N,0.7569060773480663,"the difference term
f (k+1)(X) −f (k)(X)
2
2, separately. For an input x ∈Rd1, let f (k,u)
m
(x) ≜ 1
√n nP"
N,0.7587476979742173,"r=1
crσ(⟨v(k,u)
m,r ), x⟩). By the update rule of FedAvg, the relation between the weight vector v(k)
r"
N,0.7605893186003683,in successive communication rounds is:
N,0.7624309392265194,"v(k+1)
r
= v(k)
r
−
η
|Ck| X m∈Ck τ−1
X"
N,0.7642725598526704,"u=0
∇Lv(k,u)
r
(51a)"
N,0.7661141804788214,"= v(k)
r
−
ηcr
N√n|Ck| X m∈Ck τ−1
X u=0 X"
N,0.7679558011049724,"j∈Im
(f (k,u)
m
(xj) −yj)xj1(k,u)
jmr .
(51b)"
N,0.7697974217311234,"Based on the property of the set Si, we have"
N,0.7716390423572744,"dI
i = −1
√n X m∈Ck τ−1
X u=0 X"
N,0.7734806629834254,"r /∈Si
cr
D
v(k+1)
r
−v(k)
r , xi
E
1(k)
ir
(52a)"
N,0.7753222836095764,"= −
η
Nn|Ck| X m∈Ck τ−1
X u=0 X"
N,0.7771639042357275,r /∈Si X
N,0.7790055248618785,"j∈Im
(f (k,u)
m
(xj) −yj)x⊤
i xj1(k)
ir 1(k,u)
jmr
(52b)"
N,0.7808471454880295,"= −
η
N|Ck| X m∈Ck τ−1
X u=0 X"
N,0.7826887661141805,"j∈Im
(f (k,u)
m
(xj) −yj)
h
(Λ(k,u)
m
)ij −(Λ⊥(k,u)
m
)ij
i
.
(52c)"
N,0.7845303867403315,Under review as a conference paper at ICLR 2022
N,0.7863720073664825,"For the inner product term

dI, f (k)(X) −y

, we have"
N,0.7882136279926335,"D
dI, f (k)(X) −y
E
= −
η
N|Ck| τ−1
X"
N,0.7900552486187845,"u=0
(f (k)(X) −y)⊤(Λ(k,u) −Λ⊥(k,u))(f (k,u)
m
(X) −y).
(53)"
N,0.7918968692449355,Let T1 and T2 denote the following terms
N,0.7937384898710865,"T1 ≜−(f (k)(X) −y)⊤Λ(k,u)(f (k,u)
g
(X) −y),
(54a)"
N,0.7955801104972375,"T2 ≜(f (k)(X) −y)⊤Λ⊥(k,u)(f (k,u)
g
(X) −y),
(54b)"
N,0.7974217311233885,"where f (k,u)
g
(X) ≜[f (k,u)
1
(X1)⊤, · · · , f (k,u)
|Ck| (X|Ck|)⊤]⊤. We are going to bound T1 and T2 sepa-
rately. T1 can be written as:"
N,0.7992633517495396,"T1 = −(f (k)(X) −y)⊤(Λ(k,u) −H(0) + H(0) −H∞+ H∞)(f (k,u)
g
(X) −y)
(55a)"
N,0.8011049723756906,"= −(f (k)(X) −y)⊤(Λ(k,u) −H(0))(f (k,u)
g
(X) −y)"
N,0.8029465930018416,"−(f (k)(X) −y)⊤(H(0) −H∞)(f (k,u)
g
(X) −y)"
N,0.8047882136279927,−(f (k)(X) −y)⊤H∞(f (k)(X) −y)
N,0.8066298342541437,"−(f (k)(X) −y)⊤H∞(f (k,u)
g
(X) −f (k)(X)).
(55b)"
N,0.8084714548802947,"First, we bound the norm of f (k,u)
g
(X) −y. It can be shown that"
N,0.8103130755064457,"∥f (k,u)
m
(Xm) −ym∥2 = ∥f (k,u)
m
(Xm) −f (k,u−1)
m
(Xm) + f (k,u−1)
m
(Xm) −ym∥2
(56a)"
N,0.8121546961325967,"⩽∥f (k,u)
m
(Xm) −f (k,u−1)
m
(Xm)∥2 + ∥f (k,u−1)
m
(Xm) −ym∥2
(56b)"
N,0.8139963167587477,"x
⩽(1 + η)∥f (k,u−1)
m
(Xm) −ym∥2,
(56c)"
N,0.8158379373848987,where x holds based on the derivation of (43). Applying (56c) recursively yields
N,0.8176795580110497,"∥f (k,u)
m
(Xm) −ym∥2 ⩽(1 + η)u∥f (k)(Xm) −ym∥2.
(57)"
N,0.8195211786372008,"The bound for ∥f (k,u)
g
(X) −y∥2
2 can thus be derived as"
N,0.8213627992633518,"∥f (k,u)
g
(X) −y∥2
2 = N
X i=1"
N,0.8232044198895028,"h
f (k,u)
g
(xi) −yi
i2
(58a) =
X m∈Ck"
N,0.8250460405156538,"f (k,u)
m
(Xm) −ym
2
2
(58b)"
N,0.8268876611418048,"⩽(1 + η)2uf (k)(X) −y
2
2.
(58c)"
N,0.8287292817679558,"Second, following the steps in Lemma 2, it can be shown that with probability at least 1 −δ,"
N,0.8305709023941068,"∥Λ(k,t) −H(0)∥2 ⩽2
√"
NR,0.8324125230202578,"2NR
√πδα .
(59)"
NR,0.8342541436464088,"We also bound the difference between f (k,u)
g
(X) and f (k)(X) as follows:"
NR,0.8360957642725598,"∥f (k,u)
g
(X) −f (k)(X)∥2
x
⩽ u
X"
NR,0.8379373848987108,"v=1
∥f (k,v)
g
(X) −f (k,v−1)
g
(X)∥2
(60a) y
⩽ u
X"
NR,0.8397790055248618,"v=1
η∥f (k,v−1)
g
(X) −y∥2
(60b) z
⩽ u
X"
NR,0.8416206261510129,"v=1
η(1 + η)v−1∥f (k)(X) −y∥2
(60c)"
NR,0.8434622467771639,"= [(1 + η)u −1] ∥f (k)(X) −y∥2,
(60d)"
NR,0.8453038674033149,Under review as a conference paper at ICLR 2022
NR,0.8471454880294659,"where x holds due to triangle inequality, y comes from (43), z comes from (58c). Plugging the
results from (58c), (59), and (60d) into (55b), we have with probability at least 1 −δ, T1 ⩽ """
NR,0.848987108655617,"(1 + η)u
 
2
√"
NR,0.850828729281768,"2NR
√πδα
+ N r"
NR,0.852670349907919,ln (2N 2/δ)
N,0.85451197053407,"2n
+ κλ0 !"
N,0.856353591160221,−(1 + κ)λ0 #
N,0.858195211786372,"∥f (k)(X) −y∥2
2,
(61)"
N,0.860036832412523,"where κ is the condition number of the matrix H∞. Next, consider the bound for T2. The ℓ2 norm
of Λ⊥(k,u) can be bounded as
∥Λ⊥(k,u)∥2 ⩽∥Λ⊥(k,u)∥F
(62a) =  
N
X i=1 X m∈Ck X j∈Im"
N,0.861878453038674,"1
n X"
N,0.8637200736648251,"r∈Si
x⊤
i xj1(k)
ir 1(k,u)
jmr !2  1
2 (62b) ⩽N"
N,0.8655616942909761,"n |Si|
x
⩽ r"
N,0.8674033149171271,"2
π
NR"
N,0.8692449355432781,"δα ,
(62c)"
N,0.8710865561694291,"where x comes from Lemma 1. Therefore, we have with probability at least 1 −δ,"
N,0.8729281767955801,"T2 ⩽(1 + η)u
r"
N,0.8747697974217311,"2
π
NR δα"
N,0.8766114180478821,"f (k)(X) −y
2
2.
(63)"
N,0.8784530386740331,"Combine the results of (61) and (63):
D
dI, f (k)(X) −y
E
⩽
τ
|Ck|"
N,0.8802946593001841,""" 
η + (τ −1)"
N,0.8821362799263351,"2
η2 + o(η2)
  3
√"
R,0.8839779005524862,"2R
√πδα + s"
R,0.8858195211786372,"ln
  2N2 δ
"
N,0.8876611418047882,"2n
+ κλ0 N  "
N,0.8895027624309392,−(1 + κ)ηλ0 N
N,0.8913443830570903,"#
f (k)(X) −y
2
2. (64)"
N,0.8931860036832413,"For the inner product term

dII, f (k)(X) −y

, we ﬁrst bound ∥dII∥2
2 with probability at least 1 −δ:"
N,0.8950276243093923,"∥dII∥2
2 = N
X i=1 1
√n X r∈Si"
N,0.8968692449355433,"h
crσ

(v(k+1)
r
)⊤xi

−crσ

(v(k)
r )⊤xi
i!2 (65a) ⩽1 n N
X"
N,0.8987108655616943,"i=1
|Si|
X r∈Si"
N,0.9005524861878453,"
cr⟨v(k+1)
r
−v(k)
r , xi⟩
2
(65b) ⩽1 n N
X"
N,0.9023941068139963,"i=1
|Si|
X r∈Si "
N,0.9042357274401474,"
ηcr
N√n|Ck| X m∈Ck τ−1
X u=0 X"
N,0.9060773480662984,"j∈Im
(f (k,u)
m
(xj) −yj)1(k,u)
jmr   2 (65c) ⩽
η2"
N,0.9079189686924494,"N 2n2|Ck|2 N
X"
N,0.9097605893186004,"i=1
|Si|
X r∈Si  X m∈Ck τ−1
X u=0 X j∈Im"
N,0.9116022099447514,"f (k,u)
m
(xj) −yj   2 (65d) ⩽
η2"
N,0.9134438305709024,"N 2n2|Ck|2 N
X"
N,0.9152854511970534,"i=1
|Si|
X r∈Si X m∈Ck τ−1
X"
N,0.9171270718232044,"u=0
|Im|
f (k,u)
m
(Xm) −ym

2 !2 (65e)"
N,0.9189686924493554,"x
⩽
η2"
N,0.9208103130755064,"N 2n2|Ck|2 N
X"
N,0.9226519337016574,"i=1
|Si|
X r∈Si X m∈Ck τ−1
X"
N,0.9244935543278084,"u=0
(1 + η)u|Im|
f (k)(Xm) −ym

2 !2 (65f)"
N,0.9263351749539595,"y
⩽
1
N 2n2|Ck|2 N
X"
N,0.9281767955801105,"i=1
|Si|
X r∈Si X"
N,0.9300184162062615,"m∈Ck
((1 + η)τ −1) |Im|
f (k)(Xm) −ym

1 !2 (65g)"
N,0.9318600368324125,"z
⩽
1
Nn2|Ck|2 N
X"
N,0.9337016574585635,"i=1
|Si|
X r∈Si"
N,0.9355432780847146,"
((1 + η)τ −1)
f (k)(X) −y

2"
N,0.9373848987108656,"2
(65h)"
N,0.9392265193370166,"{
⩽
2R2"
N,0.9410681399631676,πδ2α2|Ck|2
N,0.9429097605893186,"
τη + τ(τ −1)"
N,0.9447513812154696,"2
η2 + o(η2)
2 f (k)(X) −y
2
2.
(65i)"
N,0.9465930018416207,Under review as a conference paper at ICLR 2022
N,0.9484346224677717,"where x comes from (57), y holds due to ∥a∥1 ⩽∥a∥2, z holds due to ∥a∥1 ⩽
p"
N,0.9502762430939227,"dim(a)∥a∥2, {
is from Lemma 1. With probability at least 1 −δ, the inner product term can thus be bounded as
D
dII, f (k)(X) −y
E
⩽ √"
N,0.9521178637200737,"2τR
√πδα|Ck|"
N,0.9539594843462247,"
η + (τ −1)"
N,0.9558011049723757,"2
η2 + o(η2)
 f (k)(X) −y
2
2.
(66)"
N,0.9576427255985267,The bound for the difference term is derived as
N,0.9594843462246777,"f (k+1)(X) −f (k)(X)
2
2 ⩽ N
X i=1 η
√n n
X"
N,0.9613259668508287,"r=1
cr⟨v(k+1)
r
−v(k)
r , xi⟩ !2 (67a)"
N,0.9631675874769797,"⩽
1
|Ck|2"
N,0.9650092081031307,"
τη + τ(τ −1)"
N,0.9668508287292817,"2
η2 + o(η2)
2 f (k)(X) −y
2
2.
(67b)"
N,0.9686924493554327,"Combine the results of (64), (66) and (67b):"
N,0.9705340699815838,"f (k+1)(X) −y
2
2 ⩽ ("
N,0.9723756906077348,"1 + 2ητ |Ck| ""  4
√"
R,0.9742173112338858,"2R
√πδα + s"
R,0.9760589318600368,"ln
  2N 2 δ
"
N,0.9779005524861878,"2n
+ κλ0 N  "
N,0.9797421731123389,−(1 + κ)λ0 N #
N,0.9815837937384899,+ η2τ 2
N,0.9834254143646409,|Ck|2 + o(η2)
N,0.9852670349907919,")
f (k)(X) −y
2
2. (68)"
N,0.9871086556169429,"Let R = O
  δαλ0"
N,0.988950276243094,"N

, n = Ω

N2"
N,0.990791896869245,"λ2
0 ln N2"
N,0.992633517495396,"δ

, and η = O

λ0
τN|Ck|

, we have"
N,0.994475138121547,"f (k+1)(X) −y
2
2 ⩽

1 −ητλ0"
N,0.996316758747698,2N|Ck|
N,0.998158379373849," f (k+1)(X) −y
2
2.
(69a)"
