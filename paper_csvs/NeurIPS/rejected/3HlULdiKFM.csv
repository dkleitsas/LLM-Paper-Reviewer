Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018796992481203006,"Composed Image Retrieval (CoIR) has recently gained popularity as a task that
1"
ABSTRACT,0.0037593984962406013,"considers both text and image queries together, to search for relevant images in a
2"
ABSTRACT,0.005639097744360902,"database. Most CoIR approaches require manually annotated datasets, containing
3"
ABSTRACT,0.007518796992481203,"image-text-image triplets, where the text describes a modification from the query
4"
ABSTRACT,0.009398496240601503,"image to the target image. However, manual curation of CoIR triplets is expensive
5"
ABSTRACT,0.011278195488721804,"and prevents scalability. In this work, we instead propose a scalable automatic
6"
ABSTRACT,0.013157894736842105,"dataset creation methodology that generates triplets given video-caption pairs.
7"
ABSTRACT,0.015037593984962405,"To this end, we mine paired videos with a similar caption from a large database,
8"
ABSTRACT,0.016917293233082706,"and leverage a large language model to generate the corresponding modification
9"
ABSTRACT,0.018796992481203006,"text. We automatically construct our WebVid-CoVR dataset by applying this
10"
ABSTRACT,0.020676691729323307,"procedure to the large WebVid2M collection, resulting in 1.6M triplets. Moreover,
11"
ABSTRACT,0.022556390977443608,"we introduce a new benchmark for composed video retrieval (CoVR) and contribute
12"
ABSTRACT,0.02443609022556391,"a manually annotated evaluation set, along with baseline results. We further show
13"
ABSTRACT,0.02631578947368421,"that training a CoVR model on our dataset transfers well to CoIR, improving the
14"
ABSTRACT,0.02819548872180451,"state of the art in the zero-shot setup on both the CIRR and FashionIQ benchmarks.
15"
ABSTRACT,0.03007518796992481,"Our code, datasets, and models will be made publicly available.
16"
ABSTRACT,0.03195488721804511,"“with fireworks”
“at night”
“with people”
“during show”"
ABSTRACT,0.03383458646616541,"Figure 1: Task: Composed Video Retrieval (CoVR) seeks to retrieve videos from a database
by searching with both a query image and a query text. The text typically specifies the desired
modification to the query image. In this example, a traveller might wonder how the photographed
place looks like during a fountain show, by describing several modifications, such as “during show at
night, with people, with fireworks”."
INTRODUCTION,0.03571428571428571,"1
Introduction
17"
INTRODUCTION,0.03759398496240601,"Consider the scenario where a traveller takes a picture of a landmark or scenic spot and wants to
18"
INTRODUCTION,0.039473684210526314,"discover videos that capture the essence of that location, by specifying certain conditions via text. For
19"
INTRODUCTION,0.041353383458646614,"example, the query image in Figure 1 (of a fountain in Barcelona), along with the text “during show”
20"
INTRODUCTION,0.043233082706766915,"should bring the video showcasing the fountain show. Further refining the text query such as “during
21"
INTRODUCTION,0.045112781954887216,"show at night”, would allow the traveller to decide whether to wait for the show until the night time.
22"
INTRODUCTION,0.046992481203007516,"In this work, our goal is composed video retrieval (CoVR), where the user performs such multi-modal
23"
INTRODUCTION,0.04887218045112782,"search, by querying an image of a particular visual concept and a modification text, to find videos
24"
INTRODUCTION,0.05075187969924812,"that exhibit the similar visual characteristics with the desired modification, in a dynamic context.
25"
INTRODUCTION,0.05263157894736842,Young man working
INTRODUCTION,0.05451127819548872,"with tablet and
blueprints at his new home"
INTRODUCTION,0.05639097744360902,"Young man working
with smartphone and
blueprints at his new home"
M VIDEO-CAPTION PAIRS,0.05827067669172932,2.5M video-caption pairs
M VIDEO-CAPTION PAIRS,0.06015037593984962,MTG-LLM
M VIDEO-CAPTION PAIRS,0.06203007518796992,Trained with 715 text
M VIDEO-CAPTION PAIRS,0.06390977443609022,triplet annotations q t v
M VIDEO-CAPTION PAIRS,0.06578947368421052,change tablet for
M VIDEO-CAPTION PAIRS,0.06766917293233082,smartphone
M VIDEO-CAPTION PAIRS,0.06954887218045112,"Figure 2: Method overview: We automatically mine similar caption pairs from a large video-caption
database from the Web, and use our modification text generation language model (MTG-LLM) to
describe the difference between the two captions. MTG-LLM is trained on a dataset of 715 triplet
text annotations [8]. The resulting triplet of two corresponding videos (query q and target video v)
and the modification text (t) is therefore obtained fully automatically, allowing a scalable CoVR
training data generation."
M VIDEO-CAPTION PAIRS,0.07142857142857142,"CoVR has many use cases, including but not limited to searching online videos for finding reviews of
26"
M VIDEO-CAPTION PAIRS,0.07330827067669173,"a specific product, how-to videos of a tool for specific usages, live events in specific locations, sports
27"
M VIDEO-CAPTION PAIRS,0.07518796992481203,"matches of specific players. Similar to composed image retrieval (CoIR), CoVR is also particularly
28"
M VIDEO-CAPTION PAIRS,0.07706766917293233,"useful when conveying a concept with a visual is easier and/or more accurate than only using words
29"
M VIDEO-CAPTION PAIRS,0.07894736842105263,"(e.g., unknown location/object, a specific camera view, a specific color).
30"
M VIDEO-CAPTION PAIRS,0.08082706766917293,"Given the increased momentum in vision and language research in the recent years [31, 45], CoIR has
31"
M VIDEO-CAPTION PAIRS,0.08270676691729323,"emerged as a new task [57], and since then witnessed improvements of both models and benchmarks
32"
M VIDEO-CAPTION PAIRS,0.08458646616541353,"[6, 7, 21, 28, 37, 58]. However, to the best of our knowledge, CoVR was not studied before. A key
33"
M VIDEO-CAPTION PAIRS,0.08646616541353383,"challenge in building CoVR models is the difficulty of gathering suitable training data of image-text-
34"
M VIDEO-CAPTION PAIRS,0.08834586466165413,"video triplets. We overcome this limitation by developing an automatic approach to generate triplets
35"
M VIDEO-CAPTION PAIRS,0.09022556390977443,"from existing video-caption collections. Specifically, we mine video pairs whose corresponding
36"
M VIDEO-CAPTION PAIRS,0.09210526315789473,"captions slightly differ in text space. We automatically describe this difference with a language model,
37"
M VIDEO-CAPTION PAIRS,0.09398496240601503,"which we train for a modification-text generation task. In particular, we use manually annotated
38"
M VIDEO-CAPTION PAIRS,0.09586466165413533,"triplets, each containing: (a) source caption, (b) target caption, (c) the modification text. We then
39"
M VIDEO-CAPTION PAIRS,0.09774436090225563,"finetune a large language model (LLM) [54] by inputting (a-b), and outputting (c). We assume the
40"
M VIDEO-CAPTION PAIRS,0.09962406015037593,"resulting modification to describe the difference between the corresponding videos, thus obtaining
41"
M VIDEO-CAPTION PAIRS,0.10150375939849623,"video-text-video triplets (see Figure 2 for an overview). When training our CoVR/CoIR models, we
42"
M VIDEO-CAPTION PAIRS,0.10338345864661654,"can select one or more frames from the videos, enabling multiple settings (i.e., retrieving images or
43"
M VIDEO-CAPTION PAIRS,0.10526315789473684,"videos).
44"
M VIDEO-CAPTION PAIRS,0.10714285714285714,"We apply our triplet generation approach to the WebVid2M dataset [4] which contains 2.5M Web-
45"
M VIDEO-CAPTION PAIRS,0.10902255639097744,"scraped video-caption pairs. This results in the WebVid-CoVR training dataset with 1.6M CoVR
46"
M VIDEO-CAPTION PAIRS,0.11090225563909774,"triplets. By virtue of its automatic generation procedure, WebVid-CoVR is inherently noisy. To
47"
M VIDEO-CAPTION PAIRS,0.11278195488721804,"efficiently train on such large-scale and noisy training data, we use a contrastive loss [55] and
48"
M VIDEO-CAPTION PAIRS,0.11466165413533834,"additionally sample hard negatives that have the same source caption but different target captions.
49"
M VIDEO-CAPTION PAIRS,0.11654135338345864,"We design a CoVR model based on the cross-modal BLIP [31] and use query scoring [5] to exploit
50"
M VIDEO-CAPTION PAIRS,0.11842105263157894,"information from multiple video frames. Training this model on WebVid-CoVR transfers well to the
51"
M VIDEO-CAPTION PAIRS,0.12030075187969924,"CoIR task, in both zero-shot and finetuning settings, and achieves state-of-the-art results on the CIRR
52"
M VIDEO-CAPTION PAIRS,0.12218045112781954,"and FashionIQ benchmarks in the zero-shot setup. Finally, to foster research in CoVR, we repeat
53"
M VIDEO-CAPTION PAIRS,0.12406015037593984,"our generation procedure on a separate subset of the WebVid10M dataset [4] and manually select
54"
M VIDEO-CAPTION PAIRS,0.12593984962406016,"correctly generated samples to constitute WebVid-CoVRm, a test set of 2,435 CoVR triplets. We find
55"
M VIDEO-CAPTION PAIRS,0.12781954887218044,"that our model achieves promising results on WebVid-CoVRm compared to standard baselines.
56"
M VIDEO-CAPTION PAIRS,0.12969924812030076,"To summarize, our contributions are: (i) We propose a scalable approach to automatically generate
57"
M VIDEO-CAPTION PAIRS,0.13157894736842105,"composed visual retrieval training data. We apply this pipeline to the WebVid2M dataset and generate
58"
M VIDEO-CAPTION PAIRS,0.13345864661654136,"the WebVid-CoVR training dataset with 1.6M CoVR triplets. (ii) We show that training a CoVR
59"
M VIDEO-CAPTION PAIRS,0.13533834586466165,"model on WebVid-CoVR transfers well to the CoIR task, and achieves state-of-the-art results on the
60"
M VIDEO-CAPTION PAIRS,0.13721804511278196,"CIRR and FashionIQ benchmarks in the zero-shot setup. (iii) We evaluate our model on WebVid-
61"
M VIDEO-CAPTION PAIRS,0.13909774436090225,"Table 1: Existing datasets: We compare our proposed WebVid-CoVR training dataset and its
manually annotated test set WebVid-CoVRm with existing composed visual retrieval datasets. 
denotes image, Å denotes video datasets. We contribute the largest training dataset for the natural
domain. Note that, while SynthTriplets18M is larger, the transfer performance to real images is
ineffective potentially due to a domain gap (see Table 3)."
M VIDEO-CAPTION PAIRS,0.14097744360902256,"Dataset
Type
#Triplets
#Visuals
#Unique
words
Avg. text
length
Domain"
M VIDEO-CAPTION PAIRS,0.14285714285714285,"CIRR [37]

36,554
21,185
7,129
59.51
Natural
FashionIQ [58]

30,132
7,988
4,425
27.13
Fashion
CIRCO [6]

1,020
-
-
-
Natural
LaSCo [28]

389,305
121,479
13,488
30.70
Natural
SynthTriplets18M [21]

18,000,000
-
-
-
Synthetic
WebVid-CoVR
Å
1,648,789
130,775
19,163
23.36
Natural
WebVid-CoVRm
Å
2,435
2,435
1,764
22.03
Natural"
M VIDEO-CAPTION PAIRS,0.14473684210526316,"CoVRm, a new CoVR benchmark that we manually annotate. Our code and dataset are provided in
62"
M VIDEO-CAPTION PAIRS,0.14661654135338345,"the Supplementary Material, and will be publicly released together with our models.
63"
RELATED WORK,0.14849624060150377,"2
Related Work
64"
RELATED WORK,0.15037593984962405,"Composed image retrieval (CoIR). CoIR [57] has been an active area of research in recent years [7,
65"
RELATED WORK,0.15225563909774437,"14, 25]. Most methods designed for this problem use manually annotated data for training. Some
66"
RELATED WORK,0.15413533834586465,"recent works, such as Pic2Word [47] and SEARLE [6], explore zero-shot CoIR setups where no
67"
RELATED WORK,0.15601503759398497,"manually annotated CoIR triplet is used. These approaches build on CLIP [45] and train directly on
68"
RELATED WORK,0.15789473684210525,"unlabeled image(-text) data. In contrast, we use unlabeled video-text pairs to automatically generate
69"
RELATED WORK,0.15977443609022557,"composed video retrieval (CoVR) triplets, train a CoVR model on the generated data, and study
70"
RELATED WORK,0.16165413533834586,"zero-shot and finetuning transfer of the resulting model on both CoIR and CoVR.
71"
RELATED WORK,0.16353383458646617,"Datasets for composed image retrieval. CIRR [37] and Fashion-IQ [58] are the two most widely
72"
RELATED WORK,0.16541353383458646,"used CoIR benchmarks. Both are manually annotated, hence small scale (about 30K triplets, see
73"
RELATED WORK,0.16729323308270677,"Table 1) due to the high cost implied in collecting CoIR triplets. To scale up, two concurrent works
74"
RELATED WORK,0.16917293233082706,"proposed larger, automatically generated CoIR datasets: LaSCo [28] and SynthTriplets18M [21].
75"
RELATED WORK,0.17105263157894737,"However, these two datasets are currently not publicly available. The LaSCo dataset [28] is generated
76"
RELATED WORK,0.17293233082706766,"using the visual question answering annotations and the pairing between images and counterfactual
77"
RELATED WORK,0.17481203007518797,"images in the VQAv2 dataset [3]. In detail, this dataset provides for each (image, question, answer)
78"
RELATED WORK,0.17669172932330826,"triplet a counterfactual triplet with the same question and different image and answer. In contrast, we
79"
RELATED WORK,0.17857142857142858,"do not rely on such expensive annotation schemes. SynthTriplets18M [21] uses the text-conditioned
80"
RELATED WORK,0.18045112781954886,"image editing framework InstructPix2Pix [8] to automatically generate CoIR data. Their edit text
81"
RELATED WORK,0.18233082706766918,"generation process is similar to ours, but our generation process differs in that we automatically
82"
RELATED WORK,0.18421052631578946,"mine similar videos from a dataset of unlabeled video-text pairs to construct CoVR triplets instead
83"
RELATED WORK,0.18609022556390978,"of generating visual data. In experiments, we show the superiority of our generation procedure as
84"
RELATED WORK,0.18796992481203006,"we achieve much higher CoIR results (e.g., 38% vs 19% zero-shot R@1 on CIRR while generating
85"
RELATED WORK,0.18984962406015038,"fewer data). Lastly, our WebVid-CoVR dataset is composed of videos, and not limited to still images.
86"
RELATED WORK,0.19172932330827067,"Vision-language pretraining. Many strong multi-modal models have been pretrained on large
87"
RELATED WORK,0.19360902255639098,"datasets of image-caption pairs [2, 13, 24, 27, 30, 32, 34, 38, 45, 48, 51, 67, 71] or video-caption
88"
RELATED WORK,0.19548872180451127,"pairs [1, 29, 33, 41, 42, 53, 59, 60, 68, 69, 70]. In contrast, we generate CoVR training data from
89"
RELATED WORK,0.19736842105263158,"video-caption pairs instead of directly training on them. Our data generation approach is also related
90"
RELATED WORK,0.19924812030075187,"to other generation approaches used for other tasks, e.g., action recognition [43], visual question
91"
RELATED WORK,0.20112781954887218,"answering [62, 63] and visual dialog [35]. However, unlike all these tasks, the CoVR task requires
92"
RELATED WORK,0.20300751879699247,"retrieving visual data.
93"
RELATED WORK,0.20488721804511278,"Video retrieval. Text-to-video retrieval has received great attention over the last few years [17, 18,
94"
RELATED WORK,0.20676691729323307,"19, 36, 39, 40, 46, 59, 61, 64, 65]. We also make use of multiple video frames with query scoring
95"
RELATED WORK,0.20864661654135339,"similar to [5]. However, different from these methods, we focus on composed video retrieval, where
96"
RELATED WORK,0.21052631578947367,"the query consists of both text and visual data.
97"
AUTOMATIC TRIPLET GENERATION AND COVR TRAINING,0.212406015037594,"3
Automatic Triplet Generation and CoVR Training
98"
AUTOMATIC TRIPLET GENERATION AND COVR TRAINING,0.21428571428571427,"The goal of the composed video retrieval (CoVR) task is, given an input video or image q and a
99"
AUTOMATIC TRIPLET GENERATION AND COVR TRAINING,0.2161654135338346,"modification text t, to retrieve a modified video v in a large database of videos. We wish to avoid
100"
AUTOMATIC TRIPLET GENERATION AND COVR TRAINING,0.21804511278195488,"the manual annotation of (q, t, v) triplets for training. Hence we automatically generate such triplets
101"
AUTOMATIC TRIPLET GENERATION AND COVR TRAINING,0.2199248120300752,"from Web-scraped video-caption pairs, as explained in Section 3.1 and illustrated in Figure 2. The
102"
AUTOMATIC TRIPLET GENERATION AND COVR TRAINING,0.22180451127819548,"resulting WebVid-CoVR dataset, together with its manually curated evaluation set, is presented in
103"
AUTOMATIC TRIPLET GENERATION AND COVR TRAINING,0.2236842105263158,"Section 3.2. Finally, we present how we train a CoVR model using WebVid-CoVR in Section 3.3.
104"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.22556390977443608,"3.1
Generating composed video retrieval triplets
105"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2274436090225564,"Given a large (Web-scraped) dataset of video-caption pairs (v, c), we wish to automatically generate
106"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.22932330827067668,"video-text-video CoVR triplets (q, t, v) where the text t describes a modification to the visual query
107"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.231203007518797,"q. However, the dataset of video-caption pairs neither contains annotations of paired videos, nor
108"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.23308270676691728,"modification text that describes their difference. Hence we propose a methodology to automatically
109"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2349624060150376,"mine paired videos and describe their difference, as described below. Note that for illustration, we
110"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.23684210526315788,"take as an example the WebVid2M dataset [4] with 2.5M video-caption pairs, but this methodology
111"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2387218045112782,"could be applied to other large datasets of video-text (or image-text) pairs.
112"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.24060150375939848,"Mining paired videos by pairing captions. In order to obtain paired videos, we leverage their
113"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2424812030075188,"captions. The core idea is that videos with similar captions are likely to have similar visual content.
114"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.24436090225563908,"Specifically, we consider captions that differ by a single word, excluding punctuation marks. For
115"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2462406015037594,"instance, the caption ""Young woman smiling"" is paired with ""Old woman smiling"" and ""Young couple
116"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.24812030075187969,"smiling"". In the 2M distinct captions from WebVid2M, this process allows us to identify a vast pool
117"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.25,"of 1.2M distinct caption pairs with 177K distinct captions, resulting in 3.1M paired videos.
118"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2518796992481203,"Filtering caption pairs. We wish to automatically generate the modification text between paired
119"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.25375939849624063,"videos using their (paired) captions. However, caption pairs with the same meaning are likely to
120"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2556390977443609,"result in meaningless differences. On the contrary, caption pairs that differ too much are likely to
121"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2575187969924812,"result in large visual differences that cannot be easily described. To address these issues, we filter
122"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2593984962406015,"out caption pairs that are too similar and too dissimilar. Specifically, we exclude caption pairs with
123"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.26127819548872183,"CLIP text embedding similarity ≥0.96 (e.g., ""Fit and happy young couple playing in the park""
124"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2631578947368421,"and ""Fit and happy young couple play in the park"") and caption pairs with CLIP text embedding
125"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2650375939849624,"similarity ≤0.6 (e.g., ""Zebra on a white background"" and ""Coins on a white background""). We also
126"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2669172932330827,"exclude pairs where the captions differ by a digit (which mostly consist of date in practice), or by an
127"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.26879699248120303,"out-of-vocabulary word. Finally, we remove templated captions such as ""abstract of"", ""concept of"",
128"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2706766917293233,"and ""flag of"" which are over-represented.
129"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2725563909774436,"Generating a modification text from paired captions. In order to generate a modification text
130"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2744360902255639,"between paired videos, we apply a modification text generation large language model (MTG-LLM)
131"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.27631578947368424,"to their corresponding paired captions. We describe the MTG-LLM inference process below and
132"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2781954887218045,"then explain its training details. The MTG-LLM takes as input two paired captions and generates
133"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2800751879699248,"a modification text that describes the difference between the two captions (see Fig. 2). In detail,
134"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2819548872180451,"the generation is auto-regressive, i.e., we recursively sample from the token likelihood distribution
135"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.28383458646616544,"conditioned on the previously generated tokens until an end-of-sentence token is reached. To increase
136"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2857142857142857,"the diversity of the generated samples, we use top-k sampling instead of maximum-likelihood-based
137"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.287593984962406,"methods such as beam search and its variants [56]. Note that we only generate a single modification
138"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2894736842105263,"text per caption pair for computational efficiency, but the MTG-LLM could be used to generate
139"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.29135338345864664,"multiple modification texts per caption pair which could serve as a data augmentation in future work.
140"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2932330827067669,"We now describe the training details of the MTG-LLM. We start from a LLM pretrained with a
141"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.2951127819548872,"next token prediction objective on a Web-scale text dataset [54]. We then finetune this LLM for the
142"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.29699248120300753,"MTG task on a manually annotated text dataset. In particular, we repurpose the editing dataset from
143"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.29887218045112784,"InstructPix2Pix [8], which provides a modification text and a target caption for 700 input captions. We
144"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.3007518796992481,"augment this dataset with 15 additional annotations that are useful in our use case. These examples
145"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.3026315789473684,"involve transformations such as changing singular nouns to plural (tree to trees), as well as addressing
146"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.30451127819548873,"specific edge cases. More details can be found in the Supplementary Material.
147"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.30639097744360905,"Filtering video pairs. We wish to avoid some modification texts being over-represented in the dataset
148"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.3082706766917293,"as it could harm training. Hence, if there are more than 10 video pairs associated with the same
149"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.3101503759398496,"pair of captions (therefore leading to the same modification text), we only select 10 video pairs. As
150"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.31203007518796994,"the CoVR task typically involves similar query-target video pairs, we choose pairs of videos with
151"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.31390977443609025,"Figure 3: Examples of generated CoVR triplets in WebVid-CoVR: The middle frame of each video
is shown with its corresponding caption, with the distinct word highlighted in bold. Additionally, the
generated modification text is displayed on top of each pair of videos."
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.3157894736842105,"the highest visual similarity, as measured by the CLIP visual embedding similarity computed at the
152"
GENERATING COMPOSED VIDEO RETRIEVAL TRIPLETS,0.3176691729323308,"middle frame of the videos.
153"
ANALYSIS OF WEBVID-COVR,0.31954887218045114,"3.2
Analysis of WebVid-CoVR
154"
ANALYSIS OF WEBVID-COVR,0.32142857142857145,"WebVid-CoVR: a large-scale CoVR training dataset. By applying the previously described
155"
ANALYSIS OF WEBVID-COVR,0.3233082706766917,"pipeline to the WebVid2M dataset [4], we generate WebVid-CoVR, a dataset containing 1.6M CoVR
156"
ANALYSIS OF WEBVID-COVR,0.325187969924812,"triplets, which is significantly more than prior datasets (see Table 1). On average, a video lasts
157"
ANALYSIS OF WEBVID-COVR,0.32706766917293234,"16.8 seconds, a modification text contains 4.8 words, and one target video is associated with 12.7
158"
ANALYSIS OF WEBVID-COVR,0.32894736842105265,"triplets. WebVid-CoVR is highly diverse with 131K distinct videos and 467K distinct modification
159"
ANALYSIS OF WEBVID-COVR,0.3308270676691729,"texts. Examples of CoVR triplets from the WebVid-CoVR dataset are illustrated in Figure 3. These
160"
ANALYSIS OF WEBVID-COVR,0.33270676691729323,"examples show the diversity of the data in WebVid-CoVR, and its noise due to the automatic
161"
ANALYSIS OF WEBVID-COVR,0.33458646616541354,"generation procedure. We provide further analysis of the WebVid-CoVR dataset in the supplementary
162"
ANALYSIS OF WEBVID-COVR,0.33646616541353386,"material.
163"
ANALYSIS OF WEBVID-COVR,0.3383458646616541,"WebVid-CoVRm: a new CoVR evaluation benchmark. Due to the noise in WebVid-CoVR, we
164"
ANALYSIS OF WEBVID-COVR,0.34022556390977443,"manually annotate a small test set, dubbed WebVid-CoVRm, for evaluation. For this, we first repeat
165"
ANALYSIS OF WEBVID-COVR,0.34210526315789475,"the data generation procedure described in Section 3.1, but on a different corpus of video-caption
166"
ANALYSIS OF WEBVID-COVR,0.34398496240601506,"pairs. Specifically, we consider video-caption pairs from the WebVid10M corpus [4] that are not
167"
ANALYSIS OF WEBVID-COVR,0.3458646616541353,"included in the WebVid2M dataset, resulting in a pool of 8 million video-caption pairs. This ensures
168"
ANALYSIS OF WEBVID-COVR,0.34774436090225563,"that other models using WebVid2M for pretraining have not been exposed to any of the test examples.
169"
ANALYSIS OF WEBVID-COVR,0.34962406015037595,"In the video pairs filtering stage, for each pair of captions, we here only keep one pair of videos (the
170"
ANALYSIS OF WEBVID-COVR,0.35150375939849626,"one with the highest visual similarity). This results in 163K candidate triplets that could be used for
171"
ANALYSIS OF WEBVID-COVR,0.3533834586466165,"testing purposes. We randomly sample 7K triplets that we use for validation and randomly sample
172"
ANALYSIS OF WEBVID-COVR,0.35526315789473684,"3.1K other triplets that we manually annotate as described below.
173"
ANALYSIS OF WEBVID-COVR,0.35714285714285715,"We augment the 3.1K triplets by generating two additional modification texts with the MTG-LLM.
174"
ANALYSIS OF WEBVID-COVR,0.35902255639097747,"The annotator reads the three generated modification texts, looks at three frames from the query and
175"
ANALYSIS OF WEBVID-COVR,0.3609022556390977,"target videos, and either keeps the best modification text if at least one is valid or discards the sample.
176"
ANALYSIS OF WEBVID-COVR,0.36278195488721804,"Through this meticulous annotation process, we ensure that the test set comprises high-quality and
177"
ANALYSIS OF WEBVID-COVR,0.36466165413533835,"meaningful CoVR triplets. This results in a test set of 2.4K triplets, i.e., about 23% of the examples
178"
ANALYSIS OF WEBVID-COVR,0.36654135338345867,"are considered as noisy and are discarded.
179"
TRAINING ON WEBVID-COVR,0.3684210526315789,"3.3
Training on WebVid-CoVR
180"
TRAINING ON WEBVID-COVR,0.37030075187969924,"Here, we describe our CoVR model architecture and how we train it on our WebVid-CoVR dataset.
181"
TRAINING ON WEBVID-COVR,0.37218045112781956,"CoVR-BLIP model architecture. Our model architecture builds upon a pretrained image-text model,
182"
TRAINING ON WEBVID-COVR,0.37406015037593987,"BLIP [31]. The BLIP model is pretrained on a large dataset of image-caption pairs with three vision-
183"
TRAINING ON WEBVID-COVR,0.37593984962406013,"language objectives: image-text contrastive learning, image-text matching, and image-conditioned
184"
TRAINING ON WEBVID-COVR,0.37781954887218044,"language modeling. However, BLIP is not pretrained for composed visual retrieval with both visual
185"
TRAINING ON WEBVID-COVR,0.37969924812030076,"and text inputs. Therefore we adapt BLIP to the CoIR/CoVR task as follows.
186"
TRAINING ON WEBVID-COVR,0.3815789473684211,"We use the BLIP image encoder to encode the image query. The resulting visual features and the
187"
TRAINING ON WEBVID-COVR,0.38345864661654133,"modification text are then forwarded to the BLIP image-grounded text encoder together, which
188"
TRAINING ON WEBVID-COVR,0.38533834586466165,"outputs a multi-modal embedding fi ∈Rd where d is the embedding dimension. To retrieve a target
189"
TRAINING ON WEBVID-COVR,0.38721804511278196,"video from a database of videos V , we compute embedding vectors for all possible videos as follows.
190"
TRAINING ON WEBVID-COVR,0.3890977443609023,"We uniformly sample N frames from the video and compute a weighted mean of the BLIP image
191"
TRAINING ON WEBVID-COVR,0.39097744360902253,"embeddings to obtain the video embedding vector ˆv ∈Rd. The weights are obtained by computing
192"
TRAINING ON WEBVID-COVR,0.39285714285714285,"the image-caption similarity for every video frame with BLIP image and text encoder, respectively,
193"
TRAINING ON WEBVID-COVR,0.39473684210526316,"similar to [4] in the context of text-to-video retrieval. Finally, given a multi-modal embedding fi, the
194"
TRAINING ON WEBVID-COVR,0.3966165413533835,"retrieved video is the one that maximizes the embedding similarity, i.e., arg maxv∈V (ˆv.f T
i ).
195"
TRAINING ON WEBVID-COVR,0.39849624060150374,"Training. In order to train on WebVid-CoVR, we use a contrastive learning approach [44, 55], as it
196"
TRAINING ON WEBVID-COVR,0.40037593984962405,"has been shown to be effective to learn strong multi-modal representations from large-scale noisy
197"
TRAINING ON WEBVID-COVR,0.40225563909774437,"data [41, 45]. We make several design choices to maximize its efficiency. First, we create a training
198"
TRAINING ON WEBVID-COVR,0.4041353383458647,"batch by sampling distinct target videos and for each target video, we randomly sample an associated
199"
TRAINING ON WEBVID-COVR,0.40601503759398494,"query image and modification text. This ensures that the same target video appears only once in a
200"
TRAINING ON WEBVID-COVR,0.40789473684210525,"batch and maximizes the number of different target videos that can be used as negatives in contrastive
201"
TRAINING ON WEBVID-COVR,0.40977443609022557,"learning.
202"
TRAINING ON WEBVID-COVR,0.4116541353383459,"Second, following HN-NCE [44], we use as negatives all target videos vj∈B in the batch B and
203"
TRAINING ON WEBVID-COVR,0.41353383458646614,"additionally increase the weight of most similar samples. In addition, we mine hard negative samples
204"
TRAINING ON WEBVID-COVR,0.41541353383458646,"that we select based on the captions associated with the videos in WebVid2M. Specifically, for a
205"
TRAINING ON WEBVID-COVR,0.41729323308270677,"given (qi, ti, vi) triplet, we consider as hard negatives all instances in the batch (qj, tj, vj) ∈HN(i)
206"
TRAINING ON WEBVID-COVR,0.4191729323308271,"where qi and qj have the same caption but vi and vj have different captions. In addition, to reduce
207"
TRAINING ON WEBVID-COVR,0.42105263157894735,"the number of noisy negatives with the same semantic content as a given sample i, we exclude from
208"
TRAINING ON WEBVID-COVR,0.42293233082706766,"the computation of the loss samples (qj, tj, vj) ∈P(i) for which vi and vj have the same caption.
209"
TRAINING ON WEBVID-COVR,0.424812030075188,"Formally, given a training batch B of triplets (qi, ti, vi), we minimize the following loss:
210"
TRAINING ON WEBVID-COVR,0.4266917293233083,"L(B) =
X"
TRAINING ON WEBVID-COVR,0.42857142857142855,"i∈B
{ −log(
eSi,i/τ
P"
TRAINING ON WEBVID-COVR,0.43045112781954886,"j∈B\P (i) eSi,j/τwi,j + α P"
TRAINING ON WEBVID-COVR,0.4323308270676692,"j∈HN(i) eSi,j/τ )"
TRAINING ON WEBVID-COVR,0.4342105263157895,"−log(
eSi,i/τ
P"
TRAINING ON WEBVID-COVR,0.43609022556390975,"j∈B\P (i) eSj,i/τwj,i + α P"
TRAINING ON WEBVID-COVR,0.43796992481203006,"j∈HN(i) eSj,i/τ )}"
TRAINING ON WEBVID-COVR,0.4398496240601504,"where α and τ are learnable parameters, Si,j is the cosine similarity between the multi-modal
211"
TRAINING ON WEBVID-COVR,0.4417293233082707,"embedding fi and the target video embedding ˆvi, HN(i) is the set of hard negatives, P(i) is the set
212"
TRAINING ON WEBVID-COVR,0.44360902255639095,"of noisy negatives and wi,j is set as in [44].
213"
EXPERIMENTS,0.44548872180451127,"4
Experiments
214"
EXPERIMENTS,0.4473684210526316,"In this Section, we first describe the experimental protocol including the datasets, evaluation met-
215"
EXPERIMENTS,0.4492481203007519,"rics, and implementation details (Section 4.1). We then present the results of CoVR on our new
216"
EXPERIMENTS,0.45112781954887216,"video benchmark (Section 4.2), as well as transfer results of CoIR on standard image benchmarks
217"
EXPERIMENTS,0.45300751879699247,"(Section 4.3). Finally, we provide ablations on our key components (Section 4.4).
218"
EXPERIMENTAL SETUP,0.4548872180451128,"4.1
Experimental setup
219"
EXPERIMENTAL SETUP,0.4567669172932331,"Datasets. WebVid-CoVR is our proposed training CoVR dataset presented in Section 3.2, and
220"
EXPERIMENTAL SETUP,0.45864661654135336,"WebVid-CoVRm is our new CoVR benchmark presented in Section 3.2.
221"
EXPERIMENTAL SETUP,0.4605263157894737,"CIRR [37] is a manually annotated CoIR dataset that contains open-domain natural images from
222"
EXPERIMENTAL SETUP,0.462406015037594,"NLVR2 [52]. It contains 36.5K queries annotated on 19K different images. CIRR includes two
223"
EXPERIMENTAL SETUP,0.4642857142857143,"benchmarks: a standard one with the target search space as the entire validation corpus, and a
224"
EXPERIMENTAL SETUP,0.46616541353383456,"fine-grained subset, where the search space is a subgroup of six images similar to the query image
225"
EXPERIMENTAL SETUP,0.4680451127819549,"(based on pretrained ResNet15 feature distance). The dataset is divided into training, validation, and
226"
EXPERIMENTAL SETUP,0.4699248120300752,"testing splits with 28,225/16,742, 4,181/2,265 and 4,148/2,178 queries/images, respectively.
227"
EXPERIMENTAL SETUP,0.4718045112781955,"FashionIQ [58] is a CoIR dataset that contains images of fashion products, divided into three
228"
EXPERIMENTAL SETUP,0.47368421052631576,"categories of Shirts, Dresses, and Tops/Tees. The query and target images were automatically
229"
EXPERIMENTAL SETUP,0.4755639097744361,"paired based on title similarities (crawled from the web), and modification texts were then manually
230"
EXPERIMENTAL SETUP,0.4774436090225564,"annotated. This dataset consists of 30K queries annotated on 40.5K different images. It is divided
231"
EXPERIMENTAL SETUP,0.4793233082706767,"into training and validation splits with 18,000/45,429 and 6,016/15,415 queries/images, respectively.
232"
EXPERIMENTAL SETUP,0.48120300751879697,"Table 2: Benchmarking on the WebVid-CoVRm test set: We find that training on WebVid-CoVR,
using both the visual and text input modalities, and using multiple frames to model the target video
are all important factors of CoVR performance."
EXPERIMENTAL SETUP,0.4830827067669173,"Train on WebVid-CoVR
Method
Input modalities
#frames
R@1
R@5
R@10
R@50 No"
EXPERIMENTAL SETUP,0.4849624060150376,"Random
-
-
0.08
0.21
0.49
2.34
CoVR-BLIP
Text
-
19.88
37.66
45.91
66.08
CoVR-BLIP
Visual
15
37.04
61.36
69.94
87.23
CoVR-BLIP
Visual+Text
15
15.98
33.22
41.36
59.18 Yes"
EXPERIMENTAL SETUP,0.4868421052631579,"CoVR-BLIP
Text
-
20.78
41.68
51.29
71.05
CoVR-BLIP
Visual
15
37.04
61.36
69.94
87.23
CoVR-BLIP
Visual+Text
1
53.43
80.00
87.27
97.66
CoVR-BLIP
Visual+Text
15
54.87
80.99
88.30
98.11"
EXPERIMENTAL SETUP,0.48872180451127817,"Table 3: State-of-the-art comparison on the CIRR test set: Our model benefits from training on
WebVid-CoVR in the zero-shot setting, and in the finetuning setting where it performs competitively.
† denotes results reported by [37]."
EXPERIMENTAL SETUP,0.4906015037593985,"Recall@K
Rsubset@K
Mode
Method
Pretraining Data
K=1
K=5
K=10
K=50
K=1
K=2
K=3"
EXPERIMENTAL SETUP,0.4924812030075188,"Train
(CIRR)"
EXPERIMENTAL SETUP,0.4943609022556391,"TIRG [57]†
-
14.61
48.37
64.08
90.03
22.67
44.97
65.14
TIRG+LastConv [57]†
-
11.04
35.68
51.27
83.29
23.82
45.65
64.55
MAAF [15]†
-
10.31
33.03
48.30
80.06
21.05
41.81
61.60
MAAF-BERT [15]†
-
10.12
33.10
48.01
80.57
22.04
42.41
62.14
MAAF-IT [15]†
-
9.90
32.86
48.83
80.27
21.17
42.04
60.91
MAAF-RP [15]†
-
10.22
33.32
48.68
81.84
21.41
42.17
61.60
ARTEMIS [14]
-
16.96
46.10
61.31
87.73
39.99
62.20
75.67
CIRPLANT [37]
-
19.55
52.55
68.39
92.38
39.20
63.03
79.49
LF-BLIP [7, 28]
-
20.89
48.07
61.16
83.71
50.22
73.16
86.82
CompoDiff [21]
SynthTriplets18M [21]
22.35
54.36
73.41
91.77
35.84
56.11
76.60
Combiner [7]
-
33.59
65.35
77.35
95.21
62.39
81.81
92.02
CASE [28]
-
48.00
79.11
87.25
97.57
75.88
90.58
96.00
CASE [28]
LaSCo [28]
48.68
79.98
88.51
97.49
76.39
90.12
95.86
CASE [28]
LaSCo [28]+COCO [10]
49.35
80.02
88.75
97.47
76.48
90.37
95.71"
EXPERIMENTAL SETUP,0.49624060150375937,"CoVR-BLIP
-
49.33
78.51
86.53
94.53
75.81
88.29
92.99
CoVR-BLIP
WebVid-CoVR
50.55
79.23
87.30
94.70
75.69
88.58
93.33"
EXPERIMENTAL SETUP,0.4981203007518797,"Zero
Shot"
EXPERIMENTAL SETUP,0.5,"Random†
-
0.04
0.22
0.44
2.18
16.67
33.33
50.00
CompoDiff [21]
SynthTriplets18M [21]
19.37
53.81
72.02
90.85
28.96
49.21
67.03
Pic2Word [47]
Conceptual Captions [49]
23.90
51.70
65.30
87.80
-
-
-
CASE [28]
LaSCo [28]
30.89
60.75
73.88
92.84
60.17
80.17
90.41
CASE [28]
LaSCo [28]+COCO [10]
35.40
65.78
78.53
94.63
64.29
82.66
91.61"
EXPERIMENTAL SETUP,0.5018796992481203,"CoVR-BLIP
-
19.76
41.23
50.89
71.64
63.04
81.01
89.37
CoVR-BLIP
WebVid-CoVR
38.55
66.80
77.25
91.61
69.42
84.22
91.16"
EXPERIMENTAL SETUP,0.5037593984962406,"Evaluation metrics. Following standard evaluation protocols [37], we report the video retrieval
233"
EXPERIMENTAL SETUP,0.5056390977443609,"recall at rank 1, 5, 10, and 50. Recall at rank k (R@k) quantifies the number of times the correct
234"
EXPERIMENTAL SETUP,0.5075187969924813,"video is among the top k results. MeanR denotes the average of R@1, R@5, R@10, and R@50.
235"
EXPERIMENTAL SETUP,0.5093984962406015,"Higher recall means better performance.
236"
EXPERIMENTAL SETUP,0.5112781954887218,"Implementation details. For our MTG-LLM, we use LLaMA 7B model [54] that we finetune for
237"
EXPERIMENTAL SETUP,0.5131578947368421,"one epoch with an initial learning rate of 3e−5 for MTG. For our CoVR model, we use the BLIP
238"
EXPERIMENTAL SETUP,0.5150375939849624,"with ViT-L [16] at 384 pixels finetuned for text-image retrieval on COCO and freeze the ViT for
239"
EXPERIMENTAL SETUP,0.5169172932330827,"computational efficiency. We train our CoVR model on WebVid-CoVR for 3 epochs with a batch size
240"
EXPERIMENTAL SETUP,0.518796992481203,"of 2048 and an initial learning rate of 1e−5. To finetune on CIRR/FashionIQ, we train for 6/3 epochs
241"
EXPERIMENTAL SETUP,0.5206766917293233,"with a batch size of 2048/1024 and an initial learning rate of 5e−5/1e−4. Experiments are conducted
242"
EXPERIMENTAL SETUP,0.5225563909774437,"on 4 NVIDIA A100-SXM4-80GB GPUs. More details are included in the Supplementary Material.
243"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5244360902255639,"4.2
Composed video retrieval results
244"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5263157894736842,"We report CoVR results on our WebVid-CoVRm test set in Table 2. For models trained on WebVid-
245"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5281954887218046,"CoVR, we find that using both modalities is crucial for performance, as the model with visual and
246"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5300751879699248,"text inputs outperforms both the text-only and the visual-only models. Furthermore, using multiple
247"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5319548872180451,"target video frames is beneficial, as the model with 15 frames improves over the model with 1 frame.
248"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5338345864661654,"Table 4: State-of-the-art comparison on the FashionIQ validation set: Our model benefits from
training on WebVid-CoVR in the zero-shot setting, and in the finetuning setting. CC3M is Conceptual
Captions 3M [9]."
COMPOSED VIDEO RETRIEVAL RESULTS,0.5357142857142857,"Pretraining
Shirt
Dress
Toptee
Average
Mode
Method
Data
R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5375939849624061,"Train
(FashionIQ)"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5394736842105263,"JVSM [11]
-
12.0
27.1
10.7
25.9
13.0
26.9
11.9
26.6
CIRPLANT [37]
-
17.53
38.81
17.45
40.41
61.64
45.38
18.87
41.53
TRACE w/BER [23]
-
20.80
40.80
22.70
44.91
24.22
49.80
22.57
46.19
VAL w/GloVe [12]
-
22.38
44.15
22.53
44.00
27.53
51.68
24.15
46.61
MAAF [15]
-
21.3
44.2
23.8
48.6
27.9
53.6
24.3
48.8
CurlingNet [66]
-
21.45
44.56
26.15
53.24
30.12
55.23
25.90
51.01
RTIC-GCN [50]
-
23.79
47.25
29.15
54.04
31.61
57.98
28.18
53.09
CoSMo[26]
-
24.90
49.18
25.64
50.30
29.21
57.46
26.58
52.31
ARTEMIS[14]
-
21.78
43.64
27.16
52.40
29.20
53.83
26.05
50.29
DCNet[25]
-
23.95
47.30
28.95
56.07
30.44
58.29
27.78
53.89
SAC w/BERT[22]
-
28.02
51.86
26.52
51.01
32.70
61.23
29.08
54.70
FashionVLP[20]
-
31.89
58.44
32.42
60.29
38.51
68.79
34.27
62.51
LF-CLIP (Combiner) [7] -
36.36
58.00
31.63
56.67
38.19
62.42
35.39
59.03
LF-BLIP [7, 28]
-
25.39
43.57
25.31
44.05
26.54
44.48
25.75
43.98
CASE [28]
LaSCo [28]
48.48
70.23
47.44
69.36
50.18
72.24
48.79
70.68"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5413533834586466,"CoVR-BLIP
-
48.04
68.20
44.92
68.91
52.47
74.71
48.48
70.61
CoVR-BLIP
WebVid-CoVR
48.48
67.86
45.31
68.37
53.14
73.94
48.98
70.06"
COMPOSED VIDEO RETRIEVAL RESULTS,0.543233082706767,"Zero
Shot"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5451127819548872,"Random
-
00.16
00.79
00.26
1.31
00.19
00.95
00.06
00.32
Pic2Word [47]
CC3M [9]
26.2
43.6
20.0
40.2
27.9
47.4
24.7
43.7"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5469924812030075,"CoVR-BLIP
-
16.68
30.67
13.44
31.93
17.85
35.70
15.99
32.77
CoVR-BLIP
WebVid-CoVR
30.37
46.27
21.81
39.02
30.85
49.06
27.68
44.78"
COMPOSED VIDEO RETRIEVAL RESULTS,0.5488721804511278,"Table 5: Data size: We experimentally validate the importance of the number of videos used for data
generation and of filtering the generated data, evaluated by downstream performance on WebVid-
CoVRm (test), CIRR (test), and FashionIQ (val). All models are trained for the same number of
iterations on the generated data. Training batches are made up with distinct target videos."
COMPOSED VIDEO RETRIEVAL RESULTS,0.5507518796992481,"Initial
Generated
WebVid-CoVRm
CIRR
FashionIQ
#videos
#target videos
#triplets
Filtering
R@1
MeanR
R@1
MeanR
R@10
MeanR"
"-
-
-",0.5526315789473685,"0
-
-
-
15.98
37.44
19.76
45.88
15.99
24.38"
K,0.5545112781954887,"200k
10k
4k
✓
25.13
51.22
33.90
63.32
26.22
35.83
500k
14k
66k
✓
46.04
74.24
38.31
67.80
28.76
37.78
1M
38k
269k
✓
48.46
76.47
38.51
67.95
28.41
37.38
2.5M
130k
1.6M
✓
54.87
80.57
38.55
68.55
27.68
36.23"
M,0.556390977443609,"2.5M
212k
3.6M
✗
49.86
76.12
34.10
64.77
25.81
34.16"
M,0.5582706766917294,"We also evaluate baselines that are not trained on WebVid-CoVR and that directly apply the pretrained
249"
M,0.5601503759398496,"BLIP model [31] to the CoVR task. These baselines outperform the random baseline but underperform
250"
M,0.5620300751879699,"compared to models trained on WebVid-CoVR, showing the benefit of our automatically generated
251"
M,0.5639097744360902,"training dataset. Note that BLIP [31] is pretrained for image-text retrieval but not for image-text-
252"
M,0.5657894736842105,"image retrieval, hence the drop in performance when applied directly to CoVR with both input
253"
M,0.5676691729323309,"modalities compared to only using visual information.
254"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5695488721804511,"4.3
Transfer learning to composed image retrieval
255"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5714285714285714,"While our focus is video retrieval, we also experiment with transferring our CoVR models to image
256"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5733082706766918,"retrieval tasks on standard CoIR benchmarks. We define zero-shot CoIR as not using any manually
257"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.575187969924812,"annotated CoIR triplet for training. We perform zero-shot CoIR by directly applying our model trained
258"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5770676691729323,"on our automatically generated WebVid-CoVR dataset to CoIR tasks and also explore finetuning our
259"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5789473684210527,"model on the training set of the downstream benchmark.
260"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5808270676691729,"Tables 3 and 4 report results on CIRR and Fashion-IQ datasets, respectively. These results show that
261"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5827067669172933,"our model highly benefits from training on WebVid-CoVR, especially in the zero-shot setting, on
262"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5845864661654135,"both datasets. In addition, our model achieves state-of-the-art zero-shot performance on both CIRR
263"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5864661654135338,"and FashionIQ, and performs competitively in the finetuning setting on both benchmarks.
264"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5883458646616542,"Table 6: Modification text generation: We compare our MTG-LLM to a rule-based MTG baseline
and observe important gains in the downstream performance of the model trained on the generated
data. All models are trained for the same number of iterations on the generated data."
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5902255639097744,"WebVid-CoVR
CIRR
Model
R@1
R@5
R@10
R@50
R@1
R@5
R@10
R@50"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5921052631578947,"Rule-based
43.00
70.10
79.38
94.58
15.90
39.06
52.36
79.22
MTG-LLM
54.87
80.99
88.30
98.11
38.55
66.80
77.25
91.61"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5939849624060151,"Table 7: Ablations on training strategies: Constructing batches of distinct target videos (and not
CoVR triplets) and our hard negative mining both benefit the downstream CoVR/CoIR performance."
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5958646616541353,"WebVid-CoVRm
CIRR
Iteration
Hard negatives
R@1
R@5
R@10
R@50
R@1
R@5
R@10
R@50"
TRANSFER LEARNING TO COMPOSED IMAGE RETRIEVAL,0.5977443609022557,"Triplets
✓
47.68
76.14
85.46
97.25
38.53
65.66
76.22
90.34
Videos
✗
54.00
80.53
88.01
98.03
38.34
66.75
77.21
91.42
Videos
✓
54.87
80.99
88.30
98.11
38.55
66.80
77.25
91.61"
ABLATION STUDIES,0.599624060150376,"4.4
Ablation studies
265"
ABLATION STUDIES,0.6015037593984962,"In this Section, we ablate the importance of several key aspects of our method by evaluating the
266"
ABLATION STUDIES,0.6033834586466166,"downstream performance of the model trained only on WebVid-CoVR.
267"
ABLATION STUDIES,0.6052631578947368,"Importance of data scale. In Table 5, we evaluate the importance of the scale of the dataset of
268"
ABLATION STUDIES,0.6071428571428571,"video-captions used in our generation pipeline. We construct subsets of videos such that larger ones
269"
ABLATION STUDIES,0.6090225563909775,"include smaller ones, and only keep triplets that contain the sampled videos for training. We find that
270"
ABLATION STUDIES,0.6109022556390977,"results steadily increase when using more videos, demonstrating that our method largely benefits from
271"
ABLATION STUDIES,0.6127819548872181,"scaling the size of the seed dataset of video-captions. We also observe the importance of the filtering
272"
ABLATION STUDIES,0.6146616541353384,"techniques described in Section 3.1, as the model trained on unfiltered generated data underperforms.
273"
ABLATION STUDIES,0.6165413533834586,"Modification text generation. We use a large language model finetuned for modification text
274"
ABLATION STUDIES,0.618421052631579,"generation as explained in Section 3.1. We here compare this solution to a rule-based baseline that
275"
ABLATION STUDIES,0.6203007518796992,"uses several templates to generate the modification text given the two captions that differ by one word.
276"
ABLATION STUDIES,0.6221804511278195,"Specifically, the modification text is based on the two different words from the captions. We generate
277"
ABLATION STUDIES,0.6240601503759399,"templates that use these words and chose one at random during training. These templates include
278"
ABLATION STUDIES,0.6259398496240601,"variations such as ""Remove txt_diff1"" and ""Change txt_diff1 for txt_diff2"". A full list of all
279"
ABLATION STUDIES,0.6278195488721805,"the templates can be seen in the Supplementary Material. In Table 6, we show that our large language
280"
ABLATION STUDIES,0.6296992481203008,"model generates better modification texts than the rule-based baseline, by evaluating the results of
281"
ABLATION STUDIES,0.631578947368421,"the model trained on the generated data. Qualitative examples comparing the two approaches are
282"
ABLATION STUDIES,0.6334586466165414,"provided in the Supplementary Material.
283"
ABLATION STUDIES,0.6353383458646616,"Training strategies. In Table 7, we first show the benefit on WebVid-CoVR of training by iterating
284"
ABLATION STUDIES,0.6372180451127819,"on target videos instead of CoVR triplets. This is to avoid having the same target video appearing
285"
ABLATION STUDIES,0.6390977443609023,"multiple times in a training batch, hence increasing the number of correct negatives that are used in
286"
ABLATION STUDIES,0.6409774436090225,"the contrastive loss. Furthermore, sampling hard negatives, as described in Section 3.3, also slightly
287"
ABLATION STUDIES,0.6428571428571429,"benefits the downstream performance.
288"
ABLATION STUDIES,0.6447368421052632,"5
Conclusions, Limitations, and Societal Impacts
289"
ABLATION STUDIES,0.6466165413533834,"In this work, we studied the new task of CoVR by proposing a simple yet effective methodology to
290"
ABLATION STUDIES,0.6484962406015038,"create automatic training data. Our results on several benchmarks (including our manually curated
291"
ABLATION STUDIES,0.650375939849624,"video benchmark, as well as existing image benchmarks) suggest that, while noisy, such an automated
292"
ABLATION STUDIES,0.6522556390977443,"and scalable approach can provide effective CoVR model training. One potential limitation of our
293"
ABLATION STUDIES,0.6541353383458647,"method is that our dataset may not depict some visible changes due to the way we generate triplets.
294"
ABLATION STUDIES,0.6560150375939849,"Moroever, our modification text generation model is suboptimal due to only inputting text (i.e.,
295"
ABLATION STUDIES,0.6578947368421053,"without looking at images). Future work can incorporate visually grounded modification generation.
296"
ABLATION STUDIES,0.6597744360902256,"Societal impact. Our model constitutes a generic multi-modal search tool, but is not intended for
297"
ABLATION STUDIES,0.6616541353383458,"a specific application. While there are helpful use cases such as online shopping, traveling, and
298"
ABLATION STUDIES,0.6635338345864662,"personal development (i.e., how-to), there may be potential privacy risks associated to surveillance
299"
ABLATION STUDIES,0.6654135338345865,"applications, searching for a specific person in videos.
300"
REFERENCES,0.6672932330827067,"References
301"
REFERENCES,0.6691729323308271,"[1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and
302"
REFERENCES,0.6710526315789473,"Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,
303"
REFERENCES,0.6729323308270677,"audio and text. NeurIPS, 2021. 3
304"
REFERENCES,0.674812030075188,"[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
305"
REFERENCES,0.6766917293233082,"Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual
306"
REFERENCES,0.6785714285714286,"language model for few-shot learning. In NeurIPS, 2022. 3
307"
REFERENCES,0.6804511278195489,"[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence
308"
REFERENCES,0.6823308270676691,"Zitnick, and Devi Parikh. VQA: Visual Question Answering. In ICCV, 2015. 3
309"
REFERENCES,0.6842105263157895,"[4] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video
310"
REFERENCES,0.6860902255639098,"and image encoder for end-to-end retrieval. In ICCV, 2021. 2, 4, 5, 6
311"
REFERENCES,0.6879699248120301,"[5] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. A CLIP-hitchhiker’s guide to
312"
REFERENCES,0.6898496240601504,"long video retrieval. arXiv:2205.08508, 2022. 2, 3
313"
REFERENCES,0.6917293233082706,"[6] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. Zero-shot
314"
REFERENCES,0.693609022556391,"composed image retrieval with textual inversion. arXiv:2303.15247, 2023. 2, 3
315"
REFERENCES,0.6954887218045113,"[7] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Effective conditioned
316"
REFERENCES,0.6973684210526315,"and composed image retrieval combining CLIP-based features. In CVPR, 2022. 2, 3, 7, 8
317"
REFERENCES,0.6992481203007519,"[8] Tim Brooks, Aleksander Holynski, and Alexei A Efros. InstructPix2Pix: Learning to follow
318"
REFERENCES,0.7011278195488722,"image editing instructions. arXiv:2211.09800, 2022. 2, 3, 4
319"
REFERENCES,0.7030075187969925,"[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing
320"
REFERENCES,0.7048872180451128,"Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts. In CVPR, 2021.
321 8
322"
REFERENCES,0.706766917293233,"[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár,
323"
REFERENCES,0.7086466165413534,"and C Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server.
324"
REFERENCES,0.7105263157894737,"arXiv:1504.00325, 2015. 7
325"
REFERENCES,0.7124060150375939,"[11] Yanbei Chen and Loris Bazzani. Learning joint visual semantic matching embeddings for
326"
REFERENCES,0.7142857142857143,"language-guided retrieval. In ECCV, 2020. 8
327"
REFERENCES,0.7161654135338346,"[12] Yanbei Chen, Shaogang Gong, and Loris Bazzani. Image search with text feedback by visiolin-
328"
REFERENCES,0.7180451127819549,"guistic attention learning. In CVPR, 2020. 8
329"
REFERENCES,0.7199248120300752,"[13] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,
330"
REFERENCES,0.7218045112781954,"and Jingjing Liu. UNITER: Universal image-text representation learning. In ECCV, 2020. 3
331"
REFERENCES,0.7236842105263158,"[14] Ginger Delmas, Rafael Sampaio de Rezende, Gabriela Csurka, and Diane Larlus. ARTEMIS:
332"
REFERENCES,0.7255639097744361,"Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity. In ICLR, 2022.
333"
REFERENCES,0.7274436090225563,"3, 7, 8
334"
REFERENCES,0.7293233082706767,"[15] Eric Dodds, Jack Culpepper, Simao Herdade, Yang Zhang, and Kofi Boakye. Modality-agnostic
335"
REFERENCES,0.731203007518797,"attention fusion for visual search with text feedback. CoRR, abs/2007.00145, 2020. 7, 8
336"
REFERENCES,0.7330827067669173,"[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
337"
REFERENCES,0.7349624060150376,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
338"
REFERENCES,0.7368421052631579,"Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
339"
REFERENCES,0.7387218045112782,"recognition at scale. In ICLR, 2021. 7
340"
REFERENCES,0.7406015037593985,"[17] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. CLIP2Video: Mastering video-text retrieval
341"
REFERENCES,0.7424812030075187,"via image clip. arXiv:2106.11097, 2021. 3
342"
REFERENCES,0.7443609022556391,"[18] Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan. CLIP2TV:
343"
REFERENCES,0.7462406015037594,"an empirical study on transformer-based methods for video-text retrieval. arXiv:2111.05610,
344"
REFERENCES,0.7481203007518797,"2021. 3
345"
REFERENCES,0.75,"[19] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridgeformer:
346"
REFERENCES,0.7518796992481203,"Bridging video-text retrieval with multiple choice questions. In CVPR, 2022. 3
347"
REFERENCES,0.7537593984962406,"[20] Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal, Rakesh Chada, Yue Wu, Varsha Hedau,
348"
REFERENCES,0.7556390977443609,"and Pradeep Natarajan. FashionVLP: Vision language transformer for fashion retrieval with
349"
REFERENCES,0.7575187969924813,"feedback. In CVPR, 2022. 8
350"
REFERENCES,0.7593984962406015,"[21] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, and Sangdoo Yun.
351"
REFERENCES,0.7612781954887218,"CompoDiff: Versatile composed image retrieval with latent diffusion. arXiv:2303.11916, 2023.
352"
REFERENCES,0.7631578947368421,"2, 3, 7
353"
REFERENCES,0.7650375939849624,"[22] Surgan Jandial, Pinkesh Badjatiya, Pranit Chawla, Ayush Chopra, Mausoom Sarkar, and Balaji
354"
REFERENCES,0.7669172932330827,"Krishnamurthy. SAC: Semantic attention composition for text-conditioned image retrieval. In
355"
REFERENCES,0.768796992481203,"WACV, 2022. 8
356"
REFERENCES,0.7706766917293233,"[23] Surgan Jandial, Ayush Chopra, Pinkesh Badjatiya, Pranit Chawla, Mausoom Sarkar, and Balaji
357"
REFERENCES,0.7725563909774437,"Krishnamurthy. TRACE: Transform aggregate and compose visiolinguistic representations for
358"
REFERENCES,0.7744360902255639,"image search with text feedback. CoRR, abs/2009.01485, 2020. 8
359"
REFERENCES,0.7763157894736842,"[24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan
360"
REFERENCES,0.7781954887218046,"Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
361"
REFERENCES,0.7800751879699248,"with noisy text supervision. In ICML, 2021. 3
362"
REFERENCES,0.7819548872180451,"[25] Jongseok Kim, Youngjae Yu, Hoeseong Kim, and Gunhee Kim. Dual compositional learning in
363"
REFERENCES,0.7838345864661654,"interactive image retrieval. AAAI, 2021. 3, 8
364"
REFERENCES,0.7857142857142857,"[26] Seungmin Lee, Dongwan Kim, and Bohyung Han. CoSMo: Content-style modulation for image
365"
REFERENCES,0.7875939849624061,"retrieval with text feedback. In CVPR, 2021. 8
366"
REFERENCES,0.7894736842105263,"[27] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less
367"
REFERENCES,0.7913533834586466,"is more: ClipBERT for video-and-language learning via sparse sampling. In CVPR, 2021. 3
368"
REFERENCES,0.793233082706767,"[28] Matan Levy, Rami Ben-Ari, Nir Darshan, and Dani Lischinski. Data roaming and early fusion
369"
REFERENCES,0.7951127819548872,"for composed image retrieval. arXiv:2303.09429, 2023. 2, 3, 7, 8
370"
REFERENCES,0.7969924812030075,"[29] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and
371"
REFERENCES,0.7988721804511278,"prompt: Video-and-language pre-training with entity prompts. In CVPR, 2022. 3
372"
REFERENCES,0.8007518796992481,"[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image
373"
REFERENCES,0.8026315789473685,"pre-training with frozen image encoders and large language models. In ICML, 2023. 3
374"
REFERENCES,0.8045112781954887,"[31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: Bootstrapping language-
375"
REFERENCES,0.806390977443609,"image pre-training for unified vision-language understanding and generation. In ICML, 2022. 2,
376"
REFERENCES,0.8082706766917294,"5, 8
377"
REFERENCES,0.8101503759398496,"[32] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven
378"
REFERENCES,0.8120300751879699,"Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum
379"
REFERENCES,0.8139097744360902,"distillation. In NeurIPS, 2021. 3
380"
REFERENCES,0.8157894736842105,"[33] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu.
HERO:
381"
REFERENCES,0.8176691729323309,"Hierarchical encoder for video+language omni-representation pre-training. In EMNLP, 2020. 3
382"
REFERENCES,0.8195488721804511,"[34] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang,
383"
REFERENCES,0.8214285714285714,"Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for
384"
REFERENCES,0.8233082706766918,"vision-language tasks. In ECCV, 2020. 3
385"
REFERENCES,0.825187969924812,"[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning.
386"
REFERENCES,0.8270676691729323,"arXiv:2304.08485, 2023. 3
387"
REFERENCES,0.8289473684210527,"[36] Yuqi Liu, Pengfei Xiong, Luhui Xu, Shengming Cao, and Qin Jin. TS2-Net: Token shift and
388"
REFERENCES,0.8308270676691729,"selection transformer for text-video retrieval. In ECCV, 2022. 3
389"
REFERENCES,0.8327067669172933,"[37] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval
390"
REFERENCES,0.8345864661654135,"on real-life images with pre-trained vision-and-language models. In ICCV, 2021. 2, 3, 6, 7, 8
391"
REFERENCES,0.8364661654135338,"[38] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic
392"
REFERENCES,0.8383458646616542,"visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019. 3
393"
REFERENCES,0.8402255639097744,"[39] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. CLIP4Clip:
394"
REFERENCES,0.8421052631578947,"An empirical study of CLIP for end to end video clip retrieval. arXiv:2104.08860, 2021. 3
395"
REFERENCES,0.8439849624060151,"[40] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-CLIP:
396"
REFERENCES,0.8458646616541353,"End-to-end multi-grained contrastive learning for video-text retrieval. In ACMMM, 2022. 3
397"
REFERENCES,0.8477443609022557,"[41] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew
398"
REFERENCES,0.849624060150376,"Zisserman. End-to-end learning of visual representations from uncurated instructional videos.
399"
REFERENCES,0.8515037593984962,"In CVPR, 2020. 3, 6
400"
REFERENCES,0.8533834586466166,"[42] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and
401"
REFERENCES,0.8552631578947368,"Josef Sivic. HowTo100M: Learning a text-video embedding by watching hundred million
402"
REFERENCES,0.8571428571428571,"narrated video clips. In ICCV, 2019. 3
403"
REFERENCES,0.8590225563909775,"[43] Arsha Nagrani, Chen Sun, David Ross, Rahul Sukthankar, Cordelia Schmid, and Andrew
404"
REFERENCES,0.8609022556390977,"Zisserman. Speech2action: Cross-modal supervision for action recognition. In CVPR, 2020. 3
405"
REFERENCES,0.8627819548872181,"[44] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende,
406"
REFERENCES,0.8646616541353384,"Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distillation, and hard
407"
REFERENCES,0.8665413533834586,"negatives for vision-language pre-training. In arXiv, 2023. 6
408"
REFERENCES,0.868421052631579,"[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
409"
REFERENCES,0.8703007518796992,"wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
410"
REFERENCES,0.8721804511278195,"Sutskever. Learning transferable visual models from natural language supervision. In ICML,
411"
REFERENCES,0.8740601503759399,"2021. 2, 3, 6
412"
REFERENCES,0.8759398496240601,"[46] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fa-
413"
REFERENCES,0.8778195488721805,"had Shahbaz Khan. Fine-tuned clip models are efficient video learners. In CVPR, 2023.
414 3
415"
REFERENCES,0.8796992481203008,"[47] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and
416"
REFERENCES,0.881578947368421,"Tomas Pfister. Pic2word: Mapping pictures to words for zero-shot composed image retrieval.
417"
REFERENCES,0.8834586466165414,"CVPR, 2023. 3, 7, 8
418"
REFERENCES,0.8853383458646616,"[48] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman,
419"
REFERENCES,0.8872180451127819,"Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-
420"
REFERENCES,0.8890977443609023,"5B: An open large-scale dataset for training next generation image-text models. In NeurIPS
421"
REFERENCES,0.8909774436090225,"Datasets and Benchmarks Track, 2022. 3
422"
REFERENCES,0.8928571428571429,"[49] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A
423"
REFERENCES,0.8947368421052632,"cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of
424"
REFERENCES,0.8966165413533834,"the 56th Annual Meeting of the Association for Computational Linguistics, 2018. 7
425"
REFERENCES,0.8984962406015038,"[50] Minchul Shin, Yoonjae Cho, ByungSoo Ko, and Geonmo Gu. RTIC: Residual Learning for
426"
REFERENCES,0.900375939849624,"Text and Image Composition using Graph Convolutional Network. arXiv:2104.03015, 2021. 8
427"
REFERENCES,0.9022556390977443,"[51] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT:
428"
REFERENCES,0.9041353383458647,"Pre-training of generic visual-linguistic representations. In ICLR, 2019. 3
429"
REFERENCES,0.9060150375939849,"[52] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for
430"
REFERENCES,0.9078947368421053,"reasoning about natural language grounded in photographs. In ACL, 2019. 6
431"
REFERENCES,0.9097744360902256,"[53] Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan Yang, and Jianlong Fu. Long-form
432"
REFERENCES,0.9116541353383458,"video-language pre-training with multimodal temporal contrastive learning. In NeurIPS, 2022.
433 3
434"
REFERENCES,0.9135338345864662,"[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
435"
REFERENCES,0.9154135338345865,"thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
436"
REFERENCES,0.9172932330827067,"Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
437"
REFERENCES,0.9191729323308271,"language models. arXiv:2302.13971, 2023. 2, 4, 7
438"
REFERENCES,0.9210526315789473,"[55] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
439"
REFERENCES,0.9229323308270677,"predictive coding. arXiv:1807.03748, 2018. 2, 6
440"
REFERENCES,0.924812030075188,"[56] Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee,
441"
REFERENCES,0.9266917293233082,"David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural
442"
REFERENCES,0.9285714285714286,"sequence models. arXiv:1610.02424, 2016. 4
443"
REFERENCES,0.9304511278195489,"[57] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing
444"
REFERENCES,0.9323308270676691,"text and image for image retrieval - an empirical odyssey. In CVPR, 2019. 2, 3, 7
445"
REFERENCES,0.9342105263157895,"[58] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and
446"
REFERENCES,0.9360902255639098,"Rogério Feris. Fashion IQ: A new dataset towards retrieving images by natural language
447"
REFERENCES,0.9379699248120301,"feedback. In CVPR, 2021. 2, 3, 6
448"
REFERENCES,0.9398496240601504,"[59] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze,
449"
REFERENCES,0.9417293233082706,"Luke Zettlemoyer, and Christoph Feichtenhofer. VideoCLIP: Contrastive pre-training for
450"
REFERENCES,0.943609022556391,"zero-shot video-text understanding. In EMNLP, 2021. 3
451"
REFERENCES,0.9454887218045113,"[60] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu,
452"
REFERENCES,0.9473684210526315,"and Baining Guo. Advancing high-resolution video-language representation with large-scale
453"
REFERENCES,0.9492481203007519,"video transcriptions. In CVPR, 2022. 3
454"
REFERENCES,0.9511278195488722,"[61] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo.
455"
REFERENCES,0.9530075187969925,"CLIP-ViP: Adapting pre-trained image-text model to video-language representation alignment.
456"
REFERENCES,0.9548872180451128,"arXiv, 2022. 3
457"
REFERENCES,0.956766917293233,"[62] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask:
458"
REFERENCES,0.9586466165413534,"Learning to answer questions from millions of narrated videos. In ICCV, 2021. 3
459"
REFERENCES,0.9605263157894737,"[63] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Learning to
460"
REFERENCES,0.9624060150375939,"answer visual questions from web videos. IEEE TPAMI, 2022. 3
461"
REFERENCES,0.9642857142857143,"[64] Jianwei Yang, Yonatan Bisk, and Jianfeng Gao. TACo: Token-aware cascade contrastive
462"
REFERENCES,0.9661654135338346,"learning for video-text alignment. arXiv, 2021. 3
463"
REFERENCES,0.9680451127819549,"[65] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang,
464"
REFERENCES,0.9699248120300752,"Zhenguo Li, Xin Jiang, and Chunjing Xu. FILIP: Fine-grained interactive language-image
465"
REFERENCES,0.9718045112781954,"pre-training. In ICLR, 2022. 3
466"
REFERENCES,0.9736842105263158,"[66] Youngjae Yu, Seunghwan Lee, Yuncheol Choi, and Gunhee Kim. CurlingNet: Compositional
467"
REFERENCES,0.9755639097744361,"learning between images and text for fashionIQ data. arXiv:2003.1229, 2020. 8
468"
REFERENCES,0.9774436090225563,"[67] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong
469"
REFERENCES,0.9793233082706767,"Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for
470"
REFERENCES,0.981203007518797,"computer vision. arXiv:2111.11432, 2021. 3
471"
REFERENCES,0.9830827067669173,"[68] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi,
472"
REFERENCES,0.9849624060150376,"Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. MERLOT Reserve: Neural script
473"
REFERENCES,0.9868421052631579,"knowledge through vision and language and sound. In CVPR, 2022. 3
474"
REFERENCES,0.9887218045112782,"[69] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi,
475"
REFERENCES,0.9906015037593985,"and Yejin Choi. MERLOT: Multimodal neural script knowledge models. In NeurIPS, 2021. 3
476"
REFERENCES,0.9924812030075187,"[70] Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Rohit Girdhar. Learning video representations
477"
REFERENCES,0.9943609022556391,"from large language models. In CVPR, 2023. 3
478"
REFERENCES,0.9962406015037594,"[71] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao.
479"
REFERENCES,0.9981203007518797,"Unified vision-language pre-training for image captioning and VQA. In AAAI, 2020. 3
480"
