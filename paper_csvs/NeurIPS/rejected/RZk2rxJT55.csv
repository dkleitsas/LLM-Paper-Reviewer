Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010204081632653062,"We study Leaky ResNets, which interpolate between ResNets (˜L = 0) and Fully-
1"
ABSTRACT,0.0020408163265306124,"Connected nets (˜L →∞) depending on an ’effective depth’ hyper-parameter ˜L.
2"
ABSTRACT,0.003061224489795918,"In the infinite depth limit, we study ’representation geodesics’ Ap: continuous
3"
ABSTRACT,0.004081632653061225,"paths in representation space (similar to NeuralODEs) from input p = 0 to output
4"
ABSTRACT,0.00510204081632653,"p = 1 that minimize the parameter norm of the network. We give a Lagrangian
5"
ABSTRACT,0.006122448979591836,"and Hamiltonian reformulation, which highlight the importance of two terms: a
6"
ABSTRACT,0.007142857142857143,"kinetic energy which favors small layer derivatives ∂pAp and a potential energy
7"
ABSTRACT,0.00816326530612245,"that favors low-dimensional representations, as measured by the ’Cost of Identity’.
8"
ABSTRACT,0.009183673469387756,"The balance between these two forces offers an intuitive understanding of feature
9"
ABSTRACT,0.01020408163265306,"learning in ResNets. We leverage this intuition to explain the emergence of a
10"
ABSTRACT,0.011224489795918367,"bottleneck structure, as observed in previous work: for large ˜L the potential energy
11"
ABSTRACT,0.012244897959183673,"dominates and leads to a separation of timescales, where the representation jumps
12"
ABSTRACT,0.013265306122448979,"rapidly from the high dimensional inputs to a low-dimensional representation,
13"
ABSTRACT,0.014285714285714285,"move slowly inside the space of low-dimensional representations, before jumping
14"
ABSTRACT,0.015306122448979591,"back to the potentially high-dimensional outputs. Inspired by this phenomenon, we
15"
ABSTRACT,0.0163265306122449,"train with an adaptive layer step-size to adapt to the separation of timescales.
16"
INTRODUCTION,0.017346938775510204,"1
Introduction
17"
INTRODUCTION,0.018367346938775512,"Feature learning is generally considered to be at the center of the recent successes of deep neural
18"
INTRODUCTION,0.019387755102040816,"networks (DNNs), but it also remains one of the least understood aspects of DNN training.
19"
INTRODUCTION,0.02040816326530612,"There is a rich history of empirical analysis of the features learned by DNNs, for example the
20"
INTRODUCTION,0.02142857142857143,"appearance of local edge detections in CNNs with a striking similarity to the biological visual cortex
21"
INTRODUCTION,0.022448979591836733,"[19], feature arithmetic properties of word embeddings [22], similarities between representations
22"
INTRODUCTION,0.02346938775510204,"at different layers [18, 20], or properties such as Neural Collapse [24] to name a few. While some
23"
INTRODUCTION,0.024489795918367346,"of these phenomenon have been studied theoretically [3, 8, 27], a more general theory of feature
24"
INTRODUCTION,0.025510204081632654,"learning in DNNs is still lacking.
25"
INTRODUCTION,0.026530612244897958,"For shallow networks, there is now strong evidence that the first weight matrix is able to recognize a
26"
INTRODUCTION,0.027551020408163266,"low-dimensional projection of the inputs that determines the output (assuming this structure is present)
27"
INTRODUCTION,0.02857142857142857,"[4, 2, 1]. A similar phenomenon appears in linear networks, where the network is biased towards
28"
INTRODUCTION,0.02959183673469388,"learning low-rank functions and low-dimensional representations in its hidden layers [13, 21, 29].
29"
INTRODUCTION,0.030612244897959183,"But in both cases the learned features are restricted to depend linearly on the inputs, and the feature
30"
INTRODUCTION,0.03163265306122449,"learning happens in the very first weight matrix, whereas it has been observed that features increase
31"
INTRODUCTION,0.0326530612244898,"in complexity throughout the layers [31].
32"
INTRODUCTION,0.0336734693877551,"The linear feature learning ability of shallow networks has inspired a line of work that postulates that
33"
INTRODUCTION,0.03469387755102041,"the weight matrices learn to align themselves with the backward gradients and that by optimizing for
34"
INTRODUCTION,0.03571428571428571,"this alignment directly, one can achieve similar feature learning abilities even in deep nets [5, 25].
35"
INTRODUCTION,0.036734693877551024,"For deep nonlinear networks, a theory that has garnered a lot of interest is the Information Bottleneck
36"
INTRODUCTION,0.03775510204081633,"[28], which observed amongst other things that the inner representations appear to maximize their
37"
INTRODUCTION,0.03877551020408163,"mutual information with the outputs, while minimizing the mutual information with the inputs. A
38"
INTRODUCTION,0.03979591836734694,"limitation of this theory is its reliance on the notion of mutual information which has no obvious
39"
INTRODUCTION,0.04081632653061224,"definition for empirical distributions, which lead to some criticism [26].
40"
INTRODUCTION,0.04183673469387755,"A recent theory that is similar to the Information Bottleneck but with a focus on the
41"
INTRODUCTION,0.04285714285714286,"dimensionality/rank of the representations and weight matrices rather than the mutual information is
42"
INTRODUCTION,0.04387755102040816,"the Bottleneck rank/Bottleneck structure [16, 15, 30]: which describes how, for large depths, most of
43"
INTRODUCTION,0.044897959183673466,"the representations will have approximately the same low dimension, which equals the Bottleneck
44"
INTRODUCTION,0.04591836734693878,"rank of the task (the minimal dimension that the inputs can be projected to while still allowing
45"
INTRODUCTION,0.04693877551020408,"for fitting the outputs). The intuitive explanation for this bias is that a smaller parameter norm is
46"
INTRODUCTION,0.04795918367346939,"required to (approximately) represent the identity on low-dimensional representations rather than
47"
INTRODUCTION,0.04897959183673469,"high dimensional ones. Some other types of low-rank bias have been observed in recent work [9, 14].
48"
INTRODUCTION,0.05,"In this paper we will focus on describing the Bottleneck structure in ResNets, and formalize the
49"
INTRODUCTION,0.05102040816326531,"notion of ‘cost of identity’ as a driving force for the bias towards low dimensional representation.
50"
INTRODUCTION,0.05204081632653061,"The ResNet setup allows us to consider the continuous paths in representation space from input to
51"
INTRODUCTION,0.053061224489795916,"output, similar to the NeuralODE [6], and by adding weight decay, we can analyze representation
52"
INTRODUCTION,0.05408163265306123,"geodesics, which are paths that minimize parameter norm, as already studied in [23].
53"
LEAKY RESNETS,0.05510204081632653,"1.1
Leaky ResNets
54"
LEAKY RESNETS,0.05612244897959184,"Our goal is to study a variant of the NeuralODE [6, 23] approximation of ResNet with leaky skip
55"
LEAKY RESNETS,0.05714285714285714,"connections and with L2-regularization. The classical NeuralODE describes the continuous evolution
56"
LEAKY RESNETS,0.058163265306122446,"of the activations αp(x) ∈Rw starting from α0(x) = x at the input layer p = 0 and then follows
57"
LEAKY RESNETS,0.05918367346938776,∂pαp(x) = Wpσ(αp(x))
LEAKY RESNETS,0.06020408163265306,"for the w × (w + 1) matrices Wp and the nonlinearity σ : Rw →Rw+1 which maps a vector z to
58"
LEAKY RESNETS,0.061224489795918366,"σ(z) = ( [z1]+
. . .
[zw]+
1 ) , applying the ReLU nonlinearity entrywise and appending a
59"
LEAKY RESNETS,0.06224489795918367,"new entry with value 1. Thanks to the appended 1 we do not need any explicit bias, since the last
60"
LEAKY RESNETS,0.06326530612244897,"column Wp,·w+1 of the weights replaces the bias.
61"
LEAKY RESNETS,0.06428571428571428,"This can be thought of as a continuous version of the traditional ResNet with activations αℓ(x) for
62"
LEAKY RESNETS,0.0653061224489796,"ℓ= 1, . . . , L: αℓ+1(x) = αℓ(x) + Wℓσ(αℓ(x)).
63"
LEAKY RESNETS,0.0663265306122449,"We will focus on Leaky ResNets, a variant of ResNets that interpolate between ResNets and FCNNs,
64"
LEAKY RESNETS,0.0673469387755102,"by tuning the strength of the skip connections leading to the following ODE with parameter ˜L:
65"
LEAKY RESNETS,0.06836734693877551,∂pαp(x) = −˜Lαp(x) + Wpσ(αp(x)).
LEAKY RESNETS,0.06938775510204082,"This can be thought of as the continuous version of αℓ+1(x) = (1 −˜L)αℓ(x) + Wℓσ(αℓ(x)). As we
66"
LEAKY RESNETS,0.07040816326530612,"will see, the parameter ˜L plays a similar role as the depth in a FCNN.
67"
LEAKY RESNETS,0.07142857142857142,"Finally we will be interested describing the paths that minimize a cost with L2-regularization
68"
LEAKY RESNETS,0.07244897959183673,"min
Wp
1
N N
X"
LEAKY RESNETS,0.07346938775510205,"i=1
∥f ∗(xi) −α1(xi)∥2 + λ 2˜L Z 1"
LEAKY RESNETS,0.07448979591836735,"0
∥Wp∥2
F dp."
LEAKY RESNETS,0.07551020408163266,The scaling of λ
LEAKY RESNETS,0.07653061224489796,"˜L for the regularization term will be motivated in Section 1.2.
69"
LEAKY RESNETS,0.07755102040816327,"This type of optimization has been studied in [23] without leaky connections, but we will describe in
70"
LEAKY RESNETS,0.07857142857142857,"this paper large ˜L behavior which leads to a so-called Bottleneck structure [16, 15] as a result of a
71"
LEAKY RESNETS,0.07959183673469387,"separation of time scales in p.
72"
A FEW SYMMETRIES,0.08061224489795918,"1.2
A Few Symmetries
73"
A FEW SYMMETRIES,0.08163265306122448,"Changing the leakage parameter ˜L is equivalent (up to constants) to changing the integration range
74"
A FEW SYMMETRIES,0.0826530612244898,"[0, 1] or to scaling the outputs.
75"
A FEW SYMMETRIES,0.0836734693877551,"Integration range: Consider the weights Wp on the range [0, 1] and leakage parameter ˜L, leading
76"
A FEW SYMMETRIES,0.08469387755102041,"to activations αp. Then stretching the weights to a new range [0, c], by defining W ′
q = 1"
A FEW SYMMETRIES,0.08571428571428572,"cWq/c for
77"
A FEW SYMMETRIES,0.08673469387755102,"q ∈[0, c], and dividing the leakage parameter by c, stretches the activations α′
q = αp/c:
78"
A FEW SYMMETRIES,0.08775510204081632,"∂qα′
q(x) = −
˜L
c α′
q(x) + 1"
A FEW SYMMETRIES,0.08877551020408163,"c Wq/cσ(α′
q(x)) = 1"
A FEW SYMMETRIES,0.08979591836734693,"c ∂pαq/2(x),"
A FEW SYMMETRIES,0.09081632653061225,"and the parameter norm is simply divided by c:
R c
0
W ′
q
2 dq = 1"
A FEW SYMMETRIES,0.09183673469387756,"c
R 1
0 ∥Wp∥2 dp.
79"
A FEW SYMMETRIES,0.09285714285714286,"This implies that a path on the range [0, c] with leakage parameter ˜L = 1 is equivalent to a path on
80"
A FEW SYMMETRIES,0.09387755102040816,"the range [0, 1] with leakage parameter ˜L = c up to a factor of c in front of the parameter weights.
81"
A FEW SYMMETRIES,0.09489795918367347,"For this reason, instead of modeling different depths as changing the integration range, we will keep
82"
A FEW SYMMETRIES,0.09591836734693877,"the integration range to [0, 1] for convenience but change the leakage parameter ˜L instead. To get rid
83"
A FEW SYMMETRIES,0.09693877551020408,"of the factor in front of the integral, we choose a regularization term of the form λ"
A FEW SYMMETRIES,0.09795918367346938,"˜L. From now on, we
84"
A FEW SYMMETRIES,0.09897959183673469,"call ˜L the (effective) depth of the network.
85"
A FEW SYMMETRIES,0.1,"Note that this also suggests that in the absence of leakage (˜L = 0), changing the range of integration
86"
A FEW SYMMETRIES,0.10102040816326531,"has no effect on the effective depth, since 2˜L = 0 too. Instead, in the absence of leakage, the effective
87"
A FEW SYMMETRIES,0.10204081632653061,"depth can be increased by scaling the outputs as we now show.
88"
A FEW SYMMETRIES,0.10306122448979592,"Output scaling: Given a path Wp on the [0, 1] (for simplicity, we assume that there are no bias, i.e.
89"
A FEW SYMMETRIES,0.10408163265306122,"Wp,·w+1 = 0), then increasing the leakage by a constant ˜L →˜L + c leads to a scaled down path
90"
A FEW SYMMETRIES,0.10510204081632653,"α′
p = e−cpαp. Indeed we have α′
0(x) = α0(x) and
91"
A FEW SYMMETRIES,0.10612244897959183,"∂pα′
p(x) = −(˜L + c)α′
p(x) + Wpσ(α′
p(x)) = e−cp (∂pαp(x) −cαp(x)) = ∂p(e−cpαp(x))."
A FEW SYMMETRIES,0.10714285714285714,"Thus a nonleaky ResNet ˜L = 0 with very large outputs α1(x) is equivalent to a leaky ResNet ˜L > 0
92"
A FEW SYMMETRIES,0.10816326530612246,"with scaled down outputs e−˜Lα1(x). Such large outputs are common when training on cross-entropy
93"
A FEW SYMMETRIES,0.10918367346938776,"loss, and other similar losses that are only minimized at infinitely large outputs. When trained on
94"
A FEW SYMMETRIES,0.11020408163265306,"such losses, it has been shown that the outputs of neural nets will keep on growing during training
95"
A FEW SYMMETRIES,0.11122448979591837,"[12, 7], suggesting that when training ResNets on such a loss, the effective depth increases during
96"
A FEW SYMMETRIES,0.11224489795918367,"training (though quite slowly).
97"
LAGRANGIAN REFORMULATION,0.11326530612244898,"1.3
Lagrangian Reformulation
98"
LAGRANGIAN REFORMULATION,0.11428571428571428,"The optimization of Leaky ResNets can be reformulated, leading to a Lagrangian form.
99"
LAGRANGIAN REFORMULATION,0.11530612244897959,"First observe that the weights Wp at any minimizer can be expressed in terms of the matrix of
100"
LAGRANGIAN REFORMULATION,0.11632653061224489,"activations Ap = αp(X) ∈Rw×N over the whole training set X ∈Rw×N (similar to [17]):
101"
LAGRANGIAN REFORMULATION,0.11734693877551021,Wp = (˜LAp + ∂pAp)σ(Ap)+
LAGRANGIAN REFORMULATION,0.11836734693877551,"where (·)+ is the pseudo-inverse.
102"
LAGRANGIAN REFORMULATION,0.11938775510204082,"We therefore consider the equivalent optimization over the activations Ap:
103"
LAGRANGIAN REFORMULATION,0.12040816326530612,"min
Ap:A0=X
1
N ∥f ∗(X) −A1∥2 + λ 2˜L Z 1 0"
LAGRANGIAN REFORMULATION,0.12142857142857143,"˜LAp + ∂pAp

2"
LAGRANGIAN REFORMULATION,0.12244897959183673,Kp dp.
LAGRANGIAN REFORMULATION,0.12346938775510204,"This is our first encounter with the norm ∥M∥Kp = ∥Mσ(Ap)+∥F corresponding to the scalar
104"
LAGRANGIAN REFORMULATION,0.12448979591836734,"product ⟨A, B⟩Kp = Tr

AK+
p B

for Kp = σ(Ap)T σ(Ap) that will play a central role in our
105"
LAGRANGIAN REFORMULATION,0.12551020408163266,"upcoming analysis. By convention, we say that ∥M∥Kp = ∞if M does not lie in the image of Kp,
106"
LAGRANGIAN REFORMULATION,0.12653061224489795,"i.e. ImM T ⊈ImKp.
107"
LAGRANGIAN REFORMULATION,0.12755102040816327,"It can be helpful to decompose this loss along the different neurons
108"
LAGRANGIAN REFORMULATION,0.12857142857142856,"min
Ap:A0=X w
X i=1"
LAGRANGIAN REFORMULATION,0.12959183673469388,"1
N ∥f ∗
i (X) −A1,i∥2 + λ 2˜L Z 1 0"
LAGRANGIAN REFORMULATION,0.1306122448979592,"˜LAp,i· + ∂pAp,i·

2"
LAGRANGIAN REFORMULATION,0.13163265306122449,"Kp
dp,"
LAGRANGIAN REFORMULATION,0.1326530612244898,"Leading to a particle flow behavior, where the neurons Ap,i· ∈RN are the particles. At first glance, it
109"
LAGRANGIAN REFORMULATION,0.1336734693877551,"appears that there is no interaction between the particles, but remember that the norm ∥·∥Kp depends
110"
LAGRANGIAN REFORMULATION,0.1346938775510204,"on the covariance Kp = Pw
i=1 σ(Ai·)σ(Ai·)T , leading to a global interaction between the neurons.
111"
LAGRANGIAN REFORMULATION,0.1357142857142857,"If we assume that ImAT
p ⊂Imσ(Ap)T , we can decompose the inside of the integral as three terms:
112 1
2˜L"
LAGRANGIAN REFORMULATION,0.13673469387755102,"˜LAp + ∂pAp

2"
LAGRANGIAN REFORMULATION,0.1377551020408163,"K+
p
=
˜L
2 ∥Ap∥2
Kp + ˜L ⟨∂pAp, Ap⟩K+
p + 1"
LAGRANGIAN REFORMULATION,0.13877551020408163,"2˜L
∥∂pAp∥2
Kp ."
LAGRANGIAN REFORMULATION,0.13979591836734695,"The middle term ⟨∂pAp, Ap⟩K+
p plays a relatively minor role in our analysis1, so we focus more on
113"
LAGRANGIAN REFORMULATION,0.14081632653061224,"the two other terms:
114"
LAGRANGIAN REFORMULATION,0.14183673469387756,"Cost of identity ∥Ap∥2
Kp / potential energy −
˜L
2 ∥Ap∥2
Kp: This term can be interpreted as a form of
115"
LAGRANGIAN REFORMULATION,0.14285714285714285,"potential energy, since it only depends on the representation Ap and not its derivative ∂pAp. We call
116"
LAGRANGIAN REFORMULATION,0.14387755102040817,"it the cost of identity (COI), since it is the Frobenius norm of the smallest weight matrix Wp such that
117"
LAGRANGIAN REFORMULATION,0.14489795918367346,"Wpσ(Ap) = Ap. The COI can be interpreted as measuring the dimensionality of the representation,
118"
LAGRANGIAN REFORMULATION,0.14591836734693878,"inspired by the fact if the representations Ap is non-negative (and there is no bias β = 0), then
119"
LAGRANGIAN REFORMULATION,0.1469387755102041,"Ap = σ(Ap) and the COI simply equals the rank ∥Ap∥2
Kp = RankAp (this interpretation is further
120"
LAGRANGIAN REFORMULATION,0.14795918367346939,"justified in Section 1.4). We follow the convention of defining the potential energy as the negative of
121"
LAGRANGIAN REFORMULATION,0.1489795918367347,"the term that appears in the Lagrangian, so that the Hamiltonian equals the sum of these two energies.
122"
LAGRANGIAN REFORMULATION,0.15,"Kinetic energy
1
2˜L ∥∂pAp∥2
Kp: This term measures the size of the representation derivative ∂pAp
123"
LAGRANGIAN REFORMULATION,0.1510204081632653,"w.r.t. the Kp norm. It favors paths p 7→Ap that do not move too fast, especially along directions
124"
LAGRANGIAN REFORMULATION,0.1520408163265306,"where σ(Ap) is small.
125"
LAGRANGIAN REFORMULATION,0.15306122448979592,"This suggests that the local optimal paths must balance two objectives that are sometimes opposed:
126"
LAGRANGIAN REFORMULATION,0.1540816326530612,"the kinetic energy favors going from input representation to output representation in a ‘straight line’
127"
LAGRANGIAN REFORMULATION,0.15510204081632653,"that minimizes the path length, the COI on the other hand favors paths that spends most of the path in
128"
LAGRANGIAN REFORMULATION,0.15612244897959185,"low-dimensional representations that have a low COI. The balance between these two goals shifts
129"
LAGRANGIAN REFORMULATION,0.15714285714285714,"as the depth ˜L grows, and for large depths it becomes optimal for the network to rapidly move to a
130"
LAGRANGIAN REFORMULATION,0.15816326530612246,"representation of smallest possible dimension (not too small that it becomes impossible to map back
131"
LAGRANGIAN REFORMULATION,0.15918367346938775,"to the outputs), remain for most of the layers inside the space of low-dimensional representations,
132"
LAGRANGIAN REFORMULATION,0.16020408163265307,"and finally move rapidly to the output representation; even if this means doing a large ‘detour’ and
133"
LAGRANGIAN REFORMULATION,0.16122448979591836,"having a large kinetic energy. The main goal of this paper is to describe this general behavior.
134"
LAGRANGIAN REFORMULATION,0.16224489795918368,"Note that one could imagine that as ˜L →∞it would always be optimal to first go to the minimal
135"
LAGRANGIAN REFORMULATION,0.16326530612244897,"COI representation which is the zero representation Ap = 0, but once the network reaches a zero
136"
LAGRANGIAN REFORMULATION,0.16428571428571428,"representation, it can only learn constant representations afterwards (the matrix Kp = 11T is then
137"
LAGRANGIAN REFORMULATION,0.1653061224489796,"rank 1 and its image is the space of constant vectors). So the network must find a representation that
138"
LAGRANGIAN REFORMULATION,0.1663265306122449,"minimizes the COI under the condition that there is a path from this representation to the outputs.
139"
LAGRANGIAN REFORMULATION,0.1673469387755102,"Remark. While this interpretation and decomposition is a pleasant and helpful intuition, it is rather
140"
LAGRANGIAN REFORMULATION,0.1683673469387755,"difficult to leverage for theoretical proofs directly. The problem is that we will focus on regimes
141"
LAGRANGIAN REFORMULATION,0.16938775510204082,"where the representations Ap and σ(Ap) are approximately low-dimensional (since those are the
142"
LAGRANGIAN REFORMULATION,0.1704081632653061,"representations that locally minimize the COI), leading to an unbounded pseudo-inverse σ(Ap)+.
143"
LAGRANGIAN REFORMULATION,0.17142857142857143,"This is balanced by the fact that (˜LAp + ∂pAp) is small along the directions where σ(Ap)+ explodes,
144"
LAGRANGIAN REFORMULATION,0.17244897959183675,"ensuring a finite weight matrix norm
˜LAp + ∂pAp

2"
LAGRANGIAN REFORMULATION,0.17346938775510204,"K+
p
. But the suppression of (˜LAp + ∂pAp)
145"
LAGRANGIAN REFORMULATION,0.17448979591836736,"along these bad directions usually comes from cancellations, i.e. ∂pAp ≈−˜LAp. In such cases, the
146"
LAGRANGIAN REFORMULATION,0.17551020408163265,"decomposition in three terms of the Lagrangian is ill adapted since all three terms are infinite and
147"
LAGRANGIAN REFORMULATION,0.17653061224489797,"cancel each other to yield a finite sum
˜LAp + ∂pAp

2"
LAGRANGIAN REFORMULATION,0.17755102040816326,"Kp. One of our goal is to save this intuition
148"
LAGRANGIAN REFORMULATION,0.17857142857142858,"and prove a similar decomposition with stable equivalent to the cost of identity and kinetic energy
149"
LAGRANGIAN REFORMULATION,0.17959183673469387,"where K+
p is replaced by the bounded (Kp + γI)+ for the right choice of γ.
150"
"IN
LINEAR
NETWORKS",0.18061224489795918,"1In
linear
networks
σ
=
id
it
can
actually
be
discarded,
since
it
is
integrable
R 1
0 Tr

∂pApσ(Ap)+σ(Ap)+T AT
p

dp
=
log |A1|+ −log |A0|+, where |·|+ is pseudo-determinant,
the product of the non-zero singular values. Since its integral only depends on the endpoints, it has no impact on
the representation path in between, which is the focus of this paper. In nonlinear networks, we are not able to
discard in such a manner, but we will see that in the rest of analysis the two other terms play a central role, while
the second term plays less role."
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.1816326530612245,"1.4
Cost of Identity as a Measure of Dimensionality
151"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.1826530612244898,"The cost of identity can be thought of as a measure of dimensionality of the representation. It is
152"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.1836734693877551,"obvious for non-negative representations because ∥Ap∥2
K+
p = ∥ApAp+∥2
F = RankAp, but in general,
153"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.1846938775510204,"it can be shown to upper bound a notion of ‘stable rank’:
154"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.18571428571428572,"Proposition 1. ∥Aσ(A)+∥2
F ≥∥A∥2
∗
∥A∥2
F for the nuclear norm ∥A∥∗= PRankA
i=1
si(A).
155"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.186734693877551,"Proof. We know that ∥σ(A)∥F ≤∥A∥F , therefore ∥Aσ(A)+∥2
F ≥min∥B∥F ≤∥A∥F ∥AB+∥2
F which
156"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.18775510204081633,"is minimized when B =
∥A∥F
√ ∥A∥∗ √"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.18877551020408162,"A, yielding the result.
157"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.18979591836734694,"The stable rank ∥A∥2
∗
∥A∥2
F is upper bounded by RankA, with equality if all non-zero singular values
158"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.19081632653061226,"of A are equal, and it is lower bound by the more common notion of stable rank ∥A∥2
F
∥A∥2
op , because
159"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.19183673469387755,"P si max si ≥P s2
i for the singular values si.
160"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.19285714285714287,"Note that in contrast to the COI which is a very unstable quantity because of the pseudo-inverse, the
161"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.19387755102040816,"ratio ∥A∥2
∗
∥A∥2
F is continuous except at A = 0. This also makes it much easier to compute empirically
162"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.19489795918367347,"than the COI itself.
163"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.19591836734693877,"We know that the COI matches the dimension or rank for positive representations, but it turns out that
164"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.19693877551020408,"the local minima of the COI that are stable under the addition of a new neuron are all positive:
165"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.19795918367346937,"Proposition 2. A local minimum of A 7→∥Aσ(A)+∥2
F is said to be stable if it remains a local
166"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.1989795918367347,"minimum after concatenating a zero vector A′ =

A
0"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.2,"
∈R(w+1)×N. All stable minima are
167"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.2010204081632653,"non-negative, and satisfy ∥Aσ(A)+∥2
F = RankA.
168"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.20204081632653062,"Proof. The COI of the nearby point

A
ϵz"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.2030612244897959,"
for z ∈Imσ(A)T equals
169"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.20408163265306123,"Tr
h
(AT A + ϵ2zzT )
 
(σ(A)T σ(A) + ϵ2σ(z)σ(z)T +i"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.20510204081632652,"=
Aσ(A)+2 + ϵ2 zT σ(A)+2 −ϵ2 σ(z)T σ(A)+σ(A)+T AT 2 + O(ϵ4)."
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.20612244897959184,"Assume by contradiction that there is a i = 1, . . . , N such that σ(A·i) ̸= A·i, then choosing
170"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.20714285714285716,"z = σ(A)T σ(A·i) we have σ(z) = z and the two ϵ2 terms are negative:
171"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.20816326530612245,"ϵ2 ∥σ(Ai)∥2 −ϵ2 ∥Ai∥2 < 0,"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.20918367346938777,"which implies that A′ it is not a local minimum.
172"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.21020408163265306,"These stable minima will play a significant role in the rest of our analysis, as we will see that for large
173"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.21122448979591837,"˜L the representations Ap of most layers will be close to one such local minimum. Now we are not
174"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.21224489795918366,"able to rule out the existence of non-stable local minima (nor guarantee that they are avoided with
175"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.21326530612244898,"high probability), but one can show that all strict local minima of wide enough networks are stable.
176"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.21428571428571427,"Actually we can show something stronger, starting from any non-stable local minimum there is a
177"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.2153061224489796,"constant loss path that connects it to a saddle:
178"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.2163265306122449,"Proposition 3. If w > N(N + 1) then if ˆA ∈Rw×N is local minimum of A 7→∥Aσ(A)+∥2
F that is
179"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.2173469387755102,"not non-negative, then there is a continuous path At of constant COI such that A0 = ˆA and A1 is a
180"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.21836734693877552,"saddle.
181"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.2193877551020408,"This could explain why a noisy GD would avoid such negative/non-stable minima, since there is
182"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.22040816326530613,"no ‘barrier’ between the minima and a lower one, one could diffuse along the path described in
183"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.22142857142857142,"Proposition 3 until reaching a saddle and going towards a lower COI minima. But there seems to be
184"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.22244897959183674,"something else that pushes away from such non-negative minima, as in our experiments with full
185"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.22346938775510203,"population GD we have only observed stable/non-negative local minimas.
186"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.22448979591836735,"(a) Hamiltonian measures across ˜L
(b) Bottleneck structure
(c) Hamiltonian dynamics"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.22551020408163266,"Figure 1: Leaky ResNet structures: We train equidistant networks with a fixed L = 20 over a
range of effective depths ˜L. The true function f ∗: R30 →R30 is the composition of two random
FCNNs g1, g2 mapping from dim. 30 to 3 to 30. (a) Estimates of the Hamiltonian constants for
networks trained with different ˜L. The Hamiltonian refers to −2"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.22653061224489796,"˜LH which estimates the true rank
k∗. The COI refers to minp ||Ap||. The trend line follows the median estimate for −2"
COST OF IDENTITY AS A MEASURE OF DIMENSIONALITY,0.22755102040816327,"˜LH across each
network’s layers, whereas the error bars signify its minimum and maximum over p ∈[0, 1]. The
""stable"" Hamiltonians utilize the relaxation from Theorem 4. (b) Spectra of the representations Ap
and weights Wp respectively for ˜L = 7. (c) Hamiltonian dynamics of the network in (b)."
HAMILTONIAN REFORMULATION,0.22857142857142856,"1.5
Hamiltonian Reformulation
187"
HAMILTONIAN REFORMULATION,0.22959183673469388,"We can further reformulate the evolution of the optimal representations Ap in terms of a Hamiltonian,
188"
HAMILTONIAN REFORMULATION,0.23061224489795917,"similar to Pontryagin’s maximum principle.
189"
HAMILTONIAN REFORMULATION,0.2316326530612245,Let us define the backward pass variables Bp = −1
HAMILTONIAN REFORMULATION,0.23265306122448978,λ∂ApC(A1) for the cost C(A) = 1
HAMILTONIAN REFORMULATION,0.2336734693877551,"2∥f ∗(X)−A∥2
F ,
190"
HAMILTONIAN REFORMULATION,0.23469387755102042,"which play the role of the ‘momenta’ of Ap in this Hamiltonian interpretation, which follows the
191"
HAMILTONIAN REFORMULATION,0.2357142857142857,"backward differential equation
192"
HAMILTONIAN REFORMULATION,0.23673469387755103,B1 = −1
HAMILTONIAN REFORMULATION,0.23775510204081632,"λ∂A1C(A1) =
2
λN (f ∗(X) −A1)"
HAMILTONIAN REFORMULATION,0.23877551020408164,"−∂pBp = ˙σ(Ap) ⊙

W T
p Bp

−˜LBp."
HAMILTONIAN REFORMULATION,0.23979591836734693,"Now at any critical point, we have that ∂WpC(A1) +
λ"
HAMILTONIAN REFORMULATION,0.24081632653061225,"˜LWp
=
0 and thus Wp
=
193"
HAMILTONIAN REFORMULATION,0.24183673469387756,"−
˜L
λ ∂ApC(A1)σ(Ap)T = ˜LBpσ(Ap)T , leading to joint dynamics for Ap and Bp:
194"
HAMILTONIAN REFORMULATION,0.24285714285714285,∂pAp = ˜L(Bpσ(Ap)T σ(Ap) −Ap)
HAMILTONIAN REFORMULATION,0.24387755102040817,"−∂pBp = ˜L
 
˙σ(Ap) ⊙

σ(Ap)BT
p Bp

−Bp

."
HAMILTONIAN REFORMULATION,0.24489795918367346,"These are Hamiltonian dynamics ∂pAp = ∂BpH and −∂pBp = ∂ApH w.r.t. the Hamiltonian
195"
HAMILTONIAN REFORMULATION,0.24591836734693878,"H(Ap, Bp) =
˜L
2"
HAMILTONIAN REFORMULATION,0.24693877551020407,"Bpσ(Ap)T 2 −˜LTr

BpAT
p

."
HAMILTONIAN REFORMULATION,0.2479591836734694,"The Hamiltonian is a conserved quantity, i.e. it is constant in p. It will play a significant role in
196"
HAMILTONIAN REFORMULATION,0.24897959183673468,"describing a separation of timescales that appears for large depths ˜L. Another significant advantage
197"
HAMILTONIAN REFORMULATION,0.25,"of the Hamiltonian reformulation over the Lagrangian approach is the absence of the unstable
198"
HAMILTONIAN REFORMULATION,0.2510204081632653,"pseudo-inverses σ(Ap)+.
199"
HAMILTONIAN REFORMULATION,0.25204081632653064,"Remark. Note that the Lagrangian and Hamiltonian reformulations have already appeared in previous
200"
HAMILTONIAN REFORMULATION,0.2530612244897959,"work [23] for non-leaky ResNets. Our main contributions are the description in the next section of the
201"
HAMILTONIAN REFORMULATION,0.2540816326530612,"Hamiltonian as the network becomes leakier ˜L →∞, the connection to the cost of identity, and the
202"
HAMILTONIAN REFORMULATION,0.25510204081632654,"appearance of a separation of timescales. These structures are harder to observe in non-leaky ResNets
203"
HAMILTONIAN REFORMULATION,0.25612244897959185,"(though they could in theory still appear since increasing the scale of the outputs is equivalent to
204"
HAMILTONIAN REFORMULATION,0.2571428571428571,"increasing the effective depth ˜L as shown in Section 1.2).
205"
HAMILTONIAN REFORMULATION,0.25816326530612244,"The Lagrangian and Hamiltonian are also very similar to the ones in [10, 11], and the separation of
206"
HAMILTONIAN REFORMULATION,0.25918367346938775,"timescales and rapid jumps that we will describe also bear a strong similarity. Though a difference
207"
HAMILTONIAN REFORMULATION,0.2602040816326531,"with our work is that the norm ∥·∥Kp depends on Ap and can be degenerate.
208"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.2612244897959184,"2
Bottleneck Structure in Representation Geodesics
209"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.26224489795918365,"A recent line of work [16, 15] studies the appearance of a so-called Bottleneck structure in large
210"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.26326530612244897,"depth fully-connected networks, where the weight matrices and representations of ‘almost all’ layers
211"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.2642857142857143,"of the layers are approximately low-rank/low-dimensional as the depth grows. This dimension k is
212"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.2653061224489796,"consistent across layers, and can be interpreted as being equal to the so-called Bottleneck rank of the
213"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.26632653061224487,"learned function. This structure has been shown to extend to CNNs in [30], and we will observe a
214"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.2673469387755102,"similar structure in our leaky ResNets, further showcasing its generality.
215"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.2683673469387755,"More generally, our goal is to describe the ‘representation geodesics’ of DNNs: the paths in
216"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.2693877551020408,"representation space from input to output representation. The advantage of ResNets (leaky or
217"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.27040816326530615,"not) over FCNNs is that these geodesics can be approximated by continuous paths and are described
218"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.2714285714285714,"by differential equations (as described by the Hamiltonian reformulation).
219"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.2724489795918367,"This section provides an approximation of the Hamiltonian that illustrates the separation of timescales
220"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.27346938775510204,"that appears for large depths, with slow layers with low COI/dimension, and fast layers with high
221"
BOTTLENECK STRUCTURE IN REPRESENTATION GEODESICS,0.27448979591836736,"COI/dimension.
222"
SEPARATION OF TIMESCALES,0.2755102040816326,"2.1
Separation of Timescales
223"
SEPARATION OF TIMESCALES,0.27653061224489794,"If ImAT
p ⊂Imσ(Ap)T , then the Hamiltonian equals the sum of the kinetic and potential energies:
224 H = 1"
SEPARATION OF TIMESCALES,0.27755102040816326,"2˜L
∥∂pAp∥2
Kp −
˜L
2 ∥Ap∥2
Kp ."
SEPARATION OF TIMESCALES,0.2785714285714286,"This implies that ∥∂pAp∥Kp = ˜L
q"
SEPARATION OF TIMESCALES,0.2795918367346939,"∥Ap∥2
Kp + 2"
SEPARATION OF TIMESCALES,0.28061224489795916,"˜LH which implies that for large ˜L, the derivative
225"
SEPARATION OF TIMESCALES,0.2816326530612245,"∂pAp is only finite at ps where the COI ∥Ap∥2
Kp is close to −2"
SEPARATION OF TIMESCALES,0.2826530612244898,"˜LH. On the other hand, ∂pAp will
226"
SEPARATION OF TIMESCALES,0.2836734693877551,"blow up for all p with a finite gap
q"
SEPARATION OF TIMESCALES,0.28469387755102044,"∥Ap∥2
Kp + 2"
SEPARATION OF TIMESCALES,0.2857142857142857,"˜LH > 0 between the COI and the Hamiltonian. This
227"
SEPARATION OF TIMESCALES,0.286734693877551,"suggests a separation of timescales as ˜L →∞, with slow dynamics in layers whose COI/dimension
228"
SEPARATION OF TIMESCALES,0.28775510204081634,is close to −2
SEPARATION OF TIMESCALES,0.28877551020408165,"˜LH and fast dynamics in the high COI/dimension layers.
229"
SEPARATION OF TIMESCALES,0.2897959183673469,"But the assumption ImAT
p ⊂Imσ(Ap)T seems to rarely be true in practice, and both kinetic and
230"
SEPARATION OF TIMESCALES,0.29081632653061223,"COI appear to be often infinite in practice. But up to a few approximations, the same argument can
231"
SEPARATION OF TIMESCALES,0.29183673469387755,"be made for stable versions of the kinetic energy/COI:
232"
SEPARATION OF TIMESCALES,0.29285714285714287,"Theorem 4. For sequence A˜L
p of geodesics with
B ˜L
p

2
≤c < ∞, and any γ > 0, we have
233 −
 1"
SEPARATION OF TIMESCALES,0.2938775510204082,"˜L
ℓγ,˜L + √γc
2
≤−2"
SEPARATION OF TIMESCALES,0.29489795918367345,"˜L
H −min
p"
SEPARATION OF TIMESCALES,0.29591836734693877,"A
˜L
p

2"
SEPARATION OF TIMESCALES,0.2969387755102041,"(Kp+γI) ≤γc,"
SEPARATION OF TIMESCALES,0.2979591836734694,"for the path length ℓγ,˜L =
R 1
0"
SEPARATION OF TIMESCALES,0.29897959183673467,"∂pA˜L
p

(Kp+γI) dp. Finally
234"
SEPARATION OF TIMESCALES,0.3,−˜L√γc ≤∥∂pAp∥(Kp+γi) −˜L r
SEPARATION OF TIMESCALES,0.3010204081632653,"∥Ap∥2
(Kp+γI) + 2"
SEPARATION OF TIMESCALES,0.3020408163265306,"˜L
H ≤2˜L√γc."
SEPARATION OF TIMESCALES,0.30306122448979594,"Note that the size of ∥B ˜L
p ∥2 can vary a lot throughout the layers, we therefore suggest choosing
235"
SEPARATION OF TIMESCALES,0.3040816326530612,"a p-dependent γ: γp = γ0∥σ(A˜L
p )∥2
op = γ0∥Kp∥2
op. There are two motivations for this: first it is
236"
SEPARATION OF TIMESCALES,0.3051020408163265,"natural to have γ scale with Kp, ; and second, since Wp = ˜LBpσ(Ap)T is of approximately constant
237"
SEPARATION OF TIMESCALES,0.30612244897959184,"size (thanks to balancedness, see Appendix A.3), we typically have that the size of Bp is inversely
238"
SEPARATION OF TIMESCALES,0.30714285714285716,"proportional to that of σ(Ap), so that γp∥Bp∥2 should keep roughly the same size for all p.
239"
SEPARATION OF TIMESCALES,0.3081632653061224,"Theorem 4 shows that for large ˜L (and choosing e.g. γ = ˜L−1), the Hamiltonian is close to the
240"
SEPARATION OF TIMESCALES,0.30918367346938774,"minimal COI along the path. Second, the norm of the derivative ∥∂pAp∥(Kp+γi) is close to ˜L times
241"
SEPARATION OF TIMESCALES,0.31020408163265306,"the ‘extra-COI’
q"
SEPARATION OF TIMESCALES,0.3112244897959184,"∥Ap∥2
(Kp+γI) + 2"
SEPARATION OF TIMESCALES,0.3122448979591837,"˜LH ≈
q"
SEPARATION OF TIMESCALES,0.31326530612244896,"∥Ap∥2
(Kp+γI) −minq ∥Aq∥2
(Kq+γI), which describes
242"
SEPARATION OF TIMESCALES,0.3142857142857143,"the separation of timescales, with slow (∼1) dynamics at layers p where the COI is almost optimal
243"
SEPARATION OF TIMESCALES,0.3153061224489796,"and fast (∼˜L) dynamics everywhere the COI is far from optimal.
244"
SEPARATION OF TIMESCALES,0.3163265306122449,"(a) Test performance versus depth
(b) Bottleneck structure and adaptivity.
(c) Paths"
SEPARATION OF TIMESCALES,0.3173469387755102,"Figure 2: Discretization: We train networks with a fixed ˜L = 3 over a range of depths L and
definitions of ρℓs. The true function f ∗: R30 →R30 is the composition of three random ResNets
g1, g2, g3 mapping from dim. 30 to 6 to 3 to 30. (a) Test error as a function of L for different
discretization schemes. (b) Weight spectra across layers for adaptive ρℓ(L = 18), grey vertical lines
represents the steps pℓ(c) 2D projection of the representation paths Ap for L = 18. Observe how
adaptive ρℓs appears to better spread out the steps."
SEPARATION OF TIMESCALES,0.3183673469387755,"Assuming a finite length ℓγ,˜L < ∞, the norm of the derivative must be finite at almost all layers,
245"
SEPARATION OF TIMESCALES,0.3193877551020408,"meaning that the COI/dimensionality is optimal in almost all layers, with only a countable number
246"
SEPARATION OF TIMESCALES,0.32040816326530613,"of short high COI/dimension jumps. These jumps typically appear at the beginning and end of the
247"
SEPARATION OF TIMESCALES,0.32142857142857145,"network, because the input and output dimensionality and COI are (mostly) fixed, so it will typically
248"
SEPARATION OF TIMESCALES,0.3224489795918367,"be non-optimal, and so there will often be fast regions close to the beginning and end of the network.
249"
SEPARATION OF TIMESCALES,0.32346938775510203,"We have actually never observed any jump in the middle of the network, though we are not able to
250"
SEPARATION OF TIMESCALES,0.32448979591836735,"rule them out theoretically.
251"
SEPARATION OF TIMESCALES,0.32551020408163267,"If we assume that the paths A˜L
p are stable under adding a neuron, then we can additionally guarantee
252"
SEPARATION OF TIMESCALES,0.32653061224489793,"that the representations in the slow layers (‘inside the Bottleneck’) will be non-negative:
253"
SEPARATION OF TIMESCALES,0.32755102040816325,"Proposition 5. Let A˜L
p be a uniformly bounded sequence of local minima for increasing ˜L, at
254"
SEPARATION OF TIMESCALES,0.32857142857142857,"any p0 ∈(0, 1) such that ∥∂pAp∥is uniformly bounded in a neighborhood of p0 for all ˜L, then
255"
SEPARATION OF TIMESCALES,0.3295918367346939,"A∞
p0 = lim˜L A˜L
p0 is non-negative.
256"
SEPARATION OF TIMESCALES,0.3306122448979592,"We therefore know that the optimal COI minq ∥Aq∥2
(Kq+γI) is close to the dimension of the limiting
257"
SEPARATION OF TIMESCALES,0.33163265306122447,"representations A∞
p0, i.e. it must be an integer k∗which we call the Bottleneck rank of the sequence
258"
SEPARATION OF TIMESCALES,0.3326530612244898,"of minima since it is closely related to the Bottleneck rank introduced in [16]. The Hamiltonian H is
259"
SEPARATION OF TIMESCALES,0.3336734693877551,"then close to −
˜L
2 k∗.
260"
SEPARATION OF TIMESCALES,0.3346938775510204,"Figure 1 illustrates these phenomena: the Hamiltonian (and the stable Hamiltonians Hγ =
261"
SEPARATION OF TIMESCALES,0.3357142857142857,"1
2˜L ∥∂pAp∥2
(Kp+γI) −
˜L
2 ∥Ap∥2
(Kp+γI)) approach the rank k∗= 3 from below, while the minimal
262"
SEPARATION OF TIMESCALES,0.336734693877551,"COI approaches it from above; The kinetic energy is proportional to the extra COI, and they are both
263"
SEPARATION OF TIMESCALES,0.3377551020408163,"large towards the beginning and end of the network where the weights Wp are higher dimensional.
264"
SEPARATION OF TIMESCALES,0.33877551020408164,"We see in Figure 1c that the (stable) Hamiltonian are not exactly constant, but it still varies much less
265"
SEPARATION OF TIMESCALES,0.33979591836734696,"than its components, the kinetic and potential energies.
266"
SEPARATION OF TIMESCALES,0.3408163265306122,"Because of the non-convexity of the loss we are considering, one can imagine that there could exist
267"
SEPARATION OF TIMESCALES,0.34183673469387754,"distinct sequences of local minima as ˜L →∞, which could have different rank, depending on what
268"
SEPARATION OF TIMESCALES,0.34285714285714286,"low-dimension they reach inside their bottleneck. Indeed in our experiments we have seen that the
269"
SEPARATION OF TIMESCALES,0.3438775510204082,"number of dimensions that are kept inside the bottleneck can vary by 1 or 2, and in FCNN distinct
270"
SEPARATION OF TIMESCALES,0.3448979591836735,"sequences of depth increasing minima with different ranks have been observed in [15].
271"
DISCRETIZATION SCHEME,0.34591836734693876,"3
Discretization Scheme
272"
DISCRETIZATION SCHEME,0.3469387755102041,"To use such Leaky ResNets in practice, we need to discretize over the range [0, 1]. For this we
273"
DISCRETIZATION SCHEME,0.3479591836734694,"choose a set of layer-steps ρ1, . . . , ρL with P ρℓ= 1, and define the activations at the locations
274"
DISCRETIZATION SCHEME,0.3489795918367347,"pℓ= ρ1 + · · · + ρℓ∈[0, 1] recursively as
275"
DISCRETIZATION SCHEME,0.35,αp0(x) = x
DISCRETIZATION SCHEME,0.3510204081632653,"αpℓ(x) = (1 −ρℓ˜L)αpℓ−1(x) + ρℓWpℓσ
 
αpℓ−1(x)
"
DISCRETIZATION SCHEME,0.3520408163265306,"and the regularized cost L(θ) = C(α1(X)) +
λ
2˜L
PL
ℓ=1 ρℓ∥Wpℓ∥2, for the parameters θ =
276"
DISCRETIZATION SCHEME,0.35306122448979593,"(Wp1, . . . , WpL). Note that it is best to ensure that ρℓ˜L remains smaller than 1 so that the prefactor
277"
DISCRETIZATION SCHEME,0.35408163265306125,"(1 −ρℓ˜L) does not become negative, though we will also discuss certain setups where it might be
278"
DISCRETIZATION SCHEME,0.3551020408163265,"okay to take larger layer-steps.
279"
DISCRETIZATION SCHEME,0.35612244897959183,"Now comes the question of how to choose the ρℓs. We consider three options:
280"
DISCRETIZATION SCHEME,0.35714285714285715,Equidistant: The simplest choice is to choose equidistant points ρℓ= 1
DISCRETIZATION SCHEME,0.35816326530612247,"L. Note that the condition
281"
DISCRETIZATION SCHEME,0.35918367346938773,"ρℓL < 1 then becomes L > ˜L. But this choice might be ill adapted in the presence of a Bottleneck
282"
DISCRETIZATION SCHEME,0.36020408163265305,"structure due to the separation of timescales.
283"
DISCRETIZATION SCHEME,0.36122448979591837,"Irregular: Since we typically observe that the fast layers appear close to the inputs and outputs with
284"
DISCRETIZATION SCHEME,0.3622448979591837,"a slow bottleneck in the middle, one could simply choose the ρℓto be go from small to large and back
285"
DISCRETIZATION SCHEME,0.363265306122449,"to small as ℓranges from 1 to L. This way there are many discretized layers in the fast regions close
286"
DISCRETIZATION SCHEME,0.36428571428571427,"to the input and output and not too many layers inside the Bottleneck where the representations are
287"
DISCRETIZATION SCHEME,0.3653061224489796,changing less. More concretely one can choose ρℓ= 1 L + a L( 1
DISCRETIZATION SCHEME,0.3663265306122449,"4 −
 ℓ L −1"
DISCRETIZATION SCHEME,0.3673469387755102,"2
) for a ∈[0, 1), the choice
288"
DISCRETIZATION SCHEME,0.3683673469387755,"a = 0 leads to an equidistant mesh, but increasing a will lead to more points close to the inputs and
289"
DISCRETIZATION SCHEME,0.3693877551020408,"outputs. To guarantee ρℓ˜L < 1, we need L > (1 + a 1"
DISCRETIZATION SCHEME,0.3704081632653061,"4)˜L.
290"
DISCRETIZATION SCHEME,0.37142857142857144,"Adaptive: But this can be further improved by choosing the ρℓto guarantee that the distances
291"
DISCRETIZATION SCHEME,0.37244897959183676,"∥Aℓ−Aℓ−1∥/∥Ap∥are approximately the same for all ℓ(we divide by the size of Ap since
292"
DISCRETIZATION SCHEME,0.373469387755102,"it can vary a lot throughout the layers). Since the rate of change of Ap is proportional to ρℓ
293"
DISCRETIZATION SCHEME,0.37448979591836734,"(∥Aℓ−Aℓ−1∥/∥Ap∥= ρℓcℓ), it is optimal to choose ρℓ=
c−1
ℓ
P c−1
ℓ
for cℓ= ∥Aℓ−Aℓ−1∥/ρℓ∥Ap∥. The
294"
DISCRETIZATION SCHEME,0.37551020408163266,"update ρℓ←
c−1
i
P c−1
i
can be done at every training step or every few training steps.
295"
DISCRETIZATION SCHEME,0.376530612244898,"Note that the condition ρℓ˜L < 1 might not be necessary inside the bottleneck since we have the
296"
DISCRETIZATION SCHEME,0.37755102040816324,"approximation Wpσ(Apℓ−1) ≈˜LApℓ−1, canceling out the negative direction. In particular with the
297"
DISCRETIZATION SCHEME,0.37857142857142856,"adaptive layer-steps that we propose, a large ρℓis only possible for layers where cℓis small, which is
298"
DISCRETIZATION SCHEME,0.3795918367346939,"only possible when Wpσ(Apℓ−1) ≈˜LApℓ−1.
299"
DISCRETIZATION SCHEME,0.3806122448979592,"Figure 2 illustrates the effect of the choice of ρℓfor different depths L, we see a small but consistent
300"
DISCRETIZATION SCHEME,0.3816326530612245,"advantage in the test error when using adaptive or irregular ρℓs. Looking at the resulting Bottleneck
301"
DISCRETIZATION SCHEME,0.3826530612244898,"structure, we see that the adaptive ρℓs result in more steps especially in the beginning of the network,
302"
DISCRETIZATION SCHEME,0.3836734693877551,"but also at the end. This because the ‘true function’ f ∗: R30 →R30 we are fitting in these
303"
DISCRETIZATION SCHEME,0.3846938775510204,"experiments is of the form f ∗= g3 ◦g2 ◦g1 where the first inner dimension is 6 and the second is 3,
304"
DISCRETIZATION SCHEME,0.38571428571428573,"thus resulting in a rank of k∗= 3. But before reaching this minimal dimension, the network needs to
305"
DISCRETIZATION SCHEME,0.386734693877551,"represent g2 ◦g1, which requires more layers, and one can almost see that the weight matrices are
306"
DISCRETIZATION SCHEME,0.3877551020408163,"roughly 6-dimensional around p = 0.3. The adaptivity to this structure could explain the advantage
307"
DISCRETIZATION SCHEME,0.38877551020408163,"in the test error.
308"
CONCLUSION,0.38979591836734695,"4
Conclusion
309"
CONCLUSION,0.39081632653061227,"We have given a description of the representation geodesics Ap of Leaky ResNets. We have identified
310"
CONCLUSION,0.39183673469387753,"an invariant, the Hamiltonian, which is the sum of the kinetic and potential energy, where the kinetic
311"
CONCLUSION,0.39285714285714285,"energy measures the size of the derivative ∂pAp, while the potential energy is inversely proportional
312"
CONCLUSION,0.39387755102040817,"to the cost of identity, which is a measure of dimensionality of the representations. As the effective
313"
CONCLUSION,0.3948979591836735,"depth of the network grows, the potential energy dominates and we observe a separation of timescales.
314"
CONCLUSION,0.39591836734693875,"At layers with minimal dimensionality over the path, the kinetic energy (and thus the derivative ∂pAp)
315"
CONCLUSION,0.39693877551020407,"is finite. Conversely, at layers where the representation is higher-dimensional, the kinetic energy must
316"
CONCLUSION,0.3979591836734694,"scale with ˜L. This leads to a Bottleneck structure, with a short, high-dimensional jump from the input
317"
CONCLUSION,0.3989795918367347,"representation to a low dimensional representation, followed by slow dynamics inside the space of
318"
CONCLUSION,0.4,"low-dimensional representations followed by a final high-dimensional jump to the high dimensional
319"
CONCLUSION,0.4010204081632653,"outputs.
320"
REFERENCES,0.4020408163265306,"References
321"
REFERENCES,0.4030612244897959,"[1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property:
322"
REFERENCES,0.40408163265306124,"a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer
323"
REFERENCES,0.4051020408163265,"neural networks. In Conference on Learning Theory, pages 4782–4887. PMLR, 2022.
324"
REFERENCES,0.4061224489795918,"[2] Emmanuel Abbe, Enric Boix-Adserà, Matthew Stewart Brennan, Guy Bresler, and
325"
REFERENCES,0.40714285714285714,"Dheeraj Mysore Nagaraj. The staircase property: How hierarchical structure can guide deep
326"
REFERENCES,0.40816326530612246,"learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances
327"
REFERENCES,0.4091836734693878,"in Neural Information Processing Systems, 2021.
328"
REFERENCES,0.41020408163265304,"[3] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.
A latent
329"
REFERENCES,0.41122448979591836,"variable model approach to pmi-based word embeddings. Transactions of the Association
330"
REFERENCES,0.4122448979591837,"for Computational Linguistics, 4:385–399, 2016.
331"
REFERENCES,0.413265306122449,"[4] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal
332"
REFERENCES,0.4142857142857143,"of Machine Learning Research, 18(1):629–681, 2017.
333"
REFERENCES,0.4153061224489796,"[5] Daniel Beaglehole, Adityanarayanan Radhakrishnan, Parthe Pandit, and Mikhail Belkin.
334"
REFERENCES,0.4163265306122449,"Mechanism of feature learning in convolutional neural networks.
arXiv preprint
335"
REFERENCES,0.4173469387755102,"arXiv:2309.00570, 2023.
336"
REFERENCES,0.41836734693877553,"[6] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
337"
REFERENCES,0.4193877551020408,"differential equations. Advances in neural information processing systems, 31, 2018.
338"
REFERENCES,0.4204081632653061,"[7] Lénaïc Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural
339"
REFERENCES,0.42142857142857143,"networks trained with the logistic loss. In Jacob Abernethy and Shivani Agarwal, editors,
340"
REFERENCES,0.42244897959183675,"Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of
341"
REFERENCES,0.42346938775510207,"Machine Learning Research, pages 1305–1338. PMLR, 09–12 Jul 2020.
342"
REFERENCES,0.42448979591836733,"[8] Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. Towards understanding linear word
343"
REFERENCES,0.42551020408163265,"analogies. arXiv preprint arXiv:1810.04882, 2018.
344"
REFERENCES,0.42653061224489797,"[9] Tomer Galanti, Zachary S Siegel, Aparna Gupte, and Tomaso Poggio. Sgd and weight decay
345"
REFERENCES,0.4275510204081633,"provably induce a low-rank bias in neural networks. arXiv preprint arXiv:2206.05794, 2022.
346"
REFERENCES,0.42857142857142855,"[10] Tobias Grafke, Rainer Grauer, T Schäfer, and Eric Vanden-Eijnden. Arclength parametrized
347"
REFERENCES,0.42959183673469387,"hamilton’s equations for the calculation of instantons. Multiscale Modeling & Simulation,
348"
REFERENCES,0.4306122448979592,"12(2):566–580, 2014.
349"
REFERENCES,0.4316326530612245,"[11] Tobias Grafke and Eric Vanden-Eijnden. Numerical computation of rare events via large
350"
REFERENCES,0.4326530612244898,"deviation theory. Chaos: An Interdisciplinary Journal of Nonlinear Science, 29(6):063118, 06
351"
REFERENCES,0.4336734693877551,"2019.
352"
REFERENCES,0.4346938775510204,"[12] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias
353"
REFERENCES,0.4357142857142857,"in terms of optimization geometry. In Jennifer Dy and Andreas Krause, editors, Proceedings of
354"
REFERENCES,0.43673469387755104,"the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
355"
REFERENCES,0.4377551020408163,"Learning Research, pages 1832–1841. PMLR, 10–15 Jul 2018.
356"
REFERENCES,0.4387755102040816,"[13] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient
357"
REFERENCES,0.43979591836734694,"descent on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
358"
REFERENCES,0.44081632653061226,"N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,
359"
REFERENCES,0.4418367346938776,"volume 31. Curran Associates, Inc., 2018.
360"
REFERENCES,0.44285714285714284,"[14] Florentin Guth, Brice Ménard, Gaspar Rochette, and Stéphane Mallat. A rainbow in deep
361"
REFERENCES,0.44387755102040816,"network black boxes. arXiv preprint arXiv:2305.18512, 2023.
362"
REFERENCES,0.4448979591836735,"[15] Arthur Jacot. Bottleneck structure in learned features: Low-dimension vs regularity tradeoff. In
363"
REFERENCES,0.4459183673469388,"A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in
364"
REFERENCES,0.44693877551020406,"Neural Information Processing Systems, volume 36, pages 23607–23629. Curran Associates,
365"
REFERENCES,0.4479591836734694,"Inc., 2023.
366"
REFERENCES,0.4489795918367347,"[16] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. In
367"
REFERENCES,0.45,"The Eleventh International Conference on Learning Representations, 2023.
368"
REFERENCES,0.45102040816326533,"[17] Arthur Jacot, Eugene Golikov, Clément Hongler, and Franck Gabriel. Feature learning in
369"
REFERENCES,0.4520408163265306,"l2-regularized dnns: Attraction/repulsion and sparsity. In Advances in Neural Information
370"
REFERENCES,0.4530612244897959,"Processing Systems, volume 36, 2022.
371"
REFERENCES,0.45408163265306123,"[18] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
372"
REFERENCES,0.45510204081632655,"network representations revisited. In International Conference on Machine Learning, pages
373"
REFERENCES,0.4561224489795918,"3519–3529. PMLR, 2019.
374"
REFERENCES,0.45714285714285713,"[19] A. Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep
375"
REFERENCES,0.45816326530612245,"convolutional neural networks. Communications of the ACM, 60:84 – 90, 2012.
376"
REFERENCES,0.45918367346938777,"[20] Jianing Li and Vardan Papyan. Residual alignment: Uncovering the mechanisms of residual
377"
REFERENCES,0.4602040816326531,"networks. Advances in Neural Information Processing Systems, 36, 2024.
378"
REFERENCES,0.46122448979591835,"[21] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient
379"
REFERENCES,0.46224489795918366,"descent for matrix factorization: Greedy low-rank learning. In International Conference on
380"
REFERENCES,0.463265306122449,"Learning Representations, 2020.
381"
REFERENCES,0.4642857142857143,"[22] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word
382"
REFERENCES,0.46530612244897956,"representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
383"
REFERENCES,0.4663265306122449,"[23] Houman Owhadi. Do ideas have shape? plato’s theory of forms as the continuous limit of
384"
REFERENCES,0.4673469387755102,"artificial neural networks. arXiv preprint arXiv:2008.03920, 2020.
385"
REFERENCES,0.4683673469387755,"[24] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the
386"
REFERENCES,0.46938775510204084,"terminal phase of deep learning training. Proceedings of the National Academy of Sciences,
387"
REFERENCES,0.4704081632653061,"117(40):24652–24663, 2020.
388"
REFERENCES,0.4714285714285714,"[25] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin.
389"
REFERENCES,0.47244897959183674,"Mechanism for feature learning in neural networks and backpropagation-free machine learning
390"
REFERENCES,0.47346938775510206,"models. Science, 383(6690):1461–1467, 2024.
391"
REFERENCES,0.4744897959183674,"[26] Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky,
392"
REFERENCES,0.47551020408163264,"Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep
393"
REFERENCES,0.47653061224489796,"learning. In International Conference on Learning Representations, 2018.
394"
REFERENCES,0.4775510204081633,"[27] Peter Súkeník, Marco Mondelli, and Christoph H Lampert. Deep neural collapse is provably
395"
REFERENCES,0.4785714285714286,"optimal for the deep unconstrained features model. Advances in Neural Information Processing
396"
REFERENCES,0.47959183673469385,"Systems, 36, 2024.
397"
REFERENCES,0.4806122448979592,"[28] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
398"
REFERENCES,0.4816326530612245,"2015 ieee information theory workshop (itw), pages 1–5. IEEE, 2015.
399"
REFERENCES,0.4826530612244898,"[29] Zihan Wang and Arthur Jacot. Implicit bias of SGD in l2-regularized linear DNNs: One-
400"
REFERENCES,0.48367346938775513,"way jumps from high to low rank. In The Twelfth International Conference on Learning
401"
REFERENCES,0.4846938775510204,"Representations, 2024.
402"
REFERENCES,0.4857142857142857,"[30] Yuxiao Wen and Arthur Jacot. Which frequencies do cnns need? emergent bottleneck structure
403"
REFERENCES,0.48673469387755103,"in feature learning. to appear at ICML, 2024.
404"
REFERENCES,0.48775510204081635,"[31] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
405"
REFERENCES,0.4887755102040816,"Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September
406"
REFERENCES,0.4897959183673469,"6-12, 2014, Proceedings, Part I 13, pages 818–833. Springer, 2014.
407"
REFERENCES,0.49081632653061225,"A
Proofs
408"
REFERENCES,0.49183673469387756,"A.1
Cost of Identity
409"
REFERENCES,0.4928571428571429,"Proposition 6 (Proposition 3 in the main.). If w > N(N + 1) then if ˆA ∈Rw×N is local minimum
410"
REFERENCES,0.49387755102040815,"of A 7→∥Aσ(A)+∥2
F that is not non-negative, then there is a continuous path At of constant COI
411"
REFERENCES,0.49489795918367346,"such that A0 = ˆA and A1 is a saddle.
412"
REFERENCES,0.4959183673469388,"Proof. The local minimum
ˆA leads to a pair of N × N covariance matrices
ˆK
=
413"
REFERENCES,0.4969387755102041,"ˆAT ˆA
and
ˆKσ
=
σ( ˆA)T σ( ˆA).
The
pair
( ˆK, ˆKσ)
belongs
to
the
conical
hull
414"
REFERENCES,0.49795918367346936,"Cone
n
( ˆAi· ˆAT
i·, σ( ˆAi·)σ( ˆAi·)T ) : i = 1, . . . , w
o
. Since this cones lies in a N(N + 1)-dimensional
415"
REFERENCES,0.4989795918367347,"space (the space of pairs of symmetric N × N matrices), we know by Caratheodory’s
416"
REFERENCES,0.5,"theorem (for convex cones) that there is a conical combination ( ˆK, ˆKσ −β21N×N)
=
417
Pw
i=1 ai( ˆAi· ˆAT
i·, σ( ˆAi·)σ( ˆAi·)T ) such that no more than N(N + 1) of the coefficients are non-
418"
REFERENCES,0.5010204081632653,"zero. We now define At to have lines At,i· =
p"
REFERENCES,0.5020408163265306,"(1 −t) + tai ˆAi·, so that At=0 = ˆA and at t = 1 at
419"
REFERENCES,0.503061224489796,"least one line of At=1 is zero (since at least one of the ais is zero). First note that the covariance pairs
420"
REFERENCES,0.5040816326530613,"remain constant over the path: Kt = AT
t At = Pw
i=1((1 −t) + tai) ˆAi· ˆAT
i· = (1 −t) ˆK + t ˆK = ˆK
421"
REFERENCES,0.5051020408163265,"and similarly Kσ
t = ˆKσ, which implies that the cost ∥Atσ(At)+∥2
F = Tr

KtKσ+
t

is constant
422"
REFERENCES,0.5061224489795918,"too. Second, since a representation A is non-negative iff the covariances satisfy K = Kσ, the
423"
REFERENCES,0.5071428571428571,"representation path At cannot be non-negative either since it has the same kernel pairs ( ˆK, ˆKσ) with
424"
REFERENCES,0.5081632653061224,"ˆK ̸= ˆKσ.
425"
REFERENCES,0.5091836734693878,"Now (the converse of) Proposition 2 tells us that if At=1 is not non-negative and has a zero line, then
426"
REFERENCES,0.5102040816326531,"it is not a local minimum, which implies that it is a saddle.
427"
REFERENCES,0.5112244897959184,"A.2
Bottleneck
428"
REFERENCES,0.5122448979591837,"Theorem 7. For any uniformly bounded sequence A˜L
p of geodesics, i.e.
A˜L
p

2
,
B ˜L
p

2
≤c < ∞,
429"
REFERENCES,0.513265306122449,"and any γ > 0, we have
430 −
 1"
REFERENCES,0.5142857142857142,"˜L
ℓγ,˜L + √γc
2
≤−2"
REFERENCES,0.5153061224489796,"˜L
H −min
p"
REFERENCES,0.5163265306122449,"A
˜L
p

2"
REFERENCES,0.5173469387755102,"(Kp+γI) ≤γc,"
REFERENCES,0.5183673469387755,"for the path length ℓγ,˜L =
R 1
0"
REFERENCES,0.5193877551020408,"∂pA˜L
p

(Kp+γI) dp. Finally
431"
REFERENCES,0.5204081632653061,−˜L√γc ≤∥∂pAp∥(Kp+γi) −˜L r
REFERENCES,0.5214285714285715,"∥Ap∥2
(Kp+γI) + 2"
REFERENCES,0.5224489795918368,"˜L
H ≤2˜L√γc."
REFERENCES,0.523469387755102,"Proof. First observe that
432"
REFERENCES,0.5244897959183673,"1
˜L
∂pAp + γBp  2"
REFERENCES,0.5255102040816326,"(Kp+γI)
= ∥Bp(Kp + γ) −Ap∥2
(Kp+γI)"
REFERENCES,0.5265306122448979,"=
Bpσ(Ap)T 2 + γ ∥Bp∥2 −2Tr

BpAT
p

+ ∥Ap∥2
(Kp+γI) = 2"
REFERENCES,0.5275510204081633,"˜L
H + γ ∥Bp∥2 + ∥Ap∥2
(Kp+γI)"
REFERENCES,0.5285714285714286,"and thus we have
433 −2"
REFERENCES,0.5295918367346939,"˜L
H = ∥Ap∥2
(Kp+γI) −

1
˜L
∂pAp + γBp  2"
REFERENCES,0.5306122448979592,"(Kp+γI)
+ γ ∥Bp∥2 ."
REFERENCES,0.5316326530612245,(1) The upper bound −2
REFERENCES,0.5326530612244897,"˜LH −minp
A˜L
p

2"
REFERENCES,0.5336734693877551,"(Kp+γI) ≤γc then follows from the fact that ∥Bp∥2 ≤c.
434"
REFERENCES,0.5346938775510204,"For the lower bound, first observe that
435"
REFERENCES,0.5357142857142857,"1
˜L
∥∂pAp∥(Kp+γI) ≥

1
˜L
∂pAp + γBp"
REFERENCES,0.536734693877551,"(Kp+γI)
−∥γBp∥(Kp+γI) ≥ r"
REFERENCES,0.5377551020408163,"∥Ap∥2
(Kp+γI) + 2"
REFERENCES,0.5387755102040817,"˜L
H + γ ∥Bp∥2 −√γc ≥ r"
REFERENCES,0.539795918367347,"∥Ap∥2
(Kp+γI) + 2"
REFERENCES,0.5408163265306123,"˜L
H −√γc,
(1)"
REFERENCES,0.5418367346938775,"and therefore
436"
REFERENCES,0.5428571428571428,"1
˜L
ℓγ,˜L = 1 ˜L Z 1"
REFERENCES,0.5438775510204081,"0
∥∂pAp∥(Kp+γI) dp ≥
Z 1 0 r"
REFERENCES,0.5448979591836735,"∥Ap∥2
(Kp+γI) + 2"
REFERENCES,0.5459183673469388,"˜L
H −√γcdp ≥
r"
REFERENCES,0.5469387755102041,"min
p ∥Ap∥2
(Kp+γI) + 2"
REFERENCES,0.5479591836734694,"˜L
H −√γc"
REFERENCES,0.5489795918367347,"which implies the lower bound.
437"
REFERENCES,0.55,"(2) The lower bound follows from equation 1. The upper bound follows from
438"
REFERENCES,0.5510204081632653,"1
˜L
∥∂pAp∥(Kp+γI) ≤

1
˜L
∂pAp + γBp"
REFERENCES,0.5520408163265306,"(Kp+γI)
+ ∥γBp∥(Kp+γI) ≤ r"
REFERENCES,0.5530612244897959,"∥Ap∥2
(Kp+γI) + 2"
REFERENCES,0.5540816326530612,"˜L
H + γ ∥Bp∥2 + √γc ≤ r"
REFERENCES,0.5551020408163265,"∥Ap∥2
(Kp+γI) + 2"
REFERENCES,0.5561224489795918,"˜L
H + √γ ∥Bp∥+ √γc ≤ r"
REFERENCES,0.5571428571428572,"∥Ap∥2
(Kp+γI) + 2"
REFERENCES,0.5581632653061225,"˜L
H + 2√γc. 439"
REFERENCES,0.5591836734693878,"Proposition 8 (Proposition 5 in the main.). Let A˜L
p be a uniformly bounded sequence of local minima
440"
REFERENCES,0.560204081632653,"for increasing ˜L, at any p0 ∈(0, 1) such that ∥∂pAp∥is uniformly bounded in a neighborhood of p0
441"
REFERENCES,0.5612244897959183,"for all ˜L, then A∞
p0 = lim˜L A˜L
p0 is non-negative.
442"
REFERENCES,0.5622448979591836,"Proof. Given a path Ap with corresponding weight matrices Wp corresponding to a width w, then
443

A
0"
REFERENCES,0.563265306122449,"
is a path with weight matrix

Wp
0
0
0"
REFERENCES,0.5642857142857143,"
. Our goal is to show that for sufficiently large
444"
REFERENCES,0.5653061224489796,"depths, one can under certain assumptions slightly change the weights to obtain a new path with the
445"
REFERENCES,0.5663265306122449,"same endpoints but a slightly lower loss, thus ensuring that if certain assumptions are not satisfied
446"
REFERENCES,0.5673469387755102,"then the path cannot be locally optimal.
447"
REFERENCES,0.5683673469387756,"Let us assume that ∥∂pAp∥≤c1 in a neighborhood of a p0 ∈(0, 1), and assume by contradiction
448"
REFERENCES,0.5693877551020409,"that there is an input index i = 1, . . . , N such that Ap0,·i has at least one negative entry, and therefore
449"
REFERENCES,0.5704081632653061,"∥Ap0,·i∥2 −∥σ(Ap0,·i)∥2 = c0 > 0 for all ˜L.
450"
REFERENCES,0.5714285714285714,"We now consider the new weights
451"
REFERENCES,0.5724489795918367,"
Wp −˜Lϵ2t(p)Ap,·iσ(Ap,·i)T
ϵ˜Lt(p)Ap,·i
ϵ˜Lt(p)σ(Ap,·i)
0 "
REFERENCES,0.573469387755102,"for t(p) = max{0, 1 −|p−p0|"
REFERENCES,0.5744897959183674,"r
} a triangular function centered in p0 and for an ϵ > 0.
452"
REFERENCES,0.5755102040816327,"For ϵ and rsmall enough, the parameter norm will decrease:
453 Z 1 0"
REFERENCES,0.576530612244898,"Wp −˜Lϵ2t(p)Ap,·iσ(Ap,·i)T
ϵ˜Lt(p)Ap,·i
ϵ˜Lt(p)σ(Ap,·i)
0 "
DP,0.5775510204081633,"2
dp =
Z 1"
DP,0.5785714285714286,"0
∥Wp∥2 + ˜L2ϵ2t(p)2

−2"
DP,0.5795918367346938,"˜L
AT
p,·iWpσ(Ap,·i) + ∥Ap,·i∥2 + ∥σ(Ap,·i)∥2

dp."
DP,0.5806122448979592,"Now since Wpσ(Ap,·i) = ∂pAp,·i + ˜LAp,·i, this simplifies to
454 Z 1"
DP,0.5816326530612245,"0
∥Wp∥2 + ˜L2ϵ2t(p)2

−∥Ap,·i∥2 + ∥σ(Ap,·i)∥2 −1"
DP,0.5826530612244898,"˜L
AT
p,·i∂pAp,·i"
DP,0.5836734693877551,"
dp + O(ϵ4)."
DP,0.5846938775510204,"By taking r small enough, we can guarantee that −∥Ap,·i∥2 + ∥σ(Ap,·i)∥2 < −c0"
FOR ALL P SUCH THAT,0.5857142857142857,"2 for all p such that
455"
FOR ALL P SUCH THAT,0.5867346938775511,"t(p) > 0, and for ˜L large enough we can guarantee that
 1"
FOR ALL P SUCH THAT,0.5877551020408164,"˜LAT
p,·i∂pAp,·i
 is smaller then c0"
FOR ALL P SUCH THAT,0.5887755102040816,"4 , so that
456"
FOR ALL P SUCH THAT,0.5897959183673469,"we can guarantee that the parameter norm will be strictly smaller for ϵ small enough.
457"
FOR ALL P SUCH THAT,0.5908163265306122,"We will now show that with these new weights the path becomes approximately

Ap
ϵap"
FOR ALL P SUCH THAT,0.5918367346938775,"
where
458"
FOR ALL P SUCH THAT,0.5928571428571429,"ap = ˜L
Z p"
FOR ALL P SUCH THAT,0.5938775510204082,"0
t(q)Kp,i·e
˜L(q−p)dq."
FOR ALL P SUCH THAT,0.5948979591836735,"Note that ap is positive for all p since Kp has only positive entries. Also note that as ˜L →∞,
459"
FOR ALL P SUCH THAT,0.5959183673469388,"ap →t(p)Kp,i· and so that a0 →0 and a1 →1.
460"
FOR ALL P SUCH THAT,0.5969387755102041,"On one hand, we have the time derivative
461 ∂p"
FOR ALL P SUCH THAT,0.5979591836734693,"
Ap
ϵap"
FOR ALL P SUCH THAT,0.5989795918367347,"
=

Wpσ(Ap) −˜LAp
ϵ˜L (t(p)Kp,i· −ap) 
."
FOR ALL P SUCH THAT,0.6,"On the other hand the actual derivative as determined by the new weights:
462"
FOR ALL P SUCH THAT,0.6010204081632653,"
Wp −˜Lϵ2t(p)Ap,·iσ(Ap,·i)T
ϵ˜Lt(p)Ap,·i
ϵ˜Lt(p)σ(Ap,·i)
0"
FOR ALL P SUCH THAT,0.6020408163265306," 
σ(Ap)
ϵσ(ap)"
FOR ALL P SUCH THAT,0.6030612244897959,"
−˜L

Ap
ϵap "
FOR ALL P SUCH THAT,0.6040816326530613,"=

Wpσ(Ap) −˜LAp −˜Lϵ2t(p)2Ap,·iKp,i· + ˜Lϵ2t(p)Ap,·iap
ϵ˜Lt(p)Kp,i· −ϵ˜La(p) 
."
FOR ALL P SUCH THAT,0.6051020408163266,"The only difference is the two terms
463"
FOR ALL P SUCH THAT,0.6061224489795919,"−˜Lϵ2t(p)2Ap,·iKi· + ˜Lϵ2t(p)Ap,·iap = ˜Lϵ2t(p)Ap,·i (t(p)Ki· −ap) ."
FOR ALL P SUCH THAT,0.6071428571428571,"One can guarantee with a Grönwall type of argument that the representation path resulting from the
464"
FOR ALL P SUCH THAT,0.6081632653061224,"new weights must be very close to the path

Ap
ϵap"
FOR ALL P SUCH THAT,0.6091836734693877,"
.
465"
FOR ALL P SUCH THAT,0.610204081632653,"A.3
Balancedness
466"
FOR ALL P SUCH THAT,0.6112244897959184,"This paper will heavily focus on the Hamiltonian Hp that is constant throughout the layers p ∈[0, 1],
467"
FOR ALL P SUCH THAT,0.6122448979591837,"and how it can be interpreted. Note that the Hamiltonian we introduce is distinct from an already
468"
FOR ALL P SUCH THAT,0.613265306122449,"known invariant, which arises as the result of so-called balancedness, which we introduce now.
469"
FOR ALL P SUCH THAT,0.6142857142857143,"Though this balancedness also appears in ResNets, it is easiest to understand in fullyconnected
470"
FOR ALL P SUCH THAT,0.6153061224489796,"networks. First observe that for any neuron i ∈1, . . . , w at a layer ℓone can multiply the incoming
471"
FOR ALL P SUCH THAT,0.6163265306122448,"weights (Wℓ,i·, bℓ,i) by a scalar α and divide the outcoming weights Wℓ+1,·i by the same scalar
472"
FOR ALL P SUCH THAT,0.6173469387755102,"α without changing the subsequent layers. One can easily see that the scaling that minimize the
473"
FOR ALL P SUCH THAT,0.6183673469387755,"contribution to the parameter norm is such that the norm of incoming weights equals the norm
474"
FOR ALL P SUCH THAT,0.6193877551020408,"of the outcoming weights ∥Wℓ,i·∥2 + ∥bℓ,i∥2 = ∥Wℓ+1,·i∥2. Summing over the is we obtain
475"
FOR ALL P SUCH THAT,0.6204081632653061,"∥Wℓ∥2
F + ∥bℓ∥2 = ∥Wℓ+1∥2
F and thus ∥Wℓ∥2
F = ∥W1∥2
F + Pℓ−1
k=1 ∥bk∥2
F , which means that the
476"
FOR ALL P SUCH THAT,0.6214285714285714,"norm of the weights is increasing throughout the layers, and in the absence of bias, it is even constant.
477"
FOR ALL P SUCH THAT,0.6224489795918368,"Leaky ResNet exhibit the same symmetry:
478"
FOR ALL P SUCH THAT,0.6234693877551021,"Proposition 9. At any critical Wp, we have ∥Wp∥2 = ∥W0∥2 + ˜L
R p
0 ∥Wp,·w+1∥2 dq.
479"
FOR ALL P SUCH THAT,0.6244897959183674,"Proof. This proofs handles the bias Wp,·(w+1) differently to the rest of the weights Wp,·(1:w), to
480"
FOR ALL P SUCH THAT,0.6255102040816326,"simplify notations, we write Vp = Wp,·(1:w) and bp = Wp,·(w+1) for the bias.
481"
FOR ALL P SUCH THAT,0.6265306122448979,"First let us show that choosing the weight matrices ˜Vq = r′(q)Vr(q) and bias ˜bq = r′(q)e˜L(r(q)−q)br(q)
482"
FOR ALL P SUCH THAT,0.6275510204081632,"leads to the path ˜Aq = e˜L(r(q)−q)Ar(q). Indeed the path ˜Aq = e˜L(r(q)−q)Ar(q) has the right value
483"
FOR ALL P SUCH THAT,0.6285714285714286,"when p = 0 and it then satisfies the right differential equation:
484"
FOR ALL P SUCH THAT,0.6295918367346939,"∂q ˜Aq = ˜L(r′(q) −1) ˜Aq + e
˜L(r(q)−q)r′(q)∂pAr(q)"
FOR ALL P SUCH THAT,0.6306122448979592,"= ˜L(r′(q) −1) ˜Aq + e
˜L(r(q)−q)r′(q)

−˜LAr(q) + Vr(q)σ(Ar(q)) + br(q)
"
FOR ALL P SUCH THAT,0.6316326530612245,"= −˜L ˜Zq + r′(q)Ar(q)σ

˜Zq

+ e
˜L(r(q)−q)r′(q)br(q)"
FOR ALL P SUCH THAT,0.6326530612244898,"= ˜Vqσ

˜Aq

+ ˜bq −˜L ˜Aq"
FOR ALL P SUCH THAT,0.6336734693877552,"The optimal reparametrization r(q) is therefore the one that minimizes
485 Z 1 0"
FOR ALL P SUCH THAT,0.6346938775510204,"˜Wq

2
+
˜bq

2
dq =
Z 1"
FOR ALL P SUCH THAT,0.6357142857142857,"0
r′(q)2 Wr(q)
2 + e2˜L(r(q)−q) br(q)
2
dq"
FOR ALL P SUCH THAT,0.636734693877551,"For the identity reparametrization r(q) = q to be optimal, we need
486 Z 1"
FOR ALL P SUCH THAT,0.6377551020408163,"0
2dr′(p)

∥Wp∥2 + ∥bp∥2
+ 2˜Ldr(p) ∥bp∥2 dp = 0"
FOR ALL P SUCH THAT,0.6387755102040816,"for all dr(q) with dr(0) = dr(1) = 0. Since
487 Z 1"
FOR ALL P SUCH THAT,0.639795918367347,"0
dr′(p)

∥Wp∥2 + ∥bp∥2
dp = −
Z 1"
FOR ALL P SUCH THAT,0.6408163265306123,"0
dr(p)∂p

∥Wp∥2 + ∥bp∥2
dq,"
FOR ALL P SUCH THAT,0.6418367346938776,"we need
488
Z 1"
FOR ALL P SUCH THAT,0.6428571428571429,"0
dr(p)
h
−∂p

∥Wp∥2 + ∥bp∥2
+ ˜L ∥bp∥2i
dp = 0"
FOR ALL P SUCH THAT,0.6438775510204081,"and thus for all p
489"
FOR ALL P SUCH THAT,0.6448979591836734,"∂p

∥Wp∥2 + ∥bp∥2
= ˜L ∥bp∥2 ."
FOR ALL P SUCH THAT,0.6459183673469387,"Integrating, we obtain as needed
490"
FOR ALL P SUCH THAT,0.6469387755102041,"∥Wp∥2 + ∥bp∥2 = ∥W0∥2 + ∥b0∥2 + ˜L
Z p"
FOR ALL P SUCH THAT,0.6479591836734694,"0
∥bq∥2 dq. 491"
FOR ALL P SUCH THAT,0.6489795918367347,"B
Experimental Setup
492"
FOR ALL P SUCH THAT,0.65,"Our experiments make use of synthetic data to train leaky ResNets so that the Bottleneck rank k∗is
493"
FOR ALL P SUCH THAT,0.6510204081632653,"known for our experiments. The synthetic data is generated by teacher networks for a given true rank
494"
FOR ALL P SUCH THAT,0.6520408163265307,"k∗. To construct a bottleneck, the teacher network is a composition of networks for which the the
495"
FOR ALL P SUCH THAT,0.6530612244897959,"inner-dimension is k∗. Our experiments used an input and output dimension of 30, and a bottleneck
496"
FOR ALL P SUCH THAT,0.6540816326530612,"of k∗= 3. For data, we sampled a thousand data points for training, and another thousand for testing
497"
FOR ALL P SUCH THAT,0.6551020408163265,"which are collectively augmented by demeaning and normalization.
498"
FOR ALL P SUCH THAT,0.6561224489795918,"To train the leaky ResNets, it is important for them to be wide, usually wider than the input or output
499"
FOR ALL P SUCH THAT,0.6571428571428571,"dimension, we opted for a width of 100. However, the width of the representation must be constant
500"
FOR ALL P SUCH THAT,0.6581632653061225,"to implement leaky residual connections, so we introduce a single linear mapping at the start, and
501"
FOR ALL P SUCH THAT,0.6591836734693878,"another at the end, of the forward pass to project the representations into a higher dimension for the
502"
FOR ALL P SUCH THAT,0.6602040816326531,"paths. These linear mappings can be either learned or fixed.
503"
FOR ALL P SUCH THAT,0.6612244897959184,Figure 3: Various properties of the Hamiltonian dynamics of Leaky ResNets which remain bounded
FOR ALL P SUCH THAT,0.6622448979591836,"To achieve a tight convergence in training, we train primarily using Adam using Mean Squared Error
504"
FOR ALL P SUCH THAT,0.6632653061224489,"as a loss function, and our custom weight decay function. After training on Adam (we found 5000
505"
FOR ALL P SUCH THAT,0.6642857142857143,"epochs to work well), we then train briefly (usually 1000 epochs) using SGD with a smaller learning
506"
FOR ALL P SUCH THAT,0.6653061224489796,"rate to tighten the convergence.
507"
FOR ALL P SUCH THAT,0.6663265306122449,"The bottleneck structure of a trained network, as seen in Figure 1b and 2b, can be observed in the
508"
FOR ALL P SUCH THAT,0.6673469387755102,"spectra of both the representations Ap and the weight matrices Wp at each layer. As long as the
509"
FOR ALL P SUCH THAT,0.6683673469387755,"training is not over-regularized (λ too large) then the spectra reveals a clear separation between k∗
510"
FOR ALL P SUCH THAT,0.6693877551020408,"number of large values as the rest decay. In our experiments, λ = 0.001"
FOR ALL P SUCH THAT,0.6704081632653062,"˜L
to get good results. To
511"
FOR ALL P SUCH THAT,0.6714285714285714,"facilitate the formation of the bottleneck structure, L should be large, for our experiments we usually
512"
FOR ALL P SUCH THAT,0.6724489795918367,"use L = 20. Figure 2a shows how larger L, which have better separation between large and small
513"
FOR ALL P SUCH THAT,0.673469387755102,"singular values, lead to improved test performance.
514"
FOR ALL P SUCH THAT,0.6744897959183673,"As first noted in section 1.3, solving for the Cost Of Identity, the kinetic energy, and the Hamiltonian
515"
FOR ALL P SUCH THAT,0.6755102040816326,"H is difficult due to the instability of the pseudo-inverse. Although the relaxation (Kp +γI) improves
516"
FOR ALL P SUCH THAT,0.676530612244898,"the stability, we also utilize the solve function to avoid computing a pseudo-inverse altogether. The
517"
FOR ALL P SUCH THAT,0.6775510204081633,"stability of these computations rely on the boundedness of some additional properties: the path length
518
R
||∂pAp|| dp, as well as the magnitudes of Bp, and Bpσ(Ap)T from the Hamiltonian reformulation.
519"
FOR ALL P SUCH THAT,0.6785714285714286,"Figure 3 shows how their respective magnitudes remains relatively constant as the effective depth ˜L
520"
FOR ALL P SUCH THAT,0.6795918367346939,"grows.
521"
FOR ALL P SUCH THAT,0.6806122448979591,"For compute resources, these small networks are not particularly resource intensive. Even on a CPU,
522"
FOR ALL P SUCH THAT,0.6816326530612244,"it only takes a couple minutes to fully train a leaky ResNet.
523"
FOR ALL P SUCH THAT,0.6826530612244898,"NeurIPS Paper Checklist
524"
CLAIMS,0.6836734693877551,"1. Claims
525"
CLAIMS,0.6846938775510204,"Question: Do the main claims made in the abstract and introduction accurately reflect the
526"
CLAIMS,0.6857142857142857,"paper’s contributions and scope?
527"
CLAIMS,0.686734693877551,"Answer: [Yes]
528"
CLAIMS,0.6877551020408164,"Justification: The contribution section accurately describes our contributions, and all
529"
CLAIMS,0.6887755102040817,"theorems/propositions are proven in the main or the appendix.
530"
CLAIMS,0.689795918367347,"Guidelines:
531"
CLAIMS,0.6908163265306122,"• The answer NA means that the abstract and introduction do not include the claims
532"
CLAIMS,0.6918367346938775,"made in the paper.
533"
CLAIMS,0.6928571428571428,"• The abstract and/or introduction should clearly state the claims made, including the
534"
CLAIMS,0.6938775510204082,"contributions made in the paper and important assumptions and limitations. A No or
535"
CLAIMS,0.6948979591836735,"NA answer to this question will not be perceived well by the reviewers.
536"
CLAIMS,0.6959183673469388,"• The claims made should match theoretical and experimental results, and reflect how
537"
CLAIMS,0.6969387755102041,"much the results can be expected to generalize to other settings.
538"
CLAIMS,0.6979591836734694,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
539"
CLAIMS,0.6989795918367347,"are not attained by the paper.
540"
LIMITATIONS,0.7,"2. Limitations
541"
LIMITATIONS,0.7010204081632653,"Question: Does the paper discuss the limitations of the work performed by the authors?
542"
LIMITATIONS,0.7020408163265306,"Answer: [Yes]
543"
LIMITATIONS,0.7030612244897959,"Justification: We discuss limitations of our results and approach after we state them.
544"
LIMITATIONS,0.7040816326530612,"Guidelines:
545"
LIMITATIONS,0.7051020408163265,"• The answer NA means that the paper has no limitation while the answer No means that
546"
LIMITATIONS,0.7061224489795919,"the paper has limitations, but those are not discussed in the paper.
547"
LIMITATIONS,0.7071428571428572,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
548"
LIMITATIONS,0.7081632653061225,"• The paper should point out any strong assumptions and how robust the results are to
549"
LIMITATIONS,0.7091836734693877,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
550"
LIMITATIONS,0.710204081632653,"model well-specification, asymptotic approximations only holding locally). The authors
551"
LIMITATIONS,0.7112244897959183,"should reflect on how these assumptions might be violated in practice and what the
552"
LIMITATIONS,0.7122448979591837,"implications would be.
553"
LIMITATIONS,0.713265306122449,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
554"
LIMITATIONS,0.7142857142857143,"only tested on a few datasets or with a few runs. In general, empirical results often
555"
LIMITATIONS,0.7153061224489796,"depend on implicit assumptions, which should be articulated.
556"
LIMITATIONS,0.7163265306122449,"• The authors should reflect on the factors that influence the performance of the approach.
557"
LIMITATIONS,0.7173469387755103,"For example, a facial recognition algorithm may perform poorly when image resolution
558"
LIMITATIONS,0.7183673469387755,"is low or images are taken in low lighting. Or a speech-to-text system might not be
559"
LIMITATIONS,0.7193877551020408,"used reliably to provide closed captions for online lectures because it fails to handle
560"
LIMITATIONS,0.7204081632653061,"technical jargon.
561"
LIMITATIONS,0.7214285714285714,"• The authors should discuss the computational efficiency of the proposed algorithms
562"
LIMITATIONS,0.7224489795918367,"and how they scale with dataset size.
563"
LIMITATIONS,0.7234693877551021,"• If applicable, the authors should discuss possible limitations of their approach to
564"
LIMITATIONS,0.7244897959183674,"address problems of privacy and fairness.
565"
LIMITATIONS,0.7255102040816327,"• While the authors might fear that complete honesty about limitations might be used
566"
LIMITATIONS,0.726530612244898,"by reviewers as grounds for rejection, a worse outcome might be that reviewers
567"
LIMITATIONS,0.7275510204081632,"discover limitations that aren’t acknowledged in the paper. The authors should use
568"
LIMITATIONS,0.7285714285714285,"their best judgment and recognize that individual actions in favor of transparency play
569"
LIMITATIONS,0.7295918367346939,"an important role in developing norms that preserve the integrity of the community.
570"
LIMITATIONS,0.7306122448979592,"Reviewers will be specifically instructed to not penalize honesty concerning limitations.
571"
THEORY ASSUMPTIONS AND PROOFS,0.7316326530612245,"3. Theory Assumptions and Proofs
572"
THEORY ASSUMPTIONS AND PROOFS,0.7326530612244898,"Question: For each theoretical result, does the paper provide the full set of assumptions and
573"
THEORY ASSUMPTIONS AND PROOFS,0.7336734693877551,"a complete (and correct) proof?
574"
THEORY ASSUMPTIONS AND PROOFS,0.7346938775510204,"Answer: [Yes]
575"
THEORY ASSUMPTIONS AND PROOFS,0.7357142857142858,"Justification: All assumptions are stated in the Theorem statements.
576"
THEORY ASSUMPTIONS AND PROOFS,0.736734693877551,"Guidelines:
577"
THEORY ASSUMPTIONS AND PROOFS,0.7377551020408163,"• The answer NA means that the paper does not include theoretical results.
578"
THEORY ASSUMPTIONS AND PROOFS,0.7387755102040816,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
579"
THEORY ASSUMPTIONS AND PROOFS,0.7397959183673469,"referenced.
580"
THEORY ASSUMPTIONS AND PROOFS,0.7408163265306122,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
581"
THEORY ASSUMPTIONS AND PROOFS,0.7418367346938776,"• The proofs can either appear in the main paper or the supplemental material, but if
582"
THEORY ASSUMPTIONS AND PROOFS,0.7428571428571429,"they appear in the supplemental material, the authors are encouraged to provide a short
583"
THEORY ASSUMPTIONS AND PROOFS,0.7438775510204082,"proof sketch to provide intuition.
584"
THEORY ASSUMPTIONS AND PROOFS,0.7448979591836735,"• Inversely, any informal proof provided in the core of the paper should be complemented
585"
THEORY ASSUMPTIONS AND PROOFS,0.7459183673469387,"by formal proofs provided in appendix or supplemental material.
586"
THEORY ASSUMPTIONS AND PROOFS,0.746938775510204,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7479591836734694,"4. Experimental Result Reproducibility
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7489795918367347,"Question: Does the paper fully disclose all the information needed to reproduce the
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"main experimental results of the paper to the extent that it affects the main claims and/or
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7510204081632653,"conclusions of the paper (regardless of whether the code and data are provided or not)?
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7520408163265306,"Answer: [Yes]
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.753061224489796,"Justification: The experimental setup is described in the Appendix.
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7540816326530613,"Guidelines:
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7551020408163265,"• The answer NA means that the paper does not include experiments.
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7561224489795918,"• If the paper includes experiments, a No answer to this question will not be perceived
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7571428571428571,"well by the reviewers: Making the paper reproducible is important, regardless of
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7581632653061224,"whether the code and data are provided or not.
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7591836734693878,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7602040816326531,"to make their results reproducible or verifiable.
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7612244897959184,"• Depending on the contribution, reproducibility can be accomplished in various ways.
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7622448979591837,"For example, if the contribution is a novel architecture, describing the architecture fully
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.763265306122449,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7642857142857142,"be necessary to either make it possible for others to replicate the model with the same
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7653061224489796,"dataset, or provide access to the model. In general. releasing code and data is often
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7663265306122449,"one good way to accomplish this, but reproducibility can also be provided via detailed
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7673469387755102,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7683673469387755,"of a large language model), releasing of a model checkpoint, or other means that are
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693877551020408,"appropriate to the research performed.
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7704081632653061,"• While NeurIPS does not require releasing code, the conference does require all
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714285714285715,"submissions to provide some reasonable avenue for reproducibility, which may depend
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7724489795918368,"on the nature of the contribution. For example
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.773469387755102,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7744897959183673,"to reproduce that algorithm.
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7755102040816326,"(b) If the contribution is primarily a new model architecture, the paper should describe
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7765306122448979,"the architecture clearly and fully.
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775510204081633,"(c) If the contribution is a new model (e.g., a large language model), then there should
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785714285714286,"either be a way to access this model for reproducing the results or a way to reproduce
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7795918367346939,"the model (e.g., with an open-source dataset or instructions for how to construct
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7806122448979592,"the dataset).
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7816326530612245,"(d) We recognize that reproducibility may be tricky in some cases, in which case
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7826530612244897,"authors are welcome to describe the particular way they provide for reproducibility.
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7836734693877551,"In the case of closed-source models, it may be that access to the model is limited in
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7846938775510204,"some way (e.g., to registered users), but it should be possible for other researchers
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7857142857142857,"to have some path to reproducing or verifying the results.
625"
OPEN ACCESS TO DATA AND CODE,0.786734693877551,"5. Open access to data and code
626"
OPEN ACCESS TO DATA AND CODE,0.7877551020408163,"Question: Does the paper provide open access to the data and code, with sufficient
627"
OPEN ACCESS TO DATA AND CODE,0.7887755102040817,"instructions to faithfully reproduce the main experimental results, as described in
628"
OPEN ACCESS TO DATA AND CODE,0.789795918367347,"supplemental material?
629"
OPEN ACCESS TO DATA AND CODE,0.7908163265306123,"Answer: [No]
630"
OPEN ACCESS TO DATA AND CODE,0.7918367346938775,"Justification: We use synthetic data, with a description of how to build this synthetic data.
631"
OPEN ACCESS TO DATA AND CODE,0.7928571428571428,"The code is not the main contribution of the paper, so there is little reason to publish it.
632"
OPEN ACCESS TO DATA AND CODE,0.7938775510204081,"Guidelines:
633"
OPEN ACCESS TO DATA AND CODE,0.7948979591836735,"• The answer NA means that paper does not include experiments requiring code.
634"
OPEN ACCESS TO DATA AND CODE,0.7959183673469388,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
635"
OPEN ACCESS TO DATA AND CODE,0.7969387755102041,"public/guides/CodeSubmissionPolicy) for more details.
636"
OPEN ACCESS TO DATA AND CODE,0.7979591836734694,"• While we encourage the release of code and data, we understand that this might not be
637"
OPEN ACCESS TO DATA AND CODE,0.7989795918367347,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
638"
OPEN ACCESS TO DATA AND CODE,0.8,"including code, unless this is central to the contribution (e.g., for a new open-source
639"
OPEN ACCESS TO DATA AND CODE,0.8010204081632653,"benchmark).
640"
OPEN ACCESS TO DATA AND CODE,0.8020408163265306,"• The instructions should contain the exact command and environment needed to run to
641"
OPEN ACCESS TO DATA AND CODE,0.8030612244897959,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
642"
OPEN ACCESS TO DATA AND CODE,0.8040816326530612,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
643"
OPEN ACCESS TO DATA AND CODE,0.8051020408163265,"• The authors should provide instructions on data access and preparation, including how
644"
OPEN ACCESS TO DATA AND CODE,0.8061224489795918,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
645"
OPEN ACCESS TO DATA AND CODE,0.8071428571428572,"• The authors should provide scripts to reproduce all experimental results for the new
646"
OPEN ACCESS TO DATA AND CODE,0.8081632653061225,"proposed method and baselines. If only a subset of experiments are reproducible, they
647"
OPEN ACCESS TO DATA AND CODE,0.8091836734693878,"should state which ones are omitted from the script and why.
648"
OPEN ACCESS TO DATA AND CODE,0.810204081632653,"• At submission time, to preserve anonymity, the authors should release anonymized
649"
OPEN ACCESS TO DATA AND CODE,0.8112244897959183,"versions (if applicable).
650"
OPEN ACCESS TO DATA AND CODE,0.8122448979591836,"• Providing as much information as possible in supplemental material (appended to the
651"
OPEN ACCESS TO DATA AND CODE,0.813265306122449,"paper) is recommended, but including URLs to data and code is permitted.
652"
OPEN ACCESS TO DATA AND CODE,0.8142857142857143,"6. Experimental Setting/Details
653"
OPEN ACCESS TO DATA AND CODE,0.8153061224489796,"Question: Does the paper specify all the training and test details (e.g., data splits,
654"
OPEN ACCESS TO DATA AND CODE,0.8163265306122449,"hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand
655"
OPEN ACCESS TO DATA AND CODE,0.8173469387755102,"the results?
656"
OPEN ACCESS TO DATA AND CODE,0.8183673469387756,"Answer: [Yes]
657"
OPEN ACCESS TO DATA AND CODE,0.8193877551020409,"Justification: Most details are given in the experimental setup section in the Appendix.
658"
OPEN ACCESS TO DATA AND CODE,0.8204081632653061,"Guidelines:
659"
OPEN ACCESS TO DATA AND CODE,0.8214285714285714,"• The answer NA means that the paper does not include experiments.
660"
OPEN ACCESS TO DATA AND CODE,0.8224489795918367,"• The experimental setting should be presented in the core of the paper to a level of detail
661"
OPEN ACCESS TO DATA AND CODE,0.823469387755102,"that is necessary to appreciate the results and make sense of them.
662"
OPEN ACCESS TO DATA AND CODE,0.8244897959183674,"• The full details can be provided either with the code, in appendix, or as supplemental
663"
OPEN ACCESS TO DATA AND CODE,0.8255102040816327,"material.
664"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.826530612244898,"7. Experiment Statistical Significance
665"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8275510204081633,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
666"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8285714285714286,"information about the statistical significance of the experiments?
667"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8295918367346938,"Answer: [No]
668"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8306122448979592,"Justification: The numerical experiments are mostly there as a visualization of the theoretical
669"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8316326530612245,"results, our main goal is therefore clarity, which would be hurt by putting error bars
670"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326530612244898,"everywhere.
671"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336734693877551,"Guidelines:
672"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8346938775510204,"• The answer NA means that the paper does not include experiments.
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8357142857142857,"• The authors should answer ""Yes"" if the results are accompanied by error bars,
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8367346938775511,"confidence intervals, or statistical significance tests, at least for the experiments that
675"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8377551020408164,"support the main claims of the paper.
676"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8387755102040816,"• The factors of variability that the error bars are capturing should be clearly stated (for
677"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8397959183673469,"example, train/test split, initialization, random drawing of some parameter, or overall
678"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8408163265306122,"run with given experimental conditions).
679"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8418367346938775,"• The method for calculating the error bars should be explained (closed form formula,
680"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8428571428571429,"call to a library function, bootstrap, etc.)
681"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8438775510204082,"• The assumptions made should be given (e.g., Normally distributed errors).
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8448979591836735,"• It should be clear whether the error bar is the standard deviation or the standard error
683"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8459183673469388,"of the mean.
684"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8469387755102041,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
685"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8479591836734693,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
686"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8489795918367347,"of Normality of errors is not verified.
687"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.85,"• For asymmetric distributions, the authors should be careful not to show in tables or
688"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8510204081632653,"figures symmetric error bars that would yield results that are out of range (e.g. negative
689"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8520408163265306,"error rates).
690"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8530612244897959,"• If error bars are reported in tables or plots, The authors should explain in the text how
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8540816326530613,"they were calculated and reference the corresponding figures or tables in the text.
692"
EXPERIMENTS COMPUTE RESOURCES,0.8551020408163266,"8. Experiments Compute Resources
693"
EXPERIMENTS COMPUTE RESOURCES,0.8561224489795919,"Question: For each experiment, does the paper provide sufficient information on the
694"
EXPERIMENTS COMPUTE RESOURCES,0.8571428571428571,"computer resources (type of compute workers, memory, time of execution) needed to
695"
EXPERIMENTS COMPUTE RESOURCES,0.8581632653061224,"reproduce the experiments?
696"
EXPERIMENTS COMPUTE RESOURCES,0.8591836734693877,"Answer: [Yes]
697"
EXPERIMENTS COMPUTE RESOURCES,0.860204081632653,"Justification: In the experimental setup section of the Appendix.
698"
EXPERIMENTS COMPUTE RESOURCES,0.8612244897959184,"Guidelines:
699"
EXPERIMENTS COMPUTE RESOURCES,0.8622448979591837,"• The answer NA means that the paper does not include experiments.
700"
EXPERIMENTS COMPUTE RESOURCES,0.863265306122449,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
701"
EXPERIMENTS COMPUTE RESOURCES,0.8642857142857143,"or cloud provider, including relevant memory and storage.
702"
EXPERIMENTS COMPUTE RESOURCES,0.8653061224489796,"• The paper should provide the amount of compute required for each of the individual
703"
EXPERIMENTS COMPUTE RESOURCES,0.8663265306122448,"experimental runs as well as estimate the total compute.
704"
EXPERIMENTS COMPUTE RESOURCES,0.8673469387755102,"• The paper should disclose whether the full research project required more compute
705"
EXPERIMENTS COMPUTE RESOURCES,0.8683673469387755,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
706"
EXPERIMENTS COMPUTE RESOURCES,0.8693877551020408,"didn’t make it into the paper).
707"
CODE OF ETHICS,0.8704081632653061,"9. Code Of Ethics
708"
CODE OF ETHICS,0.8714285714285714,"Question: Does the research conducted in the paper conform, in every respect, with the
709"
CODE OF ETHICS,0.8724489795918368,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
710"
CODE OF ETHICS,0.8734693877551021,"Answer: [Yes]
711"
CODE OF ETHICS,0.8744897959183674,"Justification: We have read the Code of Ethics and see no issue.
712"
CODE OF ETHICS,0.8755102040816326,"Guidelines:
713"
CODE OF ETHICS,0.8765306122448979,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
714"
CODE OF ETHICS,0.8775510204081632,"• If the authors answer No, they should explain the special circumstances that require a
715"
CODE OF ETHICS,0.8785714285714286,"deviation from the Code of Ethics.
716"
CODE OF ETHICS,0.8795918367346939,"• The authors should make sure to preserve anonymity (e.g., if there is a special
717"
CODE OF ETHICS,0.8806122448979592,"consideration due to laws or regulations in their jurisdiction).
718"
BROADER IMPACTS,0.8816326530612245,"10. Broader Impacts
719"
BROADER IMPACTS,0.8826530612244898,"Question: Does the paper discuss both potential positive societal impacts and negative
720"
BROADER IMPACTS,0.8836734693877552,"societal impacts of the work performed?
721"
BROADER IMPACTS,0.8846938775510204,"Answer: [NA]
722"
BROADER IMPACTS,0.8857142857142857,"Justification: The paper is theoretical in nature, so it has no direct societal impact that can
723"
BROADER IMPACTS,0.886734693877551,"be meaningfully discussed.
724"
BROADER IMPACTS,0.8877551020408163,"Guidelines:
725"
BROADER IMPACTS,0.8887755102040816,"• The answer NA means that there is no societal impact of the work performed.
726"
BROADER IMPACTS,0.889795918367347,"• If the authors answer NA or No, they should explain why their work has no societal
727"
BROADER IMPACTS,0.8908163265306123,"impact or why the paper does not address societal impact.
728"
BROADER IMPACTS,0.8918367346938776,"• Examples of negative societal impacts include potential malicious or unintended uses
729"
BROADER IMPACTS,0.8928571428571429,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
730"
BROADER IMPACTS,0.8938775510204081,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
731"
BROADER IMPACTS,0.8948979591836734,"groups), privacy considerations, and security considerations.
732"
BROADER IMPACTS,0.8959183673469387,"• The conference expects that many papers will be foundational research and not tied
733"
BROADER IMPACTS,0.8969387755102041,"to particular applications, let alone deployments. However, if there is a direct path to
734"
BROADER IMPACTS,0.8979591836734694,"any negative applications, the authors should point it out. For example, it is legitimate
735"
BROADER IMPACTS,0.8989795918367347,"to point out that an improvement in the quality of generative models could be used to
736"
BROADER IMPACTS,0.9,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
737"
BROADER IMPACTS,0.9010204081632653,"that a generic algorithm for optimizing neural networks could enable people to train
738"
BROADER IMPACTS,0.9020408163265307,"models that generate Deepfakes faster.
739"
BROADER IMPACTS,0.9030612244897959,"• The authors should consider possible harms that could arise when the technology is
740"
BROADER IMPACTS,0.9040816326530612,"being used as intended and functioning correctly, harms that could arise when the
741"
BROADER IMPACTS,0.9051020408163265,"technology is being used as intended but gives incorrect results, and harms following
742"
BROADER IMPACTS,0.9061224489795918,"from (intentional or unintentional) misuse of the technology.
743"
BROADER IMPACTS,0.9071428571428571,"• If there are negative societal impacts, the authors could also discuss possible mitigation
744"
BROADER IMPACTS,0.9081632653061225,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
745"
BROADER IMPACTS,0.9091836734693878,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
746"
BROADER IMPACTS,0.9102040816326531,"feedback over time, improving the efficiency and accessibility of ML).
747"
SAFEGUARDS,0.9112244897959184,"11. Safeguards
748"
SAFEGUARDS,0.9122448979591836,"Question: Does the paper describe safeguards that have been put in place for responsible
749"
SAFEGUARDS,0.9132653061224489,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
750"
SAFEGUARDS,0.9142857142857143,"image generators, or scraped datasets)?
751"
SAFEGUARDS,0.9153061224489796,"Answer: [NA]
752"
SAFEGUARDS,0.9163265306122449,"Justification: Not relevant to our paper.
753"
SAFEGUARDS,0.9173469387755102,"Guidelines:
754"
SAFEGUARDS,0.9183673469387755,"• The answer NA means that the paper poses no such risks.
755"
SAFEGUARDS,0.9193877551020408,"• Released models that have a high risk for misuse or dual-use should be released with
756"
SAFEGUARDS,0.9204081632653062,"necessary safeguards to allow for controlled use of the model, for example by requiring
757"
SAFEGUARDS,0.9214285714285714,"that users adhere to usage guidelines or restrictions to access the model or implementing
758"
SAFEGUARDS,0.9224489795918367,"safety filters.
759"
SAFEGUARDS,0.923469387755102,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
760"
SAFEGUARDS,0.9244897959183673,"should describe how they avoided releasing unsafe images.
761"
SAFEGUARDS,0.9255102040816326,"• We recognize that providing effective safeguards is challenging, and many papers do
762"
SAFEGUARDS,0.926530612244898,"not require this, but we encourage authors to take this into account and make a best
763"
SAFEGUARDS,0.9275510204081633,"faith effort.
764"
LICENSES FOR EXISTING ASSETS,0.9285714285714286,"12. Licenses for existing assets
765"
LICENSES FOR EXISTING ASSETS,0.9295918367346939,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
766"
LICENSES FOR EXISTING ASSETS,0.9306122448979591,"the paper, properly credited and are the license and terms of use explicitly mentioned and
767"
LICENSES FOR EXISTING ASSETS,0.9316326530612244,"properly respected?
768"
LICENSES FOR EXISTING ASSETS,0.9326530612244898,"Answer: [NA]
769"
LICENSES FOR EXISTING ASSETS,0.9336734693877551,"Justification: We only use our own synthetic data.
770"
LICENSES FOR EXISTING ASSETS,0.9346938775510204,"Guidelines:
771"
LICENSES FOR EXISTING ASSETS,0.9357142857142857,"• The answer NA means that the paper does not use existing assets.
772"
LICENSES FOR EXISTING ASSETS,0.936734693877551,"• The authors should cite the original paper that produced the code package or dataset.
773"
LICENSES FOR EXISTING ASSETS,0.9377551020408164,"• The authors should state which version of the asset is used and, if possible, include a
774"
LICENSES FOR EXISTING ASSETS,0.9387755102040817,"URL.
775"
LICENSES FOR EXISTING ASSETS,0.939795918367347,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
776"
LICENSES FOR EXISTING ASSETS,0.9408163265306122,"• For scraped data from a particular source (e.g., website), the copyright and terms of
777"
LICENSES FOR EXISTING ASSETS,0.9418367346938775,"service of that source should be provided.
778"
LICENSES FOR EXISTING ASSETS,0.9428571428571428,"• If assets are released, the license, copyright information, and terms of use in the
779"
LICENSES FOR EXISTING ASSETS,0.9438775510204082,"package should be provided. For popular datasets, paperswithcode.com/datasets
780"
LICENSES FOR EXISTING ASSETS,0.9448979591836735,"has curated licenses for some datasets. Their licensing guide can help determine the
781"
LICENSES FOR EXISTING ASSETS,0.9459183673469388,"license of a dataset.
782"
LICENSES FOR EXISTING ASSETS,0.9469387755102041,"• For existing datasets that are re-packaged, both the original license and the license of
783"
LICENSES FOR EXISTING ASSETS,0.9479591836734694,"the derived asset (if it has changed) should be provided.
784"
LICENSES FOR EXISTING ASSETS,0.9489795918367347,"• If this information is not available online, the authors are encouraged to reach out to
785"
LICENSES FOR EXISTING ASSETS,0.95,"the asset’s creators.
786"
NEW ASSETS,0.9510204081632653,"13. New Assets
787"
NEW ASSETS,0.9520408163265306,"Question: Are new assets introduced in the paper well documented and is the documentation
788"
NEW ASSETS,0.9530612244897959,"provided alongside the assets?
789"
NEW ASSETS,0.9540816326530612,"Answer: [NA]
790"
NEW ASSETS,0.9551020408163265,"Justification: We do not release any new assets.
791"
NEW ASSETS,0.9561224489795919,"Guidelines:
792"
NEW ASSETS,0.9571428571428572,"• The answer NA means that the paper does not release new assets.
793"
NEW ASSETS,0.9581632653061225,"• Researchers should communicate the details of the dataset/code/model as part of their
794"
NEW ASSETS,0.9591836734693877,"submissions via structured templates. This includes details about training, license,
795"
NEW ASSETS,0.960204081632653,"limitations, etc.
796"
NEW ASSETS,0.9612244897959183,"• The paper should discuss whether and how consent was obtained from people whose
797"
NEW ASSETS,0.9622448979591837,"asset is used.
798"
NEW ASSETS,0.963265306122449,"• At submission time, remember to anonymize your assets (if applicable). You can either
799"
NEW ASSETS,0.9642857142857143,"create an anonymized URL or include an anonymized zip file.
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9653061224489796,"14. Crowdsourcing and Research with Human Subjects
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9663265306122449,"Question: For crowdsourcing experiments and research with human subjects, does the paper
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9673469387755103,"include the full text of instructions given to participants and screenshots, if applicable, as
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9683673469387755,"well as details about compensation (if any)?
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693877551020408,"Answer: [NA]
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704081632653061,"Justification: Not relevant to this paper.
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714285714285714,"Guidelines:
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724489795918367,"• The answer NA means that the paper does not involve crowdsourcing nor research with
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9734693877551021,"human subjects.
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744897959183674,"• Including this information in the supplemental material is fine, but if the main
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9755102040816327,"contribution of the paper involves human subjects, then as much detail as possible
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976530612244898,"should be included in the main paper.
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775510204081632,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785714285714285,"or other labor should be paid at least the minimum wage in the country of the data
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795918367346939,"collector.
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806122448979592,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816326530612245,"Subjects
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826530612244898,"Question: Does the paper describe potential risks incurred by study participants, whether
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836734693877551,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846938775510204,"approvals (or an equivalent approval/review based on the requirements of your country or
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857142857142858,"institution) were obtained?
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986734693877551,"Answer: [NA]
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877551020408163,"Justification: Not relevant to this paper.
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887755102040816,"Guidelines:
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897959183673469,"• The answer NA means that the paper does not involve crowdsourcing nor research with
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908163265306122,"human subjects.
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918367346938776,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928571428571429,"may be required for any human subjects research. If you obtained IRB approval, you
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938775510204082,"should clearly state this in the paper.
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948979591836735,"• We recognize that the procedures for this may vary significantly between institutions
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959183673469387,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996938775510204,"guidelines for their institution.
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979591836734694,"• For initial submissions, do not include any information that would break anonymity (if
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989795918367347,"applicable), such as the institution conducting the review.
834"
