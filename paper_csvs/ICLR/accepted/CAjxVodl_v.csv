Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013550135501355014,"How to extract as much learning signal from each trajectory data has been a key
problem in reinforcement learning (RL), where sample inefﬁciency has posed
serious challenges for practical applications. Recent works have shown that using
expressive policy function approximators and conditioning on future trajectory
information – such as future states in hindsight experience replay (HER) or returns-
to-go in Decision Transformer (DT) – enables efﬁcient learning of multi-task
policies, where at times online RL is fully replaced by ofﬂine behavioral cloning
(BC), e.g. sequence modeling. We demonstrate that all these approaches are doing
hindsight information matching (HIM) – training policies that can output the rest
of trajectory that matches some statistics of future state information. We present
Generalized Decision Transformer (GDT) for solving any HIM problem, and show
how different choices for the feature function and the anti-causal aggregator not
only recover DT as a special case, but also lead to novel Categorical DT (CDT)
and Bi-directional DT (BDT) for matching different statistics of the future. For
evaluating CDT and BDT, we deﬁne ofﬂine multi-task state-marginal matching
(SMM) and imitation learning (IL) as two generic HIM problems, propose a
Wasserstein distance loss as a metric for both, and empirically study them on
MuJoCo continuous control benchmarks. Categorical DT, which simply replaces
anti-causal summation with anti-causal binning in DT, enables arguably the ﬁrst
effective ofﬂine multi-task SMM algorithm that generalizes well to unseen (and
even synthetic) multi-modal reward or state-feature distributions. Bi-directional
DT, which uses an anti-causal second transformer as the aggregator, can learn to
model any statistics of the future and outperforms DT variants in ofﬂine multi-task
IL, i.e. one-shot IL. Our generalized formulations from HIM and GDT greatly
expand the role of powerful sequence modeling architectures in modern RL."
INTRODUCTION,0.0027100271002710027,"1
INTRODUCTION"
INTRODUCTION,0.0040650406504065045,"Reinforcement learning (RL) suffers from the problem of sample inefﬁciency, and a central question
is how to extract as much learning signals, or constraint equations (Pong et al., 2018; Tu & Recht,
2019; Dean et al., 2020), from each trajectory data as possible. As dynamics transitions and Bellman
equation provide a rich source of supervisory objectives and constraints, many algorithms combined
model-free with model-based, and policy-based with value-based in order to achieve maximal sample
efﬁciency, while approximately preserving stable, unbiased policy learning (Heess et al., 2015; Gu
et al., 2016; 2017; Buckman et al., 2018; Pong et al., 2018; Tu & Recht, 2019)."
INTRODUCTION,0.005420054200542005,"Orthogonal to these, in the recent years we have seen a number of algorithms that are derived
from different motivations and frameworks, but share the following common trait: they use future
trajectory information τt:T to accelerate optimization of a contextual policy π(at|st, z) with
context z with respect to a parameterized reward function r(st, at, z) (see Section 3 for notations).
These hindsight algorithms have enabled Q-learning with sparse rewards (Andrychowicz et al.,
2017), temporally-extended model-based RL with Q-function (Pong et al., 2018), mastery of 6-DoF
object manipulation in cluttered scenes from human play (Lynch et al., 2019), efﬁcient multi-task
RL (Eysenbach et al., 2020; Li et al., 2020), ofﬂine self-supervised discovery of manipulation
primitives from pixels (Chebotar et al., 2021), and ofﬂine RL using return-conditioned supervised"
INTRODUCTION,0.006775067750677507,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008130081300813009,"learning with transformers (Chen et al., 2021a; Janner et al., 2021). We derive a generic problem
formulation covering all these variants, and observe that this hindsight information matching
(HIM) framework, with behavioral cloning (BC) as the learning objective, can learn a conditional
policy to generate trajectories that each satisfy any properties, including distributional."
INTRODUCTION,0.009485094850948509,"Given this insight and recent casting of RL as sequence modeling (Chen et al., 2021a; Janner et al.,
2021), we propose Generalized Decision Transformer (GDT), a family of algorithms for future
information matching using hindsight behavioral cloning with transformers, and greatly expand
the applicability of transformers and other powerful sequential modeling architectures within RL
with only small architectural changes to DT. In summary, our key contributions are:"
INTRODUCTION,0.01084010840108401,"• We introduce hindsight information matching (HIM) (Section 4, Table 1) as a unifying view
of existing hindsight-inspired algorithms, and Generalized Decision Transformers (GDT) as a
generalization of DT for RL as sequence modeling to solve any HIM problem (Figure 1).
• Inspired by distribution RL (Bellemare et al., 2017; Dabney et al., 2018) and state-marginal
matching (SMM) (Lee et al., 2020; Ghasemipour et al., 2020; Gu et al., 2021), we deﬁne ofﬂine
multi-task SMM problems, propose Categorical DT (CDT) (Section 5), validate its empirical
performance to match feature distributions (even generalizing to a synthetic bi-modal target
distribution at times), and construct the ﬁrst benchmark tasks for ofﬂine multi-task SMM.
• Inspired by one-shot imitation learning (Duan et al., 2017; Finn et al., 2017; Dasari & Gupta,"
INTRODUCTION,0.012195121951219513,"2020), we deﬁne ofﬂine multi-task imitation learning (IL), propose a Wasserstein-distance
evaluation metric, develop Bi-directional DT (BDT) as a fully expressive variant of GDT
(Section 5), and demonstrate BDT’s competitive performance at ofﬂine multi-task IL."
INTRODUCTION,0.013550135501355014,"Method
Φ(s, a)
Aggregator
DT (Chen et al., 2021a)
r(s, a)
Summation
DT-X (Section 5.3)
Learned
Summation
CDT (Section 5.2)
r(s, a) or any
Binning
BDT (Section 5.4)
Learned
Transformer"
INTRODUCTION,0.014905149051490514,"Figure 1: Generalized Decision Transformer (GDT), where the ﬁgure is a minor generalization of the DT
architecture (Chen et al., 2021a) and the table summarizes how it leads to different classes of algorithms
with only small architectural changes. If the feature function Φ(s, a) is reward r(s, a) and the anti-causal
aggregator is γ-discounted summation, we recover DT for ofﬂine RL. If the aggregator is binning, we get
Categorical DT (CDT) for ofﬂine multi-task state-marginal matching. If the aggregator is a second transformer,
we get Bi-directional DT (BDT) for ofﬂine multi-task imitation learning (IL), or equivalently one-shot IL. The
choices of Φ(s, a) and the aggregator together decide IΦ(τ) in Hindsight Information Matching (HIM) objective
discussed in Section 4 and Table 1, where conversely GDT can essentially solve any HIM problem with proper
choices of Φ and aggregator."
RELATED WORK,0.016260162601626018,"2
RELATED WORK"
RELATED WORK,0.017615176151761516,"Hindsight Reinforcement Learning and Behavior Cloning Hindsight techniques (Kaelbling,
1993; Andrychowicz et al., 2017; Pong et al., 2018) have revolutionized off-policy optimization
with respect to parameterized reward functions. Two key insights were (1) for off-policy algorithms
such as Q-learning (Mnih et al., 2015; Gu et al., 2016) and actor-critic methods (Lillicrap et al.,
2016; Haarnoja et al., 2018; Fujimoto et al., 2018; Furuta et al., 2021a), the same transition samples
can be used to learn with respect to any reward parameters, as long as the reward function is
re-computable, i.e. “relabel”-able, like goal reaching rewards, and (2) if policy or Q-functions
are smooth with respect to the reward parameter, generalization can speed up learning even with
respect to “unexplored” rewards. In goal-based RL where future states can inform “optimal” reward
parameters with respect to the transitions’ actions, hindsight methods were applied successfully to"
RELATED WORK,0.018970189701897018,Published as a conference paper at ICLR 2022
RELATED WORK,0.02032520325203252,"enable effective training of goal-based Q-function for sparse rewards (Andrychowicz et al., 2017),
derive exact connections between Q-learning and classic model-based RL (Pong et al., 2018), data-
efﬁcient off-policy hierarchical RL (Nachum et al., 2018), multi-task RL (Eysenbach et al., 2020; Li
et al., 2020), ofﬂine RL (Chebotar et al., 2021), and more (Eysenbach et al., 2021; Choi et al., 2021;
Ren et al., 2019; Zhao & Tresp, 2018; Ghosh et al., 2021; Nasiriany et al., 2021). Additionally, Lynch
et al. (2019) and Gupta et al. (2018) have shown that often BC is sufﬁcient for learning generalizable
parameterized policies, due to rich positive examples from future states, and most recently Chen et al.
(2021a) and Janner et al. (2021), when combined with powerful transformer architectures (Vaswani
et al., 2017), it produced state-of-the-art ofﬂine RL and goal-based RL results. Lastly, while motivated
from alternative mathematical principles and not for parameterized objectives, future state information
was also explored as ways of reducing variance or improving estimations for generic policy gradient
methods (Pinto et al., 2017; Guo et al., 2021; Venuto et al., 2021)."
RELATED WORK,0.02168021680216802,"Distributional Reinforcement Learning and State-Marginal Matching Modeling the full distri-
bution of returns instead of the averages led to the development of distributional RL algorithms (Belle-
mare et al., 2017; Dabney et al., 2018; 2020; Castro et al., 2018; Barth-Maron et al., 2018) such as
Categorical Q-learning (Bellemare et al., 2017). While our work shares techniques such as discretiza-
tion and binning, these works focus on optimizing a non-conditional reward-maximizing RL policy
and therefore our problem deﬁnition is closer to that of state-marginal matching algorithms (Hazan
et al., 2019; Lee et al., 2020; Ghasemipour et al., 2020; Gu et al., 2021), or equivalently inverse RL
algorithms (Ziebart et al., 2008; Ho & Ermon, 2016; Finn et al., 2016; Fu et al., 2018; Ghasemipour
et al., 2020) whose connections to feature-expectation matching have been long discussed (Abbeel
& Ng, 2004). However, those are often exclusively online algorithms even sample-efﬁcient vari-
ants (Kostrikov et al., 2019), since density-ratio estimations with either discriminative (Ghasemipour
et al., 2020) or generative (Lee et al., 2020) approach requires on-policy samples, with a rare excep-
tion of Kostrikov et al. (2020). Building on the success of DT and brute-force hindsight imitation
learning, our Categorical DT is to the best our knowledge the ﬁrst method that benchmarks ofﬂine
state-marginal matching problem in the multi-task settings."
RELATED WORK,0.023035230352303523,"RL and Imitation Learning as Sequence Modeling When scaled to the extreme levels of data and
computing, sequence models such as transformers (Vaswani et al., 2017) can train models to master
an impressive range of capabilities in natural language processing and computer vision (Devlin et al.,
2019; Radford et al., 2019; Brown et al., 2020; Radford et al., 2021; Ramesh et al., 2021; Chen
et al., 2021b; Bommasani et al., 2021; Dosovitskiy et al., 2020). Comparing to their popularity
in other areas, the adoption of transformers or architectural innovations in RL have been slow,
partially due the difﬁculty of using transformers over temporal scales for online RL (Parisotto et al.,
2020). Recent successes have focused on processing variable-length per-timestep information such
as morphology (Kurin et al., 2021), sensory information (Tang & Ha, 2021), one-shot or few-shot
imitation learning (Dasari & Gupta, 2020), or leveraged ofﬂine learning (Chen et al., 2021a; Janner
et al., 2021). Our formulation enables sequence modeling to solve novel RL problems such as state-
marginal matching with minimal architectural modiﬁcations to DT, greatly expanding the impacts of
transformers and other powerful sequence models in RL."
PRELIMINARIES,0.024390243902439025,"3
PRELIMINARIES"
PRELIMINARIES,0.025745257452574527,"We consider a Markov Decision Process (MDP) deﬁned by the tuple of action space A, state space
S, transition probability function p(s′|s, a), initial state distribution p(s0), reward function r(s, a),
and discount factor γ ∈(0, 1]. In deep RL, a policy that maps the state space to the action space is
parameterized by the function approximators, πθ(a|s)1. The RL objective is given by:"
PRELIMINARIES,0.02710027100271003,"LRL(π) =
1
1 −γ Es∼ρπ(s),a∼π(·|s) [r(s, a)]
(1)"
PRELIMINARIES,0.028455284552845527,"where pπ
t (s) =
RR"
PRELIMINARIES,0.02981029810298103,"s0:t,a0:t−1
Q"
PRELIMINARIES,0.03116531165311653,"t p(st|st−1, at−1)π(at|st) and ρπ(s) = (1 −γ) P"
PRELIMINARIES,0.032520325203252036,"t′ γt′pπ
t′(st′ = s)
are short-hands for time-aligned and time-aggregated state marginal distributions following policy π."
PRELIMINARIES,0.03387533875338753,"1For simplicity of notations, we write Markovian policies; however, such notations can easily apply to
non-Markov policies such as Decision Transformer (Chen et al., 2021a) by converting to an augmented MDP
consisting of past N states, where N is the context window of DT."
PRELIMINARIES,0.03523035230352303,Published as a conference paper at ICLR 2022
STATE MARGINAL MATCHING,0.036585365853658534,"3.1
STATE MARGINAL MATCHING"
STATE MARGINAL MATCHING,0.037940379403794036,"State marginal matching (SMM) (Lee et al., 2020; Hazan et al., 2019; Ghasemipour et al., 2020) has
been recently studied as an alternative problem speciﬁcation in RL, where instead of stationary-reward
maximization, the objective is to ﬁnd a policy minimizing the divergence D between its state marginal
distribution ρπ(s) to a given target distribution p∗(s)2:"
STATE MARGINAL MATCHING,0.03929539295392954,"LSMM(π) = −D(ρπ(s), p∗(s))
(2)"
STATE MARGINAL MATCHING,0.04065040650406504,"where D is a divergence measure such as Kullback-Leibler (KL) divergence (Lee et al., 2020; Fu
et al., 2018) or, more generally, some f-divergences (Ghasemipour et al., 2020). For the target
distribution p∗(s), Lee et al. (2020) set a uniform distribution to enhance the exploration over the
entire state space; Ghasemipour et al. (2020) and Gu et al. (2021) set through scripted distribution
sketches to generate desired behaviors; and adversarial inverse RL methods (Ho & Ermon, 2016;
Fu et al., 2018; Ghasemipour et al., 2020; Kostrikov et al., 2020) set as the expert data for imitation
learning. Notably, unlike the RL objective in Eq.1, SMM objectives like Eq.2 no longer depend on
task rewards and are only functions of state transition dynamics and target state distribution."
PARAMETERIZED RL OBJECTIVES,0.04200542005420054,"3.2
PARAMETERIZED RL OBJECTIVES"
PARAMETERIZED RL OBJECTIVES,0.04336043360433604,"Lastly, we discuss the basis for methods like HER and TDM (Andrychowicz et al., 2017; Pong et al.,
2018), LfP (Lynch et al., 2019), and return-conditioned or upside-down RL (Srivastava et al., 2019;
Kumar et al., 2019; Chen et al., 2021a; Janner et al., 2021): parameterized RL objectives. Given
parameterized reward functions with parameter z ∈Z, a conditional policy π(a|s, z) is learned with
respect to multiple values of z simultaneously weighted by p(z). As examples, the RL objective
in Eq.1 becomes:"
PARAMETERIZED RL OBJECTIVES,0.044715447154471545,"LRL(π) = Ez [LRL(π, z)] =
1
1 −γ Ez∼p(z),s∼ρπ
z (s),a∼π(·|s,z) [rz(s, a)]
(3)"
PARAMETERIZED RL OBJECTIVES,0.04607046070460705,"where the state marginal ρπ
z is from rolling out a conditioned policy π(·|·, z). These can be considered
as a special case of contextual MDPs (Jiang et al., 2017) and are all multi-task RL problems."
HINDSIGHT INFORMATION MATCHING,0.04742547425474255,"4
HINDSIGHT INFORMATION MATCHING"
HINDSIGHT INFORMATION MATCHING,0.04878048780487805,"We show how HER and TDM (Andrychowicz et al., 2017; Pong et al., 2018), LfP (Lynch et al.,
2019), hindsight multi-task RL (Li et al., 2020; Eysenbach et al., 2020), and return-conditioned
or upside-down RL (Srivastava et al., 2019; Kumar et al., 2019; Chen et al., 2021a; Janner et al.,
2021) all belong to hindsight algorithms with a shared idea of using future state information
to automatically mine for positive, or “optimal”, examples with respect to certain contextual
parameter values, where these examples can accelerate RL or be used for behavior cloning (BC),
i.e. supervised learning. We start by deﬁning additional notations."
HINDSIGHT INFORMATION MATCHING,0.05013550135501355,"Given a partial trajectory from state st as τt = {st, at, st+1, at+1, . . . }, we deﬁne its information
statistics as I(τt). I(τt) could be any function of a trajectory that captures some statistical
properties in state-space or trajectory-space, such as sufﬁcient statistics of a distribution, like mean,
variance or higher-order moments (Wainwright & Jordan, 2008). For convenience, we further
deﬁne the notion of a feature function Φ(·, ·) : S × A →F 5, where the trajectory is then noted as
τ Φ
t = {φt, φt+1, . . . , φT }, φt = Φ(st, at) ∈F and the information statistics as IΦ(τt). Φ in practice"
HINDSIGHT INFORMATION MATCHING,0.051490514905149054,"2As discussed in Fu et al. (2018) and Ghasemipour et al. (2020), it’s also straight-forward to deﬁne state-
action-marginal matching with respect to ρπ(s, a) = ρπ(s)π(a|s) and the exact same algorithms apply.
3There is a rich literature on one-shot, or few-shot, imitation learning through meta learning. For closer
connections to parameterized policies and relabeling, we mainly discuss metric-based (or amortization-based)
methods (Duan et al., 2016), as opposed to gradient-based approaches (Finn et al., 2017)."
HINDSIGHT INFORMATION MATCHING,0.052845528455284556,"4DisCo RL (Nasiriany et al., 2021) conditions on a parameterized goal distribution and uses hindsight
techniques; however, their RL objective in Equation 1 Es∼ρπ
z (s) [log p∗
z(s)], in contrast to a proper divergence
objective like in Ghasemipour et al. (2020), is missing the H (ρπ
z (s)) entropy term and is essentially just solving
for parameterized stationary reward maximization, as also stated in their Remark 1.
5Recent mutual information maximization or empowerment methods (Eysenbach et al., 2019; Sharma et al.,
2020; Choi et al., 2021) also make similar assumptions; see Gu et al. (2021) for more details."
HINDSIGHT INFORMATION MATCHING,0.05420054200542006,Published as a conference paper at ICLR 2022
HINDSIGHT INFORMATION MATCHING,0.05555555555555555,"Method
Algo. Type
Training
IΦ(τ)
Architectures
Andrychowicz et al. (2017)
RL
Online
φT
MLP
Pong et al. (2018)
RL
Online
φT
MLP
Chebotar et al. (2021)
RL
Ofﬂine
φT
CNN
Li et al. (2020)
RL
Online
arg max P
t γtr(st, at, ·)
MLP
Eysenbach et al. (2020)
BC/RL
On/Ofﬂine
arg max P
t γtr(st, at, ·)
MLP
Lynch et al. (2019)
BC
Ofﬂine
φT
Stochastic RNN
Ghosh et al. (2021)
BC
Online
φT
MLP
Srivastava et al. (2019)
BC
Online
P
t γtrt
Fast Weights
Kumar et al. (2019)
BC
Online
P
t γtrt
MLP
Janner et al. (2021)
BC
Ofﬂine
P
t γtrt or φT
Transformer
Duan et al. (2017)3
BC
Ofﬂine
τ
MLP + LSTM
Generalized DT (ours)
BC
Ofﬂine
Any
Transformer
DT (Chen et al., 2021a)
BC
Ofﬂine
P
t γtrt
Transformer
Categorical DT (ours)4
BC
Ofﬂine
histogram(rt, γ)
Transformer
Bi-Directional DT (ours)
BC
Ofﬂine
τ
Transformer"
HINDSIGHT INFORMATION MATCHING,0.056910569105691054,"Table 1: A coarse summary of hindsight information matching (HIM) algorithms. The notation follows Section 4.
With HIM, all prior works can be categorized to four generic problem types based on IΦ(τ): (1) goal-based
φT (Andrychowicz et al., 2017), (2) multi-task arg max P"
HINDSIGHT INFORMATION MATCHING,0.058265582655826556,"t γtr(st, at, ·) (Li et al., 2020), (3) return-based
P"
HINDSIGHT INFORMATION MATCHING,0.05962059620596206,"t γtrt (Chen et al., 2021a), or (4) full trajectory imitation τ (Duan et al., 2017). Φ is the reward function
r(s, a) in (2) and (3), an indexing function for state dimensions (e.g. xy-velocities) or a learned function (Nair
et al., 2018) in (1), or an identify function in (4). Our CDT introduces a new category, (5) distribution-based
IΦ(τ) = histogram(rt, γ), based on a minimal modiﬁcation to DT, while our BDT can be considered as DT
adapted for (4), the trajectory imitation."
HINDSIGHT INFORMATION MATCHING,0.06097560975609756,"can be an identity function, the reward function r(s, a), sub-dimensions of s (e.g. xy-velocities),
or a generic parameterized function (e.g. auto-encoder). Generalizing reward-centric intuitions in
DT (Chen et al., 2021a), we deﬁne information matching (IM) problems as learning a conditional
policy π(a|s, z) whose trajectory rollouts satisfy some desired information statistics value z:"
HINDSIGHT INFORMATION MATCHING,0.06233062330623306,"min
π Ez∼p(z),τ∼ρπ
z (τ)

D(IΦ(τ), z)

(4)"
HINDSIGHT INFORMATION MATCHING,0.06368563685636856,"An important observation for the IM objective (Eq.4) is that for any given trajectory τ, setting
z∗= IΦ(τ) will minimize the inner term divergence D = 0 and therefore τ states and actions
are optimal with respect to z = z∗and samples of (τi, z∗
i ) can be used to accelerate RL or do
BC. We call these algorithms hindsight information matching (HIM) algorithms."
HINDSIGHT INFORMATION MATCHING,0.06504065040650407,"Table 1, which classiﬁes prior methods into effectively four categories based on IΦ(τ), leads us to
have the following insights around HIM algorithms:"
HINDSIGHT INFORMATION MATCHING,0.06639566395663957,"• New HIM algorithms can be proposed by simply changing IΦ(τ), as we did to propose
Categorical DT for (5) distribution-based.
• Given a choice of IΦ(τ), new HIM algorithms can be proposed by changing implementa-
tion details (Furuta et al., 2021a), such as using “RL"" or “BC"" as algorithm type, doing
“Online” or “Ofﬂine” training (Levine et al., 2020), and network architectures. All “Of-
ﬂine” “BC” methods could be adopted easily to “Online” learning through recursive data
collections (Ghosh et al., 2021; Kumar et al., 2019; Matsushima et al., 2021).
• Only (1) goal-based and (2) multi-task can use “RL” as algorithm type, while all four, plus
our (5) distribution-based, can use “BC”, because “RL” requires optimizing Eq. 4 with respect
to the policy, which gets non-trivial for some choices of IΦ(τ); e.g. (3-5) return-based, full
trajectory imitation, or distribution-based. “BC” bypasses the need to solve Eq. 4 and therefore
is universally applicable to any IΦ(τ) or HIM algorithm6."
GENERALIZED DECISION TRANSFORMER,0.06775067750677506,"5
GENERALIZED DECISION TRANSFORMER"
GENERALIZED DECISION TRANSFORMER,0.06910569105691057,"Following the insights in Section 4, we introduce Generalized Decision Transformer (GDT), which
generalizes DT (Chen et al., 2021a) based on different choices of IΦ(τ), as described in Figure 1 and
the last rows of Table 1. We chose DT as the base model since it is a simple model that uses “BC” as
the algorithm type and “Transformer” as the architecture. The choice of “BC” is a must, so we can
tractably train GDT with respect to any IΦ(τ) or HIM problem. The choice for the architecture is"
GENERALIZED DECISION TRANSFORMER,0.07046070460704607,"6Evolutionary strategies (Salimans et al., 2017), technically a non-RL black-box algorithm, could be applied,
but accurate estimations of D in Eq.4 could require prohibitively many samples."
GENERALIZED DECISION TRANSFORMER,0.07181571815718157,Published as a conference paper at ICLR 2022
GENERALIZED DECISION TRANSFORMER,0.07317073170731707,"more ﬂexible; however, we decided to use transformers (Vaswani et al., 2017) in this work due to their
enormous scaling successes in language and vision domains (Dosovitskiy et al., 2020; Brown et al.,
2020; Ramesh et al., 2021). See Algorithm 1 (in Appendix F) for the full pseudocode. While GDT
in Figure 1 can lead to different algorithms depending on different choices of the feature function
Φ(s, a) and the anti-causal aggregator (which together determine IΦ(τ)), in this work we focus our
empirical studies on the following two variants: Categorical DT (CDT) and Bi-directional DT (BDT)."
TASK DEFINITIONS AND METRICS,0.07452574525745258,"5.1
TASK DEFINITIONS AND METRICS"
TASK DEFINITIONS AND METRICS,0.07588075880758807,"Before proceeding to deﬁne CDT and BDT, we ﬁrst concretely deﬁne the tasks they are designed
to solve, namely: ofﬂine multi-task state-marginal matching (SMM), and ofﬂine multi-task
imitation learning (IL). Given the intrinsic connection or equivalence between distribution matching
and IL (Ghasemipour et al., 2020), these two separate terminologies may seem redundant. However,
inspired by the initial papers studying SMM problems (Lee et al., 2020; Ghasemipour et al., 2020)
which qualitatively evaluates distribution matching results in speciﬁed state dimensions (e.g. xy-
positions), we deﬁne the imitation task as ofﬂine multi-task SMM if speciﬁc Φ is given, and as ofﬂine
multi-task IL if Φ is an identity (i.e. φ = s) or learned (e.g. auto-encoder). We essentially view IL as
SMM evaluation on full state."
TASK DEFINITIONS AND METRICS,0.07723577235772358,"Given these deﬁnition, we also deﬁne a single metric for both ofﬂine multi-task SMM/IL: typical IL
assumes some availability of task reward or success evaluation, and indirectly measure the quality of
imitation through it (Ho & Ermon, 2016; Fu et al., 2018). Instead, again grounding on its connection
to distribution matching (Ghasemipour et al., 2020), we propose a Wasserstein loss between state-
marginal and target distributions as SMM-inspired metrics for evaluating ofﬂine multi-task SMM or
IL tasks. However, it is often intractable to measure such loss for full state or even for some state
dimensions analytically because both state-marginal and target distributions can be non-parametric
and we cannot access their densities. In practice, we empirically estimate it employing the binning of
the feature space we speciﬁed. More discussions are included in Appendix C."
CATEGORICAL DECISION TRANSFORMER FOR DISTRIBUTION MATCHING,0.07859078590785908,"5.2
CATEGORICAL DECISION TRANSFORMER FOR DISTRIBUTION MATCHING"
CATEGORICAL DECISION TRANSFORMER FOR DISTRIBUTION MATCHING,0.07994579945799458,"Inspired by the recent successes in distributional RL (Bellemare et al., 2017; Dabney et al., 2018;
2020), ofﬂine RL (Fujimoto et al., 2019; Jaques et al., 2020; Ghasemipour et al., 2021; Fujimoto & Gu,
2021) and state-marginal matching (Lee et al., 2020; Ghasemipour et al., 2020; Gu et al., 2021), we
introduce Categorical DT (CDT) for ofﬂine state-marginal matching (SMM) problem in Section 5.1.
Following the prior works (Bellemare et al., 2017; Furuta et al., 2021b), we assume low-dimensional
Φ, e.g. rewards or state dimensions like xyz-velocities, and employ the discretization of feature
spaces to form categorical approximations of continuous distributions. To compute z∗
t = IΦ(τt:T )
for all timesteps t given a trajectory τ1:T , we use similar recursive Bellman-like computation inspired
by Bellemare et al. (2017) (see Appendix F for the details). To the best of our knowledge, this is the
ﬁrst paper to study ofﬂine multi-task SMM and propose an effective algorithm for it."
CATEGORICAL DECISION TRANSFORMER FOR DISTRIBUTION MATCHING,0.08130081300813008,"5.3
DECISION TRANSFORMER WITH LEARNED Φ"
CATEGORICAL DECISION TRANSFORMER FOR DISTRIBUTION MATCHING,0.08265582655826559,"While CDT assumes some low-dimensional Φ is provided for tractable binning and distribution
approximation, we also study cases where Φ or r is not provided, and instead Φ is learned through
auto-encoding (Hinton & Salakhutdinov, 2006; Bengio et al., 2012) (DT-AE) or contrastive (van den
Oord et al., 2018; Srinivas et al., 2020; Yang & Nachum, 2021) (DT-CPC) losses for DT (see
Appendix G for the details). In this case, CDT is unnecessary because if Φ learns sufﬁcient features
of s, matching their means, i.e. moments, through DT is enough to match any distribution to an
arbitrary precision (Wainwright & Jordan, 2008; Li et al., 2015). Since Φ is differentiable with respect
to DT’s action-prediction losses, we also compare DT-E2E, where we learn Φ through end-to-end
differentiation. As Section 5.1 deﬁnes, these methods do ofﬂine multi-task SMM with full state, or
ofﬂine multi-task IL, a similar objective to state-marginal matching or adversarial inverse RL (Ho &
Ermon, 2016; Ghasemipour et al., 2020) in online RL. To the best of our knowledge, this is the ﬁrst
ofﬂine multi-task IL method that explicitly accounts for SMM through architectural bottlenecks."
BI-DIRECTIONAL DECISION TRANSFORMER FOR ONE-SHOT IMITATION LEARNING,0.08401084010840108,"5.4
BI-DIRECTIONAL DECISION TRANSFORMER FOR ONE-SHOT IMITATION LEARNING"
BI-DIRECTIONAL DECISION TRANSFORMER FOR ONE-SHOT IMITATION LEARNING,0.08536585365853659,Published as a conference paper at ICLR 2022
BI-DIRECTIONAL DECISION TRANSFORMER FOR ONE-SHOT IMITATION LEARNING,0.08672086720867209,"The absence of given Φ could be tackled with learning not only parameterized Φ as in Section 5.3,
but also a parameterized aggregator. Building on the connection to one-shot imitation learning (Duan
et al., 2017), we provide another natural extension of DT under GDT framework called Bi-directional
Decision Transformer (BDT), which assumes an identity Φ, and learns representation within the
aggregator, in this case a second (anti-causal) transformer (Radford et al., 2018) that takes a reverse-
order state sequence as an input. See Algorithm 1 in Appendix F for the pseudocode, and Appendix D
for further comments on the connection to one-shot or meta learning. While we found some positive
results for even simple unsupervised regularizer approaches (e.g. DT-AE), we observe BDT could
achieve substantially better ofﬂine multi-task IL results than DT-X variants in Section 5.3."
EXPERIMENTS,0.08807588075880758,"6
EXPERIMENTS"
EXPERIMENTS,0.08943089430894309,We empirically investigate the following questions:
EXPERIMENTS,0.09078590785907859,"• (SMM) Can CDT match unseen reward distributions?
• (SMM) Can CDT match and generalize to unseen 1D/2D state-feature distributions?
• (SMM) Can CDT match unseen synthesized state-feature distributions?
• (IL) Can BDT perform ofﬂine one-shot imitation learning in full state?"
EXPERIMENTS,0.0921409214092141,"We experiment on the OpenAI Gym, MuJoCo tasks (HalfCheetah, Hopper, Walker2d, Ant-v3), a
common benchmark for continuous control (Brockman et al., 2016; Todorov et al., 2012). Through
the experiments, we use medium-expert datasets in D4RL (Fu et al., 2020) to ensure the decent data
coverage. We sort all the trajectories by their cumulative rewards, hold out ﬁve best trajectories and
ﬁve 50 percentile trajectories as a test set (10 trajectories in total), and use the rest as a train set. We
report the results averaged over 20 rollouts every 4 random seed. We share our implementation to
ensure the reproducibility8."
EXPERIMENTS,0.09349593495934959,"As discussed in Section 5.1 and Appendix C, we evaluate CDT/BDT with approximate distribution
matching objective: Wasserstein-1 distance between categorical distributions of features in rollouts or
target trajectories. We compare CDT to DT, Meta-BC, and FOCAL (Li et al., 2021), a metric-based
ofﬂine meta RL method, as baselines. While Meta-BC and FOCAL does not solve the ofﬂine
distribution matching problem directly, they provide decent baseline performance since their ofﬂine
one-shot adaptation to the given target trajectories could deal with it (see Appendix B for the details)."
REWARD AND STATE-FEATURE MATCHING,0.0948509485094851,"6.1
REWARD AND STATE-FEATURE MATCHING"
REWARD AND STATE-FEATURE MATCHING,0.09620596205962059,"First, we evaluate whether CDT could match its rollout to the target distribution. We choose reward
and state-feature, such as x-velocity of the agents, as feature spaces to match. To specify the target
distributions during the evaluation, we feed the categorical representation of the target to CDT. As the
same as the reward case, DT takes the summation of the state-feature over a trajectory as an input."
REWARD AND STATE-FEATURE MATCHING,0.0975609756097561,"We quantitatively compare CDT against baselines (Table 2) in x-velocity case, where CDT shows
better matching results to the target distributions unseen during training. We provide the reward
distribution results and the visualization in Appendix E.1, where CDT performs very well in all
cases. To test the generalization additionally, we intervene the target distributions by (1) shifting
the target distributions in the test set by constant offsets, and (2) synthesizing novel distributions via
python scripts. See Appendix E.7 and E.8 for the results. Furthermore, to investigate the scalability
of CDT to multi-dimensional state-features, we experiment 2D state-feature distribution matching
(xy-velocities on Ant) in Appendix E.3, where CDT outperforms other baselines."
REWARD AND STATE-FEATURE MATCHING,0.0989159891598916,"Method
halfcheetah
hopper
walker2d
Average
Expert
Medium
Total
Expert
Medium
Total
Expert
Medium
Total
Categorical DT
0.633 ± 0.329
0.996 ± 1.467
0.814 ± 1.079
0.139 ± 0.043
0.059 ± 0.013
0.099 ± 0.051
0.122 ± 0.071
0.136 ± 0.045
0.129 ± 0.060
0.347
DT
0.746 ± 0.380
1.076 ± 1.549
0.911 ± 1.140
0.177 ± 0.053
0.093 ± 0.037
0.135 ± 0.063
0.083 ± 0.031
0.146 ± 0.084
0.115 ± 0.070
0.387
BC (no-context)
3.017 ± 0.891
3.468 ± 1.271
3.242 ± 1.121
0.652 ± 0.264
0.248 ± 0.199
0.450 ± 0.309
0.748 ± 0.529
0.858 ± 0.617
0.803 ± 0.577
1.498
Meta-BC
0.852 ± 0.688
0.840 ± 1.139
0.846 ± 0.941
0.799 ± 0.505
0.130 ± 0.056
0.464 ± 0.491
0.110 ± 0.082
1.462 ± 1.136
0.786 ± 1.052
0.699
FOCAL (Li et al., 2021)
1.643 ± 0.461
1.123 ± 1.550
1.383 ± 0.518
1.456 ± 0.473
0.484 ± 0.382
0.970 ± 0.649
1.571 ± 0.563
0.603 ± 0.427
1.087 ± 0.695
1.147
Table 2: Quantitative evaluation of state-feature distribution matching, measuring Wasserstein-1 distance between
the rollout and target distributions. We compare Categorical DT against DT, BC, Meta-BC, and FOCAL. CDT
achieves better matching than baselines. See Table 9 in Appendix E.1 for the reward distribution results."
REWARD AND STATE-FEATURE MATCHING,0.1002710027100271,8https://github.com/frt03/generalized_dt
REWARD AND STATE-FEATURE MATCHING,0.1016260162601626,Published as a conference paper at ICLR 2022
GENERALIZATION TO UNSEEN TARGET DISTRIBUTION,0.10298102981029811,"6.2
GENERALIZATION TO UNSEEN TARGET DISTRIBUTION"
GENERALIZATION TO UNSEEN TARGET DISTRIBUTION,0.1043360433604336,"The performance of ofﬂine methods in RL is often restricted by the coverage or quality of datasets.
While we demonstrate CDT can perform ofﬂine state-marginal matching to unseen target distributions
in Section 6.1, the standard ofﬂine datasets might not be diverse enough to observe the generalization
since those are collected by single-task reward-maximization policies. To test the generalization
to more diverse behaviors, we investigate the following tasks: (1) z-velocity distribution matching
with synthesized bi-modal behavior and (2) cheetah-velocity matching problem from meta RL/IL
literature (Rakelly et al., 2019; Li et al., 2021; Fakoor et al., 2020)"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.10569105691056911,"6.2.1
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.1070460704607046,"To generate diverse behaviors for z-axis, we obtain the expert cheetah that backﬂips towards -x
direction by modifying reward function (see Appendix E.4 for the details). Combining expert
backﬂipping trajectories and expert running forward trajectories from D4RL dataset, we construct a
novel dataset with diverse behaviors. We experiment the ofﬂine SMM with not only each uni-modal
behavior (backﬂipping or running forward), but also synthesized bi-modal behavior; running forward
ﬁrst, then backﬂipping during a single rollout, using patchworked target trajectories."
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.10840108401084012,"Table 3 and Figure 2 (a) show that CDT successfully matches the distribution to both uni-modal
(running forward or backﬂipping) and synthesized bi-modal distributions better than DT and FOCAL,
and is comparable to Meta-BC that originally designed to deal with such multi-task settings. Due to
the difﬁculty for RL algorithms to maximize Eq. 4, FOCAL struggles to solve the ofﬂine multi-task
SMM, even though FOCAL uses the same context embedding as Meta-BC. The bi-modal behavior
learned by CDT can be seen at https://sites.google.com/view/generalizeddt."
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.10975609756097561,"Method
Uni-modal
Bi-modal
Average
Categorical DT
1.562 ± 0.632
1.625 ± 0.902
1.594
DT
2.676 ± 0.765
2.703 ± 0.703
2.690
Meta-BC
1.519 ± 0.696
1.655 ± 0.990
1.587
FOCAL (Li et al., 2021)
2.203 ± 1.050
1.983 ± 0.948
2.093"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.1111111111111111,"Table 3: State-feature (z-velocity) distribution matching with uni-modal and (synthesized) bi-modal target
trajectories in HalfCheetah environment. Categorical DT matches both uni- and bi-modal trajectories better than
DT and FOCAL, and is comparable to Meta-BC that originally aims to solve multi-task problem."
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.11246612466124661,"−20
−10
0
10
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.11382113821138211,Probability
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.11517615176151762,uni-modal traj 0
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.11653116531165311,"−20
−10
0
10
0.00 0.05 0.10 0.15 0.20 0.25"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.11788617886178862,"0.30
uni-modal traj 1"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.11924119241192412,"−20
−10
0
10
0.00 0.05 0.10 0.15 0.20 0.25"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.12059620596205962,"0.30
bi-modal traj 0"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.12195121951219512,"−20
−10
0
10
0.00 0.05 0.10 0.15 0.20 0.25"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.12330623306233063,"0.30
bi-modal traj 1"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.12466124661246612,"−20
−10
0
10
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.12601626016260162,Probability
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.12737127371273713,uni-modal traj 0
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.12872628726287264,"−20
−10
0
10
0.00 0.05 0.10 0.15 0.20 0.25"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.13008130081300814,"0.30
uni-modal traj 1"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.13143631436314362,"−20
−10
0
10
0.00 0.05 0.10 0.15 0.20 0.25"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.13279132791327913,"0.30
bi-modal traj 0"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.13414634146341464,"−20
−10
0
10
0.00 0.05 0.10 0.15 0.20 0.25"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.13550135501355012,"0.30
bi-modal traj 1"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.13685636856368563,"Target
CDT
DT"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.13821138211382114,(a) Z-Velocity
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.13956639566395665,"−1
0
1
2
3
4
0.0 0.2 0.4 0.6 0.8"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.14092140921409213,Probability
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.14227642276422764,x-vel = 0.5
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.14363143631436315,"−1
0
1
2
3
4
0.0 0.2 0.4 0.6 0.8"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.14498644986449866,x-vel = 1.5
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.14634146341463414,"−1
0
1
2
3
4
0.0 0.2 0.4 0.6 0.8"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.14769647696476965,x-vel = 2.5
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.14905149051490515,"−1
0
1
2
3
4
0.0 0.2 0.4 0.6 0.8"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.15040650406504066,Probability
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.15176151761517614,x-vel = 0.5
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.15311653116531165,"−1
0
1
2
3
4
0.0 0.2 0.4 0.6 0.8"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.15447154471544716,x-vel = 1.5
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.15582655826558264,"−1
0
1
2
3
4
0.0 0.2 0.4 0.6 0.8"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.15718157181571815,x-vel = 2.5
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.15853658536585366,"Target
CDT
DT"
SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION,0.15989159891598917,"(b) Unseen Cheetah-Velocity
Figure 2: (a) Z-Velocity and (b) Unseen Cheetah-Velocity results. Blue histograms represent target distributions.
In (a), CDT (red) can match not only uni-modal behaviors for both running forward and backﬂipping, but
also bi-modal behaviors; during a single rollout running forward ﬁrst, then backﬂipping. DT (yellow) tends to
lean backﬂipping and fails to ﬁt neither uni-modal nor bi-modal ones. In (b), CDT successfully handles the
trajectories unseen during training, while DT seems to output covering behaviors over the dataset support."
DIVERSE UNSEEN DISTRIBUTION FROM META LEARNING TASK,0.16124661246612465,"6.2.2
DIVERSE UNSEEN DISTRIBUTION FROM META LEARNING TASK"
DIVERSE UNSEEN DISTRIBUTION FROM META LEARNING TASK,0.16260162601626016,"Generalization to unknown target demonstrations or tasks has been actively investigated in meta or
one-shot RL/IL literature (Duan et al., 2016; Wang et al., 2016). To verify the generalization of CDT
to diverse behaviors, we adopt the cheetah-velocity task; a popular task in meta RL/IL (Rakelly et al.,
2019; Li et al., 2021; Fakoor et al., 2020), where the cheetah tries to run with the speciﬁed velocity.
We prepare 31 target x-velocities; taken from [0.0, 3.0], uniformly spaced at 0.1 intervals, and hold
out {0.5, 1.5, 2.5} as a test set. See Appendix E.5 for the dataset generation."
DIVERSE UNSEEN DISTRIBUTION FROM META LEARNING TASK,0.16395663956639567,Published as a conference paper at ICLR 2022
DIVERSE UNSEEN DISTRIBUTION FROM META LEARNING TASK,0.16531165311653118,"Table 4 and Figure 2 (b) reveal that CDT outperforms DT or FOCAL, and is slightly better than Meta-
BC through the distribution matching evaluation, which implies CDT could solve ofﬂine multi-task
SMM generalizing to the unknown target trajectories, given sufﬁciently diverse ofﬂine datasets."
DIVERSE UNSEEN DISTRIBUTION FROM META LEARNING TASK,0.16666666666666666,"Method
x-vel: 0.5
x-vel: 1.5
x-vel: 2.5
Average
Categorical DT
0.060 ± 0.026
0.211 ± 0.022
0.149 ± 0.110
0.140
DT
1.197 ± 0.227
0.533 ± 0.105
0.861 ± 0.247
0.864
Meta-BC
0.150 ± 0.069
0.152 ± 0.127
0.167 ± 0.055
0.156
FOCAL (Li et al., 2021)
0.472 ± 0.005
0.952 ± 0.073
0.346 ± 0.186
0.590"
DIVERSE UNSEEN DISTRIBUTION FROM META LEARNING TASK,0.16802168021680217,"Table 4: Generalization to unseen target velocity (x-velocity = 0.5, 1.5, 2.5) among training dataset (x-velocity ∈
[0.0, 3.0]\{0.5, 1.5, 2.5}; uniformly spaced at 0.1 intervals), which is a popular setting in meta RL/IL. CDT
successfully deals with unseen target velocities well, outperforming DT and FOCAL, and is slightly better than
Meta-BC through the ofﬂine multi-task SMM evaluation."
ONE-SHOT DISTRIBUTION MATCHING IN FULL STATE,0.16937669376693767,"6.3
ONE-SHOT DISTRIBUTION MATCHING IN FULL STATE"
ONE-SHOT DISTRIBUTION MATCHING IN FULL STATE,0.17073170731707318,"Lastly, we investigate BDT for ofﬂine multi-task IL, where we do not observe rewards nor state
features explicitly. Instead, target full state trajectories that the agents are expected to mimic is
given. We compare BDT against parameterized Φ variants; DT-AE, -CPC, and -E2E discussed in
Section 5.3. We consider three strategies to train the encoder for DT-AE and -CPC: training with only
unsupervised loss, training with unsupervised and DT’s supervised loss jointly (called as “joint”),
and pre-training with only unsupervised loss and freezing the weights during DT training (called as
“frozen”). We aggregate the learned Φ by summation within the input sequence (length is 20). We
evaluate BDT and DT-X with the same distribution matching evaluation as Section 6.1."
ONE-SHOT DISTRIBUTION MATCHING IN FULL STATE,0.17208672086720866,"We sweep m-dim learned feature from the encoder Φ(s) with m ∈{1, 4, 16}. Table 5 presents the
ofﬂine multi-task IL results with respect to x-velocity distribution, using m = 16 embeddings (see
Appendix E.6 for the full results including m = 1, 4, and the reward distribution case). Similar to the
discussion in Yang & Nachum (2021), DT-CPC sometimes fails to obtain sufﬁcient representation
for imitation. While even simple approaches, DT-AE and DT-AE (frozen), show positive results
compared to no-context BC baselines (in Table 2), BDT outperforms all other learned Φ variants
or Meta-BC and is comparable to CDT or DT (also in Table 2) with longer input (N = 50). This
implies that even though we don’t assume the state-feature speciﬁcation, aggregator choice in GDT
with minimal architectural changes may solve the ofﬂine distribution matching problem efﬁciently.
We leave more sophisticated objectives or architectural changes as future work."
ONE-SHOT DISTRIBUTION MATCHING IN FULL STATE,0.17344173441734417,"Method
halfcheetah
hopper
walker2d
Average
Expert
Medium
Total
Expert
Medium
Total
Expert
Medium
Total
DT-AE
2.060 ± 1.076
1.089 ± 0.827
1.574 ± 1.075
0.611 ± 0.060
0.142 ± 0.049
0.377 ± 0.241
0.833 ± 0.346
0.321 ± 0.126
0.577 ± 0.365
0.843
DT-CPC
6.011 ± 0.324
1.403 ± 1.384
3.707 ± 2.514
0.610 ± 0.019
0.101 ± 0.024
0.355 ± 0.256
1.281 ± 0.022
0.138 ± 0.040
0.710 ± 0.572
1.591
DT-AE (joint)
8.643 ± 0.679
2.260 ± 0.690
5.451 ± 3.264
1.255 ± 0.363
0.649 ± 0.241
0.952 ± 0.432
2.104 ± 0.620
0.988 ± 0.413
1.546 ± 0.767
2.650
DT-CPC (joint)
4.544 ± 1.184
1.884 ± 1.465
3.214 ± 1.882
0.575 ± 0.032
0.096 ± 0.028
0.335 ± 0.241
0.949 ± 0.484
0.412 ± 0.378
0.680 ± 0.510
1.410
DT-E2E
8.208 ± 1.087
1.928 ± 0.838
5.068 ± 3.287
1.097 ± 0.217
0.542 ± 0.187
0.820 ± 0.344
2.086 ± 0.437
1.239 ± 0.712
1.662 ± 0.727
2.517
DT-AE (frozen)
1.821 ± 0.582
1.319 ± 0.863
1.570 ± 0.778
0.634 ± 0.038
0.137 ± 0.062
0.385 ± 0.253
1.180 ± 0.328
0.404 ± 0.170
0.792 ± 0.468
0.916
DT-CPC (frozen)
3.489 ± 1.159
2.543 ± 0.903
3.016 ± 1.141
0.631 ± 0.091
0.171 ± 0.130
0.401 ± 0.256
1.315 ± 0.329
0.279 ± 0.127
0.797 ± 0.575
1.405
BDT (N=20)
1.592 ± 0.201
1.208 ± 1.854
1.400 ± 1.333
0.318 ± 0.093
0.081 ± 0.013
0.200 ± 0.136
0.196 ± 0.031
0.392 ± 0.184
0.294 ± 0.164
0.631
BDT (N=50)
0.840 ± 0.063
1.223 ± 1.828
1.031 ± 1.307
0.142 ± 0.010
0.098 ± 0.025
0.120 ± 0.029
0.192 ± 0.051
0.163 ± 0.027
0.178 ± 0.043
0.443
Table 5: The results of BDT on the state-feature distribution matching problem (m = 16). While even simple
auto-encoder regularizers sometimes work well, BDT with longer contexts seems to outperform other strategies
and is comparable to CDT or DT (in Table 2). See Appendix E.6 for the full results including m = 1, 4."
CONCLUSION,0.17479674796747968,"7
CONCLUSION"
CONCLUSION,0.17615176151761516,"We provide a uniﬁed perspective on a wide range of hindsight algorithms, and generalize the problem
formulation as hindsight information matching (HIM). Inspired by recent successes in RL as sequence
modeling, we propose Generalized Decision Transformer (GDT) which includes DT, Categorical
DT, and Bi-directional DT as special cases, and is applicable to any HIM with proper choices
of Φ and aggregator. We show how Categorical DT, a minor modiﬁcation of DT, enables the ﬁrst
effective ofﬂine state-marginal matching algorithm and propose new benchmark tasks for this problem
class. We also demonstrate the effectiveness of Bi-directional DT as a one-shot imitation learner,
signiﬁcantly outperforming simple variants based on DT. We hope our proposed HIM and GDT
frameworks shed new perspectives on hindsight algorithms and the applicability of sequence modeling
to much broader classes of RL problems beyond classic reward-based RL."
CONCLUSION,0.17750677506775067,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.17886178861788618,ETHICS STATEMENT
ETHICS STATEMENT,0.1802168021680217,"Since this paper mainly focuses on the reinterpretation of hindsight RL algorithms and experiments
on existing benchmark datasets, we believe there are no ethical concerns."
REPRODUCIBILITY STATEMENT,0.18157181571815717,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.18292682926829268,"We share our codes to ensure the reproductivity. The details of hyperparameters are described in
Appendix A and B. The detailed settings of experiments are described in the Section 6, Appendix E,
and G. We report the results averaged over 20 rollouts every 4 random seed."
REFERENCES,0.1842818428184282,REFERENCES
REFERENCES,0.1856368563685637,"Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
International Conference on Machine learning, 2004."
REFERENCES,0.18699186991869918,"Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In
Advances in neural information processing systems, 2017."
REFERENCES,0.18834688346883469,"Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. arXiv preprint arXiv:1804.08617, 2018."
REFERENCES,0.1897018970189702,"Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, 2017."
REFERENCES,0.1910569105691057,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. arXiv preprint arXiv:1206.5538, 2012."
REFERENCES,0.19241192411924118,"Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,
Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,
Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano
Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter
Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil
Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar
Khattab, Pang Wei Kohd, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal
Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu
Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa,
Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles,
Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung
Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu
Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh,
Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori,
Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai
Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi
Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the
opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021."
REFERENCES,0.1937669376693767,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.1951219512195122,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165, 2020."
REFERENCES,0.19647696476964768,Published as a conference paper at ICLR 2022
REFERENCES,0.1978319783197832,"Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-efﬁcient
reinforcement learning with stochastic ensemble value expansion. arXiv preprint arXiv:1807.01675,
2018."
REFERENCES,0.1991869918699187,"Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Belle-
mare.
Dopamine: A research framework for deep reinforcement learning.
arXiv preprint
arXiv:1812.06110, 2018."
REFERENCES,0.2005420054200542,"Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex
Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, and Sergey Levine. Actionable models:
Unsupervised ofﬂine reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749,
2021."
REFERENCES,0.2018970189701897,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel,
Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence
modeling. arXiv preprint arXiv:2106.01345, 2021a."
REFERENCES,0.2032520325203252,"Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,
Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios
Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,
Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob
McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating
large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b."
REFERENCES,0.2046070460704607,"Jongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, and Shixiang Shane Gu. Variational
empowerment as representation learning for goal-based reinforcement learning. In International
Conference on Machine Learning, 2021."
REFERENCES,0.20596205962059622,"Will Dabney, Mark Rowland, Marc G Bellemare, and Rémi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018."
REFERENCES,0.2073170731707317,"Will Dabney, Zeb Kurth-Nelson, Naoshige Uchida, Clara Kwon Starkweather, Demis Hassabis, Rémi
Munos, and Matthew Botvinick. A distributional code for value in dopamine-based reinforcement
learning. Nature, 577(7792):671–675, 2020."
REFERENCES,0.2086720867208672,"Sudeep Dasari and Abhinav Gupta. Transformers for one-shot visual imitation. In Conference on
Robot Learning, 2020."
REFERENCES,0.21002710027100271,"Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity
of the linear quadratic regulator. Foundations of Computational Mathematics, 20(4):633–679,
2020."
REFERENCES,0.21138211382113822,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2019."
REFERENCES,0.2127371273712737,"Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Ofﬂine meta learning of exploration. arXiv preprint
arXiv:2008.02598, 2021."
REFERENCES,0.2140921409214092,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.21544715447154472,"Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016."
REFERENCES,0.21680216802168023,Published as a conference paper at ICLR 2022
REFERENCES,0.2181571815718157,"Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever,
Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in Neural
Information Processing Systems, 2017."
REFERENCES,0.21951219512195122,"Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning diverse skills without a reward function. In International Conference on Learning
Representations, 2019."
REFERENCES,0.22086720867208673,"Benjamin Eysenbach, Xinyang Geng, Sergey Levine, and Ruslan Salakhutdinov. Rewriting history
with inverse rl: Hindsight inference for policy improvement. In Advances in neural information
processing systems, 2020."
REFERENCES,0.2222222222222222,"Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve
goals via recursive classiﬁcation. In International Conference on Learning Representations, 2021."
REFERENCES,0.22357723577235772,"Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J. Smola. Meta-q-learning. In
International Conference on Learning Representations, 2020."
REFERENCES,0.22493224932249323,"Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative
adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint
arXiv:1611.03852, 2016."
REFERENCES,0.22628726287262874,"Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation
learning via meta-learning. In Conference on Robot Learning, 2017."
REFERENCES,0.22764227642276422,"Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforce-
ment learning. In International Conference on Learning Representations, 2018."
REFERENCES,0.22899728997289973,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.23035230352303523,"Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to ofﬂine reinforcement learning.
arXiv preprint arXiv:2106.06860, 2021."
REFERENCES,0.23170731707317074,"Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In International Conference on Machine Learning, 2018."
REFERENCES,0.23306233062330622,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, 2019."
REFERENCES,0.23441734417344173,"Hiroki Furuta, Tadashi Kozuno, Tatsuya Matsushima, Yutaka Matsuo, and Shixiang Shane Gu. Co-
adaptation of algorithmic and implementational innovations in inference-based deep reinforcement
learning. In Advances in neural information processing systems, 2021a."
REFERENCES,0.23577235772357724,"Hiroki Furuta, Tatsuya Matsushima, Tadashi Kozuno, Yutaka Matsuo, Sergey Levine, Oﬁr Nachum,
and Shixiang Shane Gu. Policy information capacity: Information-theoretic measure for task
complexity in deep reinforcement learning. In International Conference on Machine Learning,
2021b."
REFERENCES,0.23712737127371275,"Seyed Kamyar Seyed Ghasemipour, Shixiang Shane Gu, and Richard Zemel. Smile: Scalable
meta inverse reinforcement learning through context-conditional policies. In Advances in Neural
Information Processing Systems, 2019."
REFERENCES,0.23848238482384823,"Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization
perspective on imitation learning methods. In Conference on Robot Learning, 2020."
REFERENCES,0.23983739837398374,"Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max
q-learning operator for simple yet effective ofﬂine and online rl. In International Conference on
Machine Learning, 2021."
REFERENCES,0.24119241192411925,"Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Manon Devin, Benjamin Eysenbach,
and Sergey Levine. Learning to reach goals via iterated supervised learning. In International
Conference on Learning Representations, 2021."
REFERENCES,0.24254742547425473,Published as a conference paper at ICLR 2022
REFERENCES,0.24390243902439024,"Adam Gleave and Oliver Habryka. Multi-task maximum entropy inverse reinforcement learning.
arXiv preprint arXiv:1805.08882, 2018."
REFERENCES,0.24525745257452575,"Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with
model-based acceleration. In International Conference on Machine Learning, 2016."
REFERENCES,0.24661246612466126,"Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, Bernhard Schölkopf, and
Sergey Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation
for deep reinforcement learning. arXiv preprint arXiv:1706.00387, 2017."
REFERENCES,0.24796747967479674,"Shixiang Shane Gu, Manfred Diaz, Daniel C. Freeman, Hiroki Furuta, Seyed Kamyar Seyed
Ghasemipour, Anton Raichuk, Byron David, Erik Frey, Erwin Coumans, and Olivier Bachem. Brax-
lines: Fast and interactive toolkit for rl-driven behavior engineering beyond reward maximization.
arXiv preprint arXiv:2110.04686, 2021."
REFERENCES,0.24932249322493225,"Jiaming Guo, Rui Zhang, Xishan Zhang, Shaohui Peng, Qi Yi, Zidong Du, Xing Hu, Qi Guo, and
Yunji Chen. Hindsight value function for variance reduction in stochastic dynamic environment.
arXiv preprint arXiv:2107.12216, 2021."
REFERENCES,0.2506775067750677,"Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised meta-learning
for reinforcement learning. arXiv preprint arXiv:1806.04640, 2018."
REFERENCES,0.25203252032520324,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning, 2018."
REFERENCES,0.25338753387533874,"Elad Hazan, Sham M. Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum
entropy exploration. In International Conference on Machine Learning, 2019."
REFERENCES,0.25474254742547425,"Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and Tom Erez. Learning
continuous control policies by stochastic value gradients. arXiv preprint arXiv:1510.09142, 2015."
REFERENCES,0.25609756097560976,"G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504–507, 2006."
REFERENCES,0.25745257452574527,"Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
information processing systems, 2016."
REFERENCES,0.2588075880758808,"Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling
problem. arXiv preprint arXiv:2106.02039, 2021."
REFERENCES,0.2601626016260163,"Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza,
Noah Jones, Shixiang Shane Gu, and Rosalind Picard. Human-centric dialog training via ofﬂine
reinforcement learning. arXiv preprint arXiv:2010.05848, 2020."
REFERENCES,0.26151761517615174,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low bellman rank are pac-learnable. In International Conference on
Machine Learning, 2017."
REFERENCES,0.26287262872628725,"Leslie Pack Kaelbling. Learning to achieve goals. In International Joint Conference on Artiﬁcial
Intelligence, 1993."
REFERENCES,0.26422764227642276,"Patrick T. Komiske, Eric M. Metodiev, and Jesse Thaler. The Hidden Geometry of Particle Collisions.
JHEP, 07:006, 2020. doi: 10.1007/JHEP07(2020)006."
REFERENCES,0.26558265582655827,"Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson.
Discriminator-actor-critic: Addressing sample inefﬁciency and reward bias in adversarial imitation
learning. In International Conference on Learning Representations, 2019."
REFERENCES,0.2669376693766938,"Ilya Kostrikov, Oﬁr Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution
matching. In International Conference on Learning Representations, 2020."
REFERENCES,0.2682926829268293,"Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint
arXiv:1912.13465, 2019."
REFERENCES,0.2696476964769648,Published as a conference paper at ICLR 2022
REFERENCES,0.27100271002710025,"Vitaly Kurin, Maximilian Igl, Tim Rocktäschel, Wendelin Boehmer, and Shimon Whiteson. My body
is a cage: the role of morphology in graph-based incompatible control. In International Conference
on Learning Representations, 2021."
REFERENCES,0.27235772357723576,"Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov.
Efﬁcient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2020."
REFERENCES,0.27371273712737126,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.2750677506775068,"Alexander C. Li, Lerrel Pinto, and Pieter Abbeel. Generalized hindsight for reinforcement learning.
In Advances in neural information processing systems, 2020."
REFERENCES,0.2764227642276423,"Lanqing Li, Rui Yang, and Dijun Luo. Focal: Efﬁcient fully-ofﬂine meta-reinforcement learning via
distance metric learning and behavior regularization. In International Conference on Learning
Representations, 2021."
REFERENCES,0.2777777777777778,"Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International
Conference on Machine Learning, 2015."
REFERENCES,0.2791327913279133,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
International Conference on Learning Representations, 2016."
REFERENCES,0.2804878048780488,"Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and
Pierre Sermanet. Learning latent plans from play. In Conference on Robot Learning, 2019."
REFERENCES,0.28184281842818426,"Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oﬁr Nachum, and Shixiang Shane Gu.
Deployment-efﬁcient reinforcement learning via model-based ofﬂine optimization. In International
Conference on Learning Representations, 2021."
REFERENCES,0.28319783197831977,"Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Ofﬂine meta-
reinforcement learning with advantage weighting.
In International Conference on Machine
Learning, 2021."
REFERENCES,0.2845528455284553,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 2015."
REFERENCES,0.2859078590785908,"Oﬁr Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-Efﬁcient Hierarchical Reinforce-
ment Learning. In Advances in neural information processing systems, 2018."
REFERENCES,0.2872628726287263,"Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual
reinforcement learning with imagined goals. arXiv preprint arXiv:1807.04742, 2018."
REFERENCES,0.2886178861788618,"Soroush Nasiriany, Vitchyr H. Pong, Ashvin Nair, Alexander Khazatsky, Glen Berseth, and Sergey
Levine. Disco rl: Distribution-conditioned reinforcement learning for general-purpose policies. In
International Conference on Robotics and Automation, 2021."
REFERENCES,0.2899728997289973,"Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar,
Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers
for reinforcement learning. In International Conference on Machine Learning, 2020."
REFERENCES,0.29132791327913277,"Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asym-
metric actor critic for image-based robot learning. arXiv preprint arXiv:1710.06542, 2017."
REFERENCES,0.2926829268292683,"Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-
free deep rl for model-based control. International Conference on Learning Representations,
2018."
REFERENCES,0.2940379403794038,"Vitchyr H. Pong, Ashvin Nair, Laura Smith, Catherine Huang, and Sergey Levine. Ofﬂine meta-
reinforcement learning with online self-supervision. arXiv preprint arXiv:2107.03974, 2021."
REFERENCES,0.2953929539295393,"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training, 2018."
REFERENCES,0.2967479674796748,Published as a conference paper at ICLR 2022
REFERENCES,0.2981029810298103,"Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners, 2019."
REFERENCES,0.2994579945799458,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint
arXiv:2103.00020, 2021."
REFERENCES,0.3008130081300813,"Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efﬁcient off-policy
meta-reinforcement learning via probabilistic context variables. In International Conference on
Machine Learning, 2019."
REFERENCES,0.3021680216802168,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021."
REFERENCES,0.3035230352303523,"Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng. Exploration via hindsight goal
generation. In Advances in neural information processing systems, 2019."
REFERENCES,0.3048780487804878,"Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017."
REFERENCES,0.3062330623306233,"Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. In International conference on learning representations, 2020."
REFERENCES,0.3075880758807588,"Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4rl: Surprisingly simple self-supervision for
ofﬂine reinforcement learning. arXiv preprint arXiv:2103.06326, 2021."
REFERENCES,0.3089430894308943,"Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representations
for reinforcement learning. In International Conference on Machine Learning, 2020."
REFERENCES,0.31029810298102983,"Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja´skowski, and Jürgen Schmidhuber.
Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877,
2019."
REFERENCES,0.3116531165311653,"Yujin Tang and David Ha. The sensory neuron as a transformer: Permutation-invariant neural
networks for reinforcement learning. arXiv preprint arXiv:2109.02869, 2021."
REFERENCES,0.3130081300813008,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In International Conference on Intelligent Robots and Systems, 2012."
REFERENCES,0.3143631436314363,"Stephen Tu and Benjamin Recht. The gap between model-based and model-free methods on the
linear quadratic regulator: An asymptotic viewpoint. In Conference on Learning Theory, 2019."
REFERENCES,0.3157181571815718,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.3170731707317073,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, 2017."
REFERENCES,0.31842818428184283,"David Venuto, Elaine Lau, Doina Precup, and Oﬁr Nachum. Policy gradients incorporating the future.
arXiv preprint arXiv:2108.02096, 2021."
REFERENCES,0.31978319783197834,"Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and
variational inference. Now Publishers Inc, 2008."
REFERENCES,0.32113821138211385,"Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv
preprint arXiv:1611.05763, 2016."
REFERENCES,0.3224932249322493,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior Regularized Ofﬂine Reinforcement Learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.3238482384823848,"Kelvin Xu, Ellis Ratner, Anca Dragan, Sergey Levine, and Chelsea Finn. Learning a prior over intent
via meta-inverse reinforcement learning. In International Conference on Machine Learning, 2019."
REFERENCES,0.3252032520325203,Published as a conference paper at ICLR 2022
REFERENCES,0.3265582655826558,"Mengjiao Yang and Oﬁr Nachum. Representation matters: Ofﬂine pretraining for sequential decision
making. In International Conference on Machine Learning, 2021."
REFERENCES,0.32791327913279134,"Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-inverse reinforcement learning with
probabilistic context variables. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.32926829268292684,"Rui Zhao and Volker Tresp. Energy-based hindsight experience prioritization. In Conference on
Robot Learning, 2018."
REFERENCES,0.33062330623306235,"Tony Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess, Jon Scholz, Stefan
Schaal, and Sergey Levine. Ofﬂine meta-reinforcement learning for industrial insertion. arXiv
preprint arXiv:2110.04276, 2021."
REFERENCES,0.3319783197831978,"Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse
reinforcement learning. In Twenty-Third AAAI Conference on Artiﬁcial Intelligence, 2008."
REFERENCES,0.3333333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.3346883468834688,APPENDIX
REFERENCES,0.33604336043360433,"A
HYPER-PARAMETER OF GENERALIZED DECISION TRANSFORMERS"
REFERENCES,0.33739837398373984,"We implement Categorical Decision Transformer and Bi-directional Decision Transformer, built
upon the ofﬁcial codebase released by Chen et al. (2021a) (https://github.com/kzl/
decision-transformer). We follow the most of hyperparameters as they did (Table 6)."
REFERENCES,0.33875338753387535,"Hyperparameter
Value
Number of layers
3
Number of attention heads
1
Embedding dimension
128
Nonlinearity function
ReLU
Batch size
64
Context length N
20
Dropout
0.1
Learning rate
1e-4
Grad norm clip
0.25
Weight decay
1e-4
Learning rate decay
Linear warmup for ﬁrst 100k training steps
Training (Gradient) steps
1M
Number of bins for categorical distribution
31
Encoder size (for DT-X)
2 layer MLP, (128, 128)
Coefﬁcient of unsupervised loss
0.1"
REFERENCES,0.34010840108401086,"Table 6: List of hyperparameters for DT, CDT and BDT. We refer Chen et al. (2021a)."
REFERENCES,0.34146341463414637,"B
DETAILS OF BASELINES"
REFERENCES,0.3428184281842818,"While, to the best of our knowledge, there are no prior work to tackle the ofﬂine state-marginal
matching problem, we can regard meta or one-shot imitation learning as the methods solving similar
problem (see Appendix D for further discussion). In this work, we choose Meta-BC and FOCAL (Li
et al., 2021), a metric-based ofﬂine meta RL method, as decent baselines."
REFERENCES,0.34417344173441733,"FOCAL
FOCAL is a ofﬂine meta RL method combining BRAC (Wu et al., 2019) with metric-
based approach. It utilizes deterministic context encoder trained with inverse-power distance metric
losses and detached from Bellman backup gradients. We follow the hyperparameters in the ofﬁcial
implementation (https://github.com/LanqingLi1993/FOCAL-ICLR). Deterministic
context encoder takes single-step state-action-reward tuple as a context for task inference. We
parameterize it with (200, 200, 200)-layers MLP. We train deterministic context encoder with 100000
iteration ﬁrst, and then train BRAC agent with task-conditioned policy and value functions with
100000 iteration. We use (256, 256, 256)-layers MLPs for policy and value network, and set batch
size to 256."
REFERENCES,0.34552845528455284,"Meta-BC
We also use metric-based Meta-BC (Duan et al., 2017; Dasari & Gupta, 2020) as a strong
baseline method for ofﬂine multi-task SMM. We adapt deterministic context encoder from FOCAL
to infer the task. We train Meta-BC in the same way as FOCAL just replacing BRAC to BC. The
objective is mean-squared error minimization."
REFERENCES,0.34688346883468835,"Throughout our experiments, Meta-BC shows better results than FOCAL, because RL algorithms
often struggle to optimize Eq. 4 for distribution matching and FOCAL tries to maximize the task
reward, which is not necessarily required for distribution matching problem. In addition, BC often
converges faster than ofﬂine RL methods."
REFERENCES,0.34823848238482386,Published as a conference paper at ICLR 2022
REFERENCES,0.34959349593495936,"C
DETAILS OF EVALUATION"
REFERENCES,0.3509485094850949,"Throughout the paper, we evaluate both CDT and BDT from a distribution matching perspective
by formulating them as ofﬂine multi-task SMM or ofﬂine multi-task IL problem. However, the
current distribution matching research in RL (Lee et al., 2020; Ghasemipour et al., 2020; Gu et al.,
2021) has been suffered from the lack of quantitative metrics for evaluation (while standard RL
or BC are typically evaluated on the task reward performance). As summarized in Table 7, prior
works (Lee et al., 2020; Ghasemipour et al., 2020) evaluate the SMM performance by qualitative
density visualization using rollout particles."
REFERENCES,0.3523035230352303,"Although the distance between state-marginal and target distribution seems the most intuitive and
suitable metric to quantify the performance of distribution matching algorithms, it is often intractable
to measure such distance analytically because both state-marginal and target distribution can be
non-parametric; we cannot access their densities, and only their samples are available. While a
concurrent work (Gu et al., 2021) tackles this problem by leveraging sample-based energy distance
estimation, we introduce a single SMM-inspired evaluation for both ofﬂine multi-task SMM and
ofﬂine multi-task IL via estimating Wasserstein-1 distance between empirical categorical distributions.
Since the discretization of low-dimensional features, such as reward, have success in many RL
methods (Bellemare et al., 2017; Dabney et al., 2018; 2020; Furuta et al., 2021b), quantiﬁcation
of distribution matching performance by Wasserstein-1 distance could be reliable evaluations (in
addition Wasserstein-1 distance is symmetric, different from KL distance that is asymmetric). We note
that due to the equivalence between inverse RL and SMM methods, the performance of distribution
matching methods may be measured via task reward when assuming the accessivity to the expert
trajectories (Ho & Ermon, 2016; Fu et al., 2018; Kostrikov et al., 2019). However, in our experiments,
it is not suitable to evaluate the performance based on task reward since we uses the sub-optimal
trajectories (Section 6.1) or multi-task trajectories from different reward functions (Section 6.2)."
REFERENCES,0.35365853658536583,"Wasserstein-1 distance between state-marginal distribution ρπ(s) and target distribution p∗(s) is
deﬁned as:
W1(ρπ(s), p∗(s)) =
inf
ν∈Γ(ρπ,p∗)E(X,Y )∼ν[|X −Y |],
(5)"
REFERENCES,0.35501355013550134,"where Γ is the set of all possible joint distributions ν whose marginals are ρπ and p∗respectively
(X and Y are random variables). In this work, we empirically estimate Eq. 5 by focusing on the
“manually-speciﬁed” feature φ ∈F (e.g. reward, xyz-velocities, etc; deﬁned in Section 4) and
employing binning and discretization. We discretize the feature space F and obtain B bins (per
dimension), whose representatives are {¯φ1, ¯φ2, . . . ¯φB}. The range of feature space [φmin, φmax] is
pre-deﬁned from the given ofﬂine datasets. Then, we get categorical distribution from the target
trajectory τ; ˆp∗
τ(φ) = {(¯φ1, c∗
τ1), . . . (¯φB, c∗
τB)}, and from the rollouts of the policy π (using 4 seeds
× 20 rollouts); ˆρπ(φ) = {(¯φ1, cπ
1), . . . (¯φB, cπ
B)}, where c∗
τ and cπ are the weights of each bin (i.e.
PB
l=1 c∗
τl = 1 and PB
l=1 cπ
l = 1). We compute the evaluation metric averaging on the test set of the
trajectories Dtest:"
REFERENCES,0.35636856368563685,"Metric(π, Dtest) =
1
|Dtest| X"
REFERENCES,0.35772357723577236,"τ∈Dtest
min
wij B
X i=1 B
X"
REFERENCES,0.35907859078590787,"j=1
wij|c∗
τi −cπ
j |,"
REFERENCES,0.3604336043360434,"s.t. wij ≥0, B
X"
REFERENCES,0.3617886178861789,"j=1
wij ≤c∗
τi, B
X"
REFERENCES,0.36314363143631434,"i=1
wij ≤cπ
j , B
X i=1 B
X"
REFERENCES,0.36449864498644985,"j=1
wij = 1. (6)"
REFERENCES,0.36585365853658536,"In
practice,
we
utilize
the
python
package
(https://github.com/pkomiske/
Wasserstein) released by Komiske et al. (2020) to compute it."
REFERENCES,0.36720867208672087,"Evaluation
Type
Reference
Density Visualization
Qualitative
Lee et al. (2020); Ghasemipour et al. (2020)
Energy Distance
Quantitative
Gu et al. (2021)
Task Reward
Quantitative
Ho & Ermon (2016); Fu et al. (2018), etc.
Wasserstein-1 Distance
Quantitative
Ours"
REFERENCES,0.3685636856368564,Table 7: A Review of evaluation for distribution matching algorithms.
REFERENCES,0.3699186991869919,Published as a conference paper at ICLR 2022
REFERENCES,0.3712737127371274,"D
CONNECTION TO META LEARNING"
REFERENCES,0.37262872628726285,"While we solve ofﬂine information matching problems in this paper, our formulations (espe-
cially Bi-directional DT) and experimental settings are similar to ofﬂine meta RL (Dorfman
et al., 2021; Mitchell et al., 2021; Li et al., 2021) and meta/one-shot imitation learning (Yu et al.,
2019; Ghasemipour et al., 2019; Finn et al., 2017; Duan et al., 2017) (especially metric-based ap-
proach (Rakelly et al., 2019; Duan et al., 2016; Fakoor et al., 2020)). We brieﬂy clarify the relevance
and difference between them (Table 8)."
REFERENCES,0.37398373983739835,"Ofﬂine Meta RL Recently, some works deal with ofﬂine meta RL problem, assuming task distri-
bution and ofﬂine training with pre-stored transitions collected by the (sub-) optimal or scripted
task-conditioned agents (Dorfman et al., 2021; Mitchell et al., 2021; Li et al., 2021; Pong et al., 2021;
Zhao et al., 2021). In these works, few test-task trajectories, following the same task distribution but
unseen during training phase, are given at test-time (i.e. few-shot), and the agents adapt the given
task in an online (Pong et al., 2021; Zhao et al., 2021; Dorfman et al., 2021) or fully-ofﬂine (Mitchell
et al., 2021; Li et al., 2021) manner."
REFERENCES,0.37533875338753386,"Meta Imitation Learning Another related domain is meta imitation learning, also assuming task dis-
tribution and ofﬂine training with pre-stored expert transitions per tasks (Yu et al., 2019; Ghasemipour
et al., 2019; Finn et al., 2017; Duan et al., 2017; Xu et al., 2019; Gleave & Habryka, 2018; Dasari &
Gupta, 2020). While meta inverse RL methods (Yu et al., 2019; Xu et al., 2019; Gleave & Habryka,
2018) require online samples during test-time adaptation, its off-policy extension and BC-based
approach performs ofﬂine adaptation with given unseen trajectories (Ghasemipour et al., 2019; Finn
et al., 2017; Dasari & Gupta, 2020)."
REFERENCES,0.37669376693766937,"Our work, in contrast, assume no-adaptation at test time and evaluation criteria is a distance between
the feature distributions, not the task rewards."
REFERENCES,0.3780487804878049,"Method
Problem
Train
Test
Demo
Pong et al. (2021)
Ofﬂine Meta RL
ofﬂine
online
few-shot
Zhao et al. (2021)
Ofﬂine Meta RL
ofﬂine
online
few-shot
Dorfman et al. (2021)
Ofﬂine Meta RL
ofﬂine
online
few-shot
Mitchell et al. (2021)
Ofﬂine Meta RL
ofﬂine
ofﬂine
few-shot
Li et al. (2021)
Ofﬂine Meta RL
ofﬂine
ofﬂine
few-shot
Yu et al. (2019)
Meta IL
online
online
few-shot
Xu et al. (2019)
Meta IL
online
online
few-shot
Ghasemipour et al. (2019)
Meta IL
online
ofﬂine
few-shot
Finn et al. (2017)
Meta IL
ofﬂine
ofﬂine
one-shot
Duan et al. (2017)
Meta IL
ofﬂine
(no-adaptation)
one-shot
Ours
Ofﬂine multi-task SMM/IL
ofﬂine
(no-adaptation)
one-shot"
REFERENCES,0.3794037940379404,"Table 8: Review of problem settings among ofﬂine meta RL, meta IL, and ours. In ofﬂine meta RL, the agents
are trained ofﬂine and tested with several demonstrations (few-shot), in a fully-ofﬂine manner or allowing online
data-collection for adaptation. In meta IL, IRL-based methods train the agents online (Yu et al., 2019; Xu et al.,
2019; Ghasemipour et al., 2019) while BC-based methods (Finn et al., 2017; Duan et al., 2017) do ofﬂine.
Similar to our settings, Duan et al. (2017) test the agents with single demonstration and no-adaptation process,
they evaluate the agent on the task reward, while we do on the distance between the two distributions."
REFERENCES,0.3807588075880759,Published as a conference paper at ICLR 2022
REFERENCES,0.3821138211382114,"E
DETAILS OF EXPERIMENTS"
REFERENCES,0.38346883468834686,"In this section, we provide the details and supplemental quantitative/qualitative results of the experi-
ments in Section 6."
REFERENCES,0.38482384823848237,"Figure 3 visualizes the dataset distributions we use in the experiments. When we sort all the
trajectories based on their cumulative rewards, each quality of trajectory (best, middle, worst) shows
a different shape of distribution, and the distribution of the whole dataset seems a weighted mixture
of those."
REFERENCES,0.3861788617886179,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.3875338753387534,Probbility
REFERENCES,0.3888888888888889,halfcheetah-medium-expert-v2
REFERENCES,0.3902439024390244,"1
2
3
4
5
6
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.3915989159891599,hopper-medium-expert-v2
REFERENCES,0.39295392953929537,"−2
0
2
4
6
8
Reward 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.3943089430894309,walker2d-medium-expert-v2
REFERENCES,0.3956639566395664,"Dataset
Best
Middle
Worst"
REFERENCES,0.3970189701897019,(a) Reward
REFERENCES,0.3983739837398374,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
x-Velocity 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.3997289972899729,Probbility
REFERENCES,0.4010840108401084,halfcheetah-medium-expert-v2
REFERENCES,0.4024390243902439,"0
1
2
3
4
5
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.4037940379403794,hopper-medium-expert-v2
REFERENCES,0.4051490514905149,"−4
−2
0
2
4
6
8
x-Velocity 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.4065040650406504,walker2d-medium-expert-v2
REFERENCES,0.4078590785907859,"Dataset
Best
Middle
Worst"
REFERENCES,0.4092140921409214,(b) X-Velocity
REFERENCES,0.4105691056910569,"Figure 3: Distributions of the features (reward, and x-velocity) in the D4RL medium-expert datasets."
REFERENCES,0.41192411924119243,"E.1
QUANTITATIVE AND QUALITATIVE RESULTS OF REWARD AND STATE-FEATURE
MATCHING"
REFERENCES,0.4132791327913279,"We provide the quantitative comaprison of Section 6.1 between Categorical DT and DT in the reward
(Table 9) matching problem, computing Wasserstein-1 distance between the discretized rollout and
target feature distributions. Generally, similar to the x-velocity case, CDT shows the better results in
ofﬂine multi-task SMM compared to original DT. We also visualize some of CDT results in Figure 4."
REFERENCES,0.4146341463414634,"Method
halfcheetah
hopper
walker2d
Average
Expert
Medium
Total
Expert
Medium
Total
Expert
Medium
Total
Categorical DT
0.674 ± 0.213
1.002 ± 1.458
0.838 ± 1.054
0.159 ± 0.085
0.064 ± 0.017
0.111 ± 0.077
0.095 ± 0.017
0.114 ± 0.037
0.105 ± 0.030
0.351
DT
0.652 ± 0.319
1.039 ± 1.548
0.846 ± 1.134
0.227 ± 0.119
0.091 ± 0.035
0.159 ± 0.111
0.056 ± 0.015
0.626 ± 0.495
0.341 ± 0.452
0.448
BC (no-context)
3.240 ± 0.559
2.880 ± 0.614
3.060 ± 0.614
0.597 ± 0.056
0.119 ± 0.067
0.358 ± 0.247
0.977 ± 0.501
0.431 ± 0.396
0.704 ± 0.528
1.374
Meta-BC
0.839 ± 0.682
0.830 ± 1.130
0.835 ± 0.933
0.803 ± 0.505
0.134 ± 0.056
0.468 ± 0.491
0.113 ± 0.085
1.441 ± 1.113
0.777 ± 1.032
0.693
FOCAL (Li et al., 2021)
1.623 ± 0.501
1.115 ± 1.534
1.369 ± 0.516
1.463 ± 0.472
0.492 ± 0.384
0.977 ± 0.649
1.584 ± 0.570
0.604 ± 0.421
1.094 ± 0.701
1.147"
REFERENCES,0.4159891598915989,"Table 9: Quantitative evaluation of reward distribution matching via measuring Wasserstein-1 distance between
the rollout and target distributions. We compare Categorical DT and DT. Since it can capture the multi-modal
nature of target distributions, CDT matches the distribution better than the original DT."
REFERENCES,0.4173441734417344,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.4186991869918699,Probability
REFERENCES,0.42005420054200543,halfcheetah best 1
REFERENCES,0.42140921409214094,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.42276422764227645,"0.35
halfcheetah best 2"
REFERENCES,0.4241192411924119,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.4254742547425474,halfcheetah middle 1
REFERENCES,0.4268292682926829,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.4281842818428184,halfcheetah middle 2
REFERENCES,0.42953929539295393,"1
2
3
4
5
6
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.43089430894308944,Probability
REFERENCES,0.43224932249322495,hopper best 1
REFERENCES,0.43360433604336046,"1
2
3
4
5
6
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.4349593495934959,hopper best 2
REFERENCES,0.4363143631436314,"1
2
3
4
5
6
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.43766937669376693,hopper middle 1
REFERENCES,0.43902439024390244,"1
2
3
4
5
6
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.44037940379403795,hopper middle 2
REFERENCES,0.44173441734417346,"−2
0
2
4
6
8
Reward 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.44308943089430897,Probability
REFERENCES,0.4444444444444444,walker2d best 1
REFERENCES,0.44579945799457993,"−2
0
2
4
6
8
Reward 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.44715447154471544,walker2d best 2
REFERENCES,0.44850948509485095,"−2
0
2
4
6
8
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.44986449864498645,walker2d middle 1
REFERENCES,0.45121951219512196,"−2
0
2
4
6
8
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.45257452574525747,walker2d middle 2
REFERENCES,0.453929539295393,"Target
Rollout"
REFERENCES,0.45528455284552843,(a) Reward
REFERENCES,0.45663956639566394,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.45799457994579945,Probability
REFERENCES,0.45934959349593496,halfcheetah best 1
REFERENCES,0.46070460704607047,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.462059620596206,halfcheetah best 2
REFERENCES,0.4634146341463415,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.46476964769647694,halfcheetah middle 1
REFERENCES,0.46612466124661245,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.46747967479674796,halfcheetah middle 2
REFERENCES,0.46883468834688347,"0
1
2
3
4
5
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.470189701897019,Probability
REFERENCES,0.4715447154471545,hopper best 1
REFERENCES,0.47289972899729,"0
1
2
3
4
5
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.4742547425474255,hopper best 2
REFERENCES,0.47560975609756095,"0
1
2
3
4
5
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.47696476964769646,hopper middle 1
REFERENCES,0.47831978319783197,"0
1
2
3
4
5
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.4796747967479675,hopper middle 2
REFERENCES,0.481029810298103,"−4
−2
0
2
4
6
8
x-Velocity 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.4823848238482385,Probability
REFERENCES,0.483739837398374,walker2d best 1
REFERENCES,0.48509485094850946,"−4
−2
0
2
4
6
8
x-Velocity 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.48644986449864497,walker2d best 2
REFERENCES,0.4878048780487805,"−4
−2
0
2
4
6
8
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.489159891598916,walker2d middle 1
REFERENCES,0.4905149051490515,"−4
−2
0
2
4
6
8
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.491869918699187,walker2d middle 2
REFERENCES,0.4932249322493225,"Target
Rollout"
REFERENCES,0.494579945799458,(b) X-Velocity
REFERENCES,0.4959349593495935,"Figure 4: (a) Reward and (b) state-feature (x-velocity) distribution matching in halfcheetah (top), hopper
(middle), and walker2d (bottom). The left two examples are the distributions from the best trajectories, and right
two are the distributions from the middle trajectories in the held-out test set. The rollout distributions of CDT
(red) match the target distributions (blue) very well in all cases."
REFERENCES,0.497289972899729,Published as a conference paper at ICLR 2022
REFERENCES,0.4986449864498645,"E.2
EVALUATION ON TASK PERFORMANCE"
REFERENCES,0.5,"While in this paper we focus on the evaluation with the SMM-inspired distribution matching objective,
such as Wasserstein-1 distance, we here provide the evaluation on the task rewards. Table 10 shows
that DT seems consistently better methods than Categorical DT on the task rewards evaluations, and
achieves similar performances to the held-out trajectories."
REFERENCES,0.5013550135501355,"Method
halfcheetah
hopper
walker2d
Expert
Medium
Expert
Medium
Expert
Medium
Categorical DT
10476.746 ± 218.957
5782.557 ± 1666.716
2614.559 ± 657.722
1518.757 ± 63.062
4907.475 ± 11.711
3264.585 ± 209.905
DT
10500.273 ± 312.778
4985.518 ± 67.322
2113.207 ± 807.246
1528.743 ± 30.780
4965.024 ± 11.814
4055.039 ± 849.109
Held-out
11146.200 ± 59.070
5237.348 ± 37.970
3741.854 ± 7.223
1600.196 ± 0.772
4995.553 ± 5.413
3801.313 ± 0.994"
REFERENCES,0.502710027100271,"Table 10: Evaluation on the task rewards; conditioning on the held-out trajectories as done in Section 6. We
compare the performance between Categorical DT, and DT. DT seems consistently better methods on the task
rewards evaluations, and achieves similar performances to the held-out trajectories (averaged over 5 trajectories)."
REFERENCES,0.5040650406504065,"E.3
2D STATE-FEATURE DISTRIBUTION MATCHING"
REFERENCES,0.505420054200542,"We also consider two-dimensional state-features (xy-velocities) distribution matching in Ant-v3. Same
as 1D state-feature distribution matching in HalfCheetah, Hopper, and Walker2d-v3 (Section 6.1),
we also use medium-expert(-v2) datasets from D4RL (Fu et al., 2020). We bin the state-features per
dimension separately to reduce the dimension of the categorical distribution that CDT takes as input,
while in test-time we evaluate the performance with Wasserstein-1 metric on the joint distribution.
For DT, we compute the summation of x- and y-velocity each over trajectories and normalize them
with the maximum horizon. DT feeds these two scalars as information statistics to match."
REFERENCES,0.5067750677506775,"Table 11 reveals that CDT performs better even in the case of two-dimensional state-features dis-
tributions, while DT doesn’t generalize to expert-quality trajectories. As shown in Figure 5, while
CDT could cope with the distribution shift between expert and medium target distribution, DT always
ﬁts the medium one even if the expert trajectory is given as a target. CDT successfully scales to the
ofﬂine multi-task SMM problem in the multi-dimensional feature spaces."
REFERENCES,0.508130081300813,"Method
ant
Expert
Medium
Average
Categorical DT
0.797 ± 0.216
0.244 ± 0.063
0.521
DT
1.714 ± 0.121
0.260 ± 0.067
0.987
Meta-BC
1.295 ± 0.708
0.351 ± 0.205
0.823
FOCAL (Li et al., 2021)
1.473 ± 0.892
0.913 ± 0.455
1.193"
REFERENCES,0.5094850948509485,"Table 11:
Quantitative evaluation of 2D state-feature (xy-velocities) distribution matching, measuring
Wasserstein-1 distance. CDT performs better even in the two-dimensional problem."
REFERENCES,0.510840108401084,"−2
0
2
4
6
8
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200"
REFERENCES,0.5121951219512195,Expert
REFERENCES,0.513550135501355,Probability
REFERENCES,0.5149051490514905,"−2
0
2
4
6
8
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200"
REFERENCES,0.516260162601626,"−4
−2
0
2
4
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175"
REFERENCES,0.5176151761517616,"−2
0
2
4
6
8
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175"
REFERENCES,0.518970189701897,"−2
0
2
4
6
8
x-Velocity 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16"
REFERENCES,0.5203252032520326,Medium
REFERENCES,0.521680216802168,Probability
REFERENCES,0.5230352303523035,"−2
0
2
4
6
8
x-Velocity 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16"
REFERENCES,0.524390243902439,"−4
−2
0
2
4
y-Velocity 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.5257452574525745,"−2
0
2
4
6
8
y-Velocity 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.5271002710027101,"Target
CDT
DT"
REFERENCES,0.5284552845528455,"Figure 5: Visualization of 2D state-feature (xy-velocities) distribution matching, binning each dimension
separately. Top row shows the results from an expert target trajectory, and bottom row from a medium one."
REFERENCES,0.5298102981029811,Published as a conference paper at ICLR 2022
REFERENCES,0.5311653116531165,"E.4
DETAILS OF SYNTHESIZING UNSEEN BI-MODAL DISTRIBUTION"
REFERENCES,0.532520325203252,"To construct the dataset, we modiﬁed the original reward function in HalfCheetah-v3, adding abso-
lute z-velocity term (such as +np.abs(z_vel)), where the expert cheetah backﬂips towards -x
direction."
REFERENCES,0.5338753387533876,"We trained SAC agent until convergence (3 million gradient steps), using pytorch implementation
released by Furuta et al. (2021a), and then collected 500 trajectories × 1000 time steps. We combined
them with 500 trajectories × 1000 time steps from halfcheetah-expert-v2 dataset in D4RL, which
consists of both backﬂipping and running forward behaviors."
REFERENCES,0.535230352303523,"For the evaluation, we prepared additional 5 trajectories of backﬂipping and 5 of running forward as
uni-modal behaviors (10 test trajectories in total). In addition, we synthesized a bi-modal behavior by
dividing each 1000-step trajectories into 500-step sub-trajectories, and concatenating them across
different behaviors, which results in the patchworked trajectories of ﬁrst 500-step running forward
and next 500-step backﬂipping (also 10 trajectories in total)."
REFERENCES,0.5365853658536586,"E.5
DETAILS OF DIVERSE UNSEEN DISTRIBUTION FROM META LEARNING TASK"
REFERENCES,0.537940379403794,"Following prior meta RL/IL works (Rakelly et al., 2019; Ghasemipour et al., 2019; Pong et al.,
2021; Li et al., 2021; Fakoor et al., 2020), we modiﬁed the reward function for the cheetah to run
with speciﬁed velocity (such as -np.abs(x_vel - target_vel)), and set the horizon to 200
steps."
REFERENCES,0.5392953929539296,"We prepared 31 target x-velocities; taken from [0.0, 3.0], uniformly spaced at 0.1 intervals. We also
trained the SAC agents until convergence (3 million gradient steps), using pytorch implementation
released by Furuta et al. (2021a), and collected 250 trajectories × 200 time steps each. To simplify the
problem than meta learning settings, we held out the 10 trajectories whose x-velocity is {0.5, 1.5, 2.5}
as a test set, and used the rest as a train data."
REFERENCES,0.540650406504065,"E.6
QUANTITATIVE AND QUALITATIVE RESULTS OF ONE-SHOT DISTRIBUTION MATCHING IN
FULL STATE"
REFERENCES,0.5420054200542005,"Table 12 and Table 13 show the one-shot reward and state-feature distribution matching results
respectively. In addition to the embedding size m, we also sweep the different context window
N = 20, 50, 100 for BDT (m = 16). While even simple auto-encoder regularizer (DT-AE or
DT-AE (frozen)) sometimes works well compared to no-context BC baselines presented in Table 2
and Table 9, BDT, using (second) anti-causal transformer as an encoder Φ and its aggregator, seems
consistently better than other strategies or Meta-BC baseline for ofﬂine multi-task SMM and is
comparable to CDT or DT results (also presented in Table 2 and Table 9) with longer context length
(N = 50). Through the experiment we observe that while there are no clear trends in DT-AE, -CPC
or -E2E as the size of context embedding m grows, BDT improves its performance with larger size
of embedding. Such intuitive properties might a good features to design the architectures."
REFERENCES,0.5433604336043361,"We visualize the qualitative results of BDT (m = 16, N = 20) in Figure 6."
REFERENCES,0.5447154471544715,Published as a conference paper at ICLR 2022
REFERENCES,0.5460704607046071,"Method
halfcheetah
hopper
walker2d
Average
Expert
Medium
Total
Expert
Medium
Total
Expert
Medium
Total
DT-AE (m=1)
2.494 ± 1.050
2.877 ± 0.762
2.686 ± 0.937
0.586 ± 0.016
0.089 ± 0.029
0.337 ± 0.249
0.733 ± 0.522
0.586 ± 0.445
0.660 ± 0.490
1.228
DT-CPC (m=1)
3.595 ± 0.676
2.275 ± 0.341
2.935 ± 0.849
0.600 ± 0.007
0.130 ± 0.039
0.365 ± 0.237
1.180 ± 0.019
0.139 ± 0.032
0.660 ± 0.521
1.320
DT-AE (m=4)
0.782 ± 0.329
1.720 ± 1.685
1.251 ± 1.301
0.613 ± 0.108
0.140 ± 0.068
0.376 ± 0.253
0.889 ± 0.402
0.265 ± 0.137
0.577 ± 0.433
0.735
DT-CPC (m=4)
5.887 ± 0.357
1.370 ± 1.338
3.628 ± 2.462
0.737 ± 0.018
0.238 ± 0.060
0.487 ± 0.254
1.286 ± 0.018
0.145 ± 0.031
0.715 ± 0.571
1.610
DT-AE (m=16)
2.041 ± 1.080
1.074 ± 0.814
1.558 ± 1.071
0.613 ± 0.060
0.146 ± 0.051
0.379 ± 0.240
0.837 ± 0.345
0.324 ± 0.126
0.581 ± 0.365
0.839
DT-CPC (m=16)
6.022 ± 0.316
1.406 ± 1.371
3.714 ± 2.513
0.614 ± 0.019
0.104 ± 0.025
0.359 ± 0.256
1.284 ± 0.020
0.140 ± 0.039
0.712 ± 0.572
1.595
DT-AE (m=1, joint)
3.824 ± 1.114
1.988 ± 0.970
2.906 ± 1.391
0.687 ± 0.097
0.137 ± 0.057
0.412 ± 0.286
0.723 ± 0.596
0.614 ± 0.462
0.668 ± 0.536
1.329
DT-CPC (m=1, joint)
4.460 ± 1.106
2.036 ± 1.032
3.248 ± 1.616
0.587 ± 0.015
0.106 ± 0.042
0.347 ± 0.243
0.815 ± 0.711
0.710 ± 0.398
0.763 ± 0.578
1.452
DT-AE (m=4, joint)
5.563 ± 0.449
1.028 ± 1.265
3.295 ± 2.458
1.320 ± 0.208
0.485 ± 0.235
0.902 ± 0.473
0.917 ± 0.319
0.218 ± 0.112
0.567 ± 0.423
1.588
DT-CPC (m=4, joint)
3.486 ± 1.503
2.265 ± 1.077
2.876 ± 1.443
0.690 ± 0.159
0.220 ± 0.176
0.455 ± 0.288
1.021 ± 0.708
0.554 ± 0.383
0.788 ± 0.615
1.373
DT-AE (m=16, joint)
8.450 ± 0.623
2.168 ± 0.701
5.309 ± 3.210
1.257 ± 0.361
0.655 ± 0.242
0.956 ± 0.430
2.112 ± 0.618
0.994 ± 0.413
1.553 ± 0.767
2.606
DT-CPC (m=16, joint)
4.543 ± 1.179
1.869 ± 1.453
3.206 ± 1.881
0.577 ± 0.032
0.098 ± 0.028
0.338 ± 0.241
0.953 ± 0.483
0.414 ± 0.376
0.683 ± 0.510
1.409
DT-E2E (m=1)
3.220 ± 2.325
1.026 ± 1.398
2.123 ± 2.210
1.615 ± 0.536
0.540 ± 0.293
1.078 ± 0.690
1.079 ± 0.209
0.455 ± 0.085
0.767 ± 0.351
1.323
DT-E2E (m=4)
8.076 ± 0.551
2.552 ± 0.571
5.314 ± 2.818
1.205 ± 0.390
0.493 ± 0.247
0.849 ± 0.483
2.835 ± 0.946
1.239 ± 0.460
2.037 ± 1.091
2.733
DT-E2E (m=16)
8.049 ± 0.990
1.859 ± 0.839
4.954 ± 3.228
1.102 ± 0.216
0.549 ± 0.189
0.826 ± 0.343
2.095 ± 0.434
1.241 ± 0.709
1.668 ± 0.726
2.482
DT-AE (m=1, frozen)
2.225 ± 1.017
2.804 ± 1.051
2.514 ± 1.074
0.582 ± 0.042
0.131 ± 0.058
0.357 ± 0.231
1.396 ± 0.149
0.259 ± 0.083
0.827 ± 0.581
1.233
DT-CPC (m=1, frozen)
4.110 ± 0.799
2.172 ± 0.789
3.141 ± 1.253
0.582 ± 0.021
0.102 ± 0.041
0.342 ± 0.242
1.422 ± 0.099
0.275 ± 0.109
0.848 ± 0.583
1.444
DT-AE (m=4, frozen)
0.815 ± 0.183
1.415 ± 1.755
1.115 ± 1.283
0.681 ± 0.106
0.197 ± 0.061
0.439 ± 0.257
1.066 ± 0.495
0.419 ± 0.321
0.742 ± 0.528
0.765
DT-CPC (m=4, frozen)
3.275 ± 1.186
2.752 ± 1.088
3.014 ± 1.168
0.633 ± 0.055
0.139 ± 0.082
0.386 ± 0.257
1.119 ± 0.598
0.508 ± 0.354
0.814 ± 0.578
1.404
DT-AE (m=16, frozen)
1.796 ± 0.578
1.312 ± 0.852
1.554 ± 0.767
0.637 ± 0.039
0.142 ± 0.063
0.389 ± 0.253
1.184 ± 0.326
0.405 ± 0.169
0.795 ± 0.469
0.913
DT-CPC (m=16, frozen)
3.486 ± 1.164
2.538 ± 0.905
3.012 ± 1.145
0.636 ± 0.093
0.178 ± 0.132
0.407 ± 0.256
1.318 ± 0.328
0.282 ± 0.126
0.800 ± 0.574
1.407
BDT (m=1, N=20)
1.385 ± 0.207
1.180 ± 1.753
1.282 ± 1.253
0.291 ± 0.096
0.110 ± 0.043
0.201 ± 0.117
1.113 ± 0.044
0.155 ± 0.046
0.634 ± 0.481
0.706
BDT (m=4, N=20)
1.660 ± 0.167
1.058 ± 1.580
1.359 ± 1.163
0.494 ± 0.281
0.096 ± 0.029
0.295 ± 0.282
0.181 ± 0.023
0.414 ± 0.537
0.298 ± 0.397
0.650
BDT (m=16, N=20)
1.565 ± 0.190
1.191 ± 1.830
1.378 ± 1.315
0.321 ± 0.093
0.086 ± 0.017
0.204 ± 0.135
0.204 ± 0.033
0.396 ± 0.186
0.300 ± 0.165
0.627
BDT (m=16, N=50)
0.831 ± 0.064
1.204 ± 1.803
1.018 ± 1.290
0.144 ± 0.011
0.104 ± 0.026
0.124 ± 0.028
0.199 ± 0.052
0.167 ± 0.024
0.183 ± 0.044
0.442
BDT (m=16, N=100)
0.936 ± 0.166
1.280 ± 1.861
1.108 ± 1.332
0.240 ± 0.047
0.162 ± 0.033
0.201 ± 0.056
0.057 ± 0.007
0.873 ± 0.607
0.465 ± 0.593
0.591"
REFERENCES,0.5474254742547425,"Table 12: The results of BDT and DT-X variants on the reward distribution matching problem (m = 1, 4, 16)."
REFERENCES,0.5487804878048781,"Method
halfcheetah
hopper
walker2d
Average
Expert
Medium
Total
Expert
Medium
Total
Expert
Medium
Total
DT-AE (m=1)
2.504 ± 1.050
2.880 ± 0.778
2.692 ± 0.943
0.580 ± 0.015
0.084 ± 0.027
0.332 ± 0.249
0.729 ± 0.522
0.584 ± 0.447
0.656 ± 0.491
1.227
DT-CPC (m=1)
3.595 ± 0.670
2.277 ± 0.349
2.936 ± 0.848
0.601 ± 0.008
0.130 ± 0.037
0.365 ± 0.237
1.177 ± 0.021
0.135 ± 0.033
0.656 ± 0.522
1.319
DT-AE (m=4)
0.789 ± 0.333
1.729 ± 1.714
1.259 ± 1.321
0.612 ± 0.108
0.138 ± 0.066
0.375 ± 0.254
0.884 ± 0.402
0.262 ± 0.138
0.573 ± 0.433
0.736
DT-CPC (m=4)
5.883 ± 0.361
1.371 ± 1.347
3.627 ± 2.462
0.731 ± 0.018
0.229 ± 0.057
0.480 ± 0.255
1.282 ± 0.022
0.141 ± 0.032
0.712 ± 0.571
1.606
DT-AE (m=16)
2.060 ± 1.076
1.089 ± 0.827
1.574 ± 1.075
0.611 ± 0.060
0.142 ± 0.049
0.377 ± 0.241
0.833 ± 0.346
0.321 ± 0.126
0.577 ± 0.365
0.843
DT-CPC (m=16)
6.011 ± 0.324
1.403 ± 1.384
3.707 ± 2.514
0.610 ± 0.019
0.101 ± 0.024
0.355 ± 0.256
1.281 ± 0.022
0.138 ± 0.040
0.710 ± 0.572
1.591
DT-AE (m=1, joint)
3.825 ± 1.115
1.990 ± 0.988
2.908 ± 1.397
0.683 ± 0.094
0.132 ± 0.055
0.407 ± 0.286
0.719 ± 0.597
0.611 ± 0.463
0.665 ± 0.537
1.327
DT-CPC (m=1, joint)
4.460 ± 1.101
2.035 ± 1.055
3.248 ± 1.622
0.583 ± 0.015
0.099 ± 0.041
0.341 ± 0.244
0.811 ± 0.711
0.709 ± 0.399
0.760 ± 0.579
1.450
DT-AE (m=4, joint)
5.589 ± 0.458
1.040 ± 1.270
3.315 ± 2.467
1.318 ± 0.208
0.481 ± 0.234
0.899 ± 0.473
0.912 ± 0.319
0.216 ± 0.112
0.564 ± 0.422
1.593
DT-CPC (m=4, joint)
3.484 ± 1.498
2.270 ± 1.099
2.877 ± 1.448
0.685 ± 0.157
0.214 ± 0.174
0.449 ± 0.288
1.018 ± 0.713
0.555 ± 0.384
0.786 ± 0.618
1.371
DT-AE (m=16, joint)
8.643 ± 0.679
2.260 ± 0.690
5.451 ± 3.264
1.255 ± 0.363
0.649 ± 0.241
0.952 ± 0.432
2.104 ± 0.620
0.988 ± 0.413
1.546 ± 0.767
2.650
DT-CPC (m=16, joint)
4.544 ± 1.184
1.884 ± 1.465
3.214 ± 1.882
0.575 ± 0.032
0.096 ± 0.028
0.335 ± 0.241
0.949 ± 0.484
0.412 ± 0.378
0.680 ± 0.510
1.410
DT-E2E (m=1)
3.265 ± 2.346
1.050 ± 1.413
2.158 ± 2.231
1.613 ± 0.540
0.534 ± 0.293
1.073 ± 0.692
1.073 ± 0.211
0.451 ± 0.084
0.762 ± 0.350
1.331
DT-E2E (m=4)
8.215 ± 0.604
2.684 ± 0.575
5.449 ± 2.828
1.202 ± 0.392
0.487 ± 0.246
0.845 ± 0.485
2.832 ± 0.951
1.235 ± 0.463
2.034 ± 1.094
2.776
DT-E2E (m=16)
8.208 ± 1.087
1.928 ± 0.838
5.068 ± 3.287
1.097 ± 0.217
0.542 ± 0.187
0.820 ± 0.344
2.086 ± 0.437
1.239 ± 0.712
1.662 ± 0.727
2.517
DT-AE (m=1, frozen)
2.238 ± 1.011
2.807 ± 1.056
2.523 ± 1.072
0.577 ± 0.043
0.126 ± 0.056
0.352 ± 0.231
1.391 ± 0.149
0.256 ± 0.083
0.824 ± 0.580
1.233
DT-CPC (m=1, frozen)
4.110 ± 0.794
2.173 ± 0.806
3.141 ± 1.257
0.578 ± 0.022
0.097 ± 0.039
0.338 ± 0.242
1.417 ± 0.100
0.272 ± 0.109
0.845 ± 0.582
1.441
DT-AE (m=4, frozen)
0.825 ± 0.189
1.431 ± 1.782
1.128 ± 1.303
0.679 ± 0.106
0.193 ± 0.060
0.436 ± 0.258
1.063 ± 0.495
0.418 ± 0.322
0.740 ± 0.527
0.768
DT-CPC (m=4, frozen)
3.274 ± 1.187
2.756 ± 1.094
3.015 ± 1.170
0.629 ± 0.054
0.135 ± 0.081
0.382 ± 0.257
1.115 ± 0.600
0.507 ± 0.353
0.811 ± 0.578
1.403
DT-AE (m=16, frozen)
1.821 ± 0.582
1.319 ± 0.863
1.570 ± 0.778
0.634 ± 0.038
0.137 ± 0.062
0.385 ± 0.253
1.180 ± 0.328
0.404 ± 0.170
0.792 ± 0.468
0.916
DT-CPC (m=16, frozen)
3.489 ± 1.159
2.543 ± 0.903
3.016 ± 1.141
0.631 ± 0.091
0.171 ± 0.130
0.401 ± 0.256
1.315 ± 0.329
0.279 ± 0.127
0.797 ± 0.575
1.405
BDT (m=1, N=20)
1.414 ± 0.210
1.197 ± 1.770
1.305 ± 1.265
0.288 ± 0.096
0.108 ± 0.041
0.198 ± 0.116
1.108 ± 0.045
0.152 ± 0.051
0.630 ± 0.480
0.711
BDT (m=4, N=20)
1.694 ± 0.171
1.071 ± 1.594
1.382 ± 1.175
0.490 ± 0.280
0.092 ± 0.030
0.291 ± 0.281
0.173 ± 0.024
0.411 ± 0.547
0.292 ± 0.405
0.655
BDT (m=16, N=20)
1.592 ± 0.201
1.208 ± 1.854
1.400 ± 1.333
0.318 ± 0.093
0.081 ± 0.013
0.200 ± 0.136
0.196 ± 0.031
0.392 ± 0.184
0.294 ± 0.164
0.631
BDT (m=16, N=50)
0.840 ± 0.063
1.223 ± 1.828
1.031 ± 1.307
0.142 ± 0.010
0.098 ± 0.025
0.120 ± 0.029
0.192 ± 0.051
0.163 ± 0.027
0.178 ± 0.043
0.443
BDT (m=16, N=100)
0.953 ± 0.168
1.308 ± 1.881
1.130 ± 1.347
0.240 ± 0.044
0.156 ± 0.032
0.198 ± 0.057
0.051 ± 0.006
0.883 ± 0.614
0.467 ± 0.601
0.598"
REFERENCES,0.5501355013550135,"Table 13: The results of BDT and DT-X variants on the state-feature (x-velocity) distribution matching problem
(m = 1, 4, 16)."
REFERENCES,0.551490514905149,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.5528455284552846,Probability
REFERENCES,0.55420054200542,halfcheetah best 1
REFERENCES,0.5555555555555556,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.556910569105691,"0.35
halfcheetah best 2"
REFERENCES,0.5582655826558266,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.559620596205962,halfcheetah middle 1
REFERENCES,0.5609756097560976,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.5623306233062331,halfcheetah middle 2
REFERENCES,0.5636856368563685,"1
2
3
4
5
6
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.5650406504065041,Probability
REFERENCES,0.5663956639566395,hopper best 1
REFERENCES,0.5677506775067751,"1
2
3
4
5
6
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.5691056910569106,hopper best 2
REFERENCES,0.5704607046070461,"1
2
3
4
5
6
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.5718157181571816,hopper middle 1
REFERENCES,0.573170731707317,"1
2
3
4
5
6
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.5745257452574526,hopper middle 2
REFERENCES,0.575880758807588,"−2
0
2
4
6
8
Reward 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.5772357723577236,Probability
REFERENCES,0.5785907859078591,walker2d best 1
REFERENCES,0.5799457994579946,"−2
0
2
4
6
8
Reward 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.5813008130081301,"0.5
walker2d best 2"
REFERENCES,0.5826558265582655,"−2
0
2
4
6
8
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.5840108401084011,walker2d middle 1
REFERENCES,0.5853658536585366,"−2
0
2
4
6
8
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.5867208672086721,walker2d middle 2
REFERENCES,0.5880758807588076,"Target
Rollout"
REFERENCES,0.5894308943089431,(a) Reward
REFERENCES,0.5907859078590786,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.592140921409214,Probability
REFERENCES,0.5934959349593496,halfcheetah best 1
REFERENCES,0.5948509485094851,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.5962059620596206,"0.35
halfcheetah best 2"
REFERENCES,0.5975609756097561,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.5989159891598916,halfcheetah middle 1
REFERENCES,0.6002710027100271,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.6016260162601627,halfcheetah middle 2
REFERENCES,0.6029810298102981,"1
2
3
4
5
6
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6043360433604336,Probability
REFERENCES,0.6056910569105691,hopper best 1
REFERENCES,0.6070460704607046,"1
2
3
4
5
6
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6084010840108401,hopper best 2
REFERENCES,0.6097560975609756,"1
2
3
4
5
6
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.6111111111111112,hopper middle 1
REFERENCES,0.6124661246612466,"1
2
3
4
5
6
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.6138211382113821,hopper middle 2
REFERENCES,0.6151761517615176,"−2
0
2
4
6
8
Reward 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.6165311653116531,Probability
REFERENCES,0.6178861788617886,walker2d best 1
REFERENCES,0.6192411924119241,"−2
0
2
4
6
8
Reward 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.6205962059620597,"0.5
walker2d best 2"
REFERENCES,0.6219512195121951,"−2
0
2
4
6
8
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.6233062330623306,walker2d middle 1
REFERENCES,0.6246612466124661,"−2
0
2
4
6
8
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.6260162601626016,walker2d middle 2
REFERENCES,0.6273712737127372,"Target
Rollout"
REFERENCES,0.6287262872628726,(b) x-Velocity
REFERENCES,0.6300813008130082,"Figure 6:
(a) Reward and (b) state-feature distribution matching by Bi-directional Decision Transformer
(m = 16) in halfcheetah (top), hopper (middle), and walker2d (bottom). The left two examples are the
distributions from the best trajectories, and right two are the distributions from the middle trajectories."
REFERENCES,0.6314363143631436,Published as a conference paper at ICLR 2022
REFERENCES,0.6327913279132791,"E.7
SHIFTING THE TARGET DISTRIBUTION"
REFERENCES,0.6341463414634146,"To generate the unseen but manageable generalization-test trajectories within the support of dataset
distribution, we make the reward and velocities values of trajectories in the test set shift with a constant
offset: bin_size ×{−3.0, −2.0, −1.0, 0.0, +1.0, +2.0, +3.0}. Table 14 shows the quantitative
comparison between CDT and DT, based on Wasserstein-1 distance between two distributions. CDT
successfully handles the distribution shifts (especially in hopper) better than DT. We also provide the
state-feature results (Table 15) and qualitative visualizations (Figure 7 and Figure 8), which reveals
that CDT can match the rollouts to the shifted target distributions when they are within the support of
dataset distributions."
REFERENCES,0.6355013550135501,"Method
halfcheetah
hopper
walker2d
Average
Expert
Medium
Total
Expert
Medium
Total
Expert
Medium
Total
Categorical DT
1.126 ± 0.245
2.026 ± 1.180
1.576 ± 0.964
0.147 ± 0.034
0.302 ± 0.085
0.224 ± 0.101
0.285 ± 0.044
1.024 ± 0.076
0.655 ± 0.375
0.818
DT
1.133 ± 0.197
1.978 ± 1.104
1.555 ± 0.899
0.521 ± 0.041
0.531 ± 0.045
0.526 ± 0.043
0.656 ± 0.380
0.915 ± 0.106
0.786 ± 0.308
0.956"
REFERENCES,0.6368563685636857,"Table 14: Wasserstein-1 distance between shifted reward distribution and the rollout distributions. Categorical
DT handles the target distribution shifts and matches the distributions better than DT, since CDT is aware of
distributional information of entire trajectories."
REFERENCES,0.6382113821138211,"Method
halfcheetah
hopper
walker2d
Average
Expert
Medium
Total
Expert
Medium
Total
Expert
Medium
Total
Categorical DT
1.270 ± 0.242
2.371 ± 1.747
1.821 ± 1.363
0.157 ± 0.038
0.337 ± 0.088
0.247 ± 0.112
0.289 ± 0.052
0.964 ± 0.104
0.626 ± 0.347
0.898
DT
1.173 ± 0.372
2.056 ± 1.045
1.614 ± 0.901
0.432 ± 0.087
0.531 ± 0.053
0.482 ± 0.088
0.408 ± 0.246
0.885 ± 0.143
0.646 ± 0.312
0.914"
REFERENCES,0.6395663956639567,"Table 15: Wasserstein-1 distance between shifted state-feature (x-velocity) distribution and the rollout distri-
butions. Similar to the case of reward, Categorical DT handles the target distribution shifts and matches the
distributions better than DT."
REFERENCES,0.6409214092140921,"0
5
10
0.0 0.1 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.6422764227642277,Probability
REFERENCES,0.6436314363143631,halfcheetah best -3.0
REFERENCES,0.6449864498644986,"0
5
10
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.6463414634146342,"0.6
halfcheetah best -2.0"
REFERENCES,0.6476964769647696,"0
5
10
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.6490514905149052,"0.6
halfcheetah best -1.0"
REFERENCES,0.6504065040650406,"0
5
10
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.6517615176151762,"0.6
halfcheetah best 0.0"
REFERENCES,0.6531165311653117,"0
5
10
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.6544715447154471,"0.6
halfcheetah best +1.0"
REFERENCES,0.6558265582655827,"0
5
10
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.6571815718157181,"0.6
halfcheetah best +2.0"
REFERENCES,0.6585365853658537,"0
5
10
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.6598915989159891,"0.6
halfcheetah best +3.0"
REFERENCES,0.6612466124661247,"0
5
10
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6626016260162602,Probability
REFERENCES,0.6639566395663956,halfcheetah middle -3.0
REFERENCES,0.6653116531165312,"0
5
10
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6666666666666666,halfcheetah middle -2.0
REFERENCES,0.6680216802168022,"0
5
10
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6693766937669376,halfcheetah middle -1.0
REFERENCES,0.6707317073170732,"0
5
10
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6720867208672087,halfcheetah middle 0.0
REFERENCES,0.6734417344173442,"0
5
10
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6747967479674797,halfcheetah middle +1.0
REFERENCES,0.6761517615176151,"0
5
10
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6775067750677507,halfcheetah middle +2.0
REFERENCES,0.6788617886178862,"0
5
10
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6802168021680217,halfcheetah middle +3.0
REFERENCES,0.6815718157181572,"2
4
6
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6829268292682927,Probability
REFERENCES,0.6842818428184282,hopper best -3.0
REFERENCES,0.6856368563685636,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.6869918699186992,"0.25
hopper best -2.0"
REFERENCES,0.6883468834688347,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.6897018970189702,"0.25
hopper best -1.0"
REFERENCES,0.6910569105691057,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.6924119241192412,"0.25
hopper best 0.0"
REFERENCES,0.6937669376693767,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.6951219512195121,"0.25
hopper best +1.0"
REFERENCES,0.6964769647696477,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.6978319783197832,"0.25
hopper best +2.0"
REFERENCES,0.6991869918699187,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7005420054200542,"0.25
hopper best +3.0"
REFERENCES,0.7018970189701897,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7032520325203252,Probability
REFERENCES,0.7046070460704607,hopper middle -3.0
REFERENCES,0.7059620596205962,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7073170731707317,hopper middle -2.0
REFERENCES,0.7086720867208672,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7100271002710027,hopper middle -1.0
REFERENCES,0.7113821138211383,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7127371273712737,hopper middle 0.0
REFERENCES,0.7140921409214093,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7154471544715447,hopper middle +1.0
REFERENCES,0.7168021680216802,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7181571815718157,hopper middle +2.0
REFERENCES,0.7195121951219512,"2
4
6
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7208672086720868,hopper middle +3.0
REFERENCES,0.7222222222222222,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.7235772357723578,Probability
REFERENCES,0.7249322493224932,walker2d best -3.0
REFERENCES,0.7262872628726287,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.7276422764227642,walker2d best -2.0
REFERENCES,0.7289972899728997,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.7303523035230353,walker2d best -1.0
REFERENCES,0.7317073170731707,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.7330623306233063,walker2d best 0.0
REFERENCES,0.7344173441734417,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.7357723577235772,walker2d best +1.0
REFERENCES,0.7371273712737128,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.7384823848238482,walker2d best +2.0
REFERENCES,0.7398373983739838,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.7411924119241192,walker2d best +3.0
REFERENCES,0.7425474254742548,"−2.5
0.0
2.5
5.0
7.5
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.7439024390243902,Probability
REFERENCES,0.7452574525745257,walker2d middle -3.0
REFERENCES,0.7466124661246613,"−2.5
0.0
2.5
5.0
7.5
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.7479674796747967,walker2d middle -2.0
REFERENCES,0.7493224932249323,"−2.5
0.0
2.5
5.0
7.5
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.7506775067750677,walker2d middle -1.0
REFERENCES,0.7520325203252033,"−2.5
0.0
2.5
5.0
7.5
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.7533875338753387,walker2d middle 0.0
REFERENCES,0.7547425474254743,"−2.5
0.0
2.5
5.0
7.5
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.7560975609756098,walker2d middle +1.0
REFERENCES,0.7574525745257452,"−2.5
0.0
2.5
5.0
7.5
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.7588075880758808,walker2d middle +2.0
REFERENCES,0.7601626016260162,"−2.5
0.0
2.5
5.0
7.5
Reward 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.7615176151761518,walker2d middle +3.0
REFERENCES,0.7628726287262872,"Target
Rollout
Deterministic"
REFERENCES,0.7642276422764228,"Figure 7: Reward distribution matching in halfcheetah (top two rows; best and middle), hopper (middle two
rows; best and middle), and walker2d (bottom two rows; best and middle). We shift the target distribution with
(from left to right column); bin_size ×{−3.0, −2.0, −1.0, 0.0, +1.0, +2.0, +3.0} (Table 14). Categorical
DT (red) can match the rollouts to the shifted target distributions (blue) when the shifted targets are within
the support of dataset distributions. For DT (yellow, captioned as Deterministic), we only visualize the delta
function at the means of rollouts."
REFERENCES,0.7655826558265583,Published as a conference paper at ICLR 2022
REFERENCES,0.7669376693766937,"0
5
10
15
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.7682926829268293,Probability
REFERENCES,0.7696476964769647,halfcheetah best -3.0
REFERENCES,0.7710027100271003,"0
5
10
15
0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.7723577235772358,"0.5
halfcheetah best -2.0"
REFERENCES,0.7737127371273713,"0
5
10
15
0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.7750677506775068,"0.5
halfcheetah best -1.0"
REFERENCES,0.7764227642276422,"0
5
10
15
0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.7777777777777778,"0.5
halfcheetah best 0.0"
REFERENCES,0.7791327913279132,"0
5
10
15
0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.7804878048780488,"0.5
halfcheetah best +1.0"
REFERENCES,0.7818428184281843,"0
5
10
15
0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.7831978319783198,"0.5
halfcheetah best +2.0"
REFERENCES,0.7845528455284553,"0
5
10
15
0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.7859078590785907,"0.5
halfcheetah best +3.0"
REFERENCES,0.7872628726287263,"0
5
10
15
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.7886178861788617,Probability
REFERENCES,0.7899728997289973,halfcheetah middle -3.0
REFERENCES,0.7913279132791328,"0
5
10
15
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.7926829268292683,halfcheetah middle -2.0
REFERENCES,0.7940379403794038,"0
5
10
15
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.7953929539295393,halfcheetah middle -1.0
REFERENCES,0.7967479674796748,"0
5
10
15
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.7981029810298103,halfcheetah middle 0.0
REFERENCES,0.7994579945799458,"0
5
10
15
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8008130081300813,halfcheetah middle +1.0
REFERENCES,0.8021680216802168,"0
5
10
15
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8035230352303523,halfcheetah middle +2.0
REFERENCES,0.8048780487804879,"0
5
10
15
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8062330623306233,halfcheetah middle +3.0
REFERENCES,0.8075880758807588,"0
2
4
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8089430894308943,Probability
REFERENCES,0.8102981029810298,hopper best -3.0
REFERENCES,0.8116531165311653,"0
2
4
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8130081300813008,hopper best -2.0
REFERENCES,0.8143631436314364,"0
2
4
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8157181571815718,hopper best -1.0
REFERENCES,0.8170731707317073,"0
2
4
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8184281842818428,hopper best 0.0
REFERENCES,0.8197831978319783,"0
2
4
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8211382113821138,hopper best +1.0
REFERENCES,0.8224932249322493,"0
2
4
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8238482384823849,hopper best +2.0
REFERENCES,0.8252032520325203,"0
2
4
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8265582655826558,hopper best +3.0
REFERENCES,0.8279132791327913,"0
2
4
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.8292682926829268,Probability
REFERENCES,0.8306233062330624,hopper middle -3.0
REFERENCES,0.8319783197831978,"0
2
4
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.8333333333333334,hopper middle -2.0
REFERENCES,0.8346883468834688,"0
2
4
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.8360433604336044,hopper middle -1.0
REFERENCES,0.8373983739837398,"0
2
4
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.8387533875338753,hopper middle 0.0
REFERENCES,0.8401084010840109,"0
2
4
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.8414634146341463,hopper middle +1.0
REFERENCES,0.8428184281842819,"0
2
4
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.8441734417344173,hopper middle +2.0
REFERENCES,0.8455284552845529,"0
2
4
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.8468834688346883,hopper middle +3.0
REFERENCES,0.8482384823848238,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8495934959349594,Probability
REFERENCES,0.8509485094850948,walker2d best -3.0
REFERENCES,0.8523035230352304,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8536585365853658,walker2d best -2.0
REFERENCES,0.8550135501355014,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8563685636856369,walker2d best -1.0
REFERENCES,0.8577235772357723,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8590785907859079,walker2d best 0.0
REFERENCES,0.8604336043360433,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8617886178861789,walker2d best +1.0
REFERENCES,0.8631436314363143,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8644986449864499,walker2d best +2.0
REFERENCES,0.8658536585365854,"−2.5
0.0
2.5
5.0
7.5
0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8672086720867209,walker2d best +3.0
REFERENCES,0.8685636856368564,"−2.5
0.0
2.5
5.0
7.5
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.8699186991869918,Probability
REFERENCES,0.8712737127371274,walker2d middle -3.0
REFERENCES,0.8726287262872628,"−2.5
0.0
2.5
5.0
7.5
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.8739837398373984,walker2d middle -2.0
REFERENCES,0.8753387533875339,"−2.5
0.0
2.5
5.0
7.5
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.8766937669376694,walker2d middle -1.0
REFERENCES,0.8780487804878049,"−2.5
0.0
2.5
5.0
7.5
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.8794037940379403,walker2d middle 0.0
REFERENCES,0.8807588075880759,"−2.5
0.0
2.5
5.0
7.5
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.8821138211382114,walker2d middle +1.0
REFERENCES,0.8834688346883469,"−2.5
0.0
2.5
5.0
7.5
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.8848238482384824,walker2d middle +2.0
REFERENCES,0.8861788617886179,"−2.5
0.0
2.5
5.0
7.5
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.8875338753387534,walker2d middle +3.0
REFERENCES,0.8888888888888888,"Target
Rollout
Deterministic"
REFERENCES,0.8902439024390244,"Figure 8: State-feature distribution matching, especially x-velocity, in halfcheetah (top two rows; best
and middle), hopper (middle two rows; best and middle), and walker2d (bottom two rows; best and
middle).
We shift the target distribution with constant offset (from left to right column); bin_size
×{−3.0, −2.0, −1.0, 0.0, +1.0, +2.0, +3.0} (Table 15). For DT (yellow, captioned as Deterministic), we
only visualize the delta function at the means of rollouts."
REFERENCES,0.8915989159891599,Published as a conference paper at ICLR 2022
REFERENCES,0.8929539295392954,"E.8
SYNTHESIZING UNREALISTIC TARGET DISTRIBUTION"
REFERENCES,0.8943089430894309,"We synthesize six target distributions manually generating x-velocity samples from Gaussian distribu-
tions via python scripts as done in prior works (Ghasemipour et al., 2020; Gu et al., 2021). Since we
consider the one-dimensional feature space (x-velocity), we simply specify the mean and standard
deviation of Gaussian distributions referring each dataset distribution, as shown in Figure 3, and then
generate the 1000 samples per trajectory. While these targets are designed at least within the support
of the dataset, we do not consider physical realizability."
REFERENCES,0.8956639566395664,"• HalfCheetah: (µ, σ) = (5.0, 1.0), (13.0, 1.0), (9.0, 1.0), (2.5, 1.0), {(5.0, 1.0), (13.0, 1.0)}, {(9.0,
1.0), (2.5, 1.0)}.
• Hopper: (µ, σ) = (2.5, 1.0), (1.5, 1.0), (3.5, 1.0), (2.5, 0.5), {(3.5, 1.0), (1.5, 1.0)}, {(3.5, 0.5), (1.5,
0.5)}.
• Walker2d: (µ, σ) = (3.5, 1.0), (2.5, 1.0), (4.5, 1.0), (1.5, 1.0), {(2.5, 0.5), (4.5, 0.5)}, {(1.5, 0.5),
(4.5, 0.5)}."
REFERENCES,0.8970189701897019,"For the last two sets, that have different distributional parameters {(µ1, σ1), (µ2, σ2)}, we sampled
from two gaussian distributions 500 samples each, and marge them as one trajectory that has multiple
modes."
REFERENCES,0.8983739837398373,"Although they might be unrealistic, Figure 9 implies that CDT tends to match the target distributions,
even in the cases of bi-modal target distributions. Such generalization to synthesized distribution is
an important beneﬁt of distribution-conditioned training. We also quantify the performance of CDT
against DT from distributional matching perspective in Table 16."
REFERENCES,0.8997289972899729,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.9010840108401084,Probability
REFERENCES,0.9024390243902439,halfcheetah synthesized 1
REFERENCES,0.9037940379403794,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.9051490514905149,halfcheetah synthesized 2
REFERENCES,0.9065040650406504,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.907859078590786,halfcheetah synthesized 3
REFERENCES,0.9092140921409214,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.9105691056910569,halfcheetah synthesized 4
REFERENCES,0.9119241192411924,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.9132791327913279,halfcheetah synthesized 5
REFERENCES,0.9146341463414634,"−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.9159891598915989,halfcheetah synthesized 6
REFERENCES,0.9173441734417345,"0
1
2
3
4
5
0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.9186991869918699,Probability
REFERENCES,0.9200542005420054,hopper synthesized 1
REFERENCES,0.9214092140921409,"0
1
2
3
4
5
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.9227642276422764,"0.25
hopper synthesized 2"
REFERENCES,0.924119241192412,"0
1
2
3
4
5
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.9254742547425474,"0.25
hopper synthesized 3"
REFERENCES,0.926829268292683,"0
1
2
3
4
5
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.9281842818428184,"0.25
hopper synthesized 4"
REFERENCES,0.9295392953929539,"0
1
2
3
4
5
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.9308943089430894,"0.25
hopper synthesized 5"
REFERENCES,0.9322493224932249,"0
1
2
3
4
5
0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.9336043360433605,"0.25
hopper synthesized 6"
REFERENCES,0.9349593495934959,"−4
−2
0
2
4
6
8
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.9363143631436315,Probability
REFERENCES,0.9376693766937669,walker2d synthesized 1
REFERENCES,0.9390243902439024,"−4
−2
0
2
4
6
8
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.940379403794038,walker2d synthesized 2
REFERENCES,0.9417344173441734,"−4
−2
0
2
4
6
8
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.943089430894309,walker2d synthesized 3
REFERENCES,0.9444444444444444,"−4
−2
0
2
4
6
8
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.94579945799458,walker2d synthesized 4
REFERENCES,0.9471544715447154,"−4
−2
0
2
4
6
8
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.948509485094851,walker2d synthesized 5
REFERENCES,0.9498644986449865,"−4
−2
0
2
4
6
8
x-Velocity 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.9512195121951219,walker2d synthesized 6
REFERENCES,0.9525745257452575,"Target
Rollout"
REFERENCES,0.9539295392953929,"Figure 9: State-feature distribution matching in halfcheetah (top), hopper (middle), and walker2d (bottom).
We synthesize the each target distribution from Gaussian distributions. While the results are worse than for
realistic test target distributions in Figure 4, considering that many of these arbitrary synthetic targets could
be unrealizable, seeing bi-modal matching results show that indeed CDT has learned to generalize something
non-trivial."
REFERENCES,0.9552845528455285,"Method
halfcheetah
hopper
walker2d
Average
Categorical DT
2.059 ± 0.772
0.536 ± 0.225
0.920 ± 0.428
1.172
DT
4.256 ± 2.220
0.584 ± 0.298
0.974 ± 0.540
1.938"
REFERENCES,0.9566395663956639,"Table 16: State-feature (x-velocity) distribution matching with synthesized, physically unrealistic target distri-
butions generated from scripted Gaussian distributions. We compare Categorical DT and DT. Categorical DT
manages to deal with such unrealistic target matching."
REFERENCES,0.9579945799457995,Published as a conference paper at ICLR 2022
REFERENCES,0.959349593495935,"F
DETAILS FOR CATEGORICAL DECISION TRANSFORMER"
REFERENCES,0.9607046070460704,"Categorical Decision Transformer (CDT) takes histograms of categorical distribution (i.e. discrete
approximations of feature distributions; B-dim vector) as the inputs of the transformer. Here we
describe how to compute the distributions for all timesteps given a trajectory t ∈[0, 1, . . . , T]. For
simplicity, we explain the case of one-dimensional feature (e.g. scalar reward), but for n-dimensional
features, we can adopt following procedure per each dimension and arrive at n categorical features.
This essentially approximates the full joints with a product of independent marginal distribution per
dimension, and ensures that number of samples for getting reasonably discretized approximations do
not need to grow exponentially as the dimension grows."
REFERENCES,0.962059620596206,"At ﬁrst, we discretize the feature space F using B bins (per dimension), and convert the feature φt
into the one-hot representation ˜φt. The range of feature space [φmin, φmax] is pre-deﬁned from the
given ofﬂine datasets. zΦhist(t), the categorical feature distribution at time step t, can be computed
recursively following Bellman-like equation:"
REFERENCES,0.9634146341463414,"zΦhist(t) ∝˜φt + γ(1 −1[t = T])zΦhist(t + 1),
(7)"
REFERENCES,0.964769647696477,"where 1 is the indicator function. We compute a series of zΦhist in a backward manner starting from T.
After we obtain the desired information statistics for all trajectories, we feed them to the categorical
transformer during training/test time. We describe the python-like pseudocode in Algorithm 1,
coloring the changes from the original Decision Transformer (Chen et al., 2021a)."
REFERENCES,0.9661246612466124,"G
DETAILS OF DECISION TRANSFORMER WITH LEARNED Φ"
REFERENCES,0.967479674796748,"While any unsupervised regularizer for an encoder could be combined into the action MSE loss of
Decision Transformer to obtain the learned Φ efﬁciently, we observe that even simple objectives,
such as auto-encoder and contrastive loss, sometimes perform well. For auto-encoder regularization
(DT-AE), we train the MLP encoder (parameterized by ψ) and decoder with MSE loss of current
state reconstruction:
min
ψ
Es∼D

∥s −decoderψ(encoderψ(s))∥2
."
REFERENCES,0.9688346883468835,"Then, we use the output of the encoder as a learned information statistics. In addition, for contrastive
loss (DT-CPC), we adopt CURL objective (Srinivas et al., 2020) for state input while adding gaussian
perturbation ϵ ∼N(µ = 0.0, σ = 0.1) as data argumentation (following Sinha et al. (2021)). We
train the MLP encoder as a query, use its momentum encoder as a key:"
REFERENCES,0.9701897018970189,"min
ψ
E
s∼D,
ϵ,ϵ′∼N(0,σ) """
REFERENCES,0.9715447154471545,"log
exp (encoderψ(s)T Wencoder ˜
ψ(s + ϵ))
P"
REFERENCES,0.9728997289972899,"ϵ′̸=ϵ exp (encoderψ(s)T Wencoder ˜
ψ(s + ϵ′)) # ,"
REFERENCES,0.9742547425474255,"where W is a learned parameter matrix for the bi-linear inner-product, and ˜ψ is the weights of the
momentum encoder, updated as ˜ψ ←m ˜ψ + (1 −m)ψ, and m = 0.95. We treat its trained encoder
output as a learned information statistics. It remains as future work to combine more advanced,
temporary-extended objectives, such as attentive contrastive learning approach proposed in Yang
& Nachum (2021). As described in Section 6.3, we consider three strategies to train the encoder
for DT-AE and -CPC: training with only unsupervised loss, training with unsupervised and DT’s
supervised loss jointly (called as “joint”), and pre-training with only unsupervised loss and freezing
the weights during DT training (called as “frozen”). We train DT-E2E with only DT’s supervised loss
as in Algorithm 1 for CDT and BDT."
REFERENCES,0.975609756097561,Published as a conference paper at ICLR 2022
REFERENCES,0.9769647696476965,"Algorithm 1 Categorical/Bi-directional Decision Transformer Pseudocode: Orange and green
texts describe additional details on top of the base DT pseudocode from Chen et al. (2021a)."
REFERENCES,0.978319783197832,"# z: information statistics (histogram, or learned representation)
# s, a, t: states, actions, or timesteps
# transformer: transformer with causal masking (GPT)
# embed_s, embed_a, embed_z: linear embedding layers
# anti_causal_tf: second transformer as encoder and aggregator
# embed_t: learned episode positional embedding
# pred_a: linear action prediction layer
# compute_stats: a function to compute information statistics"
REFERENCES,0.9796747967479674,"# main model
def DecisionTransformer(z, s, a, t):"
REFERENCES,0.981029810298103,"# compute embeddings for tokens
pos_embedding = embed_t(t)
s_embedding = embed_s(s) + pos_embedding
a_embedding = embed_a(a) + pos_embedding
if categorical:"
REFERENCES,0.9823848238482384,"z_embedding = embed_z(z) + pos_embedding
elif bi_directional:"
REFERENCES,0.983739837398374,"# input state sequence in a reverse order
# NOTE: z is a target state sequence here
reversed = flip(z)
z_embedding = embed_z(anti_causal_tf(reversed)) + pos_embedding"
REFERENCES,0.9850948509485095,"input_embeds = stack(z_embedding, s_embedding, a_embedding)"
REFERENCES,0.986449864498645,"# use transformer to get hidden states
hidden_states = transformer(input_embeds=input_embeds)"
REFERENCES,0.9878048780487805,"# select hidden states for action prediction tokens
a_hidden = unstack(hidden_states).actions"
REFERENCES,0.989159891598916,"# predict action
return pred_a(a_hidden)"
REFERENCES,0.9905149051490515,"# training loop
train_z = compute_stats(train_dataset)
# dims: (batch_size, K, dim)
for z, (s, a, t) in zip(train_z, train_dataset):
if unsupervised:"
REFERENCES,0.991869918699187,"z = s
a_preds = DecisionTransformer(z, s, a, t)
loss = mean((a_preds - a)**2)
optimizer.zero_grad(); loss.backward(); optimizer.step()"
REFERENCES,0.9932249322493225,"# evaluation loop
test_z = compute_stats(test_dataset)
n_tests = len(test_dataset)
for index in range(n_tests):
test_trajectory = test_dataset[index]
max_time_steps = len(test_trajectory)
s, a, t, done = [env.reset()], [], [1], False
# conditioning on the desired information statistics
if categorical:"
REFERENCES,0.994579945799458,"z = [test_z[index][0]]
elif bi_directional:"
REFERENCES,0.9959349593495935,"z = [test_trajectory[’observations’][0]]
for t in range(max_time_steps):
action = DecisionTransformer(z, s, a, t)[-1]
new_s, r, done, _ = env.step(action)
# append new tokens to sequence
if categorical:"
REFERENCES,0.997289972899729,"z = z + [test_z[index][t+1]]
elif bi_directional:"
REFERENCES,0.9986449864498645,"z = z + [test_trajectory[’observations’][t+1]]
s, a, t = s + [new_s], a + [action], t + [len(z)]
z, s, a, t = z[-N:], ...
# only keep context length of N"
