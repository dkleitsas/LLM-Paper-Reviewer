Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009784735812133072,"Despite their simple intuition, convolutions are more tedious to analyze than dense
layers, which complicates the transfer of theoretical and algorithmic ideas to convo-
lutions. We simplify convolutions by viewing them as tensor networks (TNs) that
allow reasoning about the underlying tensor multiplications by drawing diagrams,
manipulating them to perform function transformations like differentiation, and
efficiently evaluating them with einsum. To demonstrate their simplicity and
expressiveness, we derive diagrams of various autodiff operations and popular cur-
vature approximations with full hyper-parameter support, batching, channel groups,
and generalization to any convolution dimension. Further, we provide convolution-
specific transformations based on the connectivity pattern which allow to simplify
diagrams before evaluation. Finally, we probe performance. Our TN implementa-
tion accelerates a recently-proposed KFAC variant up to 4.5 x while removing the
standard implementation’s memory overhead, and enables new hardware-efficient
tensor dropout for approximate backpropagation."
INTRODUCTION,0.0019569471624266144,"1
Introduction"
INTRODUCTION,0.0029354207436399216,"Convolutional neural networks [CNNs, 39] mark a milestone in the development of deep learning
architectures as their ‘sliding window’ approach represents an important inductive bias for vision
tasks. Their intuition is simple to explain with graphical illustrations [e.g. 21]. Yet, convolutions are
more challenging to analyze than dense layers in multi-layer perceptrons (MLPs) or transformers [71].
One reason is that they are hard to express in matrix notation and—even in index notation—compact
expressions that are convenient to work with only exist for special hyper-parameters [e.g. 27, 2]. Many
hyper-parameters (stride, padding, ...) and additional features like channel groups [36] introduce
even more complexity that is inherited by related routines, e.g. for autodiff. We observe a delay of
analytic and algorithmic developments between MLPs vs. CNNs, e.g."
INTRODUCTION,0.003913894324853229,• Approximate Hessian diagonal: 1989 vs. 2024
INTRODUCTION,0.004892367906066536,• Hessian rank: 2021 vs. 2023
INTRODUCTION,0.005870841487279843,• Gradient descent learning dynamics: 2014 vs. 2023
INTRODUCTION,0.00684931506849315,• Neural tangent kernel (NTK): 2018 vs. 2019
INTRODUCTION,0.007827788649706457,• Kronecker-factored quasi-Newton methods: 2021 vs. 2022
INTRODUCTION,0.008806262230919765,"• Kronecker-factored curvature (KFAC, KFRA, KFLR): (2015, 2017, 2017) vs. (2016, 2020, 2020)"
INTRODUCTION,0.009784735812133072,"The software support for less standard routines some of these methods require also reflects this gap.
Some functions only support special dimensions [15]. Others use less efficient workarounds (§5.1) or
are not provided at all (§B.4). And they are hard to modify as the code is either closed-source [12] or"
INTRODUCTION,0.010763209393346379,"from
einconv
import
conv_index_pattern ; from
einops
import
einsum; import
torch"
INTRODUCTION,0.011741682974559686,"I, K, D, P, S = ...
# convolution
hyper -parameters (2- tuples)
X = torch.rand(C_in , I[0], I[1])
# input to
convolution
kfc_shape = (C_in * K[0] * K[1], C_in * K[0] * K[1]) # final
shape"
INTRODUCTION,0.012720156555772993,"def
kfc_im2col ():
"""""" Compute
Kronecker
factor
via
im2col.""""""
X_unf = torch.nn.functional.unfold(X, K, D, P, S)
return
einsum(X_unf , X_unf , ""i out , j out
-> i j"")"
INTRODUCTION,0.0136986301369863,"def
kfc_tn ():
"""""" Compute
Kronecker
factor
via its
tensor
network.""""""
Pi1 = conv_index_pattern (I[0], K[0], S[0], P[0], D[0])
Pi2 = conv_index_pattern (I[1], K[1], S[1], P[1], D[1])
return
einsum(X, Pi1 , Pi2 , X, Pi1 , Pi2 ,
""c_in i1 i2 , i1 o1 k1 , i2 o2 k2 , c_in_
i1_ i2_ , i1_ o1_ k1_ i2_ o2_ k2_""
+ "" -> c_in k1 k2 c_in_
k1_ k2_"").reshape(kfc_shape)"
INTRODUCTION,0.014677103718199608,"def
kfc_simplified_tn (): # for
dense
convolutions
"""""" Compute
Kronecker
factor
via its
simplified
tensor
network.""""""
X = X.reshape(C_in , I[0] // K[0], K[0], I[1] // K[1], K[1])
return
einsum(X, X, ""c_in o1 k1 o2 k2 , c_in_
o1_ k1_ o2_ k2_""
+ ""-> c_in k1 k2 c_in_
k1_ k2_"").reshape(kfc_shape)"
INTRODUCTION,0.015655577299412915,"Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum.
We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard
implementation (top) requires unfolding the input (high memory). The TN (middle) enables internal
optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (Bottom)
In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost."
INTRODUCTION,0.016634050880626222,"JXK
JXK"
INTRODUCTION,0.01761252446183953,"o1
o2
cin
k1
k2"
INTRODUCTION,0.018590998043052837,"c′
in
k′
1
k′
2"
INTRODUCTION,0.019569471624266144,"Π(1)
Π(1)"
INTRODUCTION,0.02054794520547945,"Π(2)
Π(2) X
X i1"
INTRODUCTION,0.021526418786692758,"i2
i′
2
i′
1 o1 o2"
INTRODUCTION,0.022504892367906065,"k1
k′
1"
INTRODUCTION,0.023483365949119372,"k2
k′
2"
INTRODUCTION,0.02446183953033268,"cin
c′
in"
INTRODUCTION,0.025440313111545987,"Π(1)
Π(1)"
INTRODUCTION,0.026418786692759294,"Π(2)
Π(2) X
X i1"
INTRODUCTION,0.0273972602739726,"i2
i′
2
i′
1 o1 o2"
INTRODUCTION,0.02837573385518591,"k1
k′
1"
INTRODUCTION,0.029354207436399216,"k2
k′
2"
INTRODUCTION,0.030332681017612523,"cin
c′
in"
INTRODUCTION,0.03131115459882583,"written in a low-level language. This complicates the advance of existing, and the exploration of new,
algorithmic ideas for convolutions."
INTRODUCTION,0.03228962818003914,"Here, we seek to reduce this complexity gap by viewing convolutions as tensor networks [TNs,
53, 6, 9] which express the underlying tensor multiplications as diagrams. These diagrams are simpler
to parse than mathematical equations and can seamlessly be (i) manipulated to take derivatives,
add batching, or extract sub-tensors, (ii) merged with other diagrams, and (iii) evaluated with
einsum. This yields simple, modifiable implementations that benefit from automated under-the-hood-
optimizations for efficient TN contraction developed by the quantum simulation community [e.g.
66, 25, 74, 13], like finding a high-quality contraction order or distributing computations:"
INTRODUCTION,0.033268101761252444,"1. We use the TN format of convolution from Hayashi et al. [29] to derive diagrams and
einsum formulas for autodiff and less standard routines for curvature approximations with
support for all hyper-parameters, batching, groups, and any dimension (Table 1)."
INTRODUCTION,0.03424657534246575,"2. We present transformations based on the convolution’s connectivity pattern to re-wire and
symbolically simplify TNs before evaluation (example in Figure 1)."
INTRODUCTION,0.03522504892367906,"3. We compare default and TN implementations, demonstrating optimal peak memory reduc-
tion and run time improvements up to 4.5 x for a recent KFAC variant, and showcase their
flexibility to impose hardware-efficient dropout for randomized backpropagation."
INTRODUCTION,0.036203522504892366,"Our work not only provides simpler perspectives and implementations that facilitate the exploration
of algorithmic ideas for convolutions, but also directly advances second-order methods like KFAC: It
enables more frequent pre-conditioner updates, using larger batches without going out of memory,
and extending KFAC to transpose convolution. These improvements are important for second-order
optimization and other applications like Laplace approximations [20] and influence functions [28]."
PRELIMINARIES,0.03718199608610567,"2
Preliminaries"
PRELIMINARIES,0.03816046966731898,"We briefly review 2d convolution (§2.1), tensor multiplication and einsum (§2.2), then introduce the
graphical TN notation and apply it to convolution (§2.3). Bold lower-case (a), upper-case (A), and
upper-case sans-serif (A) symbols indicate vectors, matrices, and tensors. Entries follow the same
convention but use regular font weight; [·] denotes slicing ( [A]i,j = Ai,j). Parenthesized indices
mean reshapes, e.g. [a](i,j) = [A]i,j with a the flattened matrix A. Π(1) Π(2) X
W i1 i2 k1 k2"
PRELIMINARIES,0.03913894324853229,"cin
cout o1 o2 Y
= o1 o2 cout"
PRELIMINARIES,0.040117416829745595,(a) Convolution Π(1) Π(2) X i1 i2
PRELIMINARIES,0.0410958904109589,"(o1, o2)"
PRELIMINARIES,0.04207436399217221,"(cin, k1, k2) cin k1 k2 o1 o2"
PRELIMINARIES,0.043052837573385516,"JXK
(cin, k1, k2)"
PRELIMINARIES,0.04403131115459882,"(o1, o2) ="
PRELIMINARIES,0.04500978473581213,(b) Input unfolding Π(1) Π(2) W k1 k2
PRELIMINARIES,0.04598825831702544,"(cout, o1, o2)"
PRELIMINARIES,0.046966731898238745,"(cin, i1, i2)"
PRELIMINARIES,0.04794520547945205,"cin
cout i1 i2 o1 o2"
PRELIMINARIES,0.04892367906066536,"A(W)
(cin, i1, i2)"
PRELIMINARIES,0.049902152641878667,"(cout, o1, o2) ="
PRELIMINARIES,0.050880626223091974,(c) Kernel unfolding
PRELIMINARIES,0.05185909980430528,"Figure 2: TNs of (a) 2d convolution and (b,c) connections to its matrix multiplication view. The
connectivity along each dimension is explicit via an index pattern tensor Π."
CONVOLUTION,0.05283757338551859,"2.1
Convolution"
CONVOLUTION,0.053816046966731895,"2d convolutions process channels of 2d signals X ∈RCin×I1×I2 with Cin channels of spatial dimen-
sions1 I1, I2 by sliding a collection of Cout filter banks, arranged in a kernel W ∈RCout×Cin×K1×K2
with kernel size K1, K2, over the input.
The sliding operation depends on various hyper-
parameters [padding, stride, . . . , see 21]. At each step, the filters are contracted with the overlapping
area, yielding the channel values of a pixel in the output Y ∈RCout×O1×O2 with spatial dimensions
O1, O2. Optionally, a bias from b ∈RCout is added per channel."
CONVOLUTION,0.0547945205479452,"One way to implement convolution is via matrix multiplication [10], similar to fully-connected
layers. First, one extracts the overlapping patches from the input for each output, then flattens and
column-stacks them into a matrix JXK ∈RCinK1K2×O1O2, called the unfolded input (or im2col).
Multiplying a matrix view W ∈RCout×CinK1K2 of the kernel onto the unfolded input then yields a
matrix view Y of Y (the vector of ones, 1O1O2, copies the bias for each channel),"
CONVOLUTION,0.05577299412915851,"Y = W JXK + b 1⊤
O1O2 ∈RCout×O1O2 .
(1)"
CONVOLUTION,0.05675146771037182,"We can also view convolution as an affine map of the flattened input x ∈RCinI1I2 into a vector view
y of Y with a Toeplitz-structured matrix A(W) ∈RCoutO1O2×CinI1I2,"
CONVOLUTION,0.057729941291585124,"y = A(W)x + b ⊗1O1O2 ∈RCoutO1O2 .
(2)"
CONVOLUTION,0.05870841487279843,"This perspective is uncommon in code, but used in theoretical works [e.g. 65] as it highlights the
similarity between convolutions and dense layers."
TENSOR MULTIPLICATION,0.05968688845401174,"2.2
Tensor Multiplication"
TENSOR MULTIPLICATION,0.060665362035225046,"Tensor multiplication unifies outer (Kronecker), element-wise (Hadamard), and inner products and
uses the input-output index relation to infer the multiplication type. We start with the binary case,
then generalize to more inputs: Consider A, B, C whose index names are described by the index
tuples S1, S2, S3 where S3 ⊆(S1 ∪S2) (converting tuples to sets if needed). Any product of A and
B can be described by the multiplication operator ∗(S1,S2,S3) with"
TENSOR MULTIPLICATION,0.06164383561643835,"C = ∗(S1,S2,S3)(A, B)
⇔
[C]S3 = P"
TENSOR MULTIPLICATION,0.06262230919765166,"(S1∪S2)\S3[A]S1[B]S2
(3)"
TENSOR MULTIPLICATION,0.06360078277886497,"summing over indices that are not present in the output. E.g., for two matrices A, B, their product is
AB = ∗((i,j),(j,k),(i,k))(A, B) (see §H.2), their Hadamard product A⊙B = ∗((i,j),(i,j),(i,j))(A, B),
and their Kronecker product A ⊗B = ∗((i,j),(k,l),((i,k),(j,l)))(A, B). Libraries support this func-
tionality via einsum, which takes a string encoding of S1, S2, S3, followed by A, B. It also accepts
longer sequences A1, . . . , AN with index tuples S1, S2, . . . , SN and output index tuple SN+1,"
TENSOR MULTIPLICATION,0.06457925636007827,"AN+1 = ∗(S1,...,SN,SN+1)(A1, . . . , AN) ⇔[AN+1]SN+1=
X"
TENSOR MULTIPLICATION,0.06555772994129158,"(
SN
n=1 Sn)\SN+1 N
Y"
TENSOR MULTIPLICATION,0.06653620352250489,"n=1
[An]Sn ! .
(4)"
TENSOR MULTIPLICATION,0.0675146771037182,"1We prefer I1, I2 over the more common choice H, W to simplify the generalization to higher dimensions."
TENSOR MULTIPLICATION,0.0684931506849315,"Table 1: Contraction expressions of operations related to 2d convolution. They include batching
and channel groups, which are standard features in implementations. We describe each operation
by a tuple of input tensors and a contraction string that uses the einops library’s syntax [59] which
can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is
suppressed for brevity. See §B for visualizations and Table B3 for more operations."
TENSOR MULTIPLICATION,0.06947162426614481,"Operation
Operands
Contraction string (einops [59] convention)"
TENSOR MULTIPLICATION,0.07045009784735812,"Conv. (no bias)
X, Π(1), Π(2), W
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, (g c_out) c_in k1 k2
-> n (g c_out) o1 o2""
Unf. input (im2col)
X, Π(1), Π(2)
""n c_in i1 i2, i1 o1 k1, i2 o2 k2 -> n (c_in k1 k2) (o1 o2)""
Unf. kernel (Toeplitz)
Π(1), Π(2), W
""i1 o1 k1, i2 o2 k2, c_out c_in k1 k2
-> (c_out o1 o2) (c_in i1 i2)"""
TENSOR MULTIPLICATION,0.07142857142857142,"Weight VJP
X, Π(1), Π(2), V(Y)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_out) o1 o2
-> (g c_out) c_in k1 k2""
Input VJP (tr. conv.)
W, Π(1), Π(2), V(Y)
""(g c_out) c_in k1 k2, i1 o1 k1, i2 o2 k2, n (g c_out) o1 o2
-> n (g c_in) i1 i2"""
TENSOR MULTIPLICATION,0.07240704500978473,"KFC/KFAC-expand
X, Π(1), Π(2), X, Π(1), Π(2) ""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_in_) i1 i2,
i1 o1 k1_, i2 o2 k2_ -> g (c_in k1 k2) (c_in_ k1_ k2_)""
KFAC-reduce
X, Π(1), Π(2), X, Π(1), Π(2) ""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_in_) i1 i2,
i1 o1_ k1_, i2 o2_ k2_ -> g (c k1 k2) (c_ k1_ k2_)"""
TENSOR MULTIPLICATION,0.07338551859099804,"Binary and N-ary tensor multiplication are commutative: We can simultaneously permute operands
and their index tuples without changing the result,"
TENSOR MULTIPLICATION,0.07436399217221135,"∗(S1,S2,S3)(A, B) = ∗(S2,S1,S3)(B, A) ,
∗(.,Si,.,Sj,.)(., Ai, ., Aj, .)
= ∗(.,Sj,.,Si,.)(., Aj, ., Ai, .)"
TENSOR MULTIPLICATION,0.07534246575342465,"They are also associative, i.e. we can multiply operands in any order. However, the notation becomes
involved as it requires additional set arithmetic to detect summable indices (see §H.1 for an example)."
TENSOR NETWORKS & CONVOLUTION,0.07632093933463796,"2.3
Tensor Networks & Convolution"
TENSOR NETWORKS & CONVOLUTION,0.07729941291585127,"A simpler way to understand tensor multiplications is via diagrams developed by e.g. Penrose [53].
Rank-K tensors are represented by nodes with K legs labelled by the index’s name2.
a
i denotes
a vector a,
B
i
j a matrix B, and
C
i
jk a rank-3 tensor C. A Kronecker delta [δ]i,j = δi,j is
simply a line,
δ
i
j
=
I
i
j
=j
i. Multiplications are indicated by connections between legs. For
inner multiplication, we join the legs of the involved indices, e.g. the matrix multiplication diagram
is
AB
i
k =
A
B
i
k
j
. Element-wise multiplication is similar, but with a leg sticking out. The
Hadamard and Kronecker product diagrams are"
TENSOR NETWORKS & CONVOLUTION,0.07827788649706457,"A ⊙B
i
j = A"
TENSOR NETWORKS & CONVOLUTION,0.07925636007827788,"B
i
j,
A ⊗B
(i, k)
(j, l) = A"
TENSOR NETWORKS & CONVOLUTION,0.08023483365949119,"B
(i, k)
(j, l)
i k j"
TENSOR NETWORKS & CONVOLUTION,0.0812133072407045,"l
.
(5)"
TENSOR NETWORKS & CONVOLUTION,0.0821917808219178,"Note that the outer tensor product is a rank-4 tensor and must be reshaped (indicated by black
triangles3) into a matrix. This syntax allows for extracting and embedding tensors along diagonals;
e.g. taking a matrix diagonal, diag(A)
i=
A
i, or forming a diagonal matrix,
diag(a)
i
i
=
a
i
i
; and
generalizes to larger diagonal blocks (§B). In the following, we stick to the simplest case to avoid the
more advanced syntax. However, it shows the expressive power of TNs and is required to support
common features of convolutions like channel groups (known as separable convolutions)."
TENSOR NETWORKS & CONVOLUTION,0.08317025440313111,"Application to convolution:
We define a binary tensor P ∈{0, 1}I1×O1×K1×I2×O2×K2 which
represents the connectivity pattern between input, output, and kernel. Pi1,o1,k1,i2,o2,k2 is 1 if input
locations (i1, i2) overlap with kernel positions (k1, k2) when computing output locations (o1, o2)
and 0 otherwise. The spatial couplings are independent along each dimension, hence P decomposes
into Pi1,o1,k1,i2,o2,k2 = Π(1)
i1,o1,k1Π(2)
i2,o2,k2 where the index pattern tensor Π(j) ∈{0, 1}Ij×Oj×Kj
encodes the connectivity along dimension j. With that, one obtains"
TENSOR NETWORKS & CONVOLUTION,0.08414872798434442,"Ycout,o1,o2 = bcout + Cin
X cin=1"
TENSOR NETWORKS & CONVOLUTION,0.08512720156555773,"I1,I2
X"
TENSOR NETWORKS & CONVOLUTION,0.08610567514677103,"i1,i2=1"
TENSOR NETWORKS & CONVOLUTION,0.08708414872798434,"K1,K2
X"
TENSOR NETWORKS & CONVOLUTION,0.08806262230919765,"k1,k2=1
Xcin,i1,i2Π(1)
i1,o1,k1Π(2)
i2,o2,k2Wcout,cin,k1,k2"
TENSOR NETWORKS & CONVOLUTION,0.08904109589041095,"Without bias, this translates into the diagram in Figure 2a."
TENSOR NETWORKS & CONVOLUTION,0.09001956947162426,"2We use identical shapes for all tensors. Leg orientation does not assign properties like co-/contra-variance.
3Reshape can be seen as multiplication with a one-hot tensor, but we decided to use a separate symbol to
emphasize it merely serves for re-interpretation and does not cause much computation. Π(1) Π(2) X
W
I i1 i2 k1 k2"
TENSOR NETWORKS & CONVOLUTION,0.09099804305283757,"cin
c′
out o1 o2 c′
in"
TENSOR NETWORKS & CONVOLUTION,0.09197651663405088,"k′
2 k′
1 cout"
TENSOR NETWORKS & CONVOLUTION,0.09295499021526418,(a) Differentiation/weight Jac. Π(1) Π(2) W k1 k2
TENSOR NETWORKS & CONVOLUTION,0.09393346379647749,"c′
in
cout i′
1 i′
2 o1 o2"
TENSOR NETWORKS & CONVOLUTION,0.0949119373776908,(b) Input Jac. Π(1) Π(2)
TENSOR NETWORKS & CONVOLUTION,0.0958904109589041,"X
V(Y) i1 i2 c′
in k′
1 k′
2 o1 o2"
TENSOR NETWORKS & CONVOLUTION,0.09686888454011741,"c′
out"
TENSOR NETWORKS & CONVOLUTION,0.09784735812133072,(c) Weight VJP Π(1) Π(2)
TENSOR NETWORKS & CONVOLUTION,0.09882583170254403,"V(Y)
W k1 k2 c′
in i′
1 i′
2 o1 o2 cout"
TENSOR NETWORKS & CONVOLUTION,0.09980430528375733,(d) Input VJP/tr. conv.
TENSOR NETWORKS & CONVOLUTION,0.10078277886497064,"Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is
cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the
Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians
are shaded, only their contraction with V(Y) is highlighted."
TNS FOR CONVOLUTION OPERATIONS,0.10176125244618395,"3
TNs for Convolution Operations"
TNS FOR CONVOLUTION OPERATIONS,0.10273972602739725,"We now demonstrate the elegance of TNs for computing derivatives (§3.1), autodiff operations (§3.2),
and approximate second-order information (§3.3) by graphical manipulation. For simplicity, we
exclude batching (vmap-ing like in JAX [8]) and channel groups, and provide the diagrams with full
support in §B. Table 1 summarizes our derivations (with batching and groups). As a warm-up, we
identify the unfolded input and kernel from the matrix-multiplication view (Equations (1) and (2)).
They follow by contracting the index patterns with either the input or kernel (Figures 2b and 2c),"
TNS FOR CONVOLUTION OPERATIONS,0.10371819960861056,"[JXK](cin,k1,k2),(o1,o1) =
X"
TNS FOR CONVOLUTION OPERATIONS,0.10469667318982387,"i1,i2
Xcin,i1,i2Π(1)
i1,o1,k1Π(2)
i2,o2,k2 ,"
TNS FOR CONVOLUTION OPERATIONS,0.10567514677103718,"[A(W)](cout,o1,o2),(cin,i1,i2) =
X"
TNS FOR CONVOLUTION OPERATIONS,0.10665362035225048,"k1,k2
Π(1)
i1,o1,k1Π(2)
i2,o2,k2Wcout,cin,k1,k2 ."
TENSOR NETWORK DIFFERENTIATION,0.10763209393346379,"3.1
Tensor Network Differentiation"
TENSOR NETWORK DIFFERENTIATION,0.1086105675146771,"Derivatives play a crucial role in theoretical and practical ML. First, we show that differentiating a TN
diagram amounts to a simple graphical manipulation. Then, we derive the Jacobians of convolution.
Consider an arbitrary TN represented by the tensor multiplication from Equation (4). The Jacobian
tensor [JAjAN+1]SN+1,S′
j = ∂[AN+1]SN+1/∂[Aj]S′
j w.r.t. an input Aj collects all partial derivatives and
is addressed through indices Sn+1 × S′
j with S′
j an independent copy of Sj. Assume that Aj only
enters once in the tensor multiplication. Then, taking the derivative of Equation (4) w.r.t. [Aj]S′
j
simply replaces the tensor by a Kronecker delta δSj,S′
j,"
TENSOR NETWORK DIFFERENTIATION,0.1095890410958904,∂[AN+1]SN+1
TENSOR NETWORK DIFFERENTIATION,0.11056751467710371,"∂[Aj]S′
j
=
X"
TENSOR NETWORK DIFFERENTIATION,0.11154598825831702,"(
SN
n=1Sn)\Sn+1
[A1]S1 · · · [Aj−1]Sj−1
Y"
TENSOR NETWORK DIFFERENTIATION,0.11252446183953033,"i∈Sj
δi,i′[Aj+1]Sj+1 · · · [AN]SN
(6)"
TENSOR NETWORK DIFFERENTIATION,0.11350293542074363,"If an index i ∈Sj is summed, i ̸∈Sn+1, we can sum the Kronecker delta δi,i′, effectively replacing
all occurrences of i by i′. If instead i is part of the output index, i ∈Sn+1, the Kronecker delta
remains part of the Jacobian and imposes structure. Figure 3a illustrates this process in diagrams for
differentiating a convolution w.r.t. its kernel. Equation (6) amounts to cutting out the argument of
differentiation and assigning new indices to the resulting open legs. For the weight Jacobian JWY,
this introduces structure: If we re-interpret the two sub-diagrams in Figure 3a as matrices, compare
with the Kronecker diagram from Equation (5) and use Figure 2b, we find JXK⊤⊗ICout for the
Jacobian’s matrix view [e.g. 16]. Figure 3b shows the input Jacobian JXY which is a tensor view of
A(W), as expected from the matrix-vector perspective of Equation (2)."
TENSOR NETWORK DIFFERENTIATION,0.11448140900195694,"Differentiating a TN is more convenient than using matrix calculus [44] as it amounts to a simple
graphical manipulation, does not rely on a flattening convention, and therefore preserves the full
index structure. The resulting TN can still be translated back to matrix language, if desired. It also
simplifies the computation of higher-order derivatives (e.g. ∂2Y/∂W∂X), since differentiation yields
another TN and can thus be repeated. If a tensor occurs more than once in a TN, the product rule
applies and the derivative is a sum of TNs with one occurrence removed."
TENSOR NETWORK DIFFERENTIATION,0.11545988258317025,"Π(1)
Π(1)"
TENSOR NETWORK DIFFERENTIATION,0.11643835616438356,"Π(2)
Π(2) X
X i1"
TENSOR NETWORK DIFFERENTIATION,0.11741682974559686,"i2
i′
2
i′
1 o1 o2"
TENSOR NETWORK DIFFERENTIATION,0.11839530332681017,"k1
k′
1"
TENSOR NETWORK DIFFERENTIATION,0.11937377690802348,"k2
k′
2"
TENSOR NETWORK DIFFERENTIATION,0.12035225048923678,"cin
c′
in"
TENSOR NETWORK DIFFERENTIATION,0.12133072407045009,"(a) KFAC-expand 1
1"
TENSOR NETWORK DIFFERENTIATION,0.1223091976516634,"Π(1)
Π(1) 1
1"
TENSOR NETWORK DIFFERENTIATION,0.1232876712328767,"Π(2)
Π(2) X
X i1"
TENSOR NETWORK DIFFERENTIATION,0.12426614481409001,"i2
i′
2 i′
1 o1 o2 o′
1 o′
2"
TENSOR NETWORK DIFFERENTIATION,0.12524461839530332,"k1
k′
1"
TENSOR NETWORK DIFFERENTIATION,0.12622309197651663,"k2
k′
2"
TENSOR NETWORK DIFFERENTIATION,0.12720156555772993,"cin
c′
in"
TENSOR NETWORK DIFFERENTIATION,0.12818003913894324,(b) KFAC-reduce
TENSOR NETWORK DIFFERENTIATION,0.12915851272015655,"Figure 4: TNs of input-based Kronecker
factors for KFAC approximations of the
Fisher/GGN (no batching, no groups).
The unfolded input is shaded, only addi-
tional contractions are highlighted. (a)
Ω(KFC/KFAC-expand) from Grosse
& Martens [27], (b) ˆΩ(KFAC-reduce)
from Eschenhagen et al. [23] (vectors of
ones effectively amount to sums)."
AUTODIFF & CONNECTIONS TO TRANSPOSE CONVOLUTION,0.13013698630136986,"3.2
Autodiff & Connections to Transpose Convolution"
AUTODIFF & CONNECTIONS TO TRANSPOSE CONVOLUTION,0.13111545988258316,"Although Jacobians are useful, crucial routines for autodiff are vector-Jacobian and Jacobian-vector
products (VJPs, JVPs). Both are simple to realize with TNs due to access to full Jacobians. VJPs are
used in backpropagation to pull back a tensor V(Y) ∈RCout×O1×O2 from the output to the input or
weight space. The VJP results V(X) ∈RCin×I1×I2 and V(W) ∈RCout×Cin×K1×K2 are"
AUTODIFF & CONNECTIONS TO TRANSPOSE CONVOLUTION,0.13209393346379647,"V (X)
c′
in,i′
1,i′
2 =
X"
AUTODIFF & CONNECTIONS TO TRANSPOSE CONVOLUTION,0.13307240704500978,"cout,o1,o2
V (Y)
cout,o1,o2
∂Ycout,o1,o2"
AUTODIFF & CONNECTIONS TO TRANSPOSE CONVOLUTION,0.13405088062622308,"∂Xc′
in,i′
1,i′
2
,
V (W)
c′
out,c′
in,k′
1,k′
2 =
X"
AUTODIFF & CONNECTIONS TO TRANSPOSE CONVOLUTION,0.1350293542074364,"cout,o1,o2
V (Y)
cout,o1,o2
∂Ycout,o1,o2
∂Wc′
out,c′
in,k′
1,k′
2
."
AUTODIFF & CONNECTIONS TO TRANSPOSE CONVOLUTION,0.1360078277886497,"Both are simply new TNs constructed from contracting the vector with the respective Jacobian,
see Figures 3c and 3d (VJPs are analogous). The input VJP is often used to define transpose
convolution [21]. In the matrix-multiplication perspective (Equation (2)), this operation is defined
relative to a convolution with kernel W by multiplication with A(W)⊤, i.e. using the same connectivity
pattern but mapping from the convolution’s output to input space. The TN in Figure 3d makes this
sharing explicit and cleanly defines transpose convolution.4"
KRONECKER-FACTORED APPROXIMATE CURVATURE,0.136986301369863,"3.3
Kronecker-factored Approximate Curvature"
KRONECKER-FACTORED APPROXIMATE CURVATURE,0.1379647749510763,"The Jacobian diagrams allow us to construct the TNs of second-order information like the Fisher/gen-
eralized Gauss-Newton (GGN) matrix and sub-tensors like its diagonal (§C). Here, we focus on the
popular Kronecker-factored approximation of the GGN [47, 27, 23, 48] whose input-based Kronecker
factor relies on the unfolded input JXK which requires large memory. State-of-the-art libraries that
provide access to KFAC [17, 51] also use this approach. Using TNs, we can often avoid expanding
JXK explicitly and save memory. Here, we describe the existing KFAC approximations and their TNs
(see §5.1 for their run time evaluation)."
KRONECKER-FACTORED APPROXIMATE CURVATURE,0.13894324853228962,"KFC (KFAC-expand): Grosse & Martens [27] introduce a Kronecker approximation for the ker-
nel’s GGN, G ≈Ω⊗Γ where Γ ∈RCout×Cout and the input-based factor Ω= JXKJXK⊤∈
RCinK1K2×CinK1K2 (Figure 4a), the unfolded input’s self-inner product (averaged over a batch)."
KRONECKER-FACTORED APPROXIMATE CURVATURE,0.13992172211350293,"KFAC-reduce: Eschenhagen et al. [23] generalized KFAC to graph neural networks and transformers
based on the concept of weight sharing, also present in convolutions. They identify two approxi-
mations: KFAC-expand and KFAC-reduce. The former corresponds to KFC [27]. The latter shows
similar performance in downstream tasks, but is cheaper to compute. It relies on the column-averaged
unfolded input, i.e. the average over all patches sharing the same weights. KFAC-reduce approximates
G ≈ˆΩ⊗ˆΓ with ˆΓ ∈RCout×Cout and ˆΩ= 1/(O1O2)21⊤
O1O2JXK(1⊤
O1O2JXK)⊤∈RCinK1K2×CinK1K2
(Figure 4b; averaged over a batch). For convolutions, this is arguably a ‘more natural’ approximation
as it becomes exact in certain limits [23], in contrast to the expand approximation."
KRONECKER-FACTORED APPROXIMATE CURVATURE,0.14090019569471623,"KFAC for transpose convolution: Our approach enables us to derive KFAC for transpose convo-
lutions. We are not aware of previous works doing so. This seems surprising because, similar to
§2.1, transpose convolution can be seen as matrix multiplication between the kernel and an unfolded
input. From this formulation we can immediately obtain KFAC through the weight sharing view
of Eschenhagen et al. [23]. The Kronecker factor requires unfolding the input similar to im2col,
but for transpose convolutions. This operation is currently not provided by ML libraries. We can
overcome this limitation, express the unfolding operation as TN, and—for the first time—establish
KFAC (expand and reduce) for transpose convolutions (see §B.4 for details)."
STANDALONE IMPLEMENTATIONS OF TRANSPOSE CONVOLUTION REQUIRE ANOTHER PARAMETER TO UNAMBIGUOUSLY RECON-,0.14187866927592954,"4Standalone implementations of transpose convolution require another parameter to unambiguously recon-
struct the convolution’s input dimension (see §D for how to compute Π in this case)."
STANDALONE IMPLEMENTATIONS OF TRANSPOSE CONVOLUTION REQUIRE ANOTHER PARAMETER TO UNAMBIGUOUSLY RECON-,0.14285714285714285,"Π(I, K, S, P, D)
=
Π(I, K, D, P, S)
i o k
i k o"
STANDALONE IMPLEMENTATIONS OF TRANSPOSE CONVOLUTION REQUIRE ANOTHER PARAMETER TO UNAMBIGUOUSLY RECON-,0.14383561643835616,(a) Kernel-output swap
STANDALONE IMPLEMENTATIONS OF TRANSPOSE CONVOLUTION REQUIRE ANOTHER PARAMETER TO UNAMBIGUOUSLY RECON-,0.14481409001956946,"Π(I, K, K, 0, 1)
=
Π
i o k
i o k"
STANDALONE IMPLEMENTATIONS OF TRANSPOSE CONVOLUTION REQUIRE ANOTHER PARAMETER TO UNAMBIGUOUSLY RECON-,0.14579256360078277,(b) Dense convolution
STANDALONE IMPLEMENTATIONS OF TRANSPOSE CONVOLUTION REQUIRE ANOTHER PARAMETER TO UNAMBIGUOUSLY RECON-,0.14677103718199608,"V
Π(I, K, S > K, 0, 1)
=
˜V
Π(IK/S, K, K, 0, 1)
i
i′ o k o k"
STANDALONE IMPLEMENTATIONS OF TRANSPOSE CONVOLUTION REQUIRE ANOTHER PARAMETER TO UNAMBIGUOUSLY RECON-,0.14774951076320939,(c) Down-sampling convolution
STANDALONE IMPLEMENTATIONS OF TRANSPOSE CONVOLUTION REQUIRE ANOTHER PARAMETER TO UNAMBIGUOUSLY RECON-,0.1487279843444227,"Figure 5: TN illus-
trations of index pat-
tern simplifications
and transformations.
See § D.3 for the
math formulation."
TN SIMPLIFICATIONS & IMPLEMENTATION,0.149706457925636,"4
TN Simplifications & Implementation"
TN SIMPLIFICATIONS & IMPLEMENTATION,0.1506849315068493,"Many convolutions in real-world CNNs use structured connectivity patterns that allow for simplifica-
tions which we describe here along with implementation aspects."
INDEX PATTERN STRUCTURE & SIMPLIFICATIONS,0.15166340508806261,"4.1
Index Pattern Structure & Simplifications"
INDEX PATTERN STRUCTURE & SIMPLIFICATIONS,0.15264187866927592,"The index pattern Π encodes the connectivity of a convolution and depends on its hyper-parameters.
Along one dimension, Π = Π(I, K, S, P, D) with input size I, kernel size K, stride S, padding
P, and dilation D. We provide pseudo-code for computing Π in §D which is easy to implement
efficiently with standard functions from any numerical library (Algorithm D1). Its entries are"
INDEX PATTERN STRUCTURE & SIMPLIFICATIONS,0.15362035225048923,"[Π(I, K, S, P, D)]i,o,k = δi,1+(k−1)D+(o−1)S−P ,
(7)"
INDEX PATTERN STRUCTURE & SIMPLIFICATIONS,0.15459882583170254,"with i = 1, . . . , I, o = 1, . . . , O, k = 1, . . . , K and output size O(I, K, S, P, D) = 1 +
⌊(I+2P −(K+(K−1)(D−1)))/S⌋. Since Π is binary and has size linear in I, O, K, it is cheap to pre-
compute and cache. The index pattern’s symmetries allow for re-wiring a TN. For instance, the
symmetry of (k, D) and (o, S) in Equation (7) and O(I, K, S, P, D) permits a kernel-output swap,
exchanging the role of kernel and output dimension (Figure 5a). Rochette et al. [58] used this to
phrase the per-example gradient computation (Figure 3c) as convolution."
INDEX PATTERN STRUCTURE & SIMPLIFICATIONS,0.15557729941291584,"For many convolutions of real-world CNNs (see §E for a hyper-parameter study) the index pattern
possesses structure that simplifies its contraction with other tensors into either smaller contractions or
reshapes: Dense convolutions use a shared kernel size and stride, and thus process non-overlapping
adjacent tiles of the input. Their index pattern’s action can be expressed as a cheap reshape (Fig-
ure 5b). Such convolutions are common in DenseNets [33], MobileNets [31, 60], ResNets [30],
and ConvNeXts [42]. InceptionV3 [69] has 2d mixed-dense convolutions that are dense along one
dimension. Down-sampling convolutions use a larger stride than kernel size, hence only process a
sub-set of their input, and are used in ResNet18 [30], ResNext101 [72], and WideResNet101 [73].
Their pattern contracts with a tensor V like that of a dense convolution with a sub-tensor ˜V (Figure 5c).
§5.1 shows that those simplifications accelerate computations."
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.15655577299412915,"4.2
Practical Benefits of the TN Abstraction & Limitations for Convolutions"
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.15753424657534246,"Contraction order optimization: There exist various orders in which to carry out the summations
in a TN and their performance can vary by orders of magnitude. One extreme approach is to carry
out all summations via nested for-loops. This so-called Feynman path integral algorithm requires
little memory, but many FLOPS since it does not re-cycle intermediate results. The other extreme is
sequential pair-wise contraction. This builds up intermediate results and can greatly reduce FLOPS.
The schedule is represented by a binary tree, but the underlying search is in general at least #P-
hard [14]. Fortunately, there exist heuristics to find high-quality contraction trees for TNs with
hundreds of tensors [32, 25, 13], implemented in packages like opt_einsum [66]."
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.15851272015655576,"Index slicing: A common problem with high-quality schedules is that intermediates exceed memory.
Dynamic slicing [32] (e.g. cotengra [25]) is a simple method to decompose a contraction until it
becomes feasible by breaking it up into smaller identical sub-tasks whose aggregation adds a small
overhead. This enables peak memory reduction and distribution."
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.15949119373776907,"Sparsity: Π is sparse as only a small fraction of the input contributes to an output element. For
a convolution with stride S < K and default parameters (P = 0, D = 1), for fixed output and
kernel indices k, o, there is exactly one non-zero entry in [Π]:,o,k. Hence nnz(Π) = OK, which
corresponds to a sparsity of 1/I. Padding leads to more kernel elements that do not contribute to an"
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.16046966731898238,"Forward
Input VJP
Weight VJP
KFC
KFAC-red. 3 1 0.3"
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.16144814090019569,TN versus PT (logarithmic)
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.162426614481409,"Figure 6:
Run time ratios of TN
(w/o simplifications) vs. standard im-
plementation for dense convolutions of
9 CNNs. With simplifications, convo-
lution and input VJP achieve median
ratios slightly above 1, and the TN im-
plementation is faster for weight VJP,
KFC & KFAC-reduce. The code in Fig-
ure 1 corresponds to default, TN, and
simplified TN KFC implementation."
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.1634050880626223,"output pixel, and therefore a sparser Π. For down-sampling and dense convolutions, we showed
how Π’s algebraic structure allows to simplify its contraction. However, if that is not possible, Π
contains explicit zeros that add unnecessary FLOPS. One way to circumvent this is to match a TN
with that of an operation with efficient implementation (like im2col, (transpose) convolution) using
transformations like the kernel-output swap or by introducing identity tensors to complete a template,
as done in Rochette et al. [58], Dangel [15] for per-sample gradients and im2col."
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.1643835616438356,"Approximate contraction & structured dropout: TNs offer a principled approach for stochas-
tic approximation via Monte-Carlo estimation to save memory and run time at the cost of accu-
racy. The basic idea is best explained on a matrix product C := AB = PN
n=1 [A]:,n [B]n,: with
A ∈RI×N, B ∈RN,O. To approximate the sum, we introduce a distribution over n’s range, then
use column-row-sampling [CRS, 1] to form an unbiased Monte-Carlo approximation with sam-
pled indices, which only requires the sub-matrices with active column-row pairs. Bernoulli-CRS
samples without replacement by assigning a Bernoulli random variable Bernoulli(πn) with prob-
ability πn for column-row pair n to be included in the contraction. The Bernoulli estimator is
˜C := PN
n=1 zn/πn [A]n,: [B]n,: with zn ∼Bernoulli(πn). With a shared keep probability, πn := p,
this yields the unbiased estimator C′ = 1/p P"
PRACTICAL BENEFITS OF THE TN ABSTRACTION & LIMITATIONS FOR CONVOLUTIONS,0.16536203522504891,"n=1,...,N A′B′ where A′ = AK and B′ = KB with
K = diag(z1, . . . , zN) are the sub-matrices of A, B containing the active column-row pairs. CRS
applies to a single contraction. For TNs with multiple sums, we can apply it individually, and also
impose a distribution over the result indices, which computes a (scaled) sub-tensor."
EXPERIMENTS,0.16634050880626222,"5
Experiments"
EXPERIMENTS,0.16731898238747553,"Here, we demonstrate computational benefits of TNs for less standard routines of second-order
methods and showcase their flexibility to perform stochastic autodiff in novel ways."
RUN TIME EVALUATION,0.16829745596868884,"5.1
Run Time Evaluation"
RUN TIME EVALUATION,0.16927592954990214,"We implement the presented TNs’ contraction strings and operands5 in PyTorch [52]. The simplifica-
tions from §4 can be applied on top and yield a modified einsum expression. To find a contraction
schedule, we use opt_einsum [66] with default settings. We extract the unique convolutions of 9
architectures for ImageNet and smaller data sets, then compare some operations from Table 1 with
their standard implementation on an Nvidia Tesla T4 GPU (16 GB); see §F for all details. Due to
space constraints, we highlight important insights here and provide references to the corresponding
material in the appendix. In general, the performance gap between standard and TN implementation
decreases the less common an operation is (Figure F17); from forward pass (inference & training), to
VJPs (training), to KFAC (training with a second-order method). This is intuitive as more frequently
used routines have been optimized more aggressively."
RUN TIME EVALUATION,0.17025440313111545,"Impact of simplifications: While general convolutions remain unaffected (Figure F18d) when
applying the transformations of §4, mixed dense, dense, and down-sampling convolutions consistently
enjoy significant run time improvements (Figures F18a to F18c). As an example, we show the
performance comparison for dense convolutions in Figure 6: The performance ratio’s median between
TN and standard forward and input VJP is close to 1, that is both require almost the same time. In the
median, the TN even outperforms PyTorch’s highly optimized weight VJP, also for down-sampling"
RUN TIME EVALUATION,0.17123287671232876,"5einsum does not yet support index un-grouping, so we must reshape manually before and after."
RUN TIME EVALUATION,0.17221135029354206,"100
101
102
103 100 101 102 103"
RUN TIME EVALUATION,0.17318982387475537,Auxiliary memory PT [MiB]
RUN TIME EVALUATION,0.17416829745596868,Aux. mem. TN [MiB]
RUN TIME EVALUATION,0.175146771037182,"Figure 7:
Extra memory used by
the standard versus our TN imple-
mentation (simplifications enabled) of
KFAC-reduce. Each point represents
a convolution from 9 CNNs, clipped
below by 1 MiB. TNs consistently use
less memory than the standard imple-
mentation (one exception), and often
no extra memory at all. We observe
memory savings up to 3 GiB."
RUN TIME EVALUATION,0.1761252446183953,"convolutions (Figure F21). For KFC, the median performance ratios are well below 1 for dense,
mixed dense & sub-sampling convolutions (Figure F22)."
RUN TIME EVALUATION,0.1771037181996086,"KFAC-reduce: For all convolution types, the TN implementation achieves its largest improvements
for ˆΩand consistently outperforms the PyTorch implementation in the median when simplifications
are enabled (Figure F23). The standard implementation unfolds the input, takes the row-average,
then forms its outer product. The TN does not need to expand JXK in memory and instead averages
the index pattern tensors, which reduces peak memory and run time. We observe performance ratios
down to 0.22 x (speed-ups up to ≈4.5 x, Table F9) and consistently lower memory consumption
with savings up to 3 GiB (Figure 7). Hence, our approach not only significantly reduces the overhead
of 2nd-order optimizers based on KFAC-reduce, but also allows them to operate on larger batches
without exceeding memory (Eschenhagen et al. [23] specifically mention memory as important
limitation of their method). Other examples for KFAC algorithms where computing the input-based
Kronecker factor adds significant time and memory overhead are that of Petersen et al. [54], Benzing
[5] which only use Ω(setting Γ ∝I), or Lin et al. [41, 40] which remove matrix inversion."
RUN TIME EVALUATION,0.1780821917808219,"Downstream improvements with KFAC-reduce: To demonstrate the speed-ups of KFAC-reduce
in practical algorithms, we apply our work to the SINGD optimizer [41] and benchmark the impact
of our TN implementation on its memory and run time in comparison to SGD without momentum.
Concretely, we investigate SINGD with KFAC-reduce and diagonal pre-conditioners on ResNet18
and VGG19 on ImageNet-like synthetic data (3, 256, 256) using a batch size of 128. We measured
per-iteration time and peak memory on an NVIDIA A40 with 48 GiB of RAM. For SINGD, we
compare computing the Kronecker factors with the standard approach (‘SINGD’) via input unfolding
versus our TN implementation (‘SINGD+TN’). Table 2 summarizes the results."
RUN TIME EVALUATION,0.17906066536203522,"Table 2: Impact of our TN implementation on SINGD’s
run time and peak memory compared to SGD."
RUN TIME EVALUATION,0.18003913894324852,"Optimizer
Per iter. [s]
Peak mem. [GiB]"
RUN TIME EVALUATION,0.18101761252446183,ResNet18
RUN TIME EVALUATION,0.18199608610567514,"SGD
0.12 (1.0 x)
3.6 (1.0 x)
SINGD
0.19 (1.7 x)
4.5 (1.3 x)
SINGD+TN
0.16 (1.3 x)
3.6 (1.0 x) VGG19"
RUN TIME EVALUATION,0.18297455968688844,"SGD
0.69 (1.0 x)
14 (1.0 x)
SINGD
1.0 (1.5 x)
32 (2.3 x)
SINGD+TN
0.80 (1.2 x)
16 (1.1 x)"
RUN TIME EVALUATION,0.18395303326810175,"On both nets, our TN implementation halves
SINGD’s run time, and almost completely
eliminates the memory, overhead compared
to SGD. On VGG19, it dramatically lowers
the memory overhead, cutting it down by
a factor of 2 from 32 GiB to 16 GiB. This
enables using larger batches or more fre-
quently updating the pre-conditioner, under-
lining the utility of our approach for reduc-
ing the computational gap between approx-
imate second-order and first-order methods."
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.18493150684931506,"5.2
Randomized Autodiff via Approximate Contraction"
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.18590998043052837,"CRS is an alternative to checkpointing [26] to lower memory consumption of backpropagation [50,
11, 1]. Here, we focus on unbiased gradient approximations by applying the exact forward pass, but
CRS when computing the weight VJP, which requires storing a sub-tensor of X. For convolutions,
the approaches of existing works are limited by the supported functionality of ML libraries. Adelman
et al. [1] restrict to sampling X along cin, which eliminates many gradient entries as the index is part
of the gradient. The randomized gradient would thus only train a sub-tensor of the kernel per step.
Oktay et al. [50], Chen et al. [11] apply unstructured dropout to X, store it in sparse form, and restore
the sparsified tensor during the backward pass. This reduces memory, but not computation."
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.18688845401174167,"Our TN implementation is more flexible and can, for example, tackle spatial dimensions with CRS.
This reduces memory to the same extent, but also run time due to fewer contractions. Importantly, it"
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.18786692759295498,(a) Real-world data
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.1888454011741683,"0
0.2
0.4
0.6
0.8
1
0 2 4"
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.1898238747553816,Keep probability p
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.1908023483365949,Normalized error
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.1917808219178082,"channel
spatial"
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.19275929549902152,(b) Synthetic data
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.19373776908023482,"0
0.2
0.4
0.6
0.8
1
0 2 4"
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.19471624266144813,Keep probability p
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.19569471624266144,Normalized error
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.19667318982387474,"channel
spatial"
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.19765166340508805,"Figure 8: Sampling spatial axes is more effective than channels on both (a) real-world and (b)
synthetic data. We take the untrained All-CNN-C [68] for CIFAR-100 with cross-entropy loss,
disable dropout, and modify the convolutions to use a fraction p of X when computing the weight
gradient via Bernoulli-CRS. For mini-batches of size 128, we compute the deterministic gradients
for all kernels, then flatten and concatenate them into a vector g; likewise for its proxy ˆg. CRS is
described by (pcin, pi1, pi2), the keep rates along the channel and spatial dimensions. We compare
channel and spatial sampling with same memory reduction, i.e. (p, 1, 1) and (1, √p, √p). To measure
approximation quality, we use the normalized residual norm ∥g−ˆg∥2/∥g∥2 and report mean and
standard deviation of 10 different model and batch initializations."
RANDOMIZED AUTODIFF VIA APPROXIMATE CONTRACTION,0.19863013698630136,"does not zero out the gradient for entire filters. In Figure 8 we compare the gradient approximation
errors of channel and spatial sub-sampling. For the same memory reduction, spatial sub-sampling
yields a smaller approximation error on both real & synthetic data. E.g., instead of keeping 75 % of
channels, we achieve the same approximation quality using only 35 % of pixels."
RELATED WORK,0.19960861056751467,"6
Related Work"
RELATED WORK,0.20058708414872797,"Structured convolutions: We use the TN formulation of convolution from Hayashi et al. [29] who
focus on connecting kernel factorizations to existing (depth-wise separable [31, 60], factored [69],
bottleneck [30], flattened/CP decomposed, low-rank filter [67, 57, 70]) convolutions and explore new
factorizations. Our work focuses on operations related to convolutions, diagram manipulations, the
index pattern structure, and computational performance/flexibility. Structured convolutions integrate
seamlessly with our framework by replacing the kernel with its factorized TN."
RELATED WORK,0.20156555772994128,"Higher-order autodiff: ML frameworks focus on differentiating scalar-valued objectives once.
Recent works [37, 38, 43] developed a tensor calculus to compute higher-order derivatives of tensor-
valued functions and compiler optimizations through linear algebra and common sub-expression
elimination. Phrasing convolution as einsum, we allow it to be integrated into such frameworks,
benefit from their optimizations, and complement them with our convolution-specific simplifications."
CONCLUSION,0.2025440313111546,"7
Conclusion"
CONCLUSION,0.2035225048923679,"We used tensor networks (TNs), a diagrammatic representation of tensor multiplications, to simplify
convolutions and many related operations. We derived the diagrams of autodiff and less standard
routines for curvature approximations like KFAC with support for all hyper-parameters, channel
groups, batching, and generalization to arbitrary dimensions. All amount to simple einsum expres-
sions that can easily be modified—e.g. to perform stochastic backpropagation—and benefit from
under-the-hood optimizations before evaluation. We complemented those by convolution-specific
symbolic simplifications based on structure in the connectivity pattern and showed their effectiveness
to advance second-order methods. Our TN implementation accelerates the computation of KFAC
up to 4.5 x and uses significantly less memory. Beyond performance improvements, the simplifying
perspective also allowed us to formulate KFAC for transpose convolution. More broadly, our work
underlines the elegance of TNs for reasoning about tensor multiplications and function transforma-
tions (differentiation, batching, slicing, simplification) in terms of diagrams at less cognitive load
without sacrificing rigour. We believe they are a powerful tool for the ML community that will open
up new algorithmic possibilities due to their simplicity & flexibility."
CONCLUSION,0.2045009784735812,Acknowledgments and Disclosure of Funding
CONCLUSION,0.2054794520547945,"The author would like to thank Luca Thiede, Andres Fernandez Rodríguez, and Kirill Neklyudov
for providing feedback to the manuscript. Resources used in preparing this research were provided,
in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies
sponsoring the Vector Institute."
REFERENCES,0.20645792563600782,References
REFERENCES,0.20743639921722112,"[1] Adelman, M., Levy, K., Hakimi, I., and Silberstein, M. Faster neural network training with approximate
tensor operations. In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.20841487279843443,"[2] Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an
infinitely wide neural net. Advances in neural information processing systems (NeurIPS), 2019."
REFERENCES,0.20939334637964774,"[3] Bahamou, A., Goldfarb, D., and Ren, Y. A mini-block fisher method for deep neural networks. In
International Conference on Artificial Intelligence and Statistics (AISTATS), 2023."
REFERENCES,0.21037181996086105,"[4] Becker, S. and Lecun, Y. Improving the convergence of back-propagation learning with second-order
methods. 1989."
REFERENCES,0.21135029354207435,"[5] Benzing, F. Gradient descent on neurons and its link to approximate second-order optimization. In
International Conference on Machine Learning (ICML), 2022."
REFERENCES,0.21232876712328766,"[6] Biamonte, J. and Bergholm, V. Tensor networks in a nutshell, 2017."
REFERENCES,0.21330724070450097,"[7] Botev, A., Ritter, H., and Barber, D. Practical Gauss-Newton optimisation for deep learning. In International
Conference on Machine Learning (ICML), 2017."
REFERENCES,0.21428571428571427,"[8] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-Milne, S.
JAX: composable transformations of Python+NumPy programs, 2018."
REFERENCES,0.21526418786692758,"[9] Bridgeman, J. C. and Chubb, C. T. Hand-waving and interpretive dance: an introductory course on tensor
networks. Journal of Physics A: Mathematical and theoretical, 2017."
REFERENCES,0.2162426614481409,"[10] Chellapilla, K., Puri, S., and Simard, P. High performance convolutional neural networks for document
processing. In International Workshop on Frontiers in Handwriting Recognition, 2006."
REFERENCES,0.2172211350293542,"[11] Chen, J., Xu, K., Wang, Y., Cheng, Y., and Yao, A. DropIT: Dropping intermediate tensors for memory-
efficient DNN training. In International Conference on Learning Representations (ICLR), 2023."
REFERENCES,0.2181996086105675,"[12] Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., and Shelhamer, E. cudnn:
Efficient primitives for deep learning. 2014."
REFERENCES,0.2191780821917808,"[13] cuQuantum development team, T. cuQuantum SDK: A high-performance library for accelerating quantum
information science, 2023."
REFERENCES,0.22015655577299412,"[14] Damm, C., Holzer, M., and McKenzie, P. The complexity of tensor calculus. computational complexity,
2002."
REFERENCES,0.22113502935420742,"[15] Dangel, F. unfoldNd: (n=1,2,3)-dimensional unfold (im2col) and fold (col2im) in pytorch, 2021."
REFERENCES,0.22211350293542073,"[16] Dangel, F., Harmeling, S., and Hennig, P. Modular block-diagonal curvature approximations for feed-
forward architectures. In International Conference on Artificial Intelligence and Statistics (AISTATS),
2020."
REFERENCES,0.22309197651663404,"[17] Dangel, F., Kunstner, F., and Hennig, P. BackPACK: Packing more into backprop. In International
Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.22407045009784735,"[18] Dangel, F., Tatzel, L., and Hennig, P. ViViT: Curvature access through the generalized gauss-newton’s
low-rank structure. Transactions on Machine Learning Research (TMLR), 2022."
REFERENCES,0.22504892367906065,"[19] Dangel, F. J. Backpropagation beyond the gradient. 2023."
REFERENCES,0.22602739726027396,"[20] Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux -
effortless bayesian deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.22700587084148727,"[21] Dumoulin, V. and Visin, F. A guide to convolution arithmetic for deep learning. 2016."
REFERENCES,0.22798434442270057,"[22] Elsayed, M., Farrahi, H., Dangel, F., and Mahmood, A. R. Revisiting scalable hessian diagonal approx-
imations for applications in reinforcement learning. In International Conference on Machine Learning
(ICML), 2024."
REFERENCES,0.22896281800391388,"[23] Eschenhagen, R., Immer, A., Turner, R. E., Schneider, F., and Hennig, P. Kronecker-factored approximate
curvature for modern neural network architectures. In Advances in Neural Information Processing Systems
(NeurIPS), 2023."
REFERENCES,0.2299412915851272,"[24] Goldfarb, D., Ren, Y., and Bahamou, A. Practical quasi-newton methods for training deep neural networks,
2021."
REFERENCES,0.2309197651663405,"[25] Gray, J. and Kourtis, S. Hyper-optimized tensor network contraction. Quantum, 2021."
REFERENCES,0.2318982387475538,"[26] Griewank, A. and Walther, A. Evaluating derivatives: principles and techniques of algorithmic differentia-
tion. SIAM, 2008."
REFERENCES,0.2328767123287671,"[27] Grosse, R. and Martens, J. A kronecker-factored approximate Fisher matrix for convolution layers. In
International Conference on Machine Learning (ICML), 2016."
REFERENCES,0.23385518590998042,"[28] Grosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez,
E., Hubinger, E., Lukoši¯ut˙e, K., Nguyen, K., Joseph, N., McCandlish, S., Kaplan, J., and Bowman, S. R.
Studying large language model generalization with influence functions, 2023."
REFERENCES,0.23483365949119372,"[29] Hayashi, K., Yamaguchi, T., Sugawara, Y., and Maeda, S.-i. Exploring unexplored tensor network
decompositions for convolutional neural networks. In Advances in Neural Information Processing Systems
(NeurIPS), 2019."
REFERENCES,0.23581213307240703,"[30] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE conference
on computer vision and pattern recognition (CVPR), 2016."
REFERENCES,0.23679060665362034,"[31] Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H.
Mobilenets: Efficient convolutional neural networks for mobile vision applications. 2017."
REFERENCES,0.23776908023483365,"[32] Huang, C., Zhang, F., Newman, M., Ni, X., Ding, D., Cai, J., Gao, X., Wang, T., Wu, F., Zhang, G.,
et al. Efficient parallelization of tensor network contraction for simulating quantum computation. Nature
Computational Science, 2021."
REFERENCES,0.23874755381604695,"[33] Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks.
In IEEE conference on computer vision and pattern recognition (CVPR), 2017."
REFERENCES,0.23972602739726026,"[34] Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural
networks, 2020."
REFERENCES,0.24070450097847357,"[35] Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho*, K., and Geras*, K. The break-even
point on optimization trajectories of deep neural networks. In International Conference on Learning
Representations (ICLR), 2020."
REFERENCES,0.24168297455968688,"[36] Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012."
REFERENCES,0.24266144814090018,"[37] Laue, S., Mitterreiter, M., and Giesen, J. Computing higher order derivatives of matrix and tensor
expressions. Advances in Neural Information Processing Systems (NeurIPS), 2018."
REFERENCES,0.2436399217221135,"[38] Laue, S., Mitterreiter, M., and Giesen, J. A simple and efficient tensor calculus. In AAAI Conference on
Artificial Intelligence, 2020."
REFERENCES,0.2446183953033268,"[39] LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D.
Backpropagation applied to handwritten zip code recognition. Neural Computation, 1989."
REFERENCES,0.2455968688845401,"[40] Lin, W., Duruisseaux, V., Leok, M., Nielsen, F., Khan, M. E., and Schmidt, M. Simplifying momentum-
based riemannian submanifold optimization. 2023."
REFERENCES,0.2465753424657534,"[41] Lin, W., Dangel, F., Eschenhagen, R., Neklyudov, K., Kristiadi, A., Turner, R. E., and Makhzani, A.
Structured inverse-free natural gradient descent: Memory-efficient & numerically-stable KFAC. In
International Conference on Machine Learning (ICML), 2024."
REFERENCES,0.24755381604696672,"[42] Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s. IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), 2022."
REFERENCES,0.24853228962818003,"[43] Ma, L., Ye, J., and Solomonik, E. Autohoot: Automatic high-order optimization for tensors. In International
Conference on Parallel Architectures and Compilation Techniques (PACT), 2020."
REFERENCES,0.24951076320939333,"[44] Magnus, J. R. and Neudecker, H.
Matrix Differential Calculus with Applications in Statistics and
Econometrics. Probabilistics and Statistics. 1999."
REFERENCES,0.25048923679060664,"[45] Martens, J. Deep learning via Hessian-free optimization. In International Conference on Machine Learning
(ICML), 2010."
REFERENCES,0.25146771037182,"[46] Martens, J. New insights and perspectives on the natural gradient method, 2020."
REFERENCES,0.25244618395303325,"[47] Martens, J. and Grosse, R. Optimizing neural networks with Kronecker-factored approximate curvature.
In International Conference on Machine Learning (ICML), 2015."
REFERENCES,0.2534246575342466,"[48] Martens, J., Ba, J., and Johnson, M. Kronecker-factored curvature approximations for recurrent neural
networks. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.25440313111545987,"[49] Novak, R., Sohl-Dickstein, J., and Schoenholz, S. S. Fast finite width neural tangent kernel. In International
Conference on Machine Learning (ICML), 2022."
REFERENCES,0.2553816046966732,"[50] Oktay, D., McGreivy, N., Aduol, J., Beatson, A., and Adams, R. P. Randomized automatic differentiation.
In International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.2563600782778865,"[51] Osawa, K., Ishikawa, S., Yokota, R., Li, S., and Hoefler, T. Asdl: A unified interface for gradient
preconditioning in pytorch, 2023."
REFERENCES,0.2573385518590998,"[52] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy,
S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems (NeurIPS). 2019."
REFERENCES,0.2583170254403131,"[53] Penrose, R. Applications of negative dimensional tensors. Combinatorial Mathematics and its Applications,
1971."
REFERENCES,0.25929549902152643,"[54] Petersen, F., Sutter, T., Borgelt, C., Huh, D., Kuehne, H., Sun, Y., and Deussen, O. ISAAC newton:
Input-based approximate curvature for newton’s method.
In International Conference on Learning
Representations (ICLR), 2023."
REFERENCES,0.2602739726027397,"[55] Pinson, H., Lenaerts, J., and Ginis, V. Linear cnns discover the statistical structure of the dataset using only
the most dominant frequencies. In International Conference on Machine Learning (ICML), 2023."
REFERENCES,0.26125244618395305,"[56] Ren, Y., Bahamou, A., and Goldfarb, D. Kronecker-factored quasi-newton methods for deep learning,
2022."
REFERENCES,0.2622309197651663,"[57] Rigamonti, R., Sironi, A., Lepetit, V., and Fua, P. Learning separable filters. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2013."
REFERENCES,0.26320939334637966,"[58] Rochette, G., Manoel, A., and Tramel, E. W. Efficient per-example gradient computations in convolutional
neural networks, 2019."
REFERENCES,0.26418786692759294,"[59] Rogozhnikov, A. Einops: Clear and reliable tensor manipulations with einstein-like notation. In Interna-
tional Conference on Learning Representations (ICLR), 2022."
REFERENCES,0.2651663405088063,"[60] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and
linear bottlenecks. In IEEE conference on computer vision and pattern recognition (CVPR), 2018."
REFERENCES,0.26614481409001955,"[61] Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in
deep linear neural networks, 2014."
REFERENCES,0.2671232876712329,"[62] Schneider, F., Balles, L., and Hennig, P. DeepOBS: A deep learning optimizer benchmark suite. In
International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.26810176125244617,"[63] Schraudolph, N. N. Fast curvature matrix-vector products for second-order gradient descent. Neural
Computation, 2002."
REFERENCES,0.2690802348336595,"[64] Singh, S. P., Bachmann, G., and Hofmann, T. Analytic insights into structure and rank of neural network
hessian maps. Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.2700587084148728,"[65] Singh, S. P., Hofmann, T., and Schölkopf, B. The hessian perspective into the nature of convolutional
neural networks. 2023."
REFERENCES,0.2710371819960861,"[66] Smith, D. G. A. and Gray, J. opt_einsum - A python package for optimizing contraction order for
einsum-like expressions. Journal of Open Source Software (JOSS), 2018."
REFERENCES,0.2720156555772994,"[67] Smith, S. W. The scientist and engineer’s guide to digital signal processing. 1997."
REFERENCES,0.27299412915851273,"[68] Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity: The all
convolutional net, 2015."
REFERENCES,0.273972602739726,"[69] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for
computer vision. In IEEE conference on computer vision and pattern recognition (CVPR), 2016."
REFERENCES,0.27495107632093935,"[70] Tai, C., Xiao, T., Zhang, Y., Wang, X., et al. Convolutional neural networks with low-rank regularization.
2015."
REFERENCES,0.2759295499021526,"[71] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin,
I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017."
REFERENCES,0.27690802348336596,"[72] Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. Aggregated residual transformations for deep neural
networks. In IEEE conference on computer vision and pattern recognition (CVPR), 2017."
REFERENCES,0.27788649706457924,"[73] Zagoruyko, S. and Komodakis, N. Wide residual networks. 2016."
REFERENCES,0.2788649706457926,"[74] Zhang, F. A parallel tensor network contraction algorithm and its applications in quantum computation.
2020."
REFERENCES,0.27984344422700586,"Convolutions and More as Einsum: A Tensor Network
Perspective with Advances for Second-Order
Methods (Supplementary Material)"
REFERENCES,0.2808219178082192,"A Limitations
16"
REFERENCES,0.28180039138943247,"B Visual Tour of Tensor Network Operations for Convolutions
16"
REFERENCES,0.2827788649706458,"B.1
Convolution & First-order Derivatives . . . . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.2837573385518591,"B.2
Exact Second-order Information . . . . . . . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.2847358121330724,"B.3
Kronecker-factored Approximate Curvature (KFAC) for Grouped Convolutions . .
18"
REFERENCES,0.2857142857142857,"B.4
Kronecker-factored Approximate Curvature (KFAC) for Transpose Convolution . .
19"
REFERENCES,0.28669275929549903,"B.5
Further Operations & Extensive Overview . . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.2876712328767123,"C Exact Second-Order Information
24"
REFERENCES,0.28864970645792565,"D Implementation Details
25"
REFERENCES,0.2896281800391389,"D.1
Index Pattern Tensor Computation for Convolutions . . . . . . . . . . . . . . . . .
25"
REFERENCES,0.29060665362035226,"D.2
Index Pattern Tensor for Standalone Transpose Convolution . . . . . . . . . . . . .
25"
REFERENCES,0.29158512720156554,"D.3
Details on Index Pattern Simplifications . . . . . . . . . . . . . . . . . . . . . . .
25"
REFERENCES,0.2925636007827789,"E
Convolution Layer Hyper-parameter Analysis
27"
REFERENCES,0.29354207436399216,"F
Run Time Evaluation Details (GPU)
30"
REFERENCES,0.2945205479452055,"F.1
Protocol & Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30"
REFERENCES,0.29549902152641877,"F.2
Forward Pass
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32"
REFERENCES,0.2964774951076321,"F.3
Input VJP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35"
REFERENCES,0.2974559686888454,"F.4
Weight VJP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38"
REFERENCES,0.2984344422700587,"F.5
KFC Factor (KFAC-expand) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41"
REFERENCES,0.299412915851272,"F.6
KFAC-reduce Factor
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44"
REFERENCES,0.30039138943248533,"G Memory Evaluation Details (CPU)
47"
REFERENCES,0.3013698630136986,"G.1 Theoretical & Empirical Analysis for KFAC-reduce Factor . . . . . . . . . . . . .
47"
REFERENCES,0.30234833659491195,"H Miscellaneous
50"
REFERENCES,0.30332681017612523,"H.1
Example: Associativity of Tensor Multiplication . . . . . . . . . . . . . . . . . . .
50"
REFERENCES,0.30430528375733856,"H.2
Example: Matrix-matrix Multiplication as Tensor Multiplication . . . . . . . . . .
50"
REFERENCES,0.30528375733855184,"A
Limitations"
REFERENCES,0.3062622309197652,Here we comment on limitations on our approach.
REFERENCES,0.30724070450097846,"No common sub-expression elimination (CSE):
Our implementation relies on opt_einsum
which focuses on contraction order optimization. This optimization is efficient when all operands
are different. However, with multiple occurrences of operands, computing shared sub-expressions
might be an advantageous optimization approach which opt_einsum does not account for. The
second-order quantity TNs from §C and §3.3 contain such sub-expressions, for instance JXK and
1⊤
O1O2JXK in KFAC-expand and KFAC-reduce, and S(W) in the GGN quantities from Figure C16.
The efficiency of CSE depends on how costly the shared tensor is to compute. For instance, computing
S(W) is expensive and therefore CSE is the more suitable optimization technique. For the input-based
Kronecker factors which require the unfolded input, either contraction path optimization or CSE
might be better. This is because the optimal contraction order may not correspond to 2x input
unfolding and exhibit more parallelism which may lead to faster run times on a GPU. It would be
interesting to integrate CSE into the contraction path optimization and develop a heuristic to choose a
contraction path, for instance based on a weighted sum of FLOPs and memory."
REFERENCES,0.3082191780821918,"No index slicing:
We mention index slicing as a technique to reduce peak memory of, and distribute,
TN contractions. However, our implementation does not use index slicing, although there are packages
like cotengra [25] with an interface similar to opt_einsum. We did not experiment with index
slicing as our benchmark uses a single GPU and did not encounter out-of-memory errors. Still, we
mention this technique, as, in combination with CSE, it could automatically reduce peak memory of
the GGN quantities from Figure C16 which suffer from high memory requirements."
REFERENCES,0.30919765166340507,"B
Visual Tour of Tensor Network Operations for Convolutions"
REFERENCES,0.3101761252446184,"Here, we extend the presented operations with a batch axis and allow for grouped convolutions."
REFERENCES,0.3111545988258317,"B.1
Convolution & First-order Derivatives"
REFERENCES,0.312133072407045,"Adding a batch dimension (vmap-ing):
Adding a batch axis to all presented operations is trivial.
We only need to add an additional leg to the batched tensors, and connect these legs via element-wise
or inner multiplication, depending on whether the result tensor is batched or not."
REFERENCES,0.3131115459882583,"Grouped convolutions:
Grouped convolutions were originally proposed by Krizhevsky et al. [36]
and allow for parallelizing, distributing, and reducing the parameters of the convolution operation.
They split Cin input channels into G groups of size ˜Cin := Cin/G, then perform independent convolu-
tions per group, each producing ˜Cout := Cout/G output channels which are concatenated in the output.
Each group uses a kernel Wgof size ˜Cout × ˜Cin × K1 × K2. These kernels are stacked into a single
tensor W ∈RCout, ˜
Cin,K1,K2 such that [W](g,:),:,:,: = Wg. To support groups, we thus decompose the
channel indices into cin := (˜cin, g) and cout := (˜cout, g). For the forward pass this yields the grouped
convolution (without bias)"
REFERENCES,0.31409001956947163,"Y(g,˜cout),o1,o2 = P"
REFERENCES,0.3150684931506849,"i1,i2,˜cin,k1,k2 X(g,˜cin),i1,i2Π(1)
i1,o1,k1Π(2)
i2,o2,k2W(g,˜cout),cin,k1,k2 .
(B8)"
REFERENCES,0.31604696673189825,"Figure B9a shows the batched version of Equation (B8) as TN. Applying the differentiation rule from
§3 leads to the Jacobians and VJPs shown in the remaining panels of Figure B9."
REFERENCES,0.31702544031311153,"B.2
Exact Second-order Information"
REFERENCES,0.31800391389432486,"In Figure B12 we show the TNs for the GGN diagonal and the GGN Gram matrix (empirical NTK
matrix) from Figure C16 extended by channel groups and a batch axis."
REFERENCES,0.31898238747553814,"Diagonal block extraction:
Combined with index un-grouping, diagonal extraction generalizes to
larger blocks: Let A ∈RKI×KJ be a matrix of K horizontally and vertically concatenated blocks Π(1) Π(2) X
W i1 i2 k1 k2 o1 o2"
REFERENCES,0.3199608610567515,"n
˜cin
˜cout g"
REFERENCES,0.32093933463796476,"(a) Convolution (no bias)
Yn,(g,˜cout),o1,o2
Equation (B8) Π(1) Π(2) X i1 i2 k′
1 k′
2 o1 o2"
REFERENCES,0.3219178082191781,"˜c′
out
˜cout
n
˜cin g
g′"
REFERENCES,0.32289628180039137,"(b) Weight Jacobian
∂Yn,(g,˜cout),o1,o2
∂W(g′,˜c′out),˜c′
in,k′
1,k′
2 Π(1) Π(2) W k1 k2"
REFERENCES,0.3238747553816047,"˜c′
in i′
1 i′
2 o1 o2"
REFERENCES,0.324853228962818,"n
n′
˜cout g
g′"
REFERENCES,0.3258317025440313,"(c) Input Jacobian
∂Yn,(g,˜cout),o1,o2
∂Xn′,(g′,˜c′
in),i′
1,i′
2 Π(1) Π(2) X
V i1 i2 k′
1 k′
2 o1 o2 n"
REFERENCES,0.3268101761252446,"˜c′
in
˜c′
out g′"
REFERENCES,0.32778864970645794,"(d) Weight VJP
V (W)
(g′,˜c′out),˜c′
in,k′
1,k′
2
=
X"
REFERENCES,0.3287671232876712,"n,g,˜cout,
o1,o2"
REFERENCES,0.32974559686888455,"Vn,(g,˜cout),o1,o2
∂Yn,(g,˜cout),o1,o2
∂W(g′,˜c′out),˜c′
in,k′
1,k′
2 Π(1) Π(2)"
REFERENCES,0.33072407045009783,"V(Y)
W k1 k2"
REFERENCES,0.33170254403131116,"˜c′
in i′
1 i′
2 o1 o2 n′ g′ ˜cout"
REFERENCES,0.33268101761252444,"(e) Input VJP
V (X)
n′,(g′,˜c′
in),i′
1,i′
2
=
X"
REFERENCES,0.3336594911937378,"n,g,˜cout,
o1,o2"
REFERENCES,0.33463796477495106,"Vn,(g,˜cout),o1,o2
∂Yn,(g,˜cout),o1,o2
∂Xn′,(g′,˜c′
in),,i′
1,i′
2"
REFERENCES,0.3356164383561644,"Figure B9: TNs of the (a) forward pass, (b, c) Jacobians, and (d, e) VJPs with batch axis and channel
groups. They generalize Figures 2 and 3 from the main text. For the VJPs, the Jacobians are shaded. Π(1) Π(2) X S(Y) Π(1) Π(2) X S(Y) o1 o2 o′
1 o′
2 i1 i2 i′
1 i′
2 k1 k2 c n g ˜cin ˜cout"
REFERENCES,0.33659491193737767,"(a) GGN diagonal Π(1) Π(2) X S(Y) Π(1) Π(2) X S(Y) o1 o2 o′
1 o′
2 i1 i2 i′
1 i′
2 k1 k2"
REFERENCES,0.337573385518591,"c
c′
n
n′"
REFERENCES,0.3385518590998043,"g
˜cin ˜cout"
REFERENCES,0.3395303326810176,(b) GGN Gram matrix (empirical NTK)
REFERENCES,0.3405088062622309,"Figure B10: TNs of (a) the GGN diagonal and (b) the GGN Gram matrix with batching and channel
groups. They extend Figures C16b and C16c from the main text."
REFERENCES,0.34148727984344424,"A(k1,k2) ∈RI×J, ki = 1 . . . , K. We can extract the diagonal blocks by restoring the sub-structure, Π(1) Π(2) X S(Y) Π(1) Π(2) X S(Y) o1 o2 o′
1 o′
2 i1 i2 i′
1 i′
2 c"
REFERENCES,0.3424657534246575,"˜k1
˜k′
1 gK1"
REFERENCES,0.34344422700587085,"˜k2
˜k′
2 gK2"
REFERENCES,0.34442270058708413,"˜cin
˜c′
in gCin"
REFERENCES,0.34540117416829746,"˜cout
˜c′
out gCout"
REFERENCES,0.34637964774951074,Figure B11: TN of a GGN mini-block diagonal without batching and channel groups.
REFERENCES,0.3473581213307241,"Π(1)
Π(1)"
REFERENCES,0.34833659491193736,"Π(2)
Π(2) X
X i1"
REFERENCES,0.3493150684931507,"i2
i′
2
i′
1 o1 o2"
REFERENCES,0.350293542074364,"k1
k′
1"
REFERENCES,0.3512720156555773,"k2
k′
2 n"
REFERENCES,0.3522504892367906,"˜cin
˜c′
in g"
REFERENCES,0.3532289628180039,"(a) KFC/KFAC-expand factor 1
1"
REFERENCES,0.3542074363992172,"Π(1)
Π(1) 1
1"
REFERENCES,0.35518590998043054,"Π(2)
Π(2) X
X i1"
REFERENCES,0.3561643835616438,"i2
i′
2 i′
1 o1 o2 o′
1 o′
2"
REFERENCES,0.35714285714285715,"k1
k′
1"
REFERENCES,0.35812133072407043,"k2
k′
2"
REFERENCES,0.35909980430528377,"˜cin
˜c′
in g
n"
REFERENCES,0.36007827788649704,(b) KFAC-reduce factor
REFERENCES,0.3610567514677104,"Figure B12: TN diagrams of input-based factors in Kronecker approximations of the GGN for
convolutions with batching and channel groups. They extend Figure 4 from the main text."
REFERENCES,0.36203522504892366,"then taking the diagonal along the K-dimensional index,"
REFERENCES,0.363013698630137,"
A(k,k)	
ki
j =
A
(k, j)
(k, i)
k i
j ."
REFERENCES,0.3639921722113503,"We can apply this procedure to the GGN from Figure C16a. Assume we want to divide the output
channel, input channel, and spatial dimensions into GCout, GCin, GK1, GK2 groups. A group will thus
be indexed with a tuple (gCout, gCin, gK1, gK2) and the corresponding GGN block will be of dimension
Cout/GCout ×Cin/GCin ×K1/GK1 ×K2/GK2 ×Cout/GCout ×Cin/GCin ×K1/GK1 ×K2/GK2 and
correspond to the GGN for [W](gCout,:),(gCin,:),(gK1,:),(gK2,:). This process of un-grouping the output
dimensions, then taking the diagonal along the group indices, is illustrated in Figure B11. Note that
if we choose GCout = Cout, GCin = Cin, GK1 = K1, GK2 = K2, each block will be a single number
and hence we recover the GGN diagonal from Figure C16b. If instead we GCout = GCinGK1GK2 = 1,
we obtain the full GGN from Figure C16a. The outlined schemes allows to extract mini-blocks of
arbitrary size along the diagonal (subject to the total dimension)."
REFERENCES,0.3649706457925636,"B.3
Kronecker-factored Approximate Curvature (KFAC) for Grouped Convolutions"
REFERENCES,0.3659491193737769,"We were unable to find a definition of KFAC for grouped convolutions. Hence, we derive it here
and present the TN diagrams. We use the perspective that grouped convolutions are independent
convolutions over channel groups which are then concatenated. For each of those convolutions, we
can then apply established the KFAC approximation for convolutions without groups. For a group
g we have the kernel Wg = [W](g,:),:,:,: and the unfolded input of its associated input channels,
JXgK = JXK(g,:),:,: = J[X](g,:),:,:K (or JXn,gK = JXnK(g,:),:,: = J[X]n,(g,:),:,:K in the batched setting)."
REFERENCES,0.3669275929549902,"KFC/KFAC-expand for grouped convolutions:
Applying the regular KFC approximation to
the kernel of group g, this yields the Fisher approximation Ωg ⊗Γg with Γg ∈R ˜
Cout× ˜
Cout and"
REFERENCES,0.3679060665362035,"Ωg = 1/N PN
n=1JXn,gKJXn,gK⊤∈R ˜
CinK1K2× ˜
CinK1K2 where Xn,g is the input tensor for sample
n and group g (remember the index structure Xn,(g,˜cin),i1,i2). Figure B12a shows the diagram for
{NΩg}G
g=1."
REFERENCES,0.36888454011741684,"KFAC-reduce for grouped convolutions:
Proceeding in the same way, but using the unfolded input
averaged over output locations, we obtain the Fisher approximation ˆΩg ⊗ˆΓg with ˆΓg ∈R ˜
Cout× ˜
Cout"
REFERENCES,0.3698630136986301,"and ˆΩg = 1/N(O1O2)2 PN
n=1 1⊤
O1O2JXn,gK(1⊤
O1O2JXn,gK)⊤∈R ˜
CinK1K2× ˜
CinK1K2 for the kernel of
group g. Figure B12b shows the diagram for {N(O1O2)2 ˆΩg}G
g=1."
REFERENCES,0.37084148727984345,"B.4
Kronecker-factored Approximate Curvature (KFAC) for Transpose Convolution"
REFERENCES,0.37181996086105673,Here we derive the KFAC approximation for transpose convolutions.
REFERENCES,0.37279843444227007,"We describe transpose convolution in terms of its associated convolution from an input space
X = RCin×I1×I2 to an output space Y = RCout×O1×O2. The convolution has hyper-parameters
K1,2, S1,2, P1,2, D1,2 with index patterns Π(1) = Π(I1, K1, S1, P1, D1) ∈RI1×O1×K1 and
Π(2) = Π(I2, K2, S2, P2, D2) ∈RI2×O2×K2."
REFERENCES,0.37377690802348335,"Transpose convolution as matrix multiplication:
Transpose convolution maps a Y ∈Y into an
X ∈X. In ML frameworks like PyTorch, its kernel ˜W is stored as Cout × Cin × K1 × K2 tensor. The
relation X = ˜W ⋆T Y where ⋆T denotes transpose convolution is given by Figure 3d,"
REFERENCES,0.3747553816046967,"Xcin,i1,i2 ="
REFERENCES,0.37573385518590996,"Cout
X"
REFERENCES,0.3767123287671233,"cout=1 K1
X k1=1 K2
X k2=1 O1
X o1=1 O2
X"
REFERENCES,0.3776908023483366,"o2=1
Π(1)
i1,o1,k1Π(2)
i2,o2,k2Ycout,k1,k2 ˜Wcout,cin,k1,k2
(B9)"
REFERENCES,0.3786692759295499,"Our goal is to turn the express the above as matrix multiplication. To do that, we first define the
matrix reshape X of X via X ∈RCin×I1I2 such that [X]cin,(i1,i2) = Xcin,i1,i2. Next, we consider a
transposed kernel W of ˜W with changed order of the first two indices, i.e. W ∈RCin×Cout×K1×K2
such that"
REFERENCES,0.3796477495107632,"Wcin,cout,k1,k2 = ˜Wcout,cin,k1,k2 .
(B10)"
REFERENCES,0.3806262230919765,"This transposition is necessary to convert the kernel’s layout in the ML framework to a layout that
admits Equation (B9) to be expressed as matrix multiplication. Using a matrix reshape W of W via
W ∈RCin×CoutK1K2 such that [W ]cin,(cout,k1,k2) = Wcin,cout,k1,k2, we can express Equation (B9) as
matrix multiplication"
REFERENCES,0.3816046966731898,"X = W JYKT
(B11)"
REFERENCES,0.38258317025440314,"where JYKT ∈RCoutK1K2×I1I2 is the transpose-unfolded input to the transpose convolution (note that
J·K ̸= J·KT!)"
REFERENCES,0.3835616438356164,"[JYKT](cout,k1,k2),(i1,i2) = O1
X o1=1 O2
X"
REFERENCES,0.38454011741682975,"o2=1
Π(1)
i1,o1,k1Π(2)
i2,o2,k2Ycout,o1,o2 .
(B12)"
REFERENCES,0.38551859099804303,"To the best of our knowledge there is no API for J·KT in existing ML frameworks. Our approach can
provide a simple and efficient implementation of J·K through the TN shown in Figure B13a which
corresponds to Equation (B12). As Equation (B11) is of the same form as Equation (1), it is now
straightforward to write down the KFAC approximations for transpose convolution."
REFERENCES,0.38649706457925637,"KFAC-expand:
We will define the KFAC-expand approximation for the GGN w.r.t. the flattened
kernel w of W . Note that, in practise, this approximation must be properly transformed back to
the layout ˜W of the ML framework. We have G(w) ≈Ω⊗Γ, with Γ ∈RCin×Cin computed from
backpropagated gradients, and the input-based Kronecker factor"
REFERENCES,0.38747553816046965,"Ω= JYKTJYK⊤
T ∈RCoutK1K2×CoutK1K2 .
(B13)"
REFERENCES,0.388454011741683,See Figure B13b for the corresponding TN. Π(1) Π(2) Y o1 o2 cout k1 k2 i1 i2
REFERENCES,0.38943248532289626,"(a)
Transpose-
unfolded input"
REFERENCES,0.3904109589041096,"Π(1)
Π(1)"
REFERENCES,0.3913894324853229,"Π(2)
Π(2) Y
Y o1 o2 o′
1 o′
2 i1 i2"
REFERENCES,0.3923679060665362,"k1
k′
1"
REFERENCES,0.3933463796477495,"k2
k′
2"
REFERENCES,0.3943248532289628,"cout
c′
out"
REFERENCES,0.3953033268101761,(b) KFAC-expand
REFERENCES,0.39628180039138944,"Π(1)
Π(1) 1
1"
REFERENCES,0.3972602739726027,"Π(2)
Π(2) 1
1 Y
Y o1 o2 o′
1 o′
2"
REFERENCES,0.39823874755381605,"i1
i′
1"
REFERENCES,0.39921722113502933,"i2
i′
2"
REFERENCES,0.40019569471624267,"k1
k′
1"
REFERENCES,0.40117416829745595,"k2
k′
2"
REFERENCES,0.4021526418786693,"cout
c′
out"
REFERENCES,0.40313111545988256,(c) KFAC-reduce
REFERENCES,0.4041095890410959,Figure B13: TNs for extending KFAC to transpose convolutions (no batching and groups).
REFERENCES,0.4050880626223092,"KFAC-reduce:
For KFAC-reduce, we have G(w) ≈ˆΩ⊗ˆΓ, with ˆΓ ∈RCin×Cin computed from
backpropagated gradients, and the input-based Kronecker factor"
REFERENCES,0.4060665362035225,"ˆΩ=
1
(I1I2)2
 
1⊤
I1I2JYKT
  
1⊤
I1I2JYKT
⊤∈RCoutK1K2×CoutK1K2 .
(B14)"
REFERENCES,0.4070450097847358,See Figure B13c for the corresponding TN.
REFERENCES,0.4080234833659491,"With batching and groups:
In the presence of G groups, we have per-group kernels ˜Wg =
[ ˜W](g,:),:,:,: ∈R
Cout/G×Cin/G×K1×K2 and Wg ∈R
Cin/G×Cout/G×K1×K2, as well as per-group transpose-
unfolded inputs JYgKT = JYKT(g,:),:,: = J[Y](g,:),:,:KT ∈R
Cout/GK1K2×I1I2. Each group corresponds
to a transpose convolution in itself. With batching, we have an additional leading batch dimension,
i.e. JYn,gKT. Applying the same steps from above, we can define the KFAC approximation for the
GGN w.r.t. the flattened per-group kernel wg of Wg."
REFERENCES,0.4090019569471624,"For KFAC-expand, we have G(wg) ≈Ωg ⊗Γg, with Γg ∈R
Cin/G×Cin/G computed from backpropa-
gated gradients, and the input-based Kronecker factor"
REFERENCES,0.40998043052837574,"Ωg = 1 N N
X"
REFERENCES,0.410958904109589,"n=1
JYn,gKTJYn,gK⊤
T ∈R
Cout/GK1K2×Cout/GK1K2 ."
REFERENCES,0.41193737769080235,"For KFAC-reduce, we have G(wg) ≈ˆΩg ⊗ˆΓg, with ˆΓg ∈R
Cin/G×Cin/G computed from backpropa-
gated gradients, and the input-based Kronecker factor"
REFERENCES,0.41291585127201563,"ˆΩg =
1
N(O1O2)2 N
X n=1"
REFERENCES,0.41389432485322897," 
1⊤
I1I2JYn,gKT
  
1⊤
I1I2JYn,gKT
⊤∈R
Cout/GK1K2×Cout/GK1K2 ."
REFERENCES,0.41487279843444225,"B.5
Further Operations & Extensive Overview"
REFERENCES,0.4158512720156556,"Consecutive convolutions:
We can chain two, or more, convolutions into a single TN diagram
(Figure B14) to obtain a deep linear CNN [65] similar to deep linear networks which are popular for
analytical studies."
REFERENCES,0.41682974559686886,"Convolution weight/input JVPs:
In the main text, we derived the Jacobians of convolution (§3.1)
which can be used to derive the JVPs. A JVP propagates perturbations V(W) ∈RCout×Cin×K1×K2"
REFERENCES,0.4178082191780822,"and V(X) ∈RCin×I1×I2 in the input space to perturbations in the output space by contracting the
perturbation with the Jacobian. See Table B3 for the general einsum expressions."
REFERENCES,0.4187866927592955,"Batched convolution weight VJP:
To obtain per-sample gradients, the weight VJP must be carried
out without summing over the batch axis which amounts to keeping the batch index in the output
index tuple."
REFERENCES,0.4197651663405088,"VJPs and JVPs of im2col:
With the TN differentiation technique described in §3.1 we can compute
the Jacobian of the unfolding operation, then contract it with perturbations V (X) ∈RCin×K1×K2 in
input space to obtain the JVP, or with perturbations V (JXK) ∈RO1O2×CinK1K2 to obtain the VJP."
REFERENCES,0.4207436399217221,"Π(1)
Π′(1)"
REFERENCES,0.4217221135029354,"Π(2)
Π′(2)"
REFERENCES,0.4227005870841487,"X
W
W′
cin i1 i2 k1 k2 k′
1 k′
2 cout o1 o2"
REFERENCES,0.42367906066536204,"c′
out o′
1 o′
2"
REFERENCES,0.4246575342465753,Figure B14: TN of two consecutive convolutions without groups and without batch axis.
REFERENCES,0.42563600782778865,"Approximate Hessian diagonals (HesScale/BL89):
Becker & Lecun [4], Elsayed et al. [22]
proposed approximate procedures for the Hessian diagonal which cost roughly a gradient. They can
be understood as modifications of the Hessian backpropagation equations from Dangel et al. [16]."
REFERENCES,0.42661448140900193,"Consider a layer with input x, output y, and weights w inside a sequential feedforward neural network
(for a convolutional layer, these correspond to the flattened input, output, and kernel). To compute
per-layer Hessians of a loss ℓ, each layer backpropagates its incoming Hessian ∇2
yℓaccording to [16]"
REFERENCES,0.42759295499021527,"∇2
xℓ= (Jxy)⊤∇2
yℓ(Jxy) +
X i"
REFERENCES,0.42857142857142855,"∂ℓ
∂yi
∇2
xyi ,"
REFERENCES,0.4295499021526419,"∇2
wℓ= (Jwy)⊤∇2
yℓ(Jwy) +
X i"
REFERENCES,0.43052837573385516,"∂ℓ
∂yi
∇2
wyi .
(B15)"
REFERENCES,0.4315068493150685,"The scheme of [4, 22] imposes diagonal structure on the backpropagated quantity. A layer receives
a backpropagated diagonal d(y) such that diag(d(y)) ≈∇2
yℓ, and backpropagates it according to
Equation (B15), but with a post-processing step to obtain a diagonal backpropagated quantity,"
REFERENCES,0.4324853228962818,"d(x) = diag

(Jxy)⊤diag(d(y))(Jxy)

+ diag X i"
REFERENCES,0.4334637964774951,"∂ℓ
∂yi
∇2
xyi ! ,"
REFERENCES,0.4344422700587084,"d(w) = diag

(Jwy)⊤diag(d(w))(Jwy)

+ diag X i"
REFERENCES,0.4354207436399217,"∂ℓ
∂yi
∇2
wyi ! , (B16)"
REFERENCES,0.436399217221135,"where diag(d(x)) ≈∇2
xℓand diag(d(w)) ≈∇2
wℓis an approximation to the Hessian diagonal."
REFERENCES,0.43737769080234834,"For convolutional layers, which are linear in the input and weight, the second summands are zero due
to ∇2
xyi = 0 = ∇2
wyi. The first terms of Equation (B16) require (i) embedding a diagonal vector
into a matrix, (ii) applying MJPs and JMPs, and (iii) extracting the result’s diagonal. Those can
be expressed as a single TN. We show the diagrams in Figure B15, using tensors rather than their
flattened versions, that is (x, y, w, d(x), d(y), d(w)) →(X, Y, W, D(X), D(Y), D(W))."
REFERENCES,0.4383561643835616,"Π(1)
Π(1)"
REFERENCES,0.43933463796477495,"Π(2)
Π(2)"
REFERENCES,0.44031311154598823,"D(Y)
W Π(1) Π(2) W k1 k2 k′
1 k′
2 o1 o2
i1 i2 cout cin"
REFERENCES,0.44129158512720157,(a) HesScale/BL89 input backpropagation
REFERENCES,0.44227005870841485,"Π(1)
Π(1)"
REFERENCES,0.4432485322896282,"Π(2)
Π(2)"
REFERENCES,0.44422700587084146,"D(Y)
X Π(1) Π(2) X"
REFERENCES,0.4452054794520548,"i1
i′
1"
REFERENCES,0.4461839530332681,"i2
i′
2 o1 o2 k1 k2 cout cin"
REFERENCES,0.4471624266144814,(b) HesScale/BL89 weight backpropagation
REFERENCES,0.4481409001956947,"Π(1)
Π(1)"
REFERENCES,0.449119373776908,"Π(2)
Π(2)"
REFERENCES,0.4500978473581213,"D(Y)
W Π(1) Π(2) W k1 k2 k′
1 k′
2 o1 o2
i1 i2 n g ˜cout ˜cin"
REFERENCES,0.45107632093933464,"(c) HesScale/BL89 input backpropagation (+ batch,
groups)"
REFERENCES,0.4520547945205479,"Π(1)
Π(1)"
REFERENCES,0.45303326810176126,"Π(2)
Π(2)"
REFERENCES,0.45401174168297453,"D(Y)
X Π(1) Π(2) X"
REFERENCES,0.45499021526418787,"i1
i′
1"
REFERENCES,0.45596868884540115,"i2
i′
2 o1 o2 k1 k2
n ˜cout g ˜cin"
REFERENCES,0.4569471624266145,"(d) HesScale/BL89 weight backpropagation (+ batch,
groups)"
REFERENCES,0.45792563600782776,"Figure B15: TN diagrams for HesScale/BL89 [4, 22] backpropagations through convolutional layers
to approximate the Hessian diagonals D(X), D(W). JMPs and MJPs are shaded. (a, b) show the simple
versions without batching and without channel groups. (c, d) include batching and channel groups."
REFERENCES,0.4589041095890411,"Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support
batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...."
REFERENCES,0.4598825831702544,"Operation
Operands
Contraction string (einops [59] convention)"
REFERENCES,0.4608610567514677,"Convolution (no bias) [29]
X, Π(1), Π(2), W
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, (g c_out) c_in k1 k2 -> n (g c_out) o1 o2"""
REFERENCES,0.461839530332681,"Unfolded input (im2col, JXK)
X, Π(1), Π(2)
""n c_in i1 i2, i1 o1 k1, i2 o2 k2 -> n (c_in k1 k2) (o1 o2)"""
REFERENCES,0.4628180039138943,"Unfolded kernel (Toeplitz)
Π(1), Π(2), W
""i1 o1 k1, i2 o2 k2, c_out c_in k1 k2 -> (c_out o1 o2) (c_in i1 i2)"""
REFERENCES,0.4637964774951076,"Folded output (col2im)
Y, Π(1), Π(2)
""n (g c_out) o1 o2, i1 o1 k1, i2 o2 k2 -> n (g c_in) i1 i2"""
REFERENCES,0.46477495107632094,"Transpose-unfolded input (JYKT)
Y, Π(1), Π(2)
""n (g c_out) o1 o2, i1 o1 k1, i2 o2 k2 -> n (g c_in k1 k2) i1 i2"""
REFERENCES,0.4657534246575342,"Convolution weight VJP
X, Π(1), Π(2), V(Y)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_out) o1 o2 -> c_out c_in k1 k2"""
REFERENCES,0.46673189823874756,"Convolution input VJP (transpose convolution)
W, Π(1), Π(2), V(Y)
""(g c_out) c_in k1 k2, i1 o1 k1, i2 o2 k2, n (g c_out) o1 o2 -> n (g c_in) i1 i2"""
REFERENCES,0.46771037181996084,"Convolution
weight
VJP
(per-
sample/batched) [58]
X, Π(1), Π(2), V(Y)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_out) o1 o2 -> n (g c_out) c_in k1 k2"""
REFERENCES,0.46868884540117417,"Convolution weight JVP
X, Π(1), Π(2), V(W)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, (g c_out) c_in k1 k2 -> n (g c_out) o1 o2"""
REFERENCES,0.46966731898238745,"Convolution input JVP
W, Π(1), Π(2), V(X)
""(g c_out) c_in i1 i2, i1 o1 k1, i2 o2 k2, n (g c_in) i1 i2 -> n (g c_out) o1 o2"""
REFERENCES,0.4706457925636008,"im2col VJP
Π(1), Π(2), V(JXK)
""i1 o1 k1, i2 o2 k2, n (c_in k1 k2) (o1 o2) -> n c_in i1 i2"""
REFERENCES,0.47162426614481406,"im2col JVP
Π(1), Π(2), V(X)
""i1 o1 k1, i2 o2 k2, n c_in i1 i2 -> n (c_in k1 k2) (o1 o2)"""
REFERENCES,0.4726027397260274,"KFC/KFAC-expand [27, 23]
X, Π(1), Π(2), X, Π(1), Π(2)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_in_) i1_ i2_, i1_ o1 k1_, i2_ o2 k2_
-> g (c_in k1 k2) (c_in_ k1_ k2_)"""
REFERENCES,0.4735812133072407,"KFAC-reduce [23]
X, Π(1), Π(2), X, Π(1), Π(2)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_in_) i1_ i2_, i1_ o1_ k1_, i2_ o2_ k2_
-> g (c k1 k2) (c_ k1_ k2_)"""
REFERENCES,0.474559686888454,"KFC/KFAC-expand for transpose convolution
Y, Π(1), Π(2), Y, Π(1), Π(2)
""n (g c_out) o1 o2, i1 o1 k1, i2 o2 k2, n (g c_out_) o1_ o2_, i1_ o1 k1_, i2_ o2 k2_
-> g (c_out k1 k2) (c_out_ k1_ k2_)"""
REFERENCES,0.4755381604696673,"KFAC-reduce for transpose convolution
Y, Π(1), Π(2), Y, Π(1), Π(2)
""n (g c_out) o1 o2, i1 o1 k1, i2 o2 k2, n (g c_out_) o1_ o2_, i1_ o1_ k1_, i2_ o2_ k2_
-> g (c_out k1 k2) (c_out_ k1_ k2_)"""
REFERENCES,0.4765166340508806,"GGN Gram/empirical NTK matrix [18, 51, 49]
X, Π(1), Π(2), S(Y), X, Π(1), Π(2), S(Y)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, c n (g c_out) o1 o2, n_ (g c_in) i1_ i2_, i1_ o1_ k1,
i2_ o2_ k2, c_ n_ (g c_out) o1_ o2_ -> (c n) (c_ n_)"""
REFERENCES,0.4774951076320939,"GGN/Fisher diagonal [17, 51]
X, Π(1), Π(2), S(Y), X, Π(1), Π(2), S(Y)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, c n (g c_out) o1 o2, n (g c_in) i1_ i2_, i1_ o1_ k1,
i2_ o2_ k2, c n (g c_out) o1_ o2_ -> (g c_out) c_in k1 k2"""
REFERENCES,0.47847358121330724,"GGN/Fisher diagonal (per-sample/batched)
X, Π(1), Π(2), S(Y), X, Π(1), Π(2), S(Y)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, c n (g c_out) o1 o2, n (g c_in) i1_ i2_, i1_ o1_ k1,
i2_ o2_ k2, c n (g c_out) o1_ o2_ -> n (g c_out) c_in k1 k2"""
REFERENCES,0.4794520547945205,"Approximate weight Hessian diagonal [4, 22]
X, Π(1), Π(2), D(Y), X, Π(1), Π(2)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_out) o1 o2, n (g c_in) i1_ i2_, i1_ o1 k1, i2_ o2 k2
-> (g c_out) c_in k1 k2"""
REFERENCES,0.48043052837573386,"Approximate input Hessian diagonal [4, 22]
W, Π(1), Π(2), D(Y), W, Π(1), Π(2)
""(g c_out) c_in k1 k2, i1 o1 k1, i2 o2 k2, n (g c_out) o1 o2, (g c_out) c_in k1_ k2_, i1 o1 k1_,
i2 o2 k2_ -> n (g c_in) i1 i2"""
REFERENCES,0.48140900195694714,"Approximate weight Hessian diagonal (per-
sample/batched)
X, Π(1), Π(2), D(Y), X, Π(1), Π(2)
""n (g c_in) i1 i2, i1 o1 k1, i2 o2 k2, n (g c_out) o1 o2, n (g c_in) i1_ i2_, i1_ o1 k1, i2_ o2 k2
-> n (g c_out) c_in k1 k2"" Π(1) Π(2) X S(Y) Π(1) Π(2) X S(Y) o1 o2 o′
1 o′
2 i1 i2 i′
1 i′
2"
REFERENCES,0.48238747553816047,"c′
out c′
in k′
1 k′
2 cout cin k1 k2 c"
REFERENCES,0.48336594911937375,"(a) GGN/Fisher Π(1) Π(2) X S(Y) Π(1) Π(2) X S(Y) o1 o2 o′
1 o′
2 i1 i2 i′
1 i′
2 cout cin k1 k2 c"
REFERENCES,0.4843444227005871,"(b) GGN/Fisher diagonal Π(1) Π(2) X S(Y) Π(1) Π(2) X S(Y) o1 o2 o′
1 o′
2 i1 i2 i′
1 i′
2 k1 k2 cin cout c
c′"
REFERENCES,0.48532289628180036,"(c) Gram (empirical NTK)
Figure C16: TN composition and sub-tensor extraction for second-order information. Weight MJPs
from Figure 3c are shaded. (a) exact and (b) diagonal of the kernel’s GGN (the same applies to
structurally similar matrices like the gradient covariance [35]). (c) TN of the GGN Gram matrix."
REFERENCES,0.4863013698630137,"C
Exact Second-Order Information"
REFERENCES,0.487279843444227,"Here, we look at computing second-order information of a loss w.r.t. to the kernel of a convolution.
Its computation can be phrased as backpropagation with a final extraction step [19] which contains
less standard operations like Jacobian-matrix products (JMPs) and sub-tensor extraction. TNs can
express this extraction step in a single diagram."
REFERENCES,0.4882583170254403,"Consider a datum (x, t) and its loss ℓ(w) = ℓ(f, t) where f := fw(x) ∈RC is the pre-
diction of a CNN with a convolution with flattened kernel w and flattened output y (deriva-
tions carry over to a batch loss). The kernel’s generalized Gauss-Newton (GGN) matrix [63]
G(w) = (Jwf)⊤∇2
fℓ(Jwf) ∈RCoutCinK1K2×CoutCinK1K2 is a positive semi-definite Hessian proxy
preferred by many applications [e.g. 20, 45] and coincides with the Fisher information matrix for
many common losses [46]. It is the self-outer product of a backpropagated symmetric factorization
S(y) = (Jyf)⊤S(f) ∈RCoutO1O2×C of the loss Hessian, ∇2
fℓ(f, y) = S(f)(S(f))⊤. During
backpropagation, the convolution extracts information about G(w) = (Jwy)⊤S(y)(S(y))⊤Jwy."
REFERENCES,0.4892367906066536,"In TN notation, this is easy to express without flattening: We simply compose two VJP diagrams
from Figure 3c with an extra leg (MJP) and add the outer-product contraction to obtain the tensor
version G(W) ∈RCout×Cin×K1×K2×Cout×Cin×K1×K2 of G(w) (Figure C16a). The GGN is often
further approximated by sub-tensors as it is too large. These slicing operations are also easy to
integrate into the diagrams, e.g. to extract diagonal elements (Figure C16b [17, 51]), or mini-block
diagonals (Figure B11 [16, 3]). This also removes redundant computations compared to computing,
then slicing, the matrix. The same ideas apply to the GGN Gram matrix (S(w))⊤S(w) ∈RC×C
(Figure C16c). It contains the GGN spectrum [18] and is related to the empirical NTK for square loss
[49]."
REFERENCES,0.49021526418786693,"D
Implementation Details"
REFERENCES,0.4911937377690802,"Here we present details on the index pattern computation, and additional transformations."
REFERENCES,0.49217221135029354,"D.1
Index Pattern Tensor Computation for Convolutions"
REFERENCES,0.4931506849315068,"Algorithm D1 lists pseudo-code for the index pattern computation from the convolution hyper-
parameters K, S, P, D, and the spatial input dimension I, that is Π(I, K, S, P, D). Unlike in the
main text, we use 0-based indexing which is more common in numerical libraries. For self-consistency,
we re-state the relation of the hyper-parameters to output dimension from [21, Relationship 15],"
REFERENCES,0.49412915851272016,"O(I, K, S, P, D) = 1 +
I + 2P −K −(K −1)(D −1) S"
REFERENCES,0.49510763209393344,"
.
(D17)"
REFERENCES,0.49608610567514677,Algorithm D1 Computing the convolution index pattern tensor Π for a spatial dimension.
REFERENCES,0.49706457925636005,"Require: Input size I ∈N+, kernel size K ∈N+, stride S ∈N+, padding P ∈N+
0 , dilation D ∈N+"
REFERENCES,0.4980430528375734,"O ←1 +
j
I+2P −K−(K−1)(D−1)"
REFERENCES,0.49902152641878667,"S
k
▷Compute output dimension [21, Relationship 15]
Π ←0I×O×K
▷Initialize index pattern tensor
for o = 0, . . . , O −1, k = 0, . . . , K −1 do
▷Use 0-based indexing!
i ←kD + oS −P
▷Reconstruct contributing input element
if 0 ≤i ≤I −1 then
▷Check in bounds
Πi,o,k ←1
end if
end for
return Index pattern tensor Π ∈{0, 1}I×O×K"
REFERENCES,0.5,"D.2
Index Pattern Tensor for Standalone Transpose Convolution"
REFERENCES,0.5009784735812133,"Although a transpose convolution is defined w.r.t. a reference convolution with hyper-parameters
K, S, P, D, most libraries offer standalone implementations of transpose convolution. We describe
the transpose convolution by its associated convolution, that is as a mapping from RCout×O1×O2 (the
convolution’s output space) to RCin×I1×I2 (the convolution’s input space). For convolution with
S > 1, we cannot infer I from O, K, S, P, D, as multiple Is map to the same O if (I + 2P −K −
(K −1)(D −1)) mod S ̸= 0 (see the floor operation in Algorithm D1). We need to either supply I
directly, or the remainder"
REFERENCES,0.5019569471624267,A = I + 2P −K −(K −1)(D −1) −S(O −1)
REFERENCES,0.50293542074364,"(often called output_padding) to make I unambiguous. Then, we compute"
REFERENCES,0.5039138943248532,"I = (O −1)S −2P + K + (K −1)(D −1) + A .
(D18)"
REFERENCES,0.5048923679060665,"to get I(O, A) and call Algorithm D1 to obtain Π(I(O, A), K, S, P, D)."
REFERENCES,0.5058708414872799,"D.3
Details on Index Pattern Simplifications"
REFERENCES,0.5068493150684932,"In the following, we will assume the absence of boundary pixels that don’t overlap with the kernel,
that is"
REFERENCES,0.5078277886497065,"I + 2P −(K + (K −1)(D −1))
mod S = 0 ,
(D19)"
REFERENCES,0.5088062622309197,"where the floor operation in O(I, K, S, P, D) is obsolete. This can always be assured by narrowing
X before a convolution. Based on our hyper-parameter analysis of real-world CNNs (§E), we identify:"
REFERENCES,0.5097847358121331,"Transformation D1 (Dense convolutions) Assume Equation (D19). For K = S with default
padding and dilation (P = 0, D = 1), patches are adjacent non-overlapping tiles, accessible
by un-grouping the input index i into a tuple index (˜i, ˜k) of size I/K × K:"
REFERENCES,0.5107632093933464,"[Π(I, K, K, 0, 1)]i,o,k = [Π(I, K, K, 0, 1)](˜i,˜k),o,k = δ˜i,oδ˜k,k ."
REFERENCES,0.5117416829745597,"Point-wise convolutions (K = S = 1) are a special case with pattern [Π(I, 1, 1, 0, 1)]i,o,k = δi,o."
REFERENCES,0.512720156555773,"Point-wise convolutions with K = S = 1 are common in DenseNets [33], MobileNets [31, 60] and
ResNets [30]. InceptionV3 [69] has 2d ‘mixed dense’ convolutions that are point-wise along one
spatial dimension. ConvNeXt [42] uses dense convolutions with K = S ∈{2, 4}."
REFERENCES,0.5136986301369864,"Transformation D2 (Down-sampling convolutions) For S > K with default padding and dilation
(P = 0, D = 1), some elements do not overlap with the kernel. If the input dimension i is summed,
all participating tensors can be pruned to remove the explicit zeros. Assume I mod S = 0. Then,
pruning amounts to un-grouping i into (i′, s) of size I/S × S, narrowing s to K entries, and grouping
back into an index ˜i of size KI/S. After pruning, the index pattern represents a dense convolution
with input size KI/S, kernel size K, and stride K. In a contraction with some tensor V,
PI
i=1 [V]...,i,... [Π(I, K, S > K, 0, 1)]i,o,k = PI/S
˜i=1[˜V]...,˜i,... [Π(KI/S, K, K, 0, 1)]˜i,o,k"
REFERENCES,0.5146771037181996,"with sub-tensor [˜V]...,˜i,... = [[V]...,(i′,s),...]...,(:,:K),... where :K means narrowing to K elements."
REFERENCES,0.5156555772994129,"Transformation D2 converts down-sampling convolutions to dense convolutions, which can be further
simplified with Transformation D1. We find down-sampling convolutions with S = 2 > K = 1
in ResNet18 [30], ResNext101 [72], and WideResNet101 [73]. Those convolutions discard 75 %
of their input! Knowledge that an operation only consumes a fraction of its input could be used to
eliminate those ‘dead’ computations in preceding operations, reducing FLOPS and memory."
REFERENCES,0.5166340508806262,"Transformation D3 (Kernel-output dimension swap) Assume Equation (D19). Transposing ker-
nel and output dimensions in an index pattern yields another index pattern with same input size,
kernel size O(I, K, S, P, D), and swapped stride and dilation:"
REFERENCES,0.5176125244618396,"[Π(I, K, S, P, D)]i,o,k = [Π(I, O, D, P, S)]i,k,o ."
REFERENCES,0.5185909980430529,"This transformation is easy to see from the symmetry of (k, D) and (o, S) in Equation (7) and
O(I, K, S, P, D). It converts index pattern contractions over output into kernel dimensions, like in
convolutions. An example is the weight VJP from Figure 3c, which—after swapping kernel and
output dimensions—resembles the TN for convolution from Figure 2 with kernel V. Rochette et al.
[58] use this to phrase the computation of per-example gradients as convolution."
REFERENCES,0.5195694716242661,"§D.3 presents more properties of Π based on the sub-sampling interpretation of stride and dilation
along the output and kernel dimensions. We also provide a transformation for swapping input and
output dimensions, relating convolution and transpose convolution as described in [21]."
REFERENCES,0.5205479452054794,"For completeness, we state additional index pattern tensor properties here (using 1-based indexing):"
REFERENCES,0.5215264187866928,"Transformation D4 (Sub-sampling interpretation of stride) Strided convolutions (S > 1) sub-
sample non-strided convolutions along the output dimension, ignoring all but every Sth output [21].
In other words, [Π(I, K, S, P, D)]i,o,k = [Π(I, K, 1, P, D)]i,1+S(o−1),k or, in tensor notation ([·]::S
denotes slicing with steps of S),"
REFERENCES,0.5225048923679061,"Π(I, K, S, P, D) = [Π(I, K, 1, P, D)]:,::S,: ."
REFERENCES,0.5234833659491194,"Transformation D5 (Sub-sampling interpretation of dilation) Dilated convolutions (D > 1)
with kernel size K sub-sample the kernel of a non-dilated convolution of kernel size K + (D −
1)(K −1), ignoring all but every Dth kernel element. In other words, [Π(I, K, S, P, D)]i,o,k =
[Π(I, K + (K −1)(D −1), S, P, 1)]i,o,1+D(k−1) or, in tensor notation,"
REFERENCES,0.5244618395303327,"Π(I, K, S, P, D) = [Π(I, K + (K −1)(D −1), S, P, 1)]:,:,::D ."
REFERENCES,0.525440313111546,"Transformation D6 (Transpose convolution as convolution) Assume Equation (D19). Consider a
non-strided (S = 1), non-dilated (D = 1) convolution with index pattern Π(I, K, 1, P, 1) and output
dimension O(I, K, 1, P, 1). Transposing the spatial dimensions and flipping the kernel dimension
yields another index pattern with modified padding P ′ = K −P −1. In other words, for all
i = 1, . . . , I, k = 1, . . . , K, o = 1, . . . , O"
REFERENCES,0.5264187866927593,"[Π(I, K, 1, P, 1)]i,o,k = [Π(O, K, 1, P ′, 1)]o,i,K+1−k ."
REFERENCES,0.5273972602739726,"E
Convolution Layer Hyper-parameter Analysis"
REFERENCES,0.5283757338551859,"Here we give an overview of and characterize convolutions in popular architectures (see Table E4). We
include moderately deep CNNs on Fashion MNIST, CIFAR-10, and CIFAR-100 from the DeepOBS
benchmark [62], and deep CNNs on ImageNet (AlexNet, ResNet18, InceptionV3, MobileNetV2,
ResNext101). Regarding the hyper-parameters, we make the following observations:"
REFERENCES,0.5293542074363993,"• Many CNNs do not use a bias term. This is because the output of those layers feeds directly
into a batch normalization layer, which is invariant under the addition of a bias term.
• All investigated convolutions use default dilation.
• Group convolutions are rarely used. MobileNetV2 and ConvNeXt-base (Tables E4g and E4i)
use group convolutions that interpret each individual channel as a group. ResNext101
(Table E4f) uses group convolutions that interpret a collection of channels as a group.
ConvNeXt-base (Table E4g) uses dense convolutions with P = 0 and S = K ∈{2, 4}.
• Many networks use dense convolutions, that is convolutions with unit kernel size (K = 1),
unit stride (S = 1), and no padding (P = 0). These convolutions have a trivial index
pattern and can therefore be simplified.
• InceptionV3 (Table E4h) uses two-dimensional convolutions with one trivial dimension
(‘mixed dense’) with unit kernel size, unit stride, and no padding along one direction. For
this spatial dimension, the index pattern can be simplified.
• ResNet18 (Table E4e) and ResNext101 (Table E4f) use convolutions with S > K for
down-sampling whose kernel only overlaps with a fraction of the input. The index pattern
can be simplified."
REFERENCES,0.5303326810176126,"Table E4: Hyper-parameters of convolutions in different CNNs. For convolutions with identical
hyper-parameters, we only show one instance and its multiplicity."
REFERENCES,0.5313111545988258,"(a) 3c3d, CIFAR-10 (3, 32, 32)"
REFERENCES,0.5322896281800391,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.5332681017612525,"conv1.0 (1)
(3, 32, 32)
(64, 28, 28)
(5, 5)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General
conv2.0 (1)
(64, 14, 14)
(96, 12, 12)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General
conv3.1 (1)
(96, 8, 8)
(128, 6, 6)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General"
REFERENCES,0.5342465753424658,"(b) 2c2d, Fashion MNIST (1, 28, 28)"
REFERENCES,0.5352250489236791,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.5362035225048923,"conv1.1 (1)
(1, 32, 32)
(32, 28, 28)
(5, 5)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General
conv2.1 (1)
(32, 18, 18)
(64, 14, 14)
(5, 5)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General"
REFERENCES,0.5371819960861057,"(c) All-CNN-C, CIFAR-100 (3, 32, 32)"
REFERENCES,0.538160469667319,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.5391389432485323,"conv1.1 (1)
(3, 34, 34)
(96, 32, 32)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General
conv2.1 (1)
(96, 34, 34)
(96, 32, 32)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General
conv3.1 (1)
(96, 33, 33)
(96, 16, 16)
(3, 3)
(2, 2)
(0, 0)
(1, 1)
1
Yes
General
conv4.1 (1)
(96, 18, 18)
(192, 16, 16)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General
conv5.1 (1)
(192, 18, 18)
(192, 16, 16)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General
conv6.1 (1)
(192, 17, 17)
(192, 8, 8)
(3, 3)
(2, 2)
(0, 0)
(1, 1)
1
Yes
General
conv7.0 (1)
(192, 8, 8)
(192, 6, 6)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
Yes
General
conv8.1 (1)
(192, 6, 6)
(192, 6, 6)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
Yes
Dense
conv9.1 (1)
(192, 6, 6)
(100, 6, 6)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
Yes
Dense"
REFERENCES,0.5401174168297456,"(d) AlexNet, ImageNet (3, 256, 256)"
REFERENCES,0.541095890410959,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.5420743639921722,"features.0 (1)
(3, 256, 256)
(64, 63, 63)
(11, 11)
(4, 4)
(2, 2)
(1, 1)
1
Yes
General
features.3 (1)
(64, 31, 31)
(192, 31, 31)
(5, 5)
(1, 1)
(2, 2)
(1, 1)
1
Yes
General
features.6 (1)
(192, 15, 15)
(384, 15, 15)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
Yes
General
features.8 (1)
(384, 15, 15)
(256, 15, 15)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
Yes
General
features.10 (1)
(256, 15, 15)
(256, 15, 15)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
Yes
General"
REFERENCES,0.5430528375733855,"(e) ResNet18, ImageNet (3, 256, 256)"
REFERENCES,0.5440313111545988,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.5450097847358122,"conv1 (1)
(3, 256, 256)
(64, 128, 128)
(7, 7)
(2, 2)
(3, 3)
(1, 1)
1
No
General
layer1.0.conv1 (4)
(64, 64, 64)
(64, 64, 64)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
No
General
layer2.0.conv1 (1)
(64, 64, 64)
(128, 32, 32)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
1
No
General
layer2.0.conv2 (3)
(128, 32, 32)
(128, 32, 32)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
No
General
layer2.0.downsample.0 (1)
(64, 64, 64)
(128, 32, 32)
(1, 1)
(2, 2)
(0, 0)
(1, 1)
1
No
Down
layer3.0.conv1 (1)
(128, 32, 32)
(256, 16, 16)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
1
No
General
layer3.0.conv2 (3)
(256, 16, 16)
(256, 16, 16)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
No
General
layer3.0.downsample.0 (1)
(128, 32, 32)
(256, 16, 16)
(1, 1)
(2, 2)
(0, 0)
(1, 1)
1
No
Down
layer4.0.conv1 (1)
(256, 16, 16)
(512, 8, 8)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
1
No
General
layer4.0.conv2 (3)
(512, 8, 8)
(512, 8, 8)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
No
General
layer4.0.downsample.0 (1)
(256, 16, 16)
(512, 8, 8)
(1, 1)
(2, 2)
(0, 0)
(1, 1)
1
No
Down"
REFERENCES,0.5459882583170255,"(f) ResNext101_32x8d, ImageNet (3, 256, 256)"
REFERENCES,0.5469667318982387,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.547945205479452,"conv1 (1)
(3, 256, 256)
(64, 128, 128)
(7, 7)
(2, 2)
(3, 3)
(1, 1)
1
No
General
layer1.0.conv1 (2)
(64, 64, 64)
(256, 64, 64)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
layer1.0.conv2 (3)
(256, 64, 64)
(256, 64, 64)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
32
No
General
layer1.0.conv3 (5)
(256, 64, 64)
(256, 64, 64)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
layer2.0.conv1 (1)
(256, 64, 64)
(512, 64, 64)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
layer2.0.conv2 (1)
(512, 64, 64)
(512, 32, 32)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
32
No
General
layer2.0.conv3 (7)
(512, 32, 32)
(512, 32, 32)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
layer2.0.downsample.0 (1)
(256, 64, 64)
(512, 32, 32)
(1, 1)
(2, 2)
(0, 0)
(1, 1)
1
No
Down
layer2.1.conv2 (3)
(512, 32, 32)
(512, 32, 32)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
32
No
General
layer3.0.conv1 (1)
(512, 32, 32)
(1024, 32, 32)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
layer3.0.conv2 (1)
(1024, 32, 32)
(1024, 16, 16)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
32
No
General
layer3.0.conv3 (45)
(1024, 16, 16)
(1024, 16, 16)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
layer3.0.downsample.0 (1)
(512, 32, 32)
(1024, 16, 16)
(1, 1)
(2, 2)
(0, 0)
(1, 1)
1
No
Down
layer3.1.conv2 (22)
(1024, 16, 16)
(1024, 16, 16)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
32
No
General
layer4.0.conv1 (1)
(1024, 16, 16)
(2048, 16, 16)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
layer4.0.conv2 (1)
(2048, 16, 16)
(2048, 8, 8)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
32
No
General
layer4.0.conv3 (5)
(2048, 8, 8)
(2048, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
layer4.0.downsample.0 (1)
(1024, 16, 16)
(2048, 8, 8)
(1, 1)
(2, 2)
(0, 0)
(1, 1)
1
No
Down
layer4.1.conv2 (2)
(2048, 8, 8)
(2048, 8, 8)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
32
No
General"
REFERENCES,0.5489236790606654,"(g) ConvNeXt-base, ImageNet (3, 256, 256)"
REFERENCES,0.5499021526418787,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.550880626223092,"features.0.0 (1)
(3, 256, 256)
(128, 64, 64)
(4, 4)
(4, 4)
(0, 0)
(1, 1)
1
Yes
Dense
features.1.0.block.0 (3)
(128, 64, 64)
(128, 64, 64)
(7, 7)
(1, 1)
(3, 3)
(1, 1)
128
Yes
General
features.2.1 (1)
(128, 64, 64)
(256, 32, 32)
(2, 2)
(2, 2)
(0, 0)
(1, 1)
1
Yes
Dense
features.3.0.block.0 (3)
(256, 32, 32)
(256, 32, 32)
(7, 7)
(1, 1)
(3, 3)
(1, 1)
256
Yes
General
features.4.1 (1)
(256, 32, 32)
(512, 16, 16)
(2, 2)
(2, 2)
(0, 0)
(1, 1)
1
Yes
Dense
features.5.0.block.0 (27)
(512, 16, 16)
(512, 16, 16)
(7, 7)
(1, 1)
(3, 3)
(1, 1)
512
Yes
General
features.6.1 (1)
(512, 16, 16)
(1024, 8, 8)
(2, 2)
(2, 2)
(0, 0)
(1, 1)
1
Yes
Dense
features.7.0.block.0 (3)
(1024, 8, 8)
(1024, 8, 8)
(7, 7)
(1, 1)
(3, 3)
(1, 1)
1024
Yes
General"
REFERENCES,0.5518590998043053,"(h) InceptionV3, ImageNet (3, 299, 299)"
REFERENCES,0.5528375733855186,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.5538160469667319,"Conv2d_1a_3x3.conv (1)
(3, 299, 299)
(32, 149, 149)
(3, 3)
(2, 2)
(0, 0)
(1, 1)
1
No
General
Conv2d_2a_3x3.conv (1)
(32, 149, 149)
(32, 147, 147)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
No
General
Conv2d_2b_3x3.conv (1)
(32, 147, 147)
(64, 147, 147)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
No
General
Conv2d_3b_1x1.conv (1)
(64, 73, 73)
(80, 73, 73)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Conv2d_4a_3x3.conv (1)
(80, 73, 73)
(192, 71, 71)
(3, 3)
(1, 1)
(0, 0)
(1, 1)
1
No
General
Mixed_5b.branch1x1.conv (2)
(192, 35, 35)
(64, 35, 35)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_5b.branch5x5_1.conv (1)
(192, 35, 35)
(48, 35, 35)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_5b.branch5x5_2.conv (3)
(48, 35, 35)
(64, 35, 35)
(5, 5)
(1, 1)
(2, 2)
(1, 1)
1
No
General
Mixed_5b.branch3x3dbl_2.conv (4)
(64, 35, 35)
(96, 35, 35)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
No
General
Mixed_5b.branch3x3dbl_3.conv (3)
(96, 35, 35)
(96, 35, 35)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
No
General
Mixed_5b.branch_pool.conv (1)
(192, 35, 35)
(32, 35, 35)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_5c.branch1x1.conv (3)
(256, 35, 35)
(64, 35, 35)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_5c.branch5x5_1.conv (1)
(256, 35, 35)
(48, 35, 35)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_5d.branch1x1.conv (4)
(288, 35, 35)
(64, 35, 35)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_5d.branch5x5_1.conv (1)
(288, 35, 35)
(48, 35, 35)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_6a.branch3x3.conv (1)
(288, 35, 35)
(384, 17, 17)
(3, 3)
(2, 2)
(0, 0)
(1, 1)
1
No
General
Mixed_6a.branch3x3dbl_3.conv (1)
(96, 35, 35)
(96, 17, 17)
(3, 3)
(2, 2)
(0, 0)
(1, 1)
1
No
General
Mixed_6b.branch1x1.conv (12)
(768, 17, 17)
(192, 17, 17)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_6b.branch7x7_1.conv (2)
(768, 17, 17)
(128, 17, 17)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_6b.branch7x7_2.conv (2)
(128, 17, 17)
(128, 17, 17)
(1, 7)
(1, 1)
(0, 3)
(1, 1)
1
No
Dense mix
Mixed_6b.branch7x7_3.conv (1)
(128, 17, 17)
(192, 17, 17)
(7, 1)
(1, 1)
(3, 0)
(1, 1)
1
No
Dense mix
Mixed_6b.branch7x7dbl_2.conv (2)
(128, 17, 17)
(128, 17, 17)
(7, 1)
(1, 1)
(3, 0)
(1, 1)
1
No
Dense mix
Mixed_6b.branch7x7dbl_5.conv (1)
(128, 17, 17)
(192, 17, 17)
(1, 7)
(1, 1)
(0, 3)
(1, 1)
1
No
Dense mix
Mixed_6c.branch7x7_1.conv (4)
(768, 17, 17)
(160, 17, 17)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_6c.branch7x7_2.conv (4)
(160, 17, 17)
(160, 17, 17)
(1, 7)
(1, 1)
(0, 3)
(1, 1)
1
No
Dense mix
Mixed_6c.branch7x7_3.conv (2)
(160, 17, 17)
(192, 17, 17)
(7, 1)
(1, 1)
(3, 0)
(1, 1)
1
No
Dense mix
Mixed_6c.branch7x7dbl_2.conv (4)
(160, 17, 17)
(160, 17, 17)
(7, 1)
(1, 1)
(3, 0)
(1, 1)
1
No
Dense mix
Mixed_6c.branch7x7dbl_5.conv (2)
(160, 17, 17)
(192, 17, 17)
(1, 7)
(1, 1)
(0, 3)
(1, 1)
1
No
Dense mix
Mixed_6e.branch7x7_2.conv (4)
(192, 17, 17)
(192, 17, 17)
(1, 7)
(1, 1)
(0, 3)
(1, 1)
1
No
Dense mix
Mixed_6e.branch7x7_3.conv (4)
(192, 17, 17)
(192, 17, 17)
(7, 1)
(1, 1)
(3, 0)
(1, 1)
1
No
Dense mix
AuxLogits.conv0.conv (1)
(768, 5, 5)
(128, 5, 5)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
AuxLogits.conv1.conv (1)
(128, 5, 5)
(768, 1, 1)
(5, 5)
(1, 1)
(0, 0)
(1, 1)
1
No
General
Mixed_7a.branch3x3_2.conv (1)
(192, 17, 17)
(320, 8, 8)
(3, 3)
(2, 2)
(0, 0)
(1, 1)
1
No
General
Mixed_7a.branch7x7x3_4.conv (1)
(192, 17, 17)
(192, 8, 8)
(3, 3)
(2, 2)
(0, 0)
(1, 1)
1
No
General
Mixed_7b.branch1x1.conv (1)
(1280, 8, 8)
(320, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_7b.branch3x3_1.conv (1)
(1280, 8, 8)
(384, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_7b.branch3x3_2a.conv (4)
(384, 8, 8)
(384, 8, 8)
(1, 3)
(1, 1)
(0, 1)
(1, 1)
1
No
Dense mix
Mixed_7b.branch3x3_2b.conv (4)
(384, 8, 8)
(384, 8, 8)
(3, 1)
(1, 1)
(1, 0)
(1, 1)
1
No
Dense mix
Mixed_7b.branch3x3dbl_1.conv (1)
(1280, 8, 8)
(448, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_7b.branch3x3dbl_2.conv (2)
(448, 8, 8)
(384, 8, 8)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
1
No
General
Mixed_7b.branch_pool.conv (1)
(1280, 8, 8)
(192, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_7c.branch1x1.conv (1)
(2048, 8, 8)
(320, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_7c.branch3x3_1.conv (1)
(2048, 8, 8)
(384, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_7c.branch3x3dbl_1.conv (1)
(2048, 8, 8)
(448, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
Mixed_7c.branch_pool.conv (1)
(2048, 8, 8)
(192, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense"
REFERENCES,0.5547945205479452,"(i) MobileNetV2, ImageNet (3, 256, 256)"
REFERENCES,0.5557729941291585,"Name (count)
Input shape
Output shape
Kernel
Stride
Padding
Dilation
Groups
Bias
Type"
REFERENCES,0.5567514677103719,"features.0.0 (1)
(3, 256, 256)
(32, 128, 128)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
1
No
General
features.1.conv.0.0 (1)
(32, 128, 128)
(32, 128, 128)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
32
No
General
features.1.conv.1 (1)
(32, 128, 128)
(16, 128, 128)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.2.conv.0.0 (1)
(16, 128, 128)
(96, 128, 128)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.2.conv.1.0 (1)
(96, 128, 128)
(96, 64, 64)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
96
No
General
features.2.conv.2 (1)
(96, 64, 64)
(24, 64, 64)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.3.conv.0.0 (2)
(24, 64, 64)
(144, 64, 64)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.3.conv.1.0 (1)
(144, 64, 64)
(144, 64, 64)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
144
No
General
features.3.conv.2 (1)
(144, 64, 64)
(24, 64, 64)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.4.conv.1.0 (1)
(144, 64, 64)
(144, 32, 32)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
144
No
General
features.4.conv.2 (1)
(144, 32, 32)
(32, 32, 32)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.5.conv.0.0 (3)
(32, 32, 32)
(192, 32, 32)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.5.conv.1.0 (2)
(192, 32, 32)
(192, 32, 32)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
192
No
General
features.5.conv.2 (2)
(192, 32, 32)
(32, 32, 32)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.7.conv.1.0 (1)
(192, 32, 32)
(192, 16, 16)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
192
No
General
features.7.conv.2 (1)
(192, 16, 16)
(64, 16, 16)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.8.conv.0.0 (4)
(64, 16, 16)
(384, 16, 16)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.8.conv.1.0 (4)
(384, 16, 16)
(384, 16, 16)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
384
No
General
features.8.conv.2 (3)
(384, 16, 16)
(64, 16, 16)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.11.conv.2 (1)
(384, 16, 16)
(96, 16, 16)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.12.conv.0.0 (3)
(96, 16, 16)
(576, 16, 16)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.12.conv.1.0 (2)
(576, 16, 16)
(576, 16, 16)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
576
No
General
features.12.conv.2 (2)
(576, 16, 16)
(96, 16, 16)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.14.conv.1.0 (1)
(576, 16, 16)
(576, 8, 8)
(3, 3)
(2, 2)
(1, 1)
(1, 1)
576
No
General
features.14.conv.2 (1)
(576, 8, 8)
(160, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.15.conv.0.0 (3)
(160, 8, 8)
(960, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.15.conv.1.0 (3)
(960, 8, 8)
(960, 8, 8)
(3, 3)
(1, 1)
(1, 1)
(1, 1)
960
No
General
features.15.conv.2 (2)
(960, 8, 8)
(160, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.17.conv.2 (1)
(960, 8, 8)
(320, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense
features.18.0 (1)
(320, 8, 8)
(1280, 8, 8)
(1, 1)
(1, 1)
(0, 0)
(1, 1)
1
No
Dense"
REFERENCES,0.5577299412915852,Forward
REFERENCES,0.5587084148727984,Input VJP
REFERENCES,0.5596868884540117,Weight VJP KFC
REFERENCES,0.5606653620352251,KFAC-red.
REFERENCES,0.5616438356164384,Forward
REFERENCES,0.5626223091976517,Input VJP
REFERENCES,0.5636007827788649,Weight VJP KFC
REFERENCES,0.5645792563600783,KFAC-red.
REFERENCES,0.5655577299412916,Forward
REFERENCES,0.5665362035225049,Input VJP
REFERENCES,0.5675146771037182,Weight VJP KFC
REFERENCES,0.5684931506849316,KFAC-red.
REFERENCES,0.5694716242661448,Forward
REFERENCES,0.5704500978473581,Input VJP
REFERENCES,0.5714285714285714,Weight VJP KFC
REFERENCES,0.5724070450097848,KFAC-red. 10−1 100 101
REFERENCES,0.5733855185909981,"General
Dense mix
Dense
Down"
REFERENCES,0.5743639921722113,TN versus PT (logarithmic)
REFERENCES,0.5753424657534246,"Figure F17: Benchmark overview. We measure the performance ratios of our TN implementation
w.r.t. a base line in PyTorch (PT). Blue boxes show the performance ratios of TN versus PT, second-
color boxes show the performance ratios of TN+opt versus PT."
REFERENCES,0.576320939334638,"F
Run Time Evaluation Details (GPU)"
REFERENCES,0.5772994129158513,"Here we provide all details on the run time evaluation from the main text. We consider the convolutions
from the CNNs from §E. Experiments were carried out on an Nvidia Tesla T4 (16 GB memory). We
use a batch size of 32 for the ImageNet architectures, and 128 for the others."
REFERENCES,0.5782778864970646,"F.1
Protocol & Overview"
REFERENCES,0.5792563600782779,"We compare different implementations of the same operations in PyTorch. The base line (refer-
enced by ‘PT’) uses PyTorch’s built-in functionalities for convolutions and related operations, such
as torch.nn.functional.conv2d (forward), torch.nn.functional.unfold (KFC, KFAC-
reduce), and PyTorch’s built-in automatic differentiation torch.autograd.grad (VJPs)."
REFERENCES,0.5802348336594912,"Our TN implementation (referenced by ‘TN’) sets up operands and the string-valued equation for
each routine. Optionally, we can apply the simplifications from §4 as a post-processing step before
contraction, which yields a modified equation and operand list (‘TN + opt’). Finally, we determine the
contraction path using opt_einsum.contract_path and perform the contraction with its PyTorch
back-end (opt_einsum.contract). We only measure the contraction time as in practical settings,
the contraction path search would be executed once, then cached. We also exclude final operations to
obtain the correct shape or scale (flattening, reshaping, scaling by constant) in all implementations
(including the base line)."
REFERENCES,0.5812133072407045,"For each operation and each convolution layer, we perform 50 independent repetitions and report
the minimum time in tables. To summarize those tables, we extract the performance ratios, that is
the TN implementation’s run time divided by the base line’s. Ratios larger than 1 mean that the
TN implementation is slower, ratios smaller than 1 indicate that it is faster than the base line. We
collect those ratios for the different convolution types (general, mixed dense, dense, sub-sampling)
and display them separately using box plots. Each operation has two boxes, corresponding to
the un-simplified (TN), and the simplified (TN + opt) implementation. For the box plots, we use
matplotlib’s default settings (a box extends from the first quartile to the third quartile of the data,
with a line at the median; whiskers extend from the box by 1.5x the inter-quartile range; flier points are
those past the end of the whiskers). Figure F17 summarizes the entire GPU benchmark. Figure F18
shows the same information with each convolution type as an individual plot."
REFERENCES,0.5821917808219178,(a) Mixed dense
REFERENCES,0.5831702544031311,"Forward Input VJPWeight VJP
KFC
KFAC-red. 1 3.16"
REFERENCES,0.5841487279843445,TN versus PT (logarithmic)
REFERENCES,0.5851272015655578,(b) Dense
REFERENCES,0.586105675146771,"Forward Input VJPWeight VJP
KFC
KFAC-red. 1"
REFERENCES,0.5870841487279843,TN versus PT (logarithmic)
REFERENCES,0.5880626223091977,(c) Down-sampling
REFERENCES,0.589041095890411,"Forward Input VJPWeight VJP
KFC
KFAC-red. 1 3.16"
REFERENCES,0.5900195694716243,TN versus PT (logarithmic)
REFERENCES,0.5909980430528375,(d) General
REFERENCES,0.5919765166340509,"Forward Input VJPWeight VJP
KFC
KFAC-red. 0.1 1 10"
REFERENCES,0.5929549902152642,TN versus PT (logarithmic)
REFERENCES,0.5939334637964775,"Figure F18: Impact of TN simplifications (non-simplified performance ratios shown in blue). TN
simplifications improve performance on (a) mixed dense, (b) dense, and (c) down-sampling convolu-
tions. (d) General convolutions are not affected by TN simplifications."
REFERENCES,0.5949119373776908,"F.2
Forward Pass"
REFERENCES,0.5958904109589042,"We compare TN and TN+opt with PyTorch’s torch.nn.functional.conv2d. Figure F19 visual-
izes the performance ratios for different convolution categories. Table F5 contains the detailed run
times and performance factors."
REFERENCES,0.5968688845401174,"General
Dense mix
Dense
Down 100 101"
REFERENCES,0.5978473581213307,TN versus PT (logarithmic)
REFERENCES,0.598825831702544,"Figure F19: Forward pass performance ratios of TN versus PT and TN+opt versus PT for different
convolution types on GPU."
REFERENCES,0.5998043052837574,Table F5: Forward pass performance comparison on GPU.
REFERENCES,0.6007827788649707,"(a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32)"
REFERENCES,0.601761252446184,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6027397260273972,"conv1.0
1.26 · 10−3
4.63 · 10−4
2.73 x
conv2.0
1.91 · 10−3
4.52 · 10−4
4.22 x
conv3.1
1.21 · 10−3
4.11 · 10−4
2.94 x"
REFERENCES,0.6037181996086106,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6046966731898239,"1.23 · 10−3
4.64 · 10−4
2.64 x
1.79 · 10−3
4.53 · 10−4
3.95 x
1.16 · 10−3
4.10 · 10−4
2.83 x"
REFERENCES,0.6056751467710372,"(b) F-MNIST 2c2d, input shape (128, 1, 28, 28)"
REFERENCES,0.6066536203522505,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6076320939334638,"conv1.1
8.21 · 10−4
2.25 · 10−4
3.65 x
conv2.1
3.56 · 10−3
7.43 · 10−4
4.79 x"
REFERENCES,0.6086105675146771,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6095890410958904,"7.67 · 10−4
2.25 · 10−4
3.41 x
3.24 · 10−3
7.83 · 10−4
4.14 x"
REFERENCES,0.6105675146771037,"(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32)"
REFERENCES,0.6115459882583171,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6125244618395304,"conv1.1
1.01 · 10−3
4.20 · 10−4
2.41 x
conv2.1
1.94 · 10−2
3.09 · 10−3
6.26 x
conv3.1
8.56 · 10−3
2.86 · 10−3
3.00 x
conv4.1
8.58 · 10−3
1.75 · 10−3
4.91 x
conv5.1
1.67 · 10−2
2.91 · 10−3
5.74 x
conv6.1
5.13 · 10−3
2.24 · 10−3
2.29 x
conv7.0
2.58 · 10−3
8.26 · 10−4
3.12 x
conv8.1
8.20 · 10−4
2.96 · 10−4
2.77 x
conv9.1
7.52 · 10−4
2.35 · 10−4
3.19 x"
REFERENCES,0.6135029354207436,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6144814090019569,"9.45 · 10−4
4.19 · 10−4
2.25 x
1.88 · 10−2
3.10 · 10−3
6.08 x
7.77 · 10−3
2.86 · 10−3
2.72 x
7.77 · 10−3
1.75 · 10−3
4.45 x
1.51 · 10−2
2.91 · 10−3
5.19 x
5.08 · 10−3
2.24 · 10−3
2.27 x
2.51 · 10−3
8.27 · 10−4
3.03 x
3.42 · 10−4
2.97 · 10−4
1.15 x
3.01 · 10−4
2.35 · 10−4
1.28 x"
REFERENCES,0.6154598825831703,"(d) Alexnet, input shape (32, 3, 256, 256)"
REFERENCES,0.6164383561643836,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6174168297455969,"features.0
1.83 · 10−2
2.45 · 10−3
7.47 x
features.3
7.43 · 10−3
2.67 · 10−3
2.79 x
features.6
4.68 · 10−3
1.04 · 10−3
4.52 x
features.8
6.15 · 10−3
1.86 · 10−3
3.31 x
features.10
4.41 · 10−3
1.31 · 10−3
3.36 x"
REFERENCES,0.6183953033268101,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6193737769080235,"1.79 · 10−2
2.44 · 10−3
7.35 x
7.27 · 10−3
2.85 · 10−3
2.55 x
3.22 · 10−3
1.02 · 10−3
3.14 x
6.16 · 10−3
1.84 · 10−3
3.34 x
4.38 · 10−3
1.31 · 10−3
3.35 x"
REFERENCES,0.6203522504892368,"(e) ResNet18, input shape (32, 3, 256, 256)"
REFERENCES,0.6213307240704501,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6223091976516634,"conv1
1.44 · 10−2
4.07 · 10−3
3.53 x
layer1.0.conv1
1.05 · 10−2
1.78 · 10−3
5.91 x
layer2.0.conv1
6.44 · 10−3
1.89 · 10−3
3.41 x
layer2.0.conv2
6.88 · 10−3
1.51 · 10−3
4.54 x
layer2.0.downsample.0
1.60 · 10−3
3.79 · 10−4
4.23 x
layer3.0.conv1
3.82 · 10−3
2.00 · 10−3
1.91 x
layer3.0.conv2
5.02 · 10−3
1.30 · 10−3
3.85 x
layer3.0.downsample.0
1.10 · 10−3
3.78 · 10−4
2.91 x
layer4.0.conv1
2.87 · 10−3
2.36 · 10−3
1.21 x
layer4.0.conv2
4.47 · 10−3
1.40 · 10−3
3.18 x
layer4.0.downsample.0
9.90 · 10−4
3.81 · 10−4
2.60 x"
REFERENCES,0.6232876712328768,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.62426614481409,"1.44 · 10−2
4.08 · 10−3
3.53 x
1.05 · 10−2
1.79 · 10−3
5.87 x
6.46 · 10−3
1.89 · 10−3
3.42 x
6.91 · 10−3
1.52 · 10−3
4.54 x
5.19 · 10−4
3.80 · 10−4
1.37 x
3.56 · 10−3
2.01 · 10−3
1.77 x
5.05 · 10−3
1.31 · 10−3
3.87 x
5.61 · 10−4
3.79 · 10−4
1.48 x
2.86 · 10−3
2.36 · 10−3
1.21 x
4.51 · 10−3
1.40 · 10−3
3.21 x
5.16 · 10−4
3.83 · 10−4
1.35 x"
REFERENCES,0.6252446183953033,"(f) ResNext101, input shape (32, 3, 256, 256)"
REFERENCES,0.6262230919765166,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.62720156555773,"conv1
1.45 · 10−2
4.07 · 10−3
3.57 x
layer1.0.conv1
4.31 · 10−3
1.22 · 10−3
3.54 x
layer1.0.conv2
3.03 · 10−2
9.86 · 10−3
3.07 x
layer1.0.conv3
1.51 · 10−2
6.54 · 10−3
2.31 x
layer2.0.conv1
2.08 · 10−2
1.29 · 10−2
1.61 x
layer2.0.conv2
3.33 · 10−2
4.93 · 10−3
6.75 x
layer2.0.conv3
1.05 · 10−2
6.24 · 10−3
1.69 x
layer2.0.downsample.0
7.65 · 10−3
3.30 · 10−3
2.31 x
layer2.1.conv2
1.50 · 10−2
4.59 · 10−3
3.27 x
layer3.0.conv1
1.67 · 10−2
1.23 · 10−2
1.35 x
layer3.0.conv2
1.76 · 10−2
2.65 · 10−3
6.65 x
layer3.0.conv3
8.27 · 10−3
6.14 · 10−3
1.35 x
layer3.0.downsample.0
5.58 · 10−3
3.20 · 10−3
1.74 x
layer3.1.conv2
7.64 · 10−3
2.49 · 10−3
3.07 x
layer4.0.conv1
1.43 · 10−2
1.22 · 10−2
1.18 x
layer4.0.conv2
8.07 · 10−3
2.02 · 10−3
3.99 x
layer4.0.conv3
7.85 · 10−3
6.28 · 10−3
1.25 x
layer4.0.downsample.0
4.73 · 10−3
3.44 · 10−3
1.37 x
layer4.1.conv2
4.76 · 10−3
1.36 · 10−3
3.51 x"
REFERENCES,0.6281800391389433,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6291585127201565,"1.44 · 10−2
4.07 · 10−3
3.54 x
2.26 · 10−3
1.22 · 10−3
1.85 x
3.03 · 10−2
9.86 · 10−3
3.08 x
7.49 · 10−3
6.54 · 10−3
1.15 x
1.36 · 10−2
1.29 · 10−2
1.05 x
3.33 · 10−2
4.93 · 10−3
6.75 x
6.84 · 10−3
6.24 · 10−3
1.10 x
3.71 · 10−3
3.31 · 10−3
1.12 x
1.50 · 10−2
4.59 · 10−3
3.27 x
1.28 · 10−2
1.23 · 10−2
1.04 x
1.76 · 10−2
2.66 · 10−3
6.65 x
6.44 · 10−3
6.14 · 10−3
1.05 x
3.42 · 10−3
3.20 · 10−3
1.07 x
7.64 · 10−3
2.48 · 10−3
3.07 x
1.24 · 10−2
1.22 · 10−2
1.02 x
8.08 · 10−3
2.02 · 10−3
4.00 x
6.33 · 10−3
6.28 · 10−3
1.01 x
3.34 · 10−3
3.44 · 10−3
0.97 x
4.77 · 10−3
1.35 · 10−3
3.52 x"
REFERENCES,0.6301369863013698,"(g) ConvNeXt-base, input shape (32, 3, 256, 256)"
REFERENCES,0.6311154598825832,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6320939334637965,"features.0.0
4.26 · 10−3
9.88 · 10−4
4.31 x
features.1.0.block.0
5.07 · 10−2
7.61 · 10−3
6.66 x
features.2.1
7.60 · 10−3
3.21 · 10−3
2.37 x
features.3.0.block.0
2.36 · 10−2
3.81 · 10−3
6.18 x
features.4.1
5.41 · 10−3
3.38 · 10−3
1.60 x
features.5.0.block.0
1.11 · 10−2
1.94 · 10−3
5.70 x
features.6.1
4.54 · 10−3
3.69 · 10−3
1.23 x
features.7.0.block.0
1.06 · 10−3
1.01 · 10−3
1.05 x"
REFERENCES,0.6330724070450098,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6340508806262231,"1.20 · 10−3
9.94 · 10−4
1.21 x
5.07 · 10−2
7.61 · 10−3
6.66 x
3.89 · 10−3
3.20 · 10−3
1.21 x
2.35 · 10−2
3.81 · 10−3
6.17 x
3.52 · 10−3
3.38 · 10−3
1.04 x
1.10 · 10−2
1.94 · 10−3
5.69 x
3.44 · 10−3
3.70 · 10−3
0.93 x
1.02 · 10−3
1.01 · 10−3
1.01 x"
REFERENCES,0.6350293542074364,"(h) InceptionV3, input shape (32, 3, 299, 299)"
REFERENCES,0.6360078277886497,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.636986301369863,"Conv2d_1a_3x3.conv
1.02 · 10−2
9.85 · 10−4
10.35 x
Conv2d_2a_3x3.conv
3.23 · 10−2
5.14 · 10−3
6.30 x
Conv2d_2b_3x3.conv
4.83 · 10−2
8.14 · 10−3
5.93 x
Conv2d_3b_1x1.conv
4.96 · 10−3
1.17 · 10−3
4.24 x
Conv2d_4a_3x3.conv
3.69 · 10−2
7.64 · 10−3
4.83 x
Mixed_5b.branch1x1.conv
1.85 · 10−3
5.04 · 10−4
3.68 x
Mixed_5b.branch5x5_1.conv
1.64 · 10−3
4.97 · 10−4
3.30 x
Mixed_5b.branch5x5_2.conv
5.01 · 10−3
1.23 · 10−3
4.07 x
Mixed_5b.branch3x3dbl_2.conv
4.40 · 10−3
1.31 · 10−3
3.38 x
Mixed_5b.branch3x3dbl_3.conv
5.82 · 10−3
1.66 · 10−3
3.50 x
Mixed_5b.branch_pool.conv
1.33 · 10−3
3.26 · 10−4
4.09 x
Mixed_5c.branch1x1.conv
2.08 · 10−3
6.41 · 10−4
3.24 x
Mixed_5c.branch5x5_1.conv
1.87 · 10−3
6.29 · 10−4
2.97 x
Mixed_5d.branch1x1.conv
2.18 · 10−3
6.99 · 10−4
3.12 x
Mixed_5d.branch5x5_1.conv
1.96 · 10−3
6.91 · 10−4
2.84 x
Mixed_6a.branch3x3.conv
1.15 · 10−2
7.12 · 10−3
1.61 x
Mixed_6a.branch3x3dbl_3.conv
2.61 · 10−3
8.99 · 10−4
2.90 x
Mixed_6b.branch1x1.conv
2.16 · 10−3
1.22 · 10−3
1.77 x
Mixed_6b.branch7x7_1.conv
1.67 · 10−3
8.15 · 10−4
2.05 x
Mixed_6b.branch7x7_2.conv
2.14 · 10−3
8.04 · 10−4
2.66 x
Mixed_6b.branch7x7_3.conv
2.59 · 10−3
1.06 · 10−3
2.45 x
Mixed_6b.branch7x7dbl_2.conv
2.17 · 10−3
7.88 · 10−4
2.76 x
Mixed_6b.branch7x7dbl_5.conv
2.63 · 10−3
1.07 · 10−3
2.46 x
Mixed_6c.branch7x7_1.conv
2.05 · 10−3
1.16 · 10−3
1.77 x
Mixed_6c.branch7x7_2.conv
3.19 · 10−3
1.12 · 10−3
2.84 x
Mixed_6c.branch7x7_3.conv
3.12 · 10−3
1.25 · 10−3
2.50 x
Mixed_6c.branch7x7dbl_2.conv
3.25 · 10−3
1.10 · 10−3
2.96 x
Mixed_6c.branch7x7dbl_5.conv
3.19 · 10−3
1.28 · 10−3
2.49 x
Mixed_6e.branch7x7_2.conv
3.78 · 10−3
1.48 · 10−3
2.54 x
Mixed_6e.branch7x7_3.conv
3.87 · 10−3
1.45 · 10−3
2.66 x
AuxLogits.conv0.conv
6.40 · 10−4
2.38 · 10−4
2.69 x
AuxLogits.conv1.conv
8.06 · 10−4
1.53 · 10−3
0.53 x
Mixed_7a.branch3x3_2.conv
1.08 · 10−3
4.37 · 10−4
2.48 x
Mixed_7a.branch7x7x3_4.conv
1.54 · 10−3
8.89 · 10−4
1.73 x
Mixed_7b.branch1x1.conv
1.29 · 10−3
7.43 · 10−4
1.73 x
Mixed_7b.branch3x3_1.conv
1.47 · 10−3
1.03 · 10−3
1.42 x
Mixed_7b.branch3x3_2a.conv
1.49 · 10−3
9.36 · 10−4
1.59 x
Mixed_7b.branch3x3_2b.conv
1.46 · 10−3
9.37 · 10−4
1.56 x
Mixed_7b.branch3x3dbl_1.conv
1.67 · 10−3
1.04 · 10−3
1.61 x
Mixed_7b.branch3x3dbl_2.conv
3.18 · 10−3
9.82 · 10−4
3.23 x
Mixed_7b.branch_pool.conv
9.54 · 10−4
6.76 · 10−4
1.41 x
Mixed_7c.branch1x1.conv
1.68 · 10−3
1.08 · 10−3
1.56 x
Mixed_7c.branch3x3_1.conv
1.98 · 10−3
1.60 · 10−3
1.23 x
Mixed_7c.branch3x3dbl_1.conv
2.25 · 10−3
1.56 · 10−3
1.44 x
Mixed_7c.branch_pool.conv
1.25 · 10−3
1.04 · 10−3
1.20 x"
REFERENCES,0.6379647749510763,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6389432485322897,"1.01 · 10−2
9.79 · 10−4
10.30 x
3.19 · 10−2
5.16 · 10−3
6.18 x
4.78 · 10−2
8.14 · 10−3
5.87 x
1.72 · 10−3
1.17 · 10−3
1.48 x
3.65 · 10−2
7.64 · 10−3
4.77 x
8.17 · 10−4
5.03 · 10−4
1.62 x
8.11 · 10−4
4.99 · 10−4
1.63 x
4.83 · 10−3
1.23 · 10−3
3.94 x
4.31 · 10−3
1.31 · 10−3
3.30 x
5.66 · 10−3
1.66 · 10−3
3.40 x
7.04 · 10−4
3.27 · 10−4
2.15 x
1.03 · 10−3
6.40 · 10−4
1.61 x
1.03 · 10−3
6.30 · 10−4
1.63 x
1.13 · 10−3
6.98 · 10−4
1.62 x
1.13 · 10−3
6.88 · 10−4
1.64 x
1.07 · 10−2
7.13 · 10−3
1.51 x
2.36 · 10−3
9.00 · 10−4
2.62 x
1.41 · 10−3
1.22 · 10−3
1.15 x
1.10 · 10−3
8.16 · 10−4
1.35 x
1.76 · 10−3
8.05 · 10−4
2.19 x
2.27 · 10−3
1.06 · 10−3
2.15 x
1.78 · 10−3
7.88 · 10−4
2.26 x
2.25 · 10−3
1.07 · 10−3
2.11 x
1.41 · 10−3
1.16 · 10−3
1.21 x
2.72 · 10−3
1.12 · 10−3
2.42 x
2.76 · 10−3
1.25 · 10−3
2.21 x
2.75 · 10−3
1.10 · 10−3
2.51 x
2.73 · 10−3
1.29 · 10−3
2.12 x
3.21 · 10−3
1.48 · 10−3
2.16 x
3.26 · 10−3
1.46 · 10−3
2.24 x
3.20 · 10−4
2.39 · 10−4
1.34 x
6.98 · 10−4
1.52 · 10−3
0.46 x
1.09 · 10−3
5.01 · 10−4
2.18 x
1.52 · 10−3
8.88 · 10−4
1.71 x
8.76 · 10−4
7.43 · 10−4
1.18 x
1.02 · 10−3
1.03 · 10−3
0.99 x
1.26 · 10−3
9.38 · 10−4
1.34 x
1.28 · 10−3
9.37 · 10−4
1.37 x
1.17 · 10−3
1.04 · 10−3
1.13 x
3.21 · 10−3
9.83 · 10−4
3.26 x
6.30 · 10−4
6.75 · 10−4
0.93 x
1.27 · 10−3
1.08 · 10−3
1.18 x
1.51 · 10−3
1.60 · 10−3
0.94 x
1.73 · 10−3
1.56 · 10−3
1.11 x
9.35 · 10−4
1.04 · 10−3
0.90 x"
REFERENCES,0.639921722113503,"(i) MobileNetV2, input shape (32, 3, 256, 256)"
REFERENCES,0.6409001956947162,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6418786692759295,"features.0.0
6.91 · 10−3
7.23 · 10−4
9.55 x
features.1.conv.0.0
2.28 · 10−2
2.05 · 10−3
11.11 x
features.1.conv.1
5.64 · 10−3
7.61 · 10−4
7.42 x
features.2.conv.0.0
4.27 · 10−3
1.74 · 10−3
2.45 x
features.2.conv.1.0
3.31 · 10−2
1.69 · 10−3
19.55 x
features.2.conv.2
2.53 · 10−3
4.93 · 10−4
5.13 x
features.3.conv.0.0
1.78 · 10−3
7.88 · 10−4
2.26 x
features.3.conv.1.0
2.09 · 10−2
2.30 · 10−3
9.07 x
features.3.conv.2
2.93 · 10−3
6.33 · 10−4
4.63 x
features.4.conv.1.0
1.04 · 10−2
6.62 · 10−4
15.76 x
features.4.conv.2
1.10 · 10−3
2.61 · 10−4
4.23 x
features.5.conv.0.0
9.24 · 10−4
3.32 · 10−4
2.78 x
features.5.conv.1.0
5.44 · 10−3
7.87 · 10−4
6.91 x
features.5.conv.2
1.22 · 10−3
3.11 · 10−4
3.93 x
features.7.conv.1.0
2.38 · 10−3
2.49 · 10−4
9.58 x
features.7.conv.2
7.49 · 10−4
2.09 · 10−4
3.58 x
features.8.conv.0.0
8.05 · 10−4
2.91 · 10−4
2.77 x
features.8.conv.1.0
2.29 · 10−3
4.14 · 10−4
5.53 x
features.8.conv.2
7.98 · 10−4
3.07 · 10−4
2.60 x
features.11.conv.2
9.88 · 10−4
4.08 · 10−4
2.42 x
features.12.conv.0.0
1.06 · 10−3
4.92 · 10−4
2.16 x
features.12.conv.1.0
4.18 · 10−3
6.04 · 10−4
6.91 x
features.12.conv.2
1.16 · 10−3
5.53 · 10−4
2.10 x
features.14.conv.1.0
1.73 · 10−3
2.29 · 10−4
7.57 x
features.14.conv.2
6.95 · 10−4
3.90 · 10−4
1.78 x
features.15.conv.0.0
9.24 · 10−4
3.53 · 10−4
2.62 x
features.15.conv.1.0
1.49 · 10−3
2.72 · 10−4
5.46 x
features.15.conv.2
8.32 · 10−4
5.80 · 10−4
1.43 x
features.17.conv.2
1.12 · 10−3
9.74 · 10−4
1.15 x
features.18.0
1.25 · 10−3
7.31 · 10−4
1.71 x"
REFERENCES,0.6428571428571429,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6438356164383562,"6.92 · 10−3
7.24 · 10−4
9.56 x
2.28 · 10−2
2.06 · 10−3
11.09 x
1.56 · 10−3
7.57 · 10−4
2.06 x
2.02 · 10−3
1.74 · 10−3
1.16 x
3.31 · 10−2
1.69 · 10−3
19.56 x
1.08 · 10−3
4.99 · 10−4
2.16 x
9.63 · 10−4
7.88 · 10−4
1.22 x
2.09 · 10−2
2.30 · 10−3
9.06 x
1.47 · 10−3
6.34 · 10−4
2.33 x
1.04 · 10−2
6.63 · 10−4
15.72 x
5.03 · 10−4
2.61 · 10−4
1.92 x
5.07 · 10−4
3.33 · 10−4
1.52 x
5.42 · 10−3
7.88 · 10−4
6.88 x
6.16 · 10−4
3.11 · 10−4
1.98 x
2.37 · 10−3
2.51 · 10−4
9.44 x
3.20 · 10−4
2.10 · 10−4
1.53 x
4.42 · 10−4
2.92 · 10−4
1.51 x
2.27 · 10−3
4.15 · 10−4
5.48 x
4.63 · 10−4
3.06 · 10−4
1.51 x
5.67 · 10−4
4.07 · 10−4
1.39 x
5.64 · 10−4
4.92 · 10−4
1.14 x
4.16 · 10−3
6.05 · 10−4
6.87 x
7.40 · 10−4
5.55 · 10−4
1.33 x
1.72 · 10−3
2.28 · 10−4
7.53 x
4.10 · 10−4
3.90 · 10−4
1.05 x
4.36 · 10−4
3.53 · 10−4
1.23 x
1.47 · 10−3
2.73 · 10−4
5.39 x
5.44 · 10−4
5.80 · 10−4
0.94 x
7.14 · 10−4
9.75 · 10−4
0.73 x
8.01 · 10−4
7.31 · 10−4
1.10 x"
REFERENCES,0.6448140900195695,"F.3
Input VJP"
REFERENCES,0.6457925636007827,"We compare TN and TN+opt with a PyTorch implementation of the input VJP via
torch.autograd.grad. Figure F20 visualizes the performance ratios for different convolution
categories. Table F6 contains the detailed run times and performance factors."
REFERENCES,0.6467710371819961,"General
Dense mix
Dense
Down 100 101"
REFERENCES,0.6477495107632094,TN versus PT (logarithmic)
REFERENCES,0.6487279843444227,"Figure F20: Input VJP performance ratios of TN versus PT and TN+opt versus PT for different
convolution types on GPU."
REFERENCES,0.649706457925636,Table F6: Input VJP performance comparison on GPU.
REFERENCES,0.6506849315068494,"(a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32)"
REFERENCES,0.6516634050880626,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6526418786692759,"conv1.0
2.24 · 10−3
1.39 · 10−3
1.61 x
conv2.0
2.61 · 10−3
8.29 · 10−4
3.15 x
conv3.1
1.46 · 10−3
5.04 · 10−4
2.90 x"
REFERENCES,0.6536203522504892,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6545988258317026,"2.19 · 10−3
1.34 · 10−3
1.63 x
2.55 · 10−3
7.86 · 10−4
3.25 x
1.42 · 10−3
4.69 · 10−4
3.02 x"
REFERENCES,0.6555772994129159,"(b) F-MNIST 2c2d, input shape (128, 1, 28, 28)"
REFERENCES,0.6565557729941291,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6575342465753424,"conv1.1
9.47 · 10−4
4.36 · 10−4
2.17 x
conv2.1
3.67 · 10−3
9.83 · 10−4
3.74 x"
REFERENCES,0.6585127201565558,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6594911937377691,"8.86 · 10−4
4.40 · 10−4
2.02 x
3.62 · 10−3
9.83 · 10−4
3.69 x"
REFERENCES,0.6604696673189824,"(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32)"
REFERENCES,0.6614481409001957,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.662426614481409,"conv1.1
1.91 · 10−3
9.84 · 10−4
1.94 x
conv2.1
2.00 · 10−2
5.95 · 10−3
3.35 x
conv3.1
7.82 · 10−3
5.05 · 10−3
1.55 x
conv4.1
8.23 · 10−3
3.11 · 10−3
2.65 x
conv5.1
1.56 · 10−2
4.36 · 10−3
3.57 x
conv6.1
4.58 · 10−3
3.96 · 10−3
1.16 x
conv7.0
2.86 · 10−3
8.32 · 10−4
3.44 x
conv8.1
8.31 · 10−4
2.91 · 10−4
2.85 x
conv9.1
7.76 · 10−4
2.21 · 10−4
3.51 x"
REFERENCES,0.6634050880626223,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6643835616438356,"1.87 · 10−3
9.37 · 10−4
2.00 x
2.00 · 10−2
5.92 · 10−3
3.37 x
7.77 · 10−3
5.01 · 10−3
1.55 x
8.17 · 10−3
3.10 · 10−3
2.63 x
1.55 · 10−2
4.36 · 10−3
3.56 x
4.53 · 10−3
3.96 · 10−3
1.14 x
2.81 · 10−3
8.68 · 10−4
3.24 x
3.47 · 10−4
3.32 · 10−4
1.04 x
2.90 · 10−4
2.61 · 10−4
1.11 x"
REFERENCES,0.6653620352250489,"(d) Alexnet, input shape (32, 3, 256, 256)"
REFERENCES,0.6663405088062623,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6673189823874756,"features.0
1.92 · 10−2
5.48 · 10−3
3.50 x
features.3
1.15 · 10−2
4.16 · 10−3
2.77 x
features.6
5.36 · 10−3
1.49 · 10−3
3.60 x
features.8
6.26 · 10−3
1.86 · 10−3
3.36 x
features.10
4.41 · 10−3
1.32 · 10−3
3.35 x"
REFERENCES,0.6682974559686888,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6692759295499021,"1.92 · 10−2
5.54 · 10−3
3.46 x
1.15 · 10−2
4.20 · 10−3
2.75 x
5.36 · 10−3
1.49 · 10−3
3.60 x
6.25 · 10−3
1.86 · 10−3
3.37 x
4.40 · 10−3
1.35 · 10−3
3.26 x"
REFERENCES,0.6702544031311155,"(e) ResNet18, input shape (32, 3, 256, 256)"
REFERENCES,0.6712328767123288,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6722113502935421,"conv1
3.38 · 10−2
8.56 · 10−3
3.96 x
layer1.0.conv1
1.06 · 10−2
2.17 · 10−3
4.87 x
layer2.0.conv1
7.03 · 10−3
3.72 · 10−3
1.89 x
layer2.0.conv2
6.91 · 10−3
1.55 · 10−3
4.47 x
layer2.0.downsample.0
2.02 · 10−3
8.02 · 10−4
2.51 x
layer3.0.conv1
3.94 · 10−3
3.05 · 10−3
1.29 x
layer3.0.conv2
5.07 · 10−3
1.31 · 10−3
3.87 x
layer3.0.downsample.0
1.15 · 10−3
5.96 · 10−4
1.94 x
layer4.0.conv1
2.89 · 10−3
3.08 · 10−3
0.94 x
layer4.0.conv2
4.50 · 10−3
1.40 · 10−3
3.21 x
layer4.0.downsample.0
9.35 · 10−4
5.51 · 10−4
1.70 x"
REFERENCES,0.6731898238747553,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6741682974559687,"3.38 · 10−2
8.49 · 10−3
3.98 x
1.05 · 10−2
2.11 · 10−3
4.99 x
6.95 · 10−3
3.66 · 10−3
1.90 x
6.90 · 10−3
1.51 · 10−3
4.56 x
1.71 · 10−3
7.64 · 10−4
2.24 x
3.88 · 10−3
3.01 · 10−3
1.29 x
5.07 · 10−3
1.36 · 10−3
3.74 x
9.54 · 10−4
6.40 · 10−4
1.49 x
2.84 · 10−3
3.12 · 10−3
0.91 x
4.49 · 10−3
1.44 · 10−3
3.12 x
7.93 · 10−4
5.90 · 10−4
1.34 x"
REFERENCES,0.675146771037182,"(f) ResNext101, input shape (32, 3, 256, 256)"
REFERENCES,0.6761252446183953,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6771037181996086,"conv1
3.38 · 10−2
8.52 · 10−3
3.97 x
layer1.0.conv1
6.18 · 10−3
1.96 · 10−3
3.15 x
layer1.0.conv2
3.04 · 10−2
1.17 · 10−2
2.60 x
layer1.0.conv3
1.46 · 10−2
6.57 · 10−3
2.22 x
layer2.0.conv1
2.40 · 10−2
1.14 · 10−2
2.10 x
layer2.0.conv2
2.75 · 10−2
1.96 · 10−2
1.40 x
layer2.0.conv3
1.04 · 10−2
6.43 · 10−3
1.61 x
layer2.0.downsample.0
8.99 · 10−3
4.78 · 10−3
1.88 x
layer2.1.conv2
1.51 · 10−2
4.46 · 10−3
3.38 x
layer3.0.conv1
1.94 · 10−2
1.25 · 10−2
1.55 x
layer3.0.conv2
1.76 · 10−2
8.33 · 10−3
2.11 x
layer3.0.conv3
8.21 · 10−3
6.32 · 10−3
1.30 x
layer3.0.downsample.0
5.51 · 10−3
4.54 · 10−3
1.21 x
layer3.1.conv2
7.60 · 10−3
1.97 · 10−3
3.85 x
layer4.0.conv1
1.51 · 10−2
1.24 · 10−2
1.22 x
layer4.0.conv2
8.24 · 10−3
5.43 · 10−3
1.52 x
layer4.0.conv3
7.65 · 10−3
6.72 · 10−3
1.14 x
layer4.0.downsample.0
4.61 · 10−3
5.45 · 10−3
0.84 x
layer4.1.conv2
4.79 · 10−3
1.34 · 10−3
3.57 x"
REFERENCES,0.678082191780822,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6790606653620352,"3.38 · 10−2
8.48 · 10−3
3.98 x
2.86 · 10−3
1.96 · 10−3
1.46 x
3.05 · 10−2
1.17 · 10−2
2.61 x
7.39 · 10−3
6.58 · 10−3
1.12 x
1.44 · 10−2
1.17 · 10−2
1.23 x
2.75 · 10−2
1.95 · 10−2
1.41 x
6.74 · 10−3
6.43 · 10−3
1.05 x
8.06 · 10−3
4.74 · 10−3
1.70 x
1.51 · 10−2
4.45 · 10−3
3.39 x
1.32 · 10−2
1.25 · 10−2
1.06 x
1.76 · 10−2
8.34 · 10−3
2.11 x
6.39 · 10−3
6.32 · 10−3
1.01 x
5.00 · 10−3
4.52 · 10−3
1.11 x
7.60 · 10−3
1.98 · 10−3
3.84 x
1.26 · 10−2
1.24 · 10−2
1.02 x
8.24 · 10−3
5.44 · 10−3
1.51 x
6.25 · 10−3
6.73 · 10−3
0.93 x
4.31 · 10−3
5.45 · 10−3
0.79 x
4.79 · 10−3
1.39 · 10−3
3.44 x"
REFERENCES,0.6800391389432485,"(g) ConvNeXt-base, input shape (32, 3, 256, 256)"
REFERENCES,0.6810176125244618,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6819960861056752,"features.0.0
5.36 · 10−3
1.79 · 10−3
2.99 x
features.1.0.block.0
4.63 · 10−2
8.60 · 10−3
5.38 x
features.2.1
8.85 · 10−3
5.37 · 10−3
1.65 x
features.3.0.block.0
2.14 · 10−2
4.21 · 10−3
5.09 x
features.4.1
5.64 · 10−3
4.43 · 10−3
1.27 x
features.5.0.block.0
1.05 · 10−2
2.16 · 10−3
4.87 x
features.6.1
4.31 · 10−3
5.50 · 10−3
0.78 x
features.7.0.block.0
1.09 · 10−3
1.17 · 10−3
0.93 x"
REFERENCES,0.6829745596868885,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6839530332681018,"1.57 · 10−3
1.79 · 10−3
0.88 x
4.63 · 10−2
8.58 · 10−3
5.40 x
3.55 · 10−3
5.38 · 10−3
0.66 x
2.14 · 10−2
4.21 · 10−3
5.09 x
3.34 · 10−3
4.43 · 10−3
0.75 x
1.05 · 10−2
2.16 · 10−3
4.86 x
3.25 · 10−3
5.50 · 10−3
0.59 x
1.06 · 10−3
1.15 · 10−3
0.92 x"
REFERENCES,0.684931506849315,"(h) InceptionV3, input shape (32, 3, 299, 299)"
REFERENCES,0.6859099804305284,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6868884540117417,"Conv2d_1a_3x3.conv
1.27 · 10−2
3.19 · 10−3
3.97 x
Conv2d_2a_3x3.conv
3.16 · 10−2
5.13 · 10−3
6.16 x
Conv2d_2b_3x3.conv
4.32 · 10−2
8.11 · 10−3
5.33 x
Conv2d_3b_1x1.conv
5.76 · 10−3
1.09 · 10−3
5.31 x
Conv2d_4a_3x3.conv
3.71 · 10−2
1.12 · 10−2
3.30 x
Mixed_5b.branch1x1.conv
1.37 · 10−3
6.89 · 10−4
1.99 x
Mixed_5b.branch5x5_1.conv
1.13 · 10−3
5.87 · 10−4
1.92 x
Mixed_5b.branch5x5_2.conv
4.97 · 10−3
1.39 · 10−3
3.58 x
Mixed_5b.branch3x3dbl_2.conv
4.23 · 10−3
1.07 · 10−3
3.98 x
Mixed_5b.branch3x3dbl_3.conv
5.68 · 10−3
1.66 · 10−3
3.41 x
Mixed_5b.branch_pool.conv
9.70 · 10−4
5.10 · 10−4
1.90 x
Mixed_5c.branch1x1.conv
1.48 · 10−3
8.10 · 10−4
1.82 x
Mixed_5c.branch5x5_1.conv
1.23 · 10−3
6.85 · 10−4
1.79 x
Mixed_5d.branch1x1.conv
1.68 · 10−3
1.04 · 10−3
1.61 x
Mixed_5d.branch5x5_1.conv
1.38 · 10−3
8.69 · 10−4
1.59 x
Mixed_6a.branch3x3.conv
1.14 · 10−2
1.32 · 10−2
0.86 x
Mixed_6a.branch3x3dbl_3.conv
2.52 · 10−3
1.70 · 10−3
1.48 x
Mixed_6b.branch1x1.conv
1.78 · 10−3
1.18 · 10−3
1.50 x
Mixed_6b.branch7x7_1.conv
1.37 · 10−3
8.69 · 10−4
1.58 x
Mixed_6b.branch7x7_2.conv
2.13 · 10−3
8.27 · 10−4
2.58 x
Mixed_6b.branch7x7_3.conv
2.54 · 10−3
1.08 · 10−3
2.36 x
Mixed_6b.branch7x7dbl_2.conv
2.08 · 10−3
8.10 · 10−4
2.57 x
Mixed_6b.branch7x7dbl_5.conv
2.45 · 10−3
1.11 · 10−3
2.21 x
Mixed_6c.branch7x7_1.conv
1.56 · 10−3
1.03 · 10−3
1.52 x
Mixed_6c.branch7x7_2.conv
3.19 · 10−3
1.14 · 10−3
2.79 x
Mixed_6c.branch7x7_3.conv
3.06 · 10−3
1.28 · 10−3
2.40 x
Mixed_6c.branch7x7dbl_2.conv
3.10 · 10−3
1.12 · 10−3
2.78 x
Mixed_6c.branch7x7dbl_5.conv
2.96 · 10−3
1.33 · 10−3
2.22 x
Mixed_6e.branch7x7_2.conv
3.77 · 10−3
1.54 · 10−3
2.45 x
Mixed_6e.branch7x7_3.conv
3.65 · 10−3
1.51 · 10−3
2.42 x
AuxLogits.conv0.conv
5.53 · 10−4
2.74 · 10−4
2.02 x
AuxLogits.conv1.conv
6.27 · 10−4
2.04 · 10−3
0.31 x
Mixed_7a.branch3x3_2.conv
1.56 · 10−3
7.08 · 10−4
2.21 x
Mixed_7a.branch7x7x3_4.conv
1.46 · 10−3
1.10 · 10−3
1.33 x
Mixed_7b.branch1x1.conv
1.31 · 10−3
7.40 · 10−4
1.77 x
Mixed_7b.branch3x3_1.conv
1.44 · 10−3
8.55 · 10−4
1.69 x
Mixed_7b.branch3x3_2a.conv
1.51 · 10−3
9.77 · 10−4
1.55 x
Mixed_7b.branch3x3_2b.conv
1.50 · 10−3
9.77 · 10−4
1.54 x
Mixed_7b.branch3x3dbl_1.conv
1.56 · 10−3
1.02 · 10−3
1.54 x
Mixed_7b.branch3x3dbl_2.conv
3.32 · 10−3
1.02 · 10−3
3.24 x
Mixed_7b.branch_pool.conv
1.01 · 10−3
5.57 · 10−4
1.81 x
Mixed_7c.branch1x1.conv
1.69 · 10−3
1.25 · 10−3
1.35 x
Mixed_7c.branch3x3_1.conv
1.86 · 10−3
1.45 · 10−3
1.28 x
Mixed_7c.branch3x3dbl_1.conv
2.05 · 10−3
1.66 · 10−3
1.23 x
Mixed_7c.branch_pool.conv
1.27 · 10−3
8.35 · 10−4
1.53 x"
REFERENCES,0.687866927592955,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6888454011741683,"1.26 · 10−2
3.21 · 10−3
3.92 x
3.16 · 10−2
5.17 · 10−3
6.10 x
4.24 · 10−2
8.17 · 10−3
5.19 x
1.37 · 10−3
1.09 · 10−3
1.25 x
3.71 · 10−2
1.12 · 10−2
3.30 x
6.67 · 10−4
6.88 · 10−4
0.97 x
5.70 · 10−4
5.88 · 10−4
0.97 x
4.97 · 10−3
1.39 · 10−3
3.57 x
4.23 · 10−3
1.06 · 10−3
3.98 x
5.68 · 10−3
1.66 · 10−3
3.41 x
4.77 · 10−4
5.13 · 10−4
0.93 x
8.07 · 10−4
8.10 · 10−4
1.00 x
6.81 · 10−4
6.87 · 10−4
0.99 x
9.40 · 10−4
1.05 · 10−3
0.90 x
7.84 · 10−4
8.12 · 10−4
0.96 x
1.14 · 10−2
1.32 · 10−2
0.86 x
2.46 · 10−3
1.70 · 10−3
1.45 x
1.24 · 10−3
1.19 · 10−3
1.05 x
9.27 · 10−4
8.70 · 10−4
1.07 x
1.79 · 10−3
8.27 · 10−4
2.16 x
2.22 · 10−3
1.08 · 10−3
2.05 x
1.80 · 10−3
8.09 · 10−4
2.22 x
2.14 · 10−3
1.11 · 10−3
1.92 x
1.09 · 10−3
1.03 · 10−3
1.06 x
2.76 · 10−3
1.13 · 10−3
2.43 x
2.72 · 10−3
1.28 · 10−3
2.12 x
2.77 · 10−3
1.12 · 10−3
2.48 x
2.61 · 10−3
1.33 · 10−3
1.96 x
3.26 · 10−3
1.50 · 10−3
2.17 x
3.27 · 10−3
1.47 · 10−3
2.22 x
3.03 · 10−4
2.34 · 10−4
1.30 x
4.94 · 10−4
2.02 · 10−3
0.25 x
1.47 · 10−3
6.64 · 10−4
2.22 x
1.42 · 10−3
1.14 · 10−3
1.25 x
8.47 · 10−4
7.89 · 10−4
1.07 x
9.54 · 10−4
9.00 · 10−4
1.06 x
1.26 · 10−3
1.02 · 10−3
1.24 x
1.27 · 10−3
9.78 · 10−4
1.30 x
1.07 · 10−3
9.72 · 10−4
1.10 x
3.28 · 10−3
9.91 · 10−4
3.31 x
6.18 · 10−4
5.10 · 10−4
1.21 x
1.21 · 10−3
1.22 · 10−3
0.99 x
1.39 · 10−3
1.45 · 10−3
0.95 x
1.57 · 10−3
1.66 · 10−3
0.95 x
8.32 · 10−4
8.35 · 10−4
1.00 x"
REFERENCES,0.6898238747553816,"(i) MobileNetV2, input shape (32, 3, 256, 256)"
REFERENCES,0.6908023483365949,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.6917808219178082,"features.0.0
8.32 · 10−3
2.08 · 10−3
4.01 x
features.1.conv.0.0
2.27 · 10−2
2.63 · 10−3
8.64 x
features.1.conv.1
3.22 · 10−3
1.17 · 10−3
2.75 x
features.2.conv.0.0
7.67 · 10−3
2.61 · 10−3
2.94 x
features.2.conv.1.0
2.79 · 10−2
8.11 · 10−3
3.44 x
features.2.conv.2
1.48 · 10−3
6.38 · 10−4
2.33 x
features.3.conv.0.0
2.86 · 10−3
1.05 · 10−3
2.73 x
features.3.conv.1.0
2.08 · 10−2
2.95 · 10−3
7.07 x
features.3.conv.2
1.77 · 10−3
1.04 · 10−3
1.70 x
features.4.conv.1.0
7.63 · 10−3
3.15 · 10−3
2.42 x
features.4.conv.2
9.49 · 10−4
4.32 · 10−4
2.20 x
features.5.conv.0.0
1.20 · 10−3
5.26 · 10−4
2.29 x
features.5.conv.1.0
5.41 · 10−3
1.02 · 10−3
5.29 x
features.5.conv.2
9.53 · 10−4
4.00 · 10−4
2.38 x
features.7.conv.1.0
2.11 · 10−3
1.07 · 10−3
1.97 x
features.7.conv.2
7.77 · 10−4
2.33 · 10−4
3.34 x
features.8.conv.0.0
8.13 · 10−4
3.41 · 10−4
2.38 x
features.8.conv.1.0
2.09 · 10−3
5.48 · 10−4
3.81 x
features.8.conv.2
8.65 · 10−4
3.16 · 10−4
2.74 x
features.11.conv.2
9.34 · 10−4
4.22 · 10−4
2.21 x
features.12.conv.0.0
1.16 · 10−3
7.11 · 10−4
1.64 x
features.12.conv.1.0
3.84 · 10−3
7.91 · 10−4
4.85 x
features.12.conv.2
1.08 · 10−3
5.71 · 10−4
1.90 x
features.14.conv.1.0
1.61 · 10−3
8.26 · 10−4
1.95 x
features.14.conv.2
8.14 · 10−4
2.87 · 10−4
2.83 x
features.15.conv.0.0
8.46 · 10−4
6.29 · 10−4
1.34 x
features.15.conv.1.0
1.52 · 10−3
3.62 · 10−4
4.21 x
features.15.conv.2
9.64 · 10−4
4.43 · 10−4
2.18 x
features.17.conv.2
1.23 · 10−3
7.30 · 10−4
1.69 x
features.18.0
1.29 · 10−3
1.28 · 10−3
1.00 x"
REFERENCES,0.6927592954990215,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.6937377690802349,"8.26 · 10−3
2.02 · 10−3
4.09 x
2.27 · 10−2
2.60 · 10−3
8.71 x
1.02 · 10−3
1.16 · 10−3
0.87 x
3.66 · 10−3
2.61 · 10−3
1.40 x
2.79 · 10−2
8.11 · 10−3
3.43 x
7.53 · 10−4
6.40 · 10−4
1.18 x
1.45 · 10−3
1.05 · 10−3
1.38 x
2.08 · 10−2
2.95 · 10−3
7.05 x
9.75 · 10−4
1.04 · 10−3
0.94 x
7.62 · 10−3
3.15 · 10−3
2.42 x
4.38 · 10−4
3.88 · 10−4
1.13 x
6.09 · 10−4
4.83 · 10−4
1.26 x
5.39 · 10−3
1.02 · 10−3
5.27 x
4.35 · 10−4
3.98 · 10−4
1.09 x
2.10 · 10−3
1.07 · 10−3
1.97 x
3.04 · 10−4
2.33 · 10−4
1.30 x
4.63 · 10−4
3.40 · 10−4
1.36 x
2.07 · 10−3
5.47 · 10−4
3.79 x
4.04 · 10−4
3.16 · 10−4
1.28 x
4.74 · 10−4
4.24 · 10−4
1.12 x
7.37 · 10−4
7.10 · 10−4
1.04 x
3.82 · 10−3
7.91 · 10−4
4.83 x
6.13 · 10−4
5.73 · 10−4
1.07 x
1.60 · 10−3
8.26 · 10−4
1.93 x
3.84 · 10−4
2.87 · 10−4
1.34 x
5.55 · 10−4
6.08 · 10−4
0.91 x
1.50 · 10−3
3.61 · 10−4
4.17 x
4.82 · 10−4
4.44 · 10−4
1.09 x
6.98 · 10−4
7.32 · 10−4
0.95 x
8.76 · 10−4
1.28 · 10−3
0.68 x"
REFERENCES,0.6947162426614482,"F.4
Weight VJP"
REFERENCES,0.6956947162426614,"We compare TN and TN+opt with a PyTorch implementation of the weight VJP via
torch.autograd.grad. Figure F21 visualizes the performance ratios for different convolution
categories. Table F7 contains the detailed run times and performance factors."
REFERENCES,0.6966731898238747,"General
Dense mix
Dense
Down 100 101"
REFERENCES,0.6976516634050881,TN versus PT (logarithmic)
REFERENCES,0.6986301369863014,"Figure F21: Weight VJP performance ratios of TN versus PT and TN+opt versus PT for different
convolution types on GPU."
REFERENCES,0.6996086105675147,Table F7: Weight VJP performance comparison on GPU.
REFERENCES,0.700587084148728,"(a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32)"
REFERENCES,0.7015655577299413,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7025440313111546,"conv1.0
2.27 · 10−3
1.50 · 10−3
1.52 x
conv2.0
3.00 · 10−3
1.12 · 10−3
2.68 x
conv3.1
1.29 · 10−3
5.46 · 10−4
2.37 x"
REFERENCES,0.7035225048923679,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7045009784735812,"2.27 · 10−3
1.50 · 10−3
1.51 x
2.99 · 10−3
1.07 · 10−3
2.78 x
1.25 · 10−3
5.08 · 10−4
2.47 x"
REFERENCES,0.7054794520547946,"(b) F-MNIST 2c2d, input shape (128, 1, 28, 28)"
REFERENCES,0.7064579256360078,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7074363992172211,"conv1.1
1.08 · 10−3
3.81 · 10−4
2.83 x
conv2.1
4.12 · 10−3
1.02 · 10−3
4.02 x"
REFERENCES,0.7084148727984344,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7093933463796478,"1.03 · 10−3
4.05 · 10−4
2.54 x
4.09 · 10−3
1.03 · 10−3
3.98 x"
REFERENCES,0.7103718199608611,"(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32)"
REFERENCES,0.7113502935420744,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7123287671232876,"conv1.1
2.43 · 10−3
1.02 · 10−3
2.39 x
conv2.1
3.83 · 10−2
5.62 · 10−3
6.81 x
conv3.1
8.30 · 10−3
4.14 · 10−3
2.00 x
conv4.1
8.66 · 10−3
2.64 · 10−3
3.28 x
conv5.1
1.60 · 10−2
3.38 · 10−3
4.75 x
conv6.1
5.23 · 10−3
2.80 · 10−3
1.87 x
conv7.0
2.68 · 10−3
9.97 · 10−4
2.68 x
conv8.1
9.13 · 10−4
2.62 · 10−3
0.35 x
conv9.1
8.78 · 10−4
3.54 · 10−4
2.48 x"
REFERENCES,0.713307240704501,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7142857142857143,"2.42 · 10−3
1.02 · 10−3
2.37 x
1.97 · 10−2
5.62 · 10−3
3.51 x
8.33 · 10−3
4.21 · 10−3
1.98 x
8.68 · 10−3
2.68 · 10−3
3.24 x
1.61 · 10−2
3.42 · 10−3
4.70 x
5.17 · 10−3
2.81 · 10−3
1.84 x
2.59 · 10−3
1.04 · 10−3
2.49 x
4.33 · 10−4
2.62 · 10−3
0.17 x
3.93 · 10−4
3.50 · 10−4
1.12 x"
REFERENCES,0.7152641878669276,"(d) Alexnet, input shape (32, 3, 256, 256)"
REFERENCES,0.7162426614481409,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7172211350293543,"features.0
1.82 · 10−2
3.31 · 10−3
5.50 x
features.3
2.02 · 10−2
2.57 · 10−3
7.85 x
features.6
6.98 · 10−3
1.67 · 10−3
4.19 x
features.8
8.16 · 10−3
1.97 · 10−3
4.15 x
features.10
5.80 · 10−3
1.47 · 10−3
3.94 x"
REFERENCES,0.7181996086105675,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7191780821917808,"1.82 · 10−2
3.33 · 10−3
5.46 x
1.14 · 10−2
2.58 · 10−3
4.44 x
5.17 · 10−3
1.67 · 10−3
3.10 x
6.13 · 10−3
1.97 · 10−3
3.11 x
4.34 · 10−3
1.47 · 10−3
2.95 x"
REFERENCES,0.7201565557729941,"(e) ResNet18, input shape (32, 3, 256, 256)"
REFERENCES,0.7211350293542075,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7221135029354208,"conv1
3.00 · 10−2
8.23 · 10−3
3.65 x
layer1.0.conv1
2.34 · 10−2
3.18 · 10−3
7.37 x
layer2.0.conv1
5.88 · 10−3
2.97 · 10−3
1.98 x
layer2.0.conv2
1.17 · 10−2
1.66 · 10−3
7.03 x
layer2.0.downsample.0
1.85 · 10−3
7.39 · 10−4
2.51 x
layer3.0.conv1
3.79 · 10−3
2.71 · 10−3
1.40 x
layer3.0.conv2
6.62 · 10−3
1.48 · 10−3
4.46 x
layer3.0.downsample.0
1.61 · 10−3
5.39 · 10−4
2.99 x
layer4.0.conv1
2.85 · 10−3
2.46 · 10−3
1.16 x
layer4.0.conv2
4.83 · 10−3
1.72 · 10−3
2.80 x
layer4.0.downsample.0
1.00 · 10−3
1.02 · 10−3
0.98 x"
REFERENCES,0.723091976516634,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7240704500978473,"2.99 · 10−2
7.83 · 10−3
3.82 x
1.10 · 10−2
3.22 · 10−3
3.43 x
5.87 · 10−3
2.96 · 10−3
1.98 x
6.98 · 10−3
1.66 · 10−3
4.20 x
7.60 · 10−4
7.33 · 10−4
1.04 x
3.76 · 10−3
2.71 · 10−3
1.39 x
4.87 · 10−3
1.48 · 10−3
3.28 x
6.12 · 10−4
5.45 · 10−4
1.12 x
2.82 · 10−3
2.46 · 10−3
1.15 x
4.31 · 10−3
1.72 · 10−3
2.50 x
5.07 · 10−4
1.02 · 10−3
0.50 x"
REFERENCES,0.7250489236790607,"(f) ResNext101, input shape (32, 3, 256, 256)"
REFERENCES,0.726027397260274,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7270058708414873,"conv1
3.00 · 10−2
8.22 · 10−3
3.65 x
layer1.0.conv1
7.08 · 10−3
2.92 · 10−3
2.42 x
layer1.0.conv2
6.70 · 10−2
2.53 · 10−2
2.65 x
layer1.0.conv3
3.12 · 10−2
8.78 · 10−3
3.55 x
layer2.0.conv1
2.29 · 10−2
1.80 · 10−2
1.28 x
layer2.0.conv2
6.64 · 10−2
1.23 · 10−2
5.40 x
layer2.0.conv3
1.82 · 10−2
5.90 · 10−3
3.08 x
layer2.0.downsample.0
8.57 · 10−3
5.24 · 10−3
1.64 x
layer2.1.conv2
4.04 · 10−2
1.21 · 10−2
3.33 x
layer3.0.conv1
1.84 · 10−2
2.03 · 10−2
0.91 x
layer3.0.conv2
1.63 · 10−2
5.77 · 10−3
2.83 x
layer3.0.conv3
1.17 · 10−2
1.07 · 10−2
1.10 x
layer3.0.downsample.0
6.19 · 10−3
5.95 · 10−3
1.04 x
layer3.1.conv2
1.47 · 10−2
3.17 · 10−3
4.65 x
layer4.0.conv1
1.55 · 10−2
2.10 · 10−2
0.74 x
layer4.0.conv2
8.07 · 10−3
3.13 · 10−2
0.26 x
layer4.0.conv3
8.23 · 10−3
1.06 · 10−2
0.78 x
layer4.0.downsample.0
4.96 · 10−3
5.94 · 10−3
0.84 x
layer4.1.conv2
6.63 · 10−3
1.40 · 10−3
4.72 x"
REFERENCES,0.7279843444227005,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7289628180039139,"2.99 · 10−2
7.83 · 10−3
3.82 x
3.75 · 10−3
2.89 · 10−3
1.30 x
6.72 · 10−2
1.91 · 10−2
3.51 x
1.04 · 10−2
1.02 · 10−2
1.02 x
1.77 · 10−2
1.76 · 10−2
1.01 x
6.63 · 10−2
1.23 · 10−2
5.39 x
8.44 · 10−3
6.50 · 10−3
1.30 x
4.55 · 10−3
5.24 · 10−3
0.87 x
4.04 · 10−2
1.22 · 10−2
3.32 x
1.48 · 10−2
2.03 · 10−2
0.73 x
1.63 · 10−2
5.82 · 10−3
2.81 x
7.19 · 10−3
1.07 · 10−2
0.67 x
3.85 · 10−3
6.01 · 10−3
0.64 x
1.47 · 10−2
3.14 · 10−3
4.67 x
1.33 · 10−2
2.11 · 10−2
0.63 x
8.06 · 10−3
3.13 · 10−2
0.26 x
6.75 · 10−3
1.06 · 10−2
0.63 x
3.59 · 10−3
5.99 · 10−3
0.60 x
6.62 · 10−3
1.45 · 10−3
4.55 x"
REFERENCES,0.7299412915851272,"(g) ConvNeXt-base, input shape (32, 3, 256, 256)"
REFERENCES,0.7309197651663405,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7318982387475538,"features.0.0
5.93 · 10−3
1.99 · 10−3
2.98 x
features.1.0.block.0
2.53 · 10−2
1.09 · 10−2
2.33 x
features.2.1
8.29 · 10−3
4.53 · 10−3
1.83 x
features.3.0.block.0
1.23 · 10−2
5.85 · 10−3
2.10 x
features.4.1
5.74 · 10−3
5.30 · 10−3
1.08 x
features.5.0.block.0
6.05 · 10−3
3.63 · 10−3
1.66 x
features.6.1
4.74 · 10−3
5.28 · 10−3
0.90 x
features.7.0.block.0
9.08 · 10−4
3.13 · 10−3
0.29 x"
REFERENCES,0.7328767123287672,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7338551859099804,"1.87 · 10−3
1.97 · 10−3
0.95 x
2.53 · 10−2
1.09 · 10−2
2.33 x
4.32 · 10−3
4.52 · 10−3
0.96 x
1.22 · 10−2
5.82 · 10−3
2.10 x
3.74 · 10−3
5.29 · 10−3
0.71 x
6.03 · 10−3
3.64 · 10−3
1.66 x
3.53 · 10−3
5.17 · 10−3
0.68 x
8.87 · 10−4
3.13 · 10−3
0.28 x"
REFERENCES,0.7348336594911937,"(h) InceptionV3, input shape (32, 3, 299, 299)"
REFERENCES,0.735812133072407,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7367906066536204,"Conv2d_1a_3x3.conv
1.07 · 10−2
1.70 · 10−3
6.31 x
Conv2d_2a_3x3.conv
6.00 · 10−2
1.16 · 10−2
5.18 x
Conv2d_2b_3x3.conv
6.10 · 10−2
1.34 · 10−2
4.55 x
Conv2d_3b_1x1.conv
5.48 · 10−3
1.82 · 10−3
3.01 x
Conv2d_4a_3x3.conv
5.28 · 10−2
1.29 · 10−2
4.08 x
Mixed_5b.branch1x1.conv
5.14 · 10−3
1.16 · 10−3
4.43 x
Mixed_5b.branch5x5_1.conv
4.92 · 10−3
1.46 · 10−3
3.37 x
Mixed_5b.branch5x5_2.conv
9.28 · 10−3
1.28 · 10−3
7.23 x
Mixed_5b.branch3x3dbl_2.conv
7.78 · 10−3
1.75 · 10−3
4.45 x
Mixed_5b.branch3x3dbl_3.conv
1.05 · 10−2
1.86 · 10−3
5.63 x
Mixed_5b.branch_pool.conv
4.52 · 10−3
9.10 · 10−4
4.97 x
Mixed_5c.branch1x1.conv
6.55 · 10−3
2.00 · 10−3
3.27 x
Mixed_5c.branch5x5_1.conv
6.33 · 10−3
1.93 · 10−3
3.28 x
Mixed_5d.branch1x1.conv
7.46 · 10−3
2.34 · 10−3
3.19 x
Mixed_5d.branch5x5_1.conv
7.24 · 10−3
2.16 · 10−3
3.36 x
Mixed_6a.branch3x3.conv
1.10 · 10−2
8.34 · 10−3
1.32 x
Mixed_6a.branch3x3dbl_3.conv
2.46 · 10−3
1.12 · 10−3
2.20 x
Mixed_6b.branch1x1.conv
5.17 · 10−3
2.05 · 10−3
2.52 x
Mixed_6b.branch7x7_1.conv
4.63 · 10−3
1.56 · 10−3
2.96 x
Mixed_6b.branch7x7_2.conv
3.09 · 10−3
1.59 · 10−3
1.94 x
Mixed_6b.branch7x7_3.conv
3.44 · 10−3
2.29 · 10−3
1.50 x
Mixed_6b.branch7x7dbl_2.conv
4.06 · 10−3
1.64 · 10−3
2.48 x
Mixed_6b.branch7x7dbl_5.conv
2.51 · 10−3
2.24 · 10−3
1.12 x
Mixed_6c.branch7x7_1.conv
4.99 · 10−3
1.98 · 10−3
2.53 x
Mixed_6c.branch7x7_2.conv
4.87 · 10−3
2.71 · 10−3
1.80 x
Mixed_6c.branch7x7_3.conv
4.85 · 10−3
2.84 · 10−3
1.71 x
Mixed_6c.branch7x7dbl_2.conv
5.43 · 10−3
2.80 · 10−3
1.94 x
Mixed_6c.branch7x7dbl_5.conv
3.20 · 10−3
2.78 · 10−3
1.15 x
Mixed_6e.branch7x7_2.conv
5.96 · 10−3
3.19 · 10−3
1.87 x
Mixed_6e.branch7x7_3.conv
6.50 · 10−3
3.26 · 10−3
1.99 x
AuxLogits.conv0.conv
6.48 · 10−4
3.45 · 10−4
1.87 x
AuxLogits.conv1.conv
5.34 · 10−4
2.09 · 10−4
2.56 x
Mixed_7a.branch3x3_2.conv
1.80 · 10−3
5.61 · 10−4
3.22 x
Mixed_7a.branch7x7x3_4.conv
1.55 · 10−3
8.46 · 10−4
1.83 x
Mixed_7b.branch1x1.conv
2.08 · 10−3
1.62 · 10−3
1.28 x
Mixed_7b.branch3x3_1.conv
2.19 · 10−3
1.65 · 10−3
1.33 x
Mixed_7b.branch3x3_2a.conv
1.56 · 10−3
1.47 · 10−3
1.06 x
Mixed_7b.branch3x3_2b.conv
1.66 · 10−3
1.50 · 10−3
1.11 x
Mixed_7b.branch3x3dbl_1.conv
2.34 · 10−3
1.65 · 10−3
1.42 x
Mixed_7b.branch3x3dbl_2.conv
3.55 · 10−3
1.23 · 10−3
2.90 x
Mixed_7b.branch_pool.conv
1.84 · 10−3
1.46 · 10−3
1.26 x
Mixed_7c.branch1x1.conv
3.07 · 10−3
3.08 · 10−3
1.00 x
Mixed_7c.branch3x3_1.conv
3.30 · 10−3
3.11 · 10−3
1.06 x
Mixed_7c.branch3x3dbl_1.conv
3.56 · 10−3
3.11 · 10−3
1.15 x
Mixed_7c.branch_pool.conv
2.70 · 10−3
1.61 · 10−3
1.68 x"
REFERENCES,0.7377690802348337,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.738747553816047,"1.07 · 10−2
1.70 · 10−3
6.29 x
3.11 · 10−2
1.16 · 10−2
2.68 x
4.27 · 10−2
1.53 · 10−2
2.78 x
2.26 · 10−3
2.12 · 10−3
1.07 x
3.39 · 10−2
1.29 · 10−2
2.62 x
1.41 · 10−3
1.48 · 10−3
0.95 x
1.39 · 10−3
1.47 · 10−3
0.95 x
4.83 · 10−3
1.28 · 10−3
3.77 x
4.22 · 10−3
1.75 · 10−3
2.41 x
6.00 · 10−3
1.87 · 10−3
3.21 x
1.16 · 10−3
8.96 · 10−4
1.30 x
1.67 · 10−3
1.93 · 10−3
0.86 x
1.64 · 10−3
1.86 · 10−3
0.88 x
2.03 · 10−3
2.31 · 10−3
0.88 x
2.00 · 10−3
2.15 · 10−3
0.93 x
1.09 · 10−2
8.34 · 10−3
1.31 x
2.42 · 10−3
1.12 · 10−3
2.17 x
1.86 · 10−3
2.07 · 10−3
0.90 x
1.46 · 10−3
1.61 · 10−3
0.91 x
2.01 · 10−3
1.64 · 10−3
1.22 x
2.37 · 10−3
2.33 · 10−3
1.01 x
1.81 · 10−3
1.68 · 10−3
1.08 x
2.85 · 10−3
2.29 · 10−3
1.24 x
1.77 · 10−3
2.03 · 10−3
0.87 x
3.30 · 10−3
2.75 · 10−3
1.20 x
2.99 · 10−3
2.87 · 10−3
1.04 x
2.95 · 10−3
2.80 · 10−3
1.05 x
3.41 · 10−3
2.82 · 10−3
1.21 x
3.83 · 10−3
3.24 · 10−3
1.18 x
3.40 · 10−3
3.30 · 10−3
1.03 x
3.45 · 10−4
3.88 · 10−4
0.89 x
4.59 · 10−4
2.76 · 10−4
1.66 x
1.78 · 10−3
6.16 · 10−4
2.90 x
1.52 · 10−3
8.50 · 10−4
1.79 x
1.07 · 10−3
1.63 · 10−3
0.66 x
1.17 · 10−3
1.65 · 10−3
0.71 x
1.20 · 10−3
1.47 · 10−3
0.82 x
1.22 · 10−3
1.50 · 10−3
0.82 x
1.33 · 10−3
1.66 · 10−3
0.80 x
3.10 · 10−3
1.26 · 10−3
2.45 x
8.67 · 10−4
1.46 · 10−3
0.59 x
1.55 · 10−3
3.12 · 10−3
0.50 x
1.79 · 10−3
3.11 · 10−3
0.58 x
2.03 · 10−3
3.10 · 10−3
0.65 x
1.22 · 10−3
1.61 · 10−3
0.76 x"
REFERENCES,0.7397260273972602,"(i) MobileNetV2, input shape (32, 3, 256, 256)"
REFERENCES,0.7407045009784736,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7416829745596869,"features.0.0
7.70 · 10−3
1.45 · 10−3
5.30 x
features.1.conv.0.0
1.59 · 10−2
2.46 · 10−3
6.48 x
features.1.conv.1
1.47 · 10−2
2.40 · 10−3
6.11 x
features.2.conv.0.0
8.97 · 10−3
4.95 · 10−3
1.81 x
features.2.conv.1.0
2.14 · 10−2
2.40 · 10−3
8.90 x
features.2.conv.2
7.38 · 10−3
1.96 · 10−3
3.76 x
features.3.conv.0.0
3.55 · 10−3
2.24 · 10−3
1.58 x
features.3.conv.1.0
1.34 · 10−2
2.49 · 10−3
5.38 x
features.3.conv.2
1.04 · 10−2
2.89 · 10−3
3.61 x
features.4.conv.1.0
7.46 · 10−3
1.01 · 10−3
7.41 x
features.4.conv.2
3.00 · 10−3
9.40 · 10−4
3.19 x
features.5.conv.0.0
1.49 · 10−3
7.46 · 10−4
2.00 x
features.5.conv.1.0
4.77 · 10−3
9.10 · 10−4
5.24 x
features.5.conv.2
3.62 · 10−3
1.06 · 10−3
3.41 x
features.7.conv.1.0
2.61 · 10−3
3.66 · 10−4
7.13 x
features.7.conv.2
1.43 · 10−3
5.55 · 10−4
2.58 x
features.8.conv.0.0
1.14 · 10−3
5.36 · 10−4
2.12 x
features.8.conv.1.0
2.44 · 10−3
5.67 · 10−4
4.31 x
features.8.conv.2
2.23 · 10−3
8.32 · 10−4
2.68 x
features.11.conv.2
2.36 · 10−3
8.68 · 10−4
2.72 x
features.12.conv.0.0
1.55 · 10−3
1.08 · 10−3
1.44 x
features.12.conv.1.0
3.52 · 10−3
8.20 · 10−4
4.29 x
features.12.conv.2
3.27 · 10−3
1.26 · 10−3
2.59 x
features.14.conv.1.0
2.07 · 10−3
3.90 · 10−4
5.31 x
features.14.conv.2
1.06 · 10−3
1.39 · 10−3
0.76 x
features.15.conv.0.0
1.12 · 10−3
7.19 · 10−4
1.56 x
features.15.conv.1.0
2.31 · 10−3
5.96 · 10−4
3.87 x
features.15.conv.2
1.34 · 10−3
1.41 · 10−3
0.95 x
features.17.conv.2
1.59 · 10−3
1.67 · 10−3
0.95 x
features.18.0
1.53 · 10−3
1.68 · 10−3
0.91 x"
REFERENCES,0.7426614481409002,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7436399217221135,"7.67 · 10−3
1.46 · 10−3
5.26 x
1.59 · 10−2
2.47 · 10−3
6.46 x
2.64 · 10−3
2.39 · 10−3
1.10 x
4.22 · 10−3
4.95 · 10−3
0.85 x
2.14 · 10−2
2.41 · 10−3
8.89 x
1.91 · 10−3
1.89 · 10−3
1.01 x
1.83 · 10−3
2.19 · 10−3
0.84 x
1.34 · 10−2
2.46 · 10−3
5.43 x
2.89 · 10−3
2.85 · 10−3
1.01 x
7.44 · 10−3
9.84 · 10−4
7.56 x
9.20 · 10−4
8.94 · 10−4
1.03 x
7.48 · 10−4
7.46 · 10−4
1.00 x
4.75 · 10−3
8.88 · 10−4
5.34 x
1.04 · 10−3
1.01 · 10−3
1.03 x
2.60 · 10−3
3.65 · 10−4
7.13 x
4.64 · 10−4
5.55 · 10−4
0.84 x
5.22 · 10−4
5.34 · 10−4
0.98 x
2.43 · 10−3
5.68 · 10−4
4.28 x
6.80 · 10−4
8.82 · 10−4
0.77 x
7.89 · 10−4
8.69 · 10−4
0.91 x
9.00 · 10−4
1.03 · 10−3
0.88 x
3.50 · 10−3
8.19 · 10−4
4.27 x
1.10 · 10−3
1.26 · 10−3
0.87 x
2.05 · 10−3
3.90 · 10−4
5.26 x
5.10 · 10−4
1.40 · 10−3
0.36 x
6.21 · 10−4
7.10 · 10−4
0.87 x
2.28 · 10−3
5.96 · 10−4
3.83 x
6.53 · 10−4
1.40 · 10−3
0.47 x
8.70 · 10−4
1.62 · 10−3
0.54 x
1.04 · 10−3
1.63 · 10−3
0.64 x"
REFERENCES,0.7446183953033269,"F.5
KFC Factor (KFAC-expand)"
REFERENCES,0.7455968688845401,"We compare TN and TN+opt with a PyTorch implementation of the input-based KFC factor based
on torch.nn.functional.unfold. Figure F22 visualizes the performance ratios for different
convolution categories. Table F8 contains the detailed run times and performance factors."
REFERENCES,0.7465753424657534,"General
Dense mix
Dense
Down 10−1 100 101"
REFERENCES,0.7475538160469667,TN versus PT (logarithmic)
REFERENCES,0.7485322896281801,"Figure F22: KFC/KFAC-expand factor performance ratios of TN versus PT and TN+opt versus PT
for different convolution types on GPU."
REFERENCES,0.7495107632093934,Table F8: KFC (KFAC-expand) factor performance comparison on GPU.
REFERENCES,0.7504892367906066,"(a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32)"
REFERENCES,0.7514677103718199,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7524461839530333,"conv1.0
1.03 · 10−3
2.42 · 10−3
0.43 x
conv2.0
6.69 · 10−3
3.83 · 10−3
1.75 x
conv3.1
3.27 · 10−3
2.38 · 10−3
1.37 x"
REFERENCES,0.7534246575342466,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7544031311154599,"1.03 · 10−3
2.52 · 10−3
0.41 x
6.97 · 10−3
4.52 · 10−3
1.54 x
3.53 · 10−3
2.54 · 10−3
1.39 x"
REFERENCES,0.7553816046966731,"(b) F-MNIST 2c2d, input shape (128, 1, 28, 28)"
REFERENCES,0.7563600782778865,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7573385518590998,"conv1.1
1.22 · 10−3
2.01 · 10−3
0.61 x
conv2.1
1.03 · 10−2
9.54 · 10−3
1.08 x"
REFERENCES,0.7583170254403131,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7592954990215264,"9.30 · 10−4
1.72 · 10−3
0.54 x
1.02 · 10−2
9.47 · 10−3
1.08 x"
REFERENCES,0.7602739726027398,"(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32)"
REFERENCES,0.761252446183953,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7622309197651663,"conv1.1
1.37 · 10−3
1.48 · 10−3
0.93 x
conv2.1
1.48 · 10−1
6.95 · 10−2
2.13 x
conv3.1
4.77 · 10−2
1.15 · 10−2
4.17 x
conv4.1
2.32 · 10−2
1.14 · 10−2
2.03 x
conv5.1
7.03 · 10−2
5.82 · 10−2
1.21 x
conv6.1
2.84 · 10−2
1.33 · 10−2
2.14 x
conv7.0
8.68 · 10−3
5.95 · 10−3
1.46 x
conv8.1
1.03 · 10−3
9.97 · 10−4
1.03 x
conv9.1
1.06 · 10−3
1.49 · 10−3
0.71 x"
REFERENCES,0.7632093933463796,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.764187866927593,"2.72 · 10−3
2.11 · 10−3
1.29 x
1.53 · 10−1
7.14 · 10−2
2.15 x
4.56 · 10−2
1.38 · 10−2
3.30 x
2.25 · 10−2
1.14 · 10−2
1.98 x
1.01 · 10−1
6.19 · 10−2
1.63 x
2.83 · 10−2
9.77 · 10−3
2.90 x
9.30 · 10−3
6.01 · 10−3
1.55 x
3.67 · 10−4
1.44 · 10−3
0.25 x
4.61 · 10−4
1.56 · 10−3
0.30 x"
REFERENCES,0.7651663405088063,"(d) Alexnet, input shape (32, 3, 256, 256)"
REFERENCES,0.7661448140900196,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7671232876712328,"features.0
5.45 · 10−2
1.35 · 10−2
4.03 x
features.3
4.57 · 10−2
4.14 · 10−2
1.10 x
features.6
8.63 · 10−3
7.86 · 10−3
1.10 x
features.8
3.76 · 10−2
4.10 · 10−2
0.92 x
features.10
1.52 · 10−2
1.38 · 10−2
1.10 x"
REFERENCES,0.7681017612524462,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7690802348336595,"6.09 · 10−2
1.34 · 10−2
4.55 x
5.31 · 10−2
3.73 · 10−2
1.42 x
1.12 · 10−2
8.74 · 10−3
1.28 x
4.57 · 10−2
4.33 · 10−2
1.06 x
1.91 · 10−2
1.47 · 10−2
1.30 x"
REFERENCES,0.7700587084148728,"(e) ResNet18, input shape (32, 3, 256, 256)"
REFERENCES,0.7710371819960861,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7720156555772995,"conv1
5.25 · 10−2
2.07 · 10−2
2.54 x
layer1.0.conv1
4.36 · 10−2
2.98 · 10−2
1.46 x
layer2.0.conv1
2.81 · 10−2
6.38 · 10−3
4.41 x
layer2.0.conv2
2.56 · 10−2
1.90 · 10−2
1.34 x
layer2.0.downsample.0
3.66 · 10−3
8.14 · 10−4
4.49 x
layer3.0.conv1
1.34 · 10−2
9.19 · 10−3
1.46 x
layer3.0.conv2
1.90 · 10−2
1.84 · 10−2
1.03 x
layer3.0.downsample.0
1.98 · 10−3
7.00 · 10−4
2.83 x
layer4.0.conv1
8.65 · 10−3
4.79 · 10−3
1.81 x
layer4.0.conv2
2.48 · 10−2
1.63 · 10−2
1.52 x
layer4.0.downsample.0
1.19 · 10−3
5.45 · 10−4
2.18 x"
REFERENCES,0.7729941291585127,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.773972602739726,"5.22 · 10−2
2.06 · 10−2
2.54 x
5.57 · 10−2
3.02 · 10−2
1.84 x
2.78 · 10−2
1.23 · 10−2
2.25 x
3.09 · 10−2
2.01 · 10−2
1.53 x
6.45 · 10−4
7.94 · 10−4
0.81 x
1.40 · 10−2
9.17 · 10−3
1.53 x
2.25 · 10−2
1.95 · 10−2
1.16 x
4.59 · 10−4
5.72 · 10−4
0.80 x
9.12 · 10−3
4.60 · 10−3
1.98 x
2.49 · 10−2
1.88 · 10−2
1.32 x
2.88 · 10−4
5.45 · 10−4
0.53 x"
REFERENCES,0.7749510763209393,"(f) ResNext101, input shape (32, 3, 256, 256)"
REFERENCES,0.7759295499021527,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.776908023483366,"conv1
5.13 · 10−2
2.05 · 10−2
2.50 x
layer1.0.conv1
3.33 · 10−3
1.85 · 10−3
1.80 x
layer1.0.conv2
1.09 · 10−1
6.60 · 10−2
1.66 x
layer1.0.conv3
1.60 · 10−2
7.49 · 10−3
2.14 x
layer2.0.conv1
1.60 · 10−2
1.53 · 10−2
1.05 x
layer2.0.conv2
1.40 · 10−1
4.44 · 10−2
3.15 x
layer2.0.conv3
1.14 · 10−2
5.19 · 10−3
2.20 x
layer2.0.downsample.0
1.40 · 10−2
4.22 · 10−3
3.30 x
layer2.1.conv2
5.07 · 10−2
4.23 · 10−2
1.20 x
layer3.0.conv1
1.14 · 10−2
5.21 · 10−3
2.19 x
layer3.0.conv2
6.11 · 10−2
3.21 · 10−2
1.90 x
layer3.0.conv3
8.77 · 10−3
4.30 · 10−3
2.04 x
layer3.0.downsample.0
7.59 · 10−3
3.08 · 10−3
2.47 x
layer3.1.conv2
2.12 · 10−2
1.95 · 10−2
1.09 x
layer4.0.conv1
8.75 · 10−3
4.15 · 10−3
2.11 x
layer4.0.conv2
4.70 · 10−2
2.47 · 10−2
1.91 x
layer4.0.conv3
7.88 · 10−3
7.66 · 10−3
1.03 x
layer4.0.downsample.0
4.54 · 10−3
2.52 · 10−3
1.80 x
layer4.1.conv2
1.36 · 10−2
1.16 · 10−2
1.16 x"
REFERENCES,0.7778864970645792,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7788649706457925,"5.06 · 10−2
2.05 · 10−2
2.46 x
1.70 · 10−3
2.08 · 10−3
0.82 x
1.11 · 10−1
8.12 · 10−2
1.37 x
1.04 · 10−2
7.52 · 10−3
1.39 x
1.04 · 10−2
1.53 · 10−2
0.68 x
1.44 · 10−1
4.54 · 10−2
3.18 x
8.41 · 10−3
5.20 · 10−3
1.62 x
2.92 · 10−3
4.24 · 10−3
0.69 x
5.07 · 10−2
4.24 · 10−2
1.19 x
8.42 · 10−3
5.27 · 10−3
1.60 x
6.23 · 10−2
2.92 · 10−2
2.14 x
7.17 · 10−3
4.33 · 10−3
1.66 x
2.28 · 10−3
3.08 · 10−3
0.74 x
2.05 · 10−2
1.99 · 10−2
1.03 x
7.18 · 10−3
4.26 · 10−3
1.68 x
4.71 · 10−2
2.47 · 10−2
1.91 x
6.74 · 10−3
7.67 · 10−3
0.88 x
2.03 · 10−3
2.54 · 10−3
0.80 x
1.36 · 10−2
1.17 · 10−2
1.16 x"
REFERENCES,0.7798434442270059,"(g) ConvNeXt-base, input shape (32, 3, 256, 256)"
REFERENCES,0.7808219178082192,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7818003913894325,"features.0.0
9.94 · 10−3
2.11 · 10−3
4.71 x
features.1.0.block.0
4.09 · 10−2
1.37 · 10−1
0.30 x
features.2.1
2.37 · 10−2
4.90 · 10−3
4.85 x
features.3.0.block.0
1.61 · 10−2
6.99 · 10−2
0.23 x
features.4.1
1.41 · 10−2
4.08 · 10−3
3.45 x
features.5.0.block.0
3.98 · 10−3
3.35 · 10−2
0.12 x
features.6.1
6.82 · 10−3
3.30 · 10−3
2.06 x
features.7.0.block.0
1.02 · 10−3
1.61 · 10−2
0.06 x"
REFERENCES,0.7827788649706457,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7837573385518591,"1.18 · 10−3
2.11 · 10−3
0.56 x
5.25 · 10−2
1.42 · 10−1
0.37 x
7.81 · 10−3
4.93 · 10−3
1.59 x
1.57 · 10−2
7.12 · 10−2
0.22 x
6.88 · 10−3
4.15 · 10−3
1.66 x
3.96 · 10−3
3.43 · 10−2
0.12 x
4.77 · 10−3
3.31 · 10−3
1.44 x
1.00 · 10−3
1.63 · 10−2
0.06 x"
REFERENCES,0.7847358121330724,"(h) InceptionV3, input shape (32, 3, 299, 299)"
REFERENCES,0.7857142857142857,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.786692759295499,"Conv2d_1a_3x3.conv
3.42 · 10−2
4.09 · 10−3
8.36 x
Conv2d_2a_3x3.conv
1.58 · 10−1
9.29 · 10−2
1.70 x
Conv2d_2b_3x3.conv
1.56 · 10−1
9.66 · 10−2
1.61 x
Conv2d_3b_1x1.conv
5.26 · 10−3
2.33 · 10−3
2.26 x
Conv2d_4a_3x3.conv
1.01 · 10−1
6.34 · 10−2
1.58 x
Mixed_5b.branch1x1.conv
4.13 · 10−3
2.02 · 10−3
2.05 x
Mixed_5b.branch5x5_1.conv
4.12 · 10−3
3.66 · 10−3
1.13 x
Mixed_5b.branch5x5_2.conv
4.29 · 10−2
3.27 · 10−2
1.31 x
Mixed_5b.branch3x3dbl_2.conv
8.57 · 10−3
7.27 · 10−3
1.18 x
Mixed_5b.branch3x3dbl_3.conv
1.72 · 10−2
1.42 · 10−2
1.21 x
Mixed_5b.branch_pool.conv
4.12 · 10−3
2.05 · 10−3
2.01 x
Mixed_5c.branch1x1.conv
5.43 · 10−3
4.89 · 10−3
1.11 x
Mixed_5c.branch5x5_1.conv
5.40 · 10−3
4.87 · 10−3
1.11 x
Mixed_5d.branch1x1.conv
7.24 · 10−3
6.66 · 10−3
1.09 x
Mixed_5d.branch5x5_1.conv
7.25 · 10−3
6.67 · 10−3
1.09 x
Mixed_6a.branch3x3.conv
7.76 · 10−2
3.28 · 10−2
2.37 x
Mixed_6a.branch3x3dbl_3.conv
1.29 · 10−2
3.50 · 10−3
3.69 x
Mixed_6b.branch1x1.conv
6.56 · 10−3
5.66 · 10−3
1.16 x
Mixed_6b.branch7x7_1.conv
6.55 · 10−3
6.02 · 10−3
1.09 x
Mixed_6b.branch7x7_2.conv
2.01 · 10−3
3.60 · 10−3
0.56 x
Mixed_6b.branch7x7_3.conv
1.92 · 10−3
3.50 · 10−3
0.55 x
Mixed_6b.branch7x7dbl_2.conv
1.94 · 10−3
3.54 · 10−3
0.55 x
Mixed_6b.branch7x7dbl_5.conv
1.97 · 10−3
3.49 · 10−3
0.56 x
Mixed_6c.branch7x7_1.conv
6.59 · 10−3
4.60 · 10−3
1.43 x
Mixed_6c.branch7x7_2.conv
2.59 · 10−3
5.14 · 10−3
0.50 x
Mixed_6c.branch7x7_3.conv
2.58 · 10−3
5.32 · 10−3
0.48 x
Mixed_6c.branch7x7dbl_2.conv
2.51 · 10−3
5.32 · 10−3
0.47 x
Mixed_6c.branch7x7dbl_5.conv
2.53 · 10−3
5.21 · 10−3
0.49 x
Mixed_6e.branch7x7_2.conv
3.35 · 10−3
7.81 · 10−3
0.43 x
Mixed_6e.branch7x7_3.conv
3.35 · 10−3
7.52 · 10−3
0.45 x
AuxLogits.conv0.conv
1.09 · 10−3
6.14 · 10−4
1.77 x
AuxLogits.conv1.conv
8.95 · 10−4
1.07 · 10−3
0.84 x
Mixed_7a.branch3x3_2.conv
6.56 · 10−3
2.67 · 10−3
2.45 x
Mixed_7a.branch7x7x3_4.conv
6.93 · 10−3
2.93 · 10−3
2.36 x
Mixed_7b.branch1x1.conv
3.27 · 10−3
1.82 · 10−3
1.80 x
Mixed_7b.branch3x3_1.conv
3.66 · 10−3
3.34 · 10−3
1.10 x
Mixed_7b.branch3x3_2a.conv
2.51 · 10−3
2.34 · 10−3
1.07 x
Mixed_7b.branch3x3_2b.conv
2.43 · 10−3
2.24 · 10−3
1.09 x
Mixed_7b.branch3x3dbl_1.conv
3.70 · 10−3
2.57 · 10−3
1.44 x
Mixed_7b.branch3x3dbl_2.conv
2.03 · 10−2
1.45 · 10−2
1.40 x
Mixed_7b.branch_pool.conv
2.89 · 10−3
1.57 · 10−3
1.84 x
Mixed_7c.branch1x1.conv
7.88 · 10−3
7.66 · 10−3
1.03 x
Mixed_7c.branch3x3_1.conv
7.88 · 10−3
7.66 · 10−3
1.03 x
Mixed_7c.branch3x3dbl_1.conv
7.92 · 10−3
7.67 · 10−3
1.03 x
Mixed_7c.branch_pool.conv
7.92 · 10−3
7.67 · 10−3
1.03 x"
REFERENCES,0.7876712328767124,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7886497064579256,"3.43 · 10−2
4.11 · 10−3
8.33 x
1.91 · 10−1
9.18 · 10−2
2.08 x
1.88 · 10−1
9.44 · 10−2
1.99 x
1.74 · 10−3
2.41 · 10−3
0.72 x
1.08 · 10−1
6.16 · 10−2
1.76 x
2.40 · 10−3
2.13 · 10−3
1.13 x
2.41 · 10−3
3.65 · 10−3
0.66 x
4.39 · 10−2
3.29 · 10−2
1.33 x
1.38 · 10−2
7.31 · 10−3
1.89 x
2.46 · 10−2
1.42 · 10−2
1.73 x
2.40 · 10−3
2.02 · 10−3
1.19 x
3.27 · 10−3
4.89 · 10−3
0.67 x
3.27 · 10−3
4.88 · 10−3
0.67 x
4.88 · 10−3
6.68 · 10−3
0.73 x
4.88 · 10−3
6.69 · 10−3
0.73 x
7.78 · 10−2
3.23 · 10−2
2.41 x
1.41 · 10−2
7.15 · 10−3
1.97 x
4.80 · 10−3
4.22 · 10−3
1.14 x
4.80 · 10−3
6.03 · 10−3
0.80 x
1.50 · 10−3
3.58 · 10−3
0.42 x
1.46 · 10−3
3.58 · 10−3
0.41 x
1.45 · 10−3
3.56 · 10−3
0.41 x
1.46 · 10−3
3.49 · 10−3
0.42 x
4.80 · 10−3
4.90 · 10−3
0.98 x
2.08 · 10−3
5.08 · 10−3
0.41 x
2.04 · 10−3
5.23 · 10−3
0.39 x
2.05 · 10−3
5.25 · 10−3
0.39 x
2.04 · 10−3
5.12 · 10−3
0.40 x
2.90 · 10−3
7.61 · 10−3
0.38 x
2.91 · 10−3
7.31 · 10−3
0.40 x
3.82 · 10−4
6.10 · 10−4
0.63 x
8.52 · 10−4
1.09 · 10−3
0.78 x
6.98 · 10−3
2.68 · 10−3
2.60 x
7.04 · 10−3
2.94 · 10−3
2.39 x
2.39 · 10−3
1.76 · 10−3
1.36 x
2.83 · 10−3
3.36 · 10−3
0.84 x
3.03 · 10−3
2.32 · 10−3
1.31 x
2.98 · 10−3
2.16 · 10−3
1.38 x
2.83 · 10−3
2.43 · 10−3
1.17 x
1.94 · 10−2
1.40 · 10−2
1.39 x
2.26 · 10−3
1.57 · 10−3
1.44 x
6.73 · 10−3
7.66 · 10−3
0.88 x
6.73 · 10−3
7.66 · 10−3
0.88 x
6.74 · 10−3
7.67 · 10−3
0.88 x
6.74 · 10−3
7.67 · 10−3
0.88 x"
REFERENCES,0.7896281800391389,"(i) MobileNetV2, input shape (32, 3, 256, 256)"
REFERENCES,0.7906066536203522,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.7915851272015656,"features.0.0
2.30 · 10−2
3.63 · 10−3
6.34 x
features.1.conv.0.0
2.84 · 10−2
3.71 · 10−2
0.76 x
features.1.conv.1
7.35 · 10−3
5.48 · 10−3
1.34 x
features.2.conv.0.0
4.28 · 10−3
2.92 · 10−3
1.47 x
features.2.conv.1.0
3.98 · 10−2
2.51 · 10−2
1.59 x
features.2.conv.2
5.33 · 10−3
4.67 · 10−3
1.14 x
features.3.conv.0.0
1.63 · 10−3
1.37 · 10−3
1.19 x
features.3.conv.1.0
2.07 · 10−2
3.67 · 10−2
0.56 x
features.3.conv.2
9.72 · 10−3
9.38 · 10−3
1.04 x
features.4.conv.1.0
1.15 · 10−2
1.02 · 10−2
1.13 x
features.4.conv.2
2.82 · 10−3
2.65 · 10−3
1.06 x
features.5.conv.0.0
1.05 · 10−3
7.05 · 10−4
1.49 x
features.5.conv.1.0
6.38 · 10−3
1.19 · 10−2
0.54 x
features.5.conv.2
3.39 · 10−3
3.16 · 10−3
1.07 x
features.7.conv.1.0
3.66 · 10−3
3.66 · 10−3
1.00 x
features.7.conv.2
1.41 · 10−3
1.28 · 10−3
1.10 x
features.8.conv.0.0
9.96 · 10−4
6.18 · 10−4
1.61 x
features.8.conv.1.0
2.88 · 10−3
6.25 · 10−3
0.46 x
features.8.conv.2
2.36 · 10−3
2.24 · 10−3
1.06 x
features.11.conv.2
2.33 · 10−3
2.22 · 10−3
1.05 x
features.12.conv.0.0
9.43 · 10−4
7.06 · 10−4
1.34 x
features.12.conv.1.0
4.07 · 10−3
8.89 · 10−3
0.46 x
features.12.conv.2
3.97 · 10−3
3.84 · 10−3
1.03 x
features.14.conv.1.0
2.41 · 10−3
2.66 · 10−3
0.91 x
features.14.conv.2
1.50 · 10−3
1.23 · 10−3
1.22 x
features.15.conv.0.0
9.14 · 10−4
6.34 · 10−4
1.44 x
features.15.conv.1.0
9.60 · 10−4
4.01 · 10−3
0.24 x
features.15.conv.2
2.57 · 10−3
2.35 · 10−3
1.10 x
features.17.conv.2
2.57 · 10−3
2.35 · 10−3
1.10 x
features.18.0
1.15 · 10−3
7.91 · 10−4
1.46 x"
REFERENCES,0.7925636007827789,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.7935420743639922,"2.24 · 10−2
3.65 · 10−3
6.13 x
2.84 · 10−2
3.72 · 10−2
0.76 x
2.95 · 10−3
5.50 · 10−3
0.54 x
1.50 · 10−3
2.91 · 10−3
0.51 x
3.98 · 10−2
2.51 · 10−2
1.59 x
3.06 · 10−3
5.04 · 10−3
0.61 x
7.02 · 10−4
1.39 · 10−3
0.50 x
2.06 · 10−2
3.68 · 10−2
0.56 x
6.47 · 10−3
9.36 · 10−3
0.69 x
1.15 · 10−2
1.02 · 10−2
1.13 x
1.77 · 10−3
2.64 · 10−3
0.67 x
3.84 · 10−4
7.08 · 10−4
0.54 x
6.36 · 10−3
1.19 · 10−2
0.53 x
2.10 · 10−3
3.18 · 10−3
0.66 x
3.69 · 10−3
3.67 · 10−3
1.01 x
7.93 · 10−4
1.28 · 10−3
0.62 x
3.37 · 10−4
6.26 · 10−4
0.54 x
2.87 · 10−3
6.26 · 10−3
0.46 x
1.55 · 10−3
2.24 · 10−3
0.69 x
1.55 · 10−3
2.24 · 10−3
0.69 x
3.87 · 10−4
7.07 · 10−4
0.55 x
4.04 · 10−3
8.90 · 10−3
0.45 x
2.97 · 10−3
3.85 · 10−3
0.77 x
2.39 · 10−3
2.66 · 10−3
0.90 x
9.00 · 10−4
1.23 · 10−3
0.73 x
3.38 · 10−4
6.23 · 10−4
0.54 x
9.83 · 10−4
4.03 · 10−3
0.24 x
1.85 · 10−3
2.35 · 10−3
0.79 x
1.85 · 10−3
2.35 · 10−3
0.79 x
4.79 · 10−4
7.91 · 10−4
0.61 x"
REFERENCES,0.7945205479452054,"F.6
KFAC-reduce Factor"
REFERENCES,0.7954990215264188,"We compare TN and TN+opt with a PyTorch implementation of the input-based KFAC-reduce
factor based on torch.nn.functional.unfold. Figure F23 visualizes the performance ratios for
different convolution categories. Table F9 contains the detailed run times and performance factors."
REFERENCES,0.7964774951076321,"General
Dense mix
Dense
Down"
REFERENCES,0.7974559686888454,10−0.5 100 100.5
REFERENCES,0.7984344422700587,TN versus PT (logarithmic)
REFERENCES,0.799412915851272,"Figure F23: KFAC-reduce factor performance ratios of TN versus PT and TN+opt versus PT for
different convolution types on GPU."
REFERENCES,0.8003913894324853,Table F9: KFAC-reduce factor performance comparison on GPU.
REFERENCES,0.8013698630136986,"(a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32)"
REFERENCES,0.8023483365949119,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.8033268101761253,"conv1.0
8.88 · 10−4
2.26 · 10−3
0.39 x
conv2.0
1.41 · 10−3
1.79 · 10−3
0.79 x
conv3.1
1.33 · 10−3
2.31 · 10−3
0.57 x"
REFERENCES,0.8043052837573386,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.8052837573385518,"8.59 · 10−4
2.41 · 10−3
0.36 x
1.29 · 10−3
1.75 · 10−3
0.74 x
1.46 · 10−3
2.37 · 10−3
0.61 x"
REFERENCES,0.8062622309197651,"(b) F-MNIST 2c2d, input shape (128, 1, 28, 28)"
REFERENCES,0.8072407045009785,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.8082191780821918,"conv1.1
1.10 · 10−3
1.67 · 10−3
0.66 x
conv2.1
1.58 · 10−3
2.57 · 10−3
0.62 x"
REFERENCES,0.8091976516634051,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.8101761252446184,"1.01 · 10−3
1.83 · 10−3
0.55 x
1.54 · 10−3
2.76 · 10−3
0.56 x"
REFERENCES,0.8111545988258317,"(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32)"
REFERENCES,0.812133072407045,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.8131115459882583,"conv1.1
1.11 · 10−3
1.84 · 10−3
0.60 x
conv2.1
3.26 · 10−3
8.32 · 10−3
0.39 x
conv3.1
3.09 · 10−3
3.56 · 10−3
0.87 x
conv4.1
1.45 · 10−3
3.09 · 10−3
0.47 x
conv5.1
2.59 · 10−3
5.63 · 10−3
0.46 x
conv6.1
2.46 · 10−3
3.49 · 10−3
0.71 x
conv7.0
1.55 · 10−3
3.03 · 10−3
0.51 x
conv8.1
1.14 · 10−3
1.46 · 10−3
0.78 x
conv9.1
1.14 · 10−3
1.46 · 10−3
0.79 x"
REFERENCES,0.8140900195694716,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.815068493150685,"1.07 · 10−3
1.89 · 10−3
0.56 x
3.19 · 10−3
8.24 · 10−3
0.39 x
3.08 · 10−3
3.56 · 10−3
0.87 x
1.44 · 10−3
3.09 · 10−3
0.47 x
2.55 · 10−3
5.62 · 10−3
0.45 x
2.43 · 10−3
3.48 · 10−3
0.70 x
1.53 · 10−3
3.02 · 10−3
0.51 x
3.59 · 10−4
1.36 · 10−3
0.26 x
3.59 · 10−4
1.36 · 10−3
0.26 x"
REFERENCES,0.8160469667318982,"(d) Alexnet, input shape (32, 3, 256, 256)"
REFERENCES,0.8170254403131115,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.8180039138943248,"features.0
1.86 · 10−3
4.22 · 10−3
0.44 x
features.3
1.60 · 10−3
3.88 · 10−3
0.41 x
features.6
1.51 · 10−3
1.64 · 10−3
0.92 x
features.8
1.77 · 10−3
3.02 · 10−3
0.59 x
features.10
1.56 · 10−3
1.96 · 10−3
0.79 x"
REFERENCES,0.8189823874755382,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.8199608610567515,"1.84 · 10−3
4.22 · 10−3
0.44 x
1.53 · 10−3
3.89 · 10−3
0.39 x
1.43 · 10−3
1.63 · 10−3
0.88 x
1.73 · 10−3
3.02 · 10−3
0.57 x
1.51 · 10−3
1.96 · 10−3
0.77 x"
REFERENCES,0.8209393346379648,"(e) ResNet18, input shape (32, 3, 256, 256)"
REFERENCES,0.821917808219178,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.8228962818003914,"conv1
1.79 · 10−3
5.41 · 10−3
0.33 x
layer1.0.conv1
2.24 · 10−3
5.35 · 10−3
0.42 x
layer2.0.conv1
2.23 · 10−3
1.99 · 10−3
1.12 x
layer2.0.conv2
1.47 · 10−3
3.02 · 10−3
0.49 x
layer2.0.downsample.0
1.85 · 10−3
7.70 · 10−4
2.40 x
layer3.0.conv1
1.46 · 10−3
1.21 · 10−3
1.21 x
layer3.0.conv2
1.49 · 10−3
1.96 · 10−3
0.76 x
layer3.0.downsample.0
1.26 · 10−3
5.44 · 10−4
2.31 x
layer4.0.conv1
1.49 · 10−3
1.33 · 10−3
1.12 x
layer4.0.conv2
1.60 · 10−3
1.86 · 10−3
0.86 x
layer4.0.downsample.0
9.63 · 10−4
5.25 · 10−4
1.83 x"
REFERENCES,0.8238747553816047,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.824853228962818,"1.78 · 10−3
5.40 · 10−3
0.33 x
2.20 · 10−3
5.32 · 10−3
0.41 x
2.16 · 10−3
1.96 · 10−3
1.10 x
1.47 · 10−3
3.04 · 10−3
0.49 x
3.24 · 10−4
7.49 · 10−4
0.43 x
1.45 · 10−3
1.21 · 10−3
1.20 x
1.36 · 10−3
1.95 · 10−3
0.70 x
2.68 · 10−4
5.44 · 10−4
0.49 x
1.44 · 10−3
1.33 · 10−3
1.08 x
1.62 · 10−3
1.86 · 10−3
0.87 x
2.57 · 10−4
4.11 · 10−4
0.63 x"
REFERENCES,0.8258317025440313,"(f) ResNext101, input shape (32, 3, 256, 256)"
REFERENCES,0.8268101761252447,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.8277886497064579,"conv1
1.78 · 10−3
5.38 · 10−3
0.33 x
layer1.0.conv1
1.87 · 10−3
1.73 · 10−3
1.08 x
layer1.0.conv2
4.44 · 10−2
1.94 · 10−2
2.29 x
layer1.0.conv3
6.09 · 10−3
5.57 · 10−3
1.09 x
layer2.0.conv1
6.09 · 10−3
5.57 · 10−3
1.09 x
layer2.0.conv2
1.37 · 10−2
1.18 · 10−2
1.16 x
layer2.0.conv3
3.81 · 10−3
3.02 · 10−3
1.26 x
layer2.0.downsample.0
6.08 · 10−3
1.77 · 10−3
3.44 x
layer2.1.conv2
4.16 · 10−3
9.91 · 10−3
0.42 x
layer3.0.conv1
3.81 · 10−3
3.02 · 10−3
1.26 x
layer3.0.conv2
7.88 · 10−3
6.42 · 10−3
1.23 x
layer3.0.conv3
1.61 · 10−3
1.78 · 10−3
0.91 x
layer3.0.downsample.0
3.80 · 10−3
1.17 · 10−3
3.24 x
layer3.1.conv2
2.25 · 10−3
5.41 · 10−3
0.42 x
layer4.0.conv1
1.61 · 10−3
1.77 · 10−3
0.91 x
layer4.0.conv2
4.21 · 10−3
5.29 · 10−3
0.80 x
layer4.0.conv3
1.23 · 10−3
1.45 · 10−3
0.85 x
layer4.0.downsample.0
1.62 · 10−3
8.91 · 10−4
1.82 x
layer4.1.conv2
2.18 · 10−3
4.73 · 10−3
0.46 x"
REFERENCES,0.8287671232876712,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.8297455968688845,"1.77 · 10−3
5.39 · 10−3
0.33 x
4.47 · 10−4
1.73 · 10−3
0.26 x
4.43 · 10−2
1.94 · 10−2
2.29 x
1.21 · 10−3
5.57 · 10−3
0.22 x
1.21 · 10−3
5.58 · 10−3
0.22 x
1.37 · 10−2
1.18 · 10−2
1.16 x
7.44 · 10−4
3.02 · 10−3
0.25 x
7.12 · 10−4
1.77 · 10−3
0.40 x
4.16 · 10−3
9.90 · 10−3
0.42 x
7.32 · 10−4
3.02 · 10−3
0.24 x
7.90 · 10−3
6.43 · 10−3
1.23 x
5.42 · 10−4
1.80 · 10−3
0.30 x
5.10 · 10−4
1.21 · 10−3
0.42 x
2.26 · 10−3
5.41 · 10−3
0.42 x
5.44 · 10−4
1.80 · 10−3
0.30 x
4.21 · 10−3
5.29 · 10−3
0.80 x
7.68 · 10−4
1.44 · 10−3
0.53 x
4.97 · 10−4
8.93 · 10−4
0.56 x
2.16 · 10−3
4.72 · 10−3
0.46 x"
REFERENCES,0.8307240704500979,"(g) ConvNeXt-base, input shape (32, 3, 256, 256)"
REFERENCES,0.8317025440313112,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.8326810176125244,"features.0.0
1.72 · 10−3
1.02 · 10−3
1.69 x
features.1.0.block.0
1.53 · 10−2
4.41 · 10−2
0.35 x
features.2.1
3.80 · 10−3
1.99 · 10−3
1.91 x
features.3.0.block.0
8.21 · 10−3
2.22 · 10−2
0.37 x
features.4.1
2.32 · 10−3
1.21 · 10−3
1.92 x
features.5.0.block.0
4.62 · 10−3
1.18 · 10−2
0.39 x
features.6.1
1.40 · 10−3
1.10 · 10−3
1.27 x
features.7.0.block.0
1.38 · 10−3
6.35 · 10−3
0.22 x"
REFERENCES,0.8336594911937377,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.8346379647749511,"7.52 · 10−4
1.01 · 10−3
0.74 x
1.53 · 10−2
4.40 · 10−2
0.35 x
8.44 · 10−4
1.99 · 10−3
0.43 x
8.19 · 10−3
2.22 · 10−2
0.37 x
6.85 · 10−4
1.18 · 10−3
0.58 x
4.57 · 10−3
1.16 · 10−2
0.40 x
9.28 · 10−4
1.02 · 10−3
0.91 x
1.35 · 10−3
6.34 · 10−3
0.21 x"
REFERENCES,0.8356164383561644,"(h) InceptionV3, input shape (32, 3, 299, 299)"
REFERENCES,0.8365949119373777,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.837573385518591,"Conv2d_1a_3x3.conv
2.39 · 10−3
2.03 · 10−3
1.18 x
Conv2d_2a_3x3.conv
4.42 · 10−3
1.30 · 10−2
0.34 x
Conv2d_2b_3x3.conv
4.33 · 10−3
1.30 · 10−2
0.33 x
Conv2d_3b_1x1.conv
1.32 · 10−2
2.16 · 10−3
6.12 x
Conv2d_4a_3x3.conv
2.72 · 10−3
8.00 · 10−3
0.34 x
Mixed_5b.branch1x1.conv
1.43 · 10−3
1.57 · 10−3
0.91 x
Mixed_5b.branch5x5_1.conv
1.43 · 10−3
1.57 · 10−3
0.91 x
Mixed_5b.branch5x5_2.conv
1.56 · 10−3
3.73 · 10−3
0.42 x
Mixed_5b.branch3x3dbl_2.conv
1.57 · 10−3
1.91 · 10−3
0.82 x
Mixed_5b.branch3x3dbl_3.conv
1.48 · 10−3
2.66 · 10−3
0.56 x
Mixed_5b.branch_pool.conv
1.45 · 10−3
1.59 · 10−3
0.91 x
Mixed_5c.branch1x1.conv
1.79 · 10−3
2.00 · 10−3
0.90 x
Mixed_5c.branch5x5_1.conv
1.79 · 10−3
2.00 · 10−3
0.90 x
Mixed_5d.branch1x1.conv
1.99 · 10−3
2.22 · 10−3
0.90 x
Mixed_5d.branch5x5_1.conv
1.97 · 10−3
2.20 · 10−3
0.89 x
Mixed_6a.branch3x3.conv
2.81 · 10−3
3.18 · 10−3
0.88 x
Mixed_6a.branch3x3dbl_3.conv
1.30 · 10−3
1.22 · 10−3
1.07 x
Mixed_6b.branch1x1.conv
1.49 · 10−3
1.64 · 10−3
0.91 x
Mixed_6b.branch7x7_1.conv
1.46 · 10−3
1.64 · 10−3
0.89 x
Mixed_6b.branch7x7_2.conv
1.01 · 10−3
1.07 · 10−3
0.94 x
Mixed_6b.branch7x7_3.conv
9.45 · 10−4
1.23 · 10−3
0.77 x
Mixed_6b.branch7x7dbl_2.conv
1.06 · 10−3
1.23 · 10−3
0.86 x
Mixed_6b.branch7x7dbl_5.conv
1.14 · 10−3
1.10 · 10−3
1.04 x
Mixed_6c.branch7x7_1.conv
1.43 · 10−3
1.62 · 10−3
0.88 x
Mixed_6c.branch7x7_2.conv
1.01 · 10−3
1.34 · 10−3
0.75 x
Mixed_6c.branch7x7_3.conv
1.07 · 10−3
1.69 · 10−3
0.63 x
Mixed_6c.branch7x7dbl_2.conv
8.59 · 10−4
1.67 · 10−3
0.51 x
Mixed_6c.branch7x7dbl_5.conv
1.01 · 10−3
1.33 · 10−3
0.76 x
Mixed_6e.branch7x7_2.conv
1.01 · 10−3
1.48 · 10−3
0.69 x
Mixed_6e.branch7x7_3.conv
9.53 · 10−4
1.77 · 10−3
0.54 x
AuxLogits.conv0.conv
9.97 · 10−4
6.04 · 10−4
1.65 x
AuxLogits.conv1.conv
1.05 · 10−3
1.09 · 10−3
0.97 x
Mixed_7a.branch3x3_2.conv
1.30 · 10−3
7.75 · 10−4
1.68 x
Mixed_7a.branch7x7x3_4.conv
1.34 · 10−3
1.14 · 10−3
1.18 x
Mixed_7b.branch1x1.conv
1.03 · 10−3
9.07 · 10−4
1.13 x
Mixed_7b.branch3x3_1.conv
1.16 · 10−3
9.30 · 10−4
1.25 x
Mixed_7b.branch3x3_2a.conv
1.13 · 10−3
7.94 · 10−4
1.43 x
Mixed_7b.branch3x3_2b.conv
1.07 · 10−3
8.53 · 10−4
1.25 x
Mixed_7b.branch3x3dbl_1.conv
1.16 · 10−3
9.32 · 10−4
1.25 x
Mixed_7b.branch3x3dbl_2.conv
1.67 · 10−3
1.55 · 10−3
1.08 x
Mixed_7b.branch_pool.conv
1.16 · 10−3
6.79 · 10−4
1.71 x
Mixed_7c.branch1x1.conv
1.23 · 10−3
1.45 · 10−3
0.85 x
Mixed_7c.branch3x3_1.conv
1.21 · 10−3
1.44 · 10−3
0.84 x
Mixed_7c.branch3x3dbl_1.conv
1.21 · 10−3
1.43 · 10−3
0.84 x
Mixed_7c.branch_pool.conv
1.21 · 10−3
1.44 · 10−3
0.84 x"
REFERENCES,0.8385518590998043,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.8395303326810176,"2.36 · 10−3
2.00 · 10−3
1.18 x
4.38 · 10−3
1.30 · 10−2
0.34 x
4.32 · 10−3
1.30 · 10−2
0.33 x
5.53 · 10−4
2.16 · 10−3
0.26 x
2.74 · 10−3
8.02 · 10−3
0.34 x
4.52 · 10−4
1.57 · 10−3
0.29 x
4.53 · 10−4
1.57 · 10−3
0.29 x
1.35 · 10−3
3.72 · 10−3
0.36 x
1.44 · 10−3
1.91 · 10−3
0.76 x
1.41 · 10−3
2.66 · 10−3
0.53 x
4.62 · 10−4
1.59 · 10−3
0.29 x
5.48 · 10−4
2.00 · 10−3
0.27 x
5.46 · 10−4
2.00 · 10−3
0.27 x
5.90 · 10−4
2.21 · 10−3
0.27 x
5.83 · 10−4
2.18 · 10−3
0.27 x
2.76 · 10−3
3.18 · 10−3
0.87 x
1.44 · 10−3
1.29 · 10−3
1.11 x
5.47 · 10−4
1.62 · 10−3
0.34 x
5.60 · 10−4
1.64 · 10−3
0.34 x
7.56 · 10−4
1.10 · 10−3
0.69 x
7.61 · 10−4
1.23 · 10−3
0.62 x
7.62 · 10−4
1.23 · 10−3
0.62 x
7.56 · 10−4
1.10 · 10−3
0.69 x
5.47 · 10−4
1.64 · 10−3
0.33 x
7.64 · 10−4
1.35 · 10−3
0.56 x
7.69 · 10−4
1.68 · 10−3
0.46 x
7.67 · 10−4
1.69 · 10−3
0.45 x
7.64 · 10−4
1.35 · 10−3
0.57 x
7.68 · 10−4
1.49 · 10−3
0.51 x
7.38 · 10−4
1.79 · 10−3
0.41 x
3.66 · 10−4
6.58 · 10−4
0.56 x
9.31 · 10−4
1.09 · 10−3
0.85 x
1.26 · 10−3
7.67 · 10−4
1.64 x
1.36 · 10−3
1.14 · 10−3
1.19 x
5.08 · 10−4
9.07 · 10−4
0.56 x
5.20 · 10−4
9.10 · 10−4
0.57 x
6.89 · 10−4
7.93 · 10−4
0.87 x
7.60 · 10−4
8.51 · 10−4
0.89 x
5.38 · 10−4
9.32 · 10−4
0.58 x
1.59 · 10−3
1.55 · 10−3
1.02 x
5.19 · 10−4
6.85 · 10−4
0.76 x
7.69 · 10−4
1.44 · 10−3
0.53 x
7.80 · 10−4
1.45 · 10−3
0.54 x
7.66 · 10−4
1.44 · 10−3
0.53 x
7.80 · 10−4
1.45 · 10−3
0.54 x"
REFERENCES,0.8405088062622309,"(i) MobileNetV2, input shape (32, 3, 256, 256)"
REFERENCES,0.8414872798434442,"Name
TN [s]
PT [s]
Factor"
REFERENCES,0.8424657534246576,"features.0.0
1.90 · 10−3
1.66 · 10−3
1.15 x
features.1.conv.0.0
2.69 · 10−3
9.87 · 10−3
0.27 x
features.1.conv.1
1.12 · 10−2
3.03 · 10−3
3.70 x
features.2.conv.0.0
1.80 · 10−3
1.77 · 10−3
1.02 x
features.2.conv.1.0
7.01 · 10−3
9.06 · 10−3
0.77 x
features.2.conv.2
2.59 · 10−3
2.38 · 10−3
1.09 x
features.3.conv.0.0
1.44 · 10−3
9.19 · 10−4
1.57 x
features.3.conv.1.0
2.99 · 10−3
1.12 · 10−2
0.27 x
features.3.conv.2
3.65 · 10−3
3.38 · 10−3
1.08 x
features.4.conv.1.0
3.01 · 10−3
3.76 · 10−3
0.80 x
features.4.conv.2
1.38 · 10−3
1.16 · 10−3
1.19 x
features.5.conv.0.0
8.51 · 10−4
5.17 · 10−4
1.65 x
features.5.conv.1.0
1.38 · 10−3
3.99 · 10−3
0.34 x
features.5.conv.2
1.68 · 10−3
1.36 · 10−3
1.24 x
features.7.conv.1.0
1.37 · 10−3
1.69 · 10−3
0.81 x
features.7.conv.2
8.59 · 10−4
7.05 · 10−4
1.22 x
features.8.conv.0.0
8.45 · 10−4
4.92 · 10−4
1.72 x
features.8.conv.1.0
1.16 · 10−3
2.36 · 10−3
0.49 x
features.8.conv.2
8.73 · 10−4
9.30 · 10−4
0.94 x
features.11.conv.2
9.89 · 10−4
9.49 · 10−4
1.04 x
features.12.conv.0.0
9.55 · 10−4
5.32 · 10−4
1.80 x
features.12.conv.1.0
1.51 · 10−3
3.23 · 10−3
0.47 x
features.12.conv.2
1.14 · 10−3
1.24 · 10−3
0.92 x
features.14.conv.1.0
1.51 · 10−3
1.61 · 10−3
0.94 x
features.14.conv.2
1.14 · 10−3
6.83 · 10−4
1.67 x
features.15.conv.0.0
9.53 · 10−4
5.23 · 10−4
1.82 x
features.15.conv.1.0
1.41 · 10−3
2.25 · 10−3
0.63 x
features.15.conv.2
1.15 · 10−3
8.81 · 10−4
1.31 x
features.17.conv.2
1.16 · 10−3
8.80 · 10−4
1.31 x
features.18.0
9.51 · 10−4
5.40 · 10−4
1.76 x"
REFERENCES,0.8434442270058709,"TN + opt [s]
PT [s]
Factor"
REFERENCES,0.8444227005870841,"1.91 · 10−3
1.68 · 10−3
1.14 x
2.70 · 10−3
9.89 · 10−3
0.27 x
7.12 · 10−4
3.00 · 10−3
0.24 x
4.64 · 10−4
1.76 · 10−3
0.26 x
6.99 · 10−3
9.06 · 10−3
0.77 x
6.08 · 10−4
2.40 · 10−3
0.25 x
2.96 · 10−4
9.40 · 10−4
0.31 x
2.99 · 10−3
1.12 · 10−2
0.27 x
7.92 · 10−4
3.40 · 10−3
0.23 x
2.99 · 10−3
3.77 · 10−3
0.79 x
3.53 · 10−4
1.16 · 10−3
0.30 x
2.77 · 10−4
5.34 · 10−4
0.52 x
1.36 · 10−3
3.99 · 10−3
0.34 x
3.94 · 10−4
1.35 · 10−3
0.29 x
1.35 · 10−3
1.69 · 10−3
0.80 x
2.52 · 10−4
7.00 · 10−4
0.36 x
2.49 · 10−4
4.93 · 10−4
0.51 x
1.12 · 10−3
2.35 · 10−3
0.47 x
3.06 · 10−4
9.29 · 10−4
0.33 x
3.06 · 10−4
9.25 · 10−4
0.33 x
2.50 · 10−4
5.14 · 10−4
0.49 x
1.27 · 10−3
3.22 · 10−3
0.39 x
3.94 · 10−4
1.17 · 10−3
0.34 x
1.45 · 10−3
1.61 · 10−3
0.90 x
3.67 · 10−4
6.80 · 10−4
0.54 x
2.74 · 10−4
5.23 · 10−4
0.52 x
1.37 · 10−3
2.25 · 10−3
0.61 x
4.46 · 10−4
8.83 · 10−4
0.51 x
4.36 · 10−4
8.58 · 10−4
0.51 x
2.50 · 10−4
5.22 · 10−4
0.48 x"
REFERENCES,0.8454011741682974,"G
Memory Evaluation Details (CPU)"
REFERENCES,0.8463796477495108,"Here, we investigate the peak memory consumption of our proposed TN implementations."
REFERENCES,0.8473581213307241,"G.1
Theoretical & Empirical Analysis for KFAC-reduce Factor"
REFERENCES,0.8483365949119374,"We assume a two-dimensional convolution with input X of shape (Cin, I1, I2), output of shape
(Cout, O1, O2) and kernel of shape (Cout, Cin, K1, K2). The analysis with a batch dimension is
analogous; hence we suppress it here to de-clutter the notation."
REFERENCES,0.8493150684931506,"The main difference between the default and our proposed TN implementation of ˆΩfrom §3.3 lies
in the computation of the averaged unfolded input JXK(avg) := 1/(O1O2)1⊤
O1O2JXK which consists of
CinK1K2 numbers. In the following, we will look at the extra memory on top of storing the input X,
the averaged unfolded input JXK(avg), and the result ˆΩ."
REFERENCES,0.850293542074364,"Default implementation:
The standard implementation computes JXK(avg) via the unfolded input
JXK and thus requires extra storage of CinK1K2O1O2 numbers."
REFERENCES,0.8512720156555773,"TN implementation (general case):
The TN implementation requires storing the averaged index
patterns Π(i,avg) := 1/Oi
POi
o=1[Π(i)]:,o,: for i = 1, 2. These are directly computed via a slight
modification of Algorithm D1 and require storing I1K1 + I2K2 numbers. In contrast to the default
implementation, spatial dimensions are de-coupled and there is no dependency on Cin."
REFERENCES,0.8522504892367906,"TN implementation (structured case):
For structured convolutions (Figure 5) we can describe the
action of the index pattern tensor through reshape and narrowing operations. ML libraries usually
perform these without allocating additional memory. Hence, our symbolic simplifications completely
eliminate the allocation of temporary intermediates to compute JXK(avg)."
REFERENCES,0.8532289628180039,"Empirical results:
To demonstrate the memory reduction inside the computation of ˆΩwe measure
its peak memory with the memory-profiler library and subtract the memory required to store X
and ˆΩ. This approximates the extra internal memory requirement of an implementation. With the
setup of §F we report the minimum additional memory over 50 independent runs in Table G10. We
consistently observe that the TN implementation has lower peak memory, which is further reduced by
our symbolic simplifications (see for example the effect on ResNext101’s dense and down-sampling
convolutions in Table G10f)."
REFERENCES,0.8542074363992173,"Our theoretical analysis from above suggests that the peak memory difference becomes most vis-
ible for many channels with large kernel and output sizes. One example are ConxNeXt-base’s
features.1.0.block.0 convolutions with K1 = K2 = 7, O1 = O2 = 64, and Cin = 128 (Ta-
ble E4g). For those convolutions, we observe that the default implementation requires an additional
3,140 MiB (≈3 GiB!) of memory, whereas the TN implementation has zero extra memory demand
(Table G10g). This is consistent with our theoretical analysis in that the overhead is storing the
unfolded input, which has (N = 32) · (Cin = 128) · (O1 = 64) · (O2 = 64) · (K1 = 7) · (K2 =
7) = 822, 083, 584 float32 entries, corresponding to 3,136 MiB."
REFERENCES,0.8551859099804305,"Table G10: Additional internally required memory to compute the KFAC-reduce factor (measured on
CPU). The value 0 indicates that an implementation’s peak memory matches the memory consumption
of its input X and result ˆΩ."
REFERENCES,0.8561643835616438,"(a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32)"
REFERENCES,0.8571428571428571,"Name
TN [MiB]"
REFERENCES,0.8581213307240705,"conv1.0
0.0
conv2.0
0.0
conv3.1
0.0"
REFERENCES,0.8590998043052838,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.860078277886497,"0.0
0.0
0.0
0.0
0.0
0.0 Type"
REFERENCES,0.8610567514677103,"General
General
General"
REFERENCES,0.8620352250489237,"(b) F-MNIST 2c2d, input shape (128, 1, 28, 28)"
REFERENCES,0.863013698630137,"Name
TN [MiB]"
REFERENCES,0.8639921722113503,"conv1.1
0.0
conv2.1
0.0"
REFERENCES,0.8649706457925636,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.8659491193737769,"0.0
0.0
0.0
0.0 Type"
REFERENCES,0.8669275929549902,"General
General"
REFERENCES,0.8679060665362035,"(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32)"
REFERENCES,0.8688845401174168,"Name
TN [MiB]"
REFERENCES,0.8698630136986302,"conv1.1
0.0
conv2.1
0.0
conv3.1
0.0
conv4.1
0.0
conv5.1
0.0
conv6.1
0.0
conv7.0
0.0
conv8.1
0.0
conv9.1
0.0"
REFERENCES,0.8708414872798435,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.8718199608610567,"0.0
0.0
0.0
431
0.0
0.0
0.0
0.0
0.0
215
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0 Type"
REFERENCES,0.87279843444227,"General
General
General
General
General
General
General
Dense
Dense"
REFERENCES,0.8737769080234834,"(d) Alexnet, input shape (32, 3, 256, 256)"
REFERENCES,0.8747553816046967,"Name
TN [MiB]"
REFERENCES,0.87573385518591,"features.0
0.0
features.3
0.0
features.6
0.0
features.8
0.0
features.10
0.0"
REFERENCES,0.8767123287671232,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.8776908023483366,"0.0156
175
0.0
186
0.0156
0.0
0.0156
93.8
0.0195
0.0 Type"
REFERENCES,0.8786692759295499,"General
General
General
General
General"
REFERENCES,0.8796477495107632,"(e) ResNet18, input shape (32, 3, 256, 256)"
REFERENCES,0.8806262230919765,"Name
TN [MiB]"
REFERENCES,0.8816046966731899,"conv1
0.0
layer1.0.conv1
0.0
layer2.0.conv1
31.7
layer2.0.conv2
0.0
layer2.0.downsample.0
0.0
layer3.0.conv1
0.0
layer3.0.conv2
0.0
layer3.0.downsample.0
0.0
layer4.0.conv1
0.0
layer4.0.conv2
0.0
layer4.0.downsample.0
0.0"
REFERENCES,0.8825831702544031,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.8835616438356164,"0.0
293
0.0
287
0.0
71.1
0.0
143
0.0
0.0
0.0
0.0
0.0
70.8
0.0
0.0
0.0
0.0
80.3
0.0
0.0
0.0 Type"
REFERENCES,0.8845401174168297,"General
General
General
General
Down
General
General
Down
General
General
Down"
REFERENCES,0.8855185909980431,"(f) ResNext101, input shape (32, 3, 256, 256)"
REFERENCES,0.8864970645792564,"Name
TN [MiB]"
REFERENCES,0.8874755381604696,"conv1
0.0
layer1.0.conv1
0.0
layer1.0.conv2
576
layer1.0.conv3
128
layer2.0.conv1
128
layer2.0.conv2
256
layer2.0.conv3
0.0
layer2.0.downsample.0
128
layer2.1.conv2
0.0
layer3.0.conv1
0.0
layer3.0.conv2
128
layer3.0.conv3
0.0
layer3.0.downsample.0
0.0
layer3.1.conv2
0.0
layer4.0.conv1
0.0
layer4.0.conv2
0.0
layer4.0.conv3
0.0
layer4.0.downsample.0
0.0
layer4.1.conv2
0.0"
REFERENCES,0.8884540117416829,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.8894324853228963,"0.0
293
0.0
0.0
576
1150
0.0
127
0.0
127
256
575
0.0
0.0
0.0
19.3
0.0
575
0.0
0.0
128
288
0.0
0.0
0.0
0.0
0.0
288
0.0
0.0
0.0
144
0.0
0.0
0.0
0.0
0.0
144 Type"
REFERENCES,0.8904109589041096,"General
Dense
General
Dense
Dense
General
Dense
Down
General
Dense
General
Dense
Down
General
Dense
General
Dense
Down
General"
REFERENCES,0.8913894324853229,"(g) ConvNeXt-base, input shape (32, 3, 256, 256)"
REFERENCES,0.8923679060665362,"Name
TN [MiB]"
REFERENCES,0.8933463796477495,"features.0.0
0.0
features.1.0.block.0
0.0
features.2.1
0.0
features.3.0.block.0
0.0
features.4.1
0.0
features.5.0.block.0
0.0
features.6.1
0.0
features.7.0.block.0
0.0"
REFERENCES,0.8943248532289628,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.8953033268101761,"0.0
0.0
0.0
3140
0.0
0.0
0.0
1570
0.0
0.0
0.0
784
0.0
0.0
0.0
392 Type"
REFERENCES,0.8962818003913894,"Dense
General
Dense
General
Dense
General
Dense
General"
REFERENCES,0.8972602739726028,"(h) InceptionV3, input shape (32, 3, 299, 299)"
REFERENCES,0.898238747553816,"Name
TN [MiB]"
REFERENCES,0.8992172211350293,"Conv2d_1a_3x3.conv
54.6
Conv2d_2a_3x3.conv
86.7
Conv2d_2b_3x3.conv
84.4
Conv2d_3b_1x1.conv
166
Conv2d_4a_3x3.conv
52.0
Mixed_5b.branch1x1.conv
0.0
Mixed_5b.branch5x5_1.conv
0.0
Mixed_5b.branch5x5_2.conv
0.0
Mixed_5b.branch3x3dbl_2.conv
0.0
Mixed_5b.branch3x3dbl_3.conv
0.0
Mixed_5b.branch_pool.conv
0.0
Mixed_5c.branch1x1.conv
0.0
Mixed_5c.branch5x5_1.conv
0.0
Mixed_5d.branch1x1.conv
42.7
Mixed_5d.branch5x5_1.conv
42.8
Mixed_6a.branch3x3.conv
0.0
Mixed_6a.branch3x3dbl_3.conv
0.0
Mixed_6b.branch1x1.conv
0.0
Mixed_6b.branch7x7_1.conv
0.0
Mixed_6b.branch7x7_2.conv
0.0
Mixed_6b.branch7x7_3.conv
0.0
Mixed_6b.branch7x7dbl_2.conv
0.0
Mixed_6b.branch7x7dbl_5.conv
0.0
Mixed_6c.branch7x7_1.conv
0.0195
Mixed_6c.branch7x7_2.conv
0.0156
Mixed_6c.branch7x7_3.conv
0.0
Mixed_6c.branch7x7dbl_2.conv
0.0
Mixed_6c.branch7x7dbl_5.conv
0.0
Mixed_6e.branch7x7_2.conv
0.0
Mixed_6e.branch7x7_3.conv
0.0
AuxLogits.conv0.conv
0.0
AuxLogits.conv1.conv
0.0
Mixed_7a.branch3x3_2.conv
0.0
Mixed_7a.branch7x7x3_4.conv
0.0
Mixed_7b.branch1x1.conv
0.0
Mixed_7b.branch3x3_1.conv
0.0
Mixed_7b.branch3x3_2a.conv
0.0
Mixed_7b.branch3x3_2b.conv
0.0
Mixed_7b.branch3x3dbl_1.conv
0.0
Mixed_7b.branch3x3dbl_2.conv
0.0
Mixed_7b.branch_pool.conv
0.0
Mixed_7c.branch1x1.conv
0.0
Mixed_7c.branch3x3_1.conv
0.0
Mixed_7c.branch3x3dbl_1.conv
0.0
Mixed_7c.branch_pool.conv
0.0"
REFERENCES,0.9001956947162426,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.901174168297456,"0.0
73.0
86.7
759
84.4
758
0.0
0.0
0.0
442
0.0
0.0
0.0
0.0
0.0
178
0.0
84.8
0.0
128
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0 Type"
REFERENCES,0.9021526418786693,"General
General
General
Dense
General
Dense
Dense
General
General
General
Dense
Dense
Dense
Dense
Dense
General
General
Dense
Dense
Dense mix
Dense mix
Dense mix
Dense mix
Dense
Dense mix
Dense mix
Dense mix
Dense mix
Dense mix
Dense mix
Dense
General
General
General
Dense
Dense
Dense mix
Dense mix
Dense
General
Dense
Dense
Dense
Dense
Dense"
REFERENCES,0.9031311154598826,"(i) MobileNetV2, input shape (32, 3, 256, 256)"
REFERENCES,0.9041095890410958,"Name
TN [MiB]"
REFERENCES,0.9050880626223092,"features.0.0
0.0
features.1.conv.0.0
26.1
features.1.conv.1
128
features.2.conv.0.0
0.0
features.2.conv.1.0
192
features.2.conv.2
0.0
features.3.conv.0.0
0.0
features.3.conv.1.0
34.1
features.3.conv.2
71.7
features.4.conv.1.0
59.5
features.4.conv.2
0.0
features.5.conv.0.0
0.0
features.5.conv.1.0
0.0
features.5.conv.2
0.0
features.7.conv.1.0
0.0
features.7.conv.2
0.0
features.8.conv.0.0
0.0
features.8.conv.1.0
0.0
features.8.conv.2
0.0
features.11.conv.2
0.0
features.12.conv.0.0
0.0
features.12.conv.1.0
0.0
features.12.conv.2
0.0
features.14.conv.1.0
0.0
features.14.conv.2
0.0
features.15.conv.0.0
0.0
features.15.conv.1.0
0.0
features.15.conv.2
0.0
features.17.conv.2
0.0
features.18.0
0.0"
REFERENCES,0.9060665362035225,"TN + opt [MiB]
PT [MiB]"
REFERENCES,0.9070450097847358,"0.0
53.8
26.1
576
0.0
63.8
0.0
0.0
192
432
0.0
0.0
0.0
0.0
70.4
648
0.0
71.4
55.7
162
0.0
0.0
0.0
0.0
0.0
215
0.0
0.0
0.0
53.3
0.0
0.0
0.0
0.0
0.0
107
0.0
0.0
0.0
0.0
0.0
0.0
0.0
161
0.0
0.0
0.0
39.7
0.0
0.0
0.0
0.0
0.0
63.8
0.0
0.0
0.0
0.0
0.0
0.0 Type"
REFERENCES,0.9080234833659491,"General
General
Dense
Dense
General
Dense
Dense
General
Dense
General
Dense
Dense
General
Dense
General
Dense
Dense
General
Dense
Dense
Dense
General
Dense
General
Dense
Dense
General
Dense
Dense
Dense"
REFERENCES,0.9090019569471625,"H
Miscellaneous"
REFERENCES,0.9099804305283757,"H.1
Example: Associativity of Tensor Multiplication"
REFERENCES,0.910958904109589,"Here, we demonstrate associativity of tensor multiplication through an example. The technical
challenge is that an index can only be summed once there are no remaining tensors sharing it.
Therefore, we must carry indices that are summed in later multiplications in the intermediate results,
which requires some set arithmetic on the index sets."
REFERENCES,0.9119373776908023,"Let S1, S2, S3 be index tuples of the input tensors A, B, C, and S4 ⊆(S1 ∪S2 ∪S3) a valid output
index tuple of their tensor multiplication D = ∗(S1,S2,S3,S4)(A, B, C). We can either first multiply A
with B to obtain an intermediate tensor of index structure S1,2, or B with C to obtain an intermediate
tensor of index structure S2,3, before carrying out the remaining multiplications. To construct the
intermediate index structures, we divide the indices ˜S = (S1 ∪S2 ∪S3) \ S4 that are summed
over into those only shared between A, B given by ˜S1,2 = (S1 ∪S2) \ (S4 ∪S3), and those only
shared among B, C given by ˜S2,3 = (S2 ∪S3) \ (S4 ∪S1). This yields the intermediate indices
S1,2 = (S1 ∪S2) \ ˜S1,2 and S2,3 = (S2 ∪S3) \ ˜S2,3, and the parenthesizations"
REFERENCES,0.9129158512720157,"[D]S4 =
P
˜S\ ˜S1,2"
REFERENCES,0.913894324853229,"P
˜S1,2[A]S1[B]S2

[C]S3

=
P
˜S\ ˜S2,3[A]S1
P
˜S2,3[B]S2[C]S3
"
REFERENCES,0.9148727984344422,"⇔D = ∗(S1,2,S3,S4)
 
∗(S2,S3,S2,3)(A, B), C

= ∗(S1,S2,3,S4)
 
A, ∗(S1,S2,S1,2)(B, C)

.
(H20)"
REFERENCES,0.9158512720156555,"This generalizes to n-ary multiplication, allowing to break it down into smaller multiplications.
However, the index notation and set arithmetic from Equation (H20) quickly becomes impractical."
REFERENCES,0.9168297455968689,"H.2
Example: Matrix-matrix Multiplication as Tensor Multiplication"
REFERENCES,0.9178082191780822,"Here we provide a small self-contained example that demonstrates Equation (3) for matrix-matrix
multiplication."
REFERENCES,0.9187866927592955,"Consider two matrices A, B which are compatible for multiplication and let C = AB. In index
notation, we have
[C]i,k =
X"
REFERENCES,0.9197651663405088,"j
[A]i,j[B]j,k ."
REFERENCES,0.9207436399217221,"The index tuples are SA = (i, j), SB = (j, k), and SC = (i, k). Next, we evaluate which indices
are summed over. Since the order of those indices does not matter, we can interpret the tuples as sets
and use set arithmetic:"
REFERENCES,0.9217221135029354,"(SA ∪SB) \ SC = ((i, j) ∪(j, k)) \ (i, k) = (j) \ (i, k) = (j) ."
REFERENCES,0.9227005870841487,"Now we see that matrix-matrix multiplication is a case of tensor multiplication (Equation (3)),"
REFERENCES,0.923679060665362,"[C]SC =
X"
REFERENCES,0.9246575342465754,"(SA∪SB)\SC
[A]SA[B]SB = ∗(SA,SB,SC)(A, B) ."
REFERENCES,0.9256360078277887,NeurIPS Paper Checklist
CLAIMS,0.9266144814090019,1. Claims
CLAIMS,0.9275929549902152,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We provide a bullet point list of our contributions in §1, each of which
references the part of the paper that outlines the contribution.
Guidelines:"
CLAIMS,0.9285714285714286,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9295499021526419,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See §4.2 and §A.
Guidelines:"
CLAIMS,0.9305283757338552,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9315068493150684,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.9324853228962818,"Justification: The TN simplifications we provide in §4.1 follow straightforward from the
index pattern’s structure Equation (7) and are stated rigorously in §D.3, including their
assumptions."
CLAIMS,0.9334637964774951,Guidelines:
CLAIMS,0.9344422700587084,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9354207436399217,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9363992172211351,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9373776908023483,"Answer: [Yes]
Justification: We provide implementation details in §D, experimental and hardware details
in §F and G."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9383561643835616,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9393346379647749,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9403131115459883,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9412915851272016,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9422700587084148,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9432485322896281,"Justification: We will open-source the code to reproduce all our experiments, as well as the
raw data containing the results shown in the manuscript."
OPEN ACCESS TO DATA AND CODE,0.9442270058708415,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9452054794520548,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9461839530332681,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9471624266144814,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9481409001956947,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.949119373776908,Justification: See §F and G for details on the experimental setting.
OPEN ACCESS TO DATA AND CODE,0.9500978473581213,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9510763209393346,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.952054794520548,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9530332681017613,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9540117416829745,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9549902152641878,"Justification: Our run time plots contain box plots with medians and quartiles reported over
different convolutions, and the randomized backpropagation results show mean and standard
deviations for 10 different model and batch initializations."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9559686888454012,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9569471624266145,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9579256360078278,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.958904109589041,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9598825831702544,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9608610567514677,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.961839530332681,"Justification: All results were obtained on a single GPU to be comparable in terms of run
time. See §F for the details."
EXPERIMENTS COMPUTE RESOURCES,0.9628180039138943,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9637964774951077,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9647749510763209,9. Code Of Ethics
CODE OF ETHICS,0.9657534246575342,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9667318982387475,Answer: [Yes]
CODE OF ETHICS,0.9677103718199609,Justification: We have read the Code of Ethics and believe that our work conforms to it.
CODE OF ETHICS,0.9686888454011742,Guidelines:
CODE OF ETHICS,0.9696673189823874,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9706457925636007,10. Broader Impacts
BROADER IMPACTS,0.9716242661448141,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9726027397260274,Answer: [Yes]
BROADER IMPACTS,0.9735812133072407,"Justification: This work aims to provide a simplifying perspective and implementations
of otherwise hard-to-access operations for convolutions to facilitate the exploration of
algorithmic ideas and advance existing second-order methods. We don’t see any direct
negative societal impacts."
BROADER IMPACTS,0.974559686888454,Guidelines:
BROADER IMPACTS,0.9755381604696673,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9765166340508806,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not release any data or models that have a high risk for misuse.
Guidelines:"
BROADER IMPACTS,0.9774951076320939,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.9784735812133072,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We cite the papers introducing the neural network architectures and data sets
used in our experiments.
Guidelines:"
BROADER IMPACTS,0.9794520547945206,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset."
BROADER IMPACTS,0.9804305283757339,"• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9814090019569471,13. New Assets
NEW ASSETS,0.9823874755381604,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9833659491193738,Answer: [NA]
NEW ASSETS,0.9843444227005871,Justification: The paper does not release new assets.
NEW ASSETS,0.9853228962818004,Guidelines:
NEW ASSETS,0.9863013698630136,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987279843444227,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882583170254403,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892367906066536,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902152641878669,Justification: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911937377690803,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921722113502935,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931506849315068,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941291585127201,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951076320939335,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960861056751468,Justification: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99706457925636,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980430528375733,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990215264187867,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
